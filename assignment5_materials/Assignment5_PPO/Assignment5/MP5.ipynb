{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "from env import GameEnv\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "envs = []\n",
    "for i in range(num_envs):\n",
    "    envs.append(GameEnv('SpaceInvadersDeterministic-v4'))\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = envs[0].life\n",
    "state_size = envs[0].observation_space.shape\n",
    "action_size = envs[0].action_space.n\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],1)[0].unsqueeze(1))\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000250. clip: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:255: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:256: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:257: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Policy loss: -0.933571. Value loss: 8.549062. Entropy: 1.784475.\n",
      "Iteration 2: Policy loss: -0.937304. Value loss: 7.421401. Entropy: 1.785210.\n",
      "Iteration 3: Policy loss: -0.880273. Value loss: 7.842966. Entropy: 1.786776.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 4: Policy loss: -6.228714. Value loss: 57.180992. Entropy: 1.768686.\n",
      "Iteration 5: Policy loss: -6.499210. Value loss: 41.566692. Entropy: 1.783181.\n",
      "Iteration 6: Policy loss: -6.286735. Value loss: 38.287395. Entropy: 1.783888.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 7: Policy loss: -4.955023. Value loss: 394.677856. Entropy: 1.747445.\n",
      "Iteration 8: Policy loss: -4.575887. Value loss: 362.984131. Entropy: 1.758649.\n",
      "Iteration 9: Policy loss: -3.514594. Value loss: 238.865570. Entropy: 1.752203.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 10: Policy loss: -0.103457. Value loss: 48.649307. Entropy: 1.751076.\n",
      "Iteration 11: Policy loss: -0.223800. Value loss: 40.866898. Entropy: 1.757089.\n",
      "Iteration 12: Policy loss: 0.048850. Value loss: 36.668858. Entropy: 1.765509.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 13: Policy loss: 0.814252. Value loss: 32.401684. Entropy: 1.756799.\n",
      "Iteration 14: Policy loss: 0.865232. Value loss: 28.915726. Entropy: 1.755901.\n",
      "Iteration 15: Policy loss: 0.748689. Value loss: 28.484877. Entropy: 1.749890.\n",
      "now time :  2019-02-24 15:14:38.610788\n",
      "episode: 1   score: 65.0  epsilon: 1.0    steps: 317  evaluation reward: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2   score: 180.0  epsilon: 1.0    steps: 392  evaluation reward: 65.0\n",
      "episode: 3   score: 85.0  epsilon: 1.0    steps: 708  evaluation reward: 122.5\n",
      "episode: 4   score: 140.0  epsilon: 1.0    steps: 872  evaluation reward: 110.0\n",
      "episode: 5   score: 210.0  epsilon: 1.0    steps: 1020  evaluation reward: 117.5\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 16: Policy loss: -0.483179. Value loss: 36.736061. Entropy: 1.774945.\n",
      "Iteration 17: Policy loss: -0.488345. Value loss: 32.372993. Entropy: 1.774215.\n",
      "Iteration 18: Policy loss: -0.526784. Value loss: 33.224861. Entropy: 1.771197.\n",
      "episode: 6   score: 135.0  epsilon: 1.0    steps: 14  evaluation reward: 136.0\n",
      "episode: 7   score: 185.0  epsilon: 1.0    steps: 599  evaluation reward: 135.83333333333334\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 19: Policy loss: -0.984435. Value loss: 22.549641. Entropy: 1.762368.\n",
      "Iteration 20: Policy loss: -0.683435. Value loss: 18.380424. Entropy: 1.760056.\n",
      "Iteration 21: Policy loss: -0.686897. Value loss: 17.916265. Entropy: 1.761798.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 22: Policy loss: -0.927403. Value loss: 38.718159. Entropy: 1.769709.\n",
      "Iteration 23: Policy loss: -1.206169. Value loss: 35.496098. Entropy: 1.770012.\n",
      "Iteration 24: Policy loss: -0.948327. Value loss: 33.107239. Entropy: 1.766357.\n",
      "episode: 8   score: 80.0  epsilon: 1.0    steps: 373  evaluation reward: 142.85714285714286\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 25: Policy loss: -2.418429. Value loss: 52.319298. Entropy: 1.755671.\n",
      "Iteration 26: Policy loss: -2.132381. Value loss: 44.198318. Entropy: 1.755524.\n",
      "Iteration 27: Policy loss: -2.258995. Value loss: 36.286503. Entropy: 1.754331.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 28: Policy loss: 0.473067. Value loss: 72.294754. Entropy: 1.745448.\n",
      "Iteration 29: Policy loss: 0.559309. Value loss: 67.525230. Entropy: 1.748515.\n",
      "Iteration 30: Policy loss: 0.456727. Value loss: 66.774460. Entropy: 1.746452.\n",
      "episode: 9   score: 120.0  epsilon: 1.0    steps: 886  evaluation reward: 135.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 31: Policy loss: -3.148094. Value loss: 59.984165. Entropy: 1.739747.\n",
      "Iteration 32: Policy loss: -3.137136. Value loss: 49.248642. Entropy: 1.734861.\n",
      "Iteration 33: Policy loss: -3.332666. Value loss: 43.794708. Entropy: 1.730938.\n",
      "episode: 10   score: 120.0  epsilon: 1.0    steps: 425  evaluation reward: 133.33333333333334\n",
      "episode: 11   score: 100.0  epsilon: 1.0    steps: 533  evaluation reward: 132.0\n",
      "episode: 12   score: 245.0  epsilon: 1.0    steps: 935  evaluation reward: 129.0909090909091\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 34: Policy loss: 2.184305. Value loss: 36.915970. Entropy: 1.697148.\n",
      "Iteration 35: Policy loss: 2.201201. Value loss: 26.891306. Entropy: 1.705864.\n",
      "Iteration 36: Policy loss: 2.271887. Value loss: 24.268276. Entropy: 1.699276.\n",
      "episode: 13   score: 635.0  epsilon: 1.0    steps: 170  evaluation reward: 138.75\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 37: Policy loss: -2.160345. Value loss: 44.085094. Entropy: 1.653058.\n",
      "Iteration 38: Policy loss: -2.246434. Value loss: 35.697571. Entropy: 1.646736.\n",
      "Iteration 39: Policy loss: -2.196608. Value loss: 30.829710. Entropy: 1.654441.\n",
      "episode: 14   score: 265.0  epsilon: 1.0    steps: 40  evaluation reward: 176.92307692307693\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 40: Policy loss: 0.731489. Value loss: 26.191513. Entropy: 1.646406.\n",
      "Iteration 41: Policy loss: 0.973141. Value loss: 20.976532. Entropy: 1.653445.\n",
      "Iteration 42: Policy loss: 0.638206. Value loss: 20.836994. Entropy: 1.646858.\n",
      "episode: 15   score: 210.0  epsilon: 1.0    steps: 360  evaluation reward: 183.21428571428572\n",
      "episode: 16   score: 450.0  epsilon: 1.0    steps: 733  evaluation reward: 185.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 43: Policy loss: 0.027320. Value loss: 36.468151. Entropy: 1.622475.\n",
      "Iteration 44: Policy loss: 0.122344. Value loss: 28.463856. Entropy: 1.616292.\n",
      "Iteration 45: Policy loss: 0.217096. Value loss: 25.483204. Entropy: 1.626003.\n",
      "episode: 17   score: 125.0  epsilon: 1.0    steps: 486  evaluation reward: 201.5625\n",
      "episode: 18   score: 95.0  epsilon: 1.0    steps: 964  evaluation reward: 197.05882352941177\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 46: Policy loss: 0.655705. Value loss: 23.624229. Entropy: 1.635556.\n",
      "Iteration 47: Policy loss: 0.494142. Value loss: 19.895645. Entropy: 1.640614.\n",
      "Iteration 48: Policy loss: 0.451519. Value loss: 17.889841. Entropy: 1.636147.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 49: Policy loss: 1.401838. Value loss: 37.293957. Entropy: 1.607527.\n",
      "Iteration 50: Policy loss: 1.386104. Value loss: 30.390440. Entropy: 1.611473.\n",
      "Iteration 51: Policy loss: 1.537139. Value loss: 26.882795. Entropy: 1.600204.\n",
      "episode: 19   score: 75.0  epsilon: 1.0    steps: 137  evaluation reward: 191.38888888888889\n",
      "episode: 20   score: 180.0  epsilon: 1.0    steps: 858  evaluation reward: 185.26315789473685\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 52: Policy loss: 0.514818. Value loss: 35.254025. Entropy: 1.654212.\n",
      "Iteration 53: Policy loss: 0.272460. Value loss: 33.907673. Entropy: 1.612113.\n",
      "Iteration 54: Policy loss: 0.143846. Value loss: 34.160709. Entropy: 1.622679.\n",
      "episode: 21   score: 130.0  epsilon: 1.0    steps: 622  evaluation reward: 185.0\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 55: Policy loss: 0.621722. Value loss: 21.469595. Entropy: 1.606429.\n",
      "Iteration 56: Policy loss: 0.577039. Value loss: 14.756965. Entropy: 1.626859.\n",
      "Iteration 57: Policy loss: 0.747169. Value loss: 13.766615. Entropy: 1.608626.\n",
      "episode: 22   score: 135.0  epsilon: 1.0    steps: 25  evaluation reward: 182.38095238095238\n",
      "episode: 23   score: 115.0  epsilon: 1.0    steps: 350  evaluation reward: 180.22727272727272\n",
      "episode: 24   score: 110.0  epsilon: 1.0    steps: 491  evaluation reward: 177.3913043478261\n",
      "episode: 25   score: 105.0  epsilon: 1.0    steps: 653  evaluation reward: 174.58333333333334\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 58: Policy loss: 0.036170. Value loss: 17.760433. Entropy: 1.605153.\n",
      "Iteration 59: Policy loss: -0.155658. Value loss: 16.883125. Entropy: 1.610527.\n",
      "Iteration 60: Policy loss: 0.012741. Value loss: 14.732942. Entropy: 1.601800.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 61: Policy loss: -0.829420. Value loss: 27.815596. Entropy: 1.523269.\n",
      "Iteration 62: Policy loss: -0.784625. Value loss: 24.113804. Entropy: 1.542757.\n",
      "Iteration 63: Policy loss: -0.794073. Value loss: 23.167366. Entropy: 1.520177.\n",
      "episode: 26   score: 80.0  epsilon: 1.0    steps: 230  evaluation reward: 171.8\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 64: Policy loss: 0.762808. Value loss: 51.651913. Entropy: 1.613019.\n",
      "Iteration 65: Policy loss: 0.322737. Value loss: 51.440323. Entropy: 1.611616.\n",
      "Iteration 66: Policy loss: 0.814346. Value loss: 44.165154. Entropy: 1.618608.\n",
      "episode: 27   score: 140.0  epsilon: 1.0    steps: 1008  evaluation reward: 168.26923076923077\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 67: Policy loss: 1.742938. Value loss: 25.523262. Entropy: 1.584544.\n",
      "Iteration 68: Policy loss: 1.485211. Value loss: 23.226688. Entropy: 1.556361.\n",
      "Iteration 69: Policy loss: 1.383539. Value loss: 20.905457. Entropy: 1.549337.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 70: Policy loss: -1.168547. Value loss: 41.543522. Entropy: 1.612308.\n",
      "Iteration 71: Policy loss: -0.974660. Value loss: 34.523983. Entropy: 1.618903.\n",
      "Iteration 72: Policy loss: -0.974783. Value loss: 28.523718. Entropy: 1.588792.\n",
      "episode: 28   score: 50.0  epsilon: 1.0    steps: 189  evaluation reward: 167.22222222222223\n",
      "episode: 29   score: 215.0  epsilon: 1.0    steps: 792  evaluation reward: 163.03571428571428\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 73: Policy loss: -3.835442. Value loss: 424.311646. Entropy: 1.551224.\n",
      "Iteration 74: Policy loss: -3.234565. Value loss: 263.468567. Entropy: 1.522944.\n",
      "Iteration 75: Policy loss: -2.444774. Value loss: 195.061859. Entropy: 1.513202.\n",
      "episode: 30   score: 195.0  epsilon: 1.0    steps: 90  evaluation reward: 164.82758620689654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 31   score: 135.0  epsilon: 1.0    steps: 341  evaluation reward: 165.83333333333334\n",
      "episode: 32   score: 225.0  epsilon: 1.0    steps: 682  evaluation reward: 164.83870967741936\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 76: Policy loss: 2.063422. Value loss: 59.601566. Entropy: 1.223090.\n",
      "Iteration 77: Policy loss: 2.214382. Value loss: 43.739197. Entropy: 1.316381.\n",
      "Iteration 78: Policy loss: 1.914910. Value loss: 33.723267. Entropy: 1.338841.\n",
      "episode: 33   score: 125.0  epsilon: 1.0    steps: 398  evaluation reward: 166.71875\n",
      "episode: 34   score: 500.0  epsilon: 1.0    steps: 579  evaluation reward: 165.45454545454547\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 79: Policy loss: -0.890890. Value loss: 42.453869. Entropy: 1.343741.\n",
      "Iteration 80: Policy loss: -0.634966. Value loss: 35.943394. Entropy: 1.341413.\n",
      "Iteration 81: Policy loss: -0.839585. Value loss: 30.101677. Entropy: 1.356119.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 82: Policy loss: -6.095111. Value loss: 478.433167. Entropy: 1.159781.\n",
      "Iteration 83: Policy loss: -4.760579. Value loss: 517.351501. Entropy: 0.947567.\n",
      "Iteration 84: Policy loss: -3.534316. Value loss: 345.571655. Entropy: 0.960132.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 85: Policy loss: -2.726284. Value loss: 86.035400. Entropy: 1.209269.\n",
      "Iteration 86: Policy loss: -2.316062. Value loss: 57.769615. Entropy: 1.194323.\n",
      "Iteration 87: Policy loss: -2.343661. Value loss: 48.269939. Entropy: 1.175301.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 88: Policy loss: 1.848013. Value loss: 68.029846. Entropy: 1.080623.\n",
      "Iteration 89: Policy loss: 1.683489. Value loss: 48.887291. Entropy: 1.125038.\n",
      "Iteration 90: Policy loss: 1.742402. Value loss: 46.679111. Entropy: 1.062401.\n",
      "episode: 35   score: 105.0  epsilon: 1.0    steps: 445  evaluation reward: 175.2941176470588\n",
      "episode: 36   score: 225.0  epsilon: 1.0    steps: 802  evaluation reward: 173.28571428571428\n",
      "episode: 37   score: 395.0  epsilon: 1.0    steps: 920  evaluation reward: 174.72222222222223\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 91: Policy loss: 0.231775. Value loss: 52.217113. Entropy: 1.051689.\n",
      "Iteration 92: Policy loss: 0.185989. Value loss: 40.381969. Entropy: 1.066802.\n",
      "Iteration 93: Policy loss: 0.351971. Value loss: 39.504463. Entropy: 1.066107.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 94: Policy loss: 1.555954. Value loss: 74.033020. Entropy: 0.792204.\n",
      "Iteration 95: Policy loss: 1.832766. Value loss: 57.095741. Entropy: 0.771259.\n",
      "Iteration 96: Policy loss: 2.280687. Value loss: 50.518169. Entropy: 0.794537.\n",
      "episode: 38   score: 550.0  epsilon: 1.0    steps: 165  evaluation reward: 180.67567567567568\n",
      "episode: 39   score: 250.0  epsilon: 1.0    steps: 265  evaluation reward: 190.39473684210526\n",
      "episode: 40   score: 195.0  epsilon: 1.0    steps: 691  evaluation reward: 191.92307692307693\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 97: Policy loss: -0.105703. Value loss: 46.708214. Entropy: 1.235281.\n",
      "Iteration 98: Policy loss: 0.039303. Value loss: 40.741020. Entropy: 1.239292.\n",
      "Iteration 99: Policy loss: 0.114244. Value loss: 36.414970. Entropy: 1.226588.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 100: Policy loss: 2.243045. Value loss: 47.166756. Entropy: 1.244490.\n",
      "Iteration 101: Policy loss: 2.209394. Value loss: 36.118397. Entropy: 1.153321.\n",
      "Iteration 102: Policy loss: 1.941626. Value loss: 36.290817. Entropy: 1.174774.\n",
      "episode: 41   score: 525.0  epsilon: 1.0    steps: 78  evaluation reward: 192.0\n",
      "episode: 42   score: 130.0  epsilon: 1.0    steps: 417  evaluation reward: 200.1219512195122\n",
      "episode: 43   score: 260.0  epsilon: 1.0    steps: 544  evaluation reward: 198.45238095238096\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 103: Policy loss: -0.758256. Value loss: 287.920197. Entropy: 1.240597.\n",
      "Iteration 104: Policy loss: -0.166466. Value loss: 184.683243. Entropy: 1.176256.\n",
      "Iteration 105: Policy loss: 0.102351. Value loss: 175.700546. Entropy: 1.162556.\n",
      "episode: 44   score: 115.0  epsilon: 1.0    steps: 779  evaluation reward: 199.88372093023256\n",
      "episode: 45   score: 255.0  epsilon: 1.0    steps: 1012  evaluation reward: 197.95454545454547\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 106: Policy loss: -0.857957. Value loss: 324.840363. Entropy: 1.109042.\n",
      "Iteration 107: Policy loss: -0.901085. Value loss: 258.132538. Entropy: 1.114009.\n",
      "Iteration 108: Policy loss: -0.518535. Value loss: 196.284424. Entropy: 1.098460.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 109: Policy loss: -2.882002. Value loss: 50.271870. Entropy: 1.234674.\n",
      "Iteration 110: Policy loss: -3.021986. Value loss: 44.770424. Entropy: 1.227713.\n",
      "Iteration 111: Policy loss: -2.968808. Value loss: 37.769241. Entropy: 1.228093.\n",
      "episode: 46   score: 150.0  epsilon: 1.0    steps: 761  evaluation reward: 199.22222222222223\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 112: Policy loss: -0.942938. Value loss: 93.835739. Entropy: 1.293058.\n",
      "Iteration 113: Policy loss: -0.735566. Value loss: 66.421173. Entropy: 1.308941.\n",
      "Iteration 114: Policy loss: -1.129577. Value loss: 56.314602. Entropy: 1.305784.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 115: Policy loss: 1.049184. Value loss: 29.100180. Entropy: 1.146315.\n",
      "Iteration 116: Policy loss: 1.169796. Value loss: 26.082809. Entropy: 1.102093.\n",
      "Iteration 117: Policy loss: 1.095363. Value loss: 22.009142. Entropy: 1.119651.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 118: Policy loss: 2.171849. Value loss: 81.627495. Entropy: 1.311746.\n",
      "Iteration 119: Policy loss: 1.981412. Value loss: 70.023605. Entropy: 1.297196.\n",
      "Iteration 120: Policy loss: 2.153305. Value loss: 61.596512. Entropy: 1.296317.\n",
      "episode: 47   score: 235.0  epsilon: 1.0    steps: 434  evaluation reward: 198.15217391304347\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 121: Policy loss: -0.247377. Value loss: 61.294846. Entropy: 1.321579.\n",
      "Iteration 122: Policy loss: -0.385737. Value loss: 52.415356. Entropy: 1.321207.\n",
      "Iteration 123: Policy loss: -0.345120. Value loss: 46.355747. Entropy: 1.334300.\n",
      "episode: 48   score: 630.0  epsilon: 1.0    steps: 288  evaluation reward: 198.93617021276594\n",
      "episode: 49   score: 185.0  epsilon: 1.0    steps: 809  evaluation reward: 207.91666666666666\n",
      "episode: 50   score: 225.0  epsilon: 1.0    steps: 1011  evaluation reward: 207.44897959183675\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 124: Policy loss: 0.763037. Value loss: 62.514351. Entropy: 1.242839.\n",
      "Iteration 125: Policy loss: 0.545279. Value loss: 50.878548. Entropy: 1.243010.\n",
      "Iteration 126: Policy loss: 0.599525. Value loss: 43.524017. Entropy: 1.250964.\n",
      "now time :  2019-02-24 15:16:41.361986\n",
      "episode: 51   score: 320.0  epsilon: 1.0    steps: 65  evaluation reward: 207.8\n",
      "episode: 52   score: 385.0  epsilon: 1.0    steps: 541  evaluation reward: 210.0\n",
      "episode: 53   score: 180.0  epsilon: 1.0    steps: 728  evaluation reward: 213.3653846153846\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 127: Policy loss: -0.761207. Value loss: 321.229767. Entropy: 1.136396.\n",
      "Iteration 128: Policy loss: -0.273614. Value loss: 302.571930. Entropy: 1.137100.\n",
      "Iteration 129: Policy loss: 0.178142. Value loss: 173.384430. Entropy: 1.141596.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 130: Policy loss: 2.941154. Value loss: 55.238796. Entropy: 1.095191.\n",
      "Iteration 131: Policy loss: 2.948917. Value loss: 44.091404. Entropy: 1.061782.\n",
      "Iteration 132: Policy loss: 3.049692. Value loss: 40.716759. Entropy: 1.033782.\n",
      "episode: 54   score: 80.0  epsilon: 1.0    steps: 825  evaluation reward: 212.73584905660377\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 133: Policy loss: -2.674999. Value loss: 274.101166. Entropy: 1.310185.\n",
      "Iteration 134: Policy loss: -2.727012. Value loss: 328.711792. Entropy: 1.375437.\n",
      "Iteration 135: Policy loss: -1.841608. Value loss: 257.197815. Entropy: 1.281764.\n",
      "episode: 55   score: 770.0  epsilon: 1.0    steps: 184  evaluation reward: 210.27777777777777\n",
      "episode: 56   score: 145.0  epsilon: 1.0    steps: 1004  evaluation reward: 220.45454545454547\n",
      "Training network. lr: 0.000249. clip: 0.099696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 136: Policy loss: 0.279774. Value loss: 57.736980. Entropy: 1.180546.\n",
      "Iteration 137: Policy loss: 0.093887. Value loss: 44.966930. Entropy: 1.146017.\n",
      "Iteration 138: Policy loss: 0.452482. Value loss: 36.964764. Entropy: 1.147530.\n",
      "episode: 57   score: 165.0  epsilon: 1.0    steps: 538  evaluation reward: 219.10714285714286\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 139: Policy loss: -0.680494. Value loss: 89.562531. Entropy: 0.845747.\n",
      "Iteration 140: Policy loss: -0.973763. Value loss: 65.459816. Entropy: 0.879256.\n",
      "Iteration 141: Policy loss: -1.026413. Value loss: 51.685360. Entropy: 0.843506.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 142: Policy loss: 8.764532. Value loss: 116.229263. Entropy: 0.902161.\n",
      "Iteration 143: Policy loss: 8.085527. Value loss: 84.246696. Entropy: 0.814995.\n",
      "Iteration 144: Policy loss: 8.233542. Value loss: 69.685928. Entropy: 0.877447.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 145: Policy loss: 4.298084. Value loss: 63.051865. Entropy: 1.179509.\n",
      "Iteration 146: Policy loss: 4.194874. Value loss: 49.704609. Entropy: 1.101553.\n",
      "Iteration 147: Policy loss: 4.218112. Value loss: 47.062675. Entropy: 1.070224.\n",
      "episode: 58   score: 260.0  epsilon: 1.0    steps: 509  evaluation reward: 218.1578947368421\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 148: Policy loss: 2.240349. Value loss: 63.340714. Entropy: 1.030165.\n",
      "Iteration 149: Policy loss: 2.204616. Value loss: 48.536545. Entropy: 1.095853.\n",
      "Iteration 150: Policy loss: 2.039967. Value loss: 42.672794. Entropy: 1.127931.\n",
      "episode: 59   score: 285.0  epsilon: 1.0    steps: 11  evaluation reward: 218.8793103448276\n",
      "episode: 60   score: 260.0  epsilon: 1.0    steps: 683  evaluation reward: 220.0\n",
      "episode: 61   score: 180.0  epsilon: 1.0    steps: 867  evaluation reward: 220.66666666666666\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 151: Policy loss: 5.313827. Value loss: 51.757629. Entropy: 0.770835.\n",
      "Iteration 152: Policy loss: 5.188068. Value loss: 32.965027. Entropy: 0.852472.\n",
      "Iteration 153: Policy loss: 5.488963. Value loss: 28.272482. Entropy: 0.846864.\n",
      "episode: 62   score: 210.0  epsilon: 1.0    steps: 253  evaluation reward: 220.0\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 154: Policy loss: 2.682210. Value loss: 59.714203. Entropy: 0.618729.\n",
      "Iteration 155: Policy loss: 2.226426. Value loss: 53.583305. Entropy: 0.621518.\n",
      "Iteration 156: Policy loss: 2.445094. Value loss: 48.867725. Entropy: 0.649441.\n",
      "episode: 63   score: 675.0  epsilon: 1.0    steps: 305  evaluation reward: 219.83870967741936\n",
      "episode: 64   score: 155.0  epsilon: 1.0    steps: 577  evaluation reward: 227.06349206349208\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 157: Policy loss: 3.942969. Value loss: 48.878086. Entropy: 0.929768.\n",
      "Iteration 158: Policy loss: 3.364049. Value loss: 41.404636. Entropy: 0.842756.\n",
      "Iteration 159: Policy loss: 4.006340. Value loss: 29.650513. Entropy: 0.926047.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 160: Policy loss: 1.521314. Value loss: 31.965260. Entropy: 0.879169.\n",
      "Iteration 161: Policy loss: 1.263758. Value loss: 32.056568. Entropy: 0.909262.\n",
      "Iteration 162: Policy loss: 1.514593. Value loss: 27.369452. Entropy: 0.886529.\n",
      "episode: 65   score: 300.0  epsilon: 1.0    steps: 945  evaluation reward: 225.9375\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 163: Policy loss: 2.196386. Value loss: 25.030926. Entropy: 1.066625.\n",
      "Iteration 164: Policy loss: 2.231745. Value loss: 18.403429. Entropy: 1.103508.\n",
      "Iteration 165: Policy loss: 2.329993. Value loss: 18.156399. Entropy: 1.085265.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 166: Policy loss: 0.512629. Value loss: 16.979771. Entropy: 0.734082.\n",
      "Iteration 167: Policy loss: 0.572339. Value loss: 12.487455. Entropy: 0.804204.\n",
      "Iteration 168: Policy loss: 0.586212. Value loss: 11.268447. Entropy: 0.673561.\n",
      "episode: 66   score: 210.0  epsilon: 1.0    steps: 75  evaluation reward: 227.07692307692307\n",
      "episode: 67   score: 210.0  epsilon: 1.0    steps: 432  evaluation reward: 226.8181818181818\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 169: Policy loss: 1.549374. Value loss: 31.379429. Entropy: 0.924310.\n",
      "Iteration 170: Policy loss: 1.196719. Value loss: 31.209705. Entropy: 0.903314.\n",
      "Iteration 171: Policy loss: 1.512773. Value loss: 25.203621. Entropy: 0.917949.\n",
      "episode: 68   score: 180.0  epsilon: 1.0    steps: 790  evaluation reward: 226.56716417910448\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 172: Policy loss: 1.535172. Value loss: 20.059902. Entropy: 0.727462.\n",
      "Iteration 173: Policy loss: 1.567450. Value loss: 15.388036. Entropy: 0.776517.\n",
      "Iteration 174: Policy loss: 1.461262. Value loss: 17.092525. Entropy: 0.757198.\n",
      "episode: 69   score: 210.0  epsilon: 1.0    steps: 188  evaluation reward: 225.88235294117646\n",
      "episode: 70   score: 260.0  epsilon: 1.0    steps: 647  evaluation reward: 225.65217391304347\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 175: Policy loss: 1.728387. Value loss: 37.102703. Entropy: 1.047529.\n",
      "Iteration 176: Policy loss: 1.607515. Value loss: 31.610258. Entropy: 1.086722.\n",
      "Iteration 177: Policy loss: 1.682749. Value loss: 31.334249. Entropy: 1.077971.\n",
      "episode: 71   score: 210.0  epsilon: 1.0    steps: 276  evaluation reward: 226.14285714285714\n",
      "episode: 72   score: 210.0  epsilon: 1.0    steps: 544  evaluation reward: 225.91549295774647\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 178: Policy loss: 1.832071. Value loss: 13.144829. Entropy: 1.107081.\n",
      "Iteration 179: Policy loss: 1.768463. Value loss: 9.653206. Entropy: 1.123258.\n",
      "Iteration 180: Policy loss: 1.682218. Value loss: 9.373620. Entropy: 1.147418.\n",
      "episode: 73   score: 210.0  epsilon: 1.0    steps: 996  evaluation reward: 225.69444444444446\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 181: Policy loss: 0.681844. Value loss: 26.075066. Entropy: 1.156198.\n",
      "Iteration 182: Policy loss: 0.769416. Value loss: 23.626923. Entropy: 1.147141.\n",
      "Iteration 183: Policy loss: 0.533325. Value loss: 26.311274. Entropy: 1.142447.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 184: Policy loss: 0.018444. Value loss: 7.900403. Entropy: 1.151366.\n",
      "Iteration 185: Policy loss: -0.198904. Value loss: 5.997378. Entropy: 1.147987.\n",
      "Iteration 186: Policy loss: -0.091090. Value loss: 5.262963. Entropy: 1.134166.\n",
      "episode: 74   score: 210.0  epsilon: 1.0    steps: 126  evaluation reward: 225.4794520547945\n",
      "episode: 75   score: 160.0  epsilon: 1.0    steps: 461  evaluation reward: 225.27027027027026\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 187: Policy loss: 1.601226. Value loss: 25.088820. Entropy: 1.065418.\n",
      "Iteration 188: Policy loss: 1.613426. Value loss: 23.500399. Entropy: 1.066962.\n",
      "Iteration 189: Policy loss: 1.419323. Value loss: 20.509459. Entropy: 1.054477.\n",
      "episode: 76   score: 180.0  epsilon: 1.0    steps: 893  evaluation reward: 224.4\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 190: Policy loss: -2.406864. Value loss: 281.111023. Entropy: 0.999665.\n",
      "Iteration 191: Policy loss: -2.933806. Value loss: 322.002167. Entropy: 1.084216.\n",
      "Iteration 192: Policy loss: -2.230618. Value loss: 277.587463. Entropy: 1.037937.\n",
      "episode: 77   score: 210.0  epsilon: 1.0    steps: 733  evaluation reward: 223.81578947368422\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 193: Policy loss: 0.240628. Value loss: 38.002087. Entropy: 1.008538.\n",
      "Iteration 194: Policy loss: 0.064606. Value loss: 27.139458. Entropy: 1.071558.\n",
      "Iteration 195: Policy loss: 0.163174. Value loss: 24.918205. Entropy: 1.046367.\n",
      "episode: 78   score: 410.0  epsilon: 1.0    steps: 173  evaluation reward: 223.63636363636363\n",
      "episode: 79   score: 210.0  epsilon: 1.0    steps: 601  evaluation reward: 226.02564102564102\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 196: Policy loss: 0.778033. Value loss: 19.752377. Entropy: 1.211656.\n",
      "Iteration 197: Policy loss: 0.883641. Value loss: 15.123523. Entropy: 1.220977.\n",
      "Iteration 198: Policy loss: 0.708359. Value loss: 13.944030. Entropy: 1.209092.\n",
      "episode: 80   score: 75.0  epsilon: 1.0    steps: 29  evaluation reward: 225.8227848101266\n",
      "Training network. lr: 0.000249. clip: 0.099548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 199: Policy loss: -0.208454. Value loss: 16.540390. Entropy: 1.143352.\n",
      "Iteration 200: Policy loss: -0.234761. Value loss: 15.044557. Entropy: 1.144075.\n",
      "Iteration 201: Policy loss: -0.198929. Value loss: 12.358613. Entropy: 1.126790.\n",
      "episode: 81   score: 300.0  epsilon: 1.0    steps: 384  evaluation reward: 223.9375\n",
      "episode: 82   score: 75.0  epsilon: 1.0    steps: 800  evaluation reward: 224.87654320987653\n",
      "episode: 83   score: 180.0  epsilon: 1.0    steps: 915  evaluation reward: 223.0487804878049\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 202: Policy loss: 0.381033. Value loss: 16.715450. Entropy: 1.271847.\n",
      "Iteration 203: Policy loss: 0.302751. Value loss: 14.012074. Entropy: 1.257766.\n",
      "Iteration 204: Policy loss: 0.414481. Value loss: 12.156748. Entropy: 1.271204.\n",
      "episode: 84   score: 185.0  epsilon: 1.0    steps: 504  evaluation reward: 222.53012048192772\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 205: Policy loss: 0.082901. Value loss: 17.175900. Entropy: 1.201136.\n",
      "Iteration 206: Policy loss: 0.012977. Value loss: 13.387708. Entropy: 1.213865.\n",
      "Iteration 207: Policy loss: -0.120964. Value loss: 12.334860. Entropy: 1.221403.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 208: Policy loss: -0.215473. Value loss: 18.740797. Entropy: 1.168641.\n",
      "Iteration 209: Policy loss: -0.179337. Value loss: 17.592764. Entropy: 1.139788.\n",
      "Iteration 210: Policy loss: -0.168729. Value loss: 16.211761. Entropy: 1.151733.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 211: Policy loss: -2.235190. Value loss: 289.472992. Entropy: 1.019674.\n",
      "Iteration 212: Policy loss: -2.607814. Value loss: 250.257339. Entropy: 0.983227.\n",
      "Iteration 213: Policy loss: -1.999123. Value loss: 237.109344. Entropy: 0.908701.\n",
      "episode: 85   score: 380.0  epsilon: 1.0    steps: 240  evaluation reward: 222.08333333333334\n",
      "episode: 86   score: 75.0  epsilon: 1.0    steps: 287  evaluation reward: 223.94117647058823\n",
      "episode: 87   score: 155.0  epsilon: 1.0    steps: 655  evaluation reward: 222.2093023255814\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 214: Policy loss: -0.728226. Value loss: 19.765902. Entropy: 1.021173.\n",
      "Iteration 215: Policy loss: -0.383243. Value loss: 15.818055. Entropy: 0.994333.\n",
      "Iteration 216: Policy loss: -0.228190. Value loss: 13.406292. Entropy: 0.987161.\n",
      "episode: 88   score: 180.0  epsilon: 1.0    steps: 80  evaluation reward: 221.4367816091954\n",
      "episode: 89   score: 180.0  epsilon: 1.0    steps: 574  evaluation reward: 220.9659090909091\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 217: Policy loss: 0.530224. Value loss: 15.689758. Entropy: 0.816075.\n",
      "Iteration 218: Policy loss: 0.572339. Value loss: 12.063459. Entropy: 0.792961.\n",
      "Iteration 219: Policy loss: 0.442976. Value loss: 9.327829. Entropy: 0.832962.\n",
      "episode: 90   score: 210.0  epsilon: 1.0    steps: 851  evaluation reward: 220.5056179775281\n",
      "episode: 91   score: 185.0  epsilon: 1.0    steps: 964  evaluation reward: 220.38888888888889\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 220: Policy loss: -1.946445. Value loss: 28.631165. Entropy: 0.930854.\n",
      "Iteration 221: Policy loss: -2.326828. Value loss: 27.617636. Entropy: 0.894055.\n",
      "Iteration 222: Policy loss: -2.273690. Value loss: 27.324270. Entropy: 0.871246.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 223: Policy loss: -0.381061. Value loss: 14.624103. Entropy: 0.991760.\n",
      "Iteration 224: Policy loss: -0.627409. Value loss: 14.854004. Entropy: 0.972133.\n",
      "Iteration 225: Policy loss: -0.536582. Value loss: 13.584424. Entropy: 0.986186.\n",
      "episode: 92   score: 210.0  epsilon: 1.0    steps: 433  evaluation reward: 220.0\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 226: Policy loss: -0.484037. Value loss: 11.658957. Entropy: 0.999751.\n",
      "Iteration 227: Policy loss: -0.435969. Value loss: 10.245132. Entropy: 1.012999.\n",
      "Iteration 228: Policy loss: -0.501006. Value loss: 10.150037. Entropy: 1.013873.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 229: Policy loss: 0.212987. Value loss: 10.512563. Entropy: 0.907549.\n",
      "Iteration 230: Policy loss: 0.244755. Value loss: 8.651227. Entropy: 0.923055.\n",
      "Iteration 231: Policy loss: 0.313768. Value loss: 6.511497. Entropy: 0.906347.\n",
      "episode: 93   score: 105.0  epsilon: 1.0    steps: 131  evaluation reward: 219.8913043478261\n",
      "episode: 94   score: 180.0  epsilon: 1.0    steps: 329  evaluation reward: 218.65591397849462\n",
      "episode: 95   score: 210.0  epsilon: 1.0    steps: 760  evaluation reward: 218.24468085106383\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 232: Policy loss: 0.209357. Value loss: 10.943666. Entropy: 0.905838.\n",
      "Iteration 233: Policy loss: 0.087338. Value loss: 10.816657. Entropy: 0.901206.\n",
      "Iteration 234: Policy loss: 0.216643. Value loss: 8.571095. Entropy: 0.907852.\n",
      "episode: 96   score: 180.0  epsilon: 1.0    steps: 631  evaluation reward: 218.1578947368421\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 235: Policy loss: 0.320219. Value loss: 14.712046. Entropy: 0.853012.\n",
      "Iteration 236: Policy loss: 0.366738. Value loss: 15.397930. Entropy: 0.886234.\n",
      "Iteration 237: Policy loss: 0.284276. Value loss: 13.572724. Entropy: 0.852537.\n",
      "episode: 97   score: 180.0  epsilon: 1.0    steps: 6  evaluation reward: 217.76041666666666\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 238: Policy loss: -0.171027. Value loss: 19.439960. Entropy: 0.825076.\n",
      "Iteration 239: Policy loss: -0.194689. Value loss: 17.986307. Entropy: 0.811705.\n",
      "Iteration 240: Policy loss: -0.240718. Value loss: 16.526989. Entropy: 0.808293.\n",
      "episode: 98   score: 210.0  epsilon: 1.0    steps: 774  evaluation reward: 217.37113402061857\n",
      "episode: 99   score: 180.0  epsilon: 1.0    steps: 899  evaluation reward: 217.29591836734693\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 241: Policy loss: 0.122583. Value loss: 7.445455. Entropy: 0.921638.\n",
      "Iteration 242: Policy loss: 0.115245. Value loss: 7.226506. Entropy: 0.924879.\n",
      "Iteration 243: Policy loss: 0.152818. Value loss: 7.348882. Entropy: 0.929844.\n",
      "episode: 100   score: 210.0  epsilon: 1.0    steps: 484  evaluation reward: 216.91919191919192\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 244: Policy loss: -0.604644. Value loss: 16.799784. Entropy: 0.945533.\n",
      "Iteration 245: Policy loss: -0.614223. Value loss: 14.101635. Entropy: 0.904602.\n",
      "Iteration 246: Policy loss: -0.718674. Value loss: 14.648037. Entropy: 0.862377.\n",
      "now time :  2019-02-24 15:18:56.673959\n",
      "episode: 101   score: 105.0  epsilon: 1.0    steps: 150  evaluation reward: 216.85\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 247: Policy loss: -0.034614. Value loss: 8.146865. Entropy: 0.997065.\n",
      "Iteration 248: Policy loss: 0.213860. Value loss: 6.299878. Entropy: 1.011418.\n",
      "Iteration 249: Policy loss: 0.022105. Value loss: 6.013772. Entropy: 1.014082.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 250: Policy loss: -0.219333. Value loss: 12.969766. Entropy: 0.856157.\n",
      "Iteration 251: Policy loss: -0.193869. Value loss: 13.430975. Entropy: 0.823501.\n",
      "Iteration 252: Policy loss: -0.107442. Value loss: 12.710606. Entropy: 0.890003.\n",
      "episode: 102   score: 210.0  epsilon: 1.0    steps: 264  evaluation reward: 217.25\n",
      "episode: 103   score: 180.0  epsilon: 1.0    steps: 689  evaluation reward: 217.55\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 253: Policy loss: 0.561178. Value loss: 6.470990. Entropy: 1.051116.\n",
      "Iteration 254: Policy loss: 0.563864. Value loss: 5.714284. Entropy: 1.019339.\n",
      "Iteration 255: Policy loss: 0.541392. Value loss: 5.578877. Entropy: 1.049740.\n",
      "episode: 104   score: 180.0  epsilon: 1.0    steps: 63  evaluation reward: 218.5\n",
      "episode: 105   score: 180.0  epsilon: 1.0    steps: 560  evaluation reward: 218.9\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 256: Policy loss: 0.034631. Value loss: 16.312943. Entropy: 0.914697.\n",
      "Iteration 257: Policy loss: 0.090640. Value loss: 13.357584. Entropy: 0.904682.\n",
      "Iteration 258: Policy loss: -0.046790. Value loss: 11.280712. Entropy: 0.895235.\n",
      "episode: 106   score: 180.0  epsilon: 1.0    steps: 827  evaluation reward: 218.6\n",
      "episode: 107   score: 180.0  epsilon: 1.0    steps: 950  evaluation reward: 219.05\n",
      "Training network. lr: 0.000248. clip: 0.099235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 259: Policy loss: -0.268778. Value loss: 21.454350. Entropy: 0.935555.\n",
      "Iteration 260: Policy loss: -0.621317. Value loss: 18.199528. Entropy: 0.919267.\n",
      "Iteration 261: Policy loss: -0.388750. Value loss: 18.179459. Entropy: 0.929936.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 262: Policy loss: -0.230138. Value loss: 11.210797. Entropy: 0.955687.\n",
      "Iteration 263: Policy loss: -0.081947. Value loss: 10.507551. Entropy: 0.969486.\n",
      "Iteration 264: Policy loss: -0.118327. Value loss: 8.451283. Entropy: 0.987419.\n",
      "episode: 108   score: 180.0  epsilon: 1.0    steps: 207  evaluation reward: 219.0\n",
      "episode: 109   score: 180.0  epsilon: 1.0    steps: 407  evaluation reward: 220.0\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 265: Policy loss: 0.104848. Value loss: 6.911222. Entropy: 1.076585.\n",
      "Iteration 266: Policy loss: 0.060653. Value loss: 5.315980. Entropy: 1.106098.\n",
      "Iteration 267: Policy loss: 0.222223. Value loss: 6.139189. Entropy: 1.071675.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 268: Policy loss: 0.528155. Value loss: 8.247688. Entropy: 0.869992.\n",
      "Iteration 269: Policy loss: 0.495048. Value loss: 6.222245. Entropy: 0.840236.\n",
      "Iteration 270: Policy loss: 0.530850. Value loss: 5.337557. Entropy: 0.858779.\n",
      "episode: 110   score: 210.0  epsilon: 1.0    steps: 343  evaluation reward: 220.6\n",
      "episode: 111   score: 185.0  epsilon: 1.0    steps: 723  evaluation reward: 221.5\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 271: Policy loss: -0.212426. Value loss: 14.729489. Entropy: 0.886372.\n",
      "Iteration 272: Policy loss: -0.139257. Value loss: 11.030138. Entropy: 0.864199.\n",
      "Iteration 273: Policy loss: -0.343368. Value loss: 10.701234. Entropy: 0.890905.\n",
      "episode: 112   score: 185.0  epsilon: 1.0    steps: 107  evaluation reward: 222.35\n",
      "episode: 113   score: 180.0  epsilon: 1.0    steps: 597  evaluation reward: 221.75\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 274: Policy loss: 0.971726. Value loss: 8.678307. Entropy: 0.828123.\n",
      "Iteration 275: Policy loss: 0.821062. Value loss: 7.645129. Entropy: 0.857767.\n",
      "Iteration 276: Policy loss: 0.935822. Value loss: 7.909120. Entropy: 0.856967.\n",
      "episode: 114   score: 210.0  epsilon: 1.0    steps: 877  evaluation reward: 217.2\n",
      "episode: 115   score: 210.0  epsilon: 1.0    steps: 1001  evaluation reward: 216.65\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 277: Policy loss: -0.797172. Value loss: 15.468156. Entropy: 0.802383.\n",
      "Iteration 278: Policy loss: -0.727226. Value loss: 13.948704. Entropy: 0.818068.\n",
      "Iteration 279: Policy loss: -0.670892. Value loss: 14.462873. Entropy: 0.785178.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 280: Policy loss: -0.177868. Value loss: 6.644176. Entropy: 0.990798.\n",
      "Iteration 281: Policy loss: -0.205803. Value loss: 6.763305. Entropy: 0.977345.\n",
      "Iteration 282: Policy loss: -0.292183. Value loss: 5.546758. Entropy: 0.975283.\n",
      "episode: 116   score: 210.0  epsilon: 1.0    steps: 458  evaluation reward: 216.65\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 283: Policy loss: -0.388253. Value loss: 11.172213. Entropy: 1.007727.\n",
      "Iteration 284: Policy loss: -0.383353. Value loss: 10.525341. Entropy: 1.033895.\n",
      "Iteration 285: Policy loss: -0.425325. Value loss: 9.976881. Entropy: 1.014066.\n",
      "episode: 117   score: 210.0  epsilon: 1.0    steps: 130  evaluation reward: 214.25\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 286: Policy loss: 0.276568. Value loss: 2.339179. Entropy: 1.096843.\n",
      "Iteration 287: Policy loss: 0.271826. Value loss: 1.902494. Entropy: 1.109400.\n",
      "Iteration 288: Policy loss: 0.239669. Value loss: 2.074585. Entropy: 1.082706.\n",
      "episode: 118   score: 165.0  epsilon: 1.0    steps: 368  evaluation reward: 215.1\n",
      "episode: 119   score: 160.0  epsilon: 1.0    steps: 747  evaluation reward: 215.8\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 289: Policy loss: 1.077165. Value loss: 13.554803. Entropy: 0.933024.\n",
      "Iteration 290: Policy loss: 1.135784. Value loss: 10.460126. Entropy: 0.896672.\n",
      "Iteration 291: Policy loss: 0.798922. Value loss: 7.793099. Entropy: 0.936057.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 292: Policy loss: -0.171554. Value loss: 7.814137. Entropy: 0.934798.\n",
      "Iteration 293: Policy loss: -0.163887. Value loss: 7.297875. Entropy: 0.911371.\n",
      "Iteration 294: Policy loss: -0.103088. Value loss: 4.947553. Entropy: 0.922125.\n",
      "episode: 120   score: 210.0  epsilon: 1.0    steps: 74  evaluation reward: 216.65\n",
      "episode: 121   score: 180.0  epsilon: 1.0    steps: 516  evaluation reward: 216.95\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 295: Policy loss: 0.326046. Value loss: 15.269770. Entropy: 0.837872.\n",
      "Iteration 296: Policy loss: 0.342688. Value loss: 11.677950. Entropy: 0.817971.\n",
      "Iteration 297: Policy loss: 0.212876. Value loss: 13.462877. Entropy: 0.808223.\n",
      "episode: 122   score: 180.0  epsilon: 1.0    steps: 806  evaluation reward: 217.45\n",
      "episode: 123   score: 180.0  epsilon: 1.0    steps: 910  evaluation reward: 217.9\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 298: Policy loss: 0.560357. Value loss: 5.854590. Entropy: 0.872843.\n",
      "Iteration 299: Policy loss: 0.595056. Value loss: 4.362528. Entropy: 0.909262.\n",
      "Iteration 300: Policy loss: 0.573536. Value loss: 4.268181. Entropy: 0.935154.\n",
      "episode: 124   score: 180.0  epsilon: 1.0    steps: 509  evaluation reward: 218.55\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 301: Policy loss: -0.633965. Value loss: 17.888250. Entropy: 0.870777.\n",
      "Iteration 302: Policy loss: -0.671692. Value loss: 16.223232. Entropy: 0.885307.\n",
      "Iteration 303: Policy loss: -0.563456. Value loss: 14.945847. Entropy: 0.883475.\n",
      "episode: 125   score: 185.0  epsilon: 1.0    steps: 181  evaluation reward: 219.25\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 304: Policy loss: 0.353411. Value loss: 7.913526. Entropy: 1.000376.\n",
      "Iteration 305: Policy loss: 0.401349. Value loss: 7.006676. Entropy: 1.053931.\n",
      "Iteration 306: Policy loss: 0.380340. Value loss: 4.311476. Entropy: 1.045681.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 307: Policy loss: 0.235875. Value loss: 9.464314. Entropy: 0.850965.\n",
      "Iteration 308: Policy loss: 0.254546. Value loss: 8.512365. Entropy: 0.896197.\n",
      "Iteration 309: Policy loss: 0.177608. Value loss: 8.036258. Entropy: 0.858076.\n",
      "episode: 126   score: 210.0  epsilon: 1.0    steps: 291  evaluation reward: 220.05\n",
      "episode: 127   score: 105.0  epsilon: 1.0    steps: 535  evaluation reward: 221.35\n",
      "episode: 128   score: 210.0  epsilon: 1.0    steps: 670  evaluation reward: 221.0\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 310: Policy loss: 0.254928. Value loss: 14.360093. Entropy: 1.030922.\n",
      "Iteration 311: Policy loss: 0.297481. Value loss: 12.279701. Entropy: 1.072127.\n",
      "Iteration 312: Policy loss: 0.169149. Value loss: 10.550852. Entropy: 1.056516.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 313: Policy loss: 0.068174. Value loss: 10.223766. Entropy: 1.008249.\n",
      "Iteration 314: Policy loss: 0.090388. Value loss: 10.421547. Entropy: 0.979297.\n",
      "Iteration 315: Policy loss: 0.066245. Value loss: 9.100110. Entropy: 1.003353.\n",
      "episode: 129   score: 180.0  epsilon: 1.0    steps: 3  evaluation reward: 222.6\n",
      "episode: 130   score: 180.0  epsilon: 1.0    steps: 856  evaluation reward: 222.25\n",
      "episode: 131   score: 180.0  epsilon: 1.0    steps: 961  evaluation reward: 222.1\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 316: Policy loss: 0.404638. Value loss: 11.474146. Entropy: 1.053376.\n",
      "Iteration 317: Policy loss: 0.437584. Value loss: 10.502439. Entropy: 1.075557.\n",
      "Iteration 318: Policy loss: 0.415470. Value loss: 10.511460. Entropy: 1.072639.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 319: Policy loss: -0.309455. Value loss: 5.115808. Entropy: 0.947363.\n",
      "Iteration 320: Policy loss: -0.260191. Value loss: 4.589112. Entropy: 0.919005.\n",
      "Iteration 321: Policy loss: -0.241286. Value loss: 4.889927. Entropy: 0.914863.\n",
      "episode: 132   score: 135.0  epsilon: 1.0    steps: 218  evaluation reward: 222.55\n",
      "episode: 133   score: 180.0  epsilon: 1.0    steps: 432  evaluation reward: 221.65\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 322: Policy loss: 1.159921. Value loss: 14.795429. Entropy: 0.880741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 323: Policy loss: 1.591235. Value loss: 10.879167. Entropy: 0.822671.\n",
      "Iteration 324: Policy loss: 1.251508. Value loss: 11.841836. Entropy: 0.844086.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 325: Policy loss: 0.307136. Value loss: 5.592653. Entropy: 0.941788.\n",
      "Iteration 326: Policy loss: 0.240742. Value loss: 4.978774. Entropy: 0.968712.\n",
      "Iteration 327: Policy loss: 0.284613. Value loss: 4.001860. Entropy: 1.012941.\n",
      "episode: 134   score: 180.0  epsilon: 1.0    steps: 342  evaluation reward: 222.2\n",
      "episode: 135   score: 155.0  epsilon: 1.0    steps: 586  evaluation reward: 219.0\n",
      "episode: 136   score: 180.0  epsilon: 1.0    steps: 740  evaluation reward: 219.5\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 328: Policy loss: 0.714695. Value loss: 13.052287. Entropy: 0.942582.\n",
      "Iteration 329: Policy loss: 0.841820. Value loss: 11.983703. Entropy: 0.951411.\n",
      "Iteration 330: Policy loss: 0.921293. Value loss: 10.564483. Entropy: 0.892324.\n",
      "episode: 137   score: 105.0  epsilon: 1.0    steps: 970  evaluation reward: 219.05\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 331: Policy loss: -0.348262. Value loss: 5.672679. Entropy: 0.887326.\n",
      "Iteration 332: Policy loss: -0.330260. Value loss: 4.067460. Entropy: 0.923855.\n",
      "Iteration 333: Policy loss: -0.398460. Value loss: 3.708114. Entropy: 0.904308.\n",
      "episode: 138   score: 210.0  epsilon: 1.0    steps: 54  evaluation reward: 216.15\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 334: Policy loss: 1.105661. Value loss: 20.400259. Entropy: 0.770039.\n",
      "Iteration 335: Policy loss: 1.087663. Value loss: 15.207211. Entropy: 0.764656.\n",
      "Iteration 336: Policy loss: 0.873964. Value loss: 15.290364. Entropy: 0.778115.\n",
      "episode: 139   score: 180.0  epsilon: 1.0    steps: 785  evaluation reward: 212.75\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 337: Policy loss: 0.394996. Value loss: 4.831602. Entropy: 0.968460.\n",
      "Iteration 338: Policy loss: 0.365503. Value loss: 2.973787. Entropy: 1.022117.\n",
      "Iteration 339: Policy loss: 0.320908. Value loss: 3.709119. Entropy: 0.996436.\n",
      "episode: 140   score: 160.0  epsilon: 1.0    steps: 239  evaluation reward: 212.05\n",
      "episode: 141   score: 210.0  epsilon: 1.0    steps: 483  evaluation reward: 211.7\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 340: Policy loss: -0.229753. Value loss: 15.000769. Entropy: 0.902068.\n",
      "Iteration 341: Policy loss: -0.269562. Value loss: 11.073340. Entropy: 0.834155.\n",
      "Iteration 342: Policy loss: -0.197434. Value loss: 10.479964. Entropy: 0.898634.\n",
      "episode: 142   score: 105.0  epsilon: 1.0    steps: 361  evaluation reward: 208.55\n",
      "episode: 143   score: 105.0  epsilon: 1.0    steps: 759  evaluation reward: 208.3\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 343: Policy loss: 0.902523. Value loss: 10.242254. Entropy: 0.943610.\n",
      "Iteration 344: Policy loss: 0.780105. Value loss: 6.542592. Entropy: 0.995861.\n",
      "Iteration 345: Policy loss: 0.804964. Value loss: 6.303207. Entropy: 0.936743.\n",
      "episode: 144   score: 210.0  epsilon: 1.0    steps: 637  evaluation reward: 206.75\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 346: Policy loss: -0.749213. Value loss: 10.471547. Entropy: 0.924783.\n",
      "Iteration 347: Policy loss: -0.723043. Value loss: 8.984336. Entropy: 0.894894.\n",
      "Iteration 348: Policy loss: -0.815913. Value loss: 9.571670. Entropy: 0.866209.\n",
      "episode: 145   score: 210.0  epsilon: 1.0    steps: 1021  evaluation reward: 207.7\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 349: Policy loss: -1.291510. Value loss: 11.931997. Entropy: 0.903003.\n",
      "Iteration 350: Policy loss: -1.216741. Value loss: 9.810929. Entropy: 0.909419.\n",
      "Iteration 351: Policy loss: -1.343172. Value loss: 9.097175. Entropy: 0.909099.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 352: Policy loss: -0.310029. Value loss: 13.066568. Entropy: 0.890104.\n",
      "Iteration 353: Policy loss: -0.290431. Value loss: 8.720595. Entropy: 0.910502.\n",
      "Iteration 354: Policy loss: -0.576526. Value loss: 8.804913. Entropy: 0.922434.\n",
      "episode: 146   score: 180.0  epsilon: 1.0    steps: 23  evaluation reward: 207.25\n",
      "episode: 147   score: 180.0  epsilon: 1.0    steps: 836  evaluation reward: 207.55\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 355: Policy loss: 1.005923. Value loss: 7.609453. Entropy: 0.917904.\n",
      "Iteration 356: Policy loss: 1.110186. Value loss: 6.258433. Entropy: 0.888696.\n",
      "Iteration 357: Policy loss: 0.946989. Value loss: 5.628889. Entropy: 0.888578.\n",
      "episode: 148   score: 105.0  epsilon: 1.0    steps: 130  evaluation reward: 207.0\n",
      "episode: 149   score: 160.0  epsilon: 1.0    steps: 506  evaluation reward: 201.75\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 358: Policy loss: 1.303698. Value loss: 11.230929. Entropy: 0.761495.\n",
      "Iteration 359: Policy loss: 1.261800. Value loss: 8.402416. Entropy: 0.728356.\n",
      "Iteration 360: Policy loss: 1.289479. Value loss: 9.415239. Entropy: 0.813999.\n",
      "episode: 150   score: 165.0  epsilon: 1.0    steps: 380  evaluation reward: 201.5\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 361: Policy loss: 0.190248. Value loss: 11.826884. Entropy: 0.789222.\n",
      "Iteration 362: Policy loss: 0.237738. Value loss: 9.696759. Entropy: 0.799218.\n",
      "Iteration 363: Policy loss: 0.249659. Value loss: 9.910905. Entropy: 0.808450.\n",
      "now time :  2019-02-24 15:21:09.650121\n",
      "episode: 151   score: 210.0  epsilon: 1.0    steps: 682  evaluation reward: 200.9\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 364: Policy loss: -0.055677. Value loss: 8.589899. Entropy: 1.004809.\n",
      "Iteration 365: Policy loss: -0.125516. Value loss: 6.813919. Entropy: 0.998511.\n",
      "Iteration 366: Policy loss: 0.067227. Value loss: 6.534275. Entropy: 1.017392.\n",
      "episode: 152   score: 210.0  epsilon: 1.0    steps: 602  evaluation reward: 199.8\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 367: Policy loss: -0.321124. Value loss: 12.844300. Entropy: 0.910795.\n",
      "Iteration 368: Policy loss: -0.272569. Value loss: 10.188238. Entropy: 0.883429.\n",
      "Iteration 369: Policy loss: -0.248791. Value loss: 9.607800. Entropy: 0.893461.\n",
      "episode: 153   score: 105.0  epsilon: 1.0    steps: 42  evaluation reward: 198.05\n",
      "episode: 154   score: 210.0  epsilon: 1.0    steps: 944  evaluation reward: 197.3\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 370: Policy loss: 0.593426. Value loss: 5.680185. Entropy: 0.967712.\n",
      "Iteration 371: Policy loss: 0.562539. Value loss: 3.870291. Entropy: 0.965543.\n",
      "Iteration 372: Policy loss: 0.576527. Value loss: 3.837239. Entropy: 0.949754.\n",
      "episode: 155   score: 180.0  epsilon: 1.0    steps: 887  evaluation reward: 198.6\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 373: Policy loss: 0.052943. Value loss: 11.725147. Entropy: 0.824837.\n",
      "Iteration 374: Policy loss: -0.078643. Value loss: 10.730277. Entropy: 0.894639.\n",
      "Iteration 375: Policy loss: 0.103511. Value loss: 9.438437. Entropy: 0.854276.\n",
      "episode: 156   score: 210.0  epsilon: 1.0    steps: 187  evaluation reward: 192.7\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 376: Policy loss: -0.623023. Value loss: 6.936424. Entropy: 1.005350.\n",
      "Iteration 377: Policy loss: -0.632806. Value loss: 5.291239. Entropy: 0.990603.\n",
      "Iteration 378: Policy loss: -0.612000. Value loss: 5.314168. Entropy: 1.019155.\n",
      "episode: 157   score: 210.0  epsilon: 1.0    steps: 429  evaluation reward: 193.35\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 379: Policy loss: -0.280642. Value loss: 6.460195. Entropy: 0.923881.\n",
      "Iteration 380: Policy loss: -0.379607. Value loss: 5.107547. Entropy: 0.906632.\n",
      "Iteration 381: Policy loss: -0.279169. Value loss: 5.281255. Entropy: 0.893743.\n",
      "episode: 158   score: 155.0  epsilon: 1.0    steps: 276  evaluation reward: 193.8\n",
      "episode: 159   score: 155.0  epsilon: 1.0    steps: 704  evaluation reward: 192.75\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 382: Policy loss: 0.662835. Value loss: 9.165431. Entropy: 1.066910.\n",
      "Iteration 383: Policy loss: 0.873418. Value loss: 7.378702. Entropy: 0.996373.\n",
      "Iteration 384: Policy loss: 0.643694. Value loss: 4.279665. Entropy: 1.026320.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 385: Policy loss: -0.358815. Value loss: 8.334737. Entropy: 0.862493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 386: Policy loss: -0.330903. Value loss: 5.959739. Entropy: 0.835224.\n",
      "Iteration 387: Policy loss: -0.222774. Value loss: 5.280733. Entropy: 0.868079.\n",
      "episode: 160   score: 210.0  epsilon: 1.0    steps: 93  evaluation reward: 191.45\n",
      "episode: 161   score: 180.0  epsilon: 1.0    steps: 544  evaluation reward: 190.95\n",
      "episode: 162   score: 210.0  epsilon: 1.0    steps: 995  evaluation reward: 190.95\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 388: Policy loss: -0.425623. Value loss: 9.147566. Entropy: 1.102752.\n",
      "Iteration 389: Policy loss: -0.465475. Value loss: 7.375258. Entropy: 1.108688.\n",
      "Iteration 390: Policy loss: -0.470706. Value loss: 6.205242. Entropy: 1.102273.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 391: Policy loss: -0.884775. Value loss: 10.732188. Entropy: 0.927561.\n",
      "Iteration 392: Policy loss: -0.904393. Value loss: 9.218196. Entropy: 0.881861.\n",
      "Iteration 393: Policy loss: -0.982777. Value loss: 10.154572. Entropy: 0.902705.\n",
      "episode: 163   score: 210.0  epsilon: 1.0    steps: 254  evaluation reward: 190.95\n",
      "episode: 164   score: 210.0  epsilon: 1.0    steps: 810  evaluation reward: 186.3\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 394: Policy loss: -0.139641. Value loss: 10.260668. Entropy: 0.957555.\n",
      "Iteration 395: Policy loss: -0.112633. Value loss: 8.875908. Entropy: 0.998125.\n",
      "Iteration 396: Policy loss: -0.169568. Value loss: 9.831490. Entropy: 0.983359.\n",
      "episode: 165   score: 180.0  epsilon: 1.0    steps: 474  evaluation reward: 186.85\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 397: Policy loss: 0.255444. Value loss: 5.887055. Entropy: 1.027405.\n",
      "Iteration 398: Policy loss: 0.204695. Value loss: 4.434508. Entropy: 1.023049.\n",
      "Iteration 399: Policy loss: 0.294444. Value loss: 3.885136. Entropy: 1.030228.\n",
      "episode: 166   score: 210.0  epsilon: 1.0    steps: 327  evaluation reward: 185.65\n",
      "episode: 167   score: 210.0  epsilon: 1.0    steps: 755  evaluation reward: 185.65\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 400: Policy loss: 0.839418. Value loss: 8.196775. Entropy: 0.992326.\n",
      "Iteration 401: Policy loss: 0.761730. Value loss: 7.364008. Entropy: 1.073045.\n",
      "Iteration 402: Policy loss: 0.803488. Value loss: 6.913378. Entropy: 1.151597.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 403: Policy loss: 0.056969. Value loss: 9.837641. Entropy: 1.142325.\n",
      "Iteration 404: Policy loss: 0.136875. Value loss: 8.530709. Entropy: 1.165972.\n",
      "Iteration 405: Policy loss: 0.146875. Value loss: 7.655099. Entropy: 1.165956.\n",
      "episode: 168   score: 155.0  epsilon: 1.0    steps: 583  evaluation reward: 185.65\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 406: Policy loss: 0.313500. Value loss: 15.129536. Entropy: 1.138826.\n",
      "Iteration 407: Policy loss: 0.363716. Value loss: 13.038501. Entropy: 1.138626.\n",
      "Iteration 408: Policy loss: 0.195324. Value loss: 11.679303. Entropy: 1.118605.\n",
      "episode: 169   score: 210.0  epsilon: 1.0    steps: 16  evaluation reward: 185.4\n",
      "episode: 170   score: 210.0  epsilon: 1.0    steps: 924  evaluation reward: 185.4\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 409: Policy loss: 0.180937. Value loss: 4.719664. Entropy: 1.120132.\n",
      "Iteration 410: Policy loss: 0.156983. Value loss: 3.404300. Entropy: 1.117958.\n",
      "Iteration 411: Policy loss: 0.111695. Value loss: 3.467028. Entropy: 1.095852.\n",
      "episode: 171   score: 210.0  epsilon: 1.0    steps: 861  evaluation reward: 184.9\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 412: Policy loss: -0.888510. Value loss: 16.378881. Entropy: 0.997105.\n",
      "Iteration 413: Policy loss: -0.856426. Value loss: 14.545699. Entropy: 0.989820.\n",
      "Iteration 414: Policy loss: -1.042967. Value loss: 16.855806. Entropy: 1.009520.\n",
      "episode: 172   score: 210.0  epsilon: 1.0    steps: 195  evaluation reward: 184.9\n",
      "episode: 173   score: 155.0  epsilon: 1.0    steps: 507  evaluation reward: 184.9\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 415: Policy loss: 1.311343. Value loss: 8.461881. Entropy: 1.146256.\n",
      "Iteration 416: Policy loss: 1.371771. Value loss: 5.610968. Entropy: 1.179195.\n",
      "Iteration 417: Policy loss: 1.182794. Value loss: 4.611276. Entropy: 1.192554.\n",
      "episode: 174   score: 180.0  epsilon: 1.0    steps: 360  evaluation reward: 184.35\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 418: Policy loss: 1.023717. Value loss: 12.769012. Entropy: 1.116047.\n",
      "Iteration 419: Policy loss: 0.886047. Value loss: 9.057516. Entropy: 1.090533.\n",
      "Iteration 420: Policy loss: 0.981595. Value loss: 7.632521. Entropy: 1.089253.\n",
      "episode: 175   score: 105.0  epsilon: 1.0    steps: 601  evaluation reward: 184.05\n",
      "episode: 176   score: 155.0  epsilon: 1.0    steps: 672  evaluation reward: 183.5\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 421: Policy loss: 0.394509. Value loss: 10.272341. Entropy: 1.283391.\n",
      "Iteration 422: Policy loss: 0.328670. Value loss: 6.671061. Entropy: 1.246422.\n",
      "Iteration 423: Policy loss: 0.397734. Value loss: 6.386509. Entropy: 1.258809.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 424: Policy loss: -1.855461. Value loss: 299.842255. Entropy: 1.073517.\n",
      "Iteration 425: Policy loss: -1.170897. Value loss: 168.139587. Entropy: 1.078218.\n",
      "Iteration 426: Policy loss: -2.226696. Value loss: 237.975525. Entropy: 1.122698.\n",
      "episode: 177   score: 155.0  epsilon: 1.0    steps: 37  evaluation reward: 183.25\n",
      "episode: 178   score: 105.0  epsilon: 1.0    steps: 862  evaluation reward: 182.7\n",
      "episode: 179   score: 410.0  epsilon: 1.0    steps: 1009  evaluation reward: 179.65\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 427: Policy loss: 0.181832. Value loss: 19.068489. Entropy: 1.205930.\n",
      "Iteration 428: Policy loss: 0.089409. Value loss: 13.520752. Entropy: 1.213159.\n",
      "Iteration 429: Policy loss: 0.290575. Value loss: 9.546183. Entropy: 1.203884.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 430: Policy loss: -0.370945. Value loss: 10.348401. Entropy: 1.116753.\n",
      "Iteration 431: Policy loss: -0.569117. Value loss: 7.760354. Entropy: 1.136075.\n",
      "Iteration 432: Policy loss: -0.421306. Value loss: 5.368805. Entropy: 1.123727.\n",
      "episode: 180   score: 155.0  epsilon: 1.0    steps: 216  evaluation reward: 181.65\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 433: Policy loss: 0.326934. Value loss: 18.998436. Entropy: 1.078816.\n",
      "Iteration 434: Policy loss: 0.362629. Value loss: 15.218652. Entropy: 1.090596.\n",
      "Iteration 435: Policy loss: 0.307331. Value loss: 11.886792. Entropy: 1.104744.\n",
      "episode: 181   score: 210.0  epsilon: 1.0    steps: 430  evaluation reward: 182.45\n",
      "episode: 182   score: 105.0  epsilon: 1.0    steps: 620  evaluation reward: 181.55\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 436: Policy loss: 0.660577. Value loss: 17.966681. Entropy: 1.165621.\n",
      "Iteration 437: Policy loss: 0.796057. Value loss: 7.830804. Entropy: 1.156493.\n",
      "Iteration 438: Policy loss: 0.675545. Value loss: 6.498291. Entropy: 1.176094.\n",
      "episode: 183   score: 210.0  epsilon: 1.0    steps: 356  evaluation reward: 181.85\n",
      "episode: 184   score: 155.0  epsilon: 1.0    steps: 708  evaluation reward: 182.15\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 439: Policy loss: 0.522860. Value loss: 20.333387. Entropy: 1.033087.\n",
      "Iteration 440: Policy loss: 0.307484. Value loss: 13.404759. Entropy: 1.043573.\n",
      "Iteration 441: Policy loss: 0.501008. Value loss: 10.226975. Entropy: 1.030411.\n",
      "episode: 185   score: 105.0  epsilon: 1.0    steps: 866  evaluation reward: 181.85\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 442: Policy loss: -0.331492. Value loss: 18.492481. Entropy: 1.155478.\n",
      "Iteration 443: Policy loss: -0.546091. Value loss: 12.996500. Entropy: 1.167010.\n",
      "Iteration 444: Policy loss: -0.226062. Value loss: 9.423986. Entropy: 1.133995.\n",
      "episode: 186   score: 180.0  epsilon: 1.0    steps: 82  evaluation reward: 179.1\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 445: Policy loss: 0.043389. Value loss: 14.899625. Entropy: 1.163198.\n",
      "Iteration 446: Policy loss: 0.267242. Value loss: 9.655106. Entropy: 1.207433.\n",
      "Iteration 447: Policy loss: -0.058477. Value loss: 9.654602. Entropy: 1.186146.\n",
      "episode: 187   score: 210.0  epsilon: 1.0    steps: 951  evaluation reward: 180.15\n",
      "Training network. lr: 0.000247. clip: 0.098774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 448: Policy loss: 0.584292. Value loss: 12.220794. Entropy: 1.273147.\n",
      "Iteration 449: Policy loss: 0.480980. Value loss: 7.398629. Entropy: 1.290711.\n",
      "Iteration 450: Policy loss: 0.641890. Value loss: 7.367024. Entropy: 1.273775.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 451: Policy loss: -0.172685. Value loss: 8.852274. Entropy: 1.124786.\n",
      "Iteration 452: Policy loss: -0.155134. Value loss: 6.475684. Entropy: 1.133280.\n",
      "Iteration 453: Policy loss: -0.154775. Value loss: 5.889314. Entropy: 1.152002.\n",
      "episode: 188   score: 180.0  epsilon: 1.0    steps: 145  evaluation reward: 180.7\n",
      "episode: 189   score: 180.0  epsilon: 1.0    steps: 481  evaluation reward: 180.7\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 454: Policy loss: 0.121574. Value loss: 8.582655. Entropy: 1.126641.\n",
      "Iteration 455: Policy loss: 0.187911. Value loss: 7.360710. Entropy: 1.119944.\n",
      "Iteration 456: Policy loss: 0.334509. Value loss: 6.597432. Entropy: 1.114893.\n",
      "episode: 190   score: 180.0  epsilon: 1.0    steps: 534  evaluation reward: 180.7\n",
      "episode: 191   score: 155.0  epsilon: 1.0    steps: 742  evaluation reward: 180.4\n",
      "episode: 192   score: 105.0  epsilon: 1.0    steps: 877  evaluation reward: 180.1\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 457: Policy loss: 1.260834. Value loss: 10.161437. Entropy: 1.161793.\n",
      "Iteration 458: Policy loss: 1.185752. Value loss: 8.775417. Entropy: 1.200402.\n",
      "Iteration 459: Policy loss: 1.030908. Value loss: 6.956424. Entropy: 1.187553.\n",
      "episode: 193   score: 180.0  epsilon: 1.0    steps: 279  evaluation reward: 179.05\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 460: Policy loss: 0.439378. Value loss: 20.391335. Entropy: 1.117788.\n",
      "Iteration 461: Policy loss: 0.429706. Value loss: 15.279543. Entropy: 1.099409.\n",
      "Iteration 462: Policy loss: 0.522817. Value loss: 13.510757. Entropy: 1.115088.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 463: Policy loss: 0.505767. Value loss: 12.487947. Entropy: 1.315435.\n",
      "Iteration 464: Policy loss: 0.439309. Value loss: 9.449782. Entropy: 1.326036.\n",
      "Iteration 465: Policy loss: 0.455921. Value loss: 8.489925. Entropy: 1.337495.\n",
      "episode: 194   score: 210.0  epsilon: 1.0    steps: 1018  evaluation reward: 179.8\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 466: Policy loss: -0.399979. Value loss: 10.122560. Entropy: 1.323350.\n",
      "Iteration 467: Policy loss: -0.228673. Value loss: 7.236930. Entropy: 1.301603.\n",
      "Iteration 468: Policy loss: -0.440862. Value loss: 6.352990. Entropy: 1.296486.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 469: Policy loss: -0.140215. Value loss: 6.204867. Entropy: 1.216113.\n",
      "Iteration 470: Policy loss: -0.232508. Value loss: 4.751702. Entropy: 1.198999.\n",
      "Iteration 471: Policy loss: -0.325058. Value loss: 4.286199. Entropy: 1.188479.\n",
      "episode: 195   score: 180.0  epsilon: 1.0    steps: 188  evaluation reward: 180.1\n",
      "episode: 196   score: 155.0  epsilon: 1.0    steps: 499  evaluation reward: 179.8\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 472: Policy loss: 0.060476. Value loss: 13.790739. Entropy: 1.194807.\n",
      "Iteration 473: Policy loss: -0.080834. Value loss: 10.305296. Entropy: 1.200360.\n",
      "Iteration 474: Policy loss: 0.066648. Value loss: 9.400423. Entropy: 1.224394.\n",
      "episode: 197   score: 105.0  epsilon: 1.0    steps: 290  evaluation reward: 179.55\n",
      "episode: 198   score: 180.0  epsilon: 1.0    steps: 572  evaluation reward: 178.8\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 475: Policy loss: 0.262828. Value loss: 13.253641. Entropy: 1.217372.\n",
      "Iteration 476: Policy loss: 0.258509. Value loss: 11.990038. Entropy: 1.240934.\n",
      "Iteration 477: Policy loss: 0.222630. Value loss: 9.978603. Entropy: 1.242452.\n",
      "episode: 199   score: 180.0  epsilon: 1.0    steps: 661  evaluation reward: 178.5\n",
      "episode: 200   score: 180.0  epsilon: 1.0    steps: 794  evaluation reward: 178.5\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 478: Policy loss: 0.322458. Value loss: 10.265803. Entropy: 1.219050.\n",
      "Iteration 479: Policy loss: 0.317423. Value loss: 8.409859. Entropy: 1.190342.\n",
      "Iteration 480: Policy loss: 0.307685. Value loss: 7.895406. Entropy: 1.234847.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 481: Policy loss: 0.580588. Value loss: 11.399648. Entropy: 1.345731.\n",
      "Iteration 482: Policy loss: 0.616643. Value loss: 9.652639. Entropy: 1.347519.\n",
      "Iteration 483: Policy loss: 0.548449. Value loss: 8.507935. Entropy: 1.344569.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 484: Policy loss: -2.581017. Value loss: 289.396881. Entropy: 1.301469.\n",
      "Iteration 485: Policy loss: -1.846023. Value loss: 207.072021. Entropy: 1.219659.\n",
      "Iteration 486: Policy loss: -1.710508. Value loss: 153.298279. Entropy: 1.153031.\n",
      "now time :  2019-02-24 15:23:28.004123\n",
      "episode: 201   score: 180.0  epsilon: 1.0    steps: 941  evaluation reward: 178.2\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 487: Policy loss: 0.208218. Value loss: 19.381113. Entropy: 1.134060.\n",
      "Iteration 488: Policy loss: 0.413517. Value loss: 11.022045. Entropy: 1.142478.\n",
      "Iteration 489: Policy loss: 0.150034. Value loss: 9.276008. Entropy: 1.127177.\n",
      "episode: 202   score: 260.0  epsilon: 1.0    steps: 66  evaluation reward: 178.95\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 490: Policy loss: -1.573568. Value loss: 26.764536. Entropy: 1.102927.\n",
      "Iteration 491: Policy loss: -1.502850. Value loss: 15.872060. Entropy: 1.123385.\n",
      "Iteration 492: Policy loss: -1.387375. Value loss: 12.753668. Entropy: 1.109382.\n",
      "episode: 203   score: 210.0  epsilon: 1.0    steps: 366  evaluation reward: 179.45\n",
      "episode: 204   score: 180.0  epsilon: 1.0    steps: 434  evaluation reward: 179.75\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 493: Policy loss: -2.672892. Value loss: 262.176147. Entropy: 1.007032.\n",
      "Iteration 494: Policy loss: -2.331592. Value loss: 210.807251. Entropy: 1.038781.\n",
      "Iteration 495: Policy loss: -2.264919. Value loss: 207.989197. Entropy: 1.028605.\n",
      "episode: 205   score: 260.0  epsilon: 1.0    steps: 141  evaluation reward: 179.75\n",
      "episode: 206   score: 335.0  epsilon: 1.0    steps: 698  evaluation reward: 180.55\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 496: Policy loss: 1.226236. Value loss: 23.841969. Entropy: 1.177617.\n",
      "Iteration 497: Policy loss: 1.171910. Value loss: 16.517420. Entropy: 1.146934.\n",
      "Iteration 498: Policy loss: 1.168633. Value loss: 13.557096. Entropy: 1.133332.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 499: Policy loss: 1.366331. Value loss: 21.253666. Entropy: 1.135193.\n",
      "Iteration 500: Policy loss: 1.217859. Value loss: 13.897830. Entropy: 1.116769.\n",
      "Iteration 501: Policy loss: 1.246215. Value loss: 10.996458. Entropy: 1.128089.\n",
      "episode: 207   score: 490.0  epsilon: 1.0    steps: 519  evaluation reward: 182.1\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 502: Policy loss: 1.146288. Value loss: 19.852640. Entropy: 1.161234.\n",
      "Iteration 503: Policy loss: 0.730442. Value loss: 11.405548. Entropy: 1.160551.\n",
      "Iteration 504: Policy loss: 0.813342. Value loss: 9.893324. Entropy: 1.147484.\n",
      "episode: 208   score: 165.0  epsilon: 1.0    steps: 983  evaluation reward: 185.2\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 505: Policy loss: 2.444695. Value loss: 21.073559. Entropy: 1.065012.\n",
      "Iteration 506: Policy loss: 2.350103. Value loss: 8.810493. Entropy: 1.068203.\n",
      "Iteration 507: Policy loss: 2.438061. Value loss: 6.200197. Entropy: 1.091504.\n",
      "episode: 209   score: 155.0  epsilon: 1.0    steps: 109  evaluation reward: 185.05\n",
      "episode: 210   score: 265.0  epsilon: 1.0    steps: 877  evaluation reward: 184.8\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 508: Policy loss: -3.965858. Value loss: 262.464783. Entropy: 1.055991.\n",
      "Iteration 509: Policy loss: -4.802036. Value loss: 123.315727. Entropy: 0.969527.\n",
      "Iteration 510: Policy loss: -4.367569. Value loss: 107.133194. Entropy: 1.014844.\n",
      "episode: 211   score: 355.0  epsilon: 1.0    steps: 473  evaluation reward: 185.35\n",
      "episode: 212   score: 105.0  epsilon: 1.0    steps: 720  evaluation reward: 187.05\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 511: Policy loss: 4.314337. Value loss: 83.639435. Entropy: 0.968934.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 512: Policy loss: 4.647836. Value loss: 46.026466. Entropy: 0.903280.\n",
      "Iteration 513: Policy loss: 4.651721. Value loss: 37.197052. Entropy: 0.943941.\n",
      "episode: 213   score: 210.0  epsilon: 1.0    steps: 244  evaluation reward: 186.25\n",
      "episode: 214   score: 410.0  epsilon: 1.0    steps: 328  evaluation reward: 186.55\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 514: Policy loss: -1.138902. Value loss: 24.859459. Entropy: 0.949088.\n",
      "Iteration 515: Policy loss: -1.157449. Value loss: 16.479446. Entropy: 0.977673.\n",
      "Iteration 516: Policy loss: -1.319549. Value loss: 13.339704. Entropy: 0.966704.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 517: Policy loss: 0.432747. Value loss: 18.779242. Entropy: 1.000293.\n",
      "Iteration 518: Policy loss: 0.346731. Value loss: 13.435173. Entropy: 0.979491.\n",
      "Iteration 519: Policy loss: 0.346564. Value loss: 9.938462. Entropy: 0.985932.\n",
      "episode: 215   score: 210.0  epsilon: 1.0    steps: 576  evaluation reward: 188.55\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 520: Policy loss: -0.043893. Value loss: 12.497731. Entropy: 1.067684.\n",
      "Iteration 521: Policy loss: -0.072951. Value loss: 9.568281. Entropy: 1.059843.\n",
      "Iteration 522: Policy loss: 0.107551. Value loss: 8.185657. Entropy: 1.075680.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 523: Policy loss: 0.665701. Value loss: 7.350661. Entropy: 1.028890.\n",
      "Iteration 524: Policy loss: 0.765372. Value loss: 4.858927. Entropy: 0.999536.\n",
      "Iteration 525: Policy loss: 0.654181. Value loss: 4.961894. Entropy: 1.031473.\n",
      "episode: 216   score: 210.0  epsilon: 1.0    steps: 906  evaluation reward: 188.55\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 526: Policy loss: 0.260368. Value loss: 14.114881. Entropy: 0.904056.\n",
      "Iteration 527: Policy loss: 0.336260. Value loss: 10.112693. Entropy: 0.918070.\n",
      "Iteration 528: Policy loss: 0.476221. Value loss: 8.662540. Entropy: 0.927447.\n",
      "episode: 217   score: 210.0  epsilon: 1.0    steps: 48  evaluation reward: 188.55\n",
      "episode: 218   score: 180.0  epsilon: 1.0    steps: 763  evaluation reward: 188.55\n",
      "episode: 219   score: 180.0  epsilon: 1.0    steps: 798  evaluation reward: 188.7\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 529: Policy loss: 1.050231. Value loss: 14.422101. Entropy: 0.909507.\n",
      "Iteration 530: Policy loss: 1.223863. Value loss: 11.986504. Entropy: 0.929112.\n",
      "Iteration 531: Policy loss: 1.053970. Value loss: 11.741439. Entropy: 0.930513.\n",
      "episode: 220   score: 105.0  epsilon: 1.0    steps: 138  evaluation reward: 188.9\n",
      "episode: 221   score: 180.0  epsilon: 1.0    steps: 377  evaluation reward: 187.85\n",
      "episode: 222   score: 180.0  epsilon: 1.0    steps: 400  evaluation reward: 187.85\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 532: Policy loss: 0.302568. Value loss: 16.251736. Entropy: 0.933279.\n",
      "Iteration 533: Policy loss: 0.378213. Value loss: 15.254678. Entropy: 0.915817.\n",
      "Iteration 534: Policy loss: 0.486548. Value loss: 12.605220. Entropy: 0.913511.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 535: Policy loss: 0.719614. Value loss: 10.393807. Entropy: 1.003086.\n",
      "Iteration 536: Policy loss: 0.635579. Value loss: 8.986341. Entropy: 0.967070.\n",
      "Iteration 537: Policy loss: 0.610782. Value loss: 8.482715. Entropy: 0.981663.\n",
      "episode: 223   score: 155.0  epsilon: 1.0    steps: 610  evaluation reward: 187.85\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 538: Policy loss: 0.430941. Value loss: 10.821630. Entropy: 1.158988.\n",
      "Iteration 539: Policy loss: 0.497580. Value loss: 8.225559. Entropy: 1.165501.\n",
      "Iteration 540: Policy loss: 0.602674. Value loss: 8.629627. Entropy: 1.179335.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 541: Policy loss: 0.371137. Value loss: 3.884596. Entropy: 1.055410.\n",
      "Iteration 542: Policy loss: 0.326677. Value loss: 2.802761. Entropy: 1.058626.\n",
      "Iteration 543: Policy loss: 0.384108. Value loss: 2.498125. Entropy: 1.041808.\n",
      "episode: 224   score: 135.0  epsilon: 1.0    steps: 945  evaluation reward: 187.6\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 544: Policy loss: 0.316118. Value loss: 8.401275. Entropy: 0.924669.\n",
      "Iteration 545: Policy loss: 0.323645. Value loss: 6.421369. Entropy: 0.930250.\n",
      "Iteration 546: Policy loss: 0.273055. Value loss: 5.835969. Entropy: 0.942196.\n",
      "episode: 225   score: 180.0  epsilon: 1.0    steps: 111  evaluation reward: 187.15\n",
      "episode: 226   score: 165.0  epsilon: 1.0    steps: 835  evaluation reward: 187.1\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 547: Policy loss: 0.088309. Value loss: 8.672345. Entropy: 0.926234.\n",
      "Iteration 548: Policy loss: -0.081808. Value loss: 6.821295. Entropy: 0.924905.\n",
      "Iteration 549: Policy loss: -0.087489. Value loss: 7.588176. Entropy: 0.920997.\n",
      "episode: 227   score: 180.0  epsilon: 1.0    steps: 187  evaluation reward: 186.65\n",
      "episode: 228   score: 210.0  epsilon: 1.0    steps: 470  evaluation reward: 187.4\n",
      "episode: 229   score: 210.0  epsilon: 1.0    steps: 711  evaluation reward: 187.4\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 550: Policy loss: -1.351051. Value loss: 18.837578. Entropy: 0.968904.\n",
      "Iteration 551: Policy loss: -1.541543. Value loss: 13.859471. Entropy: 0.976389.\n",
      "Iteration 552: Policy loss: -1.616469. Value loss: 13.484863. Entropy: 0.974365.\n",
      "episode: 230   score: 135.0  epsilon: 1.0    steps: 286  evaluation reward: 187.7\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 553: Policy loss: 0.194714. Value loss: 12.293657. Entropy: 0.918733.\n",
      "Iteration 554: Policy loss: 0.085316. Value loss: 12.594316. Entropy: 0.924718.\n",
      "Iteration 555: Policy loss: 0.192295. Value loss: 11.313101. Entropy: 0.925121.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 556: Policy loss: -1.433830. Value loss: 15.201625. Entropy: 1.045114.\n",
      "Iteration 557: Policy loss: -1.425354. Value loss: 11.559959. Entropy: 1.020557.\n",
      "Iteration 558: Policy loss: -1.462808. Value loss: 12.088016. Entropy: 1.042741.\n",
      "episode: 231   score: 210.0  epsilon: 1.0    steps: 533  evaluation reward: 187.25\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 559: Policy loss: -3.363266. Value loss: 173.840591. Entropy: 1.152926.\n",
      "Iteration 560: Policy loss: -3.515551. Value loss: 119.801811. Entropy: 1.112400.\n",
      "Iteration 561: Policy loss: -3.662261. Value loss: 72.797798. Entropy: 1.107612.\n",
      "episode: 232   score: 410.0  epsilon: 1.0    steps: 1014  evaluation reward: 187.55\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 562: Policy loss: 0.961997. Value loss: 53.678539. Entropy: 0.842741.\n",
      "Iteration 563: Policy loss: 0.892823. Value loss: 14.763093. Entropy: 0.884763.\n",
      "Iteration 564: Policy loss: 0.696440. Value loss: 14.126914. Entropy: 0.849792.\n",
      "episode: 233   score: 180.0  epsilon: 1.0    steps: 878  evaluation reward: 190.3\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 565: Policy loss: 1.265103. Value loss: 32.763920. Entropy: 0.897318.\n",
      "Iteration 566: Policy loss: 0.893585. Value loss: 15.296128. Entropy: 0.911702.\n",
      "Iteration 567: Policy loss: 1.368269. Value loss: 12.091223. Entropy: 0.877827.\n",
      "episode: 234   score: 180.0  epsilon: 1.0    steps: 38  evaluation reward: 190.3\n",
      "episode: 235   score: 210.0  epsilon: 1.0    steps: 250  evaluation reward: 190.3\n",
      "episode: 236   score: 155.0  epsilon: 1.0    steps: 512  evaluation reward: 190.85\n",
      "episode: 237   score: 155.0  epsilon: 1.0    steps: 745  evaluation reward: 190.6\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 568: Policy loss: 2.108567. Value loss: 22.429861. Entropy: 0.938564.\n",
      "Iteration 569: Policy loss: 2.350527. Value loss: 12.866992. Entropy: 0.947178.\n",
      "Iteration 570: Policy loss: 2.244394. Value loss: 12.498936. Entropy: 0.961216.\n",
      "episode: 238   score: 155.0  epsilon: 1.0    steps: 329  evaluation reward: 191.1\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 571: Policy loss: 0.080516. Value loss: 12.152065. Entropy: 0.925396.\n",
      "Iteration 572: Policy loss: 0.033478. Value loss: 9.465691. Entropy: 0.932313.\n",
      "Iteration 573: Policy loss: 0.112719. Value loss: 9.620195. Entropy: 0.942749.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 574: Policy loss: 0.167092. Value loss: 21.676142. Entropy: 0.891531.\n",
      "Iteration 575: Policy loss: 0.293672. Value loss: 17.414074. Entropy: 0.906683.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 576: Policy loss: 0.154621. Value loss: 15.819524. Entropy: 0.894547.\n",
      "episode: 239   score: 180.0  epsilon: 1.0    steps: 588  evaluation reward: 190.55\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 577: Policy loss: 0.151186. Value loss: 7.887047. Entropy: 1.150429.\n",
      "Iteration 578: Policy loss: 0.295190. Value loss: 5.869577. Entropy: 1.132430.\n",
      "Iteration 579: Policy loss: 0.085342. Value loss: 4.558584. Entropy: 1.169071.\n",
      "episode: 240   score: 105.0  epsilon: 1.0    steps: 887  evaluation reward: 190.55\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 580: Policy loss: 0.632348. Value loss: 6.596669. Entropy: 0.979960.\n",
      "Iteration 581: Policy loss: 0.409425. Value loss: 4.676695. Entropy: 0.986844.\n",
      "Iteration 582: Policy loss: 0.606759. Value loss: 4.237300. Entropy: 1.036243.\n",
      "episode: 241   score: 180.0  epsilon: 1.0    steps: 935  evaluation reward: 190.0\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 583: Policy loss: -0.025698. Value loss: 10.846905. Entropy: 0.958357.\n",
      "Iteration 584: Policy loss: -0.093238. Value loss: 7.538711. Entropy: 0.956227.\n",
      "Iteration 585: Policy loss: -0.052667. Value loss: 6.538743. Entropy: 0.978401.\n",
      "episode: 242   score: 210.0  epsilon: 1.0    steps: 95  evaluation reward: 189.7\n",
      "episode: 243   score: 105.0  epsilon: 1.0    steps: 406  evaluation reward: 190.75\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 586: Policy loss: 0.470030. Value loss: 19.759758. Entropy: 0.951382.\n",
      "Iteration 587: Policy loss: 0.578124. Value loss: 12.933784. Entropy: 0.924100.\n",
      "Iteration 588: Policy loss: 0.432225. Value loss: 14.617553. Entropy: 0.929276.\n",
      "episode: 244   score: 180.0  epsilon: 1.0    steps: 171  evaluation reward: 190.75\n",
      "episode: 245   score: 155.0  epsilon: 1.0    steps: 651  evaluation reward: 190.45\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 589: Policy loss: -0.356603. Value loss: 11.219742. Entropy: 1.001456.\n",
      "Iteration 590: Policy loss: -0.299639. Value loss: 11.152377. Entropy: 0.983898.\n",
      "Iteration 591: Policy loss: -0.428798. Value loss: 11.409176. Entropy: 0.987308.\n",
      "episode: 246   score: 180.0  epsilon: 1.0    steps: 305  evaluation reward: 189.9\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 592: Policy loss: 0.461169. Value loss: 8.250192. Entropy: 0.982316.\n",
      "Iteration 593: Policy loss: 0.450407. Value loss: 7.890677. Entropy: 0.993494.\n",
      "Iteration 594: Policy loss: 0.448013. Value loss: 7.372036. Entropy: 0.965986.\n",
      "episode: 247   score: 180.0  epsilon: 1.0    steps: 626  evaluation reward: 189.9\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 595: Policy loss: -0.330811. Value loss: 9.242009. Entropy: 0.990381.\n",
      "Iteration 596: Policy loss: -0.400073. Value loss: 8.862845. Entropy: 1.021100.\n",
      "Iteration 597: Policy loss: -0.399590. Value loss: 6.487269. Entropy: 1.022623.\n",
      "episode: 248   score: 105.0  epsilon: 1.0    steps: 954  evaluation reward: 189.9\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 598: Policy loss: -0.100148. Value loss: 7.036375. Entropy: 1.013800.\n",
      "Iteration 599: Policy loss: -0.167249. Value loss: 4.274010. Entropy: 1.040115.\n",
      "Iteration 600: Policy loss: -0.101796. Value loss: 4.728041. Entropy: 1.023814.\n",
      "episode: 249   score: 180.0  epsilon: 1.0    steps: 810  evaluation reward: 189.9\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 601: Policy loss: -0.224213. Value loss: 8.511663. Entropy: 0.953305.\n",
      "Iteration 602: Policy loss: -0.138638. Value loss: 7.246484. Entropy: 0.927893.\n",
      "Iteration 603: Policy loss: -0.201868. Value loss: 6.690875. Entropy: 0.951581.\n",
      "episode: 250   score: 155.0  epsilon: 1.0    steps: 447  evaluation reward: 190.1\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 604: Policy loss: 0.543184. Value loss: 11.040683. Entropy: 1.121354.\n",
      "Iteration 605: Policy loss: 0.425207. Value loss: 7.793132. Entropy: 1.137315.\n",
      "Iteration 606: Policy loss: 0.407127. Value loss: 6.119440. Entropy: 1.152174.\n",
      "now time :  2019-02-24 15:25:39.703788\n",
      "episode: 251   score: 135.0  epsilon: 1.0    steps: 2  evaluation reward: 190.0\n",
      "episode: 252   score: 155.0  epsilon: 1.0    steps: 194  evaluation reward: 189.25\n",
      "episode: 253   score: 210.0  epsilon: 1.0    steps: 720  evaluation reward: 188.7\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 607: Policy loss: -0.579110. Value loss: 9.941781. Entropy: 1.138248.\n",
      "Iteration 608: Policy loss: -0.620194. Value loss: 6.466328. Entropy: 1.147966.\n",
      "Iteration 609: Policy loss: -0.579144. Value loss: 5.594921. Entropy: 1.142173.\n",
      "episode: 254   score: 180.0  epsilon: 1.0    steps: 354  evaluation reward: 189.75\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 610: Policy loss: 0.610652. Value loss: 13.247383. Entropy: 1.037699.\n",
      "Iteration 611: Policy loss: 0.485206. Value loss: 10.173313. Entropy: 1.059213.\n",
      "Iteration 612: Policy loss: 0.415523. Value loss: 8.703025. Entropy: 1.042512.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 613: Policy loss: -0.082248. Value loss: 10.856019. Entropy: 1.048802.\n",
      "Iteration 614: Policy loss: -0.024165. Value loss: 8.350759. Entropy: 1.095524.\n",
      "Iteration 615: Policy loss: -0.058045. Value loss: 8.454839. Entropy: 1.103351.\n",
      "episode: 255   score: 180.0  epsilon: 1.0    steps: 547  evaluation reward: 189.45\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 616: Policy loss: -0.851728. Value loss: 9.848048. Entropy: 1.220810.\n",
      "Iteration 617: Policy loss: -0.939761. Value loss: 6.863025. Entropy: 1.225001.\n",
      "Iteration 618: Policy loss: -0.794757. Value loss: 5.247726. Entropy: 1.208311.\n",
      "episode: 256   score: 155.0  epsilon: 1.0    steps: 856  evaluation reward: 189.45\n",
      "episode: 257   score: 210.0  epsilon: 1.0    steps: 950  evaluation reward: 188.9\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 619: Policy loss: 0.575622. Value loss: 8.469579. Entropy: 1.147497.\n",
      "Iteration 620: Policy loss: 0.797698. Value loss: 4.951663. Entropy: 1.121182.\n",
      "Iteration 621: Policy loss: 0.645952. Value loss: 4.102452. Entropy: 1.106903.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 622: Policy loss: -0.443780. Value loss: 12.842982. Entropy: 0.958049.\n",
      "Iteration 623: Policy loss: -0.723674. Value loss: 9.008340. Entropy: 0.945025.\n",
      "Iteration 624: Policy loss: -0.566361. Value loss: 8.230941. Entropy: 0.938801.\n",
      "episode: 258   score: 180.0  epsilon: 1.0    steps: 51  evaluation reward: 188.9\n",
      "episode: 259   score: 180.0  epsilon: 1.0    steps: 230  evaluation reward: 189.15\n",
      "episode: 260   score: 210.0  epsilon: 1.0    steps: 434  evaluation reward: 189.4\n",
      "episode: 261   score: 180.0  epsilon: 1.0    steps: 768  evaluation reward: 189.4\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 625: Policy loss: 1.500178. Value loss: 16.881182. Entropy: 0.963293.\n",
      "Iteration 626: Policy loss: 1.833461. Value loss: 12.224150. Entropy: 0.951393.\n",
      "Iteration 627: Policy loss: 1.611571. Value loss: 11.224674. Entropy: 0.974079.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 628: Policy loss: 0.258544. Value loss: 9.564964. Entropy: 0.898355.\n",
      "Iteration 629: Policy loss: 0.221887. Value loss: 7.831837. Entropy: 0.904841.\n",
      "Iteration 630: Policy loss: 0.166373. Value loss: 7.616278. Entropy: 0.908604.\n",
      "episode: 262   score: 210.0  epsilon: 1.0    steps: 295  evaluation reward: 189.4\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 631: Policy loss: -0.456079. Value loss: 9.551770. Entropy: 0.867161.\n",
      "Iteration 632: Policy loss: -0.565011. Value loss: 8.521111. Entropy: 0.864915.\n",
      "Iteration 633: Policy loss: -0.379931. Value loss: 8.125915. Entropy: 0.870676.\n",
      "episode: 263   score: 155.0  epsilon: 1.0    steps: 585  evaluation reward: 189.4\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 634: Policy loss: 0.715958. Value loss: 6.452884. Entropy: 1.073457.\n",
      "Iteration 635: Policy loss: 0.655256. Value loss: 4.425856. Entropy: 1.058339.\n",
      "Iteration 636: Policy loss: 0.792526. Value loss: 3.478048. Entropy: 1.087549.\n",
      "episode: 264   score: 210.0  epsilon: 1.0    steps: 1009  evaluation reward: 188.85\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 637: Policy loss: -0.571619. Value loss: 9.933737. Entropy: 0.978544.\n",
      "Iteration 638: Policy loss: -0.608994. Value loss: 9.016722. Entropy: 0.963981.\n",
      "Iteration 639: Policy loss: -0.651064. Value loss: 6.807806. Entropy: 0.944585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 265   score: 105.0  epsilon: 1.0    steps: 453  evaluation reward: 188.85\n",
      "episode: 266   score: 210.0  epsilon: 1.0    steps: 779  evaluation reward: 188.1\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 640: Policy loss: 0.930046. Value loss: 10.087639. Entropy: 0.940375.\n",
      "Iteration 641: Policy loss: 0.885050. Value loss: 5.509145. Entropy: 0.981000.\n",
      "Iteration 642: Policy loss: 0.816088. Value loss: 5.266035. Entropy: 0.982919.\n"
     ]
    }
   ],
   "source": [
    "vis_env_idx = 0\n",
    "vis_env = envs[vis_env_idx]\n",
    "e = 0\n",
    "frame = 0\n",
    "while (e < EPISODES):\n",
    "    step = 0\n",
    "    assert(num_envs * env_mem_size == train_frame)\n",
    "    frame_next_vals = []\n",
    "    for i in range(num_envs):\n",
    "        env = envs[i]\n",
    "        #history = env.history\n",
    "        #life = env.life\n",
    "        #state, reward, done, info = [env.state, env.reward, env.done, env.info]\n",
    "        for j in range(env_mem_size):\n",
    "            step += 1\n",
    "            frame += 1\n",
    "            \n",
    "            curr_state = env.history[HISTORY_SIZE-1,:,:]\n",
    "            action, value = agent.get_action(np.float32(env.history[:HISTORY_SIZE,:,:]) / 255.)\n",
    "            \n",
    "            next_state, env.reward, env.done, env.info = env.step(action)\n",
    "            \n",
    "            if (i == vis_env_idx):\n",
    "                vis_env._env.render()\n",
    "            \n",
    "            frame_next_state = get_frame(next_state)\n",
    "            env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "            terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "            \n",
    "            env.life = env.info['ale.lives']\n",
    "            r = env.reward\n",
    "            \n",
    "            agent.memory.push(i, deepcopy(curr_state), action, r, terminal_state, value, 0, 0)\n",
    "            if (j == env_mem_size-1):\n",
    "                _, frame_next_val = agent.get_action(np.float32(env.history[1:,:,:]) / 255.)\n",
    "                frame_next_vals.append(frame_next_val)\n",
    "            env.score += r\n",
    "            env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "            \n",
    "            if (env.done):\n",
    "                if (e % 50 == 0):\n",
    "                    print('now time : ', datetime.now())\n",
    "                    rewards.append(np.mean(evaluation_reward))\n",
    "                    episodes.append(e)\n",
    "                    pylab.plot(episodes, rewards, 'b')\n",
    "                    pylab.savefig(\"./save_graph/spaceinvaders_ppo.png\")\n",
    "                    torch.save(agent.policy_net, \"./save_model/spaceinvaders_ppo\")\n",
    "                e += 1\n",
    "                print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "                env.done = False\n",
    "                evaluation_reward.append(env.score)\n",
    "                env.score = 0\n",
    "                env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "                env.state = env.reset()\n",
    "                env.life = number_lives\n",
    "                get_init_state(env.history, env.state)\n",
    "                \n",
    "                \n",
    "                \n",
    "    agent.train_policy_net(frame, frame_next_vals)\n",
    "    agent.update_target_net()\n",
    "\n",
    "'''\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        if render_breakout:\n",
    "            env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        curr_state = history[3,:,:]\n",
    "        action, value = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        #r = np.clip(reward, -1, 1)\n",
    "        r = reward\n",
    "        \"\"\"\n",
    "        if terminal_state:\n",
    "            r -= 20\n",
    "        \"\"\"\n",
    "        # Store the transition in memory \n",
    "        \n",
    "        agent.memory.push(deepcopy(curr_state), action, r, terminal_state, value, 0, 0)\n",
    "        # Start training after random sample generation\n",
    "        if(frame % train_frame == 0):\n",
    "            _, frame_next_val = agent.get_action(np.float32(history[1:, :, :]) / 255.)\n",
    "            agent.train_policy_net(frame, frame_next_val)\n",
    "            # Update the target network\n",
    "            agent.update_target_net()\n",
    "        score += r\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/spaceinvaders_ppo.png\")\n",
    "            torch.save(agent.policy_net, \"./save_model/spaceinvaders_ppo\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > 700 and len(evaluation_reward) > 40:\n",
    "                torch.save(agent.policy_net, \"./save_model/spaceinvaders_ppo\")\n",
    "                sys.exit()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/spaceinvaders_ppo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
