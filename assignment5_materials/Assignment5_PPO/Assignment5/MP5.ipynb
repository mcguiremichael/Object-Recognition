{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "from env import GameEnv\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "envs = []\n",
    "for i in range(num_envs):\n",
    "    envs.append(GameEnv('SpaceInvadersDeterministic-v4'))\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = envs[0].life\n",
    "state_size = envs[0].observation_space.shape\n",
    "action_size = envs[0].action_space.n\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "torch.save(agent.policy_net.state_dict(), \"./save_model/spaceinvaders_ppo_best\")\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n",
    "reset_max = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'envs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-77aed6047ce3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvis_env_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvis_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvis_env_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmax_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'envs' is not defined"
     ]
    }
   ],
   "source": [
    "vis_env_idx = 0\n",
    "vis_env = envs[vis_env_idx]\n",
    "e = 0\n",
    "frame = 0\n",
    "max_eval = -np.inf\n",
    "reset_count = 0\n",
    "\n",
    "while (frame < 10000000):\n",
    "    step = 0\n",
    "    assert(num_envs * env_mem_size == train_frame)\n",
    "    frame_next_vals = []\n",
    "    for i in range(num_envs):\n",
    "        env = envs[i]\n",
    "        #history = env.history\n",
    "        #life = env.life\n",
    "        #state, reward, done, info = [env.state, env.reward, env.done, env.info]\n",
    "        for j in range(env_mem_size):\n",
    "            step += 1\n",
    "            frame += 1\n",
    "            \n",
    "            curr_state = env.history[HISTORY_SIZE-1,:,:]\n",
    "            action, value = agent.get_action(np.float32(env.history[:HISTORY_SIZE,:,:]) / 255.)\n",
    "            \n",
    "            next_state, env.reward, env.done, env.info = env.step(action)\n",
    "            \n",
    "            if (i == vis_env_idx):\n",
    "                vis_env._env.render()\n",
    "            \n",
    "            frame_next_state = get_frame(next_state)\n",
    "            env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "            terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "            \n",
    "            env.life = env.info['ale.lives']\n",
    "            r = env.reward\n",
    "            \n",
    "            agent.memory.push(i, deepcopy(curr_state), action, r, terminal_state, value, 0, 0)\n",
    "            if (j == env_mem_size-1):\n",
    "                _, frame_next_val = agent.get_action(np.float32(env.history[1:,:,:]) / 255.)\n",
    "                frame_next_vals.append(frame_next_val)\n",
    "            env.score += r\n",
    "            env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "            \n",
    "            if (env.done):\n",
    "                if (e % 50 == 0):\n",
    "                    print('now time : ', datetime.now())\n",
    "                    rewards.append(np.mean(evaluation_reward))\n",
    "                    episodes.append(e)\n",
    "                    pylab.plot(episodes, rewards, 'b')\n",
    "                    pylab.savefig(\"./save_graph/spaceinvaders_ppo.png\")\n",
    "                    torch.save(agent.policy_net, \"./save_model/spaceinvaders_ppo\")\n",
    "                    \n",
    "                    if np.mean(evaluation_reward) > max_eval:\n",
    "                        torch.save(agent.policy_net.state_dict(), \"./save_model/spaceinvaders_ppo_best\")\n",
    "                        max_eval = float(np.mean(evaluation_reward))\n",
    "                        reset_count = 0\n",
    "                    elif e > 5000:\n",
    "                        reset_count += 1\n",
    "                        \"\"\"\n",
    "                        if (reset_count == reset_max):\n",
    "                            print(\"Training went nowhere, starting again at best model\")\n",
    "                            agent.policy_net.load_state_dict(torch.load(\"./save_model/spaceinvaders_ppo_best\"))\n",
    "                            agent.update_target_net()\n",
    "                            reset_count = 0\n",
    "                        \"\"\"\n",
    "                e += 1\n",
    "                evaluation_reward.append(env.score)\n",
    "                print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "                \n",
    "                env.done = False\n",
    "                env.score = 0\n",
    "                env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "                env.state = env.reset()\n",
    "                env.life = number_lives\n",
    "                get_init_state(env.history, env.state)\n",
    "                \n",
    "                \n",
    "                \n",
    "    agent.train_policy_net(frame, frame_next_vals)\n",
    "    agent.update_target_net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/spaceinvaders_ppo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " ------- STARTING TRAINING FOR SpaceInvaders-v0 ------- \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determing min/max rewards of environment\n",
      "Min: 0. Max: 200.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],1)[0].unsqueeze(1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000250. clip: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:260: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:261: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:262: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Policy loss: -0.001224. Value loss: 0.402734. Entropy: 1.385494.\n",
      "Iteration 2: Policy loss: -0.002272. Value loss: 0.416963. Entropy: 1.384887.\n",
      "Iteration 3: Policy loss: -0.004752. Value loss: 0.384776. Entropy: 1.383403.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 4: Policy loss: 0.010419. Value loss: 31.506674. Entropy: 1.372683.\n",
      "Iteration 5: Policy loss: 0.001003. Value loss: 26.171925. Entropy: 1.372297.\n",
      "Iteration 6: Policy loss: 0.004994. Value loss: 25.923634. Entropy: 1.376680.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 7: Policy loss: 0.006099. Value loss: 14.835062. Entropy: 1.367709.\n",
      "Iteration 8: Policy loss: -0.003362. Value loss: 13.744712. Entropy: 1.363533.\n",
      "Iteration 9: Policy loss: -0.002212. Value loss: 15.134050. Entropy: 1.367512.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 10: Policy loss: 0.001002. Value loss: 21.589115. Entropy: 1.360367.\n",
      "Iteration 11: Policy loss: 0.012161. Value loss: 12.698215. Entropy: 1.351240.\n",
      "Iteration 12: Policy loss: 0.011225. Value loss: 11.772065. Entropy: 1.354109.\n",
      "now time :  2019-03-05 16:25:56.572448\n",
      "episode: 1   score: 80.0  epsilon: 1.0    steps: 896  evaluation reward: 80.0\n",
      "episode: 2   score: 105.0  epsilon: 1.0    steps: 928  evaluation reward: 92.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3   score: 120.0  epsilon: 1.0    steps: 1008  evaluation reward: 101.66666666666667\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 13: Policy loss: 0.020485. Value loss: 37.421394. Entropy: 1.311320.\n",
      "Iteration 14: Policy loss: 0.015823. Value loss: 31.105167. Entropy: 1.307104.\n",
      "Iteration 15: Policy loss: 0.005699. Value loss: 32.345131. Entropy: 1.322819.\n",
      "episode: 4   score: 45.0  epsilon: 1.0    steps: 104  evaluation reward: 87.5\n",
      "episode: 5   score: 60.0  epsilon: 1.0    steps: 120  evaluation reward: 82.0\n",
      "episode: 6   score: 135.0  epsilon: 1.0    steps: 864  evaluation reward: 90.83333333333333\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 16: Policy loss: -0.001415. Value loss: 9.513955. Entropy: 1.346044.\n",
      "Iteration 17: Policy loss: -0.003953. Value loss: 8.311934. Entropy: 1.356533.\n",
      "Iteration 18: Policy loss: -0.001240. Value loss: 6.599928. Entropy: 1.348485.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 19: Policy loss: 0.005755. Value loss: 38.352379. Entropy: 1.330504.\n",
      "Iteration 20: Policy loss: 0.006545. Value loss: 34.060871. Entropy: 1.319986.\n",
      "Iteration 21: Policy loss: 0.004454. Value loss: 33.181194. Entropy: 1.331296.\n",
      "episode: 7   score: 170.0  epsilon: 1.0    steps: 1008  evaluation reward: 102.14285714285714\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 22: Policy loss: 0.002385. Value loss: 33.218708. Entropy: 1.360981.\n",
      "Iteration 23: Policy loss: 0.007752. Value loss: 29.430351. Entropy: 1.361481.\n",
      "Iteration 24: Policy loss: 0.006053. Value loss: 25.475597. Entropy: 1.360946.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 25: Policy loss: 0.003475. Value loss: 16.114298. Entropy: 1.343638.\n",
      "Iteration 26: Policy loss: 0.003634. Value loss: 12.872349. Entropy: 1.358987.\n",
      "Iteration 27: Policy loss: 0.006219. Value loss: 12.062421. Entropy: 1.351955.\n",
      "episode: 8   score: 220.0  epsilon: 1.0    steps: 200  evaluation reward: 116.875\n",
      "episode: 9   score: 65.0  epsilon: 1.0    steps: 1000  evaluation reward: 111.11111111111111\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 28: Policy loss: 0.000628. Value loss: 38.502525. Entropy: 1.322293.\n",
      "Iteration 29: Policy loss: 0.013349. Value loss: 34.814777. Entropy: 1.330010.\n",
      "Iteration 30: Policy loss: 0.011676. Value loss: 30.620153. Entropy: 1.313965.\n",
      "episode: 10   score: 95.0  epsilon: 1.0    steps: 752  evaluation reward: 109.5\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 31: Policy loss: 0.001185. Value loss: 31.617374. Entropy: 1.352883.\n",
      "Iteration 32: Policy loss: 0.002092. Value loss: 26.464235. Entropy: 1.357130.\n",
      "Iteration 33: Policy loss: 0.002202. Value loss: 24.422808. Entropy: 1.349607.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 34: Policy loss: 0.004569. Value loss: 21.393324. Entropy: 1.352003.\n",
      "Iteration 35: Policy loss: 0.006362. Value loss: 17.350271. Entropy: 1.346341.\n",
      "Iteration 36: Policy loss: 0.006442. Value loss: 14.595855. Entropy: 1.341182.\n",
      "episode: 11   score: 195.0  epsilon: 1.0    steps: 744  evaluation reward: 117.27272727272727\n",
      "episode: 12   score: 140.0  epsilon: 1.0    steps: 936  evaluation reward: 119.16666666666667\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 37: Policy loss: 0.003643. Value loss: 26.773613. Entropy: 1.339790.\n",
      "Iteration 38: Policy loss: 0.003899. Value loss: 21.412115. Entropy: 1.325025.\n",
      "Iteration 39: Policy loss: 0.005264. Value loss: 19.506332. Entropy: 1.331068.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 40: Policy loss: 0.000082. Value loss: 21.594225. Entropy: 1.322627.\n",
      "Iteration 41: Policy loss: 0.019708. Value loss: 16.685316. Entropy: 1.320043.\n",
      "Iteration 42: Policy loss: 0.009445. Value loss: 15.565963. Entropy: 1.321924.\n",
      "episode: 13   score: 210.0  epsilon: 1.0    steps: 344  evaluation reward: 126.15384615384616\n",
      "episode: 14   score: 185.0  epsilon: 1.0    steps: 800  evaluation reward: 130.35714285714286\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 43: Policy loss: 0.008666. Value loss: 32.161266. Entropy: 1.309511.\n",
      "Iteration 44: Policy loss: 0.002797. Value loss: 25.463135. Entropy: 1.322813.\n",
      "Iteration 45: Policy loss: 0.011144. Value loss: 25.082850. Entropy: 1.303096.\n",
      "episode: 15   score: 260.0  epsilon: 1.0    steps: 432  evaluation reward: 139.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 46: Policy loss: 0.012613. Value loss: 60.856003. Entropy: 1.323196.\n",
      "Iteration 47: Policy loss: 0.009234. Value loss: 46.152840. Entropy: 1.334953.\n",
      "Iteration 48: Policy loss: 0.007684. Value loss: 43.329166. Entropy: 1.337564.\n",
      "episode: 16   score: 95.0  epsilon: 1.0    steps: 208  evaluation reward: 136.25\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 49: Policy loss: 0.000309. Value loss: 22.923679. Entropy: 1.326674.\n",
      "Iteration 50: Policy loss: 0.002855. Value loss: 16.067671. Entropy: 1.319141.\n",
      "Iteration 51: Policy loss: 0.003514. Value loss: 13.680696. Entropy: 1.325184.\n",
      "episode: 17   score: 95.0  epsilon: 1.0    steps: 840  evaluation reward: 133.8235294117647\n",
      "episode: 18   score: 190.0  epsilon: 1.0    steps: 864  evaluation reward: 136.94444444444446\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 52: Policy loss: 0.007419. Value loss: 27.273479. Entropy: 1.340140.\n",
      "Iteration 53: Policy loss: 0.014553. Value loss: 24.921448. Entropy: 1.321730.\n",
      "Iteration 54: Policy loss: 0.013718. Value loss: 18.491762. Entropy: 1.329284.\n",
      "episode: 19   score: 185.0  epsilon: 1.0    steps: 800  evaluation reward: 139.47368421052633\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 55: Policy loss: 0.001699. Value loss: 29.788166. Entropy: 1.298262.\n",
      "Iteration 56: Policy loss: 0.012889. Value loss: 22.306919. Entropy: 1.297556.\n",
      "Iteration 57: Policy loss: 0.001456. Value loss: 21.209816. Entropy: 1.302503.\n",
      "episode: 20   score: 155.0  epsilon: 1.0    steps: 152  evaluation reward: 140.25\n",
      "episode: 21   score: 245.0  epsilon: 1.0    steps: 280  evaluation reward: 145.23809523809524\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 58: Policy loss: 0.000645. Value loss: 26.494823. Entropy: 1.274261.\n",
      "Iteration 59: Policy loss: 0.002339. Value loss: 19.515778. Entropy: 1.277091.\n",
      "Iteration 60: Policy loss: -0.001891. Value loss: 15.167058. Entropy: 1.280107.\n",
      "episode: 22   score: 60.0  epsilon: 1.0    steps: 216  evaluation reward: 141.36363636363637\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 61: Policy loss: 0.003062. Value loss: 16.204788. Entropy: 1.298711.\n",
      "Iteration 62: Policy loss: 0.009853. Value loss: 16.857906. Entropy: 1.298450.\n",
      "Iteration 63: Policy loss: 0.001572. Value loss: 13.094543. Entropy: 1.292611.\n",
      "episode: 23   score: 120.0  epsilon: 1.0    steps: 200  evaluation reward: 140.43478260869566\n",
      "episode: 24   score: 120.0  epsilon: 1.0    steps: 992  evaluation reward: 139.58333333333334\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 64: Policy loss: 0.021259. Value loss: 321.956482. Entropy: 1.278830.\n",
      "Iteration 65: Policy loss: 0.058106. Value loss: 183.689774. Entropy: 1.252582.\n",
      "Iteration 66: Policy loss: 0.055719. Value loss: 140.963516. Entropy: 1.278736.\n",
      "episode: 25   score: 15.0  epsilon: 1.0    steps: 408  evaluation reward: 134.6\n",
      "episode: 26   score: 35.0  epsilon: 1.0    steps: 544  evaluation reward: 130.76923076923077\n",
      "episode: 27   score: 105.0  epsilon: 1.0    steps: 896  evaluation reward: 129.8148148148148\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 67: Policy loss: 0.033787. Value loss: 177.444641. Entropy: 1.132440.\n",
      "Iteration 68: Policy loss: 0.064898. Value loss: 235.132141. Entropy: 1.106145.\n",
      "Iteration 69: Policy loss: 0.047482. Value loss: 168.369537. Entropy: 1.178476.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 70: Policy loss: 0.029221. Value loss: 29.050369. Entropy: 1.131968.\n",
      "Iteration 71: Policy loss: 0.010928. Value loss: 20.771942. Entropy: 1.124938.\n",
      "Iteration 72: Policy loss: 0.009976. Value loss: 20.920654. Entropy: 1.135989.\n",
      "episode: 28   score: 410.0  epsilon: 1.0    steps: 120  evaluation reward: 139.82142857142858\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 73: Policy loss: 0.006881. Value loss: 26.292757. Entropy: 1.167187.\n",
      "Iteration 74: Policy loss: 0.001058. Value loss: 20.868124. Entropy: 1.125399.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 75: Policy loss: 0.013419. Value loss: 16.157473. Entropy: 1.152214.\n",
      "episode: 29   score: 105.0  epsilon: 1.0    steps: 192  evaluation reward: 138.6206896551724\n",
      "episode: 30   score: 410.0  epsilon: 1.0    steps: 920  evaluation reward: 147.66666666666666\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 76: Policy loss: 0.056166. Value loss: 84.095627. Entropy: 0.995897.\n",
      "Iteration 77: Policy loss: 0.039778. Value loss: 54.644123. Entropy: 0.770303.\n",
      "Iteration 78: Policy loss: 0.007443. Value loss: 47.117374. Entropy: 0.856941.\n",
      "episode: 31   score: 75.0  epsilon: 1.0    steps: 496  evaluation reward: 145.32258064516128\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 79: Policy loss: 0.012798. Value loss: 58.713314. Entropy: 0.897678.\n",
      "Iteration 80: Policy loss: 0.013950. Value loss: 36.487633. Entropy: 0.855115.\n",
      "Iteration 81: Policy loss: 0.014879. Value loss: 31.301176. Entropy: 0.873855.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 82: Policy loss: 0.003364. Value loss: 42.699799. Entropy: 0.855379.\n",
      "Iteration 83: Policy loss: 0.007402. Value loss: 31.634403. Entropy: 0.865824.\n",
      "Iteration 84: Policy loss: 0.011183. Value loss: 34.345985. Entropy: 0.864616.\n",
      "episode: 32   score: 185.0  epsilon: 1.0    steps: 464  evaluation reward: 146.5625\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 85: Policy loss: 0.011024. Value loss: 48.086315. Entropy: 1.021077.\n",
      "Iteration 86: Policy loss: 0.017829. Value loss: 36.256947. Entropy: 1.038482.\n",
      "Iteration 87: Policy loss: 0.021552. Value loss: 34.217987. Entropy: 1.042663.\n",
      "episode: 33   score: 155.0  epsilon: 1.0    steps: 72  evaluation reward: 146.8181818181818\n",
      "episode: 34   score: 105.0  epsilon: 1.0    steps: 88  evaluation reward: 145.58823529411765\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 88: Policy loss: 0.003099. Value loss: 16.270945. Entropy: 0.991210.\n",
      "Iteration 89: Policy loss: 0.005785. Value loss: 11.461843. Entropy: 1.007103.\n",
      "Iteration 90: Policy loss: 0.013972. Value loss: 14.179328. Entropy: 0.980467.\n",
      "episode: 35   score: 105.0  epsilon: 1.0    steps: 344  evaluation reward: 144.42857142857142\n",
      "episode: 36   score: 150.0  epsilon: 1.0    steps: 384  evaluation reward: 144.58333333333334\n",
      "episode: 37   score: 105.0  epsilon: 1.0    steps: 888  evaluation reward: 143.51351351351352\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 91: Policy loss: 0.005213. Value loss: 34.892239. Entropy: 1.024828.\n",
      "Iteration 92: Policy loss: -0.004324. Value loss: 24.718525. Entropy: 0.982900.\n",
      "Iteration 93: Policy loss: 0.008940. Value loss: 19.103405. Entropy: 1.010315.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 94: Policy loss: 0.022801. Value loss: 276.924988. Entropy: 0.993812.\n",
      "Iteration 95: Policy loss: 0.032338. Value loss: 172.515854. Entropy: 0.938509.\n",
      "Iteration 96: Policy loss: 0.027640. Value loss: 172.524429. Entropy: 0.912247.\n",
      "episode: 38   score: 50.0  epsilon: 1.0    steps: 480  evaluation reward: 141.05263157894737\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 97: Policy loss: 0.036257. Value loss: 265.747101. Entropy: 0.955496.\n",
      "Iteration 98: Policy loss: 0.026453. Value loss: 180.415070. Entropy: 0.957678.\n",
      "Iteration 99: Policy loss: 0.021425. Value loss: 170.993469. Entropy: 0.902061.\n",
      "episode: 39   score: 355.0  epsilon: 1.0    steps: 80  evaluation reward: 146.53846153846155\n",
      "episode: 40   score: 410.0  epsilon: 1.0    steps: 672  evaluation reward: 153.125\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 100: Policy loss: 0.016140. Value loss: 33.814068. Entropy: 0.783194.\n",
      "Iteration 101: Policy loss: 0.004312. Value loss: 19.533649. Entropy: 0.818146.\n",
      "Iteration 102: Policy loss: 0.005706. Value loss: 20.774790. Entropy: 0.810195.\n",
      "episode: 41   score: 110.0  epsilon: 1.0    steps: 856  evaluation reward: 152.0731707317073\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 103: Policy loss: 0.032630. Value loss: 128.755402. Entropy: 0.718066.\n",
      "Iteration 104: Policy loss: 0.040122. Value loss: 92.995796. Entropy: 0.766961.\n",
      "Iteration 105: Policy loss: 0.001906. Value loss: 84.264519. Entropy: 0.749227.\n",
      "episode: 42   score: 75.0  epsilon: 1.0    steps: 832  evaluation reward: 150.23809523809524\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 106: Policy loss: 0.035522. Value loss: 82.742355. Entropy: 0.775114.\n",
      "Iteration 107: Policy loss: 0.054375. Value loss: 60.114914. Entropy: 0.766935.\n",
      "Iteration 108: Policy loss: 0.036453. Value loss: 51.845615. Entropy: 0.640490.\n",
      "episode: 43   score: 210.0  epsilon: 1.0    steps: 48  evaluation reward: 151.62790697674419\n",
      "episode: 44   score: 105.0  epsilon: 1.0    steps: 408  evaluation reward: 150.5681818181818\n",
      "episode: 45   score: 155.0  epsilon: 1.0    steps: 856  evaluation reward: 150.66666666666666\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 109: Policy loss: 0.016529. Value loss: 13.784307. Entropy: 0.655264.\n",
      "Iteration 110: Policy loss: 0.011933. Value loss: 10.214406. Entropy: 0.653207.\n",
      "Iteration 111: Policy loss: 0.012852. Value loss: 8.379546. Entropy: 0.632062.\n",
      "episode: 46   score: 110.0  epsilon: 1.0    steps: 176  evaluation reward: 149.7826086956522\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 112: Policy loss: 0.008788. Value loss: 54.702515. Entropy: 0.589922.\n",
      "Iteration 113: Policy loss: 0.009196. Value loss: 31.834997. Entropy: 0.625823.\n",
      "Iteration 114: Policy loss: 0.007928. Value loss: 24.163246. Entropy: 0.570332.\n",
      "episode: 47   score: 105.0  epsilon: 1.0    steps: 184  evaluation reward: 148.82978723404256\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 115: Policy loss: 0.000457. Value loss: 28.260479. Entropy: 0.733078.\n",
      "Iteration 116: Policy loss: 0.005602. Value loss: 17.885160. Entropy: 0.744615.\n",
      "Iteration 117: Policy loss: 0.008920. Value loss: 16.095118. Entropy: 0.717401.\n",
      "episode: 48   score: 30.0  epsilon: 1.0    steps: 32  evaluation reward: 146.35416666666666\n",
      "episode: 49   score: 30.0  epsilon: 1.0    steps: 432  evaluation reward: 143.9795918367347\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 118: Policy loss: 0.002804. Value loss: 24.941124. Entropy: 0.673361.\n",
      "Iteration 119: Policy loss: 0.004166. Value loss: 19.086044. Entropy: 0.668139.\n",
      "Iteration 120: Policy loss: 0.002028. Value loss: 19.099850. Entropy: 0.665246.\n",
      "episode: 50   score: 105.0  epsilon: 1.0    steps: 176  evaluation reward: 143.2\n",
      "now time :  2019-03-05 16:28:01.833748\n",
      "episode: 51   score: 50.0  epsilon: 1.0    steps: 392  evaluation reward: 141.37254901960785\n",
      "episode: 52   score: 120.0  epsilon: 1.0    steps: 856  evaluation reward: 140.96153846153845\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 121: Policy loss: 0.003272. Value loss: 15.660686. Entropy: 0.779829.\n",
      "Iteration 122: Policy loss: 0.004229. Value loss: 13.430776. Entropy: 0.727246.\n",
      "Iteration 123: Policy loss: 0.006389. Value loss: 10.591911. Entropy: 0.751692.\n",
      "episode: 53   score: 15.0  epsilon: 1.0    steps: 200  evaluation reward: 138.58490566037736\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 124: Policy loss: 0.036549. Value loss: 196.858917. Entropy: 0.465880.\n",
      "Iteration 125: Policy loss: 0.057453. Value loss: 141.724243. Entropy: 0.280700.\n",
      "Iteration 126: Policy loss: 0.059114. Value loss: 88.640099. Entropy: 0.352157.\n",
      "episode: 54   score: 305.0  epsilon: 1.0    steps: 240  evaluation reward: 141.66666666666666\n",
      "episode: 55   score: 30.0  epsilon: 1.0    steps: 280  evaluation reward: 139.63636363636363\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 127: Policy loss: 0.007832. Value loss: 77.042297. Entropy: 0.541424.\n",
      "Iteration 128: Policy loss: 0.005876. Value loss: 57.658871. Entropy: 0.548771.\n",
      "Iteration 129: Policy loss: 0.013203. Value loss: 46.954407. Entropy: 0.537941.\n",
      "episode: 56   score: 105.0  epsilon: 1.0    steps: 168  evaluation reward: 139.01785714285714\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 130: Policy loss: 0.002409. Value loss: 29.435877. Entropy: 0.503930.\n",
      "Iteration 131: Policy loss: 0.006873. Value loss: 16.348518. Entropy: 0.565012.\n",
      "Iteration 132: Policy loss: 0.003591. Value loss: 16.124924. Entropy: 0.530447.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 133: Policy loss: 0.007562. Value loss: 26.791105. Entropy: 0.562373.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 134: Policy loss: 0.010400. Value loss: 23.425772. Entropy: 0.504878.\n",
      "Iteration 135: Policy loss: 0.001137. Value loss: 19.813118. Entropy: 0.565055.\n",
      "episode: 57   score: 120.0  epsilon: 1.0    steps: 584  evaluation reward: 138.68421052631578\n",
      "episode: 58   score: 15.0  epsilon: 1.0    steps: 760  evaluation reward: 136.55172413793105\n",
      "episode: 59   score: 50.0  epsilon: 1.0    steps: 848  evaluation reward: 135.08474576271186\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 136: Policy loss: 0.020367. Value loss: 33.883915. Entropy: 0.598921.\n",
      "Iteration 137: Policy loss: 0.006268. Value loss: 22.282089. Entropy: 0.653336.\n",
      "Iteration 138: Policy loss: 0.009452. Value loss: 18.004515. Entropy: 0.632629.\n",
      "episode: 60   score: 60.0  epsilon: 1.0    steps: 184  evaluation reward: 133.83333333333334\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 139: Policy loss: 0.002899. Value loss: 21.420612. Entropy: 0.532107.\n",
      "Iteration 140: Policy loss: 0.005199. Value loss: 17.290321. Entropy: 0.520375.\n",
      "Iteration 141: Policy loss: 0.006043. Value loss: 14.018816. Entropy: 0.509073.\n",
      "episode: 61   score: 110.0  epsilon: 1.0    steps: 328  evaluation reward: 133.44262295081967\n",
      "episode: 62   score: 105.0  epsilon: 1.0    steps: 704  evaluation reward: 132.98387096774192\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 142: Policy loss: 0.017985. Value loss: 372.032074. Entropy: 0.534344.\n",
      "Iteration 143: Policy loss: 0.031174. Value loss: 269.243408. Entropy: 0.529975.\n",
      "Iteration 144: Policy loss: 0.022374. Value loss: 240.882675. Entropy: 0.498384.\n",
      "episode: 63   score: 105.0  epsilon: 1.0    steps: 328  evaluation reward: 132.53968253968253\n",
      "episode: 64   score: 335.0  epsilon: 1.0    steps: 576  evaluation reward: 135.703125\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 145: Policy loss: 0.002084. Value loss: 24.166010. Entropy: 0.410021.\n",
      "Iteration 146: Policy loss: 0.004286. Value loss: 11.667092. Entropy: 0.381131.\n",
      "Iteration 147: Policy loss: 0.004108. Value loss: 9.473001. Entropy: 0.393564.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 148: Policy loss: 0.003997. Value loss: 13.291097. Entropy: 0.341508.\n",
      "Iteration 149: Policy loss: 0.002605. Value loss: 9.178625. Entropy: 0.339429.\n",
      "Iteration 150: Policy loss: 0.000849. Value loss: 9.790480. Entropy: 0.328625.\n",
      "episode: 65   score: 75.0  epsilon: 1.0    steps: 664  evaluation reward: 134.76923076923077\n",
      "episode: 66   score: 50.0  epsilon: 1.0    steps: 888  evaluation reward: 133.4848484848485\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 151: Policy loss: 0.002564. Value loss: 25.529465. Entropy: 0.345000.\n",
      "Iteration 152: Policy loss: -0.002517. Value loss: 21.034367. Entropy: 0.478133.\n",
      "Iteration 153: Policy loss: -0.015208. Value loss: 18.538399. Entropy: 0.494635.\n",
      "episode: 67   score: 105.0  epsilon: 1.0    steps: 56  evaluation reward: 133.0597014925373\n",
      "episode: 68   score: 105.0  epsilon: 1.0    steps: 408  evaluation reward: 132.64705882352942\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 154: Policy loss: 0.003440. Value loss: 5.829431. Entropy: 0.560736.\n",
      "Iteration 155: Policy loss: 0.000577. Value loss: 3.697730. Entropy: 0.540351.\n",
      "Iteration 156: Policy loss: 0.008430. Value loss: 3.538894. Entropy: 0.511375.\n",
      "episode: 69   score: 105.0  epsilon: 1.0    steps: 912  evaluation reward: 132.2463768115942\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 157: Policy loss: 0.003301. Value loss: 19.330666. Entropy: 0.708075.\n",
      "Iteration 158: Policy loss: 0.002158. Value loss: 14.551805. Entropy: 0.718430.\n",
      "Iteration 159: Policy loss: 0.003064. Value loss: 12.299222. Entropy: 0.669188.\n",
      "episode: 70   score: 75.0  epsilon: 1.0    steps: 200  evaluation reward: 131.42857142857142\n",
      "episode: 71   score: 75.0  epsilon: 1.0    steps: 248  evaluation reward: 130.6338028169014\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 160: Policy loss: 0.009216. Value loss: 7.195948. Entropy: 0.670315.\n",
      "Iteration 161: Policy loss: 0.004939. Value loss: 3.023375. Entropy: 0.673448.\n",
      "Iteration 162: Policy loss: 0.002878. Value loss: 2.252439. Entropy: 0.639634.\n",
      "episode: 72   score: 90.0  epsilon: 1.0    steps: 896  evaluation reward: 130.06944444444446\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 163: Policy loss: 0.008244. Value loss: 31.408260. Entropy: 0.727741.\n",
      "Iteration 164: Policy loss: 0.011509. Value loss: 21.412960. Entropy: 0.766992.\n",
      "Iteration 165: Policy loss: 0.005924. Value loss: 20.397322. Entropy: 0.763226.\n",
      "episode: 73   score: 75.0  epsilon: 1.0    steps: 336  evaluation reward: 129.31506849315068\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 166: Policy loss: 0.013839. Value loss: 32.961823. Entropy: 0.766741.\n",
      "Iteration 167: Policy loss: 0.020075. Value loss: 23.924267. Entropy: 0.771644.\n",
      "Iteration 168: Policy loss: 0.015438. Value loss: 21.747242. Entropy: 0.756809.\n",
      "episode: 74   score: 30.0  epsilon: 1.0    steps: 16  evaluation reward: 127.97297297297297\n",
      "episode: 75   score: 135.0  epsilon: 1.0    steps: 256  evaluation reward: 128.06666666666666\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 169: Policy loss: 0.037313. Value loss: 28.394989. Entropy: 0.725912.\n",
      "Iteration 170: Policy loss: 0.018202. Value loss: 21.260218. Entropy: 0.715987.\n",
      "Iteration 171: Policy loss: 0.008953. Value loss: 17.896187. Entropy: 0.708909.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 172: Policy loss: 0.010319. Value loss: 21.696291. Entropy: 0.825305.\n",
      "Iteration 173: Policy loss: 0.010223. Value loss: 17.484541. Entropy: 0.861048.\n",
      "Iteration 174: Policy loss: 0.012513. Value loss: 14.972533. Entropy: 0.814292.\n",
      "episode: 76   score: 10.0  epsilon: 1.0    steps: 88  evaluation reward: 126.51315789473684\n",
      "episode: 77   score: 130.0  epsilon: 1.0    steps: 104  evaluation reward: 126.55844155844156\n",
      "episode: 78   score: 60.0  epsilon: 1.0    steps: 400  evaluation reward: 125.7051282051282\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 175: Policy loss: 0.003525. Value loss: 16.925009. Entropy: 0.778729.\n",
      "Iteration 176: Policy loss: -0.000968. Value loss: 13.425356. Entropy: 0.762316.\n",
      "Iteration 177: Policy loss: 0.004791. Value loss: 11.549962. Entropy: 0.764791.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 178: Policy loss: 0.009459. Value loss: 22.125530. Entropy: 0.842982.\n",
      "Iteration 179: Policy loss: 0.008171. Value loss: 15.915395. Entropy: 0.909094.\n",
      "Iteration 180: Policy loss: 0.014032. Value loss: 14.449381. Entropy: 0.861687.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 181: Policy loss: 0.006816. Value loss: 28.584143. Entropy: 0.822375.\n",
      "Iteration 182: Policy loss: 0.009793. Value loss: 23.241886. Entropy: 0.833850.\n",
      "Iteration 183: Policy loss: 0.003208. Value loss: 18.656433. Entropy: 0.840742.\n",
      "episode: 79   score: 125.0  epsilon: 1.0    steps: 744  evaluation reward: 125.69620253164557\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 184: Policy loss: 0.005749. Value loss: 18.405951. Entropy: 0.806885.\n",
      "Iteration 185: Policy loss: 0.008142. Value loss: 15.565991. Entropy: 0.812465.\n",
      "Iteration 186: Policy loss: 0.007193. Value loss: 12.716683. Entropy: 0.838081.\n",
      "episode: 80   score: 315.0  epsilon: 1.0    steps: 48  evaluation reward: 128.0625\n",
      "episode: 81   score: 65.0  epsilon: 1.0    steps: 528  evaluation reward: 127.28395061728395\n",
      "episode: 82   score: 45.0  epsilon: 1.0    steps: 832  evaluation reward: 126.28048780487805\n",
      "episode: 83   score: 150.0  epsilon: 1.0    steps: 976  evaluation reward: 126.56626506024097\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 187: Policy loss: 0.002623. Value loss: 35.752670. Entropy: 0.723867.\n",
      "Iteration 188: Policy loss: -0.000942. Value loss: 24.845701. Entropy: 0.721690.\n",
      "Iteration 189: Policy loss: 0.006903. Value loss: 19.409254. Entropy: 0.735387.\n",
      "episode: 84   score: 105.0  epsilon: 1.0    steps: 32  evaluation reward: 126.30952380952381\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 190: Policy loss: -0.002376. Value loss: 28.209591. Entropy: 0.797704.\n",
      "Iteration 191: Policy loss: -0.002839. Value loss: 16.414877. Entropy: 0.723018.\n",
      "Iteration 192: Policy loss: -0.004172. Value loss: 16.121038. Entropy: 0.791442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 85   score: 235.0  epsilon: 1.0    steps: 120  evaluation reward: 127.58823529411765\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 193: Policy loss: 0.001916. Value loss: 25.115910. Entropy: 0.701388.\n",
      "Iteration 194: Policy loss: 0.006421. Value loss: 21.141359. Entropy: 0.731251.\n",
      "Iteration 195: Policy loss: 0.009058. Value loss: 17.569384. Entropy: 0.756616.\n",
      "episode: 86   score: 30.0  epsilon: 1.0    steps: 992  evaluation reward: 126.45348837209302\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 196: Policy loss: 0.010633. Value loss: 17.629299. Entropy: 0.679078.\n",
      "Iteration 197: Policy loss: 0.013444. Value loss: 10.772072. Entropy: 0.659926.\n",
      "Iteration 198: Policy loss: 0.002481. Value loss: 11.096463. Entropy: 0.650153.\n",
      "episode: 87   score: 170.0  epsilon: 1.0    steps: 960  evaluation reward: 126.95402298850574\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 199: Policy loss: -0.001349. Value loss: 30.163816. Entropy: 0.590754.\n",
      "Iteration 200: Policy loss: -0.003837. Value loss: 23.910015. Entropy: 0.593511.\n",
      "Iteration 201: Policy loss: -0.006826. Value loss: 22.226851. Entropy: 0.612170.\n",
      "episode: 88   score: 140.0  epsilon: 1.0    steps: 416  evaluation reward: 127.10227272727273\n",
      "episode: 89   score: 135.0  epsilon: 1.0    steps: 984  evaluation reward: 127.19101123595506\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 202: Policy loss: 0.007298. Value loss: 48.267437. Entropy: 0.620905.\n",
      "Iteration 203: Policy loss: 0.006905. Value loss: 35.377365. Entropy: 0.663526.\n",
      "Iteration 204: Policy loss: 0.006031. Value loss: 31.337305. Entropy: 0.670643.\n",
      "episode: 90   score: 65.0  epsilon: 1.0    steps: 576  evaluation reward: 126.5\n",
      "episode: 91   score: 365.0  epsilon: 1.0    steps: 864  evaluation reward: 129.12087912087912\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 205: Policy loss: 0.005118. Value loss: 27.604969. Entropy: 0.795133.\n",
      "Iteration 206: Policy loss: 0.011142. Value loss: 18.862041. Entropy: 0.766549.\n",
      "Iteration 207: Policy loss: 0.003264. Value loss: 13.840796. Entropy: 0.766420.\n",
      "episode: 92   score: 110.0  epsilon: 1.0    steps: 400  evaluation reward: 128.91304347826087\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 208: Policy loss: -0.000699. Value loss: 25.568081. Entropy: 0.811646.\n",
      "Iteration 209: Policy loss: 0.010637. Value loss: 18.763811. Entropy: 0.823074.\n",
      "Iteration 210: Policy loss: 0.000379. Value loss: 15.904522. Entropy: 0.827070.\n",
      "episode: 93   score: 280.0  epsilon: 1.0    steps: 1000  evaluation reward: 130.53763440860214\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 211: Policy loss: 0.002642. Value loss: 32.625443. Entropy: 0.808193.\n",
      "Iteration 212: Policy loss: 0.005134. Value loss: 19.598635. Entropy: 0.813490.\n",
      "Iteration 213: Policy loss: 0.019563. Value loss: 18.176931. Entropy: 0.825886.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 214: Policy loss: 0.008404. Value loss: 15.586195. Entropy: 0.724117.\n",
      "Iteration 215: Policy loss: 0.018512. Value loss: 11.859264. Entropy: 0.718248.\n",
      "Iteration 216: Policy loss: 0.004826. Value loss: 10.461973. Entropy: 0.767606.\n",
      "episode: 94   score: 105.0  epsilon: 1.0    steps: 192  evaluation reward: 130.2659574468085\n",
      "episode: 95   score: 135.0  epsilon: 1.0    steps: 496  evaluation reward: 130.31578947368422\n",
      "episode: 96   score: 305.0  epsilon: 1.0    steps: 696  evaluation reward: 132.13541666666666\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 217: Policy loss: 0.016679. Value loss: 307.248749. Entropy: 0.791585.\n",
      "Iteration 218: Policy loss: 0.031658. Value loss: 248.497421. Entropy: 0.804501.\n",
      "Iteration 219: Policy loss: 0.001857. Value loss: 235.314880. Entropy: 0.824274.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 220: Policy loss: 0.003495. Value loss: 109.265350. Entropy: 0.792956.\n",
      "Iteration 221: Policy loss: 0.005462. Value loss: 66.345848. Entropy: 0.794038.\n",
      "Iteration 222: Policy loss: 0.002542. Value loss: 63.063534. Entropy: 0.795390.\n",
      "episode: 97   score: 105.0  epsilon: 1.0    steps: 120  evaluation reward: 131.8556701030928\n",
      "episode: 98   score: 105.0  epsilon: 1.0    steps: 488  evaluation reward: 131.58163265306123\n",
      "episode: 99   score: 380.0  epsilon: 1.0    steps: 504  evaluation reward: 134.0909090909091\n",
      "episode: 100   score: 165.0  epsilon: 1.0    steps: 816  evaluation reward: 134.4\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 223: Policy loss: 0.007290. Value loss: 29.351585. Entropy: 0.937985.\n",
      "Iteration 224: Policy loss: 0.007362. Value loss: 23.578938. Entropy: 0.923691.\n",
      "Iteration 225: Policy loss: 0.017482. Value loss: 21.480507. Entropy: 0.955858.\n",
      "now time :  2019-03-05 16:30:06.642927\n",
      "episode: 101   score: 85.0  epsilon: 1.0    steps: 296  evaluation reward: 134.45\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 226: Policy loss: 0.001031. Value loss: 12.082208. Entropy: 0.819114.\n",
      "Iteration 227: Policy loss: 0.004521. Value loss: 11.315137. Entropy: 0.814783.\n",
      "Iteration 228: Policy loss: 0.000594. Value loss: 12.926768. Entropy: 0.814179.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 229: Policy loss: 0.009907. Value loss: 27.963223. Entropy: 0.758548.\n",
      "Iteration 230: Policy loss: 0.003792. Value loss: 18.922783. Entropy: 0.762009.\n",
      "Iteration 231: Policy loss: 0.002414. Value loss: 19.262669. Entropy: 0.734583.\n",
      "episode: 102   score: 110.0  epsilon: 1.0    steps: 248  evaluation reward: 134.5\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 232: Policy loss: -0.001414. Value loss: 7.841983. Entropy: 0.777871.\n",
      "Iteration 233: Policy loss: 0.001682. Value loss: 5.899400. Entropy: 0.780046.\n",
      "Iteration 234: Policy loss: 0.003135. Value loss: 5.192799. Entropy: 0.785891.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 235: Policy loss: 0.003116. Value loss: 17.348295. Entropy: 0.735488.\n",
      "Iteration 236: Policy loss: -0.000177. Value loss: 13.049659. Entropy: 0.763596.\n",
      "Iteration 237: Policy loss: 0.003621. Value loss: 10.345708. Entropy: 0.767342.\n",
      "episode: 103   score: 140.0  epsilon: 1.0    steps: 16  evaluation reward: 134.7\n",
      "episode: 104   score: 120.0  epsilon: 1.0    steps: 440  evaluation reward: 135.45\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 238: Policy loss: 0.017129. Value loss: 277.139618. Entropy: 0.812216.\n",
      "Iteration 239: Policy loss: 0.005432. Value loss: 353.743378. Entropy: 0.705676.\n",
      "Iteration 240: Policy loss: 0.006545. Value loss: 266.973602. Entropy: 0.761782.\n",
      "episode: 105   score: 105.0  epsilon: 1.0    steps: 432  evaluation reward: 135.9\n",
      "episode: 106   score: 140.0  epsilon: 1.0    steps: 704  evaluation reward: 135.95\n",
      "episode: 107   score: 135.0  epsilon: 1.0    steps: 776  evaluation reward: 135.6\n",
      "episode: 108   score: 355.0  epsilon: 1.0    steps: 912  evaluation reward: 136.95\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 241: Policy loss: 0.009008. Value loss: 22.633896. Entropy: 0.880736.\n",
      "Iteration 242: Policy loss: 0.011138. Value loss: 17.657419. Entropy: 0.880161.\n",
      "Iteration 243: Policy loss: 0.010036. Value loss: 15.721706. Entropy: 0.867397.\n",
      "episode: 109   score: 150.0  epsilon: 1.0    steps: 656  evaluation reward: 137.8\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 244: Policy loss: 0.003289. Value loss: 16.131609. Entropy: 0.841103.\n",
      "Iteration 245: Policy loss: 0.004228. Value loss: 12.184568. Entropy: 0.859425.\n",
      "Iteration 246: Policy loss: 0.001935. Value loss: 12.042143. Entropy: 0.864221.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 247: Policy loss: 0.005559. Value loss: 21.871099. Entropy: 0.872023.\n",
      "Iteration 248: Policy loss: 0.005970. Value loss: 16.433727. Entropy: 0.847068.\n",
      "Iteration 249: Policy loss: 0.007323. Value loss: 15.157424. Entropy: 0.830118.\n",
      "episode: 110   score: 130.0  epsilon: 1.0    steps: 664  evaluation reward: 138.15\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 250: Policy loss: 0.016495. Value loss: 13.824223. Entropy: 0.938761.\n",
      "Iteration 251: Policy loss: 0.016365. Value loss: 11.864864. Entropy: 0.942843.\n",
      "Iteration 252: Policy loss: 0.028846. Value loss: 9.647054. Entropy: 0.941059.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 253: Policy loss: 0.011578. Value loss: 4.903970. Entropy: 0.946692.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 254: Policy loss: 0.003083. Value loss: 2.591224. Entropy: 0.926070.\n",
      "Iteration 255: Policy loss: 0.002077. Value loss: 2.309840. Entropy: 0.934065.\n",
      "episode: 111   score: 110.0  epsilon: 1.0    steps: 152  evaluation reward: 137.3\n",
      "episode: 112   score: 120.0  epsilon: 1.0    steps: 792  evaluation reward: 137.1\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 256: Policy loss: 0.032546. Value loss: 333.466278. Entropy: 0.911208.\n",
      "Iteration 257: Policy loss: 0.020345. Value loss: 288.979950. Entropy: 0.944506.\n",
      "Iteration 258: Policy loss: 0.024607. Value loss: 165.856506. Entropy: 0.925961.\n",
      "episode: 113   score: 105.0  epsilon: 1.0    steps: 40  evaluation reward: 136.05\n",
      "episode: 114   score: 335.0  epsilon: 1.0    steps: 880  evaluation reward: 137.55\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 259: Policy loss: 0.011300. Value loss: 13.260080. Entropy: 0.886065.\n",
      "Iteration 260: Policy loss: 0.006510. Value loss: 11.480099. Entropy: 0.909054.\n",
      "Iteration 261: Policy loss: 0.009738. Value loss: 10.617451. Entropy: 0.932368.\n",
      "episode: 115   score: 120.0  epsilon: 1.0    steps: 72  evaluation reward: 136.15\n",
      "episode: 116   score: 120.0  epsilon: 1.0    steps: 984  evaluation reward: 136.4\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 262: Policy loss: 0.004731. Value loss: 16.916876. Entropy: 0.839282.\n",
      "Iteration 263: Policy loss: 0.006510. Value loss: 13.089040. Entropy: 0.848941.\n",
      "Iteration 264: Policy loss: 0.009948. Value loss: 12.902307. Entropy: 0.820993.\n",
      "episode: 117   score: 170.0  epsilon: 1.0    steps: 1024  evaluation reward: 137.15\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 265: Policy loss: 0.001392. Value loss: 21.433393. Entropy: 0.837432.\n",
      "Iteration 266: Policy loss: 0.003396. Value loss: 18.651495. Entropy: 0.822440.\n",
      "Iteration 267: Policy loss: 0.004453. Value loss: 15.149950. Entropy: 0.844573.\n",
      "episode: 118   score: 155.0  epsilon: 1.0    steps: 976  evaluation reward: 136.8\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 268: Policy loss: 0.005341. Value loss: 13.646283. Entropy: 0.898410.\n",
      "Iteration 269: Policy loss: 0.004857. Value loss: 9.363816. Entropy: 0.904210.\n",
      "Iteration 270: Policy loss: 0.002981. Value loss: 10.107350. Entropy: 0.900383.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 271: Policy loss: 0.003633. Value loss: 22.478226. Entropy: 0.872281.\n",
      "Iteration 272: Policy loss: 0.011021. Value loss: 14.532595. Entropy: 0.875157.\n",
      "Iteration 273: Policy loss: 0.007556. Value loss: 12.438540. Entropy: 0.872035.\n",
      "episode: 119   score: 105.0  epsilon: 1.0    steps: 200  evaluation reward: 136.0\n",
      "episode: 120   score: 155.0  epsilon: 1.0    steps: 624  evaluation reward: 136.0\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 274: Policy loss: 0.014225. Value loss: 240.770859. Entropy: 0.840446.\n",
      "Iteration 275: Policy loss: 0.043960. Value loss: 120.483276. Entropy: 0.779265.\n",
      "Iteration 276: Policy loss: 0.012227. Value loss: 62.780243. Entropy: 0.769234.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 277: Policy loss: 0.003541. Value loss: 59.071785. Entropy: 0.670486.\n",
      "Iteration 278: Policy loss: 0.008158. Value loss: 16.807037. Entropy: 0.661609.\n",
      "Iteration 279: Policy loss: 0.006881. Value loss: 12.445962. Entropy: 0.687499.\n",
      "episode: 121   score: 120.0  epsilon: 1.0    steps: 448  evaluation reward: 134.75\n",
      "episode: 122   score: 355.0  epsilon: 1.0    steps: 552  evaluation reward: 137.7\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 280: Policy loss: 0.001998. Value loss: 57.525391. Entropy: 0.799520.\n",
      "Iteration 281: Policy loss: 0.010955. Value loss: 34.497932. Entropy: 0.822716.\n",
      "Iteration 282: Policy loss: 0.010676. Value loss: 33.856377. Entropy: 0.843104.\n",
      "episode: 123   score: 120.0  epsilon: 1.0    steps: 288  evaluation reward: 137.7\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 283: Policy loss: 0.030863. Value loss: 362.614594. Entropy: 0.730685.\n",
      "Iteration 284: Policy loss: 0.051436. Value loss: 381.892883. Entropy: 0.674412.\n",
      "Iteration 285: Policy loss: 0.046017. Value loss: 218.734222. Entropy: 0.739264.\n",
      "episode: 124   score: 105.0  epsilon: 1.0    steps: 48  evaluation reward: 137.55\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 286: Policy loss: 0.007619. Value loss: 25.471680. Entropy: 0.760527.\n",
      "Iteration 287: Policy loss: 0.010120. Value loss: 19.305017. Entropy: 0.756005.\n",
      "Iteration 288: Policy loss: 0.003310. Value loss: 17.812214. Entropy: 0.763274.\n",
      "episode: 125   score: 305.0  epsilon: 1.0    steps: 520  evaluation reward: 140.45\n",
      "episode: 126   score: 440.0  epsilon: 1.0    steps: 584  evaluation reward: 144.5\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 289: Policy loss: 0.007446. Value loss: 187.199966. Entropy: 0.765826.\n",
      "Iteration 290: Policy loss: 0.011422. Value loss: 160.331024. Entropy: 0.740010.\n",
      "Iteration 291: Policy loss: 0.011091. Value loss: 132.101974. Entropy: 0.737984.\n",
      "episode: 127   score: 145.0  epsilon: 1.0    steps: 952  evaluation reward: 144.9\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 292: Policy loss: 0.019968. Value loss: 181.140717. Entropy: 0.717040.\n",
      "Iteration 293: Policy loss: 0.038925. Value loss: 144.283005. Entropy: 0.669103.\n",
      "Iteration 294: Policy loss: 0.021314. Value loss: 105.521736. Entropy: 0.711155.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 295: Policy loss: 0.003402. Value loss: 66.477440. Entropy: 0.656579.\n",
      "Iteration 296: Policy loss: 0.007712. Value loss: 20.051245. Entropy: 0.661660.\n",
      "Iteration 297: Policy loss: 0.005385. Value loss: 18.484308. Entropy: 0.701832.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 298: Policy loss: 0.022812. Value loss: 27.288109. Entropy: 0.725747.\n",
      "Iteration 299: Policy loss: 0.034921. Value loss: 16.629827. Entropy: 0.694629.\n",
      "Iteration 300: Policy loss: 0.035394. Value loss: 13.390721. Entropy: 0.724303.\n",
      "episode: 128   score: 135.0  epsilon: 1.0    steps: 8  evaluation reward: 142.15\n",
      "episode: 129   score: 155.0  epsilon: 1.0    steps: 96  evaluation reward: 142.65\n",
      "episode: 130   score: 155.0  epsilon: 1.0    steps: 808  evaluation reward: 140.1\n",
      "episode: 131   score: 685.0  epsilon: 1.0    steps: 824  evaluation reward: 146.2\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 301: Policy loss: 0.005289. Value loss: 22.282726. Entropy: 0.707634.\n",
      "Iteration 302: Policy loss: -0.007483. Value loss: 14.148664. Entropy: 0.788303.\n",
      "Iteration 303: Policy loss: -0.014530. Value loss: 11.636483. Entropy: 0.838421.\n",
      "episode: 132   score: 185.0  epsilon: 1.0    steps: 344  evaluation reward: 146.2\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 304: Policy loss: -0.002912. Value loss: 19.037434. Entropy: 0.739564.\n",
      "Iteration 305: Policy loss: -0.007321. Value loss: 10.163545. Entropy: 0.731832.\n",
      "Iteration 306: Policy loss: -0.006745. Value loss: 9.288505. Entropy: 0.727244.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 307: Policy loss: 0.021737. Value loss: 25.779852. Entropy: 0.744733.\n",
      "Iteration 308: Policy loss: 0.032521. Value loss: 17.540588. Entropy: 0.705637.\n",
      "Iteration 309: Policy loss: 0.020271. Value loss: 16.310108. Entropy: 0.679983.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 310: Policy loss: 0.014150. Value loss: 17.545294. Entropy: 0.799188.\n",
      "Iteration 311: Policy loss: 0.021838. Value loss: 10.168419. Entropy: 0.805804.\n",
      "Iteration 312: Policy loss: 0.018052. Value loss: 8.632126. Entropy: 0.812777.\n",
      "episode: 133   score: 145.0  epsilon: 1.0    steps: 200  evaluation reward: 146.1\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 313: Policy loss: 0.011574. Value loss: 21.360674. Entropy: 0.810056.\n",
      "Iteration 314: Policy loss: 0.009542. Value loss: 11.444927. Entropy: 0.847272.\n",
      "Iteration 315: Policy loss: 0.014402. Value loss: 7.850467. Entropy: 0.813586.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 316: Policy loss: 0.009227. Value loss: 238.533020. Entropy: 0.694381.\n",
      "Iteration 317: Policy loss: 0.013712. Value loss: 268.636932. Entropy: 0.717920.\n",
      "Iteration 318: Policy loss: 0.015153. Value loss: 207.669815. Entropy: 0.723996.\n",
      "episode: 134   score: 150.0  epsilon: 1.0    steps: 168  evaluation reward: 146.55\n",
      "episode: 135   score: 410.0  epsilon: 1.0    steps: 560  evaluation reward: 149.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 136   score: 245.0  epsilon: 1.0    steps: 776  evaluation reward: 150.55\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 319: Policy loss: 0.009518. Value loss: 32.841061. Entropy: 0.975189.\n",
      "Iteration 320: Policy loss: 0.013296. Value loss: 19.430138. Entropy: 0.946360.\n",
      "Iteration 321: Policy loss: 0.013718. Value loss: 16.459082. Entropy: 0.956427.\n",
      "episode: 137   score: 155.0  epsilon: 1.0    steps: 520  evaluation reward: 151.05\n",
      "episode: 138   score: 310.0  epsilon: 1.0    steps: 888  evaluation reward: 153.65\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 322: Policy loss: 0.011390. Value loss: 46.014256. Entropy: 0.894510.\n",
      "Iteration 323: Policy loss: 0.012625. Value loss: 30.372059. Entropy: 0.923241.\n",
      "Iteration 324: Policy loss: 0.000984. Value loss: 28.126980. Entropy: 0.915155.\n",
      "episode: 139   score: 155.0  epsilon: 1.0    steps: 136  evaluation reward: 151.65\n",
      "episode: 140   score: 160.0  epsilon: 1.0    steps: 296  evaluation reward: 149.15\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 325: Policy loss: 0.008753. Value loss: 38.936974. Entropy: 0.982391.\n",
      "Iteration 326: Policy loss: 0.016821. Value loss: 27.417936. Entropy: 1.015806.\n",
      "Iteration 327: Policy loss: 0.008146. Value loss: 25.024500. Entropy: 1.024663.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 328: Policy loss: 0.021466. Value loss: 41.111275. Entropy: 1.065499.\n",
      "Iteration 329: Policy loss: 0.035284. Value loss: 28.782042. Entropy: 1.062913.\n",
      "Iteration 330: Policy loss: 0.013325. Value loss: 25.568995. Entropy: 1.109278.\n",
      "episode: 141   score: 155.0  epsilon: 1.0    steps: 504  evaluation reward: 149.6\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 331: Policy loss: 0.019614. Value loss: 25.059080. Entropy: 1.100573.\n",
      "Iteration 332: Policy loss: 0.013529. Value loss: 17.805569. Entropy: 1.097876.\n",
      "Iteration 333: Policy loss: 0.030704. Value loss: 15.923117. Entropy: 1.068240.\n",
      "episode: 142   score: 120.0  epsilon: 1.0    steps: 528  evaluation reward: 150.05\n",
      "episode: 143   score: 160.0  epsilon: 1.0    steps: 640  evaluation reward: 149.55\n",
      "episode: 144   score: 145.0  epsilon: 1.0    steps: 944  evaluation reward: 149.95\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 334: Policy loss: 0.008145. Value loss: 33.232014. Entropy: 1.173794.\n",
      "Iteration 335: Policy loss: 0.009259. Value loss: 26.065992. Entropy: 1.196098.\n",
      "Iteration 336: Policy loss: 0.023447. Value loss: 22.066166. Entropy: 1.191329.\n",
      "episode: 145   score: 175.0  epsilon: 1.0    steps: 16  evaluation reward: 150.15\n",
      "episode: 146   score: 155.0  epsilon: 1.0    steps: 992  evaluation reward: 150.6\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 337: Policy loss: 0.020803. Value loss: 49.928822. Entropy: 1.106521.\n",
      "Iteration 338: Policy loss: 0.078285. Value loss: 36.784241. Entropy: 1.105621.\n",
      "Iteration 339: Policy loss: 0.056199. Value loss: 31.804029. Entropy: 1.119866.\n",
      "episode: 147   score: 195.0  epsilon: 1.0    steps: 600  evaluation reward: 151.5\n",
      "episode: 148   score: 245.0  epsilon: 1.0    steps: 1008  evaluation reward: 153.65\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 340: Policy loss: 0.010650. Value loss: 35.608963. Entropy: 1.079392.\n",
      "Iteration 341: Policy loss: 0.018376. Value loss: 25.844503. Entropy: 1.088934.\n",
      "Iteration 342: Policy loss: 0.019365. Value loss: 21.742399. Entropy: 1.068128.\n",
      "episode: 149   score: 90.0  epsilon: 1.0    steps: 984  evaluation reward: 154.25\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 343: Policy loss: 0.007013. Value loss: 31.762537. Entropy: 1.085832.\n",
      "Iteration 344: Policy loss: 0.014505. Value loss: 27.770897. Entropy: 1.060854.\n",
      "Iteration 345: Policy loss: 0.020757. Value loss: 23.405405. Entropy: 1.077732.\n",
      "episode: 150   score: 140.0  epsilon: 1.0    steps: 688  evaluation reward: 154.6\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 346: Policy loss: 0.007492. Value loss: 27.879107. Entropy: 1.051059.\n",
      "Iteration 347: Policy loss: -0.000192. Value loss: 18.268465. Entropy: 1.054941.\n",
      "Iteration 348: Policy loss: 0.003515. Value loss: 18.939810. Entropy: 1.062507.\n",
      "now time :  2019-03-05 16:32:30.455937\n",
      "episode: 151   score: 160.0  epsilon: 1.0    steps: 888  evaluation reward: 155.7\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 349: Policy loss: 0.010120. Value loss: 30.670296. Entropy: 1.054642.\n",
      "Iteration 350: Policy loss: 0.024839. Value loss: 19.728596. Entropy: 1.070049.\n",
      "Iteration 351: Policy loss: 0.028627. Value loss: 15.996984. Entropy: 1.046313.\n",
      "episode: 152   score: 155.0  epsilon: 1.0    steps: 32  evaluation reward: 156.05\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 352: Policy loss: 0.021506. Value loss: 24.305437. Entropy: 1.176830.\n",
      "Iteration 353: Policy loss: 0.041980. Value loss: 16.597351. Entropy: 1.185567.\n",
      "Iteration 354: Policy loss: 0.039252. Value loss: 12.931824. Entropy: 1.160616.\n",
      "episode: 153   score: 135.0  epsilon: 1.0    steps: 64  evaluation reward: 157.25\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 355: Policy loss: 0.011152. Value loss: 24.124489. Entropy: 1.139681.\n",
      "Iteration 356: Policy loss: 0.028465. Value loss: 16.377045. Entropy: 1.139355.\n",
      "Iteration 357: Policy loss: 0.014768. Value loss: 13.941278. Entropy: 1.150520.\n",
      "episode: 154   score: 180.0  epsilon: 1.0    steps: 360  evaluation reward: 156.0\n",
      "episode: 155   score: 120.0  epsilon: 1.0    steps: 776  evaluation reward: 156.9\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 358: Policy loss: 0.009318. Value loss: 40.681511. Entropy: 1.125754.\n",
      "Iteration 359: Policy loss: 0.007928. Value loss: 21.771412. Entropy: 1.133484.\n",
      "Iteration 360: Policy loss: 0.005602. Value loss: 15.540289. Entropy: 1.128255.\n",
      "episode: 156   score: 425.0  epsilon: 1.0    steps: 368  evaluation reward: 160.1\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 361: Policy loss: 0.013890. Value loss: 38.699150. Entropy: 1.107618.\n",
      "Iteration 362: Policy loss: 0.041857. Value loss: 28.676636. Entropy: 1.093220.\n",
      "Iteration 363: Policy loss: 0.043284. Value loss: 28.146368. Entropy: 1.073187.\n",
      "episode: 157   score: 160.0  epsilon: 1.0    steps: 528  evaluation reward: 160.5\n",
      "episode: 158   score: 120.0  epsilon: 1.0    steps: 576  evaluation reward: 161.55\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 364: Policy loss: 0.022103. Value loss: 17.782518. Entropy: 1.121046.\n",
      "Iteration 365: Policy loss: 0.024785. Value loss: 13.946718. Entropy: 1.116207.\n",
      "Iteration 366: Policy loss: 0.011606. Value loss: 11.380830. Entropy: 1.101503.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 367: Policy loss: 0.011014. Value loss: 17.925154. Entropy: 1.053424.\n",
      "Iteration 368: Policy loss: 0.020724. Value loss: 14.731697. Entropy: 1.073982.\n",
      "Iteration 369: Policy loss: 0.006666. Value loss: 12.847219. Entropy: 1.073558.\n",
      "episode: 159   score: 135.0  epsilon: 1.0    steps: 144  evaluation reward: 162.4\n",
      "episode: 160   score: 105.0  epsilon: 1.0    steps: 808  evaluation reward: 162.85\n",
      "episode: 161   score: 165.0  epsilon: 1.0    steps: 832  evaluation reward: 163.4\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 370: Policy loss: 0.008103. Value loss: 33.139076. Entropy: 1.172281.\n",
      "Iteration 371: Policy loss: 0.029731. Value loss: 23.259270. Entropy: 1.144109.\n",
      "Iteration 372: Policy loss: 0.007305. Value loss: 19.665094. Entropy: 1.155782.\n",
      "episode: 162   score: 160.0  epsilon: 1.0    steps: 424  evaluation reward: 163.95\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 373: Policy loss: 0.081421. Value loss: 269.788605. Entropy: 0.957348.\n",
      "Iteration 374: Policy loss: 0.073362. Value loss: 80.174751. Entropy: 0.950580.\n",
      "Iteration 375: Policy loss: 0.082034. Value loss: 69.874931. Entropy: 0.957901.\n",
      "episode: 163   score: 120.0  epsilon: 1.0    steps: 32  evaluation reward: 164.1\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 376: Policy loss: 0.024825. Value loss: 24.816336. Entropy: 1.049327.\n",
      "Iteration 377: Policy loss: 0.022696. Value loss: 17.399261. Entropy: 1.049045.\n",
      "Iteration 378: Policy loss: 0.023514. Value loss: 15.141685. Entropy: 1.049511.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 379: Policy loss: 0.019898. Value loss: 47.117027. Entropy: 1.025365.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 380: Policy loss: 0.036368. Value loss: 31.933233. Entropy: 1.012541.\n",
      "Iteration 381: Policy loss: 0.025358. Value loss: 26.797184. Entropy: 1.041469.\n",
      "episode: 164   score: 210.0  epsilon: 1.0    steps: 856  evaluation reward: 162.85\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 382: Policy loss: 0.017981. Value loss: 31.856136. Entropy: 1.053175.\n",
      "Iteration 383: Policy loss: 0.005816. Value loss: 23.286999. Entropy: 1.057239.\n",
      "Iteration 384: Policy loss: 0.013613. Value loss: 19.705433. Entropy: 1.057767.\n",
      "episode: 165   score: 150.0  epsilon: 1.0    steps: 664  evaluation reward: 163.6\n",
      "episode: 166   score: 460.0  epsilon: 1.0    steps: 960  evaluation reward: 167.7\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 385: Policy loss: 0.045537. Value loss: 368.855194. Entropy: 0.935411.\n",
      "Iteration 386: Policy loss: 0.097075. Value loss: 219.701660. Entropy: 0.808452.\n",
      "Iteration 387: Policy loss: 0.050678. Value loss: 144.606110. Entropy: 0.875359.\n",
      "episode: 167   score: 120.0  epsilon: 1.0    steps: 384  evaluation reward: 167.85\n",
      "episode: 168   score: 235.0  epsilon: 1.0    steps: 456  evaluation reward: 169.15\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 388: Policy loss: 0.008918. Value loss: 30.087809. Entropy: 0.788792.\n",
      "Iteration 389: Policy loss: 0.008275. Value loss: 23.445093. Entropy: 0.775388.\n",
      "Iteration 390: Policy loss: 0.011290. Value loss: 19.386827. Entropy: 0.783223.\n",
      "episode: 169   score: 110.0  epsilon: 1.0    steps: 168  evaluation reward: 169.2\n",
      "episode: 170   score: 155.0  epsilon: 1.0    steps: 248  evaluation reward: 170.0\n",
      "episode: 171   score: 355.0  epsilon: 1.0    steps: 272  evaluation reward: 172.8\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 391: Policy loss: 0.006361. Value loss: 26.003529. Entropy: 0.764230.\n",
      "Iteration 392: Policy loss: -0.002945. Value loss: 18.143316. Entropy: 0.758599.\n",
      "Iteration 393: Policy loss: -0.004132. Value loss: 14.702638. Entropy: 0.753016.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 394: Policy loss: 0.002891. Value loss: 23.404657. Entropy: 0.769135.\n",
      "Iteration 395: Policy loss: 0.008325. Value loss: 20.345961. Entropy: 0.802808.\n",
      "Iteration 396: Policy loss: 0.006254. Value loss: 18.265022. Entropy: 0.795067.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 397: Policy loss: 0.014847. Value loss: 12.833178. Entropy: 0.962745.\n",
      "Iteration 398: Policy loss: 0.018132. Value loss: 11.597749. Entropy: 0.974364.\n",
      "Iteration 399: Policy loss: 0.012605. Value loss: 9.388212. Entropy: 0.969436.\n",
      "episode: 172   score: 105.0  epsilon: 1.0    steps: 136  evaluation reward: 172.95\n",
      "episode: 173   score: 105.0  epsilon: 1.0    steps: 792  evaluation reward: 173.25\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 400: Policy loss: 0.015241. Value loss: 13.569284. Entropy: 0.951546.\n",
      "Iteration 401: Policy loss: 0.015326. Value loss: 10.572690. Entropy: 0.951819.\n",
      "Iteration 402: Policy loss: 0.021564. Value loss: 9.706686. Entropy: 0.941827.\n",
      "episode: 174   score: 120.0  epsilon: 1.0    steps: 376  evaluation reward: 174.15\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 403: Policy loss: 0.020572. Value loss: 11.286613. Entropy: 0.703081.\n",
      "Iteration 404: Policy loss: 0.004733. Value loss: 7.086088. Entropy: 0.716914.\n",
      "Iteration 405: Policy loss: 0.000718. Value loss: 5.702449. Entropy: 0.756562.\n",
      "episode: 175   score: 105.0  epsilon: 1.0    steps: 296  evaluation reward: 173.85\n",
      "episode: 176   score: 120.0  epsilon: 1.0    steps: 536  evaluation reward: 174.95\n",
      "episode: 177   score: 110.0  epsilon: 1.0    steps: 880  evaluation reward: 174.75\n",
      "episode: 178   score: 120.0  epsilon: 1.0    steps: 944  evaluation reward: 175.35\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 406: Policy loss: 0.011765. Value loss: 16.343369. Entropy: 0.766922.\n",
      "Iteration 407: Policy loss: 0.019375. Value loss: 10.846400. Entropy: 0.830494.\n",
      "Iteration 408: Policy loss: 0.002915. Value loss: 8.824671. Entropy: 0.828811.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 409: Policy loss: 0.005478. Value loss: 9.544918. Entropy: 0.733757.\n",
      "Iteration 410: Policy loss: 0.004637. Value loss: 9.631457. Entropy: 0.721372.\n",
      "Iteration 411: Policy loss: 0.007611. Value loss: 8.282871. Entropy: 0.731671.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 412: Policy loss: 0.003702. Value loss: 18.899881. Entropy: 0.813071.\n",
      "Iteration 413: Policy loss: 0.003658. Value loss: 15.723298. Entropy: 0.814656.\n",
      "Iteration 414: Policy loss: 0.000024. Value loss: 14.477972. Entropy: 0.812770.\n",
      "episode: 179   score: 160.0  epsilon: 1.0    steps: 216  evaluation reward: 175.7\n",
      "episode: 180   score: 105.0  epsilon: 1.0    steps: 280  evaluation reward: 173.6\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 415: Policy loss: 0.018573. Value loss: 9.547215. Entropy: 0.920405.\n",
      "Iteration 416: Policy loss: 0.013312. Value loss: 5.285427. Entropy: 0.868515.\n",
      "Iteration 417: Policy loss: 0.006918. Value loss: 4.354445. Entropy: 0.924327.\n",
      "episode: 181   score: 105.0  epsilon: 1.0    steps: 584  evaluation reward: 174.0\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 418: Policy loss: 0.012746. Value loss: 15.952839. Entropy: 0.770857.\n",
      "Iteration 419: Policy loss: 0.008833. Value loss: 9.552420. Entropy: 0.748639.\n",
      "Iteration 420: Policy loss: 0.003134. Value loss: 9.859870. Entropy: 0.768215.\n",
      "episode: 182   score: 215.0  epsilon: 1.0    steps: 408  evaluation reward: 175.7\n",
      "episode: 183   score: 105.0  epsilon: 1.0    steps: 696  evaluation reward: 175.25\n",
      "episode: 184   score: 105.0  epsilon: 1.0    steps: 936  evaluation reward: 175.25\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 421: Policy loss: 0.004035. Value loss: 14.257172. Entropy: 0.798917.\n",
      "Iteration 422: Policy loss: 0.010723. Value loss: 12.164435. Entropy: 0.817951.\n",
      "Iteration 423: Policy loss: 0.004887. Value loss: 10.367617. Entropy: 0.818601.\n",
      "episode: 185   score: 105.0  epsilon: 1.0    steps: 128  evaluation reward: 173.95\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 424: Policy loss: 0.007962. Value loss: 9.376056. Entropy: 0.756344.\n",
      "Iteration 425: Policy loss: 0.000058. Value loss: 8.112035. Entropy: 0.733763.\n",
      "Iteration 426: Policy loss: 0.000979. Value loss: 6.679739. Entropy: 0.729479.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 427: Policy loss: 0.006863. Value loss: 16.198698. Entropy: 0.874173.\n",
      "Iteration 428: Policy loss: 0.003469. Value loss: 15.688556. Entropy: 0.889153.\n",
      "Iteration 429: Policy loss: 0.008846. Value loss: 12.780114. Entropy: 0.902027.\n",
      "episode: 186   score: 215.0  epsilon: 1.0    steps: 912  evaluation reward: 175.8\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 430: Policy loss: 0.027544. Value loss: 163.792221. Entropy: 0.880716.\n",
      "Iteration 431: Policy loss: 0.047502. Value loss: 113.474213. Entropy: 0.859128.\n",
      "Iteration 432: Policy loss: 0.040814. Value loss: 93.418022. Entropy: 0.853218.\n",
      "episode: 187   score: 335.0  epsilon: 1.0    steps: 592  evaluation reward: 177.45\n",
      "episode: 188   score: 210.0  epsilon: 1.0    steps: 928  evaluation reward: 178.15\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 433: Policy loss: 0.040724. Value loss: 57.983242. Entropy: 0.757898.\n",
      "Iteration 434: Policy loss: 0.013978. Value loss: 28.760748. Entropy: 0.787324.\n",
      "Iteration 435: Policy loss: 0.010856. Value loss: 21.455601. Entropy: 0.796087.\n",
      "episode: 189   score: 150.0  epsilon: 1.0    steps: 840  evaluation reward: 178.3\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 436: Policy loss: 0.006654. Value loss: 27.059734. Entropy: 0.694371.\n",
      "Iteration 437: Policy loss: 0.002417. Value loss: 16.219782. Entropy: 0.682621.\n",
      "Iteration 438: Policy loss: -0.002432. Value loss: 14.794661. Entropy: 0.697937.\n",
      "episode: 190   score: 105.0  epsilon: 1.0    steps: 48  evaluation reward: 178.7\n",
      "episode: 191   score: 180.0  epsilon: 1.0    steps: 680  evaluation reward: 176.85\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 439: Policy loss: 0.012055. Value loss: 23.536413. Entropy: 0.692359.\n",
      "Iteration 440: Policy loss: 0.010155. Value loss: 22.772890. Entropy: 0.667329.\n",
      "Iteration 441: Policy loss: 0.002474. Value loss: 19.267113. Entropy: 0.689028.\n",
      "episode: 192   score: 120.0  epsilon: 1.0    steps: 8  evaluation reward: 176.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 193   score: 165.0  epsilon: 1.0    steps: 440  evaluation reward: 175.8\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 442: Policy loss: 0.009917. Value loss: 11.353271. Entropy: 0.831434.\n",
      "Iteration 443: Policy loss: 0.000232. Value loss: 8.113584. Entropy: 0.830068.\n",
      "Iteration 444: Policy loss: -0.001230. Value loss: 7.541282. Entropy: 0.825591.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 445: Policy loss: 0.004351. Value loss: 13.405587. Entropy: 0.914526.\n",
      "Iteration 446: Policy loss: 0.009998. Value loss: 11.487077. Entropy: 0.916398.\n",
      "Iteration 447: Policy loss: 0.002092. Value loss: 9.831747. Entropy: 0.915728.\n",
      "episode: 194   score: 105.0  epsilon: 1.0    steps: 128  evaluation reward: 175.8\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 448: Policy loss: -0.001152. Value loss: 10.384228. Entropy: 0.961905.\n",
      "Iteration 449: Policy loss: 0.000528. Value loss: 7.580659. Entropy: 0.954619.\n",
      "Iteration 450: Policy loss: 0.001585. Value loss: 5.629786. Entropy: 0.969991.\n",
      "episode: 195   score: 155.0  epsilon: 1.0    steps: 880  evaluation reward: 176.0\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 451: Policy loss: 0.005875. Value loss: 10.265840. Entropy: 0.929875.\n",
      "Iteration 452: Policy loss: -0.006359. Value loss: 7.237885. Entropy: 0.924192.\n",
      "Iteration 453: Policy loss: 0.003157. Value loss: 7.192984. Entropy: 0.930592.\n",
      "episode: 196   score: 155.0  epsilon: 1.0    steps: 232  evaluation reward: 174.5\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 454: Policy loss: 0.006837. Value loss: 17.735321. Entropy: 0.870862.\n",
      "Iteration 455: Policy loss: -0.000025. Value loss: 11.434254. Entropy: 0.871343.\n",
      "Iteration 456: Policy loss: 0.007897. Value loss: 9.836156. Entropy: 0.856188.\n",
      "episode: 197   score: 180.0  epsilon: 1.0    steps: 344  evaluation reward: 175.25\n",
      "episode: 198   score: 155.0  epsilon: 1.0    steps: 408  evaluation reward: 175.75\n",
      "episode: 199   score: 105.0  epsilon: 1.0    steps: 552  evaluation reward: 173.0\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 457: Policy loss: 0.006899. Value loss: 17.784580. Entropy: 0.873819.\n",
      "Iteration 458: Policy loss: 0.003040. Value loss: 13.613752. Entropy: 0.888843.\n",
      "Iteration 459: Policy loss: 0.003541. Value loss: 14.452018. Entropy: 0.876329.\n",
      "episode: 200   score: 180.0  epsilon: 1.0    steps: 464  evaluation reward: 173.15\n",
      "now time :  2019-03-05 16:34:40.773635\n",
      "episode: 201   score: 180.0  epsilon: 1.0    steps: 464  evaluation reward: 174.1\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 460: Policy loss: 0.000876. Value loss: 12.868525. Entropy: 0.873599.\n",
      "Iteration 461: Policy loss: 0.009750. Value loss: 10.527914. Entropy: 0.884395.\n",
      "Iteration 462: Policy loss: -0.000573. Value loss: 9.175625. Entropy: 0.881543.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 463: Policy loss: 0.027637. Value loss: 257.965088. Entropy: 0.838526.\n",
      "Iteration 464: Policy loss: 0.028505. Value loss: 86.056549. Entropy: 0.853207.\n",
      "Iteration 465: Policy loss: 0.031119. Value loss: 74.315475. Entropy: 0.849662.\n",
      "episode: 202   score: 380.0  epsilon: 1.0    steps: 648  evaluation reward: 176.8\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 466: Policy loss: 0.005497. Value loss: 13.974236. Entropy: 0.945218.\n",
      "Iteration 467: Policy loss: -0.001878. Value loss: 10.430782. Entropy: 0.948286.\n",
      "Iteration 468: Policy loss: -0.001385. Value loss: 9.340564. Entropy: 0.969080.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 469: Policy loss: 0.003180. Value loss: 48.903801. Entropy: 0.855237.\n",
      "Iteration 470: Policy loss: 0.000890. Value loss: 11.707011. Entropy: 0.860737.\n",
      "Iteration 471: Policy loss: -0.001259. Value loss: 9.611587. Entropy: 0.876512.\n",
      "episode: 203   score: 155.0  epsilon: 1.0    steps: 280  evaluation reward: 176.95\n",
      "episode: 204   score: 105.0  epsilon: 1.0    steps: 568  evaluation reward: 176.8\n",
      "episode: 205   score: 105.0  epsilon: 1.0    steps: 656  evaluation reward: 176.8\n",
      "episode: 206   score: 160.0  epsilon: 1.0    steps: 672  evaluation reward: 177.0\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 472: Policy loss: 0.038132. Value loss: 51.322178. Entropy: 0.873888.\n",
      "Iteration 473: Policy loss: 0.030125. Value loss: 28.058784. Entropy: 0.903998.\n",
      "Iteration 474: Policy loss: 0.019602. Value loss: 26.057558. Entropy: 0.911804.\n",
      "episode: 207   score: 105.0  epsilon: 1.0    steps: 672  evaluation reward: 176.7\n",
      "episode: 208   score: 180.0  epsilon: 1.0    steps: 928  evaluation reward: 174.95\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 475: Policy loss: 0.032837. Value loss: 235.967072. Entropy: 0.837471.\n",
      "Iteration 476: Policy loss: 0.047851. Value loss: 110.779953. Entropy: 0.782011.\n",
      "Iteration 477: Policy loss: 0.046035. Value loss: 80.654076. Entropy: 0.799373.\n",
      "episode: 209   score: 335.0  epsilon: 1.0    steps: 752  evaluation reward: 176.8\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 478: Policy loss: 0.006383. Value loss: 17.311144. Entropy: 0.828188.\n",
      "Iteration 479: Policy loss: 0.001887. Value loss: 12.683702. Entropy: 0.815011.\n",
      "Iteration 480: Policy loss: 0.004203. Value loss: 13.018471. Entropy: 0.824439.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 481: Policy loss: 0.003336. Value loss: 46.538315. Entropy: 0.884412.\n",
      "Iteration 482: Policy loss: 0.008261. Value loss: 18.999725. Entropy: 0.899130.\n",
      "Iteration 483: Policy loss: 0.003675. Value loss: 13.940693. Entropy: 0.894190.\n",
      "episode: 210   score: 160.0  epsilon: 1.0    steps: 1000  evaluation reward: 177.1\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 484: Policy loss: 0.012726. Value loss: 11.956672. Entropy: 0.926242.\n",
      "Iteration 485: Policy loss: 0.009968. Value loss: 9.009478. Entropy: 0.923683.\n",
      "Iteration 486: Policy loss: 0.011964. Value loss: 7.263271. Entropy: 0.918334.\n",
      "episode: 211   score: 105.0  epsilon: 1.0    steps: 752  evaluation reward: 177.05\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 487: Policy loss: 0.005326. Value loss: 213.621201. Entropy: 0.850712.\n",
      "Iteration 488: Policy loss: 0.016471. Value loss: 258.493347. Entropy: 0.859155.\n",
      "Iteration 489: Policy loss: 0.016681. Value loss: 225.528610. Entropy: 0.849396.\n",
      "episode: 212   score: 180.0  epsilon: 1.0    steps: 872  evaluation reward: 177.65\n",
      "episode: 213   score: 410.0  epsilon: 1.0    steps: 960  evaluation reward: 180.7\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 490: Policy loss: 0.006790. Value loss: 43.160275. Entropy: 0.779182.\n",
      "Iteration 491: Policy loss: 0.004926. Value loss: 34.669384. Entropy: 0.772833.\n",
      "Iteration 492: Policy loss: 0.004854. Value loss: 30.067125. Entropy: 0.772994.\n",
      "episode: 214   score: 210.0  epsilon: 1.0    steps: 248  evaluation reward: 179.45\n",
      "episode: 215   score: 180.0  epsilon: 1.0    steps: 1016  evaluation reward: 180.05\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 493: Policy loss: 0.004144. Value loss: 19.894844. Entropy: 0.793127.\n",
      "Iteration 494: Policy loss: 0.003681. Value loss: 14.283357. Entropy: 0.796666.\n",
      "Iteration 495: Policy loss: -0.001739. Value loss: 12.532797. Entropy: 0.798298.\n",
      "episode: 216   score: 410.0  epsilon: 1.0    steps: 448  evaluation reward: 182.95\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 496: Policy loss: 0.001149. Value loss: 23.521969. Entropy: 0.803174.\n",
      "Iteration 497: Policy loss: 0.000818. Value loss: 20.006304. Entropy: 0.811633.\n",
      "Iteration 498: Policy loss: 0.003632. Value loss: 20.460377. Entropy: 0.813884.\n",
      "episode: 217   score: 155.0  epsilon: 1.0    steps: 88  evaluation reward: 182.8\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 499: Policy loss: -0.000007. Value loss: 10.529331. Entropy: 0.876056.\n",
      "Iteration 500: Policy loss: 0.002948. Value loss: 10.498322. Entropy: 0.862798.\n",
      "Iteration 501: Policy loss: 0.012681. Value loss: 8.473152. Entropy: 0.853816.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 502: Policy loss: -0.000137. Value loss: 11.686676. Entropy: 0.905490.\n",
      "Iteration 503: Policy loss: -0.003287. Value loss: 9.009586. Entropy: 0.892814.\n",
      "Iteration 504: Policy loss: -0.003632. Value loss: 8.337723. Entropy: 0.910845.\n",
      "episode: 218   score: 105.0  epsilon: 1.0    steps: 976  evaluation reward: 182.3\n",
      "Training network. lr: 0.000246. clip: 0.098470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 505: Policy loss: 0.010547. Value loss: 9.122557. Entropy: 0.969891.\n",
      "Iteration 506: Policy loss: 0.006652. Value loss: 5.493219. Entropy: 0.956614.\n",
      "Iteration 507: Policy loss: 0.003087. Value loss: 4.431351. Entropy: 0.952705.\n",
      "episode: 219   score: 105.0  epsilon: 1.0    steps: 496  evaluation reward: 182.3\n",
      "episode: 220   score: 105.0  epsilon: 1.0    steps: 744  evaluation reward: 181.8\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 508: Policy loss: 0.003962. Value loss: 8.271399. Entropy: 0.921920.\n",
      "Iteration 509: Policy loss: 0.003386. Value loss: 6.229940. Entropy: 0.929527.\n",
      "Iteration 510: Policy loss: 0.003773. Value loss: 5.385435. Entropy: 0.919668.\n",
      "episode: 221   score: 230.0  epsilon: 1.0    steps: 104  evaluation reward: 182.9\n",
      "episode: 222   score: 105.0  epsilon: 1.0    steps: 688  evaluation reward: 180.4\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 511: Policy loss: 0.001414. Value loss: 13.622535. Entropy: 0.844119.\n",
      "Iteration 512: Policy loss: -0.000406. Value loss: 8.006329. Entropy: 0.840962.\n",
      "Iteration 513: Policy loss: -0.000382. Value loss: 7.099962. Entropy: 0.849057.\n",
      "episode: 223   score: 105.0  epsilon: 1.0    steps: 304  evaluation reward: 180.25\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 514: Policy loss: 0.012408. Value loss: 17.611797. Entropy: 0.825702.\n",
      "Iteration 515: Policy loss: 0.004020. Value loss: 14.458918. Entropy: 0.806564.\n",
      "Iteration 516: Policy loss: -0.000297. Value loss: 13.052855. Entropy: 0.823847.\n",
      "episode: 224   score: 215.0  epsilon: 1.0    steps: 776  evaluation reward: 181.35\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 517: Policy loss: 0.001224. Value loss: 16.280170. Entropy: 0.878648.\n",
      "Iteration 518: Policy loss: 0.003574. Value loss: 10.659397. Entropy: 0.905395.\n",
      "Iteration 519: Policy loss: 0.001037. Value loss: 10.551858. Entropy: 0.908223.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 520: Policy loss: 0.001343. Value loss: 6.053474. Entropy: 0.867722.\n",
      "Iteration 521: Policy loss: 0.000863. Value loss: 4.783376. Entropy: 0.881966.\n",
      "Iteration 522: Policy loss: -0.001593. Value loss: 3.687025. Entropy: 0.888119.\n",
      "episode: 225   score: 110.0  epsilon: 1.0    steps: 560  evaluation reward: 179.4\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 523: Policy loss: 0.010995. Value loss: 11.569038. Entropy: 0.900183.\n",
      "Iteration 524: Policy loss: 0.006183. Value loss: 8.881681. Entropy: 0.916399.\n",
      "Iteration 525: Policy loss: 0.004811. Value loss: 6.326375. Entropy: 0.906010.\n",
      "episode: 226   score: 135.0  epsilon: 1.0    steps: 8  evaluation reward: 176.35\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 526: Policy loss: 0.009427. Value loss: 5.752521. Entropy: 0.898862.\n",
      "Iteration 527: Policy loss: 0.012118. Value loss: 2.633027. Entropy: 0.909165.\n",
      "Iteration 528: Policy loss: 0.017389. Value loss: 2.283968. Entropy: 0.895179.\n",
      "episode: 227   score: 110.0  epsilon: 1.0    steps: 424  evaluation reward: 176.0\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 529: Policy loss: 0.008667. Value loss: 10.242423. Entropy: 0.911388.\n",
      "Iteration 530: Policy loss: 0.003967. Value loss: 6.206547. Entropy: 0.928034.\n",
      "Iteration 531: Policy loss: 0.004150. Value loss: 7.002622. Entropy: 0.932488.\n",
      "episode: 228   score: 105.0  epsilon: 1.0    steps: 664  evaluation reward: 175.7\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 532: Policy loss: 0.003349. Value loss: 5.867742. Entropy: 0.985968.\n",
      "Iteration 533: Policy loss: 0.004410. Value loss: 3.660200. Entropy: 1.002135.\n",
      "Iteration 534: Policy loss: 0.005746. Value loss: 3.577589. Entropy: 1.015684.\n",
      "episode: 229   score: 135.0  epsilon: 1.0    steps: 928  evaluation reward: 175.5\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 535: Policy loss: 0.006656. Value loss: 8.819127. Entropy: 0.969568.\n",
      "Iteration 536: Policy loss: 0.011721. Value loss: 6.373287. Entropy: 0.966883.\n",
      "Iteration 537: Policy loss: 0.016085. Value loss: 4.785459. Entropy: 0.972817.\n",
      "episode: 230   score: 110.0  epsilon: 1.0    steps: 632  evaluation reward: 175.05\n",
      "episode: 231   score: 160.0  epsilon: 1.0    steps: 736  evaluation reward: 169.8\n",
      "episode: 232   score: 105.0  epsilon: 1.0    steps: 1000  evaluation reward: 169.0\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 538: Policy loss: 0.007224. Value loss: 13.977559. Entropy: 0.944598.\n",
      "Iteration 539: Policy loss: 0.012130. Value loss: 8.860962. Entropy: 0.933905.\n",
      "Iteration 540: Policy loss: 0.012900. Value loss: 8.576056. Entropy: 0.932282.\n",
      "episode: 233   score: 160.0  epsilon: 1.0    steps: 784  evaluation reward: 169.15\n",
      "episode: 234   score: 220.0  epsilon: 1.0    steps: 792  evaluation reward: 169.85\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 541: Policy loss: 0.014181. Value loss: 19.028267. Entropy: 0.865834.\n",
      "Iteration 542: Policy loss: 0.005873. Value loss: 14.151643. Entropy: 0.877344.\n",
      "Iteration 543: Policy loss: 0.000952. Value loss: 12.457149. Entropy: 0.880697.\n",
      "episode: 235   score: 105.0  epsilon: 1.0    steps: 424  evaluation reward: 166.8\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 544: Policy loss: 0.000683. Value loss: 16.695866. Entropy: 0.805548.\n",
      "Iteration 545: Policy loss: 0.002101. Value loss: 14.785027. Entropy: 0.782392.\n",
      "Iteration 546: Policy loss: 0.000716. Value loss: 11.995522. Entropy: 0.791337.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 547: Policy loss: 0.005852. Value loss: 15.812608. Entropy: 0.905835.\n",
      "Iteration 548: Policy loss: 0.000704. Value loss: 12.652727. Entropy: 0.894503.\n",
      "Iteration 549: Policy loss: 0.004624. Value loss: 11.454373. Entropy: 0.909076.\n",
      "episode: 236   score: 105.0  epsilon: 1.0    steps: 936  evaluation reward: 165.4\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 550: Policy loss: 0.003335. Value loss: 4.914653. Entropy: 0.965953.\n",
      "Iteration 551: Policy loss: 0.003513. Value loss: 3.963663. Entropy: 0.966162.\n",
      "Iteration 552: Policy loss: 0.003542. Value loss: 3.329559. Entropy: 0.964574.\n",
      "episode: 237   score: 105.0  epsilon: 1.0    steps: 720  evaluation reward: 164.9\n",
      "episode: 238   score: 135.0  epsilon: 1.0    steps: 1008  evaluation reward: 163.15\n",
      "episode: 239   score: 105.0  epsilon: 1.0    steps: 1016  evaluation reward: 162.65\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 553: Policy loss: 0.018233. Value loss: 9.475008. Entropy: 1.047400.\n",
      "Iteration 554: Policy loss: 0.008296. Value loss: 6.356733. Entropy: 1.047314.\n",
      "Iteration 555: Policy loss: 0.006132. Value loss: 4.812872. Entropy: 1.057668.\n",
      "episode: 240   score: 105.0  epsilon: 1.0    steps: 496  evaluation reward: 162.1\n",
      "episode: 241   score: 105.0  epsilon: 1.0    steps: 552  evaluation reward: 161.6\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 556: Policy loss: 0.003568. Value loss: 11.115012. Entropy: 0.834735.\n",
      "Iteration 557: Policy loss: 0.004191. Value loss: 9.454368. Entropy: 0.848140.\n",
      "Iteration 558: Policy loss: 0.007059. Value loss: 9.184981. Entropy: 0.860883.\n",
      "episode: 242   score: 110.0  epsilon: 1.0    steps: 176  evaluation reward: 161.5\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 559: Policy loss: 0.034172. Value loss: 254.421280. Entropy: 0.721526.\n",
      "Iteration 560: Policy loss: 0.048548. Value loss: 223.925262. Entropy: 0.731552.\n",
      "Iteration 561: Policy loss: 0.031287. Value loss: 151.362915. Entropy: 0.739438.\n",
      "episode: 243   score: 325.0  epsilon: 1.0    steps: 440  evaluation reward: 163.15\n",
      "episode: 244   score: 75.0  epsilon: 1.0    steps: 976  evaluation reward: 162.45\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 562: Policy loss: 0.007752. Value loss: 11.804683. Entropy: 0.920897.\n",
      "Iteration 563: Policy loss: 0.001953. Value loss: 10.343826. Entropy: 0.933979.\n",
      "Iteration 564: Policy loss: 0.002742. Value loss: 8.421383. Entropy: 0.932473.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 565: Policy loss: 0.005594. Value loss: 12.761444. Entropy: 0.929510.\n",
      "Iteration 566: Policy loss: 0.009738. Value loss: 7.691936. Entropy: 0.947408.\n",
      "Iteration 567: Policy loss: 0.006446. Value loss: 7.500156. Entropy: 0.939590.\n",
      "episode: 245   score: 105.0  epsilon: 1.0    steps: 16  evaluation reward: 161.75\n",
      "episode: 246   score: 105.0  epsilon: 1.0    steps: 352  evaluation reward: 161.25\n",
      "episode: 247   score: 75.0  epsilon: 1.0    steps: 384  evaluation reward: 160.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 248   score: 105.0  epsilon: 1.0    steps: 752  evaluation reward: 158.65\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 568: Policy loss: 0.002210. Value loss: 7.892743. Entropy: 0.910323.\n",
      "Iteration 569: Policy loss: -0.002105. Value loss: 6.689863. Entropy: 0.880863.\n",
      "Iteration 570: Policy loss: 0.001770. Value loss: 6.416273. Entropy: 0.884119.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 571: Policy loss: 0.020494. Value loss: 33.714447. Entropy: 0.807545.\n",
      "Iteration 572: Policy loss: 0.035256. Value loss: 24.447515. Entropy: 0.856096.\n",
      "Iteration 573: Policy loss: 0.031569. Value loss: 19.758591. Entropy: 0.879890.\n",
      "episode: 249   score: 120.0  epsilon: 1.0    steps: 296  evaluation reward: 158.95\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 574: Policy loss: 0.009788. Value loss: 18.521393. Entropy: 1.031056.\n",
      "Iteration 575: Policy loss: 0.011386. Value loss: 11.206679. Entropy: 1.023563.\n",
      "Iteration 576: Policy loss: 0.008832. Value loss: 11.795790. Entropy: 1.024338.\n",
      "episode: 250   score: 115.0  epsilon: 1.0    steps: 240  evaluation reward: 158.7\n",
      "now time :  2019-03-05 16:36:58.452027\n",
      "episode: 251   score: 105.0  epsilon: 1.0    steps: 704  evaluation reward: 158.15\n",
      "episode: 252   score: 105.0  epsilon: 1.0    steps: 928  evaluation reward: 157.65\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 577: Policy loss: 0.003397. Value loss: 11.076324. Entropy: 1.015621.\n",
      "Iteration 578: Policy loss: 0.002452. Value loss: 7.746408. Entropy: 1.020716.\n",
      "Iteration 579: Policy loss: 0.003381. Value loss: 6.073564. Entropy: 1.026573.\n",
      "episode: 253   score: 110.0  epsilon: 1.0    steps: 672  evaluation reward: 157.4\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 580: Policy loss: 0.014334. Value loss: 17.426540. Entropy: 0.930507.\n",
      "Iteration 581: Policy loss: 0.015924. Value loss: 10.982036. Entropy: 0.930857.\n",
      "Iteration 582: Policy loss: 0.014295. Value loss: 8.224326. Entropy: 0.929324.\n",
      "episode: 254   score: 120.0  epsilon: 1.0    steps: 520  evaluation reward: 156.8\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 583: Policy loss: 0.042500. Value loss: 224.041016. Entropy: 0.837911.\n",
      "Iteration 584: Policy loss: 0.061258. Value loss: 116.073212. Entropy: 0.852000.\n",
      "Iteration 585: Policy loss: 0.050232. Value loss: 136.519745. Entropy: 0.884418.\n",
      "episode: 255   score: 415.0  epsilon: 1.0    steps: 624  evaluation reward: 159.75\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 586: Policy loss: 0.030900. Value loss: 224.232971. Entropy: 0.908578.\n",
      "Iteration 587: Policy loss: 0.058343. Value loss: 221.893173. Entropy: 0.878878.\n",
      "Iteration 588: Policy loss: 0.027926. Value loss: 183.051636. Entropy: 0.857432.\n",
      "episode: 256   score: 180.0  epsilon: 1.0    steps: 504  evaluation reward: 157.3\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 589: Policy loss: 0.003200. Value loss: 22.493734. Entropy: 0.847745.\n",
      "Iteration 590: Policy loss: 0.015067. Value loss: 17.381044. Entropy: 0.844748.\n",
      "Iteration 591: Policy loss: 0.011512. Value loss: 14.760795. Entropy: 0.829399.\n",
      "episode: 257   score: 155.0  epsilon: 1.0    steps: 960  evaluation reward: 157.25\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 592: Policy loss: 0.010467. Value loss: 31.132584. Entropy: 0.810404.\n",
      "Iteration 593: Policy loss: 0.004725. Value loss: 19.022999. Entropy: 0.816742.\n",
      "Iteration 594: Policy loss: -0.001829. Value loss: 16.756954. Entropy: 0.807052.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 595: Policy loss: 0.016096. Value loss: 30.103323. Entropy: 0.794372.\n",
      "Iteration 596: Policy loss: 0.008742. Value loss: 21.464958. Entropy: 0.840042.\n",
      "Iteration 597: Policy loss: -0.000565. Value loss: 20.695795. Entropy: 0.817981.\n",
      "episode: 258   score: 280.0  epsilon: 1.0    steps: 920  evaluation reward: 158.85\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 598: Policy loss: 0.004042. Value loss: 25.469109. Entropy: 0.925818.\n",
      "Iteration 599: Policy loss: 0.009682. Value loss: 18.242853. Entropy: 0.933352.\n",
      "Iteration 600: Policy loss: 0.011541. Value loss: 13.324501. Entropy: 0.925397.\n",
      "episode: 259   score: 220.0  epsilon: 1.0    steps: 640  evaluation reward: 159.7\n",
      "episode: 260   score: 105.0  epsilon: 1.0    steps: 808  evaluation reward: 159.7\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 601: Policy loss: 0.008613. Value loss: 22.907055. Entropy: 0.916485.\n",
      "Iteration 602: Policy loss: 0.019927. Value loss: 16.252781. Entropy: 0.939043.\n",
      "Iteration 603: Policy loss: 0.022694. Value loss: 14.762181. Entropy: 0.937336.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 604: Policy loss: 0.020118. Value loss: 28.234201. Entropy: 0.842543.\n",
      "Iteration 605: Policy loss: 0.013013. Value loss: 14.187515. Entropy: 0.802595.\n",
      "Iteration 606: Policy loss: 0.010349. Value loss: 12.516212. Entropy: 0.827998.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 607: Policy loss: 0.013112. Value loss: 33.836044. Entropy: 0.904922.\n",
      "Iteration 608: Policy loss: 0.018856. Value loss: 20.354830. Entropy: 0.905164.\n",
      "Iteration 609: Policy loss: 0.003084. Value loss: 16.786320. Entropy: 0.896600.\n",
      "episode: 261   score: 285.0  epsilon: 1.0    steps: 576  evaluation reward: 160.9\n",
      "episode: 262   score: 180.0  epsilon: 1.0    steps: 680  evaluation reward: 161.1\n",
      "episode: 263   score: 545.0  epsilon: 1.0    steps: 976  evaluation reward: 165.35\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 610: Policy loss: 0.027377. Value loss: 230.933701. Entropy: 0.864802.\n",
      "Iteration 611: Policy loss: 0.045004. Value loss: 151.482437. Entropy: 0.854201.\n",
      "Iteration 612: Policy loss: 0.060148. Value loss: 121.290665. Entropy: 0.859663.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 613: Policy loss: 0.009805. Value loss: 32.977127. Entropy: 0.802929.\n",
      "Iteration 614: Policy loss: 0.007213. Value loss: 19.134726. Entropy: 0.845639.\n",
      "Iteration 615: Policy loss: 0.019203. Value loss: 14.311233. Entropy: 0.839997.\n",
      "episode: 264   score: 230.0  epsilon: 1.0    steps: 216  evaluation reward: 165.55\n",
      "episode: 265   score: 130.0  epsilon: 1.0    steps: 744  evaluation reward: 165.35\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 616: Policy loss: 0.013675. Value loss: 33.599136. Entropy: 0.869805.\n",
      "Iteration 617: Policy loss: 0.024930. Value loss: 21.680447. Entropy: 0.883135.\n",
      "Iteration 618: Policy loss: 0.038285. Value loss: 17.876322. Entropy: 0.859026.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 619: Policy loss: 0.010630. Value loss: 22.407898. Entropy: 0.886057.\n",
      "Iteration 620: Policy loss: 0.016121. Value loss: 12.838200. Entropy: 0.896283.\n",
      "Iteration 621: Policy loss: 0.012546. Value loss: 10.086801. Entropy: 0.898999.\n",
      "episode: 266   score: 380.0  epsilon: 1.0    steps: 312  evaluation reward: 164.55\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 622: Policy loss: 0.020393. Value loss: 15.324241. Entropy: 0.994929.\n",
      "Iteration 623: Policy loss: 0.010408. Value loss: 9.283923. Entropy: 1.014889.\n",
      "Iteration 624: Policy loss: 0.006998. Value loss: 8.801962. Entropy: 1.016642.\n",
      "episode: 267   score: 155.0  epsilon: 1.0    steps: 752  evaluation reward: 164.9\n",
      "episode: 268   score: 120.0  epsilon: 1.0    steps: 1016  evaluation reward: 163.75\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 625: Policy loss: 0.010884. Value loss: 28.007191. Entropy: 0.856898.\n",
      "Iteration 626: Policy loss: 0.022924. Value loss: 21.305759. Entropy: 0.839953.\n",
      "Iteration 627: Policy loss: 0.014225. Value loss: 17.694683. Entropy: 0.846835.\n",
      "episode: 269   score: 160.0  epsilon: 1.0    steps: 912  evaluation reward: 164.25\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 628: Policy loss: 0.003515. Value loss: 18.764902. Entropy: 0.837991.\n",
      "Iteration 629: Policy loss: 0.009642. Value loss: 10.386155. Entropy: 0.849447.\n",
      "Iteration 630: Policy loss: 0.005682. Value loss: 9.982557. Entropy: 0.855046.\n",
      "episode: 270   score: 105.0  epsilon: 1.0    steps: 352  evaluation reward: 163.75\n",
      "episode: 271   score: 155.0  epsilon: 1.0    steps: 976  evaluation reward: 161.75\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 631: Policy loss: 0.018813. Value loss: 28.987020. Entropy: 0.837476.\n",
      "Iteration 632: Policy loss: 0.021648. Value loss: 21.625870. Entropy: 0.844379.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 633: Policy loss: 0.018825. Value loss: 15.387452. Entropy: 0.827489.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 634: Policy loss: 0.019513. Value loss: 25.465408. Entropy: 0.840675.\n",
      "Iteration 635: Policy loss: 0.020609. Value loss: 15.938982. Entropy: 0.838745.\n",
      "Iteration 636: Policy loss: 0.024434. Value loss: 14.802807. Entropy: 0.850093.\n",
      "episode: 272   score: 640.0  epsilon: 1.0    steps: 64  evaluation reward: 167.1\n",
      "episode: 273   score: 110.0  epsilon: 1.0    steps: 672  evaluation reward: 167.15\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 637: Policy loss: 0.008965. Value loss: 14.186105. Entropy: 0.908284.\n",
      "Iteration 638: Policy loss: 0.015568. Value loss: 7.404120. Entropy: 0.921935.\n",
      "Iteration 639: Policy loss: 0.009336. Value loss: 6.250698. Entropy: 0.923434.\n",
      "episode: 274   score: 230.0  epsilon: 1.0    steps: 976  evaluation reward: 168.25\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 640: Policy loss: 0.012522. Value loss: 17.005989. Entropy: 0.919873.\n",
      "Iteration 641: Policy loss: 0.002363. Value loss: 11.105515. Entropy: 0.923258.\n",
      "Iteration 642: Policy loss: -0.004586. Value loss: 11.162952. Entropy: 0.913926.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 643: Policy loss: 0.006130. Value loss: 13.610435. Entropy: 0.921208.\n",
      "Iteration 644: Policy loss: 0.007889. Value loss: 8.930134. Entropy: 0.911300.\n",
      "Iteration 645: Policy loss: 0.020674. Value loss: 9.574096. Entropy: 0.886098.\n",
      "episode: 275   score: 105.0  epsilon: 1.0    steps: 560  evaluation reward: 168.25\n",
      "episode: 276   score: 110.0  epsilon: 1.0    steps: 744  evaluation reward: 168.15\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 646: Policy loss: 0.017215. Value loss: 174.046753. Entropy: 0.955756.\n",
      "Iteration 647: Policy loss: 0.024969. Value loss: 102.093559. Entropy: 0.946322.\n",
      "Iteration 648: Policy loss: 0.020916. Value loss: 97.327614. Entropy: 0.948410.\n",
      "episode: 277   score: 110.0  epsilon: 1.0    steps: 448  evaluation reward: 168.15\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 649: Policy loss: 0.004740. Value loss: 14.300342. Entropy: 0.843395.\n",
      "Iteration 650: Policy loss: 0.002014. Value loss: 12.693882. Entropy: 0.824677.\n",
      "Iteration 651: Policy loss: -0.002920. Value loss: 11.026436. Entropy: 0.836185.\n",
      "episode: 278   score: 155.0  epsilon: 1.0    steps: 296  evaluation reward: 168.5\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 652: Policy loss: 0.004024. Value loss: 10.703476. Entropy: 0.794367.\n",
      "Iteration 653: Policy loss: 0.002819. Value loss: 8.545841. Entropy: 0.769391.\n",
      "Iteration 654: Policy loss: 0.003005. Value loss: 7.455724. Entropy: 0.792225.\n",
      "episode: 279   score: 110.0  epsilon: 1.0    steps: 264  evaluation reward: 168.0\n",
      "episode: 280   score: 240.0  epsilon: 1.0    steps: 912  evaluation reward: 169.35\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 655: Policy loss: 0.014060. Value loss: 18.046120. Entropy: 0.844154.\n",
      "Iteration 656: Policy loss: 0.011877. Value loss: 13.508235. Entropy: 0.861840.\n",
      "Iteration 657: Policy loss: 0.010323. Value loss: 12.031203. Entropy: 0.848542.\n",
      "episode: 281   score: 390.0  epsilon: 1.0    steps: 136  evaluation reward: 172.2\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 658: Policy loss: 0.003679. Value loss: 7.480022. Entropy: 0.812841.\n",
      "Iteration 659: Policy loss: 0.006189. Value loss: 6.040612. Entropy: 0.797292.\n",
      "Iteration 660: Policy loss: 0.008194. Value loss: 5.681789. Entropy: 0.794568.\n",
      "episode: 282   score: 120.0  epsilon: 1.0    steps: 920  evaluation reward: 171.25\n",
      "episode: 283   score: 105.0  epsilon: 1.0    steps: 952  evaluation reward: 171.25\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 661: Policy loss: 0.010014. Value loss: 9.784769. Entropy: 0.738417.\n",
      "Iteration 662: Policy loss: 0.003390. Value loss: 10.047137. Entropy: 0.713510.\n",
      "Iteration 663: Policy loss: 0.001209. Value loss: 9.013304. Entropy: 0.738219.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 664: Policy loss: 0.003204. Value loss: 5.277310. Entropy: 0.836179.\n",
      "Iteration 665: Policy loss: 0.005805. Value loss: 2.233545. Entropy: 0.814752.\n",
      "Iteration 666: Policy loss: -0.004026. Value loss: 2.257817. Entropy: 0.829478.\n",
      "episode: 284   score: 135.0  epsilon: 1.0    steps: 104  evaluation reward: 171.55\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 667: Policy loss: 0.021931. Value loss: 262.476318. Entropy: 0.741554.\n",
      "Iteration 668: Policy loss: 0.029902. Value loss: 146.185852. Entropy: 0.669690.\n",
      "Iteration 669: Policy loss: 0.031144. Value loss: 132.637527. Entropy: 0.671707.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 670: Policy loss: 0.008498. Value loss: 44.170742. Entropy: 0.770299.\n",
      "Iteration 671: Policy loss: 0.004540. Value loss: 14.362369. Entropy: 0.792681.\n",
      "Iteration 672: Policy loss: 0.010186. Value loss: 10.575889. Entropy: 0.779842.\n",
      "episode: 285   score: 210.0  epsilon: 1.0    steps: 208  evaluation reward: 172.6\n",
      "episode: 286   score: 105.0  epsilon: 1.0    steps: 264  evaluation reward: 171.5\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 673: Policy loss: 0.020034. Value loss: 26.260443. Entropy: 0.838768.\n",
      "Iteration 674: Policy loss: 0.030716. Value loss: 10.495220. Entropy: 0.836797.\n",
      "Iteration 675: Policy loss: 0.027192. Value loss: 6.456217. Entropy: 0.830206.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 676: Policy loss: 0.003099. Value loss: 11.501898. Entropy: 0.767425.\n",
      "Iteration 677: Policy loss: 0.007799. Value loss: 9.375066. Entropy: 0.781215.\n",
      "Iteration 678: Policy loss: 0.008797. Value loss: 9.068350. Entropy: 0.761722.\n",
      "episode: 287   score: 105.0  epsilon: 1.0    steps: 40  evaluation reward: 169.2\n",
      "episode: 288   score: 110.0  epsilon: 1.0    steps: 184  evaluation reward: 168.2\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 679: Policy loss: 0.003423. Value loss: 7.004653. Entropy: 0.843668.\n",
      "Iteration 680: Policy loss: 0.002954. Value loss: 4.081521. Entropy: 0.844847.\n",
      "Iteration 681: Policy loss: 0.007253. Value loss: 4.601309. Entropy: 0.842337.\n",
      "episode: 289   score: 105.0  epsilon: 1.0    steps: 24  evaluation reward: 167.75\n",
      "episode: 290   score: 355.0  epsilon: 1.0    steps: 200  evaluation reward: 170.25\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 682: Policy loss: 0.005964. Value loss: 13.781194. Entropy: 0.774502.\n",
      "Iteration 683: Policy loss: 0.003192. Value loss: 10.141525. Entropy: 0.766353.\n",
      "Iteration 684: Policy loss: 0.005081. Value loss: 10.775256. Entropy: 0.773635.\n",
      "episode: 291   score: 185.0  epsilon: 1.0    steps: 384  evaluation reward: 170.3\n",
      "episode: 292   score: 140.0  epsilon: 1.0    steps: 712  evaluation reward: 170.5\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 685: Policy loss: 0.005453. Value loss: 13.753260. Entropy: 0.873396.\n",
      "Iteration 686: Policy loss: 0.006878. Value loss: 9.381582. Entropy: 0.830255.\n",
      "Iteration 687: Policy loss: 0.007299. Value loss: 8.383670. Entropy: 0.852709.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 688: Policy loss: 0.019031. Value loss: 18.610056. Entropy: 0.788015.\n",
      "Iteration 689: Policy loss: 0.014523. Value loss: 8.243093. Entropy: 0.825366.\n",
      "Iteration 690: Policy loss: 0.013095. Value loss: 9.308959. Entropy: 0.809541.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 691: Policy loss: -0.003035. Value loss: 8.019459. Entropy: 0.910508.\n",
      "Iteration 692: Policy loss: 0.005837. Value loss: 5.887796. Entropy: 0.896975.\n",
      "Iteration 693: Policy loss: 0.004227. Value loss: 5.693999. Entropy: 0.923154.\n",
      "episode: 293   score: 105.0  epsilon: 1.0    steps: 744  evaluation reward: 169.9\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 694: Policy loss: 0.021501. Value loss: 11.450971. Entropy: 0.938869.\n",
      "Iteration 695: Policy loss: 0.020144. Value loss: 4.932230. Entropy: 0.926200.\n",
      "Iteration 696: Policy loss: 0.025881. Value loss: 3.583202. Entropy: 0.919889.\n",
      "episode: 294   score: 105.0  epsilon: 1.0    steps: 280  evaluation reward: 169.9\n",
      "episode: 295   score: 180.0  epsilon: 1.0    steps: 648  evaluation reward: 170.15\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 697: Policy loss: 0.006572. Value loss: 10.725657. Entropy: 0.892774.\n",
      "Iteration 698: Policy loss: 0.007798. Value loss: 8.348333. Entropy: 0.880628.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 699: Policy loss: 0.003671. Value loss: 7.094713. Entropy: 0.883266.\n",
      "episode: 296   score: 185.0  epsilon: 1.0    steps: 1024  evaluation reward: 170.45\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 700: Policy loss: 0.005047. Value loss: 21.787312. Entropy: 0.699120.\n",
      "Iteration 701: Policy loss: 0.002267. Value loss: 14.143950. Entropy: 0.725942.\n",
      "Iteration 702: Policy loss: 0.006253. Value loss: 12.851406. Entropy: 0.724204.\n",
      "episode: 297   score: 180.0  epsilon: 1.0    steps: 320  evaluation reward: 170.45\n",
      "episode: 298   score: 105.0  epsilon: 1.0    steps: 456  evaluation reward: 169.95\n",
      "episode: 299   score: 120.0  epsilon: 1.0    steps: 688  evaluation reward: 170.1\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 703: Policy loss: 0.009461. Value loss: 8.566672. Entropy: 0.820961.\n",
      "Iteration 704: Policy loss: 0.009368. Value loss: 5.685986. Entropy: 0.833686.\n",
      "Iteration 705: Policy loss: 0.008367. Value loss: 5.073797. Entropy: 0.837035.\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 706: Policy loss: 0.007974. Value loss: 14.104837. Entropy: 0.713134.\n",
      "Iteration 707: Policy loss: 0.003688. Value loss: 13.691617. Entropy: 0.688088.\n",
      "Iteration 708: Policy loss: 0.006488. Value loss: 12.211533. Entropy: 0.700037.\n",
      "episode: 300   score: 160.0  epsilon: 1.0    steps: 176  evaluation reward: 169.9\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 709: Policy loss: 0.002233. Value loss: 16.558388. Entropy: 0.868403.\n",
      "Iteration 710: Policy loss: 0.011223. Value loss: 11.949385. Entropy: 0.852455.\n",
      "Iteration 711: Policy loss: 0.011348. Value loss: 9.074662. Entropy: 0.849152.\n",
      "now time :  2019-03-05 16:39:37.124073\n",
      "episode: 301   score: 180.0  epsilon: 1.0    steps: 328  evaluation reward: 169.9\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 712: Policy loss: 0.007104. Value loss: 5.209435. Entropy: 0.775818.\n",
      "Iteration 713: Policy loss: 0.009426. Value loss: 3.376718. Entropy: 0.788806.\n",
      "Iteration 714: Policy loss: 0.002607. Value loss: 2.676011. Entropy: 0.763959.\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 715: Policy loss: 0.002982. Value loss: 19.289316. Entropy: 0.812448.\n",
      "Iteration 716: Policy loss: 0.003204. Value loss: 12.078935. Entropy: 0.789113.\n",
      "Iteration 717: Policy loss: -0.001030. Value loss: 10.041039. Entropy: 0.815176.\n",
      "episode: 302   score: 105.0  epsilon: 1.0    steps: 160  evaluation reward: 167.15\n",
      "episode: 303   score: 185.0  epsilon: 1.0    steps: 360  evaluation reward: 167.45\n",
      "episode: 304   score: 180.0  epsilon: 1.0    steps: 512  evaluation reward: 168.2\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 718: Policy loss: 0.003813. Value loss: 8.123505. Entropy: 0.803317.\n",
      "Iteration 719: Policy loss: 0.003599. Value loss: 6.175939. Entropy: 0.792252.\n",
      "Iteration 720: Policy loss: 0.003504. Value loss: 5.986456. Entropy: 0.792162.\n",
      "episode: 305   score: 110.0  epsilon: 1.0    steps: 160  evaluation reward: 168.25\n",
      "episode: 306   score: 120.0  epsilon: 1.0    steps: 648  evaluation reward: 167.85\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 721: Policy loss: 0.005587. Value loss: 17.619755. Entropy: 0.745227.\n",
      "Iteration 722: Policy loss: 0.006030. Value loss: 14.873191. Entropy: 0.702628.\n",
      "Iteration 723: Policy loss: 0.002175. Value loss: 14.182257. Entropy: 0.705546.\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 724: Policy loss: 0.011771. Value loss: 159.039520. Entropy: 0.847645.\n",
      "Iteration 725: Policy loss: 0.043189. Value loss: 86.174797. Entropy: 0.822283.\n",
      "Iteration 726: Policy loss: 0.026209. Value loss: 50.634792. Entropy: 0.805815.\n",
      "episode: 307   score: 315.0  epsilon: 1.0    steps: 464  evaluation reward: 169.95\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 727: Policy loss: 0.003718. Value loss: 19.389854. Entropy: 0.910806.\n",
      "Iteration 728: Policy loss: 0.002500. Value loss: 9.179072. Entropy: 0.899468.\n",
      "Iteration 729: Policy loss: 0.001653. Value loss: 6.886081. Entropy: 0.901997.\n",
      "episode: 308   score: 105.0  epsilon: 1.0    steps: 136  evaluation reward: 169.2\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 730: Policy loss: 0.039946. Value loss: 163.936447. Entropy: 0.860615.\n",
      "Iteration 731: Policy loss: 0.025785. Value loss: 109.064713. Entropy: 0.869769.\n",
      "Iteration 732: Policy loss: 0.028417. Value loss: 82.779510. Entropy: 0.879413.\n",
      "episode: 309   score: 105.0  epsilon: 1.0    steps: 920  evaluation reward: 166.9\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 733: Policy loss: 0.003101. Value loss: 32.728970. Entropy: 0.863123.\n",
      "Iteration 734: Policy loss: 0.009849. Value loss: 15.860862. Entropy: 0.871856.\n",
      "Iteration 735: Policy loss: 0.014087. Value loss: 13.329433. Entropy: 0.869643.\n",
      "episode: 310   score: 465.0  epsilon: 1.0    steps: 272  evaluation reward: 169.95\n",
      "episode: 311   score: 155.0  epsilon: 1.0    steps: 912  evaluation reward: 170.45\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 736: Policy loss: 0.014855. Value loss: 20.493603. Entropy: 0.756370.\n",
      "Iteration 737: Policy loss: 0.008519. Value loss: 10.401765. Entropy: 0.760059.\n",
      "Iteration 738: Policy loss: 0.011943. Value loss: 9.085764. Entropy: 0.771149.\n",
      "episode: 312   score: 125.0  epsilon: 1.0    steps: 456  evaluation reward: 169.9\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 739: Policy loss: 0.012181. Value loss: 28.431559. Entropy: 0.848412.\n",
      "Iteration 740: Policy loss: 0.012734. Value loss: 20.775949. Entropy: 0.833637.\n",
      "Iteration 741: Policy loss: 0.020737. Value loss: 16.147999. Entropy: 0.855821.\n",
      "episode: 313   score: 105.0  epsilon: 1.0    steps: 616  evaluation reward: 166.85\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 742: Policy loss: 0.001239. Value loss: 21.301228. Entropy: 0.844667.\n",
      "Iteration 743: Policy loss: 0.007264. Value loss: 13.457185. Entropy: 0.858233.\n",
      "Iteration 744: Policy loss: 0.000470. Value loss: 13.445776. Entropy: 0.859002.\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 745: Policy loss: 0.099409. Value loss: 81.281487. Entropy: 0.647921.\n",
      "Iteration 746: Policy loss: 0.045274. Value loss: 24.226692. Entropy: 0.668806.\n",
      "Iteration 747: Policy loss: 0.035163. Value loss: 15.782539. Entropy: 0.622332.\n",
      "episode: 314   score: 185.0  epsilon: 1.0    steps: 88  evaluation reward: 166.6\n",
      "episode: 315   score: 145.0  epsilon: 1.0    steps: 128  evaluation reward: 166.25\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 748: Policy loss: 0.011156. Value loss: 15.425705. Entropy: 0.851056.\n",
      "Iteration 749: Policy loss: 0.005693. Value loss: 9.176637. Entropy: 0.844837.\n",
      "Iteration 750: Policy loss: 0.011137. Value loss: 7.029860. Entropy: 0.854431.\n",
      "episode: 316   score: 155.0  epsilon: 1.0    steps: 520  evaluation reward: 163.7\n",
      "episode: 317   score: 135.0  epsilon: 1.0    steps: 608  evaluation reward: 163.5\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 751: Policy loss: 0.015148. Value loss: 30.896395. Entropy: 0.762265.\n",
      "Iteration 752: Policy loss: 0.012756. Value loss: 20.175968. Entropy: 0.775610.\n",
      "Iteration 753: Policy loss: 0.019288. Value loss: 17.471756. Entropy: 0.803678.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 754: Policy loss: 0.023581. Value loss: 26.464771. Entropy: 0.721301.\n",
      "Iteration 755: Policy loss: 0.007384. Value loss: 16.593248. Entropy: 0.722471.\n",
      "Iteration 756: Policy loss: 0.006942. Value loss: 13.280870. Entropy: 0.704093.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 757: Policy loss: 0.028089. Value loss: 318.659973. Entropy: 0.757252.\n",
      "Iteration 758: Policy loss: 0.021970. Value loss: 181.397079. Entropy: 0.727227.\n",
      "Iteration 759: Policy loss: 0.037740. Value loss: 152.060272. Entropy: 0.713481.\n",
      "episode: 318   score: 190.0  epsilon: 1.0    steps: 80  evaluation reward: 164.35\n",
      "episode: 319   score: 180.0  epsilon: 1.0    steps: 176  evaluation reward: 165.1\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 760: Policy loss: 0.005810. Value loss: 28.997271. Entropy: 0.720054.\n",
      "Iteration 761: Policy loss: 0.011813. Value loss: 14.919083. Entropy: 0.715937.\n",
      "Iteration 762: Policy loss: 0.009387. Value loss: 12.326769. Entropy: 0.723518.\n",
      "episode: 320   score: 260.0  epsilon: 1.0    steps: 464  evaluation reward: 166.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 763: Policy loss: 0.028554. Value loss: 263.595032. Entropy: 0.655357.\n",
      "Iteration 764: Policy loss: 0.038907. Value loss: 116.328506. Entropy: 0.598619.\n",
      "Iteration 765: Policy loss: 0.023802. Value loss: 76.868469. Entropy: 0.621494.\n",
      "episode: 321   score: 305.0  epsilon: 1.0    steps: 96  evaluation reward: 167.4\n",
      "episode: 322   score: 105.0  epsilon: 1.0    steps: 784  evaluation reward: 167.4\n",
      "episode: 323   score: 235.0  epsilon: 1.0    steps: 976  evaluation reward: 168.7\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 766: Policy loss: 0.006408. Value loss: 27.823837. Entropy: 0.705808.\n",
      "Iteration 767: Policy loss: 0.001982. Value loss: 17.522120. Entropy: 0.724412.\n",
      "Iteration 768: Policy loss: 0.007106. Value loss: 15.333076. Entropy: 0.720672.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 769: Policy loss: 0.002006. Value loss: 10.721659. Entropy: 0.726298.\n",
      "Iteration 770: Policy loss: 0.002600. Value loss: 7.390645. Entropy: 0.707903.\n",
      "Iteration 771: Policy loss: -0.001572. Value loss: 6.986671. Entropy: 0.713867.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 772: Policy loss: 0.007516. Value loss: 13.672387. Entropy: 0.867379.\n",
      "Iteration 773: Policy loss: 0.005893. Value loss: 11.198573. Entropy: 0.854045.\n",
      "Iteration 774: Policy loss: 0.013452. Value loss: 9.781030. Entropy: 0.871890.\n",
      "episode: 324   score: 105.0  epsilon: 1.0    steps: 328  evaluation reward: 167.6\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 775: Policy loss: 0.008948. Value loss: 31.697474. Entropy: 0.800053.\n",
      "Iteration 776: Policy loss: 0.032661. Value loss: 16.855658. Entropy: 0.785425.\n",
      "Iteration 777: Policy loss: 0.008814. Value loss: 15.066020. Entropy: 0.799101.\n",
      "episode: 325   score: 585.0  epsilon: 1.0    steps: 360  evaluation reward: 172.35\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 778: Policy loss: 0.001729. Value loss: 41.581879. Entropy: 0.725480.\n",
      "Iteration 779: Policy loss: 0.002157. Value loss: 21.722960. Entropy: 0.744009.\n",
      "Iteration 780: Policy loss: 0.004277. Value loss: 17.206995. Entropy: 0.754993.\n",
      "episode: 326   score: 150.0  epsilon: 1.0    steps: 880  evaluation reward: 172.5\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 781: Policy loss: 0.003561. Value loss: 30.657928. Entropy: 0.698558.\n",
      "Iteration 782: Policy loss: 0.015975. Value loss: 20.882942. Entropy: 0.705958.\n",
      "Iteration 783: Policy loss: 0.005572. Value loss: 17.248230. Entropy: 0.690802.\n",
      "episode: 327   score: 270.0  epsilon: 1.0    steps: 288  evaluation reward: 174.1\n",
      "episode: 328   score: 185.0  epsilon: 1.0    steps: 608  evaluation reward: 174.9\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 784: Policy loss: 0.001911. Value loss: 19.624527. Entropy: 0.688963.\n",
      "Iteration 785: Policy loss: 0.002750. Value loss: 9.387822. Entropy: 0.696296.\n",
      "Iteration 786: Policy loss: -0.001293. Value loss: 7.557263. Entropy: 0.694277.\n",
      "episode: 329   score: 185.0  epsilon: 1.0    steps: 824  evaluation reward: 175.4\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 787: Policy loss: 0.001907. Value loss: 26.530600. Entropy: 0.772394.\n",
      "Iteration 788: Policy loss: 0.008989. Value loss: 17.744469. Entropy: 0.781883.\n",
      "Iteration 789: Policy loss: 0.003084. Value loss: 18.618153. Entropy: 0.787231.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 790: Policy loss: 0.011945. Value loss: 10.674977. Entropy: 0.778459.\n",
      "Iteration 791: Policy loss: 0.029577. Value loss: 7.455363. Entropy: 0.760163.\n",
      "Iteration 792: Policy loss: 0.003406. Value loss: 7.485752. Entropy: 0.772032.\n",
      "episode: 330   score: 205.0  epsilon: 1.0    steps: 528  evaluation reward: 176.35\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 793: Policy loss: 0.025595. Value loss: 263.772644. Entropy: 0.776918.\n",
      "Iteration 794: Policy loss: 0.018216. Value loss: 182.618851. Entropy: 0.769602.\n",
      "Iteration 795: Policy loss: 0.028326. Value loss: 145.839584. Entropy: 0.770324.\n",
      "episode: 331   score: 380.0  epsilon: 1.0    steps: 552  evaluation reward: 178.55\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 796: Policy loss: 0.020313. Value loss: 35.344131. Entropy: 0.686097.\n",
      "Iteration 797: Policy loss: 0.009299. Value loss: 19.949966. Entropy: 0.712048.\n",
      "Iteration 798: Policy loss: 0.007901. Value loss: 17.402441. Entropy: 0.709570.\n",
      "episode: 332   score: 235.0  epsilon: 1.0    steps: 512  evaluation reward: 179.85\n",
      "episode: 333   score: 330.0  epsilon: 1.0    steps: 664  evaluation reward: 181.55\n",
      "episode: 334   score: 210.0  epsilon: 1.0    steps: 960  evaluation reward: 181.45\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 799: Policy loss: 0.009660. Value loss: 42.661606. Entropy: 0.665950.\n",
      "Iteration 800: Policy loss: 0.014019. Value loss: 25.245914. Entropy: 0.650706.\n",
      "Iteration 801: Policy loss: 0.006992. Value loss: 18.774609. Entropy: 0.654018.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 802: Policy loss: 0.005334. Value loss: 26.355696. Entropy: 0.612283.\n",
      "Iteration 803: Policy loss: 0.005679. Value loss: 18.753714. Entropy: 0.624638.\n",
      "Iteration 804: Policy loss: 0.011686. Value loss: 15.591232. Entropy: 0.614412.\n",
      "episode: 335   score: 165.0  epsilon: 1.0    steps: 88  evaluation reward: 182.05\n",
      "episode: 336   score: 195.0  epsilon: 1.0    steps: 384  evaluation reward: 182.95\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 805: Policy loss: 0.030062. Value loss: 207.382507. Entropy: 0.811114.\n",
      "Iteration 806: Policy loss: 0.033417. Value loss: 151.732132. Entropy: 0.789639.\n",
      "Iteration 807: Policy loss: 0.030648. Value loss: 107.959129. Entropy: 0.790095.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 808: Policy loss: 0.009688. Value loss: 51.293636. Entropy: 0.673794.\n",
      "Iteration 809: Policy loss: 0.018504. Value loss: 27.839003. Entropy: 0.609908.\n",
      "Iteration 810: Policy loss: 0.007036. Value loss: 14.648909. Entropy: 0.655537.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 811: Policy loss: 0.004372. Value loss: 27.643311. Entropy: 0.746122.\n",
      "Iteration 812: Policy loss: 0.001117. Value loss: 15.028470. Entropy: 0.763008.\n",
      "Iteration 813: Policy loss: 0.001703. Value loss: 11.943124. Entropy: 0.747164.\n",
      "episode: 337   score: 255.0  epsilon: 1.0    steps: 88  evaluation reward: 184.45\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 814: Policy loss: 0.000146. Value loss: 17.206114. Entropy: 0.713264.\n",
      "Iteration 815: Policy loss: 0.005627. Value loss: 10.157343. Entropy: 0.699385.\n",
      "Iteration 816: Policy loss: 0.002172. Value loss: 8.741473. Entropy: 0.673771.\n",
      "episode: 338   score: 105.0  epsilon: 1.0    steps: 64  evaluation reward: 184.15\n",
      "episode: 339   score: 135.0  epsilon: 1.0    steps: 664  evaluation reward: 184.45\n",
      "episode: 340   score: 135.0  epsilon: 1.0    steps: 712  evaluation reward: 184.75\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 817: Policy loss: -0.001670. Value loss: 33.281109. Entropy: 0.688869.\n",
      "Iteration 818: Policy loss: -0.001460. Value loss: 22.982443. Entropy: 0.693646.\n",
      "Iteration 819: Policy loss: 0.002015. Value loss: 18.135616. Entropy: 0.645373.\n",
      "episode: 341   score: 105.0  epsilon: 1.0    steps: 112  evaluation reward: 184.75\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 820: Policy loss: 0.023489. Value loss: 277.013153. Entropy: 0.630446.\n",
      "Iteration 821: Policy loss: 0.071422. Value loss: 124.732635. Entropy: 0.609853.\n",
      "Iteration 822: Policy loss: 0.032574. Value loss: 88.877380. Entropy: 0.587941.\n",
      "episode: 342   score: 415.0  epsilon: 1.0    steps: 536  evaluation reward: 187.8\n",
      "episode: 343   score: 180.0  epsilon: 1.0    steps: 808  evaluation reward: 186.35\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 823: Policy loss: 0.004316. Value loss: 33.852226. Entropy: 0.678588.\n",
      "Iteration 824: Policy loss: 0.002613. Value loss: 24.056215. Entropy: 0.653015.\n",
      "Iteration 825: Policy loss: 0.006410. Value loss: 23.944571. Entropy: 0.673558.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 826: Policy loss: 0.009376. Value loss: 20.338322. Entropy: 0.618497.\n",
      "Iteration 827: Policy loss: 0.005529. Value loss: 13.268706. Entropy: 0.634922.\n",
      "Iteration 828: Policy loss: 0.009352. Value loss: 11.236395. Entropy: 0.628776.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 344   score: 490.0  epsilon: 1.0    steps: 208  evaluation reward: 190.5\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 829: Policy loss: 0.000685. Value loss: 18.869003. Entropy: 0.743540.\n",
      "Iteration 830: Policy loss: 0.007519. Value loss: 11.662848. Entropy: 0.753858.\n",
      "Iteration 831: Policy loss: 0.009492. Value loss: 9.392092. Entropy: 0.751440.\n",
      "episode: 345   score: 180.0  epsilon: 1.0    steps: 800  evaluation reward: 191.25\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 832: Policy loss: 0.000296. Value loss: 35.688347. Entropy: 0.610841.\n",
      "Iteration 833: Policy loss: 0.005578. Value loss: 25.128162. Entropy: 0.596622.\n",
      "Iteration 834: Policy loss: 0.001868. Value loss: 20.504234. Entropy: 0.598680.\n",
      "episode: 346   score: 180.0  epsilon: 1.0    steps: 72  evaluation reward: 192.0\n",
      "episode: 347   score: 155.0  epsilon: 1.0    steps: 160  evaluation reward: 192.8\n",
      "episode: 348   score: 210.0  epsilon: 1.0    steps: 952  evaluation reward: 193.85\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 835: Policy loss: 0.001185. Value loss: 22.902624. Entropy: 0.684094.\n",
      "Iteration 836: Policy loss: 0.005690. Value loss: 16.047398. Entropy: 0.647965.\n",
      "Iteration 837: Policy loss: 0.003268. Value loss: 13.801296. Entropy: 0.649130.\n",
      "episode: 349   score: 155.0  epsilon: 1.0    steps: 888  evaluation reward: 194.2\n",
      "episode: 350   score: 265.0  epsilon: 1.0    steps: 928  evaluation reward: 195.7\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 838: Policy loss: -0.000800. Value loss: 41.107292. Entropy: 0.643860.\n",
      "Iteration 839: Policy loss: 0.010433. Value loss: 30.572102. Entropy: 0.642570.\n",
      "Iteration 840: Policy loss: 0.008985. Value loss: 24.165236. Entropy: 0.644141.\n",
      "now time :  2019-03-05 16:42:09.341796\n",
      "episode: 351   score: 210.0  epsilon: 1.0    steps: 512  evaluation reward: 196.75\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 841: Policy loss: 0.019502. Value loss: 17.457464. Entropy: 0.585776.\n",
      "Iteration 842: Policy loss: 0.028104. Value loss: 11.215919. Entropy: 0.564281.\n",
      "Iteration 843: Policy loss: 0.021926. Value loss: 8.957726. Entropy: 0.564010.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 844: Policy loss: 0.004889. Value loss: 16.933176. Entropy: 0.543698.\n",
      "Iteration 845: Policy loss: 0.006689. Value loss: 13.320129. Entropy: 0.510314.\n",
      "Iteration 846: Policy loss: 0.001223. Value loss: 14.008712. Entropy: 0.553360.\n",
      "episode: 352   score: 155.0  epsilon: 1.0    steps: 960  evaluation reward: 197.25\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 847: Policy loss: 0.010486. Value loss: 25.788736. Entropy: 0.595340.\n",
      "Iteration 848: Policy loss: 0.011437. Value loss: 14.767723. Entropy: 0.591077.\n",
      "Iteration 849: Policy loss: 0.022994. Value loss: 14.305925. Entropy: 0.577868.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 850: Policy loss: 0.008074. Value loss: 12.859839. Entropy: 0.475638.\n",
      "Iteration 851: Policy loss: 0.009109. Value loss: 6.871343. Entropy: 0.437993.\n",
      "Iteration 852: Policy loss: 0.009004. Value loss: 5.569913. Entropy: 0.445572.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 853: Policy loss: 0.019655. Value loss: 25.550543. Entropy: 0.583225.\n",
      "Iteration 854: Policy loss: 0.027328. Value loss: 21.325523. Entropy: 0.619810.\n",
      "Iteration 855: Policy loss: 0.030750. Value loss: 19.560381. Entropy: 0.706649.\n",
      "episode: 353   score: 210.0  epsilon: 1.0    steps: 112  evaluation reward: 198.25\n",
      "episode: 354   score: 155.0  epsilon: 1.0    steps: 688  evaluation reward: 198.6\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 856: Policy loss: 0.006540. Value loss: 15.197925. Entropy: 0.705588.\n",
      "Iteration 857: Policy loss: 0.008664. Value loss: 10.565219. Entropy: 0.705735.\n",
      "Iteration 858: Policy loss: 0.009050. Value loss: 8.477695. Entropy: 0.699055.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 859: Policy loss: 0.006113. Value loss: 345.368805. Entropy: 0.668813.\n",
      "Iteration 860: Policy loss: 0.027967. Value loss: 227.501953. Entropy: 0.673032.\n",
      "Iteration 861: Policy loss: 0.024515. Value loss: 187.181732. Entropy: 0.677194.\n",
      "episode: 355   score: 195.0  epsilon: 1.0    steps: 264  evaluation reward: 196.4\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 862: Policy loss: 0.012671. Value loss: 38.272278. Entropy: 0.780602.\n",
      "Iteration 863: Policy loss: 0.021978. Value loss: 24.771307. Entropy: 0.777713.\n",
      "Iteration 864: Policy loss: 0.022575. Value loss: 22.936159. Entropy: 0.732850.\n",
      "episode: 356   score: 315.0  epsilon: 1.0    steps: 104  evaluation reward: 197.75\n",
      "episode: 357   score: 505.0  epsilon: 1.0    steps: 680  evaluation reward: 201.25\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 865: Policy loss: 0.019747. Value loss: 21.105705. Entropy: 0.774035.\n",
      "Iteration 866: Policy loss: 0.012355. Value loss: 12.749240. Entropy: 0.756298.\n",
      "Iteration 867: Policy loss: 0.006238. Value loss: 11.169735. Entropy: 0.768603.\n",
      "episode: 358   score: 210.0  epsilon: 1.0    steps: 408  evaluation reward: 200.55\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 868: Policy loss: 0.003300. Value loss: 30.363392. Entropy: 0.768461.\n",
      "Iteration 869: Policy loss: 0.007258. Value loss: 20.030550. Entropy: 0.771406.\n",
      "Iteration 870: Policy loss: 0.000717. Value loss: 15.629988. Entropy: 0.783283.\n",
      "episode: 359   score: 280.0  epsilon: 1.0    steps: 208  evaluation reward: 201.15\n",
      "episode: 360   score: 465.0  epsilon: 1.0    steps: 704  evaluation reward: 204.75\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 871: Policy loss: 0.002727. Value loss: 18.432182. Entropy: 0.677302.\n",
      "Iteration 872: Policy loss: 0.004039. Value loss: 13.615979. Entropy: 0.671184.\n",
      "Iteration 873: Policy loss: -0.004206. Value loss: 9.633421. Entropy: 0.662596.\n",
      "episode: 361   score: 155.0  epsilon: 1.0    steps: 400  evaluation reward: 203.45\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 874: Policy loss: 0.003536. Value loss: 29.311836. Entropy: 0.620480.\n",
      "Iteration 875: Policy loss: 0.005912. Value loss: 21.092472. Entropy: 0.623345.\n",
      "Iteration 876: Policy loss: 0.005203. Value loss: 19.349035. Entropy: 0.625907.\n",
      "episode: 362   score: 210.0  epsilon: 1.0    steps: 88  evaluation reward: 203.75\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 877: Policy loss: 0.015764. Value loss: 9.913170. Entropy: 0.694468.\n",
      "Iteration 878: Policy loss: 0.031200. Value loss: 8.381422. Entropy: 0.706773.\n",
      "Iteration 879: Policy loss: 0.009951. Value loss: 6.836185. Entropy: 0.701872.\n",
      "episode: 363   score: 210.0  epsilon: 1.0    steps: 760  evaluation reward: 200.4\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 880: Policy loss: 0.001476. Value loss: 15.456206. Entropy: 0.668178.\n",
      "Iteration 881: Policy loss: 0.002404. Value loss: 13.630505. Entropy: 0.671593.\n",
      "Iteration 882: Policy loss: 0.005047. Value loss: 11.006166. Entropy: 0.668548.\n",
      "episode: 364   score: 210.0  epsilon: 1.0    steps: 576  evaluation reward: 200.2\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 883: Policy loss: 0.002205. Value loss: 15.260204. Entropy: 0.627939.\n",
      "Iteration 884: Policy loss: 0.003028. Value loss: 12.185227. Entropy: 0.643044.\n",
      "Iteration 885: Policy loss: 0.009691. Value loss: 12.243009. Entropy: 0.636536.\n",
      "episode: 365   score: 210.0  epsilon: 1.0    steps: 176  evaluation reward: 201.0\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 886: Policy loss: 0.011802. Value loss: 16.595999. Entropy: 0.579068.\n",
      "Iteration 887: Policy loss: -0.000946. Value loss: 14.049193. Entropy: 0.575680.\n",
      "Iteration 888: Policy loss: -0.002278. Value loss: 10.689799. Entropy: 0.577083.\n",
      "episode: 366   score: 180.0  epsilon: 1.0    steps: 200  evaluation reward: 199.0\n",
      "episode: 367   score: 155.0  epsilon: 1.0    steps: 600  evaluation reward: 199.0\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 889: Policy loss: 0.004129. Value loss: 20.587801. Entropy: 0.576230.\n",
      "Iteration 890: Policy loss: 0.002476. Value loss: 14.261896. Entropy: 0.551782.\n",
      "Iteration 891: Policy loss: 0.001089. Value loss: 12.074509. Entropy: 0.566522.\n",
      "episode: 368   score: 180.0  epsilon: 1.0    steps: 96  evaluation reward: 199.6\n",
      "episode: 369   score: 105.0  epsilon: 1.0    steps: 280  evaluation reward: 199.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 370   score: 210.0  epsilon: 1.0    steps: 824  evaluation reward: 200.1\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 892: Policy loss: 0.005237. Value loss: 15.850064. Entropy: 0.656283.\n",
      "Iteration 893: Policy loss: 0.005858. Value loss: 13.868116. Entropy: 0.647288.\n",
      "Iteration 894: Policy loss: 0.005596. Value loss: 11.934665. Entropy: 0.658233.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 895: Policy loss: 0.007658. Value loss: 19.289324. Entropy: 0.761513.\n",
      "Iteration 896: Policy loss: 0.007834. Value loss: 17.459211. Entropy: 0.759682.\n",
      "Iteration 897: Policy loss: 0.004058. Value loss: 15.807039. Entropy: 0.755581.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 898: Policy loss: 0.012831. Value loss: 14.578941. Entropy: 0.698027.\n",
      "Iteration 899: Policy loss: 0.017978. Value loss: 11.851262. Entropy: 0.688455.\n",
      "Iteration 900: Policy loss: 0.010766. Value loss: 11.161279. Entropy: 0.697559.\n",
      "episode: 371   score: 210.0  epsilon: 1.0    steps: 952  evaluation reward: 200.65\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 901: Policy loss: 0.003215. Value loss: 13.208875. Entropy: 0.613415.\n",
      "Iteration 902: Policy loss: 0.001372. Value loss: 10.046082. Entropy: 0.604132.\n",
      "Iteration 903: Policy loss: -0.000954. Value loss: 8.389504. Entropy: 0.621514.\n",
      "episode: 372   score: 185.0  epsilon: 1.0    steps: 552  evaluation reward: 196.1\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 904: Policy loss: 0.008431. Value loss: 281.631165. Entropy: 0.613835.\n",
      "Iteration 905: Policy loss: 0.010302. Value loss: 121.986969. Entropy: 0.576694.\n",
      "Iteration 906: Policy loss: 0.014876. Value loss: 127.194489. Entropy: 0.566410.\n",
      "episode: 373   score: 410.0  epsilon: 1.0    steps: 768  evaluation reward: 199.1\n",
      "episode: 374   score: 105.0  epsilon: 1.0    steps: 960  evaluation reward: 197.85\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 907: Policy loss: -0.000150. Value loss: 44.462696. Entropy: 0.480574.\n",
      "Iteration 908: Policy loss: 0.017806. Value loss: 28.308767. Entropy: 0.448393.\n",
      "Iteration 909: Policy loss: -0.001545. Value loss: 23.721432. Entropy: 0.461869.\n",
      "episode: 375   score: 210.0  epsilon: 1.0    steps: 24  evaluation reward: 198.9\n",
      "episode: 376   score: 135.0  epsilon: 1.0    steps: 480  evaluation reward: 199.15\n",
      "episode: 377   score: 180.0  epsilon: 1.0    steps: 584  evaluation reward: 199.85\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 910: Policy loss: 0.003175. Value loss: 28.048437. Entropy: 0.576696.\n",
      "Iteration 911: Policy loss: 0.000810. Value loss: 11.318168. Entropy: 0.562535.\n",
      "Iteration 912: Policy loss: -0.000475. Value loss: 10.758166. Entropy: 0.580633.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 913: Policy loss: 0.001499. Value loss: 24.732225. Entropy: 0.702725.\n",
      "Iteration 914: Policy loss: 0.002817. Value loss: 20.634691. Entropy: 0.687636.\n",
      "Iteration 915: Policy loss: -0.002400. Value loss: 17.379486. Entropy: 0.704612.\n",
      "episode: 378   score: 405.0  epsilon: 1.0    steps: 64  evaluation reward: 202.35\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 916: Policy loss: 0.002196. Value loss: 6.054732. Entropy: 0.791263.\n",
      "Iteration 917: Policy loss: 0.007110. Value loss: 4.201729. Entropy: 0.796368.\n",
      "Iteration 918: Policy loss: 0.009528. Value loss: 4.009293. Entropy: 0.793502.\n",
      "episode: 379   score: 105.0  epsilon: 1.0    steps: 8  evaluation reward: 202.3\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 919: Policy loss: 0.005236. Value loss: 5.774291. Entropy: 0.765364.\n",
      "Iteration 920: Policy loss: 0.003421. Value loss: 4.355486. Entropy: 0.774215.\n",
      "Iteration 921: Policy loss: 0.001375. Value loss: 3.581056. Entropy: 0.767512.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 922: Policy loss: 0.001007. Value loss: 9.749648. Entropy: 0.534331.\n",
      "Iteration 923: Policy loss: 0.002066. Value loss: 8.085170. Entropy: 0.561826.\n",
      "Iteration 924: Policy loss: 0.005467. Value loss: 6.749799. Entropy: 0.541974.\n",
      "episode: 380   score: 210.0  epsilon: 1.0    steps: 72  evaluation reward: 202.0\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 925: Policy loss: 0.003000. Value loss: 15.527928. Entropy: 0.441167.\n",
      "Iteration 926: Policy loss: -0.000208. Value loss: 8.960712. Entropy: 0.431140.\n",
      "Iteration 927: Policy loss: 0.007606. Value loss: 8.871807. Entropy: 0.452484.\n",
      "episode: 381   score: 210.0  epsilon: 1.0    steps: 432  evaluation reward: 200.2\n",
      "episode: 382   score: 210.0  epsilon: 1.0    steps: 696  evaluation reward: 201.1\n",
      "episode: 383   score: 155.0  epsilon: 1.0    steps: 808  evaluation reward: 201.6\n",
      "episode: 384   score: 180.0  epsilon: 1.0    steps: 896  evaluation reward: 202.05\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 928: Policy loss: 0.002035. Value loss: 26.874170. Entropy: 0.461174.\n",
      "Iteration 929: Policy loss: -0.002916. Value loss: 21.311714. Entropy: 0.492916.\n",
      "Iteration 930: Policy loss: -0.001209. Value loss: 18.642746. Entropy: 0.499619.\n",
      "episode: 385   score: 105.0  epsilon: 1.0    steps: 160  evaluation reward: 201.0\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 931: Policy loss: 0.008795. Value loss: 13.553938. Entropy: 0.591977.\n",
      "Iteration 932: Policy loss: 0.006157. Value loss: 11.388163. Entropy: 0.596270.\n",
      "Iteration 933: Policy loss: 0.002112. Value loss: 10.995216. Entropy: 0.585274.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 934: Policy loss: 0.000904. Value loss: 10.846697. Entropy: 0.720762.\n",
      "Iteration 935: Policy loss: 0.003521. Value loss: 8.951012. Entropy: 0.724622.\n",
      "Iteration 936: Policy loss: -0.001255. Value loss: 7.272924. Entropy: 0.730537.\n",
      "episode: 386   score: 135.0  epsilon: 1.0    steps: 280  evaluation reward: 201.3\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 937: Policy loss: 0.002085. Value loss: 6.511117. Entropy: 0.779957.\n",
      "Iteration 938: Policy loss: 0.004888. Value loss: 2.640982. Entropy: 0.776672.\n",
      "Iteration 939: Policy loss: 0.000192. Value loss: 2.154027. Entropy: 0.779518.\n",
      "episode: 387   score: 285.0  epsilon: 1.0    steps: 648  evaluation reward: 203.1\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 940: Policy loss: 0.011051. Value loss: 13.723701. Entropy: 0.618141.\n",
      "Iteration 941: Policy loss: 0.006095. Value loss: 6.827946. Entropy: 0.621937.\n",
      "Iteration 942: Policy loss: 0.006649. Value loss: 6.248940. Entropy: 0.594290.\n",
      "episode: 388   score: 210.0  epsilon: 1.0    steps: 544  evaluation reward: 204.1\n",
      "episode: 389   score: 105.0  epsilon: 1.0    steps: 1008  evaluation reward: 204.1\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 943: Policy loss: 0.007907. Value loss: 9.266675. Entropy: 0.466129.\n",
      "Iteration 944: Policy loss: -0.001609. Value loss: 8.948430. Entropy: 0.454908.\n",
      "Iteration 945: Policy loss: -0.000939. Value loss: 6.472303. Entropy: 0.487285.\n",
      "episode: 390   score: 210.0  epsilon: 1.0    steps: 840  evaluation reward: 202.65\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 946: Policy loss: 0.000446. Value loss: 22.808529. Entropy: 0.462976.\n",
      "Iteration 947: Policy loss: 0.010050. Value loss: 19.017889. Entropy: 0.456632.\n",
      "Iteration 948: Policy loss: 0.006606. Value loss: 17.786631. Entropy: 0.445257.\n",
      "episode: 391   score: 210.0  epsilon: 1.0    steps: 144  evaluation reward: 202.9\n",
      "episode: 392   score: 210.0  epsilon: 1.0    steps: 176  evaluation reward: 203.6\n",
      "episode: 393   score: 210.0  epsilon: 1.0    steps: 704  evaluation reward: 204.65\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 949: Policy loss: 0.000394. Value loss: 10.921180. Entropy: 0.588138.\n",
      "Iteration 950: Policy loss: 0.006223. Value loss: 9.979737. Entropy: 0.573922.\n",
      "Iteration 951: Policy loss: 0.007881. Value loss: 9.898809. Entropy: 0.592232.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 952: Policy loss: 0.005634. Value loss: 9.734845. Entropy: 0.618338.\n",
      "Iteration 953: Policy loss: 0.002920. Value loss: 7.714938. Entropy: 0.619849.\n",
      "Iteration 954: Policy loss: 0.005262. Value loss: 7.873026. Entropy: 0.619298.\n",
      "episode: 394   score: 210.0  epsilon: 1.0    steps: 856  evaluation reward: 205.7\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 955: Policy loss: 0.003686. Value loss: 13.603600. Entropy: 0.822953.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 956: Policy loss: 0.015347. Value loss: 10.802694. Entropy: 0.843596.\n",
      "Iteration 957: Policy loss: 0.010047. Value loss: 11.181715. Entropy: 0.809457.\n",
      "episode: 395   score: 155.0  epsilon: 1.0    steps: 920  evaluation reward: 205.45\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 958: Policy loss: 0.005698. Value loss: 7.272008. Entropy: 0.674476.\n",
      "Iteration 959: Policy loss: -0.001404. Value loss: 3.147821. Entropy: 0.686359.\n",
      "Iteration 960: Policy loss: -0.000874. Value loss: 2.927004. Entropy: 0.662631.\n",
      "episode: 396   score: 180.0  epsilon: 1.0    steps: 968  evaluation reward: 205.4\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 961: Policy loss: 0.009192. Value loss: 11.056853. Entropy: 0.441508.\n",
      "Iteration 962: Policy loss: 0.003878. Value loss: 7.909068. Entropy: 0.411175.\n",
      "Iteration 963: Policy loss: 0.006188. Value loss: 8.353244. Entropy: 0.441147.\n",
      "episode: 397   score: 210.0  epsilon: 1.0    steps: 392  evaluation reward: 205.7\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 964: Policy loss: 0.000754. Value loss: 13.935481. Entropy: 0.427066.\n",
      "Iteration 965: Policy loss: 0.002434. Value loss: 11.515200. Entropy: 0.406853.\n",
      "Iteration 966: Policy loss: -0.000216. Value loss: 9.754867. Entropy: 0.417964.\n",
      "episode: 398   score: 180.0  epsilon: 1.0    steps: 96  evaluation reward: 206.45\n",
      "episode: 399   score: 210.0  epsilon: 1.0    steps: 520  evaluation reward: 207.35\n",
      "episode: 400   score: 210.0  epsilon: 1.0    steps: 560  evaluation reward: 207.85\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 967: Policy loss: 0.000852. Value loss: 13.394711. Entropy: 0.518677.\n",
      "Iteration 968: Policy loss: 0.000518. Value loss: 11.558205. Entropy: 0.520612.\n",
      "Iteration 969: Policy loss: -0.001459. Value loss: 10.754092. Entropy: 0.534151.\n",
      "now time :  2019-03-05 16:44:40.524342\n",
      "episode: 401   score: 185.0  epsilon: 1.0    steps: 112  evaluation reward: 207.9\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 970: Policy loss: 0.021624. Value loss: 13.901205. Entropy: 0.598038.\n",
      "Iteration 971: Policy loss: 0.010805. Value loss: 11.955289. Entropy: 0.616019.\n",
      "Iteration 972: Policy loss: 0.011509. Value loss: 10.200748. Entropy: 0.612242.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 973: Policy loss: -0.002245. Value loss: 10.795720. Entropy: 0.667403.\n",
      "Iteration 974: Policy loss: -0.000493. Value loss: 9.572701. Entropy: 0.658180.\n",
      "Iteration 975: Policy loss: -0.001756. Value loss: 8.071799. Entropy: 0.657789.\n",
      "episode: 402   score: 210.0  epsilon: 1.0    steps: 232  evaluation reward: 208.95\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 976: Policy loss: 0.005199. Value loss: 7.210398. Entropy: 0.629686.\n",
      "Iteration 977: Policy loss: 0.006701. Value loss: 3.608055. Entropy: 0.637989.\n",
      "Iteration 978: Policy loss: -0.001535. Value loss: 3.081972. Entropy: 0.643864.\n",
      "episode: 403   score: 210.0  epsilon: 1.0    steps: 360  evaluation reward: 209.2\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 979: Policy loss: 0.009029. Value loss: 8.624035. Entropy: 0.445938.\n",
      "Iteration 980: Policy loss: 0.011812. Value loss: 7.912005. Entropy: 0.435205.\n",
      "Iteration 981: Policy loss: 0.006524. Value loss: 6.602810. Entropy: 0.435710.\n",
      "episode: 404   score: 155.0  epsilon: 1.0    steps: 248  evaluation reward: 208.95\n",
      "episode: 405   score: 210.0  epsilon: 1.0    steps: 920  evaluation reward: 209.95\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 982: Policy loss: -0.000175. Value loss: 23.055086. Entropy: 0.417818.\n",
      "Iteration 983: Policy loss: 0.000359. Value loss: 14.403570. Entropy: 0.415908.\n",
      "Iteration 984: Policy loss: 0.000821. Value loss: 10.004481. Entropy: 0.416206.\n",
      "episode: 406   score: 180.0  epsilon: 1.0    steps: 312  evaluation reward: 210.55\n",
      "episode: 407   score: 180.0  epsilon: 1.0    steps: 920  evaluation reward: 209.2\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 985: Policy loss: 0.007531. Value loss: 22.042419. Entropy: 0.505637.\n",
      "Iteration 986: Policy loss: -0.001275. Value loss: 16.998241. Entropy: 0.497555.\n",
      "Iteration 987: Policy loss: -0.000002. Value loss: 15.370036. Entropy: 0.498555.\n",
      "episode: 408   score: 210.0  epsilon: 1.0    steps: 32  evaluation reward: 210.25\n",
      "episode: 409   score: 180.0  epsilon: 1.0    steps: 544  evaluation reward: 211.0\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 988: Policy loss: 0.001633. Value loss: 12.231608. Entropy: 0.613315.\n",
      "Iteration 989: Policy loss: 0.001273. Value loss: 10.369547. Entropy: 0.616520.\n",
      "Iteration 990: Policy loss: 0.002381. Value loss: 11.105580. Entropy: 0.606093.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 991: Policy loss: 0.003192. Value loss: 9.333858. Entropy: 0.555587.\n",
      "Iteration 992: Policy loss: 0.003804. Value loss: 10.040330. Entropy: 0.581646.\n",
      "Iteration 993: Policy loss: 0.001261. Value loss: 6.134367. Entropy: 0.561754.\n",
      "episode: 410   score: 180.0  epsilon: 1.0    steps: 680  evaluation reward: 208.15\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 994: Policy loss: 0.007357. Value loss: 6.427750. Entropy: 0.653943.\n",
      "Iteration 995: Policy loss: 0.001698. Value loss: 3.469868. Entropy: 0.643105.\n",
      "Iteration 996: Policy loss: 0.002952. Value loss: 4.318052. Entropy: 0.639267.\n",
      "episode: 411   score: 155.0  epsilon: 1.0    steps: 704  evaluation reward: 208.15\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 997: Policy loss: 0.005552. Value loss: 6.670577. Entropy: 0.454924.\n",
      "Iteration 998: Policy loss: 0.002680. Value loss: 4.447353. Entropy: 0.451879.\n",
      "Iteration 999: Policy loss: 0.000960. Value loss: 3.613210. Entropy: 0.457907.\n",
      "episode: 412   score: 180.0  epsilon: 1.0    steps: 664  evaluation reward: 208.7\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 1000: Policy loss: 0.006732. Value loss: 12.623495. Entropy: 0.302782.\n",
      "Iteration 1001: Policy loss: -0.001148. Value loss: 9.576796. Entropy: 0.381489.\n",
      "Iteration 1002: Policy loss: -0.007799. Value loss: 8.155135. Entropy: 0.404365.\n",
      "episode: 413   score: 105.0  epsilon: 1.0    steps: 48  evaluation reward: 208.7\n",
      "episode: 414   score: 210.0  epsilon: 1.0    steps: 288  evaluation reward: 208.95\n",
      "episode: 415   score: 180.0  epsilon: 1.0    steps: 560  evaluation reward: 209.3\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1003: Policy loss: 0.003869. Value loss: 13.084081. Entropy: 0.575831.\n",
      "Iteration 1004: Policy loss: 0.005692. Value loss: 8.968392. Entropy: 0.579046.\n",
      "Iteration 1005: Policy loss: 0.003761. Value loss: 7.306227. Entropy: 0.573823.\n",
      "episode: 416   score: 355.0  epsilon: 1.0    steps: 792  evaluation reward: 211.3\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1006: Policy loss: 0.005983. Value loss: 284.182526. Entropy: 0.552619.\n",
      "Iteration 1007: Policy loss: 0.017716. Value loss: 212.956192. Entropy: 0.540436.\n",
      "Iteration 1008: Policy loss: 0.013166. Value loss: 235.104736. Entropy: 0.579035.\n",
      "episode: 417   score: 210.0  epsilon: 1.0    steps: 992  evaluation reward: 212.05\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1009: Policy loss: 0.008516. Value loss: 39.896152. Entropy: 0.694302.\n",
      "Iteration 1010: Policy loss: 0.017452. Value loss: 24.388367. Entropy: 0.684610.\n",
      "Iteration 1011: Policy loss: 0.009202. Value loss: 20.247581. Entropy: 0.709527.\n",
      "episode: 418   score: 260.0  epsilon: 1.0    steps: 912  evaluation reward: 212.75\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1012: Policy loss: 0.002880. Value loss: 9.703651. Entropy: 0.652528.\n",
      "Iteration 1013: Policy loss: 0.016282. Value loss: 5.847049. Entropy: 0.636789.\n",
      "Iteration 1014: Policy loss: 0.000612. Value loss: 4.844866. Entropy: 0.654945.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1015: Policy loss: -0.001087. Value loss: 18.349768. Entropy: 0.445372.\n",
      "Iteration 1016: Policy loss: -0.006014. Value loss: 10.662170. Entropy: 0.470437.\n",
      "Iteration 1017: Policy loss: -0.001328. Value loss: 9.481632. Entropy: 0.425156.\n",
      "episode: 419   score: 210.0  epsilon: 1.0    steps: 16  evaluation reward: 213.05\n",
      "episode: 420   score: 210.0  epsilon: 1.0    steps: 392  evaluation reward: 212.55\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1018: Policy loss: 0.005348. Value loss: 39.130005. Entropy: 0.531737.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1019: Policy loss: 0.008281. Value loss: 24.641596. Entropy: 0.529762.\n",
      "Iteration 1020: Policy loss: 0.004963. Value loss: 21.500324. Entropy: 0.541751.\n",
      "episode: 421   score: 120.0  epsilon: 1.0    steps: 88  evaluation reward: 210.7\n",
      "episode: 422   score: 75.0  epsilon: 1.0    steps: 456  evaluation reward: 210.4\n",
      "episode: 423   score: 210.0  epsilon: 1.0    steps: 608  evaluation reward: 210.15\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1021: Policy loss: 0.011460. Value loss: 38.850407. Entropy: 0.657752.\n",
      "Iteration 1022: Policy loss: 0.027963. Value loss: 26.038754. Entropy: 0.715623.\n",
      "Iteration 1023: Policy loss: 0.004744. Value loss: 20.234985. Entropy: 0.717486.\n",
      "episode: 424   score: 225.0  epsilon: 1.0    steps: 312  evaluation reward: 211.35\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1024: Policy loss: 0.011166. Value loss: 19.514853. Entropy: 0.709197.\n",
      "Iteration 1025: Policy loss: 0.017362. Value loss: 14.028433. Entropy: 0.739572.\n",
      "Iteration 1026: Policy loss: 0.019870. Value loss: 11.944608. Entropy: 0.728825.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1027: Policy loss: 0.010436. Value loss: 25.111366. Entropy: 0.667089.\n",
      "Iteration 1028: Policy loss: 0.006065. Value loss: 17.654873. Entropy: 0.699784.\n",
      "Iteration 1029: Policy loss: 0.007831. Value loss: 15.909187. Entropy: 0.677361.\n",
      "episode: 425   score: 210.0  epsilon: 1.0    steps: 104  evaluation reward: 207.6\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1030: Policy loss: 0.004198. Value loss: 12.422378. Entropy: 0.830330.\n",
      "Iteration 1031: Policy loss: 0.011555. Value loss: 8.068034. Entropy: 0.813873.\n",
      "Iteration 1032: Policy loss: 0.004243. Value loss: 8.257927. Entropy: 0.833228.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1033: Policy loss: 0.011225. Value loss: 20.902945. Entropy: 0.672309.\n",
      "Iteration 1034: Policy loss: 0.004818. Value loss: 14.009964. Entropy: 0.658614.\n",
      "Iteration 1035: Policy loss: 0.008023. Value loss: 11.627085. Entropy: 0.660930.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1036: Policy loss: -0.000874. Value loss: 26.702190. Entropy: 0.479137.\n",
      "Iteration 1037: Policy loss: -0.007545. Value loss: 16.636326. Entropy: 0.475259.\n",
      "Iteration 1038: Policy loss: -0.005489. Value loss: 12.554996. Entropy: 0.475822.\n",
      "episode: 426   score: 180.0  epsilon: 1.0    steps: 40  evaluation reward: 207.9\n",
      "episode: 427   score: 180.0  epsilon: 1.0    steps: 512  evaluation reward: 207.0\n",
      "episode: 428   score: 260.0  epsilon: 1.0    steps: 976  evaluation reward: 207.75\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1039: Policy loss: 0.012350. Value loss: 24.495859. Entropy: 0.494047.\n",
      "Iteration 1040: Policy loss: 0.016377. Value loss: 17.269922. Entropy: 0.471525.\n",
      "Iteration 1041: Policy loss: 0.022450. Value loss: 13.481587. Entropy: 0.454898.\n",
      "episode: 429   score: 210.0  epsilon: 1.0    steps: 120  evaluation reward: 208.0\n",
      "episode: 430   score: 180.0  epsilon: 1.0    steps: 392  evaluation reward: 207.75\n",
      "episode: 431   score: 310.0  epsilon: 1.0    steps: 544  evaluation reward: 207.05\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1042: Policy loss: 0.011151. Value loss: 15.628000. Entropy: 0.575168.\n",
      "Iteration 1043: Policy loss: 0.010751. Value loss: 11.956589. Entropy: 0.604073.\n",
      "Iteration 1044: Policy loss: -0.000227. Value loss: 10.794568. Entropy: 0.612765.\n",
      "episode: 432   score: 335.0  epsilon: 1.0    steps: 904  evaluation reward: 208.05\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1045: Policy loss: 0.013465. Value loss: 34.810783. Entropy: 0.457453.\n",
      "Iteration 1046: Policy loss: 0.022458. Value loss: 28.358183. Entropy: 0.428888.\n",
      "Iteration 1047: Policy loss: 0.014756. Value loss: 25.421122. Entropy: 0.428382.\n",
      "episode: 433   score: 210.0  epsilon: 1.0    steps: 208  evaluation reward: 206.85\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1048: Policy loss: 0.015557. Value loss: 8.407690. Entropy: 0.747750.\n",
      "Iteration 1049: Policy loss: 0.003553. Value loss: 7.777801. Entropy: 0.751035.\n",
      "Iteration 1050: Policy loss: 0.002987. Value loss: 5.960629. Entropy: 0.760530.\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1051: Policy loss: 0.004487. Value loss: 10.675190. Entropy: 0.822739.\n",
      "Iteration 1052: Policy loss: 0.009249. Value loss: 7.456439. Entropy: 0.827056.\n",
      "Iteration 1053: Policy loss: 0.009331. Value loss: 7.538314. Entropy: 0.816652.\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1054: Policy loss: 0.005307. Value loss: 4.505967. Entropy: 0.676924.\n",
      "Iteration 1055: Policy loss: 0.001154. Value loss: 3.682700. Entropy: 0.663203.\n",
      "Iteration 1056: Policy loss: 0.004998. Value loss: 2.582468. Entropy: 0.655295.\n",
      "episode: 434   score: 210.0  epsilon: 1.0    steps: 992  evaluation reward: 206.85\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1057: Policy loss: 0.005520. Value loss: 266.501770. Entropy: 0.516421.\n",
      "Iteration 1058: Policy loss: 0.006642. Value loss: 319.293396. Entropy: 0.444467.\n",
      "Iteration 1059: Policy loss: 0.025129. Value loss: 235.409119. Entropy: 0.454130.\n",
      "episode: 435   score: 210.0  epsilon: 1.0    steps: 240  evaluation reward: 207.3\n",
      "episode: 436   score: 155.0  epsilon: 1.0    steps: 328  evaluation reward: 206.9\n",
      "episode: 437   score: 410.0  epsilon: 1.0    steps: 424  evaluation reward: 208.45\n",
      "episode: 438   score: 210.0  epsilon: 1.0    steps: 872  evaluation reward: 209.5\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1060: Policy loss: 0.011091. Value loss: 26.247850. Entropy: 0.442967.\n",
      "Iteration 1061: Policy loss: 0.012907. Value loss: 18.924665. Entropy: 0.428305.\n",
      "Iteration 1062: Policy loss: 0.003194. Value loss: 17.217636. Entropy: 0.450392.\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1063: Policy loss: 0.003466. Value loss: 234.901642. Entropy: 0.153331.\n",
      "Iteration 1064: Policy loss: 0.005530. Value loss: 175.641403. Entropy: 0.138977.\n",
      "Iteration 1065: Policy loss: 0.003091. Value loss: 153.528625. Entropy: 0.147029.\n",
      "episode: 439   score: 240.0  epsilon: 1.0    steps: 296  evaluation reward: 210.55\n",
      "episode: 440   score: 155.0  epsilon: 1.0    steps: 544  evaluation reward: 210.75\n",
      "episode: 441   score: 410.0  epsilon: 1.0    steps: 712  evaluation reward: 213.8\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1066: Policy loss: 0.005396. Value loss: 29.389091. Entropy: 0.453141.\n",
      "Iteration 1067: Policy loss: 0.003838. Value loss: 13.720774. Entropy: 0.485276.\n",
      "Iteration 1068: Policy loss: -0.000680. Value loss: 15.951225. Entropy: 0.476053.\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1069: Policy loss: 0.002437. Value loss: 9.459462. Entropy: 0.538221.\n",
      "Iteration 1070: Policy loss: 0.018254. Value loss: 5.288941. Entropy: 0.538562.\n",
      "Iteration 1071: Policy loss: 0.014547. Value loss: 3.764679. Entropy: 0.517268.\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1072: Policy loss: 0.000696. Value loss: 10.891624. Entropy: 0.500092.\n",
      "Iteration 1073: Policy loss: -0.002539. Value loss: 8.341526. Entropy: 0.517703.\n",
      "Iteration 1074: Policy loss: 0.003804. Value loss: 7.731602. Entropy: 0.499243.\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1075: Policy loss: 0.000919. Value loss: 25.022034. Entropy: 0.306597.\n",
      "Iteration 1076: Policy loss: 0.001080. Value loss: 11.827523. Entropy: 0.329632.\n",
      "Iteration 1077: Policy loss: -0.000286. Value loss: 10.307797. Entropy: 0.333076.\n",
      "episode: 442   score: 210.0  epsilon: 1.0    steps: 416  evaluation reward: 211.75\n",
      "episode: 443   score: 210.0  epsilon: 1.0    steps: 704  evaluation reward: 212.05\n",
      "episode: 444   score: 180.0  epsilon: 1.0    steps: 728  evaluation reward: 208.95\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1078: Policy loss: 0.014775. Value loss: 28.209852. Entropy: 0.178185.\n",
      "Iteration 1079: Policy loss: 0.005990. Value loss: 18.673634. Entropy: 0.108488.\n",
      "Iteration 1080: Policy loss: 0.002764. Value loss: 17.886314. Entropy: 0.151777.\n",
      "episode: 445   score: 180.0  epsilon: 1.0    steps: 144  evaluation reward: 208.95\n",
      "episode: 446   score: 210.0  epsilon: 1.0    steps: 232  evaluation reward: 209.25\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1081: Policy loss: 0.004747. Value loss: 10.089765. Entropy: 0.233830.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1082: Policy loss: 0.004600. Value loss: 8.894964. Entropy: 0.247328.\n",
      "Iteration 1083: Policy loss: 0.000718. Value loss: 7.543555. Entropy: 0.253873.\n",
      "episode: 447   score: 180.0  epsilon: 1.0    steps: 528  evaluation reward: 209.5\n",
      "episode: 448   score: 210.0  epsilon: 1.0    steps: 912  evaluation reward: 209.5\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1084: Policy loss: -0.000800. Value loss: 27.603811. Entropy: 0.262453.\n",
      "Iteration 1085: Policy loss: -0.000532. Value loss: 19.389933. Entropy: 0.276810.\n",
      "Iteration 1086: Policy loss: -0.000231. Value loss: 18.553757. Entropy: 0.264414.\n",
      "episode: 449   score: 210.0  epsilon: 1.0    steps: 112  evaluation reward: 210.05\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1087: Policy loss: 0.006306. Value loss: 4.664412. Entropy: 0.735198.\n",
      "Iteration 1088: Policy loss: 0.016978. Value loss: 3.250221. Entropy: 0.717981.\n",
      "Iteration 1089: Policy loss: 0.017444. Value loss: 3.046385. Entropy: 0.741153.\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1090: Policy loss: 0.003410. Value loss: 5.196599. Entropy: 0.369755.\n",
      "Iteration 1091: Policy loss: 0.004414. Value loss: 4.658353. Entropy: 0.386650.\n",
      "Iteration 1092: Policy loss: 0.002343. Value loss: 3.764101. Entropy: 0.392282.\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1093: Policy loss: 0.000867. Value loss: 4.485823. Entropy: 0.417770.\n",
      "Iteration 1094: Policy loss: 0.003422. Value loss: 3.292941. Entropy: 0.430823.\n",
      "Iteration 1095: Policy loss: -0.000175. Value loss: 3.489133. Entropy: 0.423661.\n",
      "episode: 450   score: 210.0  epsilon: 1.0    steps: 808  evaluation reward: 209.5\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1096: Policy loss: 0.001037. Value loss: 14.138165. Entropy: 0.248798.\n",
      "Iteration 1097: Policy loss: 0.003925. Value loss: 11.157913. Entropy: 0.233033.\n",
      "Iteration 1098: Policy loss: 0.002494. Value loss: 10.803289. Entropy: 0.239158.\n",
      "now time :  2019-03-05 16:47:13.319145\n",
      "episode: 451   score: 210.0  epsilon: 1.0    steps: 24  evaluation reward: 209.5\n",
      "episode: 452   score: 180.0  epsilon: 1.0    steps: 40  evaluation reward: 209.75\n",
      "episode: 453   score: 180.0  epsilon: 1.0    steps: 544  evaluation reward: 209.45\n",
      "episode: 454   score: 210.0  epsilon: 1.0    steps: 568  evaluation reward: 210.0\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1099: Policy loss: 0.006230. Value loss: 8.278568. Entropy: 0.291519.\n",
      "Iteration 1100: Policy loss: 0.012986. Value loss: 6.908955. Entropy: 0.286046.\n",
      "Iteration 1101: Policy loss: -0.000641. Value loss: 6.311412. Entropy: 0.277648.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1102: Policy loss: 0.002178. Value loss: 19.524622. Entropy: 0.141139.\n",
      "Iteration 1103: Policy loss: -0.001024. Value loss: 16.221130. Entropy: 0.218797.\n",
      "Iteration 1104: Policy loss: -0.001647. Value loss: 17.024414. Entropy: 0.210882.\n",
      "episode: 455   score: 210.0  epsilon: 1.0    steps: 8  evaluation reward: 210.15\n",
      "episode: 456   score: 210.0  epsilon: 1.0    steps: 512  evaluation reward: 209.1\n",
      "episode: 457   score: 180.0  epsilon: 1.0    steps: 584  evaluation reward: 205.85\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1105: Policy loss: 0.000474. Value loss: 11.666589. Entropy: 0.684080.\n",
      "Iteration 1106: Policy loss: -0.000498. Value loss: 11.217244. Entropy: 0.701115.\n",
      "Iteration 1107: Policy loss: -0.001104. Value loss: 10.581250. Entropy: 0.733147.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1108: Policy loss: 0.006571. Value loss: 8.984119. Entropy: 0.410723.\n",
      "Iteration 1109: Policy loss: 0.006055. Value loss: 6.032999. Entropy: 0.419384.\n",
      "Iteration 1110: Policy loss: 0.004050. Value loss: 5.570323. Entropy: 0.391586.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1111: Policy loss: 0.004642. Value loss: 9.628716. Entropy: 0.438368.\n",
      "Iteration 1112: Policy loss: 0.015973. Value loss: 6.768031. Entropy: 0.437540.\n",
      "Iteration 1113: Policy loss: 0.018605. Value loss: 5.836133. Entropy: 0.435451.\n",
      "episode: 458   score: 210.0  epsilon: 1.0    steps: 880  evaluation reward: 205.85\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1114: Policy loss: 0.020183. Value loss: 34.350143. Entropy: 0.444382.\n",
      "Iteration 1115: Policy loss: 0.031376. Value loss: 21.772816. Entropy: 0.464256.\n",
      "Iteration 1116: Policy loss: 0.012211. Value loss: 19.083704. Entropy: 0.454199.\n",
      "episode: 459   score: 210.0  epsilon: 1.0    steps: 72  evaluation reward: 205.15\n",
      "episode: 460   score: 180.0  epsilon: 1.0    steps: 200  evaluation reward: 202.3\n",
      "episode: 461   score: 210.0  epsilon: 1.0    steps: 288  evaluation reward: 202.85\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1117: Policy loss: 0.014448. Value loss: 24.758245. Entropy: 0.501963.\n",
      "Iteration 1118: Policy loss: 0.013742. Value loss: 12.722381. Entropy: 0.524726.\n",
      "Iteration 1119: Policy loss: 0.023012. Value loss: 11.152730. Entropy: 0.554442.\n",
      "episode: 462   score: 255.0  epsilon: 1.0    steps: 592  evaluation reward: 203.3\n",
      "episode: 463   score: 180.0  epsilon: 1.0    steps: 912  evaluation reward: 203.0\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1120: Policy loss: 0.008472. Value loss: 36.558758. Entropy: 0.323635.\n",
      "Iteration 1121: Policy loss: 0.000875. Value loss: 21.008112. Entropy: 0.316158.\n",
      "Iteration 1122: Policy loss: -0.003058. Value loss: 17.320347. Entropy: 0.313486.\n",
      "episode: 464   score: 215.0  epsilon: 1.0    steps: 24  evaluation reward: 203.05\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1123: Policy loss: 0.003195. Value loss: 16.318205. Entropy: 0.647265.\n",
      "Iteration 1124: Policy loss: 0.002682. Value loss: 11.466093. Entropy: 0.640318.\n",
      "Iteration 1125: Policy loss: -0.001040. Value loss: 10.127454. Entropy: 0.633651.\n",
      "episode: 465   score: 290.0  epsilon: 1.0    steps: 968  evaluation reward: 203.85\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1126: Policy loss: 0.001581. Value loss: 9.427030. Entropy: 0.508152.\n",
      "Iteration 1127: Policy loss: -0.002234. Value loss: 8.833118. Entropy: 0.540138.\n",
      "Iteration 1128: Policy loss: 0.001250. Value loss: 8.936255. Entropy: 0.562092.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1129: Policy loss: 0.002477. Value loss: 7.792666. Entropy: 0.516688.\n",
      "Iteration 1130: Policy loss: 0.001013. Value loss: 4.895666. Entropy: 0.495959.\n",
      "Iteration 1131: Policy loss: 0.000221. Value loss: 4.113140. Entropy: 0.504336.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1132: Policy loss: 0.001367. Value loss: 19.714306. Entropy: 0.270396.\n",
      "Iteration 1133: Policy loss: -0.001116. Value loss: 14.112619. Entropy: 0.282864.\n",
      "Iteration 1134: Policy loss: -0.004818. Value loss: 13.623280. Entropy: 0.294966.\n",
      "episode: 466   score: 210.0  epsilon: 1.0    steps: 144  evaluation reward: 204.15\n",
      "episode: 467   score: 210.0  epsilon: 1.0    steps: 368  evaluation reward: 204.7\n",
      "episode: 468   score: 180.0  epsilon: 1.0    steps: 376  evaluation reward: 204.7\n",
      "episode: 469   score: 210.0  epsilon: 1.0    steps: 432  evaluation reward: 205.75\n",
      "episode: 470   score: 180.0  epsilon: 1.0    steps: 848  evaluation reward: 205.45\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1135: Policy loss: 0.016454. Value loss: 19.648727. Entropy: 0.502615.\n",
      "Iteration 1136: Policy loss: 0.014043. Value loss: 12.192160. Entropy: 0.499508.\n",
      "Iteration 1137: Policy loss: 0.001906. Value loss: 10.174377. Entropy: 0.487056.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1138: Policy loss: 0.000979. Value loss: 19.865070. Entropy: 0.214791.\n",
      "Iteration 1139: Policy loss: 0.001545. Value loss: 18.056984. Entropy: 0.213487.\n",
      "Iteration 1140: Policy loss: 0.000760. Value loss: 14.805292. Entropy: 0.204510.\n",
      "episode: 471   score: 180.0  epsilon: 1.0    steps: 232  evaluation reward: 205.15\n",
      "episode: 472   score: 210.0  epsilon: 1.0    steps: 272  evaluation reward: 205.4\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1141: Policy loss: 0.011107. Value loss: 19.713156. Entropy: 0.499785.\n",
      "Iteration 1142: Policy loss: 0.017434. Value loss: 10.992711. Entropy: 0.488033.\n",
      "Iteration 1143: Policy loss: 0.013966. Value loss: 11.142969. Entropy: 0.456820.\n",
      "episode: 473   score: 210.0  epsilon: 1.0    steps: 1000  evaluation reward: 203.4\n",
      "Training network. lr: 0.000242. clip: 0.096627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1144: Policy loss: 0.000031. Value loss: 9.724627. Entropy: 0.266056.\n",
      "Iteration 1145: Policy loss: 0.001835. Value loss: 9.138377. Entropy: 0.293921.\n",
      "Iteration 1146: Policy loss: 0.000192. Value loss: 7.508763. Entropy: 0.302103.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1147: Policy loss: 0.001783. Value loss: 27.587374. Entropy: 0.270442.\n",
      "Iteration 1148: Policy loss: 0.001938. Value loss: 16.241133. Entropy: 0.247648.\n",
      "Iteration 1149: Policy loss: -0.001321. Value loss: 16.412550. Entropy: 0.258554.\n",
      "episode: 474   score: 155.0  epsilon: 1.0    steps: 392  evaluation reward: 203.9\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1150: Policy loss: 0.004844. Value loss: 18.136032. Entropy: 0.221628.\n",
      "Iteration 1151: Policy loss: -0.003109. Value loss: 12.088400. Entropy: 0.303418.\n",
      "Iteration 1152: Policy loss: 0.001605. Value loss: 9.968740. Entropy: 0.266591.\n",
      "episode: 475   score: 210.0  epsilon: 1.0    steps: 680  evaluation reward: 203.9\n",
      "episode: 476   score: 210.0  epsilon: 1.0    steps: 848  evaluation reward: 204.65\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1153: Policy loss: 0.000709. Value loss: 28.814590. Entropy: 0.198369.\n",
      "Iteration 1154: Policy loss: -0.001292. Value loss: 14.259898. Entropy: 0.240889.\n",
      "Iteration 1155: Policy loss: -0.008243. Value loss: 11.408738. Entropy: 0.370881.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1156: Policy loss: 0.000194. Value loss: 20.142605. Entropy: 0.412152.\n",
      "Iteration 1157: Policy loss: 0.017310. Value loss: 12.495436. Entropy: 0.406451.\n",
      "Iteration 1158: Policy loss: 0.007449. Value loss: 10.825613. Entropy: 0.406518.\n",
      "episode: 477   score: 180.0  epsilon: 1.0    steps: 224  evaluation reward: 204.65\n",
      "episode: 478   score: 260.0  epsilon: 1.0    steps: 264  evaluation reward: 203.2\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1159: Policy loss: 0.016723. Value loss: 28.513258. Entropy: 0.534369.\n",
      "Iteration 1160: Policy loss: 0.042626. Value loss: 14.107696. Entropy: 0.514631.\n",
      "Iteration 1161: Policy loss: 0.043223. Value loss: 11.429432. Entropy: 0.507133.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1162: Policy loss: 0.008614. Value loss: 15.525803. Entropy: 0.409495.\n",
      "Iteration 1163: Policy loss: 0.003329. Value loss: 11.398790. Entropy: 0.405043.\n",
      "Iteration 1164: Policy loss: 0.004769. Value loss: 10.872880. Entropy: 0.429609.\n",
      "episode: 479   score: 285.0  epsilon: 1.0    steps: 40  evaluation reward: 205.0\n",
      "episode: 480   score: 180.0  epsilon: 1.0    steps: 216  evaluation reward: 204.7\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1165: Policy loss: -0.000816. Value loss: 17.513599. Entropy: 0.405646.\n",
      "Iteration 1166: Policy loss: -0.000238. Value loss: 8.065222. Entropy: 0.389285.\n",
      "Iteration 1167: Policy loss: -0.002808. Value loss: 7.026708. Entropy: 0.408503.\n",
      "episode: 481   score: 180.0  epsilon: 1.0    steps: 696  evaluation reward: 204.4\n",
      "episode: 482   score: 470.0  epsilon: 1.0    steps: 928  evaluation reward: 207.0\n",
      "episode: 483   score: 105.0  epsilon: 1.0    steps: 936  evaluation reward: 206.5\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1168: Policy loss: 0.006096. Value loss: 33.583836. Entropy: 0.314630.\n",
      "Iteration 1169: Policy loss: -0.001729. Value loss: 16.776628. Entropy: 0.364187.\n",
      "Iteration 1170: Policy loss: -0.008584. Value loss: 16.065441. Entropy: 0.361062.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1171: Policy loss: 0.033513. Value loss: 275.619629. Entropy: 0.509440.\n",
      "Iteration 1172: Policy loss: 0.033759. Value loss: 163.465698. Entropy: 0.460865.\n",
      "Iteration 1173: Policy loss: 0.045021. Value loss: 85.521141. Entropy: 0.461174.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1174: Policy loss: 0.017839. Value loss: 24.367256. Entropy: 0.252812.\n",
      "Iteration 1175: Policy loss: 0.019730. Value loss: 16.699329. Entropy: 0.254720.\n",
      "Iteration 1176: Policy loss: 0.016263. Value loss: 16.244490. Entropy: 0.229168.\n",
      "episode: 484   score: 210.0  epsilon: 1.0    steps: 200  evaluation reward: 206.8\n",
      "episode: 485   score: 210.0  epsilon: 1.0    steps: 224  evaluation reward: 207.85\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1177: Policy loss: 0.000332. Value loss: 12.610755. Entropy: 0.511441.\n",
      "Iteration 1178: Policy loss: -0.002873. Value loss: 8.933787. Entropy: 0.501450.\n",
      "Iteration 1179: Policy loss: -0.002367. Value loss: 6.930037. Entropy: 0.518800.\n",
      "episode: 486   score: 490.0  epsilon: 1.0    steps: 96  evaluation reward: 211.4\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1180: Policy loss: 0.000608. Value loss: 9.984895. Entropy: 0.193918.\n",
      "Iteration 1181: Policy loss: -0.001006. Value loss: 6.614521. Entropy: 0.194062.\n",
      "Iteration 1182: Policy loss: 0.001066. Value loss: 5.630958. Entropy: 0.187452.\n",
      "episode: 487   score: 155.0  epsilon: 1.0    steps: 464  evaluation reward: 210.1\n",
      "episode: 488   score: 180.0  epsilon: 1.0    steps: 480  evaluation reward: 209.8\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1183: Policy loss: 0.004485. Value loss: 36.670795. Entropy: 0.292920.\n",
      "Iteration 1184: Policy loss: 0.005664. Value loss: 18.908249. Entropy: 0.269059.\n",
      "Iteration 1185: Policy loss: 0.001937. Value loss: 14.139581. Entropy: 0.296407.\n",
      "episode: 489   score: 155.0  epsilon: 1.0    steps: 848  evaluation reward: 210.3\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1186: Policy loss: 0.000355. Value loss: 29.683521. Entropy: 0.297691.\n",
      "Iteration 1187: Policy loss: -0.000592. Value loss: 14.929214. Entropy: 0.316431.\n",
      "Iteration 1188: Policy loss: 0.000053. Value loss: 12.481761. Entropy: 0.304816.\n",
      "episode: 490   score: 180.0  epsilon: 1.0    steps: 160  evaluation reward: 210.0\n",
      "episode: 491   score: 180.0  epsilon: 1.0    steps: 192  evaluation reward: 209.7\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1189: Policy loss: -0.001205. Value loss: 6.380584. Entropy: 0.381182.\n",
      "Iteration 1190: Policy loss: -0.005172. Value loss: 3.713734. Entropy: 0.393281.\n",
      "Iteration 1191: Policy loss: -0.007757. Value loss: 3.325284. Entropy: 0.388096.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1192: Policy loss: 0.005744. Value loss: 11.047297. Entropy: 0.256275.\n",
      "Iteration 1193: Policy loss: 0.005353. Value loss: 8.857456. Entropy: 0.250820.\n",
      "Iteration 1194: Policy loss: -0.000096. Value loss: 8.889513. Entropy: 0.260098.\n",
      "episode: 492   score: 180.0  epsilon: 1.0    steps: 608  evaluation reward: 209.4\n",
      "episode: 493   score: 210.0  epsilon: 1.0    steps: 664  evaluation reward: 209.4\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1195: Policy loss: 0.009491. Value loss: 11.668155. Entropy: 0.345584.\n",
      "Iteration 1196: Policy loss: 0.002686. Value loss: 6.633184. Entropy: 0.353817.\n",
      "Iteration 1197: Policy loss: -0.003211. Value loss: 6.682502. Entropy: 0.372103.\n",
      "episode: 494   score: 180.0  epsilon: 1.0    steps: 336  evaluation reward: 209.1\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1198: Policy loss: 0.007050. Value loss: 5.400696. Entropy: 0.172893.\n",
      "Iteration 1199: Policy loss: 0.006894. Value loss: 4.046595. Entropy: 0.171104.\n",
      "Iteration 1200: Policy loss: 0.012684. Value loss: 4.527355. Entropy: 0.161944.\n",
      "episode: 495   score: 180.0  epsilon: 1.0    steps: 808  evaluation reward: 209.35\n",
      "episode: 496   score: 180.0  epsilon: 1.0    steps: 920  evaluation reward: 209.35\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1201: Policy loss: 0.002800. Value loss: 10.706046. Entropy: 0.155890.\n",
      "Iteration 1202: Policy loss: 0.002170. Value loss: 8.069058. Entropy: 0.166802.\n",
      "Iteration 1203: Policy loss: -0.003762. Value loss: 7.083607. Entropy: 0.170428.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1204: Policy loss: 0.002636. Value loss: 9.942476. Entropy: 0.217132.\n",
      "Iteration 1205: Policy loss: -0.000446. Value loss: 5.921167. Entropy: 0.235603.\n",
      "Iteration 1206: Policy loss: 0.000428. Value loss: 6.695603. Entropy: 0.219710.\n",
      "episode: 497   score: 180.0  epsilon: 1.0    steps: 296  evaluation reward: 209.05\n",
      "episode: 498   score: 155.0  epsilon: 1.0    steps: 552  evaluation reward: 208.8\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1207: Policy loss: 0.011576. Value loss: 8.572961. Entropy: 0.142323.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1208: Policy loss: 0.002520. Value loss: 7.138885. Entropy: 0.058099.\n",
      "Iteration 1209: Policy loss: -0.000629. Value loss: 6.534562. Entropy: 0.075460.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1210: Policy loss: -0.000390. Value loss: 6.153535. Entropy: 0.146082.\n",
      "Iteration 1211: Policy loss: 0.001189. Value loss: 4.262347. Entropy: 0.128552.\n",
      "Iteration 1212: Policy loss: 0.000785. Value loss: 3.076364. Entropy: 0.111006.\n",
      "episode: 499   score: 310.0  epsilon: 1.0    steps: 736  evaluation reward: 209.8\n",
      "episode: 500   score: 180.0  epsilon: 1.0    steps: 1008  evaluation reward: 209.5\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1213: Policy loss: 0.006707. Value loss: 7.613293. Entropy: 0.238812.\n",
      "Iteration 1214: Policy loss: 0.006582. Value loss: 6.734486. Entropy: 0.235535.\n",
      "Iteration 1215: Policy loss: 0.005733. Value loss: 6.183196. Entropy: 0.204226.\n",
      "now time :  2019-03-05 16:49:35.334526\n",
      "episode: 501   score: 180.0  epsilon: 1.0    steps: 40  evaluation reward: 209.45\n",
      "episode: 502   score: 180.0  epsilon: 1.0    steps: 808  evaluation reward: 209.15\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1216: Policy loss: 0.000073. Value loss: 3.657166. Entropy: 0.101742.\n",
      "Iteration 1217: Policy loss: 0.003381. Value loss: 2.730857. Entropy: 0.081321.\n",
      "Iteration 1218: Policy loss: 0.001273. Value loss: 2.439249. Entropy: 0.126413.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1219: Policy loss: -0.000591. Value loss: 198.732956. Entropy: 0.049868.\n",
      "Iteration 1220: Policy loss: 0.001170. Value loss: 225.691757. Entropy: 0.028723.\n",
      "Iteration 1221: Policy loss: 0.001190. Value loss: 144.239410. Entropy: 0.023144.\n",
      "episode: 503   score: 355.0  epsilon: 1.0    steps: 328  evaluation reward: 210.6\n",
      "episode: 504   score: 180.0  epsilon: 1.0    steps: 360  evaluation reward: 210.85\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1222: Policy loss: 0.002124. Value loss: 17.773224. Entropy: 0.064609.\n",
      "Iteration 1223: Policy loss: -0.000182. Value loss: 5.926366. Entropy: 0.065057.\n",
      "Iteration 1224: Policy loss: 0.000002. Value loss: 3.591888. Entropy: 0.066430.\n",
      "episode: 505   score: 180.0  epsilon: 1.0    steps: 560  evaluation reward: 210.55\n",
      "episode: 506   score: 180.0  epsilon: 1.0    steps: 704  evaluation reward: 210.55\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1225: Policy loss: 0.007973. Value loss: 8.878105. Entropy: 0.048783.\n",
      "Iteration 1226: Policy loss: 0.004261. Value loss: 7.327640. Entropy: 0.053685.\n",
      "Iteration 1227: Policy loss: 0.008916. Value loss: 7.226751. Entropy: 0.050907.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1228: Policy loss: -0.000008. Value loss: 8.577036. Entropy: 0.032480.\n",
      "Iteration 1229: Policy loss: 0.000065. Value loss: 6.010433. Entropy: 0.033849.\n",
      "Iteration 1230: Policy loss: -0.000275. Value loss: 5.631360. Entropy: 0.041337.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1231: Policy loss: 0.000946. Value loss: 185.380554. Entropy: 0.041313.\n",
      "Iteration 1232: Policy loss: 0.000468. Value loss: 135.330429. Entropy: 0.018542.\n",
      "Iteration 1233: Policy loss: 0.000361. Value loss: 143.953140. Entropy: 0.019439.\n",
      "episode: 507   score: 180.0  epsilon: 1.0    steps: 128  evaluation reward: 210.55\n",
      "episode: 508   score: 180.0  epsilon: 1.0    steps: 456  evaluation reward: 210.25\n",
      "episode: 509   score: 355.0  epsilon: 1.0    steps: 496  evaluation reward: 212.0\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1234: Policy loss: 0.008408. Value loss: 9.644523. Entropy: 0.032963.\n",
      "Iteration 1235: Policy loss: 0.000478. Value loss: 7.380352. Entropy: 0.079500.\n",
      "Iteration 1236: Policy loss: 0.006455. Value loss: 6.317492. Entropy: 0.118686.\n",
      "episode: 510   score: 180.0  epsilon: 1.0    steps: 136  evaluation reward: 212.0\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1237: Policy loss: 0.001700. Value loss: 10.980675. Entropy: 0.114251.\n",
      "Iteration 1238: Policy loss: 0.001437. Value loss: 7.509840. Entropy: 0.119325.\n",
      "Iteration 1239: Policy loss: 0.005696. Value loss: 6.547546. Entropy: 0.114496.\n",
      "episode: 511   score: 180.0  epsilon: 1.0    steps: 768  evaluation reward: 212.25\n",
      "episode: 512   score: 180.0  epsilon: 1.0    steps: 840  evaluation reward: 212.25\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1240: Policy loss: 0.011999. Value loss: 172.297287. Entropy: 0.104792.\n",
      "Iteration 1241: Policy loss: 0.024310. Value loss: 92.750725. Entropy: 0.050698.\n",
      "Iteration 1242: Policy loss: 0.003444. Value loss: 89.531937. Entropy: 0.169010.\n",
      "episode: 513   score: 180.0  epsilon: 1.0    steps: 976  evaluation reward: 213.0\n",
      "episode: 514   score: 355.0  epsilon: 1.0    steps: 984  evaluation reward: 214.45\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1243: Policy loss: 0.011554. Value loss: 10.333506. Entropy: 0.329645.\n",
      "Iteration 1244: Policy loss: 0.026958. Value loss: 6.068892. Entropy: 0.345750.\n",
      "Iteration 1245: Policy loss: 0.010624. Value loss: 5.672263. Entropy: 0.367074.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1246: Policy loss: 0.018981. Value loss: 12.950357. Entropy: 0.242815.\n",
      "Iteration 1247: Policy loss: 0.003386. Value loss: 8.182083. Entropy: 0.220571.\n",
      "Iteration 1248: Policy loss: 0.009757. Value loss: 6.943305. Entropy: 0.234122.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1249: Policy loss: 0.004854. Value loss: 12.530141. Entropy: 0.208257.\n",
      "Iteration 1250: Policy loss: 0.015773. Value loss: 6.984323. Entropy: 0.232248.\n",
      "Iteration 1251: Policy loss: 0.001507. Value loss: 6.567070. Entropy: 0.253028.\n",
      "episode: 515   score: 180.0  epsilon: 1.0    steps: 424  evaluation reward: 214.45\n",
      "episode: 516   score: 180.0  epsilon: 1.0    steps: 632  evaluation reward: 212.7\n",
      "episode: 517   score: 180.0  epsilon: 1.0    steps: 888  evaluation reward: 212.4\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1252: Policy loss: 0.010410. Value loss: 19.294249. Entropy: 0.360447.\n",
      "Iteration 1253: Policy loss: 0.006172. Value loss: 12.282378. Entropy: 0.348882.\n",
      "Iteration 1254: Policy loss: 0.007758. Value loss: 10.407681. Entropy: 0.382087.\n",
      "episode: 518   score: 180.0  epsilon: 1.0    steps: 264  evaluation reward: 211.6\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1255: Policy loss: 0.003995. Value loss: 11.440208. Entropy: 0.287289.\n",
      "Iteration 1256: Policy loss: 0.000289. Value loss: 8.477112. Entropy: 0.273004.\n",
      "Iteration 1257: Policy loss: 0.006934. Value loss: 8.958056. Entropy: 0.271911.\n",
      "episode: 519   score: 210.0  epsilon: 1.0    steps: 800  evaluation reward: 211.6\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1258: Policy loss: 0.005796. Value loss: 23.152594. Entropy: 0.268183.\n",
      "Iteration 1259: Policy loss: 0.002377. Value loss: 17.131203. Entropy: 0.321930.\n",
      "Iteration 1260: Policy loss: 0.003006. Value loss: 13.210308. Entropy: 0.301505.\n",
      "episode: 520   score: 180.0  epsilon: 1.0    steps: 216  evaluation reward: 211.3\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1261: Policy loss: 0.003344. Value loss: 27.692812. Entropy: 0.511204.\n",
      "Iteration 1262: Policy loss: -0.003114. Value loss: 12.271532. Entropy: 0.518115.\n",
      "Iteration 1263: Policy loss: -0.000292. Value loss: 9.222400. Entropy: 0.532692.\n",
      "episode: 521   score: 180.0  epsilon: 1.0    steps: 176  evaluation reward: 211.9\n",
      "episode: 522   score: 215.0  epsilon: 1.0    steps: 608  evaluation reward: 213.3\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1264: Policy loss: 0.014130. Value loss: 6.956901. Entropy: 0.318522.\n",
      "Iteration 1265: Policy loss: 0.019337. Value loss: 4.881751. Entropy: 0.317534.\n",
      "Iteration 1266: Policy loss: 0.002724. Value loss: 4.780932. Entropy: 0.334017.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1267: Policy loss: 0.001949. Value loss: 13.736965. Entropy: 0.378655.\n",
      "Iteration 1268: Policy loss: 0.004969. Value loss: 10.117971. Entropy: 0.365447.\n",
      "Iteration 1269: Policy loss: 0.002904. Value loss: 8.872519. Entropy: 0.368642.\n",
      "episode: 523   score: 75.0  epsilon: 1.0    steps: 496  evaluation reward: 211.95\n",
      "episode: 524   score: 180.0  epsilon: 1.0    steps: 728  evaluation reward: 211.5\n",
      "episode: 525   score: 180.0  epsilon: 1.0    steps: 896  evaluation reward: 211.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1270: Policy loss: 0.031942. Value loss: 32.520691. Entropy: 0.515231.\n",
      "Iteration 1271: Policy loss: 0.051952. Value loss: 20.840885. Entropy: 0.509933.\n",
      "Iteration 1272: Policy loss: 0.059112. Value loss: 16.513941. Entropy: 0.560158.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1273: Policy loss: 0.006697. Value loss: 14.932837. Entropy: 0.417729.\n",
      "Iteration 1274: Policy loss: 0.016607. Value loss: 10.323881. Entropy: 0.427199.\n",
      "Iteration 1275: Policy loss: 0.012288. Value loss: 8.155095. Entropy: 0.445590.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1276: Policy loss: 0.013505. Value loss: 28.463621. Entropy: 0.444217.\n",
      "Iteration 1277: Policy loss: 0.021417. Value loss: 18.786221. Entropy: 0.459084.\n",
      "Iteration 1278: Policy loss: 0.030267. Value loss: 16.643408. Entropy: 0.468520.\n",
      "episode: 526   score: 380.0  epsilon: 1.0    steps: 360  evaluation reward: 213.2\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1279: Policy loss: 0.014385. Value loss: 207.048019. Entropy: 0.550462.\n",
      "Iteration 1280: Policy loss: 0.027058. Value loss: 201.581772. Entropy: 0.483695.\n",
      "Iteration 1281: Policy loss: 0.021058. Value loss: 138.449310. Entropy: 0.518795.\n",
      "episode: 527   score: 175.0  epsilon: 1.0    steps: 520  evaluation reward: 213.15\n",
      "episode: 528   score: 210.0  epsilon: 1.0    steps: 592  evaluation reward: 212.65\n",
      "episode: 529   score: 215.0  epsilon: 1.0    steps: 960  evaluation reward: 212.7\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1282: Policy loss: 0.019680. Value loss: 30.952486. Entropy: 0.372664.\n",
      "Iteration 1283: Policy loss: 0.005172. Value loss: 16.880585. Entropy: 0.372484.\n",
      "Iteration 1284: Policy loss: 0.003210. Value loss: 14.250918. Entropy: 0.397046.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1285: Policy loss: 0.004136. Value loss: 14.647288. Entropy: 0.260834.\n",
      "Iteration 1286: Policy loss: 0.001537. Value loss: 8.775224. Entropy: 0.264288.\n",
      "Iteration 1287: Policy loss: 0.001368. Value loss: 7.470680. Entropy: 0.267904.\n",
      "episode: 530   score: 210.0  epsilon: 1.0    steps: 992  evaluation reward: 213.0\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1288: Policy loss: 0.017161. Value loss: 46.102871. Entropy: 0.340991.\n",
      "Iteration 1289: Policy loss: 0.014499. Value loss: 29.587980. Entropy: 0.347773.\n",
      "Iteration 1290: Policy loss: 0.030207. Value loss: 24.207863. Entropy: 0.349684.\n",
      "episode: 531   score: 180.0  epsilon: 1.0    steps: 80  evaluation reward: 211.7\n",
      "episode: 532   score: 425.0  epsilon: 1.0    steps: 528  evaluation reward: 212.6\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1291: Policy loss: 0.000454. Value loss: 7.692881. Entropy: 0.551698.\n",
      "Iteration 1292: Policy loss: 0.002323. Value loss: 5.372974. Entropy: 0.549601.\n",
      "Iteration 1293: Policy loss: -0.002924. Value loss: 5.698045. Entropy: 0.547834.\n",
      "episode: 533   score: 260.0  epsilon: 1.0    steps: 288  evaluation reward: 213.1\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1294: Policy loss: 0.008345. Value loss: 10.010133. Entropy: 0.280935.\n",
      "Iteration 1295: Policy loss: 0.020555. Value loss: 6.946542. Entropy: 0.225881.\n",
      "Iteration 1296: Policy loss: 0.012881. Value loss: 6.584337. Entropy: 0.255818.\n",
      "episode: 534   score: 210.0  epsilon: 1.0    steps: 856  evaluation reward: 213.1\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1297: Policy loss: 0.010920. Value loss: 19.150621. Entropy: 0.480221.\n",
      "Iteration 1298: Policy loss: 0.013262. Value loss: 12.524166. Entropy: 0.442791.\n",
      "Iteration 1299: Policy loss: 0.003914. Value loss: 11.575281. Entropy: 0.470172.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1300: Policy loss: 0.007496. Value loss: 24.216135. Entropy: 0.455086.\n",
      "Iteration 1301: Policy loss: 0.005998. Value loss: 15.438494. Entropy: 0.510552.\n",
      "Iteration 1302: Policy loss: 0.015614. Value loss: 14.881900. Entropy: 0.504776.\n",
      "episode: 535   score: 180.0  epsilon: 1.0    steps: 208  evaluation reward: 212.8\n",
      "episode: 536   score: 240.0  epsilon: 1.0    steps: 400  evaluation reward: 213.65\n",
      "episode: 537   score: 210.0  epsilon: 1.0    steps: 504  evaluation reward: 211.65\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1303: Policy loss: 0.010808. Value loss: 12.889935. Entropy: 0.443065.\n",
      "Iteration 1304: Policy loss: 0.016785. Value loss: 6.851229. Entropy: 0.417365.\n",
      "Iteration 1305: Policy loss: 0.004821. Value loss: 6.291693. Entropy: 0.376442.\n",
      "episode: 538   score: 105.0  epsilon: 1.0    steps: 96  evaluation reward: 210.6\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1306: Policy loss: 0.004805. Value loss: 13.544850. Entropy: 0.322640.\n",
      "Iteration 1307: Policy loss: 0.002554. Value loss: 9.160683. Entropy: 0.326235.\n",
      "Iteration 1308: Policy loss: 0.001640. Value loss: 6.839337. Entropy: 0.325209.\n",
      "episode: 539   score: 155.0  epsilon: 1.0    steps: 440  evaluation reward: 209.75\n",
      "episode: 540   score: 180.0  epsilon: 1.0    steps: 928  evaluation reward: 210.0\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1309: Policy loss: 0.016538. Value loss: 21.107929. Entropy: 0.466741.\n",
      "Iteration 1310: Policy loss: 0.038287. Value loss: 19.401373. Entropy: 0.485792.\n",
      "Iteration 1311: Policy loss: 0.041544. Value loss: 18.236712. Entropy: 0.504702.\n",
      "episode: 541   score: 210.0  epsilon: 1.0    steps: 776  evaluation reward: 208.0\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1312: Policy loss: 0.007189. Value loss: 10.992102. Entropy: 0.575152.\n",
      "Iteration 1313: Policy loss: 0.012861. Value loss: 6.923382. Entropy: 0.588351.\n",
      "Iteration 1314: Policy loss: 0.018554. Value loss: 7.303408. Entropy: 0.512215.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1315: Policy loss: 0.008800. Value loss: 7.945367. Entropy: 0.349742.\n",
      "Iteration 1316: Policy loss: 0.006096. Value loss: 8.388171. Entropy: 0.350896.\n",
      "Iteration 1317: Policy loss: 0.004189. Value loss: 6.264755. Entropy: 0.336864.\n",
      "episode: 542   score: 180.0  epsilon: 1.0    steps: 88  evaluation reward: 207.7\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1318: Policy loss: 0.002741. Value loss: 4.185416. Entropy: 0.345186.\n",
      "Iteration 1319: Policy loss: 0.002675. Value loss: 2.790606. Entropy: 0.357801.\n",
      "Iteration 1320: Policy loss: -0.001940. Value loss: 2.651148. Entropy: 0.354525.\n",
      "episode: 543   score: 155.0  epsilon: 1.0    steps: 600  evaluation reward: 207.15\n",
      "episode: 544   score: 180.0  epsilon: 1.0    steps: 792  evaluation reward: 207.15\n",
      "episode: 545   score: 180.0  epsilon: 1.0    steps: 800  evaluation reward: 207.15\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1321: Policy loss: 0.017081. Value loss: 13.364365. Entropy: 0.481196.\n",
      "Iteration 1322: Policy loss: 0.007913. Value loss: 9.709725. Entropy: 0.533744.\n",
      "Iteration 1323: Policy loss: 0.001614. Value loss: 9.100764. Entropy: 0.507780.\n",
      "episode: 546   score: 180.0  epsilon: 1.0    steps: 496  evaluation reward: 206.85\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1324: Policy loss: -0.002128. Value loss: 7.700996. Entropy: 0.352889.\n",
      "Iteration 1325: Policy loss: 0.001681. Value loss: 7.223319. Entropy: 0.357825.\n",
      "Iteration 1326: Policy loss: 0.003524. Value loss: 6.533776. Entropy: 0.354742.\n",
      "episode: 547   score: 180.0  epsilon: 1.0    steps: 704  evaluation reward: 206.85\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1327: Policy loss: 0.009477. Value loss: 16.444437. Entropy: 0.380956.\n",
      "Iteration 1328: Policy loss: 0.005596. Value loss: 14.419311. Entropy: 0.388997.\n",
      "Iteration 1329: Policy loss: 0.014511. Value loss: 11.851417. Entropy: 0.388025.\n",
      "episode: 548   score: 210.0  epsilon: 1.0    steps: 328  evaluation reward: 206.85\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1330: Policy loss: 0.011731. Value loss: 7.137008. Entropy: 0.637989.\n",
      "Iteration 1331: Policy loss: 0.014575. Value loss: 4.992136. Entropy: 0.633603.\n",
      "Iteration 1332: Policy loss: 0.004819. Value loss: 5.924232. Entropy: 0.654560.\n",
      "episode: 549   score: 180.0  epsilon: 1.0    steps: 208  evaluation reward: 206.55\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1333: Policy loss: 0.004831. Value loss: 6.087068. Entropy: 0.378045.\n",
      "Iteration 1334: Policy loss: 0.003020. Value loss: 4.043755. Entropy: 0.369059.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1335: Policy loss: 0.007622. Value loss: 4.456735. Entropy: 0.408282.\n",
      "episode: 550   score: 180.0  epsilon: 1.0    steps: 424  evaluation reward: 206.25\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1336: Policy loss: 0.007625. Value loss: 5.191298. Entropy: 0.356188.\n",
      "Iteration 1337: Policy loss: -0.001100. Value loss: 4.368897. Entropy: 0.351298.\n",
      "Iteration 1338: Policy loss: 0.001329. Value loss: 4.378137. Entropy: 0.341249.\n",
      "now time :  2019-03-05 16:52:06.419203\n",
      "episode: 551   score: 210.0  epsilon: 1.0    steps: 984  evaluation reward: 206.25\n",
      "episode: 552   score: 180.0  epsilon: 1.0    steps: 1008  evaluation reward: 206.25\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1339: Policy loss: 0.003849. Value loss: 11.631086. Entropy: 0.384774.\n",
      "Iteration 1340: Policy loss: 0.004064. Value loss: 8.707512. Entropy: 0.381872.\n",
      "Iteration 1341: Policy loss: 0.019512. Value loss: 8.767014. Entropy: 0.409127.\n",
      "episode: 553   score: 180.0  epsilon: 1.0    steps: 160  evaluation reward: 206.25\n",
      "episode: 554   score: 180.0  epsilon: 1.0    steps: 784  evaluation reward: 205.95\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1342: Policy loss: 0.002019. Value loss: 4.285768. Entropy: 0.408965.\n",
      "Iteration 1343: Policy loss: 0.004333. Value loss: 4.027270. Entropy: 0.400878.\n",
      "Iteration 1344: Policy loss: 0.011860. Value loss: 4.179879. Entropy: 0.414788.\n",
      "episode: 555   score: 180.0  epsilon: 1.0    steps: 840  evaluation reward: 205.65\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1345: Policy loss: 0.001866. Value loss: 18.902328. Entropy: 0.292807.\n",
      "Iteration 1346: Policy loss: -0.000024. Value loss: 13.900806. Entropy: 0.313536.\n",
      "Iteration 1347: Policy loss: -0.005352. Value loss: 13.344811. Entropy: 0.341779.\n",
      "episode: 556   score: 210.0  epsilon: 1.0    steps: 800  evaluation reward: 205.65\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1348: Policy loss: 0.015055. Value loss: 8.188507. Entropy: 0.627521.\n",
      "Iteration 1349: Policy loss: 0.016229. Value loss: 7.613947. Entropy: 0.576726.\n",
      "Iteration 1350: Policy loss: 0.011360. Value loss: 5.364295. Entropy: 0.597244.\n",
      "episode: 557   score: 180.0  epsilon: 1.0    steps: 432  evaluation reward: 205.65\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1351: Policy loss: 0.000927. Value loss: 4.289360. Entropy: 0.548995.\n",
      "Iteration 1352: Policy loss: 0.010060. Value loss: 4.325418. Entropy: 0.573411.\n",
      "Iteration 1353: Policy loss: 0.006778. Value loss: 3.077896. Entropy: 0.559153.\n",
      "episode: 558   score: 210.0  epsilon: 1.0    steps: 888  evaluation reward: 205.65\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1354: Policy loss: 0.000450. Value loss: 7.016730. Entropy: 0.455635.\n",
      "Iteration 1355: Policy loss: 0.003044. Value loss: 5.701725. Entropy: 0.431554.\n",
      "Iteration 1356: Policy loss: 0.000599. Value loss: 5.920884. Entropy: 0.445418.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1357: Policy loss: 0.005954. Value loss: 6.973798. Entropy: 0.383975.\n",
      "Iteration 1358: Policy loss: 0.013376. Value loss: 4.514019. Entropy: 0.369638.\n",
      "Iteration 1359: Policy loss: 0.012217. Value loss: 3.517431. Entropy: 0.374477.\n",
      "episode: 559   score: 180.0  epsilon: 1.0    steps: 416  evaluation reward: 205.35\n",
      "episode: 560   score: 180.0  epsilon: 1.0    steps: 432  evaluation reward: 205.35\n",
      "episode: 561   score: 210.0  epsilon: 1.0    steps: 688  evaluation reward: 205.35\n",
      "episode: 562   score: 155.0  epsilon: 1.0    steps: 1008  evaluation reward: 204.35\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1360: Policy loss: 0.012887. Value loss: 14.455922. Entropy: 0.523008.\n",
      "Iteration 1361: Policy loss: 0.014003. Value loss: 11.517676. Entropy: 0.532880.\n",
      "Iteration 1362: Policy loss: 0.004632. Value loss: 12.419347. Entropy: 0.512822.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1363: Policy loss: 0.004633. Value loss: 9.920486. Entropy: 0.356812.\n",
      "Iteration 1364: Policy loss: 0.002727. Value loss: 6.561093. Entropy: 0.356315.\n",
      "Iteration 1365: Policy loss: 0.001636. Value loss: 7.330068. Entropy: 0.333077.\n",
      "episode: 563   score: 210.0  epsilon: 1.0    steps: 440  evaluation reward: 204.65\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1366: Policy loss: 0.008934. Value loss: 10.730618. Entropy: 0.467111.\n",
      "Iteration 1367: Policy loss: 0.011859. Value loss: 9.194984. Entropy: 0.441558.\n",
      "Iteration 1368: Policy loss: 0.003351. Value loss: 10.232147. Entropy: 0.453113.\n",
      "episode: 564   score: 210.0  epsilon: 1.0    steps: 344  evaluation reward: 204.6\n",
      "episode: 565   score: 210.0  epsilon: 1.0    steps: 768  evaluation reward: 203.8\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1369: Policy loss: 0.008551. Value loss: 4.615197. Entropy: 0.661525.\n",
      "Iteration 1370: Policy loss: 0.011407. Value loss: 4.613882. Entropy: 0.654530.\n",
      "Iteration 1371: Policy loss: 0.010873. Value loss: 3.120964. Entropy: 0.655845.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1372: Policy loss: 0.008181. Value loss: 6.904709. Entropy: 0.507677.\n",
      "Iteration 1373: Policy loss: 0.009479. Value loss: 4.169425. Entropy: 0.505659.\n",
      "Iteration 1374: Policy loss: 0.011025. Value loss: 3.936954. Entropy: 0.525865.\n",
      "episode: 566   score: 210.0  epsilon: 1.0    steps: 368  evaluation reward: 203.8\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1375: Policy loss: 0.001135. Value loss: 5.949413. Entropy: 0.453113.\n",
      "Iteration 1376: Policy loss: 0.003913. Value loss: 5.309003. Entropy: 0.469101.\n",
      "Iteration 1377: Policy loss: 0.007122. Value loss: 4.228518. Entropy: 0.480304.\n",
      "episode: 567   score: 180.0  epsilon: 1.0    steps: 736  evaluation reward: 203.5\n",
      "episode: 568   score: 210.0  epsilon: 1.0    steps: 896  evaluation reward: 203.8\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1378: Policy loss: 0.030882. Value loss: 15.692916. Entropy: 0.569014.\n",
      "Iteration 1379: Policy loss: 0.025232. Value loss: 11.001593. Entropy: 0.566493.\n",
      "Iteration 1380: Policy loss: 0.016991. Value loss: 8.365132. Entropy: 0.561204.\n",
      "episode: 569   score: 210.0  epsilon: 1.0    steps: 536  evaluation reward: 203.8\n",
      "episode: 570   score: 180.0  epsilon: 1.0    steps: 760  evaluation reward: 203.8\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1381: Policy loss: 0.038358. Value loss: 9.543394. Entropy: 0.477840.\n",
      "Iteration 1382: Policy loss: 0.050785. Value loss: 4.817222. Entropy: 0.363018.\n",
      "Iteration 1383: Policy loss: 0.025843. Value loss: 3.668809. Entropy: 0.414028.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1384: Policy loss: 0.005045. Value loss: 9.346490. Entropy: 0.249267.\n",
      "Iteration 1385: Policy loss: 0.015843. Value loss: 7.613515. Entropy: 0.240851.\n",
      "Iteration 1386: Policy loss: 0.009161. Value loss: 5.837118. Entropy: 0.252148.\n",
      "episode: 571   score: 180.0  epsilon: 1.0    steps: 816  evaluation reward: 203.8\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1387: Policy loss: 0.004022. Value loss: 14.840679. Entropy: 0.507864.\n",
      "Iteration 1388: Policy loss: 0.005423. Value loss: 11.144915. Entropy: 0.512833.\n",
      "Iteration 1389: Policy loss: 0.002695. Value loss: 8.802808. Entropy: 0.504894.\n",
      "episode: 572   score: 210.0  epsilon: 1.0    steps: 144  evaluation reward: 203.8\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1390: Policy loss: 0.009579. Value loss: 7.928940. Entropy: 0.585801.\n",
      "Iteration 1391: Policy loss: 0.013234. Value loss: 5.205422. Entropy: 0.565156.\n",
      "Iteration 1392: Policy loss: 0.012543. Value loss: 3.725771. Entropy: 0.585671.\n",
      "episode: 573   score: 155.0  epsilon: 1.0    steps: 672  evaluation reward: 203.25\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1393: Policy loss: 0.002588. Value loss: 8.410536. Entropy: 0.479042.\n",
      "Iteration 1394: Policy loss: -0.003250. Value loss: 5.690802. Entropy: 0.492980.\n",
      "Iteration 1395: Policy loss: -0.008954. Value loss: 5.425452. Entropy: 0.506235.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1396: Policy loss: 0.013343. Value loss: 8.314483. Entropy: 0.321883.\n",
      "Iteration 1397: Policy loss: 0.018113. Value loss: 5.444518. Entropy: 0.287638.\n",
      "Iteration 1398: Policy loss: 0.009606. Value loss: 4.352828. Entropy: 0.310159.\n",
      "episode: 574   score: 180.0  epsilon: 1.0    steps: 168  evaluation reward: 203.5\n",
      "episode: 575   score: 285.0  epsilon: 1.0    steps: 192  evaluation reward: 204.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 576   score: 180.0  epsilon: 1.0    steps: 368  evaluation reward: 203.95\n",
      "episode: 577   score: 155.0  epsilon: 1.0    steps: 808  evaluation reward: 203.7\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1399: Policy loss: 0.006111. Value loss: 13.227863. Entropy: 0.619390.\n",
      "Iteration 1400: Policy loss: 0.017048. Value loss: 7.813485. Entropy: 0.608069.\n",
      "Iteration 1401: Policy loss: 0.019759. Value loss: 7.014798. Entropy: 0.619438.\n",
      "episode: 578   score: 210.0  epsilon: 1.0    steps: 192  evaluation reward: 203.2\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1402: Policy loss: 0.004041. Value loss: 6.194780. Entropy: 0.459858.\n",
      "Iteration 1403: Policy loss: 0.008147. Value loss: 4.102281. Entropy: 0.450624.\n",
      "Iteration 1404: Policy loss: 0.006496. Value loss: 4.058338. Entropy: 0.468030.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1405: Policy loss: 0.004336. Value loss: 11.262110. Entropy: 0.415684.\n",
      "Iteration 1406: Policy loss: 0.009344. Value loss: 5.492955. Entropy: 0.417176.\n",
      "Iteration 1407: Policy loss: 0.006283. Value loss: 4.147974. Entropy: 0.441667.\n",
      "episode: 579   score: 180.0  epsilon: 1.0    steps: 352  evaluation reward: 202.15\n",
      "episode: 580   score: 210.0  epsilon: 1.0    steps: 392  evaluation reward: 202.45\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1408: Policy loss: 0.012979. Value loss: 4.109359. Entropy: 0.572868.\n",
      "Iteration 1409: Policy loss: 0.024178. Value loss: 2.938175. Entropy: 0.539465.\n",
      "Iteration 1410: Policy loss: 0.020731. Value loss: 2.563736. Entropy: 0.548681.\n",
      "episode: 581   score: 155.0  epsilon: 1.0    steps: 824  evaluation reward: 202.2\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1411: Policy loss: 0.006809. Value loss: 7.809168. Entropy: 0.618462.\n",
      "Iteration 1412: Policy loss: 0.017325. Value loss: 6.017182. Entropy: 0.625393.\n",
      "Iteration 1413: Policy loss: 0.011686. Value loss: 5.689895. Entropy: 0.605915.\n",
      "episode: 582   score: 105.0  epsilon: 1.0    steps: 984  evaluation reward: 198.55\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1414: Policy loss: 0.013471. Value loss: 9.661618. Entropy: 0.455635.\n",
      "Iteration 1415: Policy loss: 0.009979. Value loss: 7.561418. Entropy: 0.462897.\n",
      "Iteration 1416: Policy loss: 0.006322. Value loss: 6.940989. Entropy: 0.445271.\n",
      "episode: 583   score: 210.0  epsilon: 1.0    steps: 592  evaluation reward: 199.6\n",
      "episode: 584   score: 180.0  epsilon: 1.0    steps: 712  evaluation reward: 199.3\n",
      "episode: 585   score: 180.0  epsilon: 1.0    steps: 992  evaluation reward: 199.0\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1417: Policy loss: 0.027298. Value loss: 11.916851. Entropy: 0.554037.\n",
      "Iteration 1418: Policy loss: 0.033511. Value loss: 9.241159. Entropy: 0.556196.\n",
      "Iteration 1419: Policy loss: 0.027018. Value loss: 6.899818. Entropy: 0.569885.\n",
      "episode: 586   score: 210.0  epsilon: 1.0    steps: 672  evaluation reward: 196.2\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1420: Policy loss: 0.011710. Value loss: 6.658020. Entropy: 0.515968.\n",
      "Iteration 1421: Policy loss: 0.006163. Value loss: 4.818339. Entropy: 0.523862.\n",
      "Iteration 1422: Policy loss: 0.013023. Value loss: 4.147402. Entropy: 0.517536.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1423: Policy loss: 0.014100. Value loss: 148.239044. Entropy: 0.418205.\n",
      "Iteration 1424: Policy loss: 0.011501. Value loss: 32.413605. Entropy: 0.469978.\n",
      "Iteration 1425: Policy loss: 0.010346. Value loss: 40.479527. Entropy: 0.438500.\n",
      "episode: 587   score: 355.0  epsilon: 1.0    steps: 808  evaluation reward: 198.2\n",
      "episode: 588   score: 210.0  epsilon: 1.0    steps: 896  evaluation reward: 198.5\n",
      "episode: 589   score: 105.0  epsilon: 1.0    steps: 976  evaluation reward: 198.0\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1426: Policy loss: 0.009130. Value loss: 15.505995. Entropy: 0.637639.\n",
      "Iteration 1427: Policy loss: 0.004704. Value loss: 8.262136. Entropy: 0.597613.\n",
      "Iteration 1428: Policy loss: -0.000081. Value loss: 6.935108. Entropy: 0.594910.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1429: Policy loss: 0.000203. Value loss: 2.881956. Entropy: 0.602592.\n",
      "Iteration 1430: Policy loss: 0.003818. Value loss: 2.479366. Entropy: 0.579087.\n",
      "Iteration 1431: Policy loss: 0.009161. Value loss: 2.100235. Entropy: 0.597181.\n",
      "episode: 590   score: 105.0  epsilon: 1.0    steps: 792  evaluation reward: 197.25\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1432: Policy loss: 0.000282. Value loss: 16.428629. Entropy: 0.449656.\n",
      "Iteration 1433: Policy loss: -0.001656. Value loss: 13.528691. Entropy: 0.462082.\n",
      "Iteration 1434: Policy loss: -0.004973. Value loss: 11.420339. Entropy: 0.477003.\n",
      "episode: 591   score: 105.0  epsilon: 1.0    steps: 136  evaluation reward: 196.5\n",
      "episode: 592   score: 180.0  epsilon: 1.0    steps: 352  evaluation reward: 196.5\n",
      "episode: 593   score: 105.0  epsilon: 1.0    steps: 816  evaluation reward: 195.45\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1435: Policy loss: 0.009416. Value loss: 8.466851. Entropy: 0.559077.\n",
      "Iteration 1436: Policy loss: 0.012384. Value loss: 5.997489. Entropy: 0.579717.\n",
      "Iteration 1437: Policy loss: 0.010965. Value loss: 4.475079. Entropy: 0.562895.\n",
      "episode: 594   score: 210.0  epsilon: 1.0    steps: 72  evaluation reward: 195.75\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1438: Policy loss: -0.000435. Value loss: 6.059015. Entropy: 0.555702.\n",
      "Iteration 1439: Policy loss: 0.010776. Value loss: 5.736917. Entropy: 0.576406.\n",
      "Iteration 1440: Policy loss: 0.000398. Value loss: 4.677356. Entropy: 0.571208.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1441: Policy loss: 0.008686. Value loss: 8.908972. Entropy: 0.468361.\n",
      "Iteration 1442: Policy loss: 0.002546. Value loss: 6.155387. Entropy: 0.459437.\n",
      "Iteration 1443: Policy loss: 0.007690. Value loss: 5.783928. Entropy: 0.453460.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1444: Policy loss: 0.001659. Value loss: 9.032640. Entropy: 0.410315.\n",
      "Iteration 1445: Policy loss: 0.003973. Value loss: 5.369886. Entropy: 0.425285.\n",
      "Iteration 1446: Policy loss: 0.005519. Value loss: 4.734154. Entropy: 0.408912.\n",
      "episode: 595   score: 210.0  epsilon: 1.0    steps: 264  evaluation reward: 196.05\n",
      "episode: 596   score: 210.0  epsilon: 1.0    steps: 376  evaluation reward: 196.35\n",
      "episode: 597   score: 210.0  epsilon: 1.0    steps: 408  evaluation reward: 196.65\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1447: Policy loss: 0.004702. Value loss: 6.794859. Entropy: 0.588439.\n",
      "Iteration 1448: Policy loss: 0.001732. Value loss: 4.730998. Entropy: 0.584232.\n",
      "Iteration 1449: Policy loss: 0.006083. Value loss: 3.714730. Entropy: 0.584056.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1450: Policy loss: -0.002847. Value loss: 12.651715. Entropy: 0.286453.\n",
      "Iteration 1451: Policy loss: 0.000827. Value loss: 10.372602. Entropy: 0.291635.\n",
      "Iteration 1452: Policy loss: -0.001184. Value loss: 9.344266. Entropy: 0.316693.\n",
      "episode: 598   score: 180.0  epsilon: 1.0    steps: 56  evaluation reward: 196.9\n",
      "episode: 599   score: 155.0  epsilon: 1.0    steps: 320  evaluation reward: 195.35\n",
      "episode: 600   score: 155.0  epsilon: 1.0    steps: 552  evaluation reward: 195.1\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1453: Policy loss: 0.029631. Value loss: 11.983782. Entropy: 0.482870.\n",
      "Iteration 1454: Policy loss: -0.005780. Value loss: 8.365538. Entropy: 0.499567.\n",
      "Iteration 1455: Policy loss: -0.007359. Value loss: 7.743360. Entropy: 0.487728.\n",
      "now time :  2019-03-05 16:54:32.903334\n",
      "episode: 601   score: 185.0  epsilon: 1.0    steps: 56  evaluation reward: 195.15\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1456: Policy loss: 0.015329. Value loss: 8.550979. Entropy: 0.501812.\n",
      "Iteration 1457: Policy loss: 0.031756. Value loss: 6.400784. Entropy: 0.533315.\n",
      "Iteration 1458: Policy loss: 0.030653. Value loss: 5.753612. Entropy: 0.489103.\n",
      "episode: 602   score: 230.0  epsilon: 1.0    steps: 136  evaluation reward: 195.65\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1459: Policy loss: 0.011372. Value loss: 9.580599. Entropy: 0.679613.\n",
      "Iteration 1460: Policy loss: 0.025519. Value loss: 8.422222. Entropy: 0.709589.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1461: Policy loss: 0.023801. Value loss: 7.872222. Entropy: 0.643977.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1462: Policy loss: 0.007948. Value loss: 6.885470. Entropy: 0.470411.\n",
      "Iteration 1463: Policy loss: 0.009963. Value loss: 3.687673. Entropy: 0.436145.\n",
      "Iteration 1464: Policy loss: 0.008817. Value loss: 3.067862. Entropy: 0.427217.\n",
      "episode: 603   score: 180.0  epsilon: 1.0    steps: 584  evaluation reward: 193.9\n",
      "episode: 604   score: 155.0  epsilon: 1.0    steps: 816  evaluation reward: 193.65\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1465: Policy loss: 0.011358. Value loss: 15.361852. Entropy: 0.555090.\n",
      "Iteration 1466: Policy loss: 0.005274. Value loss: 8.272793. Entropy: 0.544823.\n",
      "Iteration 1467: Policy loss: 0.006060. Value loss: 7.026684. Entropy: 0.538951.\n",
      "episode: 605   score: 105.0  epsilon: 1.0    steps: 360  evaluation reward: 192.9\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1468: Policy loss: 0.012736. Value loss: 12.406041. Entropy: 0.354550.\n",
      "Iteration 1469: Policy loss: 0.009738. Value loss: 8.284321. Entropy: 0.335492.\n",
      "Iteration 1470: Policy loss: 0.002580. Value loss: 6.315048. Entropy: 0.342604.\n",
      "episode: 606   score: 135.0  epsilon: 1.0    steps: 544  evaluation reward: 192.45\n",
      "episode: 607   score: 260.0  epsilon: 1.0    steps: 600  evaluation reward: 193.25\n",
      "episode: 608   score: 180.0  epsilon: 1.0    steps: 888  evaluation reward: 193.25\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1471: Policy loss: 0.001427. Value loss: 19.091843. Entropy: 0.428024.\n",
      "Iteration 1472: Policy loss: 0.019715. Value loss: 11.096966. Entropy: 0.414684.\n",
      "Iteration 1473: Policy loss: 0.003505. Value loss: 9.103566. Entropy: 0.410644.\n",
      "episode: 609   score: 180.0  epsilon: 1.0    steps: 464  evaluation reward: 191.5\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1474: Policy loss: 0.013583. Value loss: 15.218987. Entropy: 0.437698.\n",
      "Iteration 1475: Policy loss: 0.008391. Value loss: 6.502042. Entropy: 0.419607.\n",
      "Iteration 1476: Policy loss: 0.017754. Value loss: 4.452771. Entropy: 0.426997.\n",
      "episode: 610   score: 180.0  epsilon: 1.0    steps: 528  evaluation reward: 191.5\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1477: Policy loss: 0.007121. Value loss: 10.321670. Entropy: 0.591602.\n",
      "Iteration 1478: Policy loss: 0.007602. Value loss: 6.719767. Entropy: 0.613618.\n",
      "Iteration 1479: Policy loss: 0.003108. Value loss: 6.267105. Entropy: 0.589821.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1480: Policy loss: 0.000255. Value loss: 5.526323. Entropy: 0.568744.\n",
      "Iteration 1481: Policy loss: 0.006822. Value loss: 3.967915. Entropy: 0.579740.\n",
      "Iteration 1482: Policy loss: 0.004669. Value loss: 3.196924. Entropy: 0.569904.\n",
      "episode: 611   score: 210.0  epsilon: 1.0    steps: 928  evaluation reward: 191.8\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1483: Policy loss: 0.005446. Value loss: 13.625104. Entropy: 0.473731.\n",
      "Iteration 1484: Policy loss: 0.002589. Value loss: 7.300751. Entropy: 0.487981.\n",
      "Iteration 1485: Policy loss: 0.005081. Value loss: 7.644757. Entropy: 0.486191.\n",
      "episode: 612   score: 155.0  epsilon: 1.0    steps: 48  evaluation reward: 191.55\n",
      "episode: 613   score: 155.0  epsilon: 1.0    steps: 728  evaluation reward: 191.3\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1486: Policy loss: 0.007705. Value loss: 4.116240. Entropy: 0.471544.\n",
      "Iteration 1487: Policy loss: 0.001545. Value loss: 2.750271. Entropy: 0.466685.\n",
      "Iteration 1488: Policy loss: 0.004264. Value loss: 2.689477. Entropy: 0.461182.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1489: Policy loss: -0.000009. Value loss: 18.606524. Entropy: 0.260249.\n",
      "Iteration 1490: Policy loss: 0.001035. Value loss: 13.412476. Entropy: 0.275621.\n",
      "Iteration 1491: Policy loss: 0.000586. Value loss: 12.787539. Entropy: 0.279589.\n",
      "episode: 614   score: 210.0  epsilon: 1.0    steps: 112  evaluation reward: 189.85\n",
      "episode: 615   score: 210.0  epsilon: 1.0    steps: 184  evaluation reward: 190.15\n",
      "episode: 616   score: 180.0  epsilon: 1.0    steps: 200  evaluation reward: 190.15\n",
      "episode: 617   score: 155.0  epsilon: 1.0    steps: 768  evaluation reward: 189.9\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1492: Policy loss: -0.001588. Value loss: 9.430079. Entropy: 0.564405.\n",
      "Iteration 1493: Policy loss: -0.002151. Value loss: 4.880742. Entropy: 0.559945.\n",
      "Iteration 1494: Policy loss: -0.002845. Value loss: 6.413713. Entropy: 0.559408.\n",
      "episode: 618   score: 185.0  epsilon: 1.0    steps: 744  evaluation reward: 189.95\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1495: Policy loss: 0.001939. Value loss: 9.623029. Entropy: 0.489105.\n",
      "Iteration 1496: Policy loss: 0.006518. Value loss: 7.877315. Entropy: 0.489928.\n",
      "Iteration 1497: Policy loss: 0.006338. Value loss: 6.445085. Entropy: 0.493375.\n",
      "episode: 619   score: 105.0  epsilon: 1.0    steps: 904  evaluation reward: 188.9\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1498: Policy loss: 0.007489. Value loss: 9.210834. Entropy: 0.550256.\n",
      "Iteration 1499: Policy loss: 0.012947. Value loss: 6.350709. Entropy: 0.551529.\n",
      "Iteration 1500: Policy loss: 0.013670. Value loss: 7.504969. Entropy: 0.536743.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1501: Policy loss: 0.007078. Value loss: 4.298943. Entropy: 0.578624.\n",
      "Iteration 1502: Policy loss: -0.000698. Value loss: 2.555051. Entropy: 0.571021.\n",
      "Iteration 1503: Policy loss: 0.000487. Value loss: 1.799833. Entropy: 0.543982.\n",
      "episode: 620   score: 180.0  epsilon: 1.0    steps: 624  evaluation reward: 188.9\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1504: Policy loss: 0.000920. Value loss: 10.204855. Entropy: 0.391608.\n",
      "Iteration 1505: Policy loss: -0.001222. Value loss: 7.467279. Entropy: 0.424326.\n",
      "Iteration 1506: Policy loss: -0.003048. Value loss: 6.009618. Entropy: 0.408268.\n",
      "episode: 621   score: 210.0  epsilon: 1.0    steps: 168  evaluation reward: 189.2\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1507: Policy loss: 0.004847. Value loss: 3.001127. Entropy: 0.332941.\n",
      "Iteration 1508: Policy loss: 0.002678. Value loss: 2.668075. Entropy: 0.311087.\n",
      "Iteration 1509: Policy loss: 0.005382. Value loss: 2.492985. Entropy: 0.313994.\n",
      "episode: 622   score: 180.0  epsilon: 1.0    steps: 384  evaluation reward: 188.85\n",
      "episode: 623   score: 180.0  epsilon: 1.0    steps: 552  evaluation reward: 189.9\n",
      "episode: 624   score: 210.0  epsilon: 1.0    steps: 576  evaluation reward: 190.2\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1510: Policy loss: -0.001772. Value loss: 14.390171. Entropy: 0.322213.\n",
      "Iteration 1511: Policy loss: -0.001658. Value loss: 12.085365. Entropy: 0.326641.\n",
      "Iteration 1512: Policy loss: -0.002468. Value loss: 11.972018. Entropy: 0.342056.\n",
      "episode: 625   score: 180.0  epsilon: 1.0    steps: 640  evaluation reward: 190.2\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1513: Policy loss: 0.016607. Value loss: 10.063553. Entropy: 0.401446.\n",
      "Iteration 1514: Policy loss: 0.021948. Value loss: 4.929251. Entropy: 0.410364.\n",
      "Iteration 1515: Policy loss: 0.001762. Value loss: 5.530278. Entropy: 0.394925.\n",
      "episode: 626   score: 105.0  epsilon: 1.0    steps: 24  evaluation reward: 187.45\n",
      "episode: 627   score: 210.0  epsilon: 1.0    steps: 280  evaluation reward: 187.8\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1516: Policy loss: 0.002023. Value loss: 5.585076. Entropy: 0.525864.\n",
      "Iteration 1517: Policy loss: 0.008855. Value loss: 4.089993. Entropy: 0.532977.\n",
      "Iteration 1518: Policy loss: 0.014005. Value loss: 3.868490. Entropy: 0.540373.\n",
      "episode: 628   score: 105.0  epsilon: 1.0    steps: 680  evaluation reward: 186.75\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1519: Policy loss: 0.013545. Value loss: 9.315966. Entropy: 0.502990.\n",
      "Iteration 1520: Policy loss: 0.000902. Value loss: 6.289986. Entropy: 0.517547.\n",
      "Iteration 1521: Policy loss: 0.003391. Value loss: 6.022889. Entropy: 0.486772.\n",
      "episode: 629   score: 105.0  epsilon: 1.0    steps: 304  evaluation reward: 185.65\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1522: Policy loss: -0.001913. Value loss: 6.433477. Entropy: 0.450177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1523: Policy loss: 0.000512. Value loss: 5.604450. Entropy: 0.479644.\n",
      "Iteration 1524: Policy loss: 0.006678. Value loss: 4.641637. Entropy: 0.482540.\n",
      "episode: 630   score: 105.0  epsilon: 1.0    steps: 832  evaluation reward: 184.6\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1525: Policy loss: 0.000615. Value loss: 10.101652. Entropy: 0.416528.\n",
      "Iteration 1526: Policy loss: -0.001112. Value loss: 6.873206. Entropy: 0.444082.\n",
      "Iteration 1527: Policy loss: -0.000096. Value loss: 5.802977. Entropy: 0.448738.\n",
      "episode: 631   score: 180.0  epsilon: 1.0    steps: 680  evaluation reward: 184.6\n",
      "episode: 632   score: 210.0  epsilon: 1.0    steps: 968  evaluation reward: 182.45\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1528: Policy loss: 0.005733. Value loss: 8.595915. Entropy: 0.363086.\n",
      "Iteration 1529: Policy loss: 0.002390. Value loss: 6.275974. Entropy: 0.376746.\n",
      "Iteration 1530: Policy loss: 0.001315. Value loss: 5.236938. Entropy: 0.377746.\n",
      "episode: 633   score: 180.0  epsilon: 1.0    steps: 960  evaluation reward: 181.65\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1531: Policy loss: 0.006276. Value loss: 9.151414. Entropy: 0.390954.\n",
      "Iteration 1532: Policy loss: 0.004711. Value loss: 6.810357. Entropy: 0.425137.\n",
      "Iteration 1533: Policy loss: 0.001673. Value loss: 6.777676. Entropy: 0.388500.\n",
      "episode: 634   score: 180.0  epsilon: 1.0    steps: 360  evaluation reward: 181.35\n",
      "episode: 635   score: 210.0  epsilon: 1.0    steps: 808  evaluation reward: 181.65\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1534: Policy loss: 0.004867. Value loss: 10.747918. Entropy: 0.459118.\n",
      "Iteration 1535: Policy loss: -0.001162. Value loss: 8.985255. Entropy: 0.470867.\n",
      "Iteration 1536: Policy loss: -0.000569. Value loss: 7.637801. Entropy: 0.513106.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1537: Policy loss: 0.003706. Value loss: 8.082706. Entropy: 0.400602.\n",
      "Iteration 1538: Policy loss: 0.003936. Value loss: 6.978276. Entropy: 0.393609.\n",
      "Iteration 1539: Policy loss: 0.002250. Value loss: 6.436073. Entropy: 0.392469.\n",
      "episode: 636   score: 185.0  epsilon: 1.0    steps: 48  evaluation reward: 181.1\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1540: Policy loss: 0.001299. Value loss: 7.804110. Entropy: 0.494915.\n",
      "Iteration 1541: Policy loss: -0.000559. Value loss: 5.176610. Entropy: 0.529386.\n",
      "Iteration 1542: Policy loss: -0.001620. Value loss: 6.307529. Entropy: 0.519079.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1543: Policy loss: 0.000778. Value loss: 12.702341. Entropy: 0.477463.\n",
      "Iteration 1544: Policy loss: -0.001894. Value loss: 6.324131. Entropy: 0.487496.\n",
      "Iteration 1545: Policy loss: -0.001048. Value loss: 6.250760. Entropy: 0.484238.\n",
      "episode: 637   score: 180.0  epsilon: 1.0    steps: 160  evaluation reward: 180.8\n",
      "episode: 638   score: 210.0  epsilon: 1.0    steps: 904  evaluation reward: 181.85\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1546: Policy loss: 0.002708. Value loss: 9.127647. Entropy: 0.607721.\n",
      "Iteration 1547: Policy loss: 0.003556. Value loss: 6.736168. Entropy: 0.598145.\n",
      "Iteration 1548: Policy loss: 0.000527. Value loss: 5.424112. Entropy: 0.613315.\n",
      "episode: 639   score: 155.0  epsilon: 1.0    steps: 184  evaluation reward: 181.85\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1549: Policy loss: 0.002351. Value loss: 5.758720. Entropy: 0.432781.\n",
      "Iteration 1550: Policy loss: 0.006263. Value loss: 3.716302. Entropy: 0.410447.\n",
      "Iteration 1551: Policy loss: -0.002214. Value loss: 3.871588. Entropy: 0.436864.\n",
      "episode: 640   score: 155.0  epsilon: 1.0    steps: 672  evaluation reward: 181.6\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1552: Policy loss: 0.001905. Value loss: 24.624002. Entropy: 0.409025.\n",
      "Iteration 1553: Policy loss: 0.003759. Value loss: 14.657755. Entropy: 0.442815.\n",
      "Iteration 1554: Policy loss: -0.001956. Value loss: 12.008945. Entropy: 0.436414.\n",
      "episode: 641   score: 345.0  epsilon: 1.0    steps: 40  evaluation reward: 182.95\n",
      "episode: 642   score: 135.0  epsilon: 1.0    steps: 128  evaluation reward: 182.5\n",
      "episode: 643   score: 265.0  epsilon: 1.0    steps: 400  evaluation reward: 183.6\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1555: Policy loss: 0.009004. Value loss: 224.939957. Entropy: 0.598000.\n",
      "Iteration 1556: Policy loss: 0.019989. Value loss: 94.536217. Entropy: 0.595269.\n",
      "Iteration 1557: Policy loss: 0.008173. Value loss: 77.046280. Entropy: 0.614698.\n",
      "episode: 644   score: 410.0  epsilon: 1.0    steps: 288  evaluation reward: 185.9\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1558: Policy loss: 0.018643. Value loss: 12.742417. Entropy: 0.590595.\n",
      "Iteration 1559: Policy loss: 0.019449. Value loss: 8.868725. Entropy: 0.574261.\n",
      "Iteration 1560: Policy loss: 0.009267. Value loss: 8.753104. Entropy: 0.582788.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1561: Policy loss: 0.006444. Value loss: 36.500538. Entropy: 0.601138.\n",
      "Iteration 1562: Policy loss: 0.013289. Value loss: 18.735720. Entropy: 0.557156.\n",
      "Iteration 1563: Policy loss: 0.014329. Value loss: 16.504004. Entropy: 0.612393.\n",
      "episode: 645   score: 220.0  epsilon: 1.0    steps: 704  evaluation reward: 186.3\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1564: Policy loss: 0.021916. Value loss: 40.910748. Entropy: 0.679400.\n",
      "Iteration 1565: Policy loss: 0.037830. Value loss: 13.403897. Entropy: 0.664327.\n",
      "Iteration 1566: Policy loss: 0.041045. Value loss: 9.217219. Entropy: 0.664647.\n",
      "episode: 646   score: 210.0  epsilon: 1.0    steps: 488  evaluation reward: 186.6\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1567: Policy loss: 0.002158. Value loss: 25.605251. Entropy: 0.308712.\n",
      "Iteration 1568: Policy loss: 0.002084. Value loss: 12.327283. Entropy: 0.311109.\n",
      "Iteration 1569: Policy loss: 0.000349. Value loss: 9.735222. Entropy: 0.308079.\n",
      "episode: 647   score: 255.0  epsilon: 1.0    steps: 488  evaluation reward: 187.35\n",
      "episode: 648   score: 185.0  epsilon: 1.0    steps: 872  evaluation reward: 187.1\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1570: Policy loss: 0.006147. Value loss: 38.934624. Entropy: 0.243283.\n",
      "Iteration 1571: Policy loss: 0.001767. Value loss: 22.829866. Entropy: 0.247828.\n",
      "Iteration 1572: Policy loss: 0.000095. Value loss: 18.687469. Entropy: 0.274649.\n",
      "episode: 649   score: 210.0  epsilon: 1.0    steps: 88  evaluation reward: 187.4\n",
      "episode: 650   score: 180.0  epsilon: 1.0    steps: 776  evaluation reward: 187.4\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1573: Policy loss: -0.003533. Value loss: 25.734678. Entropy: 0.396457.\n",
      "Iteration 1574: Policy loss: -0.007577. Value loss: 14.008543. Entropy: 0.380818.\n",
      "Iteration 1575: Policy loss: -0.008637. Value loss: 10.819755. Entropy: 0.391920.\n",
      "now time :  2019-03-05 16:56:55.220638\n",
      "episode: 651   score: 260.0  epsilon: 1.0    steps: 328  evaluation reward: 187.9\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1576: Policy loss: 0.003898. Value loss: 23.047142. Entropy: 0.426618.\n",
      "Iteration 1577: Policy loss: 0.002840. Value loss: 12.708250. Entropy: 0.421507.\n",
      "Iteration 1578: Policy loss: 0.013410. Value loss: 10.484195. Entropy: 0.427436.\n",
      "episode: 652   score: 105.0  epsilon: 1.0    steps: 768  evaluation reward: 187.15\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1579: Policy loss: 0.005475. Value loss: 16.317339. Entropy: 0.564720.\n",
      "Iteration 1580: Policy loss: 0.005271. Value loss: 9.563462. Entropy: 0.609838.\n",
      "Iteration 1581: Policy loss: -0.005413. Value loss: 8.196161. Entropy: 0.602424.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1582: Policy loss: 0.005890. Value loss: 6.317990. Entropy: 0.509618.\n",
      "Iteration 1583: Policy loss: 0.003979. Value loss: 4.174840. Entropy: 0.523756.\n",
      "Iteration 1584: Policy loss: 0.006248. Value loss: 3.768777. Entropy: 0.502009.\n",
      "episode: 653   score: 210.0  epsilon: 1.0    steps: 504  evaluation reward: 187.45\n",
      "episode: 654   score: 180.0  epsilon: 1.0    steps: 912  evaluation reward: 187.45\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1585: Policy loss: 0.004653. Value loss: 28.705805. Entropy: 0.305655.\n",
      "Iteration 1586: Policy loss: 0.002686. Value loss: 22.057293. Entropy: 0.311245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1587: Policy loss: 0.001876. Value loss: 17.029335. Entropy: 0.314574.\n",
      "episode: 655   score: 210.0  epsilon: 1.0    steps: 272  evaluation reward: 187.75\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1588: Policy loss: 0.002437. Value loss: 18.724773. Entropy: 0.311004.\n",
      "Iteration 1589: Policy loss: 0.002236. Value loss: 9.408863. Entropy: 0.323500.\n",
      "Iteration 1590: Policy loss: -0.002275. Value loss: 6.129225. Entropy: 0.308942.\n",
      "episode: 656   score: 105.0  epsilon: 1.0    steps: 368  evaluation reward: 186.7\n",
      "episode: 657   score: 155.0  epsilon: 1.0    steps: 400  evaluation reward: 186.45\n",
      "episode: 658   score: 485.0  epsilon: 1.0    steps: 984  evaluation reward: 189.2\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1591: Policy loss: 0.000933. Value loss: 31.185831. Entropy: 0.429799.\n",
      "Iteration 1592: Policy loss: -0.006547. Value loss: 19.621401. Entropy: 0.475382.\n",
      "Iteration 1593: Policy loss: -0.002770. Value loss: 18.094185. Entropy: 0.449148.\n",
      "episode: 659   score: 240.0  epsilon: 1.0    steps: 464  evaluation reward: 189.8\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1594: Policy loss: 0.007582. Value loss: 7.609912. Entropy: 0.663346.\n",
      "Iteration 1595: Policy loss: 0.026644. Value loss: 5.303950. Entropy: 0.670983.\n",
      "Iteration 1596: Policy loss: 0.015786. Value loss: 4.652823. Entropy: 0.672462.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1597: Policy loss: 0.007749. Value loss: 20.717182. Entropy: 0.656443.\n",
      "Iteration 1598: Policy loss: 0.021186. Value loss: 14.070200. Entropy: 0.657442.\n",
      "Iteration 1599: Policy loss: 0.024421. Value loss: 12.722598. Entropy: 0.640560.\n",
      "episode: 660   score: 210.0  epsilon: 1.0    steps: 56  evaluation reward: 190.1\n",
      "episode: 661   score: 180.0  epsilon: 1.0    steps: 896  evaluation reward: 189.8\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1600: Policy loss: 0.017138. Value loss: 8.604536. Entropy: 0.697069.\n",
      "Iteration 1601: Policy loss: 0.014678. Value loss: 4.979880. Entropy: 0.721596.\n",
      "Iteration 1602: Policy loss: 0.005424. Value loss: 4.006282. Entropy: 0.728363.\n",
      "episode: 662   score: 180.0  epsilon: 1.0    steps: 984  evaluation reward: 190.05\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1603: Policy loss: 0.037823. Value loss: 31.670176. Entropy: 0.434620.\n",
      "Iteration 1604: Policy loss: 0.020090. Value loss: 15.297092. Entropy: 0.282897.\n"
     ]
    }
   ],
   "source": [
    "### Loop through all environments and run PPO on them\n",
    "\n",
    "env_names = ['SpaceInvaders-v0', 'MsPacman-v0', 'Asteroids-v0', 'Asterix-v0', 'Atlantis-v0', 'Alien-v0', 'Amidar-v0', 'Assault-v0', 'BankHeist-v0']\n",
    "\n",
    "for a in range(len(env_names)):\n",
    "    name = env_names[a]\n",
    "    print(\"\\n\\n\\n ------- STARTING TRAINING FOR %s ------- \\n\\n\\n\" % (name))\n",
    "    \n",
    "    envs = []\n",
    "    for i in range(num_envs):\n",
    "        envs.append(GameEnv(name))\n",
    "    #env.render()\n",
    "    \n",
    "\n",
    "    number_lives = envs[0].life\n",
    "    state_size = envs[0].observation_space.shape\n",
    "    if (name == 'SpaceInvaders-v0'):\n",
    "        action_size = 4\n",
    "    else:\n",
    "        action_size = envs[0].action_space.n\n",
    "    rewards, episodes = [], []\n",
    "\n",
    "    vis_env_idx = 0\n",
    "    vis_env = envs[vis_env_idx]\n",
    "    e = 0\n",
    "    frame = 0\n",
    "    max_eval = -np.inf\n",
    "    reset_count = 0\n",
    "\n",
    "\n",
    "    agent = Agent(action_size)\n",
    "    torch.save(agent.policy_net.state_dict(), \"./save_model/\" + name + \"_best\")\n",
    "    evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "    frame = 0\n",
    "    memory_size = 0\n",
    "    reset_max = 10\n",
    "    \n",
    "    print(\"Determing min/max rewards of environment\")\n",
    "    [low, high] = score_range = get_score_range(name)\n",
    "    print(\"Min: %d. Max: %d.\" % (low, high))\n",
    "\n",
    "    while (frame < 10000000):\n",
    "        step = 0\n",
    "        assert(num_envs * env_mem_size == train_frame)\n",
    "        frame_next_vals = []\n",
    "        \n",
    "        for j in range(env_mem_size):\n",
    "            \n",
    "            curr_states = np.stack([envs[i].history[HISTORY_SIZE-1,:,:] for i in range(num_envs)])\n",
    "            next_states = []\n",
    "            net_in = np.stack([envs[i].history[:HISTORY_SIZE,:,:] for i in range(num_envs)])\n",
    "            step += num_envs\n",
    "            frame += num_envs\n",
    "            actions, values = agent.get_action(np.float32(net_in) / 255.)\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                next_states.append(next_state)\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                \"\"\"\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "                \"\"\"\n",
    "                \n",
    "                frame_next_state = get_frame(next_states[i])\n",
    "                env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "                terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "                env.life = env.info['ale.lives']\n",
    "                r = (env.reward / high) * 20.0 #np.log(max(env.reward+1, 1))#((env.reward - low) / (high - low)) * 30\n",
    "                agent.memory.push(i, deepcopy(curr_states[i]), actions[i], r, terminal_state, values[i], 0, 0)\n",
    "                \n",
    "                if (j == env_mem_size-1):\n",
    "                    net_in = np.stack([envs[k].history[1:,:,:] for k in range(num_envs)])\n",
    "                    _, frame_next_vals = agent.get_action(np.float32(net_in) / 255.)\n",
    "                \n",
    "                env.score += env.reward\n",
    "                env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "        \n",
    "                if (env.done):\n",
    "                    if (e % 50 == 0):\n",
    "                        print('now time : ', datetime.now())\n",
    "                        rewards.append(np.mean(evaluation_reward))\n",
    "                        episodes.append(e)\n",
    "                        pylab.plot(episodes, rewards, 'b')\n",
    "                        pylab.savefig(\"./save_graph/\" + name + \"_ppo.png\")\n",
    "                        torch.save(agent.policy_net, \"./save_model/\" + name + \"_ppo\")\n",
    "\n",
    "                        if np.mean(evaluation_reward) > max_eval:\n",
    "                            torch.save(agent.policy_net.state_dict(), \"./save_model/\"  + name + \"_ppo_best\")\n",
    "                            max_eval = float(np.mean(evaluation_reward))\n",
    "                            reset_count = 0\n",
    "                        elif e > 5000:\n",
    "                            reset_count += 1\n",
    "                            \"\"\"\n",
    "                            if (reset_count == reset_max):\n",
    "                                print(\"Training went nowhere, starting again at best model\")\n",
    "                                agent.policy_net.load_state_dict(torch.load(\"./save_model/spaceinvaders_ppo_best\"))\n",
    "                                agent.update_target_net()\n",
    "                                reset_count = 0\n",
    "                            \"\"\"\n",
    "                    e += 1\n",
    "                    evaluation_reward.append(env.score)\n",
    "                    print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                      \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "                    env.done = False\n",
    "                    env.score = 0\n",
    "                    env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "                    env.state = env.reset()\n",
    "                    env.life = number_lives\n",
    "                    get_init_state(env.history, env.state)\n",
    "            \n",
    "        agent.train_policy_net(frame, frame_next_vals)\n",
    "        agent.update_target_net()\n",
    "    print(\"FINISHED TRAINING FOR %s\" % (name))\n",
    "    pylab.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
