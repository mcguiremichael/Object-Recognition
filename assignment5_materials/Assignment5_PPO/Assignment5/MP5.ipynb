{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.backends.cudnn.benchmarks = True\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "from env import GameEnv\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = []\n",
    "for i in range(num_envs):\n",
    "    envs.append(GameEnv('SpaceInvadersDeterministic-v4'))\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = envs[0].life\n",
    "state_size = envs[0].observation_space.shape\n",
    "action_size = envs[0].action_space.n\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best(name):\n",
    "    env = GameEnv(name)\n",
    "    print(\"\\n\\n\\n ------- TESTING BEST MODEL FOR %s ------- \\n\\n\\n\" % (name))\n",
    "    number_lives = env.life\n",
    "    \n",
    "    if (name == 'SpaceInvaders-v0'):\n",
    "        action_size = 4\n",
    "    else:\n",
    "        action_size = env.action_space.n\n",
    "    rewards, episodes = [], []\n",
    "    \n",
    "    e = 0\n",
    "    frame = 0\n",
    "\n",
    "    agent = Agent(action_size)\n",
    "    agent.policy_net.load_state_dict(torch.load(\"./save_model/\" + name + \"_ppo_best\"))\n",
    "    agent.update_target_net()\n",
    "    agent.policy_net.eval()\n",
    "    evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "\n",
    "    for i in range(100):\n",
    "        env.done = False\n",
    "        env.score = 0\n",
    "        env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "        env.state = env.reset()\n",
    "        env.life = number_lives\n",
    "        get_init_state(env.history, env.state)\n",
    "        step = 0\n",
    "        while not env.done:\n",
    "            curr_state = env.history[HISTORY_SIZE-1,:,:]\n",
    "            net_in = env.history[:HISTORY_SIZE,:,:]\n",
    "            action, value, _ = agent.get_action(np.float32(net_in) / 255.)\n",
    "            \n",
    "            next_state, env.reward, env.done, env.info = env.step(action)\n",
    "            env._env.render()\n",
    "            \n",
    "            frame_next_state = get_frame(next_state)\n",
    "            \n",
    "            env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "            terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "            env.life = env.info['ale.lives']\n",
    "            \n",
    "            \n",
    "            env.score += env.reward\n",
    "            env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "            step += 1\n",
    "        \n",
    "\n",
    "        evaluation_reward.append(env.score)\n",
    "        print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                      \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in env_names:\n",
    "    test_best[i]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional LSTM agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " ------- STARTING TRAINING FOR SpaceInvaders-v0 ------- \n",
      "\n",
      "\n",
      "\n",
      "Determing min/max rewards of environment\n",
      "Min: 0. Max: 200.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:49: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],1)[0].unsqueeze(1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 1: Policy loss: -0.087297. Value loss: 0.027573. Entropy: 0.346327.\n",
      "Iteration 2: Policy loss: -0.089115. Value loss: 0.025702. Entropy: 0.344373.\n",
      "Iteration 3: Policy loss: -0.089712. Value loss: 0.021268. Entropy: 0.344549.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 4: Policy loss: -0.375369. Value loss: 0.161101. Entropy: 0.345617.\n",
      "Iteration 5: Policy loss: -0.377800. Value loss: 0.134698. Entropy: 0.346057.\n",
      "Iteration 6: Policy loss: -0.375845. Value loss: 0.099258. Entropy: 0.346682.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 7: Policy loss: -0.173769. Value loss: 0.115798. Entropy: 0.345349.\n",
      "Iteration 8: Policy loss: -0.172274. Value loss: 0.092829. Entropy: 0.346002.\n",
      "Iteration 9: Policy loss: -0.166380. Value loss: 0.071452. Entropy: 0.346318.\n",
      "now time :  2019-09-28 10:16:25.975083\n",
      "episode: 1   score: 65.0  epsilon: 1.0    steps: 952  evaluation reward: 65.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 10: Policy loss: -0.224217. Value loss: 0.116623. Entropy: 0.345608.\n",
      "Iteration 11: Policy loss: -0.233139. Value loss: 0.069058. Entropy: 0.345300.\n",
      "Iteration 12: Policy loss: -0.231558. Value loss: 0.053121. Entropy: 0.345733.\n",
      "episode: 2   score: 45.0  epsilon: 1.0    steps: 152  evaluation reward: 55.0\n",
      "episode: 3   score: 135.0  epsilon: 1.0    steps: 376  evaluation reward: 81.66666666666667\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 13: Policy loss: -0.376040. Value loss: 0.381022. Entropy: 0.345861.\n",
      "Iteration 14: Policy loss: -0.377707. Value loss: 0.331064. Entropy: 0.344518.\n",
      "Iteration 15: Policy loss: -0.385020. Value loss: 0.304760. Entropy: 0.344705.\n",
      "episode: 4   score: 120.0  epsilon: 1.0    steps: 200  evaluation reward: 91.25\n",
      "episode: 5   score: 110.0  epsilon: 1.0    steps: 472  evaluation reward: 95.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 16: Policy loss: -0.297327. Value loss: 0.122086. Entropy: 0.343944.\n",
      "Iteration 17: Policy loss: -0.293653. Value loss: 0.083039. Entropy: 0.345675.\n",
      "Iteration 18: Policy loss: -0.297742. Value loss: 0.069308. Entropy: 0.346104.\n",
      "episode: 6   score: 410.0  epsilon: 1.0    steps: 600  evaluation reward: 147.5\n",
      "episode: 7   score: 380.0  epsilon: 1.0    steps: 800  evaluation reward: 180.71428571428572\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 19: Policy loss: -0.277383. Value loss: 0.166392. Entropy: 0.345093.\n",
      "Iteration 20: Policy loss: -0.280207. Value loss: 0.106432. Entropy: 0.345382.\n",
      "Iteration 21: Policy loss: -0.276164. Value loss: 0.084201. Entropy: 0.346008.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 22: Policy loss: -0.119643. Value loss: 0.113339. Entropy: 0.345376.\n",
      "Iteration 23: Policy loss: -0.123768. Value loss: 0.059447. Entropy: 0.345071.\n",
      "Iteration 24: Policy loss: -0.127775. Value loss: 0.044750. Entropy: 0.345554.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 25: Policy loss: -0.150385. Value loss: 0.126769. Entropy: 0.345109.\n",
      "Iteration 26: Policy loss: -0.154548. Value loss: 0.072769. Entropy: 0.345499.\n",
      "Iteration 27: Policy loss: -0.152643. Value loss: 0.051504. Entropy: 0.345839.\n",
      "episode: 8   score: 375.0  epsilon: 1.0    steps: 280  evaluation reward: 205.0\n",
      "episode: 9   score: 155.0  epsilon: 1.0    steps: 968  evaluation reward: 199.44444444444446\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 28: Policy loss: -0.097106. Value loss: 0.132514. Entropy: 0.345558.\n",
      "Iteration 29: Policy loss: -0.096436. Value loss: 0.075220. Entropy: 0.345829.\n",
      "Iteration 30: Policy loss: -0.099086. Value loss: 0.045201. Entropy: 0.345650.\n",
      "episode: 10   score: 105.0  epsilon: 1.0    steps: 192  evaluation reward: 190.0\n",
      "episode: 11   score: 180.0  epsilon: 1.0    steps: 208  evaluation reward: 189.0909090909091\n",
      "episode: 12   score: 155.0  epsilon: 1.0    steps: 792  evaluation reward: 186.25\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 31: Policy loss: -0.146490. Value loss: 0.091349. Entropy: 0.345900.\n",
      "Iteration 32: Policy loss: -0.133088. Value loss: 0.063185. Entropy: 0.345828.\n",
      "Iteration 33: Policy loss: -0.149056. Value loss: 0.050513. Entropy: 0.345687.\n",
      "episode: 13   score: 75.0  epsilon: 1.0    steps: 24  evaluation reward: 177.69230769230768\n",
      "episode: 14   score: 180.0  epsilon: 1.0    steps: 680  evaluation reward: 177.85714285714286\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 34: Policy loss: 0.032059. Value loss: 0.104892. Entropy: 0.344012.\n",
      "Iteration 35: Policy loss: 0.033390. Value loss: 0.064632. Entropy: 0.342830.\n",
      "Iteration 36: Policy loss: 0.037713. Value loss: 0.055211. Entropy: 0.342146.\n",
      "episode: 15   score: 135.0  epsilon: 1.0    steps: 584  evaluation reward: 175.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 37: Policy loss: 0.106569. Value loss: 0.118457. Entropy: 0.343541.\n",
      "Iteration 38: Policy loss: 0.110274. Value loss: 0.082879. Entropy: 0.342217.\n",
      "Iteration 39: Policy loss: 0.104842. Value loss: 0.065105. Entropy: 0.342776.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 40: Policy loss: -0.006898. Value loss: 0.087189. Entropy: 0.343949.\n",
      "Iteration 41: Policy loss: -0.004071. Value loss: 0.051581. Entropy: 0.343471.\n",
      "Iteration 42: Policy loss: -0.011718. Value loss: 0.038937. Entropy: 0.343174.\n",
      "episode: 16   score: 110.0  epsilon: 1.0    steps: 992  evaluation reward: 170.9375\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 43: Policy loss: 0.093697. Value loss: 0.037743. Entropy: 0.343644.\n",
      "Iteration 44: Policy loss: 0.092315. Value loss: 0.017133. Entropy: 0.343275.\n",
      "Iteration 45: Policy loss: 0.092504. Value loss: 0.013239. Entropy: 0.343335.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 46: Policy loss: -0.414206. Value loss: 0.140631. Entropy: 0.342573.\n",
      "Iteration 47: Policy loss: -0.411383. Value loss: 0.081048. Entropy: 0.340733.\n",
      "Iteration 48: Policy loss: -0.410536. Value loss: 0.064436. Entropy: 0.341812.\n",
      "episode: 17   score: 155.0  epsilon: 1.0    steps: 120  evaluation reward: 170.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 49: Policy loss: -0.018726. Value loss: 0.149563. Entropy: 0.340937.\n",
      "Iteration 50: Policy loss: -0.021417. Value loss: 0.081009. Entropy: 0.340592.\n",
      "Iteration 51: Policy loss: -0.030020. Value loss: 0.058769. Entropy: 0.341709.\n",
      "episode: 18   score: 210.0  epsilon: 1.0    steps: 88  evaluation reward: 172.22222222222223\n",
      "episode: 19   score: 210.0  epsilon: 1.0    steps: 672  evaluation reward: 174.21052631578948\n",
      "episode: 20   score: 155.0  epsilon: 1.0    steps: 880  evaluation reward: 173.25\n",
      "episode: 21   score: 255.0  epsilon: 1.0    steps: 928  evaluation reward: 177.14285714285714\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 52: Policy loss: -0.352048. Value loss: 0.404046. Entropy: 0.339392.\n",
      "Iteration 53: Policy loss: -0.359452. Value loss: 0.287886. Entropy: 0.340394.\n",
      "Iteration 54: Policy loss: -0.340144. Value loss: 0.224961. Entropy: 0.340192.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 55: Policy loss: 0.216297. Value loss: 0.116960. Entropy: 0.344116.\n",
      "Iteration 56: Policy loss: 0.204995. Value loss: 0.068634. Entropy: 0.344412.\n",
      "Iteration 57: Policy loss: 0.199635. Value loss: 0.062360. Entropy: 0.341965.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 58: Policy loss: 0.037526. Value loss: 0.154060. Entropy: 0.341798.\n",
      "Iteration 59: Policy loss: 0.032712. Value loss: 0.072623. Entropy: 0.338577.\n",
      "Iteration 60: Policy loss: 0.037028. Value loss: 0.058696. Entropy: 0.337247.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 61: Policy loss: 0.091231. Value loss: 0.063635. Entropy: 0.342720.\n",
      "Iteration 62: Policy loss: 0.094213. Value loss: 0.034959. Entropy: 0.341223.\n",
      "Iteration 63: Policy loss: 0.093551. Value loss: 0.029697. Entropy: 0.341729.\n",
      "episode: 22   score: 430.0  epsilon: 1.0    steps: 456  evaluation reward: 188.63636363636363\n",
      "episode: 23   score: 320.0  epsilon: 1.0    steps: 824  evaluation reward: 194.34782608695653\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 64: Policy loss: -0.354735. Value loss: 0.353844. Entropy: 0.341981.\n",
      "Iteration 65: Policy loss: -0.393617. Value loss: 0.319829. Entropy: 0.342714.\n",
      "Iteration 66: Policy loss: -0.396387. Value loss: 0.276726. Entropy: 0.339888.\n",
      "episode: 24   score: 140.0  epsilon: 1.0    steps: 232  evaluation reward: 192.08333333333334\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 67: Policy loss: 0.060663. Value loss: 0.213464. Entropy: 0.344035.\n",
      "Iteration 68: Policy loss: 0.050532. Value loss: 0.117771. Entropy: 0.342744.\n",
      "Iteration 69: Policy loss: 0.058104. Value loss: 0.091528. Entropy: 0.343507.\n",
      "episode: 25   score: 105.0  epsilon: 1.0    steps: 176  evaluation reward: 188.6\n",
      "episode: 26   score: 180.0  epsilon: 1.0    steps: 504  evaluation reward: 188.26923076923077\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 70: Policy loss: 0.270426. Value loss: 0.136064. Entropy: 0.341084.\n",
      "Iteration 71: Policy loss: 0.269234. Value loss: 0.079848. Entropy: 0.340712.\n",
      "Iteration 72: Policy loss: 0.270445. Value loss: 0.061569. Entropy: 0.341126.\n",
      "episode: 27   score: 515.0  epsilon: 1.0    steps: 1016  evaluation reward: 200.37037037037038\n",
      "Training network. lr: 0.000250. clip: 0.099853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 73: Policy loss: -0.266638. Value loss: 0.365303. Entropy: 0.344974.\n",
      "Iteration 74: Policy loss: -0.265152. Value loss: 0.223754. Entropy: 0.342675.\n",
      "Iteration 75: Policy loss: -0.279644. Value loss: 0.175931. Entropy: 0.344162.\n",
      "episode: 28   score: 460.0  epsilon: 1.0    steps: 312  evaluation reward: 209.64285714285714\n",
      "episode: 29   score: 240.0  epsilon: 1.0    steps: 320  evaluation reward: 210.68965517241378\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 76: Policy loss: 0.119494. Value loss: 0.055369. Entropy: 0.344947.\n",
      "Iteration 77: Policy loss: 0.115497. Value loss: 0.029697. Entropy: 0.344809.\n",
      "Iteration 78: Policy loss: 0.110617. Value loss: 0.020872. Entropy: 0.344038.\n",
      "episode: 30   score: 105.0  epsilon: 1.0    steps: 872  evaluation reward: 207.16666666666666\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 79: Policy loss: -0.281162. Value loss: 0.307344. Entropy: 0.341971.\n",
      "Iteration 80: Policy loss: -0.283545. Value loss: 0.246363. Entropy: 0.343692.\n",
      "Iteration 81: Policy loss: -0.287391. Value loss: 0.224157. Entropy: 0.342349.\n",
      "episode: 31   score: 390.0  epsilon: 1.0    steps: 848  evaluation reward: 213.06451612903226\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 82: Policy loss: 0.033481. Value loss: 0.080494. Entropy: 0.344393.\n",
      "Iteration 83: Policy loss: 0.035303. Value loss: 0.049362. Entropy: 0.344970.\n",
      "Iteration 84: Policy loss: 0.035465. Value loss: 0.041916. Entropy: 0.344596.\n",
      "episode: 32   score: 165.0  epsilon: 1.0    steps: 488  evaluation reward: 211.5625\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 85: Policy loss: -0.042326. Value loss: 0.084645. Entropy: 0.345022.\n",
      "Iteration 86: Policy loss: -0.045468. Value loss: 0.039513. Entropy: 0.344750.\n",
      "Iteration 87: Policy loss: -0.045329. Value loss: 0.028284. Entropy: 0.344146.\n",
      "episode: 33   score: 135.0  epsilon: 1.0    steps: 584  evaluation reward: 209.24242424242425\n",
      "episode: 34   score: 160.0  epsilon: 1.0    steps: 776  evaluation reward: 207.7941176470588\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 88: Policy loss: 0.140947. Value loss: 0.088977. Entropy: 0.344952.\n",
      "Iteration 89: Policy loss: 0.126820. Value loss: 0.048629. Entropy: 0.343572.\n",
      "Iteration 90: Policy loss: 0.133878. Value loss: 0.037535. Entropy: 0.343631.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 91: Policy loss: 0.254411. Value loss: 0.077157. Entropy: 0.343844.\n",
      "Iteration 92: Policy loss: 0.257394. Value loss: 0.036894. Entropy: 0.344310.\n",
      "Iteration 93: Policy loss: 0.248442. Value loss: 0.023582. Entropy: 0.343579.\n",
      "episode: 35   score: 110.0  epsilon: 1.0    steps: 400  evaluation reward: 205.0\n",
      "episode: 36   score: 155.0  epsilon: 1.0    steps: 416  evaluation reward: 203.61111111111111\n",
      "episode: 37   score: 210.0  epsilon: 1.0    steps: 952  evaluation reward: 203.78378378378378\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 94: Policy loss: 0.155490. Value loss: 0.163804. Entropy: 0.344645.\n",
      "Iteration 95: Policy loss: 0.147839. Value loss: 0.103400. Entropy: 0.344960.\n",
      "Iteration 96: Policy loss: 0.141154. Value loss: 0.079318. Entropy: 0.344811.\n",
      "episode: 38   score: 335.0  epsilon: 1.0    steps: 992  evaluation reward: 207.23684210526315\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 97: Policy loss: -0.257329. Value loss: 0.320050. Entropy: 0.343049.\n",
      "Iteration 98: Policy loss: -0.258116. Value loss: 0.243375. Entropy: 0.343220.\n",
      "Iteration 99: Policy loss: -0.255796. Value loss: 0.200756. Entropy: 0.342637.\n",
      "episode: 39   score: 105.0  epsilon: 1.0    steps: 648  evaluation reward: 204.6153846153846\n",
      "episode: 40   score: 120.0  epsilon: 1.0    steps: 992  evaluation reward: 202.5\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 100: Policy loss: 0.117870. Value loss: 0.100712. Entropy: 0.342813.\n",
      "Iteration 101: Policy loss: 0.112725. Value loss: 0.057120. Entropy: 0.340741.\n",
      "Iteration 102: Policy loss: 0.112876. Value loss: 0.049157. Entropy: 0.342014.\n",
      "episode: 41   score: 155.0  epsilon: 1.0    steps: 40  evaluation reward: 201.34146341463415\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 103: Policy loss: 0.024995. Value loss: 0.071611. Entropy: 0.344641.\n",
      "Iteration 104: Policy loss: 0.018942. Value loss: 0.023307. Entropy: 0.342896.\n",
      "Iteration 105: Policy loss: 0.015065. Value loss: 0.017815. Entropy: 0.343675.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 106: Policy loss: -0.166051. Value loss: 0.094329. Entropy: 0.343774.\n",
      "Iteration 107: Policy loss: -0.162727. Value loss: 0.053017. Entropy: 0.342109.\n",
      "Iteration 108: Policy loss: -0.167160. Value loss: 0.046622. Entropy: 0.343087.\n",
      "episode: 42   score: 210.0  epsilon: 1.0    steps: 384  evaluation reward: 201.54761904761904\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 109: Policy loss: 0.145182. Value loss: 0.073406. Entropy: 0.345895.\n",
      "Iteration 110: Policy loss: 0.152114. Value loss: 0.029607. Entropy: 0.345828.\n",
      "Iteration 111: Policy loss: 0.137059. Value loss: 0.023641. Entropy: 0.344980.\n",
      "episode: 43   score: 155.0  epsilon: 1.0    steps: 864  evaluation reward: 200.46511627906978\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 112: Policy loss: -0.253294. Value loss: 0.313989. Entropy: 0.342842.\n",
      "Iteration 113: Policy loss: -0.252808. Value loss: 0.186783. Entropy: 0.343944.\n",
      "Iteration 114: Policy loss: -0.246198. Value loss: 0.143801. Entropy: 0.342975.\n",
      "episode: 44   score: 180.0  epsilon: 1.0    steps: 592  evaluation reward: 200.0\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 115: Policy loss: 0.170561. Value loss: 0.245136. Entropy: 0.342887.\n",
      "Iteration 116: Policy loss: 0.170300. Value loss: 0.102576. Entropy: 0.341970.\n",
      "Iteration 117: Policy loss: 0.152080. Value loss: 0.066982. Entropy: 0.343051.\n",
      "episode: 45   score: 265.0  epsilon: 1.0    steps: 48  evaluation reward: 201.44444444444446\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 118: Policy loss: 0.008660. Value loss: 0.109293. Entropy: 0.347117.\n",
      "Iteration 119: Policy loss: 0.006477. Value loss: 0.066918. Entropy: 0.347996.\n",
      "Iteration 120: Policy loss: 0.001704. Value loss: 0.058472. Entropy: 0.347957.\n",
      "episode: 46   score: 185.0  epsilon: 1.0    steps: 152  evaluation reward: 201.08695652173913\n",
      "episode: 47   score: 160.0  epsilon: 1.0    steps: 368  evaluation reward: 200.2127659574468\n",
      "episode: 48   score: 460.0  epsilon: 1.0    steps: 576  evaluation reward: 205.625\n",
      "episode: 49   score: 210.0  epsilon: 1.0    steps: 680  evaluation reward: 205.71428571428572\n",
      "episode: 50   score: 75.0  epsilon: 1.0    steps: 1008  evaluation reward: 203.1\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 121: Policy loss: 0.266390. Value loss: 0.126539. Entropy: 0.346764.\n",
      "Iteration 122: Policy loss: 0.265212. Value loss: 0.061042. Entropy: 0.344696.\n",
      "Iteration 123: Policy loss: 0.259787. Value loss: 0.046600. Entropy: 0.343469.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 124: Policy loss: 0.048181. Value loss: 0.097720. Entropy: 0.346696.\n",
      "Iteration 125: Policy loss: 0.042924. Value loss: 0.062912. Entropy: 0.346845.\n",
      "Iteration 126: Policy loss: 0.033020. Value loss: 0.056036. Entropy: 0.347105.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 127: Policy loss: 0.157348. Value loss: 0.078105. Entropy: 0.346345.\n",
      "Iteration 128: Policy loss: 0.155578. Value loss: 0.055680. Entropy: 0.346226.\n",
      "Iteration 129: Policy loss: 0.154585. Value loss: 0.048934. Entropy: 0.345365.\n",
      "now time :  2019-09-28 10:18:51.367853\n",
      "episode: 51   score: 30.0  epsilon: 1.0    steps: 8  evaluation reward: 199.7058823529412\n",
      "episode: 52   score: 180.0  epsilon: 1.0    steps: 104  evaluation reward: 199.32692307692307\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 130: Policy loss: -0.026594. Value loss: 0.027951. Entropy: 0.345727.\n",
      "Iteration 131: Policy loss: -0.030058. Value loss: 0.011228. Entropy: 0.345390.\n",
      "Iteration 132: Policy loss: -0.029954. Value loss: 0.008412. Entropy: 0.345023.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 133: Policy loss: -0.152316. Value loss: 0.363406. Entropy: 0.346793.\n",
      "Iteration 134: Policy loss: -0.169648. Value loss: 0.293982. Entropy: 0.345461.\n",
      "Iteration 135: Policy loss: -0.166690. Value loss: 0.266343. Entropy: 0.345728.\n",
      "episode: 53   score: 155.0  epsilon: 1.0    steps: 48  evaluation reward: 198.49056603773585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 136: Policy loss: 0.011509. Value loss: 0.087159. Entropy: 0.346596.\n",
      "Iteration 137: Policy loss: 0.010617. Value loss: 0.047063. Entropy: 0.344803.\n",
      "Iteration 138: Policy loss: 0.010809. Value loss: 0.039403. Entropy: 0.345393.\n",
      "episode: 54   score: 355.0  epsilon: 1.0    steps: 760  evaluation reward: 201.38888888888889\n",
      "episode: 55   score: 185.0  epsilon: 1.0    steps: 1016  evaluation reward: 201.0909090909091\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 139: Policy loss: 0.128009. Value loss: 0.104885. Entropy: 0.345599.\n",
      "Iteration 140: Policy loss: 0.117074. Value loss: 0.048867. Entropy: 0.344989.\n",
      "Iteration 141: Policy loss: 0.119619. Value loss: 0.041926. Entropy: 0.344437.\n",
      "episode: 56   score: 210.0  epsilon: 1.0    steps: 144  evaluation reward: 201.25\n",
      "episode: 57   score: 135.0  epsilon: 1.0    steps: 312  evaluation reward: 200.08771929824562\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 142: Policy loss: -0.026192. Value loss: 0.042410. Entropy: 0.346435.\n",
      "Iteration 143: Policy loss: -0.027644. Value loss: 0.021118. Entropy: 0.345896.\n",
      "Iteration 144: Policy loss: -0.032130. Value loss: 0.015826. Entropy: 0.345186.\n",
      "episode: 58   score: 120.0  epsilon: 1.0    steps: 240  evaluation reward: 198.70689655172413\n",
      "episode: 59   score: 185.0  epsilon: 1.0    steps: 392  evaluation reward: 198.47457627118644\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 145: Policy loss: 0.112409. Value loss: 0.086293. Entropy: 0.345079.\n",
      "Iteration 146: Policy loss: 0.108126. Value loss: 0.058773. Entropy: 0.342843.\n",
      "Iteration 147: Policy loss: 0.099763. Value loss: 0.043204. Entropy: 0.342508.\n",
      "episode: 60   score: 155.0  epsilon: 1.0    steps: 536  evaluation reward: 197.75\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 148: Policy loss: 0.047717. Value loss: 0.051473. Entropy: 0.346950.\n",
      "Iteration 149: Policy loss: 0.047199. Value loss: 0.041854. Entropy: 0.346772.\n",
      "Iteration 150: Policy loss: 0.045124. Value loss: 0.047750. Entropy: 0.346662.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 151: Policy loss: 0.024847. Value loss: 0.023679. Entropy: 0.345342.\n",
      "Iteration 152: Policy loss: 0.024571. Value loss: 0.008954. Entropy: 0.344925.\n",
      "Iteration 153: Policy loss: 0.022092. Value loss: 0.007111. Entropy: 0.344473.\n",
      "episode: 61   score: 135.0  epsilon: 1.0    steps: 248  evaluation reward: 196.72131147540983\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 154: Policy loss: -0.071487. Value loss: 0.051750. Entropy: 0.345068.\n",
      "Iteration 155: Policy loss: -0.074095. Value loss: 0.026251. Entropy: 0.345579.\n",
      "Iteration 156: Policy loss: -0.069822. Value loss: 0.016434. Entropy: 0.345527.\n",
      "episode: 62   score: 105.0  epsilon: 1.0    steps: 400  evaluation reward: 195.24193548387098\n",
      "episode: 63   score: 135.0  epsilon: 1.0    steps: 912  evaluation reward: 194.28571428571428\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 157: Policy loss: 0.005720. Value loss: 0.055388. Entropy: 0.345280.\n",
      "Iteration 158: Policy loss: -0.002562. Value loss: 0.025213. Entropy: 0.345746.\n",
      "Iteration 159: Policy loss: 0.000542. Value loss: 0.019407. Entropy: 0.346365.\n",
      "episode: 64   score: 150.0  epsilon: 1.0    steps: 176  evaluation reward: 193.59375\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 160: Policy loss: -0.064520. Value loss: 0.057800. Entropy: 0.346559.\n",
      "Iteration 161: Policy loss: -0.066843. Value loss: 0.034393. Entropy: 0.345987.\n",
      "Iteration 162: Policy loss: -0.069886. Value loss: 0.028768. Entropy: 0.345520.\n",
      "episode: 65   score: 210.0  epsilon: 1.0    steps: 56  evaluation reward: 193.84615384615384\n",
      "episode: 66   score: 155.0  epsilon: 1.0    steps: 960  evaluation reward: 193.25757575757575\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 163: Policy loss: -0.301148. Value loss: 0.282392. Entropy: 0.341916.\n",
      "Iteration 164: Policy loss: -0.306446. Value loss: 0.171641. Entropy: 0.346661.\n",
      "Iteration 165: Policy loss: -0.309375. Value loss: 0.136959. Entropy: 0.345755.\n",
      "episode: 67   score: 180.0  epsilon: 1.0    steps: 304  evaluation reward: 193.0597014925373\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 166: Policy loss: 0.060912. Value loss: 0.080374. Entropy: 0.348755.\n",
      "Iteration 167: Policy loss: 0.065527. Value loss: 0.036321. Entropy: 0.348180.\n",
      "Iteration 168: Policy loss: 0.055056. Value loss: 0.027045. Entropy: 0.348151.\n",
      "episode: 68   score: 380.0  epsilon: 1.0    steps: 488  evaluation reward: 195.80882352941177\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 169: Policy loss: -0.023785. Value loss: 0.068570. Entropy: 0.344207.\n",
      "Iteration 170: Policy loss: -0.026957. Value loss: 0.038304. Entropy: 0.345078.\n",
      "Iteration 171: Policy loss: -0.031820. Value loss: 0.026842. Entropy: 0.344465.\n",
      "episode: 69   score: 185.0  epsilon: 1.0    steps: 880  evaluation reward: 195.65217391304347\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 172: Policy loss: -0.053040. Value loss: 0.067911. Entropy: 0.345887.\n",
      "Iteration 173: Policy loss: -0.060984. Value loss: 0.032611. Entropy: 0.346451.\n",
      "Iteration 174: Policy loss: -0.059610. Value loss: 0.024510. Entropy: 0.347111.\n",
      "episode: 70   score: 110.0  epsilon: 1.0    steps: 560  evaluation reward: 194.42857142857142\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 175: Policy loss: -0.014487. Value loss: 0.096276. Entropy: 0.346803.\n",
      "Iteration 176: Policy loss: -0.022858. Value loss: 0.044478. Entropy: 0.345662.\n",
      "Iteration 177: Policy loss: -0.024432. Value loss: 0.033081. Entropy: 0.345138.\n",
      "episode: 71   score: 180.0  epsilon: 1.0    steps: 240  evaluation reward: 194.22535211267606\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 178: Policy loss: 0.026726. Value loss: 0.044272. Entropy: 0.347821.\n",
      "Iteration 179: Policy loss: 0.025061. Value loss: 0.023842. Entropy: 0.346973.\n",
      "Iteration 180: Policy loss: 0.026571. Value loss: 0.020152. Entropy: 0.347678.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 181: Policy loss: 0.006060. Value loss: 0.051089. Entropy: 0.345591.\n",
      "Iteration 182: Policy loss: 0.007399. Value loss: 0.030210. Entropy: 0.344777.\n",
      "Iteration 183: Policy loss: 0.004445. Value loss: 0.025217. Entropy: 0.344370.\n",
      "episode: 72   score: 135.0  epsilon: 1.0    steps: 360  evaluation reward: 193.40277777777777\n",
      "episode: 73   score: 165.0  epsilon: 1.0    steps: 480  evaluation reward: 193.013698630137\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 184: Policy loss: 0.091744. Value loss: 0.058297. Entropy: 0.345165.\n",
      "Iteration 185: Policy loss: 0.087041. Value loss: 0.031451. Entropy: 0.344398.\n",
      "Iteration 186: Policy loss: 0.088796. Value loss: 0.025096. Entropy: 0.344420.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 187: Policy loss: -0.416395. Value loss: 0.296785. Entropy: 0.345812.\n",
      "Iteration 188: Policy loss: -0.429287. Value loss: 0.188560. Entropy: 0.345862.\n",
      "Iteration 189: Policy loss: -0.450768. Value loss: 0.142186. Entropy: 0.346234.\n",
      "episode: 74   score: 320.0  epsilon: 1.0    steps: 936  evaluation reward: 194.72972972972974\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 190: Policy loss: 0.175797. Value loss: 0.122437. Entropy: 0.342234.\n",
      "Iteration 191: Policy loss: 0.175846. Value loss: 0.032779. Entropy: 0.339868.\n",
      "Iteration 192: Policy loss: 0.154791. Value loss: 0.024794. Entropy: 0.340982.\n",
      "episode: 75   score: 240.0  epsilon: 1.0    steps: 144  evaluation reward: 195.33333333333334\n",
      "episode: 76   score: 355.0  epsilon: 1.0    steps: 512  evaluation reward: 197.43421052631578\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 193: Policy loss: -0.006215. Value loss: 0.095213. Entropy: 0.347162.\n",
      "Iteration 194: Policy loss: -0.012970. Value loss: 0.039390. Entropy: 0.348658.\n",
      "Iteration 195: Policy loss: -0.015310. Value loss: 0.028179. Entropy: 0.348437.\n",
      "episode: 77   score: 210.0  epsilon: 1.0    steps: 280  evaluation reward: 197.5974025974026\n",
      "episode: 78   score: 135.0  epsilon: 1.0    steps: 896  evaluation reward: 196.7948717948718\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 196: Policy loss: 0.043481. Value loss: 0.079658. Entropy: 0.347967.\n",
      "Iteration 197: Policy loss: 0.043914. Value loss: 0.039392. Entropy: 0.346352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 198: Policy loss: 0.048500. Value loss: 0.032288. Entropy: 0.345406.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 199: Policy loss: 0.117361. Value loss: 0.101837. Entropy: 0.342117.\n",
      "Iteration 200: Policy loss: 0.124306. Value loss: 0.056419. Entropy: 0.341371.\n",
      "Iteration 201: Policy loss: 0.109202. Value loss: 0.050456. Entropy: 0.341588.\n",
      "episode: 79   score: 155.0  epsilon: 1.0    steps: 528  evaluation reward: 196.26582278481013\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 202: Policy loss: -0.117868. Value loss: 0.069015. Entropy: 0.343690.\n",
      "Iteration 203: Policy loss: -0.126265. Value loss: 0.039931. Entropy: 0.344370.\n",
      "Iteration 204: Policy loss: -0.120766. Value loss: 0.033849. Entropy: 0.344203.\n",
      "episode: 80   score: 210.0  epsilon: 1.0    steps: 64  evaluation reward: 196.4375\n",
      "episode: 81   score: 75.0  epsilon: 1.0    steps: 304  evaluation reward: 194.93827160493828\n",
      "episode: 82   score: 260.0  epsilon: 1.0    steps: 496  evaluation reward: 195.73170731707316\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 205: Policy loss: -0.248306. Value loss: 0.291976. Entropy: 0.338209.\n",
      "Iteration 206: Policy loss: -0.252401. Value loss: 0.189734. Entropy: 0.339745.\n",
      "Iteration 207: Policy loss: -0.252763. Value loss: 0.142830. Entropy: 0.339372.\n",
      "episode: 83   score: 110.0  epsilon: 1.0    steps: 768  evaluation reward: 194.6987951807229\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 208: Policy loss: 0.075258. Value loss: 0.121035. Entropy: 0.344637.\n",
      "Iteration 209: Policy loss: 0.065096. Value loss: 0.078082. Entropy: 0.345524.\n",
      "Iteration 210: Policy loss: 0.068592. Value loss: 0.063346. Entropy: 0.345425.\n",
      "episode: 84   score: 155.0  epsilon: 1.0    steps: 584  evaluation reward: 194.22619047619048\n",
      "episode: 85   score: 410.0  epsilon: 1.0    steps: 792  evaluation reward: 196.76470588235293\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 211: Policy loss: 0.120317. Value loss: 0.037190. Entropy: 0.344931.\n",
      "Iteration 212: Policy loss: 0.119163. Value loss: 0.018605. Entropy: 0.344119.\n",
      "Iteration 213: Policy loss: 0.119667. Value loss: 0.015280. Entropy: 0.343832.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 214: Policy loss: -0.018401. Value loss: 0.070414. Entropy: 0.347038.\n",
      "Iteration 215: Policy loss: -0.018283. Value loss: 0.048495. Entropy: 0.347713.\n",
      "Iteration 216: Policy loss: -0.022800. Value loss: 0.041626. Entropy: 0.347738.\n",
      "episode: 86   score: 135.0  epsilon: 1.0    steps: 32  evaluation reward: 196.04651162790697\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 217: Policy loss: 0.051614. Value loss: 0.047805. Entropy: 0.343940.\n",
      "Iteration 218: Policy loss: 0.049727. Value loss: 0.022622. Entropy: 0.342966.\n",
      "Iteration 219: Policy loss: 0.049251. Value loss: 0.020255. Entropy: 0.343673.\n",
      "episode: 87   score: 105.0  epsilon: 1.0    steps: 168  evaluation reward: 195.0\n",
      "episode: 88   score: 105.0  epsilon: 1.0    steps: 712  evaluation reward: 193.97727272727272\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 220: Policy loss: 0.037697. Value loss: 0.040435. Entropy: 0.342712.\n",
      "Iteration 221: Policy loss: 0.031998. Value loss: 0.017035. Entropy: 0.342723.\n",
      "Iteration 222: Policy loss: 0.035724. Value loss: 0.015614. Entropy: 0.341400.\n",
      "episode: 89   score: 155.0  epsilon: 1.0    steps: 32  evaluation reward: 193.53932584269663\n",
      "episode: 90   score: 180.0  epsilon: 1.0    steps: 992  evaluation reward: 193.38888888888889\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 223: Policy loss: -0.070717. Value loss: 0.058022. Entropy: 0.345901.\n",
      "Iteration 224: Policy loss: -0.072392. Value loss: 0.031119. Entropy: 0.343853.\n",
      "Iteration 225: Policy loss: -0.075707. Value loss: 0.025654. Entropy: 0.344017.\n",
      "episode: 91   score: 105.0  epsilon: 1.0    steps: 760  evaluation reward: 192.41758241758242\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 226: Policy loss: -0.304794. Value loss: 0.239506. Entropy: 0.341748.\n",
      "Iteration 227: Policy loss: -0.319163. Value loss: 0.111637. Entropy: 0.342756.\n",
      "Iteration 228: Policy loss: -0.332222. Value loss: 0.074110. Entropy: 0.343925.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 229: Policy loss: -0.060707. Value loss: 0.147938. Entropy: 0.340836.\n",
      "Iteration 230: Policy loss: -0.052780. Value loss: 0.064902. Entropy: 0.339808.\n",
      "Iteration 231: Policy loss: -0.066189. Value loss: 0.045926. Entropy: 0.340878.\n",
      "episode: 92   score: 380.0  epsilon: 1.0    steps: 344  evaluation reward: 194.45652173913044\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 232: Policy loss: -0.382213. Value loss: 0.214300. Entropy: 0.347055.\n",
      "Iteration 233: Policy loss: -0.384593. Value loss: 0.096324. Entropy: 0.347344.\n",
      "Iteration 234: Policy loss: -0.385075. Value loss: 0.080676. Entropy: 0.346875.\n",
      "episode: 93   score: 210.0  epsilon: 1.0    steps: 704  evaluation reward: 194.6236559139785\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 235: Policy loss: 0.105741. Value loss: 0.142413. Entropy: 0.343428.\n",
      "Iteration 236: Policy loss: 0.086781. Value loss: 0.050992. Entropy: 0.340268.\n",
      "Iteration 237: Policy loss: 0.091662. Value loss: 0.033453. Entropy: 0.341899.\n",
      "episode: 94   score: 155.0  epsilon: 1.0    steps: 864  evaluation reward: 194.20212765957447\n",
      "episode: 95   score: 410.0  epsilon: 1.0    steps: 912  evaluation reward: 196.47368421052633\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 238: Policy loss: 0.058447. Value loss: 0.075416. Entropy: 0.342641.\n",
      "Iteration 239: Policy loss: 0.052562. Value loss: 0.027775. Entropy: 0.342991.\n",
      "Iteration 240: Policy loss: 0.051958. Value loss: 0.025560. Entropy: 0.343689.\n",
      "episode: 96   score: 315.0  epsilon: 1.0    steps: 368  evaluation reward: 197.70833333333334\n",
      "episode: 97   score: 180.0  epsilon: 1.0    steps: 528  evaluation reward: 197.5257731958763\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 241: Policy loss: 0.074660. Value loss: 0.083584. Entropy: 0.343149.\n",
      "Iteration 242: Policy loss: 0.079231. Value loss: 0.044866. Entropy: 0.341665.\n",
      "Iteration 243: Policy loss: 0.072189. Value loss: 0.037238. Entropy: 0.342050.\n",
      "episode: 98   score: 180.0  epsilon: 1.0    steps: 648  evaluation reward: 197.3469387755102\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 244: Policy loss: -0.049298. Value loss: 0.081146. Entropy: 0.347873.\n",
      "Iteration 245: Policy loss: -0.056577. Value loss: 0.046749. Entropy: 0.348120.\n",
      "Iteration 246: Policy loss: -0.058397. Value loss: 0.037341. Entropy: 0.346957.\n",
      "episode: 99   score: 180.0  epsilon: 1.0    steps: 680  evaluation reward: 197.17171717171718\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 247: Policy loss: -0.083661. Value loss: 0.038273. Entropy: 0.345401.\n",
      "Iteration 248: Policy loss: -0.084399. Value loss: 0.014927. Entropy: 0.344767.\n",
      "Iteration 249: Policy loss: -0.084160. Value loss: 0.010838. Entropy: 0.344559.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 250: Policy loss: -0.294089. Value loss: 0.291232. Entropy: 0.346087.\n",
      "Iteration 251: Policy loss: -0.311774. Value loss: 0.237874. Entropy: 0.346641.\n",
      "Iteration 252: Policy loss: -0.308324. Value loss: 0.146838. Entropy: 0.347513.\n",
      "episode: 100   score: 210.0  epsilon: 1.0    steps: 56  evaluation reward: 197.3\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 253: Policy loss: 0.012588. Value loss: 0.124268. Entropy: 0.346326.\n",
      "Iteration 254: Policy loss: 0.005445. Value loss: 0.071467. Entropy: 0.346004.\n",
      "Iteration 255: Policy loss: 0.006446. Value loss: 0.047876. Entropy: 0.346297.\n",
      "now time :  2019-09-28 10:21:27.772967\n",
      "episode: 101   score: 410.0  epsilon: 1.0    steps: 120  evaluation reward: 200.75\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 256: Policy loss: 0.236543. Value loss: 0.088136. Entropy: 0.345037.\n",
      "Iteration 257: Policy loss: 0.230607. Value loss: 0.030779. Entropy: 0.344509.\n",
      "Iteration 258: Policy loss: 0.230523. Value loss: 0.021418. Entropy: 0.344784.\n",
      "episode: 102   score: 155.0  epsilon: 1.0    steps: 176  evaluation reward: 201.85\n",
      "episode: 103   score: 185.0  epsilon: 1.0    steps: 568  evaluation reward: 202.35\n",
      "episode: 104   score: 210.0  epsilon: 1.0    steps: 888  evaluation reward: 203.25\n",
      "Training network. lr: 0.000248. clip: 0.099235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 259: Policy loss: 0.003642. Value loss: 0.089019. Entropy: 0.347699.\n",
      "Iteration 260: Policy loss: 0.005599. Value loss: 0.045933. Entropy: 0.347633.\n",
      "Iteration 261: Policy loss: 0.008168. Value loss: 0.037941. Entropy: 0.346802.\n",
      "episode: 105   score: 180.0  epsilon: 1.0    steps: 232  evaluation reward: 203.95\n",
      "episode: 106   score: 360.0  epsilon: 1.0    steps: 864  evaluation reward: 203.45\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 262: Policy loss: -0.057051. Value loss: 0.173330. Entropy: 0.346794.\n",
      "Iteration 263: Policy loss: -0.075035. Value loss: 0.136113. Entropy: 0.348440.\n",
      "Iteration 264: Policy loss: -0.079130. Value loss: 0.100282. Entropy: 0.347378.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 265: Policy loss: -0.037533. Value loss: 0.051897. Entropy: 0.346597.\n",
      "Iteration 266: Policy loss: -0.039830. Value loss: 0.027704. Entropy: 0.346949.\n",
      "Iteration 267: Policy loss: -0.039413. Value loss: 0.024909. Entropy: 0.346945.\n",
      "episode: 107   score: 105.0  epsilon: 1.0    steps: 112  evaluation reward: 200.7\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 268: Policy loss: 0.029811. Value loss: 0.060572. Entropy: 0.345873.\n",
      "Iteration 269: Policy loss: 0.023627. Value loss: 0.026805. Entropy: 0.345606.\n",
      "Iteration 270: Policy loss: 0.017300. Value loss: 0.021424. Entropy: 0.346047.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 271: Policy loss: -0.034974. Value loss: 0.049507. Entropy: 0.346449.\n",
      "Iteration 272: Policy loss: -0.038358. Value loss: 0.018183. Entropy: 0.345253.\n",
      "Iteration 273: Policy loss: -0.035337. Value loss: 0.013934. Entropy: 0.344890.\n",
      "episode: 108   score: 150.0  epsilon: 1.0    steps: 280  evaluation reward: 198.45\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 274: Policy loss: 0.083438. Value loss: 0.064772. Entropy: 0.341796.\n",
      "Iteration 275: Policy loss: 0.078525. Value loss: 0.024049. Entropy: 0.342312.\n",
      "Iteration 276: Policy loss: 0.078444. Value loss: 0.017093. Entropy: 0.341155.\n",
      "episode: 109   score: 180.0  epsilon: 1.0    steps: 944  evaluation reward: 198.7\n",
      "episode: 110   score: 105.0  epsilon: 1.0    steps: 1000  evaluation reward: 198.7\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 277: Policy loss: 0.328613. Value loss: 0.088318. Entropy: 0.346449.\n",
      "Iteration 278: Policy loss: 0.323430. Value loss: 0.035677. Entropy: 0.346589.\n",
      "Iteration 279: Policy loss: 0.314105. Value loss: 0.025652. Entropy: 0.345528.\n",
      "episode: 111   score: 135.0  epsilon: 1.0    steps: 8  evaluation reward: 198.25\n",
      "episode: 112   score: 155.0  epsilon: 1.0    steps: 968  evaluation reward: 198.25\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 280: Policy loss: -0.213765. Value loss: 0.212528. Entropy: 0.346456.\n",
      "Iteration 281: Policy loss: -0.211294. Value loss: 0.060592. Entropy: 0.344751.\n",
      "Iteration 282: Policy loss: -0.218784. Value loss: 0.047558. Entropy: 0.345892.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 283: Policy loss: 0.064939. Value loss: 0.084529. Entropy: 0.345634.\n",
      "Iteration 284: Policy loss: 0.066904. Value loss: 0.047972. Entropy: 0.345393.\n",
      "Iteration 285: Policy loss: 0.066277. Value loss: 0.034642. Entropy: 0.344351.\n",
      "episode: 113   score: 240.0  epsilon: 1.0    steps: 112  evaluation reward: 199.9\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 286: Policy loss: -0.110710. Value loss: 0.038587. Entropy: 0.344908.\n",
      "Iteration 287: Policy loss: -0.113397. Value loss: 0.021373. Entropy: 0.344900.\n",
      "Iteration 288: Policy loss: -0.116320. Value loss: 0.015617. Entropy: 0.344491.\n",
      "episode: 114   score: 260.0  epsilon: 1.0    steps: 880  evaluation reward: 200.7\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 289: Policy loss: 0.068872. Value loss: 0.129680. Entropy: 0.345591.\n",
      "Iteration 290: Policy loss: 0.064113. Value loss: 0.042773. Entropy: 0.344757.\n",
      "Iteration 291: Policy loss: 0.052416. Value loss: 0.031398. Entropy: 0.344152.\n",
      "episode: 115   score: 485.0  epsilon: 1.0    steps: 416  evaluation reward: 204.2\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 292: Policy loss: 0.205336. Value loss: 0.068817. Entropy: 0.341463.\n",
      "Iteration 293: Policy loss: 0.198093. Value loss: 0.017298. Entropy: 0.342676.\n",
      "Iteration 294: Policy loss: 0.193067. Value loss: 0.012040. Entropy: 0.341921.\n",
      "episode: 116   score: 155.0  epsilon: 1.0    steps: 248  evaluation reward: 204.65\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 295: Policy loss: 0.066314. Value loss: 0.110798. Entropy: 0.343939.\n",
      "Iteration 296: Policy loss: 0.055959. Value loss: 0.062558. Entropy: 0.343055.\n",
      "Iteration 297: Policy loss: 0.058234. Value loss: 0.032943. Entropy: 0.342854.\n",
      "episode: 117   score: 180.0  epsilon: 1.0    steps: 208  evaluation reward: 204.9\n",
      "episode: 118   score: 210.0  epsilon: 1.0    steps: 536  evaluation reward: 204.9\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 298: Policy loss: 0.086411. Value loss: 0.046217. Entropy: 0.343365.\n",
      "Iteration 299: Policy loss: 0.079742. Value loss: 0.029602. Entropy: 0.342817.\n",
      "Iteration 300: Policy loss: 0.075972. Value loss: 0.026614. Entropy: 0.342295.\n",
      "episode: 119   score: 195.0  epsilon: 1.0    steps: 336  evaluation reward: 204.75\n",
      "episode: 120   score: 180.0  epsilon: 1.0    steps: 528  evaluation reward: 205.0\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 301: Policy loss: -0.014017. Value loss: 0.070302. Entropy: 0.346814.\n",
      "Iteration 302: Policy loss: -0.017829. Value loss: 0.038797. Entropy: 0.346210.\n",
      "Iteration 303: Policy loss: -0.019144. Value loss: 0.028989. Entropy: 0.345861.\n",
      "episode: 121   score: 180.0  epsilon: 1.0    steps: 480  evaluation reward: 204.25\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 304: Policy loss: -0.025356. Value loss: 0.085981. Entropy: 0.344461.\n",
      "Iteration 305: Policy loss: -0.027480. Value loss: 0.040226. Entropy: 0.344781.\n",
      "Iteration 306: Policy loss: -0.028493. Value loss: 0.031238. Entropy: 0.344088.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 307: Policy loss: 0.058739. Value loss: 0.041941. Entropy: 0.345007.\n",
      "Iteration 308: Policy loss: 0.054740. Value loss: 0.017274. Entropy: 0.346042.\n",
      "Iteration 309: Policy loss: 0.051744. Value loss: 0.012382. Entropy: 0.345132.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 310: Policy loss: -0.345995. Value loss: 0.328207. Entropy: 0.344903.\n",
      "Iteration 311: Policy loss: -0.378228. Value loss: 0.196259. Entropy: 0.342387.\n",
      "Iteration 312: Policy loss: -0.349853. Value loss: 0.107690. Entropy: 0.343805.\n",
      "episode: 122   score: 210.0  epsilon: 1.0    steps: 136  evaluation reward: 202.05\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 313: Policy loss: -0.317049. Value loss: 0.372879. Entropy: 0.345730.\n",
      "Iteration 314: Policy loss: -0.328353. Value loss: 0.276317. Entropy: 0.344359.\n",
      "Iteration 315: Policy loss: -0.332466. Value loss: 0.230793. Entropy: 0.344278.\n",
      "episode: 123   score: 380.0  epsilon: 1.0    steps: 984  evaluation reward: 202.65\n",
      "episode: 124   score: 150.0  epsilon: 1.0    steps: 1008  evaluation reward: 202.75\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 316: Policy loss: 0.183032. Value loss: 0.096282. Entropy: 0.342735.\n",
      "Iteration 317: Policy loss: 0.182504. Value loss: 0.041294. Entropy: 0.343472.\n",
      "Iteration 318: Policy loss: 0.180581. Value loss: 0.029758. Entropy: 0.342367.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 319: Policy loss: 0.034380. Value loss: 0.081977. Entropy: 0.342107.\n",
      "Iteration 320: Policy loss: 0.032174. Value loss: 0.035904. Entropy: 0.342778.\n",
      "Iteration 321: Policy loss: 0.027344. Value loss: 0.027478. Entropy: 0.341917.\n",
      "episode: 125   score: 155.0  epsilon: 1.0    steps: 16  evaluation reward: 203.25\n",
      "episode: 126   score: 210.0  epsilon: 1.0    steps: 664  evaluation reward: 203.55\n",
      "episode: 127   score: 410.0  epsilon: 1.0    steps: 744  evaluation reward: 202.5\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 322: Policy loss: -0.125408. Value loss: 0.127225. Entropy: 0.345149.\n",
      "Iteration 323: Policy loss: -0.133949. Value loss: 0.056734. Entropy: 0.345792.\n",
      "Iteration 324: Policy loss: -0.131509. Value loss: 0.042094. Entropy: 0.344588.\n",
      "episode: 128   score: 330.0  epsilon: 1.0    steps: 112  evaluation reward: 201.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 129   score: 360.0  epsilon: 1.0    steps: 776  evaluation reward: 202.4\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 325: Policy loss: 0.019790. Value loss: 0.078088. Entropy: 0.345850.\n",
      "Iteration 326: Policy loss: 0.019424. Value loss: 0.049421. Entropy: 0.345735.\n",
      "Iteration 327: Policy loss: 0.024393. Value loss: 0.039805. Entropy: 0.344593.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 328: Policy loss: 0.011825. Value loss: 0.096937. Entropy: 0.343039.\n",
      "Iteration 329: Policy loss: 0.004697. Value loss: 0.049206. Entropy: 0.342225.\n",
      "Iteration 330: Policy loss: 0.006841. Value loss: 0.042685. Entropy: 0.341256.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 331: Policy loss: 0.032389. Value loss: 0.022956. Entropy: 0.344992.\n",
      "Iteration 332: Policy loss: 0.025535. Value loss: 0.009893. Entropy: 0.344150.\n",
      "Iteration 333: Policy loss: 0.027717. Value loss: 0.008240. Entropy: 0.343644.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 334: Policy loss: 0.182348. Value loss: 0.050082. Entropy: 0.340534.\n",
      "Iteration 335: Policy loss: 0.179687. Value loss: 0.025025. Entropy: 0.338951.\n",
      "Iteration 336: Policy loss: 0.178180. Value loss: 0.019025. Entropy: 0.338215.\n",
      "episode: 130   score: 155.0  epsilon: 1.0    steps: 224  evaluation reward: 202.9\n",
      "episode: 131   score: 245.0  epsilon: 1.0    steps: 360  evaluation reward: 201.45\n",
      "episode: 132   score: 210.0  epsilon: 1.0    steps: 448  evaluation reward: 201.9\n",
      "episode: 133   score: 105.0  epsilon: 1.0    steps: 880  evaluation reward: 201.6\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 337: Policy loss: -0.457192. Value loss: 0.418682. Entropy: 0.337568.\n",
      "Iteration 338: Policy loss: -0.458114. Value loss: 0.240857. Entropy: 0.338817.\n",
      "Iteration 339: Policy loss: -0.462535. Value loss: 0.173721. Entropy: 0.338913.\n",
      "episode: 134   score: 110.0  epsilon: 1.0    steps: 672  evaluation reward: 201.1\n",
      "episode: 135   score: 210.0  epsilon: 1.0    steps: 736  evaluation reward: 202.1\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 340: Policy loss: 0.262398. Value loss: 0.149324. Entropy: 0.340879.\n",
      "Iteration 341: Policy loss: 0.252047. Value loss: 0.075577. Entropy: 0.338779.\n",
      "Iteration 342: Policy loss: 0.248791. Value loss: 0.062457. Entropy: 0.339345.\n",
      "episode: 136   score: 410.0  epsilon: 1.0    steps: 456  evaluation reward: 204.65\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 343: Policy loss: 0.048957. Value loss: 0.057475. Entropy: 0.342331.\n",
      "Iteration 344: Policy loss: 0.050824. Value loss: 0.031225. Entropy: 0.341475.\n",
      "Iteration 345: Policy loss: 0.051668. Value loss: 0.022183. Entropy: 0.341185.\n",
      "episode: 137   score: 50.0  epsilon: 1.0    steps: 584  evaluation reward: 203.05\n",
      "episode: 138   score: 440.0  epsilon: 1.0    steps: 600  evaluation reward: 204.1\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 346: Policy loss: 0.158903. Value loss: 0.051640. Entropy: 0.342386.\n",
      "Iteration 347: Policy loss: 0.161084. Value loss: 0.032399. Entropy: 0.343160.\n",
      "Iteration 348: Policy loss: 0.155667. Value loss: 0.027535. Entropy: 0.342235.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 349: Policy loss: 0.006881. Value loss: 0.045909. Entropy: 0.345102.\n",
      "Iteration 350: Policy loss: 0.005625. Value loss: 0.023440. Entropy: 0.344382.\n",
      "Iteration 351: Policy loss: 0.003293. Value loss: 0.013684. Entropy: 0.343444.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 352: Policy loss: 0.059476. Value loss: 0.059138. Entropy: 0.344291.\n",
      "Iteration 353: Policy loss: 0.051588. Value loss: 0.031624. Entropy: 0.345206.\n",
      "Iteration 354: Policy loss: 0.048543. Value loss: 0.021130. Entropy: 0.344913.\n",
      "episode: 139   score: 210.0  epsilon: 1.0    steps: 568  evaluation reward: 205.15\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 355: Policy loss: -0.072647. Value loss: 0.090591. Entropy: 0.343699.\n",
      "Iteration 356: Policy loss: -0.090790. Value loss: 0.036490. Entropy: 0.346060.\n",
      "Iteration 357: Policy loss: -0.087125. Value loss: 0.028483. Entropy: 0.345688.\n",
      "episode: 140   score: 210.0  epsilon: 1.0    steps: 480  evaluation reward: 206.05\n",
      "episode: 141   score: 150.0  epsilon: 1.0    steps: 728  evaluation reward: 206.0\n",
      "episode: 142   score: 250.0  epsilon: 1.0    steps: 960  evaluation reward: 206.4\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 358: Policy loss: -0.081145. Value loss: 0.154057. Entropy: 0.346533.\n",
      "Iteration 359: Policy loss: -0.090181. Value loss: 0.086366. Entropy: 0.346357.\n",
      "Iteration 360: Policy loss: -0.093936. Value loss: 0.057271. Entropy: 0.346864.\n",
      "episode: 143   score: 210.0  epsilon: 1.0    steps: 280  evaluation reward: 206.95\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 361: Policy loss: 0.244520. Value loss: 0.095902. Entropy: 0.343932.\n",
      "Iteration 362: Policy loss: 0.236823. Value loss: 0.049666. Entropy: 0.342636.\n",
      "Iteration 363: Policy loss: 0.227170. Value loss: 0.038799. Entropy: 0.342312.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 364: Policy loss: 0.016938. Value loss: 0.097834. Entropy: 0.345240.\n",
      "Iteration 365: Policy loss: 0.006356. Value loss: 0.046654. Entropy: 0.343917.\n",
      "Iteration 366: Policy loss: 0.002964. Value loss: 0.036709. Entropy: 0.342933.\n",
      "episode: 144   score: 210.0  epsilon: 1.0    steps: 304  evaluation reward: 207.25\n",
      "episode: 145   score: 80.0  epsilon: 1.0    steps: 1008  evaluation reward: 205.4\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 367: Policy loss: 0.043923. Value loss: 0.043212. Entropy: 0.344689.\n",
      "Iteration 368: Policy loss: 0.041619. Value loss: 0.023593. Entropy: 0.343995.\n",
      "Iteration 369: Policy loss: 0.039336. Value loss: 0.019280. Entropy: 0.343967.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 370: Policy loss: 0.179349. Value loss: 0.104774. Entropy: 0.342726.\n",
      "Iteration 371: Policy loss: 0.106225. Value loss: 0.042615. Entropy: 0.338484.\n",
      "Iteration 372: Policy loss: 0.116514. Value loss: 0.028615. Entropy: 0.340596.\n",
      "episode: 146   score: 530.0  epsilon: 1.0    steps: 640  evaluation reward: 208.85\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 373: Policy loss: -0.155083. Value loss: 0.228345. Entropy: 0.342222.\n",
      "Iteration 374: Policy loss: -0.170294. Value loss: 0.098425. Entropy: 0.341706.\n",
      "Iteration 375: Policy loss: -0.161332. Value loss: 0.054545. Entropy: 0.341856.\n",
      "episode: 147   score: 155.0  epsilon: 1.0    steps: 784  evaluation reward: 208.8\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 376: Policy loss: 0.104905. Value loss: 0.069657. Entropy: 0.344153.\n",
      "Iteration 377: Policy loss: 0.096361. Value loss: 0.026181. Entropy: 0.343742.\n",
      "Iteration 378: Policy loss: 0.093216. Value loss: 0.020681. Entropy: 0.342653.\n",
      "episode: 148   score: 285.0  epsilon: 1.0    steps: 96  evaluation reward: 207.05\n",
      "episode: 149   score: 105.0  epsilon: 1.0    steps: 360  evaluation reward: 206.0\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 379: Policy loss: -0.326421. Value loss: 0.283375. Entropy: 0.341948.\n",
      "Iteration 380: Policy loss: -0.344003. Value loss: 0.157779. Entropy: 0.343076.\n",
      "Iteration 381: Policy loss: -0.337782. Value loss: 0.110118. Entropy: 0.342573.\n",
      "episode: 150   score: 470.0  epsilon: 1.0    steps: 432  evaluation reward: 209.95\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 382: Policy loss: 0.097773. Value loss: 0.066591. Entropy: 0.346151.\n",
      "Iteration 383: Policy loss: 0.090419. Value loss: 0.023435. Entropy: 0.345906.\n",
      "Iteration 384: Policy loss: 0.089545. Value loss: 0.020062. Entropy: 0.345287.\n",
      "now time :  2019-09-28 10:24:09.116761\n",
      "episode: 151   score: 440.0  epsilon: 1.0    steps: 848  evaluation reward: 214.05\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 385: Policy loss: 0.082909. Value loss: 0.042421. Entropy: 0.344146.\n",
      "Iteration 386: Policy loss: 0.081535. Value loss: 0.019733. Entropy: 0.343174.\n",
      "Iteration 387: Policy loss: 0.080120. Value loss: 0.014971. Entropy: 0.343437.\n",
      "episode: 152   score: 180.0  epsilon: 1.0    steps: 656  evaluation reward: 214.05\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 388: Policy loss: 0.191179. Value loss: 0.083058. Entropy: 0.344535.\n",
      "Iteration 389: Policy loss: 0.175767. Value loss: 0.026008. Entropy: 0.343148.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 390: Policy loss: 0.172374. Value loss: 0.018726. Entropy: 0.341855.\n",
      "episode: 153   score: 260.0  epsilon: 1.0    steps: 416  evaluation reward: 215.1\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 391: Policy loss: 0.091544. Value loss: 0.096255. Entropy: 0.343960.\n",
      "Iteration 392: Policy loss: 0.084675. Value loss: 0.035960. Entropy: 0.342808.\n",
      "Iteration 393: Policy loss: 0.085688. Value loss: 0.022250. Entropy: 0.343732.\n",
      "episode: 154   score: 180.0  epsilon: 1.0    steps: 128  evaluation reward: 213.35\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 394: Policy loss: -0.121226. Value loss: 0.354577. Entropy: 0.341397.\n",
      "Iteration 395: Policy loss: -0.126736. Value loss: 0.262716. Entropy: 0.341228.\n",
      "Iteration 396: Policy loss: -0.126209. Value loss: 0.205415. Entropy: 0.340648.\n",
      "episode: 155   score: 180.0  epsilon: 1.0    steps: 408  evaluation reward: 213.3\n",
      "episode: 156   score: 180.0  epsilon: 1.0    steps: 664  evaluation reward: 213.0\n",
      "episode: 157   score: 180.0  epsilon: 1.0    steps: 800  evaluation reward: 213.45\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 397: Policy loss: -0.020384. Value loss: 0.076494. Entropy: 0.346373.\n",
      "Iteration 398: Policy loss: -0.025051. Value loss: 0.039594. Entropy: 0.346392.\n",
      "Iteration 399: Policy loss: -0.032202. Value loss: 0.027771. Entropy: 0.346237.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 400: Policy loss: 0.063524. Value loss: 0.067458. Entropy: 0.343592.\n",
      "Iteration 401: Policy loss: 0.056372. Value loss: 0.036242. Entropy: 0.344693.\n",
      "Iteration 402: Policy loss: 0.051403. Value loss: 0.023135. Entropy: 0.343486.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 403: Policy loss: 0.102955. Value loss: 0.049348. Entropy: 0.341228.\n",
      "Iteration 404: Policy loss: 0.103012. Value loss: 0.027129. Entropy: 0.340620.\n",
      "Iteration 405: Policy loss: 0.101440. Value loss: 0.021329. Entropy: 0.339681.\n",
      "episode: 158   score: 260.0  epsilon: 1.0    steps: 48  evaluation reward: 214.85\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 406: Policy loss: 0.079527. Value loss: 0.035528. Entropy: 0.345858.\n",
      "Iteration 407: Policy loss: 0.068872. Value loss: 0.015753. Entropy: 0.346213.\n",
      "Iteration 408: Policy loss: 0.072472. Value loss: 0.012615. Entropy: 0.345983.\n",
      "episode: 159   score: 155.0  epsilon: 1.0    steps: 96  evaluation reward: 214.55\n",
      "episode: 160   score: 110.0  epsilon: 1.0    steps: 272  evaluation reward: 214.1\n",
      "episode: 161   score: 210.0  epsilon: 1.0    steps: 912  evaluation reward: 214.85\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 409: Policy loss: 0.074910. Value loss: 0.045924. Entropy: 0.340653.\n",
      "Iteration 410: Policy loss: 0.071692. Value loss: 0.025125. Entropy: 0.342565.\n",
      "Iteration 411: Policy loss: 0.068642. Value loss: 0.018624. Entropy: 0.342171.\n",
      "episode: 162   score: 180.0  epsilon: 1.0    steps: 344  evaluation reward: 215.6\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 412: Policy loss: -0.292873. Value loss: 0.204861. Entropy: 0.337832.\n",
      "Iteration 413: Policy loss: -0.293233. Value loss: 0.079150. Entropy: 0.337898.\n",
      "Iteration 414: Policy loss: -0.312206. Value loss: 0.059384. Entropy: 0.336789.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 415: Policy loss: 0.022008. Value loss: 0.061710. Entropy: 0.338306.\n",
      "Iteration 416: Policy loss: 0.010677. Value loss: 0.031801. Entropy: 0.338790.\n",
      "Iteration 417: Policy loss: 0.011841. Value loss: 0.025534. Entropy: 0.339384.\n",
      "episode: 163   score: 180.0  epsilon: 1.0    steps: 128  evaluation reward: 216.05\n",
      "episode: 164   score: 380.0  epsilon: 1.0    steps: 200  evaluation reward: 218.35\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 418: Policy loss: -0.371319. Value loss: 0.280879. Entropy: 0.342437.\n",
      "Iteration 419: Policy loss: -0.350663. Value loss: 0.151052. Entropy: 0.341114.\n",
      "Iteration 420: Policy loss: -0.386562. Value loss: 0.114712. Entropy: 0.342457.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 421: Policy loss: 0.149430. Value loss: 0.114390. Entropy: 0.342024.\n",
      "Iteration 422: Policy loss: 0.141012. Value loss: 0.038195. Entropy: 0.343216.\n",
      "Iteration 423: Policy loss: 0.138378. Value loss: 0.025754. Entropy: 0.341674.\n",
      "episode: 165   score: 165.0  epsilon: 1.0    steps: 296  evaluation reward: 217.9\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 424: Policy loss: 0.236894. Value loss: 0.063683. Entropy: 0.347972.\n",
      "Iteration 425: Policy loss: 0.229648. Value loss: 0.020157. Entropy: 0.346224.\n",
      "Iteration 426: Policy loss: 0.224265. Value loss: 0.016194. Entropy: 0.347192.\n",
      "episode: 166   score: 210.0  epsilon: 1.0    steps: 728  evaluation reward: 218.45\n",
      "episode: 167   score: 515.0  epsilon: 1.0    steps: 1000  evaluation reward: 221.8\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 427: Policy loss: -0.263407. Value loss: 0.350404. Entropy: 0.344212.\n",
      "Iteration 428: Policy loss: -0.264294. Value loss: 0.180011. Entropy: 0.345040.\n",
      "Iteration 429: Policy loss: -0.294725. Value loss: 0.129525. Entropy: 0.346460.\n",
      "episode: 168   score: 485.0  epsilon: 1.0    steps: 936  evaluation reward: 222.85\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 430: Policy loss: -0.009892. Value loss: 0.083710. Entropy: 0.340933.\n",
      "Iteration 431: Policy loss: -0.015120. Value loss: 0.032443. Entropy: 0.341374.\n",
      "Iteration 432: Policy loss: -0.024312. Value loss: 0.022127. Entropy: 0.340721.\n",
      "episode: 169   score: 185.0  epsilon: 1.0    steps: 768  evaluation reward: 222.85\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 433: Policy loss: -0.083499. Value loss: 0.082518. Entropy: 0.343638.\n",
      "Iteration 434: Policy loss: -0.098357. Value loss: 0.039308. Entropy: 0.342785.\n",
      "Iteration 435: Policy loss: -0.095504. Value loss: 0.031887. Entropy: 0.341492.\n",
      "episode: 170   score: 255.0  epsilon: 1.0    steps: 424  evaluation reward: 224.3\n",
      "episode: 171   score: 185.0  epsilon: 1.0    steps: 592  evaluation reward: 224.35\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 436: Policy loss: 0.253478. Value loss: 0.073983. Entropy: 0.342445.\n",
      "Iteration 437: Policy loss: 0.250697. Value loss: 0.028115. Entropy: 0.341629.\n",
      "Iteration 438: Policy loss: 0.243020. Value loss: 0.022320. Entropy: 0.341806.\n",
      "episode: 172   score: 440.0  epsilon: 1.0    steps: 736  evaluation reward: 227.4\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 439: Policy loss: 0.136305. Value loss: 0.098041. Entropy: 0.343837.\n",
      "Iteration 440: Policy loss: 0.136968. Value loss: 0.045853. Entropy: 0.342738.\n",
      "Iteration 441: Policy loss: 0.129869. Value loss: 0.034350. Entropy: 0.342902.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 442: Policy loss: -0.050801. Value loss: 0.069784. Entropy: 0.343454.\n",
      "Iteration 443: Policy loss: -0.058185. Value loss: 0.037113. Entropy: 0.343676.\n",
      "Iteration 444: Policy loss: -0.057015. Value loss: 0.030674. Entropy: 0.343387.\n",
      "episode: 173   score: 210.0  epsilon: 1.0    steps: 112  evaluation reward: 227.85\n",
      "episode: 174   score: 135.0  epsilon: 1.0    steps: 856  evaluation reward: 226.0\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 445: Policy loss: 0.312448. Value loss: 0.100070. Entropy: 0.343572.\n",
      "Iteration 446: Policy loss: 0.315691. Value loss: 0.044089. Entropy: 0.343999.\n",
      "Iteration 447: Policy loss: 0.309687. Value loss: 0.036713. Entropy: 0.343978.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 448: Policy loss: -0.022678. Value loss: 0.055707. Entropy: 0.342076.\n",
      "Iteration 449: Policy loss: -0.026586. Value loss: 0.027552. Entropy: 0.342895.\n",
      "Iteration 450: Policy loss: -0.028838. Value loss: 0.020781. Entropy: 0.342292.\n",
      "episode: 175   score: 180.0  epsilon: 1.0    steps: 464  evaluation reward: 225.4\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 451: Policy loss: 0.114226. Value loss: 0.102453. Entropy: 0.345354.\n",
      "Iteration 452: Policy loss: 0.104434. Value loss: 0.051038. Entropy: 0.344782.\n",
      "Iteration 453: Policy loss: 0.097650. Value loss: 0.040731. Entropy: 0.344949.\n",
      "episode: 176   score: 225.0  epsilon: 1.0    steps: 224  evaluation reward: 224.1\n",
      "episode: 177   score: 320.0  epsilon: 1.0    steps: 376  evaluation reward: 225.2\n",
      "episode: 178   score: 180.0  epsilon: 1.0    steps: 736  evaluation reward: 225.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 454: Policy loss: 0.012910. Value loss: 0.058570. Entropy: 0.346831.\n",
      "Iteration 455: Policy loss: 0.006990. Value loss: 0.026835. Entropy: 0.347496.\n",
      "Iteration 456: Policy loss: 0.007701. Value loss: 0.021190. Entropy: 0.348011.\n",
      "episode: 179   score: 180.0  epsilon: 1.0    steps: 408  evaluation reward: 225.9\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 457: Policy loss: -0.001848. Value loss: 0.076415. Entropy: 0.344711.\n",
      "Iteration 458: Policy loss: -0.010345. Value loss: 0.044428. Entropy: 0.342820.\n",
      "Iteration 459: Policy loss: -0.011946. Value loss: 0.034062. Entropy: 0.342836.\n",
      "episode: 180   score: 255.0  epsilon: 1.0    steps: 680  evaluation reward: 226.35\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 460: Policy loss: 0.011668. Value loss: 0.043594. Entropy: 0.343571.\n",
      "Iteration 461: Policy loss: 0.009020. Value loss: 0.025539. Entropy: 0.343418.\n",
      "Iteration 462: Policy loss: 0.010595. Value loss: 0.020370. Entropy: 0.342631.\n",
      "episode: 181   score: 210.0  epsilon: 1.0    steps: 864  evaluation reward: 227.7\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 463: Policy loss: 0.058511. Value loss: 0.032969. Entropy: 0.342466.\n",
      "Iteration 464: Policy loss: 0.055616. Value loss: 0.013691. Entropy: 0.342900.\n",
      "Iteration 465: Policy loss: 0.054949. Value loss: 0.009799. Entropy: 0.341777.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 466: Policy loss: -0.242570. Value loss: 0.265635. Entropy: 0.343049.\n",
      "Iteration 467: Policy loss: -0.261914. Value loss: 0.145359. Entropy: 0.344730.\n",
      "Iteration 468: Policy loss: -0.266949. Value loss: 0.111714. Entropy: 0.345222.\n",
      "episode: 182   score: 170.0  epsilon: 1.0    steps: 472  evaluation reward: 226.8\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 469: Policy loss: 0.172398. Value loss: 0.136763. Entropy: 0.347023.\n",
      "Iteration 470: Policy loss: 0.165727. Value loss: 0.034635. Entropy: 0.346690.\n",
      "Iteration 471: Policy loss: 0.152332. Value loss: 0.022591. Entropy: 0.346320.\n",
      "episode: 183   score: 380.0  epsilon: 1.0    steps: 488  evaluation reward: 229.5\n",
      "episode: 184   score: 210.0  epsilon: 1.0    steps: 824  evaluation reward: 230.05\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 472: Policy loss: 0.067282. Value loss: 0.122815. Entropy: 0.346634.\n",
      "Iteration 473: Policy loss: 0.063361. Value loss: 0.051778. Entropy: 0.346286.\n",
      "Iteration 474: Policy loss: 0.057418. Value loss: 0.038258. Entropy: 0.346114.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 475: Policy loss: -0.229364. Value loss: 0.355398. Entropy: 0.344325.\n",
      "Iteration 476: Policy loss: -0.242550. Value loss: 0.279720. Entropy: 0.341144.\n",
      "Iteration 477: Policy loss: -0.251488. Value loss: 0.238761. Entropy: 0.342430.\n",
      "episode: 185   score: 210.0  epsilon: 1.0    steps: 192  evaluation reward: 228.05\n",
      "episode: 186   score: 465.0  epsilon: 1.0    steps: 232  evaluation reward: 231.35\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 478: Policy loss: 0.075621. Value loss: 0.055648. Entropy: 0.344655.\n",
      "Iteration 479: Policy loss: 0.067863. Value loss: 0.028578. Entropy: 0.343187.\n",
      "Iteration 480: Policy loss: 0.065021. Value loss: 0.024156. Entropy: 0.342589.\n",
      "episode: 187   score: 210.0  epsilon: 1.0    steps: 184  evaluation reward: 232.4\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 481: Policy loss: 0.123369. Value loss: 0.060837. Entropy: 0.346494.\n",
      "Iteration 482: Policy loss: 0.116147. Value loss: 0.030443. Entropy: 0.346029.\n",
      "Iteration 483: Policy loss: 0.115632. Value loss: 0.027605. Entropy: 0.345845.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 484: Policy loss: 0.034588. Value loss: 0.053608. Entropy: 0.345312.\n",
      "Iteration 485: Policy loss: 0.033987. Value loss: 0.024317. Entropy: 0.344184.\n",
      "Iteration 486: Policy loss: 0.032647. Value loss: 0.016369. Entropy: 0.343675.\n",
      "episode: 188   score: 225.0  epsilon: 1.0    steps: 560  evaluation reward: 233.6\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 487: Policy loss: 0.237660. Value loss: 0.041807. Entropy: 0.344537.\n",
      "Iteration 488: Policy loss: 0.239859. Value loss: 0.014502. Entropy: 0.343799.\n",
      "Iteration 489: Policy loss: 0.234607. Value loss: 0.010185. Entropy: 0.344242.\n",
      "episode: 189   score: 180.0  epsilon: 1.0    steps: 984  evaluation reward: 233.85\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 490: Policy loss: -0.403030. Value loss: 0.385937. Entropy: 0.345573.\n",
      "Iteration 491: Policy loss: -0.407163. Value loss: 0.253676. Entropy: 0.343608.\n",
      "Iteration 492: Policy loss: -0.410649. Value loss: 0.185329. Entropy: 0.343991.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 493: Policy loss: 0.095244. Value loss: 0.067142. Entropy: 0.347752.\n",
      "Iteration 494: Policy loss: 0.089952. Value loss: 0.027782. Entropy: 0.347372.\n",
      "Iteration 495: Policy loss: 0.085319. Value loss: 0.020070. Entropy: 0.346643.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 496: Policy loss: 0.271951. Value loss: 0.066399. Entropy: 0.347168.\n",
      "Iteration 497: Policy loss: 0.270134. Value loss: 0.030127. Entropy: 0.346553.\n",
      "Iteration 498: Policy loss: 0.268881. Value loss: 0.024311. Entropy: 0.345517.\n",
      "episode: 190   score: 225.0  epsilon: 1.0    steps: 504  evaluation reward: 234.3\n",
      "episode: 191   score: 285.0  epsilon: 1.0    steps: 800  evaluation reward: 236.1\n",
      "episode: 192   score: 490.0  epsilon: 1.0    steps: 904  evaluation reward: 237.2\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 499: Policy loss: -0.262958. Value loss: 0.313613. Entropy: 0.345670.\n",
      "Iteration 500: Policy loss: -0.279461. Value loss: 0.238806. Entropy: 0.346009.\n",
      "Iteration 501: Policy loss: -0.277015. Value loss: 0.155248. Entropy: 0.345379.\n",
      "episode: 193   score: 335.0  epsilon: 1.0    steps: 368  evaluation reward: 238.45\n",
      "episode: 194   score: 105.0  epsilon: 1.0    steps: 1024  evaluation reward: 237.95\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 502: Policy loss: 0.052416. Value loss: 0.080558. Entropy: 0.343538.\n",
      "Iteration 503: Policy loss: 0.051718. Value loss: 0.030700. Entropy: 0.342546.\n",
      "Iteration 504: Policy loss: 0.047083. Value loss: 0.018699. Entropy: 0.342026.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 505: Policy loss: 0.066941. Value loss: 0.056868. Entropy: 0.342700.\n",
      "Iteration 506: Policy loss: 0.062561. Value loss: 0.027177. Entropy: 0.342294.\n",
      "Iteration 507: Policy loss: 0.058290. Value loss: 0.021075. Entropy: 0.342219.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 508: Policy loss: -0.018457. Value loss: 0.118676. Entropy: 0.341733.\n",
      "Iteration 509: Policy loss: -0.028365. Value loss: 0.035799. Entropy: 0.342858.\n",
      "Iteration 510: Policy loss: -0.027630. Value loss: 0.027365. Entropy: 0.343034.\n",
      "episode: 195   score: 180.0  epsilon: 1.0    steps: 968  evaluation reward: 235.65\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 511: Policy loss: -0.153986. Value loss: 0.289033. Entropy: 0.343609.\n",
      "Iteration 512: Policy loss: -0.145992. Value loss: 0.096105. Entropy: 0.343214.\n",
      "Iteration 513: Policy loss: -0.176477. Value loss: 0.048569. Entropy: 0.343941.\n",
      "episode: 196   score: 530.0  epsilon: 1.0    steps: 792  evaluation reward: 237.8\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 514: Policy loss: 0.108842. Value loss: 0.062299. Entropy: 0.344479.\n",
      "Iteration 515: Policy loss: 0.105079. Value loss: 0.019366. Entropy: 0.341695.\n",
      "Iteration 516: Policy loss: 0.098258. Value loss: 0.015057. Entropy: 0.341228.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 517: Policy loss: -0.020099. Value loss: 0.068877. Entropy: 0.340203.\n",
      "Iteration 518: Policy loss: -0.019110. Value loss: 0.034629. Entropy: 0.340795.\n",
      "Iteration 519: Policy loss: -0.023117. Value loss: 0.024350. Entropy: 0.340831.\n",
      "episode: 197   score: 815.0  epsilon: 1.0    steps: 584  evaluation reward: 244.15\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 520: Policy loss: 0.189666. Value loss: 0.127068. Entropy: 0.342498.\n",
      "Iteration 521: Policy loss: 0.190485. Value loss: 0.061015. Entropy: 0.341412.\n",
      "Iteration 522: Policy loss: 0.185458. Value loss: 0.047735. Entropy: 0.342286.\n",
      "episode: 198   score: 240.0  epsilon: 1.0    steps: 24  evaluation reward: 244.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 199   score: 270.0  epsilon: 1.0    steps: 248  evaluation reward: 245.65\n",
      "episode: 200   score: 240.0  epsilon: 1.0    steps: 624  evaluation reward: 245.95\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 523: Policy loss: -0.016730. Value loss: 0.039528. Entropy: 0.344132.\n",
      "Iteration 524: Policy loss: -0.019712. Value loss: 0.015304. Entropy: 0.344122.\n",
      "Iteration 525: Policy loss: -0.020364. Value loss: 0.011282. Entropy: 0.344045.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 526: Policy loss: 0.013520. Value loss: 0.079606. Entropy: 0.339996.\n",
      "Iteration 527: Policy loss: 0.017754. Value loss: 0.037295. Entropy: 0.340285.\n",
      "Iteration 528: Policy loss: 0.009127. Value loss: 0.027224. Entropy: 0.337733.\n",
      "now time :  2019-09-28 10:27:05.452005\n",
      "episode: 201   score: 205.0  epsilon: 1.0    steps: 944  evaluation reward: 243.9\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 529: Policy loss: -0.061711. Value loss: 0.244656. Entropy: 0.345182.\n",
      "Iteration 530: Policy loss: -0.052382. Value loss: 0.178266. Entropy: 0.344983.\n",
      "Iteration 531: Policy loss: -0.069649. Value loss: 0.071141. Entropy: 0.344486.\n",
      "episode: 202   score: 210.0  epsilon: 1.0    steps: 1008  evaluation reward: 244.45\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 532: Policy loss: 0.100232. Value loss: 0.089634. Entropy: 0.344711.\n",
      "Iteration 533: Policy loss: 0.095802. Value loss: 0.029072. Entropy: 0.342931.\n",
      "Iteration 534: Policy loss: 0.093829. Value loss: 0.020891. Entropy: 0.343525.\n",
      "episode: 203   score: 105.0  epsilon: 1.0    steps: 192  evaluation reward: 243.65\n",
      "episode: 204   score: 210.0  epsilon: 1.0    steps: 512  evaluation reward: 243.65\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 535: Policy loss: 0.016610. Value loss: 0.088583. Entropy: 0.341191.\n",
      "Iteration 536: Policy loss: 0.015428. Value loss: 0.035609. Entropy: 0.341758.\n",
      "Iteration 537: Policy loss: 0.016240. Value loss: 0.025035. Entropy: 0.341427.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 538: Policy loss: -0.105955. Value loss: 0.379954. Entropy: 0.340575.\n",
      "Iteration 539: Policy loss: -0.124170. Value loss: 0.277447. Entropy: 0.339351.\n",
      "Iteration 540: Policy loss: -0.128964. Value loss: 0.221864. Entropy: 0.338857.\n",
      "episode: 205   score: 180.0  epsilon: 1.0    steps: 216  evaluation reward: 243.65\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 541: Policy loss: -0.287055. Value loss: 0.319623. Entropy: 0.345943.\n",
      "Iteration 542: Policy loss: -0.306179. Value loss: 0.180165. Entropy: 0.346232.\n",
      "Iteration 543: Policy loss: -0.296119. Value loss: 0.114930. Entropy: 0.344446.\n",
      "episode: 206   score: 210.0  epsilon: 1.0    steps: 144  evaluation reward: 242.15\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 544: Policy loss: -0.030584. Value loss: 0.177840. Entropy: 0.344631.\n",
      "Iteration 545: Policy loss: -0.031811. Value loss: 0.124228. Entropy: 0.344400.\n",
      "Iteration 546: Policy loss: -0.034085. Value loss: 0.115007. Entropy: 0.343594.\n",
      "episode: 207   score: 685.0  epsilon: 1.0    steps: 120  evaluation reward: 247.95\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 547: Policy loss: 0.071150. Value loss: 0.052529. Entropy: 0.341763.\n",
      "Iteration 548: Policy loss: 0.069439. Value loss: 0.022205. Entropy: 0.342371.\n",
      "Iteration 549: Policy loss: 0.068686. Value loss: 0.015078. Entropy: 0.341853.\n",
      "episode: 208   score: 120.0  epsilon: 1.0    steps: 224  evaluation reward: 247.65\n",
      "episode: 209   score: 50.0  epsilon: 1.0    steps: 280  evaluation reward: 246.35\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 550: Policy loss: 0.047677. Value loss: 0.081819. Entropy: 0.342937.\n",
      "Iteration 551: Policy loss: 0.038172. Value loss: 0.031445. Entropy: 0.342589.\n",
      "Iteration 552: Policy loss: 0.042592. Value loss: 0.021428. Entropy: 0.343293.\n",
      "episode: 210   score: 210.0  epsilon: 1.0    steps: 1024  evaluation reward: 247.4\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 553: Policy loss: -0.001916. Value loss: 0.072239. Entropy: 0.342182.\n",
      "Iteration 554: Policy loss: -0.017999. Value loss: 0.031041. Entropy: 0.344066.\n",
      "Iteration 555: Policy loss: -0.019974. Value loss: 0.024936. Entropy: 0.342349.\n",
      "episode: 211   score: 460.0  epsilon: 1.0    steps: 800  evaluation reward: 250.65\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 556: Policy loss: -0.278512. Value loss: 0.213630. Entropy: 0.342472.\n",
      "Iteration 557: Policy loss: -0.294518. Value loss: 0.056602. Entropy: 0.343014.\n",
      "Iteration 558: Policy loss: -0.296926. Value loss: 0.039140. Entropy: 0.343121.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 559: Policy loss: 0.029123. Value loss: 0.104255. Entropy: 0.341127.\n",
      "Iteration 560: Policy loss: 0.023111. Value loss: 0.045201. Entropy: 0.341233.\n",
      "Iteration 561: Policy loss: 0.013850. Value loss: 0.034289. Entropy: 0.340313.\n",
      "episode: 212   score: 385.0  epsilon: 1.0    steps: 712  evaluation reward: 252.95\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 562: Policy loss: -0.474138. Value loss: 0.256092. Entropy: 0.342561.\n",
      "Iteration 563: Policy loss: -0.508125. Value loss: 0.177413. Entropy: 0.341745.\n",
      "Iteration 564: Policy loss: -0.495767. Value loss: 0.126242. Entropy: 0.342117.\n",
      "episode: 213   score: 105.0  epsilon: 1.0    steps: 240  evaluation reward: 251.6\n",
      "episode: 214   score: 360.0  epsilon: 1.0    steps: 248  evaluation reward: 252.6\n",
      "episode: 215   score: 595.0  epsilon: 1.0    steps: 608  evaluation reward: 253.7\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 565: Policy loss: 0.296367. Value loss: 0.131532. Entropy: 0.341111.\n",
      "Iteration 566: Policy loss: 0.291196. Value loss: 0.036653. Entropy: 0.342027.\n",
      "Iteration 567: Policy loss: 0.275675. Value loss: 0.022435. Entropy: 0.340186.\n",
      "episode: 216   score: 260.0  epsilon: 1.0    steps: 168  evaluation reward: 254.75\n",
      "episode: 217   score: 125.0  epsilon: 1.0    steps: 312  evaluation reward: 254.2\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 568: Policy loss: 0.207871. Value loss: 0.068683. Entropy: 0.338803.\n",
      "Iteration 569: Policy loss: 0.202270. Value loss: 0.029088. Entropy: 0.334367.\n",
      "Iteration 570: Policy loss: 0.193835. Value loss: 0.021665. Entropy: 0.334795.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 571: Policy loss: 0.161254. Value loss: 0.134770. Entropy: 0.340071.\n",
      "Iteration 572: Policy loss: 0.155119. Value loss: 0.055630. Entropy: 0.336466.\n",
      "Iteration 573: Policy loss: 0.151703. Value loss: 0.039693. Entropy: 0.336921.\n",
      "episode: 218   score: 155.0  epsilon: 1.0    steps: 72  evaluation reward: 253.65\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 574: Policy loss: 0.073742. Value loss: 0.081487. Entropy: 0.345979.\n",
      "Iteration 575: Policy loss: 0.065048. Value loss: 0.044217. Entropy: 0.346202.\n",
      "Iteration 576: Policy loss: 0.071730. Value loss: 0.031579. Entropy: 0.346347.\n",
      "episode: 219   score: 210.0  epsilon: 1.0    steps: 544  evaluation reward: 253.8\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 577: Policy loss: 0.044684. Value loss: 0.072553. Entropy: 0.341469.\n",
      "Iteration 578: Policy loss: 0.046238. Value loss: 0.031089. Entropy: 0.340687.\n",
      "Iteration 579: Policy loss: 0.044805. Value loss: 0.023877. Entropy: 0.340945.\n",
      "episode: 220   score: 50.0  epsilon: 1.0    steps: 224  evaluation reward: 252.5\n",
      "episode: 221   score: 120.0  epsilon: 1.0    steps: 240  evaluation reward: 251.9\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 580: Policy loss: -0.509552. Value loss: 0.379607. Entropy: 0.339829.\n",
      "Iteration 581: Policy loss: -0.529346. Value loss: 0.154954. Entropy: 0.342418.\n",
      "Iteration 582: Policy loss: -0.523514. Value loss: 0.104185. Entropy: 0.340795.\n",
      "episode: 222   score: 380.0  epsilon: 1.0    steps: 776  evaluation reward: 253.6\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 583: Policy loss: -0.036267. Value loss: 0.074246. Entropy: 0.334100.\n",
      "Iteration 584: Policy loss: -0.039044. Value loss: 0.035587. Entropy: 0.334372.\n",
      "Iteration 585: Policy loss: -0.042442. Value loss: 0.021978. Entropy: 0.335342.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 586: Policy loss: -0.074205. Value loss: 0.185188. Entropy: 0.340726.\n",
      "Iteration 587: Policy loss: -0.096806. Value loss: 0.064488. Entropy: 0.340995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 588: Policy loss: -0.111398. Value loss: 0.028188. Entropy: 0.340499.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 589: Policy loss: -0.163624. Value loss: 0.246637. Entropy: 0.341675.\n",
      "Iteration 590: Policy loss: -0.204632. Value loss: 0.105078. Entropy: 0.343372.\n",
      "Iteration 591: Policy loss: -0.197396. Value loss: 0.070763. Entropy: 0.342029.\n",
      "episode: 223   score: 275.0  epsilon: 1.0    steps: 136  evaluation reward: 252.55\n",
      "episode: 224   score: 475.0  epsilon: 1.0    steps: 152  evaluation reward: 255.8\n",
      "episode: 225   score: 210.0  epsilon: 1.0    steps: 856  evaluation reward: 256.35\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 592: Policy loss: 0.125052. Value loss: 0.100530. Entropy: 0.346043.\n",
      "Iteration 593: Policy loss: 0.113857. Value loss: 0.034632. Entropy: 0.346211.\n",
      "Iteration 594: Policy loss: 0.115562. Value loss: 0.024891. Entropy: 0.345471.\n",
      "episode: 226   score: 210.0  epsilon: 1.0    steps: 912  evaluation reward: 256.35\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 595: Policy loss: 0.034733. Value loss: 0.104213. Entropy: 0.331552.\n",
      "Iteration 596: Policy loss: 0.035116. Value loss: 0.047687. Entropy: 0.330113.\n",
      "Iteration 597: Policy loss: 0.020536. Value loss: 0.033674. Entropy: 0.331232.\n",
      "episode: 227   score: 120.0  epsilon: 1.0    steps: 288  evaluation reward: 253.45\n",
      "episode: 228   score: 210.0  epsilon: 1.0    steps: 784  evaluation reward: 252.25\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 598: Policy loss: 0.057498. Value loss: 0.067184. Entropy: 0.339230.\n",
      "Iteration 599: Policy loss: 0.050284. Value loss: 0.027769. Entropy: 0.339666.\n",
      "Iteration 600: Policy loss: 0.052846. Value loss: 0.022380. Entropy: 0.339982.\n",
      "episode: 229   score: 545.0  epsilon: 1.0    steps: 200  evaluation reward: 254.1\n",
      "episode: 230   score: 120.0  epsilon: 1.0    steps: 792  evaluation reward: 253.75\n",
      "episode: 231   score: 50.0  epsilon: 1.0    steps: 864  evaluation reward: 251.8\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 601: Policy loss: -0.005752. Value loss: 0.100387. Entropy: 0.338995.\n",
      "Iteration 602: Policy loss: -0.014168. Value loss: 0.051688. Entropy: 0.338894.\n",
      "Iteration 603: Policy loss: -0.017642. Value loss: 0.036229. Entropy: 0.338224.\n",
      "episode: 232   score: 210.0  epsilon: 1.0    steps: 344  evaluation reward: 251.8\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 604: Policy loss: 0.014221. Value loss: 0.050356. Entropy: 0.339068.\n",
      "Iteration 605: Policy loss: 0.015691. Value loss: 0.022134. Entropy: 0.336234.\n",
      "Iteration 606: Policy loss: 0.011517. Value loss: 0.016371. Entropy: 0.337602.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 607: Policy loss: -0.009918. Value loss: 0.083554. Entropy: 0.342036.\n",
      "Iteration 608: Policy loss: -0.021540. Value loss: 0.036606. Entropy: 0.343012.\n",
      "Iteration 609: Policy loss: -0.022199. Value loss: 0.026757. Entropy: 0.341965.\n",
      "episode: 233   score: 210.0  epsilon: 1.0    steps: 552  evaluation reward: 252.85\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 610: Policy loss: -0.943306. Value loss: 0.474541. Entropy: 0.344625.\n",
      "Iteration 611: Policy loss: -0.928103. Value loss: 0.207843. Entropy: 0.344486.\n",
      "Iteration 612: Policy loss: -0.993163. Value loss: 0.187831. Entropy: 0.345511.\n",
      "episode: 234   score: 120.0  epsilon: 1.0    steps: 968  evaluation reward: 252.95\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 613: Policy loss: -0.068851. Value loss: 0.098268. Entropy: 0.344306.\n",
      "Iteration 614: Policy loss: -0.071926. Value loss: 0.037326. Entropy: 0.345327.\n",
      "Iteration 615: Policy loss: -0.083839. Value loss: 0.026232. Entropy: 0.345305.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 616: Policy loss: 0.150114. Value loss: 0.128210. Entropy: 0.341299.\n",
      "Iteration 617: Policy loss: 0.144588. Value loss: 0.032295. Entropy: 0.340018.\n",
      "Iteration 618: Policy loss: 0.140562. Value loss: 0.020160. Entropy: 0.340328.\n",
      "episode: 235   score: 485.0  epsilon: 1.0    steps: 336  evaluation reward: 255.7\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 619: Policy loss: 0.180418. Value loss: 0.086740. Entropy: 0.345278.\n",
      "Iteration 620: Policy loss: 0.174471. Value loss: 0.040814. Entropy: 0.343535.\n",
      "Iteration 621: Policy loss: 0.179727. Value loss: 0.031320. Entropy: 0.343562.\n",
      "episode: 236   score: 215.0  epsilon: 1.0    steps: 552  evaluation reward: 253.75\n",
      "episode: 237   score: 210.0  epsilon: 1.0    steps: 632  evaluation reward: 255.35\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 622: Policy loss: 0.105414. Value loss: 0.104726. Entropy: 0.343451.\n",
      "Iteration 623: Policy loss: 0.095609. Value loss: 0.054337. Entropy: 0.341639.\n",
      "Iteration 624: Policy loss: 0.095577. Value loss: 0.044829. Entropy: 0.341507.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 625: Policy loss: 0.020744. Value loss: 0.061790. Entropy: 0.341289.\n",
      "Iteration 626: Policy loss: 0.016693. Value loss: 0.022219. Entropy: 0.341572.\n",
      "Iteration 627: Policy loss: 0.009703. Value loss: 0.016477. Entropy: 0.340893.\n",
      "episode: 238   score: 210.0  epsilon: 1.0    steps: 784  evaluation reward: 253.05\n",
      "episode: 239   score: 295.0  epsilon: 1.0    steps: 984  evaluation reward: 253.9\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 628: Policy loss: -0.148344. Value loss: 0.344865. Entropy: 0.345712.\n",
      "Iteration 629: Policy loss: -0.157470. Value loss: 0.112029. Entropy: 0.344728.\n",
      "Iteration 630: Policy loss: -0.166361. Value loss: 0.057089. Entropy: 0.345017.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 631: Policy loss: 0.036730. Value loss: 0.075879. Entropy: 0.341554.\n",
      "Iteration 632: Policy loss: 0.029467. Value loss: 0.030223. Entropy: 0.344200.\n",
      "Iteration 633: Policy loss: 0.022878. Value loss: 0.023461. Entropy: 0.342782.\n",
      "episode: 240   score: 485.0  epsilon: 1.0    steps: 56  evaluation reward: 256.65\n",
      "episode: 241   score: 210.0  epsilon: 1.0    steps: 424  evaluation reward: 257.25\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 634: Policy loss: -0.482371. Value loss: 0.250040. Entropy: 0.340854.\n",
      "Iteration 635: Policy loss: -0.513864. Value loss: 0.119449. Entropy: 0.340058.\n",
      "Iteration 636: Policy loss: -0.536298. Value loss: 0.082469. Entropy: 0.339798.\n",
      "episode: 242   score: 410.0  epsilon: 1.0    steps: 752  evaluation reward: 258.85\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 637: Policy loss: 0.000569. Value loss: 0.237106. Entropy: 0.337594.\n",
      "Iteration 638: Policy loss: -0.008274. Value loss: 0.089757. Entropy: 0.339374.\n",
      "Iteration 639: Policy loss: 0.002276. Value loss: 0.064892. Entropy: 0.340065.\n",
      "episode: 243   score: 875.0  epsilon: 1.0    steps: 224  evaluation reward: 265.5\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 640: Policy loss: 0.127023. Value loss: 0.055204. Entropy: 0.341532.\n",
      "Iteration 641: Policy loss: 0.121420. Value loss: 0.020860. Entropy: 0.341426.\n",
      "Iteration 642: Policy loss: 0.120370. Value loss: 0.013394. Entropy: 0.341465.\n",
      "episode: 244   score: 80.0  epsilon: 1.0    steps: 200  evaluation reward: 264.2\n",
      "episode: 245   score: 215.0  epsilon: 1.0    steps: 968  evaluation reward: 265.55\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 643: Policy loss: 0.222077. Value loss: 0.174849. Entropy: 0.339431.\n",
      "Iteration 644: Policy loss: 0.206008. Value loss: 0.054975. Entropy: 0.337951.\n",
      "Iteration 645: Policy loss: 0.195788. Value loss: 0.032308. Entropy: 0.337639.\n",
      "episode: 246   score: 475.0  epsilon: 1.0    steps: 304  evaluation reward: 265.0\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 646: Policy loss: 0.020566. Value loss: 0.118541. Entropy: 0.337241.\n",
      "Iteration 647: Policy loss: 0.006818. Value loss: 0.043366. Entropy: 0.338311.\n",
      "Iteration 648: Policy loss: 0.000588. Value loss: 0.030315. Entropy: 0.336701.\n",
      "episode: 247   score: 210.0  epsilon: 1.0    steps: 320  evaluation reward: 265.55\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 649: Policy loss: 0.180716. Value loss: 0.071210. Entropy: 0.340829.\n",
      "Iteration 650: Policy loss: 0.179049. Value loss: 0.038041. Entropy: 0.340304.\n",
      "Iteration 651: Policy loss: 0.174366. Value loss: 0.022808. Entropy: 0.339318.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 652: Policy loss: -0.168450. Value loss: 0.396704. Entropy: 0.343836.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 653: Policy loss: -0.209976. Value loss: 0.244345. Entropy: 0.343887.\n",
      "Iteration 654: Policy loss: -0.201735. Value loss: 0.168159. Entropy: 0.342542.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 655: Policy loss: -0.053433. Value loss: 0.217412. Entropy: 0.345275.\n",
      "Iteration 656: Policy loss: -0.057901. Value loss: 0.068231. Entropy: 0.344378.\n",
      "Iteration 657: Policy loss: -0.069220. Value loss: 0.042026. Entropy: 0.344212.\n",
      "episode: 248   score: 270.0  epsilon: 1.0    steps: 392  evaluation reward: 265.4\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 658: Policy loss: 0.132682. Value loss: 0.089981. Entropy: 0.341429.\n",
      "Iteration 659: Policy loss: 0.129864. Value loss: 0.040596. Entropy: 0.339834.\n",
      "Iteration 660: Policy loss: 0.126448. Value loss: 0.029311. Entropy: 0.338891.\n",
      "episode: 249   score: 135.0  epsilon: 1.0    steps: 416  evaluation reward: 265.7\n",
      "episode: 250   score: 225.0  epsilon: 1.0    steps: 776  evaluation reward: 263.25\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 661: Policy loss: 0.155199. Value loss: 0.095735. Entropy: 0.340257.\n",
      "Iteration 662: Policy loss: 0.147043. Value loss: 0.044819. Entropy: 0.338103.\n",
      "Iteration 663: Policy loss: 0.145138. Value loss: 0.034008. Entropy: 0.337041.\n",
      "now time :  2019-09-28 10:29:48.606622\n",
      "episode: 251   score: 330.0  epsilon: 1.0    steps: 176  evaluation reward: 262.15\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 664: Policy loss: -0.189826. Value loss: 0.180639. Entropy: 0.337948.\n",
      "Iteration 665: Policy loss: -0.203936. Value loss: 0.078108. Entropy: 0.339565.\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(action_size, mode='PPO_MHDPA')\n",
    "torch.save(agent.policy_net.state_dict(), \"./save_model/spaceinvaders_ppo_best\")\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n",
    "reset_max = 10\n",
    "\n",
    "\n",
    "### Loop through all environments and run PPO on them\n",
    "\n",
    "env_names = ['SpaceInvaders-v0', 'Boxing-v0', 'DoubleDunk-v0', 'IceHockey-v0', 'Breakout-v0', 'Phoenix-v0', 'Asteroids-v0', 'MsPacman-v0', 'Asterix-v0', 'Atlantis-v0', 'Alien-v0', 'Amidar-v0', 'Assault-v0', 'BankHeist-v0']\n",
    "#env_names = ['SpaceInvaders-v4']\n",
    "for a in range(len(env_names)):\n",
    "    name = env_names[a]\n",
    "    print(\"\\n\\n\\n ------- STARTING TRAINING FOR %s ------- \\n\\n\\n\" % (name))\n",
    "    \n",
    "    envs = []\n",
    "    for i in range(num_envs):\n",
    "        envs.append(GameEnv(name))\n",
    "    #env.render()\n",
    "    \n",
    "\n",
    "    number_lives = envs[0].life\n",
    "    state_size = envs[0].observation_space.shape\n",
    "    if (name == 'SpaceInvaders-v0' or name == 'Breakout-v0'):\n",
    "        action_size = 4\n",
    "    else:\n",
    "        action_size = envs[0].action_space.n\n",
    "    rewards, episodes = [], []\n",
    "\n",
    "    vis_env_idx = 0\n",
    "    vis_env = envs[vis_env_idx]\n",
    "    e = 0\n",
    "    frame = 0\n",
    "    max_eval = -np.inf\n",
    "    reset_count = 0\n",
    "\n",
    "\n",
    "    agent = Agent(action_size, mode='PPO')\n",
    "    torch.save(agent.policy_net.state_dict(), \"./save_model/\" + name + \"_best\")\n",
    "    evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "    frame = 0\n",
    "    memory_size = 0\n",
    "    reset_max = 10\n",
    "    \n",
    "    print(\"Determing min/max rewards of environment\")\n",
    "    [low, high] = score_range = get_score_range(name)\n",
    "    print(\"Min: %d. Max: %d.\" % (low, high))\n",
    "\n",
    "    while (frame < 20000000):\n",
    "        step = 0\n",
    "        assert(num_envs * env_mem_size == train_frame)\n",
    "        frame_next_vals = []\n",
    "        \n",
    "        for j in range(env_mem_size):\n",
    "            \n",
    "            curr_states = np.stack([envs[i].history[HISTORY_SIZE-1,:,:] for i in range(num_envs)])\n",
    "            next_states = []\n",
    "            net_in = np.stack([envs[i].history[:HISTORY_SIZE,:,:] for i in range(num_envs)])\n",
    "            step += num_envs\n",
    "            frame += num_envs\n",
    "            actions, values, _ = agent.get_action(np.float32(net_in) / 255.)\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                next_states.append(next_state)\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                \"\"\"\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "                \"\"\"\n",
    "                \n",
    "                frame_next_state = get_frame(next_states[i])\n",
    "                env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "                terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "                env.life = env.info['ale.lives']\n",
    "                r = (env.reward / high) * 20.0 #np.log(max(env.reward+1, 1))#((env.reward - low) / (high - low)) * 30\n",
    "                agent.memory.push(i, deepcopy(curr_states[i]), actions[i], r, terminal_state, values[i], 0, 0)\n",
    "                \n",
    "                if (j == env_mem_size-1):\n",
    "                    net_in = np.stack([envs[k].history[1:,:,:] for k in range(num_envs)])\n",
    "                    _, frame_next_vals, _ = agent.get_action(np.float32(net_in) / 255.)\n",
    "                \n",
    "                env.score += env.reward\n",
    "                env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "        \n",
    "                if (env.done):\n",
    "                    if (e % 50 == 0):\n",
    "                        print('now time : ', datetime.now())\n",
    "                        rewards.append(np.mean(evaluation_reward))\n",
    "                        episodes.append(e)\n",
    "                        pylab.plot(episodes, rewards, 'b')\n",
    "                        pylab.savefig(\"./save_graph/\" + name + \"_ppo.png\")\n",
    "                        torch.save(agent.policy_net, \"./save_model/\" + name + \"_ppo\")\n",
    "\n",
    "                        if np.mean(evaluation_reward) > max_eval:\n",
    "                            torch.save(agent.policy_net.state_dict(), \"./save_model/\"  + name + \"_ppo_best\")\n",
    "                            max_eval = float(np.mean(evaluation_reward))\n",
    "                            reset_count = 0\n",
    "                        elif e > 5000:\n",
    "                            reset_count += 1\n",
    "                            \"\"\"\n",
    "                            if (reset_count == reset_max):\n",
    "                                print(\"Training went nowhere, starting again at best model\")\n",
    "                                agent.policy_net.load_state_dict(torch.load(\"./save_model/spaceinvaders_ppo_best\"))\n",
    "                                agent.update_target_net()\n",
    "                                reset_count = 0\n",
    "                            \"\"\"\n",
    "                    e += 1\n",
    "                    evaluation_reward.append(env.score)\n",
    "                    print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                      \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "                    env.done = False\n",
    "                    env.score = 0\n",
    "                    env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "                    env.state = env.reset()\n",
    "                    env.life = number_lives\n",
    "                    get_init_state(env.history, env.state)\n",
    "            \n",
    "        agent.train_policy_net(frame, frame_next_vals)\n",
    "        agent.update_target_net()\n",
    "    print(\"FINISHED TRAINING FOR %s\" % (name))\n",
    "    pylab.figure()\n",
    "    \n",
    "    for i in range(len(envs)):\n",
    "        envs[i]._env.close()\n",
    "    del envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent = Agent(action_size, mode='PPO_LSTM')\n",
    "torch.save(agent.policy_net.state_dict(), \"./save_model/spaceinvaders_ppo_best\")\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n",
    "reset_max = 10\n",
    "\n",
    "\n",
    "### Loop through all environments and run PPO on them\n",
    "\n",
    "#env_names = ['Breakout-v0', 'Phoenix-v0', 'Asteroids-v0', 'SpaceInvaders-v0', 'MsPacman-v0', 'Asterix-v0', 'Atlantis-v0', 'Alien-v0', 'Amidar-v0', 'Assault-v0', 'BankHeist-v0']\n",
    "env_names = ['SpaceInvaders-v0']\n",
    "for a in range(len(env_names)):\n",
    "    name = env_names[a]\n",
    "    print(\"\\n\\n\\n ------- STARTING TRAINING FOR %s ------- \\n\\n\\n\" % (name))\n",
    "    \n",
    "    envs = []\n",
    "    for i in range(num_envs):\n",
    "        envs.append(GameEnv(name))\n",
    "        envs[i].reset_memory(agent.init_hidden())\n",
    "    #env.render()\n",
    "    \n",
    "\n",
    "    number_lives = envs[0].life\n",
    "    state_size = envs[0].observation_space.shape\n",
    "    if (name == 'SpaceInvaders-v0' or name == 'Breakout-v0'):\n",
    "        action_size = 4\n",
    "    else:\n",
    "        action_size = envs[0].action_space.n\n",
    "    rewards, episodes = [], []\n",
    "\n",
    "    vis_env_idx = 0\n",
    "    vis_env = envs[vis_env_idx]\n",
    "    e = 0\n",
    "    frame = 0\n",
    "    max_eval = -np.inf\n",
    "    reset_count = 0\n",
    "\n",
    "\n",
    "    agent = Agent(action_size, mode='PPO_LSTM')\n",
    "    torch.save(agent.policy_net.state_dict(), \"./save_model/\" + name + \"_best\")\n",
    "    evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "    frame = 0\n",
    "    memory_size = 0\n",
    "    reset_max = 10\n",
    "    \n",
    "    print(\"Determing min/max rewards of environment\")\n",
    "    [low, high] = score_range = get_score_range(name)\n",
    "    print(\"Min: %d. Max: %d.\" % (low, high))\n",
    "\n",
    "    while (frame < 50000000):\n",
    "        step = 0\n",
    "        assert(num_envs * env_mem_size == train_frame)\n",
    "        frame_next_vals = []\n",
    "        \n",
    "        for j in range(env_mem_size):\n",
    "            \n",
    "            curr_states = np.stack([envs[i].history[[HISTORY_SIZE-1],:,:] for i in range(num_envs)])\n",
    "            hiddens = torch.cat([envs[i].memory for i in range(num_envs)])\n",
    "            next_states = []\n",
    "            step += num_envs\n",
    "            frame += num_envs\n",
    "            actions, values, hiddens = agent.get_action(np.float32(curr_states) / 255., hiddens)\n",
    "            hiddens = hiddens.detach()\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                next_states.append(next_state)\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                \"\"\"\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "                \"\"\"\n",
    "                \n",
    "                frame_next_state = get_frame(next_states[i])\n",
    "                env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "                env.memory = hiddens[[i]]\n",
    "                terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "                env.life = env.info['ale.lives']\n",
    "                r = (env.reward / high) * 20.0 #np.log(max(env.reward+1, 1))#((env.reward - low) / (high - low)) * 30\n",
    "                agent.memory.push(i, [deepcopy(curr_states[i]), hiddens[i].detach().cpu().data.numpy()], actions[i], r, terminal_state, values[i], 0, 0)\n",
    "                \n",
    "                if (j == env_mem_size-1):\n",
    "                    #net_in = np.stack([envs[k].history[1:,:,:] for k in range(num_envs)])\n",
    "                    net_in = np.stack([envs[k].history[[-1],:,:] for k in range(num_envs)])\n",
    "                    _, frame_next_vals, _ = agent.get_action(np.float32(net_in) / 255., hiddens)\n",
    "                \n",
    "                env.score += env.reward\n",
    "                env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "        \n",
    "                if (env.done):\n",
    "                    if (e % 50 == 0):\n",
    "                        print('now time : ', datetime.now())\n",
    "                        rewards.append(np.mean(evaluation_reward))\n",
    "                        episodes.append(e)\n",
    "                        pylab.plot(episodes, rewards, 'b')\n",
    "                        pylab.savefig(\"./save_graph/\" + name + \"_ppo.png\")\n",
    "                        torch.save(agent.policy_net, \"./save_model/\" + name + \"_ppo\")\n",
    "\n",
    "                        if np.mean(evaluation_reward) > max_eval:\n",
    "                            torch.save(agent.policy_net.state_dict(), \"./save_model/\"  + name + \"_ppo_best\")\n",
    "                            max_eval = float(np.mean(evaluation_reward))\n",
    "                            reset_count = 0\n",
    "                        elif e > 5000:\n",
    "                            reset_count += 1\n",
    "                            \"\"\"\n",
    "                            if (reset_count == reset_max):\n",
    "                                print(\"Training went nowhere, starting again at best model\")\n",
    "                                agent.policy_net.load_state_dict(torch.load(\"./save_model/spaceinvaders_ppo_best\"))\n",
    "                                agent.update_target_net()\n",
    "                                reset_count = 0\n",
    "                            \"\"\"\n",
    "                    e += 1\n",
    "                    evaluation_reward.append(env.score)\n",
    "                    print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                      \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "                    env.done = False\n",
    "                    env.score = 0\n",
    "                    env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "                    env.state = env.reset()\n",
    "                    env.life = number_lives\n",
    "                    get_init_state(env.history, env.state)\n",
    "                    env.reset_memory(agent.init_hidden())\n",
    "            \n",
    "        agent.train_policy_net(frame, frame_next_vals)\n",
    "        agent.update_target_net()\n",
    "    print(\"FINISHED TRAINING FOR %s\" % (name))\n",
    "    pylab.figure()\n",
    "    \n",
    "    for i in range(len(envs)):\n",
    "        envs[i]._env.close()\n",
    "    del envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
