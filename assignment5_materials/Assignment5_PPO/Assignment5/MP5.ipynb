{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvadersDeterministic-v4')\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lifes(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 6\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 120.0   memory length: 699   epsilon: 1.0    steps: 699     evaluation reward: 120.0\n",
      "episode: 1   score: 210.0   memory length: 1363   epsilon: 1.0    steps: 664     evaluation reward: 165.0\n",
      "episode: 2   score: 135.0   memory length: 1982   epsilon: 1.0    steps: 619     evaluation reward: 155.0\n",
      "episode: 3   score: 110.0   memory length: 2372   epsilon: 1.0    steps: 390     evaluation reward: 143.75\n",
      "episode: 4   score: 50.0   memory length: 2827   epsilon: 1.0    steps: 455     evaluation reward: 125.0\n",
      "episode: 5   score: 110.0   memory length: 3580   epsilon: 1.0    steps: 753     evaluation reward: 122.5\n",
      "episode: 6   score: 410.0   memory length: 4681   epsilon: 1.0    steps: 1101     evaluation reward: 163.57142857142858\n",
      "episode: 7   score: 90.0   memory length: 5104   epsilon: 1.0    steps: 423     evaluation reward: 154.375\n",
      "episode: 8   score: 305.0   memory length: 6209   epsilon: 1.0    steps: 1105     evaluation reward: 171.11111111111111\n",
      "episode: 9   score: 105.0   memory length: 7030   epsilon: 1.0    steps: 821     evaluation reward: 164.5\n",
      "episode: 10   score: 120.0   memory length: 7557   epsilon: 1.0    steps: 527     evaluation reward: 160.45454545454547\n",
      "episode: 11   score: 60.0   memory length: 8062   epsilon: 1.0    steps: 505     evaluation reward: 152.08333333333334\n",
      "episode: 12   score: 185.0   memory length: 8866   epsilon: 1.0    steps: 804     evaluation reward: 154.6153846153846\n",
      "episode: 13   score: 255.0   memory length: 9693   epsilon: 1.0    steps: 827     evaluation reward: 161.78571428571428\n",
      "Training network\n",
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:157: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:158: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:159: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy loss: -12.450561. Value loss: 10.459412. Entropy: 1.768124.\n",
      "Iteration 2\n",
      "Policy loss: -12.725893. Value loss: 9.640344. Entropy: 1.765771.\n",
      "Iteration 3\n",
      "Policy loss: -12.775999. Value loss: 8.559490. Entropy: 1.765218.\n",
      "episode: 14   score: 125.0   memory length: 10518   epsilon: 1.0    steps: 825     evaluation reward: 159.33333333333334\n",
      "episode: 15   score: 120.0   memory length: 11157   epsilon: 1.0    steps: 639     evaluation reward: 156.875\n",
      "episode: 16   score: 135.0   memory length: 11792   epsilon: 1.0    steps: 635     evaluation reward: 155.58823529411765\n",
      "episode: 17   score: 125.0   memory length: 12430   epsilon: 1.0    steps: 638     evaluation reward: 153.88888888888889\n",
      "episode: 18   score: 610.0   memory length: 13747   epsilon: 1.0    steps: 1317     evaluation reward: 177.89473684210526\n",
      "episode: 19   score: 105.0   memory length: 14286   epsilon: 1.0    steps: 539     evaluation reward: 174.25\n",
      "episode: 20   score: 50.0   memory length: 14802   epsilon: 1.0    steps: 516     evaluation reward: 168.33333333333334\n",
      "episode: 21   score: 55.0   memory length: 15183   epsilon: 1.0    steps: 381     evaluation reward: 163.1818181818182\n",
      "episode: 22   score: 135.0   memory length: 15874   epsilon: 1.0    steps: 691     evaluation reward: 161.95652173913044\n",
      "episode: 23   score: 215.0   memory length: 16675   epsilon: 1.0    steps: 801     evaluation reward: 164.16666666666666\n",
      "episode: 24   score: 135.0   memory length: 17326   epsilon: 1.0    steps: 651     evaluation reward: 163.0\n",
      "episode: 25   score: 30.0   memory length: 17710   epsilon: 1.0    steps: 384     evaluation reward: 157.8846153846154\n",
      "episode: 26   score: 120.0   memory length: 18445   epsilon: 1.0    steps: 735     evaluation reward: 156.4814814814815\n",
      "episode: 27   score: 30.0   memory length: 18863   epsilon: 1.0    steps: 418     evaluation reward: 151.96428571428572\n",
      "episode: 28   score: 215.0   memory length: 19675   epsilon: 1.0    steps: 812     evaluation reward: 154.13793103448276\n",
      "episode: 29   score: 135.0   memory length: 20356   epsilon: 1.0    steps: 681     evaluation reward: 153.5\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -8.767726. Value loss: 8.514691. Entropy: 1.750885.\n",
      "Iteration 2\n",
      "Policy loss: -8.838181. Value loss: 6.712163. Entropy: 1.747914.\n",
      "Iteration 3\n",
      "Policy loss: -9.123191. Value loss: 5.521996. Entropy: 1.749008.\n",
      "episode: 30   score: 420.0   memory length: 21279   epsilon: 1.0    steps: 923     evaluation reward: 162.09677419354838\n",
      "episode: 31   score: 505.0   memory length: 22436   epsilon: 1.0    steps: 1157     evaluation reward: 172.8125\n",
      "episode: 32   score: 20.0   memory length: 22914   epsilon: 1.0    steps: 478     evaluation reward: 168.1818181818182\n",
      "episode: 33   score: 140.0   memory length: 23553   epsilon: 1.0    steps: 639     evaluation reward: 167.35294117647058\n",
      "episode: 34   score: 520.0   memory length: 24649   epsilon: 1.0    steps: 1096     evaluation reward: 177.42857142857142\n",
      "episode: 35   score: 105.0   memory length: 25181   epsilon: 1.0    steps: 532     evaluation reward: 175.41666666666666\n",
      "episode: 36   score: 70.0   memory length: 25564   epsilon: 1.0    steps: 383     evaluation reward: 172.56756756756758\n",
      "episode: 37   score: 170.0   memory length: 26729   epsilon: 1.0    steps: 1165     evaluation reward: 172.5\n",
      "episode: 38   score: 120.0   memory length: 27424   epsilon: 1.0    steps: 695     evaluation reward: 171.15384615384616\n",
      "episode: 39   score: 125.0   memory length: 27809   epsilon: 1.0    steps: 385     evaluation reward: 170.0\n",
      "episode: 40   score: 110.0   memory length: 28443   epsilon: 1.0    steps: 634     evaluation reward: 168.53658536585365\n",
      "episode: 41   score: 125.0   memory length: 29262   epsilon: 1.0    steps: 819     evaluation reward: 167.5\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -8.524515. Value loss: 7.053977. Entropy: 1.740431.\n",
      "Iteration 2\n",
      "Policy loss: -8.730565. Value loss: 5.742725. Entropy: 1.730398.\n",
      "Iteration 3\n",
      "Policy loss: -9.005101. Value loss: 5.009902. Entropy: 1.726716.\n",
      "episode: 42   score: 880.0   memory length: 30957   epsilon: 1.0    steps: 1695     evaluation reward: 184.06976744186048\n",
      "episode: 43   score: 155.0   memory length: 31618   epsilon: 1.0    steps: 661     evaluation reward: 183.4090909090909\n",
      "episode: 44   score: 215.0   memory length: 32453   epsilon: 1.0    steps: 835     evaluation reward: 184.11111111111111\n",
      "episode: 45   score: 210.0   memory length: 33332   epsilon: 1.0    steps: 879     evaluation reward: 184.67391304347825\n",
      "episode: 46   score: 185.0   memory length: 34165   epsilon: 1.0    steps: 833     evaluation reward: 184.68085106382978\n",
      "episode: 47   score: 470.0   memory length: 35492   epsilon: 1.0    steps: 1327     evaluation reward: 190.625\n",
      "episode: 48   score: 65.0   memory length: 35869   epsilon: 1.0    steps: 377     evaluation reward: 188.0612244897959\n",
      "episode: 49   score: 110.0   memory length: 36484   epsilon: 1.0    steps: 615     evaluation reward: 186.5\n",
      "episode: 50   score: 105.0   memory length: 37109   epsilon: 1.0    steps: 625     evaluation reward: 184.90196078431373\n",
      "episode: 51   score: 155.0   memory length: 37842   epsilon: 1.0    steps: 733     evaluation reward: 184.32692307692307\n",
      "episode: 52   score: 225.0   memory length: 39179   epsilon: 1.0    steps: 1337     evaluation reward: 185.0943396226415\n",
      "episode: 53   score: 105.0   memory length: 39904   epsilon: 1.0    steps: 725     evaluation reward: 183.61111111111111\n",
      "episode: 54   score: 80.0   memory length: 40291   epsilon: 1.0    steps: 387     evaluation reward: 181.72727272727272\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -6.691231. Value loss: 5.517897. Entropy: 1.673474.\n",
      "Iteration 2\n",
      "Policy loss: -7.087553. Value loss: 4.728314. Entropy: 1.647946.\n",
      "Iteration 3\n",
      "Policy loss: -7.343353. Value loss: 4.266927. Entropy: 1.639510.\n",
      "episode: 55   score: 180.0   memory length: 41120   epsilon: 1.0    steps: 829     evaluation reward: 181.69642857142858\n",
      "episode: 56   score: 220.0   memory length: 42070   epsilon: 1.0    steps: 950     evaluation reward: 182.3684210526316\n",
      "episode: 57   score: 210.0   memory length: 42837   epsilon: 1.0    steps: 767     evaluation reward: 182.8448275862069\n",
      "episode: 58   score: 105.0   memory length: 43534   epsilon: 1.0    steps: 697     evaluation reward: 181.52542372881356\n",
      "episode: 59   score: 155.0   memory length: 44218   epsilon: 1.0    steps: 684     evaluation reward: 181.08333333333334\n",
      "episode: 60   score: 155.0   memory length: 44914   epsilon: 1.0    steps: 696     evaluation reward: 180.65573770491804\n",
      "episode: 61   score: 255.0   memory length: 46080   epsilon: 1.0    steps: 1166     evaluation reward: 181.8548387096774\n",
      "episode: 62   score: 210.0   memory length: 46917   epsilon: 1.0    steps: 837     evaluation reward: 182.3015873015873\n",
      "episode: 63   score: 180.0   memory length: 47744   epsilon: 1.0    steps: 827     evaluation reward: 182.265625\n",
      "episode: 64   score: 180.0   memory length: 48381   epsilon: 1.0    steps: 637     evaluation reward: 182.23076923076923\n",
      "episode: 65   score: 260.0   memory length: 49486   epsilon: 1.0    steps: 1105     evaluation reward: 183.4090909090909\n",
      "now time :  2018-12-17 16:33:04.034699\n",
      "episode: 66   score: 125.0   memory length: 50082   epsilon: 1.0    steps: 596     evaluation reward: 182.53731343283582\n",
      "episode: 67   score: 150.0   memory length: 50585   epsilon: 1.0    steps: 503     evaluation reward: 182.05882352941177\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -5.641483. Value loss: 4.639120. Entropy: 1.582028.\n",
      "Iteration 2\n",
      "Policy loss: -6.215793. Value loss: 4.196400. Entropy: 1.555580.\n",
      "Iteration 3\n",
      "Policy loss: -6.346611. Value loss: 3.838143. Entropy: 1.550013.\n",
      "episode: 68   score: 130.0   memory length: 51224   epsilon: 1.0    steps: 639     evaluation reward: 181.30434782608697\n",
      "episode: 69   score: 110.0   memory length: 51749   epsilon: 1.0    steps: 525     evaluation reward: 180.28571428571428\n",
      "episode: 70   score: 155.0   memory length: 52398   epsilon: 1.0    steps: 649     evaluation reward: 179.92957746478874\n",
      "episode: 71   score: 75.0   memory length: 52801   epsilon: 1.0    steps: 403     evaluation reward: 178.47222222222223\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        if render_breakout:\n",
    "            env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        action, value = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state, value, 0, 0)\n",
    "        # Start training after random sample generation\n",
    "        if(frame % train_frame == 0):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network\n",
    "            agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > 400 and len(evaluation_reward) > 5:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn\")\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
