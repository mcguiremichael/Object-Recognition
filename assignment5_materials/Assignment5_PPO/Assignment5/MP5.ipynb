{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lifes(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 6\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/home/michael/Documents/CS498DL/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],0)[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 170.0   memory length: 1037   epsilon: 1.0    steps: 1037     evaluation reward: 170.0\n",
      "episode: 1   score: 655.0   memory length: 2608   epsilon: 1.0    steps: 1571     evaluation reward: 412.5\n",
      "episode: 2   score: 135.0   memory length: 3174   epsilon: 1.0    steps: 566     evaluation reward: 320.0\n",
      "episode: 3   score: 410.0   memory length: 4466   epsilon: 1.0    steps: 1292     evaluation reward: 342.5\n",
      "episode: 4   score: 85.0   memory length: 5120   epsilon: 1.0    steps: 654     evaluation reward: 291.0\n",
      "episode: 5   score: 155.0   memory length: 5820   epsilon: 1.0    steps: 700     evaluation reward: 268.3333333333333\n",
      "episode: 6   score: 120.0   memory length: 6430   epsilon: 1.0    steps: 610     evaluation reward: 247.14285714285714\n",
      "episode: 7   score: 140.0   memory length: 7206   epsilon: 1.0    steps: 776     evaluation reward: 233.75\n",
      "episode: 8   score: 210.0   memory length: 8002   epsilon: 1.0    steps: 796     evaluation reward: 231.11111111111111\n",
      "episode: 9   score: 20.0   memory length: 8383   epsilon: 1.0    steps: 381     evaluation reward: 210.0\n",
      "episode: 10   score: 55.0   memory length: 8976   epsilon: 1.0    steps: 593     evaluation reward: 195.9090909090909\n",
      "episode: 11   score: 365.0   memory length: 10043   epsilon: 1.0    steps: 1067     evaluation reward: 210.0\n",
      "Training network\n",
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/Documents/CS498DL/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:225: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/michael/Documents/CS498DL/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:226: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/michael/Documents/CS498DL/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:227: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy loss: 0.000150. Value loss: 11.088337. Entropy: 1.771215.\n",
      "Iteration 2\n",
      "Policy loss: 0.000616. Value loss: 7.356457. Entropy: 1.765690.\n",
      "Iteration 3\n",
      "Policy loss: -0.002851. Value loss: 5.608305. Entropy: 1.764209.\n",
      "Iteration 4\n",
      "Policy loss: -0.008718. Value loss: 4.618650. Entropy: 1.765876.\n",
      "Iteration 5\n",
      "Policy loss: -0.013365. Value loss: 3.936110. Entropy: 1.763928.\n",
      "episode: 12   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 380     evaluation reward: 197.69230769230768\n",
      "episode: 13   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 644     evaluation reward: 193.21428571428572\n",
      "episode: 14   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 666     evaluation reward: 189.33333333333334\n",
      "episode: 15   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 768     evaluation reward: 190.625\n",
      "episode: 16   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 524     evaluation reward: 182.64705882352942\n",
      "episode: 17   score: 190.0   memory length: 10240   epsilon: 1.0    steps: 741     evaluation reward: 183.05555555555554\n",
      "episode: 18   score: 225.0   memory length: 10240   epsilon: 1.0    steps: 892     evaluation reward: 185.26315789473685\n",
      "episode: 19   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 606     evaluation reward: 183.75\n",
      "episode: 20   score: 320.0   memory length: 10240   epsilon: 1.0    steps: 1206     evaluation reward: 190.23809523809524\n",
      "episode: 21   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 371     evaluation reward: 184.3181818181818\n",
      "episode: 22   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 642     evaluation reward: 178.69565217391303\n",
      "episode: 23   score: 40.0   memory length: 10240   epsilon: 1.0    steps: 611     evaluation reward: 172.91666666666666\n",
      "episode: 24   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 543     evaluation reward: 170.8\n",
      "episode: 25   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 389     evaluation reward: 166.34615384615384\n",
      "episode: 26   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 773     evaluation reward: 167.96296296296296\n",
      "episode: 27   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 483     evaluation reward: 163.03571428571428\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.002040. Value loss: 6.039907. Entropy: 1.760586.\n",
      "Iteration 2\n",
      "Policy loss: -0.006255. Value loss: 4.087808. Entropy: 1.755622.\n",
      "Iteration 3\n",
      "Policy loss: -0.014335. Value loss: 3.393882. Entropy: 1.751859.\n",
      "Iteration 4\n",
      "Policy loss: -0.013628. Value loss: 2.860747. Entropy: 1.753970.\n",
      "Iteration 5\n",
      "Policy loss: -0.019201. Value loss: 2.444874. Entropy: 1.751861.\n",
      "episode: 28   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 744     evaluation reward: 162.58620689655172\n",
      "episode: 29   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 611     evaluation reward: 158.33333333333334\n",
      "episode: 30   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 521     evaluation reward: 156.7741935483871\n",
      "episode: 31   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 510     evaluation reward: 155.15625\n",
      "episode: 32   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 396     evaluation reward: 153.1818181818182\n",
      "episode: 33   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 616     evaluation reward: 151.02941176470588\n",
      "episode: 34   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 865     evaluation reward: 152.0\n",
      "episode: 35   score: 425.0   memory length: 10240   epsilon: 1.0    steps: 1250     evaluation reward: 159.58333333333334\n",
      "episode: 36   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 648     evaluation reward: 158.24324324324326\n",
      "episode: 37   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 641     evaluation reward: 158.1578947368421\n",
      "episode: 38   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 475     evaluation reward: 156.92307692307693\n",
      "episode: 39   score: 165.0   memory length: 10240   epsilon: 1.0    steps: 667     evaluation reward: 157.125\n",
      "episode: 40   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 693     evaluation reward: 157.6829268292683\n",
      "episode: 41   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 408     evaluation reward: 155.11904761904762\n",
      "episode: 42   score: 420.0   memory length: 10240   epsilon: 1.0    steps: 1274     evaluation reward: 161.27906976744185\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.002659. Value loss: 7.383142. Entropy: 1.750839.\n",
      "Iteration 2\n",
      "Policy loss: -0.001256. Value loss: 4.691101. Entropy: 1.745484.\n",
      "Iteration 3\n",
      "Policy loss: -0.007248. Value loss: 3.632338. Entropy: 1.742728.\n",
      "Iteration 4\n",
      "Policy loss: -0.011483. Value loss: 3.258583. Entropy: 1.740836.\n",
      "Iteration 5\n",
      "Policy loss: -0.011866. Value loss: 2.817116. Entropy: 1.738135.\n",
      "episode: 43   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 524     evaluation reward: 159.3181818181818\n",
      "episode: 44   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 592     evaluation reward: 158.22222222222223\n",
      "episode: 45   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 733     evaluation reward: 156.7391304347826\n",
      "episode: 46   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 620     evaluation reward: 155.63829787234042\n",
      "episode: 47   score: 15.0   memory length: 10240   epsilon: 1.0    steps: 473     evaluation reward: 152.70833333333334\n",
      "episode: 48   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 662     evaluation reward: 151.73469387755102\n",
      "episode: 49   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 443     evaluation reward: 150.0\n",
      "episode: 50   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 370     evaluation reward: 148.0392156862745\n",
      "episode: 51   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 584     evaluation reward: 147.21153846153845\n",
      "episode: 52   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 694     evaluation reward: 148.39622641509433\n",
      "episode: 53   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 655     evaluation reward: 147.96296296296296\n",
      "episode: 54   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 571     evaluation reward: 147.1818181818182\n",
      "episode: 55   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 558     evaluation reward: 145.71428571428572\n",
      "episode: 56   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 683     evaluation reward: 145.52631578947367\n",
      "episode: 57   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 1027     evaluation reward: 146.63793103448276\n",
      "episode: 58   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 624     evaluation reward: 146.01694915254237\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.002681. Value loss: 5.241664. Entropy: 1.740265.\n",
      "Iteration 2\n",
      "Policy loss: -0.005850. Value loss: 3.414273. Entropy: 1.737953.\n",
      "Iteration 3\n",
      "Policy loss: -0.015085. Value loss: 2.811948. Entropy: 1.739176.\n",
      "Iteration 4\n",
      "Policy loss: -0.017332. Value loss: 2.401860. Entropy: 1.737650.\n",
      "Iteration 5\n",
      "Policy loss: -0.017570. Value loss: 2.062683. Entropy: 1.732825.\n",
      "episode: 59   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 710     evaluation reward: 145.58333333333334\n",
      "episode: 60   score: 140.0   memory length: 10240   epsilon: 1.0    steps: 698     evaluation reward: 145.49180327868854\n",
      "episode: 61   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 914     evaluation reward: 145.6451612903226\n",
      "episode: 62   score: 20.0   memory length: 10240   epsilon: 1.0    steps: 383     evaluation reward: 143.65079365079364\n",
      "episode: 63   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 663     evaluation reward: 143.75\n",
      "episode: 64   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 698     evaluation reward: 143.23076923076923\n",
      "episode: 65   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 488     evaluation reward: 142.1969696969697\n",
      "episode: 66   score: 340.0   memory length: 10240   epsilon: 1.0    steps: 700     evaluation reward: 145.1492537313433\n",
      "episode: 67   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 831     evaluation reward: 146.10294117647058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 68   score: 390.0   memory length: 10240   epsilon: 1.0    steps: 1364     evaluation reward: 149.63768115942028\n",
      "episode: 69   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 385     evaluation reward: 148.21428571428572\n",
      "episode: 70   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 787     evaluation reward: 148.30985915492957\n",
      "now time :  2018-12-18 16:43:37.071820\n",
      "episode: 71   score: 245.0   memory length: 10240   epsilon: 1.0    steps: 1000     evaluation reward: 149.65277777777777\n",
      "episode: 72   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 636     evaluation reward: 150.4794520547945\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.002079. Value loss: 6.519347. Entropy: 1.731731.\n",
      "Iteration 2\n",
      "Policy loss: -0.006203. Value loss: 4.463940. Entropy: 1.729289.\n",
      "Iteration 3\n",
      "Policy loss: -0.009409. Value loss: 3.497622. Entropy: 1.726244.\n",
      "Iteration 4\n",
      "Policy loss: -0.014321. Value loss: 3.060592. Entropy: 1.723430.\n",
      "Iteration 5\n",
      "Policy loss: -0.016801. Value loss: 2.642349. Entropy: 1.723052.\n",
      "episode: 73   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 807     evaluation reward: 150.94594594594594\n",
      "episode: 74   score: 225.0   memory length: 10240   epsilon: 1.0    steps: 865     evaluation reward: 151.93333333333334\n",
      "episode: 75   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 539     evaluation reward: 152.30263157894737\n",
      "episode: 76   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 444     evaluation reward: 152.07792207792207\n",
      "episode: 77   score: 240.0   memory length: 10240   epsilon: 1.0    steps: 881     evaluation reward: 153.2051282051282\n",
      "episode: 78   score: 400.0   memory length: 10240   epsilon: 1.0    steps: 981     evaluation reward: 156.32911392405063\n",
      "episode: 79   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 489     evaluation reward: 155.3125\n",
      "episode: 80   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 644     evaluation reward: 154.3827160493827\n",
      "episode: 81   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 498     evaluation reward: 153.29268292682926\n",
      "episode: 82   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 404     evaluation reward: 151.80722891566265\n",
      "episode: 83   score: 610.0   memory length: 10240   epsilon: 1.0    steps: 1198     evaluation reward: 157.26190476190476\n",
      "episode: 84   score: 460.0   memory length: 10240   epsilon: 1.0    steps: 839     evaluation reward: 160.8235294117647\n",
      "episode: 85   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 529     evaluation reward: 160.2325581395349\n",
      "episode: 86   score: 165.0   memory length: 10240   epsilon: 1.0    steps: 767     evaluation reward: 160.28735632183907\n",
      "episode: 87   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 512     evaluation reward: 159.375\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.000577. Value loss: 9.030591. Entropy: 1.719447.\n",
      "Iteration 2\n",
      "Policy loss: -0.002444. Value loss: 5.730543. Entropy: 1.715365.\n",
      "Iteration 3\n",
      "Policy loss: -0.004734. Value loss: 4.431989. Entropy: 1.706818.\n",
      "Iteration 4\n",
      "Policy loss: -0.006406. Value loss: 4.053585. Entropy: 1.705577.\n",
      "Iteration 5\n",
      "Policy loss: -0.009801. Value loss: 3.396978. Entropy: 1.704038.\n",
      "episode: 88   score: 100.0   memory length: 10240   epsilon: 1.0    steps: 477     evaluation reward: 158.7078651685393\n",
      "episode: 89   score: 265.0   memory length: 10240   epsilon: 1.0    steps: 914     evaluation reward: 159.88888888888889\n",
      "episode: 90   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 528     evaluation reward: 159.6153846153846\n",
      "episode: 91   score: 230.0   memory length: 10240   epsilon: 1.0    steps: 828     evaluation reward: 160.3804347826087\n",
      "episode: 92   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 578     evaluation reward: 159.19354838709677\n",
      "episode: 93   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 786     evaluation reward: 159.7340425531915\n",
      "episode: 94   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 504     evaluation reward: 158.6315789473684\n",
      "episode: 95   score: 245.0   memory length: 10240   epsilon: 1.0    steps: 926     evaluation reward: 159.53125\n",
      "episode: 96   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 497     evaluation reward: 158.4536082474227\n",
      "episode: 97   score: 10.0   memory length: 10240   epsilon: 1.0    steps: 534     evaluation reward: 156.9387755102041\n",
      "episode: 98   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 651     evaluation reward: 156.41414141414143\n",
      "episode: 99   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 708     evaluation reward: 156.65\n",
      "episode: 100   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 508     evaluation reward: 155.75\n",
      "episode: 101   score: 160.0   memory length: 10240   epsilon: 1.0    steps: 1099     evaluation reward: 150.8\n",
      "episode: 102   score: 130.0   memory length: 10240   epsilon: 1.0    steps: 719     evaluation reward: 150.75\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.003050. Value loss: 7.039941. Entropy: 1.714233.\n",
      "Iteration 2\n",
      "Policy loss: -0.010387. Value loss: 4.283922. Entropy: 1.710381.\n",
      "Iteration 3\n",
      "Policy loss: -0.016206. Value loss: 3.438028. Entropy: 1.704361.\n",
      "Iteration 4\n",
      "Policy loss: -0.019670. Value loss: 3.050887. Entropy: 1.702470.\n",
      "Iteration 5\n",
      "Policy loss: -0.024070. Value loss: 2.587640. Entropy: 1.705494.\n",
      "episode: 103   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 623     evaluation reward: 148.75\n",
      "episode: 104   score: 20.0   memory length: 10240   epsilon: 1.0    steps: 511     evaluation reward: 148.1\n",
      "episode: 105   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 488     evaluation reward: 147.3\n",
      "episode: 106   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 381     evaluation reward: 147.15\n",
      "episode: 107   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 695     evaluation reward: 146.4\n",
      "episode: 108   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 668     evaluation reward: 145.5\n",
      "episode: 109   score: 170.0   memory length: 10240   epsilon: 1.0    steps: 795     evaluation reward: 147.0\n",
      "episode: 110   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 500     evaluation reward: 147.55\n",
      "episode: 111   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 694     evaluation reward: 145.25\n",
      "episode: 112   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 554     evaluation reward: 145.95\n",
      "episode: 113   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 763     evaluation reward: 146.15\n",
      "episode: 114   score: 390.0   memory length: 10240   epsilon: 1.0    steps: 908     evaluation reward: 148.7\n",
      "episode: 115   score: 200.0   memory length: 10240   epsilon: 1.0    steps: 890     evaluation reward: 148.6\n",
      "episode: 116   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 761     evaluation reward: 149.4\n",
      "episode: 117   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 393     evaluation reward: 148.4\n",
      "episode: 118   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 485     evaluation reward: 146.6\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.001841. Value loss: 6.200920. Entropy: 1.701924.\n",
      "Iteration 2\n",
      "Policy loss: -0.011270. Value loss: 3.603487. Entropy: 1.693981.\n",
      "Iteration 3\n",
      "Policy loss: -0.011284. Value loss: 2.916139. Entropy: 1.693144.\n",
      "Iteration 4\n",
      "Policy loss: -0.016844. Value loss: 2.512875. Entropy: 1.692140.\n",
      "Iteration 5\n",
      "Policy loss: -0.016094. Value loss: 2.261851. Entropy: 1.689115.\n",
      "episode: 119   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 680     evaluation reward: 146.55\n",
      "episode: 120   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 943     evaluation reward: 145.45\n",
      "episode: 121   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 442     evaluation reward: 145.5\n",
      "episode: 122   score: 500.0   memory length: 10240   epsilon: 1.0    steps: 953     evaluation reward: 149.95\n",
      "episode: 123   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 548     evaluation reward: 151.35\n",
      "episode: 124   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 968     evaluation reward: 151.5\n",
      "episode: 125   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 504     evaluation reward: 152.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 126   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 922     evaluation reward: 152.0\n",
      "episode: 127   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 691     evaluation reward: 153.25\n",
      "episode: 128   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 389     evaluation reward: 152.35\n",
      "episode: 129   score: 225.0   memory length: 10240   epsilon: 1.0    steps: 806     evaluation reward: 154.25\n",
      "episode: 130   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 394     evaluation reward: 153.95\n",
      "episode: 131   score: 220.0   memory length: 10240   epsilon: 1.0    steps: 824     evaluation reward: 155.1\n",
      "episode: 132   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 609     evaluation reward: 155.45\n",
      "episode: 133   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 650     evaluation reward: 155.7\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.002538. Value loss: 6.374971. Entropy: 1.685458.\n",
      "Iteration 2\n",
      "Policy loss: -0.002816. Value loss: 3.681540. Entropy: 1.675928.\n",
      "Iteration 3\n",
      "Policy loss: -0.011089. Value loss: 3.029083. Entropy: 1.676196.\n",
      "Iteration 4\n",
      "Policy loss: -0.016789. Value loss: 2.674708. Entropy: 1.677760.\n",
      "Iteration 5\n",
      "Policy loss: -0.018616. Value loss: 2.356729. Entropy: 1.678050.\n",
      "episode: 134   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 658     evaluation reward: 155.2\n",
      "episode: 135   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 626     evaluation reward: 152.5\n",
      "episode: 136   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 713     evaluation reward: 153.25\n",
      "episode: 137   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 712     evaluation reward: 153.5\n",
      "episode: 138   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 478     evaluation reward: 153.05\n",
      "episode: 139   score: 165.0   memory length: 10240   epsilon: 1.0    steps: 837     evaluation reward: 153.05\n",
      "episode: 140   score: 440.0   memory length: 10240   epsilon: 1.0    steps: 954     evaluation reward: 155.65\n",
      "episode: 141   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 622     evaluation reward: 156.5\n",
      "episode: 142   score: 215.0   memory length: 10240   epsilon: 1.0    steps: 864     evaluation reward: 154.45\n",
      "episode: 143   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 711     evaluation reward: 155.5\n",
      "episode: 144   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 626     evaluation reward: 155.75\n",
      "now time :  2018-12-18 16:48:32.633776\n",
      "episode: 145   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 716     evaluation reward: 155.95\n",
      "episode: 146   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 725     evaluation reward: 156.25\n",
      "episode: 147   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 575     evaluation reward: 157.45\n",
      "episode: 148   score: 70.0   memory length: 10240   epsilon: 1.0    steps: 436     evaluation reward: 157.1\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.009262. Value loss: 6.544291. Entropy: 1.670926.\n",
      "Iteration 2\n",
      "Policy loss: -0.013101. Value loss: 3.713329. Entropy: 1.660208.\n",
      "Iteration 3\n",
      "Policy loss: -0.015705. Value loss: 2.872542. Entropy: 1.660202.\n",
      "Iteration 4\n",
      "Policy loss: -0.021030. Value loss: 2.514813. Entropy: 1.657293.\n",
      "Iteration 5\n",
      "Policy loss: -0.026863. Value loss: 2.230129. Entropy: 1.661459.\n",
      "episode: 149   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 423     evaluation reward: 157.1\n",
      "episode: 150   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 606     evaluation reward: 157.7\n",
      "episode: 151   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 518     evaluation reward: 157.1\n",
      "episode: 152   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 715     evaluation reward: 156.05\n",
      "episode: 153   score: 475.0   memory length: 10240   epsilon: 1.0    steps: 1063     evaluation reward: 159.55\n",
      "episode: 154   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 489     evaluation reward: 158.95\n",
      "episode: 155   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 401     evaluation reward: 158.6\n",
      "episode: 156   score: 230.0   memory length: 10240   epsilon: 1.0    steps: 805     evaluation reward: 159.55\n",
      "episode: 157   score: 140.0   memory length: 10240   epsilon: 1.0    steps: 525     evaluation reward: 158.85\n",
      "episode: 158   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 434     evaluation reward: 158.35\n",
      "episode: 159   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 394     evaluation reward: 157.6\n",
      "episode: 160   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 653     evaluation reward: 158.0\n",
      "episode: 161   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 947     evaluation reward: 158.0\n",
      "episode: 162   score: 130.0   memory length: 10240   epsilon: 1.0    steps: 811     evaluation reward: 159.1\n",
      "episode: 163   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 641     evaluation reward: 158.4\n",
      "episode: 164   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 660     evaluation reward: 158.65\n",
      "episode: 165   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 489     evaluation reward: 158.2\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.012716. Value loss: 6.682433. Entropy: 1.657399.\n",
      "Iteration 2\n",
      "Policy loss: -0.016456. Value loss: 4.328260. Entropy: 1.644280.\n",
      "Iteration 3\n",
      "Policy loss: -0.017100. Value loss: 3.160256. Entropy: 1.642424.\n",
      "Iteration 4\n",
      "Policy loss: -0.019177. Value loss: 2.623719. Entropy: 1.648436.\n",
      "Iteration 5\n",
      "Policy loss: -0.025712. Value loss: 2.334871. Entropy: 1.640545.\n",
      "episode: 166   score: 555.0   memory length: 10240   epsilon: 1.0    steps: 1099     evaluation reward: 160.35\n",
      "episode: 167   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 635     evaluation reward: 159.45\n",
      "episode: 168   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 456     evaluation reward: 156.05\n",
      "episode: 169   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 636     evaluation reward: 156.2\n",
      "episode: 170   score: 285.0   memory length: 10240   epsilon: 1.0    steps: 961     evaluation reward: 157.5\n",
      "episode: 171   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 501     evaluation reward: 155.55\n",
      "episode: 172   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 392     evaluation reward: 154.0\n",
      "episode: 173   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 697     evaluation reward: 153.4\n",
      "episode: 174   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 646     evaluation reward: 152.35\n",
      "episode: 175   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 490     evaluation reward: 151.2\n",
      "episode: 176   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 361     evaluation reward: 150.75\n",
      "episode: 177   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 380     evaluation reward: 149.15\n",
      "episode: 178   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 681     evaluation reward: 146.25\n",
      "episode: 179   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 402     evaluation reward: 146.25\n",
      "episode: 180   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 531     evaluation reward: 145.75\n",
      "episode: 181   score: 140.0   memory length: 10240   epsilon: 1.0    steps: 679     evaluation reward: 146.5\n",
      "episode: 182   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 564     evaluation reward: 146.65\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.004909. Value loss: 6.037765. Entropy: 1.653177.\n",
      "Iteration 2\n",
      "Policy loss: -0.007427. Value loss: 3.341216. Entropy: 1.651436.\n",
      "Iteration 3\n",
      "Policy loss: -0.013407. Value loss: 2.608368. Entropy: 1.647476.\n",
      "Iteration 4\n",
      "Policy loss: -0.018738. Value loss: 2.302254. Entropy: 1.648763.\n",
      "Iteration 5\n",
      "Policy loss: -0.022638. Value loss: 1.991019. Entropy: 1.641403.\n",
      "episode: 183   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 621     evaluation reward: 141.2\n",
      "episode: 184   score: 440.0   memory length: 10240   epsilon: 1.0    steps: 1139     evaluation reward: 141.0\n",
      "episode: 185   score: 100.0   memory length: 10240   epsilon: 1.0    steps: 655     evaluation reward: 140.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 186   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 405     evaluation reward: 139.6\n",
      "episode: 187   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 459     evaluation reward: 139.3\n",
      "episode: 188   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 463     evaluation reward: 139.35\n",
      "episode: 189   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 610     evaluation reward: 137.8\n",
      "episode: 190   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 557     evaluation reward: 137.55\n",
      "episode: 191   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 535     evaluation reward: 135.8\n",
      "episode: 192   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 875     evaluation reward: 136.4\n",
      "episode: 193   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 702     evaluation reward: 135.65\n",
      "episode: 194   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 856     evaluation reward: 136.15\n",
      "episode: 195   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 488     evaluation reward: 134.3\n",
      "episode: 196   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 388     evaluation reward: 134.2\n",
      "episode: 197   score: 445.0   memory length: 10240   epsilon: 1.0    steps: 1338     evaluation reward: 138.55\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.008452. Value loss: 5.949740. Entropy: 1.655736.\n",
      "Iteration 2\n",
      "Policy loss: -0.010389. Value loss: 3.499967. Entropy: 1.643336.\n",
      "Iteration 3\n",
      "Policy loss: -0.014272. Value loss: 2.435950. Entropy: 1.646256.\n",
      "Iteration 4\n",
      "Policy loss: -0.018090. Value loss: 2.113668. Entropy: 1.642622.\n",
      "Iteration 5\n",
      "Policy loss: -0.020671. Value loss: 1.915124. Entropy: 1.644472.\n",
      "episode: 198   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 912     evaluation reward: 139.35\n",
      "episode: 199   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 788     evaluation reward: 138.9\n",
      "episode: 200   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 811     evaluation reward: 140.2\n",
      "episode: 201   score: 225.0   memory length: 10240   epsilon: 1.0    steps: 973     evaluation reward: 140.85\n",
      "episode: 202   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 643     evaluation reward: 140.65\n",
      "episode: 203   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 655     evaluation reward: 139.75\n",
      "episode: 204   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 526     evaluation reward: 140.35\n",
      "episode: 205   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 829     evaluation reward: 141.7\n",
      "episode: 206   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 675     evaluation reward: 142.45\n",
      "episode: 207   score: 380.0   memory length: 10240   epsilon: 1.0    steps: 1556     evaluation reward: 145.6\n",
      "episode: 208   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 507     evaluation reward: 145.65\n",
      "episode: 209   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 572     evaluation reward: 145.75\n",
      "episode: 210   score: 140.0   memory length: 10240   epsilon: 1.0    steps: 742     evaluation reward: 146.05\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.004454. Value loss: 6.534460. Entropy: 1.650319.\n",
      "Iteration 2\n",
      "Policy loss: -0.015753. Value loss: 3.953263. Entropy: 1.643757.\n",
      "Iteration 3\n",
      "Policy loss: -0.023195. Value loss: 2.978962. Entropy: 1.640319.\n",
      "Iteration 4\n",
      "Policy loss: -0.022348. Value loss: 2.487592. Entropy: 1.641464.\n",
      "Iteration 5\n",
      "Policy loss: -0.027799. Value loss: 2.245767. Entropy: 1.636092.\n",
      "episode: 211   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 502     evaluation reward: 145.5\n",
      "episode: 212   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 604     evaluation reward: 145.5\n",
      "episode: 213   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 1271     evaluation reward: 145.8\n",
      "episode: 214   score: 170.0   memory length: 10240   epsilon: 1.0    steps: 867     evaluation reward: 143.6\n",
      "episode: 215   score: 170.0   memory length: 10240   epsilon: 1.0    steps: 738     evaluation reward: 143.3\n",
      "episode: 216   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 756     evaluation reward: 143.3\n",
      "episode: 217   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 646     evaluation reward: 143.6\n",
      "episode: 218   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 597     evaluation reward: 144.35\n",
      "episode: 219   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 660     evaluation reward: 144.05\n",
      "now time :  2018-12-18 16:53:31.358139\n",
      "episode: 220   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 520     evaluation reward: 142.75\n",
      "episode: 221   score: 20.0   memory length: 10240   epsilon: 1.0    steps: 622     evaluation reward: 142.3\n",
      "episode: 222   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 666     evaluation reward: 138.5\n",
      "episode: 223   score: 40.0   memory length: 10240   epsilon: 1.0    steps: 470     evaluation reward: 137.1\n",
      "episode: 224   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 617     evaluation reward: 136.85\n",
      "episode: 225   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 394     evaluation reward: 136.45\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.009376. Value loss: 4.482017. Entropy: 1.637799.\n",
      "Iteration 2\n",
      "Policy loss: -0.022034. Value loss: 2.527365. Entropy: 1.632390.\n",
      "Iteration 3\n",
      "Policy loss: -0.028181. Value loss: 1.886156. Entropy: 1.631037.\n",
      "Iteration 4\n",
      "Policy loss: -0.030827. Value loss: 1.668783. Entropy: 1.631126.\n",
      "Iteration 5\n",
      "Policy loss: -0.035355. Value loss: 1.531246. Entropy: 1.630180.\n",
      "episode: 226   score: 315.0   memory length: 10240   epsilon: 1.0    steps: 1191     evaluation reward: 137.5\n",
      "episode: 227   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 749     evaluation reward: 137.75\n",
      "episode: 228   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 658     evaluation reward: 138.35\n",
      "episode: 229   score: 175.0   memory length: 10240   epsilon: 1.0    steps: 658     evaluation reward: 137.85\n",
      "episode: 230   score: 100.0   memory length: 10240   epsilon: 1.0    steps: 672     evaluation reward: 138.05\n",
      "episode: 231   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 724     evaluation reward: 136.9\n",
      "episode: 232   score: 85.0   memory length: 10240   epsilon: 1.0    steps: 536     evaluation reward: 136.5\n",
      "episode: 233   score: 245.0   memory length: 10240   epsilon: 1.0    steps: 1198     evaluation reward: 137.9\n",
      "episode: 234   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 667     evaluation reward: 138.35\n",
      "episode: 235   score: 175.0   memory length: 10240   epsilon: 1.0    steps: 1136     evaluation reward: 138.55\n",
      "episode: 236   score: 245.0   memory length: 10240   epsilon: 1.0    steps: 1161     evaluation reward: 139.15\n",
      "episode: 237   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 372     evaluation reward: 137.8\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.011683. Value loss: 5.369479. Entropy: 1.628996.\n",
      "Iteration 2\n",
      "Policy loss: -0.022975. Value loss: 2.797726. Entropy: 1.626700.\n",
      "Iteration 3\n",
      "Policy loss: -0.027716. Value loss: 2.184389. Entropy: 1.615556.\n",
      "Iteration 4\n",
      "Policy loss: -0.033605. Value loss: 1.871548. Entropy: 1.615950.\n",
      "Iteration 5\n",
      "Policy loss: -0.032974. Value loss: 1.653016. Entropy: 1.608194.\n",
      "episode: 238   score: 250.0   memory length: 10240   epsilon: 1.0    steps: 1242     evaluation reward: 139.65\n",
      "episode: 239   score: 175.0   memory length: 10240   epsilon: 1.0    steps: 911     evaluation reward: 139.75\n",
      "episode: 240   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 550     evaluation reward: 136.6\n",
      "episode: 241   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 397     evaluation reward: 135.6\n",
      "episode: 242   score: 20.0   memory length: 10240   epsilon: 1.0    steps: 389     evaluation reward: 133.65\n",
      "episode: 243   score: 255.0   memory length: 10240   epsilon: 1.0    steps: 793     evaluation reward: 134.4\n",
      "episode: 244   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 732     evaluation reward: 134.85\n",
      "episode: 245   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 520     evaluation reward: 134.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 246   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 578     evaluation reward: 133.25\n",
      "episode: 247   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 585     evaluation reward: 132.2\n",
      "episode: 248   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 712     evaluation reward: 132.85\n",
      "episode: 249   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 592     evaluation reward: 133.55\n",
      "episode: 250   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 644     evaluation reward: 133.25\n",
      "episode: 251   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 767     evaluation reward: 134.35\n",
      "episode: 252   score: 510.0   memory length: 10240   epsilon: 1.0    steps: 1132     evaluation reward: 138.4\n",
      "episode: 253   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 587     evaluation reward: 134.7\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.003712. Value loss: 5.891301. Entropy: 1.617343.\n",
      "Iteration 2\n",
      "Policy loss: -0.011539. Value loss: 3.198100. Entropy: 1.608089.\n",
      "Iteration 3\n",
      "Policy loss: -0.023529. Value loss: 2.440703. Entropy: 1.609411.\n",
      "Iteration 4\n",
      "Policy loss: -0.022056. Value loss: 2.203489. Entropy: 1.604117.\n",
      "Iteration 5\n",
      "Policy loss: -0.031974. Value loss: 1.932464. Entropy: 1.601778.\n",
      "episode: 254   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 646     evaluation reward: 136.05\n",
      "episode: 255   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 642     evaluation reward: 137.85\n",
      "episode: 256   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 583     evaluation reward: 135.9\n",
      "episode: 257   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 695     evaluation reward: 136.3\n",
      "episode: 258   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 802     evaluation reward: 137.8\n",
      "episode: 259   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 625     evaluation reward: 138.55\n",
      "episode: 260   score: 145.0   memory length: 10240   epsilon: 1.0    steps: 793     evaluation reward: 138.2\n",
      "episode: 261   score: 290.0   memory length: 10240   epsilon: 1.0    steps: 962     evaluation reward: 139.55\n",
      "episode: 262   score: 15.0   memory length: 10240   epsilon: 1.0    steps: 445     evaluation reward: 138.4\n",
      "episode: 263   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 384     evaluation reward: 137.95\n",
      "episode: 264   score: 350.0   memory length: 10240   epsilon: 1.0    steps: 1159     evaluation reward: 140.1\n",
      "episode: 265   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 406     evaluation reward: 140.3\n",
      "episode: 266   score: 280.0   memory length: 10240   epsilon: 1.0    steps: 847     evaluation reward: 137.55\n",
      "episode: 267   score: 310.0   memory length: 10240   epsilon: 1.0    steps: 686     evaluation reward: 139.45\n",
      "episode: 268   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 552     evaluation reward: 140.2\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.009991. Value loss: 6.330991. Entropy: 1.637203.\n",
      "Iteration 2\n",
      "Policy loss: -0.021080. Value loss: 3.386089. Entropy: 1.625404.\n",
      "Iteration 3\n",
      "Policy loss: -0.026153. Value loss: 2.791052. Entropy: 1.617150.\n",
      "Iteration 4\n",
      "Policy loss: -0.025322. Value loss: 2.397770. Entropy: 1.614047.\n",
      "Iteration 5\n",
      "Policy loss: -0.031377. Value loss: 2.261672. Entropy: 1.612586.\n",
      "episode: 269   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 690     evaluation reward: 141.1\n",
      "episode: 270   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 630     evaluation reward: 139.45\n",
      "episode: 271   score: 145.0   memory length: 10240   epsilon: 1.0    steps: 541     evaluation reward: 140.4\n",
      "episode: 272   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 690     evaluation reward: 141.05\n",
      "episode: 273   score: 455.0   memory length: 10240   epsilon: 1.0    steps: 883     evaluation reward: 144.35\n",
      "episode: 274   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 400     evaluation reward: 144.2\n",
      "episode: 275   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 608     evaluation reward: 144.65\n",
      "episode: 276   score: 170.0   memory length: 10240   epsilon: 1.0    steps: 937     evaluation reward: 145.45\n",
      "episode: 277   score: 15.0   memory length: 10240   epsilon: 1.0    steps: 389     evaluation reward: 144.8\n",
      "episode: 278   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 550     evaluation reward: 144.9\n",
      "episode: 279   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 606     evaluation reward: 145.35\n",
      "episode: 280   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 615     evaluation reward: 145.7\n",
      "episode: 281   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 433     evaluation reward: 144.65\n",
      "episode: 282   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 673     evaluation reward: 145.75\n",
      "episode: 283   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 550     evaluation reward: 146.9\n",
      "episode: 284   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 787     evaluation reward: 144.0\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.007605. Value loss: 6.161860. Entropy: 1.640101.\n",
      "Iteration 2\n",
      "Policy loss: -0.024045. Value loss: 3.438861. Entropy: 1.633297.\n",
      "Iteration 3\n",
      "Policy loss: -0.028083. Value loss: 2.781745. Entropy: 1.631495.\n",
      "Iteration 4\n",
      "Policy loss: -0.025617. Value loss: 2.514025. Entropy: 1.630999.\n",
      "Iteration 5\n",
      "Policy loss: -0.030264. Value loss: 2.180026. Entropy: 1.635991.\n",
      "episode: 285   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 640     evaluation reward: 144.25\n",
      "episode: 286   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 687     evaluation reward: 145.0\n",
      "episode: 287   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 612     evaluation reward: 145.6\n",
      "episode: 288   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 792     evaluation reward: 146.05\n",
      "episode: 289   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 791     evaluation reward: 146.5\n",
      "episode: 290   score: 215.0   memory length: 10240   epsilon: 1.0    steps: 818     evaluation reward: 147.55\n",
      "episode: 291   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 622     evaluation reward: 148.05\n",
      "episode: 292   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 752     evaluation reward: 148.3\n",
      "now time :  2018-12-18 16:58:31.841388\n",
      "episode: 293   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 617     evaluation reward: 148.0\n",
      "episode: 294   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 583     evaluation reward: 148.5\n",
      "episode: 295   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 647     evaluation reward: 149.45\n",
      "episode: 296   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 728     evaluation reward: 150.55\n",
      "episode: 297   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 379     evaluation reward: 146.65\n",
      "episode: 298   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 650     evaluation reward: 146.15\n",
      "episode: 299   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 668     evaluation reward: 145.7\n",
      "episode: 300   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 628     evaluation reward: 144.7\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.012288. Value loss: 4.878797. Entropy: 1.619368.\n",
      "Iteration 2\n",
      "Policy loss: -0.021545. Value loss: 3.051989. Entropy: 1.613574.\n",
      "Iteration 3\n",
      "Policy loss: -0.025744. Value loss: 2.449471. Entropy: 1.610679.\n",
      "Iteration 4\n",
      "Policy loss: -0.031392. Value loss: 2.201094. Entropy: 1.608798.\n",
      "Iteration 5\n",
      "Policy loss: -0.031319. Value loss: 2.047060. Entropy: 1.611128.\n",
      "episode: 301   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 672     evaluation reward: 143.55\n",
      "episode: 302   score: 290.0   memory length: 10240   epsilon: 1.0    steps: 1281     evaluation reward: 145.35\n",
      "episode: 303   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 604     evaluation reward: 145.7\n",
      "episode: 304   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 537     evaluation reward: 145.95\n",
      "episode: 305   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 517     evaluation reward: 144.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 306   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 493     evaluation reward: 142.95\n",
      "episode: 307   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 610     evaluation reward: 139.9\n",
      "episode: 308   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 846     evaluation reward: 139.9\n",
      "episode: 309   score: 360.0   memory length: 10240   epsilon: 1.0    steps: 1110     evaluation reward: 141.7\n",
      "episode: 310   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 639     evaluation reward: 141.65\n",
      "episode: 311   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 672     evaluation reward: 141.65\n",
      "episode: 312   score: 425.0   memory length: 10240   epsilon: 1.0    steps: 865     evaluation reward: 144.7\n",
      "episode: 313   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 858     evaluation reward: 144.95\n",
      "episode: 314   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 637     evaluation reward: 144.45\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.010097. Value loss: 6.542595. Entropy: 1.610354.\n",
      "Iteration 2\n",
      "Policy loss: -0.017814. Value loss: 4.001567. Entropy: 1.605224.\n",
      "Iteration 3\n",
      "Policy loss: -0.020044. Value loss: 2.903257. Entropy: 1.599439.\n",
      "Iteration 4\n",
      "Policy loss: -0.023455. Value loss: 2.318392. Entropy: 1.597011.\n",
      "Iteration 5\n",
      "Policy loss: -0.024482. Value loss: 1.937050. Entropy: 1.599317.\n",
      "episode: 315   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 658     evaluation reward: 144.25\n",
      "episode: 316   score: 450.0   memory length: 10240   epsilon: 1.0    steps: 979     evaluation reward: 147.4\n",
      "episode: 317   score: 195.0   memory length: 10240   epsilon: 1.0    steps: 943     evaluation reward: 148.15\n",
      "episode: 318   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 813     evaluation reward: 149.05\n",
      "episode: 319   score: 100.0   memory length: 10240   epsilon: 1.0    steps: 659     evaluation reward: 148.85\n",
      "episode: 320   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 418     evaluation reward: 148.55\n",
      "episode: 321   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 860     evaluation reward: 149.55\n",
      "episode: 322   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 818     evaluation reward: 150.15\n",
      "episode: 323   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 632     evaluation reward: 151.0\n",
      "episode: 324   score: 195.0   memory length: 10240   epsilon: 1.0    steps: 938     evaluation reward: 151.85\n",
      "episode: 325   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 628     evaluation reward: 152.45\n",
      "episode: 326   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 751     evaluation reward: 150.2\n",
      "episode: 327   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 737     evaluation reward: 149.75\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.008520. Value loss: 5.319503. Entropy: 1.584631.\n",
      "Iteration 2\n",
      "Policy loss: -0.019390. Value loss: 3.201509. Entropy: 1.563422.\n",
      "Iteration 3\n",
      "Policy loss: -0.023431. Value loss: 2.489362. Entropy: 1.562814.\n",
      "Iteration 4\n",
      "Policy loss: -0.026407. Value loss: 2.136527. Entropy: 1.559577.\n",
      "Iteration 5\n",
      "Policy loss: -0.033957. Value loss: 1.935506. Entropy: 1.561964.\n",
      "episode: 328   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 687     evaluation reward: 150.05\n",
      "episode: 329   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 650     evaluation reward: 148.8\n",
      "episode: 330   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 598     evaluation reward: 148.25\n",
      "episode: 331   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 616     evaluation reward: 148.7\n",
      "episode: 332   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 451     evaluation reward: 148.65\n",
      "episode: 333   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 643     evaluation reward: 147.3\n",
      "episode: 334   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 798     evaluation reward: 147.3\n",
      "episode: 335   score: 160.0   memory length: 10240   epsilon: 1.0    steps: 714     evaluation reward: 147.15\n",
      "episode: 336   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 494     evaluation reward: 146.05\n",
      "episode: 337   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 685     evaluation reward: 147.7\n",
      "episode: 338   score: 225.0   memory length: 10240   epsilon: 1.0    steps: 848     evaluation reward: 147.45\n",
      "episode: 339   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 551     evaluation reward: 146.8\n",
      "episode: 340   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 733     evaluation reward: 146.65\n",
      "episode: 341   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 371     evaluation reward: 146.65\n",
      "episode: 342   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 677     evaluation reward: 148.55\n",
      "episode: 343   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 495     evaluation reward: 146.35\n",
      "episode: 344   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 661     evaluation reward: 145.6\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.019181. Value loss: 5.157097. Entropy: 1.582744.\n",
      "Iteration 2\n",
      "Policy loss: -0.027823. Value loss: 2.935156. Entropy: 1.573596.\n",
      "Iteration 3\n",
      "Policy loss: -0.034507. Value loss: 2.164186. Entropy: 1.564111.\n",
      "Iteration 4\n",
      "Policy loss: -0.039600. Value loss: 1.843061. Entropy: 1.562463.\n",
      "Iteration 5\n",
      "Policy loss: -0.038990. Value loss: 1.727520. Entropy: 1.563880.\n",
      "episode: 345   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 551     evaluation reward: 145.95\n",
      "episode: 346   score: 350.0   memory length: 10240   epsilon: 1.0    steps: 1248     evaluation reward: 149.15\n",
      "episode: 347   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 640     evaluation reward: 150.95\n",
      "episode: 348   score: 415.0   memory length: 10240   epsilon: 1.0    steps: 845     evaluation reward: 153.75\n",
      "episode: 349   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 630     evaluation reward: 153.75\n",
      "episode: 350   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 791     evaluation reward: 154.5\n",
      "episode: 351   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 781     evaluation reward: 155.05\n",
      "episode: 352   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 952     evaluation reward: 151.5\n",
      "episode: 353   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 548     evaluation reward: 151.5\n",
      "episode: 354   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 433     evaluation reward: 150.95\n",
      "episode: 355   score: 520.0   memory length: 10240   epsilon: 1.0    steps: 989     evaluation reward: 154.05\n",
      "episode: 356   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 620     evaluation reward: 155.2\n",
      "episode: 357   score: 205.0   memory length: 10240   epsilon: 1.0    steps: 978     evaluation reward: 155.45\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.011903. Value loss: 7.156602. Entropy: 1.596325.\n",
      "Iteration 2\n",
      "Policy loss: -0.022039. Value loss: 4.116855. Entropy: 1.588867.\n",
      "Iteration 3\n",
      "Policy loss: -0.021508. Value loss: 2.773904. Entropy: 1.587011.\n",
      "Iteration 4\n",
      "Policy loss: -0.023056. Value loss: 2.370266. Entropy: 1.583235.\n",
      "Iteration 5\n",
      "Policy loss: -0.034277. Value loss: 2.058634. Entropy: 1.585879.\n",
      "episode: 358   score: 140.0   memory length: 10240   epsilon: 1.0    steps: 661     evaluation reward: 154.75\n",
      "episode: 359   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 403     evaluation reward: 154.35\n",
      "episode: 360   score: 95.0   memory length: 10240   epsilon: 1.0    steps: 671     evaluation reward: 153.85\n",
      "episode: 361   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 688     evaluation reward: 152.75\n",
      "episode: 362   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 662     evaluation reward: 153.8\n",
      "episode: 363   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 526     evaluation reward: 154.35\n",
      "episode: 364   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 643     evaluation reward: 151.6\n",
      "now time :  2018-12-18 17:03:31.169213\n",
      "episode: 365   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 510     evaluation reward: 151.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 366   score: 160.0   memory length: 10240   epsilon: 1.0    steps: 780     evaluation reward: 150.55\n",
      "episode: 367   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 563     evaluation reward: 147.9\n",
      "episode: 368   score: 115.0   memory length: 10240   epsilon: 1.0    steps: 660     evaluation reward: 147.8\n",
      "episode: 369   score: 485.0   memory length: 10240   epsilon: 1.0    steps: 942     evaluation reward: 151.1\n",
      "episode: 370   score: 285.0   memory length: 10240   epsilon: 1.0    steps: 1038     evaluation reward: 152.75\n",
      "episode: 371   score: 130.0   memory length: 10240   epsilon: 1.0    steps: 589     evaluation reward: 152.6\n",
      "episode: 372   score: 235.0   memory length: 10240   epsilon: 1.0    steps: 1055     evaluation reward: 153.75\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.014632. Value loss: 6.500848. Entropy: 1.572691.\n",
      "Iteration 2\n",
      "Policy loss: -0.024054. Value loss: 3.576632. Entropy: 1.565891.\n",
      "Iteration 3\n",
      "Policy loss: -0.030016. Value loss: 2.788805. Entropy: 1.561235.\n",
      "Iteration 4\n",
      "Policy loss: -0.032543. Value loss: 2.380606. Entropy: 1.560065.\n",
      "Iteration 5\n",
      "Policy loss: -0.036150. Value loss: 2.066436. Entropy: 1.564644.\n",
      "episode: 373   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 591     evaluation reward: 150.25\n",
      "episode: 374   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 535     evaluation reward: 150.4\n",
      "episode: 375   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 736     evaluation reward: 151.4\n",
      "episode: 376   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 579     evaluation reward: 151.05\n",
      "episode: 377   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 616     evaluation reward: 151.8\n",
      "episode: 378   score: 115.0   memory length: 10240   epsilon: 1.0    steps: 676     evaluation reward: 151.75\n",
      "episode: 379   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 441     evaluation reward: 151.15\n",
      "episode: 380   score: 205.0   memory length: 10240   epsilon: 1.0    steps: 795     evaluation reward: 152.55\n",
      "episode: 381   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 914     evaluation reward: 154.05\n",
      "episode: 382   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 591     evaluation reward: 153.85\n",
      "episode: 383   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 758     evaluation reward: 153.55\n",
      "episode: 384   score: 100.0   memory length: 10240   epsilon: 1.0    steps: 523     evaluation reward: 153.05\n",
      "episode: 385   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 844     evaluation reward: 153.9\n",
      "episode: 386   score: 25.0   memory length: 10240   epsilon: 1.0    steps: 510     evaluation reward: 153.05\n",
      "episode: 387   score: 25.0   memory length: 10240   epsilon: 1.0    steps: 515     evaluation reward: 152.2\n",
      "episode: 388   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 524     evaluation reward: 151.35\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.018103. Value loss: 5.150569. Entropy: 1.544649.\n",
      "Iteration 2\n",
      "Policy loss: -0.025285. Value loss: 3.049103. Entropy: 1.543391.\n",
      "Iteration 3\n",
      "Policy loss: -0.032420. Value loss: 2.479271. Entropy: 1.544394.\n",
      "Iteration 4\n",
      "Policy loss: -0.033134. Value loss: 2.170018. Entropy: 1.534305.\n",
      "Iteration 5\n",
      "Policy loss: -0.037023. Value loss: 1.875447. Entropy: 1.535725.\n",
      "episode: 389   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 651     evaluation reward: 150.9\n",
      "episode: 390   score: 165.0   memory length: 10240   epsilon: 1.0    steps: 643     evaluation reward: 150.4\n",
      "episode: 391   score: 235.0   memory length: 10240   epsilon: 1.0    steps: 852     evaluation reward: 151.7\n",
      "episode: 392   score: 240.0   memory length: 10240   epsilon: 1.0    steps: 859     evaluation reward: 152.75\n",
      "episode: 393   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 399     evaluation reward: 152.75\n",
      "episode: 394   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 588     evaluation reward: 152.3\n",
      "episode: 395   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 539     evaluation reward: 151.5\n",
      "episode: 396   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 674     evaluation reward: 151.75\n",
      "episode: 397   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 574     evaluation reward: 151.75\n",
      "episode: 398   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 675     evaluation reward: 151.95\n",
      "episode: 399   score: 205.0   memory length: 10240   epsilon: 1.0    steps: 780     evaluation reward: 153.1\n",
      "episode: 400   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 718     evaluation reward: 153.35\n",
      "episode: 401   score: 310.0   memory length: 10240   epsilon: 1.0    steps: 982     evaluation reward: 155.35\n",
      "episode: 402   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 408     evaluation reward: 152.8\n",
      "episode: 403   score: 225.0   memory length: 10240   epsilon: 1.0    steps: 950     evaluation reward: 153.5\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.015687. Value loss: 5.586701. Entropy: 1.548824.\n",
      "Iteration 2\n",
      "Policy loss: -0.030952. Value loss: 3.083039. Entropy: 1.542223.\n",
      "Iteration 3\n",
      "Policy loss: -0.036415. Value loss: 2.455600. Entropy: 1.545098.\n",
      "Iteration 4\n",
      "Policy loss: -0.036397. Value loss: 2.218850. Entropy: 1.542003.\n",
      "Iteration 5\n",
      "Policy loss: -0.042386. Value loss: 2.062328. Entropy: 1.540228.\n",
      "episode: 404   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 710     evaluation reward: 154.0\n",
      "episode: 405   score: 200.0   memory length: 10240   epsilon: 1.0    steps: 833     evaluation reward: 155.65\n",
      "episode: 406   score: 240.0   memory length: 10240   epsilon: 1.0    steps: 1250     evaluation reward: 157.5\n",
      "episode: 407   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 539     evaluation reward: 157.05\n",
      "episode: 408   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 669     evaluation reward: 157.0\n",
      "episode: 409   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 442     evaluation reward: 153.7\n",
      "episode: 410   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 647     evaluation reward: 153.15\n",
      "episode: 411   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 667     evaluation reward: 153.7\n",
      "episode: 412   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 777     evaluation reward: 150.7\n",
      "episode: 413   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 598     evaluation reward: 150.15\n",
      "episode: 414   score: 85.0   memory length: 10240   epsilon: 1.0    steps: 712     evaluation reward: 149.8\n",
      "episode: 415   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 743     evaluation reward: 149.5\n",
      "episode: 416   score: 510.0   memory length: 10240   epsilon: 1.0    steps: 996     evaluation reward: 150.1\n",
      "episode: 417   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 469     evaluation reward: 149.7\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.016780. Value loss: 5.846007. Entropy: 1.554515.\n",
      "Iteration 2\n",
      "Policy loss: -0.029673. Value loss: 2.978432. Entropy: 1.531657.\n",
      "Iteration 3\n",
      "Policy loss: -0.032719. Value loss: 2.280695. Entropy: 1.536491.\n",
      "Iteration 4\n",
      "Policy loss: -0.036052. Value loss: 2.079138. Entropy: 1.535522.\n",
      "Iteration 5\n",
      "Policy loss: -0.037590. Value loss: 1.787984. Entropy: 1.531009.\n",
      "episode: 418   score: 20.0   memory length: 10240   epsilon: 1.0    steps: 606     evaluation reward: 147.8\n",
      "episode: 419   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 594     evaluation reward: 147.1\n",
      "episode: 420   score: 320.0   memory length: 10240   epsilon: 1.0    steps: 1086     evaluation reward: 149.8\n",
      "episode: 421   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 674     evaluation reward: 149.15\n",
      "episode: 422   score: 280.0   memory length: 10240   epsilon: 1.0    steps: 946     evaluation reward: 150.15\n",
      "episode: 423   score: 145.0   memory length: 10240   epsilon: 1.0    steps: 776     evaluation reward: 150.35\n",
      "episode: 424   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 765     evaluation reward: 148.85\n",
      "episode: 425   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 662     evaluation reward: 148.25\n",
      "episode: 426   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 692     evaluation reward: 148.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 427   score: 70.0   memory length: 10240   epsilon: 1.0    steps: 440     evaluation reward: 147.95\n",
      "episode: 428   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 638     evaluation reward: 147.7\n",
      "episode: 429   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 718     evaluation reward: 148.75\n",
      "episode: 430   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 827     evaluation reward: 150.4\n",
      "episode: 431   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 642     evaluation reward: 149.8\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.018600. Value loss: 6.125489. Entropy: 1.519716.\n",
      "Iteration 2\n",
      "Policy loss: -0.030241. Value loss: 3.214061. Entropy: 1.503461.\n",
      "Iteration 3\n",
      "Policy loss: -0.034973. Value loss: 2.429712. Entropy: 1.514540.\n",
      "Iteration 4\n",
      "Policy loss: -0.033711. Value loss: 2.112190. Entropy: 1.509143.\n",
      "Iteration 5\n",
      "Policy loss: -0.041093. Value loss: 1.896403. Entropy: 1.508600.\n",
      "episode: 432   score: 585.0   memory length: 10240   epsilon: 1.0    steps: 1172     evaluation reward: 154.85\n",
      "episode: 433   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 666     evaluation reward: 154.8\n",
      "episode: 434   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 509     evaluation reward: 154.1\n",
      "episode: 435   score: 270.0   memory length: 10240   epsilon: 1.0    steps: 751     evaluation reward: 155.2\n",
      "episode: 436   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 472     evaluation reward: 154.35\n",
      "now time :  2018-12-18 17:08:32.300451\n",
      "episode: 437   score: 145.0   memory length: 10240   epsilon: 1.0    steps: 624     evaluation reward: 153.7\n",
      "episode: 438   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 653     evaluation reward: 153.0\n",
      "episode: 439   score: 85.0   memory length: 10240   epsilon: 1.0    steps: 401     evaluation reward: 152.75\n",
      "episode: 440   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 764     evaluation reward: 153.2\n",
      "episode: 441   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 686     evaluation reward: 154.1\n",
      "episode: 442   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 532     evaluation reward: 152.75\n",
      "episode: 443   score: 250.0   memory length: 10240   epsilon: 1.0    steps: 846     evaluation reward: 154.9\n",
      "episode: 444   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 964     evaluation reward: 155.95\n",
      "episode: 445   score: 140.0   memory length: 10240   epsilon: 1.0    steps: 700     evaluation reward: 156.45\n",
      "episode: 446   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 610     evaluation reward: 154.05\n",
      "episode: 447   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 392     evaluation reward: 152.5\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.013799. Value loss: 4.972670. Entropy: 1.540946.\n",
      "Iteration 2\n",
      "Policy loss: -0.021669. Value loss: 2.829946. Entropy: 1.537994.\n",
      "Iteration 3\n",
      "Policy loss: -0.025868. Value loss: 2.358897. Entropy: 1.532287.\n",
      "Iteration 4\n",
      "Policy loss: -0.032646. Value loss: 2.079241. Entropy: 1.537132.\n",
      "Iteration 5\n",
      "Policy loss: -0.034027. Value loss: 1.860104. Entropy: 1.531869.\n",
      "episode: 448   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 466     evaluation reward: 149.7\n",
      "episode: 449   score: 225.0   memory length: 10240   epsilon: 1.0    steps: 983     evaluation reward: 150.6\n",
      "episode: 450   score: 5.0   memory length: 10240   epsilon: 1.0    steps: 489     evaluation reward: 149.1\n",
      "episode: 451   score: 530.0   memory length: 10240   epsilon: 1.0    steps: 1184     evaluation reward: 152.3\n",
      "episode: 452   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 603     evaluation reward: 151.95\n",
      "episode: 453   score: 390.0   memory length: 10240   epsilon: 1.0    steps: 1367     evaluation reward: 154.8\n",
      "episode: 454   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 467     evaluation reward: 154.35\n",
      "episode: 455   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 771     evaluation reward: 151.25\n",
      "episode: 456   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 789     evaluation reward: 151.55\n",
      "episode: 457   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 594     evaluation reward: 151.05\n",
      "episode: 458   score: 190.0   memory length: 10240   epsilon: 1.0    steps: 851     evaluation reward: 151.55\n",
      "episode: 459   score: 410.0   memory length: 10240   epsilon: 1.0    steps: 795     evaluation reward: 154.85\n",
      "episode: 460   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 503     evaluation reward: 155.0\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.019059. Value loss: 7.028117. Entropy: 1.528878.\n",
      "Iteration 2\n",
      "Policy loss: -0.024000. Value loss: 4.180926. Entropy: 1.512507.\n",
      "Iteration 3\n",
      "Policy loss: -0.030389. Value loss: 2.868172. Entropy: 1.511531.\n",
      "Iteration 4\n",
      "Policy loss: -0.035513. Value loss: 2.370953. Entropy: 1.513365.\n",
      "Iteration 5\n",
      "Policy loss: -0.039161. Value loss: 2.048398. Entropy: 1.507620.\n",
      "episode: 461   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 665     evaluation reward: 155.0\n",
      "episode: 462   score: 140.0   memory length: 10240   epsilon: 1.0    steps: 687     evaluation reward: 155.2\n",
      "episode: 463   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 708     evaluation reward: 155.65\n",
      "episode: 464   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 722     evaluation reward: 156.45\n",
      "episode: 465   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 525     evaluation reward: 156.35\n",
      "episode: 466   score: 100.0   memory length: 10240   epsilon: 1.0    steps: 706     evaluation reward: 155.75\n",
      "episode: 467   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 628     evaluation reward: 155.8\n",
      "episode: 468   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 502     evaluation reward: 155.1\n",
      "episode: 469   score: 15.0   memory length: 10240   epsilon: 1.0    steps: 451     evaluation reward: 150.4\n",
      "episode: 470   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 446     evaluation reward: 147.9\n",
      "episode: 471   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 754     evaluation reward: 147.7\n",
      "episode: 472   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 1092     evaluation reward: 146.7\n",
      "episode: 473   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 781     evaluation reward: 147.5\n",
      "episode: 474   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 656     evaluation reward: 148.1\n",
      "episode: 475   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 506     evaluation reward: 146.65\n",
      "episode: 476   score: 130.0   memory length: 10240   epsilon: 1.0    steps: 461     evaluation reward: 146.6\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.022015. Value loss: 4.418658. Entropy: 1.506074.\n",
      "Iteration 2\n",
      "Policy loss: -0.033211. Value loss: 2.659893. Entropy: 1.509310.\n",
      "Iteration 3\n",
      "Policy loss: -0.038587. Value loss: 2.216630. Entropy: 1.503267.\n",
      "Iteration 4\n",
      "Policy loss: -0.039449. Value loss: 1.967807. Entropy: 1.508085.\n",
      "Iteration 5\n",
      "Policy loss: -0.038450. Value loss: 1.763784. Entropy: 1.501536.\n",
      "episode: 477   score: 85.0   memory length: 10240   epsilon: 1.0    steps: 758     evaluation reward: 146.55\n",
      "episode: 478   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 788     evaluation reward: 146.95\n",
      "episode: 479   score: 145.0   memory length: 10240   epsilon: 1.0    steps: 476     evaluation reward: 147.8\n",
      "episode: 480   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 389     evaluation reward: 146.35\n",
      "episode: 481   score: 445.0   memory length: 10240   epsilon: 1.0    steps: 968     evaluation reward: 148.95\n",
      "episode: 482   score: 265.0   memory length: 10240   epsilon: 1.0    steps: 825     evaluation reward: 150.25\n",
      "episode: 483   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 384     evaluation reward: 149.2\n",
      "episode: 484   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 684     evaluation reward: 149.25\n",
      "episode: 485   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 798     evaluation reward: 148.95\n",
      "episode: 486   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 607     evaluation reward: 149.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 487   score: 5.0   memory length: 10240   epsilon: 1.0    steps: 585     evaluation reward: 149.6\n",
      "episode: 488   score: 335.0   memory length: 10240   epsilon: 1.0    steps: 759     evaluation reward: 152.3\n",
      "episode: 489   score: 255.0   memory length: 10240   epsilon: 1.0    steps: 833     evaluation reward: 153.75\n",
      "episode: 490   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 504     evaluation reward: 153.15\n",
      "episode: 491   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 498     evaluation reward: 151.25\n",
      "episode: 492   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 641     evaluation reward: 150.4\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.017089. Value loss: 5.967039. Entropy: 1.494846.\n",
      "Iteration 2\n",
      "Policy loss: -0.029316. Value loss: 3.386766. Entropy: 1.490639.\n",
      "Iteration 3\n",
      "Policy loss: -0.040266. Value loss: 2.575833. Entropy: 1.490611.\n",
      "Iteration 4\n",
      "Policy loss: -0.038157. Value loss: 2.299448. Entropy: 1.499021.\n",
      "Iteration 5\n",
      "Policy loss: -0.039626. Value loss: 2.143981. Entropy: 1.498093.\n",
      "episode: 493   score: 160.0   memory length: 10240   epsilon: 1.0    steps: 948     evaluation reward: 150.95\n",
      "episode: 494   score: 130.0   memory length: 10240   epsilon: 1.0    steps: 677     evaluation reward: 151.15\n",
      "episode: 495   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 520     evaluation reward: 150.85\n",
      "episode: 496   score: 230.0   memory length: 10240   epsilon: 1.0    steps: 874     evaluation reward: 151.35\n",
      "episode: 497   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 674     evaluation reward: 151.25\n",
      "episode: 498   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 485     evaluation reward: 150.35\n",
      "episode: 499   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 1167     evaluation reward: 150.4\n",
      "episode: 500   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 553     evaluation reward: 150.4\n",
      "episode: 501   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 442     evaluation reward: 147.75\n",
      "episode: 502   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 838     evaluation reward: 149.5\n",
      "episode: 503   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 664     evaluation reward: 149.35\n",
      "episode: 504   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 858     evaluation reward: 149.9\n",
      "episode: 505   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 540     evaluation reward: 148.7\n",
      "episode: 506   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 384     evaluation reward: 146.6\n",
      "episode: 507   score: 130.0   memory length: 10240   epsilon: 1.0    steps: 640     evaluation reward: 147.6\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.029425. Value loss: 4.358696. Entropy: 1.497202.\n",
      "Iteration 2\n",
      "Policy loss: -0.037066. Value loss: 2.393504. Entropy: 1.493450.\n",
      "Iteration 3\n",
      "Policy loss: -0.043158. Value loss: 1.897956. Entropy: 1.485702.\n",
      "Iteration 4\n",
      "Policy loss: -0.048798. Value loss: 1.677043. Entropy: 1.480711.\n",
      "Iteration 5\n",
      "Policy loss: -0.046543. Value loss: 1.397433. Entropy: 1.481333.\n",
      "episode: 508   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 620     evaluation reward: 147.75\n",
      "episode: 509   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 988     evaluation reward: 148.8\n",
      "now time :  2018-12-18 17:13:32.326753\n",
      "episode: 510   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 685     evaluation reward: 149.25\n",
      "episode: 511   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 775     evaluation reward: 149.7\n",
      "episode: 512   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 461     evaluation reward: 149.0\n",
      "episode: 513   score: 215.0   memory length: 10240   epsilon: 1.0    steps: 914     evaluation reward: 149.6\n",
      "episode: 514   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 666     evaluation reward: 150.0\n",
      "episode: 515   score: 410.0   memory length: 10240   epsilon: 1.0    steps: 777     evaluation reward: 152.9\n",
      "episode: 516   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 639     evaluation reward: 149.05\n",
      "episode: 517   score: 360.0   memory length: 10240   epsilon: 1.0    steps: 1097     evaluation reward: 151.1\n",
      "episode: 518   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 533     evaluation reward: 151.4\n",
      "episode: 519   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 791     evaluation reward: 152.6\n",
      "episode: 520   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 886     evaluation reward: 150.95\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.020003. Value loss: 6.291687. Entropy: 1.455899.\n",
      "Iteration 2\n",
      "Policy loss: -0.030426. Value loss: 3.236034. Entropy: 1.442772.\n",
      "Iteration 3\n",
      "Policy loss: -0.033687. Value loss: 2.232929. Entropy: 1.447488.\n",
      "Iteration 4\n",
      "Policy loss: -0.037727. Value loss: 1.917131. Entropy: 1.440297.\n",
      "Iteration 5\n",
      "Policy loss: -0.037261. Value loss: 1.777433. Entropy: 1.446967.\n",
      "episode: 521   score: 100.0   memory length: 10240   epsilon: 1.0    steps: 634     evaluation reward: 151.4\n",
      "episode: 522   score: 585.0   memory length: 10240   epsilon: 1.0    steps: 1717     evaluation reward: 154.45\n",
      "episode: 523   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 812     evaluation reward: 154.55\n",
      "episode: 524   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 627     evaluation reward: 155.0\n",
      "episode: 525   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 519     evaluation reward: 154.8\n",
      "episode: 526   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 491     evaluation reward: 154.6\n",
      "episode: 527   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 393     evaluation reward: 154.8\n",
      "episode: 528   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 517     evaluation reward: 154.65\n",
      "episode: 529   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 535     evaluation reward: 153.75\n",
      "episode: 530   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 664     evaluation reward: 152.7\n",
      "episode: 531   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 620     evaluation reward: 153.6\n",
      "episode: 532   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 623     evaluation reward: 148.85\n",
      "episode: 533   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 652     evaluation reward: 149.6\n",
      "episode: 534   score: 85.0   memory length: 10240   epsilon: 1.0    steps: 577     evaluation reward: 149.35\n",
      "episode: 535   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 781     evaluation reward: 148.45\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.030115. Value loss: 5.122715. Entropy: 1.419281.\n",
      "Iteration 2\n",
      "Policy loss: -0.033345. Value loss: 3.035265. Entropy: 1.413059.\n",
      "Iteration 3\n",
      "Policy loss: -0.036043. Value loss: 2.354501. Entropy: 1.403364.\n",
      "Iteration 4\n",
      "Policy loss: -0.040407. Value loss: 1.980096. Entropy: 1.407430.\n",
      "Iteration 5\n",
      "Policy loss: -0.044790. Value loss: 1.731683. Entropy: 1.408229.\n",
      "episode: 536   score: 160.0   memory length: 10240   epsilon: 1.0    steps: 886     evaluation reward: 149.55\n",
      "episode: 537   score: 15.0   memory length: 10240   epsilon: 1.0    steps: 387     evaluation reward: 148.25\n",
      "episode: 538   score: 655.0   memory length: 10240   epsilon: 1.0    steps: 1594     evaluation reward: 153.25\n",
      "episode: 539   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 394     evaluation reward: 152.9\n",
      "episode: 540   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 642     evaluation reward: 152.1\n",
      "episode: 541   score: 130.0   memory length: 10240   epsilon: 1.0    steps: 645     evaluation reward: 152.15\n",
      "episode: 542   score: 165.0   memory length: 10240   epsilon: 1.0    steps: 743     evaluation reward: 153.05\n",
      "episode: 543   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 616     evaluation reward: 151.45\n",
      "episode: 544   score: 140.0   memory length: 10240   epsilon: 1.0    steps: 645     evaluation reward: 150.75\n",
      "episode: 545   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 734     evaluation reward: 151.45\n",
      "episode: 546   score: 170.0   memory length: 10240   epsilon: 1.0    steps: 772     evaluation reward: 152.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 547   score: 145.0   memory length: 10240   epsilon: 1.0    steps: 739     evaluation reward: 152.95\n",
      "episode: 548   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 897     evaluation reward: 152.7\n",
      "episode: 549   score: 170.0   memory length: 10240   epsilon: 1.0    steps: 497     evaluation reward: 152.15\n",
      "episode: 550   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 553     evaluation reward: 152.45\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.017754. Value loss: 5.579874. Entropy: 1.424652.\n",
      "Iteration 2\n",
      "Policy loss: -0.028042. Value loss: 2.726170. Entropy: 1.415855.\n",
      "Iteration 3\n",
      "Policy loss: -0.033957. Value loss: 2.060862. Entropy: 1.425745.\n",
      "Iteration 4\n",
      "Policy loss: -0.038750. Value loss: 1.862630. Entropy: 1.423745.\n",
      "Iteration 5\n",
      "Policy loss: -0.042534. Value loss: 1.710075. Entropy: 1.431518.\n",
      "episode: 551   score: 305.0   memory length: 10240   epsilon: 1.0    steps: 763     evaluation reward: 150.2\n",
      "episode: 552   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 655     evaluation reward: 150.55\n",
      "episode: 553   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 714     evaluation reward: 147.75\n",
      "episode: 554   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 778     evaluation reward: 148.2\n",
      "episode: 555   score: 215.0   memory length: 10240   epsilon: 1.0    steps: 864     evaluation reward: 148.25\n",
      "episode: 556   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 568     evaluation reward: 147.8\n",
      "episode: 557   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 680     evaluation reward: 147.45\n",
      "episode: 558   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 779     evaluation reward: 147.35\n",
      "episode: 559   score: 380.0   memory length: 10240   epsilon: 1.0    steps: 799     evaluation reward: 147.05\n",
      "episode: 560   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 688     evaluation reward: 147.0\n",
      "episode: 561   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 566     evaluation reward: 146.25\n",
      "episode: 562   score: 315.0   memory length: 10240   epsilon: 1.0    steps: 1150     evaluation reward: 148.0\n",
      "episode: 563   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 711     evaluation reward: 147.7\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.023422. Value loss: 6.268098. Entropy: 1.395209.\n",
      "Iteration 2\n",
      "Policy loss: -0.034343. Value loss: 3.387109. Entropy: 1.378463.\n",
      "Iteration 3\n",
      "Policy loss: -0.043716. Value loss: 2.839872. Entropy: 1.377925.\n",
      "Iteration 4\n",
      "Policy loss: -0.041587. Value loss: 2.256231. Entropy: 1.377288.\n",
      "Iteration 5\n",
      "Policy loss: -0.044992. Value loss: 2.005783. Entropy: 1.367678.\n",
      "episode: 564   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 714     evaluation reward: 147.7\n",
      "episode: 565   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 759     evaluation reward: 149.25\n",
      "episode: 566   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 548     evaluation reward: 149.15\n",
      "episode: 567   score: 160.0   memory length: 10240   epsilon: 1.0    steps: 772     evaluation reward: 150.25\n",
      "episode: 568   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 755     evaluation reward: 150.85\n",
      "episode: 569   score: 170.0   memory length: 10240   epsilon: 1.0    steps: 925     evaluation reward: 152.4\n",
      "episode: 570   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 680     evaluation reward: 153.1\n",
      "episode: 571   score: 230.0   memory length: 10240   epsilon: 1.0    steps: 934     evaluation reward: 154.3\n",
      "episode: 572   score: 240.0   memory length: 10240   epsilon: 1.0    steps: 843     evaluation reward: 155.35\n",
      "episode: 573   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 577     evaluation reward: 154.05\n",
      "episode: 574   score: 265.0   memory length: 10240   epsilon: 1.0    steps: 829     evaluation reward: 154.9\n",
      "episode: 575   score: 385.0   memory length: 10240   epsilon: 1.0    steps: 793     evaluation reward: 158.1\n",
      "episode: 576   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 585     evaluation reward: 157.25\n",
      "episode: 577   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 663     evaluation reward: 157.75\n",
      "episode: 578   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 441     evaluation reward: 157.3\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.032271. Value loss: 5.221844. Entropy: 1.358440.\n",
      "Iteration 2\n",
      "Policy loss: -0.040515. Value loss: 2.668328. Entropy: 1.361970.\n",
      "Iteration 3\n",
      "Policy loss: -0.044522. Value loss: 2.012704. Entropy: 1.357156.\n",
      "Iteration 4\n",
      "Policy loss: -0.050312. Value loss: 1.779696. Entropy: 1.353085.\n",
      "Iteration 5\n",
      "Policy loss: -0.048625. Value loss: 1.621342. Entropy: 1.357201.\n",
      "episode: 579   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 385     evaluation reward: 156.9\n",
      "now time :  2018-12-18 17:18:34.718746\n",
      "episode: 580   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 944     evaluation reward: 157.5\n",
      "episode: 581   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 769     evaluation reward: 154.85\n",
      "episode: 582   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 654     evaluation reward: 153.1\n",
      "episode: 583   score: 290.0   memory length: 10240   epsilon: 1.0    steps: 840     evaluation reward: 155.55\n",
      "episode: 584   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 601     evaluation reward: 155.85\n",
      "episode: 585   score: 270.0   memory length: 10240   epsilon: 1.0    steps: 961     evaluation reward: 156.75\n",
      "episode: 586   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 867     evaluation reward: 157.75\n",
      "episode: 587   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 619     evaluation reward: 158.8\n",
      "episode: 588   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 566     evaluation reward: 156.8\n",
      "episode: 589   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 676     evaluation reward: 156.35\n",
      "episode: 590   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 520     evaluation reward: 156.2\n",
      "episode: 591   score: 145.0   memory length: 10240   epsilon: 1.0    steps: 712     evaluation reward: 157.2\n",
      "episode: 592   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 606     evaluation reward: 156.45\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.033395. Value loss: 5.427647. Entropy: 1.304999.\n",
      "Iteration 2\n",
      "Policy loss: -0.046550. Value loss: 3.010705. Entropy: 1.293118.\n",
      "Iteration 3\n",
      "Policy loss: -0.053846. Value loss: 2.268003. Entropy: 1.283070.\n",
      "Iteration 4\n",
      "Policy loss: -0.053813. Value loss: 1.937970. Entropy: 1.296159.\n",
      "Iteration 5\n",
      "Policy loss: -0.058126. Value loss: 1.808819. Entropy: 1.289567.\n",
      "episode: 593   score: 460.0   memory length: 10240   epsilon: 1.0    steps: 946     evaluation reward: 159.45\n",
      "episode: 594   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 652     evaluation reward: 159.35\n",
      "episode: 595   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 381     evaluation reward: 159.5\n",
      "episode: 596   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 808     evaluation reward: 159.0\n",
      "episode: 597   score: 140.0   memory length: 10240   epsilon: 1.0    steps: 798     evaluation reward: 159.95\n",
      "episode: 598   score: 100.0   memory length: 10240   epsilon: 1.0    steps: 571     evaluation reward: 160.3\n",
      "episode: 599   score: 315.0   memory length: 10240   epsilon: 1.0    steps: 1041     evaluation reward: 161.35\n",
      "episode: 600   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 380     evaluation reward: 160.65\n",
      "episode: 601   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 641     evaluation reward: 161.4\n",
      "episode: 602   score: 260.0   memory length: 10240   epsilon: 1.0    steps: 1364     evaluation reward: 161.9\n",
      "episode: 603   score: 130.0   memory length: 10240   epsilon: 1.0    steps: 762     evaluation reward: 161.1\n",
      "episode: 604   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 622     evaluation reward: 160.55\n",
      "episode: 605   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 676     evaluation reward: 160.8\n",
      "episode: 606   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 625     evaluation reward: 160.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.034095. Value loss: 5.050344. Entropy: 1.308748.\n",
      "Iteration 2\n",
      "Policy loss: -0.041481. Value loss: 2.739177. Entropy: 1.286111.\n",
      "Iteration 3\n",
      "Policy loss: -0.047750. Value loss: 2.009619. Entropy: 1.289421.\n",
      "Iteration 4\n",
      "Policy loss: -0.047737. Value loss: 1.776197. Entropy: 1.295384.\n",
      "Iteration 5\n",
      "Policy loss: -0.050093. Value loss: 1.645231. Entropy: 1.290545.\n",
      "episode: 607   score: 365.0   memory length: 10240   epsilon: 1.0    steps: 1323     evaluation reward: 163.15\n",
      "episode: 608   score: 160.0   memory length: 10240   epsilon: 1.0    steps: 687     evaluation reward: 163.4\n",
      "episode: 609   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 677     evaluation reward: 163.6\n",
      "episode: 610   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 796     evaluation reward: 164.2\n",
      "episode: 611   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 593     evaluation reward: 163.45\n",
      "episode: 612   score: 610.0   memory length: 10240   epsilon: 1.0    steps: 1482     evaluation reward: 169.0\n",
      "episode: 613   score: 460.0   memory length: 10240   epsilon: 1.0    steps: 1308     evaluation reward: 171.45\n",
      "episode: 614   score: 260.0   memory length: 10240   epsilon: 1.0    steps: 1138     evaluation reward: 172.8\n",
      "episode: 615   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 582     evaluation reward: 170.25\n",
      "episode: 616   score: 160.0   memory length: 10240   epsilon: 1.0    steps: 691     evaluation reward: 170.6\n",
      "episode: 617   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 617     evaluation reward: 167.8\n",
      "episode: 618   score: 165.0   memory length: 10240   epsilon: 1.0    steps: 695     evaluation reward: 168.95\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.038279. Value loss: 6.773267. Entropy: 1.276131.\n",
      "Iteration 2\n",
      "Policy loss: -0.052638. Value loss: 3.217662. Entropy: 1.268154.\n",
      "Iteration 3\n",
      "Policy loss: -0.054258. Value loss: 2.510579. Entropy: 1.272617.\n",
      "Iteration 4\n",
      "Policy loss: -0.058364. Value loss: 2.277186. Entropy: 1.269614.\n",
      "Iteration 5\n",
      "Policy loss: -0.060152. Value loss: 2.063595. Entropy: 1.261986.\n",
      "episode: 619   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 965     evaluation reward: 169.55\n",
      "episode: 620   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 637     evaluation reward: 169.1\n",
      "episode: 621   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 482     evaluation reward: 168.75\n",
      "episode: 622   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 644     evaluation reward: 163.95\n",
      "episode: 623   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 641     evaluation reward: 163.5\n",
      "episode: 624   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 675     evaluation reward: 163.95\n",
      "episode: 625   score: 165.0   memory length: 10240   epsilon: 1.0    steps: 826     evaluation reward: 165.15\n",
      "episode: 626   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 660     evaluation reward: 165.45\n",
      "episode: 627   score: 265.0   memory length: 10240   epsilon: 1.0    steps: 762     evaluation reward: 167.2\n",
      "episode: 628   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 421     evaluation reward: 166.65\n",
      "episode: 629   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 632     evaluation reward: 167.2\n",
      "episode: 630   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 501     evaluation reward: 167.4\n",
      "episode: 631   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 368     evaluation reward: 166.15\n",
      "episode: 632   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 471     evaluation reward: 165.8\n",
      "episode: 633   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 367     evaluation reward: 164.8\n",
      "episode: 634   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 515     evaluation reward: 164.6\n",
      "episode: 635   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 621     evaluation reward: 163.6\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.027339. Value loss: 4.728253. Entropy: 1.315722.\n",
      "Iteration 2\n",
      "Policy loss: -0.040992. Value loss: 2.853614. Entropy: 1.319298.\n",
      "Iteration 3\n",
      "Policy loss: -0.044289. Value loss: 2.288420. Entropy: 1.319655.\n",
      "Iteration 4\n",
      "Policy loss: -0.046661. Value loss: 1.903577. Entropy: 1.321744.\n",
      "Iteration 5\n",
      "Policy loss: -0.049184. Value loss: 1.665263. Entropy: 1.319085.\n",
      "episode: 636   score: 285.0   memory length: 10240   epsilon: 1.0    steps: 1026     evaluation reward: 164.85\n",
      "episode: 637   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 533     evaluation reward: 166.25\n",
      "episode: 638   score: 520.0   memory length: 10240   epsilon: 1.0    steps: 1155     evaluation reward: 164.9\n",
      "episode: 639   score: 335.0   memory length: 10240   epsilon: 1.0    steps: 750     evaluation reward: 167.75\n",
      "episode: 640   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 366     evaluation reward: 167.65\n",
      "episode: 641   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 599     evaluation reward: 167.45\n",
      "episode: 642   score: 215.0   memory length: 10240   epsilon: 1.0    steps: 907     evaluation reward: 167.95\n",
      "episode: 643   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 635     evaluation reward: 168.25\n",
      "episode: 644   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 478     evaluation reward: 167.65\n",
      "episode: 645   score: 410.0   memory length: 10240   epsilon: 1.0    steps: 779     evaluation reward: 169.65\n",
      "episode: 646   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 460     evaluation reward: 168.3\n",
      "episode: 647   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 522     evaluation reward: 167.15\n",
      "episode: 648   score: 20.0   memory length: 10240   epsilon: 1.0    steps: 543     evaluation reward: 166.25\n",
      "episode: 649   score: 315.0   memory length: 10240   epsilon: 1.0    steps: 1108     evaluation reward: 167.7\n",
      "now time :  2018-12-18 17:23:24.823232\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.032004. Value loss: 6.927610. Entropy: 1.245830.\n",
      "Iteration 2\n",
      "Policy loss: -0.042306. Value loss: 3.562603. Entropy: 1.238422.\n",
      "Iteration 3\n",
      "Policy loss: -0.046157. Value loss: 2.883557. Entropy: 1.234345.\n",
      "Iteration 4\n",
      "Policy loss: -0.049808. Value loss: 2.535264. Entropy: 1.236028.\n",
      "Iteration 5\n",
      "Policy loss: -0.052000. Value loss: 2.284819. Entropy: 1.236790.\n",
      "episode: 650   score: 200.0   memory length: 10240   epsilon: 1.0    steps: 1145     evaluation reward: 169.35\n",
      "episode: 651   score: 215.0   memory length: 10240   epsilon: 1.0    steps: 810     evaluation reward: 168.45\n",
      "episode: 652   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 787     evaluation reward: 168.45\n",
      "episode: 653   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 635     evaluation reward: 168.6\n",
      "episode: 654   score: 20.0   memory length: 10240   epsilon: 1.0    steps: 570     evaluation reward: 167.55\n",
      "episode: 655   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 532     evaluation reward: 166.5\n",
      "episode: 656   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 496     evaluation reward: 165.6\n",
      "episode: 657   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 644     evaluation reward: 165.6\n",
      "episode: 658   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 343     evaluation reward: 164.25\n",
      "episode: 659   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 730     evaluation reward: 162.25\n",
      "episode: 660   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 602     evaluation reward: 162.0\n",
      "episode: 661   score: 480.0   memory length: 10240   epsilon: 1.0    steps: 921     evaluation reward: 165.75\n",
      "episode: 662   score: 100.0   memory length: 10240   epsilon: 1.0    steps: 529     evaluation reward: 163.6\n",
      "episode: 663   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 728     evaluation reward: 164.1\n",
      "episode: 664   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 631     evaluation reward: 163.6\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.032523. Value loss: 6.243318. Entropy: 1.235309.\n",
      "Iteration 2\n",
      "Policy loss: -0.037094. Value loss: 2.818057. Entropy: 1.229555.\n",
      "Iteration 3\n",
      "Policy loss: -0.043303. Value loss: 2.166268. Entropy: 1.224653.\n",
      "Iteration 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy loss: -0.050714. Value loss: 1.850624. Entropy: 1.224182.\n",
      "Iteration 5\n",
      "Policy loss: -0.047835. Value loss: 1.678307. Entropy: 1.229036.\n",
      "episode: 665   score: 515.0   memory length: 10240   epsilon: 1.0    steps: 1166     evaluation reward: 166.65\n",
      "episode: 666   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 532     evaluation reward: 166.5\n",
      "episode: 667   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 421     evaluation reward: 165.7\n",
      "episode: 668   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 380     evaluation reward: 165.7\n",
      "episode: 669   score: 140.0   memory length: 10240   epsilon: 1.0    steps: 795     evaluation reward: 165.4\n",
      "episode: 670   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 591     evaluation reward: 164.8\n",
      "episode: 671   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 576     evaluation reward: 163.4\n",
      "episode: 672   score: 175.0   memory length: 10240   epsilon: 1.0    steps: 704     evaluation reward: 162.75\n",
      "episode: 673   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 514     evaluation reward: 162.75\n",
      "episode: 674   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 1001     evaluation reward: 162.2\n",
      "episode: 675   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 367     evaluation reward: 158.85\n",
      "episode: 676   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 610     evaluation reward: 159.0\n",
      "episode: 677   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 917     evaluation reward: 159.75\n",
      "episode: 678   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 491     evaluation reward: 159.7\n",
      "episode: 679   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 391     evaluation reward: 159.25\n",
      "episode: 680   score: 215.0   memory length: 10240   epsilon: 1.0    steps: 854     evaluation reward: 160.2\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.029200. Value loss: 5.279862. Entropy: 1.250499.\n",
      "Iteration 2\n",
      "Policy loss: -0.038972. Value loss: 2.748421. Entropy: 1.242553.\n",
      "Iteration 3\n",
      "Policy loss: -0.047301. Value loss: 2.187814. Entropy: 1.251458.\n",
      "Iteration 4\n",
      "Policy loss: -0.050601. Value loss: 1.840332. Entropy: 1.245135.\n",
      "Iteration 5\n",
      "Policy loss: -0.054386. Value loss: 1.827673. Entropy: 1.237964.\n",
      "episode: 681   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 919     evaluation reward: 160.2\n",
      "episode: 682   score: 225.0   memory length: 10240   epsilon: 1.0    steps: 941     evaluation reward: 161.55\n",
      "episode: 683   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 499     evaluation reward: 159.9\n",
      "episode: 684   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 677     evaluation reward: 159.65\n",
      "episode: 685   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 423     evaluation reward: 158.15\n",
      "episode: 686   score: 85.0   memory length: 10240   epsilon: 1.0    steps: 564     evaluation reward: 156.9\n",
      "episode: 687   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 705     evaluation reward: 157.65\n",
      "episode: 688   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 712     evaluation reward: 157.5\n",
      "episode: 689   score: 190.0   memory length: 10240   epsilon: 1.0    steps: 792     evaluation reward: 157.3\n",
      "episode: 690   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 707     evaluation reward: 157.6\n",
      "episode: 691   score: 240.0   memory length: 10240   epsilon: 1.0    steps: 1317     evaluation reward: 158.55\n",
      "episode: 692   score: 235.0   memory length: 10240   epsilon: 1.0    steps: 954     evaluation reward: 160.1\n",
      "episode: 693   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 479     evaluation reward: 156.0\n",
      "episode: 694   score: 115.0   memory length: 10240   epsilon: 1.0    steps: 730     evaluation reward: 155.95\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.033346. Value loss: 4.344596. Entropy: 1.229946.\n",
      "Iteration 2\n",
      "Policy loss: -0.047508. Value loss: 2.179441. Entropy: 1.203734.\n",
      "Iteration 3\n",
      "Policy loss: -0.058183. Value loss: 1.675590. Entropy: 1.211031.\n",
      "Iteration 4\n",
      "Policy loss: -0.058089. Value loss: 1.505826. Entropy: 1.207855.\n",
      "Iteration 5\n",
      "Policy loss: -0.059393. Value loss: 1.373698. Entropy: 1.204717.\n",
      "episode: 695   score: 240.0   memory length: 10240   epsilon: 1.0    steps: 1046     evaluation reward: 157.75\n",
      "episode: 696   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 479     evaluation reward: 156.75\n",
      "episode: 697   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 404     evaluation reward: 155.9\n",
      "episode: 698   score: 160.0   memory length: 10240   epsilon: 1.0    steps: 794     evaluation reward: 156.5\n",
      "episode: 699   score: 260.0   memory length: 10240   epsilon: 1.0    steps: 886     evaluation reward: 155.95\n",
      "episode: 700   score: 160.0   memory length: 10240   epsilon: 1.0    steps: 701     evaluation reward: 156.9\n",
      "episode: 701   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 700     evaluation reward: 157.25\n",
      "episode: 702   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 615     evaluation reward: 156.0\n",
      "episode: 703   score: 70.0   memory length: 10240   epsilon: 1.0    steps: 631     evaluation reward: 155.4\n",
      "episode: 704   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 620     evaluation reward: 155.65\n",
      "episode: 705   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 626     evaluation reward: 155.65\n",
      "episode: 706   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 487     evaluation reward: 156.15\n",
      "episode: 707   score: 370.0   memory length: 10240   epsilon: 1.0    steps: 769     evaluation reward: 156.2\n",
      "episode: 708   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 633     evaluation reward: 155.7\n",
      "episode: 709   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 628     evaluation reward: 155.35\n",
      "episode: 710   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 455     evaluation reward: 154.05\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.040972. Value loss: 5.649442. Entropy: 1.165082.\n",
      "Iteration 2\n",
      "Policy loss: -0.050426. Value loss: 3.175034. Entropy: 1.152068.\n",
      "Iteration 3\n",
      "Policy loss: -0.051003. Value loss: 2.445430. Entropy: 1.156089.\n",
      "Iteration 4\n",
      "Policy loss: -0.054845. Value loss: 2.013244. Entropy: 1.160510.\n",
      "Iteration 5\n",
      "Policy loss: -0.059494. Value loss: 1.780964. Entropy: 1.161563.\n",
      "episode: 711   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 720     evaluation reward: 154.55\n",
      "episode: 712   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 752     evaluation reward: 150.25\n",
      "episode: 713   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 627     evaluation reward: 146.7\n",
      "episode: 714   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 792     evaluation reward: 144.9\n",
      "episode: 715   score: 140.0   memory length: 10240   epsilon: 1.0    steps: 690     evaluation reward: 144.75\n",
      "episode: 716   score: 160.0   memory length: 10240   epsilon: 1.0    steps: 933     evaluation reward: 144.75\n",
      "episode: 717   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 455     evaluation reward: 144.85\n",
      "episode: 718   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 671     evaluation reward: 144.25\n",
      "episode: 719   score: 115.0   memory length: 10240   epsilon: 1.0    steps: 560     evaluation reward: 143.3\n",
      "episode: 720   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 794     evaluation reward: 144.0\n",
      "episode: 721   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 689     evaluation reward: 144.25\n",
      "episode: 722   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 739     evaluation reward: 145.05\n",
      "now time :  2018-12-18 17:28:31.394126\n",
      "episode: 723   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 488     evaluation reward: 145.15\n",
      "episode: 724   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 596     evaluation reward: 144.85\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.032569. Value loss: 6.081065. Entropy: 1.235734.\n",
      "Iteration 2\n",
      "Policy loss: -0.042921. Value loss: 3.289661. Entropy: 1.216024.\n",
      "Iteration 3\n",
      "Policy loss: -0.046956. Value loss: 2.280879. Entropy: 1.221808.\n",
      "Iteration 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy loss: -0.052476. Value loss: 1.955612. Entropy: 1.232232.\n",
      "Iteration 5\n",
      "Policy loss: -0.054092. Value loss: 1.782946. Entropy: 1.219886.\n",
      "episode: 725   score: 515.0   memory length: 10240   epsilon: 1.0    steps: 1505     evaluation reward: 148.35\n",
      "episode: 726   score: 220.0   memory length: 10240   epsilon: 1.0    steps: 1030     evaluation reward: 149.2\n",
      "episode: 727   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 805     evaluation reward: 147.9\n",
      "episode: 728   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 745     evaluation reward: 148.55\n",
      "episode: 729   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 792     evaluation reward: 148.9\n",
      "episode: 730   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 820     evaluation reward: 149.75\n",
      "episode: 731   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 506     evaluation reward: 150.4\n",
      "episode: 732   score: 300.0   memory length: 10240   epsilon: 1.0    steps: 1062     evaluation reward: 152.65\n",
      "episode: 733   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 596     evaluation reward: 153.2\n",
      "episode: 734   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 475     evaluation reward: 153.05\n",
      "episode: 735   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 549     evaluation reward: 152.8\n",
      "episode: 736   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 722     evaluation reward: 151.75\n",
      "episode: 737   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 533     evaluation reward: 150.65\n",
      "episode: 738   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 653     evaluation reward: 146.65\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.043212. Value loss: 4.411721. Entropy: 1.189995.\n",
      "Iteration 2\n",
      "Policy loss: -0.056545. Value loss: 2.325990. Entropy: 1.198936.\n",
      "Iteration 3\n",
      "Policy loss: -0.059430. Value loss: 1.823936. Entropy: 1.198772.\n",
      "Iteration 4\n",
      "Policy loss: -0.060361. Value loss: 1.515585. Entropy: 1.209151.\n",
      "Iteration 5\n",
      "Policy loss: -0.065820. Value loss: 1.394321. Entropy: 1.197995.\n",
      "episode: 739   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 543     evaluation reward: 144.5\n",
      "episode: 740   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 621     evaluation reward: 144.9\n",
      "episode: 741   score: 305.0   memory length: 10240   epsilon: 1.0    steps: 1062     evaluation reward: 146.85\n",
      "episode: 742   score: 285.0   memory length: 10240   epsilon: 1.0    steps: 973     evaluation reward: 147.55\n",
      "episode: 743   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 741     evaluation reward: 148.45\n",
      "episode: 744   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 655     evaluation reward: 149.0\n",
      "episode: 745   score: 160.0   memory length: 10240   epsilon: 1.0    steps: 798     evaluation reward: 146.5\n",
      "episode: 746   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 616     evaluation reward: 146.95\n",
      "episode: 747   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 651     evaluation reward: 148.45\n",
      "episode: 748   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 697     evaluation reward: 149.3\n",
      "episode: 749   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 511     evaluation reward: 147.25\n",
      "episode: 750   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 514     evaluation reward: 146.5\n",
      "episode: 751   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 1098     evaluation reward: 146.45\n",
      "episode: 752   score: 70.0   memory length: 10240   epsilon: 1.0    steps: 647     evaluation reward: 145.6\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.044583. Value loss: 4.928664. Entropy: 1.216352.\n",
      "Iteration 2\n",
      "Policy loss: -0.055847. Value loss: 2.724338. Entropy: 1.214159.\n",
      "Iteration 3\n",
      "Policy loss: -0.056293. Value loss: 2.078713. Entropy: 1.215561.\n",
      "Iteration 4\n",
      "Policy loss: -0.058347. Value loss: 1.757655. Entropy: 1.222035.\n",
      "Iteration 5\n",
      "Policy loss: -0.065261. Value loss: 1.607911. Entropy: 1.208827.\n",
      "episode: 753   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 630     evaluation reward: 145.85\n",
      "episode: 754   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 712     evaluation reward: 146.3\n",
      "episode: 755   score: 100.0   memory length: 10240   epsilon: 1.0    steps: 543     evaluation reward: 146.2\n",
      "episode: 756   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 607     evaluation reward: 146.5\n",
      "episode: 757   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 399     evaluation reward: 145.6\n",
      "episode: 758   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 614     evaluation reward: 146.2\n",
      "episode: 759   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 381     evaluation reward: 145.05\n",
      "episode: 760   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 622     evaluation reward: 145.3\n",
      "episode: 761   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 465     evaluation reward: 141.55\n",
      "episode: 762   score: 165.0   memory length: 10240   epsilon: 1.0    steps: 646     evaluation reward: 142.2\n",
      "episode: 763   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 678     evaluation reward: 142.2\n",
      "episode: 764   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 729     evaluation reward: 142.95\n",
      "episode: 765   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 711     evaluation reward: 139.35\n",
      "episode: 766   score: 230.0   memory length: 10240   epsilon: 1.0    steps: 975     evaluation reward: 140.9\n",
      "episode: 767   score: 160.0   memory length: 10240   epsilon: 1.0    steps: 670     evaluation reward: 141.7\n",
      "episode: 768   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 497     evaluation reward: 141.3\n",
      "episode: 769   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 805     evaluation reward: 140.55\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.034274. Value loss: 4.463120. Entropy: 1.214670.\n",
      "Iteration 2\n",
      "Policy loss: -0.049768. Value loss: 2.653398. Entropy: 1.212624.\n",
      "Iteration 3\n",
      "Policy loss: -0.054890. Value loss: 2.131629. Entropy: 1.207091.\n",
      "Iteration 4\n",
      "Policy loss: -0.056626. Value loss: 1.932721. Entropy: 1.214964.\n",
      "Iteration 5\n",
      "Policy loss: -0.060515. Value loss: 1.743397. Entropy: 1.212208.\n",
      "episode: 770   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 519     evaluation reward: 141.15\n",
      "episode: 771   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 700     evaluation reward: 141.3\n",
      "episode: 772   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 375     evaluation reward: 139.9\n",
      "episode: 773   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 580     evaluation reward: 140.7\n",
      "episode: 774   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 637     evaluation reward: 139.65\n",
      "episode: 775   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 485     evaluation reward: 140.4\n",
      "episode: 776   score: 315.0   memory length: 10240   epsilon: 1.0    steps: 1008     evaluation reward: 142.95\n",
      "episode: 777   score: 100.0   memory length: 10240   epsilon: 1.0    steps: 662     evaluation reward: 141.85\n",
      "episode: 778   score: 190.0   memory length: 10240   epsilon: 1.0    steps: 923     evaluation reward: 142.7\n",
      "episode: 779   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 644     evaluation reward: 143.15\n",
      "episode: 780   score: 115.0   memory length: 10240   epsilon: 1.0    steps: 675     evaluation reward: 142.15\n",
      "episode: 781   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 526     evaluation reward: 140.85\n",
      "episode: 782   score: 145.0   memory length: 10240   epsilon: 1.0    steps: 612     evaluation reward: 140.05\n",
      "episode: 783   score: 535.0   memory length: 10240   epsilon: 1.0    steps: 1273     evaluation reward: 144.15\n",
      "episode: 784   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 653     evaluation reward: 143.4\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.033675. Value loss: 4.725961. Entropy: 1.242996.\n",
      "Iteration 2\n",
      "Policy loss: -0.048436. Value loss: 2.542563. Entropy: 1.223069.\n",
      "Iteration 3\n",
      "Policy loss: -0.048555. Value loss: 1.982346. Entropy: 1.209997.\n",
      "Iteration 4\n",
      "Policy loss: -0.055457. Value loss: 1.713471. Entropy: 1.221149.\n",
      "Iteration 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy loss: -0.052724. Value loss: 1.595304. Entropy: 1.228303.\n",
      "episode: 785   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 747     evaluation reward: 144.3\n",
      "episode: 786   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 787     evaluation reward: 145.55\n",
      "episode: 787   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 734     evaluation reward: 145.25\n",
      "episode: 788   score: 325.0   memory length: 10240   epsilon: 1.0    steps: 1008     evaluation reward: 147.3\n",
      "episode: 789   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 1127     evaluation reward: 147.5\n",
      "episode: 790   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 353     evaluation reward: 146.8\n",
      "episode: 791   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 550     evaluation reward: 145.45\n",
      "episode: 792   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 554     evaluation reward: 144.0\n",
      "episode: 793   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 830     evaluation reward: 145.6\n",
      "now time :  2018-12-18 17:33:38.582268\n",
      "episode: 794   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 790     evaluation reward: 146.0\n",
      "episode: 795   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 622     evaluation reward: 144.7\n",
      "episode: 796   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 579     evaluation reward: 144.55\n",
      "episode: 797   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 382     evaluation reward: 144.65\n",
      "episode: 798   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 667     evaluation reward: 144.15\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.039129. Value loss: 4.175965. Entropy: 1.190071.\n",
      "Iteration 2\n",
      "Policy loss: -0.053474. Value loss: 2.161539. Entropy: 1.178675.\n",
      "Iteration 3\n",
      "Policy loss: -0.054464. Value loss: 1.686406. Entropy: 1.189302.\n",
      "Iteration 4\n",
      "Policy loss: -0.059509. Value loss: 1.448553. Entropy: 1.178091.\n",
      "Iteration 5\n",
      "Policy loss: -0.063034. Value loss: 1.299945. Entropy: 1.182437.\n",
      "episode: 799   score: 115.0   memory length: 10240   epsilon: 1.0    steps: 872     evaluation reward: 142.7\n",
      "episode: 800   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 391     evaluation reward: 141.65\n",
      "episode: 801   score: 285.0   memory length: 10240   epsilon: 1.0    steps: 968     evaluation reward: 142.95\n",
      "episode: 802   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 482     evaluation reward: 142.5\n",
      "episode: 803   score: 380.0   memory length: 10240   epsilon: 1.0    steps: 1196     evaluation reward: 145.6\n",
      "episode: 804   score: 545.0   memory length: 10240   epsilon: 1.0    steps: 1405     evaluation reward: 149.25\n",
      "episode: 805   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 785     evaluation reward: 150.0\n",
      "episode: 806   score: 380.0   memory length: 10240   epsilon: 1.0    steps: 979     evaluation reward: 153.0\n",
      "episode: 807   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 794     evaluation reward: 150.8\n",
      "episode: 808   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 517     evaluation reward: 150.75\n",
      "episode: 809   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 550     evaluation reward: 149.9\n",
      "episode: 810   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 440     evaluation reward: 150.0\n",
      "episode: 811   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 757     evaluation reward: 149.65\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.042082. Value loss: 5.279447. Entropy: 1.209547.\n",
      "Iteration 2\n",
      "Policy loss: -0.050994. Value loss: 2.849656. Entropy: 1.193992.\n",
      "Iteration 3\n",
      "Policy loss: -0.058158. Value loss: 2.120430. Entropy: 1.203870.\n",
      "Iteration 4\n",
      "Policy loss: -0.063780. Value loss: 1.786914. Entropy: 1.195192.\n",
      "Iteration 5\n",
      "Policy loss: -0.061749. Value loss: 1.616999. Entropy: 1.194261.\n",
      "episode: 812   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 749     evaluation reward: 149.4\n",
      "episode: 813   score: 270.0   memory length: 10240   epsilon: 1.0    steps: 977     evaluation reward: 151.05\n",
      "episode: 814   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 401     evaluation reward: 150.85\n",
      "episode: 815   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 490     evaluation reward: 150.65\n",
      "episode: 816   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 611     evaluation reward: 150.4\n",
      "episode: 817   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 657     evaluation reward: 150.3\n",
      "episode: 818   score: 515.0   memory length: 10240   epsilon: 1.0    steps: 1065     evaluation reward: 154.4\n",
      "episode: 819   score: 165.0   memory length: 10240   epsilon: 1.0    steps: 817     evaluation reward: 154.9\n",
      "episode: 820   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 543     evaluation reward: 154.15\n",
      "episode: 821   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 578     evaluation reward: 153.75\n",
      "episode: 822   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 402     evaluation reward: 152.25\n",
      "episode: 823   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 484     evaluation reward: 152.1\n",
      "episode: 824   score: 310.0   memory length: 10240   epsilon: 1.0    steps: 1092     evaluation reward: 154.15\n",
      "episode: 825   score: 255.0   memory length: 10240   epsilon: 1.0    steps: 890     evaluation reward: 151.55\n",
      "episode: 826   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 903     evaluation reward: 151.2\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.040198. Value loss: 5.822309. Entropy: 1.169054.\n",
      "Iteration 2\n",
      "Policy loss: -0.055048. Value loss: 2.819407. Entropy: 1.163329.\n",
      "Iteration 3\n",
      "Policy loss: -0.059935. Value loss: 2.102392. Entropy: 1.168357.\n",
      "Iteration 4\n",
      "Policy loss: -0.064284. Value loss: 1.884685. Entropy: 1.166995.\n",
      "Iteration 5\n",
      "Policy loss: -0.060998. Value loss: 1.717109. Entropy: 1.161828.\n",
      "episode: 827   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 644     evaluation reward: 151.2\n",
      "episode: 828   score: 145.0   memory length: 10240   epsilon: 1.0    steps: 929     evaluation reward: 151.45\n",
      "episode: 829   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 592     evaluation reward: 150.4\n",
      "episode: 830   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 810     evaluation reward: 149.85\n",
      "episode: 831   score: 100.0   memory length: 10240   epsilon: 1.0    steps: 522     evaluation reward: 149.65\n",
      "episode: 832   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 646     evaluation reward: 148.75\n",
      "episode: 833   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 883     evaluation reward: 149.5\n",
      "episode: 834   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 383     evaluation reward: 149.55\n",
      "episode: 835   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 564     evaluation reward: 149.9\n",
      "episode: 836   score: 345.0   memory length: 10240   epsilon: 1.0    steps: 986     evaluation reward: 151.55\n",
      "episode: 837   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 349     evaluation reward: 151.65\n",
      "episode: 838   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 679     evaluation reward: 152.25\n",
      "episode: 839   score: 460.0   memory length: 10240   epsilon: 1.0    steps: 950     evaluation reward: 155.65\n",
      "episode: 840   score: 130.0   memory length: 10240   epsilon: 1.0    steps: 693     evaluation reward: 155.9\n",
      "episode: 841   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 640     evaluation reward: 154.7\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.041829. Value loss: 5.936662. Entropy: 1.155042.\n",
      "Iteration 2\n",
      "Policy loss: -0.053657. Value loss: 3.254156. Entropy: 1.157051.\n",
      "Iteration 3\n",
      "Policy loss: -0.054132. Value loss: 2.438155. Entropy: 1.157945.\n",
      "Iteration 4\n",
      "Policy loss: -0.063565. Value loss: 2.003419. Entropy: 1.154611.\n",
      "Iteration 5\n",
      "Policy loss: -0.065070. Value loss: 1.858855. Entropy: 1.156080.\n",
      "episode: 842   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 706     evaluation reward: 153.1\n",
      "episode: 843   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 467     evaluation reward: 152.05\n",
      "episode: 844   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 685     evaluation reward: 151.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 845   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 659     evaluation reward: 151.4\n",
      "episode: 846   score: 140.0   memory length: 10240   epsilon: 1.0    steps: 557     evaluation reward: 152.0\n",
      "episode: 847   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 708     evaluation reward: 151.45\n",
      "episode: 848   score: 515.0   memory length: 10240   epsilon: 1.0    steps: 1307     evaluation reward: 155.55\n",
      "episode: 849   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 590     evaluation reward: 155.5\n",
      "episode: 850   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 523     evaluation reward: 155.5\n",
      "episode: 851   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 662     evaluation reward: 154.6\n",
      "episode: 852   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 622     evaluation reward: 155.25\n",
      "episode: 853   score: 115.0   memory length: 10240   epsilon: 1.0    steps: 644     evaluation reward: 154.9\n",
      "episode: 854   score: 200.0   memory length: 10240   epsilon: 1.0    steps: 883     evaluation reward: 156.25\n",
      "episode: 855   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 463     evaluation reward: 156.0\n",
      "episode: 856   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 417     evaluation reward: 155.85\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.033102. Value loss: 5.920628. Entropy: 1.178211.\n",
      "Iteration 2\n",
      "Policy loss: -0.046219. Value loss: 3.048255. Entropy: 1.185858.\n",
      "Iteration 3\n",
      "Policy loss: -0.056778. Value loss: 2.233344. Entropy: 1.177545.\n",
      "Iteration 4\n",
      "Policy loss: -0.054480. Value loss: 1.937754. Entropy: 1.170148.\n",
      "Iteration 5\n",
      "Policy loss: -0.060144. Value loss: 1.710424. Entropy: 1.174376.\n",
      "episode: 857   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 643     evaluation reward: 156.45\n",
      "episode: 858   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 643     evaluation reward: 156.6\n",
      "episode: 859   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 624     evaluation reward: 157.75\n",
      "episode: 860   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 627     evaluation reward: 157.9\n",
      "episode: 861   score: 95.0   memory length: 10240   epsilon: 1.0    steps: 364     evaluation reward: 157.8\n",
      "episode: 862   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 657     evaluation reward: 157.2\n",
      "episode: 863   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 765     evaluation reward: 157.0\n",
      "episode: 864   score: 165.0   memory length: 10240   epsilon: 1.0    steps: 761     evaluation reward: 156.85\n",
      "episode: 865   score: 215.0   memory length: 10240   epsilon: 1.0    steps: 806     evaluation reward: 157.45\n",
      "episode: 866   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 379     evaluation reward: 155.8\n",
      "episode: 867   score: 85.0   memory length: 10240   epsilon: 1.0    steps: 408     evaluation reward: 155.05\n",
      "now time :  2018-12-18 17:39:01.582612\n",
      "episode: 868   score: 260.0   memory length: 10240   epsilon: 1.0    steps: 1036     evaluation reward: 157.0\n",
      "episode: 869   score: 115.0   memory length: 10240   epsilon: 1.0    steps: 670     evaluation reward: 157.5\n",
      "episode: 870   score: 195.0   memory length: 10240   epsilon: 1.0    steps: 864     evaluation reward: 158.4\n",
      "episode: 871   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 727     evaluation reward: 157.85\n",
      "episode: 872   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 592     evaluation reward: 158.7\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.049337. Value loss: 4.286757. Entropy: 1.156025.\n",
      "Iteration 2\n",
      "Policy loss: -0.053264. Value loss: 2.285748. Entropy: 1.147507.\n",
      "Iteration 3\n",
      "Policy loss: -0.060010. Value loss: 1.755721. Entropy: 1.148114.\n",
      "Iteration 4\n",
      "Policy loss: -0.063894. Value loss: 1.533387. Entropy: 1.161244.\n",
      "Iteration 5\n",
      "Policy loss: -0.068279. Value loss: 1.333859. Entropy: 1.148870.\n",
      "episode: 873   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 588     evaluation reward: 157.9\n",
      "episode: 874   score: 215.0   memory length: 10240   epsilon: 1.0    steps: 1211     evaluation reward: 159.0\n",
      "episode: 875   score: 305.0   memory length: 10240   epsilon: 1.0    steps: 705     evaluation reward: 160.8\n",
      "episode: 876   score: 140.0   memory length: 10240   epsilon: 1.0    steps: 615     evaluation reward: 159.05\n",
      "episode: 877   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 496     evaluation reward: 159.25\n",
      "episode: 878   score: 175.0   memory length: 10240   epsilon: 1.0    steps: 533     evaluation reward: 159.1\n",
      "episode: 879   score: 170.0   memory length: 10240   epsilon: 1.0    steps: 826     evaluation reward: 159.75\n",
      "episode: 880   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 669     evaluation reward: 159.4\n",
      "episode: 881   score: 215.0   memory length: 10240   epsilon: 1.0    steps: 849     evaluation reward: 161.05\n",
      "episode: 882   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 475     evaluation reward: 160.65\n",
      "episode: 883   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 497     evaluation reward: 156.5\n",
      "episode: 884   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 483     evaluation reward: 157.05\n",
      "episode: 885   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 634     evaluation reward: 156.3\n",
      "episode: 886   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 614     evaluation reward: 155.0\n",
      "episode: 887   score: 440.0   memory length: 10240   epsilon: 1.0    steps: 945     evaluation reward: 157.85\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.041785. Value loss: 6.646914. Entropy: 1.160632.\n",
      "Iteration 2\n",
      "Policy loss: -0.051328. Value loss: 3.314517. Entropy: 1.165411.\n",
      "Iteration 3\n",
      "Policy loss: -0.050887. Value loss: 2.369491. Entropy: 1.171091.\n",
      "Iteration 4\n",
      "Policy loss: -0.059908. Value loss: 2.002637. Entropy: 1.162508.\n",
      "Iteration 5\n",
      "Policy loss: -0.061236. Value loss: 1.794379. Entropy: 1.175118.\n",
      "episode: 888   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 505     evaluation reward: 155.35\n",
      "episode: 889   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 617     evaluation reward: 154.6\n",
      "episode: 890   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 651     evaluation reward: 155.3\n",
      "episode: 891   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 549     evaluation reward: 154.6\n",
      "episode: 892   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 528     evaluation reward: 154.5\n",
      "episode: 893   score: 245.0   memory length: 10240   epsilon: 1.0    steps: 1024     evaluation reward: 154.85\n",
      "episode: 894   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 769     evaluation reward: 154.65\n",
      "episode: 895   score: 380.0   memory length: 10240   epsilon: 1.0    steps: 787     evaluation reward: 157.35\n",
      "episode: 896   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 502     evaluation reward: 157.6\n",
      "episode: 897   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 737     evaluation reward: 158.2\n",
      "episode: 898   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 598     evaluation reward: 158.15\n",
      "episode: 899   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 685     evaluation reward: 158.55\n",
      "episode: 900   score: 165.0   memory length: 10240   epsilon: 1.0    steps: 972     evaluation reward: 159.65\n",
      "episode: 901   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 617     evaluation reward: 158.35\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.033544. Value loss: 5.289786. Entropy: 1.184939.\n",
      "Iteration 2\n",
      "Policy loss: -0.048194. Value loss: 2.840230. Entropy: 1.171158.\n",
      "Iteration 3\n",
      "Policy loss: -0.052778. Value loss: 2.136916. Entropy: 1.164931.\n",
      "Iteration 4\n",
      "Policy loss: -0.054999. Value loss: 1.919458. Entropy: 1.177564.\n",
      "Iteration 5\n",
      "Policy loss: -0.059395. Value loss: 1.725678. Entropy: 1.163691.\n",
      "episode: 902   score: 260.0   memory length: 10240   epsilon: 1.0    steps: 1155     evaluation reward: 160.05\n",
      "episode: 903   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 512     evaluation reward: 157.0\n",
      "episode: 904   score: 15.0   memory length: 10240   epsilon: 1.0    steps: 530     evaluation reward: 151.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 905   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 401     evaluation reward: 150.2\n",
      "episode: 906   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 487     evaluation reward: 146.7\n",
      "episode: 907   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 384     evaluation reward: 146.0\n",
      "episode: 908   score: 170.0   memory length: 10240   epsilon: 1.0    steps: 681     evaluation reward: 146.65\n",
      "episode: 909   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 594     evaluation reward: 147.35\n",
      "episode: 910   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 567     evaluation reward: 147.6\n",
      "episode: 911   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 614     evaluation reward: 147.45\n",
      "episode: 912   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 650     evaluation reward: 147.1\n",
      "episode: 913   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 748     evaluation reward: 145.95\n",
      "episode: 914   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 513     evaluation reward: 146.1\n",
      "episode: 915   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 596     evaluation reward: 147.0\n",
      "episode: 916   score: 115.0   memory length: 10240   epsilon: 1.0    steps: 458     evaluation reward: 146.8\n",
      "episode: 917   score: 235.0   memory length: 10240   epsilon: 1.0    steps: 937     evaluation reward: 148.35\n",
      "episode: 918   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 636     evaluation reward: 144.55\n",
      "episode: 919   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 383     evaluation reward: 144.0\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.041381. Value loss: 4.562483. Entropy: 1.140740.\n",
      "Iteration 2\n",
      "Policy loss: -0.053491. Value loss: 2.567000. Entropy: 1.140821.\n",
      "Iteration 3\n",
      "Policy loss: -0.059819. Value loss: 2.043042. Entropy: 1.144105.\n",
      "Iteration 4\n",
      "Policy loss: -0.064165. Value loss: 1.743336. Entropy: 1.147903.\n",
      "Iteration 5\n",
      "Policy loss: -0.065759. Value loss: 1.716610. Entropy: 1.140017.\n",
      "episode: 920   score: 315.0   memory length: 10240   epsilon: 1.0    steps: 922     evaluation reward: 146.1\n",
      "episode: 921   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 626     evaluation reward: 146.15\n",
      "episode: 922   score: 380.0   memory length: 10240   epsilon: 1.0    steps: 1208     evaluation reward: 149.6\n",
      "episode: 923   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 811     evaluation reward: 150.35\n",
      "episode: 924   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 694     evaluation reward: 149.05\n",
      "episode: 925   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 697     evaluation reward: 147.4\n",
      "episode: 926   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 810     evaluation reward: 147.1\n",
      "episode: 927   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 612     evaluation reward: 146.85\n",
      "episode: 928   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 576     evaluation reward: 146.05\n",
      "episode: 929   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 787     evaluation reward: 147.65\n",
      "episode: 930   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 666     evaluation reward: 147.65\n",
      "episode: 931   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 400     evaluation reward: 147.25\n",
      "episode: 932   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 760     evaluation reward: 146.5\n",
      "episode: 933   score: 335.0   memory length: 10240   epsilon: 1.0    steps: 1058     evaluation reward: 147.75\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.043222. Value loss: 5.797835. Entropy: 1.100728.\n",
      "Iteration 2\n",
      "Policy loss: -0.055227. Value loss: 2.810525. Entropy: 1.092370.\n",
      "Iteration 3\n",
      "Policy loss: -0.060930. Value loss: 2.149790. Entropy: 1.101311.\n",
      "Iteration 4\n",
      "Policy loss: -0.065790. Value loss: 1.884741. Entropy: 1.106682.\n",
      "Iteration 5\n",
      "Policy loss: -0.068703. Value loss: 1.770438. Entropy: 1.089220.\n",
      "episode: 934   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 507     evaluation reward: 147.5\n",
      "episode: 935   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 468     evaluation reward: 146.95\n",
      "episode: 936   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 744     evaluation reward: 144.85\n",
      "episode: 937   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 492     evaluation reward: 144.85\n",
      "episode: 938   score: 430.0   memory length: 10240   epsilon: 1.0    steps: 951     evaluation reward: 147.35\n",
      "episode: 939   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 450     evaluation reward: 143.95\n",
      "now time :  2018-12-18 17:45:31.118332\n",
      "episode: 940   score: 415.0   memory length: 10240   epsilon: 1.0    steps: 1379     evaluation reward: 146.8\n",
      "episode: 941   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 718     evaluation reward: 146.3\n",
      "episode: 942   score: 170.0   memory length: 10240   epsilon: 1.0    steps: 843     evaluation reward: 146.75\n",
      "episode: 943   score: 315.0   memory length: 10240   epsilon: 1.0    steps: 781     evaluation reward: 148.85\n",
      "episode: 944   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 591     evaluation reward: 149.6\n",
      "episode: 945   score: 250.0   memory length: 10240   epsilon: 1.0    steps: 963     evaluation reward: 150.85\n",
      "episode: 946   score: 40.0   memory length: 10240   epsilon: 1.0    steps: 537     evaluation reward: 149.85\n",
      "episode: 947   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 854     evaluation reward: 150.7\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.040968. Value loss: 5.523702. Entropy: 0.983378.\n",
      "Iteration 2\n",
      "Policy loss: -0.052983. Value loss: 2.704992. Entropy: 0.977984.\n",
      "Iteration 3\n",
      "Policy loss: -0.057608. Value loss: 2.161244. Entropy: 0.972244.\n",
      "Iteration 4\n",
      "Policy loss: -0.060768. Value loss: 1.792393. Entropy: 0.980600.\n",
      "Iteration 5\n",
      "Policy loss: -0.063510. Value loss: 1.572845. Entropy: 0.973332.\n",
      "episode: 948   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 886     evaluation reward: 147.4\n",
      "episode: 949   score: 415.0   memory length: 10240   epsilon: 1.0    steps: 1367     evaluation reward: 150.5\n",
      "episode: 950   score: 570.0   memory length: 10240   epsilon: 1.0    steps: 1685     evaluation reward: 154.95\n",
      "episode: 951   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 591     evaluation reward: 154.95\n",
      "episode: 952   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 1087     evaluation reward: 154.95\n",
      "episode: 953   score: 260.0   memory length: 10240   epsilon: 1.0    steps: 983     evaluation reward: 156.4\n",
      "episode: 954   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 639     evaluation reward: 155.65\n",
      "episode: 955   score: 215.0   memory length: 10240   epsilon: 1.0    steps: 832     evaluation reward: 157.05\n",
      "episode: 956   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 752     evaluation reward: 157.55\n",
      "episode: 957   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 540     evaluation reward: 157.45\n",
      "episode: 958   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 515     evaluation reward: 157.45\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.039701. Value loss: 6.610147. Entropy: 0.939923.\n",
      "Iteration 2\n",
      "Policy loss: -0.049088. Value loss: 3.184616. Entropy: 0.934607.\n",
      "Iteration 3\n",
      "Policy loss: -0.052111. Value loss: 2.290966. Entropy: 0.930590.\n",
      "Iteration 4\n",
      "Policy loss: -0.055262. Value loss: 1.972715. Entropy: 0.921057.\n",
      "Iteration 5\n",
      "Policy loss: -0.058385. Value loss: 1.678298. Entropy: 0.928094.\n",
      "episode: 959   score: 280.0   memory length: 10240   epsilon: 1.0    steps: 830     evaluation reward: 158.45\n",
      "episode: 960   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 653     evaluation reward: 158.8\n",
      "episode: 961   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 740     evaluation reward: 158.95\n",
      "episode: 962   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 493     evaluation reward: 159.25\n",
      "episode: 963   score: 225.0   memory length: 10240   epsilon: 1.0    steps: 1085     evaluation reward: 160.15\n",
      "episode: 964   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 797     evaluation reward: 160.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 965   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 724     evaluation reward: 159.95\n",
      "episode: 966   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 672     evaluation reward: 160.35\n",
      "episode: 967   score: 260.0   memory length: 10240   epsilon: 1.0    steps: 922     evaluation reward: 162.1\n",
      "episode: 968   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 632     evaluation reward: 160.75\n",
      "episode: 969   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 406     evaluation reward: 160.2\n",
      "episode: 970   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 725     evaluation reward: 158.6\n",
      "episode: 971   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 594     evaluation reward: 159.45\n",
      "episode: 972   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 604     evaluation reward: 159.3\n",
      "episode: 973   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 408     evaluation reward: 159.55\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.043572. Value loss: 4.446709. Entropy: 0.954936.\n",
      "Iteration 2\n",
      "Policy loss: -0.060610. Value loss: 2.347246. Entropy: 0.966100.\n",
      "Iteration 3\n",
      "Policy loss: -0.068839. Value loss: 1.901002. Entropy: 0.969783.\n",
      "Iteration 4\n",
      "Policy loss: -0.067587. Value loss: 1.648027. Entropy: 0.977139.\n",
      "Iteration 5\n",
      "Policy loss: -0.071740. Value loss: 1.546982. Entropy: 0.964213.\n",
      "episode: 974   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 815     evaluation reward: 159.5\n",
      "episode: 975   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 651     evaluation reward: 158.25\n",
      "episode: 976   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 826     evaluation reward: 158.05\n",
      "episode: 977   score: 170.0   memory length: 10240   epsilon: 1.0    steps: 831     evaluation reward: 158.55\n",
      "episode: 978   score: 345.0   memory length: 10240   epsilon: 1.0    steps: 1079     evaluation reward: 160.25\n",
      "episode: 979   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 733     evaluation reward: 159.45\n",
      "episode: 980   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 383     evaluation reward: 159.0\n",
      "episode: 981   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 624     evaluation reward: 158.2\n",
      "episode: 982   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 566     evaluation reward: 157.8\n",
      "episode: 983   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 708     evaluation reward: 157.8\n",
      "episode: 984   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 640     evaluation reward: 157.7\n",
      "episode: 985   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 883     evaluation reward: 158.15\n",
      "episode: 986   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 840     evaluation reward: 158.9\n",
      "episode: 987   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 835     evaluation reward: 156.35\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.038142. Value loss: 4.521569. Entropy: 1.014562.\n",
      "Iteration 2\n",
      "Policy loss: -0.058940. Value loss: 2.393815. Entropy: 1.005422.\n",
      "Iteration 3\n",
      "Policy loss: -0.066027. Value loss: 1.784752. Entropy: 1.010073.\n",
      "Iteration 4\n",
      "Policy loss: -0.070285. Value loss: 1.526034. Entropy: 1.003726.\n",
      "Iteration 5\n",
      "Policy loss: -0.067969. Value loss: 1.424087. Entropy: 0.997446.\n",
      "episode: 988   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 624     evaluation reward: 157.15\n",
      "episode: 989   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 489     evaluation reward: 156.15\n",
      "episode: 990   score: 260.0   memory length: 10240   epsilon: 1.0    steps: 808     evaluation reward: 157.55\n",
      "episode: 991   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 525     evaluation reward: 158.25\n",
      "episode: 992   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 783     evaluation reward: 158.55\n",
      "episode: 993   score: 230.0   memory length: 10240   epsilon: 1.0    steps: 668     evaluation reward: 158.4\n",
      "episode: 994   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 738     evaluation reward: 158.25\n",
      "episode: 995   score: 100.0   memory length: 10240   epsilon: 1.0    steps: 599     evaluation reward: 155.45\n",
      "episode: 996   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 792     evaluation reward: 156.4\n",
      "episode: 997   score: 420.0   memory length: 10240   epsilon: 1.0    steps: 1114     evaluation reward: 159.35\n",
      "episode: 998   score: 20.0   memory length: 10240   epsilon: 1.0    steps: 450     evaluation reward: 158.5\n",
      "episode: 999   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 715     evaluation reward: 158.5\n",
      "episode: 1000   score: 85.0   memory length: 10240   epsilon: 1.0    steps: 556     evaluation reward: 157.7\n",
      "episode: 1001   score: 160.0   memory length: 10240   epsilon: 1.0    steps: 807     evaluation reward: 157.75\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.047534. Value loss: 5.486925. Entropy: 0.939050.\n",
      "Iteration 2\n",
      "Policy loss: -0.061076. Value loss: 2.705074. Entropy: 0.930373.\n",
      "Iteration 3\n",
      "Policy loss: -0.068266. Value loss: 2.020689. Entropy: 0.923133.\n",
      "Iteration 4\n",
      "Policy loss: -0.071501. Value loss: 1.752361. Entropy: 0.927726.\n",
      "Iteration 5\n",
      "Policy loss: -0.073503. Value loss: 1.616281. Entropy: 0.925279.\n",
      "episode: 1002   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 792     evaluation reward: 157.25\n",
      "episode: 1003   score: 260.0   memory length: 10240   epsilon: 1.0    steps: 862     evaluation reward: 159.1\n",
      "episode: 1004   score: 230.0   memory length: 10240   epsilon: 1.0    steps: 931     evaluation reward: 161.25\n",
      "episode: 1005   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 778     evaluation reward: 163.05\n",
      "episode: 1006   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 586     evaluation reward: 163.55\n",
      "now time :  2018-12-18 17:51:59.894438\n",
      "episode: 1007   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 698     evaluation reward: 163.65\n",
      "episode: 1008   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 496     evaluation reward: 163.05\n",
      "episode: 1009   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 838     evaluation reward: 163.8\n",
      "episode: 1010   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 793     evaluation reward: 164.25\n",
      "episode: 1011   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 803     evaluation reward: 165.3\n",
      "episode: 1012   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 401     evaluation reward: 164.45\n",
      "episode: 1013   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 421     evaluation reward: 163.45\n",
      "episode: 1014   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 592     evaluation reward: 164.5\n",
      "episode: 1015   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 759     evaluation reward: 163.6\n",
      "episode: 1016   score: 20.0   memory length: 10240   epsilon: 1.0    steps: 517     evaluation reward: 162.65\n",
      "episode: 1017   score: 20.0   memory length: 10240   epsilon: 1.0    steps: 389     evaluation reward: 160.5\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.043305. Value loss: 4.672362. Entropy: 0.900084.\n",
      "Iteration 2\n",
      "Policy loss: -0.063502. Value loss: 2.693857. Entropy: 0.903228.\n",
      "Iteration 3\n",
      "Policy loss: -0.066098. Value loss: 2.141230. Entropy: 0.892303.\n",
      "Iteration 4\n",
      "Policy loss: -0.067626. Value loss: 1.767604. Entropy: 0.892254.\n",
      "Iteration 5\n",
      "Policy loss: -0.069463. Value loss: 1.700230. Entropy: 0.895238.\n",
      "episode: 1018   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 721     evaluation reward: 161.25\n",
      "episode: 1019   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 408     evaluation reward: 161.05\n",
      "episode: 1020   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 869     evaluation reward: 160.0\n",
      "episode: 1021   score: 100.0   memory length: 10240   epsilon: 1.0    steps: 635     evaluation reward: 160.45\n",
      "episode: 1022   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 535     evaluation reward: 157.45\n",
      "episode: 1023   score: 140.0   memory length: 10240   epsilon: 1.0    steps: 766     evaluation reward: 157.05\n",
      "episode: 1024   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 571     evaluation reward: 155.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1025   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 508     evaluation reward: 156.15\n",
      "episode: 1026   score: 95.0   memory length: 10240   epsilon: 1.0    steps: 602     evaluation reward: 155.55\n",
      "episode: 1027   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 646     evaluation reward: 155.7\n",
      "episode: 1028   score: 140.0   memory length: 10240   epsilon: 1.0    steps: 675     evaluation reward: 156.45\n",
      "episode: 1029   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 507     evaluation reward: 155.0\n",
      "episode: 1030   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 530     evaluation reward: 154.55\n",
      "episode: 1031   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 651     evaluation reward: 155.2\n",
      "episode: 1032   score: 290.0   memory length: 10240   epsilon: 1.0    steps: 964     evaluation reward: 156.75\n",
      "episode: 1033   score: 345.0   memory length: 10240   epsilon: 1.0    steps: 783     evaluation reward: 156.85\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.043984. Value loss: 4.667171. Entropy: 0.995930.\n",
      "Iteration 2\n",
      "Policy loss: -0.058001. Value loss: 2.473742. Entropy: 0.992628.\n",
      "Iteration 3\n",
      "Policy loss: -0.061878. Value loss: 2.068758. Entropy: 0.990998.\n",
      "Iteration 4\n",
      "Policy loss: -0.064022. Value loss: 1.811508. Entropy: 0.991556.\n",
      "Iteration 5\n",
      "Policy loss: -0.069666. Value loss: 1.568818. Entropy: 0.993772.\n",
      "episode: 1034   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 645     evaluation reward: 158.1\n",
      "episode: 1035   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 643     evaluation reward: 158.85\n",
      "episode: 1036   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 507     evaluation reward: 158.15\n",
      "episode: 1037   score: 240.0   memory length: 10240   epsilon: 1.0    steps: 878     evaluation reward: 160.0\n",
      "episode: 1038   score: 100.0   memory length: 10240   epsilon: 1.0    steps: 581     evaluation reward: 156.7\n",
      "episode: 1039   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 787     evaluation reward: 157.05\n",
      "episode: 1040   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 658     evaluation reward: 153.55\n",
      "episode: 1041   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 488     evaluation reward: 152.8\n",
      "episode: 1042   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 695     evaluation reward: 152.9\n",
      "episode: 1043   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 589     evaluation reward: 151.1\n",
      "episode: 1044   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 581     evaluation reward: 150.4\n",
      "episode: 1045   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 650     evaluation reward: 149.7\n",
      "episode: 1046   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 550     evaluation reward: 150.05\n",
      "episode: 1047   score: 140.0   memory length: 10240   epsilon: 1.0    steps: 696     evaluation reward: 149.35\n",
      "episode: 1048   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 573     evaluation reward: 147.8\n",
      "episode: 1049   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 768     evaluation reward: 145.0\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.038058. Value loss: 4.289557. Entropy: 0.965364.\n",
      "Iteration 2\n",
      "Policy loss: -0.058662. Value loss: 2.463452. Entropy: 0.968324.\n",
      "Iteration 3\n",
      "Policy loss: -0.057108. Value loss: 1.922222. Entropy: 0.973474.\n",
      "Iteration 4\n",
      "Policy loss: -0.059923. Value loss: 1.745892. Entropy: 0.967835.\n",
      "Iteration 5\n",
      "Policy loss: -0.064940. Value loss: 1.596146. Entropy: 0.976080.\n",
      "episode: 1050   score: 85.0   memory length: 10240   epsilon: 1.0    steps: 563     evaluation reward: 140.15\n",
      "episode: 1051   score: 140.0   memory length: 10240   epsilon: 1.0    steps: 638     evaluation reward: 140.35\n",
      "episode: 1052   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 693     evaluation reward: 141.1\n",
      "episode: 1053   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 811     evaluation reward: 140.6\n",
      "episode: 1054   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 673     evaluation reward: 140.7\n",
      "episode: 1055   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 615     evaluation reward: 139.9\n",
      "episode: 1056   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 748     evaluation reward: 140.9\n",
      "episode: 1057   score: 230.0   memory length: 10240   epsilon: 1.0    steps: 995     evaluation reward: 142.4\n",
      "episode: 1058   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 696     evaluation reward: 142.3\n",
      "episode: 1059   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 507     evaluation reward: 140.3\n",
      "episode: 1060   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 650     evaluation reward: 139.8\n",
      "episode: 1061   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 657     evaluation reward: 139.9\n",
      "episode: 1062   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 952     evaluation reward: 140.4\n",
      "episode: 1063   score: 145.0   memory length: 10240   epsilon: 1.0    steps: 634     evaluation reward: 139.6\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.038971. Value loss: 4.329071. Entropy: 0.875026.\n",
      "Iteration 2\n",
      "Policy loss: -0.055500. Value loss: 2.487942. Entropy: 0.877647.\n",
      "Iteration 3\n",
      "Policy loss: -0.061287. Value loss: 1.962971. Entropy: 0.874497.\n",
      "Iteration 4\n",
      "Policy loss: -0.066579. Value loss: 1.645657. Entropy: 0.877591.\n",
      "Iteration 5\n",
      "Policy loss: -0.069169. Value loss: 1.451396. Entropy: 0.879255.\n",
      "episode: 1064   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 639     evaluation reward: 138.6\n",
      "episode: 1065   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 646     evaluation reward: 138.65\n",
      "episode: 1066   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 676     evaluation reward: 138.95\n",
      "episode: 1067   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 399     evaluation reward: 136.8\n",
      "episode: 1068   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 635     evaluation reward: 136.65\n",
      "episode: 1069   score: 415.0   memory length: 10240   epsilon: 1.0    steps: 1254     evaluation reward: 140.2\n",
      "episode: 1070   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 580     evaluation reward: 141.05\n",
      "episode: 1071   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 716     evaluation reward: 141.5\n",
      "episode: 1072   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 551     evaluation reward: 141.5\n",
      "episode: 1073   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 517     evaluation reward: 141.3\n",
      "episode: 1074   score: 160.0   memory length: 10240   epsilon: 1.0    steps: 518     evaluation reward: 140.8\n",
      "episode: 1075   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 464     evaluation reward: 139.35\n",
      "episode: 1076   score: 325.0   memory length: 10240   epsilon: 1.0    steps: 939     evaluation reward: 141.4\n",
      "episode: 1077   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 394     evaluation reward: 140.2\n",
      "episode: 1078   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 615     evaluation reward: 138.3\n",
      "episode: 1079   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 504     evaluation reward: 138.2\n",
      "episode: 1080   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 693     evaluation reward: 138.95\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.047610. Value loss: 4.983078. Entropy: 0.894087.\n",
      "Iteration 2\n",
      "Policy loss: -0.056693. Value loss: 2.617778. Entropy: 0.893506.\n",
      "Iteration 3\n",
      "Policy loss: -0.062844. Value loss: 1.912968. Entropy: 0.893772.\n",
      "Iteration 4\n",
      "Policy loss: -0.064436. Value loss: 1.679695. Entropy: 0.888093.\n",
      "Iteration 5\n",
      "Policy loss: -0.071231. Value loss: 1.520172. Entropy: 0.893119.\n",
      "episode: 1081   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 645     evaluation reward: 138.85\n",
      "episode: 1082   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 663     evaluation reward: 139.7\n",
      "episode: 1083   score: 230.0   memory length: 10240   epsilon: 1.0    steps: 727     evaluation reward: 140.8\n",
      "episode: 1084   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 445     evaluation reward: 140.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now time :  2018-12-18 17:58:28.815095\n",
      "episode: 1085   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 510     evaluation reward: 139.15\n",
      "episode: 1086   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 403     evaluation reward: 138.8\n",
      "episode: 1087   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 762     evaluation reward: 138.3\n",
      "episode: 1088   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 635     evaluation reward: 137.95\n",
      "episode: 1089   score: 520.0   memory length: 10240   epsilon: 1.0    steps: 1074     evaluation reward: 142.8\n",
      "episode: 1090   score: 250.0   memory length: 10240   epsilon: 1.0    steps: 971     evaluation reward: 142.7\n",
      "episode: 1091   score: 100.0   memory length: 10240   epsilon: 1.0    steps: 384     evaluation reward: 142.65\n",
      "episode: 1092   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 408     evaluation reward: 142.15\n",
      "episode: 1093   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 684     evaluation reward: 141.4\n",
      "episode: 1094   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 637     evaluation reward: 141.7\n",
      "episode: 1095   score: 290.0   memory length: 10240   epsilon: 1.0    steps: 912     evaluation reward: 143.6\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.040025. Value loss: 5.865180. Entropy: 0.930402.\n",
      "Iteration 2\n",
      "Policy loss: -0.060912. Value loss: 2.839081. Entropy: 0.928817.\n",
      "Iteration 3\n",
      "Policy loss: -0.061263. Value loss: 2.176871. Entropy: 0.942172.\n",
      "Iteration 4\n",
      "Policy loss: -0.066831. Value loss: 1.957878. Entropy: 0.941652.\n",
      "Iteration 5\n",
      "Policy loss: -0.068765. Value loss: 1.675277. Entropy: 0.943298.\n",
      "episode: 1096   score: 195.0   memory length: 10240   epsilon: 1.0    steps: 872     evaluation reward: 143.7\n",
      "episode: 1097   score: 390.0   memory length: 10240   epsilon: 1.0    steps: 1066     evaluation reward: 143.4\n",
      "episode: 1098   score: 450.0   memory length: 10240   epsilon: 1.0    steps: 1285     evaluation reward: 147.7\n",
      "episode: 1099   score: 215.0   memory length: 10240   epsilon: 1.0    steps: 800     evaluation reward: 148.3\n",
      "episode: 1100   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 749     evaluation reward: 148.65\n",
      "episode: 1101   score: 145.0   memory length: 10240   epsilon: 1.0    steps: 906     evaluation reward: 148.5\n",
      "episode: 1102   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 756     evaluation reward: 147.95\n",
      "episode: 1103   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 622     evaluation reward: 146.4\n",
      "episode: 1104   score: 115.0   memory length: 10240   epsilon: 1.0    steps: 632     evaluation reward: 145.25\n",
      "episode: 1105   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 552     evaluation reward: 144.65\n",
      "episode: 1106   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 467     evaluation reward: 144.65\n",
      "episode: 1107   score: 245.0   memory length: 10240   epsilon: 1.0    steps: 841     evaluation reward: 146.2\n",
      "episode: 1108   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 901     evaluation reward: 147.2\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.052636. Value loss: 5.977248. Entropy: 0.830258.\n",
      "Iteration 2\n",
      "Policy loss: -0.069249. Value loss: 3.073839. Entropy: 0.806944.\n",
      "Iteration 3\n",
      "Policy loss: -0.073903. Value loss: 2.527921. Entropy: 0.812319.\n",
      "Iteration 4\n",
      "Policy loss: -0.076731. Value loss: 2.247215. Entropy: 0.811828.\n",
      "Iteration 5\n",
      "Policy loss: -0.077389. Value loss: 2.068726. Entropy: 0.819311.\n",
      "episode: 1109   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 495     evaluation reward: 145.95\n",
      "episode: 1110   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 557     evaluation reward: 145.65\n",
      "episode: 1111   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 730     evaluation reward: 144.9\n",
      "episode: 1112   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 596     evaluation reward: 145.3\n",
      "episode: 1113   score: 515.0   memory length: 10240   epsilon: 1.0    steps: 1065     evaluation reward: 149.9\n",
      "episode: 1114   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 680     evaluation reward: 149.15\n",
      "episode: 1115   score: 430.0   memory length: 10240   epsilon: 1.0    steps: 1181     evaluation reward: 152.25\n",
      "episode: 1116   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 396     evaluation reward: 153.1\n",
      "episode: 1117   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 505     evaluation reward: 153.95\n",
      "episode: 1118   score: 270.0   memory length: 10240   epsilon: 1.0    steps: 1516     evaluation reward: 154.55\n",
      "episode: 1119   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 549     evaluation reward: 155.2\n",
      "episode: 1120   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 683     evaluation reward: 154.35\n",
      "episode: 1121   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 746     evaluation reward: 155.2\n",
      "episode: 1122   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 730     evaluation reward: 156.2\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.032563. Value loss: 5.418504. Entropy: 0.835017.\n",
      "Iteration 2\n",
      "Policy loss: -0.047685. Value loss: 3.006608. Entropy: 0.828662.\n",
      "Iteration 3\n",
      "Policy loss: -0.054296. Value loss: 2.233055. Entropy: 0.833057.\n",
      "Iteration 4\n",
      "Policy loss: -0.057875. Value loss: 2.096788. Entropy: 0.828933.\n",
      "Iteration 5\n",
      "Policy loss: -0.055875. Value loss: 1.838819. Entropy: 0.835767.\n",
      "episode: 1123   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 678     evaluation reward: 156.0\n",
      "episode: 1124   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 518     evaluation reward: 156.05\n",
      "episode: 1125   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 378     evaluation reward: 155.5\n",
      "episode: 1126   score: 95.0   memory length: 10240   epsilon: 1.0    steps: 668     evaluation reward: 155.5\n",
      "episode: 1127   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 775     evaluation reward: 155.8\n",
      "episode: 1128   score: 225.0   memory length: 10240   epsilon: 1.0    steps: 1065     evaluation reward: 156.65\n",
      "episode: 1129   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 467     evaluation reward: 156.35\n",
      "episode: 1130   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 527     evaluation reward: 156.15\n",
      "episode: 1131   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 788     evaluation reward: 156.4\n",
      "episode: 1132   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 642     evaluation reward: 154.55\n",
      "episode: 1133   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 623     evaluation reward: 152.2\n",
      "episode: 1134   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 647     evaluation reward: 152.45\n",
      "episode: 1135   score: 20.0   memory length: 10240   epsilon: 1.0    steps: 498     evaluation reward: 151.55\n",
      "episode: 1136   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 543     evaluation reward: 151.7\n",
      "episode: 1137   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 647     evaluation reward: 150.2\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.034324. Value loss: 4.506856. Entropy: 0.921928.\n",
      "Iteration 2\n",
      "Policy loss: -0.054053. Value loss: 2.595099. Entropy: 0.916199.\n",
      "Iteration 3\n",
      "Policy loss: -0.059676. Value loss: 1.954442. Entropy: 0.906349.\n",
      "Iteration 4\n",
      "Policy loss: -0.063579. Value loss: 1.728441. Entropy: 0.921919.\n",
      "Iteration 5\n",
      "Policy loss: -0.071569. Value loss: 1.547598. Entropy: 0.924734.\n",
      "episode: 1138   score: 285.0   memory length: 10240   epsilon: 1.0    steps: 1082     evaluation reward: 152.05\n",
      "episode: 1139   score: 170.0   memory length: 10240   epsilon: 1.0    steps: 815     evaluation reward: 152.2\n",
      "episode: 1140   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 797     evaluation reward: 152.8\n",
      "episode: 1141   score: 85.0   memory length: 10240   epsilon: 1.0    steps: 630     evaluation reward: 153.05\n",
      "episode: 1142   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 637     evaluation reward: 152.5\n",
      "episode: 1143   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 513     evaluation reward: 152.2\n",
      "episode: 1144   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 538     evaluation reward: 151.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1145   score: 140.0   memory length: 10240   epsilon: 1.0    steps: 644     evaluation reward: 151.5\n",
      "episode: 1146   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 563     evaluation reward: 151.2\n",
      "episode: 1147   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 630     evaluation reward: 150.85\n",
      "episode: 1148   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 436     evaluation reward: 150.9\n",
      "episode: 1149   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 588     evaluation reward: 150.9\n",
      "episode: 1150   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 384     evaluation reward: 150.4\n",
      "episode: 1151   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 722     evaluation reward: 150.1\n",
      "episode: 1152   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 585     evaluation reward: 149.35\n",
      "episode: 1153   score: 240.0   memory length: 10240   epsilon: 1.0    steps: 978     evaluation reward: 149.65\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.039236. Value loss: 4.490612. Entropy: 0.955318.\n",
      "Iteration 2\n",
      "Policy loss: -0.050780. Value loss: 2.411666. Entropy: 0.953064.\n",
      "Iteration 3\n",
      "Policy loss: -0.060791. Value loss: 1.815819. Entropy: 0.944044.\n",
      "Iteration 4\n",
      "Policy loss: -0.061859. Value loss: 1.549000. Entropy: 0.948843.\n",
      "Iteration 5\n",
      "Policy loss: -0.063304. Value loss: 1.419641. Entropy: 0.945075.\n",
      "episode: 1154   score: 165.0   memory length: 10240   epsilon: 1.0    steps: 834     evaluation reward: 149.95\n",
      "episode: 1155   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 588     evaluation reward: 149.85\n",
      "now time :  2018-12-18 18:04:59.318553\n",
      "episode: 1156   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 754     evaluation reward: 148.35\n",
      "episode: 1157   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 600     evaluation reward: 147.6\n",
      "episode: 1158   score: 190.0   memory length: 10240   epsilon: 1.0    steps: 836     evaluation reward: 148.4\n",
      "episode: 1159   score: 225.0   memory length: 10240   epsilon: 1.0    steps: 1375     evaluation reward: 149.85\n",
      "episode: 1160   score: 315.0   memory length: 10240   epsilon: 1.0    steps: 868     evaluation reward: 151.95\n",
      "episode: 1161   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 340     evaluation reward: 151.2\n",
      "episode: 1162   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 671     evaluation reward: 150.25\n",
      "episode: 1163   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 532     evaluation reward: 149.55\n",
      "episode: 1164   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 466     evaluation reward: 148.75\n",
      "episode: 1165   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 655     evaluation reward: 147.75\n",
      "episode: 1166   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 508     evaluation reward: 146.9\n",
      "episode: 1167   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 505     evaluation reward: 146.75\n",
      "episode: 1168   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 392     evaluation reward: 146.0\n",
      "episode: 1169   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 394     evaluation reward: 142.15\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.050389. Value loss: 4.120235. Entropy: 0.907696.\n",
      "Iteration 2\n",
      "Policy loss: -0.067011. Value loss: 1.952553. Entropy: 0.908920.\n",
      "Iteration 3\n",
      "Policy loss: -0.070127. Value loss: 1.596260. Entropy: 0.912375.\n",
      "Iteration 4\n",
      "Policy loss: -0.075546. Value loss: 1.386517. Entropy: 0.909865.\n",
      "Iteration 5\n",
      "Policy loss: -0.075348. Value loss: 1.291005. Entropy: 0.916086.\n",
      "episode: 1170   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 644     evaluation reward: 142.0\n",
      "episode: 1171   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 601     evaluation reward: 140.7\n",
      "episode: 1172   score: 215.0   memory length: 10240   epsilon: 1.0    steps: 831     evaluation reward: 141.8\n",
      "episode: 1173   score: 225.0   memory length: 10240   epsilon: 1.0    steps: 865     evaluation reward: 143.45\n",
      "episode: 1174   score: 225.0   memory length: 10240   epsilon: 1.0    steps: 1279     evaluation reward: 144.1\n",
      "episode: 1175   score: 400.0   memory length: 10240   epsilon: 1.0    steps: 964     evaluation reward: 147.75\n",
      "episode: 1176   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 444     evaluation reward: 145.15\n",
      "episode: 1177   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 646     evaluation reward: 145.85\n",
      "episode: 1178   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 783     evaluation reward: 145.2\n",
      "episode: 1179   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 941     evaluation reward: 146.5\n",
      "episode: 1180   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 703     evaluation reward: 147.2\n",
      "episode: 1181   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 486     evaluation reward: 146.7\n",
      "episode: 1182   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 710     evaluation reward: 146.7\n",
      "episode: 1183   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 391     evaluation reward: 144.75\n",
      "episode: 1184   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 376     evaluation reward: 145.05\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.038645. Value loss: 4.911716. Entropy: 0.838853.\n",
      "Iteration 2\n",
      "Policy loss: -0.056739. Value loss: 2.477446. Entropy: 0.843067.\n",
      "Iteration 3\n",
      "Policy loss: -0.066211. Value loss: 1.836587. Entropy: 0.822583.\n",
      "Iteration 4\n",
      "Policy loss: -0.066050. Value loss: 1.530425. Entropy: 0.830334.\n",
      "Iteration 5\n",
      "Policy loss: -0.071113. Value loss: 1.361678. Entropy: 0.835292.\n",
      "episode: 1185   score: 485.0   memory length: 10240   epsilon: 1.0    steps: 973     evaluation reward: 149.3\n",
      "episode: 1186   score: 430.0   memory length: 10240   epsilon: 1.0    steps: 976     evaluation reward: 152.4\n",
      "episode: 1187   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 703     evaluation reward: 152.15\n",
      "episode: 1188   score: 165.0   memory length: 10240   epsilon: 1.0    steps: 698     evaluation reward: 152.6\n",
      "episode: 1189   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 578     evaluation reward: 148.0\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        if render_breakout:\n",
    "            env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        action, value = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        #r = np.clip(reward, -1, 1)\n",
    "        r = reward\n",
    "        \n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state, value, 0, 0)\n",
    "        # Start training after random sample generation\n",
    "        if(frame % train_frame == 0):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network\n",
    "            agent.update_target_net()\n",
    "        score += r\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > 40 and len(evaluation_reward) > 350:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn\")\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
