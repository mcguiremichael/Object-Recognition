{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lifes(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 6\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/anaconda3/envs/py36/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],0)[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 155.0   memory length: 925   epsilon: 1.0    steps: 925     evaluation reward: 155.0\n",
      "episode: 1   score: 15.0   memory length: 1427   epsilon: 1.0    steps: 502     evaluation reward: 85.0\n",
      "episode: 2   score: 80.0   memory length: 2052   epsilon: 1.0    steps: 625     evaluation reward: 83.33333333333333\n",
      "episode: 3   score: 55.0   memory length: 2416   epsilon: 1.0    steps: 364     evaluation reward: 76.25\n",
      "episode: 4   score: 160.0   memory length: 3150   epsilon: 1.0    steps: 734     evaluation reward: 93.0\n",
      "episode: 5   score: 50.0   memory length: 3635   epsilon: 1.0    steps: 485     evaluation reward: 85.83333333333333\n",
      "Training network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:241: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:242: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:243: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Policy loss: -0.000008. Value loss: 2.160420. Entropy: 1.790013.\n",
      "Iteration 2: Policy loss: -0.000068. Value loss: 2.151835. Entropy: 1.789993.\n",
      "Iteration 3: Policy loss: -0.000094. Value loss: 2.162931. Entropy: 1.790013.\n",
      "episode: 6   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 616     evaluation reward: 90.71428571428571\n",
      "episode: 7   score: 60.0   memory length: 4096   epsilon: 1.0    steps: 612     evaluation reward: 86.875\n",
      "episode: 8   score: 175.0   memory length: 4096   epsilon: 1.0    steps: 1080     evaluation reward: 96.66666666666667\n",
      "episode: 9   score: 105.0   memory length: 4096   epsilon: 1.0    steps: 636     evaluation reward: 97.5\n",
      "episode: 10   score: 55.0   memory length: 4096   epsilon: 1.0    steps: 454     evaluation reward: 93.63636363636364\n",
      "episode: 11   score: 105.0   memory length: 4096   epsilon: 1.0    steps: 638     evaluation reward: 94.58333333333333\n",
      "Training network\n",
      "Iteration 4: Policy loss: 0.000001. Value loss: 2.296194. Entropy: 1.789980.\n",
      "Iteration 5: Policy loss: -0.000062. Value loss: 2.310826. Entropy: 1.789976.\n",
      "Iteration 6: Policy loss: -0.000116. Value loss: 2.284163. Entropy: 1.789992.\n",
      "episode: 12   score: 110.0   memory length: 4096   epsilon: 1.0    steps: 556     evaluation reward: 95.76923076923077\n",
      "episode: 13   score: 85.0   memory length: 4096   epsilon: 1.0    steps: 509     evaluation reward: 95.0\n",
      "episode: 14   score: 105.0   memory length: 4096   epsilon: 1.0    steps: 634     evaluation reward: 95.66666666666667\n",
      "episode: 15   score: 275.0   memory length: 4096   epsilon: 1.0    steps: 1050     evaluation reward: 106.875\n",
      "episode: 16   score: 140.0   memory length: 4096   epsilon: 1.0    steps: 689     evaluation reward: 108.82352941176471\n",
      "episode: 17   score: 65.0   memory length: 4096   epsilon: 1.0    steps: 487     evaluation reward: 106.38888888888889\n",
      "episode: 18   score: 105.0   memory length: 4096   epsilon: 1.0    steps: 590     evaluation reward: 106.3157894736842\n",
      "Training network\n",
      "Iteration 7: Policy loss: 0.000002. Value loss: 2.899918. Entropy: 1.789913.\n",
      "Iteration 8: Policy loss: -0.000056. Value loss: 2.879301. Entropy: 1.789910.\n",
      "Iteration 9: Policy loss: -0.000111. Value loss: 2.903386. Entropy: 1.789898.\n",
      "episode: 19   score: 300.0   memory length: 4096   epsilon: 1.0    steps: 1039     evaluation reward: 116.0\n",
      "episode: 20   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 735     evaluation reward: 116.19047619047619\n",
      "episode: 21   score: 155.0   memory length: 4096   epsilon: 1.0    steps: 686     evaluation reward: 117.95454545454545\n",
      "episode: 22   score: 125.0   memory length: 4096   epsilon: 1.0    steps: 647     evaluation reward: 118.26086956521739\n",
      "episode: 23   score: 395.0   memory length: 4096   epsilon: 1.0    steps: 1067     evaluation reward: 129.79166666666666\n",
      "Training network\n",
      "Iteration 10: Policy loss: 0.000007. Value loss: 4.126735. Entropy: 1.789941.\n",
      "Iteration 11: Policy loss: -0.000038. Value loss: 4.125673. Entropy: 1.789901.\n",
      "Iteration 12: Policy loss: -0.000052. Value loss: 4.132116. Entropy: 1.789900.\n",
      "episode: 24   score: 135.0   memory length: 4096   epsilon: 1.0    steps: 736     evaluation reward: 130.0\n",
      "episode: 25   score: 105.0   memory length: 4096   epsilon: 1.0    steps: 502     evaluation reward: 129.03846153846155\n",
      "episode: 26   score: 155.0   memory length: 4096   epsilon: 1.0    steps: 611     evaluation reward: 130.0\n",
      "episode: 27   score: 80.0   memory length: 4096   epsilon: 1.0    steps: 446     evaluation reward: 128.21428571428572\n",
      "episode: 28   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 564     evaluation reward: 127.93103448275862\n",
      "episode: 29   score: 215.0   memory length: 4096   epsilon: 1.0    steps: 970     evaluation reward: 130.83333333333334\n",
      "Training network\n",
      "Iteration 13: Policy loss: -0.000022. Value loss: 3.287019. Entropy: 1.789905.\n",
      "Iteration 14: Policy loss: 0.000010. Value loss: 3.287779. Entropy: 1.789894.\n",
      "Iteration 15: Policy loss: -0.000042. Value loss: 3.263491. Entropy: 1.789865.\n",
      "episode: 30   score: 105.0   memory length: 4096   epsilon: 1.0    steps: 605     evaluation reward: 130.0\n",
      "episode: 31   score: 105.0   memory length: 4096   epsilon: 1.0    steps: 527     evaluation reward: 129.21875\n",
      "episode: 32   score: 30.0   memory length: 4096   epsilon: 1.0    steps: 399     evaluation reward: 126.21212121212122\n",
      "episode: 33   score: 175.0   memory length: 4096   epsilon: 1.0    steps: 713     evaluation reward: 127.6470588235294\n",
      "episode: 34   score: 210.0   memory length: 4096   epsilon: 1.0    steps: 722     evaluation reward: 130.0\n",
      "episode: 35   score: 100.0   memory length: 4096   epsilon: 1.0    steps: 515     evaluation reward: 129.16666666666666\n",
      "Training network\n",
      "Iteration 16: Policy loss: 0.000003. Value loss: 3.252953. Entropy: 1.789972.\n",
      "Iteration 17: Policy loss: -0.000037. Value loss: 3.254792. Entropy: 1.789950.\n",
      "Iteration 18: Policy loss: -0.000072. Value loss: 3.212860. Entropy: 1.789957.\n",
      "episode: 36   score: 210.0   memory length: 4096   epsilon: 1.0    steps: 911     evaluation reward: 131.35135135135135\n",
      "episode: 37   score: 210.0   memory length: 4096   epsilon: 1.0    steps: 723     evaluation reward: 133.42105263157896\n",
      "episode: 38   score: 170.0   memory length: 4096   epsilon: 1.0    steps: 779     evaluation reward: 134.35897435897436\n",
      "episode: 39   score: 155.0   memory length: 4096   epsilon: 1.0    steps: 720     evaluation reward: 134.875\n",
      "episode: 40   score: 105.0   memory length: 4096   epsilon: 1.0    steps: 630     evaluation reward: 134.14634146341464\n",
      "episode: 41   score: 155.0   memory length: 4096   epsilon: 1.0    steps: 648     evaluation reward: 134.64285714285714\n",
      "Training network\n",
      "Iteration 19: Policy loss: 0.000006. Value loss: 3.175895. Entropy: 1.789989.\n",
      "Iteration 20: Policy loss: -0.000047. Value loss: 3.196783. Entropy: 1.790018.\n",
      "Iteration 21: Policy loss: -0.000117. Value loss: 3.191088. Entropy: 1.790025.\n",
      "episode: 42   score: 100.0   memory length: 4096   epsilon: 1.0    steps: 696     evaluation reward: 133.8372093023256\n",
      "episode: 43   score: 135.0   memory length: 4096   epsilon: 1.0    steps: 753     evaluation reward: 133.86363636363637\n",
      "episode: 44   score: 65.0   memory length: 4096   epsilon: 1.0    steps: 405     evaluation reward: 132.33333333333334\n",
      "episode: 45   score: 70.0   memory length: 4096   epsilon: 1.0    steps: 622     evaluation reward: 130.97826086956522\n",
      "episode: 46   score: 45.0   memory length: 4096   epsilon: 1.0    steps: 411     evaluation reward: 129.14893617021278\n",
      "episode: 47   score: 380.0   memory length: 4096   epsilon: 1.0    steps: 806     evaluation reward: 134.375\n",
      "episode: 48   score: 155.0   memory length: 4096   epsilon: 1.0    steps: 708     evaluation reward: 134.79591836734693\n",
      "Training network\n",
      "Iteration 22: Policy loss: 0.000000. Value loss: 3.457419. Entropy: 1.789982.\n",
      "Iteration 23: Policy loss: 0.000001. Value loss: 3.449094. Entropy: 1.789991.\n",
      "Iteration 24: Policy loss: -0.000013. Value loss: 3.372391. Entropy: 1.789985.\n",
      "episode: 49   score: 135.0   memory length: 4096   epsilon: 1.0    steps: 600     evaluation reward: 134.8\n",
      "episode: 50   score: 350.0   memory length: 4096   epsilon: 1.0    steps: 1288     evaluation reward: 139.01960784313727\n",
      "episode: 51   score: 225.0   memory length: 4096   epsilon: 1.0    steps: 874     evaluation reward: 140.67307692307693\n",
      "episode: 52   score: 80.0   memory length: 4096   epsilon: 1.0    steps: 433     evaluation reward: 139.52830188679246\n",
      "episode: 53   score: 140.0   memory length: 4096   epsilon: 1.0    steps: 838     evaluation reward: 139.53703703703704\n",
      "Training network\n",
      "Iteration 25: Policy loss: 0.000014. Value loss: 3.476827. Entropy: 1.789980.\n",
      "Iteration 26: Policy loss: -0.000029. Value loss: 3.490561. Entropy: 1.789950.\n",
      "Iteration 27: Policy loss: -0.000047. Value loss: 3.486445. Entropy: 1.789946.\n",
      "episode: 54   score: 185.0   memory length: 4096   epsilon: 1.0    steps: 638     evaluation reward: 140.36363636363637\n",
      "episode: 55   score: 80.0   memory length: 4096   epsilon: 1.0    steps: 695     evaluation reward: 139.28571428571428\n",
      "episode: 56   score: 230.0   memory length: 4096   epsilon: 1.0    steps: 844     evaluation reward: 140.87719298245614\n",
      "episode: 57   score: 135.0   memory length: 4096   epsilon: 1.0    steps: 531     evaluation reward: 140.77586206896552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 58   score: 180.0   memory length: 4096   epsilon: 1.0    steps: 635     evaluation reward: 141.4406779661017\n",
      "episode: 59   score: 355.0   memory length: 4096   epsilon: 1.0    steps: 1066     evaluation reward: 145.0\n",
      "Training network\n",
      "Iteration 28: Policy loss: 0.000003. Value loss: 4.206216. Entropy: 1.790148.\n",
      "Iteration 29: Policy loss: -0.000041. Value loss: 4.158674. Entropy: 1.790138.\n",
      "Iteration 30: Policy loss: -0.000082. Value loss: 4.215108. Entropy: 1.790112.\n",
      "episode: 60   score: 210.0   memory length: 4096   epsilon: 1.0    steps: 837     evaluation reward: 146.0655737704918\n",
      "episode: 61   score: 110.0   memory length: 4096   epsilon: 1.0    steps: 768     evaluation reward: 145.48387096774192\n",
      "episode: 62   score: 45.0   memory length: 4096   epsilon: 1.0    steps: 414     evaluation reward: 143.88888888888889\n",
      "episode: 63   score: 155.0   memory length: 4096   epsilon: 1.0    steps: 522     evaluation reward: 144.0625\n",
      "episode: 64   score: 80.0   memory length: 4096   epsilon: 1.0    steps: 510     evaluation reward: 143.07692307692307\n",
      "episode: 65   score: 45.0   memory length: 4096   epsilon: 1.0    steps: 466     evaluation reward: 141.5909090909091\n",
      "Training network\n",
      "Iteration 31: Policy loss: -0.000009. Value loss: 2.746977. Entropy: 1.790103.\n",
      "Iteration 32: Policy loss: -0.000031. Value loss: 2.725580. Entropy: 1.790100.\n",
      "Iteration 33: Policy loss: -0.000106. Value loss: 2.714659. Entropy: 1.790093.\n",
      "episode: 66   score: 110.0   memory length: 4096   epsilon: 1.0    steps: 659     evaluation reward: 141.11940298507463\n",
      "episode: 67   score: 135.0   memory length: 4096   epsilon: 1.0    steps: 623     evaluation reward: 141.02941176470588\n",
      "episode: 68   score: 125.0   memory length: 4096   epsilon: 1.0    steps: 639     evaluation reward: 140.79710144927537\n",
      "episode: 69   score: 75.0   memory length: 4096   epsilon: 1.0    steps: 686     evaluation reward: 139.85714285714286\n",
      "episode: 70   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 674     evaluation reward: 139.57746478873239\n",
      "episode: 71   score: 135.0   memory length: 4096   epsilon: 1.0    steps: 582     evaluation reward: 139.51388888888889\n",
      "episode: 72   score: 80.0   memory length: 4096   epsilon: 1.0    steps: 399     evaluation reward: 138.6986301369863\n",
      "Training network\n",
      "Iteration 34: Policy loss: 0.000014. Value loss: 2.899927. Entropy: 1.790010.\n",
      "Iteration 35: Policy loss: -0.000023. Value loss: 2.875047. Entropy: 1.790058.\n",
      "Iteration 36: Policy loss: -0.000022. Value loss: 2.872448. Entropy: 1.790027.\n",
      "episode: 73   score: 215.0   memory length: 4096   epsilon: 1.0    steps: 906     evaluation reward: 139.72972972972974\n",
      "now time :  2018-12-19 11:35:11.749897\n",
      "episode: 74   score: 135.0   memory length: 4096   epsilon: 1.0    steps: 700     evaluation reward: 139.66666666666666\n",
      "episode: 75   score: 110.0   memory length: 4096   epsilon: 1.0    steps: 625     evaluation reward: 139.27631578947367\n",
      "episode: 76   score: 20.0   memory length: 4096   epsilon: 1.0    steps: 503     evaluation reward: 137.72727272727272\n",
      "episode: 77   score: 320.0   memory length: 4096   epsilon: 1.0    steps: 718     evaluation reward: 140.06410256410257\n",
      "episode: 78   score: 130.0   memory length: 4096   epsilon: 1.0    steps: 557     evaluation reward: 139.9367088607595\n",
      "episode: 79   score: 80.0   memory length: 4096   epsilon: 1.0    steps: 419     evaluation reward: 139.1875\n",
      "Training network\n",
      "Iteration 37: Policy loss: -0.000000. Value loss: 3.504107. Entropy: 1.790183.\n",
      "Iteration 38: Policy loss: -0.000018. Value loss: 3.492367. Entropy: 1.790171.\n",
      "Iteration 39: Policy loss: -0.000064. Value loss: 3.493073. Entropy: 1.790183.\n",
      "episode: 80   score: 110.0   memory length: 4096   epsilon: 1.0    steps: 487     evaluation reward: 138.82716049382717\n",
      "episode: 81   score: 160.0   memory length: 4096   epsilon: 1.0    steps: 632     evaluation reward: 139.08536585365854\n",
      "episode: 82   score: 195.0   memory length: 4096   epsilon: 1.0    steps: 937     evaluation reward: 139.75903614457832\n",
      "episode: 83   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 717     evaluation reward: 139.52380952380952\n",
      "episode: 84   score: 55.0   memory length: 4096   epsilon: 1.0    steps: 388     evaluation reward: 138.52941176470588\n",
      "episode: 85   score: 175.0   memory length: 4096   epsilon: 1.0    steps: 787     evaluation reward: 138.95348837209303\n",
      "Training network\n",
      "Iteration 40: Policy loss: 0.000000. Value loss: 3.168995. Entropy: 1.790049.\n",
      "Iteration 41: Policy loss: -0.000012. Value loss: 3.202514. Entropy: 1.790073.\n",
      "Iteration 42: Policy loss: -0.000069. Value loss: 3.161307. Entropy: 1.790083.\n",
      "episode: 86   score: 190.0   memory length: 4096   epsilon: 1.0    steps: 695     evaluation reward: 139.54022988505747\n",
      "episode: 87   score: 105.0   memory length: 4096   epsilon: 1.0    steps: 733     evaluation reward: 139.14772727272728\n",
      "episode: 88   score: 150.0   memory length: 4096   epsilon: 1.0    steps: 619     evaluation reward: 139.26966292134833\n",
      "episode: 89   score: 180.0   memory length: 4096   epsilon: 1.0    steps: 654     evaluation reward: 139.72222222222223\n",
      "episode: 90   score: 110.0   memory length: 4096   epsilon: 1.0    steps: 641     evaluation reward: 139.3956043956044\n",
      "episode: 91   score: 55.0   memory length: 4096   epsilon: 1.0    steps: 432     evaluation reward: 138.47826086956522\n",
      "Training network\n",
      "Iteration 43: Policy loss: -0.000005. Value loss: 3.191692. Entropy: 1.790084.\n",
      "Iteration 44: Policy loss: -0.000051. Value loss: 3.220115. Entropy: 1.790119.\n",
      "Iteration 45: Policy loss: -0.000116. Value loss: 3.235722. Entropy: 1.790096.\n",
      "episode: 92   score: 180.0   memory length: 4096   epsilon: 1.0    steps: 735     evaluation reward: 138.9247311827957\n",
      "episode: 93   score: 55.0   memory length: 4096   epsilon: 1.0    steps: 503     evaluation reward: 138.03191489361703\n",
      "episode: 94   score: 65.0   memory length: 4096   epsilon: 1.0    steps: 642     evaluation reward: 137.26315789473685\n",
      "episode: 95   score: 135.0   memory length: 4096   epsilon: 1.0    steps: 665     evaluation reward: 137.23958333333334\n",
      "episode: 96   score: 55.0   memory length: 4096   epsilon: 1.0    steps: 426     evaluation reward: 136.39175257731958\n",
      "episode: 97   score: 65.0   memory length: 4096   epsilon: 1.0    steps: 631     evaluation reward: 135.66326530612244\n",
      "episode: 98   score: 85.0   memory length: 4096   epsilon: 1.0    steps: 386     evaluation reward: 135.15151515151516\n",
      "episode: 99   score: 155.0   memory length: 4096   epsilon: 1.0    steps: 587     evaluation reward: 135.35\n",
      "Training network\n",
      "Iteration 46: Policy loss: 0.000004. Value loss: 2.307490. Entropy: 1.790033.\n",
      "Iteration 47: Policy loss: -0.000020. Value loss: 2.325519. Entropy: 1.790053.\n",
      "Iteration 48: Policy loss: -0.000040. Value loss: 2.313597. Entropy: 1.790048.\n",
      "episode: 100   score: 45.0   memory length: 4096   epsilon: 1.0    steps: 412     evaluation reward: 134.25\n",
      "episode: 101   score: 410.0   memory length: 4096   epsilon: 1.0    steps: 1007     evaluation reward: 138.2\n",
      "episode: 102   score: 80.0   memory length: 4096   epsilon: 1.0    steps: 556     evaluation reward: 138.2\n",
      "episode: 103   score: 30.0   memory length: 4096   epsilon: 1.0    steps: 336     evaluation reward: 137.95\n",
      "episode: 104   score: 75.0   memory length: 4096   epsilon: 1.0    steps: 405     evaluation reward: 137.1\n",
      "episode: 105   score: 155.0   memory length: 4096   epsilon: 1.0    steps: 622     evaluation reward: 138.15\n",
      "Training network\n",
      "Iteration 49: Policy loss: -0.000000. Value loss: 3.727541. Entropy: 1.790088.\n",
      "Iteration 50: Policy loss: -0.000066. Value loss: 3.712343. Entropy: 1.790090.\n",
      "Iteration 51: Policy loss: -0.000070. Value loss: 3.741417. Entropy: 1.790114.\n",
      "episode: 106   score: 240.0   memory length: 4096   epsilon: 1.0    steps: 957     evaluation reward: 139.35\n",
      "episode: 107   score: 110.0   memory length: 4096   epsilon: 1.0    steps: 665     evaluation reward: 139.85\n",
      "episode: 108   score: 55.0   memory length: 4096   epsilon: 1.0    steps: 522     evaluation reward: 138.65\n",
      "episode: 109   score: 105.0   memory length: 4096   epsilon: 1.0    steps: 538     evaluation reward: 138.65\n",
      "episode: 110   score: 35.0   memory length: 4096   epsilon: 1.0    steps: 544     evaluation reward: 138.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 111   score: 35.0   memory length: 4096   epsilon: 1.0    steps: 333     evaluation reward: 137.75\n",
      "episode: 112   score: 80.0   memory length: 4096   epsilon: 1.0    steps: 605     evaluation reward: 137.45\n",
      "episode: 113   score: 110.0   memory length: 4096   epsilon: 1.0    steps: 602     evaluation reward: 137.7\n",
      "Training network\n",
      "Iteration 52: Policy loss: -0.000013. Value loss: 2.008911. Entropy: 1.790038.\n",
      "Iteration 53: Policy loss: -0.000075. Value loss: 2.010473. Entropy: 1.790040.\n",
      "Iteration 54: Policy loss: -0.000090. Value loss: 2.008089. Entropy: 1.790053.\n",
      "episode: 114   score: 80.0   memory length: 4096   epsilon: 1.0    steps: 684     evaluation reward: 137.45\n",
      "episode: 115   score: 180.0   memory length: 4096   epsilon: 1.0    steps: 833     evaluation reward: 136.5\n",
      "episode: 116   score: 115.0   memory length: 4096   epsilon: 1.0    steps: 749     evaluation reward: 136.25\n",
      "episode: 117   score: 165.0   memory length: 4096   epsilon: 1.0    steps: 842     evaluation reward: 137.25\n",
      "Training network\n",
      "Iteration 55: Policy loss: 0.000008. Value loss: 2.679045. Entropy: 1.790106.\n",
      "Iteration 56: Policy loss: -0.000039. Value loss: 2.722067. Entropy: 1.790093.\n",
      "Iteration 57: Policy loss: -0.000049. Value loss: 2.692668. Entropy: 1.790101.\n",
      "episode: 118   score: 225.0   memory length: 4096   epsilon: 1.0    steps: 1406     evaluation reward: 138.45\n",
      "episode: 119   score: 185.0   memory length: 4096   epsilon: 1.0    steps: 664     evaluation reward: 137.3\n",
      "episode: 120   score: 240.0   memory length: 4096   epsilon: 1.0    steps: 1389     evaluation reward: 138.5\n",
      "episode: 121   score: 125.0   memory length: 4096   epsilon: 1.0    steps: 702     evaluation reward: 138.2\n",
      "episode: 122   score: 155.0   memory length: 4096   epsilon: 1.0    steps: 706     evaluation reward: 138.5\n",
      "Training network\n",
      "Iteration 58: Policy loss: 0.000007. Value loss: 3.352524. Entropy: 1.789945.\n",
      "Iteration 59: Policy loss: -0.000036. Value loss: 3.365067. Entropy: 1.789965.\n",
      "Iteration 60: Policy loss: -0.000090. Value loss: 3.357201. Entropy: 1.789934.\n",
      "episode: 123   score: 340.0   memory length: 4096   epsilon: 1.0    steps: 937     evaluation reward: 137.95\n",
      "episode: 124   score: 95.0   memory length: 4096   epsilon: 1.0    steps: 617     evaluation reward: 137.55\n",
      "episode: 125   score: 180.0   memory length: 4096   epsilon: 1.0    steps: 664     evaluation reward: 138.3\n",
      "episode: 126   score: 45.0   memory length: 4096   epsilon: 1.0    steps: 402     evaluation reward: 137.2\n",
      "episode: 127   score: 130.0   memory length: 4096   epsilon: 1.0    steps: 613     evaluation reward: 137.7\n",
      "episode: 128   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 743     evaluation reward: 137.7\n",
      "episode: 129   score: 105.0   memory length: 4096   epsilon: 1.0    steps: 517     evaluation reward: 136.6\n",
      "Training network\n",
      "Iteration 61: Policy loss: -0.000003. Value loss: 3.067867. Entropy: 1.790055.\n",
      "Iteration 62: Policy loss: -0.000065. Value loss: 3.057745. Entropy: 1.790049.\n",
      "Iteration 63: Policy loss: -0.000131. Value loss: 3.038364. Entropy: 1.790069.\n",
      "episode: 130   score: 140.0   memory length: 4096   epsilon: 1.0    steps: 747     evaluation reward: 136.95\n",
      "episode: 131   score: 110.0   memory length: 4096   epsilon: 1.0    steps: 754     evaluation reward: 137.0\n",
      "episode: 132   score: 65.0   memory length: 4096   epsilon: 1.0    steps: 593     evaluation reward: 137.35\n",
      "episode: 133   score: 45.0   memory length: 4096   epsilon: 1.0    steps: 580     evaluation reward: 136.05\n",
      "episode: 134   score: 475.0   memory length: 4096   epsilon: 1.0    steps: 951     evaluation reward: 138.7\n",
      "Training network\n",
      "Iteration 64: Policy loss: -0.000015. Value loss: 3.352431. Entropy: 1.790163.\n",
      "Iteration 65: Policy loss: -0.000045. Value loss: 3.324771. Entropy: 1.790149.\n",
      "Iteration 66: Policy loss: -0.000094. Value loss: 3.310360. Entropy: 1.790142.\n",
      "episode: 135   score: 45.0   memory length: 4096   epsilon: 1.0    steps: 501     evaluation reward: 138.15\n",
      "episode: 136   score: 90.0   memory length: 4096   epsilon: 1.0    steps: 642     evaluation reward: 136.95\n",
      "episode: 137   score: 140.0   memory length: 4096   epsilon: 1.0    steps: 629     evaluation reward: 136.25\n",
      "episode: 138   score: 135.0   memory length: 4096   epsilon: 1.0    steps: 928     evaluation reward: 135.9\n",
      "episode: 139   score: 215.0   memory length: 4096   epsilon: 1.0    steps: 925     evaluation reward: 136.5\n",
      "episode: 140   score: 95.0   memory length: 4096   epsilon: 1.0    steps: 458     evaluation reward: 136.4\n",
      "episode: 141   score: 75.0   memory length: 4096   epsilon: 1.0    steps: 431     evaluation reward: 135.6\n",
      "Training network\n",
      "Iteration 67: Policy loss: -0.000001. Value loss: 2.789312. Entropy: 1.790215.\n",
      "Iteration 68: Policy loss: -0.000041. Value loss: 2.795434. Entropy: 1.790231.\n",
      "Iteration 69: Policy loss: -0.000073. Value loss: 2.775742. Entropy: 1.790210.\n",
      "episode: 142   score: 20.0   memory length: 4096   epsilon: 1.0    steps: 524     evaluation reward: 134.8\n",
      "episode: 143   score: 65.0   memory length: 4096   epsilon: 1.0    steps: 427     evaluation reward: 134.1\n",
      "episode: 144   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 646     evaluation reward: 134.65\n",
      "episode: 145   score: 130.0   memory length: 4096   epsilon: 1.0    steps: 726     evaluation reward: 135.25\n",
      "episode: 146   score: 45.0   memory length: 4096   epsilon: 1.0    steps: 453     evaluation reward: 135.25\n",
      "episode: 147   score: 125.0   memory length: 4096   epsilon: 1.0    steps: 596     evaluation reward: 132.7\n",
      "episode: 148   score: 135.0   memory length: 4096   epsilon: 1.0    steps: 547     evaluation reward: 132.5\n",
      "Training network\n",
      "Iteration 70: Policy loss: 0.000003. Value loss: 2.726820. Entropy: 1.790004.\n",
      "Iteration 71: Policy loss: -0.000036. Value loss: 2.728267. Entropy: 1.790038.\n",
      "Iteration 72: Policy loss: -0.000085. Value loss: 2.737261. Entropy: 1.790037.\n",
      "episode: 149   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 423     evaluation reward: 132.35\n",
      "episode: 150   score: 460.0   memory length: 4096   epsilon: 1.0    steps: 948     evaluation reward: 133.45\n",
      "now time :  2018-12-19 11:38:12.829717\n",
      "episode: 151   score: 140.0   memory length: 4096   epsilon: 1.0    steps: 782     evaluation reward: 132.6\n",
      "episode: 152   score: 210.0   memory length: 4096   epsilon: 1.0    steps: 772     evaluation reward: 133.9\n",
      "episode: 153   score: 215.0   memory length: 4096   epsilon: 1.0    steps: 820     evaluation reward: 134.65\n",
      "episode: 154   score: 80.0   memory length: 4096   epsilon: 1.0    steps: 587     evaluation reward: 133.6\n",
      "Training network\n",
      "Iteration 73: Policy loss: -0.000005. Value loss: 4.173660. Entropy: 1.790053.\n",
      "Iteration 74: Policy loss: -0.000030. Value loss: 4.218901. Entropy: 1.790069.\n",
      "Iteration 75: Policy loss: -0.000057. Value loss: 4.187990. Entropy: 1.790069.\n",
      "episode: 155   score: 110.0   memory length: 4096   epsilon: 1.0    steps: 655     evaluation reward: 133.9\n",
      "episode: 156   score: 85.0   memory length: 4096   epsilon: 1.0    steps: 387     evaluation reward: 132.45\n",
      "episode: 157   score: 380.0   memory length: 4096   epsilon: 1.0    steps: 807     evaluation reward: 134.9\n",
      "episode: 158   score: 210.0   memory length: 4096   epsilon: 1.0    steps: 777     evaluation reward: 135.2\n",
      "episode: 159   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 659     evaluation reward: 132.85\n",
      "Training network\n",
      "Iteration 76: Policy loss: 0.000004. Value loss: 4.003625. Entropy: 1.790110.\n",
      "Iteration 77: Policy loss: -0.000050. Value loss: 4.087998. Entropy: 1.790146.\n",
      "Iteration 78: Policy loss: -0.000077. Value loss: 4.087513. Entropy: 1.790128.\n",
      "episode: 160   score: 245.0   memory length: 4096   epsilon: 1.0    steps: 956     evaluation reward: 133.2\n",
      "episode: 161   score: 215.0   memory length: 4096   epsilon: 1.0    steps: 789     evaluation reward: 134.25\n",
      "episode: 162   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 527     evaluation reward: 135.0\n",
      "episode: 163   score: 125.0   memory length: 4096   epsilon: 1.0    steps: 592     evaluation reward: 134.7\n",
      "episode: 164   score: 135.0   memory length: 4096   epsilon: 1.0    steps: 656     evaluation reward: 135.25\n",
      "episode: 165   score: 130.0   memory length: 4096   epsilon: 1.0    steps: 629     evaluation reward: 136.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 166   score: 80.0   memory length: 4096   epsilon: 1.0    steps: 596     evaluation reward: 135.8\n",
      "Training network\n",
      "Iteration 79: Policy loss: 0.000003. Value loss: 3.188288. Entropy: 1.790142.\n",
      "Iteration 80: Policy loss: -0.000013. Value loss: 3.156586. Entropy: 1.790143.\n",
      "Iteration 81: Policy loss: -0.000066. Value loss: 3.185999. Entropy: 1.790141.\n",
      "episode: 167   score: 180.0   memory length: 4096   epsilon: 1.0    steps: 704     evaluation reward: 136.25\n",
      "episode: 168   score: 160.0   memory length: 4096   epsilon: 1.0    steps: 830     evaluation reward: 136.6\n",
      "episode: 169   score: 170.0   memory length: 4096   epsilon: 1.0    steps: 896     evaluation reward: 137.55\n",
      "episode: 170   score: 215.0   memory length: 4096   epsilon: 1.0    steps: 876     evaluation reward: 138.5\n",
      "episode: 171   score: 180.0   memory length: 4096   epsilon: 1.0    steps: 691     evaluation reward: 138.95\n",
      "Training network\n",
      "Iteration 82: Policy loss: 0.000002. Value loss: 3.318778. Entropy: 1.790243.\n",
      "Iteration 83: Policy loss: -0.000036. Value loss: 3.296033. Entropy: 1.790226.\n",
      "Iteration 84: Policy loss: -0.000059. Value loss: 3.286072. Entropy: 1.790239.\n",
      "episode: 172   score: 20.0   memory length: 4096   epsilon: 1.0    steps: 571     evaluation reward: 138.35\n",
      "episode: 173   score: 210.0   memory length: 4096   epsilon: 1.0    steps: 875     evaluation reward: 138.3\n",
      "episode: 174   score: 160.0   memory length: 4096   epsilon: 1.0    steps: 648     evaluation reward: 138.55\n",
      "episode: 175   score: 135.0   memory length: 4096   epsilon: 1.0    steps: 645     evaluation reward: 138.8\n",
      "episode: 176   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 672     evaluation reward: 139.8\n",
      "episode: 177   score: 35.0   memory length: 4096   epsilon: 1.0    steps: 398     evaluation reward: 136.95\n",
      "episode: 178   score: 5.0   memory length: 4096   epsilon: 1.0    steps: 400     evaluation reward: 135.7\n",
      "Training network\n",
      "Iteration 85: Policy loss: 0.000004. Value loss: 2.512448. Entropy: 1.790123.\n",
      "Iteration 86: Policy loss: -0.000052. Value loss: 2.534911. Entropy: 1.790121.\n",
      "Iteration 87: Policy loss: -0.000064. Value loss: 2.543350. Entropy: 1.790108.\n",
      "episode: 179   score: 80.0   memory length: 4096   epsilon: 1.0    steps: 600     evaluation reward: 135.7\n",
      "episode: 180   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 704     evaluation reward: 135.8\n",
      "episode: 181   score: 265.0   memory length: 4096   epsilon: 1.0    steps: 1103     evaluation reward: 136.85\n",
      "episode: 182   score: 105.0   memory length: 4096   epsilon: 1.0    steps: 429     evaluation reward: 135.95\n",
      "episode: 183   score: 90.0   memory length: 4096   epsilon: 1.0    steps: 533     evaluation reward: 135.65\n",
      "Training network\n",
      "Iteration 88: Policy loss: -0.000002. Value loss: 2.955146. Entropy: 1.790172.\n",
      "Iteration 89: Policy loss: -0.000045. Value loss: 2.967316. Entropy: 1.790151.\n",
      "Iteration 90: Policy loss: -0.000105. Value loss: 2.985900. Entropy: 1.790171.\n",
      "episode: 184   score: 395.0   memory length: 4096   epsilon: 1.0    steps: 1158     evaluation reward: 139.05\n",
      "episode: 185   score: 95.0   memory length: 4096   epsilon: 1.0    steps: 536     evaluation reward: 138.25\n",
      "episode: 186   score: 415.0   memory length: 4096   epsilon: 1.0    steps: 1460     evaluation reward: 140.5\n",
      "episode: 187   score: 105.0   memory length: 4096   epsilon: 1.0    steps: 696     evaluation reward: 140.5\n",
      "episode: 188   score: 210.0   memory length: 4096   epsilon: 1.0    steps: 812     evaluation reward: 141.1\n",
      "Training network\n",
      "Iteration 91: Policy loss: 0.000006. Value loss: 4.078733. Entropy: 1.789901.\n",
      "Iteration 92: Policy loss: -0.000027. Value loss: 4.068139. Entropy: 1.789884.\n",
      "Iteration 93: Policy loss: -0.000053. Value loss: 4.017176. Entropy: 1.789873.\n",
      "episode: 189   score: 150.0   memory length: 4096   epsilon: 1.0    steps: 658     evaluation reward: 140.8\n",
      "episode: 190   score: 110.0   memory length: 4096   epsilon: 1.0    steps: 622     evaluation reward: 140.8\n",
      "episode: 191   score: 185.0   memory length: 4096   epsilon: 1.0    steps: 843     evaluation reward: 142.1\n",
      "episode: 192   score: 50.0   memory length: 4096   epsilon: 1.0    steps: 505     evaluation reward: 140.8\n",
      "episode: 193   score: 165.0   memory length: 4096   epsilon: 1.0    steps: 670     evaluation reward: 141.9\n",
      "episode: 194   score: 40.0   memory length: 4096   epsilon: 1.0    steps: 419     evaluation reward: 141.65\n",
      "episode: 195   score: 80.0   memory length: 4096   epsilon: 1.0    steps: 440     evaluation reward: 141.1\n",
      "Training network\n",
      "Iteration 94: Policy loss: -0.000001. Value loss: 2.947179. Entropy: 1.790004.\n",
      "Iteration 95: Policy loss: -0.000061. Value loss: 2.951631. Entropy: 1.789988.\n",
      "Iteration 96: Policy loss: -0.000123. Value loss: 2.945231. Entropy: 1.790019.\n",
      "episode: 196   score: 85.0   memory length: 4096   epsilon: 1.0    steps: 400     evaluation reward: 141.4\n",
      "episode: 197   score: 50.0   memory length: 4096   epsilon: 1.0    steps: 558     evaluation reward: 141.25\n",
      "episode: 198   score: 50.0   memory length: 4096   epsilon: 1.0    steps: 510     evaluation reward: 140.9\n",
      "episode: 199   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 759     evaluation reward: 140.55\n",
      "episode: 200   score: 190.0   memory length: 4096   epsilon: 1.0    steps: 1026     evaluation reward: 142.0\n",
      "episode: 201   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 533     evaluation reward: 139.1\n",
      "Training network\n",
      "Iteration 97: Policy loss: 0.000009. Value loss: 2.382773. Entropy: 1.790266.\n",
      "Iteration 98: Policy loss: -0.000046. Value loss: 2.390593. Entropy: 1.790272.\n",
      "Iteration 99: Policy loss: -0.000088. Value loss: 2.401373. Entropy: 1.790257.\n",
      "episode: 202   score: 130.0   memory length: 4096   epsilon: 1.0    steps: 608     evaluation reward: 139.6\n",
      "episode: 203   score: 285.0   memory length: 4096   epsilon: 1.0    steps: 1298     evaluation reward: 142.15\n",
      "episode: 204   score: 315.0   memory length: 4096   epsilon: 1.0    steps: 999     evaluation reward: 144.55\n",
      "episode: 205   score: 135.0   memory length: 4096   epsilon: 1.0    steps: 679     evaluation reward: 144.35\n",
      "episode: 206   score: 460.0   memory length: 4096   epsilon: 1.0    steps: 964     evaluation reward: 146.55\n",
      "Training network\n",
      "Iteration 100: Policy loss: -0.000005. Value loss: 4.547539. Entropy: 1.790026.\n",
      "Iteration 101: Policy loss: -0.000053. Value loss: 4.535718. Entropy: 1.790007.\n",
      "Iteration 102: Policy loss: -0.000051. Value loss: 4.484466. Entropy: 1.790009.\n",
      "episode: 207   score: 155.0   memory length: 4096   epsilon: 1.0    steps: 859     evaluation reward: 147.0\n",
      "episode: 208   score: 90.0   memory length: 4096   epsilon: 1.0    steps: 623     evaluation reward: 147.35\n",
      "episode: 209   score: 15.0   memory length: 4096   epsilon: 1.0    steps: 490     evaluation reward: 146.45\n",
      "episode: 210   score: 220.0   memory length: 4096   epsilon: 1.0    steps: 911     evaluation reward: 148.3\n",
      "episode: 211   score: 55.0   memory length: 4096   epsilon: 1.0    steps: 389     evaluation reward: 148.5\n",
      "episode: 212   score: 105.0   memory length: 4096   epsilon: 1.0    steps: 575     evaluation reward: 148.75\n",
      "Training network\n",
      "Iteration 103: Policy loss: 0.000000. Value loss: 2.731044. Entropy: 1.790051.\n",
      "Iteration 104: Policy loss: -0.000030. Value loss: 2.722397. Entropy: 1.790070.\n",
      "Iteration 105: Policy loss: -0.000081. Value loss: 2.735909. Entropy: 1.790053.\n",
      "episode: 213   score: 100.0   memory length: 4096   epsilon: 1.0    steps: 396     evaluation reward: 148.65\n",
      "episode: 214   score: 30.0   memory length: 4096   epsilon: 1.0    steps: 422     evaluation reward: 148.15\n",
      "episode: 215   score: 135.0   memory length: 4096   epsilon: 1.0    steps: 664     evaluation reward: 147.7\n",
      "episode: 216   score: 155.0   memory length: 4096   epsilon: 1.0    steps: 566     evaluation reward: 148.1\n",
      "episode: 217   score: 55.0   memory length: 4096   epsilon: 1.0    steps: 392     evaluation reward: 147.0\n",
      "episode: 218   score: 180.0   memory length: 4096   epsilon: 1.0    steps: 661     evaluation reward: 146.55\n",
      "episode: 219   score: 155.0   memory length: 4096   epsilon: 1.0    steps: 605     evaluation reward: 146.25\n",
      "Training network\n",
      "Iteration 106: Policy loss: 0.000008. Value loss: 3.019021. Entropy: 1.790086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 107: Policy loss: -0.000021. Value loss: 3.037101. Entropy: 1.790059.\n",
      "Iteration 108: Policy loss: -0.000047. Value loss: 3.029384. Entropy: 1.790076.\n",
      "episode: 220   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 806     evaluation reward: 145.05\n",
      "episode: 221   score: 50.0   memory length: 4096   epsilon: 1.0    steps: 446     evaluation reward: 144.3\n",
      "episode: 222   score: 180.0   memory length: 4096   epsilon: 1.0    steps: 789     evaluation reward: 144.55\n",
      "episode: 223   score: 165.0   memory length: 4096   epsilon: 1.0    steps: 808     evaluation reward: 142.8\n",
      "now time :  2018-12-19 11:41:12.387484\n",
      "episode: 224   score: 235.0   memory length: 4096   epsilon: 1.0    steps: 807     evaluation reward: 144.2\n",
      "episode: 225   score: 125.0   memory length: 4096   epsilon: 1.0    steps: 649     evaluation reward: 143.65\n",
      "Training network\n",
      "Iteration 109: Policy loss: 0.000002. Value loss: 3.287346. Entropy: 1.790174.\n",
      "Iteration 110: Policy loss: -0.000041. Value loss: 3.299054. Entropy: 1.790198.\n",
      "Iteration 111: Policy loss: -0.000067. Value loss: 3.306298. Entropy: 1.790188.\n",
      "episode: 226   score: 175.0   memory length: 4096   epsilon: 1.0    steps: 779     evaluation reward: 144.95\n",
      "episode: 227   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 697     evaluation reward: 144.85\n",
      "episode: 228   score: 165.0   memory length: 4096   epsilon: 1.0    steps: 855     evaluation reward: 145.3\n",
      "episode: 229   score: 125.0   memory length: 4096   epsilon: 1.0    steps: 633     evaluation reward: 145.5\n",
      "episode: 230   score: 440.0   memory length: 4096   epsilon: 1.0    steps: 939     evaluation reward: 148.5\n",
      "Training network\n",
      "Iteration 112: Policy loss: -0.000004. Value loss: 3.941218. Entropy: 1.790202.\n",
      "Iteration 113: Policy loss: -0.000028. Value loss: 4.003008. Entropy: 1.790160.\n",
      "Iteration 114: Policy loss: -0.000038. Value loss: 3.962476. Entropy: 1.790178.\n",
      "episode: 231   score: 230.0   memory length: 4096   epsilon: 1.0    steps: 949     evaluation reward: 149.7\n",
      "episode: 232   score: 110.0   memory length: 4096   epsilon: 1.0    steps: 710     evaluation reward: 150.15\n",
      "episode: 233   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 1069     evaluation reward: 150.9\n",
      "episode: 234   score: 135.0   memory length: 4096   epsilon: 1.0    steps: 737     evaluation reward: 147.5\n",
      "episode: 235   score: 190.0   memory length: 4096   epsilon: 1.0    steps: 667     evaluation reward: 148.95\n",
      "Training network\n",
      "Iteration 115: Policy loss: -0.000000. Value loss: 2.709630. Entropy: 1.790104.\n",
      "Iteration 116: Policy loss: -0.000016. Value loss: 2.717326. Entropy: 1.790109.\n",
      "Iteration 117: Policy loss: -0.000049. Value loss: 2.699261. Entropy: 1.790117.\n",
      "episode: 236   score: 225.0   memory length: 4096   epsilon: 1.0    steps: 868     evaluation reward: 150.3\n",
      "episode: 237   score: 110.0   memory length: 4096   epsilon: 1.0    steps: 626     evaluation reward: 150.0\n",
      "episode: 238   score: 155.0   memory length: 4096   epsilon: 1.0    steps: 726     evaluation reward: 150.2\n",
      "episode: 239   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 662     evaluation reward: 149.25\n",
      "episode: 240   score: 180.0   memory length: 4096   epsilon: 1.0    steps: 693     evaluation reward: 150.1\n",
      "episode: 241   score: 295.0   memory length: 4096   epsilon: 1.0    steps: 1020     evaluation reward: 152.3\n",
      "Training network\n",
      "Iteration 118: Policy loss: 0.000009. Value loss: 3.516016. Entropy: 1.790129.\n",
      "Iteration 119: Policy loss: -0.000011. Value loss: 3.487107. Entropy: 1.790128.\n",
      "Iteration 120: Policy loss: -0.000041. Value loss: 3.500834. Entropy: 1.790146.\n",
      "episode: 242   score: 80.0   memory length: 4096   epsilon: 1.0    steps: 493     evaluation reward: 152.9\n",
      "episode: 243   score: 485.0   memory length: 4096   epsilon: 1.0    steps: 1240     evaluation reward: 157.1\n",
      "episode: 244   score: 220.0   memory length: 4096   epsilon: 1.0    steps: 870     evaluation reward: 158.1\n",
      "episode: 245   score: 85.0   memory length: 4096   epsilon: 1.0    steps: 739     evaluation reward: 157.65\n",
      "episode: 246   score: 135.0   memory length: 4096   epsilon: 1.0    steps: 505     evaluation reward: 158.55\n",
      "Training network\n",
      "Iteration 121: Policy loss: 0.000003. Value loss: 4.157689. Entropy: 1.789972.\n",
      "Iteration 122: Policy loss: -0.000016. Value loss: 4.133095. Entropy: 1.789950.\n",
      "Iteration 123: Policy loss: -0.000079. Value loss: 4.109786. Entropy: 1.789954.\n",
      "episode: 247   score: 435.0   memory length: 4096   epsilon: 1.0    steps: 1059     evaluation reward: 161.65\n",
      "episode: 248   score: 30.0   memory length: 4096   epsilon: 1.0    steps: 461     evaluation reward: 160.6\n",
      "episode: 249   score: 520.0   memory length: 4096   epsilon: 1.0    steps: 1426     evaluation reward: 164.6\n",
      "episode: 250   score: 145.0   memory length: 4096   epsilon: 1.0    steps: 701     evaluation reward: 161.45\n",
      "episode: 251   score: 55.0   memory length: 4096   epsilon: 1.0    steps: 384     evaluation reward: 160.6\n",
      "Training network\n",
      "Iteration 124: Policy loss: 0.000008. Value loss: 4.555478. Entropy: 1.789755.\n",
      "Iteration 125: Policy loss: -0.000020. Value loss: 4.588744. Entropy: 1.789788.\n",
      "Iteration 126: Policy loss: -0.000053. Value loss: 4.516381. Entropy: 1.789798.\n",
      "episode: 252   score: 170.0   memory length: 4096   epsilon: 1.0    steps: 1083     evaluation reward: 160.2\n",
      "episode: 253   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 594     evaluation reward: 159.25\n",
      "episode: 254   score: 90.0   memory length: 4096   epsilon: 1.0    steps: 410     evaluation reward: 159.35\n",
      "episode: 255   score: 90.0   memory length: 4096   epsilon: 1.0    steps: 672     evaluation reward: 159.15\n",
      "episode: 256   score: 155.0   memory length: 4096   epsilon: 1.0    steps: 624     evaluation reward: 159.85\n",
      "episode: 257   score: 75.0   memory length: 4096   epsilon: 1.0    steps: 568     evaluation reward: 156.8\n",
      "episode: 258   score: 155.0   memory length: 4096   epsilon: 1.0    steps: 401     evaluation reward: 156.25\n",
      "Training network\n",
      "Iteration 127: Policy loss: -0.000004. Value loss: 2.854997. Entropy: 1.790088.\n",
      "Iteration 128: Policy loss: -0.000036. Value loss: 2.856122. Entropy: 1.790106.\n",
      "Iteration 129: Policy loss: -0.000091. Value loss: 2.848223. Entropy: 1.790071.\n",
      "episode: 259   score: 210.0   memory length: 4096   epsilon: 1.0    steps: 799     evaluation reward: 157.15\n",
      "episode: 260   score: 305.0   memory length: 4096   epsilon: 1.0    steps: 1124     evaluation reward: 157.75\n",
      "episode: 261   score: 60.0   memory length: 4096   epsilon: 1.0    steps: 381     evaluation reward: 156.2\n",
      "episode: 262   score: 405.0   memory length: 4096   epsilon: 1.0    steps: 1401     evaluation reward: 159.05\n",
      "episode: 263   score: 55.0   memory length: 4096   epsilon: 1.0    steps: 447     evaluation reward: 158.35\n",
      "Training network\n",
      "Iteration 130: Policy loss: 0.000002. Value loss: 3.710586. Entropy: 1.789764.\n",
      "Iteration 131: Policy loss: -0.000037. Value loss: 3.687450. Entropy: 1.789751.\n",
      "Iteration 132: Policy loss: -0.000099. Value loss: 3.710536. Entropy: 1.789767.\n",
      "episode: 264   score: 225.0   memory length: 4096   epsilon: 1.0    steps: 835     evaluation reward: 159.25\n",
      "episode: 265   score: 105.0   memory length: 4096   epsilon: 1.0    steps: 633     evaluation reward: 159.0\n",
      "episode: 266   score: 80.0   memory length: 4096   epsilon: 1.0    steps: 709     evaluation reward: 159.0\n",
      "episode: 267   score: 140.0   memory length: 4096   epsilon: 1.0    steps: 658     evaluation reward: 158.6\n",
      "episode: 268   score: 180.0   memory length: 4096   epsilon: 1.0    steps: 952     evaluation reward: 158.8\n",
      "Training network\n",
      "Iteration 133: Policy loss: -0.000005. Value loss: 3.117847. Entropy: 1.790187.\n",
      "Iteration 134: Policy loss: -0.000061. Value loss: 3.150949. Entropy: 1.790190.\n",
      "Iteration 135: Policy loss: -0.000125. Value loss: 3.136230. Entropy: 1.790176.\n",
      "episode: 269   score: 180.0   memory length: 4096   epsilon: 1.0    steps: 547     evaluation reward: 158.9\n",
      "episode: 270   score: 155.0   memory length: 4096   epsilon: 1.0    steps: 753     evaluation reward: 158.3\n",
      "episode: 271   score: 40.0   memory length: 4096   epsilon: 1.0    steps: 396     evaluation reward: 156.9\n",
      "episode: 272   score: 165.0   memory length: 4096   epsilon: 1.0    steps: 687     evaluation reward: 158.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 273   score: 155.0   memory length: 4096   epsilon: 1.0    steps: 560     evaluation reward: 157.8\n",
      "episode: 274   score: 410.0   memory length: 4096   epsilon: 1.0    steps: 897     evaluation reward: 160.3\n",
      "episode: 275   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 622     evaluation reward: 160.15\n",
      "Training network\n",
      "Iteration 136: Policy loss: 0.000002. Value loss: 3.960633. Entropy: 1.790072.\n",
      "Iteration 137: Policy loss: -0.000029. Value loss: 3.989061. Entropy: 1.790067.\n",
      "Iteration 138: Policy loss: -0.000081. Value loss: 3.990504. Entropy: 1.790076.\n",
      "episode: 276   score: 65.0   memory length: 4096   epsilon: 1.0    steps: 704     evaluation reward: 159.6\n",
      "episode: 277   score: 180.0   memory length: 4096   epsilon: 1.0    steps: 697     evaluation reward: 161.05\n",
      "episode: 278   score: 225.0   memory length: 4096   epsilon: 1.0    steps: 1150     evaluation reward: 163.25\n",
      "episode: 279   score: 55.0   memory length: 4096   epsilon: 1.0    steps: 517     evaluation reward: 163.0\n",
      "episode: 280   score: 80.0   memory length: 4096   epsilon: 1.0    steps: 566     evaluation reward: 162.6\n",
      "episode: 281   score: 90.0   memory length: 4096   epsilon: 1.0    steps: 470     evaluation reward: 160.85\n",
      "Training network\n",
      "Iteration 139: Policy loss: -0.000001. Value loss: 2.470774. Entropy: 1.789935.\n",
      "Iteration 140: Policy loss: -0.000034. Value loss: 2.473251. Entropy: 1.789953.\n",
      "Iteration 141: Policy loss: -0.000087. Value loss: 2.471146. Entropy: 1.789935.\n",
      "episode: 282   score: 110.0   memory length: 4096   epsilon: 1.0    steps: 654     evaluation reward: 160.9\n",
      "episode: 283   score: 120.0   memory length: 4096   epsilon: 1.0    steps: 761     evaluation reward: 161.2\n",
      "episode: 284   score: 165.0   memory length: 4096   epsilon: 1.0    steps: 617     evaluation reward: 158.9\n",
      "episode: 285   score: 135.0   memory length: 4096   epsilon: 1.0    steps: 849     evaluation reward: 159.3\n",
      "episode: 286   score: 30.0   memory length: 4096   epsilon: 1.0    steps: 636     evaluation reward: 155.45\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        if render_breakout:\n",
    "            env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        action, value = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        #r = np.clip(reward, -1, 1)\n",
    "        r = reward\n",
    "        \n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state, value, 0, 0)\n",
    "        # Start training after random sample generation\n",
    "        if(frame % train_frame == 0):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network\n",
    "            agent.update_target_net()\n",
    "        score += r\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > 40 and len(evaluation_reward) > 700:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn\")\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
