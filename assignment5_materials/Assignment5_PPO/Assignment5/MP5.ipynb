{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "from env import GameEnv\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "envs = []\n",
    "for i in range(num_envs):\n",
    "    envs.append(GameEnv('SpaceInvadersDeterministic-v4'))\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = envs[0].life\n",
    "state_size = envs[0].observation_space.shape\n",
    "action_size = envs[0].action_space.n\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "torch.save(agent.policy_net.state_dict(), \"./save_model/spaceinvaders_ppo_best\")\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n",
    "reset_max = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'envs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-77aed6047ce3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvis_env_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvis_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvis_env_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmax_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'envs' is not defined"
     ]
    }
   ],
   "source": [
    "vis_env_idx = 0\n",
    "vis_env = envs[vis_env_idx]\n",
    "e = 0\n",
    "frame = 0\n",
    "max_eval = -np.inf\n",
    "reset_count = 0\n",
    "\n",
    "while (frame < 10000000):\n",
    "    step = 0\n",
    "    assert(num_envs * env_mem_size == train_frame)\n",
    "    frame_next_vals = []\n",
    "    for i in range(num_envs):\n",
    "        env = envs[i]\n",
    "        #history = env.history\n",
    "        #life = env.life\n",
    "        #state, reward, done, info = [env.state, env.reward, env.done, env.info]\n",
    "        for j in range(env_mem_size):\n",
    "            step += 1\n",
    "            frame += 1\n",
    "            \n",
    "            curr_state = env.history[HISTORY_SIZE-1,:,:]\n",
    "            action, value = agent.get_action(np.float32(env.history[:HISTORY_SIZE,:,:]) / 255.)\n",
    "            \n",
    "            next_state, env.reward, env.done, env.info = env.step(action)\n",
    "            \n",
    "            if (i == vis_env_idx):\n",
    "                vis_env._env.render()\n",
    "            \n",
    "            frame_next_state = get_frame(next_state)\n",
    "            env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "            terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "            \n",
    "            env.life = env.info['ale.lives']\n",
    "            r = env.reward\n",
    "            \n",
    "            agent.memory.push(i, deepcopy(curr_state), action, r, terminal_state, value, 0, 0)\n",
    "            if (j == env_mem_size-1):\n",
    "                _, frame_next_val = agent.get_action(np.float32(env.history[1:,:,:]) / 255.)\n",
    "                frame_next_vals.append(frame_next_val)\n",
    "            env.score += r\n",
    "            env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "            \n",
    "            if (env.done):\n",
    "                if (e % 50 == 0):\n",
    "                    print('now time : ', datetime.now())\n",
    "                    rewards.append(np.mean(evaluation_reward))\n",
    "                    episodes.append(e)\n",
    "                    pylab.plot(episodes, rewards, 'b')\n",
    "                    pylab.savefig(\"./save_graph/spaceinvaders_ppo.png\")\n",
    "                    torch.save(agent.policy_net, \"./save_model/spaceinvaders_ppo\")\n",
    "                    \n",
    "                    if np.mean(evaluation_reward) > max_eval:\n",
    "                        torch.save(agent.policy_net.state_dict(), \"./save_model/spaceinvaders_ppo_best\")\n",
    "                        max_eval = float(np.mean(evaluation_reward))\n",
    "                        reset_count = 0\n",
    "                    elif e > 5000:\n",
    "                        reset_count += 1\n",
    "                        \"\"\"\n",
    "                        if (reset_count == reset_max):\n",
    "                            print(\"Training went nowhere, starting again at best model\")\n",
    "                            agent.policy_net.load_state_dict(torch.load(\"./save_model/spaceinvaders_ppo_best\"))\n",
    "                            agent.update_target_net()\n",
    "                            reset_count = 0\n",
    "                        \"\"\"\n",
    "                e += 1\n",
    "                evaluation_reward.append(env.score)\n",
    "                print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "                \n",
    "                env.done = False\n",
    "                env.score = 0\n",
    "                env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "                env.state = env.reset()\n",
    "                env.life = number_lives\n",
    "                get_init_state(env.history, env.state)\n",
    "                \n",
    "                \n",
    "                \n",
    "    agent.train_policy_net(frame, frame_next_vals)\n",
    "    agent.update_target_net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/spaceinvaders_ppo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " ------- STARTING TRAINING FOR SpaceInvaders-v0 ------- \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determing min/max rewards of environment\n",
      "Min: 0. Max: 200.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],1)[0].unsqueeze(1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000250. clip: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:260: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:261: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:262: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Policy loss: -0.003272. Value loss: 0.026312. Entropy: 1.384223.\n",
      "Iteration 2: Policy loss: -0.004937. Value loss: 0.027108. Entropy: 1.382908.\n",
      "Iteration 3: Policy loss: -0.003928. Value loss: 0.026711. Entropy: 1.383878.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 4: Policy loss: 0.002093. Value loss: 0.144672. Entropy: 1.380014.\n",
      "Iteration 5: Policy loss: -0.002442. Value loss: 0.133692. Entropy: 1.383995.\n",
      "Iteration 6: Policy loss: -0.001728. Value loss: 0.123989. Entropy: 1.384504.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 7: Policy loss: 0.000935. Value loss: 0.086329. Entropy: 1.384142.\n",
      "Iteration 8: Policy loss: 0.000656. Value loss: 0.068617. Entropy: 1.384951.\n",
      "Iteration 9: Policy loss: -0.002952. Value loss: 0.057422. Entropy: 1.382775.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 10: Policy loss: 0.001547. Value loss: 0.448342. Entropy: 1.382997.\n",
      "Iteration 11: Policy loss: 0.001572. Value loss: 0.375268. Entropy: 1.382617.\n",
      "Iteration 12: Policy loss: 0.000392. Value loss: 0.285408. Entropy: 1.381740.\n",
      "now time :  2019-03-05 20:32:03.443871\n",
      "episode: 1   score: 50.0  epsilon: 1.0    steps: 112  evaluation reward: 50.0\n",
      "episode: 2   score: 135.0  epsilon: 1.0    steps: 152  evaluation reward: 92.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3   score: 80.0  epsilon: 1.0    steps: 400  evaluation reward: 88.33333333333333\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 13: Policy loss: 0.000025. Value loss: 0.116523. Entropy: 1.378933.\n",
      "Iteration 14: Policy loss: -0.001092. Value loss: 0.064021. Entropy: 1.380551.\n",
      "Iteration 15: Policy loss: -0.007435. Value loss: 0.051885. Entropy: 1.379807.\n",
      "episode: 4   score: 80.0  epsilon: 1.0    steps: 160  evaluation reward: 86.25\n",
      "episode: 5   score: 190.0  epsilon: 1.0    steps: 360  evaluation reward: 107.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 16: Policy loss: -0.000568. Value loss: 0.107463. Entropy: 1.377571.\n",
      "Iteration 17: Policy loss: -0.005650. Value loss: 0.074627. Entropy: 1.381477.\n",
      "Iteration 18: Policy loss: -0.006085. Value loss: 0.065890. Entropy: 1.380943.\n",
      "episode: 6   score: 105.0  epsilon: 1.0    steps: 1024  evaluation reward: 106.66666666666667\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 19: Policy loss: 0.000387. Value loss: 0.146393. Entropy: 1.372779.\n",
      "Iteration 20: Policy loss: -0.002937. Value loss: 0.096767. Entropy: 1.373880.\n",
      "Iteration 21: Policy loss: -0.006680. Value loss: 0.075268. Entropy: 1.373624.\n",
      "episode: 7   score: 215.0  epsilon: 1.0    steps: 96  evaluation reward: 122.14285714285714\n",
      "episode: 8   score: 60.0  epsilon: 1.0    steps: 112  evaluation reward: 114.375\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 22: Policy loss: 0.001257. Value loss: 0.131669. Entropy: 1.378044.\n",
      "Iteration 23: Policy loss: -0.004552. Value loss: 0.094864. Entropy: 1.369487.\n",
      "Iteration 24: Policy loss: -0.006053. Value loss: 0.076294. Entropy: 1.373375.\n",
      "episode: 9   score: 130.0  epsilon: 1.0    steps: 72  evaluation reward: 116.11111111111111\n",
      "episode: 10   score: 50.0  epsilon: 1.0    steps: 344  evaluation reward: 109.5\n",
      "episode: 11   score: 50.0  epsilon: 1.0    steps: 392  evaluation reward: 104.0909090909091\n",
      "episode: 12   score: 405.0  epsilon: 1.0    steps: 880  evaluation reward: 129.16666666666666\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 25: Policy loss: 0.000923. Value loss: 0.106072. Entropy: 1.358358.\n",
      "Iteration 26: Policy loss: -0.003809. Value loss: 0.073317. Entropy: 1.360296.\n",
      "Iteration 27: Policy loss: -0.002395. Value loss: 0.060417. Entropy: 1.365525.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 28: Policy loss: 0.003425. Value loss: 0.097212. Entropy: 1.368853.\n",
      "Iteration 29: Policy loss: -0.000163. Value loss: 0.069128. Entropy: 1.360344.\n",
      "Iteration 30: Policy loss: -0.002217. Value loss: 0.062731. Entropy: 1.359540.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 31: Policy loss: 0.002549. Value loss: 0.153585. Entropy: 1.374014.\n",
      "Iteration 32: Policy loss: -0.001512. Value loss: 0.105987. Entropy: 1.372624.\n",
      "Iteration 33: Policy loss: -0.004508. Value loss: 0.088050. Entropy: 1.373698.\n",
      "episode: 13   score: 160.0  epsilon: 1.0    steps: 600  evaluation reward: 131.53846153846155\n",
      "episode: 14   score: 210.0  epsilon: 1.0    steps: 720  evaluation reward: 137.14285714285714\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 34: Policy loss: 0.001004. Value loss: 0.128361. Entropy: 1.357402.\n",
      "Iteration 35: Policy loss: -0.003367. Value loss: 0.092781. Entropy: 1.358797.\n",
      "Iteration 36: Policy loss: -0.003456. Value loss: 0.074425. Entropy: 1.364721.\n",
      "episode: 15   score: 65.0  epsilon: 1.0    steps: 152  evaluation reward: 132.33333333333334\n",
      "episode: 16   score: 145.0  epsilon: 1.0    steps: 632  evaluation reward: 133.125\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 37: Policy loss: 0.001316. Value loss: 0.179453. Entropy: 1.358384.\n",
      "Iteration 38: Policy loss: -0.001874. Value loss: 0.123505. Entropy: 1.346912.\n",
      "Iteration 39: Policy loss: -0.004664. Value loss: 0.094477. Entropy: 1.346794.\n",
      "episode: 17   score: 120.0  epsilon: 1.0    steps: 288  evaluation reward: 132.35294117647058\n",
      "episode: 18   score: 75.0  epsilon: 1.0    steps: 440  evaluation reward: 129.16666666666666\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 40: Policy loss: 0.001058. Value loss: 0.106503. Entropy: 1.348688.\n",
      "Iteration 41: Policy loss: -0.001156. Value loss: 0.072746. Entropy: 1.342734.\n",
      "Iteration 42: Policy loss: -0.003672. Value loss: 0.059315. Entropy: 1.341644.\n",
      "episode: 19   score: 80.0  epsilon: 1.0    steps: 312  evaluation reward: 126.57894736842105\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 43: Policy loss: 0.000638. Value loss: 0.172241. Entropy: 1.331409.\n",
      "Iteration 44: Policy loss: -0.001841. Value loss: 0.122961. Entropy: 1.327702.\n",
      "Iteration 45: Policy loss: -0.002757. Value loss: 0.100608. Entropy: 1.321246.\n",
      "episode: 20   score: 80.0  epsilon: 1.0    steps: 168  evaluation reward: 124.25\n",
      "episode: 21   score: 30.0  epsilon: 1.0    steps: 360  evaluation reward: 119.76190476190476\n",
      "episode: 22   score: 280.0  epsilon: 1.0    steps: 752  evaluation reward: 127.04545454545455\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 46: Policy loss: 0.000875. Value loss: 0.111766. Entropy: 1.344514.\n",
      "Iteration 47: Policy loss: -0.003060. Value loss: 0.067827. Entropy: 1.336673.\n",
      "Iteration 48: Policy loss: -0.000720. Value loss: 0.056713. Entropy: 1.342186.\n",
      "episode: 23   score: 95.0  epsilon: 1.0    steps: 760  evaluation reward: 125.65217391304348\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 49: Policy loss: -0.000223. Value loss: 0.159802. Entropy: 1.359140.\n",
      "Iteration 50: Policy loss: -0.003414. Value loss: 0.113362. Entropy: 1.359727.\n",
      "Iteration 51: Policy loss: -0.005549. Value loss: 0.090170. Entropy: 1.360185.\n",
      "episode: 24   score: 90.0  epsilon: 1.0    steps: 336  evaluation reward: 124.16666666666667\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 52: Policy loss: 0.000638. Value loss: 0.161553. Entropy: 1.363706.\n",
      "Iteration 53: Policy loss: -0.002950. Value loss: 0.119296. Entropy: 1.359338.\n",
      "Iteration 54: Policy loss: -0.007327. Value loss: 0.107468. Entropy: 1.359037.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 55: Policy loss: 0.001797. Value loss: 0.110161. Entropy: 1.369582.\n",
      "Iteration 56: Policy loss: -0.001645. Value loss: 0.074990. Entropy: 1.367672.\n",
      "Iteration 57: Policy loss: -0.001200. Value loss: 0.065150. Entropy: 1.368126.\n",
      "episode: 25   score: 45.0  epsilon: 1.0    steps: 152  evaluation reward: 121.0\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 58: Policy loss: -0.001187. Value loss: 0.051521. Entropy: 1.367672.\n",
      "Iteration 59: Policy loss: -0.006933. Value loss: 0.033015. Entropy: 1.360788.\n",
      "Iteration 60: Policy loss: -0.008605. Value loss: 0.025104. Entropy: 1.357016.\n",
      "episode: 26   score: 180.0  epsilon: 1.0    steps: 88  evaluation reward: 123.26923076923077\n",
      "episode: 27   score: 210.0  epsilon: 1.0    steps: 768  evaluation reward: 126.48148148148148\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 61: Policy loss: 0.000189. Value loss: 0.055364. Entropy: 1.358659.\n",
      "Iteration 62: Policy loss: -0.002477. Value loss: 0.033241. Entropy: 1.350055.\n",
      "Iteration 63: Policy loss: -0.003884. Value loss: 0.028183. Entropy: 1.350820.\n",
      "episode: 28   score: 105.0  epsilon: 1.0    steps: 616  evaluation reward: 125.71428571428571\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 64: Policy loss: 0.000937. Value loss: 0.134188. Entropy: 1.347185.\n",
      "Iteration 65: Policy loss: -0.000792. Value loss: 0.092340. Entropy: 1.356088.\n",
      "Iteration 66: Policy loss: -0.006753. Value loss: 0.071451. Entropy: 1.347511.\n",
      "episode: 29   score: 110.0  epsilon: 1.0    steps: 984  evaluation reward: 125.17241379310344\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 67: Policy loss: 0.005525. Value loss: 0.440543. Entropy: 1.352322.\n",
      "Iteration 68: Policy loss: -0.002013. Value loss: 0.312509. Entropy: 1.338541.\n",
      "Iteration 69: Policy loss: -0.002224. Value loss: 0.252056. Entropy: 1.340304.\n",
      "episode: 30   score: 185.0  epsilon: 1.0    steps: 80  evaluation reward: 127.16666666666667\n",
      "episode: 31   score: 165.0  epsilon: 1.0    steps: 792  evaluation reward: 128.38709677419354\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 70: Policy loss: 0.001206. Value loss: 0.059304. Entropy: 1.319319.\n",
      "Iteration 71: Policy loss: -0.002085. Value loss: 0.036082. Entropy: 1.329631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72: Policy loss: -0.006011. Value loss: 0.029801. Entropy: 1.324212.\n",
      "episode: 32   score: 80.0  epsilon: 1.0    steps: 656  evaluation reward: 126.875\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 73: Policy loss: -0.001540. Value loss: 0.178080. Entropy: 1.324938.\n",
      "Iteration 74: Policy loss: -0.005811. Value loss: 0.105113. Entropy: 1.315156.\n",
      "Iteration 75: Policy loss: -0.006423. Value loss: 0.084247. Entropy: 1.324882.\n",
      "episode: 33   score: 180.0  epsilon: 1.0    steps: 128  evaluation reward: 128.4848484848485\n",
      "episode: 34   score: 425.0  epsilon: 1.0    steps: 840  evaluation reward: 137.2058823529412\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 76: Policy loss: -0.000492. Value loss: 0.077550. Entropy: 1.339006.\n",
      "Iteration 77: Policy loss: -0.002154. Value loss: 0.045066. Entropy: 1.336865.\n",
      "Iteration 78: Policy loss: -0.002409. Value loss: 0.034077. Entropy: 1.334661.\n",
      "episode: 35   score: 120.0  epsilon: 1.0    steps: 328  evaluation reward: 136.71428571428572\n",
      "episode: 36   score: 30.0  epsilon: 1.0    steps: 456  evaluation reward: 133.75\n",
      "episode: 37   score: 185.0  epsilon: 1.0    steps: 736  evaluation reward: 135.13513513513513\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 79: Policy loss: 0.001015. Value loss: 0.119030. Entropy: 1.333799.\n",
      "Iteration 80: Policy loss: -0.002192. Value loss: 0.076548. Entropy: 1.315515.\n",
      "Iteration 81: Policy loss: -0.006646. Value loss: 0.063627. Entropy: 1.320782.\n",
      "episode: 38   score: 155.0  epsilon: 1.0    steps: 816  evaluation reward: 135.6578947368421\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 82: Policy loss: 0.000435. Value loss: 0.170886. Entropy: 1.321918.\n",
      "Iteration 83: Policy loss: -0.002072. Value loss: 0.128860. Entropy: 1.323342.\n",
      "Iteration 84: Policy loss: -0.002245. Value loss: 0.102269. Entropy: 1.319542.\n",
      "episode: 39   score: 335.0  epsilon: 1.0    steps: 632  evaluation reward: 140.76923076923077\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 85: Policy loss: -0.002519. Value loss: 0.311456. Entropy: 1.312433.\n",
      "Iteration 86: Policy loss: -0.003192. Value loss: 0.262429. Entropy: 1.314150.\n",
      "Iteration 87: Policy loss: -0.005998. Value loss: 0.236406. Entropy: 1.302432.\n",
      "episode: 40   score: 305.0  epsilon: 1.0    steps: 896  evaluation reward: 144.875\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 88: Policy loss: 0.000413. Value loss: 0.755857. Entropy: 1.319711.\n",
      "Iteration 89: Policy loss: -0.000684. Value loss: 0.587310. Entropy: 1.308824.\n",
      "Iteration 90: Policy loss: -0.005309. Value loss: 0.492288. Entropy: 1.309219.\n",
      "episode: 41   score: 15.0  epsilon: 1.0    steps: 80  evaluation reward: 141.70731707317074\n",
      "episode: 42   score: 210.0  epsilon: 1.0    steps: 656  evaluation reward: 143.33333333333334\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 91: Policy loss: 0.005244. Value loss: 0.179168. Entropy: 1.313820.\n",
      "Iteration 92: Policy loss: -0.002405. Value loss: 0.116133. Entropy: 1.303069.\n",
      "Iteration 93: Policy loss: -0.004310. Value loss: 0.088125. Entropy: 1.304188.\n",
      "episode: 43   score: 135.0  epsilon: 1.0    steps: 48  evaluation reward: 143.13953488372093\n",
      "episode: 44   score: 120.0  epsilon: 1.0    steps: 648  evaluation reward: 142.61363636363637\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 94: Policy loss: 0.002420. Value loss: 0.165012. Entropy: 1.321780.\n",
      "Iteration 95: Policy loss: -0.001669. Value loss: 0.100919. Entropy: 1.290683.\n",
      "Iteration 96: Policy loss: -0.005272. Value loss: 0.082835. Entropy: 1.292017.\n",
      "episode: 45   score: 265.0  epsilon: 1.0    steps: 888  evaluation reward: 145.33333333333334\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 97: Policy loss: -0.000057. Value loss: 0.128067. Entropy: 1.328939.\n",
      "Iteration 98: Policy loss: -0.003644. Value loss: 0.067112. Entropy: 1.320145.\n",
      "Iteration 99: Policy loss: -0.004695. Value loss: 0.052228. Entropy: 1.326619.\n",
      "episode: 46   score: 135.0  epsilon: 1.0    steps: 712  evaluation reward: 145.1086956521739\n",
      "episode: 47   score: 130.0  epsilon: 1.0    steps: 848  evaluation reward: 144.7872340425532\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 100: Policy loss: 0.000030. Value loss: 0.207592. Entropy: 1.341723.\n",
      "Iteration 101: Policy loss: -0.000692. Value loss: 0.141732. Entropy: 1.333742.\n",
      "Iteration 102: Policy loss: -0.001381. Value loss: 0.111246. Entropy: 1.336920.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 103: Policy loss: 0.001652. Value loss: 0.128005. Entropy: 1.343791.\n",
      "Iteration 104: Policy loss: -0.003942. Value loss: 0.076752. Entropy: 1.341903.\n",
      "Iteration 105: Policy loss: -0.010842. Value loss: 0.061504. Entropy: 1.336904.\n",
      "episode: 48   score: 140.0  epsilon: 1.0    steps: 200  evaluation reward: 144.6875\n",
      "episode: 49   score: 510.0  epsilon: 1.0    steps: 224  evaluation reward: 152.14285714285714\n",
      "episode: 50   score: 45.0  epsilon: 1.0    steps: 440  evaluation reward: 150.0\n",
      "now time :  2019-03-05 20:33:51.559789\n",
      "episode: 51   score: 135.0  epsilon: 1.0    steps: 808  evaluation reward: 149.7058823529412\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 106: Policy loss: 0.001974. Value loss: 0.099873. Entropy: 1.339957.\n",
      "Iteration 107: Policy loss: -0.002978. Value loss: 0.072791. Entropy: 1.329201.\n",
      "Iteration 108: Policy loss: -0.006095. Value loss: 0.062387. Entropy: 1.326672.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 109: Policy loss: 0.000422. Value loss: 0.161865. Entropy: 1.330352.\n",
      "Iteration 110: Policy loss: -0.001173. Value loss: 0.092061. Entropy: 1.319856.\n",
      "Iteration 111: Policy loss: -0.005923. Value loss: 0.074762. Entropy: 1.318053.\n",
      "episode: 52   score: 75.0  epsilon: 1.0    steps: 160  evaluation reward: 148.26923076923077\n",
      "episode: 53   score: 150.0  epsilon: 1.0    steps: 912  evaluation reward: 148.30188679245282\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 112: Policy loss: 0.000124. Value loss: 0.130860. Entropy: 1.317119.\n",
      "Iteration 113: Policy loss: -0.006009. Value loss: 0.086891. Entropy: 1.318251.\n",
      "Iteration 114: Policy loss: -0.005938. Value loss: 0.065595. Entropy: 1.311175.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 115: Policy loss: -0.002429. Value loss: 0.103438. Entropy: 1.328157.\n",
      "Iteration 116: Policy loss: -0.005279. Value loss: 0.058499. Entropy: 1.328692.\n",
      "Iteration 117: Policy loss: -0.006305. Value loss: 0.046084. Entropy: 1.325947.\n",
      "episode: 54   score: 90.0  epsilon: 1.0    steps: 352  evaluation reward: 147.22222222222223\n",
      "episode: 55   score: 125.0  epsilon: 1.0    steps: 464  evaluation reward: 146.8181818181818\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 118: Policy loss: 0.003218. Value loss: 0.309761. Entropy: 1.318714.\n",
      "Iteration 119: Policy loss: -0.002171. Value loss: 0.243534. Entropy: 1.327163.\n",
      "Iteration 120: Policy loss: -0.007516. Value loss: 0.188546. Entropy: 1.326578.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 121: Policy loss: -0.000461. Value loss: 0.163705. Entropy: 1.329810.\n",
      "Iteration 122: Policy loss: -0.003360. Value loss: 0.090140. Entropy: 1.328791.\n",
      "Iteration 123: Policy loss: -0.005437. Value loss: 0.071922. Entropy: 1.326034.\n",
      "episode: 56   score: 465.0  epsilon: 1.0    steps: 560  evaluation reward: 152.5\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 124: Policy loss: -0.001807. Value loss: 0.228306. Entropy: 1.341709.\n",
      "Iteration 125: Policy loss: -0.005604. Value loss: 0.122272. Entropy: 1.338409.\n",
      "Iteration 126: Policy loss: -0.008062. Value loss: 0.081946. Entropy: 1.342702.\n",
      "episode: 57   score: 110.0  epsilon: 1.0    steps: 608  evaluation reward: 151.75438596491227\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 127: Policy loss: 0.004066. Value loss: 0.453300. Entropy: 1.329998.\n",
      "Iteration 128: Policy loss: 0.000323. Value loss: 0.273939. Entropy: 1.343042.\n",
      "Iteration 129: Policy loss: 0.001936. Value loss: 0.211123. Entropy: 1.337317.\n",
      "episode: 58   score: 365.0  epsilon: 1.0    steps: 192  evaluation reward: 155.43103448275863\n",
      "episode: 59   score: 135.0  epsilon: 1.0    steps: 328  evaluation reward: 155.08474576271186\n",
      "episode: 60   score: 500.0  epsilon: 1.0    steps: 880  evaluation reward: 160.83333333333334\n",
      "Training network. lr: 0.000249. clip: 0.099696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 130: Policy loss: 0.000958. Value loss: 0.139184. Entropy: 1.327122.\n",
      "Iteration 131: Policy loss: -0.003079. Value loss: 0.077704. Entropy: 1.333090.\n",
      "Iteration 132: Policy loss: -0.004087. Value loss: 0.063457. Entropy: 1.323229.\n",
      "episode: 61   score: 240.0  epsilon: 1.0    steps: 704  evaluation reward: 162.13114754098362\n",
      "episode: 62   score: 230.0  epsilon: 1.0    steps: 1000  evaluation reward: 163.2258064516129\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 133: Policy loss: 0.001340. Value loss: 0.209012. Entropy: 1.335206.\n",
      "Iteration 134: Policy loss: -0.006557. Value loss: 0.127120. Entropy: 1.313879.\n",
      "Iteration 135: Policy loss: -0.009323. Value loss: 0.101249. Entropy: 1.314960.\n",
      "episode: 63   score: 70.0  epsilon: 1.0    steps: 400  evaluation reward: 161.74603174603175\n",
      "episode: 64   score: 165.0  epsilon: 1.0    steps: 464  evaluation reward: 161.796875\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 136: Policy loss: 0.000553. Value loss: 0.113796. Entropy: 1.299657.\n",
      "Iteration 137: Policy loss: -0.000931. Value loss: 0.062700. Entropy: 1.300719.\n",
      "Iteration 138: Policy loss: -0.005779. Value loss: 0.054214. Entropy: 1.307429.\n",
      "episode: 65   score: 25.0  epsilon: 1.0    steps: 760  evaluation reward: 159.69230769230768\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 139: Policy loss: 0.000509. Value loss: 0.151016. Entropy: 1.304209.\n",
      "Iteration 140: Policy loss: -0.001841. Value loss: 0.087300. Entropy: 1.310075.\n",
      "Iteration 141: Policy loss: -0.004446. Value loss: 0.068302. Entropy: 1.307153.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 142: Policy loss: -0.001097. Value loss: 0.123854. Entropy: 1.301182.\n",
      "Iteration 143: Policy loss: -0.007834. Value loss: 0.062365. Entropy: 1.304718.\n",
      "Iteration 144: Policy loss: -0.009636. Value loss: 0.056245. Entropy: 1.305704.\n",
      "episode: 66   score: 120.0  epsilon: 1.0    steps: 720  evaluation reward: 159.0909090909091\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 145: Policy loss: -0.003312. Value loss: 0.154126. Entropy: 1.306793.\n",
      "Iteration 146: Policy loss: -0.005952. Value loss: 0.081399. Entropy: 1.299917.\n",
      "Iteration 147: Policy loss: -0.011374. Value loss: 0.068864. Entropy: 1.301669.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 148: Policy loss: 0.002490. Value loss: 0.339152. Entropy: 1.314418.\n",
      "Iteration 149: Policy loss: -0.000156. Value loss: 0.222411. Entropy: 1.301299.\n",
      "Iteration 150: Policy loss: 0.000801. Value loss: 0.173312. Entropy: 1.298782.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 151: Policy loss: 0.000712. Value loss: 0.193730. Entropy: 1.298420.\n",
      "Iteration 152: Policy loss: -0.001011. Value loss: 0.099599. Entropy: 1.312411.\n",
      "Iteration 153: Policy loss: -0.004589. Value loss: 0.074505. Entropy: 1.299099.\n",
      "episode: 67   score: 255.0  epsilon: 1.0    steps: 168  evaluation reward: 160.52238805970148\n",
      "episode: 68   score: 205.0  epsilon: 1.0    steps: 368  evaluation reward: 161.1764705882353\n",
      "episode: 69   score: 335.0  epsilon: 1.0    steps: 752  evaluation reward: 163.69565217391303\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 154: Policy loss: 0.001336. Value loss: 0.226514. Entropy: 1.290780.\n",
      "Iteration 155: Policy loss: -0.007369. Value loss: 0.105029. Entropy: 1.296005.\n",
      "Iteration 156: Policy loss: -0.009655. Value loss: 0.079581. Entropy: 1.292433.\n",
      "episode: 70   score: 340.0  epsilon: 1.0    steps: 48  evaluation reward: 166.21428571428572\n",
      "episode: 71   score: 270.0  epsilon: 1.0    steps: 480  evaluation reward: 167.67605633802816\n",
      "episode: 72   score: 125.0  epsilon: 1.0    steps: 528  evaluation reward: 167.08333333333334\n",
      "episode: 73   score: 260.0  epsilon: 1.0    steps: 544  evaluation reward: 168.35616438356163\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 157: Policy loss: -0.004370. Value loss: 0.122753. Entropy: 1.226361.\n",
      "Iteration 158: Policy loss: -0.008945. Value loss: 0.074557. Entropy: 1.218094.\n",
      "Iteration 159: Policy loss: -0.012560. Value loss: 0.060647. Entropy: 1.219614.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 160: Policy loss: -0.000215. Value loss: 0.182287. Entropy: 1.176663.\n",
      "Iteration 161: Policy loss: -0.001876. Value loss: 0.108690. Entropy: 1.181910.\n",
      "Iteration 162: Policy loss: -0.004006. Value loss: 0.087675. Entropy: 1.170731.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 163: Policy loss: 0.001113. Value loss: 0.246518. Entropy: 1.198577.\n",
      "Iteration 164: Policy loss: -0.001023. Value loss: 0.109312. Entropy: 1.166169.\n",
      "Iteration 165: Policy loss: -0.007287. Value loss: 0.087939. Entropy: 1.181010.\n",
      "episode: 74   score: 5.0  epsilon: 1.0    steps: 176  evaluation reward: 166.14864864864865\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 166: Policy loss: 0.003539. Value loss: 0.255398. Entropy: 1.227928.\n",
      "Iteration 167: Policy loss: -0.003461. Value loss: 0.137123. Entropy: 1.240486.\n",
      "Iteration 168: Policy loss: -0.008218. Value loss: 0.104598. Entropy: 1.237213.\n",
      "episode: 75   score: 90.0  epsilon: 1.0    steps: 240  evaluation reward: 165.13333333333333\n",
      "episode: 76   score: 210.0  epsilon: 1.0    steps: 432  evaluation reward: 165.72368421052633\n",
      "episode: 77   score: 35.0  epsilon: 1.0    steps: 472  evaluation reward: 164.02597402597402\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 169: Policy loss: 0.005137. Value loss: 0.221741. Entropy: 1.247627.\n",
      "Iteration 170: Policy loss: -0.001393. Value loss: 0.125501. Entropy: 1.223117.\n",
      "Iteration 171: Policy loss: -0.005522. Value loss: 0.101671. Entropy: 1.218017.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n"
     ]
    }
   ],
   "source": [
    "### Loop through all environments and run PPO on them\n",
    "\n",
    "env_names = ['SpaceInvaders-v0', 'MsPacman-v0', 'Asteroids-v0', 'Asterix-v0', 'Atlantis-v0', 'Alien-v0', 'Amidar-v0', 'Assault-v0', 'BankHeist-v0']\n",
    "\n",
    "for a in range(len(env_names)):\n",
    "    name = env_names[a]\n",
    "    print(\"\\n\\n\\n ------- STARTING TRAINING FOR %s ------- \\n\\n\\n\" % (name))\n",
    "    \n",
    "    envs = []\n",
    "    for i in range(num_envs):\n",
    "        envs.append(GameEnv(name))\n",
    "    #env.render()\n",
    "    \n",
    "\n",
    "    number_lives = envs[0].life\n",
    "    state_size = envs[0].observation_space.shape\n",
    "    if (name == 'SpaceInvaders-v0'):\n",
    "        action_size = 4\n",
    "    else:\n",
    "        action_size = envs[0].action_space.n\n",
    "    rewards, episodes = [], []\n",
    "\n",
    "    vis_env_idx = 0\n",
    "    vis_env = envs[vis_env_idx]\n",
    "    e = 0\n",
    "    frame = 0\n",
    "    max_eval = -np.inf\n",
    "    reset_count = 0\n",
    "\n",
    "\n",
    "    agent = Agent(action_size)\n",
    "    torch.save(agent.policy_net.state_dict(), \"./save_model/\" + name + \"_best\")\n",
    "    evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "    frame = 0\n",
    "    memory_size = 0\n",
    "    reset_max = 10\n",
    "    \n",
    "    print(\"Determing min/max rewards of environment\")\n",
    "    [low, high] = score_range = get_score_range(name)\n",
    "    print(\"Min: %d. Max: %d.\" % (low, high))\n",
    "\n",
    "    while (frame < 10000000):\n",
    "        step = 0\n",
    "        assert(num_envs * env_mem_size == train_frame)\n",
    "        frame_next_vals = []\n",
    "        \n",
    "        for j in range(env_mem_size):\n",
    "            \n",
    "            curr_states = np.stack([envs[i].history[HISTORY_SIZE-1,:,:] for i in range(num_envs)])\n",
    "            next_states = []\n",
    "            net_in = np.stack([envs[i].history[:HISTORY_SIZE,:,:] for i in range(num_envs)])\n",
    "            step += num_envs\n",
    "            frame += num_envs\n",
    "            actions, values = agent.get_action(np.float32(net_in) / 255.)\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                next_states.append(next_state)\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                \"\"\"\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "                \"\"\"\n",
    "                \n",
    "                frame_next_state = get_frame(next_states[i])\n",
    "                env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "                terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "                env.life = env.info['ale.lives']\n",
    "                r = (env.reward / high) * 20.0 #np.log(max(env.reward+1, 1))#((env.reward - low) / (high - low)) * 30\n",
    "                agent.memory.push(i, deepcopy(curr_states[i]), actions[i], r, terminal_state, values[i], 0, 0)\n",
    "                \n",
    "                if (j == env_mem_size-1):\n",
    "                    net_in = np.stack([envs[k].history[1:,:,:] for k in range(num_envs)])\n",
    "                    _, frame_next_vals = agent.get_action(np.float32(net_in) / 255.)\n",
    "                \n",
    "                env.score += env.reward\n",
    "                env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "        \n",
    "                if (env.done):\n",
    "                    if (e % 50 == 0):\n",
    "                        print('now time : ', datetime.now())\n",
    "                        rewards.append(np.mean(evaluation_reward))\n",
    "                        episodes.append(e)\n",
    "                        pylab.plot(episodes, rewards, 'b')\n",
    "                        pylab.savefig(\"./save_graph/\" + name + \"_ppo.png\")\n",
    "                        torch.save(agent.policy_net, \"./save_model/\" + name + \"_ppo\")\n",
    "\n",
    "                        if np.mean(evaluation_reward) > max_eval:\n",
    "                            torch.save(agent.policy_net.state_dict(), \"./save_model/\"  + name + \"_ppo_best\")\n",
    "                            max_eval = float(np.mean(evaluation_reward))\n",
    "                            reset_count = 0\n",
    "                        elif e > 5000:\n",
    "                            reset_count += 1\n",
    "                            \"\"\"\n",
    "                            if (reset_count == reset_max):\n",
    "                                print(\"Training went nowhere, starting again at best model\")\n",
    "                                agent.policy_net.load_state_dict(torch.load(\"./save_model/spaceinvaders_ppo_best\"))\n",
    "                                agent.update_target_net()\n",
    "                                reset_count = 0\n",
    "                            \"\"\"\n",
    "                    e += 1\n",
    "                    evaluation_reward.append(env.score)\n",
    "                    print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                      \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "                    env.done = False\n",
    "                    env.score = 0\n",
    "                    env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "                    env.state = env.reset()\n",
    "                    env.life = number_lives\n",
    "                    get_init_state(env.history, env.state)\n",
    "            \n",
    "        agent.train_policy_net(frame, frame_next_vals)\n",
    "        agent.update_target_net()\n",
    "    print(\"FINISHED TRAINING FOR %s\" % (name))\n",
    "    pylab.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
