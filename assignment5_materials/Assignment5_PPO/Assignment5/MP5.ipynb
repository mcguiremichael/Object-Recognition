{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lifes(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 6\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/anaconda3/envs/py36/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],0)[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 185.0   memory length: 821   epsilon: 1.0    steps: 821     evaluation reward: 185.0\n",
      "Training network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:241: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:242: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:243: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Policy loss: -13.282112. Value loss: 12.888132. Entropy: 1.790153.\n",
      "Iteration 2: Policy loss: -13.259413. Value loss: 12.864014. Entropy: 1.790195.\n",
      "Iteration 3: Policy loss: -13.290983. Value loss: 12.893475. Entropy: 1.790215.\n",
      "episode: 1   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 676     evaluation reward: 147.5\n",
      "Training network\n",
      "Iteration 4: Policy loss: -11.884912. Value loss: 11.576634. Entropy: 1.790292.\n",
      "Iteration 5: Policy loss: -12.058539. Value loss: 11.745104. Entropy: 1.790237.\n",
      "Iteration 6: Policy loss: -11.863825. Value loss: 11.551113. Entropy: 1.790263.\n",
      "episode: 2   score: 165.0   memory length: 1024   epsilon: 1.0    steps: 676     evaluation reward: 153.33333333333334\n",
      "episode: 3   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 533     evaluation reward: 141.25\n",
      "Training network\n",
      "Iteration 7: Policy loss: -7.510808. Value loss: 7.236870. Entropy: 1.790373.\n",
      "Iteration 8: Policy loss: -7.452139. Value loss: 7.178435. Entropy: 1.790372.\n",
      "Iteration 9: Policy loss: -7.481874. Value loss: 7.205517. Entropy: 1.790332.\n",
      "episode: 4   score: 20.0   memory length: 1024   epsilon: 1.0    steps: 412     evaluation reward: 117.0\n",
      "episode: 5   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 685     evaluation reward: 110.83333333333333\n",
      "Training network\n",
      "Iteration 10: Policy loss: -7.373127. Value loss: 7.179406. Entropy: 1.790124.\n",
      "Iteration 11: Policy loss: -7.484456. Value loss: 7.287157. Entropy: 1.790126.\n",
      "Iteration 12: Policy loss: -7.317205. Value loss: 7.124152. Entropy: 1.790157.\n",
      "Training network\n",
      "Iteration 13: Policy loss: -21.715111. Value loss: 21.383768. Entropy: 1.790108.\n",
      "Iteration 14: Policy loss: -21.766266. Value loss: 21.429333. Entropy: 1.790140.\n",
      "Iteration 15: Policy loss: -21.850311. Value loss: 21.508760. Entropy: 1.790092.\n",
      "episode: 6   score: 490.0   memory length: 1024   epsilon: 1.0    steps: 1513     evaluation reward: 165.0\n",
      "episode: 7   score: 325.0   memory length: 1024   epsilon: 1.0    steps: 765     evaluation reward: 185.0\n",
      "Training network\n",
      "Iteration 16: Policy loss: -22.091965. Value loss: 21.810873. Entropy: 1.790142.\n",
      "Iteration 17: Policy loss: -22.187113. Value loss: 21.903549. Entropy: 1.790147.\n",
      "Iteration 18: Policy loss: -21.705076. Value loss: 21.418398. Entropy: 1.790152.\n",
      "Training network\n",
      "Iteration 19: Policy loss: -7.879316. Value loss: 7.541477. Entropy: 1.790305.\n",
      "Iteration 20: Policy loss: -7.852594. Value loss: 7.513740. Entropy: 1.790242.\n",
      "Iteration 21: Policy loss: -7.873825. Value loss: 7.534287. Entropy: 1.790262.\n",
      "episode: 8   score: 165.0   memory length: 1024   epsilon: 1.0    steps: 1186     evaluation reward: 182.77777777777777\n",
      "episode: 9   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 655     evaluation reward: 178.0\n",
      "Training network\n",
      "Iteration 22: Policy loss: -8.569651. Value loss: 8.302444. Entropy: 1.790233.\n",
      "Iteration 23: Policy loss: -8.608527. Value loss: 8.339021. Entropy: 1.790251.\n",
      "Iteration 24: Policy loss: -8.516660. Value loss: 8.247085. Entropy: 1.790244.\n",
      "episode: 10   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 563     evaluation reward: 166.8181818181818\n",
      "Training network\n",
      "Iteration 25: Policy loss: -9.658232. Value loss: 9.377519. Entropy: 1.790320.\n",
      "Iteration 26: Policy loss: -9.655572. Value loss: 9.375465. Entropy: 1.790347.\n",
      "Iteration 27: Policy loss: -9.661311. Value loss: 9.378979. Entropy: 1.790348.\n",
      "episode: 11   score: 240.0   memory length: 1024   epsilon: 1.0    steps: 1159     evaluation reward: 172.91666666666666\n",
      "episode: 12   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 418     evaluation reward: 165.76923076923077\n",
      "Training network\n",
      "Iteration 28: Policy loss: -11.542078. Value loss: 11.208011. Entropy: 1.790271.\n",
      "Iteration 29: Policy loss: -11.586737. Value loss: 11.250969. Entropy: 1.790211.\n",
      "Iteration 30: Policy loss: -11.607514. Value loss: 11.270872. Entropy: 1.790208.\n",
      "episode: 13   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 650     evaluation reward: 165.0\n",
      "Training network\n",
      "Iteration 31: Policy loss: -8.956964. Value loss: 8.773020. Entropy: 1.790359.\n",
      "Iteration 32: Policy loss: -9.103035. Value loss: 8.914798. Entropy: 1.790357.\n",
      "Iteration 33: Policy loss: -9.075006. Value loss: 8.887077. Entropy: 1.790374.\n",
      "episode: 14   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 621     evaluation reward: 162.0\n",
      "episode: 15   score: 75.0   memory length: 1024   epsilon: 1.0    steps: 500     evaluation reward: 156.5625\n",
      "episode: 16   score: 45.0   memory length: 1024   epsilon: 1.0    steps: 384     evaluation reward: 150.0\n",
      "Training network\n",
      "Iteration 34: Policy loss: -5.612822. Value loss: 5.301008. Entropy: 1.790450.\n",
      "Iteration 35: Policy loss: -5.608717. Value loss: 5.295313. Entropy: 1.790428.\n",
      "Iteration 36: Policy loss: -5.638169. Value loss: 5.322817. Entropy: 1.790428.\n",
      "Training network\n",
      "Iteration 37: Policy loss: -23.083769. Value loss: 22.668091. Entropy: 1.790380.\n",
      "Iteration 38: Policy loss: -22.853045. Value loss: 22.432508. Entropy: 1.790408.\n",
      "Iteration 39: Policy loss: -22.876385. Value loss: 22.453154. Entropy: 1.790404.\n",
      "episode: 17   score: 450.0   memory length: 1024   epsilon: 1.0    steps: 1283     evaluation reward: 166.66666666666666\n",
      "episode: 18   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 651     evaluation reward: 167.3684210526316\n",
      "Training network\n",
      "Iteration 40: Policy loss: -15.696943. Value loss: 15.318116. Entropy: 1.790149.\n",
      "Iteration 41: Policy loss: -15.722044. Value loss: 15.343035. Entropy: 1.790148.\n",
      "Iteration 42: Policy loss: -15.769566. Value loss: 15.385404. Entropy: 1.790153.\n",
      "episode: 19   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 554     evaluation reward: 168.0\n",
      "Training network\n",
      "Iteration 43: Policy loss: -14.794363. Value loss: 14.427770. Entropy: 1.790126.\n",
      "Iteration 44: Policy loss: -14.721176. Value loss: 14.351361. Entropy: 1.790125.\n",
      "Iteration 45: Policy loss: -14.682028. Value loss: 14.310387. Entropy: 1.790144.\n",
      "episode: 20   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 769     evaluation reward: 170.0\n",
      "episode: 21   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 746     evaluation reward: 169.3181818181818\n",
      "Training network\n",
      "Iteration 46: Policy loss: -9.300600. Value loss: 8.979209. Entropy: 1.790312.\n",
      "Iteration 47: Policy loss: -9.343120. Value loss: 9.022042. Entropy: 1.790272.\n",
      "Iteration 48: Policy loss: -9.459248. Value loss: 9.135154. Entropy: 1.790272.\n",
      "episode: 22   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 403     evaluation reward: 164.1304347826087\n",
      "Training network\n",
      "Iteration 49: Policy loss: -10.375463. Value loss: 10.040130. Entropy: 1.790362.\n",
      "Iteration 50: Policy loss: -10.310765. Value loss: 9.975879. Entropy: 1.790355.\n",
      "Iteration 51: Policy loss: -10.368250. Value loss: 10.030803. Entropy: 1.790334.\n",
      "episode: 23   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 985     evaluation reward: 164.79166666666666\n",
      "episode: 24   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 641     evaluation reward: 163.0\n",
      "Training network\n",
      "Iteration 52: Policy loss: -11.041798. Value loss: 10.706614. Entropy: 1.790457.\n",
      "Iteration 53: Policy loss: -11.069140. Value loss: 10.732733. Entropy: 1.790452.\n",
      "Iteration 54: Policy loss: -11.189471. Value loss: 10.851133. Entropy: 1.790399.\n",
      "episode: 25   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 552     evaluation reward: 160.76923076923077\n",
      "Training network\n",
      "Iteration 55: Policy loss: -8.748391. Value loss: 8.539057. Entropy: 1.790304.\n",
      "Iteration 56: Policy loss: -8.814479. Value loss: 8.604316. Entropy: 1.790337.\n",
      "Iteration 57: Policy loss: -8.703513. Value loss: 8.492339. Entropy: 1.790288.\n",
      "episode: 26   score: 185.0   memory length: 1024   epsilon: 1.0    steps: 762     evaluation reward: 161.66666666666666\n",
      "episode: 27   score: 100.0   memory length: 1024   epsilon: 1.0    steps: 530     evaluation reward: 159.46428571428572\n",
      "Training network\n",
      "Iteration 58: Policy loss: -9.303262. Value loss: 9.037095. Entropy: 1.790129.\n",
      "Iteration 59: Policy loss: -9.360534. Value loss: 9.091078. Entropy: 1.790139.\n",
      "Iteration 60: Policy loss: -9.314857. Value loss: 9.044850. Entropy: 1.790104.\n",
      "episode: 28   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 745     evaluation reward: 160.17241379310346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 29   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 476     evaluation reward: 158.33333333333334\n",
      "Training network\n",
      "Iteration 61: Policy loss: -11.958920. Value loss: 11.627741. Entropy: 1.790206.\n",
      "Iteration 62: Policy loss: -11.702659. Value loss: 11.371776. Entropy: 1.790171.\n",
      "Iteration 63: Policy loss: -11.918467. Value loss: 11.584954. Entropy: 1.790172.\n",
      "episode: 30   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 419     evaluation reward: 154.83870967741936\n",
      "Training network\n",
      "Iteration 64: Policy loss: -12.631729. Value loss: 12.279160. Entropy: 1.790323.\n",
      "Iteration 65: Policy loss: -12.527327. Value loss: 12.177687. Entropy: 1.790295.\n",
      "Iteration 66: Policy loss: -12.613468. Value loss: 12.262482. Entropy: 1.790321.\n",
      "episode: 31   score: 205.0   memory length: 1024   epsilon: 1.0    steps: 976     evaluation reward: 156.40625\n",
      "Training network\n",
      "Iteration 67: Policy loss: -11.393254. Value loss: 11.068705. Entropy: 1.790371.\n",
      "Iteration 68: Policy loss: -11.225077. Value loss: 10.901644. Entropy: 1.790341.\n",
      "Iteration 69: Policy loss: -11.448865. Value loss: 11.122868. Entropy: 1.790364.\n",
      "episode: 32   score: 315.0   memory length: 1024   epsilon: 1.0    steps: 1459     evaluation reward: 161.21212121212122\n",
      "Training network\n",
      "Iteration 70: Policy loss: -13.599195. Value loss: 13.282240. Entropy: 1.790050.\n",
      "Iteration 71: Policy loss: -13.779006. Value loss: 13.455874. Entropy: 1.790109.\n",
      "Iteration 72: Policy loss: -13.425805. Value loss: 13.105148. Entropy: 1.790079.\n",
      "episode: 33   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 720     evaluation reward: 161.02941176470588\n",
      "episode: 34   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 671     evaluation reward: 160.28571428571428\n",
      "Training network\n",
      "Iteration 73: Policy loss: -8.414800. Value loss: 8.210793. Entropy: 1.790418.\n",
      "Iteration 74: Policy loss: -8.447369. Value loss: 8.241346. Entropy: 1.790408.\n",
      "Iteration 75: Policy loss: -8.395773. Value loss: 8.190094. Entropy: 1.790407.\n",
      "episode: 35   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 436     evaluation reward: 157.36111111111111\n",
      "episode: 36   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 606     evaluation reward: 156.75675675675674\n",
      "Training network\n",
      "Iteration 76: Policy loss: -9.664722. Value loss: 9.371504. Entropy: 1.790345.\n",
      "Iteration 77: Policy loss: -9.657206. Value loss: 9.364453. Entropy: 1.790304.\n",
      "Iteration 78: Policy loss: -9.532066. Value loss: 9.239928. Entropy: 1.790365.\n",
      "episode: 37   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 720     evaluation reward: 155.39473684210526\n",
      "Training network\n",
      "Iteration 79: Policy loss: -11.765502. Value loss: 11.591230. Entropy: 1.790161.\n",
      "Iteration 80: Policy loss: -11.779191. Value loss: 11.601896. Entropy: 1.790158.\n",
      "Iteration 81: Policy loss: -11.824080. Value loss: 11.646251. Entropy: 1.790127.\n",
      "episode: 38   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 756     evaluation reward: 155.3846153846154\n",
      "episode: 39   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 577     evaluation reward: 153.5\n",
      "Training network\n",
      "Iteration 82: Policy loss: -5.675731. Value loss: 5.416492. Entropy: 1.790119.\n",
      "Iteration 83: Policy loss: -5.633196. Value loss: 5.374972. Entropy: 1.790135.\n",
      "Iteration 84: Policy loss: -5.643669. Value loss: 5.383838. Entropy: 1.790144.\n",
      "episode: 40   score: 20.0   memory length: 1024   epsilon: 1.0    steps: 396     evaluation reward: 150.2439024390244\n",
      "Training network\n",
      "Iteration 85: Policy loss: -8.551227. Value loss: 8.293239. Entropy: 1.790436.\n",
      "Iteration 86: Policy loss: -8.502397. Value loss: 8.241629. Entropy: 1.790406.\n",
      "Iteration 87: Policy loss: -8.406292. Value loss: 8.147509. Entropy: 1.790443.\n",
      "episode: 41   score: 215.0   memory length: 1024   epsilon: 1.0    steps: 808     evaluation reward: 151.78571428571428\n",
      "episode: 42   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 585     evaluation reward: 149.53488372093022\n",
      "Training network\n",
      "Iteration 88: Policy loss: -6.245923. Value loss: 6.149054. Entropy: 1.790386.\n",
      "Iteration 89: Policy loss: -6.094255. Value loss: 6.001965. Entropy: 1.790309.\n",
      "Iteration 90: Policy loss: -6.145191. Value loss: 6.049493. Entropy: 1.790362.\n",
      "episode: 43   score: 100.0   memory length: 1024   epsilon: 1.0    steps: 639     evaluation reward: 148.4090909090909\n",
      "episode: 44   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 493     evaluation reward: 147.44444444444446\n",
      "Training network\n",
      "Iteration 91: Policy loss: -12.939445. Value loss: 12.561024. Entropy: 1.790272.\n",
      "Iteration 92: Policy loss: -13.018274. Value loss: 12.638163. Entropy: 1.790248.\n",
      "Iteration 93: Policy loss: -12.872133. Value loss: 12.490437. Entropy: 1.790195.\n",
      "Training network\n",
      "Iteration 94: Policy loss: -10.413865. Value loss: 10.053305. Entropy: 1.790359.\n",
      "Iteration 95: Policy loss: -10.455858. Value loss: 10.093377. Entropy: 1.790315.\n",
      "Iteration 96: Policy loss: -10.463607. Value loss: 10.099772. Entropy: 1.790334.\n",
      "episode: 45   score: 240.0   memory length: 1024   epsilon: 1.0    steps: 1434     evaluation reward: 149.45652173913044\n",
      "Training network\n",
      "Iteration 97: Policy loss: -8.401904. Value loss: 8.184417. Entropy: 1.790350.\n",
      "Iteration 98: Policy loss: -8.397725. Value loss: 8.178185. Entropy: 1.790366.\n",
      "Iteration 99: Policy loss: -8.367693. Value loss: 8.149661. Entropy: 1.790327.\n",
      "episode: 46   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 803     evaluation reward: 149.5744680851064\n",
      "episode: 47   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 554     evaluation reward: 148.75\n",
      "episode: 48   score: 85.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 147.44897959183675\n",
      "Training network\n",
      "Iteration 100: Policy loss: -9.099723. Value loss: 8.847845. Entropy: 1.790554.\n",
      "Iteration 101: Policy loss: -9.124418. Value loss: 8.871485. Entropy: 1.790478.\n",
      "Iteration 102: Policy loss: -9.132594. Value loss: 8.877842. Entropy: 1.790527.\n",
      "episode: 49   score: 100.0   memory length: 1024   epsilon: 1.0    steps: 560     evaluation reward: 146.5\n",
      "Training network\n",
      "Iteration 103: Policy loss: -10.882065. Value loss: 10.623828. Entropy: 1.790506.\n",
      "Iteration 104: Policy loss: -11.001462. Value loss: 10.739257. Entropy: 1.790486.\n",
      "Iteration 105: Policy loss: -10.971940. Value loss: 10.707632. Entropy: 1.790485.\n",
      "episode: 50   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 571     evaluation reward: 146.66666666666666\n",
      "episode: 51   score: 130.0   memory length: 1024   epsilon: 1.0    steps: 707     evaluation reward: 146.34615384615384\n",
      "Training network\n",
      "Iteration 106: Policy loss: -8.961238. Value loss: 8.682826. Entropy: 1.790388.\n",
      "Iteration 107: Policy loss: -9.014778. Value loss: 8.731627. Entropy: 1.790407.\n",
      "Iteration 108: Policy loss: -9.049094. Value loss: 8.762968. Entropy: 1.790431.\n",
      "episode: 52   score: 75.0   memory length: 1024   epsilon: 1.0    steps: 584     evaluation reward: 145.0\n",
      "episode: 53   score: 140.0   memory length: 1024   epsilon: 1.0    steps: 695     evaluation reward: 144.90740740740742\n",
      "Training network\n",
      "Iteration 109: Policy loss: -8.872529. Value loss: 8.645201. Entropy: 1.790095.\n",
      "Iteration 110: Policy loss: -8.968934. Value loss: 8.737051. Entropy: 1.790124.\n",
      "Iteration 111: Policy loss: -8.931447. Value loss: 8.696788. Entropy: 1.790061.\n",
      "episode: 54   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 654     evaluation reward: 144.54545454545453\n",
      "Training network\n",
      "Iteration 112: Policy loss: -10.844927. Value loss: 10.483985. Entropy: 1.790228.\n",
      "Iteration 113: Policy loss: -10.778573. Value loss: 10.416494. Entropy: 1.790227.\n",
      "Iteration 114: Policy loss: -10.738275. Value loss: 10.376579. Entropy: 1.790250.\n",
      "episode: 55   score: 245.0   memory length: 1024   epsilon: 1.0    steps: 976     evaluation reward: 146.33928571428572\n",
      "Training network\n",
      "Iteration 115: Policy loss: -13.439779. Value loss: 13.147773. Entropy: 1.790233.\n",
      "Iteration 116: Policy loss: -13.451746. Value loss: 13.156981. Entropy: 1.790292.\n",
      "Iteration 117: Policy loss: -13.545284. Value loss: 13.247078. Entropy: 1.790327.\n",
      "episode: 56   score: 70.0   memory length: 1024   epsilon: 1.0    steps: 520     evaluation reward: 145.0\n",
      "Training network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 118: Policy loss: -13.004067. Value loss: 12.631794. Entropy: 1.790501.\n",
      "Iteration 119: Policy loss: -12.992560. Value loss: 12.619584. Entropy: 1.790449.\n",
      "Iteration 120: Policy loss: -12.858914. Value loss: 12.486899. Entropy: 1.790491.\n",
      "episode: 57   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 981     evaluation reward: 146.1206896551724\n",
      "Training network\n",
      "Iteration 121: Policy loss: -10.303321. Value loss: 10.008017. Entropy: 1.790239.\n",
      "Iteration 122: Policy loss: -10.300719. Value loss: 10.004076. Entropy: 1.790251.\n",
      "Iteration 123: Policy loss: -10.126807. Value loss: 9.833957. Entropy: 1.790221.\n",
      "episode: 58   score: 330.0   memory length: 1024   epsilon: 1.0    steps: 1171     evaluation reward: 149.23728813559322\n",
      "episode: 59   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 436     evaluation reward: 147.66666666666666\n",
      "Training network\n",
      "Iteration 124: Policy loss: -10.517057. Value loss: 10.269119. Entropy: 1.790402.\n",
      "Iteration 125: Policy loss: -10.585332. Value loss: 10.337179. Entropy: 1.790320.\n",
      "Iteration 126: Policy loss: -10.809799. Value loss: 10.554440. Entropy: 1.790370.\n",
      "episode: 60   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 545     evaluation reward: 147.04918032786884\n",
      "episode: 61   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 637     evaluation reward: 146.4516129032258\n",
      "Training network\n",
      "Iteration 127: Policy loss: -11.263874. Value loss: 10.977137. Entropy: 1.790435.\n",
      "Iteration 128: Policy loss: -11.294565. Value loss: 11.009974. Entropy: 1.790427.\n",
      "Iteration 129: Policy loss: -11.300610. Value loss: 11.010231. Entropy: 1.790452.\n",
      "episode: 62   score: 245.0   memory length: 1024   epsilon: 1.0    steps: 1033     evaluation reward: 148.015873015873\n",
      "Training network\n",
      "Iteration 130: Policy loss: -12.384413. Value loss: 11.984793. Entropy: 1.790048.\n",
      "Iteration 131: Policy loss: -12.407426. Value loss: 12.005516. Entropy: 1.790062.\n",
      "Iteration 132: Policy loss: -12.351135. Value loss: 11.949859. Entropy: 1.790058.\n",
      "episode: 63   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 740     evaluation reward: 147.578125\n",
      "Training network\n",
      "Iteration 133: Policy loss: -8.655974. Value loss: 8.424711. Entropy: 1.790378.\n",
      "Iteration 134: Policy loss: -8.757367. Value loss: 8.518669. Entropy: 1.790386.\n",
      "Iteration 135: Policy loss: -8.765036. Value loss: 8.528448. Entropy: 1.790393.\n",
      "episode: 64   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 693     evaluation reward: 147.3846153846154\n",
      "episode: 65   score: 310.0   memory length: 1024   epsilon: 1.0    steps: 720     evaluation reward: 149.84848484848484\n",
      "Training network\n",
      "Iteration 136: Policy loss: -20.222942. Value loss: 19.944572. Entropy: 1.790185.\n",
      "Iteration 137: Policy loss: -20.183853. Value loss: 19.896643. Entropy: 1.790113.\n",
      "Iteration 138: Policy loss: -20.525322. Value loss: 20.232151. Entropy: 1.790140.\n",
      "episode: 66   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 627     evaluation reward: 149.17910447761193\n",
      "episode: 67   score: 75.0   memory length: 1024   epsilon: 1.0    steps: 512     evaluation reward: 148.08823529411765\n",
      "Training network\n",
      "Iteration 139: Policy loss: -7.721766. Value loss: 7.429597. Entropy: 1.790353.\n",
      "Iteration 140: Policy loss: -7.721419. Value loss: 7.429108. Entropy: 1.790372.\n",
      "Iteration 141: Policy loss: -7.724430. Value loss: 7.428773. Entropy: 1.790342.\n",
      "episode: 68   score: 75.0   memory length: 1024   epsilon: 1.0    steps: 552     evaluation reward: 147.02898550724638\n",
      "Training network\n",
      "Iteration 142: Policy loss: -5.495819. Value loss: 5.364004. Entropy: 1.790448.\n",
      "Iteration 143: Policy loss: -5.507636. Value loss: 5.373999. Entropy: 1.790446.\n",
      "Iteration 144: Policy loss: -5.485990. Value loss: 5.351813. Entropy: 1.790391.\n",
      "episode: 69   score: 35.0   memory length: 1024   epsilon: 1.0    steps: 514     evaluation reward: 145.42857142857142\n",
      "episode: 70   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 512     evaluation reward: 144.85915492957747\n",
      "now time :  2018-12-19 13:09:03.249030\n",
      "Training network\n",
      "Iteration 145: Policy loss: -14.639036. Value loss: 14.257813. Entropy: 1.790356.\n",
      "Iteration 146: Policy loss: -14.736741. Value loss: 14.355179. Entropy: 1.790360.\n",
      "Iteration 147: Policy loss: -14.813973. Value loss: 14.428581. Entropy: 1.790327.\n",
      "episode: 71   score: 315.0   memory length: 1024   epsilon: 1.0    steps: 1007     evaluation reward: 147.22222222222223\n",
      "Training network\n",
      "Iteration 148: Policy loss: -9.908167. Value loss: 9.653084. Entropy: 1.790260.\n",
      "Iteration 149: Policy loss: -9.760851. Value loss: 9.507225. Entropy: 1.790292.\n",
      "Iteration 150: Policy loss: -9.967649. Value loss: 9.709917. Entropy: 1.790266.\n",
      "episode: 72   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 539     evaluation reward: 146.0958904109589\n",
      "episode: 73   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 795     evaluation reward: 146.55405405405406\n",
      "Training network\n",
      "Iteration 151: Policy loss: -13.537434. Value loss: 13.181179. Entropy: 1.790454.\n",
      "Iteration 152: Policy loss: -13.641642. Value loss: 13.281605. Entropy: 1.790485.\n",
      "Iteration 153: Policy loss: -13.471231. Value loss: 13.111892. Entropy: 1.790472.\n",
      "episode: 74   score: 400.0   memory length: 1024   epsilon: 1.0    steps: 774     evaluation reward: 149.93333333333334\n",
      "Training network\n",
      "Iteration 154: Policy loss: -26.690599. Value loss: 26.406139. Entropy: 1.790466.\n",
      "Iteration 155: Policy loss: -26.983307. Value loss: 26.689587. Entropy: 1.790471.\n",
      "Iteration 156: Policy loss: -26.407654. Value loss: 26.111553. Entropy: 1.790466.\n",
      "episode: 75   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 690     evaluation reward: 149.4078947368421\n",
      "episode: 76   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 543     evaluation reward: 148.8961038961039\n",
      "Training network\n",
      "Iteration 157: Policy loss: -6.203640. Value loss: 5.962837. Entropy: 1.790396.\n",
      "Iteration 158: Policy loss: -6.173196. Value loss: 5.931044. Entropy: 1.790427.\n",
      "Iteration 159: Policy loss: -6.258083. Value loss: 6.015433. Entropy: 1.790382.\n",
      "Training network\n",
      "Iteration 160: Policy loss: -18.420485. Value loss: 17.976418. Entropy: 1.790118.\n",
      "Iteration 161: Policy loss: -18.655493. Value loss: 18.207073. Entropy: 1.790127.\n",
      "Iteration 162: Policy loss: -18.629883. Value loss: 18.176998. Entropy: 1.790129.\n",
      "episode: 77   score: 345.0   memory length: 1024   epsilon: 1.0    steps: 1379     evaluation reward: 151.4102564102564\n",
      "episode: 78   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 748     evaluation reward: 150.8860759493671\n",
      "Training network\n",
      "Iteration 163: Policy loss: -9.542002. Value loss: 9.240988. Entropy: 1.790320.\n",
      "Iteration 164: Policy loss: -9.595993. Value loss: 9.295959. Entropy: 1.790300.\n",
      "Iteration 165: Policy loss: -9.619602. Value loss: 9.317404. Entropy: 1.790289.\n",
      "episode: 79   score: 75.0   memory length: 1024   epsilon: 1.0    steps: 638     evaluation reward: 149.9375\n",
      "episode: 80   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 501     evaluation reward: 149.07407407407408\n",
      "Training network\n",
      "Iteration 166: Policy loss: -6.998269. Value loss: 6.801682. Entropy: 1.790239.\n",
      "Iteration 167: Policy loss: -6.945461. Value loss: 6.748087. Entropy: 1.790248.\n",
      "Iteration 168: Policy loss: -7.016366. Value loss: 6.818055. Entropy: 1.790271.\n",
      "episode: 81   score: 215.0   memory length: 1024   epsilon: 1.0    steps: 768     evaluation reward: 149.8780487804878\n",
      "Training network\n",
      "Iteration 169: Policy loss: -13.088398. Value loss: 12.792912. Entropy: 1.790296.\n",
      "Iteration 170: Policy loss: -12.992513. Value loss: 12.698876. Entropy: 1.790262.\n",
      "Iteration 171: Policy loss: -13.085069. Value loss: 12.788404. Entropy: 1.790300.\n",
      "episode: 82   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 751     evaluation reward: 149.33734939759037\n",
      "Training network\n",
      "Iteration 172: Policy loss: -12.224189. Value loss: 11.819757. Entropy: 1.790402.\n",
      "Iteration 173: Policy loss: -12.248778. Value loss: 11.842112. Entropy: 1.790425.\n",
      "Iteration 174: Policy loss: -12.272951. Value loss: 11.864486. Entropy: 1.790407.\n",
      "episode: 83   score: 215.0   memory length: 1024   epsilon: 1.0    steps: 835     evaluation reward: 150.11904761904762\n",
      "episode: 84   score: 70.0   memory length: 1024   epsilon: 1.0    steps: 510     evaluation reward: 149.1764705882353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network\n",
      "Iteration 175: Policy loss: -9.840235. Value loss: 9.622861. Entropy: 1.790485.\n",
      "Iteration 176: Policy loss: -9.797318. Value loss: 9.575883. Entropy: 1.790477.\n",
      "Iteration 177: Policy loss: -9.848945. Value loss: 9.627436. Entropy: 1.790483.\n",
      "episode: 85   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 531     evaluation reward: 148.6627906976744\n",
      "episode: 86   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 637     evaluation reward: 149.02298850574712\n",
      "Training network\n",
      "Iteration 178: Policy loss: -12.567269. Value loss: 12.266815. Entropy: 1.790225.\n",
      "Iteration 179: Policy loss: -12.529522. Value loss: 12.225691. Entropy: 1.790250.\n",
      "Iteration 180: Policy loss: -12.347881. Value loss: 12.047434. Entropy: 1.790233.\n",
      "episode: 87   score: 485.0   memory length: 1024   epsilon: 1.0    steps: 947     evaluation reward: 152.8409090909091\n",
      "Training network\n",
      "Iteration 181: Policy loss: -30.344065. Value loss: 29.938408. Entropy: 1.790461.\n",
      "Iteration 182: Policy loss: -30.454756. Value loss: 30.042713. Entropy: 1.790449.\n",
      "Iteration 183: Policy loss: -30.868380. Value loss: 30.447598. Entropy: 1.790449.\n",
      "episode: 88   score: 140.0   memory length: 1024   epsilon: 1.0    steps: 540     evaluation reward: 152.69662921348313\n",
      "Training network\n",
      "Iteration 184: Policy loss: -13.700653. Value loss: 13.374303. Entropy: 1.790457.\n",
      "Iteration 185: Policy loss: -13.758615. Value loss: 13.428609. Entropy: 1.790410.\n",
      "Iteration 186: Policy loss: -13.609795. Value loss: 13.282275. Entropy: 1.790413.\n",
      "episode: 89   score: 230.0   memory length: 1024   epsilon: 1.0    steps: 941     evaluation reward: 153.55555555555554\n",
      "episode: 90   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 691     evaluation reward: 153.1868131868132\n",
      "Training network\n",
      "Iteration 187: Policy loss: -10.254296. Value loss: 10.026326. Entropy: 1.790399.\n",
      "Iteration 188: Policy loss: -10.296202. Value loss: 10.065631. Entropy: 1.790326.\n",
      "Iteration 189: Policy loss: -10.271460. Value loss: 10.038907. Entropy: 1.790342.\n",
      "episode: 91   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 496     evaluation reward: 152.22826086956522\n",
      "Training network\n",
      "Iteration 190: Policy loss: -7.950138. Value loss: 7.796180. Entropy: 1.790441.\n",
      "Iteration 191: Policy loss: -7.896889. Value loss: 7.744249. Entropy: 1.790459.\n",
      "Iteration 192: Policy loss: -7.959673. Value loss: 7.806466. Entropy: 1.790471.\n",
      "episode: 92   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 618     evaluation reward: 151.88172043010752\n",
      "episode: 93   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 665     evaluation reward: 151.38297872340425\n",
      "Training network\n",
      "Iteration 193: Policy loss: -10.764642. Value loss: 10.583499. Entropy: 1.790432.\n",
      "Iteration 194: Policy loss: -10.831485. Value loss: 10.646637. Entropy: 1.790462.\n",
      "Iteration 195: Policy loss: -10.856805. Value loss: 10.668669. Entropy: 1.790444.\n",
      "episode: 94   score: 250.0   memory length: 1024   epsilon: 1.0    steps: 1054     evaluation reward: 152.42105263157896\n",
      "Training network\n",
      "Iteration 196: Policy loss: -11.658212. Value loss: 11.380584. Entropy: 1.790015.\n",
      "Iteration 197: Policy loss: -11.681133. Value loss: 11.401938. Entropy: 1.789946.\n",
      "Iteration 198: Policy loss: -11.571572. Value loss: 11.293985. Entropy: 1.789938.\n",
      "episode: 95   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 399     evaluation reward: 151.51041666666666\n",
      "episode: 96   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 757     evaluation reward: 151.18556701030928\n",
      "Training network\n",
      "Iteration 199: Policy loss: -9.609989. Value loss: 9.295197. Entropy: 1.790317.\n",
      "Iteration 200: Policy loss: -9.625712. Value loss: 9.306858. Entropy: 1.790339.\n",
      "Iteration 201: Policy loss: -9.669487. Value loss: 9.350611. Entropy: 1.790277.\n",
      "episode: 97   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 650     evaluation reward: 150.71428571428572\n",
      "Training network\n",
      "Iteration 202: Policy loss: -4.644237. Value loss: 4.589681. Entropy: 1.790482.\n",
      "Iteration 203: Policy loss: -4.663635. Value loss: 4.607111. Entropy: 1.790432.\n",
      "Iteration 204: Policy loss: -4.698611. Value loss: 4.641523. Entropy: 1.790460.\n",
      "episode: 98   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 649     evaluation reward: 150.25252525252526\n",
      "episode: 99   score: 30.0   memory length: 1024   epsilon: 1.0    steps: 492     evaluation reward: 149.05\n",
      "Training network\n",
      "Iteration 205: Policy loss: -8.415298. Value loss: 8.235127. Entropy: 1.790377.\n",
      "Iteration 206: Policy loss: -8.536106. Value loss: 8.353476. Entropy: 1.790408.\n",
      "Iteration 207: Policy loss: -8.542346. Value loss: 8.357714. Entropy: 1.790424.\n",
      "episode: 100   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 628     evaluation reward: 148.45\n",
      "episode: 101   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 683     evaluation reward: 149.45\n",
      "Training network\n",
      "Iteration 208: Policy loss: -14.168724. Value loss: 13.858088. Entropy: 1.790481.\n",
      "Iteration 209: Policy loss: -14.152000. Value loss: 13.838839. Entropy: 1.790435.\n",
      "Iteration 210: Policy loss: -14.024712. Value loss: 13.709579. Entropy: 1.790418.\n",
      "episode: 102   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 927     evaluation reward: 149.35\n",
      "Training network\n",
      "Iteration 211: Policy loss: -9.326588. Value loss: 9.070604. Entropy: 1.790384.\n",
      "Iteration 212: Policy loss: -9.443001. Value loss: 9.183957. Entropy: 1.790375.\n",
      "Iteration 213: Policy loss: -9.205135. Value loss: 8.950907. Entropy: 1.790398.\n",
      "episode: 103   score: 440.0   memory length: 1024   epsilon: 1.0    steps: 927     evaluation reward: 152.7\n",
      "Training network\n",
      "Iteration 214: Policy loss: -25.846996. Value loss: 25.519152. Entropy: 1.790421.\n",
      "Iteration 215: Policy loss: -26.423040. Value loss: 26.082273. Entropy: 1.790403.\n",
      "Iteration 216: Policy loss: -26.131563. Value loss: 25.785219. Entropy: 1.790429.\n",
      "episode: 104   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 943     evaluation reward: 154.3\n",
      "Training network\n",
      "Iteration 217: Policy loss: -10.703682. Value loss: 10.333688. Entropy: 1.790514.\n",
      "Iteration 218: Policy loss: -10.636295. Value loss: 10.266000. Entropy: 1.790522.\n",
      "Iteration 219: Policy loss: -10.722936. Value loss: 10.350692. Entropy: 1.790540.\n",
      "episode: 105   score: 185.0   memory length: 1024   epsilon: 1.0    steps: 812     evaluation reward: 155.35\n",
      "episode: 106   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 333     evaluation reward: 150.6\n",
      "Training network\n",
      "Iteration 220: Policy loss: -10.891165. Value loss: 10.647608. Entropy: 1.790239.\n",
      "Iteration 221: Policy loss: -10.959068. Value loss: 10.714138. Entropy: 1.790254.\n",
      "Iteration 222: Policy loss: -10.857704. Value loss: 10.613160. Entropy: 1.790264.\n",
      "episode: 107   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 436     evaluation reward: 148.4\n",
      "episode: 108   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 645     evaluation reward: 148.3\n",
      "Training network\n",
      "Iteration 223: Policy loss: -13.366867. Value loss: 13.139915. Entropy: 1.790483.\n",
      "Iteration 224: Policy loss: -13.269662. Value loss: 13.044779. Entropy: 1.790492.\n",
      "Iteration 225: Policy loss: -13.459499. Value loss: 13.226990. Entropy: 1.790472.\n",
      "Training network\n",
      "Iteration 226: Policy loss: -6.913586. Value loss: 6.688354. Entropy: 1.790233.\n",
      "Iteration 227: Policy loss: -6.848060. Value loss: 6.624384. Entropy: 1.790272.\n",
      "Iteration 228: Policy loss: -6.904707. Value loss: 6.677349. Entropy: 1.790264.\n",
      "episode: 109   score: 230.0   memory length: 1024   epsilon: 1.0    steps: 1264     evaluation reward: 149.25\n",
      "Training network\n",
      "Iteration 229: Policy loss: -11.881215. Value loss: 11.645987. Entropy: 1.790218.\n",
      "Iteration 230: Policy loss: -11.831656. Value loss: 11.598303. Entropy: 1.790227.\n",
      "Iteration 231: Policy loss: -11.820336. Value loss: 11.581766. Entropy: 1.790212.\n",
      "episode: 110   score: 410.0   memory length: 1024   epsilon: 1.0    steps: 1276     evaluation reward: 152.8\n",
      "episode: 111   score: 75.0   memory length: 1024   epsilon: 1.0    steps: 692     evaluation reward: 151.15\n",
      "Training network\n",
      "Iteration 232: Policy loss: -16.798010. Value loss: 16.532305. Entropy: 1.790159.\n",
      "Iteration 233: Policy loss: -16.892693. Value loss: 16.617882. Entropy: 1.790166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 234: Policy loss: -17.092455. Value loss: 16.809120. Entropy: 1.790196.\n",
      "episode: 112   score: 150.0   memory length: 1024   epsilon: 1.0    steps: 731     evaluation reward: 151.85\n",
      "Training network\n",
      "Iteration 235: Policy loss: -8.787663. Value loss: 8.465008. Entropy: 1.790401.\n",
      "Iteration 236: Policy loss: -8.803160. Value loss: 8.479132. Entropy: 1.790430.\n",
      "Iteration 237: Policy loss: -8.834949. Value loss: 8.508969. Entropy: 1.790434.\n",
      "episode: 113   score: 75.0   memory length: 1024   epsilon: 1.0    steps: 654     evaluation reward: 151.05\n",
      "episode: 114   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 365     evaluation reward: 150.35\n",
      "Training network\n",
      "Iteration 238: Policy loss: -6.784424. Value loss: 6.629796. Entropy: 1.790388.\n",
      "Iteration 239: Policy loss: -6.885087. Value loss: 6.727115. Entropy: 1.790395.\n",
      "Iteration 240: Policy loss: -6.971486. Value loss: 6.811074. Entropy: 1.790373.\n",
      "episode: 115   score: 140.0   memory length: 1024   epsilon: 1.0    steps: 1021     evaluation reward: 151.0\n",
      "Training network\n",
      "Iteration 241: Policy loss: -13.117484. Value loss: 12.808868. Entropy: 1.790309.\n",
      "Iteration 242: Policy loss: -13.271792. Value loss: 12.957620. Entropy: 1.790298.\n",
      "Iteration 243: Policy loss: -13.308939. Value loss: 12.993373. Entropy: 1.790273.\n",
      "episode: 116   score: 285.0   memory length: 1024   epsilon: 1.0    steps: 950     evaluation reward: 153.4\n",
      "Training network\n",
      "Iteration 244: Policy loss: -15.539209. Value loss: 15.262029. Entropy: 1.790594.\n",
      "Iteration 245: Policy loss: -15.350268. Value loss: 15.070172. Entropy: 1.790555.\n",
      "Iteration 246: Policy loss: -15.581963. Value loss: 15.297097. Entropy: 1.790596.\n",
      "episode: 117   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 635     evaluation reward: 150.45\n",
      "episode: 118   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 765     evaluation reward: 149.7\n",
      "Training network\n",
      "Iteration 247: Policy loss: -7.860641. Value loss: 7.616909. Entropy: 1.790506.\n",
      "Iteration 248: Policy loss: -7.852203. Value loss: 7.604607. Entropy: 1.790501.\n",
      "Iteration 249: Policy loss: -7.824851. Value loss: 7.582673. Entropy: 1.790466.\n",
      "episode: 119   score: 260.0   memory length: 1024   epsilon: 1.0    steps: 782     evaluation reward: 150.5\n",
      "Training network\n",
      "Iteration 250: Policy loss: -19.496500. Value loss: 19.175873. Entropy: 1.790292.\n",
      "Iteration 251: Policy loss: -19.512602. Value loss: 19.186174. Entropy: 1.790377.\n",
      "Iteration 252: Policy loss: -19.571602. Value loss: 19.242657. Entropy: 1.790335.\n",
      "episode: 120   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 687     evaluation reward: 149.95\n",
      "episode: 121   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 533     evaluation reward: 149.45\n",
      "Training network\n",
      "Iteration 253: Policy loss: -7.606161. Value loss: 7.403947. Entropy: 1.790604.\n",
      "Iteration 254: Policy loss: -7.552315. Value loss: 7.350244. Entropy: 1.790604.\n",
      "Iteration 255: Policy loss: -7.536275. Value loss: 7.335721. Entropy: 1.790620.\n",
      "episode: 122   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 503     evaluation reward: 149.75\n",
      "Training network\n",
      "Iteration 256: Policy loss: -13.211834. Value loss: 12.925099. Entropy: 1.790278.\n",
      "Iteration 257: Policy loss: -13.099756. Value loss: 12.815495. Entropy: 1.790238.\n",
      "Iteration 258: Policy loss: -13.221662. Value loss: 12.932665. Entropy: 1.790285.\n",
      "episode: 123   score: 230.0   memory length: 1024   epsilon: 1.0    steps: 1022     evaluation reward: 150.25\n",
      "episode: 124   score: 60.0   memory length: 1024   epsilon: 1.0    steps: 596     evaluation reward: 149.65\n",
      "Training network\n",
      "Iteration 259: Policy loss: -7.272468. Value loss: 7.173322. Entropy: 1.790554.\n",
      "Iteration 260: Policy loss: -7.340380. Value loss: 7.238640. Entropy: 1.790542.\n",
      "Iteration 261: Policy loss: -7.249034. Value loss: 7.149149. Entropy: 1.790531.\n",
      "episode: 125   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 930     evaluation reward: 150.15\n",
      "Training network\n",
      "Iteration 262: Policy loss: -8.653486. Value loss: 8.407433. Entropy: 1.790326.\n",
      "Iteration 263: Policy loss: -8.608061. Value loss: 8.362885. Entropy: 1.790324.\n",
      "Iteration 264: Policy loss: -8.589588. Value loss: 8.341054. Entropy: 1.790311.\n",
      "episode: 126   score: 130.0   memory length: 1024   epsilon: 1.0    steps: 687     evaluation reward: 149.6\n",
      "Training network\n",
      "Iteration 265: Policy loss: -8.884527. Value loss: 8.560040. Entropy: 1.790400.\n",
      "Iteration 266: Policy loss: -8.955283. Value loss: 8.629497. Entropy: 1.790450.\n",
      "Iteration 267: Policy loss: -8.880413. Value loss: 8.555976. Entropy: 1.790413.\n",
      "episode: 127   score: 150.0   memory length: 1024   epsilon: 1.0    steps: 712     evaluation reward: 150.1\n",
      "episode: 128   score: 70.0   memory length: 1024   epsilon: 1.0    steps: 478     evaluation reward: 149.0\n",
      "Training network\n",
      "Iteration 268: Policy loss: -9.774860. Value loss: 9.454003. Entropy: 1.790365.\n",
      "Iteration 269: Policy loss: -9.754523. Value loss: 9.433978. Entropy: 1.790374.\n",
      "Iteration 270: Policy loss: -9.732427. Value loss: 9.411362. Entropy: 1.790365.\n",
      "episode: 129   score: 70.0   memory length: 1024   epsilon: 1.0    steps: 573     evaluation reward: 148.65\n",
      "episode: 130   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 456     evaluation reward: 149.4\n",
      "Training network\n",
      "Iteration 271: Policy loss: -8.122289. Value loss: 7.932819. Entropy: 1.790425.\n",
      "Iteration 272: Policy loss: -8.059958. Value loss: 7.868383. Entropy: 1.790432.\n",
      "Iteration 273: Policy loss: -8.135002. Value loss: 7.941770. Entropy: 1.790436.\n",
      "episode: 131   score: 260.0   memory length: 1024   epsilon: 1.0    steps: 1027     evaluation reward: 149.95\n",
      "Training network\n",
      "Iteration 274: Policy loss: -18.584513. Value loss: 18.274544. Entropy: 1.790421.\n",
      "Iteration 275: Policy loss: -18.643457. Value loss: 18.332468. Entropy: 1.790409.\n",
      "Iteration 276: Policy loss: -18.789543. Value loss: 18.471647. Entropy: 1.790413.\n",
      "episode: 132   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 502     evaluation reward: 147.9\n",
      "episode: 133   score: 60.0   memory length: 1024   epsilon: 1.0    steps: 496     evaluation reward: 146.95\n",
      "Training network\n",
      "Iteration 277: Policy loss: -5.578574. Value loss: 5.472301. Entropy: 1.790282.\n",
      "Iteration 278: Policy loss: -5.585494. Value loss: 5.478331. Entropy: 1.790291.\n",
      "Iteration 279: Policy loss: -5.544972. Value loss: 5.439628. Entropy: 1.790295.\n",
      "episode: 134   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 634     evaluation reward: 146.15\n",
      "Training network\n",
      "Iteration 280: Policy loss: -12.425809. Value loss: 12.306342. Entropy: 1.790480.\n",
      "Iteration 281: Policy loss: -12.404138. Value loss: 12.283685. Entropy: 1.790531.\n",
      "Iteration 282: Policy loss: -12.275546. Value loss: 12.156773. Entropy: 1.790497.\n",
      "episode: 135   score: 255.0   memory length: 1024   epsilon: 1.0    steps: 980     evaluation reward: 148.15\n",
      "Training network\n",
      "Iteration 283: Policy loss: -8.019298. Value loss: 7.665840. Entropy: 1.790310.\n",
      "Iteration 284: Policy loss: -8.050323. Value loss: 7.694059. Entropy: 1.790310.\n",
      "Iteration 285: Policy loss: -8.097729. Value loss: 7.740323. Entropy: 1.790298.\n",
      "episode: 136   score: 165.0   memory length: 1024   epsilon: 1.0    steps: 936     evaluation reward: 148.45\n",
      "episode: 137   score: 90.0   memory length: 1024   epsilon: 1.0    steps: 402     evaluation reward: 148.3\n",
      "Training network\n",
      "Iteration 286: Policy loss: -7.458295. Value loss: 7.243220. Entropy: 1.790433.\n",
      "Iteration 287: Policy loss: -7.402360. Value loss: 7.185714. Entropy: 1.790425.\n",
      "Iteration 288: Policy loss: -7.438969. Value loss: 7.219042. Entropy: 1.790447.\n",
      "episode: 138   score: 140.0   memory length: 1024   epsilon: 1.0    steps: 706     evaluation reward: 148.15\n",
      "Training network\n",
      "Iteration 289: Policy loss: -33.346081. Value loss: 32.935741. Entropy: 1.790418.\n",
      "Iteration 290: Policy loss: -33.826893. Value loss: 33.407852. Entropy: 1.790413.\n",
      "Iteration 291: Policy loss: -33.446846. Value loss: 33.023418. Entropy: 1.790411.\n",
      "episode: 139   score: 515.0   memory length: 1024   epsilon: 1.0    steps: 953     evaluation reward: 152.5\n",
      "now time :  2018-12-19 13:11:58.344005\n",
      "episode: 140   score: 150.0   memory length: 1024   epsilon: 1.0    steps: 656     evaluation reward: 153.8\n",
      "Training network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 292: Policy loss: -11.864009. Value loss: 11.634341. Entropy: 1.790278.\n",
      "Iteration 293: Policy loss: -11.960702. Value loss: 11.725328. Entropy: 1.790285.\n",
      "Iteration 294: Policy loss: -11.827771. Value loss: 11.597469. Entropy: 1.790292.\n",
      "episode: 141   score: 100.0   memory length: 1024   epsilon: 1.0    steps: 625     evaluation reward: 152.65\n",
      "episode: 142   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 553     evaluation reward: 152.9\n",
      "Training network\n",
      "Iteration 295: Policy loss: -9.150613. Value loss: 8.996709. Entropy: 1.790313.\n",
      "Iteration 296: Policy loss: -9.135756. Value loss: 8.981361. Entropy: 1.790288.\n",
      "Iteration 297: Policy loss: -9.171033. Value loss: 9.014949. Entropy: 1.790341.\n",
      "episode: 143   score: 295.0   memory length: 1024   epsilon: 1.0    steps: 977     evaluation reward: 154.85\n",
      "Training network\n",
      "Iteration 298: Policy loss: -17.562700. Value loss: 17.257673. Entropy: 1.790212.\n",
      "Iteration 299: Policy loss: -17.664442. Value loss: 17.353514. Entropy: 1.790224.\n",
      "Iteration 300: Policy loss: -17.728264. Value loss: 17.416981. Entropy: 1.790210.\n",
      "episode: 144   score: 30.0   memory length: 1024   epsilon: 1.0    steps: 504     evaluation reward: 154.1\n",
      "Training network\n",
      "Iteration 301: Policy loss: -6.358027. Value loss: 6.177408. Entropy: 1.790561.\n",
      "Iteration 302: Policy loss: -6.368221. Value loss: 6.185368. Entropy: 1.790519.\n",
      "Iteration 303: Policy loss: -6.507759. Value loss: 6.322543. Entropy: 1.790573.\n",
      "episode: 145   score: 165.0   memory length: 1024   epsilon: 1.0    steps: 791     evaluation reward: 153.35\n",
      "episode: 146   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 631     evaluation reward: 153.0\n",
      "Training network\n",
      "Iteration 304: Policy loss: -12.873081. Value loss: 12.554512. Entropy: 1.790472.\n",
      "Iteration 305: Policy loss: -12.774025. Value loss: 12.454170. Entropy: 1.790535.\n",
      "Iteration 306: Policy loss: -12.947244. Value loss: 12.623614. Entropy: 1.790528.\n",
      "episode: 147   score: 90.0   memory length: 1024   epsilon: 1.0    steps: 451     evaluation reward: 152.8\n",
      "episode: 148   score: 85.0   memory length: 1024   epsilon: 1.0    steps: 509     evaluation reward: 152.8\n",
      "Training network\n",
      "Iteration 307: Policy loss: -5.362844. Value loss: 5.174002. Entropy: 1.790405.\n",
      "Iteration 308: Policy loss: -5.315410. Value loss: 5.127190. Entropy: 1.790447.\n",
      "Iteration 309: Policy loss: -5.377283. Value loss: 5.184627. Entropy: 1.790424.\n",
      "episode: 149   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 757     evaluation reward: 153.6\n",
      "Training network\n",
      "Iteration 310: Policy loss: -14.126121. Value loss: 13.921515. Entropy: 1.790428.\n",
      "Iteration 311: Policy loss: -13.878587. Value loss: 13.676039. Entropy: 1.790399.\n",
      "Iteration 312: Policy loss: -14.136414. Value loss: 13.930224. Entropy: 1.790370.\n",
      "episode: 150   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 630     evaluation reward: 153.85\n",
      "episode: 151   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 678     evaluation reward: 153.9\n",
      "Training network\n",
      "Iteration 313: Policy loss: -9.165389. Value loss: 8.858174. Entropy: 1.790218.\n",
      "Iteration 314: Policy loss: -9.152143. Value loss: 8.843746. Entropy: 1.790271.\n",
      "Iteration 315: Policy loss: -9.170843. Value loss: 8.860412. Entropy: 1.790275.\n",
      "episode: 152   score: 20.0   memory length: 1024   epsilon: 1.0    steps: 437     evaluation reward: 153.35\n",
      "episode: 153   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 635     evaluation reward: 153.5\n",
      "Training network\n",
      "Iteration 316: Policy loss: -8.813734. Value loss: 8.713075. Entropy: 1.790433.\n",
      "Iteration 317: Policy loss: -8.895487. Value loss: 8.793457. Entropy: 1.790482.\n",
      "Iteration 318: Policy loss: -9.097528. Value loss: 8.992774. Entropy: 1.790456.\n",
      "episode: 154   score: 30.0   memory length: 1024   epsilon: 1.0    steps: 402     evaluation reward: 152.55\n",
      "episode: 155   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 767     evaluation reward: 152.2\n",
      "Training network\n",
      "Iteration 319: Policy loss: -13.288488. Value loss: 13.029659. Entropy: 1.790545.\n",
      "Iteration 320: Policy loss: -12.954120. Value loss: 12.700282. Entropy: 1.790536.\n",
      "Iteration 321: Policy loss: -13.324440. Value loss: 13.059760. Entropy: 1.790518.\n",
      "Training network\n",
      "Iteration 322: Policy loss: -14.387289. Value loss: 14.030826. Entropy: 1.790331.\n",
      "Iteration 323: Policy loss: -14.328740. Value loss: 13.972461. Entropy: 1.790368.\n",
      "Iteration 324: Policy loss: -14.391458. Value loss: 14.032472. Entropy: 1.790316.\n",
      "episode: 156   score: 275.0   memory length: 1024   epsilon: 1.0    steps: 1263     evaluation reward: 154.25\n",
      "Training network\n",
      "Iteration 325: Policy loss: -13.848778. Value loss: 13.490435. Entropy: 1.790172.\n",
      "Iteration 326: Policy loss: -13.895309. Value loss: 13.540242. Entropy: 1.790158.\n",
      "Iteration 327: Policy loss: -13.725800. Value loss: 13.369133. Entropy: 1.790161.\n",
      "episode: 157   score: 240.0   memory length: 1024   epsilon: 1.0    steps: 987     evaluation reward: 154.55\n",
      "episode: 158   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 620     evaluation reward: 152.6\n",
      "Training network\n",
      "Iteration 328: Policy loss: -11.840306. Value loss: 11.533511. Entropy: 1.790394.\n",
      "Iteration 329: Policy loss: -11.881376. Value loss: 11.571406. Entropy: 1.790404.\n",
      "Iteration 330: Policy loss: -11.985114. Value loss: 11.674368. Entropy: 1.790367.\n",
      "episode: 159   score: 145.0   memory length: 1024   epsilon: 1.0    steps: 789     evaluation reward: 153.5\n",
      "Training network\n",
      "Iteration 331: Policy loss: -11.516685. Value loss: 11.140177. Entropy: 1.790365.\n",
      "Iteration 332: Policy loss: -11.454541. Value loss: 11.078059. Entropy: 1.790380.\n",
      "Iteration 333: Policy loss: -11.584124. Value loss: 11.201564. Entropy: 1.790378.\n",
      "episode: 160   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 806     evaluation reward: 154.5\n",
      "episode: 161   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 556     evaluation reward: 154.6\n",
      "Training network\n",
      "Iteration 334: Policy loss: -11.037898. Value loss: 10.761591. Entropy: 1.790592.\n",
      "Iteration 335: Policy loss: -10.976975. Value loss: 10.697786. Entropy: 1.790628.\n",
      "Iteration 336: Policy loss: -11.101888. Value loss: 10.822636. Entropy: 1.790627.\n",
      "episode: 162   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 634     evaluation reward: 153.5\n",
      "Training network\n",
      "Iteration 337: Policy loss: -6.608340. Value loss: 6.517105. Entropy: 1.790475.\n",
      "Iteration 338: Policy loss: -6.718925. Value loss: 6.623870. Entropy: 1.790421.\n",
      "Iteration 339: Policy loss: -6.733133. Value loss: 6.636642. Entropy: 1.790409.\n",
      "episode: 163   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 544     evaluation reward: 152.95\n",
      "episode: 164   score: 140.0   memory length: 1024   epsilon: 1.0    steps: 958     evaluation reward: 153.0\n",
      "Training network\n",
      "Iteration 340: Policy loss: -8.457984. Value loss: 8.206462. Entropy: 1.790249.\n",
      "Iteration 341: Policy loss: -8.429720. Value loss: 8.175456. Entropy: 1.790241.\n",
      "Iteration 342: Policy loss: -8.606147. Value loss: 8.345461. Entropy: 1.790231.\n",
      "episode: 165   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 639     evaluation reward: 150.55\n",
      "Training network\n",
      "Iteration 343: Policy loss: -6.292218. Value loss: 6.177113. Entropy: 1.790343.\n",
      "Iteration 344: Policy loss: -6.275617. Value loss: 6.162358. Entropy: 1.790332.\n",
      "Iteration 345: Policy loss: -6.328030. Value loss: 6.213643. Entropy: 1.790352.\n",
      "episode: 166   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 646     evaluation reward: 150.15\n",
      "episode: 167   score: 160.0   memory length: 1024   epsilon: 1.0    steps: 688     evaluation reward: 151.0\n",
      "Training network\n",
      "Iteration 346: Policy loss: -8.956273. Value loss: 8.743090. Entropy: 1.790540.\n",
      "Iteration 347: Policy loss: -8.859385. Value loss: 8.647417. Entropy: 1.790533.\n",
      "Iteration 348: Policy loss: -8.915256. Value loss: 8.702336. Entropy: 1.790541.\n",
      "Training network\n",
      "Iteration 349: Policy loss: -11.919481. Value loss: 11.584221. Entropy: 1.790324.\n",
      "Iteration 350: Policy loss: -11.877792. Value loss: 11.542554. Entropy: 1.790313.\n",
      "Iteration 351: Policy loss: -11.740632. Value loss: 11.405676. Entropy: 1.790320.\n",
      "episode: 168   score: 225.0   memory length: 1024   epsilon: 1.0    steps: 1469     evaluation reward: 152.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 169   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 594     evaluation reward: 152.95\n",
      "Training network\n",
      "Iteration 352: Policy loss: -5.483321. Value loss: 5.197840. Entropy: 1.790164.\n",
      "Iteration 353: Policy loss: -5.456127. Value loss: 5.170141. Entropy: 1.790117.\n",
      "Iteration 354: Policy loss: -5.576268. Value loss: 5.287753. Entropy: 1.790116.\n",
      "episode: 170   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 517     evaluation reward: 152.7\n",
      "Training network\n",
      "Iteration 355: Policy loss: -8.815100. Value loss: 8.541373. Entropy: 1.790378.\n",
      "Iteration 356: Policy loss: -8.786934. Value loss: 8.513477. Entropy: 1.790404.\n",
      "Iteration 357: Policy loss: -8.698499. Value loss: 8.423737. Entropy: 1.790358.\n",
      "episode: 171   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 664     evaluation reward: 150.6\n",
      "Training network\n",
      "Iteration 358: Policy loss: -11.218341. Value loss: 11.016890. Entropy: 1.790445.\n",
      "Iteration 359: Policy loss: -11.337604. Value loss: 11.128952. Entropy: 1.790412.\n",
      "Iteration 360: Policy loss: -11.120334. Value loss: 10.916827. Entropy: 1.790424.\n",
      "episode: 172   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 1156     evaluation reward: 152.05\n",
      "episode: 173   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 393     evaluation reward: 150.35\n",
      "Training network\n",
      "Iteration 361: Policy loss: -7.001563. Value loss: 6.868566. Entropy: 1.790302.\n",
      "Iteration 362: Policy loss: -7.068974. Value loss: 6.931168. Entropy: 1.790296.\n",
      "Iteration 363: Policy loss: -6.979692. Value loss: 6.846613. Entropy: 1.790344.\n",
      "episode: 174   score: 260.0   memory length: 1024   epsilon: 1.0    steps: 910     evaluation reward: 148.95\n",
      "episode: 175   score: 20.0   memory length: 1024   epsilon: 1.0    steps: 398     evaluation reward: 148.05\n",
      "Training network\n",
      "Iteration 364: Policy loss: -10.714694. Value loss: 10.505335. Entropy: 1.790456.\n",
      "Iteration 365: Policy loss: -10.689973. Value loss: 10.479439. Entropy: 1.790514.\n",
      "Iteration 366: Policy loss: -10.539512. Value loss: 10.327890. Entropy: 1.790509.\n",
      "episode: 176   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 608     evaluation reward: 148.0\n",
      "Training network\n",
      "Iteration 367: Policy loss: -7.238986. Value loss: 7.059296. Entropy: 1.790200.\n",
      "Iteration 368: Policy loss: -7.305084. Value loss: 7.126554. Entropy: 1.790146.\n",
      "Iteration 369: Policy loss: -7.284335. Value loss: 7.104215. Entropy: 1.790202.\n",
      "episode: 177   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 825     evaluation reward: 146.1\n",
      "episode: 178   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 590     evaluation reward: 146.05\n",
      "Training network\n",
      "Iteration 370: Policy loss: -12.454034. Value loss: 12.302235. Entropy: 1.790545.\n",
      "Iteration 371: Policy loss: -12.484404. Value loss: 12.331414. Entropy: 1.790548.\n",
      "Iteration 372: Policy loss: -12.245196. Value loss: 12.094800. Entropy: 1.790591.\n",
      "episode: 179   score: 255.0   memory length: 1024   epsilon: 1.0    steps: 1000     evaluation reward: 147.85\n",
      "Training network\n",
      "Iteration 373: Policy loss: -9.635161. Value loss: 9.340893. Entropy: 1.790316.\n",
      "Iteration 374: Policy loss: -9.683533. Value loss: 9.380881. Entropy: 1.790277.\n",
      "Iteration 375: Policy loss: -9.514461. Value loss: 9.216106. Entropy: 1.790304.\n",
      "episode: 180   score: 35.0   memory length: 1024   epsilon: 1.0    steps: 506     evaluation reward: 147.4\n",
      "episode: 181   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 463     evaluation reward: 146.3\n",
      "Training network\n",
      "Iteration 376: Policy loss: -7.044686. Value loss: 6.835480. Entropy: 1.790381.\n",
      "Iteration 377: Policy loss: -7.094124. Value loss: 6.879450. Entropy: 1.790343.\n",
      "Iteration 378: Policy loss: -7.068050. Value loss: 6.853498. Entropy: 1.790405.\n",
      "episode: 182   score: 70.0   memory length: 1024   epsilon: 1.0    steps: 682     evaluation reward: 145.95\n",
      "episode: 183   score: 130.0   memory length: 1024   epsilon: 1.0    steps: 387     evaluation reward: 145.1\n",
      "Training network\n",
      "Iteration 379: Policy loss: -9.185627. Value loss: 8.877184. Entropy: 1.790429.\n",
      "Iteration 380: Policy loss: -9.247113. Value loss: 8.930112. Entropy: 1.790443.\n",
      "Iteration 381: Policy loss: -9.197087. Value loss: 8.880151. Entropy: 1.790457.\n",
      "episode: 184   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 559     evaluation reward: 145.2\n",
      "episode: 185   score: 30.0   memory length: 1024   epsilon: 1.0    steps: 577     evaluation reward: 144.45\n",
      "Training network\n",
      "Iteration 382: Policy loss: -4.149274. Value loss: 4.076388. Entropy: 1.790393.\n",
      "Iteration 383: Policy loss: -4.170166. Value loss: 4.094737. Entropy: 1.790408.\n",
      "Iteration 384: Policy loss: -4.085016. Value loss: 4.012465. Entropy: 1.790420.\n",
      "episode: 186   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 395     evaluation reward: 143.3\n",
      "episode: 187   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 496     evaluation reward: 139.5\n",
      "Training network\n",
      "Iteration 385: Policy loss: -7.974261. Value loss: 7.805552. Entropy: 1.790518.\n",
      "Iteration 386: Policy loss: -8.078700. Value loss: 7.905421. Entropy: 1.790480.\n",
      "Iteration 387: Policy loss: -8.053207. Value loss: 7.882273. Entropy: 1.790498.\n",
      "episode: 188   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 645     evaluation reward: 139.2\n",
      "Training network\n",
      "Iteration 388: Policy loss: -12.516255. Value loss: 12.170342. Entropy: 1.790224.\n",
      "Iteration 389: Policy loss: -12.527765. Value loss: 12.181141. Entropy: 1.790208.\n",
      "Iteration 390: Policy loss: -12.506688. Value loss: 12.158234. Entropy: 1.790265.\n",
      "episode: 189   score: 200.0   memory length: 1024   epsilon: 1.0    steps: 1083     evaluation reward: 138.9\n",
      "episode: 190   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 530     evaluation reward: 138.75\n",
      "Training network\n",
      "Iteration 391: Policy loss: -6.300798. Value loss: 6.190005. Entropy: 1.790467.\n",
      "Iteration 392: Policy loss: -6.344738. Value loss: 6.232223. Entropy: 1.790488.\n",
      "Iteration 393: Policy loss: -6.269543. Value loss: 6.159793. Entropy: 1.790447.\n",
      "episode: 191   score: 95.0   memory length: 1024   epsilon: 1.0    steps: 625     evaluation reward: 139.05\n",
      "Training network\n",
      "Iteration 394: Policy loss: -7.824476. Value loss: 7.610225. Entropy: 1.790314.\n",
      "Iteration 395: Policy loss: -7.782848. Value loss: 7.571288. Entropy: 1.790300.\n",
      "Iteration 396: Policy loss: -7.679885. Value loss: 7.468114. Entropy: 1.790310.\n",
      "episode: 192   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 536     evaluation reward: 138.9\n",
      "episode: 193   score: 185.0   memory length: 1024   epsilon: 1.0    steps: 780     evaluation reward: 139.7\n",
      "Training network\n",
      "Iteration 397: Policy loss: -12.773623. Value loss: 12.440772. Entropy: 1.790537.\n",
      "Iteration 398: Policy loss: -12.894135. Value loss: 12.557126. Entropy: 1.790540.\n",
      "Iteration 399: Policy loss: -12.810287. Value loss: 12.473157. Entropy: 1.790565.\n",
      "episode: 194   score: 250.0   memory length: 1024   epsilon: 1.0    steps: 860     evaluation reward: 139.7\n",
      "Training network\n",
      "Iteration 400: Policy loss: -12.359326. Value loss: 12.007442. Entropy: 1.790443.\n",
      "Iteration 401: Policy loss: -12.299788. Value loss: 11.949519. Entropy: 1.790467.\n",
      "Iteration 402: Policy loss: -12.285137. Value loss: 11.933347. Entropy: 1.790461.\n",
      "episode: 195   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 664     evaluation reward: 139.85\n",
      "episode: 196   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 559     evaluation reward: 139.9\n",
      "Training network\n",
      "Iteration 403: Policy loss: -8.424894. Value loss: 8.331262. Entropy: 1.790385.\n",
      "Iteration 404: Policy loss: -8.390899. Value loss: 8.296201. Entropy: 1.790388.\n",
      "Iteration 405: Policy loss: -8.314581. Value loss: 8.221781. Entropy: 1.790419.\n",
      "episode: 197   score: 90.0   memory length: 1024   epsilon: 1.0    steps: 396     evaluation reward: 139.75\n",
      "episode: 198   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 610     evaluation reward: 139.75\n",
      "Training network\n",
      "Iteration 406: Policy loss: -7.919176. Value loss: 7.807858. Entropy: 1.790562.\n",
      "Iteration 407: Policy loss: -7.945678. Value loss: 7.834156. Entropy: 1.790585.\n",
      "Iteration 408: Policy loss: -7.963135. Value loss: 7.852848. Entropy: 1.790535.\n",
      "episode: 199   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 648     evaluation reward: 140.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network\n",
      "Iteration 409: Policy loss: -10.127960. Value loss: 9.884206. Entropy: 1.790458.\n",
      "Iteration 410: Policy loss: -10.143192. Value loss: 9.900547. Entropy: 1.790484.\n",
      "Iteration 411: Policy loss: -10.250078. Value loss: 10.001839. Entropy: 1.790487.\n",
      "episode: 200   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 897     evaluation reward: 141.55\n",
      "episode: 201   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 511     evaluation reward: 141.0\n",
      "Training network\n",
      "Iteration 412: Policy loss: -13.018665. Value loss: 12.868150. Entropy: 1.790340.\n",
      "Iteration 413: Policy loss: -12.793745. Value loss: 12.647044. Entropy: 1.790326.\n",
      "Iteration 414: Policy loss: -12.914585. Value loss: 12.766488. Entropy: 1.790329.\n",
      "episode: 202   score: 335.0   memory length: 1024   epsilon: 1.0    steps: 833     evaluation reward: 142.8\n",
      "Training network\n",
      "Iteration 415: Policy loss: -21.584349. Value loss: 21.303173. Entropy: 1.790346.\n",
      "Iteration 416: Policy loss: -21.189678. Value loss: 20.906645. Entropy: 1.790361.\n",
      "Iteration 417: Policy loss: -21.579842. Value loss: 21.286173. Entropy: 1.790342.\n",
      "episode: 203   score: 35.0   memory length: 1024   epsilon: 1.0    steps: 513     evaluation reward: 138.75\n",
      "episode: 204   score: 70.0   memory length: 1024   epsilon: 1.0    steps: 685     evaluation reward: 137.65\n",
      "Training network\n",
      "Iteration 418: Policy loss: -3.676683. Value loss: 3.721492. Entropy: 1.790639.\n",
      "Iteration 419: Policy loss: -3.755744. Value loss: 3.799686. Entropy: 1.790635.\n",
      "Iteration 420: Policy loss: -3.655924. Value loss: 3.700559. Entropy: 1.790642.\n",
      "episode: 205   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 614     evaluation reward: 136.3\n",
      "Training network\n",
      "Iteration 421: Policy loss: -12.913576. Value loss: 12.698210. Entropy: 1.790405.\n",
      "Iteration 422: Policy loss: -12.784133. Value loss: 12.569287. Entropy: 1.790425.\n",
      "Iteration 423: Policy loss: -12.775678. Value loss: 12.561582. Entropy: 1.790427.\n",
      "episode: 206   score: 360.0   memory length: 1024   epsilon: 1.0    steps: 897     evaluation reward: 139.75\n",
      "episode: 207   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 570     evaluation reward: 139.25\n",
      "Training network\n",
      "Iteration 424: Policy loss: -12.168537. Value loss: 11.922620. Entropy: 1.790258.\n",
      "Iteration 425: Policy loss: -12.188925. Value loss: 11.940749. Entropy: 1.790243.\n",
      "Iteration 426: Policy loss: -12.331354. Value loss: 12.077474. Entropy: 1.790265.\n",
      "episode: 208   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 386     evaluation reward: 138.25\n",
      "episode: 209   score: 70.0   memory length: 1024   epsilon: 1.0    steps: 504     evaluation reward: 136.65\n",
      "Training network\n",
      "Iteration 427: Policy loss: -6.473193. Value loss: 6.370398. Entropy: 1.790435.\n",
      "Iteration 428: Policy loss: -6.563095. Value loss: 6.453774. Entropy: 1.790448.\n",
      "Iteration 429: Policy loss: -6.559084. Value loss: 6.449803. Entropy: 1.790473.\n",
      "episode: 210   score: 170.0   memory length: 1024   epsilon: 1.0    steps: 777     evaluation reward: 134.25\n",
      "Training network\n",
      "Iteration 430: Policy loss: -10.247089. Value loss: 9.884248. Entropy: 1.790475.\n",
      "Iteration 431: Policy loss: -10.215112. Value loss: 9.847810. Entropy: 1.790498.\n",
      "Iteration 432: Policy loss: -10.274522. Value loss: 9.908363. Entropy: 1.790490.\n",
      "episode: 211   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 755     evaluation reward: 135.6\n",
      "episode: 212   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 631     evaluation reward: 135.9\n",
      "Training network\n",
      "Iteration 433: Policy loss: -16.301531. Value loss: 16.039188. Entropy: 1.790442.\n",
      "Iteration 434: Policy loss: -16.430649. Value loss: 16.166872. Entropy: 1.790426.\n",
      "Iteration 435: Policy loss: -16.363476. Value loss: 16.098804. Entropy: 1.790417.\n",
      "episode: 213   score: 285.0   memory length: 1024   epsilon: 1.0    steps: 942     evaluation reward: 138.0\n",
      "Training network\n",
      "Iteration 436: Policy loss: -14.239268. Value loss: 14.024804. Entropy: 1.790428.\n",
      "Iteration 437: Policy loss: -14.173157. Value loss: 13.955087. Entropy: 1.790399.\n",
      "Iteration 438: Policy loss: -14.227079. Value loss: 14.008422. Entropy: 1.790414.\n",
      "episode: 214   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 502     evaluation reward: 138.55\n",
      "now time :  2018-12-19 13:14:53.613308\n",
      "Training network\n",
      "Iteration 439: Policy loss: -10.315495. Value loss: 10.105200. Entropy: 1.790136.\n",
      "Iteration 440: Policy loss: -10.278177. Value loss: 10.073514. Entropy: 1.790184.\n",
      "Iteration 441: Policy loss: -10.362230. Value loss: 10.152994. Entropy: 1.790238.\n",
      "episode: 215   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 807     evaluation reward: 138.5\n",
      "Training network\n",
      "Iteration 442: Policy loss: -21.156506. Value loss: 20.894753. Entropy: 1.790349.\n",
      "Iteration 443: Policy loss: -21.751287. Value loss: 21.476810. Entropy: 1.790314.\n",
      "Iteration 444: Policy loss: -21.555876. Value loss: 21.278120. Entropy: 1.790342.\n",
      "episode: 216   score: 380.0   memory length: 1024   epsilon: 1.0    steps: 1210     evaluation reward: 139.45\n",
      "episode: 217   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 588     evaluation reward: 139.45\n",
      "Training network\n",
      "Iteration 445: Policy loss: -11.463251. Value loss: 11.158766. Entropy: 1.790288.\n",
      "Iteration 446: Policy loss: -11.570861. Value loss: 11.258238. Entropy: 1.790351.\n",
      "Iteration 447: Policy loss: -11.745126. Value loss: 11.429776. Entropy: 1.790336.\n",
      "episode: 218   score: 255.0   memory length: 1024   epsilon: 1.0    steps: 1018     evaluation reward: 140.95\n",
      "Training network\n",
      "Iteration 448: Policy loss: -10.343591. Value loss: 9.992590. Entropy: 1.790534.\n",
      "Iteration 449: Policy loss: -10.307405. Value loss: 9.954102. Entropy: 1.790577.\n",
      "Iteration 450: Policy loss: -10.302368. Value loss: 9.949632. Entropy: 1.790545.\n",
      "episode: 219   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 703     evaluation reward: 139.7\n",
      "episode: 220   score: 45.0   memory length: 1024   epsilon: 1.0    steps: 495     evaluation reward: 138.6\n",
      "Training network\n",
      "Iteration 451: Policy loss: -6.313198. Value loss: 6.054681. Entropy: 1.790317.\n",
      "Iteration 452: Policy loss: -6.352986. Value loss: 6.094442. Entropy: 1.790323.\n",
      "Iteration 453: Policy loss: -6.297508. Value loss: 6.040275. Entropy: 1.790324.\n",
      "episode: 221   score: 140.0   memory length: 1024   epsilon: 1.0    steps: 672     evaluation reward: 138.95\n",
      "Training network\n",
      "Iteration 454: Policy loss: -12.097273. Value loss: 11.918062. Entropy: 1.790451.\n",
      "Iteration 455: Policy loss: -11.987257. Value loss: 11.809650. Entropy: 1.790473.\n",
      "Iteration 456: Policy loss: -12.158436. Value loss: 11.973291. Entropy: 1.790468.\n",
      "episode: 222   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 682     evaluation reward: 139.2\n",
      "Training network\n",
      "Iteration 457: Policy loss: -11.378976. Value loss: 11.257991. Entropy: 1.790651.\n",
      "Iteration 458: Policy loss: -11.471796. Value loss: 11.346524. Entropy: 1.790672.\n",
      "Iteration 459: Policy loss: -11.561964. Value loss: 11.432421. Entropy: 1.790661.\n",
      "episode: 223   score: 225.0   memory length: 1024   epsilon: 1.0    steps: 934     evaluation reward: 139.15\n",
      "Training network\n",
      "Iteration 460: Policy loss: -11.417302. Value loss: 11.187064. Entropy: 1.790451.\n",
      "Iteration 461: Policy loss: -11.435159. Value loss: 11.206315. Entropy: 1.790434.\n",
      "Iteration 462: Policy loss: -11.320612. Value loss: 11.090209. Entropy: 1.790452.\n",
      "episode: 224   score: 185.0   memory length: 1024   epsilon: 1.0    steps: 796     evaluation reward: 140.4\n",
      "episode: 225   score: 45.0   memory length: 1024   epsilon: 1.0    steps: 485     evaluation reward: 139.3\n",
      "Training network\n",
      "Iteration 463: Policy loss: -5.822923. Value loss: 5.562879. Entropy: 1.790323.\n",
      "Iteration 464: Policy loss: -5.877948. Value loss: 5.613056. Entropy: 1.790342.\n",
      "Iteration 465: Policy loss: -5.782366. Value loss: 5.523992. Entropy: 1.790321.\n",
      "episode: 226   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 782     evaluation reward: 138.65\n",
      "Training network\n",
      "Iteration 466: Policy loss: -8.076653. Value loss: 8.004684. Entropy: 1.790291.\n",
      "Iteration 467: Policy loss: -8.152734. Value loss: 8.074888. Entropy: 1.790335.\n",
      "Iteration 468: Policy loss: -8.244196. Value loss: 8.164575. Entropy: 1.790265.\n",
      "episode: 227   score: 275.0   memory length: 1024   epsilon: 1.0    steps: 1502     evaluation reward: 139.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network\n",
      "Iteration 469: Policy loss: -9.090879. Value loss: 8.889984. Entropy: 1.789951.\n",
      "Iteration 470: Policy loss: -9.261212. Value loss: 9.052521. Entropy: 1.789976.\n",
      "Iteration 471: Policy loss: -9.067354. Value loss: 8.864246. Entropy: 1.789935.\n",
      "episode: 228   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 609     evaluation reward: 139.75\n",
      "episode: 229   score: 90.0   memory length: 1024   epsilon: 1.0    steps: 551     evaluation reward: 139.95\n",
      "Training network\n",
      "Iteration 472: Policy loss: -4.740092. Value loss: 4.624137. Entropy: 1.790464.\n",
      "Iteration 473: Policy loss: -4.740327. Value loss: 4.621662. Entropy: 1.790450.\n",
      "Iteration 474: Policy loss: -4.717375. Value loss: 4.601945. Entropy: 1.790468.\n",
      "episode: 230   score: 410.0   memory length: 1024   epsilon: 1.0    steps: 1039     evaluation reward: 142.8\n",
      "Training network\n",
      "Iteration 475: Policy loss: -22.973169. Value loss: 22.628332. Entropy: 1.790506.\n",
      "Iteration 476: Policy loss: -23.199913. Value loss: 22.850729. Entropy: 1.790488.\n",
      "Iteration 477: Policy loss: -23.277708. Value loss: 22.921881. Entropy: 1.790523.\n",
      "episode: 231   score: 370.0   memory length: 1024   epsilon: 1.0    steps: 893     evaluation reward: 143.9\n",
      "Training network\n",
      "Iteration 478: Policy loss: -22.100937. Value loss: 21.924639. Entropy: 1.790393.\n",
      "Iteration 479: Policy loss: -22.627674. Value loss: 22.446325. Entropy: 1.790385.\n",
      "Iteration 480: Policy loss: -22.253277. Value loss: 22.060322. Entropy: 1.790330.\n",
      "episode: 232   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 387     evaluation reward: 143.35\n",
      "episode: 233   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 495     evaluation reward: 143.55\n",
      "Training network\n",
      "Iteration 481: Policy loss: -12.412041. Value loss: 12.168381. Entropy: 1.790442.\n",
      "Iteration 482: Policy loss: -12.352676. Value loss: 12.106093. Entropy: 1.790470.\n",
      "Iteration 483: Policy loss: -12.427243. Value loss: 12.177462. Entropy: 1.790453.\n",
      "episode: 234   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 793     evaluation reward: 145.1\n",
      "episode: 235   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 378     evaluation reward: 143.05\n",
      "Training network\n",
      "Iteration 484: Policy loss: -8.348981. Value loss: 8.156790. Entropy: 1.790511.\n",
      "Iteration 485: Policy loss: -8.444190. Value loss: 8.248628. Entropy: 1.790501.\n",
      "Iteration 486: Policy loss: -8.361879. Value loss: 8.166189. Entropy: 1.790522.\n",
      "episode: 236   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 385     evaluation reward: 142.05\n",
      "episode: 237   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 779     evaluation reward: 142.7\n",
      "Training network\n",
      "Iteration 487: Policy loss: -9.822114. Value loss: 9.690995. Entropy: 1.790504.\n",
      "Iteration 488: Policy loss: -9.737577. Value loss: 9.604703. Entropy: 1.790510.\n",
      "Iteration 489: Policy loss: -9.727346. Value loss: 9.592220. Entropy: 1.790497.\n",
      "episode: 238   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 610     evaluation reward: 142.4\n",
      "Training network\n",
      "Iteration 490: Policy loss: -12.965637. Value loss: 12.743013. Entropy: 1.790442.\n",
      "Iteration 491: Policy loss: -12.908349. Value loss: 12.687879. Entropy: 1.790443.\n",
      "Iteration 492: Policy loss: -12.926219. Value loss: 12.699971. Entropy: 1.790437.\n",
      "episode: 239   score: 150.0   memory length: 1024   epsilon: 1.0    steps: 720     evaluation reward: 138.75\n",
      "episode: 240   score: 145.0   memory length: 1024   epsilon: 1.0    steps: 844     evaluation reward: 138.7\n",
      "Training network\n",
      "Iteration 493: Policy loss: -7.695022. Value loss: 7.553702. Entropy: 1.790201.\n",
      "Iteration 494: Policy loss: -7.689818. Value loss: 7.549747. Entropy: 1.790194.\n",
      "Iteration 495: Policy loss: -7.657091. Value loss: 7.517750. Entropy: 1.790139.\n",
      "episode: 241   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 575     evaluation reward: 139.05\n",
      "Training network\n",
      "Iteration 496: Policy loss: -8.827628. Value loss: 8.669289. Entropy: 1.790464.\n",
      "Iteration 497: Policy loss: -8.767348. Value loss: 8.612663. Entropy: 1.790478.\n",
      "Iteration 498: Policy loss: -8.700253. Value loss: 8.545606. Entropy: 1.790495.\n",
      "episode: 242   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 604     evaluation reward: 138.8\n",
      "episode: 243   score: 45.0   memory length: 1024   epsilon: 1.0    steps: 498     evaluation reward: 136.3\n",
      "Training network\n",
      "Iteration 499: Policy loss: -8.907273. Value loss: 8.798976. Entropy: 1.790448.\n",
      "Iteration 500: Policy loss: -9.059238. Value loss: 8.945912. Entropy: 1.790479.\n",
      "Iteration 501: Policy loss: -9.156829. Value loss: 9.040207. Entropy: 1.790491.\n",
      "episode: 244   score: 270.0   memory length: 1024   epsilon: 1.0    steps: 905     evaluation reward: 138.7\n",
      "Training network\n",
      "Iteration 502: Policy loss: -6.863802. Value loss: 6.814303. Entropy: 1.790548.\n",
      "Iteration 503: Policy loss: -6.841867. Value loss: 6.797820. Entropy: 1.790537.\n",
      "Iteration 504: Policy loss: -6.934916. Value loss: 6.887042. Entropy: 1.790589.\n",
      "episode: 245   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 596     evaluation reward: 137.2\n",
      "episode: 246   score: 150.0   memory length: 1024   epsilon: 1.0    steps: 697     evaluation reward: 137.5\n",
      "Training network\n",
      "Iteration 505: Policy loss: -11.288594. Value loss: 11.059974. Entropy: 1.790476.\n",
      "Iteration 506: Policy loss: -11.162179. Value loss: 10.936558. Entropy: 1.790443.\n",
      "Iteration 507: Policy loss: -11.247483. Value loss: 11.019091. Entropy: 1.790449.\n",
      "episode: 247   score: 60.0   memory length: 1024   epsilon: 1.0    steps: 647     evaluation reward: 137.2\n",
      "episode: 248   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 507     evaluation reward: 137.55\n",
      "Training network\n",
      "Iteration 508: Policy loss: -6.762488. Value loss: 6.651815. Entropy: 1.790330.\n",
      "Iteration 509: Policy loss: -6.818196. Value loss: 6.705908. Entropy: 1.790375.\n",
      "Iteration 510: Policy loss: -6.842810. Value loss: 6.728424. Entropy: 1.790355.\n",
      "episode: 249   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 734     evaluation reward: 136.85\n",
      "Training network\n",
      "Iteration 511: Policy loss: -8.350197. Value loss: 8.296750. Entropy: 1.790437.\n",
      "Iteration 512: Policy loss: -8.389541. Value loss: 8.334230. Entropy: 1.790416.\n",
      "Iteration 513: Policy loss: -8.299596. Value loss: 8.246864. Entropy: 1.790394.\n",
      "episode: 250   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 135.85\n",
      "episode: 251   score: 150.0   memory length: 1024   epsilon: 1.0    steps: 598     evaluation reward: 136.0\n",
      "Training network\n",
      "Iteration 514: Policy loss: -14.865096. Value loss: 14.628002. Entropy: 1.790412.\n",
      "Iteration 515: Policy loss: -14.811946. Value loss: 14.574759. Entropy: 1.790439.\n",
      "Iteration 516: Policy loss: -14.899316. Value loss: 14.658426. Entropy: 1.790401.\n",
      "episode: 252   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 625     evaluation reward: 137.9\n",
      "episode: 253   score: 45.0   memory length: 1024   epsilon: 1.0    steps: 500     evaluation reward: 136.8\n",
      "Training network\n",
      "Iteration 517: Policy loss: -6.822471. Value loss: 6.786738. Entropy: 1.790465.\n",
      "Iteration 518: Policy loss: -6.813076. Value loss: 6.775792. Entropy: 1.790463.\n",
      "Iteration 519: Policy loss: -6.808147. Value loss: 6.769978. Entropy: 1.790462.\n",
      "episode: 254   score: 130.0   memory length: 1024   epsilon: 1.0    steps: 819     evaluation reward: 137.8\n",
      "Training network\n",
      "Iteration 520: Policy loss: -9.760262. Value loss: 9.581425. Entropy: 1.790560.\n",
      "Iteration 521: Policy loss: -9.729083. Value loss: 9.551781. Entropy: 1.790552.\n",
      "Iteration 522: Policy loss: -9.722205. Value loss: 9.544899. Entropy: 1.790530.\n",
      "episode: 255   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 546     evaluation reward: 136.75\n",
      "episode: 256   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 627     evaluation reward: 134.8\n",
      "Training network\n",
      "Iteration 523: Policy loss: -6.396864. Value loss: 6.234476. Entropy: 1.790548.\n",
      "Iteration 524: Policy loss: -6.341857. Value loss: 6.182104. Entropy: 1.790529.\n",
      "Iteration 525: Policy loss: -6.380420. Value loss: 6.216962. Entropy: 1.790474.\n",
      "episode: 257   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 563     evaluation reward: 133.05\n",
      "Training network\n",
      "Iteration 526: Policy loss: -13.179385. Value loss: 12.905507. Entropy: 1.790499.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 527: Policy loss: -13.330326. Value loss: 13.051749. Entropy: 1.790506.\n",
      "Iteration 528: Policy loss: -13.072192. Value loss: 12.793443. Entropy: 1.790530.\n",
      "episode: 258   score: 255.0   memory length: 1024   epsilon: 1.0    steps: 911     evaluation reward: 134.25\n",
      "episode: 259   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 635     evaluation reward: 134.9\n",
      "Training network\n",
      "Iteration 529: Policy loss: -17.130033. Value loss: 16.871250. Entropy: 1.790529.\n",
      "Iteration 530: Policy loss: -17.176376. Value loss: 16.909576. Entropy: 1.790534.\n",
      "Iteration 531: Policy loss: -17.156563. Value loss: 16.892328. Entropy: 1.790556.\n",
      "episode: 260   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 723     evaluation reward: 134.9\n",
      "Training network\n",
      "Iteration 532: Policy loss: -13.760493. Value loss: 13.504955. Entropy: 1.790334.\n",
      "Iteration 533: Policy loss: -13.771202. Value loss: 13.513225. Entropy: 1.790390.\n",
      "Iteration 534: Policy loss: -13.698047. Value loss: 13.440574. Entropy: 1.790359.\n",
      "episode: 261   score: 215.0   memory length: 1024   epsilon: 1.0    steps: 924     evaluation reward: 135.85\n",
      "episode: 262   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 542     evaluation reward: 135.6\n",
      "Training network\n",
      "Iteration 535: Policy loss: -11.339431. Value loss: 11.033020. Entropy: 1.790463.\n",
      "Iteration 536: Policy loss: -11.179405. Value loss: 10.875418. Entropy: 1.790487.\n",
      "Iteration 537: Policy loss: -11.380772. Value loss: 11.071584. Entropy: 1.790489.\n",
      "episode: 263   score: 300.0   memory length: 1024   epsilon: 1.0    steps: 1071     evaluation reward: 137.95\n",
      "Training network\n",
      "Iteration 538: Policy loss: -13.808116. Value loss: 13.555488. Entropy: 1.790444.\n",
      "Iteration 539: Policy loss: -13.883155. Value loss: 13.624450. Entropy: 1.790450.\n",
      "Iteration 540: Policy loss: -13.976498. Value loss: 13.715183. Entropy: 1.790451.\n",
      "episode: 264   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 718     evaluation reward: 137.6\n",
      "Training network\n",
      "Iteration 541: Policy loss: -7.353178. Value loss: 7.321346. Entropy: 1.790425.\n",
      "Iteration 542: Policy loss: -7.397583. Value loss: 7.362555. Entropy: 1.790415.\n",
      "Iteration 543: Policy loss: -7.376294. Value loss: 7.345749. Entropy: 1.790406.\n",
      "episode: 265   score: 355.0   memory length: 1024   epsilon: 1.0    steps: 996     evaluation reward: 140.5\n",
      "Training network\n",
      "Iteration 544: Policy loss: -24.313837. Value loss: 23.972755. Entropy: 1.790328.\n",
      "Iteration 545: Policy loss: -24.733362. Value loss: 24.384497. Entropy: 1.790293.\n",
      "Iteration 546: Policy loss: -24.936100. Value loss: 24.585007. Entropy: 1.790305.\n",
      "episode: 266   score: 215.0   memory length: 1024   epsilon: 1.0    steps: 892     evaluation reward: 142.0\n",
      "Training network\n",
      "Iteration 547: Policy loss: -9.252063. Value loss: 8.986674. Entropy: 1.790655.\n",
      "Iteration 548: Policy loss: -9.228991. Value loss: 8.964771. Entropy: 1.790606.\n",
      "Iteration 549: Policy loss: -9.166495. Value loss: 8.900522. Entropy: 1.790687.\n",
      "episode: 267   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 767     evaluation reward: 141.65\n",
      "episode: 268   score: 45.0   memory length: 1024   epsilon: 1.0    steps: 580     evaluation reward: 139.85\n",
      "Training network\n",
      "Iteration 550: Policy loss: -9.147446. Value loss: 9.062199. Entropy: 1.790587.\n",
      "Iteration 551: Policy loss: -9.250237. Value loss: 9.162870. Entropy: 1.790602.\n",
      "Iteration 552: Policy loss: -9.217139. Value loss: 9.130061. Entropy: 1.790623.\n",
      "episode: 269   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 508     evaluation reward: 140.25\n",
      "episode: 270   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 402     evaluation reward: 139.95\n",
      "Training network\n",
      "Iteration 553: Policy loss: -4.906732. Value loss: 4.912774. Entropy: 1.790343.\n",
      "Iteration 554: Policy loss: -4.946905. Value loss: 4.948003. Entropy: 1.790348.\n",
      "Iteration 555: Policy loss: -4.867960. Value loss: 4.874780. Entropy: 1.790361.\n",
      "episode: 271   score: 45.0   memory length: 1024   epsilon: 1.0    steps: 396     evaluation reward: 139.35\n",
      "episode: 272   score: 115.0   memory length: 1024   epsilon: 1.0    steps: 679     evaluation reward: 138.4\n",
      "Training network\n",
      "Iteration 556: Policy loss: -8.057253. Value loss: 7.817952. Entropy: 1.790485.\n",
      "Iteration 557: Policy loss: -8.149407. Value loss: 7.904915. Entropy: 1.790497.\n",
      "Iteration 558: Policy loss: -8.090596. Value loss: 7.847245. Entropy: 1.790479.\n",
      "episode: 273   score: 85.0   memory length: 1024   epsilon: 1.0    steps: 604     evaluation reward: 139.15\n",
      "episode: 274   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 681     evaluation reward: 137.75\n",
      "Training network\n",
      "Iteration 559: Policy loss: -8.650606. Value loss: 8.471480. Entropy: 1.790315.\n",
      "Iteration 560: Policy loss: -8.684260. Value loss: 8.498269. Entropy: 1.790301.\n",
      "Iteration 561: Policy loss: -8.761267. Value loss: 8.574069. Entropy: 1.790327.\n",
      "episode: 275   score: 395.0   memory length: 1024   epsilon: 1.0    steps: 1030     evaluation reward: 141.5\n",
      "Training network\n",
      "Iteration 562: Policy loss: -22.005711. Value loss: 21.692381. Entropy: 1.790384.\n",
      "Iteration 563: Policy loss: -22.151472. Value loss: 21.836340. Entropy: 1.790406.\n",
      "Iteration 564: Policy loss: -22.243465. Value loss: 21.919380. Entropy: 1.790373.\n",
      "episode: 276   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 667     evaluation reward: 141.65\n",
      "Training network\n",
      "Iteration 565: Policy loss: -9.797710. Value loss: 9.639041. Entropy: 1.790394.\n",
      "Iteration 566: Policy loss: -9.784367. Value loss: 9.626567. Entropy: 1.790400.\n",
      "Iteration 567: Policy loss: -9.741467. Value loss: 9.581175. Entropy: 1.790382.\n",
      "episode: 277   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 696     evaluation reward: 141.65\n",
      "Training network\n",
      "Iteration 568: Policy loss: -16.286503. Value loss: 15.960580. Entropy: 1.790462.\n",
      "Iteration 569: Policy loss: -16.272282. Value loss: 15.944696. Entropy: 1.790471.\n",
      "Iteration 570: Policy loss: -16.124332. Value loss: 15.794168. Entropy: 1.790492.\n",
      "episode: 278   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 993     evaluation reward: 142.4\n",
      "episode: 279   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 685     evaluation reward: 140.35\n",
      "Training network\n",
      "Iteration 571: Policy loss: -2.155681. Value loss: 2.344157. Entropy: 1.790471.\n",
      "Iteration 572: Policy loss: -2.152072. Value loss: 2.340330. Entropy: 1.790470.\n",
      "Iteration 573: Policy loss: -2.146402. Value loss: 2.334491. Entropy: 1.790464.\n",
      "episode: 280   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 644     evaluation reward: 141.55\n",
      "Training network\n",
      "Iteration 574: Policy loss: -14.050230. Value loss: 13.933038. Entropy: 1.790468.\n",
      "Iteration 575: Policy loss: -14.099627. Value loss: 13.979654. Entropy: 1.790471.\n",
      "Iteration 576: Policy loss: -14.188747. Value loss: 14.064343. Entropy: 1.790509.\n",
      "episode: 281   score: 260.0   memory length: 1024   epsilon: 1.0    steps: 1096     evaluation reward: 143.1\n",
      "Training network\n",
      "Iteration 577: Policy loss: -12.637607. Value loss: 12.462886. Entropy: 1.790001.\n",
      "Iteration 578: Policy loss: -12.726882. Value loss: 12.552379. Entropy: 1.789971.\n",
      "Iteration 579: Policy loss: -12.559040. Value loss: 12.387211. Entropy: 1.789981.\n",
      "episode: 282   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 686     evaluation reward: 144.2\n",
      "episode: 283   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 665     evaluation reward: 143.95\n",
      "Training network\n",
      "Iteration 580: Policy loss: -8.268797. Value loss: 8.186754. Entropy: 1.790572.\n",
      "Iteration 581: Policy loss: -8.341444. Value loss: 8.254874. Entropy: 1.790572.\n",
      "Iteration 582: Policy loss: -8.387061. Value loss: 8.296185. Entropy: 1.790593.\n",
      "episode: 284   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 753     evaluation reward: 143.95\n",
      "Training network\n",
      "Iteration 583: Policy loss: -4.596804. Value loss: 4.519390. Entropy: 1.790475.\n",
      "Iteration 584: Policy loss: -4.587498. Value loss: 4.514379. Entropy: 1.790518.\n",
      "Iteration 585: Policy loss: -4.517727. Value loss: 4.446226. Entropy: 1.790467.\n",
      "episode: 285   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 594     evaluation reward: 144.2\n",
      "now time :  2018-12-19 13:17:48.219991\n",
      "episode: 286   score: 150.0   memory length: 1024   epsilon: 1.0    steps: 753     evaluation reward: 145.05\n",
      "Training network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 586: Policy loss: -9.609241. Value loss: 9.577874. Entropy: 1.790381.\n",
      "Iteration 587: Policy loss: -9.735984. Value loss: 9.696740. Entropy: 1.790365.\n",
      "Iteration 588: Policy loss: -9.669606. Value loss: 9.635387. Entropy: 1.790370.\n",
      "episode: 287   score: 75.0   memory length: 1024   epsilon: 1.0    steps: 388     evaluation reward: 144.75\n",
      "episode: 288   score: 150.0   memory length: 1024   epsilon: 1.0    steps: 656     evaluation reward: 145.15\n",
      "Training network\n",
      "Iteration 589: Policy loss: -11.020981. Value loss: 10.808487. Entropy: 1.790333.\n",
      "Iteration 590: Policy loss: -11.079111. Value loss: 10.859413. Entropy: 1.790348.\n",
      "Iteration 591: Policy loss: -10.988745. Value loss: 10.771398. Entropy: 1.790333.\n",
      "Training network\n",
      "Iteration 592: Policy loss: -8.558768. Value loss: 8.289751. Entropy: 1.790473.\n",
      "Iteration 593: Policy loss: -8.835973. Value loss: 8.565042. Entropy: 1.790470.\n",
      "Iteration 594: Policy loss: -8.714750. Value loss: 8.441933. Entropy: 1.790443.\n",
      "episode: 289   score: 185.0   memory length: 1024   epsilon: 1.0    steps: 1163     evaluation reward: 145.0\n",
      "episode: 290   score: 85.0   memory length: 1024   epsilon: 1.0    steps: 384     evaluation reward: 144.8\n",
      "Training network\n",
      "Iteration 595: Policy loss: -7.467605. Value loss: 7.474667. Entropy: 1.790160.\n",
      "Iteration 596: Policy loss: -7.383043. Value loss: 7.393266. Entropy: 1.790209.\n",
      "Iteration 597: Policy loss: -7.478433. Value loss: 7.486392. Entropy: 1.790215.\n",
      "episode: 291   score: 75.0   memory length: 1024   epsilon: 1.0    steps: 672     evaluation reward: 144.6\n",
      "episode: 292   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 759     evaluation reward: 145.35\n",
      "Training network\n",
      "Iteration 598: Policy loss: -11.272808. Value loss: 11.121687. Entropy: 1.790543.\n",
      "Iteration 599: Policy loss: -11.058071. Value loss: 10.908111. Entropy: 1.790564.\n",
      "Iteration 600: Policy loss: -11.145573. Value loss: 10.993094. Entropy: 1.790588.\n",
      "episode: 293   score: 290.0   memory length: 1024   epsilon: 1.0    steps: 861     evaluation reward: 146.4\n",
      "Training network\n",
      "Iteration 601: Policy loss: -18.103455. Value loss: 17.771299. Entropy: 1.790423.\n",
      "Iteration 602: Policy loss: -17.875551. Value loss: 17.544640. Entropy: 1.790396.\n",
      "Iteration 603: Policy loss: -17.998358. Value loss: 17.658506. Entropy: 1.790400.\n",
      "episode: 294   score: 60.0   memory length: 1024   epsilon: 1.0    steps: 624     evaluation reward: 144.5\n",
      "Training network\n",
      "Iteration 604: Policy loss: -10.114402. Value loss: 9.838962. Entropy: 1.790483.\n",
      "Iteration 605: Policy loss: -10.198615. Value loss: 9.918830. Entropy: 1.790473.\n",
      "Iteration 606: Policy loss: -10.069850. Value loss: 9.792895. Entropy: 1.790474.\n",
      "episode: 295   score: 265.0   memory length: 1024   epsilon: 1.0    steps: 1023     evaluation reward: 146.35\n",
      "Training network\n",
      "Iteration 607: Policy loss: -8.989807. Value loss: 8.938482. Entropy: 1.790421.\n",
      "Iteration 608: Policy loss: -8.893807. Value loss: 8.848414. Entropy: 1.790446.\n",
      "Iteration 609: Policy loss: -9.099761. Value loss: 9.048084. Entropy: 1.790408.\n",
      "episode: 296   score: 100.0   memory length: 1024   epsilon: 1.0    steps: 675     evaluation reward: 146.1\n",
      "episode: 297   score: 130.0   memory length: 1024   epsilon: 1.0    steps: 689     evaluation reward: 146.5\n",
      "Training network\n",
      "Iteration 610: Policy loss: -8.972030. Value loss: 8.704082. Entropy: 1.790535.\n",
      "Iteration 611: Policy loss: -8.943677. Value loss: 8.676532. Entropy: 1.790535.\n",
      "Iteration 612: Policy loss: -9.073658. Value loss: 8.804085. Entropy: 1.790555.\n",
      "episode: 298   score: 190.0   memory length: 1024   epsilon: 1.0    steps: 952     evaluation reward: 147.35\n",
      "Training network\n",
      "Iteration 613: Policy loss: -14.956865. Value loss: 14.733447. Entropy: 1.790329.\n",
      "Iteration 614: Policy loss: -14.927114. Value loss: 14.700718. Entropy: 1.790331.\n",
      "Iteration 615: Policy loss: -14.943671. Value loss: 14.716225. Entropy: 1.790313.\n",
      "episode: 299   score: 245.0   memory length: 1024   epsilon: 1.0    steps: 845     evaluation reward: 148.55\n",
      "Training network\n",
      "Iteration 616: Policy loss: -12.742334. Value loss: 12.446822. Entropy: 1.790586.\n",
      "Iteration 617: Policy loss: -12.814798. Value loss: 12.516733. Entropy: 1.790594.\n",
      "Iteration 618: Policy loss: -12.811368. Value loss: 12.513139. Entropy: 1.790587.\n",
      "episode: 300   score: 460.0   memory length: 1024   epsilon: 1.0    steps: 1357     evaluation reward: 151.05\n",
      "Training network\n",
      "Iteration 619: Policy loss: -22.064775. Value loss: 21.688122. Entropy: 1.790105.\n",
      "Iteration 620: Policy loss: -22.164007. Value loss: 21.778799. Entropy: 1.790143.\n",
      "Iteration 621: Policy loss: -22.502218. Value loss: 22.108824. Entropy: 1.790125.\n",
      "episode: 301   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 682     evaluation reward: 150.7\n",
      "Training network\n",
      "Iteration 622: Policy loss: -11.991657. Value loss: 11.811882. Entropy: 1.790592.\n",
      "Iteration 623: Policy loss: -12.087661. Value loss: 11.902515. Entropy: 1.790643.\n",
      "Iteration 624: Policy loss: -12.022528. Value loss: 11.843063. Entropy: 1.790620.\n",
      "episode: 302   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 638     evaluation reward: 148.9\n",
      "episode: 303   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 719     evaluation reward: 150.65\n",
      "Training network\n",
      "Iteration 625: Policy loss: -17.334429. Value loss: 17.132275. Entropy: 1.790544.\n",
      "Iteration 626: Policy loss: -17.477510. Value loss: 17.265682. Entropy: 1.790567.\n",
      "Iteration 627: Policy loss: -17.524923. Value loss: 17.310741. Entropy: 1.790581.\n",
      "episode: 304   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 509     evaluation reward: 151.5\n",
      "episode: 305   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 638     evaluation reward: 152.2\n",
      "Training network\n",
      "Iteration 628: Policy loss: -8.830506. Value loss: 8.589187. Entropy: 1.790309.\n",
      "Iteration 629: Policy loss: -8.795002. Value loss: 8.555420. Entropy: 1.790266.\n",
      "Iteration 630: Policy loss: -8.757007. Value loss: 8.516354. Entropy: 1.790252.\n",
      "episode: 306   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 636     evaluation reward: 149.95\n",
      "Training network\n",
      "Iteration 631: Policy loss: -12.425488. Value loss: 12.248795. Entropy: 1.790546.\n",
      "Iteration 632: Policy loss: -12.349677. Value loss: 12.169169. Entropy: 1.790542.\n",
      "Iteration 633: Policy loss: -12.499229. Value loss: 12.315482. Entropy: 1.790572.\n",
      "episode: 307   score: 165.0   memory length: 1024   epsilon: 1.0    steps: 786     evaluation reward: 151.05\n",
      "episode: 308   score: 70.0   memory length: 1024   epsilon: 1.0    steps: 544     evaluation reward: 151.2\n",
      "Training network\n",
      "Iteration 634: Policy loss: -9.030155. Value loss: 9.005946. Entropy: 1.790392.\n",
      "Iteration 635: Policy loss: -9.129951. Value loss: 9.099558. Entropy: 1.790431.\n",
      "Iteration 636: Policy loss: -8.911284. Value loss: 8.888428. Entropy: 1.790383.\n",
      "episode: 309   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 700     evaluation reward: 152.3\n",
      "Training network\n",
      "Iteration 637: Policy loss: -12.626562. Value loss: 12.435304. Entropy: 1.790548.\n",
      "Iteration 638: Policy loss: -12.527760. Value loss: 12.332737. Entropy: 1.790558.\n",
      "Iteration 639: Policy loss: -12.552433. Value loss: 12.357110. Entropy: 1.790548.\n",
      "episode: 310   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 807     evaluation reward: 152.4\n",
      "episode: 311   score: 35.0   memory length: 1024   epsilon: 1.0    steps: 423     evaluation reward: 150.65\n",
      "Training network\n",
      "Iteration 640: Policy loss: -6.897240. Value loss: 6.943625. Entropy: 1.790431.\n",
      "Iteration 641: Policy loss: -6.816053. Value loss: 6.868292. Entropy: 1.790395.\n",
      "Iteration 642: Policy loss: -6.798087. Value loss: 6.850739. Entropy: 1.790394.\n",
      "episode: 312   score: 170.0   memory length: 1024   epsilon: 1.0    steps: 778     evaluation reward: 150.55\n",
      "episode: 313   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 503     evaluation reward: 148.95\n",
      "Training network\n",
      "Iteration 643: Policy loss: -12.578978. Value loss: 12.356068. Entropy: 1.790309.\n",
      "Iteration 644: Policy loss: -12.545189. Value loss: 12.320988. Entropy: 1.790345.\n",
      "Iteration 645: Policy loss: -12.616265. Value loss: 12.390513. Entropy: 1.790292.\n",
      "episode: 314   score: 215.0   memory length: 1024   epsilon: 1.0    steps: 968     evaluation reward: 150.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network\n",
      "Iteration 646: Policy loss: -11.717524. Value loss: 11.544561. Entropy: 1.790489.\n",
      "Iteration 647: Policy loss: -11.650399. Value loss: 11.479871. Entropy: 1.790488.\n",
      "Iteration 648: Policy loss: -11.582023. Value loss: 11.412333. Entropy: 1.790480.\n",
      "episode: 315   score: 460.0   memory length: 1024   epsilon: 1.0    steps: 981     evaluation reward: 153.3\n",
      "Training network\n",
      "Iteration 649: Policy loss: -27.060955. Value loss: 26.764496. Entropy: 1.790462.\n",
      "Iteration 650: Policy loss: -27.087791. Value loss: 26.786234. Entropy: 1.790483.\n",
      "Iteration 651: Policy loss: -27.333565. Value loss: 27.022682. Entropy: 1.790447.\n",
      "episode: 316   score: 225.0   memory length: 1024   epsilon: 1.0    steps: 994     evaluation reward: 151.75\n",
      "Training network\n",
      "Iteration 652: Policy loss: -12.501622. Value loss: 12.451762. Entropy: 1.790420.\n",
      "Iteration 653: Policy loss: -12.266083. Value loss: 12.220886. Entropy: 1.790401.\n",
      "Iteration 654: Policy loss: -12.204730. Value loss: 12.159493. Entropy: 1.790442.\n",
      "episode: 317   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 824     evaluation reward: 152.3\n",
      "Training network\n",
      "Iteration 655: Policy loss: -10.786132. Value loss: 10.663830. Entropy: 1.790532.\n",
      "Iteration 656: Policy loss: -10.911984. Value loss: 10.786438. Entropy: 1.790541.\n",
      "Iteration 657: Policy loss: -10.734810. Value loss: 10.612857. Entropy: 1.790514.\n",
      "episode: 318   score: 90.0   memory length: 1024   epsilon: 1.0    steps: 526     evaluation reward: 150.65\n",
      "episode: 319   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 375     evaluation reward: 149.8\n",
      "Training network\n",
      "Iteration 658: Policy loss: -11.886074. Value loss: 11.748384. Entropy: 1.790472.\n",
      "Iteration 659: Policy loss: -11.919086. Value loss: 11.778236. Entropy: 1.790470.\n",
      "Iteration 660: Policy loss: -11.981870. Value loss: 11.838496. Entropy: 1.790453.\n",
      "episode: 320   score: 315.0   memory length: 1024   epsilon: 1.0    steps: 1133     evaluation reward: 152.5\n",
      "Training network\n",
      "Iteration 661: Policy loss: -11.909094. Value loss: 11.707529. Entropy: 1.790222.\n",
      "Iteration 662: Policy loss: -11.943727. Value loss: 11.740437. Entropy: 1.790231.\n",
      "Iteration 663: Policy loss: -11.971655. Value loss: 11.766224. Entropy: 1.790208.\n",
      "episode: 321   score: 410.0   memory length: 1024   epsilon: 1.0    steps: 822     evaluation reward: 155.2\n",
      "episode: 322   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 503     evaluation reward: 155.5\n",
      "Training network\n",
      "Iteration 664: Policy loss: -25.492018. Value loss: 25.265238. Entropy: 1.790493.\n",
      "Iteration 665: Policy loss: -25.664139. Value loss: 25.433472. Entropy: 1.790518.\n",
      "Iteration 666: Policy loss: -25.489408. Value loss: 25.255459. Entropy: 1.790557.\n",
      "episode: 323   score: 100.0   memory length: 1024   epsilon: 1.0    steps: 510     evaluation reward: 154.25\n",
      "Training network\n",
      "Iteration 667: Policy loss: -16.244114. Value loss: 15.956510. Entropy: 1.790369.\n",
      "Iteration 668: Policy loss: -16.334648. Value loss: 16.041647. Entropy: 1.790385.\n",
      "Iteration 669: Policy loss: -16.187847. Value loss: 15.897948. Entropy: 1.790334.\n",
      "episode: 324   score: 280.0   memory length: 1024   epsilon: 1.0    steps: 972     evaluation reward: 155.2\n",
      "Training network\n",
      "Iteration 670: Policy loss: -14.251304. Value loss: 14.025250. Entropy: 1.790465.\n",
      "Iteration 671: Policy loss: -14.200409. Value loss: 13.971063. Entropy: 1.790478.\n",
      "Iteration 672: Policy loss: -14.174299. Value loss: 13.945331. Entropy: 1.790446.\n",
      "episode: 325   score: 220.0   memory length: 1024   epsilon: 1.0    steps: 781     evaluation reward: 156.95\n",
      "Training network\n",
      "Iteration 673: Policy loss: -29.927841. Value loss: 29.552219. Entropy: 1.790013.\n",
      "Iteration 674: Policy loss: -29.924074. Value loss: 29.543901. Entropy: 1.790062.\n",
      "Iteration 675: Policy loss: -30.124552. Value loss: 29.732437. Entropy: 1.790057.\n",
      "episode: 326   score: 500.0   memory length: 1024   epsilon: 1.0    steps: 1010     evaluation reward: 161.3\n",
      "episode: 327   score: 75.0   memory length: 1024   epsilon: 1.0    steps: 505     evaluation reward: 159.3\n",
      "Training network\n",
      "Iteration 676: Policy loss: -10.622979. Value loss: 10.440799. Entropy: 1.790472.\n",
      "Iteration 677: Policy loss: -10.672821. Value loss: 10.491170. Entropy: 1.790407.\n",
      "Iteration 678: Policy loss: -10.568305. Value loss: 10.390645. Entropy: 1.790432.\n",
      "episode: 328   score: 245.0   memory length: 1024   epsilon: 1.0    steps: 915     evaluation reward: 161.2\n",
      "Training network\n",
      "Iteration 679: Policy loss: -13.279162. Value loss: 13.057244. Entropy: 1.790442.\n",
      "Iteration 680: Policy loss: -13.465308. Value loss: 13.239955. Entropy: 1.790428.\n",
      "Iteration 681: Policy loss: -13.371677. Value loss: 13.151974. Entropy: 1.790435.\n",
      "episode: 329   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 709     evaluation reward: 161.55\n",
      "episode: 330   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 592     evaluation reward: 159.25\n",
      "Training network\n",
      "Iteration 682: Policy loss: -17.022953. Value loss: 16.853571. Entropy: 1.790578.\n",
      "Iteration 683: Policy loss: -17.232262. Value loss: 17.059370. Entropy: 1.790537.\n",
      "Iteration 684: Policy loss: -17.196318. Value loss: 17.024420. Entropy: 1.790584.\n",
      "episode: 331   score: 440.0   memory length: 1024   epsilon: 1.0    steps: 985     evaluation reward: 159.95\n",
      "Training network\n",
      "Iteration 685: Policy loss: -23.853355. Value loss: 23.559246. Entropy: 1.790580.\n",
      "Iteration 686: Policy loss: -23.763767. Value loss: 23.462801. Entropy: 1.790572.\n",
      "Iteration 687: Policy loss: -23.833761. Value loss: 23.523457. Entropy: 1.790565.\n",
      "episode: 332   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 463     evaluation reward: 160.45\n",
      "episode: 333   score: 115.0   memory length: 1024   epsilon: 1.0    steps: 518     evaluation reward: 160.8\n",
      "Training network\n",
      "Iteration 688: Policy loss: -13.178188. Value loss: 13.001840. Entropy: 1.790445.\n",
      "Iteration 689: Policy loss: -13.294857. Value loss: 13.116838. Entropy: 1.790462.\n",
      "Iteration 690: Policy loss: -13.245787. Value loss: 13.070397. Entropy: 1.790469.\n",
      "episode: 334   score: 305.0   memory length: 1024   epsilon: 1.0    steps: 1119     evaluation reward: 161.75\n",
      "Training network\n",
      "Iteration 691: Policy loss: -13.217175. Value loss: 12.932156. Entropy: 1.790388.\n",
      "Iteration 692: Policy loss: -13.251239. Value loss: 12.966043. Entropy: 1.790411.\n",
      "Iteration 693: Policy loss: -13.358915. Value loss: 13.068693. Entropy: 1.790343.\n",
      "episode: 335   score: 360.0   memory length: 1024   epsilon: 1.0    steps: 1188     evaluation reward: 164.85\n",
      "Training network\n",
      "Iteration 694: Policy loss: -19.917639. Value loss: 19.680796. Entropy: 1.790195.\n",
      "Iteration 695: Policy loss: -19.809267. Value loss: 19.566288. Entropy: 1.790192.\n",
      "Iteration 696: Policy loss: -19.498640. Value loss: 19.262329. Entropy: 1.790231.\n",
      "episode: 336   score: 280.0   memory length: 1024   epsilon: 1.0    steps: 900     evaluation reward: 167.0\n",
      "Training network\n",
      "Iteration 697: Policy loss: -16.463690. Value loss: 16.161037. Entropy: 1.790129.\n",
      "Iteration 698: Policy loss: -16.401581. Value loss: 16.101801. Entropy: 1.790097.\n",
      "Iteration 699: Policy loss: -16.229107. Value loss: 15.931358. Entropy: 1.790115.\n",
      "episode: 337   score: 215.0   memory length: 1024   epsilon: 1.0    steps: 925     evaluation reward: 167.6\n",
      "Training network\n",
      "Iteration 700: Policy loss: -16.632547. Value loss: 16.332663. Entropy: 1.790714.\n",
      "Iteration 701: Policy loss: -16.634779. Value loss: 16.333649. Entropy: 1.790713.\n",
      "Iteration 702: Policy loss: -16.822989. Value loss: 16.517570. Entropy: 1.790699.\n",
      "episode: 338   score: 430.0   memory length: 1024   epsilon: 1.0    steps: 889     evaluation reward: 170.8\n",
      "Training network\n",
      "Iteration 703: Policy loss: -24.142656. Value loss: 23.954195. Entropy: 1.790509.\n",
      "Iteration 704: Policy loss: -23.948893. Value loss: 23.751976. Entropy: 1.790476.\n",
      "Iteration 705: Policy loss: -24.317297. Value loss: 24.110315. Entropy: 1.790529.\n",
      "episode: 339   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 493     evaluation reward: 170.4\n",
      "episode: 340   score: 75.0   memory length: 1024   epsilon: 1.0    steps: 580     evaluation reward: 169.7\n",
      "Training network\n",
      "Iteration 706: Policy loss: -5.823079. Value loss: 5.821119. Entropy: 1.790462.\n",
      "Iteration 707: Policy loss: -5.792518. Value loss: 5.785550. Entropy: 1.790457.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 708: Policy loss: -5.813091. Value loss: 5.807712. Entropy: 1.790463.\n",
      "episode: 341   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 644     evaluation reward: 169.9\n",
      "Training network\n",
      "Iteration 709: Policy loss: -15.002002. Value loss: 14.971511. Entropy: 1.790446.\n",
      "Iteration 710: Policy loss: -14.969588. Value loss: 14.937591. Entropy: 1.790469.\n",
      "Iteration 711: Policy loss: -15.002453. Value loss: 14.969148. Entropy: 1.790431.\n",
      "episode: 342   score: 300.0   memory length: 1024   epsilon: 1.0    steps: 1018     evaluation reward: 172.35\n",
      "episode: 343   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 620     evaluation reward: 173.45\n",
      "Training network\n",
      "Iteration 712: Policy loss: -12.123165. Value loss: 12.097475. Entropy: 1.790542.\n",
      "Iteration 713: Policy loss: -12.181132. Value loss: 12.150473. Entropy: 1.790546.\n",
      "Iteration 714: Policy loss: -12.178974. Value loss: 12.148952. Entropy: 1.790526.\n",
      "episode: 344   score: 185.0   memory length: 1024   epsilon: 1.0    steps: 947     evaluation reward: 172.6\n",
      "Training network\n",
      "Iteration 715: Policy loss: -10.433893. Value loss: 10.271696. Entropy: 1.790552.\n",
      "Iteration 716: Policy loss: -10.431615. Value loss: 10.271324. Entropy: 1.790589.\n",
      "Iteration 717: Policy loss: -10.383060. Value loss: 10.225248. Entropy: 1.790581.\n",
      "episode: 345   score: 45.0   memory length: 1024   epsilon: 1.0    steps: 363     evaluation reward: 172.9\n",
      "episode: 346   score: 410.0   memory length: 1024   epsilon: 1.0    steps: 871     evaluation reward: 175.5\n",
      "Training network\n",
      "Iteration 718: Policy loss: -25.584326. Value loss: 25.460157. Entropy: 1.790486.\n",
      "Iteration 719: Policy loss: -25.767122. Value loss: 25.636421. Entropy: 1.790505.\n",
      "Iteration 720: Policy loss: -26.315304. Value loss: 26.174177. Entropy: 1.790511.\n",
      "episode: 347   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 175.7\n",
      "Training network\n",
      "Iteration 721: Policy loss: -9.889054. Value loss: 9.666582. Entropy: 1.790595.\n",
      "Iteration 722: Policy loss: -9.859600. Value loss: 9.635880. Entropy: 1.790580.\n",
      "Iteration 723: Policy loss: -9.836686. Value loss: 9.611415. Entropy: 1.790590.\n",
      "episode: 348   score: 170.0   memory length: 1024   epsilon: 1.0    steps: 798     evaluation reward: 176.2\n",
      "episode: 349   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 399     evaluation reward: 175.9\n",
      "Training network\n",
      "Iteration 724: Policy loss: -9.790987. Value loss: 9.587472. Entropy: 1.790590.\n",
      "Iteration 725: Policy loss: -9.866053. Value loss: 9.659035. Entropy: 1.790569.\n",
      "Iteration 726: Policy loss: -9.771218. Value loss: 9.570824. Entropy: 1.790621.\n",
      "episode: 350   score: 145.0   memory length: 1024   epsilon: 1.0    steps: 677     evaluation reward: 176.55\n",
      "episode: 351   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 622     evaluation reward: 176.4\n",
      "Training network\n",
      "Iteration 727: Policy loss: -11.740335. Value loss: 11.485693. Entropy: 1.790595.\n",
      "Iteration 728: Policy loss: -11.654136. Value loss: 11.399777. Entropy: 1.790601.\n",
      "Iteration 729: Policy loss: -11.700437. Value loss: 11.445242. Entropy: 1.790601.\n",
      "episode: 352   score: 240.0   memory length: 1024   epsilon: 1.0    steps: 829     evaluation reward: 176.7\n",
      "Training network\n",
      "Iteration 730: Policy loss: -12.617673. Value loss: 12.439013. Entropy: 1.790166.\n",
      "Iteration 731: Policy loss: -12.706450. Value loss: 12.521550. Entropy: 1.790184.\n",
      "Iteration 732: Policy loss: -12.721611. Value loss: 12.533932. Entropy: 1.790188.\n",
      "episode: 353   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 530     evaluation reward: 177.05\n",
      "now time :  2018-12-19 13:20:43.400435\n",
      "Training network\n",
      "Iteration 733: Policy loss: -29.381514. Value loss: 29.126564. Entropy: 1.790401.\n",
      "Iteration 734: Policy loss: -29.303457. Value loss: 29.045280. Entropy: 1.790418.\n",
      "Iteration 735: Policy loss: -29.284607. Value loss: 29.015638. Entropy: 1.790434.\n",
      "episode: 354   score: 515.0   memory length: 1024   epsilon: 1.0    steps: 1057     evaluation reward: 180.9\n",
      "episode: 355   score: 130.0   memory length: 1024   epsilon: 1.0    steps: 730     evaluation reward: 181.15\n",
      "Training network\n",
      "Iteration 736: Policy loss: -9.315697. Value loss: 9.043330. Entropy: 1.790526.\n",
      "Iteration 737: Policy loss: -9.301768. Value loss: 9.027603. Entropy: 1.790541.\n",
      "Iteration 738: Policy loss: -9.378135. Value loss: 9.101294. Entropy: 1.790502.\n",
      "episode: 356   score: 480.0   memory length: 1024   epsilon: 1.0    steps: 1143     evaluation reward: 185.15\n",
      "Training network\n",
      "Iteration 739: Policy loss: -26.584471. Value loss: 26.394629. Entropy: 1.790322.\n",
      "Iteration 740: Policy loss: -26.835123. Value loss: 26.637531. Entropy: 1.790340.\n",
      "Iteration 741: Policy loss: -26.593504. Value loss: 26.394951. Entropy: 1.790351.\n",
      "episode: 357   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 598     evaluation reward: 185.05\n",
      "episode: 358   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 379     evaluation reward: 183.0\n",
      "Training network\n",
      "Iteration 742: Policy loss: -3.803399. Value loss: 3.897891. Entropy: 1.790516.\n",
      "Iteration 743: Policy loss: -3.798393. Value loss: 3.891987. Entropy: 1.790532.\n",
      "Iteration 744: Policy loss: -3.777329. Value loss: 3.873318. Entropy: 1.790512.\n",
      "episode: 359   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 686     evaluation reward: 182.25\n",
      "Training network\n",
      "Iteration 745: Policy loss: -8.377800. Value loss: 8.245951. Entropy: 1.790571.\n",
      "Iteration 746: Policy loss: -8.451274. Value loss: 8.318302. Entropy: 1.790543.\n",
      "Iteration 747: Policy loss: -8.409092. Value loss: 8.278421. Entropy: 1.790527.\n",
      "episode: 360   score: 95.0   memory length: 1024   epsilon: 1.0    steps: 653     evaluation reward: 181.1\n",
      "episode: 361   score: 100.0   memory length: 1024   epsilon: 1.0    steps: 490     evaluation reward: 179.95\n",
      "Training network\n",
      "Iteration 748: Policy loss: -11.945866. Value loss: 11.772753. Entropy: 1.790614.\n",
      "Iteration 749: Policy loss: -11.999677. Value loss: 11.821939. Entropy: 1.790633.\n",
      "Iteration 750: Policy loss: -12.010343. Value loss: 11.830565. Entropy: 1.790616.\n",
      "episode: 362   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 710     evaluation reward: 179.95\n",
      "Training network\n",
      "Iteration 751: Policy loss: -8.026363. Value loss: 8.006564. Entropy: 1.790411.\n",
      "Iteration 752: Policy loss: -8.194702. Value loss: 8.168344. Entropy: 1.790405.\n",
      "Iteration 753: Policy loss: -8.223431. Value loss: 8.194624. Entropy: 1.790410.\n",
      "episode: 363   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 893     evaluation reward: 178.05\n",
      "Training network\n",
      "Iteration 754: Policy loss: -9.551902. Value loss: 9.407431. Entropy: 1.790498.\n",
      "Iteration 755: Policy loss: -9.676362. Value loss: 9.530637. Entropy: 1.790489.\n",
      "Iteration 756: Policy loss: -9.648683. Value loss: 9.502699. Entropy: 1.790480.\n",
      "episode: 364   score: 160.0   memory length: 1024   epsilon: 1.0    steps: 878     evaluation reward: 178.6\n",
      "episode: 365   score: 145.0   memory length: 1024   epsilon: 1.0    steps: 616     evaluation reward: 176.5\n",
      "Training network\n",
      "Iteration 757: Policy loss: -7.540441. Value loss: 7.455021. Entropy: 1.790402.\n",
      "Iteration 758: Policy loss: -7.378567. Value loss: 7.294925. Entropy: 1.790375.\n",
      "Iteration 759: Policy loss: -7.495819. Value loss: 7.413357. Entropy: 1.790398.\n",
      "episode: 366   score: 740.0   memory length: 1024   epsilon: 1.0    steps: 1221     evaluation reward: 181.75\n",
      "Training network\n",
      "Iteration 760: Policy loss: -45.135811. Value loss: 44.840588. Entropy: 1.790246.\n",
      "Iteration 761: Policy loss: -45.309311. Value loss: 45.003036. Entropy: 1.790234.\n",
      "Iteration 762: Policy loss: -45.982304. Value loss: 45.665936. Entropy: 1.790282.\n",
      "episode: 367   score: 215.0   memory length: 1024   epsilon: 1.0    steps: 807     evaluation reward: 182.65\n",
      "Training network\n",
      "Iteration 763: Policy loss: -14.411792. Value loss: 14.296477. Entropy: 1.790600.\n",
      "Iteration 764: Policy loss: -14.454066. Value loss: 14.334920. Entropy: 1.790610.\n",
      "Iteration 765: Policy loss: -14.542517. Value loss: 14.422234. Entropy: 1.790609.\n",
      "episode: 368   score: 130.0   memory length: 1024   epsilon: 1.0    steps: 455     evaluation reward: 183.5\n",
      "episode: 369   score: 85.0   memory length: 1024   epsilon: 1.0    steps: 637     evaluation reward: 183.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network\n",
      "Iteration 766: Policy loss: -7.764306. Value loss: 7.675940. Entropy: 1.790544.\n",
      "Iteration 767: Policy loss: -7.608905. Value loss: 7.522952. Entropy: 1.790554.\n",
      "Iteration 768: Policy loss: -7.878951. Value loss: 7.780045. Entropy: 1.790575.\n",
      "episode: 370   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 618     evaluation reward: 183.2\n",
      "Training network\n",
      "Iteration 769: Policy loss: -8.598514. Value loss: 8.722417. Entropy: 1.790594.\n",
      "Iteration 770: Policy loss: -8.624310. Value loss: 8.745901. Entropy: 1.790595.\n",
      "Iteration 771: Policy loss: -8.545507. Value loss: 8.668560. Entropy: 1.790600.\n",
      "episode: 371   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 699     evaluation reward: 184.85\n",
      "episode: 372   score: 225.0   memory length: 1024   epsilon: 1.0    steps: 754     evaluation reward: 185.95\n",
      "Training network\n",
      "Iteration 772: Policy loss: -13.701069. Value loss: 13.657098. Entropy: 1.790636.\n",
      "Iteration 773: Policy loss: -13.723630. Value loss: 13.682191. Entropy: 1.790616.\n",
      "Iteration 774: Policy loss: -13.824175. Value loss: 13.775286. Entropy: 1.790618.\n",
      "episode: 373   score: 35.0   memory length: 1024   epsilon: 1.0    steps: 589     evaluation reward: 185.45\n",
      "Training network\n",
      "Iteration 775: Policy loss: -3.292214. Value loss: 3.081350. Entropy: 1.790416.\n",
      "Iteration 776: Policy loss: -3.291975. Value loss: 3.081091. Entropy: 1.790405.\n",
      "Iteration 777: Policy loss: -3.279273. Value loss: 3.070294. Entropy: 1.790411.\n",
      "episode: 374   score: 60.0   memory length: 1024   epsilon: 1.0    steps: 668     evaluation reward: 184.85\n",
      "Training network\n",
      "Iteration 778: Policy loss: -15.033614. Value loss: 14.780361. Entropy: 1.790322.\n",
      "Iteration 779: Policy loss: -15.008080. Value loss: 14.758939. Entropy: 1.790320.\n",
      "Iteration 780: Policy loss: -15.091830. Value loss: 14.836969. Entropy: 1.790383.\n",
      "episode: 375   score: 260.0   memory length: 1024   epsilon: 1.0    steps: 988     evaluation reward: 183.5\n",
      "episode: 376   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 639     evaluation reward: 183.65\n",
      "Training network\n",
      "Iteration 781: Policy loss: -8.062059. Value loss: 8.049053. Entropy: 1.790659.\n",
      "Iteration 782: Policy loss: -8.049717. Value loss: 8.034334. Entropy: 1.790655.\n",
      "Iteration 783: Policy loss: -7.981205. Value loss: 7.972772. Entropy: 1.790686.\n",
      "episode: 377   score: 85.0   memory length: 1024   epsilon: 1.0    steps: 645     evaluation reward: 182.95\n",
      "episode: 378   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 504     evaluation reward: 182.4\n",
      "Training network\n",
      "Iteration 784: Policy loss: -10.081803. Value loss: 9.889849. Entropy: 1.790518.\n",
      "Iteration 785: Policy loss: -9.908640. Value loss: 9.722984. Entropy: 1.790483.\n",
      "Iteration 786: Policy loss: -9.960274. Value loss: 9.778375. Entropy: 1.790530.\n",
      "episode: 379   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 673     evaluation reward: 182.95\n",
      "episode: 380   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 518     evaluation reward: 182.45\n",
      "Training network\n",
      "Iteration 787: Policy loss: -7.901453. Value loss: 7.851887. Entropy: 1.790565.\n",
      "Iteration 788: Policy loss: -7.928237. Value loss: 7.873999. Entropy: 1.790545.\n",
      "Iteration 789: Policy loss: -7.940979. Value loss: 7.885644. Entropy: 1.790568.\n",
      "episode: 381   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 693     evaluation reward: 180.95\n",
      "Training network\n",
      "Iteration 790: Policy loss: -11.369111. Value loss: 11.289962. Entropy: 1.790330.\n",
      "Iteration 791: Policy loss: -11.346688. Value loss: 11.273193. Entropy: 1.790334.\n",
      "Iteration 792: Policy loss: -11.238457. Value loss: 11.162123. Entropy: 1.790361.\n",
      "episode: 382   score: 310.0   memory length: 1024   epsilon: 1.0    steps: 654     evaluation reward: 182.25\n",
      "Training network\n",
      "Iteration 793: Policy loss: -13.798954. Value loss: 13.639944. Entropy: 1.790492.\n",
      "Iteration 794: Policy loss: -13.709994. Value loss: 13.558526. Entropy: 1.790512.\n",
      "Iteration 795: Policy loss: -13.603338. Value loss: 13.442340. Entropy: 1.790504.\n",
      "episode: 383   score: 190.0   memory length: 1024   epsilon: 1.0    steps: 1001     evaluation reward: 183.1\n",
      "episode: 384   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 681     evaluation reward: 183.1\n",
      "Training network\n",
      "Iteration 796: Policy loss: -8.793007. Value loss: 8.558657. Entropy: 1.790428.\n",
      "Iteration 797: Policy loss: -8.807360. Value loss: 8.570147. Entropy: 1.790409.\n",
      "Iteration 798: Policy loss: -8.741497. Value loss: 8.509319. Entropy: 1.790416.\n",
      "episode: 385   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 502     evaluation reward: 183.75\n",
      "episode: 386   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 457     evaluation reward: 182.8\n",
      "Training network\n",
      "Iteration 799: Policy loss: -9.068463. Value loss: 8.999085. Entropy: 1.790620.\n",
      "Iteration 800: Policy loss: -9.076529. Value loss: 9.011969. Entropy: 1.790628.\n",
      "Iteration 801: Policy loss: -8.983968. Value loss: 8.917095. Entropy: 1.790614.\n",
      "episode: 387   score: 90.0   memory length: 1024   epsilon: 1.0    steps: 648     evaluation reward: 182.95\n",
      "Training network\n",
      "Iteration 802: Policy loss: -13.387338. Value loss: 13.263739. Entropy: 1.790433.\n",
      "Iteration 803: Policy loss: -13.232535. Value loss: 13.111652. Entropy: 1.790442.\n",
      "Iteration 804: Policy loss: -13.274256. Value loss: 13.150766. Entropy: 1.790436.\n",
      "episode: 388   score: 215.0   memory length: 1024   epsilon: 1.0    steps: 891     evaluation reward: 183.6\n",
      "Training network\n",
      "Iteration 805: Policy loss: -8.638218. Value loss: 8.553709. Entropy: 1.790417.\n",
      "Iteration 806: Policy loss: -8.745841. Value loss: 8.661336. Entropy: 1.790455.\n",
      "Iteration 807: Policy loss: -8.647937. Value loss: 8.564186. Entropy: 1.790424.\n",
      "episode: 389   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 708     evaluation reward: 183.1\n",
      "episode: 390   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 516     evaluation reward: 183.6\n",
      "Training network\n",
      "Iteration 808: Policy loss: -11.233322. Value loss: 11.106598. Entropy: 1.790415.\n",
      "Iteration 809: Policy loss: -11.396628. Value loss: 11.264030. Entropy: 1.790442.\n",
      "Iteration 810: Policy loss: -11.386418. Value loss: 11.248601. Entropy: 1.790449.\n",
      "episode: 391   score: 150.0   memory length: 1024   epsilon: 1.0    steps: 733     evaluation reward: 184.35\n",
      "episode: 392   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 653     evaluation reward: 183.9\n",
      "Training network\n",
      "Iteration 811: Policy loss: -10.221718. Value loss: 10.236614. Entropy: 1.790392.\n",
      "Iteration 812: Policy loss: -10.330748. Value loss: 10.338325. Entropy: 1.790389.\n",
      "Iteration 813: Policy loss: -10.325463. Value loss: 10.329077. Entropy: 1.790399.\n",
      "episode: 393   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 373     evaluation reward: 181.5\n",
      "episode: 394   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 597     evaluation reward: 182.1\n",
      "Training network\n",
      "Iteration 814: Policy loss: -11.117094. Value loss: 11.049646. Entropy: 1.790548.\n",
      "Iteration 815: Policy loss: -10.936219. Value loss: 10.873569. Entropy: 1.790531.\n",
      "Iteration 816: Policy loss: -11.044646. Value loss: 10.972375. Entropy: 1.790571.\n",
      "episode: 395   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 399     evaluation reward: 180.25\n",
      "Training network\n",
      "Iteration 817: Policy loss: -7.788452. Value loss: 7.793465. Entropy: 1.790697.\n",
      "Iteration 818: Policy loss: -7.700532. Value loss: 7.708868. Entropy: 1.790694.\n",
      "Iteration 819: Policy loss: -7.654094. Value loss: 7.660576. Entropy: 1.790709.\n",
      "episode: 396   score: 225.0   memory length: 1024   epsilon: 1.0    steps: 1203     evaluation reward: 181.5\n",
      "episode: 397   score: 75.0   memory length: 1024   epsilon: 1.0    steps: 582     evaluation reward: 180.95\n",
      "Training network\n",
      "Iteration 820: Policy loss: -10.114883. Value loss: 10.130711. Entropy: 1.790222.\n",
      "Iteration 821: Policy loss: -10.131189. Value loss: 10.136983. Entropy: 1.790169.\n",
      "Iteration 822: Policy loss: -10.217572. Value loss: 10.218874. Entropy: 1.790206.\n",
      "episode: 398   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 498     evaluation reward: 179.2\n",
      "Training network\n",
      "Iteration 823: Policy loss: -4.010180. Value loss: 4.092396. Entropy: 1.790661.\n",
      "Iteration 824: Policy loss: -4.057853. Value loss: 4.136798. Entropy: 1.790632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 825: Policy loss: -3.982701. Value loss: 4.061330. Entropy: 1.790659.\n",
      "episode: 399   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 636     evaluation reward: 178.0\n",
      "episode: 400   score: 165.0   memory length: 1024   epsilon: 1.0    steps: 690     evaluation reward: 175.05\n",
      "Training network\n",
      "Iteration 826: Policy loss: -10.842195. Value loss: 10.879139. Entropy: 1.790572.\n",
      "Iteration 827: Policy loss: -10.866974. Value loss: 10.905962. Entropy: 1.790586.\n",
      "Iteration 828: Policy loss: -10.898937. Value loss: 10.936378. Entropy: 1.790580.\n",
      "episode: 401   score: 185.0   memory length: 1024   epsilon: 1.0    steps: 796     evaluation reward: 175.7\n",
      "episode: 402   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 174.95\n",
      "Training network\n",
      "Iteration 829: Policy loss: -11.282341. Value loss: 11.060919. Entropy: 1.790599.\n",
      "Iteration 830: Policy loss: -11.144481. Value loss: 10.927216. Entropy: 1.790634.\n",
      "Iteration 831: Policy loss: -11.262127. Value loss: 11.040515. Entropy: 1.790606.\n",
      "episode: 403   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 475     evaluation reward: 173.4\n",
      "Training network\n",
      "Iteration 832: Policy loss: -10.980256. Value loss: 11.002929. Entropy: 1.790470.\n",
      "Iteration 833: Policy loss: -10.972217. Value loss: 10.988880. Entropy: 1.790475.\n",
      "Iteration 834: Policy loss: -10.983784. Value loss: 11.001884. Entropy: 1.790456.\n",
      "episode: 404   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 955     evaluation reward: 173.95\n",
      "episode: 405   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 595     evaluation reward: 173.8\n",
      "Training network\n",
      "Iteration 835: Policy loss: -5.295068. Value loss: 5.590812. Entropy: 1.790511.\n",
      "Iteration 836: Policy loss: -5.263158. Value loss: 5.559698. Entropy: 1.790535.\n",
      "Iteration 837: Policy loss: -5.171211. Value loss: 5.471766. Entropy: 1.790514.\n",
      "episode: 406   score: 60.0   memory length: 1024   epsilon: 1.0    steps: 371     evaluation reward: 173.05\n",
      "episode: 407   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 584     evaluation reward: 171.9\n",
      "Training network\n",
      "Iteration 838: Policy loss: -5.161760. Value loss: 5.143049. Entropy: 1.790539.\n",
      "Iteration 839: Policy loss: -5.207822. Value loss: 5.183789. Entropy: 1.790521.\n",
      "Iteration 840: Policy loss: -5.185023. Value loss: 5.166740. Entropy: 1.790498.\n",
      "episode: 408   score: 190.0   memory length: 1024   epsilon: 1.0    steps: 924     evaluation reward: 173.1\n",
      "Training network\n",
      "Iteration 841: Policy loss: -10.878219. Value loss: 10.725750. Entropy: 1.790233.\n",
      "Iteration 842: Policy loss: -10.791451. Value loss: 10.643038. Entropy: 1.790261.\n",
      "Iteration 843: Policy loss: -10.873460. Value loss: 10.723131. Entropy: 1.790192.\n",
      "episode: 409   score: 410.0   memory length: 1024   epsilon: 1.0    steps: 827     evaluation reward: 175.4\n",
      "episode: 410   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 476     evaluation reward: 174.7\n",
      "Training network\n",
      "Iteration 844: Policy loss: -25.527210. Value loss: 25.486994. Entropy: 1.790660.\n",
      "Iteration 845: Policy loss: -25.915581. Value loss: 25.869652. Entropy: 1.790637.\n",
      "Iteration 846: Policy loss: -25.821951. Value loss: 25.768410. Entropy: 1.790651.\n",
      "episode: 411   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 652     evaluation reward: 175.7\n",
      "Training network\n",
      "Iteration 847: Policy loss: -12.739033. Value loss: 12.657227. Entropy: 1.790417.\n",
      "Iteration 848: Policy loss: -12.827146. Value loss: 12.738487. Entropy: 1.790411.\n",
      "Iteration 849: Policy loss: -12.805628. Value loss: 12.716735. Entropy: 1.790431.\n",
      "episode: 412   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 694     evaluation reward: 175.2\n",
      "episode: 413   score: 75.0   memory length: 1024   epsilon: 1.0    steps: 646     evaluation reward: 174.7\n",
      "Training network\n",
      "Iteration 850: Policy loss: -4.970375. Value loss: 4.919339. Entropy: 1.790526.\n",
      "Iteration 851: Policy loss: -5.050473. Value loss: 4.991551. Entropy: 1.790529.\n",
      "Iteration 852: Policy loss: -4.929099. Value loss: 4.873711. Entropy: 1.790509.\n",
      "episode: 414   score: 45.0   memory length: 1024   epsilon: 1.0    steps: 401     evaluation reward: 173.0\n",
      "Training network\n",
      "Iteration 853: Policy loss: -7.544553. Value loss: 7.364388. Entropy: 1.790495.\n",
      "Iteration 854: Policy loss: -7.572037. Value loss: 7.390207. Entropy: 1.790502.\n",
      "Iteration 855: Policy loss: -7.508231. Value loss: 7.328413. Entropy: 1.790547.\n",
      "episode: 415   score: 130.0   memory length: 1024   epsilon: 1.0    steps: 941     evaluation reward: 169.7\n",
      "episode: 416   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 401     evaluation reward: 167.95\n",
      "Training network\n",
      "Iteration 856: Policy loss: -6.148470. Value loss: 6.371809. Entropy: 1.790447.\n",
      "Iteration 857: Policy loss: -6.170164. Value loss: 6.394432. Entropy: 1.790429.\n",
      "Iteration 858: Policy loss: -6.175889. Value loss: 6.399451. Entropy: 1.790451.\n",
      "episode: 417   score: 100.0   memory length: 1024   epsilon: 1.0    steps: 420     evaluation reward: 166.85\n",
      "episode: 418   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 690     evaluation reward: 167.15\n",
      "Training network\n",
      "Iteration 859: Policy loss: -8.973973. Value loss: 8.895938. Entropy: 1.790643.\n",
      "Iteration 860: Policy loss: -9.053789. Value loss: 8.970884. Entropy: 1.790609.\n",
      "Iteration 861: Policy loss: -9.102851. Value loss: 9.014134. Entropy: 1.790612.\n",
      "episode: 419   score: 45.0   memory length: 1024   epsilon: 1.0    steps: 584     evaluation reward: 167.1\n",
      "episode: 420   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 508     evaluation reward: 164.5\n",
      "Training network\n",
      "Iteration 862: Policy loss: -3.957460. Value loss: 4.206379. Entropy: 1.790515.\n",
      "Iteration 863: Policy loss: -3.991741. Value loss: 4.239445. Entropy: 1.790508.\n",
      "Iteration 864: Policy loss: -3.930783. Value loss: 4.183165. Entropy: 1.790524.\n",
      "episode: 421   score: 150.0   memory length: 1024   epsilon: 1.0    steps: 625     evaluation reward: 161.9\n",
      "episode: 422   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 458     evaluation reward: 161.1\n",
      "Training network\n",
      "Iteration 865: Policy loss: -7.729654. Value loss: 7.681276. Entropy: 1.790586.\n",
      "Iteration 866: Policy loss: -7.850924. Value loss: 7.794181. Entropy: 1.790591.\n",
      "Iteration 867: Policy loss: -7.793869. Value loss: 7.741773. Entropy: 1.790563.\n",
      "episode: 423   score: 45.0   memory length: 1024   epsilon: 1.0    steps: 439     evaluation reward: 160.55\n",
      "episode: 424   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 525     evaluation reward: 158.85\n",
      "Training network\n",
      "Iteration 868: Policy loss: -7.840994. Value loss: 7.929168. Entropy: 1.790598.\n",
      "Iteration 869: Policy loss: -8.006166. Value loss: 8.091040. Entropy: 1.790580.\n",
      "Iteration 870: Policy loss: -8.029202. Value loss: 8.108384. Entropy: 1.790592.\n",
      "episode: 425   score: 170.0   memory length: 1024   epsilon: 1.0    steps: 946     evaluation reward: 158.35\n",
      "Training network\n",
      "Iteration 871: Policy loss: -11.770972. Value loss: 11.595102. Entropy: 1.790423.\n",
      "Iteration 872: Policy loss: -11.555184. Value loss: 11.381598. Entropy: 1.790441.\n",
      "Iteration 873: Policy loss: -11.775423. Value loss: 11.594654. Entropy: 1.790442.\n",
      "episode: 426   score: 585.0   memory length: 1024   epsilon: 1.0    steps: 1187     evaluation reward: 159.2\n",
      "Training network\n",
      "Iteration 874: Policy loss: -29.216860. Value loss: 28.984396. Entropy: 1.790363.\n",
      "Iteration 875: Policy loss: -29.357609. Value loss: 29.116039. Entropy: 1.790354.\n",
      "Iteration 876: Policy loss: -29.642828. Value loss: 29.393000. Entropy: 1.790388.\n",
      "episode: 427   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 375     evaluation reward: 159.25\n",
      "now time :  2018-12-19 13:23:37.973086\n",
      "Training network\n",
      "Iteration 877: Policy loss: -8.411397. Value loss: 8.397514. Entropy: 1.790444.\n",
      "Iteration 878: Policy loss: -8.432077. Value loss: 8.414787. Entropy: 1.790435.\n",
      "Iteration 879: Policy loss: -8.425168. Value loss: 8.403979. Entropy: 1.790444.\n",
      "episode: 428   score: 150.0   memory length: 1024   epsilon: 1.0    steps: 908     evaluation reward: 158.3\n",
      "Training network\n",
      "Iteration 880: Policy loss: -11.849800. Value loss: 11.607387. Entropy: 1.790201.\n",
      "Iteration 881: Policy loss: -11.812577. Value loss: 11.572580. Entropy: 1.790243.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 882: Policy loss: -11.741304. Value loss: 11.500789. Entropy: 1.790209.\n",
      "episode: 429   score: 190.0   memory length: 1024   epsilon: 1.0    steps: 1059     evaluation reward: 158.95\n",
      "Training network\n",
      "Iteration 883: Policy loss: -12.723302. Value loss: 12.712211. Entropy: 1.790322.\n",
      "Iteration 884: Policy loss: -12.779024. Value loss: 12.763927. Entropy: 1.790314.\n",
      "Iteration 885: Policy loss: -12.754311. Value loss: 12.744040. Entropy: 1.790329.\n",
      "episode: 430   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 986     evaluation reward: 159.25\n",
      "Training network\n",
      "Iteration 886: Policy loss: -20.325565. Value loss: 20.224117. Entropy: 1.790319.\n",
      "Iteration 887: Policy loss: -20.211386. Value loss: 20.106924. Entropy: 1.790315.\n",
      "Iteration 888: Policy loss: -20.193645. Value loss: 20.088160. Entropy: 1.790302.\n",
      "episode: 431   score: 430.0   memory length: 1024   epsilon: 1.0    steps: 1512     evaluation reward: 159.15\n",
      "Training network\n",
      "Iteration 889: Policy loss: -9.553556. Value loss: 9.252105. Entropy: 1.790122.\n",
      "Iteration 890: Policy loss: -9.405567. Value loss: 9.104614. Entropy: 1.790123.\n",
      "Iteration 891: Policy loss: -9.435191. Value loss: 9.136433. Entropy: 1.790132.\n",
      "episode: 432   score: 260.0   memory length: 1024   epsilon: 1.0    steps: 1096     evaluation reward: 160.7\n",
      "Training network\n",
      "Iteration 892: Policy loss: -13.855110. Value loss: 13.536778. Entropy: 1.790354.\n",
      "Iteration 893: Policy loss: -13.712160. Value loss: 13.393314. Entropy: 1.790365.\n",
      "Iteration 894: Policy loss: -13.728847. Value loss: 13.411425. Entropy: 1.790399.\n",
      "episode: 433   score: 140.0   memory length: 1024   epsilon: 1.0    steps: 699     evaluation reward: 160.95\n",
      "Training network\n",
      "Iteration 895: Policy loss: -25.203068. Value loss: 25.016642. Entropy: 1.790481.\n",
      "Iteration 896: Policy loss: -25.518927. Value loss: 25.325882. Entropy: 1.790471.\n",
      "Iteration 897: Policy loss: -25.205254. Value loss: 25.014317. Entropy: 1.790499.\n",
      "episode: 434   score: 725.0   memory length: 1024   epsilon: 1.0    steps: 1471     evaluation reward: 165.15\n",
      "Training network\n",
      "Iteration 898: Policy loss: -22.213888. Value loss: 21.945974. Entropy: 1.789760.\n",
      "Iteration 899: Policy loss: -21.894678. Value loss: 21.628160. Entropy: 1.789800.\n",
      "Iteration 900: Policy loss: -21.987467. Value loss: 21.716759. Entropy: 1.789805.\n",
      "episode: 435   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 610     evaluation reward: 162.9\n",
      "Training network\n",
      "Iteration 901: Policy loss: -8.309825. Value loss: 8.242672. Entropy: 1.790596.\n",
      "Iteration 902: Policy loss: -8.331198. Value loss: 8.260430. Entropy: 1.790641.\n",
      "Iteration 903: Policy loss: -8.272317. Value loss: 8.204246. Entropy: 1.790605.\n",
      "episode: 436   score: 100.0   memory length: 1024   epsilon: 1.0    steps: 698     evaluation reward: 161.1\n",
      "episode: 437   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 750     evaluation reward: 161.05\n",
      "Training network\n",
      "Iteration 904: Policy loss: -13.792540. Value loss: 13.760987. Entropy: 1.790648.\n",
      "Iteration 905: Policy loss: -14.133874. Value loss: 14.092749. Entropy: 1.790643.\n",
      "Iteration 906: Policy loss: -13.995557. Value loss: 13.959191. Entropy: 1.790598.\n",
      "episode: 438   score: 365.0   memory length: 1024   epsilon: 1.0    steps: 718     evaluation reward: 160.4\n",
      "episode: 439   score: 85.0   memory length: 1024   epsilon: 1.0    steps: 418     evaluation reward: 160.15\n",
      "Training network\n",
      "Iteration 907: Policy loss: -22.710312. Value loss: 22.696659. Entropy: 1.790245.\n",
      "Iteration 908: Policy loss: -22.212023. Value loss: 22.201866. Entropy: 1.790180.\n",
      "Iteration 909: Policy loss: -22.391308. Value loss: 22.381552. Entropy: 1.790256.\n",
      "episode: 440   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 762     evaluation reward: 160.75\n",
      "Training network\n",
      "Iteration 910: Policy loss: -11.107799. Value loss: 10.954915. Entropy: 1.790297.\n",
      "Iteration 911: Policy loss: -11.159943. Value loss: 11.004064. Entropy: 1.790371.\n",
      "Iteration 912: Policy loss: -11.134896. Value loss: 10.980349. Entropy: 1.790354.\n",
      "episode: 441   score: 115.0   memory length: 1024   epsilon: 1.0    steps: 677     evaluation reward: 160.35\n",
      "episode: 442   score: 60.0   memory length: 1024   epsilon: 1.0    steps: 393     evaluation reward: 157.95\n",
      "Training network\n",
      "Iteration 913: Policy loss: -5.556014. Value loss: 5.707354. Entropy: 1.790607.\n",
      "Iteration 914: Policy loss: -5.489928. Value loss: 5.646410. Entropy: 1.790595.\n",
      "Iteration 915: Policy loss: -5.565707. Value loss: 5.718956. Entropy: 1.790599.\n",
      "episode: 443   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 156.95\n",
      "episode: 444   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 764     evaluation reward: 156.45\n",
      "Training network\n",
      "Iteration 916: Policy loss: -9.530876. Value loss: 9.449708. Entropy: 1.790490.\n",
      "Iteration 917: Policy loss: -9.525011. Value loss: 9.444427. Entropy: 1.790488.\n",
      "Iteration 918: Policy loss: -9.540918. Value loss: 9.455521. Entropy: 1.790495.\n",
      "episode: 445   score: 85.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 156.85\n",
      "episode: 446   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 678     evaluation reward: 153.4\n",
      "Training network\n",
      "Iteration 919: Policy loss: -4.856730. Value loss: 4.874524. Entropy: 1.790502.\n",
      "Iteration 920: Policy loss: -4.899069. Value loss: 4.910984. Entropy: 1.790484.\n",
      "Iteration 921: Policy loss: -4.875232. Value loss: 4.889631. Entropy: 1.790524.\n",
      "episode: 447   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 674     evaluation reward: 153.7\n",
      "Training network\n",
      "Iteration 922: Policy loss: -11.084318. Value loss: 11.080514. Entropy: 1.790470.\n",
      "Iteration 923: Policy loss: -11.120293. Value loss: 11.113709. Entropy: 1.790476.\n",
      "Iteration 924: Policy loss: -11.039498. Value loss: 11.036363. Entropy: 1.790498.\n",
      "episode: 448   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 588     evaluation reward: 153.05\n",
      "episode: 449   score: 100.0   memory length: 1024   epsilon: 1.0    steps: 536     evaluation reward: 153.25\n",
      "Training network\n",
      "Iteration 925: Policy loss: -6.352921. Value loss: 6.413707. Entropy: 1.790566.\n",
      "Iteration 926: Policy loss: -6.272887. Value loss: 6.337356. Entropy: 1.790612.\n",
      "Iteration 927: Policy loss: -6.320930. Value loss: 6.384416. Entropy: 1.790606.\n",
      "episode: 450   score: 45.0   memory length: 1024   epsilon: 1.0    steps: 489     evaluation reward: 152.25\n",
      "episode: 451   score: 20.0   memory length: 1024   epsilon: 1.0    steps: 568     evaluation reward: 151.1\n",
      "Training network\n",
      "Iteration 928: Policy loss: -2.513865. Value loss: 2.771700. Entropy: 1.790452.\n",
      "Iteration 929: Policy loss: -2.478984. Value loss: 2.739907. Entropy: 1.790452.\n",
      "Iteration 930: Policy loss: -2.520929. Value loss: 2.775121. Entropy: 1.790479.\n",
      "episode: 452   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 760     evaluation reward: 149.9\n",
      "episode: 453   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 397     evaluation reward: 149.9\n",
      "Training network\n",
      "Iteration 931: Policy loss: -9.921350. Value loss: 9.834698. Entropy: 1.790534.\n",
      "Iteration 932: Policy loss: -9.971917. Value loss: 9.878531. Entropy: 1.790591.\n",
      "Iteration 933: Policy loss: -9.952413. Value loss: 9.859076. Entropy: 1.790539.\n",
      "episode: 454   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 474     evaluation reward: 145.25\n",
      "episode: 455   score: 230.0   memory length: 1024   epsilon: 1.0    steps: 776     evaluation reward: 146.25\n",
      "Training network\n",
      "Iteration 934: Policy loss: -11.848856. Value loss: 11.895208. Entropy: 1.790390.\n",
      "Iteration 935: Policy loss: -11.890430. Value loss: 11.931680. Entropy: 1.790473.\n",
      "Iteration 936: Policy loss: -11.884184. Value loss: 11.926379. Entropy: 1.790436.\n",
      "episode: 456   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 367     evaluation reward: 141.95\n",
      "Training network\n",
      "Iteration 937: Policy loss: -14.072354. Value loss: 13.975542. Entropy: 1.790174.\n",
      "Iteration 938: Policy loss: -14.152877. Value loss: 14.047416. Entropy: 1.790189.\n",
      "Iteration 939: Policy loss: -14.084410. Value loss: 13.985493. Entropy: 1.790223.\n",
      "episode: 457   score: 255.0   memory length: 1024   epsilon: 1.0    steps: 1118     evaluation reward: 143.95\n",
      "Training network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 940: Policy loss: -5.770843. Value loss: 5.672334. Entropy: 1.790347.\n",
      "Iteration 941: Policy loss: -5.780067. Value loss: 5.678028. Entropy: 1.790316.\n",
      "Iteration 942: Policy loss: -5.755987. Value loss: 5.658277. Entropy: 1.790288.\n",
      "episode: 458   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 687     evaluation reward: 144.55\n",
      "episode: 459   score: 400.0   memory length: 1024   epsilon: 1.0    steps: 731     evaluation reward: 147.2\n",
      "Training network\n",
      "Iteration 943: Policy loss: -26.439016. Value loss: 26.221245. Entropy: 1.790645.\n",
      "Iteration 944: Policy loss: -26.582270. Value loss: 26.363089. Entropy: 1.790618.\n",
      "Iteration 945: Policy loss: -26.552385. Value loss: 26.316864. Entropy: 1.790609.\n",
      "episode: 460   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 585     evaluation reward: 147.3\n",
      "episode: 461   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 401     evaluation reward: 146.85\n",
      "Training network\n",
      "Iteration 946: Policy loss: -7.114702. Value loss: 7.010202. Entropy: 1.790652.\n",
      "Iteration 947: Policy loss: -7.151466. Value loss: 7.045672. Entropy: 1.790638.\n",
      "Iteration 948: Policy loss: -7.122696. Value loss: 7.010942. Entropy: 1.790628.\n",
      "episode: 462   score: 160.0   memory length: 1024   epsilon: 1.0    steps: 709     evaluation reward: 147.35\n",
      "Training network\n",
      "Iteration 949: Policy loss: -6.484502. Value loss: 6.506483. Entropy: 1.790397.\n",
      "Iteration 950: Policy loss: -6.558414. Value loss: 6.576355. Entropy: 1.790379.\n",
      "Iteration 951: Policy loss: -6.506739. Value loss: 6.527417. Entropy: 1.790384.\n",
      "episode: 463   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 573     evaluation reward: 146.8\n",
      "episode: 464   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 702     evaluation reward: 146.4\n",
      "Training network\n",
      "Iteration 952: Policy loss: -7.089437. Value loss: 6.953339. Entropy: 1.790471.\n",
      "Iteration 953: Policy loss: -7.063276. Value loss: 6.928507. Entropy: 1.790463.\n",
      "Iteration 954: Policy loss: -7.079422. Value loss: 6.937106. Entropy: 1.790462.\n",
      "episode: 465   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 724     evaluation reward: 145.75\n",
      "Training network\n",
      "Iteration 955: Policy loss: -17.594707. Value loss: 17.352943. Entropy: 1.790481.\n",
      "Iteration 956: Policy loss: -17.458147. Value loss: 17.227058. Entropy: 1.790477.\n",
      "Iteration 957: Policy loss: -17.600281. Value loss: 17.363605. Entropy: 1.790529.\n",
      "episode: 466   score: 270.0   memory length: 1024   epsilon: 1.0    steps: 833     evaluation reward: 141.05\n",
      "episode: 467   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 509     evaluation reward: 140.15\n",
      "Training network\n",
      "Iteration 958: Policy loss: -11.172294. Value loss: 11.192366. Entropy: 1.790128.\n",
      "Iteration 959: Policy loss: -11.224394. Value loss: 11.243240. Entropy: 1.790160.\n",
      "Iteration 960: Policy loss: -11.216162. Value loss: 11.233499. Entropy: 1.790103.\n",
      "episode: 468   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 679     evaluation reward: 140.1\n",
      "Training network\n",
      "Iteration 961: Policy loss: -9.071460. Value loss: 8.962444. Entropy: 1.790576.\n",
      "Iteration 962: Policy loss: -9.167091. Value loss: 9.060761. Entropy: 1.790595.\n",
      "Iteration 963: Policy loss: -8.986839. Value loss: 8.881708. Entropy: 1.790535.\n",
      "episode: 469   score: 160.0   memory length: 1024   epsilon: 1.0    steps: 930     evaluation reward: 140.85\n",
      "episode: 470   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 371     evaluation reward: 140.95\n",
      "Training network\n",
      "Iteration 964: Policy loss: -5.375494. Value loss: 5.275994. Entropy: 1.790571.\n",
      "Iteration 965: Policy loss: -5.387902. Value loss: 5.280879. Entropy: 1.790556.\n",
      "Iteration 966: Policy loss: -5.321555. Value loss: 5.219135. Entropy: 1.790521.\n",
      "episode: 471   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 919     evaluation reward: 140.65\n",
      "Training network\n",
      "Iteration 967: Policy loss: -9.344538. Value loss: 9.141577. Entropy: 1.790429.\n",
      "Iteration 968: Policy loss: -9.433562. Value loss: 9.233605. Entropy: 1.790443.\n",
      "Iteration 969: Policy loss: -9.403413. Value loss: 9.202138. Entropy: 1.790453.\n",
      "episode: 472   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 502     evaluation reward: 138.9\n",
      "episode: 473   score: 70.0   memory length: 1024   epsilon: 1.0    steps: 631     evaluation reward: 139.25\n",
      "Training network\n",
      "Iteration 970: Policy loss: -6.295006. Value loss: 6.418350. Entropy: 1.790639.\n",
      "Iteration 971: Policy loss: -6.319654. Value loss: 6.441844. Entropy: 1.790658.\n",
      "Iteration 972: Policy loss: -6.425820. Value loss: 6.541190. Entropy: 1.790644.\n",
      "episode: 474   score: 215.0   memory length: 1024   epsilon: 1.0    steps: 1005     evaluation reward: 140.8\n",
      "Training network\n",
      "Iteration 973: Policy loss: -12.872493. Value loss: 12.575480. Entropy: 1.790393.\n",
      "Iteration 974: Policy loss: -12.886695. Value loss: 12.584911. Entropy: 1.790420.\n",
      "Iteration 975: Policy loss: -12.980930. Value loss: 12.680260. Entropy: 1.790412.\n",
      "episode: 475   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 505     evaluation reward: 139.25\n",
      "episode: 476   score: 35.0   memory length: 1024   epsilon: 1.0    steps: 485     evaluation reward: 138.25\n",
      "Training network\n",
      "Iteration 976: Policy loss: -6.914343. Value loss: 7.133855. Entropy: 1.790637.\n",
      "Iteration 977: Policy loss: -6.929076. Value loss: 7.150338. Entropy: 1.790627.\n",
      "Iteration 978: Policy loss: -6.851236. Value loss: 7.073078. Entropy: 1.790626.\n",
      "episode: 477   score: 195.0   memory length: 1024   epsilon: 1.0    steps: 1030     evaluation reward: 139.35\n",
      "Training network\n",
      "Iteration 979: Policy loss: -8.639481. Value loss: 8.514273. Entropy: 1.790124.\n",
      "Iteration 980: Policy loss: -8.562648. Value loss: 8.441339. Entropy: 1.790128.\n",
      "Iteration 981: Policy loss: -8.664074. Value loss: 8.534340. Entropy: 1.790142.\n",
      "episode: 478   score: 90.0   memory length: 1024   epsilon: 1.0    steps: 383     evaluation reward: 139.0\n",
      "episode: 479   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 628     evaluation reward: 138.6\n",
      "Training network\n",
      "Iteration 982: Policy loss: -5.342944. Value loss: 5.348742. Entropy: 1.790495.\n",
      "Iteration 983: Policy loss: -5.323297. Value loss: 5.332666. Entropy: 1.790509.\n",
      "Iteration 984: Policy loss: -5.336970. Value loss: 5.340010. Entropy: 1.790529.\n",
      "episode: 480   score: 225.0   memory length: 1024   epsilon: 1.0    steps: 962     evaluation reward: 139.8\n",
      "episode: 481   score: 35.0   memory length: 1024   epsilon: 1.0    steps: 408     evaluation reward: 139.05\n",
      "Training network\n",
      "Iteration 985: Policy loss: -9.152804. Value loss: 9.252151. Entropy: 1.790470.\n",
      "Iteration 986: Policy loss: -9.208560. Value loss: 9.303186. Entropy: 1.790516.\n",
      "Iteration 987: Policy loss: -9.177490. Value loss: 9.270414. Entropy: 1.790466.\n",
      "episode: 482   score: 250.0   memory length: 1024   epsilon: 1.0    steps: 973     evaluation reward: 138.45\n",
      "Training network\n",
      "Iteration 988: Policy loss: -14.425323. Value loss: 14.216963. Entropy: 1.790334.\n",
      "Iteration 989: Policy loss: -14.348524. Value loss: 14.132044. Entropy: 1.790288.\n",
      "Iteration 990: Policy loss: -14.230069. Value loss: 14.015340. Entropy: 1.790376.\n",
      "episode: 483   score: 85.0   memory length: 1024   epsilon: 1.0    steps: 570     evaluation reward: 137.4\n",
      "Training network\n",
      "Iteration 991: Policy loss: -9.019743. Value loss: 8.952473. Entropy: 1.790547.\n",
      "Iteration 992: Policy loss: -9.069178. Value loss: 9.003953. Entropy: 1.790581.\n",
      "Iteration 993: Policy loss: -9.108208. Value loss: 9.035323. Entropy: 1.790542.\n",
      "episode: 484   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 630     evaluation reward: 138.15\n",
      "episode: 485   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 393     evaluation reward: 137.75\n",
      "Training network\n",
      "Iteration 994: Policy loss: -10.660232. Value loss: 10.616092. Entropy: 1.790656.\n",
      "Iteration 995: Policy loss: -10.647338. Value loss: 10.602335. Entropy: 1.790656.\n",
      "Iteration 996: Policy loss: -10.697646. Value loss: 10.645873. Entropy: 1.790634.\n",
      "episode: 486   score: 140.0   memory length: 1024   epsilon: 1.0    steps: 788     evaluation reward: 138.6\n",
      "episode: 487   score: 35.0   memory length: 1024   epsilon: 1.0    steps: 520     evaluation reward: 138.05\n",
      "Training network\n",
      "Iteration 997: Policy loss: -2.473735. Value loss: 2.687627. Entropy: 1.790594.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 998: Policy loss: -2.450995. Value loss: 2.669004. Entropy: 1.790593.\n",
      "Iteration 999: Policy loss: -2.498639. Value loss: 2.706491. Entropy: 1.790598.\n",
      "episode: 488   score: 20.0   memory length: 1024   epsilon: 1.0    steps: 393     evaluation reward: 136.1\n",
      "episode: 489   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 659     evaluation reward: 136.1\n",
      "Training network\n",
      "Iteration 1000: Policy loss: -10.066287. Value loss: 9.985717. Entropy: 1.790461.\n",
      "Iteration 1001: Policy loss: -10.088377. Value loss: 10.002498. Entropy: 1.790467.\n",
      "Iteration 1002: Policy loss: -10.015796. Value loss: 9.932034. Entropy: 1.790447.\n",
      "episode: 490   score: 115.0   memory length: 1024   epsilon: 1.0    steps: 774     evaluation reward: 135.9\n",
      "Training network\n",
      "Iteration 1003: Policy loss: -7.247592. Value loss: 7.124660. Entropy: 1.790644.\n",
      "Iteration 1004: Policy loss: -7.198859. Value loss: 7.072421. Entropy: 1.790643.\n",
      "Iteration 1005: Policy loss: -7.242066. Value loss: 7.110168. Entropy: 1.790629.\n",
      "episode: 491   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 519     evaluation reward: 135.45\n",
      "episode: 492   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 647     evaluation reward: 135.15\n",
      "Training network\n",
      "Iteration 1006: Policy loss: -9.006759. Value loss: 8.929869. Entropy: 1.790462.\n",
      "Iteration 1007: Policy loss: -8.868279. Value loss: 8.798643. Entropy: 1.790483.\n",
      "Iteration 1008: Policy loss: -8.849847. Value loss: 8.777760. Entropy: 1.790482.\n",
      "episode: 493   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 677     evaluation reward: 136.45\n",
      "episode: 494   score: 75.0   memory length: 1024   epsilon: 1.0    steps: 595     evaluation reward: 136.0\n",
      "Training network\n",
      "Iteration 1009: Policy loss: -9.868187. Value loss: 10.004179. Entropy: 1.790700.\n",
      "Iteration 1010: Policy loss: -9.919724. Value loss: 10.054069. Entropy: 1.790680.\n",
      "Iteration 1011: Policy loss: -10.043289. Value loss: 10.169621. Entropy: 1.790668.\n",
      "episode: 495   score: 95.0   memory length: 1024   epsilon: 1.0    steps: 436     evaluation reward: 136.15\n",
      "episode: 496   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 135.1\n",
      "Training network\n",
      "Iteration 1012: Policy loss: -11.365433. Value loss: 11.442466. Entropy: 1.790726.\n",
      "Iteration 1013: Policy loss: -11.243601. Value loss: 11.324333. Entropy: 1.790761.\n",
      "Iteration 1014: Policy loss: -11.335496. Value loss: 11.403580. Entropy: 1.790740.\n",
      "episode: 497   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 608     evaluation reward: 134.9\n",
      "episode: 498   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 553     evaluation reward: 136.0\n",
      "Training network\n",
      "Iteration 1015: Policy loss: -6.691993. Value loss: 6.975058. Entropy: 1.790577.\n",
      "Iteration 1016: Policy loss: -6.592212. Value loss: 6.879078. Entropy: 1.790551.\n",
      "Iteration 1017: Policy loss: -6.672746. Value loss: 6.954386. Entropy: 1.790632.\n",
      "Training network\n",
      "Iteration 1018: Policy loss: -7.899234. Value loss: 7.822702. Entropy: 1.790189.\n",
      "Iteration 1019: Policy loss: -7.990269. Value loss: 7.909542. Entropy: 1.790211.\n",
      "Iteration 1020: Policy loss: -7.922476. Value loss: 7.844388. Entropy: 1.790125.\n",
      "episode: 499   score: 390.0   memory length: 1024   epsilon: 1.0    steps: 1562     evaluation reward: 138.65\n",
      "episode: 500   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 392     evaluation reward: 137.5\n",
      "Training network\n",
      "Iteration 1021: Policy loss: -16.952202. Value loss: 16.976160. Entropy: 1.790148.\n",
      "Iteration 1022: Policy loss: -17.119614. Value loss: 17.129976. Entropy: 1.790216.\n",
      "Iteration 1023: Policy loss: -17.084021. Value loss: 17.099556. Entropy: 1.790224.\n",
      "episode: 501   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 813     evaluation reward: 137.75\n",
      "now time :  2018-12-19 13:26:35.652635\n",
      "Training network\n",
      "Iteration 1024: Policy loss: -13.829934. Value loss: 13.857005. Entropy: 1.790582.\n",
      "Iteration 1025: Policy loss: -13.809449. Value loss: 13.840474. Entropy: 1.790575.\n",
      "Iteration 1026: Policy loss: -13.845520. Value loss: 13.873911. Entropy: 1.790562.\n",
      "episode: 502   score: 195.0   memory length: 1024   epsilon: 1.0    steps: 1017     evaluation reward: 138.9\n",
      "Training network\n",
      "Iteration 1027: Policy loss: -5.531502. Value loss: 5.453585. Entropy: 1.790397.\n",
      "Iteration 1028: Policy loss: -5.571702. Value loss: 5.490606. Entropy: 1.790403.\n",
      "Iteration 1029: Policy loss: -5.566533. Value loss: 5.485698. Entropy: 1.790400.\n",
      "episode: 503   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 593     evaluation reward: 139.4\n",
      "episode: 504   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 523     evaluation reward: 138.4\n",
      "Training network\n",
      "Iteration 1030: Policy loss: -8.771108. Value loss: 8.706848. Entropy: 1.790659.\n",
      "Iteration 1031: Policy loss: -8.785219. Value loss: 8.722782. Entropy: 1.790684.\n",
      "Iteration 1032: Policy loss: -8.841839. Value loss: 8.770542. Entropy: 1.790653.\n",
      "episode: 505   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 560     evaluation reward: 138.0\n",
      "episode: 506   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 396     evaluation reward: 138.2\n",
      "Training network\n",
      "Iteration 1033: Policy loss: -7.731580. Value loss: 7.721300. Entropy: 1.790608.\n",
      "Iteration 1034: Policy loss: -7.669480. Value loss: 7.664067. Entropy: 1.790610.\n",
      "Iteration 1035: Policy loss: -7.600085. Value loss: 7.596407. Entropy: 1.790612.\n",
      "episode: 507   score: 100.0   memory length: 1024   epsilon: 1.0    steps: 454     evaluation reward: 138.7\n",
      "episode: 508   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 663     evaluation reward: 137.85\n",
      "Training network\n",
      "Iteration 1036: Policy loss: -7.408267. Value loss: 7.288358. Entropy: 1.790704.\n",
      "Iteration 1037: Policy loss: -7.473416. Value loss: 7.347546. Entropy: 1.790656.\n",
      "Iteration 1038: Policy loss: -7.514560. Value loss: 7.387389. Entropy: 1.790705.\n",
      "episode: 509   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 380     evaluation reward: 134.4\n",
      "episode: 510   score: 90.0   memory length: 1024   epsilon: 1.0    steps: 663     evaluation reward: 134.2\n",
      "Training network\n",
      "Iteration 1039: Policy loss: -9.739594. Value loss: 9.805706. Entropy: 1.790580.\n",
      "Iteration 1040: Policy loss: -9.908772. Value loss: 9.972602. Entropy: 1.790590.\n",
      "Iteration 1041: Policy loss: -9.832541. Value loss: 9.897096. Entropy: 1.790605.\n",
      "episode: 511   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 388     evaluation reward: 133.95\n",
      "episode: 512   score: 345.0   memory length: 1024   epsilon: 1.0    steps: 827     evaluation reward: 136.2\n",
      "Training network\n",
      "Iteration 1042: Policy loss: -22.507622. Value loss: 22.228815. Entropy: 1.790169.\n",
      "Iteration 1043: Policy loss: -22.429825. Value loss: 22.145927. Entropy: 1.790203.\n",
      "Iteration 1044: Policy loss: -22.345928. Value loss: 22.059343. Entropy: 1.790222.\n",
      "episode: 513   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 397     evaluation reward: 136.25\n",
      "episode: 514   score: 75.0   memory length: 1024   epsilon: 1.0    steps: 403     evaluation reward: 136.55\n",
      "Training network\n",
      "Iteration 1045: Policy loss: -8.931575. Value loss: 8.975752. Entropy: 1.790680.\n",
      "Iteration 1046: Policy loss: -8.840672. Value loss: 8.890883. Entropy: 1.790682.\n",
      "Iteration 1047: Policy loss: -8.810653. Value loss: 8.858568. Entropy: 1.790688.\n",
      "episode: 515   score: 315.0   memory length: 1024   epsilon: 1.0    steps: 1051     evaluation reward: 138.4\n",
      "Training network\n",
      "Iteration 1048: Policy loss: -14.558301. Value loss: 14.505903. Entropy: 1.790257.\n",
      "Iteration 1049: Policy loss: -14.412820. Value loss: 14.362123. Entropy: 1.790256.\n",
      "Iteration 1050: Policy loss: -14.369521. Value loss: 14.321524. Entropy: 1.790249.\n",
      "episode: 516   score: 205.0   memory length: 1024   epsilon: 1.0    steps: 779     evaluation reward: 139.95\n",
      "Training network\n",
      "Iteration 1051: Policy loss: -10.381562. Value loss: 10.412910. Entropy: 1.790619.\n",
      "Iteration 1052: Policy loss: -10.493376. Value loss: 10.524064. Entropy: 1.790606.\n",
      "Iteration 1053: Policy loss: -10.514373. Value loss: 10.543748. Entropy: 1.790605.\n",
      "episode: 517   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 607     evaluation reward: 140.05\n",
      "episode: 518   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 544     evaluation reward: 139.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network\n",
      "Iteration 1054: Policy loss: -6.298542. Value loss: 6.403737. Entropy: 1.790455.\n",
      "Iteration 1055: Policy loss: -6.224356. Value loss: 6.331198. Entropy: 1.790466.\n",
      "Iteration 1056: Policy loss: -6.310217. Value loss: 6.412447. Entropy: 1.790443.\n",
      "episode: 519   score: 60.0   memory length: 1024   epsilon: 1.0    steps: 617     evaluation reward: 140.05\n",
      "episode: 520   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 812     evaluation reward: 140.75\n",
      "Training network\n",
      "Iteration 1057: Policy loss: -8.166762. Value loss: 8.026156. Entropy: 1.790380.\n",
      "Iteration 1058: Policy loss: -8.208202. Value loss: 8.060854. Entropy: 1.790404.\n",
      "Iteration 1059: Policy loss: -8.176497. Value loss: 8.036833. Entropy: 1.790329.\n",
      "episode: 521   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 768     evaluation reward: 140.5\n",
      "Training network\n",
      "Iteration 1060: Policy loss: -7.227183. Value loss: 7.099004. Entropy: 1.790560.\n",
      "Iteration 1061: Policy loss: -7.167808. Value loss: 7.038353. Entropy: 1.790626.\n",
      "Iteration 1062: Policy loss: -7.273412. Value loss: 7.139727. Entropy: 1.790597.\n",
      "episode: 522   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 699     evaluation reward: 142.05\n",
      "Training network\n",
      "Iteration 1063: Policy loss: -22.307541. Value loss: 21.920492. Entropy: 1.790541.\n",
      "Iteration 1064: Policy loss: -22.164667. Value loss: 21.776487. Entropy: 1.790567.\n",
      "Iteration 1065: Policy loss: -22.086899. Value loss: 21.694296. Entropy: 1.790536.\n",
      "episode: 523   score: 305.0   memory length: 1024   epsilon: 1.0    steps: 946     evaluation reward: 144.65\n",
      "episode: 524   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 427     evaluation reward: 143.6\n",
      "Training network\n",
      "Iteration 1066: Policy loss: -4.398473. Value loss: 4.800974. Entropy: 1.790328.\n",
      "Iteration 1067: Policy loss: -4.567159. Value loss: 4.963966. Entropy: 1.790311.\n",
      "Iteration 1068: Policy loss: -4.437562. Value loss: 4.841219. Entropy: 1.790313.\n",
      "episode: 525   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 629     evaluation reward: 143.0\n",
      "Training network\n",
      "Iteration 1069: Policy loss: -12.790958. Value loss: 12.732409. Entropy: 1.790531.\n",
      "Iteration 1070: Policy loss: -12.718649. Value loss: 12.667067. Entropy: 1.790515.\n",
      "Iteration 1071: Policy loss: -12.707666. Value loss: 12.652899. Entropy: 1.790515.\n",
      "episode: 526   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 1401     evaluation reward: 139.25\n",
      "Training network\n",
      "Iteration 1072: Policy loss: -6.756687. Value loss: 6.796353. Entropy: 1.790145.\n",
      "Iteration 1073: Policy loss: -6.772866. Value loss: 6.814941. Entropy: 1.790182.\n",
      "Iteration 1074: Policy loss: -6.697363. Value loss: 6.741981. Entropy: 1.790122.\n",
      "episode: 527   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 958     evaluation reward: 140.55\n",
      "Training network\n",
      "Iteration 1075: Policy loss: -18.883369. Value loss: 18.638035. Entropy: 1.790493.\n",
      "Iteration 1076: Policy loss: -18.739197. Value loss: 18.491947. Entropy: 1.790501.\n",
      "Iteration 1077: Policy loss: -18.971321. Value loss: 18.716736. Entropy: 1.790495.\n",
      "episode: 528   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 650     evaluation reward: 141.15\n",
      "episode: 529   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 663     evaluation reward: 140.5\n",
      "Training network\n",
      "Iteration 1078: Policy loss: -6.939610. Value loss: 7.340386. Entropy: 1.790684.\n",
      "Iteration 1079: Policy loss: -6.926482. Value loss: 7.324677. Entropy: 1.790662.\n",
      "Iteration 1080: Policy loss: -6.876198. Value loss: 7.279578. Entropy: 1.790660.\n",
      "episode: 530   score: 30.0   memory length: 1024   epsilon: 1.0    steps: 412     evaluation reward: 138.7\n",
      "Training network\n",
      "Iteration 1081: Policy loss: -9.471781. Value loss: 9.379807. Entropy: 1.790473.\n",
      "Iteration 1082: Policy loss: -9.572070. Value loss: 9.477559. Entropy: 1.790474.\n",
      "Iteration 1083: Policy loss: -9.425617. Value loss: 9.332903. Entropy: 1.790487.\n",
      "episode: 531   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 779     evaluation reward: 136.5\n",
      "episode: 532   score: 165.0   memory length: 1024   epsilon: 1.0    steps: 628     evaluation reward: 135.55\n",
      "Training network\n",
      "Iteration 1084: Policy loss: -12.857926. Value loss: 12.865455. Entropy: 1.790587.\n",
      "Iteration 1085: Policy loss: -12.901930. Value loss: 12.906058. Entropy: 1.790542.\n",
      "Iteration 1086: Policy loss: -12.774100. Value loss: 12.784258. Entropy: 1.790568.\n",
      "episode: 533   score: 90.0   memory length: 1024   epsilon: 1.0    steps: 450     evaluation reward: 135.05\n",
      "Training network\n",
      "Iteration 1087: Policy loss: -9.432960. Value loss: 9.592762. Entropy: 1.790647.\n",
      "Iteration 1088: Policy loss: -9.349144. Value loss: 9.503377. Entropy: 1.790645.\n",
      "Iteration 1089: Policy loss: -9.334551. Value loss: 9.487243. Entropy: 1.790642.\n",
      "episode: 534   score: 415.0   memory length: 1024   epsilon: 1.0    steps: 1148     evaluation reward: 131.95\n",
      "episode: 535   score: 20.0   memory length: 1024   epsilon: 1.0    steps: 493     evaluation reward: 130.8\n",
      "Training network\n",
      "Iteration 1090: Policy loss: -17.278072. Value loss: 17.210363. Entropy: 1.790401.\n",
      "Iteration 1091: Policy loss: -17.179026. Value loss: 17.116634. Entropy: 1.790364.\n",
      "Iteration 1092: Policy loss: -17.120546. Value loss: 17.048923. Entropy: 1.790382.\n",
      "episode: 536   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 781     evaluation reward: 131.0\n",
      "Training network\n",
      "Iteration 1093: Policy loss: -10.674733. Value loss: 10.647854. Entropy: 1.790526.\n",
      "Iteration 1094: Policy loss: -10.665135. Value loss: 10.646259. Entropy: 1.790485.\n",
      "Iteration 1095: Policy loss: -10.685276. Value loss: 10.665582. Entropy: 1.790480.\n",
      "episode: 537   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 611     evaluation reward: 131.0\n",
      "episode: 538   score: 60.0   memory length: 1024   epsilon: 1.0    steps: 538     evaluation reward: 127.95\n",
      "Training network\n",
      "Iteration 1096: Policy loss: -9.418267. Value loss: 9.439693. Entropy: 1.790580.\n",
      "Iteration 1097: Policy loss: -9.536269. Value loss: 9.552383. Entropy: 1.790577.\n",
      "Iteration 1098: Policy loss: -9.519172. Value loss: 9.537896. Entropy: 1.790562.\n",
      "episode: 539   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 675     evaluation reward: 128.45\n",
      "episode: 540   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 469     evaluation reward: 128.15\n",
      "Training network\n",
      "Iteration 1099: Policy loss: -6.921576. Value loss: 6.834875. Entropy: 1.790534.\n",
      "Iteration 1100: Policy loss: -6.941817. Value loss: 6.850903. Entropy: 1.790518.\n",
      "Iteration 1101: Policy loss: -6.930423. Value loss: 6.843037. Entropy: 1.790537.\n",
      "episode: 541   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 629     evaluation reward: 128.05\n",
      "Training network\n",
      "Iteration 1102: Policy loss: -10.132161. Value loss: 10.379886. Entropy: 1.790673.\n",
      "Iteration 1103: Policy loss: -10.079103. Value loss: 10.325646. Entropy: 1.790657.\n",
      "Iteration 1104: Policy loss: -10.054383. Value loss: 10.302396. Entropy: 1.790651.\n",
      "episode: 542   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 722     evaluation reward: 128.65\n",
      "episode: 543   score: 115.0   memory length: 1024   epsilon: 1.0    steps: 558     evaluation reward: 129.25\n",
      "Training network\n",
      "Iteration 1105: Policy loss: -7.181525. Value loss: 7.217422. Entropy: 1.790730.\n",
      "Iteration 1106: Policy loss: -7.104528. Value loss: 7.147324. Entropy: 1.790709.\n",
      "Iteration 1107: Policy loss: -7.029407. Value loss: 7.076295. Entropy: 1.790721.\n",
      "episode: 544   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 772     evaluation reward: 129.25\n",
      "Training network\n",
      "Iteration 1108: Policy loss: -11.204535. Value loss: 11.109638. Entropy: 1.790615.\n",
      "Iteration 1109: Policy loss: -11.247695. Value loss: 11.145790. Entropy: 1.790595.\n",
      "Iteration 1110: Policy loss: -11.274343. Value loss: 11.172271. Entropy: 1.790584.\n",
      "episode: 545   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 694     evaluation reward: 129.6\n",
      "episode: 546   score: 245.0   memory length: 1024   epsilon: 1.0    steps: 914     evaluation reward: 131.4\n",
      "Training network\n",
      "Iteration 1111: Policy loss: -13.764508. Value loss: 13.616517. Entropy: 1.790428.\n",
      "Iteration 1112: Policy loss: -13.839323. Value loss: 13.684542. Entropy: 1.790472.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1113: Policy loss: -13.726963. Value loss: 13.574418. Entropy: 1.790482.\n",
      "episode: 547   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 751     evaluation reward: 131.4\n",
      "Training network\n",
      "Iteration 1114: Policy loss: -7.874037. Value loss: 7.901690. Entropy: 1.790627.\n",
      "Iteration 1115: Policy loss: -7.816551. Value loss: 7.840342. Entropy: 1.790635.\n",
      "Iteration 1116: Policy loss: -7.930241. Value loss: 7.944368. Entropy: 1.790639.\n",
      "episode: 548   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 430     evaluation reward: 131.15\n",
      "episode: 549   score: 190.0   memory length: 1024   epsilon: 1.0    steps: 855     evaluation reward: 132.05\n",
      "Training network\n",
      "Iteration 1117: Policy loss: -10.307676. Value loss: 10.230753. Entropy: 1.790551.\n",
      "Iteration 1118: Policy loss: -10.297706. Value loss: 10.221038. Entropy: 1.790509.\n",
      "Iteration 1119: Policy loss: -10.136799. Value loss: 10.058465. Entropy: 1.790535.\n",
      "episode: 550   score: 255.0   memory length: 1024   epsilon: 1.0    steps: 989     evaluation reward: 134.15\n",
      "Training network\n",
      "Iteration 1120: Policy loss: -15.557610. Value loss: 15.349652. Entropy: 1.790417.\n",
      "Iteration 1121: Policy loss: -15.499434. Value loss: 15.284917. Entropy: 1.790489.\n",
      "Iteration 1122: Policy loss: -15.552134. Value loss: 15.338125. Entropy: 1.790450.\n",
      "episode: 551   score: 215.0   memory length: 1024   epsilon: 1.0    steps: 889     evaluation reward: 136.1\n",
      "Training network\n",
      "Iteration 1123: Policy loss: -9.634188. Value loss: 9.501145. Entropy: 1.790577.\n",
      "Iteration 1124: Policy loss: -9.587789. Value loss: 9.456999. Entropy: 1.790580.\n",
      "Iteration 1125: Policy loss: -9.722629. Value loss: 9.583383. Entropy: 1.790610.\n",
      "episode: 552   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 643     evaluation reward: 135.05\n",
      "episode: 553   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 502     evaluation reward: 135.3\n",
      "Training network\n",
      "Iteration 1126: Policy loss: -4.806850. Value loss: 4.905015. Entropy: 1.790561.\n",
      "Iteration 1127: Policy loss: -4.647132. Value loss: 4.765359. Entropy: 1.790537.\n",
      "Iteration 1128: Policy loss: -4.751661. Value loss: 4.853732. Entropy: 1.790539.\n",
      "episode: 554   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 634     evaluation reward: 136.15\n",
      "Training network\n",
      "Iteration 1129: Policy loss: -10.058157. Value loss: 9.929891. Entropy: 1.790413.\n",
      "Iteration 1130: Policy loss: -9.988822. Value loss: 9.862831. Entropy: 1.790442.\n",
      "Iteration 1131: Policy loss: -9.990579. Value loss: 9.857947. Entropy: 1.790407.\n",
      "episode: 555   score: 270.0   memory length: 1024   epsilon: 1.0    steps: 1349     evaluation reward: 136.55\n",
      "Training network\n",
      "Iteration 1132: Policy loss: -12.076406. Value loss: 11.756530. Entropy: 1.790127.\n",
      "Iteration 1133: Policy loss: -12.162630. Value loss: 11.838145. Entropy: 1.790124.\n",
      "Iteration 1134: Policy loss: -12.169962. Value loss: 11.844736. Entropy: 1.790135.\n",
      "episode: 556   score: 60.0   memory length: 1024   epsilon: 1.0    steps: 409     evaluation reward: 136.65\n",
      "episode: 557   score: 60.0   memory length: 1024   epsilon: 1.0    steps: 609     evaluation reward: 134.7\n",
      "Training network\n",
      "Iteration 1135: Policy loss: -6.119091. Value loss: 6.354979. Entropy: 1.790558.\n",
      "Iteration 1136: Policy loss: -6.203818. Value loss: 6.433672. Entropy: 1.790609.\n",
      "Iteration 1137: Policy loss: -6.155754. Value loss: 6.393822. Entropy: 1.790564.\n",
      "episode: 558   score: 100.0   memory length: 1024   epsilon: 1.0    steps: 495     evaluation reward: 134.6\n",
      "Training network\n",
      "Iteration 1138: Policy loss: -21.815094. Value loss: 21.696297. Entropy: 1.790501.\n",
      "Iteration 1139: Policy loss: -21.266590. Value loss: 21.145653. Entropy: 1.790524.\n",
      "Iteration 1140: Policy loss: -21.356445. Value loss: 21.236217. Entropy: 1.790519.\n",
      "episode: 559   score: 415.0   memory length: 1024   epsilon: 1.0    steps: 881     evaluation reward: 134.75\n",
      "episode: 560   score: 380.0   memory length: 1024   epsilon: 1.0    steps: 835     evaluation reward: 137.5\n",
      "Training network\n",
      "Iteration 1141: Policy loss: -23.996569. Value loss: 23.942198. Entropy: 1.790589.\n",
      "Iteration 1142: Policy loss: -23.742128. Value loss: 23.678877. Entropy: 1.790612.\n",
      "Iteration 1143: Policy loss: -23.654259. Value loss: 23.593044. Entropy: 1.790601.\n",
      "episode: 561   score: 160.0   memory length: 1024   epsilon: 1.0    steps: 676     evaluation reward: 138.55\n",
      "Training network\n",
      "Iteration 1144: Policy loss: -9.741213. Value loss: 9.740446. Entropy: 1.790606.\n",
      "Iteration 1145: Policy loss: -9.797064. Value loss: 9.793705. Entropy: 1.790611.\n",
      "Iteration 1146: Policy loss: -9.875667. Value loss: 9.870458. Entropy: 1.790631.\n",
      "episode: 562   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 557     evaluation reward: 137.75\n",
      "episode: 563   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 626     evaluation reward: 138.25\n",
      "Training network\n",
      "Iteration 1147: Policy loss: -9.542307. Value loss: 9.480938. Entropy: 1.790673.\n",
      "Iteration 1148: Policy loss: -9.475387. Value loss: 9.412407. Entropy: 1.790646.\n",
      "Iteration 1149: Policy loss: -9.432489. Value loss: 9.369281. Entropy: 1.790661.\n",
      "episode: 564   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 654     evaluation reward: 138.6\n",
      "episode: 565   score: 90.0   memory length: 1024   epsilon: 1.0    steps: 434     evaluation reward: 138.7\n",
      "Training network\n",
      "Iteration 1150: Policy loss: -10.117927. Value loss: 10.048555. Entropy: 1.790606.\n",
      "Iteration 1151: Policy loss: -9.968551. Value loss: 9.893305. Entropy: 1.790592.\n",
      "Iteration 1152: Policy loss: -10.125142. Value loss: 10.042711. Entropy: 1.790616.\n",
      "episode: 566   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 786     evaluation reward: 137.2\n",
      "Training network\n",
      "Iteration 1153: Policy loss: -10.999830. Value loss: 11.154410. Entropy: 1.790541.\n",
      "Iteration 1154: Policy loss: -11.013664. Value loss: 11.163868. Entropy: 1.790504.\n",
      "Iteration 1155: Policy loss: -11.176661. Value loss: 11.317191. Entropy: 1.790526.\n",
      "episode: 567   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 817     evaluation reward: 137.15\n",
      "episode: 568   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 459     evaluation reward: 136.7\n",
      "Training network\n",
      "Iteration 1156: Policy loss: -3.795707. Value loss: 4.101070. Entropy: 1.790542.\n",
      "Iteration 1157: Policy loss: -3.802916. Value loss: 4.112000. Entropy: 1.790533.\n",
      "Iteration 1158: Policy loss: -3.786180. Value loss: 4.093146. Entropy: 1.790580.\n",
      "episode: 569   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 430     evaluation reward: 136.3\n",
      "episode: 570   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 480     evaluation reward: 137.2\n",
      "Training network\n",
      "Iteration 1159: Policy loss: -12.052779. Value loss: 11.996726. Entropy: 1.790505.\n",
      "Iteration 1160: Policy loss: -12.067060. Value loss: 12.005689. Entropy: 1.790542.\n",
      "Iteration 1161: Policy loss: -12.163720. Value loss: 12.100001. Entropy: 1.790543.\n",
      "episode: 571   score: 130.0   memory length: 1024   epsilon: 1.0    steps: 658     evaluation reward: 136.7\n",
      "episode: 572   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 419     evaluation reward: 136.75\n",
      "Training network\n",
      "Iteration 1162: Policy loss: -6.210980. Value loss: 6.155023. Entropy: 1.790546.\n",
      "Iteration 1163: Policy loss: -6.146991. Value loss: 6.082513. Entropy: 1.790569.\n",
      "Iteration 1164: Policy loss: -6.157779. Value loss: 6.103970. Entropy: 1.790567.\n",
      "episode: 573   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 778     evaluation reward: 137.4\n",
      "Training network\n",
      "Iteration 1165: Policy loss: -25.934116. Value loss: 25.879854. Entropy: 1.790346.\n",
      "Iteration 1166: Policy loss: -26.264393. Value loss: 26.198330. Entropy: 1.790312.\n",
      "Iteration 1167: Policy loss: -25.633261. Value loss: 25.568806. Entropy: 1.790399.\n",
      "episode: 574   score: 360.0   memory length: 1024   epsilon: 1.0    steps: 775     evaluation reward: 138.85\n",
      "episode: 575   score: 85.0   memory length: 1024   epsilon: 1.0    steps: 607     evaluation reward: 138.65\n",
      "Training network\n",
      "Iteration 1168: Policy loss: -4.592854. Value loss: 4.684610. Entropy: 1.790227.\n",
      "Iteration 1169: Policy loss: -4.457469. Value loss: 4.556407. Entropy: 1.790164.\n",
      "Iteration 1170: Policy loss: -4.491270. Value loss: 4.591008. Entropy: 1.790213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 576   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 543     evaluation reward: 139.1\n",
      "now time :  2018-12-19 13:29:31.058383\n",
      "Training network\n",
      "Iteration 1171: Policy loss: -4.866477. Value loss: 5.069601. Entropy: 1.790415.\n",
      "Iteration 1172: Policy loss: -4.739642. Value loss: 4.953796. Entropy: 1.790439.\n",
      "Iteration 1173: Policy loss: -4.931689. Value loss: 5.133544. Entropy: 1.790458.\n",
      "episode: 577   score: 45.0   memory length: 1024   epsilon: 1.0    steps: 616     evaluation reward: 137.6\n",
      "episode: 578   score: 185.0   memory length: 1024   epsilon: 1.0    steps: 764     evaluation reward: 138.55\n",
      "Training network\n",
      "Iteration 1174: Policy loss: -11.210770. Value loss: 11.064082. Entropy: 1.790772.\n",
      "Iteration 1175: Policy loss: -11.229569. Value loss: 11.084403. Entropy: 1.790781.\n",
      "Iteration 1176: Policy loss: -11.248122. Value loss: 11.101554. Entropy: 1.790757.\n",
      "episode: 579   score: 90.0   memory length: 1024   epsilon: 1.0    steps: 696     evaluation reward: 138.8\n",
      "Training network\n",
      "Iteration 1177: Policy loss: -8.282515. Value loss: 8.446815. Entropy: 1.790401.\n",
      "Iteration 1178: Policy loss: -8.331099. Value loss: 8.488695. Entropy: 1.790394.\n",
      "Iteration 1179: Policy loss: -8.261753. Value loss: 8.418243. Entropy: 1.790409.\n",
      "episode: 580   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 624     evaluation reward: 137.9\n",
      "episode: 581   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 751     evaluation reward: 139.35\n",
      "Training network\n",
      "Iteration 1180: Policy loss: -13.582591. Value loss: 13.517573. Entropy: 1.790548.\n",
      "Iteration 1181: Policy loss: -13.609719. Value loss: 13.546447. Entropy: 1.790524.\n",
      "Iteration 1182: Policy loss: -13.672272. Value loss: 13.600593. Entropy: 1.790558.\n",
      "episode: 582   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 404     evaluation reward: 137.9\n",
      "Training network\n",
      "Iteration 1183: Policy loss: -10.742008. Value loss: 10.865712. Entropy: 1.790629.\n",
      "Iteration 1184: Policy loss: -10.689485. Value loss: 10.820295. Entropy: 1.790632.\n",
      "Iteration 1185: Policy loss: -10.613720. Value loss: 10.748983. Entropy: 1.790628.\n",
      "episode: 583   score: 225.0   memory length: 1024   epsilon: 1.0    steps: 1188     evaluation reward: 139.3\n",
      "episode: 584   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 373     evaluation reward: 138.55\n",
      "Training network\n",
      "Iteration 1186: Policy loss: -6.965131. Value loss: 7.057880. Entropy: 1.790568.\n",
      "Iteration 1187: Policy loss: -6.991404. Value loss: 7.084395. Entropy: 1.790586.\n",
      "Iteration 1188: Policy loss: -6.992190. Value loss: 7.085038. Entropy: 1.790577.\n",
      "episode: 585   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 597     evaluation reward: 138.85\n",
      "episode: 586   score: 35.0   memory length: 1024   epsilon: 1.0    steps: 623     evaluation reward: 137.8\n",
      "Training network\n",
      "Iteration 1189: Policy loss: -4.273189. Value loss: 4.511968. Entropy: 1.790488.\n",
      "Iteration 1190: Policy loss: -4.226983. Value loss: 4.474566. Entropy: 1.790492.\n",
      "Iteration 1191: Policy loss: -4.153141. Value loss: 4.402752. Entropy: 1.790472.\n",
      "episode: 587   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 687     evaluation reward: 138.7\n",
      "Training network\n",
      "Iteration 1192: Policy loss: -8.731995. Value loss: 8.917868. Entropy: 1.790550.\n",
      "Iteration 1193: Policy loss: -8.820726. Value loss: 9.004629. Entropy: 1.790541.\n",
      "Iteration 1194: Policy loss: -8.784621. Value loss: 8.970626. Entropy: 1.790576.\n",
      "episode: 588   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 548     evaluation reward: 139.15\n",
      "episode: 589   score: 150.0   memory length: 1024   epsilon: 1.0    steps: 787     evaluation reward: 139.3\n",
      "Training network\n",
      "Iteration 1195: Policy loss: -8.116853. Value loss: 8.356386. Entropy: 1.790542.\n",
      "Iteration 1196: Policy loss: -8.139084. Value loss: 8.378288. Entropy: 1.790534.\n",
      "Iteration 1197: Policy loss: -8.122555. Value loss: 8.352456. Entropy: 1.790546.\n",
      "episode: 590   score: 20.0   memory length: 1024   epsilon: 1.0    steps: 346     evaluation reward: 138.35\n",
      "Training network\n",
      "Iteration 1198: Policy loss: -9.903877. Value loss: 9.891186. Entropy: 1.790410.\n",
      "Iteration 1199: Policy loss: -10.019283. Value loss: 10.005784. Entropy: 1.790392.\n",
      "Iteration 1200: Policy loss: -9.998916. Value loss: 9.981369. Entropy: 1.790383.\n",
      "episode: 591   score: 170.0   memory length: 1024   epsilon: 1.0    steps: 875     evaluation reward: 139.0\n",
      "episode: 592   score: 485.0   memory length: 1024   epsilon: 1.0    steps: 913     evaluation reward: 142.8\n",
      "Training network\n",
      "Iteration 1201: Policy loss: -29.554996. Value loss: 29.632111. Entropy: 1.790544.\n",
      "Iteration 1202: Policy loss: -29.518221. Value loss: 29.596331. Entropy: 1.790534.\n",
      "Iteration 1203: Policy loss: -29.236423. Value loss: 29.311016. Entropy: 1.790522.\n",
      "Training network\n",
      "Iteration 1204: Policy loss: -27.263346. Value loss: 27.121887. Entropy: 1.790498.\n",
      "Iteration 1205: Policy loss: -27.224909. Value loss: 27.077457. Entropy: 1.790499.\n",
      "Iteration 1206: Policy loss: -27.103428. Value loss: 26.957098. Entropy: 1.790428.\n",
      "episode: 593   score: 485.0   memory length: 1024   epsilon: 1.0    steps: 1184     evaluation reward: 145.85\n",
      "episode: 594   score: 165.0   memory length: 1024   epsilon: 1.0    steps: 645     evaluation reward: 146.75\n",
      "Training network\n",
      "Iteration 1207: Policy loss: -10.062773. Value loss: 10.208366. Entropy: 1.790539.\n",
      "Iteration 1208: Policy loss: -9.895238. Value loss: 10.043748. Entropy: 1.790516.\n",
      "Iteration 1209: Policy loss: -9.979025. Value loss: 10.119058. Entropy: 1.790514.\n",
      "episode: 595   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 840     evaluation reward: 147.35\n",
      "Training network\n",
      "Iteration 1210: Policy loss: -9.588690. Value loss: 9.661889. Entropy: 1.790413.\n",
      "Iteration 1211: Policy loss: -9.790816. Value loss: 9.854725. Entropy: 1.790420.\n",
      "Iteration 1212: Policy loss: -9.729259. Value loss: 9.788144. Entropy: 1.790392.\n",
      "episode: 596   score: 250.0   memory length: 1024   epsilon: 1.0    steps: 986     evaluation reward: 148.65\n",
      "Training network\n",
      "Iteration 1213: Policy loss: -17.510403. Value loss: 17.322456. Entropy: 1.790455.\n",
      "Iteration 1214: Policy loss: -17.448393. Value loss: 17.268471. Entropy: 1.790470.\n",
      "Iteration 1215: Policy loss: -17.459110. Value loss: 17.273745. Entropy: 1.790465.\n",
      "episode: 597   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 626     evaluation reward: 149.15\n",
      "episode: 598   score: 160.0   memory length: 1024   epsilon: 1.0    steps: 648     evaluation reward: 149.5\n",
      "Training network\n",
      "Iteration 1216: Policy loss: -8.620576. Value loss: 8.853697. Entropy: 1.790728.\n",
      "Iteration 1217: Policy loss: -8.678801. Value loss: 8.908875. Entropy: 1.790735.\n",
      "Iteration 1218: Policy loss: -8.682212. Value loss: 8.912086. Entropy: 1.790734.\n",
      "episode: 599   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 537     evaluation reward: 146.4\n",
      "episode: 600   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 692     evaluation reward: 148.0\n",
      "Training network\n",
      "Iteration 1219: Policy loss: -13.857563. Value loss: 13.683045. Entropy: 1.790599.\n",
      "Iteration 1220: Policy loss: -13.971823. Value loss: 13.795736. Entropy: 1.790601.\n",
      "Iteration 1221: Policy loss: -14.182872. Value loss: 14.000552. Entropy: 1.790602.\n",
      "episode: 601   score: 35.0   memory length: 1024   epsilon: 1.0    steps: 448     evaluation reward: 146.25\n",
      "episode: 602   score: 90.0   memory length: 1024   epsilon: 1.0    steps: 530     evaluation reward: 145.2\n",
      "Training network\n",
      "Iteration 1222: Policy loss: -5.530733. Value loss: 5.923628. Entropy: 1.790729.\n",
      "Iteration 1223: Policy loss: -5.503559. Value loss: 5.900700. Entropy: 1.790722.\n",
      "Iteration 1224: Policy loss: -5.520274. Value loss: 5.913759. Entropy: 1.790715.\n",
      "episode: 603   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 488     evaluation reward: 144.25\n",
      "Training network\n",
      "Iteration 1225: Policy loss: -6.329383. Value loss: 6.394969. Entropy: 1.790511.\n",
      "Iteration 1226: Policy loss: -6.361918. Value loss: 6.428565. Entropy: 1.790494.\n",
      "Iteration 1227: Policy loss: -6.298859. Value loss: 6.371772. Entropy: 1.790499.\n",
      "episode: 604   score: 200.0   memory length: 1024   epsilon: 1.0    steps: 860     evaluation reward: 145.15\n",
      "episode: 605   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 729     evaluation reward: 145.75\n",
      "Training network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1228: Policy loss: -9.701288. Value loss: 9.802184. Entropy: 1.790663.\n",
      "Iteration 1229: Policy loss: -9.683365. Value loss: 9.780836. Entropy: 1.790641.\n",
      "Iteration 1230: Policy loss: -9.642731. Value loss: 9.739333. Entropy: 1.790642.\n",
      "Training network\n",
      "Iteration 1231: Policy loss: -16.801811. Value loss: 16.514418. Entropy: 1.790289.\n",
      "Iteration 1232: Policy loss: -16.876871. Value loss: 16.581997. Entropy: 1.790318.\n",
      "Iteration 1233: Policy loss: -16.872894. Value loss: 16.581369. Entropy: 1.790296.\n",
      "episode: 606   score: 320.0   memory length: 1024   epsilon: 1.0    steps: 1133     evaluation reward: 148.15\n",
      "episode: 607   score: 145.0   memory length: 1024   epsilon: 1.0    steps: 857     evaluation reward: 148.6\n",
      "Training network\n",
      "Iteration 1234: Policy loss: -8.407542. Value loss: 8.217421. Entropy: 1.790520.\n",
      "Iteration 1235: Policy loss: -8.385584. Value loss: 8.194049. Entropy: 1.790526.\n",
      "Iteration 1236: Policy loss: -8.274394. Value loss: 8.085949. Entropy: 1.790511.\n",
      "episode: 608   score: 90.0   memory length: 1024   epsilon: 1.0    steps: 509     evaluation reward: 148.45\n",
      "episode: 609   score: 30.0   memory length: 1024   epsilon: 1.0    steps: 434     evaluation reward: 148.1\n",
      "Training network\n",
      "Iteration 1237: Policy loss: -8.131483. Value loss: 8.191953. Entropy: 1.790781.\n",
      "Iteration 1238: Policy loss: -8.207148. Value loss: 8.258774. Entropy: 1.790776.\n",
      "Iteration 1239: Policy loss: -8.215524. Value loss: 8.267782. Entropy: 1.790776.\n",
      "episode: 610   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 578     evaluation reward: 149.0\n",
      "episode: 611   score: 90.0   memory length: 1024   epsilon: 1.0    steps: 519     evaluation reward: 148.8\n",
      "Training network\n",
      "Iteration 1240: Policy loss: -9.152637. Value loss: 9.329147. Entropy: 1.790629.\n",
      "Iteration 1241: Policy loss: -9.146935. Value loss: 9.326687. Entropy: 1.790609.\n",
      "Iteration 1242: Policy loss: -9.005306. Value loss: 9.192064. Entropy: 1.790620.\n",
      "episode: 612   score: 230.0   memory length: 1024   epsilon: 1.0    steps: 964     evaluation reward: 147.65\n",
      "Training network\n",
      "Iteration 1243: Policy loss: -11.177630. Value loss: 10.881435. Entropy: 1.790341.\n",
      "Iteration 1244: Policy loss: -11.247643. Value loss: 10.952559. Entropy: 1.790347.\n",
      "Iteration 1245: Policy loss: -11.147296. Value loss: 10.846650. Entropy: 1.790374.\n",
      "episode: 613   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 779     evaluation reward: 148.95\n",
      "Training network\n",
      "Iteration 1246: Policy loss: -9.845319. Value loss: 9.819440. Entropy: 1.790593.\n",
      "Iteration 1247: Policy loss: -9.800446. Value loss: 9.774003. Entropy: 1.790602.\n",
      "Iteration 1248: Policy loss: -9.863666. Value loss: 9.838355. Entropy: 1.790580.\n",
      "episode: 614   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 520     evaluation reward: 148.35\n",
      "episode: 615   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 416     evaluation reward: 145.85\n",
      "episode: 616   score: 85.0   memory length: 1024   epsilon: 1.0    steps: 370     evaluation reward: 144.65\n",
      "Training network\n",
      "Iteration 1249: Policy loss: -5.054934. Value loss: 5.409857. Entropy: 1.790648.\n",
      "Iteration 1250: Policy loss: -5.082318. Value loss: 5.434099. Entropy: 1.790671.\n",
      "Iteration 1251: Policy loss: -5.048527. Value loss: 5.407278. Entropy: 1.790664.\n",
      "episode: 617   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 740     evaluation reward: 144.9\n",
      "Training network\n",
      "Iteration 1252: Policy loss: -8.054778. Value loss: 8.153915. Entropy: 1.790715.\n",
      "Iteration 1253: Policy loss: -7.971673. Value loss: 8.067174. Entropy: 1.790740.\n",
      "Iteration 1254: Policy loss: -8.071964. Value loss: 8.162501. Entropy: 1.790712.\n",
      "episode: 618   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 774     evaluation reward: 145.4\n",
      "episode: 619   score: 40.0   memory length: 1024   epsilon: 1.0    steps: 648     evaluation reward: 145.2\n",
      "Training network\n",
      "Iteration 1255: Policy loss: -5.414944. Value loss: 5.605296. Entropy: 1.790444.\n",
      "Iteration 1256: Policy loss: -5.520541. Value loss: 5.703068. Entropy: 1.790443.\n",
      "Iteration 1257: Policy loss: -5.514848. Value loss: 5.702690. Entropy: 1.790432.\n",
      "episode: 620   score: 75.0   memory length: 1024   epsilon: 1.0    steps: 827     evaluation reward: 144.7\n",
      "Training network\n",
      "Iteration 1258: Policy loss: -3.617739. Value loss: 3.525970. Entropy: 1.790489.\n",
      "Iteration 1259: Policy loss: -3.637526. Value loss: 3.555119. Entropy: 1.790500.\n",
      "Iteration 1260: Policy loss: -3.654894. Value loss: 3.564198. Entropy: 1.790512.\n",
      "episode: 621   score: 275.0   memory length: 1024   epsilon: 1.0    steps: 891     evaluation reward: 146.2\n",
      "Training network\n",
      "Iteration 1261: Policy loss: -19.713013. Value loss: 19.606869. Entropy: 1.790257.\n",
      "Iteration 1262: Policy loss: -19.620762. Value loss: 19.517536. Entropy: 1.790254.\n",
      "Iteration 1263: Policy loss: -19.672590. Value loss: 19.562037. Entropy: 1.790253.\n",
      "episode: 622   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 398     evaluation reward: 144.9\n",
      "episode: 623   score: 385.0   memory length: 1024   epsilon: 1.0    steps: 837     evaluation reward: 145.7\n",
      "Training network\n",
      "Iteration 1264: Policy loss: -22.656612. Value loss: 22.741470. Entropy: 1.790597.\n",
      "Iteration 1265: Policy loss: -22.656126. Value loss: 22.727381. Entropy: 1.790579.\n",
      "Iteration 1266: Policy loss: -22.703880. Value loss: 22.776817. Entropy: 1.790580.\n",
      "episode: 624   score: 70.0   memory length: 1024   epsilon: 1.0    steps: 613     evaluation reward: 146.35\n",
      "Training network\n",
      "Iteration 1267: Policy loss: -7.243771. Value loss: 7.325209. Entropy: 1.790501.\n",
      "Iteration 1268: Policy loss: -7.252184. Value loss: 7.329954. Entropy: 1.790469.\n",
      "Iteration 1269: Policy loss: -7.188003. Value loss: 7.274536. Entropy: 1.790487.\n",
      "episode: 625   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 742     evaluation reward: 146.35\n",
      "episode: 626   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 568     evaluation reward: 145.05\n",
      "Training network\n",
      "Iteration 1270: Policy loss: -7.935545. Value loss: 8.122119. Entropy: 1.790511.\n",
      "Iteration 1271: Policy loss: -7.818759. Value loss: 8.006875. Entropy: 1.790525.\n",
      "Iteration 1272: Policy loss: -7.804212. Value loss: 7.993769. Entropy: 1.790536.\n",
      "episode: 627   score: 240.0   memory length: 1024   epsilon: 1.0    steps: 959     evaluation reward: 145.35\n",
      "Training network\n",
      "Iteration 1273: Policy loss: -9.722280. Value loss: 9.422742. Entropy: 1.790480.\n",
      "Iteration 1274: Policy loss: -9.805789. Value loss: 9.500613. Entropy: 1.790411.\n",
      "Iteration 1275: Policy loss: -9.741696. Value loss: 9.436617. Entropy: 1.790399.\n",
      "episode: 628   score: 170.0   memory length: 1024   epsilon: 1.0    steps: 911     evaluation reward: 144.95\n",
      "Training network\n",
      "Iteration 1276: Policy loss: -15.352948. Value loss: 15.205477. Entropy: 1.790449.\n",
      "Iteration 1277: Policy loss: -15.490479. Value loss: 15.334251. Entropy: 1.790449.\n",
      "Iteration 1278: Policy loss: -15.395966. Value loss: 15.244523. Entropy: 1.790457.\n",
      "episode: 629   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 964     evaluation reward: 145.8\n",
      "Training network\n",
      "Iteration 1279: Policy loss: -9.379723. Value loss: 9.558092. Entropy: 1.790659.\n",
      "Iteration 1280: Policy loss: -9.293307. Value loss: 9.477625. Entropy: 1.790635.\n",
      "Iteration 1281: Policy loss: -9.351869. Value loss: 9.531374. Entropy: 1.790623.\n",
      "episode: 630   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 743     evaluation reward: 147.6\n",
      "episode: 631   score: 130.0   memory length: 1024   epsilon: 1.0    steps: 743     evaluation reward: 146.8\n",
      "Training network\n",
      "Iteration 1282: Policy loss: -8.324836. Value loss: 8.495111. Entropy: 1.790555.\n",
      "Iteration 1283: Policy loss: -8.337454. Value loss: 8.502110. Entropy: 1.790560.\n",
      "Iteration 1284: Policy loss: -8.310991. Value loss: 8.482497. Entropy: 1.790583.\n",
      "episode: 632   score: 30.0   memory length: 1024   epsilon: 1.0    steps: 492     evaluation reward: 145.45\n",
      "episode: 633   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 528     evaluation reward: 145.75\n",
      "Training network\n",
      "Iteration 1285: Policy loss: -6.591013. Value loss: 6.789236. Entropy: 1.790496.\n",
      "Iteration 1286: Policy loss: -6.585869. Value loss: 6.779094. Entropy: 1.790460.\n",
      "Iteration 1287: Policy loss: -6.616259. Value loss: 6.810154. Entropy: 1.790491.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 634   score: 60.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 142.2\n",
      "Training network\n",
      "Iteration 1288: Policy loss: -7.855975. Value loss: 7.837366. Entropy: 1.790529.\n",
      "Iteration 1289: Policy loss: -7.757934. Value loss: 7.740611. Entropy: 1.790514.\n",
      "Iteration 1290: Policy loss: -7.957574. Value loss: 7.929995. Entropy: 1.790510.\n",
      "episode: 635   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 619     evaluation reward: 143.05\n",
      "episode: 636   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 512     evaluation reward: 142.9\n",
      "episode: 637   score: 70.0   memory length: 1024   epsilon: 1.0    steps: 415     evaluation reward: 141.5\n",
      "Training network\n",
      "Iteration 1291: Policy loss: -6.905950. Value loss: 7.227736. Entropy: 1.790635.\n",
      "Iteration 1292: Policy loss: -6.994492. Value loss: 7.296245. Entropy: 1.790651.\n",
      "Iteration 1293: Policy loss: -6.980555. Value loss: 7.290988. Entropy: 1.790655.\n",
      "episode: 638   score: 75.0   memory length: 1024   epsilon: 1.0    steps: 629     evaluation reward: 141.65\n",
      "Training network\n",
      "Iteration 1294: Policy loss: -7.637625. Value loss: 7.648618. Entropy: 1.790501.\n",
      "Iteration 1295: Policy loss: -7.576552. Value loss: 7.588542. Entropy: 1.790519.\n",
      "Iteration 1296: Policy loss: -7.563732. Value loss: 7.581394. Entropy: 1.790523.\n",
      "episode: 639   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 715     evaluation reward: 142.1\n",
      "episode: 640   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 609     evaluation reward: 141.55\n",
      "Training network\n",
      "Iteration 1297: Policy loss: -6.148588. Value loss: 6.240558. Entropy: 1.790588.\n",
      "Iteration 1298: Policy loss: -6.080686. Value loss: 6.170652. Entropy: 1.790612.\n",
      "Iteration 1299: Policy loss: -6.154974. Value loss: 6.239105. Entropy: 1.790596.\n",
      "episode: 641   score: 225.0   memory length: 1024   epsilon: 1.0    steps: 670     evaluation reward: 142.75\n",
      "Training network\n",
      "Iteration 1300: Policy loss: -11.309058. Value loss: 11.489470. Entropy: 1.790686.\n",
      "Iteration 1301: Policy loss: -11.122156. Value loss: 11.316362. Entropy: 1.790690.\n",
      "Iteration 1302: Policy loss: -11.260015. Value loss: 11.445155. Entropy: 1.790717.\n",
      "episode: 642   score: 35.0   memory length: 1024   epsilon: 1.0    steps: 509     evaluation reward: 141.9\n",
      "episode: 643   score: 35.0   memory length: 1024   epsilon: 1.0    steps: 384     evaluation reward: 141.1\n",
      "Training network\n",
      "Iteration 1303: Policy loss: -6.904613. Value loss: 6.847221. Entropy: 1.790637.\n",
      "Iteration 1304: Policy loss: -6.897659. Value loss: 6.840047. Entropy: 1.790630.\n",
      "Iteration 1305: Policy loss: -6.877916. Value loss: 6.818151. Entropy: 1.790654.\n",
      "episode: 644   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 654     evaluation reward: 140.85\n",
      "episode: 645   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 793     evaluation reward: 141.45\n",
      "Training network\n",
      "Iteration 1306: Policy loss: -11.260692. Value loss: 11.029497. Entropy: 1.790651.\n",
      "Iteration 1307: Policy loss: -11.288872. Value loss: 11.060376. Entropy: 1.790639.\n",
      "Iteration 1308: Policy loss: -11.213820. Value loss: 10.976782. Entropy: 1.790643.\n",
      "episode: 646   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 773     evaluation reward: 141.1\n",
      "Training network\n",
      "Iteration 1309: Policy loss: -13.517276. Value loss: 13.555596. Entropy: 1.790468.\n",
      "Iteration 1310: Policy loss: -13.463341. Value loss: 13.503925. Entropy: 1.790402.\n",
      "Iteration 1311: Policy loss: -13.617332. Value loss: 13.647095. Entropy: 1.790425.\n",
      "episode: 647   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 501     evaluation reward: 140.8\n",
      "episode: 648   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 754     evaluation reward: 141.1\n",
      "Training network\n",
      "Iteration 1312: Policy loss: -5.941835. Value loss: 5.924325. Entropy: 1.790531.\n",
      "Iteration 1313: Policy loss: -5.935661. Value loss: 5.919902. Entropy: 1.790519.\n",
      "Iteration 1314: Policy loss: -5.940073. Value loss: 5.921101. Entropy: 1.790467.\n",
      "episode: 649   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 557     evaluation reward: 140.3\n",
      "episode: 650   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 632     evaluation reward: 138.85\n",
      "Training network\n",
      "Iteration 1315: Policy loss: -9.676749. Value loss: 9.888243. Entropy: 1.790689.\n",
      "Iteration 1316: Policy loss: -9.559577. Value loss: 9.777696. Entropy: 1.790679.\n",
      "Iteration 1317: Policy loss: -9.577446. Value loss: 9.794700. Entropy: 1.790711.\n",
      "now time :  2018-12-19 13:32:29.920019\n",
      "episode: 651   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 625     evaluation reward: 137.25\n",
      "Training network\n",
      "Iteration 1318: Policy loss: -7.900214. Value loss: 8.081092. Entropy: 1.790440.\n",
      "Iteration 1319: Policy loss: -7.973577. Value loss: 8.147844. Entropy: 1.790441.\n",
      "Iteration 1320: Policy loss: -7.958076. Value loss: 8.138078. Entropy: 1.790466.\n",
      "episode: 652   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 688     evaluation reward: 138.65\n",
      "episode: 653   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 541     evaluation reward: 138.4\n",
      "Training network\n",
      "Iteration 1321: Policy loss: -4.903399. Value loss: 5.242956. Entropy: 1.790726.\n",
      "Iteration 1322: Policy loss: -4.883623. Value loss: 5.221085. Entropy: 1.790770.\n",
      "Iteration 1323: Policy loss: -4.881584. Value loss: 5.220701. Entropy: 1.790744.\n",
      "episode: 654   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 436     evaluation reward: 137.15\n",
      "episode: 655   score: 165.0   memory length: 1024   epsilon: 1.0    steps: 643     evaluation reward: 136.1\n",
      "Training network\n",
      "Iteration 1324: Policy loss: -9.530783. Value loss: 9.586069. Entropy: 1.790595.\n",
      "Iteration 1325: Policy loss: -9.568440. Value loss: 9.625295. Entropy: 1.790562.\n",
      "Iteration 1326: Policy loss: -9.565973. Value loss: 9.626162. Entropy: 1.790586.\n",
      "episode: 656   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 749     evaluation reward: 137.6\n",
      "Training network\n",
      "Iteration 1327: Policy loss: -11.228149. Value loss: 11.257550. Entropy: 1.790553.\n",
      "Iteration 1328: Policy loss: -11.141742. Value loss: 11.172050. Entropy: 1.790571.\n",
      "Iteration 1329: Policy loss: -10.997655. Value loss: 11.030327. Entropy: 1.790573.\n",
      "episode: 657   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 643     evaluation reward: 137.8\n",
      "Training network\n",
      "Iteration 1330: Policy loss: -9.518007. Value loss: 9.312941. Entropy: 1.790471.\n",
      "Iteration 1331: Policy loss: -9.532825. Value loss: 9.337151. Entropy: 1.790463.\n",
      "Iteration 1332: Policy loss: -9.532211. Value loss: 9.332295. Entropy: 1.790448.\n",
      "episode: 658   score: 205.0   memory length: 1024   epsilon: 1.0    steps: 1316     evaluation reward: 138.85\n",
      "episode: 659   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 459     evaluation reward: 135.5\n",
      "Training network\n",
      "Iteration 1333: Policy loss: -7.349844. Value loss: 7.611370. Entropy: 1.790210.\n",
      "Iteration 1334: Policy loss: -7.428077. Value loss: 7.687637. Entropy: 1.790197.\n",
      "Iteration 1335: Policy loss: -7.356276. Value loss: 7.612794. Entropy: 1.790152.\n",
      "episode: 660   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 547     evaluation reward: 133.5\n",
      "Training network\n",
      "Iteration 1336: Policy loss: -11.246977. Value loss: 11.412424. Entropy: 1.790607.\n",
      "Iteration 1337: Policy loss: -10.973651. Value loss: 11.150363. Entropy: 1.790579.\n",
      "Iteration 1338: Policy loss: -11.030843. Value loss: 11.206344. Entropy: 1.790573.\n",
      "episode: 661   score: 410.0   memory length: 1024   epsilon: 1.0    steps: 855     evaluation reward: 136.0\n",
      "Training network\n",
      "Iteration 1339: Policy loss: -30.153008. Value loss: 29.930389. Entropy: 1.790605.\n",
      "Iteration 1340: Policy loss: -30.404121. Value loss: 30.171171. Entropy: 1.790604.\n",
      "Iteration 1341: Policy loss: -30.312010. Value loss: 30.062475. Entropy: 1.790646.\n",
      "episode: 662   score: 240.0   memory length: 1024   epsilon: 1.0    steps: 771     evaluation reward: 137.6\n",
      "episode: 663   score: 140.0   memory length: 1024   epsilon: 1.0    steps: 839     evaluation reward: 137.95\n",
      "Training network\n",
      "Iteration 1342: Policy loss: -8.325754. Value loss: 8.202561. Entropy: 1.790610.\n",
      "Iteration 1343: Policy loss: -8.255582. Value loss: 8.134216. Entropy: 1.790620.\n",
      "Iteration 1344: Policy loss: -8.230184. Value loss: 8.111689. Entropy: 1.790652.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 664   score: 165.0   memory length: 1024   epsilon: 1.0    steps: 1094     evaluation reward: 138.05\n",
      "Training network\n",
      "Iteration 1345: Policy loss: -5.520286. Value loss: 5.967849. Entropy: 1.790428.\n",
      "Iteration 1346: Policy loss: -5.535381. Value loss: 5.985423. Entropy: 1.790440.\n",
      "Iteration 1347: Policy loss: -5.522268. Value loss: 5.971350. Entropy: 1.790423.\n",
      "episode: 665   score: 335.0   memory length: 1024   epsilon: 1.0    steps: 806     evaluation reward: 140.5\n",
      "Training network\n",
      "Iteration 1348: Policy loss: -18.719652. Value loss: 18.490595. Entropy: 1.790364.\n",
      "Iteration 1349: Policy loss: -18.810081. Value loss: 18.569818. Entropy: 1.790370.\n",
      "Iteration 1350: Policy loss: -18.539978. Value loss: 18.301695. Entropy: 1.790400.\n",
      "episode: 666   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 671     evaluation reward: 140.65\n",
      "episode: 667   score: 75.0   memory length: 1024   epsilon: 1.0    steps: 533     evaluation reward: 140.2\n",
      "Training network\n",
      "Iteration 1351: Policy loss: -6.562121. Value loss: 6.814466. Entropy: 1.790666.\n",
      "Iteration 1352: Policy loss: -6.643538. Value loss: 6.894863. Entropy: 1.790654.\n",
      "Iteration 1353: Policy loss: -6.669698. Value loss: 6.915906. Entropy: 1.790673.\n",
      "Training network\n",
      "Iteration 1354: Policy loss: -8.983753. Value loss: 9.079741. Entropy: 1.790457.\n",
      "Iteration 1355: Policy loss: -9.002968. Value loss: 9.103337. Entropy: 1.790466.\n",
      "Iteration 1356: Policy loss: -8.915180. Value loss: 9.014462. Entropy: 1.790405.\n",
      "episode: 668   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 1356     evaluation reward: 141.5\n",
      "episode: 669   score: 35.0   memory length: 1024   epsilon: 1.0    steps: 503     evaluation reward: 140.65\n",
      "Training network\n",
      "Iteration 1357: Policy loss: -2.692876. Value loss: 2.989868. Entropy: 1.790502.\n",
      "Iteration 1358: Policy loss: -2.691066. Value loss: 2.986409. Entropy: 1.790495.\n",
      "Iteration 1359: Policy loss: -2.680541. Value loss: 2.980427. Entropy: 1.790502.\n",
      "episode: 670   score: 30.0   memory length: 1024   epsilon: 1.0    steps: 593     evaluation reward: 139.4\n",
      "Training network\n",
      "Iteration 1360: Policy loss: -24.236839. Value loss: 24.406790. Entropy: 1.790407.\n",
      "Iteration 1361: Policy loss: -23.884796. Value loss: 24.063868. Entropy: 1.790450.\n",
      "Iteration 1362: Policy loss: -23.977053. Value loss: 24.148272. Entropy: 1.790404.\n",
      "episode: 671   score: 415.0   memory length: 1024   epsilon: 1.0    steps: 857     evaluation reward: 142.25\n",
      "Training network\n",
      "Iteration 1363: Policy loss: -19.585520. Value loss: 19.402971. Entropy: 1.790107.\n",
      "Iteration 1364: Policy loss: -19.645023. Value loss: 19.458986. Entropy: 1.790085.\n",
      "Iteration 1365: Policy loss: -19.643082. Value loss: 19.455458. Entropy: 1.790148.\n",
      "episode: 672   score: 325.0   memory length: 1024   epsilon: 1.0    steps: 976     evaluation reward: 144.95\n",
      "episode: 673   score: 165.0   memory length: 1024   epsilon: 1.0    steps: 815     evaluation reward: 145.25\n",
      "Training network\n",
      "Iteration 1366: Policy loss: -9.821670. Value loss: 9.800068. Entropy: 1.790475.\n",
      "Iteration 1367: Policy loss: -9.904181. Value loss: 9.873470. Entropy: 1.790464.\n",
      "Iteration 1368: Policy loss: -9.885071. Value loss: 9.850343. Entropy: 1.790488.\n",
      "episode: 674   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 362     evaluation reward: 142.15\n",
      "episode: 675   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 774     evaluation reward: 142.65\n",
      "Training network\n",
      "Iteration 1369: Policy loss: -7.711899. Value loss: 7.989147. Entropy: 1.790516.\n",
      "Iteration 1370: Policy loss: -7.635396. Value loss: 7.920375. Entropy: 1.790527.\n",
      "Iteration 1371: Policy loss: -7.550293. Value loss: 7.838470. Entropy: 1.790521.\n",
      "episode: 676   score: 380.0   memory length: 1024   epsilon: 1.0    steps: 648     evaluation reward: 145.65\n",
      "Training network\n",
      "Iteration 1372: Policy loss: -25.268185. Value loss: 25.322308. Entropy: 1.790582.\n",
      "Iteration 1373: Policy loss: -25.798214. Value loss: 25.835928. Entropy: 1.790566.\n",
      "Iteration 1374: Policy loss: -25.756315. Value loss: 25.798151. Entropy: 1.790562.\n",
      "episode: 677   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 822     evaluation reward: 147.3\n",
      "episode: 678   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 617     evaluation reward: 146.55\n",
      "Training network\n",
      "Iteration 1375: Policy loss: -13.576807. Value loss: 13.457961. Entropy: 1.790534.\n",
      "Iteration 1376: Policy loss: -13.562443. Value loss: 13.440619. Entropy: 1.790521.\n",
      "Iteration 1377: Policy loss: -13.616752. Value loss: 13.496123. Entropy: 1.790517.\n",
      "episode: 679   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 724     evaluation reward: 146.7\n",
      "Training network\n",
      "Iteration 1378: Policy loss: -6.395392. Value loss: 6.423008. Entropy: 1.790641.\n",
      "Iteration 1379: Policy loss: -6.472342. Value loss: 6.491603. Entropy: 1.790635.\n",
      "Iteration 1380: Policy loss: -6.364324. Value loss: 6.391409. Entropy: 1.790630.\n",
      "episode: 680   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 579     evaluation reward: 146.6\n",
      "episode: 681   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 721     evaluation reward: 146.35\n",
      "Training network\n",
      "Iteration 1381: Policy loss: -12.154386. Value loss: 12.252748. Entropy: 1.790623.\n",
      "Iteration 1382: Policy loss: -12.149879. Value loss: 12.251694. Entropy: 1.790598.\n",
      "Iteration 1383: Policy loss: -12.199999. Value loss: 12.296042. Entropy: 1.790608.\n",
      "episode: 682   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 734     evaluation reward: 147.1\n",
      "Training network\n",
      "Iteration 1384: Policy loss: -12.663972. Value loss: 12.607098. Entropy: 1.790627.\n",
      "Iteration 1385: Policy loss: -12.560120. Value loss: 12.504234. Entropy: 1.790635.\n",
      "Iteration 1386: Policy loss: -12.589340. Value loss: 12.533910. Entropy: 1.790626.\n",
      "episode: 683   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 704     evaluation reward: 146.65\n",
      "episode: 684   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 580     evaluation reward: 146.5\n",
      "Training network\n",
      "Iteration 1387: Policy loss: -9.170215. Value loss: 9.404575. Entropy: 1.790709.\n",
      "Iteration 1388: Policy loss: -9.170849. Value loss: 9.396508. Entropy: 1.790690.\n",
      "Iteration 1389: Policy loss: -9.164750. Value loss: 9.394894. Entropy: 1.790692.\n",
      "Training network\n",
      "Iteration 1390: Policy loss: -20.878880. Value loss: 20.787493. Entropy: 1.790358.\n",
      "Iteration 1391: Policy loss: -20.891441. Value loss: 20.793129. Entropy: 1.790386.\n",
      "Iteration 1392: Policy loss: -20.734379. Value loss: 20.641783. Entropy: 1.790344.\n",
      "episode: 685   score: 425.0   memory length: 1024   epsilon: 1.0    steps: 1281     evaluation reward: 149.65\n",
      "episode: 686   score: 20.0   memory length: 1024   epsilon: 1.0    steps: 346     evaluation reward: 149.5\n",
      "Training network\n",
      "Iteration 1393: Policy loss: -11.659929. Value loss: 11.608088. Entropy: 1.790460.\n",
      "Iteration 1394: Policy loss: -11.606307. Value loss: 11.554037. Entropy: 1.790444.\n",
      "Iteration 1395: Policy loss: -11.734198. Value loss: 11.674237. Entropy: 1.790483.\n",
      "episode: 687   score: 225.0   memory length: 1024   epsilon: 1.0    steps: 908     evaluation reward: 150.5\n",
      "episode: 688   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 392     evaluation reward: 149.9\n",
      "Training network\n",
      "Iteration 1396: Policy loss: -5.124393. Value loss: 5.451501. Entropy: 1.790573.\n",
      "Iteration 1397: Policy loss: -5.092990. Value loss: 5.427180. Entropy: 1.790528.\n",
      "Iteration 1398: Policy loss: -5.092326. Value loss: 5.425633. Entropy: 1.790576.\n",
      "episode: 689   score: 145.0   memory length: 1024   epsilon: 1.0    steps: 669     evaluation reward: 149.85\n",
      "episode: 690   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 472     evaluation reward: 150.9\n",
      "Training network\n",
      "Iteration 1399: Policy loss: -11.296782. Value loss: 11.519017. Entropy: 1.790718.\n",
      "Iteration 1400: Policy loss: -11.231503. Value loss: 11.468757. Entropy: 1.790695.\n",
      "Iteration 1401: Policy loss: -11.239063. Value loss: 11.472130. Entropy: 1.790694.\n",
      "episode: 691   score: 165.0   memory length: 1024   epsilon: 1.0    steps: 615     evaluation reward: 150.85\n",
      "Training network\n",
      "Iteration 1402: Policy loss: -8.998245. Value loss: 9.019135. Entropy: 1.790273.\n",
      "Iteration 1403: Policy loss: -8.953835. Value loss: 8.976001. Entropy: 1.790316.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1404: Policy loss: -8.955646. Value loss: 8.973459. Entropy: 1.790337.\n",
      "episode: 692   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 622     evaluation reward: 147.05\n",
      "episode: 693   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 739     evaluation reward: 143.0\n",
      "Training network\n",
      "Iteration 1405: Policy loss: -6.645695. Value loss: 7.048760. Entropy: 1.790682.\n",
      "Iteration 1406: Policy loss: -6.548018. Value loss: 6.954194. Entropy: 1.790653.\n",
      "Iteration 1407: Policy loss: -6.453750. Value loss: 6.864163. Entropy: 1.790635.\n",
      "episode: 694   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 593     evaluation reward: 142.4\n",
      "episode: 695   score: 90.0   memory length: 1024   epsilon: 1.0    steps: 556     evaluation reward: 141.75\n",
      "Training network\n",
      "Iteration 1408: Policy loss: -8.863056. Value loss: 8.936185. Entropy: 1.790668.\n",
      "Iteration 1409: Policy loss: -8.640089. Value loss: 8.723355. Entropy: 1.790679.\n",
      "Iteration 1410: Policy loss: -8.679076. Value loss: 8.750808. Entropy: 1.790674.\n",
      "episode: 696   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 389     evaluation reward: 139.75\n",
      "episode: 697   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 501     evaluation reward: 138.85\n",
      "Training network\n",
      "Iteration 1411: Policy loss: -1.386059. Value loss: 2.062100. Entropy: 1.790642.\n",
      "Iteration 1412: Policy loss: -1.434584. Value loss: 2.107962. Entropy: 1.790641.\n",
      "Iteration 1413: Policy loss: -1.430004. Value loss: 2.100800. Entropy: 1.790634.\n",
      "episode: 698   score: 185.0   memory length: 1024   epsilon: 1.0    steps: 1030     evaluation reward: 139.1\n",
      "Training network\n",
      "Iteration 1414: Policy loss: -9.531079. Value loss: 9.757404. Entropy: 1.790340.\n",
      "Iteration 1415: Policy loss: -9.433486. Value loss: 9.668562. Entropy: 1.790356.\n",
      "Iteration 1416: Policy loss: -9.496114. Value loss: 9.724600. Entropy: 1.790345.\n",
      "episode: 699   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 834     evaluation reward: 140.4\n",
      "Training network\n",
      "Iteration 1417: Policy loss: -12.588100. Value loss: 12.545786. Entropy: 1.790572.\n",
      "Iteration 1418: Policy loss: -12.432405. Value loss: 12.389239. Entropy: 1.790542.\n",
      "Iteration 1419: Policy loss: -12.600202. Value loss: 12.553596. Entropy: 1.790564.\n",
      "episode: 700   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 648     evaluation reward: 139.4\n",
      "episode: 701   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 645     evaluation reward: 140.3\n",
      "Training network\n",
      "Iteration 1420: Policy loss: -8.956032. Value loss: 9.199012. Entropy: 1.790725.\n",
      "Iteration 1421: Policy loss: -8.959206. Value loss: 9.192680. Entropy: 1.790735.\n",
      "Iteration 1422: Policy loss: -8.894843. Value loss: 9.136622. Entropy: 1.790713.\n",
      "episode: 702   score: 290.0   memory length: 1024   epsilon: 1.0    steps: 968     evaluation reward: 142.3\n",
      "Training network\n",
      "Iteration 1423: Policy loss: -17.157307. Value loss: 16.971333. Entropy: 1.790373.\n",
      "Iteration 1424: Policy loss: -17.234118. Value loss: 17.052475. Entropy: 1.790385.\n",
      "Iteration 1425: Policy loss: -16.957756. Value loss: 16.774710. Entropy: 1.790425.\n",
      "episode: 703   score: 240.0   memory length: 1024   epsilon: 1.0    steps: 871     evaluation reward: 144.6\n",
      "Training network\n",
      "Iteration 1426: Policy loss: -10.831762. Value loss: 10.761541. Entropy: 1.790519.\n",
      "Iteration 1427: Policy loss: -10.771809. Value loss: 10.699002. Entropy: 1.790555.\n",
      "Iteration 1428: Policy loss: -10.650805. Value loss: 10.579194. Entropy: 1.790539.\n",
      "episode: 704   score: 140.0   memory length: 1024   epsilon: 1.0    steps: 626     evaluation reward: 144.0\n",
      "episode: 705   score: 75.0   memory length: 1024   epsilon: 1.0    steps: 650     evaluation reward: 143.5\n",
      "Training network\n",
      "Iteration 1429: Policy loss: -9.566314. Value loss: 9.662103. Entropy: 1.790530.\n",
      "Iteration 1430: Policy loss: -9.676989. Value loss: 9.776026. Entropy: 1.790515.\n",
      "Iteration 1431: Policy loss: -9.596189. Value loss: 9.686973. Entropy: 1.790535.\n",
      "episode: 706   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 606     evaluation reward: 141.1\n",
      "Training network\n",
      "Iteration 1432: Policy loss: -3.010696. Value loss: 3.271100. Entropy: 1.790469.\n",
      "Iteration 1433: Policy loss: -2.992058. Value loss: 3.251993. Entropy: 1.790510.\n",
      "Iteration 1434: Policy loss: -2.973303. Value loss: 3.244015. Entropy: 1.790523.\n",
      "episode: 707   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 583     evaluation reward: 139.75\n",
      "episode: 708   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 715     evaluation reward: 139.95\n",
      "Training network\n",
      "Iteration 1435: Policy loss: -7.050622. Value loss: 7.002158. Entropy: 1.790602.\n",
      "Iteration 1436: Policy loss: -7.062264. Value loss: 7.013586. Entropy: 1.790613.\n",
      "Iteration 1437: Policy loss: -7.125241. Value loss: 7.072920. Entropy: 1.790596.\n",
      "episode: 709   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 588     evaluation reward: 140.75\n",
      "Training network\n",
      "Iteration 1438: Policy loss: -10.219445. Value loss: 10.179633. Entropy: 1.790514.\n",
      "Iteration 1439: Policy loss: -10.354877. Value loss: 10.305699. Entropy: 1.790465.\n",
      "Iteration 1440: Policy loss: -10.332161. Value loss: 10.284768. Entropy: 1.790487.\n",
      "episode: 710   score: 255.0   memory length: 1024   epsilon: 1.0    steps: 986     evaluation reward: 141.5\n",
      "episode: 711   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 651     evaluation reward: 142.4\n",
      "Training network\n",
      "Iteration 1441: Policy loss: -15.543248. Value loss: 15.634919. Entropy: 1.790681.\n",
      "Iteration 1442: Policy loss: -15.582405. Value loss: 15.666088. Entropy: 1.790634.\n",
      "Iteration 1443: Policy loss: -15.516731. Value loss: 15.605538. Entropy: 1.790620.\n",
      "episode: 712   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 673     evaluation reward: 141.15\n",
      "Training network\n",
      "Iteration 1444: Policy loss: -11.176479. Value loss: 11.533989. Entropy: 1.790709.\n",
      "Iteration 1445: Policy loss: -11.215394. Value loss: 11.575624. Entropy: 1.790699.\n",
      "Iteration 1446: Policy loss: -11.235012. Value loss: 11.593887. Entropy: 1.790705.\n",
      "episode: 713   score: 365.0   memory length: 1024   epsilon: 1.0    steps: 1124     evaluation reward: 142.7\n",
      "Training network\n",
      "Iteration 1447: Policy loss: -12.668106. Value loss: 12.616153. Entropy: 1.790626.\n",
      "Iteration 1448: Policy loss: -12.650907. Value loss: 12.596224. Entropy: 1.790635.\n",
      "Iteration 1449: Policy loss: -12.705973. Value loss: 12.642819. Entropy: 1.790664.\n",
      "episode: 714   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 624     evaluation reward: 143.8\n",
      "episode: 715   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 403     evaluation reward: 143.7\n",
      "Training network\n",
      "Iteration 1450: Policy loss: -7.616903. Value loss: 7.968564. Entropy: 1.790725.\n",
      "Iteration 1451: Policy loss: -7.514303. Value loss: 7.865159. Entropy: 1.790717.\n",
      "Iteration 1452: Policy loss: -7.574981. Value loss: 7.930107. Entropy: 1.790736.\n",
      "episode: 716   score: 35.0   memory length: 1024   epsilon: 1.0    steps: 389     evaluation reward: 143.2\n",
      "episode: 717   score: 80.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 142.65\n",
      "episode: 718   score: 85.0   memory length: 1024   epsilon: 1.0    steps: 470     evaluation reward: 141.95\n",
      "Training network\n",
      "Iteration 1453: Policy loss: -8.217809. Value loss: 8.466524. Entropy: 1.790774.\n",
      "Iteration 1454: Policy loss: -8.220109. Value loss: 8.465586. Entropy: 1.790804.\n",
      "Iteration 1455: Policy loss: -8.217211. Value loss: 8.466805. Entropy: 1.790830.\n",
      "episode: 719   score: 395.0   memory length: 1024   epsilon: 1.0    steps: 819     evaluation reward: 145.5\n",
      "Training network\n",
      "Iteration 1456: Policy loss: -26.313519. Value loss: 26.153736. Entropy: 1.790600.\n",
      "Iteration 1457: Policy loss: -26.321184. Value loss: 26.155268. Entropy: 1.790592.\n",
      "Iteration 1458: Policy loss: -26.220764. Value loss: 26.045111. Entropy: 1.790605.\n",
      "episode: 720   score: 410.0   memory length: 1024   epsilon: 1.0    steps: 900     evaluation reward: 148.85\n",
      "Training network\n",
      "Iteration 1459: Policy loss: -18.829119. Value loss: 18.964212. Entropy: 1.790684.\n",
      "Iteration 1460: Policy loss: -18.779741. Value loss: 18.897398. Entropy: 1.790710.\n",
      "Iteration 1461: Policy loss: -18.906115. Value loss: 19.030460. Entropy: 1.790703.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 721   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 630     evaluation reward: 147.2\n",
      "episode: 722   score: 45.0   memory length: 1024   epsilon: 1.0    steps: 382     evaluation reward: 146.85\n",
      "Training network\n",
      "Iteration 1462: Policy loss: -7.250010. Value loss: 7.356271. Entropy: 1.790648.\n",
      "Iteration 1463: Policy loss: -7.244108. Value loss: 7.350345. Entropy: 1.790645.\n",
      "Iteration 1464: Policy loss: -7.183852. Value loss: 7.295191. Entropy: 1.790651.\n",
      "episode: 723   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 416     evaluation reward: 143.65\n",
      "now time :  2018-12-19 13:35:29.101346\n",
      "episode: 724   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 890     evaluation reward: 144.5\n",
      "Training network\n",
      "Iteration 1465: Policy loss: -8.102412. Value loss: 8.287089. Entropy: 1.790618.\n",
      "Iteration 1466: Policy loss: -8.208789. Value loss: 8.384427. Entropy: 1.790605.\n",
      "Iteration 1467: Policy loss: -8.122915. Value loss: 8.301895. Entropy: 1.790610.\n",
      "episode: 725   score: 260.0   memory length: 1024   epsilon: 1.0    steps: 969     evaluation reward: 146.0\n",
      "Training network\n",
      "Iteration 1468: Policy loss: -14.291777. Value loss: 14.164371. Entropy: 1.790558.\n",
      "Iteration 1469: Policy loss: -14.391094. Value loss: 14.254456. Entropy: 1.790530.\n",
      "Iteration 1470: Policy loss: -14.373341. Value loss: 14.238310. Entropy: 1.790548.\n",
      "episode: 726   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 680     evaluation reward: 147.0\n",
      "Training network\n",
      "Iteration 1471: Policy loss: -11.336830. Value loss: 11.130016. Entropy: 1.790734.\n",
      "Iteration 1472: Policy loss: -11.576606. Value loss: 11.357301. Entropy: 1.790681.\n",
      "Iteration 1473: Policy loss: -11.460981. Value loss: 11.245999. Entropy: 1.790679.\n",
      "episode: 727   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 776     evaluation reward: 146.4\n",
      "episode: 728   score: 95.0   memory length: 1024   epsilon: 1.0    steps: 407     evaluation reward: 145.65\n",
      "Training network\n",
      "Iteration 1474: Policy loss: -9.960941. Value loss: 10.072693. Entropy: 1.790641.\n",
      "Iteration 1475: Policy loss: -9.882942. Value loss: 10.006830. Entropy: 1.790637.\n",
      "Iteration 1476: Policy loss: -9.965390. Value loss: 10.083768. Entropy: 1.790669.\n",
      "episode: 729   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 703     evaluation reward: 144.6\n",
      "episode: 730   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 406     evaluation reward: 143.0\n",
      "Training network\n",
      "Iteration 1477: Policy loss: -8.076178. Value loss: 8.136338. Entropy: 1.790611.\n",
      "Iteration 1478: Policy loss: -8.093438. Value loss: 8.158533. Entropy: 1.790604.\n",
      "Iteration 1479: Policy loss: -8.136302. Value loss: 8.194521. Entropy: 1.790610.\n",
      "episode: 731   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 595     evaluation reward: 142.25\n",
      "episode: 732   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 515     evaluation reward: 143.0\n",
      "Training network\n",
      "Iteration 1480: Policy loss: -6.644083. Value loss: 6.709899. Entropy: 1.790614.\n",
      "Iteration 1481: Policy loss: -6.583611. Value loss: 6.647136. Entropy: 1.790595.\n",
      "Iteration 1482: Policy loss: -6.667956. Value loss: 6.735925. Entropy: 1.790603.\n",
      "episode: 733   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 779     evaluation reward: 143.35\n",
      "Training network\n",
      "Iteration 1483: Policy loss: -7.387706. Value loss: 7.580969. Entropy: 1.790639.\n",
      "Iteration 1484: Policy loss: -7.524824. Value loss: 7.706874. Entropy: 1.790650.\n",
      "Iteration 1485: Policy loss: -7.455690. Value loss: 7.648017. Entropy: 1.790640.\n",
      "episode: 734   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 685     evaluation reward: 143.25\n",
      "episode: 735   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 688     evaluation reward: 144.3\n",
      "Training network\n",
      "Iteration 1486: Policy loss: -11.448865. Value loss: 11.890175. Entropy: 1.790607.\n",
      "Iteration 1487: Policy loss: -11.533914. Value loss: 11.972161. Entropy: 1.790634.\n",
      "Iteration 1488: Policy loss: -11.260760. Value loss: 11.714491. Entropy: 1.790617.\n",
      "episode: 736   score: 35.0   memory length: 1024   epsilon: 1.0    steps: 561     evaluation reward: 143.6\n",
      "Training network\n",
      "Iteration 1489: Policy loss: -5.606968. Value loss: 5.818930. Entropy: 1.790592.\n",
      "Iteration 1490: Policy loss: -5.664777. Value loss: 5.870643. Entropy: 1.790623.\n",
      "Iteration 1491: Policy loss: -5.605335. Value loss: 5.809521. Entropy: 1.790591.\n",
      "episode: 737   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 687     evaluation reward: 143.95\n",
      "episode: 738   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 790     evaluation reward: 144.45\n",
      "Training network\n",
      "Iteration 1492: Policy loss: -7.463602. Value loss: 7.541098. Entropy: 1.790514.\n",
      "Iteration 1493: Policy loss: -7.322433. Value loss: 7.408253. Entropy: 1.790543.\n",
      "Iteration 1494: Policy loss: -7.328077. Value loss: 7.415329. Entropy: 1.790517.\n",
      "episode: 739   score: 395.0   memory length: 1024   epsilon: 1.0    steps: 915     evaluation reward: 146.6\n",
      "Training network\n",
      "Iteration 1495: Policy loss: -24.716595. Value loss: 24.391039. Entropy: 1.790441.\n",
      "Iteration 1496: Policy loss: -24.487236. Value loss: 24.161728. Entropy: 1.790431.\n",
      "Iteration 1497: Policy loss: -24.213663. Value loss: 23.884897. Entropy: 1.790387.\n",
      "episode: 740   score: 325.0   memory length: 1024   epsilon: 1.0    steps: 822     evaluation reward: 149.35\n",
      "Training network\n",
      "Iteration 1498: Policy loss: -18.584883. Value loss: 18.758678. Entropy: 1.790614.\n",
      "Iteration 1499: Policy loss: -18.897282. Value loss: 19.068209. Entropy: 1.790597.\n",
      "Iteration 1500: Policy loss: -19.041956. Value loss: 19.197744. Entropy: 1.790596.\n",
      "episode: 741   score: 180.0   memory length: 1024   epsilon: 1.0    steps: 818     evaluation reward: 148.9\n",
      "episode: 742   score: 20.0   memory length: 1024   epsilon: 1.0    steps: 546     evaluation reward: 148.75\n",
      "Training network\n",
      "Iteration 1501: Policy loss: -4.949606. Value loss: 5.457601. Entropy: 1.790392.\n",
      "Iteration 1502: Policy loss: -5.083189. Value loss: 5.580954. Entropy: 1.790388.\n",
      "Iteration 1503: Policy loss: -4.992513. Value loss: 5.500812. Entropy: 1.790409.\n",
      "episode: 743   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 422     evaluation reward: 149.05\n",
      "Training network\n",
      "Iteration 1504: Policy loss: -9.221720. Value loss: 9.303843. Entropy: 1.790473.\n",
      "Iteration 1505: Policy loss: -9.257468. Value loss: 9.332239. Entropy: 1.790509.\n",
      "Iteration 1506: Policy loss: -9.272963. Value loss: 9.349364. Entropy: 1.790485.\n",
      "episode: 744   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 646     evaluation reward: 149.3\n",
      "episode: 745   score: 90.0   memory length: 1024   epsilon: 1.0    steps: 624     evaluation reward: 148.4\n",
      "Training network\n",
      "Iteration 1507: Policy loss: -5.813475. Value loss: 6.050494. Entropy: 1.790728.\n",
      "Iteration 1508: Policy loss: -5.853018. Value loss: 6.084646. Entropy: 1.790738.\n",
      "Iteration 1509: Policy loss: -5.845524. Value loss: 6.068887. Entropy: 1.790686.\n",
      "episode: 746   score: 60.0   memory length: 1024   epsilon: 1.0    steps: 545     evaluation reward: 146.9\n",
      "episode: 747   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 398     evaluation reward: 146.75\n",
      "Training network\n",
      "Iteration 1510: Policy loss: -5.072761. Value loss: 5.097855. Entropy: 1.790603.\n",
      "Iteration 1511: Policy loss: -5.173299. Value loss: 5.190445. Entropy: 1.790562.\n",
      "Iteration 1512: Policy loss: -5.125851. Value loss: 5.144802. Entropy: 1.790597.\n",
      "episode: 748   score: 45.0   memory length: 1024   epsilon: 1.0    steps: 585     evaluation reward: 146.1\n",
      "episode: 749   score: 50.0   memory length: 1024   epsilon: 1.0    steps: 402     evaluation reward: 145.5\n",
      "Training network\n",
      "Iteration 1513: Policy loss: -5.920592. Value loss: 6.528543. Entropy: 1.790475.\n",
      "Iteration 1514: Policy loss: -6.117984. Value loss: 6.708114. Entropy: 1.790511.\n",
      "Iteration 1515: Policy loss: -5.996997. Value loss: 6.598285. Entropy: 1.790530.\n",
      "episode: 750   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 607     evaluation reward: 145.6\n",
      "episode: 751   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 606     evaluation reward: 146.25\n",
      "Training network\n",
      "Iteration 1516: Policy loss: -10.055236. Value loss: 10.013755. Entropy: 1.790657.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1517: Policy loss: -10.066642. Value loss: 10.015295. Entropy: 1.790622.\n",
      "Iteration 1518: Policy loss: -9.987080. Value loss: 9.946028. Entropy: 1.790677.\n",
      "episode: 752   score: 130.0   memory length: 1024   epsilon: 1.0    steps: 406     evaluation reward: 146.0\n",
      "episode: 753   score: 100.0   memory length: 1024   epsilon: 1.0    steps: 850     evaluation reward: 146.2\n",
      "Training network\n",
      "Iteration 1519: Policy loss: -5.543866. Value loss: 5.984920. Entropy: 1.790499.\n",
      "Iteration 1520: Policy loss: -5.501387. Value loss: 5.939048. Entropy: 1.790498.\n",
      "Iteration 1521: Policy loss: -5.397815. Value loss: 5.848903. Entropy: 1.790493.\n",
      "episode: 754   score: 130.0   memory length: 1024   epsilon: 1.0    steps: 632     evaluation reward: 147.4\n",
      "Training network\n",
      "Iteration 1522: Policy loss: -8.617120. Value loss: 9.008932. Entropy: 1.790521.\n",
      "Iteration 1523: Policy loss: -8.607680. Value loss: 8.993595. Entropy: 1.790482.\n",
      "Iteration 1524: Policy loss: -8.681946. Value loss: 9.065740. Entropy: 1.790491.\n",
      "episode: 755   score: 85.0   memory length: 1024   epsilon: 1.0    steps: 588     evaluation reward: 146.6\n",
      "episode: 756   score: 350.0   memory length: 1024   epsilon: 1.0    steps: 774     evaluation reward: 148.0\n",
      "Training network\n",
      "Iteration 1525: Policy loss: -21.605612. Value loss: 21.500061. Entropy: 1.790504.\n",
      "Iteration 1526: Policy loss: -21.167147. Value loss: 21.071251. Entropy: 1.790534.\n",
      "Iteration 1527: Policy loss: -21.051647. Value loss: 20.953558. Entropy: 1.790499.\n",
      "episode: 757   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 747     evaluation reward: 148.4\n",
      "Training network\n",
      "Iteration 1528: Policy loss: -7.676427. Value loss: 7.894244. Entropy: 1.790614.\n",
      "Iteration 1529: Policy loss: -7.669649. Value loss: 7.886813. Entropy: 1.790608.\n",
      "Iteration 1530: Policy loss: -7.773918. Value loss: 7.983281. Entropy: 1.790631.\n",
      "episode: 758   score: 75.0   memory length: 1024   epsilon: 1.0    steps: 529     evaluation reward: 147.1\n",
      "Training network\n",
      "Iteration 1531: Policy loss: -13.490516. Value loss: 13.350410. Entropy: 1.790602.\n",
      "Iteration 1532: Policy loss: -13.379101. Value loss: 13.237129. Entropy: 1.790596.\n",
      "Iteration 1533: Policy loss: -13.458987. Value loss: 13.319797. Entropy: 1.790605.\n",
      "episode: 759   score: 425.0   memory length: 1024   epsilon: 1.0    steps: 1253     evaluation reward: 150.55\n",
      "episode: 760   score: 40.0   memory length: 1024   epsilon: 1.0    steps: 388     evaluation reward: 149.15\n",
      "Training network\n",
      "Iteration 1534: Policy loss: -19.406216. Value loss: 19.401751. Entropy: 1.790514.\n",
      "Iteration 1535: Policy loss: -19.766415. Value loss: 19.768599. Entropy: 1.790516.\n",
      "Iteration 1536: Policy loss: -19.655655. Value loss: 19.655270. Entropy: 1.790535.\n",
      "episode: 761   score: 240.0   memory length: 1024   epsilon: 1.0    steps: 854     evaluation reward: 147.45\n",
      "Training network\n",
      "Iteration 1537: Policy loss: -13.679770. Value loss: 13.565018. Entropy: 1.790687.\n",
      "Iteration 1538: Policy loss: -13.590331. Value loss: 13.481752. Entropy: 1.790721.\n",
      "Iteration 1539: Policy loss: -13.630819. Value loss: 13.519170. Entropy: 1.790699.\n",
      "episode: 762   score: 155.0   memory length: 1024   epsilon: 1.0    steps: 796     evaluation reward: 146.6\n",
      "episode: 763   score: 35.0   memory length: 1024   epsilon: 1.0    steps: 666     evaluation reward: 145.55\n",
      "Training network\n",
      "Iteration 1540: Policy loss: -3.620353. Value loss: 4.210487. Entropy: 1.790619.\n",
      "Iteration 1541: Policy loss: -3.609493. Value loss: 4.193143. Entropy: 1.790621.\n",
      "Iteration 1542: Policy loss: -3.612832. Value loss: 4.203353. Entropy: 1.790612.\n",
      "Training network\n",
      "Iteration 1543: Policy loss: -12.015282. Value loss: 11.922524. Entropy: 1.790522.\n",
      "Iteration 1544: Policy loss: -12.113840. Value loss: 12.019812. Entropy: 1.790499.\n",
      "Iteration 1545: Policy loss: -12.026180. Value loss: 11.932858. Entropy: 1.790522.\n",
      "episode: 764   score: 440.0   memory length: 1024   epsilon: 1.0    steps: 1538     evaluation reward: 148.3\n",
      "Training network\n",
      "Iteration 1546: Policy loss: -17.997305. Value loss: 17.900324. Entropy: 1.790256.\n",
      "Iteration 1547: Policy loss: -18.087339. Value loss: 17.987585. Entropy: 1.790246.\n",
      "Iteration 1548: Policy loss: -17.796581. Value loss: 17.697411. Entropy: 1.790252.\n",
      "episode: 765   score: 160.0   memory length: 1024   epsilon: 1.0    steps: 790     evaluation reward: 146.55\n",
      "episode: 766   score: 125.0   memory length: 1024   epsilon: 1.0    steps: 655     evaluation reward: 146.45\n",
      "Training network\n",
      "Iteration 1549: Policy loss: -7.979834. Value loss: 8.068192. Entropy: 1.790552.\n",
      "Iteration 1550: Policy loss: -7.988826. Value loss: 8.067358. Entropy: 1.790538.\n",
      "Iteration 1551: Policy loss: -7.946293. Value loss: 8.033857. Entropy: 1.790542.\n",
      "episode: 767   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 412     evaluation reward: 146.8\n",
      "Training network\n",
      "Iteration 1552: Policy loss: -9.761954. Value loss: 9.864137. Entropy: 1.790452.\n",
      "Iteration 1553: Policy loss: -9.703877. Value loss: 9.806850. Entropy: 1.790501.\n",
      "Iteration 1554: Policy loss: -9.800235. Value loss: 9.896179. Entropy: 1.790488.\n",
      "episode: 768   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 810     evaluation reward: 146.8\n",
      "episode: 769   score: 210.0   memory length: 1024   epsilon: 1.0    steps: 823     evaluation reward: 148.55\n",
      "Training network\n",
      "Iteration 1555: Policy loss: -13.743323. Value loss: 13.710101. Entropy: 1.790732.\n",
      "Iteration 1556: Policy loss: -13.778011. Value loss: 13.748305. Entropy: 1.790681.\n",
      "Iteration 1557: Policy loss: -13.704082. Value loss: 13.672780. Entropy: 1.790703.\n",
      "episode: 770   score: 95.0   memory length: 1024   epsilon: 1.0    steps: 486     evaluation reward: 149.2\n",
      "Training network\n",
      "Iteration 1558: Policy loss: -13.530310. Value loss: 13.762313. Entropy: 1.790574.\n",
      "Iteration 1559: Policy loss: -13.428344. Value loss: 13.668252. Entropy: 1.790519.\n",
      "Iteration 1560: Policy loss: -13.323939. Value loss: 13.564903. Entropy: 1.790551.\n",
      "episode: 771   score: 240.0   memory length: 1024   epsilon: 1.0    steps: 717     evaluation reward: 147.45\n",
      "episode: 772   score: 25.0   memory length: 1024   epsilon: 1.0    steps: 687     evaluation reward: 144.45\n",
      "Training network\n",
      "Iteration 1561: Policy loss: -2.983720. Value loss: 3.355320. Entropy: 1.790649.\n",
      "Iteration 1562: Policy loss: -2.908040. Value loss: 3.288737. Entropy: 1.790657.\n",
      "Iteration 1563: Policy loss: -2.936801. Value loss: 3.312799. Entropy: 1.790635.\n",
      "episode: 773   score: 410.0   memory length: 1024   epsilon: 1.0    steps: 804     evaluation reward: 146.9\n",
      "Training network\n",
      "Iteration 1564: Policy loss: -26.675604. Value loss: 26.643415. Entropy: 1.790613.\n",
      "Iteration 1565: Policy loss: -26.773106. Value loss: 26.733135. Entropy: 1.790611.\n",
      "Iteration 1566: Policy loss: -26.695864. Value loss: 26.647680. Entropy: 1.790622.\n",
      "episode: 774   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 633     evaluation reward: 147.45\n",
      "episode: 775   score: 55.0   memory length: 1024   epsilon: 1.0    steps: 429     evaluation reward: 146.65\n",
      "Training network\n",
      "Iteration 1567: Policy loss: -4.100451. Value loss: 4.762351. Entropy: 1.790592.\n",
      "Iteration 1568: Policy loss: -4.183871. Value loss: 4.839297. Entropy: 1.790606.\n",
      "Iteration 1569: Policy loss: -4.015556. Value loss: 4.687495. Entropy: 1.790601.\n",
      "episode: 776   score: 95.0   memory length: 1024   epsilon: 1.0    steps: 583     evaluation reward: 143.8\n",
      "episode: 777   score: 60.0   memory length: 1024   epsilon: 1.0    steps: 502     evaluation reward: 142.3\n",
      "Training network\n",
      "Iteration 1570: Policy loss: -8.254140. Value loss: 8.341858. Entropy: 1.790611.\n",
      "Iteration 1571: Policy loss: -8.172273. Value loss: 8.257493. Entropy: 1.790609.\n",
      "Iteration 1572: Policy loss: -8.271572. Value loss: 8.349730. Entropy: 1.790604.\n",
      "episode: 778   score: 245.0   memory length: 1024   epsilon: 1.0    steps: 776     evaluation reward: 143.65\n",
      "episode: 779   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 637     evaluation reward: 143.8\n",
      "Training network\n",
      "Iteration 1573: Policy loss: -11.828882. Value loss: 11.932900. Entropy: 1.790523.\n",
      "Iteration 1574: Policy loss: -11.659308. Value loss: 11.766931. Entropy: 1.790525.\n",
      "Iteration 1575: Policy loss: -11.798887. Value loss: 11.897728. Entropy: 1.790507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 780   score: 20.0   memory length: 1024   epsilon: 1.0    steps: 409     evaluation reward: 142.75\n",
      "Training network\n",
      "Iteration 1576: Policy loss: -13.459755. Value loss: 13.743070. Entropy: 1.790712.\n",
      "Iteration 1577: Policy loss: -13.242714. Value loss: 13.536271. Entropy: 1.790722.\n",
      "Iteration 1578: Policy loss: -13.224059. Value loss: 13.513487. Entropy: 1.790715.\n",
      "episode: 781   score: 225.0   memory length: 1024   epsilon: 1.0    steps: 814     evaluation reward: 143.45\n",
      "episode: 782   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 750     evaluation reward: 142.75\n",
      "Training network\n",
      "Iteration 1579: Policy loss: -6.319079. Value loss: 6.797122. Entropy: 1.790463.\n",
      "Iteration 1580: Policy loss: -6.416790. Value loss: 6.889909. Entropy: 1.790490.\n",
      "Iteration 1581: Policy loss: -6.238068. Value loss: 6.717836. Entropy: 1.790449.\n",
      "episode: 783   score: 140.0   memory length: 1024   epsilon: 1.0    steps: 641     evaluation reward: 142.35\n",
      "Training network\n",
      "Iteration 1582: Policy loss: -9.887044. Value loss: 10.144403. Entropy: 1.790615.\n",
      "Iteration 1583: Policy loss: -9.854081. Value loss: 10.111789. Entropy: 1.790648.\n",
      "Iteration 1584: Policy loss: -9.889287. Value loss: 10.149372. Entropy: 1.790616.\n",
      "episode: 784   score: 110.0   memory length: 1024   epsilon: 1.0    steps: 628     evaluation reward: 142.8\n",
      "episode: 785   score: 30.0   memory length: 1024   epsilon: 1.0    steps: 404     evaluation reward: 138.85\n",
      "Training network\n",
      "Iteration 1585: Policy loss: -5.947296. Value loss: 6.555223. Entropy: 1.790694.\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        if render_breakout:\n",
    "            env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        action, value = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        #r = np.clip(reward, -1, 1)\n",
    "        r = reward\n",
    "        \n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state, value, 0, 0)\n",
    "        # Start training after random sample generation\n",
    "        if(frame % train_frame == 0):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network\n",
    "            agent.update_target_net()\n",
    "        score += r\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > 40 and len(evaluation_reward) > 700:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn\")\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
