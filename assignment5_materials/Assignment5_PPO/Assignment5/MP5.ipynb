{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lifes(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 6\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/anaconda3/envs/py36/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],0)[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 80.0   memory length: 503   epsilon: 1.0    steps: 503     evaluation reward: 80.0\n",
      "episode: 1   score: 160.0   memory length: 1164   epsilon: 1.0    steps: 661     evaluation reward: 120.0\n",
      "episode: 2   score: 40.0   memory length: 1591   epsilon: 1.0    steps: 427     evaluation reward: 93.33333333333333\n",
      "episode: 3   score: 125.0   memory length: 2208   epsilon: 1.0    steps: 617     evaluation reward: 101.25\n",
      "episode: 4   score: 100.0   memory length: 2595   epsilon: 1.0    steps: 387     evaluation reward: 101.0\n",
      "episode: 5   score: 105.0   memory length: 3112   epsilon: 1.0    steps: 517     evaluation reward: 101.66666666666667\n",
      "episode: 6   score: 155.0   memory length: 3792   epsilon: 1.0    steps: 680     evaluation reward: 109.28571428571429\n",
      "episode: 7   score: 120.0   memory length: 4491   epsilon: 1.0    steps: 699     evaluation reward: 110.625\n",
      "episode: 8   score: 105.0   memory length: 4967   epsilon: 1.0    steps: 476     evaluation reward: 110.0\n",
      "episode: 9   score: 170.0   memory length: 5502   epsilon: 1.0    steps: 535     evaluation reward: 116.0\n",
      "episode: 10   score: 135.0   memory length: 6021   epsilon: 1.0    steps: 519     evaluation reward: 117.72727272727273\n",
      "episode: 11   score: 105.0   memory length: 6583   epsilon: 1.0    steps: 562     evaluation reward: 116.66666666666667\n",
      "episode: 12   score: 100.0   memory length: 6978   epsilon: 1.0    steps: 395     evaluation reward: 115.38461538461539\n",
      "episode: 13   score: 425.0   memory length: 7937   epsilon: 1.0    steps: 959     evaluation reward: 137.5\n",
      "episode: 14   score: 180.0   memory length: 8577   epsilon: 1.0    steps: 640     evaluation reward: 140.33333333333334\n",
      "episode: 15   score: 105.0   memory length: 9220   epsilon: 1.0    steps: 643     evaluation reward: 138.125\n",
      "episode: 16   score: 120.0   memory length: 9924   epsilon: 1.0    steps: 704     evaluation reward: 137.05882352941177\n",
      "Training network\n",
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:225: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:226: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:227: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy loss: 0.004986. Value loss: 9.844961. Entropy: 1.778321.\n",
      "Iteration 2\n",
      "Policy loss: 0.005097. Value loss: 7.800111. Entropy: 1.770913.\n",
      "Iteration 3\n",
      "Policy loss: -0.003542. Value loss: 6.126852. Entropy: 1.772685.\n",
      "episode: 17   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 594     evaluation reward: 135.27777777777777\n",
      "episode: 18   score: 220.0   memory length: 10240   epsilon: 1.0    steps: 828     evaluation reward: 139.73684210526315\n",
      "episode: 19   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 808     evaluation reward: 141.75\n",
      "episode: 20   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 591     evaluation reward: 143.57142857142858\n",
      "episode: 21   score: 170.0   memory length: 10240   epsilon: 1.0    steps: 1136     evaluation reward: 144.77272727272728\n",
      "episode: 22   score: 225.0   memory length: 10240   epsilon: 1.0    steps: 1143     evaluation reward: 148.2608695652174\n",
      "episode: 23   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 339     evaluation reward: 143.95833333333334\n",
      "episode: 24   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 705     evaluation reward: 145.4\n",
      "episode: 25   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 653     evaluation reward: 142.8846153846154\n",
      "episode: 26   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 419     evaluation reward: 139.8148148148148\n",
      "episode: 27   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 374     evaluation reward: 137.67857142857142\n",
      "episode: 28   score: 245.0   memory length: 10240   epsilon: 1.0    steps: 802     evaluation reward: 141.3793103448276\n",
      "episode: 29   score: 145.0   memory length: 10240   epsilon: 1.0    steps: 693     evaluation reward: 141.5\n",
      "episode: 30   score: 85.0   memory length: 10240   epsilon: 1.0    steps: 419     evaluation reward: 139.67741935483872\n",
      "episode: 31   score: 130.0   memory length: 10240   epsilon: 1.0    steps: 599     evaluation reward: 139.375\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.008567. Value loss: 7.764249. Entropy: 1.766217.\n",
      "Iteration 2\n",
      "Policy loss: 0.000009. Value loss: 5.459113. Entropy: 1.766154.\n",
      "Iteration 3\n",
      "Policy loss: -0.008989. Value loss: 4.164093. Entropy: 1.765753.\n",
      "episode: 32   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 698     evaluation reward: 138.33333333333334\n",
      "episode: 33   score: 260.0   memory length: 10240   epsilon: 1.0    steps: 1013     evaluation reward: 141.91176470588235\n",
      "episode: 34   score: 225.0   memory length: 10240   epsilon: 1.0    steps: 857     evaluation reward: 144.28571428571428\n",
      "episode: 35   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 641     evaluation reward: 143.75\n",
      "episode: 36   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 634     evaluation reward: 141.48648648648648\n",
      "episode: 37   score: 215.0   memory length: 10240   epsilon: 1.0    steps: 920     evaluation reward: 143.42105263157896\n",
      "episode: 38   score: 265.0   memory length: 10240   epsilon: 1.0    steps: 935     evaluation reward: 146.53846153846155\n",
      "episode: 39   score: 85.0   memory length: 10240   epsilon: 1.0    steps: 723     evaluation reward: 145.0\n",
      "episode: 40   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 931     evaluation reward: 145.2439024390244\n",
      "episode: 41   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 608     evaluation reward: 144.64285714285714\n",
      "episode: 42   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 590     evaluation reward: 143.72093023255815\n",
      "episode: 43   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 556     evaluation reward: 141.25\n",
      "episode: 44   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 655     evaluation reward: 140.44444444444446\n",
      "episode: 45   score: 215.0   memory length: 10240   epsilon: 1.0    steps: 875     evaluation reward: 142.06521739130434\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.005297. Value loss: 7.065381. Entropy: 1.763826.\n",
      "Iteration 2\n",
      "Policy loss: -0.002343. Value loss: 4.558880. Entropy: 1.759780.\n",
      "Iteration 3\n",
      "Policy loss: -0.007393. Value loss: 3.617945. Entropy: 1.757417.\n",
      "episode: 46   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 519     evaluation reward: 141.27659574468086\n",
      "episode: 47   score: 145.0   memory length: 10240   epsilon: 1.0    steps: 1043     evaluation reward: 141.35416666666666\n",
      "episode: 48   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 618     evaluation reward: 142.14285714285714\n",
      "episode: 49   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 697     evaluation reward: 142.3\n",
      "episode: 50   score: 200.0   memory length: 10240   epsilon: 1.0    steps: 829     evaluation reward: 143.4313725490196\n",
      "episode: 51   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 633     evaluation reward: 142.98076923076923\n",
      "episode: 52   score: 400.0   memory length: 10240   epsilon: 1.0    steps: 907     evaluation reward: 147.83018867924528\n",
      "episode: 53   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 548     evaluation reward: 147.40740740740742\n",
      "episode: 54   score: 190.0   memory length: 10240   epsilon: 1.0    steps: 777     evaluation reward: 148.1818181818182\n",
      "episode: 55   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 624     evaluation reward: 147.41071428571428\n",
      "episode: 56   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 426     evaluation reward: 145.78947368421052\n",
      "episode: 57   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 493     evaluation reward: 144.05172413793105\n",
      "episode: 58   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 662     evaluation reward: 144.66101694915255\n",
      "episode: 59   score: 320.0   memory length: 10240   epsilon: 1.0    steps: 1395     evaluation reward: 147.58333333333334\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.002757. Value loss: 7.899052. Entropy: 1.751645.\n",
      "Iteration 2\n",
      "Policy loss: -0.001138. Value loss: 5.520876. Entropy: 1.749361.\n",
      "Iteration 3\n",
      "Policy loss: -0.004118. Value loss: 4.580127. Entropy: 1.745655.\n",
      "episode: 60   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 583     evaluation reward: 147.13114754098362\n",
      "episode: 61   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 626     evaluation reward: 146.53225806451613\n",
      "episode: 62   score: 530.0   memory length: 10240   epsilon: 1.0    steps: 1282     evaluation reward: 152.61904761904762\n",
      "episode: 63   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 614     evaluation reward: 151.25\n",
      "episode: 64   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 492     evaluation reward: 149.6153846153846\n",
      "episode: 65   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 631     evaluation reward: 148.93939393939394\n",
      "episode: 66   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 505     evaluation reward: 148.28358208955223\n",
      "episode: 67   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 420     evaluation reward: 147.05882352941177\n",
      "episode: 68   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 379     evaluation reward: 145.7246376811594\n",
      "episode: 69   score: 225.0   memory length: 10240   epsilon: 1.0    steps: 761     evaluation reward: 146.85714285714286\n",
      "episode: 70   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 403     evaluation reward: 146.26760563380282\n",
      "episode: 71   score: 380.0   memory length: 10240   epsilon: 1.0    steps: 938     evaluation reward: 149.51388888888889\n",
      "episode: 72   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 704     evaluation reward: 149.93150684931507\n",
      "episode: 73   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 390     evaluation reward: 148.3783783783784\n",
      "episode: 74   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 413     evaluation reward: 147.13333333333333\n",
      "now time :  2018-12-18 18:50:18.798830\n",
      "episode: 75   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 958     evaluation reward: 147.23684210526315\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.000707. Value loss: 8.280407. Entropy: 1.747159.\n",
      "Iteration 2\n",
      "Policy loss: -0.005589. Value loss: 6.049726. Entropy: 1.739772.\n",
      "Iteration 3\n",
      "Policy loss: -0.010779. Value loss: 5.054049. Entropy: 1.741184.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 76   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 629     evaluation reward: 146.88311688311688\n",
      "episode: 77   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 634     evaluation reward: 146.92307692307693\n",
      "episode: 78   score: 320.0   memory length: 10240   epsilon: 1.0    steps: 697     evaluation reward: 149.1139240506329\n",
      "episode: 79   score: 25.0   memory length: 10240   epsilon: 1.0    steps: 401     evaluation reward: 147.5625\n",
      "episode: 80   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 640     evaluation reward: 147.22222222222223\n",
      "episode: 81   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 567     evaluation reward: 145.85365853658536\n",
      "episode: 82   score: 40.0   memory length: 10240   epsilon: 1.0    steps: 349     evaluation reward: 144.57831325301206\n",
      "episode: 83   score: 305.0   memory length: 10240   epsilon: 1.0    steps: 974     evaluation reward: 146.48809523809524\n",
      "episode: 84   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 562     evaluation reward: 146.0\n",
      "episode: 85   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 526     evaluation reward: 145.69767441860466\n",
      "episode: 86   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 498     evaluation reward: 145.22988505747125\n",
      "episode: 87   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 521     evaluation reward: 144.82954545454547\n",
      "episode: 88   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 647     evaluation reward: 144.1011235955056\n",
      "episode: 89   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 552     evaluation reward: 143.38888888888889\n",
      "episode: 90   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 651     evaluation reward: 143.5164835164835\n",
      "episode: 91   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 781     evaluation reward: 143.31521739130434\n",
      "episode: 92   score: 260.0   memory length: 10240   epsilon: 1.0    steps: 789     evaluation reward: 144.56989247311827\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.002757. Value loss: 7.651225. Entropy: 1.746927.\n",
      "Iteration 2\n",
      "Policy loss: -0.008849. Value loss: 5.082746. Entropy: 1.736992.\n",
      "Iteration 3\n",
      "Policy loss: -0.015288. Value loss: 4.048498. Entropy: 1.737176.\n",
      "episode: 93   score: 355.0   memory length: 10240   epsilon: 1.0    steps: 800     evaluation reward: 146.80851063829786\n",
      "episode: 94   score: 100.0   memory length: 10240   epsilon: 1.0    steps: 533     evaluation reward: 146.31578947368422\n",
      "episode: 95   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 991     evaluation reward: 146.35416666666666\n",
      "episode: 96   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 737     evaluation reward: 146.08247422680412\n",
      "episode: 97   score: 170.0   memory length: 10240   epsilon: 1.0    steps: 635     evaluation reward: 146.3265306122449\n",
      "episode: 98   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 625     evaluation reward: 145.40404040404042\n",
      "episode: 99   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 630     evaluation reward: 145.0\n",
      "episode: 100   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 616     evaluation reward: 145.1\n",
      "episode: 101   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 748     evaluation reward: 144.7\n",
      "episode: 102   score: 170.0   memory length: 10240   epsilon: 1.0    steps: 734     evaluation reward: 146.0\n",
      "episode: 103   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 639     evaluation reward: 145.4\n",
      "episode: 104   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 606     evaluation reward: 145.05\n",
      "episode: 105   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 534     evaluation reward: 144.5\n",
      "episode: 106   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 799     evaluation reward: 145.05\n",
      "episode: 107   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 693     evaluation reward: 144.95\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.001879. Value loss: 6.848035. Entropy: 1.733357.\n",
      "Iteration 2\n",
      "Policy loss: -0.006022. Value loss: 4.717567. Entropy: 1.721821.\n",
      "Iteration 3\n",
      "Policy loss: -0.007950. Value loss: 3.824402. Entropy: 1.719160.\n",
      "episode: 108   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 432     evaluation reward: 144.45\n",
      "episode: 109   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 624     evaluation reward: 144.1\n",
      "episode: 110   score: 240.0   memory length: 10240   epsilon: 1.0    steps: 928     evaluation reward: 145.15\n",
      "episode: 111   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 671     evaluation reward: 145.3\n",
      "episode: 112   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 397     evaluation reward: 144.6\n",
      "episode: 113   score: 145.0   memory length: 10240   epsilon: 1.0    steps: 703     evaluation reward: 141.8\n",
      "episode: 114   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 603     evaluation reward: 140.9\n",
      "episode: 115   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 1102     evaluation reward: 141.95\n",
      "episode: 116   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 609     evaluation reward: 141.65\n",
      "episode: 117   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 721     evaluation reward: 141.7\n",
      "episode: 118   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 391     evaluation reward: 140.4\n",
      "episode: 119   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 712     evaluation reward: 139.8\n",
      "episode: 120   score: 480.0   memory length: 10240   epsilon: 1.0    steps: 1368     evaluation reward: 142.8\n",
      "episode: 121   score: 215.0   memory length: 10240   epsilon: 1.0    steps: 677     evaluation reward: 143.25\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.002466. Value loss: 6.406432. Entropy: 1.706132.\n",
      "Iteration 2\n",
      "Policy loss: -0.012622. Value loss: 4.156787. Entropy: 1.696870.\n",
      "Iteration 3\n",
      "Policy loss: -0.017729. Value loss: 3.245766. Entropy: 1.692444.\n",
      "episode: 122   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 770     evaluation reward: 142.55\n",
      "episode: 123   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 616     evaluation reward: 143.35\n",
      "episode: 124   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 500     evaluation reward: 142.1\n",
      "episode: 125   score: 320.0   memory length: 10240   epsilon: 1.0    steps: 918     evaluation reward: 144.5\n",
      "episode: 126   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 710     evaluation reward: 145.75\n",
      "episode: 127   score: 55.0   memory length: 10240   epsilon: 1.0    steps: 349     evaluation reward: 145.5\n",
      "episode: 128   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 776     evaluation reward: 145.15\n",
      "episode: 129   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 562     evaluation reward: 144.9\n",
      "episode: 130   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 681     evaluation reward: 145.6\n",
      "episode: 131   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 626     evaluation reward: 145.85\n",
      "episode: 132   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 587     evaluation reward: 146.35\n",
      "episode: 133   score: 205.0   memory length: 10240   epsilon: 1.0    steps: 1597     evaluation reward: 145.8\n",
      "episode: 134   score: 165.0   memory length: 10240   epsilon: 1.0    steps: 770     evaluation reward: 145.2\n",
      "episode: 135   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 662     evaluation reward: 145.45\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.003300. Value loss: 6.446067. Entropy: 1.700023.\n",
      "Iteration 2\n",
      "Policy loss: -0.016406. Value loss: 3.861035. Entropy: 1.689651.\n",
      "Iteration 3\n",
      "Policy loss: -0.020956. Value loss: 3.105480. Entropy: 1.686679.\n",
      "episode: 136   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 641     evaluation reward: 146.1\n",
      "episode: 137   score: 170.0   memory length: 10240   epsilon: 1.0    steps: 637     evaluation reward: 145.65\n",
      "episode: 138   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 872     evaluation reward: 145.1\n",
      "episode: 139   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 484     evaluation reward: 145.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 140   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 373     evaluation reward: 143.85\n",
      "episode: 141   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 454     evaluation reward: 143.7\n",
      "episode: 142   score: 255.0   memory length: 10240   epsilon: 1.0    steps: 967     evaluation reward: 145.2\n",
      "episode: 143   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 782     evaluation reward: 146.65\n",
      "episode: 144   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 723     evaluation reward: 146.8\n",
      "episode: 145   score: 15.0   memory length: 10240   epsilon: 1.0    steps: 575     evaluation reward: 144.8\n",
      "episode: 146   score: 115.0   memory length: 10240   epsilon: 1.0    steps: 739     evaluation reward: 144.9\n",
      "episode: 147   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 611     evaluation reward: 144.05\n",
      "now time :  2018-12-18 18:53:08.529188\n",
      "episode: 148   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 633     evaluation reward: 143.75\n",
      "episode: 149   score: 300.0   memory length: 10240   epsilon: 1.0    steps: 1205     evaluation reward: 145.25\n",
      "episode: 150   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 696     evaluation reward: 145.05\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.010485. Value loss: 6.262900. Entropy: 1.695564.\n",
      "Iteration 2\n",
      "Policy loss: -0.021285. Value loss: 4.126655. Entropy: 1.689858.\n",
      "Iteration 3\n",
      "Policy loss: -0.026018. Value loss: 3.467538. Entropy: 1.687569.\n",
      "episode: 151   score: 240.0   memory length: 10240   epsilon: 1.0    steps: 983     evaluation reward: 146.25\n",
      "episode: 152   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 688     evaluation reward: 143.35\n",
      "episode: 153   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 385     evaluation reward: 142.9\n",
      "episode: 154   score: 355.0   memory length: 10240   epsilon: 1.0    steps: 714     evaluation reward: 144.55\n",
      "episode: 155   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 515     evaluation reward: 144.15\n",
      "episode: 156   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 384     evaluation reward: 144.25\n",
      "episode: 157   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 682     evaluation reward: 145.0\n",
      "episode: 158   score: 275.0   memory length: 10240   epsilon: 1.0    steps: 850     evaluation reward: 145.95\n",
      "episode: 159   score: 15.0   memory length: 10240   epsilon: 1.0    steps: 459     evaluation reward: 142.9\n",
      "episode: 160   score: 190.0   memory length: 10240   epsilon: 1.0    steps: 1023     evaluation reward: 143.6\n",
      "episode: 161   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 501     evaluation reward: 143.0\n",
      "episode: 162   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 511     evaluation reward: 138.05\n",
      "episode: 163   score: 200.0   memory length: 10240   epsilon: 1.0    steps: 846     evaluation reward: 139.4\n",
      "episode: 164   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 678     evaluation reward: 140.75\n",
      "episode: 165   score: 350.0   memory length: 10240   epsilon: 1.0    steps: 817     evaluation reward: 143.2\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.008315. Value loss: 7.749315. Entropy: 1.694988.\n",
      "Iteration 2\n",
      "Policy loss: -0.018487. Value loss: 5.303226. Entropy: 1.684709.\n",
      "Iteration 3\n",
      "Policy loss: -0.022304. Value loss: 4.402117. Entropy: 1.680386.\n",
      "episode: 166   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 620     evaluation reward: 142.8\n",
      "episode: 167   score: 375.0   memory length: 10240   epsilon: 1.0    steps: 1018     evaluation reward: 145.9\n",
      "episode: 168   score: 320.0   memory length: 10240   epsilon: 1.0    steps: 976     evaluation reward: 148.55\n",
      "episode: 169   score: 365.0   memory length: 10240   epsilon: 1.0    steps: 1083     evaluation reward: 149.95\n",
      "episode: 170   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 795     evaluation reward: 150.75\n",
      "episode: 171   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 404     evaluation reward: 148.0\n",
      "episode: 172   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 474     evaluation reward: 147.3\n",
      "episode: 173   score: 150.0   memory length: 10240   epsilon: 1.0    steps: 726     evaluation reward: 148.45\n",
      "episode: 174   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 623     evaluation reward: 149.0\n",
      "episode: 175   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 811     evaluation reward: 148.25\n",
      "episode: 176   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 404     evaluation reward: 147.95\n",
      "episode: 177   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 769     evaluation reward: 147.65\n",
      "episode: 178   score: 315.0   memory length: 10240   epsilon: 1.0    steps: 1081     evaluation reward: 147.6\n",
      "episode: 179   score: 215.0   memory length: 10240   epsilon: 1.0    steps: 875     evaluation reward: 149.5\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.008085. Value loss: 7.290788. Entropy: 1.682997.\n",
      "Iteration 2\n",
      "Policy loss: -0.018287. Value loss: 4.522900. Entropy: 1.672036.\n",
      "Iteration 3\n",
      "Policy loss: -0.025154. Value loss: 3.572050. Entropy: 1.669531.\n",
      "episode: 180   score: 255.0   memory length: 10240   epsilon: 1.0    steps: 1161     evaluation reward: 150.85\n",
      "episode: 181   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 540     evaluation reward: 151.55\n",
      "episode: 182   score: 165.0   memory length: 10240   epsilon: 1.0    steps: 806     evaluation reward: 152.8\n",
      "episode: 183   score: 530.0   memory length: 10240   epsilon: 1.0    steps: 1315     evaluation reward: 155.05\n",
      "episode: 184   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 527     evaluation reward: 155.35\n",
      "episode: 185   score: 100.0   memory length: 10240   epsilon: 1.0    steps: 666     evaluation reward: 155.15\n",
      "episode: 186   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 605     evaluation reward: 154.85\n",
      "episode: 187   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 751     evaluation reward: 155.3\n",
      "episode: 188   score: 90.0   memory length: 10240   epsilon: 1.0    steps: 385     evaluation reward: 155.4\n",
      "episode: 189   score: 215.0   memory length: 10240   epsilon: 1.0    steps: 828     evaluation reward: 156.75\n",
      "episode: 190   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 548     evaluation reward: 155.5\n",
      "episode: 191   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 844     evaluation reward: 156.35\n",
      "episode: 192   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 673     evaluation reward: 155.1\n",
      "episode: 193   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 618     evaluation reward: 152.3\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.004479. Value loss: 6.967394. Entropy: 1.684943.\n",
      "Iteration 2\n",
      "Policy loss: -0.013260. Value loss: 4.593103. Entropy: 1.680687.\n",
      "Iteration 3\n",
      "Policy loss: -0.019027. Value loss: 3.835661. Entropy: 1.676417.\n",
      "episode: 194   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 763     evaluation reward: 153.15\n",
      "episode: 195   score: 285.0   memory length: 10240   epsilon: 1.0    steps: 935     evaluation reward: 154.5\n",
      "episode: 196   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 661     evaluation reward: 154.65\n",
      "episode: 197   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 659     evaluation reward: 154.15\n",
      "episode: 198   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 571     evaluation reward: 154.95\n",
      "episode: 199   score: 240.0   memory length: 10240   epsilon: 1.0    steps: 774     evaluation reward: 156.3\n",
      "episode: 200   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 969     evaluation reward: 156.45\n",
      "episode: 201   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 509     evaluation reward: 156.3\n",
      "episode: 202   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 823     evaluation reward: 156.7\n",
      "episode: 203   score: 400.0   memory length: 10240   epsilon: 1.0    steps: 1027     evaluation reward: 160.05\n",
      "episode: 204   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 815     evaluation reward: 160.95\n",
      "episode: 205   score: 65.0   memory length: 10240   epsilon: 1.0    steps: 662     evaluation reward: 161.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 206   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 507     evaluation reward: 159.8\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.010070. Value loss: 7.779859. Entropy: 1.660876.\n",
      "Iteration 2\n",
      "Policy loss: -0.024947. Value loss: 5.270581. Entropy: 1.650039.\n",
      "Iteration 3\n",
      "Policy loss: -0.031708. Value loss: 4.283082. Entropy: 1.642962.\n",
      "episode: 207   score: 225.0   memory length: 10240   epsilon: 1.0    steps: 908     evaluation reward: 160.95\n",
      "episode: 208   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 813     evaluation reward: 162.5\n",
      "episode: 209   score: 240.0   memory length: 10240   epsilon: 1.0    steps: 925     evaluation reward: 163.55\n",
      "episode: 210   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 638     evaluation reward: 161.5\n",
      "episode: 211   score: 115.0   memory length: 10240   epsilon: 1.0    steps: 727     evaluation reward: 161.45\n",
      "episode: 212   score: 45.0   memory length: 10240   epsilon: 1.0    steps: 623     evaluation reward: 161.6\n",
      "episode: 213   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 490     evaluation reward: 160.9\n",
      "episode: 214   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 813     evaluation reward: 162.1\n",
      "episode: 215   score: 35.0   memory length: 10240   epsilon: 1.0    steps: 461     evaluation reward: 160.35\n",
      "episode: 216   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 665     evaluation reward: 161.25\n",
      "now time :  2018-12-18 18:55:58.675198\n",
      "episode: 217   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 482     evaluation reward: 161.4\n",
      "episode: 218   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 630     evaluation reward: 161.7\n",
      "episode: 219   score: 50.0   memory length: 10240   epsilon: 1.0    steps: 416     evaluation reward: 161.0\n",
      "episode: 220   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 840     evaluation reward: 158.3\n",
      "episode: 221   score: 185.0   memory length: 10240   epsilon: 1.0    steps: 794     evaluation reward: 158.0\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.012162. Value loss: 6.149227. Entropy: 1.653550.\n",
      "Iteration 2\n",
      "Policy loss: -0.025760. Value loss: 4.045955. Entropy: 1.649319.\n",
      "Iteration 3\n",
      "Policy loss: -0.030430. Value loss: 3.401728. Entropy: 1.642746.\n",
      "episode: 222   score: 625.0   memory length: 10240   epsilon: 1.0    steps: 1352     evaluation reward: 162.7\n",
      "episode: 223   score: 95.0   memory length: 10240   epsilon: 1.0    steps: 804     evaluation reward: 162.4\n",
      "episode: 224   score: 105.0   memory length: 10240   epsilon: 1.0    steps: 499     evaluation reward: 162.9\n",
      "episode: 225   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 692     evaluation reward: 160.9\n",
      "episode: 226   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 603     evaluation reward: 159.85\n",
      "episode: 227   score: 390.0   memory length: 10240   epsilon: 1.0    steps: 1075     evaluation reward: 163.2\n",
      "episode: 228   score: 125.0   memory length: 10240   epsilon: 1.0    steps: 643     evaluation reward: 162.35\n",
      "episode: 229   score: 80.0   memory length: 10240   epsilon: 1.0    steps: 620     evaluation reward: 161.95\n",
      "episode: 230   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 672     evaluation reward: 162.2\n",
      "episode: 231   score: 95.0   memory length: 10240   epsilon: 1.0    steps: 626     evaluation reward: 161.6\n",
      "episode: 232   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 762     evaluation reward: 161.6\n",
      "episode: 233   score: 5.0   memory length: 10240   epsilon: 1.0    steps: 477     evaluation reward: 159.6\n",
      "episode: 234   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 651     evaluation reward: 159.3\n",
      "episode: 235   score: 160.0   memory length: 10240   epsilon: 1.0    steps: 717     evaluation reward: 159.4\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.013356. Value loss: 6.801291. Entropy: 1.652577.\n",
      "Iteration 2\n",
      "Policy loss: -0.023089. Value loss: 4.119770. Entropy: 1.644451.\n",
      "Iteration 3\n",
      "Policy loss: -0.026086. Value loss: 3.332998. Entropy: 1.637487.\n",
      "episode: 236   score: 380.0   memory length: 10240   epsilon: 1.0    steps: 1014     evaluation reward: 161.95\n",
      "episode: 237   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 748     evaluation reward: 161.6\n",
      "episode: 238   score: 210.0   memory length: 10240   epsilon: 1.0    steps: 751     evaluation reward: 161.6\n",
      "episode: 239   score: 100.0   memory length: 10240   epsilon: 1.0    steps: 631     evaluation reward: 161.8\n",
      "episode: 240   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 682     evaluation reward: 162.8\n",
      "episode: 241   score: 180.0   memory length: 10240   epsilon: 1.0    steps: 794     evaluation reward: 163.55\n",
      "episode: 242   score: 120.0   memory length: 10240   epsilon: 1.0    steps: 700     evaluation reward: 162.2\n",
      "episode: 243   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 575     evaluation reward: 160.7\n",
      "episode: 244   score: 60.0   memory length: 10240   epsilon: 1.0    steps: 670     evaluation reward: 160.1\n",
      "episode: 245   score: 155.0   memory length: 10240   epsilon: 1.0    steps: 615     evaluation reward: 161.5\n",
      "episode: 246   score: 75.0   memory length: 10240   epsilon: 1.0    steps: 781     evaluation reward: 161.1\n",
      "episode: 247   score: 115.0   memory length: 10240   epsilon: 1.0    steps: 625     evaluation reward: 161.65\n",
      "episode: 248   score: 285.0   memory length: 10240   epsilon: 1.0    steps: 888     evaluation reward: 163.0\n",
      "episode: 249   score: 135.0   memory length: 10240   epsilon: 1.0    steps: 747     evaluation reward: 161.35\n",
      "episode: 250   score: 30.0   memory length: 10240   epsilon: 1.0    steps: 491     evaluation reward: 159.85\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.011366. Value loss: 6.390187. Entropy: 1.627222.\n",
      "Iteration 2\n",
      "Policy loss: -0.021055. Value loss: 4.274398. Entropy: 1.620334.\n",
      "Iteration 3\n",
      "Policy loss: -0.025613. Value loss: 3.475528. Entropy: 1.615434.\n",
      "episode: 251   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 771     evaluation reward: 158.55\n",
      "episode: 252   score: 110.0   memory length: 10240   epsilon: 1.0    steps: 536     evaluation reward: 158.55\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        if render_breakout:\n",
    "            env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        action, value = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        #r = np.clip(reward, -1, 1)\n",
    "        r = reward\n",
    "        \n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state, value, 0, 0)\n",
    "        # Start training after random sample generation\n",
    "        if(frame % train_frame == 0):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network\n",
    "            agent.update_target_net()\n",
    "        score += r\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > 40 and len(evaluation_reward) > 350:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn\")\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
