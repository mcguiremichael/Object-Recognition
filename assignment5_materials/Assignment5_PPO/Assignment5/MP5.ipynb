{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvadersDeterministic-v4')\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lifes(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 6\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/home/sigai/Documents/Projects/assignment5_materials/Assignment5_PPO/Assignment5/model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 120.0   memory length: 566   epsilon: 1.0    steps: 566     evaluation reward: 120.0\n",
      "episode: 1   score: 75.0   memory length: 984   epsilon: 1.0    steps: 418     evaluation reward: 97.5\n",
      "episode: 2   score: 135.0   memory length: 1763   epsilon: 1.0    steps: 779     evaluation reward: 110.0\n",
      "episode: 3   score: 130.0   memory length: 2370   epsilon: 1.0    steps: 607     evaluation reward: 115.0\n",
      "episode: 4   score: 85.0   memory length: 3049   epsilon: 1.0    steps: 679     evaluation reward: 109.0\n",
      "episode: 5   score: 190.0   memory length: 3970   epsilon: 1.0    steps: 921     evaluation reward: 122.5\n",
      "episode: 6   score: 65.0   memory length: 4547   epsilon: 1.0    steps: 577     evaluation reward: 114.28571428571429\n",
      "episode: 7   score: 125.0   memory length: 5202   epsilon: 1.0    steps: 655     evaluation reward: 115.625\n",
      "episode: 8   score: 90.0   memory length: 5713   epsilon: 1.0    steps: 511     evaluation reward: 112.77777777777777\n",
      "episode: 9   score: 155.0   memory length: 6450   epsilon: 1.0    steps: 737     evaluation reward: 117.0\n",
      "episode: 10   score: 120.0   memory length: 7136   epsilon: 1.0    steps: 686     evaluation reward: 117.27272727272727\n",
      "episode: 11   score: 80.0   memory length: 7661   epsilon: 1.0    steps: 525     evaluation reward: 114.16666666666667\n",
      "episode: 12   score: 250.0   memory length: 8998   epsilon: 1.0    steps: 1337     evaluation reward: 124.61538461538461\n",
      "episode: 13   score: 265.0   memory length: 10023   epsilon: 1.0    steps: 1025     evaluation reward: 134.64285714285714\n",
      "Training network\n",
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:150: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:151: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy loss: -0.781280. Value loss: 0.283367.\n",
      "Iteration 2\n",
      "Policy loss: -0.793153. Value loss: 0.147693.\n",
      "Iteration 3\n",
      "Policy loss: -0.792216. Value loss: 0.103779.\n",
      "episode: 14   score: 100.0   memory length: 10510   epsilon: 1.0    steps: 487     evaluation reward: 132.33333333333334\n",
      "episode: 15   score: 225.0   memory length: 11485   epsilon: 1.0    steps: 975     evaluation reward: 138.125\n",
      "episode: 16   score: 110.0   memory length: 12238   epsilon: 1.0    steps: 753     evaluation reward: 136.47058823529412\n",
      "episode: 17   score: 70.0   memory length: 12772   epsilon: 1.0    steps: 534     evaluation reward: 132.77777777777777\n",
      "episode: 18   score: 120.0   memory length: 13507   epsilon: 1.0    steps: 735     evaluation reward: 132.10526315789474\n",
      "episode: 19   score: 160.0   memory length: 14329   epsilon: 1.0    steps: 822     evaluation reward: 133.5\n",
      "episode: 20   score: 110.0   memory length: 14854   epsilon: 1.0    steps: 525     evaluation reward: 132.38095238095238\n",
      "episode: 21   score: 65.0   memory length: 15251   epsilon: 1.0    steps: 397     evaluation reward: 129.3181818181818\n",
      "episode: 22   score: 225.0   memory length: 16233   epsilon: 1.0    steps: 982     evaluation reward: 133.47826086956522\n",
      "episode: 23   score: 80.0   memory length: 16759   epsilon: 1.0    steps: 526     evaluation reward: 131.25\n",
      "episode: 24   score: 125.0   memory length: 17511   epsilon: 1.0    steps: 752     evaluation reward: 131.0\n",
      "episode: 25   score: 135.0   memory length: 18347   epsilon: 1.0    steps: 836     evaluation reward: 131.15384615384616\n",
      "episode: 26   score: 150.0   memory length: 19099   epsilon: 1.0    steps: 752     evaluation reward: 131.85185185185185\n",
      "episode: 27   score: 65.0   memory length: 19599   epsilon: 1.0    steps: 500     evaluation reward: 129.46428571428572\n",
      "episode: 28   score: 20.0   memory length: 20207   epsilon: 1.0    steps: 608     evaluation reward: 125.6896551724138\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.379060. Value loss: 0.179105.\n",
      "Iteration 2\n",
      "Policy loss: -0.389883. Value loss: 0.106773.\n",
      "Iteration 3\n",
      "Policy loss: -0.396165. Value loss: 0.081871.\n",
      "episode: 29   score: 210.0   memory length: 21032   epsilon: 1.0    steps: 825     evaluation reward: 128.5\n",
      "episode: 30   score: 120.0   memory length: 21687   epsilon: 1.0    steps: 655     evaluation reward: 128.2258064516129\n",
      "episode: 31   score: 110.0   memory length: 22263   epsilon: 1.0    steps: 576     evaluation reward: 127.65625\n",
      "episode: 32   score: 45.0   memory length: 22758   epsilon: 1.0    steps: 495     evaluation reward: 125.15151515151516\n",
      "episode: 33   score: 255.0   memory length: 23484   epsilon: 1.0    steps: 726     evaluation reward: 128.97058823529412\n",
      "episode: 34   score: 315.0   memory length: 24515   epsilon: 1.0    steps: 1031     evaluation reward: 134.28571428571428\n",
      "episode: 35   score: 150.0   memory length: 25346   epsilon: 1.0    steps: 831     evaluation reward: 134.72222222222223\n",
      "episode: 36   score: 210.0   memory length: 26149   epsilon: 1.0    steps: 803     evaluation reward: 136.75675675675674\n",
      "episode: 37   score: 165.0   memory length: 27112   epsilon: 1.0    steps: 963     evaluation reward: 137.5\n",
      "episode: 38   score: 110.0   memory length: 27635   epsilon: 1.0    steps: 523     evaluation reward: 136.7948717948718\n",
      "episode: 39   score: 180.0   memory length: 28602   epsilon: 1.0    steps: 967     evaluation reward: 137.875\n",
      "episode: 40   score: 200.0   memory length: 29407   epsilon: 1.0    steps: 805     evaluation reward: 139.390243902439\n",
      "episode: 41   score: 185.0   memory length: 30430   epsilon: 1.0    steps: 1023     evaluation reward: 140.47619047619048\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.327408. Value loss: 0.150553.\n",
      "Iteration 2\n",
      "Policy loss: -0.335810. Value loss: 0.090835.\n",
      "Iteration 3\n",
      "Policy loss: -0.344373. Value loss: 0.075879.\n",
      "episode: 42   score: 155.0   memory length: 31125   epsilon: 1.0    steps: 695     evaluation reward: 140.8139534883721\n",
      "episode: 43   score: 125.0   memory length: 31784   epsilon: 1.0    steps: 659     evaluation reward: 140.45454545454547\n",
      "episode: 44   score: 205.0   memory length: 32487   epsilon: 1.0    steps: 703     evaluation reward: 141.88888888888889\n",
      "episode: 45   score: 45.0   memory length: 32983   epsilon: 1.0    steps: 496     evaluation reward: 139.7826086956522\n",
      "episode: 46   score: 110.0   memory length: 33463   epsilon: 1.0    steps: 480     evaluation reward: 139.14893617021278\n",
      "episode: 47   score: 75.0   memory length: 33980   epsilon: 1.0    steps: 517     evaluation reward: 137.8125\n",
      "episode: 48   score: 55.0   memory length: 34472   epsilon: 1.0    steps: 492     evaluation reward: 136.12244897959184\n",
      "episode: 49   score: 465.0   memory length: 35404   epsilon: 1.0    steps: 932     evaluation reward: 142.7\n",
      "episode: 50   score: 210.0   memory length: 36144   epsilon: 1.0    steps: 740     evaluation reward: 144.01960784313727\n",
      "episode: 51   score: 105.0   memory length: 36780   epsilon: 1.0    steps: 636     evaluation reward: 143.26923076923077\n",
      "episode: 52   score: 80.0   memory length: 37145   epsilon: 1.0    steps: 365     evaluation reward: 142.0754716981132\n",
      "episode: 53   score: 180.0   memory length: 37787   epsilon: 1.0    steps: 642     evaluation reward: 142.77777777777777\n",
      "episode: 54   score: 110.0   memory length: 38658   epsilon: 1.0    steps: 871     evaluation reward: 142.1818181818182\n",
      "episode: 55   score: 250.0   memory length: 39614   epsilon: 1.0    steps: 956     evaluation reward: 144.10714285714286\n",
      "episode: 56   score: 110.0   memory length: 40282   epsilon: 1.0    steps: 668     evaluation reward: 143.50877192982455\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.262827. Value loss: 0.121073.\n",
      "Iteration 2\n",
      "Policy loss: -0.280770. Value loss: 0.082495.\n",
      "Iteration 3\n",
      "Policy loss: -0.297491. Value loss: 0.074095.\n",
      "episode: 57   score: 330.0   memory length: 41175   epsilon: 1.0    steps: 893     evaluation reward: 146.72413793103448\n",
      "episode: 58   score: 35.0   memory length: 41740   epsilon: 1.0    steps: 565     evaluation reward: 144.83050847457628\n",
      "episode: 59   score: 165.0   memory length: 42491   epsilon: 1.0    steps: 751     evaluation reward: 145.16666666666666\n",
      "episode: 60   score: 65.0   memory length: 43096   epsilon: 1.0    steps: 605     evaluation reward: 143.85245901639345\n",
      "episode: 61   score: 30.0   memory length: 43770   epsilon: 1.0    steps: 674     evaluation reward: 142.01612903225808\n",
      "episode: 62   score: 165.0   memory length: 44422   epsilon: 1.0    steps: 652     evaluation reward: 142.38095238095238\n",
      "episode: 63   score: 185.0   memory length: 45058   epsilon: 1.0    steps: 636     evaluation reward: 143.046875\n",
      "episode: 64   score: 195.0   memory length: 45917   epsilon: 1.0    steps: 859     evaluation reward: 143.84615384615384\n",
      "episode: 65   score: 275.0   memory length: 47105   epsilon: 1.0    steps: 1188     evaluation reward: 145.83333333333334\n",
      "episode: 66   score: 60.0   memory length: 47603   epsilon: 1.0    steps: 498     evaluation reward: 144.55223880597015\n",
      "episode: 67   score: 120.0   memory length: 48166   epsilon: 1.0    steps: 563     evaluation reward: 144.19117647058823\n",
      "episode: 68   score: 40.0   memory length: 48658   epsilon: 1.0    steps: 492     evaluation reward: 142.68115942028984\n",
      "episode: 69   score: 35.0   memory length: 49046   epsilon: 1.0    steps: 388     evaluation reward: 141.14285714285714\n",
      "episode: 70   score: 200.0   memory length: 49941   epsilon: 1.0    steps: 895     evaluation reward: 141.9718309859155\n",
      "now time :  2018-12-17 15:45:35.075122\n",
      "episode: 71   score: 255.0   memory length: 50711   epsilon: 1.0    steps: 770     evaluation reward: 143.54166666666666\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.217169. Value loss: 0.104185.\n",
      "Iteration 2\n",
      "Policy loss: -0.230970. Value loss: 0.077212.\n",
      "Iteration 3\n",
      "Policy loss: -0.248079. Value loss: 0.070541.\n",
      "episode: 72   score: 100.0   memory length: 51370   epsilon: 1.0    steps: 659     evaluation reward: 142.94520547945206\n",
      "episode: 73   score: 130.0   memory length: 52177   epsilon: 1.0    steps: 807     evaluation reward: 142.77027027027026\n",
      "episode: 74   score: 35.0   memory length: 52721   epsilon: 1.0    steps: 544     evaluation reward: 141.33333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 75   score: 130.0   memory length: 53328   epsilon: 1.0    steps: 607     evaluation reward: 141.18421052631578\n",
      "episode: 76   score: 290.0   memory length: 54293   epsilon: 1.0    steps: 965     evaluation reward: 143.11688311688312\n",
      "episode: 77   score: 315.0   memory length: 55103   epsilon: 1.0    steps: 810     evaluation reward: 145.32051282051282\n",
      "episode: 78   score: 130.0   memory length: 55943   epsilon: 1.0    steps: 840     evaluation reward: 145.126582278481\n",
      "episode: 79   score: 135.0   memory length: 56818   epsilon: 1.0    steps: 875     evaluation reward: 145.0\n",
      "episode: 80   score: 10.0   memory length: 57275   epsilon: 1.0    steps: 457     evaluation reward: 143.33333333333334\n",
      "episode: 81   score: 55.0   memory length: 57862   epsilon: 1.0    steps: 587     evaluation reward: 142.2560975609756\n",
      "episode: 82   score: 235.0   memory length: 58947   epsilon: 1.0    steps: 1085     evaluation reward: 143.3734939759036\n",
      "episode: 83   score: 325.0   memory length: 59805   epsilon: 1.0    steps: 858     evaluation reward: 145.53571428571428\n",
      "episode: 84   score: 345.0   memory length: 60676   epsilon: 1.0    steps: 871     evaluation reward: 147.88235294117646\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.199335. Value loss: 0.101330.\n",
      "Iteration 2\n",
      "Policy loss: -0.218736. Value loss: 0.076101.\n",
      "Iteration 3\n",
      "Policy loss: -0.226668. Value loss: 0.068671.\n",
      "episode: 85   score: 300.0   memory length: 61641   epsilon: 1.0    steps: 965     evaluation reward: 149.65116279069767\n",
      "episode: 86   score: 75.0   memory length: 62196   epsilon: 1.0    steps: 555     evaluation reward: 148.79310344827587\n",
      "episode: 87   score: 120.0   memory length: 62847   epsilon: 1.0    steps: 651     evaluation reward: 148.4659090909091\n",
      "episode: 88   score: 440.0   memory length: 63852   epsilon: 1.0    steps: 1005     evaluation reward: 151.74157303370785\n",
      "episode: 89   score: 230.0   memory length: 64940   epsilon: 1.0    steps: 1088     evaluation reward: 152.61111111111111\n",
      "episode: 90   score: 245.0   memory length: 65848   epsilon: 1.0    steps: 908     evaluation reward: 153.62637362637363\n",
      "episode: 91   score: 215.0   memory length: 67169   epsilon: 1.0    steps: 1321     evaluation reward: 154.29347826086956\n",
      "episode: 92   score: 235.0   memory length: 68183   epsilon: 1.0    steps: 1014     evaluation reward: 155.16129032258064\n",
      "episode: 93   score: 270.0   memory length: 69156   epsilon: 1.0    steps: 973     evaluation reward: 156.38297872340425\n",
      "episode: 94   score: 45.0   memory length: 69749   epsilon: 1.0    steps: 593     evaluation reward: 155.21052631578948\n",
      "episode: 95   score: 190.0   memory length: 70826   epsilon: 1.0    steps: 1077     evaluation reward: 155.57291666666666\n",
      "episode: 96   score: 60.0   memory length: 71325   epsilon: 1.0    steps: 499     evaluation reward: 154.58762886597938\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.175495. Value loss: 0.091102.\n",
      "Iteration 2\n",
      "Policy loss: -0.191913. Value loss: 0.069107.\n",
      "Iteration 3\n",
      "Policy loss: -0.204442. Value loss: 0.065490.\n",
      "episode: 97   score: 180.0   memory length: 71948   epsilon: 1.0    steps: 623     evaluation reward: 154.8469387755102\n",
      "episode: 98   score: 210.0   memory length: 72827   epsilon: 1.0    steps: 879     evaluation reward: 155.40404040404042\n",
      "episode: 99   score: 195.0   memory length: 73529   epsilon: 1.0    steps: 702     evaluation reward: 155.8\n",
      "episode: 100   score: 210.0   memory length: 74355   epsilon: 1.0    steps: 826     evaluation reward: 156.7\n",
      "episode: 101   score: 100.0   memory length: 74968   epsilon: 1.0    steps: 613     evaluation reward: 156.95\n",
      "episode: 102   score: 190.0   memory length: 75670   epsilon: 1.0    steps: 702     evaluation reward: 157.5\n",
      "episode: 103   score: 185.0   memory length: 76323   epsilon: 1.0    steps: 653     evaluation reward: 158.05\n",
      "episode: 104   score: 370.0   memory length: 77546   epsilon: 1.0    steps: 1223     evaluation reward: 160.9\n",
      "episode: 105   score: 55.0   memory length: 78037   epsilon: 1.0    steps: 491     evaluation reward: 159.55\n",
      "episode: 106   score: 145.0   memory length: 78634   epsilon: 1.0    steps: 597     evaluation reward: 160.35\n",
      "episode: 107   score: 110.0   memory length: 79171   epsilon: 1.0    steps: 537     evaluation reward: 160.2\n",
      "episode: 108   score: 320.0   memory length: 80032   epsilon: 1.0    steps: 861     evaluation reward: 162.5\n",
      "episode: 109   score: 240.0   memory length: 80917   epsilon: 1.0    steps: 885     evaluation reward: 163.35\n",
      "episode: 110   score: 225.0   memory length: 81897   epsilon: 1.0    steps: 980     evaluation reward: 164.4\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.177511. Value loss: 0.100255.\n",
      "Iteration 2\n",
      "Policy loss: -0.193195. Value loss: 0.075458.\n",
      "Iteration 3\n",
      "Policy loss: -0.206882. Value loss: 0.070863.\n",
      "episode: 111   score: 180.0   memory length: 82813   epsilon: 1.0    steps: 916     evaluation reward: 165.4\n",
      "episode: 112   score: 160.0   memory length: 83462   epsilon: 1.0    steps: 649     evaluation reward: 164.5\n",
      "episode: 113   score: 150.0   memory length: 84171   epsilon: 1.0    steps: 709     evaluation reward: 163.35\n",
      "episode: 114   score: 215.0   memory length: 84969   epsilon: 1.0    steps: 798     evaluation reward: 164.5\n",
      "episode: 115   score: 205.0   memory length: 85907   epsilon: 1.0    steps: 938     evaluation reward: 164.3\n",
      "episode: 116   score: 55.0   memory length: 86566   epsilon: 1.0    steps: 659     evaluation reward: 163.75\n",
      "episode: 117   score: 50.0   memory length: 86935   epsilon: 1.0    steps: 369     evaluation reward: 163.55\n",
      "episode: 118   score: 100.0   memory length: 87602   epsilon: 1.0    steps: 667     evaluation reward: 163.35\n",
      "episode: 119   score: 140.0   memory length: 88211   epsilon: 1.0    steps: 609     evaluation reward: 163.15\n",
      "episode: 120   score: 320.0   memory length: 89116   epsilon: 1.0    steps: 905     evaluation reward: 165.25\n",
      "episode: 121   score: 315.0   memory length: 90262   epsilon: 1.0    steps: 1146     evaluation reward: 167.75\n",
      "episode: 122   score: 200.0   memory length: 90945   epsilon: 1.0    steps: 683     evaluation reward: 167.5\n",
      "episode: 123   score: 160.0   memory length: 92124   epsilon: 1.0    steps: 1179     evaluation reward: 168.3\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.014371. Value loss: 0.106443.\n",
      "Iteration 2\n",
      "Policy loss: -0.130013. Value loss: 0.078328.\n",
      "Iteration 3\n",
      "Policy loss: -0.159187. Value loss: 0.070667.\n",
      "episode: 124   score: 110.0   memory length: 92652   epsilon: 1.0    steps: 528     evaluation reward: 168.15\n",
      "episode: 125   score: 430.0   memory length: 93592   epsilon: 1.0    steps: 940     evaluation reward: 171.1\n",
      "episode: 126   score: 275.0   memory length: 94444   epsilon: 1.0    steps: 852     evaluation reward: 172.35\n",
      "episode: 127   score: 325.0   memory length: 95370   epsilon: 1.0    steps: 926     evaluation reward: 174.95\n",
      "episode: 128   score: 505.0   memory length: 96462   epsilon: 1.0    steps: 1092     evaluation reward: 179.8\n",
      "episode: 129   score: 180.0   memory length: 97156   epsilon: 1.0    steps: 694     evaluation reward: 179.5\n",
      "episode: 130   score: 70.0   memory length: 97641   epsilon: 1.0    steps: 485     evaluation reward: 179.0\n",
      "episode: 131   score: 350.0   memory length: 98892   epsilon: 1.0    steps: 1251     evaluation reward: 181.4\n",
      "episode: 132   score: 105.0   memory length: 99335   epsilon: 1.0    steps: 443     evaluation reward: 182.0\n",
      "now time :  2018-12-17 15:54:22.445338\n",
      "episode: 133   score: 195.0   memory length: 100012   epsilon: 1.0    steps: 677     evaluation reward: 181.4\n",
      "episode: 134   score: 185.0   memory length: 100807   epsilon: 1.0    steps: 795     evaluation reward: 180.1\n",
      "episode: 135   score: 365.0   memory length: 101758   epsilon: 1.0    steps: 951     evaluation reward: 182.25\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.135602. Value loss: 0.118329.\n",
      "Iteration 2\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        if render_breakout:\n",
    "            env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        action, value = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state, value, 0, 0)\n",
    "        # Start training after random sample generation\n",
    "        if(frame % train_frame == 0):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network\n",
    "            agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > 400 and len(evaluation_reward) > 5:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn\")\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
