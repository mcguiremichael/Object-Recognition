{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lifes(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = env.action_space.n\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/home/michael/Documents/CS498DL/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],1)[0].unsqueeze(1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 1.0   memory length: 236   epsilon: 1.0    steps: 236     evaluation reward: 1.0\n",
      "episode: 1   score: 2.0   memory length: 532   epsilon: 1.0    steps: 296     evaluation reward: 1.5\n",
      "episode: 2   score: 2.0   memory length: 806   epsilon: 1.0    steps: 274     evaluation reward: 1.6666666666666667\n",
      "episode: 3   score: 0.0   memory length: 978   epsilon: 1.0    steps: 172     evaluation reward: 1.25\n",
      "Training network. lr: 0.000250. clip: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/Documents/CS498DL/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:259: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/michael/Documents/CS498DL/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:260: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/michael/Documents/CS498DL/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:261: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Policy loss: 0.015309. Value loss: 0.022334. Entropy: 1.365773.\n",
      "Iteration 2: Policy loss: 0.007594. Value loss: 0.022622. Entropy: 1.375099.\n",
      "Iteration 3: Policy loss: 0.001630. Value loss: 0.021370. Entropy: 1.376557.\n",
      "episode: 4   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 211     evaluation reward: 1.2\n",
      "episode: 5   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 250     evaluation reward: 1.3333333333333333\n",
      "episode: 6   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 303     evaluation reward: 1.4285714285714286\n",
      "episode: 7   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 210     evaluation reward: 1.25\n",
      "Training network. lr: 0.000250. clip: 0.099991\n",
      "Iteration 4: Policy loss: 0.001487. Value loss: 0.022242. Entropy: 1.366803.\n",
      "Iteration 5: Policy loss: 0.000655. Value loss: 0.022145. Entropy: 1.367186.\n",
      "Iteration 6: Policy loss: 0.000348. Value loss: 0.022892. Entropy: 1.365525.\n",
      "episode: 8   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 226     evaluation reward: 1.2222222222222223\n",
      "episode: 9   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 216     evaluation reward: 1.2\n",
      "episode: 10   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 176     evaluation reward: 1.0909090909090908\n",
      "episode: 11   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 168     evaluation reward: 1.0\n",
      "Training network. lr: 0.000250. clip: 0.099982\n",
      "Iteration 7: Policy loss: 0.001322. Value loss: 0.022510. Entropy: 1.365423.\n",
      "Iteration 8: Policy loss: 0.004152. Value loss: 0.022474. Entropy: 1.362302.\n",
      "Iteration 9: Policy loss: 0.002572. Value loss: 0.020184. Entropy: 1.370157.\n",
      "episode: 12   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 370     evaluation reward: 1.2307692307692308\n",
      "episode: 13   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 198     evaluation reward: 1.2142857142857142\n",
      "episode: 14   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 310     evaluation reward: 1.3333333333333333\n",
      "episode: 15   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 206     evaluation reward: 1.3125\n",
      "episode: 16   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 177     evaluation reward: 1.2352941176470589\n",
      "Training network. lr: 0.000250. clip: 0.099973\n",
      "Iteration 10: Policy loss: 0.002502. Value loss: 0.025990. Entropy: 1.356825.\n",
      "Iteration 11: Policy loss: 0.005970. Value loss: 0.024169. Entropy: 1.361402.\n",
      "Iteration 12: Policy loss: 0.001139. Value loss: 0.022963. Entropy: 1.356541.\n",
      "episode: 17   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 199     evaluation reward: 1.2222222222222223\n",
      "episode: 18   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 205     evaluation reward: 1.2105263157894737\n",
      "episode: 19   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 276     evaluation reward: 1.25\n",
      "episode: 20   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 304     evaluation reward: 1.2857142857142858\n",
      "Training network. lr: 0.000250. clip: 0.099964\n",
      "Iteration 13: Policy loss: 0.001586. Value loss: 0.022148. Entropy: 1.361711.\n",
      "Iteration 14: Policy loss: 0.002324. Value loss: 0.020780. Entropy: 1.366678.\n",
      "Iteration 15: Policy loss: 0.003406. Value loss: 0.019292. Entropy: 1.358757.\n",
      "episode: 21   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 253     evaluation reward: 1.3181818181818181\n",
      "episode: 22   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 336     evaluation reward: 1.391304347826087\n",
      "episode: 23   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 169     evaluation reward: 1.3333333333333333\n",
      "episode: 24   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 208     evaluation reward: 1.32\n",
      "Training network. lr: 0.000250. clip: 0.099955\n",
      "Iteration 16: Policy loss: 0.002135. Value loss: 0.028215. Entropy: 1.343829.\n",
      "Iteration 17: Policy loss: 0.003148. Value loss: 0.027440. Entropy: 1.352247.\n",
      "Iteration 18: Policy loss: 0.001027. Value loss: 0.025685. Entropy: 1.352279.\n",
      "episode: 25   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 303     evaluation reward: 1.3846153846153846\n",
      "episode: 26   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 256     evaluation reward: 1.4074074074074074\n",
      "episode: 27   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 172     evaluation reward: 1.3571428571428572\n",
      "episode: 28   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 262     evaluation reward: 1.3793103448275863\n",
      "episode: 29   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 204     evaluation reward: 1.3666666666666667\n",
      "Training network. lr: 0.000250. clip: 0.099946\n",
      "Iteration 19: Policy loss: 0.000381. Value loss: 0.026490. Entropy: 1.361707.\n",
      "Iteration 20: Policy loss: -0.000995. Value loss: 0.023686. Entropy: 1.362506.\n",
      "Iteration 21: Policy loss: -0.005134. Value loss: 0.021275. Entropy: 1.363207.\n",
      "episode: 30   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 177     evaluation reward: 1.3225806451612903\n",
      "episode: 31   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 173     evaluation reward: 1.28125\n",
      "episode: 32   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 410     evaluation reward: 1.3636363636363635\n",
      "Training network. lr: 0.000250. clip: 0.099937\n",
      "Iteration 22: Policy loss: 0.000609. Value loss: 0.030428. Entropy: 1.378645.\n",
      "Iteration 23: Policy loss: -0.001202. Value loss: 0.027837. Entropy: 1.376691.\n",
      "Iteration 24: Policy loss: -0.000841. Value loss: 0.025188. Entropy: 1.374615.\n",
      "episode: 33   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 306     evaluation reward: 1.411764705882353\n",
      "episode: 34   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 225     evaluation reward: 1.4\n",
      "episode: 35   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 263     evaluation reward: 1.4166666666666667\n",
      "episode: 36   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 320     evaluation reward: 1.4594594594594594\n",
      "Training network. lr: 0.000250. clip: 0.099928\n",
      "Iteration 25: Policy loss: -0.005619. Value loss: 0.025667. Entropy: 1.363223.\n",
      "Iteration 26: Policy loss: -0.003413. Value loss: 0.022810. Entropy: 1.364463.\n",
      "Iteration 27: Policy loss: -0.001949. Value loss: 0.022315. Entropy: 1.368077.\n",
      "episode: 37   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 220     evaluation reward: 1.4473684210526316\n",
      "episode: 38   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 217     evaluation reward: 1.435897435897436\n",
      "episode: 39   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 236     evaluation reward: 1.425\n",
      "episode: 40   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 172     evaluation reward: 1.3902439024390243\n",
      "episode: 41   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 189     evaluation reward: 1.3571428571428572\n",
      "Training network. lr: 0.000250. clip: 0.099919\n",
      "Iteration 28: Policy loss: 0.000143. Value loss: 0.016697. Entropy: 1.361936.\n",
      "Iteration 29: Policy loss: 0.001541. Value loss: 0.016524. Entropy: 1.367608.\n",
      "Iteration 30: Policy loss: -0.002167. Value loss: 0.015858. Entropy: 1.366311.\n",
      "episode: 42   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 345     evaluation reward: 1.3953488372093024\n",
      "episode: 43   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 175     evaluation reward: 1.3636363636363635\n",
      "episode: 44   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 248     evaluation reward: 1.3777777777777778\n",
      "episode: 45   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 174     evaluation reward: 1.3478260869565217\n",
      "episode: 46   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 218     evaluation reward: 1.3404255319148937\n",
      "Training network. lr: 0.000250. clip: 0.099910\n",
      "Iteration 31: Policy loss: -0.001859. Value loss: 0.020691. Entropy: 1.369705.\n",
      "Iteration 32: Policy loss: 0.000092. Value loss: 0.019310. Entropy: 1.372020.\n",
      "Iteration 33: Policy loss: -0.001687. Value loss: 0.019188. Entropy: 1.367379.\n",
      "episode: 47   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 289     evaluation reward: 1.3541666666666667\n",
      "episode: 48   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 179     evaluation reward: 1.3265306122448979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 49   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 236     evaluation reward: 1.32\n",
      "episode: 50   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 280     evaluation reward: 1.3333333333333333\n",
      "Training network. lr: 0.000250. clip: 0.099901\n",
      "Iteration 34: Policy loss: -0.001704. Value loss: 0.019973. Entropy: 1.368357.\n",
      "Iteration 35: Policy loss: -0.002906. Value loss: 0.018659. Entropy: 1.364466.\n",
      "Iteration 36: Policy loss: -0.004754. Value loss: 0.017865. Entropy: 1.363726.\n",
      "episode: 51   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 182     evaluation reward: 1.3076923076923077\n",
      "episode: 52   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 201     evaluation reward: 1.3018867924528301\n",
      "episode: 53   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 169     evaluation reward: 1.2777777777777777\n",
      "episode: 54   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 170     evaluation reward: 1.2545454545454546\n",
      "episode: 55   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 177     evaluation reward: 1.2321428571428572\n",
      "Training network. lr: 0.000250. clip: 0.099892\n",
      "Iteration 37: Policy loss: -0.000039. Value loss: 0.012215. Entropy: 1.353589.\n",
      "Iteration 38: Policy loss: 0.000587. Value loss: 0.011455. Entropy: 1.356884.\n",
      "Iteration 39: Policy loss: -0.001077. Value loss: 0.011284. Entropy: 1.364268.\n",
      "episode: 56   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 239     evaluation reward: 1.2456140350877194\n",
      "episode: 57   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 171     evaluation reward: 1.2241379310344827\n",
      "episode: 58   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 273     evaluation reward: 1.2372881355932204\n",
      "episode: 59   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 208     evaluation reward: 1.2333333333333334\n",
      "episode: 60   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 203     evaluation reward: 1.2295081967213115\n",
      "Training network. lr: 0.000250. clip: 0.099883\n",
      "Iteration 40: Policy loss: 0.000633. Value loss: 0.017135. Entropy: 1.362750.\n",
      "Iteration 41: Policy loss: -0.001238. Value loss: 0.017207. Entropy: 1.369634.\n",
      "Iteration 42: Policy loss: 0.000093. Value loss: 0.016694. Entropy: 1.365629.\n",
      "episode: 61   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 163     evaluation reward: 1.2096774193548387\n",
      "episode: 62   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 330     evaluation reward: 1.2380952380952381\n",
      "episode: 63   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 179     evaluation reward: 1.21875\n",
      "episode: 64   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 247     evaluation reward: 1.2153846153846153\n",
      "episode: 65   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 168     evaluation reward: 1.196969696969697\n",
      "Training network. lr: 0.000250. clip: 0.099874\n",
      "Iteration 43: Policy loss: -0.000923. Value loss: 0.017636. Entropy: 1.359921.\n",
      "Iteration 44: Policy loss: -0.002267. Value loss: 0.016733. Entropy: 1.362027.\n",
      "Iteration 45: Policy loss: -0.001687. Value loss: 0.016140. Entropy: 1.363826.\n",
      "episode: 66   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 342     evaluation reward: 1.2238805970149254\n",
      "episode: 67   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 166     evaluation reward: 1.2058823529411764\n",
      "episode: 68   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 188     evaluation reward: 1.1884057971014492\n",
      "episode: 69   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 177     evaluation reward: 1.1714285714285715\n",
      "episode: 70   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 181     evaluation reward: 1.1549295774647887\n",
      "Training network. lr: 0.000250. clip: 0.099865\n",
      "Iteration 46: Policy loss: -0.000802. Value loss: 0.011134. Entropy: 1.359748.\n",
      "Iteration 47: Policy loss: -0.002409. Value loss: 0.010253. Entropy: 1.362523.\n",
      "Iteration 48: Policy loss: 0.000504. Value loss: 0.009640. Entropy: 1.361939.\n",
      "episode: 71   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 227     evaluation reward: 1.1527777777777777\n",
      "episode: 72   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 171     evaluation reward: 1.1369863013698631\n",
      "episode: 73   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 262     evaluation reward: 1.1486486486486487\n",
      "episode: 74   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 266     evaluation reward: 1.16\n",
      "Training network. lr: 0.000250. clip: 0.099856\n",
      "Iteration 49: Policy loss: -0.000019. Value loss: 0.023960. Entropy: 1.364008.\n",
      "Iteration 50: Policy loss: 0.000743. Value loss: 0.022615. Entropy: 1.364441.\n",
      "Iteration 51: Policy loss: -0.000952. Value loss: 0.022557. Entropy: 1.364432.\n",
      "episode: 75   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 222     evaluation reward: 1.1578947368421053\n",
      "episode: 76   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 278     evaluation reward: 1.1688311688311688\n",
      "episode: 77   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 183     evaluation reward: 1.1538461538461537\n",
      "episode: 78   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 167     evaluation reward: 1.139240506329114\n",
      "episode: 79   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 249     evaluation reward: 1.15\n",
      "Training network. lr: 0.000250. clip: 0.099847\n",
      "Iteration 52: Policy loss: 0.000049. Value loss: 0.015644. Entropy: 1.358019.\n",
      "Iteration 53: Policy loss: -0.001229. Value loss: 0.014524. Entropy: 1.356569.\n",
      "Iteration 54: Policy loss: -0.002848. Value loss: 0.013238. Entropy: 1.358486.\n",
      "episode: 80   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 215     evaluation reward: 1.1481481481481481\n",
      "episode: 81   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 259     evaluation reward: 1.1585365853658536\n",
      "episode: 82   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 163     evaluation reward: 1.144578313253012\n",
      "episode: 83   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 227     evaluation reward: 1.1428571428571428\n",
      "Training network. lr: 0.000250. clip: 0.099838\n",
      "Iteration 55: Policy loss: -0.000369. Value loss: 0.024019. Entropy: 1.363370.\n",
      "Iteration 56: Policy loss: -0.001182. Value loss: 0.021107. Entropy: 1.363452.\n",
      "Iteration 57: Policy loss: -0.000983. Value loss: 0.018804. Entropy: 1.363619.\n",
      "episode: 84   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 254     evaluation reward: 1.1529411764705881\n",
      "episode: 85   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 210     evaluation reward: 1.1511627906976745\n",
      "episode: 86   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 210     evaluation reward: 1.1494252873563218\n",
      "episode: 87   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 177     evaluation reward: 1.1363636363636365\n",
      "episode: 88   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 400     evaluation reward: 1.1685393258426966\n",
      "Training network. lr: 0.000250. clip: 0.099829\n",
      "Iteration 58: Policy loss: -0.002260. Value loss: 0.023488. Entropy: 1.352371.\n",
      "Iteration 59: Policy loss: 0.000040. Value loss: 0.020316. Entropy: 1.351796.\n",
      "Iteration 60: Policy loss: -0.002673. Value loss: 0.019696. Entropy: 1.355409.\n",
      "episode: 89   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 175     evaluation reward: 1.1555555555555554\n",
      "episode: 90   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 269     evaluation reward: 1.164835164835165\n",
      "episode: 91   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 183     evaluation reward: 1.1521739130434783\n",
      "episode: 92   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 208     evaluation reward: 1.1505376344086022\n",
      "Training network. lr: 0.000250. clip: 0.099820\n",
      "Iteration 61: Policy loss: 0.001417. Value loss: 0.022733. Entropy: 1.352974.\n",
      "Iteration 62: Policy loss: 0.000611. Value loss: 0.021655. Entropy: 1.361535.\n",
      "Iteration 63: Policy loss: -0.001447. Value loss: 0.020513. Entropy: 1.359370.\n",
      "episode: 93   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 338     evaluation reward: 1.1702127659574468\n",
      "episode: 94   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 226     evaluation reward: 1.168421052631579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 95   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 237     evaluation reward: 1.1666666666666667\n",
      "episode: 96   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 167     evaluation reward: 1.1546391752577319\n",
      "episode: 97   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 211     evaluation reward: 1.153061224489796\n",
      "Training network. lr: 0.000250. clip: 0.099811\n",
      "Iteration 64: Policy loss: 0.000791. Value loss: 0.015426. Entropy: 1.355216.\n",
      "Iteration 65: Policy loss: 0.001012. Value loss: 0.014380. Entropy: 1.348447.\n",
      "Iteration 66: Policy loss: 0.001699. Value loss: 0.013505. Entropy: 1.358719.\n",
      "episode: 98   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 367     evaluation reward: 1.1818181818181819\n",
      "episode: 99   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 280     evaluation reward: 1.19\n",
      "episode: 100   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 210     evaluation reward: 1.19\n",
      "Training network. lr: 0.000250. clip: 0.099802\n",
      "Iteration 67: Policy loss: -0.000181. Value loss: 0.022679. Entropy: 1.347854.\n",
      "Iteration 68: Policy loss: -0.001593. Value loss: 0.019693. Entropy: 1.343330.\n",
      "Iteration 69: Policy loss: -0.006422. Value loss: 0.017365. Entropy: 1.341288.\n",
      "episode: 101   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 214     evaluation reward: 1.18\n",
      "episode: 102   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 315     evaluation reward: 1.19\n",
      "episode: 103   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 179     evaluation reward: 1.19\n",
      "episode: 104   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 311     evaluation reward: 1.21\n",
      "Training network. lr: 0.000249. clip: 0.099793\n",
      "Iteration 70: Policy loss: 0.002951. Value loss: 0.024355. Entropy: 1.362629.\n",
      "Iteration 71: Policy loss: 0.001255. Value loss: 0.020217. Entropy: 1.357468.\n",
      "Iteration 72: Policy loss: -0.002479. Value loss: 0.018376. Entropy: 1.353116.\n",
      "episode: 105   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 214     evaluation reward: 1.2\n",
      "episode: 106   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 284     evaluation reward: 1.2\n",
      "episode: 107   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 172     evaluation reward: 1.2\n",
      "episode: 108   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 346     evaluation reward: 1.22\n",
      "Training network. lr: 0.000249. clip: 0.099784\n",
      "Iteration 73: Policy loss: -0.002318. Value loss: 0.022192. Entropy: 1.360274.\n",
      "Iteration 74: Policy loss: -0.000051. Value loss: 0.018266. Entropy: 1.363087.\n",
      "Iteration 75: Policy loss: -0.003562. Value loss: 0.016736. Entropy: 1.362292.\n",
      "episode: 109   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 233     evaluation reward: 1.22\n",
      "episode: 110   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 267     evaluation reward: 1.24\n",
      "episode: 111   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 217     evaluation reward: 1.25\n",
      "episode: 112   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 241     evaluation reward: 1.23\n",
      "episode: 113   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 231     evaluation reward: 1.23\n",
      "Training network. lr: 0.000249. clip: 0.099775\n",
      "Iteration 76: Policy loss: 0.002823. Value loss: 0.020826. Entropy: 1.343366.\n",
      "Iteration 77: Policy loss: -0.000451. Value loss: 0.018291. Entropy: 1.338534.\n",
      "Iteration 78: Policy loss: 0.000790. Value loss: 0.016988. Entropy: 1.342788.\n",
      "episode: 114   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 249     evaluation reward: 1.21\n",
      "episode: 115   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 226     evaluation reward: 1.21\n",
      "episode: 116   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 184     evaluation reward: 1.21\n",
      "episode: 117   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 214     evaluation reward: 1.21\n",
      "Training network. lr: 0.000249. clip: 0.099766\n",
      "Iteration 79: Policy loss: -0.000638. Value loss: 0.013903. Entropy: 1.353915.\n",
      "Iteration 80: Policy loss: 0.000229. Value loss: 0.011150. Entropy: 1.359422.\n",
      "Iteration 81: Policy loss: -0.000816. Value loss: 0.010606. Entropy: 1.356183.\n",
      "episode: 118   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 257     evaluation reward: 1.22\n",
      "episode: 119   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 182     evaluation reward: 1.2\n",
      "episode: 120   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 182     evaluation reward: 1.18\n",
      "episode: 121   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 367     evaluation reward: 1.19\n",
      "episode: 122   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 224     evaluation reward: 1.17\n",
      "Training network. lr: 0.000249. clip: 0.099757\n",
      "Iteration 82: Policy loss: 0.002824. Value loss: 0.018686. Entropy: 1.371055.\n",
      "Iteration 83: Policy loss: -0.000044. Value loss: 0.015196. Entropy: 1.365554.\n",
      "Iteration 84: Policy loss: -0.002362. Value loss: 0.013081. Entropy: 1.359596.\n",
      "episode: 123   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 282     evaluation reward: 1.19\n",
      "episode: 124   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 182     evaluation reward: 1.18\n",
      "episode: 125   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 228     evaluation reward: 1.16\n",
      "episode: 126   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 208     evaluation reward: 1.15\n",
      "Training network. lr: 0.000249. clip: 0.099748\n",
      "Iteration 85: Policy loss: 0.002587. Value loss: 0.016819. Entropy: 1.360311.\n",
      "Iteration 86: Policy loss: 0.000178. Value loss: 0.014747. Entropy: 1.362417.\n",
      "Iteration 87: Policy loss: -0.001218. Value loss: 0.013953. Entropy: 1.362971.\n",
      "episode: 127   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 189     evaluation reward: 1.15\n",
      "episode: 128   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 253     evaluation reward: 1.15\n",
      "episode: 129   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 330     evaluation reward: 1.17\n",
      "episode: 130   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 222     evaluation reward: 1.18\n",
      "Training network. lr: 0.000249. clip: 0.099739\n",
      "Iteration 88: Policy loss: 0.004036. Value loss: 0.021695. Entropy: 1.364056.\n",
      "Iteration 89: Policy loss: 0.003246. Value loss: 0.018584. Entropy: 1.365358.\n",
      "Iteration 90: Policy loss: 0.000479. Value loss: 0.016389. Entropy: 1.364365.\n",
      "episode: 131   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 264     evaluation reward: 1.2\n",
      "episode: 132   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 254     evaluation reward: 1.18\n",
      "episode: 133   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 233     evaluation reward: 1.16\n",
      "episode: 134   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 218     evaluation reward: 1.16\n",
      "episode: 135   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 159     evaluation reward: 1.14\n",
      "Training network. lr: 0.000249. clip: 0.099730\n",
      "Iteration 91: Policy loss: -0.000076. Value loss: 0.019864. Entropy: 1.370584.\n",
      "Iteration 92: Policy loss: -0.005700. Value loss: 0.015309. Entropy: 1.369440.\n",
      "Iteration 93: Policy loss: -0.004395. Value loss: 0.013267. Entropy: 1.365645.\n",
      "episode: 136   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 176     evaluation reward: 1.11\n",
      "episode: 137   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 348     evaluation reward: 1.13\n",
      "episode: 138   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 166     evaluation reward: 1.12\n",
      "episode: 139   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 166     evaluation reward: 1.11\n",
      "Training network. lr: 0.000249. clip: 0.099721\n",
      "Iteration 94: Policy loss: 0.001816. Value loss: 0.019467. Entropy: 1.361928.\n",
      "Iteration 95: Policy loss: -0.002449. Value loss: 0.016072. Entropy: 1.360092.\n",
      "Iteration 96: Policy loss: -0.003110. Value loss: 0.014365. Entropy: 1.359852.\n",
      "episode: 140   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 276     evaluation reward: 1.13\n",
      "episode: 141   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 255     evaluation reward: 1.15\n",
      "episode: 142   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 255     evaluation reward: 1.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 143   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 171     evaluation reward: 1.14\n",
      "episode: 144   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 183     evaluation reward: 1.12\n",
      "Training network. lr: 0.000249. clip: 0.099712\n",
      "Iteration 97: Policy loss: 0.001695. Value loss: 0.014277. Entropy: 1.353512.\n",
      "Iteration 98: Policy loss: -0.003296. Value loss: 0.011550. Entropy: 1.358497.\n",
      "Iteration 99: Policy loss: -0.005294. Value loss: 0.009463. Entropy: 1.354294.\n",
      "episode: 145   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 222     evaluation reward: 1.13\n",
      "episode: 146   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 276     evaluation reward: 1.14\n",
      "episode: 147   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 328     evaluation reward: 1.15\n",
      "episode: 148   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 211     evaluation reward: 1.16\n",
      "Training network. lr: 0.000249. clip: 0.099703\n",
      "Iteration 100: Policy loss: 0.003609. Value loss: 0.023114. Entropy: 1.361226.\n",
      "Iteration 101: Policy loss: -0.000197. Value loss: 0.018974. Entropy: 1.360856.\n",
      "Iteration 102: Policy loss: -0.005423. Value loss: 0.017512. Entropy: 1.363490.\n",
      "episode: 149   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 163     evaluation reward: 1.15\n",
      "episode: 150   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 167     evaluation reward: 1.13\n",
      "episode: 151   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 286     evaluation reward: 1.15\n",
      "episode: 152   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 232     evaluation reward: 1.15\n",
      "episode: 153   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 162     evaluation reward: 1.15\n",
      "Training network. lr: 0.000249. clip: 0.099694\n",
      "Iteration 103: Policy loss: 0.002467. Value loss: 0.015286. Entropy: 1.353400.\n",
      "Iteration 104: Policy loss: -0.000912. Value loss: 0.012849. Entropy: 1.352921.\n",
      "Iteration 105: Policy loss: -0.003442. Value loss: 0.011884. Entropy: 1.354488.\n",
      "episode: 154   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 165     evaluation reward: 1.15\n",
      "episode: 155   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 237     evaluation reward: 1.16\n",
      "episode: 156   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 210     evaluation reward: 1.15\n",
      "episode: 157   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 207     evaluation reward: 1.16\n",
      "episode: 158   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 250     evaluation reward: 1.15\n",
      "Training network. lr: 0.000249. clip: 0.099685\n",
      "Iteration 106: Policy loss: 0.005012. Value loss: 0.017593. Entropy: 1.348925.\n",
      "Iteration 107: Policy loss: -0.001722. Value loss: 0.014559. Entropy: 1.349879.\n",
      "Iteration 108: Policy loss: -0.002962. Value loss: 0.013137. Entropy: 1.349078.\n",
      "episode: 159   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 234     evaluation reward: 1.15\n",
      "episode: 160   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 370     evaluation reward: 1.17\n",
      "episode: 161   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 253     evaluation reward: 1.19\n",
      "episode: 162   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 223     evaluation reward: 1.17\n",
      "Training network. lr: 0.000249. clip: 0.099676\n",
      "Iteration 109: Policy loss: 0.000360. Value loss: 0.024955. Entropy: 1.354404.\n",
      "Iteration 110: Policy loss: -0.003308. Value loss: 0.019531. Entropy: 1.359309.\n",
      "Iteration 111: Policy loss: -0.005097. Value loss: 0.017922. Entropy: 1.350518.\n",
      "episode: 163   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 250     evaluation reward: 1.19\n",
      "episode: 164   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 172     evaluation reward: 1.18\n",
      "episode: 165   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 207     evaluation reward: 1.19\n",
      "episode: 166   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 243     evaluation reward: 1.17\n",
      "Training network. lr: 0.000249. clip: 0.099667\n",
      "Iteration 112: Policy loss: 0.001185. Value loss: 0.016387. Entropy: 1.359577.\n",
      "Iteration 113: Policy loss: -0.001593. Value loss: 0.013367. Entropy: 1.350992.\n",
      "Iteration 114: Policy loss: -0.003772. Value loss: 0.012496. Entropy: 1.353268.\n",
      "episode: 167   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 176     evaluation reward: 1.17\n",
      "episode: 168   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 322     evaluation reward: 1.2\n",
      "episode: 169   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 181     evaluation reward: 1.2\n",
      "episode: 170   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 184     evaluation reward: 1.2\n",
      "episode: 171   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 236     evaluation reward: 1.2\n",
      "Training network. lr: 0.000249. clip: 0.099658\n",
      "Iteration 115: Policy loss: 0.002987. Value loss: 0.019023. Entropy: 1.346191.\n",
      "Iteration 116: Policy loss: -0.002850. Value loss: 0.015310. Entropy: 1.352829.\n",
      "Iteration 117: Policy loss: -0.007626. Value loss: 0.014199. Entropy: 1.347156.\n",
      "episode: 172   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 353     evaluation reward: 1.23\n",
      "episode: 173   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 229     evaluation reward: 1.22\n",
      "episode: 174   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 386     evaluation reward: 1.24\n",
      "Training network. lr: 0.000249. clip: 0.099649\n",
      "Iteration 118: Policy loss: 0.004141. Value loss: 0.019158. Entropy: 1.348032.\n",
      "Iteration 119: Policy loss: 0.000953. Value loss: 0.015910. Entropy: 1.345578.\n",
      "Iteration 120: Policy loss: -0.003130. Value loss: 0.014349. Entropy: 1.344308.\n",
      "episode: 175   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 218     evaluation reward: 1.24\n",
      "episode: 176   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 205     evaluation reward: 1.23\n",
      "episode: 177   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 220     evaluation reward: 1.24\n",
      "episode: 178   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 206     evaluation reward: 1.25\n",
      "episode: 179   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 186     evaluation reward: 1.23\n",
      "Training network. lr: 0.000249. clip: 0.099640\n",
      "Iteration 121: Policy loss: 0.002337. Value loss: 0.014965. Entropy: 1.346830.\n",
      "Iteration 122: Policy loss: -0.003785. Value loss: 0.012828. Entropy: 1.350371.\n",
      "Iteration 123: Policy loss: -0.002714. Value loss: 0.011160. Entropy: 1.351089.\n",
      "episode: 180   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 228     evaluation reward: 1.23\n",
      "episode: 181   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 238     evaluation reward: 1.23\n",
      "episode: 182   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 294     evaluation reward: 1.25\n",
      "episode: 183   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 227     evaluation reward: 1.25\n",
      "Training network. lr: 0.000249. clip: 0.099631\n",
      "Iteration 124: Policy loss: 0.003774. Value loss: 0.018365. Entropy: 1.352119.\n",
      "Iteration 125: Policy loss: -0.002337. Value loss: 0.013539. Entropy: 1.357204.\n",
      "Iteration 126: Policy loss: -0.004835. Value loss: 0.011451. Entropy: 1.357042.\n",
      "episode: 184   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 365     evaluation reward: 1.27\n",
      "episode: 185   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 372     evaluation reward: 1.3\n",
      "episode: 186   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 236     evaluation reward: 1.3\n",
      "Training network. lr: 0.000249. clip: 0.099622\n",
      "Iteration 127: Policy loss: 0.001174. Value loss: 0.028650. Entropy: 1.347155.\n",
      "Iteration 128: Policy loss: -0.000444. Value loss: 0.021872. Entropy: 1.345960.\n",
      "Iteration 129: Policy loss: -0.005797. Value loss: 0.020421. Entropy: 1.347859.\n",
      "episode: 187   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 315     evaluation reward: 1.33\n",
      "episode: 188   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 206     evaluation reward: 1.3\n",
      "episode: 189   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 183     evaluation reward: 1.3\n",
      "episode: 190   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 284     evaluation reward: 1.3\n",
      "episode: 191   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 234     evaluation reward: 1.31\n",
      "Training network. lr: 0.000249. clip: 0.099613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 130: Policy loss: 0.000856. Value loss: 0.027582. Entropy: 1.358510.\n",
      "Iteration 131: Policy loss: -0.003005. Value loss: 0.018906. Entropy: 1.357869.\n",
      "Iteration 132: Policy loss: -0.003899. Value loss: 0.015984. Entropy: 1.356809.\n",
      "episode: 192   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 177     evaluation reward: 1.3\n",
      "episode: 193   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 449     evaluation reward: 1.32\n",
      "episode: 194   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 291     evaluation reward: 1.33\n",
      "Training network. lr: 0.000249. clip: 0.099604\n",
      "Iteration 133: Policy loss: 0.001044. Value loss: 0.025018. Entropy: 1.365763.\n",
      "Iteration 134: Policy loss: -0.005035. Value loss: 0.019862. Entropy: 1.365883.\n",
      "Iteration 135: Policy loss: -0.007619. Value loss: 0.017555. Entropy: 1.358311.\n",
      "episode: 195   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 214     evaluation reward: 1.33\n",
      "episode: 196   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 347     evaluation reward: 1.36\n",
      "episode: 197   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 267     evaluation reward: 1.36\n",
      "episode: 198   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 305     evaluation reward: 1.34\n",
      "Training network. lr: 0.000249. clip: 0.099595\n",
      "Iteration 136: Policy loss: 0.001389. Value loss: 0.021502. Entropy: 1.364742.\n",
      "Iteration 137: Policy loss: -0.006502. Value loss: 0.018590. Entropy: 1.365417.\n",
      "Iteration 138: Policy loss: -0.008643. Value loss: 0.014447. Entropy: 1.365960.\n",
      "episode: 199   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 241     evaluation reward: 1.33\n",
      "episode: 200   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 260     evaluation reward: 1.33\n",
      "episode: 201   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 250     evaluation reward: 1.33\n",
      "episode: 202   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 215     evaluation reward: 1.31\n",
      "Training network. lr: 0.000249. clip: 0.099586\n",
      "Iteration 139: Policy loss: 0.001650. Value loss: 0.015164. Entropy: 1.358279.\n",
      "Iteration 140: Policy loss: -0.003360. Value loss: 0.011849. Entropy: 1.362679.\n",
      "Iteration 141: Policy loss: -0.004987. Value loss: 0.010372. Entropy: 1.356824.\n",
      "episode: 203   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 218     evaluation reward: 1.32\n",
      "episode: 204   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 187     evaluation reward: 1.29\n",
      "episode: 205   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 276     evaluation reward: 1.3\n",
      "episode: 206   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 164     evaluation reward: 1.28\n",
      "episode: 207   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 248     evaluation reward: 1.29\n",
      "Training network. lr: 0.000249. clip: 0.099577\n",
      "Iteration 142: Policy loss: -0.000422. Value loss: 0.021110. Entropy: 1.362529.\n",
      "Iteration 143: Policy loss: -0.004799. Value loss: 0.015788. Entropy: 1.363961.\n",
      "Iteration 144: Policy loss: -0.007766. Value loss: 0.014030. Entropy: 1.358869.\n",
      "episode: 208   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 217     evaluation reward: 1.27\n",
      "episode: 209   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 192     evaluation reward: 1.26\n",
      "episode: 210   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 177     evaluation reward: 1.24\n",
      "now time :  2018-12-26 12:09:21.906852\n",
      "episode: 211   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 336     evaluation reward: 1.26\n",
      "Training network. lr: 0.000249. clip: 0.099568\n",
      "Iteration 145: Policy loss: -0.001288. Value loss: 0.015737. Entropy: 1.353795.\n",
      "Iteration 146: Policy loss: -0.007318. Value loss: 0.010559. Entropy: 1.345764.\n",
      "Iteration 147: Policy loss: -0.008816. Value loss: 0.009021. Entropy: 1.346608.\n",
      "episode: 212   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 185     evaluation reward: 1.24\n",
      "episode: 213   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 243     evaluation reward: 1.25\n",
      "episode: 214   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 230     evaluation reward: 1.25\n",
      "episode: 215   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 229     evaluation reward: 1.25\n",
      "episode: 216   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 249     evaluation reward: 1.26\n",
      "Training network. lr: 0.000249. clip: 0.099559\n",
      "Iteration 148: Policy loss: 0.001981. Value loss: 0.017407. Entropy: 1.335530.\n",
      "Iteration 149: Policy loss: -0.006525. Value loss: 0.013479. Entropy: 1.331527.\n",
      "Iteration 150: Policy loss: -0.008949. Value loss: 0.011890. Entropy: 1.336971.\n",
      "episode: 217   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 289     evaluation reward: 1.27\n",
      "episode: 218   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 458     evaluation reward: 1.29\n",
      "episode: 219   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 173     evaluation reward: 1.29\n",
      "Training network. lr: 0.000249. clip: 0.099550\n",
      "Iteration 151: Policy loss: 0.000398. Value loss: 0.026224. Entropy: 1.336940.\n",
      "Iteration 152: Policy loss: -0.008668. Value loss: 0.020345. Entropy: 1.336510.\n",
      "Iteration 153: Policy loss: -0.009705. Value loss: 0.018245. Entropy: 1.337175.\n",
      "episode: 220   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 166     evaluation reward: 1.29\n",
      "episode: 221   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 211     evaluation reward: 1.27\n",
      "episode: 222   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 178     evaluation reward: 1.26\n",
      "episode: 223   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 179     evaluation reward: 1.24\n",
      "episode: 224   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 197     evaluation reward: 1.24\n",
      "Training network. lr: 0.000249. clip: 0.099541\n",
      "Iteration 154: Policy loss: 0.003183. Value loss: 0.012222. Entropy: 1.336334.\n",
      "Iteration 155: Policy loss: -0.003235. Value loss: 0.009874. Entropy: 1.338183.\n",
      "Iteration 156: Policy loss: -0.005708. Value loss: 0.010183. Entropy: 1.332222.\n",
      "episode: 225   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 245     evaluation reward: 1.25\n",
      "episode: 226   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 276     evaluation reward: 1.26\n",
      "episode: 227   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 245     evaluation reward: 1.27\n",
      "episode: 228   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 256     evaluation reward: 1.27\n",
      "Training network. lr: 0.000249. clip: 0.099532\n",
      "Iteration 157: Policy loss: 0.004524. Value loss: 0.021871. Entropy: 1.336502.\n",
      "Iteration 158: Policy loss: -0.000646. Value loss: 0.016844. Entropy: 1.346769.\n",
      "Iteration 159: Policy loss: -0.005959. Value loss: 0.015108. Entropy: 1.342481.\n",
      "episode: 229   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 255     evaluation reward: 1.26\n",
      "episode: 230   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 236     evaluation reward: 1.26\n",
      "episode: 231   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 335     evaluation reward: 1.27\n",
      "episode: 232   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 181     evaluation reward: 1.25\n",
      "episode: 233   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 169     evaluation reward: 1.24\n",
      "Training network. lr: 0.000249. clip: 0.099523\n",
      "Iteration 160: Policy loss: 0.001563. Value loss: 0.019198. Entropy: 1.347112.\n",
      "Iteration 161: Policy loss: -0.003521. Value loss: 0.015045. Entropy: 1.353273.\n",
      "Iteration 162: Policy loss: -0.003941. Value loss: 0.012945. Entropy: 1.348480.\n",
      "episode: 234   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 233     evaluation reward: 1.24\n",
      "episode: 235   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 238     evaluation reward: 1.25\n",
      "episode: 236   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 220     evaluation reward: 1.26\n",
      "episode: 237   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 218     evaluation reward: 1.24\n",
      "Training network. lr: 0.000249. clip: 0.099514\n",
      "Iteration 163: Policy loss: 0.003338. Value loss: 0.015320. Entropy: 1.356884.\n",
      "Iteration 164: Policy loss: -0.000540. Value loss: 0.011770. Entropy: 1.355997.\n",
      "Iteration 165: Policy loss: -0.008007. Value loss: 0.010227. Entropy: 1.360651.\n",
      "episode: 238   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 216     evaluation reward: 1.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 239   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 184     evaluation reward: 1.25\n",
      "episode: 240   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 220     evaluation reward: 1.24\n",
      "episode: 241   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 294     evaluation reward: 1.24\n",
      "Training network. lr: 0.000249. clip: 0.099505\n",
      "Iteration 166: Policy loss: 0.002892. Value loss: 0.016123. Entropy: 1.356560.\n",
      "Iteration 167: Policy loss: -0.004986. Value loss: 0.013055. Entropy: 1.353376.\n",
      "Iteration 168: Policy loss: -0.010098. Value loss: 0.011506. Entropy: 1.354755.\n",
      "episode: 242   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 394     evaluation reward: 1.26\n",
      "episode: 243   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 322     evaluation reward: 1.29\n",
      "episode: 244   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 306     evaluation reward: 1.31\n",
      "episode: 245   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 249     evaluation reward: 1.32\n",
      "Training network. lr: 0.000249. clip: 0.099496\n",
      "Iteration 169: Policy loss: 0.002803. Value loss: 0.024971. Entropy: 1.338099.\n",
      "Iteration 170: Policy loss: -0.005151. Value loss: 0.019359. Entropy: 1.336255.\n",
      "Iteration 171: Policy loss: -0.010235. Value loss: 0.015987. Entropy: 1.336152.\n",
      "episode: 246   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 338     evaluation reward: 1.33\n",
      "episode: 247   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 183     evaluation reward: 1.3\n",
      "episode: 248   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 169     evaluation reward: 1.29\n",
      "Training network. lr: 0.000249. clip: 0.099487\n",
      "Iteration 172: Policy loss: 0.004421. Value loss: 0.019933. Entropy: 1.349586.\n",
      "Iteration 173: Policy loss: -0.004695. Value loss: 0.015072. Entropy: 1.344004.\n",
      "Iteration 174: Policy loss: -0.009180. Value loss: 0.013281. Entropy: 1.346147.\n",
      "episode: 249   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 400     evaluation reward: 1.33\n",
      "episode: 250   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 258     evaluation reward: 1.35\n",
      "episode: 251   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 216     evaluation reward: 1.34\n",
      "episode: 252   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 242     evaluation reward: 1.35\n",
      "episode: 253   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 205     evaluation reward: 1.36\n",
      "Training network. lr: 0.000249. clip: 0.099478\n",
      "Iteration 175: Policy loss: 0.001495. Value loss: 0.022594. Entropy: 1.339756.\n",
      "Iteration 176: Policy loss: -0.005007. Value loss: 0.018703. Entropy: 1.335859.\n",
      "Iteration 177: Policy loss: -0.009176. Value loss: 0.014824. Entropy: 1.335404.\n",
      "episode: 254   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 281     evaluation reward: 1.38\n",
      "episode: 255   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 269     evaluation reward: 1.39\n",
      "episode: 256   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 191     evaluation reward: 1.38\n",
      "episode: 257   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 236     evaluation reward: 1.38\n",
      "Training network. lr: 0.000249. clip: 0.099469\n",
      "Iteration 178: Policy loss: 0.003055. Value loss: 0.017839. Entropy: 1.339947.\n",
      "Iteration 179: Policy loss: -0.000724. Value loss: 0.013634. Entropy: 1.341873.\n",
      "Iteration 180: Policy loss: -0.008035. Value loss: 0.011903. Entropy: 1.341867.\n",
      "episode: 258   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 213     evaluation reward: 1.38\n",
      "episode: 259   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 195     evaluation reward: 1.37\n",
      "episode: 260   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 360     evaluation reward: 1.37\n",
      "episode: 261   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 211     evaluation reward: 1.36\n",
      "Training network. lr: 0.000249. clip: 0.099460\n",
      "Iteration 181: Policy loss: 0.003497. Value loss: 0.016776. Entropy: 1.325118.\n",
      "Iteration 182: Policy loss: -0.006902. Value loss: 0.014129. Entropy: 1.324169.\n",
      "Iteration 183: Policy loss: -0.009877. Value loss: 0.012044. Entropy: 1.320664.\n",
      "episode: 262   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 223     evaluation reward: 1.36\n",
      "episode: 263   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 234     evaluation reward: 1.35\n",
      "episode: 264   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 175     evaluation reward: 1.35\n",
      "episode: 265   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 207     evaluation reward: 1.35\n",
      "episode: 266   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 196     evaluation reward: 1.35\n",
      "Training network. lr: 0.000249. clip: 0.099451\n",
      "Iteration 184: Policy loss: 0.002883. Value loss: 0.012941. Entropy: 1.313849.\n",
      "Iteration 185: Policy loss: -0.005469. Value loss: 0.009603. Entropy: 1.304424.\n",
      "Iteration 186: Policy loss: -0.013412. Value loss: 0.007833. Entropy: 1.297484.\n",
      "episode: 267   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 295     evaluation reward: 1.38\n",
      "episode: 268   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 176     evaluation reward: 1.35\n",
      "episode: 269   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 289     evaluation reward: 1.37\n",
      "episode: 270   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 243     evaluation reward: 1.39\n",
      "Training network. lr: 0.000249. clip: 0.099442\n",
      "Iteration 187: Policy loss: 0.004416. Value loss: 0.018727. Entropy: 1.318759.\n",
      "Iteration 188: Policy loss: -0.000634. Value loss: 0.012474. Entropy: 1.330909.\n",
      "Iteration 189: Policy loss: -0.005425. Value loss: 0.011453. Entropy: 1.320354.\n",
      "episode: 271   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 297     evaluation reward: 1.41\n",
      "episode: 272   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 207     evaluation reward: 1.39\n",
      "episode: 273   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 309     evaluation reward: 1.41\n",
      "episode: 274   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 243     evaluation reward: 1.39\n",
      "Training network. lr: 0.000249. clip: 0.099433\n",
      "Iteration 190: Policy loss: 0.001385. Value loss: 0.018519. Entropy: 1.322282.\n",
      "Iteration 191: Policy loss: -0.006353. Value loss: 0.012895. Entropy: 1.324159.\n",
      "Iteration 192: Policy loss: -0.013019. Value loss: 0.011095. Entropy: 1.319108.\n",
      "episode: 275   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 390     evaluation reward: 1.42\n",
      "episode: 276   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 207     evaluation reward: 1.42\n",
      "episode: 277   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 222     evaluation reward: 1.42\n",
      "episode: 278   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 213     evaluation reward: 1.42\n",
      "Training network. lr: 0.000249. clip: 0.099424\n",
      "Iteration 193: Policy loss: 0.005351. Value loss: 0.011784. Entropy: 1.329900.\n",
      "Iteration 194: Policy loss: -0.004818. Value loss: 0.009967. Entropy: 1.329642.\n",
      "Iteration 195: Policy loss: -0.006001. Value loss: 0.009404. Entropy: 1.326580.\n",
      "episode: 279   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 218     evaluation reward: 1.42\n",
      "episode: 280   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 214     evaluation reward: 1.42\n",
      "episode: 281   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 380     evaluation reward: 1.44\n",
      "episode: 282   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 208     evaluation reward: 1.43\n",
      "Training network. lr: 0.000249. clip: 0.099415\n",
      "Iteration 196: Policy loss: 0.003447. Value loss: 0.019279. Entropy: 1.333177.\n",
      "Iteration 197: Policy loss: -0.005784. Value loss: 0.014194. Entropy: 1.336812.\n",
      "Iteration 198: Policy loss: -0.009453. Value loss: 0.011920. Entropy: 1.328796.\n",
      "episode: 283   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 278     evaluation reward: 1.44\n",
      "episode: 284   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 339     evaluation reward: 1.43\n",
      "episode: 285   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 273     evaluation reward: 1.41\n",
      "Training network. lr: 0.000249. clip: 0.099406\n",
      "Iteration 199: Policy loss: 0.002330. Value loss: 0.018070. Entropy: 1.334667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200: Policy loss: -0.006313. Value loss: 0.013304. Entropy: 1.335308.\n",
      "Iteration 201: Policy loss: -0.012231. Value loss: 0.011435. Entropy: 1.332890.\n",
      "episode: 286   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 343     evaluation reward: 1.44\n",
      "episode: 287   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 270     evaluation reward: 1.43\n",
      "episode: 288   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 256     evaluation reward: 1.44\n",
      "episode: 289   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 312     evaluation reward: 1.47\n",
      "Training network. lr: 0.000248. clip: 0.099397\n",
      "Iteration 202: Policy loss: 0.005933. Value loss: 0.022496. Entropy: 1.330503.\n",
      "Iteration 203: Policy loss: -0.002463. Value loss: 0.017232. Entropy: 1.329263.\n",
      "Iteration 204: Policy loss: -0.008245. Value loss: 0.013967. Entropy: 1.326074.\n",
      "episode: 290   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 274     evaluation reward: 1.47\n",
      "episode: 291   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 307     evaluation reward: 1.49\n",
      "episode: 292   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 239     evaluation reward: 1.51\n",
      "episode: 293   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 222     evaluation reward: 1.47\n",
      "Training network. lr: 0.000248. clip: 0.099388\n",
      "Iteration 205: Policy loss: 0.004862. Value loss: 0.021815. Entropy: 1.324966.\n",
      "Iteration 206: Policy loss: -0.009060. Value loss: 0.016825. Entropy: 1.320264.\n",
      "Iteration 207: Policy loss: -0.015368. Value loss: 0.013803. Entropy: 1.319492.\n",
      "episode: 294   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 418     evaluation reward: 1.49\n",
      "episode: 295   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 263     evaluation reward: 1.5\n",
      "episode: 296   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 293     evaluation reward: 1.49\n",
      "Training network. lr: 0.000248. clip: 0.099379\n",
      "Iteration 208: Policy loss: 0.002422. Value loss: 0.021461. Entropy: 1.330141.\n",
      "Iteration 209: Policy loss: -0.004830. Value loss: 0.014760. Entropy: 1.327214.\n",
      "Iteration 210: Policy loss: -0.014099. Value loss: 0.013131. Entropy: 1.326444.\n",
      "episode: 297   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 317     evaluation reward: 1.51\n",
      "episode: 298   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 183     evaluation reward: 1.49\n",
      "episode: 299   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 273     evaluation reward: 1.5\n",
      "episode: 300   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 258     evaluation reward: 1.51\n",
      "Training network. lr: 0.000248. clip: 0.099370\n",
      "Iteration 211: Policy loss: 0.004533. Value loss: 0.020070. Entropy: 1.324660.\n",
      "Iteration 212: Policy loss: -0.003110. Value loss: 0.014250. Entropy: 1.321631.\n",
      "Iteration 213: Policy loss: -0.007585. Value loss: 0.013619. Entropy: 1.323421.\n",
      "episode: 301   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 220     evaluation reward: 1.51\n",
      "episode: 302   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 242     evaluation reward: 1.51\n",
      "episode: 303   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 367     evaluation reward: 1.54\n",
      "Training network. lr: 0.000248. clip: 0.099361\n",
      "Iteration 214: Policy loss: 0.004684. Value loss: 0.018758. Entropy: 1.308946.\n",
      "Iteration 215: Policy loss: -0.011380. Value loss: 0.013746. Entropy: 1.307697.\n",
      "Iteration 216: Policy loss: -0.017849. Value loss: 0.011402. Entropy: 1.306676.\n",
      "episode: 304   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 340     evaluation reward: 1.58\n",
      "episode: 305   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 237     evaluation reward: 1.58\n",
      "episode: 306   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 199     evaluation reward: 1.58\n",
      "episode: 307   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 525     evaluation reward: 1.63\n",
      "Training network. lr: 0.000248. clip: 0.099352\n",
      "Iteration 217: Policy loss: 0.002276. Value loss: 0.038288. Entropy: 1.308215.\n",
      "Iteration 218: Policy loss: -0.004099. Value loss: 0.025724. Entropy: 1.310438.\n",
      "Iteration 219: Policy loss: -0.010526. Value loss: 0.022064. Entropy: 1.308833.\n",
      "episode: 308   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 276     evaluation reward: 1.64\n",
      "episode: 309   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 305     evaluation reward: 1.66\n",
      "episode: 310   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 402     evaluation reward: 1.7\n",
      "Training network. lr: 0.000248. clip: 0.099343\n",
      "Iteration 220: Policy loss: 0.002388. Value loss: 0.023230. Entropy: 1.313386.\n",
      "Iteration 221: Policy loss: -0.009807. Value loss: 0.017533. Entropy: 1.313240.\n",
      "Iteration 222: Policy loss: -0.017704. Value loss: 0.015165. Entropy: 1.304692.\n",
      "episode: 311   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 264     evaluation reward: 1.69\n",
      "episode: 312   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 286     evaluation reward: 1.71\n",
      "episode: 313   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 232     evaluation reward: 1.7\n",
      "episode: 314   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 201     evaluation reward: 1.7\n",
      "Training network. lr: 0.000248. clip: 0.099334\n",
      "Iteration 223: Policy loss: -0.001482. Value loss: 0.018622. Entropy: 1.319900.\n",
      "Iteration 224: Policy loss: -0.009165. Value loss: 0.013985. Entropy: 1.318976.\n",
      "Iteration 225: Policy loss: -0.012806. Value loss: 0.012135. Entropy: 1.316383.\n",
      "episode: 315   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 271     evaluation reward: 1.71\n",
      "episode: 316   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 205     evaluation reward: 1.71\n",
      "episode: 317   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 247     evaluation reward: 1.7\n",
      "episode: 318   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 170     evaluation reward: 1.66\n",
      "episode: 319   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 244     evaluation reward: 1.67\n",
      "Training network. lr: 0.000248. clip: 0.099325\n",
      "Iteration 226: Policy loss: 0.001947. Value loss: 0.014818. Entropy: 1.321627.\n",
      "Iteration 227: Policy loss: -0.004627. Value loss: 0.011349. Entropy: 1.320135.\n",
      "Iteration 228: Policy loss: -0.012079. Value loss: 0.009453. Entropy: 1.317871.\n",
      "episode: 320   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 234     evaluation reward: 1.68\n",
      "episode: 321   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 279     evaluation reward: 1.69\n",
      "episode: 322   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 340     evaluation reward: 1.72\n",
      "Training network. lr: 0.000248. clip: 0.099316\n",
      "Iteration 229: Policy loss: 0.003298. Value loss: 0.019126. Entropy: 1.326906.\n",
      "Iteration 230: Policy loss: -0.006467. Value loss: 0.013793. Entropy: 1.327005.\n",
      "Iteration 231: Policy loss: -0.015036. Value loss: 0.011469. Entropy: 1.322034.\n",
      "episode: 323   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 225     evaluation reward: 1.73\n",
      "episode: 324   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 212     evaluation reward: 1.74\n",
      "episode: 325   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 215     evaluation reward: 1.73\n",
      "episode: 326   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 380     evaluation reward: 1.75\n",
      "Training network. lr: 0.000248. clip: 0.099307\n",
      "Iteration 232: Policy loss: 0.005122. Value loss: 0.019156. Entropy: 1.320076.\n",
      "Iteration 233: Policy loss: -0.003115. Value loss: 0.014399. Entropy: 1.318838.\n",
      "Iteration 234: Policy loss: -0.011020. Value loss: 0.012004. Entropy: 1.324080.\n",
      "episode: 327   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 341     evaluation reward: 1.77\n",
      "episode: 328   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 602     evaluation reward: 1.83\n",
      "episode: 329   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 242     evaluation reward: 1.82\n",
      "Training network. lr: 0.000248. clip: 0.099298\n",
      "Iteration 235: Policy loss: 0.002966. Value loss: 0.040317. Entropy: 1.321468.\n",
      "Iteration 236: Policy loss: -0.003908. Value loss: 0.028517. Entropy: 1.314862.\n",
      "Iteration 237: Policy loss: -0.007774. Value loss: 0.023697. Entropy: 1.312096.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 330   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 215     evaluation reward: 1.82\n",
      "episode: 331   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 205     evaluation reward: 1.8\n",
      "episode: 332   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 211     evaluation reward: 1.81\n",
      "episode: 333   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 231     evaluation reward: 1.82\n",
      "Training network. lr: 0.000248. clip: 0.099289\n",
      "Iteration 238: Policy loss: -0.000774. Value loss: 0.010376. Entropy: 1.314741.\n",
      "Iteration 239: Policy loss: -0.009184. Value loss: 0.007226. Entropy: 1.306343.\n",
      "Iteration 240: Policy loss: -0.017871. Value loss: 0.006412. Entropy: 1.303159.\n",
      "episode: 334   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 314     evaluation reward: 1.84\n",
      "episode: 335   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 220     evaluation reward: 1.84\n",
      "episode: 336   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 213     evaluation reward: 1.84\n",
      "episode: 337   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 309     evaluation reward: 1.86\n",
      "Training network. lr: 0.000248. clip: 0.099280\n",
      "Iteration 241: Policy loss: 0.005330. Value loss: 0.013266. Entropy: 1.301104.\n",
      "Iteration 242: Policy loss: -0.006544. Value loss: 0.009869. Entropy: 1.297430.\n",
      "Iteration 243: Policy loss: -0.012572. Value loss: 0.008941. Entropy: 1.298121.\n",
      "episode: 338   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 309     evaluation reward: 1.88\n",
      "episode: 339   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 277     evaluation reward: 1.9\n",
      "episode: 340   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 208     evaluation reward: 1.9\n",
      "episode: 341   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 374     evaluation reward: 1.91\n",
      "Training network. lr: 0.000248. clip: 0.099271\n",
      "Iteration 244: Policy loss: 0.003873. Value loss: 0.015179. Entropy: 1.279945.\n",
      "Iteration 245: Policy loss: -0.005425. Value loss: 0.012454. Entropy: 1.278436.\n",
      "Iteration 246: Policy loss: -0.012390. Value loss: 0.010433. Entropy: 1.278679.\n",
      "episode: 342   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 227     evaluation reward: 1.88\n",
      "episode: 343   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 421     evaluation reward: 1.89\n",
      "episode: 344   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 253     evaluation reward: 1.89\n",
      "Training network. lr: 0.000248. clip: 0.099262\n",
      "Iteration 247: Policy loss: 0.007207. Value loss: 0.032336. Entropy: 1.285704.\n",
      "Iteration 248: Policy loss: -0.002444. Value loss: 0.020331. Entropy: 1.276629.\n",
      "Iteration 249: Policy loss: -0.012370. Value loss: 0.015364. Entropy: 1.272347.\n",
      "episode: 345   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 356     evaluation reward: 1.9\n",
      "episode: 346   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 181     evaluation reward: 1.87\n",
      "episode: 347   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 359     evaluation reward: 1.9\n",
      "Training network. lr: 0.000248. clip: 0.099253\n",
      "Iteration 250: Policy loss: 0.004705. Value loss: 0.018284. Entropy: 1.297606.\n",
      "Iteration 251: Policy loss: -0.007862. Value loss: 0.014045. Entropy: 1.293633.\n",
      "Iteration 252: Policy loss: -0.013848. Value loss: 0.011880. Entropy: 1.290217.\n",
      "episode: 348   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 278     evaluation reward: 1.92\n",
      "episode: 349   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 275     evaluation reward: 1.9\n",
      "episode: 350   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 205     evaluation reward: 1.89\n",
      "episode: 351   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 250     evaluation reward: 1.9\n",
      "episode: 352   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 211     evaluation reward: 1.89\n",
      "Training network. lr: 0.000248. clip: 0.099244\n",
      "Iteration 253: Policy loss: 0.003251. Value loss: 0.013741. Entropy: 1.275905.\n",
      "Iteration 254: Policy loss: -0.009293. Value loss: 0.011161. Entropy: 1.276588.\n",
      "Iteration 255: Policy loss: -0.016091. Value loss: 0.008988. Entropy: 1.281047.\n",
      "episode: 353   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 375     evaluation reward: 1.92\n",
      "episode: 354   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 296     evaluation reward: 1.92\n",
      "episode: 355   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 235     evaluation reward: 1.91\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 256: Policy loss: 0.006415. Value loss: 0.019278. Entropy: 1.285793.\n",
      "Iteration 257: Policy loss: -0.008142. Value loss: 0.014436. Entropy: 1.292996.\n",
      "Iteration 258: Policy loss: -0.014171. Value loss: 0.011124. Entropy: 1.288230.\n",
      "episode: 356   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 432     evaluation reward: 1.96\n",
      "episode: 357   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 247     evaluation reward: 1.96\n",
      "episode: 358   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 520     evaluation reward: 2.02\n",
      "Training network. lr: 0.000248. clip: 0.099226\n",
      "Iteration 259: Policy loss: 0.006243. Value loss: 0.036175. Entropy: 1.276818.\n",
      "Iteration 260: Policy loss: -0.009960. Value loss: 0.025251. Entropy: 1.267402.\n",
      "Iteration 261: Policy loss: -0.015733. Value loss: 0.018903. Entropy: 1.268595.\n",
      "episode: 359   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 172     evaluation reward: 2.02\n",
      "episode: 360   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 172     evaluation reward: 1.99\n",
      "episode: 361   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 323     evaluation reward: 2.0\n",
      "episode: 362   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 274     evaluation reward: 2.01\n",
      "Training network. lr: 0.000248. clip: 0.099217\n",
      "Iteration 262: Policy loss: 0.000391. Value loss: 0.015816. Entropy: 1.305641.\n",
      "Iteration 263: Policy loss: -0.009147. Value loss: 0.012412. Entropy: 1.298276.\n",
      "Iteration 264: Policy loss: -0.018957. Value loss: 0.010176. Entropy: 1.293349.\n",
      "episode: 363   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 206     evaluation reward: 2.01\n",
      "episode: 364   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 215     evaluation reward: 2.02\n",
      "episode: 365   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 481     evaluation reward: 2.07\n",
      "Training network. lr: 0.000248. clip: 0.099208\n",
      "Iteration 265: Policy loss: 0.002906. Value loss: 0.021626. Entropy: 1.331831.\n",
      "Iteration 266: Policy loss: -0.005529. Value loss: 0.014506. Entropy: 1.317968.\n",
      "Iteration 267: Policy loss: -0.011457. Value loss: 0.012355. Entropy: 1.317405.\n",
      "episode: 366   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 396     evaluation reward: 2.1\n",
      "episode: 367   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 231     evaluation reward: 2.08\n",
      "episode: 368   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 213     evaluation reward: 2.09\n",
      "episode: 369   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 182     evaluation reward: 2.07\n",
      "Training network. lr: 0.000248. clip: 0.099199\n",
      "Iteration 268: Policy loss: 0.001456. Value loss: 0.025021. Entropy: 1.281883.\n",
      "Iteration 269: Policy loss: -0.010665. Value loss: 0.017408. Entropy: 1.281988.\n",
      "Iteration 270: Policy loss: -0.014754. Value loss: 0.013917. Entropy: 1.275746.\n",
      "episode: 370   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 343     evaluation reward: 2.08\n",
      "episode: 371   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 309     evaluation reward: 2.07\n",
      "episode: 372   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 294     evaluation reward: 2.08\n",
      "episode: 373   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 211     evaluation reward: 2.06\n",
      "Training network. lr: 0.000248. clip: 0.099190\n",
      "Iteration 271: Policy loss: 0.002824. Value loss: 0.019082. Entropy: 1.290897.\n",
      "Iteration 272: Policy loss: -0.008909. Value loss: 0.014621. Entropy: 1.283168.\n",
      "Iteration 273: Policy loss: -0.016865. Value loss: 0.012309. Entropy: 1.287222.\n",
      "episode: 374   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 313     evaluation reward: 2.07\n",
      "episode: 375   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 288     evaluation reward: 2.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 376   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 279     evaluation reward: 2.06\n",
      "Training network. lr: 0.000248. clip: 0.099181\n",
      "Iteration 274: Policy loss: 0.002666. Value loss: 0.015027. Entropy: 1.295640.\n",
      "Iteration 275: Policy loss: -0.013597. Value loss: 0.011367. Entropy: 1.299512.\n",
      "Iteration 276: Policy loss: -0.020172. Value loss: 0.008674. Entropy: 1.292650.\n",
      "episode: 377   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 271     evaluation reward: 2.07\n",
      "episode: 378   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 252     evaluation reward: 2.08\n",
      "episode: 379   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 204     evaluation reward: 2.08\n",
      "episode: 380   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 226     evaluation reward: 2.08\n",
      "Training network. lr: 0.000248. clip: 0.099172\n",
      "Iteration 277: Policy loss: 0.005054. Value loss: 0.021686. Entropy: 1.291196.\n",
      "Iteration 278: Policy loss: -0.007048. Value loss: 0.016108. Entropy: 1.292662.\n",
      "Iteration 279: Policy loss: -0.016566. Value loss: 0.013019. Entropy: 1.289417.\n",
      "episode: 381   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 342     evaluation reward: 2.08\n",
      "episode: 382   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 242     evaluation reward: 2.09\n",
      "episode: 383   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 217     evaluation reward: 2.08\n",
      "episode: 384   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 201     evaluation reward: 2.05\n",
      "episode: 385   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 225     evaluation reward: 2.04\n",
      "Training network. lr: 0.000248. clip: 0.099163\n",
      "Iteration 280: Policy loss: 0.004254. Value loss: 0.009722. Entropy: 1.225427.\n",
      "Iteration 281: Policy loss: -0.008303. Value loss: 0.007323. Entropy: 1.227023.\n",
      "Iteration 282: Policy loss: -0.014195. Value loss: 0.006867. Entropy: 1.224966.\n",
      "episode: 386   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 258     evaluation reward: 2.02\n",
      "episode: 387   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 345     evaluation reward: 2.03\n",
      "episode: 388   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 375     evaluation reward: 2.05\n",
      "Training network. lr: 0.000248. clip: 0.099154\n",
      "Iteration 283: Policy loss: 0.008830. Value loss: 0.019867. Entropy: 1.261191.\n",
      "Iteration 284: Policy loss: -0.006285. Value loss: 0.014913. Entropy: 1.261595.\n",
      "Iteration 285: Policy loss: -0.016380. Value loss: 0.012337. Entropy: 1.257898.\n",
      "episode: 389   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 213     evaluation reward: 2.03\n",
      "episode: 390   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 307     evaluation reward: 2.04\n",
      "episode: 391   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 199     evaluation reward: 2.01\n",
      "episode: 392   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 372     evaluation reward: 2.02\n",
      "Training network. lr: 0.000248. clip: 0.099145\n",
      "Iteration 286: Policy loss: 0.007089. Value loss: 0.018192. Entropy: 1.251467.\n",
      "Iteration 287: Policy loss: -0.006350. Value loss: 0.012997. Entropy: 1.243087.\n",
      "Iteration 288: Policy loss: -0.014730. Value loss: 0.011388. Entropy: 1.246392.\n",
      "episode: 393   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 340     evaluation reward: 2.05\n",
      "episode: 394   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 270     evaluation reward: 2.03\n",
      "episode: 395   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 282     evaluation reward: 2.03\n",
      "Training network. lr: 0.000248. clip: 0.099136\n",
      "Iteration 289: Policy loss: 0.010223. Value loss: 0.016060. Entropy: 1.194575.\n",
      "Iteration 290: Policy loss: -0.002487. Value loss: 0.012770. Entropy: 1.194840.\n",
      "Iteration 291: Policy loss: -0.016766. Value loss: 0.010299. Entropy: 1.189821.\n",
      "episode: 396   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 340     evaluation reward: 2.05\n",
      "episode: 397   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 268     evaluation reward: 2.03\n",
      "now time :  2018-12-26 12:14:02.672451\n",
      "episode: 398   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 321     evaluation reward: 2.07\n",
      "episode: 399   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 187     evaluation reward: 2.05\n",
      "Training network. lr: 0.000248. clip: 0.099127\n",
      "Iteration 292: Policy loss: 0.005465. Value loss: 0.014742. Entropy: 1.222235.\n",
      "Iteration 293: Policy loss: -0.008147. Value loss: 0.012158. Entropy: 1.213452.\n",
      "Iteration 294: Policy loss: -0.014860. Value loss: 0.009492. Entropy: 1.220813.\n",
      "episode: 400   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 242     evaluation reward: 2.05\n",
      "episode: 401   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 259     evaluation reward: 2.06\n",
      "episode: 402   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 414     evaluation reward: 2.09\n",
      "Training network. lr: 0.000248. clip: 0.099118\n",
      "Iteration 295: Policy loss: 0.008089. Value loss: 0.019812. Entropy: 1.201918.\n",
      "Iteration 296: Policy loss: -0.002511. Value loss: 0.014017. Entropy: 1.190763.\n",
      "Iteration 297: Policy loss: -0.013607. Value loss: 0.010837. Entropy: 1.199870.\n",
      "episode: 403   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 375     evaluation reward: 2.09\n",
      "episode: 404   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 216     evaluation reward: 2.06\n",
      "episode: 405   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 2.08\n",
      "episode: 406   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 235     evaluation reward: 2.1\n",
      "Training network. lr: 0.000248. clip: 0.099109\n",
      "Iteration 298: Policy loss: 0.007213. Value loss: 0.021783. Entropy: 1.200068.\n",
      "Iteration 299: Policy loss: -0.006427. Value loss: 0.015275. Entropy: 1.202841.\n",
      "Iteration 300: Policy loss: -0.017588. Value loss: 0.012426. Entropy: 1.200742.\n",
      "episode: 407   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 259     evaluation reward: 2.06\n",
      "episode: 408   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 228     evaluation reward: 2.05\n",
      "episode: 409   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 316     evaluation reward: 2.06\n",
      "episode: 410   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 221     evaluation reward: 2.03\n",
      "Training network. lr: 0.000248. clip: 0.099100\n",
      "Iteration 301: Policy loss: 0.003200. Value loss: 0.015470. Entropy: 1.208590.\n",
      "Iteration 302: Policy loss: -0.011725. Value loss: 0.011574. Entropy: 1.202918.\n",
      "Iteration 303: Policy loss: -0.020913. Value loss: 0.009162. Entropy: 1.194364.\n",
      "episode: 411   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 215     evaluation reward: 2.02\n",
      "episode: 412   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 270     evaluation reward: 2.02\n",
      "episode: 413   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 239     evaluation reward: 2.02\n",
      "episode: 414   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 300     evaluation reward: 2.04\n",
      "Training network. lr: 0.000248. clip: 0.099091\n",
      "Iteration 304: Policy loss: 0.003960. Value loss: 0.014528. Entropy: 1.219527.\n",
      "Iteration 305: Policy loss: -0.005695. Value loss: 0.011909. Entropy: 1.228109.\n",
      "Iteration 306: Policy loss: -0.016964. Value loss: 0.009725. Entropy: 1.226204.\n",
      "episode: 415   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 243     evaluation reward: 2.04\n",
      "episode: 416   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 410     evaluation reward: 2.07\n",
      "episode: 417   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 326     evaluation reward: 2.09\n",
      "Training network. lr: 0.000248. clip: 0.099082\n",
      "Iteration 307: Policy loss: 0.005440. Value loss: 0.016938. Entropy: 1.190139.\n",
      "Iteration 308: Policy loss: -0.007931. Value loss: 0.012022. Entropy: 1.185985.\n",
      "Iteration 309: Policy loss: -0.013027. Value loss: 0.011220. Entropy: 1.186145.\n",
      "episode: 418   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 271     evaluation reward: 2.11\n",
      "episode: 419   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 298     evaluation reward: 2.13\n",
      "episode: 420   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 296     evaluation reward: 2.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 421   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 236     evaluation reward: 2.13\n",
      "Training network. lr: 0.000248. clip: 0.099073\n",
      "Iteration 310: Policy loss: 0.009015. Value loss: 0.014376. Entropy: 1.220987.\n",
      "Iteration 311: Policy loss: -0.007546. Value loss: 0.010351. Entropy: 1.225841.\n",
      "Iteration 312: Policy loss: -0.016346. Value loss: 0.007500. Entropy: 1.232211.\n",
      "episode: 422   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 540     evaluation reward: 2.17\n",
      "episode: 423   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 332     evaluation reward: 2.2\n",
      "Training network. lr: 0.000248. clip: 0.099064\n",
      "Iteration 313: Policy loss: 0.015961. Value loss: 0.022654. Entropy: 1.166992.\n",
      "Iteration 314: Policy loss: -0.004754. Value loss: 0.015201. Entropy: 1.169362.\n",
      "Iteration 315: Policy loss: -0.011294. Value loss: 0.013310. Entropy: 1.166469.\n",
      "episode: 424   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 323     evaluation reward: 2.22\n",
      "episode: 425   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 250     evaluation reward: 2.23\n",
      "episode: 426   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 212     evaluation reward: 2.2\n",
      "episode: 427   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 266     evaluation reward: 2.19\n",
      "Training network. lr: 0.000248. clip: 0.099055\n",
      "Iteration 316: Policy loss: 0.005355. Value loss: 0.014174. Entropy: 1.232410.\n",
      "Iteration 317: Policy loss: -0.009790. Value loss: 0.009572. Entropy: 1.236882.\n",
      "Iteration 318: Policy loss: -0.019343. Value loss: 0.007957. Entropy: 1.228450.\n",
      "episode: 428   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 252     evaluation reward: 2.12\n",
      "episode: 429   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 402     evaluation reward: 2.15\n",
      "episode: 430   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 370     evaluation reward: 2.18\n",
      "Training network. lr: 0.000248. clip: 0.099046\n",
      "Iteration 319: Policy loss: 0.004627. Value loss: 0.022108. Entropy: 1.233117.\n",
      "Iteration 320: Policy loss: -0.009872. Value loss: 0.014667. Entropy: 1.231474.\n",
      "Iteration 321: Policy loss: -0.020848. Value loss: 0.011988. Entropy: 1.221700.\n",
      "episode: 431   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 244     evaluation reward: 2.18\n",
      "episode: 432   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 277     evaluation reward: 2.19\n",
      "episode: 433   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 386     evaluation reward: 2.22\n",
      "Training network. lr: 0.000248. clip: 0.099037\n",
      "Iteration 322: Policy loss: 0.007967. Value loss: 0.024582. Entropy: 1.185801.\n",
      "Iteration 323: Policy loss: -0.008667. Value loss: 0.015348. Entropy: 1.181437.\n",
      "Iteration 324: Policy loss: -0.015686. Value loss: 0.012295. Entropy: 1.178380.\n",
      "episode: 434   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 451     evaluation reward: 2.25\n",
      "episode: 435   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 288     evaluation reward: 2.26\n",
      "episode: 436   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 364     evaluation reward: 2.28\n",
      "Training network. lr: 0.000248. clip: 0.099028\n",
      "Iteration 325: Policy loss: 0.013112. Value loss: 0.024979. Entropy: 1.213164.\n",
      "Iteration 326: Policy loss: -0.005942. Value loss: 0.017664. Entropy: 1.205961.\n",
      "Iteration 327: Policy loss: -0.012570. Value loss: 0.013795. Entropy: 1.205084.\n",
      "episode: 437   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 273     evaluation reward: 2.27\n",
      "episode: 438   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 341     evaluation reward: 2.27\n",
      "episode: 439   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 216     evaluation reward: 2.26\n",
      "episode: 440   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 213     evaluation reward: 2.26\n",
      "Training network. lr: 0.000248. clip: 0.099019\n",
      "Iteration 328: Policy loss: 0.004892. Value loss: 0.011265. Entropy: 1.210027.\n",
      "Iteration 329: Policy loss: -0.006893. Value loss: 0.008027. Entropy: 1.204494.\n",
      "Iteration 330: Policy loss: -0.011958. Value loss: 0.006789. Entropy: 1.208560.\n",
      "episode: 441   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 259     evaluation reward: 2.24\n",
      "episode: 442   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 337     evaluation reward: 2.26\n",
      "episode: 443   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 262     evaluation reward: 2.24\n",
      "episode: 444   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 282     evaluation reward: 2.24\n",
      "Training network. lr: 0.000248. clip: 0.099010\n",
      "Iteration 331: Policy loss: 0.004928. Value loss: 0.022594. Entropy: 1.233507.\n",
      "Iteration 332: Policy loss: -0.011134. Value loss: 0.015857. Entropy: 1.233443.\n",
      "Iteration 333: Policy loss: -0.016459. Value loss: 0.011832. Entropy: 1.230366.\n",
      "episode: 445   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 416     evaluation reward: 2.26\n",
      "episode: 446   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 315     evaluation reward: 2.29\n",
      "Training network. lr: 0.000248. clip: 0.099001\n",
      "Iteration 334: Policy loss: 0.011150. Value loss: 0.018619. Entropy: 1.233158.\n",
      "Iteration 335: Policy loss: -0.009994. Value loss: 0.012435. Entropy: 1.232682.\n",
      "Iteration 336: Policy loss: -0.019212. Value loss: 0.010543. Entropy: 1.235382.\n",
      "episode: 447   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 352     evaluation reward: 2.29\n",
      "episode: 448   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 255     evaluation reward: 2.29\n",
      "episode: 449   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 240     evaluation reward: 2.28\n",
      "episode: 450   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 281     evaluation reward: 2.29\n",
      "episode: 451   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 184     evaluation reward: 2.27\n",
      "Training network. lr: 0.000247. clip: 0.098992\n",
      "Iteration 337: Policy loss: 0.010374. Value loss: 0.017557. Entropy: 1.223995.\n",
      "Iteration 338: Policy loss: -0.010124. Value loss: 0.012528. Entropy: 1.212688.\n",
      "Iteration 339: Policy loss: -0.017261. Value loss: 0.010669. Entropy: 1.216156.\n",
      "episode: 452   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 269     evaluation reward: 2.28\n",
      "episode: 453   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 276     evaluation reward: 2.26\n",
      "episode: 454   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 335     evaluation reward: 2.27\n",
      "Training network. lr: 0.000247. clip: 0.098983\n",
      "Iteration 340: Policy loss: 0.008583. Value loss: 0.015001. Entropy: 1.205567.\n",
      "Iteration 341: Policy loss: -0.009261. Value loss: 0.010839. Entropy: 1.189381.\n",
      "Iteration 342: Policy loss: -0.017471. Value loss: 0.008544. Entropy: 1.201372.\n",
      "episode: 455   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 318     evaluation reward: 2.29\n",
      "episode: 456   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 323     evaluation reward: 2.27\n",
      "episode: 457   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 209     evaluation reward: 2.27\n",
      "Training network. lr: 0.000247. clip: 0.098974\n",
      "Iteration 343: Policy loss: 0.002639. Value loss: 0.020489. Entropy: 1.238825.\n",
      "Iteration 344: Policy loss: -0.008279. Value loss: 0.014661. Entropy: 1.229663.\n",
      "Iteration 345: Policy loss: -0.016340. Value loss: 0.012001. Entropy: 1.227299.\n",
      "episode: 458   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 443     evaluation reward: 2.25\n",
      "episode: 459   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 346     evaluation reward: 2.28\n",
      "episode: 460   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 316     evaluation reward: 2.31\n",
      "Training network. lr: 0.000247. clip: 0.098965\n",
      "Iteration 346: Policy loss: 0.011476. Value loss: 0.019900. Entropy: 1.221050.\n",
      "Iteration 347: Policy loss: -0.006034. Value loss: 0.012768. Entropy: 1.214395.\n",
      "Iteration 348: Policy loss: -0.015312. Value loss: 0.010168. Entropy: 1.221608.\n",
      "episode: 461   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 396     evaluation reward: 2.33\n",
      "episode: 462   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 266     evaluation reward: 2.33\n",
      "episode: 463   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 332     evaluation reward: 2.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 464   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 289     evaluation reward: 2.36\n",
      "Training network. lr: 0.000247. clip: 0.098956\n",
      "Iteration 349: Policy loss: 0.007561. Value loss: 0.019898. Entropy: 1.183044.\n",
      "Iteration 350: Policy loss: -0.009371. Value loss: 0.013550. Entropy: 1.176934.\n",
      "Iteration 351: Policy loss: -0.019551. Value loss: 0.011777. Entropy: 1.176751.\n",
      "episode: 465   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 376     evaluation reward: 2.33\n",
      "episode: 466   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 325     evaluation reward: 2.32\n",
      "Training network. lr: 0.000247. clip: 0.098947\n",
      "Iteration 352: Policy loss: 0.004882. Value loss: 0.024058. Entropy: 1.224743.\n",
      "Iteration 353: Policy loss: -0.012262. Value loss: 0.016726. Entropy: 1.223568.\n",
      "Iteration 354: Policy loss: -0.022677. Value loss: 0.013678. Entropy: 1.216209.\n",
      "episode: 467   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 411     evaluation reward: 2.34\n",
      "episode: 468   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 263     evaluation reward: 2.35\n",
      "episode: 469   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 375     evaluation reward: 2.39\n",
      "Training network. lr: 0.000247. clip: 0.098938\n",
      "Iteration 355: Policy loss: 0.009682. Value loss: 0.013155. Entropy: 1.204881.\n",
      "Iteration 356: Policy loss: -0.006532. Value loss: 0.009699. Entropy: 1.211995.\n",
      "Iteration 357: Policy loss: -0.018103. Value loss: 0.008240. Entropy: 1.205947.\n",
      "episode: 470   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 317     evaluation reward: 2.39\n",
      "episode: 471   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 233     evaluation reward: 2.38\n",
      "episode: 472   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 335     evaluation reward: 2.39\n",
      "episode: 473   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 397     evaluation reward: 2.43\n",
      "Training network. lr: 0.000247. clip: 0.098929\n",
      "Iteration 358: Policy loss: 0.004825. Value loss: 0.027220. Entropy: 1.212018.\n",
      "Iteration 359: Policy loss: -0.006161. Value loss: 0.019561. Entropy: 1.208488.\n",
      "Iteration 360: Policy loss: -0.015718. Value loss: 0.016387. Entropy: 1.206942.\n",
      "episode: 474   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 289     evaluation reward: 2.42\n",
      "episode: 475   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 313     evaluation reward: 2.43\n",
      "episode: 476   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 212     evaluation reward: 2.42\n",
      "Training network. lr: 0.000247. clip: 0.098920\n",
      "Iteration 361: Policy loss: 0.001182. Value loss: 0.016759. Entropy: 1.198147.\n",
      "Iteration 362: Policy loss: -0.005362. Value loss: 0.010415. Entropy: 1.190170.\n",
      "Iteration 363: Policy loss: -0.018554. Value loss: 0.008774. Entropy: 1.191910.\n",
      "episode: 477   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 364     evaluation reward: 2.44\n",
      "episode: 478   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 283     evaluation reward: 2.44\n",
      "episode: 479   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 210     evaluation reward: 2.45\n",
      "episode: 480   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 387     evaluation reward: 2.49\n",
      "Training network. lr: 0.000247. clip: 0.098911\n",
      "Iteration 364: Policy loss: 0.005666. Value loss: 0.019147. Entropy: 1.193563.\n",
      "Iteration 365: Policy loss: -0.013293. Value loss: 0.012170. Entropy: 1.206073.\n",
      "Iteration 366: Policy loss: -0.020387. Value loss: 0.009740. Entropy: 1.203998.\n",
      "episode: 481   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 343     evaluation reward: 2.48\n",
      "episode: 482   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 404     evaluation reward: 2.5\n",
      "Training network. lr: 0.000247. clip: 0.098902\n",
      "Iteration 367: Policy loss: 0.005497. Value loss: 0.027491. Entropy: 1.245459.\n",
      "Iteration 368: Policy loss: -0.008141. Value loss: 0.017184. Entropy: 1.241292.\n",
      "Iteration 369: Policy loss: -0.020009. Value loss: 0.013012. Entropy: 1.236352.\n",
      "episode: 483   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 323     evaluation reward: 2.52\n",
      "episode: 484   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 272     evaluation reward: 2.54\n",
      "episode: 485   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 182     evaluation reward: 2.53\n",
      "episode: 486   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 301     evaluation reward: 2.54\n",
      "Training network. lr: 0.000247. clip: 0.098893\n",
      "Iteration 370: Policy loss: 0.005468. Value loss: 0.020813. Entropy: 1.225751.\n",
      "Iteration 371: Policy loss: -0.009548. Value loss: 0.014708. Entropy: 1.219948.\n",
      "Iteration 372: Policy loss: -0.017728. Value loss: 0.012390. Entropy: 1.217830.\n",
      "episode: 487   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 424     evaluation reward: 2.56\n",
      "episode: 488   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 340     evaluation reward: 2.55\n",
      "episode: 489   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 204     evaluation reward: 2.55\n",
      "Training network. lr: 0.000247. clip: 0.098884\n",
      "Iteration 373: Policy loss: 0.001899. Value loss: 0.022870. Entropy: 1.221130.\n",
      "Iteration 374: Policy loss: -0.004717. Value loss: 0.015973. Entropy: 1.218930.\n",
      "Iteration 375: Policy loss: -0.013205. Value loss: 0.012959. Entropy: 1.211961.\n",
      "episode: 490   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 362     evaluation reward: 2.56\n",
      "episode: 491   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 313     evaluation reward: 2.59\n",
      "episode: 492   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 231     evaluation reward: 2.57\n",
      "episode: 493   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 321     evaluation reward: 2.56\n",
      "Training network. lr: 0.000247. clip: 0.098875\n",
      "Iteration 376: Policy loss: 0.010919. Value loss: 0.014785. Entropy: 1.211054.\n",
      "Iteration 377: Policy loss: -0.011308. Value loss: 0.011106. Entropy: 1.203822.\n",
      "Iteration 378: Policy loss: -0.020683. Value loss: 0.009041. Entropy: 1.201406.\n",
      "episode: 494   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 273     evaluation reward: 2.56\n",
      "episode: 495   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 271     evaluation reward: 2.56\n",
      "episode: 496   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 234     evaluation reward: 2.53\n",
      "episode: 497   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 168     evaluation reward: 2.52\n",
      "Training network. lr: 0.000247. clip: 0.098866\n",
      "Iteration 379: Policy loss: 0.006235. Value loss: 0.015838. Entropy: 1.224154.\n",
      "Iteration 380: Policy loss: -0.008330. Value loss: 0.012065. Entropy: 1.225377.\n",
      "Iteration 381: Policy loss: -0.016167. Value loss: 0.010132. Entropy: 1.219814.\n",
      "episode: 498   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 417     evaluation reward: 2.53\n",
      "episode: 499   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 373     evaluation reward: 2.57\n",
      "episode: 500   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 234     evaluation reward: 2.56\n",
      "Training network. lr: 0.000247. clip: 0.098857\n",
      "Iteration 382: Policy loss: 0.004460. Value loss: 0.022656. Entropy: 1.234107.\n",
      "Iteration 383: Policy loss: -0.008939. Value loss: 0.016580. Entropy: 1.217067.\n",
      "Iteration 384: Policy loss: -0.019566. Value loss: 0.013055. Entropy: 1.214756.\n",
      "episode: 501   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 285     evaluation reward: 2.56\n",
      "episode: 502   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 507     evaluation reward: 2.58\n",
      "episode: 503   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 382     evaluation reward: 2.58\n",
      "Training network. lr: 0.000247. clip: 0.098848\n",
      "Iteration 385: Policy loss: 0.006735. Value loss: 0.022590. Entropy: 1.235900.\n",
      "Iteration 386: Policy loss: -0.007477. Value loss: 0.015031. Entropy: 1.237366.\n",
      "Iteration 387: Policy loss: -0.015163. Value loss: 0.012705. Entropy: 1.239970.\n",
      "episode: 504   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 173     evaluation reward: 2.57\n",
      "episode: 505   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 318     evaluation reward: 2.56\n",
      "episode: 506   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 211     evaluation reward: 2.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 507   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 276     evaluation reward: 2.55\n",
      "Training network. lr: 0.000247. clip: 0.098839\n",
      "Iteration 388: Policy loss: 0.004707. Value loss: 0.013374. Entropy: 1.235665.\n",
      "Iteration 389: Policy loss: -0.014416. Value loss: 0.010097. Entropy: 1.224148.\n",
      "Iteration 390: Policy loss: -0.022300. Value loss: 0.008411. Entropy: 1.226563.\n",
      "episode: 508   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 278     evaluation reward: 2.56\n",
      "episode: 509   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 349     evaluation reward: 2.57\n",
      "episode: 510   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 2.62\n",
      "Training network. lr: 0.000247. clip: 0.098830\n",
      "Iteration 391: Policy loss: 0.004065. Value loss: 0.021599. Entropy: 1.204677.\n",
      "Iteration 392: Policy loss: -0.008531. Value loss: 0.015622. Entropy: 1.210770.\n",
      "Iteration 393: Policy loss: -0.015618. Value loss: 0.013236. Entropy: 1.198266.\n",
      "episode: 511   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 287     evaluation reward: 2.64\n",
      "episode: 512   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 255     evaluation reward: 2.64\n",
      "episode: 513   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 266     evaluation reward: 2.64\n",
      "Training network. lr: 0.000247. clip: 0.098821\n",
      "Iteration 394: Policy loss: 0.002758. Value loss: 0.022509. Entropy: 1.204965.\n",
      "Iteration 395: Policy loss: -0.011271. Value loss: 0.016188. Entropy: 1.198061.\n",
      "Iteration 396: Policy loss: -0.018781. Value loss: 0.012707. Entropy: 1.202692.\n",
      "episode: 514   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 379     evaluation reward: 2.65\n",
      "episode: 515   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 287     evaluation reward: 2.66\n",
      "episode: 516   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 443     evaluation reward: 2.67\n",
      "Training network. lr: 0.000247. clip: 0.098812\n",
      "Iteration 397: Policy loss: 0.007291. Value loss: 0.027269. Entropy: 1.188929.\n",
      "Iteration 398: Policy loss: -0.011380. Value loss: 0.019501. Entropy: 1.184653.\n",
      "Iteration 399: Policy loss: -0.020603. Value loss: 0.014651. Entropy: 1.184281.\n",
      "episode: 517   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 353     evaluation reward: 2.68\n",
      "episode: 518   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 340     evaluation reward: 2.69\n",
      "episode: 519   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 292     evaluation reward: 2.68\n",
      "Training network. lr: 0.000247. clip: 0.098803\n",
      "Iteration 400: Policy loss: 0.005050. Value loss: 0.021373. Entropy: 1.171680.\n",
      "Iteration 401: Policy loss: -0.007651. Value loss: 0.015536. Entropy: 1.171589.\n",
      "Iteration 402: Policy loss: -0.022120. Value loss: 0.013100. Entropy: 1.163273.\n",
      "episode: 520   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 389     evaluation reward: 2.7\n",
      "episode: 521   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 251     evaluation reward: 2.71\n",
      "episode: 522   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 274     evaluation reward: 2.66\n",
      "episode: 523   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 211     evaluation reward: 2.63\n",
      "Training network. lr: 0.000247. clip: 0.098794\n",
      "Iteration 403: Policy loss: 0.004638. Value loss: 0.018790. Entropy: 1.165266.\n",
      "Iteration 404: Policy loss: -0.009194. Value loss: 0.013162. Entropy: 1.150362.\n",
      "Iteration 405: Policy loss: -0.018119. Value loss: 0.010159. Entropy: 1.158260.\n",
      "episode: 524   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 355     evaluation reward: 2.64\n",
      "episode: 525   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 319     evaluation reward: 2.64\n",
      "episode: 526   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 223     evaluation reward: 2.64\n",
      "Training network. lr: 0.000247. clip: 0.098785\n",
      "Iteration 406: Policy loss: 0.008507. Value loss: 0.016861. Entropy: 1.209968.\n",
      "Iteration 407: Policy loss: -0.012659. Value loss: 0.012542. Entropy: 1.204556.\n",
      "Iteration 408: Policy loss: -0.022793. Value loss: 0.009693. Entropy: 1.202026.\n",
      "episode: 527   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 251     evaluation reward: 2.63\n",
      "episode: 528   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 360     evaluation reward: 2.66\n",
      "episode: 529   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 269     evaluation reward: 2.64\n",
      "Training network. lr: 0.000247. clip: 0.098776\n",
      "Iteration 409: Policy loss: 0.009038. Value loss: 0.017500. Entropy: 1.188735.\n",
      "Iteration 410: Policy loss: -0.012970. Value loss: 0.010224. Entropy: 1.181812.\n",
      "Iteration 411: Policy loss: -0.024780. Value loss: 0.008237. Entropy: 1.187467.\n",
      "episode: 530   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 433     evaluation reward: 2.65\n",
      "episode: 531   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 359     evaluation reward: 2.67\n",
      "episode: 532   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 267     evaluation reward: 2.67\n",
      "Training network. lr: 0.000247. clip: 0.098767\n",
      "Iteration 412: Policy loss: 0.015618. Value loss: 0.021437. Entropy: 1.191957.\n",
      "Iteration 413: Policy loss: -0.010708. Value loss: 0.014447. Entropy: 1.173382.\n",
      "Iteration 414: Policy loss: -0.024667. Value loss: 0.011695. Entropy: 1.181949.\n",
      "episode: 533   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 411     evaluation reward: 2.68\n",
      "episode: 534   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 337     evaluation reward: 2.65\n",
      "episode: 535   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 313     evaluation reward: 2.66\n",
      "Training network. lr: 0.000247. clip: 0.098758\n",
      "Iteration 415: Policy loss: 0.005696. Value loss: 0.017313. Entropy: 1.210371.\n",
      "Iteration 416: Policy loss: -0.012249. Value loss: 0.012625. Entropy: 1.209109.\n",
      "Iteration 417: Policy loss: -0.023522. Value loss: 0.010103. Entropy: 1.204902.\n",
      "episode: 536   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 428     evaluation reward: 2.67\n",
      "episode: 537   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 427     evaluation reward: 2.69\n",
      "episode: 538   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 369     evaluation reward: 2.7\n",
      "Training network. lr: 0.000247. clip: 0.098749\n",
      "Iteration 418: Policy loss: 0.006123. Value loss: 0.018777. Entropy: 1.247268.\n",
      "Iteration 419: Policy loss: -0.012731. Value loss: 0.014222. Entropy: 1.246957.\n",
      "Iteration 420: Policy loss: -0.020202. Value loss: 0.011228. Entropy: 1.237759.\n",
      "episode: 539   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 249     evaluation reward: 2.7\n",
      "episode: 540   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 517     evaluation reward: 2.77\n",
      "episode: 541   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 208     evaluation reward: 2.77\n",
      "Training network. lr: 0.000247. clip: 0.098740\n",
      "Iteration 421: Policy loss: 0.005238. Value loss: 0.026073. Entropy: 1.205577.\n",
      "Iteration 422: Policy loss: -0.011584. Value loss: 0.019131. Entropy: 1.185853.\n",
      "Iteration 423: Policy loss: -0.019778. Value loss: 0.016297. Entropy: 1.192482.\n",
      "episode: 542   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 455     evaluation reward: 2.8\n",
      "episode: 543   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 396     evaluation reward: 2.82\n",
      "episode: 544   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 242     evaluation reward: 2.81\n",
      "Training network. lr: 0.000247. clip: 0.098731\n",
      "Iteration 424: Policy loss: 0.006031. Value loss: 0.028033. Entropy: 1.233322.\n",
      "Iteration 425: Policy loss: -0.007361. Value loss: 0.019178. Entropy: 1.242965.\n",
      "Iteration 426: Policy loss: -0.017279. Value loss: 0.015027. Entropy: 1.240933.\n",
      "episode: 545   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 370     evaluation reward: 2.8\n",
      "episode: 546   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 421     evaluation reward: 2.82\n",
      "episode: 547   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 278     evaluation reward: 2.81\n",
      "Training network. lr: 0.000247. clip: 0.098722\n",
      "Iteration 427: Policy loss: 0.005941. Value loss: 0.021175. Entropy: 1.232488.\n",
      "Iteration 428: Policy loss: -0.014198. Value loss: 0.013739. Entropy: 1.227589.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 429: Policy loss: -0.022920. Value loss: 0.011876. Entropy: 1.228173.\n",
      "episode: 548   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 360     evaluation reward: 2.83\n",
      "episode: 549   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 377     evaluation reward: 2.86\n",
      "Training network. lr: 0.000247. clip: 0.098713\n",
      "Iteration 430: Policy loss: 0.007654. Value loss: 0.025837. Entropy: 1.214751.\n",
      "Iteration 431: Policy loss: -0.009846. Value loss: 0.016657. Entropy: 1.192993.\n",
      "Iteration 432: Policy loss: -0.017719. Value loss: 0.013763. Entropy: 1.205499.\n",
      "episode: 550   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 318     evaluation reward: 2.87\n",
      "episode: 551   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 321     evaluation reward: 2.9\n",
      "episode: 552   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 504     evaluation reward: 2.94\n",
      "Training network. lr: 0.000247. clip: 0.098704\n",
      "Iteration 433: Policy loss: 0.009740. Value loss: 0.024770. Entropy: 1.230999.\n",
      "Iteration 434: Policy loss: -0.012756. Value loss: 0.017625. Entropy: 1.225484.\n",
      "Iteration 435: Policy loss: -0.021631. Value loss: 0.013501. Entropy: 1.216258.\n",
      "episode: 553   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 327     evaluation reward: 2.95\n",
      "episode: 554   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 180     evaluation reward: 2.92\n",
      "episode: 555   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 271     evaluation reward: 2.91\n",
      "episode: 556   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 329     evaluation reward: 2.91\n",
      "Training network. lr: 0.000247. clip: 0.098695\n",
      "Iteration 436: Policy loss: 0.006051. Value loss: 0.018047. Entropy: 1.226737.\n",
      "Iteration 437: Policy loss: -0.013235. Value loss: 0.011780. Entropy: 1.227335.\n",
      "Iteration 438: Policy loss: -0.025012. Value loss: 0.009855. Entropy: 1.215720.\n",
      "episode: 557   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 217     evaluation reward: 2.91\n",
      "episode: 558   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 279     evaluation reward: 2.88\n",
      "now time :  2018-12-26 12:18:38.687830\n",
      "episode: 559   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 201     evaluation reward: 2.86\n",
      "episode: 560   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 309     evaluation reward: 2.86\n",
      "Training network. lr: 0.000247. clip: 0.098686\n",
      "Iteration 439: Policy loss: 0.007324. Value loss: 0.015431. Entropy: 1.185170.\n",
      "Iteration 440: Policy loss: -0.009208. Value loss: 0.009968. Entropy: 1.177727.\n",
      "Iteration 441: Policy loss: -0.017361. Value loss: 0.007995. Entropy: 1.167457.\n",
      "episode: 561   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 381     evaluation reward: 2.86\n",
      "episode: 562   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 322     evaluation reward: 2.87\n",
      "episode: 563   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 338     evaluation reward: 2.87\n",
      "Training network. lr: 0.000247. clip: 0.098677\n",
      "Iteration 442: Policy loss: 0.002439. Value loss: 0.025119. Entropy: 1.228152.\n",
      "Iteration 443: Policy loss: -0.013056. Value loss: 0.017352. Entropy: 1.228536.\n",
      "Iteration 444: Policy loss: -0.023216. Value loss: 0.014402. Entropy: 1.219462.\n",
      "episode: 564   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 477     evaluation reward: 2.91\n",
      "episode: 565   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 221     evaluation reward: 2.89\n",
      "episode: 566   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 355     evaluation reward: 2.9\n",
      "Training network. lr: 0.000247. clip: 0.098668\n",
      "Iteration 445: Policy loss: 0.008775. Value loss: 0.023751. Entropy: 1.225527.\n",
      "Iteration 446: Policy loss: -0.011691. Value loss: 0.015725. Entropy: 1.223508.\n",
      "Iteration 447: Policy loss: -0.023290. Value loss: 0.012705. Entropy: 1.213331.\n",
      "episode: 567   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 357     evaluation reward: 2.91\n",
      "episode: 568   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 447     evaluation reward: 2.94\n",
      "Training network. lr: 0.000247. clip: 0.098659\n",
      "Iteration 448: Policy loss: 0.009635. Value loss: 0.022467. Entropy: 1.227647.\n",
      "Iteration 449: Policy loss: -0.011460. Value loss: 0.015734. Entropy: 1.227180.\n",
      "Iteration 450: Policy loss: -0.019438. Value loss: 0.012856. Entropy: 1.221165.\n",
      "episode: 569   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 390     evaluation reward: 2.94\n",
      "episode: 570   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 293     evaluation reward: 2.94\n",
      "episode: 571   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 253     evaluation reward: 2.95\n",
      "episode: 572   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 344     evaluation reward: 2.95\n",
      "Training network. lr: 0.000247. clip: 0.098650\n",
      "Iteration 451: Policy loss: 0.009189. Value loss: 0.020655. Entropy: 1.170727.\n",
      "Iteration 452: Policy loss: -0.012813. Value loss: 0.014621. Entropy: 1.177847.\n",
      "Iteration 453: Policy loss: -0.021106. Value loss: 0.012589. Entropy: 1.167730.\n",
      "episode: 573   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 324     evaluation reward: 2.93\n",
      "episode: 574   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 527     evaluation reward: 3.01\n",
      "Training network. lr: 0.000247. clip: 0.098641\n",
      "Iteration 454: Policy loss: 0.013667. Value loss: 0.047609. Entropy: 1.202831.\n",
      "Iteration 455: Policy loss: -0.007132. Value loss: 0.034206. Entropy: 1.202225.\n",
      "Iteration 456: Policy loss: -0.015182. Value loss: 0.027981. Entropy: 1.206690.\n",
      "episode: 575   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 429     evaluation reward: 3.03\n",
      "episode: 576   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 406     evaluation reward: 3.06\n",
      "episode: 577   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 292     evaluation reward: 3.04\n",
      "Training network. lr: 0.000247. clip: 0.098632\n",
      "Iteration 457: Policy loss: 0.004042. Value loss: 0.022762. Entropy: 1.217289.\n",
      "Iteration 458: Policy loss: -0.013860. Value loss: 0.015580. Entropy: 1.211282.\n",
      "Iteration 459: Policy loss: -0.019382. Value loss: 0.011934. Entropy: 1.204242.\n",
      "episode: 578   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 392     evaluation reward: 3.06\n",
      "episode: 579   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 524     evaluation reward: 3.12\n",
      "Training network. lr: 0.000247. clip: 0.098623\n",
      "Iteration 460: Policy loss: 0.004233. Value loss: 0.023780. Entropy: 1.216190.\n",
      "Iteration 461: Policy loss: -0.013028. Value loss: 0.015787. Entropy: 1.216978.\n",
      "Iteration 462: Policy loss: -0.023654. Value loss: 0.012133. Entropy: 1.210252.\n",
      "episode: 580   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 411     evaluation reward: 3.12\n",
      "episode: 581   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 284     evaluation reward: 3.11\n",
      "episode: 582   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 353     evaluation reward: 3.11\n",
      "Training network. lr: 0.000247. clip: 0.098614\n",
      "Iteration 463: Policy loss: 0.007304. Value loss: 0.018027. Entropy: 1.190217.\n",
      "Iteration 464: Policy loss: -0.009441. Value loss: 0.011052. Entropy: 1.205575.\n",
      "Iteration 465: Policy loss: -0.022585. Value loss: 0.008806. Entropy: 1.195591.\n",
      "episode: 583   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 384     evaluation reward: 3.12\n",
      "episode: 584   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 403     evaluation reward: 3.14\n",
      "episode: 585   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 393     evaluation reward: 3.18\n",
      "Training network. lr: 0.000247. clip: 0.098605\n",
      "Iteration 466: Policy loss: 0.006957. Value loss: 0.018709. Entropy: 1.211853.\n",
      "Iteration 467: Policy loss: -0.007019. Value loss: 0.014341. Entropy: 1.204029.\n",
      "Iteration 468: Policy loss: -0.020078. Value loss: 0.012403. Entropy: 1.192758.\n",
      "episode: 586   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 286     evaluation reward: 3.18\n",
      "episode: 587   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 355     evaluation reward: 3.17\n",
      "Training network. lr: 0.000246. clip: 0.098596\n",
      "Iteration 469: Policy loss: 0.006286. Value loss: 0.023518. Entropy: 1.171548.\n",
      "Iteration 470: Policy loss: -0.013134. Value loss: 0.015839. Entropy: 1.164239.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 471: Policy loss: -0.022176. Value loss: 0.013533. Entropy: 1.161919.\n",
      "episode: 588   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 497     evaluation reward: 3.2\n",
      "episode: 589   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 328     evaluation reward: 3.22\n",
      "episode: 590   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 367     evaluation reward: 3.21\n",
      "episode: 591   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 214     evaluation reward: 3.19\n",
      "Training network. lr: 0.000246. clip: 0.098587\n",
      "Iteration 472: Policy loss: 0.006619. Value loss: 0.022121. Entropy: 1.196375.\n",
      "Iteration 473: Policy loss: -0.011832. Value loss: 0.016160. Entropy: 1.191671.\n",
      "Iteration 474: Policy loss: -0.025891. Value loss: 0.013092. Entropy: 1.187290.\n",
      "episode: 592   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 437     evaluation reward: 3.23\n",
      "episode: 593   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 440     evaluation reward: 3.25\n",
      "Training network. lr: 0.000246. clip: 0.098578\n",
      "Iteration 475: Policy loss: 0.009125. Value loss: 0.024434. Entropy: 1.153735.\n",
      "Iteration 476: Policy loss: -0.011193. Value loss: 0.016347. Entropy: 1.167055.\n",
      "Iteration 477: Policy loss: -0.019517. Value loss: 0.012286. Entropy: 1.150758.\n",
      "episode: 594   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 449     evaluation reward: 3.28\n",
      "episode: 595   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 337     evaluation reward: 3.29\n",
      "episode: 596   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 306     evaluation reward: 3.31\n",
      "Training network. lr: 0.000246. clip: 0.098569\n",
      "Iteration 478: Policy loss: 0.011310. Value loss: 0.025348. Entropy: 1.154657.\n",
      "Iteration 479: Policy loss: -0.008502. Value loss: 0.017052. Entropy: 1.146343.\n",
      "Iteration 480: Policy loss: -0.017674. Value loss: 0.013696. Entropy: 1.151842.\n",
      "episode: 597   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 492     evaluation reward: 3.37\n",
      "episode: 598   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 290     evaluation reward: 3.35\n",
      "episode: 599   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 268     evaluation reward: 3.33\n",
      "Training network. lr: 0.000246. clip: 0.098560\n",
      "Iteration 481: Policy loss: 0.002043. Value loss: 0.023319. Entropy: 1.155413.\n",
      "Iteration 482: Policy loss: -0.016744. Value loss: 0.015647. Entropy: 1.153410.\n",
      "Iteration 483: Policy loss: -0.024139. Value loss: 0.012472. Entropy: 1.152968.\n",
      "episode: 600   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 239     evaluation reward: 3.34\n",
      "episode: 601   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 344     evaluation reward: 3.36\n",
      "episode: 602   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 331     evaluation reward: 3.33\n",
      "Training network. lr: 0.000246. clip: 0.098551\n",
      "Iteration 484: Policy loss: 0.005753. Value loss: 0.019428. Entropy: 1.130545.\n",
      "Iteration 485: Policy loss: -0.011597. Value loss: 0.014099. Entropy: 1.118580.\n",
      "Iteration 486: Policy loss: -0.022456. Value loss: 0.012659. Entropy: 1.126179.\n",
      "episode: 603   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 296     evaluation reward: 3.32\n",
      "episode: 604   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 331     evaluation reward: 3.35\n",
      "episode: 605   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 417     evaluation reward: 3.36\n",
      "Training network. lr: 0.000246. clip: 0.098542\n",
      "Iteration 487: Policy loss: 0.006788. Value loss: 0.012768. Entropy: 1.168270.\n",
      "Iteration 488: Policy loss: -0.011193. Value loss: 0.008945. Entropy: 1.158916.\n",
      "Iteration 489: Policy loss: -0.025012. Value loss: 0.007523. Entropy: 1.160639.\n",
      "episode: 606   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 438     evaluation reward: 3.43\n",
      "episode: 607   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 368     evaluation reward: 3.45\n",
      "episode: 608   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 312     evaluation reward: 3.45\n",
      "Training network. lr: 0.000246. clip: 0.098533\n",
      "Iteration 490: Policy loss: 0.013148. Value loss: 0.044400. Entropy: 1.189456.\n",
      "Iteration 491: Policy loss: -0.004443. Value loss: 0.035975. Entropy: 1.183547.\n",
      "Iteration 492: Policy loss: -0.012959. Value loss: 0.027469. Entropy: 1.169926.\n",
      "episode: 609   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 487     evaluation reward: 3.47\n",
      "episode: 610   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 274     evaluation reward: 3.43\n",
      "episode: 611   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 241     evaluation reward: 3.42\n",
      "Training network. lr: 0.000246. clip: 0.098524\n",
      "Iteration 493: Policy loss: 0.014646. Value loss: 0.020230. Entropy: 1.156152.\n",
      "Iteration 494: Policy loss: -0.005584. Value loss: 0.013659. Entropy: 1.144775.\n",
      "Iteration 495: Policy loss: -0.019394. Value loss: 0.010991. Entropy: 1.144804.\n",
      "episode: 612   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 445     evaluation reward: 3.45\n",
      "episode: 613   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 319     evaluation reward: 3.47\n",
      "episode: 614   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 313     evaluation reward: 3.46\n",
      "Training network. lr: 0.000246. clip: 0.098515\n",
      "Iteration 496: Policy loss: 0.007032. Value loss: 0.019631. Entropy: 1.147532.\n",
      "Iteration 497: Policy loss: -0.010418. Value loss: 0.012708. Entropy: 1.163396.\n",
      "Iteration 498: Policy loss: -0.023719. Value loss: 0.010270. Entropy: 1.161881.\n",
      "episode: 615   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 449     evaluation reward: 3.48\n",
      "episode: 616   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 270     evaluation reward: 3.45\n",
      "Training network. lr: 0.000246. clip: 0.098506\n",
      "Iteration 499: Policy loss: 0.008409. Value loss: 0.019273. Entropy: 1.117363.\n",
      "Iteration 500: Policy loss: -0.013060. Value loss: 0.012366. Entropy: 1.105190.\n",
      "Iteration 501: Policy loss: -0.025332. Value loss: 0.010291. Entropy: 1.109871.\n",
      "episode: 617   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 518     evaluation reward: 3.47\n",
      "episode: 618   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 497     evaluation reward: 3.5\n",
      "episode: 619   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 262     evaluation reward: 3.5\n",
      "Training network. lr: 0.000246. clip: 0.098497\n",
      "Iteration 502: Policy loss: 0.010345. Value loss: 0.023884. Entropy: 1.124005.\n",
      "Iteration 503: Policy loss: -0.006958. Value loss: 0.017264. Entropy: 1.127090.\n",
      "Iteration 504: Policy loss: -0.018137. Value loss: 0.014411. Entropy: 1.129102.\n",
      "episode: 620   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 362     evaluation reward: 3.49\n",
      "episode: 621   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 405     evaluation reward: 3.51\n",
      "Training network. lr: 0.000246. clip: 0.098488\n",
      "Iteration 505: Policy loss: 0.009409. Value loss: 0.015441. Entropy: 1.197825.\n",
      "Iteration 506: Policy loss: -0.012868. Value loss: 0.010472. Entropy: 1.193245.\n",
      "Iteration 507: Policy loss: -0.025137. Value loss: 0.008747. Entropy: 1.187109.\n",
      "episode: 622   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 520     evaluation reward: 3.55\n",
      "episode: 623   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 412     evaluation reward: 3.59\n",
      "Training network. lr: 0.000246. clip: 0.098479\n",
      "Iteration 508: Policy loss: 0.007224. Value loss: 0.017223. Entropy: 1.181328.\n",
      "Iteration 509: Policy loss: -0.013812. Value loss: 0.012557. Entropy: 1.162575.\n",
      "Iteration 510: Policy loss: -0.025216. Value loss: 0.009031. Entropy: 1.165433.\n",
      "episode: 624   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 3.6\n",
      "episode: 625   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 398     evaluation reward: 3.62\n",
      "episode: 626   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 354     evaluation reward: 3.65\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 511: Policy loss: 0.002140. Value loss: 0.017815. Entropy: 1.130236.\n",
      "Iteration 512: Policy loss: -0.017361. Value loss: 0.013234. Entropy: 1.115571.\n",
      "Iteration 513: Policy loss: -0.026702. Value loss: 0.010772. Entropy: 1.120041.\n",
      "episode: 627   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 304     evaluation reward: 3.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 628   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 429     evaluation reward: 3.69\n",
      "episode: 629   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 275     evaluation reward: 3.7\n",
      "Training network. lr: 0.000246. clip: 0.098461\n",
      "Iteration 514: Policy loss: 0.010526. Value loss: 0.016647. Entropy: 1.089678.\n",
      "Iteration 515: Policy loss: -0.008598. Value loss: 0.011972. Entropy: 1.086168.\n",
      "Iteration 516: Policy loss: -0.020152. Value loss: 0.009361. Entropy: 1.086967.\n",
      "episode: 630   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 366     evaluation reward: 3.69\n",
      "episode: 631   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 480     evaluation reward: 3.72\n",
      "episode: 632   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 397     evaluation reward: 3.74\n",
      "Training network. lr: 0.000246. clip: 0.098452\n",
      "Iteration 517: Policy loss: 0.001887. Value loss: 0.019747. Entropy: 1.116406.\n",
      "Iteration 518: Policy loss: -0.018419. Value loss: 0.013009. Entropy: 1.113992.\n",
      "Iteration 519: Policy loss: -0.023339. Value loss: 0.009901. Entropy: 1.116454.\n",
      "episode: 633   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 477     evaluation reward: 3.74\n",
      "episode: 634   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 456     evaluation reward: 3.77\n",
      "Training network. lr: 0.000246. clip: 0.098443\n",
      "Iteration 520: Policy loss: 0.010373. Value loss: 0.027779. Entropy: 1.134371.\n",
      "Iteration 521: Policy loss: -0.010935. Value loss: 0.019828. Entropy: 1.132197.\n",
      "Iteration 522: Policy loss: -0.028806. Value loss: 0.015823. Entropy: 1.116892.\n",
      "episode: 635   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 369     evaluation reward: 3.78\n",
      "episode: 636   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 262     evaluation reward: 3.76\n",
      "episode: 637   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 306     evaluation reward: 3.75\n",
      "Training network. lr: 0.000246. clip: 0.098434\n",
      "Iteration 523: Policy loss: 0.011580. Value loss: 0.027650. Entropy: 1.076016.\n",
      "Iteration 524: Policy loss: -0.007158. Value loss: 0.019215. Entropy: 1.075371.\n",
      "Iteration 525: Policy loss: -0.025262. Value loss: 0.015468. Entropy: 1.071183.\n",
      "episode: 638   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 478     evaluation reward: 3.77\n",
      "episode: 639   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 389     evaluation reward: 3.8\n",
      "Training network. lr: 0.000246. clip: 0.098425\n",
      "Iteration 526: Policy loss: 0.010244. Value loss: 0.024161. Entropy: 1.117162.\n",
      "Iteration 527: Policy loss: -0.013862. Value loss: 0.015600. Entropy: 1.120582.\n",
      "Iteration 528: Policy loss: -0.024043. Value loss: 0.012927. Entropy: 1.121759.\n",
      "episode: 640   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 449     evaluation reward: 3.77\n",
      "episode: 641   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 340     evaluation reward: 3.79\n",
      "episode: 642   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 370     evaluation reward: 3.77\n",
      "Training network. lr: 0.000246. clip: 0.098416\n",
      "Iteration 529: Policy loss: 0.007408. Value loss: 0.014451. Entropy: 1.098820.\n",
      "Iteration 530: Policy loss: -0.011356. Value loss: 0.010175. Entropy: 1.092212.\n",
      "Iteration 531: Policy loss: -0.022820. Value loss: 0.008389. Entropy: 1.091982.\n",
      "episode: 643   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 279     evaluation reward: 3.75\n",
      "episode: 644   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 394     evaluation reward: 3.79\n",
      "episode: 645   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 409     evaluation reward: 3.8\n",
      "Training network. lr: 0.000246. clip: 0.098407\n",
      "Iteration 532: Policy loss: 0.006457. Value loss: 0.021581. Entropy: 1.062569.\n",
      "Iteration 533: Policy loss: -0.009285. Value loss: 0.014064. Entropy: 1.051109.\n",
      "Iteration 534: Policy loss: -0.018624. Value loss: 0.012493. Entropy: 1.049778.\n",
      "episode: 646   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 458     evaluation reward: 3.8\n",
      "episode: 647   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 257     evaluation reward: 3.8\n",
      "episode: 648   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 271     evaluation reward: 3.78\n",
      "Training network. lr: 0.000246. clip: 0.098398\n",
      "Iteration 535: Policy loss: 0.009106. Value loss: 0.020460. Entropy: 1.050722.\n",
      "Iteration 536: Policy loss: -0.004337. Value loss: 0.014081. Entropy: 1.079034.\n",
      "Iteration 537: Policy loss: -0.020902. Value loss: 0.011209. Entropy: 1.049340.\n",
      "episode: 649   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 288     evaluation reward: 3.77\n",
      "episode: 650   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 250     evaluation reward: 3.76\n",
      "episode: 651   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 226     evaluation reward: 3.74\n",
      "episode: 652   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 338     evaluation reward: 3.72\n",
      "Training network. lr: 0.000246. clip: 0.098389\n",
      "Iteration 538: Policy loss: 0.004876. Value loss: 0.013362. Entropy: 1.071210.\n",
      "Iteration 539: Policy loss: -0.012353. Value loss: 0.009545. Entropy: 1.080754.\n",
      "Iteration 540: Policy loss: -0.021757. Value loss: 0.007822. Entropy: 1.071568.\n",
      "episode: 653   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 352     evaluation reward: 3.73\n",
      "episode: 654   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 278     evaluation reward: 3.75\n",
      "episode: 655   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 429     evaluation reward: 3.78\n",
      "Training network. lr: 0.000246. clip: 0.098380\n",
      "Iteration 541: Policy loss: 0.014113. Value loss: 0.018884. Entropy: 1.064986.\n",
      "Iteration 542: Policy loss: -0.005690. Value loss: 0.013006. Entropy: 1.068747.\n",
      "Iteration 543: Policy loss: -0.019947. Value loss: 0.010915. Entropy: 1.061857.\n",
      "episode: 656   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 382     evaluation reward: 3.79\n",
      "episode: 657   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 257     evaluation reward: 3.8\n",
      "episode: 658   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 282     evaluation reward: 3.81\n",
      "Training network. lr: 0.000246. clip: 0.098371\n",
      "Iteration 544: Policy loss: 0.009090. Value loss: 0.016163. Entropy: 1.079451.\n",
      "Iteration 545: Policy loss: -0.006203. Value loss: 0.010531. Entropy: 1.085917.\n",
      "Iteration 546: Policy loss: -0.018174. Value loss: 0.008536. Entropy: 1.076280.\n",
      "episode: 659   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 470     evaluation reward: 3.86\n",
      "episode: 660   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 215     evaluation reward: 3.84\n",
      "Training network. lr: 0.000246. clip: 0.098362\n",
      "Iteration 547: Policy loss: 0.009511. Value loss: 0.021836. Entropy: 1.146567.\n",
      "Iteration 548: Policy loss: -0.015952. Value loss: 0.014255. Entropy: 1.144751.\n",
      "Iteration 549: Policy loss: -0.026024. Value loss: 0.010401. Entropy: 1.137936.\n",
      "episode: 661   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 589     evaluation reward: 3.88\n",
      "episode: 662   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 327     evaluation reward: 3.89\n",
      "episode: 663   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 381     evaluation reward: 3.9\n",
      "episode: 664   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 234     evaluation reward: 3.85\n",
      "Training network. lr: 0.000246. clip: 0.098353\n",
      "Iteration 550: Policy loss: 0.011597. Value loss: 0.015244. Entropy: 1.101995.\n",
      "Iteration 551: Policy loss: -0.017137. Value loss: 0.011301. Entropy: 1.087605.\n",
      "Iteration 552: Policy loss: -0.023799. Value loss: 0.008756. Entropy: 1.089953.\n",
      "episode: 665   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 522     evaluation reward: 3.91\n",
      "episode: 666   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 449     evaluation reward: 3.92\n",
      "Training network. lr: 0.000246. clip: 0.098344\n",
      "Iteration 553: Policy loss: 0.009284. Value loss: 0.021226. Entropy: 1.139984.\n",
      "Iteration 554: Policy loss: -0.010888. Value loss: 0.015424. Entropy: 1.137560.\n",
      "Iteration 555: Policy loss: -0.019377. Value loss: 0.012567. Entropy: 1.132635.\n",
      "episode: 667   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 284     evaluation reward: 3.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 668   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 394     evaluation reward: 3.89\n",
      "episode: 669   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 314     evaluation reward: 3.88\n",
      "Training network. lr: 0.000246. clip: 0.098335\n",
      "Iteration 556: Policy loss: 0.008145. Value loss: 0.023488. Entropy: 1.088807.\n",
      "Iteration 557: Policy loss: -0.005740. Value loss: 0.017150. Entropy: 1.087662.\n",
      "Iteration 558: Policy loss: -0.022760. Value loss: 0.013826. Entropy: 1.093592.\n",
      "episode: 670   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 252     evaluation reward: 3.87\n",
      "episode: 671   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 360     evaluation reward: 3.89\n",
      "episode: 672   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 376     evaluation reward: 3.9\n",
      "Training network. lr: 0.000246. clip: 0.098326\n",
      "Iteration 559: Policy loss: 0.002029. Value loss: 0.019374. Entropy: 1.097467.\n",
      "Iteration 560: Policy loss: -0.020867. Value loss: 0.013632. Entropy: 1.101114.\n",
      "Iteration 561: Policy loss: -0.031148. Value loss: 0.011335. Entropy: 1.086000.\n",
      "episode: 673   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 359     evaluation reward: 3.91\n",
      "episode: 674   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 301     evaluation reward: 3.84\n",
      "episode: 675   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 410     evaluation reward: 3.84\n",
      "Training network. lr: 0.000246. clip: 0.098317\n",
      "Iteration 562: Policy loss: 0.004711. Value loss: 0.014390. Entropy: 1.059711.\n",
      "Iteration 563: Policy loss: -0.011832. Value loss: 0.009436. Entropy: 1.052237.\n",
      "Iteration 564: Policy loss: -0.022787. Value loss: 0.008578. Entropy: 1.057292.\n",
      "episode: 676   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 532     evaluation reward: 3.86\n",
      "episode: 677   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 223     evaluation reward: 3.85\n",
      "episode: 678   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 394     evaluation reward: 3.86\n",
      "Training network. lr: 0.000246. clip: 0.098308\n",
      "Iteration 565: Policy loss: 0.013130. Value loss: 0.025547. Entropy: 1.088530.\n",
      "Iteration 566: Policy loss: -0.008826. Value loss: 0.015121. Entropy: 1.088678.\n",
      "Iteration 567: Policy loss: -0.022682. Value loss: 0.011640. Entropy: 1.083601.\n",
      "episode: 679   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 402     evaluation reward: 3.83\n",
      "episode: 680   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 361     evaluation reward: 3.82\n",
      "episode: 681   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 210     evaluation reward: 3.81\n",
      "Training network. lr: 0.000246. clip: 0.098299\n",
      "Iteration 568: Policy loss: 0.009561. Value loss: 0.016851. Entropy: 1.070134.\n",
      "Iteration 569: Policy loss: -0.013766. Value loss: 0.011404. Entropy: 1.070736.\n",
      "Iteration 570: Policy loss: -0.024807. Value loss: 0.008511. Entropy: 1.073150.\n",
      "episode: 682   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 413     evaluation reward: 3.82\n",
      "episode: 683   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 522     evaluation reward: 3.85\n",
      "Training network. lr: 0.000246. clip: 0.098290\n",
      "Iteration 571: Policy loss: 0.008870. Value loss: 0.025821. Entropy: 1.094029.\n",
      "Iteration 572: Policy loss: -0.014783. Value loss: 0.017836. Entropy: 1.081926.\n",
      "Iteration 573: Policy loss: -0.026717. Value loss: 0.014404. Entropy: 1.070530.\n",
      "episode: 684   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 315     evaluation reward: 3.84\n",
      "episode: 685   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 544     evaluation reward: 3.86\n",
      "Training network. lr: 0.000246. clip: 0.098281\n",
      "Iteration 574: Policy loss: 0.005976. Value loss: 0.023487. Entropy: 1.100696.\n",
      "Iteration 575: Policy loss: -0.015449. Value loss: 0.014982. Entropy: 1.114396.\n",
      "Iteration 576: Policy loss: -0.025653. Value loss: 0.011816. Entropy: 1.100420.\n",
      "episode: 686   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 358     evaluation reward: 3.87\n",
      "episode: 687   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 440     evaluation reward: 3.89\n",
      "episode: 688   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 520     evaluation reward: 3.89\n",
      "Training network. lr: 0.000246. clip: 0.098272\n",
      "Iteration 577: Policy loss: 0.004943. Value loss: 0.027524. Entropy: 1.092023.\n",
      "Iteration 578: Policy loss: -0.012418. Value loss: 0.018391. Entropy: 1.077940.\n",
      "Iteration 579: Policy loss: -0.026717. Value loss: 0.014286. Entropy: 1.076480.\n",
      "episode: 689   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 366     evaluation reward: 3.9\n",
      "episode: 690   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 239     evaluation reward: 3.89\n",
      "episode: 691   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 426     evaluation reward: 3.93\n",
      "Training network. lr: 0.000246. clip: 0.098263\n",
      "Iteration 580: Policy loss: 0.009274. Value loss: 0.026057. Entropy: 1.049199.\n",
      "Iteration 581: Policy loss: -0.012846. Value loss: 0.019635. Entropy: 1.042178.\n",
      "Iteration 582: Policy loss: -0.023984. Value loss: 0.014735. Entropy: 1.030078.\n",
      "episode: 692   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 522     evaluation reward: 3.94\n",
      "Training network. lr: 0.000246. clip: 0.098254\n",
      "Iteration 583: Policy loss: 0.010909. Value loss: 0.023722. Entropy: 1.094140.\n",
      "Iteration 584: Policy loss: -0.011707. Value loss: 0.017656. Entropy: 1.081959.\n",
      "Iteration 585: Policy loss: -0.025074. Value loss: 0.015021. Entropy: 1.074891.\n",
      "episode: 693   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 609     evaluation reward: 3.97\n",
      "now time :  2018-12-26 12:23:02.404052\n",
      "episode: 694   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 440     evaluation reward: 4.01\n",
      "episode: 695   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 341     evaluation reward: 4.01\n",
      "Training network. lr: 0.000246. clip: 0.098245\n",
      "Iteration 586: Policy loss: 0.008458. Value loss: 0.050677. Entropy: 1.013785.\n",
      "Iteration 587: Policy loss: -0.012287. Value loss: 0.031513. Entropy: 1.016518.\n",
      "Iteration 588: Policy loss: -0.020358. Value loss: 0.026164. Entropy: 1.013997.\n",
      "episode: 696   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 392     evaluation reward: 4.02\n",
      "episode: 697   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 446     evaluation reward: 4.01\n",
      "Training network. lr: 0.000246. clip: 0.098236\n",
      "Iteration 589: Policy loss: 0.005863. Value loss: 0.022593. Entropy: 1.004861.\n",
      "Iteration 590: Policy loss: -0.012967. Value loss: 0.015859. Entropy: 0.988575.\n",
      "Iteration 591: Policy loss: -0.025247. Value loss: 0.012566. Entropy: 0.993296.\n",
      "episode: 698   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 463     evaluation reward: 4.04\n",
      "episode: 699   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 257     evaluation reward: 4.04\n",
      "episode: 700   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 350     evaluation reward: 4.06\n",
      "Training network. lr: 0.000246. clip: 0.098227\n",
      "Iteration 592: Policy loss: 0.012239. Value loss: 0.019502. Entropy: 1.003160.\n",
      "Iteration 593: Policy loss: -0.011176. Value loss: 0.014090. Entropy: 0.994573.\n",
      "Iteration 594: Policy loss: -0.020974. Value loss: 0.011344. Entropy: 0.997492.\n",
      "episode: 701   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 319     evaluation reward: 4.05\n",
      "episode: 702   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 398     evaluation reward: 4.06\n",
      "episode: 703   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 422     evaluation reward: 4.08\n",
      "Training network. lr: 0.000246. clip: 0.098218\n",
      "Iteration 595: Policy loss: 0.007031. Value loss: 0.027245. Entropy: 1.017194.\n",
      "Iteration 596: Policy loss: -0.011516. Value loss: 0.017914. Entropy: 1.023711.\n",
      "Iteration 597: Policy loss: -0.024267. Value loss: 0.015365. Entropy: 1.026139.\n",
      "episode: 704   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 387     evaluation reward: 4.09\n",
      "episode: 705   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 300     evaluation reward: 4.08\n",
      "episode: 706   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 243     evaluation reward: 4.02\n",
      "Training network. lr: 0.000246. clip: 0.098209\n",
      "Iteration 598: Policy loss: 0.007373. Value loss: 0.024350. Entropy: 0.976430.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 599: Policy loss: -0.014960. Value loss: 0.016919. Entropy: 0.976886.\n",
      "Iteration 600: Policy loss: -0.029066. Value loss: 0.014233. Entropy: 0.973951.\n",
      "episode: 707   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 585     evaluation reward: 4.06\n",
      "episode: 708   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 525     evaluation reward: 4.14\n",
      "Training network. lr: 0.000246. clip: 0.098200\n",
      "Iteration 601: Policy loss: 0.007396. Value loss: 0.062610. Entropy: 1.049932.\n",
      "Iteration 602: Policy loss: -0.008206. Value loss: 0.047049. Entropy: 1.048678.\n",
      "Iteration 603: Policy loss: -0.015444. Value loss: 0.034708. Entropy: 1.036131.\n",
      "episode: 709   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 346     evaluation reward: 4.11\n",
      "episode: 710   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 392     evaluation reward: 4.14\n",
      "episode: 711   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 464     evaluation reward: 4.18\n",
      "Training network. lr: 0.000245. clip: 0.098191\n",
      "Iteration 604: Policy loss: 0.007916. Value loss: 0.028347. Entropy: 0.991474.\n",
      "Iteration 605: Policy loss: -0.012152. Value loss: 0.019970. Entropy: 0.982060.\n",
      "Iteration 606: Policy loss: -0.026018. Value loss: 0.016149. Entropy: 0.977083.\n",
      "episode: 712   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 397     evaluation reward: 4.18\n",
      "episode: 713   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 388     evaluation reward: 4.19\n",
      "Training network. lr: 0.000245. clip: 0.098182\n",
      "Iteration 607: Policy loss: 0.006476. Value loss: 0.027733. Entropy: 1.015890.\n",
      "Iteration 608: Policy loss: -0.016527. Value loss: 0.017133. Entropy: 1.005159.\n",
      "Iteration 609: Policy loss: -0.027845. Value loss: 0.013840. Entropy: 1.006215.\n",
      "episode: 714   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 493     evaluation reward: 4.23\n",
      "episode: 715   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 407     evaluation reward: 4.23\n",
      "episode: 716   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 277     evaluation reward: 4.24\n",
      "Training network. lr: 0.000245. clip: 0.098173\n",
      "Iteration 610: Policy loss: 0.004982. Value loss: 0.032776. Entropy: 1.012694.\n",
      "Iteration 611: Policy loss: -0.012124. Value loss: 0.022082. Entropy: 0.998505.\n",
      "Iteration 612: Policy loss: -0.022461. Value loss: 0.018769. Entropy: 1.009331.\n",
      "episode: 717   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 285     evaluation reward: 4.21\n",
      "episode: 718   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 251     evaluation reward: 4.17\n",
      "episode: 719   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 483     evaluation reward: 4.23\n",
      "Training network. lr: 0.000245. clip: 0.098164\n",
      "Iteration 613: Policy loss: 0.006302. Value loss: 0.032343. Entropy: 1.030046.\n",
      "Iteration 614: Policy loss: -0.006461. Value loss: 0.020548. Entropy: 1.020403.\n",
      "Iteration 615: Policy loss: -0.024428. Value loss: 0.015676. Entropy: 1.011119.\n",
      "episode: 720   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 404     evaluation reward: 4.24\n",
      "episode: 721   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 378     evaluation reward: 4.24\n",
      "Training network. lr: 0.000245. clip: 0.098155\n",
      "Iteration 616: Policy loss: 0.008462. Value loss: 0.018908. Entropy: 1.022562.\n",
      "Iteration 617: Policy loss: -0.015363. Value loss: 0.012280. Entropy: 1.020329.\n",
      "Iteration 618: Policy loss: -0.026790. Value loss: 0.009555. Entropy: 1.019538.\n",
      "episode: 722   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 403     evaluation reward: 4.23\n",
      "episode: 723   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 430     evaluation reward: 4.23\n",
      "episode: 724   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 480     evaluation reward: 4.24\n",
      "Training network. lr: 0.000245. clip: 0.098146\n",
      "Iteration 619: Policy loss: 0.011080. Value loss: 0.024082. Entropy: 0.900174.\n",
      "Iteration 620: Policy loss: -0.004875. Value loss: 0.015066. Entropy: 0.896974.\n",
      "Iteration 621: Policy loss: -0.018812. Value loss: 0.012721. Entropy: 0.905674.\n",
      "episode: 725   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 333     evaluation reward: 4.23\n",
      "episode: 726   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 315     evaluation reward: 4.22\n",
      "episode: 727   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 310     evaluation reward: 4.22\n",
      "Training network. lr: 0.000245. clip: 0.098137\n",
      "Iteration 622: Policy loss: 0.009566. Value loss: 0.023002. Entropy: 1.014954.\n",
      "Iteration 623: Policy loss: -0.011079. Value loss: 0.015865. Entropy: 0.998448.\n",
      "Iteration 624: Policy loss: -0.023483. Value loss: 0.013080. Entropy: 0.996547.\n",
      "episode: 728   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 584     evaluation reward: 4.24\n",
      "episode: 729   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 253     evaluation reward: 4.23\n",
      "Training network. lr: 0.000245. clip: 0.098128\n",
      "Iteration 625: Policy loss: 0.017909. Value loss: 0.031188. Entropy: 0.951749.\n",
      "Iteration 626: Policy loss: -0.008843. Value loss: 0.020505. Entropy: 0.944445.\n",
      "Iteration 627: Policy loss: -0.021388. Value loss: 0.015676. Entropy: 0.945247.\n",
      "episode: 730   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 573     evaluation reward: 4.27\n",
      "episode: 731   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 404     evaluation reward: 4.26\n",
      "Training network. lr: 0.000245. clip: 0.098119\n",
      "Iteration 628: Policy loss: 0.012938. Value loss: 0.023775. Entropy: 0.969887.\n",
      "Iteration 629: Policy loss: -0.005393. Value loss: 0.015200. Entropy: 0.962888.\n",
      "Iteration 630: Policy loss: -0.020495. Value loss: 0.012035. Entropy: 0.975887.\n",
      "episode: 732   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 365     evaluation reward: 4.25\n",
      "episode: 733   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 348     evaluation reward: 4.24\n",
      "episode: 734   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 480     evaluation reward: 4.24\n",
      "Training network. lr: 0.000245. clip: 0.098110\n",
      "Iteration 631: Policy loss: 0.007375. Value loss: 0.021722. Entropy: 1.021655.\n",
      "Iteration 632: Policy loss: -0.008980. Value loss: 0.014048. Entropy: 1.031350.\n",
      "Iteration 633: Policy loss: -0.024910. Value loss: 0.010891. Entropy: 1.020096.\n",
      "episode: 735   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 384     evaluation reward: 4.24\n",
      "episode: 736   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 523     evaluation reward: 4.29\n",
      "episode: 737   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 281     evaluation reward: 4.29\n",
      "Training network. lr: 0.000245. clip: 0.098101\n",
      "Iteration 634: Policy loss: 0.007782. Value loss: 0.021800. Entropy: 1.007911.\n",
      "Iteration 635: Policy loss: -0.010109. Value loss: 0.015329. Entropy: 1.003949.\n",
      "Iteration 636: Policy loss: -0.024120. Value loss: 0.012231. Entropy: 0.997122.\n",
      "episode: 738   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 354     evaluation reward: 4.27\n",
      "episode: 739   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 517     evaluation reward: 4.3\n",
      "Training network. lr: 0.000245. clip: 0.098092\n",
      "Iteration 637: Policy loss: 0.010082. Value loss: 0.025236. Entropy: 0.970056.\n",
      "Iteration 638: Policy loss: -0.014525. Value loss: 0.018450. Entropy: 0.963852.\n",
      "Iteration 639: Policy loss: -0.025560. Value loss: 0.014586. Entropy: 0.965462.\n",
      "episode: 740   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 373     evaluation reward: 4.29\n",
      "episode: 741   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 285     evaluation reward: 4.29\n",
      "episode: 742   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 450     evaluation reward: 4.3\n",
      "Training network. lr: 0.000245. clip: 0.098083\n",
      "Iteration 640: Policy loss: 0.010309. Value loss: 0.025536. Entropy: 0.988029.\n",
      "Iteration 641: Policy loss: -0.011311. Value loss: 0.018645. Entropy: 0.987249.\n",
      "Iteration 642: Policy loss: -0.027766. Value loss: 0.016136. Entropy: 0.982847.\n",
      "episode: 743   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 197     evaluation reward: 4.29\n",
      "episode: 744   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 366     evaluation reward: 4.28\n",
      "episode: 745   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 458     evaluation reward: 4.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000245. clip: 0.098074\n",
      "Iteration 643: Policy loss: 0.008991. Value loss: 0.024199. Entropy: 0.952169.\n",
      "Iteration 644: Policy loss: -0.011966. Value loss: 0.016880. Entropy: 0.936028.\n",
      "Iteration 645: Policy loss: -0.022576. Value loss: 0.013873. Entropy: 0.941839.\n",
      "episode: 746   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 429     evaluation reward: 4.29\n",
      "episode: 747   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 493     evaluation reward: 4.33\n",
      "Training network. lr: 0.000245. clip: 0.098065\n",
      "Iteration 646: Policy loss: 0.013235. Value loss: 0.021108. Entropy: 1.039946.\n",
      "Iteration 647: Policy loss: -0.008801. Value loss: 0.014363. Entropy: 1.032929.\n",
      "Iteration 648: Policy loss: -0.025042. Value loss: 0.011452. Entropy: 1.029423.\n",
      "episode: 748   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 442     evaluation reward: 4.36\n",
      "episode: 749   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 314     evaluation reward: 4.36\n",
      "episode: 750   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 315     evaluation reward: 4.37\n",
      "Training network. lr: 0.000245. clip: 0.098056\n",
      "Iteration 649: Policy loss: 0.010229. Value loss: 0.019988. Entropy: 0.913958.\n",
      "Iteration 650: Policy loss: -0.009374. Value loss: 0.013721. Entropy: 0.935258.\n",
      "Iteration 651: Policy loss: -0.022386. Value loss: 0.010963. Entropy: 0.919493.\n",
      "episode: 751   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 411     evaluation reward: 4.41\n",
      "episode: 752   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 243     evaluation reward: 4.39\n",
      "episode: 753   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 399     evaluation reward: 4.4\n",
      "Training network. lr: 0.000245. clip: 0.098047\n",
      "Iteration 652: Policy loss: 0.004816. Value loss: 0.024648. Entropy: 0.935548.\n",
      "Iteration 653: Policy loss: -0.012743. Value loss: 0.017890. Entropy: 0.936279.\n",
      "Iteration 654: Policy loss: -0.023562. Value loss: 0.014532. Entropy: 0.932176.\n",
      "episode: 754   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 448     evaluation reward: 4.44\n",
      "episode: 755   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 336     evaluation reward: 4.42\n",
      "episode: 756   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 353     evaluation reward: 4.42\n",
      "Training network. lr: 0.000245. clip: 0.098038\n",
      "Iteration 655: Policy loss: 0.012535. Value loss: 0.016083. Entropy: 0.961953.\n",
      "Iteration 656: Policy loss: -0.011330. Value loss: 0.011122. Entropy: 0.952495.\n",
      "Iteration 657: Policy loss: -0.025234. Value loss: 0.009233. Entropy: 0.947594.\n",
      "episode: 757   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 246     evaluation reward: 4.42\n",
      "episode: 758   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 343     evaluation reward: 4.43\n",
      "episode: 759   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 428     evaluation reward: 4.42\n",
      "Training network. lr: 0.000245. clip: 0.098029\n",
      "Iteration 658: Policy loss: 0.010115. Value loss: 0.016025. Entropy: 0.961273.\n",
      "Iteration 659: Policy loss: -0.009325. Value loss: 0.011759. Entropy: 0.947415.\n",
      "Iteration 660: Policy loss: -0.019443. Value loss: 0.009972. Entropy: 0.955536.\n",
      "episode: 760   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 310     evaluation reward: 4.44\n",
      "episode: 761   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 349     evaluation reward: 4.4\n",
      "episode: 762   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 250     evaluation reward: 4.38\n",
      "Training network. lr: 0.000245. clip: 0.098020\n",
      "Iteration 661: Policy loss: 0.004789. Value loss: 0.017610. Entropy: 0.875242.\n",
      "Iteration 662: Policy loss: -0.011431. Value loss: 0.011947. Entropy: 0.885668.\n",
      "Iteration 663: Policy loss: -0.021075. Value loss: 0.009278. Entropy: 0.885186.\n",
      "episode: 763   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 409     evaluation reward: 4.39\n",
      "episode: 764   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 417     evaluation reward: 4.43\n",
      "Training network. lr: 0.000245. clip: 0.098011\n",
      "Iteration 664: Policy loss: 0.010110. Value loss: 0.014396. Entropy: 0.892054.\n",
      "Iteration 665: Policy loss: -0.012292. Value loss: 0.010455. Entropy: 0.878775.\n",
      "Iteration 666: Policy loss: -0.027622. Value loss: 0.008853. Entropy: 0.888848.\n",
      "episode: 765   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 410     evaluation reward: 4.41\n",
      "episode: 766   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 309     evaluation reward: 4.39\n",
      "episode: 767   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 353     evaluation reward: 4.41\n",
      "Training network. lr: 0.000245. clip: 0.098002\n",
      "Iteration 667: Policy loss: 0.010253. Value loss: 0.015233. Entropy: 0.871324.\n",
      "Iteration 668: Policy loss: -0.007287. Value loss: 0.010282. Entropy: 0.866207.\n",
      "Iteration 669: Policy loss: -0.020500. Value loss: 0.008473. Entropy: 0.867349.\n",
      "episode: 768   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 350     evaluation reward: 4.41\n",
      "episode: 769   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 550     evaluation reward: 4.45\n",
      "episode: 770   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 350     evaluation reward: 4.47\n",
      "Training network. lr: 0.000245. clip: 0.097993\n",
      "Iteration 670: Policy loss: 0.012953. Value loss: 0.020092. Entropy: 0.886878.\n",
      "Iteration 671: Policy loss: -0.012270. Value loss: 0.012972. Entropy: 0.889785.\n",
      "Iteration 672: Policy loss: -0.024549. Value loss: 0.010580. Entropy: 0.897312.\n",
      "episode: 771   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 368     evaluation reward: 4.47\n",
      "episode: 772   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 307     evaluation reward: 4.46\n",
      "episode: 773   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 377     evaluation reward: 4.46\n",
      "Training network. lr: 0.000245. clip: 0.097984\n",
      "Iteration 673: Policy loss: 0.011429. Value loss: 0.018848. Entropy: 0.843806.\n",
      "Iteration 674: Policy loss: -0.008486. Value loss: 0.013667. Entropy: 0.847502.\n",
      "Iteration 675: Policy loss: -0.019313. Value loss: 0.011143. Entropy: 0.855610.\n",
      "episode: 774   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 279     evaluation reward: 4.46\n",
      "episode: 775   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 344     evaluation reward: 4.45\n",
      "episode: 776   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 323     evaluation reward: 4.43\n",
      "Training network. lr: 0.000245. clip: 0.097975\n",
      "Iteration 676: Policy loss: 0.010956. Value loss: 0.013463. Entropy: 0.770661.\n",
      "Iteration 677: Policy loss: -0.010875. Value loss: 0.010009. Entropy: 0.767550.\n",
      "Iteration 678: Policy loss: -0.018816. Value loss: 0.008189. Entropy: 0.771007.\n",
      "episode: 777   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 373     evaluation reward: 4.46\n",
      "episode: 778   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 341     evaluation reward: 4.48\n",
      "episode: 779   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 277     evaluation reward: 4.46\n",
      "Training network. lr: 0.000245. clip: 0.097966\n",
      "Iteration 679: Policy loss: 0.014640. Value loss: 0.033244. Entropy: 0.849026.\n",
      "Iteration 680: Policy loss: 0.000142. Value loss: 0.021166. Entropy: 0.849946.\n",
      "Iteration 681: Policy loss: -0.015362. Value loss: 0.015646. Entropy: 0.854984.\n",
      "episode: 780   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 305     evaluation reward: 4.45\n",
      "episode: 781   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 385     evaluation reward: 4.48\n",
      "episode: 782   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 365     evaluation reward: 4.47\n",
      "Training network. lr: 0.000245. clip: 0.097957\n",
      "Iteration 682: Policy loss: 0.012777. Value loss: 0.019281. Entropy: 0.888856.\n",
      "Iteration 683: Policy loss: -0.008788. Value loss: 0.012611. Entropy: 0.886488.\n",
      "Iteration 684: Policy loss: -0.020584. Value loss: 0.009632. Entropy: 0.873596.\n",
      "episode: 783   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 308     evaluation reward: 4.43\n",
      "episode: 784   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 378     evaluation reward: 4.44\n",
      "episode: 785   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 198     evaluation reward: 4.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000245. clip: 0.097948\n",
      "Iteration 685: Policy loss: 0.004310. Value loss: 0.016884. Entropy: 0.778091.\n",
      "Iteration 686: Policy loss: -0.010412. Value loss: 0.010778. Entropy: 0.796954.\n",
      "Iteration 687: Policy loss: -0.018366. Value loss: 0.008159. Entropy: 0.807277.\n",
      "episode: 786   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 348     evaluation reward: 4.39\n",
      "episode: 787   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 261     evaluation reward: 4.35\n",
      "episode: 788   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 411     evaluation reward: 4.34\n",
      "Training network. lr: 0.000245. clip: 0.097939\n",
      "Iteration 688: Policy loss: 0.010480. Value loss: 0.016744. Entropy: 0.881528.\n",
      "Iteration 689: Policy loss: -0.009053. Value loss: 0.012251. Entropy: 0.864681.\n",
      "Iteration 690: Policy loss: -0.020992. Value loss: 0.009306. Entropy: 0.879114.\n",
      "episode: 789   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 415     evaluation reward: 4.35\n",
      "episode: 790   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 469     evaluation reward: 4.39\n",
      "episode: 791   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 342     evaluation reward: 4.37\n",
      "Training network. lr: 0.000245. clip: 0.097930\n",
      "Iteration 691: Policy loss: 0.008116. Value loss: 0.017773. Entropy: 0.909434.\n",
      "Iteration 692: Policy loss: -0.006593. Value loss: 0.012284. Entropy: 0.920087.\n",
      "Iteration 693: Policy loss: -0.019226. Value loss: 0.010421. Entropy: 0.916867.\n",
      "episode: 792   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 497     evaluation reward: 4.37\n",
      "episode: 793   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 469     evaluation reward: 4.35\n",
      "Training network. lr: 0.000245. clip: 0.097921\n",
      "Iteration 694: Policy loss: 0.026081. Value loss: 0.021281. Entropy: 0.876896.\n",
      "Iteration 695: Policy loss: -0.003608. Value loss: 0.015457. Entropy: 0.863749.\n",
      "Iteration 696: Policy loss: -0.015719. Value loss: 0.013548. Entropy: 0.875494.\n",
      "episode: 794   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 363     evaluation reward: 4.3\n",
      "episode: 795   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 305     evaluation reward: 4.3\n",
      "episode: 796   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 330     evaluation reward: 4.29\n",
      "Training network. lr: 0.000245. clip: 0.097912\n",
      "Iteration 697: Policy loss: 0.007406. Value loss: 0.015090. Entropy: 0.824686.\n",
      "Iteration 698: Policy loss: -0.004044. Value loss: 0.011082. Entropy: 0.854170.\n",
      "Iteration 699: Policy loss: -0.023223. Value loss: 0.008911. Entropy: 0.841416.\n",
      "episode: 797   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 268     evaluation reward: 4.26\n",
      "episode: 798   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 307     evaluation reward: 4.23\n",
      "episode: 799   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 282     evaluation reward: 4.24\n",
      "episode: 800   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 293     evaluation reward: 4.23\n",
      "Training network. lr: 0.000245. clip: 0.097903\n",
      "Iteration 700: Policy loss: 0.008039. Value loss: 0.021832. Entropy: 0.861537.\n",
      "Iteration 701: Policy loss: -0.009325. Value loss: 0.015301. Entropy: 0.875284.\n",
      "Iteration 702: Policy loss: -0.019545. Value loss: 0.012120. Entropy: 0.876775.\n",
      "episode: 801   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 561     evaluation reward: 4.27\n",
      "episode: 802   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 411     evaluation reward: 4.28\n",
      "Training network. lr: 0.000245. clip: 0.097894\n",
      "Iteration 703: Policy loss: 0.006770. Value loss: 0.015435. Entropy: 0.892765.\n",
      "Iteration 704: Policy loss: -0.011733. Value loss: 0.009234. Entropy: 0.892136.\n",
      "Iteration 705: Policy loss: -0.024158. Value loss: 0.007625. Entropy: 0.896279.\n",
      "episode: 803   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 529     evaluation reward: 4.29\n",
      "episode: 804   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 402     evaluation reward: 4.29\n",
      "Training network. lr: 0.000245. clip: 0.097885\n",
      "Iteration 706: Policy loss: 0.006424. Value loss: 0.020271. Entropy: 0.890943.\n",
      "Iteration 707: Policy loss: -0.008691. Value loss: 0.015583. Entropy: 0.897801.\n",
      "Iteration 708: Policy loss: -0.020784. Value loss: 0.012733. Entropy: 0.905040.\n",
      "episode: 805   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 373     evaluation reward: 4.3\n",
      "episode: 806   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 386     evaluation reward: 4.32\n",
      "episode: 807   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 253     evaluation reward: 4.26\n",
      "Training network. lr: 0.000245. clip: 0.097876\n",
      "Iteration 709: Policy loss: 0.003626. Value loss: 0.021787. Entropy: 0.914109.\n",
      "Iteration 710: Policy loss: -0.010863. Value loss: 0.015982. Entropy: 0.924107.\n",
      "Iteration 711: Policy loss: -0.020931. Value loss: 0.014019. Entropy: 0.929061.\n",
      "episode: 808   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 407     evaluation reward: 4.21\n",
      "episode: 809   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 245     evaluation reward: 4.2\n",
      "episode: 810   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 307     evaluation reward: 4.18\n",
      "Training network. lr: 0.000245. clip: 0.097867\n",
      "Iteration 712: Policy loss: 0.008874. Value loss: 0.015745. Entropy: 0.925830.\n",
      "Iteration 713: Policy loss: -0.009680. Value loss: 0.011066. Entropy: 0.925532.\n",
      "Iteration 714: Policy loss: -0.022040. Value loss: 0.008895. Entropy: 0.911414.\n",
      "episode: 811   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 306     evaluation reward: 4.15\n",
      "episode: 812   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 480     evaluation reward: 4.16\n",
      "episode: 813   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 300     evaluation reward: 4.15\n",
      "Training network. lr: 0.000245. clip: 0.097858\n",
      "Iteration 715: Policy loss: 0.009701. Value loss: 0.014355. Entropy: 0.862041.\n",
      "Iteration 716: Policy loss: -0.006854. Value loss: 0.010579. Entropy: 0.854046.\n",
      "Iteration 717: Policy loss: -0.014578. Value loss: 0.009301. Entropy: 0.842135.\n",
      "episode: 814   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 257     evaluation reward: 4.1\n",
      "episode: 815   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 242     evaluation reward: 4.07\n",
      "episode: 816   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 248     evaluation reward: 4.06\n",
      "Training network. lr: 0.000245. clip: 0.097849\n",
      "Iteration 718: Policy loss: 0.005843. Value loss: 0.015483. Entropy: 0.910555.\n",
      "Iteration 719: Policy loss: -0.008105. Value loss: 0.008857. Entropy: 0.922980.\n",
      "Iteration 720: Policy loss: -0.017157. Value loss: 0.007124. Entropy: 0.929080.\n",
      "episode: 817   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 543     evaluation reward: 4.1\n",
      "episode: 818   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 304     evaluation reward: 4.11\n",
      "episode: 819   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 329     evaluation reward: 4.06\n",
      "episode: 820   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 196     evaluation reward: 4.03\n",
      "Training network. lr: 0.000245. clip: 0.097840\n",
      "Iteration 721: Policy loss: 0.004809. Value loss: 0.017950. Entropy: 0.879560.\n",
      "Iteration 722: Policy loss: -0.009992. Value loss: 0.011920. Entropy: 0.890531.\n",
      "Iteration 723: Policy loss: -0.020938. Value loss: 0.009578. Entropy: 0.891713.\n",
      "episode: 821   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 205     evaluation reward: 4.0\n",
      "episode: 822   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 345     evaluation reward: 3.99\n",
      "episode: 823   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 333     evaluation reward: 3.98\n",
      "Training network. lr: 0.000245. clip: 0.097831\n",
      "Iteration 724: Policy loss: 0.011142. Value loss: 0.016646. Entropy: 0.878772.\n",
      "Iteration 725: Policy loss: -0.010337. Value loss: 0.011298. Entropy: 0.880231.\n",
      "Iteration 726: Policy loss: -0.019450. Value loss: 0.008992. Entropy: 0.871815.\n",
      "episode: 824   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 313     evaluation reward: 3.95\n",
      "episode: 825   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 435     evaluation reward: 3.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 826   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 455     evaluation reward: 3.99\n",
      "Training network. lr: 0.000245. clip: 0.097822\n",
      "Iteration 727: Policy loss: 0.011500. Value loss: 0.014353. Entropy: 0.888078.\n",
      "Iteration 728: Policy loss: -0.010930. Value loss: 0.010278. Entropy: 0.890605.\n",
      "Iteration 729: Policy loss: -0.020773. Value loss: 0.008391. Entropy: 0.893369.\n",
      "episode: 827   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 387     evaluation reward: 4.01\n",
      "episode: 828   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 308     evaluation reward: 3.96\n",
      "episode: 829   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 381     evaluation reward: 3.98\n",
      "Training network. lr: 0.000245. clip: 0.097813\n",
      "Iteration 730: Policy loss: 0.013815. Value loss: 0.014631. Entropy: 0.941853.\n",
      "Iteration 731: Policy loss: -0.009023. Value loss: 0.009758. Entropy: 0.946997.\n",
      "Iteration 732: Policy loss: -0.025687. Value loss: 0.008500. Entropy: 0.935238.\n",
      "now time :  2018-12-26 12:27:27.095284\n",
      "episode: 830   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 473     evaluation reward: 3.96\n",
      "episode: 831   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 388     evaluation reward: 3.95\n",
      "Training network. lr: 0.000245. clip: 0.097804\n",
      "Iteration 733: Policy loss: 0.009651. Value loss: 0.014722. Entropy: 0.861780.\n",
      "Iteration 734: Policy loss: -0.004866. Value loss: 0.011136. Entropy: 0.865602.\n",
      "Iteration 735: Policy loss: -0.021114. Value loss: 0.009520. Entropy: 0.860106.\n",
      "episode: 832   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 308     evaluation reward: 3.95\n",
      "episode: 833   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 315     evaluation reward: 3.94\n",
      "episode: 834   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 419     evaluation reward: 3.92\n",
      "Training network. lr: 0.000244. clip: 0.097795\n",
      "Iteration 736: Policy loss: 0.011177. Value loss: 0.011465. Entropy: 0.969289.\n",
      "Iteration 737: Policy loss: -0.006556. Value loss: 0.008553. Entropy: 0.959640.\n",
      "Iteration 738: Policy loss: -0.019094. Value loss: 0.007664. Entropy: 0.962591.\n",
      "episode: 835   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 447     evaluation reward: 3.93\n",
      "episode: 836   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 452     evaluation reward: 3.91\n",
      "Training network. lr: 0.000244. clip: 0.097786\n",
      "Iteration 739: Policy loss: 0.012124. Value loss: 0.016790. Entropy: 0.960459.\n",
      "Iteration 740: Policy loss: -0.006953. Value loss: 0.013036. Entropy: 0.968980.\n",
      "Iteration 741: Policy loss: -0.015233. Value loss: 0.011754. Entropy: 0.969260.\n",
      "episode: 837   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 447     evaluation reward: 3.94\n",
      "episode: 838   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 353     evaluation reward: 3.94\n",
      "episode: 839   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 307     evaluation reward: 3.9\n",
      "Training network. lr: 0.000244. clip: 0.097777\n",
      "Iteration 742: Policy loss: 0.003285. Value loss: 0.013911. Entropy: 0.942177.\n",
      "Iteration 743: Policy loss: -0.014824. Value loss: 0.009808. Entropy: 0.943096.\n",
      "Iteration 744: Policy loss: -0.024466. Value loss: 0.007936. Entropy: 0.940543.\n",
      "episode: 840   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 544     evaluation reward: 3.93\n",
      "episode: 841   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 415     evaluation reward: 3.95\n",
      "Training network. lr: 0.000244. clip: 0.097768\n",
      "Iteration 745: Policy loss: 0.014002. Value loss: 0.022768. Entropy: 0.968786.\n",
      "Iteration 746: Policy loss: -0.004796. Value loss: 0.015480. Entropy: 0.956676.\n",
      "Iteration 747: Policy loss: -0.019058. Value loss: 0.013839. Entropy: 0.956383.\n",
      "episode: 842   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 409     evaluation reward: 3.94\n",
      "episode: 843   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 426     evaluation reward: 3.98\n",
      "Training network. lr: 0.000244. clip: 0.097759\n",
      "Iteration 748: Policy loss: 0.015014. Value loss: 0.013308. Entropy: 0.942800.\n",
      "Iteration 749: Policy loss: -0.007490. Value loss: 0.011079. Entropy: 0.952541.\n",
      "Iteration 750: Policy loss: -0.016520. Value loss: 0.009090. Entropy: 0.942471.\n",
      "episode: 844   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 464     evaluation reward: 4.0\n",
      "episode: 845   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 401     evaluation reward: 3.98\n",
      "episode: 846   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 329     evaluation reward: 3.96\n",
      "Training network. lr: 0.000244. clip: 0.097750\n",
      "Iteration 751: Policy loss: 0.010254. Value loss: 0.019013. Entropy: 0.922932.\n",
      "Iteration 752: Policy loss: -0.011637. Value loss: 0.013778. Entropy: 0.922744.\n",
      "Iteration 753: Policy loss: -0.021815. Value loss: 0.011496. Entropy: 0.930807.\n",
      "episode: 847   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 547     evaluation reward: 3.98\n",
      "episode: 848   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 374     evaluation reward: 3.97\n",
      "Training network. lr: 0.000244. clip: 0.097741\n",
      "Iteration 754: Policy loss: 0.009532. Value loss: 0.018352. Entropy: 0.990375.\n",
      "Iteration 755: Policy loss: -0.007346. Value loss: 0.013397. Entropy: 0.980624.\n",
      "Iteration 756: Policy loss: -0.022035. Value loss: 0.011334. Entropy: 0.974761.\n",
      "episode: 849   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 419     evaluation reward: 3.99\n",
      "episode: 850   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 385     evaluation reward: 4.01\n",
      "episode: 851   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 344     evaluation reward: 4.0\n",
      "Training network. lr: 0.000244. clip: 0.097732\n",
      "Iteration 757: Policy loss: 0.013488. Value loss: 0.015785. Entropy: 0.902176.\n",
      "Iteration 758: Policy loss: -0.006013. Value loss: 0.010684. Entropy: 0.916480.\n",
      "Iteration 759: Policy loss: -0.016921. Value loss: 0.009032. Entropy: 0.889545.\n",
      "episode: 852   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 260     evaluation reward: 4.0\n",
      "episode: 853   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 371     evaluation reward: 3.99\n",
      "episode: 854   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 407     evaluation reward: 3.98\n",
      "Training network. lr: 0.000244. clip: 0.097723\n",
      "Iteration 760: Policy loss: 0.007520. Value loss: 0.019008. Entropy: 0.918867.\n",
      "Iteration 761: Policy loss: -0.010027. Value loss: 0.013995. Entropy: 0.922058.\n",
      "Iteration 762: Policy loss: -0.023040. Value loss: 0.012981. Entropy: 0.921054.\n",
      "episode: 855   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 363     evaluation reward: 3.99\n",
      "episode: 856   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 427     evaluation reward: 4.0\n",
      "episode: 857   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 261     evaluation reward: 4.0\n",
      "Training network. lr: 0.000244. clip: 0.097714\n",
      "Iteration 763: Policy loss: 0.001229. Value loss: 0.013975. Entropy: 0.916317.\n",
      "Iteration 764: Policy loss: -0.011446. Value loss: 0.009874. Entropy: 0.923056.\n",
      "Iteration 765: Policy loss: -0.024104. Value loss: 0.008316. Entropy: 0.913092.\n",
      "episode: 858   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 462     evaluation reward: 4.02\n",
      "episode: 859   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 342     evaluation reward: 4.0\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 766: Policy loss: 0.003242. Value loss: 0.022446. Entropy: 0.982908.\n",
      "Iteration 767: Policy loss: -0.014219. Value loss: 0.015877. Entropy: 0.977680.\n",
      "Iteration 768: Policy loss: -0.023321. Value loss: 0.013676. Entropy: 0.972188.\n",
      "episode: 860   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 447     evaluation reward: 4.03\n",
      "episode: 861   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 212     evaluation reward: 4.0\n",
      "episode: 862   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 259     evaluation reward: 4.0\n",
      "episode: 863   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 344     evaluation reward: 3.99\n",
      "Training network. lr: 0.000244. clip: 0.097696\n",
      "Iteration 769: Policy loss: 0.009496. Value loss: 0.024523. Entropy: 0.889977.\n",
      "Iteration 770: Policy loss: -0.009242. Value loss: 0.018436. Entropy: 0.886053.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 771: Policy loss: -0.016969. Value loss: 0.014608. Entropy: 0.889086.\n",
      "episode: 864   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 343     evaluation reward: 3.98\n",
      "episode: 865   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 250     evaluation reward: 3.95\n",
      "episode: 866   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 441     evaluation reward: 3.97\n",
      "Training network. lr: 0.000244. clip: 0.097687\n",
      "Iteration 772: Policy loss: 0.006539. Value loss: 0.014685. Entropy: 0.993245.\n",
      "Iteration 773: Policy loss: -0.016453. Value loss: 0.010527. Entropy: 0.987869.\n",
      "Iteration 774: Policy loss: -0.023817. Value loss: 0.008086. Entropy: 0.988609.\n",
      "episode: 867   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 250     evaluation reward: 3.95\n",
      "episode: 868   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 340     evaluation reward: 3.95\n",
      "episode: 869   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 424     evaluation reward: 3.93\n",
      "Training network. lr: 0.000244. clip: 0.097678\n",
      "Iteration 775: Policy loss: 0.015669. Value loss: 0.022403. Entropy: 0.911717.\n",
      "Iteration 776: Policy loss: -0.008863. Value loss: 0.015305. Entropy: 0.906432.\n",
      "Iteration 777: Policy loss: -0.020434. Value loss: 0.012242. Entropy: 0.896169.\n",
      "episode: 870   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 3.95\n",
      "episode: 871   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 452     evaluation reward: 3.96\n",
      "Training network. lr: 0.000244. clip: 0.097669\n",
      "Iteration 778: Policy loss: 0.007738. Value loss: 0.022281. Entropy: 1.038916.\n",
      "Iteration 779: Policy loss: -0.013802. Value loss: 0.014374. Entropy: 1.032971.\n",
      "Iteration 780: Policy loss: -0.019597. Value loss: 0.011220. Entropy: 1.029597.\n",
      "episode: 872   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 349     evaluation reward: 3.96\n",
      "episode: 873   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 264     evaluation reward: 3.94\n",
      "episode: 874   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 315     evaluation reward: 3.94\n",
      "Training network. lr: 0.000244. clip: 0.097660\n",
      "Iteration 781: Policy loss: 0.009077. Value loss: 0.018482. Entropy: 0.943010.\n",
      "Iteration 782: Policy loss: -0.012312. Value loss: 0.012723. Entropy: 0.944176.\n",
      "Iteration 783: Policy loss: -0.026993. Value loss: 0.010031. Entropy: 0.936701.\n",
      "episode: 875   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 395     evaluation reward: 3.94\n",
      "episode: 876   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 564     evaluation reward: 3.97\n",
      "episode: 877   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 319     evaluation reward: 3.96\n",
      "Training network. lr: 0.000244. clip: 0.097651\n",
      "Iteration 784: Policy loss: 0.012504. Value loss: 0.023250. Entropy: 0.925287.\n",
      "Iteration 785: Policy loss: -0.008850. Value loss: 0.016670. Entropy: 0.919976.\n",
      "Iteration 786: Policy loss: -0.022548. Value loss: 0.013015. Entropy: 0.909495.\n",
      "episode: 878   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 375     evaluation reward: 3.93\n",
      "episode: 879   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 351     evaluation reward: 3.95\n",
      "episode: 880   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 362     evaluation reward: 3.96\n",
      "Training network. lr: 0.000244. clip: 0.097642\n",
      "Iteration 787: Policy loss: 0.008162. Value loss: 0.016758. Entropy: 0.924329.\n",
      "Iteration 788: Policy loss: -0.011908. Value loss: 0.011935. Entropy: 0.915154.\n",
      "Iteration 789: Policy loss: -0.023506. Value loss: 0.009246. Entropy: 0.907423.\n",
      "episode: 881   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 247     evaluation reward: 3.94\n",
      "episode: 882   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 430     evaluation reward: 3.95\n",
      "Training network. lr: 0.000244. clip: 0.097633\n",
      "Iteration 790: Policy loss: 0.008890. Value loss: 0.014672. Entropy: 0.968817.\n",
      "Iteration 791: Policy loss: -0.011934. Value loss: 0.010307. Entropy: 0.965171.\n",
      "Iteration 792: Policy loss: -0.024037. Value loss: 0.008825. Entropy: 0.955978.\n",
      "episode: 883   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 419     evaluation reward: 3.97\n",
      "episode: 884   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 346     evaluation reward: 3.97\n",
      "episode: 885   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 337     evaluation reward: 4.0\n",
      "Training network. lr: 0.000244. clip: 0.097624\n",
      "Iteration 793: Policy loss: 0.008550. Value loss: 0.010870. Entropy: 0.918178.\n",
      "Iteration 794: Policy loss: -0.010708. Value loss: 0.007975. Entropy: 0.912904.\n",
      "Iteration 795: Policy loss: -0.019650. Value loss: 0.007544. Entropy: 0.914304.\n",
      "episode: 886   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 448     evaluation reward: 4.01\n",
      "episode: 887   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 419     evaluation reward: 4.04\n",
      "episode: 888   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 364     evaluation reward: 4.03\n",
      "Training network. lr: 0.000244. clip: 0.097615\n",
      "Iteration 796: Policy loss: 0.007883. Value loss: 0.021648. Entropy: 0.941334.\n",
      "Iteration 797: Policy loss: -0.013973. Value loss: 0.015298. Entropy: 0.933851.\n",
      "Iteration 798: Policy loss: -0.024164. Value loss: 0.012068. Entropy: 0.945125.\n",
      "episode: 889   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 331     evaluation reward: 4.01\n",
      "episode: 890   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 333     evaluation reward: 3.99\n",
      "Training network. lr: 0.000244. clip: 0.097606\n",
      "Iteration 799: Policy loss: 0.009664. Value loss: 0.015767. Entropy: 0.831769.\n",
      "Iteration 800: Policy loss: -0.009574. Value loss: 0.011179. Entropy: 0.833955.\n",
      "Iteration 801: Policy loss: -0.018377. Value loss: 0.009773. Entropy: 0.846184.\n",
      "episode: 891   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 577     evaluation reward: 4.04\n",
      "episode: 892   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 469     evaluation reward: 4.04\n",
      "episode: 893   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 436     evaluation reward: 4.03\n",
      "Training network. lr: 0.000244. clip: 0.097597\n",
      "Iteration 802: Policy loss: 0.005250. Value loss: 0.021964. Entropy: 0.943997.\n",
      "Iteration 803: Policy loss: -0.009401. Value loss: 0.016277. Entropy: 0.945780.\n",
      "Iteration 804: Policy loss: -0.021383. Value loss: 0.013918. Entropy: 0.933307.\n",
      "episode: 894   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 286     evaluation reward: 4.01\n",
      "episode: 895   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 426     evaluation reward: 4.03\n",
      "episode: 896   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 306     evaluation reward: 4.03\n",
      "Training network. lr: 0.000244. clip: 0.097588\n",
      "Iteration 805: Policy loss: 0.007617. Value loss: 0.016231. Entropy: 0.962731.\n",
      "Iteration 806: Policy loss: -0.011716. Value loss: 0.011878. Entropy: 0.947705.\n",
      "Iteration 807: Policy loss: -0.024443. Value loss: 0.009179. Entropy: 0.944966.\n",
      "episode: 897   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 337     evaluation reward: 4.04\n",
      "episode: 898   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 535     evaluation reward: 4.08\n",
      "Training network. lr: 0.000244. clip: 0.097579\n",
      "Iteration 808: Policy loss: 0.009121. Value loss: 0.023253. Entropy: 0.901444.\n",
      "Iteration 809: Policy loss: -0.007624. Value loss: 0.016389. Entropy: 0.898997.\n",
      "Iteration 810: Policy loss: -0.020201. Value loss: 0.013107. Entropy: 0.898308.\n",
      "episode: 899   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 525     evaluation reward: 4.11\n",
      "episode: 900   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 445     evaluation reward: 4.13\n",
      "Training network. lr: 0.000244. clip: 0.097570\n",
      "Iteration 811: Policy loss: 0.009991. Value loss: 0.021069. Entropy: 0.953526.\n",
      "Iteration 812: Policy loss: -0.011738. Value loss: 0.015484. Entropy: 0.964391.\n",
      "Iteration 813: Policy loss: -0.023172. Value loss: 0.012449. Entropy: 0.957142.\n",
      "episode: 901   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 250     evaluation reward: 4.08\n",
      "episode: 902   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 352     evaluation reward: 4.07\n",
      "episode: 903   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 312     evaluation reward: 4.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000244. clip: 0.097561\n",
      "Iteration 814: Policy loss: 0.012395. Value loss: 0.015267. Entropy: 0.876491.\n",
      "Iteration 815: Policy loss: -0.006606. Value loss: 0.011315. Entropy: 0.871247.\n",
      "Iteration 816: Policy loss: -0.019661. Value loss: 0.008950. Entropy: 0.868152.\n",
      "episode: 904   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 409     evaluation reward: 4.05\n",
      "episode: 905   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 408     evaluation reward: 4.06\n",
      "episode: 906   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 395     evaluation reward: 4.06\n",
      "Training network. lr: 0.000244. clip: 0.097552\n",
      "Iteration 817: Policy loss: 0.003445. Value loss: 0.015451. Entropy: 0.912970.\n",
      "Iteration 818: Policy loss: -0.012175. Value loss: 0.011975. Entropy: 0.911690.\n",
      "Iteration 819: Policy loss: -0.020789. Value loss: 0.010706. Entropy: 0.900104.\n",
      "episode: 907   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 369     evaluation reward: 4.08\n",
      "episode: 908   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 389     evaluation reward: 4.08\n",
      "episode: 909   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 344     evaluation reward: 4.09\n",
      "Training network. lr: 0.000244. clip: 0.097543\n",
      "Iteration 820: Policy loss: 0.005451. Value loss: 0.019026. Entropy: 0.956224.\n",
      "Iteration 821: Policy loss: -0.008285. Value loss: 0.013517. Entropy: 0.946504.\n",
      "Iteration 822: Policy loss: -0.020252. Value loss: 0.010268. Entropy: 0.934757.\n",
      "episode: 910   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 455     evaluation reward: 4.11\n",
      "episode: 911   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 561     evaluation reward: 4.15\n",
      "Training network. lr: 0.000244. clip: 0.097534\n",
      "Iteration 823: Policy loss: 0.002130. Value loss: 0.018899. Entropy: 0.885507.\n",
      "Iteration 824: Policy loss: -0.012842. Value loss: 0.013621. Entropy: 0.887043.\n",
      "Iteration 825: Policy loss: -0.023125. Value loss: 0.010739. Entropy: 0.888941.\n",
      "episode: 912   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 458     evaluation reward: 4.15\n",
      "episode: 913   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 310     evaluation reward: 4.15\n",
      "Training network. lr: 0.000244. clip: 0.097525\n",
      "Iteration 826: Policy loss: 0.006783. Value loss: 0.022251. Entropy: 0.861173.\n",
      "Iteration 827: Policy loss: -0.010970. Value loss: 0.015064. Entropy: 0.857146.\n",
      "Iteration 828: Policy loss: -0.027608. Value loss: 0.012353. Entropy: 0.856841.\n",
      "episode: 914   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 701     evaluation reward: 4.23\n",
      "episode: 915   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 565     evaluation reward: 4.29\n",
      "Training network. lr: 0.000244. clip: 0.097516\n",
      "Iteration 829: Policy loss: 0.012211. Value loss: 0.022560. Entropy: 0.838677.\n",
      "Iteration 830: Policy loss: -0.008190. Value loss: 0.014035. Entropy: 0.855927.\n",
      "Iteration 831: Policy loss: -0.019409. Value loss: 0.011865. Entropy: 0.858301.\n",
      "episode: 916   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 346     evaluation reward: 4.31\n",
      "episode: 917   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 500     evaluation reward: 4.3\n",
      "Training network. lr: 0.000244. clip: 0.097507\n",
      "Iteration 832: Policy loss: 0.012283. Value loss: 0.020950. Entropy: 0.839489.\n",
      "Iteration 833: Policy loss: -0.013100. Value loss: 0.014920. Entropy: 0.849659.\n",
      "Iteration 834: Policy loss: -0.018957. Value loss: 0.012612. Entropy: 0.847434.\n",
      "episode: 918   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 305     evaluation reward: 4.3\n",
      "episode: 919   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 376     evaluation reward: 4.31\n",
      "episode: 920   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 409     evaluation reward: 4.35\n",
      "Training network. lr: 0.000244. clip: 0.097498\n",
      "Iteration 835: Policy loss: 0.009613. Value loss: 0.018516. Entropy: 0.845709.\n",
      "Iteration 836: Policy loss: -0.006155. Value loss: 0.012909. Entropy: 0.845179.\n",
      "Iteration 837: Policy loss: -0.017502. Value loss: 0.010326. Entropy: 0.835472.\n",
      "episode: 921   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 367     evaluation reward: 4.38\n",
      "episode: 922   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 346     evaluation reward: 4.38\n",
      "episode: 923   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 280     evaluation reward: 4.36\n",
      "Training network. lr: 0.000244. clip: 0.097489\n",
      "Iteration 838: Policy loss: 0.008688. Value loss: 0.019283. Entropy: 0.959476.\n",
      "Iteration 839: Policy loss: -0.009424. Value loss: 0.014723. Entropy: 0.944047.\n",
      "Iteration 840: Policy loss: -0.026219. Value loss: 0.011762. Entropy: 0.944351.\n",
      "episode: 924   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 427     evaluation reward: 4.38\n",
      "episode: 925   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 334     evaluation reward: 4.37\n",
      "Training network. lr: 0.000244. clip: 0.097480\n",
      "Iteration 841: Policy loss: 0.015661. Value loss: 0.054578. Entropy: 0.885960.\n",
      "Iteration 842: Policy loss: -0.003307. Value loss: 0.043543. Entropy: 0.880329.\n",
      "Iteration 843: Policy loss: -0.018223. Value loss: 0.037372. Entropy: 0.891740.\n",
      "episode: 926   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 687     evaluation reward: 4.45\n",
      "episode: 927   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 302     evaluation reward: 4.43\n",
      "episode: 928   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 469     evaluation reward: 4.46\n",
      "Training network. lr: 0.000244. clip: 0.097471\n",
      "Iteration 844: Policy loss: 0.004904. Value loss: 0.025958. Entropy: 0.818475.\n",
      "Iteration 845: Policy loss: -0.011403. Value loss: 0.017229. Entropy: 0.814191.\n",
      "Iteration 846: Policy loss: -0.023122. Value loss: 0.014253. Entropy: 0.813796.\n",
      "episode: 929   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 436     evaluation reward: 4.48\n",
      "episode: 930   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 504     evaluation reward: 4.49\n",
      "Training network. lr: 0.000244. clip: 0.097462\n",
      "Iteration 847: Policy loss: 0.015264. Value loss: 0.022323. Entropy: 0.843480.\n",
      "Iteration 848: Policy loss: -0.005394. Value loss: 0.016763. Entropy: 0.835033.\n",
      "Iteration 849: Policy loss: -0.018780. Value loss: 0.012969. Entropy: 0.834829.\n",
      "episode: 931   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 445     evaluation reward: 4.5\n",
      "episode: 932   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 406     evaluation reward: 4.52\n",
      "Training network. lr: 0.000244. clip: 0.097453\n",
      "Iteration 850: Policy loss: 0.008117. Value loss: 0.025772. Entropy: 0.921418.\n",
      "Iteration 851: Policy loss: -0.012918. Value loss: 0.017393. Entropy: 0.925860.\n",
      "Iteration 852: Policy loss: -0.028458. Value loss: 0.013795. Entropy: 0.917764.\n",
      "episode: 933   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 340     evaluation reward: 4.52\n",
      "episode: 934   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 419     evaluation reward: 4.53\n",
      "episode: 935   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 375     evaluation reward: 4.52\n",
      "Training network. lr: 0.000244. clip: 0.097444\n",
      "Iteration 853: Policy loss: 0.011995. Value loss: 0.020847. Entropy: 0.857954.\n",
      "Iteration 854: Policy loss: -0.005721. Value loss: 0.013035. Entropy: 0.859010.\n",
      "Iteration 855: Policy loss: -0.018155. Value loss: 0.010435. Entropy: 0.855913.\n",
      "episode: 936   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 282     evaluation reward: 4.5\n",
      "episode: 937   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 468     evaluation reward: 4.49\n",
      "episode: 938   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 464     evaluation reward: 4.5\n",
      "Training network. lr: 0.000244. clip: 0.097435\n",
      "Iteration 856: Policy loss: 0.005388. Value loss: 0.019137. Entropy: 0.879590.\n",
      "Iteration 857: Policy loss: -0.014885. Value loss: 0.012444. Entropy: 0.874999.\n",
      "Iteration 858: Policy loss: -0.022251. Value loss: 0.009583. Entropy: 0.877483.\n",
      "episode: 939   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 495     evaluation reward: 4.54\n",
      "episode: 940   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 406     evaluation reward: 4.51\n",
      "Training network. lr: 0.000244. clip: 0.097426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 859: Policy loss: 0.010998. Value loss: 0.024943. Entropy: 0.885631.\n",
      "Iteration 860: Policy loss: -0.011381. Value loss: 0.017732. Entropy: 0.884178.\n",
      "Iteration 861: Policy loss: -0.021254. Value loss: 0.012719. Entropy: 0.883739.\n",
      "episode: 941   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 489     evaluation reward: 4.53\n",
      "episode: 942   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 302     evaluation reward: 4.52\n",
      "Training network. lr: 0.000244. clip: 0.097417\n",
      "Iteration 862: Policy loss: 0.012476. Value loss: 0.023320. Entropy: 0.892411.\n",
      "Iteration 863: Policy loss: -0.008488. Value loss: 0.016640. Entropy: 0.893884.\n",
      "Iteration 864: Policy loss: -0.016227. Value loss: 0.013902. Entropy: 0.898849.\n",
      "episode: 943   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 4.52\n",
      "episode: 944   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 420     evaluation reward: 4.51\n",
      "episode: 945   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 361     evaluation reward: 4.51\n",
      "Training network. lr: 0.000244. clip: 0.097408\n",
      "Iteration 865: Policy loss: 0.007368. Value loss: 0.014177. Entropy: 0.846168.\n",
      "Iteration 866: Policy loss: -0.014691. Value loss: 0.010489. Entropy: 0.851943.\n",
      "Iteration 867: Policy loss: -0.024874. Value loss: 0.009387. Entropy: 0.839623.\n",
      "episode: 946   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 606     evaluation reward: 4.57\n",
      "episode: 947   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 526     evaluation reward: 4.58\n",
      "Training network. lr: 0.000243. clip: 0.097399\n",
      "Iteration 868: Policy loss: 0.007793. Value loss: 0.046186. Entropy: 0.848275.\n",
      "Iteration 869: Policy loss: -0.007083. Value loss: 0.035598. Entropy: 0.847215.\n",
      "Iteration 870: Policy loss: -0.017620. Value loss: 0.029448. Entropy: 0.835408.\n",
      "episode: 948   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 470     evaluation reward: 4.6\n",
      "episode: 949   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 598     evaluation reward: 4.63\n",
      "Training network. lr: 0.000243. clip: 0.097390\n",
      "Iteration 871: Policy loss: 0.015460. Value loss: 0.023074. Entropy: 0.883311.\n",
      "Iteration 872: Policy loss: -0.009382. Value loss: 0.017568. Entropy: 0.871440.\n",
      "Iteration 873: Policy loss: -0.020685. Value loss: 0.014504. Entropy: 0.880482.\n",
      "episode: 950   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 469     evaluation reward: 4.64\n",
      "episode: 951   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 488     evaluation reward: 4.66\n",
      "Training network. lr: 0.000243. clip: 0.097381\n",
      "Iteration 874: Policy loss: 0.009292. Value loss: 0.020307. Entropy: 0.857783.\n",
      "Iteration 875: Policy loss: -0.011345. Value loss: 0.014584. Entropy: 0.857125.\n",
      "Iteration 876: Policy loss: -0.026937. Value loss: 0.012471. Entropy: 0.860821.\n",
      "episode: 952   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 430     evaluation reward: 4.73\n",
      "episode: 953   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 581     evaluation reward: 4.77\n",
      "now time :  2018-12-26 12:31:50.281935\n",
      "Training network. lr: 0.000243. clip: 0.097372\n",
      "Iteration 877: Policy loss: 0.021808. Value loss: 0.105827. Entropy: 0.921218.\n",
      "Iteration 878: Policy loss: 0.000857. Value loss: 0.056859. Entropy: 0.911417.\n",
      "Iteration 879: Policy loss: -0.008834. Value loss: 0.041282. Entropy: 0.892899.\n",
      "episode: 954   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 417     evaluation reward: 4.77\n",
      "episode: 955   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 304     evaluation reward: 4.76\n",
      "episode: 956   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 301     evaluation reward: 4.74\n",
      "Training network. lr: 0.000243. clip: 0.097363\n",
      "Iteration 880: Policy loss: 0.006218. Value loss: 0.026050. Entropy: 0.849440.\n",
      "Iteration 881: Policy loss: -0.013051. Value loss: 0.016000. Entropy: 0.829641.\n",
      "Iteration 882: Policy loss: -0.021109. Value loss: 0.012989. Entropy: 0.826161.\n",
      "episode: 957   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 401     evaluation reward: 4.77\n",
      "episode: 958   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 473     evaluation reward: 4.77\n",
      "Training network. lr: 0.000243. clip: 0.097354\n",
      "Iteration 883: Policy loss: 0.014114. Value loss: 0.016815. Entropy: 0.783339.\n",
      "Iteration 884: Policy loss: -0.009404. Value loss: 0.012217. Entropy: 0.790313.\n",
      "Iteration 885: Policy loss: -0.021936. Value loss: 0.010490. Entropy: 0.794397.\n",
      "episode: 959   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 443     evaluation reward: 4.83\n",
      "episode: 960   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 261     evaluation reward: 4.79\n",
      "episode: 961   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 374     evaluation reward: 4.82\n",
      "Training network. lr: 0.000243. clip: 0.097345\n",
      "Iteration 886: Policy loss: 0.002441. Value loss: 0.052881. Entropy: 0.921488.\n",
      "Iteration 887: Policy loss: -0.010963. Value loss: 0.035261. Entropy: 0.924050.\n",
      "Iteration 888: Policy loss: -0.019109. Value loss: 0.027711. Entropy: 0.926929.\n",
      "episode: 962   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 398     evaluation reward: 4.84\n",
      "episode: 963   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 688     evaluation reward: 4.9\n",
      "Training network. lr: 0.000243. clip: 0.097336\n",
      "Iteration 889: Policy loss: 0.005012. Value loss: 0.021559. Entropy: 0.875274.\n",
      "Iteration 890: Policy loss: -0.016065. Value loss: 0.014767. Entropy: 0.871199.\n",
      "Iteration 891: Policy loss: -0.030033. Value loss: 0.011751. Entropy: 0.873359.\n",
      "episode: 964   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 460     evaluation reward: 4.92\n",
      "episode: 965   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 466     evaluation reward: 4.97\n",
      "Training network. lr: 0.000243. clip: 0.097327\n",
      "Iteration 892: Policy loss: 0.007282. Value loss: 0.021582. Entropy: 0.854942.\n",
      "Iteration 893: Policy loss: -0.016132. Value loss: 0.014093. Entropy: 0.846936.\n",
      "Iteration 894: Policy loss: -0.027247. Value loss: 0.011861. Entropy: 0.858720.\n",
      "episode: 966   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 547     evaluation reward: 4.99\n",
      "episode: 967   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 504     evaluation reward: 5.03\n",
      "Training network. lr: 0.000243. clip: 0.097318\n",
      "Iteration 895: Policy loss: 0.012113. Value loss: 0.022125. Entropy: 0.924904.\n",
      "Iteration 896: Policy loss: -0.013426. Value loss: 0.014344. Entropy: 0.912612.\n",
      "Iteration 897: Policy loss: -0.026666. Value loss: 0.012097. Entropy: 0.917152.\n",
      "episode: 968   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 656     evaluation reward: 5.12\n",
      "episode: 969   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 424     evaluation reward: 5.12\n",
      "Training network. lr: 0.000243. clip: 0.097309\n",
      "Iteration 898: Policy loss: 0.012891. Value loss: 0.049366. Entropy: 0.881786.\n",
      "Iteration 899: Policy loss: -0.007128. Value loss: 0.034811. Entropy: 0.866325.\n",
      "Iteration 900: Policy loss: -0.020439. Value loss: 0.031324. Entropy: 0.866837.\n",
      "episode: 970   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 302     evaluation reward: 5.09\n",
      "episode: 971   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 382     evaluation reward: 5.08\n",
      "Training network. lr: 0.000243. clip: 0.097300\n",
      "Iteration 901: Policy loss: 0.011736. Value loss: 0.028921. Entropy: 0.924356.\n",
      "Iteration 902: Policy loss: -0.012139. Value loss: 0.022179. Entropy: 0.929526.\n",
      "Iteration 903: Policy loss: -0.022147. Value loss: 0.018551. Entropy: 0.915291.\n",
      "episode: 972   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 569     evaluation reward: 5.12\n",
      "episode: 973   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 431     evaluation reward: 5.15\n",
      "episode: 974   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 494     evaluation reward: 5.18\n",
      "Training network. lr: 0.000243. clip: 0.097291\n",
      "Iteration 904: Policy loss: 0.007810. Value loss: 0.028511. Entropy: 0.833901.\n",
      "Iteration 905: Policy loss: -0.014671. Value loss: 0.018504. Entropy: 0.829160.\n",
      "Iteration 906: Policy loss: -0.023079. Value loss: 0.015762. Entropy: 0.821083.\n",
      "episode: 975   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 368     evaluation reward: 5.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 976   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 512     evaluation reward: 5.17\n",
      "Training network. lr: 0.000243. clip: 0.097282\n",
      "Iteration 907: Policy loss: 0.011576. Value loss: 0.021388. Entropy: 0.850032.\n",
      "Iteration 908: Policy loss: -0.008367. Value loss: 0.015684. Entropy: 0.850560.\n",
      "Iteration 909: Policy loss: -0.020079. Value loss: 0.012697. Entropy: 0.847494.\n",
      "episode: 977   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 499     evaluation reward: 5.2\n",
      "episode: 978   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 572     evaluation reward: 5.24\n",
      "Training network. lr: 0.000243. clip: 0.097273\n",
      "Iteration 910: Policy loss: 0.013904. Value loss: 0.022457. Entropy: 0.901448.\n",
      "Iteration 911: Policy loss: -0.007821. Value loss: 0.016684. Entropy: 0.862777.\n",
      "Iteration 912: Policy loss: -0.018443. Value loss: 0.014051. Entropy: 0.865607.\n",
      "episode: 979   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 594     evaluation reward: 5.28\n",
      "episode: 980   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 307     evaluation reward: 5.27\n",
      "Training network. lr: 0.000243. clip: 0.097264\n",
      "Iteration 913: Policy loss: 0.007416. Value loss: 0.027704. Entropy: 0.851547.\n",
      "Iteration 914: Policy loss: -0.007669. Value loss: 0.019244. Entropy: 0.860358.\n",
      "Iteration 915: Policy loss: -0.023208. Value loss: 0.016694. Entropy: 0.852122.\n",
      "episode: 981   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 306     evaluation reward: 5.28\n",
      "episode: 982   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 471     evaluation reward: 5.29\n",
      "Training network. lr: 0.000243. clip: 0.097255\n",
      "Iteration 916: Policy loss: 0.012566. Value loss: 0.024437. Entropy: 0.814707.\n",
      "Iteration 917: Policy loss: -0.006053. Value loss: 0.018165. Entropy: 0.823742.\n",
      "Iteration 918: Policy loss: -0.021231. Value loss: 0.015631. Entropy: 0.823941.\n",
      "episode: 983   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 579     evaluation reward: 5.32\n",
      "episode: 984   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 375     evaluation reward: 5.32\n",
      "episode: 985   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 330     evaluation reward: 5.31\n",
      "Training network. lr: 0.000243. clip: 0.097246\n",
      "Iteration 919: Policy loss: 0.010886. Value loss: 0.017589. Entropy: 0.786260.\n",
      "Iteration 920: Policy loss: -0.016323. Value loss: 0.014586. Entropy: 0.777837.\n",
      "Iteration 921: Policy loss: -0.026690. Value loss: 0.012097. Entropy: 0.781658.\n",
      "episode: 986   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 416     evaluation reward: 5.31\n",
      "episode: 987   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 300     evaluation reward: 5.29\n",
      "Training network. lr: 0.000243. clip: 0.097237\n",
      "Iteration 922: Policy loss: 0.016606. Value loss: 0.023332. Entropy: 0.888294.\n",
      "Iteration 923: Policy loss: -0.010217. Value loss: 0.016860. Entropy: 0.894922.\n",
      "Iteration 924: Policy loss: -0.027696. Value loss: 0.012769. Entropy: 0.893304.\n",
      "episode: 988   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 581     evaluation reward: 5.32\n",
      "episode: 989   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 360     evaluation reward: 5.33\n",
      "episode: 990   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 451     evaluation reward: 5.34\n",
      "Training network. lr: 0.000243. clip: 0.097228\n",
      "Iteration 925: Policy loss: 0.016145. Value loss: 0.017953. Entropy: 0.868643.\n",
      "Iteration 926: Policy loss: -0.013498. Value loss: 0.012852. Entropy: 0.861864.\n",
      "Iteration 927: Policy loss: -0.021310. Value loss: 0.010710. Entropy: 0.868459.\n",
      "episode: 991   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 435     evaluation reward: 5.31\n",
      "episode: 992   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 417     evaluation reward: 5.3\n",
      "Training network. lr: 0.000243. clip: 0.097219\n",
      "Iteration 928: Policy loss: 0.010660. Value loss: 0.015788. Entropy: 0.878047.\n",
      "Iteration 929: Policy loss: -0.011116. Value loss: 0.011780. Entropy: 0.882272.\n",
      "Iteration 930: Policy loss: -0.022025. Value loss: 0.009885. Entropy: 0.878804.\n",
      "episode: 993   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 477     evaluation reward: 5.3\n",
      "episode: 994   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 500     evaluation reward: 5.35\n",
      "Training network. lr: 0.000243. clip: 0.097210\n",
      "Iteration 931: Policy loss: 0.009558. Value loss: 0.024648. Entropy: 0.823025.\n",
      "Iteration 932: Policy loss: -0.012216. Value loss: 0.017124. Entropy: 0.818717.\n",
      "Iteration 933: Policy loss: -0.022373. Value loss: 0.012739. Entropy: 0.821639.\n",
      "episode: 995   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 434     evaluation reward: 5.35\n",
      "episode: 996   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 398     evaluation reward: 5.37\n",
      "episode: 997   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 439     evaluation reward: 5.39\n",
      "Training network. lr: 0.000243. clip: 0.097201\n",
      "Iteration 934: Policy loss: 0.008474. Value loss: 0.019106. Entropy: 0.859320.\n",
      "Iteration 935: Policy loss: -0.013828. Value loss: 0.014000. Entropy: 0.859728.\n",
      "Iteration 936: Policy loss: -0.029108. Value loss: 0.010853. Entropy: 0.845801.\n",
      "episode: 998   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 721     evaluation reward: 5.45\n",
      "Training network. lr: 0.000243. clip: 0.097192\n",
      "Iteration 937: Policy loss: 0.009836. Value loss: 0.056942. Entropy: 0.890039.\n",
      "Iteration 938: Policy loss: -0.009696. Value loss: 0.044525. Entropy: 0.887754.\n",
      "Iteration 939: Policy loss: -0.017852. Value loss: 0.038556. Entropy: 0.898614.\n",
      "episode: 999   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 500     evaluation reward: 5.45\n",
      "episode: 1000   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 362     evaluation reward: 5.43\n",
      "episode: 1001   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 508     evaluation reward: 5.47\n",
      "Training network. lr: 0.000243. clip: 0.097183\n",
      "Iteration 940: Policy loss: 0.015174. Value loss: 0.033114. Entropy: 0.997467.\n",
      "Iteration 941: Policy loss: -0.016673. Value loss: 0.022761. Entropy: 0.990672.\n",
      "Iteration 942: Policy loss: -0.028187. Value loss: 0.018746. Entropy: 0.987691.\n",
      "episode: 1002   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 5.49\n",
      "episode: 1003   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 366     evaluation reward: 5.5\n",
      "Training network. lr: 0.000243. clip: 0.097174\n",
      "Iteration 943: Policy loss: 0.007729. Value loss: 0.023095. Entropy: 0.881205.\n",
      "Iteration 944: Policy loss: -0.016518. Value loss: 0.015490. Entropy: 0.872740.\n",
      "Iteration 945: Policy loss: -0.028266. Value loss: 0.013294. Entropy: 0.863655.\n",
      "episode: 1004   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 369     evaluation reward: 5.49\n",
      "episode: 1005   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 554     evaluation reward: 5.51\n",
      "Training network. lr: 0.000243. clip: 0.097165\n",
      "Iteration 946: Policy loss: 0.013321. Value loss: 0.026276. Entropy: 0.899759.\n",
      "Iteration 947: Policy loss: -0.007856. Value loss: 0.018537. Entropy: 0.886932.\n",
      "Iteration 948: Policy loss: -0.025537. Value loss: 0.013817. Entropy: 0.886732.\n",
      "episode: 1006   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 426     evaluation reward: 5.52\n",
      "episode: 1007   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 352     evaluation reward: 5.52\n",
      "episode: 1008   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 444     evaluation reward: 5.52\n",
      "Training network. lr: 0.000243. clip: 0.097156\n",
      "Iteration 949: Policy loss: 0.017506. Value loss: 0.021499. Entropy: 0.807943.\n",
      "Iteration 950: Policy loss: -0.014949. Value loss: 0.013842. Entropy: 0.800270.\n",
      "Iteration 951: Policy loss: -0.023022. Value loss: 0.012676. Entropy: 0.808117.\n",
      "episode: 1009   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 366     evaluation reward: 5.53\n",
      "episode: 1010   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 421     evaluation reward: 5.53\n",
      "Training network. lr: 0.000243. clip: 0.097147\n",
      "Iteration 952: Policy loss: 0.011219. Value loss: 0.023136. Entropy: 0.871318.\n",
      "Iteration 953: Policy loss: -0.011245. Value loss: 0.017879. Entropy: 0.865802.\n",
      "Iteration 954: Policy loss: -0.024557. Value loss: 0.015148. Entropy: 0.869483.\n",
      "episode: 1011   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 5.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1012   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 285     evaluation reward: 5.48\n",
      "episode: 1013   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 358     evaluation reward: 5.49\n",
      "Training network. lr: 0.000243. clip: 0.097138\n",
      "Iteration 955: Policy loss: 0.008760. Value loss: 0.020274. Entropy: 0.822965.\n",
      "Iteration 956: Policy loss: -0.007781. Value loss: 0.015018. Entropy: 0.815059.\n",
      "Iteration 957: Policy loss: -0.021386. Value loss: 0.012200. Entropy: 0.808836.\n",
      "episode: 1014   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 646     evaluation reward: 5.48\n",
      "episode: 1015   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 480     evaluation reward: 5.45\n",
      "Training network. lr: 0.000243. clip: 0.097129\n",
      "Iteration 958: Policy loss: 0.016023. Value loss: 0.022177. Entropy: 0.848856.\n",
      "Iteration 959: Policy loss: -0.010645. Value loss: 0.015507. Entropy: 0.852336.\n",
      "Iteration 960: Policy loss: -0.020329. Value loss: 0.012657. Entropy: 0.853227.\n",
      "episode: 1016   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 385     evaluation reward: 5.45\n",
      "episode: 1017   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 490     evaluation reward: 5.45\n",
      "Training network. lr: 0.000243. clip: 0.097120\n",
      "Iteration 961: Policy loss: 0.010523. Value loss: 0.022510. Entropy: 0.800956.\n",
      "Iteration 962: Policy loss: -0.009386. Value loss: 0.015742. Entropy: 0.788703.\n",
      "Iteration 963: Policy loss: -0.022631. Value loss: 0.013610. Entropy: 0.798372.\n",
      "episode: 1018   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 433     evaluation reward: 5.47\n",
      "episode: 1019   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 352     evaluation reward: 5.47\n",
      "episode: 1020   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 273     evaluation reward: 5.44\n",
      "episode: 1021   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 350     evaluation reward: 5.44\n",
      "Training network. lr: 0.000243. clip: 0.097111\n",
      "Iteration 964: Policy loss: 0.012375. Value loss: 0.020463. Entropy: 0.870912.\n",
      "Iteration 965: Policy loss: -0.007961. Value loss: 0.013891. Entropy: 0.856316.\n",
      "Iteration 966: Policy loss: -0.021126. Value loss: 0.011521. Entropy: 0.851541.\n",
      "episode: 1022   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 454     evaluation reward: 5.45\n",
      "episode: 1023   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 490     evaluation reward: 5.5\n",
      "Training network. lr: 0.000243. clip: 0.097102\n",
      "Iteration 967: Policy loss: 0.003047. Value loss: 0.022502. Entropy: 0.747698.\n",
      "Iteration 968: Policy loss: -0.012256. Value loss: 0.016292. Entropy: 0.739635.\n",
      "Iteration 969: Policy loss: -0.023464. Value loss: 0.014282. Entropy: 0.733855.\n",
      "episode: 1024   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 525     evaluation reward: 5.52\n",
      "episode: 1025   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 257     evaluation reward: 5.5\n",
      "Training network. lr: 0.000243. clip: 0.097093\n",
      "Iteration 970: Policy loss: 0.017233. Value loss: 0.022852. Entropy: 0.828461.\n",
      "Iteration 971: Policy loss: -0.011494. Value loss: 0.015216. Entropy: 0.819715.\n",
      "Iteration 972: Policy loss: -0.026294. Value loss: 0.011696. Entropy: 0.817202.\n",
      "episode: 1026   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 450     evaluation reward: 5.43\n",
      "episode: 1027   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 248     evaluation reward: 5.42\n",
      "episode: 1028   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 460     evaluation reward: 5.44\n",
      "Training network. lr: 0.000243. clip: 0.097084\n",
      "Iteration 973: Policy loss: 0.006703. Value loss: 0.052593. Entropy: 0.837542.\n",
      "Iteration 974: Policy loss: -0.013971. Value loss: 0.037810. Entropy: 0.851220.\n",
      "Iteration 975: Policy loss: -0.022835. Value loss: 0.029670. Entropy: 0.850831.\n",
      "episode: 1029   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 392     evaluation reward: 5.43\n",
      "episode: 1030   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 459     evaluation reward: 5.41\n",
      "episode: 1031   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 339     evaluation reward: 5.4\n",
      "Training network. lr: 0.000243. clip: 0.097075\n",
      "Iteration 976: Policy loss: 0.007808. Value loss: 0.028139. Entropy: 0.836046.\n",
      "Iteration 977: Policy loss: -0.010209. Value loss: 0.020319. Entropy: 0.850204.\n",
      "Iteration 978: Policy loss: -0.026537. Value loss: 0.014760. Entropy: 0.849118.\n",
      "episode: 1032   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 690     evaluation reward: 5.44\n",
      "Training network. lr: 0.000243. clip: 0.097066\n",
      "Iteration 979: Policy loss: 0.010731. Value loss: 0.033086. Entropy: 0.927660.\n",
      "Iteration 980: Policy loss: -0.011604. Value loss: 0.022287. Entropy: 0.924344.\n",
      "Iteration 981: Policy loss: -0.025773. Value loss: 0.017100. Entropy: 0.923255.\n",
      "episode: 1033   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 403     evaluation reward: 5.46\n",
      "episode: 1034   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 409     evaluation reward: 5.45\n",
      "episode: 1035   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 485     evaluation reward: 5.46\n",
      "Training network. lr: 0.000243. clip: 0.097057\n",
      "Iteration 982: Policy loss: 0.010728. Value loss: 0.025267. Entropy: 0.877348.\n",
      "Iteration 983: Policy loss: -0.009820. Value loss: 0.017731. Entropy: 0.876849.\n",
      "Iteration 984: Policy loss: -0.023781. Value loss: 0.014407. Entropy: 0.862541.\n",
      "episode: 1036   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 288     evaluation reward: 5.45\n",
      "episode: 1037   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 358     evaluation reward: 5.44\n",
      "episode: 1038   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 361     evaluation reward: 5.43\n",
      "Training network. lr: 0.000243. clip: 0.097048\n",
      "Iteration 985: Policy loss: 0.007481. Value loss: 0.018337. Entropy: 0.834604.\n",
      "Iteration 986: Policy loss: -0.011629. Value loss: 0.011352. Entropy: 0.828825.\n",
      "Iteration 987: Policy loss: -0.020820. Value loss: 0.008959. Entropy: 0.834857.\n",
      "episode: 1039   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 331     evaluation reward: 5.39\n",
      "episode: 1040   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 377     evaluation reward: 5.39\n",
      "episode: 1041   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 374     evaluation reward: 5.35\n",
      "Training network. lr: 0.000243. clip: 0.097039\n",
      "Iteration 988: Policy loss: 0.011383. Value loss: 0.014191. Entropy: 0.919389.\n",
      "Iteration 989: Policy loss: -0.012567. Value loss: 0.010795. Entropy: 0.892220.\n",
      "Iteration 990: Policy loss: -0.029786. Value loss: 0.009411. Entropy: 0.892750.\n",
      "episode: 1042   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 274     evaluation reward: 5.34\n",
      "episode: 1043   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 321     evaluation reward: 5.32\n",
      "episode: 1044   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 296     evaluation reward: 5.3\n",
      "Training network. lr: 0.000243. clip: 0.097030\n",
      "Iteration 991: Policy loss: 0.012373. Value loss: 0.017966. Entropy: 0.849800.\n",
      "Iteration 992: Policy loss: -0.014086. Value loss: 0.012616. Entropy: 0.844114.\n",
      "Iteration 993: Policy loss: -0.028574. Value loss: 0.010092. Entropy: 0.844403.\n",
      "episode: 1045   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 412     evaluation reward: 5.31\n",
      "episode: 1046   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 479     evaluation reward: 5.28\n",
      "episode: 1047   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 232     evaluation reward: 5.2\n",
      "Training network. lr: 0.000243. clip: 0.097021\n",
      "Iteration 994: Policy loss: 0.010537. Value loss: 0.023206. Entropy: 0.883999.\n",
      "Iteration 995: Policy loss: -0.012801. Value loss: 0.016287. Entropy: 0.879297.\n",
      "Iteration 996: Policy loss: -0.020676. Value loss: 0.012030. Entropy: 0.883872.\n",
      "episode: 1048   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 495     evaluation reward: 5.21\n",
      "episode: 1049   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 248     evaluation reward: 5.14\n",
      "Training network. lr: 0.000243. clip: 0.097012\n",
      "Iteration 997: Policy loss: 0.006684. Value loss: 0.024193. Entropy: 0.883382.\n",
      "Iteration 998: Policy loss: -0.010037. Value loss: 0.015289. Entropy: 0.879656.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 999: Policy loss: -0.022106. Value loss: 0.012634. Entropy: 0.868810.\n",
      "episode: 1050   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 504     evaluation reward: 5.14\n",
      "episode: 1051   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 384     evaluation reward: 5.13\n",
      "episode: 1052   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 5.1\n",
      "Training network. lr: 0.000243. clip: 0.097003\n",
      "Iteration 1000: Policy loss: 0.012537. Value loss: 0.019351. Entropy: 0.831502.\n",
      "Iteration 1001: Policy loss: -0.016549. Value loss: 0.013697. Entropy: 0.830711.\n",
      "Iteration 1002: Policy loss: -0.024015. Value loss: 0.010984. Entropy: 0.832556.\n",
      "episode: 1053   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 623     evaluation reward: 5.1\n",
      "episode: 1054   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 408     evaluation reward: 5.1\n",
      "Training network. lr: 0.000242. clip: 0.096994\n",
      "Iteration 1003: Policy loss: 0.013506. Value loss: 0.021353. Entropy: 0.930077.\n",
      "Iteration 1004: Policy loss: -0.016098. Value loss: 0.016147. Entropy: 0.926850.\n",
      "Iteration 1005: Policy loss: -0.026590. Value loss: 0.012419. Entropy: 0.918814.\n",
      "episode: 1055   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 419     evaluation reward: 5.11\n",
      "episode: 1056   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 353     evaluation reward: 5.11\n",
      "Training network. lr: 0.000242. clip: 0.096985\n",
      "Iteration 1006: Policy loss: 0.010738. Value loss: 0.026061. Entropy: 0.906442.\n",
      "Iteration 1007: Policy loss: -0.011463. Value loss: 0.018353. Entropy: 0.899768.\n",
      "Iteration 1008: Policy loss: -0.027106. Value loss: 0.014587. Entropy: 0.902784.\n",
      "episode: 1057   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 387     evaluation reward: 5.11\n",
      "episode: 1058   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 414     evaluation reward: 5.1\n",
      "episode: 1059   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 338     evaluation reward: 5.04\n",
      "Training network. lr: 0.000242. clip: 0.096976\n",
      "Iteration 1009: Policy loss: 0.009235. Value loss: 0.018343. Entropy: 0.884633.\n",
      "Iteration 1010: Policy loss: -0.009675. Value loss: 0.011503. Entropy: 0.884028.\n",
      "Iteration 1011: Policy loss: -0.025633. Value loss: 0.008437. Entropy: 0.879174.\n",
      "episode: 1060   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 246     evaluation reward: 5.04\n",
      "episode: 1061   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 470     evaluation reward: 5.06\n",
      "episode: 1062   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 237     evaluation reward: 5.03\n",
      "Training network. lr: 0.000242. clip: 0.096967\n",
      "Iteration 1012: Policy loss: 0.005698. Value loss: 0.019763. Entropy: 0.834553.\n",
      "Iteration 1013: Policy loss: -0.012201. Value loss: 0.014011. Entropy: 0.839380.\n",
      "Iteration 1014: Policy loss: -0.026393. Value loss: 0.011654. Entropy: 0.845208.\n",
      "episode: 1063   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 432     evaluation reward: 4.98\n",
      "episode: 1064   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 433     evaluation reward: 4.97\n",
      "Training network. lr: 0.000242. clip: 0.096958\n",
      "Iteration 1015: Policy loss: 0.007807. Value loss: 0.024484. Entropy: 0.831142.\n",
      "Iteration 1016: Policy loss: -0.009451. Value loss: 0.016159. Entropy: 0.829810.\n",
      "Iteration 1017: Policy loss: -0.024863. Value loss: 0.012230. Entropy: 0.820547.\n",
      "episode: 1065   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 395     evaluation reward: 4.94\n",
      "episode: 1066   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 410     evaluation reward: 4.92\n",
      "episode: 1067   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 538     evaluation reward: 4.93\n",
      "Training network. lr: 0.000242. clip: 0.096949\n",
      "Iteration 1018: Policy loss: 0.018914. Value loss: 0.023853. Entropy: 0.867375.\n",
      "Iteration 1019: Policy loss: -0.007121. Value loss: 0.014580. Entropy: 0.861602.\n",
      "Iteration 1020: Policy loss: -0.019743. Value loss: 0.011826. Entropy: 0.857390.\n",
      "episode: 1068   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 400     evaluation reward: 4.84\n",
      "episode: 1069   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 497     evaluation reward: 4.85\n",
      "Training network. lr: 0.000242. clip: 0.096940\n",
      "Iteration 1021: Policy loss: 0.009576. Value loss: 0.019231. Entropy: 0.866009.\n",
      "Iteration 1022: Policy loss: -0.012595. Value loss: 0.014372. Entropy: 0.865826.\n",
      "Iteration 1023: Policy loss: -0.027423. Value loss: 0.011312. Entropy: 0.862519.\n",
      "episode: 1070   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 504     evaluation reward: 4.89\n",
      "episode: 1071   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 343     evaluation reward: 4.88\n",
      "now time :  2018-12-26 12:36:16.921707\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1024: Policy loss: 0.009650. Value loss: 0.023124. Entropy: 0.887130.\n",
      "Iteration 1025: Policy loss: -0.016765. Value loss: 0.016184. Entropy: 0.890815.\n",
      "Iteration 1026: Policy loss: -0.026901. Value loss: 0.012671. Entropy: 0.883190.\n",
      "episode: 1072   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 4.87\n",
      "episode: 1073   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 458     evaluation reward: 4.88\n",
      "episode: 1074   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 287     evaluation reward: 4.85\n",
      "Training network. lr: 0.000242. clip: 0.096922\n",
      "Iteration 1027: Policy loss: 0.010182. Value loss: 0.016081. Entropy: 0.814292.\n",
      "Iteration 1028: Policy loss: -0.010614. Value loss: 0.011090. Entropy: 0.815495.\n",
      "Iteration 1029: Policy loss: -0.022607. Value loss: 0.009275. Entropy: 0.816703.\n",
      "episode: 1075   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 486     evaluation reward: 4.86\n",
      "episode: 1076   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 413     evaluation reward: 4.85\n",
      "Training network. lr: 0.000242. clip: 0.096913\n",
      "Iteration 1030: Policy loss: 0.007737. Value loss: 0.025763. Entropy: 0.815709.\n",
      "Iteration 1031: Policy loss: -0.013230. Value loss: 0.019777. Entropy: 0.821111.\n",
      "Iteration 1032: Policy loss: -0.023702. Value loss: 0.016082. Entropy: 0.817974.\n",
      "episode: 1077   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 492     evaluation reward: 4.86\n",
      "episode: 1078   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 4.83\n",
      "episode: 1079   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 286     evaluation reward: 4.77\n",
      "Training network. lr: 0.000242. clip: 0.096904\n",
      "Iteration 1033: Policy loss: 0.007498. Value loss: 0.024971. Entropy: 0.900088.\n",
      "Iteration 1034: Policy loss: -0.014930. Value loss: 0.015980. Entropy: 0.888221.\n",
      "Iteration 1035: Policy loss: -0.027594. Value loss: 0.012544. Entropy: 0.882889.\n",
      "episode: 1080   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 308     evaluation reward: 4.76\n",
      "episode: 1081   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 515     evaluation reward: 4.79\n",
      "episode: 1082   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 280     evaluation reward: 4.75\n",
      "Training network. lr: 0.000242. clip: 0.096895\n",
      "Iteration 1036: Policy loss: 0.016512. Value loss: 0.025903. Entropy: 0.880358.\n",
      "Iteration 1037: Policy loss: -0.008725. Value loss: 0.017511. Entropy: 0.881469.\n",
      "Iteration 1038: Policy loss: -0.024695. Value loss: 0.013263. Entropy: 0.882605.\n",
      "episode: 1083   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 399     evaluation reward: 4.71\n",
      "episode: 1084   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 547     evaluation reward: 4.74\n",
      "Training network. lr: 0.000242. clip: 0.096886\n",
      "Iteration 1039: Policy loss: 0.012044. Value loss: 0.029698. Entropy: 0.933473.\n",
      "Iteration 1040: Policy loss: -0.003864. Value loss: 0.019203. Entropy: 0.924624.\n",
      "Iteration 1041: Policy loss: -0.020412. Value loss: 0.014789. Entropy: 0.934338.\n",
      "episode: 1085   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 329     evaluation reward: 4.74\n",
      "episode: 1086   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 468     evaluation reward: 4.74\n",
      "Training network. lr: 0.000242. clip: 0.096877\n",
      "Iteration 1042: Policy loss: 0.008801. Value loss: 0.023449. Entropy: 0.811442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1043: Policy loss: -0.010744. Value loss: 0.018480. Entropy: 0.812152.\n",
      "Iteration 1044: Policy loss: -0.023799. Value loss: 0.014360. Entropy: 0.809356.\n",
      "episode: 1087   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 558     evaluation reward: 4.78\n",
      "episode: 1088   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 258     evaluation reward: 4.73\n",
      "episode: 1089   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 421     evaluation reward: 4.74\n",
      "Training network. lr: 0.000242. clip: 0.096868\n",
      "Iteration 1045: Policy loss: 0.010563. Value loss: 0.022569. Entropy: 0.875363.\n",
      "Iteration 1046: Policy loss: -0.010523. Value loss: 0.014894. Entropy: 0.871239.\n",
      "Iteration 1047: Policy loss: -0.022097. Value loss: 0.011160. Entropy: 0.851476.\n",
      "episode: 1090   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 239     evaluation reward: 4.71\n",
      "episode: 1091   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 410     evaluation reward: 4.7\n",
      "episode: 1092   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 388     evaluation reward: 4.69\n",
      "Training network. lr: 0.000242. clip: 0.096859\n",
      "Iteration 1048: Policy loss: 0.005461. Value loss: 0.025340. Entropy: 0.903563.\n",
      "Iteration 1049: Policy loss: -0.013143. Value loss: 0.017289. Entropy: 0.897174.\n",
      "Iteration 1050: Policy loss: -0.028357. Value loss: 0.013692. Entropy: 0.899300.\n",
      "episode: 1093   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 445     evaluation reward: 4.7\n",
      "episode: 1094   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 419     evaluation reward: 4.68\n",
      "Training network. lr: 0.000242. clip: 0.096850\n",
      "Iteration 1051: Policy loss: 0.005996. Value loss: 0.017494. Entropy: 0.818313.\n",
      "Iteration 1052: Policy loss: -0.017600. Value loss: 0.012494. Entropy: 0.814485.\n",
      "Iteration 1053: Policy loss: -0.025902. Value loss: 0.010199. Entropy: 0.815301.\n",
      "episode: 1095   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 477     evaluation reward: 4.69\n",
      "episode: 1096   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 475     evaluation reward: 4.69\n",
      "episode: 1097   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 346     evaluation reward: 4.68\n",
      "Training network. lr: 0.000242. clip: 0.096841\n",
      "Iteration 1054: Policy loss: 0.009861. Value loss: 0.015435. Entropy: 0.841572.\n",
      "Iteration 1055: Policy loss: -0.011328. Value loss: 0.011354. Entropy: 0.834802.\n",
      "Iteration 1056: Policy loss: -0.020663. Value loss: 0.009296. Entropy: 0.827066.\n",
      "episode: 1098   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 295     evaluation reward: 4.58\n",
      "episode: 1099   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 210     evaluation reward: 4.53\n",
      "episode: 1100   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 393     evaluation reward: 4.54\n",
      "Training network. lr: 0.000242. clip: 0.096832\n",
      "Iteration 1057: Policy loss: 0.006588. Value loss: 0.019987. Entropy: 0.874755.\n",
      "Iteration 1058: Policy loss: -0.016081. Value loss: 0.015535. Entropy: 0.878702.\n",
      "Iteration 1059: Policy loss: -0.021535. Value loss: 0.012652. Entropy: 0.878261.\n",
      "episode: 1101   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 304     evaluation reward: 4.5\n",
      "episode: 1102   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 410     evaluation reward: 4.49\n",
      "episode: 1103   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 322     evaluation reward: 4.48\n",
      "Training network. lr: 0.000242. clip: 0.096823\n",
      "Iteration 1060: Policy loss: 0.010789. Value loss: 0.017359. Entropy: 0.908379.\n",
      "Iteration 1061: Policy loss: -0.012368. Value loss: 0.010826. Entropy: 0.912860.\n",
      "Iteration 1062: Policy loss: -0.024675. Value loss: 0.008756. Entropy: 0.910231.\n",
      "episode: 1104   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 370     evaluation reward: 4.47\n",
      "episode: 1105   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 285     evaluation reward: 4.43\n",
      "episode: 1106   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 369     evaluation reward: 4.42\n",
      "Training network. lr: 0.000242. clip: 0.096814\n",
      "Iteration 1063: Policy loss: 0.011590. Value loss: 0.017290. Entropy: 0.841371.\n",
      "Iteration 1064: Policy loss: -0.008716. Value loss: 0.013605. Entropy: 0.835989.\n",
      "Iteration 1065: Policy loss: -0.020305. Value loss: 0.010856. Entropy: 0.836257.\n",
      "episode: 1107   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 374     evaluation reward: 4.42\n",
      "episode: 1108   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 352     evaluation reward: 4.4\n",
      "episode: 1109   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 250     evaluation reward: 4.38\n",
      "Training network. lr: 0.000242. clip: 0.096805\n",
      "Iteration 1066: Policy loss: 0.009949. Value loss: 0.015271. Entropy: 0.854982.\n",
      "Iteration 1067: Policy loss: -0.015877. Value loss: 0.010848. Entropy: 0.842433.\n",
      "Iteration 1068: Policy loss: -0.022466. Value loss: 0.008639. Entropy: 0.848155.\n",
      "episode: 1110   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 343     evaluation reward: 4.37\n",
      "episode: 1111   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 505     evaluation reward: 4.37\n",
      "Training network. lr: 0.000242. clip: 0.096796\n",
      "Iteration 1069: Policy loss: 0.007390. Value loss: 0.019167. Entropy: 0.868112.\n",
      "Iteration 1070: Policy loss: -0.014875. Value loss: 0.013697. Entropy: 0.869633.\n",
      "Iteration 1071: Policy loss: -0.024930. Value loss: 0.010352. Entropy: 0.861676.\n",
      "episode: 1112   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 451     evaluation reward: 4.4\n",
      "episode: 1113   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 572     evaluation reward: 4.43\n",
      "episode: 1114   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 364     evaluation reward: 4.38\n",
      "Training network. lr: 0.000242. clip: 0.096787\n",
      "Iteration 1072: Policy loss: 0.012358. Value loss: 0.027142. Entropy: 0.893479.\n",
      "Iteration 1073: Policy loss: -0.006893. Value loss: 0.014071. Entropy: 0.901107.\n",
      "Iteration 1074: Policy loss: -0.024692. Value loss: 0.011334. Entropy: 0.895938.\n",
      "episode: 1115   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 511     evaluation reward: 4.39\n",
      "episode: 1116   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 408     evaluation reward: 4.39\n",
      "Training network. lr: 0.000242. clip: 0.096778\n",
      "Iteration 1075: Policy loss: 0.012959. Value loss: 0.021669. Entropy: 0.868980.\n",
      "Iteration 1076: Policy loss: -0.009003. Value loss: 0.015294. Entropy: 0.879366.\n",
      "Iteration 1077: Policy loss: -0.028857. Value loss: 0.012853. Entropy: 0.876923.\n",
      "episode: 1117   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 410     evaluation reward: 4.38\n",
      "episode: 1118   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 400     evaluation reward: 4.37\n",
      "Training network. lr: 0.000242. clip: 0.096769\n",
      "Iteration 1078: Policy loss: 0.004877. Value loss: 0.021619. Entropy: 0.822283.\n",
      "Iteration 1079: Policy loss: -0.011684. Value loss: 0.015995. Entropy: 0.805460.\n",
      "Iteration 1080: Policy loss: -0.024947. Value loss: 0.012731. Entropy: 0.802262.\n",
      "episode: 1119   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 359     evaluation reward: 4.37\n",
      "episode: 1120   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 432     evaluation reward: 4.4\n",
      "episode: 1121   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 396     evaluation reward: 4.4\n",
      "Training network. lr: 0.000242. clip: 0.096760\n",
      "Iteration 1081: Policy loss: 0.005302. Value loss: 0.022207. Entropy: 0.825045.\n",
      "Iteration 1082: Policy loss: -0.011961. Value loss: 0.016633. Entropy: 0.828313.\n",
      "Iteration 1083: Policy loss: -0.025716. Value loss: 0.013610. Entropy: 0.843751.\n",
      "episode: 1122   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 493     evaluation reward: 4.41\n",
      "episode: 1123   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 442     evaluation reward: 4.4\n",
      "Training network. lr: 0.000242. clip: 0.096751\n",
      "Iteration 1084: Policy loss: 0.005676. Value loss: 0.019111. Entropy: 0.917089.\n",
      "Iteration 1085: Policy loss: -0.015774. Value loss: 0.013422. Entropy: 0.900150.\n",
      "Iteration 1086: Policy loss: -0.031698. Value loss: 0.010395. Entropy: 0.900583.\n",
      "episode: 1124   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 458     evaluation reward: 4.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1125   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 413     evaluation reward: 4.42\n",
      "episode: 1126   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 381     evaluation reward: 4.4\n",
      "Training network. lr: 0.000242. clip: 0.096742\n",
      "Iteration 1087: Policy loss: 0.012565. Value loss: 0.020055. Entropy: 0.865530.\n",
      "Iteration 1088: Policy loss: -0.013439. Value loss: 0.016422. Entropy: 0.863944.\n",
      "Iteration 1089: Policy loss: -0.027970. Value loss: 0.011870. Entropy: 0.858428.\n",
      "episode: 1127   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 371     evaluation reward: 4.42\n",
      "episode: 1128   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 502     evaluation reward: 4.4\n",
      "Training network. lr: 0.000242. clip: 0.096733\n",
      "Iteration 1090: Policy loss: 0.009722. Value loss: 0.024910. Entropy: 0.975217.\n",
      "Iteration 1091: Policy loss: -0.013767. Value loss: 0.016980. Entropy: 0.967840.\n",
      "Iteration 1092: Policy loss: -0.029485. Value loss: 0.013794. Entropy: 0.960057.\n",
      "episode: 1129   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 462     evaluation reward: 4.4\n",
      "episode: 1130   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 467     evaluation reward: 4.41\n",
      "Training network. lr: 0.000242. clip: 0.096724\n",
      "Iteration 1093: Policy loss: 0.008081. Value loss: 0.017508. Entropy: 0.807803.\n",
      "Iteration 1094: Policy loss: -0.013106. Value loss: 0.011530. Entropy: 0.824395.\n",
      "Iteration 1095: Policy loss: -0.026347. Value loss: 0.009731. Entropy: 0.804778.\n",
      "episode: 1131   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 493     evaluation reward: 4.43\n",
      "episode: 1132   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 423     evaluation reward: 4.39\n",
      "episode: 1133   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 248     evaluation reward: 4.36\n",
      "Training network. lr: 0.000242. clip: 0.096715\n",
      "Iteration 1096: Policy loss: 0.011433. Value loss: 0.024242. Entropy: 0.848118.\n",
      "Iteration 1097: Policy loss: -0.008412. Value loss: 0.017585. Entropy: 0.838128.\n",
      "Iteration 1098: Policy loss: -0.023385. Value loss: 0.014342. Entropy: 0.826279.\n",
      "episode: 1134   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 447     evaluation reward: 4.37\n",
      "episode: 1135   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 373     evaluation reward: 4.36\n",
      "Training network. lr: 0.000242. clip: 0.096706\n",
      "Iteration 1099: Policy loss: 0.019185. Value loss: 0.027766. Entropy: 0.926016.\n",
      "Iteration 1100: Policy loss: -0.012955. Value loss: 0.019096. Entropy: 0.914091.\n",
      "Iteration 1101: Policy loss: -0.025028. Value loss: 0.014616. Entropy: 0.900605.\n",
      "episode: 1136   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 503     evaluation reward: 4.4\n",
      "episode: 1137   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 377     evaluation reward: 4.4\n",
      "Training network. lr: 0.000242. clip: 0.096697\n",
      "Iteration 1102: Policy loss: 0.011446. Value loss: 0.031321. Entropy: 0.850947.\n",
      "Iteration 1103: Policy loss: -0.014631. Value loss: 0.021953. Entropy: 0.854694.\n",
      "Iteration 1104: Policy loss: -0.030087. Value loss: 0.018116. Entropy: 0.852620.\n",
      "episode: 1138   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 498     evaluation reward: 4.42\n",
      "episode: 1139   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 375     evaluation reward: 4.43\n",
      "episode: 1140   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 368     evaluation reward: 4.43\n",
      "Training network. lr: 0.000242. clip: 0.096688\n",
      "Iteration 1105: Policy loss: 0.007506. Value loss: 0.021099. Entropy: 0.721465.\n",
      "Iteration 1106: Policy loss: -0.016783. Value loss: 0.013800. Entropy: 0.728227.\n",
      "Iteration 1107: Policy loss: -0.027605. Value loss: 0.010678. Entropy: 0.722895.\n",
      "episode: 1141   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 608     evaluation reward: 4.48\n",
      "episode: 1142   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 384     evaluation reward: 4.5\n",
      "Training network. lr: 0.000242. clip: 0.096679\n",
      "Iteration 1108: Policy loss: 0.014339. Value loss: 0.026122. Entropy: 0.851753.\n",
      "Iteration 1109: Policy loss: -0.010535. Value loss: 0.016590. Entropy: 0.870445.\n",
      "Iteration 1110: Policy loss: -0.019565. Value loss: 0.013142. Entropy: 0.868841.\n",
      "episode: 1143   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 412     evaluation reward: 4.52\n",
      "episode: 1144   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 500     evaluation reward: 4.56\n",
      "episode: 1145   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 313     evaluation reward: 4.54\n",
      "Training network. lr: 0.000242. clip: 0.096670\n",
      "Iteration 1111: Policy loss: 0.009938. Value loss: 0.024260. Entropy: 0.866650.\n",
      "Iteration 1112: Policy loss: -0.010804. Value loss: 0.014803. Entropy: 0.851408.\n",
      "Iteration 1113: Policy loss: -0.022388. Value loss: 0.011952. Entropy: 0.848054.\n",
      "episode: 1146   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 483     evaluation reward: 4.55\n",
      "episode: 1147   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 503     evaluation reward: 4.6\n",
      "Training network. lr: 0.000242. clip: 0.096661\n",
      "Iteration 1114: Policy loss: 0.008489. Value loss: 0.018809. Entropy: 0.820861.\n",
      "Iteration 1115: Policy loss: -0.013614. Value loss: 0.014244. Entropy: 0.830965.\n",
      "Iteration 1116: Policy loss: -0.026538. Value loss: 0.011575. Entropy: 0.822610.\n",
      "episode: 1148   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 348     evaluation reward: 4.56\n",
      "episode: 1149   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 252     evaluation reward: 4.57\n",
      "episode: 1150   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 400     evaluation reward: 4.56\n",
      "Training network. lr: 0.000242. clip: 0.096652\n",
      "Iteration 1117: Policy loss: 0.012416. Value loss: 0.019621. Entropy: 0.878545.\n",
      "Iteration 1118: Policy loss: -0.013051. Value loss: 0.013765. Entropy: 0.872600.\n",
      "Iteration 1119: Policy loss: -0.028319. Value loss: 0.011379. Entropy: 0.875208.\n",
      "episode: 1151   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 437     evaluation reward: 4.56\n",
      "episode: 1152   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 319     evaluation reward: 4.53\n",
      "Training network. lr: 0.000242. clip: 0.096643\n",
      "Iteration 1120: Policy loss: 0.012326. Value loss: 0.019116. Entropy: 0.881001.\n",
      "Iteration 1121: Policy loss: -0.015248. Value loss: 0.013995. Entropy: 0.879400.\n",
      "Iteration 1122: Policy loss: -0.025368. Value loss: 0.011288. Entropy: 0.867147.\n",
      "episode: 1153   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 423     evaluation reward: 4.5\n",
      "episode: 1154   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 272     evaluation reward: 4.47\n",
      "episode: 1155   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 278     evaluation reward: 4.46\n",
      "Training network. lr: 0.000242. clip: 0.096634\n",
      "Iteration 1123: Policy loss: 0.008956. Value loss: 0.021986. Entropy: 0.864195.\n",
      "Iteration 1124: Policy loss: -0.014838. Value loss: 0.014985. Entropy: 0.843197.\n",
      "Iteration 1125: Policy loss: -0.024042. Value loss: 0.012623. Entropy: 0.846306.\n",
      "episode: 1156   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 468     evaluation reward: 4.49\n",
      "episode: 1157   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 523     evaluation reward: 4.5\n",
      "Training network. lr: 0.000242. clip: 0.096625\n",
      "Iteration 1126: Policy loss: 0.011366. Value loss: 0.015761. Entropy: 0.848328.\n",
      "Iteration 1127: Policy loss: -0.015254. Value loss: 0.013061. Entropy: 0.857012.\n",
      "Iteration 1128: Policy loss: -0.025330. Value loss: 0.011079. Entropy: 0.856056.\n",
      "episode: 1158   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 512     evaluation reward: 4.51\n",
      "episode: 1159   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 405     evaluation reward: 4.52\n",
      "episode: 1160   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 395     evaluation reward: 4.54\n",
      "Training network. lr: 0.000242. clip: 0.096616\n",
      "Iteration 1129: Policy loss: 0.013574. Value loss: 0.017461. Entropy: 0.855166.\n",
      "Iteration 1130: Policy loss: -0.003123. Value loss: 0.012516. Entropy: 0.869452.\n",
      "Iteration 1131: Policy loss: -0.020391. Value loss: 0.010456. Entropy: 0.866378.\n",
      "episode: 1161   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 292     evaluation reward: 4.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1162   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 344     evaluation reward: 4.54\n",
      "episode: 1163   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 350     evaluation reward: 4.53\n",
      "Training network. lr: 0.000242. clip: 0.096607\n",
      "Iteration 1132: Policy loss: 0.012514. Value loss: 0.014448. Entropy: 0.819892.\n",
      "Iteration 1133: Policy loss: -0.010321. Value loss: 0.010322. Entropy: 0.799826.\n",
      "Iteration 1134: Policy loss: -0.020964. Value loss: 0.008806. Entropy: 0.799889.\n",
      "episode: 1164   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 565     evaluation reward: 4.58\n",
      "episode: 1165   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 448     evaluation reward: 4.59\n",
      "Training network. lr: 0.000241. clip: 0.096598\n",
      "Iteration 1135: Policy loss: 0.015421. Value loss: 0.046471. Entropy: 0.819187.\n",
      "Iteration 1136: Policy loss: -0.003158. Value loss: 0.035085. Entropy: 0.826189.\n",
      "Iteration 1137: Policy loss: -0.014274. Value loss: 0.030726. Entropy: 0.828854.\n",
      "episode: 1166   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 468     evaluation reward: 4.59\n",
      "episode: 1167   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 312     evaluation reward: 4.55\n",
      "episode: 1168   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 439     evaluation reward: 4.56\n",
      "Training network. lr: 0.000241. clip: 0.096589\n",
      "Iteration 1138: Policy loss: 0.007301. Value loss: 0.025987. Entropy: 0.919773.\n",
      "Iteration 1139: Policy loss: -0.010434. Value loss: 0.013167. Entropy: 0.914643.\n",
      "Iteration 1140: Policy loss: -0.026182. Value loss: 0.010710. Entropy: 0.911827.\n",
      "episode: 1169   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 388     evaluation reward: 4.54\n",
      "episode: 1170   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 340     evaluation reward: 4.5\n",
      "Training network. lr: 0.000241. clip: 0.096580\n",
      "Iteration 1141: Policy loss: 0.010415. Value loss: 0.014013. Entropy: 0.851367.\n",
      "Iteration 1142: Policy loss: -0.008390. Value loss: 0.010477. Entropy: 0.838107.\n",
      "Iteration 1143: Policy loss: -0.019847. Value loss: 0.008146. Entropy: 0.839815.\n",
      "episode: 1171   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 535     evaluation reward: 4.54\n",
      "episode: 1172   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 368     evaluation reward: 4.52\n",
      "episode: 1173   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 231     evaluation reward: 4.47\n",
      "Training network. lr: 0.000241. clip: 0.096571\n",
      "Iteration 1144: Policy loss: 0.008733. Value loss: 0.017206. Entropy: 0.883630.\n",
      "Iteration 1145: Policy loss: -0.011776. Value loss: 0.012689. Entropy: 0.877767.\n",
      "Iteration 1146: Policy loss: -0.022625. Value loss: 0.009799. Entropy: 0.868472.\n",
      "episode: 1174   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 599     evaluation reward: 4.52\n",
      "episode: 1175   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 520     evaluation reward: 4.54\n",
      "Training network. lr: 0.000241. clip: 0.096562\n",
      "Iteration 1147: Policy loss: 0.016404. Value loss: 0.036308. Entropy: 0.815801.\n",
      "Iteration 1148: Policy loss: -0.001974. Value loss: 0.015088. Entropy: 0.795947.\n",
      "Iteration 1149: Policy loss: -0.014150. Value loss: 0.012659. Entropy: 0.795344.\n",
      "episode: 1176   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 677     evaluation reward: 4.59\n",
      "episode: 1177   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 451     evaluation reward: 4.58\n",
      "Training network. lr: 0.000241. clip: 0.096553\n",
      "Iteration 1150: Policy loss: 0.008996. Value loss: 0.025342. Entropy: 0.950821.\n",
      "Iteration 1151: Policy loss: -0.016247. Value loss: 0.017837. Entropy: 0.938350.\n",
      "Iteration 1152: Policy loss: -0.029978. Value loss: 0.014031. Entropy: 0.937750.\n",
      "episode: 1178   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 463     evaluation reward: 4.59\n",
      "episode: 1179   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 383     evaluation reward: 4.62\n",
      "Training network. lr: 0.000241. clip: 0.096544\n",
      "Iteration 1153: Policy loss: 0.010923. Value loss: 0.020593. Entropy: 0.812261.\n",
      "Iteration 1154: Policy loss: -0.010221. Value loss: 0.014882. Entropy: 0.824965.\n",
      "Iteration 1155: Policy loss: -0.018858. Value loss: 0.012461. Entropy: 0.814238.\n",
      "episode: 1180   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 464     evaluation reward: 4.66\n",
      "episode: 1181   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 440     evaluation reward: 4.65\n",
      "Training network. lr: 0.000241. clip: 0.096535\n",
      "Iteration 1156: Policy loss: 0.010807. Value loss: 0.023912. Entropy: 0.891875.\n",
      "Iteration 1157: Policy loss: -0.003556. Value loss: 0.016998. Entropy: 0.888294.\n",
      "Iteration 1158: Policy loss: -0.022735. Value loss: 0.013678. Entropy: 0.888026.\n",
      "episode: 1182   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 399     evaluation reward: 4.68\n",
      "episode: 1183   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 295     evaluation reward: 4.67\n",
      "episode: 1184   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 459     evaluation reward: 4.66\n",
      "Training network. lr: 0.000241. clip: 0.096526\n",
      "Iteration 1159: Policy loss: 0.012846. Value loss: 0.026810. Entropy: 0.851522.\n",
      "Iteration 1160: Policy loss: -0.012819. Value loss: 0.019063. Entropy: 0.862615.\n",
      "Iteration 1161: Policy loss: -0.025181. Value loss: 0.014697. Entropy: 0.853145.\n",
      "episode: 1185   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 345     evaluation reward: 4.67\n",
      "episode: 1186   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 290     evaluation reward: 4.65\n",
      "episode: 1187   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 496     evaluation reward: 4.64\n",
      "Training network. lr: 0.000241. clip: 0.096517\n",
      "Iteration 1162: Policy loss: 0.009765. Value loss: 0.021098. Entropy: 0.888746.\n",
      "Iteration 1163: Policy loss: -0.012350. Value loss: 0.014873. Entropy: 0.885250.\n",
      "Iteration 1164: Policy loss: -0.024069. Value loss: 0.013635. Entropy: 0.875466.\n",
      "episode: 1188   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 276     evaluation reward: 4.65\n",
      "episode: 1189   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 303     evaluation reward: 4.63\n",
      "episode: 1190   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 384     evaluation reward: 4.65\n",
      "Training network. lr: 0.000241. clip: 0.096508\n",
      "Iteration 1165: Policy loss: 0.010105. Value loss: 0.014718. Entropy: 0.886951.\n",
      "Iteration 1166: Policy loss: -0.015987. Value loss: 0.011526. Entropy: 0.901292.\n",
      "Iteration 1167: Policy loss: -0.024650. Value loss: 0.009333. Entropy: 0.889601.\n",
      "episode: 1191   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 302     evaluation reward: 4.64\n",
      "episode: 1192   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 246     evaluation reward: 4.62\n",
      "episode: 1193   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 522     evaluation reward: 4.62\n",
      "Training network. lr: 0.000241. clip: 0.096499\n",
      "Iteration 1168: Policy loss: 0.011038. Value loss: 0.014077. Entropy: 0.875026.\n",
      "Iteration 1169: Policy loss: -0.012284. Value loss: 0.011314. Entropy: 0.869132.\n",
      "Iteration 1170: Policy loss: -0.022183. Value loss: 0.008250. Entropy: 0.875084.\n",
      "episode: 1194   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 447     evaluation reward: 4.62\n",
      "now time :  2018-12-26 12:40:53.962514\n",
      "episode: 1195   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 547     evaluation reward: 4.63\n",
      "Training network. lr: 0.000241. clip: 0.096490\n",
      "Iteration 1171: Policy loss: 0.012109. Value loss: 0.024215. Entropy: 0.897682.\n",
      "Iteration 1172: Policy loss: -0.013178. Value loss: 0.015409. Entropy: 0.891366.\n",
      "Iteration 1173: Policy loss: -0.029629. Value loss: 0.012612. Entropy: 0.884933.\n",
      "episode: 1196   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 377     evaluation reward: 4.62\n",
      "episode: 1197   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 502     evaluation reward: 4.64\n",
      "Training network. lr: 0.000241. clip: 0.096481\n",
      "Iteration 1174: Policy loss: 0.009555. Value loss: 0.023759. Entropy: 0.899079.\n",
      "Iteration 1175: Policy loss: -0.018464. Value loss: 0.017090. Entropy: 0.899622.\n",
      "Iteration 1176: Policy loss: -0.025180. Value loss: 0.014144. Entropy: 0.899497.\n",
      "episode: 1198   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 460     evaluation reward: 4.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1199   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 524     evaluation reward: 4.71\n",
      "episode: 1200   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 342     evaluation reward: 4.7\n",
      "Training network. lr: 0.000241. clip: 0.096472\n",
      "Iteration 1177: Policy loss: 0.008438. Value loss: 0.018523. Entropy: 0.811035.\n",
      "Iteration 1178: Policy loss: -0.006226. Value loss: 0.013263. Entropy: 0.807521.\n",
      "Iteration 1179: Policy loss: -0.022941. Value loss: 0.011607. Entropy: 0.805975.\n",
      "episode: 1201   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 661     evaluation reward: 4.77\n",
      "Training network. lr: 0.000241. clip: 0.096463\n",
      "Iteration 1180: Policy loss: 0.009001. Value loss: 0.017899. Entropy: 0.804773.\n",
      "Iteration 1181: Policy loss: -0.008245. Value loss: 0.012809. Entropy: 0.796705.\n",
      "Iteration 1182: Policy loss: -0.022684. Value loss: 0.010263. Entropy: 0.791877.\n",
      "episode: 1202   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 451     evaluation reward: 4.78\n",
      "episode: 1203   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 400     evaluation reward: 4.79\n",
      "episode: 1204   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 503     evaluation reward: 4.82\n",
      "Training network. lr: 0.000241. clip: 0.096454\n",
      "Iteration 1183: Policy loss: 0.010539. Value loss: 0.016092. Entropy: 0.864785.\n",
      "Iteration 1184: Policy loss: -0.006014. Value loss: 0.011919. Entropy: 0.867579.\n",
      "Iteration 1185: Policy loss: -0.022643. Value loss: 0.009557. Entropy: 0.848436.\n",
      "episode: 1205   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 497     evaluation reward: 4.86\n",
      "episode: 1206   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 422     evaluation reward: 4.87\n",
      "Training network. lr: 0.000241. clip: 0.096445\n",
      "Iteration 1186: Policy loss: 0.013243. Value loss: 0.029860. Entropy: 0.765661.\n",
      "Iteration 1187: Policy loss: -0.009282. Value loss: 0.019597. Entropy: 0.764391.\n",
      "Iteration 1188: Policy loss: -0.024929. Value loss: 0.015753. Entropy: 0.764241.\n",
      "episode: 1207   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 306     evaluation reward: 4.86\n",
      "episode: 1208   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 520     evaluation reward: 4.89\n",
      "episode: 1209   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 309     evaluation reward: 4.9\n",
      "Training network. lr: 0.000241. clip: 0.096436\n",
      "Iteration 1189: Policy loss: 0.014406. Value loss: 0.013867. Entropy: 0.890663.\n",
      "Iteration 1190: Policy loss: -0.011096. Value loss: 0.010030. Entropy: 0.885539.\n",
      "Iteration 1191: Policy loss: -0.028244. Value loss: 0.007853. Entropy: 0.874954.\n",
      "episode: 1210   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 390     evaluation reward: 4.9\n",
      "episode: 1211   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 293     evaluation reward: 4.87\n",
      "Training network. lr: 0.000241. clip: 0.096427\n",
      "Iteration 1192: Policy loss: 0.012488. Value loss: 0.016100. Entropy: 0.829605.\n",
      "Iteration 1193: Policy loss: -0.016463. Value loss: 0.012280. Entropy: 0.815593.\n",
      "Iteration 1194: Policy loss: -0.027625. Value loss: 0.010235. Entropy: 0.810153.\n",
      "episode: 1212   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 389     evaluation reward: 4.87\n",
      "episode: 1213   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 509     evaluation reward: 4.86\n",
      "Training network. lr: 0.000241. clip: 0.096418\n",
      "Iteration 1195: Policy loss: 0.020210. Value loss: 0.017452. Entropy: 0.809756.\n",
      "Iteration 1196: Policy loss: -0.009534. Value loss: 0.013652. Entropy: 0.793418.\n",
      "Iteration 1197: Policy loss: -0.025945. Value loss: 0.010586. Entropy: 0.797442.\n",
      "episode: 1214   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 520     evaluation reward: 4.89\n",
      "episode: 1215   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 454     evaluation reward: 4.89\n",
      "episode: 1216   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 409     evaluation reward: 4.9\n",
      "Training network. lr: 0.000241. clip: 0.096409\n",
      "Iteration 1198: Policy loss: 0.006207. Value loss: 0.018472. Entropy: 0.766060.\n",
      "Iteration 1199: Policy loss: -0.007757. Value loss: 0.014203. Entropy: 0.772334.\n",
      "Iteration 1200: Policy loss: -0.018353. Value loss: 0.012057. Entropy: 0.756944.\n",
      "episode: 1217   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 387     evaluation reward: 4.9\n",
      "episode: 1218   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 552     evaluation reward: 4.93\n",
      "Training network. lr: 0.000241. clip: 0.096400\n",
      "Iteration 1201: Policy loss: 0.009140. Value loss: 0.019870. Entropy: 0.813410.\n",
      "Iteration 1202: Policy loss: -0.014900. Value loss: 0.014352. Entropy: 0.803237.\n",
      "Iteration 1203: Policy loss: -0.026174. Value loss: 0.011551. Entropy: 0.798896.\n",
      "episode: 1219   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 602     evaluation reward: 4.99\n",
      "episode: 1220   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 474     evaluation reward: 5.0\n",
      "Training network. lr: 0.000241. clip: 0.096391\n",
      "Iteration 1204: Policy loss: 0.006027. Value loss: 0.030247. Entropy: 0.915249.\n",
      "Iteration 1205: Policy loss: -0.013893. Value loss: 0.020816. Entropy: 0.926100.\n",
      "Iteration 1206: Policy loss: -0.021600. Value loss: 0.017489. Entropy: 0.908931.\n",
      "episode: 1221   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 308     evaluation reward: 4.99\n",
      "episode: 1222   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 395     evaluation reward: 4.98\n",
      "episode: 1223   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 4.97\n",
      "Training network. lr: 0.000241. clip: 0.096382\n",
      "Iteration 1207: Policy loss: 0.012932. Value loss: 0.020543. Entropy: 0.840173.\n",
      "Iteration 1208: Policy loss: -0.013748. Value loss: 0.015240. Entropy: 0.826848.\n",
      "Iteration 1209: Policy loss: -0.025971. Value loss: 0.012708. Entropy: 0.823393.\n",
      "episode: 1224   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 426     evaluation reward: 4.97\n",
      "episode: 1225   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 584     evaluation reward: 5.0\n",
      "Training network. lr: 0.000241. clip: 0.096373\n",
      "Iteration 1210: Policy loss: 0.006522. Value loss: 0.022012. Entropy: 0.953691.\n",
      "Iteration 1211: Policy loss: -0.010900. Value loss: 0.016250. Entropy: 0.962233.\n",
      "Iteration 1212: Policy loss: -0.025693. Value loss: 0.012749. Entropy: 0.962861.\n",
      "episode: 1226   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 466     evaluation reward: 5.01\n",
      "episode: 1227   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 458     evaluation reward: 5.02\n",
      "Training network. lr: 0.000241. clip: 0.096364\n",
      "Iteration 1213: Policy loss: 0.001310. Value loss: 0.019645. Entropy: 0.950265.\n",
      "Iteration 1214: Policy loss: -0.015632. Value loss: 0.014969. Entropy: 0.941988.\n",
      "Iteration 1215: Policy loss: -0.028164. Value loss: 0.012576. Entropy: 0.938521.\n",
      "episode: 1228   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 518     evaluation reward: 5.03\n",
      "episode: 1229   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 390     evaluation reward: 5.03\n",
      "episode: 1230   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 219     evaluation reward: 4.98\n",
      "Training network. lr: 0.000241. clip: 0.096355\n",
      "Iteration 1216: Policy loss: 0.006195. Value loss: 0.033561. Entropy: 0.891517.\n",
      "Iteration 1217: Policy loss: -0.008980. Value loss: 0.020521. Entropy: 0.885473.\n",
      "Iteration 1218: Policy loss: -0.025491. Value loss: 0.016487. Entropy: 0.885628.\n",
      "episode: 1231   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 578     evaluation reward: 5.0\n",
      "episode: 1232   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 358     evaluation reward: 4.99\n",
      "Training network. lr: 0.000241. clip: 0.096346\n",
      "Iteration 1219: Policy loss: 0.009505. Value loss: 0.022165. Entropy: 0.844226.\n",
      "Iteration 1220: Policy loss: -0.011359. Value loss: 0.017571. Entropy: 0.829845.\n",
      "Iteration 1221: Policy loss: -0.025617. Value loss: 0.013239. Entropy: 0.829776.\n",
      "episode: 1233   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 409     evaluation reward: 5.01\n",
      "episode: 1234   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 283     evaluation reward: 4.99\n",
      "episode: 1235   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 267     evaluation reward: 4.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000241. clip: 0.096337\n",
      "Iteration 1222: Policy loss: 0.008141. Value loss: 0.019961. Entropy: 0.774977.\n",
      "Iteration 1223: Policy loss: -0.013692. Value loss: 0.015923. Entropy: 0.773141.\n",
      "Iteration 1224: Policy loss: -0.019706. Value loss: 0.013590. Entropy: 0.762391.\n",
      "episode: 1236   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 351     evaluation reward: 4.95\n",
      "episode: 1237   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 421     evaluation reward: 4.96\n",
      "episode: 1238   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 501     evaluation reward: 4.97\n",
      "Training network. lr: 0.000241. clip: 0.096328\n",
      "Iteration 1225: Policy loss: 0.007262. Value loss: 0.018128. Entropy: 0.810985.\n",
      "Iteration 1226: Policy loss: -0.010516. Value loss: 0.011577. Entropy: 0.807455.\n",
      "Iteration 1227: Policy loss: -0.027913. Value loss: 0.008950. Entropy: 0.810214.\n",
      "episode: 1239   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 331     evaluation reward: 4.97\n",
      "episode: 1240   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 373     evaluation reward: 4.97\n",
      "Training network. lr: 0.000241. clip: 0.096319\n",
      "Iteration 1228: Policy loss: 0.006220. Value loss: 0.014710. Entropy: 0.764552.\n",
      "Iteration 1229: Policy loss: -0.010921. Value loss: 0.009805. Entropy: 0.763216.\n",
      "Iteration 1230: Policy loss: -0.020785. Value loss: 0.008819. Entropy: 0.759850.\n",
      "episode: 1241   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 352     evaluation reward: 4.93\n",
      "episode: 1242   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 392     evaluation reward: 4.93\n",
      "episode: 1243   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 600     evaluation reward: 4.95\n",
      "Training network. lr: 0.000241. clip: 0.096310\n",
      "Iteration 1231: Policy loss: 0.009660. Value loss: 0.023201. Entropy: 0.882137.\n",
      "Iteration 1232: Policy loss: -0.010248. Value loss: 0.015267. Entropy: 0.871664.\n",
      "Iteration 1233: Policy loss: -0.025496. Value loss: 0.012211. Entropy: 0.876829.\n",
      "episode: 1244   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 365     evaluation reward: 4.92\n",
      "episode: 1245   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 260     evaluation reward: 4.91\n",
      "Training network. lr: 0.000241. clip: 0.096301\n",
      "Iteration 1234: Policy loss: 0.007627. Value loss: 0.018224. Entropy: 0.762493.\n",
      "Iteration 1235: Policy loss: -0.008396. Value loss: 0.014114. Entropy: 0.739565.\n",
      "Iteration 1236: Policy loss: -0.022162. Value loss: 0.010859. Entropy: 0.752499.\n",
      "episode: 1246   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 547     evaluation reward: 4.91\n",
      "episode: 1247   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 203     evaluation reward: 4.86\n",
      "episode: 1248   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 315     evaluation reward: 4.86\n",
      "Training network. lr: 0.000241. clip: 0.096292\n",
      "Iteration 1237: Policy loss: 0.009923. Value loss: 0.025372. Entropy: 0.862456.\n",
      "Iteration 1238: Policy loss: -0.012780. Value loss: 0.017883. Entropy: 0.858127.\n",
      "Iteration 1239: Policy loss: -0.019235. Value loss: 0.014418. Entropy: 0.853587.\n",
      "episode: 1249   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 550     evaluation reward: 4.91\n",
      "episode: 1250   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 378     evaluation reward: 4.9\n",
      "Training network. lr: 0.000241. clip: 0.096283\n",
      "Iteration 1240: Policy loss: 0.011756. Value loss: 0.018575. Entropy: 0.821191.\n",
      "Iteration 1241: Policy loss: -0.012324. Value loss: 0.014282. Entropy: 0.821867.\n",
      "Iteration 1242: Policy loss: -0.025587. Value loss: 0.012112. Entropy: 0.822235.\n",
      "episode: 1251   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 499     evaluation reward: 4.91\n",
      "episode: 1252   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 494     evaluation reward: 4.94\n",
      "episode: 1253   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 421     evaluation reward: 4.95\n",
      "Training network. lr: 0.000241. clip: 0.096274\n",
      "Iteration 1243: Policy loss: 0.010901. Value loss: 0.023156. Entropy: 0.729478.\n",
      "Iteration 1244: Policy loss: -0.009478. Value loss: 0.015436. Entropy: 0.727980.\n",
      "Iteration 1245: Policy loss: -0.020259. Value loss: 0.012512. Entropy: 0.717100.\n",
      "episode: 1254   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 354     evaluation reward: 4.97\n",
      "episode: 1255   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 431     evaluation reward: 4.99\n",
      "Training network. lr: 0.000241. clip: 0.096265\n",
      "Iteration 1246: Policy loss: 0.011527. Value loss: 0.017967. Entropy: 0.731412.\n",
      "Iteration 1247: Policy loss: -0.007580. Value loss: 0.014099. Entropy: 0.728517.\n",
      "Iteration 1248: Policy loss: -0.017926. Value loss: 0.010942. Entropy: 0.729010.\n",
      "episode: 1256   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 508     evaluation reward: 4.99\n",
      "episode: 1257   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 316     evaluation reward: 4.96\n",
      "episode: 1258   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 379     evaluation reward: 4.94\n",
      "Training network. lr: 0.000241. clip: 0.096256\n",
      "Iteration 1249: Policy loss: 0.008388. Value loss: 0.020532. Entropy: 0.823452.\n",
      "Iteration 1250: Policy loss: -0.015117. Value loss: 0.015549. Entropy: 0.820310.\n",
      "Iteration 1251: Policy loss: -0.022246. Value loss: 0.012978. Entropy: 0.815051.\n",
      "episode: 1259   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 346     evaluation reward: 4.93\n",
      "episode: 1260   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 347     evaluation reward: 4.92\n",
      "episode: 1261   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 250     evaluation reward: 4.91\n",
      "Training network. lr: 0.000241. clip: 0.096247\n",
      "Iteration 1252: Policy loss: 0.007907. Value loss: 0.013202. Entropy: 0.778317.\n",
      "Iteration 1253: Policy loss: -0.009639. Value loss: 0.010150. Entropy: 0.783248.\n",
      "Iteration 1254: Policy loss: -0.019479. Value loss: 0.008672. Entropy: 0.778858.\n",
      "episode: 1262   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 440     evaluation reward: 4.93\n",
      "episode: 1263   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 333     evaluation reward: 4.93\n",
      "episode: 1264   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 4.87\n",
      "Training network. lr: 0.000241. clip: 0.096238\n",
      "Iteration 1255: Policy loss: 0.014454. Value loss: 0.023464. Entropy: 0.779740.\n",
      "Iteration 1256: Policy loss: -0.007349. Value loss: 0.013372. Entropy: 0.776207.\n",
      "Iteration 1257: Policy loss: -0.020673. Value loss: 0.011453. Entropy: 0.787240.\n",
      "episode: 1265   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 255     evaluation reward: 4.84\n",
      "episode: 1266   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 356     evaluation reward: 4.83\n",
      "episode: 1267   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 425     evaluation reward: 4.85\n",
      "Training network. lr: 0.000241. clip: 0.096229\n",
      "Iteration 1258: Policy loss: 0.014006. Value loss: 0.017186. Entropy: 0.823492.\n",
      "Iteration 1259: Policy loss: -0.010615. Value loss: 0.013927. Entropy: 0.815965.\n",
      "Iteration 1260: Policy loss: -0.024699. Value loss: 0.011718. Entropy: 0.814890.\n",
      "episode: 1268   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 447     evaluation reward: 4.88\n",
      "episode: 1269   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 570     evaluation reward: 4.91\n",
      "Training network. lr: 0.000241. clip: 0.096220\n",
      "Iteration 1261: Policy loss: 0.009276. Value loss: 0.041162. Entropy: 0.881412.\n",
      "Iteration 1262: Policy loss: -0.010289. Value loss: 0.031194. Entropy: 0.886996.\n",
      "Iteration 1263: Policy loss: -0.018674. Value loss: 0.025395. Entropy: 0.885891.\n",
      "episode: 1270   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 274     evaluation reward: 4.9\n",
      "episode: 1271   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 412     evaluation reward: 4.88\n",
      "episode: 1272   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 351     evaluation reward: 4.88\n",
      "Training network. lr: 0.000241. clip: 0.096211\n",
      "Iteration 1264: Policy loss: 0.006003. Value loss: 0.019332. Entropy: 0.934981.\n",
      "Iteration 1265: Policy loss: -0.014184. Value loss: 0.013147. Entropy: 0.934524.\n",
      "Iteration 1266: Policy loss: -0.022342. Value loss: 0.011111. Entropy: 0.918921.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1273   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 398     evaluation reward: 4.91\n",
      "episode: 1274   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 246     evaluation reward: 4.85\n",
      "Training network. lr: 0.000241. clip: 0.096202\n",
      "Iteration 1267: Policy loss: 0.009608. Value loss: 0.017515. Entropy: 0.871477.\n",
      "Iteration 1268: Policy loss: -0.009360. Value loss: 0.012639. Entropy: 0.889411.\n",
      "Iteration 1269: Policy loss: -0.028124. Value loss: 0.010757. Entropy: 0.877365.\n",
      "episode: 1275   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 477     evaluation reward: 4.84\n",
      "episode: 1276   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 572     evaluation reward: 4.81\n",
      "Training network. lr: 0.000240. clip: 0.096193\n",
      "Iteration 1270: Policy loss: 0.033792. Value loss: 0.034226. Entropy: 0.900452.\n",
      "Iteration 1271: Policy loss: 0.004066. Value loss: 0.011120. Entropy: 0.894218.\n",
      "Iteration 1272: Policy loss: -0.014705. Value loss: 0.010510. Entropy: 0.888059.\n",
      "episode: 1277   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 434     evaluation reward: 4.79\n",
      "episode: 1278   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 357     evaluation reward: 4.76\n",
      "episode: 1279   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 436     evaluation reward: 4.76\n",
      "Training network. lr: 0.000240. clip: 0.096184\n",
      "Iteration 1273: Policy loss: 0.009078. Value loss: 0.019239. Entropy: 0.846588.\n",
      "Iteration 1274: Policy loss: -0.009764. Value loss: 0.014175. Entropy: 0.826542.\n",
      "Iteration 1275: Policy loss: -0.024810. Value loss: 0.011643. Entropy: 0.835219.\n",
      "episode: 1280   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 413     evaluation reward: 4.75\n",
      "episode: 1281   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 333     evaluation reward: 4.74\n",
      "episode: 1282   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 464     evaluation reward: 4.75\n",
      "Training network. lr: 0.000240. clip: 0.096175\n",
      "Iteration 1276: Policy loss: 0.012251. Value loss: 0.015540. Entropy: 0.837859.\n",
      "Iteration 1277: Policy loss: -0.009770. Value loss: 0.011337. Entropy: 0.838306.\n",
      "Iteration 1278: Policy loss: -0.021802. Value loss: 0.009285. Entropy: 0.817090.\n",
      "episode: 1283   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 428     evaluation reward: 4.78\n",
      "episode: 1284   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 326     evaluation reward: 4.75\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1279: Policy loss: 0.011982. Value loss: 0.016082. Entropy: 0.806890.\n",
      "Iteration 1280: Policy loss: -0.009677. Value loss: 0.010629. Entropy: 0.805653.\n",
      "Iteration 1281: Policy loss: -0.020921. Value loss: 0.008700. Entropy: 0.812862.\n",
      "episode: 1285   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 323     evaluation reward: 4.74\n",
      "episode: 1286   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 383     evaluation reward: 4.75\n",
      "episode: 1287   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 291     evaluation reward: 4.72\n",
      "episode: 1288   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 281     evaluation reward: 4.71\n",
      "Training network. lr: 0.000240. clip: 0.096157\n",
      "Iteration 1282: Policy loss: 0.008912. Value loss: 0.018818. Entropy: 0.818342.\n",
      "Iteration 1283: Policy loss: -0.011256. Value loss: 0.014522. Entropy: 0.824761.\n",
      "Iteration 1284: Policy loss: -0.015818. Value loss: 0.012311. Entropy: 0.811276.\n",
      "episode: 1289   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 455     evaluation reward: 4.73\n",
      "episode: 1290   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 413     evaluation reward: 4.74\n",
      "Training network. lr: 0.000240. clip: 0.096148\n",
      "Iteration 1285: Policy loss: 0.012142. Value loss: 0.014583. Entropy: 0.896856.\n",
      "Iteration 1286: Policy loss: -0.010567. Value loss: 0.010824. Entropy: 0.882361.\n",
      "Iteration 1287: Policy loss: -0.023503. Value loss: 0.008972. Entropy: 0.893425.\n",
      "episode: 1291   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 286     evaluation reward: 4.74\n",
      "episode: 1292   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 379     evaluation reward: 4.76\n",
      "episode: 1293   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 246     evaluation reward: 4.72\n",
      "Training network. lr: 0.000240. clip: 0.096139\n",
      "Iteration 1288: Policy loss: 0.009170. Value loss: 0.017002. Entropy: 0.871815.\n",
      "Iteration 1289: Policy loss: -0.009166. Value loss: 0.013545. Entropy: 0.881696.\n",
      "Iteration 1290: Policy loss: -0.020580. Value loss: 0.012015. Entropy: 0.879547.\n",
      "episode: 1294   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 379     evaluation reward: 4.71\n",
      "episode: 1295   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 244     evaluation reward: 4.66\n",
      "episode: 1296   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 666     evaluation reward: 4.71\n",
      "Training network. lr: 0.000240. clip: 0.096130\n",
      "Iteration 1291: Policy loss: 0.009152. Value loss: 0.021377. Entropy: 0.865095.\n",
      "Iteration 1292: Policy loss: -0.010619. Value loss: 0.014282. Entropy: 0.836670.\n",
      "Iteration 1293: Policy loss: -0.026180. Value loss: 0.011589. Entropy: 0.849553.\n",
      "episode: 1297   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 316     evaluation reward: 4.68\n",
      "episode: 1298   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 224     evaluation reward: 4.64\n",
      "episode: 1299   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 220     evaluation reward: 4.59\n",
      "Training network. lr: 0.000240. clip: 0.096121\n",
      "Iteration 1294: Policy loss: 0.010310. Value loss: 0.016452. Entropy: 0.859520.\n",
      "Iteration 1295: Policy loss: -0.008902. Value loss: 0.012697. Entropy: 0.863110.\n",
      "Iteration 1296: Policy loss: -0.022582. Value loss: 0.010816. Entropy: 0.859392.\n",
      "episode: 1300   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 437     evaluation reward: 4.61\n",
      "episode: 1301   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 369     evaluation reward: 4.56\n",
      "episode: 1302   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 449     evaluation reward: 4.56\n",
      "Training network. lr: 0.000240. clip: 0.096112\n",
      "Iteration 1297: Policy loss: 0.007252. Value loss: 0.015527. Entropy: 0.843690.\n",
      "Iteration 1298: Policy loss: -0.008841. Value loss: 0.013070. Entropy: 0.842629.\n",
      "Iteration 1299: Policy loss: -0.018199. Value loss: 0.012062. Entropy: 0.836694.\n",
      "episode: 1303   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 246     evaluation reward: 4.54\n",
      "episode: 1304   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 470     evaluation reward: 4.53\n",
      "Training network. lr: 0.000240. clip: 0.096103\n",
      "Iteration 1300: Policy loss: 0.011696. Value loss: 0.018534. Entropy: 0.842024.\n",
      "Iteration 1301: Policy loss: -0.009801. Value loss: 0.013103. Entropy: 0.832495.\n",
      "Iteration 1302: Policy loss: -0.024291. Value loss: 0.011177. Entropy: 0.836926.\n",
      "episode: 1305   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 450     evaluation reward: 4.51\n",
      "episode: 1306   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 619     evaluation reward: 4.54\n",
      "Training network. lr: 0.000240. clip: 0.096094\n",
      "Iteration 1303: Policy loss: 0.010864. Value loss: 0.017541. Entropy: 0.813420.\n",
      "Iteration 1304: Policy loss: -0.008920. Value loss: 0.011939. Entropy: 0.807290.\n",
      "Iteration 1305: Policy loss: -0.019653. Value loss: 0.010250. Entropy: 0.805632.\n",
      "episode: 1307   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 527     evaluation reward: 4.58\n",
      "episode: 1308   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 414     evaluation reward: 4.56\n",
      "episode: 1309   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 219     evaluation reward: 4.54\n",
      "Training network. lr: 0.000240. clip: 0.096085\n",
      "Iteration 1306: Policy loss: 0.007484. Value loss: 0.020720. Entropy: 0.846762.\n",
      "Iteration 1307: Policy loss: -0.010610. Value loss: 0.014632. Entropy: 0.841073.\n",
      "Iteration 1308: Policy loss: -0.015913. Value loss: 0.012738. Entropy: 0.841516.\n",
      "episode: 1310   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 467     evaluation reward: 4.55\n",
      "episode: 1311   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 582     evaluation reward: 4.58\n",
      "Training network. lr: 0.000240. clip: 0.096076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1309: Policy loss: 0.011047. Value loss: 0.020501. Entropy: 0.809393.\n",
      "Iteration 1310: Policy loss: -0.012759. Value loss: 0.015155. Entropy: 0.810217.\n",
      "Iteration 1311: Policy loss: -0.021661. Value loss: 0.012595. Entropy: 0.801782.\n",
      "episode: 1312   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 568     evaluation reward: 4.6\n",
      "episode: 1313   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 326     evaluation reward: 4.57\n",
      "Training network. lr: 0.000240. clip: 0.096067\n",
      "Iteration 1312: Policy loss: 0.014457. Value loss: 0.016546. Entropy: 0.706377.\n",
      "Iteration 1313: Policy loss: -0.005671. Value loss: 0.014155. Entropy: 0.697767.\n",
      "Iteration 1314: Policy loss: -0.019699. Value loss: 0.010892. Entropy: 0.696981.\n",
      "episode: 1314   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 497     evaluation reward: 4.55\n",
      "episode: 1315   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 471     evaluation reward: 4.54\n",
      "Training network. lr: 0.000240. clip: 0.096058\n",
      "Iteration 1315: Policy loss: 0.005584. Value loss: 0.013800. Entropy: 0.786356.\n",
      "Iteration 1316: Policy loss: -0.012540. Value loss: 0.009917. Entropy: 0.789916.\n",
      "Iteration 1317: Policy loss: -0.022647. Value loss: 0.007726. Entropy: 0.782399.\n",
      "episode: 1316   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 415     evaluation reward: 4.54\n",
      "now time :  2018-12-26 12:45:26.304437\n",
      "episode: 1317   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 450     evaluation reward: 4.54\n",
      "episode: 1318   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 435     evaluation reward: 4.52\n",
      "Training network. lr: 0.000240. clip: 0.096049\n",
      "Iteration 1318: Policy loss: 0.011666. Value loss: 0.017862. Entropy: 0.761435.\n",
      "Iteration 1319: Policy loss: -0.009957. Value loss: 0.012981. Entropy: 0.748938.\n",
      "Iteration 1320: Policy loss: -0.018501. Value loss: 0.011123. Entropy: 0.744767.\n",
      "episode: 1319   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 452     evaluation reward: 4.47\n",
      "episode: 1320   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 334     evaluation reward: 4.44\n",
      "episode: 1321   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 317     evaluation reward: 4.44\n",
      "Training network. lr: 0.000240. clip: 0.096040\n",
      "Iteration 1321: Policy loss: 0.005647. Value loss: 0.013306. Entropy: 0.795629.\n",
      "Iteration 1322: Policy loss: -0.011989. Value loss: 0.009980. Entropy: 0.778968.\n",
      "Iteration 1323: Policy loss: -0.022121. Value loss: 0.009007. Entropy: 0.779338.\n",
      "episode: 1322   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 386     evaluation reward: 4.43\n",
      "episode: 1323   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 327     evaluation reward: 4.41\n",
      "Training network. lr: 0.000240. clip: 0.096031\n",
      "Iteration 1324: Policy loss: 0.006691. Value loss: 0.013673. Entropy: 0.722604.\n",
      "Iteration 1325: Policy loss: -0.010945. Value loss: 0.012032. Entropy: 0.719316.\n",
      "Iteration 1326: Policy loss: -0.021504. Value loss: 0.009208. Entropy: 0.713446.\n",
      "episode: 1324   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 574     evaluation reward: 4.42\n",
      "episode: 1325   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 431     evaluation reward: 4.42\n",
      "episode: 1326   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 213     evaluation reward: 4.38\n",
      "Training network. lr: 0.000240. clip: 0.096022\n",
      "Iteration 1327: Policy loss: 0.013249. Value loss: 0.047645. Entropy: 0.881620.\n",
      "Iteration 1328: Policy loss: -0.007999. Value loss: 0.037355. Entropy: 0.885390.\n",
      "Iteration 1329: Policy loss: -0.021512. Value loss: 0.032311. Entropy: 0.885974.\n",
      "episode: 1327   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 431     evaluation reward: 4.38\n",
      "episode: 1328   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 364     evaluation reward: 4.34\n",
      "Training network. lr: 0.000240. clip: 0.096013\n",
      "Iteration 1330: Policy loss: 0.011000. Value loss: 0.022492. Entropy: 0.820549.\n",
      "Iteration 1331: Policy loss: -0.012924. Value loss: 0.015585. Entropy: 0.823022.\n",
      "Iteration 1332: Policy loss: -0.027459. Value loss: 0.012389. Entropy: 0.812606.\n",
      "episode: 1329   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 533     evaluation reward: 4.36\n",
      "episode: 1330   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 476     evaluation reward: 4.4\n",
      "Training network. lr: 0.000240. clip: 0.096004\n",
      "Iteration 1333: Policy loss: 0.004482. Value loss: 0.019776. Entropy: 0.788207.\n",
      "Iteration 1334: Policy loss: -0.008993. Value loss: 0.015009. Entropy: 0.785292.\n",
      "Iteration 1335: Policy loss: -0.027045. Value loss: 0.011593. Entropy: 0.776461.\n",
      "episode: 1331   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 425     evaluation reward: 4.36\n",
      "episode: 1332   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 323     evaluation reward: 4.35\n",
      "episode: 1333   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 492     evaluation reward: 4.37\n",
      "Training network. lr: 0.000240. clip: 0.095995\n",
      "Iteration 1336: Policy loss: 0.005126. Value loss: 0.015163. Entropy: 0.775367.\n",
      "Iteration 1337: Policy loss: -0.012819. Value loss: 0.011051. Entropy: 0.774024.\n",
      "Iteration 1338: Policy loss: -0.022428. Value loss: 0.009268. Entropy: 0.777747.\n",
      "episode: 1334   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 327     evaluation reward: 4.37\n",
      "episode: 1335   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 383     evaluation reward: 4.39\n",
      "Training network. lr: 0.000240. clip: 0.095986\n",
      "Iteration 1339: Policy loss: 0.013652. Value loss: 0.011210. Entropy: 0.804385.\n",
      "Iteration 1340: Policy loss: -0.009463. Value loss: 0.008711. Entropy: 0.799256.\n",
      "Iteration 1341: Policy loss: -0.016909. Value loss: 0.007391. Entropy: 0.810579.\n",
      "episode: 1336   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 628     evaluation reward: 4.42\n",
      "episode: 1337   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 424     evaluation reward: 4.42\n",
      "Training network. lr: 0.000240. clip: 0.095977\n",
      "Iteration 1342: Policy loss: 0.005497. Value loss: 0.010688. Entropy: 0.738535.\n",
      "Iteration 1343: Policy loss: -0.013375. Value loss: 0.008419. Entropy: 0.742263.\n",
      "Iteration 1344: Policy loss: -0.022572. Value loss: 0.007203. Entropy: 0.752951.\n",
      "episode: 1338   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 498     evaluation reward: 4.41\n",
      "episode: 1339   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 438     evaluation reward: 4.42\n",
      "episode: 1340   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 490     evaluation reward: 4.43\n",
      "Training network. lr: 0.000240. clip: 0.095968\n",
      "Iteration 1345: Policy loss: 0.008238. Value loss: 0.017313. Entropy: 0.843800.\n",
      "Iteration 1346: Policy loss: -0.016130. Value loss: 0.011943. Entropy: 0.847152.\n",
      "Iteration 1347: Policy loss: -0.026470. Value loss: 0.009685. Entropy: 0.840712.\n",
      "episode: 1341   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 4.44\n",
      "episode: 1342   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 226     evaluation reward: 4.41\n",
      "episode: 1343   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 419     evaluation reward: 4.39\n",
      "Training network. lr: 0.000240. clip: 0.095959\n",
      "Iteration 1348: Policy loss: 0.015343. Value loss: 0.021853. Entropy: 0.831549.\n",
      "Iteration 1349: Policy loss: -0.005799. Value loss: 0.016378. Entropy: 0.855397.\n",
      "Iteration 1350: Policy loss: -0.022296. Value loss: 0.012608. Entropy: 0.838860.\n",
      "episode: 1344   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 274     evaluation reward: 4.37\n",
      "episode: 1345   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 433     evaluation reward: 4.4\n",
      "Training network. lr: 0.000240. clip: 0.095950\n",
      "Iteration 1351: Policy loss: 0.013927. Value loss: 0.016640. Entropy: 0.815057.\n",
      "Iteration 1352: Policy loss: -0.009392. Value loss: 0.011125. Entropy: 0.819430.\n",
      "Iteration 1353: Policy loss: -0.016265. Value loss: 0.009722. Entropy: 0.800521.\n",
      "episode: 1346   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 483     evaluation reward: 4.39\n",
      "episode: 1347   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 401     evaluation reward: 4.42\n",
      "episode: 1348   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 477     evaluation reward: 4.44\n",
      "Training network. lr: 0.000240. clip: 0.095941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1354: Policy loss: 0.006944. Value loss: 0.017690. Entropy: 0.844730.\n",
      "Iteration 1355: Policy loss: -0.011350. Value loss: 0.013028. Entropy: 0.838928.\n",
      "Iteration 1356: Policy loss: -0.025742. Value loss: 0.010367. Entropy: 0.835970.\n",
      "episode: 1349   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 601     evaluation reward: 4.46\n",
      "Training network. lr: 0.000240. clip: 0.095932\n",
      "Iteration 1357: Policy loss: 0.010788. Value loss: 0.020870. Entropy: 0.825567.\n",
      "Iteration 1358: Policy loss: -0.009295. Value loss: 0.014248. Entropy: 0.818267.\n",
      "Iteration 1359: Policy loss: -0.026991. Value loss: 0.011467. Entropy: 0.816529.\n",
      "episode: 1350   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 687     evaluation reward: 4.55\n",
      "episode: 1351   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 275     evaluation reward: 4.51\n",
      "episode: 1352   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 363     evaluation reward: 4.49\n",
      "Training network. lr: 0.000240. clip: 0.095923\n",
      "Iteration 1360: Policy loss: 0.011807. Value loss: 0.047570. Entropy: 0.823956.\n",
      "Iteration 1361: Policy loss: -0.005650. Value loss: 0.035567. Entropy: 0.820599.\n",
      "Iteration 1362: Policy loss: -0.010234. Value loss: 0.027649. Entropy: 0.804251.\n",
      "episode: 1353   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 672     evaluation reward: 4.52\n",
      "episode: 1354   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 485     evaluation reward: 4.53\n",
      "Training network. lr: 0.000240. clip: 0.095914\n",
      "Iteration 1363: Policy loss: 0.012024. Value loss: 0.050132. Entropy: 0.894396.\n",
      "Iteration 1364: Policy loss: -0.008236. Value loss: 0.019553. Entropy: 0.887997.\n",
      "Iteration 1365: Policy loss: -0.018758. Value loss: 0.014250. Entropy: 0.889448.\n",
      "episode: 1355   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 357     evaluation reward: 4.52\n",
      "episode: 1356   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 465     evaluation reward: 4.51\n",
      "Training network. lr: 0.000240. clip: 0.095905\n",
      "Iteration 1366: Policy loss: 0.003814. Value loss: 0.026386. Entropy: 0.809934.\n",
      "Iteration 1367: Policy loss: -0.019138. Value loss: 0.020306. Entropy: 0.816616.\n",
      "Iteration 1368: Policy loss: -0.027496. Value loss: 0.015487. Entropy: 0.808722.\n",
      "episode: 1357   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 445     evaluation reward: 4.54\n",
      "episode: 1358   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 432     evaluation reward: 4.55\n",
      "episode: 1359   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 329     evaluation reward: 4.55\n",
      "Training network. lr: 0.000240. clip: 0.095896\n",
      "Iteration 1369: Policy loss: 0.013047. Value loss: 0.018984. Entropy: 0.811561.\n",
      "Iteration 1370: Policy loss: -0.006922. Value loss: 0.014793. Entropy: 0.812918.\n",
      "Iteration 1371: Policy loss: -0.019136. Value loss: 0.011923. Entropy: 0.802208.\n",
      "episode: 1360   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 419     evaluation reward: 4.57\n",
      "episode: 1361   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 589     evaluation reward: 4.62\n",
      "Training network. lr: 0.000240. clip: 0.095887\n",
      "Iteration 1372: Policy loss: 0.010332. Value loss: 0.016060. Entropy: 0.833395.\n",
      "Iteration 1373: Policy loss: -0.013402. Value loss: 0.011709. Entropy: 0.825709.\n",
      "Iteration 1374: Policy loss: -0.019485. Value loss: 0.009514. Entropy: 0.833210.\n",
      "episode: 1362   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 458     evaluation reward: 4.61\n",
      "episode: 1363   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 376     evaluation reward: 4.61\n",
      "Training network. lr: 0.000240. clip: 0.095878\n",
      "Iteration 1375: Policy loss: 0.004710. Value loss: 0.020729. Entropy: 0.812348.\n",
      "Iteration 1376: Policy loss: -0.010930. Value loss: 0.014958. Entropy: 0.815035.\n",
      "Iteration 1377: Policy loss: -0.024684. Value loss: 0.012838. Entropy: 0.820774.\n",
      "episode: 1364   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 343     evaluation reward: 4.6\n",
      "episode: 1365   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 537     evaluation reward: 4.68\n",
      "episode: 1366   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 356     evaluation reward: 4.68\n",
      "Training network. lr: 0.000240. clip: 0.095869\n",
      "Iteration 1378: Policy loss: 0.003280. Value loss: 0.044851. Entropy: 0.774618.\n",
      "Iteration 1379: Policy loss: -0.013037. Value loss: 0.033714. Entropy: 0.767965.\n",
      "Iteration 1380: Policy loss: -0.020999. Value loss: 0.027718. Entropy: 0.773600.\n",
      "episode: 1367   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 532     evaluation reward: 4.7\n",
      "episode: 1368   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 358     evaluation reward: 4.66\n",
      "Training network. lr: 0.000240. clip: 0.095860\n",
      "Iteration 1381: Policy loss: 0.015417. Value loss: 0.027357. Entropy: 0.746850.\n",
      "Iteration 1382: Policy loss: -0.009212. Value loss: 0.021052. Entropy: 0.759831.\n",
      "Iteration 1383: Policy loss: -0.020999. Value loss: 0.017130. Entropy: 0.752718.\n",
      "episode: 1369   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 486     evaluation reward: 4.65\n",
      "episode: 1370   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 473     evaluation reward: 4.68\n",
      "Training network. lr: 0.000240. clip: 0.095851\n",
      "Iteration 1384: Policy loss: 0.001620. Value loss: 0.018254. Entropy: 0.803211.\n",
      "Iteration 1385: Policy loss: -0.012729. Value loss: 0.012875. Entropy: 0.801645.\n",
      "Iteration 1386: Policy loss: -0.023892. Value loss: 0.011277. Entropy: 0.795457.\n",
      "episode: 1371   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 531     evaluation reward: 4.7\n",
      "episode: 1372   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 320     evaluation reward: 4.69\n",
      "Training network. lr: 0.000240. clip: 0.095842\n",
      "Iteration 1387: Policy loss: 0.008697. Value loss: 0.019773. Entropy: 0.725074.\n",
      "Iteration 1388: Policy loss: -0.011451. Value loss: 0.014530. Entropy: 0.726419.\n",
      "Iteration 1389: Policy loss: -0.019216. Value loss: 0.011904. Entropy: 0.722945.\n",
      "episode: 1373   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 480     evaluation reward: 4.71\n",
      "episode: 1374   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 566     evaluation reward: 4.77\n",
      "Training network. lr: 0.000240. clip: 0.095833\n",
      "Iteration 1390: Policy loss: 0.009493. Value loss: 0.020271. Entropy: 0.786956.\n",
      "Iteration 1391: Policy loss: -0.013949. Value loss: 0.015362. Entropy: 0.780447.\n",
      "Iteration 1392: Policy loss: -0.026274. Value loss: 0.012142. Entropy: 0.780812.\n",
      "episode: 1375   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 764     evaluation reward: 4.85\n",
      "episode: 1376   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 257     evaluation reward: 4.8\n",
      "episode: 1377   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 253     evaluation reward: 4.78\n",
      "Training network. lr: 0.000240. clip: 0.095824\n",
      "Iteration 1393: Policy loss: 0.005695. Value loss: 0.039061. Entropy: 0.773034.\n",
      "Iteration 1394: Policy loss: -0.009387. Value loss: 0.026187. Entropy: 0.750564.\n",
      "Iteration 1395: Policy loss: -0.019572. Value loss: 0.021095. Entropy: 0.753576.\n",
      "episode: 1378   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 510     evaluation reward: 4.82\n",
      "episode: 1379   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 321     evaluation reward: 4.8\n",
      "Training network. lr: 0.000240. clip: 0.095815\n",
      "Iteration 1396: Policy loss: 0.002644. Value loss: 0.022353. Entropy: 0.770758.\n",
      "Iteration 1397: Policy loss: -0.006928. Value loss: 0.017384. Entropy: 0.761880.\n",
      "Iteration 1398: Policy loss: -0.020937. Value loss: 0.015068. Entropy: 0.760211.\n",
      "episode: 1380   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 508     evaluation reward: 4.82\n",
      "episode: 1381   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 311     evaluation reward: 4.81\n",
      "episode: 1382   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 241     evaluation reward: 4.77\n",
      "Training network. lr: 0.000240. clip: 0.095806\n",
      "Iteration 1399: Policy loss: 0.010126. Value loss: 0.018760. Entropy: 0.809223.\n",
      "Iteration 1400: Policy loss: -0.005745. Value loss: 0.013526. Entropy: 0.798121.\n",
      "Iteration 1401: Policy loss: -0.015268. Value loss: 0.010805. Entropy: 0.793923.\n",
      "episode: 1383   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 309     evaluation reward: 4.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1384   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 448     evaluation reward: 4.76\n",
      "Training network. lr: 0.000239. clip: 0.095797\n",
      "Iteration 1402: Policy loss: 0.009656. Value loss: 0.021603. Entropy: 0.752489.\n",
      "Iteration 1403: Policy loss: -0.010448. Value loss: 0.014920. Entropy: 0.755571.\n",
      "Iteration 1404: Policy loss: -0.021494. Value loss: 0.012025. Entropy: 0.743659.\n",
      "episode: 1385   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 587     evaluation reward: 4.81\n",
      "episode: 1386   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 258     evaluation reward: 4.79\n",
      "episode: 1387   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 395     evaluation reward: 4.81\n",
      "episode: 1388   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 308     evaluation reward: 4.82\n",
      "Training network. lr: 0.000239. clip: 0.095788\n",
      "Iteration 1405: Policy loss: 0.004711. Value loss: 0.018326. Entropy: 0.753437.\n",
      "Iteration 1406: Policy loss: -0.015587. Value loss: 0.012571. Entropy: 0.751445.\n",
      "Iteration 1407: Policy loss: -0.024982. Value loss: 0.010808. Entropy: 0.747978.\n",
      "episode: 1389   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 403     evaluation reward: 4.82\n",
      "episode: 1390   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 357     evaluation reward: 4.81\n",
      "Training network. lr: 0.000239. clip: 0.095779\n",
      "Iteration 1408: Policy loss: 0.011243. Value loss: 0.019921. Entropy: 0.691952.\n",
      "Iteration 1409: Policy loss: -0.007831. Value loss: 0.015380. Entropy: 0.708264.\n",
      "Iteration 1410: Policy loss: -0.026072. Value loss: 0.013246. Entropy: 0.696065.\n",
      "episode: 1391   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 552     evaluation reward: 4.85\n",
      "episode: 1392   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 401     evaluation reward: 4.86\n",
      "Training network. lr: 0.000239. clip: 0.095770\n",
      "Iteration 1411: Policy loss: 0.031164. Value loss: 0.021995. Entropy: 0.779912.\n",
      "Iteration 1412: Policy loss: -0.003312. Value loss: 0.015679. Entropy: 0.779035.\n",
      "Iteration 1413: Policy loss: -0.016853. Value loss: 0.012838. Entropy: 0.773541.\n",
      "episode: 1393   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 503     evaluation reward: 4.91\n",
      "episode: 1394   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 289     evaluation reward: 4.89\n",
      "episode: 1395   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 447     evaluation reward: 4.93\n",
      "Training network. lr: 0.000239. clip: 0.095761\n",
      "Iteration 1414: Policy loss: 0.008597. Value loss: 0.026875. Entropy: 0.697622.\n",
      "Iteration 1415: Policy loss: -0.008267. Value loss: 0.018832. Entropy: 0.714473.\n",
      "Iteration 1416: Policy loss: -0.022905. Value loss: 0.014955. Entropy: 0.713219.\n",
      "episode: 1396   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 566     evaluation reward: 4.94\n",
      "episode: 1397   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 491     evaluation reward: 4.97\n",
      "Training network. lr: 0.000239. clip: 0.095752\n",
      "Iteration 1417: Policy loss: 0.009718. Value loss: 0.051059. Entropy: 0.758238.\n",
      "Iteration 1418: Policy loss: -0.005933. Value loss: 0.039575. Entropy: 0.771688.\n",
      "Iteration 1419: Policy loss: -0.020906. Value loss: 0.032775. Entropy: 0.757656.\n",
      "episode: 1398   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 502     evaluation reward: 5.02\n",
      "episode: 1399   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 306     evaluation reward: 5.04\n",
      "Training network. lr: 0.000239. clip: 0.095743\n",
      "Iteration 1420: Policy loss: 0.010754. Value loss: 0.023717. Entropy: 0.811981.\n",
      "Iteration 1421: Policy loss: -0.011656. Value loss: 0.018091. Entropy: 0.799069.\n",
      "Iteration 1422: Policy loss: -0.022494. Value loss: 0.015737. Entropy: 0.782853.\n",
      "episode: 1400   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 360     evaluation reward: 5.03\n",
      "episode: 1401   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 464     evaluation reward: 5.04\n",
      "episode: 1402   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 483     evaluation reward: 5.04\n",
      "Training network. lr: 0.000239. clip: 0.095734\n",
      "Iteration 1423: Policy loss: 0.009953. Value loss: 0.021112. Entropy: 0.820322.\n",
      "Iteration 1424: Policy loss: -0.011238. Value loss: 0.014996. Entropy: 0.813633.\n",
      "Iteration 1425: Policy loss: -0.023323. Value loss: 0.012348. Entropy: 0.809774.\n",
      "episode: 1403   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 474     evaluation reward: 5.08\n",
      "episode: 1404   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 460     evaluation reward: 5.08\n",
      "Training network. lr: 0.000239. clip: 0.095725\n",
      "Iteration 1426: Policy loss: 0.016107. Value loss: 0.017382. Entropy: 0.831302.\n",
      "Iteration 1427: Policy loss: -0.007251. Value loss: 0.013882. Entropy: 0.854225.\n",
      "Iteration 1428: Policy loss: -0.020464. Value loss: 0.011220. Entropy: 0.851890.\n",
      "episode: 1405   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 424     evaluation reward: 5.08\n",
      "episode: 1406   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 421     evaluation reward: 5.05\n",
      "Training network. lr: 0.000239. clip: 0.095716\n",
      "Iteration 1429: Policy loss: 0.017021. Value loss: 0.021743. Entropy: 0.795179.\n",
      "Iteration 1430: Policy loss: -0.008712. Value loss: 0.015175. Entropy: 0.796316.\n",
      "Iteration 1431: Policy loss: -0.019581. Value loss: 0.011419. Entropy: 0.791275.\n",
      "episode: 1407   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 446     evaluation reward: 5.03\n",
      "episode: 1408   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 386     evaluation reward: 5.03\n",
      "episode: 1409   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 354     evaluation reward: 5.05\n",
      "Training network. lr: 0.000239. clip: 0.095707\n",
      "Iteration 1432: Policy loss: 0.005739. Value loss: 0.019122. Entropy: 0.803971.\n",
      "Iteration 1433: Policy loss: -0.013208. Value loss: 0.012812. Entropy: 0.807322.\n",
      "Iteration 1434: Policy loss: -0.020365. Value loss: 0.009840. Entropy: 0.797301.\n",
      "episode: 1410   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 663     evaluation reward: 5.08\n",
      "Training network. lr: 0.000239. clip: 0.095698\n",
      "Iteration 1435: Policy loss: 0.011247. Value loss: 0.019543. Entropy: 0.972928.\n",
      "Iteration 1436: Policy loss: -0.010140. Value loss: 0.014660. Entropy: 0.967648.\n",
      "Iteration 1437: Policy loss: -0.024708. Value loss: 0.011646. Entropy: 0.972105.\n",
      "episode: 1411   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 514     evaluation reward: 5.08\n",
      "episode: 1412   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 337     evaluation reward: 5.05\n",
      "episode: 1413   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 319     evaluation reward: 5.06\n",
      "Training network. lr: 0.000239. clip: 0.095689\n",
      "Iteration 1438: Policy loss: 0.013446. Value loss: 0.019174. Entropy: 0.818417.\n",
      "Iteration 1439: Policy loss: -0.010390. Value loss: 0.013893. Entropy: 0.808482.\n",
      "Iteration 1440: Policy loss: -0.020368. Value loss: 0.010928. Entropy: 0.812794.\n",
      "episode: 1414   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 445     evaluation reward: 5.06\n",
      "episode: 1415   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 301     evaluation reward: 5.04\n",
      "episode: 1416   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 466     evaluation reward: 5.05\n",
      "Training network. lr: 0.000239. clip: 0.095680\n",
      "Iteration 1441: Policy loss: 0.017089. Value loss: 0.024645. Entropy: 0.834261.\n",
      "Iteration 1442: Policy loss: -0.007522. Value loss: 0.017645. Entropy: 0.846961.\n",
      "Iteration 1443: Policy loss: -0.017523. Value loss: 0.014705. Entropy: 0.841365.\n",
      "episode: 1417   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 515     evaluation reward: 5.06\n",
      "episode: 1418   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 222     evaluation reward: 5.02\n",
      "episode: 1419   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 377     evaluation reward: 5.01\n",
      "Training network. lr: 0.000239. clip: 0.095671\n",
      "Iteration 1444: Policy loss: 0.007025. Value loss: 0.023753. Entropy: 0.862843.\n",
      "Iteration 1445: Policy loss: -0.012980. Value loss: 0.016228. Entropy: 0.858557.\n",
      "Iteration 1446: Policy loss: -0.026096. Value loss: 0.013328. Entropy: 0.854652.\n",
      "episode: 1420   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 559     evaluation reward: 5.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1421   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 441     evaluation reward: 5.07\n",
      "Training network. lr: 0.000239. clip: 0.095662\n",
      "Iteration 1447: Policy loss: 0.008163. Value loss: 0.018839. Entropy: 0.791189.\n",
      "Iteration 1448: Policy loss: -0.007210. Value loss: 0.014105. Entropy: 0.792789.\n",
      "Iteration 1449: Policy loss: -0.023171. Value loss: 0.011814. Entropy: 0.796964.\n",
      "episode: 1422   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 452     evaluation reward: 5.09\n",
      "episode: 1423   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 353     evaluation reward: 5.1\n",
      "episode: 1424   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 205     evaluation reward: 5.04\n",
      "Training network. lr: 0.000239. clip: 0.095653\n",
      "Iteration 1450: Policy loss: 0.010006. Value loss: 0.018920. Entropy: 0.832936.\n",
      "Iteration 1451: Policy loss: -0.009920. Value loss: 0.013616. Entropy: 0.836748.\n",
      "Iteration 1452: Policy loss: -0.023197. Value loss: 0.010778. Entropy: 0.820325.\n",
      "episode: 1425   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 538     evaluation reward: 5.03\n",
      "Training network. lr: 0.000239. clip: 0.095644\n",
      "Iteration 1453: Policy loss: 0.007095. Value loss: 0.045960. Entropy: 0.936675.\n",
      "Iteration 1454: Policy loss: -0.014477. Value loss: 0.032986. Entropy: 0.940012.\n",
      "Iteration 1455: Policy loss: -0.021351. Value loss: 0.026786. Entropy: 0.936317.\n",
      "episode: 1426   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 640     evaluation reward: 5.13\n",
      "episode: 1427   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 563     evaluation reward: 5.16\n",
      "episode: 1428   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 336     evaluation reward: 5.16\n",
      "Training network. lr: 0.000239. clip: 0.095635\n",
      "Iteration 1456: Policy loss: 0.005174. Value loss: 0.026956. Entropy: 0.885187.\n",
      "Iteration 1457: Policy loss: -0.012431. Value loss: 0.017203. Entropy: 0.871394.\n",
      "Iteration 1458: Policy loss: -0.025094. Value loss: 0.013976. Entropy: 0.869701.\n",
      "episode: 1429   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 733     evaluation reward: 5.22\n",
      "Training network. lr: 0.000239. clip: 0.095626\n",
      "Iteration 1459: Policy loss: 0.008786. Value loss: 0.063902. Entropy: 0.962454.\n",
      "Iteration 1460: Policy loss: -0.010298. Value loss: 0.047559. Entropy: 0.956473.\n",
      "Iteration 1461: Policy loss: -0.022412. Value loss: 0.037829. Entropy: 0.952478.\n",
      "episode: 1430   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 397     evaluation reward: 5.21\n",
      "episode: 1431   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 434     evaluation reward: 5.22\n",
      "Training network. lr: 0.000239. clip: 0.095617\n",
      "Iteration 1462: Policy loss: 0.013589. Value loss: 0.026744. Entropy: 0.961194.\n",
      "Iteration 1463: Policy loss: -0.010313. Value loss: 0.018373. Entropy: 0.955053.\n",
      "Iteration 1464: Policy loss: -0.024289. Value loss: 0.014191. Entropy: 0.948119.\n",
      "episode: 1432   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 595     evaluation reward: 5.27\n",
      "now time :  2018-12-26 12:49:50.208161\n",
      "episode: 1433   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 442     evaluation reward: 5.26\n",
      "episode: 1434   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 459     evaluation reward: 5.28\n",
      "Training network. lr: 0.000239. clip: 0.095608\n",
      "Iteration 1465: Policy loss: 0.014657. Value loss: 0.021914. Entropy: 0.813868.\n",
      "Iteration 1466: Policy loss: -0.012247. Value loss: 0.015906. Entropy: 0.816486.\n",
      "Iteration 1467: Policy loss: -0.026139. Value loss: 0.013243. Entropy: 0.818519.\n",
      "episode: 1435   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 441     evaluation reward: 5.3\n",
      "episode: 1436   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 422     evaluation reward: 5.28\n",
      "Training network. lr: 0.000239. clip: 0.095599\n",
      "Iteration 1468: Policy loss: 0.009482. Value loss: 0.019887. Entropy: 0.875804.\n",
      "Iteration 1469: Policy loss: -0.011130. Value loss: 0.014313. Entropy: 0.870074.\n",
      "Iteration 1470: Policy loss: -0.025033. Value loss: 0.012240. Entropy: 0.879929.\n",
      "episode: 1437   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 360     evaluation reward: 5.27\n",
      "episode: 1438   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 410     evaluation reward: 5.26\n",
      "episode: 1439   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 419     evaluation reward: 5.26\n",
      "Training network. lr: 0.000239. clip: 0.095590\n",
      "Iteration 1471: Policy loss: 0.001877. Value loss: 0.016894. Entropy: 0.882109.\n",
      "Iteration 1472: Policy loss: -0.017972. Value loss: 0.012174. Entropy: 0.871810.\n",
      "Iteration 1473: Policy loss: -0.028783. Value loss: 0.010347. Entropy: 0.868565.\n",
      "episode: 1440   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 411     evaluation reward: 5.26\n",
      "episode: 1441   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 682     evaluation reward: 5.31\n",
      "Training network. lr: 0.000239. clip: 0.095581\n",
      "Iteration 1474: Policy loss: 0.008159. Value loss: 0.026727. Entropy: 0.945769.\n",
      "Iteration 1475: Policy loss: -0.014024. Value loss: 0.016779. Entropy: 0.941283.\n",
      "Iteration 1476: Policy loss: -0.024090. Value loss: 0.012660. Entropy: 0.952098.\n",
      "episode: 1442   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 394     evaluation reward: 5.34\n",
      "episode: 1443   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 488     evaluation reward: 5.35\n",
      "Training network. lr: 0.000239. clip: 0.095572\n",
      "Iteration 1477: Policy loss: 0.009228. Value loss: 0.018809. Entropy: 0.924982.\n",
      "Iteration 1478: Policy loss: -0.013206. Value loss: 0.014232. Entropy: 0.914281.\n",
      "Iteration 1479: Policy loss: -0.023797. Value loss: 0.012130. Entropy: 0.918158.\n",
      "episode: 1444   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 481     evaluation reward: 5.38\n",
      "Training network. lr: 0.000239. clip: 0.095563\n",
      "Iteration 1480: Policy loss: 0.007116. Value loss: 0.018526. Entropy: 0.961898.\n",
      "Iteration 1481: Policy loss: -0.013333. Value loss: 0.013492. Entropy: 0.949920.\n",
      "Iteration 1482: Policy loss: -0.023765. Value loss: 0.010416. Entropy: 0.957967.\n",
      "episode: 1445   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 802     evaluation reward: 5.44\n",
      "episode: 1446   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 373     evaluation reward: 5.42\n",
      "episode: 1447   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 533     evaluation reward: 5.44\n",
      "Training network. lr: 0.000239. clip: 0.095554\n",
      "Iteration 1483: Policy loss: 0.008392. Value loss: 0.027190. Entropy: 0.969681.\n",
      "Iteration 1484: Policy loss: -0.008286. Value loss: 0.018251. Entropy: 0.956664.\n",
      "Iteration 1485: Policy loss: -0.021955. Value loss: 0.013359. Entropy: 0.953433.\n",
      "episode: 1448   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 537     evaluation reward: 5.45\n",
      "episode: 1449   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 224     evaluation reward: 5.37\n",
      "Training network. lr: 0.000239. clip: 0.095545\n",
      "Iteration 1486: Policy loss: 0.008253. Value loss: 0.017674. Entropy: 0.907736.\n",
      "Iteration 1487: Policy loss: -0.016213. Value loss: 0.014754. Entropy: 0.904868.\n",
      "Iteration 1488: Policy loss: -0.023878. Value loss: 0.011991. Entropy: 0.890264.\n",
      "episode: 1450   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 354     evaluation reward: 5.28\n",
      "episode: 1451   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 370     evaluation reward: 5.3\n",
      "episode: 1452   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 340     evaluation reward: 5.3\n",
      "Training network. lr: 0.000239. clip: 0.095536\n",
      "Iteration 1489: Policy loss: 0.005206. Value loss: 0.020356. Entropy: 0.899759.\n",
      "Iteration 1490: Policy loss: -0.018053. Value loss: 0.015133. Entropy: 0.898777.\n",
      "Iteration 1491: Policy loss: -0.029550. Value loss: 0.012362. Entropy: 0.893052.\n",
      "episode: 1453   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 648     evaluation reward: 5.3\n",
      "episode: 1454   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 361     evaluation reward: 5.29\n",
      "Training network. lr: 0.000239. clip: 0.095527\n",
      "Iteration 1492: Policy loss: 0.007924. Value loss: 0.026844. Entropy: 0.885409.\n",
      "Iteration 1493: Policy loss: -0.014757. Value loss: 0.020505. Entropy: 0.871518.\n",
      "Iteration 1494: Policy loss: -0.021677. Value loss: 0.017597. Entropy: 0.883417.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1455   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 561     evaluation reward: 5.35\n",
      "episode: 1456   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 411     evaluation reward: 5.34\n",
      "Training network. lr: 0.000239. clip: 0.095518\n",
      "Iteration 1495: Policy loss: 0.013040. Value loss: 0.048713. Entropy: 0.825261.\n",
      "Iteration 1496: Policy loss: -0.006665. Value loss: 0.038332. Entropy: 0.834109.\n",
      "Iteration 1497: Policy loss: -0.012826. Value loss: 0.033760. Entropy: 0.818998.\n",
      "episode: 1457   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 458     evaluation reward: 5.34\n",
      "episode: 1458   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 361     evaluation reward: 5.33\n",
      "Training network. lr: 0.000239. clip: 0.095509\n",
      "Iteration 1498: Policy loss: 0.010258. Value loss: 0.023192. Entropy: 0.848072.\n",
      "Iteration 1499: Policy loss: -0.011159. Value loss: 0.015274. Entropy: 0.836074.\n",
      "Iteration 1500: Policy loss: -0.025159. Value loss: 0.014323. Entropy: 0.829009.\n",
      "episode: 1459   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 650     evaluation reward: 5.4\n",
      "episode: 1460   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 366     evaluation reward: 5.39\n",
      "episode: 1461   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 486     evaluation reward: 5.39\n",
      "Training network. lr: 0.000239. clip: 0.095500\n",
      "Iteration 1501: Policy loss: 0.006356. Value loss: 0.021223. Entropy: 0.874528.\n",
      "Iteration 1502: Policy loss: -0.009804. Value loss: 0.016298. Entropy: 0.880226.\n",
      "Iteration 1503: Policy loss: -0.029258. Value loss: 0.013417. Entropy: 0.867183.\n",
      "episode: 1462   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 382     evaluation reward: 5.38\n",
      "episode: 1463   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 384     evaluation reward: 5.38\n",
      "Training network. lr: 0.000239. clip: 0.095491\n",
      "Iteration 1504: Policy loss: 0.006345. Value loss: 0.013651. Entropy: 0.867800.\n",
      "Iteration 1505: Policy loss: -0.013550. Value loss: 0.010358. Entropy: 0.853051.\n",
      "Iteration 1506: Policy loss: -0.024339. Value loss: 0.009410. Entropy: 0.854037.\n",
      "episode: 1464   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 322     evaluation reward: 5.38\n",
      "episode: 1465   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 559     evaluation reward: 5.35\n",
      "episode: 1466   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 317     evaluation reward: 5.34\n",
      "Training network. lr: 0.000239. clip: 0.095482\n",
      "Iteration 1507: Policy loss: 0.009398. Value loss: 0.021983. Entropy: 0.887231.\n",
      "Iteration 1508: Policy loss: -0.014593. Value loss: 0.015435. Entropy: 0.874154.\n",
      "Iteration 1509: Policy loss: -0.022620. Value loss: 0.012103. Entropy: 0.867195.\n",
      "episode: 1467   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 293     evaluation reward: 5.3\n",
      "episode: 1468   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 518     evaluation reward: 5.32\n",
      "Training network. lr: 0.000239. clip: 0.095473\n",
      "Iteration 1510: Policy loss: 0.015270. Value loss: 0.019797. Entropy: 0.839402.\n",
      "Iteration 1511: Policy loss: -0.003699. Value loss: 0.015844. Entropy: 0.846260.\n",
      "Iteration 1512: Policy loss: -0.019120. Value loss: 0.013143. Entropy: 0.835814.\n",
      "episode: 1469   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 630     evaluation reward: 5.35\n",
      "episode: 1470   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 580     evaluation reward: 5.37\n",
      "Training network. lr: 0.000239. clip: 0.095464\n",
      "Iteration 1513: Policy loss: 0.002081. Value loss: 0.013474. Entropy: 0.898242.\n",
      "Iteration 1514: Policy loss: -0.014449. Value loss: 0.010002. Entropy: 0.900475.\n",
      "Iteration 1515: Policy loss: -0.029293. Value loss: 0.008764. Entropy: 0.897959.\n",
      "episode: 1471   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 5.35\n",
      "episode: 1472   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 420     evaluation reward: 5.36\n",
      "episode: 1473   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 272     evaluation reward: 5.32\n",
      "Training network. lr: 0.000239. clip: 0.095455\n",
      "Iteration 1516: Policy loss: 0.011123. Value loss: 0.018772. Entropy: 0.997868.\n",
      "Iteration 1517: Policy loss: -0.015857. Value loss: 0.013008. Entropy: 0.991352.\n",
      "Iteration 1518: Policy loss: -0.031158. Value loss: 0.011328. Entropy: 0.981001.\n",
      "episode: 1474   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 559     evaluation reward: 5.31\n",
      "episode: 1475   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 281     evaluation reward: 5.19\n",
      "Training network. lr: 0.000239. clip: 0.095446\n",
      "Iteration 1519: Policy loss: 0.008979. Value loss: 0.031170. Entropy: 0.886309.\n",
      "Iteration 1520: Policy loss: -0.013518. Value loss: 0.022166. Entropy: 0.886961.\n",
      "Iteration 1521: Policy loss: -0.023804. Value loss: 0.017803. Entropy: 0.888952.\n",
      "episode: 1476   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 504     evaluation reward: 5.23\n",
      "episode: 1477   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 400     evaluation reward: 5.25\n",
      "Training network. lr: 0.000239. clip: 0.095437\n",
      "Iteration 1522: Policy loss: 0.010579. Value loss: 0.022491. Entropy: 0.919060.\n",
      "Iteration 1523: Policy loss: -0.013380. Value loss: 0.016422. Entropy: 0.906093.\n",
      "Iteration 1524: Policy loss: -0.024960. Value loss: 0.013605. Entropy: 0.899153.\n",
      "episode: 1478   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 525     evaluation reward: 5.24\n",
      "episode: 1479   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 426     evaluation reward: 5.26\n",
      "Training network. lr: 0.000239. clip: 0.095428\n",
      "Iteration 1525: Policy loss: 0.003443. Value loss: 0.015568. Entropy: 0.833329.\n",
      "Iteration 1526: Policy loss: -0.014858. Value loss: 0.011922. Entropy: 0.828151.\n",
      "Iteration 1527: Policy loss: -0.027380. Value loss: 0.010174. Entropy: 0.827704.\n",
      "episode: 1480   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 555     evaluation reward: 5.26\n",
      "episode: 1481   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 483     evaluation reward: 5.28\n",
      "episode: 1482   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 376     evaluation reward: 5.3\n",
      "Training network. lr: 0.000239. clip: 0.095419\n",
      "Iteration 1528: Policy loss: 0.008285. Value loss: 0.019083. Entropy: 0.859703.\n",
      "Iteration 1529: Policy loss: -0.012842. Value loss: 0.013964. Entropy: 0.850313.\n",
      "Iteration 1530: Policy loss: -0.022209. Value loss: 0.011557. Entropy: 0.844159.\n",
      "episode: 1483   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 491     evaluation reward: 5.34\n",
      "episode: 1484   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 288     evaluation reward: 5.31\n",
      "Training network. lr: 0.000239. clip: 0.095410\n",
      "Iteration 1531: Policy loss: 0.011336. Value loss: 0.026327. Entropy: 0.912509.\n",
      "Iteration 1532: Policy loss: -0.009043. Value loss: 0.018394. Entropy: 0.907584.\n",
      "Iteration 1533: Policy loss: -0.019633. Value loss: 0.015156. Entropy: 0.900077.\n",
      "episode: 1485   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 489     evaluation reward: 5.29\n",
      "episode: 1486   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 570     evaluation reward: 5.34\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1534: Policy loss: 0.006247. Value loss: 0.016479. Entropy: 0.926078.\n",
      "Iteration 1535: Policy loss: -0.019812. Value loss: 0.012014. Entropy: 0.925007.\n",
      "Iteration 1536: Policy loss: -0.029369. Value loss: 0.009692. Entropy: 0.912747.\n",
      "episode: 1487   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 530     evaluation reward: 5.35\n",
      "episode: 1488   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 680     evaluation reward: 5.41\n",
      "Training network. lr: 0.000238. clip: 0.095392\n",
      "Iteration 1537: Policy loss: 0.011496. Value loss: 0.018963. Entropy: 0.897615.\n",
      "Iteration 1538: Policy loss: -0.011601. Value loss: 0.014924. Entropy: 0.885846.\n",
      "Iteration 1539: Policy loss: -0.024349. Value loss: 0.011199. Entropy: 0.877373.\n",
      "episode: 1489   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 638     evaluation reward: 5.44\n",
      "Training network. lr: 0.000238. clip: 0.095383\n",
      "Iteration 1540: Policy loss: 0.013201. Value loss: 0.022811. Entropy: 0.909314.\n",
      "Iteration 1541: Policy loss: -0.009707. Value loss: 0.015681. Entropy: 0.916788.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1542: Policy loss: -0.027359. Value loss: 0.014137. Entropy: 0.908321.\n",
      "episode: 1490   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 494     evaluation reward: 5.47\n",
      "episode: 1491   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 450     evaluation reward: 5.45\n",
      "episode: 1492   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 233     evaluation reward: 5.41\n",
      "Training network. lr: 0.000238. clip: 0.095374\n",
      "Iteration 1543: Policy loss: 0.006372. Value loss: 0.020775. Entropy: 0.857714.\n",
      "Iteration 1544: Policy loss: -0.011251. Value loss: 0.014828. Entropy: 0.858173.\n",
      "Iteration 1545: Policy loss: -0.025936. Value loss: 0.012753. Entropy: 0.846334.\n",
      "episode: 1493   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 692     evaluation reward: 5.44\n",
      "episode: 1494   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 454     evaluation reward: 5.48\n",
      "Training network. lr: 0.000238. clip: 0.095365\n",
      "Iteration 1546: Policy loss: 0.003366. Value loss: 0.029101. Entropy: 0.943626.\n",
      "Iteration 1547: Policy loss: -0.017948. Value loss: 0.019246. Entropy: 0.935263.\n",
      "Iteration 1548: Policy loss: -0.029164. Value loss: 0.015034. Entropy: 0.936616.\n",
      "episode: 1495   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 413     evaluation reward: 5.47\n",
      "episode: 1496   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 433     evaluation reward: 5.42\n",
      "Training network. lr: 0.000238. clip: 0.095356\n",
      "Iteration 1549: Policy loss: 0.009204. Value loss: 0.018232. Entropy: 0.762914.\n",
      "Iteration 1550: Policy loss: -0.009579. Value loss: 0.011530. Entropy: 0.754549.\n",
      "Iteration 1551: Policy loss: -0.021626. Value loss: 0.010244. Entropy: 0.757845.\n",
      "episode: 1497   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 493     evaluation reward: 5.43\n",
      "episode: 1498   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 387     evaluation reward: 5.41\n",
      "episode: 1499   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 334     evaluation reward: 5.41\n",
      "Training network. lr: 0.000238. clip: 0.095347\n",
      "Iteration 1552: Policy loss: 0.010826. Value loss: 0.023815. Entropy: 0.822432.\n",
      "Iteration 1553: Policy loss: -0.009417. Value loss: 0.015179. Entropy: 0.819461.\n",
      "Iteration 1554: Policy loss: -0.021970. Value loss: 0.011892. Entropy: 0.822672.\n",
      "episode: 1500   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 548     evaluation reward: 5.45\n",
      "episode: 1501   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 5.46\n",
      "Training network. lr: 0.000238. clip: 0.095338\n",
      "Iteration 1555: Policy loss: 0.003887. Value loss: 0.020198. Entropy: 0.833684.\n",
      "Iteration 1556: Policy loss: -0.016779. Value loss: 0.015691. Entropy: 0.833370.\n",
      "Iteration 1557: Policy loss: -0.025972. Value loss: 0.012003. Entropy: 0.844816.\n",
      "episode: 1502   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 424     evaluation reward: 5.45\n",
      "episode: 1503   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 715     evaluation reward: 5.5\n",
      "Training network. lr: 0.000238. clip: 0.095329\n",
      "Iteration 1558: Policy loss: 0.012059. Value loss: 0.026140. Entropy: 0.949825.\n",
      "Iteration 1559: Policy loss: -0.011312. Value loss: 0.017537. Entropy: 0.941334.\n",
      "Iteration 1560: Policy loss: -0.025458. Value loss: 0.014006. Entropy: 0.933310.\n",
      "episode: 1504   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 534     evaluation reward: 5.53\n",
      "episode: 1505   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 449     evaluation reward: 5.54\n",
      "Training network. lr: 0.000238. clip: 0.095320\n",
      "Iteration 1561: Policy loss: 0.007594. Value loss: 0.024107. Entropy: 0.907416.\n",
      "Iteration 1562: Policy loss: -0.013642. Value loss: 0.017876. Entropy: 0.892285.\n",
      "Iteration 1563: Policy loss: -0.022784. Value loss: 0.014928. Entropy: 0.891078.\n",
      "episode: 1506   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 5.54\n",
      "episode: 1507   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 516     evaluation reward: 5.55\n",
      "Training network. lr: 0.000238. clip: 0.095311\n",
      "Iteration 1564: Policy loss: 0.003192. Value loss: 0.017530. Entropy: 0.826980.\n",
      "Iteration 1565: Policy loss: -0.013232. Value loss: 0.013050. Entropy: 0.821791.\n",
      "Iteration 1566: Policy loss: -0.025688. Value loss: 0.011820. Entropy: 0.819433.\n",
      "episode: 1508   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 455     evaluation reward: 5.57\n",
      "episode: 1509   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 609     evaluation reward: 5.62\n",
      "Training network. lr: 0.000238. clip: 0.095302\n",
      "Iteration 1567: Policy loss: 0.008695. Value loss: 0.018286. Entropy: 0.830548.\n",
      "Iteration 1568: Policy loss: -0.014134. Value loss: 0.012967. Entropy: 0.853595.\n",
      "Iteration 1569: Policy loss: -0.026186. Value loss: 0.010518. Entropy: 0.837877.\n",
      "episode: 1510   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 428     evaluation reward: 5.59\n",
      "episode: 1511   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 546     evaluation reward: 5.64\n",
      "Training network. lr: 0.000238. clip: 0.095293\n",
      "Iteration 1570: Policy loss: 0.006219. Value loss: 0.045121. Entropy: 0.759216.\n",
      "Iteration 1571: Policy loss: -0.006859. Value loss: 0.031481. Entropy: 0.748774.\n",
      "Iteration 1572: Policy loss: -0.020915. Value loss: 0.027472. Entropy: 0.749821.\n",
      "episode: 1512   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 508     evaluation reward: 5.66\n",
      "episode: 1513   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 460     evaluation reward: 5.67\n",
      "Training network. lr: 0.000238. clip: 0.095284\n",
      "Iteration 1573: Policy loss: 0.015122. Value loss: 0.023061. Entropy: 0.863524.\n",
      "Iteration 1574: Policy loss: -0.014560. Value loss: 0.017537. Entropy: 0.861247.\n",
      "Iteration 1575: Policy loss: -0.028174. Value loss: 0.013392. Entropy: 0.853248.\n",
      "episode: 1514   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 658     evaluation reward: 5.74\n",
      "episode: 1515   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 417     evaluation reward: 5.76\n",
      "Training network. lr: 0.000238. clip: 0.095275\n",
      "Iteration 1576: Policy loss: 0.007986. Value loss: 0.049367. Entropy: 0.861347.\n",
      "Iteration 1577: Policy loss: -0.005967. Value loss: 0.039096. Entropy: 0.863373.\n",
      "Iteration 1578: Policy loss: -0.015494. Value loss: 0.033294. Entropy: 0.865475.\n",
      "episode: 1516   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 380     evaluation reward: 5.74\n",
      "episode: 1517   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 344     evaluation reward: 5.71\n",
      "episode: 1518   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 288     evaluation reward: 5.73\n",
      "Training network. lr: 0.000238. clip: 0.095266\n",
      "Iteration 1579: Policy loss: 0.009164. Value loss: 0.027657. Entropy: 0.852666.\n",
      "Iteration 1580: Policy loss: -0.010529. Value loss: 0.017196. Entropy: 0.850718.\n",
      "Iteration 1581: Policy loss: -0.021237. Value loss: 0.015390. Entropy: 0.846334.\n",
      "episode: 1519   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 402     evaluation reward: 5.74\n",
      "episode: 1520   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 451     evaluation reward: 5.73\n",
      "Training network. lr: 0.000238. clip: 0.095257\n",
      "Iteration 1582: Policy loss: 0.006871. Value loss: 0.016119. Entropy: 0.811935.\n",
      "Iteration 1583: Policy loss: -0.008490. Value loss: 0.012360. Entropy: 0.795763.\n",
      "Iteration 1584: Policy loss: -0.023766. Value loss: 0.010555. Entropy: 0.793270.\n",
      "episode: 1521   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 543     evaluation reward: 5.75\n",
      "episode: 1522   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 466     evaluation reward: 5.75\n",
      "Training network. lr: 0.000238. clip: 0.095248\n",
      "Iteration 1585: Policy loss: 0.003576. Value loss: 0.020835. Entropy: 0.821599.\n",
      "Iteration 1586: Policy loss: -0.016501. Value loss: 0.014513. Entropy: 0.823768.\n",
      "Iteration 1587: Policy loss: -0.024432. Value loss: 0.011654. Entropy: 0.823964.\n",
      "episode: 1523   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 425     evaluation reward: 5.76\n",
      "episode: 1524   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 401     evaluation reward: 5.79\n",
      "episode: 1525   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 444     evaluation reward: 5.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000238. clip: 0.095239\n",
      "Iteration 1588: Policy loss: 0.009802. Value loss: 0.024785. Entropy: 0.809793.\n",
      "Iteration 1589: Policy loss: -0.010528. Value loss: 0.018981. Entropy: 0.798693.\n",
      "Iteration 1590: Policy loss: -0.021847. Value loss: 0.015375. Entropy: 0.804750.\n",
      "episode: 1526   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 437     evaluation reward: 5.71\n",
      "episode: 1527   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 467     evaluation reward: 5.69\n",
      "Training network. lr: 0.000238. clip: 0.095230\n",
      "Iteration 1591: Policy loss: 0.007215. Value loss: 0.020751. Entropy: 0.817367.\n",
      "Iteration 1592: Policy loss: -0.013453. Value loss: 0.016202. Entropy: 0.812631.\n",
      "Iteration 1593: Policy loss: -0.028324. Value loss: 0.012939. Entropy: 0.807170.\n",
      "episode: 1528   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 620     evaluation reward: 5.73\n",
      "episode: 1529   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 558     evaluation reward: 5.67\n",
      "Training network. lr: 0.000238. clip: 0.095221\n",
      "Iteration 1594: Policy loss: 0.005452. Value loss: 0.020175. Entropy: 0.803226.\n",
      "Iteration 1595: Policy loss: -0.015728. Value loss: 0.015184. Entropy: 0.805735.\n",
      "Iteration 1596: Policy loss: -0.026738. Value loss: 0.012237. Entropy: 0.799290.\n",
      "episode: 1530   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 566     evaluation reward: 5.7\n",
      "episode: 1531   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 356     evaluation reward: 5.68\n",
      "Training network. lr: 0.000238. clip: 0.095212\n",
      "Iteration 1597: Policy loss: 0.007673. Value loss: 0.023145. Entropy: 0.919780.\n",
      "Iteration 1598: Policy loss: -0.014300. Value loss: 0.015605. Entropy: 0.912749.\n",
      "Iteration 1599: Policy loss: -0.026067. Value loss: 0.012154. Entropy: 0.912803.\n",
      "episode: 1532   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 684     evaluation reward: 5.69\n",
      "Training network. lr: 0.000238. clip: 0.095203\n",
      "Iteration 1600: Policy loss: 0.006852. Value loss: 0.020546. Entropy: 0.894216.\n",
      "Iteration 1601: Policy loss: -0.015171. Value loss: 0.016457. Entropy: 0.884617.\n",
      "Iteration 1602: Policy loss: -0.020750. Value loss: 0.013509. Entropy: 0.874673.\n",
      "episode: 1533   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 635     evaluation reward: 5.73\n",
      "episode: 1534   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 694     evaluation reward: 5.77\n",
      "Training network. lr: 0.000238. clip: 0.095194\n",
      "Iteration 1603: Policy loss: 0.011530. Value loss: 0.018357. Entropy: 0.779315.\n",
      "Iteration 1604: Policy loss: -0.013614. Value loss: 0.014264. Entropy: 0.790397.\n",
      "Iteration 1605: Policy loss: -0.021795. Value loss: 0.011380. Entropy: 0.788090.\n",
      "episode: 1535   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 713     evaluation reward: 5.8\n",
      "episode: 1536   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 460     evaluation reward: 5.81\n",
      "Training network. lr: 0.000238. clip: 0.095185\n",
      "Iteration 1606: Policy loss: 0.006085. Value loss: 0.020159. Entropy: 0.831793.\n",
      "Iteration 1607: Policy loss: -0.009853. Value loss: 0.012373. Entropy: 0.835266.\n",
      "Iteration 1608: Policy loss: -0.020377. Value loss: 0.010566. Entropy: 0.828089.\n",
      "episode: 1537   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 404     evaluation reward: 5.82\n",
      "episode: 1538   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 582     evaluation reward: 5.85\n",
      "Training network. lr: 0.000238. clip: 0.095176\n",
      "Iteration 1609: Policy loss: 0.003708. Value loss: 0.020969. Entropy: 0.852238.\n",
      "Iteration 1610: Policy loss: -0.015208. Value loss: 0.014216. Entropy: 0.857198.\n",
      "Iteration 1611: Policy loss: -0.027654. Value loss: 0.010858. Entropy: 0.852100.\n",
      "now time :  2018-12-26 12:54:14.375536\n",
      "episode: 1539   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 560     evaluation reward: 5.87\n",
      "episode: 1540   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 267     evaluation reward: 5.84\n",
      "Training network. lr: 0.000238. clip: 0.095167\n",
      "Iteration 1612: Policy loss: 0.003147. Value loss: 0.028944. Entropy: 0.797772.\n",
      "Iteration 1613: Policy loss: -0.015271. Value loss: 0.019395. Entropy: 0.783630.\n",
      "Iteration 1614: Policy loss: -0.022492. Value loss: 0.015487. Entropy: 0.783904.\n",
      "episode: 1541   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 394     evaluation reward: 5.79\n",
      "episode: 1542   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 307     evaluation reward: 5.78\n",
      "episode: 1543   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 396     evaluation reward: 5.77\n",
      "Training network. lr: 0.000238. clip: 0.095158\n",
      "Iteration 1615: Policy loss: 0.006263. Value loss: 0.025672. Entropy: 0.796836.\n",
      "Iteration 1616: Policy loss: -0.012204. Value loss: 0.020127. Entropy: 0.789529.\n",
      "Iteration 1617: Policy loss: -0.024346. Value loss: 0.017325. Entropy: 0.788404.\n",
      "episode: 1544   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 560     evaluation reward: 5.8\n",
      "episode: 1545   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 479     evaluation reward: 5.75\n",
      "Training network. lr: 0.000238. clip: 0.095149\n",
      "Iteration 1618: Policy loss: 0.007346. Value loss: 0.025104. Entropy: 0.807872.\n",
      "Iteration 1619: Policy loss: -0.013591. Value loss: 0.017965. Entropy: 0.811749.\n",
      "Iteration 1620: Policy loss: -0.024367. Value loss: 0.013853. Entropy: 0.798990.\n",
      "episode: 1546   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 509     evaluation reward: 5.77\n",
      "episode: 1547   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 355     evaluation reward: 5.74\n",
      "Training network. lr: 0.000238. clip: 0.095140\n",
      "Iteration 1621: Policy loss: 0.015226. Value loss: 0.025257. Entropy: 0.818215.\n",
      "Iteration 1622: Policy loss: -0.012860. Value loss: 0.017019. Entropy: 0.825944.\n",
      "Iteration 1623: Policy loss: -0.022642. Value loss: 0.013853. Entropy: 0.825816.\n",
      "episode: 1548   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 466     evaluation reward: 5.73\n",
      "episode: 1549   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 412     evaluation reward: 5.76\n",
      "episode: 1550   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 454     evaluation reward: 5.77\n",
      "Training network. lr: 0.000238. clip: 0.095131\n",
      "Iteration 1624: Policy loss: 0.003639. Value loss: 0.018810. Entropy: 0.872825.\n",
      "Iteration 1625: Policy loss: -0.017418. Value loss: 0.013842. Entropy: 0.871174.\n",
      "Iteration 1626: Policy loss: -0.028701. Value loss: 0.011666. Entropy: 0.869583.\n",
      "episode: 1551   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 618     evaluation reward: 5.81\n",
      "Training network. lr: 0.000238. clip: 0.095122\n",
      "Iteration 1627: Policy loss: 0.008902. Value loss: 0.019460. Entropy: 0.781394.\n",
      "Iteration 1628: Policy loss: -0.016343. Value loss: 0.014821. Entropy: 0.766590.\n",
      "Iteration 1629: Policy loss: -0.024802. Value loss: 0.012691. Entropy: 0.764134.\n",
      "episode: 1552   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 564     evaluation reward: 5.84\n",
      "episode: 1553   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 441     evaluation reward: 5.8\n",
      "Training network. lr: 0.000238. clip: 0.095113\n",
      "Iteration 1630: Policy loss: 0.007955. Value loss: 0.019349. Entropy: 0.878488.\n",
      "Iteration 1631: Policy loss: -0.011572. Value loss: 0.013955. Entropy: 0.876446.\n",
      "Iteration 1632: Policy loss: -0.021632. Value loss: 0.011156. Entropy: 0.875750.\n",
      "episode: 1554   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 500     evaluation reward: 5.82\n",
      "episode: 1555   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 578     evaluation reward: 5.8\n",
      "Training network. lr: 0.000238. clip: 0.095104\n",
      "Iteration 1633: Policy loss: 0.015836. Value loss: 0.023238. Entropy: 0.757180.\n",
      "Iteration 1634: Policy loss: -0.008103. Value loss: 0.017519. Entropy: 0.763633.\n",
      "Iteration 1635: Policy loss: -0.022091. Value loss: 0.013685. Entropy: 0.764780.\n",
      "episode: 1556   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 537     evaluation reward: 5.83\n",
      "episode: 1557   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 417     evaluation reward: 5.82\n",
      "Training network. lr: 0.000238. clip: 0.095095\n",
      "Iteration 1636: Policy loss: 0.010326. Value loss: 0.025374. Entropy: 0.821080.\n",
      "Iteration 1637: Policy loss: -0.012102. Value loss: 0.018017. Entropy: 0.830007.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1638: Policy loss: -0.027292. Value loss: 0.013952. Entropy: 0.819785.\n",
      "episode: 1558   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 555     evaluation reward: 5.85\n",
      "episode: 1559   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 427     evaluation reward: 5.8\n",
      "episode: 1560   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 398     evaluation reward: 5.8\n",
      "Training network. lr: 0.000238. clip: 0.095086\n",
      "Iteration 1639: Policy loss: 0.009146. Value loss: 0.021654. Entropy: 0.840994.\n",
      "Iteration 1640: Policy loss: -0.012995. Value loss: 0.015960. Entropy: 0.839430.\n",
      "Iteration 1641: Policy loss: -0.022670. Value loss: 0.012799. Entropy: 0.832768.\n",
      "episode: 1561   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 696     evaluation reward: 5.84\n",
      "Training network. lr: 0.000238. clip: 0.095077\n",
      "Iteration 1642: Policy loss: 0.004937. Value loss: 0.021391. Entropy: 0.765707.\n",
      "Iteration 1643: Policy loss: -0.010767. Value loss: 0.014561. Entropy: 0.765151.\n",
      "Iteration 1644: Policy loss: -0.022783. Value loss: 0.013628. Entropy: 0.761910.\n",
      "episode: 1562   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 582     evaluation reward: 5.87\n",
      "episode: 1563   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 491     evaluation reward: 5.89\n",
      "Training network. lr: 0.000238. clip: 0.095068\n",
      "Iteration 1645: Policy loss: 0.014390. Value loss: 0.032126. Entropy: 0.757939.\n",
      "Iteration 1646: Policy loss: 0.000227. Value loss: 0.015132. Entropy: 0.734024.\n",
      "Iteration 1647: Policy loss: -0.016108. Value loss: 0.011748. Entropy: 0.735615.\n",
      "episode: 1564   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 582     evaluation reward: 5.95\n",
      "episode: 1565   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 638     evaluation reward: 5.98\n",
      "Training network. lr: 0.000238. clip: 0.095059\n",
      "Iteration 1648: Policy loss: 0.012342. Value loss: 0.025024. Entropy: 0.749523.\n",
      "Iteration 1649: Policy loss: -0.008017. Value loss: 0.016289. Entropy: 0.734855.\n",
      "Iteration 1650: Policy loss: -0.018116. Value loss: 0.013622. Entropy: 0.740520.\n",
      "episode: 1566   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 326     evaluation reward: 6.02\n",
      "episode: 1567   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 489     evaluation reward: 6.05\n",
      "Training network. lr: 0.000238. clip: 0.095050\n",
      "Iteration 1651: Policy loss: 0.003329. Value loss: 0.041973. Entropy: 0.700038.\n",
      "Iteration 1652: Policy loss: -0.008129. Value loss: 0.032946. Entropy: 0.704941.\n",
      "Iteration 1653: Policy loss: -0.018212. Value loss: 0.028251. Entropy: 0.686935.\n",
      "episode: 1568   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 864     evaluation reward: 6.11\n",
      "episode: 1569   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 346     evaluation reward: 6.06\n",
      "Training network. lr: 0.000238. clip: 0.095041\n",
      "Iteration 1654: Policy loss: 0.047411. Value loss: 0.065708. Entropy: 0.722909.\n",
      "Iteration 1655: Policy loss: 0.000211. Value loss: 0.022965. Entropy: 0.712931.\n",
      "Iteration 1656: Policy loss: -0.012477. Value loss: 0.019199. Entropy: 0.702311.\n",
      "episode: 1570   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 590     evaluation reward: 6.07\n",
      "episode: 1571   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 386     evaluation reward: 6.06\n",
      "Training network. lr: 0.000238. clip: 0.095032\n",
      "Iteration 1657: Policy loss: 0.009554. Value loss: 0.020383. Entropy: 0.712858.\n",
      "Iteration 1658: Policy loss: -0.012724. Value loss: 0.014891. Entropy: 0.703856.\n",
      "Iteration 1659: Policy loss: -0.023036. Value loss: 0.012126. Entropy: 0.717244.\n",
      "episode: 1572   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 609     evaluation reward: 6.11\n",
      "episode: 1573   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 325     evaluation reward: 6.12\n",
      "Training network. lr: 0.000238. clip: 0.095023\n",
      "Iteration 1660: Policy loss: 0.009097. Value loss: 0.020267. Entropy: 0.791908.\n",
      "Iteration 1661: Policy loss: -0.015859. Value loss: 0.012466. Entropy: 0.793253.\n",
      "Iteration 1662: Policy loss: -0.023200. Value loss: 0.009687. Entropy: 0.790235.\n",
      "episode: 1574   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 622     evaluation reward: 6.13\n",
      "episode: 1575   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 660     evaluation reward: 6.2\n",
      "Training network. lr: 0.000238. clip: 0.095014\n",
      "Iteration 1663: Policy loss: 0.010109. Value loss: 0.025334. Entropy: 0.744986.\n",
      "Iteration 1664: Policy loss: -0.011889. Value loss: 0.019566. Entropy: 0.732069.\n",
      "Iteration 1665: Policy loss: -0.027853. Value loss: 0.015434. Entropy: 0.745554.\n",
      "episode: 1576   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 569     evaluation reward: 6.22\n",
      "episode: 1577   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 404     evaluation reward: 6.22\n",
      "Training network. lr: 0.000238. clip: 0.095005\n",
      "Iteration 1666: Policy loss: 0.008817. Value loss: 0.027277. Entropy: 0.603177.\n",
      "Iteration 1667: Policy loss: -0.007432. Value loss: 0.020484. Entropy: 0.624181.\n",
      "Iteration 1668: Policy loss: -0.026115. Value loss: 0.016952. Entropy: 0.604635.\n",
      "episode: 1578   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 632     evaluation reward: 6.26\n",
      "episode: 1579   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 326     evaluation reward: 6.24\n",
      "Training network. lr: 0.000237. clip: 0.094996\n",
      "Iteration 1669: Policy loss: 0.017642. Value loss: 0.025996. Entropy: 0.652824.\n",
      "Iteration 1670: Policy loss: -0.002899. Value loss: 0.018924. Entropy: 0.656389.\n",
      "Iteration 1671: Policy loss: -0.015688. Value loss: 0.015768. Entropy: 0.665127.\n",
      "episode: 1580   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 621     evaluation reward: 6.27\n",
      "episode: 1581   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 443     evaluation reward: 6.27\n",
      "Training network. lr: 0.000237. clip: 0.094987\n",
      "Iteration 1672: Policy loss: 0.017029. Value loss: 0.034585. Entropy: 0.669465.\n",
      "Iteration 1673: Policy loss: -0.003315. Value loss: 0.025530. Entropy: 0.675849.\n",
      "Iteration 1674: Policy loss: -0.015471. Value loss: 0.021789. Entropy: 0.670548.\n",
      "episode: 1582   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 649     evaluation reward: 6.32\n",
      "Training network. lr: 0.000237. clip: 0.094978\n",
      "Iteration 1675: Policy loss: 0.009876. Value loss: 0.027461. Entropy: 0.738953.\n",
      "Iteration 1676: Policy loss: -0.010435. Value loss: 0.020573. Entropy: 0.738334.\n",
      "Iteration 1677: Policy loss: -0.030336. Value loss: 0.015667. Entropy: 0.722948.\n",
      "episode: 1583   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 592     evaluation reward: 6.34\n",
      "episode: 1584   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 359     evaluation reward: 6.36\n",
      "episode: 1585   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 423     evaluation reward: 6.35\n",
      "Training network. lr: 0.000237. clip: 0.094969\n",
      "Iteration 1678: Policy loss: 0.010689. Value loss: 0.024688. Entropy: 0.745331.\n",
      "Iteration 1679: Policy loss: -0.010283. Value loss: 0.019270. Entropy: 0.751367.\n",
      "Iteration 1680: Policy loss: -0.022398. Value loss: 0.016681. Entropy: 0.751346.\n",
      "episode: 1586   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 375     evaluation reward: 6.32\n",
      "episode: 1587   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 635     evaluation reward: 6.34\n",
      "Training network. lr: 0.000237. clip: 0.094960\n",
      "Iteration 1681: Policy loss: 0.005832. Value loss: 0.023578. Entropy: 0.713811.\n",
      "Iteration 1682: Policy loss: -0.011518. Value loss: 0.018163. Entropy: 0.700158.\n",
      "Iteration 1683: Policy loss: -0.022969. Value loss: 0.015135. Entropy: 0.698164.\n",
      "episode: 1588   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 659     evaluation reward: 6.33\n",
      "Training network. lr: 0.000237. clip: 0.094951\n",
      "Iteration 1684: Policy loss: 0.013268. Value loss: 0.023143. Entropy: 0.747340.\n",
      "Iteration 1685: Policy loss: -0.005712. Value loss: 0.017730. Entropy: 0.754490.\n",
      "Iteration 1686: Policy loss: -0.018139. Value loss: 0.014600. Entropy: 0.758725.\n",
      "episode: 1589   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 617     evaluation reward: 6.33\n",
      "episode: 1590   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 625     evaluation reward: 6.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000237. clip: 0.094942\n",
      "Iteration 1687: Policy loss: 0.009070. Value loss: 0.016440. Entropy: 0.637769.\n",
      "Iteration 1688: Policy loss: -0.010778. Value loss: 0.012190. Entropy: 0.648292.\n",
      "Iteration 1689: Policy loss: -0.019149. Value loss: 0.010758. Entropy: 0.639824.\n",
      "episode: 1591   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 521     evaluation reward: 6.37\n",
      "episode: 1592   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 558     evaluation reward: 6.43\n",
      "Training network. lr: 0.000237. clip: 0.094933\n",
      "Iteration 1690: Policy loss: 0.008745. Value loss: 0.019361. Entropy: 0.672722.\n",
      "Iteration 1691: Policy loss: -0.008540. Value loss: 0.013357. Entropy: 0.675218.\n",
      "Iteration 1692: Policy loss: -0.019865. Value loss: 0.010845. Entropy: 0.671289.\n",
      "episode: 1593   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 541     evaluation reward: 6.39\n",
      "episode: 1594   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 644     evaluation reward: 6.42\n",
      "Training network. lr: 0.000237. clip: 0.094924\n",
      "Iteration 1693: Policy loss: 0.021266. Value loss: 0.024490. Entropy: 0.755038.\n",
      "Iteration 1694: Policy loss: -0.005446. Value loss: 0.017291. Entropy: 0.752938.\n",
      "Iteration 1695: Policy loss: -0.019197. Value loss: 0.013468. Entropy: 0.760455.\n",
      "episode: 1595   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 438     evaluation reward: 6.42\n",
      "episode: 1596   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 357     evaluation reward: 6.41\n",
      "Training network. lr: 0.000237. clip: 0.094915\n",
      "Iteration 1696: Policy loss: 0.011784. Value loss: 0.020861. Entropy: 0.718304.\n",
      "Iteration 1697: Policy loss: -0.013034. Value loss: 0.015694. Entropy: 0.710236.\n",
      "Iteration 1698: Policy loss: -0.024875. Value loss: 0.012564. Entropy: 0.705832.\n",
      "episode: 1597   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 649     evaluation reward: 6.43\n",
      "episode: 1598   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 502     evaluation reward: 6.45\n",
      "Training network. lr: 0.000237. clip: 0.094906\n",
      "Iteration 1699: Policy loss: 0.014692. Value loss: 0.023348. Entropy: 0.778802.\n",
      "Iteration 1700: Policy loss: -0.012033. Value loss: 0.017754. Entropy: 0.777953.\n",
      "Iteration 1701: Policy loss: -0.022015. Value loss: 0.013677. Entropy: 0.768155.\n",
      "episode: 1599   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 650     evaluation reward: 6.51\n",
      "episode: 1600   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 630     evaluation reward: 6.51\n",
      "Training network. lr: 0.000237. clip: 0.094897\n",
      "Iteration 1702: Policy loss: 0.010648. Value loss: 0.020777. Entropy: 0.837538.\n",
      "Iteration 1703: Policy loss: -0.014776. Value loss: 0.014342. Entropy: 0.834012.\n",
      "Iteration 1704: Policy loss: -0.025011. Value loss: 0.012589. Entropy: 0.826964.\n",
      "episode: 1601   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 614     evaluation reward: 6.53\n",
      "episode: 1602   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 383     evaluation reward: 6.52\n",
      "Training network. lr: 0.000237. clip: 0.094888\n",
      "Iteration 1705: Policy loss: 0.011120. Value loss: 0.024572. Entropy: 0.729387.\n",
      "Iteration 1706: Policy loss: -0.015274. Value loss: 0.017544. Entropy: 0.721728.\n",
      "Iteration 1707: Policy loss: -0.027052. Value loss: 0.014805. Entropy: 0.720075.\n",
      "episode: 1603   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 494     evaluation reward: 6.48\n",
      "Training network. lr: 0.000237. clip: 0.094879\n",
      "Iteration 1708: Policy loss: 0.011207. Value loss: 0.020481. Entropy: 0.722335.\n",
      "Iteration 1709: Policy loss: -0.010060. Value loss: 0.015513. Entropy: 0.714408.\n",
      "Iteration 1710: Policy loss: -0.026094. Value loss: 0.012241. Entropy: 0.701482.\n",
      "episode: 1604   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 591     evaluation reward: 6.48\n",
      "episode: 1605   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 643     evaluation reward: 6.51\n",
      "Training network. lr: 0.000237. clip: 0.094870\n",
      "Iteration 1711: Policy loss: 0.013771. Value loss: 0.016408. Entropy: 0.576532.\n",
      "Iteration 1712: Policy loss: -0.000644. Value loss: 0.011453. Entropy: 0.579360.\n",
      "Iteration 1713: Policy loss: -0.018733. Value loss: 0.010146. Entropy: 0.594785.\n",
      "episode: 1606   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 623     evaluation reward: 6.55\n",
      "episode: 1607   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 508     evaluation reward: 6.54\n",
      "Training network. lr: 0.000237. clip: 0.094861\n",
      "Iteration 1714: Policy loss: 0.012390. Value loss: 0.020521. Entropy: 0.683328.\n",
      "Iteration 1715: Policy loss: -0.004996. Value loss: 0.015754. Entropy: 0.666023.\n",
      "Iteration 1716: Policy loss: -0.020192. Value loss: 0.012332. Entropy: 0.663362.\n",
      "episode: 1608   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 692     evaluation reward: 6.58\n",
      "episode: 1609   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 517     evaluation reward: 6.57\n",
      "Training network. lr: 0.000237. clip: 0.094852\n",
      "Iteration 1717: Policy loss: 0.010008. Value loss: 0.025061. Entropy: 0.781739.\n",
      "Iteration 1718: Policy loss: -0.009030. Value loss: 0.016294. Entropy: 0.782586.\n",
      "Iteration 1719: Policy loss: -0.023002. Value loss: 0.013193. Entropy: 0.786520.\n",
      "episode: 1610   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 530     evaluation reward: 6.58\n",
      "Training network. lr: 0.000237. clip: 0.094843\n",
      "Iteration 1720: Policy loss: 0.010892. Value loss: 0.019070. Entropy: 0.701792.\n",
      "Iteration 1721: Policy loss: -0.007843. Value loss: 0.014880. Entropy: 0.703696.\n",
      "Iteration 1722: Policy loss: -0.015740. Value loss: 0.012408. Entropy: 0.690769.\n",
      "episode: 1611   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 813     evaluation reward: 6.59\n",
      "episode: 1612   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 610     evaluation reward: 6.61\n",
      "Training network. lr: 0.000237. clip: 0.094834\n",
      "Iteration 1723: Policy loss: 0.010245. Value loss: 0.024642. Entropy: 0.683374.\n",
      "Iteration 1724: Policy loss: -0.012998. Value loss: 0.016916. Entropy: 0.681353.\n",
      "Iteration 1725: Policy loss: -0.026507. Value loss: 0.013721. Entropy: 0.683333.\n",
      "episode: 1613   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 536     evaluation reward: 6.63\n",
      "Training network. lr: 0.000237. clip: 0.094825\n",
      "Iteration 1726: Policy loss: 0.007533. Value loss: 0.021125. Entropy: 0.702602.\n",
      "Iteration 1727: Policy loss: -0.007200. Value loss: 0.015188. Entropy: 0.712252.\n",
      "Iteration 1728: Policy loss: -0.022399. Value loss: 0.013408. Entropy: 0.711284.\n",
      "episode: 1614   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 759     evaluation reward: 6.61\n",
      "episode: 1615   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 343     evaluation reward: 6.6\n",
      "Training network. lr: 0.000237. clip: 0.094816\n",
      "Iteration 1729: Policy loss: 0.007534. Value loss: 0.027624. Entropy: 0.771248.\n",
      "Iteration 1730: Policy loss: -0.008162. Value loss: 0.020012. Entropy: 0.765693.\n",
      "Iteration 1731: Policy loss: -0.022934. Value loss: 0.015523. Entropy: 0.776379.\n",
      "episode: 1616   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 805     evaluation reward: 6.67\n",
      "episode: 1617   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 337     evaluation reward: 6.68\n",
      "Training network. lr: 0.000237. clip: 0.094807\n",
      "Iteration 1732: Policy loss: 0.013952. Value loss: 0.025977. Entropy: 0.794930.\n",
      "Iteration 1733: Policy loss: -0.013466. Value loss: 0.017924. Entropy: 0.798732.\n",
      "Iteration 1734: Policy loss: -0.024808. Value loss: 0.013269. Entropy: 0.797662.\n",
      "episode: 1618   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 549     evaluation reward: 6.71\n",
      "episode: 1619   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 382     evaluation reward: 6.7\n",
      "Training network. lr: 0.000237. clip: 0.094798\n",
      "Iteration 1735: Policy loss: 0.009362. Value loss: 0.025903. Entropy: 0.779441.\n",
      "Iteration 1736: Policy loss: -0.016654. Value loss: 0.018200. Entropy: 0.770206.\n",
      "Iteration 1737: Policy loss: -0.021300. Value loss: 0.014594. Entropy: 0.768614.\n",
      "episode: 1620   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 576     evaluation reward: 6.76\n",
      "episode: 1621   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 662     evaluation reward: 6.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000237. clip: 0.094789\n",
      "Iteration 1738: Policy loss: 0.016547. Value loss: 0.034204. Entropy: 0.690071.\n",
      "Iteration 1739: Policy loss: -0.008633. Value loss: 0.026619. Entropy: 0.700312.\n",
      "Iteration 1740: Policy loss: -0.017667. Value loss: 0.022370. Entropy: 0.692164.\n",
      "episode: 1622   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 610     evaluation reward: 6.81\n",
      "episode: 1623   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 508     evaluation reward: 6.82\n",
      "Training network. lr: 0.000237. clip: 0.094780\n",
      "Iteration 1741: Policy loss: 0.008237. Value loss: 0.019456. Entropy: 0.717944.\n",
      "Iteration 1742: Policy loss: -0.010908. Value loss: 0.014147. Entropy: 0.714642.\n",
      "Iteration 1743: Policy loss: -0.022420. Value loss: 0.011230. Entropy: 0.720054.\n",
      "episode: 1624   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 509     evaluation reward: 6.84\n",
      "episode: 1625   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 563     evaluation reward: 6.86\n",
      "Training network. lr: 0.000237. clip: 0.094771\n",
      "Iteration 1744: Policy loss: 0.007575. Value loss: 0.019471. Entropy: 0.665050.\n",
      "Iteration 1745: Policy loss: -0.012057. Value loss: 0.014230. Entropy: 0.670904.\n",
      "Iteration 1746: Policy loss: -0.020091. Value loss: 0.011482. Entropy: 0.684603.\n",
      "episode: 1626   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 520     evaluation reward: 6.88\n",
      "episode: 1627   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 565     evaluation reward: 6.91\n",
      "Training network. lr: 0.000237. clip: 0.094762\n",
      "Iteration 1747: Policy loss: 0.014491. Value loss: 0.021556. Entropy: 0.740955.\n",
      "Iteration 1748: Policy loss: -0.010996. Value loss: 0.016707. Entropy: 0.738176.\n",
      "Iteration 1749: Policy loss: -0.024752. Value loss: 0.012929. Entropy: 0.728008.\n",
      "episode: 1628   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 472     evaluation reward: 6.9\n",
      "episode: 1629   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 526     evaluation reward: 6.89\n",
      "Training network. lr: 0.000237. clip: 0.094753\n",
      "Iteration 1750: Policy loss: 0.010741. Value loss: 0.017822. Entropy: 0.690865.\n",
      "Iteration 1751: Policy loss: -0.011163. Value loss: 0.013897. Entropy: 0.695283.\n",
      "Iteration 1752: Policy loss: -0.018691. Value loss: 0.010492. Entropy: 0.700496.\n",
      "episode: 1630   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 519     evaluation reward: 6.89\n",
      "episode: 1631   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 517     evaluation reward: 6.92\n",
      "Training network. lr: 0.000237. clip: 0.094744\n",
      "Iteration 1753: Policy loss: 0.007123. Value loss: 0.016197. Entropy: 0.694530.\n",
      "Iteration 1754: Policy loss: -0.005585. Value loss: 0.012568. Entropy: 0.690713.\n",
      "Iteration 1755: Policy loss: -0.016335. Value loss: 0.011161. Entropy: 0.699424.\n",
      "episode: 1632   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 643     evaluation reward: 6.93\n",
      "episode: 1633   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 263     evaluation reward: 6.86\n",
      "now time :  2018-12-26 12:58:36.994040\n",
      "Training network. lr: 0.000237. clip: 0.094735\n",
      "Iteration 1756: Policy loss: 0.008783. Value loss: 0.019931. Entropy: 0.750563.\n",
      "Iteration 1757: Policy loss: -0.014797. Value loss: 0.013437. Entropy: 0.752188.\n",
      "Iteration 1758: Policy loss: -0.022507. Value loss: 0.011470. Entropy: 0.748532.\n",
      "episode: 1634   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 481     evaluation reward: 6.83\n",
      "episode: 1635   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 289     evaluation reward: 6.76\n",
      "Training network. lr: 0.000237. clip: 0.094726\n",
      "Iteration 1759: Policy loss: 0.007124. Value loss: 0.028267. Entropy: 0.734503.\n",
      "Iteration 1760: Policy loss: -0.007302. Value loss: 0.017686. Entropy: 0.728463.\n",
      "Iteration 1761: Policy loss: -0.019109. Value loss: 0.014214. Entropy: 0.728466.\n",
      "episode: 1636   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 602     evaluation reward: 6.79\n",
      "episode: 1637   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 732     evaluation reward: 6.88\n",
      "Training network. lr: 0.000237. clip: 0.094717\n",
      "Iteration 1762: Policy loss: 0.010609. Value loss: 0.055134. Entropy: 0.716300.\n",
      "Iteration 1763: Policy loss: -0.007333. Value loss: 0.041560. Entropy: 0.696265.\n",
      "Iteration 1764: Policy loss: -0.017473. Value loss: 0.035513. Entropy: 0.713117.\n",
      "episode: 1638   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 628     evaluation reward: 6.89\n",
      "episode: 1639   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 338     evaluation reward: 6.86\n",
      "Training network. lr: 0.000237. clip: 0.094708\n",
      "Iteration 1765: Policy loss: 0.012208. Value loss: 0.021736. Entropy: 0.731327.\n",
      "Iteration 1766: Policy loss: -0.008429. Value loss: 0.014311. Entropy: 0.733997.\n",
      "Iteration 1767: Policy loss: -0.022587. Value loss: 0.012482. Entropy: 0.744154.\n",
      "episode: 1640   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 544     evaluation reward: 6.91\n",
      "episode: 1641   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 514     evaluation reward: 6.93\n",
      "Training network. lr: 0.000237. clip: 0.094699\n",
      "Iteration 1768: Policy loss: 0.020702. Value loss: 0.021638. Entropy: 0.744606.\n",
      "Iteration 1769: Policy loss: -0.006500. Value loss: 0.016864. Entropy: 0.736214.\n",
      "Iteration 1770: Policy loss: -0.020089. Value loss: 0.013547. Entropy: 0.746889.\n",
      "episode: 1642   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 434     evaluation reward: 6.95\n",
      "episode: 1643   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 412     evaluation reward: 6.94\n",
      "episode: 1644   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 378     evaluation reward: 6.9\n",
      "Training network. lr: 0.000237. clip: 0.094690\n",
      "Iteration 1771: Policy loss: 0.011998. Value loss: 0.024405. Entropy: 0.827213.\n",
      "Iteration 1772: Policy loss: -0.007918. Value loss: 0.017629. Entropy: 0.818143.\n",
      "Iteration 1773: Policy loss: -0.020717. Value loss: 0.014490. Entropy: 0.822432.\n",
      "episode: 1645   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 466     evaluation reward: 6.9\n",
      "episode: 1646   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 320     evaluation reward: 6.87\n",
      "Training network. lr: 0.000237. clip: 0.094681\n",
      "Iteration 1774: Policy loss: 0.009007. Value loss: 0.016985. Entropy: 0.736602.\n",
      "Iteration 1775: Policy loss: -0.014525. Value loss: 0.012648. Entropy: 0.742517.\n",
      "Iteration 1776: Policy loss: -0.026283. Value loss: 0.010186. Entropy: 0.746918.\n",
      "episode: 1647   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 580     evaluation reward: 6.91\n",
      "episode: 1648   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 506     evaluation reward: 6.92\n",
      "Training network. lr: 0.000237. clip: 0.094672\n",
      "Iteration 1777: Policy loss: 0.012087. Value loss: 0.020586. Entropy: 0.741994.\n",
      "Iteration 1778: Policy loss: -0.009891. Value loss: 0.015789. Entropy: 0.729255.\n",
      "Iteration 1779: Policy loss: -0.022817. Value loss: 0.012825. Entropy: 0.730665.\n",
      "episode: 1649   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 622     evaluation reward: 6.97\n",
      "Training network. lr: 0.000237. clip: 0.094663\n",
      "Iteration 1780: Policy loss: 0.008391. Value loss: 0.014587. Entropy: 0.816321.\n",
      "Iteration 1781: Policy loss: -0.013598. Value loss: 0.010342. Entropy: 0.816849.\n",
      "Iteration 1782: Policy loss: -0.023023. Value loss: 0.009085. Entropy: 0.816272.\n",
      "episode: 1650   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 621     evaluation reward: 7.01\n",
      "episode: 1651   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 337     evaluation reward: 6.97\n",
      "episode: 1652   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 382     evaluation reward: 6.94\n",
      "Training network. lr: 0.000237. clip: 0.094654\n",
      "Iteration 1783: Policy loss: 0.007802. Value loss: 0.026418. Entropy: 0.748501.\n",
      "Iteration 1784: Policy loss: -0.016066. Value loss: 0.019050. Entropy: 0.742688.\n",
      "Iteration 1785: Policy loss: -0.025339. Value loss: 0.016154. Entropy: 0.734307.\n",
      "episode: 1653   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 625     evaluation reward: 7.01\n",
      "episode: 1654   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 622     evaluation reward: 7.04\n",
      "Training network. lr: 0.000237. clip: 0.094645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1786: Policy loss: 0.010602. Value loss: 0.047249. Entropy: 0.799429.\n",
      "Iteration 1787: Policy loss: -0.003121. Value loss: 0.039370. Entropy: 0.793058.\n",
      "Iteration 1788: Policy loss: -0.016522. Value loss: 0.031707. Entropy: 0.788974.\n",
      "episode: 1655   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 7.02\n",
      "episode: 1656   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 594     evaluation reward: 7.02\n",
      "Training network. lr: 0.000237. clip: 0.094636\n",
      "Iteration 1789: Policy loss: 0.009022. Value loss: 0.024115. Entropy: 0.775667.\n",
      "Iteration 1790: Policy loss: -0.011769. Value loss: 0.019193. Entropy: 0.764524.\n",
      "Iteration 1791: Policy loss: -0.023262. Value loss: 0.016067. Entropy: 0.763424.\n",
      "episode: 1657   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 468     evaluation reward: 7.03\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1792: Policy loss: 0.008167. Value loss: 0.024152. Entropy: 0.681536.\n",
      "Iteration 1793: Policy loss: -0.013272. Value loss: 0.016265. Entropy: 0.689492.\n",
      "Iteration 1794: Policy loss: -0.024255. Value loss: 0.012378. Entropy: 0.688664.\n",
      "episode: 1658   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 594     evaluation reward: 7.04\n",
      "episode: 1659   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 485     evaluation reward: 7.05\n",
      "Training network. lr: 0.000237. clip: 0.094618\n",
      "Iteration 1795: Policy loss: 0.011567. Value loss: 0.034043. Entropy: 0.748615.\n",
      "Iteration 1796: Policy loss: -0.012254. Value loss: 0.024862. Entropy: 0.747166.\n",
      "Iteration 1797: Policy loss: -0.019900. Value loss: 0.020125. Entropy: 0.733258.\n",
      "episode: 1660   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 604     evaluation reward: 7.1\n",
      "episode: 1661   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 463     evaluation reward: 7.05\n",
      "Training network. lr: 0.000237. clip: 0.094609\n",
      "Iteration 1798: Policy loss: 0.010366. Value loss: 0.023323. Entropy: 0.816111.\n",
      "Iteration 1799: Policy loss: -0.018252. Value loss: 0.017288. Entropy: 0.812979.\n",
      "Iteration 1800: Policy loss: -0.027653. Value loss: 0.014037. Entropy: 0.813752.\n",
      "episode: 1662   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 520     evaluation reward: 7.04\n",
      "episode: 1663   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 603     evaluation reward: 7.07\n",
      "Training network. lr: 0.000237. clip: 0.094600\n",
      "Iteration 1801: Policy loss: 0.010903. Value loss: 0.017587. Entropy: 0.804473.\n",
      "Iteration 1802: Policy loss: -0.011375. Value loss: 0.012073. Entropy: 0.796920.\n",
      "Iteration 1803: Policy loss: -0.026662. Value loss: 0.010081. Entropy: 0.796632.\n",
      "episode: 1664   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 588     evaluation reward: 7.06\n",
      "episode: 1665   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 570     evaluation reward: 7.04\n",
      "Training network. lr: 0.000236. clip: 0.094591\n",
      "Iteration 1804: Policy loss: 0.007795. Value loss: 0.020473. Entropy: 0.788194.\n",
      "Iteration 1805: Policy loss: -0.011979. Value loss: 0.013893. Entropy: 0.785153.\n",
      "Iteration 1806: Policy loss: -0.027974. Value loss: 0.011572. Entropy: 0.791367.\n",
      "episode: 1666   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 722     evaluation reward: 7.08\n",
      "episode: 1667   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 537     evaluation reward: 7.1\n",
      "Training network. lr: 0.000236. clip: 0.094582\n",
      "Iteration 1807: Policy loss: 0.009657. Value loss: 0.024043. Entropy: 0.766671.\n",
      "Iteration 1808: Policy loss: -0.012362. Value loss: 0.016620. Entropy: 0.766968.\n",
      "Iteration 1809: Policy loss: -0.025879. Value loss: 0.013150. Entropy: 0.769831.\n",
      "episode: 1668   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 637     evaluation reward: 7.07\n",
      "Training network. lr: 0.000236. clip: 0.094573\n",
      "Iteration 1810: Policy loss: 0.008238. Value loss: 0.021689. Entropy: 0.775487.\n",
      "Iteration 1811: Policy loss: -0.014291. Value loss: 0.015577. Entropy: 0.771047.\n",
      "Iteration 1812: Policy loss: -0.023324. Value loss: 0.012966. Entropy: 0.761116.\n",
      "episode: 1669   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 530     evaluation reward: 7.13\n",
      "episode: 1670   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 695     evaluation reward: 7.16\n",
      "Training network. lr: 0.000236. clip: 0.094564\n",
      "Iteration 1813: Policy loss: 0.006379. Value loss: 0.044111. Entropy: 0.726751.\n",
      "Iteration 1814: Policy loss: -0.010678. Value loss: 0.035597. Entropy: 0.748557.\n",
      "Iteration 1815: Policy loss: -0.020960. Value loss: 0.032340. Entropy: 0.729001.\n",
      "episode: 1671   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 359     evaluation reward: 7.16\n",
      "episode: 1672   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 527     evaluation reward: 7.15\n",
      "Training network. lr: 0.000236. clip: 0.094555\n",
      "Iteration 1816: Policy loss: 0.006652. Value loss: 0.026389. Entropy: 0.736233.\n",
      "Iteration 1817: Policy loss: -0.014167. Value loss: 0.016495. Entropy: 0.716698.\n",
      "Iteration 1818: Policy loss: -0.025943. Value loss: 0.013416. Entropy: 0.708285.\n",
      "episode: 1673   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 400     evaluation reward: 7.16\n",
      "episode: 1674   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 673     evaluation reward: 7.2\n",
      "Training network. lr: 0.000236. clip: 0.094546\n",
      "Iteration 1819: Policy loss: 0.011970. Value loss: 0.048244. Entropy: 0.780848.\n",
      "Iteration 1820: Policy loss: -0.007717. Value loss: 0.033176. Entropy: 0.753087.\n",
      "Iteration 1821: Policy loss: -0.021081. Value loss: 0.025470. Entropy: 0.765524.\n",
      "episode: 1675   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 575     evaluation reward: 7.18\n",
      "episode: 1676   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 466     evaluation reward: 7.19\n",
      "Training network. lr: 0.000236. clip: 0.094537\n",
      "Iteration 1822: Policy loss: 0.006670. Value loss: 0.038984. Entropy: 0.729867.\n",
      "Iteration 1823: Policy loss: -0.010786. Value loss: 0.031398. Entropy: 0.741023.\n",
      "Iteration 1824: Policy loss: -0.015303. Value loss: 0.025614. Entropy: 0.744906.\n",
      "episode: 1677   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 637     evaluation reward: 7.23\n",
      "episode: 1678   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 7.19\n",
      "Training network. lr: 0.000236. clip: 0.094528\n",
      "Iteration 1825: Policy loss: 0.010930. Value loss: 0.024539. Entropy: 0.832928.\n",
      "Iteration 1826: Policy loss: -0.013045. Value loss: 0.017718. Entropy: 0.827960.\n",
      "Iteration 1827: Policy loss: -0.027047. Value loss: 0.014508. Entropy: 0.829734.\n",
      "episode: 1679   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 438     evaluation reward: 7.21\n",
      "episode: 1680   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 561     evaluation reward: 7.19\n",
      "Training network. lr: 0.000236. clip: 0.094519\n",
      "Iteration 1828: Policy loss: 0.004540. Value loss: 0.026711. Entropy: 0.717711.\n",
      "Iteration 1829: Policy loss: -0.010383. Value loss: 0.018334. Entropy: 0.715429.\n",
      "Iteration 1830: Policy loss: -0.024418. Value loss: 0.015813. Entropy: 0.717460.\n",
      "episode: 1681   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 468     evaluation reward: 7.21\n",
      "episode: 1682   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 318     evaluation reward: 7.15\n",
      "Training network. lr: 0.000236. clip: 0.094510\n",
      "Iteration 1831: Policy loss: 0.005541. Value loss: 0.028239. Entropy: 0.798791.\n",
      "Iteration 1832: Policy loss: -0.014158. Value loss: 0.017743. Entropy: 0.786717.\n",
      "Iteration 1833: Policy loss: -0.026492. Value loss: 0.014442. Entropy: 0.799073.\n",
      "episode: 1683   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 721     evaluation reward: 7.18\n",
      "episode: 1684   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 470     evaluation reward: 7.19\n",
      "Training network. lr: 0.000236. clip: 0.094501\n",
      "Iteration 1834: Policy loss: 0.011196. Value loss: 0.029676. Entropy: 0.811478.\n",
      "Iteration 1835: Policy loss: -0.008061. Value loss: 0.021255. Entropy: 0.811345.\n",
      "Iteration 1836: Policy loss: -0.024236. Value loss: 0.017080. Entropy: 0.815630.\n",
      "episode: 1685   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 562     evaluation reward: 7.22\n",
      "episode: 1686   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 452     evaluation reward: 7.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000236. clip: 0.094492\n",
      "Iteration 1837: Policy loss: 0.010777. Value loss: 0.028933. Entropy: 0.734801.\n",
      "Iteration 1838: Policy loss: -0.010436. Value loss: 0.022825. Entropy: 0.754028.\n",
      "Iteration 1839: Policy loss: -0.024045. Value loss: 0.019159. Entropy: 0.750539.\n",
      "episode: 1687   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 456     evaluation reward: 7.21\n",
      "episode: 1688   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 656     evaluation reward: 7.22\n",
      "Training network. lr: 0.000236. clip: 0.094483\n",
      "Iteration 1840: Policy loss: 0.012167. Value loss: 0.019356. Entropy: 0.741945.\n",
      "Iteration 1841: Policy loss: -0.007180. Value loss: 0.015503. Entropy: 0.738160.\n",
      "Iteration 1842: Policy loss: -0.019476. Value loss: 0.012300. Entropy: 0.740586.\n",
      "episode: 1689   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 619     evaluation reward: 7.22\n",
      "Training network. lr: 0.000236. clip: 0.094474\n",
      "Iteration 1843: Policy loss: 0.007534. Value loss: 0.024499. Entropy: 0.754336.\n",
      "Iteration 1844: Policy loss: -0.016564. Value loss: 0.017439. Entropy: 0.752182.\n",
      "Iteration 1845: Policy loss: -0.028485. Value loss: 0.014388. Entropy: 0.754033.\n",
      "episode: 1690   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 644     evaluation reward: 7.22\n",
      "episode: 1691   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 566     evaluation reward: 7.26\n",
      "episode: 1692   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 303     evaluation reward: 7.22\n",
      "Training network. lr: 0.000236. clip: 0.094465\n",
      "Iteration 1846: Policy loss: 0.018574. Value loss: 0.070435. Entropy: 0.703557.\n",
      "Iteration 1847: Policy loss: -0.003389. Value loss: 0.039457. Entropy: 0.693315.\n",
      "Iteration 1848: Policy loss: -0.019014. Value loss: 0.031165. Entropy: 0.689888.\n",
      "episode: 1693   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 588     evaluation reward: 7.27\n",
      "Training network. lr: 0.000236. clip: 0.094456\n",
      "Iteration 1849: Policy loss: 0.010669. Value loss: 0.041428. Entropy: 0.784609.\n",
      "Iteration 1850: Policy loss: -0.008526. Value loss: 0.031893. Entropy: 0.784766.\n",
      "Iteration 1851: Policy loss: -0.019706. Value loss: 0.027080. Entropy: 0.785195.\n",
      "episode: 1694   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 723     evaluation reward: 7.29\n",
      "episode: 1695   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 412     evaluation reward: 7.28\n",
      "Training network. lr: 0.000236. clip: 0.094447\n",
      "Iteration 1852: Policy loss: 0.012192. Value loss: 0.030298. Entropy: 0.808311.\n",
      "Iteration 1853: Policy loss: -0.005203. Value loss: 0.018424. Entropy: 0.817123.\n",
      "Iteration 1854: Policy loss: -0.016350. Value loss: 0.015176. Entropy: 0.796532.\n",
      "episode: 1696   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 569     evaluation reward: 7.32\n",
      "episode: 1697   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 516     evaluation reward: 7.33\n",
      "Training network. lr: 0.000236. clip: 0.094438\n",
      "Iteration 1855: Policy loss: 0.008623. Value loss: 0.048818. Entropy: 0.794410.\n",
      "Iteration 1856: Policy loss: -0.014965. Value loss: 0.036090. Entropy: 0.778929.\n",
      "Iteration 1857: Policy loss: -0.019466. Value loss: 0.029687. Entropy: 0.767336.\n",
      "episode: 1698   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 514     evaluation reward: 7.34\n",
      "episode: 1699   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 504     evaluation reward: 7.32\n",
      "Training network. lr: 0.000236. clip: 0.094429\n",
      "Iteration 1858: Policy loss: 0.004052. Value loss: 0.025655. Entropy: 0.779046.\n",
      "Iteration 1859: Policy loss: -0.011685. Value loss: 0.018629. Entropy: 0.769155.\n",
      "Iteration 1860: Policy loss: -0.025662. Value loss: 0.014293. Entropy: 0.769547.\n",
      "episode: 1700   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 611     evaluation reward: 7.34\n",
      "episode: 1701   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 371     evaluation reward: 7.3\n",
      "Training network. lr: 0.000236. clip: 0.094420\n",
      "Iteration 1861: Policy loss: 0.009627. Value loss: 0.036748. Entropy: 0.772979.\n",
      "Iteration 1862: Policy loss: -0.009611. Value loss: 0.025621. Entropy: 0.771676.\n",
      "Iteration 1863: Policy loss: -0.024484. Value loss: 0.021472. Entropy: 0.760346.\n",
      "episode: 1702   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 492     evaluation reward: 7.32\n",
      "episode: 1703   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 730     evaluation reward: 7.34\n",
      "Training network. lr: 0.000236. clip: 0.094411\n",
      "Iteration 1864: Policy loss: 0.014497. Value loss: 0.028768. Entropy: 0.951976.\n",
      "Iteration 1865: Policy loss: -0.014892. Value loss: 0.021435. Entropy: 0.950072.\n",
      "Iteration 1866: Policy loss: -0.025066. Value loss: 0.016418. Entropy: 0.941387.\n",
      "episode: 1704   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 528     evaluation reward: 7.33\n",
      "episode: 1705   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 490     evaluation reward: 7.3\n",
      "Training network. lr: 0.000236. clip: 0.094402\n",
      "Iteration 1867: Policy loss: 0.008595. Value loss: 0.023053. Entropy: 0.719311.\n",
      "Iteration 1868: Policy loss: -0.013219. Value loss: 0.016801. Entropy: 0.717304.\n",
      "Iteration 1869: Policy loss: -0.020141. Value loss: 0.014353. Entropy: 0.722947.\n",
      "episode: 1706   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 514     evaluation reward: 7.27\n",
      "episode: 1707   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 640     evaluation reward: 7.3\n",
      "Training network. lr: 0.000236. clip: 0.094393\n",
      "Iteration 1870: Policy loss: 0.009123. Value loss: 0.032931. Entropy: 0.819683.\n",
      "Iteration 1871: Policy loss: -0.010971. Value loss: 0.021979. Entropy: 0.808313.\n",
      "Iteration 1872: Policy loss: -0.023952. Value loss: 0.017861. Entropy: 0.811566.\n",
      "episode: 1708   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 512     evaluation reward: 7.3\n",
      "episode: 1709   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 333     evaluation reward: 7.26\n",
      "Training network. lr: 0.000236. clip: 0.094384\n",
      "Iteration 1873: Policy loss: 0.008386. Value loss: 0.050982. Entropy: 0.771879.\n",
      "Iteration 1874: Policy loss: -0.010406. Value loss: 0.041906. Entropy: 0.776697.\n",
      "Iteration 1875: Policy loss: -0.020751. Value loss: 0.035431. Entropy: 0.778545.\n",
      "episode: 1710   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 408     evaluation reward: 7.25\n",
      "episode: 1711   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 519     evaluation reward: 7.19\n",
      "Training network. lr: 0.000236. clip: 0.094375\n",
      "Iteration 1876: Policy loss: 0.015328. Value loss: 0.034515. Entropy: 0.788180.\n",
      "Iteration 1877: Policy loss: -0.007125. Value loss: 0.025012. Entropy: 0.797277.\n",
      "Iteration 1878: Policy loss: -0.008292. Value loss: 0.020060. Entropy: 0.799942.\n",
      "episode: 1712   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 459     evaluation reward: 7.16\n",
      "episode: 1713   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 568     evaluation reward: 7.17\n",
      "Training network. lr: 0.000236. clip: 0.094366\n",
      "Iteration 1879: Policy loss: 0.008441. Value loss: 0.022509. Entropy: 0.690229.\n",
      "Iteration 1880: Policy loss: -0.010817. Value loss: 0.016115. Entropy: 0.701606.\n",
      "Iteration 1881: Policy loss: -0.021382. Value loss: 0.013088. Entropy: 0.703327.\n",
      "episode: 1714   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 713     evaluation reward: 7.18\n",
      "episode: 1715   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 598     evaluation reward: 7.23\n",
      "Training network. lr: 0.000236. clip: 0.094357\n",
      "Iteration 1882: Policy loss: 0.013398. Value loss: 0.021249. Entropy: 0.845600.\n",
      "Iteration 1883: Policy loss: -0.010272. Value loss: 0.015339. Entropy: 0.848238.\n",
      "Iteration 1884: Policy loss: -0.027318. Value loss: 0.012359. Entropy: 0.860389.\n",
      "episode: 1716   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 544     evaluation reward: 7.18\n",
      "Training network. lr: 0.000236. clip: 0.094348\n",
      "Iteration 1885: Policy loss: 0.007977. Value loss: 0.023905. Entropy: 0.800891.\n",
      "Iteration 1886: Policy loss: -0.008057. Value loss: 0.016831. Entropy: 0.796073.\n",
      "Iteration 1887: Policy loss: -0.022687. Value loss: 0.013395. Entropy: 0.806072.\n",
      "episode: 1717   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 684     evaluation reward: 7.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1718   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 579     evaluation reward: 7.26\n",
      "Training network. lr: 0.000236. clip: 0.094339\n",
      "Iteration 1888: Policy loss: 0.008875. Value loss: 0.033358. Entropy: 0.878953.\n",
      "Iteration 1889: Policy loss: -0.008651. Value loss: 0.021222. Entropy: 0.864872.\n",
      "Iteration 1890: Policy loss: -0.012495. Value loss: 0.016944. Entropy: 0.860854.\n",
      "episode: 1719   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 455     evaluation reward: 7.27\n",
      "episode: 1720   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 566     evaluation reward: 7.25\n",
      "Training network. lr: 0.000236. clip: 0.094330\n",
      "Iteration 1891: Policy loss: 0.006976. Value loss: 0.048517. Entropy: 0.797968.\n",
      "Iteration 1892: Policy loss: -0.008255. Value loss: 0.035645. Entropy: 0.782167.\n",
      "Iteration 1893: Policy loss: -0.018857. Value loss: 0.029227. Entropy: 0.770229.\n",
      "episode: 1721   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 625     evaluation reward: 7.25\n",
      "episode: 1722   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 598     evaluation reward: 7.24\n",
      "Training network. lr: 0.000236. clip: 0.094321\n",
      "Iteration 1894: Policy loss: 0.005879. Value loss: 0.030194. Entropy: 0.835445.\n",
      "Iteration 1895: Policy loss: -0.013989. Value loss: 0.021152. Entropy: 0.837794.\n",
      "Iteration 1896: Policy loss: -0.025452. Value loss: 0.017426. Entropy: 0.821613.\n",
      "episode: 1723   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 494     evaluation reward: 7.24\n",
      "episode: 1724   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 480     evaluation reward: 7.24\n",
      "Training network. lr: 0.000236. clip: 0.094312\n",
      "Iteration 1897: Policy loss: 0.006471. Value loss: 0.037091. Entropy: 0.664988.\n",
      "Iteration 1898: Policy loss: -0.009823. Value loss: 0.028556. Entropy: 0.669204.\n",
      "Iteration 1899: Policy loss: -0.022485. Value loss: 0.021611. Entropy: 0.667064.\n",
      "episode: 1725   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 559     evaluation reward: 7.24\n",
      "episode: 1726   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 454     evaluation reward: 7.22\n",
      "Training network. lr: 0.000236. clip: 0.094303\n",
      "Iteration 1900: Policy loss: 0.007125. Value loss: 0.029515. Entropy: 0.830693.\n",
      "Iteration 1901: Policy loss: -0.008685. Value loss: 0.022586. Entropy: 0.834118.\n",
      "Iteration 1902: Policy loss: -0.022118. Value loss: 0.018621. Entropy: 0.821048.\n",
      "episode: 1727   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 352     evaluation reward: 7.17\n",
      "now time :  2018-12-26 13:03:02.521535\n",
      "episode: 1728   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 685     evaluation reward: 7.21\n",
      "Training network. lr: 0.000236. clip: 0.094294\n",
      "Iteration 1903: Policy loss: 0.004312. Value loss: 0.024595. Entropy: 0.751743.\n",
      "Iteration 1904: Policy loss: -0.008296. Value loss: 0.014694. Entropy: 0.756468.\n",
      "Iteration 1905: Policy loss: -0.020959. Value loss: 0.011985. Entropy: 0.760233.\n",
      "episode: 1729   score: 16.0   memory length: 1024   epsilon: 1.0    steps: 833     evaluation reward: 7.31\n",
      "Training network. lr: 0.000236. clip: 0.094285\n",
      "Iteration 1906: Policy loss: 0.006441. Value loss: 0.048136. Entropy: 0.797300.\n",
      "Iteration 1907: Policy loss: -0.007421. Value loss: 0.039014. Entropy: 0.800115.\n",
      "Iteration 1908: Policy loss: -0.016595. Value loss: 0.033280. Entropy: 0.792472.\n",
      "episode: 1730   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 590     evaluation reward: 7.32\n",
      "episode: 1731   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 667     evaluation reward: 7.35\n",
      "Training network. lr: 0.000236. clip: 0.094276\n",
      "Iteration 1909: Policy loss: 0.005510. Value loss: 0.030021. Entropy: 0.827730.\n",
      "Iteration 1910: Policy loss: -0.013754. Value loss: 0.020802. Entropy: 0.824211.\n",
      "Iteration 1911: Policy loss: -0.021624. Value loss: 0.016685. Entropy: 0.808134.\n",
      "episode: 1732   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 427     evaluation reward: 7.31\n",
      "episode: 1733   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 599     evaluation reward: 7.37\n",
      "Training network. lr: 0.000236. clip: 0.094267\n",
      "Iteration 1912: Policy loss: 0.010023. Value loss: 0.024930. Entropy: 0.806319.\n",
      "Iteration 1913: Policy loss: -0.009103. Value loss: 0.016944. Entropy: 0.804402.\n",
      "Iteration 1914: Policy loss: -0.022798. Value loss: 0.013602. Entropy: 0.807204.\n",
      "episode: 1734   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 576     evaluation reward: 7.38\n",
      "Training network. lr: 0.000236. clip: 0.094258\n",
      "Iteration 1915: Policy loss: 0.022176. Value loss: 0.046609. Entropy: 0.816290.\n",
      "Iteration 1916: Policy loss: 0.001900. Value loss: 0.030285. Entropy: 0.813038.\n",
      "Iteration 1917: Policy loss: -0.004786. Value loss: 0.025660. Entropy: 0.803666.\n",
      "episode: 1735   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 724     evaluation reward: 7.49\n",
      "episode: 1736   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 540     evaluation reward: 7.51\n",
      "Training network. lr: 0.000236. clip: 0.094249\n",
      "Iteration 1918: Policy loss: 0.006635. Value loss: 0.043266. Entropy: 0.796713.\n",
      "Iteration 1919: Policy loss: -0.010496. Value loss: 0.030128. Entropy: 0.782335.\n",
      "Iteration 1920: Policy loss: -0.024988. Value loss: 0.025981. Entropy: 0.776653.\n",
      "episode: 1737   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 398     evaluation reward: 7.45\n",
      "episode: 1738   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 677     evaluation reward: 7.44\n",
      "Training network. lr: 0.000236. clip: 0.094240\n",
      "Iteration 1921: Policy loss: 0.013509. Value loss: 0.036905. Entropy: 0.769195.\n",
      "Iteration 1922: Policy loss: -0.003205. Value loss: 0.027018. Entropy: 0.761684.\n",
      "Iteration 1923: Policy loss: -0.015056. Value loss: 0.020554. Entropy: 0.754492.\n",
      "episode: 1739   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 638     evaluation reward: 7.48\n",
      "episode: 1740   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 661     evaluation reward: 7.5\n",
      "Training network. lr: 0.000236. clip: 0.094231\n",
      "Iteration 1924: Policy loss: 0.014374. Value loss: 0.033504. Entropy: 0.899785.\n",
      "Iteration 1925: Policy loss: -0.011772. Value loss: 0.022539. Entropy: 0.903193.\n",
      "Iteration 1926: Policy loss: -0.024317. Value loss: 0.018501. Entropy: 0.901412.\n",
      "episode: 1741   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 605     evaluation reward: 7.52\n",
      "Training network. lr: 0.000236. clip: 0.094222\n",
      "Iteration 1927: Policy loss: 0.008074. Value loss: 0.025956. Entropy: 0.862306.\n",
      "Iteration 1928: Policy loss: -0.013057. Value loss: 0.019511. Entropy: 0.861162.\n",
      "Iteration 1929: Policy loss: -0.021522. Value loss: 0.016569. Entropy: 0.854308.\n",
      "episode: 1742   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 486     evaluation reward: 7.53\n",
      "episode: 1743   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 509     evaluation reward: 7.56\n",
      "Training network. lr: 0.000236. clip: 0.094213\n",
      "Iteration 1930: Policy loss: 0.009661. Value loss: 0.031303. Entropy: 0.789166.\n",
      "Iteration 1931: Policy loss: -0.006260. Value loss: 0.019362. Entropy: 0.764485.\n",
      "Iteration 1932: Policy loss: -0.021779. Value loss: 0.017797. Entropy: 0.777917.\n",
      "episode: 1744   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 585     evaluation reward: 7.61\n",
      "episode: 1745   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 567     evaluation reward: 7.64\n",
      "Training network. lr: 0.000236. clip: 0.094204\n",
      "Iteration 1933: Policy loss: 0.005767. Value loss: 0.024466. Entropy: 0.851619.\n",
      "Iteration 1934: Policy loss: -0.012386. Value loss: 0.017218. Entropy: 0.845137.\n",
      "Iteration 1935: Policy loss: -0.025219. Value loss: 0.016190. Entropy: 0.839046.\n",
      "episode: 1746   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 503     evaluation reward: 7.69\n",
      "episode: 1747   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 841     evaluation reward: 7.75\n",
      "Training network. lr: 0.000235. clip: 0.094195\n",
      "Iteration 1936: Policy loss: 0.010761. Value loss: 0.021183. Entropy: 0.903779.\n",
      "Iteration 1937: Policy loss: -0.011469. Value loss: 0.016333. Entropy: 0.906408.\n",
      "Iteration 1938: Policy loss: -0.021372. Value loss: 0.012493. Entropy: 0.910600.\n",
      "episode: 1748   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 348     evaluation reward: 7.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1749   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 514     evaluation reward: 7.71\n",
      "Training network. lr: 0.000235. clip: 0.094186\n",
      "Iteration 1939: Policy loss: 0.009349. Value loss: 0.040854. Entropy: 0.888805.\n",
      "Iteration 1940: Policy loss: -0.009078. Value loss: 0.027729. Entropy: 0.879776.\n",
      "Iteration 1941: Policy loss: -0.025562. Value loss: 0.022684. Entropy: 0.871891.\n",
      "episode: 1750   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 584     evaluation reward: 7.71\n",
      "episode: 1751   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 7.73\n",
      "Training network. lr: 0.000235. clip: 0.094177\n",
      "Iteration 1942: Policy loss: 0.009047. Value loss: 0.024982. Entropy: 0.815590.\n",
      "Iteration 1943: Policy loss: -0.011508. Value loss: 0.018117. Entropy: 0.833230.\n",
      "Iteration 1944: Policy loss: -0.024519. Value loss: 0.014095. Entropy: 0.824665.\n",
      "episode: 1752   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 388     evaluation reward: 7.73\n",
      "episode: 1753   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 624     evaluation reward: 7.69\n",
      "Training network. lr: 0.000235. clip: 0.094168\n",
      "Iteration 1945: Policy loss: 0.006051. Value loss: 0.028188. Entropy: 0.881481.\n",
      "Iteration 1946: Policy loss: -0.013899. Value loss: 0.020715. Entropy: 0.877276.\n",
      "Iteration 1947: Policy loss: -0.024266. Value loss: 0.018445. Entropy: 0.863744.\n",
      "episode: 1754   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 515     evaluation reward: 7.67\n",
      "episode: 1755   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 549     evaluation reward: 7.68\n",
      "Training network. lr: 0.000235. clip: 0.094159\n",
      "Iteration 1948: Policy loss: 0.010522. Value loss: 0.025123. Entropy: 0.817595.\n",
      "Iteration 1949: Policy loss: -0.007195. Value loss: 0.017308. Entropy: 0.815223.\n",
      "Iteration 1950: Policy loss: -0.025260. Value loss: 0.014219. Entropy: 0.816801.\n",
      "episode: 1756   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 719     evaluation reward: 7.73\n",
      "Training network. lr: 0.000235. clip: 0.094150\n",
      "Iteration 1951: Policy loss: 0.007363. Value loss: 0.043621. Entropy: 0.839674.\n",
      "Iteration 1952: Policy loss: -0.012429. Value loss: 0.032756. Entropy: 0.828877.\n",
      "Iteration 1953: Policy loss: -0.021431. Value loss: 0.027952. Entropy: 0.827654.\n",
      "episode: 1757   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 643     evaluation reward: 7.76\n",
      "episode: 1758   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 720     evaluation reward: 7.78\n",
      "Training network. lr: 0.000235. clip: 0.094141\n",
      "Iteration 1954: Policy loss: 0.004438. Value loss: 0.024259. Entropy: 0.792414.\n",
      "Iteration 1955: Policy loss: -0.014411. Value loss: 0.018150. Entropy: 0.795869.\n",
      "Iteration 1956: Policy loss: -0.026851. Value loss: 0.014806. Entropy: 0.791438.\n",
      "episode: 1759   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 463     evaluation reward: 7.78\n",
      "Training network. lr: 0.000235. clip: 0.094132\n",
      "Iteration 1957: Policy loss: 0.006593. Value loss: 0.027176. Entropy: 0.793923.\n",
      "Iteration 1958: Policy loss: -0.010773. Value loss: 0.018229. Entropy: 0.795219.\n",
      "Iteration 1959: Policy loss: -0.023128. Value loss: 0.014889. Entropy: 0.793272.\n",
      "episode: 1760   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 748     evaluation reward: 7.8\n",
      "episode: 1761   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 793     evaluation reward: 7.85\n",
      "Training network. lr: 0.000235. clip: 0.094123\n",
      "Iteration 1960: Policy loss: 0.009464. Value loss: 0.017378. Entropy: 0.850862.\n",
      "Iteration 1961: Policy loss: -0.011270. Value loss: 0.011221. Entropy: 0.836112.\n",
      "Iteration 1962: Policy loss: -0.025271. Value loss: 0.008751. Entropy: 0.836101.\n",
      "episode: 1762   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 445     evaluation reward: 7.84\n",
      "episode: 1763   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 696     evaluation reward: 7.85\n",
      "Training network. lr: 0.000235. clip: 0.094114\n",
      "Iteration 1963: Policy loss: 0.003213. Value loss: 0.023605. Entropy: 0.911541.\n",
      "Iteration 1964: Policy loss: -0.011312. Value loss: 0.014559. Entropy: 0.896579.\n",
      "Iteration 1965: Policy loss: -0.025445. Value loss: 0.010771. Entropy: 0.890678.\n",
      "episode: 1764   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 456     evaluation reward: 7.83\n",
      "Training network. lr: 0.000235. clip: 0.094105\n",
      "Iteration 1966: Policy loss: 0.010611. Value loss: 0.019891. Entropy: 0.884538.\n",
      "Iteration 1967: Policy loss: -0.010891. Value loss: 0.015417. Entropy: 0.877637.\n",
      "Iteration 1968: Policy loss: -0.023948. Value loss: 0.013264. Entropy: 0.871081.\n",
      "episode: 1765   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 626     evaluation reward: 7.84\n",
      "episode: 1766   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 314     evaluation reward: 7.76\n",
      "episode: 1767   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 571     evaluation reward: 7.76\n",
      "Training network. lr: 0.000235. clip: 0.094096\n",
      "Iteration 1969: Policy loss: 0.010055. Value loss: 0.032293. Entropy: 0.887473.\n",
      "Iteration 1970: Policy loss: -0.012704. Value loss: 0.022027. Entropy: 0.897626.\n",
      "Iteration 1971: Policy loss: -0.023108. Value loss: 0.017553. Entropy: 0.877503.\n",
      "episode: 1768   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 614     evaluation reward: 7.75\n",
      "episode: 1769   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 425     evaluation reward: 7.71\n",
      "Training network. lr: 0.000235. clip: 0.094087\n",
      "Iteration 1972: Policy loss: 0.010131. Value loss: 0.024445. Entropy: 0.783342.\n",
      "Iteration 1973: Policy loss: -0.009087. Value loss: 0.017665. Entropy: 0.782946.\n",
      "Iteration 1974: Policy loss: -0.019386. Value loss: 0.014823. Entropy: 0.773256.\n",
      "episode: 1770   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 523     evaluation reward: 7.66\n",
      "episode: 1771   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 444     evaluation reward: 7.67\n",
      "Training network. lr: 0.000235. clip: 0.094078\n",
      "Iteration 1975: Policy loss: 0.007635. Value loss: 0.021733. Entropy: 0.885589.\n",
      "Iteration 1976: Policy loss: -0.011480. Value loss: 0.015099. Entropy: 0.868584.\n",
      "Iteration 1977: Policy loss: -0.020001. Value loss: 0.012448. Entropy: 0.879629.\n",
      "episode: 1772   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 580     evaluation reward: 7.67\n",
      "episode: 1773   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 579     evaluation reward: 7.7\n",
      "Training network. lr: 0.000235. clip: 0.094069\n",
      "Iteration 1978: Policy loss: 0.009226. Value loss: 0.018013. Entropy: 0.707539.\n",
      "Iteration 1979: Policy loss: -0.006665. Value loss: 0.012472. Entropy: 0.700754.\n",
      "Iteration 1980: Policy loss: -0.021276. Value loss: 0.009654. Entropy: 0.709544.\n",
      "episode: 1774   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 788     evaluation reward: 7.72\n",
      "Training network. lr: 0.000235. clip: 0.094060\n",
      "Iteration 1981: Policy loss: 0.011709. Value loss: 0.059257. Entropy: 0.772907.\n",
      "Iteration 1982: Policy loss: -0.000668. Value loss: 0.042996. Entropy: 0.765270.\n",
      "Iteration 1983: Policy loss: -0.007741. Value loss: 0.037595. Entropy: 0.766590.\n",
      "episode: 1775   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 489     evaluation reward: 7.71\n",
      "episode: 1776   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 427     evaluation reward: 7.67\n",
      "Training network. lr: 0.000235. clip: 0.094051\n",
      "Iteration 1984: Policy loss: 0.007658. Value loss: 0.030924. Entropy: 0.841524.\n",
      "Iteration 1985: Policy loss: -0.011273. Value loss: 0.021669. Entropy: 0.825108.\n",
      "Iteration 1986: Policy loss: -0.022910. Value loss: 0.018267. Entropy: 0.828638.\n",
      "episode: 1777   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 427     evaluation reward: 7.64\n",
      "episode: 1778   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 533     evaluation reward: 7.65\n",
      "Training network. lr: 0.000235. clip: 0.094042\n",
      "Iteration 1987: Policy loss: 0.009523. Value loss: 0.029105. Entropy: 0.858590.\n",
      "Iteration 1988: Policy loss: -0.014419. Value loss: 0.020829. Entropy: 0.839239.\n",
      "Iteration 1989: Policy loss: -0.029257. Value loss: 0.016918. Entropy: 0.842282.\n",
      "episode: 1779   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 546     evaluation reward: 7.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1780   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 475     evaluation reward: 7.64\n",
      "Training network. lr: 0.000235. clip: 0.094033\n",
      "Iteration 1990: Policy loss: 0.009850. Value loss: 0.018082. Entropy: 0.801488.\n",
      "Iteration 1991: Policy loss: -0.012278. Value loss: 0.013577. Entropy: 0.798291.\n",
      "Iteration 1992: Policy loss: -0.023906. Value loss: 0.010870. Entropy: 0.787063.\n",
      "episode: 1781   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 587     evaluation reward: 7.65\n",
      "episode: 1782   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 543     evaluation reward: 7.7\n",
      "Training network. lr: 0.000235. clip: 0.094024\n",
      "Iteration 1993: Policy loss: 0.010935. Value loss: 0.036784. Entropy: 0.817348.\n",
      "Iteration 1994: Policy loss: -0.010976. Value loss: 0.024436. Entropy: 0.820655.\n",
      "Iteration 1995: Policy loss: -0.025461. Value loss: 0.018604. Entropy: 0.807301.\n",
      "episode: 1783   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 403     evaluation reward: 7.62\n",
      "episode: 1784   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 411     evaluation reward: 7.61\n",
      "Training network. lr: 0.000235. clip: 0.094015\n",
      "Iteration 1996: Policy loss: 0.007297. Value loss: 0.027808. Entropy: 0.906291.\n",
      "Iteration 1997: Policy loss: -0.013302. Value loss: 0.017572. Entropy: 0.898465.\n",
      "Iteration 1998: Policy loss: -0.031232. Value loss: 0.014336. Entropy: 0.891761.\n",
      "episode: 1785   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 687     evaluation reward: 7.63\n",
      "episode: 1786   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 655     evaluation reward: 7.66\n",
      "Training network. lr: 0.000235. clip: 0.094006\n",
      "Iteration 1999: Policy loss: 0.010806. Value loss: 0.022048. Entropy: 0.796508.\n",
      "Iteration 2000: Policy loss: -0.009117. Value loss: 0.016753. Entropy: 0.812946.\n",
      "Iteration 2001: Policy loss: -0.024569. Value loss: 0.013534. Entropy: 0.796516.\n",
      "episode: 1787   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 521     evaluation reward: 7.68\n",
      "episode: 1788   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 609     evaluation reward: 7.67\n",
      "Training network. lr: 0.000235. clip: 0.093997\n",
      "Iteration 2002: Policy loss: 0.004763. Value loss: 0.021044. Entropy: 0.787768.\n",
      "Iteration 2003: Policy loss: -0.013950. Value loss: 0.014710. Entropy: 0.785328.\n",
      "Iteration 2004: Policy loss: -0.024460. Value loss: 0.011278. Entropy: 0.776592.\n",
      "episode: 1789   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 373     evaluation reward: 7.63\n",
      "episode: 1790   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 730     evaluation reward: 7.63\n",
      "Training network. lr: 0.000235. clip: 0.093988\n",
      "Iteration 2005: Policy loss: 0.018472. Value loss: 0.028157. Entropy: 0.790325.\n",
      "Iteration 2006: Policy loss: -0.009389. Value loss: 0.019841. Entropy: 0.794977.\n",
      "Iteration 2007: Policy loss: -0.016823. Value loss: 0.013518. Entropy: 0.787559.\n",
      "episode: 1791   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 688     evaluation reward: 7.67\n",
      "Training network. lr: 0.000235. clip: 0.093979\n",
      "Iteration 2008: Policy loss: 0.005607. Value loss: 0.067488. Entropy: 0.820327.\n",
      "Iteration 2009: Policy loss: -0.007152. Value loss: 0.053750. Entropy: 0.828438.\n",
      "Iteration 2010: Policy loss: -0.015811. Value loss: 0.047280. Entropy: 0.832736.\n",
      "episode: 1792   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 434     evaluation reward: 7.69\n",
      "episode: 1793   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 452     evaluation reward: 7.63\n",
      "episode: 1794   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 497     evaluation reward: 7.59\n",
      "Training network. lr: 0.000235. clip: 0.093970\n",
      "Iteration 2011: Policy loss: 0.004003. Value loss: 0.028759. Entropy: 0.875870.\n",
      "Iteration 2012: Policy loss: -0.014341. Value loss: 0.022733. Entropy: 0.869973.\n",
      "Iteration 2013: Policy loss: -0.025875. Value loss: 0.018726. Entropy: 0.865675.\n",
      "episode: 1795   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 404     evaluation reward: 7.6\n",
      "episode: 1796   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 340     evaluation reward: 7.56\n",
      "Training network. lr: 0.000235. clip: 0.093961\n",
      "Iteration 2014: Policy loss: 0.011696. Value loss: 0.022067. Entropy: 0.752921.\n",
      "Iteration 2015: Policy loss: -0.009685. Value loss: 0.015851. Entropy: 0.752651.\n",
      "Iteration 2016: Policy loss: -0.020851. Value loss: 0.014864. Entropy: 0.745818.\n",
      "episode: 1797   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 616     evaluation reward: 7.54\n",
      "episode: 1798   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 408     evaluation reward: 7.52\n",
      "Training network. lr: 0.000235. clip: 0.093952\n",
      "Iteration 2017: Policy loss: 0.007749. Value loss: 0.025518. Entropy: 0.707937.\n",
      "Iteration 2018: Policy loss: -0.012050. Value loss: 0.017867. Entropy: 0.715344.\n",
      "Iteration 2019: Policy loss: -0.024560. Value loss: 0.013391. Entropy: 0.707662.\n",
      "episode: 1799   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 518     evaluation reward: 7.53\n",
      "episode: 1800   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 636     evaluation reward: 7.52\n",
      "Training network. lr: 0.000235. clip: 0.093943\n",
      "Iteration 2020: Policy loss: 0.009644. Value loss: 0.017130. Entropy: 0.733342.\n",
      "Iteration 2021: Policy loss: -0.011974. Value loss: 0.012806. Entropy: 0.730623.\n",
      "Iteration 2022: Policy loss: -0.023578. Value loss: 0.010646. Entropy: 0.723256.\n",
      "episode: 1801   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 549     evaluation reward: 7.54\n",
      "Training network. lr: 0.000235. clip: 0.093934\n",
      "Iteration 2023: Policy loss: 0.010876. Value loss: 0.021183. Entropy: 0.789225.\n",
      "Iteration 2024: Policy loss: -0.009374. Value loss: 0.015494. Entropy: 0.804609.\n",
      "Iteration 2025: Policy loss: -0.015126. Value loss: 0.012099. Entropy: 0.797342.\n",
      "episode: 1802   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 670     evaluation reward: 7.57\n",
      "episode: 1803   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 311     evaluation reward: 7.51\n",
      "episode: 1804   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 508     evaluation reward: 7.5\n",
      "Training network. lr: 0.000235. clip: 0.093925\n",
      "Iteration 2026: Policy loss: 0.007271. Value loss: 0.017833. Entropy: 0.755569.\n",
      "Iteration 2027: Policy loss: -0.014337. Value loss: 0.012783. Entropy: 0.749402.\n",
      "Iteration 2028: Policy loss: -0.024218. Value loss: 0.010802. Entropy: 0.748567.\n",
      "episode: 1805   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 583     evaluation reward: 7.54\n",
      "episode: 1806   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 553     evaluation reward: 7.56\n",
      "Training network. lr: 0.000235. clip: 0.093916\n",
      "Iteration 2029: Policy loss: 0.020825. Value loss: 0.051348. Entropy: 0.778578.\n",
      "Iteration 2030: Policy loss: 0.004355. Value loss: 0.038120. Entropy: 0.759905.\n",
      "Iteration 2031: Policy loss: -0.004953. Value loss: 0.031775. Entropy: 0.780862.\n",
      "episode: 1807   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 467     evaluation reward: 7.54\n",
      "episode: 1808   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 555     evaluation reward: 7.51\n",
      "Training network. lr: 0.000235. clip: 0.093907\n",
      "Iteration 2032: Policy loss: 0.007193. Value loss: 0.016588. Entropy: 0.771707.\n",
      "Iteration 2033: Policy loss: -0.011041. Value loss: 0.013027. Entropy: 0.774940.\n",
      "Iteration 2034: Policy loss: -0.025232. Value loss: 0.009461. Entropy: 0.752152.\n",
      "episode: 1809   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 708     evaluation reward: 7.58\n",
      "Training network. lr: 0.000235. clip: 0.093898\n",
      "Iteration 2035: Policy loss: 0.011488. Value loss: 0.017209. Entropy: 0.776801.\n",
      "Iteration 2036: Policy loss: -0.008446. Value loss: 0.011579. Entropy: 0.765236.\n",
      "Iteration 2037: Policy loss: -0.018567. Value loss: 0.010143. Entropy: 0.771823.\n",
      "episode: 1810   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 561     evaluation reward: 7.63\n",
      "episode: 1811   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 415     evaluation reward: 7.62\n",
      "Training network. lr: 0.000235. clip: 0.093889\n",
      "Iteration 2038: Policy loss: 0.008306. Value loss: 0.051395. Entropy: 0.792084.\n",
      "Iteration 2039: Policy loss: -0.013462. Value loss: 0.037910. Entropy: 0.771021.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2040: Policy loss: -0.022740. Value loss: 0.032369. Entropy: 0.774762.\n",
      "episode: 1812   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 646     evaluation reward: 7.65\n",
      "episode: 1813   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 406     evaluation reward: 7.62\n",
      "Training network. lr: 0.000235. clip: 0.093880\n",
      "Iteration 2041: Policy loss: 0.012261. Value loss: 0.020052. Entropy: 0.801726.\n",
      "Iteration 2042: Policy loss: -0.009315. Value loss: 0.013668. Entropy: 0.784484.\n",
      "Iteration 2043: Policy loss: -0.026737. Value loss: 0.011078. Entropy: 0.790429.\n",
      "episode: 1814   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 733     evaluation reward: 7.62\n",
      "episode: 1815   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 590     evaluation reward: 7.61\n",
      "Training network. lr: 0.000235. clip: 0.093871\n",
      "Iteration 2044: Policy loss: 0.012081. Value loss: 0.021785. Entropy: 0.779667.\n",
      "Iteration 2045: Policy loss: -0.008470. Value loss: 0.015447. Entropy: 0.778127.\n",
      "Iteration 2046: Policy loss: -0.020970. Value loss: 0.012001. Entropy: 0.770053.\n",
      "episode: 1816   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 632     evaluation reward: 7.65\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2047: Policy loss: 0.005305. Value loss: 0.018191. Entropy: 0.773469.\n",
      "Iteration 2048: Policy loss: -0.012086. Value loss: 0.014358. Entropy: 0.767811.\n",
      "Iteration 2049: Policy loss: -0.023090. Value loss: 0.010685. Entropy: 0.751525.\n",
      "episode: 1817   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 574     evaluation reward: 7.62\n",
      "episode: 1818   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 455     evaluation reward: 7.61\n",
      "now time :  2018-12-26 13:07:25.347715\n",
      "Training network. lr: 0.000235. clip: 0.093853\n",
      "Iteration 2050: Policy loss: 0.005353. Value loss: 0.026849. Entropy: 0.792768.\n",
      "Iteration 2051: Policy loss: -0.009850. Value loss: 0.021041. Entropy: 0.792958.\n",
      "Iteration 2052: Policy loss: -0.023400. Value loss: 0.017410. Entropy: 0.787464.\n",
      "episode: 1819   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 631     evaluation reward: 7.65\n",
      "episode: 1820   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 681     evaluation reward: 7.64\n",
      "Training network. lr: 0.000235. clip: 0.093844\n",
      "Iteration 2053: Policy loss: 0.007453. Value loss: 0.027941. Entropy: 0.807982.\n",
      "Iteration 2054: Policy loss: -0.009494. Value loss: 0.018071. Entropy: 0.812902.\n",
      "Iteration 2055: Policy loss: -0.021947. Value loss: 0.014424. Entropy: 0.802881.\n",
      "episode: 1821   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 357     evaluation reward: 7.59\n",
      "episode: 1822   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 276     evaluation reward: 7.53\n",
      "Training network. lr: 0.000235. clip: 0.093835\n",
      "Iteration 2056: Policy loss: 0.008395. Value loss: 0.030842. Entropy: 0.777457.\n",
      "Iteration 2057: Policy loss: -0.015674. Value loss: 0.021555. Entropy: 0.786349.\n",
      "Iteration 2058: Policy loss: -0.023534. Value loss: 0.016143. Entropy: 0.779015.\n",
      "episode: 1823   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 740     evaluation reward: 7.58\n",
      "episode: 1824   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 576     evaluation reward: 7.6\n",
      "Training network. lr: 0.000235. clip: 0.093826\n",
      "Iteration 2059: Policy loss: 0.010299. Value loss: 0.022423. Entropy: 0.742006.\n",
      "Iteration 2060: Policy loss: -0.013444. Value loss: 0.013894. Entropy: 0.733036.\n",
      "Iteration 2061: Policy loss: -0.023961. Value loss: 0.011191. Entropy: 0.733451.\n",
      "episode: 1825   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 567     evaluation reward: 7.65\n",
      "episode: 1826   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 600     evaluation reward: 7.69\n",
      "Training network. lr: 0.000235. clip: 0.093817\n",
      "Iteration 2062: Policy loss: 0.008450. Value loss: 0.041069. Entropy: 0.756890.\n",
      "Iteration 2063: Policy loss: -0.009141. Value loss: 0.032628. Entropy: 0.748247.\n",
      "Iteration 2064: Policy loss: -0.017823. Value loss: 0.027099. Entropy: 0.750483.\n",
      "episode: 1827   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 619     evaluation reward: 7.73\n",
      "episode: 1828   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 383     evaluation reward: 7.67\n",
      "Training network. lr: 0.000235. clip: 0.093808\n",
      "Iteration 2065: Policy loss: 0.008268. Value loss: 0.019980. Entropy: 0.761761.\n",
      "Iteration 2066: Policy loss: -0.012186. Value loss: 0.014966. Entropy: 0.758837.\n",
      "Iteration 2067: Policy loss: -0.025809. Value loss: 0.012298. Entropy: 0.760796.\n",
      "episode: 1829   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 644     evaluation reward: 7.61\n",
      "episode: 1830   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 413     evaluation reward: 7.58\n",
      "Training network. lr: 0.000234. clip: 0.093799\n",
      "Iteration 2068: Policy loss: 0.008115. Value loss: 0.029864. Entropy: 0.820841.\n",
      "Iteration 2069: Policy loss: -0.012590. Value loss: 0.020359. Entropy: 0.812706.\n",
      "Iteration 2070: Policy loss: -0.026165. Value loss: 0.017605. Entropy: 0.803784.\n",
      "episode: 1831   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 392     evaluation reward: 7.57\n",
      "episode: 1832   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 625     evaluation reward: 7.6\n",
      "Training network. lr: 0.000234. clip: 0.093790\n",
      "Iteration 2071: Policy loss: 0.004258. Value loss: 0.035979. Entropy: 0.694951.\n",
      "Iteration 2072: Policy loss: -0.008829. Value loss: 0.028048. Entropy: 0.704317.\n",
      "Iteration 2073: Policy loss: -0.017745. Value loss: 0.020464. Entropy: 0.698284.\n",
      "episode: 1833   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 600     evaluation reward: 7.59\n",
      "episode: 1834   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 510     evaluation reward: 7.59\n",
      "Training network. lr: 0.000234. clip: 0.093781\n",
      "Iteration 2074: Policy loss: 0.006943. Value loss: 0.030706. Entropy: 0.786721.\n",
      "Iteration 2075: Policy loss: -0.016009. Value loss: 0.021120. Entropy: 0.788580.\n",
      "Iteration 2076: Policy loss: -0.022584. Value loss: 0.016743. Entropy: 0.801491.\n",
      "episode: 1835   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 383     evaluation reward: 7.51\n",
      "episode: 1836   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 507     evaluation reward: 7.47\n",
      "Training network. lr: 0.000234. clip: 0.093772\n",
      "Iteration 2077: Policy loss: 0.018366. Value loss: 0.020181. Entropy: 0.682209.\n",
      "Iteration 2078: Policy loss: -0.006551. Value loss: 0.013252. Entropy: 0.679445.\n",
      "Iteration 2079: Policy loss: -0.019028. Value loss: 0.010575. Entropy: 0.666483.\n",
      "episode: 1837   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 822     evaluation reward: 7.51\n",
      "Training network. lr: 0.000234. clip: 0.093763\n",
      "Iteration 2080: Policy loss: 0.012518. Value loss: 0.026610. Entropy: 0.774141.\n",
      "Iteration 2081: Policy loss: -0.012403. Value loss: 0.019235. Entropy: 0.769563.\n",
      "Iteration 2082: Policy loss: -0.022193. Value loss: 0.015137. Entropy: 0.765387.\n",
      "episode: 1838   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 755     evaluation reward: 7.54\n",
      "episode: 1839   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 483     evaluation reward: 7.52\n",
      "Training network. lr: 0.000234. clip: 0.093754\n",
      "Iteration 2083: Policy loss: 0.009167. Value loss: 0.024297. Entropy: 0.772198.\n",
      "Iteration 2084: Policy loss: -0.012168. Value loss: 0.018118. Entropy: 0.759215.\n",
      "Iteration 2085: Policy loss: -0.016920. Value loss: 0.015407. Entropy: 0.751617.\n",
      "episode: 1840   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 654     evaluation reward: 7.52\n",
      "Training network. lr: 0.000234. clip: 0.093745\n",
      "Iteration 2086: Policy loss: 0.012676. Value loss: 0.023350. Entropy: 0.766535.\n",
      "Iteration 2087: Policy loss: -0.012565. Value loss: 0.016364. Entropy: 0.775561.\n",
      "Iteration 2088: Policy loss: -0.024648. Value loss: 0.013309. Entropy: 0.767531.\n",
      "episode: 1841   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 679     evaluation reward: 7.53\n",
      "episode: 1842   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 545     evaluation reward: 7.55\n",
      "Training network. lr: 0.000234. clip: 0.093736\n",
      "Iteration 2089: Policy loss: 0.006472. Value loss: 0.019654. Entropy: 0.754154.\n",
      "Iteration 2090: Policy loss: -0.015687. Value loss: 0.015038. Entropy: 0.750093.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2091: Policy loss: -0.026618. Value loss: 0.012769. Entropy: 0.756992.\n",
      "episode: 1843   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 7.55\n",
      "episode: 1844   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 610     evaluation reward: 7.54\n",
      "Training network. lr: 0.000234. clip: 0.093727\n",
      "Iteration 2092: Policy loss: 0.013287. Value loss: 0.020210. Entropy: 0.785651.\n",
      "Iteration 2093: Policy loss: -0.008662. Value loss: 0.015152. Entropy: 0.783859.\n",
      "Iteration 2094: Policy loss: -0.020992. Value loss: 0.012561. Entropy: 0.786359.\n",
      "episode: 1845   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 738     evaluation reward: 7.55\n",
      "episode: 1846   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 302     evaluation reward: 7.5\n",
      "Training network. lr: 0.000234. clip: 0.093718\n",
      "Iteration 2095: Policy loss: 0.006648. Value loss: 0.028025. Entropy: 0.746876.\n",
      "Iteration 2096: Policy loss: -0.015433. Value loss: 0.018635. Entropy: 0.749454.\n",
      "Iteration 2097: Policy loss: -0.021757. Value loss: 0.015160. Entropy: 0.756108.\n",
      "episode: 1847   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 571     evaluation reward: 7.46\n",
      "episode: 1848   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 533     evaluation reward: 7.49\n",
      "Training network. lr: 0.000234. clip: 0.093709\n",
      "Iteration 2098: Policy loss: 0.011744. Value loss: 0.028687. Entropy: 0.740012.\n",
      "Iteration 2099: Policy loss: -0.010413. Value loss: 0.019310. Entropy: 0.745874.\n",
      "Iteration 2100: Policy loss: -0.020452. Value loss: 0.015913. Entropy: 0.748972.\n",
      "episode: 1849   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 652     evaluation reward: 7.52\n",
      "Training network. lr: 0.000234. clip: 0.093700\n",
      "Iteration 2101: Policy loss: 0.009645. Value loss: 0.026898. Entropy: 0.782665.\n",
      "Iteration 2102: Policy loss: -0.013483. Value loss: 0.019350. Entropy: 0.776944.\n",
      "Iteration 2103: Policy loss: -0.026498. Value loss: 0.017452. Entropy: 0.764358.\n",
      "episode: 1850   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 651     evaluation reward: 7.52\n",
      "episode: 1851   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 545     evaluation reward: 7.53\n",
      "Training network. lr: 0.000234. clip: 0.093691\n",
      "Iteration 2104: Policy loss: 0.009300. Value loss: 0.024288. Entropy: 0.728520.\n",
      "Iteration 2105: Policy loss: -0.012423. Value loss: 0.016050. Entropy: 0.725254.\n",
      "Iteration 2106: Policy loss: -0.023253. Value loss: 0.013540. Entropy: 0.729174.\n",
      "episode: 1852   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 589     evaluation reward: 7.57\n",
      "episode: 1853   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 523     evaluation reward: 7.56\n",
      "Training network. lr: 0.000234. clip: 0.093682\n",
      "Iteration 2107: Policy loss: 0.005742. Value loss: 0.025884. Entropy: 0.722028.\n",
      "Iteration 2108: Policy loss: -0.012910. Value loss: 0.018144. Entropy: 0.722274.\n",
      "Iteration 2109: Policy loss: -0.023103. Value loss: 0.014689. Entropy: 0.719052.\n",
      "episode: 1854   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 490     evaluation reward: 7.55\n",
      "episode: 1855   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 656     evaluation reward: 7.57\n",
      "Training network. lr: 0.000234. clip: 0.093673\n",
      "Iteration 2110: Policy loss: 0.012839. Value loss: 0.024456. Entropy: 0.705664.\n",
      "Iteration 2111: Policy loss: -0.008356. Value loss: 0.019323. Entropy: 0.711668.\n",
      "Iteration 2112: Policy loss: -0.022954. Value loss: 0.015339. Entropy: 0.711122.\n",
      "episode: 1856   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 374     evaluation reward: 7.5\n",
      "episode: 1857   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 638     evaluation reward: 7.49\n",
      "Training network. lr: 0.000234. clip: 0.093664\n",
      "Iteration 2113: Policy loss: 0.010512. Value loss: 0.023820. Entropy: 0.789821.\n",
      "Iteration 2114: Policy loss: -0.009508. Value loss: 0.016075. Entropy: 0.784589.\n",
      "Iteration 2115: Policy loss: -0.024957. Value loss: 0.013240. Entropy: 0.786048.\n",
      "episode: 1858   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 283     evaluation reward: 7.42\n",
      "episode: 1859   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 715     evaluation reward: 7.51\n",
      "Training network. lr: 0.000234. clip: 0.093655\n",
      "Iteration 2116: Policy loss: 0.013105. Value loss: 0.053532. Entropy: 0.781045.\n",
      "Iteration 2117: Policy loss: -0.008763. Value loss: 0.035311. Entropy: 0.775025.\n",
      "Iteration 2118: Policy loss: -0.018734. Value loss: 0.029142. Entropy: 0.781761.\n",
      "episode: 1860   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 673     evaluation reward: 7.5\n",
      "episode: 1861   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 502     evaluation reward: 7.46\n",
      "Training network. lr: 0.000234. clip: 0.093646\n",
      "Iteration 2119: Policy loss: 0.009322. Value loss: 0.030884. Entropy: 0.803859.\n",
      "Iteration 2120: Policy loss: -0.009234. Value loss: 0.019059. Entropy: 0.807403.\n",
      "Iteration 2121: Policy loss: -0.020152. Value loss: 0.014338. Entropy: 0.799892.\n",
      "episode: 1862   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 432     evaluation reward: 7.46\n",
      "episode: 1863   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 575     evaluation reward: 7.43\n",
      "Training network. lr: 0.000234. clip: 0.093637\n",
      "Iteration 2122: Policy loss: 0.006923. Value loss: 0.027314. Entropy: 0.698986.\n",
      "Iteration 2123: Policy loss: -0.012449. Value loss: 0.018755. Entropy: 0.691517.\n",
      "Iteration 2124: Policy loss: -0.021731. Value loss: 0.015515. Entropy: 0.704205.\n",
      "episode: 1864   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 626     evaluation reward: 7.45\n",
      "Training network. lr: 0.000234. clip: 0.093628\n",
      "Iteration 2125: Policy loss: 0.008430. Value loss: 0.018677. Entropy: 0.744643.\n",
      "Iteration 2126: Policy loss: -0.012559. Value loss: 0.014409. Entropy: 0.738853.\n",
      "Iteration 2127: Policy loss: -0.023211. Value loss: 0.011906. Entropy: 0.753828.\n",
      "episode: 1865   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 598     evaluation reward: 7.43\n",
      "episode: 1866   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 607     evaluation reward: 7.49\n",
      "Training network. lr: 0.000234. clip: 0.093619\n",
      "Iteration 2128: Policy loss: 0.005724. Value loss: 0.017410. Entropy: 0.761336.\n",
      "Iteration 2129: Policy loss: -0.009895. Value loss: 0.012371. Entropy: 0.765345.\n",
      "Iteration 2130: Policy loss: -0.020708. Value loss: 0.010605. Entropy: 0.765659.\n",
      "episode: 1867   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 402     evaluation reward: 7.46\n",
      "episode: 1868   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 317     evaluation reward: 7.41\n",
      "episode: 1869   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 310     evaluation reward: 7.38\n",
      "Training network. lr: 0.000234. clip: 0.093610\n",
      "Iteration 2131: Policy loss: 0.004977. Value loss: 0.028867. Entropy: 0.851824.\n",
      "Iteration 2132: Policy loss: -0.014809. Value loss: 0.020150. Entropy: 0.857268.\n",
      "Iteration 2133: Policy loss: -0.020092. Value loss: 0.018030. Entropy: 0.844155.\n",
      "episode: 1870   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 344     evaluation reward: 7.36\n",
      "episode: 1871   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 421     evaluation reward: 7.36\n",
      "episode: 1872   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 314     evaluation reward: 7.31\n",
      "Training network. lr: 0.000234. clip: 0.093601\n",
      "Iteration 2134: Policy loss: 0.007820. Value loss: 0.019634. Entropy: 0.756661.\n",
      "Iteration 2135: Policy loss: -0.012301. Value loss: 0.014022. Entropy: 0.758545.\n",
      "Iteration 2136: Policy loss: -0.020224. Value loss: 0.012030. Entropy: 0.768142.\n",
      "episode: 1873   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 470     evaluation reward: 7.3\n",
      "episode: 1874   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 351     evaluation reward: 7.2\n",
      "Training network. lr: 0.000234. clip: 0.093592\n",
      "Iteration 2137: Policy loss: 0.013835. Value loss: 0.021467. Entropy: 0.785922.\n",
      "Iteration 2138: Policy loss: -0.012739. Value loss: 0.014231. Entropy: 0.788495.\n",
      "Iteration 2139: Policy loss: -0.023282. Value loss: 0.011372. Entropy: 0.786942.\n",
      "episode: 1875   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 687     evaluation reward: 7.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1876   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 548     evaluation reward: 7.26\n",
      "Training network. lr: 0.000234. clip: 0.093583\n",
      "Iteration 2140: Policy loss: 0.010937. Value loss: 0.019927. Entropy: 0.743115.\n",
      "Iteration 2141: Policy loss: -0.003272. Value loss: 0.014034. Entropy: 0.733597.\n",
      "Iteration 2142: Policy loss: -0.020068. Value loss: 0.011542. Entropy: 0.737342.\n",
      "episode: 1877   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 553     evaluation reward: 7.28\n",
      "episode: 1878   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 589     evaluation reward: 7.28\n",
      "Training network. lr: 0.000234. clip: 0.093574\n",
      "Iteration 2143: Policy loss: 0.011945. Value loss: 0.022159. Entropy: 0.806346.\n",
      "Iteration 2144: Policy loss: -0.007336. Value loss: 0.016045. Entropy: 0.802415.\n",
      "Iteration 2145: Policy loss: -0.020903. Value loss: 0.012852. Entropy: 0.797014.\n",
      "episode: 1879   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 388     evaluation reward: 7.26\n",
      "episode: 1880   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 341     evaluation reward: 7.25\n",
      "Training network. lr: 0.000234. clip: 0.093565\n",
      "Iteration 2146: Policy loss: 0.005527. Value loss: 0.026605. Entropy: 0.800700.\n",
      "Iteration 2147: Policy loss: -0.013416. Value loss: 0.020047. Entropy: 0.800849.\n",
      "Iteration 2148: Policy loss: -0.023916. Value loss: 0.016183. Entropy: 0.800893.\n",
      "episode: 1881   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 666     evaluation reward: 7.26\n",
      "episode: 1882   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 351     evaluation reward: 7.22\n",
      "Training network. lr: 0.000234. clip: 0.093556\n",
      "Iteration 2149: Policy loss: 0.007766. Value loss: 0.021813. Entropy: 0.736771.\n",
      "Iteration 2150: Policy loss: -0.010008. Value loss: 0.015876. Entropy: 0.745335.\n",
      "Iteration 2151: Policy loss: -0.021333. Value loss: 0.013222. Entropy: 0.741763.\n",
      "episode: 1883   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 7.25\n",
      "episode: 1884   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 654     evaluation reward: 7.34\n",
      "Training network. lr: 0.000234. clip: 0.093547\n",
      "Iteration 2152: Policy loss: 0.008116. Value loss: 0.051486. Entropy: 0.825942.\n",
      "Iteration 2153: Policy loss: -0.011940. Value loss: 0.039840. Entropy: 0.812224.\n",
      "Iteration 2154: Policy loss: -0.021271. Value loss: 0.032504. Entropy: 0.806391.\n",
      "episode: 1885   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 442     evaluation reward: 7.29\n",
      "episode: 1886   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 250     evaluation reward: 7.22\n",
      "episode: 1887   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 307     evaluation reward: 7.18\n",
      "Training network. lr: 0.000234. clip: 0.093538\n",
      "Iteration 2155: Policy loss: 0.011188. Value loss: 0.025594. Entropy: 0.802774.\n",
      "Iteration 2156: Policy loss: -0.014095. Value loss: 0.018410. Entropy: 0.790594.\n",
      "Iteration 2157: Policy loss: -0.026019. Value loss: 0.015518. Entropy: 0.778870.\n",
      "episode: 1888   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 458     evaluation reward: 7.16\n",
      "episode: 1889   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 515     evaluation reward: 7.18\n",
      "Training network. lr: 0.000234. clip: 0.093529\n",
      "Iteration 2158: Policy loss: 0.010282. Value loss: 0.021920. Entropy: 0.763222.\n",
      "Iteration 2159: Policy loss: -0.011645. Value loss: 0.017350. Entropy: 0.758182.\n",
      "Iteration 2160: Policy loss: -0.024141. Value loss: 0.014928. Entropy: 0.754319.\n",
      "episode: 1890   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 567     evaluation reward: 7.16\n",
      "episode: 1891   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 752     evaluation reward: 7.11\n",
      "Training network. lr: 0.000234. clip: 0.093520\n",
      "Iteration 2161: Policy loss: 0.009715. Value loss: 0.021459. Entropy: 0.776628.\n",
      "Iteration 2162: Policy loss: -0.011734. Value loss: 0.016490. Entropy: 0.784317.\n",
      "Iteration 2163: Policy loss: -0.028378. Value loss: 0.014355. Entropy: 0.772244.\n",
      "episode: 1892   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 510     evaluation reward: 7.12\n",
      "episode: 1893   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 560     evaluation reward: 7.15\n",
      "Training network. lr: 0.000234. clip: 0.093511\n",
      "Iteration 2164: Policy loss: 0.010903. Value loss: 0.018292. Entropy: 0.744337.\n",
      "Iteration 2165: Policy loss: -0.010344. Value loss: 0.012367. Entropy: 0.743893.\n",
      "Iteration 2166: Policy loss: -0.023219. Value loss: 0.010744. Entropy: 0.750523.\n",
      "episode: 1894   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 349     evaluation reward: 7.12\n",
      "episode: 1895   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 514     evaluation reward: 7.14\n",
      "Training network. lr: 0.000234. clip: 0.093502\n",
      "Iteration 2167: Policy loss: 0.004810. Value loss: 0.018773. Entropy: 0.681092.\n",
      "Iteration 2168: Policy loss: -0.012084. Value loss: 0.015827. Entropy: 0.692941.\n",
      "Iteration 2169: Policy loss: -0.026992. Value loss: 0.012281. Entropy: 0.684784.\n",
      "episode: 1896   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 440     evaluation reward: 7.16\n",
      "episode: 1897   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 386     evaluation reward: 7.12\n",
      "Training network. lr: 0.000234. clip: 0.093493\n",
      "Iteration 2170: Policy loss: 0.008104. Value loss: 0.023901. Entropy: 0.728150.\n",
      "Iteration 2171: Policy loss: -0.007918. Value loss: 0.016750. Entropy: 0.725025.\n",
      "Iteration 2172: Policy loss: -0.020611. Value loss: 0.013227. Entropy: 0.718635.\n",
      "episode: 1898   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 571     evaluation reward: 7.15\n",
      "episode: 1899   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 534     evaluation reward: 7.14\n",
      "Training network. lr: 0.000234. clip: 0.093484\n",
      "Iteration 2173: Policy loss: 0.011832. Value loss: 0.016749. Entropy: 0.739118.\n",
      "Iteration 2174: Policy loss: -0.010185. Value loss: 0.013396. Entropy: 0.737977.\n",
      "Iteration 2175: Policy loss: -0.021316. Value loss: 0.011374. Entropy: 0.733083.\n",
      "episode: 1900   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 588     evaluation reward: 7.12\n",
      "episode: 1901   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 716     evaluation reward: 7.16\n",
      "Training network. lr: 0.000234. clip: 0.093475\n",
      "Iteration 2176: Policy loss: 0.010313. Value loss: 0.019779. Entropy: 0.813956.\n",
      "Iteration 2177: Policy loss: -0.007133. Value loss: 0.014841. Entropy: 0.796497.\n",
      "Iteration 2178: Policy loss: -0.023951. Value loss: 0.012244. Entropy: 0.796602.\n",
      "episode: 1902   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 277     evaluation reward: 7.09\n",
      "episode: 1903   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 456     evaluation reward: 7.12\n",
      "Training network. lr: 0.000234. clip: 0.093466\n",
      "Iteration 2179: Policy loss: 0.008312. Value loss: 0.020668. Entropy: 0.772139.\n",
      "Iteration 2180: Policy loss: -0.010772. Value loss: 0.014157. Entropy: 0.772411.\n",
      "Iteration 2181: Policy loss: -0.022141. Value loss: 0.011384. Entropy: 0.769796.\n",
      "episode: 1904   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 549     evaluation reward: 7.17\n",
      "episode: 1905   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 566     evaluation reward: 7.15\n",
      "Training network. lr: 0.000234. clip: 0.093457\n",
      "Iteration 2182: Policy loss: 0.008524. Value loss: 0.039860. Entropy: 0.693701.\n",
      "Iteration 2183: Policy loss: -0.009464. Value loss: 0.031105. Entropy: 0.681056.\n",
      "Iteration 2184: Policy loss: -0.019156. Value loss: 0.025712. Entropy: 0.688412.\n",
      "episode: 1906   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 518     evaluation reward: 7.16\n",
      "episode: 1907   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 506     evaluation reward: 7.16\n",
      "Training network. lr: 0.000234. clip: 0.093448\n",
      "Iteration 2185: Policy loss: 0.007432. Value loss: 0.044386. Entropy: 0.849734.\n",
      "Iteration 2186: Policy loss: -0.010657. Value loss: 0.035443. Entropy: 0.863668.\n",
      "Iteration 2187: Policy loss: -0.018732. Value loss: 0.030954. Entropy: 0.863203.\n",
      "episode: 1908   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 407     evaluation reward: 7.14\n",
      "episode: 1909   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 681     evaluation reward: 7.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000234. clip: 0.093439\n",
      "Iteration 2188: Policy loss: 0.010616. Value loss: 0.021865. Entropy: 0.703080.\n",
      "Iteration 2189: Policy loss: -0.008344. Value loss: 0.014095. Entropy: 0.711887.\n",
      "Iteration 2190: Policy loss: -0.021293. Value loss: 0.012404. Entropy: 0.711891.\n",
      "episode: 1910   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 783     evaluation reward: 7.16\n",
      "Training network. lr: 0.000234. clip: 0.093430\n",
      "Iteration 2191: Policy loss: 0.007198. Value loss: 0.045847. Entropy: 0.791842.\n",
      "Iteration 2192: Policy loss: -0.003750. Value loss: 0.028997. Entropy: 0.792632.\n",
      "Iteration 2193: Policy loss: -0.016864. Value loss: 0.025281. Entropy: 0.790632.\n",
      "episode: 1911   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 527     evaluation reward: 7.2\n",
      "episode: 1912   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 630     evaluation reward: 7.21\n",
      "Training network. lr: 0.000234. clip: 0.093421\n",
      "Iteration 2194: Policy loss: 0.013827. Value loss: 0.056958. Entropy: 0.751924.\n",
      "Iteration 2195: Policy loss: -0.006938. Value loss: 0.043574. Entropy: 0.751653.\n",
      "Iteration 2196: Policy loss: -0.016915. Value loss: 0.034591. Entropy: 0.751084.\n",
      "episode: 1913   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 583     evaluation reward: 7.24\n",
      "now time :  2018-12-26 13:11:49.634801\n",
      "episode: 1914   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 399     evaluation reward: 7.18\n",
      "Training network. lr: 0.000234. clip: 0.093412\n",
      "Iteration 2197: Policy loss: 0.009302. Value loss: 0.030277. Entropy: 0.811664.\n",
      "Iteration 2198: Policy loss: -0.010662. Value loss: 0.024149. Entropy: 0.806521.\n",
      "Iteration 2199: Policy loss: -0.021564. Value loss: 0.020565. Entropy: 0.801627.\n",
      "episode: 1915   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 424     evaluation reward: 7.15\n",
      "episode: 1916   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 595     evaluation reward: 7.13\n",
      "Training network. lr: 0.000234. clip: 0.093403\n",
      "Iteration 2200: Policy loss: 0.012630. Value loss: 0.023296. Entropy: 0.783807.\n",
      "Iteration 2201: Policy loss: -0.011905. Value loss: 0.018158. Entropy: 0.786560.\n",
      "Iteration 2202: Policy loss: -0.020974. Value loss: 0.014766. Entropy: 0.788878.\n",
      "episode: 1917   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 426     evaluation reward: 7.1\n",
      "episode: 1918   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 424     evaluation reward: 7.08\n",
      "episode: 1919   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 492     evaluation reward: 7.05\n",
      "Training network. lr: 0.000233. clip: 0.093394\n",
      "Iteration 2203: Policy loss: 0.008363. Value loss: 0.026290. Entropy: 0.860339.\n",
      "Iteration 2204: Policy loss: -0.011920. Value loss: 0.019014. Entropy: 0.849830.\n",
      "Iteration 2205: Policy loss: -0.025236. Value loss: 0.015168. Entropy: 0.848431.\n",
      "episode: 1920   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 483     evaluation reward: 7.02\n",
      "episode: 1921   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 541     evaluation reward: 7.05\n",
      "Training network. lr: 0.000233. clip: 0.093385\n",
      "Iteration 2206: Policy loss: 0.012098. Value loss: 0.025791. Entropy: 0.806982.\n",
      "Iteration 2207: Policy loss: -0.010595. Value loss: 0.017609. Entropy: 0.824541.\n",
      "Iteration 2208: Policy loss: -0.022787. Value loss: 0.015474. Entropy: 0.822702.\n",
      "episode: 1922   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 483     evaluation reward: 7.09\n",
      "episode: 1923   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 421     evaluation reward: 7.03\n",
      "Training network. lr: 0.000233. clip: 0.093376\n",
      "Iteration 2209: Policy loss: 0.010749. Value loss: 0.021810. Entropy: 0.791609.\n",
      "Iteration 2210: Policy loss: -0.010338. Value loss: 0.017277. Entropy: 0.788026.\n",
      "Iteration 2211: Policy loss: -0.024122. Value loss: 0.015220. Entropy: 0.785841.\n",
      "episode: 1924   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 571     evaluation reward: 7.06\n",
      "Training network. lr: 0.000233. clip: 0.093367\n",
      "Iteration 2212: Policy loss: 0.006660. Value loss: 0.038036. Entropy: 0.819814.\n",
      "Iteration 2213: Policy loss: -0.008042. Value loss: 0.028743. Entropy: 0.818743.\n",
      "Iteration 2214: Policy loss: -0.019158. Value loss: 0.024155. Entropy: 0.834673.\n",
      "episode: 1925   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 651     evaluation reward: 7.04\n",
      "episode: 1926   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 680     evaluation reward: 7.07\n",
      "Training network. lr: 0.000233. clip: 0.093358\n",
      "Iteration 2215: Policy loss: 0.005132. Value loss: 0.046311. Entropy: 0.862796.\n",
      "Iteration 2216: Policy loss: -0.012747. Value loss: 0.030741. Entropy: 0.860173.\n",
      "Iteration 2217: Policy loss: -0.023939. Value loss: 0.025324. Entropy: 0.881626.\n",
      "episode: 1927   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 475     evaluation reward: 7.05\n",
      "episode: 1928   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 578     evaluation reward: 7.09\n",
      "Training network. lr: 0.000233. clip: 0.093349\n",
      "Iteration 2218: Policy loss: 0.009018. Value loss: 0.025649. Entropy: 0.728993.\n",
      "Iteration 2219: Policy loss: -0.013382. Value loss: 0.017533. Entropy: 0.724832.\n",
      "Iteration 2220: Policy loss: -0.021478. Value loss: 0.014053. Entropy: 0.725571.\n",
      "episode: 1929   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 784     evaluation reward: 7.11\n",
      "Training network. lr: 0.000233. clip: 0.093340\n",
      "Iteration 2221: Policy loss: 0.005925. Value loss: 0.025571. Entropy: 0.851949.\n",
      "Iteration 2222: Policy loss: -0.014477. Value loss: 0.016422. Entropy: 0.863451.\n",
      "Iteration 2223: Policy loss: -0.022049. Value loss: 0.013770. Entropy: 0.855354.\n",
      "episode: 1930   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 647     evaluation reward: 7.15\n",
      "episode: 1931   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 640     evaluation reward: 7.16\n",
      "Training network. lr: 0.000233. clip: 0.093331\n",
      "Iteration 2224: Policy loss: 0.009713. Value loss: 0.027381. Entropy: 0.872221.\n",
      "Iteration 2225: Policy loss: -0.011285. Value loss: 0.018512. Entropy: 0.862366.\n",
      "Iteration 2226: Policy loss: -0.021639. Value loss: 0.015046. Entropy: 0.852682.\n",
      "episode: 1932   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 648     evaluation reward: 7.16\n",
      "episode: 1933   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 550     evaluation reward: 7.16\n",
      "Training network. lr: 0.000233. clip: 0.093322\n",
      "Iteration 2227: Policy loss: 0.010186. Value loss: 0.021681. Entropy: 0.747392.\n",
      "Iteration 2228: Policy loss: -0.009783. Value loss: 0.015799. Entropy: 0.751958.\n",
      "Iteration 2229: Policy loss: -0.024909. Value loss: 0.013421. Entropy: 0.742065.\n",
      "episode: 1934   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 458     evaluation reward: 7.15\n",
      "episode: 1935   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 591     evaluation reward: 7.18\n",
      "Training network. lr: 0.000233. clip: 0.093313\n",
      "Iteration 2230: Policy loss: 0.007587. Value loss: 0.022458. Entropy: 0.800160.\n",
      "Iteration 2231: Policy loss: -0.008390. Value loss: 0.017187. Entropy: 0.802238.\n",
      "Iteration 2232: Policy loss: -0.024625. Value loss: 0.014146. Entropy: 0.804276.\n",
      "episode: 1936   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 765     evaluation reward: 7.23\n",
      "Training network. lr: 0.000233. clip: 0.093304\n",
      "Iteration 2233: Policy loss: 0.009580. Value loss: 0.030212. Entropy: 0.857097.\n",
      "Iteration 2234: Policy loss: -0.011580. Value loss: 0.018767. Entropy: 0.854600.\n",
      "Iteration 2235: Policy loss: -0.025063. Value loss: 0.015875. Entropy: 0.842703.\n",
      "episode: 1937   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 493     evaluation reward: 7.17\n",
      "episode: 1938   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 619     evaluation reward: 7.14\n",
      "Training network. lr: 0.000233. clip: 0.093295\n",
      "Iteration 2236: Policy loss: 0.010079. Value loss: 0.026210. Entropy: 0.810368.\n",
      "Iteration 2237: Policy loss: -0.011916. Value loss: 0.018140. Entropy: 0.792358.\n",
      "Iteration 2238: Policy loss: -0.025429. Value loss: 0.015557. Entropy: 0.786530.\n",
      "episode: 1939   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 7.14\n",
      "episode: 1940   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 615     evaluation reward: 7.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000233. clip: 0.093286\n",
      "Iteration 2239: Policy loss: 0.022851. Value loss: 0.019081. Entropy: 0.752867.\n",
      "Iteration 2240: Policy loss: -0.007983. Value loss: 0.013917. Entropy: 0.754174.\n",
      "Iteration 2241: Policy loss: -0.021536. Value loss: 0.011455. Entropy: 0.755832.\n",
      "episode: 1941   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 502     evaluation reward: 7.09\n",
      "episode: 1942   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 489     evaluation reward: 7.07\n",
      "Training network. lr: 0.000233. clip: 0.093277\n",
      "Iteration 2242: Policy loss: 0.008672. Value loss: 0.020313. Entropy: 0.855314.\n",
      "Iteration 2243: Policy loss: -0.013016. Value loss: 0.014909. Entropy: 0.873946.\n",
      "Iteration 2244: Policy loss: -0.023887. Value loss: 0.013292. Entropy: 0.856726.\n",
      "episode: 1943   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 555     evaluation reward: 7.08\n",
      "episode: 1944   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 501     evaluation reward: 7.06\n",
      "Training network. lr: 0.000233. clip: 0.093268\n",
      "Iteration 2245: Policy loss: 0.004320. Value loss: 0.014724. Entropy: 0.762604.\n",
      "Iteration 2246: Policy loss: -0.014745. Value loss: 0.010917. Entropy: 0.773340.\n",
      "Iteration 2247: Policy loss: -0.023919. Value loss: 0.009944. Entropy: 0.761495.\n",
      "episode: 1945   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 591     evaluation reward: 7.03\n",
      "Training network. lr: 0.000233. clip: 0.093259\n",
      "Iteration 2248: Policy loss: 0.012075. Value loss: 0.017651. Entropy: 0.813127.\n",
      "Iteration 2249: Policy loss: -0.013197. Value loss: 0.013063. Entropy: 0.801947.\n",
      "Iteration 2250: Policy loss: -0.023158. Value loss: 0.010946. Entropy: 0.800825.\n",
      "episode: 1946   score: 16.0   memory length: 1024   epsilon: 1.0    steps: 693     evaluation reward: 7.16\n",
      "episode: 1947   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 347     evaluation reward: 7.1\n",
      "Training network. lr: 0.000233. clip: 0.093250\n",
      "Iteration 2251: Policy loss: 0.003828. Value loss: 0.099462. Entropy: 0.840587.\n",
      "Iteration 2252: Policy loss: -0.006405. Value loss: 0.054480. Entropy: 0.843315.\n",
      "Iteration 2253: Policy loss: -0.018993. Value loss: 0.041848. Entropy: 0.840378.\n",
      "episode: 1948   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 626     evaluation reward: 7.12\n",
      "episode: 1949   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 471     evaluation reward: 7.07\n",
      "Training network. lr: 0.000233. clip: 0.093241\n",
      "Iteration 2254: Policy loss: 0.003388. Value loss: 0.031064. Entropy: 0.907382.\n",
      "Iteration 2255: Policy loss: -0.011011. Value loss: 0.022298. Entropy: 0.904995.\n",
      "Iteration 2256: Policy loss: -0.024980. Value loss: 0.019350. Entropy: 0.904885.\n",
      "episode: 1950   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 596     evaluation reward: 7.06\n",
      "episode: 1951   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 551     evaluation reward: 7.06\n",
      "Training network. lr: 0.000233. clip: 0.093232\n",
      "Iteration 2257: Policy loss: 0.005595. Value loss: 0.033116. Entropy: 0.748969.\n",
      "Iteration 2258: Policy loss: -0.009286. Value loss: 0.024599. Entropy: 0.733822.\n",
      "Iteration 2259: Policy loss: -0.020400. Value loss: 0.019601. Entropy: 0.733219.\n",
      "episode: 1952   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 371     evaluation reward: 7.03\n",
      "episode: 1953   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 641     evaluation reward: 7.04\n",
      "Training network. lr: 0.000233. clip: 0.093223\n",
      "Iteration 2260: Policy loss: 0.011441. Value loss: 0.021089. Entropy: 0.770485.\n",
      "Iteration 2261: Policy loss: -0.013299. Value loss: 0.014807. Entropy: 0.776239.\n",
      "Iteration 2262: Policy loss: -0.024333. Value loss: 0.012583. Entropy: 0.775619.\n",
      "episode: 1954   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 832     evaluation reward: 7.11\n",
      "episode: 1955   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 535     evaluation reward: 7.09\n",
      "Training network. lr: 0.000233. clip: 0.093214\n",
      "Iteration 2263: Policy loss: 0.013318. Value loss: 0.035692. Entropy: 0.886234.\n",
      "Iteration 2264: Policy loss: -0.009426. Value loss: 0.020423. Entropy: 0.885973.\n",
      "Iteration 2265: Policy loss: -0.019785. Value loss: 0.016394. Entropy: 0.881054.\n",
      "episode: 1956   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 653     evaluation reward: 7.13\n",
      "Training network. lr: 0.000233. clip: 0.093205\n",
      "Iteration 2266: Policy loss: 0.007227. Value loss: 0.020565. Entropy: 0.733583.\n",
      "Iteration 2267: Policy loss: -0.008546. Value loss: 0.013678. Entropy: 0.734405.\n",
      "Iteration 2268: Policy loss: -0.020983. Value loss: 0.012276. Entropy: 0.744275.\n",
      "episode: 1957   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 513     evaluation reward: 7.12\n",
      "episode: 1958   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 760     evaluation reward: 7.22\n",
      "Training network. lr: 0.000233. clip: 0.093196\n",
      "Iteration 2269: Policy loss: 0.013264. Value loss: 0.052846. Entropy: 0.762202.\n",
      "Iteration 2270: Policy loss: -0.003662. Value loss: 0.038454. Entropy: 0.782554.\n",
      "Iteration 2271: Policy loss: -0.007972. Value loss: 0.035977. Entropy: 0.781647.\n",
      "episode: 1959   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 699     evaluation reward: 7.17\n",
      "episode: 1960   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 388     evaluation reward: 7.11\n",
      "Training network. lr: 0.000233. clip: 0.093187\n",
      "Iteration 2272: Policy loss: 0.005483. Value loss: 0.022117. Entropy: 0.880669.\n",
      "Iteration 2273: Policy loss: -0.017530. Value loss: 0.014461. Entropy: 0.865253.\n",
      "Iteration 2274: Policy loss: -0.026273. Value loss: 0.012328. Entropy: 0.862176.\n",
      "episode: 1961   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 536     evaluation reward: 7.11\n",
      "episode: 1962   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 465     evaluation reward: 7.12\n",
      "Training network. lr: 0.000233. clip: 0.093178\n",
      "Iteration 2275: Policy loss: 0.012354. Value loss: 0.025180. Entropy: 0.742050.\n",
      "Iteration 2276: Policy loss: -0.007636. Value loss: 0.017965. Entropy: 0.755868.\n",
      "Iteration 2277: Policy loss: -0.019788. Value loss: 0.015680. Entropy: 0.761538.\n",
      "episode: 1963   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 445     evaluation reward: 7.11\n",
      "episode: 1964   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 437     evaluation reward: 7.08\n",
      "Training network. lr: 0.000233. clip: 0.093169\n",
      "Iteration 2278: Policy loss: 0.012545. Value loss: 0.028443. Entropy: 0.762105.\n",
      "Iteration 2279: Policy loss: -0.009276. Value loss: 0.022940. Entropy: 0.763267.\n",
      "Iteration 2280: Policy loss: -0.023058. Value loss: 0.019113. Entropy: 0.753487.\n",
      "episode: 1965   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 512     evaluation reward: 7.08\n",
      "episode: 1966   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 505     evaluation reward: 7.05\n",
      "Training network. lr: 0.000233. clip: 0.093160\n",
      "Iteration 2281: Policy loss: 0.006391. Value loss: 0.021633. Entropy: 0.854373.\n",
      "Iteration 2282: Policy loss: -0.015471. Value loss: 0.015862. Entropy: 0.849377.\n",
      "Iteration 2283: Policy loss: -0.020256. Value loss: 0.012577. Entropy: 0.858822.\n",
      "episode: 1967   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 628     evaluation reward: 7.09\n",
      "episode: 1968   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 412     evaluation reward: 7.11\n",
      "Training network. lr: 0.000233. clip: 0.093151\n",
      "Iteration 2284: Policy loss: 0.004454. Value loss: 0.020664. Entropy: 0.872604.\n",
      "Iteration 2285: Policy loss: -0.016466. Value loss: 0.014462. Entropy: 0.861335.\n",
      "Iteration 2286: Policy loss: -0.026909. Value loss: 0.012112. Entropy: 0.852375.\n",
      "episode: 1969   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 619     evaluation reward: 7.17\n",
      "Training network. lr: 0.000233. clip: 0.093142\n",
      "Iteration 2287: Policy loss: 0.008798. Value loss: 0.018658. Entropy: 0.835668.\n",
      "Iteration 2288: Policy loss: -0.010407. Value loss: 0.013064. Entropy: 0.822279.\n",
      "Iteration 2289: Policy loss: -0.024054. Value loss: 0.010105. Entropy: 0.816054.\n",
      "episode: 1970   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 686     evaluation reward: 7.23\n",
      "episode: 1971   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 501     evaluation reward: 7.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1972   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 469     evaluation reward: 7.32\n",
      "Training network. lr: 0.000233. clip: 0.093133\n",
      "Iteration 2290: Policy loss: 0.004736. Value loss: 0.036029. Entropy: 0.794461.\n",
      "Iteration 2291: Policy loss: -0.007338. Value loss: 0.029426. Entropy: 0.792805.\n",
      "Iteration 2292: Policy loss: -0.021085. Value loss: 0.025095. Entropy: 0.789236.\n",
      "episode: 1973   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 574     evaluation reward: 7.33\n",
      "episode: 1974   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 403     evaluation reward: 7.33\n",
      "Training network. lr: 0.000233. clip: 0.093124\n",
      "Iteration 2293: Policy loss: 0.009405. Value loss: 0.028188. Entropy: 0.830916.\n",
      "Iteration 2294: Policy loss: -0.011981. Value loss: 0.019930. Entropy: 0.815290.\n",
      "Iteration 2295: Policy loss: -0.021837. Value loss: 0.016995. Entropy: 0.809077.\n",
      "episode: 1975   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 867     evaluation reward: 7.37\n",
      "Training network. lr: 0.000233. clip: 0.093115\n",
      "Iteration 2296: Policy loss: 0.010358. Value loss: 0.026007. Entropy: 0.766417.\n",
      "Iteration 2297: Policy loss: -0.008328. Value loss: 0.017902. Entropy: 0.766013.\n",
      "Iteration 2298: Policy loss: -0.023951. Value loss: 0.014557. Entropy: 0.774067.\n",
      "episode: 1976   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 404     evaluation reward: 7.34\n",
      "episode: 1977   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 614     evaluation reward: 7.36\n",
      "Training network. lr: 0.000233. clip: 0.093106\n",
      "Iteration 2299: Policy loss: 0.007721. Value loss: 0.023168. Entropy: 0.801472.\n",
      "Iteration 2300: Policy loss: -0.009481. Value loss: 0.017547. Entropy: 0.793388.\n",
      "Iteration 2301: Policy loss: -0.023162. Value loss: 0.013951. Entropy: 0.795575.\n",
      "episode: 1978   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 547     evaluation reward: 7.37\n",
      "episode: 1979   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 390     evaluation reward: 7.37\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2302: Policy loss: 0.010234. Value loss: 0.015808. Entropy: 0.715382.\n",
      "Iteration 2303: Policy loss: -0.009719. Value loss: 0.012509. Entropy: 0.716053.\n",
      "Iteration 2304: Policy loss: -0.019594. Value loss: 0.009897. Entropy: 0.716683.\n",
      "episode: 1980   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 692     evaluation reward: 7.43\n",
      "episode: 1981   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 492     evaluation reward: 7.41\n",
      "Training network. lr: 0.000233. clip: 0.093088\n",
      "Iteration 2305: Policy loss: 0.006955. Value loss: 0.019167. Entropy: 0.821420.\n",
      "Iteration 2306: Policy loss: -0.012132. Value loss: 0.013734. Entropy: 0.815179.\n",
      "Iteration 2307: Policy loss: -0.021769. Value loss: 0.011517. Entropy: 0.814514.\n",
      "episode: 1982   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 454     evaluation reward: 7.44\n",
      "episode: 1983   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 607     evaluation reward: 7.45\n",
      "Training network. lr: 0.000233. clip: 0.093079\n",
      "Iteration 2308: Policy loss: 0.004795. Value loss: 0.016144. Entropy: 0.733760.\n",
      "Iteration 2309: Policy loss: -0.014985. Value loss: 0.012529. Entropy: 0.747952.\n",
      "Iteration 2310: Policy loss: -0.027960. Value loss: 0.010143. Entropy: 0.744988.\n",
      "episode: 1984   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 563     evaluation reward: 7.4\n",
      "episode: 1985   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 415     evaluation reward: 7.4\n",
      "Training network. lr: 0.000233. clip: 0.093070\n",
      "Iteration 2311: Policy loss: 0.008146. Value loss: 0.024115. Entropy: 0.810633.\n",
      "Iteration 2312: Policy loss: -0.009127. Value loss: 0.016325. Entropy: 0.825253.\n",
      "Iteration 2313: Policy loss: -0.021361. Value loss: 0.013298. Entropy: 0.811749.\n",
      "episode: 1986   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 665     evaluation reward: 7.51\n",
      "episode: 1987   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 457     evaluation reward: 7.53\n",
      "Training network. lr: 0.000233. clip: 0.093061\n",
      "Iteration 2314: Policy loss: 0.007689. Value loss: 0.047856. Entropy: 0.748694.\n",
      "Iteration 2315: Policy loss: -0.006353. Value loss: 0.034082. Entropy: 0.734975.\n",
      "Iteration 2316: Policy loss: -0.018448. Value loss: 0.028311. Entropy: 0.750858.\n",
      "episode: 1988   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 414     evaluation reward: 7.53\n",
      "episode: 1989   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 610     evaluation reward: 7.56\n",
      "Training network. lr: 0.000233. clip: 0.093052\n",
      "Iteration 2317: Policy loss: 0.008403. Value loss: 0.024122. Entropy: 0.809049.\n",
      "Iteration 2318: Policy loss: -0.011400. Value loss: 0.016556. Entropy: 0.798097.\n",
      "Iteration 2319: Policy loss: -0.024416. Value loss: 0.013352. Entropy: 0.793388.\n",
      "episode: 1990   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 573     evaluation reward: 7.57\n",
      "episode: 1991   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 421     evaluation reward: 7.52\n",
      "Training network. lr: 0.000233. clip: 0.093043\n",
      "Iteration 2320: Policy loss: 0.011475. Value loss: 0.016986. Entropy: 0.758930.\n",
      "Iteration 2321: Policy loss: -0.011058. Value loss: 0.014133. Entropy: 0.750254.\n",
      "Iteration 2322: Policy loss: -0.022710. Value loss: 0.011720. Entropy: 0.754057.\n",
      "episode: 1992   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 369     evaluation reward: 7.5\n",
      "episode: 1993   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 435     evaluation reward: 7.47\n",
      "Training network. lr: 0.000233. clip: 0.093034\n",
      "Iteration 2323: Policy loss: 0.011268. Value loss: 0.020035. Entropy: 0.806666.\n",
      "Iteration 2324: Policy loss: -0.007253. Value loss: 0.013425. Entropy: 0.819873.\n",
      "Iteration 2325: Policy loss: -0.021081. Value loss: 0.011743. Entropy: 0.809909.\n",
      "episode: 1994   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 461     evaluation reward: 7.49\n",
      "episode: 1995   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 348     evaluation reward: 7.46\n",
      "Training network. lr: 0.000233. clip: 0.093025\n",
      "Iteration 2326: Policy loss: 0.005342. Value loss: 0.018039. Entropy: 0.871458.\n",
      "Iteration 2327: Policy loss: -0.011635. Value loss: 0.010374. Entropy: 0.861224.\n",
      "Iteration 2328: Policy loss: -0.023661. Value loss: 0.009314. Entropy: 0.858667.\n",
      "episode: 1996   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 598     evaluation reward: 7.48\n",
      "episode: 1997   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 373     evaluation reward: 7.48\n",
      "episode: 1998   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 491     evaluation reward: 7.46\n",
      "Training network. lr: 0.000233. clip: 0.093016\n",
      "Iteration 2329: Policy loss: 0.008336. Value loss: 0.021749. Entropy: 0.784186.\n",
      "Iteration 2330: Policy loss: -0.012586. Value loss: 0.014740. Entropy: 0.771700.\n",
      "Iteration 2331: Policy loss: -0.020622. Value loss: 0.012455. Entropy: 0.768853.\n",
      "episode: 1999   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 613     evaluation reward: 7.47\n",
      "Training network. lr: 0.000233. clip: 0.093007\n",
      "Iteration 2332: Policy loss: 0.019037. Value loss: 0.022132. Entropy: 0.834967.\n",
      "Iteration 2333: Policy loss: -0.012657. Value loss: 0.015767. Entropy: 0.855113.\n",
      "Iteration 2334: Policy loss: -0.025902. Value loss: 0.012830. Entropy: 0.850881.\n",
      "episode: 2000   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 710     evaluation reward: 7.5\n",
      "episode: 2001   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 473     evaluation reward: 7.46\n",
      "Training network. lr: 0.000232. clip: 0.092998\n",
      "Iteration 2335: Policy loss: 0.010049. Value loss: 0.019128. Entropy: 0.714380.\n",
      "Iteration 2336: Policy loss: -0.008000. Value loss: 0.013760. Entropy: 0.710449.\n",
      "Iteration 2337: Policy loss: -0.021113. Value loss: 0.010660. Entropy: 0.715761.\n",
      "episode: 2002   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 515     evaluation reward: 7.51\n",
      "episode: 2003   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 560     evaluation reward: 7.57\n",
      "Training network. lr: 0.000232. clip: 0.092989\n",
      "Iteration 2338: Policy loss: 0.006977. Value loss: 0.041917. Entropy: 0.805977.\n",
      "Iteration 2339: Policy loss: -0.011584. Value loss: 0.030621. Entropy: 0.804457.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2340: Policy loss: -0.022619. Value loss: 0.026250. Entropy: 0.807844.\n",
      "episode: 2004   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 669     evaluation reward: 7.57\n",
      "episode: 2005   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 482     evaluation reward: 7.56\n",
      "Training network. lr: 0.000232. clip: 0.092980\n",
      "Iteration 2341: Policy loss: 0.007606. Value loss: 0.023997. Entropy: 0.698180.\n",
      "Iteration 2342: Policy loss: -0.010051. Value loss: 0.015059. Entropy: 0.709601.\n",
      "Iteration 2343: Policy loss: -0.023314. Value loss: 0.012782. Entropy: 0.702789.\n",
      "episode: 2006   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 371     evaluation reward: 7.51\n",
      "now time :  2018-12-26 13:16:13.467821\n",
      "episode: 2007   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 427     evaluation reward: 7.51\n",
      "episode: 2008   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 356     evaluation reward: 7.5\n",
      "Training network. lr: 0.000232. clip: 0.092971\n",
      "Iteration 2344: Policy loss: 0.004224. Value loss: 0.024697. Entropy: 0.750348.\n",
      "Iteration 2345: Policy loss: -0.010037. Value loss: 0.018331. Entropy: 0.742202.\n",
      "Iteration 2346: Policy loss: -0.019282. Value loss: 0.016332. Entropy: 0.750784.\n",
      "episode: 2009   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 640     evaluation reward: 7.51\n",
      "Training network. lr: 0.000232. clip: 0.092962\n",
      "Iteration 2347: Policy loss: 0.008190. Value loss: 0.019354. Entropy: 0.790795.\n",
      "Iteration 2348: Policy loss: -0.011068. Value loss: 0.014599. Entropy: 0.779265.\n",
      "Iteration 2349: Policy loss: -0.026032. Value loss: 0.012940. Entropy: 0.781999.\n",
      "episode: 2010   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 488     evaluation reward: 7.43\n",
      "episode: 2011   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 549     evaluation reward: 7.42\n",
      "Training network. lr: 0.000232. clip: 0.092953\n",
      "Iteration 2350: Policy loss: 0.003840. Value loss: 0.024442. Entropy: 0.796330.\n",
      "Iteration 2351: Policy loss: -0.012612. Value loss: 0.017542. Entropy: 0.790552.\n",
      "Iteration 2352: Policy loss: -0.022510. Value loss: 0.013582. Entropy: 0.786121.\n",
      "episode: 2012   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 588     evaluation reward: 7.41\n",
      "episode: 2013   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 371     evaluation reward: 7.37\n",
      "Training network. lr: 0.000232. clip: 0.092944\n",
      "Iteration 2353: Policy loss: 0.008421. Value loss: 0.021156. Entropy: 0.768208.\n",
      "Iteration 2354: Policy loss: -0.007911. Value loss: 0.014044. Entropy: 0.775705.\n",
      "Iteration 2355: Policy loss: -0.022452. Value loss: 0.012086. Entropy: 0.761975.\n",
      "episode: 2014   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 634     evaluation reward: 7.42\n",
      "episode: 2015   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 382     evaluation reward: 7.42\n",
      "Training network. lr: 0.000232. clip: 0.092935\n",
      "Iteration 2356: Policy loss: 0.011124. Value loss: 0.024762. Entropy: 0.747951.\n",
      "Iteration 2357: Policy loss: -0.011005. Value loss: 0.016806. Entropy: 0.742979.\n",
      "Iteration 2358: Policy loss: -0.021428. Value loss: 0.013003. Entropy: 0.726363.\n",
      "episode: 2016   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 554     evaluation reward: 7.42\n",
      "episode: 2017   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 457     evaluation reward: 7.43\n",
      "episode: 2018   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 388     evaluation reward: 7.44\n",
      "Training network. lr: 0.000232. clip: 0.092926\n",
      "Iteration 2359: Policy loss: 0.004379. Value loss: 0.016472. Entropy: 0.741991.\n",
      "Iteration 2360: Policy loss: -0.014970. Value loss: 0.012673. Entropy: 0.759379.\n",
      "Iteration 2361: Policy loss: -0.026663. Value loss: 0.011170. Entropy: 0.758193.\n",
      "episode: 2019   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 706     evaluation reward: 7.52\n",
      "Training network. lr: 0.000232. clip: 0.092917\n",
      "Iteration 2362: Policy loss: 0.000709. Value loss: 0.042611. Entropy: 0.774804.\n",
      "Iteration 2363: Policy loss: -0.007551. Value loss: 0.030047. Entropy: 0.788806.\n",
      "Iteration 2364: Policy loss: -0.017797. Value loss: 0.024769. Entropy: 0.777555.\n",
      "episode: 2020   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 530     evaluation reward: 7.53\n",
      "episode: 2021   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 376     evaluation reward: 7.51\n",
      "episode: 2022   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 388     evaluation reward: 7.5\n",
      "Training network. lr: 0.000232. clip: 0.092908\n",
      "Iteration 2365: Policy loss: 0.004846. Value loss: 0.027573. Entropy: 0.823264.\n",
      "Iteration 2366: Policy loss: -0.012868. Value loss: 0.020898. Entropy: 0.816839.\n",
      "Iteration 2367: Policy loss: -0.023727. Value loss: 0.017505. Entropy: 0.817142.\n",
      "episode: 2023   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 493     evaluation reward: 7.51\n",
      "episode: 2024   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 544     evaluation reward: 7.47\n",
      "Training network. lr: 0.000232. clip: 0.092899\n",
      "Iteration 2368: Policy loss: 0.007015. Value loss: 0.015664. Entropy: 0.710290.\n",
      "Iteration 2369: Policy loss: -0.008861. Value loss: 0.012080. Entropy: 0.698861.\n",
      "Iteration 2370: Policy loss: -0.019618. Value loss: 0.010621. Entropy: 0.703901.\n",
      "episode: 2025   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 511     evaluation reward: 7.43\n",
      "Training network. lr: 0.000232. clip: 0.092890\n",
      "Iteration 2371: Policy loss: 0.010421. Value loss: 0.027270. Entropy: 0.888671.\n",
      "Iteration 2372: Policy loss: -0.008088. Value loss: 0.017136. Entropy: 0.885870.\n",
      "Iteration 2373: Policy loss: -0.025216. Value loss: 0.014138. Entropy: 0.874759.\n",
      "episode: 2026   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 651     evaluation reward: 7.4\n",
      "episode: 2027   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 362     evaluation reward: 7.38\n",
      "Training network. lr: 0.000232. clip: 0.092881\n",
      "Iteration 2374: Policy loss: 0.006498. Value loss: 0.025504. Entropy: 0.780362.\n",
      "Iteration 2375: Policy loss: -0.013794. Value loss: 0.018202. Entropy: 0.771487.\n",
      "Iteration 2376: Policy loss: -0.025844. Value loss: 0.014890. Entropy: 0.780607.\n",
      "episode: 2028   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 664     evaluation reward: 7.4\n",
      "episode: 2029   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 288     evaluation reward: 7.31\n",
      "episode: 2030   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 489     evaluation reward: 7.28\n",
      "Training network. lr: 0.000232. clip: 0.092872\n",
      "Iteration 2377: Policy loss: 0.012162. Value loss: 0.022019. Entropy: 0.819902.\n",
      "Iteration 2378: Policy loss: -0.016410. Value loss: 0.016945. Entropy: 0.810407.\n",
      "Iteration 2379: Policy loss: -0.028769. Value loss: 0.016079. Entropy: 0.809668.\n",
      "episode: 2031   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 607     evaluation reward: 7.28\n",
      "episode: 2032   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 468     evaluation reward: 7.25\n",
      "Training network. lr: 0.000232. clip: 0.092863\n",
      "Iteration 2380: Policy loss: 0.007883. Value loss: 0.015641. Entropy: 0.832665.\n",
      "Iteration 2381: Policy loss: -0.013315. Value loss: 0.011818. Entropy: 0.823623.\n",
      "Iteration 2382: Policy loss: -0.024201. Value loss: 0.010354. Entropy: 0.815701.\n",
      "episode: 2033   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 390     evaluation reward: 7.22\n",
      "episode: 2034   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 370     evaluation reward: 7.21\n",
      "Training network. lr: 0.000232. clip: 0.092854\n",
      "Iteration 2383: Policy loss: 0.011276. Value loss: 0.017436. Entropy: 0.833261.\n",
      "Iteration 2384: Policy loss: -0.011251. Value loss: 0.013578. Entropy: 0.839730.\n",
      "Iteration 2385: Policy loss: -0.024602. Value loss: 0.011458. Entropy: 0.842313.\n",
      "episode: 2035   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 588     evaluation reward: 7.21\n",
      "episode: 2036   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 354     evaluation reward: 7.13\n",
      "episode: 2037   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 514     evaluation reward: 7.14\n",
      "Training network. lr: 0.000232. clip: 0.092845\n",
      "Iteration 2386: Policy loss: 0.006511. Value loss: 0.016803. Entropy: 0.803655.\n",
      "Iteration 2387: Policy loss: -0.011260. Value loss: 0.012233. Entropy: 0.817990.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2388: Policy loss: -0.023322. Value loss: 0.010571. Entropy: 0.822809.\n",
      "episode: 2038   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 577     evaluation reward: 7.14\n",
      "Training network. lr: 0.000232. clip: 0.092836\n",
      "Iteration 2389: Policy loss: 0.006061. Value loss: 0.016506. Entropy: 0.780780.\n",
      "Iteration 2390: Policy loss: -0.014117. Value loss: 0.012295. Entropy: 0.771479.\n",
      "Iteration 2391: Policy loss: -0.026441. Value loss: 0.009564. Entropy: 0.769389.\n",
      "episode: 2039   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 618     evaluation reward: 7.16\n",
      "episode: 2040   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 705     evaluation reward: 7.18\n",
      "Training network. lr: 0.000232. clip: 0.092827\n",
      "Iteration 2392: Policy loss: 0.006073. Value loss: 0.018156. Entropy: 0.785334.\n",
      "Iteration 2393: Policy loss: -0.012679. Value loss: 0.013687. Entropy: 0.779331.\n",
      "Iteration 2394: Policy loss: -0.027968. Value loss: 0.010974. Entropy: 0.772200.\n",
      "episode: 2041   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 416     evaluation reward: 7.17\n",
      "episode: 2042   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 731     evaluation reward: 7.21\n",
      "Training network. lr: 0.000232. clip: 0.092818\n",
      "Iteration 2395: Policy loss: 0.005988. Value loss: 0.021179. Entropy: 0.783373.\n",
      "Iteration 2396: Policy loss: -0.009478. Value loss: 0.016127. Entropy: 0.780937.\n",
      "Iteration 2397: Policy loss: -0.023305. Value loss: 0.012461. Entropy: 0.780432.\n",
      "episode: 2043   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 577     evaluation reward: 7.21\n",
      "episode: 2044   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 387     evaluation reward: 7.2\n",
      "Training network. lr: 0.000232. clip: 0.092809\n",
      "Iteration 2398: Policy loss: 0.009786. Value loss: 0.015630. Entropy: 0.818218.\n",
      "Iteration 2399: Policy loss: -0.012336. Value loss: 0.012383. Entropy: 0.813743.\n",
      "Iteration 2400: Policy loss: -0.022598. Value loss: 0.010524. Entropy: 0.818521.\n",
      "episode: 2045   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 623     evaluation reward: 7.23\n",
      "Training network. lr: 0.000232. clip: 0.092800\n",
      "Iteration 2401: Policy loss: 0.006286. Value loss: 0.021039. Entropy: 0.841339.\n",
      "Iteration 2402: Policy loss: -0.014675. Value loss: 0.016992. Entropy: 0.832721.\n",
      "Iteration 2403: Policy loss: -0.024872. Value loss: 0.013741. Entropy: 0.838103.\n",
      "episode: 2046   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 594     evaluation reward: 7.15\n",
      "episode: 2047   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 624     evaluation reward: 7.2\n",
      "Training network. lr: 0.000232. clip: 0.092791\n",
      "Iteration 2404: Policy loss: 0.003598. Value loss: 0.015761. Entropy: 0.825816.\n",
      "Iteration 2405: Policy loss: -0.013574. Value loss: 0.011045. Entropy: 0.832457.\n",
      "Iteration 2406: Policy loss: -0.022533. Value loss: 0.009453. Entropy: 0.824876.\n",
      "episode: 2048   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 459     evaluation reward: 7.17\n",
      "episode: 2049   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 365     evaluation reward: 7.16\n",
      "Training network. lr: 0.000232. clip: 0.092782\n",
      "Iteration 2407: Policy loss: 0.005850. Value loss: 0.022630. Entropy: 0.844707.\n",
      "Iteration 2408: Policy loss: -0.014266. Value loss: 0.016899. Entropy: 0.823025.\n",
      "Iteration 2409: Policy loss: -0.026290. Value loss: 0.015041. Entropy: 0.823621.\n",
      "episode: 2050   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 532     evaluation reward: 7.15\n",
      "episode: 2051   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 591     evaluation reward: 7.17\n",
      "Training network. lr: 0.000232. clip: 0.092773\n",
      "Iteration 2410: Policy loss: 0.009516. Value loss: 0.016956. Entropy: 0.844128.\n",
      "Iteration 2411: Policy loss: -0.013250. Value loss: 0.014188. Entropy: 0.830057.\n",
      "Iteration 2412: Policy loss: -0.024810. Value loss: 0.010954. Entropy: 0.811208.\n",
      "episode: 2052   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 587     evaluation reward: 7.19\n",
      "episode: 2053   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 527     evaluation reward: 7.18\n",
      "Training network. lr: 0.000232. clip: 0.092764\n",
      "Iteration 2413: Policy loss: 0.006476. Value loss: 0.021166. Entropy: 0.783553.\n",
      "Iteration 2414: Policy loss: -0.016182. Value loss: 0.014885. Entropy: 0.773206.\n",
      "Iteration 2415: Policy loss: -0.028563. Value loss: 0.012475. Entropy: 0.769385.\n",
      "episode: 2054   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 470     evaluation reward: 7.15\n",
      "episode: 2055   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 383     evaluation reward: 7.13\n",
      "Training network. lr: 0.000232. clip: 0.092755\n",
      "Iteration 2416: Policy loss: 0.006405. Value loss: 0.042840. Entropy: 0.794198.\n",
      "Iteration 2417: Policy loss: -0.003195. Value loss: 0.032451. Entropy: 0.783269.\n",
      "Iteration 2418: Policy loss: -0.016999. Value loss: 0.028591. Entropy: 0.790039.\n",
      "episode: 2056   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 642     evaluation reward: 7.13\n",
      "episode: 2057   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 451     evaluation reward: 7.11\n",
      "Training network. lr: 0.000232. clip: 0.092746\n",
      "Iteration 2419: Policy loss: 0.008887. Value loss: 0.023874. Entropy: 0.768148.\n",
      "Iteration 2420: Policy loss: -0.011463. Value loss: 0.018243. Entropy: 0.765252.\n",
      "Iteration 2421: Policy loss: -0.024610. Value loss: 0.015050. Entropy: 0.770128.\n",
      "episode: 2058   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 445     evaluation reward: 7.04\n",
      "episode: 2059   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 548     evaluation reward: 7.01\n",
      "Training network. lr: 0.000232. clip: 0.092737\n",
      "Iteration 2422: Policy loss: 0.007273. Value loss: 0.015378. Entropy: 0.811069.\n",
      "Iteration 2423: Policy loss: -0.013673. Value loss: 0.012108. Entropy: 0.808071.\n",
      "Iteration 2424: Policy loss: -0.025065. Value loss: 0.011107. Entropy: 0.806726.\n",
      "episode: 2060   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 491     evaluation reward: 7.03\n",
      "episode: 2061   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 430     evaluation reward: 7.02\n",
      "Training network. lr: 0.000232. clip: 0.092728\n",
      "Iteration 2425: Policy loss: 0.009291. Value loss: 0.026675. Entropy: 0.811166.\n",
      "Iteration 2426: Policy loss: -0.013035. Value loss: 0.019215. Entropy: 0.802796.\n",
      "Iteration 2427: Policy loss: -0.020124. Value loss: 0.015385. Entropy: 0.794898.\n",
      "episode: 2062   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 576     evaluation reward: 7.05\n",
      "episode: 2063   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 610     evaluation reward: 7.07\n",
      "Training network. lr: 0.000232. clip: 0.092719\n",
      "Iteration 2428: Policy loss: 0.008682. Value loss: 0.015298. Entropy: 0.781404.\n",
      "Iteration 2429: Policy loss: -0.014996. Value loss: 0.012214. Entropy: 0.772670.\n",
      "Iteration 2430: Policy loss: -0.024607. Value loss: 0.009788. Entropy: 0.779119.\n",
      "episode: 2064   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 707     evaluation reward: 7.15\n",
      "episode: 2065   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 568     evaluation reward: 7.16\n",
      "Training network. lr: 0.000232. clip: 0.092710\n",
      "Iteration 2431: Policy loss: 0.002792. Value loss: 0.046570. Entropy: 0.738072.\n",
      "Iteration 2432: Policy loss: -0.010819. Value loss: 0.034428. Entropy: 0.743323.\n",
      "Iteration 2433: Policy loss: -0.019628. Value loss: 0.029753. Entropy: 0.745271.\n",
      "episode: 2066   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 483     evaluation reward: 7.17\n",
      "episode: 2067   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 511     evaluation reward: 7.14\n",
      "Training network. lr: 0.000232. clip: 0.092701\n",
      "Iteration 2434: Policy loss: 0.008292. Value loss: 0.014669. Entropy: 0.757810.\n",
      "Iteration 2435: Policy loss: -0.010726. Value loss: 0.011004. Entropy: 0.750664.\n",
      "Iteration 2436: Policy loss: -0.023721. Value loss: 0.010167. Entropy: 0.756806.\n",
      "episode: 2068   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 348     evaluation reward: 7.13\n",
      "episode: 2069   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 510     evaluation reward: 7.1\n",
      "Training network. lr: 0.000232. clip: 0.092692\n",
      "Iteration 2437: Policy loss: 0.002375. Value loss: 0.027974. Entropy: 0.718806.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2438: Policy loss: -0.009971. Value loss: 0.020371. Entropy: 0.712835.\n",
      "Iteration 2439: Policy loss: -0.021655. Value loss: 0.016224. Entropy: 0.705213.\n",
      "episode: 2070   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 562     evaluation reward: 7.08\n",
      "episode: 2071   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 482     evaluation reward: 7.11\n",
      "episode: 2072   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 206     evaluation reward: 7.02\n",
      "Training network. lr: 0.000232. clip: 0.092683\n",
      "Iteration 2440: Policy loss: 0.013096. Value loss: 0.062221. Entropy: 0.852261.\n",
      "Iteration 2441: Policy loss: -0.005929. Value loss: 0.040432. Entropy: 0.841650.\n",
      "Iteration 2442: Policy loss: -0.018766. Value loss: 0.034219. Entropy: 0.838703.\n",
      "episode: 2073   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 680     evaluation reward: 7.04\n",
      "Training network. lr: 0.000232. clip: 0.092674\n",
      "Iteration 2443: Policy loss: 0.010795. Value loss: 0.024600. Entropy: 0.853607.\n",
      "Iteration 2444: Policy loss: -0.009966. Value loss: 0.019280. Entropy: 0.845546.\n",
      "Iteration 2445: Policy loss: -0.022775. Value loss: 0.016104. Entropy: 0.841961.\n",
      "episode: 2074   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 653     evaluation reward: 7.1\n",
      "episode: 2075   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 465     evaluation reward: 7.03\n",
      "Training network. lr: 0.000232. clip: 0.092665\n",
      "Iteration 2446: Policy loss: 0.008270. Value loss: 0.024600. Entropy: 0.829271.\n",
      "Iteration 2447: Policy loss: -0.017408. Value loss: 0.018292. Entropy: 0.822047.\n",
      "Iteration 2448: Policy loss: -0.027489. Value loss: 0.014835. Entropy: 0.812855.\n",
      "episode: 2076   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 450     evaluation reward: 7.04\n",
      "episode: 2077   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 282     evaluation reward: 6.98\n",
      "Training network. lr: 0.000232. clip: 0.092656\n",
      "Iteration 2449: Policy loss: 0.010316. Value loss: 0.029833. Entropy: 0.738164.\n",
      "Iteration 2450: Policy loss: -0.013173. Value loss: 0.021278. Entropy: 0.751988.\n",
      "Iteration 2451: Policy loss: -0.028602. Value loss: 0.016650. Entropy: 0.747069.\n",
      "episode: 2078   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 721     evaluation reward: 7.01\n",
      "episode: 2079   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 511     evaluation reward: 7.04\n",
      "episode: 2080   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 370     evaluation reward: 6.98\n",
      "Training network. lr: 0.000232. clip: 0.092647\n",
      "Iteration 2452: Policy loss: 0.015783. Value loss: 0.020753. Entropy: 0.731022.\n",
      "Iteration 2453: Policy loss: -0.006870. Value loss: 0.014517. Entropy: 0.726347.\n",
      "Iteration 2454: Policy loss: -0.022844. Value loss: 0.012369. Entropy: 0.730815.\n",
      "episode: 2081   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 530     evaluation reward: 6.98\n",
      "episode: 2082   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 6.97\n",
      "Training network. lr: 0.000232. clip: 0.092638\n",
      "Iteration 2455: Policy loss: 0.005959. Value loss: 0.017457. Entropy: 0.698033.\n",
      "Iteration 2456: Policy loss: -0.010167. Value loss: 0.013105. Entropy: 0.698035.\n",
      "Iteration 2457: Policy loss: -0.023017. Value loss: 0.011665. Entropy: 0.697396.\n",
      "episode: 2083   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 573     evaluation reward: 6.97\n",
      "Training network. lr: 0.000232. clip: 0.092629\n",
      "Iteration 2458: Policy loss: 0.011497. Value loss: 0.019517. Entropy: 0.740099.\n",
      "Iteration 2459: Policy loss: -0.008382. Value loss: 0.015229. Entropy: 0.714907.\n",
      "Iteration 2460: Policy loss: -0.024845. Value loss: 0.013500. Entropy: 0.708775.\n",
      "episode: 2084   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 666     evaluation reward: 6.99\n",
      "episode: 2085   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 456     evaluation reward: 7.0\n",
      "Training network. lr: 0.000232. clip: 0.092620\n",
      "Iteration 2461: Policy loss: 0.008526. Value loss: 0.020151. Entropy: 0.753085.\n",
      "Iteration 2462: Policy loss: -0.009473. Value loss: 0.015155. Entropy: 0.755951.\n",
      "Iteration 2463: Policy loss: -0.020926. Value loss: 0.013062. Entropy: 0.755268.\n",
      "episode: 2086   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 612     evaluation reward: 6.95\n",
      "episode: 2087   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 541     evaluation reward: 6.98\n",
      "Training network. lr: 0.000232. clip: 0.092611\n",
      "Iteration 2464: Policy loss: 0.008284. Value loss: 0.027742. Entropy: 0.802436.\n",
      "Iteration 2465: Policy loss: -0.012124. Value loss: 0.020638. Entropy: 0.814500.\n",
      "Iteration 2466: Policy loss: -0.026047. Value loss: 0.016999. Entropy: 0.811886.\n",
      "episode: 2088   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 470     evaluation reward: 6.97\n",
      "episode: 2089   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 355     evaluation reward: 6.92\n",
      "Training network. lr: 0.000232. clip: 0.092602\n",
      "Iteration 2467: Policy loss: 0.008196. Value loss: 0.024022. Entropy: 0.740446.\n",
      "Iteration 2468: Policy loss: -0.012322. Value loss: 0.016897. Entropy: 0.745519.\n",
      "Iteration 2469: Policy loss: -0.022712. Value loss: 0.014219. Entropy: 0.744000.\n",
      "episode: 2090   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 610     evaluation reward: 6.93\n",
      "episode: 2091   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 592     evaluation reward: 6.96\n",
      "Training network. lr: 0.000231. clip: 0.092593\n",
      "Iteration 2470: Policy loss: 0.005969. Value loss: 0.018199. Entropy: 0.812670.\n",
      "Iteration 2471: Policy loss: -0.012878. Value loss: 0.014490. Entropy: 0.817060.\n",
      "Iteration 2472: Policy loss: -0.025952. Value loss: 0.012387. Entropy: 0.808256.\n",
      "episode: 2092   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 446     evaluation reward: 6.97\n",
      "episode: 2093   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 590     evaluation reward: 6.99\n",
      "Training network. lr: 0.000231. clip: 0.092584\n",
      "Iteration 2473: Policy loss: 0.008326. Value loss: 0.017836. Entropy: 0.711734.\n",
      "Iteration 2474: Policy loss: -0.010964. Value loss: 0.013440. Entropy: 0.720260.\n",
      "Iteration 2475: Policy loss: -0.020865. Value loss: 0.010755. Entropy: 0.709747.\n",
      "episode: 2094   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 744     evaluation reward: 7.04\n",
      "episode: 2095   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 513     evaluation reward: 7.08\n",
      "Training network. lr: 0.000231. clip: 0.092575\n",
      "Iteration 2476: Policy loss: 0.012546. Value loss: 0.024097. Entropy: 0.698986.\n",
      "Iteration 2477: Policy loss: -0.008672. Value loss: 0.017790. Entropy: 0.720182.\n",
      "Iteration 2478: Policy loss: -0.022245. Value loss: 0.014312. Entropy: 0.701926.\n",
      "episode: 2096   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 436     evaluation reward: 7.05\n",
      "episode: 2097   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 403     evaluation reward: 7.06\n",
      "Training network. lr: 0.000231. clip: 0.092566\n",
      "Iteration 2479: Policy loss: 0.005527. Value loss: 0.023412. Entropy: 0.860240.\n",
      "Iteration 2480: Policy loss: -0.014245. Value loss: 0.018190. Entropy: 0.860115.\n",
      "Iteration 2481: Policy loss: -0.027721. Value loss: 0.015560. Entropy: 0.860623.\n",
      "episode: 2098   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 503     evaluation reward: 7.07\n",
      "episode: 2099   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 589     evaluation reward: 7.08\n",
      "Training network. lr: 0.000231. clip: 0.092557\n",
      "Iteration 2482: Policy loss: 0.006057. Value loss: 0.018062. Entropy: 0.766886.\n",
      "Iteration 2483: Policy loss: -0.018250. Value loss: 0.013511. Entropy: 0.750644.\n",
      "Iteration 2484: Policy loss: -0.029032. Value loss: 0.010704. Entropy: 0.749556.\n",
      "episode: 2100   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 750     evaluation reward: 7.12\n",
      "Training network. lr: 0.000231. clip: 0.092548\n",
      "Iteration 2485: Policy loss: 0.006874. Value loss: 0.046120. Entropy: 0.778278.\n",
      "Iteration 2486: Policy loss: -0.011629. Value loss: 0.037113. Entropy: 0.790659.\n",
      "Iteration 2487: Policy loss: -0.020272. Value loss: 0.033478. Entropy: 0.794480.\n",
      "episode: 2101   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 598     evaluation reward: 7.14\n",
      "episode: 2102   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 469     evaluation reward: 7.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2103   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 452     evaluation reward: 7.07\n",
      "Training network. lr: 0.000231. clip: 0.092539\n",
      "Iteration 2488: Policy loss: 0.003252. Value loss: 0.023786. Entropy: 0.792792.\n",
      "Iteration 2489: Policy loss: -0.010653. Value loss: 0.018366. Entropy: 0.792709.\n",
      "Iteration 2490: Policy loss: -0.023688. Value loss: 0.015093. Entropy: 0.791246.\n",
      "now time :  2018-12-26 13:20:36.800548\n",
      "episode: 2104   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 592     evaluation reward: 7.06\n",
      "Training network. lr: 0.000231. clip: 0.092530\n",
      "Iteration 2491: Policy loss: 0.018582. Value loss: 0.080537. Entropy: 0.714300.\n",
      "Iteration 2492: Policy loss: -0.005067. Value loss: 0.045746. Entropy: 0.707660.\n",
      "Iteration 2493: Policy loss: -0.010994. Value loss: 0.038759. Entropy: 0.700814.\n",
      "episode: 2105   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 581     evaluation reward: 7.07\n",
      "episode: 2106   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 614     evaluation reward: 7.11\n",
      "Training network. lr: 0.000231. clip: 0.092521\n",
      "Iteration 2494: Policy loss: 0.007192. Value loss: 0.022744. Entropy: 0.791955.\n",
      "Iteration 2495: Policy loss: -0.011906. Value loss: 0.018102. Entropy: 0.794294.\n",
      "Iteration 2496: Policy loss: -0.021029. Value loss: 0.015508. Entropy: 0.795948.\n",
      "episode: 2107   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 642     evaluation reward: 7.14\n",
      "episode: 2108   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 404     evaluation reward: 7.14\n",
      "Training network. lr: 0.000231. clip: 0.092512\n",
      "Iteration 2497: Policy loss: 0.013783. Value loss: 0.023120. Entropy: 0.908186.\n",
      "Iteration 2498: Policy loss: -0.017843. Value loss: 0.016934. Entropy: 0.896603.\n",
      "Iteration 2499: Policy loss: -0.031611. Value loss: 0.013867. Entropy: 0.892392.\n",
      "episode: 2109   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 802     evaluation reward: 7.14\n",
      "episode: 2110   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 454     evaluation reward: 7.15\n",
      "Training network. lr: 0.000231. clip: 0.092503\n",
      "Iteration 2500: Policy loss: 0.005830. Value loss: 0.019417. Entropy: 0.868200.\n",
      "Iteration 2501: Policy loss: -0.010609. Value loss: 0.013570. Entropy: 0.852004.\n",
      "Iteration 2502: Policy loss: -0.018136. Value loss: 0.011098. Entropy: 0.857002.\n",
      "episode: 2111   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 323     evaluation reward: 7.1\n",
      "episode: 2112   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 438     evaluation reward: 7.08\n",
      "Training network. lr: 0.000231. clip: 0.092494\n",
      "Iteration 2503: Policy loss: 0.016287. Value loss: 0.015959. Entropy: 0.782755.\n",
      "Iteration 2504: Policy loss: -0.009248. Value loss: 0.012622. Entropy: 0.773745.\n",
      "Iteration 2505: Policy loss: -0.017695. Value loss: 0.010475. Entropy: 0.779772.\n",
      "episode: 2113   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 560     evaluation reward: 7.12\n",
      "episode: 2114   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 547     evaluation reward: 7.09\n",
      "Training network. lr: 0.000231. clip: 0.092485\n",
      "Iteration 2506: Policy loss: 0.010794. Value loss: 0.025438. Entropy: 0.795642.\n",
      "Iteration 2507: Policy loss: -0.013972. Value loss: 0.016674. Entropy: 0.793232.\n",
      "Iteration 2508: Policy loss: -0.019876. Value loss: 0.014559. Entropy: 0.780204.\n",
      "episode: 2115   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 501     evaluation reward: 7.1\n",
      "episode: 2116   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 600     evaluation reward: 7.12\n",
      "Training network. lr: 0.000231. clip: 0.092476\n",
      "Iteration 2509: Policy loss: 0.009221. Value loss: 0.024480. Entropy: 0.734168.\n",
      "Iteration 2510: Policy loss: -0.013880. Value loss: 0.016714. Entropy: 0.738470.\n",
      "Iteration 2511: Policy loss: -0.023335. Value loss: 0.012169. Entropy: 0.732949.\n",
      "episode: 2117   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 504     evaluation reward: 7.13\n",
      "episode: 2118   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 471     evaluation reward: 7.14\n",
      "Training network. lr: 0.000231. clip: 0.092467\n",
      "Iteration 2512: Policy loss: 0.005818. Value loss: 0.022342. Entropy: 0.787701.\n",
      "Iteration 2513: Policy loss: -0.011656. Value loss: 0.017446. Entropy: 0.780283.\n",
      "Iteration 2514: Policy loss: -0.022348. Value loss: 0.016798. Entropy: 0.779141.\n",
      "episode: 2119   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 669     evaluation reward: 7.09\n",
      "episode: 2120   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 429     evaluation reward: 7.06\n",
      "Training network. lr: 0.000231. clip: 0.092458\n",
      "Iteration 2515: Policy loss: 0.005004. Value loss: 0.017294. Entropy: 0.876499.\n",
      "Iteration 2516: Policy loss: -0.014874. Value loss: 0.014288. Entropy: 0.874255.\n",
      "Iteration 2517: Policy loss: -0.029523. Value loss: 0.011688. Entropy: 0.873978.\n",
      "episode: 2121   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 580     evaluation reward: 7.09\n",
      "episode: 2122   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 487     evaluation reward: 7.11\n",
      "Training network. lr: 0.000231. clip: 0.092449\n",
      "Iteration 2518: Policy loss: 0.007332. Value loss: 0.020777. Entropy: 0.784508.\n",
      "Iteration 2519: Policy loss: -0.012171. Value loss: 0.015899. Entropy: 0.780204.\n",
      "Iteration 2520: Policy loss: -0.025451. Value loss: 0.013309. Entropy: 0.772751.\n",
      "episode: 2123   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 443     evaluation reward: 7.11\n",
      "episode: 2124   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 479     evaluation reward: 7.1\n",
      "Training network. lr: 0.000231. clip: 0.092440\n",
      "Iteration 2521: Policy loss: 0.008775. Value loss: 0.019532. Entropy: 0.733600.\n",
      "Iteration 2522: Policy loss: -0.013466. Value loss: 0.014699. Entropy: 0.730423.\n",
      "Iteration 2523: Policy loss: -0.024176. Value loss: 0.012233. Entropy: 0.733448.\n",
      "episode: 2125   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 383     evaluation reward: 7.08\n",
      "episode: 2126   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 540     evaluation reward: 7.07\n",
      "Training network. lr: 0.000231. clip: 0.092431\n",
      "Iteration 2524: Policy loss: 0.008671. Value loss: 0.022643. Entropy: 0.783669.\n",
      "Iteration 2525: Policy loss: -0.012873. Value loss: 0.018046. Entropy: 0.782065.\n",
      "Iteration 2526: Policy loss: -0.021146. Value loss: 0.014818. Entropy: 0.786706.\n",
      "episode: 2127   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 339     evaluation reward: 7.06\n",
      "episode: 2128   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 579     evaluation reward: 7.04\n",
      "Training network. lr: 0.000231. clip: 0.092422\n",
      "Iteration 2527: Policy loss: 0.000851. Value loss: 0.027770. Entropy: 0.822411.\n",
      "Iteration 2528: Policy loss: -0.018623. Value loss: 0.018228. Entropy: 0.823460.\n",
      "Iteration 2529: Policy loss: -0.032977. Value loss: 0.015918. Entropy: 0.824998.\n",
      "episode: 2129   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 509     evaluation reward: 7.08\n",
      "episode: 2130   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 733     evaluation reward: 7.12\n",
      "Training network. lr: 0.000231. clip: 0.092413\n",
      "Iteration 2530: Policy loss: 0.008448. Value loss: 0.015573. Entropy: 0.679688.\n",
      "Iteration 2531: Policy loss: -0.013498. Value loss: 0.011183. Entropy: 0.690880.\n",
      "Iteration 2532: Policy loss: -0.022511. Value loss: 0.008693. Entropy: 0.684260.\n",
      "episode: 2131   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 578     evaluation reward: 7.11\n",
      "Training network. lr: 0.000231. clip: 0.092404\n",
      "Iteration 2533: Policy loss: 0.004343. Value loss: 0.023457. Entropy: 0.807886.\n",
      "Iteration 2534: Policy loss: -0.010876. Value loss: 0.018234. Entropy: 0.805739.\n",
      "Iteration 2535: Policy loss: -0.026363. Value loss: 0.014303. Entropy: 0.802020.\n",
      "episode: 2132   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 608     evaluation reward: 7.13\n",
      "episode: 2133   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 503     evaluation reward: 7.16\n",
      "episode: 2134   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 504     evaluation reward: 7.19\n",
      "Training network. lr: 0.000231. clip: 0.092395\n",
      "Iteration 2536: Policy loss: 0.006729. Value loss: 0.022370. Entropy: 0.766313.\n",
      "Iteration 2537: Policy loss: -0.015297. Value loss: 0.015657. Entropy: 0.769912.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2538: Policy loss: -0.025219. Value loss: 0.013016. Entropy: 0.766449.\n",
      "episode: 2135   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 587     evaluation reward: 7.22\n",
      "Training network. lr: 0.000231. clip: 0.092386\n",
      "Iteration 2539: Policy loss: 0.003482. Value loss: 0.042139. Entropy: 0.704425.\n",
      "Iteration 2540: Policy loss: -0.008599. Value loss: 0.034137. Entropy: 0.701008.\n",
      "Iteration 2541: Policy loss: -0.019779. Value loss: 0.029128. Entropy: 0.701642.\n",
      "episode: 2136   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 452     evaluation reward: 7.24\n",
      "episode: 2137   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 461     evaluation reward: 7.23\n",
      "episode: 2138   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 407     evaluation reward: 7.2\n",
      "Training network. lr: 0.000231. clip: 0.092377\n",
      "Iteration 2542: Policy loss: 0.001542. Value loss: 0.030031. Entropy: 0.771642.\n",
      "Iteration 2543: Policy loss: -0.018564. Value loss: 0.023421. Entropy: 0.770251.\n",
      "Iteration 2544: Policy loss: -0.028172. Value loss: 0.019235. Entropy: 0.767564.\n",
      "episode: 2139   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 470     evaluation reward: 7.18\n",
      "episode: 2140   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 490     evaluation reward: 7.14\n",
      "Training network. lr: 0.000231. clip: 0.092368\n",
      "Iteration 2545: Policy loss: 0.005618. Value loss: 0.023300. Entropy: 0.801244.\n",
      "Iteration 2546: Policy loss: -0.011420. Value loss: 0.017431. Entropy: 0.797740.\n",
      "Iteration 2547: Policy loss: -0.019946. Value loss: 0.014558. Entropy: 0.791037.\n",
      "episode: 2141   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 526     evaluation reward: 7.17\n",
      "Training network. lr: 0.000231. clip: 0.092359\n",
      "Iteration 2548: Policy loss: 0.010252. Value loss: 0.028716. Entropy: 0.749519.\n",
      "Iteration 2549: Policy loss: -0.007790. Value loss: 0.018191. Entropy: 0.775262.\n",
      "Iteration 2550: Policy loss: -0.018544. Value loss: 0.015092. Entropy: 0.751283.\n",
      "episode: 2142   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 779     evaluation reward: 7.19\n",
      "episode: 2143   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 595     evaluation reward: 7.19\n",
      "Training network. lr: 0.000231. clip: 0.092350\n",
      "Iteration 2551: Policy loss: 0.012763. Value loss: 0.025568. Entropy: 0.795442.\n",
      "Iteration 2552: Policy loss: -0.008580. Value loss: 0.018378. Entropy: 0.812328.\n",
      "Iteration 2553: Policy loss: -0.026080. Value loss: 0.015350. Entropy: 0.813666.\n",
      "episode: 2144   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 477     evaluation reward: 7.2\n",
      "episode: 2145   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 510     evaluation reward: 7.16\n",
      "Training network. lr: 0.000231. clip: 0.092341\n",
      "Iteration 2554: Policy loss: 0.007316. Value loss: 0.022454. Entropy: 0.842092.\n",
      "Iteration 2555: Policy loss: -0.010838. Value loss: 0.016364. Entropy: 0.847524.\n",
      "Iteration 2556: Policy loss: -0.025423. Value loss: 0.015227. Entropy: 0.841429.\n",
      "episode: 2146   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 431     evaluation reward: 7.13\n",
      "episode: 2147   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 356     evaluation reward: 7.09\n",
      "episode: 2148   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 234     evaluation reward: 7.05\n",
      "Training network. lr: 0.000231. clip: 0.092332\n",
      "Iteration 2557: Policy loss: 0.008193. Value loss: 0.024754. Entropy: 0.813980.\n",
      "Iteration 2558: Policy loss: -0.007205. Value loss: 0.017356. Entropy: 0.806887.\n",
      "Iteration 2559: Policy loss: -0.017428. Value loss: 0.014479. Entropy: 0.811017.\n",
      "episode: 2149   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 428     evaluation reward: 7.06\n",
      "episode: 2150   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 393     evaluation reward: 7.04\n",
      "episode: 2151   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 534     evaluation reward: 7.02\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2560: Policy loss: 0.002576. Value loss: 0.019272. Entropy: 0.761704.\n",
      "Iteration 2561: Policy loss: -0.015804. Value loss: 0.015245. Entropy: 0.769437.\n",
      "Iteration 2562: Policy loss: -0.026336. Value loss: 0.012891. Entropy: 0.769171.\n",
      "episode: 2152   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 462     evaluation reward: 7.01\n",
      "episode: 2153   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 325     evaluation reward: 6.97\n",
      "Training network. lr: 0.000231. clip: 0.092314\n",
      "Iteration 2563: Policy loss: 0.005246. Value loss: 0.018553. Entropy: 0.704161.\n",
      "Iteration 2564: Policy loss: -0.007443. Value loss: 0.013656. Entropy: 0.705007.\n",
      "Iteration 2565: Policy loss: -0.020978. Value loss: 0.011177. Entropy: 0.701777.\n",
      "episode: 2154   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 543     evaluation reward: 6.95\n",
      "episode: 2155   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 566     evaluation reward: 7.01\n",
      "Training network. lr: 0.000231. clip: 0.092305\n",
      "Iteration 2566: Policy loss: 0.004120. Value loss: 0.039732. Entropy: 0.709160.\n",
      "Iteration 2567: Policy loss: -0.004384. Value loss: 0.032418. Entropy: 0.714899.\n",
      "Iteration 2568: Policy loss: -0.014077. Value loss: 0.029006. Entropy: 0.720333.\n",
      "episode: 2156   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 744     evaluation reward: 7.03\n",
      "episode: 2157   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 314     evaluation reward: 7.01\n",
      "Training network. lr: 0.000231. clip: 0.092296\n",
      "Iteration 2569: Policy loss: 0.007520. Value loss: 0.023731. Entropy: 0.777707.\n",
      "Iteration 2570: Policy loss: -0.015501. Value loss: 0.015724. Entropy: 0.788239.\n",
      "Iteration 2571: Policy loss: -0.026163. Value loss: 0.013097. Entropy: 0.783660.\n",
      "episode: 2158   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 597     evaluation reward: 7.03\n",
      "episode: 2159   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 458     evaluation reward: 7.02\n",
      "Training network. lr: 0.000231. clip: 0.092287\n",
      "Iteration 2572: Policy loss: 0.009644. Value loss: 0.023105. Entropy: 0.761330.\n",
      "Iteration 2573: Policy loss: -0.013361. Value loss: 0.016817. Entropy: 0.763667.\n",
      "Iteration 2574: Policy loss: -0.024934. Value loss: 0.014594. Entropy: 0.763643.\n",
      "episode: 2160   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 442     evaluation reward: 7.02\n",
      "episode: 2161   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 529     evaluation reward: 7.03\n",
      "Training network. lr: 0.000231. clip: 0.092278\n",
      "Iteration 2575: Policy loss: 0.005075. Value loss: 0.019634. Entropy: 0.777106.\n",
      "Iteration 2576: Policy loss: -0.016130. Value loss: 0.014690. Entropy: 0.779885.\n",
      "Iteration 2577: Policy loss: -0.027653. Value loss: 0.012284. Entropy: 0.771879.\n",
      "episode: 2162   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 398     evaluation reward: 6.99\n",
      "episode: 2163   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 327     evaluation reward: 6.95\n",
      "episode: 2164   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 468     evaluation reward: 6.88\n",
      "Training network. lr: 0.000231. clip: 0.092269\n",
      "Iteration 2578: Policy loss: 0.005789. Value loss: 0.016700. Entropy: 0.847675.\n",
      "Iteration 2579: Policy loss: -0.013893. Value loss: 0.013239. Entropy: 0.850031.\n",
      "Iteration 2580: Policy loss: -0.021844. Value loss: 0.010537. Entropy: 0.837484.\n",
      "episode: 2165   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 536     evaluation reward: 6.87\n",
      "episode: 2166   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 455     evaluation reward: 6.85\n",
      "Training network. lr: 0.000231. clip: 0.092260\n",
      "Iteration 2581: Policy loss: 0.011100. Value loss: 0.018975. Entropy: 0.804452.\n",
      "Iteration 2582: Policy loss: -0.013667. Value loss: 0.014661. Entropy: 0.799641.\n",
      "Iteration 2583: Policy loss: -0.026695. Value loss: 0.012970. Entropy: 0.797264.\n",
      "episode: 2167   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 436     evaluation reward: 6.84\n",
      "episode: 2168   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 471     evaluation reward: 6.85\n",
      "Training network. lr: 0.000231. clip: 0.092251\n",
      "Iteration 2584: Policy loss: 0.008276. Value loss: 0.019174. Entropy: 0.924549.\n",
      "Iteration 2585: Policy loss: -0.009042. Value loss: 0.015110. Entropy: 0.912215.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2586: Policy loss: -0.023718. Value loss: 0.012780. Entropy: 0.917789.\n",
      "episode: 2169   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 477     evaluation reward: 6.85\n",
      "episode: 2170   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 726     evaluation reward: 6.87\n",
      "Training network. lr: 0.000231. clip: 0.092242\n",
      "Iteration 2587: Policy loss: 0.004451. Value loss: 0.016521. Entropy: 0.787754.\n",
      "Iteration 2588: Policy loss: -0.010291. Value loss: 0.012346. Entropy: 0.783736.\n",
      "Iteration 2589: Policy loss: -0.021353. Value loss: 0.009966. Entropy: 0.777177.\n",
      "episode: 2171   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 356     evaluation reward: 6.81\n",
      "episode: 2172   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 545     evaluation reward: 6.87\n",
      "Training network. lr: 0.000231. clip: 0.092233\n",
      "Iteration 2590: Policy loss: 0.004435. Value loss: 0.021384. Entropy: 0.747002.\n",
      "Iteration 2591: Policy loss: -0.008491. Value loss: 0.013059. Entropy: 0.743399.\n",
      "Iteration 2592: Policy loss: -0.019867. Value loss: 0.010541. Entropy: 0.733698.\n",
      "episode: 2173   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 389     evaluation reward: 6.83\n",
      "episode: 2174   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 424     evaluation reward: 6.79\n",
      "Training network. lr: 0.000231. clip: 0.092224\n",
      "Iteration 2593: Policy loss: 0.002805. Value loss: 0.014014. Entropy: 0.759470.\n",
      "Iteration 2594: Policy loss: -0.012052. Value loss: 0.010817. Entropy: 0.753464.\n",
      "Iteration 2595: Policy loss: -0.025043. Value loss: 0.008751. Entropy: 0.753853.\n",
      "episode: 2175   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 623     evaluation reward: 6.85\n",
      "episode: 2176   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 578     evaluation reward: 6.86\n",
      "Training network. lr: 0.000231. clip: 0.092215\n",
      "Iteration 2596: Policy loss: 0.008500. Value loss: 0.040995. Entropy: 0.886327.\n",
      "Iteration 2597: Policy loss: -0.008000. Value loss: 0.029492. Entropy: 0.878210.\n",
      "Iteration 2598: Policy loss: -0.016858. Value loss: 0.025773. Entropy: 0.881511.\n",
      "episode: 2177   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 534     evaluation reward: 6.9\n",
      "episode: 2178   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 605     evaluation reward: 6.87\n",
      "Training network. lr: 0.000231. clip: 0.092206\n",
      "Iteration 2599: Policy loss: 0.004409. Value loss: 0.022752. Entropy: 0.744749.\n",
      "Iteration 2600: Policy loss: -0.012864. Value loss: 0.017937. Entropy: 0.745812.\n",
      "Iteration 2601: Policy loss: -0.026095. Value loss: 0.013844. Entropy: 0.743213.\n",
      "episode: 2179   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 656     evaluation reward: 6.88\n",
      "episode: 2180   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 333     evaluation reward: 6.88\n",
      "Training network. lr: 0.000230. clip: 0.092197\n",
      "Iteration 2602: Policy loss: 0.007384. Value loss: 0.026947. Entropy: 0.742861.\n",
      "Iteration 2603: Policy loss: -0.014184. Value loss: 0.019039. Entropy: 0.759866.\n",
      "Iteration 2604: Policy loss: -0.025475. Value loss: 0.016082. Entropy: 0.758575.\n",
      "episode: 2181   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 481     evaluation reward: 6.87\n",
      "episode: 2182   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 395     evaluation reward: 6.86\n",
      "Training network. lr: 0.000230. clip: 0.092188\n",
      "Iteration 2605: Policy loss: 0.007436. Value loss: 0.023991. Entropy: 0.790379.\n",
      "Iteration 2606: Policy loss: -0.012977. Value loss: 0.016877. Entropy: 0.790021.\n",
      "Iteration 2607: Policy loss: -0.026425. Value loss: 0.014440. Entropy: 0.787392.\n",
      "episode: 2183   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 485     evaluation reward: 6.84\n",
      "episode: 2184   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 242     evaluation reward: 6.76\n",
      "episode: 2185   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 494     evaluation reward: 6.76\n",
      "Training network. lr: 0.000230. clip: 0.092179\n",
      "Iteration 2608: Policy loss: 0.004629. Value loss: 0.020615. Entropy: 0.796316.\n",
      "Iteration 2609: Policy loss: -0.010011. Value loss: 0.015665. Entropy: 0.793099.\n",
      "Iteration 2610: Policy loss: -0.025423. Value loss: 0.013540. Entropy: 0.784438.\n",
      "episode: 2186   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 422     evaluation reward: 6.73\n",
      "episode: 2187   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 517     evaluation reward: 6.72\n",
      "Training network. lr: 0.000230. clip: 0.092170\n",
      "Iteration 2611: Policy loss: 0.008154. Value loss: 0.017845. Entropy: 0.832784.\n",
      "Iteration 2612: Policy loss: -0.013309. Value loss: 0.014552. Entropy: 0.825775.\n",
      "Iteration 2613: Policy loss: -0.024530. Value loss: 0.012540. Entropy: 0.827792.\n",
      "episode: 2188   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 542     evaluation reward: 6.78\n",
      "Training network. lr: 0.000230. clip: 0.092161\n",
      "Iteration 2614: Policy loss: 0.006512. Value loss: 0.048218. Entropy: 0.771289.\n",
      "Iteration 2615: Policy loss: -0.010641. Value loss: 0.038876. Entropy: 0.771526.\n",
      "Iteration 2616: Policy loss: -0.017498. Value loss: 0.032066. Entropy: 0.765564.\n",
      "episode: 2189   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 713     evaluation reward: 6.84\n",
      "episode: 2190   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 528     evaluation reward: 6.83\n",
      "Training network. lr: 0.000230. clip: 0.092152\n",
      "Iteration 2617: Policy loss: 0.011581. Value loss: 0.027547. Entropy: 0.788516.\n",
      "Iteration 2618: Policy loss: -0.011515. Value loss: 0.019801. Entropy: 0.792158.\n",
      "Iteration 2619: Policy loss: -0.021678. Value loss: 0.015261. Entropy: 0.794082.\n",
      "episode: 2191   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 511     evaluation reward: 6.83\n",
      "episode: 2192   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 528     evaluation reward: 6.85\n",
      "Training network. lr: 0.000230. clip: 0.092143\n",
      "Iteration 2620: Policy loss: 0.011788. Value loss: 0.029896. Entropy: 0.759776.\n",
      "Iteration 2621: Policy loss: -0.010872. Value loss: 0.020445. Entropy: 0.755139.\n",
      "Iteration 2622: Policy loss: -0.024870. Value loss: 0.017642. Entropy: 0.754595.\n",
      "episode: 2193   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 568     evaluation reward: 6.86\n",
      "episode: 2194   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 479     evaluation reward: 6.81\n",
      "Training network. lr: 0.000230. clip: 0.092134\n",
      "Iteration 2623: Policy loss: 0.006503. Value loss: 0.023757. Entropy: 0.816637.\n",
      "Iteration 2624: Policy loss: -0.014237. Value loss: 0.018129. Entropy: 0.808351.\n",
      "Iteration 2625: Policy loss: -0.022791. Value loss: 0.015631. Entropy: 0.800299.\n",
      "episode: 2195   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 466     evaluation reward: 6.79\n",
      "episode: 2196   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 599     evaluation reward: 6.82\n",
      "Training network. lr: 0.000230. clip: 0.092125\n",
      "Iteration 2626: Policy loss: 0.009468. Value loss: 0.026553. Entropy: 0.759451.\n",
      "Iteration 2627: Policy loss: -0.010249. Value loss: 0.017608. Entropy: 0.759404.\n",
      "Iteration 2628: Policy loss: -0.023430. Value loss: 0.014446. Entropy: 0.755825.\n",
      "episode: 2197   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 865     evaluation reward: 6.9\n",
      "Training network. lr: 0.000230. clip: 0.092116\n",
      "Iteration 2629: Policy loss: 0.007952. Value loss: 0.021462. Entropy: 0.779489.\n",
      "Iteration 2630: Policy loss: -0.017196. Value loss: 0.015686. Entropy: 0.763248.\n",
      "Iteration 2631: Policy loss: -0.024900. Value loss: 0.014809. Entropy: 0.770197.\n",
      "episode: 2198   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 550     evaluation reward: 6.9\n",
      "episode: 2199   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 6.87\n",
      "Training network. lr: 0.000230. clip: 0.092107\n",
      "Iteration 2632: Policy loss: 0.014938. Value loss: 0.028075. Entropy: 0.763240.\n",
      "Iteration 2633: Policy loss: -0.008608. Value loss: 0.021088. Entropy: 0.768710.\n",
      "Iteration 2634: Policy loss: -0.022961. Value loss: 0.016988. Entropy: 0.760585.\n",
      "episode: 2200   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 540     evaluation reward: 6.8\n",
      "episode: 2201   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 579     evaluation reward: 6.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now time :  2018-12-26 13:24:59.384128\n",
      "Training network. lr: 0.000230. clip: 0.092098\n",
      "Iteration 2635: Policy loss: 0.001163. Value loss: 0.020107. Entropy: 0.707561.\n",
      "Iteration 2636: Policy loss: -0.015331. Value loss: 0.015740. Entropy: 0.702475.\n",
      "Iteration 2637: Policy loss: -0.024758. Value loss: 0.012519. Entropy: 0.699726.\n",
      "episode: 2202   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 555     evaluation reward: 6.81\n",
      "episode: 2203   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 525     evaluation reward: 6.82\n",
      "Training network. lr: 0.000230. clip: 0.092089\n",
      "Iteration 2638: Policy loss: 0.005295. Value loss: 0.025208. Entropy: 0.683546.\n",
      "Iteration 2639: Policy loss: -0.010494. Value loss: 0.017054. Entropy: 0.676397.\n",
      "Iteration 2640: Policy loss: -0.020695. Value loss: 0.013283. Entropy: 0.674209.\n",
      "episode: 2204   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 572     evaluation reward: 6.79\n",
      "episode: 2205   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 510     evaluation reward: 6.77\n",
      "Training network. lr: 0.000230. clip: 0.092080\n",
      "Iteration 2641: Policy loss: 0.004741. Value loss: 0.021091. Entropy: 0.788589.\n",
      "Iteration 2642: Policy loss: -0.006957. Value loss: 0.014789. Entropy: 0.786969.\n",
      "Iteration 2643: Policy loss: -0.018783. Value loss: 0.012055. Entropy: 0.782209.\n",
      "episode: 2206   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 591     evaluation reward: 6.77\n",
      "Training network. lr: 0.000230. clip: 0.092071\n",
      "Iteration 2644: Policy loss: 0.016097. Value loss: 0.018067. Entropy: 0.721084.\n",
      "Iteration 2645: Policy loss: -0.010363. Value loss: 0.013367. Entropy: 0.724140.\n",
      "Iteration 2646: Policy loss: -0.018451. Value loss: 0.010795. Entropy: 0.725330.\n",
      "episode: 2207   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 773     evaluation reward: 6.8\n",
      "episode: 2208   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 650     evaluation reward: 6.85\n",
      "Training network. lr: 0.000230. clip: 0.092062\n",
      "Iteration 2647: Policy loss: 0.002694. Value loss: 0.022788. Entropy: 0.678412.\n",
      "Iteration 2648: Policy loss: -0.011411. Value loss: 0.016321. Entropy: 0.683434.\n",
      "Iteration 2649: Policy loss: -0.021786. Value loss: 0.012850. Entropy: 0.678221.\n",
      "episode: 2209   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 491     evaluation reward: 6.84\n",
      "episode: 2210   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 706     evaluation reward: 6.89\n",
      "Training network. lr: 0.000230. clip: 0.092053\n",
      "Iteration 2650: Policy loss: 0.014169. Value loss: 0.045505. Entropy: 0.800899.\n",
      "Iteration 2651: Policy loss: -0.001600. Value loss: 0.037436. Entropy: 0.804711.\n",
      "Iteration 2652: Policy loss: -0.017286. Value loss: 0.032025. Entropy: 0.797764.\n",
      "episode: 2211   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 319     evaluation reward: 6.89\n",
      "episode: 2212   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 663     evaluation reward: 6.92\n",
      "Training network. lr: 0.000230. clip: 0.092044\n",
      "Iteration 2653: Policy loss: 0.005785. Value loss: 0.024756. Entropy: 0.763980.\n",
      "Iteration 2654: Policy loss: -0.011578. Value loss: 0.016553. Entropy: 0.754832.\n",
      "Iteration 2655: Policy loss: -0.019905. Value loss: 0.014374. Entropy: 0.752762.\n",
      "episode: 2213   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 668     evaluation reward: 6.93\n",
      "Training network. lr: 0.000230. clip: 0.092035\n",
      "Iteration 2656: Policy loss: 0.009365. Value loss: 0.015655. Entropy: 0.749361.\n",
      "Iteration 2657: Policy loss: -0.010175. Value loss: 0.010676. Entropy: 0.756814.\n",
      "Iteration 2658: Policy loss: -0.024910. Value loss: 0.009003. Entropy: 0.758846.\n",
      "episode: 2214   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 599     evaluation reward: 6.94\n",
      "episode: 2215   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 487     evaluation reward: 6.95\n",
      "Training network. lr: 0.000230. clip: 0.092026\n",
      "Iteration 2659: Policy loss: 0.001930. Value loss: 0.017999. Entropy: 0.689188.\n",
      "Iteration 2660: Policy loss: -0.017664. Value loss: 0.013435. Entropy: 0.683650.\n",
      "Iteration 2661: Policy loss: -0.025043. Value loss: 0.011815. Entropy: 0.683668.\n",
      "episode: 2216   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 634     evaluation reward: 6.94\n",
      "episode: 2217   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 432     evaluation reward: 6.93\n",
      "Training network. lr: 0.000230. clip: 0.092017\n",
      "Iteration 2662: Policy loss: 0.006651. Value loss: 0.019382. Entropy: 0.834986.\n",
      "Iteration 2663: Policy loss: -0.011326. Value loss: 0.014383. Entropy: 0.821220.\n",
      "Iteration 2664: Policy loss: -0.025359. Value loss: 0.012422. Entropy: 0.827242.\n",
      "episode: 2218   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 820     evaluation reward: 7.0\n",
      "episode: 2219   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 504     evaluation reward: 6.97\n",
      "Training network. lr: 0.000230. clip: 0.092008\n",
      "Iteration 2665: Policy loss: 0.004089. Value loss: 0.025814. Entropy: 0.779883.\n",
      "Iteration 2666: Policy loss: -0.012841. Value loss: 0.016528. Entropy: 0.791417.\n",
      "Iteration 2667: Policy loss: -0.021021. Value loss: 0.013171. Entropy: 0.797758.\n",
      "episode: 2220   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 508     evaluation reward: 7.0\n",
      "episode: 2221   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 512     evaluation reward: 6.99\n",
      "Training network. lr: 0.000230. clip: 0.091999\n",
      "Iteration 2668: Policy loss: 0.007922. Value loss: 0.019316. Entropy: 0.697684.\n",
      "Iteration 2669: Policy loss: -0.015342. Value loss: 0.015728. Entropy: 0.683223.\n",
      "Iteration 2670: Policy loss: -0.023365. Value loss: 0.013382. Entropy: 0.686203.\n",
      "episode: 2222   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 598     evaluation reward: 7.0\n",
      "Training network. lr: 0.000230. clip: 0.091990\n",
      "Iteration 2671: Policy loss: 0.007149. Value loss: 0.021854. Entropy: 0.879164.\n",
      "Iteration 2672: Policy loss: -0.013989. Value loss: 0.015623. Entropy: 0.875085.\n",
      "Iteration 2673: Policy loss: -0.024448. Value loss: 0.013447. Entropy: 0.879422.\n",
      "episode: 2223   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 662     evaluation reward: 7.03\n",
      "episode: 2224   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 335     evaluation reward: 7.0\n",
      "Training network. lr: 0.000230. clip: 0.091981\n",
      "Iteration 2674: Policy loss: 0.009933. Value loss: 0.030059. Entropy: 0.762584.\n",
      "Iteration 2675: Policy loss: -0.011298. Value loss: 0.021475. Entropy: 0.757720.\n",
      "Iteration 2676: Policy loss: -0.022555. Value loss: 0.017710. Entropy: 0.767001.\n",
      "episode: 2225   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 688     evaluation reward: 7.05\n",
      "episode: 2226   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 556     evaluation reward: 7.05\n",
      "Training network. lr: 0.000230. clip: 0.091972\n",
      "Iteration 2677: Policy loss: 0.007752. Value loss: 0.018555. Entropy: 0.704237.\n",
      "Iteration 2678: Policy loss: -0.011608. Value loss: 0.014310. Entropy: 0.700527.\n",
      "Iteration 2679: Policy loss: -0.022698. Value loss: 0.011859. Entropy: 0.703674.\n",
      "episode: 2227   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 559     evaluation reward: 7.1\n",
      "episode: 2228   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 696     evaluation reward: 7.11\n",
      "Training network. lr: 0.000230. clip: 0.091963\n",
      "Iteration 2680: Policy loss: 0.007433. Value loss: 0.021497. Entropy: 0.816862.\n",
      "Iteration 2681: Policy loss: -0.011883. Value loss: 0.017007. Entropy: 0.815830.\n",
      "Iteration 2682: Policy loss: -0.023360. Value loss: 0.013638. Entropy: 0.810502.\n",
      "episode: 2229   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 716     evaluation reward: 7.19\n",
      "Training network. lr: 0.000230. clip: 0.091954\n",
      "Iteration 2683: Policy loss: 0.003500. Value loss: 0.045579. Entropy: 0.757406.\n",
      "Iteration 2684: Policy loss: -0.004366. Value loss: 0.034013. Entropy: 0.756740.\n",
      "Iteration 2685: Policy loss: -0.017984. Value loss: 0.029019. Entropy: 0.753345.\n",
      "episode: 2230   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 695     evaluation reward: 7.18\n",
      "episode: 2231   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 594     evaluation reward: 7.17\n",
      "Training network. lr: 0.000230. clip: 0.091945\n",
      "Iteration 2686: Policy loss: 0.007686. Value loss: 0.020085. Entropy: 0.792280.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2687: Policy loss: -0.014155. Value loss: 0.015308. Entropy: 0.792536.\n",
      "Iteration 2688: Policy loss: -0.023913. Value loss: 0.012687. Entropy: 0.792080.\n",
      "episode: 2232   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 582     evaluation reward: 7.17\n",
      "Training network. lr: 0.000230. clip: 0.091936\n",
      "Iteration 2689: Policy loss: 0.011046. Value loss: 0.015878. Entropy: 0.738629.\n",
      "Iteration 2690: Policy loss: -0.011525. Value loss: 0.013809. Entropy: 0.723001.\n",
      "Iteration 2691: Policy loss: -0.021896. Value loss: 0.011887. Entropy: 0.714976.\n",
      "episode: 2233   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 669     evaluation reward: 7.19\n",
      "episode: 2234   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 431     evaluation reward: 7.16\n",
      "episode: 2235   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 444     evaluation reward: 7.11\n",
      "Training network. lr: 0.000230. clip: 0.091927\n",
      "Iteration 2692: Policy loss: 0.008286. Value loss: 0.028976. Entropy: 0.718262.\n",
      "Iteration 2693: Policy loss: -0.008169. Value loss: 0.020282. Entropy: 0.715468.\n",
      "Iteration 2694: Policy loss: -0.022909. Value loss: 0.017471. Entropy: 0.713302.\n",
      "episode: 2236   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 624     evaluation reward: 7.14\n",
      "episode: 2237   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 456     evaluation reward: 7.14\n",
      "Training network. lr: 0.000230. clip: 0.091918\n",
      "Iteration 2695: Policy loss: 0.009245. Value loss: 0.018927. Entropy: 0.728185.\n",
      "Iteration 2696: Policy loss: -0.012313. Value loss: 0.015982. Entropy: 0.739745.\n",
      "Iteration 2697: Policy loss: -0.021798. Value loss: 0.013242. Entropy: 0.734183.\n",
      "episode: 2238   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 440     evaluation reward: 7.14\n",
      "episode: 2239   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 385     evaluation reward: 7.13\n",
      "Training network. lr: 0.000230. clip: 0.091909\n",
      "Iteration 2698: Policy loss: 0.006358. Value loss: 0.017264. Entropy: 0.739090.\n",
      "Iteration 2699: Policy loss: -0.012153. Value loss: 0.014834. Entropy: 0.734679.\n",
      "Iteration 2700: Policy loss: -0.023051. Value loss: 0.012789. Entropy: 0.726466.\n",
      "episode: 2240   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 562     evaluation reward: 7.15\n",
      "Training network. lr: 0.000230. clip: 0.091900\n",
      "Iteration 2701: Policy loss: 0.014088. Value loss: 0.016786. Entropy: 0.690271.\n",
      "Iteration 2702: Policy loss: -0.011396. Value loss: 0.013270. Entropy: 0.690982.\n",
      "Iteration 2703: Policy loss: -0.022106. Value loss: 0.011645. Entropy: 0.698546.\n",
      "episode: 2241   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 740     evaluation reward: 7.18\n",
      "episode: 2242   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 656     evaluation reward: 7.19\n",
      "Training network. lr: 0.000230. clip: 0.091891\n",
      "Iteration 2704: Policy loss: 0.008694. Value loss: 0.041551. Entropy: 0.728817.\n",
      "Iteration 2705: Policy loss: -0.009685. Value loss: 0.033380. Entropy: 0.739865.\n",
      "Iteration 2706: Policy loss: -0.017780. Value loss: 0.028524. Entropy: 0.732750.\n",
      "episode: 2243   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 452     evaluation reward: 7.17\n",
      "episode: 2244   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 440     evaluation reward: 7.17\n",
      "Training network. lr: 0.000230. clip: 0.091882\n",
      "Iteration 2707: Policy loss: 0.012401. Value loss: 0.018166. Entropy: 0.682154.\n",
      "Iteration 2708: Policy loss: -0.005540. Value loss: 0.013441. Entropy: 0.682994.\n",
      "Iteration 2709: Policy loss: -0.022130. Value loss: 0.011806. Entropy: 0.675494.\n",
      "episode: 2245   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 545     evaluation reward: 7.19\n",
      "episode: 2246   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 538     evaluation reward: 7.21\n",
      "Training network. lr: 0.000230. clip: 0.091873\n",
      "Iteration 2710: Policy loss: 0.008184. Value loss: 0.025945. Entropy: 0.686340.\n",
      "Iteration 2711: Policy loss: -0.013110. Value loss: 0.018948. Entropy: 0.682526.\n",
      "Iteration 2712: Policy loss: -0.024784. Value loss: 0.014683. Entropy: 0.681536.\n",
      "episode: 2247   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 690     evaluation reward: 7.3\n",
      "episode: 2248   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 544     evaluation reward: 7.35\n",
      "Training network. lr: 0.000230. clip: 0.091864\n",
      "Iteration 2713: Policy loss: 0.012029. Value loss: 0.035379. Entropy: 0.856086.\n",
      "Iteration 2714: Policy loss: -0.009018. Value loss: 0.027559. Entropy: 0.848667.\n",
      "Iteration 2715: Policy loss: -0.019283. Value loss: 0.024061. Entropy: 0.846244.\n",
      "episode: 2249   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 402     evaluation reward: 7.34\n",
      "Training network. lr: 0.000230. clip: 0.091855\n",
      "Iteration 2716: Policy loss: 0.015996. Value loss: 0.123308. Entropy: 0.750281.\n",
      "Iteration 2717: Policy loss: 0.000675. Value loss: 0.050111. Entropy: 0.758636.\n",
      "Iteration 2718: Policy loss: -0.010752. Value loss: 0.035298. Entropy: 0.755198.\n",
      "episode: 2250   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 789     evaluation reward: 7.43\n",
      "episode: 2251   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 760     evaluation reward: 7.48\n",
      "Training network. lr: 0.000230. clip: 0.091846\n",
      "Iteration 2719: Policy loss: 0.010789. Value loss: 0.018192. Entropy: 0.686509.\n",
      "Iteration 2720: Policy loss: -0.010836. Value loss: 0.014003. Entropy: 0.692956.\n",
      "Iteration 2721: Policy loss: -0.020240. Value loss: 0.012052. Entropy: 0.689275.\n",
      "episode: 2252   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 517     evaluation reward: 7.49\n",
      "episode: 2253   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 545     evaluation reward: 7.57\n",
      "Training network. lr: 0.000230. clip: 0.091837\n",
      "Iteration 2722: Policy loss: 0.004368. Value loss: 0.044694. Entropy: 0.709240.\n",
      "Iteration 2723: Policy loss: -0.010167. Value loss: 0.036954. Entropy: 0.709767.\n",
      "Iteration 2724: Policy loss: -0.012422. Value loss: 0.031491. Entropy: 0.723113.\n",
      "episode: 2254   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 654     evaluation reward: 7.59\n",
      "Training network. lr: 0.000230. clip: 0.091828\n",
      "Iteration 2725: Policy loss: 0.006250. Value loss: 0.018186. Entropy: 0.765489.\n",
      "Iteration 2726: Policy loss: -0.009321. Value loss: 0.014089. Entropy: 0.751731.\n",
      "Iteration 2727: Policy loss: -0.022706. Value loss: 0.012977. Entropy: 0.745521.\n",
      "episode: 2255   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 599     evaluation reward: 7.56\n",
      "episode: 2256   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 658     evaluation reward: 7.54\n",
      "Training network. lr: 0.000230. clip: 0.091819\n",
      "Iteration 2728: Policy loss: 0.010026. Value loss: 0.020628. Entropy: 0.824318.\n",
      "Iteration 2729: Policy loss: -0.008034. Value loss: 0.015673. Entropy: 0.830140.\n",
      "Iteration 2730: Policy loss: -0.020775. Value loss: 0.013243. Entropy: 0.820584.\n",
      "episode: 2257   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 551     evaluation reward: 7.58\n",
      "episode: 2258   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 611     evaluation reward: 7.59\n",
      "Training network. lr: 0.000230. clip: 0.091810\n",
      "Iteration 2731: Policy loss: 0.003771. Value loss: 0.025335. Entropy: 0.704579.\n",
      "Iteration 2732: Policy loss: -0.009523. Value loss: 0.018857. Entropy: 0.689033.\n",
      "Iteration 2733: Policy loss: -0.019113. Value loss: 0.017340. Entropy: 0.695740.\n",
      "episode: 2259   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 522     evaluation reward: 7.6\n",
      "episode: 2260   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 715     evaluation reward: 7.66\n",
      "Training network. lr: 0.000230. clip: 0.091801\n",
      "Iteration 2734: Policy loss: 0.009613. Value loss: 0.025350. Entropy: 0.798871.\n",
      "Iteration 2735: Policy loss: -0.011872. Value loss: 0.017421. Entropy: 0.794767.\n",
      "Iteration 2736: Policy loss: -0.020672. Value loss: 0.014909. Entropy: 0.792451.\n",
      "episode: 2261   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 503     evaluation reward: 7.65\n",
      "Training network. lr: 0.000229. clip: 0.091792\n",
      "Iteration 2737: Policy loss: 0.009599. Value loss: 0.027950. Entropy: 0.742860.\n",
      "Iteration 2738: Policy loss: -0.010932. Value loss: 0.020541. Entropy: 0.751294.\n",
      "Iteration 2739: Policy loss: -0.015853. Value loss: 0.017033. Entropy: 0.751485.\n",
      "episode: 2262   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 567     evaluation reward: 7.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2263   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 343     evaluation reward: 7.68\n",
      "episode: 2264   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 519     evaluation reward: 7.69\n",
      "Training network. lr: 0.000229. clip: 0.091783\n",
      "Iteration 2740: Policy loss: 0.000218. Value loss: 0.024968. Entropy: 0.819675.\n",
      "Iteration 2741: Policy loss: -0.014160. Value loss: 0.020160. Entropy: 0.817583.\n",
      "Iteration 2742: Policy loss: -0.024237. Value loss: 0.016924. Entropy: 0.813703.\n",
      "episode: 2265   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 671     evaluation reward: 7.71\n",
      "Training network. lr: 0.000229. clip: 0.091774\n",
      "Iteration 2743: Policy loss: 0.005535. Value loss: 0.025876. Entropy: 0.806869.\n",
      "Iteration 2744: Policy loss: -0.012500. Value loss: 0.019388. Entropy: 0.808125.\n",
      "Iteration 2745: Policy loss: -0.022451. Value loss: 0.016711. Entropy: 0.800845.\n",
      "episode: 2266   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 519     evaluation reward: 7.74\n",
      "episode: 2267   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 629     evaluation reward: 7.78\n",
      "Training network. lr: 0.000229. clip: 0.091765\n",
      "Iteration 2746: Policy loss: 0.005474. Value loss: 0.017809. Entropy: 0.730036.\n",
      "Iteration 2747: Policy loss: -0.013634. Value loss: 0.014161. Entropy: 0.722115.\n",
      "Iteration 2748: Policy loss: -0.020741. Value loss: 0.012726. Entropy: 0.720153.\n",
      "episode: 2268   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 469     evaluation reward: 7.79\n",
      "episode: 2269   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 574     evaluation reward: 7.83\n",
      "Training network. lr: 0.000229. clip: 0.091756\n",
      "Iteration 2749: Policy loss: 0.010450. Value loss: 0.045877. Entropy: 0.732628.\n",
      "Iteration 2750: Policy loss: -0.007655. Value loss: 0.038681. Entropy: 0.721240.\n",
      "Iteration 2751: Policy loss: -0.015925. Value loss: 0.033306. Entropy: 0.721654.\n",
      "episode: 2270   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 541     evaluation reward: 7.81\n",
      "episode: 2271   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 585     evaluation reward: 7.85\n",
      "Training network. lr: 0.000229. clip: 0.091747\n",
      "Iteration 2752: Policy loss: 0.006291. Value loss: 0.018842. Entropy: 0.754699.\n",
      "Iteration 2753: Policy loss: -0.009306. Value loss: 0.014272. Entropy: 0.748424.\n",
      "Iteration 2754: Policy loss: -0.023675. Value loss: 0.012124. Entropy: 0.748309.\n",
      "episode: 2272   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 630     evaluation reward: 7.87\n",
      "episode: 2273   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 356     evaluation reward: 7.86\n",
      "Training network. lr: 0.000229. clip: 0.091738\n",
      "Iteration 2755: Policy loss: 0.008525. Value loss: 0.022907. Entropy: 0.791384.\n",
      "Iteration 2756: Policy loss: -0.008843. Value loss: 0.016627. Entropy: 0.788954.\n",
      "Iteration 2757: Policy loss: -0.022865. Value loss: 0.014033. Entropy: 0.777290.\n",
      "episode: 2274   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 695     evaluation reward: 7.9\n",
      "episode: 2275   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 558     evaluation reward: 7.85\n",
      "Training network. lr: 0.000229. clip: 0.091729\n",
      "Iteration 2758: Policy loss: 0.008596. Value loss: 0.023321. Entropy: 0.800114.\n",
      "Iteration 2759: Policy loss: -0.010746. Value loss: 0.016055. Entropy: 0.813444.\n",
      "Iteration 2760: Policy loss: -0.025412. Value loss: 0.013827. Entropy: 0.794038.\n",
      "episode: 2276   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 473     evaluation reward: 7.83\n",
      "episode: 2277   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 439     evaluation reward: 7.81\n",
      "Training network. lr: 0.000229. clip: 0.091720\n",
      "Iteration 2761: Policy loss: 0.012981. Value loss: 0.022385. Entropy: 0.783211.\n",
      "Iteration 2762: Policy loss: -0.008372. Value loss: 0.017238. Entropy: 0.776092.\n",
      "Iteration 2763: Policy loss: -0.019964. Value loss: 0.014340. Entropy: 0.782423.\n",
      "episode: 2278   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 463     evaluation reward: 7.79\n",
      "episode: 2279   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 333     evaluation reward: 7.74\n",
      "Training network. lr: 0.000229. clip: 0.091711\n",
      "Iteration 2764: Policy loss: 0.007251. Value loss: 0.023988. Entropy: 0.820746.\n",
      "Iteration 2765: Policy loss: -0.012920. Value loss: 0.018498. Entropy: 0.813102.\n",
      "Iteration 2766: Policy loss: -0.023689. Value loss: 0.015605. Entropy: 0.810726.\n",
      "episode: 2280   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 674     evaluation reward: 7.83\n",
      "episode: 2281   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 475     evaluation reward: 7.83\n",
      "Training network. lr: 0.000229. clip: 0.091702\n",
      "Iteration 2767: Policy loss: 0.008855. Value loss: 0.052266. Entropy: 0.772931.\n",
      "Iteration 2768: Policy loss: -0.011411. Value loss: 0.038270. Entropy: 0.758561.\n",
      "Iteration 2769: Policy loss: -0.018242. Value loss: 0.030747. Entropy: 0.765737.\n",
      "episode: 2282   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 620     evaluation reward: 7.89\n",
      "episode: 2283   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 609     evaluation reward: 7.92\n",
      "Training network. lr: 0.000229. clip: 0.091693\n",
      "Iteration 2770: Policy loss: 0.006404. Value loss: 0.047256. Entropy: 0.811299.\n",
      "Iteration 2771: Policy loss: -0.013587. Value loss: 0.036604. Entropy: 0.807951.\n",
      "Iteration 2772: Policy loss: -0.020310. Value loss: 0.029543. Entropy: 0.804300.\n",
      "episode: 2284   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 428     evaluation reward: 7.95\n",
      "Training network. lr: 0.000229. clip: 0.091684\n",
      "Iteration 2773: Policy loss: 0.006457. Value loss: 0.027721. Entropy: 0.696193.\n",
      "Iteration 2774: Policy loss: -0.012271. Value loss: 0.018667. Entropy: 0.705098.\n",
      "Iteration 2775: Policy loss: -0.020607. Value loss: 0.014034. Entropy: 0.704769.\n",
      "episode: 2285   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 822     evaluation reward: 8.03\n",
      "episode: 2286   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 599     evaluation reward: 8.07\n",
      "Training network. lr: 0.000229. clip: 0.091675\n",
      "Iteration 2776: Policy loss: 0.003913. Value loss: 0.020277. Entropy: 0.761459.\n",
      "Iteration 2777: Policy loss: -0.012963. Value loss: 0.015396. Entropy: 0.760514.\n",
      "Iteration 2778: Policy loss: -0.018410. Value loss: 0.013109. Entropy: 0.735847.\n",
      "episode: 2287   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 583     evaluation reward: 8.09\n",
      "Training network. lr: 0.000229. clip: 0.091666\n",
      "Iteration 2779: Policy loss: 0.009782. Value loss: 0.019569. Entropy: 0.710122.\n",
      "Iteration 2780: Policy loss: -0.010938. Value loss: 0.015120. Entropy: 0.710659.\n",
      "Iteration 2781: Policy loss: -0.017902. Value loss: 0.012769. Entropy: 0.715309.\n",
      "episode: 2288   score: 16.0   memory length: 1024   epsilon: 1.0    steps: 794     evaluation reward: 8.14\n",
      "now time :  2018-12-26 13:29:22.503662\n",
      "episode: 2289   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 757     evaluation reward: 8.15\n",
      "Training network. lr: 0.000229. clip: 0.091657\n",
      "Iteration 2782: Policy loss: 0.012822. Value loss: 0.057145. Entropy: 0.730376.\n",
      "Iteration 2783: Policy loss: -0.005701. Value loss: 0.046000. Entropy: 0.737341.\n",
      "Iteration 2784: Policy loss: -0.015211. Value loss: 0.038221. Entropy: 0.738107.\n",
      "episode: 2290   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 289     evaluation reward: 8.1\n",
      "episode: 2291   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 557     evaluation reward: 8.09\n",
      "Training network. lr: 0.000229. clip: 0.091648\n",
      "Iteration 2785: Policy loss: 0.009146. Value loss: 0.045860. Entropy: 0.689130.\n",
      "Iteration 2786: Policy loss: -0.013947. Value loss: 0.034295. Entropy: 0.687106.\n",
      "Iteration 2787: Policy loss: -0.024984. Value loss: 0.028912. Entropy: 0.680230.\n",
      "episode: 2292   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 704     evaluation reward: 8.13\n",
      "episode: 2293   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 285     evaluation reward: 8.07\n",
      "Training network. lr: 0.000229. clip: 0.091639\n",
      "Iteration 2788: Policy loss: 0.005494. Value loss: 0.032255. Entropy: 0.738853.\n",
      "Iteration 2789: Policy loss: -0.011151. Value loss: 0.025685. Entropy: 0.731817.\n",
      "Iteration 2790: Policy loss: -0.023312. Value loss: 0.020981. Entropy: 0.722712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2294   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 623     evaluation reward: 8.13\n",
      "episode: 2295   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 674     evaluation reward: 8.17\n",
      "Training network. lr: 0.000229. clip: 0.091630\n",
      "Iteration 2791: Policy loss: 0.008607. Value loss: 0.044044. Entropy: 0.663202.\n",
      "Iteration 2792: Policy loss: -0.005137. Value loss: 0.034658. Entropy: 0.671978.\n",
      "Iteration 2793: Policy loss: -0.015709. Value loss: 0.029697. Entropy: 0.675227.\n",
      "episode: 2296   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 456     evaluation reward: 8.15\n",
      "episode: 2297   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 633     evaluation reward: 8.12\n",
      "Training network. lr: 0.000229. clip: 0.091621\n",
      "Iteration 2794: Policy loss: 0.011210. Value loss: 0.024278. Entropy: 0.729630.\n",
      "Iteration 2795: Policy loss: -0.008967. Value loss: 0.016139. Entropy: 0.714013.\n",
      "Iteration 2796: Policy loss: -0.023848. Value loss: 0.013891. Entropy: 0.712996.\n",
      "episode: 2298   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 795     evaluation reward: 8.17\n",
      "Training network. lr: 0.000229. clip: 0.091612\n",
      "Iteration 2797: Policy loss: 0.005207. Value loss: 0.019921. Entropy: 0.713684.\n",
      "Iteration 2798: Policy loss: -0.006459. Value loss: 0.015039. Entropy: 0.718544.\n",
      "Iteration 2799: Policy loss: -0.022683. Value loss: 0.012971. Entropy: 0.716089.\n",
      "episode: 2299   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 596     evaluation reward: 8.19\n",
      "episode: 2300   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 632     evaluation reward: 8.22\n",
      "Training network. lr: 0.000229. clip: 0.091603\n",
      "Iteration 2800: Policy loss: 0.004909. Value loss: 0.025157. Entropy: 0.692476.\n",
      "Iteration 2801: Policy loss: -0.008059. Value loss: 0.017217. Entropy: 0.697618.\n",
      "Iteration 2802: Policy loss: -0.020697. Value loss: 0.015704. Entropy: 0.695342.\n",
      "episode: 2301   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 705     evaluation reward: 8.24\n",
      "Training network. lr: 0.000229. clip: 0.091594\n",
      "Iteration 2803: Policy loss: 0.009490. Value loss: 0.021697. Entropy: 0.706767.\n",
      "Iteration 2804: Policy loss: -0.013452. Value loss: 0.016246. Entropy: 0.706213.\n",
      "Iteration 2805: Policy loss: -0.020377. Value loss: 0.013549. Entropy: 0.705825.\n",
      "episode: 2302   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 490     evaluation reward: 8.23\n",
      "episode: 2303   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 384     evaluation reward: 8.2\n",
      "episode: 2304   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 462     evaluation reward: 8.19\n",
      "Training network. lr: 0.000229. clip: 0.091585\n",
      "Iteration 2806: Policy loss: 0.008831. Value loss: 0.036198. Entropy: 0.790015.\n",
      "Iteration 2807: Policy loss: -0.010072. Value loss: 0.026288. Entropy: 0.779470.\n",
      "Iteration 2808: Policy loss: -0.021897. Value loss: 0.022184. Entropy: 0.774777.\n",
      "episode: 2305   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 621     evaluation reward: 8.21\n",
      "Training network. lr: 0.000229. clip: 0.091576\n",
      "Iteration 2809: Policy loss: 0.002181. Value loss: 0.028853. Entropy: 0.588520.\n",
      "Iteration 2810: Policy loss: -0.012851. Value loss: 0.023582. Entropy: 0.597818.\n",
      "Iteration 2811: Policy loss: -0.018128. Value loss: 0.019217. Entropy: 0.609173.\n",
      "episode: 2306   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 478     evaluation reward: 8.19\n",
      "episode: 2307   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 500     evaluation reward: 8.13\n",
      "Training network. lr: 0.000229. clip: 0.091567\n",
      "Iteration 2812: Policy loss: 0.008232. Value loss: 0.022428. Entropy: 0.654914.\n",
      "Iteration 2813: Policy loss: -0.011096. Value loss: 0.017635. Entropy: 0.650359.\n",
      "Iteration 2814: Policy loss: -0.023721. Value loss: 0.014851. Entropy: 0.664242.\n",
      "episode: 2308   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 611     evaluation reward: 8.13\n",
      "episode: 2309   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 400     evaluation reward: 8.09\n",
      "episode: 2310   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 434     evaluation reward: 8.03\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2815: Policy loss: 0.011638. Value loss: 0.031461. Entropy: 0.714642.\n",
      "Iteration 2816: Policy loss: -0.010038. Value loss: 0.024127. Entropy: 0.719814.\n",
      "Iteration 2817: Policy loss: -0.022433. Value loss: 0.020578. Entropy: 0.709455.\n",
      "episode: 2311   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 630     evaluation reward: 8.09\n",
      "Training network. lr: 0.000229. clip: 0.091549\n",
      "Iteration 2818: Policy loss: 0.004512. Value loss: 0.017206. Entropy: 0.771578.\n",
      "Iteration 2819: Policy loss: -0.014384. Value loss: 0.013577. Entropy: 0.774483.\n",
      "Iteration 2820: Policy loss: -0.021585. Value loss: 0.011366. Entropy: 0.768180.\n",
      "episode: 2312   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 520     evaluation reward: 8.07\n",
      "episode: 2313   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 745     evaluation reward: 8.12\n",
      "Training network. lr: 0.000229. clip: 0.091540\n",
      "Iteration 2821: Policy loss: 0.008831. Value loss: 0.053902. Entropy: 0.701226.\n",
      "Iteration 2822: Policy loss: -0.005346. Value loss: 0.039435. Entropy: 0.697743.\n",
      "Iteration 2823: Policy loss: -0.015263. Value loss: 0.029516. Entropy: 0.699482.\n",
      "episode: 2314   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 643     evaluation reward: 8.13\n",
      "episode: 2315   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 529     evaluation reward: 8.13\n",
      "Training network. lr: 0.000229. clip: 0.091531\n",
      "Iteration 2824: Policy loss: 0.003965. Value loss: 0.020077. Entropy: 0.659575.\n",
      "Iteration 2825: Policy loss: -0.013100. Value loss: 0.014368. Entropy: 0.661475.\n",
      "Iteration 2826: Policy loss: -0.022454. Value loss: 0.012440. Entropy: 0.661967.\n",
      "episode: 2316   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 688     evaluation reward: 8.13\n",
      "Training network. lr: 0.000229. clip: 0.091522\n",
      "Iteration 2827: Policy loss: 0.007044. Value loss: 0.017595. Entropy: 0.685703.\n",
      "Iteration 2828: Policy loss: -0.011274. Value loss: 0.014208. Entropy: 0.684999.\n",
      "Iteration 2829: Policy loss: -0.020588. Value loss: 0.012949. Entropy: 0.693187.\n",
      "episode: 2317   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 595     evaluation reward: 8.15\n",
      "episode: 2318   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 8.07\n",
      "Training network. lr: 0.000229. clip: 0.091513\n",
      "Iteration 2830: Policy loss: 0.004678. Value loss: 0.027871. Entropy: 0.715096.\n",
      "Iteration 2831: Policy loss: -0.015100. Value loss: 0.020889. Entropy: 0.713665.\n",
      "Iteration 2832: Policy loss: -0.022783. Value loss: 0.018797. Entropy: 0.712963.\n",
      "episode: 2319   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 671     evaluation reward: 8.1\n",
      "episode: 2320   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 727     evaluation reward: 8.13\n",
      "Training network. lr: 0.000229. clip: 0.091504\n",
      "Iteration 2833: Policy loss: 0.003843. Value loss: 0.019899. Entropy: 0.708078.\n",
      "Iteration 2834: Policy loss: -0.017951. Value loss: 0.014817. Entropy: 0.711961.\n",
      "Iteration 2835: Policy loss: -0.024203. Value loss: 0.012234. Entropy: 0.720016.\n",
      "episode: 2321   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 8.11\n",
      "episode: 2322   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 629     evaluation reward: 8.12\n",
      "Training network. lr: 0.000229. clip: 0.091495\n",
      "Iteration 2836: Policy loss: 0.009938. Value loss: 0.025149. Entropy: 0.820723.\n",
      "Iteration 2837: Policy loss: -0.013661. Value loss: 0.019628. Entropy: 0.792504.\n",
      "Iteration 2838: Policy loss: -0.025381. Value loss: 0.015999. Entropy: 0.790289.\n",
      "episode: 2323   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 625     evaluation reward: 8.11\n",
      "Training network. lr: 0.000229. clip: 0.091486\n",
      "Iteration 2839: Policy loss: 0.012901. Value loss: 0.020621. Entropy: 0.771515.\n",
      "Iteration 2840: Policy loss: -0.011672. Value loss: 0.016240. Entropy: 0.763397.\n",
      "Iteration 2841: Policy loss: -0.020440. Value loss: 0.014035. Entropy: 0.764825.\n",
      "episode: 2324   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 573     evaluation reward: 8.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2325   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 486     evaluation reward: 8.13\n",
      "Training network. lr: 0.000229. clip: 0.091477\n",
      "Iteration 2842: Policy loss: 0.009021. Value loss: 0.017590. Entropy: 0.665597.\n",
      "Iteration 2843: Policy loss: -0.008074. Value loss: 0.013126. Entropy: 0.663866.\n",
      "Iteration 2844: Policy loss: -0.019128. Value loss: 0.011284. Entropy: 0.666802.\n",
      "episode: 2326   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 672     evaluation reward: 8.15\n",
      "episode: 2327   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 500     evaluation reward: 8.13\n",
      "Training network. lr: 0.000229. clip: 0.091468\n",
      "Iteration 2845: Policy loss: 0.008285. Value loss: 0.025468. Entropy: 0.759405.\n",
      "Iteration 2846: Policy loss: -0.008903. Value loss: 0.018480. Entropy: 0.762244.\n",
      "Iteration 2847: Policy loss: -0.019766. Value loss: 0.015573. Entropy: 0.755488.\n",
      "episode: 2328   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 646     evaluation reward: 8.13\n",
      "episode: 2329   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 687     evaluation reward: 8.08\n",
      "Training network. lr: 0.000229. clip: 0.091459\n",
      "Iteration 2848: Policy loss: 0.007250. Value loss: 0.016046. Entropy: 0.719532.\n",
      "Iteration 2849: Policy loss: -0.009949. Value loss: 0.011688. Entropy: 0.718953.\n",
      "Iteration 2850: Policy loss: -0.025807. Value loss: 0.009832. Entropy: 0.709840.\n",
      "episode: 2330   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 399     evaluation reward: 8.04\n",
      "episode: 2331   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 634     evaluation reward: 8.07\n",
      "Training network. lr: 0.000229. clip: 0.091450\n",
      "Iteration 2851: Policy loss: 0.009263. Value loss: 0.021934. Entropy: 0.766477.\n",
      "Iteration 2852: Policy loss: -0.013415. Value loss: 0.015948. Entropy: 0.769097.\n",
      "Iteration 2853: Policy loss: -0.026289. Value loss: 0.012409. Entropy: 0.760431.\n",
      "episode: 2332   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 702     evaluation reward: 8.09\n",
      "Training network. lr: 0.000229. clip: 0.091441\n",
      "Iteration 2854: Policy loss: 0.013905. Value loss: 0.020447. Entropy: 0.752173.\n",
      "Iteration 2855: Policy loss: -0.014497. Value loss: 0.017764. Entropy: 0.750521.\n",
      "Iteration 2856: Policy loss: -0.020109. Value loss: 0.014024. Entropy: 0.747392.\n",
      "episode: 2333   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 738     evaluation reward: 8.11\n",
      "Training network. lr: 0.000229. clip: 0.091432\n",
      "Iteration 2857: Policy loss: 0.012732. Value loss: 0.047739. Entropy: 0.665021.\n",
      "Iteration 2858: Policy loss: -0.014428. Value loss: 0.039401. Entropy: 0.646006.\n",
      "Iteration 2859: Policy loss: -0.021186. Value loss: 0.033865. Entropy: 0.650380.\n",
      "episode: 2334   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 648     evaluation reward: 8.18\n",
      "episode: 2335   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 533     evaluation reward: 8.19\n",
      "episode: 2336   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 381     evaluation reward: 8.14\n",
      "Training network. lr: 0.000229. clip: 0.091423\n",
      "Iteration 2860: Policy loss: 0.007007. Value loss: 0.020822. Entropy: 0.752286.\n",
      "Iteration 2861: Policy loss: -0.012780. Value loss: 0.014728. Entropy: 0.753191.\n",
      "Iteration 2862: Policy loss: -0.022585. Value loss: 0.011718. Entropy: 0.753831.\n",
      "episode: 2337   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 508     evaluation reward: 8.14\n",
      "episode: 2338   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 362     evaluation reward: 8.13\n",
      "Training network. lr: 0.000229. clip: 0.091414\n",
      "Iteration 2863: Policy loss: 0.006428. Value loss: 0.018606. Entropy: 0.790991.\n",
      "Iteration 2864: Policy loss: -0.011689. Value loss: 0.014702. Entropy: 0.799158.\n",
      "Iteration 2865: Policy loss: -0.023692. Value loss: 0.013181. Entropy: 0.803359.\n",
      "episode: 2339   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 646     evaluation reward: 8.17\n",
      "episode: 2340   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 577     evaluation reward: 8.17\n",
      "Training network. lr: 0.000229. clip: 0.091405\n",
      "Iteration 2866: Policy loss: 0.005631. Value loss: 0.019529. Entropy: 0.718685.\n",
      "Iteration 2867: Policy loss: -0.012186. Value loss: 0.013182. Entropy: 0.710510.\n",
      "Iteration 2868: Policy loss: -0.023949. Value loss: 0.011845. Entropy: 0.703128.\n",
      "episode: 2341   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 595     evaluation reward: 8.15\n",
      "Training network. lr: 0.000228. clip: 0.091396\n",
      "Iteration 2869: Policy loss: 0.004495. Value loss: 0.016634. Entropy: 0.726553.\n",
      "Iteration 2870: Policy loss: -0.011167. Value loss: 0.013868. Entropy: 0.715415.\n",
      "Iteration 2871: Policy loss: -0.017505. Value loss: 0.012241. Entropy: 0.717999.\n",
      "episode: 2342   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 582     evaluation reward: 8.1\n",
      "episode: 2343   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 471     evaluation reward: 8.1\n",
      "Training network. lr: 0.000228. clip: 0.091387\n",
      "Iteration 2872: Policy loss: 0.006248. Value loss: 0.033089. Entropy: 0.702008.\n",
      "Iteration 2873: Policy loss: -0.014797. Value loss: 0.023917. Entropy: 0.698302.\n",
      "Iteration 2874: Policy loss: -0.023351. Value loss: 0.018575. Entropy: 0.696236.\n",
      "episode: 2344   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 667     evaluation reward: 8.16\n",
      "episode: 2345   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 735     evaluation reward: 8.18\n",
      "Training network. lr: 0.000228. clip: 0.091378\n",
      "Iteration 2875: Policy loss: 0.007919. Value loss: 0.049182. Entropy: 0.827122.\n",
      "Iteration 2876: Policy loss: -0.003251. Value loss: 0.041176. Entropy: 0.821958.\n",
      "Iteration 2877: Policy loss: -0.013672. Value loss: 0.035925. Entropy: 0.825500.\n",
      "episode: 2346   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 744     evaluation reward: 8.24\n",
      "Training network. lr: 0.000228. clip: 0.091369\n",
      "Iteration 2878: Policy loss: 0.008222. Value loss: 0.045598. Entropy: 0.841180.\n",
      "Iteration 2879: Policy loss: -0.009391. Value loss: 0.038340. Entropy: 0.835774.\n",
      "Iteration 2880: Policy loss: -0.018937. Value loss: 0.032023. Entropy: 0.830653.\n",
      "episode: 2347   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 444     evaluation reward: 8.16\n",
      "episode: 2348   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 360     evaluation reward: 8.13\n",
      "episode: 2349   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 500     evaluation reward: 8.15\n",
      "Training network. lr: 0.000228. clip: 0.091360\n",
      "Iteration 2881: Policy loss: 0.007159. Value loss: 0.034143. Entropy: 0.802495.\n",
      "Iteration 2882: Policy loss: -0.008453. Value loss: 0.026434. Entropy: 0.800184.\n",
      "Iteration 2883: Policy loss: -0.018043. Value loss: 0.022382. Entropy: 0.800514.\n",
      "episode: 2350   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 454     evaluation reward: 8.06\n",
      "Training network. lr: 0.000228. clip: 0.091351\n",
      "Iteration 2884: Policy loss: 0.010633. Value loss: 0.024660. Entropy: 0.767364.\n",
      "Iteration 2885: Policy loss: -0.012416. Value loss: 0.017879. Entropy: 0.748949.\n",
      "Iteration 2886: Policy loss: -0.022722. Value loss: 0.014974. Entropy: 0.749470.\n",
      "episode: 2351   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 675     evaluation reward: 8.04\n",
      "episode: 2352   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 690     evaluation reward: 8.07\n",
      "Training network. lr: 0.000228. clip: 0.091342\n",
      "Iteration 2887: Policy loss: 0.009388. Value loss: 0.023734. Entropy: 0.779928.\n",
      "Iteration 2888: Policy loss: -0.009841. Value loss: 0.018065. Entropy: 0.773400.\n",
      "Iteration 2889: Policy loss: -0.025844. Value loss: 0.014755. Entropy: 0.758018.\n",
      "episode: 2353   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 590     evaluation reward: 8.04\n",
      "episode: 2354   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 538     evaluation reward: 8.01\n",
      "Training network. lr: 0.000228. clip: 0.091333\n",
      "Iteration 2890: Policy loss: 0.006363. Value loss: 0.025951. Entropy: 0.766193.\n",
      "Iteration 2891: Policy loss: -0.011085. Value loss: 0.020210. Entropy: 0.763604.\n",
      "Iteration 2892: Policy loss: -0.021684. Value loss: 0.017365. Entropy: 0.760806.\n",
      "episode: 2355   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 547     evaluation reward: 8.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2356   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 488     evaluation reward: 7.98\n",
      "Training network. lr: 0.000228. clip: 0.091324\n",
      "Iteration 2893: Policy loss: 0.003115. Value loss: 0.021417. Entropy: 0.666996.\n",
      "Iteration 2894: Policy loss: -0.012390. Value loss: 0.014229. Entropy: 0.661156.\n",
      "Iteration 2895: Policy loss: -0.023252. Value loss: 0.011759. Entropy: 0.659603.\n",
      "episode: 2357   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 599     evaluation reward: 8.0\n",
      "episode: 2358   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 291     evaluation reward: 7.94\n",
      "Training network. lr: 0.000228. clip: 0.091315\n",
      "Iteration 2896: Policy loss: 0.002067. Value loss: 0.032326. Entropy: 0.771159.\n",
      "Iteration 2897: Policy loss: -0.013551. Value loss: 0.020877. Entropy: 0.779396.\n",
      "Iteration 2898: Policy loss: -0.023190. Value loss: 0.015466. Entropy: 0.779367.\n",
      "episode: 2359   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 461     evaluation reward: 7.93\n",
      "episode: 2360   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 388     evaluation reward: 7.86\n",
      "episode: 2361   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 384     evaluation reward: 7.84\n",
      "Training network. lr: 0.000228. clip: 0.091306\n",
      "Iteration 2899: Policy loss: 0.009085. Value loss: 0.013829. Entropy: 0.768839.\n",
      "Iteration 2900: Policy loss: -0.010637. Value loss: 0.011144. Entropy: 0.757108.\n",
      "Iteration 2901: Policy loss: -0.023865. Value loss: 0.009733. Entropy: 0.756256.\n",
      "episode: 2362   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 581     evaluation reward: 7.84\n",
      "Training network. lr: 0.000228. clip: 0.091297\n",
      "Iteration 2902: Policy loss: 0.011031. Value loss: 0.013985. Entropy: 0.739033.\n",
      "Iteration 2903: Policy loss: -0.008523. Value loss: 0.009988. Entropy: 0.731712.\n",
      "Iteration 2904: Policy loss: -0.023153. Value loss: 0.008015. Entropy: 0.723562.\n",
      "episode: 2363   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 885     evaluation reward: 7.94\n",
      "episode: 2364   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 660     evaluation reward: 7.97\n",
      "Training network. lr: 0.000228. clip: 0.091288\n",
      "Iteration 2905: Policy loss: 0.011820. Value loss: 0.022721. Entropy: 0.672046.\n",
      "Iteration 2906: Policy loss: -0.010186. Value loss: 0.016263. Entropy: 0.673866.\n",
      "Iteration 2907: Policy loss: -0.020943. Value loss: 0.012129. Entropy: 0.681719.\n",
      "episode: 2365   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 632     evaluation reward: 7.97\n",
      "Training network. lr: 0.000228. clip: 0.091279\n",
      "Iteration 2908: Policy loss: 0.001593. Value loss: 0.014976. Entropy: 0.661936.\n",
      "Iteration 2909: Policy loss: -0.008208. Value loss: 0.011000. Entropy: 0.658666.\n",
      "Iteration 2910: Policy loss: -0.020646. Value loss: 0.009138. Entropy: 0.659218.\n",
      "episode: 2366   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 510     evaluation reward: 7.96\n",
      "episode: 2367   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 645     evaluation reward: 7.97\n",
      "Training network. lr: 0.000228. clip: 0.091270\n",
      "Iteration 2911: Policy loss: 0.006987. Value loss: 0.018664. Entropy: 0.706512.\n",
      "Iteration 2912: Policy loss: -0.009941. Value loss: 0.013019. Entropy: 0.707238.\n",
      "Iteration 2913: Policy loss: -0.021926. Value loss: 0.010109. Entropy: 0.698607.\n",
      "episode: 2368   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 490     evaluation reward: 7.98\n",
      "episode: 2369   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 598     evaluation reward: 7.97\n",
      "Training network. lr: 0.000228. clip: 0.091261\n",
      "Iteration 2914: Policy loss: 0.003859. Value loss: 0.015631. Entropy: 0.655992.\n",
      "Iteration 2915: Policy loss: -0.015183. Value loss: 0.011253. Entropy: 0.659911.\n",
      "Iteration 2916: Policy loss: -0.024836. Value loss: 0.010271. Entropy: 0.655254.\n",
      "episode: 2370   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 467     evaluation reward: 7.95\n",
      "episode: 2371   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 510     evaluation reward: 7.95\n",
      "Training network. lr: 0.000228. clip: 0.091252\n",
      "Iteration 2917: Policy loss: 0.008807. Value loss: 0.017731. Entropy: 0.592340.\n",
      "Iteration 2918: Policy loss: -0.005815. Value loss: 0.012916. Entropy: 0.596572.\n",
      "Iteration 2919: Policy loss: -0.017766. Value loss: 0.010424. Entropy: 0.588971.\n",
      "episode: 2372   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 744     evaluation reward: 7.96\n",
      "Training network. lr: 0.000228. clip: 0.091243\n",
      "Iteration 2920: Policy loss: 0.007733. Value loss: 0.018247. Entropy: 0.785435.\n",
      "Iteration 2921: Policy loss: -0.009394. Value loss: 0.013136. Entropy: 0.793063.\n",
      "Iteration 2922: Policy loss: -0.019130. Value loss: 0.010600. Entropy: 0.789160.\n",
      "episode: 2373   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 579     evaluation reward: 8.0\n",
      "episode: 2374   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 667     evaluation reward: 8.03\n",
      "Training network. lr: 0.000228. clip: 0.091234\n",
      "Iteration 2923: Policy loss: 0.015562. Value loss: 0.052297. Entropy: 0.730423.\n",
      "Iteration 2924: Policy loss: -0.007384. Value loss: 0.040102. Entropy: 0.722820.\n",
      "Iteration 2925: Policy loss: -0.017342. Value loss: 0.033485. Entropy: 0.721045.\n",
      "episode: 2375   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 620     evaluation reward: 8.05\n",
      "episode: 2376   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 608     evaluation reward: 8.08\n",
      "Training network. lr: 0.000228. clip: 0.091225\n",
      "Iteration 2926: Policy loss: 0.005090. Value loss: 0.018614. Entropy: 0.733605.\n",
      "Iteration 2927: Policy loss: -0.007130. Value loss: 0.013837. Entropy: 0.723217.\n",
      "Iteration 2928: Policy loss: -0.021847. Value loss: 0.011759. Entropy: 0.730512.\n",
      "episode: 2377   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 505     evaluation reward: 8.09\n",
      "now time :  2018-12-26 13:33:45.678000\n",
      "episode: 2378   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 544     evaluation reward: 8.1\n",
      "Training network. lr: 0.000228. clip: 0.091216\n",
      "Iteration 2929: Policy loss: 0.007629. Value loss: 0.016267. Entropy: 0.688070.\n",
      "Iteration 2930: Policy loss: -0.010867. Value loss: 0.012281. Entropy: 0.694078.\n",
      "Iteration 2931: Policy loss: -0.019777. Value loss: 0.010345. Entropy: 0.689721.\n",
      "episode: 2379   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 571     evaluation reward: 8.13\n",
      "Training network. lr: 0.000228. clip: 0.091207\n",
      "Iteration 2932: Policy loss: 0.008135. Value loss: 0.022528. Entropy: 0.790267.\n",
      "Iteration 2933: Policy loss: -0.008292. Value loss: 0.013630. Entropy: 0.781014.\n",
      "Iteration 2934: Policy loss: -0.021157. Value loss: 0.010421. Entropy: 0.775051.\n",
      "episode: 2380   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 685     evaluation reward: 8.1\n",
      "episode: 2381   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 717     evaluation reward: 8.18\n",
      "Training network. lr: 0.000228. clip: 0.091198\n",
      "Iteration 2935: Policy loss: 0.013176. Value loss: 0.059573. Entropy: 0.782253.\n",
      "Iteration 2936: Policy loss: -0.005819. Value loss: 0.026969. Entropy: 0.783926.\n",
      "Iteration 2937: Policy loss: -0.014723. Value loss: 0.021745. Entropy: 0.782151.\n",
      "episode: 2382   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 432     evaluation reward: 8.13\n",
      "episode: 2383   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 459     evaluation reward: 8.1\n",
      "Training network. lr: 0.000228. clip: 0.091189\n",
      "Iteration 2938: Policy loss: 0.008114. Value loss: 0.020635. Entropy: 0.807296.\n",
      "Iteration 2939: Policy loss: -0.014559. Value loss: 0.014381. Entropy: 0.791992.\n",
      "Iteration 2940: Policy loss: -0.027554. Value loss: 0.012082. Entropy: 0.793808.\n",
      "episode: 2384   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 402     evaluation reward: 8.1\n",
      "episode: 2385   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 371     evaluation reward: 8.0\n",
      "Training network. lr: 0.000228. clip: 0.091180\n",
      "Iteration 2941: Policy loss: 0.007360. Value loss: 0.029159. Entropy: 0.761708.\n",
      "Iteration 2942: Policy loss: -0.011491. Value loss: 0.018353. Entropy: 0.764214.\n",
      "Iteration 2943: Policy loss: -0.021778. Value loss: 0.015307. Entropy: 0.755206.\n",
      "episode: 2386   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 596     evaluation reward: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2387   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 511     evaluation reward: 7.97\n",
      "Training network. lr: 0.000228. clip: 0.091171\n",
      "Iteration 2944: Policy loss: 0.006147. Value loss: 0.018356. Entropy: 0.780235.\n",
      "Iteration 2945: Policy loss: -0.015011. Value loss: 0.013783. Entropy: 0.775316.\n",
      "Iteration 2946: Policy loss: -0.022681. Value loss: 0.011337. Entropy: 0.771997.\n",
      "episode: 2388   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 622     evaluation reward: 7.89\n",
      "episode: 2389   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 496     evaluation reward: 7.84\n",
      "episode: 2390   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 354     evaluation reward: 7.85\n",
      "Training network. lr: 0.000228. clip: 0.091162\n",
      "Iteration 2947: Policy loss: 0.005499. Value loss: 0.021153. Entropy: 0.841752.\n",
      "Iteration 2948: Policy loss: -0.011191. Value loss: 0.016563. Entropy: 0.846882.\n",
      "Iteration 2949: Policy loss: -0.024072. Value loss: 0.014424. Entropy: 0.847428.\n",
      "episode: 2391   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 579     evaluation reward: 7.86\n",
      "Training network. lr: 0.000228. clip: 0.091153\n",
      "Iteration 2950: Policy loss: 0.008290. Value loss: 0.018447. Entropy: 0.753943.\n",
      "Iteration 2951: Policy loss: -0.010312. Value loss: 0.013242. Entropy: 0.758520.\n",
      "Iteration 2952: Policy loss: -0.025448. Value loss: 0.011876. Entropy: 0.754199.\n",
      "episode: 2392   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 515     evaluation reward: 7.81\n",
      "episode: 2393   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 722     evaluation reward: 7.89\n",
      "Training network. lr: 0.000228. clip: 0.091144\n",
      "Iteration 2953: Policy loss: 0.007160. Value loss: 0.018963. Entropy: 0.804440.\n",
      "Iteration 2954: Policy loss: -0.014632. Value loss: 0.014477. Entropy: 0.818792.\n",
      "Iteration 2955: Policy loss: -0.022360. Value loss: 0.012095. Entropy: 0.812883.\n",
      "episode: 2394   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 666     evaluation reward: 7.9\n",
      "episode: 2395   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 495     evaluation reward: 7.86\n",
      "Training network. lr: 0.000228. clip: 0.091135\n",
      "Iteration 2956: Policy loss: 0.002523. Value loss: 0.039212. Entropy: 0.707202.\n",
      "Iteration 2957: Policy loss: -0.007887. Value loss: 0.031048. Entropy: 0.713875.\n",
      "Iteration 2958: Policy loss: -0.019896. Value loss: 0.027073. Entropy: 0.706798.\n",
      "episode: 2396   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 631     evaluation reward: 7.9\n",
      "Training network. lr: 0.000228. clip: 0.091126\n",
      "Iteration 2959: Policy loss: 0.007894. Value loss: 0.016868. Entropy: 0.812417.\n",
      "Iteration 2960: Policy loss: -0.012546. Value loss: 0.013032. Entropy: 0.812220.\n",
      "Iteration 2961: Policy loss: -0.021597. Value loss: 0.010958. Entropy: 0.796949.\n",
      "episode: 2397   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 570     evaluation reward: 7.88\n",
      "episode: 2398   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 273     evaluation reward: 7.78\n",
      "episode: 2399   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 567     evaluation reward: 7.78\n",
      "Training network. lr: 0.000228. clip: 0.091117\n",
      "Iteration 2962: Policy loss: 0.006464. Value loss: 0.022964. Entropy: 0.855954.\n",
      "Iteration 2963: Policy loss: -0.013302. Value loss: 0.014782. Entropy: 0.858274.\n",
      "Iteration 2964: Policy loss: -0.020668. Value loss: 0.012565. Entropy: 0.844399.\n",
      "episode: 2400   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 486     evaluation reward: 7.74\n",
      "Training network. lr: 0.000228. clip: 0.091108\n",
      "Iteration 2965: Policy loss: 0.010836. Value loss: 0.054527. Entropy: 0.871812.\n",
      "Iteration 2966: Policy loss: -0.005495. Value loss: 0.036606. Entropy: 0.859111.\n",
      "Iteration 2967: Policy loss: -0.012886. Value loss: 0.029535. Entropy: 0.837956.\n",
      "episode: 2401   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 727     evaluation reward: 7.77\n",
      "episode: 2402   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 747     evaluation reward: 7.86\n",
      "Training network. lr: 0.000228. clip: 0.091099\n",
      "Iteration 2968: Policy loss: 0.008468. Value loss: 0.045296. Entropy: 0.742235.\n",
      "Iteration 2969: Policy loss: -0.010760. Value loss: 0.034441. Entropy: 0.749347.\n",
      "Iteration 2970: Policy loss: -0.018951. Value loss: 0.028938. Entropy: 0.737349.\n",
      "episode: 2403   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 385     evaluation reward: 7.86\n",
      "episode: 2404   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 691     evaluation reward: 7.9\n",
      "Training network. lr: 0.000228. clip: 0.091090\n",
      "Iteration 2971: Policy loss: 0.001853. Value loss: 0.021118. Entropy: 0.624819.\n",
      "Iteration 2972: Policy loss: -0.010828. Value loss: 0.014052. Entropy: 0.623350.\n",
      "Iteration 2973: Policy loss: -0.020944. Value loss: 0.011856. Entropy: 0.605360.\n",
      "episode: 2405   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 609     evaluation reward: 7.94\n",
      "episode: 2406   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 538     evaluation reward: 7.96\n",
      "Training network. lr: 0.000228. clip: 0.091081\n",
      "Iteration 2974: Policy loss: 0.006884. Value loss: 0.041752. Entropy: 0.762633.\n",
      "Iteration 2975: Policy loss: -0.005924. Value loss: 0.033345. Entropy: 0.757967.\n",
      "Iteration 2976: Policy loss: -0.018707. Value loss: 0.029470. Entropy: 0.760259.\n",
      "episode: 2407   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 568     evaluation reward: 7.98\n",
      "Training network. lr: 0.000228. clip: 0.091072\n",
      "Iteration 2977: Policy loss: 0.006571. Value loss: 0.045197. Entropy: 0.703563.\n",
      "Iteration 2978: Policy loss: -0.008929. Value loss: 0.035689. Entropy: 0.695773.\n",
      "Iteration 2979: Policy loss: -0.016092. Value loss: 0.030210. Entropy: 0.698909.\n",
      "episode: 2408   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 646     evaluation reward: 8.0\n",
      "episode: 2409   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 547     evaluation reward: 8.01\n",
      "Training network. lr: 0.000228. clip: 0.091063\n",
      "Iteration 2980: Policy loss: 0.004868. Value loss: 0.031506. Entropy: 0.785166.\n",
      "Iteration 2981: Policy loss: -0.013234. Value loss: 0.024054. Entropy: 0.782610.\n",
      "Iteration 2982: Policy loss: -0.022427. Value loss: 0.018771. Entropy: 0.778707.\n",
      "episode: 2410   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 524     evaluation reward: 8.03\n",
      "episode: 2411   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 411     evaluation reward: 7.99\n",
      "Training network. lr: 0.000228. clip: 0.091054\n",
      "Iteration 2983: Policy loss: 0.002489. Value loss: 0.024304. Entropy: 0.733568.\n",
      "Iteration 2984: Policy loss: -0.016240. Value loss: 0.017071. Entropy: 0.728445.\n",
      "Iteration 2985: Policy loss: -0.024928. Value loss: 0.013056. Entropy: 0.716707.\n",
      "episode: 2412   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 813     evaluation reward: 8.07\n",
      "episode: 2413   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 602     evaluation reward: 8.01\n",
      "Training network. lr: 0.000228. clip: 0.091045\n",
      "Iteration 2986: Policy loss: 0.011007. Value loss: 0.048918. Entropy: 0.745361.\n",
      "Iteration 2987: Policy loss: -0.007897. Value loss: 0.036826. Entropy: 0.729630.\n",
      "Iteration 2988: Policy loss: -0.022077. Value loss: 0.030575. Entropy: 0.723371.\n",
      "episode: 2414   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 566     evaluation reward: 8.0\n",
      "Training network. lr: 0.000228. clip: 0.091036\n",
      "Iteration 2989: Policy loss: 0.009079. Value loss: 0.023848. Entropy: 0.670355.\n",
      "Iteration 2990: Policy loss: -0.012376. Value loss: 0.017858. Entropy: 0.661762.\n",
      "Iteration 2991: Policy loss: -0.024862. Value loss: 0.014748. Entropy: 0.668074.\n",
      "episode: 2415   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 643     evaluation reward: 8.02\n",
      "episode: 2416   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 597     evaluation reward: 8.01\n",
      "Training network. lr: 0.000228. clip: 0.091027\n",
      "Iteration 2992: Policy loss: 0.012035. Value loss: 0.023696. Entropy: 0.689777.\n",
      "Iteration 2993: Policy loss: -0.009479. Value loss: 0.016923. Entropy: 0.690474.\n",
      "Iteration 2994: Policy loss: -0.021440. Value loss: 0.013731. Entropy: 0.696870.\n",
      "episode: 2417   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 568     evaluation reward: 8.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2418   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 659     evaluation reward: 8.06\n",
      "Training network. lr: 0.000228. clip: 0.091018\n",
      "Iteration 2995: Policy loss: 0.010901. Value loss: 0.028582. Entropy: 0.710890.\n",
      "Iteration 2996: Policy loss: -0.010294. Value loss: 0.021958. Entropy: 0.718640.\n",
      "Iteration 2997: Policy loss: -0.024809. Value loss: 0.018116. Entropy: 0.705437.\n",
      "episode: 2419   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 414     evaluation reward: 8.02\n",
      "episode: 2420   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 650     evaluation reward: 8.01\n",
      "Training network. lr: 0.000228. clip: 0.091009\n",
      "Iteration 2998: Policy loss: 0.005454. Value loss: 0.034700. Entropy: 0.777545.\n",
      "Iteration 2999: Policy loss: -0.015752. Value loss: 0.026834. Entropy: 0.776944.\n",
      "Iteration 3000: Policy loss: -0.021542. Value loss: 0.023731. Entropy: 0.771248.\n",
      "episode: 2421   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 654     evaluation reward: 8.06\n",
      "Training network. lr: 0.000228. clip: 0.091000\n",
      "Iteration 3001: Policy loss: 0.009558. Value loss: 0.026718. Entropy: 0.739088.\n",
      "Iteration 3002: Policy loss: -0.009122. Value loss: 0.018131. Entropy: 0.751475.\n",
      "Iteration 3003: Policy loss: -0.023123. Value loss: 0.014762. Entropy: 0.747436.\n",
      "episode: 2422   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 768     evaluation reward: 8.11\n",
      "Training network. lr: 0.000227. clip: 0.090991\n",
      "Iteration 3004: Policy loss: 0.011214. Value loss: 0.045970. Entropy: 0.762730.\n",
      "Iteration 3005: Policy loss: -0.005209. Value loss: 0.036564. Entropy: 0.760959.\n",
      "Iteration 3006: Policy loss: -0.006528. Value loss: 0.031343. Entropy: 0.758798.\n",
      "episode: 2423   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 741     evaluation reward: 8.17\n",
      "episode: 2424   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 607     evaluation reward: 8.18\n",
      "Training network. lr: 0.000227. clip: 0.090982\n",
      "Iteration 3007: Policy loss: 0.002218. Value loss: 0.058491. Entropy: 0.720272.\n",
      "Iteration 3008: Policy loss: -0.014105. Value loss: 0.040115. Entropy: 0.715497.\n",
      "Iteration 3009: Policy loss: -0.021174. Value loss: 0.032675. Entropy: 0.708810.\n",
      "episode: 2425   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 454     evaluation reward: 8.17\n",
      "episode: 2426   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 559     evaluation reward: 8.15\n",
      "Training network. lr: 0.000227. clip: 0.090973\n",
      "Iteration 3010: Policy loss: 0.012314. Value loss: 0.035555. Entropy: 0.760148.\n",
      "Iteration 3011: Policy loss: -0.007810. Value loss: 0.026785. Entropy: 0.763072.\n",
      "Iteration 3012: Policy loss: -0.021429. Value loss: 0.022128. Entropy: 0.762969.\n",
      "episode: 2427   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 709     evaluation reward: 8.18\n",
      "episode: 2428   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 548     evaluation reward: 8.17\n",
      "Training network. lr: 0.000227. clip: 0.090964\n",
      "Iteration 3013: Policy loss: 0.010677. Value loss: 0.027628. Entropy: 0.698134.\n",
      "Iteration 3014: Policy loss: -0.009973. Value loss: 0.020209. Entropy: 0.687945.\n",
      "Iteration 3015: Policy loss: -0.018957. Value loss: 0.017441. Entropy: 0.694855.\n",
      "episode: 2429   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 695     evaluation reward: 8.2\n",
      "episode: 2430   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 332     evaluation reward: 8.18\n",
      "Training network. lr: 0.000227. clip: 0.090955\n",
      "Iteration 3016: Policy loss: 0.005834. Value loss: 0.056470. Entropy: 0.751847.\n",
      "Iteration 3017: Policy loss: -0.009382. Value loss: 0.043081. Entropy: 0.741088.\n",
      "Iteration 3018: Policy loss: -0.020533. Value loss: 0.035951. Entropy: 0.740319.\n",
      "episode: 2431   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 682     evaluation reward: 8.17\n",
      "Training network. lr: 0.000227. clip: 0.090946\n",
      "Iteration 3019: Policy loss: 0.010208. Value loss: 0.021858. Entropy: 0.706028.\n",
      "Iteration 3020: Policy loss: -0.013403. Value loss: 0.018552. Entropy: 0.695509.\n",
      "Iteration 3021: Policy loss: -0.021247. Value loss: 0.015824. Entropy: 0.687633.\n",
      "episode: 2432   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 802     evaluation reward: 8.19\n",
      "episode: 2433   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 533     evaluation reward: 8.15\n",
      "Training network. lr: 0.000227. clip: 0.090937\n",
      "Iteration 3022: Policy loss: 0.007356. Value loss: 0.032091. Entropy: 0.749106.\n",
      "Iteration 3023: Policy loss: -0.009722. Value loss: 0.021975. Entropy: 0.746055.\n",
      "Iteration 3024: Policy loss: -0.022118. Value loss: 0.017874. Entropy: 0.742249.\n",
      "episode: 2434   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 483     evaluation reward: 8.09\n",
      "episode: 2435   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 382     evaluation reward: 8.07\n",
      "Training network. lr: 0.000227. clip: 0.090928\n",
      "Iteration 3025: Policy loss: 0.009961. Value loss: 0.030009. Entropy: 0.736824.\n",
      "Iteration 3026: Policy loss: -0.012290. Value loss: 0.021884. Entropy: 0.724338.\n",
      "Iteration 3027: Policy loss: -0.022502. Value loss: 0.017588. Entropy: 0.722416.\n",
      "episode: 2436   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 547     evaluation reward: 8.11\n",
      "episode: 2437   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 540     evaluation reward: 8.13\n",
      "Training network. lr: 0.000227. clip: 0.090919\n",
      "Iteration 3028: Policy loss: 0.009298. Value loss: 0.024066. Entropy: 0.656603.\n",
      "Iteration 3029: Policy loss: -0.011355. Value loss: 0.017788. Entropy: 0.669909.\n",
      "Iteration 3030: Policy loss: -0.020287. Value loss: 0.014140. Entropy: 0.665938.\n",
      "episode: 2438   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 985     evaluation reward: 8.23\n",
      "Training network. lr: 0.000227. clip: 0.090910\n",
      "Iteration 3031: Policy loss: 0.007758. Value loss: 0.021348. Entropy: 0.722280.\n",
      "Iteration 3032: Policy loss: -0.007978. Value loss: 0.016236. Entropy: 0.722251.\n",
      "Iteration 3033: Policy loss: -0.019406. Value loss: 0.012654. Entropy: 0.734885.\n",
      "episode: 2439   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 775     evaluation reward: 8.25\n",
      "Training network. lr: 0.000227. clip: 0.090901\n",
      "Iteration 3034: Policy loss: 0.009759. Value loss: 0.018179. Entropy: 0.796253.\n",
      "Iteration 3035: Policy loss: -0.014575. Value loss: 0.015446. Entropy: 0.772153.\n",
      "Iteration 3036: Policy loss: -0.025492. Value loss: 0.013191. Entropy: 0.784233.\n",
      "episode: 2440   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 518     evaluation reward: 8.24\n",
      "episode: 2441   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 702     evaluation reward: 8.29\n",
      "Training network. lr: 0.000227. clip: 0.090892\n",
      "Iteration 3037: Policy loss: 0.006960. Value loss: 0.047388. Entropy: 0.718431.\n",
      "Iteration 3038: Policy loss: -0.001856. Value loss: 0.035802. Entropy: 0.714324.\n",
      "Iteration 3039: Policy loss: -0.014245. Value loss: 0.031617. Entropy: 0.722827.\n",
      "episode: 2442   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 519     evaluation reward: 8.27\n",
      "episode: 2443   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 626     evaluation reward: 8.29\n",
      "Training network. lr: 0.000227. clip: 0.090883\n",
      "Iteration 3040: Policy loss: 0.004778. Value loss: 0.020473. Entropy: 0.698827.\n",
      "Iteration 3041: Policy loss: -0.007837. Value loss: 0.015294. Entropy: 0.705527.\n",
      "Iteration 3042: Policy loss: -0.022687. Value loss: 0.013357. Entropy: 0.701311.\n",
      "episode: 2444   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 517     evaluation reward: 8.24\n",
      "episode: 2445   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 482     evaluation reward: 8.2\n",
      "Training network. lr: 0.000227. clip: 0.090874\n",
      "Iteration 3043: Policy loss: 0.007197. Value loss: 0.020454. Entropy: 0.779368.\n",
      "Iteration 3044: Policy loss: -0.006442. Value loss: 0.015459. Entropy: 0.775248.\n",
      "Iteration 3045: Policy loss: -0.020858. Value loss: 0.012992. Entropy: 0.778680.\n",
      "episode: 2446   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 439     evaluation reward: 8.13\n",
      "episode: 2447   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 514     evaluation reward: 8.15\n",
      "episode: 2448   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 245     evaluation reward: 8.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000227. clip: 0.090865\n",
      "Iteration 3046: Policy loss: 0.007085. Value loss: 0.040341. Entropy: 0.843810.\n",
      "Iteration 3047: Policy loss: -0.009384. Value loss: 0.027865. Entropy: 0.832349.\n",
      "Iteration 3048: Policy loss: -0.019914. Value loss: 0.024348. Entropy: 0.832296.\n",
      "episode: 2449   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 564     evaluation reward: 8.15\n",
      "episode: 2450   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 383     evaluation reward: 8.14\n",
      "Training network. lr: 0.000227. clip: 0.090856\n",
      "Iteration 3049: Policy loss: 0.011815. Value loss: 0.024853. Entropy: 0.819611.\n",
      "Iteration 3050: Policy loss: -0.005111. Value loss: 0.017287. Entropy: 0.816526.\n",
      "Iteration 3051: Policy loss: -0.018914. Value loss: 0.014891. Entropy: 0.814586.\n",
      "episode: 2451   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 573     evaluation reward: 8.14\n",
      "episode: 2452   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 468     evaluation reward: 8.1\n",
      "Training network. lr: 0.000227. clip: 0.090847\n",
      "Iteration 3052: Policy loss: 0.005125. Value loss: 0.050521. Entropy: 0.742451.\n",
      "Iteration 3053: Policy loss: -0.006646. Value loss: 0.039326. Entropy: 0.744052.\n",
      "Iteration 3054: Policy loss: -0.011127. Value loss: 0.035997. Entropy: 0.745580.\n",
      "episode: 2453   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 515     evaluation reward: 8.09\n",
      "episode: 2454   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 477     evaluation reward: 8.08\n",
      "Training network. lr: 0.000227. clip: 0.090838\n",
      "Iteration 3055: Policy loss: 0.007793. Value loss: 0.024412. Entropy: 0.764896.\n",
      "Iteration 3056: Policy loss: -0.012147. Value loss: 0.018147. Entropy: 0.749172.\n",
      "Iteration 3057: Policy loss: -0.021082. Value loss: 0.016577. Entropy: 0.753141.\n",
      "episode: 2455   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 434     evaluation reward: 8.06\n",
      "episode: 2456   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 571     evaluation reward: 8.11\n",
      "Training network. lr: 0.000227. clip: 0.090829\n",
      "Iteration 3058: Policy loss: 0.008020. Value loss: 0.047968. Entropy: 0.728737.\n",
      "Iteration 3059: Policy loss: -0.005673. Value loss: 0.038329. Entropy: 0.730519.\n",
      "Iteration 3060: Policy loss: -0.015892. Value loss: 0.033747. Entropy: 0.729855.\n",
      "episode: 2457   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 476     evaluation reward: 8.07\n",
      "episode: 2458   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 436     evaluation reward: 8.1\n",
      "Training network. lr: 0.000227. clip: 0.090820\n",
      "Iteration 3061: Policy loss: 0.009784. Value loss: 0.021483. Entropy: 0.822624.\n",
      "Iteration 3062: Policy loss: -0.008544. Value loss: 0.015210. Entropy: 0.810777.\n",
      "Iteration 3063: Policy loss: -0.019126. Value loss: 0.013112. Entropy: 0.817608.\n",
      "episode: 2459   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 741     evaluation reward: 8.14\n",
      "Training network. lr: 0.000227. clip: 0.090811\n",
      "Iteration 3064: Policy loss: 0.013671. Value loss: 0.017609. Entropy: 0.813295.\n",
      "Iteration 3065: Policy loss: -0.013635. Value loss: 0.013653. Entropy: 0.815230.\n",
      "Iteration 3066: Policy loss: -0.024824. Value loss: 0.010817. Entropy: 0.803330.\n",
      "episode: 2460   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 659     evaluation reward: 8.18\n",
      "episode: 2461   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 613     evaluation reward: 8.23\n",
      "Training network. lr: 0.000227. clip: 0.090802\n",
      "Iteration 3067: Policy loss: 0.007440. Value loss: 0.016236. Entropy: 0.747354.\n",
      "Iteration 3068: Policy loss: -0.016907. Value loss: 0.012548. Entropy: 0.742656.\n",
      "Iteration 3069: Policy loss: -0.024135. Value loss: 0.010444. Entropy: 0.741023.\n",
      "episode: 2462   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 718     evaluation reward: 8.25\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3070: Policy loss: 0.006778. Value loss: 0.019482. Entropy: 0.750495.\n",
      "Iteration 3071: Policy loss: -0.015054. Value loss: 0.014123. Entropy: 0.741825.\n",
      "Iteration 3072: Policy loss: -0.021438. Value loss: 0.011961. Entropy: 0.746618.\n",
      "episode: 2463   score: 17.0   memory length: 1024   epsilon: 1.0    steps: 889     evaluation reward: 8.28\n",
      "episode: 2464   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 729     evaluation reward: 8.29\n",
      "Training network. lr: 0.000227. clip: 0.090784\n",
      "Iteration 3073: Policy loss: 0.003349. Value loss: 0.048591. Entropy: 0.729128.\n",
      "Iteration 3074: Policy loss: -0.010513. Value loss: 0.039828. Entropy: 0.729509.\n",
      "Iteration 3075: Policy loss: -0.020853. Value loss: 0.033119. Entropy: 0.720470.\n",
      "now time :  2018-12-26 13:38:15.670188\n",
      "episode: 2465   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 651     evaluation reward: 8.3\n",
      "Training network. lr: 0.000227. clip: 0.090775\n",
      "Iteration 3076: Policy loss: 0.005889. Value loss: 0.023359. Entropy: 0.678541.\n",
      "Iteration 3077: Policy loss: -0.010395. Value loss: 0.017494. Entropy: 0.676223.\n",
      "Iteration 3078: Policy loss: -0.017222. Value loss: 0.014499. Entropy: 0.671980.\n",
      "episode: 2466   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 575     evaluation reward: 8.31\n",
      "episode: 2467   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 553     evaluation reward: 8.29\n",
      "Training network. lr: 0.000227. clip: 0.090766\n",
      "Iteration 3079: Policy loss: 0.007294. Value loss: 0.019960. Entropy: 0.641621.\n",
      "Iteration 3080: Policy loss: -0.006487. Value loss: 0.015269. Entropy: 0.649276.\n",
      "Iteration 3081: Policy loss: -0.021716. Value loss: 0.013249. Entropy: 0.636247.\n",
      "episode: 2468   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 622     evaluation reward: 8.31\n",
      "Training network. lr: 0.000227. clip: 0.090757\n",
      "Iteration 3082: Policy loss: 0.004737. Value loss: 0.045667. Entropy: 0.719893.\n",
      "Iteration 3083: Policy loss: -0.006564. Value loss: 0.036492. Entropy: 0.711742.\n",
      "Iteration 3084: Policy loss: -0.019558. Value loss: 0.029989. Entropy: 0.711325.\n",
      "episode: 2469   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 722     evaluation reward: 8.36\n",
      "episode: 2470   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 514     evaluation reward: 8.44\n",
      "Training network. lr: 0.000227. clip: 0.090748\n",
      "Iteration 3085: Policy loss: 0.004720. Value loss: 0.075790. Entropy: 0.679723.\n",
      "Iteration 3086: Policy loss: -0.008980. Value loss: 0.062384. Entropy: 0.675271.\n",
      "Iteration 3087: Policy loss: -0.012138. Value loss: 0.053810. Entropy: 0.682785.\n",
      "episode: 2471   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 571     evaluation reward: 8.44\n",
      "episode: 2472   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 624     evaluation reward: 8.43\n",
      "Training network. lr: 0.000227. clip: 0.090739\n",
      "Iteration 3088: Policy loss: 0.003709. Value loss: 0.031205. Entropy: 0.707942.\n",
      "Iteration 3089: Policy loss: -0.011736. Value loss: 0.020302. Entropy: 0.709444.\n",
      "Iteration 3090: Policy loss: -0.022239. Value loss: 0.016538. Entropy: 0.698732.\n",
      "episode: 2473   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 538     evaluation reward: 8.42\n",
      "episode: 2474   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 670     evaluation reward: 8.38\n",
      "Training network. lr: 0.000227. clip: 0.090730\n",
      "Iteration 3091: Policy loss: 0.010985. Value loss: 0.022511. Entropy: 0.740524.\n",
      "Iteration 3092: Policy loss: -0.014747. Value loss: 0.017478. Entropy: 0.739116.\n",
      "Iteration 3093: Policy loss: -0.023239. Value loss: 0.015035. Entropy: 0.733344.\n",
      "episode: 2475   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 435     evaluation reward: 8.34\n",
      "episode: 2476   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 279     evaluation reward: 8.29\n",
      "Training network. lr: 0.000227. clip: 0.090721\n",
      "Iteration 3094: Policy loss: 0.006830. Value loss: 0.031073. Entropy: 0.712244.\n",
      "Iteration 3095: Policy loss: -0.013826. Value loss: 0.019460. Entropy: 0.715044.\n",
      "Iteration 3096: Policy loss: -0.021052. Value loss: 0.015915. Entropy: 0.709839.\n",
      "episode: 2477   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 514     evaluation reward: 8.3\n",
      "episode: 2478   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 442     evaluation reward: 8.29\n",
      "Training network. lr: 0.000227. clip: 0.090712\n",
      "Iteration 3097: Policy loss: 0.006316. Value loss: 0.022985. Entropy: 0.661592.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3098: Policy loss: -0.012811. Value loss: 0.017632. Entropy: 0.662578.\n",
      "Iteration 3099: Policy loss: -0.022371. Value loss: 0.014927. Entropy: 0.662259.\n",
      "episode: 2479   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 536     evaluation reward: 8.3\n",
      "episode: 2480   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 634     evaluation reward: 8.28\n",
      "Training network. lr: 0.000227. clip: 0.090703\n",
      "Iteration 3100: Policy loss: 0.010680. Value loss: 0.017804. Entropy: 0.734036.\n",
      "Iteration 3101: Policy loss: -0.011022. Value loss: 0.014475. Entropy: 0.740612.\n",
      "Iteration 3102: Policy loss: -0.021459. Value loss: 0.013104. Entropy: 0.743877.\n",
      "episode: 2481   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 474     evaluation reward: 8.2\n",
      "episode: 2482   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 626     evaluation reward: 8.24\n",
      "Training network. lr: 0.000227. clip: 0.090694\n",
      "Iteration 3103: Policy loss: 0.008316. Value loss: 0.015443. Entropy: 0.556825.\n",
      "Iteration 3104: Policy loss: -0.008818. Value loss: 0.012483. Entropy: 0.558601.\n",
      "Iteration 3105: Policy loss: -0.018573. Value loss: 0.010980. Entropy: 0.541232.\n",
      "episode: 2483   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 613     evaluation reward: 8.28\n",
      "episode: 2484   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 433     evaluation reward: 8.28\n",
      "Training network. lr: 0.000227. clip: 0.090685\n",
      "Iteration 3106: Policy loss: 0.007708. Value loss: 0.028120. Entropy: 0.679602.\n",
      "Iteration 3107: Policy loss: -0.012158. Value loss: 0.022970. Entropy: 0.679571.\n",
      "Iteration 3108: Policy loss: -0.024309. Value loss: 0.018671. Entropy: 0.681928.\n",
      "episode: 2485   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 726     evaluation reward: 8.38\n",
      "Training network. lr: 0.000227. clip: 0.090676\n",
      "Iteration 3109: Policy loss: 0.013576. Value loss: 0.041248. Entropy: 0.689916.\n",
      "Iteration 3110: Policy loss: 0.001454. Value loss: 0.029873. Entropy: 0.693206.\n",
      "Iteration 3111: Policy loss: -0.014665. Value loss: 0.025586. Entropy: 0.673170.\n",
      "episode: 2486   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 655     evaluation reward: 8.39\n",
      "episode: 2487   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 598     evaluation reward: 8.45\n",
      "Training network. lr: 0.000227. clip: 0.090667\n",
      "Iteration 3112: Policy loss: 0.005771. Value loss: 0.043816. Entropy: 0.677212.\n",
      "Iteration 3113: Policy loss: -0.008165. Value loss: 0.034718. Entropy: 0.685484.\n",
      "Iteration 3114: Policy loss: -0.020848. Value loss: 0.030624. Entropy: 0.677420.\n",
      "episode: 2488   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 452     evaluation reward: 8.43\n",
      "episode: 2489   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 567     evaluation reward: 8.45\n",
      "Training network. lr: 0.000227. clip: 0.090658\n",
      "Iteration 3115: Policy loss: 0.006394. Value loss: 0.025610. Entropy: 0.683742.\n",
      "Iteration 3116: Policy loss: -0.011542. Value loss: 0.018244. Entropy: 0.692448.\n",
      "Iteration 3117: Policy loss: -0.019898. Value loss: 0.016214. Entropy: 0.684038.\n",
      "episode: 2490   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 859     evaluation reward: 8.55\n",
      "Training network. lr: 0.000227. clip: 0.090649\n",
      "Iteration 3118: Policy loss: 0.010319. Value loss: 0.027268. Entropy: 0.734598.\n",
      "Iteration 3119: Policy loss: -0.009107. Value loss: 0.019900. Entropy: 0.732093.\n",
      "Iteration 3120: Policy loss: -0.023541. Value loss: 0.016780. Entropy: 0.731907.\n",
      "episode: 2491   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 702     evaluation reward: 8.57\n",
      "episode: 2492   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 395     evaluation reward: 8.56\n",
      "Training network. lr: 0.000227. clip: 0.090640\n",
      "Iteration 3121: Policy loss: 0.009093. Value loss: 0.021576. Entropy: 0.709360.\n",
      "Iteration 3122: Policy loss: -0.009409. Value loss: 0.016459. Entropy: 0.701151.\n",
      "Iteration 3123: Policy loss: -0.024002. Value loss: 0.013796. Entropy: 0.706016.\n",
      "episode: 2493   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 704     evaluation reward: 8.55\n",
      "Training network. lr: 0.000227. clip: 0.090631\n",
      "Iteration 3124: Policy loss: 0.002982. Value loss: 0.022200. Entropy: 0.764882.\n",
      "Iteration 3125: Policy loss: -0.018697. Value loss: 0.013424. Entropy: 0.756663.\n",
      "Iteration 3126: Policy loss: -0.028556. Value loss: 0.010833. Entropy: 0.741891.\n",
      "episode: 2494   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 812     evaluation reward: 8.56\n",
      "episode: 2495   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 776     evaluation reward: 8.61\n",
      "Training network. lr: 0.000227. clip: 0.090622\n",
      "Iteration 3127: Policy loss: 0.005854. Value loss: 0.022712. Entropy: 0.791145.\n",
      "Iteration 3128: Policy loss: -0.014305. Value loss: 0.017079. Entropy: 0.776872.\n",
      "Iteration 3129: Policy loss: -0.023871. Value loss: 0.014422. Entropy: 0.771354.\n",
      "episode: 2496   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 535     evaluation reward: 8.59\n",
      "episode: 2497   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 586     evaluation reward: 8.59\n",
      "Training network. lr: 0.000227. clip: 0.090613\n",
      "Iteration 3130: Policy loss: 0.006198. Value loss: 0.025857. Entropy: 0.740041.\n",
      "Iteration 3131: Policy loss: -0.014377. Value loss: 0.019015. Entropy: 0.751551.\n",
      "Iteration 3132: Policy loss: -0.024650. Value loss: 0.015876. Entropy: 0.745257.\n",
      "episode: 2498   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 468     evaluation reward: 8.63\n",
      "Training network. lr: 0.000227. clip: 0.090604\n",
      "Iteration 3133: Policy loss: 0.009762. Value loss: 0.021815. Entropy: 0.780620.\n",
      "Iteration 3134: Policy loss: -0.005536. Value loss: 0.016464. Entropy: 0.779761.\n",
      "Iteration 3135: Policy loss: -0.021883. Value loss: 0.014746. Entropy: 0.780471.\n",
      "episode: 2499   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 650     evaluation reward: 8.64\n",
      "episode: 2500   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 595     evaluation reward: 8.66\n",
      "Training network. lr: 0.000226. clip: 0.090595\n",
      "Iteration 3136: Policy loss: 0.007158. Value loss: 0.016318. Entropy: 0.754359.\n",
      "Iteration 3137: Policy loss: -0.009117. Value loss: 0.012823. Entropy: 0.744464.\n",
      "Iteration 3138: Policy loss: -0.019805. Value loss: 0.010346. Entropy: 0.747871.\n",
      "episode: 2501   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 585     evaluation reward: 8.61\n",
      "episode: 2502   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 701     evaluation reward: 8.6\n",
      "Training network. lr: 0.000226. clip: 0.090586\n",
      "Iteration 3139: Policy loss: 0.012609. Value loss: 0.045178. Entropy: 0.716666.\n",
      "Iteration 3140: Policy loss: -0.010649. Value loss: 0.033388. Entropy: 0.711312.\n",
      "Iteration 3141: Policy loss: -0.017963. Value loss: 0.028630. Entropy: 0.712381.\n",
      "episode: 2503   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 539     evaluation reward: 8.63\n",
      "episode: 2504   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 408     evaluation reward: 8.58\n",
      "Training network. lr: 0.000226. clip: 0.090577\n",
      "Iteration 3142: Policy loss: 0.008261. Value loss: 0.023762. Entropy: 0.805297.\n",
      "Iteration 3143: Policy loss: -0.008441. Value loss: 0.017557. Entropy: 0.802901.\n",
      "Iteration 3144: Policy loss: -0.016836. Value loss: 0.014587. Entropy: 0.792968.\n",
      "episode: 2505   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 445     evaluation reward: 8.52\n",
      "episode: 2506   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 571     evaluation reward: 8.53\n",
      "Training network. lr: 0.000226. clip: 0.090568\n",
      "Iteration 3145: Policy loss: 0.009245. Value loss: 0.028378. Entropy: 0.711674.\n",
      "Iteration 3146: Policy loss: -0.007510. Value loss: 0.022751. Entropy: 0.710636.\n",
      "Iteration 3147: Policy loss: -0.021945. Value loss: 0.018300. Entropy: 0.707778.\n",
      "episode: 2507   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 569     evaluation reward: 8.52\n",
      "episode: 2508   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 480     evaluation reward: 8.47\n",
      "Training network. lr: 0.000226. clip: 0.090559\n",
      "Iteration 3148: Policy loss: 0.009581. Value loss: 0.025669. Entropy: 0.731133.\n",
      "Iteration 3149: Policy loss: -0.011372. Value loss: 0.020020. Entropy: 0.733634.\n",
      "Iteration 3150: Policy loss: -0.019790. Value loss: 0.017391. Entropy: 0.725586.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2509   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 547     evaluation reward: 8.56\n",
      "Training network. lr: 0.000226. clip: 0.090550\n",
      "Iteration 3151: Policy loss: 0.009400. Value loss: 0.070543. Entropy: 0.759586.\n",
      "Iteration 3152: Policy loss: -0.004021. Value loss: 0.057739. Entropy: 0.763372.\n",
      "Iteration 3153: Policy loss: -0.013987. Value loss: 0.048590. Entropy: 0.774067.\n",
      "episode: 2510   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 812     evaluation reward: 8.61\n",
      "episode: 2511   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 658     evaluation reward: 8.68\n",
      "Training network. lr: 0.000226. clip: 0.090541\n",
      "Iteration 3154: Policy loss: 0.008841. Value loss: 0.046513. Entropy: 0.652330.\n",
      "Iteration 3155: Policy loss: -0.007337. Value loss: 0.035146. Entropy: 0.652760.\n",
      "Iteration 3156: Policy loss: -0.017030. Value loss: 0.029302. Entropy: 0.648448.\n",
      "episode: 2512   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 702     evaluation reward: 8.63\n",
      "Training network. lr: 0.000226. clip: 0.090532\n",
      "Iteration 3157: Policy loss: 0.009677. Value loss: 0.025203. Entropy: 0.720077.\n",
      "Iteration 3158: Policy loss: -0.006051. Value loss: 0.016852. Entropy: 0.733717.\n",
      "Iteration 3159: Policy loss: -0.018200. Value loss: 0.013487. Entropy: 0.721863.\n",
      "episode: 2513   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 648     evaluation reward: 8.66\n",
      "episode: 2514   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 597     evaluation reward: 8.67\n",
      "Training network. lr: 0.000226. clip: 0.090523\n",
      "Iteration 3160: Policy loss: 0.014167. Value loss: 0.020734. Entropy: 0.694312.\n",
      "Iteration 3161: Policy loss: -0.007781. Value loss: 0.014666. Entropy: 0.679537.\n",
      "Iteration 3162: Policy loss: -0.019451. Value loss: 0.012208. Entropy: 0.675295.\n",
      "episode: 2515   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 715     evaluation reward: 8.73\n",
      "episode: 2516   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 572     evaluation reward: 8.74\n",
      "Training network. lr: 0.000226. clip: 0.090514\n",
      "Iteration 3163: Policy loss: 0.002226. Value loss: 0.038956. Entropy: 0.745462.\n",
      "Iteration 3164: Policy loss: -0.010760. Value loss: 0.029925. Entropy: 0.734624.\n",
      "Iteration 3165: Policy loss: -0.019375. Value loss: 0.023760. Entropy: 0.739973.\n",
      "episode: 2517   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 569     evaluation reward: 8.73\n",
      "episode: 2518   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 371     evaluation reward: 8.67\n",
      "Training network. lr: 0.000226. clip: 0.090505\n",
      "Iteration 3166: Policy loss: 0.003432. Value loss: 0.032544. Entropy: 0.812461.\n",
      "Iteration 3167: Policy loss: -0.012274. Value loss: 0.023233. Entropy: 0.814453.\n",
      "Iteration 3168: Policy loss: -0.019608. Value loss: 0.021062. Entropy: 0.800049.\n",
      "episode: 2519   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 446     evaluation reward: 8.68\n",
      "episode: 2520   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 737     evaluation reward: 8.71\n",
      "Training network. lr: 0.000226. clip: 0.090496\n",
      "Iteration 3169: Policy loss: 0.002208. Value loss: 0.035929. Entropy: 0.738724.\n",
      "Iteration 3170: Policy loss: -0.010336. Value loss: 0.022639. Entropy: 0.739597.\n",
      "Iteration 3171: Policy loss: -0.019027. Value loss: 0.017118. Entropy: 0.740250.\n",
      "episode: 2521   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 310     evaluation reward: 8.64\n",
      "episode: 2522   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 716     evaluation reward: 8.64\n",
      "Training network. lr: 0.000226. clip: 0.090487\n",
      "Iteration 3172: Policy loss: 0.004771. Value loss: 0.048490. Entropy: 0.818812.\n",
      "Iteration 3173: Policy loss: -0.013608. Value loss: 0.034771. Entropy: 0.807044.\n",
      "Iteration 3174: Policy loss: -0.015548. Value loss: 0.028242. Entropy: 0.815476.\n",
      "episode: 2523   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 777     evaluation reward: 8.62\n",
      "Training network. lr: 0.000226. clip: 0.090478\n",
      "Iteration 3175: Policy loss: 0.007939. Value loss: 0.022039. Entropy: 0.693890.\n",
      "Iteration 3176: Policy loss: -0.009128. Value loss: 0.015647. Entropy: 0.697215.\n",
      "Iteration 3177: Policy loss: -0.017934. Value loss: 0.012731. Entropy: 0.696275.\n",
      "episode: 2524   score: 19.0   memory length: 1024   epsilon: 1.0    steps: 818     evaluation reward: 8.73\n",
      "Training network. lr: 0.000226. clip: 0.090469\n",
      "Iteration 3178: Policy loss: 0.008321. Value loss: 0.075335. Entropy: 0.776580.\n",
      "Iteration 3179: Policy loss: -0.003194. Value loss: 0.057479. Entropy: 0.758380.\n",
      "Iteration 3180: Policy loss: -0.012851. Value loss: 0.050257. Entropy: 0.760802.\n",
      "episode: 2525   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 850     evaluation reward: 8.79\n",
      "episode: 2526   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 551     evaluation reward: 8.79\n",
      "Training network. lr: 0.000226. clip: 0.090460\n",
      "Iteration 3181: Policy loss: 0.011151. Value loss: 0.029763. Entropy: 0.808496.\n",
      "Iteration 3182: Policy loss: -0.009270. Value loss: 0.024901. Entropy: 0.799556.\n",
      "Iteration 3183: Policy loss: -0.024797. Value loss: 0.020225. Entropy: 0.802209.\n",
      "episode: 2527   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 432     evaluation reward: 8.75\n",
      "episode: 2528   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 504     evaluation reward: 8.73\n",
      "Training network. lr: 0.000226. clip: 0.090451\n",
      "Iteration 3184: Policy loss: 0.003453. Value loss: 0.030881. Entropy: 0.784918.\n",
      "Iteration 3185: Policy loss: -0.010470. Value loss: 0.025344. Entropy: 0.782193.\n",
      "Iteration 3186: Policy loss: -0.023964. Value loss: 0.022989. Entropy: 0.782856.\n",
      "episode: 2529   score: 21.0   memory length: 1024   epsilon: 1.0    steps: 869     evaluation reward: 8.81\n",
      "Training network. lr: 0.000226. clip: 0.090442\n",
      "Iteration 3187: Policy loss: 0.019898. Value loss: 0.108339. Entropy: 0.799242.\n",
      "Iteration 3188: Policy loss: 0.005223. Value loss: 0.068218. Entropy: 0.807144.\n",
      "Iteration 3189: Policy loss: -0.010705. Value loss: 0.054934. Entropy: 0.811875.\n",
      "episode: 2530   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 665     evaluation reward: 8.87\n",
      "episode: 2531   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 670     evaluation reward: 8.89\n",
      "Training network. lr: 0.000226. clip: 0.090433\n",
      "Iteration 3190: Policy loss: 0.009372. Value loss: 0.030365. Entropy: 0.816824.\n",
      "Iteration 3191: Policy loss: -0.011060. Value loss: 0.022283. Entropy: 0.818622.\n",
      "Iteration 3192: Policy loss: -0.022249. Value loss: 0.017516. Entropy: 0.815643.\n",
      "episode: 2532   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 642     evaluation reward: 8.87\n",
      "Training network. lr: 0.000226. clip: 0.090424\n",
      "Iteration 3193: Policy loss: 0.008501. Value loss: 0.024695. Entropy: 0.749163.\n",
      "Iteration 3194: Policy loss: -0.007567. Value loss: 0.017246. Entropy: 0.757164.\n",
      "Iteration 3195: Policy loss: -0.020069. Value loss: 0.013005. Entropy: 0.762548.\n",
      "episode: 2533   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 802     evaluation reward: 8.91\n",
      "Training network. lr: 0.000226. clip: 0.090415\n",
      "Iteration 3196: Policy loss: 0.006042. Value loss: 0.026789. Entropy: 0.800268.\n",
      "Iteration 3197: Policy loss: -0.009806. Value loss: 0.020216. Entropy: 0.799627.\n",
      "Iteration 3198: Policy loss: -0.019561. Value loss: 0.015984. Entropy: 0.808782.\n",
      "episode: 2534   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 647     evaluation reward: 8.94\n",
      "episode: 2535   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 510     evaluation reward: 8.99\n",
      "Training network. lr: 0.000226. clip: 0.090406\n",
      "Iteration 3199: Policy loss: 0.005723. Value loss: 0.048469. Entropy: 0.767882.\n",
      "Iteration 3200: Policy loss: -0.010406. Value loss: 0.038088. Entropy: 0.777134.\n",
      "Iteration 3201: Policy loss: -0.018621. Value loss: 0.032220. Entropy: 0.766319.\n",
      "episode: 2536   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 782     evaluation reward: 9.03\n",
      "episode: 2537   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 668     evaluation reward: 9.05\n",
      "Training network. lr: 0.000226. clip: 0.090397\n",
      "Iteration 3202: Policy loss: 0.009135. Value loss: 0.023897. Entropy: 0.795489.\n",
      "Iteration 3203: Policy loss: -0.010711. Value loss: 0.018061. Entropy: 0.788020.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3204: Policy loss: -0.021999. Value loss: 0.015004. Entropy: 0.772888.\n",
      "episode: 2538   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 343     evaluation reward: 8.95\n",
      "episode: 2539   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 394     evaluation reward: 8.89\n",
      "Training network. lr: 0.000226. clip: 0.090388\n",
      "Iteration 3205: Policy loss: 0.013046. Value loss: 0.034495. Entropy: 0.774739.\n",
      "Iteration 3206: Policy loss: -0.006632. Value loss: 0.025181. Entropy: 0.790038.\n",
      "Iteration 3207: Policy loss: -0.022835. Value loss: 0.020425. Entropy: 0.780043.\n",
      "episode: 2540   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 545     evaluation reward: 8.9\n",
      "episode: 2541   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 466     evaluation reward: 8.82\n",
      "Training network. lr: 0.000226. clip: 0.090379\n",
      "Iteration 3208: Policy loss: 0.004899. Value loss: 0.021334. Entropy: 0.805297.\n",
      "Iteration 3209: Policy loss: -0.007610. Value loss: 0.016301. Entropy: 0.805906.\n",
      "Iteration 3210: Policy loss: -0.020211. Value loss: 0.013707. Entropy: 0.799709.\n",
      "episode: 2542   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 472     evaluation reward: 8.83\n",
      "episode: 2543   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 407     evaluation reward: 8.79\n",
      "Training network. lr: 0.000226. clip: 0.090370\n",
      "Iteration 3211: Policy loss: 0.005594. Value loss: 0.033217. Entropy: 0.798254.\n",
      "Iteration 3212: Policy loss: -0.009815. Value loss: 0.023167. Entropy: 0.791030.\n",
      "Iteration 3213: Policy loss: -0.021751. Value loss: 0.019741. Entropy: 0.787740.\n",
      "episode: 2544   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 516     evaluation reward: 8.79\n",
      "episode: 2545   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 610     evaluation reward: 8.81\n",
      "Training network. lr: 0.000226. clip: 0.090361\n",
      "Iteration 3214: Policy loss: 0.012520. Value loss: 0.020707. Entropy: 0.802986.\n",
      "Iteration 3215: Policy loss: -0.011889. Value loss: 0.015888. Entropy: 0.798074.\n",
      "Iteration 3216: Policy loss: -0.023155. Value loss: 0.013822. Entropy: 0.801820.\n",
      "episode: 2546   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 672     evaluation reward: 8.85\n",
      "episode: 2547   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 558     evaluation reward: 8.85\n",
      "Training network. lr: 0.000226. clip: 0.090352\n",
      "Iteration 3217: Policy loss: 0.004166. Value loss: 0.023420. Entropy: 0.786339.\n",
      "Iteration 3218: Policy loss: -0.012174. Value loss: 0.016259. Entropy: 0.783586.\n",
      "Iteration 3219: Policy loss: -0.020725. Value loss: 0.014258. Entropy: 0.784619.\n",
      "episode: 2548   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 398     evaluation reward: 8.88\n",
      "episode: 2549   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 604     evaluation reward: 8.88\n",
      "Training network. lr: 0.000226. clip: 0.090343\n",
      "Iteration 3220: Policy loss: 0.010491. Value loss: 0.026429. Entropy: 0.740808.\n",
      "Iteration 3221: Policy loss: -0.007132. Value loss: 0.018462. Entropy: 0.733091.\n",
      "Iteration 3222: Policy loss: -0.020973. Value loss: 0.015907. Entropy: 0.734247.\n",
      "now time :  2018-12-26 13:44:15.744926\n",
      "episode: 2550   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 495     evaluation reward: 8.9\n",
      "episode: 2551   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 645     evaluation reward: 8.88\n",
      "Training network. lr: 0.000226. clip: 0.090334\n",
      "Iteration 3223: Policy loss: 0.009003. Value loss: 0.016248. Entropy: 0.816653.\n",
      "Iteration 3224: Policy loss: -0.010106. Value loss: 0.012535. Entropy: 0.814712.\n",
      "Iteration 3225: Policy loss: -0.020253. Value loss: 0.009921. Entropy: 0.810735.\n",
      "episode: 2552   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 536     evaluation reward: 8.9\n",
      "Training network. lr: 0.000226. clip: 0.090325\n",
      "Iteration 3226: Policy loss: 0.010948. Value loss: 0.022117. Entropy: 0.792429.\n",
      "Iteration 3227: Policy loss: -0.008880. Value loss: 0.016859. Entropy: 0.799742.\n",
      "Iteration 3228: Policy loss: -0.023282. Value loss: 0.014046. Entropy: 0.795697.\n",
      "episode: 2553   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 664     evaluation reward: 8.92\n",
      "episode: 2554   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 724     evaluation reward: 8.98\n",
      "Training network. lr: 0.000226. clip: 0.090316\n",
      "Iteration 3229: Policy loss: 0.010018. Value loss: 0.023287. Entropy: 0.662904.\n",
      "Iteration 3230: Policy loss: -0.006070. Value loss: 0.015866. Entropy: 0.650084.\n",
      "Iteration 3231: Policy loss: -0.018989. Value loss: 0.013696. Entropy: 0.633969.\n",
      "episode: 2555   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 448     evaluation reward: 8.98\n",
      "episode: 2556   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 529     evaluation reward: 8.95\n",
      "Training network. lr: 0.000226. clip: 0.090307\n",
      "Iteration 3232: Policy loss: 0.007225. Value loss: 0.019740. Entropy: 0.769951.\n",
      "Iteration 3233: Policy loss: -0.005182. Value loss: 0.015482. Entropy: 0.780907.\n",
      "Iteration 3234: Policy loss: -0.017526. Value loss: 0.014180. Entropy: 0.774839.\n",
      "episode: 2557   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 615     evaluation reward: 8.98\n",
      "Training network. lr: 0.000226. clip: 0.090298\n",
      "Iteration 3235: Policy loss: 0.010377. Value loss: 0.020619. Entropy: 0.717981.\n",
      "Iteration 3236: Policy loss: -0.010194. Value loss: 0.014704. Entropy: 0.725787.\n",
      "Iteration 3237: Policy loss: -0.023088. Value loss: 0.012617. Entropy: 0.714506.\n",
      "episode: 2558   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 818     evaluation reward: 9.06\n",
      "episode: 2559   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 647     evaluation reward: 9.06\n",
      "Training network. lr: 0.000226. clip: 0.090289\n",
      "Iteration 3238: Policy loss: 0.004165. Value loss: 0.024354. Entropy: 0.665192.\n",
      "Iteration 3239: Policy loss: -0.008805. Value loss: 0.013709. Entropy: 0.664605.\n",
      "Iteration 3240: Policy loss: -0.022378. Value loss: 0.012399. Entropy: 0.669359.\n",
      "episode: 2560   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 301     evaluation reward: 9.0\n",
      "episode: 2561   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 491     evaluation reward: 8.97\n",
      "episode: 2562   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 375     evaluation reward: 8.91\n",
      "Training network. lr: 0.000226. clip: 0.090280\n",
      "Iteration 3241: Policy loss: 0.004517. Value loss: 0.027811. Entropy: 0.661417.\n",
      "Iteration 3242: Policy loss: -0.013114. Value loss: 0.021024. Entropy: 0.648328.\n",
      "Iteration 3243: Policy loss: -0.024419. Value loss: 0.016701. Entropy: 0.643690.\n",
      "episode: 2563   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 626     evaluation reward: 8.83\n",
      "episode: 2564   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 476     evaluation reward: 8.79\n",
      "Training network. lr: 0.000226. clip: 0.090271\n",
      "Iteration 3244: Policy loss: 0.008623. Value loss: 0.020099. Entropy: 0.709392.\n",
      "Iteration 3245: Policy loss: -0.012679. Value loss: 0.014575. Entropy: 0.704895.\n",
      "Iteration 3246: Policy loss: -0.019792. Value loss: 0.012124. Entropy: 0.708572.\n",
      "episode: 2565   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 502     evaluation reward: 8.75\n",
      "Training network. lr: 0.000226. clip: 0.090262\n",
      "Iteration 3247: Policy loss: 0.002796. Value loss: 0.020382. Entropy: 0.715290.\n",
      "Iteration 3248: Policy loss: -0.015685. Value loss: 0.015604. Entropy: 0.711571.\n",
      "Iteration 3249: Policy loss: -0.023931. Value loss: 0.013226. Entropy: 0.709921.\n",
      "episode: 2566   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 562     evaluation reward: 8.75\n",
      "episode: 2567   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 510     evaluation reward: 8.74\n",
      "Training network. lr: 0.000226. clip: 0.090253\n",
      "Iteration 3250: Policy loss: 0.009298. Value loss: 0.020900. Entropy: 0.739547.\n",
      "Iteration 3251: Policy loss: -0.011408. Value loss: 0.016321. Entropy: 0.759251.\n",
      "Iteration 3252: Policy loss: -0.022063. Value loss: 0.013881. Entropy: 0.756490.\n",
      "episode: 2568   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 641     evaluation reward: 8.75\n",
      "episode: 2569   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 605     evaluation reward: 8.7\n",
      "Training network. lr: 0.000226. clip: 0.090244\n",
      "Iteration 3253: Policy loss: 0.006234. Value loss: 0.015995. Entropy: 0.678829.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3254: Policy loss: -0.008389. Value loss: 0.013113. Entropy: 0.682747.\n",
      "Iteration 3255: Policy loss: -0.017940. Value loss: 0.011215. Entropy: 0.682351.\n",
      "episode: 2570   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 470     evaluation reward: 8.62\n",
      "episode: 2571   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 655     evaluation reward: 8.64\n",
      "Training network. lr: 0.000226. clip: 0.090235\n",
      "Iteration 3256: Policy loss: 0.008667. Value loss: 0.015401. Entropy: 0.675977.\n",
      "Iteration 3257: Policy loss: -0.011681. Value loss: 0.011564. Entropy: 0.669910.\n",
      "Iteration 3258: Policy loss: -0.024355. Value loss: 0.009475. Entropy: 0.661044.\n",
      "episode: 2572   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 586     evaluation reward: 8.63\n",
      "episode: 2573   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 561     evaluation reward: 8.63\n",
      "Training network. lr: 0.000226. clip: 0.090226\n",
      "Iteration 3259: Policy loss: 0.007571. Value loss: 0.019868. Entropy: 0.774190.\n",
      "Iteration 3260: Policy loss: -0.012889. Value loss: 0.015362. Entropy: 0.767470.\n",
      "Iteration 3261: Policy loss: -0.025097. Value loss: 0.012909. Entropy: 0.762027.\n",
      "episode: 2574   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 826     evaluation reward: 8.68\n",
      "Training network. lr: 0.000226. clip: 0.090217\n",
      "Iteration 3262: Policy loss: 0.005238. Value loss: 0.018279. Entropy: 0.712871.\n",
      "Iteration 3263: Policy loss: -0.008332. Value loss: 0.012719. Entropy: 0.712374.\n",
      "Iteration 3264: Policy loss: -0.023615. Value loss: 0.010276. Entropy: 0.708481.\n",
      "episode: 2575   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 599     evaluation reward: 8.72\n",
      "episode: 2576   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 556     evaluation reward: 8.77\n",
      "Training network. lr: 0.000226. clip: 0.090208\n",
      "Iteration 3265: Policy loss: 0.009956. Value loss: 0.017845. Entropy: 0.716197.\n",
      "Iteration 3266: Policy loss: -0.008339. Value loss: 0.013400. Entropy: 0.711924.\n",
      "Iteration 3267: Policy loss: -0.022074. Value loss: 0.010301. Entropy: 0.710942.\n",
      "episode: 2577   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 489     evaluation reward: 8.77\n",
      "episode: 2578   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 536     evaluation reward: 8.79\n",
      "Training network. lr: 0.000225. clip: 0.090199\n",
      "Iteration 3268: Policy loss: 0.007682. Value loss: 0.023782. Entropy: 0.731155.\n",
      "Iteration 3269: Policy loss: -0.011494. Value loss: 0.018169. Entropy: 0.726917.\n",
      "Iteration 3270: Policy loss: -0.020043. Value loss: 0.015156. Entropy: 0.730832.\n",
      "episode: 2579   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 651     evaluation reward: 8.81\n",
      "Training network. lr: 0.000225. clip: 0.090190\n",
      "Iteration 3271: Policy loss: 0.012794. Value loss: 0.016343. Entropy: 0.673401.\n",
      "Iteration 3272: Policy loss: -0.012201. Value loss: 0.012296. Entropy: 0.665487.\n",
      "Iteration 3273: Policy loss: -0.021576. Value loss: 0.010700. Entropy: 0.664564.\n",
      "episode: 2580   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 654     evaluation reward: 8.82\n",
      "episode: 2581   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 714     evaluation reward: 8.87\n",
      "Training network. lr: 0.000225. clip: 0.090181\n",
      "Iteration 3274: Policy loss: 0.009032. Value loss: 0.016929. Entropy: 0.730450.\n",
      "Iteration 3275: Policy loss: -0.006908. Value loss: 0.012640. Entropy: 0.727997.\n",
      "Iteration 3276: Policy loss: -0.013851. Value loss: 0.010414. Entropy: 0.727225.\n",
      "episode: 2582   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 801     evaluation reward: 8.89\n",
      "Training network. lr: 0.000225. clip: 0.090172\n",
      "Iteration 3277: Policy loss: 0.009145. Value loss: 0.028135. Entropy: 0.746525.\n",
      "Iteration 3278: Policy loss: -0.009697. Value loss: 0.018407. Entropy: 0.739751.\n",
      "Iteration 3279: Policy loss: -0.021398. Value loss: 0.014302. Entropy: 0.730204.\n",
      "episode: 2583   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 630     evaluation reward: 8.87\n",
      "episode: 2584   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 640     evaluation reward: 8.91\n",
      "Training network. lr: 0.000225. clip: 0.090163\n",
      "Iteration 3280: Policy loss: 0.008237. Value loss: 0.022723. Entropy: 0.681480.\n",
      "Iteration 3281: Policy loss: -0.010588. Value loss: 0.016605. Entropy: 0.678003.\n",
      "Iteration 3282: Policy loss: -0.021035. Value loss: 0.014891. Entropy: 0.667842.\n",
      "episode: 2585   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 585     evaluation reward: 8.85\n",
      "episode: 2586   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 502     evaluation reward: 8.82\n",
      "Training network. lr: 0.000225. clip: 0.090154\n",
      "Iteration 3283: Policy loss: 0.001583. Value loss: 0.023126. Entropy: 0.701988.\n",
      "Iteration 3284: Policy loss: -0.009332. Value loss: 0.017220. Entropy: 0.696693.\n",
      "Iteration 3285: Policy loss: -0.022747. Value loss: 0.015413. Entropy: 0.691346.\n",
      "episode: 2587   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 648     evaluation reward: 8.8\n",
      "Training network. lr: 0.000225. clip: 0.090145\n",
      "Iteration 3286: Policy loss: 0.006534. Value loss: 0.019908. Entropy: 0.681995.\n",
      "Iteration 3287: Policy loss: -0.015021. Value loss: 0.015134. Entropy: 0.683450.\n",
      "Iteration 3288: Policy loss: -0.022669. Value loss: 0.013911. Entropy: 0.686124.\n",
      "episode: 2588   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 432     evaluation reward: 8.8\n",
      "episode: 2589   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 623     evaluation reward: 8.8\n",
      "Training network. lr: 0.000225. clip: 0.090136\n",
      "Iteration 3289: Policy loss: 0.008524. Value loss: 0.020863. Entropy: 0.728920.\n",
      "Iteration 3290: Policy loss: -0.012919. Value loss: 0.015337. Entropy: 0.728399.\n",
      "Iteration 3291: Policy loss: -0.025832. Value loss: 0.012537. Entropy: 0.724919.\n",
      "episode: 2590   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 575     evaluation reward: 8.77\n",
      "episode: 2591   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 317     evaluation reward: 8.7\n",
      "Training network. lr: 0.000225. clip: 0.090127\n",
      "Iteration 3292: Policy loss: 0.004209. Value loss: 0.053331. Entropy: 0.760604.\n",
      "Iteration 3293: Policy loss: -0.004787. Value loss: 0.040347. Entropy: 0.755534.\n",
      "Iteration 3294: Policy loss: -0.015039. Value loss: 0.033106. Entropy: 0.754160.\n",
      "episode: 2592   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 594     evaluation reward: 8.73\n",
      "episode: 2593   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 489     evaluation reward: 8.71\n",
      "Training network. lr: 0.000225. clip: 0.090118\n",
      "Iteration 3295: Policy loss: 0.006896. Value loss: 0.028508. Entropy: 0.723596.\n",
      "Iteration 3296: Policy loss: -0.011417. Value loss: 0.021583. Entropy: 0.714588.\n",
      "Iteration 3297: Policy loss: -0.020261. Value loss: 0.015206. Entropy: 0.710423.\n",
      "episode: 2594   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 533     evaluation reward: 8.64\n",
      "episode: 2595   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 710     evaluation reward: 8.65\n",
      "Training network. lr: 0.000225. clip: 0.090109\n",
      "Iteration 3298: Policy loss: 0.008088. Value loss: 0.020027. Entropy: 0.701688.\n",
      "Iteration 3299: Policy loss: -0.008549. Value loss: 0.014922. Entropy: 0.686628.\n",
      "Iteration 3300: Policy loss: -0.023363. Value loss: 0.012804. Entropy: 0.685789.\n",
      "episode: 2596   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 670     evaluation reward: 8.66\n",
      "Training network. lr: 0.000225. clip: 0.090100\n",
      "Iteration 3301: Policy loss: 0.008856. Value loss: 0.015874. Entropy: 0.718293.\n",
      "Iteration 3302: Policy loss: -0.012863. Value loss: 0.012403. Entropy: 0.712216.\n",
      "Iteration 3303: Policy loss: -0.023953. Value loss: 0.010334. Entropy: 0.709668.\n",
      "episode: 2597   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 700     evaluation reward: 8.67\n",
      "episode: 2598   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 675     evaluation reward: 8.7\n",
      "Training network. lr: 0.000225. clip: 0.090091\n",
      "Iteration 3304: Policy loss: 0.002351. Value loss: 0.018490. Entropy: 0.686835.\n",
      "Iteration 3305: Policy loss: -0.011561. Value loss: 0.013859. Entropy: 0.682343.\n",
      "Iteration 3306: Policy loss: -0.017755. Value loss: 0.013686. Entropy: 0.675091.\n",
      "episode: 2599   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 431     evaluation reward: 8.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2600   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 435     evaluation reward: 8.65\n",
      "episode: 2601   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 405     evaluation reward: 8.62\n",
      "Training network. lr: 0.000225. clip: 0.090082\n",
      "Iteration 3307: Policy loss: 0.006439. Value loss: 0.030938. Entropy: 0.660282.\n",
      "Iteration 3308: Policy loss: -0.012364. Value loss: 0.023354. Entropy: 0.651728.\n",
      "Iteration 3309: Policy loss: -0.014785. Value loss: 0.019096. Entropy: 0.657106.\n",
      "episode: 2602   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 459     evaluation reward: 8.54\n",
      "Training network. lr: 0.000225. clip: 0.090073\n",
      "Iteration 3310: Policy loss: 0.002737. Value loss: 0.018275. Entropy: 0.692276.\n",
      "Iteration 3311: Policy loss: -0.015831. Value loss: 0.012402. Entropy: 0.683721.\n",
      "Iteration 3312: Policy loss: -0.025466. Value loss: 0.010213. Entropy: 0.685232.\n",
      "episode: 2603   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 605     evaluation reward: 8.56\n",
      "episode: 2604   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 694     evaluation reward: 8.63\n",
      "Training network. lr: 0.000225. clip: 0.090064\n",
      "Iteration 3313: Policy loss: 0.007551. Value loss: 0.042777. Entropy: 0.620610.\n",
      "Iteration 3314: Policy loss: -0.007427. Value loss: 0.035112. Entropy: 0.629268.\n",
      "Iteration 3315: Policy loss: -0.015084. Value loss: 0.027830. Entropy: 0.623601.\n",
      "episode: 2605   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 338     evaluation reward: 8.64\n",
      "episode: 2606   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 679     evaluation reward: 8.65\n",
      "Training network. lr: 0.000225. clip: 0.090055\n",
      "Iteration 3316: Policy loss: 0.007602. Value loss: 0.034558. Entropy: 0.654549.\n",
      "Iteration 3317: Policy loss: -0.011478. Value loss: 0.028228. Entropy: 0.652575.\n",
      "Iteration 3318: Policy loss: -0.022262. Value loss: 0.024419. Entropy: 0.654859.\n",
      "episode: 2607   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 578     evaluation reward: 8.66\n",
      "episode: 2608   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 510     evaluation reward: 8.66\n",
      "Training network. lr: 0.000225. clip: 0.090046\n",
      "Iteration 3319: Policy loss: 0.010306. Value loss: 0.027378. Entropy: 0.712965.\n",
      "Iteration 3320: Policy loss: -0.003327. Value loss: 0.018856. Entropy: 0.717102.\n",
      "Iteration 3321: Policy loss: -0.014706. Value loss: 0.017678. Entropy: 0.704207.\n",
      "episode: 2609   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 612     evaluation reward: 8.59\n",
      "episode: 2610   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 639     evaluation reward: 8.56\n",
      "Training network. lr: 0.000225. clip: 0.090037\n",
      "Iteration 3322: Policy loss: 0.010725. Value loss: 0.030409. Entropy: 0.687492.\n",
      "Iteration 3323: Policy loss: -0.007221. Value loss: 0.020344. Entropy: 0.700606.\n",
      "Iteration 3324: Policy loss: -0.018703. Value loss: 0.017237. Entropy: 0.700280.\n",
      "episode: 2611   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 639     evaluation reward: 8.53\n",
      "Training network. lr: 0.000225. clip: 0.090028\n",
      "Iteration 3325: Policy loss: 0.011027. Value loss: 0.021009. Entropy: 0.683777.\n",
      "Iteration 3326: Policy loss: -0.011894. Value loss: 0.014488. Entropy: 0.682217.\n",
      "Iteration 3327: Policy loss: -0.021419. Value loss: 0.012343. Entropy: 0.674645.\n",
      "episode: 2612   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 513     evaluation reward: 8.5\n",
      "episode: 2613   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 556     evaluation reward: 8.47\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3328: Policy loss: 0.010027. Value loss: 0.028840. Entropy: 0.705067.\n",
      "Iteration 3329: Policy loss: -0.006572. Value loss: 0.020899. Entropy: 0.706427.\n",
      "Iteration 3330: Policy loss: -0.020276. Value loss: 0.018064. Entropy: 0.698031.\n",
      "episode: 2614   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 675     evaluation reward: 8.48\n",
      "episode: 2615   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 629     evaluation reward: 8.43\n",
      "Training network. lr: 0.000225. clip: 0.090010\n",
      "Iteration 3331: Policy loss: 0.008345. Value loss: 0.021013. Entropy: 0.623868.\n",
      "Iteration 3332: Policy loss: -0.011103. Value loss: 0.014983. Entropy: 0.631598.\n",
      "Iteration 3333: Policy loss: -0.019932. Value loss: 0.012439. Entropy: 0.635422.\n",
      "episode: 2616   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 505     evaluation reward: 8.41\n",
      "episode: 2617   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 394     evaluation reward: 8.42\n",
      "Training network. lr: 0.000225. clip: 0.090001\n",
      "Iteration 3334: Policy loss: 0.004910. Value loss: 0.045811. Entropy: 0.710555.\n",
      "Iteration 3335: Policy loss: -0.003780. Value loss: 0.033958. Entropy: 0.705152.\n",
      "Iteration 3336: Policy loss: -0.014588. Value loss: 0.031393. Entropy: 0.707456.\n",
      "episode: 2618   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 572     evaluation reward: 8.47\n",
      "Training network. lr: 0.000225. clip: 0.089992\n",
      "Iteration 3337: Policy loss: 0.010425. Value loss: 0.021707. Entropy: 0.638879.\n",
      "Iteration 3338: Policy loss: -0.002123. Value loss: 0.017299. Entropy: 0.642931.\n",
      "Iteration 3339: Policy loss: -0.019316. Value loss: 0.014255. Entropy: 0.640472.\n",
      "episode: 2619   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 665     evaluation reward: 8.51\n",
      "episode: 2620   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 728     evaluation reward: 8.53\n",
      "Training network. lr: 0.000225. clip: 0.089983\n",
      "Iteration 3340: Policy loss: 0.006938. Value loss: 0.050015. Entropy: 0.759656.\n",
      "Iteration 3341: Policy loss: -0.007294. Value loss: 0.036988. Entropy: 0.747580.\n",
      "Iteration 3342: Policy loss: -0.017757. Value loss: 0.031673. Entropy: 0.749459.\n",
      "episode: 2621   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 297     evaluation reward: 8.53\n",
      "episode: 2622   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 608     evaluation reward: 8.48\n",
      "Training network. lr: 0.000225. clip: 0.089974\n",
      "Iteration 3343: Policy loss: 0.014793. Value loss: 0.035908. Entropy: 0.627890.\n",
      "Iteration 3344: Policy loss: -0.007558. Value loss: 0.026621. Entropy: 0.632446.\n",
      "Iteration 3345: Policy loss: -0.020201. Value loss: 0.022608. Entropy: 0.631642.\n",
      "episode: 2623   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 572     evaluation reward: 8.44\n",
      "episode: 2624   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 483     evaluation reward: 8.31\n",
      "Training network. lr: 0.000225. clip: 0.089965\n",
      "Iteration 3346: Policy loss: 0.009458. Value loss: 0.027212. Entropy: 0.708323.\n",
      "Iteration 3347: Policy loss: -0.006713. Value loss: 0.020492. Entropy: 0.704718.\n",
      "Iteration 3348: Policy loss: -0.015914. Value loss: 0.018309. Entropy: 0.701922.\n",
      "episode: 2625   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 8.25\n",
      "episode: 2626   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 545     evaluation reward: 8.24\n",
      "Training network. lr: 0.000225. clip: 0.089956\n",
      "Iteration 3349: Policy loss: 0.007597. Value loss: 0.022235. Entropy: 0.690248.\n",
      "Iteration 3350: Policy loss: -0.007275. Value loss: 0.016521. Entropy: 0.690622.\n",
      "Iteration 3351: Policy loss: -0.017040. Value loss: 0.014374. Entropy: 0.686330.\n",
      "episode: 2627   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 505     evaluation reward: 8.25\n",
      "episode: 2628   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 759     evaluation reward: 8.3\n",
      "Training network. lr: 0.000225. clip: 0.089947\n",
      "Iteration 3352: Policy loss: 0.004122. Value loss: 0.021910. Entropy: 0.681733.\n",
      "Iteration 3353: Policy loss: -0.010074. Value loss: 0.016146. Entropy: 0.664060.\n",
      "Iteration 3354: Policy loss: -0.017937. Value loss: 0.013748. Entropy: 0.677122.\n",
      "episode: 2629   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 821     evaluation reward: 8.21\n",
      "Training network. lr: 0.000225. clip: 0.089938\n",
      "Iteration 3355: Policy loss: 0.007637. Value loss: 0.026217. Entropy: 0.677071.\n",
      "Iteration 3356: Policy loss: -0.010253. Value loss: 0.019210. Entropy: 0.668060.\n",
      "Iteration 3357: Policy loss: -0.023495. Value loss: 0.015867. Entropy: 0.659308.\n",
      "episode: 2630   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 521     evaluation reward: 8.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2631   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 626     evaluation reward: 8.2\n",
      "Training network. lr: 0.000225. clip: 0.089929\n",
      "Iteration 3358: Policy loss: 0.007925. Value loss: 0.038777. Entropy: 0.693948.\n",
      "Iteration 3359: Policy loss: -0.007890. Value loss: 0.032065. Entropy: 0.679016.\n",
      "Iteration 3360: Policy loss: -0.014230. Value loss: 0.028580. Entropy: 0.683634.\n",
      "episode: 2632   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 715     evaluation reward: 8.19\n",
      "Training network. lr: 0.000225. clip: 0.089920\n",
      "Iteration 3361: Policy loss: 0.011826. Value loss: 0.017379. Entropy: 0.749569.\n",
      "Iteration 3362: Policy loss: -0.007333. Value loss: 0.015819. Entropy: 0.753205.\n",
      "Iteration 3363: Policy loss: -0.019475. Value loss: 0.014346. Entropy: 0.753244.\n",
      "episode: 2633   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 622     evaluation reward: 8.17\n",
      "episode: 2634   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 574     evaluation reward: 8.16\n",
      "Training network. lr: 0.000225. clip: 0.089911\n",
      "Iteration 3364: Policy loss: 0.003404. Value loss: 0.018437. Entropy: 0.766172.\n",
      "Iteration 3365: Policy loss: -0.013540. Value loss: 0.014181. Entropy: 0.755922.\n",
      "Iteration 3366: Policy loss: -0.024639. Value loss: 0.011661. Entropy: 0.760006.\n",
      "episode: 2635   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 775     evaluation reward: 8.18\n",
      "Training network. lr: 0.000225. clip: 0.089902\n",
      "Iteration 3367: Policy loss: 0.004668. Value loss: 0.019198. Entropy: 0.747605.\n",
      "Iteration 3368: Policy loss: -0.011152. Value loss: 0.013951. Entropy: 0.741223.\n",
      "Iteration 3369: Policy loss: -0.019186. Value loss: 0.011070. Entropy: 0.732267.\n",
      "episode: 2636   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 628     evaluation reward: 8.15\n",
      "now time :  2018-12-26 13:49:11.537720\n",
      "episode: 2637   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 580     evaluation reward: 8.14\n",
      "Training network. lr: 0.000225. clip: 0.089893\n",
      "Iteration 3370: Policy loss: 0.011936. Value loss: 0.039132. Entropy: 0.742718.\n",
      "Iteration 3371: Policy loss: -0.001920. Value loss: 0.031824. Entropy: 0.734922.\n",
      "Iteration 3372: Policy loss: -0.012220. Value loss: 0.028202. Entropy: 0.737266.\n",
      "episode: 2638   score: 19.0   memory length: 1024   epsilon: 1.0    steps: 574     evaluation reward: 8.29\n",
      "episode: 2639   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 616     evaluation reward: 8.34\n",
      "Training network. lr: 0.000225. clip: 0.089884\n",
      "Iteration 3373: Policy loss: 0.007778. Value loss: 0.081350. Entropy: 0.710945.\n",
      "Iteration 3374: Policy loss: -0.009922. Value loss: 0.057853. Entropy: 0.702342.\n",
      "Iteration 3375: Policy loss: -0.015127. Value loss: 0.047103. Entropy: 0.701479.\n",
      "episode: 2640   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 8.32\n",
      "episode: 2641   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 559     evaluation reward: 8.34\n",
      "Training network. lr: 0.000225. clip: 0.089875\n",
      "Iteration 3376: Policy loss: 0.009613. Value loss: 0.020518. Entropy: 0.627799.\n",
      "Iteration 3377: Policy loss: -0.010262. Value loss: 0.017457. Entropy: 0.617633.\n",
      "Iteration 3378: Policy loss: -0.018224. Value loss: 0.014052. Entropy: 0.613659.\n",
      "episode: 2642   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 507     evaluation reward: 8.34\n",
      "episode: 2643   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 470     evaluation reward: 8.37\n",
      "Training network. lr: 0.000225. clip: 0.089866\n",
      "Iteration 3379: Policy loss: 0.005040. Value loss: 0.032306. Entropy: 0.716780.\n",
      "Iteration 3380: Policy loss: -0.008763. Value loss: 0.025788. Entropy: 0.719404.\n",
      "Iteration 3381: Policy loss: -0.020665. Value loss: 0.019635. Entropy: 0.711209.\n",
      "episode: 2644   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 549     evaluation reward: 8.37\n",
      "episode: 2645   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 708     evaluation reward: 8.39\n",
      "Training network. lr: 0.000225. clip: 0.089857\n",
      "Iteration 3382: Policy loss: 0.006118. Value loss: 0.029325. Entropy: 0.725829.\n",
      "Iteration 3383: Policy loss: -0.004724. Value loss: 0.019304. Entropy: 0.721279.\n",
      "Iteration 3384: Policy loss: -0.015288. Value loss: 0.017007. Entropy: 0.704419.\n",
      "episode: 2646   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 643     evaluation reward: 8.38\n",
      "Training network. lr: 0.000225. clip: 0.089848\n",
      "Iteration 3385: Policy loss: 0.006064. Value loss: 0.025303. Entropy: 0.739919.\n",
      "Iteration 3386: Policy loss: -0.010713. Value loss: 0.019478. Entropy: 0.748873.\n",
      "Iteration 3387: Policy loss: -0.020573. Value loss: 0.016049. Entropy: 0.750350.\n",
      "episode: 2647   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 586     evaluation reward: 8.4\n",
      "episode: 2648   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 562     evaluation reward: 8.42\n",
      "Training network. lr: 0.000225. clip: 0.089839\n",
      "Iteration 3388: Policy loss: 0.008450. Value loss: 0.019625. Entropy: 0.635559.\n",
      "Iteration 3389: Policy loss: -0.007896. Value loss: 0.014366. Entropy: 0.633511.\n",
      "Iteration 3390: Policy loss: -0.020945. Value loss: 0.012527. Entropy: 0.628917.\n",
      "episode: 2649   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 541     evaluation reward: 8.41\n",
      "episode: 2650   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 576     evaluation reward: 8.42\n",
      "Training network. lr: 0.000225. clip: 0.089830\n",
      "Iteration 3391: Policy loss: 0.004328. Value loss: 0.018756. Entropy: 0.718592.\n",
      "Iteration 3392: Policy loss: -0.004582. Value loss: 0.013055. Entropy: 0.699358.\n",
      "Iteration 3393: Policy loss: -0.017383. Value loss: 0.011031. Entropy: 0.713341.\n",
      "episode: 2651   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 563     evaluation reward: 8.41\n",
      "episode: 2652   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 520     evaluation reward: 8.4\n",
      "Training network. lr: 0.000225. clip: 0.089821\n",
      "Iteration 3394: Policy loss: 0.001589. Value loss: 0.016147. Entropy: 0.676625.\n",
      "Iteration 3395: Policy loss: -0.006456. Value loss: 0.013870. Entropy: 0.673082.\n",
      "Iteration 3396: Policy loss: -0.016317. Value loss: 0.011605. Entropy: 0.676245.\n",
      "episode: 2653   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 424     evaluation reward: 8.36\n",
      "episode: 2654   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 491     evaluation reward: 8.31\n",
      "Training network. lr: 0.000225. clip: 0.089812\n",
      "Iteration 3397: Policy loss: 0.012748. Value loss: 0.022228. Entropy: 0.711937.\n",
      "Iteration 3398: Policy loss: -0.008441. Value loss: 0.015786. Entropy: 0.700010.\n",
      "Iteration 3399: Policy loss: -0.018837. Value loss: 0.012646. Entropy: 0.697085.\n",
      "episode: 2655   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 579     evaluation reward: 8.33\n",
      "episode: 2656   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 8.3\n",
      "Training network. lr: 0.000225. clip: 0.089803\n",
      "Iteration 3400: Policy loss: 0.007947. Value loss: 0.019971. Entropy: 0.649592.\n",
      "Iteration 3401: Policy loss: -0.003352. Value loss: 0.015200. Entropy: 0.657038.\n",
      "Iteration 3402: Policy loss: -0.011308. Value loss: 0.012932. Entropy: 0.653416.\n",
      "episode: 2657   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 550     evaluation reward: 8.3\n",
      "episode: 2658   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 446     evaluation reward: 8.22\n",
      "Training network. lr: 0.000224. clip: 0.089794\n",
      "Iteration 3403: Policy loss: 0.016089. Value loss: 0.026178. Entropy: 0.624267.\n",
      "Iteration 3404: Policy loss: -0.003202. Value loss: 0.020251. Entropy: 0.617197.\n",
      "Iteration 3405: Policy loss: -0.008430. Value loss: 0.016794. Entropy: 0.598993.\n",
      "episode: 2659   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 687     evaluation reward: 8.22\n",
      "episode: 2660   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 568     evaluation reward: 8.28\n",
      "Training network. lr: 0.000224. clip: 0.089785\n",
      "Iteration 3406: Policy loss: 0.011054. Value loss: 0.015205. Entropy: 0.649099.\n",
      "Iteration 3407: Policy loss: -0.007556. Value loss: 0.011491. Entropy: 0.636718.\n",
      "Iteration 3408: Policy loss: -0.019134. Value loss: 0.009635. Entropy: 0.629041.\n",
      "episode: 2661   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 466     evaluation reward: 8.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000224. clip: 0.089776\n",
      "Iteration 3409: Policy loss: 0.007878. Value loss: 0.050798. Entropy: 0.703179.\n",
      "Iteration 3410: Policy loss: -0.006436. Value loss: 0.039507. Entropy: 0.712100.\n",
      "Iteration 3411: Policy loss: -0.015357. Value loss: 0.032828. Entropy: 0.696039.\n",
      "episode: 2662   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 695     evaluation reward: 8.36\n",
      "episode: 2663   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 805     evaluation reward: 8.39\n",
      "Training network. lr: 0.000224. clip: 0.089767\n",
      "Iteration 3412: Policy loss: 0.005877. Value loss: 0.021424. Entropy: 0.650977.\n",
      "Iteration 3413: Policy loss: -0.008665. Value loss: 0.015686. Entropy: 0.634467.\n",
      "Iteration 3414: Policy loss: -0.016869. Value loss: 0.013769. Entropy: 0.645113.\n",
      "episode: 2664   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 674     evaluation reward: 8.42\n",
      "Training network. lr: 0.000224. clip: 0.089758\n",
      "Iteration 3415: Policy loss: 0.004840. Value loss: 0.021185. Entropy: 0.727569.\n",
      "Iteration 3416: Policy loss: -0.009838. Value loss: 0.016778. Entropy: 0.736951.\n",
      "Iteration 3417: Policy loss: -0.017711. Value loss: 0.013398. Entropy: 0.750658.\n",
      "episode: 2665   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 834     evaluation reward: 8.51\n",
      "Training network. lr: 0.000224. clip: 0.089749\n",
      "Iteration 3418: Policy loss: 0.007117. Value loss: 0.041988. Entropy: 0.673921.\n",
      "Iteration 3419: Policy loss: -0.003212. Value loss: 0.032329. Entropy: 0.667100.\n",
      "Iteration 3420: Policy loss: -0.001221. Value loss: 0.027958. Entropy: 0.665602.\n",
      "episode: 2666   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 734     evaluation reward: 8.54\n",
      "episode: 2667   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 572     evaluation reward: 8.55\n",
      "Training network. lr: 0.000224. clip: 0.089740\n",
      "Iteration 3421: Policy loss: 0.002940. Value loss: 0.018268. Entropy: 0.679700.\n",
      "Iteration 3422: Policy loss: -0.011140. Value loss: 0.013777. Entropy: 0.678355.\n",
      "Iteration 3423: Policy loss: -0.017136. Value loss: 0.009971. Entropy: 0.679116.\n",
      "episode: 2668   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 773     evaluation reward: 8.57\n",
      "episode: 2669   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 673     evaluation reward: 8.58\n",
      "Training network. lr: 0.000224. clip: 0.089731\n",
      "Iteration 3424: Policy loss: 0.006272. Value loss: 0.022320. Entropy: 0.591310.\n",
      "Iteration 3425: Policy loss: -0.012717. Value loss: 0.016831. Entropy: 0.595018.\n",
      "Iteration 3426: Policy loss: -0.015853. Value loss: 0.012933. Entropy: 0.592791.\n",
      "episode: 2670   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 746     evaluation reward: 8.63\n",
      "Training network. lr: 0.000224. clip: 0.089722\n",
      "Iteration 3427: Policy loss: 0.009872. Value loss: 0.019556. Entropy: 0.647639.\n",
      "Iteration 3428: Policy loss: -0.008893. Value loss: 0.014200. Entropy: 0.636807.\n",
      "Iteration 3429: Policy loss: -0.020779. Value loss: 0.011811. Entropy: 0.640408.\n",
      "episode: 2671   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 592     evaluation reward: 8.62\n",
      "episode: 2672   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 586     evaluation reward: 8.62\n",
      "Training network. lr: 0.000224. clip: 0.089713\n",
      "Iteration 3430: Policy loss: 0.006468. Value loss: 0.027789. Entropy: 0.732716.\n",
      "Iteration 3431: Policy loss: -0.010686. Value loss: 0.017250. Entropy: 0.722464.\n",
      "Iteration 3432: Policy loss: -0.017575. Value loss: 0.013555. Entropy: 0.712981.\n",
      "episode: 2673   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 480     evaluation reward: 8.6\n",
      "Training network. lr: 0.000224. clip: 0.089704\n",
      "Iteration 3433: Policy loss: 0.011596. Value loss: 0.031660. Entropy: 0.749649.\n",
      "Iteration 3434: Policy loss: -0.010014. Value loss: 0.021913. Entropy: 0.740433.\n",
      "Iteration 3435: Policy loss: -0.019883. Value loss: 0.018586. Entropy: 0.732226.\n",
      "episode: 2674   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 828     evaluation reward: 8.57\n",
      "episode: 2675   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 660     evaluation reward: 8.58\n",
      "Training network. lr: 0.000224. clip: 0.089695\n",
      "Iteration 3436: Policy loss: 0.011894. Value loss: 0.024860. Entropy: 0.676009.\n",
      "Iteration 3437: Policy loss: -0.008338. Value loss: 0.019394. Entropy: 0.665970.\n",
      "Iteration 3438: Policy loss: -0.018962. Value loss: 0.017754. Entropy: 0.667448.\n",
      "episode: 2676   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 497     evaluation reward: 8.57\n",
      "episode: 2677   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 541     evaluation reward: 8.58\n",
      "Training network. lr: 0.000224. clip: 0.089686\n",
      "Iteration 3439: Policy loss: 0.006710. Value loss: 0.027232. Entropy: 0.567585.\n",
      "Iteration 3440: Policy loss: -0.005792. Value loss: 0.018679. Entropy: 0.559198.\n",
      "Iteration 3441: Policy loss: -0.009960. Value loss: 0.016287. Entropy: 0.554743.\n",
      "episode: 2678   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 549     evaluation reward: 8.57\n",
      "episode: 2679   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 617     evaluation reward: 8.59\n",
      "Training network. lr: 0.000224. clip: 0.089677\n",
      "Iteration 3442: Policy loss: 0.004258. Value loss: 0.044206. Entropy: 0.672402.\n",
      "Iteration 3443: Policy loss: -0.007200. Value loss: 0.035371. Entropy: 0.663415.\n",
      "Iteration 3444: Policy loss: -0.018602. Value loss: 0.029661. Entropy: 0.665009.\n",
      "episode: 2680   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 551     evaluation reward: 8.58\n",
      "episode: 2681   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 392     evaluation reward: 8.51\n",
      "Training network. lr: 0.000224. clip: 0.089668\n",
      "Iteration 3445: Policy loss: 0.007754. Value loss: 0.032547. Entropy: 0.654384.\n",
      "Iteration 3446: Policy loss: -0.006506. Value loss: 0.022598. Entropy: 0.652874.\n",
      "Iteration 3447: Policy loss: -0.015314. Value loss: 0.018541. Entropy: 0.647517.\n",
      "episode: 2682   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 504     evaluation reward: 8.46\n",
      "episode: 2683   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 568     evaluation reward: 8.45\n",
      "Training network. lr: 0.000224. clip: 0.089659\n",
      "Iteration 3448: Policy loss: 0.008054. Value loss: 0.017405. Entropy: 0.728399.\n",
      "Iteration 3449: Policy loss: -0.008174. Value loss: 0.012434. Entropy: 0.715803.\n",
      "Iteration 3450: Policy loss: -0.016837. Value loss: 0.010927. Entropy: 0.703464.\n",
      "episode: 2684   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 629     evaluation reward: 8.46\n",
      "Training network. lr: 0.000224. clip: 0.089650\n",
      "Iteration 3451: Policy loss: 0.007268. Value loss: 0.021984. Entropy: 0.665327.\n",
      "Iteration 3452: Policy loss: -0.010002. Value loss: 0.014370. Entropy: 0.650414.\n",
      "Iteration 3453: Policy loss: -0.021042. Value loss: 0.011887. Entropy: 0.659687.\n",
      "episode: 2685   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 824     evaluation reward: 8.51\n",
      "Training network. lr: 0.000224. clip: 0.089641\n",
      "Iteration 3454: Policy loss: 0.003530. Value loss: 0.020566. Entropy: 0.644697.\n",
      "Iteration 3455: Policy loss: -0.007995. Value loss: 0.015366. Entropy: 0.642147.\n",
      "Iteration 3456: Policy loss: -0.017729. Value loss: 0.013042. Entropy: 0.637813.\n",
      "episode: 2686   score: 16.0   memory length: 1024   epsilon: 1.0    steps: 811     evaluation reward: 8.6\n",
      "episode: 2687   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 657     evaluation reward: 8.59\n",
      "Training network. lr: 0.000224. clip: 0.089632\n",
      "Iteration 3457: Policy loss: 0.031934. Value loss: 0.054710. Entropy: 0.607326.\n",
      "Iteration 3458: Policy loss: -0.000973. Value loss: 0.037367. Entropy: 0.598622.\n",
      "Iteration 3459: Policy loss: -0.010836. Value loss: 0.031274. Entropy: 0.593267.\n",
      "episode: 2688   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 588     evaluation reward: 8.61\n",
      "Training network. lr: 0.000224. clip: 0.089623\n",
      "Iteration 3460: Policy loss: 0.004422. Value loss: 0.021005. Entropy: 0.704428.\n",
      "Iteration 3461: Policy loss: -0.011383. Value loss: 0.014595. Entropy: 0.704989.\n",
      "Iteration 3462: Policy loss: -0.024086. Value loss: 0.012581. Entropy: 0.702872.\n",
      "episode: 2689   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 753     evaluation reward: 8.64\n",
      "episode: 2690   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 431     evaluation reward: 8.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000224. clip: 0.089614\n",
      "Iteration 3463: Policy loss: 0.007459. Value loss: 0.023507. Entropy: 0.705432.\n",
      "Iteration 3464: Policy loss: -0.003856. Value loss: 0.014745. Entropy: 0.683161.\n",
      "Iteration 3465: Policy loss: -0.014749. Value loss: 0.012281. Entropy: 0.685236.\n",
      "episode: 2691   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 638     evaluation reward: 8.63\n",
      "episode: 2692   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 412     evaluation reward: 8.6\n",
      "episode: 2693   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 399     evaluation reward: 8.58\n",
      "Training network. lr: 0.000224. clip: 0.089605\n",
      "Iteration 3466: Policy loss: 0.009901. Value loss: 0.022478. Entropy: 0.699077.\n",
      "Iteration 3467: Policy loss: -0.011146. Value loss: 0.017462. Entropy: 0.685416.\n",
      "Iteration 3468: Policy loss: -0.020614. Value loss: 0.015802. Entropy: 0.678303.\n",
      "episode: 2694   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 559     evaluation reward: 8.62\n",
      "Training network. lr: 0.000224. clip: 0.089596\n",
      "Iteration 3469: Policy loss: 0.006708. Value loss: 0.051397. Entropy: 0.638696.\n",
      "Iteration 3470: Policy loss: -0.008758. Value loss: 0.038828. Entropy: 0.637042.\n",
      "Iteration 3471: Policy loss: -0.015649. Value loss: 0.031263. Entropy: 0.637409.\n",
      "episode: 2695   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 755     evaluation reward: 8.61\n",
      "episode: 2696   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 400     evaluation reward: 8.57\n",
      "Training network. lr: 0.000224. clip: 0.089587\n",
      "Iteration 3472: Policy loss: 0.008940. Value loss: 0.030442. Entropy: 0.708777.\n",
      "Iteration 3473: Policy loss: -0.011265. Value loss: 0.020771. Entropy: 0.713465.\n",
      "Iteration 3474: Policy loss: -0.016302. Value loss: 0.017100. Entropy: 0.702291.\n",
      "episode: 2697   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 697     evaluation reward: 8.58\n",
      "episode: 2698   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 576     evaluation reward: 8.57\n",
      "Training network. lr: 0.000224. clip: 0.089578\n",
      "Iteration 3475: Policy loss: 0.006223. Value loss: 0.017231. Entropy: 0.599845.\n",
      "Iteration 3476: Policy loss: -0.009139. Value loss: 0.012667. Entropy: 0.604845.\n",
      "Iteration 3477: Policy loss: -0.016622. Value loss: 0.010339. Entropy: 0.610278.\n",
      "episode: 2699   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 829     evaluation reward: 8.65\n",
      "Training network. lr: 0.000224. clip: 0.089569\n",
      "Iteration 3478: Policy loss: 0.010886. Value loss: 0.018249. Entropy: 0.718033.\n",
      "Iteration 3479: Policy loss: -0.001765. Value loss: 0.013678. Entropy: 0.712638.\n",
      "Iteration 3480: Policy loss: -0.014680. Value loss: 0.010937. Entropy: 0.706180.\n",
      "episode: 2700   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 547     evaluation reward: 8.67\n",
      "episode: 2701   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 695     evaluation reward: 8.72\n",
      "Training network. lr: 0.000224. clip: 0.089560\n",
      "Iteration 3481: Policy loss: 0.005700. Value loss: 0.019134. Entropy: 0.589385.\n",
      "Iteration 3482: Policy loss: -0.012497. Value loss: 0.014233. Entropy: 0.582584.\n",
      "Iteration 3483: Policy loss: -0.018193. Value loss: 0.012083. Entropy: 0.592418.\n",
      "episode: 2702   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 403     evaluation reward: 8.7\n",
      "episode: 2703   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 538     evaluation reward: 8.68\n",
      "Training network. lr: 0.000224. clip: 0.089551\n",
      "Iteration 3484: Policy loss: 0.012235. Value loss: 0.033516. Entropy: 0.654074.\n",
      "Iteration 3485: Policy loss: -0.009868. Value loss: 0.024894. Entropy: 0.653095.\n",
      "Iteration 3486: Policy loss: -0.019872. Value loss: 0.020045. Entropy: 0.654242.\n",
      "episode: 2704   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 359     evaluation reward: 8.6\n",
      "episode: 2705   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 326     evaluation reward: 8.57\n",
      "Training network. lr: 0.000224. clip: 0.089542\n",
      "Iteration 3487: Policy loss: 0.003289. Value loss: 0.021238. Entropy: 0.616482.\n",
      "Iteration 3488: Policy loss: -0.012900. Value loss: 0.016335. Entropy: 0.606583.\n",
      "Iteration 3489: Policy loss: -0.024926. Value loss: 0.014474. Entropy: 0.609420.\n",
      "episode: 2706   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 677     evaluation reward: 8.57\n",
      "episode: 2707   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 657     evaluation reward: 8.59\n",
      "Training network. lr: 0.000224. clip: 0.089533\n",
      "Iteration 3490: Policy loss: 0.019389. Value loss: 0.015498. Entropy: 0.630830.\n",
      "Iteration 3491: Policy loss: -0.000966. Value loss: 0.010752. Entropy: 0.621678.\n",
      "Iteration 3492: Policy loss: -0.016579. Value loss: 0.008629. Entropy: 0.629247.\n",
      "episode: 2708   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 720     evaluation reward: 8.64\n",
      "Training network. lr: 0.000224. clip: 0.089524\n",
      "Iteration 3493: Policy loss: 0.002415. Value loss: 0.015114. Entropy: 0.622077.\n",
      "Iteration 3494: Policy loss: -0.017826. Value loss: 0.011047. Entropy: 0.618572.\n",
      "Iteration 3495: Policy loss: -0.026127. Value loss: 0.009363. Entropy: 0.616406.\n",
      "episode: 2709   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 596     evaluation reward: 8.64\n",
      "episode: 2710   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 388     evaluation reward: 8.6\n",
      "Training network. lr: 0.000224. clip: 0.089515\n",
      "Iteration 3496: Policy loss: 0.007940. Value loss: 0.014454. Entropy: 0.675846.\n",
      "Iteration 3497: Policy loss: -0.007413. Value loss: 0.011737. Entropy: 0.665411.\n",
      "Iteration 3498: Policy loss: -0.020830. Value loss: 0.010421. Entropy: 0.661104.\n",
      "episode: 2711   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 681     evaluation reward: 8.6\n",
      "episode: 2712   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 752     evaluation reward: 8.64\n",
      "Training network. lr: 0.000224. clip: 0.089506\n",
      "Iteration 3499: Policy loss: 0.003261. Value loss: 0.019177. Entropy: 0.617496.\n",
      "Iteration 3500: Policy loss: -0.012242. Value loss: 0.013758. Entropy: 0.605094.\n",
      "Iteration 3501: Policy loss: -0.020152. Value loss: 0.011700. Entropy: 0.602015.\n",
      "episode: 2713   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 369     evaluation reward: 8.6\n",
      "episode: 2714   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 336     evaluation reward: 8.53\n",
      "Training network. lr: 0.000224. clip: 0.089497\n",
      "Iteration 3502: Policy loss: 0.000743. Value loss: 0.065790. Entropy: 0.667498.\n",
      "Iteration 3503: Policy loss: -0.008698. Value loss: 0.045498. Entropy: 0.648773.\n",
      "Iteration 3504: Policy loss: -0.018820. Value loss: 0.040061. Entropy: 0.652132.\n",
      "episode: 2715   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 661     evaluation reward: 8.55\n",
      "episode: 2716   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 818     evaluation reward: 8.6\n",
      "Training network. lr: 0.000224. clip: 0.089488\n",
      "Iteration 3505: Policy loss: 0.009429. Value loss: 0.025539. Entropy: 0.639554.\n",
      "Iteration 3506: Policy loss: -0.002500. Value loss: 0.018875. Entropy: 0.620685.\n",
      "Iteration 3507: Policy loss: -0.016571. Value loss: 0.015233. Entropy: 0.627149.\n",
      "episode: 2717   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 530     evaluation reward: 8.59\n",
      "episode: 2718   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 520     evaluation reward: 8.57\n",
      "Training network. lr: 0.000224. clip: 0.089479\n",
      "Iteration 3508: Policy loss: 0.007965. Value loss: 0.023417. Entropy: 0.621398.\n",
      "Iteration 3509: Policy loss: -0.006564. Value loss: 0.016970. Entropy: 0.611427.\n",
      "Iteration 3510: Policy loss: -0.018224. Value loss: 0.014182. Entropy: 0.612622.\n",
      "episode: 2719   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 489     evaluation reward: 8.53\n",
      "episode: 2720   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 485     evaluation reward: 8.46\n",
      "Training network. lr: 0.000224. clip: 0.089470\n",
      "Iteration 3511: Policy loss: 0.010665. Value loss: 0.016228. Entropy: 0.683011.\n",
      "Iteration 3512: Policy loss: -0.006274. Value loss: 0.013740. Entropy: 0.691423.\n",
      "Iteration 3513: Policy loss: -0.019993. Value loss: 0.012505. Entropy: 0.685573.\n",
      "episode: 2721   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 778     evaluation reward: 8.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now time :  2018-12-26 13:54:55.456226\n",
      "Training network. lr: 0.000224. clip: 0.089461\n",
      "Iteration 3514: Policy loss: 0.010064. Value loss: 0.017775. Entropy: 0.675509.\n",
      "Iteration 3515: Policy loss: -0.007693. Value loss: 0.012485. Entropy: 0.677653.\n",
      "Iteration 3516: Policy loss: -0.017092. Value loss: 0.011229. Entropy: 0.675183.\n",
      "episode: 2722   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 597     evaluation reward: 8.54\n",
      "episode: 2723   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 670     evaluation reward: 8.56\n",
      "Training network. lr: 0.000224. clip: 0.089452\n",
      "Iteration 3517: Policy loss: 0.006378. Value loss: 0.015788. Entropy: 0.636058.\n",
      "Iteration 3518: Policy loss: -0.006063. Value loss: 0.012861. Entropy: 0.626239.\n",
      "Iteration 3519: Policy loss: -0.019242. Value loss: 0.010613. Entropy: 0.624026.\n",
      "episode: 2724   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 754     evaluation reward: 8.65\n",
      "Training network. lr: 0.000224. clip: 0.089443\n",
      "Iteration 3520: Policy loss: 0.007965. Value loss: 0.040236. Entropy: 0.627561.\n",
      "Iteration 3521: Policy loss: -0.001486. Value loss: 0.026783. Entropy: 0.622850.\n",
      "Iteration 3522: Policy loss: -0.008239. Value loss: 0.020423. Entropy: 0.627686.\n",
      "episode: 2725   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 622     evaluation reward: 8.69\n",
      "episode: 2726   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 733     evaluation reward: 8.73\n",
      "Training network. lr: 0.000224. clip: 0.089434\n",
      "Iteration 3523: Policy loss: 0.006508. Value loss: 0.029241. Entropy: 0.590655.\n",
      "Iteration 3524: Policy loss: -0.007920. Value loss: 0.020754. Entropy: 0.593367.\n",
      "Iteration 3525: Policy loss: -0.014000. Value loss: 0.016158. Entropy: 0.593137.\n",
      "episode: 2727   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 584     evaluation reward: 8.76\n",
      "Training network. lr: 0.000224. clip: 0.089425\n",
      "Iteration 3526: Policy loss: 0.004336. Value loss: 0.023193. Entropy: 0.699274.\n",
      "Iteration 3527: Policy loss: -0.007344. Value loss: 0.016840. Entropy: 0.694484.\n",
      "Iteration 3528: Policy loss: -0.016395. Value loss: 0.015203. Entropy: 0.682664.\n",
      "episode: 2728   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 577     evaluation reward: 8.73\n",
      "episode: 2729   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 354     evaluation reward: 8.65\n",
      "Training network. lr: 0.000224. clip: 0.089416\n",
      "Iteration 3529: Policy loss: 0.009160. Value loss: 0.019612. Entropy: 0.634227.\n",
      "Iteration 3530: Policy loss: -0.007575. Value loss: 0.013940. Entropy: 0.635543.\n",
      "Iteration 3531: Policy loss: -0.016881. Value loss: 0.012383. Entropy: 0.633314.\n",
      "episode: 2730   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 769     evaluation reward: 8.73\n",
      "episode: 2731   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 622     evaluation reward: 8.71\n",
      "Training network. lr: 0.000224. clip: 0.089407\n",
      "Iteration 3532: Policy loss: 0.000977. Value loss: 0.045163. Entropy: 0.621318.\n",
      "Iteration 3533: Policy loss: -0.006661. Value loss: 0.032359. Entropy: 0.646942.\n",
      "Iteration 3534: Policy loss: -0.018720. Value loss: 0.025249. Entropy: 0.629877.\n",
      "episode: 2732   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 550     evaluation reward: 8.69\n",
      "Training network. lr: 0.000223. clip: 0.089398\n",
      "Iteration 3535: Policy loss: 0.003883. Value loss: 0.025057. Entropy: 0.721382.\n",
      "Iteration 3536: Policy loss: -0.012570. Value loss: 0.018018. Entropy: 0.715321.\n",
      "Iteration 3537: Policy loss: -0.022613. Value loss: 0.014562. Entropy: 0.722930.\n",
      "episode: 2733   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 772     evaluation reward: 8.72\n",
      "episode: 2734   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 571     evaluation reward: 8.72\n",
      "Training network. lr: 0.000223. clip: 0.089389\n",
      "Iteration 3538: Policy loss: 0.009870. Value loss: 0.026583. Entropy: 0.696243.\n",
      "Iteration 3539: Policy loss: -0.007159. Value loss: 0.020876. Entropy: 0.695283.\n",
      "Iteration 3540: Policy loss: -0.017676. Value loss: 0.017801. Entropy: 0.700757.\n",
      "episode: 2735   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 649     evaluation reward: 8.69\n",
      "Training network. lr: 0.000223. clip: 0.089380\n",
      "Iteration 3541: Policy loss: 0.004983. Value loss: 0.022367. Entropy: 0.719828.\n",
      "Iteration 3542: Policy loss: -0.013896. Value loss: 0.018027. Entropy: 0.710838.\n",
      "Iteration 3543: Policy loss: -0.016514. Value loss: 0.015597. Entropy: 0.700523.\n",
      "episode: 2736   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 804     evaluation reward: 8.72\n",
      "episode: 2737   score: 16.0   memory length: 1024   epsilon: 1.0    steps: 761     evaluation reward: 8.79\n",
      "Training network. lr: 0.000223. clip: 0.089371\n",
      "Iteration 3544: Policy loss: 0.017506. Value loss: 0.052135. Entropy: 0.638451.\n",
      "Iteration 3545: Policy loss: -0.003046. Value loss: 0.039632. Entropy: 0.631899.\n",
      "Iteration 3546: Policy loss: -0.011611. Value loss: 0.035153. Entropy: 0.630935.\n",
      "episode: 2738   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 553     evaluation reward: 8.67\n",
      "episode: 2739   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 597     evaluation reward: 8.65\n",
      "Training network. lr: 0.000223. clip: 0.089362\n",
      "Iteration 3547: Policy loss: 0.010943. Value loss: 0.034938. Entropy: 0.686307.\n",
      "Iteration 3548: Policy loss: -0.008264. Value loss: 0.027685. Entropy: 0.680454.\n",
      "Iteration 3549: Policy loss: -0.021513. Value loss: 0.024728. Entropy: 0.677664.\n",
      "episode: 2740   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 699     evaluation reward: 8.7\n",
      "Training network. lr: 0.000223. clip: 0.089353\n",
      "Iteration 3550: Policy loss: 0.006411. Value loss: 0.031467. Entropy: 0.687603.\n",
      "Iteration 3551: Policy loss: -0.010259. Value loss: 0.024567. Entropy: 0.680455.\n",
      "Iteration 3552: Policy loss: -0.019983. Value loss: 0.020012. Entropy: 0.675193.\n",
      "episode: 2741   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 406     evaluation reward: 8.66\n",
      "episode: 2742   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 601     evaluation reward: 8.68\n",
      "episode: 2743   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 394     evaluation reward: 8.66\n",
      "Training network. lr: 0.000223. clip: 0.089344\n",
      "Iteration 3553: Policy loss: 0.008875. Value loss: 0.024321. Entropy: 0.642489.\n",
      "Iteration 3554: Policy loss: -0.007691. Value loss: 0.019633. Entropy: 0.624486.\n",
      "Iteration 3555: Policy loss: -0.020430. Value loss: 0.016749. Entropy: 0.627930.\n",
      "episode: 2744   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 677     evaluation reward: 8.71\n",
      "Training network. lr: 0.000223. clip: 0.089335\n",
      "Iteration 3556: Policy loss: 0.009490. Value loss: 0.046313. Entropy: 0.660630.\n",
      "Iteration 3557: Policy loss: -0.006257. Value loss: 0.037328. Entropy: 0.658967.\n",
      "Iteration 3558: Policy loss: -0.015127. Value loss: 0.031647. Entropy: 0.663772.\n",
      "episode: 2745   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 604     evaluation reward: 8.7\n",
      "episode: 2746   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 555     evaluation reward: 8.69\n",
      "Training network. lr: 0.000223. clip: 0.089326\n",
      "Iteration 3559: Policy loss: 0.004883. Value loss: 0.026607. Entropy: 0.588056.\n",
      "Iteration 3560: Policy loss: -0.013477. Value loss: 0.019508. Entropy: 0.576093.\n",
      "Iteration 3561: Policy loss: -0.017742. Value loss: 0.016461. Entropy: 0.576917.\n",
      "episode: 2747   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 899     evaluation reward: 8.75\n",
      "Training network. lr: 0.000223. clip: 0.089317\n",
      "Iteration 3562: Policy loss: 0.003661. Value loss: 0.027154. Entropy: 0.659384.\n",
      "Iteration 3563: Policy loss: -0.007348. Value loss: 0.020421. Entropy: 0.652270.\n",
      "Iteration 3564: Policy loss: -0.017265. Value loss: 0.015985. Entropy: 0.656524.\n",
      "episode: 2748   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 786     evaluation reward: 8.81\n",
      "Training network. lr: 0.000223. clip: 0.089308\n",
      "Iteration 3565: Policy loss: 0.007959. Value loss: 0.019734. Entropy: 0.704648.\n",
      "Iteration 3566: Policy loss: -0.004210. Value loss: 0.012798. Entropy: 0.696546.\n",
      "Iteration 3567: Policy loss: -0.017973. Value loss: 0.010846. Entropy: 0.694480.\n",
      "episode: 2749   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 708     evaluation reward: 8.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2750   score: 16.0   memory length: 1024   epsilon: 1.0    steps: 846     evaluation reward: 8.93\n",
      "Training network. lr: 0.000223. clip: 0.089299\n",
      "Iteration 3568: Policy loss: 0.008745. Value loss: 0.053600. Entropy: 0.632169.\n",
      "Iteration 3569: Policy loss: -0.006385. Value loss: 0.044159. Entropy: 0.638593.\n",
      "Iteration 3570: Policy loss: -0.015633. Value loss: 0.039847. Entropy: 0.629088.\n",
      "episode: 2751   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 591     evaluation reward: 8.94\n",
      "episode: 2752   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 486     evaluation reward: 8.93\n",
      "Training network. lr: 0.000223. clip: 0.089290\n",
      "Iteration 3571: Policy loss: 0.006016. Value loss: 0.027110. Entropy: 0.691088.\n",
      "Iteration 3572: Policy loss: -0.009562. Value loss: 0.020282. Entropy: 0.682329.\n",
      "Iteration 3573: Policy loss: -0.022236. Value loss: 0.017232. Entropy: 0.678372.\n",
      "episode: 2753   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 619     evaluation reward: 8.96\n",
      "episode: 2754   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 286     evaluation reward: 8.92\n",
      "Training network. lr: 0.000223. clip: 0.089281\n",
      "Iteration 3574: Policy loss: 0.007872. Value loss: 0.035998. Entropy: 0.667640.\n",
      "Iteration 3575: Policy loss: -0.010952. Value loss: 0.026890. Entropy: 0.660349.\n",
      "Iteration 3576: Policy loss: -0.020971. Value loss: 0.023578. Entropy: 0.659889.\n",
      "episode: 2755   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 423     evaluation reward: 8.89\n",
      "Training network. lr: 0.000223. clip: 0.089272\n",
      "Iteration 3577: Policy loss: 0.002416. Value loss: 0.091024. Entropy: 0.690273.\n",
      "Iteration 3578: Policy loss: -0.011645. Value loss: 0.070010. Entropy: 0.684465.\n",
      "Iteration 3579: Policy loss: -0.015910. Value loss: 0.056274. Entropy: 0.696261.\n",
      "episode: 2756   score: 20.0   memory length: 1024   epsilon: 1.0    steps: 794     evaluation reward: 9.04\n",
      "episode: 2757   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 459     evaluation reward: 9.06\n",
      "Training network. lr: 0.000223. clip: 0.089263\n",
      "Iteration 3580: Policy loss: 0.008490. Value loss: 0.036420. Entropy: 0.672096.\n",
      "Iteration 3581: Policy loss: -0.007694. Value loss: 0.027631. Entropy: 0.657998.\n",
      "Iteration 3582: Policy loss: -0.021488. Value loss: 0.024357. Entropy: 0.660388.\n",
      "episode: 2758   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 662     evaluation reward: 9.1\n",
      "episode: 2759   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 701     evaluation reward: 9.11\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3583: Policy loss: 0.009166. Value loss: 0.025507. Entropy: 0.660465.\n",
      "Iteration 3584: Policy loss: -0.010103. Value loss: 0.019368. Entropy: 0.640352.\n",
      "Iteration 3585: Policy loss: -0.019796. Value loss: 0.016526. Entropy: 0.644514.\n",
      "episode: 2760   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 501     evaluation reward: 9.09\n",
      "Training network. lr: 0.000223. clip: 0.089245\n",
      "Iteration 3586: Policy loss: 0.010576. Value loss: 0.027885. Entropy: 0.679803.\n",
      "Iteration 3587: Policy loss: -0.007048. Value loss: 0.020927. Entropy: 0.681581.\n",
      "Iteration 3588: Policy loss: -0.013264. Value loss: 0.016821. Entropy: 0.688942.\n",
      "episode: 2761   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 703     evaluation reward: 9.13\n",
      "episode: 2762   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 624     evaluation reward: 9.1\n",
      "Training network. lr: 0.000223. clip: 0.089236\n",
      "Iteration 3589: Policy loss: 0.008320. Value loss: 0.027424. Entropy: 0.634517.\n",
      "Iteration 3590: Policy loss: -0.007089. Value loss: 0.020445. Entropy: 0.635620.\n",
      "Iteration 3591: Policy loss: -0.015087. Value loss: 0.016507. Entropy: 0.630499.\n",
      "episode: 2763   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 715     evaluation reward: 9.08\n",
      "episode: 2764   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 593     evaluation reward: 9.07\n",
      "Training network. lr: 0.000223. clip: 0.089227\n",
      "Iteration 3592: Policy loss: -0.001771. Value loss: 0.023668. Entropy: 0.686361.\n",
      "Iteration 3593: Policy loss: -0.016651. Value loss: 0.015939. Entropy: 0.672300.\n",
      "Iteration 3594: Policy loss: -0.025465. Value loss: 0.013091. Entropy: 0.671908.\n",
      "episode: 2765   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 687     evaluation reward: 9.02\n",
      "Training network. lr: 0.000223. clip: 0.089218\n",
      "Iteration 3595: Policy loss: 0.004239. Value loss: 0.021004. Entropy: 0.690778.\n",
      "Iteration 3596: Policy loss: -0.008971. Value loss: 0.015286. Entropy: 0.682044.\n",
      "Iteration 3597: Policy loss: -0.017447. Value loss: 0.012829. Entropy: 0.679792.\n",
      "episode: 2766   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 491     evaluation reward: 8.98\n",
      "episode: 2767   score: 18.0   memory length: 1024   epsilon: 1.0    steps: 934     evaluation reward: 9.08\n",
      "Training network. lr: 0.000223. clip: 0.089209\n",
      "Iteration 3598: Policy loss: 0.005550. Value loss: 0.051505. Entropy: 0.662650.\n",
      "Iteration 3599: Policy loss: -0.009739. Value loss: 0.036688. Entropy: 0.664997.\n",
      "Iteration 3600: Policy loss: -0.014215. Value loss: 0.028948. Entropy: 0.658134.\n",
      "episode: 2768   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 668     evaluation reward: 9.06\n",
      "Training network. lr: 0.000223. clip: 0.089200\n",
      "Iteration 3601: Policy loss: 0.004908. Value loss: 0.025624. Entropy: 0.666808.\n",
      "Iteration 3602: Policy loss: -0.006234. Value loss: 0.020227. Entropy: 0.667839.\n",
      "Iteration 3603: Policy loss: -0.015352. Value loss: 0.016942. Entropy: 0.677488.\n",
      "episode: 2769   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 504     evaluation reward: 9.02\n",
      "episode: 2770   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 745     evaluation reward: 9.03\n",
      "Training network. lr: 0.000223. clip: 0.089191\n",
      "Iteration 3604: Policy loss: 0.015607. Value loss: 0.041418. Entropy: 0.641148.\n",
      "Iteration 3605: Policy loss: -0.009223. Value loss: 0.029100. Entropy: 0.621659.\n",
      "Iteration 3606: Policy loss: -0.020938. Value loss: 0.024353. Entropy: 0.624105.\n",
      "episode: 2771   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 679     evaluation reward: 9.06\n",
      "episode: 2772   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 457     evaluation reward: 9.04\n",
      "Training network. lr: 0.000223. clip: 0.089182\n",
      "Iteration 3607: Policy loss: 0.008320. Value loss: 0.056322. Entropy: 0.672042.\n",
      "Iteration 3608: Policy loss: -0.002789. Value loss: 0.043967. Entropy: 0.666574.\n",
      "Iteration 3609: Policy loss: -0.006370. Value loss: 0.035852. Entropy: 0.673533.\n",
      "episode: 2773   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 608     evaluation reward: 9.1\n",
      "episode: 2774   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 468     evaluation reward: 9.05\n",
      "Training network. lr: 0.000223. clip: 0.089173\n",
      "Iteration 3610: Policy loss: 0.003041. Value loss: 0.055345. Entropy: 0.655802.\n",
      "Iteration 3611: Policy loss: -0.012076. Value loss: 0.041442. Entropy: 0.658959.\n",
      "Iteration 3612: Policy loss: -0.016266. Value loss: 0.034247. Entropy: 0.660106.\n",
      "episode: 2775   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 754     evaluation reward: 9.05\n",
      "Training network. lr: 0.000223. clip: 0.089164\n",
      "Iteration 3613: Policy loss: 0.008247. Value loss: 0.019568. Entropy: 0.701051.\n",
      "Iteration 3614: Policy loss: -0.007270. Value loss: 0.014115. Entropy: 0.705231.\n",
      "Iteration 3615: Policy loss: -0.021272. Value loss: 0.013479. Entropy: 0.700296.\n",
      "episode: 2776   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 431     evaluation reward: 9.04\n",
      "episode: 2777   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 479     evaluation reward: 9.02\n",
      "Training network. lr: 0.000223. clip: 0.089155\n",
      "Iteration 3616: Policy loss: 0.007619. Value loss: 0.021610. Entropy: 0.681191.\n",
      "Iteration 3617: Policy loss: -0.004113. Value loss: 0.018662. Entropy: 0.676641.\n",
      "Iteration 3618: Policy loss: -0.017618. Value loss: 0.015135. Entropy: 0.678626.\n",
      "episode: 2778   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 476     evaluation reward: 9.02\n",
      "episode: 2779   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 662     evaluation reward: 9.0\n",
      "Training network. lr: 0.000223. clip: 0.089146\n",
      "Iteration 3619: Policy loss: 0.010405. Value loss: 0.032179. Entropy: 0.692146.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3620: Policy loss: -0.000580. Value loss: 0.021774. Entropy: 0.674553.\n",
      "Iteration 3621: Policy loss: -0.011666. Value loss: 0.018584. Entropy: 0.683427.\n",
      "episode: 2780   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 657     evaluation reward: 9.06\n",
      "Training network. lr: 0.000223. clip: 0.089137\n",
      "Iteration 3622: Policy loss: 0.008780. Value loss: 0.051735. Entropy: 0.749256.\n",
      "Iteration 3623: Policy loss: -0.003205. Value loss: 0.033650. Entropy: 0.738799.\n",
      "Iteration 3624: Policy loss: -0.011451. Value loss: 0.027986. Entropy: 0.739272.\n",
      "episode: 2781   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 728     evaluation reward: 9.17\n",
      "episode: 2782   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 738     evaluation reward: 9.22\n",
      "Training network. lr: 0.000223. clip: 0.089128\n",
      "Iteration 3625: Policy loss: 0.013496. Value loss: 0.051819. Entropy: 0.783741.\n",
      "Iteration 3626: Policy loss: -0.002150. Value loss: 0.037764. Entropy: 0.784678.\n",
      "Iteration 3627: Policy loss: -0.010779. Value loss: 0.038959. Entropy: 0.787335.\n",
      "episode: 2783   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 457     evaluation reward: 9.2\n",
      "episode: 2784   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 487     evaluation reward: 9.16\n",
      "Training network. lr: 0.000223. clip: 0.089119\n",
      "Iteration 3628: Policy loss: 0.010591. Value loss: 0.034335. Entropy: 0.734281.\n",
      "Iteration 3629: Policy loss: -0.007541. Value loss: 0.023435. Entropy: 0.728692.\n",
      "Iteration 3630: Policy loss: -0.016349. Value loss: 0.019709. Entropy: 0.721992.\n",
      "episode: 2785   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 465     evaluation reward: 9.1\n",
      "episode: 2786   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 612     evaluation reward: 9.02\n",
      "Training network. lr: 0.000223. clip: 0.089110\n",
      "Iteration 3631: Policy loss: 0.006940. Value loss: 0.022896. Entropy: 0.616260.\n",
      "Iteration 3632: Policy loss: 0.004793. Value loss: 0.017382. Entropy: 0.614576.\n",
      "Iteration 3633: Policy loss: -0.011937. Value loss: 0.014367. Entropy: 0.616445.\n",
      "episode: 2787   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 703     evaluation reward: 9.04\n",
      "Training network. lr: 0.000223. clip: 0.089101\n",
      "Iteration 3634: Policy loss: 0.008026. Value loss: 0.026644. Entropy: 0.830468.\n",
      "Iteration 3635: Policy loss: -0.010789. Value loss: 0.016678. Entropy: 0.825252.\n",
      "Iteration 3636: Policy loss: -0.020087. Value loss: 0.015450. Entropy: 0.823387.\n",
      "episode: 2788   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 613     evaluation reward: 9.05\n",
      "episode: 2789   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 423     evaluation reward: 8.99\n",
      "Training network. lr: 0.000223. clip: 0.089092\n",
      "Iteration 3637: Policy loss: 0.010935. Value loss: 0.026829. Entropy: 0.732872.\n",
      "Iteration 3638: Policy loss: -0.007450. Value loss: 0.019563. Entropy: 0.721345.\n",
      "Iteration 3639: Policy loss: -0.013171. Value loss: 0.015867. Entropy: 0.724992.\n",
      "episode: 2790   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 841     evaluation reward: 9.06\n",
      "episode: 2791   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 695     evaluation reward: 9.08\n",
      "Training network. lr: 0.000223. clip: 0.089083\n",
      "Iteration 3640: Policy loss: 0.007129. Value loss: 0.021981. Entropy: 0.747670.\n",
      "Iteration 3641: Policy loss: -0.006947. Value loss: 0.016660. Entropy: 0.737022.\n",
      "Iteration 3642: Policy loss: -0.019207. Value loss: 0.013388. Entropy: 0.735759.\n",
      "episode: 2792   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 400     evaluation reward: 9.08\n",
      "episode: 2793   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 621     evaluation reward: 9.12\n",
      "Training network. lr: 0.000223. clip: 0.089074\n",
      "Iteration 3643: Policy loss: 0.008854. Value loss: 0.028716. Entropy: 0.692715.\n",
      "Iteration 3644: Policy loss: -0.003195. Value loss: 0.022380. Entropy: 0.701472.\n",
      "Iteration 3645: Policy loss: -0.015433. Value loss: 0.018855. Entropy: 0.705319.\n",
      "episode: 2794   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 469     evaluation reward: 9.07\n",
      "episode: 2795   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 343     evaluation reward: 9.0\n",
      "Training network. lr: 0.000223. clip: 0.089065\n",
      "Iteration 3646: Policy loss: 0.014067. Value loss: 0.045985. Entropy: 0.685152.\n",
      "Iteration 3647: Policy loss: -0.008717. Value loss: 0.033062. Entropy: 0.679949.\n",
      "Iteration 3648: Policy loss: -0.017596. Value loss: 0.027034. Entropy: 0.677055.\n",
      "episode: 2796   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 579     evaluation reward: 9.03\n",
      "episode: 2797   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 390     evaluation reward: 8.97\n",
      "Training network. lr: 0.000223. clip: 0.089056\n",
      "Iteration 3649: Policy loss: 0.010967. Value loss: 0.028216. Entropy: 0.776474.\n",
      "Iteration 3650: Policy loss: -0.006663. Value loss: 0.018913. Entropy: 0.758500.\n",
      "Iteration 3651: Policy loss: -0.016112. Value loss: 0.015651. Entropy: 0.764193.\n",
      "episode: 2798   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 730     evaluation reward: 8.99\n",
      "Training network. lr: 0.000223. clip: 0.089047\n",
      "Iteration 3652: Policy loss: 0.009917. Value loss: 0.022124. Entropy: 0.693533.\n",
      "Iteration 3653: Policy loss: -0.007370. Value loss: 0.017178. Entropy: 0.703004.\n",
      "Iteration 3654: Policy loss: -0.020799. Value loss: 0.014686. Entropy: 0.695835.\n",
      "episode: 2799   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 792     evaluation reward: 8.96\n",
      "episode: 2800   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 663     evaluation reward: 9.01\n",
      "Training network. lr: 0.000223. clip: 0.089038\n",
      "Iteration 3655: Policy loss: 0.000232. Value loss: 0.040772. Entropy: 0.690025.\n",
      "Iteration 3656: Policy loss: -0.005059. Value loss: 0.031152. Entropy: 0.702812.\n",
      "Iteration 3657: Policy loss: -0.011326. Value loss: 0.028280. Entropy: 0.689908.\n",
      "episode: 2801   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 616     evaluation reward: 8.99\n",
      "episode: 2802   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 519     evaluation reward: 9.03\n",
      "Training network. lr: 0.000223. clip: 0.089029\n",
      "Iteration 3658: Policy loss: 0.006079. Value loss: 0.026962. Entropy: 0.702139.\n",
      "Iteration 3659: Policy loss: -0.009353. Value loss: 0.021817. Entropy: 0.710335.\n",
      "Iteration 3660: Policy loss: -0.016733. Value loss: 0.017476. Entropy: 0.702746.\n",
      "episode: 2803   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 614     evaluation reward: 9.04\n",
      "now time :  2018-12-26 13:59:19.621019\n",
      "Training network. lr: 0.000223. clip: 0.089020\n",
      "Iteration 3661: Policy loss: 0.007555. Value loss: 0.019401. Entropy: 0.771191.\n",
      "Iteration 3662: Policy loss: -0.008955. Value loss: 0.013496. Entropy: 0.761731.\n",
      "Iteration 3663: Policy loss: -0.006435. Value loss: 0.011657. Entropy: 0.738544.\n",
      "episode: 2804   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 556     evaluation reward: 9.09\n",
      "episode: 2805   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 496     evaluation reward: 9.12\n",
      "Training network. lr: 0.000223. clip: 0.089011\n",
      "Iteration 3664: Policy loss: 0.008279. Value loss: 0.027105. Entropy: 0.713643.\n",
      "Iteration 3665: Policy loss: -0.008840. Value loss: 0.019916. Entropy: 0.717294.\n",
      "Iteration 3666: Policy loss: -0.015923. Value loss: 0.015069. Entropy: 0.716948.\n",
      "episode: 2806   score: 17.0   memory length: 1024   epsilon: 1.0    steps: 893     evaluation reward: 9.19\n",
      "Training network. lr: 0.000223. clip: 0.089002\n",
      "Iteration 3667: Policy loss: 0.000730. Value loss: 0.039372. Entropy: 0.697600.\n",
      "Iteration 3668: Policy loss: -0.007745. Value loss: 0.027329. Entropy: 0.693855.\n",
      "Iteration 3669: Policy loss: -0.014541. Value loss: 0.024384. Entropy: 0.687582.\n",
      "episode: 2807   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 642     evaluation reward: 9.17\n",
      "episode: 2808   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 524     evaluation reward: 9.13\n",
      "Training network. lr: 0.000222. clip: 0.088993\n",
      "Iteration 3670: Policy loss: 0.003762. Value loss: 0.033348. Entropy: 0.695219.\n",
      "Iteration 3671: Policy loss: -0.006554. Value loss: 0.025353. Entropy: 0.684648.\n",
      "Iteration 3672: Policy loss: -0.016164. Value loss: 0.021273. Entropy: 0.687929.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2809   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 762     evaluation reward: 9.16\n",
      "episode: 2810   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 541     evaluation reward: 9.19\n",
      "Training network. lr: 0.000222. clip: 0.088984\n",
      "Iteration 3673: Policy loss: 0.009442. Value loss: 0.027833. Entropy: 0.606149.\n",
      "Iteration 3674: Policy loss: -0.007881. Value loss: 0.019439. Entropy: 0.587244.\n",
      "Iteration 3675: Policy loss: -0.015773. Value loss: 0.016062. Entropy: 0.588037.\n",
      "episode: 2811   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 508     evaluation reward: 9.16\n",
      "episode: 2812   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 402     evaluation reward: 9.1\n",
      "Training network. lr: 0.000222. clip: 0.088975\n",
      "Iteration 3676: Policy loss: 0.002870. Value loss: 0.038212. Entropy: 0.665588.\n",
      "Iteration 3677: Policy loss: -0.007295. Value loss: 0.028851. Entropy: 0.660527.\n",
      "Iteration 3678: Policy loss: -0.019188. Value loss: 0.025073. Entropy: 0.657941.\n",
      "episode: 2813   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 314     evaluation reward: 9.09\n",
      "episode: 2814   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 528     evaluation reward: 9.17\n",
      "Training network. lr: 0.000222. clip: 0.088966\n",
      "Iteration 3679: Policy loss: 0.003248. Value loss: 0.052131. Entropy: 0.715377.\n",
      "Iteration 3680: Policy loss: -0.006800. Value loss: 0.033832. Entropy: 0.708413.\n",
      "Iteration 3681: Policy loss: -0.016736. Value loss: 0.027021. Entropy: 0.715546.\n",
      "episode: 2815   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 655     evaluation reward: 9.14\n",
      "episode: 2816   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 476     evaluation reward: 9.08\n",
      "Training network. lr: 0.000222. clip: 0.088957\n",
      "Iteration 3682: Policy loss: 0.012661. Value loss: 0.022376. Entropy: 0.709846.\n",
      "Iteration 3683: Policy loss: -0.006067. Value loss: 0.016287. Entropy: 0.716668.\n",
      "Iteration 3684: Policy loss: -0.015486. Value loss: 0.013494. Entropy: 0.709551.\n",
      "episode: 2817   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 471     evaluation reward: 9.11\n",
      "episode: 2818   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 664     evaluation reward: 9.14\n",
      "Training network. lr: 0.000222. clip: 0.088948\n",
      "Iteration 3685: Policy loss: 0.004067. Value loss: 0.049007. Entropy: 0.719660.\n",
      "Iteration 3686: Policy loss: -0.005735. Value loss: 0.038816. Entropy: 0.728275.\n",
      "Iteration 3687: Policy loss: -0.015560. Value loss: 0.033037. Entropy: 0.726012.\n",
      "episode: 2819   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 820     evaluation reward: 9.19\n",
      "Training network. lr: 0.000222. clip: 0.088939\n",
      "Iteration 3688: Policy loss: 0.009538. Value loss: 0.040692. Entropy: 0.707337.\n",
      "Iteration 3689: Policy loss: -0.008181. Value loss: 0.028083. Entropy: 0.705410.\n",
      "Iteration 3690: Policy loss: -0.016292. Value loss: 0.023200. Entropy: 0.700437.\n",
      "episode: 2820   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 540     evaluation reward: 9.19\n",
      "episode: 2821   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 445     evaluation reward: 9.12\n",
      "Training network. lr: 0.000222. clip: 0.088930\n",
      "Iteration 3691: Policy loss: 0.008325. Value loss: 0.029618. Entropy: 0.624219.\n",
      "Iteration 3692: Policy loss: -0.007274. Value loss: 0.022690. Entropy: 0.627944.\n",
      "Iteration 3693: Policy loss: -0.018245. Value loss: 0.018624. Entropy: 0.621865.\n",
      "episode: 2822   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 465     evaluation reward: 9.1\n",
      "episode: 2823   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 539     evaluation reward: 9.08\n",
      "Training network. lr: 0.000222. clip: 0.088921\n",
      "Iteration 3694: Policy loss: 0.010799. Value loss: 0.023818. Entropy: 0.733894.\n",
      "Iteration 3695: Policy loss: -0.007050. Value loss: 0.019240. Entropy: 0.742711.\n",
      "Iteration 3696: Policy loss: -0.017014. Value loss: 0.015940. Entropy: 0.744401.\n",
      "episode: 2824   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 647     evaluation reward: 9.02\n",
      "episode: 2825   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 495     evaluation reward: 8.98\n",
      "Training network. lr: 0.000222. clip: 0.088912\n",
      "Iteration 3697: Policy loss: -0.000645. Value loss: 0.025507. Entropy: 0.699667.\n",
      "Iteration 3698: Policy loss: -0.013001. Value loss: 0.020049. Entropy: 0.691562.\n",
      "Iteration 3699: Policy loss: -0.020368. Value loss: 0.017187. Entropy: 0.690229.\n",
      "episode: 2826   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 725     evaluation reward: 8.97\n",
      "episode: 2827   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 568     evaluation reward: 8.96\n",
      "Training network. lr: 0.000222. clip: 0.088903\n",
      "Iteration 3700: Policy loss: 0.005299. Value loss: 0.017443. Entropy: 0.692025.\n",
      "Iteration 3701: Policy loss: -0.006803. Value loss: 0.013845. Entropy: 0.693569.\n",
      "Iteration 3702: Policy loss: -0.020461. Value loss: 0.011237. Entropy: 0.694001.\n",
      "episode: 2828   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 500     evaluation reward: 8.94\n",
      "Training network. lr: 0.000222. clip: 0.088894\n",
      "Iteration 3703: Policy loss: 0.007866. Value loss: 0.053045. Entropy: 0.661259.\n",
      "Iteration 3704: Policy loss: -0.008504. Value loss: 0.038244. Entropy: 0.650956.\n",
      "Iteration 3705: Policy loss: -0.021486. Value loss: 0.031926. Entropy: 0.644111.\n",
      "episode: 2829   score: 19.0   memory length: 1024   epsilon: 1.0    steps: 798     evaluation reward: 9.09\n",
      "Training network. lr: 0.000222. clip: 0.088885\n",
      "Iteration 3706: Policy loss: 0.008501. Value loss: 0.039132. Entropy: 0.732474.\n",
      "Iteration 3707: Policy loss: -0.013120. Value loss: 0.031886. Entropy: 0.732234.\n",
      "Iteration 3708: Policy loss: -0.019264. Value loss: 0.026514. Entropy: 0.736724.\n",
      "episode: 2830   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 865     evaluation reward: 9.08\n",
      "episode: 2831   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 658     evaluation reward: 9.08\n",
      "Training network. lr: 0.000222. clip: 0.088876\n",
      "Iteration 3709: Policy loss: 0.006747. Value loss: 0.019508. Entropy: 0.779882.\n",
      "Iteration 3710: Policy loss: -0.008164. Value loss: 0.014984. Entropy: 0.782621.\n",
      "Iteration 3711: Policy loss: -0.020309. Value loss: 0.012217. Entropy: 0.771542.\n",
      "episode: 2832   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 463     evaluation reward: 9.07\n",
      "episode: 2833   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 393     evaluation reward: 9.0\n",
      "Training network. lr: 0.000222. clip: 0.088867\n",
      "Iteration 3712: Policy loss: 0.006515. Value loss: 0.026963. Entropy: 0.701766.\n",
      "Iteration 3713: Policy loss: -0.009279. Value loss: 0.018547. Entropy: 0.703156.\n",
      "Iteration 3714: Policy loss: -0.017979. Value loss: 0.015140. Entropy: 0.697640.\n",
      "episode: 2834   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 560     evaluation reward: 9.0\n",
      "episode: 2835   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 710     evaluation reward: 9.01\n",
      "Training network. lr: 0.000222. clip: 0.088858\n",
      "Iteration 3715: Policy loss: 0.010539. Value loss: 0.022254. Entropy: 0.742052.\n",
      "Iteration 3716: Policy loss: -0.009417. Value loss: 0.017004. Entropy: 0.734320.\n",
      "Iteration 3717: Policy loss: -0.016684. Value loss: 0.013839. Entropy: 0.725690.\n",
      "episode: 2836   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 644     evaluation reward: 8.98\n",
      "episode: 2837   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 531     evaluation reward: 8.89\n",
      "Training network. lr: 0.000222. clip: 0.088849\n",
      "Iteration 3718: Policy loss: 0.005083. Value loss: 0.023312. Entropy: 0.672391.\n",
      "Iteration 3719: Policy loss: -0.004285. Value loss: 0.017070. Entropy: 0.667341.\n",
      "Iteration 3720: Policy loss: -0.014736. Value loss: 0.013599. Entropy: 0.673633.\n",
      "episode: 2838   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 515     evaluation reward: 8.89\n",
      "Training network. lr: 0.000222. clip: 0.088840\n",
      "Iteration 3721: Policy loss: 0.008862. Value loss: 0.024959. Entropy: 0.621466.\n",
      "Iteration 3722: Policy loss: -0.007938. Value loss: 0.019077. Entropy: 0.608343.\n",
      "Iteration 3723: Policy loss: -0.013602. Value loss: 0.015790. Entropy: 0.611535.\n",
      "episode: 2839   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 692     evaluation reward: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2840   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 764     evaluation reward: 8.9\n",
      "Training network. lr: 0.000222. clip: 0.088831\n",
      "Iteration 3724: Policy loss: 0.008967. Value loss: 0.021419. Entropy: 0.661391.\n",
      "Iteration 3725: Policy loss: -0.007801. Value loss: 0.015616. Entropy: 0.655325.\n",
      "Iteration 3726: Policy loss: -0.015066. Value loss: 0.013836. Entropy: 0.642371.\n",
      "episode: 2841   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 533     evaluation reward: 8.94\n",
      "episode: 2842   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 610     evaluation reward: 8.93\n",
      "Training network. lr: 0.000222. clip: 0.088822\n",
      "Iteration 3727: Policy loss: 0.006889. Value loss: 0.019383. Entropy: 0.647327.\n",
      "Iteration 3728: Policy loss: -0.004904. Value loss: 0.015006. Entropy: 0.651797.\n",
      "Iteration 3729: Policy loss: -0.012525. Value loss: 0.012751. Entropy: 0.645474.\n",
      "episode: 2843   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 587     evaluation reward: 8.97\n",
      "episode: 2844   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 401     evaluation reward: 8.9\n",
      "Training network. lr: 0.000222. clip: 0.088813\n",
      "Iteration 3730: Policy loss: 0.007724. Value loss: 0.021256. Entropy: 0.729468.\n",
      "Iteration 3731: Policy loss: -0.008517. Value loss: 0.015470. Entropy: 0.731793.\n",
      "Iteration 3732: Policy loss: -0.019773. Value loss: 0.013170. Entropy: 0.730771.\n",
      "episode: 2845   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 664     evaluation reward: 8.94\n",
      "episode: 2846   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 407     evaluation reward: 8.91\n",
      "Training network. lr: 0.000222. clip: 0.088804\n",
      "Iteration 3733: Policy loss: 0.002365. Value loss: 0.049571. Entropy: 0.766285.\n",
      "Iteration 3734: Policy loss: -0.004711. Value loss: 0.036395. Entropy: 0.757146.\n",
      "Iteration 3735: Policy loss: -0.012633. Value loss: 0.030240. Entropy: 0.781929.\n",
      "episode: 2847   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 465     evaluation reward: 8.82\n",
      "episode: 2848   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 376     evaluation reward: 8.73\n",
      "Training network. lr: 0.000222. clip: 0.088795\n",
      "Iteration 3736: Policy loss: 0.003903. Value loss: 0.030195. Entropy: 0.788816.\n",
      "Iteration 3737: Policy loss: -0.004842. Value loss: 0.020219. Entropy: 0.792174.\n",
      "Iteration 3738: Policy loss: -0.010276. Value loss: 0.018179. Entropy: 0.782180.\n",
      "episode: 2849   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 675     evaluation reward: 8.77\n",
      "Training network. lr: 0.000222. clip: 0.088786\n",
      "Iteration 3739: Policy loss: 0.009103. Value loss: 0.037171. Entropy: 0.750044.\n",
      "Iteration 3740: Policy loss: -0.009298. Value loss: 0.029311. Entropy: 0.749205.\n",
      "Iteration 3741: Policy loss: -0.016232. Value loss: 0.024359. Entropy: 0.746192.\n",
      "episode: 2850   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 639     evaluation reward: 8.69\n",
      "episode: 2851   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 601     evaluation reward: 8.7\n",
      "episode: 2852   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 354     evaluation reward: 8.69\n",
      "Training network. lr: 0.000222. clip: 0.088777\n",
      "Iteration 3742: Policy loss: 0.007093. Value loss: 0.022779. Entropy: 0.679860.\n",
      "Iteration 3743: Policy loss: -0.010395. Value loss: 0.017488. Entropy: 0.667515.\n",
      "Iteration 3744: Policy loss: -0.014414. Value loss: 0.013763. Entropy: 0.670985.\n",
      "episode: 2853   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 730     evaluation reward: 8.71\n",
      "Training network. lr: 0.000222. clip: 0.088768\n",
      "Iteration 3745: Policy loss: 0.003502. Value loss: 0.022659. Entropy: 0.767866.\n",
      "Iteration 3746: Policy loss: -0.009808. Value loss: 0.019936. Entropy: 0.763193.\n",
      "Iteration 3747: Policy loss: -0.021466. Value loss: 0.015018. Entropy: 0.763565.\n",
      "episode: 2854   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 547     evaluation reward: 8.78\n",
      "episode: 2855   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 726     evaluation reward: 8.84\n",
      "Training network. lr: 0.000222. clip: 0.088759\n",
      "Iteration 3748: Policy loss: 0.011536. Value loss: 0.060482. Entropy: 0.791275.\n",
      "Iteration 3749: Policy loss: 0.003367. Value loss: 0.028864. Entropy: 0.783646.\n",
      "Iteration 3750: Policy loss: -0.003484. Value loss: 0.024605. Entropy: 0.781557.\n",
      "episode: 2856   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 405     evaluation reward: 8.69\n",
      "episode: 2857   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 368     evaluation reward: 8.63\n",
      "Training network. lr: 0.000222. clip: 0.088750\n",
      "Iteration 3751: Policy loss: 0.009233. Value loss: 0.035528. Entropy: 0.746893.\n",
      "Iteration 3752: Policy loss: -0.008156. Value loss: 0.023350. Entropy: 0.749202.\n",
      "Iteration 3753: Policy loss: -0.016050. Value loss: 0.019798. Entropy: 0.739702.\n",
      "episode: 2858   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 367     evaluation reward: 8.58\n",
      "episode: 2859   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 619     evaluation reward: 8.56\n",
      "Training network. lr: 0.000222. clip: 0.088741\n",
      "Iteration 3754: Policy loss: 0.007967. Value loss: 0.020739. Entropy: 0.684378.\n",
      "Iteration 3755: Policy loss: -0.005899. Value loss: 0.014811. Entropy: 0.693090.\n",
      "Iteration 3756: Policy loss: -0.016024. Value loss: 0.012849. Entropy: 0.677383.\n",
      "episode: 2860   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 619     evaluation reward: 8.57\n",
      "episode: 2861   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 8.53\n",
      "Training network. lr: 0.000222. clip: 0.088732\n",
      "Iteration 3757: Policy loss: 0.013774. Value loss: 0.017178. Entropy: 0.729766.\n",
      "Iteration 3758: Policy loss: -0.006964. Value loss: 0.014552. Entropy: 0.730057.\n",
      "Iteration 3759: Policy loss: -0.015492. Value loss: 0.012510. Entropy: 0.716543.\n",
      "episode: 2862   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 413     evaluation reward: 8.49\n",
      "episode: 2863   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 677     evaluation reward: 8.48\n",
      "Training network. lr: 0.000222. clip: 0.088723\n",
      "Iteration 3760: Policy loss: 0.003915. Value loss: 0.018210. Entropy: 0.706184.\n",
      "Iteration 3761: Policy loss: -0.012289. Value loss: 0.012121. Entropy: 0.693733.\n",
      "Iteration 3762: Policy loss: -0.015722. Value loss: 0.010853. Entropy: 0.695431.\n",
      "episode: 2864   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 455     evaluation reward: 8.45\n",
      "episode: 2865   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 567     evaluation reward: 8.43\n",
      "Training network. lr: 0.000222. clip: 0.088714\n",
      "Iteration 3763: Policy loss: 0.005393. Value loss: 0.020241. Entropy: 0.627019.\n",
      "Iteration 3764: Policy loss: -0.010464. Value loss: 0.015516. Entropy: 0.631683.\n",
      "Iteration 3765: Policy loss: -0.014234. Value loss: 0.012539. Entropy: 0.632613.\n",
      "episode: 2866   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 519     evaluation reward: 8.43\n",
      "episode: 2867   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 482     evaluation reward: 8.32\n",
      "Training network. lr: 0.000222. clip: 0.088705\n",
      "Iteration 3766: Policy loss: 0.012866. Value loss: 0.017619. Entropy: 0.740629.\n",
      "Iteration 3767: Policy loss: -0.003906. Value loss: 0.014590. Entropy: 0.736009.\n",
      "Iteration 3768: Policy loss: -0.010809. Value loss: 0.012286. Entropy: 0.738269.\n",
      "episode: 2868   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 604     evaluation reward: 8.31\n",
      "Training network. lr: 0.000222. clip: 0.088696\n",
      "Iteration 3769: Policy loss: 0.007295. Value loss: 0.018196. Entropy: 0.701464.\n",
      "Iteration 3770: Policy loss: -0.009496. Value loss: 0.012594. Entropy: 0.708129.\n",
      "Iteration 3771: Policy loss: -0.022262. Value loss: 0.010774. Entropy: 0.694497.\n",
      "episode: 2869   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 802     evaluation reward: 8.38\n",
      "episode: 2870   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 479     evaluation reward: 8.32\n",
      "Training network. lr: 0.000222. clip: 0.088687\n",
      "Iteration 3772: Policy loss: 0.000969. Value loss: 0.021392. Entropy: 0.674743.\n",
      "Iteration 3773: Policy loss: -0.006809. Value loss: 0.016485. Entropy: 0.670872.\n",
      "Iteration 3774: Policy loss: -0.020889. Value loss: 0.013693. Entropy: 0.671281.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2871   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 593     evaluation reward: 8.27\n",
      "episode: 2872   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 482     evaluation reward: 8.27\n",
      "Training network. lr: 0.000222. clip: 0.088678\n",
      "Iteration 3775: Policy loss: 0.008923. Value loss: 0.028987. Entropy: 0.661699.\n",
      "Iteration 3776: Policy loss: -0.007440. Value loss: 0.021152. Entropy: 0.655860.\n",
      "Iteration 3777: Policy loss: -0.016841. Value loss: 0.017191. Entropy: 0.657527.\n",
      "episode: 2873   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 441     evaluation reward: 8.22\n",
      "episode: 2874   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 727     evaluation reward: 8.29\n",
      "Training network. lr: 0.000222. clip: 0.088669\n",
      "Iteration 3778: Policy loss: 0.010421. Value loss: 0.038319. Entropy: 0.706007.\n",
      "Iteration 3779: Policy loss: -0.007423. Value loss: 0.029358. Entropy: 0.702274.\n",
      "Iteration 3780: Policy loss: -0.015285. Value loss: 0.024840. Entropy: 0.719214.\n",
      "episode: 2875   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 657     evaluation reward: 8.28\n",
      "episode: 2876   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 545     evaluation reward: 8.29\n",
      "Training network. lr: 0.000222. clip: 0.088660\n",
      "Iteration 3781: Policy loss: 0.006809. Value loss: 0.022147. Entropy: 0.655586.\n",
      "Iteration 3782: Policy loss: -0.006754. Value loss: 0.017586. Entropy: 0.657500.\n",
      "Iteration 3783: Policy loss: -0.016628. Value loss: 0.014486. Entropy: 0.648594.\n",
      "episode: 2877   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 671     evaluation reward: 8.35\n",
      "Training network. lr: 0.000222. clip: 0.088651\n",
      "Iteration 3784: Policy loss: 0.006453. Value loss: 0.040392. Entropy: 0.693225.\n",
      "Iteration 3785: Policy loss: -0.006446. Value loss: 0.029666. Entropy: 0.687591.\n",
      "Iteration 3786: Policy loss: -0.012643. Value loss: 0.025507. Entropy: 0.677758.\n",
      "episode: 2878   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 652     evaluation reward: 8.41\n",
      "episode: 2879   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 573     evaluation reward: 8.38\n",
      "Training network. lr: 0.000222. clip: 0.088642\n",
      "Iteration 3787: Policy loss: 0.006802. Value loss: 0.038142. Entropy: 0.745453.\n",
      "Iteration 3788: Policy loss: -0.005753. Value loss: 0.028511. Entropy: 0.735157.\n",
      "Iteration 3789: Policy loss: -0.009338. Value loss: 0.026901. Entropy: 0.738579.\n",
      "episode: 2880   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 531     evaluation reward: 8.31\n",
      "episode: 2881   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 640     evaluation reward: 8.25\n",
      "Training network. lr: 0.000222. clip: 0.088633\n",
      "Iteration 3790: Policy loss: 0.009102. Value loss: 0.032208. Entropy: 0.734397.\n",
      "Iteration 3791: Policy loss: -0.006855. Value loss: 0.023610. Entropy: 0.728747.\n",
      "Iteration 3792: Policy loss: -0.015865. Value loss: 0.019450. Entropy: 0.719639.\n",
      "episode: 2882   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 410     evaluation reward: 8.18\n",
      "episode: 2883   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 568     evaluation reward: 8.21\n",
      "Training network. lr: 0.000222. clip: 0.088624\n",
      "Iteration 3793: Policy loss: 0.006286. Value loss: 0.025295. Entropy: 0.728063.\n",
      "Iteration 3794: Policy loss: -0.008091. Value loss: 0.021201. Entropy: 0.735692.\n",
      "Iteration 3795: Policy loss: -0.015499. Value loss: 0.015869. Entropy: 0.722230.\n",
      "episode: 2884   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 8.21\n",
      "episode: 2885   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 446     evaluation reward: 8.19\n",
      "Training network. lr: 0.000222. clip: 0.088615\n",
      "Iteration 3796: Policy loss: 0.001760. Value loss: 0.030482. Entropy: 0.651658.\n",
      "Iteration 3797: Policy loss: -0.012963. Value loss: 0.023058. Entropy: 0.637152.\n",
      "Iteration 3798: Policy loss: -0.020876. Value loss: 0.019224. Entropy: 0.639561.\n",
      "episode: 2886   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 806     evaluation reward: 8.25\n",
      "Training network. lr: 0.000222. clip: 0.088606\n",
      "Iteration 3799: Policy loss: 0.007305. Value loss: 0.045506. Entropy: 0.746865.\n",
      "Iteration 3800: Policy loss: -0.010685. Value loss: 0.028169. Entropy: 0.740964.\n",
      "Iteration 3801: Policy loss: -0.017352. Value loss: 0.023213. Entropy: 0.746104.\n",
      "episode: 2887   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 594     evaluation reward: 8.22\n",
      "episode: 2888   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 762     evaluation reward: 8.24\n",
      "Training network. lr: 0.000221. clip: 0.088597\n",
      "Iteration 3802: Policy loss: 0.005159. Value loss: 0.021252. Entropy: 0.637187.\n",
      "Iteration 3803: Policy loss: -0.009172. Value loss: 0.015968. Entropy: 0.637558.\n",
      "Iteration 3804: Policy loss: -0.015661. Value loss: 0.013685. Entropy: 0.629504.\n",
      "episode: 2889   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 553     evaluation reward: 8.26\n",
      "Training network. lr: 0.000221. clip: 0.088588\n",
      "Iteration 3805: Policy loss: 0.017486. Value loss: 0.030268. Entropy: 0.770821.\n",
      "Iteration 3806: Policy loss: -0.002512. Value loss: 0.024493. Entropy: 0.770949.\n",
      "Iteration 3807: Policy loss: -0.011750. Value loss: 0.020904. Entropy: 0.763035.\n",
      "episode: 2890   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 701     evaluation reward: 8.24\n",
      "now time :  2018-12-26 14:03:47.518867\n",
      "episode: 2891   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 597     evaluation reward: 8.23\n",
      "Training network. lr: 0.000221. clip: 0.088579\n",
      "Iteration 3808: Policy loss: 0.005919. Value loss: 0.025576. Entropy: 0.597404.\n",
      "Iteration 3809: Policy loss: -0.004368. Value loss: 0.017482. Entropy: 0.597562.\n",
      "Iteration 3810: Policy loss: -0.017410. Value loss: 0.014707. Entropy: 0.589573.\n",
      "episode: 2892   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 674     evaluation reward: 8.27\n",
      "episode: 2893   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 528     evaluation reward: 8.26\n",
      "Training network. lr: 0.000221. clip: 0.088570\n",
      "Iteration 3811: Policy loss: 0.010743. Value loss: 0.027336. Entropy: 0.660989.\n",
      "Iteration 3812: Policy loss: -0.006265. Value loss: 0.020088. Entropy: 0.659077.\n",
      "Iteration 3813: Policy loss: -0.018748. Value loss: 0.015846. Entropy: 0.656732.\n",
      "episode: 2894   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 517     evaluation reward: 8.26\n",
      "episode: 2895   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 641     evaluation reward: 8.31\n",
      "Training network. lr: 0.000221. clip: 0.088561\n",
      "Iteration 3814: Policy loss: 0.006965. Value loss: 0.031684. Entropy: 0.606497.\n",
      "Iteration 3815: Policy loss: -0.008748. Value loss: 0.021510. Entropy: 0.605230.\n",
      "Iteration 3816: Policy loss: -0.015111. Value loss: 0.018667. Entropy: 0.598225.\n",
      "episode: 2896   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 477     evaluation reward: 8.3\n",
      "Training network. lr: 0.000221. clip: 0.088552\n",
      "Iteration 3817: Policy loss: 0.007491. Value loss: 0.032771. Entropy: 0.744392.\n",
      "Iteration 3818: Policy loss: -0.000775. Value loss: 0.022644. Entropy: 0.736814.\n",
      "Iteration 3819: Policy loss: -0.012077. Value loss: 0.019490. Entropy: 0.725142.\n",
      "episode: 2897   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 674     evaluation reward: 8.37\n",
      "episode: 2898   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 825     evaluation reward: 8.39\n",
      "Training network. lr: 0.000221. clip: 0.088543\n",
      "Iteration 3820: Policy loss: 0.016270. Value loss: 0.023322. Entropy: 0.637113.\n",
      "Iteration 3821: Policy loss: -0.007158. Value loss: 0.015053. Entropy: 0.616724.\n",
      "Iteration 3822: Policy loss: -0.012476. Value loss: 0.012298. Entropy: 0.616521.\n",
      "episode: 2899   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 492     evaluation reward: 8.34\n",
      "episode: 2900   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 535     evaluation reward: 8.28\n",
      "Training network. lr: 0.000221. clip: 0.088534\n",
      "Iteration 3823: Policy loss: 0.004809. Value loss: 0.029566. Entropy: 0.620197.\n",
      "Iteration 3824: Policy loss: -0.011032. Value loss: 0.024350. Entropy: 0.615814.\n",
      "Iteration 3825: Policy loss: -0.017172. Value loss: 0.020060. Entropy: 0.609327.\n",
      "episode: 2901   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 422     evaluation reward: 8.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2902   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 594     evaluation reward: 8.25\n",
      "Training network. lr: 0.000221. clip: 0.088525\n",
      "Iteration 3826: Policy loss: 0.010004. Value loss: 0.024902. Entropy: 0.639134.\n",
      "Iteration 3827: Policy loss: -0.003379. Value loss: 0.018662. Entropy: 0.642522.\n",
      "Iteration 3828: Policy loss: -0.016503. Value loss: 0.017256. Entropy: 0.635195.\n",
      "episode: 2903   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 658     evaluation reward: 8.26\n",
      "Training network. lr: 0.000221. clip: 0.088516\n",
      "Iteration 3829: Policy loss: 0.006688. Value loss: 0.027070. Entropy: 0.686582.\n",
      "Iteration 3830: Policy loss: -0.004878. Value loss: 0.017242. Entropy: 0.666253.\n",
      "Iteration 3831: Policy loss: -0.016296. Value loss: 0.014612. Entropy: 0.663842.\n",
      "episode: 2904   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 658     evaluation reward: 8.27\n",
      "episode: 2905   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 653     evaluation reward: 8.3\n",
      "Training network. lr: 0.000221. clip: 0.088507\n",
      "Iteration 3832: Policy loss: 0.010349. Value loss: 0.025082. Entropy: 0.668749.\n",
      "Iteration 3833: Policy loss: -0.005097. Value loss: 0.017142. Entropy: 0.679913.\n",
      "Iteration 3834: Policy loss: -0.011337. Value loss: 0.015332. Entropy: 0.674170.\n",
      "episode: 2906   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 574     evaluation reward: 8.22\n",
      "episode: 2907   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 352     evaluation reward: 8.18\n",
      "Training network. lr: 0.000221. clip: 0.088498\n",
      "Iteration 3835: Policy loss: 0.005946. Value loss: 0.038431. Entropy: 0.670447.\n",
      "Iteration 3836: Policy loss: -0.001727. Value loss: 0.029070. Entropy: 0.673659.\n",
      "Iteration 3837: Policy loss: -0.007683. Value loss: 0.024308. Entropy: 0.669196.\n",
      "episode: 2908   score: 17.0   memory length: 1024   epsilon: 1.0    steps: 838     evaluation reward: 8.28\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3838: Policy loss: 0.008696. Value loss: 0.043728. Entropy: 0.654245.\n",
      "Iteration 3839: Policy loss: -0.005104. Value loss: 0.032413. Entropy: 0.639620.\n",
      "Iteration 3840: Policy loss: -0.013319. Value loss: 0.028342. Entropy: 0.644509.\n",
      "episode: 2909   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 782     evaluation reward: 8.31\n",
      "episode: 2910   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 575     evaluation reward: 8.34\n",
      "Training network. lr: 0.000221. clip: 0.088480\n",
      "Iteration 3841: Policy loss: 0.008694. Value loss: 0.067523. Entropy: 0.648089.\n",
      "Iteration 3842: Policy loss: -0.002148. Value loss: 0.052074. Entropy: 0.654733.\n",
      "Iteration 3843: Policy loss: -0.011969. Value loss: 0.044947. Entropy: 0.637374.\n",
      "episode: 2911   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 637     evaluation reward: 8.37\n",
      "episode: 2912   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 457     evaluation reward: 8.38\n",
      "Training network. lr: 0.000221. clip: 0.088471\n",
      "Iteration 3844: Policy loss: 0.009228. Value loss: 0.035825. Entropy: 0.633692.\n",
      "Iteration 3845: Policy loss: -0.003389. Value loss: 0.025751. Entropy: 0.636369.\n",
      "Iteration 3846: Policy loss: -0.012860. Value loss: 0.021810. Entropy: 0.637380.\n",
      "episode: 2913   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 863     evaluation reward: 8.49\n",
      "Training network. lr: 0.000221. clip: 0.088462\n",
      "Iteration 3847: Policy loss: 0.010643. Value loss: 0.024578. Entropy: 0.668155.\n",
      "Iteration 3848: Policy loss: -0.006502. Value loss: 0.017122. Entropy: 0.666783.\n",
      "Iteration 3849: Policy loss: -0.019285. Value loss: 0.014805. Entropy: 0.669990.\n",
      "episode: 2914   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 676     evaluation reward: 8.48\n",
      "Training network. lr: 0.000221. clip: 0.088453\n",
      "Iteration 3850: Policy loss: 0.008350. Value loss: 0.022368. Entropy: 0.736490.\n",
      "Iteration 3851: Policy loss: -0.004863. Value loss: 0.017396. Entropy: 0.728466.\n",
      "Iteration 3852: Policy loss: -0.015646. Value loss: 0.014568. Entropy: 0.733353.\n",
      "episode: 2915   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 651     evaluation reward: 8.47\n",
      "episode: 2916   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 545     evaluation reward: 8.48\n",
      "Training network. lr: 0.000221. clip: 0.088444\n",
      "Iteration 3853: Policy loss: 0.011139. Value loss: 0.025105. Entropy: 0.569160.\n",
      "Iteration 3854: Policy loss: -0.004690. Value loss: 0.018076. Entropy: 0.552841.\n",
      "Iteration 3855: Policy loss: -0.015428. Value loss: 0.015295. Entropy: 0.552932.\n",
      "episode: 2917   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 771     evaluation reward: 8.5\n",
      "episode: 2918   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 423     evaluation reward: 8.46\n",
      "Training network. lr: 0.000221. clip: 0.088435\n",
      "Iteration 3856: Policy loss: 0.011102. Value loss: 0.034028. Entropy: 0.633476.\n",
      "Iteration 3857: Policy loss: -0.006543. Value loss: 0.021497. Entropy: 0.625769.\n",
      "Iteration 3858: Policy loss: -0.018069. Value loss: 0.017706. Entropy: 0.623181.\n",
      "episode: 2919   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 657     evaluation reward: 8.45\n",
      "Training network. lr: 0.000221. clip: 0.088426\n",
      "Iteration 3859: Policy loss: 0.018521. Value loss: 0.019083. Entropy: 0.700104.\n",
      "Iteration 3860: Policy loss: -0.005784. Value loss: 0.016085. Entropy: 0.694499.\n",
      "Iteration 3861: Policy loss: -0.017984. Value loss: 0.013132. Entropy: 0.695754.\n",
      "episode: 2920   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 621     evaluation reward: 8.45\n",
      "episode: 2921   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 581     evaluation reward: 8.52\n",
      "Training network. lr: 0.000221. clip: 0.088417\n",
      "Iteration 3862: Policy loss: 0.010566. Value loss: 0.049650. Entropy: 0.641865.\n",
      "Iteration 3863: Policy loss: 0.001506. Value loss: 0.036742. Entropy: 0.629130.\n",
      "Iteration 3864: Policy loss: -0.014424. Value loss: 0.029726. Entropy: 0.637220.\n",
      "episode: 2922   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 702     evaluation reward: 8.56\n",
      "episode: 2923   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 568     evaluation reward: 8.56\n",
      "Training network. lr: 0.000221. clip: 0.088408\n",
      "Iteration 3865: Policy loss: 0.010579. Value loss: 0.018811. Entropy: 0.540623.\n",
      "Iteration 3866: Policy loss: -0.005062. Value loss: 0.014447. Entropy: 0.548880.\n",
      "Iteration 3867: Policy loss: -0.014085. Value loss: 0.013339. Entropy: 0.537362.\n",
      "episode: 2924   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 577     evaluation reward: 8.56\n",
      "episode: 2925   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 565     evaluation reward: 8.58\n",
      "Training network. lr: 0.000221. clip: 0.088399\n",
      "Iteration 3868: Policy loss: 0.007692. Value loss: 0.024320. Entropy: 0.674543.\n",
      "Iteration 3869: Policy loss: -0.004983. Value loss: 0.017014. Entropy: 0.646002.\n",
      "Iteration 3870: Policy loss: -0.015061. Value loss: 0.013708. Entropy: 0.657385.\n",
      "episode: 2926   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 656     evaluation reward: 8.62\n",
      "Training network. lr: 0.000221. clip: 0.088390\n",
      "Iteration 3871: Policy loss: 0.009957. Value loss: 0.047509. Entropy: 0.674671.\n",
      "Iteration 3872: Policy loss: -0.001541. Value loss: 0.033696. Entropy: 0.681969.\n",
      "Iteration 3873: Policy loss: -0.008337. Value loss: 0.027526. Entropy: 0.686395.\n",
      "episode: 2927   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 442     evaluation reward: 8.59\n",
      "episode: 2928   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 680     evaluation reward: 8.64\n",
      "Training network. lr: 0.000221. clip: 0.088381\n",
      "Iteration 3874: Policy loss: 0.006204. Value loss: 0.022858. Entropy: 0.592400.\n",
      "Iteration 3875: Policy loss: -0.005091. Value loss: 0.016006. Entropy: 0.587799.\n",
      "Iteration 3876: Policy loss: -0.013475. Value loss: 0.014754. Entropy: 0.590375.\n",
      "episode: 2929   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 563     evaluation reward: 8.53\n",
      "episode: 2930   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 584     evaluation reward: 8.47\n",
      "Training network. lr: 0.000221. clip: 0.088372\n",
      "Iteration 3877: Policy loss: 0.010333. Value loss: 0.019316. Entropy: 0.608735.\n",
      "Iteration 3878: Policy loss: -0.008707. Value loss: 0.014743. Entropy: 0.618048.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3879: Policy loss: -0.018273. Value loss: 0.014904. Entropy: 0.612786.\n",
      "episode: 2931   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 816     evaluation reward: 8.52\n",
      "Training network. lr: 0.000221. clip: 0.088363\n",
      "Iteration 3880: Policy loss: 0.009566. Value loss: 0.041882. Entropy: 0.615328.\n",
      "Iteration 3881: Policy loss: -0.006857. Value loss: 0.034046. Entropy: 0.613871.\n",
      "Iteration 3882: Policy loss: -0.013980. Value loss: 0.029653. Entropy: 0.612184.\n",
      "episode: 2932   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 736     evaluation reward: 8.57\n",
      "episode: 2933   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 537     evaluation reward: 8.6\n",
      "Training network. lr: 0.000221. clip: 0.088354\n",
      "Iteration 3883: Policy loss: 0.007846. Value loss: 0.030614. Entropy: 0.529897.\n",
      "Iteration 3884: Policy loss: -0.005617. Value loss: 0.018386. Entropy: 0.521620.\n",
      "Iteration 3885: Policy loss: -0.013782. Value loss: 0.014992. Entropy: 0.523066.\n",
      "episode: 2934   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 734     evaluation reward: 8.63\n",
      "episode: 2935   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 408     evaluation reward: 8.58\n",
      "Training network. lr: 0.000221. clip: 0.088345\n",
      "Iteration 3886: Policy loss: 0.009445. Value loss: 0.038736. Entropy: 0.643294.\n",
      "Iteration 3887: Policy loss: -0.006595. Value loss: 0.026217. Entropy: 0.633404.\n",
      "Iteration 3888: Policy loss: -0.007384. Value loss: 0.021387. Entropy: 0.623748.\n",
      "episode: 2936   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 640     evaluation reward: 8.58\n",
      "Training network. lr: 0.000221. clip: 0.088336\n",
      "Iteration 3889: Policy loss: 0.008673. Value loss: 0.022992. Entropy: 0.628091.\n",
      "Iteration 3890: Policy loss: -0.001142. Value loss: 0.016397. Entropy: 0.644328.\n",
      "Iteration 3891: Policy loss: -0.009900. Value loss: 0.013577. Entropy: 0.636415.\n",
      "episode: 2937   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 489     evaluation reward: 8.6\n",
      "episode: 2938   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 461     evaluation reward: 8.59\n",
      "Training network. lr: 0.000221. clip: 0.088327\n",
      "Iteration 3892: Policy loss: 0.003927. Value loss: 0.046244. Entropy: 0.643596.\n",
      "Iteration 3893: Policy loss: -0.011355. Value loss: 0.033110. Entropy: 0.636598.\n",
      "Iteration 3894: Policy loss: -0.018858. Value loss: 0.028580. Entropy: 0.635052.\n",
      "episode: 2939   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 477     evaluation reward: 8.57\n",
      "episode: 2940   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 773     evaluation reward: 8.58\n",
      "Training network. lr: 0.000221. clip: 0.088318\n",
      "Iteration 3895: Policy loss: 0.008779. Value loss: 0.020348. Entropy: 0.701766.\n",
      "Iteration 3896: Policy loss: -0.005170. Value loss: 0.015752. Entropy: 0.704028.\n",
      "Iteration 3897: Policy loss: -0.017281. Value loss: 0.013200. Entropy: 0.705471.\n",
      "episode: 2941   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 872     evaluation reward: 8.64\n",
      "Training network. lr: 0.000221. clip: 0.088309\n",
      "Iteration 3898: Policy loss: 0.008443. Value loss: 0.021989. Entropy: 0.580931.\n",
      "Iteration 3899: Policy loss: -0.002151. Value loss: 0.016090. Entropy: 0.572789.\n",
      "Iteration 3900: Policy loss: -0.011708. Value loss: 0.013710. Entropy: 0.576613.\n",
      "episode: 2942   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 828     evaluation reward: 8.7\n",
      "episode: 2943   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 423     evaluation reward: 8.66\n",
      "Training network. lr: 0.000221. clip: 0.088300\n",
      "Iteration 3901: Policy loss: 0.010904. Value loss: 0.039326. Entropy: 0.582773.\n",
      "Iteration 3902: Policy loss: -0.004913. Value loss: 0.024201. Entropy: 0.587409.\n",
      "Iteration 3903: Policy loss: -0.012459. Value loss: 0.019911. Entropy: 0.585049.\n",
      "episode: 2944   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 716     evaluation reward: 8.71\n",
      "Training network. lr: 0.000221. clip: 0.088291\n",
      "Iteration 3904: Policy loss: 0.005268. Value loss: 0.027060. Entropy: 0.754620.\n",
      "Iteration 3905: Policy loss: -0.002371. Value loss: 0.021393. Entropy: 0.758756.\n",
      "Iteration 3906: Policy loss: -0.016765. Value loss: 0.017347. Entropy: 0.747303.\n",
      "episode: 2945   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 555     evaluation reward: 8.65\n",
      "episode: 2946   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 300     evaluation reward: 8.63\n",
      "episode: 2947   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 424     evaluation reward: 8.62\n",
      "Training network. lr: 0.000221. clip: 0.088282\n",
      "Iteration 3907: Policy loss: 0.014562. Value loss: 0.036650. Entropy: 0.541033.\n",
      "Iteration 3908: Policy loss: -0.004816. Value loss: 0.025968. Entropy: 0.542364.\n",
      "Iteration 3909: Policy loss: -0.009451. Value loss: 0.021921. Entropy: 0.524411.\n",
      "episode: 2948   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 560     evaluation reward: 8.66\n",
      "episode: 2949   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 339     evaluation reward: 8.56\n",
      "episode: 2950   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 327     evaluation reward: 8.52\n",
      "Training network. lr: 0.000221. clip: 0.088273\n",
      "Iteration 3910: Policy loss: 0.006493. Value loss: 0.025300. Entropy: 0.560357.\n",
      "Iteration 3911: Policy loss: -0.010396. Value loss: 0.017792. Entropy: 0.567620.\n",
      "Iteration 3912: Policy loss: -0.019076. Value loss: 0.015082. Entropy: 0.566554.\n",
      "episode: 2951   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 522     evaluation reward: 8.5\n",
      "episode: 2952   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 361     evaluation reward: 8.49\n",
      "Training network. lr: 0.000221. clip: 0.088264\n",
      "Iteration 3913: Policy loss: 0.024899. Value loss: 0.019612. Entropy: 0.665125.\n",
      "Iteration 3914: Policy loss: -0.012079. Value loss: 0.014609. Entropy: 0.664051.\n",
      "Iteration 3915: Policy loss: -0.019713. Value loss: 0.012028. Entropy: 0.655925.\n",
      "episode: 2953   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 584     evaluation reward: 8.51\n",
      "episode: 2954   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 557     evaluation reward: 8.49\n",
      "Training network. lr: 0.000221. clip: 0.088255\n",
      "Iteration 3916: Policy loss: 0.010619. Value loss: 0.043615. Entropy: 0.610982.\n",
      "Iteration 3917: Policy loss: -0.007032. Value loss: 0.036645. Entropy: 0.605092.\n",
      "Iteration 3918: Policy loss: -0.017002. Value loss: 0.030037. Entropy: 0.602889.\n",
      "episode: 2955   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 515     evaluation reward: 8.46\n",
      "episode: 2956   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 406     evaluation reward: 8.47\n",
      "Training network. lr: 0.000221. clip: 0.088246\n",
      "Iteration 3919: Policy loss: 0.013300. Value loss: 0.029464. Entropy: 0.622640.\n",
      "Iteration 3920: Policy loss: -0.005777. Value loss: 0.021851. Entropy: 0.609812.\n",
      "Iteration 3921: Policy loss: -0.017904. Value loss: 0.018074. Entropy: 0.601706.\n",
      "episode: 2957   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 607     evaluation reward: 8.52\n",
      "Training network. lr: 0.000221. clip: 0.088237\n",
      "Iteration 3922: Policy loss: 0.001350. Value loss: 0.021448. Entropy: 0.668332.\n",
      "Iteration 3923: Policy loss: -0.010595. Value loss: 0.015867. Entropy: 0.670936.\n",
      "Iteration 3924: Policy loss: -0.022691. Value loss: 0.013739. Entropy: 0.670019.\n",
      "episode: 2958   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 603     evaluation reward: 8.55\n",
      "episode: 2959   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 876     evaluation reward: 8.59\n",
      "Training network. lr: 0.000221. clip: 0.088228\n",
      "Iteration 3925: Policy loss: 0.005914. Value loss: 0.019754. Entropy: 0.628878.\n",
      "Iteration 3926: Policy loss: -0.004319. Value loss: 0.014095. Entropy: 0.628475.\n",
      "Iteration 3927: Policy loss: -0.016289. Value loss: 0.011852. Entropy: 0.618809.\n",
      "episode: 2960   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 486     evaluation reward: 8.57\n",
      "episode: 2961   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 539     evaluation reward: 8.58\n",
      "Training network. lr: 0.000221. clip: 0.088219\n",
      "Iteration 3928: Policy loss: 0.004591. Value loss: 0.019976. Entropy: 0.639201.\n",
      "Iteration 3929: Policy loss: -0.008632. Value loss: 0.016236. Entropy: 0.631548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3930: Policy loss: -0.018239. Value loss: 0.014218. Entropy: 0.622410.\n",
      "episode: 2962   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 513     evaluation reward: 8.61\n",
      "episode: 2963   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 476     evaluation reward: 8.58\n",
      "Training network. lr: 0.000221. clip: 0.088210\n",
      "Iteration 3931: Policy loss: 0.004357. Value loss: 0.019007. Entropy: 0.611947.\n",
      "Iteration 3932: Policy loss: -0.007774. Value loss: 0.014956. Entropy: 0.593867.\n",
      "Iteration 3933: Policy loss: -0.018460. Value loss: 0.011976. Entropy: 0.597978.\n",
      "episode: 2964   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 623     evaluation reward: 8.61\n",
      "Training network. lr: 0.000221. clip: 0.088201\n",
      "Iteration 3934: Policy loss: 0.010058. Value loss: 0.017685. Entropy: 0.667688.\n",
      "Iteration 3935: Policy loss: -0.008793. Value loss: 0.013471. Entropy: 0.668567.\n",
      "Iteration 3936: Policy loss: -0.016448. Value loss: 0.011184. Entropy: 0.663385.\n",
      "episode: 2965   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 566     evaluation reward: 8.6\n",
      "episode: 2966   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 546     evaluation reward: 8.61\n",
      "Training network. lr: 0.000220. clip: 0.088192\n",
      "Iteration 3937: Policy loss: 0.004926. Value loss: 0.016136. Entropy: 0.587354.\n",
      "Iteration 3938: Policy loss: -0.004944. Value loss: 0.012878. Entropy: 0.575886.\n",
      "Iteration 3939: Policy loss: -0.020142. Value loss: 0.013137. Entropy: 0.573024.\n",
      "episode: 2967   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 590     evaluation reward: 8.62\n",
      "episode: 2968   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 530     evaluation reward: 8.59\n",
      "Training network. lr: 0.000220. clip: 0.088183\n",
      "Iteration 3940: Policy loss: 0.005959. Value loss: 0.018367. Entropy: 0.586303.\n",
      "Iteration 3941: Policy loss: -0.004541. Value loss: 0.014576. Entropy: 0.579054.\n",
      "Iteration 3942: Policy loss: -0.014701. Value loss: 0.012757. Entropy: 0.583938.\n",
      "episode: 2969   score: 16.0   memory length: 1024   epsilon: 1.0    steps: 783     evaluation reward: 8.62\n",
      "episode: 2970   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 511     evaluation reward: 8.63\n",
      "Training network. lr: 0.000220. clip: 0.088174\n",
      "Iteration 3943: Policy loss: 0.008617. Value loss: 0.044977. Entropy: 0.617193.\n",
      "Iteration 3944: Policy loss: 0.000009. Value loss: 0.032558. Entropy: 0.612372.\n",
      "Iteration 3945: Policy loss: -0.011252. Value loss: 0.026086. Entropy: 0.609136.\n",
      "episode: 2971   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 697     evaluation reward: 8.67\n",
      "Training network. lr: 0.000220. clip: 0.088165\n",
      "Iteration 3946: Policy loss: 0.005795. Value loss: 0.029099. Entropy: 0.638219.\n",
      "Iteration 3947: Policy loss: -0.008389. Value loss: 0.015402. Entropy: 0.646401.\n",
      "Iteration 3948: Policy loss: -0.016525. Value loss: 0.013542. Entropy: 0.634049.\n",
      "episode: 2972   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 624     evaluation reward: 8.73\n",
      "episode: 2973   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 589     evaluation reward: 8.75\n",
      "Training network. lr: 0.000220. clip: 0.088156\n",
      "Iteration 3949: Policy loss: 0.006601. Value loss: 0.057365. Entropy: 0.678343.\n",
      "Iteration 3950: Policy loss: -0.001221. Value loss: 0.045121. Entropy: 0.687482.\n",
      "Iteration 3951: Policy loss: -0.015674. Value loss: 0.039055. Entropy: 0.674912.\n",
      "episode: 2974   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 675     evaluation reward: 8.72\n",
      "episode: 2975   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 366     evaluation reward: 8.66\n",
      "Training network. lr: 0.000220. clip: 0.088147\n",
      "Iteration 3952: Policy loss: 0.006524. Value loss: 0.025410. Entropy: 0.640822.\n",
      "Iteration 3953: Policy loss: -0.009758. Value loss: 0.017234. Entropy: 0.655237.\n",
      "Iteration 3954: Policy loss: -0.016176. Value loss: 0.015924. Entropy: 0.656731.\n",
      "now time :  2018-12-26 14:08:57.237776\n",
      "episode: 2976   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 659     evaluation reward: 8.68\n",
      "Training network. lr: 0.000220. clip: 0.088138\n",
      "Iteration 3955: Policy loss: 0.009782. Value loss: 0.022571. Entropy: 0.701964.\n",
      "Iteration 3956: Policy loss: -0.005542. Value loss: 0.016122. Entropy: 0.705582.\n",
      "Iteration 3957: Policy loss: -0.015351. Value loss: 0.013181. Entropy: 0.699662.\n",
      "episode: 2977   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 821     evaluation reward: 8.71\n",
      "episode: 2978   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 657     evaluation reward: 8.67\n",
      "Training network. lr: 0.000220. clip: 0.088129\n",
      "Iteration 3958: Policy loss: 0.006601. Value loss: 0.039643. Entropy: 0.585552.\n",
      "Iteration 3959: Policy loss: -0.005441. Value loss: 0.028876. Entropy: 0.578468.\n",
      "Iteration 3960: Policy loss: -0.015125. Value loss: 0.024430. Entropy: 0.579285.\n",
      "episode: 2979   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 658     evaluation reward: 8.69\n",
      "Training network. lr: 0.000220. clip: 0.088120\n",
      "Iteration 3961: Policy loss: 0.007607. Value loss: 0.026041. Entropy: 0.643953.\n",
      "Iteration 3962: Policy loss: -0.004857. Value loss: 0.018587. Entropy: 0.649000.\n",
      "Iteration 3963: Policy loss: -0.017517. Value loss: 0.016252. Entropy: 0.630240.\n",
      "episode: 2980   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 592     evaluation reward: 8.7\n",
      "episode: 2981   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 446     evaluation reward: 8.66\n",
      "episode: 2982   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 468     evaluation reward: 8.68\n",
      "Training network. lr: 0.000220. clip: 0.088111\n",
      "Iteration 3964: Policy loss: 0.008232. Value loss: 0.032222. Entropy: 0.596092.\n",
      "Iteration 3965: Policy loss: -0.011747. Value loss: 0.024863. Entropy: 0.585726.\n",
      "Iteration 3966: Policy loss: -0.015294. Value loss: 0.022205. Entropy: 0.581693.\n",
      "episode: 2983   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 597     evaluation reward: 8.68\n",
      "Training network. lr: 0.000220. clip: 0.088102\n",
      "Iteration 3967: Policy loss: 0.009887. Value loss: 0.020908. Entropy: 0.587312.\n",
      "Iteration 3968: Policy loss: -0.005539. Value loss: 0.015731. Entropy: 0.582771.\n",
      "Iteration 3969: Policy loss: -0.011892. Value loss: 0.013895. Entropy: 0.582417.\n",
      "episode: 2984   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 858     evaluation reward: 8.74\n",
      "Training network. lr: 0.000220. clip: 0.088093\n",
      "Iteration 3970: Policy loss: 0.013904. Value loss: 0.015875. Entropy: 0.649061.\n",
      "Iteration 3971: Policy loss: -0.002462. Value loss: 0.011187. Entropy: 0.635376.\n",
      "Iteration 3972: Policy loss: -0.013351. Value loss: 0.009784. Entropy: 0.635380.\n",
      "episode: 2985   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 824     evaluation reward: 8.81\n",
      "episode: 2986   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 493     evaluation reward: 8.73\n",
      "Training network. lr: 0.000220. clip: 0.088084\n",
      "Iteration 3973: Policy loss: 0.001172. Value loss: 0.018640. Entropy: 0.567923.\n",
      "Iteration 3974: Policy loss: -0.009218. Value loss: 0.013762. Entropy: 0.567586.\n",
      "Iteration 3975: Policy loss: -0.016649. Value loss: 0.012182. Entropy: 0.584958.\n",
      "episode: 2987   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 743     evaluation reward: 8.79\n",
      "episode: 2988   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 557     evaluation reward: 8.76\n",
      "Training network. lr: 0.000220. clip: 0.088075\n",
      "Iteration 3976: Policy loss: 0.007144. Value loss: 0.043182. Entropy: 0.651116.\n",
      "Iteration 3977: Policy loss: -0.005050. Value loss: 0.028374. Entropy: 0.641888.\n",
      "Iteration 3978: Policy loss: -0.015093. Value loss: 0.024295. Entropy: 0.646781.\n",
      "episode: 2989   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 421     evaluation reward: 8.74\n",
      "episode: 2990   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 574     evaluation reward: 8.71\n",
      "Training network. lr: 0.000220. clip: 0.088066\n",
      "Iteration 3979: Policy loss: 0.008357. Value loss: 0.022309. Entropy: 0.707196.\n",
      "Iteration 3980: Policy loss: -0.005706. Value loss: 0.016382. Entropy: 0.710678.\n",
      "Iteration 3981: Policy loss: -0.016096. Value loss: 0.014111. Entropy: 0.704297.\n",
      "episode: 2991   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 620     evaluation reward: 8.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000220. clip: 0.088057\n",
      "Iteration 3982: Policy loss: 0.009872. Value loss: 0.047922. Entropy: 0.687034.\n",
      "Iteration 3983: Policy loss: -0.009690. Value loss: 0.036476. Entropy: 0.684753.\n",
      "Iteration 3984: Policy loss: -0.012607. Value loss: 0.031174. Entropy: 0.673903.\n",
      "episode: 2992   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 722     evaluation reward: 8.75\n",
      "episode: 2993   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 563     evaluation reward: 8.75\n",
      "Training network. lr: 0.000220. clip: 0.088048\n",
      "Iteration 3985: Policy loss: 0.008109. Value loss: 0.019667. Entropy: 0.586878.\n",
      "Iteration 3986: Policy loss: -0.003090. Value loss: 0.014573. Entropy: 0.587025.\n",
      "Iteration 3987: Policy loss: -0.016474. Value loss: 0.011393. Entropy: 0.581459.\n",
      "episode: 2994   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 625     evaluation reward: 8.76\n",
      "episode: 2995   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 539     evaluation reward: 8.74\n",
      "Training network. lr: 0.000220. clip: 0.088039\n",
      "Iteration 3988: Policy loss: 0.013811. Value loss: 0.027724. Entropy: 0.686412.\n",
      "Iteration 3989: Policy loss: -0.008479. Value loss: 0.019214. Entropy: 0.677102.\n",
      "Iteration 3990: Policy loss: -0.014113. Value loss: 0.016553. Entropy: 0.682668.\n",
      "episode: 2996   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 480     evaluation reward: 8.73\n",
      "episode: 2997   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 518     evaluation reward: 8.68\n",
      "Training network. lr: 0.000220. clip: 0.088030\n",
      "Iteration 3991: Policy loss: 0.006942. Value loss: 0.025454. Entropy: 0.579928.\n",
      "Iteration 3992: Policy loss: -0.009654. Value loss: 0.018643. Entropy: 0.570150.\n",
      "Iteration 3993: Policy loss: -0.020718. Value loss: 0.015832. Entropy: 0.572096.\n",
      "episode: 2998   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 684     evaluation reward: 8.69\n",
      "Training network. lr: 0.000220. clip: 0.088021\n",
      "Iteration 3994: Policy loss: 0.008724. Value loss: 0.035460. Entropy: 0.685875.\n",
      "Iteration 3995: Policy loss: -0.004065. Value loss: 0.027738. Entropy: 0.697466.\n",
      "Iteration 3996: Policy loss: -0.016672. Value loss: 0.022661. Entropy: 0.686944.\n",
      "episode: 2999   score: 17.0   memory length: 1024   epsilon: 1.0    steps: 869     evaluation reward: 8.8\n",
      "Training network. lr: 0.000220. clip: 0.088012\n",
      "Iteration 3997: Policy loss: 0.014441. Value loss: 0.047470. Entropy: 0.698082.\n",
      "Iteration 3998: Policy loss: 0.001326. Value loss: 0.035368. Entropy: 0.708651.\n",
      "Iteration 3999: Policy loss: -0.004151. Value loss: 0.030343. Entropy: 0.711497.\n",
      "episode: 3000   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 815     evaluation reward: 8.84\n",
      "episode: 3001   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 375     evaluation reward: 8.83\n",
      "Training network. lr: 0.000220. clip: 0.088003\n",
      "Iteration 4000: Policy loss: 0.013315. Value loss: 0.033678. Entropy: 0.666358.\n",
      "Iteration 4001: Policy loss: -0.007979. Value loss: 0.024500. Entropy: 0.670398.\n",
      "Iteration 4002: Policy loss: -0.018796. Value loss: 0.020125. Entropy: 0.665250.\n",
      "episode: 3002   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 8.81\n",
      "episode: 3003   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 720     evaluation reward: 8.83\n",
      "Training network. lr: 0.000220. clip: 0.087994\n",
      "Iteration 4003: Policy loss: 0.008411. Value loss: 0.021325. Entropy: 0.552285.\n",
      "Iteration 4004: Policy loss: -0.004167. Value loss: 0.015754. Entropy: 0.545395.\n",
      "Iteration 4005: Policy loss: -0.015268. Value loss: 0.012663. Entropy: 0.551261.\n",
      "episode: 3004   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 631     evaluation reward: 8.82\n",
      "episode: 3005   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 436     evaluation reward: 8.77\n",
      "Training network. lr: 0.000220. clip: 0.087985\n",
      "Iteration 4006: Policy loss: 0.002343. Value loss: 0.037282. Entropy: 0.623552.\n",
      "Iteration 4007: Policy loss: -0.001985. Value loss: 0.024112. Entropy: 0.614146.\n",
      "Iteration 4008: Policy loss: -0.018044. Value loss: 0.020642. Entropy: 0.618151.\n",
      "episode: 3006   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 666     evaluation reward: 8.78\n",
      "Training network. lr: 0.000220. clip: 0.087976\n",
      "Iteration 4009: Policy loss: 0.007911. Value loss: 0.031856. Entropy: 0.593171.\n",
      "Iteration 4010: Policy loss: -0.005873. Value loss: 0.023091. Entropy: 0.583995.\n",
      "Iteration 4011: Policy loss: -0.015224. Value loss: 0.019501. Entropy: 0.591566.\n",
      "episode: 3007   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 586     evaluation reward: 8.83\n",
      "episode: 3008   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 593     evaluation reward: 8.75\n",
      "Training network. lr: 0.000220. clip: 0.087967\n",
      "Iteration 4012: Policy loss: 0.004152. Value loss: 0.026606. Entropy: 0.598381.\n",
      "Iteration 4013: Policy loss: -0.010768. Value loss: 0.020676. Entropy: 0.592767.\n",
      "Iteration 4014: Policy loss: -0.018509. Value loss: 0.017663. Entropy: 0.600514.\n",
      "episode: 3009   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 491     evaluation reward: 8.67\n",
      "episode: 3010   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 727     evaluation reward: 8.66\n",
      "Training network. lr: 0.000220. clip: 0.087958\n",
      "Iteration 4015: Policy loss: 0.008454. Value loss: 0.019607. Entropy: 0.618375.\n",
      "Iteration 4016: Policy loss: -0.008101. Value loss: 0.016724. Entropy: 0.621310.\n",
      "Iteration 4017: Policy loss: -0.019162. Value loss: 0.012773. Entropy: 0.619936.\n",
      "episode: 3011   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 452     evaluation reward: 8.63\n",
      "episode: 3012   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 597     evaluation reward: 8.64\n",
      "Training network. lr: 0.000220. clip: 0.087949\n",
      "Iteration 4018: Policy loss: 0.018705. Value loss: 0.023065. Entropy: 0.556544.\n",
      "Iteration 4019: Policy loss: -0.006248. Value loss: 0.016620. Entropy: 0.560223.\n",
      "Iteration 4020: Policy loss: -0.007795. Value loss: 0.014288. Entropy: 0.557413.\n",
      "episode: 3013   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 690     evaluation reward: 8.6\n",
      "Training network. lr: 0.000220. clip: 0.087940\n",
      "Iteration 4021: Policy loss: 0.012509. Value loss: 0.023864. Entropy: 0.689871.\n",
      "Iteration 4022: Policy loss: -0.000554. Value loss: 0.013686. Entropy: 0.679619.\n",
      "Iteration 4023: Policy loss: -0.013230. Value loss: 0.012336. Entropy: 0.672571.\n",
      "episode: 3014   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 575     evaluation reward: 8.57\n",
      "episode: 3015   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 642     evaluation reward: 8.58\n",
      "Training network. lr: 0.000220. clip: 0.087931\n",
      "Iteration 4024: Policy loss: 0.012472. Value loss: 0.022115. Entropy: 0.613412.\n",
      "Iteration 4025: Policy loss: -0.006215. Value loss: 0.014481. Entropy: 0.604584.\n",
      "Iteration 4026: Policy loss: -0.016690. Value loss: 0.012460. Entropy: 0.605202.\n",
      "episode: 3016   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 617     evaluation reward: 8.6\n",
      "episode: 3017   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 477     evaluation reward: 8.54\n",
      "Training network. lr: 0.000220. clip: 0.087922\n",
      "Iteration 4027: Policy loss: 0.011842. Value loss: 0.021447. Entropy: 0.619950.\n",
      "Iteration 4028: Policy loss: -0.010331. Value loss: 0.015964. Entropy: 0.613704.\n",
      "Iteration 4029: Policy loss: -0.022828. Value loss: 0.013737. Entropy: 0.616457.\n",
      "episode: 3018   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 579     evaluation reward: 8.56\n",
      "Training network. lr: 0.000220. clip: 0.087913\n",
      "Iteration 4030: Policy loss: 0.005952. Value loss: 0.016907. Entropy: 0.599899.\n",
      "Iteration 4031: Policy loss: -0.006747. Value loss: 0.013043. Entropy: 0.586942.\n",
      "Iteration 4032: Policy loss: -0.016169. Value loss: 0.010665. Entropy: 0.580308.\n",
      "episode: 3019   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 800     evaluation reward: 8.58\n",
      "episode: 3020   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 647     evaluation reward: 8.62\n",
      "Training network. lr: 0.000220. clip: 0.087904\n",
      "Iteration 4033: Policy loss: 0.015199. Value loss: 0.038469. Entropy: 0.617219.\n",
      "Iteration 4034: Policy loss: -0.005319. Value loss: 0.030301. Entropy: 0.608152.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4035: Policy loss: -0.013275. Value loss: 0.025008. Entropy: 0.616985.\n",
      "episode: 3021   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 571     evaluation reward: 8.61\n",
      "Training network. lr: 0.000220. clip: 0.087895\n",
      "Iteration 4036: Policy loss: 0.007223. Value loss: 0.046764. Entropy: 0.654047.\n",
      "Iteration 4037: Policy loss: -0.007597. Value loss: 0.034285. Entropy: 0.652664.\n",
      "Iteration 4038: Policy loss: -0.014524. Value loss: 0.028781. Entropy: 0.645650.\n",
      "episode: 3022   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 836     evaluation reward: 8.65\n",
      "episode: 3023   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 710     evaluation reward: 8.67\n",
      "Training network. lr: 0.000220. clip: 0.087886\n",
      "Iteration 4039: Policy loss: 0.010973. Value loss: 0.032962. Entropy: 0.566461.\n",
      "Iteration 4040: Policy loss: -0.003787. Value loss: 0.024182. Entropy: 0.583028.\n",
      "Iteration 4041: Policy loss: -0.016045. Value loss: 0.020327. Entropy: 0.571056.\n",
      "episode: 3024   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 643     evaluation reward: 8.67\n",
      "episode: 3025   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 559     evaluation reward: 8.69\n",
      "Training network. lr: 0.000220. clip: 0.087877\n",
      "Iteration 4042: Policy loss: 0.011127. Value loss: 0.044106. Entropy: 0.554106.\n",
      "Iteration 4043: Policy loss: -0.001207. Value loss: 0.034750. Entropy: 0.542924.\n",
      "Iteration 4044: Policy loss: -0.012198. Value loss: 0.029036. Entropy: 0.538126.\n",
      "episode: 3026   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 473     evaluation reward: 8.61\n",
      "episode: 3027   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 537     evaluation reward: 8.64\n",
      "Training network. lr: 0.000220. clip: 0.087868\n",
      "Iteration 4045: Policy loss: 0.007818. Value loss: 0.033541. Entropy: 0.555342.\n",
      "Iteration 4046: Policy loss: -0.006619. Value loss: 0.025008. Entropy: 0.541105.\n",
      "Iteration 4047: Policy loss: -0.016272. Value loss: 0.021362. Entropy: 0.551842.\n",
      "episode: 3028   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 501     evaluation reward: 8.59\n",
      "Training network. lr: 0.000220. clip: 0.087859\n",
      "Iteration 4048: Policy loss: 0.013985. Value loss: 0.065535. Entropy: 0.539364.\n",
      "Iteration 4049: Policy loss: -0.003515. Value loss: 0.045878. Entropy: 0.540460.\n",
      "Iteration 4050: Policy loss: -0.007613. Value loss: 0.038856. Entropy: 0.540771.\n",
      "episode: 3029   score: 16.0   memory length: 1024   epsilon: 1.0    steps: 770     evaluation reward: 8.67\n",
      "Training network. lr: 0.000220. clip: 0.087850\n",
      "Iteration 4051: Policy loss: 0.013099. Value loss: 0.024580. Entropy: 0.661148.\n",
      "Iteration 4052: Policy loss: -0.004876. Value loss: 0.019038. Entropy: 0.669670.\n",
      "Iteration 4053: Policy loss: -0.012867. Value loss: 0.016466. Entropy: 0.666682.\n",
      "episode: 3030   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 879     evaluation reward: 8.73\n",
      "episode: 3031   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 750     evaluation reward: 8.69\n",
      "Training network. lr: 0.000220. clip: 0.087841\n",
      "Iteration 4054: Policy loss: 0.005412. Value loss: 0.026087. Entropy: 0.593625.\n",
      "Iteration 4055: Policy loss: -0.012677. Value loss: 0.019143. Entropy: 0.595335.\n",
      "Iteration 4056: Policy loss: -0.019051. Value loss: 0.015440. Entropy: 0.580617.\n",
      "episode: 3032   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 406     evaluation reward: 8.63\n",
      "episode: 3033   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 494     evaluation reward: 8.62\n",
      "Training network. lr: 0.000220. clip: 0.087832\n",
      "Iteration 4057: Policy loss: 0.010954. Value loss: 0.026843. Entropy: 0.588032.\n",
      "Iteration 4058: Policy loss: -0.004270. Value loss: 0.019732. Entropy: 0.577251.\n",
      "Iteration 4059: Policy loss: -0.016246. Value loss: 0.017013. Entropy: 0.574085.\n",
      "episode: 3034   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 584     evaluation reward: 8.6\n",
      "episode: 3035   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 753     evaluation reward: 8.68\n",
      "Training network. lr: 0.000220. clip: 0.087823\n",
      "Iteration 4060: Policy loss: 0.008703. Value loss: 0.036657. Entropy: 0.601537.\n",
      "Iteration 4061: Policy loss: -0.003918. Value loss: 0.031320. Entropy: 0.602316.\n",
      "Iteration 4062: Policy loss: -0.013354. Value loss: 0.026241. Entropy: 0.605086.\n",
      "episode: 3036   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 704     evaluation reward: 8.69\n",
      "Training network. lr: 0.000220. clip: 0.087814\n",
      "Iteration 4063: Policy loss: 0.007092. Value loss: 0.025363. Entropy: 0.566975.\n",
      "Iteration 4064: Policy loss: -0.009090. Value loss: 0.020873. Entropy: 0.573205.\n",
      "Iteration 4065: Policy loss: -0.011354. Value loss: 0.018271. Entropy: 0.562255.\n",
      "episode: 3037   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 642     evaluation reward: 8.67\n",
      "episode: 3038   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 511     evaluation reward: 8.67\n",
      "Training network. lr: 0.000220. clip: 0.087805\n",
      "Iteration 4066: Policy loss: 0.020147. Value loss: 0.024890. Entropy: 0.594033.\n",
      "Iteration 4067: Policy loss: -0.002007. Value loss: 0.017851. Entropy: 0.599829.\n",
      "Iteration 4068: Policy loss: -0.013308. Value loss: 0.015196. Entropy: 0.596736.\n",
      "episode: 3039   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 827     evaluation reward: 8.74\n",
      "Training network. lr: 0.000219. clip: 0.087796\n",
      "Iteration 4069: Policy loss: 0.014705. Value loss: 0.017197. Entropy: 0.606152.\n",
      "Iteration 4070: Policy loss: -0.000140. Value loss: 0.012032. Entropy: 0.609667.\n",
      "Iteration 4071: Policy loss: -0.010521. Value loss: 0.011682. Entropy: 0.600386.\n",
      "episode: 3040   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 694     evaluation reward: 8.72\n",
      "episode: 3041   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 648     evaluation reward: 8.67\n",
      "Training network. lr: 0.000219. clip: 0.087787\n",
      "Iteration 4072: Policy loss: 0.007243. Value loss: 0.017085. Entropy: 0.622656.\n",
      "Iteration 4073: Policy loss: -0.004467. Value loss: 0.013020. Entropy: 0.615702.\n",
      "Iteration 4074: Policy loss: -0.019205. Value loss: 0.010595. Entropy: 0.611742.\n",
      "episode: 3042   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 512     evaluation reward: 8.6\n",
      "episode: 3043   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 564     evaluation reward: 8.62\n",
      "Training network. lr: 0.000219. clip: 0.087778\n",
      "Iteration 4075: Policy loss: 0.009677. Value loss: 0.014055. Entropy: 0.627495.\n",
      "Iteration 4076: Policy loss: -0.009469. Value loss: 0.011867. Entropy: 0.623208.\n",
      "Iteration 4077: Policy loss: -0.014121. Value loss: 0.010789. Entropy: 0.628508.\n",
      "episode: 3044   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 713     evaluation reward: 8.62\n",
      "Training network. lr: 0.000219. clip: 0.087769\n",
      "Iteration 4078: Policy loss: 0.014361. Value loss: 0.015908. Entropy: 0.676223.\n",
      "Iteration 4079: Policy loss: -0.002362. Value loss: 0.011699. Entropy: 0.663158.\n",
      "Iteration 4080: Policy loss: -0.013658. Value loss: 0.011329. Entropy: 0.667283.\n",
      "episode: 3045   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 523     evaluation reward: 8.62\n",
      "episode: 3046   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 569     evaluation reward: 8.67\n",
      "Training network. lr: 0.000219. clip: 0.087760\n",
      "Iteration 4081: Policy loss: 0.010551. Value loss: 0.015328. Entropy: 0.600395.\n",
      "Iteration 4082: Policy loss: -0.008467. Value loss: 0.011870. Entropy: 0.594459.\n",
      "Iteration 4083: Policy loss: -0.015432. Value loss: 0.011201. Entropy: 0.601209.\n",
      "episode: 3047   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 507     evaluation reward: 8.69\n",
      "episode: 3048   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 720     evaluation reward: 8.73\n",
      "Training network. lr: 0.000219. clip: 0.087751\n",
      "Iteration 4084: Policy loss: 0.005909. Value loss: 0.021555. Entropy: 0.606611.\n",
      "Iteration 4085: Policy loss: -0.009839. Value loss: 0.015627. Entropy: 0.597580.\n",
      "Iteration 4086: Policy loss: -0.021935. Value loss: 0.013203. Entropy: 0.595638.\n",
      "episode: 3049   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 563     evaluation reward: 8.77\n",
      "episode: 3050   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 461     evaluation reward: 8.79\n",
      "Training network. lr: 0.000219. clip: 0.087742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4087: Policy loss: 0.007455. Value loss: 0.021541. Entropy: 0.602093.\n",
      "Iteration 4088: Policy loss: -0.006294. Value loss: 0.014857. Entropy: 0.592687.\n",
      "Iteration 4089: Policy loss: -0.014446. Value loss: 0.012494. Entropy: 0.587416.\n",
      "episode: 3051   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 446     evaluation reward: 8.78\n",
      "episode: 3052   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 606     evaluation reward: 8.89\n",
      "Training network. lr: 0.000219. clip: 0.087733\n",
      "Iteration 4090: Policy loss: 0.005803. Value loss: 0.062679. Entropy: 0.623096.\n",
      "Iteration 4091: Policy loss: -0.005926. Value loss: 0.044547. Entropy: 0.615593.\n",
      "Iteration 4092: Policy loss: -0.008614. Value loss: 0.038052. Entropy: 0.605284.\n",
      "episode: 3053   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 580     evaluation reward: 8.88\n",
      "Training network. lr: 0.000219. clip: 0.087724\n",
      "Iteration 4093: Policy loss: 0.002744. Value loss: 0.046686. Entropy: 0.597204.\n",
      "Iteration 4094: Policy loss: -0.006324. Value loss: 0.034305. Entropy: 0.593070.\n",
      "Iteration 4095: Policy loss: -0.013577. Value loss: 0.028141. Entropy: 0.592386.\n",
      "episode: 3054   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 519     evaluation reward: 8.88\n",
      "episode: 3055   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 692     evaluation reward: 8.91\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4096: Policy loss: 0.011845. Value loss: 0.022710. Entropy: 0.651151.\n",
      "Iteration 4097: Policy loss: -0.003533. Value loss: 0.014755. Entropy: 0.654246.\n",
      "Iteration 4098: Policy loss: -0.016542. Value loss: 0.012338. Entropy: 0.661665.\n",
      "episode: 3056   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 361     evaluation reward: 8.89\n",
      "episode: 3057   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 599     evaluation reward: 8.88\n",
      "episode: 3058   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 371     evaluation reward: 8.84\n",
      "Training network. lr: 0.000219. clip: 0.087706\n",
      "Iteration 4099: Policy loss: 0.011096. Value loss: 0.024133. Entropy: 0.635398.\n",
      "Iteration 4100: Policy loss: -0.007274. Value loss: 0.018227. Entropy: 0.615305.\n",
      "Iteration 4101: Policy loss: -0.017231. Value loss: 0.014846. Entropy: 0.622434.\n",
      "now time :  2018-12-26 14:14:36.428470\n",
      "episode: 3059   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 549     evaluation reward: 8.78\n",
      "Training network. lr: 0.000219. clip: 0.087697\n",
      "Iteration 4102: Policy loss: 0.004835. Value loss: 0.019613. Entropy: 0.661048.\n",
      "Iteration 4103: Policy loss: -0.011447. Value loss: 0.014863. Entropy: 0.654499.\n",
      "Iteration 4104: Policy loss: -0.017825. Value loss: 0.012554. Entropy: 0.653951.\n",
      "episode: 3060   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 757     evaluation reward: 8.84\n",
      "episode: 3061   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 627     evaluation reward: 8.85\n",
      "Training network. lr: 0.000219. clip: 0.087688\n",
      "Iteration 4105: Policy loss: 0.003811. Value loss: 0.025196. Entropy: 0.640372.\n",
      "Iteration 4106: Policy loss: -0.013167. Value loss: 0.016130. Entropy: 0.629707.\n",
      "Iteration 4107: Policy loss: -0.016805. Value loss: 0.013501. Entropy: 0.620744.\n",
      "episode: 3062   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 457     evaluation reward: 8.82\n",
      "Training network. lr: 0.000219. clip: 0.087679\n",
      "Iteration 4108: Policy loss: 0.006265. Value loss: 0.018935. Entropy: 0.688272.\n",
      "Iteration 4109: Policy loss: -0.007172. Value loss: 0.015479. Entropy: 0.697454.\n",
      "Iteration 4110: Policy loss: -0.017620. Value loss: 0.012711. Entropy: 0.685945.\n",
      "episode: 3063   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 869     evaluation reward: 8.91\n",
      "episode: 3064   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 548     evaluation reward: 8.92\n",
      "Training network. lr: 0.000219. clip: 0.087670\n",
      "Iteration 4111: Policy loss: 0.007866. Value loss: 0.073212. Entropy: 0.608771.\n",
      "Iteration 4112: Policy loss: -0.004299. Value loss: 0.056764. Entropy: 0.608300.\n",
      "Iteration 4113: Policy loss: -0.013918. Value loss: 0.048262. Entropy: 0.608174.\n",
      "episode: 3065   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 447     evaluation reward: 8.91\n",
      "episode: 3066   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 558     evaluation reward: 8.9\n",
      "Training network. lr: 0.000219. clip: 0.087661\n",
      "Iteration 4114: Policy loss: 0.008040. Value loss: 0.023995. Entropy: 0.685662.\n",
      "Iteration 4115: Policy loss: -0.007637. Value loss: 0.016224. Entropy: 0.693497.\n",
      "Iteration 4116: Policy loss: -0.012554. Value loss: 0.015392. Entropy: 0.682461.\n",
      "episode: 3067   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 362     evaluation reward: 8.86\n",
      "episode: 3068   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 570     evaluation reward: 8.88\n",
      "Training network. lr: 0.000219. clip: 0.087652\n",
      "Iteration 4117: Policy loss: 0.003882. Value loss: 0.022531. Entropy: 0.568867.\n",
      "Iteration 4118: Policy loss: -0.008760. Value loss: 0.012666. Entropy: 0.567803.\n",
      "Iteration 4119: Policy loss: -0.016865. Value loss: 0.010368. Entropy: 0.571821.\n",
      "episode: 3069   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 660     evaluation reward: 8.81\n",
      "episode: 3070   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 698     evaluation reward: 8.84\n",
      "Training network. lr: 0.000219. clip: 0.087643\n",
      "Iteration 4120: Policy loss: 0.010266. Value loss: 0.015639. Entropy: 0.598466.\n",
      "Iteration 4121: Policy loss: -0.005030. Value loss: 0.011722. Entropy: 0.596539.\n",
      "Iteration 4122: Policy loss: -0.018379. Value loss: 0.008914. Entropy: 0.583250.\n",
      "episode: 3071   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 573     evaluation reward: 8.8\n",
      "Training network. lr: 0.000219. clip: 0.087634\n",
      "Iteration 4123: Policy loss: 0.009259. Value loss: 0.021096. Entropy: 0.659597.\n",
      "Iteration 4124: Policy loss: -0.008028. Value loss: 0.014946. Entropy: 0.660965.\n",
      "Iteration 4125: Policy loss: -0.013919. Value loss: 0.012210. Entropy: 0.662921.\n",
      "episode: 3072   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 736     evaluation reward: 8.82\n",
      "episode: 3073   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 625     evaluation reward: 8.87\n",
      "Training network. lr: 0.000219. clip: 0.087625\n",
      "Iteration 4126: Policy loss: 0.008568. Value loss: 0.052791. Entropy: 0.631986.\n",
      "Iteration 4127: Policy loss: -0.006150. Value loss: 0.036767. Entropy: 0.630161.\n",
      "Iteration 4128: Policy loss: -0.007668. Value loss: 0.031823. Entropy: 0.640317.\n",
      "episode: 3074   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 660     evaluation reward: 8.87\n",
      "Training network. lr: 0.000219. clip: 0.087616\n",
      "Iteration 4129: Policy loss: 0.006139. Value loss: 0.020682. Entropy: 0.644501.\n",
      "Iteration 4130: Policy loss: -0.007754. Value loss: 0.014134. Entropy: 0.634052.\n",
      "Iteration 4131: Policy loss: -0.017526. Value loss: 0.012198. Entropy: 0.632988.\n",
      "episode: 3075   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 651     evaluation reward: 8.94\n",
      "episode: 3076   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 628     evaluation reward: 8.95\n",
      "Training network. lr: 0.000219. clip: 0.087607\n",
      "Iteration 4132: Policy loss: 0.004876. Value loss: 0.024491. Entropy: 0.616426.\n",
      "Iteration 4133: Policy loss: -0.010843. Value loss: 0.018639. Entropy: 0.608025.\n",
      "Iteration 4134: Policy loss: -0.017883. Value loss: 0.015150. Entropy: 0.600465.\n",
      "episode: 3077   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 612     evaluation reward: 8.89\n",
      "Training network. lr: 0.000219. clip: 0.087598\n",
      "Iteration 4135: Policy loss: 0.007685. Value loss: 0.047417. Entropy: 0.678085.\n",
      "Iteration 4136: Policy loss: -0.003381. Value loss: 0.039066. Entropy: 0.665621.\n",
      "Iteration 4137: Policy loss: -0.013314. Value loss: 0.033604. Entropy: 0.672329.\n",
      "episode: 3078   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 851     evaluation reward: 8.94\n",
      "episode: 3079   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 845     evaluation reward: 8.98\n",
      "Training network. lr: 0.000219. clip: 0.087589\n",
      "Iteration 4138: Policy loss: 0.006319. Value loss: 0.018514. Entropy: 0.643808.\n",
      "Iteration 4139: Policy loss: -0.009255. Value loss: 0.013687. Entropy: 0.642929.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4140: Policy loss: -0.016271. Value loss: 0.011602. Entropy: 0.647391.\n",
      "episode: 3080   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 825     evaluation reward: 9.03\n",
      "Training network. lr: 0.000219. clip: 0.087580\n",
      "Iteration 4141: Policy loss: 0.010068. Value loss: 0.026306. Entropy: 0.635313.\n",
      "Iteration 4142: Policy loss: -0.009810. Value loss: 0.020005. Entropy: 0.632057.\n",
      "Iteration 4143: Policy loss: -0.017703. Value loss: 0.016865. Entropy: 0.638863.\n",
      "episode: 3081   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 556     evaluation reward: 9.05\n",
      "Training network. lr: 0.000219. clip: 0.087571\n",
      "Iteration 4144: Policy loss: 0.003186. Value loss: 0.024830. Entropy: 0.614438.\n",
      "Iteration 4145: Policy loss: -0.007008. Value loss: 0.018024. Entropy: 0.622127.\n",
      "Iteration 4146: Policy loss: -0.013911. Value loss: 0.015401. Entropy: 0.618567.\n",
      "episode: 3082   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 719     evaluation reward: 9.1\n",
      "episode: 3083   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 462     evaluation reward: 9.07\n",
      "Training network. lr: 0.000219. clip: 0.087562\n",
      "Iteration 4147: Policy loss: 0.005238. Value loss: 0.030554. Entropy: 0.743464.\n",
      "Iteration 4148: Policy loss: -0.008875. Value loss: 0.022577. Entropy: 0.753058.\n",
      "Iteration 4149: Policy loss: -0.015364. Value loss: 0.020282. Entropy: 0.741696.\n",
      "episode: 3084   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 615     evaluation reward: 9.04\n",
      "episode: 3085   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 478     evaluation reward: 9.01\n",
      "episode: 3086   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 400     evaluation reward: 9.0\n",
      "Training network. lr: 0.000219. clip: 0.087553\n",
      "Iteration 4150: Policy loss: 0.001000. Value loss: 0.046853. Entropy: 0.656595.\n",
      "Iteration 4151: Policy loss: -0.006031. Value loss: 0.039429. Entropy: 0.661566.\n",
      "Iteration 4152: Policy loss: -0.013574. Value loss: 0.036379. Entropy: 0.648333.\n",
      "episode: 3087   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 718     evaluation reward: 8.95\n",
      "Training network. lr: 0.000219. clip: 0.087544\n",
      "Iteration 4153: Policy loss: 0.004950. Value loss: 0.029351. Entropy: 0.656001.\n",
      "Iteration 4154: Policy loss: -0.013100. Value loss: 0.021623. Entropy: 0.649394.\n",
      "Iteration 4155: Policy loss: -0.024345. Value loss: 0.020070. Entropy: 0.645522.\n",
      "episode: 3088   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 476     evaluation reward: 8.92\n",
      "episode: 3089   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 605     evaluation reward: 8.96\n",
      "Training network. lr: 0.000219. clip: 0.087535\n",
      "Iteration 4156: Policy loss: 0.006995. Value loss: 0.025200. Entropy: 0.666202.\n",
      "Iteration 4157: Policy loss: -0.006738. Value loss: 0.016176. Entropy: 0.657279.\n",
      "Iteration 4158: Policy loss: -0.014914. Value loss: 0.013256. Entropy: 0.652413.\n",
      "episode: 3090   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 679     evaluation reward: 8.99\n",
      "episode: 3091   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 532     evaluation reward: 8.94\n",
      "Training network. lr: 0.000219. clip: 0.087526\n",
      "Iteration 4159: Policy loss: 0.009408. Value loss: 0.019286. Entropy: 0.695705.\n",
      "Iteration 4160: Policy loss: -0.008666. Value loss: 0.015531. Entropy: 0.693566.\n",
      "Iteration 4161: Policy loss: -0.015408. Value loss: 0.013212. Entropy: 0.681308.\n",
      "episode: 3092   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 576     evaluation reward: 8.93\n",
      "episode: 3093   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 579     evaluation reward: 8.94\n",
      "Training network. lr: 0.000219. clip: 0.087517\n",
      "Iteration 4162: Policy loss: 0.007781. Value loss: 0.025371. Entropy: 0.744981.\n",
      "Iteration 4163: Policy loss: -0.005522. Value loss: 0.018204. Entropy: 0.724608.\n",
      "Iteration 4164: Policy loss: -0.014627. Value loss: 0.015921. Entropy: 0.726735.\n",
      "episode: 3094   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 542     evaluation reward: 8.94\n",
      "episode: 3095   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 474     evaluation reward: 8.92\n",
      "Training network. lr: 0.000219. clip: 0.087508\n",
      "Iteration 4165: Policy loss: 0.001512. Value loss: 0.026121. Entropy: 0.694649.\n",
      "Iteration 4166: Policy loss: -0.012238. Value loss: 0.019971. Entropy: 0.681434.\n",
      "Iteration 4167: Policy loss: -0.020702. Value loss: 0.016402. Entropy: 0.686676.\n",
      "episode: 3096   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 642     evaluation reward: 8.98\n",
      "episode: 3097   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 284     evaluation reward: 8.95\n",
      "Training network. lr: 0.000219. clip: 0.087499\n",
      "Iteration 4168: Policy loss: 0.008118. Value loss: 0.052772. Entropy: 0.723123.\n",
      "Iteration 4169: Policy loss: -0.003251. Value loss: 0.037959. Entropy: 0.712546.\n",
      "Iteration 4170: Policy loss: -0.014369. Value loss: 0.032644. Entropy: 0.706894.\n",
      "episode: 3098   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 538     evaluation reward: 8.89\n",
      "episode: 3099   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 426     evaluation reward: 8.77\n",
      "Training network. lr: 0.000219. clip: 0.087490\n",
      "Iteration 4171: Policy loss: 0.009697. Value loss: 0.016646. Entropy: 0.676810.\n",
      "Iteration 4172: Policy loss: -0.010150. Value loss: 0.012908. Entropy: 0.679070.\n",
      "Iteration 4173: Policy loss: -0.015062. Value loss: 0.010944. Entropy: 0.674212.\n",
      "episode: 3100   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 439     evaluation reward: 8.71\n",
      "episode: 3101   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 641     evaluation reward: 8.76\n",
      "Training network. lr: 0.000219. clip: 0.087481\n",
      "Iteration 4174: Policy loss: 0.003913. Value loss: 0.021524. Entropy: 0.626554.\n",
      "Iteration 4175: Policy loss: -0.008110. Value loss: 0.013600. Entropy: 0.615501.\n",
      "Iteration 4176: Policy loss: -0.015471. Value loss: 0.011784. Entropy: 0.622141.\n",
      "episode: 3102   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 344     evaluation reward: 8.74\n",
      "episode: 3103   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 507     evaluation reward: 8.69\n",
      "Training network. lr: 0.000219. clip: 0.087472\n",
      "Iteration 4177: Policy loss: 0.008297. Value loss: 0.022102. Entropy: 0.669972.\n",
      "Iteration 4178: Policy loss: -0.007475. Value loss: 0.017864. Entropy: 0.675651.\n",
      "Iteration 4179: Policy loss: -0.017793. Value loss: 0.014624. Entropy: 0.683593.\n",
      "episode: 3104   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 658     evaluation reward: 8.7\n",
      "episode: 3105   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 368     evaluation reward: 8.69\n",
      "Training network. lr: 0.000219. clip: 0.087463\n",
      "Iteration 4180: Policy loss: 0.012859. Value loss: 0.028362. Entropy: 0.763289.\n",
      "Iteration 4181: Policy loss: 0.000539. Value loss: 0.019135. Entropy: 0.756279.\n",
      "Iteration 4182: Policy loss: -0.018360. Value loss: 0.016739. Entropy: 0.753896.\n",
      "episode: 3106   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 468     evaluation reward: 8.65\n",
      "episode: 3107   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 623     evaluation reward: 8.65\n",
      "Training network. lr: 0.000219. clip: 0.087454\n",
      "Iteration 4183: Policy loss: 0.006192. Value loss: 0.017714. Entropy: 0.656957.\n",
      "Iteration 4184: Policy loss: -0.010474. Value loss: 0.013670. Entropy: 0.648048.\n",
      "Iteration 4185: Policy loss: -0.016962. Value loss: 0.011453. Entropy: 0.650242.\n",
      "episode: 3108   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 481     evaluation reward: 8.62\n",
      "episode: 3109   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 606     evaluation reward: 8.65\n",
      "Training network. lr: 0.000219. clip: 0.087445\n",
      "Iteration 4186: Policy loss: 0.013854. Value loss: 0.015178. Entropy: 0.674277.\n",
      "Iteration 4187: Policy loss: -0.002772. Value loss: 0.011026. Entropy: 0.661861.\n",
      "Iteration 4188: Policy loss: -0.013798. Value loss: 0.009973. Entropy: 0.651123.\n",
      "episode: 3110   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 583     evaluation reward: 8.64\n",
      "episode: 3111   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 599     evaluation reward: 8.66\n",
      "Training network. lr: 0.000219. clip: 0.087436\n",
      "Iteration 4189: Policy loss: 0.011473. Value loss: 0.026430. Entropy: 0.755156.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4190: Policy loss: -0.009034. Value loss: 0.017394. Entropy: 0.757720.\n",
      "Iteration 4191: Policy loss: -0.014515. Value loss: 0.014561. Entropy: 0.754336.\n",
      "episode: 3112   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 442     evaluation reward: 8.64\n",
      "episode: 3113   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 444     evaluation reward: 8.6\n",
      "Training network. lr: 0.000219. clip: 0.087427\n",
      "Iteration 4192: Policy loss: 0.009141. Value loss: 0.022424. Entropy: 0.665890.\n",
      "Iteration 4193: Policy loss: -0.009977. Value loss: 0.015892. Entropy: 0.662690.\n",
      "Iteration 4194: Policy loss: -0.015493. Value loss: 0.014319. Entropy: 0.656221.\n",
      "episode: 3114   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 402     evaluation reward: 8.58\n",
      "episode: 3115   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 726     evaluation reward: 8.6\n",
      "Training network. lr: 0.000219. clip: 0.087418\n",
      "Iteration 4195: Policy loss: 0.010938. Value loss: 0.025360. Entropy: 0.779564.\n",
      "Iteration 4196: Policy loss: -0.009822. Value loss: 0.014496. Entropy: 0.758948.\n",
      "Iteration 4197: Policy loss: -0.018470. Value loss: 0.011875. Entropy: 0.764491.\n",
      "episode: 3116   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 515     evaluation reward: 8.58\n",
      "episode: 3117   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 376     evaluation reward: 8.56\n",
      "Training network. lr: 0.000219. clip: 0.087409\n",
      "Iteration 4198: Policy loss: 0.009463. Value loss: 0.015997. Entropy: 0.682220.\n",
      "Iteration 4199: Policy loss: -0.007172. Value loss: 0.013048. Entropy: 0.682962.\n",
      "Iteration 4200: Policy loss: -0.011826. Value loss: 0.011118. Entropy: 0.681707.\n",
      "episode: 3118   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 513     evaluation reward: 8.54\n",
      "episode: 3119   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 665     evaluation reward: 8.52\n",
      "Training network. lr: 0.000218. clip: 0.087400\n",
      "Iteration 4201: Policy loss: 0.004930. Value loss: 0.017560. Entropy: 0.596848.\n",
      "Iteration 4202: Policy loss: -0.008520. Value loss: 0.012271. Entropy: 0.584747.\n",
      "Iteration 4203: Policy loss: -0.018929. Value loss: 0.010289. Entropy: 0.581568.\n",
      "episode: 3120   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 630     evaluation reward: 8.54\n",
      "Training network. lr: 0.000218. clip: 0.087391\n",
      "Iteration 4204: Policy loss: 0.036361. Value loss: 0.249615. Entropy: 0.717869.\n",
      "Iteration 4205: Policy loss: 0.016581. Value loss: 0.181207. Entropy: 0.710786.\n",
      "Iteration 4206: Policy loss: 0.008965. Value loss: 0.160700. Entropy: 0.725675.\n",
      "episode: 3121   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 665     evaluation reward: 8.51\n",
      "episode: 3122   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 602     evaluation reward: 8.48\n",
      "Training network. lr: 0.000218. clip: 0.087382\n",
      "Iteration 4207: Policy loss: 0.007641. Value loss: 0.048237. Entropy: 0.657425.\n",
      "Iteration 4208: Policy loss: -0.003991. Value loss: 0.034386. Entropy: 0.651459.\n",
      "Iteration 4209: Policy loss: -0.010607. Value loss: 0.029391. Entropy: 0.655335.\n",
      "episode: 3123   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 768     evaluation reward: 8.51\n",
      "Training network. lr: 0.000218. clip: 0.087373\n",
      "Iteration 4210: Policy loss: 0.006872. Value loss: 0.017751. Entropy: 0.759958.\n",
      "Iteration 4211: Policy loss: -0.008882. Value loss: 0.014384. Entropy: 0.746878.\n",
      "Iteration 4212: Policy loss: -0.020397. Value loss: 0.012895. Entropy: 0.749062.\n",
      "episode: 3124   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 596     evaluation reward: 8.54\n",
      "episode: 3125   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 786     evaluation reward: 8.55\n",
      "Training network. lr: 0.000218. clip: 0.087364\n",
      "Iteration 4213: Policy loss: 0.008085. Value loss: 0.043474. Entropy: 0.663019.\n",
      "Iteration 4214: Policy loss: -0.003237. Value loss: 0.030601. Entropy: 0.659222.\n",
      "Iteration 4215: Policy loss: -0.013957. Value loss: 0.025725. Entropy: 0.644469.\n",
      "episode: 3126   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 552     evaluation reward: 8.57\n",
      "Training network. lr: 0.000218. clip: 0.087355\n",
      "Iteration 4216: Policy loss: 0.008043. Value loss: 0.026489. Entropy: 0.666445.\n",
      "Iteration 4217: Policy loss: -0.008127. Value loss: 0.021053. Entropy: 0.649384.\n",
      "Iteration 4218: Policy loss: -0.017738. Value loss: 0.017953. Entropy: 0.654079.\n",
      "episode: 3127   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 665     evaluation reward: 8.58\n",
      "episode: 3128   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 342     evaluation reward: 8.55\n",
      "episode: 3129   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 479     evaluation reward: 8.44\n",
      "Training network. lr: 0.000218. clip: 0.087346\n",
      "Iteration 4219: Policy loss: 0.008721. Value loss: 0.031749. Entropy: 0.674050.\n",
      "Iteration 4220: Policy loss: -0.003511. Value loss: 0.023701. Entropy: 0.668278.\n",
      "Iteration 4221: Policy loss: -0.012511. Value loss: 0.019905. Entropy: 0.666489.\n",
      "episode: 3130   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 480     evaluation reward: 8.37\n",
      "Training network. lr: 0.000218. clip: 0.087337\n",
      "Iteration 4222: Policy loss: 0.006989. Value loss: 0.019868. Entropy: 0.640583.\n",
      "Iteration 4223: Policy loss: -0.008545. Value loss: 0.015097. Entropy: 0.645194.\n",
      "Iteration 4224: Policy loss: -0.016557. Value loss: 0.012865. Entropy: 0.647780.\n",
      "episode: 3131   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 707     evaluation reward: 8.35\n",
      "episode: 3132   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 534     evaluation reward: 8.37\n",
      "Training network. lr: 0.000218. clip: 0.087328\n",
      "Iteration 4225: Policy loss: 0.007986. Value loss: 0.015057. Entropy: 0.588089.\n",
      "Iteration 4226: Policy loss: -0.009050. Value loss: 0.011524. Entropy: 0.586102.\n",
      "Iteration 4227: Policy loss: -0.017237. Value loss: 0.010094. Entropy: 0.586538.\n",
      "episode: 3133   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 624     evaluation reward: 8.39\n",
      "episode: 3134   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 555     evaluation reward: 8.38\n",
      "Training network. lr: 0.000218. clip: 0.087319\n",
      "Iteration 4228: Policy loss: 0.007777. Value loss: 0.024217. Entropy: 0.626808.\n",
      "Iteration 4229: Policy loss: -0.007361. Value loss: 0.018324. Entropy: 0.618514.\n",
      "Iteration 4230: Policy loss: -0.016274. Value loss: 0.016216. Entropy: 0.620779.\n",
      "episode: 3135   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 635     evaluation reward: 8.34\n",
      "Training network. lr: 0.000218. clip: 0.087310\n",
      "Iteration 4231: Policy loss: 0.007909. Value loss: 0.041718. Entropy: 0.694857.\n",
      "Iteration 4232: Policy loss: -0.005253. Value loss: 0.033596. Entropy: 0.692372.\n",
      "Iteration 4233: Policy loss: -0.013152. Value loss: 0.030491. Entropy: 0.689043.\n",
      "episode: 3136   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 836     evaluation reward: 8.39\n",
      "episode: 3137   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 601     evaluation reward: 8.44\n",
      "Training network. lr: 0.000218. clip: 0.087301\n",
      "Iteration 4234: Policy loss: 0.008736. Value loss: 0.040359. Entropy: 0.617491.\n",
      "Iteration 4235: Policy loss: -0.007213. Value loss: 0.032722. Entropy: 0.618289.\n",
      "Iteration 4236: Policy loss: -0.012225. Value loss: 0.026774. Entropy: 0.617824.\n",
      "episode: 3138   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 740     evaluation reward: 8.49\n",
      "Training network. lr: 0.000218. clip: 0.087292\n",
      "Iteration 4237: Policy loss: 0.008126. Value loss: 0.021129. Entropy: 0.634649.\n",
      "Iteration 4238: Policy loss: -0.005076. Value loss: 0.016654. Entropy: 0.643562.\n",
      "Iteration 4239: Policy loss: -0.017203. Value loss: 0.014614. Entropy: 0.644385.\n",
      "episode: 3139   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 730     evaluation reward: 8.45\n",
      "episode: 3140   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 767     evaluation reward: 8.47\n",
      "Training network. lr: 0.000218. clip: 0.087283\n",
      "Iteration 4240: Policy loss: 0.006910. Value loss: 0.020005. Entropy: 0.566678.\n",
      "Iteration 4241: Policy loss: -0.008898. Value loss: 0.015410. Entropy: 0.563113.\n",
      "Iteration 4242: Policy loss: -0.015449. Value loss: 0.013150. Entropy: 0.567709.\n",
      "episode: 3141   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 794     evaluation reward: 8.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000218. clip: 0.087274\n",
      "Iteration 4243: Policy loss: 0.002106. Value loss: 0.020268. Entropy: 0.622031.\n",
      "Iteration 4244: Policy loss: -0.012260. Value loss: 0.015372. Entropy: 0.630067.\n",
      "Iteration 4245: Policy loss: -0.022290. Value loss: 0.013023. Entropy: 0.616951.\n",
      "episode: 3142   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 707     evaluation reward: 8.53\n",
      "episode: 3143   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 555     evaluation reward: 8.53\n",
      "Training network. lr: 0.000218. clip: 0.087265\n",
      "Iteration 4246: Policy loss: 0.003260. Value loss: 0.025080. Entropy: 0.686987.\n",
      "Iteration 4247: Policy loss: -0.006636. Value loss: 0.017320. Entropy: 0.673879.\n",
      "Iteration 4248: Policy loss: -0.013783. Value loss: 0.015174. Entropy: 0.684247.\n",
      "now time :  2018-12-26 14:20:14.818490\n",
      "episode: 3144   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 687     evaluation reward: 8.52\n",
      "Training network. lr: 0.000218. clip: 0.087256\n",
      "Iteration 4249: Policy loss: 0.011602. Value loss: 0.021816. Entropy: 0.599902.\n",
      "Iteration 4250: Policy loss: -0.003060. Value loss: 0.016317. Entropy: 0.608408.\n",
      "Iteration 4251: Policy loss: -0.013045. Value loss: 0.014029. Entropy: 0.604098.\n",
      "episode: 3145   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 863     evaluation reward: 8.58\n",
      "episode: 3146   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 508     evaluation reward: 8.56\n",
      "Training network. lr: 0.000218. clip: 0.087247\n",
      "Iteration 4252: Policy loss: 0.006330. Value loss: 0.036663. Entropy: 0.621174.\n",
      "Iteration 4253: Policy loss: -0.010786. Value loss: 0.021197. Entropy: 0.625774.\n",
      "Iteration 4254: Policy loss: -0.016887. Value loss: 0.018455. Entropy: 0.626442.\n",
      "episode: 3147   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 668     evaluation reward: 8.59\n",
      "Training network. lr: 0.000218. clip: 0.087238\n",
      "Iteration 4255: Policy loss: 0.009544. Value loss: 0.062480. Entropy: 0.707125.\n",
      "Iteration 4256: Policy loss: -0.000296. Value loss: 0.047220. Entropy: 0.710708.\n",
      "Iteration 4257: Policy loss: -0.013284. Value loss: 0.040978. Entropy: 0.717290.\n",
      "episode: 3148   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 638     evaluation reward: 8.61\n",
      "Training network. lr: 0.000218. clip: 0.087229\n",
      "Iteration 4258: Policy loss: 0.005988. Value loss: 0.048468. Entropy: 0.625714.\n",
      "Iteration 4259: Policy loss: -0.007199. Value loss: 0.034318. Entropy: 0.629429.\n",
      "Iteration 4260: Policy loss: -0.008644. Value loss: 0.028024. Entropy: 0.621806.\n",
      "episode: 3149   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 789     evaluation reward: 8.67\n",
      "episode: 3150   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 745     evaluation reward: 8.7\n",
      "Training network. lr: 0.000218. clip: 0.087220\n",
      "Iteration 4261: Policy loss: 0.003582. Value loss: 0.022713. Entropy: 0.646856.\n",
      "Iteration 4262: Policy loss: -0.011276. Value loss: 0.015642. Entropy: 0.633114.\n",
      "Iteration 4263: Policy loss: -0.014831. Value loss: 0.013983. Entropy: 0.637430.\n",
      "episode: 3151   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 593     evaluation reward: 8.73\n",
      "Training network. lr: 0.000218. clip: 0.087211\n",
      "Iteration 4264: Policy loss: 0.010134. Value loss: 0.023541. Entropy: 0.678296.\n",
      "Iteration 4265: Policy loss: -0.005540. Value loss: 0.019128. Entropy: 0.681138.\n",
      "Iteration 4266: Policy loss: -0.012038. Value loss: 0.016353. Entropy: 0.679489.\n",
      "episode: 3152   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 833     evaluation reward: 8.69\n",
      "episode: 3153   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 486     evaluation reward: 8.65\n",
      "episode: 3154   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 345     evaluation reward: 8.61\n",
      "Training network. lr: 0.000218. clip: 0.087202\n",
      "Iteration 4267: Policy loss: 0.006416. Value loss: 0.037703. Entropy: 0.557016.\n",
      "Iteration 4268: Policy loss: -0.010207. Value loss: 0.029017. Entropy: 0.552674.\n",
      "Iteration 4269: Policy loss: -0.016395. Value loss: 0.023807. Entropy: 0.540849.\n",
      "episode: 3155   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 564     evaluation reward: 8.56\n",
      "Training network. lr: 0.000218. clip: 0.087193\n",
      "Iteration 4270: Policy loss: 0.002711. Value loss: 0.019409. Entropy: 0.638611.\n",
      "Iteration 4271: Policy loss: -0.008741. Value loss: 0.016195. Entropy: 0.640000.\n",
      "Iteration 4272: Policy loss: -0.014281. Value loss: 0.014368. Entropy: 0.640361.\n",
      "episode: 3156   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 559     evaluation reward: 8.62\n",
      "episode: 3157   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 435     evaluation reward: 8.59\n",
      "Training network. lr: 0.000218. clip: 0.087184\n",
      "Iteration 4273: Policy loss: 0.014875. Value loss: 0.039799. Entropy: 0.616360.\n",
      "Iteration 4274: Policy loss: -0.006687. Value loss: 0.033093. Entropy: 0.610348.\n",
      "Iteration 4275: Policy loss: -0.013276. Value loss: 0.029102. Entropy: 0.608689.\n",
      "episode: 3158   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 708     evaluation reward: 8.65\n",
      "episode: 3159   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 673     evaluation reward: 8.67\n",
      "Training network. lr: 0.000218. clip: 0.087175\n",
      "Iteration 4276: Policy loss: 0.006882. Value loss: 0.019354. Entropy: 0.581635.\n",
      "Iteration 4277: Policy loss: -0.006866. Value loss: 0.014997. Entropy: 0.577009.\n",
      "Iteration 4278: Policy loss: -0.014918. Value loss: 0.013652. Entropy: 0.574336.\n",
      "episode: 3160   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 575     evaluation reward: 8.62\n",
      "episode: 3161   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 618     evaluation reward: 8.63\n",
      "Training network. lr: 0.000218. clip: 0.087166\n",
      "Iteration 4279: Policy loss: 0.006652. Value loss: 0.020871. Entropy: 0.673159.\n",
      "Iteration 4280: Policy loss: -0.006928. Value loss: 0.016014. Entropy: 0.672640.\n",
      "Iteration 4281: Policy loss: -0.016885. Value loss: 0.013772. Entropy: 0.674318.\n",
      "episode: 3162   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 818     evaluation reward: 8.71\n",
      "Training network. lr: 0.000218. clip: 0.087157\n",
      "Iteration 4282: Policy loss: 0.009135. Value loss: 0.025414. Entropy: 0.611021.\n",
      "Iteration 4283: Policy loss: -0.006836. Value loss: 0.016292. Entropy: 0.601874.\n",
      "Iteration 4284: Policy loss: -0.013080. Value loss: 0.013324. Entropy: 0.591153.\n",
      "episode: 3163   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 536     evaluation reward: 8.63\n",
      "Training network. lr: 0.000218. clip: 0.087148\n",
      "Iteration 4285: Policy loss: 0.008339. Value loss: 0.026410. Entropy: 0.538303.\n",
      "Iteration 4286: Policy loss: -0.003799. Value loss: 0.016670. Entropy: 0.536849.\n",
      "Iteration 4287: Policy loss: -0.014715. Value loss: 0.013800. Entropy: 0.530068.\n",
      "episode: 3164   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 721     evaluation reward: 8.64\n",
      "episode: 3165   score: 21.0   memory length: 1024   epsilon: 1.0    steps: 915     evaluation reward: 8.79\n",
      "Training network. lr: 0.000218. clip: 0.087139\n",
      "Iteration 4288: Policy loss: 0.014684. Value loss: 0.074338. Entropy: 0.596834.\n",
      "Iteration 4289: Policy loss: -0.000337. Value loss: 0.053137. Entropy: 0.600743.\n",
      "Iteration 4290: Policy loss: -0.007148. Value loss: 0.046959. Entropy: 0.601287.\n",
      "episode: 3166   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 502     evaluation reward: 8.78\n",
      "Training network. lr: 0.000218. clip: 0.087130\n",
      "Iteration 4291: Policy loss: 0.010114. Value loss: 0.029491. Entropy: 0.614463.\n",
      "Iteration 4292: Policy loss: -0.006510. Value loss: 0.021638. Entropy: 0.614853.\n",
      "Iteration 4293: Policy loss: -0.015537. Value loss: 0.017917. Entropy: 0.612575.\n",
      "episode: 3167   score: 19.0   memory length: 1024   epsilon: 1.0    steps: 1029     evaluation reward: 8.93\n",
      "Training network. lr: 0.000218. clip: 0.087121\n",
      "Iteration 4294: Policy loss: 0.006669. Value loss: 0.050503. Entropy: 0.654684.\n",
      "Iteration 4295: Policy loss: -0.008777. Value loss: 0.039260. Entropy: 0.648816.\n",
      "Iteration 4296: Policy loss: -0.016937. Value loss: 0.031416. Entropy: 0.654044.\n",
      "episode: 3168   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 826     evaluation reward: 8.97\n",
      "episode: 3169   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 550     evaluation reward: 8.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000218. clip: 0.087112\n",
      "Iteration 4297: Policy loss: 0.006500. Value loss: 0.029874. Entropy: 0.509004.\n",
      "Iteration 4298: Policy loss: -0.004537. Value loss: 0.022983. Entropy: 0.504811.\n",
      "Iteration 4299: Policy loss: -0.013322. Value loss: 0.019202. Entropy: 0.503925.\n",
      "episode: 3170   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 598     evaluation reward: 8.93\n",
      "episode: 3171   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 663     evaluation reward: 8.96\n",
      "Training network. lr: 0.000218. clip: 0.087103\n",
      "Iteration 4300: Policy loss: 0.013148. Value loss: 0.026761. Entropy: 0.565582.\n",
      "Iteration 4301: Policy loss: -0.007679. Value loss: 0.017998. Entropy: 0.560548.\n",
      "Iteration 4302: Policy loss: -0.011803. Value loss: 0.015838. Entropy: 0.558411.\n",
      "episode: 3172   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 585     evaluation reward: 8.9\n",
      "Training network. lr: 0.000218. clip: 0.087094\n",
      "Iteration 4303: Policy loss: 0.002620. Value loss: 0.021849. Entropy: 0.600267.\n",
      "Iteration 4304: Policy loss: -0.011780. Value loss: 0.016546. Entropy: 0.600236.\n",
      "Iteration 4305: Policy loss: -0.019752. Value loss: 0.013470. Entropy: 0.604953.\n",
      "episode: 3173   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 542     evaluation reward: 8.84\n",
      "episode: 3174   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 626     evaluation reward: 8.83\n",
      "Training network. lr: 0.000218. clip: 0.087085\n",
      "Iteration 4306: Policy loss: 0.008233. Value loss: 0.024775. Entropy: 0.532790.\n",
      "Iteration 4307: Policy loss: -0.003358. Value loss: 0.015849. Entropy: 0.531109.\n",
      "Iteration 4308: Policy loss: -0.013362. Value loss: 0.012327. Entropy: 0.526660.\n",
      "episode: 3175   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 735     evaluation reward: 8.83\n",
      "Training network. lr: 0.000218. clip: 0.087076\n",
      "Iteration 4309: Policy loss: 0.010772. Value loss: 0.017009. Entropy: 0.577473.\n",
      "Iteration 4310: Policy loss: -0.000991. Value loss: 0.012289. Entropy: 0.576862.\n",
      "Iteration 4311: Policy loss: -0.014569. Value loss: 0.010573. Entropy: 0.570766.\n",
      "episode: 3176   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 812     evaluation reward: 8.84\n",
      "episode: 3177   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 723     evaluation reward: 8.86\n",
      "Training network. lr: 0.000218. clip: 0.087067\n",
      "Iteration 4312: Policy loss: 0.007938. Value loss: 0.018628. Entropy: 0.596706.\n",
      "Iteration 4313: Policy loss: -0.007580. Value loss: 0.012869. Entropy: 0.593132.\n",
      "Iteration 4314: Policy loss: -0.012083. Value loss: 0.010052. Entropy: 0.583372.\n",
      "episode: 3178   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 553     evaluation reward: 8.8\n",
      "Training network. lr: 0.000218. clip: 0.087058\n",
      "Iteration 4315: Policy loss: 0.010449. Value loss: 0.019433. Entropy: 0.617660.\n",
      "Iteration 4316: Policy loss: -0.009050. Value loss: 0.015283. Entropy: 0.608002.\n",
      "Iteration 4317: Policy loss: -0.018470. Value loss: 0.012897. Entropy: 0.611271.\n",
      "episode: 3179   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 652     evaluation reward: 8.76\n",
      "episode: 3180   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 544     evaluation reward: 8.7\n",
      "Training network. lr: 0.000218. clip: 0.087049\n",
      "Iteration 4318: Policy loss: 0.003312. Value loss: 0.021324. Entropy: 0.572761.\n",
      "Iteration 4319: Policy loss: -0.007285. Value loss: 0.017768. Entropy: 0.571552.\n",
      "Iteration 4320: Policy loss: -0.015023. Value loss: 0.015069. Entropy: 0.565238.\n",
      "episode: 3181   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 504     evaluation reward: 8.7\n",
      "episode: 3182   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 644     evaluation reward: 8.7\n",
      "Training network. lr: 0.000218. clip: 0.087040\n",
      "Iteration 4321: Policy loss: 0.009729. Value loss: 0.044058. Entropy: 0.656839.\n",
      "Iteration 4322: Policy loss: 0.002361. Value loss: 0.040009. Entropy: 0.626929.\n",
      "Iteration 4323: Policy loss: -0.012718. Value loss: 0.034905. Entropy: 0.647744.\n",
      "episode: 3183   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 735     evaluation reward: 8.76\n",
      "Training network. lr: 0.000218. clip: 0.087031\n",
      "Iteration 4324: Policy loss: 0.009880. Value loss: 0.019923. Entropy: 0.629536.\n",
      "Iteration 4325: Policy loss: -0.008668. Value loss: 0.014881. Entropy: 0.623076.\n",
      "Iteration 4326: Policy loss: -0.019219. Value loss: 0.013657. Entropy: 0.618982.\n",
      "episode: 3184   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 777     evaluation reward: 8.78\n",
      "episode: 3185   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 581     evaluation reward: 8.76\n",
      "Training network. lr: 0.000218. clip: 0.087022\n",
      "Iteration 4327: Policy loss: 0.005214. Value loss: 0.020248. Entropy: 0.569108.\n",
      "Iteration 4328: Policy loss: -0.007529. Value loss: 0.016130. Entropy: 0.563722.\n",
      "Iteration 4329: Policy loss: -0.013896. Value loss: 0.013862. Entropy: 0.558517.\n",
      "episode: 3186   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 553     evaluation reward: 8.79\n",
      "episode: 3187   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 642     evaluation reward: 8.79\n",
      "Training network. lr: 0.000218. clip: 0.087013\n",
      "Iteration 4330: Policy loss: 0.004701. Value loss: 0.025741. Entropy: 0.620899.\n",
      "Iteration 4331: Policy loss: -0.007098. Value loss: 0.019693. Entropy: 0.609791.\n",
      "Iteration 4332: Policy loss: -0.014264. Value loss: 0.015722. Entropy: 0.610785.\n",
      "episode: 3188   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 576     evaluation reward: 8.82\n",
      "Training network. lr: 0.000218. clip: 0.087004\n",
      "Iteration 4333: Policy loss: 0.002685. Value loss: 0.050969. Entropy: 0.664692.\n",
      "Iteration 4334: Policy loss: -0.008747. Value loss: 0.035338. Entropy: 0.668128.\n",
      "Iteration 4335: Policy loss: -0.016232. Value loss: 0.028306. Entropy: 0.663825.\n",
      "episode: 3189   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 535     evaluation reward: 8.84\n",
      "episode: 3190   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 606     evaluation reward: 8.88\n",
      "Training network. lr: 0.000217. clip: 0.086995\n",
      "Iteration 4336: Policy loss: 0.009255. Value loss: 0.076286. Entropy: 0.596505.\n",
      "Iteration 4337: Policy loss: -0.006618. Value loss: 0.057963. Entropy: 0.596605.\n",
      "Iteration 4338: Policy loss: -0.013397. Value loss: 0.049444. Entropy: 0.602513.\n",
      "episode: 3191   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 708     evaluation reward: 8.91\n",
      "episode: 3192   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 664     evaluation reward: 8.92\n",
      "Training network. lr: 0.000217. clip: 0.086986\n",
      "Iteration 4339: Policy loss: 0.005841. Value loss: 0.021541. Entropy: 0.588246.\n",
      "Iteration 4340: Policy loss: -0.009007. Value loss: 0.016763. Entropy: 0.596174.\n",
      "Iteration 4341: Policy loss: -0.017159. Value loss: 0.013190. Entropy: 0.588561.\n",
      "episode: 3193   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 779     evaluation reward: 8.98\n",
      "Training network. lr: 0.000217. clip: 0.086977\n",
      "Iteration 4342: Policy loss: 0.006858. Value loss: 0.051814. Entropy: 0.679432.\n",
      "Iteration 4343: Policy loss: -0.006795. Value loss: 0.036720. Entropy: 0.690298.\n",
      "Iteration 4344: Policy loss: -0.012389. Value loss: 0.030023. Entropy: 0.703157.\n",
      "episode: 3194   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 658     evaluation reward: 9.0\n",
      "episode: 3195   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 575     evaluation reward: 9.09\n",
      "Training network. lr: 0.000217. clip: 0.086968\n",
      "Iteration 4345: Policy loss: 0.013875. Value loss: 0.077262. Entropy: 0.576537.\n",
      "Iteration 4346: Policy loss: -0.001032. Value loss: 0.051407. Entropy: 0.578809.\n",
      "Iteration 4347: Policy loss: -0.011027. Value loss: 0.042642. Entropy: 0.566240.\n",
      "episode: 3196   score: 17.0   memory length: 1024   epsilon: 1.0    steps: 1071     evaluation reward: 9.14\n",
      "Training network. lr: 0.000217. clip: 0.086959\n",
      "Iteration 4348: Policy loss: 0.008310. Value loss: 0.023384. Entropy: 0.556428.\n",
      "Iteration 4349: Policy loss: -0.006668. Value loss: 0.018037. Entropy: 0.550430.\n",
      "Iteration 4350: Policy loss: -0.015678. Value loss: 0.014698. Entropy: 0.551228.\n",
      "episode: 3197   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 749     evaluation reward: 9.23\n",
      "Training network. lr: 0.000217. clip: 0.086950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4351: Policy loss: 0.005715. Value loss: 0.041098. Entropy: 0.659496.\n",
      "Iteration 4352: Policy loss: -0.006866. Value loss: 0.030943. Entropy: 0.640086.\n",
      "Iteration 4353: Policy loss: -0.010794. Value loss: 0.024740. Entropy: 0.636020.\n",
      "episode: 3198   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 807     evaluation reward: 9.28\n",
      "Training network. lr: 0.000217. clip: 0.086941\n",
      "Iteration 4354: Policy loss: 0.002607. Value loss: 0.028483. Entropy: 0.577292.\n",
      "Iteration 4355: Policy loss: -0.010413. Value loss: 0.020355. Entropy: 0.573711.\n",
      "Iteration 4356: Policy loss: -0.020788. Value loss: 0.017075. Entropy: 0.568245.\n",
      "episode: 3199   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 716     evaluation reward: 9.33\n",
      "episode: 3200   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 793     evaluation reward: 9.41\n",
      "Training network. lr: 0.000217. clip: 0.086932\n",
      "Iteration 4357: Policy loss: 0.006186. Value loss: 0.029342. Entropy: 0.542614.\n",
      "Iteration 4358: Policy loss: -0.007305. Value loss: 0.020341. Entropy: 0.550580.\n",
      "Iteration 4359: Policy loss: -0.016786. Value loss: 0.016942. Entropy: 0.542207.\n",
      "episode: 3201   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 693     evaluation reward: 9.47\n",
      "Training network. lr: 0.000217. clip: 0.086923\n",
      "Iteration 4360: Policy loss: 0.007403. Value loss: 0.064817. Entropy: 0.519437.\n",
      "Iteration 4361: Policy loss: -0.000277. Value loss: 0.049338. Entropy: 0.514591.\n",
      "Iteration 4362: Policy loss: -0.005188. Value loss: 0.044092. Entropy: 0.512117.\n",
      "episode: 3202   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 788     evaluation reward: 9.56\n",
      "Training network. lr: 0.000217. clip: 0.086914\n",
      "Iteration 4363: Policy loss: 0.007685. Value loss: 0.036301. Entropy: 0.585610.\n",
      "Iteration 4364: Policy loss: -0.003945. Value loss: 0.026889. Entropy: 0.594324.\n",
      "Iteration 4365: Policy loss: -0.011828. Value loss: 0.024712. Entropy: 0.591382.\n",
      "episode: 3203   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 805     evaluation reward: 9.63\n",
      "episode: 3204   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 751     evaluation reward: 9.68\n",
      "Training network. lr: 0.000217. clip: 0.086905\n",
      "Iteration 4366: Policy loss: 0.004975. Value loss: 0.040375. Entropy: 0.441467.\n",
      "Iteration 4367: Policy loss: -0.000959. Value loss: 0.031886. Entropy: 0.455667.\n",
      "Iteration 4368: Policy loss: -0.010335. Value loss: 0.027158. Entropy: 0.436611.\n",
      "episode: 3205   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 819     evaluation reward: 9.76\n",
      "Training network. lr: 0.000217. clip: 0.086896\n",
      "Iteration 4369: Policy loss: 0.006838. Value loss: 0.027763. Entropy: 0.647350.\n",
      "Iteration 4370: Policy loss: -0.012695. Value loss: 0.020425. Entropy: 0.647302.\n",
      "Iteration 4371: Policy loss: -0.020872. Value loss: 0.015408. Entropy: 0.643859.\n",
      "episode: 3206   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 706     evaluation reward: 9.8\n",
      "Training network. lr: 0.000217. clip: 0.086887\n",
      "Iteration 4372: Policy loss: 0.009525. Value loss: 0.025935. Entropy: 0.527462.\n",
      "Iteration 4373: Policy loss: 0.000546. Value loss: 0.022378. Entropy: 0.521702.\n",
      "Iteration 4374: Policy loss: -0.014126. Value loss: 0.018162. Entropy: 0.520655.\n",
      "episode: 3207   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 597     evaluation reward: 9.8\n",
      "episode: 3208   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 531     evaluation reward: 9.84\n",
      "Training network. lr: 0.000217. clip: 0.086878\n",
      "Iteration 4375: Policy loss: 0.004200. Value loss: 0.054161. Entropy: 0.566336.\n",
      "Iteration 4376: Policy loss: -0.003693. Value loss: 0.045301. Entropy: 0.578223.\n",
      "Iteration 4377: Policy loss: -0.011297. Value loss: 0.037209. Entropy: 0.568078.\n",
      "episode: 3209   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 773     evaluation reward: 9.85\n",
      "episode: 3210   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 629     evaluation reward: 9.85\n",
      "Training network. lr: 0.000217. clip: 0.086869\n",
      "Iteration 4378: Policy loss: 0.003230. Value loss: 0.024937. Entropy: 0.544093.\n",
      "Iteration 4379: Policy loss: -0.008103. Value loss: 0.021175. Entropy: 0.542102.\n",
      "Iteration 4380: Policy loss: -0.015479. Value loss: 0.017147. Entropy: 0.545654.\n",
      "episode: 3211   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 532     evaluation reward: 9.83\n",
      "Training network. lr: 0.000217. clip: 0.086860\n",
      "Iteration 4381: Policy loss: 0.006061. Value loss: 0.042935. Entropy: 0.559168.\n",
      "Iteration 4382: Policy loss: -0.008923. Value loss: 0.027471. Entropy: 0.550304.\n",
      "Iteration 4383: Policy loss: -0.014901. Value loss: 0.021716. Entropy: 0.550454.\n",
      "episode: 3212   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 842     evaluation reward: 9.92\n",
      "Training network. lr: 0.000217. clip: 0.086851\n",
      "Iteration 4384: Policy loss: 0.005797. Value loss: 0.037795. Entropy: 0.631836.\n",
      "Iteration 4385: Policy loss: -0.007109. Value loss: 0.029707. Entropy: 0.620230.\n",
      "Iteration 4386: Policy loss: -0.007257. Value loss: 0.024622. Entropy: 0.630756.\n",
      "episode: 3213   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 945     evaluation reward: 9.97\n",
      "Training network. lr: 0.000217. clip: 0.086842\n",
      "Iteration 4387: Policy loss: 0.007914. Value loss: 0.021347. Entropy: 0.579487.\n",
      "Iteration 4388: Policy loss: -0.001743. Value loss: 0.014893. Entropy: 0.575701.\n",
      "Iteration 4389: Policy loss: -0.004785. Value loss: 0.011837. Entropy: 0.577473.\n",
      "episode: 3214   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 860     evaluation reward: 10.03\n",
      "episode: 3215   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 398     evaluation reward: 10.0\n",
      "Training network. lr: 0.000217. clip: 0.086833\n",
      "Iteration 4390: Policy loss: 0.007922. Value loss: 0.050192. Entropy: 0.651992.\n",
      "Iteration 4391: Policy loss: -0.004520. Value loss: 0.038025. Entropy: 0.632955.\n",
      "Iteration 4392: Policy loss: -0.010536. Value loss: 0.032001. Entropy: 0.626776.\n",
      "episode: 3216   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 693     evaluation reward: 10.05\n",
      "episode: 3217   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 645     evaluation reward: 10.12\n",
      "now time :  2018-12-26 14:26:02.225251\n",
      "Training network. lr: 0.000217. clip: 0.086824\n",
      "Iteration 4393: Policy loss: 0.015002. Value loss: 0.055311. Entropy: 0.642469.\n",
      "Iteration 4394: Policy loss: -0.007353. Value loss: 0.037772. Entropy: 0.627158.\n",
      "Iteration 4395: Policy loss: -0.008280. Value loss: 0.029022. Entropy: 0.636652.\n",
      "episode: 3218   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 713     evaluation reward: 10.16\n",
      "Training network. lr: 0.000217. clip: 0.086815\n",
      "Iteration 4396: Policy loss: 0.004625. Value loss: 0.040858. Entropy: 0.608898.\n",
      "Iteration 4397: Policy loss: -0.008013. Value loss: 0.028019. Entropy: 0.598693.\n",
      "Iteration 4398: Policy loss: -0.014325. Value loss: 0.022139. Entropy: 0.595983.\n",
      "episode: 3219   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 670     evaluation reward: 10.15\n",
      "episode: 3220   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 708     evaluation reward: 10.11\n",
      "Training network. lr: 0.000217. clip: 0.086806\n",
      "Iteration 4399: Policy loss: 0.013417. Value loss: 0.029792. Entropy: 0.551601.\n",
      "Iteration 4400: Policy loss: -0.006084. Value loss: 0.016779. Entropy: 0.550829.\n",
      "Iteration 4401: Policy loss: -0.010224. Value loss: 0.015294. Entropy: 0.538578.\n",
      "episode: 3221   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 739     evaluation reward: 10.13\n",
      "Training network. lr: 0.000217. clip: 0.086797\n",
      "Iteration 4402: Policy loss: 0.005511. Value loss: 0.055089. Entropy: 0.604894.\n",
      "Iteration 4403: Policy loss: -0.005826. Value loss: 0.043123. Entropy: 0.595109.\n",
      "Iteration 4404: Policy loss: -0.011678. Value loss: 0.037075. Entropy: 0.599595.\n",
      "episode: 3222   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 733     evaluation reward: 10.16\n",
      "episode: 3223   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 609     evaluation reward: 10.12\n",
      "Training network. lr: 0.000217. clip: 0.086788\n",
      "Iteration 4405: Policy loss: 0.009655. Value loss: 0.026938. Entropy: 0.485187.\n",
      "Iteration 4406: Policy loss: -0.006630. Value loss: 0.020806. Entropy: 0.478115.\n",
      "Iteration 4407: Policy loss: -0.018719. Value loss: 0.016880. Entropy: 0.489879.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3224   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 552     evaluation reward: 10.08\n",
      "episode: 3225   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 643     evaluation reward: 10.09\n",
      "Training network. lr: 0.000217. clip: 0.086779\n",
      "Iteration 4408: Policy loss: 0.003749. Value loss: 0.059928. Entropy: 0.516122.\n",
      "Iteration 4409: Policy loss: -0.003722. Value loss: 0.046842. Entropy: 0.512026.\n",
      "Iteration 4410: Policy loss: -0.012701. Value loss: 0.041674. Entropy: 0.507424.\n",
      "episode: 3226   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 591     evaluation reward: 10.09\n",
      "episode: 3227   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 401     evaluation reward: 10.05\n",
      "Training network. lr: 0.000217. clip: 0.086770\n",
      "Iteration 4411: Policy loss: 0.014444. Value loss: 0.044180. Entropy: 0.545634.\n",
      "Iteration 4412: Policy loss: -0.000539. Value loss: 0.030319. Entropy: 0.554638.\n",
      "Iteration 4413: Policy loss: -0.017362. Value loss: 0.025109. Entropy: 0.536271.\n",
      "episode: 3228   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 762     evaluation reward: 10.14\n",
      "Training network. lr: 0.000217. clip: 0.086761\n",
      "Iteration 4414: Policy loss: 0.009712. Value loss: 0.027952. Entropy: 0.610731.\n",
      "Iteration 4415: Policy loss: -0.005761. Value loss: 0.021490. Entropy: 0.613389.\n",
      "Iteration 4416: Policy loss: -0.012657. Value loss: 0.018530. Entropy: 0.614955.\n",
      "episode: 3229   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 714     evaluation reward: 10.19\n",
      "episode: 3230   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 508     evaluation reward: 10.2\n",
      "Training network. lr: 0.000217. clip: 0.086752\n",
      "Iteration 4417: Policy loss: 0.010112. Value loss: 0.032107. Entropy: 0.550333.\n",
      "Iteration 4418: Policy loss: -0.006511. Value loss: 0.022721. Entropy: 0.543412.\n",
      "Iteration 4419: Policy loss: -0.016682. Value loss: 0.019119. Entropy: 0.546919.\n",
      "episode: 3231   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 713     evaluation reward: 10.21\n",
      "Training network. lr: 0.000217. clip: 0.086743\n",
      "Iteration 4420: Policy loss: 0.029993. Value loss: 0.025345. Entropy: 0.643478.\n",
      "Iteration 4421: Policy loss: -0.005638. Value loss: 0.017181. Entropy: 0.637105.\n",
      "Iteration 4422: Policy loss: -0.012997. Value loss: 0.014707. Entropy: 0.624323.\n",
      "episode: 3232   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 613     evaluation reward: 10.22\n",
      "Training network. lr: 0.000217. clip: 0.086734\n",
      "Iteration 4423: Policy loss: 0.009933. Value loss: 0.014004. Entropy: 0.714222.\n",
      "Iteration 4424: Policy loss: -0.004129. Value loss: 0.011462. Entropy: 0.724257.\n",
      "Iteration 4425: Policy loss: -0.016596. Value loss: 0.009197. Entropy: 0.720652.\n",
      "episode: 3233   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 1038     evaluation reward: 10.23\n",
      "episode: 3234   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 782     evaluation reward: 10.24\n",
      "Training network. lr: 0.000217. clip: 0.086725\n",
      "Iteration 4426: Policy loss: 0.005315. Value loss: 0.019624. Entropy: 0.635685.\n",
      "Iteration 4427: Policy loss: -0.011372. Value loss: 0.016194. Entropy: 0.630753.\n",
      "Iteration 4428: Policy loss: -0.019820. Value loss: 0.012463. Entropy: 0.629588.\n",
      "episode: 3235   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 582     evaluation reward: 10.22\n",
      "Training network. lr: 0.000217. clip: 0.086716\n",
      "Iteration 4429: Policy loss: 0.012337. Value loss: 0.019230. Entropy: 0.592397.\n",
      "Iteration 4430: Policy loss: -0.001114. Value loss: 0.013924. Entropy: 0.588237.\n",
      "Iteration 4431: Policy loss: -0.007657. Value loss: 0.011904. Entropy: 0.592318.\n",
      "episode: 3236   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 704     evaluation reward: 10.17\n",
      "episode: 3237   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 581     evaluation reward: 10.14\n",
      "Training network. lr: 0.000217. clip: 0.086707\n",
      "Iteration 4432: Policy loss: 0.002538. Value loss: 0.016978. Entropy: 0.575733.\n",
      "Iteration 4433: Policy loss: -0.005564. Value loss: 0.013944. Entropy: 0.585640.\n",
      "Iteration 4434: Policy loss: -0.014602. Value loss: 0.011854. Entropy: 0.578200.\n",
      "episode: 3238   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 715     evaluation reward: 10.13\n",
      "episode: 3239   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 475     evaluation reward: 10.09\n",
      "Training network. lr: 0.000217. clip: 0.086698\n",
      "Iteration 4435: Policy loss: 0.009344. Value loss: 0.023181. Entropy: 0.641801.\n",
      "Iteration 4436: Policy loss: -0.005339. Value loss: 0.016461. Entropy: 0.634935.\n",
      "Iteration 4437: Policy loss: -0.012762. Value loss: 0.015755. Entropy: 0.628586.\n",
      "episode: 3240   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 435     evaluation reward: 10.03\n",
      "episode: 3241   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 482     evaluation reward: 9.97\n",
      "Training network. lr: 0.000217. clip: 0.086689\n",
      "Iteration 4438: Policy loss: 0.016298. Value loss: 0.025524. Entropy: 0.644822.\n",
      "Iteration 4439: Policy loss: -0.005391. Value loss: 0.017938. Entropy: 0.641294.\n",
      "Iteration 4440: Policy loss: -0.012520. Value loss: 0.016629. Entropy: 0.645567.\n",
      "episode: 3242   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 704     evaluation reward: 9.97\n",
      "Training network. lr: 0.000217. clip: 0.086680\n",
      "Iteration 4441: Policy loss: 0.003247. Value loss: 0.018514. Entropy: 0.572043.\n",
      "Iteration 4442: Policy loss: -0.007374. Value loss: 0.013939. Entropy: 0.570074.\n",
      "Iteration 4443: Policy loss: -0.018045. Value loss: 0.012966. Entropy: 0.558319.\n",
      "episode: 3243   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 536     evaluation reward: 9.96\n",
      "episode: 3244   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 580     evaluation reward: 9.94\n",
      "Training network. lr: 0.000217. clip: 0.086671\n",
      "Iteration 4444: Policy loss: 0.002844. Value loss: 0.030230. Entropy: 0.648669.\n",
      "Iteration 4445: Policy loss: -0.005357. Value loss: 0.021217. Entropy: 0.636937.\n",
      "Iteration 4446: Policy loss: -0.016494. Value loss: 0.016177. Entropy: 0.638214.\n",
      "episode: 3245   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 714     evaluation reward: 9.91\n",
      "episode: 3246   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 719     evaluation reward: 9.98\n",
      "Training network. lr: 0.000217. clip: 0.086662\n",
      "Iteration 4447: Policy loss: 0.009220. Value loss: 0.039742. Entropy: 0.625894.\n",
      "Iteration 4448: Policy loss: -0.003701. Value loss: 0.030041. Entropy: 0.619870.\n",
      "Iteration 4449: Policy loss: -0.011631. Value loss: 0.025923. Entropy: 0.623209.\n",
      "episode: 3247   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 610     evaluation reward: 9.97\n",
      "Training network. lr: 0.000217. clip: 0.086653\n",
      "Iteration 4450: Policy loss: 0.007456. Value loss: 0.015471. Entropy: 0.534492.\n",
      "Iteration 4451: Policy loss: -0.005353. Value loss: 0.011774. Entropy: 0.523834.\n",
      "Iteration 4452: Policy loss: -0.014518. Value loss: 0.010113. Entropy: 0.532610.\n",
      "episode: 3248   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 642     evaluation reward: 9.92\n",
      "episode: 3249   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 737     evaluation reward: 9.92\n",
      "Training network. lr: 0.000217. clip: 0.086644\n",
      "Iteration 4453: Policy loss: 0.070904. Value loss: 0.080702. Entropy: 0.707126.\n",
      "Iteration 4454: Policy loss: 0.034804. Value loss: 0.036985. Entropy: 0.681757.\n",
      "Iteration 4455: Policy loss: -0.003078. Value loss: 0.034287. Entropy: 0.671699.\n",
      "episode: 3250   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 447     evaluation reward: 9.89\n",
      "episode: 3251   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 481     evaluation reward: 9.87\n",
      "Training network. lr: 0.000217. clip: 0.086635\n",
      "Iteration 4456: Policy loss: 0.018014. Value loss: 0.027994. Entropy: 0.606323.\n",
      "Iteration 4457: Policy loss: -0.001868. Value loss: 0.021363. Entropy: 0.598460.\n",
      "Iteration 4458: Policy loss: -0.012656. Value loss: 0.016742. Entropy: 0.592261.\n",
      "episode: 3252   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 621     evaluation reward: 9.84\n",
      "episode: 3253   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 9.84\n",
      "Training network. lr: 0.000217. clip: 0.086626\n",
      "Iteration 4459: Policy loss: 0.006057. Value loss: 0.032455. Entropy: 0.563386.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4460: Policy loss: -0.001669. Value loss: 0.023230. Entropy: 0.567508.\n",
      "Iteration 4461: Policy loss: -0.014362. Value loss: 0.019139. Entropy: 0.570536.\n",
      "episode: 3254   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 616     evaluation reward: 9.91\n",
      "Training network. lr: 0.000217. clip: 0.086617\n",
      "Iteration 4462: Policy loss: 0.006567. Value loss: 0.033667. Entropy: 0.635569.\n",
      "Iteration 4463: Policy loss: -0.001918. Value loss: 0.025409. Entropy: 0.649728.\n",
      "Iteration 4464: Policy loss: -0.010612. Value loss: 0.020533. Entropy: 0.640461.\n",
      "episode: 3255   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 772     evaluation reward: 9.96\n",
      "episode: 3256   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 672     evaluation reward: 9.94\n",
      "Training network. lr: 0.000217. clip: 0.086608\n",
      "Iteration 4465: Policy loss: 0.004943. Value loss: 0.024844. Entropy: 0.655402.\n",
      "Iteration 4466: Policy loss: -0.006822. Value loss: 0.015456. Entropy: 0.646637.\n",
      "Iteration 4467: Policy loss: -0.014652. Value loss: 0.012194. Entropy: 0.650988.\n",
      "episode: 3257   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 638     evaluation reward: 10.0\n",
      "Training network. lr: 0.000216. clip: 0.086599\n",
      "Iteration 4468: Policy loss: 0.003359. Value loss: 0.047813. Entropy: 0.656611.\n",
      "Iteration 4469: Policy loss: -0.008241. Value loss: 0.039075. Entropy: 0.666092.\n",
      "Iteration 4470: Policy loss: -0.016195. Value loss: 0.035731. Entropy: 0.665237.\n",
      "episode: 3258   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 544     evaluation reward: 9.97\n",
      "episode: 3259   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 569     evaluation reward: 9.94\n",
      "Training network. lr: 0.000216. clip: 0.086590\n",
      "Iteration 4471: Policy loss: 0.010999. Value loss: 0.026320. Entropy: 0.638230.\n",
      "Iteration 4472: Policy loss: -0.007871. Value loss: 0.018344. Entropy: 0.632906.\n",
      "Iteration 4473: Policy loss: -0.016243. Value loss: 0.015890. Entropy: 0.635331.\n",
      "episode: 3260   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 520     evaluation reward: 9.94\n",
      "episode: 3261   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 481     evaluation reward: 9.91\n",
      "Training network. lr: 0.000216. clip: 0.086581\n",
      "Iteration 4474: Policy loss: 0.006487. Value loss: 0.024016. Entropy: 0.655574.\n",
      "Iteration 4475: Policy loss: -0.002653. Value loss: 0.018036. Entropy: 0.656724.\n",
      "Iteration 4476: Policy loss: -0.013020. Value loss: 0.016743. Entropy: 0.645173.\n",
      "episode: 3262   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 626     evaluation reward: 9.86\n",
      "episode: 3263   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 597     evaluation reward: 9.87\n",
      "Training network. lr: 0.000216. clip: 0.086572\n",
      "Iteration 4477: Policy loss: 0.004939. Value loss: 0.021920. Entropy: 0.641598.\n",
      "Iteration 4478: Policy loss: -0.008468. Value loss: 0.017043. Entropy: 0.651290.\n",
      "Iteration 4479: Policy loss: -0.019244. Value loss: 0.014364. Entropy: 0.653866.\n",
      "episode: 3264   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 685     evaluation reward: 9.88\n",
      "Training network. lr: 0.000216. clip: 0.086563\n",
      "Iteration 4480: Policy loss: 0.007123. Value loss: 0.043273. Entropy: 0.641437.\n",
      "Iteration 4481: Policy loss: -0.005969. Value loss: 0.032523. Entropy: 0.645997.\n",
      "Iteration 4482: Policy loss: -0.013541. Value loss: 0.029639. Entropy: 0.637147.\n",
      "episode: 3265   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 675     evaluation reward: 9.75\n",
      "episode: 3266   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 772     evaluation reward: 9.8\n",
      "Training network. lr: 0.000216. clip: 0.086554\n",
      "Iteration 4483: Policy loss: 0.007372. Value loss: 0.018579. Entropy: 0.651150.\n",
      "Iteration 4484: Policy loss: -0.006910. Value loss: 0.014298. Entropy: 0.639971.\n",
      "Iteration 4485: Policy loss: -0.012903. Value loss: 0.012539. Entropy: 0.645837.\n",
      "episode: 3267   score: 16.0   memory length: 1024   epsilon: 1.0    steps: 812     evaluation reward: 9.77\n",
      "Training network. lr: 0.000216. clip: 0.086545\n",
      "Iteration 4486: Policy loss: 0.008472. Value loss: 0.047922. Entropy: 0.562937.\n",
      "Iteration 4487: Policy loss: -0.004319. Value loss: 0.032685. Entropy: 0.551708.\n",
      "Iteration 4488: Policy loss: -0.011228. Value loss: 0.026957. Entropy: 0.554032.\n",
      "episode: 3268   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 839     evaluation reward: 9.8\n",
      "Training network. lr: 0.000216. clip: 0.086536\n",
      "Iteration 4489: Policy loss: 0.014357. Value loss: 0.053539. Entropy: 0.576395.\n",
      "Iteration 4490: Policy loss: 0.000923. Value loss: 0.032608. Entropy: 0.579439.\n",
      "Iteration 4491: Policy loss: -0.004611. Value loss: 0.025987. Entropy: 0.585814.\n",
      "episode: 3269   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 802     evaluation reward: 9.84\n",
      "episode: 3270   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 550     evaluation reward: 9.84\n",
      "Training network. lr: 0.000216. clip: 0.086527\n",
      "Iteration 4492: Policy loss: 0.002033. Value loss: 0.039464. Entropy: 0.603130.\n",
      "Iteration 4493: Policy loss: -0.013330. Value loss: 0.028273. Entropy: 0.609418.\n",
      "Iteration 4494: Policy loss: -0.016576. Value loss: 0.023716. Entropy: 0.619951.\n",
      "episode: 3271   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 650     evaluation reward: 9.83\n",
      "Training network. lr: 0.000216. clip: 0.086518\n",
      "Iteration 4495: Policy loss: 0.007258. Value loss: 0.026694. Entropy: 0.732560.\n",
      "Iteration 4496: Policy loss: -0.007333. Value loss: 0.020345. Entropy: 0.730020.\n",
      "Iteration 4497: Policy loss: -0.012165. Value loss: 0.018942. Entropy: 0.723433.\n",
      "episode: 3272   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 820     evaluation reward: 9.9\n",
      "episode: 3273   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 633     evaluation reward: 9.93\n",
      "Training network. lr: 0.000216. clip: 0.086509\n",
      "Iteration 4498: Policy loss: 0.006236. Value loss: 0.049191. Entropy: 0.581219.\n",
      "Iteration 4499: Policy loss: -0.002481. Value loss: 0.037198. Entropy: 0.576721.\n",
      "Iteration 4500: Policy loss: -0.004826. Value loss: 0.030685. Entropy: 0.569181.\n",
      "episode: 3274   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 758     evaluation reward: 9.97\n",
      "Training network. lr: 0.000216. clip: 0.086500\n",
      "Iteration 4501: Policy loss: 0.005149. Value loss: 0.035170. Entropy: 0.680174.\n",
      "Iteration 4502: Policy loss: -0.009693. Value loss: 0.026577. Entropy: 0.670428.\n",
      "Iteration 4503: Policy loss: -0.017305. Value loss: 0.022097. Entropy: 0.665532.\n",
      "episode: 3275   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 497     evaluation reward: 9.93\n",
      "episode: 3276   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 548     evaluation reward: 9.9\n",
      "Training network. lr: 0.000216. clip: 0.086491\n",
      "Iteration 4504: Policy loss: 0.015013. Value loss: 0.047969. Entropy: 0.613052.\n",
      "Iteration 4505: Policy loss: -0.008080. Value loss: 0.034288. Entropy: 0.624772.\n",
      "Iteration 4506: Policy loss: -0.013329. Value loss: 0.028219. Entropy: 0.625110.\n",
      "episode: 3277   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 620     evaluation reward: 9.87\n",
      "episode: 3278   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 653     evaluation reward: 9.88\n",
      "Training network. lr: 0.000216. clip: 0.086482\n",
      "Iteration 4507: Policy loss: 0.002761. Value loss: 0.023347. Entropy: 0.565987.\n",
      "Iteration 4508: Policy loss: -0.010644. Value loss: 0.020219. Entropy: 0.565648.\n",
      "Iteration 4509: Policy loss: -0.016127. Value loss: 0.018255. Entropy: 0.554083.\n",
      "episode: 3279   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 646     evaluation reward: 9.88\n",
      "Training network. lr: 0.000216. clip: 0.086473\n",
      "Iteration 4510: Policy loss: 0.008251. Value loss: 0.021204. Entropy: 0.664732.\n",
      "Iteration 4511: Policy loss: -0.005976. Value loss: 0.015240. Entropy: 0.662020.\n",
      "Iteration 4512: Policy loss: -0.016012. Value loss: 0.012799. Entropy: 0.674279.\n",
      "episode: 3280   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 763     evaluation reward: 9.92\n",
      "episode: 3281   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 562     evaluation reward: 9.93\n",
      "Training network. lr: 0.000216. clip: 0.086464\n",
      "Iteration 4513: Policy loss: 0.005869. Value loss: 0.017038. Entropy: 0.530512.\n",
      "Iteration 4514: Policy loss: -0.007310. Value loss: 0.011738. Entropy: 0.532992.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4515: Policy loss: -0.016019. Value loss: 0.009684. Entropy: 0.537097.\n",
      "episode: 3282   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 651     evaluation reward: 9.9\n",
      "Training network. lr: 0.000216. clip: 0.086455\n",
      "Iteration 4516: Policy loss: 0.005793. Value loss: 0.026744. Entropy: 0.680283.\n",
      "Iteration 4517: Policy loss: -0.010894. Value loss: 0.020484. Entropy: 0.683255.\n",
      "Iteration 4518: Policy loss: -0.018956. Value loss: 0.016943. Entropy: 0.680037.\n",
      "episode: 3283   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 611     evaluation reward: 9.87\n",
      "episode: 3284   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 710     evaluation reward: 9.85\n",
      "Training network. lr: 0.000216. clip: 0.086446\n",
      "Iteration 4519: Policy loss: 0.009382. Value loss: 0.023322. Entropy: 0.674959.\n",
      "Iteration 4520: Policy loss: -0.003827. Value loss: 0.014731. Entropy: 0.669255.\n",
      "Iteration 4521: Policy loss: -0.014236. Value loss: 0.013528. Entropy: 0.675674.\n",
      "episode: 3285   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 606     evaluation reward: 9.86\n",
      "episode: 3286   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 561     evaluation reward: 9.86\n",
      "Training network. lr: 0.000216. clip: 0.086437\n",
      "Iteration 4522: Policy loss: 0.006257. Value loss: 0.018687. Entropy: 0.651252.\n",
      "Iteration 4523: Policy loss: -0.003743. Value loss: 0.013191. Entropy: 0.640396.\n",
      "Iteration 4524: Policy loss: -0.011382. Value loss: 0.012144. Entropy: 0.640728.\n",
      "episode: 3287   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 402     evaluation reward: 9.82\n",
      "episode: 3288   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 589     evaluation reward: 9.84\n",
      "Training network. lr: 0.000216. clip: 0.086428\n",
      "Iteration 4525: Policy loss: 0.026712. Value loss: 0.059938. Entropy: 0.670523.\n",
      "Iteration 4526: Policy loss: 0.006372. Value loss: 0.036961. Entropy: 0.660781.\n",
      "Iteration 4527: Policy loss: -0.001557. Value loss: 0.031505. Entropy: 0.656695.\n",
      "episode: 3289   score: 17.0   memory length: 1024   epsilon: 1.0    steps: 893     evaluation reward: 9.9\n",
      "Training network. lr: 0.000216. clip: 0.086419\n",
      "Iteration 4528: Policy loss: 0.007045. Value loss: 0.039692. Entropy: 0.501378.\n",
      "Iteration 4529: Policy loss: -0.000179. Value loss: 0.028526. Entropy: 0.510541.\n",
      "Iteration 4530: Policy loss: -0.010881. Value loss: 0.025667. Entropy: 0.514509.\n",
      "episode: 3290   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 646     evaluation reward: 9.86\n",
      "Training network. lr: 0.000216. clip: 0.086410\n",
      "Iteration 4531: Policy loss: 0.012990. Value loss: 0.016200. Entropy: 0.648528.\n",
      "Iteration 4532: Policy loss: -0.000819. Value loss: 0.014493. Entropy: 0.655757.\n",
      "Iteration 4533: Policy loss: -0.017094. Value loss: 0.012261. Entropy: 0.656434.\n",
      "episode: 3291   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 911     evaluation reward: 9.89\n",
      "episode: 3292   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 395     evaluation reward: 9.84\n",
      "Training network. lr: 0.000216. clip: 0.086401\n",
      "Iteration 4534: Policy loss: 0.009203. Value loss: 0.029782. Entropy: 0.603941.\n",
      "Iteration 4535: Policy loss: -0.005446. Value loss: 0.019524. Entropy: 0.598507.\n",
      "Iteration 4536: Policy loss: -0.006144. Value loss: 0.016147. Entropy: 0.598509.\n",
      "episode: 3293   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 693     evaluation reward: 9.79\n",
      "episode: 3294   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 458     evaluation reward: 9.76\n",
      "Training network. lr: 0.000216. clip: 0.086392\n",
      "Iteration 4537: Policy loss: 0.010172. Value loss: 0.033576. Entropy: 0.652758.\n",
      "Iteration 4538: Policy loss: -0.008517. Value loss: 0.026742. Entropy: 0.650832.\n",
      "Iteration 4539: Policy loss: -0.018042. Value loss: 0.021807. Entropy: 0.637593.\n",
      "episode: 3295   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 457     evaluation reward: 9.68\n",
      "episode: 3296   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 438     evaluation reward: 9.56\n",
      "now time :  2018-12-26 14:36:31.021430\n",
      "Training network. lr: 0.000216. clip: 0.086383\n",
      "Iteration 4540: Policy loss: 0.010092. Value loss: 0.022713. Entropy: 0.724719.\n",
      "Iteration 4541: Policy loss: -0.003927. Value loss: 0.018330. Entropy: 0.719055.\n",
      "Iteration 4542: Policy loss: -0.012115. Value loss: 0.015155. Entropy: 0.721505.\n",
      "episode: 3297   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 403     evaluation reward: 9.48\n",
      "episode: 3298   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 714     evaluation reward: 9.46\n",
      "Training network. lr: 0.000216. clip: 0.086374\n",
      "Iteration 4543: Policy loss: 0.008780. Value loss: 0.023362. Entropy: 0.723344.\n",
      "Iteration 4544: Policy loss: -0.002452. Value loss: 0.018292. Entropy: 0.707276.\n",
      "Iteration 4545: Policy loss: -0.017853. Value loss: 0.014324. Entropy: 0.706595.\n",
      "episode: 3299   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 554     evaluation reward: 9.44\n",
      "episode: 3300   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 492     evaluation reward: 9.37\n",
      "Training network. lr: 0.000216. clip: 0.086365\n",
      "Iteration 4546: Policy loss: 0.008404. Value loss: 0.020809. Entropy: 0.691966.\n",
      "Iteration 4547: Policy loss: -0.003822. Value loss: 0.017531. Entropy: 0.693412.\n",
      "Iteration 4548: Policy loss: -0.015266. Value loss: 0.015538. Entropy: 0.692873.\n",
      "episode: 3301   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 556     evaluation reward: 9.3\n",
      "episode: 3302   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 619     evaluation reward: 9.28\n",
      "Training network. lr: 0.000216. clip: 0.086356\n",
      "Iteration 4549: Policy loss: 0.010808. Value loss: 0.040574. Entropy: 0.646416.\n",
      "Iteration 4550: Policy loss: -0.004077. Value loss: 0.032249. Entropy: 0.648257.\n",
      "Iteration 4551: Policy loss: -0.014650. Value loss: 0.027735. Entropy: 0.647431.\n",
      "episode: 3303   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 432     evaluation reward: 9.24\n",
      "episode: 3304   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 638     evaluation reward: 9.18\n",
      "Training network. lr: 0.000216. clip: 0.086347\n",
      "Iteration 4552: Policy loss: 0.006070. Value loss: 0.038990. Entropy: 0.643786.\n",
      "Iteration 4553: Policy loss: -0.003521. Value loss: 0.031582. Entropy: 0.650791.\n",
      "Iteration 4554: Policy loss: -0.016210. Value loss: 0.028493. Entropy: 0.650839.\n",
      "episode: 3305   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 425     evaluation reward: 9.11\n",
      "episode: 3306   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 642     evaluation reward: 9.09\n",
      "Training network. lr: 0.000216. clip: 0.086338\n",
      "Iteration 4555: Policy loss: 0.007347. Value loss: 0.029881. Entropy: 0.692829.\n",
      "Iteration 4556: Policy loss: -0.005124. Value loss: 0.019056. Entropy: 0.674358.\n",
      "Iteration 4557: Policy loss: -0.014392. Value loss: 0.016272. Entropy: 0.674321.\n",
      "episode: 3307   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 803     evaluation reward: 9.11\n",
      "Training network. lr: 0.000216. clip: 0.086329\n",
      "Iteration 4558: Policy loss: 0.004253. Value loss: 0.022043. Entropy: 0.675786.\n",
      "Iteration 4559: Policy loss: -0.008863. Value loss: 0.016368. Entropy: 0.664419.\n",
      "Iteration 4560: Policy loss: -0.019960. Value loss: 0.014108. Entropy: 0.662579.\n",
      "episode: 3308   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 428     evaluation reward: 9.07\n",
      "episode: 3309   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 718     evaluation reward: 9.07\n",
      "Training network. lr: 0.000216. clip: 0.086320\n",
      "Iteration 4561: Policy loss: 0.006468. Value loss: 0.024363. Entropy: 0.634782.\n",
      "Iteration 4562: Policy loss: -0.000880. Value loss: 0.016247. Entropy: 0.630678.\n",
      "Iteration 4563: Policy loss: -0.010961. Value loss: 0.013564. Entropy: 0.639278.\n",
      "episode: 3310   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 592     evaluation reward: 9.06\n",
      "Training network. lr: 0.000216. clip: 0.086311\n",
      "Iteration 4564: Policy loss: 0.005489. Value loss: 0.022643. Entropy: 0.677663.\n",
      "Iteration 4565: Policy loss: -0.007131. Value loss: 0.017098. Entropy: 0.673171.\n",
      "Iteration 4566: Policy loss: -0.015863. Value loss: 0.015571. Entropy: 0.674901.\n",
      "episode: 3311   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 597     evaluation reward: 9.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3312   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 645     evaluation reward: 9.03\n",
      "Training network. lr: 0.000216. clip: 0.086302\n",
      "Iteration 4567: Policy loss: 0.010615. Value loss: 0.022953. Entropy: 0.644818.\n",
      "Iteration 4568: Policy loss: 0.001384. Value loss: 0.016653. Entropy: 0.640916.\n",
      "Iteration 4569: Policy loss: -0.007724. Value loss: 0.014424. Entropy: 0.644011.\n",
      "episode: 3313   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 719     evaluation reward: 9.02\n",
      "episode: 3314   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 517     evaluation reward: 8.99\n",
      "Training network. lr: 0.000216. clip: 0.086293\n",
      "Iteration 4570: Policy loss: 0.004482. Value loss: 0.020291. Entropy: 0.600978.\n",
      "Iteration 4571: Policy loss: -0.010349. Value loss: 0.016654. Entropy: 0.595308.\n",
      "Iteration 4572: Policy loss: -0.014913. Value loss: 0.013985. Entropy: 0.586321.\n",
      "episode: 3315   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 543     evaluation reward: 8.99\n",
      "episode: 3316   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 621     evaluation reward: 8.96\n",
      "Training network. lr: 0.000216. clip: 0.086284\n",
      "Iteration 4573: Policy loss: 0.010537. Value loss: 0.017161. Entropy: 0.621548.\n",
      "Iteration 4574: Policy loss: -0.006428. Value loss: 0.013374. Entropy: 0.621049.\n",
      "Iteration 4575: Policy loss: -0.014931. Value loss: 0.011753. Entropy: 0.621100.\n",
      "episode: 3317   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 567     evaluation reward: 8.92\n",
      "Training network. lr: 0.000216. clip: 0.086275\n",
      "Iteration 4576: Policy loss: 0.010526. Value loss: 0.017460. Entropy: 0.734898.\n",
      "Iteration 4577: Policy loss: -0.006644. Value loss: 0.014944. Entropy: 0.731717.\n",
      "Iteration 4578: Policy loss: -0.011552. Value loss: 0.013026. Entropy: 0.719054.\n",
      "episode: 3318   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 8.88\n",
      "episode: 3319   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 610     evaluation reward: 8.87\n",
      "Training network. lr: 0.000216. clip: 0.086266\n",
      "Iteration 4579: Policy loss: 0.012376. Value loss: 0.028373. Entropy: 0.596982.\n",
      "Iteration 4580: Policy loss: -0.004877. Value loss: 0.021398. Entropy: 0.587286.\n",
      "Iteration 4581: Policy loss: -0.011814. Value loss: 0.018326. Entropy: 0.576562.\n",
      "episode: 3320   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 623     evaluation reward: 8.87\n",
      "episode: 3321   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 455     evaluation reward: 8.83\n",
      "Training network. lr: 0.000216. clip: 0.086257\n",
      "Iteration 4582: Policy loss: 0.005489. Value loss: 0.029530. Entropy: 0.622737.\n",
      "Iteration 4583: Policy loss: -0.008351. Value loss: 0.021456. Entropy: 0.627600.\n",
      "Iteration 4584: Policy loss: -0.018051. Value loss: 0.018885. Entropy: 0.621425.\n",
      "episode: 3322   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 765     evaluation reward: 8.81\n",
      "Training network. lr: 0.000216. clip: 0.086248\n",
      "Iteration 4585: Policy loss: 0.011227. Value loss: 0.020215. Entropy: 0.567302.\n",
      "Iteration 4586: Policy loss: 0.001867. Value loss: 0.014707. Entropy: 0.570063.\n",
      "Iteration 4587: Policy loss: -0.007522. Value loss: 0.012909. Entropy: 0.583317.\n",
      "episode: 3323   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 827     evaluation reward: 8.85\n",
      "episode: 3324   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 654     evaluation reward: 8.86\n",
      "Training network. lr: 0.000216. clip: 0.086239\n",
      "Iteration 4588: Policy loss: 0.007485. Value loss: 0.022397. Entropy: 0.603023.\n",
      "Iteration 4589: Policy loss: -0.005478. Value loss: 0.021571. Entropy: 0.590808.\n",
      "Iteration 4590: Policy loss: -0.018933. Value loss: 0.015842. Entropy: 0.586679.\n",
      "episode: 3325   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 637     evaluation reward: 8.84\n",
      "Training network. lr: 0.000216. clip: 0.086230\n",
      "Iteration 4591: Policy loss: 0.006053. Value loss: 0.020432. Entropy: 0.635390.\n",
      "Iteration 4592: Policy loss: -0.005389. Value loss: 0.017193. Entropy: 0.632528.\n",
      "Iteration 4593: Policy loss: -0.012952. Value loss: 0.015992. Entropy: 0.633161.\n",
      "episode: 3326   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 777     evaluation reward: 8.88\n",
      "episode: 3327   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 710     evaluation reward: 8.94\n",
      "Training network. lr: 0.000216. clip: 0.086221\n",
      "Iteration 4594: Policy loss: 0.005666. Value loss: 0.021504. Entropy: 0.532443.\n",
      "Iteration 4595: Policy loss: -0.008435. Value loss: 0.015966. Entropy: 0.526096.\n",
      "Iteration 4596: Policy loss: -0.015913. Value loss: 0.013379. Entropy: 0.533784.\n",
      "episode: 3328   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 635     evaluation reward: 8.9\n",
      "Training network. lr: 0.000216. clip: 0.086212\n",
      "Iteration 4597: Policy loss: 0.014092. Value loss: 0.025397. Entropy: 0.615556.\n",
      "Iteration 4598: Policy loss: -0.002211. Value loss: 0.018983. Entropy: 0.618860.\n",
      "Iteration 4599: Policy loss: -0.013405. Value loss: 0.016683. Entropy: 0.609505.\n",
      "episode: 3329   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 559     evaluation reward: 8.88\n",
      "episode: 3330   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 607     evaluation reward: 8.89\n",
      "Training network. lr: 0.000216. clip: 0.086203\n",
      "Iteration 4600: Policy loss: 0.007148. Value loss: 0.026408. Entropy: 0.576118.\n",
      "Iteration 4601: Policy loss: -0.003508. Value loss: 0.019689. Entropy: 0.568224.\n",
      "Iteration 4602: Policy loss: -0.014797. Value loss: 0.016612. Entropy: 0.575913.\n",
      "episode: 3331   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 680     evaluation reward: 8.89\n",
      "episode: 3332   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 568     evaluation reward: 8.89\n",
      "Training network. lr: 0.000215. clip: 0.086194\n",
      "Iteration 4603: Policy loss: 0.008609. Value loss: 0.021713. Entropy: 0.560108.\n",
      "Iteration 4604: Policy loss: -0.004961. Value loss: 0.016174. Entropy: 0.554429.\n",
      "Iteration 4605: Policy loss: -0.015962. Value loss: 0.014053. Entropy: 0.560869.\n",
      "episode: 3333   score: 18.0   memory length: 1024   epsilon: 1.0    steps: 978     evaluation reward: 8.97\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4606: Policy loss: 0.012150. Value loss: 0.047290. Entropy: 0.572495.\n",
      "Iteration 4607: Policy loss: -0.003375. Value loss: 0.034198. Entropy: 0.589296.\n",
      "Iteration 4608: Policy loss: -0.006223. Value loss: 0.029446. Entropy: 0.573816.\n",
      "episode: 3334   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 832     evaluation reward: 9.03\n",
      "Training network. lr: 0.000215. clip: 0.086176\n",
      "Iteration 4609: Policy loss: -0.001277. Value loss: 0.040644. Entropy: 0.654190.\n",
      "Iteration 4610: Policy loss: -0.011409. Value loss: 0.031603. Entropy: 0.661786.\n",
      "Iteration 4611: Policy loss: -0.020929. Value loss: 0.027835. Entropy: 0.655380.\n",
      "episode: 3335   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 614     evaluation reward: 9.06\n",
      "episode: 3336   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 561     evaluation reward: 9.04\n",
      "Training network. lr: 0.000215. clip: 0.086167\n",
      "Iteration 4612: Policy loss: 0.005490. Value loss: 0.043173. Entropy: 0.603161.\n",
      "Iteration 4613: Policy loss: -0.007872. Value loss: 0.027500. Entropy: 0.599522.\n",
      "Iteration 4614: Policy loss: -0.018364. Value loss: 0.022957. Entropy: 0.597308.\n",
      "episode: 3337   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 528     evaluation reward: 9.03\n",
      "episode: 3338   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 452     evaluation reward: 8.99\n",
      "Training network. lr: 0.000215. clip: 0.086158\n",
      "Iteration 4615: Policy loss: 0.002587. Value loss: 0.026635. Entropy: 0.566171.\n",
      "Iteration 4616: Policy loss: -0.010816. Value loss: 0.022581. Entropy: 0.567614.\n",
      "Iteration 4617: Policy loss: -0.016925. Value loss: 0.018379. Entropy: 0.575320.\n",
      "episode: 3339   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 691     evaluation reward: 9.03\n",
      "Training network. lr: 0.000215. clip: 0.086149\n",
      "Iteration 4618: Policy loss: 0.016043. Value loss: 0.026118. Entropy: 0.640545.\n",
      "Iteration 4619: Policy loss: -0.000123. Value loss: 0.019621. Entropy: 0.635924.\n",
      "Iteration 4620: Policy loss: -0.012568. Value loss: 0.017336. Entropy: 0.633319.\n",
      "episode: 3340   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 769     evaluation reward: 9.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3341   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 728     evaluation reward: 9.13\n",
      "Training network. lr: 0.000215. clip: 0.086140\n",
      "Iteration 4621: Policy loss: 0.015138. Value loss: 0.022409. Entropy: 0.504729.\n",
      "Iteration 4622: Policy loss: -0.008161. Value loss: 0.017444. Entropy: 0.516327.\n",
      "Iteration 4623: Policy loss: -0.014130. Value loss: 0.013761. Entropy: 0.514145.\n",
      "episode: 3342   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 638     evaluation reward: 9.12\n",
      "Training network. lr: 0.000215. clip: 0.086131\n",
      "Iteration 4624: Policy loss: 0.011875. Value loss: 0.030188. Entropy: 0.610531.\n",
      "Iteration 4625: Policy loss: -0.000394. Value loss: 0.022402. Entropy: 0.610859.\n",
      "Iteration 4626: Policy loss: -0.011330. Value loss: 0.020885. Entropy: 0.591667.\n",
      "episode: 3343   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 597     evaluation reward: 9.14\n",
      "episode: 3344   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 693     evaluation reward: 9.17\n",
      "Training network. lr: 0.000215. clip: 0.086122\n",
      "Iteration 4627: Policy loss: 0.004014. Value loss: 0.028949. Entropy: 0.602632.\n",
      "Iteration 4628: Policy loss: -0.008650. Value loss: 0.020098. Entropy: 0.602428.\n",
      "Iteration 4629: Policy loss: -0.013201. Value loss: 0.017611. Entropy: 0.593413.\n",
      "episode: 3345   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 571     evaluation reward: 9.15\n",
      "Training network. lr: 0.000215. clip: 0.086113\n",
      "Iteration 4630: Policy loss: 0.006351. Value loss: 0.034393. Entropy: 0.567579.\n",
      "Iteration 4631: Policy loss: -0.005884. Value loss: 0.026488. Entropy: 0.564180.\n",
      "Iteration 4632: Policy loss: -0.013845. Value loss: 0.022279. Entropy: 0.560337.\n",
      "episode: 3346   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 689     evaluation reward: 9.11\n",
      "episode: 3347   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 579     evaluation reward: 9.09\n",
      "Training network. lr: 0.000215. clip: 0.086104\n",
      "Iteration 4633: Policy loss: 0.002574. Value loss: 0.036988. Entropy: 0.569237.\n",
      "Iteration 4634: Policy loss: -0.008164. Value loss: 0.023976. Entropy: 0.576354.\n",
      "Iteration 4635: Policy loss: -0.016701. Value loss: 0.021669. Entropy: 0.565396.\n",
      "episode: 3348   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 679     evaluation reward: 9.11\n",
      "episode: 3349   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 685     evaluation reward: 9.07\n",
      "Training network. lr: 0.000215. clip: 0.086095\n",
      "Iteration 4636: Policy loss: 0.007197. Value loss: 0.021233. Entropy: 0.486888.\n",
      "Iteration 4637: Policy loss: -0.000216. Value loss: 0.016684. Entropy: 0.497812.\n",
      "Iteration 4638: Policy loss: -0.008193. Value loss: 0.012991. Entropy: 0.489328.\n",
      "episode: 3350   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 523     evaluation reward: 9.07\n",
      "episode: 3351   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 417     evaluation reward: 9.05\n",
      "Training network. lr: 0.000215. clip: 0.086086\n",
      "Iteration 4639: Policy loss: 0.005472. Value loss: 0.029132. Entropy: 0.584507.\n",
      "Iteration 4640: Policy loss: -0.007664. Value loss: 0.021440. Entropy: 0.578891.\n",
      "Iteration 4641: Policy loss: -0.015711. Value loss: 0.019920. Entropy: 0.575202.\n",
      "episode: 3352   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 502     evaluation reward: 9.04\n",
      "episode: 3353   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 555     evaluation reward: 9.04\n",
      "Training network. lr: 0.000215. clip: 0.086077\n",
      "Iteration 4642: Policy loss: 0.000868. Value loss: 0.023883. Entropy: 0.622104.\n",
      "Iteration 4643: Policy loss: 0.000426. Value loss: 0.017599. Entropy: 0.610768.\n",
      "Iteration 4644: Policy loss: -0.015724. Value loss: 0.015292. Entropy: 0.619066.\n",
      "episode: 3354   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 713     evaluation reward: 9.02\n",
      "Training network. lr: 0.000215. clip: 0.086068\n",
      "Iteration 4645: Policy loss: 0.008484. Value loss: 0.026502. Entropy: 0.635924.\n",
      "Iteration 4646: Policy loss: -0.009168. Value loss: 0.017085. Entropy: 0.626569.\n",
      "Iteration 4647: Policy loss: -0.020122. Value loss: 0.013551. Entropy: 0.624719.\n",
      "episode: 3355   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 528     evaluation reward: 8.97\n",
      "episode: 3356   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 556     evaluation reward: 8.96\n",
      "Training network. lr: 0.000215. clip: 0.086059\n",
      "Iteration 4648: Policy loss: 0.009026. Value loss: 0.024938. Entropy: 0.551670.\n",
      "Iteration 4649: Policy loss: -0.002465. Value loss: 0.021157. Entropy: 0.549871.\n",
      "Iteration 4650: Policy loss: -0.011792. Value loss: 0.018628. Entropy: 0.557798.\n",
      "episode: 3357   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 762     evaluation reward: 8.95\n",
      "episode: 3358   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 424     evaluation reward: 8.93\n",
      "Training network. lr: 0.000215. clip: 0.086050\n",
      "Iteration 4651: Policy loss: 0.007545. Value loss: 0.051472. Entropy: 0.538945.\n",
      "Iteration 4652: Policy loss: 0.001203. Value loss: 0.031933. Entropy: 0.536397.\n",
      "Iteration 4653: Policy loss: -0.014655. Value loss: 0.025134. Entropy: 0.542837.\n",
      "episode: 3359   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 629     evaluation reward: 8.93\n",
      "episode: 3360   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 429     evaluation reward: 8.91\n",
      "Training network. lr: 0.000215. clip: 0.086041\n",
      "Iteration 4654: Policy loss: 0.007230. Value loss: 0.023914. Entropy: 0.521378.\n",
      "Iteration 4655: Policy loss: -0.002814. Value loss: 0.018191. Entropy: 0.536997.\n",
      "Iteration 4656: Policy loss: -0.013398. Value loss: 0.015854. Entropy: 0.528284.\n",
      "episode: 3361   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 615     evaluation reward: 8.94\n",
      "Training network. lr: 0.000215. clip: 0.086032\n",
      "Iteration 4657: Policy loss: 0.001685. Value loss: 0.049110. Entropy: 0.482407.\n",
      "Iteration 4658: Policy loss: -0.003445. Value loss: 0.033383. Entropy: 0.481672.\n",
      "Iteration 4659: Policy loss: -0.013858. Value loss: 0.028078. Entropy: 0.474447.\n",
      "episode: 3362   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 840     evaluation reward: 9.01\n",
      "episode: 3363   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 425     evaluation reward: 8.98\n",
      "Training network. lr: 0.000215. clip: 0.086023\n",
      "Iteration 4660: Policy loss: 0.011875. Value loss: 0.028492. Entropy: 0.545682.\n",
      "Iteration 4661: Policy loss: -0.009154. Value loss: 0.019396. Entropy: 0.543390.\n",
      "Iteration 4662: Policy loss: -0.013823. Value loss: 0.015709. Entropy: 0.537407.\n",
      "episode: 3364   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 476     evaluation reward: 8.93\n",
      "episode: 3365   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 504     evaluation reward: 8.92\n",
      "Training network. lr: 0.000215. clip: 0.086014\n",
      "Iteration 4663: Policy loss: 0.008693. Value loss: 0.018803. Entropy: 0.581307.\n",
      "Iteration 4664: Policy loss: -0.007338. Value loss: 0.014780. Entropy: 0.573081.\n",
      "Iteration 4665: Policy loss: -0.012776. Value loss: 0.013777. Entropy: 0.563759.\n",
      "episode: 3366   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 568     evaluation reward: 8.88\n",
      "episode: 3367   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 603     evaluation reward: 8.81\n",
      "Training network. lr: 0.000215. clip: 0.086005\n",
      "Iteration 4666: Policy loss: 0.007821. Value loss: 0.020019. Entropy: 0.493599.\n",
      "Iteration 4667: Policy loss: -0.004408. Value loss: 0.014022. Entropy: 0.480969.\n",
      "Iteration 4668: Policy loss: -0.016454. Value loss: 0.011775. Entropy: 0.490403.\n",
      "episode: 3368   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 774     evaluation reward: 8.75\n",
      "Training network. lr: 0.000215. clip: 0.085996\n",
      "Iteration 4669: Policy loss: 0.009667. Value loss: 0.019491. Entropy: 0.507943.\n",
      "Iteration 4670: Policy loss: -0.002692. Value loss: 0.014411. Entropy: 0.497243.\n",
      "Iteration 4671: Policy loss: -0.011845. Value loss: 0.011712. Entropy: 0.506270.\n",
      "episode: 3369   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 579     evaluation reward: 8.71\n",
      "episode: 3370   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 476     evaluation reward: 8.7\n",
      "Training network. lr: 0.000215. clip: 0.085987\n",
      "Iteration 4672: Policy loss: 0.007185. Value loss: 0.025195. Entropy: 0.541675.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4673: Policy loss: -0.006938. Value loss: 0.018332. Entropy: 0.535646.\n",
      "Iteration 4674: Policy loss: -0.014390. Value loss: 0.015121. Entropy: 0.541192.\n",
      "episode: 3371   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 463     evaluation reward: 8.66\n",
      "episode: 3372   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 341     evaluation reward: 8.55\n",
      "episode: 3373   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 481     evaluation reward: 8.52\n",
      "Training network. lr: 0.000215. clip: 0.085978\n",
      "Iteration 4675: Policy loss: 0.002543. Value loss: 0.030181. Entropy: 0.535030.\n",
      "Iteration 4676: Policy loss: -0.005170. Value loss: 0.021477. Entropy: 0.541090.\n",
      "Iteration 4677: Policy loss: -0.018998. Value loss: 0.017214. Entropy: 0.556650.\n",
      "episode: 3374   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 548     evaluation reward: 8.47\n",
      "episode: 3375   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 611     evaluation reward: 8.49\n",
      "Training network. lr: 0.000215. clip: 0.085969\n",
      "Iteration 4678: Policy loss: 0.003621. Value loss: 0.016927. Entropy: 0.464593.\n",
      "Iteration 4679: Policy loss: -0.009793. Value loss: 0.013243. Entropy: 0.445320.\n",
      "Iteration 4680: Policy loss: -0.014500. Value loss: 0.010823. Entropy: 0.448358.\n",
      "episode: 3376   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 606     evaluation reward: 8.49\n",
      "Training network. lr: 0.000215. clip: 0.085960\n",
      "Iteration 4681: Policy loss: 0.005952. Value loss: 0.016158. Entropy: 0.546451.\n",
      "Iteration 4682: Policy loss: -0.009004. Value loss: 0.012483. Entropy: 0.535706.\n",
      "Iteration 4683: Policy loss: -0.017428. Value loss: 0.011492. Entropy: 0.535959.\n",
      "episode: 3377   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 521     evaluation reward: 8.48\n",
      "episode: 3378   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 670     evaluation reward: 8.48\n",
      "Training network. lr: 0.000215. clip: 0.085951\n",
      "Iteration 4684: Policy loss: 0.005115. Value loss: 0.021084. Entropy: 0.536736.\n",
      "Iteration 4685: Policy loss: -0.005648. Value loss: 0.015274. Entropy: 0.536547.\n",
      "Iteration 4686: Policy loss: -0.014874. Value loss: 0.013171. Entropy: 0.529408.\n",
      "episode: 3379   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 468     evaluation reward: 8.45\n",
      "now time :  2018-12-26 14:42:21.026838\n",
      "episode: 3380   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 538     evaluation reward: 8.42\n",
      "Training network. lr: 0.000215. clip: 0.085942\n",
      "Iteration 4687: Policy loss: 0.007249. Value loss: 0.017975. Entropy: 0.521887.\n",
      "Iteration 4688: Policy loss: 0.004606. Value loss: 0.013649. Entropy: 0.518344.\n",
      "Iteration 4689: Policy loss: -0.010282. Value loss: 0.012511. Entropy: 0.508989.\n",
      "episode: 3381   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 408     evaluation reward: 8.42\n",
      "episode: 3382   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 581     evaluation reward: 8.41\n",
      "Training network. lr: 0.000215. clip: 0.085933\n",
      "Iteration 4690: Policy loss: 0.009796. Value loss: 0.035945. Entropy: 0.429076.\n",
      "Iteration 4691: Policy loss: -0.003906. Value loss: 0.024070. Entropy: 0.435353.\n",
      "Iteration 4692: Policy loss: -0.003001. Value loss: 0.020601. Entropy: 0.431500.\n",
      "episode: 3383   score: 16.0   memory length: 1024   epsilon: 1.0    steps: 869     evaluation reward: 8.49\n",
      "Training network. lr: 0.000215. clip: 0.085924\n",
      "Iteration 4693: Policy loss: 0.016199. Value loss: 0.066886. Entropy: 0.522474.\n",
      "Iteration 4694: Policy loss: -0.001638. Value loss: 0.051727. Entropy: 0.533165.\n",
      "Iteration 4695: Policy loss: -0.009180. Value loss: 0.043898. Entropy: 0.526168.\n",
      "episode: 3384   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 534     evaluation reward: 8.47\n",
      "episode: 3385   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 533     evaluation reward: 8.46\n",
      "Training network. lr: 0.000215. clip: 0.085915\n",
      "Iteration 4696: Policy loss: 0.013245. Value loss: 0.029592. Entropy: 0.475358.\n",
      "Iteration 4697: Policy loss: -0.000753. Value loss: 0.021807. Entropy: 0.471558.\n",
      "Iteration 4698: Policy loss: -0.009145. Value loss: 0.017059. Entropy: 0.484751.\n",
      "episode: 3386   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 813     evaluation reward: 8.48\n",
      "Training network. lr: 0.000215. clip: 0.085906\n",
      "Iteration 4699: Policy loss: 0.011713. Value loss: 0.016169. Entropy: 0.391238.\n",
      "Iteration 4700: Policy loss: -0.000363. Value loss: 0.012031. Entropy: 0.393810.\n",
      "Iteration 4701: Policy loss: -0.006153. Value loss: 0.010505. Entropy: 0.405492.\n",
      "episode: 3387   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 746     evaluation reward: 8.53\n",
      "episode: 3388   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 783     evaluation reward: 8.51\n",
      "Training network. lr: 0.000215. clip: 0.085897\n",
      "Iteration 4702: Policy loss: 0.006378. Value loss: 0.021457. Entropy: 0.450157.\n",
      "Iteration 4703: Policy loss: -0.004611. Value loss: 0.016567. Entropy: 0.456738.\n",
      "Iteration 4704: Policy loss: -0.009761. Value loss: 0.014733. Entropy: 0.454068.\n",
      "episode: 3389   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 483     evaluation reward: 8.43\n",
      "episode: 3390   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 547     evaluation reward: 8.4\n",
      "Training network. lr: 0.000215. clip: 0.085888\n",
      "Iteration 4705: Policy loss: 0.006734. Value loss: 0.043390. Entropy: 0.450822.\n",
      "Iteration 4706: Policy loss: -0.005141. Value loss: 0.036306. Entropy: 0.446033.\n",
      "Iteration 4707: Policy loss: -0.009094. Value loss: 0.032645. Entropy: 0.442083.\n",
      "episode: 3391   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 679     evaluation reward: 8.37\n",
      "Training network. lr: 0.000215. clip: 0.085879\n",
      "Iteration 4708: Policy loss: 0.014037. Value loss: 0.017612. Entropy: 0.497794.\n",
      "Iteration 4709: Policy loss: -0.005084. Value loss: 0.014609. Entropy: 0.499542.\n",
      "Iteration 4710: Policy loss: -0.015017. Value loss: 0.012460. Entropy: 0.505030.\n",
      "episode: 3392   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 698     evaluation reward: 8.41\n",
      "Training network. lr: 0.000215. clip: 0.085870\n",
      "Iteration 4711: Policy loss: 0.009434. Value loss: 0.014555. Entropy: 0.300565.\n",
      "Iteration 4712: Policy loss: -0.001274. Value loss: 0.009501. Entropy: 0.305838.\n",
      "Iteration 4713: Policy loss: -0.002425. Value loss: 0.007430. Entropy: 0.282236.\n",
      "episode: 3393   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 966     evaluation reward: 8.4\n",
      "episode: 3394   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 533     evaluation reward: 8.41\n",
      "Training network. lr: 0.000215. clip: 0.085861\n",
      "Iteration 4714: Policy loss: 0.012111. Value loss: 0.026525. Entropy: 0.448887.\n",
      "Iteration 4715: Policy loss: 0.001046. Value loss: 0.016909. Entropy: 0.448390.\n",
      "Iteration 4716: Policy loss: -0.009042. Value loss: 0.014644. Entropy: 0.438690.\n",
      "episode: 3395   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 541     evaluation reward: 8.42\n",
      "episode: 3396   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 517     evaluation reward: 8.44\n",
      "Training network. lr: 0.000215. clip: 0.085852\n",
      "Iteration 4717: Policy loss: 0.004161. Value loss: 0.019885. Entropy: 0.431412.\n",
      "Iteration 4718: Policy loss: -0.009679. Value loss: 0.014905. Entropy: 0.433664.\n",
      "Iteration 4719: Policy loss: -0.012610. Value loss: 0.013553. Entropy: 0.432631.\n",
      "episode: 3397   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 537     evaluation reward: 8.47\n",
      "episode: 3398   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 483     evaluation reward: 8.43\n",
      "Training network. lr: 0.000215. clip: 0.085843\n",
      "Iteration 4720: Policy loss: 0.001096. Value loss: 0.020249. Entropy: 0.475193.\n",
      "Iteration 4721: Policy loss: -0.004423. Value loss: 0.012738. Entropy: 0.468951.\n",
      "Iteration 4722: Policy loss: -0.013475. Value loss: 0.011284. Entropy: 0.467272.\n",
      "episode: 3399   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 1179     evaluation reward: 8.45\n",
      "Training network. lr: 0.000215. clip: 0.085834\n",
      "Iteration 4723: Policy loss: 0.012861. Value loss: 0.018734. Entropy: 0.366313.\n",
      "Iteration 4724: Policy loss: 0.005332. Value loss: 0.011544. Entropy: 0.362379.\n",
      "Iteration 4725: Policy loss: -0.010654. Value loss: 0.010020. Entropy: 0.358432.\n",
      "episode: 3400   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 511     evaluation reward: 8.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3401   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 524     evaluation reward: 8.45\n",
      "Training network. lr: 0.000215. clip: 0.085825\n",
      "Iteration 4726: Policy loss: 0.002396. Value loss: 0.019165. Entropy: 0.370457.\n",
      "Iteration 4727: Policy loss: -0.010008. Value loss: 0.014523. Entropy: 0.372180.\n",
      "Iteration 4728: Policy loss: -0.011646. Value loss: 0.012815. Entropy: 0.369674.\n",
      "episode: 3402   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 495     evaluation reward: 8.4\n",
      "episode: 3403   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 489     evaluation reward: 8.41\n",
      "Training network. lr: 0.000215. clip: 0.085816\n",
      "Iteration 4729: Policy loss: 0.006453. Value loss: 0.053779. Entropy: 0.452587.\n",
      "Iteration 4730: Policy loss: -0.003888. Value loss: 0.038318. Entropy: 0.454285.\n",
      "Iteration 4731: Policy loss: -0.007939. Value loss: 0.031038. Entropy: 0.438150.\n",
      "episode: 3404   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 518     evaluation reward: 8.39\n",
      "episode: 3405   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 542     evaluation reward: 8.41\n",
      "Training network. lr: 0.000215. clip: 0.085807\n",
      "Iteration 4732: Policy loss: 0.003434. Value loss: 0.023054. Entropy: 0.453323.\n",
      "Iteration 4733: Policy loss: 0.000150. Value loss: 0.015990. Entropy: 0.460680.\n",
      "Iteration 4734: Policy loss: -0.011005. Value loss: 0.014491. Entropy: 0.469087.\n",
      "episode: 3406   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 695     evaluation reward: 8.4\n",
      "Training network. lr: 0.000214. clip: 0.085798\n",
      "Iteration 4735: Policy loss: 0.013981. Value loss: 0.028541. Entropy: 0.498189.\n",
      "Iteration 4736: Policy loss: -0.003468. Value loss: 0.019979. Entropy: 0.510663.\n",
      "Iteration 4737: Policy loss: -0.009173. Value loss: 0.016142. Entropy: 0.511697.\n",
      "episode: 3407   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 699     evaluation reward: 8.4\n",
      "episode: 3408   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 507     evaluation reward: 8.41\n",
      "Training network. lr: 0.000214. clip: 0.085789\n",
      "Iteration 4738: Policy loss: 0.009653. Value loss: 0.042710. Entropy: 0.369294.\n",
      "Iteration 4739: Policy loss: 0.003466. Value loss: 0.031078. Entropy: 0.383282.\n",
      "Iteration 4740: Policy loss: -0.008312. Value loss: 0.023339. Entropy: 0.379385.\n",
      "episode: 3409   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 501     evaluation reward: 8.37\n",
      "episode: 3410   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 786     evaluation reward: 8.39\n",
      "Training network. lr: 0.000214. clip: 0.085780\n",
      "Iteration 4741: Policy loss: -0.000107. Value loss: 0.029711. Entropy: 0.455352.\n",
      "Iteration 4742: Policy loss: -0.005854. Value loss: 0.020786. Entropy: 0.453025.\n",
      "Iteration 4743: Policy loss: -0.014420. Value loss: 0.017944. Entropy: 0.456624.\n",
      "episode: 3411   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 506     evaluation reward: 8.35\n",
      "Training network. lr: 0.000214. clip: 0.085771\n",
      "Iteration 4744: Policy loss: 0.004206. Value loss: 0.030751. Entropy: 0.380975.\n",
      "Iteration 4745: Policy loss: -0.004484. Value loss: 0.020199. Entropy: 0.394026.\n",
      "Iteration 4746: Policy loss: -0.014623. Value loss: 0.015547. Entropy: 0.402258.\n",
      "episode: 3412   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 599     evaluation reward: 8.33\n",
      "episode: 3413   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 500     evaluation reward: 8.3\n",
      "Training network. lr: 0.000214. clip: 0.085762\n",
      "Iteration 4747: Policy loss: 0.019409. Value loss: 0.018698. Entropy: 0.463110.\n",
      "Iteration 4748: Policy loss: -0.005620. Value loss: 0.013926. Entropy: 0.466053.\n",
      "Iteration 4749: Policy loss: -0.009409. Value loss: 0.011159. Entropy: 0.465022.\n",
      "episode: 3414   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 571     evaluation reward: 8.29\n",
      "episode: 3415   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 476     evaluation reward: 8.31\n",
      "Training network. lr: 0.000214. clip: 0.085753\n",
      "Iteration 4750: Policy loss: 0.001270. Value loss: 0.053173. Entropy: 0.435102.\n",
      "Iteration 4751: Policy loss: -0.003037. Value loss: 0.038884. Entropy: 0.429218.\n",
      "Iteration 4752: Policy loss: -0.006063. Value loss: 0.033065. Entropy: 0.412755.\n",
      "episode: 3416   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 813     evaluation reward: 8.3\n",
      "episode: 3417   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 669     evaluation reward: 8.31\n",
      "Training network. lr: 0.000214. clip: 0.085744\n",
      "Iteration 4753: Policy loss: 0.014935. Value loss: 0.020467. Entropy: 0.414437.\n",
      "Iteration 4754: Policy loss: -0.005568. Value loss: 0.014890. Entropy: 0.406856.\n",
      "Iteration 4755: Policy loss: -0.003140. Value loss: 0.011982. Entropy: 0.418696.\n",
      "episode: 3418   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 696     evaluation reward: 8.28\n",
      "Training network. lr: 0.000214. clip: 0.085735\n",
      "Iteration 4756: Policy loss: 0.005401. Value loss: 0.018975. Entropy: 0.429080.\n",
      "Iteration 4757: Policy loss: 0.002512. Value loss: 0.014065. Entropy: 0.438249.\n",
      "Iteration 4758: Policy loss: -0.001561. Value loss: 0.011231. Entropy: 0.413136.\n",
      "episode: 3419   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 945     evaluation reward: 8.27\n",
      "Training network. lr: 0.000214. clip: 0.085726\n",
      "Iteration 4759: Policy loss: 0.011189. Value loss: 0.016160. Entropy: 0.378890.\n",
      "Iteration 4760: Policy loss: -0.003257. Value loss: 0.008969. Entropy: 0.374318.\n",
      "Iteration 4761: Policy loss: -0.004615. Value loss: 0.008612. Entropy: 0.372558.\n",
      "episode: 3420   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 682     evaluation reward: 8.26\n",
      "episode: 3421   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 621     evaluation reward: 8.25\n",
      "Training network. lr: 0.000214. clip: 0.085717\n",
      "Iteration 4762: Policy loss: 0.006756. Value loss: 0.019379. Entropy: 0.565314.\n",
      "Iteration 4763: Policy loss: -0.002952. Value loss: 0.013564. Entropy: 0.555008.\n",
      "Iteration 4764: Policy loss: -0.012347. Value loss: 0.011561. Entropy: 0.564178.\n",
      "episode: 3422   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 705     evaluation reward: 8.19\n",
      "Training network. lr: 0.000214. clip: 0.085708\n",
      "Iteration 4765: Policy loss: 0.001152. Value loss: 0.022040. Entropy: 0.430246.\n",
      "Iteration 4766: Policy loss: -0.004435. Value loss: 0.018766. Entropy: 0.422557.\n",
      "Iteration 4767: Policy loss: -0.014189. Value loss: 0.015668. Entropy: 0.425586.\n",
      "episode: 3423   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 584     evaluation reward: 8.12\n",
      "episode: 3424   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 488     evaluation reward: 8.08\n",
      "Training network. lr: 0.000214. clip: 0.085699\n",
      "Iteration 4768: Policy loss: 0.006653. Value loss: 0.023769. Entropy: 0.383310.\n",
      "Iteration 4769: Policy loss: -0.004351. Value loss: 0.019031. Entropy: 0.371541.\n",
      "Iteration 4770: Policy loss: -0.012075. Value loss: 0.014607. Entropy: 0.364499.\n",
      "episode: 3425   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 536     evaluation reward: 8.02\n",
      "episode: 3426   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 561     evaluation reward: 7.96\n",
      "Training network. lr: 0.000214. clip: 0.085690\n",
      "Iteration 4771: Policy loss: 0.005714. Value loss: 0.016232. Entropy: 0.419480.\n",
      "Iteration 4772: Policy loss: -0.001118. Value loss: 0.013238. Entropy: 0.397501.\n",
      "Iteration 4773: Policy loss: -0.005430. Value loss: 0.012134. Entropy: 0.401193.\n",
      "episode: 3427   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 770     evaluation reward: 7.92\n",
      "Training network. lr: 0.000214. clip: 0.085681\n",
      "Iteration 4774: Policy loss: 0.005382. Value loss: 0.017585. Entropy: 0.351940.\n",
      "Iteration 4775: Policy loss: -0.006599. Value loss: 0.013104. Entropy: 0.327494.\n",
      "Iteration 4776: Policy loss: -0.014360. Value loss: 0.011326. Entropy: 0.336596.\n",
      "Training network. lr: 0.000214. clip: 0.085672\n",
      "Iteration 4777: Policy loss: 0.034515. Value loss: 0.001508. Entropy: 0.196693.\n",
      "Iteration 4778: Policy loss: 0.019285. Value loss: 0.001351. Entropy: 0.126950.\n",
      "Iteration 4779: Policy loss: 0.031002. Value loss: 0.000805. Entropy: 0.175843.\n",
      "episode: 3428   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 1870     evaluation reward: 7.89\n",
      "episode: 3429   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 551     evaluation reward: 7.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000214. clip: 0.085663\n",
      "Iteration 4780: Policy loss: 0.005248. Value loss: 0.015379. Entropy: 0.488742.\n",
      "Iteration 4781: Policy loss: -0.003174. Value loss: 0.011461. Entropy: 0.497208.\n",
      "Iteration 4782: Policy loss: -0.008847. Value loss: 0.010649. Entropy: 0.498926.\n",
      "episode: 3430   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 294     evaluation reward: 7.78\n",
      "episode: 3431   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 830     evaluation reward: 7.73\n",
      "Training network. lr: 0.000214. clip: 0.085654\n",
      "Iteration 4783: Policy loss: 0.008021. Value loss: 0.011453. Entropy: 0.299852.\n",
      "Iteration 4784: Policy loss: 0.000723. Value loss: 0.009036. Entropy: 0.280825.\n",
      "Iteration 4785: Policy loss: -0.006181. Value loss: 0.006689. Entropy: 0.271431.\n",
      "episode: 3432   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 372     evaluation reward: 7.68\n",
      "episode: 3433   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 738     evaluation reward: 7.53\n",
      "Training network. lr: 0.000214. clip: 0.085645\n",
      "Iteration 4786: Policy loss: 0.010280. Value loss: 0.010877. Entropy: 0.389197.\n",
      "Iteration 4787: Policy loss: -0.003462. Value loss: 0.008270. Entropy: 0.389536.\n",
      "Iteration 4788: Policy loss: -0.003481. Value loss: 0.007303. Entropy: 0.415392.\n",
      "episode: 3434   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 618     evaluation reward: 7.44\n",
      "Training network. lr: 0.000214. clip: 0.085636\n",
      "Iteration 4789: Policy loss: 0.012901. Value loss: 0.014292. Entropy: 0.343458.\n",
      "Iteration 4790: Policy loss: 0.007090. Value loss: 0.010257. Entropy: 0.346832.\n",
      "Iteration 4791: Policy loss: -0.001713. Value loss: 0.008214. Entropy: 0.353833.\n",
      "episode: 3435   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 790     evaluation reward: 7.41\n",
      "Training network. lr: 0.000214. clip: 0.085627\n",
      "Iteration 4792: Policy loss: 0.011771. Value loss: 0.008359. Entropy: 0.252036.\n",
      "Iteration 4793: Policy loss: 0.011068. Value loss: 0.005622. Entropy: 0.243116.\n",
      "Iteration 4794: Policy loss: -0.002041. Value loss: 0.004443. Entropy: 0.257867.\n",
      "Training network. lr: 0.000214. clip: 0.085618\n",
      "Iteration 4795: Policy loss: 0.008001. Value loss: 0.005762. Entropy: 0.135120.\n",
      "Iteration 4796: Policy loss: 0.009818. Value loss: 0.003109. Entropy: 0.130943.\n",
      "Iteration 4797: Policy loss: 0.004516. Value loss: 0.002235. Entropy: 0.137295.\n",
      "episode: 3436   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 1860     evaluation reward: 7.39\n",
      "Training network. lr: 0.000214. clip: 0.085609\n",
      "Iteration 4798: Policy loss: 0.005270. Value loss: 0.006471. Entropy: 0.415005.\n",
      "Iteration 4799: Policy loss: 0.000599. Value loss: 0.004673. Entropy: 0.408002.\n",
      "Iteration 4800: Policy loss: -0.004562. Value loss: 0.004471. Entropy: 0.415712.\n",
      "episode: 3437   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 1316     evaluation reward: 7.37\n",
      "Training network. lr: 0.000214. clip: 0.085600\n",
      "Iteration 4801: Policy loss: 0.008228. Value loss: 0.009302. Entropy: 0.288992.\n",
      "Iteration 4802: Policy loss: 0.001241. Value loss: 0.007468. Entropy: 0.303841.\n",
      "Iteration 4803: Policy loss: -0.015771. Value loss: 0.006318. Entropy: 0.326718.\n",
      "episode: 3438   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 801     evaluation reward: 7.34\n",
      "Training network. lr: 0.000214. clip: 0.085591\n",
      "Iteration 4804: Policy loss: 0.007409. Value loss: 0.013591. Entropy: 0.331120.\n",
      "Iteration 4805: Policy loss: 0.001626. Value loss: 0.008876. Entropy: 0.346090.\n",
      "Iteration 4806: Policy loss: -0.006850. Value loss: 0.006157. Entropy: 0.339347.\n",
      "episode: 3439   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 823     evaluation reward: 7.27\n",
      "episode: 3440   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 536     evaluation reward: 7.21\n",
      "Training network. lr: 0.000214. clip: 0.085582\n",
      "Iteration 4807: Policy loss: 0.026946. Value loss: 0.011010. Entropy: 0.461968.\n",
      "Iteration 4808: Policy loss: -0.002735. Value loss: 0.009324. Entropy: 0.459682.\n",
      "Iteration 4809: Policy loss: -0.012754. Value loss: 0.007452. Entropy: 0.461581.\n",
      "Training network. lr: 0.000214. clip: 0.085573\n",
      "Iteration 4810: Policy loss: 0.005202. Value loss: 0.008126. Entropy: 0.375457.\n",
      "Iteration 4811: Policy loss: -0.001608. Value loss: 0.006106. Entropy: 0.399886.\n",
      "Iteration 4812: Policy loss: -0.011431. Value loss: 0.005453. Entropy: 0.402526.\n",
      "episode: 3441   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 1645     evaluation reward: 7.17\n",
      "Training network. lr: 0.000214. clip: 0.085564\n",
      "Iteration 4813: Policy loss: 0.005235. Value loss: 0.008895. Entropy: 0.449428.\n",
      "Iteration 4814: Policy loss: -0.007035. Value loss: 0.006016. Entropy: 0.440120.\n",
      "Iteration 4815: Policy loss: -0.014897. Value loss: 0.005003. Entropy: 0.421250.\n",
      "episode: 3442   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 1023     evaluation reward: 7.13\n",
      "episode: 3443   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 579     evaluation reward: 7.08\n",
      "Training network. lr: 0.000214. clip: 0.085555\n",
      "Iteration 4816: Policy loss: 0.015258. Value loss: 0.008035. Entropy: 0.347814.\n",
      "Iteration 4817: Policy loss: 0.005414. Value loss: 0.006353. Entropy: 0.301086.\n",
      "Iteration 4818: Policy loss: -0.001174. Value loss: 0.005424. Entropy: 0.289463.\n",
      "episode: 3444   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 1292     evaluation reward: 7.05\n",
      "Training network. lr: 0.000214. clip: 0.085546\n",
      "Iteration 4819: Policy loss: 0.008186. Value loss: 0.013622. Entropy: 0.285879.\n",
      "Iteration 4820: Policy loss: 0.001270. Value loss: 0.010355. Entropy: 0.276431.\n",
      "Iteration 4821: Policy loss: -0.007748. Value loss: 0.009043. Entropy: 0.287211.\n",
      "episode: 3445   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 399     evaluation reward: 6.99\n",
      "Training network. lr: 0.000214. clip: 0.085537\n",
      "Iteration 4822: Policy loss: 0.010468. Value loss: 0.020726. Entropy: 0.365647.\n",
      "Iteration 4823: Policy loss: -0.002866. Value loss: 0.012626. Entropy: 0.378990.\n",
      "Iteration 4824: Policy loss: -0.005908. Value loss: 0.010170. Entropy: 0.369720.\n",
      "episode: 3446   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 722     evaluation reward: 6.95\n",
      "episode: 3447   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 664     evaluation reward: 6.94\n",
      "Training network. lr: 0.000214. clip: 0.085528\n",
      "Iteration 4825: Policy loss: 0.004975. Value loss: 0.016303. Entropy: 0.334272.\n",
      "Iteration 4826: Policy loss: -0.006425. Value loss: 0.011529. Entropy: 0.314870.\n",
      "Iteration 4827: Policy loss: -0.009502. Value loss: 0.010599. Entropy: 0.302652.\n",
      "episode: 3448   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 592     evaluation reward: 6.93\n",
      "episode: 3449   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 531     evaluation reward: 6.87\n",
      "Training network. lr: 0.000214. clip: 0.085519\n",
      "Iteration 4828: Policy loss: 0.009394. Value loss: 0.106425. Entropy: 0.362123.\n",
      "Iteration 4829: Policy loss: 0.000013. Value loss: 0.032324. Entropy: 0.353950.\n",
      "Iteration 4830: Policy loss: -0.006691. Value loss: 0.023887. Entropy: 0.350678.\n",
      "episode: 3450   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 470     evaluation reward: 6.85\n",
      "episode: 3451   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 475     evaluation reward: 6.85\n",
      "Training network. lr: 0.000214. clip: 0.085510\n",
      "Iteration 4831: Policy loss: 0.001456. Value loss: 0.017735. Entropy: 0.352774.\n",
      "Iteration 4832: Policy loss: 0.001679. Value loss: 0.012084. Entropy: 0.348801.\n",
      "Iteration 4833: Policy loss: -0.010998. Value loss: 0.010370. Entropy: 0.342312.\n",
      "now time :  2018-12-26 14:48:14.456219\n",
      "episode: 3452   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 617     evaluation reward: 6.84\n",
      "episode: 3453   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 412     evaluation reward: 6.8\n",
      "Training network. lr: 0.000214. clip: 0.085501\n",
      "Iteration 4834: Policy loss: 0.003000. Value loss: 0.019557. Entropy: 0.301201.\n",
      "Iteration 4835: Policy loss: 0.000398. Value loss: 0.012864. Entropy: 0.303423.\n",
      "Iteration 4836: Policy loss: -0.008890. Value loss: 0.011254. Entropy: 0.316115.\n",
      "episode: 3454   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 532     evaluation reward: 6.75\n",
      "episode: 3455   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 645     evaluation reward: 6.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000214. clip: 0.085492\n",
      "Iteration 4837: Policy loss: 0.010376. Value loss: 0.012235. Entropy: 0.318890.\n",
      "Iteration 4838: Policy loss: -0.002616. Value loss: 0.008774. Entropy: 0.342413.\n",
      "Iteration 4839: Policy loss: -0.007662. Value loss: 0.007848. Entropy: 0.355952.\n",
      "episode: 3456   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 587     evaluation reward: 6.75\n",
      "Training network. lr: 0.000214. clip: 0.085483\n",
      "Iteration 4840: Policy loss: 0.005458. Value loss: 0.012644. Entropy: 0.420800.\n",
      "Iteration 4841: Policy loss: -0.003918. Value loss: 0.008345. Entropy: 0.421443.\n",
      "Iteration 4842: Policy loss: -0.008358. Value loss: 0.007838. Entropy: 0.414732.\n",
      "episode: 3457   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 693     evaluation reward: 6.71\n",
      "episode: 3458   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 657     evaluation reward: 6.7\n",
      "Training network. lr: 0.000214. clip: 0.085474\n",
      "Iteration 4843: Policy loss: 0.007820. Value loss: 0.013794. Entropy: 0.298328.\n",
      "Iteration 4844: Policy loss: 0.003856. Value loss: 0.009408. Entropy: 0.286389.\n",
      "Iteration 4845: Policy loss: -0.009078. Value loss: 0.007985. Entropy: 0.273327.\n",
      "episode: 3459   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 705     evaluation reward: 6.69\n",
      "Training network. lr: 0.000214. clip: 0.085465\n",
      "Iteration 4846: Policy loss: 0.012309. Value loss: 0.016932. Entropy: 0.258895.\n",
      "Iteration 4847: Policy loss: 0.011167. Value loss: 0.010672. Entropy: 0.286761.\n",
      "Iteration 4848: Policy loss: -0.003267. Value loss: 0.008680. Entropy: 0.252242.\n",
      "episode: 3460   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 860     evaluation reward: 6.73\n",
      "episode: 3461   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 487     evaluation reward: 6.7\n",
      "Training network. lr: 0.000214. clip: 0.085456\n",
      "Iteration 4849: Policy loss: 0.007466. Value loss: 0.021983. Entropy: 0.319300.\n",
      "Iteration 4850: Policy loss: 0.000935. Value loss: 0.016183. Entropy: 0.314079.\n",
      "Iteration 4851: Policy loss: 0.002574. Value loss: 0.013968. Entropy: 0.322831.\n",
      "episode: 3462   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 425     evaluation reward: 6.59\n",
      "episode: 3463   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 551     evaluation reward: 6.61\n",
      "Training network. lr: 0.000214. clip: 0.085447\n",
      "Iteration 4852: Policy loss: 0.009453. Value loss: 0.018570. Entropy: 0.382295.\n",
      "Iteration 4853: Policy loss: -0.002492. Value loss: 0.013708. Entropy: 0.376509.\n",
      "Iteration 4854: Policy loss: -0.010181. Value loss: 0.012150. Entropy: 0.376959.\n",
      "episode: 3464   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 499     evaluation reward: 6.59\n",
      "episode: 3465   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 368     evaluation reward: 6.56\n",
      "Training network. lr: 0.000214. clip: 0.085438\n",
      "Iteration 4855: Policy loss: 0.011490. Value loss: 0.018140. Entropy: 0.329322.\n",
      "Iteration 4856: Policy loss: 0.000116. Value loss: 0.013765. Entropy: 0.320599.\n",
      "Iteration 4857: Policy loss: -0.006740. Value loss: 0.011624. Entropy: 0.322788.\n",
      "episode: 3466   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 547     evaluation reward: 6.58\n",
      "episode: 3467   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 535     evaluation reward: 6.54\n",
      "Training network. lr: 0.000214. clip: 0.085429\n",
      "Iteration 4858: Policy loss: 0.004474. Value loss: 0.038216. Entropy: 0.306474.\n",
      "Iteration 4859: Policy loss: -0.002454. Value loss: 0.027263. Entropy: 0.313139.\n",
      "Iteration 4860: Policy loss: -0.007427. Value loss: 0.024113. Entropy: 0.323821.\n",
      "episode: 3468   score: 14.0   memory length: 1024   epsilon: 1.0    steps: 494     evaluation reward: 6.59\n",
      "episode: 3469   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 751     evaluation reward: 6.6\n",
      "Training network. lr: 0.000214. clip: 0.085420\n",
      "Iteration 4861: Policy loss: 0.015686. Value loss: 0.064824. Entropy: 0.312380.\n",
      "Iteration 4862: Policy loss: 0.003458. Value loss: 0.050875. Entropy: 0.331334.\n",
      "Iteration 4863: Policy loss: -0.002347. Value loss: 0.043575. Entropy: 0.324217.\n",
      "episode: 3470   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 519     evaluation reward: 6.59\n",
      "episode: 3471   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 537     evaluation reward: 6.6\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4864: Policy loss: 0.008710. Value loss: 0.021027. Entropy: 0.224949.\n",
      "Iteration 4865: Policy loss: -0.001753. Value loss: 0.014445. Entropy: 0.244734.\n",
      "Iteration 4866: Policy loss: -0.000112. Value loss: 0.011899. Entropy: 0.246089.\n",
      "Training network. lr: 0.000214. clip: 0.085402\n",
      "Iteration 4867: Policy loss: 0.160340. Value loss: 0.000882. Entropy: 0.036358.\n",
      "Iteration 4868: Policy loss: 0.134465. Value loss: 0.000755. Entropy: 0.049955.\n",
      "Iteration 4869: Policy loss: 0.020249. Value loss: 0.000731. Entropy: 0.051320.\n",
      "episode: 3472   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 1983     evaluation reward: 6.6\n",
      "Training network. lr: 0.000213. clip: 0.085393\n",
      "Iteration 4870: Policy loss: 0.005256. Value loss: 0.016012. Entropy: 0.216329.\n",
      "Iteration 4871: Policy loss: 0.000740. Value loss: 0.011552. Entropy: 0.211234.\n",
      "Iteration 4872: Policy loss: -0.011774. Value loss: 0.009620. Entropy: 0.223303.\n",
      "episode: 3473   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 366     evaluation reward: 6.56\n",
      "episode: 3474   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 736     evaluation reward: 6.58\n",
      "Training network. lr: 0.000213. clip: 0.085384\n",
      "Iteration 4873: Policy loss: 0.020103. Value loss: 0.025110. Entropy: 0.412036.\n",
      "Iteration 4874: Policy loss: 0.006435. Value loss: 0.015831. Entropy: 0.395356.\n",
      "Iteration 4875: Policy loss: -0.004089. Value loss: 0.012553. Entropy: 0.386673.\n",
      "episode: 3475   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 521     evaluation reward: 6.57\n",
      "episode: 3476   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 505     evaluation reward: 6.55\n",
      "Training network. lr: 0.000213. clip: 0.085375\n",
      "Iteration 4876: Policy loss: 0.008720. Value loss: 0.019357. Entropy: 0.353650.\n",
      "Iteration 4877: Policy loss: 0.000121. Value loss: 0.013904. Entropy: 0.368851.\n",
      "Iteration 4878: Policy loss: -0.005822. Value loss: 0.011524. Entropy: 0.375727.\n",
      "episode: 3477   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 577     evaluation reward: 6.56\n",
      "Training network. lr: 0.000213. clip: 0.085366\n",
      "Iteration 4879: Policy loss: 0.019562. Value loss: 0.015937. Entropy: 0.385678.\n",
      "Iteration 4880: Policy loss: 0.001959. Value loss: 0.012592. Entropy: 0.379291.\n",
      "Iteration 4881: Policy loss: -0.006368. Value loss: 0.010635. Entropy: 0.384732.\n",
      "episode: 3478   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 594     evaluation reward: 6.54\n",
      "episode: 3479   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 427     evaluation reward: 6.52\n",
      "episode: 3480   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 407     evaluation reward: 6.49\n",
      "Training network. lr: 0.000213. clip: 0.085357\n",
      "Iteration 4882: Policy loss: 0.008115. Value loss: 0.015989. Entropy: 0.392195.\n",
      "Iteration 4883: Policy loss: -0.001351. Value loss: 0.013206. Entropy: 0.391874.\n",
      "Iteration 4884: Policy loss: -0.012451. Value loss: 0.011449. Entropy: 0.389430.\n",
      "episode: 3481   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 366     evaluation reward: 6.45\n",
      "episode: 3482   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 493     evaluation reward: 6.44\n",
      "Training network. lr: 0.000213. clip: 0.085348\n",
      "Iteration 4885: Policy loss: 0.014812. Value loss: 0.020517. Entropy: 0.376987.\n",
      "Iteration 4886: Policy loss: 0.002371. Value loss: 0.014256. Entropy: 0.387011.\n",
      "Iteration 4887: Policy loss: -0.004305. Value loss: 0.012092. Entropy: 0.376256.\n",
      "episode: 3483   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 415     evaluation reward: 6.32\n",
      "episode: 3484   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 448     evaluation reward: 6.3\n",
      "Training network. lr: 0.000213. clip: 0.085339\n",
      "Iteration 4888: Policy loss: 0.015883. Value loss: 0.011180. Entropy: 0.396689.\n",
      "Iteration 4889: Policy loss: 0.007006. Value loss: 0.009421. Entropy: 0.389324.\n",
      "Iteration 4890: Policy loss: -0.004205. Value loss: 0.007586. Entropy: 0.398673.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3485   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 505     evaluation reward: 6.29\n",
      "episode: 3486   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 436     evaluation reward: 6.23\n",
      "episode: 3487   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 501     evaluation reward: 6.18\n",
      "Training network. lr: 0.000213. clip: 0.085330\n",
      "Iteration 4891: Policy loss: 0.007240. Value loss: 0.015608. Entropy: 0.280500.\n",
      "Iteration 4892: Policy loss: 0.001150. Value loss: 0.012253. Entropy: 0.279809.\n",
      "Iteration 4893: Policy loss: -0.006992. Value loss: 0.010400. Entropy: 0.303771.\n",
      "episode: 3488   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 499     evaluation reward: 6.15\n",
      "episode: 3489   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 560     evaluation reward: 6.12\n",
      "Training network. lr: 0.000213. clip: 0.085321\n",
      "Iteration 4894: Policy loss: 0.010711. Value loss: 0.012596. Entropy: 0.412544.\n",
      "Iteration 4895: Policy loss: 0.002537. Value loss: 0.010248. Entropy: 0.418789.\n",
      "Iteration 4896: Policy loss: -0.005100. Value loss: 0.008907. Entropy: 0.415699.\n",
      "episode: 3490   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 465     evaluation reward: 6.1\n",
      "episode: 3491   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 419     evaluation reward: 6.05\n",
      "Training network. lr: 0.000213. clip: 0.085312\n",
      "Iteration 4897: Policy loss: 0.013144. Value loss: 0.010518. Entropy: 0.394631.\n",
      "Iteration 4898: Policy loss: -0.001569. Value loss: 0.008840. Entropy: 0.389375.\n",
      "Iteration 4899: Policy loss: -0.002405. Value loss: 0.007689. Entropy: 0.391703.\n",
      "episode: 3492   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 499     evaluation reward: 6.02\n",
      "episode: 3493   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 637     evaluation reward: 6.04\n",
      "Training network. lr: 0.000213. clip: 0.085303\n",
      "Iteration 4900: Policy loss: 0.006836. Value loss: 0.046941. Entropy: 0.404662.\n",
      "Iteration 4901: Policy loss: 0.003977. Value loss: 0.034380. Entropy: 0.405145.\n",
      "Iteration 4902: Policy loss: -0.007458. Value loss: 0.029831. Entropy: 0.385537.\n",
      "episode: 3494   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 401     evaluation reward: 6.02\n",
      "episode: 3495   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 577     evaluation reward: 6.03\n",
      "Training network. lr: 0.000213. clip: 0.085294\n",
      "Iteration 4903: Policy loss: 0.005735. Value loss: 0.018298. Entropy: 0.400479.\n",
      "Iteration 4904: Policy loss: 0.001831. Value loss: 0.012575. Entropy: 0.393576.\n",
      "Iteration 4905: Policy loss: -0.003768. Value loss: 0.009984. Entropy: 0.405196.\n",
      "episode: 3496   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 6.0\n",
      "episode: 3497   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 446     evaluation reward: 5.99\n",
      "Training network. lr: 0.000213. clip: 0.085285\n",
      "Iteration 4906: Policy loss: 0.014533. Value loss: 0.013359. Entropy: 0.467548.\n",
      "Iteration 4907: Policy loss: 0.000293. Value loss: 0.010104. Entropy: 0.449703.\n",
      "Iteration 4908: Policy loss: -0.006718. Value loss: 0.009104. Entropy: 0.457768.\n",
      "episode: 3498   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 508     evaluation reward: 5.98\n",
      "episode: 3499   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 646     evaluation reward: 5.94\n",
      "Training network. lr: 0.000213. clip: 0.085276\n",
      "Iteration 4909: Policy loss: 0.010408. Value loss: 0.011440. Entropy: 0.356200.\n",
      "Iteration 4910: Policy loss: -0.003999. Value loss: 0.009194. Entropy: 0.367552.\n",
      "Iteration 4911: Policy loss: -0.009312. Value loss: 0.007989. Entropy: 0.376411.\n",
      "episode: 3500   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 533     evaluation reward: 5.92\n",
      "episode: 3501   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 500     evaluation reward: 5.91\n",
      "Training network. lr: 0.000213. clip: 0.085267\n",
      "Iteration 4912: Policy loss: 0.012461. Value loss: 0.010332. Entropy: 0.428019.\n",
      "Iteration 4913: Policy loss: 0.001159. Value loss: 0.008672. Entropy: 0.448159.\n",
      "Iteration 4914: Policy loss: -0.010689. Value loss: 0.007924. Entropy: 0.432377.\n",
      "episode: 3502   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 665     evaluation reward: 5.91\n",
      "Training network. lr: 0.000213. clip: 0.085258\n",
      "Iteration 4915: Policy loss: 0.003681. Value loss: 0.007906. Entropy: 0.366935.\n",
      "Iteration 4916: Policy loss: 0.000157. Value loss: 0.006775. Entropy: 0.358477.\n",
      "Iteration 4917: Policy loss: -0.006652. Value loss: 0.005474. Entropy: 0.363609.\n",
      "episode: 3503   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 665     evaluation reward: 5.86\n",
      "episode: 3504   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 544     evaluation reward: 5.85\n",
      "Training network. lr: 0.000213. clip: 0.085249\n",
      "Iteration 4918: Policy loss: 0.010138. Value loss: 0.020536. Entropy: 0.537250.\n",
      "Iteration 4919: Policy loss: -0.006144. Value loss: 0.014562. Entropy: 0.542974.\n",
      "Iteration 4920: Policy loss: -0.011199. Value loss: 0.012440. Entropy: 0.537178.\n",
      "episode: 3505   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 371     evaluation reward: 5.82\n",
      "episode: 3506   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 721     evaluation reward: 5.84\n",
      "Training network. lr: 0.000213. clip: 0.085240\n",
      "Iteration 4921: Policy loss: 0.003801. Value loss: 0.017456. Entropy: 0.491621.\n",
      "Iteration 4922: Policy loss: -0.003495. Value loss: 0.013439. Entropy: 0.496248.\n",
      "Iteration 4923: Policy loss: -0.007549. Value loss: 0.011390. Entropy: 0.499235.\n",
      "episode: 3507   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 573     evaluation reward: 5.78\n",
      "episode: 3508   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 519     evaluation reward: 5.76\n",
      "Training network. lr: 0.000213. clip: 0.085231\n",
      "Iteration 4924: Policy loss: 0.010210. Value loss: 0.015711. Entropy: 0.440915.\n",
      "Iteration 4925: Policy loss: -0.002552. Value loss: 0.010474. Entropy: 0.453371.\n",
      "Iteration 4926: Policy loss: -0.007832. Value loss: 0.010868. Entropy: 0.462814.\n",
      "episode: 3509   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 403     evaluation reward: 5.74\n",
      "episode: 3510   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 365     evaluation reward: 5.68\n",
      "episode: 3511   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 303     evaluation reward: 5.67\n",
      "Training network. lr: 0.000213. clip: 0.085222\n",
      "Iteration 4927: Policy loss: 0.006506. Value loss: 0.012837. Entropy: 0.520620.\n",
      "Iteration 4928: Policy loss: -0.001987. Value loss: 0.010764. Entropy: 0.517402.\n",
      "Iteration 4929: Policy loss: -0.014277. Value loss: 0.009475. Entropy: 0.518449.\n",
      "episode: 3512   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 567     evaluation reward: 5.69\n",
      "episode: 3513   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 482     evaluation reward: 5.67\n",
      "Training network. lr: 0.000213. clip: 0.085213\n",
      "Iteration 4930: Policy loss: 0.008057. Value loss: 0.040140. Entropy: 0.571169.\n",
      "Iteration 4931: Policy loss: 0.002866. Value loss: 0.030320. Entropy: 0.557540.\n",
      "Iteration 4932: Policy loss: -0.010636. Value loss: 0.025490. Entropy: 0.547747.\n",
      "episode: 3514   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 463     evaluation reward: 5.66\n",
      "Training network. lr: 0.000213. clip: 0.085204\n",
      "Iteration 4933: Policy loss: 0.008925. Value loss: 0.012391. Entropy: 0.327380.\n",
      "Iteration 4934: Policy loss: -0.008399. Value loss: 0.007859. Entropy: 0.350861.\n",
      "Iteration 4935: Policy loss: -0.006048. Value loss: 0.006160. Entropy: 0.412478.\n",
      "episode: 3515   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 919     evaluation reward: 5.61\n",
      "episode: 3516   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 429     evaluation reward: 5.58\n",
      "Training network. lr: 0.000213. clip: 0.085195\n",
      "Iteration 4936: Policy loss: 0.010238. Value loss: 0.014984. Entropy: 0.425800.\n",
      "Iteration 4937: Policy loss: -0.004774. Value loss: 0.010899. Entropy: 0.436208.\n",
      "Iteration 4938: Policy loss: -0.001804. Value loss: 0.008730. Entropy: 0.448380.\n",
      "episode: 3517   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 431     evaluation reward: 5.55\n",
      "episode: 3518   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 541     evaluation reward: 5.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000213. clip: 0.085186\n",
      "Iteration 4939: Policy loss: 0.012677. Value loss: 0.011782. Entropy: 0.387551.\n",
      "Iteration 4940: Policy loss: -0.002382. Value loss: 0.009442. Entropy: 0.412488.\n",
      "Iteration 4941: Policy loss: -0.014180. Value loss: 0.008848. Entropy: 0.415172.\n",
      "episode: 3519   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 612     evaluation reward: 5.56\n",
      "episode: 3520   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 446     evaluation reward: 5.52\n",
      "Training network. lr: 0.000213. clip: 0.085177\n",
      "Iteration 4942: Policy loss: 0.000439. Value loss: 0.012645. Entropy: 0.457190.\n",
      "Iteration 4943: Policy loss: -0.010721. Value loss: 0.009398. Entropy: 0.439025.\n",
      "Iteration 4944: Policy loss: -0.017057. Value loss: 0.007947. Entropy: 0.427618.\n",
      "episode: 3521   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 365     evaluation reward: 5.51\n",
      "episode: 3522   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 527     evaluation reward: 5.52\n",
      "episode: 3523   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 462     evaluation reward: 5.51\n",
      "Training network. lr: 0.000213. clip: 0.085168\n",
      "Iteration 4945: Policy loss: 0.005989. Value loss: 0.018511. Entropy: 0.465832.\n",
      "Iteration 4946: Policy loss: -0.004410. Value loss: 0.014118. Entropy: 0.457885.\n",
      "Iteration 4947: Policy loss: -0.013303. Value loss: 0.011845. Entropy: 0.458561.\n",
      "episode: 3524   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 456     evaluation reward: 5.52\n",
      "Training network. lr: 0.000213. clip: 0.085159\n",
      "Iteration 4948: Policy loss: 0.007124. Value loss: 0.013585. Entropy: 0.450839.\n",
      "Iteration 4949: Policy loss: -0.003046. Value loss: 0.010003. Entropy: 0.439324.\n",
      "Iteration 4950: Policy loss: -0.005964. Value loss: 0.007591. Entropy: 0.430736.\n",
      "episode: 3525   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 608     evaluation reward: 5.56\n",
      "episode: 3526   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 398     evaluation reward: 5.54\n",
      "episode: 3527   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 5.52\n",
      "Training network. lr: 0.000213. clip: 0.085150\n",
      "Iteration 4951: Policy loss: 0.010784. Value loss: 0.020654. Entropy: 0.433295.\n",
      "Iteration 4952: Policy loss: -0.003734. Value loss: 0.015458. Entropy: 0.436212.\n",
      "Iteration 4953: Policy loss: -0.005769. Value loss: 0.012934. Entropy: 0.444576.\n",
      "episode: 3528   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 461     evaluation reward: 5.51\n",
      "episode: 3529   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 574     evaluation reward: 5.54\n",
      "Training network. lr: 0.000213. clip: 0.085141\n",
      "Iteration 4954: Policy loss: 0.002090. Value loss: 0.016666. Entropy: 0.460278.\n",
      "Iteration 4955: Policy loss: -0.005801. Value loss: 0.013700. Entropy: 0.473109.\n",
      "Iteration 4956: Policy loss: -0.013805. Value loss: 0.011544. Entropy: 0.450371.\n",
      "episode: 3530   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 362     evaluation reward: 5.55\n",
      "episode: 3531   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 636     evaluation reward: 5.57\n",
      "Training network. lr: 0.000213. clip: 0.085132\n",
      "Iteration 4957: Policy loss: 0.004688. Value loss: 0.014142. Entropy: 0.409392.\n",
      "Iteration 4958: Policy loss: -0.000756. Value loss: 0.010915. Entropy: 0.410778.\n",
      "Iteration 4959: Policy loss: -0.008263. Value loss: 0.009346. Entropy: 0.415922.\n",
      "episode: 3532   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 500     evaluation reward: 5.6\n",
      "episode: 3533   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 490     evaluation reward: 5.62\n",
      "Training network. lr: 0.000213. clip: 0.085123\n",
      "Iteration 4960: Policy loss: 0.011873. Value loss: 0.011205. Entropy: 0.310271.\n",
      "Iteration 4961: Policy loss: 0.000831. Value loss: 0.008778. Entropy: 0.296965.\n",
      "Iteration 4962: Policy loss: -0.005427. Value loss: 0.007784. Entropy: 0.298059.\n",
      "episode: 3534   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 434     evaluation reward: 5.61\n",
      "episode: 3535   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 417     evaluation reward: 5.59\n",
      "Training network. lr: 0.000213. clip: 0.085114\n",
      "Iteration 4963: Policy loss: 0.007456. Value loss: 0.013148. Entropy: 0.373308.\n",
      "Iteration 4964: Policy loss: -0.002305. Value loss: 0.010805. Entropy: 0.362349.\n",
      "Iteration 4965: Policy loss: -0.008900. Value loss: 0.009871. Entropy: 0.354295.\n",
      "episode: 3536   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 603     evaluation reward: 5.59\n",
      "episode: 3537   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 560     evaluation reward: 5.59\n",
      "Training network. lr: 0.000213. clip: 0.085105\n",
      "Iteration 4966: Policy loss: 0.004988. Value loss: 0.011420. Entropy: 0.385736.\n",
      "Iteration 4967: Policy loss: 0.001764. Value loss: 0.007523. Entropy: 0.369717.\n",
      "Iteration 4968: Policy loss: -0.008006. Value loss: 0.006484. Entropy: 0.372019.\n",
      "episode: 3538   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 602     evaluation reward: 5.63\n",
      "episode: 3539   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 280     evaluation reward: 5.62\n",
      "Training network. lr: 0.000213. clip: 0.085096\n",
      "Iteration 4969: Policy loss: -0.001199. Value loss: 0.018990. Entropy: 0.431254.\n",
      "Iteration 4970: Policy loss: -0.011702. Value loss: 0.014167. Entropy: 0.430719.\n",
      "Iteration 4971: Policy loss: -0.017837. Value loss: 0.011912. Entropy: 0.422842.\n",
      "episode: 3540   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 587     evaluation reward: 5.63\n",
      "episode: 3541   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 478     evaluation reward: 5.61\n",
      "Training network. lr: 0.000213. clip: 0.085087\n",
      "Iteration 4972: Policy loss: 0.008714. Value loss: 0.013441. Entropy: 0.362183.\n",
      "Iteration 4973: Policy loss: -0.001490. Value loss: 0.010785. Entropy: 0.358336.\n",
      "Iteration 4974: Policy loss: -0.004834. Value loss: 0.008997. Entropy: 0.362977.\n",
      "episode: 3542   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 598     evaluation reward: 5.64\n",
      "Training network. lr: 0.000213. clip: 0.085078\n",
      "Iteration 4975: Policy loss: 0.019083. Value loss: 0.016115. Entropy: 0.289449.\n",
      "Iteration 4976: Policy loss: 0.004632. Value loss: 0.012476. Entropy: 0.294452.\n",
      "Iteration 4977: Policy loss: -0.004480. Value loss: 0.010771. Entropy: 0.296565.\n",
      "episode: 3543   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 802     evaluation reward: 5.69\n",
      "episode: 3544   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 490     evaluation reward: 5.69\n",
      "episode: 3545   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 494     evaluation reward: 5.72\n",
      "Training network. lr: 0.000213. clip: 0.085069\n",
      "Iteration 4978: Policy loss: 0.011076. Value loss: 0.013801. Entropy: 0.300301.\n",
      "Iteration 4979: Policy loss: -0.000872. Value loss: 0.010351. Entropy: 0.305396.\n",
      "Iteration 4980: Policy loss: -0.000190. Value loss: 0.008645. Entropy: 0.306783.\n",
      "now time :  2018-12-26 14:54:02.869922\n",
      "episode: 3546   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 670     evaluation reward: 5.73\n",
      "Training network. lr: 0.000213. clip: 0.085060\n",
      "Iteration 4981: Policy loss: 0.012645. Value loss: 0.023154. Entropy: 0.378869.\n",
      "Iteration 4982: Policy loss: 0.000037. Value loss: 0.017553. Entropy: 0.389497.\n",
      "Iteration 4983: Policy loss: -0.011605. Value loss: 0.015619. Entropy: 0.392631.\n",
      "episode: 3547   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 396     evaluation reward: 5.71\n",
      "episode: 3548   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 776     evaluation reward: 5.71\n",
      "Training network. lr: 0.000213. clip: 0.085051\n",
      "Iteration 4984: Policy loss: 0.008245. Value loss: 0.018831. Entropy: 0.386672.\n",
      "Iteration 4985: Policy loss: -0.002002. Value loss: 0.011593. Entropy: 0.373749.\n",
      "Iteration 4986: Policy loss: -0.002296. Value loss: 0.008945. Entropy: 0.383239.\n",
      "episode: 3549   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 923     evaluation reward: 5.74\n",
      "Training network. lr: 0.000213. clip: 0.085042\n",
      "Iteration 4987: Policy loss: 0.010198. Value loss: 0.011784. Entropy: 0.372776.\n",
      "Iteration 4988: Policy loss: -0.003037. Value loss: 0.009882. Entropy: 0.379095.\n",
      "Iteration 4989: Policy loss: -0.013424. Value loss: 0.007526. Entropy: 0.376212.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3550   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 503     evaluation reward: 5.75\n",
      "episode: 3551   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 444     evaluation reward: 5.74\n",
      "Training network. lr: 0.000213. clip: 0.085033\n",
      "Iteration 4990: Policy loss: 0.006162. Value loss: 0.016240. Entropy: 0.294435.\n",
      "Iteration 4991: Policy loss: -0.003451. Value loss: 0.013122. Entropy: 0.298079.\n",
      "Iteration 4992: Policy loss: -0.006912. Value loss: 0.011304. Entropy: 0.278999.\n",
      "episode: 3552   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 625     evaluation reward: 5.75\n",
      "episode: 3553   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 581     evaluation reward: 5.78\n",
      "Training network. lr: 0.000213. clip: 0.085024\n",
      "Iteration 4993: Policy loss: 0.013600. Value loss: 0.017517. Entropy: 0.340597.\n",
      "Iteration 4994: Policy loss: -0.001559. Value loss: 0.013824. Entropy: 0.329174.\n",
      "Iteration 4995: Policy loss: -0.010718. Value loss: 0.010798. Entropy: 0.342901.\n",
      "episode: 3554   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 946     evaluation reward: 5.81\n",
      "Training network. lr: 0.000213. clip: 0.085015\n",
      "Iteration 4996: Policy loss: 0.008873. Value loss: 0.012919. Entropy: 0.303011.\n",
      "Iteration 4997: Policy loss: -0.002281. Value loss: 0.009730. Entropy: 0.291917.\n",
      "Iteration 4998: Policy loss: -0.009175. Value loss: 0.008177. Entropy: 0.294050.\n",
      "episode: 3555   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 493     evaluation reward: 5.79\n",
      "episode: 3556   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 493     evaluation reward: 5.75\n",
      "Training network. lr: 0.000213. clip: 0.085006\n",
      "Iteration 4999: Policy loss: 0.011420. Value loss: 0.012645. Entropy: 0.376668.\n",
      "Iteration 5000: Policy loss: 0.001912. Value loss: 0.008253. Entropy: 0.382636.\n",
      "Iteration 5001: Policy loss: -0.009427. Value loss: 0.007374. Entropy: 0.388077.\n",
      "episode: 3557   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 884     evaluation reward: 5.74\n",
      "Training network. lr: 0.000212. clip: 0.084997\n",
      "Iteration 5002: Policy loss: 0.007991. Value loss: 0.011553. Entropy: 0.316604.\n",
      "Iteration 5003: Policy loss: -0.001221. Value loss: 0.007928. Entropy: 0.339997.\n",
      "Iteration 5004: Policy loss: -0.010031. Value loss: 0.007191. Entropy: 0.338917.\n",
      "episode: 3558   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 760     evaluation reward: 5.78\n",
      "episode: 3559   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 497     evaluation reward: 5.78\n",
      "Training network. lr: 0.000212. clip: 0.084988\n",
      "Iteration 5005: Policy loss: 0.008382. Value loss: 0.010969. Entropy: 0.363025.\n",
      "Iteration 5006: Policy loss: -0.005609. Value loss: 0.009841. Entropy: 0.385485.\n",
      "Iteration 5007: Policy loss: -0.010429. Value loss: 0.007424. Entropy: 0.373195.\n",
      "episode: 3560   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 683     evaluation reward: 5.79\n",
      "Training network. lr: 0.000212. clip: 0.084979\n",
      "Iteration 5008: Policy loss: 0.012434. Value loss: 0.035455. Entropy: 0.318701.\n",
      "Iteration 5009: Policy loss: 0.020949. Value loss: 0.034611. Entropy: 0.372284.\n",
      "Iteration 5010: Policy loss: 0.002612. Value loss: 0.030150. Entropy: 0.340773.\n",
      "episode: 3561   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 624     evaluation reward: 5.8\n",
      "episode: 3562   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 432     evaluation reward: 5.81\n",
      "episode: 3563   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 399     evaluation reward: 5.78\n",
      "Training network. lr: 0.000212. clip: 0.084970\n",
      "Iteration 5011: Policy loss: 0.009248. Value loss: 0.022789. Entropy: 0.396225.\n",
      "Iteration 5012: Policy loss: -0.001684. Value loss: 0.017902. Entropy: 0.410776.\n",
      "Iteration 5013: Policy loss: -0.001506. Value loss: 0.016112. Entropy: 0.398258.\n",
      "episode: 3564   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 565     evaluation reward: 5.79\n",
      "episode: 3565   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 5.79\n",
      "Training network. lr: 0.000212. clip: 0.084961\n",
      "Iteration 5014: Policy loss: 0.038216. Value loss: 0.019518. Entropy: 0.378115.\n",
      "Iteration 5015: Policy loss: 0.010157. Value loss: 0.014242. Entropy: 0.380375.\n",
      "Iteration 5016: Policy loss: -0.003960. Value loss: 0.011869. Entropy: 0.358913.\n",
      "episode: 3566   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 668     evaluation reward: 5.74\n",
      "episode: 3567   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 419     evaluation reward: 5.73\n",
      "Training network. lr: 0.000212. clip: 0.084952\n",
      "Iteration 5017: Policy loss: 0.002156. Value loss: 0.011500. Entropy: 0.358071.\n",
      "Iteration 5018: Policy loss: -0.004596. Value loss: 0.008827. Entropy: 0.360801.\n",
      "Iteration 5019: Policy loss: -0.012924. Value loss: 0.007208. Entropy: 0.376415.\n",
      "episode: 3568   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 433     evaluation reward: 5.64\n",
      "episode: 3569   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 568     evaluation reward: 5.62\n",
      "Training network. lr: 0.000212. clip: 0.084943\n",
      "Iteration 5020: Policy loss: 0.007627. Value loss: 0.015210. Entropy: 0.387787.\n",
      "Iteration 5021: Policy loss: 0.004800. Value loss: 0.010998. Entropy: 0.391332.\n",
      "Iteration 5022: Policy loss: -0.006983. Value loss: 0.009917. Entropy: 0.382146.\n",
      "episode: 3570   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 551     evaluation reward: 5.62\n",
      "episode: 3571   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 531     evaluation reward: 5.6\n",
      "Training network. lr: 0.000212. clip: 0.084934\n",
      "Iteration 5023: Policy loss: 0.008560. Value loss: 0.011763. Entropy: 0.388197.\n",
      "Iteration 5024: Policy loss: 0.002728. Value loss: 0.008899. Entropy: 0.380259.\n",
      "Iteration 5025: Policy loss: -0.005727. Value loss: 0.008560. Entropy: 0.386467.\n",
      "episode: 3572   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 448     evaluation reward: 5.6\n",
      "episode: 3573   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 508     evaluation reward: 5.63\n",
      "Training network. lr: 0.000212. clip: 0.084925\n",
      "Iteration 5026: Policy loss: 0.008315. Value loss: 0.012164. Entropy: 0.412088.\n",
      "Iteration 5027: Policy loss: 0.000476. Value loss: 0.009899. Entropy: 0.421041.\n",
      "Iteration 5028: Policy loss: -0.008724. Value loss: 0.009084. Entropy: 0.419655.\n",
      "episode: 3574   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 604     evaluation reward: 5.59\n",
      "episode: 3575   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 431     evaluation reward: 5.56\n",
      "Training network. lr: 0.000212. clip: 0.084916\n",
      "Iteration 5029: Policy loss: 0.007702. Value loss: 0.016033. Entropy: 0.458520.\n",
      "Iteration 5030: Policy loss: -0.003419. Value loss: 0.011705. Entropy: 0.460788.\n",
      "Iteration 5031: Policy loss: -0.007287. Value loss: 0.010280. Entropy: 0.479485.\n",
      "episode: 3576   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 439     evaluation reward: 5.54\n",
      "episode: 3577   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 573     evaluation reward: 5.5\n",
      "Training network. lr: 0.000212. clip: 0.084907\n",
      "Iteration 5032: Policy loss: 0.009773. Value loss: 0.012696. Entropy: 0.419815.\n",
      "Iteration 5033: Policy loss: 0.000098. Value loss: 0.010048. Entropy: 0.410118.\n",
      "Iteration 5034: Policy loss: -0.008744. Value loss: 0.008832. Entropy: 0.416839.\n",
      "episode: 3578   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 450     evaluation reward: 5.48\n",
      "episode: 3579   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 245     evaluation reward: 5.46\n",
      "episode: 3580   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 349     evaluation reward: 5.44\n",
      "Training network. lr: 0.000212. clip: 0.084898\n",
      "Iteration 5035: Policy loss: 0.009695. Value loss: 0.020959. Entropy: 0.514952.\n",
      "Iteration 5036: Policy loss: 0.006027. Value loss: 0.014794. Entropy: 0.504776.\n",
      "Iteration 5037: Policy loss: -0.009054. Value loss: 0.012460. Entropy: 0.501565.\n",
      "episode: 3581   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 451     evaluation reward: 5.47\n",
      "episode: 3582   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 586     evaluation reward: 5.47\n",
      "Training network. lr: 0.000212. clip: 0.084889\n",
      "Iteration 5038: Policy loss: 0.011715. Value loss: 0.037126. Entropy: 0.451323.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5039: Policy loss: 0.002069. Value loss: 0.027443. Entropy: 0.463299.\n",
      "Iteration 5040: Policy loss: -0.006399. Value loss: 0.024057. Entropy: 0.497597.\n",
      "episode: 3583   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 780     evaluation reward: 5.51\n",
      "Training network. lr: 0.000212. clip: 0.084880\n",
      "Iteration 5041: Policy loss: 0.000867. Value loss: 0.011436. Entropy: 0.520217.\n",
      "Iteration 5042: Policy loss: -0.005059. Value loss: 0.008111. Entropy: 0.531909.\n",
      "Iteration 5043: Policy loss: -0.010819. Value loss: 0.007090. Entropy: 0.531075.\n",
      "episode: 3584   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 593     evaluation reward: 5.53\n",
      "episode: 3585   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 368     evaluation reward: 5.51\n",
      "Training network. lr: 0.000212. clip: 0.084871\n",
      "Iteration 5044: Policy loss: 0.009450. Value loss: 0.018904. Entropy: 0.504448.\n",
      "Iteration 5045: Policy loss: -0.004511. Value loss: 0.014702. Entropy: 0.485476.\n",
      "Iteration 5046: Policy loss: -0.008183. Value loss: 0.013485. Entropy: 0.479869.\n",
      "episode: 3586   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 652     evaluation reward: 5.54\n",
      "episode: 3587   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 542     evaluation reward: 5.57\n",
      "Training network. lr: 0.000212. clip: 0.084862\n",
      "Iteration 5047: Policy loss: 0.012227. Value loss: 0.038555. Entropy: 0.460037.\n",
      "Iteration 5048: Policy loss: -0.003798. Value loss: 0.032008. Entropy: 0.456880.\n",
      "Iteration 5049: Policy loss: -0.009227. Value loss: 0.029405. Entropy: 0.460539.\n",
      "episode: 3588   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 608     evaluation reward: 5.59\n",
      "Training network. lr: 0.000212. clip: 0.084853\n",
      "Iteration 5050: Policy loss: 0.013214. Value loss: 0.019024. Entropy: 0.555154.\n",
      "Iteration 5051: Policy loss: -0.006656. Value loss: 0.013849. Entropy: 0.537414.\n",
      "Iteration 5052: Policy loss: -0.012170. Value loss: 0.012006. Entropy: 0.541127.\n",
      "episode: 3589   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 652     evaluation reward: 5.6\n",
      "episode: 3590   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 752     evaluation reward: 5.67\n",
      "Training network. lr: 0.000212. clip: 0.084844\n",
      "Iteration 5053: Policy loss: 0.015026. Value loss: 0.047829. Entropy: 0.512070.\n",
      "Iteration 5054: Policy loss: -0.001209. Value loss: 0.030492. Entropy: 0.515939.\n",
      "Iteration 5055: Policy loss: -0.012675. Value loss: 0.026100. Entropy: 0.516409.\n",
      "episode: 3591   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 529     evaluation reward: 5.71\n",
      "episode: 3592   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 398     evaluation reward: 5.69\n",
      "Training network. lr: 0.000212. clip: 0.084835\n",
      "Iteration 5056: Policy loss: 0.005138. Value loss: 0.057369. Entropy: 0.506831.\n",
      "Iteration 5057: Policy loss: -0.004373. Value loss: 0.043626. Entropy: 0.496979.\n",
      "Iteration 5058: Policy loss: -0.006574. Value loss: 0.035617. Entropy: 0.493148.\n",
      "episode: 3593   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 439     evaluation reward: 5.63\n",
      "episode: 3594   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 498     evaluation reward: 5.63\n",
      "Training network. lr: 0.000212. clip: 0.084826\n",
      "Iteration 5059: Policy loss: 0.017836. Value loss: 0.022455. Entropy: 0.455996.\n",
      "Iteration 5060: Policy loss: 0.001984. Value loss: 0.016622. Entropy: 0.461668.\n",
      "Iteration 5061: Policy loss: -0.005907. Value loss: 0.015308. Entropy: 0.440655.\n",
      "episode: 3595   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 435     evaluation reward: 5.6\n",
      "episode: 3596   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 600     evaluation reward: 5.63\n",
      "Training network. lr: 0.000212. clip: 0.084817\n",
      "Iteration 5062: Policy loss: 0.012169. Value loss: 0.017924. Entropy: 0.477024.\n",
      "Iteration 5063: Policy loss: -0.001427. Value loss: 0.013301. Entropy: 0.486418.\n",
      "Iteration 5064: Policy loss: -0.010094. Value loss: 0.011116. Entropy: 0.476103.\n",
      "episode: 3597   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 621     evaluation reward: 5.64\n",
      "episode: 3598   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 520     evaluation reward: 5.66\n",
      "Training network. lr: 0.000212. clip: 0.084808\n",
      "Iteration 5065: Policy loss: 0.006852. Value loss: 0.021430. Entropy: 0.539338.\n",
      "Iteration 5066: Policy loss: -0.001860. Value loss: 0.015732. Entropy: 0.525625.\n",
      "Iteration 5067: Policy loss: -0.010134. Value loss: 0.013528. Entropy: 0.539654.\n",
      "episode: 3599   score: 15.0   memory length: 1024   epsilon: 1.0    steps: 669     evaluation reward: 5.75\n",
      "episode: 3600   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 532     evaluation reward: 5.76\n",
      "Training network. lr: 0.000212. clip: 0.084799\n",
      "Iteration 5068: Policy loss: 0.005147. Value loss: 0.060987. Entropy: 0.555392.\n",
      "Iteration 5069: Policy loss: -0.001499. Value loss: 0.033693. Entropy: 0.552043.\n",
      "Iteration 5070: Policy loss: -0.006935. Value loss: 0.029254. Entropy: 0.557601.\n",
      "episode: 3601   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 563     evaluation reward: 5.8\n",
      "episode: 3602   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 543     evaluation reward: 5.83\n",
      "Training network. lr: 0.000212. clip: 0.084790\n",
      "Iteration 5071: Policy loss: 0.007503. Value loss: 0.066200. Entropy: 0.453619.\n",
      "Iteration 5072: Policy loss: -0.002916. Value loss: 0.053051. Entropy: 0.475986.\n",
      "Iteration 5073: Policy loss: -0.005046. Value loss: 0.046986. Entropy: 0.469996.\n",
      "episode: 3603   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 409     evaluation reward: 5.82\n",
      "episode: 3604   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 432     evaluation reward: 5.8\n",
      "Training network. lr: 0.000212. clip: 0.084781\n",
      "Iteration 5074: Policy loss: 0.007358. Value loss: 0.023674. Entropy: 0.610436.\n",
      "Iteration 5075: Policy loss: -0.000406. Value loss: 0.015018. Entropy: 0.600267.\n",
      "Iteration 5076: Policy loss: -0.012526. Value loss: 0.013198. Entropy: 0.601582.\n",
      "episode: 3605   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 409     evaluation reward: 5.8\n",
      "episode: 3606   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 458     evaluation reward: 5.76\n",
      "Training network. lr: 0.000212. clip: 0.084772\n",
      "Iteration 5077: Policy loss: 0.009469. Value loss: 0.018912. Entropy: 0.565735.\n",
      "Iteration 5078: Policy loss: -0.004289. Value loss: 0.015101. Entropy: 0.557167.\n",
      "Iteration 5079: Policy loss: -0.007016. Value loss: 0.013252. Entropy: 0.554802.\n",
      "episode: 3607   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 471     evaluation reward: 5.76\n",
      "episode: 3608   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 574     evaluation reward: 5.77\n",
      "Training network. lr: 0.000212. clip: 0.084763\n",
      "Iteration 5080: Policy loss: 0.007525. Value loss: 0.017630. Entropy: 0.524550.\n",
      "Iteration 5081: Policy loss: -0.001289. Value loss: 0.013627. Entropy: 0.525091.\n",
      "Iteration 5082: Policy loss: -0.014022. Value loss: 0.011249. Entropy: 0.517457.\n",
      "episode: 3609   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 485     evaluation reward: 5.79\n",
      "episode: 3610   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 518     evaluation reward: 5.81\n",
      "Training network. lr: 0.000212. clip: 0.084754\n",
      "Iteration 5083: Policy loss: 0.008646. Value loss: 0.015531. Entropy: 0.469752.\n",
      "Iteration 5084: Policy loss: -0.000858. Value loss: 0.012783. Entropy: 0.468109.\n",
      "Iteration 5085: Policy loss: -0.008789. Value loss: 0.011170. Entropy: 0.471196.\n",
      "episode: 3611   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 422     evaluation reward: 5.82\n",
      "episode: 3612   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 5.79\n",
      "episode: 3613   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 496     evaluation reward: 5.78\n",
      "Training network. lr: 0.000212. clip: 0.084745\n",
      "Iteration 5086: Policy loss: 0.005443. Value loss: 0.015785. Entropy: 0.489147.\n",
      "Iteration 5087: Policy loss: -0.004428. Value loss: 0.013810. Entropy: 0.499117.\n",
      "Iteration 5088: Policy loss: -0.013521. Value loss: 0.013822. Entropy: 0.485438.\n",
      "episode: 3614   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 474     evaluation reward: 5.78\n",
      "Training network. lr: 0.000212. clip: 0.084736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5089: Policy loss: 0.016685. Value loss: 0.030988. Entropy: 0.460144.\n",
      "Iteration 5090: Policy loss: -0.007732. Value loss: 0.024754. Entropy: 0.478520.\n",
      "Iteration 5091: Policy loss: -0.009048. Value loss: 0.023397. Entropy: 0.478969.\n",
      "episode: 3615   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 634     evaluation reward: 5.8\n",
      "episode: 3616   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 558     evaluation reward: 5.82\n",
      "Training network. lr: 0.000212. clip: 0.084727\n",
      "Iteration 5092: Policy loss: 0.008061. Value loss: 0.013908. Entropy: 0.514627.\n",
      "Iteration 5093: Policy loss: -0.006430. Value loss: 0.011271. Entropy: 0.510566.\n",
      "Iteration 5094: Policy loss: -0.009557. Value loss: 0.009196. Entropy: 0.510587.\n",
      "episode: 3617   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 559     evaluation reward: 5.81\n",
      "episode: 3618   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 537     evaluation reward: 5.81\n",
      "Training network. lr: 0.000212. clip: 0.084718\n",
      "Iteration 5095: Policy loss: 0.003565. Value loss: 0.012243. Entropy: 0.511169.\n",
      "Iteration 5096: Policy loss: -0.005369. Value loss: 0.010087. Entropy: 0.491587.\n",
      "Iteration 5097: Policy loss: -0.012575. Value loss: 0.008314. Entropy: 0.500626.\n",
      "episode: 3619   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 569     evaluation reward: 5.82\n",
      "episode: 3620   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 542     evaluation reward: 5.85\n",
      "Training network. lr: 0.000212. clip: 0.084709\n",
      "Iteration 5098: Policy loss: 0.007986. Value loss: 0.012824. Entropy: 0.483767.\n",
      "Iteration 5099: Policy loss: -0.000489. Value loss: 0.009777. Entropy: 0.468072.\n",
      "Iteration 5100: Policy loss: -0.005642. Value loss: 0.008948. Entropy: 0.467629.\n",
      "episode: 3621   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 702     evaluation reward: 5.89\n",
      "episode: 3622   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 427     evaluation reward: 5.85\n",
      "Training network. lr: 0.000212. clip: 0.084700\n",
      "Iteration 5101: Policy loss: 0.013733. Value loss: 0.016151. Entropy: 0.482397.\n",
      "Iteration 5102: Policy loss: -0.003158. Value loss: 0.011719. Entropy: 0.464976.\n",
      "Iteration 5103: Policy loss: -0.009694. Value loss: 0.009945. Entropy: 0.466580.\n",
      "episode: 3623   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 483     evaluation reward: 5.85\n",
      "episode: 3624   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 339     evaluation reward: 5.81\n",
      "Training network. lr: 0.000212. clip: 0.084691\n",
      "Iteration 5104: Policy loss: 0.017211. Value loss: 0.031266. Entropy: 0.549552.\n",
      "Iteration 5105: Policy loss: -0.002918. Value loss: 0.023459. Entropy: 0.554178.\n",
      "Iteration 5106: Policy loss: -0.009793. Value loss: 0.021584. Entropy: 0.555550.\n",
      "episode: 3625   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 494     evaluation reward: 5.77\n",
      "episode: 3626   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 493     evaluation reward: 5.79\n",
      "Training network. lr: 0.000212. clip: 0.084682\n",
      "Iteration 5107: Policy loss: 0.014365. Value loss: 0.019023. Entropy: 0.510738.\n",
      "Iteration 5108: Policy loss: -0.002658. Value loss: 0.016005. Entropy: 0.508752.\n",
      "Iteration 5109: Policy loss: -0.008733. Value loss: 0.012932. Entropy: 0.515264.\n",
      "episode: 3627   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 412     evaluation reward: 5.78\n",
      "episode: 3628   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 502     evaluation reward: 5.8\n",
      "episode: 3629   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 404     evaluation reward: 5.77\n",
      "Training network. lr: 0.000212. clip: 0.084673\n",
      "Iteration 5110: Policy loss: 0.010164. Value loss: 0.019880. Entropy: 0.476687.\n",
      "Iteration 5111: Policy loss: -0.003049. Value loss: 0.017255. Entropy: 0.481803.\n",
      "Iteration 5112: Policy loss: -0.006128. Value loss: 0.015622. Entropy: 0.487090.\n",
      "episode: 3630   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 542     evaluation reward: 5.79\n",
      "Training network. lr: 0.000212. clip: 0.084664\n",
      "Iteration 5113: Policy loss: 0.010191. Value loss: 0.011946. Entropy: 0.623442.\n",
      "Iteration 5114: Policy loss: -0.000374. Value loss: 0.010753. Entropy: 0.611972.\n",
      "Iteration 5115: Policy loss: -0.006129. Value loss: 0.009060. Entropy: 0.626938.\n",
      "episode: 3631   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 797     evaluation reward: 5.76\n",
      "episode: 3632   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 565     evaluation reward: 5.76\n",
      "Training network. lr: 0.000212. clip: 0.084655\n",
      "Iteration 5116: Policy loss: 0.006322. Value loss: 0.012236. Entropy: 0.501277.\n",
      "Iteration 5117: Policy loss: -0.001437. Value loss: 0.009572. Entropy: 0.528378.\n",
      "Iteration 5118: Policy loss: -0.006493. Value loss: 0.009212. Entropy: 0.523535.\n",
      "episode: 3633   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 406     evaluation reward: 5.74\n",
      "episode: 3634   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 414     evaluation reward: 5.73\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5119: Policy loss: 0.014559. Value loss: 0.020718. Entropy: 0.583751.\n",
      "Iteration 5120: Policy loss: -0.004539. Value loss: 0.015417. Entropy: 0.597079.\n",
      "Iteration 5121: Policy loss: -0.016145. Value loss: 0.012815. Entropy: 0.578623.\n",
      "episode: 3635   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 607     evaluation reward: 5.74\n",
      "episode: 3636   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 574     evaluation reward: 5.75\n",
      "Training network. lr: 0.000212. clip: 0.084637\n",
      "Iteration 5122: Policy loss: 0.012057. Value loss: 0.020015. Entropy: 0.513470.\n",
      "Iteration 5123: Policy loss: -0.004360. Value loss: 0.016962. Entropy: 0.498348.\n",
      "Iteration 5124: Policy loss: -0.009591. Value loss: 0.014135. Entropy: 0.497082.\n",
      "episode: 3637   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 468     evaluation reward: 5.74\n",
      "episode: 3638   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 591     evaluation reward: 5.73\n",
      "now time :  2018-12-26 14:59:54.905074\n",
      "Training network. lr: 0.000212. clip: 0.084628\n",
      "Iteration 5125: Policy loss: 0.014355. Value loss: 0.014714. Entropy: 0.507183.\n",
      "Iteration 5126: Policy loss: 0.000543. Value loss: 0.011114. Entropy: 0.516087.\n",
      "Iteration 5127: Policy loss: -0.007552. Value loss: 0.009009. Entropy: 0.504131.\n",
      "episode: 3639   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 551     evaluation reward: 5.77\n",
      "episode: 3640   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 563     evaluation reward: 5.75\n",
      "Training network. lr: 0.000212. clip: 0.084619\n",
      "Iteration 5128: Policy loss: 0.016785. Value loss: 0.011197. Entropy: 0.498430.\n",
      "Iteration 5129: Policy loss: -0.000328. Value loss: 0.009324. Entropy: 0.486997.\n",
      "Iteration 5130: Policy loss: -0.010069. Value loss: 0.009583. Entropy: 0.488053.\n",
      "episode: 3641   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 441     evaluation reward: 5.76\n",
      "episode: 3642   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 529     evaluation reward: 5.73\n",
      "Training network. lr: 0.000212. clip: 0.084610\n",
      "Iteration 5131: Policy loss: 0.003819. Value loss: 0.013207. Entropy: 0.449334.\n",
      "Iteration 5132: Policy loss: -0.006382. Value loss: 0.011378. Entropy: 0.459857.\n",
      "Iteration 5133: Policy loss: -0.011812. Value loss: 0.009518. Entropy: 0.465009.\n",
      "episode: 3643   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 422     evaluation reward: 5.69\n",
      "episode: 3644   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 464     evaluation reward: 5.66\n",
      "Training network. lr: 0.000212. clip: 0.084601\n",
      "Iteration 5134: Policy loss: 0.015244. Value loss: 0.018576. Entropy: 0.535146.\n",
      "Iteration 5135: Policy loss: 0.002385. Value loss: 0.014934. Entropy: 0.538515.\n",
      "Iteration 5136: Policy loss: -0.006374. Value loss: 0.012573. Entropy: 0.534105.\n",
      "episode: 3645   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 491     evaluation reward: 5.67\n",
      "episode: 3646   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 505     evaluation reward: 5.66\n",
      "Training network. lr: 0.000211. clip: 0.084592\n",
      "Iteration 5137: Policy loss: 0.010130. Value loss: 0.012820. Entropy: 0.527362.\n",
      "Iteration 5138: Policy loss: -0.005186. Value loss: 0.010223. Entropy: 0.528386.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5139: Policy loss: -0.012647. Value loss: 0.009163. Entropy: 0.517636.\n",
      "episode: 3647   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 596     evaluation reward: 5.69\n",
      "episode: 3648   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 403     evaluation reward: 5.63\n",
      "Training network. lr: 0.000211. clip: 0.084583\n",
      "Iteration 5140: Policy loss: 0.018083. Value loss: 0.015093. Entropy: 0.532991.\n",
      "Iteration 5141: Policy loss: 0.015119. Value loss: 0.012573. Entropy: 0.540318.\n",
      "Iteration 5142: Policy loss: -0.005593. Value loss: 0.011233. Entropy: 0.538552.\n",
      "episode: 3649   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 499     evaluation reward: 5.62\n",
      "episode: 3650   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 507     evaluation reward: 5.62\n",
      "Training network. lr: 0.000211. clip: 0.084574\n",
      "Iteration 5143: Policy loss: 0.005172. Value loss: 0.022980. Entropy: 0.534905.\n",
      "Iteration 5144: Policy loss: -0.007320. Value loss: 0.017229. Entropy: 0.524934.\n",
      "Iteration 5145: Policy loss: -0.016324. Value loss: 0.015016. Entropy: 0.530154.\n",
      "episode: 3651   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 525     evaluation reward: 5.65\n",
      "episode: 3652   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 476     evaluation reward: 5.62\n",
      "episode: 3653   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 385     evaluation reward: 5.6\n",
      "Training network. lr: 0.000211. clip: 0.084565\n",
      "Iteration 5146: Policy loss: 0.010618. Value loss: 0.014896. Entropy: 0.529958.\n",
      "Iteration 5147: Policy loss: -0.006401. Value loss: 0.013010. Entropy: 0.532461.\n",
      "Iteration 5148: Policy loss: -0.005221. Value loss: 0.012235. Entropy: 0.519565.\n",
      "episode: 3654   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 477     evaluation reward: 5.59\n",
      "episode: 3655   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 480     evaluation reward: 5.58\n",
      "Training network. lr: 0.000211. clip: 0.084556\n",
      "Iteration 5149: Policy loss: 0.006877. Value loss: 0.013489. Entropy: 0.495013.\n",
      "Iteration 5150: Policy loss: -0.005454. Value loss: 0.011946. Entropy: 0.494123.\n",
      "Iteration 5151: Policy loss: -0.012944. Value loss: 0.010267. Entropy: 0.485540.\n",
      "episode: 3656   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 909     evaluation reward: 5.58\n",
      "Training network. lr: 0.000211. clip: 0.084547\n",
      "Iteration 5152: Policy loss: 0.003729. Value loss: 0.008471. Entropy: 0.337574.\n",
      "Iteration 5153: Policy loss: -0.004623. Value loss: 0.006427. Entropy: 0.348344.\n",
      "Iteration 5154: Policy loss: -0.008629. Value loss: 0.005673. Entropy: 0.353966.\n",
      "episode: 3657   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 423     evaluation reward: 5.58\n",
      "episode: 3658   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 454     evaluation reward: 5.55\n",
      "Training network. lr: 0.000211. clip: 0.084538\n",
      "Iteration 5155: Policy loss: 0.013529. Value loss: 0.015923. Entropy: 0.485506.\n",
      "Iteration 5156: Policy loss: -0.004568. Value loss: 0.013251. Entropy: 0.487849.\n",
      "Iteration 5157: Policy loss: -0.008323. Value loss: 0.010594. Entropy: 0.488394.\n",
      "episode: 3659   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 415     evaluation reward: 5.55\n",
      "episode: 3660   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 528     evaluation reward: 5.51\n",
      "Training network. lr: 0.000211. clip: 0.084529\n",
      "Iteration 5158: Policy loss: 0.012156. Value loss: 0.047742. Entropy: 0.530092.\n",
      "Iteration 5159: Policy loss: -0.004616. Value loss: 0.040370. Entropy: 0.524079.\n",
      "Iteration 5160: Policy loss: 0.002283. Value loss: 0.038179. Entropy: 0.515035.\n",
      "episode: 3661   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 548     evaluation reward: 5.53\n",
      "episode: 3662   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 435     evaluation reward: 5.53\n",
      "episode: 3663   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 469     evaluation reward: 5.54\n",
      "Training network. lr: 0.000211. clip: 0.084520\n",
      "Iteration 5161: Policy loss: 0.003990. Value loss: 0.031714. Entropy: 0.557586.\n",
      "Iteration 5162: Policy loss: -0.010175. Value loss: 0.021041. Entropy: 0.549837.\n",
      "Iteration 5163: Policy loss: -0.014147. Value loss: 0.025110. Entropy: 0.545791.\n",
      "episode: 3664   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 606     evaluation reward: 5.54\n",
      "Training network. lr: 0.000211. clip: 0.084511\n",
      "Iteration 5164: Policy loss: 0.011446. Value loss: 0.018434. Entropy: 0.470666.\n",
      "Iteration 5165: Policy loss: 0.002182. Value loss: 0.014884. Entropy: 0.474232.\n",
      "Iteration 5166: Policy loss: -0.011345. Value loss: 0.013113. Entropy: 0.470097.\n",
      "episode: 3665   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 463     evaluation reward: 5.55\n",
      "episode: 3666   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 411     evaluation reward: 5.56\n",
      "episode: 3667   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 544     evaluation reward: 5.59\n",
      "Training network. lr: 0.000211. clip: 0.084502\n",
      "Iteration 5167: Policy loss: 0.018617. Value loss: 0.016219. Entropy: 0.528123.\n",
      "Iteration 5168: Policy loss: -0.000831. Value loss: 0.014611. Entropy: 0.518161.\n",
      "Iteration 5169: Policy loss: -0.013443. Value loss: 0.012561. Entropy: 0.508019.\n",
      "episode: 3668   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 482     evaluation reward: 5.58\n",
      "episode: 3669   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 542     evaluation reward: 5.55\n",
      "Training network. lr: 0.000211. clip: 0.084493\n",
      "Iteration 5170: Policy loss: 0.009469. Value loss: 0.012677. Entropy: 0.466974.\n",
      "Iteration 5171: Policy loss: -0.004925. Value loss: 0.010285. Entropy: 0.449932.\n",
      "Iteration 5172: Policy loss: -0.011454. Value loss: 0.008335. Entropy: 0.455980.\n",
      "episode: 3670   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 475     evaluation reward: 5.54\n",
      "episode: 3671   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 486     evaluation reward: 5.55\n",
      "Training network. lr: 0.000211. clip: 0.084484\n",
      "Iteration 5173: Policy loss: 0.011917. Value loss: 0.015846. Entropy: 0.510943.\n",
      "Iteration 5174: Policy loss: -0.003758. Value loss: 0.012131. Entropy: 0.502239.\n",
      "Iteration 5175: Policy loss: -0.009557. Value loss: 0.011621. Entropy: 0.498468.\n",
      "episode: 3672   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 537     evaluation reward: 5.57\n",
      "episode: 3673   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 420     evaluation reward: 5.56\n",
      "Training network. lr: 0.000211. clip: 0.084475\n",
      "Iteration 5176: Policy loss: 0.015248. Value loss: 0.012407. Entropy: 0.496454.\n",
      "Iteration 5177: Policy loss: -0.000073. Value loss: 0.011721. Entropy: 0.501736.\n",
      "Iteration 5178: Policy loss: -0.004772. Value loss: 0.008710. Entropy: 0.506010.\n",
      "episode: 3674   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 461     evaluation reward: 5.55\n",
      "episode: 3675   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 426     evaluation reward: 5.56\n",
      "Training network. lr: 0.000211. clip: 0.084466\n",
      "Iteration 5179: Policy loss: 0.015396. Value loss: 0.014055. Entropy: 0.585558.\n",
      "Iteration 5180: Policy loss: -0.007638. Value loss: 0.011981. Entropy: 0.588469.\n",
      "Iteration 5181: Policy loss: -0.013556. Value loss: 0.010471. Entropy: 0.589968.\n",
      "episode: 3676   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 439     evaluation reward: 5.56\n",
      "episode: 3677   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 429     evaluation reward: 5.57\n",
      "episode: 3678   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 354     evaluation reward: 5.55\n",
      "Training network. lr: 0.000211. clip: 0.084457\n",
      "Iteration 5182: Policy loss: 0.006414. Value loss: 0.026173. Entropy: 0.520733.\n",
      "Iteration 5183: Policy loss: -0.003440. Value loss: 0.019249. Entropy: 0.515260.\n",
      "Iteration 5184: Policy loss: -0.012485. Value loss: 0.017117. Entropy: 0.510130.\n",
      "episode: 3679   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 403     evaluation reward: 5.57\n",
      "episode: 3680   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 396     evaluation reward: 5.58\n",
      "Training network. lr: 0.000211. clip: 0.084448\n",
      "Iteration 5185: Policy loss: 0.001729. Value loss: 0.018700. Entropy: 0.582748.\n",
      "Iteration 5186: Policy loss: -0.008339. Value loss: 0.015164. Entropy: 0.591014.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5187: Policy loss: -0.015295. Value loss: 0.012703. Entropy: 0.594334.\n",
      "episode: 3681   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 436     evaluation reward: 5.56\n",
      "episode: 3682   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 438     evaluation reward: 5.52\n",
      "Training network. lr: 0.000211. clip: 0.084439\n",
      "Iteration 5188: Policy loss: 0.027887. Value loss: 0.012865. Entropy: 0.587331.\n",
      "Iteration 5189: Policy loss: -0.005993. Value loss: 0.011193. Entropy: 0.586479.\n",
      "Iteration 5190: Policy loss: -0.006801. Value loss: 0.010676. Entropy: 0.590893.\n",
      "episode: 3683   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 503     evaluation reward: 5.5\n",
      "episode: 3684   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 370     evaluation reward: 5.47\n",
      "episode: 3685   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 358     evaluation reward: 5.47\n",
      "Training network. lr: 0.000211. clip: 0.084430\n",
      "Iteration 5191: Policy loss: 0.008186. Value loss: 0.024947. Entropy: 0.527963.\n",
      "Iteration 5192: Policy loss: -0.000818. Value loss: 0.020720. Entropy: 0.528535.\n",
      "Iteration 5193: Policy loss: -0.006922. Value loss: 0.018252. Entropy: 0.523004.\n",
      "episode: 3686   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 456     evaluation reward: 5.45\n",
      "episode: 3687   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 483     evaluation reward: 5.42\n",
      "Training network. lr: 0.000211. clip: 0.084421\n",
      "Iteration 5194: Policy loss: 0.007857. Value loss: 0.011501. Entropy: 0.578697.\n",
      "Iteration 5195: Policy loss: 0.006900. Value loss: 0.009489. Entropy: 0.570042.\n",
      "Iteration 5196: Policy loss: -0.005890. Value loss: 0.008591. Entropy: 0.567464.\n",
      "episode: 3688   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 634     evaluation reward: 5.4\n",
      "episode: 3689   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 356     evaluation reward: 5.35\n",
      "episode: 3690   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 337     evaluation reward: 5.26\n",
      "Training network. lr: 0.000211. clip: 0.084412\n",
      "Iteration 5197: Policy loss: -0.001160. Value loss: 0.024244. Entropy: 0.575846.\n",
      "Iteration 5198: Policy loss: -0.011722. Value loss: 0.015998. Entropy: 0.579472.\n",
      "Iteration 5199: Policy loss: -0.015228. Value loss: 0.013833. Entropy: 0.586399.\n",
      "episode: 3691   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 414     evaluation reward: 5.21\n",
      "episode: 3692   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 400     evaluation reward: 5.21\n",
      "Training network. lr: 0.000211. clip: 0.084403\n",
      "Iteration 5200: Policy loss: 0.014839. Value loss: 0.014536. Entropy: 0.580451.\n",
      "Iteration 5201: Policy loss: -0.000537. Value loss: 0.013070. Entropy: 0.564669.\n",
      "Iteration 5202: Policy loss: -0.002316. Value loss: 0.012388. Entropy: 0.568950.\n",
      "episode: 3693   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 407     evaluation reward: 5.2\n",
      "episode: 3694   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 454     evaluation reward: 5.2\n",
      "Training network. lr: 0.000211. clip: 0.084394\n",
      "Iteration 5203: Policy loss: 0.018697. Value loss: 0.009631. Entropy: 0.450431.\n",
      "Iteration 5204: Policy loss: -0.002615. Value loss: 0.007755. Entropy: 0.453738.\n",
      "Iteration 5205: Policy loss: -0.003289. Value loss: 0.007203. Entropy: 0.453662.\n",
      "episode: 3695   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 464     evaluation reward: 5.19\n",
      "episode: 3696   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 387     evaluation reward: 5.16\n",
      "episode: 3697   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 461     evaluation reward: 5.15\n",
      "Training network. lr: 0.000211. clip: 0.084385\n",
      "Iteration 5206: Policy loss: 0.017859. Value loss: 0.015848. Entropy: 0.500190.\n",
      "Iteration 5207: Policy loss: 0.002567. Value loss: 0.014042. Entropy: 0.499049.\n",
      "Iteration 5208: Policy loss: -0.000934. Value loss: 0.012825. Entropy: 0.495946.\n",
      "episode: 3698   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 470     evaluation reward: 5.13\n",
      "episode: 3699   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 389     evaluation reward: 5.02\n",
      "Training network. lr: 0.000211. clip: 0.084376\n",
      "Iteration 5209: Policy loss: 0.019075. Value loss: 0.013335. Entropy: 0.492542.\n",
      "Iteration 5210: Policy loss: 0.007748. Value loss: 0.011219. Entropy: 0.487108.\n",
      "Iteration 5211: Policy loss: -0.000293. Value loss: 0.010002. Entropy: 0.492798.\n",
      "episode: 3700   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 507     evaluation reward: 5.02\n",
      "episode: 3701   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 477     evaluation reward: 4.96\n",
      "Training network. lr: 0.000211. clip: 0.084367\n",
      "Iteration 5212: Policy loss: 0.015826. Value loss: 0.010893. Entropy: 0.480637.\n",
      "Iteration 5213: Policy loss: -0.000281. Value loss: 0.009199. Entropy: 0.485186.\n",
      "Iteration 5214: Policy loss: -0.000073. Value loss: 0.008831. Entropy: 0.477166.\n",
      "episode: 3702   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 577     evaluation reward: 4.93\n",
      "episode: 3703   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 479     evaluation reward: 4.94\n",
      "Training network. lr: 0.000211. clip: 0.084358\n",
      "Iteration 5215: Policy loss: 0.021084. Value loss: 0.016705. Entropy: 0.511260.\n",
      "Iteration 5216: Policy loss: 0.006900. Value loss: 0.012661. Entropy: 0.510590.\n",
      "Iteration 5217: Policy loss: -0.011882. Value loss: 0.011168. Entropy: 0.516156.\n",
      "episode: 3704   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 464     evaluation reward: 4.94\n",
      "episode: 3705   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 526     evaluation reward: 4.95\n",
      "episode: 3706   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 327     evaluation reward: 4.93\n",
      "Training network. lr: 0.000211. clip: 0.084349\n",
      "Iteration 5218: Policy loss: -0.000558. Value loss: 0.015247. Entropy: 0.521564.\n",
      "Iteration 5219: Policy loss: -0.007671. Value loss: 0.011372. Entropy: 0.512433.\n",
      "Iteration 5220: Policy loss: -0.012340. Value loss: 0.009841. Entropy: 0.505301.\n",
      "episode: 3707   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 408     evaluation reward: 4.92\n",
      "episode: 3708   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 513     evaluation reward: 4.92\n",
      "Training network. lr: 0.000211. clip: 0.084340\n",
      "Iteration 5221: Policy loss: 0.015784. Value loss: 0.012859. Entropy: 0.495754.\n",
      "Iteration 5222: Policy loss: 0.005748. Value loss: 0.008696. Entropy: 0.493591.\n",
      "Iteration 5223: Policy loss: -0.007179. Value loss: 0.007590. Entropy: 0.493613.\n",
      "episode: 3709   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 587     evaluation reward: 4.92\n",
      "episode: 3710   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 325     evaluation reward: 4.89\n",
      "Training network. lr: 0.000211. clip: 0.084331\n",
      "Iteration 5224: Policy loss: 0.019689. Value loss: 0.019631. Entropy: 0.525734.\n",
      "Iteration 5225: Policy loss: 0.003714. Value loss: 0.016538. Entropy: 0.530039.\n",
      "Iteration 5226: Policy loss: -0.010232. Value loss: 0.014634. Entropy: 0.519880.\n",
      "episode: 3711   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 418     evaluation reward: 4.88\n",
      "episode: 3712   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 505     evaluation reward: 4.87\n",
      "Training network. lr: 0.000211. clip: 0.084322\n",
      "Iteration 5227: Policy loss: 0.006877. Value loss: 0.012908. Entropy: 0.455541.\n",
      "Iteration 5228: Policy loss: -0.001402. Value loss: 0.010433. Entropy: 0.454651.\n",
      "Iteration 5229: Policy loss: -0.010077. Value loss: 0.008802. Entropy: 0.460365.\n",
      "episode: 3713   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 523     evaluation reward: 4.89\n",
      "episode: 3714   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 504     evaluation reward: 4.89\n",
      "Training network. lr: 0.000211. clip: 0.084313\n",
      "Iteration 5230: Policy loss: 0.011513. Value loss: 0.013118. Entropy: 0.446455.\n",
      "Iteration 5231: Policy loss: 0.004066. Value loss: 0.011158. Entropy: 0.445112.\n",
      "Iteration 5232: Policy loss: -0.012198. Value loss: 0.009971. Entropy: 0.442166.\n",
      "episode: 3715   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 452     evaluation reward: 4.87\n",
      "episode: 3716   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 446     evaluation reward: 4.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000211. clip: 0.084304\n",
      "Iteration 5233: Policy loss: 0.028590. Value loss: 0.013852. Entropy: 0.500646.\n",
      "Iteration 5234: Policy loss: 0.003981. Value loss: 0.010963. Entropy: 0.493407.\n",
      "Iteration 5235: Policy loss: -0.000547. Value loss: 0.009338. Entropy: 0.489859.\n",
      "episode: 3717   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 837     evaluation reward: 4.9\n",
      "episode: 3718   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 583     evaluation reward: 4.91\n",
      "Training network. lr: 0.000211. clip: 0.084295\n",
      "Iteration 5236: Policy loss: 0.007920. Value loss: 0.021477. Entropy: 0.440983.\n",
      "Iteration 5237: Policy loss: -0.013171. Value loss: 0.014685. Entropy: 0.447305.\n",
      "Iteration 5238: Policy loss: -0.012619. Value loss: 0.011815. Entropy: 0.451590.\n",
      "episode: 3719   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 372     evaluation reward: 4.88\n",
      "episode: 3720   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 293     evaluation reward: 4.83\n",
      "Training network. lr: 0.000211. clip: 0.084286\n",
      "Iteration 5239: Policy loss: 0.004309. Value loss: 0.015281. Entropy: 0.550572.\n",
      "Iteration 5240: Policy loss: -0.003760. Value loss: 0.012472. Entropy: 0.545927.\n",
      "Iteration 5241: Policy loss: -0.009885. Value loss: 0.010777. Entropy: 0.548039.\n",
      "episode: 3721   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 505     evaluation reward: 4.8\n",
      "episode: 3722   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 568     evaluation reward: 4.83\n",
      "episode: 3723   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 348     evaluation reward: 4.81\n",
      "Training network. lr: 0.000211. clip: 0.084277\n",
      "Iteration 5242: Policy loss: 0.013429. Value loss: 0.014474. Entropy: 0.533080.\n",
      "Iteration 5243: Policy loss: -0.002752. Value loss: 0.011838. Entropy: 0.525555.\n",
      "Iteration 5244: Policy loss: -0.010783. Value loss: 0.010353. Entropy: 0.515659.\n",
      "episode: 3724   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 414     evaluation reward: 4.82\n",
      "episode: 3725   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 359     evaluation reward: 4.81\n",
      "Training network. lr: 0.000211. clip: 0.084268\n",
      "Iteration 5245: Policy loss: 0.016626. Value loss: 0.014787. Entropy: 0.522837.\n",
      "Iteration 5246: Policy loss: 0.002253. Value loss: 0.012368. Entropy: 0.512127.\n",
      "Iteration 5247: Policy loss: -0.005934. Value loss: 0.010841. Entropy: 0.516335.\n",
      "episode: 3726   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 403     evaluation reward: 4.79\n",
      "episode: 3727   score: 10.0   memory length: 1024   epsilon: 1.0    steps: 526     evaluation reward: 4.85\n",
      "Training network. lr: 0.000211. clip: 0.084259\n",
      "Iteration 5248: Policy loss: 0.017966. Value loss: 0.047264. Entropy: 0.561171.\n",
      "Iteration 5249: Policy loss: 0.021289. Value loss: 0.040968. Entropy: 0.572808.\n",
      "Iteration 5250: Policy loss: 0.000319. Value loss: 0.036252. Entropy: 0.559872.\n",
      "episode: 3728   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 406     evaluation reward: 4.82\n",
      "episode: 3729   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 442     evaluation reward: 4.83\n",
      "episode: 3730   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 340     evaluation reward: 4.81\n",
      "Training network. lr: 0.000211. clip: 0.084250\n",
      "Iteration 5251: Policy loss: 0.020527. Value loss: 0.012152. Entropy: 0.533526.\n",
      "Iteration 5252: Policy loss: 0.003555. Value loss: 0.010480. Entropy: 0.535628.\n",
      "Iteration 5253: Policy loss: -0.002144. Value loss: 0.009313. Entropy: 0.548349.\n",
      "episode: 3731   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 535     evaluation reward: 4.83\n",
      "episode: 3732   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 522     evaluation reward: 4.83\n",
      "Training network. lr: 0.000211. clip: 0.084241\n",
      "Iteration 5254: Policy loss: 0.016282. Value loss: 0.010117. Entropy: 0.586402.\n",
      "Iteration 5255: Policy loss: -0.003730. Value loss: 0.008683. Entropy: 0.579717.\n",
      "Iteration 5256: Policy loss: -0.009434. Value loss: 0.007839. Entropy: 0.576695.\n",
      "episode: 3733   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 451     evaluation reward: 4.85\n",
      "episode: 3734   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 403     evaluation reward: 4.84\n",
      "Training network. lr: 0.000211. clip: 0.084232\n",
      "Iteration 5257: Policy loss: 0.016931. Value loss: 0.013655. Entropy: 0.491596.\n",
      "Iteration 5258: Policy loss: 0.001648. Value loss: 0.011010. Entropy: 0.491726.\n",
      "Iteration 5259: Policy loss: -0.007970. Value loss: 0.009255. Entropy: 0.488800.\n",
      "episode: 3735   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 410     evaluation reward: 4.82\n",
      "episode: 3736   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 614     evaluation reward: 4.82\n",
      "Training network. lr: 0.000211. clip: 0.084223\n",
      "Iteration 5260: Policy loss: 0.011332. Value loss: 0.014678. Entropy: 0.557287.\n",
      "Iteration 5261: Policy loss: 0.000357. Value loss: 0.012170. Entropy: 0.548952.\n",
      "Iteration 5262: Policy loss: -0.012176. Value loss: 0.010780. Entropy: 0.556137.\n",
      "episode: 3737   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 558     evaluation reward: 4.84\n",
      "episode: 3738   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 4.82\n",
      "Training network. lr: 0.000211. clip: 0.084214\n",
      "Iteration 5263: Policy loss: 0.013329. Value loss: 0.014034. Entropy: 0.576954.\n",
      "Iteration 5264: Policy loss: -0.004384. Value loss: 0.011591. Entropy: 0.573596.\n",
      "Iteration 5265: Policy loss: -0.012544. Value loss: 0.009959. Entropy: 0.565382.\n",
      "episode: 3739   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 436     evaluation reward: 4.81\n",
      "episode: 3740   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 357     evaluation reward: 4.8\n",
      "episode: 3741   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 438     evaluation reward: 4.78\n",
      "Training network. lr: 0.000211. clip: 0.084205\n",
      "Iteration 5266: Policy loss: 0.010607. Value loss: 0.014847. Entropy: 0.594328.\n",
      "Iteration 5267: Policy loss: -0.002938. Value loss: 0.011719. Entropy: 0.596739.\n",
      "Iteration 5268: Policy loss: -0.012574. Value loss: 0.009877. Entropy: 0.602043.\n",
      "episode: 3742   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 497     evaluation reward: 4.79\n",
      "episode: 3743   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 351     evaluation reward: 4.79\n",
      "Training network. lr: 0.000210. clip: 0.084196\n",
      "Iteration 5269: Policy loss: 0.015404. Value loss: 0.014116. Entropy: 0.569541.\n",
      "Iteration 5270: Policy loss: -0.002759. Value loss: 0.010258. Entropy: 0.580458.\n",
      "Iteration 5271: Policy loss: -0.009912. Value loss: 0.008943. Entropy: 0.578636.\n",
      "episode: 3744   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 395     evaluation reward: 4.78\n",
      "episode: 3745   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 473     evaluation reward: 4.77\n",
      "now time :  2018-12-26 15:05:22.834773\n",
      "episode: 3746   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 314     evaluation reward: 4.75\n",
      "Training network. lr: 0.000210. clip: 0.084187\n",
      "Iteration 5272: Policy loss: 0.004147. Value loss: 0.017108. Entropy: 0.552821.\n",
      "Iteration 5273: Policy loss: -0.005361. Value loss: 0.014737. Entropy: 0.534561.\n",
      "Iteration 5274: Policy loss: -0.015798. Value loss: 0.012559. Entropy: 0.540841.\n",
      "episode: 3747   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 471     evaluation reward: 4.73\n",
      "episode: 3748   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 400     evaluation reward: 4.73\n",
      "Training network. lr: 0.000210. clip: 0.084178\n",
      "Iteration 5275: Policy loss: 0.006005. Value loss: 0.013413. Entropy: 0.564637.\n",
      "Iteration 5276: Policy loss: -0.005380. Value loss: 0.011574. Entropy: 0.564585.\n",
      "Iteration 5277: Policy loss: -0.005457. Value loss: 0.010423. Entropy: 0.562365.\n",
      "episode: 3749   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 439     evaluation reward: 4.72\n",
      "episode: 3750   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 499     evaluation reward: 4.72\n",
      "episode: 3751   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 371     evaluation reward: 4.68\n",
      "Training network. lr: 0.000210. clip: 0.084169\n",
      "Iteration 5278: Policy loss: 0.014605. Value loss: 0.014024. Entropy: 0.582246.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5279: Policy loss: -0.006303. Value loss: 0.011555. Entropy: 0.579077.\n",
      "Iteration 5280: Policy loss: -0.017538. Value loss: 0.010245. Entropy: 0.568065.\n",
      "episode: 3752   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 404     evaluation reward: 4.68\n",
      "episode: 3753   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 418     evaluation reward: 4.69\n",
      "Training network. lr: 0.000210. clip: 0.084160\n",
      "Iteration 5281: Policy loss: 0.010741. Value loss: 0.012438. Entropy: 0.497550.\n",
      "Iteration 5282: Policy loss: -0.009694. Value loss: 0.010268. Entropy: 0.494810.\n",
      "Iteration 5283: Policy loss: -0.011873. Value loss: 0.008711. Entropy: 0.483065.\n",
      "episode: 3754   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 464     evaluation reward: 4.69\n",
      "episode: 3755   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 417     evaluation reward: 4.7\n",
      "Training network. lr: 0.000210. clip: 0.084151\n",
      "Iteration 5284: Policy loss: 0.010273. Value loss: 0.011546. Entropy: 0.481682.\n",
      "Iteration 5285: Policy loss: -0.000874. Value loss: 0.010098. Entropy: 0.479614.\n",
      "Iteration 5286: Policy loss: -0.012978. Value loss: 0.010250. Entropy: 0.487533.\n",
      "episode: 3756   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 388     evaluation reward: 4.71\n",
      "episode: 3757   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 328     evaluation reward: 4.69\n",
      "episode: 3758   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 377     evaluation reward: 4.68\n",
      "Training network. lr: 0.000210. clip: 0.084142\n",
      "Iteration 5287: Policy loss: 0.004846. Value loss: 0.019304. Entropy: 0.531148.\n",
      "Iteration 5288: Policy loss: -0.001643. Value loss: 0.015068. Entropy: 0.522395.\n",
      "Iteration 5289: Policy loss: -0.004067. Value loss: 0.012918. Entropy: 0.524084.\n",
      "episode: 3759   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 518     evaluation reward: 4.7\n",
      "episode: 3760   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 4.69\n",
      "Training network. lr: 0.000210. clip: 0.084133\n",
      "Iteration 5290: Policy loss: 0.012128. Value loss: 0.015500. Entropy: 0.449924.\n",
      "Iteration 5291: Policy loss: 0.000041. Value loss: 0.011680. Entropy: 0.456044.\n",
      "Iteration 5292: Policy loss: -0.011852. Value loss: 0.010080. Entropy: 0.446069.\n",
      "episode: 3761   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 552     evaluation reward: 4.67\n",
      "episode: 3762   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 349     evaluation reward: 4.65\n",
      "episode: 3763   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 445     evaluation reward: 4.65\n",
      "Training network. lr: 0.000210. clip: 0.084124\n",
      "Iteration 5293: Policy loss: 0.011338. Value loss: 0.020079. Entropy: 0.547497.\n",
      "Iteration 5294: Policy loss: -0.002900. Value loss: 0.015898. Entropy: 0.542163.\n",
      "Iteration 5295: Policy loss: -0.011396. Value loss: 0.014244. Entropy: 0.547800.\n",
      "episode: 3764   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 613     evaluation reward: 4.67\n",
      "Training network. lr: 0.000210. clip: 0.084115\n",
      "Iteration 5296: Policy loss: 0.001807. Value loss: 0.015262. Entropy: 0.493871.\n",
      "Iteration 5297: Policy loss: -0.002216. Value loss: 0.013426. Entropy: 0.498569.\n",
      "Iteration 5298: Policy loss: -0.012935. Value loss: 0.010134. Entropy: 0.487999.\n",
      "episode: 3765   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 532     evaluation reward: 4.69\n",
      "episode: 3766   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 508     evaluation reward: 4.7\n",
      "episode: 3767   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 351     evaluation reward: 4.66\n",
      "Training network. lr: 0.000210. clip: 0.084106\n",
      "Iteration 5299: Policy loss: 0.008988. Value loss: 0.023111. Entropy: 0.537407.\n",
      "Iteration 5300: Policy loss: -0.004944. Value loss: 0.017657. Entropy: 0.536795.\n",
      "Iteration 5301: Policy loss: -0.013154. Value loss: 0.013975. Entropy: 0.542466.\n",
      "episode: 3768   score: 13.0   memory length: 1024   epsilon: 1.0    steps: 705     evaluation reward: 4.75\n",
      "Training network. lr: 0.000210. clip: 0.084097\n",
      "Iteration 5302: Policy loss: 0.016944. Value loss: 0.051799. Entropy: 0.544210.\n",
      "Iteration 5303: Policy loss: 0.001519. Value loss: 0.038902. Entropy: 0.555650.\n",
      "Iteration 5304: Policy loss: -0.006981. Value loss: 0.033143. Entropy: 0.546702.\n",
      "episode: 3769   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 487     evaluation reward: 4.79\n",
      "episode: 3770   score: 11.0   memory length: 1024   epsilon: 1.0    steps: 541     evaluation reward: 4.85\n",
      "Training network. lr: 0.000210. clip: 0.084088\n",
      "Iteration 5305: Policy loss: 0.008598. Value loss: 0.051913. Entropy: 0.563635.\n",
      "Iteration 5306: Policy loss: 0.004328. Value loss: 0.040465. Entropy: 0.551874.\n",
      "Iteration 5307: Policy loss: -0.000568. Value loss: 0.033986. Entropy: 0.545762.\n",
      "episode: 3771   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 458     evaluation reward: 4.85\n",
      "episode: 3772   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 4.85\n",
      "episode: 3773   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 4.84\n",
      "Training network. lr: 0.000210. clip: 0.084079\n",
      "Iteration 5308: Policy loss: 0.004715. Value loss: 0.020953. Entropy: 0.488980.\n",
      "Iteration 5309: Policy loss: -0.003395. Value loss: 0.017165. Entropy: 0.487311.\n",
      "Iteration 5310: Policy loss: -0.011917. Value loss: 0.014695. Entropy: 0.472291.\n",
      "episode: 3774   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 459     evaluation reward: 4.84\n",
      "episode: 3775   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 491     evaluation reward: 4.86\n",
      "Training network. lr: 0.000210. clip: 0.084070\n",
      "Iteration 5311: Policy loss: 0.010808. Value loss: 0.025792. Entropy: 0.571723.\n",
      "Iteration 5312: Policy loss: 0.003485. Value loss: 0.018920. Entropy: 0.575157.\n",
      "Iteration 5313: Policy loss: -0.008970. Value loss: 0.016178. Entropy: 0.576393.\n",
      "episode: 3776   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 257     evaluation reward: 4.84\n",
      "episode: 3777   score: 12.0   memory length: 1024   epsilon: 1.0    steps: 590     evaluation reward: 4.91\n",
      "Training network. lr: 0.000210. clip: 0.084061\n",
      "Iteration 5314: Policy loss: 0.004347. Value loss: 0.050631. Entropy: 0.581781.\n",
      "Iteration 5315: Policy loss: -0.001961. Value loss: 0.041645. Entropy: 0.571006.\n",
      "Iteration 5316: Policy loss: -0.009961. Value loss: 0.036929. Entropy: 0.572841.\n",
      "episode: 3778   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 456     evaluation reward: 4.93\n",
      "episode: 3779   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 411     evaluation reward: 4.93\n",
      "episode: 3780   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 465     evaluation reward: 4.94\n",
      "Training network. lr: 0.000210. clip: 0.084052\n",
      "Iteration 5317: Policy loss: 0.014682. Value loss: 0.018309. Entropy: 0.485740.\n",
      "Iteration 5318: Policy loss: -0.003469. Value loss: 0.013524. Entropy: 0.486640.\n",
      "Iteration 5319: Policy loss: -0.004503. Value loss: 0.011584. Entropy: 0.495515.\n",
      "episode: 3781   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 402     evaluation reward: 4.93\n",
      "episode: 3782   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 567     evaluation reward: 4.97\n",
      "Training network. lr: 0.000210. clip: 0.084043\n",
      "Iteration 5320: Policy loss: 0.000146. Value loss: 0.013664. Entropy: 0.536432.\n",
      "Iteration 5321: Policy loss: -0.011754. Value loss: 0.011710. Entropy: 0.542308.\n",
      "Iteration 5322: Policy loss: -0.014218. Value loss: 0.010318. Entropy: 0.535226.\n",
      "episode: 3783   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 388     evaluation reward: 4.96\n",
      "episode: 3784   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 600     evaluation reward: 4.99\n",
      "Training network. lr: 0.000210. clip: 0.084034\n",
      "Iteration 5323: Policy loss: 0.002114. Value loss: 0.018514. Entropy: 0.533187.\n",
      "Iteration 5324: Policy loss: -0.002487. Value loss: 0.013532. Entropy: 0.524692.\n",
      "Iteration 5325: Policy loss: -0.018028. Value loss: 0.012239. Entropy: 0.530944.\n",
      "episode: 3785   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 472     evaluation reward: 5.0\n",
      "episode: 3786   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 556     evaluation reward: 5.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000210. clip: 0.084025\n",
      "Iteration 5326: Policy loss: 0.011153. Value loss: 0.019324. Entropy: 0.554111.\n",
      "Iteration 5327: Policy loss: -0.008788. Value loss: 0.014340. Entropy: 0.544467.\n",
      "Iteration 5328: Policy loss: -0.012194. Value loss: 0.012559. Entropy: 0.559929.\n",
      "episode: 3787   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 270     evaluation reward: 4.99\n",
      "episode: 3788   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 529     evaluation reward: 5.0\n",
      "Training network. lr: 0.000210. clip: 0.084016\n",
      "Iteration 5329: Policy loss: 0.003212. Value loss: 0.015887. Entropy: 0.677283.\n",
      "Iteration 5330: Policy loss: -0.009725. Value loss: 0.012568. Entropy: 0.665851.\n",
      "Iteration 5331: Policy loss: -0.016071. Value loss: 0.010514. Entropy: 0.668169.\n",
      "episode: 3789   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 381     evaluation reward: 5.02\n",
      "episode: 3790   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 378     evaluation reward: 5.03\n",
      "episode: 3791   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 390     evaluation reward: 5.04\n",
      "Training network. lr: 0.000210. clip: 0.084007\n",
      "Iteration 5332: Policy loss: 0.015098. Value loss: 0.023439. Entropy: 0.515363.\n",
      "Iteration 5333: Policy loss: -0.003169. Value loss: 0.019564. Entropy: 0.527406.\n",
      "Iteration 5334: Policy loss: -0.011127. Value loss: 0.017120. Entropy: 0.517065.\n",
      "episode: 3792   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 451     evaluation reward: 5.06\n",
      "episode: 3793   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 368     evaluation reward: 5.06\n",
      "episode: 3794   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 435     evaluation reward: 5.06\n",
      "Training network. lr: 0.000210. clip: 0.083998\n",
      "Iteration 5335: Policy loss: 0.010021. Value loss: 0.014669. Entropy: 0.514023.\n",
      "Iteration 5336: Policy loss: -0.003795. Value loss: 0.012419. Entropy: 0.522115.\n",
      "Iteration 5337: Policy loss: -0.009214. Value loss: 0.011384. Entropy: 0.510854.\n",
      "episode: 3795   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 379     evaluation reward: 5.05\n",
      "episode: 3796   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 413     evaluation reward: 5.05\n",
      "Training network. lr: 0.000210. clip: 0.083989\n",
      "Iteration 5338: Policy loss: 0.006219. Value loss: 0.014211. Entropy: 0.542594.\n",
      "Iteration 5339: Policy loss: -0.008524. Value loss: 0.010876. Entropy: 0.532937.\n",
      "Iteration 5340: Policy loss: -0.010790. Value loss: 0.009387. Entropy: 0.532784.\n",
      "episode: 3797   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 5.04\n",
      "episode: 3798   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 426     evaluation reward: 5.04\n",
      "Training network. lr: 0.000210. clip: 0.083980\n",
      "Iteration 5341: Policy loss: 0.008617. Value loss: 0.018153. Entropy: 0.540719.\n",
      "Iteration 5342: Policy loss: -0.008391. Value loss: 0.012823. Entropy: 0.531935.\n",
      "Iteration 5343: Policy loss: -0.015210. Value loss: 0.011242. Entropy: 0.524531.\n",
      "episode: 3799   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 473     evaluation reward: 5.06\n",
      "episode: 3800   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 438     evaluation reward: 5.05\n",
      "Training network. lr: 0.000210. clip: 0.083971\n",
      "Iteration 5344: Policy loss: 0.017087. Value loss: 0.013638. Entropy: 0.461740.\n",
      "Iteration 5345: Policy loss: 0.002142. Value loss: 0.012270. Entropy: 0.447722.\n",
      "Iteration 5346: Policy loss: 0.011105. Value loss: 0.008591. Entropy: 0.429719.\n",
      "episode: 3801   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 535     evaluation reward: 5.08\n",
      "episode: 3802   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 360     evaluation reward: 5.05\n",
      "episode: 3803   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 442     evaluation reward: 5.05\n",
      "Training network. lr: 0.000210. clip: 0.083962\n",
      "Iteration 5347: Policy loss: 0.015281. Value loss: 0.015042. Entropy: 0.510964.\n",
      "Iteration 5348: Policy loss: 0.001338. Value loss: 0.012131. Entropy: 0.505585.\n",
      "Iteration 5349: Policy loss: -0.010415. Value loss: 0.010495. Entropy: 0.500839.\n",
      "episode: 3804   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 276     evaluation reward: 5.03\n",
      "episode: 3805   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 397     evaluation reward: 5.02\n",
      "episode: 3806   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 417     evaluation reward: 5.03\n",
      "Training network. lr: 0.000210. clip: 0.083953\n",
      "Iteration 5350: Policy loss: 0.008168. Value loss: 0.017973. Entropy: 0.503655.\n",
      "Iteration 5351: Policy loss: -0.003685. Value loss: 0.014310. Entropy: 0.505039.\n",
      "Iteration 5352: Policy loss: -0.010509. Value loss: 0.010446. Entropy: 0.498827.\n",
      "episode: 3807   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 366     evaluation reward: 5.03\n",
      "episode: 3808   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 622     evaluation reward: 5.05\n",
      "Training network. lr: 0.000210. clip: 0.083944\n",
      "Iteration 5353: Policy loss: 0.002061. Value loss: 0.014849. Entropy: 0.485411.\n",
      "Iteration 5354: Policy loss: -0.003899. Value loss: 0.010776. Entropy: 0.484317.\n",
      "Iteration 5355: Policy loss: -0.010068. Value loss: 0.009491. Entropy: 0.478392.\n",
      "episode: 3809   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 326     evaluation reward: 5.02\n",
      "episode: 3810   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 447     evaluation reward: 5.07\n",
      "episode: 3811   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 357     evaluation reward: 5.07\n",
      "Training network. lr: 0.000210. clip: 0.083935\n",
      "Iteration 5356: Policy loss: 0.008700. Value loss: 0.033972. Entropy: 0.503489.\n",
      "Iteration 5357: Policy loss: 0.003902. Value loss: 0.029484. Entropy: 0.497359.\n",
      "Iteration 5358: Policy loss: -0.001199. Value loss: 0.025717. Entropy: 0.500013.\n",
      "episode: 3812   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 435     evaluation reward: 5.07\n",
      "episode: 3813   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 273     evaluation reward: 5.03\n",
      "episode: 3814   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 244     evaluation reward: 4.98\n",
      "Training network. lr: 0.000210. clip: 0.083926\n",
      "Iteration 5359: Policy loss: 0.013143. Value loss: 0.022425. Entropy: 0.541671.\n",
      "Iteration 5360: Policy loss: 0.001107. Value loss: 0.015496. Entropy: 0.540704.\n",
      "Iteration 5361: Policy loss: -0.006787. Value loss: 0.012600. Entropy: 0.529524.\n",
      "episode: 3815   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 330     evaluation reward: 4.96\n",
      "episode: 3816   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 450     evaluation reward: 4.98\n",
      "episode: 3817   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 338     evaluation reward: 4.92\n",
      "Training network. lr: 0.000210. clip: 0.083917\n",
      "Iteration 5362: Policy loss: 0.016288. Value loss: 0.018824. Entropy: 0.559595.\n",
      "Iteration 5363: Policy loss: -0.007206. Value loss: 0.014911. Entropy: 0.550303.\n",
      "Iteration 5364: Policy loss: -0.015029. Value loss: 0.012691. Entropy: 0.548320.\n",
      "episode: 3818   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 291     evaluation reward: 4.87\n",
      "episode: 3819   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 495     evaluation reward: 4.9\n",
      "Training network. lr: 0.000210. clip: 0.083908\n",
      "Iteration 5365: Policy loss: 0.010905. Value loss: 0.011664. Entropy: 0.406422.\n",
      "Iteration 5366: Policy loss: -0.005102. Value loss: 0.008939. Entropy: 0.409278.\n",
      "Iteration 5367: Policy loss: -0.006869. Value loss: 0.007888. Entropy: 0.416840.\n",
      "episode: 3820   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 533     evaluation reward: 4.94\n",
      "episode: 3821   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 450     evaluation reward: 4.94\n",
      "Training network. lr: 0.000210. clip: 0.083899\n",
      "Iteration 5368: Policy loss: 0.011144. Value loss: 0.012026. Entropy: 0.439189.\n",
      "Iteration 5369: Policy loss: -0.007493. Value loss: 0.010046. Entropy: 0.433253.\n",
      "Iteration 5370: Policy loss: -0.005978. Value loss: 0.009368. Entropy: 0.436121.\n",
      "episode: 3822   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 580     evaluation reward: 4.95\n",
      "episode: 3823   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 666     evaluation reward: 5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000210. clip: 0.083890\n",
      "Iteration 5371: Policy loss: 0.008890. Value loss: 0.016104. Entropy: 0.471921.\n",
      "Iteration 5372: Policy loss: 0.003780. Value loss: 0.012790. Entropy: 0.464872.\n",
      "Iteration 5373: Policy loss: -0.008956. Value loss: 0.010587. Entropy: 0.452186.\n",
      "episode: 3824   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 439     evaluation reward: 5.02\n",
      "episode: 3825   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 404     evaluation reward: 5.03\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5374: Policy loss: 0.007058. Value loss: 0.015161. Entropy: 0.548321.\n",
      "Iteration 5375: Policy loss: -0.004737. Value loss: 0.010628. Entropy: 0.546592.\n",
      "Iteration 5376: Policy loss: -0.012067. Value loss: 0.009104. Entropy: 0.539995.\n",
      "episode: 3826   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 474     evaluation reward: 5.04\n",
      "episode: 3827   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 423     evaluation reward: 4.99\n",
      "Training network. lr: 0.000210. clip: 0.083872\n",
      "Iteration 5377: Policy loss: 0.007091. Value loss: 0.048005. Entropy: 0.466831.\n",
      "Iteration 5378: Policy loss: -0.003925. Value loss: 0.033269. Entropy: 0.441786.\n",
      "Iteration 5379: Policy loss: -0.013283. Value loss: 0.028517. Entropy: 0.437476.\n",
      "episode: 3828   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 521     evaluation reward: 5.05\n",
      "episode: 3829   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 420     evaluation reward: 5.04\n",
      "episode: 3830   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 390     evaluation reward: 5.05\n",
      "Training network. lr: 0.000210. clip: 0.083863\n",
      "Iteration 5380: Policy loss: 0.008200. Value loss: 0.017956. Entropy: 0.512454.\n",
      "Iteration 5381: Policy loss: -0.006189. Value loss: 0.013430. Entropy: 0.519888.\n",
      "Iteration 5382: Policy loss: -0.015415. Value loss: 0.011898. Entropy: 0.515189.\n",
      "episode: 3831   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 417     evaluation reward: 5.04\n",
      "episode: 3832   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 407     evaluation reward: 5.03\n",
      "Training network. lr: 0.000210. clip: 0.083854\n",
      "Iteration 5383: Policy loss: 0.012003. Value loss: 0.017923. Entropy: 0.473201.\n",
      "Iteration 5384: Policy loss: 0.002048. Value loss: 0.014188. Entropy: 0.476562.\n",
      "Iteration 5385: Policy loss: -0.003947. Value loss: 0.013259. Entropy: 0.460980.\n",
      "episode: 3833   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 461     evaluation reward: 5.03\n",
      "episode: 3834   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 519     evaluation reward: 5.07\n",
      "Training network. lr: 0.000210. clip: 0.083845\n",
      "Iteration 5386: Policy loss: 0.005170. Value loss: 0.013039. Entropy: 0.461012.\n",
      "Iteration 5387: Policy loss: -0.002923. Value loss: 0.010636. Entropy: 0.480138.\n",
      "Iteration 5388: Policy loss: -0.010305. Value loss: 0.009258. Entropy: 0.475558.\n",
      "episode: 3835   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 482     evaluation reward: 5.09\n",
      "episode: 3836   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 346     evaluation reward: 5.06\n",
      "episode: 3837   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 416     evaluation reward: 5.04\n",
      "Training network. lr: 0.000210. clip: 0.083836\n",
      "Iteration 5389: Policy loss: 0.006636. Value loss: 0.018442. Entropy: 0.530912.\n",
      "Iteration 5390: Policy loss: -0.001812. Value loss: 0.014189. Entropy: 0.525044.\n",
      "Iteration 5391: Policy loss: -0.010739. Value loss: 0.013205. Entropy: 0.531404.\n",
      "episode: 3838   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 386     evaluation reward: 5.04\n",
      "episode: 3839   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 286     evaluation reward: 5.01\n",
      "Training network. lr: 0.000210. clip: 0.083827\n",
      "Iteration 5392: Policy loss: 0.009358. Value loss: 0.017527. Entropy: 0.495299.\n",
      "Iteration 5393: Policy loss: -0.010318. Value loss: 0.011917. Entropy: 0.489320.\n",
      "Iteration 5394: Policy loss: -0.016185. Value loss: 0.010498. Entropy: 0.485832.\n",
      "episode: 3840   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 719     evaluation reward: 5.03\n",
      "episode: 3841   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 596     evaluation reward: 5.06\n",
      "Training network. lr: 0.000210. clip: 0.083818\n",
      "Iteration 5395: Policy loss: 0.003487. Value loss: 0.015098. Entropy: 0.375967.\n",
      "Iteration 5396: Policy loss: -0.004175. Value loss: 0.011496. Entropy: 0.364490.\n",
      "Iteration 5397: Policy loss: -0.013407. Value loss: 0.010404. Entropy: 0.359467.\n",
      "episode: 3842   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 299     evaluation reward: 5.03\n",
      "episode: 3843   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 433     evaluation reward: 5.04\n",
      "Training network. lr: 0.000210. clip: 0.083809\n",
      "Iteration 5398: Policy loss: 0.009880. Value loss: 0.021485. Entropy: 0.421676.\n",
      "Iteration 5399: Policy loss: -0.007185. Value loss: 0.017132. Entropy: 0.420191.\n",
      "Iteration 5400: Policy loss: -0.016307. Value loss: 0.014304. Entropy: 0.415568.\n",
      "episode: 3844   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 634     evaluation reward: 5.09\n",
      "episode: 3845   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 507     evaluation reward: 5.09\n",
      "Training network. lr: 0.000209. clip: 0.083800\n",
      "Iteration 5401: Policy loss: 0.014622. Value loss: 0.015496. Entropy: 0.535954.\n",
      "Iteration 5402: Policy loss: 0.001063. Value loss: 0.013444. Entropy: 0.534407.\n",
      "Iteration 5403: Policy loss: -0.006511. Value loss: 0.012188. Entropy: 0.527180.\n",
      "episode: 3846   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 614     evaluation reward: 5.11\n",
      "episode: 3847   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 331     evaluation reward: 5.08\n",
      "episode: 3848   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 325     evaluation reward: 5.07\n",
      "Training network. lr: 0.000209. clip: 0.083791\n",
      "Iteration 5404: Policy loss: 0.013041. Value loss: 0.013292. Entropy: 0.471094.\n",
      "Iteration 5405: Policy loss: 0.002137. Value loss: 0.010702. Entropy: 0.469668.\n",
      "Iteration 5406: Policy loss: -0.008637. Value loss: 0.009726. Entropy: 0.476023.\n",
      "episode: 3849   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 998     evaluation reward: 5.09\n",
      "Training network. lr: 0.000209. clip: 0.083782\n",
      "Iteration 5407: Policy loss: 0.003221. Value loss: 0.010148. Entropy: 0.320378.\n",
      "Iteration 5408: Policy loss: -0.002098. Value loss: 0.007444. Entropy: 0.301384.\n",
      "Iteration 5409: Policy loss: -0.004243. Value loss: 0.006111. Entropy: 0.320491.\n",
      "episode: 3850   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 423     evaluation reward: 5.08\n",
      "episode: 3851   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 514     evaluation reward: 5.11\n",
      "Training network. lr: 0.000209. clip: 0.083773\n",
      "Iteration 5410: Policy loss: 0.018015. Value loss: 0.010934. Entropy: 0.425005.\n",
      "Iteration 5411: Policy loss: 0.013899. Value loss: 0.008848. Entropy: 0.426269.\n",
      "Iteration 5412: Policy loss: 0.004215. Value loss: 0.007755. Entropy: 0.420312.\n",
      "episode: 3852   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 406     evaluation reward: 5.11\n",
      "episode: 3853   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 620     evaluation reward: 5.08\n",
      "Training network. lr: 0.000209. clip: 0.083764\n",
      "Iteration 5413: Policy loss: 0.007624. Value loss: 0.010245. Entropy: 0.292367.\n",
      "Iteration 5414: Policy loss: 0.002729. Value loss: 0.006871. Entropy: 0.271467.\n",
      "Iteration 5415: Policy loss: -0.004219. Value loss: 0.005808. Entropy: 0.277735.\n",
      "episode: 3854   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 466     evaluation reward: 5.06\n",
      "episode: 3855   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 420     evaluation reward: 5.07\n",
      "Training network. lr: 0.000209. clip: 0.083755\n",
      "Iteration 5416: Policy loss: 0.007978. Value loss: 0.012393. Entropy: 0.401095.\n",
      "Iteration 5417: Policy loss: -0.007566. Value loss: 0.009540. Entropy: 0.400851.\n",
      "Iteration 5418: Policy loss: -0.006828. Value loss: 0.007524. Entropy: 0.396491.\n",
      "episode: 3856   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 457     evaluation reward: 5.08\n",
      "now time :  2018-12-26 15:11:06.343593\n",
      "episode: 3857   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 609     evaluation reward: 5.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000209. clip: 0.083746\n",
      "Iteration 5419: Policy loss: 0.000671. Value loss: 0.016351. Entropy: 0.389515.\n",
      "Iteration 5420: Policy loss: -0.006875. Value loss: 0.011867. Entropy: 0.389262.\n",
      "Iteration 5421: Policy loss: -0.014436. Value loss: 0.010335. Entropy: 0.383246.\n",
      "episode: 3858   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 544     evaluation reward: 5.14\n",
      "episode: 3859   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 467     evaluation reward: 5.12\n",
      "Training network. lr: 0.000209. clip: 0.083737\n",
      "Iteration 5422: Policy loss: 0.008329. Value loss: 0.012100. Entropy: 0.456683.\n",
      "Iteration 5423: Policy loss: 0.003592. Value loss: 0.009628. Entropy: 0.454334.\n",
      "Iteration 5424: Policy loss: -0.015682. Value loss: 0.007432. Entropy: 0.454604.\n",
      "episode: 3860   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 424     evaluation reward: 5.11\n",
      "episode: 3861   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 421     evaluation reward: 5.08\n",
      "Training network. lr: 0.000209. clip: 0.083728\n",
      "Iteration 5425: Policy loss: 0.030478. Value loss: 0.011695. Entropy: 0.309244.\n",
      "Iteration 5426: Policy loss: -0.001150. Value loss: 0.009704. Entropy: 0.312017.\n",
      "Iteration 5427: Policy loss: -0.001677. Value loss: 0.008260. Entropy: 0.301758.\n",
      "episode: 3862   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 1021     evaluation reward: 5.13\n",
      "episode: 3863   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 427     evaluation reward: 5.12\n",
      "Training network. lr: 0.000209. clip: 0.083719\n",
      "Iteration 5428: Policy loss: 0.005326. Value loss: 0.012765. Entropy: 0.362906.\n",
      "Iteration 5429: Policy loss: -0.005116. Value loss: 0.007436. Entropy: 0.370331.\n",
      "Iteration 5430: Policy loss: -0.010893. Value loss: 0.006896. Entropy: 0.371702.\n",
      "episode: 3864   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 379     evaluation reward: 5.07\n",
      "episode: 3865   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 5.05\n",
      "Training network. lr: 0.000209. clip: 0.083710\n",
      "Iteration 5431: Policy loss: 0.025502. Value loss: 0.010003. Entropy: 0.437533.\n",
      "Iteration 5432: Policy loss: 0.002337. Value loss: 0.007419. Entropy: 0.453841.\n",
      "Iteration 5433: Policy loss: -0.002971. Value loss: 0.007021. Entropy: 0.442203.\n",
      "episode: 3866   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 470     evaluation reward: 5.04\n",
      "episode: 3867   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 383     evaluation reward: 5.05\n",
      "episode: 3868   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 299     evaluation reward: 4.95\n",
      "Training network. lr: 0.000209. clip: 0.083701\n",
      "Iteration 5434: Policy loss: 0.013545. Value loss: 0.019998. Entropy: 0.452930.\n",
      "Iteration 5435: Policy loss: -0.003181. Value loss: 0.015774. Entropy: 0.453998.\n",
      "Iteration 5436: Policy loss: -0.008613. Value loss: 0.013506. Entropy: 0.451729.\n",
      "episode: 3869   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 558     evaluation reward: 4.94\n",
      "episode: 3870   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 411     evaluation reward: 4.88\n",
      "Training network. lr: 0.000209. clip: 0.083692\n",
      "Iteration 5437: Policy loss: 0.013167. Value loss: 0.015990. Entropy: 0.370210.\n",
      "Iteration 5438: Policy loss: 0.002577. Value loss: 0.011837. Entropy: 0.364926.\n",
      "Iteration 5439: Policy loss: -0.008180. Value loss: 0.010132. Entropy: 0.362309.\n",
      "episode: 3871   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 336     evaluation reward: 4.86\n",
      "episode: 3872   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 455     evaluation reward: 4.84\n",
      "Training network. lr: 0.000209. clip: 0.083683\n",
      "Iteration 5440: Policy loss: 0.006204. Value loss: 0.010683. Entropy: 0.412831.\n",
      "Iteration 5441: Policy loss: -0.003657. Value loss: 0.008895. Entropy: 0.398457.\n",
      "Iteration 5442: Policy loss: -0.013269. Value loss: 0.007805. Entropy: 0.402801.\n",
      "episode: 3873   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 432     evaluation reward: 4.83\n",
      "episode: 3874   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 383     evaluation reward: 4.82\n",
      "episode: 3875   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 582     evaluation reward: 4.79\n",
      "Training network. lr: 0.000209. clip: 0.083674\n",
      "Iteration 5443: Policy loss: 0.019172. Value loss: 0.010505. Entropy: 0.485229.\n",
      "Iteration 5444: Policy loss: 0.000718. Value loss: 0.008669. Entropy: 0.478724.\n",
      "Iteration 5445: Policy loss: -0.010805. Value loss: 0.007168. Entropy: 0.477156.\n",
      "episode: 3876   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 347     evaluation reward: 4.8\n",
      "episode: 3877   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 414     evaluation reward: 4.72\n",
      "Training network. lr: 0.000209. clip: 0.083665\n",
      "Iteration 5446: Policy loss: 0.005926. Value loss: 0.010003. Entropy: 0.290301.\n",
      "Iteration 5447: Policy loss: 0.002627. Value loss: 0.007370. Entropy: 0.289732.\n",
      "Iteration 5448: Policy loss: 0.001014. Value loss: 0.005948. Entropy: 0.290657.\n",
      "Training network. lr: 0.000209. clip: 0.083656\n",
      "Iteration 5449: Policy loss: -0.003064. Value loss: 0.002878. Entropy: 0.150186.\n",
      "Iteration 5450: Policy loss: -0.010155. Value loss: 0.001663. Entropy: 0.182255.\n",
      "Iteration 5451: Policy loss: -0.022091. Value loss: 0.001555. Entropy: 0.365121.\n",
      "episode: 3878   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 1349     evaluation reward: 4.73\n",
      "episode: 3879   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 549     evaluation reward: 4.76\n",
      "episode: 3880   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 380     evaluation reward: 4.75\n",
      "Training network. lr: 0.000209. clip: 0.083647\n",
      "Iteration 5452: Policy loss: 0.010107. Value loss: 0.012043. Entropy: 0.447511.\n",
      "Iteration 5453: Policy loss: -0.003711. Value loss: 0.009473. Entropy: 0.447789.\n",
      "Iteration 5454: Policy loss: -0.016608. Value loss: 0.008000. Entropy: 0.456742.\n",
      "episode: 3881   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 406     evaluation reward: 4.75\n",
      "episode: 3882   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 481     evaluation reward: 4.74\n",
      "Training network. lr: 0.000209. clip: 0.083638\n",
      "Iteration 5455: Policy loss: 0.009396. Value loss: 0.011248. Entropy: 0.345310.\n",
      "Iteration 5456: Policy loss: 0.000221. Value loss: 0.009465. Entropy: 0.338764.\n",
      "Iteration 5457: Policy loss: -0.007998. Value loss: 0.008580. Entropy: 0.332724.\n",
      "episode: 3883   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 295     evaluation reward: 4.72\n",
      "episode: 3884   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 478     evaluation reward: 4.71\n",
      "episode: 3885   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 368     evaluation reward: 4.7\n",
      "Training network. lr: 0.000209. clip: 0.083629\n",
      "Iteration 5458: Policy loss: 0.013198. Value loss: 0.018188. Entropy: 0.343846.\n",
      "Iteration 5459: Policy loss: -0.002487. Value loss: 0.013499. Entropy: 0.342347.\n",
      "Iteration 5460: Policy loss: -0.007079. Value loss: 0.012073. Entropy: 0.348268.\n",
      "episode: 3886   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 299     evaluation reward: 4.66\n",
      "episode: 3887   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 324     evaluation reward: 4.67\n",
      "episode: 3888   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 409     evaluation reward: 4.66\n",
      "Training network. lr: 0.000209. clip: 0.083620\n",
      "Iteration 5461: Policy loss: 0.009151. Value loss: 0.012165. Entropy: 0.478539.\n",
      "Iteration 5462: Policy loss: -0.004927. Value loss: 0.009875. Entropy: 0.470780.\n",
      "Iteration 5463: Policy loss: -0.014681. Value loss: 0.008476. Entropy: 0.476443.\n",
      "episode: 3889   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 436     evaluation reward: 4.67\n",
      "episode: 3890   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 336     evaluation reward: 4.66\n",
      "Training network. lr: 0.000209. clip: 0.083611\n",
      "Iteration 5464: Policy loss: 0.006352. Value loss: 0.015507. Entropy: 0.380416.\n",
      "Iteration 5465: Policy loss: -0.005885. Value loss: 0.010138. Entropy: 0.391213.\n",
      "Iteration 5466: Policy loss: -0.012545. Value loss: 0.009032. Entropy: 0.385146.\n",
      "episode: 3891   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 516     evaluation reward: 4.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3892   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 483     evaluation reward: 4.68\n",
      "Training network. lr: 0.000209. clip: 0.083602\n",
      "Iteration 5467: Policy loss: 0.007769. Value loss: 0.015396. Entropy: 0.395205.\n",
      "Iteration 5468: Policy loss: 0.001480. Value loss: 0.012055. Entropy: 0.384756.\n",
      "Iteration 5469: Policy loss: -0.009663. Value loss: 0.009898. Entropy: 0.394727.\n",
      "episode: 3893   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 398     evaluation reward: 4.68\n",
      "episode: 3894   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 408     evaluation reward: 4.67\n",
      "episode: 3895   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 399     evaluation reward: 4.68\n",
      "Training network. lr: 0.000209. clip: 0.083593\n",
      "Iteration 5470: Policy loss: 0.007885. Value loss: 0.014410. Entropy: 0.352340.\n",
      "Iteration 5471: Policy loss: -0.005304. Value loss: 0.011119. Entropy: 0.350745.\n",
      "Iteration 5472: Policy loss: -0.010920. Value loss: 0.010129. Entropy: 0.353543.\n",
      "episode: 3896   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 337     evaluation reward: 4.67\n",
      "episode: 3897   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 535     evaluation reward: 4.69\n",
      "Training network. lr: 0.000209. clip: 0.083584\n",
      "Iteration 5473: Policy loss: 0.008308. Value loss: 0.014239. Entropy: 0.425148.\n",
      "Iteration 5474: Policy loss: -0.005420. Value loss: 0.011149. Entropy: 0.428029.\n",
      "Iteration 5475: Policy loss: -0.007054. Value loss: 0.009467. Entropy: 0.431583.\n",
      "episode: 3898   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 371     evaluation reward: 4.68\n",
      "episode: 3899   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 437     evaluation reward: 4.67\n",
      "episode: 3900   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 362     evaluation reward: 4.66\n",
      "Training network. lr: 0.000209. clip: 0.083575\n",
      "Iteration 5476: Policy loss: 0.011913. Value loss: 0.013213. Entropy: 0.323090.\n",
      "Iteration 5477: Policy loss: 0.001313. Value loss: 0.010573. Entropy: 0.336987.\n",
      "Iteration 5478: Policy loss: -0.009109. Value loss: 0.008570. Entropy: 0.344192.\n",
      "episode: 3901   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 368     evaluation reward: 4.63\n",
      "episode: 3902   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 362     evaluation reward: 4.64\n",
      "episode: 3903   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 409     evaluation reward: 4.63\n",
      "Training network. lr: 0.000209. clip: 0.083566\n",
      "Iteration 5479: Policy loss: 0.002407. Value loss: 0.009053. Entropy: 0.350082.\n",
      "Iteration 5480: Policy loss: -0.005755. Value loss: 0.006986. Entropy: 0.349368.\n",
      "Iteration 5481: Policy loss: -0.008403. Value loss: 0.006558. Entropy: 0.352921.\n",
      "episode: 3904   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 374     evaluation reward: 4.65\n",
      "episode: 3905   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 4.68\n",
      "Training network. lr: 0.000209. clip: 0.083557\n",
      "Iteration 5482: Policy loss: 0.007640. Value loss: 0.011983. Entropy: 0.367311.\n",
      "Iteration 5483: Policy loss: -0.007418. Value loss: 0.008523. Entropy: 0.369345.\n",
      "Iteration 5484: Policy loss: -0.010387. Value loss: 0.007828. Entropy: 0.354138.\n",
      "episode: 3906   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 522     evaluation reward: 4.7\n",
      "episode: 3907   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 375     evaluation reward: 4.7\n",
      "Training network. lr: 0.000209. clip: 0.083548\n",
      "Iteration 5485: Policy loss: 0.021977. Value loss: 0.011493. Entropy: 0.449884.\n",
      "Iteration 5486: Policy loss: 0.000063. Value loss: 0.010139. Entropy: 0.447091.\n",
      "Iteration 5487: Policy loss: -0.000985. Value loss: 0.008540. Entropy: 0.423916.\n",
      "episode: 3908   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 384     evaluation reward: 4.66\n",
      "episode: 3909   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 388     evaluation reward: 4.67\n",
      "episode: 3910   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 492     evaluation reward: 4.64\n",
      "Training network. lr: 0.000209. clip: 0.083539\n",
      "Iteration 5488: Policy loss: 0.095543. Value loss: 0.114010. Entropy: 0.403564.\n",
      "Iteration 5489: Policy loss: 0.023357. Value loss: 0.031163. Entropy: 0.396628.\n",
      "Iteration 5490: Policy loss: 0.003486. Value loss: 0.011002. Entropy: 0.390616.\n",
      "episode: 3911   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 456     evaluation reward: 4.66\n",
      "episode: 3912   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 331     evaluation reward: 4.64\n",
      "Training network. lr: 0.000209. clip: 0.083530\n",
      "Iteration 5491: Policy loss: 0.008829. Value loss: 0.013560. Entropy: 0.340685.\n",
      "Iteration 5492: Policy loss: 0.000016. Value loss: 0.010442. Entropy: 0.342563.\n",
      "Iteration 5493: Policy loss: -0.003767. Value loss: 0.008085. Entropy: 0.356598.\n",
      "episode: 3913   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 444     evaluation reward: 4.66\n",
      "episode: 3914   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 488     evaluation reward: 4.7\n",
      "episode: 3915   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 378     evaluation reward: 4.71\n",
      "Training network. lr: 0.000209. clip: 0.083521\n",
      "Iteration 5494: Policy loss: 0.013284. Value loss: 0.014600. Entropy: 0.365077.\n",
      "Iteration 5495: Policy loss: 0.000210. Value loss: 0.010797. Entropy: 0.370437.\n",
      "Iteration 5496: Policy loss: -0.009907. Value loss: 0.009119. Entropy: 0.360744.\n",
      "episode: 3916   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 408     evaluation reward: 4.69\n",
      "episode: 3917   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 338     evaluation reward: 4.68\n",
      "Training network. lr: 0.000209. clip: 0.083512\n",
      "Iteration 5497: Policy loss: 0.011226. Value loss: 0.011061. Entropy: 0.345725.\n",
      "Iteration 5498: Policy loss: 0.000013. Value loss: 0.007102. Entropy: 0.340567.\n",
      "Iteration 5499: Policy loss: 0.000411. Value loss: 0.005753. Entropy: 0.337133.\n",
      "episode: 3918   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 423     evaluation reward: 4.71\n",
      "episode: 3919   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 495     evaluation reward: 4.7\n",
      "episode: 3920   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 404     evaluation reward: 4.68\n",
      "Training network. lr: 0.000209. clip: 0.083503\n",
      "Iteration 5500: Policy loss: 0.007859. Value loss: 0.011369. Entropy: 0.315304.\n",
      "Iteration 5501: Policy loss: 0.001423. Value loss: 0.008087. Entropy: 0.318820.\n",
      "Iteration 5502: Policy loss: -0.004294. Value loss: 0.008414. Entropy: 0.313736.\n",
      "episode: 3921   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 537     evaluation reward: 4.68\n",
      "Training network. lr: 0.000209. clip: 0.083494\n",
      "Iteration 5503: Policy loss: 0.009934. Value loss: 0.008583. Entropy: 0.284906.\n",
      "Iteration 5504: Policy loss: -0.000995. Value loss: 0.006816. Entropy: 0.286380.\n",
      "Iteration 5505: Policy loss: -0.005782. Value loss: 0.006145. Entropy: 0.292285.\n",
      "episode: 3922   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 683     evaluation reward: 4.67\n",
      "episode: 3923   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 519     evaluation reward: 4.63\n",
      "Training network. lr: 0.000209. clip: 0.083485\n",
      "Iteration 5506: Policy loss: 0.005878. Value loss: 0.009397. Entropy: 0.376682.\n",
      "Iteration 5507: Policy loss: -0.001982. Value loss: 0.007621. Entropy: 0.366254.\n",
      "Iteration 5508: Policy loss: -0.004935. Value loss: 0.006426. Entropy: 0.361953.\n",
      "episode: 3924   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 393     evaluation reward: 4.61\n",
      "episode: 3925   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 500     evaluation reward: 4.62\n",
      "episode: 3926   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 478     evaluation reward: 4.62\n",
      "Training network. lr: 0.000209. clip: 0.083476\n",
      "Iteration 5509: Policy loss: 0.013706. Value loss: 0.015443. Entropy: 0.311064.\n",
      "Iteration 5510: Policy loss: 0.000301. Value loss: 0.009163. Entropy: 0.299536.\n",
      "Iteration 5511: Policy loss: 0.000633. Value loss: 0.007594. Entropy: 0.304800.\n",
      "episode: 3927   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 467     evaluation reward: 4.61\n",
      "Training network. lr: 0.000209. clip: 0.083467\n",
      "Iteration 5512: Policy loss: 0.016077. Value loss: 0.012073. Entropy: 0.328410.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5513: Policy loss: 0.003202. Value loss: 0.009310. Entropy: 0.340342.\n",
      "Iteration 5514: Policy loss: 0.000172. Value loss: 0.007987. Entropy: 0.327823.\n",
      "episode: 3928   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 640     evaluation reward: 4.58\n",
      "episode: 3929   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 390     evaluation reward: 4.58\n",
      "episode: 3930   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 554     evaluation reward: 4.6\n",
      "Training network. lr: 0.000209. clip: 0.083458\n",
      "Iteration 5515: Policy loss: 0.007111. Value loss: 0.011652. Entropy: 0.358320.\n",
      "Iteration 5516: Policy loss: -0.007594. Value loss: 0.009673. Entropy: 0.364007.\n",
      "Iteration 5517: Policy loss: -0.015405. Value loss: 0.008649. Entropy: 0.372743.\n",
      "episode: 3931   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 389     evaluation reward: 4.59\n",
      "episode: 3932   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 380     evaluation reward: 4.58\n",
      "Training network. lr: 0.000209. clip: 0.083449\n",
      "Iteration 5518: Policy loss: 0.017997. Value loss: 0.016594. Entropy: 0.333032.\n",
      "Iteration 5519: Policy loss: 0.003978. Value loss: 0.013435. Entropy: 0.338953.\n",
      "Iteration 5520: Policy loss: -0.006579. Value loss: 0.012481. Entropy: 0.332975.\n",
      "episode: 3933   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 419     evaluation reward: 4.57\n",
      "episode: 3934   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 510     evaluation reward: 4.57\n",
      "Training network. lr: 0.000209. clip: 0.083440\n",
      "Iteration 5521: Policy loss: 0.013860. Value loss: 0.013009. Entropy: 0.345014.\n",
      "Iteration 5522: Policy loss: 0.004142. Value loss: 0.008958. Entropy: 0.341723.\n",
      "Iteration 5523: Policy loss: -0.006128. Value loss: 0.008289. Entropy: 0.351525.\n",
      "episode: 3935   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 366     evaluation reward: 4.55\n",
      "episode: 3936   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 512     evaluation reward: 4.56\n",
      "episode: 3937   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 372     evaluation reward: 4.55\n",
      "Training network. lr: 0.000209. clip: 0.083431\n",
      "Iteration 5524: Policy loss: 0.007277. Value loss: 0.015558. Entropy: 0.348617.\n",
      "Iteration 5525: Policy loss: -0.006877. Value loss: 0.011755. Entropy: 0.352573.\n",
      "Iteration 5526: Policy loss: -0.008601. Value loss: 0.010480. Entropy: 0.348626.\n",
      "episode: 3938   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 359     evaluation reward: 4.54\n",
      "episode: 3939   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 443     evaluation reward: 4.57\n",
      "Training network. lr: 0.000209. clip: 0.083422\n",
      "Iteration 5527: Policy loss: 0.015559. Value loss: 0.011364. Entropy: 0.369869.\n",
      "Iteration 5528: Policy loss: -0.001051. Value loss: 0.009314. Entropy: 0.366282.\n",
      "Iteration 5529: Policy loss: -0.008077. Value loss: 0.007830. Entropy: 0.355242.\n",
      "episode: 3940   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 456     evaluation reward: 4.55\n",
      "episode: 3941   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 395     evaluation reward: 4.53\n",
      "Training network. lr: 0.000209. clip: 0.083413\n",
      "Iteration 5530: Policy loss: 0.015350. Value loss: 0.009810. Entropy: 0.324845.\n",
      "Iteration 5531: Policy loss: 0.005575. Value loss: 0.007382. Entropy: 0.323995.\n",
      "Iteration 5532: Policy loss: -0.005568. Value loss: 0.005933. Entropy: 0.335105.\n",
      "episode: 3942   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 980     evaluation reward: 4.56\n",
      "episode: 3943   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 539     evaluation reward: 4.56\n",
      "Training network. lr: 0.000209. clip: 0.083404\n",
      "Iteration 5533: Policy loss: 0.003562. Value loss: 0.012474. Entropy: 0.385929.\n",
      "Iteration 5534: Policy loss: -0.007040. Value loss: 0.008863. Entropy: 0.398150.\n",
      "Iteration 5535: Policy loss: -0.014164. Value loss: 0.008343. Entropy: 0.389965.\n",
      "episode: 3944   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 602     evaluation reward: 4.53\n",
      "episode: 3945   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 413     evaluation reward: 4.52\n",
      "Training network. lr: 0.000208. clip: 0.083395\n",
      "Iteration 5536: Policy loss: 0.013209. Value loss: 0.010842. Entropy: 0.432816.\n",
      "Iteration 5537: Policy loss: -0.001652. Value loss: 0.009159. Entropy: 0.430186.\n",
      "Iteration 5538: Policy loss: -0.009215. Value loss: 0.008418. Entropy: 0.427197.\n",
      "episode: 3946   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 645     evaluation reward: 4.5\n",
      "Training network. lr: 0.000208. clip: 0.083386\n",
      "Iteration 5539: Policy loss: 0.017726. Value loss: 0.012996. Entropy: 0.564565.\n",
      "Iteration 5540: Policy loss: 0.007705. Value loss: 0.009685. Entropy: 0.530312.\n",
      "Iteration 5541: Policy loss: -0.003117. Value loss: 0.007511. Entropy: 0.537195.\n",
      "episode: 3947   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 530     evaluation reward: 4.53\n",
      "episode: 3948   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 338     evaluation reward: 4.53\n",
      "episode: 3949   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 369     evaluation reward: 4.5\n",
      "Training network. lr: 0.000208. clip: 0.083377\n",
      "Iteration 5542: Policy loss: 0.017260. Value loss: 0.014370. Entropy: 0.387015.\n",
      "Iteration 5543: Policy loss: 0.004289. Value loss: 0.011959. Entropy: 0.396078.\n",
      "Iteration 5544: Policy loss: 0.000472. Value loss: 0.010557. Entropy: 0.388199.\n",
      "episode: 3950   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 556     evaluation reward: 4.52\n",
      "Training network. lr: 0.000208. clip: 0.083368\n",
      "Iteration 5545: Policy loss: 0.012556. Value loss: 0.010612. Entropy: 0.333618.\n",
      "Iteration 5546: Policy loss: 0.004766. Value loss: 0.006752. Entropy: 0.324577.\n",
      "Iteration 5547: Policy loss: -0.001127. Value loss: 0.006271. Entropy: 0.334476.\n",
      "episode: 3951   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 741     evaluation reward: 4.54\n",
      "episode: 3952   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 684     evaluation reward: 4.56\n",
      "Training network. lr: 0.000208. clip: 0.083359\n",
      "Iteration 5548: Policy loss: 0.001664. Value loss: 0.011004. Entropy: 0.448219.\n",
      "Iteration 5549: Policy loss: -0.001442. Value loss: 0.008087. Entropy: 0.454819.\n",
      "Iteration 5550: Policy loss: -0.004515. Value loss: 0.005639. Entropy: 0.447469.\n",
      "episode: 3953   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 1033     evaluation reward: 4.6\n",
      "Training network. lr: 0.000208. clip: 0.083350\n",
      "Iteration 5551: Policy loss: 0.014788. Value loss: 0.007509. Entropy: 0.317942.\n",
      "Iteration 5552: Policy loss: 0.034143. Value loss: 0.006591. Entropy: 0.326353.\n",
      "Iteration 5553: Policy loss: -0.006523. Value loss: 0.005484. Entropy: 0.302853.\n",
      "episode: 3954   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 658     evaluation reward: 4.62\n",
      "episode: 3955   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 346     evaluation reward: 4.6\n",
      "Training network. lr: 0.000208. clip: 0.083341\n",
      "Iteration 5554: Policy loss: 0.008640. Value loss: 0.013405. Entropy: 0.335187.\n",
      "Iteration 5555: Policy loss: 0.005943. Value loss: 0.009725. Entropy: 0.328970.\n",
      "Iteration 5556: Policy loss: -0.006598. Value loss: 0.007762. Entropy: 0.332822.\n",
      "episode: 3956   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 588     evaluation reward: 4.59\n",
      "Training network. lr: 0.000208. clip: 0.083332\n",
      "Iteration 5557: Policy loss: 0.010296. Value loss: 0.003059. Entropy: 0.424618.\n",
      "Iteration 5558: Policy loss: 0.002480. Value loss: 0.001764. Entropy: 0.421943.\n",
      "Iteration 5559: Policy loss: -0.004617. Value loss: 0.001516. Entropy: 0.432540.\n",
      "episode: 3957   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 1555     evaluation reward: 4.55\n",
      "Training network. lr: 0.000208. clip: 0.083323\n",
      "Iteration 5560: Policy loss: 0.009193. Value loss: 0.004863. Entropy: 0.545148.\n",
      "Iteration 5561: Policy loss: 0.032737. Value loss: 0.003276. Entropy: 0.556638.\n",
      "Iteration 5562: Policy loss: 0.002486. Value loss: 0.003205. Entropy: 0.542000.\n",
      "Training network. lr: 0.000208. clip: 0.083314\n",
      "Iteration 5563: Policy loss: 0.011729. Value loss: 0.001117. Entropy: 0.319159.\n",
      "Iteration 5564: Policy loss: 0.008316. Value loss: 0.000810. Entropy: 0.368229.\n",
      "Iteration 5565: Policy loss: 0.000968. Value loss: 0.000699. Entropy: 0.356113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now time :  2018-12-26 15:16:41.857225\n",
      "episode: 3958   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 1771     evaluation reward: 4.52\n",
      "episode: 3959   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 451     evaluation reward: 4.5\n",
      "Training network. lr: 0.000208. clip: 0.083305\n",
      "Iteration 5566: Policy loss: 0.013765. Value loss: 0.008908. Entropy: 0.480697.\n",
      "Iteration 5567: Policy loss: 0.000172. Value loss: 0.006567. Entropy: 0.490342.\n",
      "Iteration 5568: Policy loss: -0.004252. Value loss: 0.005715. Entropy: 0.486397.\n",
      "Training network. lr: 0.000208. clip: 0.083296\n",
      "Iteration 5569: Policy loss: 0.003898. Value loss: 0.006267. Entropy: 0.579369.\n",
      "Iteration 5570: Policy loss: -0.004312. Value loss: 0.004766. Entropy: 0.647911.\n",
      "Iteration 5571: Policy loss: -0.014692. Value loss: 0.004049. Entropy: 0.631929.\n",
      "episode: 3960   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 1147     evaluation reward: 4.53\n",
      "episode: 3961   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 505     evaluation reward: 4.54\n",
      "episode: 3962   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 296     evaluation reward: 4.49\n",
      "Training network. lr: 0.000208. clip: 0.083287\n",
      "Iteration 5572: Policy loss: 0.001626. Value loss: 0.014240. Entropy: 0.393568.\n",
      "Iteration 5573: Policy loss: -0.004619. Value loss: 0.012392. Entropy: 0.389729.\n",
      "Iteration 5574: Policy loss: -0.014226. Value loss: 0.010842. Entropy: 0.385888.\n",
      "episode: 3963   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 372     evaluation reward: 4.49\n",
      "episode: 3964   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 488     evaluation reward: 4.51\n",
      "Training network. lr: 0.000208. clip: 0.083278\n",
      "Iteration 5575: Policy loss: 0.008561. Value loss: 0.012508. Entropy: 0.419649.\n",
      "Iteration 5576: Policy loss: -0.004116. Value loss: 0.010246. Entropy: 0.416439.\n",
      "Iteration 5577: Policy loss: -0.011937. Value loss: 0.008830. Entropy: 0.421661.\n",
      "episode: 3965   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 358     evaluation reward: 4.5\n",
      "episode: 3966   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 450     evaluation reward: 4.48\n",
      "Training network. lr: 0.000208. clip: 0.083269\n",
      "Iteration 5578: Policy loss: 0.012709. Value loss: 0.011967. Entropy: 0.471267.\n",
      "Iteration 5579: Policy loss: -0.004700. Value loss: 0.008823. Entropy: 0.485660.\n",
      "Iteration 5580: Policy loss: -0.015278. Value loss: 0.007761. Entropy: 0.484104.\n",
      "episode: 3967   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 1061     evaluation reward: 4.51\n",
      "episode: 3968   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 429     evaluation reward: 4.53\n",
      "Training network. lr: 0.000208. clip: 0.083260\n",
      "Iteration 5581: Policy loss: 0.012557. Value loss: 0.010531. Entropy: 0.286611.\n",
      "Iteration 5582: Policy loss: -0.002939. Value loss: 0.008114. Entropy: 0.298120.\n",
      "Iteration 5583: Policy loss: -0.011904. Value loss: 0.006556. Entropy: 0.371760.\n",
      "episode: 3969   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 994     evaluation reward: 4.55\n",
      "Training network. lr: 0.000208. clip: 0.083251\n",
      "Iteration 5584: Policy loss: 0.009189. Value loss: 0.009147. Entropy: 0.328511.\n",
      "Iteration 5585: Policy loss: 0.000715. Value loss: 0.007155. Entropy: 0.343109.\n",
      "Iteration 5586: Policy loss: -0.009789. Value loss: 0.006162. Entropy: 0.324349.\n",
      "episode: 3970   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 582     evaluation reward: 4.56\n",
      "Training network. lr: 0.000208. clip: 0.083242\n",
      "Iteration 5587: Policy loss: 0.016753. Value loss: 0.014990. Entropy: 0.363698.\n",
      "Iteration 5588: Policy loss: 0.000029. Value loss: 0.012842. Entropy: 0.371686.\n",
      "Iteration 5589: Policy loss: -0.004280. Value loss: 0.011103. Entropy: 0.372838.\n",
      "episode: 3971   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 498     evaluation reward: 4.58\n",
      "episode: 3972   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 4.57\n",
      "episode: 3973   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 407     evaluation reward: 4.58\n",
      "Training network. lr: 0.000208. clip: 0.083233\n",
      "Iteration 5590: Policy loss: 0.007937. Value loss: 0.016613. Entropy: 0.513797.\n",
      "Iteration 5591: Policy loss: -0.003857. Value loss: 0.013166. Entropy: 0.516346.\n",
      "Iteration 5592: Policy loss: -0.012925. Value loss: 0.011246. Entropy: 0.499546.\n",
      "episode: 3974   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 371     evaluation reward: 4.57\n",
      "episode: 3975   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 718     evaluation reward: 4.6\n",
      "Training network. lr: 0.000208. clip: 0.083224\n",
      "Iteration 5593: Policy loss: 0.012546. Value loss: 0.016464. Entropy: 0.591054.\n",
      "Iteration 5594: Policy loss: -0.003292. Value loss: 0.011763. Entropy: 0.580174.\n",
      "Iteration 5595: Policy loss: -0.017781. Value loss: 0.009906. Entropy: 0.589181.\n",
      "episode: 3976   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 497     evaluation reward: 4.62\n",
      "episode: 3977   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 461     evaluation reward: 4.64\n",
      "Training network. lr: 0.000208. clip: 0.083215\n",
      "Iteration 5596: Policy loss: 0.008138. Value loss: 0.010229. Entropy: 0.403678.\n",
      "Iteration 5597: Policy loss: 0.004404. Value loss: 0.008157. Entropy: 0.393856.\n",
      "Iteration 5598: Policy loss: -0.012518. Value loss: 0.006557. Entropy: 0.392335.\n",
      "episode: 3978   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 591     evaluation reward: 4.65\n",
      "episode: 3979   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 313     evaluation reward: 4.6\n",
      "Training network. lr: 0.000208. clip: 0.083206\n",
      "Iteration 5599: Policy loss: 0.006008. Value loss: 0.015045. Entropy: 0.449521.\n",
      "Iteration 5600: Policy loss: 0.000913. Value loss: 0.011251. Entropy: 0.462444.\n",
      "Iteration 5601: Policy loss: -0.009261. Value loss: 0.010837. Entropy: 0.459714.\n",
      "episode: 3980   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 398     evaluation reward: 4.6\n",
      "episode: 3981   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 374     evaluation reward: 4.6\n",
      "episode: 3982   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 544     evaluation reward: 4.6\n",
      "Training network. lr: 0.000208. clip: 0.083197\n",
      "Iteration 5602: Policy loss: 0.016300. Value loss: 0.013464. Entropy: 0.409366.\n",
      "Iteration 5603: Policy loss: 0.007337. Value loss: 0.011643. Entropy: 0.414584.\n",
      "Iteration 5604: Policy loss: -0.007233. Value loss: 0.010245. Entropy: 0.407245.\n",
      "episode: 3983   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 526     evaluation reward: 4.63\n",
      "episode: 3984   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 478     evaluation reward: 4.62\n",
      "Training network. lr: 0.000208. clip: 0.083188\n",
      "Iteration 5605: Policy loss: 0.004000. Value loss: 0.014728. Entropy: 0.420689.\n",
      "Iteration 5606: Policy loss: -0.001442. Value loss: 0.011367. Entropy: 0.415227.\n",
      "Iteration 5607: Policy loss: -0.012231. Value loss: 0.009269. Entropy: 0.417838.\n",
      "episode: 3985   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 603     evaluation reward: 4.65\n",
      "episode: 3986   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 304     evaluation reward: 4.64\n",
      "Training network. lr: 0.000208. clip: 0.083179\n",
      "Iteration 5608: Policy loss: 0.007412. Value loss: 0.012490. Entropy: 0.482930.\n",
      "Iteration 5609: Policy loss: -0.007344. Value loss: 0.009882. Entropy: 0.473811.\n",
      "Iteration 5610: Policy loss: -0.011763. Value loss: 0.008737. Entropy: 0.476806.\n",
      "episode: 3987   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 349     evaluation reward: 4.64\n",
      "episode: 3988   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 645     evaluation reward: 4.66\n",
      "Training network. lr: 0.000208. clip: 0.083170\n",
      "Iteration 5611: Policy loss: 0.006146. Value loss: 0.010751. Entropy: 0.386429.\n",
      "Iteration 5612: Policy loss: 0.004994. Value loss: 0.007525. Entropy: 0.388752.\n",
      "Iteration 5613: Policy loss: -0.001597. Value loss: 0.006449. Entropy: 0.391880.\n",
      "episode: 3989   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 434     evaluation reward: 4.66\n",
      "episode: 3990   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 493     evaluation reward: 4.69\n",
      "Training network. lr: 0.000208. clip: 0.083161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5614: Policy loss: 0.017251. Value loss: 0.011687. Entropy: 0.415843.\n",
      "Iteration 5615: Policy loss: 0.000856. Value loss: 0.009365. Entropy: 0.408438.\n",
      "Iteration 5616: Policy loss: -0.003634. Value loss: 0.008389. Entropy: 0.412461.\n",
      "episode: 3991   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 587     evaluation reward: 4.7\n",
      "episode: 3992   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 577     evaluation reward: 4.71\n",
      "Training network. lr: 0.000208. clip: 0.083152\n",
      "Iteration 5617: Policy loss: 0.008751. Value loss: 0.010506. Entropy: 0.418470.\n",
      "Iteration 5618: Policy loss: 0.011746. Value loss: 0.009052. Entropy: 0.419489.\n",
      "Iteration 5619: Policy loss: -0.002245. Value loss: 0.008033. Entropy: 0.419827.\n",
      "episode: 3993   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 370     evaluation reward: 4.71\n",
      "episode: 3994   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 386     evaluation reward: 4.72\n",
      "Training network. lr: 0.000208. clip: 0.083143\n",
      "Iteration 5620: Policy loss: 0.005452. Value loss: 0.012055. Entropy: 0.384771.\n",
      "Iteration 5621: Policy loss: -0.006374. Value loss: 0.009420. Entropy: 0.380979.\n",
      "Iteration 5622: Policy loss: -0.013082. Value loss: 0.008545. Entropy: 0.382452.\n",
      "episode: 3995   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 519     evaluation reward: 4.74\n",
      "episode: 3996   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 598     evaluation reward: 4.75\n",
      "episode: 3997   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 284     evaluation reward: 4.7\n",
      "Training network. lr: 0.000208. clip: 0.083134\n",
      "Iteration 5623: Policy loss: 0.015837. Value loss: 0.009571. Entropy: 0.364257.\n",
      "Iteration 5624: Policy loss: 0.004431. Value loss: 0.006157. Entropy: 0.373225.\n",
      "Iteration 5625: Policy loss: -0.002620. Value loss: 0.005078. Entropy: 0.360699.\n",
      "episode: 3998   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 506     evaluation reward: 4.72\n",
      "episode: 3999   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 410     evaluation reward: 4.71\n",
      "Training network. lr: 0.000208. clip: 0.083125\n",
      "Iteration 5626: Policy loss: 0.011358. Value loss: 0.015881. Entropy: 0.479981.\n",
      "Iteration 5627: Policy loss: -0.000329. Value loss: 0.012724. Entropy: 0.484656.\n",
      "Iteration 5628: Policy loss: -0.014159. Value loss: 0.010549. Entropy: 0.477944.\n",
      "episode: 4000   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 428     evaluation reward: 4.72\n",
      "episode: 4001   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 659     evaluation reward: 4.76\n",
      "Training network. lr: 0.000208. clip: 0.083116\n",
      "Iteration 5629: Policy loss: 0.023149. Value loss: 0.018137. Entropy: 0.328690.\n",
      "Iteration 5630: Policy loss: 0.005255. Value loss: 0.010421. Entropy: 0.311017.\n",
      "Iteration 5631: Policy loss: -0.004751. Value loss: 0.009793. Entropy: 0.310959.\n",
      "episode: 4002   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 513     evaluation reward: 4.78\n",
      "episode: 4003   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 571     evaluation reward: 4.81\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5632: Policy loss: 0.007295. Value loss: 0.015841. Entropy: 0.374515.\n",
      "Iteration 5633: Policy loss: 0.002922. Value loss: 0.013366. Entropy: 0.369143.\n",
      "Iteration 5634: Policy loss: -0.007888. Value loss: 0.011649. Entropy: 0.367607.\n",
      "episode: 4004   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 447     evaluation reward: 4.82\n",
      "episode: 4005   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 360     evaluation reward: 4.79\n",
      "Training network. lr: 0.000208. clip: 0.083098\n",
      "Iteration 5635: Policy loss: 0.019741. Value loss: 0.017296. Entropy: 0.485846.\n",
      "Iteration 5636: Policy loss: 0.008798. Value loss: 0.014073. Entropy: 0.473954.\n",
      "Iteration 5637: Policy loss: -0.001010. Value loss: 0.012145. Entropy: 0.473930.\n",
      "episode: 4006   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 489     evaluation reward: 4.79\n",
      "episode: 4007   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 487     evaluation reward: 4.81\n",
      "episode: 4008   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 318     evaluation reward: 4.8\n",
      "Training network. lr: 0.000208. clip: 0.083089\n",
      "Iteration 5638: Policy loss: 0.018120. Value loss: 0.015865. Entropy: 0.421915.\n",
      "Iteration 5639: Policy loss: -0.004083. Value loss: 0.012492. Entropy: 0.417062.\n",
      "Iteration 5640: Policy loss: -0.013762. Value loss: 0.010486. Entropy: 0.414508.\n",
      "episode: 4009   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 525     evaluation reward: 4.83\n",
      "Training network. lr: 0.000208. clip: 0.083080\n",
      "Iteration 5641: Policy loss: 0.012748. Value loss: 0.012329. Entropy: 0.369326.\n",
      "Iteration 5642: Policy loss: 0.001885. Value loss: 0.009382. Entropy: 0.355609.\n",
      "Iteration 5643: Policy loss: -0.005906. Value loss: 0.008081. Entropy: 0.356573.\n",
      "episode: 4010   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 530     evaluation reward: 4.84\n",
      "episode: 4011   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 557     evaluation reward: 4.86\n",
      "Training network. lr: 0.000208. clip: 0.083071\n",
      "Iteration 5644: Policy loss: 0.012195. Value loss: 0.012909. Entropy: 0.360516.\n",
      "Iteration 5645: Policy loss: 0.006729. Value loss: 0.009700. Entropy: 0.368360.\n",
      "Iteration 5646: Policy loss: -0.005570. Value loss: 0.009245. Entropy: 0.359822.\n",
      "episode: 4012   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 609     evaluation reward: 4.9\n",
      "episode: 4013   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 4.91\n",
      "Training network. lr: 0.000208. clip: 0.083062\n",
      "Iteration 5647: Policy loss: 0.005379. Value loss: 0.015163. Entropy: 0.452101.\n",
      "Iteration 5648: Policy loss: -0.008573. Value loss: 0.011734. Entropy: 0.437967.\n",
      "Iteration 5649: Policy loss: -0.017338. Value loss: 0.010148. Entropy: 0.431425.\n",
      "episode: 4014   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 454     evaluation reward: 4.91\n",
      "episode: 4015   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 522     evaluation reward: 4.93\n",
      "episode: 4016   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 318     evaluation reward: 4.92\n",
      "Training network. lr: 0.000208. clip: 0.083053\n",
      "Iteration 5650: Policy loss: 0.011228. Value loss: 0.013391. Entropy: 0.393275.\n",
      "Iteration 5651: Policy loss: -0.002230. Value loss: 0.009888. Entropy: 0.387399.\n",
      "Iteration 5652: Policy loss: -0.008333. Value loss: 0.008535. Entropy: 0.371219.\n",
      "episode: 4017   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 621     evaluation reward: 4.96\n",
      "episode: 4018   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 423     evaluation reward: 4.95\n",
      "Training network. lr: 0.000208. clip: 0.083044\n",
      "Iteration 5653: Policy loss: 0.004162. Value loss: 0.016443. Entropy: 0.433148.\n",
      "Iteration 5654: Policy loss: -0.005879. Value loss: 0.012644. Entropy: 0.420610.\n",
      "Iteration 5655: Policy loss: -0.016709. Value loss: 0.010708. Entropy: 0.418848.\n",
      "episode: 4019   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 592     evaluation reward: 4.97\n",
      "Training network. lr: 0.000208. clip: 0.083035\n",
      "Iteration 5656: Policy loss: 0.012331. Value loss: 0.011825. Entropy: 0.442663.\n",
      "Iteration 5657: Policy loss: -0.000743. Value loss: 0.010107. Entropy: 0.435818.\n",
      "Iteration 5658: Policy loss: -0.006708. Value loss: 0.009019. Entropy: 0.432563.\n",
      "episode: 4020   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 545     evaluation reward: 4.99\n",
      "episode: 4021   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 522     evaluation reward: 4.99\n",
      "episode: 4022   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 465     evaluation reward: 4.98\n",
      "Training network. lr: 0.000208. clip: 0.083026\n",
      "Iteration 5659: Policy loss: 0.014609. Value loss: 0.016428. Entropy: 0.347721.\n",
      "Iteration 5660: Policy loss: 0.009581. Value loss: 0.015149. Entropy: 0.334813.\n",
      "Iteration 5661: Policy loss: -0.005546. Value loss: 0.011347. Entropy: 0.346093.\n",
      "episode: 4023   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 518     evaluation reward: 5.0\n",
      "episode: 4024   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 492     evaluation reward: 5.06\n",
      "Training network. lr: 0.000208. clip: 0.083017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5662: Policy loss: 0.013572. Value loss: 0.037569. Entropy: 0.331766.\n",
      "Iteration 5663: Policy loss: -0.003841. Value loss: 0.032016. Entropy: 0.336421.\n",
      "Iteration 5664: Policy loss: -0.007925. Value loss: 0.028203. Entropy: 0.338477.\n",
      "episode: 4025   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 456     evaluation reward: 5.06\n",
      "episode: 4026   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 510     evaluation reward: 5.07\n",
      "Training network. lr: 0.000208. clip: 0.083008\n",
      "Iteration 5665: Policy loss: 0.004874. Value loss: 0.012460. Entropy: 0.297048.\n",
      "Iteration 5666: Policy loss: -0.004646. Value loss: 0.009893. Entropy: 0.290692.\n",
      "Iteration 5667: Policy loss: -0.008511. Value loss: 0.008506. Entropy: 0.287838.\n",
      "episode: 4027   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 528     evaluation reward: 5.08\n",
      "episode: 4028   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 353     evaluation reward: 5.06\n",
      "Training network. lr: 0.000207. clip: 0.082999\n",
      "Iteration 5668: Policy loss: 0.054861. Value loss: 0.022322. Entropy: 0.441025.\n",
      "Iteration 5669: Policy loss: 0.016520. Value loss: 0.009462. Entropy: 0.439330.\n",
      "Iteration 5670: Policy loss: -0.001288. Value loss: 0.008494. Entropy: 0.422319.\n",
      "episode: 4029   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 653     evaluation reward: 5.09\n",
      "episode: 4030   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 506     evaluation reward: 5.12\n",
      "Training network. lr: 0.000207. clip: 0.082990\n",
      "Iteration 5671: Policy loss: 0.012124. Value loss: 0.037592. Entropy: 0.313603.\n",
      "Iteration 5672: Policy loss: 0.002883. Value loss: 0.030975. Entropy: 0.318060.\n",
      "Iteration 5673: Policy loss: -0.004790. Value loss: 0.025892. Entropy: 0.313695.\n",
      "episode: 4031   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 402     evaluation reward: 5.12\n",
      "Training network. lr: 0.000207. clip: 0.082981\n",
      "Iteration 5674: Policy loss: 0.013973. Value loss: 0.009117. Entropy: 0.383082.\n",
      "Iteration 5675: Policy loss: -0.002654. Value loss: 0.007440. Entropy: 0.378606.\n",
      "Iteration 5676: Policy loss: -0.002972. Value loss: 0.006804. Entropy: 0.376821.\n",
      "episode: 4032   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 765     evaluation reward: 5.15\n",
      "episode: 4033   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 468     evaluation reward: 5.15\n",
      "episode: 4034   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 397     evaluation reward: 5.12\n",
      "Training network. lr: 0.000207. clip: 0.082972\n",
      "Iteration 5677: Policy loss: 0.012279. Value loss: 0.013551. Entropy: 0.430625.\n",
      "Iteration 5678: Policy loss: -0.001380. Value loss: 0.010938. Entropy: 0.426840.\n",
      "Iteration 5679: Policy loss: -0.004211. Value loss: 0.010499. Entropy: 0.433453.\n",
      "episode: 4035   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 474     evaluation reward: 5.14\n",
      "episode: 4036   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 456     evaluation reward: 5.13\n",
      "Training network. lr: 0.000207. clip: 0.082963\n",
      "Iteration 5680: Policy loss: 0.010057. Value loss: 0.012219. Entropy: 0.382558.\n",
      "Iteration 5681: Policy loss: 0.007675. Value loss: 0.009323. Entropy: 0.375512.\n",
      "Iteration 5682: Policy loss: -0.002847. Value loss: 0.008295. Entropy: 0.378066.\n",
      "episode: 4037   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 443     evaluation reward: 5.13\n",
      "episode: 4038   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 493     evaluation reward: 5.14\n",
      "Training network. lr: 0.000207. clip: 0.082954\n",
      "Iteration 5683: Policy loss: 0.012823. Value loss: 0.008960. Entropy: 0.363652.\n",
      "Iteration 5684: Policy loss: -0.001208. Value loss: 0.007469. Entropy: 0.349259.\n",
      "Iteration 5685: Policy loss: -0.006463. Value loss: 0.006704. Entropy: 0.343704.\n",
      "episode: 4039   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 353     evaluation reward: 5.13\n",
      "episode: 4040   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 436     evaluation reward: 5.13\n",
      "Training network. lr: 0.000207. clip: 0.082945\n",
      "Iteration 5686: Policy loss: 0.015911. Value loss: 0.012160. Entropy: 0.355937.\n",
      "Iteration 5687: Policy loss: 0.030715. Value loss: 0.006882. Entropy: 0.343190.\n",
      "Iteration 5688: Policy loss: 0.012585. Value loss: 0.006305. Entropy: 0.339856.\n",
      "episode: 4041   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 743     evaluation reward: 5.16\n",
      "Training network. lr: 0.000207. clip: 0.082936\n",
      "Iteration 5689: Policy loss: 0.011586. Value loss: 0.007415. Entropy: 0.491357.\n",
      "Iteration 5690: Policy loss: -0.006340. Value loss: 0.002860. Entropy: 0.514728.\n",
      "Iteration 5691: Policy loss: 0.019488. Value loss: 0.002235. Entropy: 0.563475.\n",
      "episode: 4042   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 1236     evaluation reward: 5.15\n",
      "episode: 4043   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 535     evaluation reward: 5.16\n",
      "Training network. lr: 0.000207. clip: 0.082927\n",
      "Iteration 5692: Policy loss: 0.013892. Value loss: 0.010526. Entropy: 0.366232.\n",
      "Iteration 5693: Policy loss: -0.002379. Value loss: 0.008169. Entropy: 0.352497.\n",
      "Iteration 5694: Policy loss: -0.009331. Value loss: 0.006674. Entropy: 0.349574.\n",
      "episode: 4044   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 545     evaluation reward: 5.18\n",
      "episode: 4045   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 295     evaluation reward: 5.17\n",
      "Training network. lr: 0.000207. clip: 0.082918\n",
      "Iteration 5695: Policy loss: 0.015529. Value loss: 0.013923. Entropy: 0.340587.\n",
      "Iteration 5696: Policy loss: 0.001282. Value loss: 0.011710. Entropy: 0.335990.\n",
      "Iteration 5697: Policy loss: -0.008849. Value loss: 0.010065. Entropy: 0.326445.\n",
      "episode: 4046   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 561     evaluation reward: 5.2\n",
      "episode: 4047   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 379     evaluation reward: 5.18\n",
      "Training network. lr: 0.000207. clip: 0.082909\n",
      "Iteration 5698: Policy loss: 0.010078. Value loss: 0.013748. Entropy: 0.320175.\n",
      "Iteration 5699: Policy loss: 0.001967. Value loss: 0.010923. Entropy: 0.315215.\n",
      "Iteration 5700: Policy loss: -0.005382. Value loss: 0.009439. Entropy: 0.325259.\n",
      "episode: 4048   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 492     evaluation reward: 5.22\n",
      "episode: 4049   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 442     evaluation reward: 5.23\n",
      "Training network. lr: 0.000207. clip: 0.082900\n",
      "Iteration 5701: Policy loss: 0.007005. Value loss: 0.013510. Entropy: 0.331328.\n",
      "Iteration 5702: Policy loss: -0.002560. Value loss: 0.011324. Entropy: 0.330828.\n",
      "Iteration 5703: Policy loss: -0.006066. Value loss: 0.010018. Entropy: 0.329442.\n",
      "episode: 4050   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 482     evaluation reward: 5.23\n",
      "episode: 4051   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 383     evaluation reward: 5.19\n",
      "episode: 4052   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 395     evaluation reward: 5.17\n",
      "Training network. lr: 0.000207. clip: 0.082891\n",
      "Iteration 5704: Policy loss: 0.007768. Value loss: 0.048928. Entropy: 0.427170.\n",
      "Iteration 5705: Policy loss: -0.004203. Value loss: 0.037514. Entropy: 0.434483.\n",
      "Iteration 5706: Policy loss: -0.008931. Value loss: 0.031950. Entropy: 0.418262.\n",
      "episode: 4053   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 520     evaluation reward: 5.18\n",
      "episode: 4054   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 474     evaluation reward: 5.17\n",
      "Training network. lr: 0.000207. clip: 0.082882\n",
      "Iteration 5707: Policy loss: 0.002429. Value loss: 0.019417. Entropy: 0.259011.\n",
      "Iteration 5708: Policy loss: -0.000472. Value loss: 0.015136. Entropy: 0.250632.\n",
      "Iteration 5709: Policy loss: -0.002862. Value loss: 0.011600. Entropy: 0.248124.\n",
      "episode: 4055   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 593     evaluation reward: 5.22\n",
      "episode: 4056   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 423     evaluation reward: 5.23\n",
      "Training network. lr: 0.000207. clip: 0.082873\n",
      "Iteration 5710: Policy loss: 0.007161. Value loss: 0.011780. Entropy: 0.367491.\n",
      "Iteration 5711: Policy loss: -0.001933. Value loss: 0.009360. Entropy: 0.352637.\n",
      "Iteration 5712: Policy loss: -0.010585. Value loss: 0.007657. Entropy: 0.361596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4057   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 371     evaluation reward: 5.24\n",
      "now time :  2018-12-26 15:22:09.257965\n",
      "episode: 4058   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 444     evaluation reward: 5.26\n",
      "Training network. lr: 0.000207. clip: 0.082864\n",
      "Iteration 5713: Policy loss: 0.015119. Value loss: 0.014157. Entropy: 0.436013.\n",
      "Iteration 5714: Policy loss: 0.003979. Value loss: 0.011651. Entropy: 0.422555.\n",
      "Iteration 5715: Policy loss: -0.004416. Value loss: 0.009546. Entropy: 0.434142.\n",
      "episode: 4059   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 477     evaluation reward: 5.28\n",
      "episode: 4060   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 552     evaluation reward: 5.27\n",
      "Training network. lr: 0.000207. clip: 0.082855\n",
      "Iteration 5716: Policy loss: 0.008213. Value loss: 0.013208. Entropy: 0.332598.\n",
      "Iteration 5717: Policy loss: 0.008593. Value loss: 0.010444. Entropy: 0.335527.\n",
      "Iteration 5718: Policy loss: 0.008198. Value loss: 0.013882. Entropy: 0.338982.\n",
      "episode: 4061   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 518     evaluation reward: 5.28\n",
      "episode: 4062   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 348     evaluation reward: 5.29\n",
      "episode: 4063   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 442     evaluation reward: 5.3\n",
      "Training network. lr: 0.000207. clip: 0.082846\n",
      "Iteration 5719: Policy loss: 0.000982. Value loss: 0.011535. Entropy: 0.390217.\n",
      "Iteration 5720: Policy loss: -0.005644. Value loss: 0.008578. Entropy: 0.385388.\n",
      "Iteration 5721: Policy loss: -0.007588. Value loss: 0.007178. Entropy: 0.379406.\n",
      "episode: 4064   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 608     evaluation reward: 5.32\n",
      "episode: 4065   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 459     evaluation reward: 5.33\n",
      "Training network. lr: 0.000207. clip: 0.082837\n",
      "Iteration 5722: Policy loss: 0.001614. Value loss: 0.010960. Entropy: 0.387811.\n",
      "Iteration 5723: Policy loss: -0.010579. Value loss: 0.007755. Entropy: 0.375301.\n",
      "Iteration 5724: Policy loss: -0.012259. Value loss: 0.007080. Entropy: 0.385698.\n",
      "episode: 4066   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 396     evaluation reward: 5.35\n",
      "episode: 4067   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 570     evaluation reward: 5.35\n",
      "Training network. lr: 0.000207. clip: 0.082828\n",
      "Iteration 5725: Policy loss: 0.011095. Value loss: 0.008958. Entropy: 0.456590.\n",
      "Iteration 5726: Policy loss: -0.002797. Value loss: 0.007018. Entropy: 0.441292.\n",
      "Iteration 5727: Policy loss: -0.013365. Value loss: 0.006228. Entropy: 0.444461.\n",
      "episode: 4068   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 322     evaluation reward: 5.33\n",
      "episode: 4069   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 382     evaluation reward: 5.29\n",
      "Training network. lr: 0.000207. clip: 0.082819\n",
      "Iteration 5728: Policy loss: 0.008132. Value loss: 0.011894. Entropy: 0.342440.\n",
      "Iteration 5729: Policy loss: 0.002729. Value loss: 0.006180. Entropy: 0.396756.\n",
      "Iteration 5730: Policy loss: -0.005571. Value loss: 0.005629. Entropy: 0.370600.\n",
      "Training network. lr: 0.000207. clip: 0.082810\n",
      "Iteration 5731: Policy loss: 0.013409. Value loss: 0.004609. Entropy: 0.166925.\n",
      "Iteration 5732: Policy loss: 0.021147. Value loss: 0.002632. Entropy: 0.259156.\n",
      "Iteration 5733: Policy loss: 0.003773. Value loss: 0.002296. Entropy: 0.159730.\n",
      "episode: 4070   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 1544     evaluation reward: 5.25\n",
      "episode: 4071   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 493     evaluation reward: 5.29\n",
      "episode: 4072   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 5.31\n",
      "Training network. lr: 0.000207. clip: 0.082801\n",
      "Iteration 5734: Policy loss: 0.003713. Value loss: 0.039060. Entropy: 0.403758.\n",
      "Iteration 5735: Policy loss: -0.000798. Value loss: 0.030983. Entropy: 0.399249.\n",
      "Iteration 5736: Policy loss: -0.005791. Value loss: 0.026153. Entropy: 0.393361.\n",
      "episode: 4073   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 414     evaluation reward: 5.32\n",
      "episode: 4074   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 5.33\n",
      "Training network. lr: 0.000207. clip: 0.082792\n",
      "Iteration 5737: Policy loss: 0.013895. Value loss: 0.012446. Entropy: 0.363861.\n",
      "Iteration 5738: Policy loss: -0.003232. Value loss: 0.009827. Entropy: 0.365101.\n",
      "Iteration 5739: Policy loss: -0.005500. Value loss: 0.008016. Entropy: 0.360792.\n",
      "episode: 4075   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 413     evaluation reward: 5.31\n",
      "episode: 4076   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 546     evaluation reward: 5.33\n",
      "Training network. lr: 0.000207. clip: 0.082783\n",
      "Iteration 5740: Policy loss: 0.010171. Value loss: 0.013084. Entropy: 0.298420.\n",
      "Iteration 5741: Policy loss: 0.005470. Value loss: 0.011089. Entropy: 0.297173.\n",
      "Iteration 5742: Policy loss: 0.003278. Value loss: 0.009480. Entropy: 0.305521.\n",
      "episode: 4077   score: 9.0   memory length: 1024   epsilon: 1.0    steps: 485     evaluation reward: 5.36\n",
      "episode: 4078   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 520     evaluation reward: 5.35\n",
      "Training network. lr: 0.000207. clip: 0.082774\n",
      "Iteration 5743: Policy loss: 0.009225. Value loss: 0.034631. Entropy: 0.376796.\n",
      "Iteration 5744: Policy loss: 0.001647. Value loss: 0.030406. Entropy: 0.357127.\n",
      "Iteration 5745: Policy loss: -0.003358. Value loss: 0.027302. Entropy: 0.368880.\n",
      "episode: 4079   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 497     evaluation reward: 5.38\n",
      "episode: 4080   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 487     evaluation reward: 5.4\n",
      "Training network. lr: 0.000207. clip: 0.082765\n",
      "Iteration 5746: Policy loss: 0.010503. Value loss: 0.014656. Entropy: 0.340601.\n",
      "Iteration 5747: Policy loss: 0.002786. Value loss: 0.012873. Entropy: 0.348185.\n",
      "Iteration 5748: Policy loss: -0.010038. Value loss: 0.010719. Entropy: 0.353143.\n",
      "episode: 4081   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 424     evaluation reward: 5.41\n",
      "episode: 4082   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 303     evaluation reward: 5.38\n",
      "episode: 4083   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 554     evaluation reward: 5.38\n",
      "Training network. lr: 0.000207. clip: 0.082756\n",
      "Iteration 5749: Policy loss: 0.005156. Value loss: 0.015203. Entropy: 0.355687.\n",
      "Iteration 5750: Policy loss: -0.002629. Value loss: 0.011427. Entropy: 0.345213.\n",
      "Iteration 5751: Policy loss: -0.005299. Value loss: 0.009038. Entropy: 0.342818.\n",
      "episode: 4084   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 583     evaluation reward: 5.41\n",
      "episode: 4085   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 460     evaluation reward: 5.4\n",
      "Training network. lr: 0.000207. clip: 0.082747\n",
      "Iteration 5752: Policy loss: 0.008625. Value loss: 0.015327. Entropy: 0.416005.\n",
      "Iteration 5753: Policy loss: -0.002145. Value loss: 0.010723. Entropy: 0.411425.\n",
      "Iteration 5754: Policy loss: -0.012511. Value loss: 0.009065. Entropy: 0.406590.\n",
      "episode: 4086   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 488     evaluation reward: 5.42\n",
      "episode: 4087   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 560     evaluation reward: 5.46\n",
      "Training network. lr: 0.000207. clip: 0.082738\n",
      "Iteration 5755: Policy loss: 0.020287. Value loss: 0.015759. Entropy: 0.347887.\n",
      "Iteration 5756: Policy loss: 0.001908. Value loss: 0.013128. Entropy: 0.360449.\n",
      "Iteration 5757: Policy loss: -0.007390. Value loss: 0.011874. Entropy: 0.354864.\n",
      "episode: 4088   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 411     evaluation reward: 5.43\n",
      "episode: 4089   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 350     evaluation reward: 5.42\n",
      "Training network. lr: 0.000207. clip: 0.082729\n",
      "Iteration 5758: Policy loss: 0.011794. Value loss: 0.010757. Entropy: 0.346312.\n",
      "Iteration 5759: Policy loss: -0.000343. Value loss: 0.008277. Entropy: 0.337094.\n",
      "Iteration 5760: Policy loss: -0.003541. Value loss: 0.007816. Entropy: 0.344950.\n",
      "episode: 4090   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 435     evaluation reward: 5.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4091   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 542     evaluation reward: 5.4\n",
      "Training network. lr: 0.000207. clip: 0.082720\n",
      "Iteration 5761: Policy loss: 0.007146. Value loss: 0.014819. Entropy: 0.341410.\n",
      "Iteration 5762: Policy loss: -0.002510. Value loss: 0.011443. Entropy: 0.342122.\n",
      "Iteration 5763: Policy loss: -0.006223. Value loss: 0.009819. Entropy: 0.341323.\n",
      "episode: 4092   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 622     evaluation reward: 5.41\n",
      "episode: 4093   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 433     evaluation reward: 5.42\n",
      "episode: 4094   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 352     evaluation reward: 5.41\n",
      "Training network. lr: 0.000207. clip: 0.082711\n",
      "Iteration 5764: Policy loss: 0.013070. Value loss: 0.012969. Entropy: 0.383564.\n",
      "Iteration 5765: Policy loss: 0.010529. Value loss: 0.010149. Entropy: 0.382438.\n",
      "Iteration 5766: Policy loss: -0.005056. Value loss: 0.008626. Entropy: 0.387555.\n",
      "episode: 4095   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 499     evaluation reward: 5.41\n",
      "episode: 4096   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 468     evaluation reward: 5.42\n",
      "Training network. lr: 0.000207. clip: 0.082702\n",
      "Iteration 5767: Policy loss: 0.002855. Value loss: 0.011707. Entropy: 0.380850.\n",
      "Iteration 5768: Policy loss: -0.001240. Value loss: 0.009539. Entropy: 0.387355.\n",
      "Iteration 5769: Policy loss: -0.009469. Value loss: 0.008697. Entropy: 0.381118.\n",
      "episode: 4097   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 457     evaluation reward: 5.45\n",
      "episode: 4098   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 646     evaluation reward: 5.47\n",
      "Training network. lr: 0.000207. clip: 0.082693\n",
      "Iteration 5770: Policy loss: 0.000963. Value loss: 0.014014. Entropy: 0.350520.\n",
      "Iteration 5771: Policy loss: -0.007960. Value loss: 0.009949. Entropy: 0.350782.\n",
      "Iteration 5772: Policy loss: -0.015671. Value loss: 0.008783. Entropy: 0.360400.\n",
      "episode: 4099   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 461     evaluation reward: 5.48\n",
      "episode: 4100   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 332     evaluation reward: 5.46\n",
      "Training network. lr: 0.000207. clip: 0.082684\n",
      "Iteration 5773: Policy loss: 0.000901. Value loss: 0.012368. Entropy: 0.309667.\n",
      "Iteration 5774: Policy loss: -0.008425. Value loss: 0.007700. Entropy: 0.304505.\n",
      "Iteration 5775: Policy loss: -0.010851. Value loss: 0.006315. Entropy: 0.284205.\n",
      "episode: 4101   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 467     evaluation reward: 5.43\n",
      "episode: 4102   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 346     evaluation reward: 5.41\n",
      "episode: 4103   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 319     evaluation reward: 5.37\n",
      "Training network. lr: 0.000207. clip: 0.082675\n",
      "Iteration 5776: Policy loss: 0.005483. Value loss: 0.010579. Entropy: 0.334683.\n",
      "Iteration 5777: Policy loss: 0.000345. Value loss: 0.009807. Entropy: 0.325100.\n",
      "Iteration 5778: Policy loss: -0.005495. Value loss: 0.007448. Entropy: 0.326076.\n",
      "episode: 4104   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 414     evaluation reward: 5.37\n",
      "episode: 4105   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 357     evaluation reward: 5.36\n",
      "Training network. lr: 0.000207. clip: 0.082666\n",
      "Iteration 5779: Policy loss: 0.011675. Value loss: 0.012293. Entropy: 0.329599.\n",
      "Iteration 5780: Policy loss: -0.003051. Value loss: 0.009176. Entropy: 0.324264.\n",
      "Iteration 5781: Policy loss: -0.013372. Value loss: 0.007676. Entropy: 0.324382.\n",
      "episode: 4106   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 487     evaluation reward: 5.36\n",
      "episode: 4107   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 415     evaluation reward: 5.35\n",
      "episode: 4108   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 475     evaluation reward: 5.37\n",
      "Training network. lr: 0.000207. clip: 0.082657\n",
      "Iteration 5782: Policy loss: 0.012439. Value loss: 0.014934. Entropy: 0.400807.\n",
      "Iteration 5783: Policy loss: -0.002302. Value loss: 0.011717. Entropy: 0.405877.\n",
      "Iteration 5784: Policy loss: -0.007125. Value loss: 0.009875. Entropy: 0.408153.\n",
      "episode: 4109   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 466     evaluation reward: 5.35\n",
      "episode: 4110   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 469     evaluation reward: 5.34\n",
      "Training network. lr: 0.000207. clip: 0.082648\n",
      "Iteration 5785: Policy loss: 0.012161. Value loss: 0.015686. Entropy: 0.337974.\n",
      "Iteration 5786: Policy loss: -0.000408. Value loss: 0.011421. Entropy: 0.338218.\n",
      "Iteration 5787: Policy loss: -0.013085. Value loss: 0.010281. Entropy: 0.336808.\n",
      "episode: 4111   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 376     evaluation reward: 5.31\n",
      "episode: 4112   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 427     evaluation reward: 5.28\n",
      "Training network. lr: 0.000207. clip: 0.082639\n",
      "Iteration 5788: Policy loss: 0.003068. Value loss: 0.023403. Entropy: 0.396891.\n",
      "Iteration 5789: Policy loss: -0.008110. Value loss: 0.017273. Entropy: 0.375238.\n",
      "Iteration 5790: Policy loss: -0.010527. Value loss: 0.014314. Entropy: 0.368914.\n",
      "episode: 4113   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 437     evaluation reward: 5.26\n",
      "episode: 4114   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 431     evaluation reward: 5.25\n",
      "Training network. lr: 0.000207. clip: 0.082630\n",
      "Iteration 5791: Policy loss: 0.012236. Value loss: 0.011966. Entropy: 0.301852.\n",
      "Iteration 5792: Policy loss: 0.005078. Value loss: 0.008897. Entropy: 0.305845.\n",
      "Iteration 5793: Policy loss: 0.000978. Value loss: 0.007913. Entropy: 0.307437.\n",
      "episode: 4115   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 592     evaluation reward: 5.25\n",
      "episode: 4116   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 429     evaluation reward: 5.27\n",
      "episode: 4117   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 486     evaluation reward: 5.26\n",
      "Training network. lr: 0.000207. clip: 0.082621\n",
      "Iteration 5794: Policy loss: 0.002391. Value loss: 0.018478. Entropy: 0.399313.\n",
      "Iteration 5795: Policy loss: -0.004927. Value loss: 0.011676. Entropy: 0.407012.\n",
      "Iteration 5796: Policy loss: -0.015597. Value loss: 0.009928. Entropy: 0.400391.\n",
      "episode: 4118   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 479     evaluation reward: 5.27\n",
      "Training network. lr: 0.000207. clip: 0.082612\n",
      "Iteration 5797: Policy loss: 0.009745. Value loss: 0.010315. Entropy: 0.317373.\n",
      "Iteration 5798: Policy loss: 0.003211. Value loss: 0.007868. Entropy: 0.299780.\n",
      "Iteration 5799: Policy loss: -0.009822. Value loss: 0.007060. Entropy: 0.320086.\n",
      "episode: 4119   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 594     evaluation reward: 5.27\n",
      "episode: 4120   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 293     evaluation reward: 5.23\n",
      "episode: 4121   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 455     evaluation reward: 5.23\n",
      "Training network. lr: 0.000207. clip: 0.082603\n",
      "Iteration 5800: Policy loss: 0.007317. Value loss: 0.011154. Entropy: 0.427800.\n",
      "Iteration 5801: Policy loss: 0.005385. Value loss: 0.008680. Entropy: 0.435010.\n",
      "Iteration 5802: Policy loss: -0.004976. Value loss: 0.007493. Entropy: 0.420289.\n",
      "episode: 4122   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 441     evaluation reward: 5.23\n",
      "episode: 4123   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 409     evaluation reward: 5.22\n",
      "Training network. lr: 0.000206. clip: 0.082594\n",
      "Iteration 5803: Policy loss: 0.011504. Value loss: 0.011564. Entropy: 0.398573.\n",
      "Iteration 5804: Policy loss: 0.016966. Value loss: 0.008972. Entropy: 0.409053.\n",
      "Iteration 5805: Policy loss: -0.000892. Value loss: 0.007116. Entropy: 0.398379.\n",
      "episode: 4124   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 493     evaluation reward: 5.19\n",
      "episode: 4125   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 458     evaluation reward: 5.19\n",
      "episode: 4126   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 451     evaluation reward: 5.19\n",
      "Training network. lr: 0.000206. clip: 0.082585\n",
      "Iteration 5806: Policy loss: 0.009081. Value loss: 0.009553. Entropy: 0.364913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5807: Policy loss: 0.003171. Value loss: 0.007958. Entropy: 0.352814.\n",
      "Iteration 5808: Policy loss: -0.005697. Value loss: 0.006751. Entropy: 0.352840.\n",
      "episode: 4127   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 319     evaluation reward: 5.17\n",
      "episode: 4128   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 549     evaluation reward: 5.21\n",
      "Training network. lr: 0.000206. clip: 0.082576\n",
      "Iteration 5809: Policy loss: 0.005791. Value loss: 0.012326. Entropy: 0.444594.\n",
      "Iteration 5810: Policy loss: -0.003069. Value loss: 0.009038. Entropy: 0.450354.\n",
      "Iteration 5811: Policy loss: -0.009258. Value loss: 0.007738. Entropy: 0.439137.\n",
      "episode: 4129   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 5.18\n",
      "episode: 4130   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 463     evaluation reward: 5.14\n",
      "Training network. lr: 0.000206. clip: 0.082567\n",
      "Iteration 5812: Policy loss: 0.006853. Value loss: 0.011467. Entropy: 0.298575.\n",
      "Iteration 5813: Policy loss: -0.004079. Value loss: 0.009491. Entropy: 0.291102.\n",
      "Iteration 5814: Policy loss: -0.011077. Value loss: 0.008030. Entropy: 0.278834.\n",
      "episode: 4131   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 349     evaluation reward: 5.13\n",
      "episode: 4132   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 449     evaluation reward: 5.11\n",
      "episode: 4133   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 450     evaluation reward: 5.11\n",
      "Training network. lr: 0.000206. clip: 0.082558\n",
      "Iteration 5815: Policy loss: 0.002851. Value loss: 0.011231. Entropy: 0.385118.\n",
      "Iteration 5816: Policy loss: -0.004162. Value loss: 0.009845. Entropy: 0.379275.\n",
      "Iteration 5817: Policy loss: -0.008973. Value loss: 0.008101. Entropy: 0.379990.\n",
      "episode: 4134   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 562     evaluation reward: 5.14\n",
      "episode: 4135   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 482     evaluation reward: 5.14\n",
      "Training network. lr: 0.000206. clip: 0.082549\n",
      "Iteration 5818: Policy loss: 0.010193. Value loss: 0.011728. Entropy: 0.408217.\n",
      "Iteration 5819: Policy loss: -0.002386. Value loss: 0.008171. Entropy: 0.408486.\n",
      "Iteration 5820: Policy loss: -0.014416. Value loss: 0.006752. Entropy: 0.411456.\n",
      "episode: 4136   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 282     evaluation reward: 5.12\n",
      "episode: 4137   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 467     evaluation reward: 5.13\n",
      "Training network. lr: 0.000206. clip: 0.082540\n",
      "Iteration 5821: Policy loss: 0.015365. Value loss: 0.010461. Entropy: 0.433812.\n",
      "Iteration 5822: Policy loss: -0.002143. Value loss: 0.008110. Entropy: 0.422389.\n",
      "Iteration 5823: Policy loss: -0.005686. Value loss: 0.007244. Entropy: 0.418889.\n",
      "episode: 4138   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 418     evaluation reward: 5.13\n",
      "episode: 4139   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 314     evaluation reward: 5.1\n",
      "episode: 4140   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 273     evaluation reward: 5.07\n",
      "Training network. lr: 0.000206. clip: 0.082531\n",
      "Iteration 5824: Policy loss: 0.024326. Value loss: 0.007443. Entropy: 0.403032.\n",
      "Iteration 5825: Policy loss: 0.008338. Value loss: 0.005317. Entropy: 0.407481.\n",
      "Iteration 5826: Policy loss: -0.003367. Value loss: 0.004609. Entropy: 0.433246.\n",
      "episode: 4141   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 791     evaluation reward: 5.04\n",
      "episode: 4142   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 436     evaluation reward: 5.04\n",
      "Training network. lr: 0.000206. clip: 0.082522\n",
      "Iteration 5827: Policy loss: 0.007636. Value loss: 0.010703. Entropy: 0.325562.\n",
      "Iteration 5828: Policy loss: -0.004660. Value loss: 0.008176. Entropy: 0.308469.\n",
      "Iteration 5829: Policy loss: -0.015126. Value loss: 0.007521. Entropy: 0.302451.\n",
      "episode: 4143   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 5.03\n",
      "episode: 4144   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 415     evaluation reward: 5.01\n",
      "Training network. lr: 0.000206. clip: 0.082513\n",
      "Iteration 5830: Policy loss: 0.007991. Value loss: 0.008803. Entropy: 0.345659.\n",
      "Iteration 5831: Policy loss: -0.004447. Value loss: 0.006997. Entropy: 0.344743.\n",
      "Iteration 5832: Policy loss: -0.011249. Value loss: 0.006035. Entropy: 0.344038.\n",
      "episode: 4145   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 480     evaluation reward: 5.03\n",
      "Training network. lr: 0.000206. clip: 0.082504\n",
      "Iteration 5833: Policy loss: 0.024120. Value loss: 0.008288. Entropy: 0.397430.\n",
      "Iteration 5834: Policy loss: 0.000230. Value loss: 0.006218. Entropy: 0.397215.\n",
      "Iteration 5835: Policy loss: 0.002593. Value loss: 0.005217. Entropy: 0.428535.\n",
      "episode: 4146   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 1246     evaluation reward: 5.0\n",
      "episode: 4147   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 341     evaluation reward: 4.99\n",
      "Training network. lr: 0.000206. clip: 0.082495\n",
      "Iteration 5836: Policy loss: 0.013657. Value loss: 0.009692. Entropy: 0.373399.\n",
      "Iteration 5837: Policy loss: 0.006957. Value loss: 0.007184. Entropy: 0.398769.\n",
      "Iteration 5838: Policy loss: -0.000493. Value loss: 0.005847. Entropy: 0.422205.\n",
      "episode: 4148   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 429     evaluation reward: 4.97\n",
      "episode: 4149   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 446     evaluation reward: 4.97\n",
      "Training network. lr: 0.000206. clip: 0.082486\n",
      "Iteration 5839: Policy loss: 0.006780. Value loss: 0.010596. Entropy: 0.351766.\n",
      "Iteration 5840: Policy loss: -0.008307. Value loss: 0.007428. Entropy: 0.349370.\n",
      "Iteration 5841: Policy loss: -0.011145. Value loss: 0.006389. Entropy: 0.346805.\n",
      "episode: 4150   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 514     evaluation reward: 4.97\n",
      "episode: 4151   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 444     evaluation reward: 4.97\n",
      "episode: 4152   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 446     evaluation reward: 4.98\n",
      "Training network. lr: 0.000206. clip: 0.082477\n",
      "Iteration 5842: Policy loss: 0.002194. Value loss: 0.007885. Entropy: 0.298108.\n",
      "Iteration 5843: Policy loss: 0.001034. Value loss: 0.006905. Entropy: 0.298914.\n",
      "Iteration 5844: Policy loss: -0.002404. Value loss: 0.006359. Entropy: 0.289715.\n",
      "episode: 4153   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 435     evaluation reward: 4.95\n",
      "Training network. lr: 0.000206. clip: 0.082468\n",
      "Iteration 5845: Policy loss: 0.010082. Value loss: 0.011155. Entropy: 0.369691.\n",
      "Iteration 5846: Policy loss: 0.001607. Value loss: 0.008163. Entropy: 0.381129.\n",
      "Iteration 5847: Policy loss: -0.008254. Value loss: 0.006877. Entropy: 0.376650.\n",
      "episode: 4154   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 731     evaluation reward: 4.97\n",
      "episode: 4155   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 434     evaluation reward: 4.93\n",
      "Training network. lr: 0.000206. clip: 0.082459\n",
      "Iteration 5848: Policy loss: 0.015794. Value loss: 0.008224. Entropy: 0.367992.\n",
      "Iteration 5849: Policy loss: 0.001940. Value loss: 0.006214. Entropy: 0.398442.\n",
      "Iteration 5850: Policy loss: -0.004574. Value loss: 0.005003. Entropy: 0.410753.\n",
      "episode: 4156   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 724     evaluation reward: 4.92\n",
      "episode: 4157   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 384     evaluation reward: 4.92\n",
      "Training network. lr: 0.000206. clip: 0.082450\n",
      "Iteration 5851: Policy loss: 0.017872. Value loss: 0.012880. Entropy: 0.353861.\n",
      "Iteration 5852: Policy loss: -0.001042. Value loss: 0.010289. Entropy: 0.349088.\n",
      "Iteration 5853: Policy loss: -0.010494. Value loss: 0.009000. Entropy: 0.333594.\n",
      "episode: 4158   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 543     evaluation reward: 4.93\n",
      "episode: 4159   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 606     evaluation reward: 4.95\n",
      "Training network. lr: 0.000206. clip: 0.082441\n",
      "Iteration 5854: Policy loss: 0.016510. Value loss: 0.012088. Entropy: 0.415374.\n",
      "Iteration 5855: Policy loss: 0.014107. Value loss: 0.010584. Entropy: 0.420399.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5856: Policy loss: -0.003015. Value loss: 0.009013. Entropy: 0.433396.\n",
      "episode: 4160   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 491     evaluation reward: 4.95\n",
      "episode: 4161   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 505     evaluation reward: 4.96\n",
      "Training network. lr: 0.000206. clip: 0.082432\n",
      "Iteration 5857: Policy loss: 0.010561. Value loss: 0.011502. Entropy: 0.376866.\n",
      "Iteration 5858: Policy loss: -0.001032. Value loss: 0.008254. Entropy: 0.373266.\n",
      "Iteration 5859: Policy loss: -0.006451. Value loss: 0.007432. Entropy: 0.369136.\n",
      "now time :  2018-12-26 15:27:30.399834\n",
      "episode: 4162   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 701     evaluation reward: 5.0\n",
      "episode: 4163   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 579     evaluation reward: 5.01\n",
      "Training network. lr: 0.000206. clip: 0.082423\n",
      "Iteration 5860: Policy loss: 0.008121. Value loss: 0.012720. Entropy: 0.398137.\n",
      "Iteration 5861: Policy loss: -0.001919. Value loss: 0.010037. Entropy: 0.393793.\n",
      "Iteration 5862: Policy loss: -0.001754. Value loss: 0.009251. Entropy: 0.395194.\n",
      "episode: 4164   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 524     evaluation reward: 5.0\n",
      "episode: 4165   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 542     evaluation reward: 5.01\n",
      "Training network. lr: 0.000206. clip: 0.082414\n",
      "Iteration 5863: Policy loss: 0.006035. Value loss: 0.009691. Entropy: 0.372494.\n",
      "Iteration 5864: Policy loss: -0.006033. Value loss: 0.007802. Entropy: 0.362839.\n",
      "Iteration 5865: Policy loss: -0.002440. Value loss: 0.006825. Entropy: 0.356792.\n",
      "episode: 4166   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 424     evaluation reward: 4.99\n",
      "episode: 4167   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 300     evaluation reward: 4.94\n",
      "Training network. lr: 0.000206. clip: 0.082405\n",
      "Iteration 5866: Policy loss: 0.022382. Value loss: 0.016124. Entropy: 0.431769.\n",
      "Iteration 5867: Policy loss: 0.001360. Value loss: 0.008487. Entropy: 0.445798.\n",
      "Iteration 5868: Policy loss: -0.001212. Value loss: 0.006632. Entropy: 0.445543.\n",
      "episode: 4168   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 556     evaluation reward: 4.97\n",
      "episode: 4169   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 441     evaluation reward: 4.98\n",
      "Training network. lr: 0.000206. clip: 0.082396\n",
      "Iteration 5869: Policy loss: 0.016072. Value loss: 0.013850. Entropy: 0.308179.\n",
      "Iteration 5870: Policy loss: 0.008103. Value loss: 0.009557. Entropy: 0.305600.\n",
      "Iteration 5871: Policy loss: -0.004444. Value loss: 0.008308. Entropy: 0.298079.\n",
      "episode: 4170   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 788     evaluation reward: 5.04\n",
      "episode: 4171   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 315     evaluation reward: 4.98\n",
      "Training network. lr: 0.000206. clip: 0.082387\n",
      "Iteration 5872: Policy loss: 0.012893. Value loss: 0.015148. Entropy: 0.334108.\n",
      "Iteration 5873: Policy loss: -0.003339. Value loss: 0.010666. Entropy: 0.337595.\n",
      "Iteration 5874: Policy loss: -0.010294. Value loss: 0.008311. Entropy: 0.320856.\n",
      "episode: 4172   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 625     evaluation reward: 4.99\n",
      "episode: 4173   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 447     evaluation reward: 4.99\n",
      "Training network. lr: 0.000206. clip: 0.082378\n",
      "Iteration 5875: Policy loss: 0.007778. Value loss: 0.008136. Entropy: 0.298539.\n",
      "Iteration 5876: Policy loss: -0.002228. Value loss: 0.006427. Entropy: 0.290943.\n",
      "Iteration 5877: Policy loss: -0.002317. Value loss: 0.005628. Entropy: 0.299217.\n",
      "episode: 4174   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 432     evaluation reward: 4.98\n",
      "episode: 4175   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 385     evaluation reward: 4.97\n",
      "episode: 4176   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 295     evaluation reward: 4.92\n",
      "Training network. lr: 0.000206. clip: 0.082369\n",
      "Iteration 5878: Policy loss: 0.013682. Value loss: 0.008182. Entropy: 0.438091.\n",
      "Iteration 5879: Policy loss: -0.005665. Value loss: 0.007226. Entropy: 0.436347.\n",
      "Iteration 5880: Policy loss: -0.002464. Value loss: 0.006277. Entropy: 0.444762.\n",
      "episode: 4177   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 496     evaluation reward: 4.89\n",
      "episode: 4178   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 582     evaluation reward: 4.88\n",
      "Training network. lr: 0.000206. clip: 0.082360\n",
      "Iteration 5881: Policy loss: 0.004094. Value loss: 0.008125. Entropy: 0.409794.\n",
      "Iteration 5882: Policy loss: 0.002690. Value loss: 0.006020. Entropy: 0.416800.\n",
      "Iteration 5883: Policy loss: -0.010233. Value loss: 0.005185. Entropy: 0.422467.\n",
      "Training network. lr: 0.000206. clip: 0.082351\n",
      "Iteration 5884: Policy loss: 0.036840. Value loss: 0.001355. Entropy: 0.114302.\n",
      "Iteration 5885: Policy loss: 0.007909. Value loss: 0.000740. Entropy: 0.081314.\n",
      "Iteration 5886: Policy loss: 0.006155. Value loss: 0.000724. Entropy: 0.073285.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5887: Policy loss: 0.000650. Value loss: 0.000361. Entropy: 0.028630.\n",
      "Iteration 5888: Policy loss: -0.000152. Value loss: 0.001109. Entropy: 0.019829.\n",
      "Iteration 5889: Policy loss: -0.000004. Value loss: 0.000517. Entropy: 0.020827.\n",
      "Training network. lr: 0.000206. clip: 0.082333\n",
      "Iteration 5890: Policy loss: 0.002293. Value loss: 0.005582. Entropy: 0.180927.\n",
      "Iteration 5891: Policy loss: -0.002269. Value loss: 0.003970. Entropy: 0.331504.\n",
      "Iteration 5892: Policy loss: -0.011430. Value loss: 0.003295. Entropy: 0.350743.\n",
      "episode: 4179   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 3305     evaluation reward: 4.88\n",
      "episode: 4180   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 429     evaluation reward: 4.85\n",
      "episode: 4181   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 435     evaluation reward: 4.85\n",
      "Training network. lr: 0.000206. clip: 0.082324\n",
      "Iteration 5893: Policy loss: 0.009328. Value loss: 0.008645. Entropy: 0.411449.\n",
      "Iteration 5894: Policy loss: -0.001508. Value loss: 0.008118. Entropy: 0.428543.\n",
      "Iteration 5895: Policy loss: -0.013267. Value loss: 0.005736. Entropy: 0.416356.\n",
      "episode: 4182   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 399     evaluation reward: 4.86\n",
      "Training network. lr: 0.000206. clip: 0.082315\n",
      "Iteration 5896: Policy loss: 0.035916. Value loss: 0.006709. Entropy: 0.525006.\n",
      "Iteration 5897: Policy loss: -0.007560. Value loss: 0.005460. Entropy: 0.536441.\n",
      "Iteration 5898: Policy loss: -0.005408. Value loss: 0.004438. Entropy: 0.524916.\n",
      "episode: 4183   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 777     evaluation reward: 4.84\n",
      "episode: 4184   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 524     evaluation reward: 4.82\n",
      "episode: 4185   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 302     evaluation reward: 4.77\n",
      "Training network. lr: 0.000206. clip: 0.082306\n",
      "Iteration 5899: Policy loss: 0.020097. Value loss: 0.020792. Entropy: 0.456225.\n",
      "Iteration 5900: Policy loss: -0.001107. Value loss: 0.016135. Entropy: 0.463259.\n",
      "Iteration 5901: Policy loss: -0.005140. Value loss: 0.014120. Entropy: 0.461042.\n",
      "episode: 4186   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 470     evaluation reward: 4.78\n",
      "episode: 4187   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 334     evaluation reward: 4.74\n",
      "Training network. lr: 0.000206. clip: 0.082297\n",
      "Iteration 5902: Policy loss: 0.010239. Value loss: 0.013789. Entropy: 0.323012.\n",
      "Iteration 5903: Policy loss: -0.002186. Value loss: 0.011503. Entropy: 0.332655.\n",
      "Iteration 5904: Policy loss: -0.014234. Value loss: 0.008967. Entropy: 0.328701.\n",
      "episode: 4188   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 362     evaluation reward: 4.73\n",
      "episode: 4189   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 502     evaluation reward: 4.75\n",
      "episode: 4190   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 429     evaluation reward: 4.75\n",
      "Training network. lr: 0.000206. clip: 0.082288\n",
      "Iteration 5905: Policy loss: 0.010856. Value loss: 0.015342. Entropy: 0.315144.\n",
      "Iteration 5906: Policy loss: -0.001877. Value loss: 0.013288. Entropy: 0.310863.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5907: Policy loss: -0.009137. Value loss: 0.011348. Entropy: 0.305348.\n",
      "episode: 4191   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 468     evaluation reward: 4.73\n",
      "episode: 4192   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 418     evaluation reward: 4.69\n",
      "Training network. lr: 0.000206. clip: 0.082279\n",
      "Iteration 5908: Policy loss: 0.011578. Value loss: 0.009174. Entropy: 0.328884.\n",
      "Iteration 5909: Policy loss: 0.009761. Value loss: 0.007347. Entropy: 0.309053.\n",
      "Iteration 5910: Policy loss: -0.008202. Value loss: 0.006201. Entropy: 0.310377.\n",
      "episode: 4193   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 365     evaluation reward: 4.68\n",
      "episode: 4194   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 423     evaluation reward: 4.69\n",
      "episode: 4195   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 429     evaluation reward: 4.68\n",
      "Training network. lr: 0.000206. clip: 0.082270\n",
      "Iteration 5911: Policy loss: 0.006992. Value loss: 0.008139. Entropy: 0.241018.\n",
      "Iteration 5912: Policy loss: 0.000078. Value loss: 0.006817. Entropy: 0.242967.\n",
      "Iteration 5913: Policy loss: -0.010457. Value loss: 0.006039. Entropy: 0.240843.\n",
      "episode: 4196   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 442     evaluation reward: 4.68\n",
      "episode: 4197   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 456     evaluation reward: 4.69\n",
      "Training network. lr: 0.000206. clip: 0.082261\n",
      "Iteration 5914: Policy loss: 0.008815. Value loss: 0.013902. Entropy: 0.304189.\n",
      "Iteration 5915: Policy loss: -0.008088. Value loss: 0.011288. Entropy: 0.300896.\n",
      "Iteration 5916: Policy loss: -0.006164. Value loss: 0.009256. Entropy: 0.293879.\n",
      "episode: 4198   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 397     evaluation reward: 4.65\n",
      "episode: 4199   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 339     evaluation reward: 4.63\n",
      "Training network. lr: 0.000206. clip: 0.082252\n",
      "Iteration 5917: Policy loss: 0.017274. Value loss: 0.014276. Entropy: 0.311733.\n",
      "Iteration 5918: Policy loss: -0.006159. Value loss: 0.011350. Entropy: 0.318660.\n",
      "Iteration 5919: Policy loss: -0.005388. Value loss: 0.009504. Entropy: 0.327232.\n",
      "episode: 4200   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 452     evaluation reward: 4.66\n",
      "episode: 4201   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 350     evaluation reward: 4.64\n",
      "episode: 4202   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 428     evaluation reward: 4.65\n",
      "Training network. lr: 0.000206. clip: 0.082243\n",
      "Iteration 5920: Policy loss: 0.012475. Value loss: 0.013131. Entropy: 0.401676.\n",
      "Iteration 5921: Policy loss: 0.001364. Value loss: 0.010329. Entropy: 0.385748.\n",
      "Iteration 5922: Policy loss: -0.010709. Value loss: 0.009145. Entropy: 0.391734.\n",
      "episode: 4203   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 321     evaluation reward: 4.65\n",
      "episode: 4204   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 536     evaluation reward: 4.65\n",
      "Training network. lr: 0.000206. clip: 0.082234\n",
      "Iteration 5923: Policy loss: 0.018750. Value loss: 0.007472. Entropy: 0.309262.\n",
      "Iteration 5924: Policy loss: -0.004423. Value loss: 0.006056. Entropy: 0.308561.\n",
      "Iteration 5925: Policy loss: -0.008910. Value loss: 0.005370. Entropy: 0.306837.\n",
      "episode: 4205   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 499     evaluation reward: 4.68\n",
      "episode: 4206   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 428     evaluation reward: 4.66\n",
      "Training network. lr: 0.000206. clip: 0.082225\n",
      "Iteration 5926: Policy loss: 0.018644. Value loss: 0.007643. Entropy: 0.235678.\n",
      "Iteration 5927: Policy loss: 0.001855. Value loss: 0.006820. Entropy: 0.233027.\n",
      "Iteration 5928: Policy loss: -0.004360. Value loss: 0.006867. Entropy: 0.217740.\n",
      "episode: 4207   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 525     evaluation reward: 4.64\n",
      "episode: 4208   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 390     evaluation reward: 4.64\n",
      "episode: 4209   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 459     evaluation reward: 4.64\n",
      "Training network. lr: 0.000206. clip: 0.082216\n",
      "Iteration 5929: Policy loss: 0.011597. Value loss: 0.011728. Entropy: 0.331672.\n",
      "Iteration 5930: Policy loss: -0.001463. Value loss: 0.009633. Entropy: 0.333817.\n",
      "Iteration 5931: Policy loss: -0.012152. Value loss: 0.007716. Entropy: 0.334725.\n",
      "episode: 4210   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 277     evaluation reward: 4.61\n",
      "episode: 4211   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 446     evaluation reward: 4.6\n",
      "Training network. lr: 0.000206. clip: 0.082207\n",
      "Iteration 5932: Policy loss: 0.022562. Value loss: 0.013332. Entropy: 0.329703.\n",
      "Iteration 5933: Policy loss: 0.001542. Value loss: 0.009317. Entropy: 0.340342.\n",
      "Iteration 5934: Policy loss: -0.005263. Value loss: 0.006932. Entropy: 0.357642.\n",
      "episode: 4212   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 516     evaluation reward: 4.62\n",
      "episode: 4213   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 367     evaluation reward: 4.63\n",
      "episode: 4214   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 546     evaluation reward: 4.64\n",
      "Training network. lr: 0.000205. clip: 0.082198\n",
      "Iteration 5935: Policy loss: 0.002362. Value loss: 0.008318. Entropy: 0.304522.\n",
      "Iteration 5936: Policy loss: -0.003560. Value loss: 0.005387. Entropy: 0.303145.\n",
      "Iteration 5937: Policy loss: -0.008727. Value loss: 0.004595. Entropy: 0.303090.\n",
      "episode: 4215   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 634     evaluation reward: 4.64\n",
      "Training network. lr: 0.000205. clip: 0.082189\n",
      "Iteration 5938: Policy loss: 0.002374. Value loss: 0.012708. Entropy: 0.353971.\n",
      "Iteration 5939: Policy loss: -0.009871. Value loss: 0.008298. Entropy: 0.373592.\n",
      "Iteration 5940: Policy loss: -0.016522. Value loss: 0.005032. Entropy: 0.375086.\n",
      "episode: 4216   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 694     evaluation reward: 4.65\n",
      "episode: 4217   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 522     evaluation reward: 4.64\n",
      "Training network. lr: 0.000205. clip: 0.082180\n",
      "Iteration 5941: Policy loss: 0.004419. Value loss: 0.012919. Entropy: 0.371462.\n",
      "Iteration 5942: Policy loss: -0.004488. Value loss: 0.011577. Entropy: 0.370704.\n",
      "Iteration 5943: Policy loss: -0.005557. Value loss: 0.009572. Entropy: 0.373799.\n",
      "episode: 4218   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 342     evaluation reward: 4.6\n",
      "episode: 4219   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 266     evaluation reward: 4.55\n",
      "episode: 4220   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 538     evaluation reward: 4.57\n",
      "Training network. lr: 0.000205. clip: 0.082171\n",
      "Iteration 5944: Policy loss: 0.003815. Value loss: 0.014527. Entropy: 0.304931.\n",
      "Iteration 5945: Policy loss: -0.004659. Value loss: 0.011094. Entropy: 0.301602.\n",
      "Iteration 5946: Policy loss: -0.010327. Value loss: 0.009214. Entropy: 0.292394.\n",
      "episode: 4221   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 381     evaluation reward: 4.55\n",
      "episode: 4222   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 430     evaluation reward: 4.52\n",
      "Training network. lr: 0.000205. clip: 0.082162\n",
      "Iteration 5947: Policy loss: 0.010479. Value loss: 0.016637. Entropy: 0.319578.\n",
      "Iteration 5948: Policy loss: 0.012246. Value loss: 0.011685. Entropy: 0.299986.\n",
      "Iteration 5949: Policy loss: 0.006343. Value loss: 0.010035. Entropy: 0.282479.\n",
      "episode: 4223   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 508     evaluation reward: 4.5\n",
      "episode: 4224   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 692     evaluation reward: 4.49\n",
      "Training network. lr: 0.000205. clip: 0.082153\n",
      "Iteration 5950: Policy loss: 0.017577. Value loss: 0.007543. Entropy: 0.257435.\n",
      "Iteration 5951: Policy loss: 0.006084. Value loss: 0.006211. Entropy: 0.262549.\n",
      "Iteration 5952: Policy loss: -0.004035. Value loss: 0.005591. Entropy: 0.260537.\n",
      "episode: 4225   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 433     evaluation reward: 4.48\n",
      "episode: 4226   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 401     evaluation reward: 4.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000205. clip: 0.082144\n",
      "Iteration 5953: Policy loss: 0.019885. Value loss: 0.007761. Entropy: 0.247051.\n",
      "Iteration 5954: Policy loss: 0.007734. Value loss: 0.005963. Entropy: 0.281439.\n",
      "Iteration 5955: Policy loss: -0.003515. Value loss: 0.005721. Entropy: 0.229911.\n",
      "Training network. lr: 0.000205. clip: 0.082135\n",
      "Iteration 5956: Policy loss: -0.000044. Value loss: 0.000311. Entropy: 0.064126.\n",
      "Iteration 5957: Policy loss: 0.000002. Value loss: 0.000092. Entropy: 0.193768.\n",
      "Iteration 5958: Policy loss: 0.000170. Value loss: 0.000208. Entropy: 0.298529.\n",
      "episode: 4227   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 1627     evaluation reward: 4.44\n",
      "episode: 4228   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 449     evaluation reward: 4.41\n",
      "Training network. lr: 0.000205. clip: 0.082126\n",
      "Iteration 5959: Policy loss: 0.010996. Value loss: 0.010662. Entropy: 0.247103.\n",
      "Iteration 5960: Policy loss: 0.001875. Value loss: 0.009381. Entropy: 0.243337.\n",
      "Iteration 5961: Policy loss: -0.007876. Value loss: 0.008525. Entropy: 0.234964.\n",
      "episode: 4229   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 474     evaluation reward: 4.42\n",
      "episode: 4230   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 357     evaluation reward: 4.4\n",
      "episode: 4231   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 444     evaluation reward: 4.42\n",
      "Training network. lr: 0.000205. clip: 0.082117\n",
      "Iteration 5962: Policy loss: 0.007078. Value loss: 0.007192. Entropy: 0.252974.\n",
      "Iteration 5963: Policy loss: 0.010448. Value loss: 0.006276. Entropy: 0.260913.\n",
      "Iteration 5964: Policy loss: -0.005659. Value loss: 0.006043. Entropy: 0.268409.\n",
      "episode: 4232   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 449     evaluation reward: 4.4\n",
      "episode: 4233   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 539     evaluation reward: 4.41\n",
      "Training network. lr: 0.000205. clip: 0.082108\n",
      "Iteration 5965: Policy loss: 0.013516. Value loss: 0.007422. Entropy: 0.233569.\n",
      "Iteration 5966: Policy loss: -0.002828. Value loss: 0.005983. Entropy: 0.237661.\n",
      "Iteration 5967: Policy loss: 0.206278. Value loss: 0.005358. Entropy: 0.219555.\n",
      "episode: 4234   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 408     evaluation reward: 4.37\n",
      "episode: 4235   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 425     evaluation reward: 4.35\n",
      "Training network. lr: 0.000205. clip: 0.082099\n",
      "Iteration 5968: Policy loss: 0.091280. Value loss: 0.026330. Entropy: 0.246930.\n",
      "Iteration 5969: Policy loss: 0.012039. Value loss: 0.012065. Entropy: 0.254558.\n",
      "Iteration 5970: Policy loss: -0.005022. Value loss: 0.012484. Entropy: 0.267599.\n",
      "episode: 4236   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 421     evaluation reward: 4.37\n",
      "episode: 4237   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 324     evaluation reward: 4.35\n",
      "episode: 4238   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 355     evaluation reward: 4.33\n",
      "Training network. lr: 0.000205. clip: 0.082090\n",
      "Iteration 5971: Policy loss: 0.009955. Value loss: 0.009450. Entropy: 0.299802.\n",
      "Iteration 5972: Policy loss: 0.003661. Value loss: 0.007623. Entropy: 0.308111.\n",
      "Iteration 5973: Policy loss: -0.002353. Value loss: 0.007348. Entropy: 0.303896.\n",
      "episode: 4239   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 407     evaluation reward: 4.36\n",
      "episode: 4240   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 435     evaluation reward: 4.4\n",
      "episode: 4241   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 308     evaluation reward: 4.37\n",
      "Training network. lr: 0.000205. clip: 0.082081\n",
      "Iteration 5974: Policy loss: 0.009357. Value loss: 0.011878. Entropy: 0.285208.\n",
      "Iteration 5975: Policy loss: -0.001738. Value loss: 0.008515. Entropy: 0.301770.\n",
      "Iteration 5976: Policy loss: -0.011192. Value loss: 0.006878. Entropy: 0.293192.\n",
      "episode: 4242   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 379     evaluation reward: 4.36\n",
      "episode: 4243   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 324     evaluation reward: 4.34\n",
      "episode: 4244   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 328     evaluation reward: 4.32\n",
      "Training network. lr: 0.000205. clip: 0.082072\n",
      "Iteration 5977: Policy loss: 0.016920. Value loss: 0.011520. Entropy: 0.352450.\n",
      "Iteration 5978: Policy loss: 0.004607. Value loss: 0.009406. Entropy: 0.350307.\n",
      "Iteration 5979: Policy loss: -0.009380. Value loss: 0.007820. Entropy: 0.342859.\n",
      "episode: 4245   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 400     evaluation reward: 4.31\n",
      "episode: 4246   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 381     evaluation reward: 4.32\n",
      "Training network. lr: 0.000205. clip: 0.082063\n",
      "Iteration 5980: Policy loss: 0.015717. Value loss: 0.009559. Entropy: 0.254393.\n",
      "Iteration 5981: Policy loss: 0.008479. Value loss: 0.007457. Entropy: 0.266571.\n",
      "Iteration 5982: Policy loss: -0.008442. Value loss: 0.006679. Entropy: 0.262414.\n",
      "episode: 4247   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 474     evaluation reward: 4.35\n",
      "episode: 4248   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 451     evaluation reward: 4.35\n",
      "Training network. lr: 0.000205. clip: 0.082054\n",
      "Iteration 5983: Policy loss: 0.009422. Value loss: 0.006953. Entropy: 0.233199.\n",
      "Iteration 5984: Policy loss: 0.004412. Value loss: 0.006155. Entropy: 0.214952.\n",
      "Iteration 5985: Policy loss: 0.011675. Value loss: 0.005947. Entropy: 0.227245.\n",
      "episode: 4249   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 478     evaluation reward: 4.36\n",
      "episode: 4250   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 306     evaluation reward: 4.32\n",
      "episode: 4251   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 447     evaluation reward: 4.33\n",
      "Training network. lr: 0.000205. clip: 0.082045\n",
      "Iteration 5986: Policy loss: 0.014618. Value loss: 0.010826. Entropy: 0.342131.\n",
      "Iteration 5987: Policy loss: 0.006638. Value loss: 0.008406. Entropy: 0.336077.\n",
      "Iteration 5988: Policy loss: -0.004319. Value loss: 0.007720. Entropy: 0.337414.\n",
      "episode: 4252   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 420     evaluation reward: 4.32\n",
      "episode: 4253   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 409     evaluation reward: 4.32\n",
      "Training network. lr: 0.000205. clip: 0.082036\n",
      "Iteration 5989: Policy loss: 0.014164. Value loss: 0.009105. Entropy: 0.304572.\n",
      "Iteration 5990: Policy loss: 0.002872. Value loss: 0.007035. Entropy: 0.298667.\n",
      "Iteration 5991: Policy loss: -0.009675. Value loss: 0.005698. Entropy: 0.296473.\n",
      "episode: 4254   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 516     evaluation reward: 4.32\n",
      "episode: 4255   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 443     evaluation reward: 4.33\n",
      "episode: 4256   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 332     evaluation reward: 4.32\n",
      "Training network. lr: 0.000205. clip: 0.082027\n",
      "Iteration 5992: Policy loss: 0.014743. Value loss: 0.010483. Entropy: 0.320494.\n",
      "Iteration 5993: Policy loss: 0.007139. Value loss: 0.008580. Entropy: 0.316882.\n",
      "Iteration 5994: Policy loss: 0.009359. Value loss: 0.007159. Entropy: 0.319355.\n",
      "episode: 4257   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 458     evaluation reward: 4.33\n",
      "episode: 4258   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 483     evaluation reward: 4.32\n",
      "Training network. lr: 0.000205. clip: 0.082018\n",
      "Iteration 5995: Policy loss: 0.008723. Value loss: 0.015129. Entropy: 0.259937.\n",
      "Iteration 5996: Policy loss: 0.003408. Value loss: 0.011175. Entropy: 0.253190.\n",
      "Iteration 5997: Policy loss: -0.008366. Value loss: 0.010711. Entropy: 0.253315.\n",
      "episode: 4259   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 613     evaluation reward: 4.3\n",
      "episode: 4260   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 407     evaluation reward: 4.29\n",
      "Training network. lr: 0.000205. clip: 0.082009\n",
      "Iteration 5998: Policy loss: 0.006309. Value loss: 0.010143. Entropy: 0.258620.\n",
      "Iteration 5999: Policy loss: -0.004887. Value loss: 0.008250. Entropy: 0.270252.\n",
      "Iteration 6000: Policy loss: -0.008500. Value loss: 0.006928. Entropy: 0.265534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4261   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 620     evaluation reward: 4.25\n",
      "episode: 4262   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 425     evaluation reward: 4.21\n",
      "Training network. lr: 0.000205. clip: 0.082000\n",
      "Iteration 6001: Policy loss: 0.013660. Value loss: 0.012801. Entropy: 0.210301.\n",
      "Iteration 6002: Policy loss: 0.018201. Value loss: 0.009566. Entropy: 0.209940.\n",
      "Iteration 6003: Policy loss: -0.005082. Value loss: 0.008250. Entropy: 0.202152.\n",
      "episode: 4263   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 519     evaluation reward: 4.18\n",
      "episode: 4264   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 350     evaluation reward: 4.15\n",
      "now time :  2018-12-26 15:32:48.503481\n",
      "Training network. lr: 0.000205. clip: 0.081991\n",
      "Iteration 6004: Policy loss: 0.012127. Value loss: 0.007932. Entropy: 0.266080.\n",
      "Iteration 6005: Policy loss: 0.007829. Value loss: 0.006876. Entropy: 0.259098.\n",
      "Iteration 6006: Policy loss: 0.001528. Value loss: 0.005719. Entropy: 0.262573.\n",
      "episode: 4265   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 387     evaluation reward: 4.12\n",
      "episode: 4266   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 575     evaluation reward: 4.15\n",
      "Training network. lr: 0.000205. clip: 0.081982\n",
      "Iteration 6007: Policy loss: 0.020698. Value loss: 0.008460. Entropy: 0.275609.\n",
      "Iteration 6008: Policy loss: 0.008601. Value loss: 0.006585. Entropy: 0.282362.\n",
      "Iteration 6009: Policy loss: -0.004495. Value loss: 0.005530. Entropy: 0.279107.\n",
      "episode: 4267   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 516     evaluation reward: 4.16\n",
      "episode: 4268   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 422     evaluation reward: 4.14\n",
      "episode: 4269   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 291     evaluation reward: 4.12\n",
      "Training network. lr: 0.000205. clip: 0.081973\n",
      "Iteration 6010: Policy loss: 0.014818. Value loss: 0.009348. Entropy: 0.319767.\n",
      "Iteration 6011: Policy loss: -0.003728. Value loss: 0.006763. Entropy: 0.308468.\n",
      "Iteration 6012: Policy loss: -0.003975. Value loss: 0.006119. Entropy: 0.307969.\n",
      "episode: 4270   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 777     evaluation reward: 4.09\n",
      "episode: 4271   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 408     evaluation reward: 4.1\n",
      "Training network. lr: 0.000205. clip: 0.081964\n",
      "Iteration 6013: Policy loss: 0.002627. Value loss: 0.008392. Entropy: 0.170142.\n",
      "Iteration 6014: Policy loss: 0.003590. Value loss: 0.007402. Entropy: 0.182025.\n",
      "Iteration 6015: Policy loss: 0.003665. Value loss: 0.006518. Entropy: 0.171716.\n",
      "episode: 4272   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 385     evaluation reward: 4.07\n",
      "episode: 4273   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 351     evaluation reward: 4.05\n",
      "Training network. lr: 0.000205. clip: 0.081955\n",
      "Iteration 6016: Policy loss: 0.010243. Value loss: 0.009962. Entropy: 0.242901.\n",
      "Iteration 6017: Policy loss: 0.003153. Value loss: 0.007774. Entropy: 0.239322.\n",
      "Iteration 6018: Policy loss: -0.007648. Value loss: 0.006680. Entropy: 0.247245.\n",
      "episode: 4274   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 361     evaluation reward: 4.05\n",
      "episode: 4275   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 412     evaluation reward: 4.05\n",
      "Training network. lr: 0.000205. clip: 0.081946\n",
      "Iteration 6019: Policy loss: 0.004339. Value loss: 0.007482. Entropy: 0.235752.\n",
      "Iteration 6020: Policy loss: -0.000830. Value loss: 0.006000. Entropy: 0.229196.\n",
      "Iteration 6021: Policy loss: -0.006552. Value loss: 0.005564. Entropy: 0.222551.\n",
      "episode: 4276   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 647     evaluation reward: 4.08\n",
      "episode: 4277   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 397     evaluation reward: 4.06\n",
      "episode: 4278   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 455     evaluation reward: 4.04\n",
      "Training network. lr: 0.000205. clip: 0.081937\n",
      "Iteration 6022: Policy loss: 0.010239. Value loss: 0.011131. Entropy: 0.322466.\n",
      "Iteration 6023: Policy loss: 0.000002. Value loss: 0.008522. Entropy: 0.351018.\n",
      "Iteration 6024: Policy loss: -0.004871. Value loss: 0.007264. Entropy: 0.344032.\n",
      "episode: 4279   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 356     evaluation reward: 4.01\n",
      "episode: 4280   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 593     evaluation reward: 4.03\n",
      "Training network. lr: 0.000205. clip: 0.081928\n",
      "Iteration 6025: Policy loss: 0.009103. Value loss: 0.008740. Entropy: 0.282948.\n",
      "Iteration 6026: Policy loss: 0.000620. Value loss: 0.007781. Entropy: 0.284498.\n",
      "Iteration 6027: Policy loss: -0.008741. Value loss: 0.006780. Entropy: 0.291805.\n",
      "episode: 4281   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 336     evaluation reward: 4.0\n",
      "episode: 4282   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 528     evaluation reward: 4.01\n",
      "Training network. lr: 0.000205. clip: 0.081919\n",
      "Iteration 6028: Policy loss: 0.027234. Value loss: 0.011372. Entropy: 0.258391.\n",
      "Iteration 6029: Policy loss: 0.012434. Value loss: 0.008616. Entropy: 0.248372.\n",
      "Iteration 6030: Policy loss: -0.004949. Value loss: 0.007269. Entropy: 0.257168.\n",
      "episode: 4283   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 562     evaluation reward: 4.04\n",
      "episode: 4284   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 511     evaluation reward: 4.04\n",
      "Training network. lr: 0.000205. clip: 0.081910\n",
      "Iteration 6031: Policy loss: 0.015509. Value loss: 0.012005. Entropy: 0.226971.\n",
      "Iteration 6032: Policy loss: 0.005917. Value loss: 0.009275. Entropy: 0.230287.\n",
      "Iteration 6033: Policy loss: -0.006972. Value loss: 0.008119. Entropy: 0.224273.\n",
      "episode: 4285   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 491     evaluation reward: 4.08\n",
      "episode: 4286   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 555     evaluation reward: 4.1\n",
      "Training network. lr: 0.000205. clip: 0.081901\n",
      "Iteration 6034: Policy loss: 0.010638. Value loss: 0.008248. Entropy: 0.273794.\n",
      "Iteration 6035: Policy loss: 0.003700. Value loss: 0.006791. Entropy: 0.277825.\n",
      "Iteration 6036: Policy loss: -0.005682. Value loss: 0.006103. Entropy: 0.266246.\n",
      "episode: 4287   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 342     evaluation reward: 4.1\n",
      "episode: 4288   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 345     evaluation reward: 4.1\n",
      "episode: 4289   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 453     evaluation reward: 4.08\n",
      "Training network. lr: 0.000205. clip: 0.081892\n",
      "Iteration 6037: Policy loss: 0.018422. Value loss: 0.010636. Entropy: 0.346460.\n",
      "Iteration 6038: Policy loss: 0.000274. Value loss: 0.008815. Entropy: 0.350913.\n",
      "Iteration 6039: Policy loss: -0.007030. Value loss: 0.007695. Entropy: 0.350258.\n",
      "episode: 4290   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 570     evaluation reward: 4.09\n",
      "episode: 4291   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 451     evaluation reward: 4.09\n",
      "Training network. lr: 0.000205. clip: 0.081883\n",
      "Iteration 6040: Policy loss: 0.007454. Value loss: 0.009689. Entropy: 0.326076.\n",
      "Iteration 6041: Policy loss: 0.000586. Value loss: 0.007085. Entropy: 0.311344.\n",
      "Iteration 6042: Policy loss: -0.005737. Value loss: 0.006140. Entropy: 0.315331.\n",
      "episode: 4292   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 429     evaluation reward: 4.1\n",
      "episode: 4293   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 385     evaluation reward: 4.09\n",
      "Training network. lr: 0.000205. clip: 0.081874\n",
      "Iteration 6043: Policy loss: 0.013374. Value loss: 0.016831. Entropy: 0.252268.\n",
      "Iteration 6044: Policy loss: 0.000909. Value loss: 0.015038. Entropy: 0.240348.\n",
      "Iteration 6045: Policy loss: -0.001181. Value loss: 0.016637. Entropy: 0.237881.\n",
      "episode: 4294   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 435     evaluation reward: 4.09\n",
      "Training network. lr: 0.000205. clip: 0.081865\n",
      "Iteration 6046: Policy loss: 0.000783. Value loss: 0.004941. Entropy: 0.168812.\n",
      "Iteration 6047: Policy loss: 4.719033. Value loss: 0.001149. Entropy: 0.117720.\n",
      "Iteration 6048: Policy loss: 0.017128. Value loss: 0.001212. Entropy: 0.080829.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4295   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 1796     evaluation reward: 4.07\n",
      "Training network. lr: 0.000205. clip: 0.081856\n",
      "Iteration 6049: Policy loss: 0.008402. Value loss: 0.006552. Entropy: 0.172645.\n",
      "Iteration 6050: Policy loss: 0.001397. Value loss: 0.004978. Entropy: 0.185247.\n",
      "Iteration 6051: Policy loss: -0.004934. Value loss: 0.005965. Entropy: 0.296154.\n",
      "episode: 4296   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 514     evaluation reward: 4.05\n",
      "episode: 4297   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 464     evaluation reward: 4.03\n",
      "Training network. lr: 0.000205. clip: 0.081847\n",
      "Iteration 6052: Policy loss: 0.019828. Value loss: 0.008785. Entropy: 0.411256.\n",
      "Iteration 6053: Policy loss: 0.005222. Value loss: 0.006200. Entropy: 0.399453.\n",
      "Iteration 6054: Policy loss: -0.005246. Value loss: 0.005341. Entropy: 0.401586.\n",
      "episode: 4298   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 509     evaluation reward: 4.04\n",
      "Training network. lr: 0.000205. clip: 0.081838\n",
      "Iteration 6055: Policy loss: 0.044818. Value loss: 0.004306. Entropy: 0.392011.\n",
      "Iteration 6056: Policy loss: 0.033366. Value loss: 0.003109. Entropy: 0.433659.\n",
      "Iteration 6057: Policy loss: 0.039870. Value loss: 0.002513. Entropy: 0.352965.\n",
      "episode: 4299   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 1135     evaluation reward: 4.01\n",
      "episode: 4300   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 315     evaluation reward: 3.97\n",
      "Training network. lr: 0.000205. clip: 0.081829\n",
      "Iteration 6058: Policy loss: 0.013619. Value loss: 0.004329. Entropy: 0.196639.\n",
      "Iteration 6059: Policy loss: 0.008819. Value loss: 0.002645. Entropy: 0.177229.\n",
      "Iteration 6060: Policy loss: -0.003492. Value loss: 0.002753. Entropy: 0.201980.\n",
      "Training network. lr: 0.000205. clip: 0.081820\n",
      "Iteration 6061: Policy loss: 0.004317. Value loss: 0.000576. Entropy: 0.079713.\n",
      "Iteration 6062: Policy loss: 0.007568. Value loss: 0.000494. Entropy: 0.117923.\n",
      "Iteration 6063: Policy loss: 0.026465. Value loss: 0.000644. Entropy: 0.106337.\n",
      "Training network. lr: 0.000205. clip: 0.081811\n",
      "Iteration 6064: Policy loss: 0.002364. Value loss: 0.000440. Entropy: 0.064520.\n",
      "Iteration 6065: Policy loss: 0.001857. Value loss: 0.000564. Entropy: 0.134570.\n",
      "Iteration 6066: Policy loss: 0.001830. Value loss: 0.000524. Entropy: 0.205501.\n",
      "episode: 4301   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 2446     evaluation reward: 3.94\n",
      "episode: 4302   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 357     evaluation reward: 3.93\n",
      "episode: 4303   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 371     evaluation reward: 3.93\n",
      "Training network. lr: 0.000205. clip: 0.081802\n",
      "Iteration 6067: Policy loss: 0.000532. Value loss: 0.008893. Entropy: 0.368091.\n",
      "Iteration 6068: Policy loss: -0.006670. Value loss: 0.006422. Entropy: 0.365486.\n",
      "Iteration 6069: Policy loss: -0.004728. Value loss: 0.006019. Entropy: 0.368744.\n",
      "episode: 4304   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 413     evaluation reward: 3.91\n",
      "episode: 4305   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 297     evaluation reward: 3.87\n",
      "episode: 4306   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 309     evaluation reward: 3.86\n",
      "Training network. lr: 0.000204. clip: 0.081793\n",
      "Iteration 6070: Policy loss: 0.006123. Value loss: 0.011630. Entropy: 0.415903.\n",
      "Iteration 6071: Policy loss: -0.005674. Value loss: 0.007991. Entropy: 0.412529.\n",
      "Iteration 6072: Policy loss: -0.004937. Value loss: 0.006394. Entropy: 0.417063.\n",
      "episode: 4307   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 313     evaluation reward: 3.86\n",
      "episode: 4308   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 286     evaluation reward: 3.83\n",
      "episode: 4309   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 387     evaluation reward: 3.82\n",
      "Training network. lr: 0.000204. clip: 0.081784\n",
      "Iteration 6073: Policy loss: 0.006816. Value loss: 0.010629. Entropy: 0.353170.\n",
      "Iteration 6074: Policy loss: -0.003511. Value loss: 0.008653. Entropy: 0.343954.\n",
      "Iteration 6075: Policy loss: -0.008426. Value loss: 0.007504. Entropy: 0.351616.\n",
      "episode: 4310   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 790     evaluation reward: 3.87\n",
      "episode: 4311   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 378     evaluation reward: 3.88\n",
      "Training network. lr: 0.000204. clip: 0.081775\n",
      "Iteration 6076: Policy loss: 0.007410. Value loss: 0.007408. Entropy: 0.202089.\n",
      "Iteration 6077: Policy loss: 0.003517. Value loss: 0.006320. Entropy: 0.201798.\n",
      "Iteration 6078: Policy loss: -0.000627. Value loss: 0.005613. Entropy: 0.202071.\n",
      "episode: 4312   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 358     evaluation reward: 3.86\n",
      "episode: 4313   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 289     evaluation reward: 3.85\n",
      "episode: 4314   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 309     evaluation reward: 3.83\n",
      "Training network. lr: 0.000204. clip: 0.081766\n",
      "Iteration 6079: Policy loss: 0.006492. Value loss: 0.009398. Entropy: 0.319837.\n",
      "Iteration 6080: Policy loss: 0.000925. Value loss: 0.007175. Entropy: 0.319870.\n",
      "Iteration 6081: Policy loss: -0.004070. Value loss: 0.006266. Entropy: 0.323096.\n",
      "episode: 4315   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 502     evaluation reward: 3.83\n",
      "episode: 4316   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 306     evaluation reward: 3.8\n",
      "Training network. lr: 0.000204. clip: 0.081757\n",
      "Iteration 6082: Policy loss: 0.000727. Value loss: 0.007232. Entropy: 0.306377.\n",
      "Iteration 6083: Policy loss: -0.002089. Value loss: 0.006293. Entropy: 0.308013.\n",
      "Iteration 6084: Policy loss: -0.010775. Value loss: 0.005713. Entropy: 0.299130.\n",
      "episode: 4317   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 432     evaluation reward: 3.78\n",
      "episode: 4318   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 348     evaluation reward: 3.8\n",
      "episode: 4319   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 432     evaluation reward: 3.83\n",
      "Training network. lr: 0.000204. clip: 0.081748\n",
      "Iteration 6085: Policy loss: 0.003668. Value loss: 0.008322. Entropy: 0.327278.\n",
      "Iteration 6086: Policy loss: -0.009194. Value loss: 0.006583. Entropy: 0.328790.\n",
      "Iteration 6087: Policy loss: -0.014434. Value loss: 0.006335. Entropy: 0.334816.\n",
      "episode: 4320   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 465     evaluation reward: 3.84\n",
      "episode: 4321   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 276     evaluation reward: 3.83\n",
      "Training network. lr: 0.000204. clip: 0.081739\n",
      "Iteration 6088: Policy loss: -0.001863. Value loss: 0.008794. Entropy: 0.369403.\n",
      "Iteration 6089: Policy loss: -0.013149. Value loss: 0.007112. Entropy: 0.360027.\n",
      "Iteration 6090: Policy loss: -0.016487. Value loss: 0.006302. Entropy: 0.371480.\n",
      "episode: 4322   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 571     evaluation reward: 3.86\n",
      "episode: 4323   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 426     evaluation reward: 3.87\n",
      "episode: 4324   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 393     evaluation reward: 3.86\n",
      "Training network. lr: 0.000204. clip: 0.081730\n",
      "Iteration 6091: Policy loss: 0.000163. Value loss: 0.009364. Entropy: 0.219947.\n",
      "Iteration 6092: Policy loss: 0.002318. Value loss: 0.007766. Entropy: 0.207036.\n",
      "Iteration 6093: Policy loss: -0.012262. Value loss: 0.006370. Entropy: 0.213964.\n",
      "episode: 4325   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 512     evaluation reward: 3.88\n",
      "episode: 4326   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 309     evaluation reward: 3.87\n",
      "episode: 4327   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 239     evaluation reward: 3.86\n",
      "Training network. lr: 0.000204. clip: 0.081721\n",
      "Iteration 6094: Policy loss: 0.003814. Value loss: 0.012155. Entropy: 0.514335.\n",
      "Iteration 6095: Policy loss: -0.006637. Value loss: 0.008035. Entropy: 0.508102.\n",
      "Iteration 6096: Policy loss: -0.002414. Value loss: 0.007089. Entropy: 0.509118.\n",
      "episode: 4328   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 578     evaluation reward: 3.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4329   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 342     evaluation reward: 3.84\n",
      "Training network. lr: 0.000204. clip: 0.081712\n",
      "Iteration 6097: Policy loss: 0.007323. Value loss: 0.005985. Entropy: 0.316913.\n",
      "Iteration 6098: Policy loss: 0.000926. Value loss: 0.004785. Entropy: 0.318854.\n",
      "Iteration 6099: Policy loss: -0.000774. Value loss: 0.004265. Entropy: 0.311476.\n",
      "episode: 4330   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 528     evaluation reward: 3.85\n",
      "episode: 4331   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 403     evaluation reward: 3.84\n",
      "Training network. lr: 0.000204. clip: 0.081703\n",
      "Iteration 6100: Policy loss: 0.014233. Value loss: 0.010442. Entropy: 0.298563.\n",
      "Iteration 6101: Policy loss: 0.002415. Value loss: 0.008181. Entropy: 0.307831.\n",
      "Iteration 6102: Policy loss: -0.002552. Value loss: 0.007100. Entropy: 0.305983.\n",
      "episode: 4332   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 468     evaluation reward: 3.87\n",
      "episode: 4333   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 454     evaluation reward: 3.87\n",
      "Training network. lr: 0.000204. clip: 0.081694\n",
      "Iteration 6103: Policy loss: 0.011180. Value loss: 0.013546. Entropy: 0.274119.\n",
      "Iteration 6104: Policy loss: -0.004019. Value loss: 0.011150. Entropy: 0.285883.\n",
      "Iteration 6105: Policy loss: -0.012462. Value loss: 0.008915. Entropy: 0.293308.\n",
      "episode: 4334   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 534     evaluation reward: 3.89\n",
      "episode: 4335   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 496     evaluation reward: 3.91\n",
      "episode: 4336   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 327     evaluation reward: 3.89\n",
      "Training network. lr: 0.000204. clip: 0.081685\n",
      "Iteration 6106: Policy loss: 0.008942. Value loss: 0.010712. Entropy: 0.336160.\n",
      "Iteration 6107: Policy loss: -0.008927. Value loss: 0.008881. Entropy: 0.328092.\n",
      "Iteration 6108: Policy loss: -0.010811. Value loss: 0.008040. Entropy: 0.332870.\n",
      "episode: 4337   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 395     evaluation reward: 3.9\n",
      "episode: 4338   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 352     evaluation reward: 3.91\n",
      "episode: 4339   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 322     evaluation reward: 3.89\n",
      "Training network. lr: 0.000204. clip: 0.081676\n",
      "Iteration 6109: Policy loss: 0.003747. Value loss: 0.008966. Entropy: 0.366263.\n",
      "Iteration 6110: Policy loss: -0.001838. Value loss: 0.009183. Entropy: 0.359047.\n",
      "Iteration 6111: Policy loss: -0.007143. Value loss: 0.006447. Entropy: 0.362392.\n",
      "episode: 4340   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 306     evaluation reward: 3.86\n",
      "episode: 4341   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 406     evaluation reward: 3.88\n",
      "Training network. lr: 0.000204. clip: 0.081667\n",
      "Iteration 6112: Policy loss: 0.015950. Value loss: 0.010411. Entropy: 0.318627.\n",
      "Iteration 6113: Policy loss: -0.000109. Value loss: 0.010307. Entropy: 0.313473.\n",
      "Iteration 6114: Policy loss: -0.001425. Value loss: 0.008217. Entropy: 0.314094.\n",
      "episode: 4342   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 724     evaluation reward: 3.91\n",
      "episode: 4343   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 442     evaluation reward: 3.91\n",
      "Training network. lr: 0.000204. clip: 0.081658\n",
      "Iteration 6115: Policy loss: 0.005630. Value loss: 0.006762. Entropy: 0.293147.\n",
      "Iteration 6116: Policy loss: -0.001342. Value loss: 0.005953. Entropy: 0.301922.\n",
      "Iteration 6117: Policy loss: -0.009805. Value loss: 0.005275. Entropy: 0.297214.\n",
      "episode: 4344   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 534     evaluation reward: 3.93\n",
      "episode: 4345   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 604     evaluation reward: 3.93\n",
      "Training network. lr: 0.000204. clip: 0.081649\n",
      "Iteration 6118: Policy loss: -0.004175. Value loss: 0.007262. Entropy: 0.369552.\n",
      "Iteration 6119: Policy loss: -0.013757. Value loss: 0.006314. Entropy: 0.349332.\n",
      "Iteration 6120: Policy loss: -0.014630. Value loss: 0.006077. Entropy: 0.347972.\n",
      "episode: 4346   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 536     evaluation reward: 3.96\n",
      "episode: 4347   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 427     evaluation reward: 3.96\n",
      "Training network. lr: 0.000204. clip: 0.081640\n",
      "Iteration 6121: Policy loss: 0.001956. Value loss: 0.005750. Entropy: 0.257246.\n",
      "Iteration 6122: Policy loss: -0.003894. Value loss: 0.005005. Entropy: 0.255724.\n",
      "Iteration 6123: Policy loss: -0.009198. Value loss: 0.004846. Entropy: 0.255155.\n",
      "episode: 4348   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 507     evaluation reward: 3.96\n",
      "episode: 4349   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 384     evaluation reward: 3.93\n",
      "Training network. lr: 0.000204. clip: 0.081631\n",
      "Iteration 6124: Policy loss: 0.011499. Value loss: 0.007086. Entropy: 0.334104.\n",
      "Iteration 6125: Policy loss: 0.001559. Value loss: 0.005652. Entropy: 0.340524.\n",
      "Iteration 6126: Policy loss: -0.008213. Value loss: 0.005112. Entropy: 0.342116.\n",
      "episode: 4350   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 290     evaluation reward: 3.94\n",
      "episode: 4351   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 325     evaluation reward: 3.92\n",
      "episode: 4352   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 491     evaluation reward: 3.95\n",
      "Training network. lr: 0.000204. clip: 0.081622\n",
      "Iteration 6127: Policy loss: 0.056466. Value loss: 0.112433. Entropy: 0.409338.\n",
      "Iteration 6128: Policy loss: 0.033272. Value loss: 0.051366. Entropy: 0.405769.\n",
      "Iteration 6129: Policy loss: 0.048316. Value loss: 0.019427. Entropy: 0.411631.\n",
      "episode: 4353   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 412     evaluation reward: 3.95\n",
      "episode: 4354   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 348     evaluation reward: 3.92\n",
      "episode: 4355   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 348     evaluation reward: 3.9\n",
      "Training network. lr: 0.000204. clip: 0.081613\n",
      "Iteration 6130: Policy loss: 0.004544. Value loss: 0.008424. Entropy: 0.312530.\n",
      "Iteration 6131: Policy loss: -0.010690. Value loss: 0.006685. Entropy: 0.298394.\n",
      "Iteration 6132: Policy loss: -0.008969. Value loss: 0.005611. Entropy: 0.293062.\n",
      "episode: 4356   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 365     evaluation reward: 3.91\n",
      "episode: 4357   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 447     evaluation reward: 3.91\n",
      "Training network. lr: 0.000204. clip: 0.081604\n",
      "Iteration 6133: Policy loss: 0.011071. Value loss: 0.007135. Entropy: 0.277948.\n",
      "Iteration 6134: Policy loss: -0.002677. Value loss: 0.005749. Entropy: 0.283634.\n",
      "Iteration 6135: Policy loss: -0.008354. Value loss: 0.004778. Entropy: 0.274312.\n",
      "episode: 4358   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 362     evaluation reward: 3.89\n",
      "episode: 4359   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 354     evaluation reward: 3.88\n",
      "episode: 4360   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 443     evaluation reward: 3.88\n",
      "Training network. lr: 0.000204. clip: 0.081595\n",
      "Iteration 6136: Policy loss: 0.008259. Value loss: 0.006215. Entropy: 0.322540.\n",
      "Iteration 6137: Policy loss: -0.008897. Value loss: 0.004613. Entropy: 0.338335.\n",
      "Iteration 6138: Policy loss: -0.010450. Value loss: 0.004451. Entropy: 0.332981.\n",
      "episode: 4361   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 436     evaluation reward: 3.9\n",
      "episode: 4362   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 324     evaluation reward: 3.89\n",
      "episode: 4363   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 360     evaluation reward: 3.89\n",
      "Training network. lr: 0.000204. clip: 0.081586\n",
      "Iteration 6139: Policy loss: 0.008438. Value loss: 0.013324. Entropy: 0.388911.\n",
      "Iteration 6140: Policy loss: 0.004315. Value loss: 0.009502. Entropy: 0.373634.\n",
      "Iteration 6141: Policy loss: -0.006708. Value loss: 0.008202. Entropy: 0.363770.\n",
      "episode: 4364   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 304     evaluation reward: 3.89\n",
      "episode: 4365   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 340     evaluation reward: 3.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4366   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 355     evaluation reward: 3.87\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6142: Policy loss: 0.015903. Value loss: 0.007859. Entropy: 0.383651.\n",
      "Iteration 6143: Policy loss: 0.008361. Value loss: 0.006722. Entropy: 0.382484.\n",
      "Iteration 6144: Policy loss: 0.002929. Value loss: 0.006397. Entropy: 0.373023.\n",
      "episode: 4367   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 497     evaluation reward: 3.9\n",
      "episode: 4368   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 353     evaluation reward: 3.89\n",
      "Training network. lr: 0.000204. clip: 0.081568\n",
      "Iteration 6145: Policy loss: 0.005750. Value loss: 0.014065. Entropy: 0.369246.\n",
      "Iteration 6146: Policy loss: -0.002063. Value loss: 0.010717. Entropy: 0.374226.\n",
      "Iteration 6147: Policy loss: -0.008392. Value loss: 0.008733. Entropy: 0.382293.\n",
      "episode: 4369   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 462     evaluation reward: 3.91\n",
      "episode: 4370   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 304     evaluation reward: 3.88\n",
      "episode: 4371   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 321     evaluation reward: 3.87\n",
      "Training network. lr: 0.000204. clip: 0.081559\n",
      "Iteration 6148: Policy loss: 0.011155. Value loss: 0.011012. Entropy: 0.325482.\n",
      "Iteration 6149: Policy loss: -0.005106. Value loss: 0.008671. Entropy: 0.316928.\n",
      "Iteration 6150: Policy loss: -0.003991. Value loss: 0.007589. Entropy: 0.304976.\n",
      "episode: 4372   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 333     evaluation reward: 3.87\n",
      "episode: 4373   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 268     evaluation reward: 3.85\n",
      "now time :  2018-12-26 15:38:32.587903\n",
      "episode: 4374   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 503     evaluation reward: 3.88\n",
      "Training network. lr: 0.000204. clip: 0.081550\n",
      "Iteration 6151: Policy loss: 0.009400. Value loss: 0.014351. Entropy: 0.327436.\n",
      "Iteration 6152: Policy loss: 0.000433. Value loss: 0.010768. Entropy: 0.310152.\n",
      "Iteration 6153: Policy loss: 0.000093. Value loss: 0.009641. Entropy: 0.317948.\n",
      "episode: 4375   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 307     evaluation reward: 3.87\n",
      "episode: 4376   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 414     evaluation reward: 3.86\n",
      "episode: 4377   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 405     evaluation reward: 3.86\n",
      "Training network. lr: 0.000204. clip: 0.081541\n",
      "Iteration 6154: Policy loss: 0.004005. Value loss: 0.006617. Entropy: 0.250697.\n",
      "Iteration 6155: Policy loss: -0.002364. Value loss: 0.006014. Entropy: 0.229849.\n",
      "Iteration 6156: Policy loss: -0.009647. Value loss: 0.004712. Entropy: 0.245519.\n",
      "episode: 4378   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 361     evaluation reward: 3.86\n",
      "episode: 4379   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 369     evaluation reward: 3.88\n",
      "Training network. lr: 0.000204. clip: 0.081532\n",
      "Iteration 6157: Policy loss: 0.002426. Value loss: 0.012207. Entropy: 0.373666.\n",
      "Iteration 6158: Policy loss: 0.006383. Value loss: 0.008170. Entropy: 0.357753.\n",
      "Iteration 6159: Policy loss: -0.006782. Value loss: 0.007069. Entropy: 0.370167.\n",
      "episode: 4380   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 461     evaluation reward: 3.88\n",
      "episode: 4381   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 3.91\n",
      "episode: 4382   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 374     evaluation reward: 3.89\n",
      "Training network. lr: 0.000204. clip: 0.081523\n",
      "Iteration 6160: Policy loss: 0.012439. Value loss: 0.010820. Entropy: 0.312696.\n",
      "Iteration 6161: Policy loss: -0.000328. Value loss: 0.009189. Entropy: 0.312447.\n",
      "Iteration 6162: Policy loss: 0.000799. Value loss: 0.007745. Entropy: 0.317837.\n",
      "episode: 4383   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 350     evaluation reward: 3.85\n",
      "episode: 4384   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 374     evaluation reward: 3.82\n",
      "Training network. lr: 0.000204. clip: 0.081514\n",
      "Iteration 6163: Policy loss: 0.012842. Value loss: 0.010149. Entropy: 0.347856.\n",
      "Iteration 6164: Policy loss: -0.006146. Value loss: 0.009082. Entropy: 0.349028.\n",
      "Iteration 6165: Policy loss: -0.012531. Value loss: 0.007562. Entropy: 0.343161.\n",
      "episode: 4385   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 487     evaluation reward: 3.83\n",
      "episode: 4386   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 379     evaluation reward: 3.79\n",
      "Training network. lr: 0.000204. clip: 0.081505\n",
      "Iteration 6166: Policy loss: 0.009726. Value loss: 0.008551. Entropy: 0.293332.\n",
      "Iteration 6167: Policy loss: 0.000116. Value loss: 0.006832. Entropy: 0.302697.\n",
      "Iteration 6168: Policy loss: -0.004352. Value loss: 0.005862. Entropy: 0.294689.\n",
      "episode: 4387   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 494     evaluation reward: 3.81\n",
      "episode: 4388   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 547     evaluation reward: 3.82\n",
      "episode: 4389   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 313     evaluation reward: 3.81\n",
      "Training network. lr: 0.000204. clip: 0.081496\n",
      "Iteration 6169: Policy loss: 0.009144. Value loss: 0.008737. Entropy: 0.294119.\n",
      "Iteration 6170: Policy loss: -0.000047. Value loss: 0.007355. Entropy: 0.295206.\n",
      "Iteration 6171: Policy loss: 0.000655. Value loss: 0.006258. Entropy: 0.285573.\n",
      "episode: 4390   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 357     evaluation reward: 3.79\n",
      "episode: 4391   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 361     evaluation reward: 3.77\n",
      "episode: 4392   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 442     evaluation reward: 3.76\n",
      "Training network. lr: 0.000204. clip: 0.081487\n",
      "Iteration 6172: Policy loss: 0.004185. Value loss: 0.010428. Entropy: 0.259149.\n",
      "Iteration 6173: Policy loss: -0.005076. Value loss: 0.008053. Entropy: 0.255274.\n",
      "Iteration 6174: Policy loss: -0.005822. Value loss: 0.007438. Entropy: 0.257659.\n",
      "episode: 4393   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 485     evaluation reward: 3.78\n",
      "Training network. lr: 0.000204. clip: 0.081478\n",
      "Iteration 6175: Policy loss: 0.009494. Value loss: 0.004480. Entropy: 0.147205.\n",
      "Iteration 6176: Policy loss: 0.003513. Value loss: 0.003348. Entropy: 0.139646.\n",
      "Iteration 6177: Policy loss: -0.002135. Value loss: 0.002975. Entropy: 0.140734.\n",
      "episode: 4394   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 597     evaluation reward: 3.77\n",
      "episode: 4395   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 564     evaluation reward: 3.8\n",
      "episode: 4396   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 411     evaluation reward: 3.82\n",
      "Training network. lr: 0.000204. clip: 0.081469\n",
      "Iteration 6178: Policy loss: 0.021456. Value loss: 0.010243. Entropy: 0.276886.\n",
      "Iteration 6179: Policy loss: 0.016024. Value loss: 0.007626. Entropy: 0.298962.\n",
      "Iteration 6180: Policy loss: 0.000810. Value loss: 0.006568. Entropy: 0.299610.\n",
      "episode: 4397   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 384     evaluation reward: 3.82\n",
      "episode: 4398   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 264     evaluation reward: 3.79\n",
      "Training network. lr: 0.000204. clip: 0.081460\n",
      "Iteration 6181: Policy loss: 0.005384. Value loss: 0.008704. Entropy: 0.306591.\n",
      "Iteration 6182: Policy loss: -0.002285. Value loss: 0.006159. Entropy: 0.318115.\n",
      "Iteration 6183: Policy loss: -0.008046. Value loss: 0.005555. Entropy: 0.310141.\n",
      "episode: 4399   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 410     evaluation reward: 3.83\n",
      "episode: 4400   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 486     evaluation reward: 3.87\n",
      "episode: 4401   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 326     evaluation reward: 3.89\n",
      "Training network. lr: 0.000204. clip: 0.081451\n",
      "Iteration 6184: Policy loss: 0.001776. Value loss: 0.012807. Entropy: 0.385424.\n",
      "Iteration 6185: Policy loss: -0.006779. Value loss: 0.009383. Entropy: 0.377257.\n",
      "Iteration 6186: Policy loss: -0.012246. Value loss: 0.007902. Entropy: 0.385171.\n",
      "episode: 4402   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 533     evaluation reward: 3.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4403   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 513     evaluation reward: 3.94\n",
      "Training network. lr: 0.000204. clip: 0.081442\n",
      "Iteration 6187: Policy loss: 0.018224. Value loss: 0.008867. Entropy: 0.226253.\n",
      "Iteration 6188: Policy loss: 0.001098. Value loss: 0.007034. Entropy: 0.239473.\n",
      "Iteration 6189: Policy loss: -0.000211. Value loss: 0.006466. Entropy: 0.235732.\n",
      "episode: 4404   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 293     evaluation reward: 3.93\n",
      "episode: 4405   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 450     evaluation reward: 3.95\n",
      "episode: 4406   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 447     evaluation reward: 3.96\n",
      "Training network. lr: 0.000204. clip: 0.081433\n",
      "Iteration 6190: Policy loss: 0.016430. Value loss: 0.013498. Entropy: 0.351308.\n",
      "Iteration 6191: Policy loss: -0.001178. Value loss: 0.010415. Entropy: 0.352530.\n",
      "Iteration 6192: Policy loss: 0.000125. Value loss: 0.008578. Entropy: 0.361932.\n",
      "episode: 4407   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 273     evaluation reward: 3.95\n",
      "episode: 4408   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 415     evaluation reward: 3.97\n",
      "Training network. lr: 0.000204. clip: 0.081424\n",
      "Iteration 6193: Policy loss: 0.007524. Value loss: 0.011197. Entropy: 0.353375.\n",
      "Iteration 6194: Policy loss: 0.027389. Value loss: 0.008675. Entropy: 0.342934.\n",
      "Iteration 6195: Policy loss: 0.021640. Value loss: 0.007362. Entropy: 0.345551.\n",
      "episode: 4409   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 433     evaluation reward: 3.98\n",
      "episode: 4410   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 486     evaluation reward: 3.96\n",
      "episode: 4411   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 411     evaluation reward: 3.96\n",
      "Training network. lr: 0.000204. clip: 0.081415\n",
      "Iteration 6196: Policy loss: 0.001713. Value loss: 0.010216. Entropy: 0.267238.\n",
      "Iteration 6197: Policy loss: 0.002151. Value loss: 0.007155. Entropy: 0.271212.\n",
      "Iteration 6198: Policy loss: -0.009251. Value loss: 0.006222. Entropy: 0.263510.\n",
      "episode: 4412   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 399     evaluation reward: 3.96\n",
      "episode: 4413   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 270     evaluation reward: 3.95\n",
      "episode: 4414   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 268     evaluation reward: 3.94\n",
      "Training network. lr: 0.000204. clip: 0.081406\n",
      "Iteration 6199: Policy loss: 0.023195. Value loss: 0.010546. Entropy: 0.404895.\n",
      "Iteration 6200: Policy loss: 0.000799. Value loss: 0.008298. Entropy: 0.409503.\n",
      "Iteration 6201: Policy loss: -0.005604. Value loss: 0.007421. Entropy: 0.416556.\n",
      "episode: 4415   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 440     evaluation reward: 3.93\n",
      "episode: 4416   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 438     evaluation reward: 3.95\n",
      "Training network. lr: 0.000203. clip: 0.081397\n",
      "Iteration 6202: Policy loss: 0.007379. Value loss: 0.011163. Entropy: 0.357507.\n",
      "Iteration 6203: Policy loss: -0.002419. Value loss: 0.009674. Entropy: 0.345055.\n",
      "Iteration 6204: Policy loss: -0.007947. Value loss: 0.008422. Entropy: 0.351944.\n",
      "episode: 4417   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 717     evaluation reward: 3.96\n",
      "episode: 4418   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 284     evaluation reward: 3.95\n",
      "Training network. lr: 0.000203. clip: 0.081388\n",
      "Iteration 6205: Policy loss: 0.012493. Value loss: 0.006802. Entropy: 0.249871.\n",
      "Iteration 6206: Policy loss: -0.003906. Value loss: 0.005064. Entropy: 0.255744.\n",
      "Iteration 6207: Policy loss: -0.002936. Value loss: 0.004466. Entropy: 0.253187.\n",
      "episode: 4419   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 479     evaluation reward: 3.95\n",
      "episode: 4420   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 313     evaluation reward: 3.93\n",
      "episode: 4421   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 424     evaluation reward: 3.96\n",
      "Training network. lr: 0.000203. clip: 0.081379\n",
      "Iteration 6208: Policy loss: 0.011545. Value loss: 0.008046. Entropy: 0.289299.\n",
      "Iteration 6209: Policy loss: 0.000183. Value loss: 0.007035. Entropy: 0.282547.\n",
      "Iteration 6210: Policy loss: -0.008843. Value loss: 0.006798. Entropy: 0.285746.\n",
      "episode: 4422   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 425     evaluation reward: 3.96\n",
      "episode: 4423   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 380     evaluation reward: 3.96\n",
      "Training network. lr: 0.000203. clip: 0.081370\n",
      "Iteration 6211: Policy loss: 0.014197. Value loss: 0.010880. Entropy: 0.348176.\n",
      "Iteration 6212: Policy loss: 0.005043. Value loss: 0.009882. Entropy: 0.354587.\n",
      "Iteration 6213: Policy loss: 0.000140. Value loss: 0.007119. Entropy: 0.352518.\n",
      "episode: 4424   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 365     evaluation reward: 3.96\n",
      "episode: 4425   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 573     evaluation reward: 3.97\n",
      "episode: 4426   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 272     evaluation reward: 3.97\n",
      "Training network. lr: 0.000203. clip: 0.081361\n",
      "Iteration 6214: Policy loss: 0.013090. Value loss: 0.009154. Entropy: 0.249272.\n",
      "Iteration 6215: Policy loss: 0.007954. Value loss: 0.006890. Entropy: 0.250430.\n",
      "Iteration 6216: Policy loss: -0.004418. Value loss: 0.005422. Entropy: 0.251100.\n",
      "episode: 4427   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 337     evaluation reward: 3.99\n",
      "episode: 4428   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 413     evaluation reward: 3.98\n",
      "episode: 4429   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 324     evaluation reward: 3.98\n",
      "Training network. lr: 0.000203. clip: 0.081352\n",
      "Iteration 6217: Policy loss: 0.017156. Value loss: 0.010186. Entropy: 0.288670.\n",
      "Iteration 6218: Policy loss: -0.003714. Value loss: 0.008698. Entropy: 0.288581.\n",
      "Iteration 6219: Policy loss: -0.008255. Value loss: 0.007849. Entropy: 0.284391.\n",
      "episode: 4430   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 309     evaluation reward: 3.96\n",
      "episode: 4431   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 266     evaluation reward: 3.94\n",
      "episode: 4432   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 435     evaluation reward: 3.92\n",
      "Training network. lr: 0.000203. clip: 0.081343\n",
      "Iteration 6220: Policy loss: 0.015917. Value loss: 0.016003. Entropy: 0.367439.\n",
      "Iteration 6221: Policy loss: -0.003752. Value loss: 0.011330. Entropy: 0.368977.\n",
      "Iteration 6222: Policy loss: -0.003759. Value loss: 0.010064. Entropy: 0.355337.\n",
      "episode: 4433   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 278     evaluation reward: 3.9\n",
      "episode: 4434   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 357     evaluation reward: 3.89\n",
      "episode: 4435   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 286     evaluation reward: 3.85\n",
      "Training network. lr: 0.000203. clip: 0.081334\n",
      "Iteration 6223: Policy loss: 0.020447. Value loss: 0.012321. Entropy: 0.319086.\n",
      "Iteration 6224: Policy loss: 0.019279. Value loss: 0.009891. Entropy: 0.311337.\n",
      "Iteration 6225: Policy loss: 0.007703. Value loss: 0.008633. Entropy: 0.321991.\n",
      "episode: 4436   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 464     evaluation reward: 3.88\n",
      "episode: 4437   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 438     evaluation reward: 3.89\n",
      "Training network. lr: 0.000203. clip: 0.081325\n",
      "Iteration 6226: Policy loss: 0.005695. Value loss: 0.007463. Entropy: 0.253403.\n",
      "Iteration 6227: Policy loss: 0.001253. Value loss: 0.005621. Entropy: 0.265334.\n",
      "Iteration 6228: Policy loss: -0.009042. Value loss: 0.005208. Entropy: 0.259820.\n",
      "episode: 4438   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 436     evaluation reward: 3.88\n",
      "episode: 4439   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 680     evaluation reward: 3.9\n",
      "Training network. lr: 0.000203. clip: 0.081316\n",
      "Iteration 6229: Policy loss: 0.009249. Value loss: 0.008627. Entropy: 0.378707.\n",
      "Iteration 6230: Policy loss: -0.005195. Value loss: 0.006703. Entropy: 0.379881.\n",
      "Iteration 6231: Policy loss: -0.000070. Value loss: 0.005903. Entropy: 0.384739.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4440   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 285     evaluation reward: 3.9\n",
      "episode: 4441   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 355     evaluation reward: 3.9\n",
      "episode: 4442   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 317     evaluation reward: 3.85\n",
      "Training network. lr: 0.000203. clip: 0.081307\n",
      "Iteration 6232: Policy loss: 0.007500. Value loss: 0.014886. Entropy: 0.349011.\n",
      "Iteration 6233: Policy loss: -0.001191. Value loss: 0.011934. Entropy: 0.359718.\n",
      "Iteration 6234: Policy loss: -0.003911. Value loss: 0.010680. Entropy: 0.356230.\n",
      "episode: 4443   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 533     evaluation reward: 3.85\n",
      "episode: 4444   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 389     evaluation reward: 3.84\n",
      "episode: 4445   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 350     evaluation reward: 3.83\n",
      "Training network. lr: 0.000203. clip: 0.081298\n",
      "Iteration 6235: Policy loss: 0.011857. Value loss: 0.009822. Entropy: 0.249030.\n",
      "Iteration 6236: Policy loss: -0.002228. Value loss: 0.007487. Entropy: 0.247220.\n",
      "Iteration 6237: Policy loss: -0.007445. Value loss: 0.006731. Entropy: 0.244368.\n",
      "episode: 4446   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 435     evaluation reward: 3.81\n",
      "episode: 4447   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 312     evaluation reward: 3.79\n",
      "Training network. lr: 0.000203. clip: 0.081289\n",
      "Iteration 6238: Policy loss: 0.011463. Value loss: 0.008735. Entropy: 0.295379.\n",
      "Iteration 6239: Policy loss: 0.002693. Value loss: 0.007911. Entropy: 0.290726.\n",
      "Iteration 6240: Policy loss: -0.006809. Value loss: 0.006987. Entropy: 0.283659.\n",
      "episode: 4448   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 463     evaluation reward: 3.79\n",
      "episode: 4449   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 380     evaluation reward: 3.8\n",
      "episode: 4450   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 371     evaluation reward: 3.81\n",
      "Training network. lr: 0.000203. clip: 0.081280\n",
      "Iteration 6241: Policy loss: 0.008142. Value loss: 0.008859. Entropy: 0.289779.\n",
      "Iteration 6242: Policy loss: -0.002476. Value loss: 0.006573. Entropy: 0.300353.\n",
      "Iteration 6243: Policy loss: -0.007816. Value loss: 0.005551. Entropy: 0.305036.\n",
      "episode: 4451   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 484     evaluation reward: 3.84\n",
      "episode: 4452   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 470     evaluation reward: 3.82\n",
      "Training network. lr: 0.000203. clip: 0.081271\n",
      "Iteration 6244: Policy loss: 0.001989. Value loss: 0.009033. Entropy: 0.411515.\n",
      "Iteration 6245: Policy loss: -0.013166. Value loss: 0.007016. Entropy: 0.414436.\n",
      "Iteration 6246: Policy loss: -0.014250. Value loss: 0.006036. Entropy: 0.411643.\n",
      "episode: 4453   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 334     evaluation reward: 3.81\n",
      "episode: 4454   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 318     evaluation reward: 3.79\n",
      "episode: 4455   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 412     evaluation reward: 3.8\n",
      "Training network. lr: 0.000203. clip: 0.081262\n",
      "Iteration 6247: Policy loss: 0.014400. Value loss: 0.007103. Entropy: 0.275481.\n",
      "Iteration 6248: Policy loss: -0.000298. Value loss: 0.005801. Entropy: 0.274097.\n",
      "Iteration 6249: Policy loss: -0.005419. Value loss: 0.005151. Entropy: 0.266504.\n",
      "episode: 4456   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 308     evaluation reward: 3.78\n",
      "episode: 4457   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 399     evaluation reward: 3.75\n",
      "episode: 4458   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 438     evaluation reward: 3.76\n",
      "Training network. lr: 0.000203. clip: 0.081253\n",
      "Iteration 6250: Policy loss: 0.010095. Value loss: 0.009264. Entropy: 0.341725.\n",
      "Iteration 6251: Policy loss: 0.001328. Value loss: 0.006663. Entropy: 0.331829.\n",
      "Iteration 6252: Policy loss: -0.002511. Value loss: 0.005614. Entropy: 0.331423.\n",
      "episode: 4459   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 274     evaluation reward: 3.74\n",
      "episode: 4460   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 306     evaluation reward: 3.71\n",
      "episode: 4461   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 344     evaluation reward: 3.69\n",
      "Training network. lr: 0.000203. clip: 0.081244\n",
      "Iteration 6253: Policy loss: 0.037000. Value loss: 0.013573. Entropy: 0.422261.\n",
      "Iteration 6254: Policy loss: -0.001489. Value loss: 0.013301. Entropy: 0.416417.\n",
      "Iteration 6255: Policy loss: -0.001419. Value loss: 0.008553. Entropy: 0.413171.\n",
      "episode: 4462   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 354     evaluation reward: 3.69\n",
      "episode: 4463   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 470     evaluation reward: 3.72\n",
      "Training network. lr: 0.000203. clip: 0.081235\n",
      "Iteration 6256: Policy loss: 0.004315. Value loss: 0.008191. Entropy: 0.295952.\n",
      "Iteration 6257: Policy loss: 0.006029. Value loss: 0.006369. Entropy: 0.307824.\n",
      "Iteration 6258: Policy loss: -0.005713. Value loss: 0.005308. Entropy: 0.306085.\n",
      "episode: 4464   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 578     evaluation reward: 3.75\n",
      "episode: 4465   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 310     evaluation reward: 3.75\n",
      "episode: 4466   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 408     evaluation reward: 3.75\n",
      "Training network. lr: 0.000203. clip: 0.081226\n",
      "Iteration 6259: Policy loss: 0.016669. Value loss: 0.006399. Entropy: 0.227821.\n",
      "Iteration 6260: Policy loss: 0.003511. Value loss: 0.005153. Entropy: 0.231824.\n",
      "Iteration 6261: Policy loss: 0.002394. Value loss: 0.004965. Entropy: 0.223157.\n",
      "episode: 4467   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 461     evaluation reward: 3.74\n",
      "episode: 4468   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 529     evaluation reward: 3.77\n",
      "Training network. lr: 0.000203. clip: 0.081217\n",
      "Iteration 6262: Policy loss: 0.015855. Value loss: 0.005690. Entropy: 0.219286.\n",
      "Iteration 6263: Policy loss: 0.003481. Value loss: 0.004581. Entropy: 0.206727.\n",
      "Iteration 6264: Policy loss: -0.001022. Value loss: 0.004185. Entropy: 0.214477.\n",
      "episode: 4469   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 443     evaluation reward: 3.76\n",
      "episode: 4470   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 352     evaluation reward: 3.77\n",
      "Training network. lr: 0.000203. clip: 0.081208\n",
      "Iteration 6265: Policy loss: 0.003153. Value loss: 0.009522. Entropy: 0.363763.\n",
      "Iteration 6266: Policy loss: -0.006713. Value loss: 0.009393. Entropy: 0.356597.\n",
      "Iteration 6267: Policy loss: -0.007886. Value loss: 0.006497. Entropy: 0.354672.\n",
      "episode: 4471   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 479     evaluation reward: 3.79\n",
      "episode: 4472   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 305     evaluation reward: 3.78\n",
      "episode: 4473   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 334     evaluation reward: 3.8\n",
      "Training network. lr: 0.000203. clip: 0.081199\n",
      "Iteration 6268: Policy loss: 0.015735. Value loss: 0.010118. Entropy: 0.266448.\n",
      "Iteration 6269: Policy loss: -0.001437. Value loss: 0.006949. Entropy: 0.264237.\n",
      "Iteration 6270: Policy loss: 0.001529. Value loss: 0.005604. Entropy: 0.265392.\n",
      "episode: 4474   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 412     evaluation reward: 3.78\n",
      "episode: 4475   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 447     evaluation reward: 3.8\n",
      "Training network. lr: 0.000203. clip: 0.081190\n",
      "Iteration 6271: Policy loss: -0.004227. Value loss: 0.009584. Entropy: 0.274796.\n",
      "Iteration 6272: Policy loss: -0.012855. Value loss: 0.006982. Entropy: 0.279201.\n",
      "Iteration 6273: Policy loss: -0.017914. Value loss: 0.006329. Entropy: 0.294279.\n",
      "episode: 4476   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 459     evaluation reward: 3.81\n",
      "episode: 4477   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 471     evaluation reward: 3.82\n",
      "episode: 4478   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 385     evaluation reward: 3.82\n",
      "Training network. lr: 0.000203. clip: 0.081181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6274: Policy loss: 0.002906. Value loss: 0.007019. Entropy: 0.282886.\n",
      "Iteration 6275: Policy loss: 0.001060. Value loss: 0.004801. Entropy: 0.294622.\n",
      "Iteration 6276: Policy loss: -0.004072. Value loss: 0.004206. Entropy: 0.280899.\n",
      "episode: 4479   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 438     evaluation reward: 3.83\n",
      "episode: 4480   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 501     evaluation reward: 3.83\n",
      "Training network. lr: 0.000203. clip: 0.081172\n",
      "Iteration 6277: Policy loss: 0.016113. Value loss: 0.007262. Entropy: 0.226870.\n",
      "Iteration 6278: Policy loss: 0.001680. Value loss: 0.005102. Entropy: 0.215612.\n",
      "Iteration 6279: Policy loss: -0.008918. Value loss: 0.004499. Entropy: 0.218053.\n",
      "episode: 4481   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 428     evaluation reward: 3.82\n",
      "episode: 4482   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 450     evaluation reward: 3.84\n",
      "episode: 4483   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 348     evaluation reward: 3.84\n",
      "Training network. lr: 0.000203. clip: 0.081163\n",
      "Iteration 6280: Policy loss: 0.014193. Value loss: 0.007616. Entropy: 0.268474.\n",
      "Iteration 6281: Policy loss: 0.002075. Value loss: 0.005400. Entropy: 0.276042.\n",
      "Iteration 6282: Policy loss: -0.005907. Value loss: 0.004727. Entropy: 0.265215.\n",
      "episode: 4484   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 273     evaluation reward: 3.83\n",
      "episode: 4485   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 295     evaluation reward: 3.79\n",
      "Training network. lr: 0.000203. clip: 0.081154\n",
      "Iteration 6283: Policy loss: 0.018848. Value loss: 0.012866. Entropy: 0.346755.\n",
      "Iteration 6284: Policy loss: -0.001658. Value loss: 0.009801. Entropy: 0.355410.\n",
      "Iteration 6285: Policy loss: -0.007202. Value loss: 0.008041. Entropy: 0.342982.\n",
      "episode: 4486   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 461     evaluation reward: 3.81\n",
      "episode: 4487   score: 0.0   memory length: 1024   epsilon: 1.0    steps: 319     evaluation reward: 3.76\n",
      "episode: 4488   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 298     evaluation reward: 3.74\n",
      "episode: 4489   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 375     evaluation reward: 3.74\n",
      "Training network. lr: 0.000203. clip: 0.081145\n",
      "Iteration 6286: Policy loss: 0.001401. Value loss: 0.009938. Entropy: 0.370102.\n",
      "Iteration 6287: Policy loss: -0.001857. Value loss: 0.006771. Entropy: 0.353845.\n",
      "Iteration 6288: Policy loss: -0.005213. Value loss: 0.005372. Entropy: 0.338286.\n",
      "episode: 4490   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 332     evaluation reward: 3.73\n",
      "episode: 4491   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 227     evaluation reward: 3.71\n",
      "Training network. lr: 0.000203. clip: 0.081136\n",
      "Iteration 6289: Policy loss: 0.000687. Value loss: 0.008311. Entropy: 0.391396.\n",
      "Iteration 6290: Policy loss: -0.005444. Value loss: 0.006099. Entropy: 0.386794.\n",
      "Iteration 6291: Policy loss: -0.007557. Value loss: 0.005320. Entropy: 0.390319.\n",
      "episode: 4492   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 497     evaluation reward: 3.71\n",
      "episode: 4493   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 397     evaluation reward: 3.71\n",
      "episode: 4494   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 409     evaluation reward: 3.71\n",
      "Training network. lr: 0.000203. clip: 0.081127\n",
      "Iteration 6292: Policy loss: 0.007412. Value loss: 0.008233. Entropy: 0.203616.\n",
      "Iteration 6293: Policy loss: 0.006254. Value loss: 0.007820. Entropy: 0.199455.\n",
      "Iteration 6294: Policy loss: -0.001087. Value loss: 0.005679. Entropy: 0.199475.\n",
      "episode: 4495   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 352     evaluation reward: 3.69\n",
      "episode: 4496   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 414     evaluation reward: 3.68\n",
      "episode: 4497   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 273     evaluation reward: 3.65\n",
      "Training network. lr: 0.000203. clip: 0.081118\n",
      "Iteration 6295: Policy loss: 0.005715. Value loss: 0.009369. Entropy: 0.328923.\n",
      "Iteration 6296: Policy loss: -0.000436. Value loss: 0.008159. Entropy: 0.342791.\n",
      "Iteration 6297: Policy loss: -0.002794. Value loss: 0.007076. Entropy: 0.331103.\n",
      "episode: 4498   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 413     evaluation reward: 3.67\n",
      "now time :  2018-12-26 15:44:16.343498\n",
      "episode: 4499   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 445     evaluation reward: 3.69\n",
      "episode: 4500   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 354     evaluation reward: 3.66\n",
      "Training network. lr: 0.000203. clip: 0.081109\n",
      "Iteration 6298: Policy loss: 0.009323. Value loss: 0.011760. Entropy: 0.337455.\n",
      "Iteration 6299: Policy loss: -0.014018. Value loss: 0.009297. Entropy: 0.329738.\n",
      "Iteration 6300: Policy loss: -0.013048. Value loss: 0.008357. Entropy: 0.327803.\n",
      "episode: 4501   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 402     evaluation reward: 3.67\n",
      "episode: 4502   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 425     evaluation reward: 3.65\n",
      "Training network. lr: 0.000203. clip: 0.081100\n",
      "Iteration 6301: Policy loss: 0.012968. Value loss: 0.008245. Entropy: 0.189266.\n",
      "Iteration 6302: Policy loss: 0.004663. Value loss: 0.007323. Entropy: 0.197340.\n",
      "Iteration 6303: Policy loss: -0.008124. Value loss: 0.006452. Entropy: 0.185343.\n",
      "episode: 4503   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 538     evaluation reward: 3.64\n",
      "episode: 4504   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 273     evaluation reward: 3.64\n",
      "episode: 4505   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 331     evaluation reward: 3.63\n",
      "Training network. lr: 0.000203. clip: 0.081091\n",
      "Iteration 6304: Policy loss: 0.010720. Value loss: 0.092945. Entropy: 0.326749.\n",
      "Iteration 6305: Policy loss: 0.023500. Value loss: 0.046844. Entropy: 0.322417.\n",
      "Iteration 6306: Policy loss: 0.003033. Value loss: 0.019601. Entropy: 0.312262.\n",
      "episode: 4506   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 252     evaluation reward: 3.6\n",
      "episode: 4507   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 341     evaluation reward: 3.61\n",
      "episode: 4508   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 518     evaluation reward: 3.61\n",
      "Training network. lr: 0.000203. clip: 0.081082\n",
      "Iteration 6307: Policy loss: 0.004431. Value loss: 0.008995. Entropy: 0.280982.\n",
      "Iteration 6308: Policy loss: -0.000845. Value loss: 0.006497. Entropy: 0.268179.\n",
      "Iteration 6309: Policy loss: -0.003820. Value loss: 0.005059. Entropy: 0.266613.\n",
      "episode: 4509   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 393     evaluation reward: 3.59\n",
      "episode: 4510   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 372     evaluation reward: 3.58\n",
      "Training network. lr: 0.000203. clip: 0.081073\n",
      "Iteration 6310: Policy loss: 0.004661. Value loss: 0.009609. Entropy: 0.336557.\n",
      "Iteration 6311: Policy loss: -0.006142. Value loss: 0.007879. Entropy: 0.338907.\n",
      "Iteration 6312: Policy loss: -0.009199. Value loss: 0.007665. Entropy: 0.337771.\n",
      "episode: 4511   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 362     evaluation reward: 3.57\n",
      "episode: 4512   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 390     evaluation reward: 3.56\n",
      "episode: 4513   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 334     evaluation reward: 3.58\n",
      "Training network. lr: 0.000203. clip: 0.081064\n",
      "Iteration 6313: Policy loss: 0.007079. Value loss: 0.007795. Entropy: 0.261498.\n",
      "Iteration 6314: Policy loss: -0.004801. Value loss: 0.006166. Entropy: 0.262568.\n",
      "Iteration 6315: Policy loss: -0.004389. Value loss: 0.005467. Entropy: 0.263369.\n",
      "episode: 4514   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 408     evaluation reward: 3.6\n",
      "episode: 4515   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 3.56\n",
      "Training network. lr: 0.000203. clip: 0.081055\n",
      "Iteration 6316: Policy loss: -0.006657. Value loss: 0.005755. Entropy: 0.304883.\n",
      "Iteration 6317: Policy loss: -0.006473. Value loss: 0.004562. Entropy: 0.310953.\n",
      "Iteration 6318: Policy loss: -0.012286. Value loss: 0.004108. Entropy: 0.306530.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4516   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 544     evaluation reward: 3.57\n",
      "episode: 4517   score: 6.0   memory length: 1024   epsilon: 1.0    steps: 509     evaluation reward: 3.59\n",
      "episode: 4518   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 365     evaluation reward: 3.61\n",
      "Training network. lr: 0.000203. clip: 0.081046\n",
      "Iteration 6319: Policy loss: 0.009383. Value loss: 0.009285. Entropy: 0.294036.\n",
      "Iteration 6320: Policy loss: 0.001861. Value loss: 0.008770. Entropy: 0.300908.\n",
      "Iteration 6321: Policy loss: -0.005476. Value loss: 0.007693. Entropy: 0.293728.\n",
      "episode: 4519   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 382     evaluation reward: 3.6\n",
      "episode: 4520   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 438     evaluation reward: 3.61\n",
      "Training network. lr: 0.000203. clip: 0.081037\n",
      "Iteration 6322: Policy loss: 0.006909. Value loss: 0.009491. Entropy: 0.300893.\n",
      "Iteration 6323: Policy loss: -0.008531. Value loss: 0.007769. Entropy: 0.310232.\n",
      "Iteration 6324: Policy loss: -0.009419. Value loss: 0.006285. Entropy: 0.309743.\n",
      "episode: 4521   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 322     evaluation reward: 3.59\n",
      "episode: 4522   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 379     evaluation reward: 3.58\n",
      "episode: 4523   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 420     evaluation reward: 3.58\n",
      "Training network. lr: 0.000203. clip: 0.081028\n",
      "Iteration 6325: Policy loss: 0.040587. Value loss: 0.006396. Entropy: 0.215994.\n",
      "Iteration 6326: Policy loss: 0.008080. Value loss: 0.004680. Entropy: 0.202505.\n",
      "Iteration 6327: Policy loss: 0.065356. Value loss: 0.004140. Entropy: 0.207856.\n",
      "episode: 4524   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 506     evaluation reward: 3.59\n",
      "episode: 4525   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 250     evaluation reward: 3.54\n",
      "episode: 4526   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 297     evaluation reward: 3.55\n",
      "Training network. lr: 0.000203. clip: 0.081019\n",
      "Iteration 6328: Policy loss: 0.009170. Value loss: 0.013185. Entropy: 0.328343.\n",
      "Iteration 6329: Policy loss: -0.007715. Value loss: 0.010289. Entropy: 0.322685.\n",
      "Iteration 6330: Policy loss: -0.014272. Value loss: 0.008521. Entropy: 0.329980.\n",
      "episode: 4527   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 297     evaluation reward: 3.55\n",
      "episode: 4528   score: 7.0   memory length: 1024   epsilon: 1.0    steps: 752     evaluation reward: 3.58\n",
      "Training network. lr: 0.000203. clip: 0.081010\n",
      "Iteration 6331: Policy loss: 0.001023. Value loss: 0.009025. Entropy: 0.240830.\n",
      "Iteration 6332: Policy loss: -0.008212. Value loss: 0.007761. Entropy: 0.244284.\n",
      "Iteration 6333: Policy loss: -0.004589. Value loss: 0.006620. Entropy: 0.243389.\n",
      "episode: 4529   score: 1.0   memory length: 1024   epsilon: 1.0    steps: 218     evaluation reward: 3.56\n",
      "episode: 4530   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 336     evaluation reward: 3.57\n",
      "episode: 4531   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 373     evaluation reward: 3.59\n",
      "Training network. lr: 0.000203. clip: 0.081001\n",
      "Iteration 6334: Policy loss: 0.006710. Value loss: 0.007772. Entropy: 0.361223.\n",
      "Iteration 6335: Policy loss: 0.005440. Value loss: 0.006659. Entropy: 0.356508.\n",
      "Iteration 6336: Policy loss: 0.003258. Value loss: 0.005738. Entropy: 0.357624.\n",
      "episode: 4532   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 354     evaluation reward: 3.59\n",
      "episode: 4533   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 341     evaluation reward: 3.59\n",
      "episode: 4534   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 377     evaluation reward: 3.59\n",
      "Training network. lr: 0.000202. clip: 0.080992\n",
      "Iteration 6337: Policy loss: 0.014206. Value loss: 0.008873. Entropy: 0.244371.\n",
      "Iteration 6338: Policy loss: 0.001425. Value loss: 0.007180. Entropy: 0.247934.\n",
      "Iteration 6339: Policy loss: 0.005805. Value loss: 0.006598. Entropy: 0.246938.\n",
      "episode: 4535   score: 4.0   memory length: 1024   epsilon: 1.0    steps: 393     evaluation reward: 3.61\n",
      "episode: 4536   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 352     evaluation reward: 3.59\n",
      "Training network. lr: 0.000202. clip: 0.080983\n",
      "Iteration 6340: Policy loss: 0.001132. Value loss: 0.006606. Entropy: 0.308200.\n",
      "Iteration 6341: Policy loss: -0.005311. Value loss: 0.005166. Entropy: 0.298944.\n",
      "Iteration 6342: Policy loss: -0.008282. Value loss: 0.004214. Entropy: 0.311677.\n",
      "episode: 4537   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 488     evaluation reward: 3.59\n",
      "episode: 4538   score: 2.0   memory length: 1024   epsilon: 1.0    steps: 296     evaluation reward: 3.59\n",
      "episode: 4539   score: 3.0   memory length: 1024   epsilon: 1.0    steps: 391     evaluation reward: 3.58\n",
      "Training network. lr: 0.000202. clip: 0.080974\n",
      "Iteration 6343: Policy loss: 0.009555. Value loss: 0.008477. Entropy: 0.358096.\n",
      "Iteration 6344: Policy loss: 0.002447. Value loss: 0.006410. Entropy: 0.348648.\n",
      "Iteration 6345: Policy loss: -0.003201. Value loss: 0.005650. Entropy: 0.346362.\n",
      "episode: 4540   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 457     evaluation reward: 3.61\n",
      "episode: 4541   score: 8.0   memory length: 1024   epsilon: 1.0    steps: 462     evaluation reward: 3.66\n",
      "Training network. lr: 0.000202. clip: 0.080965\n",
      "Iteration 6346: Policy loss: 0.013388. Value loss: 0.034783. Entropy: 0.171215.\n",
      "Iteration 6347: Policy loss: 0.001406. Value loss: 0.029033. Entropy: 0.169831.\n",
      "Iteration 6348: Policy loss: -0.006443. Value loss: 0.023029. Entropy: 0.173725.\n",
      "episode: 4542   score: 5.0   memory length: 1024   epsilon: 1.0    steps: 514     evaluation reward: 3.69\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        if render_breakout:\n",
    "            env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        curr_state = history[3,:,:]\n",
    "        action, value = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        #r = np.clip(reward, -1, 1)\n",
    "        r = reward\n",
    "        \"\"\"\n",
    "        if terminal_state:\n",
    "            r -= 20\n",
    "        \"\"\"\n",
    "        # Store the transition in memory \n",
    "        \n",
    "        agent.memory.push(deepcopy(curr_state), action, r, terminal_state, value, 0, 0)\n",
    "        # Start training after random sample generation\n",
    "        if(frame % train_frame == 0):\n",
    "            _, frame_next_val = agent.get_action(np.float32(history[1:, :, :]) / 255.)\n",
    "            agent.train_policy_net(frame, frame_next_val)\n",
    "            # Update the target network\n",
    "            agent.update_target_net()\n",
    "        score += r\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > 700 and len(evaluation_reward) > 40:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn\")\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
