{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "from env import GameEnv\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/anaconda3/envs/py36/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "envs = []\n",
    "for i in range(num_envs):\n",
    "    envs.append(GameEnv('SpaceInvadersDeterministic-v4'))\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = envs[0].life\n",
    "state_size = envs[0].observation_space.shape\n",
    "action_size = envs[0].action_space.n\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " ------- STARTING TRAINING FOR SpaceInvaders-v4 ------- \n",
      "\n",
      "\n",
      "\n",
      "Determing min/max rewards of environment\n",
      "Min: 0. Max: 200.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],1)[0].unsqueeze(1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000250. clip: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:276: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:277: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:278: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Policy loss: 0.000542. Value loss: 0.009386. Entropy: 1.788949.\n",
      "load: 0.222877\n",
      "forward: 0.028469\n",
      "loss: 0.078945\n",
      "clip: 0.000004\n",
      "backward: 0.029757\n",
      "step: 0.013512\n",
      "total: 0.373564\n",
      "\n",
      "Iteration 2: Policy loss: -0.002922. Value loss: 0.009325. Entropy: 1.789178.\n",
      "load: 0.217304\n",
      "forward: 0.029601\n",
      "loss: 0.075127\n",
      "clip: 0.000005\n",
      "backward: 0.030560\n",
      "step: 0.014437\n",
      "total: 0.367034\n",
      "\n",
      "Iteration 3: Policy loss: -0.001228. Value loss: 0.009557. Entropy: 1.788826.\n",
      "load: 0.218923\n",
      "forward: 0.027762\n",
      "loss: 0.076977\n",
      "clip: 0.000011\n",
      "backward: 0.031800\n",
      "step: 0.013444\n",
      "total: 0.368917\n",
      "\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 4: Policy loss: 0.001917. Value loss: 0.194006. Entropy: 1.785831.\n",
      "load: 0.206221\n",
      "forward: 0.028512\n",
      "loss: 0.076011\n",
      "clip: 0.000003\n",
      "backward: 0.028849\n",
      "step: 0.013279\n",
      "total: 0.352875\n",
      "\n",
      "Iteration 5: Policy loss: -0.002426. Value loss: 0.155647. Entropy: 1.788443.\n",
      "load: 0.203776\n",
      "forward: 0.028186\n",
      "loss: 0.075319\n",
      "clip: 0.000004\n",
      "backward: 0.029050\n",
      "step: 0.013371\n",
      "total: 0.349705\n",
      "\n",
      "Iteration 6: Policy loss: -0.002379. Value loss: 0.148882. Entropy: 1.787544.\n",
      "load: 0.203162\n",
      "forward: 0.028069\n",
      "loss: 0.073910\n",
      "clip: 0.000004\n",
      "backward: 0.031333\n",
      "step: 0.013229\n",
      "total: 0.349708\n",
      "\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 7: Policy loss: 0.002706. Value loss: 0.200490. Entropy: 1.789087.\n",
      "load: 0.213641\n",
      "forward: 0.029565\n",
      "loss: 0.072493\n",
      "clip: 0.000005\n",
      "backward: 0.028801\n",
      "step: 0.014145\n",
      "total: 0.358650\n",
      "\n",
      "Iteration 8: Policy loss: 0.001243. Value loss: 0.178200. Entropy: 1.788737.\n",
      "load: 0.213227\n",
      "forward: 0.028635\n",
      "loss: 0.073483\n",
      "clip: 0.000004\n",
      "backward: 0.029747\n",
      "step: 0.013004\n",
      "total: 0.358099\n",
      "\n",
      "Iteration 9: Policy loss: -0.001784. Value loss: 0.160686. Entropy: 1.787146.\n",
      "load: 0.219390\n",
      "forward: 0.029118\n",
      "loss: 0.073140\n",
      "clip: 0.000005\n",
      "backward: 0.029197\n",
      "step: 0.013984\n",
      "total: 0.364833\n",
      "\n",
      "now time :  2019-08-30 14:08:38.261155\n",
      "episode: 1   score: 65.0  epsilon: 1.0    steps: 448  evaluation reward: 65.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/sigai/.local/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 10: Policy loss: 0.007812. Value loss: 0.090573. Entropy: 1.788031.\n",
      "load: 0.209718\n",
      "forward: 0.028206\n",
      "loss: 0.074531\n",
      "clip: 0.000003\n",
      "backward: 0.029182\n",
      "step: 0.013379\n",
      "total: 0.355019\n",
      "\n",
      "Iteration 11: Policy loss: -0.000473. Value loss: 0.058757. Entropy: 1.790620.\n",
      "load: 0.213780\n",
      "forward: 0.028509\n",
      "loss: 0.073828\n",
      "clip: 0.000005\n",
      "backward: 0.029465\n",
      "step: 0.013089\n",
      "total: 0.358677\n",
      "\n",
      "Iteration 12: Policy loss: -0.003563. Value loss: 0.050626. Entropy: 1.790224.\n",
      "load: 0.218368\n",
      "forward: 0.029473\n",
      "loss: 0.073363\n",
      "clip: 0.000004\n",
      "backward: 0.029905\n",
      "step: 0.014127\n",
      "total: 0.365238\n",
      "\n",
      "episode: 2   score: 85.0  epsilon: 1.0    steps: 1024  evaluation reward: 75.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 13: Policy loss: 0.004388. Value loss: 0.113065. Entropy: 1.788931.\n",
      "load: 0.207942\n",
      "forward: 0.027733\n",
      "loss: 0.073529\n",
      "clip: 0.000003\n",
      "backward: 0.029806\n",
      "step: 0.013783\n",
      "total: 0.352796\n",
      "\n",
      "Iteration 14: Policy loss: -0.000368. Value loss: 0.081253. Entropy: 1.787037.\n",
      "load: 0.209973\n",
      "forward: 0.028249\n",
      "loss: 0.071378\n",
      "clip: 0.000014\n",
      "backward: 0.028324\n",
      "step: 0.013879\n",
      "total: 0.351818\n",
      "\n",
      "Iteration 15: Policy loss: -0.002437. Value loss: 0.061882. Entropy: 1.786896.\n",
      "load: 0.213163\n",
      "forward: 0.027753\n",
      "loss: 0.071799\n",
      "clip: 0.000005\n",
      "backward: 0.032770\n",
      "step: 0.013895\n",
      "total: 0.359385\n",
      "\n",
      "episode: 3   score: 105.0  epsilon: 1.0    steps: 384  evaluation reward: 85.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 16: Policy loss: 0.003838. Value loss: 0.169433. Entropy: 1.779498.\n",
      "load: 0.204802\n",
      "forward: 0.030429\n",
      "loss: 0.069213\n",
      "clip: 0.000005\n",
      "backward: 0.030750\n",
      "step: 0.013550\n",
      "total: 0.348748\n",
      "\n",
      "Iteration 17: Policy loss: -0.002149. Value loss: 0.124651. Entropy: 1.780753.\n",
      "load: 0.206828\n",
      "forward: 0.028255\n",
      "loss: 0.071265\n",
      "clip: 0.000014\n",
      "backward: 0.030428\n",
      "step: 0.013042\n",
      "total: 0.349832\n",
      "\n",
      "Iteration 18: Policy loss: -0.006005. Value loss: 0.098730. Entropy: 1.782234.\n",
      "load: 0.209872\n",
      "forward: 0.028282\n",
      "loss: 0.071629\n",
      "clip: 0.000004\n",
      "backward: 0.028710\n",
      "step: 0.014578\n",
      "total: 0.353076\n",
      "\n",
      "episode: 4   score: 210.0  epsilon: 1.0    steps: 8  evaluation reward: 116.25\n",
      "episode: 5   score: 180.0  epsilon: 1.0    steps: 104  evaluation reward: 129.0\n",
      "episode: 6   score: 155.0  epsilon: 1.0    steps: 104  evaluation reward: 133.33333333333334\n",
      "episode: 7   score: 210.0  epsilon: 1.0    steps: 128  evaluation reward: 144.28571428571428\n",
      "episode: 8   score: 40.0  epsilon: 1.0    steps: 616  evaluation reward: 131.25\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 19: Policy loss: 0.000975. Value loss: 0.043087. Entropy: 1.781210.\n",
      "load: 0.208841\n",
      "forward: 0.027232\n",
      "loss: 0.072750\n",
      "clip: 0.000004\n",
      "backward: 0.037248\n",
      "step: 0.013453\n",
      "total: 0.359528\n",
      "\n",
      "Iteration 20: Policy loss: -0.004898. Value loss: 0.024512. Entropy: 1.783062.\n",
      "load: 0.215196\n",
      "forward: 0.028851\n",
      "loss: 0.071223\n",
      "clip: 0.000006\n",
      "backward: 0.034603\n",
      "step: 0.014037\n",
      "total: 0.363916\n",
      "\n",
      "Iteration 21: Policy loss: -0.006328. Value loss: 0.019349. Entropy: 1.780753.\n",
      "load: 0.211830\n",
      "forward: 0.027292\n",
      "loss: 0.071549\n",
      "clip: 0.000004\n",
      "backward: 0.029857\n",
      "step: 0.013347\n",
      "total: 0.353880\n",
      "\n",
      "episode: 9   score: 225.0  epsilon: 1.0    steps: 32  evaluation reward: 141.66666666666666\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 22: Policy loss: 0.001017. Value loss: 0.165039. Entropy: 1.784274.\n",
      "load: 0.211187\n",
      "forward: 0.030870\n",
      "loss: 0.069155\n",
      "clip: 0.000005\n",
      "backward: 0.031776\n",
      "step: 0.013542\n",
      "total: 0.356534\n",
      "\n",
      "Iteration 23: Policy loss: -0.001855. Value loss: 0.129260. Entropy: 1.781376.\n",
      "load: 0.213841\n",
      "forward: 0.028735\n",
      "loss: 0.072433\n",
      "clip: 0.000004\n",
      "backward: 0.030945\n",
      "step: 0.014137\n",
      "total: 0.360096\n",
      "\n",
      "Iteration 24: Policy loss: -0.001243. Value loss: 0.110365. Entropy: 1.780401.\n",
      "load: 0.225701\n",
      "forward: 0.032657\n",
      "loss: 0.068283\n",
      "clip: 0.000006\n",
      "backward: 0.032619\n",
      "step: 0.014910\n",
      "total: 0.374176\n",
      "\n",
      "episode: 10   score: 65.0  epsilon: 1.0    steps: 296  evaluation reward: 134.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 25: Policy loss: -0.000177. Value loss: 0.149921. Entropy: 1.783545.\n",
      "load: 0.214437\n",
      "forward: 0.028675\n",
      "loss: 0.071807\n",
      "clip: 0.000014\n",
      "backward: 0.031511\n",
      "step: 0.014564\n",
      "total: 0.361007\n",
      "\n",
      "Iteration 26: Policy loss: -0.000712. Value loss: 0.106105. Entropy: 1.779969.\n",
      "load: 0.221951\n",
      "forward: 0.033209\n",
      "loss: 0.067893\n",
      "clip: 0.000005\n",
      "backward: 0.029989\n",
      "step: 0.016594\n",
      "total: 0.369641\n",
      "\n",
      "Iteration 27: Policy loss: -0.003209. Value loss: 0.084513. Entropy: 1.781808.\n",
      "load: 0.230443\n",
      "forward: 0.031541\n",
      "loss: 0.069756\n",
      "clip: 0.000004\n",
      "backward: 0.030402\n",
      "step: 0.016008\n",
      "total: 0.378154\n",
      "\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 28: Policy loss: -0.002475. Value loss: 0.098798. Entropy: 1.780538.\n",
      "load: 0.232039\n",
      "forward: 0.027169\n",
      "loss: 0.065374\n",
      "clip: 0.000005\n",
      "backward: 0.032400\n",
      "step: 0.013941\n",
      "total: 0.370928\n",
      "\n",
      "Iteration 29: Policy loss: -0.007056. Value loss: 0.062995. Entropy: 1.778672.\n",
      "load: 0.218310\n",
      "forward: 0.027727\n",
      "loss: 0.064306\n",
      "clip: 0.000005\n",
      "backward: 0.035599\n",
      "step: 0.014676\n",
      "total: 0.360623\n",
      "\n",
      "Iteration 30: Policy loss: -0.007225. Value loss: 0.053287. Entropy: 1.778731.\n",
      "load: 0.212048\n",
      "forward: 0.029662\n",
      "loss: 0.061442\n",
      "clip: 0.000004\n",
      "backward: 0.027544\n",
      "step: 0.013068\n",
      "total: 0.343768\n",
      "\n",
      "episode: 11   score: 120.0  epsilon: 1.0    steps: 728  evaluation reward: 132.72727272727272\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 31: Policy loss: -0.001280. Value loss: 0.395926. Entropy: 1.777587.\n",
      "load: 0.213357\n",
      "forward: 0.028209\n",
      "loss: 0.069493\n",
      "clip: 0.000004\n",
      "backward: 0.031660\n",
      "step: 0.014422\n",
      "total: 0.357145\n",
      "\n",
      "Iteration 32: Policy loss: -0.011356. Value loss: 0.334863. Entropy: 1.777624.\n",
      "load: 0.214733\n",
      "forward: 0.028882\n",
      "loss: 0.069631\n",
      "clip: 0.000005\n",
      "backward: 0.033502\n",
      "step: 0.012892\n",
      "total: 0.359645\n",
      "\n",
      "Iteration 33: Policy loss: -0.014964. Value loss: 0.322105. Entropy: 1.778324.\n",
      "load: 0.217294\n",
      "forward: 0.026743\n",
      "loss: 0.071183\n",
      "clip: 0.000004\n",
      "backward: 0.034065\n",
      "step: 0.012999\n",
      "total: 0.362288\n",
      "\n",
      "Training network. lr: 0.000250. clip: 0.100000\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(action_size)\n",
    "torch.save(agent.policy_net.state_dict(), \"./save_model/spaceinvaders_ppo_best\")\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n",
    "reset_max = 10\n",
    "\n",
    "\n",
    "### Loop through all environments and run PPO on them\n",
    "\n",
    "#env_names = ['Breakout-v0', 'Phoenix-v0', 'Asteroids-v0', 'SpaceInvaders-v0', 'MsPacman-v0', 'Asterix-v0', 'Atlantis-v0', 'Alien-v0', 'Amidar-v0', 'Assault-v0', 'BankHeist-v0']\n",
    "env_names = ['SpaceInvaders-v4']\n",
    "for a in range(len(env_names)):\n",
    "    name = env_names[a]\n",
    "    print(\"\\n\\n\\n ------- STARTING TRAINING FOR %s ------- \\n\\n\\n\" % (name))\n",
    "    \n",
    "    envs = []\n",
    "    for i in range(num_envs):\n",
    "        envs.append(GameEnv(name))\n",
    "    #env.render()\n",
    "    \n",
    "\n",
    "    number_lives = envs[0].life\n",
    "    state_size = envs[0].observation_space.shape\n",
    "    if (name == 'SpaceInvaders-v0' or name == 'Breakout-v0'):\n",
    "        action_size = 4\n",
    "    else:\n",
    "        action_size = envs[0].action_space.n\n",
    "    rewards, episodes = [], []\n",
    "\n",
    "    vis_env_idx = 0\n",
    "    vis_env = envs[vis_env_idx]\n",
    "    e = 0\n",
    "    frame = 0\n",
    "    max_eval = -np.inf\n",
    "    reset_count = 0\n",
    "\n",
    "\n",
    "    agent = Agent(action_size)\n",
    "    torch.save(agent.policy_net.state_dict(), \"./save_model/\" + name + \"_best\")\n",
    "    evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "    frame = 0\n",
    "    memory_size = 0\n",
    "    reset_max = 10\n",
    "    \n",
    "    print(\"Determing min/max rewards of environment\")\n",
    "    [low, high] = score_range = get_score_range(name)\n",
    "    print(\"Min: %d. Max: %d.\" % (low, high))\n",
    "\n",
    "    while (frame < 10000000):\n",
    "        step = 0\n",
    "        assert(num_envs * env_mem_size == train_frame)\n",
    "        frame_next_vals = []\n",
    "        \n",
    "        for j in range(env_mem_size):\n",
    "            \n",
    "            curr_states = np.stack([envs[i].history[HISTORY_SIZE-1,:,:] for i in range(num_envs)])\n",
    "            next_states = []\n",
    "            net_in = np.stack([envs[i].history[:HISTORY_SIZE,:,:] for i in range(num_envs)])\n",
    "            step += num_envs\n",
    "            frame += num_envs\n",
    "            actions, values, _ = agent.get_action(np.float32(net_in) / 255.)\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                next_states.append(next_state)\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                \"\"\"\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "                \"\"\"\n",
    "                \n",
    "                frame_next_state = get_frame(next_states[i])\n",
    "                env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "                terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "                env.life = env.info['ale.lives']\n",
    "                r = (env.reward / high) * 20.0 #np.log(max(env.reward+1, 1))#((env.reward - low) / (high - low)) * 30\n",
    "                agent.memory.push(i, deepcopy(curr_states[i]), actions[i], r, terminal_state, values[i], 0, 0)\n",
    "                \n",
    "                if (j == env_mem_size-1):\n",
    "                    net_in = np.stack([envs[k].history[1:,:,:] for k in range(num_envs)])\n",
    "                    _, frame_next_vals, _ = agent.get_action(np.float32(net_in) / 255.)\n",
    "                \n",
    "                env.score += env.reward\n",
    "                env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "        \n",
    "                if (env.done):\n",
    "                    if (e % 50 == 0):\n",
    "                        print('now time : ', datetime.now())\n",
    "                        rewards.append(np.mean(evaluation_reward))\n",
    "                        episodes.append(e)\n",
    "                        pylab.plot(episodes, rewards, 'b')\n",
    "                        pylab.savefig(\"./save_graph/\" + name + \"_ppo.png\")\n",
    "                        torch.save(agent.policy_net, \"./save_model/\" + name + \"_ppo\")\n",
    "\n",
    "                        if np.mean(evaluation_reward) > max_eval:\n",
    "                            torch.save(agent.policy_net.state_dict(), \"./save_model/\"  + name + \"_ppo_best\")\n",
    "                            max_eval = float(np.mean(evaluation_reward))\n",
    "                            reset_count = 0\n",
    "                        elif e > 5000:\n",
    "                            reset_count += 1\n",
    "                            \"\"\"\n",
    "                            if (reset_count == reset_max):\n",
    "                                print(\"Training went nowhere, starting again at best model\")\n",
    "                                agent.policy_net.load_state_dict(torch.load(\"./save_model/spaceinvaders_ppo_best\"))\n",
    "                                agent.update_target_net()\n",
    "                                reset_count = 0\n",
    "                            \"\"\"\n",
    "                    e += 1\n",
    "                    evaluation_reward.append(env.score)\n",
    "                    print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                      \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "                    env.done = False\n",
    "                    env.score = 0\n",
    "                    env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "                    env.state = env.reset()\n",
    "                    env.life = number_lives\n",
    "                    get_init_state(env.history, env.state)\n",
    "            \n",
    "        agent.train_policy_net(frame, frame_next_vals)\n",
    "        agent.update_target_net()\n",
    "    print(\"FINISHED TRAINING FOR %s\" % (name))\n",
    "    pylab.figure()\n",
    "    \n",
    "    for i in range(len(envs)):\n",
    "        envs[i]._env.close()\n",
    "    del envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best(name):\n",
    "    env = GameEnv(name)\n",
    "    print(\"\\n\\n\\n ------- TESTING BEST MODEL FOR %s ------- \\n\\n\\n\" % (name))\n",
    "    number_lives = env.life\n",
    "    \n",
    "    if (name == 'SpaceInvaders-v0'):\n",
    "        action_size = 4\n",
    "    else:\n",
    "        action_size = env.action_space.n\n",
    "    rewards, episodes = [], []\n",
    "    \n",
    "    e = 0\n",
    "    frame = 0\n",
    "\n",
    "    agent = Agent(action_size)\n",
    "    agent.policy_net.load_state_dict(torch.load(\"./save_model/\" + name + \"_ppo_best\"))\n",
    "    agent.update_target_net()\n",
    "    agent.policy_net.eval()\n",
    "    evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "\n",
    "    for i in range(100):\n",
    "        env.done = False\n",
    "        env.score = 0\n",
    "        env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "        env.state = env.reset()\n",
    "        env.life = number_lives\n",
    "        get_init_state(env.history, env.state)\n",
    "        step = 0\n",
    "        while not env.done:\n",
    "            curr_state = env.history[HISTORY_SIZE-1,:,:]\n",
    "            net_in = env.history[:HISTORY_SIZE,:,:]\n",
    "            action, value, _ = agent.get_action(np.float32(net_in) / 255.)\n",
    "            \n",
    "            next_state, env.reward, env.done, env.info = env.step(action)\n",
    "            env._env.render()\n",
    "            \n",
    "            frame_next_state = get_frame(next_state)\n",
    "            \n",
    "            env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "            terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "            env.life = env.info['ale.lives']\n",
    "            \n",
    "            \n",
    "            env.score += env.reward\n",
    "            env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "            step += 1\n",
    "        \n",
    "\n",
    "        evaluation_reward.append(env.score)\n",
    "        print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                      \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_best('MsPacman-v0')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional LSTM agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " ------- STARTING TRAINING FOR SpaceInvaders-v4 ------- \n",
      "\n",
      "\n",
      "\n",
      "Determing min/max rewards of environment\n",
      "Min: 0. Max: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/sigai/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],1)[0].unsqueeze(1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000250. clip: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:89: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],1)[0].unsqueeze(1))\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:276: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:277: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:278: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Policy loss: 0.018577. Value loss: 0.865079. Entropy: 1.764202.\n",
      "Iteration 2: Policy loss: -0.000534. Value loss: 0.863796. Entropy: 1.787912.\n",
      "Iteration 3: Policy loss: 0.000039. Value loss: 0.859918. Entropy: 1.789157.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 4: Policy loss: 0.016562. Value loss: 2.537752. Entropy: 1.774985.\n",
      "Iteration 5: Policy loss: 0.003056. Value loss: 2.486270. Entropy: 1.782428.\n",
      "Iteration 6: Policy loss: 0.007223. Value loss: 2.419378. Entropy: 1.786161.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 7: Policy loss: 0.006031. Value loss: 1.130184. Entropy: 1.780825.\n",
      "Iteration 8: Policy loss: 0.003517. Value loss: 1.098652. Entropy: 1.784235.\n",
      "Iteration 9: Policy loss: -0.002592. Value loss: 1.061716. Entropy: 1.784130.\n",
      "now time :  2019-08-30 14:19:20.523799\n",
      "episode: 1   score: 80.0  epsilon: 1.0    steps: 896  evaluation reward: 80.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/sigai/.local/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 10: Policy loss: 0.000855. Value loss: 2.279566. Entropy: 1.783752.\n",
      "Iteration 11: Policy loss: -0.002447. Value loss: 2.219868. Entropy: 1.785918.\n",
      "Iteration 12: Policy loss: 0.001141. Value loss: 2.147106. Entropy: 1.782934.\n",
      "episode: 2   score: 100.0  epsilon: 1.0    steps: 120  evaluation reward: 90.0\n",
      "episode: 3   score: 185.0  epsilon: 1.0    steps: 1016  evaluation reward: 121.66666666666667\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 13: Policy loss: -0.000154. Value loss: 2.336051. Entropy: 1.780688.\n",
      "Iteration 14: Policy loss: 0.000784. Value loss: 2.339136. Entropy: 1.781157.\n",
      "Iteration 15: Policy loss: -0.005321. Value loss: 2.311987. Entropy: 1.781651.\n",
      "episode: 4   score: 110.0  epsilon: 1.0    steps: 16  evaluation reward: 118.75\n",
      "episode: 5   score: 105.0  epsilon: 1.0    steps: 128  evaluation reward: 116.0\n",
      "episode: 6   score: 155.0  epsilon: 1.0    steps: 784  evaluation reward: 122.5\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 16: Policy loss: 0.003234. Value loss: 1.318422. Entropy: 1.777462.\n",
      "Iteration 17: Policy loss: 0.002082. Value loss: 1.290864. Entropy: 1.774358.\n",
      "Iteration 18: Policy loss: -0.001792. Value loss: 1.255872. Entropy: 1.774028.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 19: Policy loss: 0.009516. Value loss: 2.007296. Entropy: 1.764004.\n",
      "Iteration 20: Policy loss: 0.005659. Value loss: 1.944179. Entropy: 1.764167.\n",
      "Iteration 21: Policy loss: -0.001839. Value loss: 1.881129. Entropy: 1.769751.\n",
      "episode: 7   score: 315.0  epsilon: 1.0    steps: 568  evaluation reward: 150.0\n",
      "episode: 8   score: 60.0  epsilon: 1.0    steps: 728  evaluation reward: 138.75\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 22: Policy loss: 0.003518. Value loss: 2.158647. Entropy: 1.762014.\n",
      "Iteration 23: Policy loss: 0.002442. Value loss: 2.111408. Entropy: 1.769789.\n",
      "Iteration 24: Policy loss: 0.002250. Value loss: 2.010688. Entropy: 1.765549.\n",
      "episode: 9   score: 455.0  epsilon: 1.0    steps: 80  evaluation reward: 173.88888888888889\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 25: Policy loss: 0.013594. Value loss: 1.748458. Entropy: 1.738298.\n",
      "Iteration 26: Policy loss: 0.018197. Value loss: 1.583281. Entropy: 1.740256.\n",
      "Iteration 27: Policy loss: 0.022959. Value loss: 1.371774. Entropy: 1.732627.\n",
      "episode: 10   score: 105.0  epsilon: 1.0    steps: 24  evaluation reward: 167.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 28: Policy loss: 0.005018. Value loss: 2.146758. Entropy: 1.725497.\n",
      "Iteration 29: Policy loss: 0.013013. Value loss: 1.860298. Entropy: 1.729912.\n",
      "Iteration 30: Policy loss: 0.015976. Value loss: 1.649424. Entropy: 1.716528.\n",
      "episode: 11   score: 170.0  epsilon: 1.0    steps: 400  evaluation reward: 167.27272727272728\n",
      "episode: 12   score: 305.0  epsilon: 1.0    steps: 896  evaluation reward: 178.75\n",
      "episode: 13   score: 90.0  epsilon: 1.0    steps: 1024  evaluation reward: 171.92307692307693\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 31: Policy loss: 0.006387. Value loss: 4.307769. Entropy: 1.753418.\n",
      "Iteration 32: Policy loss: 0.013003. Value loss: 4.047128. Entropy: 1.743598.\n",
      "Iteration 33: Policy loss: 0.003530. Value loss: 3.667296. Entropy: 1.759358.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 34: Policy loss: 0.011252. Value loss: 2.184304. Entropy: 1.762130.\n",
      "Iteration 35: Policy loss: 0.020602. Value loss: 1.613842. Entropy: 1.756765.\n",
      "Iteration 36: Policy loss: 0.017225. Value loss: 1.336233. Entropy: 1.758429.\n",
      "episode: 14   score: 80.0  epsilon: 1.0    steps: 352  evaluation reward: 165.35714285714286\n",
      "episode: 15   score: 245.0  epsilon: 1.0    steps: 624  evaluation reward: 170.66666666666666\n",
      "episode: 16   score: 360.0  epsilon: 1.0    steps: 816  evaluation reward: 182.5\n",
      "episode: 17   score: 340.0  epsilon: 1.0    steps: 1016  evaluation reward: 191.76470588235293\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 37: Policy loss: 0.012154. Value loss: 2.235898. Entropy: 1.764088.\n",
      "Iteration 38: Policy loss: 0.006054. Value loss: 1.962996. Entropy: 1.766522.\n",
      "Iteration 39: Policy loss: 0.004499. Value loss: 1.642519. Entropy: 1.758982.\n",
      "episode: 18   score: 80.0  epsilon: 1.0    steps: 1008  evaluation reward: 185.55555555555554\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 40: Policy loss: 0.022741. Value loss: 2.525501. Entropy: 1.732076.\n",
      "Iteration 41: Policy loss: 0.017522. Value loss: 2.098820. Entropy: 1.731265.\n",
      "Iteration 42: Policy loss: 0.008436. Value loss: 1.813029. Entropy: 1.725126.\n",
      "episode: 19   score: 140.0  epsilon: 1.0    steps: 840  evaluation reward: 183.1578947368421\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 43: Policy loss: 0.020680. Value loss: 2.875384. Entropy: 1.737550.\n",
      "Iteration 44: Policy loss: 0.018830. Value loss: 2.258288. Entropy: 1.743815.\n",
      "Iteration 45: Policy loss: 0.018754. Value loss: 1.722201. Entropy: 1.744192.\n",
      "episode: 20   score: 115.0  epsilon: 1.0    steps: 248  evaluation reward: 179.75\n",
      "episode: 21   score: 210.0  epsilon: 1.0    steps: 416  evaluation reward: 181.1904761904762\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 46: Policy loss: 0.016533. Value loss: 1.469413. Entropy: 1.725132.\n",
      "Iteration 47: Policy loss: 0.012033. Value loss: 0.843308. Entropy: 1.717767.\n",
      "Iteration 48: Policy loss: 0.032101. Value loss: 0.711429. Entropy: 1.725820.\n",
      "episode: 22   score: 125.0  epsilon: 1.0    steps: 544  evaluation reward: 178.63636363636363\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 49: Policy loss: 0.029893. Value loss: 2.986689. Entropy: 1.729504.\n",
      "Iteration 50: Policy loss: 0.018952. Value loss: 2.384624. Entropy: 1.734885.\n",
      "Iteration 51: Policy loss: 0.023200. Value loss: 1.948782. Entropy: 1.729606.\n",
      "episode: 23   score: 135.0  epsilon: 1.0    steps: 256  evaluation reward: 176.7391304347826\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 52: Policy loss: 0.015006. Value loss: 2.347783. Entropy: 1.748033.\n",
      "Iteration 53: Policy loss: 0.019590. Value loss: 1.688510. Entropy: 1.754490.\n",
      "Iteration 54: Policy loss: 0.022712. Value loss: 1.350888. Entropy: 1.745875.\n",
      "episode: 24   score: 180.0  epsilon: 1.0    steps: 784  evaluation reward: 176.875\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 55: Policy loss: 0.011830. Value loss: 2.572527. Entropy: 1.763704.\n",
      "Iteration 56: Policy loss: 0.005463. Value loss: 2.066998. Entropy: 1.767440.\n",
      "Iteration 57: Policy loss: 0.006175. Value loss: 1.830497. Entropy: 1.765348.\n",
      "episode: 25   score: 145.0  epsilon: 1.0    steps: 184  evaluation reward: 175.6\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 58: Policy loss: 0.001034. Value loss: 1.279498. Entropy: 1.763233.\n",
      "Iteration 59: Policy loss: -0.001696. Value loss: 0.826685. Entropy: 1.762606.\n",
      "Iteration 60: Policy loss: -0.003288. Value loss: 0.696611. Entropy: 1.758897.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 61: Policy loss: 0.013321. Value loss: 1.872060. Entropy: 1.767555.\n",
      "Iteration 62: Policy loss: 0.007125. Value loss: 1.305990. Entropy: 1.762610.\n",
      "Iteration 63: Policy loss: 0.013476. Value loss: 1.075585. Entropy: 1.759855.\n",
      "episode: 26   score: 180.0  epsilon: 1.0    steps: 16  evaluation reward: 175.76923076923077\n",
      "episode: 27   score: 245.0  epsilon: 1.0    steps: 248  evaluation reward: 178.33333333333334\n",
      "episode: 28   score: 135.0  epsilon: 1.0    steps: 248  evaluation reward: 176.78571428571428\n",
      "episode: 29   score: 185.0  epsilon: 1.0    steps: 664  evaluation reward: 177.06896551724137\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 64: Policy loss: 0.005354. Value loss: 2.325709. Entropy: 1.746932.\n",
      "Iteration 65: Policy loss: 0.005401. Value loss: 1.573020. Entropy: 1.747858.\n",
      "Iteration 66: Policy loss: 0.008218. Value loss: 1.354924. Entropy: 1.749358.\n",
      "episode: 30   score: 155.0  epsilon: 1.0    steps: 72  evaluation reward: 176.33333333333334\n",
      "episode: 31   score: 520.0  epsilon: 1.0    steps: 688  evaluation reward: 187.41935483870967\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 67: Policy loss: 0.013349. Value loss: 2.091111. Entropy: 1.733617.\n",
      "Iteration 68: Policy loss: 0.016527. Value loss: 1.674383. Entropy: 1.718151.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 69: Policy loss: 0.014637. Value loss: 1.419362. Entropy: 1.735388.\n",
      "episode: 32   score: 110.0  epsilon: 1.0    steps: 920  evaluation reward: 185.0\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 70: Policy loss: 0.007490. Value loss: 2.640646. Entropy: 1.711904.\n",
      "Iteration 71: Policy loss: 0.012676. Value loss: 1.896738. Entropy: 1.710703.\n",
      "Iteration 72: Policy loss: 0.009383. Value loss: 1.587145. Entropy: 1.708450.\n",
      "episode: 33   score: 160.0  epsilon: 1.0    steps: 200  evaluation reward: 184.24242424242425\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 73: Policy loss: 0.021450. Value loss: 4.197939. Entropy: 1.703299.\n",
      "Iteration 74: Policy loss: 0.038168. Value loss: 3.080142. Entropy: 1.677595.\n",
      "Iteration 75: Policy loss: 0.030708. Value loss: 2.412647. Entropy: 1.690186.\n",
      "episode: 34   score: 105.0  epsilon: 1.0    steps: 424  evaluation reward: 181.91176470588235\n",
      "episode: 35   score: 65.0  epsilon: 1.0    steps: 696  evaluation reward: 178.57142857142858\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 76: Policy loss: 0.029435. Value loss: 2.548459. Entropy: 1.650163.\n",
      "Iteration 77: Policy loss: 0.023493. Value loss: 1.570838. Entropy: 1.665074.\n",
      "Iteration 78: Policy loss: 0.016217. Value loss: 1.190634. Entropy: 1.664969.\n",
      "episode: 36   score: 75.0  epsilon: 1.0    steps: 392  evaluation reward: 175.69444444444446\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(action_size, mode='PPO_LSTM')\n",
    "torch.save(agent.policy_net.state_dict(), \"./save_model/spaceinvaders_ppo_best\")\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n",
    "reset_max = 10\n",
    "\n",
    "\n",
    "### Loop through all environments and run PPO on them\n",
    "\n",
    "#env_names = ['Breakout-v0', 'Phoenix-v0', 'Asteroids-v0', 'SpaceInvaders-v0', 'MsPacman-v0', 'Asterix-v0', 'Atlantis-v0', 'Alien-v0', 'Amidar-v0', 'Assault-v0', 'BankHeist-v0']\n",
    "env_names = ['SpaceInvaders-v4']\n",
    "for a in range(len(env_names)):\n",
    "    name = env_names[a]\n",
    "    print(\"\\n\\n\\n ------- STARTING TRAINING FOR %s ------- \\n\\n\\n\" % (name))\n",
    "    \n",
    "    envs = []\n",
    "    for i in range(num_envs):\n",
    "        envs.append(GameEnv(name))\n",
    "        envs[i].reset_memory(agent.init_hidden())\n",
    "    #env.render()\n",
    "    \n",
    "\n",
    "    number_lives = envs[0].life\n",
    "    state_size = envs[0].observation_space.shape\n",
    "    if (name == 'SpaceInvaders-v0' or name == 'Breakout-v0'):\n",
    "        action_size = 4\n",
    "    else:\n",
    "        action_size = envs[0].action_space.n\n",
    "    rewards, episodes = [], []\n",
    "\n",
    "    vis_env_idx = 0\n",
    "    vis_env = envs[vis_env_idx]\n",
    "    e = 0\n",
    "    frame = 0\n",
    "    max_eval = -np.inf\n",
    "    reset_count = 0\n",
    "\n",
    "\n",
    "    agent = Agent(action_size, mode='PPO_LSTM')\n",
    "    torch.save(agent.policy_net.state_dict(), \"./save_model/\" + name + \"_best\")\n",
    "    evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "    frame = 0\n",
    "    memory_size = 0\n",
    "    reset_max = 10\n",
    "    \n",
    "    print(\"Determing min/max rewards of environment\")\n",
    "    [low, high] = score_range = get_score_range(name)\n",
    "    print(\"Min: %d. Max: %d.\" % (low, high))\n",
    "\n",
    "    while (frame < 10000000):\n",
    "        step = 0\n",
    "        assert(num_envs * env_mem_size == train_frame)\n",
    "        frame_next_vals = []\n",
    "        \n",
    "        for j in range(env_mem_size):\n",
    "            \n",
    "            curr_states = np.stack([envs[i].history[[HISTORY_SIZE-1],:,:] for i in range(num_envs)])\n",
    "            hiddens = torch.cat([envs[i].memory for i in range(num_envs)])\n",
    "            next_states = []\n",
    "            step += num_envs\n",
    "            frame += num_envs\n",
    "            actions, values, hiddens = agent.get_action(np.float32(curr_states) / 255., hiddens)\n",
    "            hiddens = hiddens.detach()\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                next_states.append(next_state)\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                \"\"\"\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "                \"\"\"\n",
    "                \n",
    "                frame_next_state = get_frame(next_states[i])\n",
    "                env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "                env.memory = hiddens[[i]]\n",
    "                terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "                env.life = env.info['ale.lives']\n",
    "                r = (env.reward / high) * 20.0 #np.log(max(env.reward+1, 1))#((env.reward - low) / (high - low)) * 30\n",
    "                agent.memory.push(i, [deepcopy(curr_states[i]), hiddens[i].detach().cpu().data.numpy()], actions[i], r, terminal_state, values[i], 0, 0)\n",
    "                \n",
    "                if (j == env_mem_size-1):\n",
    "                    #net_in = np.stack([envs[k].history[1:,:,:] for k in range(num_envs)])\n",
    "                    net_in = np.stack([envs[k].history[[-1],:,:] for k in range(num_envs)])\n",
    "                    _, frame_next_vals, _ = agent.get_action(np.float32(net_in) / 255., hiddens)\n",
    "                \n",
    "                env.score += env.reward\n",
    "                env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "        \n",
    "                if (env.done):\n",
    "                    if (e % 50 == 0):\n",
    "                        print('now time : ', datetime.now())\n",
    "                        rewards.append(np.mean(evaluation_reward))\n",
    "                        episodes.append(e)\n",
    "                        pylab.plot(episodes, rewards, 'b')\n",
    "                        pylab.savefig(\"./save_graph/\" + name + \"_ppo.png\")\n",
    "                        torch.save(agent.policy_net, \"./save_model/\" + name + \"_ppo\")\n",
    "\n",
    "                        if np.mean(evaluation_reward) > max_eval:\n",
    "                            torch.save(agent.policy_net.state_dict(), \"./save_model/\"  + name + \"_ppo_best\")\n",
    "                            max_eval = float(np.mean(evaluation_reward))\n",
    "                            reset_count = 0\n",
    "                        elif e > 5000:\n",
    "                            reset_count += 1\n",
    "                            \"\"\"\n",
    "                            if (reset_count == reset_max):\n",
    "                                print(\"Training went nowhere, starting again at best model\")\n",
    "                                agent.policy_net.load_state_dict(torch.load(\"./save_model/spaceinvaders_ppo_best\"))\n",
    "                                agent.update_target_net()\n",
    "                                reset_count = 0\n",
    "                            \"\"\"\n",
    "                    e += 1\n",
    "                    evaluation_reward.append(env.score)\n",
    "                    print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                      \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "                    env.done = False\n",
    "                    env.score = 0\n",
    "                    env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "                    env.state = env.reset()\n",
    "                    env.life = number_lives\n",
    "                    get_init_state(env.history, env.state)\n",
    "                    env.reset_memory(agent.init_hidden())\n",
    "            \n",
    "        agent.train_policy_net(frame, frame_next_vals)\n",
    "        agent.update_target_net()\n",
    "    print(\"FINISHED TRAINING FOR %s\" % (name))\n",
    "    pylab.figure()\n",
    "    \n",
    "    for i in range(len(envs)):\n",
    "        envs[i]._env.close()\n",
    "    del envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
