{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "from env import GameEnv\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/anaconda3/envs/py36/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "envs = []\n",
    "for i in range(num_envs):\n",
    "    envs.append(GameEnv('SpaceInvadersDeterministic-v4'))\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = envs[0].life\n",
    "state_size = envs[0].observation_space.shape\n",
    "action_size = envs[0].action_space.n\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "torch.save(agent.policy_net.state_dict(), \"./save_model/spaceinvaders_ppo_best\")\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n",
    "reset_max = 10\n",
    "\n",
    "\n",
    "### Loop through all environments and run PPO on them\n",
    "\n",
    "#env_names = ['Breakout-v0', 'Phoenix-v0', 'Asteroids-v0', 'SpaceInvaders-v0', 'MsPacman-v0', 'Asterix-v0', 'Atlantis-v0', 'Alien-v0', 'Amidar-v0', 'Assault-v0', 'BankHeist-v0']\n",
    "env_names = ['SpaceInvaders-v4']\n",
    "for a in range(len(env_names)):\n",
    "    name = env_names[a]\n",
    "    print(\"\\n\\n\\n ------- STARTING TRAINING FOR %s ------- \\n\\n\\n\" % (name))\n",
    "    \n",
    "    envs = []\n",
    "    for i in range(num_envs):\n",
    "        envs.append(GameEnv(name))\n",
    "    #env.render()\n",
    "    \n",
    "\n",
    "    number_lives = envs[0].life\n",
    "    state_size = envs[0].observation_space.shape\n",
    "    if (name == 'SpaceInvaders-v0' or name == 'Breakout-v0'):\n",
    "        action_size = 4\n",
    "    else:\n",
    "        action_size = envs[0].action_space.n\n",
    "    rewards, episodes = [], []\n",
    "\n",
    "    vis_env_idx = 0\n",
    "    vis_env = envs[vis_env_idx]\n",
    "    e = 0\n",
    "    frame = 0\n",
    "    max_eval = -np.inf\n",
    "    reset_count = 0\n",
    "\n",
    "\n",
    "    agent = Agent(action_size)\n",
    "    torch.save(agent.policy_net.state_dict(), \"./save_model/\" + name + \"_best\")\n",
    "    evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "    frame = 0\n",
    "    memory_size = 0\n",
    "    reset_max = 10\n",
    "    \n",
    "    print(\"Determing min/max rewards of environment\")\n",
    "    [low, high] = score_range = get_score_range(name)\n",
    "    print(\"Min: %d. Max: %d.\" % (low, high))\n",
    "\n",
    "    while (frame < 10000000):\n",
    "        step = 0\n",
    "        assert(num_envs * env_mem_size == train_frame)\n",
    "        frame_next_vals = []\n",
    "        \n",
    "        for j in range(env_mem_size):\n",
    "            \n",
    "            curr_states = np.stack([envs[i].history[HISTORY_SIZE-1,:,:] for i in range(num_envs)])\n",
    "            next_states = []\n",
    "            net_in = np.stack([envs[i].history[:HISTORY_SIZE,:,:] for i in range(num_envs)])\n",
    "            step += num_envs\n",
    "            frame += num_envs\n",
    "            actions, values, _ = agent.get_action(np.float32(net_in) / 255.)\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                next_states.append(next_state)\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                \"\"\"\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "                \"\"\"\n",
    "                \n",
    "                frame_next_state = get_frame(next_states[i])\n",
    "                env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "                terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "                env.life = env.info['ale.lives']\n",
    "                r = (env.reward / high) * 20.0 #np.log(max(env.reward+1, 1))#((env.reward - low) / (high - low)) * 30\n",
    "                agent.memory.push(i, deepcopy(curr_states[i]), actions[i], r, terminal_state, values[i], 0, 0)\n",
    "                \n",
    "                if (j == env_mem_size-1):\n",
    "                    net_in = np.stack([envs[k].history[1:,:,:] for k in range(num_envs)])\n",
    "                    _, frame_next_vals, _ = agent.get_action(np.float32(net_in) / 255.)\n",
    "                \n",
    "                env.score += env.reward\n",
    "                env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "        \n",
    "                if (env.done):\n",
    "                    if (e % 50 == 0):\n",
    "                        print('now time : ', datetime.now())\n",
    "                        rewards.append(np.mean(evaluation_reward))\n",
    "                        episodes.append(e)\n",
    "                        pylab.plot(episodes, rewards, 'b')\n",
    "                        pylab.savefig(\"./save_graph/\" + name + \"_ppo.png\")\n",
    "                        torch.save(agent.policy_net, \"./save_model/\" + name + \"_ppo\")\n",
    "\n",
    "                        if np.mean(evaluation_reward) > max_eval:\n",
    "                            torch.save(agent.policy_net.state_dict(), \"./save_model/\"  + name + \"_ppo_best\")\n",
    "                            max_eval = float(np.mean(evaluation_reward))\n",
    "                            reset_count = 0\n",
    "                        elif e > 5000:\n",
    "                            reset_count += 1\n",
    "                            \"\"\"\n",
    "                            if (reset_count == reset_max):\n",
    "                                print(\"Training went nowhere, starting again at best model\")\n",
    "                                agent.policy_net.load_state_dict(torch.load(\"./save_model/spaceinvaders_ppo_best\"))\n",
    "                                agent.update_target_net()\n",
    "                                reset_count = 0\n",
    "                            \"\"\"\n",
    "                    e += 1\n",
    "                    evaluation_reward.append(env.score)\n",
    "                    print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                      \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "                    env.done = False\n",
    "                    env.score = 0\n",
    "                    env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "                    env.state = env.reset()\n",
    "                    env.life = number_lives\n",
    "                    get_init_state(env.history, env.state)\n",
    "            \n",
    "        agent.train_policy_net(frame, frame_next_vals)\n",
    "        agent.update_target_net()\n",
    "    print(\"FINISHED TRAINING FOR %s\" % (name))\n",
    "    pylab.figure()\n",
    "    \n",
    "    for i in range(len(envs)):\n",
    "        envs[i]._env.close()\n",
    "    del envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best(name):\n",
    "    env = GameEnv(name)\n",
    "    print(\"\\n\\n\\n ------- TESTING BEST MODEL FOR %s ------- \\n\\n\\n\" % (name))\n",
    "    number_lives = env.life\n",
    "    \n",
    "    if (name == 'SpaceInvaders-v0'):\n",
    "        action_size = 4\n",
    "    else:\n",
    "        action_size = env.action_space.n\n",
    "    rewards, episodes = [], []\n",
    "    \n",
    "    e = 0\n",
    "    frame = 0\n",
    "\n",
    "    agent = Agent(action_size)\n",
    "    agent.policy_net.load_state_dict(torch.load(\"./save_model/\" + name + \"_ppo_best\"))\n",
    "    agent.update_target_net()\n",
    "    agent.policy_net.eval()\n",
    "    evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "\n",
    "    for i in range(100):\n",
    "        env.done = False\n",
    "        env.score = 0\n",
    "        env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "        env.state = env.reset()\n",
    "        env.life = number_lives\n",
    "        get_init_state(env.history, env.state)\n",
    "        step = 0\n",
    "        while not env.done:\n",
    "            curr_state = env.history[HISTORY_SIZE-1,:,:]\n",
    "            net_in = env.history[:HISTORY_SIZE,:,:]\n",
    "            action, value, _ = agent.get_action(np.float32(net_in) / 255.)\n",
    "            \n",
    "            next_state, env.reward, env.done, env.info = env.step(action)\n",
    "            env._env.render()\n",
    "            \n",
    "            frame_next_state = get_frame(next_state)\n",
    "            \n",
    "            env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "            terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "            env.life = env.info['ale.lives']\n",
    "            \n",
    "            \n",
    "            env.score += env.reward\n",
    "            env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "            step += 1\n",
    "        \n",
    "\n",
    "        evaluation_reward.append(env.score)\n",
    "        print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                      \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_best('MsPacman-v0')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional LSTM agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " ------- STARTING TRAINING FOR SpaceInvaders-v4 ------- \n",
      "\n",
      "\n",
      "\n",
      "Determing min/max rewards of environment\n",
      "Min: 0. Max: 200.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/sigai/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:74: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],1)[0].unsqueeze(1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000250. clip: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],1)[0].unsqueeze(1))\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:276: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:277: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:278: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Policy loss: 0.006135. Value loss: 0.055704. Entropy: 1.785254.\n",
      "Iteration 2: Policy loss: 0.000211. Value loss: 0.044286. Entropy: 1.790547.\n",
      "Iteration 3: Policy loss: 0.000724. Value loss: 0.043297. Entropy: 1.789542.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 4: Policy loss: 0.000174. Value loss: 0.207896. Entropy: 1.783337.\n",
      "Iteration 5: Policy loss: 0.000053. Value loss: 0.194291. Entropy: 1.789207.\n",
      "Iteration 6: Policy loss: 0.001433. Value loss: 0.191184. Entropy: 1.784513.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 7: Policy loss: 0.002668. Value loss: 0.135473. Entropy: 1.786790.\n",
      "Iteration 8: Policy loss: 0.003191. Value loss: 0.137322. Entropy: 1.785902.\n",
      "Iteration 9: Policy loss: 0.000914. Value loss: 0.133643. Entropy: 1.786068.\n",
      "now time :  2019-08-31 17:22:57.701110\n",
      "episode: 1   score: 105.0  epsilon: 1.0    steps: 760  evaluation reward: 105.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/sigai/.local/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2   score: 125.0  epsilon: 1.0    steps: 944  evaluation reward: 115.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 10: Policy loss: -0.000820. Value loss: 0.176695. Entropy: 1.784604.\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(action_size, mode='PPO_LSTM')\n",
    "torch.save(agent.policy_net.state_dict(), \"./save_model/spaceinvaders_ppo_best\")\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n",
    "reset_max = 10\n",
    "\n",
    "\n",
    "### Loop through all environments and run PPO on them\n",
    "\n",
    "#env_names = ['Breakout-v0', 'Phoenix-v0', 'Asteroids-v0', 'SpaceInvaders-v0', 'MsPacman-v0', 'Asterix-v0', 'Atlantis-v0', 'Alien-v0', 'Amidar-v0', 'Assault-v0', 'BankHeist-v0']\n",
    "env_names = ['SpaceInvaders-v4']\n",
    "for a in range(len(env_names)):\n",
    "    name = env_names[a]\n",
    "    print(\"\\n\\n\\n ------- STARTING TRAINING FOR %s ------- \\n\\n\\n\" % (name))\n",
    "    \n",
    "    envs = []\n",
    "    for i in range(num_envs):\n",
    "        envs.append(GameEnv(name))\n",
    "        envs[i].reset_memory(agent.init_hidden())\n",
    "    #env.render()\n",
    "    \n",
    "\n",
    "    number_lives = envs[0].life\n",
    "    state_size = envs[0].observation_space.shape\n",
    "    if (name == 'SpaceInvaders-v0' or name == 'Breakout-v0'):\n",
    "        action_size = 4\n",
    "    else:\n",
    "        action_size = envs[0].action_space.n\n",
    "    rewards, episodes = [], []\n",
    "\n",
    "    vis_env_idx = 0\n",
    "    vis_env = envs[vis_env_idx]\n",
    "    e = 0\n",
    "    frame = 0\n",
    "    max_eval = -np.inf\n",
    "    reset_count = 0\n",
    "\n",
    "\n",
    "    agent = Agent(action_size, mode='PPO_LSTM')\n",
    "    torch.save(agent.policy_net.state_dict(), \"./save_model/\" + name + \"_best\")\n",
    "    evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "    frame = 0\n",
    "    memory_size = 0\n",
    "    reset_max = 10\n",
    "    \n",
    "    print(\"Determing min/max rewards of environment\")\n",
    "    [low, high] = score_range = get_score_range(name)\n",
    "    print(\"Min: %d. Max: %d.\" % (low, high))\n",
    "\n",
    "    while (frame < 10000000):\n",
    "        step = 0\n",
    "        assert(num_envs * env_mem_size == train_frame)\n",
    "        frame_next_vals = []\n",
    "        \n",
    "        for j in range(env_mem_size):\n",
    "            \n",
    "            curr_states = np.stack([envs[i].history[[HISTORY_SIZE-1],:,:] for i in range(num_envs)])\n",
    "            hiddens = torch.cat([envs[i].memory for i in range(num_envs)])\n",
    "            next_states = []\n",
    "            step += num_envs\n",
    "            frame += num_envs\n",
    "            actions, values, hiddens = agent.get_action(np.float32(curr_states) / 255., hiddens)\n",
    "            hiddens = hiddens.detach()\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                next_states.append(next_state)\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                \"\"\"\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "                \"\"\"\n",
    "                \n",
    "                frame_next_state = get_frame(next_states[i])\n",
    "                env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "                env.memory = hiddens[[i]]\n",
    "                terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "                env.life = env.info['ale.lives']\n",
    "                r = (env.reward / high) * 20.0 #np.log(max(env.reward+1, 1))#((env.reward - low) / (high - low)) * 30\n",
    "                agent.memory.push(i, [deepcopy(curr_states[i]), hiddens[i].detach().cpu().data.numpy()], actions[i], r, terminal_state, values[i], 0, 0)\n",
    "                \n",
    "                if (j == env_mem_size-1):\n",
    "                    #net_in = np.stack([envs[k].history[1:,:,:] for k in range(num_envs)])\n",
    "                    net_in = np.stack([envs[k].history[[-1],:,:] for k in range(num_envs)])\n",
    "                    _, frame_next_vals, _ = agent.get_action(np.float32(net_in) / 255., hiddens)\n",
    "                \n",
    "                env.score += env.reward\n",
    "                env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "        \n",
    "                if (env.done):\n",
    "                    if (e % 50 == 0):\n",
    "                        print('now time : ', datetime.now())\n",
    "                        rewards.append(np.mean(evaluation_reward))\n",
    "                        episodes.append(e)\n",
    "                        pylab.plot(episodes, rewards, 'b')\n",
    "                        pylab.savefig(\"./save_graph/\" + name + \"_ppo.png\")\n",
    "                        torch.save(agent.policy_net, \"./save_model/\" + name + \"_ppo\")\n",
    "\n",
    "                        if np.mean(evaluation_reward) > max_eval:\n",
    "                            torch.save(agent.policy_net.state_dict(), \"./save_model/\"  + name + \"_ppo_best\")\n",
    "                            max_eval = float(np.mean(evaluation_reward))\n",
    "                            reset_count = 0\n",
    "                        elif e > 5000:\n",
    "                            reset_count += 1\n",
    "                            \"\"\"\n",
    "                            if (reset_count == reset_max):\n",
    "                                print(\"Training went nowhere, starting again at best model\")\n",
    "                                agent.policy_net.load_state_dict(torch.load(\"./save_model/spaceinvaders_ppo_best\"))\n",
    "                                agent.update_target_net()\n",
    "                                reset_count = 0\n",
    "                            \"\"\"\n",
    "                    e += 1\n",
    "                    evaluation_reward.append(env.score)\n",
    "                    print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                      \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "                    env.done = False\n",
    "                    env.score = 0\n",
    "                    env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "                    env.state = env.reset()\n",
    "                    env.life = number_lives\n",
    "                    get_init_state(env.history, env.state)\n",
    "                    env.reset_memory(agent.init_hidden())\n",
    "            \n",
    "        agent.train_policy_net(frame, frame_next_vals)\n",
    "        agent.update_target_net()\n",
    "    print(\"FINISHED TRAINING FOR %s\" % (name))\n",
    "    pylab.figure()\n",
    "    \n",
    "    for i in range(len(envs)):\n",
    "        envs[i]._env.close()\n",
    "    del envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
