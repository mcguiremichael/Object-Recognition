{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvadersDeterministic-v4')\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lifes(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 6\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/home/michael/Documents/CS498DL/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],0)[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 90.0   memory length: 566   epsilon: 1.0    steps: 566     evaluation reward: 90.0\n",
      "Training network\n",
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/Documents/CS498DL/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:225: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/michael/Documents/CS498DL/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:226: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/michael/Documents/CS498DL/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:227: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy loss: -0.007590. Value loss: 9.183960. Entropy: 1.782009.\n",
      "\n",
      "load: 0.125391\n",
      "forward: 0.060244\n",
      "loss: 0.016833\n",
      "clip: 0.010742\n",
      "backward: 0.061074\n",
      "step: 0.041012\n",
      "total: 0.315297\n",
      "\n",
      "Iteration 2\n",
      "Policy loss: -0.016150. Value loss: 6.841040. Entropy: 1.771372.\n",
      "\n",
      "load: 0.121090\n",
      "forward: 0.056096\n",
      "loss: 0.016732\n",
      "clip: 0.010488\n",
      "backward: 0.060100\n",
      "step: 0.038951\n",
      "total: 0.303455\n",
      "\n",
      "Iteration 3\n",
      "Policy loss: -0.016073. Value loss: 4.636711. Entropy: 1.761170.\n",
      "\n",
      "load: 0.120648\n",
      "forward: 0.056216\n",
      "loss: 0.017094\n",
      "clip: 0.010195\n",
      "backward: 0.059795\n",
      "step: 0.038394\n",
      "total: 0.302341\n",
      "\n",
      "Iteration 4\n",
      "Policy loss: -0.014230. Value loss: 4.763137. Entropy: 1.745392.\n",
      "\n",
      "load: 0.120423\n",
      "forward: 0.055136\n",
      "loss: 0.016663\n",
      "clip: 0.010163\n",
      "backward: 0.058842\n",
      "step: 0.037626\n",
      "total: 0.298854\n",
      "\n",
      "Iteration 5\n",
      "Policy loss: -0.036654. Value loss: 3.803099. Entropy: 1.753364.\n",
      "\n",
      "load: 0.121626\n",
      "forward: 0.055560\n",
      "loss: 0.017443\n",
      "clip: 0.010147\n",
      "backward: 0.059712\n",
      "step: 0.037637\n",
      "total: 0.302124\n",
      "\n",
      "episode: 1   score: 135.0   memory length: 1024   epsilon: 1.0    steps: 984     evaluation reward: 112.5\n",
      "episode: 2   score: 65.0   memory length: 1024   epsilon: 1.0    steps: 390     evaluation reward: 96.66666666666667\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.000088. Value loss: 3.789489. Entropy: 1.754737.\n",
      "\n",
      "load: 0.120079\n",
      "forward: 0.056208\n",
      "loss: 0.017045\n",
      "clip: 0.010251\n",
      "backward: 0.061265\n",
      "step: 0.037452\n",
      "total: 0.302301\n",
      "\n",
      "Iteration 2\n",
      "Policy loss: 0.010517. Value loss: 2.664348. Entropy: 1.750947.\n",
      "\n",
      "load: 0.125585\n",
      "forward: 0.059376\n",
      "loss: 0.018210\n",
      "clip: 0.011002\n",
      "backward: 0.061529\n",
      "step: 0.040386\n",
      "total: 0.316088\n",
      "\n",
      "Iteration 3\n",
      "Policy loss: -0.008028. Value loss: 2.180576. Entropy: 1.747995.\n",
      "\n",
      "load: 0.133336\n",
      "forward: 0.062260\n",
      "loss: 0.019695\n",
      "clip: 0.011660\n",
      "backward: 0.064795\n",
      "step: 0.042519\n",
      "total: 0.334264\n",
      "\n",
      "Iteration 4\n",
      "Policy loss: -0.010339. Value loss: 1.739343. Entropy: 1.744571.\n",
      "\n",
      "load: 0.120122\n",
      "forward: 0.056608\n",
      "loss: 0.016896\n",
      "clip: 0.010602\n",
      "backward: 0.060512\n",
      "step: 0.038469\n",
      "total: 0.303210\n",
      "\n",
      "Iteration 5\n",
      "Policy loss: -0.020458. Value loss: 1.287633. Entropy: 1.749579.\n",
      "\n",
      "load: 0.135750\n",
      "forward: 0.061584\n",
      "loss: 0.019639\n",
      "clip: 0.011582\n",
      "backward: 0.064778\n",
      "step: 0.041376\n",
      "total: 0.334709\n",
      "\n",
      "episode: 3   score: 105.0   memory length: 1024   epsilon: 1.0    steps: 633     evaluation reward: 98.75\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.000730. Value loss: 8.273971. Entropy: 1.746608.\n",
      "\n",
      "load: 0.119044\n",
      "forward: 0.055241\n",
      "loss: 0.016577\n",
      "clip: 0.010617\n",
      "backward: 0.062248\n",
      "step: 0.039187\n",
      "total: 0.302913\n",
      "\n",
      "Iteration 2\n",
      "Policy loss: -0.018359. Value loss: 5.872566. Entropy: 1.721916.\n",
      "\n",
      "load: 0.121830\n",
      "forward: 0.055675\n",
      "loss: 0.016619\n",
      "clip: 0.010440\n",
      "backward: 0.060580\n",
      "step: 0.037830\n",
      "total: 0.302973\n",
      "\n",
      "Iteration 3\n",
      "Policy loss: -0.013144. Value loss: 3.907565. Entropy: 1.708509.\n",
      "\n",
      "load: 0.117786\n",
      "forward: 0.056131\n",
      "loss: 0.016825\n",
      "clip: 0.010090\n",
      "backward: 0.059065\n",
      "step: 0.037329\n",
      "total: 0.297227\n",
      "\n",
      "Iteration 4\n",
      "Policy loss: -0.026335. Value loss: 2.926078. Entropy: 1.728471.\n",
      "\n",
      "load: 0.120600\n",
      "forward: 0.056146\n",
      "loss: 0.016646\n",
      "clip: 0.010559\n",
      "backward: 0.060183\n",
      "step: 0.038462\n",
      "total: 0.302595\n",
      "\n",
      "Iteration 5\n",
      "Policy loss: -0.022063. Value loss: 2.770447. Entropy: 1.726988.\n",
      "\n",
      "load: 0.136745\n",
      "forward: 0.060729\n",
      "loss: 0.019269\n",
      "clip: 0.010957\n",
      "backward: 0.065587\n",
      "step: 0.040422\n",
      "total: 0.333708\n",
      "\n",
      "episode: 4   score: 120.0   memory length: 1024   epsilon: 1.0    steps: 605     evaluation reward: 103.0\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.007861. Value loss: 6.303124. Entropy: 1.726764.\n",
      "\n",
      "load: 0.122470\n",
      "forward: 0.055467\n",
      "loss: 0.016314\n",
      "clip: 0.009882\n",
      "backward: 0.058996\n",
      "step: 0.037577\n",
      "total: 0.300706\n",
      "\n",
      "Iteration 2\n",
      "Policy loss: 0.002577. Value loss: 2.989630. Entropy: 1.731790.\n",
      "\n",
      "load: 0.118794\n",
      "forward: 0.057581\n",
      "loss: 0.017008\n",
      "clip: 0.010279\n",
      "backward: 0.058593\n",
      "step: 0.037789\n",
      "total: 0.300044\n",
      "\n",
      "Iteration 3\n",
      "Policy loss: -0.008741. Value loss: 1.622386. Entropy: 1.726124.\n",
      "\n",
      "load: 0.121838\n",
      "forward: 0.056423\n",
      "loss: 0.017155\n",
      "clip: 0.010579\n",
      "backward: 0.061388\n",
      "step: 0.039167\n",
      "total: 0.306550\n",
      "\n",
      "Iteration 4\n",
      "Policy loss: -0.020177. Value loss: 1.185111. Entropy: 1.718955.\n",
      "\n",
      "load: 0.122236\n",
      "forward: 0.058333\n",
      "loss: 0.017565\n",
      "clip: 0.011583\n",
      "backward: 0.062368\n",
      "step: 0.040208\n",
      "total: 0.312293\n",
      "\n",
      "Iteration 5\n",
      "Policy loss: -0.004882. Value loss: 1.216337. Entropy: 1.705816.\n",
      "\n",
      "load: 0.119241\n",
      "forward: 0.055685\n",
      "loss: 0.016840\n",
      "clip: 0.010012\n",
      "backward: 0.058662\n",
      "step: 0.037199\n",
      "total: 0.297638\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        if render_breakout:\n",
    "            env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        action, value = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        #r = np.clip(reward, -1, 1)\n",
    "        r = reward\n",
    "        \n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state, value, 0, 0)\n",
    "        # Start training after random sample generation\n",
    "        if(frame % train_frame == 0):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network\n",
    "            agent.update_target_net()\n",
    "        score += r\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > 40 and len(evaluation_reward) > 350:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn\")\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
