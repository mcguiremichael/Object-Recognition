{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "from env import GameEnv\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "envs = []\n",
    "for i in range(num_envs):\n",
    "    envs.append(GameEnv('SpaceInvadersDeterministic-v4'))\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = envs[0].life\n",
    "state_size = envs[0].observation_space.shape\n",
    "action_size = envs[0].action_space.n\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "torch.save(agent.policy_net.state_dict(), \"./save_model/spaceinvaders_ppo_best\")\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n",
    "reset_max = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'envs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-77aed6047ce3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvis_env_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvis_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvis_env_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmax_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'envs' is not defined"
     ]
    }
   ],
   "source": [
    "vis_env_idx = 0\n",
    "vis_env = envs[vis_env_idx]\n",
    "e = 0\n",
    "frame = 0\n",
    "max_eval = -np.inf\n",
    "reset_count = 0\n",
    "\n",
    "while (frame < 10000000):\n",
    "    step = 0\n",
    "    assert(num_envs * env_mem_size == train_frame)\n",
    "    frame_next_vals = []\n",
    "    for i in range(num_envs):\n",
    "        env = envs[i]\n",
    "        #history = env.history\n",
    "        #life = env.life\n",
    "        #state, reward, done, info = [env.state, env.reward, env.done, env.info]\n",
    "        for j in range(env_mem_size):\n",
    "            step += 1\n",
    "            frame += 1\n",
    "            \n",
    "            curr_state = env.history[HISTORY_SIZE-1,:,:]\n",
    "            action, value = agent.get_action(np.float32(env.history[:HISTORY_SIZE,:,:]) / 255.)\n",
    "            \n",
    "            next_state, env.reward, env.done, env.info = env.step(action)\n",
    "            \n",
    "            if (i == vis_env_idx):\n",
    "                vis_env._env.render()\n",
    "            \n",
    "            frame_next_state = get_frame(next_state)\n",
    "            env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "            terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "            \n",
    "            env.life = env.info['ale.lives']\n",
    "            r = env.reward\n",
    "            \n",
    "            agent.memory.push(i, deepcopy(curr_state), action, r, terminal_state, value, 0, 0)\n",
    "            if (j == env_mem_size-1):\n",
    "                _, frame_next_val = agent.get_action(np.float32(env.history[1:,:,:]) / 255.)\n",
    "                frame_next_vals.append(frame_next_val)\n",
    "            env.score += r\n",
    "            env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "            \n",
    "            if (env.done):\n",
    "                if (e % 50 == 0):\n",
    "                    print('now time : ', datetime.now())\n",
    "                    rewards.append(np.mean(evaluation_reward))\n",
    "                    episodes.append(e)\n",
    "                    pylab.plot(episodes, rewards, 'b')\n",
    "                    pylab.savefig(\"./save_graph/spaceinvaders_ppo.png\")\n",
    "                    torch.save(agent.policy_net, \"./save_model/spaceinvaders_ppo\")\n",
    "                    \n",
    "                    if np.mean(evaluation_reward) > max_eval:\n",
    "                        torch.save(agent.policy_net.state_dict(), \"./save_model/spaceinvaders_ppo_best\")\n",
    "                        max_eval = float(np.mean(evaluation_reward))\n",
    "                        reset_count = 0\n",
    "                    elif e > 5000:\n",
    "                        reset_count += 1\n",
    "                        \"\"\"\n",
    "                        if (reset_count == reset_max):\n",
    "                            print(\"Training went nowhere, starting again at best model\")\n",
    "                            agent.policy_net.load_state_dict(torch.load(\"./save_model/spaceinvaders_ppo_best\"))\n",
    "                            agent.update_target_net()\n",
    "                            reset_count = 0\n",
    "                        \"\"\"\n",
    "                e += 1\n",
    "                evaluation_reward.append(env.score)\n",
    "                print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "                \n",
    "                env.done = False\n",
    "                env.score = 0\n",
    "                env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "                env.state = env.reset()\n",
    "                env.life = number_lives\n",
    "                get_init_state(env.history, env.state)\n",
    "                \n",
    "                \n",
    "                \n",
    "    agent.train_policy_net(frame, frame_next_vals)\n",
    "    agent.update_target_net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/spaceinvaders_ppo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " ------- STARTING TRAINING FOR Breakout-v0 ------- \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determing min/max rewards of environment\n",
      "Min: 0. Max: 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],1)[0].unsqueeze(1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000250. clip: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:260: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:261: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:262: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Policy loss: 0.002390. Value loss: 0.227342. Entropy: 1.384487.\n",
      "Iteration 2: Policy loss: 0.001047. Value loss: 0.233750. Entropy: 1.385055.\n",
      "Iteration 3: Policy loss: 0.000125. Value loss: 0.223898. Entropy: 1.384596.\n",
      "now time :  2019-03-06 12:27:39.413569\n",
      "episode: 1   score: 0.0  epsilon: 1.0    steps: 304  evaluation reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2   score: 0.0  epsilon: 1.0    steps: 448  evaluation reward: 0.0\n",
      "episode: 3   score: 0.0  epsilon: 1.0    steps: 464  evaluation reward: 0.0\n",
      "episode: 4   score: 1.0  epsilon: 1.0    steps: 704  evaluation reward: 0.25\n",
      "episode: 5   score: 1.0  epsilon: 1.0    steps: 936  evaluation reward: 0.4\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 4: Policy loss: 0.000030. Value loss: 0.191240. Entropy: 1.384423.\n",
      "Iteration 5: Policy loss: -0.001439. Value loss: 0.187264. Entropy: 1.383807.\n",
      "Iteration 6: Policy loss: 0.000962. Value loss: 0.168607. Entropy: 1.384462.\n",
      "episode: 6   score: 2.0  epsilon: 1.0    steps: 32  evaluation reward: 0.6666666666666666\n",
      "episode: 7   score: 2.0  epsilon: 1.0    steps: 56  evaluation reward: 0.8571428571428571\n",
      "episode: 8   score: 2.0  epsilon: 1.0    steps: 144  evaluation reward: 1.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 7: Policy loss: -0.001907. Value loss: 0.206907. Entropy: 1.383122.\n",
      "Iteration 8: Policy loss: -0.004782. Value loss: 0.200348. Entropy: 1.380134.\n",
      "Iteration 9: Policy loss: -0.000873. Value loss: 0.194645. Entropy: 1.381763.\n",
      "episode: 9   score: 0.0  epsilon: 1.0    steps: 264  evaluation reward: 0.8888888888888888\n",
      "episode: 10   score: 0.0  epsilon: 1.0    steps: 536  evaluation reward: 0.8\n",
      "episode: 11   score: 1.0  epsilon: 1.0    steps: 640  evaluation reward: 0.8181818181818182\n",
      "episode: 12   score: 2.0  epsilon: 1.0    steps: 744  evaluation reward: 0.9166666666666666\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 10: Policy loss: 0.001427. Value loss: 0.252104. Entropy: 1.374563.\n",
      "Iteration 11: Policy loss: -0.002750. Value loss: 0.238132. Entropy: 1.376571.\n",
      "Iteration 12: Policy loss: -0.002860. Value loss: 0.246388. Entropy: 1.376266.\n",
      "episode: 13   score: 3.0  epsilon: 1.0    steps: 160  evaluation reward: 1.0769230769230769\n",
      "episode: 14   score: 2.0  epsilon: 1.0    steps: 224  evaluation reward: 1.1428571428571428\n",
      "episode: 15   score: 2.0  epsilon: 1.0    steps: 312  evaluation reward: 1.2\n",
      "episode: 16   score: 3.0  epsilon: 1.0    steps: 328  evaluation reward: 1.3125\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 13: Policy loss: 0.000630. Value loss: 0.185171. Entropy: 1.379278.\n",
      "Iteration 14: Policy loss: -0.000906. Value loss: 0.182811. Entropy: 1.381926.\n",
      "Iteration 15: Policy loss: -0.000910. Value loss: 0.177678. Entropy: 1.383776.\n",
      "episode: 17   score: 1.0  epsilon: 1.0    steps: 120  evaluation reward: 1.2941176470588236\n",
      "episode: 18   score: 1.0  epsilon: 1.0    steps: 192  evaluation reward: 1.2777777777777777\n",
      "episode: 19   score: 1.0  epsilon: 1.0    steps: 288  evaluation reward: 1.263157894736842\n",
      "episode: 20   score: 0.0  epsilon: 1.0    steps: 408  evaluation reward: 1.2\n",
      "episode: 21   score: 0.0  epsilon: 1.0    steps: 576  evaluation reward: 1.1428571428571428\n",
      "episode: 22   score: 0.0  epsilon: 1.0    steps: 680  evaluation reward: 1.0909090909090908\n",
      "episode: 23   score: 0.0  epsilon: 1.0    steps: 744  evaluation reward: 1.0434782608695652\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 16: Policy loss: -0.000029. Value loss: 0.190640. Entropy: 1.385725.\n",
      "Iteration 17: Policy loss: -0.000658. Value loss: 0.180125. Entropy: 1.386030.\n",
      "Iteration 18: Policy loss: -0.003739. Value loss: 0.176787. Entropy: 1.385976.\n",
      "episode: 24   score: 2.0  epsilon: 1.0    steps: 288  evaluation reward: 1.0833333333333333\n",
      "episode: 25   score: 0.0  epsilon: 1.0    steps: 672  evaluation reward: 1.04\n",
      "episode: 26   score: 0.0  epsilon: 1.0    steps: 936  evaluation reward: 1.0\n",
      "episode: 27   score: 1.0  epsilon: 1.0    steps: 1024  evaluation reward: 1.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 19: Policy loss: -0.000216. Value loss: 0.295414. Entropy: 1.385554.\n",
      "Iteration 20: Policy loss: -0.000560. Value loss: 0.280805. Entropy: 1.385140.\n",
      "Iteration 21: Policy loss: -0.002749. Value loss: 0.267274. Entropy: 1.384040.\n",
      "episode: 28   score: 2.0  epsilon: 1.0    steps: 344  evaluation reward: 1.0357142857142858\n",
      "episode: 29   score: 1.0  epsilon: 1.0    steps: 696  evaluation reward: 1.0344827586206897\n",
      "episode: 30   score: 3.0  epsilon: 1.0    steps: 928  evaluation reward: 1.1\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 22: Policy loss: 0.000718. Value loss: 0.403065. Entropy: 1.384118.\n",
      "Iteration 23: Policy loss: 0.000272. Value loss: 0.385657. Entropy: 1.383701.\n",
      "Iteration 24: Policy loss: -0.001374. Value loss: 0.362969. Entropy: 1.381380.\n",
      "episode: 31   score: 2.0  epsilon: 1.0    steps: 648  evaluation reward: 1.1290322580645162\n",
      "episode: 32   score: 1.0  epsilon: 1.0    steps: 656  evaluation reward: 1.125\n",
      "episode: 33   score: 2.0  epsilon: 1.0    steps: 664  evaluation reward: 1.1515151515151516\n",
      "episode: 34   score: 4.0  epsilon: 1.0    steps: 1008  evaluation reward: 1.2352941176470589\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 25: Policy loss: -0.000135. Value loss: 0.198167. Entropy: 1.385378.\n",
      "Iteration 26: Policy loss: -0.002314. Value loss: 0.184334. Entropy: 1.385175.\n",
      "Iteration 27: Policy loss: -0.004368. Value loss: 0.163342. Entropy: 1.385221.\n",
      "episode: 35   score: 0.0  epsilon: 1.0    steps: 56  evaluation reward: 1.2\n",
      "episode: 36   score: 0.0  epsilon: 1.0    steps: 304  evaluation reward: 1.1666666666666667\n",
      "episode: 37   score: 2.0  epsilon: 1.0    steps: 368  evaluation reward: 1.1891891891891893\n",
      "episode: 38   score: 3.0  epsilon: 1.0    steps: 824  evaluation reward: 1.236842105263158\n",
      "episode: 39   score: 0.0  epsilon: 1.0    steps: 992  evaluation reward: 1.205128205128205\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 28: Policy loss: 0.001589. Value loss: 0.382712. Entropy: 1.384433.\n",
      "Iteration 29: Policy loss: -0.001130. Value loss: 0.361658. Entropy: 1.382692.\n",
      "Iteration 30: Policy loss: -0.003238. Value loss: 0.341042. Entropy: 1.382035.\n",
      "episode: 40   score: 0.0  epsilon: 1.0    steps: 64  evaluation reward: 1.175\n",
      "episode: 41   score: 1.0  epsilon: 1.0    steps: 768  evaluation reward: 1.170731707317073\n",
      "episode: 42   score: 1.0  epsilon: 1.0    steps: 952  evaluation reward: 1.1666666666666667\n",
      "episode: 43   score: 3.0  epsilon: 1.0    steps: 968  evaluation reward: 1.2093023255813953\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 31: Policy loss: -0.001074. Value loss: 0.373317. Entropy: 1.382792.\n",
      "Iteration 32: Policy loss: -0.002719. Value loss: 0.314688. Entropy: 1.382058.\n",
      "Iteration 33: Policy loss: -0.005532. Value loss: 0.295705. Entropy: 1.381906.\n",
      "episode: 44   score: 0.0  epsilon: 1.0    steps: 552  evaluation reward: 1.1818181818181819\n",
      "episode: 45   score: 2.0  epsilon: 1.0    steps: 728  evaluation reward: 1.2\n",
      "episode: 46   score: 1.0  epsilon: 1.0    steps: 872  evaluation reward: 1.1956521739130435\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 34: Policy loss: 0.001361. Value loss: 0.300529. Entropy: 1.376502.\n",
      "Iteration 35: Policy loss: -0.002074. Value loss: 0.273008. Entropy: 1.379179.\n",
      "Iteration 36: Policy loss: -0.003510. Value loss: 0.250062. Entropy: 1.379163.\n",
      "episode: 47   score: 1.0  epsilon: 1.0    steps: 328  evaluation reward: 1.1914893617021276\n",
      "episode: 48   score: 2.0  epsilon: 1.0    steps: 416  evaluation reward: 1.2083333333333333\n",
      "episode: 49   score: 4.0  epsilon: 1.0    steps: 520  evaluation reward: 1.2653061224489797\n",
      "episode: 50   score: 1.0  epsilon: 1.0    steps: 704  evaluation reward: 1.26\n",
      "now time :  2019-03-06 12:28:24.439633\n",
      "episode: 51   score: 0.0  epsilon: 1.0    steps: 904  evaluation reward: 1.2352941176470589\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 37: Policy loss: 0.001602. Value loss: 0.269088. Entropy: 1.380272.\n",
      "Iteration 38: Policy loss: -0.000237. Value loss: 0.237008. Entropy: 1.378294.\n",
      "Iteration 39: Policy loss: -0.001905. Value loss: 0.230638. Entropy: 1.378410.\n",
      "episode: 52   score: 2.0  epsilon: 1.0    steps: 256  evaluation reward: 1.25\n",
      "episode: 53   score: 0.0  epsilon: 1.0    steps: 656  evaluation reward: 1.2264150943396226\n",
      "episode: 54   score: 1.0  epsilon: 1.0    steps: 696  evaluation reward: 1.2222222222222223\n",
      "episode: 55   score: 0.0  epsilon: 1.0    steps: 920  evaluation reward: 1.2\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 40: Policy loss: 0.005840. Value loss: 0.097307. Entropy: 1.381558.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 41: Policy loss: 0.001151. Value loss: 0.080172. Entropy: 1.379997.\n",
      "Iteration 42: Policy loss: -0.000961. Value loss: 0.077325. Entropy: 1.377936.\n",
      "episode: 56   score: 1.0  epsilon: 1.0    steps: 400  evaluation reward: 1.1964285714285714\n",
      "episode: 57   score: 0.0  epsilon: 1.0    steps: 408  evaluation reward: 1.1754385964912282\n",
      "episode: 58   score: 2.0  epsilon: 1.0    steps: 704  evaluation reward: 1.1896551724137931\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 43: Policy loss: 0.000578. Value loss: 0.341171. Entropy: 1.375225.\n",
      "Iteration 44: Policy loss: -0.001975. Value loss: 0.312889. Entropy: 1.378906.\n",
      "Iteration 45: Policy loss: -0.003862. Value loss: 0.288536. Entropy: 1.378144.\n",
      "episode: 59   score: 0.0  epsilon: 1.0    steps: 16  evaluation reward: 1.1694915254237288\n",
      "episode: 60   score: 2.0  epsilon: 1.0    steps: 440  evaluation reward: 1.1833333333333333\n",
      "episode: 61   score: 5.0  epsilon: 1.0    steps: 464  evaluation reward: 1.2459016393442623\n",
      "episode: 62   score: 1.0  epsilon: 1.0    steps: 552  evaluation reward: 1.2419354838709677\n",
      "episode: 63   score: 1.0  epsilon: 1.0    steps: 608  evaluation reward: 1.2380952380952381\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 46: Policy loss: -0.000152. Value loss: 0.199240. Entropy: 1.380429.\n",
      "Iteration 47: Policy loss: -0.003062. Value loss: 0.166384. Entropy: 1.379268.\n",
      "Iteration 48: Policy loss: -0.005106. Value loss: 0.147932. Entropy: 1.379454.\n",
      "episode: 64   score: 0.0  epsilon: 1.0    steps: 168  evaluation reward: 1.21875\n",
      "episode: 65   score: 1.0  epsilon: 1.0    steps: 320  evaluation reward: 1.2153846153846153\n",
      "episode: 66   score: 2.0  epsilon: 1.0    steps: 576  evaluation reward: 1.2272727272727273\n",
      "episode: 67   score: 0.0  epsilon: 1.0    steps: 936  evaluation reward: 1.208955223880597\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 49: Policy loss: 0.001032. Value loss: 0.173337. Entropy: 1.370782.\n",
      "Iteration 50: Policy loss: -0.002366. Value loss: 0.161013. Entropy: 1.368817.\n",
      "Iteration 51: Policy loss: -0.007510. Value loss: 0.159371. Entropy: 1.368875.\n",
      "episode: 68   score: 2.0  epsilon: 1.0    steps: 184  evaluation reward: 1.2205882352941178\n",
      "episode: 69   score: 1.0  epsilon: 1.0    steps: 240  evaluation reward: 1.2173913043478262\n",
      "episode: 70   score: 1.0  epsilon: 1.0    steps: 432  evaluation reward: 1.2142857142857142\n",
      "episode: 71   score: 0.0  epsilon: 1.0    steps: 704  evaluation reward: 1.1971830985915493\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 52: Policy loss: 0.003000. Value loss: 0.288634. Entropy: 1.371059.\n",
      "Iteration 53: Policy loss: -0.002761. Value loss: 0.237655. Entropy: 1.371512.\n",
      "Iteration 54: Policy loss: -0.006272. Value loss: 0.198092. Entropy: 1.373974.\n",
      "episode: 72   score: 1.0  epsilon: 1.0    steps: 48  evaluation reward: 1.1944444444444444\n",
      "episode: 73   score: 2.0  epsilon: 1.0    steps: 200  evaluation reward: 1.2054794520547945\n",
      "episode: 74   score: 1.0  epsilon: 1.0    steps: 464  evaluation reward: 1.2027027027027026\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 55: Policy loss: 0.001193. Value loss: 0.332973. Entropy: 1.371256.\n",
      "Iteration 56: Policy loss: -0.003115. Value loss: 0.260810. Entropy: 1.373965.\n",
      "Iteration 57: Policy loss: -0.009132. Value loss: 0.217787. Entropy: 1.375144.\n",
      "episode: 75   score: 2.0  epsilon: 1.0    steps: 256  evaluation reward: 1.2133333333333334\n",
      "episode: 76   score: 2.0  epsilon: 1.0    steps: 344  evaluation reward: 1.2236842105263157\n",
      "episode: 77   score: 3.0  epsilon: 1.0    steps: 528  evaluation reward: 1.2467532467532467\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 58: Policy loss: 0.002054. Value loss: 0.300822. Entropy: 1.370121.\n",
      "Iteration 59: Policy loss: -0.002156. Value loss: 0.206323. Entropy: 1.367163.\n",
      "Iteration 60: Policy loss: -0.002154. Value loss: 0.181574. Entropy: 1.372517.\n",
      "episode: 78   score: 3.0  epsilon: 1.0    steps: 96  evaluation reward: 1.2692307692307692\n",
      "episode: 79   score: 2.0  epsilon: 1.0    steps: 280  evaluation reward: 1.2784810126582278\n",
      "episode: 80   score: 3.0  epsilon: 1.0    steps: 280  evaluation reward: 1.3\n",
      "episode: 81   score: 2.0  epsilon: 1.0    steps: 288  evaluation reward: 1.308641975308642\n",
      "episode: 82   score: 0.0  epsilon: 1.0    steps: 648  evaluation reward: 1.2926829268292683\n",
      "episode: 83   score: 0.0  epsilon: 1.0    steps: 800  evaluation reward: 1.2771084337349397\n",
      "episode: 84   score: 0.0  epsilon: 1.0    steps: 944  evaluation reward: 1.2619047619047619\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 61: Policy loss: 0.000258. Value loss: 0.223022. Entropy: 1.371602.\n",
      "Iteration 62: Policy loss: -0.002562. Value loss: 0.148885. Entropy: 1.371441.\n",
      "Iteration 63: Policy loss: -0.004593. Value loss: 0.118190. Entropy: 1.370094.\n",
      "episode: 85   score: 3.0  epsilon: 1.0    steps: 152  evaluation reward: 1.2823529411764707\n",
      "episode: 86   score: 0.0  epsilon: 1.0    steps: 672  evaluation reward: 1.2674418604651163\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 64: Policy loss: 0.000732. Value loss: 0.216904. Entropy: 1.363962.\n",
      "Iteration 65: Policy loss: -0.003378. Value loss: 0.142562. Entropy: 1.367622.\n",
      "Iteration 66: Policy loss: -0.007597. Value loss: 0.116874. Entropy: 1.366244.\n",
      "episode: 87   score: 2.0  epsilon: 1.0    steps: 128  evaluation reward: 1.2758620689655173\n",
      "episode: 88   score: 1.0  epsilon: 1.0    steps: 480  evaluation reward: 1.2727272727272727\n",
      "episode: 89   score: 0.0  epsilon: 1.0    steps: 512  evaluation reward: 1.2584269662921348\n",
      "episode: 90   score: 2.0  epsilon: 1.0    steps: 872  evaluation reward: 1.2666666666666666\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 67: Policy loss: 0.001675. Value loss: 0.253729. Entropy: 1.362549.\n",
      "Iteration 68: Policy loss: -0.001416. Value loss: 0.189114. Entropy: 1.364703.\n",
      "Iteration 69: Policy loss: -0.003926. Value loss: 0.152604. Entropy: 1.362493.\n",
      "episode: 91   score: 4.0  epsilon: 1.0    steps: 168  evaluation reward: 1.2967032967032968\n",
      "episode: 92   score: 4.0  epsilon: 1.0    steps: 448  evaluation reward: 1.326086956521739\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 70: Policy loss: -0.001651. Value loss: 0.171594. Entropy: 1.360752.\n",
      "Iteration 71: Policy loss: -0.002856. Value loss: 0.094763. Entropy: 1.357530.\n",
      "Iteration 72: Policy loss: -0.002951. Value loss: 0.075846. Entropy: 1.368802.\n",
      "episode: 93   score: 2.0  epsilon: 1.0    steps: 208  evaluation reward: 1.3333333333333333\n",
      "episode: 94   score: 3.0  epsilon: 1.0    steps: 352  evaluation reward: 1.351063829787234\n",
      "episode: 95   score: 5.0  epsilon: 1.0    steps: 592  evaluation reward: 1.3894736842105264\n",
      "episode: 96   score: 2.0  epsilon: 1.0    steps: 736  evaluation reward: 1.3958333333333333\n",
      "episode: 97   score: 2.0  epsilon: 1.0    steps: 880  evaluation reward: 1.402061855670103\n",
      "episode: 98   score: 0.0  epsilon: 1.0    steps: 936  evaluation reward: 1.3877551020408163\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 73: Policy loss: 0.001700. Value loss: 0.201359. Entropy: 1.360654.\n",
      "Iteration 74: Policy loss: -0.003682. Value loss: 0.120801. Entropy: 1.365431.\n",
      "Iteration 75: Policy loss: -0.007028. Value loss: 0.094141. Entropy: 1.361657.\n",
      "episode: 99   score: 2.0  epsilon: 1.0    steps: 296  evaluation reward: 1.393939393939394\n",
      "episode: 100   score: 0.0  epsilon: 1.0    steps: 664  evaluation reward: 1.38\n",
      "now time :  2019-03-06 12:29:15.086710\n",
      "episode: 101   score: 4.0  epsilon: 1.0    steps: 736  evaluation reward: 1.42\n",
      "episode: 102   score: 0.0  epsilon: 1.0    steps: 896  evaluation reward: 1.42\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 76: Policy loss: 0.000447. Value loss: 0.164927. Entropy: 1.361299.\n",
      "Iteration 77: Policy loss: -0.005887. Value loss: 0.113442. Entropy: 1.367377.\n",
      "Iteration 78: Policy loss: -0.005554. Value loss: 0.084971. Entropy: 1.367161.\n",
      "episode: 103   score: 0.0  epsilon: 1.0    steps: 320  evaluation reward: 1.42\n",
      "episode: 104   score: 1.0  epsilon: 1.0    steps: 552  evaluation reward: 1.42\n",
      "episode: 105   score: 3.0  epsilon: 1.0    steps: 568  evaluation reward: 1.44\n",
      "episode: 106   score: 0.0  epsilon: 1.0    steps: 712  evaluation reward: 1.42\n",
      "episode: 107   score: 0.0  epsilon: 1.0    steps: 1024  evaluation reward: 1.4\n",
      "Training network. lr: 0.000250. clip: 0.099853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 79: Policy loss: -0.000197. Value loss: 0.154334. Entropy: 1.368435.\n",
      "Iteration 80: Policy loss: -0.007038. Value loss: 0.111079. Entropy: 1.366695.\n",
      "Iteration 81: Policy loss: -0.013366. Value loss: 0.083446. Entropy: 1.367448.\n",
      "episode: 108   score: 2.0  epsilon: 1.0    steps: 16  evaluation reward: 1.4\n",
      "episode: 109   score: 0.0  epsilon: 1.0    steps: 216  evaluation reward: 1.4\n",
      "episode: 110   score: 0.0  epsilon: 1.0    steps: 696  evaluation reward: 1.4\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 82: Policy loss: 0.000120. Value loss: 0.161663. Entropy: 1.375745.\n",
      "Iteration 83: Policy loss: -0.006593. Value loss: 0.121681. Entropy: 1.376585.\n",
      "Iteration 84: Policy loss: -0.007413. Value loss: 0.105201. Entropy: 1.376545.\n",
      "episode: 111   score: 1.0  epsilon: 1.0    steps: 168  evaluation reward: 1.4\n",
      "episode: 112   score: 0.0  epsilon: 1.0    steps: 176  evaluation reward: 1.38\n",
      "episode: 113   score: 0.0  epsilon: 1.0    steps: 424  evaluation reward: 1.35\n",
      "episode: 114   score: 1.0  epsilon: 1.0    steps: 440  evaluation reward: 1.34\n",
      "episode: 115   score: 0.0  epsilon: 1.0    steps: 560  evaluation reward: 1.32\n",
      "episode: 116   score: 3.0  epsilon: 1.0    steps: 568  evaluation reward: 1.32\n",
      "episode: 117   score: 0.0  epsilon: 1.0    steps: 616  evaluation reward: 1.31\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 85: Policy loss: -0.000020. Value loss: 0.377511. Entropy: 1.377961.\n",
      "Iteration 86: Policy loss: -0.002230. Value loss: 0.274044. Entropy: 1.377717.\n",
      "Iteration 87: Policy loss: -0.008715. Value loss: 0.193150. Entropy: 1.376592.\n",
      "episode: 118   score: 1.0  epsilon: 1.0    steps: 360  evaluation reward: 1.31\n",
      "episode: 119   score: 0.0  epsilon: 1.0    steps: 520  evaluation reward: 1.3\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 88: Policy loss: 0.001257. Value loss: 0.240190. Entropy: 1.365840.\n",
      "Iteration 89: Policy loss: -0.005136. Value loss: 0.133872. Entropy: 1.363824.\n",
      "Iteration 90: Policy loss: -0.009280. Value loss: 0.095060. Entropy: 1.362865.\n",
      "episode: 120   score: 2.0  epsilon: 1.0    steps: 712  evaluation reward: 1.32\n",
      "episode: 121   score: 2.0  epsilon: 1.0    steps: 840  evaluation reward: 1.34\n",
      "episode: 122   score: 3.0  epsilon: 1.0    steps: 936  evaluation reward: 1.37\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 91: Policy loss: 0.006413. Value loss: 0.268630. Entropy: 1.367921.\n",
      "Iteration 92: Policy loss: 0.000715. Value loss: 0.151285. Entropy: 1.366587.\n",
      "Iteration 93: Policy loss: -0.004165. Value loss: 0.096577. Entropy: 1.366031.\n",
      "episode: 123   score: 3.0  epsilon: 1.0    steps: 160  evaluation reward: 1.4\n",
      "episode: 124   score: 1.0  epsilon: 1.0    steps: 376  evaluation reward: 1.39\n",
      "episode: 125   score: 5.0  epsilon: 1.0    steps: 536  evaluation reward: 1.44\n",
      "episode: 126   score: 4.0  epsilon: 1.0    steps: 960  evaluation reward: 1.48\n",
      "episode: 127   score: 3.0  epsilon: 1.0    steps: 1000  evaluation reward: 1.5\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 94: Policy loss: 0.001538. Value loss: 0.455703. Entropy: 1.373173.\n",
      "Iteration 95: Policy loss: -0.002723. Value loss: 0.330829. Entropy: 1.368672.\n",
      "Iteration 96: Policy loss: -0.005421. Value loss: 0.248275. Entropy: 1.368705.\n",
      "episode: 128   score: 0.0  epsilon: 1.0    steps: 256  evaluation reward: 1.48\n",
      "episode: 129   score: 1.0  epsilon: 1.0    steps: 840  evaluation reward: 1.48\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 97: Policy loss: 0.005460. Value loss: 0.327554. Entropy: 1.365154.\n",
      "Iteration 98: Policy loss: -0.001225. Value loss: 0.173665. Entropy: 1.360915.\n",
      "Iteration 99: Policy loss: -0.010006. Value loss: 0.125134. Entropy: 1.360900.\n",
      "episode: 130   score: 1.0  epsilon: 1.0    steps: 48  evaluation reward: 1.46\n",
      "episode: 131   score: 0.0  epsilon: 1.0    steps: 464  evaluation reward: 1.44\n",
      "episode: 132   score: 0.0  epsilon: 1.0    steps: 504  evaluation reward: 1.43\n",
      "episode: 133   score: 4.0  epsilon: 1.0    steps: 880  evaluation reward: 1.45\n",
      "episode: 134   score: 2.0  epsilon: 1.0    steps: 944  evaluation reward: 1.43\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 100: Policy loss: 0.002200. Value loss: 0.315351. Entropy: 1.364667.\n",
      "Iteration 101: Policy loss: -0.005322. Value loss: 0.194544. Entropy: 1.363072.\n",
      "Iteration 102: Policy loss: -0.011348. Value loss: 0.148883. Entropy: 1.363229.\n",
      "episode: 135   score: 4.0  epsilon: 1.0    steps: 8  evaluation reward: 1.47\n",
      "episode: 136   score: 2.0  epsilon: 1.0    steps: 120  evaluation reward: 1.49\n",
      "episode: 137   score: 0.0  epsilon: 1.0    steps: 296  evaluation reward: 1.47\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 103: Policy loss: 0.002114. Value loss: 0.316556. Entropy: 1.366582.\n",
      "Iteration 104: Policy loss: -0.004336. Value loss: 0.187127. Entropy: 1.366247.\n",
      "Iteration 105: Policy loss: -0.009381. Value loss: 0.130769. Entropy: 1.368025.\n",
      "episode: 138   score: 0.0  epsilon: 1.0    steps: 264  evaluation reward: 1.44\n",
      "episode: 139   score: 2.0  epsilon: 1.0    steps: 472  evaluation reward: 1.46\n",
      "episode: 140   score: 2.0  epsilon: 1.0    steps: 696  evaluation reward: 1.48\n",
      "episode: 141   score: 1.0  epsilon: 1.0    steps: 800  evaluation reward: 1.48\n",
      "episode: 142   score: 1.0  epsilon: 1.0    steps: 816  evaluation reward: 1.48\n",
      "episode: 143   score: 2.0  epsilon: 1.0    steps: 856  evaluation reward: 1.47\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 106: Policy loss: 0.002164. Value loss: 0.277367. Entropy: 1.364372.\n",
      "Iteration 107: Policy loss: -0.002287. Value loss: 0.166473. Entropy: 1.365683.\n",
      "Iteration 108: Policy loss: -0.004925. Value loss: 0.102702. Entropy: 1.365299.\n",
      "episode: 144   score: 2.0  epsilon: 1.0    steps: 424  evaluation reward: 1.49\n",
      "episode: 145   score: 2.0  epsilon: 1.0    steps: 664  evaluation reward: 1.49\n",
      "episode: 146   score: 0.0  epsilon: 1.0    steps: 992  evaluation reward: 1.48\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 109: Policy loss: 0.003555. Value loss: 0.236756. Entropy: 1.371664.\n",
      "Iteration 110: Policy loss: -0.004854. Value loss: 0.167803. Entropy: 1.375785.\n",
      "Iteration 111: Policy loss: -0.007065. Value loss: 0.122712. Entropy: 1.375056.\n",
      "episode: 147   score: 0.0  epsilon: 1.0    steps: 168  evaluation reward: 1.47\n",
      "episode: 148   score: 0.0  epsilon: 1.0    steps: 168  evaluation reward: 1.45\n",
      "episode: 149   score: 0.0  epsilon: 1.0    steps: 184  evaluation reward: 1.41\n",
      "episode: 150   score: 1.0  epsilon: 1.0    steps: 416  evaluation reward: 1.41\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 112: Policy loss: -0.001671. Value loss: 0.176027. Entropy: 1.378110.\n",
      "Iteration 113: Policy loss: -0.004058. Value loss: 0.117139. Entropy: 1.378459.\n",
      "Iteration 114: Policy loss: -0.005620. Value loss: 0.084125. Entropy: 1.377447.\n",
      "now time :  2019-03-06 12:30:04.851707\n",
      "episode: 151   score: 3.0  epsilon: 1.0    steps: 32  evaluation reward: 1.44\n",
      "episode: 152   score: 0.0  epsilon: 1.0    steps: 56  evaluation reward: 1.42\n",
      "episode: 153   score: 1.0  epsilon: 1.0    steps: 312  evaluation reward: 1.43\n",
      "episode: 154   score: 0.0  epsilon: 1.0    steps: 360  evaluation reward: 1.42\n",
      "episode: 155   score: 0.0  epsilon: 1.0    steps: 488  evaluation reward: 1.42\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 115: Policy loss: 0.000051. Value loss: 0.310926. Entropy: 1.367813.\n",
      "Iteration 116: Policy loss: -0.002122. Value loss: 0.198788. Entropy: 1.367731.\n",
      "Iteration 117: Policy loss: -0.007709. Value loss: 0.165710. Entropy: 1.367856.\n",
      "episode: 156   score: 2.0  epsilon: 1.0    steps: 304  evaluation reward: 1.43\n",
      "episode: 157   score: 2.0  epsilon: 1.0    steps: 424  evaluation reward: 1.45\n",
      "episode: 158   score: 2.0  epsilon: 1.0    steps: 688  evaluation reward: 1.45\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 118: Policy loss: 0.002081. Value loss: 0.200241. Entropy: 1.374904.\n",
      "Iteration 119: Policy loss: -0.005578. Value loss: 0.127696. Entropy: 1.371191.\n",
      "Iteration 120: Policy loss: -0.009269. Value loss: 0.090909. Entropy: 1.368444.\n",
      "episode: 159   score: 2.0  epsilon: 1.0    steps: 160  evaluation reward: 1.47\n",
      "episode: 160   score: 2.0  epsilon: 1.0    steps: 392  evaluation reward: 1.47\n",
      "episode: 161   score: 1.0  epsilon: 1.0    steps: 432  evaluation reward: 1.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 162   score: 2.0  epsilon: 1.0    steps: 528  evaluation reward: 1.44\n",
      "episode: 163   score: 3.0  epsilon: 1.0    steps: 624  evaluation reward: 1.46\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 121: Policy loss: -0.000204. Value loss: 0.212706. Entropy: 1.366190.\n",
      "Iteration 122: Policy loss: -0.008553. Value loss: 0.120244. Entropy: 1.364973.\n",
      "Iteration 123: Policy loss: -0.013215. Value loss: 0.072746. Entropy: 1.362384.\n",
      "episode: 164   score: 0.0  epsilon: 1.0    steps: 88  evaluation reward: 1.46\n",
      "episode: 165   score: 1.0  epsilon: 1.0    steps: 272  evaluation reward: 1.46\n",
      "episode: 166   score: 2.0  epsilon: 1.0    steps: 696  evaluation reward: 1.46\n",
      "episode: 167   score: 0.0  epsilon: 1.0    steps: 704  evaluation reward: 1.46\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 124: Policy loss: 0.004117. Value loss: 0.192623. Entropy: 1.370597.\n",
      "Iteration 125: Policy loss: -0.001583. Value loss: 0.106195. Entropy: 1.373031.\n",
      "Iteration 126: Policy loss: -0.007686. Value loss: 0.077301. Entropy: 1.371117.\n",
      "episode: 168   score: 1.0  epsilon: 1.0    steps: 128  evaluation reward: 1.45\n",
      "episode: 169   score: 0.0  epsilon: 1.0    steps: 336  evaluation reward: 1.44\n",
      "episode: 170   score: 1.0  epsilon: 1.0    steps: 496  evaluation reward: 1.44\n",
      "episode: 171   score: 2.0  epsilon: 1.0    steps: 608  evaluation reward: 1.46\n",
      "episode: 172   score: 3.0  epsilon: 1.0    steps: 640  evaluation reward: 1.48\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 127: Policy loss: -0.002491. Value loss: 0.188231. Entropy: 1.368151.\n",
      "Iteration 128: Policy loss: -0.012377. Value loss: 0.108941. Entropy: 1.371470.\n",
      "Iteration 129: Policy loss: -0.015095. Value loss: 0.077507. Entropy: 1.369793.\n",
      "episode: 173   score: 0.0  epsilon: 1.0    steps: 144  evaluation reward: 1.46\n",
      "episode: 174   score: 2.0  epsilon: 1.0    steps: 416  evaluation reward: 1.47\n",
      "episode: 175   score: 0.0  epsilon: 1.0    steps: 880  evaluation reward: 1.45\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 130: Policy loss: 0.002978. Value loss: 0.180061. Entropy: 1.367872.\n",
      "Iteration 131: Policy loss: -0.005691. Value loss: 0.106646. Entropy: 1.366809.\n",
      "Iteration 132: Policy loss: -0.009318. Value loss: 0.075979. Entropy: 1.364616.\n",
      "episode: 176   score: 0.0  epsilon: 1.0    steps: 48  evaluation reward: 1.43\n",
      "episode: 177   score: 2.0  epsilon: 1.0    steps: 288  evaluation reward: 1.42\n",
      "episode: 178   score: 3.0  epsilon: 1.0    steps: 400  evaluation reward: 1.42\n",
      "episode: 179   score: 2.0  epsilon: 1.0    steps: 456  evaluation reward: 1.42\n",
      "episode: 180   score: 0.0  epsilon: 1.0    steps: 840  evaluation reward: 1.39\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 133: Policy loss: 0.001992. Value loss: 0.181123. Entropy: 1.364504.\n",
      "Iteration 134: Policy loss: -0.007784. Value loss: 0.107831. Entropy: 1.360393.\n",
      "Iteration 135: Policy loss: -0.010585. Value loss: 0.073409. Entropy: 1.360176.\n",
      "episode: 181   score: 3.0  epsilon: 1.0    steps: 552  evaluation reward: 1.4\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 136: Policy loss: 0.003273. Value loss: 0.232352. Entropy: 1.355844.\n",
      "Iteration 137: Policy loss: -0.006438. Value loss: 0.110891. Entropy: 1.356680.\n",
      "Iteration 138: Policy loss: -0.010163. Value loss: 0.077296. Entropy: 1.359123.\n",
      "episode: 182   score: 3.0  epsilon: 1.0    steps: 128  evaluation reward: 1.43\n",
      "episode: 183   score: 2.0  epsilon: 1.0    steps: 216  evaluation reward: 1.45\n",
      "episode: 184   score: 2.0  epsilon: 1.0    steps: 464  evaluation reward: 1.47\n",
      "episode: 185   score: 3.0  epsilon: 1.0    steps: 536  evaluation reward: 1.47\n",
      "episode: 186   score: 1.0  epsilon: 1.0    steps: 704  evaluation reward: 1.48\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 139: Policy loss: 0.001739. Value loss: 0.228401. Entropy: 1.361656.\n",
      "Iteration 140: Policy loss: -0.008313. Value loss: 0.125914. Entropy: 1.359225.\n",
      "Iteration 141: Policy loss: -0.012757. Value loss: 0.097414. Entropy: 1.359248.\n",
      "episode: 187   score: 3.0  epsilon: 1.0    steps: 176  evaluation reward: 1.49\n",
      "episode: 188   score: 0.0  epsilon: 1.0    steps: 600  evaluation reward: 1.48\n",
      "episode: 189   score: 2.0  epsilon: 1.0    steps: 648  evaluation reward: 1.5\n",
      "episode: 190   score: 5.0  epsilon: 1.0    steps: 752  evaluation reward: 1.53\n",
      "episode: 191   score: 0.0  epsilon: 1.0    steps: 864  evaluation reward: 1.49\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 142: Policy loss: 0.000293. Value loss: 0.152807. Entropy: 1.368970.\n",
      "Iteration 143: Policy loss: -0.005252. Value loss: 0.069417. Entropy: 1.369525.\n",
      "Iteration 144: Policy loss: -0.012610. Value loss: 0.044366. Entropy: 1.365651.\n",
      "episode: 192   score: 0.0  epsilon: 1.0    steps: 160  evaluation reward: 1.45\n",
      "episode: 193   score: 2.0  epsilon: 1.0    steps: 464  evaluation reward: 1.45\n",
      "episode: 194   score: 0.0  epsilon: 1.0    steps: 920  evaluation reward: 1.42\n",
      "episode: 195   score: 1.0  epsilon: 1.0    steps: 960  evaluation reward: 1.38\n",
      "episode: 196   score: 0.0  epsilon: 1.0    steps: 1000  evaluation reward: 1.36\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 145: Policy loss: 0.000955. Value loss: 0.163546. Entropy: 1.345777.\n",
      "Iteration 146: Policy loss: -0.005376. Value loss: 0.074637. Entropy: 1.348327.\n",
      "Iteration 147: Policy loss: -0.011751. Value loss: 0.050840. Entropy: 1.342287.\n",
      "episode: 197   score: 0.0  epsilon: 1.0    steps: 152  evaluation reward: 1.34\n",
      "episode: 198   score: 3.0  epsilon: 1.0    steps: 312  evaluation reward: 1.37\n",
      "episode: 199   score: 0.0  epsilon: 1.0    steps: 512  evaluation reward: 1.35\n",
      "episode: 200   score: 0.0  epsilon: 1.0    steps: 840  evaluation reward: 1.35\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 148: Policy loss: 0.001386. Value loss: 0.099751. Entropy: 1.358115.\n",
      "Iteration 149: Policy loss: -0.008231. Value loss: 0.059307. Entropy: 1.356561.\n",
      "Iteration 150: Policy loss: -0.009113. Value loss: 0.046470. Entropy: 1.356886.\n",
      "now time :  2019-03-06 12:30:51.398072\n",
      "episode: 201   score: 3.0  epsilon: 1.0    steps: 304  evaluation reward: 1.34\n",
      "episode: 202   score: 1.0  epsilon: 1.0    steps: 736  evaluation reward: 1.35\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 151: Policy loss: 0.001630. Value loss: 0.187045. Entropy: 1.323441.\n",
      "Iteration 152: Policy loss: -0.008578. Value loss: 0.108248. Entropy: 1.328945.\n",
      "Iteration 153: Policy loss: -0.013193. Value loss: 0.087101. Entropy: 1.326121.\n",
      "episode: 203   score: 2.0  epsilon: 1.0    steps: 80  evaluation reward: 1.37\n",
      "episode: 204   score: 2.0  epsilon: 1.0    steps: 192  evaluation reward: 1.38\n",
      "episode: 205   score: 1.0  epsilon: 1.0    steps: 224  evaluation reward: 1.36\n",
      "episode: 206   score: 2.0  epsilon: 1.0    steps: 472  evaluation reward: 1.38\n",
      "episode: 207   score: 2.0  epsilon: 1.0    steps: 584  evaluation reward: 1.4\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 154: Policy loss: 0.003269. Value loss: 0.213617. Entropy: 1.355624.\n",
      "Iteration 155: Policy loss: -0.004724. Value loss: 0.118437. Entropy: 1.356398.\n",
      "Iteration 156: Policy loss: -0.008169. Value loss: 0.079184. Entropy: 1.356880.\n",
      "episode: 208   score: 2.0  epsilon: 1.0    steps: 264  evaluation reward: 1.4\n",
      "episode: 209   score: 0.0  epsilon: 1.0    steps: 424  evaluation reward: 1.4\n",
      "episode: 210   score: 2.0  epsilon: 1.0    steps: 648  evaluation reward: 1.42\n",
      "episode: 211   score: 4.0  epsilon: 1.0    steps: 784  evaluation reward: 1.45\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 157: Policy loss: 0.001968. Value loss: 0.263864. Entropy: 1.342357.\n",
      "Iteration 158: Policy loss: -0.007328. Value loss: 0.152921. Entropy: 1.343634.\n",
      "Iteration 159: Policy loss: -0.013922. Value loss: 0.099065. Entropy: 1.342149.\n",
      "episode: 212   score: 2.0  epsilon: 1.0    steps: 304  evaluation reward: 1.47\n",
      "episode: 213   score: 2.0  epsilon: 1.0    steps: 328  evaluation reward: 1.49\n",
      "episode: 214   score: 2.0  epsilon: 1.0    steps: 600  evaluation reward: 1.5\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 160: Policy loss: 0.001690. Value loss: 0.223718. Entropy: 1.328004.\n",
      "Iteration 161: Policy loss: -0.006708. Value loss: 0.116324. Entropy: 1.333098.\n",
      "Iteration 162: Policy loss: -0.017158. Value loss: 0.092817. Entropy: 1.326954.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 215   score: 4.0  epsilon: 1.0    steps: 648  evaluation reward: 1.54\n",
      "episode: 216   score: 3.0  epsilon: 1.0    steps: 712  evaluation reward: 1.54\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 163: Policy loss: 0.000111. Value loss: 0.209568. Entropy: 1.347807.\n",
      "Iteration 164: Policy loss: -0.009831. Value loss: 0.114119. Entropy: 1.347252.\n",
      "Iteration 165: Policy loss: -0.017544. Value loss: 0.078958. Entropy: 1.337964.\n",
      "episode: 217   score: 2.0  epsilon: 1.0    steps: 104  evaluation reward: 1.56\n",
      "episode: 218   score: 1.0  epsilon: 1.0    steps: 136  evaluation reward: 1.56\n",
      "episode: 219   score: 3.0  epsilon: 1.0    steps: 200  evaluation reward: 1.59\n",
      "episode: 220   score: 5.0  epsilon: 1.0    steps: 624  evaluation reward: 1.62\n",
      "episode: 221   score: 3.0  epsilon: 1.0    steps: 736  evaluation reward: 1.63\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 166: Policy loss: 0.001925. Value loss: 0.225465. Entropy: 1.323175.\n",
      "Iteration 167: Policy loss: -0.003743. Value loss: 0.111479. Entropy: 1.329298.\n",
      "Iteration 168: Policy loss: -0.008438. Value loss: 0.084852. Entropy: 1.316428.\n",
      "episode: 222   score: 3.0  epsilon: 1.0    steps: 256  evaluation reward: 1.63\n",
      "episode: 223   score: 2.0  epsilon: 1.0    steps: 896  evaluation reward: 1.62\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 169: Policy loss: 0.002649. Value loss: 0.201838. Entropy: 1.337384.\n",
      "Iteration 170: Policy loss: -0.005193. Value loss: 0.105880. Entropy: 1.338207.\n",
      "Iteration 171: Policy loss: -0.012600. Value loss: 0.082058. Entropy: 1.343704.\n",
      "episode: 224   score: 2.0  epsilon: 1.0    steps: 200  evaluation reward: 1.63\n",
      "episode: 225   score: 2.0  epsilon: 1.0    steps: 360  evaluation reward: 1.6\n",
      "episode: 226   score: 4.0  epsilon: 1.0    steps: 800  evaluation reward: 1.6\n",
      "episode: 227   score: 3.0  epsilon: 1.0    steps: 848  evaluation reward: 1.6\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 172: Policy loss: -0.000582. Value loss: 0.346548. Entropy: 1.332116.\n",
      "Iteration 173: Policy loss: -0.009636. Value loss: 0.202334. Entropy: 1.339924.\n",
      "Iteration 174: Policy loss: -0.014556. Value loss: 0.148697. Entropy: 1.330887.\n",
      "episode: 228   score: 4.0  epsilon: 1.0    steps: 504  evaluation reward: 1.64\n",
      "episode: 229   score: 3.0  epsilon: 1.0    steps: 560  evaluation reward: 1.66\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 175: Policy loss: -0.000236. Value loss: 0.224665. Entropy: 1.326093.\n",
      "Iteration 176: Policy loss: -0.007292. Value loss: 0.106279. Entropy: 1.330488.\n",
      "Iteration 177: Policy loss: -0.013657. Value loss: 0.074937. Entropy: 1.326436.\n",
      "episode: 230   score: 1.0  epsilon: 1.0    steps: 96  evaluation reward: 1.66\n",
      "episode: 231   score: 4.0  epsilon: 1.0    steps: 224  evaluation reward: 1.7\n",
      "episode: 232   score: 2.0  epsilon: 1.0    steps: 832  evaluation reward: 1.72\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 178: Policy loss: 0.001809. Value loss: 0.183254. Entropy: 1.317337.\n",
      "Iteration 179: Policy loss: -0.008638. Value loss: 0.094941. Entropy: 1.318309.\n",
      "Iteration 180: Policy loss: -0.012605. Value loss: 0.069716. Entropy: 1.317256.\n",
      "episode: 233   score: 4.0  epsilon: 1.0    steps: 136  evaluation reward: 1.72\n",
      "episode: 234   score: 5.0  epsilon: 1.0    steps: 272  evaluation reward: 1.75\n",
      "episode: 235   score: 3.0  epsilon: 1.0    steps: 528  evaluation reward: 1.74\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 181: Policy loss: 0.001577. Value loss: 0.203265. Entropy: 1.335842.\n",
      "Iteration 182: Policy loss: -0.006554. Value loss: 0.135536. Entropy: 1.333580.\n",
      "Iteration 183: Policy loss: -0.016042. Value loss: 0.097138. Entropy: 1.332701.\n",
      "episode: 236   score: 3.0  epsilon: 1.0    steps: 224  evaluation reward: 1.75\n",
      "episode: 237   score: 3.0  epsilon: 1.0    steps: 240  evaluation reward: 1.78\n",
      "episode: 238   score: 3.0  epsilon: 1.0    steps: 704  evaluation reward: 1.81\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 184: Policy loss: 0.002761. Value loss: 0.289970. Entropy: 1.315978.\n",
      "Iteration 185: Policy loss: -0.009721. Value loss: 0.141971. Entropy: 1.313411.\n",
      "Iteration 186: Policy loss: -0.020684. Value loss: 0.096227. Entropy: 1.318435.\n",
      "episode: 239   score: 3.0  epsilon: 1.0    steps: 560  evaluation reward: 1.82\n",
      "episode: 240   score: 2.0  epsilon: 1.0    steps: 608  evaluation reward: 1.82\n",
      "episode: 241   score: 5.0  epsilon: 1.0    steps: 624  evaluation reward: 1.86\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 187: Policy loss: -0.000545. Value loss: 0.284502. Entropy: 1.308961.\n",
      "Iteration 188: Policy loss: -0.009484. Value loss: 0.168792. Entropy: 1.318465.\n",
      "Iteration 189: Policy loss: -0.020154. Value loss: 0.117599. Entropy: 1.306860.\n",
      "episode: 242   score: 4.0  epsilon: 1.0    steps: 480  evaluation reward: 1.89\n",
      "episode: 243   score: 5.0  epsilon: 1.0    steps: 696  evaluation reward: 1.92\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 190: Policy loss: 0.002326. Value loss: 0.249762. Entropy: 1.306082.\n",
      "Iteration 191: Policy loss: -0.007935. Value loss: 0.120854. Entropy: 1.307166.\n",
      "Iteration 192: Policy loss: -0.014790. Value loss: 0.084067. Entropy: 1.303883.\n",
      "episode: 244   score: 4.0  epsilon: 1.0    steps: 152  evaluation reward: 1.94\n",
      "episode: 245   score: 4.0  epsilon: 1.0    steps: 552  evaluation reward: 1.96\n",
      "episode: 246   score: 2.0  epsilon: 1.0    steps: 672  evaluation reward: 1.98\n",
      "episode: 247   score: 2.0  epsilon: 1.0    steps: 712  evaluation reward: 2.0\n",
      "episode: 248   score: 2.0  epsilon: 1.0    steps: 832  evaluation reward: 2.02\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 193: Policy loss: 0.005459. Value loss: 0.307113. Entropy: 1.301112.\n",
      "Iteration 194: Policy loss: -0.005618. Value loss: 0.173083. Entropy: 1.302319.\n",
      "Iteration 195: Policy loss: -0.012747. Value loss: 0.128088. Entropy: 1.296374.\n",
      "episode: 249   score: 5.0  epsilon: 1.0    steps: 456  evaluation reward: 2.07\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 196: Policy loss: 0.006008. Value loss: 0.230502. Entropy: 1.331468.\n",
      "Iteration 197: Policy loss: -0.005836. Value loss: 0.129196. Entropy: 1.324075.\n",
      "Iteration 198: Policy loss: -0.011933. Value loss: 0.098337. Entropy: 1.322850.\n",
      "episode: 250   score: 3.0  epsilon: 1.0    steps: 80  evaluation reward: 2.09\n",
      "now time :  2019-03-06 12:31:53.613261\n",
      "episode: 251   score: 0.0  epsilon: 1.0    steps: 120  evaluation reward: 2.06\n",
      "episode: 252   score: 3.0  epsilon: 1.0    steps: 360  evaluation reward: 2.09\n",
      "episode: 253   score: 1.0  epsilon: 1.0    steps: 560  evaluation reward: 2.09\n",
      "episode: 254   score: 2.0  epsilon: 1.0    steps: 648  evaluation reward: 2.11\n",
      "episode: 255   score: 0.0  epsilon: 1.0    steps: 800  evaluation reward: 2.11\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 199: Policy loss: 0.000300. Value loss: 0.222814. Entropy: 1.316335.\n",
      "Iteration 200: Policy loss: -0.006015. Value loss: 0.139811. Entropy: 1.317201.\n",
      "Iteration 201: Policy loss: -0.015905. Value loss: 0.094596. Entropy: 1.308706.\n",
      "episode: 256   score: 3.0  epsilon: 1.0    steps: 280  evaluation reward: 2.12\n",
      "episode: 257   score: 5.0  epsilon: 1.0    steps: 552  evaluation reward: 2.15\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 202: Policy loss: -0.000525. Value loss: 0.258458. Entropy: 1.278050.\n",
      "Iteration 203: Policy loss: -0.006774. Value loss: 0.143449. Entropy: 1.273181.\n",
      "Iteration 204: Policy loss: -0.016850. Value loss: 0.110958. Entropy: 1.270327.\n",
      "episode: 258   score: 1.0  epsilon: 1.0    steps: 288  evaluation reward: 2.14\n",
      "episode: 259   score: 2.0  epsilon: 1.0    steps: 536  evaluation reward: 2.14\n",
      "episode: 260   score: 3.0  epsilon: 1.0    steps: 960  evaluation reward: 2.15\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 205: Policy loss: 0.010160. Value loss: 0.247146. Entropy: 1.311682.\n",
      "Iteration 206: Policy loss: -0.001733. Value loss: 0.129041. Entropy: 1.296939.\n",
      "Iteration 207: Policy loss: -0.012687. Value loss: 0.088139. Entropy: 1.292666.\n",
      "episode: 261   score: 4.0  epsilon: 1.0    steps: 360  evaluation reward: 2.18\n",
      "episode: 262   score: 3.0  epsilon: 1.0    steps: 440  evaluation reward: 2.19\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 208: Policy loss: 0.003912. Value loss: 0.265553. Entropy: 1.277777.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 209: Policy loss: -0.005182. Value loss: 0.149935. Entropy: 1.272631.\n",
      "Iteration 210: Policy loss: -0.009526. Value loss: 0.103117. Entropy: 1.270735.\n",
      "episode: 263   score: 3.0  epsilon: 1.0    steps: 112  evaluation reward: 2.19\n",
      "episode: 264   score: 5.0  epsilon: 1.0    steps: 152  evaluation reward: 2.24\n",
      "episode: 265   score: 2.0  epsilon: 1.0    steps: 792  evaluation reward: 2.25\n",
      "episode: 266   score: 3.0  epsilon: 1.0    steps: 816  evaluation reward: 2.26\n",
      "episode: 267   score: 5.0  epsilon: 1.0    steps: 856  evaluation reward: 2.31\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 211: Policy loss: 0.001765. Value loss: 0.230060. Entropy: 1.280344.\n",
      "Iteration 212: Policy loss: -0.012747. Value loss: 0.123466. Entropy: 1.274100.\n",
      "Iteration 213: Policy loss: -0.017580. Value loss: 0.093540. Entropy: 1.274479.\n",
      "episode: 268   score: 2.0  epsilon: 1.0    steps: 560  evaluation reward: 2.32\n",
      "episode: 269   score: 3.0  epsilon: 1.0    steps: 856  evaluation reward: 2.35\n",
      "episode: 270   score: 4.0  epsilon: 1.0    steps: 888  evaluation reward: 2.38\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 214: Policy loss: 0.004596. Value loss: 0.325344. Entropy: 1.267821.\n",
      "Iteration 215: Policy loss: -0.008355. Value loss: 0.203006. Entropy: 1.262097.\n",
      "Iteration 216: Policy loss: -0.015398. Value loss: 0.145314. Entropy: 1.254679.\n",
      "episode: 271   score: 1.0  epsilon: 1.0    steps: 600  evaluation reward: 2.37\n",
      "episode: 272   score: 3.0  epsilon: 1.0    steps: 840  evaluation reward: 2.37\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 217: Policy loss: 0.001495. Value loss: 0.211882. Entropy: 1.271023.\n",
      "Iteration 218: Policy loss: -0.006589. Value loss: 0.117460. Entropy: 1.268295.\n",
      "Iteration 219: Policy loss: -0.014449. Value loss: 0.076229. Entropy: 1.267390.\n",
      "episode: 273   score: 3.0  epsilon: 1.0    steps: 472  evaluation reward: 2.4\n",
      "episode: 274   score: 1.0  epsilon: 1.0    steps: 640  evaluation reward: 2.39\n",
      "episode: 275   score: 6.0  epsilon: 1.0    steps: 680  evaluation reward: 2.45\n",
      "episode: 276   score: 0.0  epsilon: 1.0    steps: 968  evaluation reward: 2.45\n",
      "episode: 277   score: 2.0  epsilon: 1.0    steps: 1008  evaluation reward: 2.45\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 220: Policy loss: 0.003637. Value loss: 0.277341. Entropy: 1.276085.\n",
      "Iteration 221: Policy loss: -0.008724. Value loss: 0.163038. Entropy: 1.285038.\n",
      "Iteration 222: Policy loss: -0.014276. Value loss: 0.126676. Entropy: 1.262384.\n",
      "episode: 278   score: 5.0  epsilon: 1.0    steps: 80  evaluation reward: 2.47\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 223: Policy loss: -0.000156. Value loss: 0.273994. Entropy: 1.253728.\n",
      "Iteration 224: Policy loss: -0.009960. Value loss: 0.156704. Entropy: 1.256351.\n",
      "Iteration 225: Policy loss: -0.016045. Value loss: 0.107442. Entropy: 1.253597.\n",
      "episode: 279   score: 7.0  epsilon: 1.0    steps: 440  evaluation reward: 2.52\n",
      "episode: 280   score: 3.0  epsilon: 1.0    steps: 472  evaluation reward: 2.55\n",
      "episode: 281   score: 2.0  epsilon: 1.0    steps: 864  evaluation reward: 2.54\n",
      "episode: 282   score: 2.0  epsilon: 1.0    steps: 1016  evaluation reward: 2.53\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 226: Policy loss: 0.003802. Value loss: 0.246262. Entropy: 1.262676.\n",
      "Iteration 227: Policy loss: -0.006858. Value loss: 0.123152. Entropy: 1.276390.\n",
      "Iteration 228: Policy loss: -0.014424. Value loss: 0.088140. Entropy: 1.267252.\n",
      "episode: 283   score: 3.0  epsilon: 1.0    steps: 280  evaluation reward: 2.54\n",
      "episode: 284   score: 5.0  epsilon: 1.0    steps: 568  evaluation reward: 2.57\n",
      "episode: 285   score: 3.0  epsilon: 1.0    steps: 784  evaluation reward: 2.57\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 229: Policy loss: 0.004387. Value loss: 0.328216. Entropy: 1.237218.\n",
      "Iteration 230: Policy loss: -0.009660. Value loss: 0.201560. Entropy: 1.237465.\n",
      "Iteration 231: Policy loss: -0.014442. Value loss: 0.152179. Entropy: 1.234481.\n",
      "episode: 286   score: 5.0  epsilon: 1.0    steps: 232  evaluation reward: 2.61\n",
      "episode: 287   score: 1.0  epsilon: 1.0    steps: 976  evaluation reward: 2.59\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 232: Policy loss: 0.006617. Value loss: 0.341114. Entropy: 1.266220.\n",
      "Iteration 233: Policy loss: -0.002559. Value loss: 0.200888. Entropy: 1.265006.\n",
      "Iteration 234: Policy loss: -0.008546. Value loss: 0.133387. Entropy: 1.267457.\n",
      "episode: 288   score: 3.0  epsilon: 1.0    steps: 280  evaluation reward: 2.62\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 235: Policy loss: 0.002195. Value loss: 0.329266. Entropy: 1.285956.\n",
      "Iteration 236: Policy loss: -0.007370. Value loss: 0.184392. Entropy: 1.279819.\n",
      "Iteration 237: Policy loss: -0.020343. Value loss: 0.121248. Entropy: 1.278150.\n",
      "episode: 289   score: 6.0  epsilon: 1.0    steps: 448  evaluation reward: 2.66\n",
      "episode: 290   score: 4.0  epsilon: 1.0    steps: 664  evaluation reward: 2.65\n",
      "episode: 291   score: 6.0  epsilon: 1.0    steps: 720  evaluation reward: 2.71\n",
      "episode: 292   score: 3.0  epsilon: 1.0    steps: 728  evaluation reward: 2.74\n",
      "episode: 293   score: 4.0  epsilon: 1.0    steps: 792  evaluation reward: 2.76\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 238: Policy loss: 0.007324. Value loss: 0.352500. Entropy: 1.282569.\n",
      "Iteration 239: Policy loss: -0.008184. Value loss: 0.219101. Entropy: 1.287014.\n",
      "Iteration 240: Policy loss: -0.013587. Value loss: 0.150407. Entropy: 1.277911.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 241: Policy loss: 0.002255. Value loss: 0.283735. Entropy: 1.232549.\n",
      "Iteration 242: Policy loss: -0.011381. Value loss: 0.147324. Entropy: 1.240036.\n",
      "Iteration 243: Policy loss: -0.018619. Value loss: 0.108377. Entropy: 1.234486.\n",
      "episode: 294   score: 4.0  epsilon: 1.0    steps: 120  evaluation reward: 2.8\n",
      "episode: 295   score: 4.0  epsilon: 1.0    steps: 240  evaluation reward: 2.83\n",
      "episode: 296   score: 1.0  epsilon: 1.0    steps: 624  evaluation reward: 2.84\n",
      "episode: 297   score: 14.0  epsilon: 1.0    steps: 752  evaluation reward: 2.98\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 244: Policy loss: 0.002823. Value loss: 0.518702. Entropy: 1.263867.\n",
      "Iteration 245: Policy loss: -0.005968. Value loss: 0.292191. Entropy: 1.260551.\n",
      "Iteration 246: Policy loss: -0.011081. Value loss: 0.212278. Entropy: 1.247434.\n",
      "episode: 298   score: 4.0  epsilon: 1.0    steps: 456  evaluation reward: 2.99\n",
      "episode: 299   score: 5.0  epsilon: 1.0    steps: 1016  evaluation reward: 3.04\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 247: Policy loss: 0.000823. Value loss: 0.326399. Entropy: 1.246475.\n",
      "Iteration 248: Policy loss: -0.009800. Value loss: 0.172082. Entropy: 1.245159.\n",
      "Iteration 249: Policy loss: -0.020005. Value loss: 0.110847. Entropy: 1.238636.\n",
      "episode: 300   score: 6.0  epsilon: 1.0    steps: 816  evaluation reward: 3.1\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 250: Policy loss: 0.000961. Value loss: 0.295370. Entropy: 1.227383.\n",
      "Iteration 251: Policy loss: -0.009701. Value loss: 0.169189. Entropy: 1.234890.\n",
      "Iteration 252: Policy loss: -0.019249. Value loss: 0.122135. Entropy: 1.237759.\n",
      "now time :  2019-03-06 12:33:03.159280\n",
      "episode: 301   score: 4.0  epsilon: 1.0    steps: 128  evaluation reward: 3.11\n",
      "episode: 302   score: 7.0  epsilon: 1.0    steps: 160  evaluation reward: 3.17\n",
      "episode: 303   score: 5.0  epsilon: 1.0    steps: 432  evaluation reward: 3.2\n",
      "episode: 304   score: 4.0  epsilon: 1.0    steps: 632  evaluation reward: 3.22\n",
      "episode: 305   score: 4.0  epsilon: 1.0    steps: 1000  evaluation reward: 3.25\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 253: Policy loss: 0.002198. Value loss: 0.430300. Entropy: 1.238826.\n",
      "Iteration 254: Policy loss: -0.008636. Value loss: 0.261096. Entropy: 1.241875.\n",
      "Iteration 255: Policy loss: -0.017274. Value loss: 0.185485. Entropy: 1.232816.\n",
      "episode: 306   score: 1.0  epsilon: 1.0    steps: 744  evaluation reward: 3.24\n",
      "episode: 307   score: 1.0  epsilon: 1.0    steps: 784  evaluation reward: 3.23\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 256: Policy loss: 0.002527. Value loss: 0.365360. Entropy: 1.238847.\n",
      "Iteration 257: Policy loss: -0.007734. Value loss: 0.225672. Entropy: 1.231196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 258: Policy loss: -0.015665. Value loss: 0.169533. Entropy: 1.231574.\n",
      "episode: 308   score: 5.0  epsilon: 1.0    steps: 728  evaluation reward: 3.26\n",
      "episode: 309   score: 9.0  epsilon: 1.0    steps: 904  evaluation reward: 3.35\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 259: Policy loss: 0.003702. Value loss: 0.370378. Entropy: 1.238711.\n",
      "Iteration 260: Policy loss: -0.003704. Value loss: 0.221941. Entropy: 1.244420.\n",
      "Iteration 261: Policy loss: -0.017160. Value loss: 0.159266. Entropy: 1.242695.\n",
      "episode: 310   score: 6.0  epsilon: 1.0    steps: 920  evaluation reward: 3.39\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 262: Policy loss: 0.002642. Value loss: 0.270457. Entropy: 1.235396.\n",
      "Iteration 263: Policy loss: -0.008673. Value loss: 0.135339. Entropy: 1.233327.\n",
      "Iteration 264: Policy loss: -0.021226. Value loss: 0.097609. Entropy: 1.232366.\n",
      "episode: 311   score: 5.0  epsilon: 1.0    steps: 320  evaluation reward: 3.4\n",
      "episode: 312   score: 4.0  epsilon: 1.0    steps: 784  evaluation reward: 3.42\n",
      "episode: 313   score: 4.0  epsilon: 1.0    steps: 928  evaluation reward: 3.44\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 265: Policy loss: 0.004845. Value loss: 0.248833. Entropy: 1.234333.\n",
      "Iteration 266: Policy loss: -0.006996. Value loss: 0.130135. Entropy: 1.229727.\n",
      "Iteration 267: Policy loss: -0.017906. Value loss: 0.093868. Entropy: 1.230960.\n",
      "episode: 314   score: 7.0  epsilon: 1.0    steps: 64  evaluation reward: 3.49\n",
      "episode: 315   score: 9.0  epsilon: 1.0    steps: 72  evaluation reward: 3.54\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 268: Policy loss: 0.002954. Value loss: 0.336058. Entropy: 1.222162.\n",
      "Iteration 269: Policy loss: -0.011937. Value loss: 0.194635. Entropy: 1.225431.\n",
      "Iteration 270: Policy loss: -0.018198. Value loss: 0.139466. Entropy: 1.223818.\n",
      "episode: 316   score: 1.0  epsilon: 1.0    steps: 752  evaluation reward: 3.52\n",
      "episode: 317   score: 7.0  epsilon: 1.0    steps: 928  evaluation reward: 3.57\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 271: Policy loss: 0.004537. Value loss: 0.415822. Entropy: 1.269980.\n",
      "Iteration 272: Policy loss: -0.004060. Value loss: 0.207833. Entropy: 1.271219.\n",
      "Iteration 273: Policy loss: -0.012707. Value loss: 0.141780. Entropy: 1.275772.\n",
      "episode: 318   score: 7.0  epsilon: 1.0    steps: 704  evaluation reward: 3.63\n",
      "episode: 319   score: 4.0  epsilon: 1.0    steps: 760  evaluation reward: 3.64\n",
      "episode: 320   score: 4.0  epsilon: 1.0    steps: 928  evaluation reward: 3.63\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 274: Policy loss: -0.000518. Value loss: 0.430941. Entropy: 1.264058.\n",
      "Iteration 275: Policy loss: -0.009434. Value loss: 0.239682. Entropy: 1.261382.\n",
      "Iteration 276: Policy loss: -0.019386. Value loss: 0.164257. Entropy: 1.257015.\n",
      "episode: 321   score: 7.0  epsilon: 1.0    steps: 200  evaluation reward: 3.67\n",
      "episode: 322   score: 10.0  epsilon: 1.0    steps: 384  evaluation reward: 3.74\n",
      "episode: 323   score: 6.0  epsilon: 1.0    steps: 752  evaluation reward: 3.78\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 277: Policy loss: 0.004492. Value loss: 0.374216. Entropy: 1.223545.\n",
      "Iteration 278: Policy loss: -0.006123. Value loss: 0.189680. Entropy: 1.230553.\n",
      "Iteration 279: Policy loss: -0.020711. Value loss: 0.121594. Entropy: 1.220044.\n",
      "episode: 324   score: 4.0  epsilon: 1.0    steps: 544  evaluation reward: 3.8\n",
      "episode: 325   score: 3.0  epsilon: 1.0    steps: 1000  evaluation reward: 3.81\n",
      "episode: 326   score: 5.0  epsilon: 1.0    steps: 1024  evaluation reward: 3.82\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 280: Policy loss: 0.002249. Value loss: 0.342366. Entropy: 1.215560.\n",
      "Iteration 281: Policy loss: -0.005034. Value loss: 0.179900. Entropy: 1.235771.\n",
      "Iteration 282: Policy loss: -0.017761. Value loss: 0.111594. Entropy: 1.235541.\n",
      "episode: 327   score: 1.0  epsilon: 1.0    steps: 368  evaluation reward: 3.8\n",
      "episode: 328   score: 3.0  epsilon: 1.0    steps: 488  evaluation reward: 3.79\n",
      "episode: 329   score: 4.0  epsilon: 1.0    steps: 744  evaluation reward: 3.8\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 283: Policy loss: 0.007636. Value loss: 0.248540. Entropy: 1.225248.\n",
      "Iteration 284: Policy loss: -0.007348. Value loss: 0.133605. Entropy: 1.217528.\n",
      "Iteration 285: Policy loss: -0.015583. Value loss: 0.105742. Entropy: 1.222676.\n",
      "episode: 330   score: 5.0  epsilon: 1.0    steps: 400  evaluation reward: 3.84\n",
      "episode: 331   score: 5.0  epsilon: 1.0    steps: 944  evaluation reward: 3.85\n",
      "episode: 332   score: 2.0  epsilon: 1.0    steps: 992  evaluation reward: 3.85\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 286: Policy loss: 0.001029. Value loss: 0.297989. Entropy: 1.242119.\n",
      "Iteration 287: Policy loss: -0.012606. Value loss: 0.155090. Entropy: 1.238899.\n",
      "Iteration 288: Policy loss: -0.017556. Value loss: 0.103577. Entropy: 1.233423.\n",
      "episode: 333   score: 1.0  epsilon: 1.0    steps: 552  evaluation reward: 3.82\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 289: Policy loss: 0.002043. Value loss: 0.246225. Entropy: 1.238869.\n",
      "Iteration 290: Policy loss: -0.012850. Value loss: 0.136268. Entropy: 1.240174.\n",
      "Iteration 291: Policy loss: -0.020813. Value loss: 0.089299. Entropy: 1.227715.\n",
      "episode: 334   score: 3.0  epsilon: 1.0    steps: 24  evaluation reward: 3.8\n",
      "episode: 335   score: 5.0  epsilon: 1.0    steps: 136  evaluation reward: 3.82\n",
      "episode: 336   score: 4.0  epsilon: 1.0    steps: 408  evaluation reward: 3.83\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 292: Policy loss: 0.001628. Value loss: 0.279806. Entropy: 1.230668.\n",
      "Iteration 293: Policy loss: -0.006808. Value loss: 0.134346. Entropy: 1.236964.\n",
      "Iteration 294: Policy loss: -0.019437. Value loss: 0.092769. Entropy: 1.233418.\n",
      "episode: 337   score: 3.0  epsilon: 1.0    steps: 424  evaluation reward: 3.83\n",
      "episode: 338   score: 3.0  epsilon: 1.0    steps: 544  evaluation reward: 3.83\n",
      "episode: 339   score: 9.0  epsilon: 1.0    steps: 824  evaluation reward: 3.89\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 295: Policy loss: 0.001340. Value loss: 0.327920. Entropy: 1.206660.\n",
      "Iteration 296: Policy loss: -0.009520. Value loss: 0.177465. Entropy: 1.209931.\n",
      "Iteration 297: Policy loss: -0.017232. Value loss: 0.125867. Entropy: 1.207701.\n",
      "episode: 340   score: 7.0  epsilon: 1.0    steps: 360  evaluation reward: 3.94\n",
      "episode: 341   score: 4.0  epsilon: 1.0    steps: 432  evaluation reward: 3.93\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 298: Policy loss: 0.007980. Value loss: 0.281153. Entropy: 1.216907.\n",
      "Iteration 299: Policy loss: -0.006715. Value loss: 0.134109. Entropy: 1.217005.\n",
      "Iteration 300: Policy loss: -0.012886. Value loss: 0.114244. Entropy: 1.201927.\n",
      "episode: 342   score: 4.0  epsilon: 1.0    steps: 72  evaluation reward: 3.93\n",
      "episode: 343   score: 5.0  epsilon: 1.0    steps: 552  evaluation reward: 3.93\n",
      "episode: 344   score: 0.0  epsilon: 1.0    steps: 752  evaluation reward: 3.89\n",
      "episode: 345   score: 5.0  epsilon: 1.0    steps: 880  evaluation reward: 3.9\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 301: Policy loss: -0.000417. Value loss: 0.285967. Entropy: 1.203987.\n",
      "Iteration 302: Policy loss: -0.011522. Value loss: 0.144854. Entropy: 1.204767.\n",
      "Iteration 303: Policy loss: -0.019330. Value loss: 0.089052. Entropy: 1.199485.\n",
      "episode: 346   score: 6.0  epsilon: 1.0    steps: 872  evaluation reward: 3.94\n",
      "episode: 347   score: 2.0  epsilon: 1.0    steps: 968  evaluation reward: 3.94\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 304: Policy loss: 0.001313. Value loss: 0.265890. Entropy: 1.247589.\n",
      "Iteration 305: Policy loss: -0.013047. Value loss: 0.139571. Entropy: 1.239237.\n",
      "Iteration 306: Policy loss: -0.020282. Value loss: 0.097111. Entropy: 1.232862.\n",
      "episode: 348   score: 5.0  epsilon: 1.0    steps: 448  evaluation reward: 3.97\n",
      "episode: 349   score: 6.0  epsilon: 1.0    steps: 480  evaluation reward: 3.98\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 307: Policy loss: -0.001251. Value loss: 0.254237. Entropy: 1.216298.\n",
      "Iteration 308: Policy loss: -0.015565. Value loss: 0.130953. Entropy: 1.220018.\n",
      "Iteration 309: Policy loss: -0.016443. Value loss: 0.086817. Entropy: 1.206442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 350   score: 4.0  epsilon: 1.0    steps: 392  evaluation reward: 3.99\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 310: Policy loss: 0.004105. Value loss: 0.328511. Entropy: 1.187344.\n",
      "Iteration 311: Policy loss: -0.011275. Value loss: 0.135428. Entropy: 1.188210.\n",
      "Iteration 312: Policy loss: -0.019156. Value loss: 0.087905. Entropy: 1.190449.\n",
      "now time :  2019-03-06 12:34:21.633487\n",
      "episode: 351   score: 5.0  epsilon: 1.0    steps: 280  evaluation reward: 4.04\n",
      "episode: 352   score: 8.0  epsilon: 1.0    steps: 328  evaluation reward: 4.09\n",
      "episode: 353   score: 3.0  epsilon: 1.0    steps: 360  evaluation reward: 4.11\n",
      "episode: 354   score: 3.0  epsilon: 1.0    steps: 800  evaluation reward: 4.12\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 313: Policy loss: 0.004304. Value loss: 0.237943. Entropy: 1.215490.\n",
      "Iteration 314: Policy loss: -0.009863. Value loss: 0.104690. Entropy: 1.208585.\n",
      "Iteration 315: Policy loss: -0.022156. Value loss: 0.074907. Entropy: 1.202690.\n",
      "episode: 355   score: 7.0  epsilon: 1.0    steps: 240  evaluation reward: 4.19\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 316: Policy loss: 0.001859. Value loss: 0.412041. Entropy: 1.201271.\n",
      "Iteration 317: Policy loss: -0.011228. Value loss: 0.229751. Entropy: 1.213624.\n",
      "Iteration 318: Policy loss: -0.019518. Value loss: 0.146798. Entropy: 1.202411.\n",
      "episode: 356   score: 8.0  epsilon: 1.0    steps: 8  evaluation reward: 4.24\n",
      "episode: 357   score: 7.0  epsilon: 1.0    steps: 224  evaluation reward: 4.26\n",
      "episode: 358   score: 3.0  epsilon: 1.0    steps: 536  evaluation reward: 4.28\n",
      "episode: 359   score: 3.0  epsilon: 1.0    steps: 544  evaluation reward: 4.29\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 319: Policy loss: 0.010015. Value loss: 0.263636. Entropy: 1.236706.\n",
      "Iteration 320: Policy loss: -0.007747. Value loss: 0.137095. Entropy: 1.236579.\n",
      "Iteration 321: Policy loss: -0.018098. Value loss: 0.101796. Entropy: 1.236459.\n",
      "episode: 360   score: 3.0  epsilon: 1.0    steps: 600  evaluation reward: 4.29\n",
      "episode: 361   score: 5.0  epsilon: 1.0    steps: 888  evaluation reward: 4.3\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 322: Policy loss: 0.005519. Value loss: 0.285412. Entropy: 1.189544.\n",
      "Iteration 323: Policy loss: -0.013469. Value loss: 0.124793. Entropy: 1.184318.\n",
      "Iteration 324: Policy loss: -0.021865. Value loss: 0.082543. Entropy: 1.181787.\n",
      "episode: 362   score: 2.0  epsilon: 1.0    steps: 496  evaluation reward: 4.29\n",
      "episode: 363   score: 10.0  epsilon: 1.0    steps: 680  evaluation reward: 4.36\n",
      "episode: 364   score: 4.0  epsilon: 1.0    steps: 856  evaluation reward: 4.35\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 325: Policy loss: 0.004525. Value loss: 0.228146. Entropy: 1.215239.\n",
      "Iteration 326: Policy loss: -0.008162. Value loss: 0.118687. Entropy: 1.210327.\n",
      "Iteration 327: Policy loss: -0.020410. Value loss: 0.085426. Entropy: 1.212348.\n",
      "episode: 365   score: 4.0  epsilon: 1.0    steps: 40  evaluation reward: 4.37\n",
      "episode: 366   score: 7.0  epsilon: 1.0    steps: 56  evaluation reward: 4.41\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 328: Policy loss: 0.005410. Value loss: 0.342331. Entropy: 1.178449.\n",
      "Iteration 329: Policy loss: -0.012571. Value loss: 0.167417. Entropy: 1.177132.\n",
      "Iteration 330: Policy loss: -0.022688. Value loss: 0.106760. Entropy: 1.182561.\n",
      "episode: 367   score: 8.0  epsilon: 1.0    steps: 792  evaluation reward: 4.44\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 331: Policy loss: 0.003351. Value loss: 0.301057. Entropy: 1.197536.\n",
      "Iteration 332: Policy loss: -0.013176. Value loss: 0.164776. Entropy: 1.192797.\n",
      "Iteration 333: Policy loss: -0.022816. Value loss: 0.113970. Entropy: 1.186451.\n",
      "episode: 368   score: 3.0  epsilon: 1.0    steps: 712  evaluation reward: 4.45\n",
      "episode: 369   score: 7.0  epsilon: 1.0    steps: 792  evaluation reward: 4.49\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 334: Policy loss: -0.000254. Value loss: 0.307857. Entropy: 1.214635.\n",
      "Iteration 335: Policy loss: -0.013207. Value loss: 0.120404. Entropy: 1.223971.\n",
      "Iteration 336: Policy loss: -0.024487. Value loss: 0.081021. Entropy: 1.221691.\n",
      "episode: 370   score: 4.0  epsilon: 1.0    steps: 120  evaluation reward: 4.49\n",
      "episode: 371   score: 9.0  epsilon: 1.0    steps: 280  evaluation reward: 4.57\n",
      "episode: 372   score: 6.0  epsilon: 1.0    steps: 456  evaluation reward: 4.6\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 337: Policy loss: -0.000878. Value loss: 0.341599. Entropy: 1.220499.\n",
      "Iteration 338: Policy loss: -0.014950. Value loss: 0.186112. Entropy: 1.224870.\n",
      "Iteration 339: Policy loss: -0.021277. Value loss: 0.137899. Entropy: 1.218041.\n",
      "episode: 373   score: 8.0  epsilon: 1.0    steps: 184  evaluation reward: 4.65\n",
      "episode: 374   score: 8.0  epsilon: 1.0    steps: 560  evaluation reward: 4.72\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 340: Policy loss: 0.005988. Value loss: 0.306695. Entropy: 1.183590.\n",
      "Iteration 341: Policy loss: -0.014010. Value loss: 0.150032. Entropy: 1.190204.\n",
      "Iteration 342: Policy loss: -0.023877. Value loss: 0.111619. Entropy: 1.189393.\n",
      "episode: 375   score: 3.0  epsilon: 1.0    steps: 56  evaluation reward: 4.69\n",
      "episode: 376   score: 8.0  epsilon: 1.0    steps: 984  evaluation reward: 4.77\n",
      "episode: 377   score: 4.0  epsilon: 1.0    steps: 1024  evaluation reward: 4.79\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 343: Policy loss: 0.001872. Value loss: 0.339845. Entropy: 1.189182.\n",
      "Iteration 344: Policy loss: -0.016638. Value loss: 0.173087. Entropy: 1.193548.\n",
      "Iteration 345: Policy loss: -0.022346. Value loss: 0.113028. Entropy: 1.186597.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 346: Policy loss: 0.002320. Value loss: 0.354192. Entropy: 1.166822.\n",
      "Iteration 347: Policy loss: -0.013483. Value loss: 0.156500. Entropy: 1.171897.\n",
      "Iteration 348: Policy loss: -0.020071. Value loss: 0.086402. Entropy: 1.171099.\n",
      "episode: 378   score: 8.0  epsilon: 1.0    steps: 16  evaluation reward: 4.82\n",
      "episode: 379   score: 6.0  epsilon: 1.0    steps: 48  evaluation reward: 4.81\n",
      "episode: 380   score: 5.0  epsilon: 1.0    steps: 648  evaluation reward: 4.83\n",
      "episode: 381   score: 2.0  epsilon: 1.0    steps: 848  evaluation reward: 4.83\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 349: Policy loss: 0.002087. Value loss: 0.296977. Entropy: 1.173820.\n",
      "Iteration 350: Policy loss: -0.008596. Value loss: 0.168281. Entropy: 1.178643.\n",
      "Iteration 351: Policy loss: -0.016508. Value loss: 0.115557. Entropy: 1.159704.\n",
      "episode: 382   score: 9.0  epsilon: 1.0    steps: 256  evaluation reward: 4.9\n",
      "episode: 383   score: 6.0  epsilon: 1.0    steps: 288  evaluation reward: 4.93\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 352: Policy loss: 0.006448. Value loss: 0.299039. Entropy: 1.174913.\n",
      "Iteration 353: Policy loss: -0.007275. Value loss: 0.148422. Entropy: 1.162319.\n",
      "Iteration 354: Policy loss: -0.017985. Value loss: 0.094877. Entropy: 1.159493.\n",
      "episode: 384   score: 9.0  epsilon: 1.0    steps: 872  evaluation reward: 4.97\n",
      "episode: 385   score: 6.0  epsilon: 1.0    steps: 1000  evaluation reward: 5.0\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 355: Policy loss: 0.003244. Value loss: 0.218026. Entropy: 1.155451.\n",
      "Iteration 356: Policy loss: -0.010842. Value loss: 0.104384. Entropy: 1.147270.\n",
      "Iteration 357: Policy loss: -0.019675. Value loss: 0.080038. Entropy: 1.146579.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 358: Policy loss: 0.000158. Value loss: 0.267156. Entropy: 1.174337.\n",
      "Iteration 359: Policy loss: -0.011502. Value loss: 0.129768. Entropy: 1.188003.\n",
      "Iteration 360: Policy loss: -0.017941. Value loss: 0.085930. Entropy: 1.178678.\n",
      "episode: 386   score: 8.0  epsilon: 1.0    steps: 312  evaluation reward: 5.03\n",
      "episode: 387   score: 6.0  epsilon: 1.0    steps: 752  evaluation reward: 5.08\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 361: Policy loss: 0.012596. Value loss: 0.522460. Entropy: 1.212781.\n",
      "Iteration 362: Policy loss: 0.001444. Value loss: 0.313341. Entropy: 1.220867.\n",
      "Iteration 363: Policy loss: -0.008245. Value loss: 0.231181. Entropy: 1.220392.\n",
      "episode: 388   score: 9.0  epsilon: 1.0    steps: 528  evaluation reward: 5.14\n",
      "episode: 389   score: 8.0  epsilon: 1.0    steps: 576  evaluation reward: 5.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 390   score: 17.0  epsilon: 1.0    steps: 1008  evaluation reward: 5.29\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 364: Policy loss: 0.003036. Value loss: 0.589540. Entropy: 1.197549.\n",
      "Iteration 365: Policy loss: -0.007362. Value loss: 0.325766. Entropy: 1.191405.\n",
      "Iteration 366: Policy loss: -0.014861. Value loss: 0.214987. Entropy: 1.186694.\n",
      "episode: 391   score: 10.0  epsilon: 1.0    steps: 352  evaluation reward: 5.33\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 367: Policy loss: 0.014604. Value loss: 0.525025. Entropy: 1.129842.\n",
      "Iteration 368: Policy loss: -0.008341. Value loss: 0.279076. Entropy: 1.130459.\n",
      "Iteration 369: Policy loss: -0.009523. Value loss: 0.176983. Entropy: 1.124755.\n",
      "episode: 392   score: 6.0  epsilon: 1.0    steps: 40  evaluation reward: 5.36\n",
      "episode: 393   score: 10.0  epsilon: 1.0    steps: 624  evaluation reward: 5.42\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 370: Policy loss: 0.002637. Value loss: 0.445558. Entropy: 1.165602.\n",
      "Iteration 371: Policy loss: -0.010445. Value loss: 0.219594. Entropy: 1.171318.\n",
      "Iteration 372: Policy loss: -0.020555. Value loss: 0.144859. Entropy: 1.172899.\n",
      "episode: 394   score: 6.0  epsilon: 1.0    steps: 304  evaluation reward: 5.44\n",
      "episode: 395   score: 7.0  epsilon: 1.0    steps: 592  evaluation reward: 5.47\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 373: Policy loss: 0.007983. Value loss: 0.350661. Entropy: 1.157665.\n",
      "Iteration 374: Policy loss: -0.005950. Value loss: 0.149344. Entropy: 1.156450.\n",
      "Iteration 375: Policy loss: -0.011475. Value loss: 0.101416. Entropy: 1.153702.\n",
      "episode: 396   score: 9.0  epsilon: 1.0    steps: 912  evaluation reward: 5.55\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 376: Policy loss: 0.001530. Value loss: 0.325489. Entropy: 1.209818.\n",
      "Iteration 377: Policy loss: -0.010836. Value loss: 0.151317. Entropy: 1.195151.\n",
      "Iteration 378: Policy loss: -0.020710. Value loss: 0.107011. Entropy: 1.195752.\n",
      "episode: 397   score: 8.0  epsilon: 1.0    steps: 688  evaluation reward: 5.49\n",
      "episode: 398   score: 3.0  epsilon: 1.0    steps: 752  evaluation reward: 5.48\n",
      "episode: 399   score: 10.0  epsilon: 1.0    steps: 848  evaluation reward: 5.53\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 379: Policy loss: 0.011800. Value loss: 0.445971. Entropy: 1.177433.\n",
      "Iteration 380: Policy loss: -0.005140. Value loss: 0.225686. Entropy: 1.180955.\n",
      "Iteration 381: Policy loss: -0.020981. Value loss: 0.147820. Entropy: 1.184569.\n",
      "episode: 400   score: 10.0  epsilon: 1.0    steps: 496  evaluation reward: 5.57\n",
      "now time :  2019-03-06 12:35:54.458448\n",
      "episode: 401   score: 8.0  epsilon: 1.0    steps: 872  evaluation reward: 5.61\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 382: Policy loss: 0.004222. Value loss: 0.399330. Entropy: 1.177571.\n",
      "Iteration 383: Policy loss: -0.001017. Value loss: 0.205697. Entropy: 1.202318.\n",
      "Iteration 384: Policy loss: -0.015878. Value loss: 0.138810. Entropy: 1.189010.\n",
      "episode: 402   score: 9.0  epsilon: 1.0    steps: 296  evaluation reward: 5.63\n",
      "episode: 403   score: 4.0  epsilon: 1.0    steps: 632  evaluation reward: 5.62\n",
      "episode: 404   score: 7.0  epsilon: 1.0    steps: 840  evaluation reward: 5.65\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 385: Policy loss: 0.002974. Value loss: 0.334685. Entropy: 1.182006.\n",
      "Iteration 386: Policy loss: -0.010784. Value loss: 0.174087. Entropy: 1.184833.\n",
      "Iteration 387: Policy loss: -0.018591. Value loss: 0.116723. Entropy: 1.173998.\n",
      "episode: 405   score: 4.0  epsilon: 1.0    steps: 600  evaluation reward: 5.65\n",
      "episode: 406   score: 4.0  epsilon: 1.0    steps: 712  evaluation reward: 5.68\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 388: Policy loss: -0.000984. Value loss: 0.282904. Entropy: 1.142361.\n",
      "Iteration 389: Policy loss: -0.012668. Value loss: 0.149619. Entropy: 1.138518.\n",
      "Iteration 390: Policy loss: -0.020694. Value loss: 0.100201. Entropy: 1.136673.\n",
      "episode: 407   score: 4.0  epsilon: 1.0    steps: 184  evaluation reward: 5.71\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 391: Policy loss: 0.004386. Value loss: 0.328131. Entropy: 1.132767.\n",
      "Iteration 392: Policy loss: -0.010402. Value loss: 0.184251. Entropy: 1.141540.\n",
      "Iteration 393: Policy loss: -0.018428. Value loss: 0.143333. Entropy: 1.141674.\n",
      "episode: 408   score: 9.0  epsilon: 1.0    steps: 944  evaluation reward: 5.75\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 394: Policy loss: 0.002754. Value loss: 0.329392. Entropy: 1.175057.\n",
      "Iteration 395: Policy loss: -0.014169. Value loss: 0.156402. Entropy: 1.186137.\n",
      "Iteration 396: Policy loss: -0.024470. Value loss: 0.111194. Entropy: 1.178469.\n",
      "episode: 409   score: 8.0  epsilon: 1.0    steps: 96  evaluation reward: 5.74\n",
      "episode: 410   score: 6.0  epsilon: 1.0    steps: 152  evaluation reward: 5.74\n",
      "episode: 411   score: 4.0  epsilon: 1.0    steps: 392  evaluation reward: 5.73\n",
      "episode: 412   score: 6.0  epsilon: 1.0    steps: 408  evaluation reward: 5.75\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 397: Policy loss: 0.004681. Value loss: 0.328516. Entropy: 1.114475.\n",
      "Iteration 398: Policy loss: -0.011143. Value loss: 0.197754. Entropy: 1.097121.\n",
      "Iteration 399: Policy loss: -0.015359. Value loss: 0.151519. Entropy: 1.111283.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 400: Policy loss: 0.005502. Value loss: 0.323662. Entropy: 1.158091.\n",
      "Iteration 401: Policy loss: -0.011851. Value loss: 0.174926. Entropy: 1.160462.\n",
      "Iteration 402: Policy loss: -0.021536. Value loss: 0.125274. Entropy: 1.165376.\n",
      "episode: 413   score: 8.0  epsilon: 1.0    steps: 160  evaluation reward: 5.79\n",
      "episode: 414   score: 12.0  epsilon: 1.0    steps: 216  evaluation reward: 5.84\n",
      "episode: 415   score: 8.0  epsilon: 1.0    steps: 688  evaluation reward: 5.83\n",
      "episode: 416   score: 3.0  epsilon: 1.0    steps: 784  evaluation reward: 5.85\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 403: Policy loss: 0.002938. Value loss: 0.271718. Entropy: 1.135885.\n",
      "Iteration 404: Policy loss: -0.009262. Value loss: 0.131442. Entropy: 1.150780.\n",
      "Iteration 405: Policy loss: -0.020359. Value loss: 0.084061. Entropy: 1.138226.\n",
      "episode: 417   score: 4.0  epsilon: 1.0    steps: 328  evaluation reward: 5.82\n",
      "episode: 418   score: 6.0  epsilon: 1.0    steps: 480  evaluation reward: 5.81\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 406: Policy loss: 0.004344. Value loss: 0.343572. Entropy: 1.111438.\n",
      "Iteration 407: Policy loss: -0.011748. Value loss: 0.164814. Entropy: 1.117030.\n",
      "Iteration 408: Policy loss: -0.017668. Value loss: 0.114969. Entropy: 1.112379.\n",
      "episode: 419   score: 8.0  epsilon: 1.0    steps: 264  evaluation reward: 5.85\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 409: Policy loss: 0.003142. Value loss: 0.290020. Entropy: 1.160146.\n",
      "Iteration 410: Policy loss: -0.010378. Value loss: 0.132624. Entropy: 1.162426.\n",
      "Iteration 411: Policy loss: -0.020472. Value loss: 0.082472. Entropy: 1.148804.\n",
      "episode: 420   score: 3.0  epsilon: 1.0    steps: 16  evaluation reward: 5.84\n",
      "episode: 421   score: 4.0  epsilon: 1.0    steps: 312  evaluation reward: 5.81\n",
      "episode: 422   score: 2.0  epsilon: 1.0    steps: 448  evaluation reward: 5.73\n",
      "episode: 423   score: 3.0  epsilon: 1.0    steps: 568  evaluation reward: 5.7\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 412: Policy loss: 0.003372. Value loss: 0.481320. Entropy: 1.080903.\n",
      "Iteration 413: Policy loss: -0.012099. Value loss: 0.291941. Entropy: 1.087251.\n",
      "Iteration 414: Policy loss: -0.017198. Value loss: 0.219794. Entropy: 1.097582.\n",
      "episode: 424   score: 13.0  epsilon: 1.0    steps: 208  evaluation reward: 5.79\n",
      "episode: 425   score: 7.0  epsilon: 1.0    steps: 752  evaluation reward: 5.83\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 415: Policy loss: 0.000903. Value loss: 0.243645. Entropy: 1.082073.\n",
      "Iteration 416: Policy loss: -0.009734. Value loss: 0.124353. Entropy: 1.090378.\n",
      "Iteration 417: Policy loss: -0.021556. Value loss: 0.087938. Entropy: 1.090977.\n",
      "episode: 426   score: 9.0  epsilon: 1.0    steps: 128  evaluation reward: 5.87\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 418: Policy loss: 0.001519. Value loss: 0.251833. Entropy: 1.119485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 419: Policy loss: -0.014698. Value loss: 0.119089. Entropy: 1.125947.\n",
      "Iteration 420: Policy loss: -0.026087. Value loss: 0.078811. Entropy: 1.123764.\n",
      "episode: 427   score: 8.0  epsilon: 1.0    steps: 520  evaluation reward: 5.94\n",
      "episode: 428   score: 5.0  epsilon: 1.0    steps: 632  evaluation reward: 5.96\n",
      "episode: 429   score: 5.0  epsilon: 1.0    steps: 768  evaluation reward: 5.97\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 421: Policy loss: 0.004599. Value loss: 0.257992. Entropy: 1.184000.\n",
      "Iteration 422: Policy loss: -0.015314. Value loss: 0.122958. Entropy: 1.177583.\n",
      "Iteration 423: Policy loss: -0.025110. Value loss: 0.071340. Entropy: 1.176385.\n",
      "episode: 430   score: 5.0  epsilon: 1.0    steps: 272  evaluation reward: 5.97\n",
      "episode: 431   score: 8.0  epsilon: 1.0    steps: 880  evaluation reward: 6.0\n",
      "episode: 432   score: 5.0  epsilon: 1.0    steps: 960  evaluation reward: 6.03\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 424: Policy loss: 0.005306. Value loss: 0.226293. Entropy: 1.148086.\n",
      "Iteration 425: Policy loss: -0.014259. Value loss: 0.134908. Entropy: 1.138822.\n",
      "Iteration 426: Policy loss: -0.022998. Value loss: 0.086344. Entropy: 1.138994.\n",
      "episode: 433   score: 6.0  epsilon: 1.0    steps: 656  evaluation reward: 6.08\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 427: Policy loss: 0.003523. Value loss: 0.474100. Entropy: 1.105492.\n",
      "Iteration 428: Policy loss: -0.008798. Value loss: 0.235879. Entropy: 1.100317.\n",
      "Iteration 429: Policy loss: -0.013500. Value loss: 0.153852. Entropy: 1.103020.\n",
      "episode: 434   score: 3.0  epsilon: 1.0    steps: 104  evaluation reward: 6.08\n",
      "episode: 435   score: 14.0  epsilon: 1.0    steps: 240  evaluation reward: 6.17\n",
      "episode: 436   score: 3.0  epsilon: 1.0    steps: 344  evaluation reward: 6.16\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 430: Policy loss: -0.002004. Value loss: 0.299230. Entropy: 1.141904.\n",
      "Iteration 431: Policy loss: -0.010492. Value loss: 0.165027. Entropy: 1.129084.\n",
      "Iteration 432: Policy loss: -0.022412. Value loss: 0.121524. Entropy: 1.122585.\n",
      "episode: 437   score: 6.0  epsilon: 1.0    steps: 264  evaluation reward: 6.19\n",
      "episode: 438   score: 7.0  epsilon: 1.0    steps: 480  evaluation reward: 6.23\n",
      "episode: 439   score: 5.0  epsilon: 1.0    steps: 584  evaluation reward: 6.19\n",
      "episode: 440   score: 4.0  epsilon: 1.0    steps: 696  evaluation reward: 6.16\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 433: Policy loss: 0.004060. Value loss: 0.457913. Entropy: 1.140830.\n",
      "Iteration 434: Policy loss: 0.000441. Value loss: 0.268535. Entropy: 1.144784.\n",
      "Iteration 435: Policy loss: -0.013266. Value loss: 0.216852. Entropy: 1.136989.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 436: Policy loss: 0.001967. Value loss: 0.208783. Entropy: 1.127235.\n",
      "Iteration 437: Policy loss: -0.015093. Value loss: 0.101206. Entropy: 1.129410.\n",
      "Iteration 438: Policy loss: -0.021639. Value loss: 0.069290. Entropy: 1.126908.\n",
      "episode: 441   score: 7.0  epsilon: 1.0    steps: 600  evaluation reward: 6.19\n",
      "episode: 442   score: 3.0  epsilon: 1.0    steps: 944  evaluation reward: 6.18\n",
      "episode: 443   score: 7.0  epsilon: 1.0    steps: 960  evaluation reward: 6.2\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 439: Policy loss: 0.007756. Value loss: 0.323321. Entropy: 1.076437.\n",
      "Iteration 440: Policy loss: -0.010793. Value loss: 0.154003. Entropy: 1.054029.\n",
      "Iteration 441: Policy loss: -0.017625. Value loss: 0.104796. Entropy: 1.061984.\n",
      "episode: 444   score: 8.0  epsilon: 1.0    steps: 424  evaluation reward: 6.28\n",
      "episode: 445   score: 4.0  epsilon: 1.0    steps: 696  evaluation reward: 6.27\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 442: Policy loss: 0.003024. Value loss: 0.243964. Entropy: 1.086867.\n",
      "Iteration 443: Policy loss: -0.012877. Value loss: 0.090714. Entropy: 1.085551.\n",
      "Iteration 444: Policy loss: -0.024628. Value loss: 0.064860. Entropy: 1.082197.\n",
      "episode: 446   score: 7.0  epsilon: 1.0    steps: 40  evaluation reward: 6.28\n",
      "episode: 447   score: 7.0  epsilon: 1.0    steps: 56  evaluation reward: 6.33\n",
      "episode: 448   score: 9.0  epsilon: 1.0    steps: 224  evaluation reward: 6.37\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 445: Policy loss: 0.003281. Value loss: 0.245358. Entropy: 1.087798.\n",
      "Iteration 446: Policy loss: -0.009965. Value loss: 0.102004. Entropy: 1.070171.\n",
      "Iteration 447: Policy loss: -0.020079. Value loss: 0.065131. Entropy: 1.084389.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 448: Policy loss: 0.004820. Value loss: 0.256629. Entropy: 1.046576.\n",
      "Iteration 449: Policy loss: -0.014185. Value loss: 0.142996. Entropy: 1.046897.\n",
      "Iteration 450: Policy loss: -0.015427. Value loss: 0.102182. Entropy: 1.036110.\n",
      "episode: 449   score: 6.0  epsilon: 1.0    steps: 160  evaluation reward: 6.37\n",
      "episode: 450   score: 5.0  epsilon: 1.0    steps: 168  evaluation reward: 6.38\n",
      "now time :  2019-03-06 12:37:22.577718\n",
      "episode: 451   score: 3.0  epsilon: 1.0    steps: 232  evaluation reward: 6.36\n",
      "episode: 452   score: 6.0  epsilon: 1.0    steps: 904  evaluation reward: 6.34\n",
      "episode: 453   score: 5.0  epsilon: 1.0    steps: 1000  evaluation reward: 6.36\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 451: Policy loss: -0.002486. Value loss: 0.251945. Entropy: 1.062445.\n",
      "Iteration 452: Policy loss: -0.016349. Value loss: 0.133576. Entropy: 1.063551.\n",
      "Iteration 453: Policy loss: -0.022531. Value loss: 0.100139. Entropy: 1.053988.\n",
      "episode: 454   score: 5.0  epsilon: 1.0    steps: 496  evaluation reward: 6.38\n",
      "episode: 455   score: 6.0  epsilon: 1.0    steps: 1008  evaluation reward: 6.37\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 454: Policy loss: 0.002520. Value loss: 0.227223. Entropy: 1.024327.\n",
      "Iteration 455: Policy loss: -0.010640. Value loss: 0.123682. Entropy: 1.033580.\n",
      "Iteration 456: Policy loss: -0.024343. Value loss: 0.090856. Entropy: 1.029642.\n",
      "episode: 456   score: 11.0  epsilon: 1.0    steps: 608  evaluation reward: 6.4\n",
      "episode: 457   score: 4.0  epsilon: 1.0    steps: 824  evaluation reward: 6.37\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 457: Policy loss: 0.001996. Value loss: 0.405675. Entropy: 1.078808.\n",
      "Iteration 458: Policy loss: -0.006846. Value loss: 0.223363. Entropy: 1.085994.\n",
      "Iteration 459: Policy loss: -0.016524. Value loss: 0.160593. Entropy: 1.075081.\n",
      "episode: 458   score: 5.0  epsilon: 1.0    steps: 392  evaluation reward: 6.39\n",
      "episode: 459   score: 5.0  epsilon: 1.0    steps: 544  evaluation reward: 6.41\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 460: Policy loss: 0.002629. Value loss: 0.249648. Entropy: 0.953420.\n",
      "Iteration 461: Policy loss: -0.012730. Value loss: 0.116010. Entropy: 0.942077.\n",
      "Iteration 462: Policy loss: -0.023180. Value loss: 0.073612. Entropy: 0.957887.\n",
      "episode: 460   score: 4.0  epsilon: 1.0    steps: 672  evaluation reward: 6.42\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 463: Policy loss: 0.005927. Value loss: 0.194929. Entropy: 0.964210.\n",
      "Iteration 464: Policy loss: -0.009206. Value loss: 0.106206. Entropy: 0.958262.\n",
      "Iteration 465: Policy loss: -0.016726. Value loss: 0.069238. Entropy: 0.968276.\n",
      "episode: 461   score: 8.0  epsilon: 1.0    steps: 128  evaluation reward: 6.45\n",
      "episode: 462   score: 8.0  epsilon: 1.0    steps: 168  evaluation reward: 6.51\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 466: Policy loss: -0.000705. Value loss: 0.272428. Entropy: 1.022224.\n",
      "Iteration 467: Policy loss: -0.012155. Value loss: 0.123830. Entropy: 1.011427.\n",
      "Iteration 468: Policy loss: -0.023247. Value loss: 0.075369. Entropy: 1.008451.\n",
      "episode: 463   score: 8.0  epsilon: 1.0    steps: 608  evaluation reward: 6.49\n",
      "episode: 464   score: 9.0  epsilon: 1.0    steps: 792  evaluation reward: 6.54\n",
      "episode: 465   score: 7.0  epsilon: 1.0    steps: 840  evaluation reward: 6.57\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 469: Policy loss: 0.002960. Value loss: 0.203012. Entropy: 0.985263.\n",
      "Iteration 470: Policy loss: -0.011702. Value loss: 0.080700. Entropy: 0.983588.\n",
      "Iteration 471: Policy loss: -0.023146. Value loss: 0.058812. Entropy: 0.985642.\n",
      "episode: 466   score: 7.0  epsilon: 1.0    steps: 328  evaluation reward: 6.57\n",
      "episode: 467   score: 4.0  epsilon: 1.0    steps: 336  evaluation reward: 6.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 468   score: 8.0  epsilon: 1.0    steps: 952  evaluation reward: 6.58\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 472: Policy loss: 0.003594. Value loss: 0.220788. Entropy: 1.052667.\n",
      "Iteration 473: Policy loss: -0.013559. Value loss: 0.123360. Entropy: 1.058410.\n",
      "Iteration 474: Policy loss: -0.020842. Value loss: 0.085157. Entropy: 1.058885.\n",
      "episode: 469   score: 5.0  epsilon: 1.0    steps: 488  evaluation reward: 6.56\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 475: Policy loss: 0.005394. Value loss: 0.320724. Entropy: 0.932983.\n",
      "Iteration 476: Policy loss: -0.014013. Value loss: 0.144004. Entropy: 0.951383.\n",
      "Iteration 477: Policy loss: -0.026345. Value loss: 0.101709. Entropy: 0.946220.\n",
      "episode: 470   score: 7.0  epsilon: 1.0    steps: 96  evaluation reward: 6.59\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 478: Policy loss: 0.004355. Value loss: 0.294110. Entropy: 1.043341.\n",
      "Iteration 479: Policy loss: -0.010199. Value loss: 0.161768. Entropy: 1.026707.\n",
      "Iteration 480: Policy loss: -0.019548. Value loss: 0.104370. Entropy: 1.033244.\n",
      "episode: 471   score: 4.0  epsilon: 1.0    steps: 104  evaluation reward: 6.54\n",
      "episode: 472   score: 4.0  epsilon: 1.0    steps: 168  evaluation reward: 6.52\n",
      "episode: 473   score: 8.0  epsilon: 1.0    steps: 728  evaluation reward: 6.52\n",
      "episode: 474   score: 9.0  epsilon: 1.0    steps: 904  evaluation reward: 6.53\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 481: Policy loss: 0.007425. Value loss: 0.300516. Entropy: 0.998406.\n",
      "Iteration 482: Policy loss: -0.012664. Value loss: 0.154985. Entropy: 0.997569.\n",
      "Iteration 483: Policy loss: -0.024634. Value loss: 0.102894. Entropy: 0.990266.\n",
      "episode: 475   score: 5.0  epsilon: 1.0    steps: 360  evaluation reward: 6.55\n",
      "episode: 476   score: 9.0  epsilon: 1.0    steps: 712  evaluation reward: 6.56\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 484: Policy loss: 0.007169. Value loss: 0.247374. Entropy: 0.991968.\n",
      "Iteration 485: Policy loss: -0.005007. Value loss: 0.104987. Entropy: 1.009004.\n",
      "Iteration 486: Policy loss: -0.017582. Value loss: 0.065621. Entropy: 1.008229.\n",
      "episode: 477   score: 6.0  epsilon: 1.0    steps: 96  evaluation reward: 6.58\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 487: Policy loss: 0.001312. Value loss: 0.214484. Entropy: 1.035314.\n",
      "Iteration 488: Policy loss: -0.017035. Value loss: 0.118720. Entropy: 1.034228.\n",
      "Iteration 489: Policy loss: -0.026120. Value loss: 0.083328. Entropy: 1.016650.\n",
      "episode: 478   score: 3.0  epsilon: 1.0    steps: 96  evaluation reward: 6.53\n",
      "episode: 479   score: 7.0  epsilon: 1.0    steps: 128  evaluation reward: 6.54\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 490: Policy loss: 0.007455. Value loss: 0.247988. Entropy: 0.981390.\n",
      "Iteration 491: Policy loss: -0.008894. Value loss: 0.112802. Entropy: 0.997264.\n",
      "Iteration 492: Policy loss: -0.023197. Value loss: 0.074415. Entropy: 0.988420.\n",
      "episode: 480   score: 4.0  epsilon: 1.0    steps: 120  evaluation reward: 6.53\n",
      "episode: 481   score: 7.0  epsilon: 1.0    steps: 192  evaluation reward: 6.58\n",
      "episode: 482   score: 4.0  epsilon: 1.0    steps: 616  evaluation reward: 6.53\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 493: Policy loss: 0.007985. Value loss: 0.208391. Entropy: 1.007518.\n",
      "Iteration 494: Policy loss: -0.009598. Value loss: 0.112318. Entropy: 1.009963.\n",
      "Iteration 495: Policy loss: -0.023703. Value loss: 0.069721. Entropy: 1.000684.\n",
      "episode: 483   score: 12.0  epsilon: 1.0    steps: 840  evaluation reward: 6.59\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 496: Policy loss: 0.003836. Value loss: 0.250556. Entropy: 1.019810.\n",
      "Iteration 497: Policy loss: -0.013903. Value loss: 0.106474. Entropy: 1.020001.\n",
      "Iteration 498: Policy loss: -0.025395. Value loss: 0.066252. Entropy: 1.010013.\n",
      "episode: 484   score: 8.0  epsilon: 1.0    steps: 24  evaluation reward: 6.58\n",
      "episode: 485   score: 11.0  epsilon: 1.0    steps: 528  evaluation reward: 6.63\n",
      "episode: 486   score: 4.0  epsilon: 1.0    steps: 1000  evaluation reward: 6.59\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 499: Policy loss: 0.008404. Value loss: 0.315367. Entropy: 1.042337.\n",
      "Iteration 500: Policy loss: -0.004646. Value loss: 0.162924. Entropy: 1.043696.\n",
      "Iteration 501: Policy loss: -0.021774. Value loss: 0.106323. Entropy: 1.042847.\n",
      "episode: 487   score: 8.0  epsilon: 1.0    steps: 256  evaluation reward: 6.61\n",
      "episode: 488   score: 8.0  epsilon: 1.0    steps: 336  evaluation reward: 6.6\n",
      "episode: 489   score: 5.0  epsilon: 1.0    steps: 960  evaluation reward: 6.57\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 502: Policy loss: 0.010898. Value loss: 0.219142. Entropy: 1.041586.\n",
      "Iteration 503: Policy loss: -0.014418. Value loss: 0.097542. Entropy: 1.033161.\n",
      "Iteration 504: Policy loss: -0.023830. Value loss: 0.068686. Entropy: 1.043823.\n",
      "episode: 490   score: 8.0  epsilon: 1.0    steps: 136  evaluation reward: 6.48\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 505: Policy loss: 0.003116. Value loss: 0.230673. Entropy: 0.971703.\n",
      "Iteration 506: Policy loss: -0.014233. Value loss: 0.123416. Entropy: 0.980815.\n",
      "Iteration 507: Policy loss: -0.023018. Value loss: 0.082765. Entropy: 0.974803.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 508: Policy loss: 0.005716. Value loss: 0.180809. Entropy: 1.086758.\n",
      "Iteration 509: Policy loss: -0.015588. Value loss: 0.089453. Entropy: 1.096965.\n",
      "Iteration 510: Policy loss: -0.024729. Value loss: 0.059676. Entropy: 1.082171.\n",
      "episode: 491   score: 4.0  epsilon: 1.0    steps: 144  evaluation reward: 6.42\n",
      "episode: 492   score: 5.0  epsilon: 1.0    steps: 240  evaluation reward: 6.41\n",
      "episode: 493   score: 5.0  epsilon: 1.0    steps: 376  evaluation reward: 6.36\n",
      "episode: 494   score: 8.0  epsilon: 1.0    steps: 864  evaluation reward: 6.38\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 511: Policy loss: 0.006320. Value loss: 0.234375. Entropy: 1.051592.\n",
      "Iteration 512: Policy loss: -0.011678. Value loss: 0.112076. Entropy: 1.037107.\n",
      "Iteration 513: Policy loss: -0.022135. Value loss: 0.084382. Entropy: 1.045680.\n",
      "episode: 495   score: 4.0  epsilon: 1.0    steps: 88  evaluation reward: 6.35\n",
      "episode: 496   score: 12.0  epsilon: 1.0    steps: 280  evaluation reward: 6.38\n",
      "episode: 497   score: 11.0  epsilon: 1.0    steps: 752  evaluation reward: 6.41\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 514: Policy loss: 0.007487. Value loss: 0.225359. Entropy: 1.027879.\n",
      "Iteration 515: Policy loss: -0.011752. Value loss: 0.103805. Entropy: 1.024319.\n",
      "Iteration 516: Policy loss: -0.019484. Value loss: 0.078224. Entropy: 1.024211.\n",
      "episode: 498   score: 2.0  epsilon: 1.0    steps: 536  evaluation reward: 6.4\n",
      "episode: 499   score: 3.0  epsilon: 1.0    steps: 584  evaluation reward: 6.33\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 517: Policy loss: 0.010903. Value loss: 0.449142. Entropy: 0.968571.\n",
      "Iteration 518: Policy loss: -0.005239. Value loss: 0.246412. Entropy: 0.986679.\n",
      "Iteration 519: Policy loss: -0.018556. Value loss: 0.158708. Entropy: 0.986246.\n",
      "episode: 500   score: 14.0  epsilon: 1.0    steps: 152  evaluation reward: 6.37\n",
      "now time :  2019-03-06 12:38:52.347263\n",
      "episode: 501   score: 5.0  epsilon: 1.0    steps: 456  evaluation reward: 6.34\n",
      "episode: 502   score: 4.0  epsilon: 1.0    steps: 840  evaluation reward: 6.29\n",
      "episode: 503   score: 3.0  epsilon: 1.0    steps: 968  evaluation reward: 6.28\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 520: Policy loss: 0.006511. Value loss: 0.240870. Entropy: 0.994301.\n",
      "Iteration 521: Policy loss: -0.013591. Value loss: 0.121426. Entropy: 0.984338.\n",
      "Iteration 522: Policy loss: -0.025687. Value loss: 0.083459. Entropy: 0.991822.\n",
      "episode: 504   score: 6.0  epsilon: 1.0    steps: 256  evaluation reward: 6.27\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 523: Policy loss: 0.006996. Value loss: 0.271711. Entropy: 0.999249.\n",
      "Iteration 524: Policy loss: -0.013338. Value loss: 0.132771. Entropy: 1.004230.\n",
      "Iteration 525: Policy loss: -0.024387. Value loss: 0.097210. Entropy: 1.014602.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 526: Policy loss: 0.005153. Value loss: 0.189177. Entropy: 1.036298.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 527: Policy loss: -0.010588. Value loss: 0.104489. Entropy: 1.020597.\n",
      "Iteration 528: Policy loss: -0.024173. Value loss: 0.077180. Entropy: 1.025509.\n",
      "episode: 505   score: 3.0  epsilon: 1.0    steps: 288  evaluation reward: 6.26\n",
      "episode: 506   score: 4.0  epsilon: 1.0    steps: 344  evaluation reward: 6.26\n",
      "episode: 507   score: 7.0  epsilon: 1.0    steps: 432  evaluation reward: 6.29\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 529: Policy loss: 0.005713. Value loss: 0.237102. Entropy: 0.954925.\n",
      "Iteration 530: Policy loss: -0.014112. Value loss: 0.131813. Entropy: 0.956520.\n",
      "Iteration 531: Policy loss: -0.023617. Value loss: 0.091946. Entropy: 0.958781.\n",
      "episode: 508   score: 11.0  epsilon: 1.0    steps: 64  evaluation reward: 6.31\n",
      "episode: 509   score: 9.0  epsilon: 1.0    steps: 488  evaluation reward: 6.32\n",
      "episode: 510   score: 12.0  epsilon: 1.0    steps: 944  evaluation reward: 6.38\n",
      "episode: 511   score: 7.0  epsilon: 1.0    steps: 960  evaluation reward: 6.41\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 532: Policy loss: 0.001088. Value loss: 0.482242. Entropy: 1.024464.\n",
      "Iteration 533: Policy loss: -0.004490. Value loss: 0.253444. Entropy: 1.023964.\n",
      "Iteration 534: Policy loss: -0.012150. Value loss: 0.174975. Entropy: 1.023716.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 535: Policy loss: 0.007910. Value loss: 0.211856. Entropy: 0.902834.\n",
      "Iteration 536: Policy loss: -0.007908. Value loss: 0.103080. Entropy: 0.895911.\n",
      "Iteration 537: Policy loss: -0.018580. Value loss: 0.079953. Entropy: 0.900966.\n",
      "episode: 512   score: 4.0  epsilon: 1.0    steps: 336  evaluation reward: 6.39\n",
      "episode: 513   score: 9.0  epsilon: 1.0    steps: 424  evaluation reward: 6.4\n",
      "episode: 514   score: 4.0  epsilon: 1.0    steps: 792  evaluation reward: 6.32\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 538: Policy loss: 0.003729. Value loss: 0.215601. Entropy: 0.916046.\n",
      "Iteration 539: Policy loss: -0.008919. Value loss: 0.105836. Entropy: 0.914944.\n",
      "Iteration 540: Policy loss: -0.021606. Value loss: 0.063771. Entropy: 0.903426.\n",
      "episode: 515   score: 7.0  epsilon: 1.0    steps: 624  evaluation reward: 6.31\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 541: Policy loss: 0.004406. Value loss: 0.279430. Entropy: 1.051323.\n",
      "Iteration 542: Policy loss: -0.011044. Value loss: 0.125686. Entropy: 1.053959.\n",
      "Iteration 543: Policy loss: -0.025357. Value loss: 0.094522. Entropy: 1.051799.\n",
      "episode: 516   score: 10.0  epsilon: 1.0    steps: 416  evaluation reward: 6.38\n",
      "episode: 517   score: 7.0  epsilon: 1.0    steps: 520  evaluation reward: 6.41\n",
      "episode: 518   score: 7.0  epsilon: 1.0    steps: 928  evaluation reward: 6.42\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 544: Policy loss: 0.002019. Value loss: 0.150260. Entropy: 0.873614.\n",
      "Iteration 545: Policy loss: -0.012650. Value loss: 0.077200. Entropy: 0.887859.\n",
      "Iteration 546: Policy loss: -0.023438. Value loss: 0.055763. Entropy: 0.879640.\n",
      "episode: 519   score: 8.0  epsilon: 1.0    steps: 144  evaluation reward: 6.42\n",
      "episode: 520   score: 6.0  epsilon: 1.0    steps: 872  evaluation reward: 6.45\n",
      "episode: 521   score: 6.0  epsilon: 1.0    steps: 912  evaluation reward: 6.47\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 547: Policy loss: 0.005620. Value loss: 0.252169. Entropy: 0.932608.\n",
      "Iteration 548: Policy loss: -0.010770. Value loss: 0.125045. Entropy: 0.938283.\n",
      "Iteration 549: Policy loss: -0.022594. Value loss: 0.086707. Entropy: 0.925843.\n",
      "episode: 522   score: 2.0  epsilon: 1.0    steps: 440  evaluation reward: 6.47\n",
      "episode: 523   score: 8.0  epsilon: 1.0    steps: 1016  evaluation reward: 6.52\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 550: Policy loss: 0.002655. Value loss: 0.257713. Entropy: 0.963547.\n",
      "Iteration 551: Policy loss: -0.011444. Value loss: 0.108785. Entropy: 0.962728.\n",
      "Iteration 552: Policy loss: -0.019778. Value loss: 0.073146. Entropy: 0.969381.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 553: Policy loss: 0.006414. Value loss: 0.181294. Entropy: 0.893887.\n",
      "Iteration 554: Policy loss: -0.015908. Value loss: 0.073009. Entropy: 0.897191.\n",
      "Iteration 555: Policy loss: -0.026694. Value loss: 0.040940. Entropy: 0.904088.\n",
      "episode: 524   score: 5.0  epsilon: 1.0    steps: 48  evaluation reward: 6.44\n",
      "episode: 525   score: 8.0  epsilon: 1.0    steps: 72  evaluation reward: 6.45\n",
      "episode: 526   score: 8.0  epsilon: 1.0    steps: 976  evaluation reward: 6.44\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 556: Policy loss: 0.003197. Value loss: 0.225964. Entropy: 1.007610.\n",
      "Iteration 557: Policy loss: -0.014743. Value loss: 0.099722. Entropy: 1.000572.\n",
      "Iteration 558: Policy loss: -0.026070. Value loss: 0.065391. Entropy: 1.010213.\n",
      "episode: 527   score: 6.0  epsilon: 1.0    steps: 648  evaluation reward: 6.42\n",
      "episode: 528   score: 5.0  epsilon: 1.0    steps: 1016  evaluation reward: 6.42\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 559: Policy loss: 0.004729. Value loss: 0.193270. Entropy: 0.963589.\n",
      "Iteration 560: Policy loss: -0.008104. Value loss: 0.087034. Entropy: 0.983161.\n",
      "Iteration 561: Policy loss: -0.022646. Value loss: 0.052617. Entropy: 0.983390.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 562: Policy loss: 0.007182. Value loss: 0.215969. Entropy: 1.091229.\n",
      "Iteration 563: Policy loss: -0.012626. Value loss: 0.103178. Entropy: 1.100213.\n",
      "Iteration 564: Policy loss: -0.022049. Value loss: 0.070475. Entropy: 1.100418.\n",
      "episode: 529   score: 9.0  epsilon: 1.0    steps: 336  evaluation reward: 6.46\n",
      "episode: 530   score: 7.0  epsilon: 1.0    steps: 696  evaluation reward: 6.48\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 565: Policy loss: 0.005085. Value loss: 0.228387. Entropy: 1.032557.\n",
      "Iteration 566: Policy loss: -0.011516. Value loss: 0.090831. Entropy: 1.033258.\n",
      "Iteration 567: Policy loss: -0.024633. Value loss: 0.061800. Entropy: 1.039165.\n",
      "episode: 531   score: 3.0  epsilon: 1.0    steps: 232  evaluation reward: 6.43\n",
      "episode: 532   score: 16.0  epsilon: 1.0    steps: 448  evaluation reward: 6.54\n",
      "episode: 533   score: 15.0  epsilon: 1.0    steps: 552  evaluation reward: 6.63\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 568: Policy loss: 0.005671. Value loss: 0.304843. Entropy: 1.007776.\n",
      "Iteration 569: Policy loss: -0.008654. Value loss: 0.189999. Entropy: 1.004174.\n",
      "Iteration 570: Policy loss: -0.014024. Value loss: 0.140672. Entropy: 1.001556.\n",
      "episode: 534   score: 10.0  epsilon: 1.0    steps: 320  evaluation reward: 6.7\n",
      "episode: 535   score: 7.0  epsilon: 1.0    steps: 824  evaluation reward: 6.63\n",
      "episode: 536   score: 9.0  epsilon: 1.0    steps: 1008  evaluation reward: 6.69\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 571: Policy loss: 0.005619. Value loss: 0.225558. Entropy: 1.054925.\n",
      "Iteration 572: Policy loss: -0.012078. Value loss: 0.093553. Entropy: 1.053017.\n",
      "Iteration 573: Policy loss: -0.026369. Value loss: 0.059591. Entropy: 1.040604.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 574: Policy loss: 0.009513. Value loss: 0.200033. Entropy: 0.994993.\n",
      "Iteration 575: Policy loss: -0.017432. Value loss: 0.085901. Entropy: 1.008884.\n",
      "Iteration 576: Policy loss: -0.029132. Value loss: 0.047735. Entropy: 1.009040.\n",
      "episode: 537   score: 7.0  epsilon: 1.0    steps: 632  evaluation reward: 6.7\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 577: Policy loss: 0.001587. Value loss: 0.187167. Entropy: 1.029395.\n",
      "Iteration 578: Policy loss: -0.014979. Value loss: 0.080870. Entropy: 1.023122.\n",
      "Iteration 579: Policy loss: -0.024560. Value loss: 0.050641. Entropy: 1.019853.\n",
      "episode: 538   score: 6.0  epsilon: 1.0    steps: 168  evaluation reward: 6.69\n",
      "episode: 539   score: 5.0  epsilon: 1.0    steps: 384  evaluation reward: 6.69\n",
      "episode: 540   score: 7.0  epsilon: 1.0    steps: 552  evaluation reward: 6.72\n",
      "episode: 541   score: 7.0  epsilon: 1.0    steps: 848  evaluation reward: 6.72\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 580: Policy loss: 0.015364. Value loss: 0.366541. Entropy: 1.019935.\n",
      "Iteration 581: Policy loss: -0.006300. Value loss: 0.178932. Entropy: 1.026333.\n",
      "Iteration 582: Policy loss: -0.007565. Value loss: 0.129947. Entropy: 1.016827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 583: Policy loss: 0.004868. Value loss: 0.232219. Entropy: 0.994816.\n",
      "Iteration 584: Policy loss: -0.008912. Value loss: 0.111045. Entropy: 0.988831.\n",
      "Iteration 585: Policy loss: -0.018923. Value loss: 0.072158. Entropy: 0.990723.\n",
      "episode: 542   score: 15.0  epsilon: 1.0    steps: 184  evaluation reward: 6.84\n",
      "episode: 543   score: 7.0  epsilon: 1.0    steps: 272  evaluation reward: 6.84\n",
      "episode: 544   score: 8.0  epsilon: 1.0    steps: 472  evaluation reward: 6.84\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 586: Policy loss: 0.004587. Value loss: 0.207427. Entropy: 0.971878.\n",
      "Iteration 587: Policy loss: -0.012034. Value loss: 0.083219. Entropy: 0.964421.\n",
      "Iteration 588: Policy loss: -0.028973. Value loss: 0.057785. Entropy: 0.955684.\n",
      "episode: 545   score: 4.0  epsilon: 1.0    steps: 120  evaluation reward: 6.84\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 589: Policy loss: 0.013430. Value loss: 0.419853. Entropy: 0.987145.\n",
      "Iteration 590: Policy loss: -0.005295. Value loss: 0.245150. Entropy: 0.992453.\n",
      "Iteration 591: Policy loss: -0.010660. Value loss: 0.156388. Entropy: 0.992573.\n",
      "episode: 546   score: 5.0  epsilon: 1.0    steps: 64  evaluation reward: 6.82\n",
      "episode: 547   score: 11.0  epsilon: 1.0    steps: 312  evaluation reward: 6.86\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 592: Policy loss: 0.006447. Value loss: 0.246981. Entropy: 0.952846.\n",
      "Iteration 593: Policy loss: -0.009811. Value loss: 0.100229. Entropy: 0.939535.\n",
      "Iteration 594: Policy loss: -0.022855. Value loss: 0.062313. Entropy: 0.949706.\n",
      "episode: 548   score: 3.0  epsilon: 1.0    steps: 616  evaluation reward: 6.8\n",
      "episode: 549   score: 5.0  epsilon: 1.0    steps: 952  evaluation reward: 6.79\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 595: Policy loss: 0.005951. Value loss: 0.256389. Entropy: 0.951491.\n",
      "Iteration 596: Policy loss: -0.012048. Value loss: 0.110173. Entropy: 0.951481.\n",
      "Iteration 597: Policy loss: -0.022032. Value loss: 0.072102. Entropy: 0.952717.\n",
      "episode: 550   score: 11.0  epsilon: 1.0    steps: 392  evaluation reward: 6.85\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 598: Policy loss: 0.006200. Value loss: 0.321937. Entropy: 0.994813.\n",
      "Iteration 599: Policy loss: -0.007007. Value loss: 0.138332. Entropy: 1.002705.\n",
      "Iteration 600: Policy loss: -0.018015. Value loss: 0.102797. Entropy: 0.996527.\n",
      "now time :  2019-03-06 12:40:36.884425\n",
      "episode: 551   score: 13.0  epsilon: 1.0    steps: 336  evaluation reward: 6.95\n",
      "episode: 552   score: 5.0  epsilon: 1.0    steps: 440  evaluation reward: 6.94\n",
      "episode: 553   score: 9.0  epsilon: 1.0    steps: 448  evaluation reward: 6.98\n",
      "episode: 554   score: 6.0  epsilon: 1.0    steps: 952  evaluation reward: 6.99\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 601: Policy loss: 0.006476. Value loss: 0.507600. Entropy: 0.939737.\n",
      "Iteration 602: Policy loss: -0.004620. Value loss: 0.265289. Entropy: 0.949386.\n",
      "Iteration 603: Policy loss: -0.016271. Value loss: 0.190421. Entropy: 0.939082.\n",
      "episode: 555   score: 15.0  epsilon: 1.0    steps: 560  evaluation reward: 7.08\n",
      "episode: 556   score: 5.0  epsilon: 1.0    steps: 976  evaluation reward: 7.02\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 604: Policy loss: 0.011886. Value loss: 0.294239. Entropy: 0.855027.\n",
      "Iteration 605: Policy loss: -0.013453. Value loss: 0.127952. Entropy: 0.856293.\n",
      "Iteration 606: Policy loss: -0.022718. Value loss: 0.088519. Entropy: 0.861664.\n",
      "episode: 557   score: 5.0  epsilon: 1.0    steps: 608  evaluation reward: 7.03\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 607: Policy loss: 0.001546. Value loss: 0.287915. Entropy: 0.896314.\n",
      "Iteration 608: Policy loss: -0.013129. Value loss: 0.142897. Entropy: 0.907507.\n",
      "Iteration 609: Policy loss: -0.024918. Value loss: 0.103526. Entropy: 0.913899.\n",
      "episode: 558   score: 3.0  epsilon: 1.0    steps: 8  evaluation reward: 7.01\n",
      "episode: 559   score: 10.0  epsilon: 1.0    steps: 792  evaluation reward: 7.06\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 610: Policy loss: 0.003796. Value loss: 0.248887. Entropy: 0.954469.\n",
      "Iteration 611: Policy loss: -0.009347. Value loss: 0.121011. Entropy: 0.959057.\n",
      "Iteration 612: Policy loss: -0.021075. Value loss: 0.076475. Entropy: 0.963302.\n",
      "episode: 560   score: 7.0  epsilon: 1.0    steps: 216  evaluation reward: 7.09\n",
      "episode: 561   score: 3.0  epsilon: 1.0    steps: 896  evaluation reward: 7.04\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 613: Policy loss: -0.002048. Value loss: 0.186219. Entropy: 0.935598.\n",
      "Iteration 614: Policy loss: -0.009757. Value loss: 0.095588. Entropy: 0.935680.\n",
      "Iteration 615: Policy loss: -0.023232. Value loss: 0.065860. Entropy: 0.939133.\n",
      "episode: 562   score: 9.0  epsilon: 1.0    steps: 96  evaluation reward: 7.05\n",
      "episode: 563   score: 5.0  epsilon: 1.0    steps: 120  evaluation reward: 7.02\n",
      "episode: 564   score: 6.0  epsilon: 1.0    steps: 304  evaluation reward: 6.99\n",
      "episode: 565   score: 8.0  epsilon: 1.0    steps: 448  evaluation reward: 7.0\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 616: Policy loss: 0.006608. Value loss: 0.168267. Entropy: 0.925252.\n",
      "Iteration 617: Policy loss: -0.014825. Value loss: 0.079832. Entropy: 0.948414.\n",
      "Iteration 618: Policy loss: -0.023378. Value loss: 0.067526. Entropy: 0.952909.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 619: Policy loss: 0.003201. Value loss: 0.295922. Entropy: 1.002609.\n",
      "Iteration 620: Policy loss: -0.013519. Value loss: 0.142822. Entropy: 1.003085.\n",
      "Iteration 621: Policy loss: -0.021819. Value loss: 0.101531. Entropy: 0.988427.\n",
      "episode: 566   score: 5.0  epsilon: 1.0    steps: 176  evaluation reward: 6.98\n",
      "episode: 567   score: 9.0  epsilon: 1.0    steps: 536  evaluation reward: 7.03\n",
      "episode: 568   score: 4.0  epsilon: 1.0    steps: 880  evaluation reward: 6.99\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 622: Policy loss: 0.007177. Value loss: 0.261073. Entropy: 0.994512.\n",
      "Iteration 623: Policy loss: -0.009587. Value loss: 0.109162. Entropy: 0.989423.\n",
      "Iteration 624: Policy loss: -0.023941. Value loss: 0.065274. Entropy: 0.988867.\n",
      "episode: 569   score: 4.0  epsilon: 1.0    steps: 40  evaluation reward: 6.98\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 625: Policy loss: 0.004121. Value loss: 0.189855. Entropy: 0.946535.\n",
      "Iteration 626: Policy loss: -0.014892. Value loss: 0.087907. Entropy: 0.955432.\n",
      "Iteration 627: Policy loss: -0.025416. Value loss: 0.049271. Entropy: 0.952955.\n",
      "episode: 570   score: 10.0  epsilon: 1.0    steps: 456  evaluation reward: 7.01\n",
      "episode: 571   score: 8.0  epsilon: 1.0    steps: 552  evaluation reward: 7.05\n",
      "episode: 572   score: 7.0  epsilon: 1.0    steps: 752  evaluation reward: 7.08\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 628: Policy loss: 0.008450. Value loss: 0.184631. Entropy: 0.917919.\n",
      "Iteration 629: Policy loss: -0.012369. Value loss: 0.084332. Entropy: 0.927003.\n",
      "Iteration 630: Policy loss: -0.020810. Value loss: 0.060272. Entropy: 0.910804.\n",
      "episode: 573   score: 9.0  epsilon: 1.0    steps: 176  evaluation reward: 7.09\n",
      "episode: 574   score: 5.0  epsilon: 1.0    steps: 184  evaluation reward: 7.05\n",
      "episode: 575   score: 5.0  epsilon: 1.0    steps: 808  evaluation reward: 7.05\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 631: Policy loss: -0.000776. Value loss: 0.361889. Entropy: 1.040225.\n",
      "Iteration 632: Policy loss: -0.017732. Value loss: 0.198387. Entropy: 1.043545.\n",
      "Iteration 633: Policy loss: -0.027568. Value loss: 0.135103. Entropy: 1.033396.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 634: Policy loss: 0.005035. Value loss: 0.250599. Entropy: 0.959224.\n",
      "Iteration 635: Policy loss: -0.015406. Value loss: 0.103110. Entropy: 0.957792.\n",
      "Iteration 636: Policy loss: -0.026361. Value loss: 0.069792. Entropy: 0.948837.\n",
      "episode: 576   score: 4.0  epsilon: 1.0    steps: 296  evaluation reward: 7.0\n",
      "episode: 577   score: 4.0  epsilon: 1.0    steps: 320  evaluation reward: 6.98\n",
      "episode: 578   score: 8.0  epsilon: 1.0    steps: 336  evaluation reward: 7.03\n",
      "episode: 579   score: 10.0  epsilon: 1.0    steps: 632  evaluation reward: 7.06\n",
      "Training network. lr: 0.000245. clip: 0.098166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 637: Policy loss: 0.005602. Value loss: 0.374420. Entropy: 0.916068.\n",
      "Iteration 638: Policy loss: 0.005869. Value loss: 0.196664. Entropy: 0.918266.\n",
      "Iteration 639: Policy loss: -0.003681. Value loss: 0.145378. Entropy: 0.906557.\n",
      "episode: 580   score: 7.0  epsilon: 1.0    steps: 472  evaluation reward: 7.09\n",
      "episode: 581   score: 8.0  epsilon: 1.0    steps: 688  evaluation reward: 7.1\n",
      "episode: 582   score: 6.0  epsilon: 1.0    steps: 1024  evaluation reward: 7.12\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 640: Policy loss: 0.002203. Value loss: 0.213241. Entropy: 0.981409.\n",
      "Iteration 641: Policy loss: -0.014982. Value loss: 0.105491. Entropy: 0.972633.\n",
      "Iteration 642: Policy loss: -0.021469. Value loss: 0.066577. Entropy: 0.970861.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 643: Policy loss: -0.000285. Value loss: 0.220339. Entropy: 0.985958.\n",
      "Iteration 644: Policy loss: -0.014611. Value loss: 0.111689. Entropy: 0.980875.\n",
      "Iteration 645: Policy loss: -0.024150. Value loss: 0.078841. Entropy: 0.979319.\n",
      "episode: 583   score: 8.0  epsilon: 1.0    steps: 464  evaluation reward: 7.08\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 646: Policy loss: 0.007298. Value loss: 0.440738. Entropy: 0.978182.\n",
      "Iteration 647: Policy loss: -0.007573. Value loss: 0.263528. Entropy: 0.986608.\n",
      "Iteration 648: Policy loss: -0.016186. Value loss: 0.185915. Entropy: 0.981429.\n",
      "episode: 584   score: 6.0  epsilon: 1.0    steps: 64  evaluation reward: 7.06\n",
      "episode: 585   score: 11.0  epsilon: 1.0    steps: 440  evaluation reward: 7.06\n",
      "episode: 586   score: 7.0  epsilon: 1.0    steps: 1016  evaluation reward: 7.09\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 649: Policy loss: 0.006245. Value loss: 0.222360. Entropy: 0.943239.\n",
      "Iteration 650: Policy loss: -0.002974. Value loss: 0.114925. Entropy: 0.941500.\n",
      "Iteration 651: Policy loss: -0.021424. Value loss: 0.069953. Entropy: 0.948305.\n",
      "episode: 587   score: 7.0  epsilon: 1.0    steps: 200  evaluation reward: 7.08\n",
      "episode: 588   score: 8.0  epsilon: 1.0    steps: 376  evaluation reward: 7.08\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 652: Policy loss: 0.003801. Value loss: 0.304368. Entropy: 1.046416.\n",
      "Iteration 653: Policy loss: -0.019239. Value loss: 0.150162. Entropy: 1.039258.\n",
      "Iteration 654: Policy loss: -0.028195. Value loss: 0.096807. Entropy: 1.049265.\n",
      "episode: 589   score: 9.0  epsilon: 1.0    steps: 256  evaluation reward: 7.12\n",
      "episode: 590   score: 8.0  epsilon: 1.0    steps: 288  evaluation reward: 7.12\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 655: Policy loss: 0.010891. Value loss: 0.230184. Entropy: 0.973066.\n",
      "Iteration 656: Policy loss: -0.010558. Value loss: 0.083298. Entropy: 0.967165.\n",
      "Iteration 657: Policy loss: -0.026214. Value loss: 0.060458. Entropy: 0.964254.\n",
      "episode: 591   score: 7.0  epsilon: 1.0    steps: 360  evaluation reward: 7.15\n",
      "episode: 592   score: 2.0  epsilon: 1.0    steps: 464  evaluation reward: 7.12\n",
      "episode: 593   score: 6.0  epsilon: 1.0    steps: 880  evaluation reward: 7.13\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 658: Policy loss: 0.006742. Value loss: 0.228026. Entropy: 0.977037.\n",
      "Iteration 659: Policy loss: -0.010628. Value loss: 0.100451. Entropy: 0.970745.\n",
      "Iteration 660: Policy loss: -0.021718. Value loss: 0.062662. Entropy: 0.958616.\n",
      "episode: 594   score: 6.0  epsilon: 1.0    steps: 40  evaluation reward: 7.11\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 661: Policy loss: 0.001716. Value loss: 0.194806. Entropy: 0.963722.\n",
      "Iteration 662: Policy loss: -0.014738. Value loss: 0.080097. Entropy: 0.965181.\n",
      "Iteration 663: Policy loss: -0.025644. Value loss: 0.054514. Entropy: 0.958857.\n",
      "episode: 595   score: 12.0  epsilon: 1.0    steps: 464  evaluation reward: 7.19\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 664: Policy loss: 0.010720. Value loss: 0.360216. Entropy: 0.856379.\n",
      "Iteration 665: Policy loss: -0.005979. Value loss: 0.187325. Entropy: 0.860306.\n",
      "Iteration 666: Policy loss: -0.013137. Value loss: 0.111864. Entropy: 0.861141.\n",
      "episode: 596   score: 10.0  epsilon: 1.0    steps: 832  evaluation reward: 7.17\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 667: Policy loss: 0.003846. Value loss: 0.312042. Entropy: 0.973301.\n",
      "Iteration 668: Policy loss: -0.011443. Value loss: 0.157805. Entropy: 0.962015.\n",
      "Iteration 669: Policy loss: -0.022890. Value loss: 0.098075. Entropy: 0.956347.\n",
      "episode: 597   score: 9.0  epsilon: 1.0    steps: 48  evaluation reward: 7.15\n",
      "episode: 598   score: 6.0  epsilon: 1.0    steps: 248  evaluation reward: 7.19\n",
      "episode: 599   score: 14.0  epsilon: 1.0    steps: 728  evaluation reward: 7.3\n",
      "episode: 600   score: 9.0  epsilon: 1.0    steps: 1000  evaluation reward: 7.25\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 670: Policy loss: 0.011003. Value loss: 0.535056. Entropy: 0.908082.\n",
      "Iteration 671: Policy loss: -0.003734. Value loss: 0.258416. Entropy: 0.907367.\n",
      "Iteration 672: Policy loss: -0.015702. Value loss: 0.176933. Entropy: 0.912122.\n",
      "now time :  2019-03-06 12:42:09.721428\n",
      "episode: 601   score: 8.0  epsilon: 1.0    steps: 184  evaluation reward: 7.28\n",
      "episode: 602   score: 9.0  epsilon: 1.0    steps: 544  evaluation reward: 7.33\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 673: Policy loss: 0.006182. Value loss: 0.270900. Entropy: 0.888702.\n",
      "Iteration 674: Policy loss: -0.010804. Value loss: 0.150670. Entropy: 0.879899.\n",
      "Iteration 675: Policy loss: -0.021119. Value loss: 0.093872. Entropy: 0.888400.\n",
      "episode: 603   score: 7.0  epsilon: 1.0    steps: 328  evaluation reward: 7.37\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 676: Policy loss: 0.006035. Value loss: 0.323154. Entropy: 0.957391.\n",
      "Iteration 677: Policy loss: -0.012791. Value loss: 0.151590. Entropy: 0.966908.\n",
      "Iteration 678: Policy loss: -0.020121. Value loss: 0.105989. Entropy: 0.952170.\n",
      "episode: 604   score: 5.0  epsilon: 1.0    steps: 480  evaluation reward: 7.36\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 679: Policy loss: 0.002812. Value loss: 0.395454. Entropy: 0.975132.\n",
      "Iteration 680: Policy loss: -0.007947. Value loss: 0.176793. Entropy: 0.978872.\n",
      "Iteration 681: Policy loss: -0.023343. Value loss: 0.090176. Entropy: 0.977720.\n",
      "episode: 605   score: 7.0  epsilon: 1.0    steps: 96  evaluation reward: 7.4\n",
      "episode: 606   score: 10.0  epsilon: 1.0    steps: 584  evaluation reward: 7.46\n",
      "episode: 607   score: 4.0  epsilon: 1.0    steps: 664  evaluation reward: 7.43\n",
      "episode: 608   score: 3.0  epsilon: 1.0    steps: 968  evaluation reward: 7.35\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 682: Policy loss: 0.007791. Value loss: 0.252410. Entropy: 0.997559.\n",
      "Iteration 683: Policy loss: -0.011361. Value loss: 0.159326. Entropy: 1.001097.\n",
      "Iteration 684: Policy loss: -0.021488. Value loss: 0.109159. Entropy: 0.988599.\n",
      "episode: 609   score: 8.0  epsilon: 1.0    steps: 648  evaluation reward: 7.34\n",
      "episode: 610   score: 10.0  epsilon: 1.0    steps: 656  evaluation reward: 7.32\n",
      "episode: 611   score: 9.0  epsilon: 1.0    steps: 776  evaluation reward: 7.34\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 685: Policy loss: 0.005286. Value loss: 0.239789. Entropy: 0.909256.\n",
      "Iteration 686: Policy loss: -0.007474. Value loss: 0.119706. Entropy: 0.915187.\n",
      "Iteration 687: Policy loss: -0.022936. Value loss: 0.081805. Entropy: 0.915705.\n",
      "episode: 612   score: 6.0  epsilon: 1.0    steps: 864  evaluation reward: 7.36\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 688: Policy loss: 0.003171. Value loss: 0.278032. Entropy: 0.969407.\n",
      "Iteration 689: Policy loss: -0.011778. Value loss: 0.135043. Entropy: 0.974605.\n",
      "Iteration 690: Policy loss: -0.021296. Value loss: 0.084372. Entropy: 0.973721.\n",
      "episode: 613   score: 5.0  epsilon: 1.0    steps: 824  evaluation reward: 7.32\n",
      "episode: 614   score: 7.0  epsilon: 1.0    steps: 904  evaluation reward: 7.35\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 691: Policy loss: 0.002202. Value loss: 0.279723. Entropy: 0.963953.\n",
      "Iteration 692: Policy loss: -0.016237. Value loss: 0.114190. Entropy: 0.954309.\n",
      "Iteration 693: Policy loss: -0.022054. Value loss: 0.071188. Entropy: 0.949496.\n",
      "episode: 615   score: 3.0  epsilon: 1.0    steps: 16  evaluation reward: 7.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 616   score: 6.0  epsilon: 1.0    steps: 200  evaluation reward: 7.27\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 694: Policy loss: 0.004269. Value loss: 0.174792. Entropy: 0.951645.\n",
      "Iteration 695: Policy loss: -0.013438. Value loss: 0.085032. Entropy: 0.957506.\n",
      "Iteration 696: Policy loss: -0.025275. Value loss: 0.060568. Entropy: 0.961180.\n",
      "episode: 617   score: 4.0  epsilon: 1.0    steps: 416  evaluation reward: 7.24\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 697: Policy loss: 0.005816. Value loss: 0.252816. Entropy: 0.926916.\n",
      "Iteration 698: Policy loss: -0.014084. Value loss: 0.108484. Entropy: 0.920964.\n",
      "Iteration 699: Policy loss: -0.024784. Value loss: 0.058554. Entropy: 0.917857.\n",
      "episode: 618   score: 8.0  epsilon: 1.0    steps: 8  evaluation reward: 7.25\n",
      "episode: 619   score: 10.0  epsilon: 1.0    steps: 280  evaluation reward: 7.27\n",
      "episode: 620   score: 10.0  epsilon: 1.0    steps: 840  evaluation reward: 7.31\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 700: Policy loss: 0.005688. Value loss: 0.130973. Entropy: 0.884806.\n",
      "Iteration 701: Policy loss: -0.016804. Value loss: 0.048233. Entropy: 0.892672.\n",
      "Iteration 702: Policy loss: -0.025774. Value loss: 0.028369. Entropy: 0.889011.\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 703: Policy loss: 0.000549. Value loss: 0.195716. Entropy: 0.944573.\n",
      "Iteration 704: Policy loss: -0.013197. Value loss: 0.083322. Entropy: 0.948313.\n",
      "Iteration 705: Policy loss: -0.025261. Value loss: 0.053618. Entropy: 0.951765.\n",
      "episode: 621   score: 7.0  epsilon: 1.0    steps: 48  evaluation reward: 7.32\n",
      "episode: 622   score: 8.0  epsilon: 1.0    steps: 336  evaluation reward: 7.38\n",
      "episode: 623   score: 11.0  epsilon: 1.0    steps: 672  evaluation reward: 7.41\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 706: Policy loss: 0.005407. Value loss: 0.483156. Entropy: 0.995959.\n",
      "Iteration 707: Policy loss: -0.007133. Value loss: 0.221523. Entropy: 0.984617.\n",
      "Iteration 708: Policy loss: -0.019748. Value loss: 0.148924. Entropy: 0.980188.\n",
      "episode: 624   score: 3.0  epsilon: 1.0    steps: 96  evaluation reward: 7.39\n",
      "episode: 625   score: 13.0  epsilon: 1.0    steps: 640  evaluation reward: 7.44\n",
      "episode: 626   score: 8.0  epsilon: 1.0    steps: 800  evaluation reward: 7.44\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 709: Policy loss: 0.006923. Value loss: 0.293790. Entropy: 0.871612.\n",
      "Iteration 710: Policy loss: -0.011276. Value loss: 0.129255. Entropy: 0.869169.\n",
      "Iteration 711: Policy loss: -0.022398. Value loss: 0.090093. Entropy: 0.861474.\n",
      "episode: 627   score: 6.0  epsilon: 1.0    steps: 200  evaluation reward: 7.44\n",
      "episode: 628   score: 7.0  epsilon: 1.0    steps: 424  evaluation reward: 7.46\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 712: Policy loss: 0.004285. Value loss: 0.341875. Entropy: 0.850068.\n",
      "Iteration 713: Policy loss: -0.013814. Value loss: 0.147483. Entropy: 0.848969.\n",
      "Iteration 714: Policy loss: -0.023491. Value loss: 0.095585. Entropy: 0.840898.\n",
      "episode: 629   score: 4.0  epsilon: 1.0    steps: 760  evaluation reward: 7.41\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 715: Policy loss: 0.002146. Value loss: 0.230856. Entropy: 0.923709.\n",
      "Iteration 716: Policy loss: -0.012919. Value loss: 0.096289. Entropy: 0.923854.\n",
      "Iteration 717: Policy loss: -0.026748. Value loss: 0.062726. Entropy: 0.930784.\n",
      "episode: 630   score: 5.0  epsilon: 1.0    steps: 400  evaluation reward: 7.39\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 718: Policy loss: 0.006516. Value loss: 0.409540. Entropy: 0.921440.\n",
      "Iteration 719: Policy loss: -0.010412. Value loss: 0.171259. Entropy: 0.927600.\n",
      "Iteration 720: Policy loss: -0.019696. Value loss: 0.098829. Entropy: 0.914773.\n",
      "episode: 631   score: 11.0  epsilon: 1.0    steps: 216  evaluation reward: 7.47\n",
      "episode: 632   score: 6.0  epsilon: 1.0    steps: 456  evaluation reward: 7.37\n",
      "episode: 633   score: 11.0  epsilon: 1.0    steps: 512  evaluation reward: 7.33\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 721: Policy loss: 0.003212. Value loss: 0.179931. Entropy: 0.887463.\n",
      "Iteration 722: Policy loss: -0.017656. Value loss: 0.097631. Entropy: 0.907343.\n",
      "Iteration 723: Policy loss: -0.029040. Value loss: 0.069317. Entropy: 0.897108.\n",
      "episode: 634   score: 9.0  epsilon: 1.0    steps: 704  evaluation reward: 7.32\n",
      "episode: 635   score: 8.0  epsilon: 1.0    steps: 832  evaluation reward: 7.33\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 724: Policy loss: 0.014136. Value loss: 0.282374. Entropy: 0.845011.\n",
      "Iteration 725: Policy loss: -0.012157. Value loss: 0.111279. Entropy: 0.827691.\n",
      "Iteration 726: Policy loss: -0.024826. Value loss: 0.070005. Entropy: 0.841466.\n",
      "episode: 636   score: 9.0  epsilon: 1.0    steps: 720  evaluation reward: 7.33\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 727: Policy loss: -0.000059. Value loss: 0.204254. Entropy: 0.851485.\n",
      "Iteration 728: Policy loss: -0.016567. Value loss: 0.082762. Entropy: 0.860322.\n",
      "Iteration 729: Policy loss: -0.027918. Value loss: 0.051633. Entropy: 0.858157.\n",
      "episode: 637   score: 11.0  epsilon: 1.0    steps: 672  evaluation reward: 7.37\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 730: Policy loss: 0.005362. Value loss: 0.407960. Entropy: 0.950710.\n",
      "Iteration 731: Policy loss: -0.006395. Value loss: 0.205050. Entropy: 0.976547.\n",
      "Iteration 732: Policy loss: -0.015097. Value loss: 0.124427. Entropy: 0.951785.\n",
      "episode: 638   score: 6.0  epsilon: 1.0    steps: 144  evaluation reward: 7.37\n",
      "episode: 639   score: 10.0  epsilon: 1.0    steps: 232  evaluation reward: 7.42\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 733: Policy loss: 0.001115. Value loss: 0.410273. Entropy: 0.902216.\n",
      "Iteration 734: Policy loss: -0.011398. Value loss: 0.205347. Entropy: 0.887902.\n",
      "Iteration 735: Policy loss: -0.021797. Value loss: 0.154492. Entropy: 0.895932.\n",
      "episode: 640   score: 5.0  epsilon: 1.0    steps: 56  evaluation reward: 7.4\n",
      "episode: 641   score: 4.0  epsilon: 1.0    steps: 456  evaluation reward: 7.37\n",
      "episode: 642   score: 7.0  epsilon: 1.0    steps: 1024  evaluation reward: 7.29\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 736: Policy loss: 0.007013. Value loss: 0.194524. Entropy: 0.861798.\n",
      "Iteration 737: Policy loss: -0.009945. Value loss: 0.075616. Entropy: 0.853825.\n",
      "Iteration 738: Policy loss: -0.022453. Value loss: 0.051332. Entropy: 0.860997.\n",
      "episode: 643   score: 15.0  epsilon: 1.0    steps: 624  evaluation reward: 7.37\n",
      "episode: 644   score: 14.0  epsilon: 1.0    steps: 1024  evaluation reward: 7.43\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 739: Policy loss: 0.006210. Value loss: 0.249714. Entropy: 0.851693.\n",
      "Iteration 740: Policy loss: -0.008644. Value loss: 0.087226. Entropy: 0.856007.\n",
      "Iteration 741: Policy loss: -0.025053. Value loss: 0.070631. Entropy: 0.860713.\n",
      "episode: 645   score: 6.0  epsilon: 1.0    steps: 360  evaluation reward: 7.45\n",
      "episode: 646   score: 2.0  epsilon: 1.0    steps: 360  evaluation reward: 7.42\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 742: Policy loss: 0.005029. Value loss: 0.239732. Entropy: 0.935419.\n",
      "Iteration 743: Policy loss: -0.013110. Value loss: 0.124906. Entropy: 0.940599.\n",
      "Iteration 744: Policy loss: -0.026675. Value loss: 0.096074. Entropy: 0.925494.\n",
      "episode: 647   score: 3.0  epsilon: 1.0    steps: 856  evaluation reward: 7.34\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 745: Policy loss: 0.004278. Value loss: 0.245914. Entropy: 0.828444.\n",
      "Iteration 746: Policy loss: -0.012920. Value loss: 0.117573. Entropy: 0.829542.\n",
      "Iteration 747: Policy loss: -0.022909. Value loss: 0.071110. Entropy: 0.818658.\n",
      "episode: 648   score: 7.0  epsilon: 1.0    steps: 296  evaluation reward: 7.38\n",
      "episode: 649   score: 10.0  epsilon: 1.0    steps: 744  evaluation reward: 7.43\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 748: Policy loss: 0.003734. Value loss: 0.393300. Entropy: 0.875264.\n",
      "Iteration 749: Policy loss: -0.007422. Value loss: 0.216019. Entropy: 0.900577.\n",
      "Iteration 750: Policy loss: -0.013766. Value loss: 0.141764. Entropy: 0.890374.\n",
      "episode: 650   score: 14.0  epsilon: 1.0    steps: 208  evaluation reward: 7.46\n",
      "now time :  2019-03-06 12:43:49.767141\n",
      "episode: 651   score: 5.0  epsilon: 1.0    steps: 232  evaluation reward: 7.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 652   score: 8.0  epsilon: 1.0    steps: 360  evaluation reward: 7.41\n",
      "episode: 653   score: 5.0  epsilon: 1.0    steps: 480  evaluation reward: 7.37\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 751: Policy loss: 0.009177. Value loss: 0.274535. Entropy: 0.883856.\n",
      "Iteration 752: Policy loss: -0.014975. Value loss: 0.140607. Entropy: 0.889370.\n",
      "Iteration 753: Policy loss: -0.024476. Value loss: 0.101394. Entropy: 0.883403.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 754: Policy loss: 0.007065. Value loss: 0.209752. Entropy: 0.899666.\n",
      "Iteration 755: Policy loss: -0.007187. Value loss: 0.083595. Entropy: 0.896861.\n",
      "Iteration 756: Policy loss: -0.025177. Value loss: 0.058675. Entropy: 0.893979.\n",
      "episode: 654   score: 14.0  epsilon: 1.0    steps: 296  evaluation reward: 7.45\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 757: Policy loss: 0.012556. Value loss: 0.260536. Entropy: 0.916277.\n",
      "Iteration 758: Policy loss: -0.008790. Value loss: 0.117691. Entropy: 0.912596.\n",
      "Iteration 759: Policy loss: -0.022046. Value loss: 0.080338. Entropy: 0.917687.\n",
      "episode: 655   score: 6.0  epsilon: 1.0    steps: 608  evaluation reward: 7.36\n",
      "episode: 656   score: 5.0  epsilon: 1.0    steps: 680  evaluation reward: 7.36\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 760: Policy loss: 0.004224. Value loss: 0.235616. Entropy: 0.883606.\n",
      "Iteration 761: Policy loss: -0.016111. Value loss: 0.107096. Entropy: 0.879946.\n",
      "Iteration 762: Policy loss: -0.030693. Value loss: 0.082129. Entropy: 0.876629.\n",
      "episode: 657   score: 8.0  epsilon: 1.0    steps: 16  evaluation reward: 7.39\n",
      "episode: 658   score: 8.0  epsilon: 1.0    steps: 608  evaluation reward: 7.44\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 763: Policy loss: 0.005569. Value loss: 0.283385. Entropy: 0.859691.\n",
      "Iteration 764: Policy loss: -0.014361. Value loss: 0.112008. Entropy: 0.866920.\n",
      "Iteration 765: Policy loss: -0.028598. Value loss: 0.070291. Entropy: 0.868670.\n",
      "episode: 659   score: 11.0  epsilon: 1.0    steps: 104  evaluation reward: 7.45\n",
      "episode: 660   score: 15.0  epsilon: 1.0    steps: 400  evaluation reward: 7.53\n",
      "episode: 661   score: 11.0  epsilon: 1.0    steps: 664  evaluation reward: 7.61\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 766: Policy loss: 0.010470. Value loss: 0.262614. Entropy: 0.936320.\n",
      "Iteration 767: Policy loss: -0.012109. Value loss: 0.106268. Entropy: 0.942476.\n",
      "Iteration 768: Policy loss: -0.024370. Value loss: 0.073099. Entropy: 0.932713.\n",
      "episode: 662   score: 8.0  epsilon: 1.0    steps: 256  evaluation reward: 7.6\n",
      "episode: 663   score: 5.0  epsilon: 1.0    steps: 712  evaluation reward: 7.6\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 769: Policy loss: 0.009615. Value loss: 0.295070. Entropy: 0.999760.\n",
      "Iteration 770: Policy loss: -0.007290. Value loss: 0.138280. Entropy: 0.988215.\n",
      "Iteration 771: Policy loss: -0.020820. Value loss: 0.092599. Entropy: 0.985907.\n",
      "episode: 664   score: 6.0  epsilon: 1.0    steps: 256  evaluation reward: 7.6\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 772: Policy loss: 0.006646. Value loss: 0.235789. Entropy: 0.909451.\n",
      "Iteration 773: Policy loss: -0.017386. Value loss: 0.105628. Entropy: 0.932380.\n",
      "Iteration 774: Policy loss: -0.024806. Value loss: 0.063513. Entropy: 0.914221.\n",
      "episode: 665   score: 7.0  epsilon: 1.0    steps: 280  evaluation reward: 7.59\n",
      "episode: 666   score: 6.0  epsilon: 1.0    steps: 920  evaluation reward: 7.6\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 775: Policy loss: 0.010554. Value loss: 0.268140. Entropy: 0.913230.\n",
      "Iteration 776: Policy loss: -0.012351. Value loss: 0.121747. Entropy: 0.920765.\n",
      "Iteration 777: Policy loss: -0.022963. Value loss: 0.072857. Entropy: 0.905021.\n",
      "episode: 667   score: 10.0  epsilon: 1.0    steps: 456  evaluation reward: 7.61\n",
      "episode: 668   score: 8.0  epsilon: 1.0    steps: 792  evaluation reward: 7.65\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 778: Policy loss: 0.005922. Value loss: 0.682014. Entropy: 0.897759.\n",
      "Iteration 779: Policy loss: -0.002420. Value loss: 0.462264. Entropy: 0.895782.\n",
      "Iteration 780: Policy loss: -0.013272. Value loss: 0.290737. Entropy: 0.895003.\n",
      "episode: 669   score: 17.0  epsilon: 1.0    steps: 304  evaluation reward: 7.78\n",
      "episode: 670   score: 6.0  epsilon: 1.0    steps: 536  evaluation reward: 7.74\n",
      "episode: 671   score: 7.0  epsilon: 1.0    steps: 776  evaluation reward: 7.73\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 781: Policy loss: 0.005355. Value loss: 0.318176. Entropy: 0.926978.\n",
      "Iteration 782: Policy loss: -0.014480. Value loss: 0.142890. Entropy: 0.895955.\n",
      "Iteration 783: Policy loss: -0.024978. Value loss: 0.081142. Entropy: 0.894731.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 784: Policy loss: 0.010278. Value loss: 0.182683. Entropy: 0.886765.\n",
      "Iteration 785: Policy loss: -0.011294. Value loss: 0.077195. Entropy: 0.879568.\n",
      "Iteration 786: Policy loss: -0.025320. Value loss: 0.051250. Entropy: 0.886319.\n",
      "episode: 672   score: 11.0  epsilon: 1.0    steps: 704  evaluation reward: 7.77\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 787: Policy loss: 0.009600. Value loss: 0.305665. Entropy: 0.944073.\n",
      "Iteration 788: Policy loss: -0.010917. Value loss: 0.127233. Entropy: 0.945139.\n",
      "Iteration 789: Policy loss: -0.020893. Value loss: 0.082217. Entropy: 0.941573.\n",
      "episode: 673   score: 9.0  epsilon: 1.0    steps: 624  evaluation reward: 7.77\n",
      "episode: 674   score: 8.0  epsilon: 1.0    steps: 1000  evaluation reward: 7.8\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 790: Policy loss: 0.009444. Value loss: 0.489363. Entropy: 0.940169.\n",
      "Iteration 791: Policy loss: -0.004953. Value loss: 0.241689. Entropy: 0.949513.\n",
      "Iteration 792: Policy loss: -0.009964. Value loss: 0.153242. Entropy: 0.943183.\n",
      "episode: 675   score: 9.0  epsilon: 1.0    steps: 224  evaluation reward: 7.84\n",
      "episode: 676   score: 6.0  epsilon: 1.0    steps: 264  evaluation reward: 7.86\n",
      "episode: 677   score: 7.0  epsilon: 1.0    steps: 528  evaluation reward: 7.89\n",
      "episode: 678   score: 8.0  epsilon: 1.0    steps: 936  evaluation reward: 7.89\n",
      "episode: 679   score: 20.0  epsilon: 1.0    steps: 1008  evaluation reward: 7.99\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 793: Policy loss: 0.004833. Value loss: 0.366367. Entropy: 0.870342.\n",
      "Iteration 794: Policy loss: -0.016420. Value loss: 0.159835. Entropy: 0.868602.\n",
      "Iteration 795: Policy loss: -0.027939. Value loss: 0.111597. Entropy: 0.871725.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 796: Policy loss: 0.006549. Value loss: 0.301134. Entropy: 0.875650.\n",
      "Iteration 797: Policy loss: -0.013398. Value loss: 0.137026. Entropy: 0.867204.\n",
      "Iteration 798: Policy loss: -0.027865. Value loss: 0.082692. Entropy: 0.865991.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 799: Policy loss: 0.008245. Value loss: 0.229802. Entropy: 0.952324.\n",
      "Iteration 800: Policy loss: -0.004180. Value loss: 0.107895. Entropy: 0.944344.\n",
      "Iteration 801: Policy loss: -0.020795. Value loss: 0.067265. Entropy: 0.935455.\n",
      "episode: 680   score: 5.0  epsilon: 1.0    steps: 368  evaluation reward: 7.97\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 802: Policy loss: 0.005518. Value loss: 0.311210. Entropy: 0.941650.\n",
      "Iteration 803: Policy loss: -0.013673. Value loss: 0.149864. Entropy: 0.941379.\n",
      "Iteration 804: Policy loss: -0.028135. Value loss: 0.093232. Entropy: 0.962465.\n",
      "episode: 681   score: 7.0  epsilon: 1.0    steps: 120  evaluation reward: 7.96\n",
      "episode: 682   score: 6.0  epsilon: 1.0    steps: 152  evaluation reward: 7.96\n",
      "episode: 683   score: 13.0  epsilon: 1.0    steps: 640  evaluation reward: 8.01\n",
      "episode: 684   score: 7.0  epsilon: 1.0    steps: 728  evaluation reward: 8.02\n",
      "episode: 685   score: 8.0  epsilon: 1.0    steps: 928  evaluation reward: 7.99\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 805: Policy loss: 0.006887. Value loss: 0.456631. Entropy: 0.895917.\n",
      "Iteration 806: Policy loss: -0.009735. Value loss: 0.235891. Entropy: 0.892084.\n",
      "Iteration 807: Policy loss: -0.017210. Value loss: 0.134310. Entropy: 0.902828.\n",
      "episode: 686   score: 14.0  epsilon: 1.0    steps: 232  evaluation reward: 8.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 808: Policy loss: 0.004424. Value loss: 0.314796. Entropy: 0.895955.\n",
      "Iteration 809: Policy loss: -0.019663. Value loss: 0.126500. Entropy: 0.880473.\n",
      "Iteration 810: Policy loss: -0.025777. Value loss: 0.080488. Entropy: 0.886988.\n",
      "episode: 687   score: 11.0  epsilon: 1.0    steps: 440  evaluation reward: 8.1\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 811: Policy loss: 0.008154. Value loss: 0.382092. Entropy: 0.979760.\n",
      "Iteration 812: Policy loss: -0.012580. Value loss: 0.158239. Entropy: 0.968898.\n",
      "Iteration 813: Policy loss: -0.023038. Value loss: 0.094272. Entropy: 0.972876.\n",
      "episode: 688   score: 3.0  epsilon: 1.0    steps: 112  evaluation reward: 8.05\n",
      "episode: 689   score: 8.0  epsilon: 1.0    steps: 760  evaluation reward: 8.04\n",
      "episode: 690   score: 5.0  epsilon: 1.0    steps: 816  evaluation reward: 8.01\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 814: Policy loss: 0.008319. Value loss: 0.424509. Entropy: 0.910886.\n",
      "Iteration 815: Policy loss: -0.006122. Value loss: 0.216916. Entropy: 0.912225.\n",
      "Iteration 816: Policy loss: -0.019865. Value loss: 0.131797. Entropy: 0.884906.\n",
      "episode: 691   score: 7.0  epsilon: 1.0    steps: 104  evaluation reward: 8.01\n",
      "episode: 692   score: 5.0  epsilon: 1.0    steps: 312  evaluation reward: 8.04\n",
      "episode: 693   score: 5.0  epsilon: 1.0    steps: 328  evaluation reward: 8.03\n",
      "episode: 694   score: 10.0  epsilon: 1.0    steps: 632  evaluation reward: 8.07\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 817: Policy loss: 0.012815. Value loss: 0.255010. Entropy: 0.867746.\n",
      "Iteration 818: Policy loss: -0.001066. Value loss: 0.091733. Entropy: 0.887848.\n",
      "Iteration 819: Policy loss: -0.014818. Value loss: 0.060717. Entropy: 0.851897.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 820: Policy loss: 0.003358. Value loss: 0.251720. Entropy: 0.926317.\n",
      "Iteration 821: Policy loss: -0.012246. Value loss: 0.109943. Entropy: 0.923342.\n",
      "Iteration 822: Policy loss: -0.026411. Value loss: 0.074432. Entropy: 0.913478.\n",
      "episode: 695   score: 9.0  epsilon: 1.0    steps: 912  evaluation reward: 8.04\n",
      "episode: 696   score: 7.0  epsilon: 1.0    steps: 992  evaluation reward: 8.01\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 823: Policy loss: 0.004684. Value loss: 0.228737. Entropy: 0.941188.\n",
      "Iteration 824: Policy loss: -0.010392. Value loss: 0.097503. Entropy: 0.941778.\n",
      "Iteration 825: Policy loss: -0.026138. Value loss: 0.062430. Entropy: 0.944251.\n",
      "episode: 697   score: 4.0  epsilon: 1.0    steps: 96  evaluation reward: 7.96\n",
      "episode: 698   score: 6.0  epsilon: 1.0    steps: 408  evaluation reward: 7.96\n",
      "episode: 699   score: 7.0  epsilon: 1.0    steps: 456  evaluation reward: 7.89\n",
      "episode: 700   score: 4.0  epsilon: 1.0    steps: 464  evaluation reward: 7.84\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 826: Policy loss: 0.018018. Value loss: 0.305141. Entropy: 0.996271.\n",
      "Iteration 827: Policy loss: -0.006108. Value loss: 0.155246. Entropy: 0.969528.\n",
      "Iteration 828: Policy loss: -0.020776. Value loss: 0.110453. Entropy: 0.963352.\n",
      "now time :  2019-03-06 12:45:31.607673\n",
      "episode: 701   score: 9.0  epsilon: 1.0    steps: 936  evaluation reward: 7.85\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 829: Policy loss: 0.008174. Value loss: 0.258889. Entropy: 0.979770.\n",
      "Iteration 830: Policy loss: -0.012468. Value loss: 0.120987. Entropy: 0.993532.\n",
      "Iteration 831: Policy loss: -0.025162. Value loss: 0.073880. Entropy: 0.979457.\n",
      "episode: 702   score: 3.0  epsilon: 1.0    steps: 48  evaluation reward: 7.79\n",
      "episode: 703   score: 9.0  epsilon: 1.0    steps: 680  evaluation reward: 7.81\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 832: Policy loss: 0.007232. Value loss: 0.132058. Entropy: 0.920966.\n",
      "Iteration 833: Policy loss: -0.016829. Value loss: 0.060991. Entropy: 0.913149.\n",
      "Iteration 834: Policy loss: -0.023760. Value loss: 0.043858. Entropy: 0.921421.\n",
      "episode: 704   score: 4.0  epsilon: 1.0    steps: 56  evaluation reward: 7.8\n",
      "episode: 705   score: 7.0  epsilon: 1.0    steps: 816  evaluation reward: 7.8\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 835: Policy loss: 0.008662. Value loss: 0.210176. Entropy: 0.957869.\n",
      "Iteration 836: Policy loss: -0.014175. Value loss: 0.103165. Entropy: 0.946142.\n",
      "Iteration 837: Policy loss: -0.025608. Value loss: 0.072273. Entropy: 0.944996.\n",
      "episode: 706   score: 7.0  epsilon: 1.0    steps: 376  evaluation reward: 7.77\n",
      "episode: 707   score: 8.0  epsilon: 1.0    steps: 904  evaluation reward: 7.81\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 838: Policy loss: 0.002867. Value loss: 0.189196. Entropy: 0.942488.\n",
      "Iteration 839: Policy loss: -0.011457. Value loss: 0.076451. Entropy: 0.962313.\n",
      "Iteration 840: Policy loss: -0.028861. Value loss: 0.050762. Entropy: 0.948574.\n",
      "episode: 708   score: 10.0  epsilon: 1.0    steps: 224  evaluation reward: 7.88\n",
      "episode: 709   score: 4.0  epsilon: 1.0    steps: 568  evaluation reward: 7.84\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 841: Policy loss: 0.013124. Value loss: 0.227650. Entropy: 0.913314.\n",
      "Iteration 842: Policy loss: -0.012376. Value loss: 0.097052. Entropy: 0.930758.\n",
      "Iteration 843: Policy loss: -0.024911. Value loss: 0.053321. Entropy: 0.914242.\n",
      "episode: 710   score: 9.0  epsilon: 1.0    steps: 912  evaluation reward: 7.83\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 844: Policy loss: 0.005298. Value loss: 0.175909. Entropy: 0.943888.\n",
      "Iteration 845: Policy loss: -0.015714. Value loss: 0.074416. Entropy: 0.947562.\n",
      "Iteration 846: Policy loss: -0.027203. Value loss: 0.050203. Entropy: 0.950893.\n",
      "episode: 711   score: 7.0  epsilon: 1.0    steps: 328  evaluation reward: 7.81\n",
      "episode: 712   score: 11.0  epsilon: 1.0    steps: 512  evaluation reward: 7.86\n",
      "episode: 713   score: 12.0  epsilon: 1.0    steps: 880  evaluation reward: 7.93\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 847: Policy loss: 0.004928. Value loss: 0.414922. Entropy: 0.966658.\n",
      "Iteration 848: Policy loss: 0.003759. Value loss: 0.234847. Entropy: 0.973934.\n",
      "Iteration 849: Policy loss: -0.011319. Value loss: 0.178983. Entropy: 0.964449.\n",
      "episode: 714   score: 6.0  epsilon: 1.0    steps: 624  evaluation reward: 7.92\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 850: Policy loss: 0.009395. Value loss: 0.185953. Entropy: 0.944457.\n",
      "Iteration 851: Policy loss: -0.012542. Value loss: 0.093744. Entropy: 0.955774.\n",
      "Iteration 852: Policy loss: -0.027022. Value loss: 0.062670. Entropy: 0.949506.\n",
      "episode: 715   score: 8.0  epsilon: 1.0    steps: 128  evaluation reward: 7.97\n",
      "episode: 716   score: 7.0  epsilon: 1.0    steps: 600  evaluation reward: 7.98\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 853: Policy loss: 0.005719. Value loss: 0.300367. Entropy: 0.912505.\n",
      "Iteration 854: Policy loss: -0.016607. Value loss: 0.133731. Entropy: 0.894878.\n",
      "Iteration 855: Policy loss: -0.027717. Value loss: 0.087059. Entropy: 0.901210.\n",
      "episode: 717   score: 9.0  epsilon: 1.0    steps: 464  evaluation reward: 8.03\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 856: Policy loss: 0.004831. Value loss: 0.243303. Entropy: 0.886721.\n",
      "Iteration 857: Policy loss: -0.016155. Value loss: 0.106917. Entropy: 0.895402.\n",
      "Iteration 858: Policy loss: -0.028428. Value loss: 0.067251. Entropy: 0.881772.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 859: Policy loss: 0.005726. Value loss: 0.195493. Entropy: 0.921990.\n",
      "Iteration 860: Policy loss: -0.011639. Value loss: 0.088591. Entropy: 0.910311.\n",
      "Iteration 861: Policy loss: -0.023146. Value loss: 0.058179. Entropy: 0.916722.\n",
      "episode: 718   score: 8.0  epsilon: 1.0    steps: 56  evaluation reward: 8.03\n",
      "episode: 719   score: 9.0  epsilon: 1.0    steps: 112  evaluation reward: 8.02\n",
      "episode: 720   score: 10.0  epsilon: 1.0    steps: 128  evaluation reward: 8.02\n",
      "episode: 721   score: 10.0  epsilon: 1.0    steps: 648  evaluation reward: 8.05\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 862: Policy loss: 0.006425. Value loss: 0.727896. Entropy: 0.930843.\n",
      "Iteration 863: Policy loss: -0.000769. Value loss: 0.438402. Entropy: 0.934652.\n",
      "Iteration 864: Policy loss: -0.011404. Value loss: 0.299957. Entropy: 0.925673.\n",
      "episode: 722   score: 5.0  epsilon: 1.0    steps: 16  evaluation reward: 8.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 723   score: 15.0  epsilon: 1.0    steps: 280  evaluation reward: 8.06\n",
      "episode: 724   score: 9.0  epsilon: 1.0    steps: 576  evaluation reward: 8.12\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 865: Policy loss: 0.010822. Value loss: 0.408522. Entropy: 0.946239.\n",
      "Iteration 866: Policy loss: -0.009088. Value loss: 0.248609. Entropy: 0.962762.\n",
      "Iteration 867: Policy loss: -0.021831. Value loss: 0.181634. Entropy: 0.956745.\n",
      "episode: 725   score: 9.0  epsilon: 1.0    steps: 432  evaluation reward: 8.08\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 868: Policy loss: 0.010106. Value loss: 0.355103. Entropy: 0.911203.\n",
      "Iteration 869: Policy loss: -0.006777. Value loss: 0.139914. Entropy: 0.930746.\n",
      "Iteration 870: Policy loss: -0.020767. Value loss: 0.091713. Entropy: 0.914166.\n",
      "episode: 726   score: 4.0  epsilon: 1.0    steps: 184  evaluation reward: 8.04\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 871: Policy loss: 0.008415. Value loss: 0.312662. Entropy: 0.846220.\n",
      "Iteration 872: Policy loss: -0.015226. Value loss: 0.139722. Entropy: 0.823225.\n",
      "Iteration 873: Policy loss: -0.025272. Value loss: 0.079141. Entropy: 0.814667.\n",
      "episode: 727   score: 7.0  epsilon: 1.0    steps: 608  evaluation reward: 8.05\n",
      "episode: 728   score: 9.0  epsilon: 1.0    steps: 880  evaluation reward: 8.07\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 874: Policy loss: 0.005745. Value loss: 0.253666. Entropy: 0.887398.\n",
      "Iteration 875: Policy loss: -0.013357. Value loss: 0.101323. Entropy: 0.898575.\n",
      "Iteration 876: Policy loss: -0.027018. Value loss: 0.056063. Entropy: 0.893382.\n",
      "episode: 729   score: 7.0  epsilon: 1.0    steps: 72  evaluation reward: 8.1\n",
      "episode: 730   score: 9.0  epsilon: 1.0    steps: 352  evaluation reward: 8.14\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 877: Policy loss: 0.003568. Value loss: 0.254186. Entropy: 0.930406.\n",
      "Iteration 878: Policy loss: -0.012193. Value loss: 0.104470. Entropy: 0.929984.\n",
      "Iteration 879: Policy loss: -0.026103. Value loss: 0.071460. Entropy: 0.937894.\n",
      "episode: 731   score: 9.0  epsilon: 1.0    steps: 144  evaluation reward: 8.12\n",
      "episode: 732   score: 8.0  epsilon: 1.0    steps: 576  evaluation reward: 8.14\n",
      "episode: 733   score: 11.0  epsilon: 1.0    steps: 728  evaluation reward: 8.14\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 880: Policy loss: 0.003832. Value loss: 0.244684. Entropy: 0.879092.\n",
      "Iteration 881: Policy loss: -0.017964. Value loss: 0.094370. Entropy: 0.886959.\n",
      "Iteration 882: Policy loss: -0.029910. Value loss: 0.057257. Entropy: 0.874362.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 883: Policy loss: 0.007587. Value loss: 0.190682. Entropy: 0.820879.\n",
      "Iteration 884: Policy loss: -0.013565. Value loss: 0.088976. Entropy: 0.816821.\n",
      "Iteration 885: Policy loss: -0.022068. Value loss: 0.059139. Entropy: 0.813996.\n",
      "episode: 734   score: 11.0  epsilon: 1.0    steps: 984  evaluation reward: 8.16\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 886: Policy loss: 0.013659. Value loss: 0.263142. Entropy: 0.856718.\n",
      "Iteration 887: Policy loss: -0.008437. Value loss: 0.115704. Entropy: 0.869186.\n",
      "Iteration 888: Policy loss: -0.027716. Value loss: 0.080225. Entropy: 0.869404.\n",
      "episode: 735   score: 9.0  epsilon: 1.0    steps: 552  evaluation reward: 8.17\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 889: Policy loss: 0.003465. Value loss: 0.244348. Entropy: 0.887928.\n",
      "Iteration 890: Policy loss: -0.017563. Value loss: 0.095570. Entropy: 0.902138.\n",
      "Iteration 891: Policy loss: -0.025983. Value loss: 0.057948. Entropy: 0.896350.\n",
      "episode: 736   score: 9.0  epsilon: 1.0    steps: 96  evaluation reward: 8.17\n",
      "episode: 737   score: 11.0  epsilon: 1.0    steps: 256  evaluation reward: 8.17\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 892: Policy loss: 0.013513. Value loss: 0.264815. Entropy: 0.843228.\n",
      "Iteration 893: Policy loss: -0.011322. Value loss: 0.133945. Entropy: 0.823709.\n",
      "Iteration 894: Policy loss: -0.026405. Value loss: 0.089332. Entropy: 0.823603.\n",
      "episode: 738   score: 8.0  epsilon: 1.0    steps: 424  evaluation reward: 8.19\n",
      "episode: 739   score: 13.0  epsilon: 1.0    steps: 512  evaluation reward: 8.22\n",
      "episode: 740   score: 13.0  epsilon: 1.0    steps: 768  evaluation reward: 8.3\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 895: Policy loss: 0.007292. Value loss: 0.395454. Entropy: 0.969205.\n",
      "Iteration 896: Policy loss: -0.004470. Value loss: 0.197970. Entropy: 0.941340.\n",
      "Iteration 897: Policy loss: -0.017659. Value loss: 0.140244. Entropy: 0.940827.\n",
      "episode: 741   score: 11.0  epsilon: 1.0    steps: 32  evaluation reward: 8.37\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 898: Policy loss: 0.011483. Value loss: 0.262703. Entropy: 0.780881.\n",
      "Iteration 899: Policy loss: -0.010657. Value loss: 0.117792. Entropy: 0.793485.\n",
      "Iteration 900: Policy loss: -0.012808. Value loss: 0.103792. Entropy: 0.792422.\n",
      "episode: 742   score: 9.0  epsilon: 1.0    steps: 696  evaluation reward: 8.39\n",
      "episode: 743   score: 6.0  epsilon: 1.0    steps: 904  evaluation reward: 8.3\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 901: Policy loss: 0.011668. Value loss: 0.251096. Entropy: 0.827012.\n",
      "Iteration 902: Policy loss: -0.008954. Value loss: 0.118278. Entropy: 0.839368.\n",
      "Iteration 903: Policy loss: -0.026562. Value loss: 0.070581. Entropy: 0.835135.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 904: Policy loss: 0.002890. Value loss: 0.226242. Entropy: 0.874488.\n",
      "Iteration 905: Policy loss: -0.013131. Value loss: 0.123546. Entropy: 0.884952.\n",
      "Iteration 906: Policy loss: -0.026496. Value loss: 0.087019. Entropy: 0.882270.\n",
      "episode: 744   score: 11.0  epsilon: 1.0    steps: 704  evaluation reward: 8.27\n",
      "episode: 745   score: 15.0  epsilon: 1.0    steps: 800  evaluation reward: 8.36\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 907: Policy loss: 0.006379. Value loss: 0.499746. Entropy: 0.798685.\n",
      "Iteration 908: Policy loss: -0.003282. Value loss: 0.233925. Entropy: 0.793855.\n",
      "Iteration 909: Policy loss: -0.013786. Value loss: 0.128630. Entropy: 0.787561.\n",
      "episode: 746   score: 9.0  epsilon: 1.0    steps: 192  evaluation reward: 8.43\n",
      "episode: 747   score: 8.0  epsilon: 1.0    steps: 352  evaluation reward: 8.48\n",
      "episode: 748   score: 7.0  epsilon: 1.0    steps: 520  evaluation reward: 8.48\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 910: Policy loss: 0.006192. Value loss: 0.626184. Entropy: 0.873786.\n",
      "Iteration 911: Policy loss: -0.011348. Value loss: 0.338516. Entropy: 0.873035.\n",
      "Iteration 912: Policy loss: -0.023048. Value loss: 0.192530. Entropy: 0.869816.\n",
      "episode: 749   score: 15.0  epsilon: 1.0    steps: 440  evaluation reward: 8.53\n",
      "episode: 750   score: 3.0  epsilon: 1.0    steps: 1016  evaluation reward: 8.42\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 913: Policy loss: 0.013211. Value loss: 0.408516. Entropy: 0.884099.\n",
      "Iteration 914: Policy loss: -0.007823. Value loss: 0.149162. Entropy: 0.865537.\n",
      "Iteration 915: Policy loss: -0.024042. Value loss: 0.094429. Entropy: 0.876614.\n",
      "now time :  2019-03-06 12:47:24.723485\n",
      "episode: 751   score: 13.0  epsilon: 1.0    steps: 912  evaluation reward: 8.5\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 916: Policy loss: 0.002185. Value loss: 0.382446. Entropy: 0.786440.\n",
      "Iteration 917: Policy loss: -0.014478. Value loss: 0.154354. Entropy: 0.797100.\n",
      "Iteration 918: Policy loss: -0.023523. Value loss: 0.097827. Entropy: 0.801565.\n",
      "episode: 752   score: 6.0  epsilon: 1.0    steps: 824  evaluation reward: 8.48\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 919: Policy loss: 0.008454. Value loss: 0.302890. Entropy: 0.880437.\n",
      "Iteration 920: Policy loss: -0.014137. Value loss: 0.116369. Entropy: 0.870688.\n",
      "Iteration 921: Policy loss: -0.020805. Value loss: 0.064467. Entropy: 0.875611.\n",
      "episode: 753   score: 6.0  epsilon: 1.0    steps: 128  evaluation reward: 8.49\n",
      "episode: 754   score: 12.0  epsilon: 1.0    steps: 264  evaluation reward: 8.47\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 922: Policy loss: 0.006825. Value loss: 0.330031. Entropy: 0.867598.\n",
      "Iteration 923: Policy loss: -0.013143. Value loss: 0.161183. Entropy: 0.851276.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 924: Policy loss: -0.026774. Value loss: 0.102142. Entropy: 0.857549.\n",
      "episode: 755   score: 12.0  epsilon: 1.0    steps: 456  evaluation reward: 8.53\n",
      "episode: 756   score: 7.0  epsilon: 1.0    steps: 696  evaluation reward: 8.55\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 925: Policy loss: 0.006055. Value loss: 0.233318. Entropy: 0.851122.\n",
      "Iteration 926: Policy loss: -0.017909. Value loss: 0.108400. Entropy: 0.858987.\n",
      "Iteration 927: Policy loss: -0.027862. Value loss: 0.057239. Entropy: 0.858833.\n",
      "episode: 757   score: 8.0  epsilon: 1.0    steps: 488  evaluation reward: 8.55\n",
      "episode: 758   score: 11.0  epsilon: 1.0    steps: 544  evaluation reward: 8.58\n",
      "episode: 759   score: 7.0  epsilon: 1.0    steps: 1000  evaluation reward: 8.54\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 928: Policy loss: 0.013225. Value loss: 0.290612. Entropy: 0.869187.\n",
      "Iteration 929: Policy loss: -0.011103. Value loss: 0.124348. Entropy: 0.880308.\n",
      "Iteration 930: Policy loss: -0.020180. Value loss: 0.090545. Entropy: 0.875690.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 931: Policy loss: 0.008250. Value loss: 0.220805. Entropy: 0.949524.\n",
      "Iteration 932: Policy loss: -0.007809. Value loss: 0.088124. Entropy: 0.948959.\n",
      "Iteration 933: Policy loss: -0.023163. Value loss: 0.058415. Entropy: 0.945576.\n",
      "episode: 760   score: 9.0  epsilon: 1.0    steps: 728  evaluation reward: 8.48\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 934: Policy loss: 0.008686. Value loss: 0.265701. Entropy: 0.895049.\n",
      "Iteration 935: Policy loss: -0.011238. Value loss: 0.119743. Entropy: 0.888893.\n",
      "Iteration 936: Policy loss: -0.021686. Value loss: 0.082502. Entropy: 0.909413.\n",
      "episode: 761   score: 9.0  epsilon: 1.0    steps: 8  evaluation reward: 8.46\n",
      "episode: 762   score: 6.0  epsilon: 1.0    steps: 16  evaluation reward: 8.44\n",
      "episode: 763   score: 9.0  epsilon: 1.0    steps: 80  evaluation reward: 8.48\n",
      "episode: 764   score: 7.0  epsilon: 1.0    steps: 360  evaluation reward: 8.49\n",
      "episode: 765   score: 5.0  epsilon: 1.0    steps: 504  evaluation reward: 8.47\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 937: Policy loss: 0.004992. Value loss: 0.230666. Entropy: 0.880943.\n",
      "Iteration 938: Policy loss: -0.015194. Value loss: 0.146538. Entropy: 0.885847.\n",
      "Iteration 939: Policy loss: -0.022469. Value loss: 0.116231. Entropy: 0.887999.\n",
      "episode: 766   score: 7.0  epsilon: 1.0    steps: 392  evaluation reward: 8.48\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 940: Policy loss: 0.007419. Value loss: 0.172298. Entropy: 0.925220.\n",
      "Iteration 941: Policy loss: -0.015881. Value loss: 0.076612. Entropy: 0.943955.\n",
      "Iteration 942: Policy loss: -0.028211. Value loss: 0.057181. Entropy: 0.937767.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 943: Policy loss: 0.005765. Value loss: 0.197059. Entropy: 0.837074.\n",
      "Iteration 944: Policy loss: -0.018980. Value loss: 0.065889. Entropy: 0.830655.\n",
      "Iteration 945: Policy loss: -0.026149. Value loss: 0.035792. Entropy: 0.826909.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 946: Policy loss: 0.003597. Value loss: 0.222406. Entropy: 0.927395.\n",
      "Iteration 947: Policy loss: -0.011866. Value loss: 0.094276. Entropy: 0.927329.\n",
      "Iteration 948: Policy loss: -0.029451. Value loss: 0.056203. Entropy: 0.918046.\n",
      "episode: 767   score: 7.0  epsilon: 1.0    steps: 120  evaluation reward: 8.45\n",
      "episode: 768   score: 11.0  epsilon: 1.0    steps: 144  evaluation reward: 8.48\n",
      "episode: 769   score: 8.0  epsilon: 1.0    steps: 160  evaluation reward: 8.39\n",
      "episode: 770   score: 8.0  epsilon: 1.0    steps: 384  evaluation reward: 8.41\n",
      "episode: 771   score: 9.0  epsilon: 1.0    steps: 616  evaluation reward: 8.43\n",
      "episode: 772   score: 12.0  epsilon: 1.0    steps: 888  evaluation reward: 8.44\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 949: Policy loss: 0.007520. Value loss: 0.486850. Entropy: 0.944179.\n",
      "Iteration 950: Policy loss: 0.000522. Value loss: 0.341238. Entropy: 0.961406.\n",
      "Iteration 951: Policy loss: -0.017614. Value loss: 0.247550. Entropy: 0.958498.\n",
      "episode: 773   score: 9.0  epsilon: 1.0    steps: 352  evaluation reward: 8.44\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 952: Policy loss: 0.005444. Value loss: 0.206930. Entropy: 0.944241.\n",
      "Iteration 953: Policy loss: -0.012703. Value loss: 0.103970. Entropy: 0.943528.\n",
      "Iteration 954: Policy loss: -0.023274. Value loss: 0.069881. Entropy: 0.938285.\n",
      "episode: 774   score: 11.0  epsilon: 1.0    steps: 104  evaluation reward: 8.47\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 955: Policy loss: 0.010365. Value loss: 0.225652. Entropy: 0.912915.\n",
      "Iteration 956: Policy loss: -0.014031. Value loss: 0.102430. Entropy: 0.921816.\n",
      "Iteration 957: Policy loss: -0.024421. Value loss: 0.064147. Entropy: 0.917540.\n",
      "episode: 775   score: 5.0  epsilon: 1.0    steps: 704  evaluation reward: 8.43\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 958: Policy loss: 0.006984. Value loss: 0.246058. Entropy: 0.919939.\n",
      "Iteration 959: Policy loss: -0.011644. Value loss: 0.091740. Entropy: 0.931215.\n",
      "Iteration 960: Policy loss: -0.028636. Value loss: 0.050658. Entropy: 0.933669.\n",
      "episode: 776   score: 8.0  epsilon: 1.0    steps: 400  evaluation reward: 8.45\n",
      "episode: 777   score: 9.0  epsilon: 1.0    steps: 920  evaluation reward: 8.47\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 961: Policy loss: 0.004562. Value loss: 0.364534. Entropy: 0.983841.\n",
      "Iteration 962: Policy loss: -0.005964. Value loss: 0.167984. Entropy: 0.980919.\n",
      "Iteration 963: Policy loss: -0.017489. Value loss: 0.117027. Entropy: 0.970618.\n",
      "episode: 778   score: 9.0  epsilon: 1.0    steps: 240  evaluation reward: 8.48\n",
      "episode: 779   score: 9.0  epsilon: 1.0    steps: 616  evaluation reward: 8.37\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 964: Policy loss: 0.008165. Value loss: 0.255829. Entropy: 0.937133.\n",
      "Iteration 965: Policy loss: -0.017864. Value loss: 0.093105. Entropy: 0.927018.\n",
      "Iteration 966: Policy loss: -0.029149. Value loss: 0.053907. Entropy: 0.932786.\n",
      "episode: 780   score: 11.0  epsilon: 1.0    steps: 760  evaluation reward: 8.43\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 967: Policy loss: 0.004641. Value loss: 0.180035. Entropy: 0.807595.\n",
      "Iteration 968: Policy loss: -0.014414. Value loss: 0.066164. Entropy: 0.828518.\n",
      "Iteration 969: Policy loss: -0.027189. Value loss: 0.035830. Entropy: 0.832222.\n",
      "episode: 781   score: 17.0  epsilon: 1.0    steps: 232  evaluation reward: 8.53\n",
      "episode: 782   score: 11.0  epsilon: 1.0    steps: 440  evaluation reward: 8.58\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 970: Policy loss: 0.013716. Value loss: 0.208752. Entropy: 0.925971.\n",
      "Iteration 971: Policy loss: -0.018254. Value loss: 0.095957. Entropy: 0.919006.\n",
      "Iteration 972: Policy loss: -0.030364. Value loss: 0.060481. Entropy: 0.912068.\n",
      "episode: 783   score: 8.0  epsilon: 1.0    steps: 944  evaluation reward: 8.53\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 973: Policy loss: 0.003202. Value loss: 0.432521. Entropy: 0.908612.\n",
      "Iteration 974: Policy loss: -0.012198. Value loss: 0.203091. Entropy: 0.940812.\n",
      "Iteration 975: Policy loss: -0.025123. Value loss: 0.129104. Entropy: 0.913037.\n",
      "episode: 784   score: 15.0  epsilon: 1.0    steps: 208  evaluation reward: 8.61\n",
      "episode: 785   score: 8.0  epsilon: 1.0    steps: 544  evaluation reward: 8.61\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 976: Policy loss: 0.009368. Value loss: 0.255472. Entropy: 0.949704.\n",
      "Iteration 977: Policy loss: -0.012050. Value loss: 0.101836. Entropy: 0.945896.\n",
      "Iteration 978: Policy loss: -0.026379. Value loss: 0.072259. Entropy: 0.941596.\n",
      "episode: 786   score: 9.0  epsilon: 1.0    steps: 208  evaluation reward: 8.56\n",
      "episode: 787   score: 10.0  epsilon: 1.0    steps: 920  evaluation reward: 8.55\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 979: Policy loss: 0.010642. Value loss: 0.205002. Entropy: 0.856504.\n",
      "Iteration 980: Policy loss: -0.011718. Value loss: 0.074323. Entropy: 0.856485.\n",
      "Iteration 981: Policy loss: -0.024255. Value loss: 0.054923. Entropy: 0.869449.\n",
      "episode: 788   score: 8.0  epsilon: 1.0    steps: 808  evaluation reward: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 982: Policy loss: 0.004105. Value loss: 0.324327. Entropy: 0.978900.\n",
      "Iteration 983: Policy loss: -0.011882. Value loss: 0.123388. Entropy: 0.975132.\n",
      "Iteration 984: Policy loss: -0.027810. Value loss: 0.071406. Entropy: 0.980858.\n",
      "episode: 789   score: 8.0  epsilon: 1.0    steps: 896  evaluation reward: 8.6\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 985: Policy loss: 0.006139. Value loss: 0.365846. Entropy: 1.005429.\n",
      "Iteration 986: Policy loss: -0.011320. Value loss: 0.188579. Entropy: 1.005322.\n",
      "Iteration 987: Policy loss: -0.022325. Value loss: 0.119578. Entropy: 1.008986.\n",
      "episode: 790   score: 15.0  epsilon: 1.0    steps: 88  evaluation reward: 8.7\n",
      "episode: 791   score: 17.0  epsilon: 1.0    steps: 648  evaluation reward: 8.8\n",
      "episode: 792   score: 10.0  epsilon: 1.0    steps: 872  evaluation reward: 8.85\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 988: Policy loss: 0.009538. Value loss: 0.343763. Entropy: 0.883886.\n",
      "Iteration 989: Policy loss: -0.004311. Value loss: 0.118970. Entropy: 0.887477.\n",
      "Iteration 990: Policy loss: -0.010569. Value loss: 0.086523. Entropy: 0.896898.\n",
      "episode: 793   score: 12.0  epsilon: 1.0    steps: 864  evaluation reward: 8.92\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 991: Policy loss: 0.010995. Value loss: 0.374211. Entropy: 0.974509.\n",
      "Iteration 992: Policy loss: -0.008727. Value loss: 0.191701. Entropy: 0.964004.\n",
      "Iteration 993: Policy loss: -0.021750. Value loss: 0.119826. Entropy: 0.957169.\n",
      "episode: 794   score: 10.0  epsilon: 1.0    steps: 112  evaluation reward: 8.92\n",
      "episode: 795   score: 8.0  epsilon: 1.0    steps: 552  evaluation reward: 8.91\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 994: Policy loss: 0.009900. Value loss: 0.465352. Entropy: 0.964686.\n",
      "Iteration 995: Policy loss: -0.005776. Value loss: 0.225790. Entropy: 0.967247.\n",
      "Iteration 996: Policy loss: -0.016788. Value loss: 0.153877. Entropy: 0.968922.\n",
      "episode: 796   score: 8.0  epsilon: 1.0    steps: 48  evaluation reward: 8.92\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 997: Policy loss: 0.004873. Value loss: 0.423829. Entropy: 0.934807.\n",
      "Iteration 998: Policy loss: -0.009113. Value loss: 0.206091. Entropy: 0.934141.\n",
      "Iteration 999: Policy loss: -0.019153. Value loss: 0.098596. Entropy: 0.932798.\n",
      "episode: 797   score: 9.0  epsilon: 1.0    steps: 1008  evaluation reward: 8.97\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 1000: Policy loss: -0.000181. Value loss: 0.321223. Entropy: 0.982118.\n",
      "Iteration 1001: Policy loss: -0.015248. Value loss: 0.152964. Entropy: 0.975725.\n",
      "Iteration 1002: Policy loss: -0.027692. Value loss: 0.093381. Entropy: 0.973816.\n",
      "episode: 798   score: 9.0  epsilon: 1.0    steps: 544  evaluation reward: 9.0\n",
      "episode: 799   score: 14.0  epsilon: 1.0    steps: 936  evaluation reward: 9.07\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1003: Policy loss: 0.009068. Value loss: 0.233185. Entropy: 0.967383.\n",
      "Iteration 1004: Policy loss: -0.009932. Value loss: 0.112098. Entropy: 0.966337.\n",
      "Iteration 1005: Policy loss: -0.023765. Value loss: 0.071664. Entropy: 0.966521.\n",
      "episode: 800   score: 6.0  epsilon: 1.0    steps: 48  evaluation reward: 9.09\n",
      "now time :  2019-03-06 12:49:18.621973\n",
      "episode: 801   score: 13.0  epsilon: 1.0    steps: 328  evaluation reward: 9.13\n",
      "episode: 802   score: 7.0  epsilon: 1.0    steps: 904  evaluation reward: 9.17\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1006: Policy loss: 0.003102. Value loss: 0.295422. Entropy: 0.967077.\n",
      "Iteration 1007: Policy loss: -0.015256. Value loss: 0.132629. Entropy: 0.965478.\n",
      "Iteration 1008: Policy loss: -0.026773. Value loss: 0.093574. Entropy: 0.962323.\n",
      "episode: 803   score: 10.0  epsilon: 1.0    steps: 224  evaluation reward: 9.18\n",
      "episode: 804   score: 11.0  epsilon: 1.0    steps: 544  evaluation reward: 9.25\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1009: Policy loss: 0.000995. Value loss: 0.252029. Entropy: 0.944179.\n",
      "Iteration 1010: Policy loss: -0.018640. Value loss: 0.086101. Entropy: 0.953332.\n",
      "Iteration 1011: Policy loss: -0.027917. Value loss: 0.060499. Entropy: 0.944694.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1012: Policy loss: 0.004817. Value loss: 0.184492. Entropy: 0.867778.\n",
      "Iteration 1013: Policy loss: -0.011620. Value loss: 0.083970. Entropy: 0.859799.\n",
      "Iteration 1014: Policy loss: -0.025258. Value loss: 0.057927. Entropy: 0.865645.\n",
      "episode: 805   score: 6.0  epsilon: 1.0    steps: 456  evaluation reward: 9.24\n",
      "episode: 806   score: 7.0  epsilon: 1.0    steps: 768  evaluation reward: 9.24\n",
      "episode: 807   score: 6.0  epsilon: 1.0    steps: 816  evaluation reward: 9.22\n",
      "episode: 808   score: 5.0  epsilon: 1.0    steps: 880  evaluation reward: 9.17\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1015: Policy loss: 0.010936. Value loss: 0.352411. Entropy: 0.935066.\n",
      "Iteration 1016: Policy loss: -0.006333. Value loss: 0.118439. Entropy: 0.952259.\n",
      "Iteration 1017: Policy loss: -0.020507. Value loss: 0.072738. Entropy: 0.955706.\n",
      "episode: 809   score: 13.0  epsilon: 1.0    steps: 624  evaluation reward: 9.26\n",
      "episode: 810   score: 14.0  epsilon: 1.0    steps: 984  evaluation reward: 9.31\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1018: Policy loss: 0.011199. Value loss: 0.485463. Entropy: 0.951393.\n",
      "Iteration 1019: Policy loss: -0.001183. Value loss: 0.221434. Entropy: 0.958032.\n",
      "Iteration 1020: Policy loss: -0.011272. Value loss: 0.134845. Entropy: 0.952130.\n",
      "episode: 811   score: 8.0  epsilon: 1.0    steps: 488  evaluation reward: 9.32\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1021: Policy loss: 0.009472. Value loss: 0.330531. Entropy: 0.907098.\n",
      "Iteration 1022: Policy loss: -0.008558. Value loss: 0.159542. Entropy: 0.911212.\n",
      "Iteration 1023: Policy loss: -0.020672. Value loss: 0.109339. Entropy: 0.899945.\n",
      "episode: 812   score: 4.0  epsilon: 1.0    steps: 792  evaluation reward: 9.25\n",
      "episode: 813   score: 13.0  epsilon: 1.0    steps: 936  evaluation reward: 9.26\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1024: Policy loss: 0.008002. Value loss: 0.417331. Entropy: 0.819769.\n",
      "Iteration 1025: Policy loss: -0.013268. Value loss: 0.168834. Entropy: 0.812936.\n",
      "Iteration 1026: Policy loss: -0.023225. Value loss: 0.106824. Entropy: 0.808911.\n",
      "episode: 814   score: 7.0  epsilon: 1.0    steps: 648  evaluation reward: 9.27\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1027: Policy loss: 0.003525. Value loss: 0.256584. Entropy: 0.899632.\n",
      "Iteration 1028: Policy loss: -0.013212. Value loss: 0.104318. Entropy: 0.897589.\n",
      "Iteration 1029: Policy loss: -0.023482. Value loss: 0.059954. Entropy: 0.898425.\n",
      "episode: 815   score: 9.0  epsilon: 1.0    steps: 496  evaluation reward: 9.28\n",
      "episode: 816   score: 7.0  epsilon: 1.0    steps: 952  evaluation reward: 9.28\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1030: Policy loss: 0.003476. Value loss: 0.383524. Entropy: 0.867072.\n",
      "Iteration 1031: Policy loss: -0.007894. Value loss: 0.178747. Entropy: 0.859122.\n",
      "Iteration 1032: Policy loss: -0.015733. Value loss: 0.111638. Entropy: 0.855275.\n",
      "episode: 817   score: 7.0  epsilon: 1.0    steps: 368  evaluation reward: 9.26\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1033: Policy loss: 0.012261. Value loss: 0.262710. Entropy: 0.856038.\n",
      "Iteration 1034: Policy loss: -0.010832. Value loss: 0.098550. Entropy: 0.849568.\n",
      "Iteration 1035: Policy loss: -0.024600. Value loss: 0.058409. Entropy: 0.853612.\n",
      "episode: 818   score: 19.0  epsilon: 1.0    steps: 288  evaluation reward: 9.37\n",
      "episode: 819   score: 11.0  epsilon: 1.0    steps: 464  evaluation reward: 9.39\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1036: Policy loss: 0.009520. Value loss: 0.187308. Entropy: 0.890281.\n",
      "Iteration 1037: Policy loss: -0.010915. Value loss: 0.089039. Entropy: 0.907956.\n",
      "Iteration 1038: Policy loss: -0.028692. Value loss: 0.060477. Entropy: 0.906217.\n",
      "episode: 820   score: 8.0  epsilon: 1.0    steps: 448  evaluation reward: 9.37\n",
      "episode: 821   score: 8.0  epsilon: 1.0    steps: 960  evaluation reward: 9.35\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1039: Policy loss: 0.001402. Value loss: 0.222923. Entropy: 0.873870.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1040: Policy loss: -0.013032. Value loss: 0.075764. Entropy: 0.875184.\n",
      "Iteration 1041: Policy loss: -0.024277. Value loss: 0.049007. Entropy: 0.884646.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1042: Policy loss: 0.008416. Value loss: 0.671047. Entropy: 0.931283.\n",
      "Iteration 1043: Policy loss: 0.000787. Value loss: 0.322875. Entropy: 0.921815.\n",
      "Iteration 1044: Policy loss: -0.012139. Value loss: 0.228717. Entropy: 0.929457.\n",
      "episode: 822   score: 14.0  epsilon: 1.0    steps: 304  evaluation reward: 9.44\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1045: Policy loss: 0.006323. Value loss: 0.498190. Entropy: 0.938171.\n",
      "Iteration 1046: Policy loss: -0.007857. Value loss: 0.201767. Entropy: 0.936110.\n",
      "Iteration 1047: Policy loss: -0.019002. Value loss: 0.113329. Entropy: 0.920014.\n",
      "episode: 823   score: 16.0  epsilon: 1.0    steps: 424  evaluation reward: 9.45\n",
      "episode: 824   score: 10.0  epsilon: 1.0    steps: 576  evaluation reward: 9.46\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1048: Policy loss: 0.014294. Value loss: 0.780504. Entropy: 0.889183.\n",
      "Iteration 1049: Policy loss: -0.001707. Value loss: 0.331080. Entropy: 0.874763.\n",
      "Iteration 1050: Policy loss: -0.009852. Value loss: 0.233179. Entropy: 0.884793.\n",
      "episode: 825   score: 20.0  epsilon: 1.0    steps: 32  evaluation reward: 9.57\n",
      "episode: 826   score: 10.0  epsilon: 1.0    steps: 384  evaluation reward: 9.63\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1051: Policy loss: 0.011603. Value loss: 0.389405. Entropy: 0.893689.\n",
      "Iteration 1052: Policy loss: -0.007919. Value loss: 0.176255. Entropy: 0.894625.\n",
      "Iteration 1053: Policy loss: -0.020545. Value loss: 0.117030. Entropy: 0.883026.\n",
      "episode: 827   score: 14.0  epsilon: 1.0    steps: 1000  evaluation reward: 9.7\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1054: Policy loss: 0.004936. Value loss: 0.364214. Entropy: 0.870184.\n",
      "Iteration 1055: Policy loss: -0.013543. Value loss: 0.156420. Entropy: 0.865959.\n",
      "Iteration 1056: Policy loss: -0.025672. Value loss: 0.099951. Entropy: 0.873012.\n",
      "episode: 828   score: 9.0  epsilon: 1.0    steps: 40  evaluation reward: 9.7\n",
      "episode: 829   score: 15.0  epsilon: 1.0    steps: 200  evaluation reward: 9.78\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1057: Policy loss: 0.006989. Value loss: 0.278583. Entropy: 0.933715.\n",
      "Iteration 1058: Policy loss: -0.012967. Value loss: 0.131220. Entropy: 0.917405.\n",
      "Iteration 1059: Policy loss: -0.023016. Value loss: 0.088350. Entropy: 0.915543.\n",
      "episode: 830   score: 8.0  epsilon: 1.0    steps: 1024  evaluation reward: 9.77\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1060: Policy loss: 0.006967. Value loss: 0.239049. Entropy: 0.914913.\n",
      "Iteration 1061: Policy loss: -0.021538. Value loss: 0.092623. Entropy: 0.897456.\n",
      "Iteration 1062: Policy loss: -0.031452. Value loss: 0.048995. Entropy: 0.905628.\n",
      "episode: 831   score: 11.0  epsilon: 1.0    steps: 144  evaluation reward: 9.79\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1063: Policy loss: 0.003470. Value loss: 0.302480. Entropy: 0.913041.\n",
      "Iteration 1064: Policy loss: -0.014479. Value loss: 0.137132. Entropy: 0.926791.\n",
      "Iteration 1065: Policy loss: -0.024412. Value loss: 0.090261. Entropy: 0.922641.\n",
      "episode: 832   score: 8.0  epsilon: 1.0    steps: 64  evaluation reward: 9.79\n",
      "episode: 833   score: 13.0  epsilon: 1.0    steps: 272  evaluation reward: 9.81\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1066: Policy loss: 0.006124. Value loss: 0.284445. Entropy: 0.875182.\n",
      "Iteration 1067: Policy loss: -0.012367. Value loss: 0.126904. Entropy: 0.879940.\n",
      "Iteration 1068: Policy loss: -0.027424. Value loss: 0.081330. Entropy: 0.872610.\n",
      "episode: 834   score: 16.0  epsilon: 1.0    steps: 424  evaluation reward: 9.86\n",
      "episode: 835   score: 8.0  epsilon: 1.0    steps: 664  evaluation reward: 9.85\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1069: Policy loss: 0.008893. Value loss: 0.307740. Entropy: 0.885169.\n",
      "Iteration 1070: Policy loss: -0.012928. Value loss: 0.168069. Entropy: 0.885062.\n",
      "Iteration 1071: Policy loss: -0.025342. Value loss: 0.110055. Entropy: 0.883743.\n",
      "episode: 836   score: 13.0  epsilon: 1.0    steps: 216  evaluation reward: 9.89\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1072: Policy loss: 0.002062. Value loss: 0.451071. Entropy: 0.969837.\n",
      "Iteration 1073: Policy loss: -0.015534. Value loss: 0.208546. Entropy: 0.957041.\n",
      "Iteration 1074: Policy loss: -0.023388. Value loss: 0.130955. Entropy: 0.958409.\n",
      "episode: 837   score: 7.0  epsilon: 1.0    steps: 72  evaluation reward: 9.85\n",
      "episode: 838   score: 15.0  epsilon: 1.0    steps: 88  evaluation reward: 9.92\n",
      "episode: 839   score: 7.0  epsilon: 1.0    steps: 152  evaluation reward: 9.86\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1075: Policy loss: 0.006951. Value loss: 0.333886. Entropy: 0.947127.\n",
      "Iteration 1076: Policy loss: -0.005599. Value loss: 0.157340. Entropy: 0.969473.\n",
      "Iteration 1077: Policy loss: -0.020850. Value loss: 0.117629. Entropy: 0.953169.\n",
      "episode: 840   score: 8.0  epsilon: 1.0    steps: 488  evaluation reward: 9.81\n",
      "episode: 841   score: 9.0  epsilon: 1.0    steps: 856  evaluation reward: 9.79\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1078: Policy loss: 0.012065. Value loss: 0.211762. Entropy: 0.857207.\n",
      "Iteration 1079: Policy loss: -0.013155. Value loss: 0.070217. Entropy: 0.856506.\n",
      "Iteration 1080: Policy loss: -0.025602. Value loss: 0.045108. Entropy: 0.857255.\n",
      "episode: 842   score: 7.0  epsilon: 1.0    steps: 640  evaluation reward: 9.77\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1081: Policy loss: 0.008611. Value loss: 0.200570. Entropy: 0.914681.\n",
      "Iteration 1082: Policy loss: -0.013771. Value loss: 0.092027. Entropy: 0.916889.\n",
      "Iteration 1083: Policy loss: -0.027356. Value loss: 0.064274. Entropy: 0.924612.\n",
      "episode: 843   score: 10.0  epsilon: 1.0    steps: 232  evaluation reward: 9.81\n",
      "episode: 844   score: 6.0  epsilon: 1.0    steps: 432  evaluation reward: 9.76\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1084: Policy loss: 0.010040. Value loss: 0.271061. Entropy: 0.858919.\n",
      "Iteration 1085: Policy loss: -0.013756. Value loss: 0.123178. Entropy: 0.854472.\n",
      "Iteration 1086: Policy loss: -0.023693. Value loss: 0.093401. Entropy: 0.846809.\n",
      "episode: 845   score: 9.0  epsilon: 1.0    steps: 704  evaluation reward: 9.7\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1087: Policy loss: 0.009698. Value loss: 0.291956. Entropy: 0.900473.\n",
      "Iteration 1088: Policy loss: -0.016447. Value loss: 0.135834. Entropy: 0.889473.\n",
      "Iteration 1089: Policy loss: -0.029124. Value loss: 0.080596. Entropy: 0.892092.\n",
      "episode: 846   score: 10.0  epsilon: 1.0    steps: 264  evaluation reward: 9.71\n",
      "episode: 847   score: 16.0  epsilon: 1.0    steps: 800  evaluation reward: 9.79\n",
      "episode: 848   score: 7.0  epsilon: 1.0    steps: 960  evaluation reward: 9.79\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1090: Policy loss: 0.008693. Value loss: 0.390219. Entropy: 0.981709.\n",
      "Iteration 1091: Policy loss: -0.001421. Value loss: 0.147401. Entropy: 0.979519.\n",
      "Iteration 1092: Policy loss: -0.016799. Value loss: 0.092969. Entropy: 0.975394.\n",
      "episode: 849   score: 7.0  epsilon: 1.0    steps: 568  evaluation reward: 9.71\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1093: Policy loss: 0.007998. Value loss: 0.321861. Entropy: 0.924891.\n",
      "Iteration 1094: Policy loss: -0.015971. Value loss: 0.138083. Entropy: 0.925307.\n",
      "Iteration 1095: Policy loss: -0.028569. Value loss: 0.089988. Entropy: 0.923592.\n",
      "episode: 850   score: 11.0  epsilon: 1.0    steps: 40  evaluation reward: 9.79\n",
      "now time :  2019-03-06 12:51:14.122758\n",
      "episode: 851   score: 6.0  epsilon: 1.0    steps: 336  evaluation reward: 9.72\n",
      "episode: 852   score: 8.0  epsilon: 1.0    steps: 608  evaluation reward: 9.74\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1096: Policy loss: 0.004528. Value loss: 0.237699. Entropy: 1.008192.\n",
      "Iteration 1097: Policy loss: -0.011835. Value loss: 0.104865. Entropy: 0.980969.\n",
      "Iteration 1098: Policy loss: -0.027755. Value loss: 0.069547. Entropy: 0.978858.\n",
      "Training network. lr: 0.000242. clip: 0.096784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1099: Policy loss: 0.005590. Value loss: 0.215687. Entropy: 0.898543.\n",
      "Iteration 1100: Policy loss: -0.009642. Value loss: 0.113661. Entropy: 0.897153.\n",
      "Iteration 1101: Policy loss: -0.026488. Value loss: 0.073727. Entropy: 0.891273.\n",
      "episode: 853   score: 5.0  epsilon: 1.0    steps: 80  evaluation reward: 9.73\n",
      "episode: 854   score: 4.0  epsilon: 1.0    steps: 512  evaluation reward: 9.65\n",
      "episode: 855   score: 10.0  epsilon: 1.0    steps: 616  evaluation reward: 9.63\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1102: Policy loss: 0.005229. Value loss: 0.163519. Entropy: 0.831308.\n",
      "Iteration 1103: Policy loss: -0.013441. Value loss: 0.059883. Entropy: 0.847743.\n",
      "Iteration 1104: Policy loss: -0.024745. Value loss: 0.039509. Entropy: 0.837201.\n",
      "episode: 856   score: 9.0  epsilon: 1.0    steps: 48  evaluation reward: 9.65\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1105: Policy loss: 0.006416. Value loss: 0.268750. Entropy: 0.880405.\n",
      "Iteration 1106: Policy loss: -0.013588. Value loss: 0.106729. Entropy: 0.895046.\n",
      "Iteration 1107: Policy loss: -0.022652. Value loss: 0.069955. Entropy: 0.888865.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1108: Policy loss: 0.007304. Value loss: 0.235597. Entropy: 0.869986.\n",
      "Iteration 1109: Policy loss: -0.007171. Value loss: 0.094750. Entropy: 0.881921.\n",
      "Iteration 1110: Policy loss: -0.023441. Value loss: 0.062079. Entropy: 0.871541.\n",
      "episode: 857   score: 9.0  epsilon: 1.0    steps: 192  evaluation reward: 9.66\n",
      "episode: 858   score: 12.0  epsilon: 1.0    steps: 312  evaluation reward: 9.67\n",
      "episode: 859   score: 15.0  epsilon: 1.0    steps: 840  evaluation reward: 9.75\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1111: Policy loss: 0.009092. Value loss: 0.504020. Entropy: 0.919015.\n",
      "Iteration 1112: Policy loss: -0.009005. Value loss: 0.295989. Entropy: 0.922660.\n",
      "Iteration 1113: Policy loss: -0.019094. Value loss: 0.192978. Entropy: 0.931222.\n",
      "episode: 860   score: 11.0  epsilon: 1.0    steps: 600  evaluation reward: 9.77\n",
      "episode: 861   score: 14.0  epsilon: 1.0    steps: 672  evaluation reward: 9.82\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1114: Policy loss: 0.003286. Value loss: 0.183737. Entropy: 0.946717.\n",
      "Iteration 1115: Policy loss: -0.017185. Value loss: 0.072942. Entropy: 0.930424.\n",
      "Iteration 1116: Policy loss: -0.028179. Value loss: 0.043259. Entropy: 0.932137.\n",
      "episode: 862   score: 10.0  epsilon: 1.0    steps: 616  evaluation reward: 9.86\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1117: Policy loss: 0.004125. Value loss: 0.268997. Entropy: 0.849839.\n",
      "Iteration 1118: Policy loss: -0.011953. Value loss: 0.127057. Entropy: 0.874402.\n",
      "Iteration 1119: Policy loss: -0.023699. Value loss: 0.080441. Entropy: 0.863668.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1120: Policy loss: 0.005546. Value loss: 0.203047. Entropy: 0.867602.\n",
      "Iteration 1121: Policy loss: -0.009020. Value loss: 0.073576. Entropy: 0.853647.\n",
      "Iteration 1122: Policy loss: -0.023848. Value loss: 0.048053. Entropy: 0.850315.\n",
      "episode: 863   score: 12.0  epsilon: 1.0    steps: 48  evaluation reward: 9.89\n",
      "episode: 864   score: 15.0  epsilon: 1.0    steps: 792  evaluation reward: 9.97\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1123: Policy loss: 0.005255. Value loss: 0.167793. Entropy: 0.883183.\n",
      "Iteration 1124: Policy loss: -0.016863. Value loss: 0.072154. Entropy: 0.883805.\n",
      "Iteration 1125: Policy loss: -0.031513. Value loss: 0.043929. Entropy: 0.884134.\n",
      "episode: 865   score: 10.0  epsilon: 1.0    steps: 360  evaluation reward: 10.02\n",
      "episode: 866   score: 8.0  epsilon: 1.0    steps: 864  evaluation reward: 10.03\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1126: Policy loss: 0.012285. Value loss: 0.227764. Entropy: 0.840579.\n",
      "Iteration 1127: Policy loss: -0.008953. Value loss: 0.096770. Entropy: 0.840983.\n",
      "Iteration 1128: Policy loss: -0.022437. Value loss: 0.063849. Entropy: 0.848587.\n",
      "episode: 867   score: 11.0  epsilon: 1.0    steps: 192  evaluation reward: 10.07\n",
      "episode: 868   score: 9.0  epsilon: 1.0    steps: 472  evaluation reward: 10.05\n",
      "episode: 869   score: 10.0  epsilon: 1.0    steps: 720  evaluation reward: 10.07\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1129: Policy loss: 0.003836. Value loss: 0.464435. Entropy: 0.866970.\n",
      "Iteration 1130: Policy loss: -0.001962. Value loss: 0.304122. Entropy: 0.860320.\n",
      "Iteration 1131: Policy loss: -0.016799. Value loss: 0.231719. Entropy: 0.876327.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1132: Policy loss: -0.000887. Value loss: 0.443962. Entropy: 0.962037.\n",
      "Iteration 1133: Policy loss: -0.006580. Value loss: 0.217474. Entropy: 0.944364.\n",
      "Iteration 1134: Policy loss: -0.019840. Value loss: 0.100674. Entropy: 0.937147.\n",
      "episode: 870   score: 15.0  epsilon: 1.0    steps: 264  evaluation reward: 10.14\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1135: Policy loss: 0.006477. Value loss: 0.390238. Entropy: 0.810772.\n",
      "Iteration 1136: Policy loss: -0.010558. Value loss: 0.171418. Entropy: 0.789900.\n",
      "Iteration 1137: Policy loss: -0.020659. Value loss: 0.098170. Entropy: 0.803663.\n",
      "episode: 871   score: 9.0  epsilon: 1.0    steps: 24  evaluation reward: 10.14\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1138: Policy loss: 0.007965. Value loss: 0.261989. Entropy: 0.880275.\n",
      "Iteration 1139: Policy loss: -0.009171. Value loss: 0.090819. Entropy: 0.897493.\n",
      "Iteration 1140: Policy loss: -0.026124. Value loss: 0.058467. Entropy: 0.894049.\n",
      "episode: 872   score: 7.0  epsilon: 1.0    steps: 336  evaluation reward: 10.09\n",
      "episode: 873   score: 17.0  epsilon: 1.0    steps: 680  evaluation reward: 10.17\n",
      "episode: 874   score: 12.0  epsilon: 1.0    steps: 776  evaluation reward: 10.18\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1141: Policy loss: 0.009295. Value loss: 0.449136. Entropy: 0.868270.\n",
      "Iteration 1142: Policy loss: -0.003400. Value loss: 0.213608. Entropy: 0.869484.\n",
      "Iteration 1143: Policy loss: -0.021262. Value loss: 0.129132. Entropy: 0.857490.\n",
      "episode: 875   score: 14.0  epsilon: 1.0    steps: 592  evaluation reward: 10.27\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1144: Policy loss: 0.006532. Value loss: 0.231877. Entropy: 0.879597.\n",
      "Iteration 1145: Policy loss: -0.014775. Value loss: 0.075147. Entropy: 0.884674.\n",
      "Iteration 1146: Policy loss: -0.023802. Value loss: 0.051511. Entropy: 0.883224.\n",
      "episode: 876   score: 10.0  epsilon: 1.0    steps: 264  evaluation reward: 10.29\n",
      "episode: 877   score: 11.0  epsilon: 1.0    steps: 632  evaluation reward: 10.31\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1147: Policy loss: -0.000489. Value loss: 0.244553. Entropy: 0.880261.\n",
      "Iteration 1148: Policy loss: -0.018848. Value loss: 0.140139. Entropy: 0.879179.\n",
      "Iteration 1149: Policy loss: -0.029482. Value loss: 0.096072. Entropy: 0.872271.\n",
      "episode: 878   score: 9.0  epsilon: 1.0    steps: 384  evaluation reward: 10.31\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1150: Policy loss: 0.007220. Value loss: 0.302936. Entropy: 0.931337.\n",
      "Iteration 1151: Policy loss: -0.012019. Value loss: 0.141174. Entropy: 0.946074.\n",
      "Iteration 1152: Policy loss: -0.025607. Value loss: 0.085085. Entropy: 0.940335.\n",
      "episode: 879   score: 11.0  epsilon: 1.0    steps: 272  evaluation reward: 10.33\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1153: Policy loss: 0.008712. Value loss: 0.182890. Entropy: 0.821426.\n",
      "Iteration 1154: Policy loss: -0.014261. Value loss: 0.078165. Entropy: 0.814270.\n",
      "Iteration 1155: Policy loss: -0.024100. Value loss: 0.047633. Entropy: 0.820234.\n",
      "episode: 880   score: 9.0  epsilon: 1.0    steps: 832  evaluation reward: 10.31\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1156: Policy loss: 0.013107. Value loss: 0.399978. Entropy: 0.968398.\n",
      "Iteration 1157: Policy loss: -0.005544. Value loss: 0.174281. Entropy: 0.953898.\n",
      "Iteration 1158: Policy loss: -0.014218. Value loss: 0.109457. Entropy: 0.945360.\n",
      "episode: 881   score: 15.0  epsilon: 1.0    steps: 104  evaluation reward: 10.29\n",
      "episode: 882   score: 11.0  epsilon: 1.0    steps: 608  evaluation reward: 10.29\n",
      "episode: 883   score: 9.0  epsilon: 1.0    steps: 872  evaluation reward: 10.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1159: Policy loss: 0.004377. Value loss: 0.192739. Entropy: 0.865663.\n",
      "Iteration 1160: Policy loss: -0.011746. Value loss: 0.074778. Entropy: 0.877056.\n",
      "Iteration 1161: Policy loss: -0.022311. Value loss: 0.048618. Entropy: 0.871350.\n",
      "episode: 884   score: 9.0  epsilon: 1.0    steps: 312  evaluation reward: 10.24\n",
      "episode: 885   score: 9.0  epsilon: 1.0    steps: 752  evaluation reward: 10.25\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1162: Policy loss: 0.004321. Value loss: 0.291606. Entropy: 0.895706.\n",
      "Iteration 1163: Policy loss: -0.011254. Value loss: 0.134868. Entropy: 0.906427.\n",
      "Iteration 1164: Policy loss: -0.025813. Value loss: 0.099976. Entropy: 0.898935.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1165: Policy loss: 0.001872. Value loss: 0.408787. Entropy: 0.854197.\n",
      "Iteration 1166: Policy loss: -0.015736. Value loss: 0.170107. Entropy: 0.838178.\n",
      "Iteration 1167: Policy loss: -0.024692. Value loss: 0.106488. Entropy: 0.838544.\n",
      "episode: 886   score: 11.0  epsilon: 1.0    steps: 48  evaluation reward: 10.27\n",
      "episode: 887   score: 11.0  epsilon: 1.0    steps: 200  evaluation reward: 10.28\n",
      "episode: 888   score: 7.0  epsilon: 1.0    steps: 984  evaluation reward: 10.27\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1168: Policy loss: 0.007337. Value loss: 0.264865. Entropy: 0.918793.\n",
      "Iteration 1169: Policy loss: -0.010807. Value loss: 0.096452. Entropy: 0.916084.\n",
      "Iteration 1170: Policy loss: -0.024655. Value loss: 0.065715. Entropy: 0.906140.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1171: Policy loss: 0.007292. Value loss: 0.231019. Entropy: 0.887068.\n",
      "Iteration 1172: Policy loss: -0.008927. Value loss: 0.091858. Entropy: 0.877927.\n",
      "Iteration 1173: Policy loss: -0.022861. Value loss: 0.054117. Entropy: 0.891615.\n",
      "episode: 889   score: 9.0  epsilon: 1.0    steps: 368  evaluation reward: 10.28\n",
      "episode: 890   score: 7.0  epsilon: 1.0    steps: 424  evaluation reward: 10.2\n",
      "episode: 891   score: 11.0  epsilon: 1.0    steps: 576  evaluation reward: 10.14\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1174: Policy loss: 0.008418. Value loss: 0.319581. Entropy: 0.913388.\n",
      "Iteration 1175: Policy loss: -0.016518. Value loss: 0.118811. Entropy: 0.925998.\n",
      "Iteration 1176: Policy loss: -0.029508. Value loss: 0.079147. Entropy: 0.923505.\n",
      "episode: 892   score: 9.0  epsilon: 1.0    steps: 680  evaluation reward: 10.13\n",
      "episode: 893   score: 12.0  epsilon: 1.0    steps: 720  evaluation reward: 10.13\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1177: Policy loss: 0.006038. Value loss: 0.196103. Entropy: 0.939059.\n",
      "Iteration 1178: Policy loss: -0.017350. Value loss: 0.094336. Entropy: 0.957933.\n",
      "Iteration 1179: Policy loss: -0.029069. Value loss: 0.070016. Entropy: 0.946464.\n",
      "episode: 894   score: 9.0  epsilon: 1.0    steps: 888  evaluation reward: 10.12\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1180: Policy loss: 0.002540. Value loss: 0.169126. Entropy: 0.932609.\n",
      "Iteration 1181: Policy loss: -0.011136. Value loss: 0.084374. Entropy: 0.932883.\n",
      "Iteration 1182: Policy loss: -0.024212. Value loss: 0.058764. Entropy: 0.927742.\n",
      "episode: 895   score: 9.0  epsilon: 1.0    steps: 8  evaluation reward: 10.13\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1183: Policy loss: 0.004149. Value loss: 0.227370. Entropy: 0.922311.\n",
      "Iteration 1184: Policy loss: -0.013509. Value loss: 0.120211. Entropy: 0.910688.\n",
      "Iteration 1185: Policy loss: -0.025140. Value loss: 0.087579. Entropy: 0.907174.\n",
      "episode: 896   score: 9.0  epsilon: 1.0    steps: 152  evaluation reward: 10.14\n",
      "episode: 897   score: 7.0  epsilon: 1.0    steps: 648  evaluation reward: 10.12\n",
      "episode: 898   score: 8.0  epsilon: 1.0    steps: 672  evaluation reward: 10.11\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1186: Policy loss: 0.005020. Value loss: 0.190655. Entropy: 0.851433.\n",
      "Iteration 1187: Policy loss: -0.014821. Value loss: 0.078354. Entropy: 0.850779.\n",
      "Iteration 1188: Policy loss: -0.025900. Value loss: 0.049739. Entropy: 0.850998.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1189: Policy loss: 0.005781. Value loss: 0.652884. Entropy: 0.845526.\n",
      "Iteration 1190: Policy loss: -0.005437. Value loss: 0.324975. Entropy: 0.860389.\n",
      "Iteration 1191: Policy loss: -0.011691. Value loss: 0.182805. Entropy: 0.865715.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1192: Policy loss: 0.004194. Value loss: 0.248258. Entropy: 0.935184.\n",
      "Iteration 1193: Policy loss: -0.014475. Value loss: 0.094832. Entropy: 0.938039.\n",
      "Iteration 1194: Policy loss: -0.028323. Value loss: 0.057581. Entropy: 0.933843.\n",
      "episode: 899   score: 13.0  epsilon: 1.0    steps: 264  evaluation reward: 10.1\n",
      "episode: 900   score: 15.0  epsilon: 1.0    steps: 496  evaluation reward: 10.19\n",
      "now time :  2019-03-06 12:53:22.207029\n",
      "episode: 901   score: 14.0  epsilon: 1.0    steps: 712  evaluation reward: 10.2\n",
      "episode: 902   score: 11.0  epsilon: 1.0    steps: 960  evaluation reward: 10.24\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1195: Policy loss: 0.008187. Value loss: 0.340356. Entropy: 0.985411.\n",
      "Iteration 1196: Policy loss: -0.011037. Value loss: 0.167338. Entropy: 0.963178.\n",
      "Iteration 1197: Policy loss: -0.027827. Value loss: 0.123975. Entropy: 0.962338.\n",
      "episode: 903   score: 7.0  epsilon: 1.0    steps: 584  evaluation reward: 10.21\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1198: Policy loss: 0.005875. Value loss: 0.242083. Entropy: 0.952358.\n",
      "Iteration 1199: Policy loss: -0.011660. Value loss: 0.105094. Entropy: 0.940062.\n",
      "Iteration 1200: Policy loss: -0.019916. Value loss: 0.076558. Entropy: 0.926154.\n",
      "episode: 904   score: 12.0  epsilon: 1.0    steps: 376  evaluation reward: 10.22\n",
      "episode: 905   score: 9.0  epsilon: 1.0    steps: 392  evaluation reward: 10.25\n",
      "episode: 906   score: 12.0  epsilon: 1.0    steps: 992  evaluation reward: 10.3\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1201: Policy loss: 0.001091. Value loss: 0.205854. Entropy: 0.950144.\n",
      "Iteration 1202: Policy loss: -0.019022. Value loss: 0.107023. Entropy: 0.954052.\n",
      "Iteration 1203: Policy loss: -0.029996. Value loss: 0.075762. Entropy: 0.944333.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1204: Policy loss: 0.010097. Value loss: 0.218090. Entropy: 0.913449.\n",
      "Iteration 1205: Policy loss: -0.011941. Value loss: 0.105048. Entropy: 0.896702.\n",
      "Iteration 1206: Policy loss: -0.025843. Value loss: 0.064860. Entropy: 0.901891.\n",
      "episode: 907   score: 8.0  epsilon: 1.0    steps: 848  evaluation reward: 10.32\n",
      "episode: 908   score: 5.0  epsilon: 1.0    steps: 992  evaluation reward: 10.32\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1207: Policy loss: 0.007706. Value loss: 0.370636. Entropy: 0.929971.\n",
      "Iteration 1208: Policy loss: -0.020490. Value loss: 0.165121. Entropy: 0.953648.\n",
      "Iteration 1209: Policy loss: -0.030343. Value loss: 0.092015. Entropy: 0.932490.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1210: Policy loss: 0.005188. Value loss: 0.249143. Entropy: 0.950121.\n",
      "Iteration 1211: Policy loss: -0.015945. Value loss: 0.096729. Entropy: 0.961850.\n",
      "Iteration 1212: Policy loss: -0.026475. Value loss: 0.057156. Entropy: 0.963219.\n",
      "episode: 909   score: 10.0  epsilon: 1.0    steps: 72  evaluation reward: 10.29\n",
      "episode: 910   score: 11.0  epsilon: 1.0    steps: 256  evaluation reward: 10.26\n",
      "episode: 911   score: 8.0  epsilon: 1.0    steps: 512  evaluation reward: 10.26\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1213: Policy loss: 0.006718. Value loss: 0.211221. Entropy: 0.863939.\n",
      "Iteration 1214: Policy loss: -0.011004. Value loss: 0.084518. Entropy: 0.866013.\n",
      "Iteration 1215: Policy loss: -0.024186. Value loss: 0.046379. Entropy: 0.861407.\n",
      "episode: 912   score: 14.0  epsilon: 1.0    steps: 112  evaluation reward: 10.36\n",
      "episode: 913   score: 8.0  epsilon: 1.0    steps: 256  evaluation reward: 10.31\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1216: Policy loss: 0.007668. Value loss: 0.250526. Entropy: 0.946486.\n",
      "Iteration 1217: Policy loss: -0.013084. Value loss: 0.089662. Entropy: 0.945680.\n",
      "Iteration 1218: Policy loss: -0.028383. Value loss: 0.052455. Entropy: 0.933129.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 914   score: 12.0  epsilon: 1.0    steps: 200  evaluation reward: 10.36\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1219: Policy loss: 0.005353. Value loss: 0.281860. Entropy: 0.964594.\n",
      "Iteration 1220: Policy loss: -0.006029. Value loss: 0.102638. Entropy: 0.943313.\n",
      "Iteration 1221: Policy loss: -0.020596. Value loss: 0.068427. Entropy: 0.949444.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1222: Policy loss: 0.012749. Value loss: 0.198580. Entropy: 0.842208.\n",
      "Iteration 1223: Policy loss: -0.010227. Value loss: 0.087983. Entropy: 0.864899.\n",
      "Iteration 1224: Policy loss: -0.027452. Value loss: 0.056504. Entropy: 0.857709.\n",
      "episode: 915   score: 10.0  epsilon: 1.0    steps: 16  evaluation reward: 10.37\n",
      "episode: 916   score: 6.0  epsilon: 1.0    steps: 112  evaluation reward: 10.36\n",
      "episode: 917   score: 9.0  epsilon: 1.0    steps: 168  evaluation reward: 10.38\n",
      "episode: 918   score: 8.0  epsilon: 1.0    steps: 1008  evaluation reward: 10.27\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1225: Policy loss: 0.000532. Value loss: 0.252482. Entropy: 0.885182.\n",
      "Iteration 1226: Policy loss: -0.017624. Value loss: 0.122756. Entropy: 0.882862.\n",
      "Iteration 1227: Policy loss: -0.029672. Value loss: 0.091529. Entropy: 0.882007.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1228: Policy loss: 0.004179. Value loss: 0.191030. Entropy: 0.846657.\n",
      "Iteration 1229: Policy loss: -0.012363. Value loss: 0.088467. Entropy: 0.854518.\n",
      "Iteration 1230: Policy loss: -0.023902. Value loss: 0.057493. Entropy: 0.857480.\n",
      "episode: 919   score: 13.0  epsilon: 1.0    steps: 128  evaluation reward: 10.29\n",
      "episode: 920   score: 10.0  epsilon: 1.0    steps: 696  evaluation reward: 10.31\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1231: Policy loss: 0.010293. Value loss: 0.251637. Entropy: 0.895959.\n",
      "Iteration 1232: Policy loss: -0.015235. Value loss: 0.114187. Entropy: 0.900793.\n",
      "Iteration 1233: Policy loss: -0.022939. Value loss: 0.078337. Entropy: 0.896380.\n",
      "episode: 921   score: 5.0  epsilon: 1.0    steps: 224  evaluation reward: 10.28\n",
      "episode: 922   score: 13.0  epsilon: 1.0    steps: 472  evaluation reward: 10.27\n",
      "episode: 923   score: 11.0  epsilon: 1.0    steps: 496  evaluation reward: 10.22\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1234: Policy loss: 0.006354. Value loss: 0.269614. Entropy: 0.869587.\n",
      "Iteration 1235: Policy loss: -0.014953. Value loss: 0.094858. Entropy: 0.869485.\n",
      "Iteration 1236: Policy loss: -0.025968. Value loss: 0.061667. Entropy: 0.891743.\n",
      "episode: 924   score: 8.0  epsilon: 1.0    steps: 528  evaluation reward: 10.2\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1237: Policy loss: 0.004576. Value loss: 0.381420. Entropy: 0.907780.\n",
      "Iteration 1238: Policy loss: -0.020323. Value loss: 0.158798. Entropy: 0.893899.\n",
      "Iteration 1239: Policy loss: -0.029841. Value loss: 0.092319. Entropy: 0.894958.\n",
      "episode: 925   score: 11.0  epsilon: 1.0    steps: 296  evaluation reward: 10.11\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1240: Policy loss: 0.009396. Value loss: 0.505259. Entropy: 0.854520.\n",
      "Iteration 1241: Policy loss: -0.008806. Value loss: 0.239691. Entropy: 0.858347.\n",
      "Iteration 1242: Policy loss: -0.021476. Value loss: 0.154176. Entropy: 0.856167.\n",
      "episode: 926   score: 5.0  epsilon: 1.0    steps: 352  evaluation reward: 10.06\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1243: Policy loss: 0.010778. Value loss: 0.294675. Entropy: 0.916211.\n",
      "Iteration 1244: Policy loss: -0.005777. Value loss: 0.102561. Entropy: 0.922809.\n",
      "Iteration 1245: Policy loss: -0.023022. Value loss: 0.067637. Entropy: 0.924672.\n",
      "episode: 927   score: 14.0  epsilon: 1.0    steps: 1008  evaluation reward: 10.06\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1246: Policy loss: 0.007892. Value loss: 0.279058. Entropy: 0.875197.\n",
      "Iteration 1247: Policy loss: -0.012792. Value loss: 0.092842. Entropy: 0.864106.\n",
      "Iteration 1248: Policy loss: -0.024713. Value loss: 0.059111. Entropy: 0.865750.\n",
      "episode: 928   score: 10.0  epsilon: 1.0    steps: 424  evaluation reward: 10.07\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1249: Policy loss: 0.003253. Value loss: 0.224467. Entropy: 0.933071.\n",
      "Iteration 1250: Policy loss: -0.013494. Value loss: 0.131445. Entropy: 0.938544.\n",
      "Iteration 1251: Policy loss: -0.023427. Value loss: 0.097132. Entropy: 0.920273.\n",
      "episode: 929   score: 14.0  epsilon: 1.0    steps: 56  evaluation reward: 10.06\n",
      "episode: 930   score: 16.0  epsilon: 1.0    steps: 112  evaluation reward: 10.14\n",
      "episode: 931   score: 13.0  epsilon: 1.0    steps: 696  evaluation reward: 10.16\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1252: Policy loss: 0.003857. Value loss: 0.646084. Entropy: 0.924097.\n",
      "Iteration 1253: Policy loss: -0.007527. Value loss: 0.373756. Entropy: 0.904535.\n",
      "Iteration 1254: Policy loss: -0.015490. Value loss: 0.242531. Entropy: 0.890153.\n",
      "episode: 932   score: 10.0  epsilon: 1.0    steps: 104  evaluation reward: 10.18\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1255: Policy loss: 0.003671. Value loss: 0.367287. Entropy: 0.832129.\n",
      "Iteration 1256: Policy loss: -0.011798. Value loss: 0.151116. Entropy: 0.835138.\n",
      "Iteration 1257: Policy loss: -0.022852. Value loss: 0.087013. Entropy: 0.839614.\n",
      "episode: 933   score: 12.0  epsilon: 1.0    steps: 192  evaluation reward: 10.17\n",
      "episode: 934   score: 18.0  epsilon: 1.0    steps: 400  evaluation reward: 10.19\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1258: Policy loss: 0.006138. Value loss: 0.216246. Entropy: 0.880822.\n",
      "Iteration 1259: Policy loss: -0.012677. Value loss: 0.073287. Entropy: 0.882764.\n",
      "Iteration 1260: Policy loss: -0.028005. Value loss: 0.045856. Entropy: 0.878013.\n",
      "episode: 935   score: 7.0  epsilon: 1.0    steps: 640  evaluation reward: 10.18\n",
      "episode: 936   score: 6.0  epsilon: 1.0    steps: 656  evaluation reward: 10.11\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1261: Policy loss: 0.006491. Value loss: 0.287476. Entropy: 0.856145.\n",
      "Iteration 1262: Policy loss: -0.011045. Value loss: 0.117118. Entropy: 0.842021.\n",
      "Iteration 1263: Policy loss: -0.026308. Value loss: 0.075036. Entropy: 0.846834.\n",
      "episode: 937   score: 13.0  epsilon: 1.0    steps: 896  evaluation reward: 10.17\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1264: Policy loss: 0.007818. Value loss: 0.444678. Entropy: 0.873258.\n",
      "Iteration 1265: Policy loss: -0.013055. Value loss: 0.208492. Entropy: 0.876215.\n",
      "Iteration 1266: Policy loss: -0.020120. Value loss: 0.123008. Entropy: 0.870106.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1267: Policy loss: 0.012982. Value loss: 0.608481. Entropy: 0.913016.\n",
      "Iteration 1268: Policy loss: 0.000564. Value loss: 0.275306. Entropy: 0.907746.\n",
      "Iteration 1269: Policy loss: -0.011439. Value loss: 0.170877. Entropy: 0.887531.\n",
      "episode: 938   score: 10.0  epsilon: 1.0    steps: 200  evaluation reward: 10.12\n",
      "episode: 939   score: 15.0  epsilon: 1.0    steps: 440  evaluation reward: 10.2\n",
      "episode: 940   score: 17.0  epsilon: 1.0    steps: 712  evaluation reward: 10.29\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1270: Policy loss: 0.004604. Value loss: 0.294250. Entropy: 0.873893.\n",
      "Iteration 1271: Policy loss: -0.014015. Value loss: 0.134719. Entropy: 0.867509.\n",
      "Iteration 1272: Policy loss: -0.024622. Value loss: 0.086047. Entropy: 0.869937.\n",
      "episode: 941   score: 11.0  epsilon: 1.0    steps: 680  evaluation reward: 10.31\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1273: Policy loss: 0.006968. Value loss: 0.249886. Entropy: 0.818073.\n",
      "Iteration 1274: Policy loss: -0.010319. Value loss: 0.098650. Entropy: 0.817622.\n",
      "Iteration 1275: Policy loss: -0.023888. Value loss: 0.070006. Entropy: 0.809811.\n",
      "episode: 942   score: 8.0  epsilon: 1.0    steps: 96  evaluation reward: 10.32\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1276: Policy loss: 0.012210. Value loss: 0.187404. Entropy: 0.897688.\n",
      "Iteration 1277: Policy loss: -0.015071. Value loss: 0.073586. Entropy: 0.900349.\n",
      "Iteration 1278: Policy loss: -0.025597. Value loss: 0.043765. Entropy: 0.896075.\n",
      "episode: 943   score: 15.0  epsilon: 1.0    steps: 752  evaluation reward: 10.37\n",
      "episode: 944   score: 13.0  epsilon: 1.0    steps: 832  evaluation reward: 10.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1279: Policy loss: 0.020509. Value loss: 0.415443. Entropy: 0.824099.\n",
      "Iteration 1280: Policy loss: -0.007576. Value loss: 0.222829. Entropy: 0.839329.\n",
      "Iteration 1281: Policy loss: -0.017271. Value loss: 0.150809. Entropy: 0.828868.\n",
      "episode: 945   score: 14.0  epsilon: 1.0    steps: 400  evaluation reward: 10.49\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1282: Policy loss: 0.011866. Value loss: 0.275953. Entropy: 0.892047.\n",
      "Iteration 1283: Policy loss: -0.006528. Value loss: 0.128170. Entropy: 0.884848.\n",
      "Iteration 1284: Policy loss: -0.021205. Value loss: 0.072632. Entropy: 0.890934.\n",
      "episode: 946   score: 9.0  epsilon: 1.0    steps: 448  evaluation reward: 10.48\n",
      "episode: 947   score: 10.0  epsilon: 1.0    steps: 504  evaluation reward: 10.42\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1285: Policy loss: 0.007530. Value loss: 0.457742. Entropy: 0.970403.\n",
      "Iteration 1286: Policy loss: -0.011525. Value loss: 0.242922. Entropy: 0.952851.\n",
      "Iteration 1287: Policy loss: -0.015688. Value loss: 0.169865. Entropy: 0.952775.\n",
      "episode: 948   score: 15.0  epsilon: 1.0    steps: 696  evaluation reward: 10.5\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1288: Policy loss: 0.004928. Value loss: 0.458522. Entropy: 0.939599.\n",
      "Iteration 1289: Policy loss: -0.010440. Value loss: 0.200797. Entropy: 0.928624.\n",
      "Iteration 1290: Policy loss: -0.016518. Value loss: 0.129293. Entropy: 0.921209.\n",
      "episode: 949   score: 10.0  epsilon: 1.0    steps: 248  evaluation reward: 10.53\n",
      "episode: 950   score: 16.0  epsilon: 1.0    steps: 888  evaluation reward: 10.58\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1291: Policy loss: 0.007562. Value loss: 0.451798. Entropy: 0.934104.\n",
      "Iteration 1292: Policy loss: -0.000580. Value loss: 0.203327. Entropy: 0.949039.\n",
      "Iteration 1293: Policy loss: -0.009156. Value loss: 0.137414. Entropy: 0.948680.\n",
      "now time :  2019-03-06 12:55:28.565637\n",
      "episode: 951   score: 5.0  epsilon: 1.0    steps: 480  evaluation reward: 10.57\n",
      "episode: 952   score: 5.0  epsilon: 1.0    steps: 752  evaluation reward: 10.54\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1294: Policy loss: 0.013970. Value loss: 0.449768. Entropy: 0.953959.\n",
      "Iteration 1295: Policy loss: -0.000916. Value loss: 0.166277. Entropy: 0.961774.\n",
      "Iteration 1296: Policy loss: -0.028026. Value loss: 0.113615. Entropy: 0.955528.\n",
      "episode: 953   score: 9.0  epsilon: 1.0    steps: 568  evaluation reward: 10.58\n",
      "episode: 954   score: 14.0  epsilon: 1.0    steps: 608  evaluation reward: 10.68\n",
      "episode: 955   score: 5.0  epsilon: 1.0    steps: 992  evaluation reward: 10.63\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1297: Policy loss: 0.007359. Value loss: 0.352658. Entropy: 0.949276.\n",
      "Iteration 1298: Policy loss: -0.011738. Value loss: 0.153846. Entropy: 0.967373.\n",
      "Iteration 1299: Policy loss: -0.025720. Value loss: 0.106446. Entropy: 0.952422.\n",
      "episode: 956   score: 14.0  epsilon: 1.0    steps: 1000  evaluation reward: 10.68\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1300: Policy loss: 0.006208. Value loss: 0.166175. Entropy: 0.929496.\n",
      "Iteration 1301: Policy loss: -0.014550. Value loss: 0.068305. Entropy: 0.946917.\n",
      "Iteration 1302: Policy loss: -0.024386. Value loss: 0.047344. Entropy: 0.943960.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1303: Policy loss: 0.005373. Value loss: 0.198013. Entropy: 0.891645.\n",
      "Iteration 1304: Policy loss: -0.017115. Value loss: 0.084071. Entropy: 0.891109.\n",
      "Iteration 1305: Policy loss: -0.027592. Value loss: 0.049370. Entropy: 0.886536.\n",
      "episode: 957   score: 6.0  epsilon: 1.0    steps: 320  evaluation reward: 10.65\n",
      "episode: 958   score: 7.0  epsilon: 1.0    steps: 472  evaluation reward: 10.6\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1306: Policy loss: 0.007508. Value loss: 0.575412. Entropy: 0.913247.\n",
      "Iteration 1307: Policy loss: -0.003816. Value loss: 0.233190. Entropy: 0.914234.\n",
      "Iteration 1308: Policy loss: -0.012262. Value loss: 0.141642. Entropy: 0.919100.\n",
      "episode: 959   score: 13.0  epsilon: 1.0    steps: 8  evaluation reward: 10.58\n",
      "episode: 960   score: 11.0  epsilon: 1.0    steps: 480  evaluation reward: 10.58\n",
      "episode: 961   score: 10.0  epsilon: 1.0    steps: 528  evaluation reward: 10.54\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1309: Policy loss: 0.013710. Value loss: 0.291390. Entropy: 0.909792.\n",
      "Iteration 1310: Policy loss: -0.009165. Value loss: 0.123154. Entropy: 0.923079.\n",
      "Iteration 1311: Policy loss: -0.020566. Value loss: 0.076033. Entropy: 0.940092.\n",
      "episode: 962   score: 7.0  epsilon: 1.0    steps: 24  evaluation reward: 10.51\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1312: Policy loss: 0.014516. Value loss: 0.441140. Entropy: 0.860706.\n",
      "Iteration 1313: Policy loss: -0.006116. Value loss: 0.153339. Entropy: 0.859543.\n",
      "Iteration 1314: Policy loss: -0.018955. Value loss: 0.088122. Entropy: 0.852253.\n",
      "episode: 963   score: 8.0  epsilon: 1.0    steps: 752  evaluation reward: 10.47\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1315: Policy loss: 0.004206. Value loss: 0.296859. Entropy: 0.961292.\n",
      "Iteration 1316: Policy loss: -0.017265. Value loss: 0.128418. Entropy: 0.959503.\n",
      "Iteration 1317: Policy loss: -0.031531. Value loss: 0.076652. Entropy: 0.962532.\n",
      "episode: 964   score: 5.0  epsilon: 1.0    steps: 208  evaluation reward: 10.37\n",
      "episode: 965   score: 17.0  epsilon: 1.0    steps: 480  evaluation reward: 10.44\n",
      "episode: 966   score: 9.0  epsilon: 1.0    steps: 832  evaluation reward: 10.45\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1318: Policy loss: 0.008207. Value loss: 0.406663. Entropy: 0.931779.\n",
      "Iteration 1319: Policy loss: -0.008449. Value loss: 0.197933. Entropy: 0.954779.\n",
      "Iteration 1320: Policy loss: -0.023018. Value loss: 0.133504. Entropy: 0.946462.\n",
      "episode: 967   score: 10.0  epsilon: 1.0    steps: 384  evaluation reward: 10.44\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1321: Policy loss: 0.003527. Value loss: 0.245512. Entropy: 0.885840.\n",
      "Iteration 1322: Policy loss: -0.016733. Value loss: 0.102858. Entropy: 0.872791.\n",
      "Iteration 1323: Policy loss: -0.028273. Value loss: 0.068924. Entropy: 0.862793.\n",
      "episode: 968   score: 10.0  epsilon: 1.0    steps: 248  evaluation reward: 10.45\n",
      "episode: 969   score: 8.0  epsilon: 1.0    steps: 336  evaluation reward: 10.43\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1324: Policy loss: 0.004447. Value loss: 0.294666. Entropy: 0.901619.\n",
      "Iteration 1325: Policy loss: -0.015562. Value loss: 0.122673. Entropy: 0.901610.\n",
      "Iteration 1326: Policy loss: -0.023435. Value loss: 0.069188. Entropy: 0.903472.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1327: Policy loss: 0.006762. Value loss: 0.173893. Entropy: 0.937106.\n",
      "Iteration 1328: Policy loss: -0.015814. Value loss: 0.079411. Entropy: 0.934327.\n",
      "Iteration 1329: Policy loss: -0.029693. Value loss: 0.054209. Entropy: 0.930200.\n",
      "episode: 970   score: 13.0  epsilon: 1.0    steps: 248  evaluation reward: 10.41\n",
      "episode: 971   score: 6.0  epsilon: 1.0    steps: 384  evaluation reward: 10.38\n",
      "episode: 972   score: 8.0  epsilon: 1.0    steps: 384  evaluation reward: 10.39\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1330: Policy loss: 0.002707. Value loss: 0.249492. Entropy: 0.835473.\n",
      "Iteration 1331: Policy loss: -0.012129. Value loss: 0.100316. Entropy: 0.822176.\n",
      "Iteration 1332: Policy loss: -0.026166. Value loss: 0.073191. Entropy: 0.823674.\n",
      "episode: 973   score: 11.0  epsilon: 1.0    steps: 808  evaluation reward: 10.33\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1333: Policy loss: 0.006758. Value loss: 0.231133. Entropy: 0.893419.\n",
      "Iteration 1334: Policy loss: -0.008965. Value loss: 0.091940. Entropy: 0.905377.\n",
      "Iteration 1335: Policy loss: -0.027421. Value loss: 0.057325. Entropy: 0.891740.\n",
      "episode: 974   score: 13.0  epsilon: 1.0    steps: 688  evaluation reward: 10.34\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1336: Policy loss: 0.005770. Value loss: 0.177820. Entropy: 0.921149.\n",
      "Iteration 1337: Policy loss: -0.015826. Value loss: 0.072942. Entropy: 0.924149.\n",
      "Iteration 1338: Policy loss: -0.030503. Value loss: 0.049177. Entropy: 0.924533.\n",
      "episode: 975   score: 10.0  epsilon: 1.0    steps: 80  evaluation reward: 10.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1339: Policy loss: 0.007004. Value loss: 0.391934. Entropy: 0.881328.\n",
      "Iteration 1340: Policy loss: -0.009637. Value loss: 0.164788. Entropy: 0.875725.\n",
      "Iteration 1341: Policy loss: -0.019040. Value loss: 0.100977. Entropy: 0.867941.\n",
      "episode: 976   score: 7.0  epsilon: 1.0    steps: 120  evaluation reward: 10.27\n",
      "episode: 977   score: 13.0  epsilon: 1.0    steps: 136  evaluation reward: 10.29\n",
      "episode: 978   score: 12.0  epsilon: 1.0    steps: 536  evaluation reward: 10.32\n",
      "episode: 979   score: 8.0  epsilon: 1.0    steps: 840  evaluation reward: 10.29\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1342: Policy loss: 0.004121. Value loss: 0.214954. Entropy: 0.924905.\n",
      "Iteration 1343: Policy loss: -0.007326. Value loss: 0.096164. Entropy: 0.924296.\n",
      "Iteration 1344: Policy loss: -0.021045. Value loss: 0.066329. Entropy: 0.933173.\n",
      "episode: 980   score: 13.0  epsilon: 1.0    steps: 320  evaluation reward: 10.33\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1345: Policy loss: 0.004233. Value loss: 0.168644. Entropy: 0.838886.\n",
      "Iteration 1346: Policy loss: -0.013561. Value loss: 0.065268. Entropy: 0.834622.\n",
      "Iteration 1347: Policy loss: -0.024121. Value loss: 0.045962. Entropy: 0.822448.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1348: Policy loss: 0.008393. Value loss: 0.181432. Entropy: 0.892027.\n",
      "Iteration 1349: Policy loss: -0.013428. Value loss: 0.083963. Entropy: 0.882663.\n",
      "Iteration 1350: Policy loss: -0.021962. Value loss: 0.066349. Entropy: 0.870588.\n",
      "episode: 981   score: 10.0  epsilon: 1.0    steps: 672  evaluation reward: 10.28\n",
      "episode: 982   score: 11.0  epsilon: 1.0    steps: 776  evaluation reward: 10.28\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1351: Policy loss: 0.005784. Value loss: 0.198769. Entropy: 0.992366.\n",
      "Iteration 1352: Policy loss: -0.014332. Value loss: 0.083996. Entropy: 0.988470.\n",
      "Iteration 1353: Policy loss: -0.029560. Value loss: 0.050940. Entropy: 0.995226.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1354: Policy loss: 0.005599. Value loss: 0.381387. Entropy: 0.917594.\n",
      "Iteration 1355: Policy loss: -0.015459. Value loss: 0.159274. Entropy: 0.912200.\n",
      "Iteration 1356: Policy loss: -0.024282. Value loss: 0.113142. Entropy: 0.917926.\n",
      "episode: 983   score: 12.0  epsilon: 1.0    steps: 152  evaluation reward: 10.31\n",
      "episode: 984   score: 9.0  epsilon: 1.0    steps: 344  evaluation reward: 10.31\n",
      "episode: 985   score: 2.0  epsilon: 1.0    steps: 736  evaluation reward: 10.24\n",
      "episode: 986   score: 10.0  epsilon: 1.0    steps: 800  evaluation reward: 10.23\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1357: Policy loss: 0.008280. Value loss: 0.407958. Entropy: 0.961528.\n",
      "Iteration 1358: Policy loss: -0.011376. Value loss: 0.132242. Entropy: 0.960374.\n",
      "Iteration 1359: Policy loss: -0.021904. Value loss: 0.082596. Entropy: 0.946323.\n",
      "episode: 987   score: 8.0  epsilon: 1.0    steps: 72  evaluation reward: 10.2\n",
      "episode: 988   score: 15.0  epsilon: 1.0    steps: 352  evaluation reward: 10.28\n",
      "episode: 989   score: 11.0  epsilon: 1.0    steps: 720  evaluation reward: 10.3\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1360: Policy loss: 0.006759. Value loss: 0.195619. Entropy: 0.949466.\n",
      "Iteration 1361: Policy loss: -0.019369. Value loss: 0.114608. Entropy: 0.940194.\n",
      "Iteration 1362: Policy loss: -0.027317. Value loss: 0.092660. Entropy: 0.946473.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1363: Policy loss: 0.008467. Value loss: 0.233417. Entropy: 0.872610.\n",
      "Iteration 1364: Policy loss: -0.012363. Value loss: 0.095017. Entropy: 0.869611.\n",
      "Iteration 1365: Policy loss: -0.024852. Value loss: 0.053065. Entropy: 0.862838.\n",
      "episode: 990   score: 8.0  epsilon: 1.0    steps: 24  evaluation reward: 10.31\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1366: Policy loss: 0.005875. Value loss: 0.185145. Entropy: 0.977142.\n",
      "Iteration 1367: Policy loss: -0.020680. Value loss: 0.072602. Entropy: 0.974625.\n",
      "Iteration 1368: Policy loss: -0.033239. Value loss: 0.042970. Entropy: 0.974523.\n",
      "episode: 991   score: 6.0  epsilon: 1.0    steps: 840  evaluation reward: 10.26\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1369: Policy loss: -0.000757. Value loss: 0.436189. Entropy: 0.920986.\n",
      "Iteration 1370: Policy loss: -0.005725. Value loss: 0.214587. Entropy: 0.932939.\n",
      "Iteration 1371: Policy loss: -0.019642. Value loss: 0.141101. Entropy: 0.913109.\n",
      "episode: 992   score: 10.0  epsilon: 1.0    steps: 800  evaluation reward: 10.27\n",
      "episode: 993   score: 12.0  epsilon: 1.0    steps: 840  evaluation reward: 10.27\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1372: Policy loss: 0.007236. Value loss: 0.208985. Entropy: 0.925476.\n",
      "Iteration 1373: Policy loss: -0.015824. Value loss: 0.071492. Entropy: 0.937097.\n",
      "Iteration 1374: Policy loss: -0.025962. Value loss: 0.047463. Entropy: 0.918873.\n",
      "episode: 994   score: 11.0  epsilon: 1.0    steps: 896  evaluation reward: 10.29\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1375: Policy loss: 0.008538. Value loss: 0.199969. Entropy: 0.987375.\n",
      "Iteration 1376: Policy loss: -0.013889. Value loss: 0.075197. Entropy: 0.984440.\n",
      "Iteration 1377: Policy loss: -0.024059. Value loss: 0.049968. Entropy: 0.979667.\n",
      "episode: 995   score: 16.0  epsilon: 1.0    steps: 272  evaluation reward: 10.36\n",
      "episode: 996   score: 9.0  epsilon: 1.0    steps: 824  evaluation reward: 10.36\n",
      "episode: 997   score: 15.0  epsilon: 1.0    steps: 904  evaluation reward: 10.44\n",
      "episode: 998   score: 18.0  epsilon: 1.0    steps: 960  evaluation reward: 10.54\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1378: Policy loss: 0.004166. Value loss: 0.538540. Entropy: 0.890826.\n",
      "Iteration 1379: Policy loss: -0.007103. Value loss: 0.315698. Entropy: 0.873955.\n",
      "Iteration 1380: Policy loss: -0.015429. Value loss: 0.204900. Entropy: 0.881864.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1381: Policy loss: 0.010277. Value loss: 0.253314. Entropy: 0.924785.\n",
      "Iteration 1382: Policy loss: -0.010993. Value loss: 0.107895. Entropy: 0.906641.\n",
      "Iteration 1383: Policy loss: -0.022419. Value loss: 0.057356. Entropy: 0.906563.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1384: Policy loss: 0.008082. Value loss: 0.462888. Entropy: 0.902323.\n",
      "Iteration 1385: Policy loss: -0.007413. Value loss: 0.208363. Entropy: 0.912725.\n",
      "Iteration 1386: Policy loss: -0.021729. Value loss: 0.129013. Entropy: 0.902719.\n",
      "episode: 999   score: 8.0  epsilon: 1.0    steps: 40  evaluation reward: 10.49\n",
      "episode: 1000   score: 12.0  epsilon: 1.0    steps: 504  evaluation reward: 10.46\n",
      "now time :  2019-03-06 12:57:30.269130\n",
      "episode: 1001   score: 9.0  epsilon: 1.0    steps: 736  evaluation reward: 10.41\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1387: Policy loss: 0.004396. Value loss: 0.248331. Entropy: 0.916920.\n",
      "Iteration 1388: Policy loss: -0.013108. Value loss: 0.109888. Entropy: 0.922735.\n",
      "Iteration 1389: Policy loss: -0.027227. Value loss: 0.054395. Entropy: 0.908251.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1390: Policy loss: 0.013919. Value loss: 0.373062. Entropy: 0.806352.\n",
      "Iteration 1391: Policy loss: -0.009118. Value loss: 0.188558. Entropy: 0.817008.\n",
      "Iteration 1392: Policy loss: -0.019810. Value loss: 0.128520. Entropy: 0.817689.\n",
      "episode: 1002   score: 12.0  epsilon: 1.0    steps: 168  evaluation reward: 10.42\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1393: Policy loss: 0.006130. Value loss: 0.647522. Entropy: 0.950216.\n",
      "Iteration 1394: Policy loss: -0.005947. Value loss: 0.276164. Entropy: 0.936556.\n",
      "Iteration 1395: Policy loss: -0.012073. Value loss: 0.149185. Entropy: 0.932009.\n",
      "episode: 1003   score: 10.0  epsilon: 1.0    steps: 104  evaluation reward: 10.45\n",
      "episode: 1004   score: 18.0  epsilon: 1.0    steps: 120  evaluation reward: 10.51\n",
      "episode: 1005   score: 15.0  epsilon: 1.0    steps: 536  evaluation reward: 10.57\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1396: Policy loss: 0.007883. Value loss: 0.276674. Entropy: 0.860652.\n",
      "Iteration 1397: Policy loss: -0.000838. Value loss: 0.113943. Entropy: 0.864118.\n",
      "Iteration 1398: Policy loss: -0.018120. Value loss: 0.070973. Entropy: 0.863829.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1399: Policy loss: 0.005807. Value loss: 0.453549. Entropy: 0.946991.\n",
      "Iteration 1400: Policy loss: -0.008924. Value loss: 0.254986. Entropy: 0.953626.\n",
      "Iteration 1401: Policy loss: -0.023060. Value loss: 0.165376. Entropy: 0.936317.\n",
      "episode: 1006   score: 16.0  epsilon: 1.0    steps: 88  evaluation reward: 10.61\n",
      "episode: 1007   score: 12.0  epsilon: 1.0    steps: 264  evaluation reward: 10.65\n",
      "episode: 1008   score: 9.0  epsilon: 1.0    steps: 856  evaluation reward: 10.69\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1402: Policy loss: 0.006443. Value loss: 0.301993. Entropy: 0.848523.\n",
      "Iteration 1403: Policy loss: -0.012082. Value loss: 0.109209. Entropy: 0.848968.\n",
      "Iteration 1404: Policy loss: -0.024073. Value loss: 0.067482. Entropy: 0.833815.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1405: Policy loss: 0.005300. Value loss: 0.206262. Entropy: 1.004983.\n",
      "Iteration 1406: Policy loss: -0.016521. Value loss: 0.088918. Entropy: 1.000220.\n",
      "Iteration 1407: Policy loss: -0.028579. Value loss: 0.055868. Entropy: 1.008505.\n",
      "episode: 1009   score: 15.0  epsilon: 1.0    steps: 344  evaluation reward: 10.74\n",
      "episode: 1010   score: 10.0  epsilon: 1.0    steps: 504  evaluation reward: 10.73\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1408: Policy loss: 0.012370. Value loss: 0.268299. Entropy: 0.901474.\n",
      "Iteration 1409: Policy loss: -0.007075. Value loss: 0.116867. Entropy: 0.892509.\n",
      "Iteration 1410: Policy loss: -0.027407. Value loss: 0.079867. Entropy: 0.882950.\n",
      "episode: 1011   score: 13.0  epsilon: 1.0    steps: 168  evaluation reward: 10.78\n",
      "episode: 1012   score: 11.0  epsilon: 1.0    steps: 928  evaluation reward: 10.75\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1411: Policy loss: 0.007381. Value loss: 0.275265. Entropy: 0.923097.\n",
      "Iteration 1412: Policy loss: -0.012939. Value loss: 0.118765. Entropy: 0.934741.\n",
      "Iteration 1413: Policy loss: -0.024270. Value loss: 0.077135. Entropy: 0.929261.\n",
      "episode: 1013   score: 11.0  epsilon: 1.0    steps: 504  evaluation reward: 10.78\n",
      "episode: 1014   score: 8.0  epsilon: 1.0    steps: 600  evaluation reward: 10.74\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1414: Policy loss: 0.010616. Value loss: 0.326649. Entropy: 0.944088.\n",
      "Iteration 1415: Policy loss: -0.015296. Value loss: 0.111489. Entropy: 0.943536.\n",
      "Iteration 1416: Policy loss: -0.025479. Value loss: 0.070326. Entropy: 0.941323.\n",
      "episode: 1015   score: 9.0  epsilon: 1.0    steps: 728  evaluation reward: 10.73\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1417: Policy loss: 0.008062. Value loss: 0.191027. Entropy: 0.888479.\n",
      "Iteration 1418: Policy loss: -0.014932. Value loss: 0.068425. Entropy: 0.893986.\n",
      "Iteration 1419: Policy loss: -0.024823. Value loss: 0.040375. Entropy: 0.898297.\n",
      "episode: 1016   score: 11.0  epsilon: 1.0    steps: 392  evaluation reward: 10.78\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1420: Policy loss: 0.002786. Value loss: 0.427885. Entropy: 0.950560.\n",
      "Iteration 1421: Policy loss: -0.008972. Value loss: 0.230099. Entropy: 0.966205.\n",
      "Iteration 1422: Policy loss: -0.020799. Value loss: 0.142263. Entropy: 0.956844.\n",
      "episode: 1017   score: 14.0  epsilon: 1.0    steps: 680  evaluation reward: 10.83\n",
      "episode: 1018   score: 14.0  epsilon: 1.0    steps: 904  evaluation reward: 10.89\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1423: Policy loss: 0.004175. Value loss: 0.468675. Entropy: 0.869986.\n",
      "Iteration 1424: Policy loss: 0.001032. Value loss: 0.188959. Entropy: 0.859186.\n",
      "Iteration 1425: Policy loss: -0.019385. Value loss: 0.128629. Entropy: 0.867629.\n",
      "episode: 1019   score: 9.0  epsilon: 1.0    steps: 376  evaluation reward: 10.85\n",
      "episode: 1020   score: 14.0  epsilon: 1.0    steps: 1016  evaluation reward: 10.89\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1426: Policy loss: 0.006202. Value loss: 0.342792. Entropy: 0.966559.\n",
      "Iteration 1427: Policy loss: -0.014584. Value loss: 0.150869. Entropy: 0.970909.\n",
      "Iteration 1428: Policy loss: -0.025610. Value loss: 0.096128. Entropy: 0.965867.\n",
      "episode: 1021   score: 6.0  epsilon: 1.0    steps: 8  evaluation reward: 10.9\n",
      "episode: 1022   score: 9.0  epsilon: 1.0    steps: 8  evaluation reward: 10.86\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1429: Policy loss: 0.007955. Value loss: 0.364680. Entropy: 0.852426.\n",
      "Iteration 1430: Policy loss: -0.010437. Value loss: 0.156416. Entropy: 0.856334.\n",
      "Iteration 1431: Policy loss: -0.024044. Value loss: 0.104628. Entropy: 0.867410.\n",
      "episode: 1023   score: 7.0  epsilon: 1.0    steps: 136  evaluation reward: 10.82\n",
      "episode: 1024   score: 14.0  epsilon: 1.0    steps: 752  evaluation reward: 10.88\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1432: Policy loss: 0.010134. Value loss: 0.428043. Entropy: 0.904789.\n",
      "Iteration 1433: Policy loss: -0.002966. Value loss: 0.178285. Entropy: 0.927708.\n",
      "Iteration 1434: Policy loss: -0.008941. Value loss: 0.089048. Entropy: 0.915656.\n",
      "episode: 1025   score: 7.0  epsilon: 1.0    steps: 400  evaluation reward: 10.84\n",
      "episode: 1026   score: 5.0  epsilon: 1.0    steps: 432  evaluation reward: 10.84\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1435: Policy loss: 0.011792. Value loss: 0.350826. Entropy: 0.943535.\n",
      "Iteration 1436: Policy loss: 0.000427. Value loss: 0.142358. Entropy: 0.941748.\n",
      "Iteration 1437: Policy loss: -0.018944. Value loss: 0.101813. Entropy: 0.930752.\n",
      "episode: 1027   score: 6.0  epsilon: 1.0    steps: 496  evaluation reward: 10.76\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1438: Policy loss: 0.005283. Value loss: 0.326905. Entropy: 0.876361.\n",
      "Iteration 1439: Policy loss: -0.009455. Value loss: 0.121629. Entropy: 0.879135.\n",
      "Iteration 1440: Policy loss: -0.023533. Value loss: 0.074470. Entropy: 0.886749.\n",
      "episode: 1028   score: 14.0  epsilon: 1.0    steps: 720  evaluation reward: 10.8\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1441: Policy loss: 0.012840. Value loss: 0.490374. Entropy: 0.981519.\n",
      "Iteration 1442: Policy loss: -0.001599. Value loss: 0.190599. Entropy: 0.969750.\n",
      "Iteration 1443: Policy loss: -0.007158. Value loss: 0.118965. Entropy: 0.972674.\n",
      "episode: 1029   score: 17.0  epsilon: 1.0    steps: 312  evaluation reward: 10.83\n",
      "episode: 1030   score: 8.0  epsilon: 1.0    steps: 712  evaluation reward: 10.75\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1444: Policy loss: 0.004320. Value loss: 0.480858. Entropy: 0.874959.\n",
      "Iteration 1445: Policy loss: -0.009384. Value loss: 0.217805. Entropy: 0.890531.\n",
      "Iteration 1446: Policy loss: -0.013002. Value loss: 0.141452. Entropy: 0.887387.\n",
      "episode: 1031   score: 15.0  epsilon: 1.0    steps: 112  evaluation reward: 10.77\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1447: Policy loss: 0.007926. Value loss: 0.386424. Entropy: 0.903072.\n",
      "Iteration 1448: Policy loss: -0.013656. Value loss: 0.145393. Entropy: 0.902087.\n",
      "Iteration 1449: Policy loss: -0.027413. Value loss: 0.077858. Entropy: 0.902218.\n",
      "episode: 1032   score: 11.0  epsilon: 1.0    steps: 512  evaluation reward: 10.78\n",
      "episode: 1033   score: 10.0  epsilon: 1.0    steps: 536  evaluation reward: 10.76\n",
      "episode: 1034   score: 10.0  epsilon: 1.0    steps: 544  evaluation reward: 10.68\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1450: Policy loss: 0.006168. Value loss: 0.230678. Entropy: 0.895566.\n",
      "Iteration 1451: Policy loss: -0.014500. Value loss: 0.087194. Entropy: 0.910775.\n",
      "Iteration 1452: Policy loss: -0.025218. Value loss: 0.048812. Entropy: 0.911598.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1453: Policy loss: 0.002924. Value loss: 0.215246. Entropy: 0.975346.\n",
      "Iteration 1454: Policy loss: -0.011840. Value loss: 0.093970. Entropy: 0.967038.\n",
      "Iteration 1455: Policy loss: -0.026781. Value loss: 0.066850. Entropy: 0.968678.\n",
      "episode: 1035   score: 13.0  epsilon: 1.0    steps: 272  evaluation reward: 10.74\n",
      "episode: 1036   score: 8.0  epsilon: 1.0    steps: 664  evaluation reward: 10.76\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1456: Policy loss: 0.007182. Value loss: 0.178620. Entropy: 0.863307.\n",
      "Iteration 1457: Policy loss: -0.002295. Value loss: 0.077779. Entropy: 0.841604.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1458: Policy loss: -0.023776. Value loss: 0.058087. Entropy: 0.856623.\n",
      "episode: 1037   score: 10.0  epsilon: 1.0    steps: 760  evaluation reward: 10.73\n",
      "episode: 1038   score: 10.0  epsilon: 1.0    steps: 768  evaluation reward: 10.73\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1459: Policy loss: 0.003428. Value loss: 0.194650. Entropy: 0.887814.\n",
      "Iteration 1460: Policy loss: -0.016426. Value loss: 0.080036. Entropy: 0.901654.\n",
      "Iteration 1461: Policy loss: -0.030160. Value loss: 0.046743. Entropy: 0.892899.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1462: Policy loss: 0.008200. Value loss: 0.193633. Entropy: 0.973219.\n",
      "Iteration 1463: Policy loss: -0.005269. Value loss: 0.079252. Entropy: 0.969751.\n",
      "Iteration 1464: Policy loss: -0.022365. Value loss: 0.054617. Entropy: 0.961433.\n",
      "episode: 1039   score: 9.0  epsilon: 1.0    steps: 160  evaluation reward: 10.67\n",
      "episode: 1040   score: 13.0  epsilon: 1.0    steps: 288  evaluation reward: 10.63\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1465: Policy loss: 0.002631. Value loss: 0.215567. Entropy: 0.905100.\n",
      "Iteration 1466: Policy loss: -0.014561. Value loss: 0.085682. Entropy: 0.891223.\n",
      "Iteration 1467: Policy loss: -0.028557. Value loss: 0.049190. Entropy: 0.892125.\n",
      "episode: 1041   score: 11.0  epsilon: 1.0    steps: 424  evaluation reward: 10.63\n",
      "episode: 1042   score: 11.0  epsilon: 1.0    steps: 608  evaluation reward: 10.66\n",
      "episode: 1043   score: 8.0  epsilon: 1.0    steps: 824  evaluation reward: 10.59\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1468: Policy loss: 0.020316. Value loss: 0.272804. Entropy: 0.988186.\n",
      "Iteration 1469: Policy loss: -0.006452. Value loss: 0.124453. Entropy: 0.958597.\n",
      "Iteration 1470: Policy loss: -0.022836. Value loss: 0.088469. Entropy: 0.964505.\n",
      "episode: 1044   score: 6.0  epsilon: 1.0    steps: 552  evaluation reward: 10.52\n",
      "episode: 1045   score: 10.0  epsilon: 1.0    steps: 832  evaluation reward: 10.48\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1471: Policy loss: 0.006442. Value loss: 0.195017. Entropy: 0.853101.\n",
      "Iteration 1472: Policy loss: -0.015927. Value loss: 0.067404. Entropy: 0.860374.\n",
      "Iteration 1473: Policy loss: -0.024874. Value loss: 0.045508. Entropy: 0.867389.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1474: Policy loss: 0.007224. Value loss: 0.194753. Entropy: 0.865372.\n",
      "Iteration 1475: Policy loss: -0.015209. Value loss: 0.087659. Entropy: 0.867340.\n",
      "Iteration 1476: Policy loss: -0.025927. Value loss: 0.055364. Entropy: 0.866122.\n",
      "episode: 1046   score: 10.0  epsilon: 1.0    steps: 256  evaluation reward: 10.49\n",
      "episode: 1047   score: 14.0  epsilon: 1.0    steps: 320  evaluation reward: 10.53\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1477: Policy loss: 0.004766. Value loss: 0.303141. Entropy: 0.927079.\n",
      "Iteration 1478: Policy loss: -0.016681. Value loss: 0.133414. Entropy: 0.929359.\n",
      "Iteration 1479: Policy loss: -0.026350. Value loss: 0.087016. Entropy: 0.929987.\n",
      "episode: 1048   score: 7.0  epsilon: 1.0    steps: 856  evaluation reward: 10.45\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1480: Policy loss: 0.005116. Value loss: 0.421437. Entropy: 0.888766.\n",
      "Iteration 1481: Policy loss: -0.005889. Value loss: 0.192041. Entropy: 0.891414.\n",
      "Iteration 1482: Policy loss: -0.015911. Value loss: 0.132571. Entropy: 0.891582.\n",
      "episode: 1049   score: 13.0  epsilon: 1.0    steps: 320  evaluation reward: 10.48\n",
      "episode: 1050   score: 13.0  epsilon: 1.0    steps: 616  evaluation reward: 10.45\n",
      "now time :  2019-03-06 12:59:33.651416\n",
      "episode: 1051   score: 11.0  epsilon: 1.0    steps: 848  evaluation reward: 10.51\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1483: Policy loss: 0.007262. Value loss: 0.418677. Entropy: 0.912504.\n",
      "Iteration 1484: Policy loss: -0.011954. Value loss: 0.211220. Entropy: 0.893566.\n",
      "Iteration 1485: Policy loss: -0.021463. Value loss: 0.121743. Entropy: 0.902943.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1486: Policy loss: 0.008422. Value loss: 0.219941. Entropy: 0.901204.\n",
      "Iteration 1487: Policy loss: -0.012183. Value loss: 0.079919. Entropy: 0.909393.\n",
      "Iteration 1488: Policy loss: -0.024810. Value loss: 0.056454. Entropy: 0.906439.\n",
      "episode: 1052   score: 8.0  epsilon: 1.0    steps: 312  evaluation reward: 10.54\n",
      "episode: 1053   score: 11.0  epsilon: 1.0    steps: 392  evaluation reward: 10.56\n",
      "episode: 1054   score: 12.0  epsilon: 1.0    steps: 512  evaluation reward: 10.54\n",
      "episode: 1055   score: 13.0  epsilon: 1.0    steps: 528  evaluation reward: 10.62\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1489: Policy loss: 0.005075. Value loss: 0.346001. Entropy: 0.890602.\n",
      "Iteration 1490: Policy loss: -0.010962. Value loss: 0.169842. Entropy: 0.897832.\n",
      "Iteration 1491: Policy loss: -0.020709. Value loss: 0.091162. Entropy: 0.905324.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1492: Policy loss: 0.017919. Value loss: 0.374801. Entropy: 0.975609.\n",
      "Iteration 1493: Policy loss: -0.005392. Value loss: 0.177432. Entropy: 0.945762.\n",
      "Iteration 1494: Policy loss: -0.019893. Value loss: 0.109394. Entropy: 0.942582.\n",
      "episode: 1056   score: 10.0  epsilon: 1.0    steps: 872  evaluation reward: 10.58\n",
      "episode: 1057   score: 7.0  epsilon: 1.0    steps: 880  evaluation reward: 10.59\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1495: Policy loss: 0.005937. Value loss: 0.245820. Entropy: 0.852996.\n",
      "Iteration 1496: Policy loss: -0.013414. Value loss: 0.092549. Entropy: 0.847277.\n",
      "Iteration 1497: Policy loss: -0.028213. Value loss: 0.053815. Entropy: 0.844177.\n",
      "episode: 1058   score: 8.0  epsilon: 1.0    steps: 384  evaluation reward: 10.6\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1498: Policy loss: 0.003895. Value loss: 0.307069. Entropy: 0.906693.\n",
      "Iteration 1499: Policy loss: -0.020287. Value loss: 0.123793. Entropy: 0.906088.\n",
      "Iteration 1500: Policy loss: -0.026490. Value loss: 0.087707. Entropy: 0.894577.\n",
      "episode: 1059   score: 15.0  epsilon: 1.0    steps: 392  evaluation reward: 10.62\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1501: Policy loss: 0.006930. Value loss: 0.426076. Entropy: 0.915789.\n",
      "Iteration 1502: Policy loss: -0.003634. Value loss: 0.201049. Entropy: 0.915828.\n",
      "Iteration 1503: Policy loss: -0.014473. Value loss: 0.125584. Entropy: 0.908199.\n",
      "episode: 1060   score: 13.0  epsilon: 1.0    steps: 80  evaluation reward: 10.64\n",
      "episode: 1061   score: 9.0  epsilon: 1.0    steps: 104  evaluation reward: 10.63\n",
      "episode: 1062   score: 10.0  epsilon: 1.0    steps: 552  evaluation reward: 10.66\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1504: Policy loss: 0.001386. Value loss: 0.330886. Entropy: 0.938309.\n",
      "Iteration 1505: Policy loss: -0.011864. Value loss: 0.204747. Entropy: 0.913062.\n",
      "Iteration 1506: Policy loss: -0.024562. Value loss: 0.153839. Entropy: 0.919298.\n",
      "episode: 1063   score: 13.0  epsilon: 1.0    steps: 808  evaluation reward: 10.71\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1507: Policy loss: 0.009188. Value loss: 0.311793. Entropy: 0.779327.\n",
      "Iteration 1508: Policy loss: -0.012594. Value loss: 0.142943. Entropy: 0.782349.\n",
      "Iteration 1509: Policy loss: -0.024537. Value loss: 0.100767. Entropy: 0.785475.\n",
      "episode: 1064   score: 11.0  epsilon: 1.0    steps: 712  evaluation reward: 10.77\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1510: Policy loss: 0.002949. Value loss: 0.248057. Entropy: 0.895110.\n",
      "Iteration 1511: Policy loss: -0.019469. Value loss: 0.102027. Entropy: 0.887556.\n",
      "Iteration 1512: Policy loss: -0.030196. Value loss: 0.063879. Entropy: 0.884448.\n",
      "episode: 1065   score: 9.0  epsilon: 1.0    steps: 256  evaluation reward: 10.69\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1513: Policy loss: 0.004921. Value loss: 0.342025. Entropy: 0.871886.\n",
      "Iteration 1514: Policy loss: -0.011794. Value loss: 0.116218. Entropy: 0.878607.\n",
      "Iteration 1515: Policy loss: -0.020682. Value loss: 0.069838. Entropy: 0.866128.\n",
      "episode: 1066   score: 6.0  epsilon: 1.0    steps: 232  evaluation reward: 10.66\n",
      "episode: 1067   score: 10.0  epsilon: 1.0    steps: 936  evaluation reward: 10.66\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1516: Policy loss: 0.007888. Value loss: 0.233091. Entropy: 0.977440.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1517: Policy loss: -0.015074. Value loss: 0.094223. Entropy: 0.991179.\n",
      "Iteration 1518: Policy loss: -0.025978. Value loss: 0.061228. Entropy: 0.982353.\n",
      "episode: 1068   score: 10.0  epsilon: 1.0    steps: 416  evaluation reward: 10.66\n",
      "episode: 1069   score: 19.0  epsilon: 1.0    steps: 968  evaluation reward: 10.77\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1519: Policy loss: 0.005788. Value loss: 0.354369. Entropy: 1.001547.\n",
      "Iteration 1520: Policy loss: -0.008555. Value loss: 0.131320. Entropy: 0.991732.\n",
      "Iteration 1521: Policy loss: -0.019634. Value loss: 0.072902. Entropy: 0.985028.\n",
      "episode: 1070   score: 14.0  epsilon: 1.0    steps: 880  evaluation reward: 10.78\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1522: Policy loss: 0.015720. Value loss: 0.209139. Entropy: 0.910844.\n",
      "Iteration 1523: Policy loss: -0.010920. Value loss: 0.090670. Entropy: 0.901037.\n",
      "Iteration 1524: Policy loss: -0.026482. Value loss: 0.060028. Entropy: 0.905201.\n",
      "episode: 1071   score: 12.0  epsilon: 1.0    steps: 488  evaluation reward: 10.84\n",
      "episode: 1072   score: 11.0  epsilon: 1.0    steps: 776  evaluation reward: 10.87\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1525: Policy loss: 0.012091. Value loss: 0.333469. Entropy: 0.949502.\n",
      "Iteration 1526: Policy loss: -0.010914. Value loss: 0.152677. Entropy: 0.930656.\n",
      "Iteration 1527: Policy loss: -0.020378. Value loss: 0.113312. Entropy: 0.932557.\n",
      "episode: 1073   score: 9.0  epsilon: 1.0    steps: 280  evaluation reward: 10.85\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1528: Policy loss: 0.006130. Value loss: 0.207214. Entropy: 0.850208.\n",
      "Iteration 1529: Policy loss: -0.014291. Value loss: 0.080711. Entropy: 0.857374.\n",
      "Iteration 1530: Policy loss: -0.025685. Value loss: 0.049313. Entropy: 0.852684.\n",
      "episode: 1074   score: 9.0  epsilon: 1.0    steps: 440  evaluation reward: 10.81\n",
      "episode: 1075   score: 11.0  epsilon: 1.0    steps: 760  evaluation reward: 10.82\n",
      "episode: 1076   score: 9.0  epsilon: 1.0    steps: 896  evaluation reward: 10.84\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1531: Policy loss: 0.000603. Value loss: 0.228462. Entropy: 0.911847.\n",
      "Iteration 1532: Policy loss: -0.012541. Value loss: 0.097833. Entropy: 0.905111.\n",
      "Iteration 1533: Policy loss: -0.026015. Value loss: 0.063741. Entropy: 0.917030.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1534: Policy loss: 0.003275. Value loss: 0.257502. Entropy: 0.880364.\n",
      "Iteration 1535: Policy loss: -0.016508. Value loss: 0.098649. Entropy: 0.907352.\n",
      "Iteration 1536: Policy loss: -0.022056. Value loss: 0.055436. Entropy: 0.894677.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1537: Policy loss: 0.007913. Value loss: 0.235488. Entropy: 0.849533.\n",
      "Iteration 1538: Policy loss: -0.014721. Value loss: 0.084417. Entropy: 0.852596.\n",
      "Iteration 1539: Policy loss: -0.026121. Value loss: 0.046927. Entropy: 0.839139.\n",
      "episode: 1077   score: 14.0  epsilon: 1.0    steps: 800  evaluation reward: 10.85\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1540: Policy loss: 0.005383. Value loss: 0.215521. Entropy: 0.931207.\n",
      "Iteration 1541: Policy loss: -0.012469. Value loss: 0.080427. Entropy: 0.943493.\n",
      "Iteration 1542: Policy loss: -0.028309. Value loss: 0.052446. Entropy: 0.932794.\n",
      "episode: 1078   score: 12.0  epsilon: 1.0    steps: 200  evaluation reward: 10.85\n",
      "episode: 1079   score: 16.0  epsilon: 1.0    steps: 224  evaluation reward: 10.93\n",
      "episode: 1080   score: 10.0  epsilon: 1.0    steps: 680  evaluation reward: 10.9\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1543: Policy loss: 0.003870. Value loss: 0.471451. Entropy: 0.956846.\n",
      "Iteration 1544: Policy loss: 0.001381. Value loss: 0.202754. Entropy: 0.953307.\n",
      "Iteration 1545: Policy loss: -0.022555. Value loss: 0.092766. Entropy: 0.945158.\n",
      "episode: 1081   score: 17.0  epsilon: 1.0    steps: 376  evaluation reward: 10.97\n",
      "episode: 1082   score: 13.0  epsilon: 1.0    steps: 592  evaluation reward: 10.99\n",
      "episode: 1083   score: 12.0  epsilon: 1.0    steps: 712  evaluation reward: 10.99\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1546: Policy loss: 0.014410. Value loss: 0.207166. Entropy: 0.889944.\n",
      "Iteration 1547: Policy loss: -0.008188. Value loss: 0.079073. Entropy: 0.902793.\n",
      "Iteration 1548: Policy loss: -0.019935. Value loss: 0.053617. Entropy: 0.896661.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1549: Policy loss: 0.010082. Value loss: 0.432209. Entropy: 0.820197.\n",
      "Iteration 1550: Policy loss: -0.009435. Value loss: 0.213626. Entropy: 0.825115.\n",
      "Iteration 1551: Policy loss: -0.017289. Value loss: 0.121365. Entropy: 0.821224.\n",
      "episode: 1084   score: 17.0  epsilon: 1.0    steps: 32  evaluation reward: 11.07\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1552: Policy loss: 0.006764. Value loss: 0.364611. Entropy: 0.893917.\n",
      "Iteration 1553: Policy loss: 0.000514. Value loss: 0.143409. Entropy: 0.908710.\n",
      "Iteration 1554: Policy loss: -0.017839. Value loss: 0.081774. Entropy: 0.909067.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1555: Policy loss: 0.009062. Value loss: 0.502051. Entropy: 0.793284.\n",
      "Iteration 1556: Policy loss: -0.011489. Value loss: 0.214471. Entropy: 0.788008.\n",
      "Iteration 1557: Policy loss: -0.017915. Value loss: 0.120456. Entropy: 0.785325.\n",
      "episode: 1085   score: 9.0  epsilon: 1.0    steps: 336  evaluation reward: 11.14\n",
      "episode: 1086   score: 11.0  epsilon: 1.0    steps: 672  evaluation reward: 11.15\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1558: Policy loss: 0.011914. Value loss: 0.576598. Entropy: 0.983753.\n",
      "Iteration 1559: Policy loss: -0.005576. Value loss: 0.255499. Entropy: 0.988872.\n",
      "Iteration 1560: Policy loss: -0.015816. Value loss: 0.169023. Entropy: 0.982397.\n",
      "episode: 1087   score: 9.0  epsilon: 1.0    steps: 240  evaluation reward: 11.16\n",
      "episode: 1088   score: 17.0  epsilon: 1.0    steps: 336  evaluation reward: 11.18\n",
      "episode: 1089   score: 11.0  epsilon: 1.0    steps: 736  evaluation reward: 11.18\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1561: Policy loss: 0.013681. Value loss: 0.346679. Entropy: 0.925749.\n",
      "Iteration 1562: Policy loss: -0.012691. Value loss: 0.149260. Entropy: 0.934061.\n",
      "Iteration 1563: Policy loss: -0.028371. Value loss: 0.094290. Entropy: 0.927115.\n",
      "episode: 1090   score: 8.0  epsilon: 1.0    steps: 776  evaluation reward: 11.18\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1564: Policy loss: 0.007196. Value loss: 0.291193. Entropy: 0.878594.\n",
      "Iteration 1565: Policy loss: -0.009295. Value loss: 0.127833. Entropy: 0.867580.\n",
      "Iteration 1566: Policy loss: -0.025346. Value loss: 0.078040. Entropy: 0.865487.\n",
      "episode: 1091   score: 29.0  epsilon: 1.0    steps: 144  evaluation reward: 11.41\n",
      "episode: 1092   score: 14.0  epsilon: 1.0    steps: 336  evaluation reward: 11.45\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1567: Policy loss: 0.010635. Value loss: 0.225011. Entropy: 0.898082.\n",
      "Iteration 1568: Policy loss: -0.012236. Value loss: 0.109945. Entropy: 0.890070.\n",
      "Iteration 1569: Policy loss: -0.024256. Value loss: 0.084417. Entropy: 0.885245.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1570: Policy loss: 0.006721. Value loss: 0.283362. Entropy: 0.953317.\n",
      "Iteration 1571: Policy loss: -0.017182. Value loss: 0.118544. Entropy: 0.948545.\n",
      "Iteration 1572: Policy loss: -0.026758. Value loss: 0.072473. Entropy: 0.955187.\n",
      "episode: 1093   score: 10.0  epsilon: 1.0    steps: 808  evaluation reward: 11.43\n",
      "episode: 1094   score: 9.0  epsilon: 1.0    steps: 936  evaluation reward: 11.41\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1573: Policy loss: 0.007192. Value loss: 0.217723. Entropy: 0.971238.\n",
      "Iteration 1574: Policy loss: -0.016456. Value loss: 0.071412. Entropy: 0.966678.\n",
      "Iteration 1575: Policy loss: -0.027925. Value loss: 0.048378. Entropy: 0.961742.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1576: Policy loss: 0.006617. Value loss: 0.378759. Entropy: 0.904429.\n",
      "Iteration 1577: Policy loss: -0.009427. Value loss: 0.162186. Entropy: 0.896520.\n",
      "Iteration 1578: Policy loss: -0.019096. Value loss: 0.088396. Entropy: 0.905417.\n",
      "episode: 1095   score: 8.0  epsilon: 1.0    steps: 240  evaluation reward: 11.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1096   score: 14.0  epsilon: 1.0    steps: 440  evaluation reward: 11.38\n",
      "episode: 1097   score: 7.0  epsilon: 1.0    steps: 648  evaluation reward: 11.3\n",
      "episode: 1098   score: 11.0  epsilon: 1.0    steps: 744  evaluation reward: 11.23\n",
      "episode: 1099   score: 9.0  epsilon: 1.0    steps: 912  evaluation reward: 11.24\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1579: Policy loss: 0.009604. Value loss: 0.529378. Entropy: 0.926107.\n",
      "Iteration 1580: Policy loss: -0.012790. Value loss: 0.265163. Entropy: 0.915911.\n",
      "Iteration 1581: Policy loss: -0.015097. Value loss: 0.186672. Entropy: 0.918625.\n",
      "episode: 1100   score: 21.0  epsilon: 1.0    steps: 312  evaluation reward: 11.33\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1582: Policy loss: 0.007395. Value loss: 0.321938. Entropy: 0.871949.\n",
      "Iteration 1583: Policy loss: -0.007522. Value loss: 0.152280. Entropy: 0.866510.\n",
      "Iteration 1584: Policy loss: -0.022405. Value loss: 0.107793. Entropy: 0.867844.\n",
      "now time :  2019-03-06 13:01:46.112240\n",
      "episode: 1101   score: 7.0  epsilon: 1.0    steps: 1000  evaluation reward: 11.31\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1585: Policy loss: 0.004207. Value loss: 0.200961. Entropy: 0.894942.\n",
      "Iteration 1586: Policy loss: -0.012891. Value loss: 0.079804. Entropy: 0.891505.\n",
      "Iteration 1587: Policy loss: -0.023780. Value loss: 0.052890. Entropy: 0.892229.\n",
      "episode: 1102   score: 10.0  epsilon: 1.0    steps: 952  evaluation reward: 11.29\n",
      "episode: 1103   score: 7.0  epsilon: 1.0    steps: 1024  evaluation reward: 11.26\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1588: Policy loss: 0.014871. Value loss: 0.245964. Entropy: 0.848915.\n",
      "Iteration 1589: Policy loss: -0.009205. Value loss: 0.106703. Entropy: 0.857710.\n",
      "Iteration 1590: Policy loss: -0.022321. Value loss: 0.061798. Entropy: 0.861448.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1591: Policy loss: 0.004286. Value loss: 0.240352. Entropy: 0.919800.\n",
      "Iteration 1592: Policy loss: -0.014936. Value loss: 0.113854. Entropy: 0.934394.\n",
      "Iteration 1593: Policy loss: -0.025872. Value loss: 0.087819. Entropy: 0.910750.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1594: Policy loss: 0.005193. Value loss: 0.323282. Entropy: 0.917451.\n",
      "Iteration 1595: Policy loss: -0.010567. Value loss: 0.142852. Entropy: 0.908217.\n",
      "Iteration 1596: Policy loss: -0.016317. Value loss: 0.089243. Entropy: 0.908185.\n",
      "episode: 1104   score: 13.0  epsilon: 1.0    steps: 136  evaluation reward: 11.21\n",
      "episode: 1105   score: 11.0  epsilon: 1.0    steps: 488  evaluation reward: 11.17\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1597: Policy loss: 0.006718. Value loss: 0.386389. Entropy: 0.939278.\n",
      "Iteration 1598: Policy loss: -0.001870. Value loss: 0.149064. Entropy: 0.942436.\n",
      "Iteration 1599: Policy loss: -0.018456. Value loss: 0.090707. Entropy: 0.943653.\n",
      "episode: 1106   score: 11.0  epsilon: 1.0    steps: 184  evaluation reward: 11.12\n",
      "episode: 1107   score: 16.0  epsilon: 1.0    steps: 312  evaluation reward: 11.16\n",
      "episode: 1108   score: 17.0  epsilon: 1.0    steps: 472  evaluation reward: 11.24\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1600: Policy loss: 0.011897. Value loss: 0.403290. Entropy: 0.895916.\n",
      "Iteration 1601: Policy loss: -0.005337. Value loss: 0.238213. Entropy: 0.895249.\n",
      "Iteration 1602: Policy loss: -0.020108. Value loss: 0.159772. Entropy: 0.877693.\n",
      "episode: 1109   score: 9.0  epsilon: 1.0    steps: 432  evaluation reward: 11.18\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1603: Policy loss: 0.009945. Value loss: 0.364219. Entropy: 0.945126.\n",
      "Iteration 1604: Policy loss: -0.019660. Value loss: 0.117650. Entropy: 0.939250.\n",
      "Iteration 1605: Policy loss: -0.031112. Value loss: 0.070987. Entropy: 0.945424.\n",
      "episode: 1110   score: 17.0  epsilon: 1.0    steps: 1008  evaluation reward: 11.25\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1606: Policy loss: 0.010201. Value loss: 0.591295. Entropy: 0.954613.\n",
      "Iteration 1607: Policy loss: -0.003887. Value loss: 0.274653. Entropy: 0.953325.\n",
      "Iteration 1608: Policy loss: -0.011375. Value loss: 0.182705. Entropy: 0.944778.\n",
      "episode: 1111   score: 19.0  epsilon: 1.0    steps: 184  evaluation reward: 11.31\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1609: Policy loss: 0.006976. Value loss: 0.238600. Entropy: 0.967150.\n",
      "Iteration 1610: Policy loss: -0.014411. Value loss: 0.097684. Entropy: 0.955798.\n",
      "Iteration 1611: Policy loss: -0.028354. Value loss: 0.056416. Entropy: 0.952247.\n",
      "episode: 1112   score: 11.0  epsilon: 1.0    steps: 536  evaluation reward: 11.31\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1612: Policy loss: 0.007050. Value loss: 0.304167. Entropy: 0.902888.\n",
      "Iteration 1613: Policy loss: -0.010860. Value loss: 0.115862. Entropy: 0.909220.\n",
      "Iteration 1614: Policy loss: -0.026226. Value loss: 0.068611. Entropy: 0.914845.\n",
      "episode: 1113   score: 11.0  epsilon: 1.0    steps: 32  evaluation reward: 11.31\n",
      "episode: 1114   score: 9.0  epsilon: 1.0    steps: 440  evaluation reward: 11.32\n",
      "episode: 1115   score: 13.0  epsilon: 1.0    steps: 840  evaluation reward: 11.36\n",
      "episode: 1116   score: 13.0  epsilon: 1.0    steps: 992  evaluation reward: 11.38\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1615: Policy loss: 0.008619. Value loss: 0.414056. Entropy: 0.948388.\n",
      "Iteration 1616: Policy loss: -0.002681. Value loss: 0.154722. Entropy: 0.945450.\n",
      "Iteration 1617: Policy loss: -0.018242. Value loss: 0.082105. Entropy: 0.947979.\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1618: Policy loss: 0.013914. Value loss: 0.470656. Entropy: 0.836305.\n",
      "Iteration 1619: Policy loss: -0.003783. Value loss: 0.163028. Entropy: 0.842835.\n",
      "Iteration 1620: Policy loss: -0.012928. Value loss: 0.093543. Entropy: 0.851255.\n",
      "episode: 1117   score: 23.0  epsilon: 1.0    steps: 712  evaluation reward: 11.47\n",
      "episode: 1118   score: 13.0  epsilon: 1.0    steps: 984  evaluation reward: 11.46\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1621: Policy loss: 0.011658. Value loss: 0.595003. Entropy: 0.888764.\n",
      "Iteration 1622: Policy loss: -0.006803. Value loss: 0.236171. Entropy: 0.898040.\n",
      "Iteration 1623: Policy loss: -0.017284. Value loss: 0.137855. Entropy: 0.891970.\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1624: Policy loss: 0.014926. Value loss: 0.580478. Entropy: 0.907716.\n",
      "Iteration 1625: Policy loss: -0.007118. Value loss: 0.217095. Entropy: 0.901083.\n",
      "Iteration 1626: Policy loss: -0.015961. Value loss: 0.136370. Entropy: 0.890271.\n",
      "episode: 1119   score: 12.0  epsilon: 1.0    steps: 144  evaluation reward: 11.49\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1627: Policy loss: 0.000550. Value loss: 0.428351. Entropy: 0.922024.\n",
      "Iteration 1628: Policy loss: -0.009295. Value loss: 0.190003. Entropy: 0.934937.\n",
      "Iteration 1629: Policy loss: -0.023550. Value loss: 0.099848. Entropy: 0.906881.\n",
      "episode: 1120   score: 19.0  epsilon: 1.0    steps: 352  evaluation reward: 11.54\n",
      "episode: 1121   score: 10.0  epsilon: 1.0    steps: 752  evaluation reward: 11.58\n",
      "episode: 1122   score: 14.0  epsilon: 1.0    steps: 992  evaluation reward: 11.63\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1630: Policy loss: 0.010383. Value loss: 0.444114. Entropy: 0.924342.\n",
      "Iteration 1631: Policy loss: -0.009861. Value loss: 0.207632. Entropy: 0.938752.\n",
      "Iteration 1632: Policy loss: -0.016944. Value loss: 0.122179. Entropy: 0.930896.\n",
      "episode: 1123   score: 13.0  epsilon: 1.0    steps: 120  evaluation reward: 11.69\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1633: Policy loss: 0.008319. Value loss: 0.204113. Entropy: 0.901696.\n",
      "Iteration 1634: Policy loss: -0.006054. Value loss: 0.104496. Entropy: 0.909806.\n",
      "Iteration 1635: Policy loss: -0.018831. Value loss: 0.072477. Entropy: 0.901735.\n",
      "episode: 1124   score: 12.0  epsilon: 1.0    steps: 72  evaluation reward: 11.67\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1636: Policy loss: 0.001778. Value loss: 0.283178. Entropy: 0.980835.\n",
      "Iteration 1637: Policy loss: -0.017423. Value loss: 0.121310. Entropy: 0.983175.\n",
      "Iteration 1638: Policy loss: -0.027829. Value loss: 0.078496. Entropy: 0.976341.\n",
      "episode: 1125   score: 10.0  epsilon: 1.0    steps: 128  evaluation reward: 11.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1126   score: 10.0  epsilon: 1.0    steps: 416  evaluation reward: 11.75\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1639: Policy loss: 0.007203. Value loss: 0.399707. Entropy: 0.892922.\n",
      "Iteration 1640: Policy loss: -0.012740. Value loss: 0.184333. Entropy: 0.901279.\n",
      "Iteration 1641: Policy loss: -0.024325. Value loss: 0.104307. Entropy: 0.899323.\n",
      "episode: 1127   score: 13.0  epsilon: 1.0    steps: 584  evaluation reward: 11.82\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1642: Policy loss: 0.009294. Value loss: 0.240882. Entropy: 0.878542.\n",
      "Iteration 1643: Policy loss: -0.011169. Value loss: 0.090883. Entropy: 0.879468.\n",
      "Iteration 1644: Policy loss: -0.021998. Value loss: 0.051691. Entropy: 0.871585.\n",
      "episode: 1128   score: 10.0  epsilon: 1.0    steps: 704  evaluation reward: 11.78\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1645: Policy loss: 0.002869. Value loss: 0.328186. Entropy: 0.972070.\n",
      "Iteration 1646: Policy loss: -0.017073. Value loss: 0.135130. Entropy: 0.975480.\n",
      "Iteration 1647: Policy loss: -0.029386. Value loss: 0.083515. Entropy: 0.990678.\n",
      "episode: 1129   score: 8.0  epsilon: 1.0    steps: 320  evaluation reward: 11.69\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1648: Policy loss: 0.007345. Value loss: 0.510014. Entropy: 0.969960.\n",
      "Iteration 1649: Policy loss: -0.007743. Value loss: 0.294883. Entropy: 0.969692.\n",
      "Iteration 1650: Policy loss: -0.008972. Value loss: 0.192880. Entropy: 0.973243.\n",
      "episode: 1130   score: 10.0  epsilon: 1.0    steps: 184  evaluation reward: 11.71\n",
      "episode: 1131   score: 17.0  epsilon: 1.0    steps: 448  evaluation reward: 11.73\n",
      "episode: 1132   score: 7.0  epsilon: 1.0    steps: 480  evaluation reward: 11.69\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1651: Policy loss: 0.005915. Value loss: 0.224988. Entropy: 1.004357.\n",
      "Iteration 1652: Policy loss: -0.012018. Value loss: 0.085845. Entropy: 1.034100.\n",
      "Iteration 1653: Policy loss: -0.025219. Value loss: 0.054965. Entropy: 1.024045.\n",
      "episode: 1133   score: 16.0  epsilon: 1.0    steps: 336  evaluation reward: 11.75\n",
      "episode: 1134   score: 11.0  epsilon: 1.0    steps: 800  evaluation reward: 11.76\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1654: Policy loss: 0.009291. Value loss: 0.200397. Entropy: 0.912918.\n",
      "Iteration 1655: Policy loss: -0.011344. Value loss: 0.097183. Entropy: 0.928960.\n",
      "Iteration 1656: Policy loss: -0.025959. Value loss: 0.065080. Entropy: 0.924826.\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1657: Policy loss: 0.005322. Value loss: 0.261698. Entropy: 0.933705.\n",
      "Iteration 1658: Policy loss: -0.015259. Value loss: 0.085368. Entropy: 0.929979.\n",
      "Iteration 1659: Policy loss: -0.029192. Value loss: 0.063995. Entropy: 0.942644.\n",
      "episode: 1135   score: 5.0  epsilon: 1.0    steps: 272  evaluation reward: 11.68\n",
      "episode: 1136   score: 9.0  epsilon: 1.0    steps: 536  evaluation reward: 11.69\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1660: Policy loss: 0.009133. Value loss: 0.399662. Entropy: 0.968262.\n",
      "Iteration 1661: Policy loss: -0.006473. Value loss: 0.152264. Entropy: 0.963109.\n",
      "Iteration 1662: Policy loss: -0.021968. Value loss: 0.090014. Entropy: 0.961749.\n",
      "episode: 1137   score: 12.0  epsilon: 1.0    steps: 376  evaluation reward: 11.71\n",
      "episode: 1138   score: 8.0  epsilon: 1.0    steps: 488  evaluation reward: 11.69\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1663: Policy loss: 0.004305. Value loss: 0.242013. Entropy: 0.939395.\n",
      "Iteration 1664: Policy loss: -0.015395. Value loss: 0.097000. Entropy: 0.946889.\n",
      "Iteration 1665: Policy loss: -0.025327. Value loss: 0.055142. Entropy: 0.944323.\n",
      "episode: 1139   score: 9.0  epsilon: 1.0    steps: 896  evaluation reward: 11.69\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1666: Policy loss: 0.002521. Value loss: 0.151100. Entropy: 0.949787.\n",
      "Iteration 1667: Policy loss: -0.018685. Value loss: 0.056134. Entropy: 0.941025.\n",
      "Iteration 1668: Policy loss: -0.027902. Value loss: 0.039107. Entropy: 0.940690.\n",
      "episode: 1140   score: 9.0  epsilon: 1.0    steps: 648  evaluation reward: 11.65\n",
      "episode: 1141   score: 14.0  epsilon: 1.0    steps: 728  evaluation reward: 11.68\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1669: Policy loss: 0.006734. Value loss: 0.259841. Entropy: 0.962144.\n",
      "Iteration 1670: Policy loss: -0.013672. Value loss: 0.100461. Entropy: 0.952146.\n",
      "Iteration 1671: Policy loss: -0.026529. Value loss: 0.071955. Entropy: 0.941041.\n",
      "episode: 1142   score: 12.0  epsilon: 1.0    steps: 704  evaluation reward: 11.69\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1672: Policy loss: 0.010199. Value loss: 0.353508. Entropy: 1.015586.\n",
      "Iteration 1673: Policy loss: -0.009937. Value loss: 0.184103. Entropy: 0.979463.\n",
      "Iteration 1674: Policy loss: -0.023806. Value loss: 0.126107. Entropy: 0.985904.\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1675: Policy loss: 0.009054. Value loss: 0.235019. Entropy: 0.888508.\n",
      "Iteration 1676: Policy loss: -0.015775. Value loss: 0.074569. Entropy: 0.906002.\n",
      "Iteration 1677: Policy loss: -0.027284. Value loss: 0.046163. Entropy: 0.909719.\n",
      "episode: 1143   score: 15.0  epsilon: 1.0    steps: 752  evaluation reward: 11.76\n",
      "episode: 1144   score: 14.0  epsilon: 1.0    steps: 960  evaluation reward: 11.84\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1678: Policy loss: 0.015400. Value loss: 0.689138. Entropy: 0.920031.\n",
      "Iteration 1679: Policy loss: -0.003107. Value loss: 0.304681. Entropy: 0.903489.\n",
      "Iteration 1680: Policy loss: -0.010371. Value loss: 0.176214. Entropy: 0.899667.\n",
      "episode: 1145   score: 17.0  epsilon: 1.0    steps: 32  evaluation reward: 11.91\n",
      "episode: 1146   score: 14.0  epsilon: 1.0    steps: 296  evaluation reward: 11.95\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1681: Policy loss: 0.004371. Value loss: 0.335482. Entropy: 0.956136.\n",
      "Iteration 1682: Policy loss: -0.011458. Value loss: 0.152298. Entropy: 0.954520.\n",
      "Iteration 1683: Policy loss: -0.018977. Value loss: 0.094431. Entropy: 0.943693.\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1684: Policy loss: 0.010293. Value loss: 0.344707. Entropy: 0.873743.\n",
      "Iteration 1685: Policy loss: -0.001416. Value loss: 0.117796. Entropy: 0.881957.\n",
      "Iteration 1686: Policy loss: -0.015976. Value loss: 0.067217. Entropy: 0.880899.\n",
      "episode: 1147   score: 10.0  epsilon: 1.0    steps: 416  evaluation reward: 11.91\n",
      "episode: 1148   score: 9.0  epsilon: 1.0    steps: 592  evaluation reward: 11.93\n",
      "episode: 1149   score: 12.0  epsilon: 1.0    steps: 728  evaluation reward: 11.92\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1687: Policy loss: 0.004706. Value loss: 0.329724. Entropy: 0.955379.\n",
      "Iteration 1688: Policy loss: -0.015992. Value loss: 0.119943. Entropy: 0.936214.\n",
      "Iteration 1689: Policy loss: -0.029959. Value loss: 0.079351. Entropy: 0.939564.\n",
      "episode: 1150   score: 22.0  epsilon: 1.0    steps: 416  evaluation reward: 12.01\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1690: Policy loss: 0.005410. Value loss: 0.187603. Entropy: 0.975045.\n",
      "Iteration 1691: Policy loss: -0.011393. Value loss: 0.073348. Entropy: 0.960954.\n",
      "Iteration 1692: Policy loss: -0.025526. Value loss: 0.056717. Entropy: 0.956695.\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1693: Policy loss: 0.007534. Value loss: 0.283739. Entropy: 0.945056.\n",
      "Iteration 1694: Policy loss: -0.016772. Value loss: 0.128119. Entropy: 0.947307.\n",
      "Iteration 1695: Policy loss: -0.028293. Value loss: 0.074425. Entropy: 0.948397.\n",
      "now time :  2019-03-06 13:04:09.090128\n",
      "episode: 1151   score: 11.0  epsilon: 1.0    steps: 832  evaluation reward: 12.01\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1696: Policy loss: 0.007316. Value loss: 0.282110. Entropy: 0.895313.\n",
      "Iteration 1697: Policy loss: -0.009696. Value loss: 0.161676. Entropy: 0.905067.\n",
      "Iteration 1698: Policy loss: -0.020958. Value loss: 0.105047. Entropy: 0.902203.\n",
      "episode: 1152   score: 14.0  epsilon: 1.0    steps: 392  evaluation reward: 12.07\n",
      "episode: 1153   score: 14.0  epsilon: 1.0    steps: 784  evaluation reward: 12.1\n",
      "episode: 1154   score: 19.0  epsilon: 1.0    steps: 792  evaluation reward: 12.17\n",
      "Training network. lr: 0.000237. clip: 0.094940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1699: Policy loss: 0.014094. Value loss: 0.313079. Entropy: 0.880387.\n",
      "Iteration 1700: Policy loss: -0.002959. Value loss: 0.131767. Entropy: 0.883337.\n",
      "Iteration 1701: Policy loss: -0.018538. Value loss: 0.081687. Entropy: 0.892912.\n",
      "episode: 1155   score: 10.0  epsilon: 1.0    steps: 776  evaluation reward: 12.14\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1702: Policy loss: 0.002118. Value loss: 0.219735. Entropy: 0.959242.\n",
      "Iteration 1703: Policy loss: -0.019113. Value loss: 0.090237. Entropy: 0.958543.\n",
      "Iteration 1704: Policy loss: -0.027015. Value loss: 0.058220. Entropy: 0.945415.\n",
      "episode: 1156   score: 11.0  epsilon: 1.0    steps: 344  evaluation reward: 12.15\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1705: Policy loss: 0.007133. Value loss: 0.228436. Entropy: 0.935890.\n",
      "Iteration 1706: Policy loss: -0.012821. Value loss: 0.101094. Entropy: 0.939311.\n",
      "Iteration 1707: Policy loss: -0.026900. Value loss: 0.069929. Entropy: 0.939920.\n",
      "episode: 1157   score: 17.0  epsilon: 1.0    steps: 632  evaluation reward: 12.25\n",
      "episode: 1158   score: 12.0  epsilon: 1.0    steps: 696  evaluation reward: 12.29\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1708: Policy loss: 0.007096. Value loss: 0.288563. Entropy: 0.957208.\n",
      "Iteration 1709: Policy loss: -0.014228. Value loss: 0.102385. Entropy: 0.959930.\n",
      "Iteration 1710: Policy loss: -0.026112. Value loss: 0.070871. Entropy: 0.953617.\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1711: Policy loss: 0.009961. Value loss: 0.520029. Entropy: 0.989088.\n",
      "Iteration 1712: Policy loss: -0.005452. Value loss: 0.183082. Entropy: 0.978682.\n",
      "Iteration 1713: Policy loss: -0.015990. Value loss: 0.119226. Entropy: 0.973833.\n",
      "episode: 1159   score: 9.0  epsilon: 1.0    steps: 224  evaluation reward: 12.23\n",
      "episode: 1160   score: 11.0  epsilon: 1.0    steps: 656  evaluation reward: 12.21\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1714: Policy loss: 0.006070. Value loss: 0.268733. Entropy: 0.931010.\n",
      "Iteration 1715: Policy loss: -0.014366. Value loss: 0.095136. Entropy: 0.925964.\n",
      "Iteration 1716: Policy loss: -0.029200. Value loss: 0.060143. Entropy: 0.918882.\n",
      "episode: 1161   score: 8.0  epsilon: 1.0    steps: 536  evaluation reward: 12.2\n",
      "episode: 1162   score: 10.0  epsilon: 1.0    steps: 600  evaluation reward: 12.2\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1717: Policy loss: 0.007429. Value loss: 0.472466. Entropy: 0.967261.\n",
      "Iteration 1718: Policy loss: -0.005479. Value loss: 0.187552. Entropy: 0.957332.\n",
      "Iteration 1719: Policy loss: -0.018405. Value loss: 0.113816. Entropy: 0.953815.\n",
      "episode: 1163   score: 21.0  epsilon: 1.0    steps: 712  evaluation reward: 12.28\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1720: Policy loss: 0.006375. Value loss: 0.762509. Entropy: 0.886345.\n",
      "Iteration 1721: Policy loss: -0.008724. Value loss: 0.325176. Entropy: 0.879744.\n",
      "Iteration 1722: Policy loss: -0.014894. Value loss: 0.155840. Entropy: 0.884754.\n",
      "episode: 1164   score: 8.0  epsilon: 1.0    steps: 136  evaluation reward: 12.25\n",
      "episode: 1165   score: 22.0  epsilon: 1.0    steps: 776  evaluation reward: 12.38\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1723: Policy loss: 0.011464. Value loss: 0.425047. Entropy: 0.928102.\n",
      "Iteration 1724: Policy loss: -0.005692. Value loss: 0.147390. Entropy: 0.929318.\n",
      "Iteration 1725: Policy loss: -0.019958. Value loss: 0.073335. Entropy: 0.916220.\n",
      "episode: 1166   score: 16.0  epsilon: 1.0    steps: 824  evaluation reward: 12.48\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1726: Policy loss: 0.008268. Value loss: 0.311300. Entropy: 0.889701.\n",
      "Iteration 1727: Policy loss: -0.009479. Value loss: 0.127789. Entropy: 0.890402.\n",
      "Iteration 1728: Policy loss: -0.021501. Value loss: 0.073256. Entropy: 0.881988.\n",
      "episode: 1167   score: 8.0  epsilon: 1.0    steps: 800  evaluation reward: 12.46\n",
      "episode: 1168   score: 9.0  epsilon: 1.0    steps: 936  evaluation reward: 12.45\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1729: Policy loss: 0.007129. Value loss: 0.398504. Entropy: 0.924789.\n",
      "Iteration 1730: Policy loss: -0.003855. Value loss: 0.150993. Entropy: 0.932225.\n",
      "Iteration 1731: Policy loss: -0.014226. Value loss: 0.077148. Entropy: 0.925816.\n",
      "episode: 1169   score: 12.0  epsilon: 1.0    steps: 416  evaluation reward: 12.38\n",
      "episode: 1170   score: 16.0  epsilon: 1.0    steps: 760  evaluation reward: 12.4\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1732: Policy loss: 0.002244. Value loss: 0.274553. Entropy: 0.962310.\n",
      "Iteration 1733: Policy loss: -0.013588. Value loss: 0.094342. Entropy: 0.945041.\n",
      "Iteration 1734: Policy loss: -0.023958. Value loss: 0.059083. Entropy: 0.941991.\n",
      "episode: 1171   score: 8.0  epsilon: 1.0    steps: 400  evaluation reward: 12.36\n",
      "episode: 1172   score: 9.0  epsilon: 1.0    steps: 752  evaluation reward: 12.34\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1735: Policy loss: 0.005323. Value loss: 0.242835. Entropy: 0.891524.\n",
      "Iteration 1736: Policy loss: -0.009860. Value loss: 0.089523. Entropy: 0.898686.\n",
      "Iteration 1737: Policy loss: -0.021292. Value loss: 0.047808. Entropy: 0.896828.\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1738: Policy loss: 0.009946. Value loss: 0.349975. Entropy: 0.934488.\n",
      "Iteration 1739: Policy loss: -0.013668. Value loss: 0.149074. Entropy: 0.940150.\n",
      "Iteration 1740: Policy loss: -0.024783. Value loss: 0.091405. Entropy: 0.928348.\n",
      "episode: 1173   score: 10.0  epsilon: 1.0    steps: 856  evaluation reward: 12.35\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1741: Policy loss: 0.011624. Value loss: 0.330503. Entropy: 0.938954.\n",
      "Iteration 1742: Policy loss: -0.013261. Value loss: 0.127041. Entropy: 0.944169.\n",
      "Iteration 1743: Policy loss: -0.022764. Value loss: 0.065656. Entropy: 0.934186.\n",
      "episode: 1174   score: 12.0  epsilon: 1.0    steps: 120  evaluation reward: 12.38\n",
      "episode: 1175   score: 9.0  epsilon: 1.0    steps: 848  evaluation reward: 12.36\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1744: Policy loss: 0.010470. Value loss: 0.400973. Entropy: 1.010415.\n",
      "Iteration 1745: Policy loss: -0.005881. Value loss: 0.144270. Entropy: 0.988520.\n",
      "Iteration 1746: Policy loss: -0.025023. Value loss: 0.094859. Entropy: 0.999054.\n",
      "episode: 1176   score: 10.0  epsilon: 1.0    steps: 608  evaluation reward: 12.37\n",
      "episode: 1177   score: 12.0  epsilon: 1.0    steps: 896  evaluation reward: 12.35\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1747: Policy loss: 0.008571. Value loss: 0.400830. Entropy: 1.011773.\n",
      "Iteration 1748: Policy loss: -0.012811. Value loss: 0.166553. Entropy: 1.010836.\n",
      "Iteration 1749: Policy loss: -0.026925. Value loss: 0.097939. Entropy: 1.014596.\n",
      "episode: 1178   score: 12.0  epsilon: 1.0    steps: 664  evaluation reward: 12.35\n",
      "episode: 1179   score: 10.0  epsilon: 1.0    steps: 696  evaluation reward: 12.29\n",
      "episode: 1180   score: 5.0  epsilon: 1.0    steps: 928  evaluation reward: 12.24\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1750: Policy loss: 0.015102. Value loss: 0.416482. Entropy: 0.927349.\n",
      "Iteration 1751: Policy loss: -0.008342. Value loss: 0.204786. Entropy: 0.920652.\n",
      "Iteration 1752: Policy loss: -0.013117. Value loss: 0.100240. Entropy: 0.909681.\n",
      "episode: 1181   score: 3.0  epsilon: 1.0    steps: 128  evaluation reward: 12.1\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1753: Policy loss: 0.004992. Value loss: 0.215395. Entropy: 0.899684.\n",
      "Iteration 1754: Policy loss: -0.018639. Value loss: 0.100858. Entropy: 0.905494.\n",
      "Iteration 1755: Policy loss: -0.022305. Value loss: 0.056918. Entropy: 0.896195.\n",
      "episode: 1182   score: 17.0  epsilon: 1.0    steps: 272  evaluation reward: 12.14\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1756: Policy loss: 0.006477. Value loss: 0.312071. Entropy: 0.988435.\n",
      "Iteration 1757: Policy loss: -0.013792. Value loss: 0.148142. Entropy: 0.990827.\n",
      "Iteration 1758: Policy loss: -0.021390. Value loss: 0.091240. Entropy: 0.982278.\n",
      "episode: 1183   score: 11.0  epsilon: 1.0    steps: 432  evaluation reward: 12.13\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1759: Policy loss: 0.005239. Value loss: 0.195585. Entropy: 0.988548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1760: Policy loss: -0.019488. Value loss: 0.074172. Entropy: 0.984953.\n",
      "Iteration 1761: Policy loss: -0.028290. Value loss: 0.043308. Entropy: 0.974683.\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1762: Policy loss: 0.004548. Value loss: 0.164005. Entropy: 0.995727.\n",
      "Iteration 1763: Policy loss: -0.018252. Value loss: 0.068095. Entropy: 0.992514.\n",
      "Iteration 1764: Policy loss: -0.030754. Value loss: 0.052994. Entropy: 0.993463.\n",
      "episode: 1184   score: 10.0  epsilon: 1.0    steps: 632  evaluation reward: 12.06\n",
      "episode: 1185   score: 12.0  epsilon: 1.0    steps: 792  evaluation reward: 12.09\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1765: Policy loss: 0.007321. Value loss: 0.236581. Entropy: 1.031030.\n",
      "Iteration 1766: Policy loss: -0.007123. Value loss: 0.089358. Entropy: 1.012167.\n",
      "Iteration 1767: Policy loss: -0.026401. Value loss: 0.055079. Entropy: 1.033027.\n",
      "episode: 1186   score: 10.0  epsilon: 1.0    steps: 320  evaluation reward: 12.08\n",
      "episode: 1187   score: 12.0  epsilon: 1.0    steps: 800  evaluation reward: 12.11\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1768: Policy loss: 0.006417. Value loss: 0.279266. Entropy: 0.911464.\n",
      "Iteration 1769: Policy loss: -0.005602. Value loss: 0.125255. Entropy: 0.920600.\n",
      "Iteration 1770: Policy loss: -0.012567. Value loss: 0.057776. Entropy: 0.923273.\n",
      "episode: 1188   score: 9.0  epsilon: 1.0    steps: 408  evaluation reward: 12.03\n",
      "episode: 1189   score: 20.0  epsilon: 1.0    steps: 488  evaluation reward: 12.12\n",
      "episode: 1190   score: 18.0  epsilon: 1.0    steps: 1016  evaluation reward: 12.22\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1771: Policy loss: 0.010855. Value loss: 0.464370. Entropy: 0.958671.\n",
      "Iteration 1772: Policy loss: -0.006339. Value loss: 0.199399. Entropy: 0.969756.\n",
      "Iteration 1773: Policy loss: -0.010686. Value loss: 0.108388. Entropy: 0.969859.\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1774: Policy loss: 0.010850. Value loss: 0.305177. Entropy: 0.977816.\n",
      "Iteration 1775: Policy loss: -0.013995. Value loss: 0.114156. Entropy: 0.961545.\n",
      "Iteration 1776: Policy loss: -0.028170. Value loss: 0.069344. Entropy: 0.961465.\n",
      "episode: 1191   score: 12.0  epsilon: 1.0    steps: 560  evaluation reward: 12.05\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1777: Policy loss: 0.012769. Value loss: 0.529277. Entropy: 0.967897.\n",
      "Iteration 1778: Policy loss: -0.003801. Value loss: 0.268361. Entropy: 0.966866.\n",
      "Iteration 1779: Policy loss: -0.014586. Value loss: 0.163824. Entropy: 0.954677.\n",
      "episode: 1192   score: 9.0  epsilon: 1.0    steps: 720  evaluation reward: 12.0\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1780: Policy loss: 0.005157. Value loss: 0.256502. Entropy: 0.984811.\n",
      "Iteration 1781: Policy loss: -0.019441. Value loss: 0.103166. Entropy: 0.993689.\n",
      "Iteration 1782: Policy loss: -0.031481. Value loss: 0.060921. Entropy: 0.989313.\n",
      "episode: 1193   score: 5.0  epsilon: 1.0    steps: 368  evaluation reward: 11.95\n",
      "episode: 1194   score: 11.0  epsilon: 1.0    steps: 568  evaluation reward: 11.97\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1783: Policy loss: 0.001481. Value loss: 0.323590. Entropy: 0.985863.\n",
      "Iteration 1784: Policy loss: -0.015130. Value loss: 0.157199. Entropy: 0.987318.\n",
      "Iteration 1785: Policy loss: -0.028682. Value loss: 0.111181. Entropy: 0.988286.\n",
      "episode: 1195   score: 11.0  epsilon: 1.0    steps: 88  evaluation reward: 12.0\n",
      "episode: 1196   score: 15.0  epsilon: 1.0    steps: 304  evaluation reward: 12.01\n",
      "episode: 1197   score: 11.0  epsilon: 1.0    steps: 832  evaluation reward: 12.05\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1786: Policy loss: 0.004814. Value loss: 0.364608. Entropy: 0.966359.\n",
      "Iteration 1787: Policy loss: -0.011139. Value loss: 0.202022. Entropy: 0.959677.\n",
      "Iteration 1788: Policy loss: -0.029431. Value loss: 0.130159. Entropy: 0.945500.\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1789: Policy loss: 0.012487. Value loss: 0.452551. Entropy: 0.912476.\n",
      "Iteration 1790: Policy loss: -0.010985. Value loss: 0.218944. Entropy: 0.913216.\n",
      "Iteration 1791: Policy loss: -0.012881. Value loss: 0.119344. Entropy: 0.911601.\n",
      "episode: 1198   score: 9.0  epsilon: 1.0    steps: 80  evaluation reward: 12.03\n",
      "episode: 1199   score: 18.0  epsilon: 1.0    steps: 448  evaluation reward: 12.12\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1792: Policy loss: 0.006804. Value loss: 0.231656. Entropy: 0.918159.\n",
      "Iteration 1793: Policy loss: -0.015812. Value loss: 0.103619. Entropy: 0.924895.\n",
      "Iteration 1794: Policy loss: -0.025841. Value loss: 0.066557. Entropy: 0.922456.\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1795: Policy loss: 0.001226. Value loss: 0.214994. Entropy: 0.914417.\n",
      "Iteration 1796: Policy loss: -0.019980. Value loss: 0.096427. Entropy: 0.918205.\n",
      "Iteration 1797: Policy loss: -0.031016. Value loss: 0.061418. Entropy: 0.921476.\n",
      "episode: 1200   score: 7.0  epsilon: 1.0    steps: 368  evaluation reward: 11.98\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1798: Policy loss: 0.003034. Value loss: 0.441218. Entropy: 0.944728.\n",
      "Iteration 1799: Policy loss: -0.006393. Value loss: 0.171346. Entropy: 0.946141.\n",
      "Iteration 1800: Policy loss: -0.012722. Value loss: 0.100620. Entropy: 0.962259.\n",
      "now time :  2019-03-06 13:06:22.790578\n",
      "episode: 1201   score: 11.0  epsilon: 1.0    steps: 448  evaluation reward: 12.02\n",
      "episode: 1202   score: 17.0  epsilon: 1.0    steps: 448  evaluation reward: 12.09\n",
      "episode: 1203   score: 10.0  epsilon: 1.0    steps: 1000  evaluation reward: 12.12\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1801: Policy loss: 0.005871. Value loss: 0.578296. Entropy: 0.937147.\n",
      "Iteration 1802: Policy loss: -0.008318. Value loss: 0.274688. Entropy: 0.934868.\n",
      "Iteration 1803: Policy loss: -0.016246. Value loss: 0.174761. Entropy: 0.936138.\n",
      "episode: 1204   score: 16.0  epsilon: 1.0    steps: 312  evaluation reward: 12.15\n",
      "episode: 1205   score: 16.0  epsilon: 1.0    steps: 464  evaluation reward: 12.2\n",
      "episode: 1206   score: 12.0  epsilon: 1.0    steps: 792  evaluation reward: 12.21\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1804: Policy loss: 0.007208. Value loss: 0.398560. Entropy: 0.896120.\n",
      "Iteration 1805: Policy loss: -0.007667. Value loss: 0.150323. Entropy: 0.891208.\n",
      "Iteration 1806: Policy loss: -0.021739. Value loss: 0.095704. Entropy: 0.896501.\n",
      "episode: 1207   score: 10.0  epsilon: 1.0    steps: 568  evaluation reward: 12.15\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1807: Policy loss: 0.006612. Value loss: 0.312214. Entropy: 0.860133.\n",
      "Iteration 1808: Policy loss: -0.011702. Value loss: 0.154891. Entropy: 0.852648.\n",
      "Iteration 1809: Policy loss: -0.023014. Value loss: 0.096808. Entropy: 0.851205.\n",
      "episode: 1208   score: 8.0  epsilon: 1.0    steps: 344  evaluation reward: 12.06\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1810: Policy loss: 0.013439. Value loss: 0.526891. Entropy: 0.860612.\n",
      "Iteration 1811: Policy loss: -0.013933. Value loss: 0.236503. Entropy: 0.864399.\n",
      "Iteration 1812: Policy loss: -0.026763. Value loss: 0.152980. Entropy: 0.861697.\n",
      "episode: 1209   score: 6.0  epsilon: 1.0    steps: 504  evaluation reward: 12.03\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1813: Policy loss: 0.003901. Value loss: 0.388830. Entropy: 0.934173.\n",
      "Iteration 1814: Policy loss: -0.016950. Value loss: 0.149995. Entropy: 0.937558.\n",
      "Iteration 1815: Policy loss: -0.027339. Value loss: 0.107526. Entropy: 0.950078.\n",
      "episode: 1210   score: 6.0  epsilon: 1.0    steps: 136  evaluation reward: 11.92\n",
      "episode: 1211   score: 10.0  epsilon: 1.0    steps: 712  evaluation reward: 11.83\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1816: Policy loss: 0.009380. Value loss: 0.252972. Entropy: 0.899560.\n",
      "Iteration 1817: Policy loss: -0.007576. Value loss: 0.107256. Entropy: 0.891246.\n",
      "Iteration 1818: Policy loss: -0.025201. Value loss: 0.064791. Entropy: 0.889924.\n",
      "episode: 1212   score: 10.0  epsilon: 1.0    steps: 464  evaluation reward: 11.82\n",
      "episode: 1213   score: 14.0  epsilon: 1.0    steps: 808  evaluation reward: 11.85\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1819: Policy loss: 0.004940. Value loss: 0.287978. Entropy: 0.932068.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1820: Policy loss: -0.016341. Value loss: 0.144837. Entropy: 0.923727.\n",
      "Iteration 1821: Policy loss: -0.027876. Value loss: 0.099031. Entropy: 0.932524.\n",
      "episode: 1214   score: 10.0  epsilon: 1.0    steps: 776  evaluation reward: 11.86\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1822: Policy loss: 0.010275. Value loss: 0.321320. Entropy: 0.937263.\n",
      "Iteration 1823: Policy loss: -0.019473. Value loss: 0.123685. Entropy: 0.929962.\n",
      "Iteration 1824: Policy loss: -0.031566. Value loss: 0.084031. Entropy: 0.930928.\n",
      "episode: 1215   score: 7.0  epsilon: 1.0    steps: 376  evaluation reward: 11.8\n",
      "episode: 1216   score: 10.0  epsilon: 1.0    steps: 440  evaluation reward: 11.77\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1825: Policy loss: 0.006564. Value loss: 0.253332. Entropy: 0.921863.\n",
      "Iteration 1826: Policy loss: -0.013615. Value loss: 0.096205. Entropy: 0.928733.\n",
      "Iteration 1827: Policy loss: -0.027212. Value loss: 0.055144. Entropy: 0.914977.\n",
      "episode: 1217   score: 17.0  epsilon: 1.0    steps: 144  evaluation reward: 11.71\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1828: Policy loss: 0.010268. Value loss: 0.275623. Entropy: 0.897921.\n",
      "Iteration 1829: Policy loss: -0.013608. Value loss: 0.105270. Entropy: 0.907473.\n",
      "Iteration 1830: Policy loss: -0.028762. Value loss: 0.064267. Entropy: 0.900694.\n",
      "episode: 1218   score: 11.0  epsilon: 1.0    steps: 624  evaluation reward: 11.69\n",
      "episode: 1219   score: 11.0  epsilon: 1.0    steps: 864  evaluation reward: 11.68\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1831: Policy loss: 0.009017. Value loss: 0.273336. Entropy: 0.881899.\n",
      "Iteration 1832: Policy loss: -0.018525. Value loss: 0.108208. Entropy: 0.864818.\n",
      "Iteration 1833: Policy loss: -0.027315. Value loss: 0.062143. Entropy: 0.870112.\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1834: Policy loss: 0.003384. Value loss: 0.238153. Entropy: 0.883332.\n",
      "Iteration 1835: Policy loss: -0.010627. Value loss: 0.094159. Entropy: 0.897832.\n",
      "Iteration 1836: Policy loss: -0.027936. Value loss: 0.056147. Entropy: 0.878046.\n",
      "episode: 1220   score: 12.0  epsilon: 1.0    steps: 176  evaluation reward: 11.61\n",
      "episode: 1221   score: 9.0  epsilon: 1.0    steps: 888  evaluation reward: 11.6\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1837: Policy loss: 0.001752. Value loss: 0.287297. Entropy: 0.951835.\n",
      "Iteration 1838: Policy loss: -0.018531. Value loss: 0.097348. Entropy: 0.943940.\n",
      "Iteration 1839: Policy loss: -0.028621. Value loss: 0.057798. Entropy: 0.950500.\n",
      "episode: 1222   score: 14.0  epsilon: 1.0    steps: 232  evaluation reward: 11.6\n",
      "episode: 1223   score: 12.0  epsilon: 1.0    steps: 560  evaluation reward: 11.59\n",
      "episode: 1224   score: 8.0  epsilon: 1.0    steps: 856  evaluation reward: 11.55\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1840: Policy loss: 0.003716. Value loss: 0.266752. Entropy: 0.864621.\n",
      "Iteration 1841: Policy loss: -0.013259. Value loss: 0.093399. Entropy: 0.873977.\n",
      "Iteration 1842: Policy loss: -0.029381. Value loss: 0.051025. Entropy: 0.866250.\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1843: Policy loss: 0.011810. Value loss: 0.403230. Entropy: 0.880532.\n",
      "Iteration 1844: Policy loss: -0.009334. Value loss: 0.133586. Entropy: 0.886141.\n",
      "Iteration 1845: Policy loss: -0.021752. Value loss: 0.058385. Entropy: 0.872953.\n",
      "episode: 1225   score: 16.0  epsilon: 1.0    steps: 200  evaluation reward: 11.61\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1846: Policy loss: 0.007508. Value loss: 0.272608. Entropy: 0.850701.\n",
      "Iteration 1847: Policy loss: -0.015529. Value loss: 0.094059. Entropy: 0.856513.\n",
      "Iteration 1848: Policy loss: -0.022840. Value loss: 0.058567. Entropy: 0.863376.\n",
      "episode: 1226   score: 12.0  epsilon: 1.0    steps: 16  evaluation reward: 11.63\n",
      "episode: 1227   score: 18.0  epsilon: 1.0    steps: 584  evaluation reward: 11.68\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1849: Policy loss: -0.000848. Value loss: 0.581911. Entropy: 0.865042.\n",
      "Iteration 1850: Policy loss: -0.005981. Value loss: 0.271145. Entropy: 0.870447.\n",
      "Iteration 1851: Policy loss: -0.018455. Value loss: 0.130857. Entropy: 0.877910.\n",
      "episode: 1228   score: 7.0  epsilon: 1.0    steps: 344  evaluation reward: 11.65\n",
      "episode: 1229   score: 14.0  epsilon: 1.0    steps: 368  evaluation reward: 11.71\n",
      "episode: 1230   score: 13.0  epsilon: 1.0    steps: 760  evaluation reward: 11.74\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1852: Policy loss: 0.009531. Value loss: 0.576433. Entropy: 0.871803.\n",
      "Iteration 1853: Policy loss: -0.008710. Value loss: 0.215687. Entropy: 0.884015.\n",
      "Iteration 1854: Policy loss: -0.017844. Value loss: 0.125604. Entropy: 0.868263.\n",
      "episode: 1231   score: 8.0  epsilon: 1.0    steps: 88  evaluation reward: 11.65\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1855: Policy loss: 0.011736. Value loss: 0.513014. Entropy: 0.850818.\n",
      "Iteration 1856: Policy loss: -0.014667. Value loss: 0.210259. Entropy: 0.849799.\n",
      "Iteration 1857: Policy loss: -0.020091. Value loss: 0.135497. Entropy: 0.852063.\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1858: Policy loss: 0.008294. Value loss: 0.319480. Entropy: 0.891361.\n",
      "Iteration 1859: Policy loss: -0.010572. Value loss: 0.120213. Entropy: 0.886138.\n",
      "Iteration 1860: Policy loss: -0.025906. Value loss: 0.072455. Entropy: 0.888878.\n",
      "episode: 1232   score: 9.0  epsilon: 1.0    steps: 360  evaluation reward: 11.67\n",
      "episode: 1233   score: 8.0  epsilon: 1.0    steps: 400  evaluation reward: 11.59\n",
      "episode: 1234   score: 19.0  epsilon: 1.0    steps: 624  evaluation reward: 11.67\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1861: Policy loss: 0.013630. Value loss: 0.278982. Entropy: 0.920114.\n",
      "Iteration 1862: Policy loss: -0.012952. Value loss: 0.098927. Entropy: 0.919008.\n",
      "Iteration 1863: Policy loss: -0.018715. Value loss: 0.057179. Entropy: 0.919580.\n",
      "episode: 1235   score: 9.0  epsilon: 1.0    steps: 104  evaluation reward: 11.71\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1864: Policy loss: 0.005107. Value loss: 0.345931. Entropy: 0.944191.\n",
      "Iteration 1865: Policy loss: -0.015626. Value loss: 0.180317. Entropy: 0.963515.\n",
      "Iteration 1866: Policy loss: -0.024619. Value loss: 0.110280. Entropy: 0.965184.\n",
      "episode: 1236   score: 11.0  epsilon: 1.0    steps: 856  evaluation reward: 11.73\n",
      "episode: 1237   score: 10.0  epsilon: 1.0    steps: 912  evaluation reward: 11.71\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1867: Policy loss: 0.013256. Value loss: 0.318140. Entropy: 0.906279.\n",
      "Iteration 1868: Policy loss: -0.007782. Value loss: 0.111450. Entropy: 0.908965.\n",
      "Iteration 1869: Policy loss: -0.022662. Value loss: 0.056266. Entropy: 0.916847.\n",
      "episode: 1238   score: 13.0  epsilon: 1.0    steps: 120  evaluation reward: 11.76\n",
      "episode: 1239   score: 12.0  epsilon: 1.0    steps: 280  evaluation reward: 11.79\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1870: Policy loss: 0.004382. Value loss: 0.256959. Entropy: 0.900739.\n",
      "Iteration 1871: Policy loss: -0.015207. Value loss: 0.118513. Entropy: 0.905732.\n",
      "Iteration 1872: Policy loss: -0.025738. Value loss: 0.081249. Entropy: 0.895521.\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1873: Policy loss: 0.005644. Value loss: 0.323204. Entropy: 0.814276.\n",
      "Iteration 1874: Policy loss: -0.006095. Value loss: 0.099172. Entropy: 0.846331.\n",
      "Iteration 1875: Policy loss: -0.016429. Value loss: 0.053348. Entropy: 0.852441.\n",
      "episode: 1240   score: 13.0  epsilon: 1.0    steps: 624  evaluation reward: 11.83\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1876: Policy loss: 0.007028. Value loss: 0.257017. Entropy: 0.948222.\n",
      "Iteration 1877: Policy loss: -0.017484. Value loss: 0.112709. Entropy: 0.950961.\n",
      "Iteration 1878: Policy loss: -0.028198. Value loss: 0.073249. Entropy: 0.951588.\n",
      "episode: 1241   score: 13.0  epsilon: 1.0    steps: 384  evaluation reward: 11.82\n",
      "episode: 1242   score: 12.0  epsilon: 1.0    steps: 792  evaluation reward: 11.82\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1879: Policy loss: 0.013467. Value loss: 0.405961. Entropy: 0.974605.\n",
      "Iteration 1880: Policy loss: -0.006256. Value loss: 0.122706. Entropy: 0.988864.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1881: Policy loss: -0.019183. Value loss: 0.084036. Entropy: 0.974525.\n",
      "episode: 1243   score: 12.0  epsilon: 1.0    steps: 512  evaluation reward: 11.79\n",
      "episode: 1244   score: 18.0  epsilon: 1.0    steps: 688  evaluation reward: 11.83\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1882: Policy loss: 0.009944. Value loss: 0.749979. Entropy: 0.938971.\n",
      "Iteration 1883: Policy loss: -0.007536. Value loss: 0.378135. Entropy: 0.942299.\n",
      "Iteration 1884: Policy loss: -0.022273. Value loss: 0.243996. Entropy: 0.931246.\n",
      "episode: 1245   score: 18.0  epsilon: 1.0    steps: 960  evaluation reward: 11.84\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1885: Policy loss: 0.007818. Value loss: 0.448368. Entropy: 0.900951.\n",
      "Iteration 1886: Policy loss: -0.013128. Value loss: 0.119803. Entropy: 0.915607.\n",
      "Iteration 1887: Policy loss: -0.021217. Value loss: 0.050723. Entropy: 0.899789.\n",
      "episode: 1246   score: 15.0  epsilon: 1.0    steps: 304  evaluation reward: 11.85\n",
      "episode: 1247   score: 15.0  epsilon: 1.0    steps: 400  evaluation reward: 11.9\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1888: Policy loss: 0.010725. Value loss: 0.732014. Entropy: 0.974654.\n",
      "Iteration 1889: Policy loss: -0.002483. Value loss: 0.303693. Entropy: 0.983107.\n",
      "Iteration 1890: Policy loss: -0.009013. Value loss: 0.147234. Entropy: 0.971531.\n",
      "episode: 1248   score: 8.0  epsilon: 1.0    steps: 288  evaluation reward: 11.89\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1891: Policy loss: 0.012625. Value loss: 0.396542. Entropy: 0.910184.\n",
      "Iteration 1892: Policy loss: -0.004455. Value loss: 0.163771. Entropy: 0.911284.\n",
      "Iteration 1893: Policy loss: -0.017782. Value loss: 0.104743. Entropy: 0.900881.\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1894: Policy loss: 0.006630. Value loss: 0.340820. Entropy: 0.926967.\n",
      "Iteration 1895: Policy loss: -0.012657. Value loss: 0.130249. Entropy: 0.905838.\n",
      "Iteration 1896: Policy loss: -0.027392. Value loss: 0.083189. Entropy: 0.914025.\n",
      "episode: 1249   score: 11.0  epsilon: 1.0    steps: 120  evaluation reward: 11.88\n",
      "episode: 1250   score: 8.0  epsilon: 1.0    steps: 208  evaluation reward: 11.74\n",
      "now time :  2019-03-06 13:08:27.316568\n",
      "episode: 1251   score: 9.0  epsilon: 1.0    steps: 608  evaluation reward: 11.72\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1897: Policy loss: 0.009156. Value loss: 0.344443. Entropy: 0.919981.\n",
      "Iteration 1898: Policy loss: -0.008569. Value loss: 0.143955. Entropy: 0.903582.\n",
      "Iteration 1899: Policy loss: -0.026269. Value loss: 0.093419. Entropy: 0.906672.\n",
      "episode: 1252   score: 6.0  epsilon: 1.0    steps: 224  evaluation reward: 11.64\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1900: Policy loss: 0.025283. Value loss: 0.277245. Entropy: 0.891465.\n",
      "Iteration 1901: Policy loss: -0.011531. Value loss: 0.119994. Entropy: 0.867324.\n",
      "Iteration 1902: Policy loss: -0.024134. Value loss: 0.082461. Entropy: 0.876730.\n",
      "episode: 1253   score: 7.0  epsilon: 1.0    steps: 40  evaluation reward: 11.57\n",
      "episode: 1254   score: 10.0  epsilon: 1.0    steps: 144  evaluation reward: 11.48\n",
      "episode: 1255   score: 28.0  epsilon: 1.0    steps: 536  evaluation reward: 11.66\n",
      "episode: 1256   score: 10.0  epsilon: 1.0    steps: 912  evaluation reward: 11.65\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1903: Policy loss: 0.005451. Value loss: 0.346082. Entropy: 0.953651.\n",
      "Iteration 1904: Policy loss: -0.014351. Value loss: 0.143169. Entropy: 0.932421.\n",
      "Iteration 1905: Policy loss: -0.022514. Value loss: 0.098393. Entropy: 0.934432.\n",
      "episode: 1257   score: 7.0  epsilon: 1.0    steps: 1008  evaluation reward: 11.55\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1906: Policy loss: 0.003005. Value loss: 0.223105. Entropy: 0.874877.\n",
      "Iteration 1907: Policy loss: -0.011791. Value loss: 0.084564. Entropy: 0.890751.\n",
      "Iteration 1908: Policy loss: -0.024768. Value loss: 0.051188. Entropy: 0.885731.\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1909: Policy loss: 0.005540. Value loss: 0.214545. Entropy: 0.900614.\n",
      "Iteration 1910: Policy loss: -0.013867. Value loss: 0.086452. Entropy: 0.908108.\n",
      "Iteration 1911: Policy loss: -0.024781. Value loss: 0.058872. Entropy: 0.909697.\n",
      "episode: 1258   score: 14.0  epsilon: 1.0    steps: 536  evaluation reward: 11.57\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1912: Policy loss: 0.004659. Value loss: 0.273684. Entropy: 0.954463.\n",
      "Iteration 1913: Policy loss: -0.018695. Value loss: 0.105588. Entropy: 0.929757.\n",
      "Iteration 1914: Policy loss: -0.030393. Value loss: 0.066747. Entropy: 0.935193.\n",
      "episode: 1259   score: 10.0  epsilon: 1.0    steps: 664  evaluation reward: 11.58\n",
      "episode: 1260   score: 12.0  epsilon: 1.0    steps: 688  evaluation reward: 11.59\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1915: Policy loss: 0.012000. Value loss: 0.520830. Entropy: 0.927368.\n",
      "Iteration 1916: Policy loss: -0.000919. Value loss: 0.280391. Entropy: 0.915890.\n",
      "Iteration 1917: Policy loss: -0.008739. Value loss: 0.163430. Entropy: 0.919765.\n",
      "episode: 1261   score: 8.0  epsilon: 1.0    steps: 128  evaluation reward: 11.59\n",
      "episode: 1262   score: 10.0  epsilon: 1.0    steps: 592  evaluation reward: 11.59\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1918: Policy loss: 0.007401. Value loss: 0.316900. Entropy: 0.863616.\n",
      "Iteration 1919: Policy loss: -0.012261. Value loss: 0.126273. Entropy: 0.856797.\n",
      "Iteration 1920: Policy loss: -0.026033. Value loss: 0.086820. Entropy: 0.856730.\n",
      "episode: 1263   score: 19.0  epsilon: 1.0    steps: 208  evaluation reward: 11.57\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1921: Policy loss: 0.009133. Value loss: 0.226928. Entropy: 0.846175.\n",
      "Iteration 1922: Policy loss: 0.010804. Value loss: 0.077899. Entropy: 0.820489.\n",
      "Iteration 1923: Policy loss: -0.023925. Value loss: 0.053713. Entropy: 0.827932.\n",
      "episode: 1264   score: 11.0  epsilon: 1.0    steps: 504  evaluation reward: 11.6\n",
      "episode: 1265   score: 16.0  epsilon: 1.0    steps: 776  evaluation reward: 11.54\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1924: Policy loss: 0.003069. Value loss: 0.330829. Entropy: 0.899606.\n",
      "Iteration 1925: Policy loss: -0.015393. Value loss: 0.128239. Entropy: 0.893392.\n",
      "Iteration 1926: Policy loss: -0.027668. Value loss: 0.076442. Entropy: 0.890007.\n",
      "episode: 1266   score: 7.0  epsilon: 1.0    steps: 488  evaluation reward: 11.45\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1927: Policy loss: 0.010213. Value loss: 0.270453. Entropy: 0.938817.\n",
      "Iteration 1928: Policy loss: -0.011832. Value loss: 0.118120. Entropy: 0.903135.\n",
      "Iteration 1929: Policy loss: -0.020990. Value loss: 0.082413. Entropy: 0.924025.\n",
      "episode: 1267   score: 8.0  epsilon: 1.0    steps: 432  evaluation reward: 11.45\n",
      "episode: 1268   score: 12.0  epsilon: 1.0    steps: 952  evaluation reward: 11.48\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1930: Policy loss: 0.009214. Value loss: 0.263116. Entropy: 0.841430.\n",
      "Iteration 1931: Policy loss: -0.015265. Value loss: 0.106801. Entropy: 0.846365.\n",
      "Iteration 1932: Policy loss: -0.026792. Value loss: 0.068960. Entropy: 0.850332.\n",
      "episode: 1269   score: 11.0  epsilon: 1.0    steps: 904  evaluation reward: 11.47\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1933: Policy loss: 0.006406. Value loss: 0.190001. Entropy: 0.893611.\n",
      "Iteration 1934: Policy loss: -0.011441. Value loss: 0.083951. Entropy: 0.914477.\n",
      "Iteration 1935: Policy loss: -0.022738. Value loss: 0.055328. Entropy: 0.903049.\n",
      "episode: 1270   score: 11.0  epsilon: 1.0    steps: 40  evaluation reward: 11.42\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1936: Policy loss: 0.005744. Value loss: 0.137334. Entropy: 0.824841.\n",
      "Iteration 1937: Policy loss: -0.016726. Value loss: 0.064167. Entropy: 0.825898.\n",
      "Iteration 1938: Policy loss: -0.028593. Value loss: 0.038647. Entropy: 0.818575.\n",
      "episode: 1271   score: 10.0  epsilon: 1.0    steps: 320  evaluation reward: 11.44\n",
      "episode: 1272   score: 16.0  epsilon: 1.0    steps: 856  evaluation reward: 11.51\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1939: Policy loss: 0.009808. Value loss: 0.340818. Entropy: 0.879350.\n",
      "Iteration 1940: Policy loss: -0.005105. Value loss: 0.128516. Entropy: 0.875284.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1941: Policy loss: -0.019117. Value loss: 0.060674. Entropy: 0.864118.\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1942: Policy loss: 0.011448. Value loss: 0.242038. Entropy: 0.855808.\n",
      "Iteration 1943: Policy loss: -0.008880. Value loss: 0.098930. Entropy: 0.861727.\n",
      "Iteration 1944: Policy loss: -0.021079. Value loss: 0.065529. Entropy: 0.854136.\n",
      "episode: 1273   score: 16.0  epsilon: 1.0    steps: 176  evaluation reward: 11.57\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1945: Policy loss: 0.008053. Value loss: 0.289323. Entropy: 0.938878.\n",
      "Iteration 1946: Policy loss: -0.016510. Value loss: 0.113620. Entropy: 0.941346.\n",
      "Iteration 1947: Policy loss: -0.026387. Value loss: 0.079315. Entropy: 0.939838.\n",
      "episode: 1274   score: 9.0  epsilon: 1.0    steps: 240  evaluation reward: 11.54\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1948: Policy loss: 0.006570. Value loss: 0.193970. Entropy: 0.858706.\n",
      "Iteration 1949: Policy loss: -0.014888. Value loss: 0.078224. Entropy: 0.876870.\n",
      "Iteration 1950: Policy loss: -0.021004. Value loss: 0.060426. Entropy: 0.871456.\n",
      "episode: 1275   score: 15.0  epsilon: 1.0    steps: 24  evaluation reward: 11.6\n",
      "episode: 1276   score: 19.0  epsilon: 1.0    steps: 584  evaluation reward: 11.69\n",
      "episode: 1277   score: 11.0  epsilon: 1.0    steps: 1016  evaluation reward: 11.68\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1951: Policy loss: 0.008865. Value loss: 0.437727. Entropy: 0.933062.\n",
      "Iteration 1952: Policy loss: -0.004676. Value loss: 0.145255. Entropy: 0.919237.\n",
      "Iteration 1953: Policy loss: -0.018382. Value loss: 0.098933. Entropy: 0.919300.\n",
      "episode: 1278   score: 10.0  epsilon: 1.0    steps: 792  evaluation reward: 11.66\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1954: Policy loss: 0.013367. Value loss: 0.341902. Entropy: 0.922980.\n",
      "Iteration 1955: Policy loss: -0.010666. Value loss: 0.166663. Entropy: 0.940894.\n",
      "Iteration 1956: Policy loss: -0.022725. Value loss: 0.112179. Entropy: 0.935062.\n",
      "episode: 1279   score: 18.0  epsilon: 1.0    steps: 232  evaluation reward: 11.74\n",
      "episode: 1280   score: 12.0  epsilon: 1.0    steps: 360  evaluation reward: 11.81\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1957: Policy loss: 0.004431. Value loss: 0.267901. Entropy: 0.860959.\n",
      "Iteration 1958: Policy loss: -0.012417. Value loss: 0.136655. Entropy: 0.871354.\n",
      "Iteration 1959: Policy loss: -0.022984. Value loss: 0.095540. Entropy: 0.860856.\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1960: Policy loss: 0.004124. Value loss: 0.248135. Entropy: 0.863644.\n",
      "Iteration 1961: Policy loss: -0.010348. Value loss: 0.110769. Entropy: 0.855552.\n",
      "Iteration 1962: Policy loss: -0.024973. Value loss: 0.060708. Entropy: 0.864606.\n",
      "episode: 1281   score: 12.0  epsilon: 1.0    steps: 336  evaluation reward: 11.9\n",
      "episode: 1282   score: 7.0  epsilon: 1.0    steps: 352  evaluation reward: 11.8\n",
      "episode: 1283   score: 16.0  epsilon: 1.0    steps: 640  evaluation reward: 11.85\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1963: Policy loss: 0.006908. Value loss: 0.379010. Entropy: 0.865822.\n",
      "Iteration 1964: Policy loss: -0.010527. Value loss: 0.148933. Entropy: 0.848604.\n",
      "Iteration 1965: Policy loss: -0.019997. Value loss: 0.092966. Entropy: 0.853984.\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1966: Policy loss: 0.006679. Value loss: 0.267739. Entropy: 0.881167.\n",
      "Iteration 1967: Policy loss: -0.014872. Value loss: 0.112579. Entropy: 0.895555.\n",
      "Iteration 1968: Policy loss: -0.026799. Value loss: 0.070359. Entropy: 0.887896.\n",
      "episode: 1284   score: 8.0  epsilon: 1.0    steps: 528  evaluation reward: 11.83\n",
      "episode: 1285   score: 10.0  epsilon: 1.0    steps: 640  evaluation reward: 11.81\n",
      "episode: 1286   score: 13.0  epsilon: 1.0    steps: 912  evaluation reward: 11.84\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1969: Policy loss: 0.006859. Value loss: 0.481633. Entropy: 0.860705.\n",
      "Iteration 1970: Policy loss: -0.000246. Value loss: 0.221080. Entropy: 0.857636.\n",
      "Iteration 1971: Policy loss: -0.017160. Value loss: 0.137541. Entropy: 0.863813.\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1972: Policy loss: 0.009901. Value loss: 0.184572. Entropy: 0.844788.\n",
      "Iteration 1973: Policy loss: -0.014323. Value loss: 0.085950. Entropy: 0.861543.\n",
      "Iteration 1974: Policy loss: -0.023250. Value loss: 0.046094. Entropy: 0.852734.\n",
      "episode: 1287   score: 13.0  epsilon: 1.0    steps: 120  evaluation reward: 11.85\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1975: Policy loss: 0.004900. Value loss: 0.201741. Entropy: 0.890571.\n",
      "Iteration 1976: Policy loss: -0.015035. Value loss: 0.079319. Entropy: 0.886789.\n",
      "Iteration 1977: Policy loss: -0.026455. Value loss: 0.046374. Entropy: 0.893649.\n",
      "episode: 1288   score: 20.0  epsilon: 1.0    steps: 408  evaluation reward: 11.96\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1978: Policy loss: 0.003928. Value loss: 0.182369. Entropy: 0.862229.\n",
      "Iteration 1979: Policy loss: -0.017539. Value loss: 0.081469. Entropy: 0.877813.\n",
      "Iteration 1980: Policy loss: -0.027152. Value loss: 0.046700. Entropy: 0.867644.\n",
      "episode: 1289   score: 12.0  epsilon: 1.0    steps: 312  evaluation reward: 11.88\n",
      "episode: 1290   score: 12.0  epsilon: 1.0    steps: 560  evaluation reward: 11.82\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1981: Policy loss: 0.009258. Value loss: 0.263303. Entropy: 0.911910.\n",
      "Iteration 1982: Policy loss: -0.010035. Value loss: 0.100737. Entropy: 0.892095.\n",
      "Iteration 1983: Policy loss: -0.025236. Value loss: 0.069264. Entropy: 0.906008.\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1984: Policy loss: 0.007388. Value loss: 0.186269. Entropy: 0.903132.\n",
      "Iteration 1985: Policy loss: -0.012816. Value loss: 0.072130. Entropy: 0.901025.\n",
      "Iteration 1986: Policy loss: -0.029314. Value loss: 0.043543. Entropy: 0.914354.\n",
      "episode: 1291   score: 17.0  epsilon: 1.0    steps: 96  evaluation reward: 11.87\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1987: Policy loss: 0.006181. Value loss: 0.376403. Entropy: 0.892587.\n",
      "Iteration 1988: Policy loss: -0.012061. Value loss: 0.177728. Entropy: 0.872416.\n",
      "Iteration 1989: Policy loss: -0.019518. Value loss: 0.115339. Entropy: 0.864078.\n",
      "episode: 1292   score: 17.0  epsilon: 1.0    steps: 368  evaluation reward: 11.95\n",
      "episode: 1293   score: 15.0  epsilon: 1.0    steps: 440  evaluation reward: 12.05\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1990: Policy loss: 0.005386. Value loss: 0.406733. Entropy: 0.896252.\n",
      "Iteration 1991: Policy loss: -0.009972. Value loss: 0.184103. Entropy: 0.909131.\n",
      "Iteration 1992: Policy loss: -0.014179. Value loss: 0.113555. Entropy: 0.906174.\n",
      "episode: 1294   score: 19.0  epsilon: 1.0    steps: 136  evaluation reward: 12.13\n",
      "episode: 1295   score: 17.0  epsilon: 1.0    steps: 608  evaluation reward: 12.19\n",
      "episode: 1296   score: 10.0  epsilon: 1.0    steps: 792  evaluation reward: 12.14\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1993: Policy loss: 0.001552. Value loss: 0.255672. Entropy: 0.872713.\n",
      "Iteration 1994: Policy loss: -0.015441. Value loss: 0.094674. Entropy: 0.882681.\n",
      "Iteration 1995: Policy loss: -0.026229. Value loss: 0.052910. Entropy: 0.879561.\n",
      "episode: 1297   score: 10.0  epsilon: 1.0    steps: 656  evaluation reward: 12.13\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1996: Policy loss: 0.016875. Value loss: 0.448634. Entropy: 0.836836.\n",
      "Iteration 1997: Policy loss: 0.003948. Value loss: 0.149954. Entropy: 0.864768.\n",
      "Iteration 1998: Policy loss: -0.017955. Value loss: 0.096154. Entropy: 0.849736.\n",
      "episode: 1298   score: 20.0  epsilon: 1.0    steps: 792  evaluation reward: 12.24\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1999: Policy loss: 0.009647. Value loss: 0.339382. Entropy: 0.949454.\n",
      "Iteration 2000: Policy loss: -0.005641. Value loss: 0.140594. Entropy: 0.938501.\n",
      "Iteration 2001: Policy loss: -0.018502. Value loss: 0.079291. Entropy: 0.949869.\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2002: Policy loss: 0.006876. Value loss: 0.283464. Entropy: 0.885776.\n",
      "Iteration 2003: Policy loss: -0.012489. Value loss: 0.104810. Entropy: 0.892336.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2004: Policy loss: -0.024890. Value loss: 0.063668. Entropy: 0.900285.\n",
      "episode: 1299   score: 14.0  epsilon: 1.0    steps: 440  evaluation reward: 12.2\n",
      "episode: 1300   score: 11.0  epsilon: 1.0    steps: 848  evaluation reward: 12.24\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2005: Policy loss: 0.016901. Value loss: 0.579897. Entropy: 0.957166.\n",
      "Iteration 2006: Policy loss: -0.002704. Value loss: 0.184676. Entropy: 0.951098.\n",
      "Iteration 2007: Policy loss: -0.014932. Value loss: 0.110005. Entropy: 0.950915.\n",
      "now time :  2019-03-06 13:10:50.636281\n",
      "episode: 1301   score: 10.0  epsilon: 1.0    steps: 416  evaluation reward: 12.23\n",
      "episode: 1302   score: 15.0  epsilon: 1.0    steps: 480  evaluation reward: 12.21\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2008: Policy loss: 0.008262. Value loss: 0.267966. Entropy: 0.971428.\n",
      "Iteration 2009: Policy loss: -0.012867. Value loss: 0.102500. Entropy: 0.959227.\n",
      "Iteration 2010: Policy loss: -0.021217. Value loss: 0.059743. Entropy: 0.954788.\n",
      "episode: 1303   score: 11.0  epsilon: 1.0    steps: 464  evaluation reward: 12.22\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2011: Policy loss: 0.011783. Value loss: 0.251775. Entropy: 0.883749.\n",
      "Iteration 2012: Policy loss: -0.014863. Value loss: 0.110508. Entropy: 0.883483.\n",
      "Iteration 2013: Policy loss: -0.027380. Value loss: 0.068513. Entropy: 0.876990.\n",
      "episode: 1304   score: 16.0  epsilon: 1.0    steps: 488  evaluation reward: 12.22\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2014: Policy loss: 0.004014. Value loss: 0.309531. Entropy: 0.867811.\n",
      "Iteration 2015: Policy loss: -0.013353. Value loss: 0.130420. Entropy: 0.870215.\n",
      "Iteration 2016: Policy loss: -0.025065. Value loss: 0.067682. Entropy: 0.866314.\n",
      "episode: 1305   score: 14.0  epsilon: 1.0    steps: 376  evaluation reward: 12.2\n",
      "episode: 1306   score: 8.0  epsilon: 1.0    steps: 592  evaluation reward: 12.16\n",
      "episode: 1307   score: 14.0  epsilon: 1.0    steps: 920  evaluation reward: 12.2\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2017: Policy loss: 0.007065. Value loss: 0.444066. Entropy: 0.898293.\n",
      "Iteration 2018: Policy loss: -0.009948. Value loss: 0.225408. Entropy: 0.905212.\n",
      "Iteration 2019: Policy loss: -0.018870. Value loss: 0.141603. Entropy: 0.881818.\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2020: Policy loss: 0.003144. Value loss: 0.184583. Entropy: 0.868049.\n",
      "Iteration 2021: Policy loss: -0.019835. Value loss: 0.068081. Entropy: 0.862906.\n",
      "Iteration 2022: Policy loss: -0.029611. Value loss: 0.045304. Entropy: 0.866081.\n",
      "episode: 1308   score: 10.0  epsilon: 1.0    steps: 640  evaluation reward: 12.22\n",
      "episode: 1309   score: 12.0  epsilon: 1.0    steps: 888  evaluation reward: 12.28\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2023: Policy loss: 0.016284. Value loss: 0.253016. Entropy: 0.973711.\n",
      "Iteration 2024: Policy loss: -0.013661. Value loss: 0.101558. Entropy: 0.943442.\n",
      "Iteration 2025: Policy loss: -0.028545. Value loss: 0.062374. Entropy: 0.942132.\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2026: Policy loss: 0.006417. Value loss: 0.407104. Entropy: 0.906194.\n",
      "Iteration 2027: Policy loss: -0.010040. Value loss: 0.212939. Entropy: 0.879026.\n",
      "Iteration 2028: Policy loss: -0.019472. Value loss: 0.112456. Entropy: 0.868364.\n",
      "episode: 1310   score: 11.0  epsilon: 1.0    steps: 16  evaluation reward: 12.33\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2029: Policy loss: 0.009366. Value loss: 0.466381. Entropy: 0.926479.\n",
      "Iteration 2030: Policy loss: -0.007282. Value loss: 0.208888. Entropy: 0.924776.\n",
      "Iteration 2031: Policy loss: -0.019656. Value loss: 0.116522. Entropy: 0.926510.\n",
      "episode: 1311   score: 8.0  epsilon: 1.0    steps: 72  evaluation reward: 12.31\n",
      "episode: 1312   score: 19.0  epsilon: 1.0    steps: 184  evaluation reward: 12.4\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2032: Policy loss: 0.005814. Value loss: 0.191634. Entropy: 0.874708.\n",
      "Iteration 2033: Policy loss: -0.013920. Value loss: 0.074736. Entropy: 0.880395.\n",
      "Iteration 2034: Policy loss: -0.028592. Value loss: 0.045203. Entropy: 0.875708.\n",
      "episode: 1313   score: 12.0  epsilon: 1.0    steps: 152  evaluation reward: 12.38\n",
      "episode: 1314   score: 17.0  epsilon: 1.0    steps: 544  evaluation reward: 12.45\n",
      "episode: 1315   score: 14.0  epsilon: 1.0    steps: 672  evaluation reward: 12.52\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2035: Policy loss: 0.005024. Value loss: 0.456844. Entropy: 0.940591.\n",
      "Iteration 2036: Policy loss: -0.015145. Value loss: 0.185135. Entropy: 0.923476.\n",
      "Iteration 2037: Policy loss: -0.023065. Value loss: 0.106550. Entropy: 0.932807.\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2038: Policy loss: 0.002015. Value loss: 0.343388. Entropy: 0.913734.\n",
      "Iteration 2039: Policy loss: -0.011341. Value loss: 0.123423. Entropy: 0.896985.\n",
      "Iteration 2040: Policy loss: -0.018860. Value loss: 0.076422. Entropy: 0.900196.\n",
      "episode: 1316   score: 14.0  epsilon: 1.0    steps: 248  evaluation reward: 12.56\n",
      "episode: 1317   score: 8.0  epsilon: 1.0    steps: 352  evaluation reward: 12.47\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2041: Policy loss: 0.014597. Value loss: 0.620685. Entropy: 0.887259.\n",
      "Iteration 2042: Policy loss: 0.005052. Value loss: 0.231765. Entropy: 0.907189.\n",
      "Iteration 2043: Policy loss: -0.013370. Value loss: 0.138655. Entropy: 0.898109.\n",
      "episode: 1318   score: 13.0  epsilon: 1.0    steps: 672  evaluation reward: 12.49\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2044: Policy loss: 0.006984. Value loss: 0.510536. Entropy: 0.907647.\n",
      "Iteration 2045: Policy loss: -0.008571. Value loss: 0.187342. Entropy: 0.924161.\n",
      "Iteration 2046: Policy loss: -0.022320. Value loss: 0.127228. Entropy: 0.936739.\n",
      "episode: 1319   score: 22.0  epsilon: 1.0    steps: 440  evaluation reward: 12.6\n",
      "episode: 1320   score: 11.0  epsilon: 1.0    steps: 992  evaluation reward: 12.59\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2047: Policy loss: 0.009201. Value loss: 0.448381. Entropy: 0.902706.\n",
      "Iteration 2048: Policy loss: -0.010713. Value loss: 0.159262. Entropy: 0.910653.\n",
      "Iteration 2049: Policy loss: -0.026660. Value loss: 0.095367. Entropy: 0.909421.\n",
      "episode: 1321   score: 6.0  epsilon: 1.0    steps: 1024  evaluation reward: 12.56\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2050: Policy loss: 0.004398. Value loss: 0.481978. Entropy: 0.925664.\n",
      "Iteration 2051: Policy loss: -0.011038. Value loss: 0.225708. Entropy: 0.933535.\n",
      "Iteration 2052: Policy loss: -0.023581. Value loss: 0.136299. Entropy: 0.922812.\n",
      "episode: 1322   score: 11.0  epsilon: 1.0    steps: 184  evaluation reward: 12.53\n",
      "episode: 1323   score: 11.0  epsilon: 1.0    steps: 536  evaluation reward: 12.52\n",
      "episode: 1324   score: 21.0  epsilon: 1.0    steps: 592  evaluation reward: 12.65\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2053: Policy loss: 0.013259. Value loss: 0.233830. Entropy: 0.940789.\n",
      "Iteration 2054: Policy loss: -0.013317. Value loss: 0.100399. Entropy: 0.926713.\n",
      "Iteration 2055: Policy loss: -0.022192. Value loss: 0.060137. Entropy: 0.929551.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2056: Policy loss: 0.009537. Value loss: 0.223252. Entropy: 0.834028.\n",
      "Iteration 2057: Policy loss: -0.011506. Value loss: 0.078654. Entropy: 0.838821.\n",
      "Iteration 2058: Policy loss: -0.022808. Value loss: 0.051924. Entropy: 0.837853.\n",
      "episode: 1325   score: 15.0  epsilon: 1.0    steps: 80  evaluation reward: 12.64\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2059: Policy loss: 0.009851. Value loss: 0.246391. Entropy: 0.842558.\n",
      "Iteration 2060: Policy loss: -0.013805. Value loss: 0.101700. Entropy: 0.856345.\n",
      "Iteration 2061: Policy loss: -0.024387. Value loss: 0.050955. Entropy: 0.867499.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2062: Policy loss: 0.004143. Value loss: 0.254094. Entropy: 0.899295.\n",
      "Iteration 2063: Policy loss: -0.015720. Value loss: 0.097202. Entropy: 0.889640.\n",
      "Iteration 2064: Policy loss: -0.024780. Value loss: 0.058993. Entropy: 0.895190.\n",
      "episode: 1326   score: 14.0  epsilon: 1.0    steps: 456  evaluation reward: 12.66\n",
      "episode: 1327   score: 12.0  epsilon: 1.0    steps: 976  evaluation reward: 12.6\n",
      "Training network. lr: 0.000234. clip: 0.093705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2065: Policy loss: 0.011297. Value loss: 0.288050. Entropy: 0.990432.\n",
      "Iteration 2066: Policy loss: -0.009187. Value loss: 0.128058. Entropy: 0.970316.\n",
      "Iteration 2067: Policy loss: -0.028128. Value loss: 0.083383. Entropy: 0.991571.\n",
      "episode: 1328   score: 12.0  epsilon: 1.0    steps: 984  evaluation reward: 12.65\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2068: Policy loss: 0.012309. Value loss: 0.430921. Entropy: 1.003952.\n",
      "Iteration 2069: Policy loss: -0.006226. Value loss: 0.162561. Entropy: 0.994184.\n",
      "Iteration 2070: Policy loss: -0.017702. Value loss: 0.092242. Entropy: 0.989265.\n",
      "episode: 1329   score: 17.0  epsilon: 1.0    steps: 56  evaluation reward: 12.68\n",
      "episode: 1330   score: 12.0  epsilon: 1.0    steps: 536  evaluation reward: 12.67\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2071: Policy loss: 0.001322. Value loss: 0.144513. Entropy: 0.950807.\n",
      "Iteration 2072: Policy loss: -0.017407. Value loss: 0.063505. Entropy: 0.946189.\n",
      "Iteration 2073: Policy loss: -0.028339. Value loss: 0.041624. Entropy: 0.944752.\n",
      "episode: 1331   score: 15.0  epsilon: 1.0    steps: 360  evaluation reward: 12.74\n",
      "episode: 1332   score: 14.0  epsilon: 1.0    steps: 512  evaluation reward: 12.79\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2074: Policy loss: 0.003288. Value loss: 0.299526. Entropy: 0.928413.\n",
      "Iteration 2075: Policy loss: -0.018332. Value loss: 0.123821. Entropy: 0.914574.\n",
      "Iteration 2076: Policy loss: -0.028508. Value loss: 0.081812. Entropy: 0.917373.\n",
      "episode: 1333   score: 15.0  epsilon: 1.0    steps: 888  evaluation reward: 12.86\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2077: Policy loss: 0.002112. Value loss: 0.199760. Entropy: 0.923892.\n",
      "Iteration 2078: Policy loss: -0.017870. Value loss: 0.074157. Entropy: 0.932632.\n",
      "Iteration 2079: Policy loss: -0.030262. Value loss: 0.050652. Entropy: 0.934084.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2080: Policy loss: 0.006844. Value loss: 0.322267. Entropy: 0.940007.\n",
      "Iteration 2081: Policy loss: -0.010087. Value loss: 0.140193. Entropy: 0.936881.\n",
      "Iteration 2082: Policy loss: -0.025521. Value loss: 0.081335. Entropy: 0.933806.\n",
      "episode: 1334   score: 15.0  epsilon: 1.0    steps: 544  evaluation reward: 12.82\n",
      "episode: 1335   score: 12.0  epsilon: 1.0    steps: 800  evaluation reward: 12.85\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2083: Policy loss: 0.006765. Value loss: 0.253082. Entropy: 0.992362.\n",
      "Iteration 2084: Policy loss: -0.018640. Value loss: 0.101015. Entropy: 1.002693.\n",
      "Iteration 2085: Policy loss: -0.030355. Value loss: 0.065873. Entropy: 0.993278.\n",
      "episode: 1336   score: 10.0  epsilon: 1.0    steps: 400  evaluation reward: 12.84\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2086: Policy loss: 0.006594. Value loss: 0.344314. Entropy: 0.969243.\n",
      "Iteration 2087: Policy loss: -0.011012. Value loss: 0.111963. Entropy: 0.971196.\n",
      "Iteration 2088: Policy loss: -0.021118. Value loss: 0.067815. Entropy: 0.965583.\n",
      "episode: 1337   score: 12.0  epsilon: 1.0    steps: 112  evaluation reward: 12.86\n",
      "episode: 1338   score: 11.0  epsilon: 1.0    steps: 248  evaluation reward: 12.84\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2089: Policy loss: 0.008279. Value loss: 0.346321. Entropy: 0.996625.\n",
      "Iteration 2090: Policy loss: -0.007944. Value loss: 0.130685. Entropy: 0.990417.\n",
      "Iteration 2091: Policy loss: -0.023792. Value loss: 0.086785. Entropy: 0.975418.\n",
      "episode: 1339   score: 19.0  epsilon: 1.0    steps: 984  evaluation reward: 12.91\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2092: Policy loss: 0.011403. Value loss: 0.206045. Entropy: 0.954133.\n",
      "Iteration 2093: Policy loss: -0.008962. Value loss: 0.079678. Entropy: 0.949756.\n",
      "Iteration 2094: Policy loss: -0.023584. Value loss: 0.049828. Entropy: 0.952083.\n",
      "episode: 1340   score: 16.0  epsilon: 1.0    steps: 704  evaluation reward: 12.94\n",
      "episode: 1341   score: 13.0  epsilon: 1.0    steps: 952  evaluation reward: 12.94\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2095: Policy loss: 0.004581. Value loss: 0.313391. Entropy: 0.987355.\n",
      "Iteration 2096: Policy loss: -0.011303. Value loss: 0.117977. Entropy: 0.987589.\n",
      "Iteration 2097: Policy loss: -0.024664. Value loss: 0.073857. Entropy: 0.976318.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2098: Policy loss: 0.008261. Value loss: 0.244892. Entropy: 0.950820.\n",
      "Iteration 2099: Policy loss: -0.006100. Value loss: 0.104448. Entropy: 0.958063.\n",
      "Iteration 2100: Policy loss: -0.023082. Value loss: 0.063592. Entropy: 0.945365.\n",
      "episode: 1342   score: 11.0  epsilon: 1.0    steps: 392  evaluation reward: 12.93\n",
      "episode: 1343   score: 11.0  epsilon: 1.0    steps: 832  evaluation reward: 12.92\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2101: Policy loss: 0.006514. Value loss: 0.345188. Entropy: 0.998880.\n",
      "Iteration 2102: Policy loss: -0.018566. Value loss: 0.139473. Entropy: 0.993360.\n",
      "Iteration 2103: Policy loss: -0.028760. Value loss: 0.093244. Entropy: 0.998550.\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2104: Policy loss: 0.007654. Value loss: 0.334808. Entropy: 0.952713.\n",
      "Iteration 2105: Policy loss: -0.010754. Value loss: 0.176889. Entropy: 0.949354.\n",
      "Iteration 2106: Policy loss: -0.016847. Value loss: 0.093931. Entropy: 0.960949.\n",
      "episode: 1344   score: 14.0  epsilon: 1.0    steps: 88  evaluation reward: 12.88\n",
      "episode: 1345   score: 9.0  epsilon: 1.0    steps: 600  evaluation reward: 12.79\n",
      "episode: 1346   score: 18.0  epsilon: 1.0    steps: 872  evaluation reward: 12.82\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2107: Policy loss: 0.010604. Value loss: 0.558114. Entropy: 0.970563.\n",
      "Iteration 2108: Policy loss: -0.011929. Value loss: 0.173040. Entropy: 0.975943.\n",
      "Iteration 2109: Policy loss: -0.024151. Value loss: 0.085319. Entropy: 0.982729.\n",
      "episode: 1347   score: 17.0  epsilon: 1.0    steps: 440  evaluation reward: 12.84\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2110: Policy loss: 0.004107. Value loss: 0.410574. Entropy: 0.916136.\n",
      "Iteration 2111: Policy loss: -0.013715. Value loss: 0.152710. Entropy: 0.902835.\n",
      "Iteration 2112: Policy loss: -0.020960. Value loss: 0.086077. Entropy: 0.918864.\n",
      "episode: 1348   score: 23.0  epsilon: 1.0    steps: 728  evaluation reward: 12.99\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2113: Policy loss: 0.006728. Value loss: 0.624002. Entropy: 0.962259.\n",
      "Iteration 2114: Policy loss: -0.006465. Value loss: 0.272705. Entropy: 0.950921.\n",
      "Iteration 2115: Policy loss: -0.012102. Value loss: 0.173961. Entropy: 0.954572.\n",
      "episode: 1349   score: 9.0  epsilon: 1.0    steps: 128  evaluation reward: 12.97\n",
      "episode: 1350   score: 14.0  epsilon: 1.0    steps: 344  evaluation reward: 13.03\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2116: Policy loss: 0.007209. Value loss: 0.274685. Entropy: 0.932849.\n",
      "Iteration 2117: Policy loss: -0.013005. Value loss: 0.130826. Entropy: 0.929450.\n",
      "Iteration 2118: Policy loss: -0.020781. Value loss: 0.078137. Entropy: 0.935391.\n",
      "now time :  2019-03-06 13:13:16.515445\n",
      "episode: 1351   score: 5.0  epsilon: 1.0    steps: 488  evaluation reward: 12.99\n",
      "episode: 1352   score: 12.0  epsilon: 1.0    steps: 776  evaluation reward: 13.05\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2119: Policy loss: 0.006918. Value loss: 0.350307. Entropy: 0.927913.\n",
      "Iteration 2120: Policy loss: -0.014015. Value loss: 0.113181. Entropy: 0.915460.\n",
      "Iteration 2121: Policy loss: -0.024353. Value loss: 0.068652. Entropy: 0.918936.\n",
      "episode: 1353   score: 11.0  epsilon: 1.0    steps: 992  evaluation reward: 13.09\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2122: Policy loss: 0.005886. Value loss: 0.255623. Entropy: 0.958624.\n",
      "Iteration 2123: Policy loss: -0.013776. Value loss: 0.099556. Entropy: 0.958901.\n",
      "Iteration 2124: Policy loss: -0.028864. Value loss: 0.070686. Entropy: 0.957697.\n",
      "episode: 1354   score: 11.0  epsilon: 1.0    steps: 320  evaluation reward: 13.1\n",
      "episode: 1355   score: 10.0  epsilon: 1.0    steps: 416  evaluation reward: 12.92\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2125: Policy loss: 0.009503. Value loss: 0.277341. Entropy: 0.971348.\n",
      "Iteration 2126: Policy loss: -0.018013. Value loss: 0.090617. Entropy: 0.979100.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2127: Policy loss: -0.027780. Value loss: 0.057506. Entropy: 0.973666.\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2128: Policy loss: 0.006338. Value loss: 0.380209. Entropy: 0.956306.\n",
      "Iteration 2129: Policy loss: -0.013518. Value loss: 0.128799. Entropy: 0.954074.\n",
      "Iteration 2130: Policy loss: -0.017475. Value loss: 0.064597. Entropy: 0.951406.\n",
      "episode: 1356   score: 11.0  epsilon: 1.0    steps: 408  evaluation reward: 12.93\n",
      "episode: 1357   score: 11.0  epsilon: 1.0    steps: 424  evaluation reward: 12.97\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2131: Policy loss: 0.015744. Value loss: 0.300083. Entropy: 0.954482.\n",
      "Iteration 2132: Policy loss: -0.013426. Value loss: 0.155588. Entropy: 0.933046.\n",
      "Iteration 2133: Policy loss: -0.026384. Value loss: 0.116340. Entropy: 0.934705.\n",
      "episode: 1358   score: 17.0  epsilon: 1.0    steps: 648  evaluation reward: 13.0\n",
      "episode: 1359   score: 10.0  epsilon: 1.0    steps: 712  evaluation reward: 13.0\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2134: Policy loss: 0.007841. Value loss: 0.252430. Entropy: 0.911249.\n",
      "Iteration 2135: Policy loss: -0.020071. Value loss: 0.099846. Entropy: 0.905718.\n",
      "Iteration 2136: Policy loss: -0.030621. Value loss: 0.058421. Entropy: 0.909953.\n",
      "episode: 1360   score: 13.0  epsilon: 1.0    steps: 904  evaluation reward: 13.01\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2137: Policy loss: 0.007922. Value loss: 0.418166. Entropy: 0.928709.\n",
      "Iteration 2138: Policy loss: -0.007044. Value loss: 0.190525. Entropy: 0.916746.\n",
      "Iteration 2139: Policy loss: -0.022159. Value loss: 0.093622. Entropy: 0.921700.\n",
      "episode: 1361   score: 9.0  epsilon: 1.0    steps: 776  evaluation reward: 13.02\n",
      "episode: 1362   score: 11.0  epsilon: 1.0    steps: 856  evaluation reward: 13.03\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2140: Policy loss: 0.006746. Value loss: 0.231079. Entropy: 0.834722.\n",
      "Iteration 2141: Policy loss: -0.013616. Value loss: 0.071946. Entropy: 0.843247.\n",
      "Iteration 2142: Policy loss: -0.029006. Value loss: 0.042280. Entropy: 0.834771.\n",
      "episode: 1363   score: 18.0  epsilon: 1.0    steps: 984  evaluation reward: 13.02\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2143: Policy loss: 0.004413. Value loss: 0.442788. Entropy: 0.947578.\n",
      "Iteration 2144: Policy loss: -0.013176. Value loss: 0.192841. Entropy: 0.942942.\n",
      "Iteration 2145: Policy loss: -0.021520. Value loss: 0.117644. Entropy: 0.932239.\n",
      "episode: 1364   score: 14.0  epsilon: 1.0    steps: 920  evaluation reward: 13.05\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2146: Policy loss: 0.008395. Value loss: 0.365461. Entropy: 0.942350.\n",
      "Iteration 2147: Policy loss: -0.009757. Value loss: 0.140240. Entropy: 0.944648.\n",
      "Iteration 2148: Policy loss: -0.021408. Value loss: 0.082635. Entropy: 0.946804.\n",
      "episode: 1365   score: 17.0  epsilon: 1.0    steps: 552  evaluation reward: 13.06\n",
      "episode: 1366   score: 8.0  epsilon: 1.0    steps: 584  evaluation reward: 13.07\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2149: Policy loss: 0.003751. Value loss: 0.235853. Entropy: 0.917794.\n",
      "Iteration 2150: Policy loss: -0.013961. Value loss: 0.118570. Entropy: 0.919116.\n",
      "Iteration 2151: Policy loss: -0.021881. Value loss: 0.090768. Entropy: 0.921712.\n",
      "episode: 1367   score: 11.0  epsilon: 1.0    steps: 448  evaluation reward: 13.1\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2152: Policy loss: 0.010652. Value loss: 0.305807. Entropy: 0.895086.\n",
      "Iteration 2153: Policy loss: -0.012085. Value loss: 0.143826. Entropy: 0.881363.\n",
      "Iteration 2154: Policy loss: -0.026739. Value loss: 0.102217. Entropy: 0.871807.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2155: Policy loss: 0.005891. Value loss: 0.182471. Entropy: 0.898418.\n",
      "Iteration 2156: Policy loss: -0.017046. Value loss: 0.063890. Entropy: 0.905457.\n",
      "Iteration 2157: Policy loss: -0.026323. Value loss: 0.034605. Entropy: 0.905901.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2158: Policy loss: 0.008053. Value loss: 0.513911. Entropy: 0.896408.\n",
      "Iteration 2159: Policy loss: -0.011509. Value loss: 0.226236. Entropy: 0.900780.\n",
      "Iteration 2160: Policy loss: -0.017808. Value loss: 0.133512. Entropy: 0.898848.\n",
      "episode: 1368   score: 19.0  epsilon: 1.0    steps: 144  evaluation reward: 13.17\n",
      "episode: 1369   score: 12.0  epsilon: 1.0    steps: 264  evaluation reward: 13.18\n",
      "episode: 1370   score: 10.0  epsilon: 1.0    steps: 344  evaluation reward: 13.17\n",
      "episode: 1371   score: 13.0  epsilon: 1.0    steps: 728  evaluation reward: 13.2\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2161: Policy loss: 0.013945. Value loss: 0.569667. Entropy: 0.964159.\n",
      "Iteration 2162: Policy loss: -0.002536. Value loss: 0.230771. Entropy: 0.945783.\n",
      "Iteration 2163: Policy loss: -0.014545. Value loss: 0.134626. Entropy: 0.940705.\n",
      "episode: 1372   score: 10.0  epsilon: 1.0    steps: 656  evaluation reward: 13.14\n",
      "episode: 1373   score: 10.0  epsilon: 1.0    steps: 856  evaluation reward: 13.08\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2164: Policy loss: 0.005733. Value loss: 0.293974. Entropy: 0.932912.\n",
      "Iteration 2165: Policy loss: -0.010454. Value loss: 0.109495. Entropy: 0.928104.\n",
      "Iteration 2166: Policy loss: -0.022416. Value loss: 0.073038. Entropy: 0.935475.\n",
      "episode: 1374   score: 16.0  epsilon: 1.0    steps: 232  evaluation reward: 13.15\n",
      "episode: 1375   score: 16.0  epsilon: 1.0    steps: 608  evaluation reward: 13.16\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2167: Policy loss: 0.001123. Value loss: 0.268110. Entropy: 0.872858.\n",
      "Iteration 2168: Policy loss: -0.018072. Value loss: 0.099362. Entropy: 0.860884.\n",
      "Iteration 2169: Policy loss: -0.026205. Value loss: 0.062618. Entropy: 0.867261.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2170: Policy loss: 0.010360. Value loss: 0.234473. Entropy: 0.886199.\n",
      "Iteration 2171: Policy loss: -0.011761. Value loss: 0.087509. Entropy: 0.887216.\n",
      "Iteration 2172: Policy loss: -0.026631. Value loss: 0.047482. Entropy: 0.896207.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2173: Policy loss: 0.005170. Value loss: 0.190048. Entropy: 0.964671.\n",
      "Iteration 2174: Policy loss: -0.010944. Value loss: 0.080349. Entropy: 0.954608.\n",
      "Iteration 2175: Policy loss: -0.025641. Value loss: 0.055124. Entropy: 0.961932.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2176: Policy loss: 0.006891. Value loss: 0.614346. Entropy: 0.930544.\n",
      "Iteration 2177: Policy loss: -0.011103. Value loss: 0.328176. Entropy: 0.943218.\n",
      "Iteration 2178: Policy loss: -0.022927. Value loss: 0.188630. Entropy: 0.946526.\n",
      "episode: 1376   score: 13.0  epsilon: 1.0    steps: 96  evaluation reward: 13.1\n",
      "episode: 1377   score: 11.0  epsilon: 1.0    steps: 200  evaluation reward: 13.1\n",
      "episode: 1378   score: 19.0  epsilon: 1.0    steps: 456  evaluation reward: 13.19\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2179: Policy loss: 0.008910. Value loss: 0.398350. Entropy: 0.939569.\n",
      "Iteration 2180: Policy loss: -0.012326. Value loss: 0.146481. Entropy: 0.954476.\n",
      "Iteration 2181: Policy loss: -0.019453. Value loss: 0.085722. Entropy: 0.945528.\n",
      "episode: 1379   score: 11.0  epsilon: 1.0    steps: 424  evaluation reward: 13.12\n",
      "episode: 1380   score: 20.0  epsilon: 1.0    steps: 856  evaluation reward: 13.2\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2182: Policy loss: 0.007293. Value loss: 0.513929. Entropy: 0.957344.\n",
      "Iteration 2183: Policy loss: -0.005891. Value loss: 0.181046. Entropy: 0.957028.\n",
      "Iteration 2184: Policy loss: -0.022954. Value loss: 0.103184. Entropy: 0.951790.\n",
      "episode: 1381   score: 18.0  epsilon: 1.0    steps: 232  evaluation reward: 13.26\n",
      "episode: 1382   score: 14.0  epsilon: 1.0    steps: 616  evaluation reward: 13.33\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2185: Policy loss: 0.009165. Value loss: 0.184611. Entropy: 0.879928.\n",
      "Iteration 2186: Policy loss: -0.016665. Value loss: 0.050942. Entropy: 0.880031.\n",
      "Iteration 2187: Policy loss: -0.025310. Value loss: 0.029723. Entropy: 0.878783.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2188: Policy loss: 0.007251. Value loss: 0.221715. Entropy: 0.944821.\n",
      "Iteration 2189: Policy loss: -0.012746. Value loss: 0.103727. Entropy: 0.949268.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2190: Policy loss: -0.028783. Value loss: 0.069443. Entropy: 0.940223.\n",
      "episode: 1383   score: 19.0  epsilon: 1.0    steps: 440  evaluation reward: 13.36\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2191: Policy loss: 0.005421. Value loss: 0.493050. Entropy: 0.964396.\n",
      "Iteration 2192: Policy loss: -0.006249. Value loss: 0.182818. Entropy: 0.960808.\n",
      "Iteration 2193: Policy loss: -0.018462. Value loss: 0.111066. Entropy: 0.968387.\n",
      "episode: 1384   score: 13.0  epsilon: 1.0    steps: 264  evaluation reward: 13.41\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2194: Policy loss: 0.003230. Value loss: 0.201443. Entropy: 0.955197.\n",
      "Iteration 2195: Policy loss: -0.016879. Value loss: 0.076741. Entropy: 0.954670.\n",
      "Iteration 2196: Policy loss: -0.026546. Value loss: 0.044621. Entropy: 0.953244.\n",
      "episode: 1385   score: 11.0  epsilon: 1.0    steps: 448  evaluation reward: 13.42\n",
      "episode: 1386   score: 16.0  epsilon: 1.0    steps: 616  evaluation reward: 13.45\n",
      "episode: 1387   score: 14.0  epsilon: 1.0    steps: 896  evaluation reward: 13.46\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2197: Policy loss: 0.005196. Value loss: 0.401284. Entropy: 0.950005.\n",
      "Iteration 2198: Policy loss: -0.005359. Value loss: 0.180675. Entropy: 0.945768.\n",
      "Iteration 2199: Policy loss: -0.018736. Value loss: 0.087914. Entropy: 0.945092.\n",
      "episode: 1388   score: 15.0  epsilon: 1.0    steps: 608  evaluation reward: 13.41\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2200: Policy loss: 0.018313. Value loss: 0.218760. Entropy: 0.867651.\n",
      "Iteration 2201: Policy loss: -0.015673. Value loss: 0.080817. Entropy: 0.856089.\n",
      "Iteration 2202: Policy loss: -0.027598. Value loss: 0.048998. Entropy: 0.857304.\n",
      "episode: 1389   score: 12.0  epsilon: 1.0    steps: 336  evaluation reward: 13.41\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2203: Policy loss: 0.009553. Value loss: 0.391377. Entropy: 0.855190.\n",
      "Iteration 2204: Policy loss: -0.004434. Value loss: 0.136728. Entropy: 0.866763.\n",
      "Iteration 2205: Policy loss: -0.019675. Value loss: 0.084551. Entropy: 0.873065.\n",
      "episode: 1390   score: 13.0  epsilon: 1.0    steps: 216  evaluation reward: 13.42\n",
      "episode: 1391   score: 16.0  epsilon: 1.0    steps: 624  evaluation reward: 13.41\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2206: Policy loss: 0.004186. Value loss: 0.310191. Entropy: 0.990409.\n",
      "Iteration 2207: Policy loss: -0.018151. Value loss: 0.118854. Entropy: 0.971939.\n",
      "Iteration 2208: Policy loss: -0.028487. Value loss: 0.071755. Entropy: 0.975811.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2209: Policy loss: 0.006198. Value loss: 0.343665. Entropy: 0.959071.\n",
      "Iteration 2210: Policy loss: -0.012533. Value loss: 0.162371. Entropy: 0.958498.\n",
      "Iteration 2211: Policy loss: -0.027304. Value loss: 0.114596. Entropy: 0.958213.\n",
      "episode: 1392   score: 17.0  epsilon: 1.0    steps: 976  evaluation reward: 13.41\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2212: Policy loss: 0.000588. Value loss: 0.393915. Entropy: 0.915720.\n",
      "Iteration 2213: Policy loss: -0.006984. Value loss: 0.146488. Entropy: 0.924787.\n",
      "Iteration 2214: Policy loss: -0.014787. Value loss: 0.084497. Entropy: 0.917696.\n",
      "episode: 1393   score: 14.0  epsilon: 1.0    steps: 136  evaluation reward: 13.4\n",
      "episode: 1394   score: 9.0  epsilon: 1.0    steps: 272  evaluation reward: 13.3\n",
      "episode: 1395   score: 11.0  epsilon: 1.0    steps: 560  evaluation reward: 13.24\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2215: Policy loss: 0.010879. Value loss: 0.540401. Entropy: 0.874477.\n",
      "Iteration 2216: Policy loss: -0.004583. Value loss: 0.239714. Entropy: 0.868055.\n",
      "Iteration 2217: Policy loss: -0.015573. Value loss: 0.151482. Entropy: 0.871217.\n",
      "episode: 1396   score: 18.0  epsilon: 1.0    steps: 320  evaluation reward: 13.32\n",
      "episode: 1397   score: 7.0  epsilon: 1.0    steps: 520  evaluation reward: 13.29\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2218: Policy loss: 0.007841. Value loss: 0.459048. Entropy: 0.941974.\n",
      "Iteration 2219: Policy loss: -0.003959. Value loss: 0.180569. Entropy: 0.957076.\n",
      "Iteration 2220: Policy loss: -0.016683. Value loss: 0.120889. Entropy: 0.942536.\n",
      "episode: 1398   score: 10.0  epsilon: 1.0    steps: 352  evaluation reward: 13.19\n",
      "episode: 1399   score: 13.0  epsilon: 1.0    steps: 848  evaluation reward: 13.18\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2221: Policy loss: 0.006973. Value loss: 0.276988. Entropy: 0.915248.\n",
      "Iteration 2222: Policy loss: -0.015295. Value loss: 0.094530. Entropy: 0.914736.\n",
      "Iteration 2223: Policy loss: -0.023409. Value loss: 0.054330. Entropy: 0.910071.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2224: Policy loss: 0.004353. Value loss: 0.254218. Entropy: 0.908295.\n",
      "Iteration 2225: Policy loss: -0.012600. Value loss: 0.106398. Entropy: 0.942446.\n",
      "Iteration 2226: Policy loss: -0.025632. Value loss: 0.056097. Entropy: 0.923118.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2227: Policy loss: 0.008688. Value loss: 0.368205. Entropy: 0.954105.\n",
      "Iteration 2228: Policy loss: -0.005719. Value loss: 0.173735. Entropy: 0.962024.\n",
      "Iteration 2229: Policy loss: -0.009593. Value loss: 0.109983. Entropy: 0.965642.\n",
      "episode: 1400   score: 11.0  epsilon: 1.0    steps: 432  evaluation reward: 13.18\n",
      "now time :  2019-03-06 13:15:40.730974\n",
      "episode: 1401   score: 10.0  epsilon: 1.0    steps: 888  evaluation reward: 13.18\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2230: Policy loss: 0.005715. Value loss: 0.284841. Entropy: 0.983258.\n",
      "Iteration 2231: Policy loss: -0.017908. Value loss: 0.107808. Entropy: 0.971746.\n",
      "Iteration 2232: Policy loss: -0.030167. Value loss: 0.075600. Entropy: 0.972725.\n",
      "episode: 1402   score: 16.0  epsilon: 1.0    steps: 64  evaluation reward: 13.19\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2233: Policy loss: 0.006668. Value loss: 0.178683. Entropy: 0.957890.\n",
      "Iteration 2234: Policy loss: -0.013813. Value loss: 0.074159. Entropy: 0.956372.\n",
      "Iteration 2235: Policy loss: -0.025088. Value loss: 0.046830. Entropy: 0.963219.\n",
      "episode: 1403   score: 14.0  epsilon: 1.0    steps: 408  evaluation reward: 13.22\n",
      "episode: 1404   score: 13.0  epsilon: 1.0    steps: 824  evaluation reward: 13.19\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2236: Policy loss: 0.008748. Value loss: 0.199454. Entropy: 0.998946.\n",
      "Iteration 2237: Policy loss: -0.015599. Value loss: 0.073436. Entropy: 0.990812.\n",
      "Iteration 2238: Policy loss: -0.028499. Value loss: 0.052614. Entropy: 0.995525.\n",
      "episode: 1405   score: 15.0  epsilon: 1.0    steps: 152  evaluation reward: 13.2\n",
      "episode: 1406   score: 10.0  epsilon: 1.0    steps: 320  evaluation reward: 13.22\n",
      "episode: 1407   score: 13.0  epsilon: 1.0    steps: 880  evaluation reward: 13.21\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2239: Policy loss: 0.013386. Value loss: 0.270520. Entropy: 0.980538.\n",
      "Iteration 2240: Policy loss: -0.018820. Value loss: 0.129707. Entropy: 0.962576.\n",
      "Iteration 2241: Policy loss: -0.029491. Value loss: 0.089375. Entropy: 0.964400.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2242: Policy loss: 0.006354. Value loss: 0.420406. Entropy: 0.945836.\n",
      "Iteration 2243: Policy loss: 0.000472. Value loss: 0.162457. Entropy: 0.935016.\n",
      "Iteration 2244: Policy loss: -0.016058. Value loss: 0.088028. Entropy: 0.939551.\n",
      "episode: 1408   score: 17.0  epsilon: 1.0    steps: 656  evaluation reward: 13.28\n",
      "episode: 1409   score: 10.0  epsilon: 1.0    steps: 872  evaluation reward: 13.26\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2245: Policy loss: 0.008885. Value loss: 0.422800. Entropy: 0.967510.\n",
      "Iteration 2246: Policy loss: -0.011440. Value loss: 0.141876. Entropy: 0.983173.\n",
      "Iteration 2247: Policy loss: -0.018419. Value loss: 0.074990. Entropy: 0.976808.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2248: Policy loss: 0.019374. Value loss: 0.528064. Entropy: 0.957692.\n",
      "Iteration 2249: Policy loss: -0.005499. Value loss: 0.242005. Entropy: 0.967940.\n",
      "Iteration 2250: Policy loss: -0.013764. Value loss: 0.112952. Entropy: 0.955346.\n",
      "episode: 1410   score: 24.0  epsilon: 1.0    steps: 744  evaluation reward: 13.39\n",
      "Training network. lr: 0.000233. clip: 0.093097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2251: Policy loss: 0.008536. Value loss: 0.404933. Entropy: 0.999645.\n",
      "Iteration 2252: Policy loss: -0.007419. Value loss: 0.157093. Entropy: 0.995257.\n",
      "Iteration 2253: Policy loss: -0.023836. Value loss: 0.087133. Entropy: 0.991634.\n",
      "episode: 1411   score: 11.0  epsilon: 1.0    steps: 920  evaluation reward: 13.42\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2254: Policy loss: 0.005789. Value loss: 0.459524. Entropy: 1.004807.\n",
      "Iteration 2255: Policy loss: -0.008276. Value loss: 0.189733. Entropy: 0.999203.\n",
      "Iteration 2256: Policy loss: -0.022728. Value loss: 0.105492. Entropy: 0.993856.\n",
      "episode: 1412   score: 12.0  epsilon: 1.0    steps: 248  evaluation reward: 13.35\n",
      "episode: 1413   score: 14.0  epsilon: 1.0    steps: 360  evaluation reward: 13.37\n",
      "episode: 1414   score: 16.0  epsilon: 1.0    steps: 840  evaluation reward: 13.36\n",
      "episode: 1415   score: 15.0  epsilon: 1.0    steps: 888  evaluation reward: 13.37\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2257: Policy loss: 0.010062. Value loss: 0.510019. Entropy: 1.017703.\n",
      "Iteration 2258: Policy loss: -0.009105. Value loss: 0.243491. Entropy: 1.013178.\n",
      "Iteration 2259: Policy loss: -0.020732. Value loss: 0.162412. Entropy: 1.018882.\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2260: Policy loss: 0.007434. Value loss: 0.599018. Entropy: 0.968993.\n",
      "Iteration 2261: Policy loss: -0.006031. Value loss: 0.295842. Entropy: 0.967730.\n",
      "Iteration 2262: Policy loss: -0.019391. Value loss: 0.165534. Entropy: 0.964227.\n",
      "episode: 1416   score: 13.0  epsilon: 1.0    steps: 40  evaluation reward: 13.36\n",
      "episode: 1417   score: 13.0  epsilon: 1.0    steps: 560  evaluation reward: 13.41\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2263: Policy loss: 0.000019. Value loss: 0.442116. Entropy: 0.955642.\n",
      "Iteration 2264: Policy loss: -0.015187. Value loss: 0.201955. Entropy: 0.949316.\n",
      "Iteration 2265: Policy loss: -0.019242. Value loss: 0.125753. Entropy: 0.949975.\n",
      "episode: 1418   score: 11.0  epsilon: 1.0    steps: 216  evaluation reward: 13.39\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2266: Policy loss: 0.001570. Value loss: 0.367067. Entropy: 0.941380.\n",
      "Iteration 2267: Policy loss: -0.019475. Value loss: 0.147535. Entropy: 0.942984.\n",
      "Iteration 2268: Policy loss: -0.029070. Value loss: 0.100518. Entropy: 0.935647.\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2269: Policy loss: 0.003514. Value loss: 0.575204. Entropy: 0.936535.\n",
      "Iteration 2270: Policy loss: -0.009008. Value loss: 0.263856. Entropy: 0.932325.\n",
      "Iteration 2271: Policy loss: -0.020833. Value loss: 0.145236. Entropy: 0.942566.\n",
      "episode: 1419   score: 10.0  epsilon: 1.0    steps: 96  evaluation reward: 13.27\n",
      "episode: 1420   score: 16.0  epsilon: 1.0    steps: 328  evaluation reward: 13.32\n",
      "episode: 1421   score: 10.0  epsilon: 1.0    steps: 440  evaluation reward: 13.36\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2272: Policy loss: 0.006798. Value loss: 0.518517. Entropy: 0.900419.\n",
      "Iteration 2273: Policy loss: -0.009359. Value loss: 0.287563. Entropy: 0.885249.\n",
      "Iteration 2274: Policy loss: -0.020566. Value loss: 0.200787. Entropy: 0.891907.\n",
      "episode: 1422   score: 13.0  epsilon: 1.0    steps: 304  evaluation reward: 13.38\n",
      "episode: 1423   score: 9.0  epsilon: 1.0    steps: 640  evaluation reward: 13.36\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2275: Policy loss: 0.007697. Value loss: 0.372264. Entropy: 0.897982.\n",
      "Iteration 2276: Policy loss: -0.016213. Value loss: 0.142700. Entropy: 0.900484.\n",
      "Iteration 2277: Policy loss: -0.025508. Value loss: 0.090180. Entropy: 0.893232.\n",
      "episode: 1424   score: 14.0  epsilon: 1.0    steps: 840  evaluation reward: 13.29\n",
      "episode: 1425   score: 11.0  epsilon: 1.0    steps: 864  evaluation reward: 13.25\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2278: Policy loss: 0.006839. Value loss: 0.269881. Entropy: 0.962165.\n",
      "Iteration 2279: Policy loss: -0.013271. Value loss: 0.111541. Entropy: 0.968159.\n",
      "Iteration 2280: Policy loss: -0.027321. Value loss: 0.058789. Entropy: 0.960849.\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2281: Policy loss: 0.013119. Value loss: 0.453885. Entropy: 0.937706.\n",
      "Iteration 2282: Policy loss: -0.006189. Value loss: 0.205012. Entropy: 0.953385.\n",
      "Iteration 2283: Policy loss: -0.019817. Value loss: 0.121408. Entropy: 0.944377.\n",
      "episode: 1426   score: 13.0  epsilon: 1.0    steps: 184  evaluation reward: 13.24\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2284: Policy loss: 0.004487. Value loss: 0.317774. Entropy: 0.969916.\n",
      "Iteration 2285: Policy loss: -0.018626. Value loss: 0.120978. Entropy: 0.975552.\n",
      "Iteration 2286: Policy loss: -0.026818. Value loss: 0.072317. Entropy: 0.967693.\n",
      "episode: 1427   score: 17.0  epsilon: 1.0    steps: 440  evaluation reward: 13.29\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2287: Policy loss: 0.010290. Value loss: 0.962940. Entropy: 0.909276.\n",
      "Iteration 2288: Policy loss: -0.002434. Value loss: 0.358523. Entropy: 0.932083.\n",
      "Iteration 2289: Policy loss: -0.013835. Value loss: 0.198470. Entropy: 0.933564.\n",
      "episode: 1428   score: 13.0  epsilon: 1.0    steps: 208  evaluation reward: 13.3\n",
      "episode: 1429   score: 16.0  epsilon: 1.0    steps: 320  evaluation reward: 13.29\n",
      "episode: 1430   score: 17.0  epsilon: 1.0    steps: 584  evaluation reward: 13.34\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2290: Policy loss: 0.009465. Value loss: 0.476398. Entropy: 0.932471.\n",
      "Iteration 2291: Policy loss: -0.009591. Value loss: 0.150975. Entropy: 0.930307.\n",
      "Iteration 2292: Policy loss: -0.023749. Value loss: 0.087920. Entropy: 0.925908.\n",
      "episode: 1431   score: 13.0  epsilon: 1.0    steps: 976  evaluation reward: 13.32\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2293: Policy loss: 0.006563. Value loss: 0.389761. Entropy: 0.976268.\n",
      "Iteration 2294: Policy loss: -0.008085. Value loss: 0.152499. Entropy: 0.966884.\n",
      "Iteration 2295: Policy loss: -0.021026. Value loss: 0.091681. Entropy: 0.979186.\n",
      "episode: 1432   score: 20.0  epsilon: 1.0    steps: 144  evaluation reward: 13.38\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2296: Policy loss: 0.014946. Value loss: 0.351284. Entropy: 0.969251.\n",
      "Iteration 2297: Policy loss: -0.009048. Value loss: 0.137566. Entropy: 0.965791.\n",
      "Iteration 2298: Policy loss: -0.021275. Value loss: 0.084935. Entropy: 0.961713.\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2299: Policy loss: 0.008427. Value loss: 0.347451. Entropy: 0.984540.\n",
      "Iteration 2300: Policy loss: -0.005389. Value loss: 0.168242. Entropy: 0.993060.\n",
      "Iteration 2301: Policy loss: -0.020535. Value loss: 0.104567. Entropy: 0.980009.\n",
      "episode: 1433   score: 12.0  epsilon: 1.0    steps: 80  evaluation reward: 13.35\n",
      "episode: 1434   score: 22.0  epsilon: 1.0    steps: 168  evaluation reward: 13.42\n",
      "episode: 1435   score: 8.0  epsilon: 1.0    steps: 968  evaluation reward: 13.38\n",
      "episode: 1436   score: 10.0  epsilon: 1.0    steps: 992  evaluation reward: 13.38\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2302: Policy loss: 0.004236. Value loss: 0.408786. Entropy: 0.971609.\n",
      "Iteration 2303: Policy loss: -0.014410. Value loss: 0.175479. Entropy: 0.970782.\n",
      "Iteration 2304: Policy loss: -0.027066. Value loss: 0.110759. Entropy: 0.965370.\n",
      "episode: 1437   score: 11.0  epsilon: 1.0    steps: 56  evaluation reward: 13.37\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2305: Policy loss: 0.003859. Value loss: 0.292602. Entropy: 0.980715.\n",
      "Iteration 2306: Policy loss: -0.016599. Value loss: 0.160989. Entropy: 0.968960.\n",
      "Iteration 2307: Policy loss: -0.023690. Value loss: 0.116459. Entropy: 0.971456.\n",
      "episode: 1438   score: 15.0  epsilon: 1.0    steps: 752  evaluation reward: 13.41\n",
      "episode: 1439   score: 9.0  epsilon: 1.0    steps: 952  evaluation reward: 13.31\n",
      "episode: 1440   score: 4.0  epsilon: 1.0    steps: 968  evaluation reward: 13.19\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2308: Policy loss: 0.010165. Value loss: 0.438106. Entropy: 0.983279.\n",
      "Iteration 2309: Policy loss: -0.014689. Value loss: 0.165172. Entropy: 0.981788.\n",
      "Iteration 2310: Policy loss: -0.024318. Value loss: 0.105037. Entropy: 0.973937.\n",
      "Training network. lr: 0.000232. clip: 0.092941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2311: Policy loss: 0.003030. Value loss: 0.324171. Entropy: 0.993034.\n",
      "Iteration 2312: Policy loss: -0.020024. Value loss: 0.151287. Entropy: 0.980640.\n",
      "Iteration 2313: Policy loss: -0.030158. Value loss: 0.098911. Entropy: 0.978059.\n",
      "episode: 1441   score: 7.0  epsilon: 1.0    steps: 528  evaluation reward: 13.13\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2314: Policy loss: 0.001358. Value loss: 0.210203. Entropy: 0.984172.\n",
      "Iteration 2315: Policy loss: -0.010619. Value loss: 0.068954. Entropy: 0.983608.\n",
      "Iteration 2316: Policy loss: -0.027515. Value loss: 0.040975. Entropy: 0.982932.\n",
      "episode: 1442   score: 14.0  epsilon: 1.0    steps: 520  evaluation reward: 13.16\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2317: Policy loss: 0.004017. Value loss: 0.476132. Entropy: 1.015882.\n",
      "Iteration 2318: Policy loss: -0.006678. Value loss: 0.204829. Entropy: 1.013546.\n",
      "Iteration 2319: Policy loss: -0.015965. Value loss: 0.134337. Entropy: 1.014585.\n",
      "episode: 1443   score: 10.0  epsilon: 1.0    steps: 152  evaluation reward: 13.15\n",
      "episode: 1444   score: 16.0  epsilon: 1.0    steps: 1024  evaluation reward: 13.17\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2320: Policy loss: 0.011197. Value loss: 0.448024. Entropy: 0.955421.\n",
      "Iteration 2321: Policy loss: -0.004946. Value loss: 0.231709. Entropy: 0.954598.\n",
      "Iteration 2322: Policy loss: -0.020709. Value loss: 0.137794. Entropy: 0.969306.\n",
      "episode: 1445   score: 14.0  epsilon: 1.0    steps: 192  evaluation reward: 13.22\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2323: Policy loss: 0.004930. Value loss: 0.411499. Entropy: 0.990387.\n",
      "Iteration 2324: Policy loss: -0.011256. Value loss: 0.211515. Entropy: 0.980555.\n",
      "Iteration 2325: Policy loss: -0.022566. Value loss: 0.150616. Entropy: 0.977310.\n",
      "episode: 1446   score: 15.0  epsilon: 1.0    steps: 1024  evaluation reward: 13.19\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2326: Policy loss: 0.013066. Value loss: 0.317175. Entropy: 1.069046.\n",
      "Iteration 2327: Policy loss: -0.013550. Value loss: 0.118756. Entropy: 1.068930.\n",
      "Iteration 2328: Policy loss: -0.028478. Value loss: 0.083104. Entropy: 1.072123.\n",
      "episode: 1447   score: 15.0  epsilon: 1.0    steps: 16  evaluation reward: 13.17\n",
      "episode: 1448   score: 19.0  epsilon: 1.0    steps: 736  evaluation reward: 13.13\n",
      "episode: 1449   score: 11.0  epsilon: 1.0    steps: 960  evaluation reward: 13.15\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2329: Policy loss: 0.009498. Value loss: 0.328474. Entropy: 1.020432.\n",
      "Iteration 2330: Policy loss: -0.010956. Value loss: 0.159608. Entropy: 1.023474.\n",
      "Iteration 2331: Policy loss: -0.018215. Value loss: 0.090407. Entropy: 1.020780.\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2332: Policy loss: 0.007470. Value loss: 0.288673. Entropy: 0.980227.\n",
      "Iteration 2333: Policy loss: -0.017670. Value loss: 0.127267. Entropy: 0.994099.\n",
      "Iteration 2334: Policy loss: -0.026687. Value loss: 0.086780. Entropy: 0.981077.\n",
      "episode: 1450   score: 18.0  epsilon: 1.0    steps: 624  evaluation reward: 13.19\n",
      "now time :  2019-03-06 13:17:54.066093\n",
      "episode: 1451   score: 16.0  epsilon: 1.0    steps: 640  evaluation reward: 13.3\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2335: Policy loss: 0.015682. Value loss: 0.473238. Entropy: 0.962932.\n",
      "Iteration 2336: Policy loss: 0.002082. Value loss: 0.139605. Entropy: 0.970905.\n",
      "Iteration 2337: Policy loss: -0.018141. Value loss: 0.088501. Entropy: 0.974134.\n",
      "episode: 1452   score: 11.0  epsilon: 1.0    steps: 568  evaluation reward: 13.29\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2338: Policy loss: 0.008345. Value loss: 0.266882. Entropy: 1.001937.\n",
      "Iteration 2339: Policy loss: -0.013518. Value loss: 0.109528. Entropy: 0.998430.\n",
      "Iteration 2340: Policy loss: -0.026043. Value loss: 0.067409. Entropy: 0.990053.\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2341: Policy loss: 0.001413. Value loss: 0.203682. Entropy: 1.020725.\n",
      "Iteration 2342: Policy loss: -0.012006. Value loss: 0.071226. Entropy: 1.021127.\n",
      "Iteration 2343: Policy loss: -0.024435. Value loss: 0.047743. Entropy: 1.027372.\n",
      "episode: 1453   score: 20.0  epsilon: 1.0    steps: 440  evaluation reward: 13.38\n",
      "episode: 1454   score: 11.0  epsilon: 1.0    steps: 568  evaluation reward: 13.38\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2344: Policy loss: 0.013268. Value loss: 0.410964. Entropy: 1.035544.\n",
      "Iteration 2345: Policy loss: -0.003567. Value loss: 0.141590. Entropy: 1.015290.\n",
      "Iteration 2346: Policy loss: -0.016254. Value loss: 0.093631. Entropy: 1.011800.\n",
      "episode: 1455   score: 16.0  epsilon: 1.0    steps: 896  evaluation reward: 13.44\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2347: Policy loss: 0.005290. Value loss: 0.612292. Entropy: 1.013263.\n",
      "Iteration 2348: Policy loss: 0.004992. Value loss: 0.324646. Entropy: 1.013633.\n",
      "Iteration 2349: Policy loss: -0.004713. Value loss: 0.167961. Entropy: 1.001380.\n",
      "episode: 1456   score: 17.0  epsilon: 1.0    steps: 176  evaluation reward: 13.5\n",
      "episode: 1457   score: 13.0  epsilon: 1.0    steps: 232  evaluation reward: 13.52\n",
      "episode: 1458   score: 12.0  epsilon: 1.0    steps: 784  evaluation reward: 13.47\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2350: Policy loss: 0.011764. Value loss: 0.362243. Entropy: 1.062549.\n",
      "Iteration 2351: Policy loss: -0.010750. Value loss: 0.200424. Entropy: 1.045148.\n",
      "Iteration 2352: Policy loss: -0.025864. Value loss: 0.142466. Entropy: 1.035648.\n",
      "episode: 1459   score: 10.0  epsilon: 1.0    steps: 112  evaluation reward: 13.47\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2353: Policy loss: 0.009007. Value loss: 0.263050. Entropy: 0.968000.\n",
      "Iteration 2354: Policy loss: -0.014409. Value loss: 0.110015. Entropy: 0.976330.\n",
      "Iteration 2355: Policy loss: -0.024376. Value loss: 0.063896. Entropy: 0.962579.\n",
      "episode: 1460   score: 7.0  epsilon: 1.0    steps: 872  evaluation reward: 13.41\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2356: Policy loss: 0.004441. Value loss: 0.543381. Entropy: 1.009967.\n",
      "Iteration 2357: Policy loss: -0.007637. Value loss: 0.239136. Entropy: 0.998624.\n",
      "Iteration 2358: Policy loss: -0.013872. Value loss: 0.135270. Entropy: 1.005818.\n",
      "episode: 1461   score: 16.0  epsilon: 1.0    steps: 320  evaluation reward: 13.48\n",
      "episode: 1462   score: 10.0  epsilon: 1.0    steps: 1016  evaluation reward: 13.47\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2359: Policy loss: 0.011693. Value loss: 0.466204. Entropy: 1.011805.\n",
      "Iteration 2360: Policy loss: -0.011304. Value loss: 0.166594. Entropy: 1.023313.\n",
      "Iteration 2361: Policy loss: -0.025351. Value loss: 0.095517. Entropy: 1.023175.\n",
      "episode: 1463   score: 12.0  epsilon: 1.0    steps: 760  evaluation reward: 13.41\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2362: Policy loss: 0.008963. Value loss: 0.247080. Entropy: 0.944582.\n",
      "Iteration 2363: Policy loss: -0.017897. Value loss: 0.103227. Entropy: 0.937735.\n",
      "Iteration 2364: Policy loss: -0.024403. Value loss: 0.061191. Entropy: 0.943243.\n",
      "episode: 1464   score: 12.0  epsilon: 1.0    steps: 1024  evaluation reward: 13.39\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2365: Policy loss: 0.017381. Value loss: 0.350754. Entropy: 0.999938.\n",
      "Iteration 2366: Policy loss: -0.003802. Value loss: 0.123695. Entropy: 0.998682.\n",
      "Iteration 2367: Policy loss: -0.018239. Value loss: 0.061889. Entropy: 0.995002.\n",
      "episode: 1465   score: 16.0  epsilon: 1.0    steps: 136  evaluation reward: 13.38\n",
      "episode: 1466   score: 9.0  epsilon: 1.0    steps: 344  evaluation reward: 13.39\n",
      "episode: 1467   score: 12.0  epsilon: 1.0    steps: 344  evaluation reward: 13.4\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2368: Policy loss: 0.006882. Value loss: 0.275771. Entropy: 1.024657.\n",
      "Iteration 2369: Policy loss: -0.016879. Value loss: 0.135787. Entropy: 1.017754.\n",
      "Iteration 2370: Policy loss: -0.031600. Value loss: 0.086679. Entropy: 1.006553.\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2371: Policy loss: 0.003488. Value loss: 0.496614. Entropy: 0.940076.\n",
      "Iteration 2372: Policy loss: -0.004707. Value loss: 0.244620. Entropy: 0.935477.\n",
      "Iteration 2373: Policy loss: -0.016112. Value loss: 0.169718. Entropy: 0.938567.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1468   score: 11.0  epsilon: 1.0    steps: 256  evaluation reward: 13.32\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2374: Policy loss: 0.006704. Value loss: 0.378905. Entropy: 0.936321.\n",
      "Iteration 2375: Policy loss: -0.018326. Value loss: 0.152550. Entropy: 0.927119.\n",
      "Iteration 2376: Policy loss: -0.026819. Value loss: 0.091939. Entropy: 0.934751.\n",
      "episode: 1469   score: 18.0  epsilon: 1.0    steps: 272  evaluation reward: 13.38\n",
      "episode: 1470   score: 12.0  epsilon: 1.0    steps: 536  evaluation reward: 13.4\n",
      "episode: 1471   score: 10.0  epsilon: 1.0    steps: 864  evaluation reward: 13.37\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2377: Policy loss: 0.020842. Value loss: 0.417945. Entropy: 0.979231.\n",
      "Iteration 2378: Policy loss: -0.008545. Value loss: 0.147617. Entropy: 0.952598.\n",
      "Iteration 2379: Policy loss: -0.017333. Value loss: 0.081430. Entropy: 0.954991.\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2380: Policy loss: 0.007890. Value loss: 0.336361. Entropy: 0.943268.\n",
      "Iteration 2381: Policy loss: -0.002439. Value loss: 0.119273. Entropy: 0.947120.\n",
      "Iteration 2382: Policy loss: -0.021516. Value loss: 0.064241. Entropy: 0.940667.\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2383: Policy loss: 0.004516. Value loss: 0.234475. Entropy: 0.952359.\n",
      "Iteration 2384: Policy loss: -0.011293. Value loss: 0.089480. Entropy: 0.946403.\n",
      "Iteration 2385: Policy loss: -0.021251. Value loss: 0.054586. Entropy: 0.956184.\n",
      "episode: 1472   score: 12.0  epsilon: 1.0    steps: 24  evaluation reward: 13.39\n",
      "episode: 1473   score: 12.0  epsilon: 1.0    steps: 632  evaluation reward: 13.41\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2386: Policy loss: 0.006467. Value loss: 0.251099. Entropy: 0.963595.\n",
      "Iteration 2387: Policy loss: -0.015808. Value loss: 0.095526. Entropy: 0.967692.\n",
      "Iteration 2388: Policy loss: -0.027264. Value loss: 0.059270. Entropy: 0.970376.\n",
      "episode: 1474   score: 22.0  epsilon: 1.0    steps: 416  evaluation reward: 13.47\n",
      "episode: 1475   score: 16.0  epsilon: 1.0    steps: 560  evaluation reward: 13.47\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2389: Policy loss: 0.001803. Value loss: 0.373865. Entropy: 0.992558.\n",
      "Iteration 2390: Policy loss: -0.010218. Value loss: 0.191144. Entropy: 0.988030.\n",
      "Iteration 2391: Policy loss: -0.022598. Value loss: 0.094108. Entropy: 0.977411.\n",
      "episode: 1476   score: 15.0  epsilon: 1.0    steps: 472  evaluation reward: 13.49\n",
      "episode: 1477   score: 11.0  epsilon: 1.0    steps: 736  evaluation reward: 13.49\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2392: Policy loss: 0.012534. Value loss: 0.385121. Entropy: 0.969192.\n",
      "Iteration 2393: Policy loss: -0.010906. Value loss: 0.134970. Entropy: 0.980021.\n",
      "Iteration 2394: Policy loss: -0.021083. Value loss: 0.083391. Entropy: 0.970653.\n",
      "episode: 1478   score: 14.0  epsilon: 1.0    steps: 288  evaluation reward: 13.44\n",
      "episode: 1479   score: 16.0  epsilon: 1.0    steps: 776  evaluation reward: 13.49\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2395: Policy loss: 0.014231. Value loss: 0.524408. Entropy: 0.938500.\n",
      "Iteration 2396: Policy loss: -0.002722. Value loss: 0.212885. Entropy: 0.935357.\n",
      "Iteration 2397: Policy loss: -0.018368. Value loss: 0.129289. Entropy: 0.926713.\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2398: Policy loss: 0.011111. Value loss: 0.291701. Entropy: 0.937768.\n",
      "Iteration 2399: Policy loss: -0.012632. Value loss: 0.102391. Entropy: 0.942767.\n",
      "Iteration 2400: Policy loss: -0.021191. Value loss: 0.076467. Entropy: 0.939048.\n",
      "episode: 1480   score: 10.0  epsilon: 1.0    steps: 248  evaluation reward: 13.39\n",
      "episode: 1481   score: 9.0  epsilon: 1.0    steps: 360  evaluation reward: 13.3\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2401: Policy loss: 0.012035. Value loss: 0.330649. Entropy: 0.991277.\n",
      "Iteration 2402: Policy loss: -0.010695. Value loss: 0.119796. Entropy: 0.991022.\n",
      "Iteration 2403: Policy loss: -0.024974. Value loss: 0.070792. Entropy: 0.993824.\n",
      "episode: 1482   score: 11.0  epsilon: 1.0    steps: 304  evaluation reward: 13.27\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2404: Policy loss: 0.010129. Value loss: 0.257634. Entropy: 0.951818.\n",
      "Iteration 2405: Policy loss: -0.014591. Value loss: 0.085965. Entropy: 0.958310.\n",
      "Iteration 2406: Policy loss: -0.025044. Value loss: 0.053539. Entropy: 0.958865.\n",
      "episode: 1483   score: 12.0  epsilon: 1.0    steps: 536  evaluation reward: 13.2\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2407: Policy loss: 0.008852. Value loss: 0.411109. Entropy: 1.003571.\n",
      "Iteration 2408: Policy loss: -0.010724. Value loss: 0.136665. Entropy: 0.988867.\n",
      "Iteration 2409: Policy loss: -0.014897. Value loss: 0.082297. Entropy: 0.990168.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2410: Policy loss: 0.010044. Value loss: 0.631108. Entropy: 0.967211.\n",
      "Iteration 2411: Policy loss: -0.008378. Value loss: 0.286276. Entropy: 0.971976.\n",
      "Iteration 2412: Policy loss: -0.015213. Value loss: 0.155464. Entropy: 0.978736.\n",
      "episode: 1484   score: 17.0  epsilon: 1.0    steps: 320  evaluation reward: 13.24\n",
      "episode: 1485   score: 14.0  epsilon: 1.0    steps: 848  evaluation reward: 13.27\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2413: Policy loss: 0.013135. Value loss: 0.454051. Entropy: 1.001570.\n",
      "Iteration 2414: Policy loss: -0.012178. Value loss: 0.167801. Entropy: 1.011985.\n",
      "Iteration 2415: Policy loss: -0.025575. Value loss: 0.086144. Entropy: 1.017952.\n",
      "episode: 1486   score: 19.0  epsilon: 1.0    steps: 272  evaluation reward: 13.3\n",
      "episode: 1487   score: 15.0  epsilon: 1.0    steps: 456  evaluation reward: 13.31\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2416: Policy loss: 0.004704. Value loss: 0.407029. Entropy: 0.991485.\n",
      "Iteration 2417: Policy loss: -0.014611. Value loss: 0.159235. Entropy: 0.984446.\n",
      "Iteration 2418: Policy loss: -0.027261. Value loss: 0.105803. Entropy: 0.982645.\n",
      "episode: 1488   score: 15.0  epsilon: 1.0    steps: 352  evaluation reward: 13.31\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2419: Policy loss: 0.009435. Value loss: 0.424792. Entropy: 0.991278.\n",
      "Iteration 2420: Policy loss: -0.014014. Value loss: 0.191418. Entropy: 0.981212.\n",
      "Iteration 2421: Policy loss: -0.027568. Value loss: 0.113300. Entropy: 0.979729.\n",
      "episode: 1489   score: 22.0  epsilon: 1.0    steps: 920  evaluation reward: 13.41\n",
      "episode: 1490   score: 15.0  epsilon: 1.0    steps: 1000  evaluation reward: 13.43\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2422: Policy loss: 0.016570. Value loss: 0.395061. Entropy: 0.924863.\n",
      "Iteration 2423: Policy loss: -0.007434. Value loss: 0.129529. Entropy: 0.929043.\n",
      "Iteration 2424: Policy loss: -0.013157. Value loss: 0.082595. Entropy: 0.928607.\n",
      "episode: 1491   score: 14.0  epsilon: 1.0    steps: 208  evaluation reward: 13.41\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2425: Policy loss: 0.005762. Value loss: 0.558545. Entropy: 1.013603.\n",
      "Iteration 2426: Policy loss: -0.011660. Value loss: 0.246072. Entropy: 1.004783.\n",
      "Iteration 2427: Policy loss: -0.021354. Value loss: 0.122774. Entropy: 0.998065.\n",
      "episode: 1492   score: 17.0  epsilon: 1.0    steps: 720  evaluation reward: 13.41\n",
      "episode: 1493   score: 9.0  epsilon: 1.0    steps: 752  evaluation reward: 13.36\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2428: Policy loss: 0.004002. Value loss: 0.315337. Entropy: 1.009745.\n",
      "Iteration 2429: Policy loss: -0.013948. Value loss: 0.105898. Entropy: 1.022364.\n",
      "Iteration 2430: Policy loss: -0.026037. Value loss: 0.066001. Entropy: 1.024301.\n",
      "episode: 1494   score: 14.0  epsilon: 1.0    steps: 848  evaluation reward: 13.41\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2431: Policy loss: 0.005378. Value loss: 0.331513. Entropy: 0.983248.\n",
      "Iteration 2432: Policy loss: -0.009576. Value loss: 0.130392. Entropy: 0.983042.\n",
      "Iteration 2433: Policy loss: -0.019689. Value loss: 0.078776. Entropy: 0.988293.\n",
      "episode: 1495   score: 16.0  epsilon: 1.0    steps: 192  evaluation reward: 13.46\n",
      "episode: 1496   score: 11.0  epsilon: 1.0    steps: 848  evaluation reward: 13.39\n",
      "Training network. lr: 0.000232. clip: 0.092636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2434: Policy loss: 0.007802. Value loss: 0.315769. Entropy: 0.961735.\n",
      "Iteration 2435: Policy loss: -0.018396. Value loss: 0.126520. Entropy: 0.948296.\n",
      "Iteration 2436: Policy loss: -0.025661. Value loss: 0.082653. Entropy: 0.940174.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2437: Policy loss: 0.005981. Value loss: 0.266731. Entropy: 1.007636.\n",
      "Iteration 2438: Policy loss: -0.015305. Value loss: 0.108564. Entropy: 0.998221.\n",
      "Iteration 2439: Policy loss: -0.030490. Value loss: 0.068534. Entropy: 1.003260.\n",
      "episode: 1497   score: 10.0  epsilon: 1.0    steps: 112  evaluation reward: 13.42\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2440: Policy loss: 0.009011. Value loss: 0.324503. Entropy: 1.003050.\n",
      "Iteration 2441: Policy loss: -0.018119. Value loss: 0.134891. Entropy: 0.992400.\n",
      "Iteration 2442: Policy loss: -0.028587. Value loss: 0.078033. Entropy: 0.981605.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2443: Policy loss: 0.015900. Value loss: 0.670975. Entropy: 0.952926.\n",
      "Iteration 2444: Policy loss: -0.006353. Value loss: 0.277616. Entropy: 0.945022.\n",
      "Iteration 2445: Policy loss: -0.013824. Value loss: 0.153852. Entropy: 0.953274.\n",
      "episode: 1498   score: 10.0  epsilon: 1.0    steps: 8  evaluation reward: 13.42\n",
      "episode: 1499   score: 20.0  epsilon: 1.0    steps: 208  evaluation reward: 13.49\n",
      "episode: 1500   score: 12.0  epsilon: 1.0    steps: 744  evaluation reward: 13.5\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2446: Policy loss: 0.008714. Value loss: 0.468202. Entropy: 0.963245.\n",
      "Iteration 2447: Policy loss: -0.002151. Value loss: 0.223409. Entropy: 0.950775.\n",
      "Iteration 2448: Policy loss: -0.018161. Value loss: 0.134664. Entropy: 0.962830.\n",
      "now time :  2019-03-06 13:20:20.445420\n",
      "episode: 1501   score: 33.0  epsilon: 1.0    steps: 512  evaluation reward: 13.73\n",
      "episode: 1502   score: 13.0  epsilon: 1.0    steps: 984  evaluation reward: 13.7\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2449: Policy loss: 0.003856. Value loss: 0.771746. Entropy: 0.953652.\n",
      "Iteration 2450: Policy loss: -0.002201. Value loss: 0.506901. Entropy: 0.952989.\n",
      "Iteration 2451: Policy loss: -0.015482. Value loss: 0.326627. Entropy: 0.950425.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2452: Policy loss: 0.008679. Value loss: 0.711785. Entropy: 0.927152.\n",
      "Iteration 2453: Policy loss: -0.001302. Value loss: 0.328949. Entropy: 0.926763.\n",
      "Iteration 2454: Policy loss: -0.017323. Value loss: 0.176719. Entropy: 0.916010.\n",
      "episode: 1503   score: 17.0  epsilon: 1.0    steps: 640  evaluation reward: 13.73\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2455: Policy loss: 0.007372. Value loss: 0.648406. Entropy: 0.929948.\n",
      "Iteration 2456: Policy loss: -0.010872. Value loss: 0.271699. Entropy: 0.932049.\n",
      "Iteration 2457: Policy loss: -0.020932. Value loss: 0.144040. Entropy: 0.924143.\n",
      "episode: 1504   score: 19.0  epsilon: 1.0    steps: 296  evaluation reward: 13.79\n",
      "episode: 1505   score: 23.0  epsilon: 1.0    steps: 920  evaluation reward: 13.87\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2458: Policy loss: 0.002794. Value loss: 0.657028. Entropy: 0.942112.\n",
      "Iteration 2459: Policy loss: -0.011933. Value loss: 0.314581. Entropy: 0.944540.\n",
      "Iteration 2460: Policy loss: -0.024674. Value loss: 0.177462. Entropy: 0.940037.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2461: Policy loss: 0.017062. Value loss: 0.599637. Entropy: 0.966521.\n",
      "Iteration 2462: Policy loss: -0.004375. Value loss: 0.290189. Entropy: 0.935714.\n",
      "Iteration 2463: Policy loss: -0.017512. Value loss: 0.177206. Entropy: 0.935941.\n",
      "episode: 1506   score: 15.0  epsilon: 1.0    steps: 16  evaluation reward: 13.92\n",
      "episode: 1507   score: 12.0  epsilon: 1.0    steps: 264  evaluation reward: 13.91\n",
      "episode: 1508   score: 8.0  epsilon: 1.0    steps: 360  evaluation reward: 13.82\n",
      "episode: 1509   score: 13.0  epsilon: 1.0    steps: 624  evaluation reward: 13.85\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2464: Policy loss: 0.011448. Value loss: 0.366983. Entropy: 0.960636.\n",
      "Iteration 2465: Policy loss: -0.008228. Value loss: 0.124140. Entropy: 0.958341.\n",
      "Iteration 2466: Policy loss: -0.023016. Value loss: 0.081562. Entropy: 0.957846.\n",
      "episode: 1510   score: 15.0  epsilon: 1.0    steps: 552  evaluation reward: 13.76\n",
      "episode: 1511   score: 7.0  epsilon: 1.0    steps: 968  evaluation reward: 13.72\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2467: Policy loss: 0.008088. Value loss: 0.373667. Entropy: 0.954723.\n",
      "Iteration 2468: Policy loss: -0.017560. Value loss: 0.138116. Entropy: 0.953745.\n",
      "Iteration 2469: Policy loss: -0.029927. Value loss: 0.091199. Entropy: 0.944295.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2470: Policy loss: 0.005460. Value loss: 0.318230. Entropy: 0.938244.\n",
      "Iteration 2471: Policy loss: -0.008544. Value loss: 0.125134. Entropy: 0.929969.\n",
      "Iteration 2472: Policy loss: -0.024860. Value loss: 0.072989. Entropy: 0.931263.\n",
      "episode: 1512   score: 9.0  epsilon: 1.0    steps: 616  evaluation reward: 13.69\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2473: Policy loss: 0.006751. Value loss: 0.259023. Entropy: 0.960961.\n",
      "Iteration 2474: Policy loss: -0.018297. Value loss: 0.100665. Entropy: 0.958783.\n",
      "Iteration 2475: Policy loss: -0.031420. Value loss: 0.063042. Entropy: 0.957269.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2476: Policy loss: 0.003920. Value loss: 0.264446. Entropy: 0.921109.\n",
      "Iteration 2477: Policy loss: -0.014801. Value loss: 0.083849. Entropy: 0.914799.\n",
      "Iteration 2478: Policy loss: -0.027945. Value loss: 0.051165. Entropy: 0.925053.\n",
      "episode: 1513   score: 12.0  epsilon: 1.0    steps: 344  evaluation reward: 13.67\n",
      "episode: 1514   score: 15.0  epsilon: 1.0    steps: 392  evaluation reward: 13.66\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2479: Policy loss: 0.000416. Value loss: 0.425928. Entropy: 0.935822.\n",
      "Iteration 2480: Policy loss: -0.011211. Value loss: 0.205418. Entropy: 0.952778.\n",
      "Iteration 2481: Policy loss: -0.014380. Value loss: 0.109965. Entropy: 0.940319.\n",
      "episode: 1515   score: 12.0  epsilon: 1.0    steps: 200  evaluation reward: 13.63\n",
      "episode: 1516   score: 17.0  epsilon: 1.0    steps: 856  evaluation reward: 13.67\n",
      "episode: 1517   score: 10.0  epsilon: 1.0    steps: 888  evaluation reward: 13.64\n",
      "episode: 1518   score: 13.0  epsilon: 1.0    steps: 904  evaluation reward: 13.66\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2482: Policy loss: 0.007875. Value loss: 0.309494. Entropy: 0.965220.\n",
      "Iteration 2483: Policy loss: -0.016990. Value loss: 0.107482. Entropy: 0.954062.\n",
      "Iteration 2484: Policy loss: -0.028533. Value loss: 0.069205. Entropy: 0.960558.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2485: Policy loss: 0.013593. Value loss: 0.339950. Entropy: 0.935602.\n",
      "Iteration 2486: Policy loss: -0.002731. Value loss: 0.113672. Entropy: 0.948581.\n",
      "Iteration 2487: Policy loss: -0.021901. Value loss: 0.067983. Entropy: 0.932718.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2488: Policy loss: 0.012450. Value loss: 0.554449. Entropy: 0.924808.\n",
      "Iteration 2489: Policy loss: -0.003607. Value loss: 0.211732. Entropy: 0.930783.\n",
      "Iteration 2490: Policy loss: -0.019685. Value loss: 0.112432. Entropy: 0.941570.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2491: Policy loss: 0.013552. Value loss: 0.547041. Entropy: 0.894487.\n",
      "Iteration 2492: Policy loss: -0.004272. Value loss: 0.220798. Entropy: 0.899804.\n",
      "Iteration 2493: Policy loss: -0.015088. Value loss: 0.137436. Entropy: 0.901723.\n",
      "episode: 1519   score: 29.0  epsilon: 1.0    steps: 16  evaluation reward: 13.85\n",
      "episode: 1520   score: 14.0  epsilon: 1.0    steps: 384  evaluation reward: 13.83\n",
      "episode: 1521   score: 17.0  epsilon: 1.0    steps: 832  evaluation reward: 13.9\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2494: Policy loss: 0.004407. Value loss: 0.336815. Entropy: 0.926281.\n",
      "Iteration 2495: Policy loss: -0.008177. Value loss: 0.159393. Entropy: 0.934393.\n",
      "Iteration 2496: Policy loss: -0.025333. Value loss: 0.106109. Entropy: 0.930885.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2497: Policy loss: 0.012550. Value loss: 0.577455. Entropy: 0.971105.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2498: Policy loss: -0.009785. Value loss: 0.270903. Entropy: 0.982279.\n",
      "Iteration 2499: Policy loss: -0.017802. Value loss: 0.177477. Entropy: 0.973957.\n",
      "episode: 1522   score: 19.0  epsilon: 1.0    steps: 704  evaluation reward: 13.96\n",
      "episode: 1523   score: 12.0  epsilon: 1.0    steps: 720  evaluation reward: 13.99\n",
      "episode: 1524   score: 14.0  epsilon: 1.0    steps: 1016  evaluation reward: 13.99\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2500: Policy loss: 0.009664. Value loss: 0.325823. Entropy: 0.968622.\n",
      "Iteration 2501: Policy loss: -0.014209. Value loss: 0.123984. Entropy: 0.975544.\n",
      "Iteration 2502: Policy loss: -0.029978. Value loss: 0.065356. Entropy: 0.967041.\n",
      "episode: 1525   score: 15.0  epsilon: 1.0    steps: 216  evaluation reward: 14.03\n",
      "episode: 1526   score: 14.0  epsilon: 1.0    steps: 248  evaluation reward: 14.04\n",
      "episode: 1527   score: 6.0  epsilon: 1.0    steps: 720  evaluation reward: 13.93\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2503: Policy loss: 0.003159. Value loss: 0.326943. Entropy: 0.965039.\n",
      "Iteration 2504: Policy loss: -0.014902. Value loss: 0.129779. Entropy: 0.949965.\n",
      "Iteration 2505: Policy loss: -0.024312. Value loss: 0.080491. Entropy: 0.951145.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2506: Policy loss: 0.004045. Value loss: 0.357901. Entropy: 0.906916.\n",
      "Iteration 2507: Policy loss: -0.010028. Value loss: 0.163047. Entropy: 0.898726.\n",
      "Iteration 2508: Policy loss: -0.028269. Value loss: 0.098330. Entropy: 0.904559.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2509: Policy loss: 0.009886. Value loss: 0.548842. Entropy: 0.970008.\n",
      "Iteration 2510: Policy loss: -0.010275. Value loss: 0.225666. Entropy: 0.955918.\n",
      "Iteration 2511: Policy loss: -0.027000. Value loss: 0.135554. Entropy: 0.965587.\n",
      "episode: 1528   score: 20.0  epsilon: 1.0    steps: 552  evaluation reward: 14.0\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2512: Policy loss: 0.007733. Value loss: 0.598452. Entropy: 0.956064.\n",
      "Iteration 2513: Policy loss: -0.006270. Value loss: 0.232911. Entropy: 0.973278.\n",
      "Iteration 2514: Policy loss: -0.017412. Value loss: 0.131119. Entropy: 0.965099.\n",
      "episode: 1529   score: 11.0  epsilon: 1.0    steps: 616  evaluation reward: 13.95\n",
      "episode: 1530   score: 13.0  epsilon: 1.0    steps: 768  evaluation reward: 13.91\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2515: Policy loss: 0.007970. Value loss: 0.584618. Entropy: 1.038522.\n",
      "Iteration 2516: Policy loss: -0.013723. Value loss: 0.217569. Entropy: 1.037886.\n",
      "Iteration 2517: Policy loss: -0.025424. Value loss: 0.112169. Entropy: 1.043606.\n",
      "episode: 1531   score: 10.0  epsilon: 1.0    steps: 160  evaluation reward: 13.88\n",
      "episode: 1532   score: 12.0  epsilon: 1.0    steps: 672  evaluation reward: 13.8\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2518: Policy loss: 0.011043. Value loss: 0.372239. Entropy: 0.945048.\n",
      "Iteration 2519: Policy loss: -0.004749. Value loss: 0.158134. Entropy: 0.956372.\n",
      "Iteration 2520: Policy loss: -0.019684. Value loss: 0.098664. Entropy: 0.948389.\n",
      "episode: 1533   score: 11.0  epsilon: 1.0    steps: 600  evaluation reward: 13.79\n",
      "episode: 1534   score: 29.0  epsilon: 1.0    steps: 752  evaluation reward: 13.86\n",
      "episode: 1535   score: 14.0  epsilon: 1.0    steps: 760  evaluation reward: 13.92\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2521: Policy loss: 0.006267. Value loss: 0.629732. Entropy: 0.971214.\n",
      "Iteration 2522: Policy loss: -0.005780. Value loss: 0.292291. Entropy: 0.981550.\n",
      "Iteration 2523: Policy loss: -0.016366. Value loss: 0.177591. Entropy: 0.974910.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2524: Policy loss: 0.007534. Value loss: 0.172495. Entropy: 0.948585.\n",
      "Iteration 2525: Policy loss: -0.015932. Value loss: 0.062577. Entropy: 0.960172.\n",
      "Iteration 2526: Policy loss: -0.028308. Value loss: 0.032453. Entropy: 0.954945.\n",
      "episode: 1536   score: 7.0  epsilon: 1.0    steps: 712  evaluation reward: 13.89\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2527: Policy loss: 0.007506. Value loss: 0.489102. Entropy: 0.955512.\n",
      "Iteration 2528: Policy loss: -0.015156. Value loss: 0.215821. Entropy: 0.942735.\n",
      "Iteration 2529: Policy loss: -0.027532. Value loss: 0.117983. Entropy: 0.946110.\n",
      "episode: 1537   score: 12.0  epsilon: 1.0    steps: 456  evaluation reward: 13.9\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2530: Policy loss: 0.009977. Value loss: 0.430493. Entropy: 0.923711.\n",
      "Iteration 2531: Policy loss: 0.000805. Value loss: 0.142996. Entropy: 0.915033.\n",
      "Iteration 2532: Policy loss: -0.019223. Value loss: 0.084787. Entropy: 0.914248.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2533: Policy loss: 0.003191. Value loss: 0.538699. Entropy: 0.940219.\n",
      "Iteration 2534: Policy loss: -0.005441. Value loss: 0.196303. Entropy: 0.952927.\n",
      "Iteration 2535: Policy loss: -0.019297. Value loss: 0.106516. Entropy: 0.952180.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2536: Policy loss: 0.011737. Value loss: 0.548493. Entropy: 1.047019.\n",
      "Iteration 2537: Policy loss: -0.009255. Value loss: 0.200059. Entropy: 1.063493.\n",
      "Iteration 2538: Policy loss: -0.026405. Value loss: 0.114934. Entropy: 1.055718.\n",
      "episode: 1538   score: 22.0  epsilon: 1.0    steps: 456  evaluation reward: 13.97\n",
      "episode: 1539   score: 14.0  epsilon: 1.0    steps: 568  evaluation reward: 14.02\n",
      "episode: 1540   score: 15.0  epsilon: 1.0    steps: 712  evaluation reward: 14.13\n",
      "episode: 1541   score: 24.0  epsilon: 1.0    steps: 856  evaluation reward: 14.3\n",
      "episode: 1542   score: 14.0  epsilon: 1.0    steps: 944  evaluation reward: 14.3\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2539: Policy loss: 0.014377. Value loss: 0.713308. Entropy: 0.995004.\n",
      "Iteration 2540: Policy loss: -0.012598. Value loss: 0.323321. Entropy: 0.976546.\n",
      "Iteration 2541: Policy loss: -0.023777. Value loss: 0.160960. Entropy: 0.973186.\n",
      "episode: 1543   score: 16.0  epsilon: 1.0    steps: 208  evaluation reward: 14.36\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2542: Policy loss: 0.004027. Value loss: 0.367320. Entropy: 0.945528.\n",
      "Iteration 2543: Policy loss: -0.010505. Value loss: 0.179400. Entropy: 0.942955.\n",
      "Iteration 2544: Policy loss: -0.025665. Value loss: 0.109827. Entropy: 0.929394.\n",
      "episode: 1544   score: 10.0  epsilon: 1.0    steps: 560  evaluation reward: 14.3\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2545: Policy loss: 0.006524. Value loss: 0.370848. Entropy: 0.938509.\n",
      "Iteration 2546: Policy loss: -0.006622. Value loss: 0.145441. Entropy: 0.936485.\n",
      "Iteration 2547: Policy loss: -0.025495. Value loss: 0.084680. Entropy: 0.943990.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2548: Policy loss: 0.008547. Value loss: 0.355456. Entropy: 0.959080.\n",
      "Iteration 2549: Policy loss: -0.015040. Value loss: 0.149920. Entropy: 0.961478.\n",
      "Iteration 2550: Policy loss: -0.021633. Value loss: 0.100495. Entropy: 0.964248.\n",
      "episode: 1545   score: 23.0  epsilon: 1.0    steps: 520  evaluation reward: 14.39\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2551: Policy loss: 0.007649. Value loss: 0.228796. Entropy: 0.977264.\n",
      "Iteration 2552: Policy loss: -0.019368. Value loss: 0.103830. Entropy: 0.972475.\n",
      "Iteration 2553: Policy loss: -0.031177. Value loss: 0.067144. Entropy: 0.975598.\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2554: Policy loss: 0.008121. Value loss: 0.508826. Entropy: 0.989217.\n",
      "Iteration 2555: Policy loss: -0.012080. Value loss: 0.284566. Entropy: 0.989024.\n",
      "Iteration 2556: Policy loss: -0.020912. Value loss: 0.197113. Entropy: 0.992915.\n",
      "episode: 1546   score: 11.0  epsilon: 1.0    steps: 128  evaluation reward: 14.35\n",
      "episode: 1547   score: 12.0  epsilon: 1.0    steps: 272  evaluation reward: 14.32\n",
      "episode: 1548   score: 20.0  epsilon: 1.0    steps: 760  evaluation reward: 14.33\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2557: Policy loss: 0.007314. Value loss: 0.505731. Entropy: 0.983298.\n",
      "Iteration 2558: Policy loss: -0.005234. Value loss: 0.204093. Entropy: 0.966283.\n",
      "Iteration 2559: Policy loss: -0.011328. Value loss: 0.105927. Entropy: 0.977862.\n",
      "episode: 1549   score: 14.0  epsilon: 1.0    steps: 512  evaluation reward: 14.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1550   score: 13.0  epsilon: 1.0    steps: 624  evaluation reward: 14.31\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2560: Policy loss: 0.002833. Value loss: 0.303095. Entropy: 1.024130.\n",
      "Iteration 2561: Policy loss: -0.014050. Value loss: 0.127674. Entropy: 1.007955.\n",
      "Iteration 2562: Policy loss: -0.026187. Value loss: 0.081466. Entropy: 1.012031.\n",
      "now time :  2019-03-06 13:22:46.956943\n",
      "episode: 1551   score: 22.0  epsilon: 1.0    steps: 432  evaluation reward: 14.37\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2563: Policy loss: 0.008660. Value loss: 0.376915. Entropy: 0.983708.\n",
      "Iteration 2564: Policy loss: -0.007050. Value loss: 0.188357. Entropy: 0.976421.\n",
      "Iteration 2565: Policy loss: -0.024596. Value loss: 0.116538. Entropy: 0.974557.\n",
      "episode: 1552   score: 17.0  epsilon: 1.0    steps: 896  evaluation reward: 14.43\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2566: Policy loss: 0.006787. Value loss: 0.578530. Entropy: 0.981319.\n",
      "Iteration 2567: Policy loss: -0.004791. Value loss: 0.290054. Entropy: 0.987021.\n",
      "Iteration 2568: Policy loss: -0.018136. Value loss: 0.168361. Entropy: 0.985116.\n",
      "episode: 1553   score: 13.0  epsilon: 1.0    steps: 880  evaluation reward: 14.36\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2569: Policy loss: 0.001656. Value loss: 0.376143. Entropy: 0.968724.\n",
      "Iteration 2570: Policy loss: -0.019332. Value loss: 0.166403. Entropy: 0.978235.\n",
      "Iteration 2571: Policy loss: -0.028571. Value loss: 0.104333. Entropy: 0.980209.\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2572: Policy loss: 0.011264. Value loss: 0.476349. Entropy: 1.024377.\n",
      "Iteration 2573: Policy loss: -0.009369. Value loss: 0.211205. Entropy: 1.039695.\n",
      "Iteration 2574: Policy loss: -0.023595. Value loss: 0.102158. Entropy: 1.023712.\n",
      "episode: 1554   score: 15.0  epsilon: 1.0    steps: 232  evaluation reward: 14.4\n",
      "episode: 1555   score: 14.0  epsilon: 1.0    steps: 384  evaluation reward: 14.38\n",
      "episode: 1556   score: 11.0  epsilon: 1.0    steps: 392  evaluation reward: 14.32\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2575: Policy loss: 0.016405. Value loss: 0.640279. Entropy: 0.949812.\n",
      "Iteration 2576: Policy loss: -0.007282. Value loss: 0.219713. Entropy: 0.929653.\n",
      "Iteration 2577: Policy loss: -0.015322. Value loss: 0.146671. Entropy: 0.941601.\n",
      "episode: 1557   score: 17.0  epsilon: 1.0    steps: 240  evaluation reward: 14.36\n",
      "episode: 1558   score: 12.0  epsilon: 1.0    steps: 272  evaluation reward: 14.36\n",
      "episode: 1559   score: 13.0  epsilon: 1.0    steps: 808  evaluation reward: 14.39\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2578: Policy loss: 0.007443. Value loss: 0.305258. Entropy: 0.989722.\n",
      "Iteration 2579: Policy loss: -0.015356. Value loss: 0.112298. Entropy: 0.994883.\n",
      "Iteration 2580: Policy loss: -0.022845. Value loss: 0.065490. Entropy: 0.984657.\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2581: Policy loss: 0.009046. Value loss: 0.369948. Entropy: 1.016694.\n",
      "Iteration 2582: Policy loss: -0.010877. Value loss: 0.179108. Entropy: 1.000689.\n",
      "Iteration 2583: Policy loss: -0.020618. Value loss: 0.125889. Entropy: 0.993039.\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2584: Policy loss: 0.010334. Value loss: 0.306791. Entropy: 1.025399.\n",
      "Iteration 2585: Policy loss: -0.013339. Value loss: 0.151600. Entropy: 1.019514.\n",
      "Iteration 2586: Policy loss: -0.024154. Value loss: 0.095705. Entropy: 1.017008.\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2587: Policy loss: 0.007331. Value loss: 0.642799. Entropy: 0.975358.\n",
      "Iteration 2588: Policy loss: -0.004526. Value loss: 0.266196. Entropy: 0.984129.\n",
      "Iteration 2589: Policy loss: -0.017439. Value loss: 0.139471. Entropy: 0.980782.\n",
      "episode: 1560   score: 21.0  epsilon: 1.0    steps: 888  evaluation reward: 14.53\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2590: Policy loss: 0.007032. Value loss: 0.551469. Entropy: 1.026083.\n",
      "Iteration 2591: Policy loss: -0.009702. Value loss: 0.250345. Entropy: 1.041287.\n",
      "Iteration 2592: Policy loss: -0.015566. Value loss: 0.149698. Entropy: 1.026353.\n",
      "episode: 1561   score: 17.0  epsilon: 1.0    steps: 240  evaluation reward: 14.54\n",
      "episode: 1562   score: 16.0  epsilon: 1.0    steps: 592  evaluation reward: 14.6\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2593: Policy loss: 0.009555. Value loss: 0.242961. Entropy: 1.022958.\n",
      "Iteration 2594: Policy loss: -0.016572. Value loss: 0.093933. Entropy: 1.024845.\n",
      "Iteration 2595: Policy loss: -0.027915. Value loss: 0.063740. Entropy: 1.014239.\n",
      "episode: 1563   score: 17.0  epsilon: 1.0    steps: 336  evaluation reward: 14.65\n",
      "episode: 1564   score: 17.0  epsilon: 1.0    steps: 488  evaluation reward: 14.7\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2596: Policy loss: 0.008098. Value loss: 0.553213. Entropy: 1.014217.\n",
      "Iteration 2597: Policy loss: -0.010635. Value loss: 0.220810. Entropy: 1.014899.\n",
      "Iteration 2598: Policy loss: -0.020271. Value loss: 0.121815. Entropy: 1.013405.\n",
      "episode: 1565   score: 17.0  epsilon: 1.0    steps: 120  evaluation reward: 14.71\n",
      "episode: 1566   score: 17.0  epsilon: 1.0    steps: 160  evaluation reward: 14.79\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2599: Policy loss: 0.007781. Value loss: 0.398643. Entropy: 0.994825.\n",
      "Iteration 2600: Policy loss: -0.007482. Value loss: 0.200437. Entropy: 0.996293.\n",
      "Iteration 2601: Policy loss: -0.023217. Value loss: 0.134421. Entropy: 0.998901.\n",
      "episode: 1567   score: 20.0  epsilon: 1.0    steps: 32  evaluation reward: 14.87\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2602: Policy loss: 0.002925. Value loss: 0.247535. Entropy: 0.970458.\n",
      "Iteration 2603: Policy loss: -0.015451. Value loss: 0.108122. Entropy: 0.955960.\n",
      "Iteration 2604: Policy loss: -0.026888. Value loss: 0.070861. Entropy: 0.957406.\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2605: Policy loss: 0.010642. Value loss: 0.352240. Entropy: 0.970354.\n",
      "Iteration 2606: Policy loss: -0.012866. Value loss: 0.153042. Entropy: 0.946229.\n",
      "Iteration 2607: Policy loss: -0.024041. Value loss: 0.091127. Entropy: 0.954136.\n",
      "episode: 1568   score: 12.0  epsilon: 1.0    steps: 424  evaluation reward: 14.88\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2608: Policy loss: 0.011214. Value loss: 0.553437. Entropy: 0.986447.\n",
      "Iteration 2609: Policy loss: -0.014407. Value loss: 0.220164. Entropy: 0.966640.\n",
      "Iteration 2610: Policy loss: -0.021740. Value loss: 0.124615. Entropy: 0.968710.\n",
      "episode: 1569   score: 14.0  epsilon: 1.0    steps: 720  evaluation reward: 14.84\n",
      "episode: 1570   score: 11.0  epsilon: 1.0    steps: 768  evaluation reward: 14.83\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2611: Policy loss: 0.017599. Value loss: 0.381007. Entropy: 0.975365.\n",
      "Iteration 2612: Policy loss: -0.009168. Value loss: 0.137152. Entropy: 0.983521.\n",
      "Iteration 2613: Policy loss: -0.017216. Value loss: 0.085791. Entropy: 0.983672.\n",
      "episode: 1571   score: 16.0  epsilon: 1.0    steps: 304  evaluation reward: 14.89\n",
      "episode: 1572   score: 15.0  epsilon: 1.0    steps: 544  evaluation reward: 14.92\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2614: Policy loss: 0.009271. Value loss: 0.642222. Entropy: 1.018464.\n",
      "Iteration 2615: Policy loss: -0.009647. Value loss: 0.295179. Entropy: 1.021865.\n",
      "Iteration 2616: Policy loss: -0.024529. Value loss: 0.176299. Entropy: 1.016292.\n",
      "episode: 1573   score: 19.0  epsilon: 1.0    steps: 176  evaluation reward: 14.99\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2617: Policy loss: 0.008789. Value loss: 0.511133. Entropy: 0.972570.\n",
      "Iteration 2618: Policy loss: -0.006102. Value loss: 0.210223. Entropy: 0.971423.\n",
      "Iteration 2619: Policy loss: -0.018663. Value loss: 0.119483. Entropy: 0.976643.\n",
      "episode: 1574   score: 15.0  epsilon: 1.0    steps: 120  evaluation reward: 14.92\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2620: Policy loss: 0.005751. Value loss: 0.361699. Entropy: 0.960505.\n",
      "Iteration 2621: Policy loss: -0.012078. Value loss: 0.152902. Entropy: 0.965828.\n",
      "Iteration 2622: Policy loss: -0.029953. Value loss: 0.097663. Entropy: 0.960103.\n",
      "episode: 1575   score: 11.0  epsilon: 1.0    steps: 592  evaluation reward: 14.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2623: Policy loss: 0.002520. Value loss: 0.434515. Entropy: 0.943364.\n",
      "Iteration 2624: Policy loss: -0.015520. Value loss: 0.200766. Entropy: 0.941901.\n",
      "Iteration 2625: Policy loss: -0.025825. Value loss: 0.125201. Entropy: 0.932680.\n",
      "episode: 1576   score: 23.0  epsilon: 1.0    steps: 688  evaluation reward: 14.95\n",
      "episode: 1577   score: 8.0  epsilon: 1.0    steps: 920  evaluation reward: 14.92\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2626: Policy loss: 0.014530. Value loss: 0.647370. Entropy: 1.005454.\n",
      "Iteration 2627: Policy loss: -0.003901. Value loss: 0.279019. Entropy: 1.008839.\n",
      "Iteration 2628: Policy loss: -0.019330. Value loss: 0.149249. Entropy: 0.995072.\n",
      "episode: 1578   score: 12.0  epsilon: 1.0    steps: 296  evaluation reward: 14.9\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2629: Policy loss: 0.011149. Value loss: 0.392473. Entropy: 0.956267.\n",
      "Iteration 2630: Policy loss: -0.017153. Value loss: 0.191877. Entropy: 0.960689.\n",
      "Iteration 2631: Policy loss: -0.025871. Value loss: 0.122151. Entropy: 0.947942.\n",
      "episode: 1579   score: 11.0  epsilon: 1.0    steps: 200  evaluation reward: 14.85\n",
      "episode: 1580   score: 14.0  epsilon: 1.0    steps: 1024  evaluation reward: 14.89\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2632: Policy loss: 0.011546. Value loss: 0.250558. Entropy: 0.943265.\n",
      "Iteration 2633: Policy loss: -0.012070. Value loss: 0.093194. Entropy: 0.943426.\n",
      "Iteration 2634: Policy loss: -0.025772. Value loss: 0.060058. Entropy: 0.941795.\n",
      "episode: 1581   score: 16.0  epsilon: 1.0    steps: 288  evaluation reward: 14.96\n",
      "episode: 1582   score: 11.0  epsilon: 1.0    steps: 1008  evaluation reward: 14.96\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2635: Policy loss: 0.008107. Value loss: 0.215236. Entropy: 0.914964.\n",
      "Iteration 2636: Policy loss: -0.016020. Value loss: 0.077082. Entropy: 0.912319.\n",
      "Iteration 2637: Policy loss: -0.026554. Value loss: 0.054414. Entropy: 0.904667.\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2638: Policy loss: 0.012162. Value loss: 0.269453. Entropy: 0.949697.\n",
      "Iteration 2639: Policy loss: -0.012002. Value loss: 0.107153. Entropy: 0.953464.\n",
      "Iteration 2640: Policy loss: -0.025977. Value loss: 0.060575. Entropy: 0.956599.\n",
      "episode: 1583   score: 9.0  epsilon: 1.0    steps: 424  evaluation reward: 14.93\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2641: Policy loss: 0.007759. Value loss: 0.154193. Entropy: 0.962810.\n",
      "Iteration 2642: Policy loss: -0.015299. Value loss: 0.065532. Entropy: 0.955308.\n",
      "Iteration 2643: Policy loss: -0.031122. Value loss: 0.044950. Entropy: 0.950587.\n",
      "episode: 1584   score: 12.0  epsilon: 1.0    steps: 168  evaluation reward: 14.88\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2644: Policy loss: -0.000053. Value loss: 0.439014. Entropy: 0.929128.\n",
      "Iteration 2645: Policy loss: -0.013098. Value loss: 0.181577. Entropy: 0.928180.\n",
      "Iteration 2646: Policy loss: -0.019823. Value loss: 0.120965. Entropy: 0.924936.\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2647: Policy loss: 0.011993. Value loss: 0.341456. Entropy: 0.947402.\n",
      "Iteration 2648: Policy loss: -0.008242. Value loss: 0.151636. Entropy: 0.957393.\n",
      "Iteration 2649: Policy loss: -0.022973. Value loss: 0.091718. Entropy: 0.951638.\n",
      "episode: 1585   score: 15.0  epsilon: 1.0    steps: 280  evaluation reward: 14.89\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2650: Policy loss: 0.008628. Value loss: 0.458030. Entropy: 0.966125.\n",
      "Iteration 2651: Policy loss: -0.012402. Value loss: 0.204460. Entropy: 0.975122.\n",
      "Iteration 2652: Policy loss: -0.023494. Value loss: 0.132555. Entropy: 0.962108.\n",
      "episode: 1586   score: 15.0  epsilon: 1.0    steps: 80  evaluation reward: 14.85\n",
      "episode: 1587   score: 24.0  epsilon: 1.0    steps: 352  evaluation reward: 14.94\n",
      "episode: 1588   score: 15.0  epsilon: 1.0    steps: 600  evaluation reward: 14.94\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2653: Policy loss: 0.006241. Value loss: 0.185249. Entropy: 0.959078.\n",
      "Iteration 2654: Policy loss: -0.014398. Value loss: 0.080436. Entropy: 0.961777.\n",
      "Iteration 2655: Policy loss: -0.028684. Value loss: 0.056437. Entropy: 0.954910.\n",
      "episode: 1589   score: 16.0  epsilon: 1.0    steps: 424  evaluation reward: 14.88\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2656: Policy loss: 0.012759. Value loss: 0.552062. Entropy: 0.972421.\n",
      "Iteration 2657: Policy loss: -0.004993. Value loss: 0.212344. Entropy: 0.951985.\n",
      "Iteration 2658: Policy loss: -0.015384. Value loss: 0.116139. Entropy: 0.958110.\n",
      "episode: 1590   score: 22.0  epsilon: 1.0    steps: 72  evaluation reward: 14.95\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2659: Policy loss: 0.002355. Value loss: 0.330201. Entropy: 0.935678.\n",
      "Iteration 2660: Policy loss: -0.012297. Value loss: 0.151240. Entropy: 0.916303.\n",
      "Iteration 2661: Policy loss: -0.027398. Value loss: 0.082978. Entropy: 0.914050.\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2662: Policy loss: 0.006559. Value loss: 0.388431. Entropy: 0.940787.\n",
      "Iteration 2663: Policy loss: -0.014081. Value loss: 0.152407. Entropy: 0.931034.\n",
      "Iteration 2664: Policy loss: -0.021991. Value loss: 0.077769. Entropy: 0.934621.\n",
      "episode: 1591   score: 18.0  epsilon: 1.0    steps: 1008  evaluation reward: 14.99\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2665: Policy loss: 0.009280. Value loss: 0.643274. Entropy: 0.933532.\n",
      "Iteration 2666: Policy loss: -0.010952. Value loss: 0.224963. Entropy: 0.932614.\n",
      "Iteration 2667: Policy loss: -0.022073. Value loss: 0.123477. Entropy: 0.927410.\n",
      "episode: 1592   score: 21.0  epsilon: 1.0    steps: 320  evaluation reward: 15.03\n",
      "episode: 1593   score: 14.0  epsilon: 1.0    steps: 800  evaluation reward: 15.08\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2668: Policy loss: -0.003180. Value loss: 0.393719. Entropy: 0.898542.\n",
      "Iteration 2669: Policy loss: -0.014654. Value loss: 0.190834. Entropy: 0.889634.\n",
      "Iteration 2670: Policy loss: -0.022099. Value loss: 0.111112. Entropy: 0.903021.\n",
      "episode: 1594   score: 22.0  epsilon: 1.0    steps: 320  evaluation reward: 15.16\n",
      "episode: 1595   score: 13.0  epsilon: 1.0    steps: 384  evaluation reward: 15.13\n",
      "episode: 1596   score: 21.0  epsilon: 1.0    steps: 872  evaluation reward: 15.23\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2671: Policy loss: 0.013616. Value loss: 0.396334. Entropy: 1.009096.\n",
      "Iteration 2672: Policy loss: -0.003539. Value loss: 0.176859. Entropy: 1.013602.\n",
      "Iteration 2673: Policy loss: -0.019490. Value loss: 0.124420. Entropy: 1.010699.\n",
      "episode: 1597   score: 12.0  epsilon: 1.0    steps: 592  evaluation reward: 15.25\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2674: Policy loss: 0.007567. Value loss: 0.276402. Entropy: 0.994792.\n",
      "Iteration 2675: Policy loss: -0.013702. Value loss: 0.110364. Entropy: 0.994159.\n",
      "Iteration 2676: Policy loss: -0.028388. Value loss: 0.065204. Entropy: 0.993692.\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2677: Policy loss: -0.000752. Value loss: 0.388932. Entropy: 0.945584.\n",
      "Iteration 2678: Policy loss: -0.018064. Value loss: 0.162674. Entropy: 0.938747.\n",
      "Iteration 2679: Policy loss: -0.027709. Value loss: 0.097975. Entropy: 0.939028.\n",
      "episode: 1598   score: 17.0  epsilon: 1.0    steps: 936  evaluation reward: 15.32\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2680: Policy loss: 0.005410. Value loss: 0.284609. Entropy: 0.980164.\n",
      "Iteration 2681: Policy loss: -0.017506. Value loss: 0.119352. Entropy: 0.980682.\n",
      "Iteration 2682: Policy loss: -0.028472. Value loss: 0.076682. Entropy: 0.973528.\n",
      "episode: 1599   score: 11.0  epsilon: 1.0    steps: 840  evaluation reward: 15.23\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2683: Policy loss: 0.013827. Value loss: 0.447408. Entropy: 0.985399.\n",
      "Iteration 2684: Policy loss: -0.010540. Value loss: 0.172500. Entropy: 0.979007.\n",
      "Iteration 2685: Policy loss: -0.018737. Value loss: 0.114233. Entropy: 0.987970.\n",
      "episode: 1600   score: 17.0  epsilon: 1.0    steps: 128  evaluation reward: 15.28\n",
      "now time :  2019-03-06 13:25:25.263296\n",
      "episode: 1601   score: 12.0  epsilon: 1.0    steps: 272  evaluation reward: 15.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2686: Policy loss: 0.005594. Value loss: 0.315412. Entropy: 0.960151.\n",
      "Iteration 2687: Policy loss: -0.004424. Value loss: 0.091881. Entropy: 0.962539.\n",
      "Iteration 2688: Policy loss: -0.017757. Value loss: 0.049319. Entropy: 0.963804.\n",
      "episode: 1602   score: 10.0  epsilon: 1.0    steps: 272  evaluation reward: 15.04\n",
      "episode: 1603   score: 12.0  epsilon: 1.0    steps: 384  evaluation reward: 14.99\n",
      "episode: 1604   score: 13.0  epsilon: 1.0    steps: 576  evaluation reward: 14.93\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2689: Policy loss: 0.009702. Value loss: 0.245472. Entropy: 0.963877.\n",
      "Iteration 2690: Policy loss: -0.013791. Value loss: 0.105485. Entropy: 0.962312.\n",
      "Iteration 2691: Policy loss: -0.026143. Value loss: 0.073039. Entropy: 0.958318.\n",
      "episode: 1605   score: 18.0  epsilon: 1.0    steps: 208  evaluation reward: 14.88\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2692: Policy loss: 0.003943. Value loss: 0.274282. Entropy: 0.957315.\n",
      "Iteration 2693: Policy loss: -0.013913. Value loss: 0.123058. Entropy: 0.955196.\n",
      "Iteration 2694: Policy loss: -0.030197. Value loss: 0.077265. Entropy: 0.950214.\n",
      "episode: 1606   score: 9.0  epsilon: 1.0    steps: 672  evaluation reward: 14.82\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2695: Policy loss: 0.004471. Value loss: 0.225783. Entropy: 0.916689.\n",
      "Iteration 2696: Policy loss: -0.012736. Value loss: 0.089837. Entropy: 0.916180.\n",
      "Iteration 2697: Policy loss: -0.023669. Value loss: 0.062032. Entropy: 0.906869.\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2698: Policy loss: 0.005355. Value loss: 0.301664. Entropy: 0.972671.\n",
      "Iteration 2699: Policy loss: -0.016366. Value loss: 0.135016. Entropy: 0.961654.\n",
      "Iteration 2700: Policy loss: -0.023390. Value loss: 0.095318. Entropy: 0.966804.\n",
      "episode: 1607   score: 10.0  epsilon: 1.0    steps: 120  evaluation reward: 14.8\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2701: Policy loss: 0.006831. Value loss: 0.272311. Entropy: 0.947020.\n",
      "Iteration 2702: Policy loss: -0.011751. Value loss: 0.105440. Entropy: 0.943669.\n",
      "Iteration 2703: Policy loss: -0.026045. Value loss: 0.072110. Entropy: 0.940216.\n",
      "episode: 1608   score: 13.0  epsilon: 1.0    steps: 368  evaluation reward: 14.85\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2704: Policy loss: 0.005414. Value loss: 0.280992. Entropy: 0.962711.\n",
      "Iteration 2705: Policy loss: -0.011680. Value loss: 0.098543. Entropy: 0.976845.\n",
      "Iteration 2706: Policy loss: -0.026771. Value loss: 0.055164. Entropy: 0.958204.\n",
      "episode: 1609   score: 17.0  epsilon: 1.0    steps: 848  evaluation reward: 14.89\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2707: Policy loss: 0.012657. Value loss: 0.854012. Entropy: 0.937249.\n",
      "Iteration 2708: Policy loss: -0.005599. Value loss: 0.423105. Entropy: 0.944811.\n",
      "Iteration 2709: Policy loss: -0.017324. Value loss: 0.215958. Entropy: 0.949941.\n",
      "episode: 1610   score: 21.0  epsilon: 1.0    steps: 192  evaluation reward: 14.95\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2710: Policy loss: 0.009688. Value loss: 0.464362. Entropy: 1.016280.\n",
      "Iteration 2711: Policy loss: -0.006401. Value loss: 0.252692. Entropy: 1.018575.\n",
      "Iteration 2712: Policy loss: -0.019067. Value loss: 0.153102. Entropy: 1.002973.\n",
      "episode: 1611   score: 20.0  epsilon: 1.0    steps: 96  evaluation reward: 15.08\n",
      "episode: 1612   score: 11.0  epsilon: 1.0    steps: 112  evaluation reward: 15.1\n",
      "episode: 1613   score: 20.0  epsilon: 1.0    steps: 136  evaluation reward: 15.18\n",
      "episode: 1614   score: 22.0  epsilon: 1.0    steps: 208  evaluation reward: 15.25\n",
      "episode: 1615   score: 10.0  epsilon: 1.0    steps: 896  evaluation reward: 15.23\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2713: Policy loss: 0.007022. Value loss: 0.377859. Entropy: 0.939135.\n",
      "Iteration 2714: Policy loss: -0.010107. Value loss: 0.184278. Entropy: 0.939402.\n",
      "Iteration 2715: Policy loss: -0.024006. Value loss: 0.135909. Entropy: 0.936732.\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2716: Policy loss: 0.004094. Value loss: 0.292031. Entropy: 0.937285.\n",
      "Iteration 2717: Policy loss: -0.013183. Value loss: 0.138408. Entropy: 0.938013.\n",
      "Iteration 2718: Policy loss: -0.022616. Value loss: 0.088728. Entropy: 0.940751.\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2719: Policy loss: 0.012810. Value loss: 0.360359. Entropy: 0.942728.\n",
      "Iteration 2720: Policy loss: -0.005726. Value loss: 0.129460. Entropy: 0.931397.\n",
      "Iteration 2721: Policy loss: -0.015719. Value loss: 0.066990. Entropy: 0.945439.\n",
      "episode: 1616   score: 9.0  epsilon: 1.0    steps: 736  evaluation reward: 15.15\n",
      "episode: 1617   score: 17.0  epsilon: 1.0    steps: 952  evaluation reward: 15.22\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2722: Policy loss: 0.007482. Value loss: 0.381631. Entropy: 0.955437.\n",
      "Iteration 2723: Policy loss: -0.014391. Value loss: 0.120096. Entropy: 0.963448.\n",
      "Iteration 2724: Policy loss: -0.025328. Value loss: 0.069678. Entropy: 0.971334.\n",
      "episode: 1618   score: 14.0  epsilon: 1.0    steps: 72  evaluation reward: 15.23\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2725: Policy loss: 0.007800. Value loss: 0.276671. Entropy: 0.912494.\n",
      "Iteration 2726: Policy loss: -0.012054. Value loss: 0.095349. Entropy: 0.893985.\n",
      "Iteration 2727: Policy loss: -0.028231. Value loss: 0.051424. Entropy: 0.908151.\n",
      "episode: 1619   score: 11.0  epsilon: 1.0    steps: 808  evaluation reward: 15.05\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2728: Policy loss: 0.009617. Value loss: 0.456467. Entropy: 1.018061.\n",
      "Iteration 2729: Policy loss: -0.012206. Value loss: 0.191184. Entropy: 1.025765.\n",
      "Iteration 2730: Policy loss: -0.021595. Value loss: 0.122410. Entropy: 1.019807.\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2731: Policy loss: 0.006436. Value loss: 0.301680. Entropy: 0.923257.\n",
      "Iteration 2732: Policy loss: -0.015370. Value loss: 0.146714. Entropy: 0.914661.\n",
      "Iteration 2733: Policy loss: -0.026502. Value loss: 0.078797. Entropy: 0.922748.\n",
      "episode: 1620   score: 16.0  epsilon: 1.0    steps: 512  evaluation reward: 15.07\n",
      "episode: 1621   score: 23.0  epsilon: 1.0    steps: 712  evaluation reward: 15.13\n",
      "episode: 1622   score: 17.0  epsilon: 1.0    steps: 976  evaluation reward: 15.11\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2734: Policy loss: 0.012227. Value loss: 0.437098. Entropy: 0.986518.\n",
      "Iteration 2735: Policy loss: -0.016082. Value loss: 0.182388. Entropy: 0.983702.\n",
      "Iteration 2736: Policy loss: -0.025724. Value loss: 0.117238. Entropy: 0.971989.\n",
      "episode: 1623   score: 18.0  epsilon: 1.0    steps: 64  evaluation reward: 15.17\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2737: Policy loss: 0.006730. Value loss: 0.325674. Entropy: 0.933076.\n",
      "Iteration 2738: Policy loss: -0.008222. Value loss: 0.147050. Entropy: 0.924839.\n",
      "Iteration 2739: Policy loss: -0.023544. Value loss: 0.078931. Entropy: 0.924659.\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2740: Policy loss: 0.010511. Value loss: 0.259462. Entropy: 0.904074.\n",
      "Iteration 2741: Policy loss: -0.009911. Value loss: 0.103341. Entropy: 0.913846.\n",
      "Iteration 2742: Policy loss: -0.025165. Value loss: 0.055057. Entropy: 0.904483.\n",
      "episode: 1624   score: 11.0  epsilon: 1.0    steps: 128  evaluation reward: 15.14\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2743: Policy loss: 0.016628. Value loss: 0.206151. Entropy: 0.951948.\n",
      "Iteration 2744: Policy loss: -0.006690. Value loss: 0.083899. Entropy: 0.950219.\n",
      "Iteration 2745: Policy loss: -0.023954. Value loss: 0.052137. Entropy: 0.942613.\n",
      "episode: 1625   score: 15.0  epsilon: 1.0    steps: 216  evaluation reward: 15.14\n",
      "episode: 1626   score: 19.0  epsilon: 1.0    steps: 656  evaluation reward: 15.19\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2746: Policy loss: 0.005544. Value loss: 0.389063. Entropy: 0.944445.\n",
      "Iteration 2747: Policy loss: -0.006251. Value loss: 0.150038. Entropy: 0.949535.\n",
      "Iteration 2748: Policy loss: -0.021538. Value loss: 0.070896. Entropy: 0.956963.\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2749: Policy loss: 0.010048. Value loss: 0.184946. Entropy: 1.001448.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2750: Policy loss: -0.009208. Value loss: 0.081969. Entropy: 1.003793.\n",
      "Iteration 2751: Policy loss: -0.025351. Value loss: 0.043235. Entropy: 1.005085.\n",
      "episode: 1627   score: 16.0  epsilon: 1.0    steps: 240  evaluation reward: 15.29\n",
      "episode: 1628   score: 13.0  epsilon: 1.0    steps: 520  evaluation reward: 15.22\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2752: Policy loss: 0.010016. Value loss: 0.207708. Entropy: 0.984071.\n",
      "Iteration 2753: Policy loss: -0.014470. Value loss: 0.086537. Entropy: 0.995929.\n",
      "Iteration 2754: Policy loss: -0.028461. Value loss: 0.063618. Entropy: 0.999630.\n",
      "episode: 1629   score: 16.0  epsilon: 1.0    steps: 464  evaluation reward: 15.27\n",
      "episode: 1630   score: 19.0  epsilon: 1.0    steps: 936  evaluation reward: 15.33\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2755: Policy loss: 0.008493. Value loss: 0.321952. Entropy: 0.999318.\n",
      "Iteration 2756: Policy loss: -0.010182. Value loss: 0.117193. Entropy: 0.995548.\n",
      "Iteration 2757: Policy loss: -0.020685. Value loss: 0.064950. Entropy: 0.997390.\n",
      "episode: 1631   score: 15.0  epsilon: 1.0    steps: 800  evaluation reward: 15.38\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2758: Policy loss: 0.009472. Value loss: 0.246372. Entropy: 0.971104.\n",
      "Iteration 2759: Policy loss: -0.014913. Value loss: 0.093925. Entropy: 0.963676.\n",
      "Iteration 2760: Policy loss: -0.028407. Value loss: 0.059551. Entropy: 0.971555.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2761: Policy loss: 0.003798. Value loss: 0.221728. Entropy: 0.958865.\n",
      "Iteration 2762: Policy loss: -0.019466. Value loss: 0.100178. Entropy: 0.964902.\n",
      "Iteration 2763: Policy loss: -0.029976. Value loss: 0.053233. Entropy: 0.964816.\n",
      "episode: 1632   score: 12.0  epsilon: 1.0    steps: 600  evaluation reward: 15.38\n",
      "episode: 1633   score: 17.0  epsilon: 1.0    steps: 872  evaluation reward: 15.44\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2764: Policy loss: 0.005169. Value loss: 0.484016. Entropy: 1.016427.\n",
      "Iteration 2765: Policy loss: -0.012858. Value loss: 0.201452. Entropy: 1.004756.\n",
      "Iteration 2766: Policy loss: -0.021983. Value loss: 0.110208. Entropy: 1.008302.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2767: Policy loss: 0.012239. Value loss: 0.347318. Entropy: 1.005371.\n",
      "Iteration 2768: Policy loss: -0.008576. Value loss: 0.122281. Entropy: 1.018902.\n",
      "Iteration 2769: Policy loss: -0.022255. Value loss: 0.054414. Entropy: 1.015114.\n",
      "episode: 1634   score: 18.0  epsilon: 1.0    steps: 368  evaluation reward: 15.33\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2770: Policy loss: 0.020390. Value loss: 0.568904. Entropy: 0.952671.\n",
      "Iteration 2771: Policy loss: -0.015425. Value loss: 0.228816. Entropy: 0.947772.\n",
      "Iteration 2772: Policy loss: -0.029019. Value loss: 0.128933. Entropy: 0.953379.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2773: Policy loss: 0.006529. Value loss: 0.568309. Entropy: 0.971392.\n",
      "Iteration 2774: Policy loss: -0.009056. Value loss: 0.271834. Entropy: 0.990563.\n",
      "Iteration 2775: Policy loss: -0.017365. Value loss: 0.153656. Entropy: 0.979972.\n",
      "episode: 1635   score: 11.0  epsilon: 1.0    steps: 48  evaluation reward: 15.3\n",
      "episode: 1636   score: 21.0  epsilon: 1.0    steps: 312  evaluation reward: 15.44\n",
      "episode: 1637   score: 21.0  epsilon: 1.0    steps: 640  evaluation reward: 15.53\n",
      "episode: 1638   score: 29.0  epsilon: 1.0    steps: 704  evaluation reward: 15.6\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2776: Policy loss: 0.005000. Value loss: 0.320953. Entropy: 0.945287.\n",
      "Iteration 2777: Policy loss: -0.010903. Value loss: 0.124407. Entropy: 0.930140.\n",
      "Iteration 2778: Policy loss: -0.024971. Value loss: 0.082451. Entropy: 0.938781.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2779: Policy loss: 0.003955. Value loss: 0.474477. Entropy: 0.912692.\n",
      "Iteration 2780: Policy loss: -0.007869. Value loss: 0.168908. Entropy: 0.911087.\n",
      "Iteration 2781: Policy loss: -0.023346. Value loss: 0.078070. Entropy: 0.911008.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2782: Policy loss: 0.009741. Value loss: 0.519439. Entropy: 0.925031.\n",
      "Iteration 2783: Policy loss: -0.005037. Value loss: 0.263595. Entropy: 0.932298.\n",
      "Iteration 2784: Policy loss: -0.018975. Value loss: 0.171190. Entropy: 0.922220.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2785: Policy loss: 0.008770. Value loss: 0.647637. Entropy: 0.986358.\n",
      "Iteration 2786: Policy loss: -0.008562. Value loss: 0.324105. Entropy: 0.974624.\n",
      "Iteration 2787: Policy loss: -0.014958. Value loss: 0.206933. Entropy: 0.979384.\n",
      "episode: 1639   score: 16.0  epsilon: 1.0    steps: 16  evaluation reward: 15.62\n",
      "episode: 1640   score: 20.0  epsilon: 1.0    steps: 80  evaluation reward: 15.67\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2788: Policy loss: 0.015059. Value loss: 0.405312. Entropy: 0.937055.\n",
      "Iteration 2789: Policy loss: -0.004164. Value loss: 0.181351. Entropy: 0.953808.\n",
      "Iteration 2790: Policy loss: -0.022260. Value loss: 0.123361. Entropy: 0.942002.\n",
      "episode: 1641   score: 35.0  epsilon: 1.0    steps: 64  evaluation reward: 15.78\n",
      "episode: 1642   score: 16.0  epsilon: 1.0    steps: 368  evaluation reward: 15.8\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2791: Policy loss: 0.011167. Value loss: 0.592558. Entropy: 0.946449.\n",
      "Iteration 2792: Policy loss: -0.004207. Value loss: 0.280475. Entropy: 0.938757.\n",
      "Iteration 2793: Policy loss: -0.016960. Value loss: 0.216726. Entropy: 0.947631.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2794: Policy loss: 0.009456. Value loss: 0.481656. Entropy: 0.960129.\n",
      "Iteration 2795: Policy loss: -0.006040. Value loss: 0.208703. Entropy: 0.952152.\n",
      "Iteration 2796: Policy loss: -0.015743. Value loss: 0.124408. Entropy: 0.935305.\n",
      "episode: 1643   score: 21.0  epsilon: 1.0    steps: 96  evaluation reward: 15.85\n",
      "episode: 1644   score: 18.0  epsilon: 1.0    steps: 496  evaluation reward: 15.93\n",
      "episode: 1645   score: 23.0  epsilon: 1.0    steps: 832  evaluation reward: 15.93\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2797: Policy loss: 0.016060. Value loss: 0.526538. Entropy: 0.966668.\n",
      "Iteration 2798: Policy loss: 0.001985. Value loss: 0.193650. Entropy: 0.962326.\n",
      "Iteration 2799: Policy loss: -0.015619. Value loss: 0.110558. Entropy: 0.955465.\n",
      "episode: 1646   score: 22.0  epsilon: 1.0    steps: 656  evaluation reward: 16.04\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2800: Policy loss: 0.009053. Value loss: 0.326926. Entropy: 0.904041.\n",
      "Iteration 2801: Policy loss: -0.014611. Value loss: 0.160473. Entropy: 0.904322.\n",
      "Iteration 2802: Policy loss: -0.024783. Value loss: 0.106325. Entropy: 0.910933.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2803: Policy loss: 0.009207. Value loss: 0.441613. Entropy: 0.932728.\n",
      "Iteration 2804: Policy loss: -0.013580. Value loss: 0.207321. Entropy: 0.925677.\n",
      "Iteration 2805: Policy loss: -0.023841. Value loss: 0.139674. Entropy: 0.936471.\n",
      "episode: 1647   score: 11.0  epsilon: 1.0    steps: 376  evaluation reward: 16.03\n",
      "episode: 1648   score: 15.0  epsilon: 1.0    steps: 928  evaluation reward: 15.98\n",
      "episode: 1649   score: 15.0  epsilon: 1.0    steps: 984  evaluation reward: 15.99\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2806: Policy loss: 0.009128. Value loss: 0.388525. Entropy: 0.958229.\n",
      "Iteration 2807: Policy loss: -0.012532. Value loss: 0.152181. Entropy: 0.961210.\n",
      "Iteration 2808: Policy loss: -0.023917. Value loss: 0.086496. Entropy: 0.953730.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2809: Policy loss: 0.002699. Value loss: 0.500966. Entropy: 0.916611.\n",
      "Iteration 2810: Policy loss: -0.014179. Value loss: 0.246917. Entropy: 0.905658.\n",
      "Iteration 2811: Policy loss: -0.020147. Value loss: 0.152285. Entropy: 0.913306.\n",
      "episode: 1650   score: 19.0  epsilon: 1.0    steps: 208  evaluation reward: 16.05\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2812: Policy loss: 0.008201. Value loss: 0.464567. Entropy: 0.932518.\n",
      "Iteration 2813: Policy loss: -0.016303. Value loss: 0.214517. Entropy: 0.934044.\n",
      "Iteration 2814: Policy loss: -0.028719. Value loss: 0.123588. Entropy: 0.940976.\n",
      "now time :  2019-03-06 13:28:12.188548\n",
      "episode: 1651   score: 16.0  epsilon: 1.0    steps: 864  evaluation reward: 15.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2815: Policy loss: 0.006575. Value loss: 0.404810. Entropy: 0.920530.\n",
      "Iteration 2816: Policy loss: -0.005249. Value loss: 0.165931. Entropy: 0.931600.\n",
      "Iteration 2817: Policy loss: -0.022544. Value loss: 0.089741. Entropy: 0.931034.\n",
      "episode: 1652   score: 14.0  epsilon: 1.0    steps: 344  evaluation reward: 15.96\n",
      "episode: 1653   score: 15.0  epsilon: 1.0    steps: 720  evaluation reward: 15.98\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2818: Policy loss: 0.007616. Value loss: 0.525238. Entropy: 0.976810.\n",
      "Iteration 2819: Policy loss: -0.001002. Value loss: 0.232646. Entropy: 0.979263.\n",
      "Iteration 2820: Policy loss: -0.014816. Value loss: 0.118037. Entropy: 0.981658.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2821: Policy loss: 0.006997. Value loss: 0.463742. Entropy: 0.920814.\n",
      "Iteration 2822: Policy loss: -0.003525. Value loss: 0.181436. Entropy: 0.900965.\n",
      "Iteration 2823: Policy loss: -0.018273. Value loss: 0.086680. Entropy: 0.911850.\n",
      "episode: 1654   score: 9.0  epsilon: 1.0    steps: 104  evaluation reward: 15.92\n",
      "episode: 1655   score: 22.0  epsilon: 1.0    steps: 336  evaluation reward: 16.0\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2824: Policy loss: 0.010754. Value loss: 0.184125. Entropy: 0.894510.\n",
      "Iteration 2825: Policy loss: -0.010632. Value loss: 0.071639. Entropy: 0.896873.\n",
      "Iteration 2826: Policy loss: -0.024205. Value loss: 0.037736. Entropy: 0.892058.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2827: Policy loss: 0.003716. Value loss: 0.697845. Entropy: 0.980628.\n",
      "Iteration 2828: Policy loss: -0.009312. Value loss: 0.234764. Entropy: 0.960671.\n",
      "Iteration 2829: Policy loss: -0.022141. Value loss: 0.116383. Entropy: 0.962819.\n",
      "episode: 1656   score: 17.0  epsilon: 1.0    steps: 520  evaluation reward: 16.06\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2830: Policy loss: 0.012984. Value loss: 0.229827. Entropy: 0.970872.\n",
      "Iteration 2831: Policy loss: -0.015931. Value loss: 0.084412. Entropy: 0.958833.\n",
      "Iteration 2832: Policy loss: -0.026978. Value loss: 0.046392. Entropy: 0.964639.\n",
      "episode: 1657   score: 26.0  epsilon: 1.0    steps: 72  evaluation reward: 16.15\n",
      "episode: 1658   score: 15.0  epsilon: 1.0    steps: 728  evaluation reward: 16.18\n",
      "episode: 1659   score: 20.0  epsilon: 1.0    steps: 784  evaluation reward: 16.25\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2833: Policy loss: 0.010918. Value loss: 0.505917. Entropy: 0.969944.\n",
      "Iteration 2834: Policy loss: -0.002607. Value loss: 0.208520. Entropy: 0.962775.\n",
      "Iteration 2835: Policy loss: -0.016221. Value loss: 0.127035. Entropy: 0.967277.\n",
      "episode: 1660   score: 15.0  epsilon: 1.0    steps: 296  evaluation reward: 16.19\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2836: Policy loss: 0.011350. Value loss: 0.387214. Entropy: 0.943852.\n",
      "Iteration 2837: Policy loss: -0.005782. Value loss: 0.153143. Entropy: 0.933193.\n",
      "Iteration 2838: Policy loss: -0.023404. Value loss: 0.092660. Entropy: 0.932367.\n",
      "episode: 1661   score: 14.0  epsilon: 1.0    steps: 448  evaluation reward: 16.16\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2839: Policy loss: 0.014128. Value loss: 0.718845. Entropy: 0.954203.\n",
      "Iteration 2840: Policy loss: -0.005442. Value loss: 0.308130. Entropy: 0.944728.\n",
      "Iteration 2841: Policy loss: -0.018847. Value loss: 0.177947. Entropy: 0.943063.\n",
      "episode: 1662   score: 24.0  epsilon: 1.0    steps: 120  evaluation reward: 16.24\n",
      "episode: 1663   score: 12.0  epsilon: 1.0    steps: 536  evaluation reward: 16.19\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2842: Policy loss: 0.013421. Value loss: 0.626583. Entropy: 0.954799.\n",
      "Iteration 2843: Policy loss: -0.005997. Value loss: 0.214539. Entropy: 0.941103.\n",
      "Iteration 2844: Policy loss: -0.022816. Value loss: 0.129284. Entropy: 0.941196.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2845: Policy loss: 0.008475. Value loss: 0.447886. Entropy: 0.901813.\n",
      "Iteration 2846: Policy loss: -0.012222. Value loss: 0.201863. Entropy: 0.895682.\n",
      "Iteration 2847: Policy loss: -0.026959. Value loss: 0.123757. Entropy: 0.906500.\n",
      "episode: 1664   score: 16.0  epsilon: 1.0    steps: 504  evaluation reward: 16.18\n",
      "episode: 1665   score: 12.0  epsilon: 1.0    steps: 664  evaluation reward: 16.13\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2848: Policy loss: 0.005732. Value loss: 0.622266. Entropy: 0.944948.\n",
      "Iteration 2849: Policy loss: -0.014298. Value loss: 0.269678. Entropy: 0.941229.\n",
      "Iteration 2850: Policy loss: -0.018735. Value loss: 0.202606. Entropy: 0.930737.\n",
      "episode: 1666   score: 9.0  epsilon: 1.0    steps: 360  evaluation reward: 16.05\n",
      "episode: 1667   score: 16.0  epsilon: 1.0    steps: 736  evaluation reward: 16.01\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2851: Policy loss: 0.011253. Value loss: 0.418375. Entropy: 0.969125.\n",
      "Iteration 2852: Policy loss: -0.013715. Value loss: 0.189078. Entropy: 0.944222.\n",
      "Iteration 2853: Policy loss: -0.028306. Value loss: 0.112578. Entropy: 0.945063.\n",
      "episode: 1668   score: 15.0  epsilon: 1.0    steps: 880  evaluation reward: 16.04\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2854: Policy loss: -0.003195. Value loss: 0.464118. Entropy: 0.971101.\n",
      "Iteration 2855: Policy loss: -0.018630. Value loss: 0.219871. Entropy: 0.972861.\n",
      "Iteration 2856: Policy loss: -0.027259. Value loss: 0.115838. Entropy: 0.962837.\n",
      "episode: 1669   score: 11.0  epsilon: 1.0    steps: 136  evaluation reward: 16.01\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2857: Policy loss: 0.005688. Value loss: 0.368472. Entropy: 0.885057.\n",
      "Iteration 2858: Policy loss: -0.016415. Value loss: 0.167139. Entropy: 0.890522.\n",
      "Iteration 2859: Policy loss: -0.027074. Value loss: 0.088963. Entropy: 0.881579.\n",
      "episode: 1670   score: 17.0  epsilon: 1.0    steps: 736  evaluation reward: 16.07\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2860: Policy loss: 0.011150. Value loss: 0.336937. Entropy: 0.948835.\n",
      "Iteration 2861: Policy loss: -0.011185. Value loss: 0.124614. Entropy: 0.964044.\n",
      "Iteration 2862: Policy loss: -0.026769. Value loss: 0.077261. Entropy: 0.955270.\n",
      "episode: 1671   score: 18.0  epsilon: 1.0    steps: 232  evaluation reward: 16.09\n",
      "episode: 1672   score: 8.0  epsilon: 1.0    steps: 768  evaluation reward: 16.02\n",
      "episode: 1673   score: 11.0  epsilon: 1.0    steps: 992  evaluation reward: 15.94\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2863: Policy loss: 0.005686. Value loss: 0.322925. Entropy: 0.986629.\n",
      "Iteration 2864: Policy loss: -0.013716. Value loss: 0.109725. Entropy: 0.975871.\n",
      "Iteration 2865: Policy loss: -0.026849. Value loss: 0.062524. Entropy: 0.979091.\n",
      "episode: 1674   score: 13.0  epsilon: 1.0    steps: 712  evaluation reward: 15.92\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2866: Policy loss: 0.009658. Value loss: 0.245847. Entropy: 0.952163.\n",
      "Iteration 2867: Policy loss: -0.016622. Value loss: 0.084300. Entropy: 0.961808.\n",
      "Iteration 2868: Policy loss: -0.025128. Value loss: 0.052585. Entropy: 0.967983.\n",
      "episode: 1675   score: 10.0  epsilon: 1.0    steps: 680  evaluation reward: 15.91\n",
      "episode: 1676   score: 13.0  epsilon: 1.0    steps: 848  evaluation reward: 15.81\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2869: Policy loss: 0.012381. Value loss: 0.276128. Entropy: 0.974794.\n",
      "Iteration 2870: Policy loss: -0.013795. Value loss: 0.110419. Entropy: 0.979313.\n",
      "Iteration 2871: Policy loss: -0.030364. Value loss: 0.065531. Entropy: 0.977444.\n",
      "episode: 1677   score: 10.0  epsilon: 1.0    steps: 184  evaluation reward: 15.83\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2872: Policy loss: 0.006083. Value loss: 0.265307. Entropy: 0.933509.\n",
      "Iteration 2873: Policy loss: -0.017036. Value loss: 0.122146. Entropy: 0.935094.\n",
      "Iteration 2874: Policy loss: -0.031975. Value loss: 0.086654. Entropy: 0.932208.\n",
      "episode: 1678   score: 10.0  epsilon: 1.0    steps: 752  evaluation reward: 15.81\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2875: Policy loss: 0.004876. Value loss: 0.183524. Entropy: 0.967339.\n",
      "Iteration 2876: Policy loss: -0.014249. Value loss: 0.083459. Entropy: 0.970637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2877: Policy loss: -0.027395. Value loss: 0.051987. Entropy: 0.967799.\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2878: Policy loss: 0.002675. Value loss: 0.237810. Entropy: 0.947212.\n",
      "Iteration 2879: Policy loss: -0.011990. Value loss: 0.089712. Entropy: 0.951279.\n",
      "Iteration 2880: Policy loss: -0.025306. Value loss: 0.043594. Entropy: 0.946636.\n",
      "episode: 1679   score: 11.0  epsilon: 1.0    steps: 480  evaluation reward: 15.81\n",
      "episode: 1680   score: 11.0  epsilon: 1.0    steps: 1008  evaluation reward: 15.78\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2881: Policy loss: 0.007846. Value loss: 0.538031. Entropy: 0.983177.\n",
      "Iteration 2882: Policy loss: -0.006832. Value loss: 0.224617. Entropy: 0.996897.\n",
      "Iteration 2883: Policy loss: -0.015650. Value loss: 0.101724. Entropy: 0.999264.\n",
      "episode: 1681   score: 16.0  epsilon: 1.0    steps: 784  evaluation reward: 15.78\n",
      "episode: 1682   score: 17.0  epsilon: 1.0    steps: 888  evaluation reward: 15.84\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2884: Policy loss: 0.009638. Value loss: 0.429560. Entropy: 1.052852.\n",
      "Iteration 2885: Policy loss: -0.004751. Value loss: 0.143483. Entropy: 1.064429.\n",
      "Iteration 2886: Policy loss: -0.014472. Value loss: 0.082907. Entropy: 1.068504.\n",
      "episode: 1683   score: 13.0  epsilon: 1.0    steps: 16  evaluation reward: 15.88\n",
      "episode: 1684   score: 20.0  epsilon: 1.0    steps: 104  evaluation reward: 15.96\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2887: Policy loss: 0.005551. Value loss: 0.259232. Entropy: 0.996089.\n",
      "Iteration 2888: Policy loss: -0.019789. Value loss: 0.105349. Entropy: 0.991660.\n",
      "Iteration 2889: Policy loss: -0.027473. Value loss: 0.066132. Entropy: 0.993032.\n",
      "episode: 1685   score: 17.0  epsilon: 1.0    steps: 680  evaluation reward: 15.98\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2890: Policy loss: 0.009653. Value loss: 0.393723. Entropy: 0.970057.\n",
      "Iteration 2891: Policy loss: -0.009470. Value loss: 0.131747. Entropy: 0.961688.\n",
      "Iteration 2892: Policy loss: -0.022797. Value loss: 0.072706. Entropy: 0.962793.\n",
      "episode: 1686   score: 13.0  epsilon: 1.0    steps: 936  evaluation reward: 15.96\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2893: Policy loss: 0.005164. Value loss: 0.206587. Entropy: 0.931744.\n",
      "Iteration 2894: Policy loss: -0.016705. Value loss: 0.078517. Entropy: 0.942959.\n",
      "Iteration 2895: Policy loss: -0.032571. Value loss: 0.045812. Entropy: 0.935829.\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2896: Policy loss: 0.004659. Value loss: 0.390715. Entropy: 1.021838.\n",
      "Iteration 2897: Policy loss: -0.013095. Value loss: 0.180217. Entropy: 1.015908.\n",
      "Iteration 2898: Policy loss: -0.024581. Value loss: 0.115731. Entropy: 1.025103.\n",
      "episode: 1687   score: 9.0  epsilon: 1.0    steps: 592  evaluation reward: 15.81\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2899: Policy loss: 0.010373. Value loss: 0.402184. Entropy: 0.963396.\n",
      "Iteration 2900: Policy loss: -0.004628. Value loss: 0.176971. Entropy: 0.959146.\n",
      "Iteration 2901: Policy loss: -0.019473. Value loss: 0.098126. Entropy: 0.961575.\n",
      "episode: 1688   score: 15.0  epsilon: 1.0    steps: 16  evaluation reward: 15.81\n",
      "episode: 1689   score: 13.0  epsilon: 1.0    steps: 304  evaluation reward: 15.78\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2902: Policy loss: 0.007301. Value loss: 0.196246. Entropy: 1.009448.\n",
      "Iteration 2903: Policy loss: -0.018237. Value loss: 0.077604. Entropy: 1.011827.\n",
      "Iteration 2904: Policy loss: -0.030506. Value loss: 0.042511. Entropy: 1.011894.\n",
      "episode: 1690   score: 15.0  epsilon: 1.0    steps: 136  evaluation reward: 15.71\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2905: Policy loss: 0.008750. Value loss: 0.231569. Entropy: 0.991017.\n",
      "Iteration 2906: Policy loss: -0.017892. Value loss: 0.106562. Entropy: 0.993224.\n",
      "Iteration 2907: Policy loss: -0.029058. Value loss: 0.071036. Entropy: 0.992203.\n",
      "episode: 1691   score: 12.0  epsilon: 1.0    steps: 168  evaluation reward: 15.65\n",
      "episode: 1692   score: 19.0  epsilon: 1.0    steps: 192  evaluation reward: 15.63\n",
      "episode: 1693   score: 10.0  epsilon: 1.0    steps: 976  evaluation reward: 15.59\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2908: Policy loss: 0.011979. Value loss: 0.502494. Entropy: 0.979397.\n",
      "Iteration 2909: Policy loss: -0.003684. Value loss: 0.227690. Entropy: 0.978676.\n",
      "Iteration 2910: Policy loss: -0.018594. Value loss: 0.119833. Entropy: 0.971450.\n",
      "episode: 1694   score: 27.0  epsilon: 1.0    steps: 376  evaluation reward: 15.64\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2911: Policy loss: 0.006531. Value loss: 0.500017. Entropy: 0.966339.\n",
      "Iteration 2912: Policy loss: -0.009619. Value loss: 0.274622. Entropy: 0.962714.\n",
      "Iteration 2913: Policy loss: -0.019768. Value loss: 0.141583. Entropy: 0.977077.\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2914: Policy loss: 0.005617. Value loss: 0.464033. Entropy: 1.015097.\n",
      "Iteration 2915: Policy loss: -0.014438. Value loss: 0.211701. Entropy: 1.011780.\n",
      "Iteration 2916: Policy loss: -0.023676. Value loss: 0.096172. Entropy: 1.009530.\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2917: Policy loss: 0.008202. Value loss: 0.269262. Entropy: 0.992446.\n",
      "Iteration 2918: Policy loss: -0.014110. Value loss: 0.076383. Entropy: 0.980308.\n",
      "Iteration 2919: Policy loss: -0.027209. Value loss: 0.041853. Entropy: 0.985302.\n",
      "episode: 1695   score: 10.0  epsilon: 1.0    steps: 56  evaluation reward: 15.61\n",
      "episode: 1696   score: 18.0  epsilon: 1.0    steps: 272  evaluation reward: 15.58\n",
      "episode: 1697   score: 12.0  epsilon: 1.0    steps: 752  evaluation reward: 15.58\n",
      "episode: 1698   score: 14.0  epsilon: 1.0    steps: 808  evaluation reward: 15.55\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2920: Policy loss: 0.006204. Value loss: 0.291526. Entropy: 0.987213.\n",
      "Iteration 2921: Policy loss: -0.018452. Value loss: 0.112194. Entropy: 0.983807.\n",
      "Iteration 2922: Policy loss: -0.030548. Value loss: 0.067360. Entropy: 0.981914.\n",
      "episode: 1699   score: 10.0  epsilon: 1.0    steps: 1008  evaluation reward: 15.54\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2923: Policy loss: 0.003987. Value loss: 0.355690. Entropy: 0.993764.\n",
      "Iteration 2924: Policy loss: -0.018607. Value loss: 0.132750. Entropy: 0.986026.\n",
      "Iteration 2925: Policy loss: -0.029154. Value loss: 0.086962. Entropy: 0.991225.\n",
      "episode: 1700   score: 14.0  epsilon: 1.0    steps: 40  evaluation reward: 15.51\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2926: Policy loss: 0.004375. Value loss: 0.251459. Entropy: 0.935183.\n",
      "Iteration 2927: Policy loss: -0.016392. Value loss: 0.075289. Entropy: 0.947574.\n",
      "Iteration 2928: Policy loss: -0.029809. Value loss: 0.046394. Entropy: 0.957956.\n",
      "now time :  2019-03-06 13:30:37.157943\n",
      "episode: 1701   score: 15.0  epsilon: 1.0    steps: 288  evaluation reward: 15.54\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2929: Policy loss: 0.004805. Value loss: 0.189021. Entropy: 0.968075.\n",
      "Iteration 2930: Policy loss: -0.013940. Value loss: 0.085267. Entropy: 0.986820.\n",
      "Iteration 2931: Policy loss: -0.026850. Value loss: 0.057368. Entropy: 0.971406.\n",
      "episode: 1702   score: 9.0  epsilon: 1.0    steps: 512  evaluation reward: 15.53\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2932: Policy loss: 0.008516. Value loss: 0.387326. Entropy: 0.977266.\n",
      "Iteration 2933: Policy loss: -0.008329. Value loss: 0.135036. Entropy: 0.964449.\n",
      "Iteration 2934: Policy loss: -0.018978. Value loss: 0.090242. Entropy: 0.956142.\n",
      "episode: 1703   score: 22.0  epsilon: 1.0    steps: 144  evaluation reward: 15.63\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2935: Policy loss: 0.012261. Value loss: 0.191888. Entropy: 0.979121.\n",
      "Iteration 2936: Policy loss: -0.018166. Value loss: 0.080248. Entropy: 0.973200.\n",
      "Iteration 2937: Policy loss: -0.030405. Value loss: 0.052860. Entropy: 0.973765.\n",
      "episode: 1704   score: 13.0  epsilon: 1.0    steps: 936  evaluation reward: 15.63\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2938: Policy loss: 0.011320. Value loss: 0.386924. Entropy: 0.955954.\n",
      "Iteration 2939: Policy loss: -0.001385. Value loss: 0.166889. Entropy: 0.955884.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2940: Policy loss: -0.010410. Value loss: 0.074226. Entropy: 0.958802.\n",
      "episode: 1705   score: 17.0  epsilon: 1.0    steps: 528  evaluation reward: 15.62\n",
      "episode: 1706   score: 20.0  epsilon: 1.0    steps: 776  evaluation reward: 15.73\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2941: Policy loss: 0.006354. Value loss: 0.431884. Entropy: 0.951040.\n",
      "Iteration 2942: Policy loss: -0.012808. Value loss: 0.138571. Entropy: 0.934056.\n",
      "Iteration 2943: Policy loss: -0.021711. Value loss: 0.071569. Entropy: 0.926040.\n",
      "episode: 1707   score: 11.0  epsilon: 1.0    steps: 168  evaluation reward: 15.74\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2944: Policy loss: 0.012244. Value loss: 0.200730. Entropy: 1.000986.\n",
      "Iteration 2945: Policy loss: -0.015354. Value loss: 0.065380. Entropy: 0.988558.\n",
      "Iteration 2946: Policy loss: -0.029380. Value loss: 0.038649. Entropy: 0.988390.\n",
      "episode: 1708   score: 20.0  epsilon: 1.0    steps: 904  evaluation reward: 15.81\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2947: Policy loss: 0.010868. Value loss: 0.225317. Entropy: 0.955439.\n",
      "Iteration 2948: Policy loss: -0.011892. Value loss: 0.091728. Entropy: 0.957183.\n",
      "Iteration 2949: Policy loss: -0.028707. Value loss: 0.064524. Entropy: 0.957330.\n",
      "episode: 1709   score: 19.0  epsilon: 1.0    steps: 400  evaluation reward: 15.83\n",
      "episode: 1710   score: 11.0  epsilon: 1.0    steps: 776  evaluation reward: 15.73\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2950: Policy loss: 0.004324. Value loss: 0.416212. Entropy: 0.940813.\n",
      "Iteration 2951: Policy loss: -0.015533. Value loss: 0.239199. Entropy: 0.928823.\n",
      "Iteration 2952: Policy loss: -0.027820. Value loss: 0.176172. Entropy: 0.930075.\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2953: Policy loss: 0.014321. Value loss: 0.455783. Entropy: 0.986237.\n",
      "Iteration 2954: Policy loss: -0.012310. Value loss: 0.175469. Entropy: 0.985795.\n",
      "Iteration 2955: Policy loss: -0.024653. Value loss: 0.091549. Entropy: 0.984108.\n",
      "episode: 1711   score: 15.0  epsilon: 1.0    steps: 24  evaluation reward: 15.68\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2956: Policy loss: 0.003452. Value loss: 0.593112. Entropy: 0.975168.\n",
      "Iteration 2957: Policy loss: -0.017620. Value loss: 0.243587. Entropy: 0.969665.\n",
      "Iteration 2958: Policy loss: -0.028514. Value loss: 0.141431. Entropy: 0.969779.\n",
      "episode: 1712   score: 13.0  epsilon: 1.0    steps: 1000  evaluation reward: 15.7\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2959: Policy loss: 0.017172. Value loss: 0.787847. Entropy: 0.945897.\n",
      "Iteration 2960: Policy loss: -0.004469. Value loss: 0.304582. Entropy: 0.926732.\n",
      "Iteration 2961: Policy loss: -0.019775. Value loss: 0.172539. Entropy: 0.921584.\n",
      "episode: 1713   score: 8.0  epsilon: 1.0    steps: 184  evaluation reward: 15.58\n",
      "episode: 1714   score: 17.0  epsilon: 1.0    steps: 752  evaluation reward: 15.53\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2962: Policy loss: 0.005604. Value loss: 0.374620. Entropy: 0.945711.\n",
      "Iteration 2963: Policy loss: -0.016311. Value loss: 0.138594. Entropy: 0.934493.\n",
      "Iteration 2964: Policy loss: -0.027674. Value loss: 0.085965. Entropy: 0.934958.\n",
      "episode: 1715   score: 23.0  epsilon: 1.0    steps: 392  evaluation reward: 15.66\n",
      "episode: 1716   score: 33.0  epsilon: 1.0    steps: 744  evaluation reward: 15.9\n",
      "episode: 1717   score: 9.0  epsilon: 1.0    steps: 904  evaluation reward: 15.82\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2965: Policy loss: 0.012460. Value loss: 0.806041. Entropy: 0.899188.\n",
      "Iteration 2966: Policy loss: 0.008559. Value loss: 0.315988. Entropy: 0.890059.\n",
      "Iteration 2967: Policy loss: -0.012956. Value loss: 0.175669. Entropy: 0.880608.\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2968: Policy loss: 0.005851. Value loss: 0.386531. Entropy: 0.876961.\n",
      "Iteration 2969: Policy loss: -0.003010. Value loss: 0.179259. Entropy: 0.890900.\n",
      "Iteration 2970: Policy loss: -0.021436. Value loss: 0.118728. Entropy: 0.891415.\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2971: Policy loss: 0.007013. Value loss: 0.281308. Entropy: 0.958050.\n",
      "Iteration 2972: Policy loss: -0.017396. Value loss: 0.113873. Entropy: 0.957784.\n",
      "Iteration 2973: Policy loss: -0.029694. Value loss: 0.067747. Entropy: 0.956753.\n",
      "episode: 1718   score: 24.0  epsilon: 1.0    steps: 520  evaluation reward: 15.92\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2974: Policy loss: 0.005178. Value loss: 0.465835. Entropy: 0.954597.\n",
      "Iteration 2975: Policy loss: -0.019594. Value loss: 0.192889. Entropy: 0.968779.\n",
      "Iteration 2976: Policy loss: -0.028902. Value loss: 0.124329. Entropy: 0.959522.\n",
      "episode: 1719   score: 16.0  epsilon: 1.0    steps: 624  evaluation reward: 15.97\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2977: Policy loss: 0.009299. Value loss: 0.445921. Entropy: 0.989803.\n",
      "Iteration 2978: Policy loss: -0.007474. Value loss: 0.181473. Entropy: 0.994516.\n",
      "Iteration 2979: Policy loss: -0.016570. Value loss: 0.113627. Entropy: 0.989177.\n",
      "episode: 1720   score: 17.0  epsilon: 1.0    steps: 24  evaluation reward: 15.98\n",
      "episode: 1721   score: 13.0  epsilon: 1.0    steps: 824  evaluation reward: 15.88\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2980: Policy loss: 0.003049. Value loss: 0.394559. Entropy: 0.979194.\n",
      "Iteration 2981: Policy loss: -0.017124. Value loss: 0.157839. Entropy: 0.956446.\n",
      "Iteration 2982: Policy loss: -0.027952. Value loss: 0.104488. Entropy: 0.967983.\n",
      "episode: 1722   score: 13.0  epsilon: 1.0    steps: 152  evaluation reward: 15.84\n",
      "episode: 1723   score: 21.0  epsilon: 1.0    steps: 680  evaluation reward: 15.87\n",
      "episode: 1724   score: 15.0  epsilon: 1.0    steps: 744  evaluation reward: 15.91\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2983: Policy loss: 0.003470. Value loss: 0.340255. Entropy: 0.925619.\n",
      "Iteration 2984: Policy loss: -0.008361. Value loss: 0.133043. Entropy: 0.934552.\n",
      "Iteration 2985: Policy loss: -0.018190. Value loss: 0.074204. Entropy: 0.939129.\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2986: Policy loss: 0.004787. Value loss: 0.447922. Entropy: 0.947524.\n",
      "Iteration 2987: Policy loss: -0.007284. Value loss: 0.204749. Entropy: 0.955556.\n",
      "Iteration 2988: Policy loss: -0.017122. Value loss: 0.119421. Entropy: 0.961669.\n",
      "episode: 1725   score: 19.0  epsilon: 1.0    steps: 96  evaluation reward: 15.95\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2989: Policy loss: 0.009172. Value loss: 0.483535. Entropy: 0.933199.\n",
      "Iteration 2990: Policy loss: -0.010423. Value loss: 0.185715. Entropy: 0.946153.\n",
      "Iteration 2991: Policy loss: -0.019420. Value loss: 0.103618. Entropy: 0.957838.\n",
      "episode: 1726   score: 16.0  epsilon: 1.0    steps: 816  evaluation reward: 15.92\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2992: Policy loss: 0.009588. Value loss: 0.473171. Entropy: 0.995035.\n",
      "Iteration 2993: Policy loss: -0.009255. Value loss: 0.187184. Entropy: 1.001951.\n",
      "Iteration 2994: Policy loss: -0.022342. Value loss: 0.093385. Entropy: 0.988715.\n",
      "episode: 1727   score: 11.0  epsilon: 1.0    steps: 760  evaluation reward: 15.87\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2995: Policy loss: 0.003809. Value loss: 0.386828. Entropy: 0.909235.\n",
      "Iteration 2996: Policy loss: -0.015727. Value loss: 0.145360. Entropy: 0.914022.\n",
      "Iteration 2997: Policy loss: -0.022878. Value loss: 0.078715. Entropy: 0.904133.\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2998: Policy loss: 0.010731. Value loss: 0.416820. Entropy: 1.015518.\n",
      "Iteration 2999: Policy loss: -0.017467. Value loss: 0.148523. Entropy: 1.017715.\n",
      "Iteration 3000: Policy loss: -0.025670. Value loss: 0.111501. Entropy: 1.018159.\n",
      "episode: 1728   score: 11.0  epsilon: 1.0    steps: 96  evaluation reward: 15.85\n",
      "episode: 1729   score: 21.0  epsilon: 1.0    steps: 320  evaluation reward: 15.9\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3001: Policy loss: 0.008939. Value loss: 0.339738. Entropy: 0.988924.\n",
      "Iteration 3002: Policy loss: -0.007032. Value loss: 0.194506. Entropy: 0.972141.\n",
      "Iteration 3003: Policy loss: -0.025135. Value loss: 0.138934. Entropy: 0.971512.\n",
      "episode: 1730   score: 14.0  epsilon: 1.0    steps: 208  evaluation reward: 15.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3004: Policy loss: 0.005679. Value loss: 0.366318. Entropy: 1.011024.\n",
      "Iteration 3005: Policy loss: -0.011233. Value loss: 0.140017. Entropy: 1.007031.\n",
      "Iteration 3006: Policy loss: -0.023599. Value loss: 0.081643. Entropy: 1.018013.\n",
      "episode: 1731   score: 25.0  epsilon: 1.0    steps: 144  evaluation reward: 15.95\n",
      "episode: 1732   score: 17.0  epsilon: 1.0    steps: 560  evaluation reward: 16.0\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3007: Policy loss: 0.008029. Value loss: 0.542695. Entropy: 0.921114.\n",
      "Iteration 3008: Policy loss: -0.011444. Value loss: 0.191048. Entropy: 0.922637.\n",
      "Iteration 3009: Policy loss: -0.017682. Value loss: 0.088894. Entropy: 0.914391.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3010: Policy loss: 0.011244. Value loss: 0.448879. Entropy: 0.992171.\n",
      "Iteration 3011: Policy loss: -0.012237. Value loss: 0.224424. Entropy: 0.983006.\n",
      "Iteration 3012: Policy loss: -0.024276. Value loss: 0.138537. Entropy: 0.978496.\n",
      "episode: 1733   score: 18.0  epsilon: 1.0    steps: 280  evaluation reward: 16.01\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3013: Policy loss: 0.011703. Value loss: 0.445948. Entropy: 0.968586.\n",
      "Iteration 3014: Policy loss: -0.006100. Value loss: 0.135937. Entropy: 0.950716.\n",
      "Iteration 3015: Policy loss: -0.021749. Value loss: 0.073414. Entropy: 0.966429.\n",
      "episode: 1734   score: 26.0  epsilon: 1.0    steps: 520  evaluation reward: 16.09\n",
      "episode: 1735   score: 24.0  epsilon: 1.0    steps: 864  evaluation reward: 16.22\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3016: Policy loss: 0.004783. Value loss: 0.440552. Entropy: 0.984903.\n",
      "Iteration 3017: Policy loss: -0.012691. Value loss: 0.149138. Entropy: 0.966380.\n",
      "Iteration 3018: Policy loss: -0.023535. Value loss: 0.083104. Entropy: 0.970532.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3019: Policy loss: 0.005512. Value loss: 0.312004. Entropy: 0.974173.\n",
      "Iteration 3020: Policy loss: -0.011580. Value loss: 0.119551. Entropy: 0.970402.\n",
      "Iteration 3021: Policy loss: -0.027550. Value loss: 0.074284. Entropy: 0.968001.\n",
      "episode: 1736   score: 18.0  epsilon: 1.0    steps: 248  evaluation reward: 16.19\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3022: Policy loss: 0.009818. Value loss: 0.551022. Entropy: 0.969422.\n",
      "Iteration 3023: Policy loss: -0.011228. Value loss: 0.219702. Entropy: 0.970000.\n",
      "Iteration 3024: Policy loss: -0.024455. Value loss: 0.113734. Entropy: 0.960497.\n",
      "episode: 1737   score: 21.0  epsilon: 1.0    steps: 144  evaluation reward: 16.19\n",
      "episode: 1738   score: 11.0  epsilon: 1.0    steps: 360  evaluation reward: 16.01\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3025: Policy loss: 0.005049. Value loss: 0.427354. Entropy: 0.925201.\n",
      "Iteration 3026: Policy loss: -0.014107. Value loss: 0.165387. Entropy: 0.920417.\n",
      "Iteration 3027: Policy loss: -0.019560. Value loss: 0.099070. Entropy: 0.922584.\n",
      "episode: 1739   score: 18.0  epsilon: 1.0    steps: 600  evaluation reward: 16.03\n",
      "episode: 1740   score: 26.0  epsilon: 1.0    steps: 784  evaluation reward: 16.09\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3028: Policy loss: 0.012660. Value loss: 0.572178. Entropy: 0.912852.\n",
      "Iteration 3029: Policy loss: -0.008432. Value loss: 0.221044. Entropy: 0.915808.\n",
      "Iteration 3030: Policy loss: -0.018130. Value loss: 0.117371. Entropy: 0.915304.\n",
      "episode: 1741   score: 17.0  epsilon: 1.0    steps: 320  evaluation reward: 15.91\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3031: Policy loss: 0.007951. Value loss: 0.412106. Entropy: 0.904813.\n",
      "Iteration 3032: Policy loss: -0.010146. Value loss: 0.212788. Entropy: 0.895692.\n",
      "Iteration 3033: Policy loss: -0.014372. Value loss: 0.163977. Entropy: 0.906558.\n",
      "episode: 1742   score: 16.0  epsilon: 1.0    steps: 752  evaluation reward: 15.91\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3034: Policy loss: 0.007227. Value loss: 0.391964. Entropy: 0.941474.\n",
      "Iteration 3035: Policy loss: -0.012635. Value loss: 0.152122. Entropy: 0.933556.\n",
      "Iteration 3036: Policy loss: -0.024870. Value loss: 0.107507. Entropy: 0.945628.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3037: Policy loss: 0.009535. Value loss: 0.391758. Entropy: 0.995661.\n",
      "Iteration 3038: Policy loss: -0.011489. Value loss: 0.162779. Entropy: 0.992072.\n",
      "Iteration 3039: Policy loss: -0.024057. Value loss: 0.076390. Entropy: 0.992296.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3040: Policy loss: 0.017445. Value loss: 0.595220. Entropy: 0.947030.\n",
      "Iteration 3041: Policy loss: -0.009722. Value loss: 0.208258. Entropy: 0.977089.\n",
      "Iteration 3042: Policy loss: -0.024391. Value loss: 0.125949. Entropy: 0.963580.\n",
      "episode: 1743   score: 19.0  epsilon: 1.0    steps: 152  evaluation reward: 15.89\n",
      "episode: 1744   score: 23.0  epsilon: 1.0    steps: 208  evaluation reward: 15.94\n",
      "episode: 1745   score: 14.0  epsilon: 1.0    steps: 392  evaluation reward: 15.85\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3043: Policy loss: 0.011036. Value loss: 0.499016. Entropy: 0.954051.\n",
      "Iteration 3044: Policy loss: -0.006453. Value loss: 0.223108. Entropy: 0.953472.\n",
      "Iteration 3045: Policy loss: -0.017084. Value loss: 0.115527. Entropy: 0.956807.\n",
      "episode: 1746   score: 24.0  epsilon: 1.0    steps: 64  evaluation reward: 15.87\n",
      "episode: 1747   score: 14.0  epsilon: 1.0    steps: 928  evaluation reward: 15.9\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3046: Policy loss: 0.011585. Value loss: 0.616354. Entropy: 1.004761.\n",
      "Iteration 3047: Policy loss: -0.004699. Value loss: 0.207130. Entropy: 1.022601.\n",
      "Iteration 3048: Policy loss: -0.022261. Value loss: 0.116610. Entropy: 1.019149.\n",
      "episode: 1748   score: 13.0  epsilon: 1.0    steps: 392  evaluation reward: 15.88\n",
      "episode: 1749   score: 20.0  epsilon: 1.0    steps: 880  evaluation reward: 15.93\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3049: Policy loss: 0.013003. Value loss: 0.577704. Entropy: 0.925173.\n",
      "Iteration 3050: Policy loss: -0.001060. Value loss: 0.212238. Entropy: 0.928880.\n",
      "Iteration 3051: Policy loss: -0.013935. Value loss: 0.114427. Entropy: 0.930436.\n",
      "episode: 1750   score: 16.0  epsilon: 1.0    steps: 872  evaluation reward: 15.9\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3052: Policy loss: 0.006891. Value loss: 0.395763. Entropy: 0.941430.\n",
      "Iteration 3053: Policy loss: -0.005982. Value loss: 0.173222. Entropy: 0.943552.\n",
      "Iteration 3054: Policy loss: -0.020140. Value loss: 0.100075. Entropy: 0.934072.\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3055: Policy loss: 0.011438. Value loss: 0.372385. Entropy: 0.964981.\n",
      "Iteration 3056: Policy loss: -0.010956. Value loss: 0.136858. Entropy: 0.970336.\n",
      "Iteration 3057: Policy loss: -0.022641. Value loss: 0.079311. Entropy: 0.967542.\n",
      "now time :  2019-03-06 13:33:23.069266\n",
      "episode: 1751   score: 12.0  epsilon: 1.0    steps: 600  evaluation reward: 15.86\n",
      "episode: 1752   score: 15.0  epsilon: 1.0    steps: 968  evaluation reward: 15.87\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3058: Policy loss: 0.006088. Value loss: 0.593954. Entropy: 0.967044.\n",
      "Iteration 3059: Policy loss: -0.008274. Value loss: 0.221550. Entropy: 0.982221.\n",
      "Iteration 3060: Policy loss: -0.018433. Value loss: 0.142107. Entropy: 0.969488.\n",
      "episode: 1753   score: 21.0  epsilon: 1.0    steps: 1000  evaluation reward: 15.93\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3061: Policy loss: 0.017754. Value loss: 0.424268. Entropy: 0.932631.\n",
      "Iteration 3062: Policy loss: -0.005557. Value loss: 0.157476. Entropy: 0.929806.\n",
      "Iteration 3063: Policy loss: -0.017364. Value loss: 0.097113. Entropy: 0.925288.\n",
      "episode: 1754   score: 10.0  epsilon: 1.0    steps: 32  evaluation reward: 15.94\n",
      "episode: 1755   score: 12.0  epsilon: 1.0    steps: 72  evaluation reward: 15.84\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3064: Policy loss: 0.007732. Value loss: 0.380598. Entropy: 0.968895.\n",
      "Iteration 3065: Policy loss: -0.010499. Value loss: 0.151979. Entropy: 0.972313.\n",
      "Iteration 3066: Policy loss: -0.021667. Value loss: 0.090857. Entropy: 0.962625.\n",
      "Training network. lr: 0.000227. clip: 0.090637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3067: Policy loss: 0.013318. Value loss: 0.302992. Entropy: 0.937832.\n",
      "Iteration 3068: Policy loss: -0.016776. Value loss: 0.115078. Entropy: 0.931359.\n",
      "Iteration 3069: Policy loss: -0.027425. Value loss: 0.061821. Entropy: 0.937585.\n",
      "episode: 1756   score: 15.0  epsilon: 1.0    steps: 928  evaluation reward: 15.82\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3070: Policy loss: 0.011804. Value loss: 0.328916. Entropy: 0.996334.\n",
      "Iteration 3071: Policy loss: -0.014153. Value loss: 0.133599. Entropy: 1.000337.\n",
      "Iteration 3072: Policy loss: -0.023729. Value loss: 0.080072. Entropy: 0.997341.\n",
      "episode: 1757   score: 16.0  epsilon: 1.0    steps: 48  evaluation reward: 15.72\n",
      "episode: 1758   score: 21.0  epsilon: 1.0    steps: 440  evaluation reward: 15.78\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3073: Policy loss: 0.009119. Value loss: 0.506576. Entropy: 0.979036.\n",
      "Iteration 3074: Policy loss: -0.010379. Value loss: 0.233189. Entropy: 0.983949.\n",
      "Iteration 3075: Policy loss: -0.016441. Value loss: 0.143630. Entropy: 0.986788.\n",
      "episode: 1759   score: 11.0  epsilon: 1.0    steps: 80  evaluation reward: 15.69\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3076: Policy loss: 0.009934. Value loss: 0.326126. Entropy: 0.924272.\n",
      "Iteration 3077: Policy loss: -0.007837. Value loss: 0.140863. Entropy: 0.907487.\n",
      "Iteration 3078: Policy loss: -0.015549. Value loss: 0.084103. Entropy: 0.907331.\n",
      "episode: 1760   score: 18.0  epsilon: 1.0    steps: 488  evaluation reward: 15.72\n",
      "episode: 1761   score: 14.0  epsilon: 1.0    steps: 920  evaluation reward: 15.72\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3079: Policy loss: 0.004426. Value loss: 0.514039. Entropy: 0.955568.\n",
      "Iteration 3080: Policy loss: -0.011123. Value loss: 0.248726. Entropy: 0.943115.\n",
      "Iteration 3081: Policy loss: -0.020021. Value loss: 0.147226. Entropy: 0.950715.\n",
      "episode: 1762   score: 15.0  epsilon: 1.0    steps: 8  evaluation reward: 15.63\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3082: Policy loss: 0.011231. Value loss: 0.357538. Entropy: 0.952061.\n",
      "Iteration 3083: Policy loss: -0.006766. Value loss: 0.110873. Entropy: 0.943402.\n",
      "Iteration 3084: Policy loss: -0.019702. Value loss: 0.062080. Entropy: 0.928045.\n",
      "episode: 1763   score: 24.0  epsilon: 1.0    steps: 568  evaluation reward: 15.75\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3085: Policy loss: 0.012272. Value loss: 0.291519. Entropy: 0.959009.\n",
      "Iteration 3086: Policy loss: -0.013147. Value loss: 0.125615. Entropy: 0.940333.\n",
      "Iteration 3087: Policy loss: -0.025344. Value loss: 0.061379. Entropy: 0.951336.\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3088: Policy loss: 0.011157. Value loss: 0.350870. Entropy: 0.931270.\n",
      "Iteration 3089: Policy loss: -0.011083. Value loss: 0.151962. Entropy: 0.935765.\n",
      "Iteration 3090: Policy loss: -0.017164. Value loss: 0.082003. Entropy: 0.908524.\n",
      "episode: 1764   score: 13.0  epsilon: 1.0    steps: 608  evaluation reward: 15.72\n",
      "episode: 1765   score: 13.0  epsilon: 1.0    steps: 960  evaluation reward: 15.73\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3091: Policy loss: 0.014781. Value loss: 0.655385. Entropy: 0.976988.\n",
      "Iteration 3092: Policy loss: -0.009506. Value loss: 0.274141. Entropy: 0.948104.\n",
      "Iteration 3093: Policy loss: -0.020334. Value loss: 0.148787. Entropy: 0.952546.\n",
      "episode: 1766   score: 18.0  epsilon: 1.0    steps: 480  evaluation reward: 15.82\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3094: Policy loss: 0.010357. Value loss: 0.540838. Entropy: 0.942255.\n",
      "Iteration 3095: Policy loss: -0.015185. Value loss: 0.195658. Entropy: 0.946137.\n",
      "Iteration 3096: Policy loss: -0.023797. Value loss: 0.113369. Entropy: 0.934273.\n",
      "episode: 1767   score: 23.0  epsilon: 1.0    steps: 80  evaluation reward: 15.89\n",
      "episode: 1768   score: 13.0  epsilon: 1.0    steps: 1016  evaluation reward: 15.87\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3097: Policy loss: 0.006126. Value loss: 0.348313. Entropy: 0.914533.\n",
      "Iteration 3098: Policy loss: -0.011655. Value loss: 0.128867. Entropy: 0.921991.\n",
      "Iteration 3099: Policy loss: -0.027670. Value loss: 0.073087. Entropy: 0.888341.\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3100: Policy loss: 0.011376. Value loss: 0.478596. Entropy: 0.926681.\n",
      "Iteration 3101: Policy loss: -0.011081. Value loss: 0.186061. Entropy: 0.932006.\n",
      "Iteration 3102: Policy loss: -0.024562. Value loss: 0.101705. Entropy: 0.936522.\n",
      "episode: 1769   score: 15.0  epsilon: 1.0    steps: 240  evaluation reward: 15.91\n",
      "episode: 1770   score: 15.0  epsilon: 1.0    steps: 560  evaluation reward: 15.89\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3103: Policy loss: 0.010682. Value loss: 0.361627. Entropy: 0.934750.\n",
      "Iteration 3104: Policy loss: -0.012936. Value loss: 0.141820. Entropy: 0.923136.\n",
      "Iteration 3105: Policy loss: -0.021582. Value loss: 0.073111. Entropy: 0.914679.\n",
      "episode: 1771   score: 21.0  epsilon: 1.0    steps: 944  evaluation reward: 15.92\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3106: Policy loss: 0.008916. Value loss: 0.502789. Entropy: 0.944923.\n",
      "Iteration 3107: Policy loss: -0.008695. Value loss: 0.226647. Entropy: 0.929058.\n",
      "Iteration 3108: Policy loss: -0.017562. Value loss: 0.140356. Entropy: 0.927306.\n",
      "episode: 1772   score: 13.0  epsilon: 1.0    steps: 800  evaluation reward: 15.97\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3109: Policy loss: 0.006026. Value loss: 0.337193. Entropy: 0.870171.\n",
      "Iteration 3110: Policy loss: -0.017274. Value loss: 0.123191. Entropy: 0.865763.\n",
      "Iteration 3111: Policy loss: -0.027757. Value loss: 0.075345. Entropy: 0.871371.\n",
      "episode: 1773   score: 11.0  epsilon: 1.0    steps: 408  evaluation reward: 15.97\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3112: Policy loss: 0.012079. Value loss: 0.399768. Entropy: 0.956040.\n",
      "Iteration 3113: Policy loss: -0.010063. Value loss: 0.118397. Entropy: 0.950829.\n",
      "Iteration 3114: Policy loss: -0.019263. Value loss: 0.060910. Entropy: 0.949523.\n",
      "episode: 1774   score: 22.0  epsilon: 1.0    steps: 808  evaluation reward: 16.06\n",
      "episode: 1775   score: 18.0  epsilon: 1.0    steps: 840  evaluation reward: 16.14\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3115: Policy loss: 0.009282. Value loss: 0.425789. Entropy: 0.925530.\n",
      "Iteration 3116: Policy loss: -0.011773. Value loss: 0.130383. Entropy: 0.920223.\n",
      "Iteration 3117: Policy loss: -0.020948. Value loss: 0.068388. Entropy: 0.925731.\n",
      "episode: 1776   score: 13.0  epsilon: 1.0    steps: 160  evaluation reward: 16.14\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3118: Policy loss: 0.010288. Value loss: 0.279042. Entropy: 0.920842.\n",
      "Iteration 3119: Policy loss: -0.016783. Value loss: 0.132536. Entropy: 0.912911.\n",
      "Iteration 3120: Policy loss: -0.027303. Value loss: 0.085575. Entropy: 0.918205.\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3121: Policy loss: 0.012722. Value loss: 0.300229. Entropy: 0.922566.\n",
      "Iteration 3122: Policy loss: -0.016536. Value loss: 0.102942. Entropy: 0.925666.\n",
      "Iteration 3123: Policy loss: -0.027249. Value loss: 0.051446. Entropy: 0.920976.\n",
      "episode: 1777   score: 16.0  epsilon: 1.0    steps: 400  evaluation reward: 16.2\n",
      "episode: 1778   score: 14.0  epsilon: 1.0    steps: 552  evaluation reward: 16.24\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3124: Policy loss: 0.004874. Value loss: 0.476492. Entropy: 1.022067.\n",
      "Iteration 3125: Policy loss: -0.013500. Value loss: 0.200782. Entropy: 1.013840.\n",
      "Iteration 3126: Policy loss: -0.023435. Value loss: 0.116825. Entropy: 1.007033.\n",
      "episode: 1779   score: 12.0  epsilon: 1.0    steps: 688  evaluation reward: 16.25\n",
      "episode: 1780   score: 15.0  epsilon: 1.0    steps: 1016  evaluation reward: 16.29\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3127: Policy loss: 0.012543. Value loss: 0.395677. Entropy: 0.910015.\n",
      "Iteration 3128: Policy loss: 0.003867. Value loss: 0.142312. Entropy: 0.907328.\n",
      "Iteration 3129: Policy loss: -0.013006. Value loss: 0.088747. Entropy: 0.890632.\n",
      "episode: 1781   score: 19.0  epsilon: 1.0    steps: 232  evaluation reward: 16.32\n",
      "Training network. lr: 0.000226. clip: 0.090480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3130: Policy loss: 0.008522. Value loss: 0.279534. Entropy: 0.962633.\n",
      "Iteration 3131: Policy loss: -0.016590. Value loss: 0.113188. Entropy: 0.957533.\n",
      "Iteration 3132: Policy loss: -0.027277. Value loss: 0.064233. Entropy: 0.951689.\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3133: Policy loss: 0.004894. Value loss: 0.660686. Entropy: 0.917975.\n",
      "Iteration 3134: Policy loss: -0.008839. Value loss: 0.268759. Entropy: 0.916193.\n",
      "Iteration 3135: Policy loss: -0.015835. Value loss: 0.121830. Entropy: 0.927228.\n",
      "episode: 1782   score: 13.0  epsilon: 1.0    steps: 152  evaluation reward: 16.28\n",
      "episode: 1783   score: 16.0  epsilon: 1.0    steps: 704  evaluation reward: 16.31\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3136: Policy loss: 0.005742. Value loss: 0.301512. Entropy: 0.889556.\n",
      "Iteration 3137: Policy loss: -0.013402. Value loss: 0.111083. Entropy: 0.907054.\n",
      "Iteration 3138: Policy loss: -0.027208. Value loss: 0.071651. Entropy: 0.911626.\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3139: Policy loss: 0.007238. Value loss: 0.367323. Entropy: 0.909230.\n",
      "Iteration 3140: Policy loss: -0.007939. Value loss: 0.128459. Entropy: 0.917463.\n",
      "Iteration 3141: Policy loss: -0.015063. Value loss: 0.079269. Entropy: 0.927244.\n",
      "episode: 1784   score: 21.0  epsilon: 1.0    steps: 328  evaluation reward: 16.32\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3142: Policy loss: 0.006192. Value loss: 0.398068. Entropy: 0.922736.\n",
      "Iteration 3143: Policy loss: -0.008118. Value loss: 0.204472. Entropy: 0.926959.\n",
      "Iteration 3144: Policy loss: -0.023508. Value loss: 0.140912. Entropy: 0.918533.\n",
      "episode: 1785   score: 27.0  epsilon: 1.0    steps: 432  evaluation reward: 16.42\n",
      "episode: 1786   score: 14.0  epsilon: 1.0    steps: 432  evaluation reward: 16.43\n",
      "episode: 1787   score: 12.0  epsilon: 1.0    steps: 864  evaluation reward: 16.46\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3145: Policy loss: 0.006568. Value loss: 0.542108. Entropy: 0.980676.\n",
      "Iteration 3146: Policy loss: -0.004869. Value loss: 0.174829. Entropy: 0.959277.\n",
      "Iteration 3147: Policy loss: -0.016937. Value loss: 0.078796. Entropy: 0.966853.\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3148: Policy loss: 0.007275. Value loss: 0.222848. Entropy: 0.901241.\n",
      "Iteration 3149: Policy loss: -0.014731. Value loss: 0.073525. Entropy: 0.902421.\n",
      "Iteration 3150: Policy loss: -0.019527. Value loss: 0.068321. Entropy: 0.900808.\n",
      "episode: 1788   score: 19.0  epsilon: 1.0    steps: 624  evaluation reward: 16.5\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3151: Policy loss: 0.013048. Value loss: 0.296814. Entropy: 0.940371.\n",
      "Iteration 3152: Policy loss: -0.014945. Value loss: 0.115984. Entropy: 0.913441.\n",
      "Iteration 3153: Policy loss: -0.024120. Value loss: 0.072114. Entropy: 0.927841.\n",
      "episode: 1789   score: 24.0  epsilon: 1.0    steps: 384  evaluation reward: 16.61\n",
      "episode: 1790   score: 19.0  epsilon: 1.0    steps: 768  evaluation reward: 16.65\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3154: Policy loss: 0.010337. Value loss: 0.306852. Entropy: 0.966507.\n",
      "Iteration 3155: Policy loss: -0.011689. Value loss: 0.131334. Entropy: 0.963397.\n",
      "Iteration 3156: Policy loss: -0.026485. Value loss: 0.082824. Entropy: 0.956597.\n",
      "episode: 1791   score: 16.0  epsilon: 1.0    steps: 760  evaluation reward: 16.69\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3157: Policy loss: 0.014803. Value loss: 0.323773. Entropy: 0.945572.\n",
      "Iteration 3158: Policy loss: -0.015816. Value loss: 0.138434. Entropy: 0.950983.\n",
      "Iteration 3159: Policy loss: -0.023594. Value loss: 0.075418. Entropy: 0.942591.\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3160: Policy loss: 0.006914. Value loss: 0.411485. Entropy: 0.924958.\n",
      "Iteration 3161: Policy loss: -0.011263. Value loss: 0.171873. Entropy: 0.910383.\n",
      "Iteration 3162: Policy loss: -0.020751. Value loss: 0.095801. Entropy: 0.916442.\n",
      "episode: 1792   score: 15.0  epsilon: 1.0    steps: 328  evaluation reward: 16.65\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3163: Policy loss: 0.004679. Value loss: 0.266888. Entropy: 0.961806.\n",
      "Iteration 3164: Policy loss: -0.012632. Value loss: 0.104138. Entropy: 0.963600.\n",
      "Iteration 3165: Policy loss: -0.025644. Value loss: 0.066381. Entropy: 0.964102.\n",
      "episode: 1793   score: 17.0  epsilon: 1.0    steps: 56  evaluation reward: 16.72\n",
      "episode: 1794   score: 17.0  epsilon: 1.0    steps: 104  evaluation reward: 16.62\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3166: Policy loss: 0.002050. Value loss: 0.253400. Entropy: 0.929093.\n",
      "Iteration 3167: Policy loss: -0.014967. Value loss: 0.119270. Entropy: 0.938618.\n",
      "Iteration 3168: Policy loss: -0.028806. Value loss: 0.083105. Entropy: 0.928085.\n",
      "episode: 1795   score: 9.0  epsilon: 1.0    steps: 512  evaluation reward: 16.61\n",
      "episode: 1796   score: 25.0  epsilon: 1.0    steps: 912  evaluation reward: 16.68\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3169: Policy loss: 0.004975. Value loss: 0.407460. Entropy: 0.952837.\n",
      "Iteration 3170: Policy loss: -0.008662. Value loss: 0.160290. Entropy: 0.955922.\n",
      "Iteration 3171: Policy loss: -0.018922. Value loss: 0.087941. Entropy: 0.946198.\n",
      "episode: 1797   score: 13.0  epsilon: 1.0    steps: 304  evaluation reward: 16.69\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3172: Policy loss: 0.013344. Value loss: 0.208125. Entropy: 0.896503.\n",
      "Iteration 3173: Policy loss: -0.011619. Value loss: 0.085779. Entropy: 0.882852.\n",
      "Iteration 3174: Policy loss: -0.026761. Value loss: 0.045805. Entropy: 0.871831.\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3175: Policy loss: 0.012014. Value loss: 0.275394. Entropy: 0.977003.\n",
      "Iteration 3176: Policy loss: -0.017296. Value loss: 0.120162. Entropy: 0.984088.\n",
      "Iteration 3177: Policy loss: -0.030085. Value loss: 0.080540. Entropy: 0.983179.\n",
      "episode: 1798   score: 16.0  epsilon: 1.0    steps: 80  evaluation reward: 16.71\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3178: Policy loss: 0.012385. Value loss: 0.209337. Entropy: 0.946758.\n",
      "Iteration 3179: Policy loss: -0.017686. Value loss: 0.092109. Entropy: 0.927479.\n",
      "Iteration 3180: Policy loss: -0.029442. Value loss: 0.063794. Entropy: 0.927784.\n",
      "episode: 1799   score: 13.0  epsilon: 1.0    steps: 232  evaluation reward: 16.74\n",
      "episode: 1800   score: 15.0  epsilon: 1.0    steps: 312  evaluation reward: 16.75\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3181: Policy loss: 0.010674. Value loss: 0.478637. Entropy: 0.933235.\n",
      "Iteration 3182: Policy loss: -0.010912. Value loss: 0.196086. Entropy: 0.930930.\n",
      "Iteration 3183: Policy loss: -0.019840. Value loss: 0.133321. Entropy: 0.935162.\n",
      "now time :  2019-03-06 13:36:04.276392\n",
      "episode: 1801   score: 18.0  epsilon: 1.0    steps: 736  evaluation reward: 16.78\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3184: Policy loss: 0.000265. Value loss: 0.274934. Entropy: 0.918916.\n",
      "Iteration 3185: Policy loss: -0.014809. Value loss: 0.103735. Entropy: 0.914379.\n",
      "Iteration 3186: Policy loss: -0.028621. Value loss: 0.067377. Entropy: 0.908669.\n",
      "episode: 1802   score: 16.0  epsilon: 1.0    steps: 608  evaluation reward: 16.85\n",
      "episode: 1803   score: 15.0  epsilon: 1.0    steps: 632  evaluation reward: 16.78\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3187: Policy loss: 0.008772. Value loss: 0.352399. Entropy: 0.940118.\n",
      "Iteration 3188: Policy loss: -0.003652. Value loss: 0.134918. Entropy: 0.919151.\n",
      "Iteration 3189: Policy loss: -0.018038. Value loss: 0.076557. Entropy: 0.912596.\n",
      "episode: 1804   score: 13.0  epsilon: 1.0    steps: 72  evaluation reward: 16.78\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3190: Policy loss: 0.010156. Value loss: 0.195166. Entropy: 0.937787.\n",
      "Iteration 3191: Policy loss: -0.016422. Value loss: 0.106963. Entropy: 0.928532.\n",
      "Iteration 3192: Policy loss: -0.028629. Value loss: 0.072790. Entropy: 0.935535.\n",
      "episode: 1805   score: 14.0  epsilon: 1.0    steps: 304  evaluation reward: 16.75\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3193: Policy loss: 0.009115. Value loss: 0.301778. Entropy: 0.886445.\n",
      "Iteration 3194: Policy loss: -0.011264. Value loss: 0.118854. Entropy: 0.882013.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3195: Policy loss: -0.024154. Value loss: 0.068238. Entropy: 0.881082.\n",
      "episode: 1806   score: 6.0  epsilon: 1.0    steps: 968  evaluation reward: 16.61\n",
      "episode: 1807   score: 18.0  epsilon: 1.0    steps: 1024  evaluation reward: 16.68\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3196: Policy loss: 0.010503. Value loss: 0.482039. Entropy: 0.939395.\n",
      "Iteration 3197: Policy loss: -0.001027. Value loss: 0.179233. Entropy: 0.925121.\n",
      "Iteration 3198: Policy loss: -0.020756. Value loss: 0.106850. Entropy: 0.926515.\n",
      "episode: 1808   score: 14.0  epsilon: 1.0    steps: 768  evaluation reward: 16.62\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3199: Policy loss: 0.011011. Value loss: 0.409375. Entropy: 0.902911.\n",
      "Iteration 3200: Policy loss: -0.002642. Value loss: 0.164576. Entropy: 0.908808.\n",
      "Iteration 3201: Policy loss: -0.024487. Value loss: 0.087513. Entropy: 0.905635.\n",
      "episode: 1809   score: 17.0  epsilon: 1.0    steps: 440  evaluation reward: 16.6\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3202: Policy loss: 0.001094. Value loss: 0.267964. Entropy: 0.885884.\n",
      "Iteration 3203: Policy loss: -0.020089. Value loss: 0.095174. Entropy: 0.876067.\n",
      "Iteration 3204: Policy loss: -0.027931. Value loss: 0.056984. Entropy: 0.882419.\n",
      "episode: 1810   score: 14.0  epsilon: 1.0    steps: 8  evaluation reward: 16.63\n",
      "episode: 1811   score: 12.0  epsilon: 1.0    steps: 232  evaluation reward: 16.6\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3205: Policy loss: 0.006574. Value loss: 0.389067. Entropy: 0.904619.\n",
      "Iteration 3206: Policy loss: -0.007471. Value loss: 0.166104. Entropy: 0.914435.\n",
      "Iteration 3207: Policy loss: -0.017533. Value loss: 0.094217. Entropy: 0.916741.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3208: Policy loss: 0.012646. Value loss: 0.387703. Entropy: 0.977315.\n",
      "Iteration 3209: Policy loss: -0.012652. Value loss: 0.131310. Entropy: 0.961902.\n",
      "Iteration 3210: Policy loss: -0.026431. Value loss: 0.063668. Entropy: 0.950230.\n",
      "episode: 1812   score: 10.0  epsilon: 1.0    steps: 96  evaluation reward: 16.57\n",
      "episode: 1813   score: 22.0  epsilon: 1.0    steps: 320  evaluation reward: 16.71\n",
      "episode: 1814   score: 18.0  epsilon: 1.0    steps: 616  evaluation reward: 16.72\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3211: Policy loss: 0.007874. Value loss: 0.618856. Entropy: 0.935242.\n",
      "Iteration 3212: Policy loss: -0.013404. Value loss: 0.328203. Entropy: 0.934952.\n",
      "Iteration 3213: Policy loss: -0.020419. Value loss: 0.223562. Entropy: 0.932405.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3214: Policy loss: 0.009065. Value loss: 0.359331. Entropy: 0.892385.\n",
      "Iteration 3215: Policy loss: -0.007470. Value loss: 0.122636. Entropy: 0.896139.\n",
      "Iteration 3216: Policy loss: -0.023713. Value loss: 0.066231. Entropy: 0.901673.\n",
      "episode: 1815   score: 19.0  epsilon: 1.0    steps: 392  evaluation reward: 16.68\n",
      "episode: 1816   score: 12.0  epsilon: 1.0    steps: 936  evaluation reward: 16.47\n",
      "episode: 1817   score: 4.0  epsilon: 1.0    steps: 1000  evaluation reward: 16.42\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3217: Policy loss: 0.010606. Value loss: 0.789578. Entropy: 0.923310.\n",
      "Iteration 3218: Policy loss: -0.003678. Value loss: 0.281273. Entropy: 0.931728.\n",
      "Iteration 3219: Policy loss: -0.020447. Value loss: 0.159203. Entropy: 0.922276.\n",
      "episode: 1818   score: 10.0  epsilon: 1.0    steps: 280  evaluation reward: 16.28\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3220: Policy loss: 0.008631. Value loss: 0.488319. Entropy: 0.893955.\n",
      "Iteration 3221: Policy loss: -0.016019. Value loss: 0.197476. Entropy: 0.882195.\n",
      "Iteration 3222: Policy loss: -0.023536. Value loss: 0.114387. Entropy: 0.890559.\n",
      "episode: 1819   score: 20.0  epsilon: 1.0    steps: 32  evaluation reward: 16.32\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3223: Policy loss: 0.000947. Value loss: 0.426580. Entropy: 0.885361.\n",
      "Iteration 3224: Policy loss: -0.016416. Value loss: 0.159310. Entropy: 0.887622.\n",
      "Iteration 3225: Policy loss: -0.027555. Value loss: 0.096987. Entropy: 0.871084.\n",
      "episode: 1820   score: 13.0  epsilon: 1.0    steps: 776  evaluation reward: 16.28\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3226: Policy loss: 0.004488. Value loss: 0.443553. Entropy: 0.927035.\n",
      "Iteration 3227: Policy loss: -0.016891. Value loss: 0.164286. Entropy: 0.934162.\n",
      "Iteration 3228: Policy loss: -0.031300. Value loss: 0.100588. Entropy: 0.929628.\n",
      "episode: 1821   score: 12.0  epsilon: 1.0    steps: 176  evaluation reward: 16.27\n",
      "episode: 1822   score: 5.0  epsilon: 1.0    steps: 232  evaluation reward: 16.19\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3229: Policy loss: 0.005526. Value loss: 0.375864. Entropy: 0.919211.\n",
      "Iteration 3230: Policy loss: -0.014125. Value loss: 0.127897. Entropy: 0.914937.\n",
      "Iteration 3231: Policy loss: -0.025626. Value loss: 0.083547. Entropy: 0.906885.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3232: Policy loss: 0.006336. Value loss: 0.233242. Entropy: 0.899165.\n",
      "Iteration 3233: Policy loss: -0.016153. Value loss: 0.071489. Entropy: 0.898213.\n",
      "Iteration 3234: Policy loss: -0.029411. Value loss: 0.041112. Entropy: 0.894251.\n",
      "episode: 1823   score: 10.0  epsilon: 1.0    steps: 120  evaluation reward: 16.08\n",
      "episode: 1824   score: 23.0  epsilon: 1.0    steps: 560  evaluation reward: 16.16\n",
      "episode: 1825   score: 11.0  epsilon: 1.0    steps: 704  evaluation reward: 16.08\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3235: Policy loss: 0.005912. Value loss: 0.430255. Entropy: 0.813487.\n",
      "Iteration 3236: Policy loss: -0.002712. Value loss: 0.123216. Entropy: 0.806930.\n",
      "Iteration 3237: Policy loss: -0.007750. Value loss: 0.061224. Entropy: 0.814014.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3238: Policy loss: 0.008115. Value loss: 0.425746. Entropy: 0.905163.\n",
      "Iteration 3239: Policy loss: -0.013133. Value loss: 0.223853. Entropy: 0.914848.\n",
      "Iteration 3240: Policy loss: -0.022322. Value loss: 0.132527. Entropy: 0.906724.\n",
      "episode: 1826   score: 22.0  epsilon: 1.0    steps: 216  evaluation reward: 16.14\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3241: Policy loss: 0.008562. Value loss: 0.321943. Entropy: 0.867591.\n",
      "Iteration 3242: Policy loss: -0.014915. Value loss: 0.118162. Entropy: 0.848392.\n",
      "Iteration 3243: Policy loss: -0.027779. Value loss: 0.059449. Entropy: 0.859503.\n",
      "episode: 1827   score: 21.0  epsilon: 1.0    steps: 848  evaluation reward: 16.24\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3244: Policy loss: 0.006923. Value loss: 0.328807. Entropy: 0.850183.\n",
      "Iteration 3245: Policy loss: -0.013959. Value loss: 0.154120. Entropy: 0.853422.\n",
      "Iteration 3246: Policy loss: -0.025533. Value loss: 0.108765. Entropy: 0.863386.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3247: Policy loss: 0.009140. Value loss: 0.272128. Entropy: 0.778639.\n",
      "Iteration 3248: Policy loss: -0.005212. Value loss: 0.085770. Entropy: 0.792105.\n",
      "Iteration 3249: Policy loss: -0.016478. Value loss: 0.043557. Entropy: 0.779190.\n",
      "episode: 1828   score: 17.0  epsilon: 1.0    steps: 560  evaluation reward: 16.3\n",
      "episode: 1829   score: 18.0  epsilon: 1.0    steps: 856  evaluation reward: 16.27\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3250: Policy loss: 0.002115. Value loss: 0.456466. Entropy: 0.866978.\n",
      "Iteration 3251: Policy loss: -0.013006. Value loss: 0.209186. Entropy: 0.864862.\n",
      "Iteration 3252: Policy loss: -0.021992. Value loss: 0.118921. Entropy: 0.867922.\n",
      "episode: 1830   score: 21.0  epsilon: 1.0    steps: 8  evaluation reward: 16.34\n",
      "episode: 1831   score: 14.0  epsilon: 1.0    steps: 680  evaluation reward: 16.23\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3253: Policy loss: 0.011610. Value loss: 0.439716. Entropy: 0.802023.\n",
      "Iteration 3254: Policy loss: -0.006589. Value loss: 0.187090. Entropy: 0.809177.\n",
      "Iteration 3255: Policy loss: -0.017693. Value loss: 0.109189. Entropy: 0.805604.\n",
      "episode: 1832   score: 20.0  epsilon: 1.0    steps: 776  evaluation reward: 16.26\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3256: Policy loss: 0.010557. Value loss: 0.238477. Entropy: 0.898509.\n",
      "Iteration 3257: Policy loss: -0.015418. Value loss: 0.085563. Entropy: 0.884539.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3258: Policy loss: -0.028048. Value loss: 0.054315. Entropy: 0.890603.\n",
      "episode: 1833   score: 20.0  epsilon: 1.0    steps: 72  evaluation reward: 16.28\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3259: Policy loss: 0.011364. Value loss: 0.264771. Entropy: 0.816774.\n",
      "Iteration 3260: Policy loss: -0.009133. Value loss: 0.115046. Entropy: 0.818856.\n",
      "Iteration 3261: Policy loss: -0.023851. Value loss: 0.079698. Entropy: 0.815225.\n",
      "episode: 1834   score: 18.0  epsilon: 1.0    steps: 80  evaluation reward: 16.2\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3262: Policy loss: 0.013898. Value loss: 0.306786. Entropy: 0.899282.\n",
      "Iteration 3263: Policy loss: -0.010358. Value loss: 0.136033. Entropy: 0.889879.\n",
      "Iteration 3264: Policy loss: -0.022720. Value loss: 0.088593. Entropy: 0.886395.\n",
      "episode: 1835   score: 12.0  epsilon: 1.0    steps: 288  evaluation reward: 16.08\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3265: Policy loss: 0.014928. Value loss: 0.501544. Entropy: 0.903412.\n",
      "Iteration 3266: Policy loss: -0.008989. Value loss: 0.183035. Entropy: 0.878636.\n",
      "Iteration 3267: Policy loss: -0.020209. Value loss: 0.106993. Entropy: 0.884235.\n",
      "episode: 1836   score: 10.0  epsilon: 1.0    steps: 424  evaluation reward: 16.0\n",
      "episode: 1837   score: 12.0  epsilon: 1.0    steps: 504  evaluation reward: 15.91\n",
      "episode: 1838   score: 15.0  epsilon: 1.0    steps: 992  evaluation reward: 15.95\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3268: Policy loss: 0.014045. Value loss: 0.447776. Entropy: 0.885463.\n",
      "Iteration 3269: Policy loss: -0.011484. Value loss: 0.124093. Entropy: 0.886522.\n",
      "Iteration 3270: Policy loss: -0.023695. Value loss: 0.066310. Entropy: 0.892294.\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3271: Policy loss: 0.005938. Value loss: 0.365422. Entropy: 0.883249.\n",
      "Iteration 3272: Policy loss: -0.012158. Value loss: 0.144042. Entropy: 0.880994.\n",
      "Iteration 3273: Policy loss: -0.027892. Value loss: 0.092754. Entropy: 0.886198.\n",
      "episode: 1839   score: 18.0  epsilon: 1.0    steps: 576  evaluation reward: 15.95\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3274: Policy loss: 0.009087. Value loss: 0.235629. Entropy: 0.857905.\n",
      "Iteration 3275: Policy loss: -0.014904. Value loss: 0.069824. Entropy: 0.865702.\n",
      "Iteration 3276: Policy loss: -0.025602. Value loss: 0.036087. Entropy: 0.867178.\n",
      "episode: 1840   score: 14.0  epsilon: 1.0    steps: 184  evaluation reward: 15.83\n",
      "episode: 1841   score: 15.0  epsilon: 1.0    steps: 560  evaluation reward: 15.81\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3277: Policy loss: 0.013738. Value loss: 0.420342. Entropy: 0.907679.\n",
      "Iteration 3278: Policy loss: -0.004779. Value loss: 0.137802. Entropy: 0.920016.\n",
      "Iteration 3279: Policy loss: -0.008878. Value loss: 0.079268. Entropy: 0.915069.\n",
      "episode: 1842   score: 14.0  epsilon: 1.0    steps: 304  evaluation reward: 15.79\n",
      "episode: 1843   score: 12.0  epsilon: 1.0    steps: 712  evaluation reward: 15.72\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3280: Policy loss: 0.005340. Value loss: 0.253662. Entropy: 0.863418.\n",
      "Iteration 3281: Policy loss: -0.010793. Value loss: 0.070512. Entropy: 0.869097.\n",
      "Iteration 3282: Policy loss: -0.023771. Value loss: 0.043607. Entropy: 0.864048.\n",
      "episode: 1844   score: 13.0  epsilon: 1.0    steps: 504  evaluation reward: 15.62\n",
      "episode: 1845   score: 9.0  epsilon: 1.0    steps: 936  evaluation reward: 15.57\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3283: Policy loss: 0.011073. Value loss: 0.410568. Entropy: 0.895511.\n",
      "Iteration 3284: Policy loss: -0.007894. Value loss: 0.159795. Entropy: 0.871549.\n",
      "Iteration 3285: Policy loss: -0.020424. Value loss: 0.085135. Entropy: 0.878194.\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3286: Policy loss: 0.005257. Value loss: 0.243735. Entropy: 0.806809.\n",
      "Iteration 3287: Policy loss: -0.021085. Value loss: 0.085297. Entropy: 0.802295.\n",
      "Iteration 3288: Policy loss: -0.027539. Value loss: 0.042506. Entropy: 0.810581.\n",
      "episode: 1846   score: 15.0  epsilon: 1.0    steps: 256  evaluation reward: 15.48\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3289: Policy loss: 0.003661. Value loss: 0.335981. Entropy: 0.892479.\n",
      "Iteration 3290: Policy loss: -0.021325. Value loss: 0.129820. Entropy: 0.888136.\n",
      "Iteration 3291: Policy loss: -0.028558. Value loss: 0.087844. Entropy: 0.883520.\n",
      "episode: 1847   score: 10.0  epsilon: 1.0    steps: 512  evaluation reward: 15.44\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3292: Policy loss: 0.010956. Value loss: 0.284787. Entropy: 0.868994.\n",
      "Iteration 3293: Policy loss: -0.013467. Value loss: 0.115611. Entropy: 0.841506.\n",
      "Iteration 3294: Policy loss: -0.026216. Value loss: 0.064850. Entropy: 0.855241.\n",
      "episode: 1848   score: 19.0  epsilon: 1.0    steps: 296  evaluation reward: 15.5\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3295: Policy loss: 0.003874. Value loss: 0.298518. Entropy: 0.895035.\n",
      "Iteration 3296: Policy loss: -0.017788. Value loss: 0.138997. Entropy: 0.900343.\n",
      "Iteration 3297: Policy loss: -0.030891. Value loss: 0.081933. Entropy: 0.886827.\n",
      "episode: 1849   score: 11.0  epsilon: 1.0    steps: 24  evaluation reward: 15.41\n",
      "episode: 1850   score: 10.0  epsilon: 1.0    steps: 872  evaluation reward: 15.35\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3298: Policy loss: 0.004681. Value loss: 0.294819. Entropy: 0.844859.\n",
      "Iteration 3299: Policy loss: -0.018644. Value loss: 0.121166. Entropy: 0.856714.\n",
      "Iteration 3300: Policy loss: -0.020405. Value loss: 0.080650. Entropy: 0.845458.\n",
      "now time :  2019-03-06 13:38:33.521441\n",
      "episode: 1851   score: 22.0  epsilon: 1.0    steps: 672  evaluation reward: 15.45\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3301: Policy loss: 0.015830. Value loss: 0.596281. Entropy: 0.838605.\n",
      "Iteration 3302: Policy loss: -0.005989. Value loss: 0.305132. Entropy: 0.847138.\n",
      "Iteration 3303: Policy loss: -0.016805. Value loss: 0.177704. Entropy: 0.849331.\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3304: Policy loss: 0.006805. Value loss: 0.292001. Entropy: 0.827372.\n",
      "Iteration 3305: Policy loss: -0.011497. Value loss: 0.107589. Entropy: 0.826425.\n",
      "Iteration 3306: Policy loss: -0.019836. Value loss: 0.057360. Entropy: 0.831135.\n",
      "episode: 1852   score: 17.0  epsilon: 1.0    steps: 696  evaluation reward: 15.47\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3307: Policy loss: 0.004587. Value loss: 0.395824. Entropy: 0.887555.\n",
      "Iteration 3308: Policy loss: -0.015939. Value loss: 0.176159. Entropy: 0.882038.\n",
      "Iteration 3309: Policy loss: -0.026619. Value loss: 0.108306. Entropy: 0.875595.\n",
      "episode: 1853   score: 25.0  epsilon: 1.0    steps: 48  evaluation reward: 15.51\n",
      "episode: 1854   score: 21.0  epsilon: 1.0    steps: 408  evaluation reward: 15.62\n",
      "episode: 1855   score: 10.0  epsilon: 1.0    steps: 536  evaluation reward: 15.6\n",
      "episode: 1856   score: 14.0  epsilon: 1.0    steps: 552  evaluation reward: 15.59\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3310: Policy loss: 0.013439. Value loss: 0.297696. Entropy: 0.872251.\n",
      "Iteration 3311: Policy loss: -0.016586. Value loss: 0.117643. Entropy: 0.874867.\n",
      "Iteration 3312: Policy loss: -0.029792. Value loss: 0.074187. Entropy: 0.871476.\n",
      "episode: 1857   score: 8.0  epsilon: 1.0    steps: 624  evaluation reward: 15.51\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3313: Policy loss: 0.012883. Value loss: 0.305702. Entropy: 0.905836.\n",
      "Iteration 3314: Policy loss: -0.013149. Value loss: 0.125098. Entropy: 0.912023.\n",
      "Iteration 3315: Policy loss: -0.029540. Value loss: 0.079767. Entropy: 0.899441.\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3316: Policy loss: 0.011996. Value loss: 0.338917. Entropy: 0.877015.\n",
      "Iteration 3317: Policy loss: -0.016750. Value loss: 0.156573. Entropy: 0.891340.\n",
      "Iteration 3318: Policy loss: -0.025892. Value loss: 0.097656. Entropy: 0.881011.\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3319: Policy loss: 0.015598. Value loss: 0.338939. Entropy: 0.893520.\n",
      "Iteration 3320: Policy loss: -0.006031. Value loss: 0.126652. Entropy: 0.889131.\n",
      "Iteration 3321: Policy loss: -0.022013. Value loss: 0.080595. Entropy: 0.882814.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1858   score: 24.0  epsilon: 1.0    steps: 152  evaluation reward: 15.54\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3322: Policy loss: 0.004273. Value loss: 0.672447. Entropy: 0.885250.\n",
      "Iteration 3323: Policy loss: -0.006428. Value loss: 0.306428. Entropy: 0.870292.\n",
      "Iteration 3324: Policy loss: -0.019787. Value loss: 0.179931. Entropy: 0.864391.\n",
      "episode: 1859   score: 7.0  epsilon: 1.0    steps: 152  evaluation reward: 15.5\n",
      "episode: 1860   score: 19.0  epsilon: 1.0    steps: 296  evaluation reward: 15.51\n",
      "episode: 1861   score: 10.0  epsilon: 1.0    steps: 568  evaluation reward: 15.47\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3325: Policy loss: 0.003395. Value loss: 0.256394. Entropy: 0.858848.\n",
      "Iteration 3326: Policy loss: -0.013185. Value loss: 0.091088. Entropy: 0.859863.\n",
      "Iteration 3327: Policy loss: -0.024380. Value loss: 0.056772. Entropy: 0.853177.\n",
      "episode: 1862   score: 16.0  epsilon: 1.0    steps: 392  evaluation reward: 15.48\n",
      "episode: 1863   score: 15.0  epsilon: 1.0    steps: 560  evaluation reward: 15.39\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3328: Policy loss: 0.012410. Value loss: 0.362147. Entropy: 0.837616.\n",
      "Iteration 3329: Policy loss: -0.012004. Value loss: 0.102766. Entropy: 0.839083.\n",
      "Iteration 3330: Policy loss: -0.025035. Value loss: 0.062376. Entropy: 0.854572.\n",
      "episode: 1864   score: 22.0  epsilon: 1.0    steps: 952  evaluation reward: 15.48\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3331: Policy loss: 0.010204. Value loss: 0.535694. Entropy: 0.904606.\n",
      "Iteration 3332: Policy loss: -0.010622. Value loss: 0.270524. Entropy: 0.901332.\n",
      "Iteration 3333: Policy loss: -0.018969. Value loss: 0.163704. Entropy: 0.897773.\n",
      "episode: 1865   score: 18.0  epsilon: 1.0    steps: 96  evaluation reward: 15.53\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3334: Policy loss: 0.005447. Value loss: 0.274793. Entropy: 0.788025.\n",
      "Iteration 3335: Policy loss: -0.010247. Value loss: 0.099696. Entropy: 0.798714.\n",
      "Iteration 3336: Policy loss: -0.016934. Value loss: 0.055568. Entropy: 0.800062.\n",
      "episode: 1866   score: 13.0  epsilon: 1.0    steps: 1016  evaluation reward: 15.48\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3337: Policy loss: 0.010764. Value loss: 0.604910. Entropy: 0.838717.\n",
      "Iteration 3338: Policy loss: -0.009278. Value loss: 0.197314. Entropy: 0.839299.\n",
      "Iteration 3339: Policy loss: -0.022952. Value loss: 0.130284. Entropy: 0.848812.\n",
      "episode: 1867   score: 10.0  epsilon: 1.0    steps: 904  evaluation reward: 15.35\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3340: Policy loss: 0.008515. Value loss: 0.347892. Entropy: 0.852890.\n",
      "Iteration 3341: Policy loss: -0.009957. Value loss: 0.130695. Entropy: 0.837245.\n",
      "Iteration 3342: Policy loss: -0.025278. Value loss: 0.090633. Entropy: 0.836530.\n",
      "episode: 1868   score: 15.0  epsilon: 1.0    steps: 864  evaluation reward: 15.37\n",
      "episode: 1869   score: 18.0  epsilon: 1.0    steps: 944  evaluation reward: 15.4\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3343: Policy loss: 0.015042. Value loss: 0.387840. Entropy: 0.809518.\n",
      "Iteration 3344: Policy loss: -0.017706. Value loss: 0.127428. Entropy: 0.814326.\n",
      "Iteration 3345: Policy loss: -0.024454. Value loss: 0.075603. Entropy: 0.810758.\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3346: Policy loss: 0.013500. Value loss: 0.435275. Entropy: 0.863328.\n",
      "Iteration 3347: Policy loss: -0.005196. Value loss: 0.228940. Entropy: 0.842873.\n",
      "Iteration 3348: Policy loss: -0.018413. Value loss: 0.124853. Entropy: 0.857095.\n",
      "episode: 1870   score: 16.0  epsilon: 1.0    steps: 240  evaluation reward: 15.41\n",
      "episode: 1871   score: 18.0  epsilon: 1.0    steps: 728  evaluation reward: 15.38\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3349: Policy loss: 0.008703. Value loss: 0.637086. Entropy: 0.832163.\n",
      "Iteration 3350: Policy loss: 0.000435. Value loss: 0.247410. Entropy: 0.821333.\n",
      "Iteration 3351: Policy loss: -0.011941. Value loss: 0.121734. Entropy: 0.815269.\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3352: Policy loss: 0.011627. Value loss: 0.422376. Entropy: 0.918853.\n",
      "Iteration 3353: Policy loss: -0.013378. Value loss: 0.185223. Entropy: 0.908661.\n",
      "Iteration 3354: Policy loss: -0.023057. Value loss: 0.121402. Entropy: 0.905806.\n",
      "episode: 1872   score: 20.0  epsilon: 1.0    steps: 32  evaluation reward: 15.45\n",
      "episode: 1873   score: 16.0  epsilon: 1.0    steps: 616  evaluation reward: 15.5\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3355: Policy loss: 0.008626. Value loss: 0.219003. Entropy: 0.799027.\n",
      "Iteration 3356: Policy loss: -0.012308. Value loss: 0.082508. Entropy: 0.796193.\n",
      "Iteration 3357: Policy loss: -0.025110. Value loss: 0.052858. Entropy: 0.806621.\n",
      "episode: 1874   score: 16.0  epsilon: 1.0    steps: 608  evaluation reward: 15.44\n",
      "episode: 1875   score: 15.0  epsilon: 1.0    steps: 960  evaluation reward: 15.41\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3358: Policy loss: 0.011137. Value loss: 0.533002. Entropy: 0.859104.\n",
      "Iteration 3359: Policy loss: -0.009488. Value loss: 0.204271. Entropy: 0.870679.\n",
      "Iteration 3360: Policy loss: -0.025573. Value loss: 0.135944. Entropy: 0.864322.\n",
      "episode: 1876   score: 12.0  epsilon: 1.0    steps: 800  evaluation reward: 15.4\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3361: Policy loss: 0.007055. Value loss: 0.321519. Entropy: 0.768151.\n",
      "Iteration 3362: Policy loss: -0.012807. Value loss: 0.133045. Entropy: 0.760747.\n",
      "Iteration 3363: Policy loss: -0.026840. Value loss: 0.081794. Entropy: 0.758508.\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3364: Policy loss: 0.005747. Value loss: 0.393131. Entropy: 0.803428.\n",
      "Iteration 3365: Policy loss: -0.015638. Value loss: 0.139586. Entropy: 0.789709.\n",
      "Iteration 3366: Policy loss: -0.025011. Value loss: 0.082281. Entropy: 0.798023.\n",
      "episode: 1877   score: 15.0  epsilon: 1.0    steps: 512  evaluation reward: 15.39\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3367: Policy loss: 0.012097. Value loss: 0.302352. Entropy: 0.816267.\n",
      "Iteration 3368: Policy loss: -0.012011. Value loss: 0.090081. Entropy: 0.820229.\n",
      "Iteration 3369: Policy loss: -0.023116. Value loss: 0.050172. Entropy: 0.814293.\n",
      "episode: 1878   score: 22.0  epsilon: 1.0    steps: 264  evaluation reward: 15.47\n",
      "episode: 1879   score: 22.0  epsilon: 1.0    steps: 1000  evaluation reward: 15.57\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3370: Policy loss: 0.013609. Value loss: 0.750825. Entropy: 0.893838.\n",
      "Iteration 3371: Policy loss: -0.001370. Value loss: 0.309542. Entropy: 0.892322.\n",
      "Iteration 3372: Policy loss: -0.012700. Value loss: 0.163002. Entropy: 0.886661.\n",
      "episode: 1880   score: 17.0  epsilon: 1.0    steps: 152  evaluation reward: 15.59\n",
      "episode: 1881   score: 10.0  epsilon: 1.0    steps: 640  evaluation reward: 15.5\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3373: Policy loss: 0.016557. Value loss: 0.292659. Entropy: 0.871557.\n",
      "Iteration 3374: Policy loss: -0.014370. Value loss: 0.123177. Entropy: 0.857069.\n",
      "Iteration 3375: Policy loss: -0.025888. Value loss: 0.073818. Entropy: 0.859635.\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3376: Policy loss: 0.007860. Value loss: 0.457619. Entropy: 0.859032.\n",
      "Iteration 3377: Policy loss: -0.009435. Value loss: 0.205031. Entropy: 0.845740.\n",
      "Iteration 3378: Policy loss: -0.017603. Value loss: 0.098335. Entropy: 0.845231.\n",
      "episode: 1882   score: 23.0  epsilon: 1.0    steps: 512  evaluation reward: 15.6\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3379: Policy loss: 0.010826. Value loss: 0.482148. Entropy: 0.838410.\n",
      "Iteration 3380: Policy loss: -0.012321. Value loss: 0.211394. Entropy: 0.852976.\n",
      "Iteration 3381: Policy loss: -0.021768. Value loss: 0.119893. Entropy: 0.839958.\n",
      "episode: 1883   score: 20.0  epsilon: 1.0    steps: 304  evaluation reward: 15.64\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3382: Policy loss: 0.010284. Value loss: 0.237565. Entropy: 0.859360.\n",
      "Iteration 3383: Policy loss: -0.020024. Value loss: 0.101530. Entropy: 0.847765.\n",
      "Iteration 3384: Policy loss: -0.028717. Value loss: 0.064310. Entropy: 0.844136.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1884   score: 22.0  epsilon: 1.0    steps: 928  evaluation reward: 15.65\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3385: Policy loss: 0.012684. Value loss: 0.580624. Entropy: 0.818608.\n",
      "Iteration 3386: Policy loss: -0.010780. Value loss: 0.286041. Entropy: 0.820897.\n",
      "Iteration 3387: Policy loss: -0.018204. Value loss: 0.177500. Entropy: 0.819639.\n",
      "episode: 1885   score: 12.0  epsilon: 1.0    steps: 80  evaluation reward: 15.5\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3388: Policy loss: 0.004565. Value loss: 0.301538. Entropy: 0.839974.\n",
      "Iteration 3389: Policy loss: -0.010758. Value loss: 0.125932. Entropy: 0.835253.\n",
      "Iteration 3390: Policy loss: -0.022759. Value loss: 0.082180. Entropy: 0.831830.\n",
      "episode: 1886   score: 18.0  epsilon: 1.0    steps: 640  evaluation reward: 15.54\n",
      "episode: 1887   score: 18.0  epsilon: 1.0    steps: 856  evaluation reward: 15.6\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3391: Policy loss: 0.012242. Value loss: 0.442207. Entropy: 0.885125.\n",
      "Iteration 3392: Policy loss: -0.008474. Value loss: 0.198407. Entropy: 0.880429.\n",
      "Iteration 3393: Policy loss: -0.023285. Value loss: 0.123997. Entropy: 0.888474.\n",
      "episode: 1888   score: 13.0  epsilon: 1.0    steps: 56  evaluation reward: 15.54\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3394: Policy loss: 0.009482. Value loss: 0.655062. Entropy: 0.803090.\n",
      "Iteration 3395: Policy loss: 0.006994. Value loss: 0.243095. Entropy: 0.805692.\n",
      "Iteration 3396: Policy loss: -0.007327. Value loss: 0.136273. Entropy: 0.795800.\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3397: Policy loss: 0.007094. Value loss: 0.383301. Entropy: 0.854286.\n",
      "Iteration 3398: Policy loss: -0.013952. Value loss: 0.116396. Entropy: 0.855586.\n",
      "Iteration 3399: Policy loss: -0.023029. Value loss: 0.067158. Entropy: 0.863888.\n",
      "episode: 1889   score: 14.0  epsilon: 1.0    steps: 168  evaluation reward: 15.44\n",
      "episode: 1890   score: 36.0  epsilon: 1.0    steps: 672  evaluation reward: 15.61\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3400: Policy loss: 0.009318. Value loss: 0.409815. Entropy: 0.809662.\n",
      "Iteration 3401: Policy loss: -0.006337. Value loss: 0.192511. Entropy: 0.818671.\n",
      "Iteration 3402: Policy loss: -0.018638. Value loss: 0.109099. Entropy: 0.811624.\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3403: Policy loss: 0.010179. Value loss: 0.258100. Entropy: 0.845810.\n",
      "Iteration 3404: Policy loss: -0.017258. Value loss: 0.109693. Entropy: 0.846764.\n",
      "Iteration 3405: Policy loss: -0.029432. Value loss: 0.069051. Entropy: 0.832362.\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3406: Policy loss: 0.019937. Value loss: 0.446432. Entropy: 0.892212.\n",
      "Iteration 3407: Policy loss: -0.011588. Value loss: 0.185458. Entropy: 0.884002.\n",
      "Iteration 3408: Policy loss: -0.020656. Value loss: 0.079432. Entropy: 0.877296.\n",
      "episode: 1891   score: 12.0  epsilon: 1.0    steps: 1016  evaluation reward: 15.57\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3409: Policy loss: 0.018820. Value loss: 0.462426. Entropy: 0.910423.\n",
      "Iteration 3410: Policy loss: -0.006633. Value loss: 0.158276. Entropy: 0.915745.\n",
      "Iteration 3411: Policy loss: -0.024072. Value loss: 0.082583. Entropy: 0.924206.\n",
      "episode: 1892   score: 29.0  epsilon: 1.0    steps: 328  evaluation reward: 15.71\n",
      "episode: 1893   score: 18.0  epsilon: 1.0    steps: 880  evaluation reward: 15.72\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3412: Policy loss: 0.009402. Value loss: 0.553058. Entropy: 0.823231.\n",
      "Iteration 3413: Policy loss: -0.002673. Value loss: 0.241033. Entropy: 0.809029.\n",
      "Iteration 3414: Policy loss: -0.016366. Value loss: 0.127041. Entropy: 0.798245.\n",
      "episode: 1894   score: 20.0  epsilon: 1.0    steps: 472  evaluation reward: 15.75\n",
      "episode: 1895   score: 16.0  epsilon: 1.0    steps: 640  evaluation reward: 15.82\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3415: Policy loss: 0.017475. Value loss: 0.631265. Entropy: 0.813611.\n",
      "Iteration 3416: Policy loss: 0.002738. Value loss: 0.289845. Entropy: 0.826653.\n",
      "Iteration 3417: Policy loss: -0.011443. Value loss: 0.183707. Entropy: 0.822406.\n",
      "episode: 1896   score: 31.0  epsilon: 1.0    steps: 24  evaluation reward: 15.88\n",
      "episode: 1897   score: 12.0  epsilon: 1.0    steps: 352  evaluation reward: 15.87\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3418: Policy loss: 0.019016. Value loss: 0.488383. Entropy: 0.841879.\n",
      "Iteration 3419: Policy loss: -0.008623. Value loss: 0.172548. Entropy: 0.845302.\n",
      "Iteration 3420: Policy loss: -0.019778. Value loss: 0.096652. Entropy: 0.830397.\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3421: Policy loss: 0.011713. Value loss: 0.476935. Entropy: 0.834439.\n",
      "Iteration 3422: Policy loss: -0.011764. Value loss: 0.181648. Entropy: 0.835840.\n",
      "Iteration 3423: Policy loss: -0.017863. Value loss: 0.101763. Entropy: 0.850021.\n",
      "episode: 1898   score: 8.0  epsilon: 1.0    steps: 480  evaluation reward: 15.79\n",
      "episode: 1899   score: 33.0  epsilon: 1.0    steps: 944  evaluation reward: 15.99\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3424: Policy loss: 0.012113. Value loss: 0.675731. Entropy: 0.847378.\n",
      "Iteration 3425: Policy loss: -0.008726. Value loss: 0.300478. Entropy: 0.853888.\n",
      "Iteration 3426: Policy loss: -0.014172. Value loss: 0.157508. Entropy: 0.857024.\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3427: Policy loss: 0.004673. Value loss: 0.452018. Entropy: 0.867471.\n",
      "Iteration 3428: Policy loss: -0.012186. Value loss: 0.166319. Entropy: 0.875779.\n",
      "Iteration 3429: Policy loss: -0.020713. Value loss: 0.109268. Entropy: 0.856878.\n",
      "episode: 1900   score: 9.0  epsilon: 1.0    steps: 184  evaluation reward: 15.93\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3430: Policy loss: 0.008173. Value loss: 0.821976. Entropy: 0.866847.\n",
      "Iteration 3431: Policy loss: -0.005996. Value loss: 0.378531. Entropy: 0.847690.\n",
      "Iteration 3432: Policy loss: -0.013797. Value loss: 0.200815. Entropy: 0.854927.\n",
      "now time :  2019-03-06 13:41:21.182017\n",
      "episode: 1901   score: 14.0  epsilon: 1.0    steps: 360  evaluation reward: 15.89\n",
      "episode: 1902   score: 12.0  epsilon: 1.0    steps: 496  evaluation reward: 15.85\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3433: Policy loss: 0.006996. Value loss: 0.491089. Entropy: 0.872245.\n",
      "Iteration 3434: Policy loss: -0.009996. Value loss: 0.221949. Entropy: 0.886052.\n",
      "Iteration 3435: Policy loss: -0.016889. Value loss: 0.145106. Entropy: 0.872474.\n",
      "episode: 1903   score: 23.0  epsilon: 1.0    steps: 728  evaluation reward: 15.93\n",
      "episode: 1904   score: 21.0  epsilon: 1.0    steps: 744  evaluation reward: 16.01\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3436: Policy loss: 0.010912. Value loss: 0.615859. Entropy: 0.861420.\n",
      "Iteration 3437: Policy loss: -0.011527. Value loss: 0.202116. Entropy: 0.861302.\n",
      "Iteration 3438: Policy loss: -0.018784. Value loss: 0.115916. Entropy: 0.867451.\n",
      "episode: 1905   score: 10.0  epsilon: 1.0    steps: 504  evaluation reward: 15.97\n",
      "episode: 1906   score: 18.0  epsilon: 1.0    steps: 792  evaluation reward: 16.09\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3439: Policy loss: 0.006663. Value loss: 0.399970. Entropy: 0.869131.\n",
      "Iteration 3440: Policy loss: -0.008099. Value loss: 0.192566. Entropy: 0.858451.\n",
      "Iteration 3441: Policy loss: -0.020094. Value loss: 0.109239. Entropy: 0.861426.\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3442: Policy loss: 0.014055. Value loss: 0.412888. Entropy: 0.867644.\n",
      "Iteration 3443: Policy loss: -0.009710. Value loss: 0.157888. Entropy: 0.878363.\n",
      "Iteration 3444: Policy loss: -0.026728. Value loss: 0.088696. Entropy: 0.868599.\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3445: Policy loss: 0.009750. Value loss: 0.537914. Entropy: 0.862203.\n",
      "Iteration 3446: Policy loss: -0.006876. Value loss: 0.231307. Entropy: 0.854618.\n",
      "Iteration 3447: Policy loss: -0.019833. Value loss: 0.121638. Entropy: 0.849055.\n",
      "episode: 1907   score: 9.0  epsilon: 1.0    steps: 216  evaluation reward: 16.0\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3448: Policy loss: 0.014870. Value loss: 0.547257. Entropy: 0.757470.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3449: Policy loss: 0.000021. Value loss: 0.232204. Entropy: 0.759768.\n",
      "Iteration 3450: Policy loss: -0.015944. Value loss: 0.147373. Entropy: 0.768426.\n",
      "episode: 1908   score: 28.0  epsilon: 1.0    steps: 184  evaluation reward: 16.14\n",
      "episode: 1909   score: 16.0  epsilon: 1.0    steps: 640  evaluation reward: 16.13\n",
      "episode: 1910   score: 12.0  epsilon: 1.0    steps: 832  evaluation reward: 16.11\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3451: Policy loss: 0.008998. Value loss: 0.335356. Entropy: 0.810232.\n",
      "Iteration 3452: Policy loss: -0.013450. Value loss: 0.144948. Entropy: 0.804763.\n",
      "Iteration 3453: Policy loss: -0.026613. Value loss: 0.084982. Entropy: 0.794584.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3454: Policy loss: 0.009853. Value loss: 0.416856. Entropy: 0.829799.\n",
      "Iteration 3455: Policy loss: -0.009236. Value loss: 0.170147. Entropy: 0.826939.\n",
      "Iteration 3456: Policy loss: -0.017917. Value loss: 0.097769. Entropy: 0.832024.\n",
      "episode: 1911   score: 18.0  epsilon: 1.0    steps: 728  evaluation reward: 16.17\n",
      "episode: 1912   score: 18.0  epsilon: 1.0    steps: 760  evaluation reward: 16.25\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3457: Policy loss: 0.010246. Value loss: 0.487179. Entropy: 0.817315.\n",
      "Iteration 3458: Policy loss: -0.011505. Value loss: 0.201998. Entropy: 0.820077.\n",
      "Iteration 3459: Policy loss: -0.022686. Value loss: 0.126942. Entropy: 0.813089.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3460: Policy loss: 0.008943. Value loss: 0.365944. Entropy: 0.808732.\n",
      "Iteration 3461: Policy loss: -0.015819. Value loss: 0.131939. Entropy: 0.794725.\n",
      "Iteration 3462: Policy loss: -0.022534. Value loss: 0.082162. Entropy: 0.795476.\n",
      "episode: 1913   score: 20.0  epsilon: 1.0    steps: 192  evaluation reward: 16.23\n",
      "episode: 1914   score: 19.0  epsilon: 1.0    steps: 920  evaluation reward: 16.24\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3463: Policy loss: 0.013169. Value loss: 0.337663. Entropy: 0.814853.\n",
      "Iteration 3464: Policy loss: -0.012382. Value loss: 0.109376. Entropy: 0.818415.\n",
      "Iteration 3465: Policy loss: -0.025183. Value loss: 0.066967. Entropy: 0.820100.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3466: Policy loss: 0.005791. Value loss: 0.280962. Entropy: 0.865820.\n",
      "Iteration 3467: Policy loss: -0.016125. Value loss: 0.127766. Entropy: 0.850666.\n",
      "Iteration 3468: Policy loss: -0.028640. Value loss: 0.073357. Entropy: 0.846544.\n",
      "episode: 1915   score: 14.0  epsilon: 1.0    steps: 160  evaluation reward: 16.19\n",
      "episode: 1916   score: 13.0  epsilon: 1.0    steps: 264  evaluation reward: 16.2\n",
      "episode: 1917   score: 15.0  epsilon: 1.0    steps: 472  evaluation reward: 16.31\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3469: Policy loss: 0.007932. Value loss: 0.482279. Entropy: 0.852511.\n",
      "Iteration 3470: Policy loss: -0.015831. Value loss: 0.275759. Entropy: 0.865693.\n",
      "Iteration 3471: Policy loss: -0.024197. Value loss: 0.190748. Entropy: 0.858372.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3472: Policy loss: 0.012295. Value loss: 0.269947. Entropy: 0.826733.\n",
      "Iteration 3473: Policy loss: -0.013045. Value loss: 0.091090. Entropy: 0.831928.\n",
      "Iteration 3474: Policy loss: -0.026836. Value loss: 0.049656. Entropy: 0.827995.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3475: Policy loss: 0.005735. Value loss: 0.352530. Entropy: 0.832937.\n",
      "Iteration 3476: Policy loss: -0.008200. Value loss: 0.108180. Entropy: 0.824020.\n",
      "Iteration 3477: Policy loss: -0.021698. Value loss: 0.066512. Entropy: 0.835497.\n",
      "episode: 1918   score: 18.0  epsilon: 1.0    steps: 160  evaluation reward: 16.39\n",
      "episode: 1919   score: 21.0  epsilon: 1.0    steps: 704  evaluation reward: 16.4\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3478: Policy loss: 0.010500. Value loss: 0.470411. Entropy: 0.798141.\n",
      "Iteration 3479: Policy loss: -0.012176. Value loss: 0.131577. Entropy: 0.802491.\n",
      "Iteration 3480: Policy loss: -0.024264. Value loss: 0.074514. Entropy: 0.801027.\n",
      "episode: 1920   score: 20.0  epsilon: 1.0    steps: 72  evaluation reward: 16.47\n",
      "episode: 1921   score: 17.0  epsilon: 1.0    steps: 280  evaluation reward: 16.52\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3481: Policy loss: 0.006570. Value loss: 0.423688. Entropy: 0.909973.\n",
      "Iteration 3482: Policy loss: -0.005209. Value loss: 0.164997. Entropy: 0.905935.\n",
      "Iteration 3483: Policy loss: -0.021066. Value loss: 0.107051. Entropy: 0.907836.\n",
      "episode: 1922   score: 14.0  epsilon: 1.0    steps: 592  evaluation reward: 16.61\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3484: Policy loss: 0.013083. Value loss: 0.532203. Entropy: 0.883699.\n",
      "Iteration 3485: Policy loss: -0.007126. Value loss: 0.185120. Entropy: 0.897057.\n",
      "Iteration 3486: Policy loss: -0.019979. Value loss: 0.092797. Entropy: 0.883482.\n",
      "episode: 1923   score: 19.0  epsilon: 1.0    steps: 224  evaluation reward: 16.7\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3487: Policy loss: 0.004819. Value loss: 0.309293. Entropy: 0.896814.\n",
      "Iteration 3488: Policy loss: -0.009015. Value loss: 0.165241. Entropy: 0.899344.\n",
      "Iteration 3489: Policy loss: -0.022218. Value loss: 0.085888. Entropy: 0.892178.\n",
      "episode: 1924   score: 21.0  epsilon: 1.0    steps: 528  evaluation reward: 16.68\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3490: Policy loss: 0.007316. Value loss: 0.259974. Entropy: 0.885709.\n",
      "Iteration 3491: Policy loss: -0.013287. Value loss: 0.089314. Entropy: 0.883596.\n",
      "Iteration 3492: Policy loss: -0.026252. Value loss: 0.053958. Entropy: 0.883685.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3493: Policy loss: 0.011276. Value loss: 0.459118. Entropy: 0.932128.\n",
      "Iteration 3494: Policy loss: -0.006890. Value loss: 0.185124. Entropy: 0.914652.\n",
      "Iteration 3495: Policy loss: -0.021845. Value loss: 0.076750. Entropy: 0.915217.\n",
      "episode: 1925   score: 18.0  epsilon: 1.0    steps: 944  evaluation reward: 16.75\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3496: Policy loss: 0.012967. Value loss: 0.613831. Entropy: 0.947779.\n",
      "Iteration 3497: Policy loss: -0.003945. Value loss: 0.215387. Entropy: 0.941543.\n",
      "Iteration 3498: Policy loss: -0.018952. Value loss: 0.117714. Entropy: 0.954498.\n",
      "episode: 1926   score: 16.0  epsilon: 1.0    steps: 112  evaluation reward: 16.69\n",
      "episode: 1927   score: 26.0  epsilon: 1.0    steps: 128  evaluation reward: 16.74\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3499: Policy loss: 0.013099. Value loss: 0.308849. Entropy: 0.852543.\n",
      "Iteration 3500: Policy loss: 0.000367. Value loss: 0.129877. Entropy: 0.860551.\n",
      "Iteration 3501: Policy loss: -0.023273. Value loss: 0.080667. Entropy: 0.839022.\n",
      "episode: 1928   score: 19.0  epsilon: 1.0    steps: 1016  evaluation reward: 16.76\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3502: Policy loss: 0.013481. Value loss: 0.468809. Entropy: 0.879787.\n",
      "Iteration 3503: Policy loss: -0.011173. Value loss: 0.218204. Entropy: 0.873296.\n",
      "Iteration 3504: Policy loss: -0.024792. Value loss: 0.116872. Entropy: 0.865434.\n",
      "episode: 1929   score: 18.0  epsilon: 1.0    steps: 384  evaluation reward: 16.76\n",
      "episode: 1930   score: 17.0  epsilon: 1.0    steps: 440  evaluation reward: 16.72\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3505: Policy loss: 0.003337. Value loss: 0.438648. Entropy: 0.907057.\n",
      "Iteration 3506: Policy loss: -0.014674. Value loss: 0.192261. Entropy: 0.907345.\n",
      "Iteration 3507: Policy loss: -0.021709. Value loss: 0.104705. Entropy: 0.890739.\n",
      "episode: 1931   score: 20.0  epsilon: 1.0    steps: 776  evaluation reward: 16.78\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3508: Policy loss: 0.012091. Value loss: 0.448164. Entropy: 0.940453.\n",
      "Iteration 3509: Policy loss: -0.004228. Value loss: 0.185402. Entropy: 0.956143.\n",
      "Iteration 3510: Policy loss: -0.017289. Value loss: 0.091736. Entropy: 0.943240.\n",
      "episode: 1932   score: 15.0  epsilon: 1.0    steps: 656  evaluation reward: 16.73\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3511: Policy loss: 0.010221. Value loss: 0.444502. Entropy: 0.938166.\n",
      "Iteration 3512: Policy loss: -0.012426. Value loss: 0.164932. Entropy: 0.923846.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3513: Policy loss: -0.022602. Value loss: 0.121665. Entropy: 0.928949.\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3514: Policy loss: 0.003150. Value loss: 0.353288. Entropy: 0.923616.\n",
      "Iteration 3515: Policy loss: -0.015113. Value loss: 0.125434. Entropy: 0.908834.\n",
      "Iteration 3516: Policy loss: -0.027544. Value loss: 0.080917. Entropy: 0.913477.\n",
      "episode: 1933   score: 7.0  epsilon: 1.0    steps: 568  evaluation reward: 16.6\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3517: Policy loss: 0.008045. Value loss: 0.398451. Entropy: 0.916299.\n",
      "Iteration 3518: Policy loss: -0.013635. Value loss: 0.142317. Entropy: 0.899250.\n",
      "Iteration 3519: Policy loss: -0.024113. Value loss: 0.103454. Entropy: 0.912974.\n",
      "episode: 1934   score: 14.0  epsilon: 1.0    steps: 200  evaluation reward: 16.56\n",
      "episode: 1935   score: 23.0  epsilon: 1.0    steps: 592  evaluation reward: 16.67\n",
      "episode: 1936   score: 11.0  epsilon: 1.0    steps: 936  evaluation reward: 16.68\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3520: Policy loss: 0.010086. Value loss: 0.592597. Entropy: 0.915922.\n",
      "Iteration 3521: Policy loss: -0.005984. Value loss: 0.237980. Entropy: 0.939864.\n",
      "Iteration 3522: Policy loss: -0.013728. Value loss: 0.137956. Entropy: 0.922952.\n",
      "episode: 1937   score: 22.0  epsilon: 1.0    steps: 64  evaluation reward: 16.78\n",
      "episode: 1938   score: 18.0  epsilon: 1.0    steps: 656  evaluation reward: 16.81\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3523: Policy loss: 0.011213. Value loss: 0.241602. Entropy: 0.879539.\n",
      "Iteration 3524: Policy loss: -0.015019. Value loss: 0.096461. Entropy: 0.865671.\n",
      "Iteration 3525: Policy loss: -0.029335. Value loss: 0.055544. Entropy: 0.859545.\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3526: Policy loss: 0.000651. Value loss: 0.544874. Entropy: 0.868870.\n",
      "Iteration 3527: Policy loss: -0.004397. Value loss: 0.252852. Entropy: 0.871858.\n",
      "Iteration 3528: Policy loss: -0.020291. Value loss: 0.154655. Entropy: 0.886955.\n",
      "episode: 1939   score: 16.0  epsilon: 1.0    steps: 560  evaluation reward: 16.79\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3529: Policy loss: 0.009025. Value loss: 0.663172. Entropy: 0.901269.\n",
      "Iteration 3530: Policy loss: -0.009062. Value loss: 0.271515. Entropy: 0.892794.\n",
      "Iteration 3531: Policy loss: -0.016347. Value loss: 0.125352. Entropy: 0.892781.\n",
      "episode: 1940   score: 23.0  epsilon: 1.0    steps: 664  evaluation reward: 16.88\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3532: Policy loss: 0.009359. Value loss: 0.314351. Entropy: 0.934454.\n",
      "Iteration 3533: Policy loss: -0.008521. Value loss: 0.114721. Entropy: 0.929653.\n",
      "Iteration 3534: Policy loss: -0.023489. Value loss: 0.070849. Entropy: 0.923002.\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3535: Policy loss: 0.007512. Value loss: 0.392956. Entropy: 0.892506.\n",
      "Iteration 3536: Policy loss: -0.012981. Value loss: 0.150292. Entropy: 0.899203.\n",
      "Iteration 3537: Policy loss: -0.028895. Value loss: 0.087301. Entropy: 0.893945.\n",
      "episode: 1941   score: 16.0  epsilon: 1.0    steps: 136  evaluation reward: 16.89\n",
      "episode: 1942   score: 12.0  epsilon: 1.0    steps: 240  evaluation reward: 16.87\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3538: Policy loss: 0.010691. Value loss: 0.750375. Entropy: 0.954230.\n",
      "Iteration 3539: Policy loss: -0.006322. Value loss: 0.346684. Entropy: 0.976648.\n",
      "Iteration 3540: Policy loss: -0.013163. Value loss: 0.210957. Entropy: 0.972755.\n",
      "episode: 1943   score: 23.0  epsilon: 1.0    steps: 960  evaluation reward: 16.98\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3541: Policy loss: 0.020029. Value loss: 0.462916. Entropy: 0.945745.\n",
      "Iteration 3542: Policy loss: -0.006577. Value loss: 0.160169. Entropy: 0.934406.\n",
      "Iteration 3543: Policy loss: -0.019137. Value loss: 0.104752. Entropy: 0.951078.\n",
      "episode: 1944   score: 16.0  epsilon: 1.0    steps: 232  evaluation reward: 17.01\n",
      "episode: 1945   score: 23.0  epsilon: 1.0    steps: 400  evaluation reward: 17.15\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3544: Policy loss: 0.013212. Value loss: 0.614663. Entropy: 0.966563.\n",
      "Iteration 3545: Policy loss: -0.007063. Value loss: 0.306697. Entropy: 0.969762.\n",
      "Iteration 3546: Policy loss: -0.023483. Value loss: 0.214046. Entropy: 0.948552.\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3547: Policy loss: 0.015361. Value loss: 0.792572. Entropy: 0.894445.\n",
      "Iteration 3548: Policy loss: 0.000137. Value loss: 0.351289. Entropy: 0.887741.\n",
      "Iteration 3549: Policy loss: -0.014150. Value loss: 0.216992. Entropy: 0.886750.\n",
      "episode: 1946   score: 29.0  epsilon: 1.0    steps: 472  evaluation reward: 17.29\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3550: Policy loss: 0.004932. Value loss: 0.525657. Entropy: 0.932007.\n",
      "Iteration 3551: Policy loss: -0.011937. Value loss: 0.236325. Entropy: 0.925600.\n",
      "Iteration 3552: Policy loss: -0.019178. Value loss: 0.148403. Entropy: 0.929470.\n",
      "episode: 1947   score: 17.0  epsilon: 1.0    steps: 232  evaluation reward: 17.36\n",
      "episode: 1948   score: 28.0  epsilon: 1.0    steps: 904  evaluation reward: 17.45\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3553: Policy loss: 0.008872. Value loss: 0.591115. Entropy: 0.892633.\n",
      "Iteration 3554: Policy loss: -0.009976. Value loss: 0.336656. Entropy: 0.861682.\n",
      "Iteration 3555: Policy loss: -0.017267. Value loss: 0.207751. Entropy: 0.875067.\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3556: Policy loss: 0.012995. Value loss: 0.318977. Entropy: 0.913378.\n",
      "Iteration 3557: Policy loss: -0.009316. Value loss: 0.133458. Entropy: 0.903224.\n",
      "Iteration 3558: Policy loss: -0.024747. Value loss: 0.092403. Entropy: 0.894780.\n",
      "episode: 1949   score: 14.0  epsilon: 1.0    steps: 656  evaluation reward: 17.48\n",
      "episode: 1950   score: 17.0  epsilon: 1.0    steps: 760  evaluation reward: 17.55\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3559: Policy loss: 0.014699. Value loss: 0.752679. Entropy: 0.858290.\n",
      "Iteration 3560: Policy loss: 0.002029. Value loss: 0.238568. Entropy: 0.850780.\n",
      "Iteration 3561: Policy loss: -0.013062. Value loss: 0.129880. Entropy: 0.862600.\n",
      "now time :  2019-03-06 13:44:06.555378\n",
      "episode: 1951   score: 33.0  epsilon: 1.0    steps: 544  evaluation reward: 17.66\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3562: Policy loss: 0.007586. Value loss: 0.445679. Entropy: 0.910580.\n",
      "Iteration 3563: Policy loss: -0.012316. Value loss: 0.190911. Entropy: 0.900732.\n",
      "Iteration 3564: Policy loss: -0.023536. Value loss: 0.109078. Entropy: 0.902098.\n",
      "episode: 1952   score: 18.0  epsilon: 1.0    steps: 72  evaluation reward: 17.67\n",
      "episode: 1953   score: 20.0  epsilon: 1.0    steps: 856  evaluation reward: 17.62\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3565: Policy loss: 0.006427. Value loss: 0.308935. Entropy: 0.887400.\n",
      "Iteration 3566: Policy loss: -0.010333. Value loss: 0.133271. Entropy: 0.898067.\n",
      "Iteration 3567: Policy loss: -0.024619. Value loss: 0.070467. Entropy: 0.889296.\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3568: Policy loss: 0.008114. Value loss: 0.407466. Entropy: 0.903242.\n",
      "Iteration 3569: Policy loss: -0.008914. Value loss: 0.159586. Entropy: 0.890873.\n",
      "Iteration 3570: Policy loss: -0.023969. Value loss: 0.087319. Entropy: 0.881273.\n",
      "episode: 1954   score: 16.0  epsilon: 1.0    steps: 312  evaluation reward: 17.57\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3571: Policy loss: 0.006684. Value loss: 0.558215. Entropy: 0.816523.\n",
      "Iteration 3572: Policy loss: -0.010915. Value loss: 0.228960. Entropy: 0.811191.\n",
      "Iteration 3573: Policy loss: -0.022031. Value loss: 0.136495. Entropy: 0.808506.\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3574: Policy loss: 0.010691. Value loss: 0.397894. Entropy: 0.827835.\n",
      "Iteration 3575: Policy loss: -0.001644. Value loss: 0.197379. Entropy: 0.800821.\n",
      "Iteration 3576: Policy loss: -0.008079. Value loss: 0.114458. Entropy: 0.825142.\n",
      "episode: 1955   score: 23.0  epsilon: 1.0    steps: 176  evaluation reward: 17.7\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3577: Policy loss: 0.013814. Value loss: 0.525502. Entropy: 0.883590.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3578: Policy loss: -0.005793. Value loss: 0.216353. Entropy: 0.869292.\n",
      "Iteration 3579: Policy loss: -0.018776. Value loss: 0.126944. Entropy: 0.868219.\n",
      "episode: 1956   score: 23.0  epsilon: 1.0    steps: 264  evaluation reward: 17.79\n",
      "episode: 1957   score: 11.0  epsilon: 1.0    steps: 584  evaluation reward: 17.82\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3580: Policy loss: 0.011581. Value loss: 0.604469. Entropy: 0.854712.\n",
      "Iteration 3581: Policy loss: -0.008425. Value loss: 0.238278. Entropy: 0.848099.\n",
      "Iteration 3582: Policy loss: -0.018759. Value loss: 0.123264. Entropy: 0.841752.\n",
      "episode: 1958   score: 24.0  epsilon: 1.0    steps: 896  evaluation reward: 17.82\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3583: Policy loss: 0.008650. Value loss: 0.614708. Entropy: 0.847016.\n",
      "Iteration 3584: Policy loss: -0.008148. Value loss: 0.195859. Entropy: 0.833183.\n",
      "Iteration 3585: Policy loss: -0.015981. Value loss: 0.112544. Entropy: 0.852746.\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3586: Policy loss: 0.010946. Value loss: 0.588190. Entropy: 0.836314.\n",
      "Iteration 3587: Policy loss: -0.013598. Value loss: 0.227461. Entropy: 0.848286.\n",
      "Iteration 3588: Policy loss: -0.023656. Value loss: 0.142892. Entropy: 0.844341.\n",
      "episode: 1959   score: 28.0  epsilon: 1.0    steps: 1024  evaluation reward: 18.03\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3589: Policy loss: 0.009416. Value loss: 1.201317. Entropy: 0.904253.\n",
      "Iteration 3590: Policy loss: 0.000620. Value loss: 0.500027. Entropy: 0.890141.\n",
      "Iteration 3591: Policy loss: -0.012673. Value loss: 0.280710. Entropy: 0.904752.\n",
      "episode: 1960   score: 39.0  epsilon: 1.0    steps: 16  evaluation reward: 18.23\n",
      "episode: 1961   score: 19.0  epsilon: 1.0    steps: 296  evaluation reward: 18.32\n",
      "episode: 1962   score: 7.0  epsilon: 1.0    steps: 488  evaluation reward: 18.23\n",
      "episode: 1963   score: 25.0  epsilon: 1.0    steps: 792  evaluation reward: 18.33\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3592: Policy loss: 0.013118. Value loss: 0.548185. Entropy: 0.899146.\n",
      "Iteration 3593: Policy loss: -0.005783. Value loss: 0.209853. Entropy: 0.893034.\n",
      "Iteration 3594: Policy loss: -0.018961. Value loss: 0.123752. Entropy: 0.879187.\n",
      "episode: 1964   score: 18.0  epsilon: 1.0    steps: 840  evaluation reward: 18.29\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3595: Policy loss: 0.021835. Value loss: 0.426693. Entropy: 0.868240.\n",
      "Iteration 3596: Policy loss: -0.003210. Value loss: 0.205217. Entropy: 0.857831.\n",
      "Iteration 3597: Policy loss: -0.021254. Value loss: 0.127265. Entropy: 0.861835.\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3598: Policy loss: 0.009503. Value loss: 0.492927. Entropy: 0.847679.\n",
      "Iteration 3599: Policy loss: -0.012269. Value loss: 0.187433. Entropy: 0.842122.\n",
      "Iteration 3600: Policy loss: -0.016952. Value loss: 0.110855. Entropy: 0.850484.\n",
      "episode: 1965   score: 18.0  epsilon: 1.0    steps: 520  evaluation reward: 18.29\n",
      "episode: 1966   score: 20.0  epsilon: 1.0    steps: 832  evaluation reward: 18.36\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3601: Policy loss: 0.006584. Value loss: 0.307378. Entropy: 0.822054.\n",
      "Iteration 3602: Policy loss: -0.013068. Value loss: 0.112546. Entropy: 0.837458.\n",
      "Iteration 3603: Policy loss: -0.024712. Value loss: 0.077189. Entropy: 0.832436.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3604: Policy loss: 0.011547. Value loss: 0.574759. Entropy: 0.809049.\n",
      "Iteration 3605: Policy loss: -0.004098. Value loss: 0.334125. Entropy: 0.800539.\n",
      "Iteration 3606: Policy loss: -0.016680. Value loss: 0.183252. Entropy: 0.799324.\n",
      "episode: 1967   score: 10.0  epsilon: 1.0    steps: 248  evaluation reward: 18.36\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3607: Policy loss: 0.012777. Value loss: 0.402095. Entropy: 0.888649.\n",
      "Iteration 3608: Policy loss: -0.007997. Value loss: 0.195537. Entropy: 0.905726.\n",
      "Iteration 3609: Policy loss: -0.025661. Value loss: 0.120295. Entropy: 0.880165.\n",
      "episode: 1968   score: 14.0  epsilon: 1.0    steps: 752  evaluation reward: 18.35\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3610: Policy loss: 0.013649. Value loss: 0.588081. Entropy: 0.882328.\n",
      "Iteration 3611: Policy loss: -0.002689. Value loss: 0.217225. Entropy: 0.909045.\n",
      "Iteration 3612: Policy loss: -0.020937. Value loss: 0.119935. Entropy: 0.887091.\n",
      "episode: 1969   score: 17.0  epsilon: 1.0    steps: 64  evaluation reward: 18.34\n",
      "episode: 1970   score: 20.0  epsilon: 1.0    steps: 416  evaluation reward: 18.38\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3613: Policy loss: 0.012551. Value loss: 0.194706. Entropy: 0.914754.\n",
      "Iteration 3614: Policy loss: -0.012702. Value loss: 0.068437. Entropy: 0.907438.\n",
      "Iteration 3615: Policy loss: -0.023840. Value loss: 0.042526. Entropy: 0.916737.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3616: Policy loss: 0.006734. Value loss: 0.433275. Entropy: 0.896854.\n",
      "Iteration 3617: Policy loss: -0.000768. Value loss: 0.207052. Entropy: 0.883203.\n",
      "Iteration 3618: Policy loss: -0.019215. Value loss: 0.135195. Entropy: 0.882684.\n",
      "episode: 1971   score: 25.0  epsilon: 1.0    steps: 680  evaluation reward: 18.45\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3619: Policy loss: 0.019223. Value loss: 0.381341. Entropy: 0.923989.\n",
      "Iteration 3620: Policy loss: -0.006346. Value loss: 0.147568. Entropy: 0.949117.\n",
      "Iteration 3621: Policy loss: -0.022008. Value loss: 0.101243. Entropy: 0.931542.\n",
      "episode: 1972   score: 25.0  epsilon: 1.0    steps: 352  evaluation reward: 18.5\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3622: Policy loss: 0.015096. Value loss: 0.536166. Entropy: 0.848404.\n",
      "Iteration 3623: Policy loss: -0.007593. Value loss: 0.239142. Entropy: 0.839422.\n",
      "Iteration 3624: Policy loss: -0.022832. Value loss: 0.123761. Entropy: 0.840035.\n",
      "episode: 1973   score: 16.0  epsilon: 1.0    steps: 776  evaluation reward: 18.5\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3625: Policy loss: 0.009960. Value loss: 0.332456. Entropy: 0.906698.\n",
      "Iteration 3626: Policy loss: -0.012497. Value loss: 0.107233. Entropy: 0.904025.\n",
      "Iteration 3627: Policy loss: -0.025077. Value loss: 0.062091. Entropy: 0.895742.\n",
      "episode: 1974   score: 19.0  epsilon: 1.0    steps: 144  evaluation reward: 18.53\n",
      "episode: 1975   score: 25.0  epsilon: 1.0    steps: 464  evaluation reward: 18.63\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3628: Policy loss: 0.007763. Value loss: 0.618730. Entropy: 0.856096.\n",
      "Iteration 3629: Policy loss: -0.007669. Value loss: 0.217965. Entropy: 0.829278.\n",
      "Iteration 3630: Policy loss: -0.013310. Value loss: 0.099372. Entropy: 0.835473.\n",
      "episode: 1976   score: 16.0  epsilon: 1.0    steps: 128  evaluation reward: 18.67\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3631: Policy loss: 0.004857. Value loss: 0.208559. Entropy: 0.839823.\n",
      "Iteration 3632: Policy loss: -0.015766. Value loss: 0.069597. Entropy: 0.833097.\n",
      "Iteration 3633: Policy loss: -0.025898. Value loss: 0.033213. Entropy: 0.850194.\n",
      "episode: 1977   score: 14.0  epsilon: 1.0    steps: 296  evaluation reward: 18.66\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3634: Policy loss: 0.007041. Value loss: 0.574582. Entropy: 0.860908.\n",
      "Iteration 3635: Policy loss: 0.005711. Value loss: 0.230778. Entropy: 0.863403.\n",
      "Iteration 3636: Policy loss: -0.008868. Value loss: 0.119890. Entropy: 0.838117.\n",
      "episode: 1978   score: 22.0  epsilon: 1.0    steps: 664  evaluation reward: 18.66\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3637: Policy loss: 0.007779. Value loss: 0.637992. Entropy: 0.818941.\n",
      "Iteration 3638: Policy loss: -0.008043. Value loss: 0.275899. Entropy: 0.812173.\n",
      "Iteration 3639: Policy loss: -0.021985. Value loss: 0.174470. Entropy: 0.807811.\n",
      "episode: 1979   score: 12.0  epsilon: 1.0    steps: 128  evaluation reward: 18.56\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3640: Policy loss: 0.004251. Value loss: 0.482424. Entropy: 0.905723.\n",
      "Iteration 3641: Policy loss: -0.001412. Value loss: 0.171691. Entropy: 0.900561.\n",
      "Iteration 3642: Policy loss: -0.020529. Value loss: 0.100235. Entropy: 0.894526.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1980   score: 33.0  epsilon: 1.0    steps: 672  evaluation reward: 18.72\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3643: Policy loss: 0.013788. Value loss: 0.637524. Entropy: 0.797390.\n",
      "Iteration 3644: Policy loss: -0.000328. Value loss: 0.253536. Entropy: 0.780453.\n",
      "Iteration 3645: Policy loss: -0.018046. Value loss: 0.132073. Entropy: 0.779478.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3646: Policy loss: 0.010101. Value loss: 0.502147. Entropy: 0.829994.\n",
      "Iteration 3647: Policy loss: -0.010323. Value loss: 0.203587. Entropy: 0.835126.\n",
      "Iteration 3648: Policy loss: -0.020494. Value loss: 0.116537. Entropy: 0.835797.\n",
      "episode: 1981   score: 22.0  epsilon: 1.0    steps: 896  evaluation reward: 18.84\n",
      "episode: 1982   score: 15.0  epsilon: 1.0    steps: 1000  evaluation reward: 18.76\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3649: Policy loss: 0.006406. Value loss: 0.843912. Entropy: 0.796881.\n",
      "Iteration 3650: Policy loss: 0.002437. Value loss: 0.301149. Entropy: 0.767379.\n",
      "Iteration 3651: Policy loss: -0.016647. Value loss: 0.155005. Entropy: 0.771611.\n",
      "episode: 1983   score: 15.0  epsilon: 1.0    steps: 456  evaluation reward: 18.71\n",
      "episode: 1984   score: 21.0  epsilon: 1.0    steps: 472  evaluation reward: 18.7\n",
      "episode: 1985   score: 23.0  epsilon: 1.0    steps: 672  evaluation reward: 18.81\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3652: Policy loss: 0.011797. Value loss: 0.511602. Entropy: 0.772874.\n",
      "Iteration 3653: Policy loss: -0.002832. Value loss: 0.221790. Entropy: 0.769181.\n",
      "Iteration 3654: Policy loss: -0.016998. Value loss: 0.134839. Entropy: 0.757842.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3655: Policy loss: 0.003347. Value loss: 0.624565. Entropy: 0.805642.\n",
      "Iteration 3656: Policy loss: -0.011955. Value loss: 0.299169. Entropy: 0.813284.\n",
      "Iteration 3657: Policy loss: -0.016632. Value loss: 0.174312. Entropy: 0.805443.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3658: Policy loss: 0.010586. Value loss: 0.335297. Entropy: 0.794893.\n",
      "Iteration 3659: Policy loss: -0.007471. Value loss: 0.121277. Entropy: 0.782424.\n",
      "Iteration 3660: Policy loss: -0.019233. Value loss: 0.070232. Entropy: 0.783374.\n",
      "episode: 1986   score: 20.0  epsilon: 1.0    steps: 488  evaluation reward: 18.83\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3661: Policy loss: 0.012566. Value loss: 0.495292. Entropy: 0.853118.\n",
      "Iteration 3662: Policy loss: -0.012456. Value loss: 0.231672. Entropy: 0.843432.\n",
      "Iteration 3663: Policy loss: -0.023203. Value loss: 0.154886. Entropy: 0.836640.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3664: Policy loss: 0.004684. Value loss: 0.370716. Entropy: 0.839472.\n",
      "Iteration 3665: Policy loss: -0.013560. Value loss: 0.135828. Entropy: 0.830945.\n",
      "Iteration 3666: Policy loss: -0.021746. Value loss: 0.064445. Entropy: 0.841372.\n",
      "episode: 1987   score: 31.0  epsilon: 1.0    steps: 568  evaluation reward: 18.96\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3667: Policy loss: 0.011820. Value loss: 0.353401. Entropy: 0.805776.\n",
      "Iteration 3668: Policy loss: -0.003792. Value loss: 0.133371. Entropy: 0.808848.\n",
      "Iteration 3669: Policy loss: -0.006087. Value loss: 0.068217. Entropy: 0.813481.\n",
      "episode: 1988   score: 22.0  epsilon: 1.0    steps: 112  evaluation reward: 19.05\n",
      "episode: 1989   score: 11.0  epsilon: 1.0    steps: 520  evaluation reward: 19.02\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3670: Policy loss: 0.004246. Value loss: 0.479062. Entropy: 0.898081.\n",
      "Iteration 3671: Policy loss: -0.006669. Value loss: 0.196825. Entropy: 0.900270.\n",
      "Iteration 3672: Policy loss: -0.017258. Value loss: 0.120063. Entropy: 0.898335.\n",
      "episode: 1990   score: 16.0  epsilon: 1.0    steps: 120  evaluation reward: 18.82\n",
      "episode: 1991   score: 17.0  epsilon: 1.0    steps: 192  evaluation reward: 18.87\n",
      "episode: 1992   score: 16.0  epsilon: 1.0    steps: 344  evaluation reward: 18.74\n",
      "episode: 1993   score: 19.0  epsilon: 1.0    steps: 736  evaluation reward: 18.75\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3673: Policy loss: 0.008481. Value loss: 0.354215. Entropy: 0.853962.\n",
      "Iteration 3674: Policy loss: -0.003900. Value loss: 0.127998. Entropy: 0.858787.\n",
      "Iteration 3675: Policy loss: -0.017376. Value loss: 0.079459. Entropy: 0.869990.\n",
      "episode: 1994   score: 10.0  epsilon: 1.0    steps: 840  evaluation reward: 18.65\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3676: Policy loss: 0.006267. Value loss: 0.409106. Entropy: 0.814737.\n",
      "Iteration 3677: Policy loss: -0.012341. Value loss: 0.216530. Entropy: 0.802721.\n",
      "Iteration 3678: Policy loss: -0.020509. Value loss: 0.141593. Entropy: 0.785673.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3679: Policy loss: 0.010929. Value loss: 0.240593. Entropy: 0.779076.\n",
      "Iteration 3680: Policy loss: -0.006667. Value loss: 0.096341. Entropy: 0.781872.\n",
      "Iteration 3681: Policy loss: -0.020311. Value loss: 0.061861. Entropy: 0.790419.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3682: Policy loss: 0.004251. Value loss: 0.440049. Entropy: 0.810233.\n",
      "Iteration 3683: Policy loss: -0.010313. Value loss: 0.225499. Entropy: 0.821269.\n",
      "Iteration 3684: Policy loss: -0.019027. Value loss: 0.142901. Entropy: 0.838307.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3685: Policy loss: 0.009759. Value loss: 0.392877. Entropy: 0.844718.\n",
      "Iteration 3686: Policy loss: -0.003153. Value loss: 0.147353. Entropy: 0.834932.\n",
      "Iteration 3687: Policy loss: -0.018915. Value loss: 0.084682. Entropy: 0.827322.\n",
      "episode: 1995   score: 12.0  epsilon: 1.0    steps: 888  evaluation reward: 18.61\n",
      "episode: 1996   score: 14.0  epsilon: 1.0    steps: 904  evaluation reward: 18.44\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3688: Policy loss: 0.003875. Value loss: 0.481777. Entropy: 0.885160.\n",
      "Iteration 3689: Policy loss: -0.014652. Value loss: 0.201713. Entropy: 0.882202.\n",
      "Iteration 3690: Policy loss: -0.024682. Value loss: 0.112912. Entropy: 0.871815.\n",
      "episode: 1997   score: 13.0  epsilon: 1.0    steps: 280  evaluation reward: 18.45\n",
      "episode: 1998   score: 19.0  epsilon: 1.0    steps: 560  evaluation reward: 18.56\n",
      "episode: 1999   score: 15.0  epsilon: 1.0    steps: 720  evaluation reward: 18.38\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3691: Policy loss: 0.013264. Value loss: 0.844385. Entropy: 0.871868.\n",
      "Iteration 3692: Policy loss: -0.006597. Value loss: 0.310556. Entropy: 0.878289.\n",
      "Iteration 3693: Policy loss: -0.016531. Value loss: 0.149046. Entropy: 0.874709.\n",
      "episode: 2000   score: 24.0  epsilon: 1.0    steps: 32  evaluation reward: 18.53\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3694: Policy loss: 0.008420. Value loss: 0.254852. Entropy: 0.876491.\n",
      "Iteration 3695: Policy loss: -0.013108. Value loss: 0.104391. Entropy: 0.887368.\n",
      "Iteration 3696: Policy loss: -0.025445. Value loss: 0.064179. Entropy: 0.879491.\n",
      "now time :  2019-03-06 13:46:58.338893\n",
      "episode: 2001   score: 20.0  epsilon: 1.0    steps: 144  evaluation reward: 18.59\n",
      "episode: 2002   score: 20.0  epsilon: 1.0    steps: 808  evaluation reward: 18.67\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3697: Policy loss: 0.009840. Value loss: 0.535925. Entropy: 0.916258.\n",
      "Iteration 3698: Policy loss: -0.004516. Value loss: 0.202595. Entropy: 0.903216.\n",
      "Iteration 3699: Policy loss: -0.018536. Value loss: 0.103009. Entropy: 0.887634.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3700: Policy loss: 0.012769. Value loss: 0.313464. Entropy: 0.831821.\n",
      "Iteration 3701: Policy loss: -0.005395. Value loss: 0.164790. Entropy: 0.820754.\n",
      "Iteration 3702: Policy loss: -0.021162. Value loss: 0.103474. Entropy: 0.815719.\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3703: Policy loss: 0.008271. Value loss: 0.660541. Entropy: 0.893982.\n",
      "Iteration 3704: Policy loss: -0.006763. Value loss: 0.345257. Entropy: 0.906881.\n",
      "Iteration 3705: Policy loss: -0.018193. Value loss: 0.183738. Entropy: 0.892182.\n",
      "episode: 2003   score: 14.0  epsilon: 1.0    steps: 912  evaluation reward: 18.58\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3706: Policy loss: 0.006564. Value loss: 0.444187. Entropy: 0.839822.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3707: Policy loss: -0.010944. Value loss: 0.173525. Entropy: 0.837394.\n",
      "Iteration 3708: Policy loss: -0.016686. Value loss: 0.096341. Entropy: 0.842201.\n",
      "episode: 2004   score: 13.0  epsilon: 1.0    steps: 720  evaluation reward: 18.5\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3709: Policy loss: 0.010488. Value loss: 0.598637. Entropy: 0.850326.\n",
      "Iteration 3710: Policy loss: -0.007601. Value loss: 0.257452. Entropy: 0.847586.\n",
      "Iteration 3711: Policy loss: -0.019954. Value loss: 0.143889. Entropy: 0.846201.\n",
      "episode: 2005   score: 15.0  epsilon: 1.0    steps: 152  evaluation reward: 18.55\n",
      "episode: 2006   score: 20.0  epsilon: 1.0    steps: 440  evaluation reward: 18.57\n",
      "episode: 2007   score: 18.0  epsilon: 1.0    steps: 664  evaluation reward: 18.66\n",
      "episode: 2008   score: 11.0  epsilon: 1.0    steps: 936  evaluation reward: 18.49\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3712: Policy loss: 0.004013. Value loss: 0.613218. Entropy: 0.891493.\n",
      "Iteration 3713: Policy loss: -0.007105. Value loss: 0.277340. Entropy: 0.901439.\n",
      "Iteration 3714: Policy loss: -0.022972. Value loss: 0.174524. Entropy: 0.900632.\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3715: Policy loss: 0.014996. Value loss: 0.468980. Entropy: 0.907622.\n",
      "Iteration 3716: Policy loss: -0.010148. Value loss: 0.194953. Entropy: 0.889070.\n",
      "Iteration 3717: Policy loss: -0.021412. Value loss: 0.117304. Entropy: 0.891830.\n",
      "episode: 2009   score: 25.0  epsilon: 1.0    steps: 64  evaluation reward: 18.58\n",
      "episode: 2010   score: 19.0  epsilon: 1.0    steps: 640  evaluation reward: 18.65\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3718: Policy loss: 0.007615. Value loss: 0.304533. Entropy: 0.830682.\n",
      "Iteration 3719: Policy loss: -0.014718. Value loss: 0.116700. Entropy: 0.867088.\n",
      "Iteration 3720: Policy loss: -0.022401. Value loss: 0.066194. Entropy: 0.854766.\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3721: Policy loss: 0.006858. Value loss: 0.374994. Entropy: 0.837896.\n",
      "Iteration 3722: Policy loss: -0.006421. Value loss: 0.134939. Entropy: 0.855269.\n",
      "Iteration 3723: Policy loss: -0.018777. Value loss: 0.061044. Entropy: 0.853363.\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3724: Policy loss: 0.012876. Value loss: 0.363757. Entropy: 0.956615.\n",
      "Iteration 3725: Policy loss: -0.012238. Value loss: 0.149867. Entropy: 0.949821.\n",
      "Iteration 3726: Policy loss: -0.024842. Value loss: 0.105593. Entropy: 0.943974.\n",
      "episode: 2011   score: 10.0  epsilon: 1.0    steps: 240  evaluation reward: 18.57\n",
      "episode: 2012   score: 14.0  epsilon: 1.0    steps: 272  evaluation reward: 18.53\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3727: Policy loss: 0.011200. Value loss: 0.437025. Entropy: 0.955387.\n",
      "Iteration 3728: Policy loss: -0.011210. Value loss: 0.173432. Entropy: 0.942824.\n",
      "Iteration 3729: Policy loss: -0.023689. Value loss: 0.075092. Entropy: 0.948444.\n",
      "episode: 2013   score: 10.0  epsilon: 1.0    steps: 8  evaluation reward: 18.43\n",
      "episode: 2014   score: 18.0  epsilon: 1.0    steps: 408  evaluation reward: 18.42\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3730: Policy loss: 0.014001. Value loss: 0.402543. Entropy: 0.900653.\n",
      "Iteration 3731: Policy loss: -0.006630. Value loss: 0.147099. Entropy: 0.906553.\n",
      "Iteration 3732: Policy loss: -0.019437. Value loss: 0.075083. Entropy: 0.911702.\n",
      "episode: 2015   score: 15.0  epsilon: 1.0    steps: 872  evaluation reward: 18.43\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3733: Policy loss: 0.012795. Value loss: 0.640182. Entropy: 0.935443.\n",
      "Iteration 3734: Policy loss: -0.005301. Value loss: 0.233816. Entropy: 0.945480.\n",
      "Iteration 3735: Policy loss: -0.017648. Value loss: 0.104376. Entropy: 0.958100.\n",
      "episode: 2016   score: 18.0  epsilon: 1.0    steps: 144  evaluation reward: 18.48\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3736: Policy loss: 0.011901. Value loss: 0.289327. Entropy: 0.937637.\n",
      "Iteration 3737: Policy loss: -0.007798. Value loss: 0.114402. Entropy: 0.946209.\n",
      "Iteration 3738: Policy loss: -0.022100. Value loss: 0.073487. Entropy: 0.940284.\n",
      "episode: 2017   score: 23.0  epsilon: 1.0    steps: 248  evaluation reward: 18.56\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3739: Policy loss: 0.006559. Value loss: 0.435311. Entropy: 0.977381.\n",
      "Iteration 3740: Policy loss: -0.012460. Value loss: 0.147876. Entropy: 0.971013.\n",
      "Iteration 3741: Policy loss: -0.022631. Value loss: 0.085933. Entropy: 0.967362.\n",
      "episode: 2018   score: 15.0  epsilon: 1.0    steps: 136  evaluation reward: 18.53\n",
      "episode: 2019   score: 11.0  epsilon: 1.0    steps: 824  evaluation reward: 18.43\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3742: Policy loss: 0.010497. Value loss: 0.416065. Entropy: 0.963519.\n",
      "Iteration 3743: Policy loss: -0.010690. Value loss: 0.141089. Entropy: 0.973289.\n",
      "Iteration 3744: Policy loss: -0.021417. Value loss: 0.092183. Entropy: 0.961543.\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3745: Policy loss: 0.005387. Value loss: 0.377142. Entropy: 0.940169.\n",
      "Iteration 3746: Policy loss: -0.008285. Value loss: 0.155610. Entropy: 0.923164.\n",
      "Iteration 3747: Policy loss: -0.021993. Value loss: 0.091682. Entropy: 0.936057.\n",
      "episode: 2020   score: 12.0  epsilon: 1.0    steps: 664  evaluation reward: 18.35\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3748: Policy loss: 0.008532. Value loss: 0.438011. Entropy: 0.879890.\n",
      "Iteration 3749: Policy loss: -0.013481. Value loss: 0.177313. Entropy: 0.866131.\n",
      "Iteration 3750: Policy loss: -0.019393. Value loss: 0.097500. Entropy: 0.863826.\n",
      "episode: 2021   score: 8.0  epsilon: 1.0    steps: 584  evaluation reward: 18.26\n",
      "episode: 2022   score: 25.0  epsilon: 1.0    steps: 768  evaluation reward: 18.37\n",
      "episode: 2023   score: 16.0  epsilon: 1.0    steps: 944  evaluation reward: 18.34\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3751: Policy loss: 0.004562. Value loss: 0.503733. Entropy: 0.956852.\n",
      "Iteration 3752: Policy loss: -0.013904. Value loss: 0.164143. Entropy: 0.938908.\n",
      "Iteration 3753: Policy loss: -0.029437. Value loss: 0.088887. Entropy: 0.938421.\n",
      "episode: 2024   score: 17.0  epsilon: 1.0    steps: 704  evaluation reward: 18.3\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3754: Policy loss: 0.010888. Value loss: 0.296604. Entropy: 0.882540.\n",
      "Iteration 3755: Policy loss: -0.013705. Value loss: 0.110800. Entropy: 0.881080.\n",
      "Iteration 3756: Policy loss: -0.024695. Value loss: 0.062616. Entropy: 0.876055.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3757: Policy loss: 0.009482. Value loss: 0.692084. Entropy: 0.929750.\n",
      "Iteration 3758: Policy loss: -0.005468. Value loss: 0.299451. Entropy: 0.897276.\n",
      "Iteration 3759: Policy loss: -0.017300. Value loss: 0.148103. Entropy: 0.906107.\n",
      "episode: 2025   score: 23.0  epsilon: 1.0    steps: 544  evaluation reward: 18.35\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3760: Policy loss: 0.013979. Value loss: 0.764061. Entropy: 0.842611.\n",
      "Iteration 3761: Policy loss: -0.002246. Value loss: 0.337622. Entropy: 0.848757.\n",
      "Iteration 3762: Policy loss: -0.015310. Value loss: 0.169592. Entropy: 0.838872.\n",
      "episode: 2026   score: 21.0  epsilon: 1.0    steps: 568  evaluation reward: 18.4\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3763: Policy loss: 0.006123. Value loss: 0.467869. Entropy: 0.887378.\n",
      "Iteration 3764: Policy loss: -0.017503. Value loss: 0.182048. Entropy: 0.906990.\n",
      "Iteration 3765: Policy loss: -0.027391. Value loss: 0.113113. Entropy: 0.900231.\n",
      "episode: 2027   score: 22.0  epsilon: 1.0    steps: 592  evaluation reward: 18.36\n",
      "episode: 2028   score: 9.0  epsilon: 1.0    steps: 720  evaluation reward: 18.26\n",
      "episode: 2029   score: 14.0  epsilon: 1.0    steps: 1016  evaluation reward: 18.22\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3766: Policy loss: 0.006342. Value loss: 0.717695. Entropy: 0.906036.\n",
      "Iteration 3767: Policy loss: -0.008393. Value loss: 0.255657. Entropy: 0.910754.\n",
      "Iteration 3768: Policy loss: -0.018057. Value loss: 0.158233. Entropy: 0.908842.\n",
      "episode: 2030   score: 7.0  epsilon: 1.0    steps: 280  evaluation reward: 18.12\n",
      "episode: 2031   score: 18.0  epsilon: 1.0    steps: 536  evaluation reward: 18.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3769: Policy loss: 0.008580. Value loss: 0.507958. Entropy: 0.870177.\n",
      "Iteration 3770: Policy loss: -0.007906. Value loss: 0.179094. Entropy: 0.865520.\n",
      "Iteration 3771: Policy loss: -0.020374. Value loss: 0.103945. Entropy: 0.848276.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3772: Policy loss: 0.011577. Value loss: 0.369504. Entropy: 0.857267.\n",
      "Iteration 3773: Policy loss: -0.012361. Value loss: 0.143934. Entropy: 0.852940.\n",
      "Iteration 3774: Policy loss: -0.021045. Value loss: 0.073881. Entropy: 0.848005.\n",
      "episode: 2032   score: 14.0  epsilon: 1.0    steps: 96  evaluation reward: 18.09\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3775: Policy loss: 0.002051. Value loss: 0.277320. Entropy: 0.844962.\n",
      "Iteration 3776: Policy loss: -0.012425. Value loss: 0.114810. Entropy: 0.848659.\n",
      "Iteration 3777: Policy loss: -0.027489. Value loss: 0.065414. Entropy: 0.843800.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3778: Policy loss: 0.006379. Value loss: 0.647442. Entropy: 0.799470.\n",
      "Iteration 3779: Policy loss: -0.001960. Value loss: 0.244125. Entropy: 0.812619.\n",
      "Iteration 3780: Policy loss: -0.016201. Value loss: 0.131411. Entropy: 0.809070.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3781: Policy loss: 0.007080. Value loss: 0.470081. Entropy: 0.833696.\n",
      "Iteration 3782: Policy loss: -0.012258. Value loss: 0.233492. Entropy: 0.839844.\n",
      "Iteration 3783: Policy loss: -0.019829. Value loss: 0.108344. Entropy: 0.822370.\n",
      "episode: 2033   score: 18.0  epsilon: 1.0    steps: 32  evaluation reward: 18.2\n",
      "episode: 2034   score: 10.0  epsilon: 1.0    steps: 120  evaluation reward: 18.16\n",
      "episode: 2035   score: 12.0  epsilon: 1.0    steps: 632  evaluation reward: 18.05\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3784: Policy loss: 0.014652. Value loss: 0.293897. Entropy: 0.862177.\n",
      "Iteration 3785: Policy loss: -0.007269. Value loss: 0.109816. Entropy: 0.884847.\n",
      "Iteration 3786: Policy loss: -0.023358. Value loss: 0.064625. Entropy: 0.856951.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3787: Policy loss: 0.008788. Value loss: 0.406969. Entropy: 0.818044.\n",
      "Iteration 3788: Policy loss: -0.007728. Value loss: 0.173639. Entropy: 0.812346.\n",
      "Iteration 3789: Policy loss: -0.019290. Value loss: 0.096551. Entropy: 0.801887.\n",
      "episode: 2036   score: 15.0  epsilon: 1.0    steps: 64  evaluation reward: 18.09\n",
      "episode: 2037   score: 16.0  epsilon: 1.0    steps: 144  evaluation reward: 18.03\n",
      "episode: 2038   score: 27.0  epsilon: 1.0    steps: 784  evaluation reward: 18.12\n",
      "episode: 2039   score: 25.0  epsilon: 1.0    steps: 992  evaluation reward: 18.21\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3790: Policy loss: 0.011222. Value loss: 0.289252. Entropy: 0.830709.\n",
      "Iteration 3791: Policy loss: -0.012583. Value loss: 0.101612. Entropy: 0.836684.\n",
      "Iteration 3792: Policy loss: -0.024710. Value loss: 0.054640. Entropy: 0.839602.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3793: Policy loss: 0.004410. Value loss: 0.269900. Entropy: 0.851569.\n",
      "Iteration 3794: Policy loss: -0.016988. Value loss: 0.104180. Entropy: 0.840733.\n",
      "Iteration 3795: Policy loss: -0.032087. Value loss: 0.048744. Entropy: 0.836273.\n",
      "episode: 2040   score: 16.0  epsilon: 1.0    steps: 872  evaluation reward: 18.14\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3796: Policy loss: 0.011145. Value loss: 0.308376. Entropy: 0.916479.\n",
      "Iteration 3797: Policy loss: -0.013371. Value loss: 0.134793. Entropy: 0.901528.\n",
      "Iteration 3798: Policy loss: -0.028336. Value loss: 0.077548. Entropy: 0.891438.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3799: Policy loss: 0.015176. Value loss: 0.493361. Entropy: 0.853906.\n",
      "Iteration 3800: Policy loss: -0.007336. Value loss: 0.203797. Entropy: 0.851558.\n",
      "Iteration 3801: Policy loss: -0.023358. Value loss: 0.114713. Entropy: 0.847538.\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3802: Policy loss: 0.006484. Value loss: 0.447379. Entropy: 0.813682.\n",
      "Iteration 3803: Policy loss: -0.009613. Value loss: 0.196577. Entropy: 0.815102.\n",
      "Iteration 3804: Policy loss: -0.019263. Value loss: 0.098200. Entropy: 0.825826.\n",
      "episode: 2041   score: 19.0  epsilon: 1.0    steps: 728  evaluation reward: 18.17\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3805: Policy loss: 0.018303. Value loss: 0.654727. Entropy: 0.848862.\n",
      "Iteration 3806: Policy loss: -0.004410. Value loss: 0.278425. Entropy: 0.849344.\n",
      "Iteration 3807: Policy loss: -0.014396. Value loss: 0.146809. Entropy: 0.841952.\n",
      "episode: 2042   score: 13.0  epsilon: 1.0    steps: 320  evaluation reward: 18.18\n",
      "episode: 2043   score: 14.0  epsilon: 1.0    steps: 336  evaluation reward: 18.09\n",
      "episode: 2044   score: 26.0  epsilon: 1.0    steps: 584  evaluation reward: 18.19\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3808: Policy loss: 0.010673. Value loss: 0.365585. Entropy: 0.901206.\n",
      "Iteration 3809: Policy loss: -0.001059. Value loss: 0.126153. Entropy: 0.921292.\n",
      "Iteration 3810: Policy loss: -0.021564. Value loss: 0.079342. Entropy: 0.903781.\n",
      "episode: 2045   score: 23.0  epsilon: 1.0    steps: 368  evaluation reward: 18.19\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3811: Policy loss: 0.010380. Value loss: 0.492030. Entropy: 0.884087.\n",
      "Iteration 3812: Policy loss: -0.008861. Value loss: 0.176704. Entropy: 0.864732.\n",
      "Iteration 3813: Policy loss: -0.018211. Value loss: 0.082319. Entropy: 0.877214.\n",
      "episode: 2046   score: 28.0  epsilon: 1.0    steps: 824  evaluation reward: 18.18\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3814: Policy loss: 0.011358. Value loss: 0.409074. Entropy: 0.882789.\n",
      "Iteration 3815: Policy loss: -0.005193. Value loss: 0.153082. Entropy: 0.864115.\n",
      "Iteration 3816: Policy loss: -0.021080. Value loss: 0.083535. Entropy: 0.882814.\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3817: Policy loss: 0.011740. Value loss: 0.576870. Entropy: 0.815589.\n",
      "Iteration 3818: Policy loss: -0.015387. Value loss: 0.226177. Entropy: 0.810328.\n",
      "Iteration 3819: Policy loss: -0.014857. Value loss: 0.111847. Entropy: 0.818183.\n",
      "episode: 2047   score: 22.0  epsilon: 1.0    steps: 648  evaluation reward: 18.23\n",
      "episode: 2048   score: 19.0  epsilon: 1.0    steps: 832  evaluation reward: 18.14\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3820: Policy loss: 0.008482. Value loss: 0.644668. Entropy: 0.825567.\n",
      "Iteration 3821: Policy loss: -0.010994. Value loss: 0.286230. Entropy: 0.809618.\n",
      "Iteration 3822: Policy loss: -0.018314. Value loss: 0.179764. Entropy: 0.826861.\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3823: Policy loss: 0.008811. Value loss: 0.405629. Entropy: 0.813902.\n",
      "Iteration 3824: Policy loss: -0.004875. Value loss: 0.179703. Entropy: 0.798651.\n",
      "Iteration 3825: Policy loss: -0.019741. Value loss: 0.116524. Entropy: 0.800766.\n",
      "episode: 2049   score: 13.0  epsilon: 1.0    steps: 176  evaluation reward: 18.13\n",
      "episode: 2050   score: 22.0  epsilon: 1.0    steps: 648  evaluation reward: 18.18\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3826: Policy loss: 0.012626. Value loss: 0.470187. Entropy: 0.828845.\n",
      "Iteration 3827: Policy loss: -0.010357. Value loss: 0.157147. Entropy: 0.821819.\n",
      "Iteration 3828: Policy loss: -0.018920. Value loss: 0.085962. Entropy: 0.824689.\n",
      "now time :  2019-03-06 13:49:48.765712\n",
      "episode: 2051   score: 26.0  epsilon: 1.0    steps: 768  evaluation reward: 18.11\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3829: Policy loss: 0.008107. Value loss: 0.508601. Entropy: 0.851876.\n",
      "Iteration 3830: Policy loss: -0.008143. Value loss: 0.207867. Entropy: 0.866101.\n",
      "Iteration 3831: Policy loss: -0.020340. Value loss: 0.114918. Entropy: 0.867934.\n",
      "episode: 2052   score: 14.0  epsilon: 1.0    steps: 144  evaluation reward: 18.07\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3832: Policy loss: 0.007653. Value loss: 0.523343. Entropy: 0.910686.\n",
      "Iteration 3833: Policy loss: -0.005903. Value loss: 0.215856. Entropy: 0.896010.\n",
      "Iteration 3834: Policy loss: -0.022084. Value loss: 0.104837. Entropy: 0.888905.\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3835: Policy loss: 0.012090. Value loss: 0.386658. Entropy: 0.812064.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3836: Policy loss: -0.004713. Value loss: 0.170513. Entropy: 0.813807.\n",
      "Iteration 3837: Policy loss: -0.015398. Value loss: 0.083417. Entropy: 0.805668.\n",
      "episode: 2053   score: 19.0  epsilon: 1.0    steps: 192  evaluation reward: 18.06\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3838: Policy loss: 0.013536. Value loss: 0.783276. Entropy: 0.822039.\n",
      "Iteration 3839: Policy loss: -0.000184. Value loss: 0.288500. Entropy: 0.826431.\n",
      "Iteration 3840: Policy loss: -0.012430. Value loss: 0.142704. Entropy: 0.832382.\n",
      "episode: 2054   score: 37.0  epsilon: 1.0    steps: 336  evaluation reward: 18.27\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3841: Policy loss: 0.010863. Value loss: 0.592626. Entropy: 0.814467.\n",
      "Iteration 3842: Policy loss: -0.006975. Value loss: 0.187672. Entropy: 0.811462.\n",
      "Iteration 3843: Policy loss: -0.018270. Value loss: 0.095494. Entropy: 0.827151.\n",
      "episode: 2055   score: 23.0  epsilon: 1.0    steps: 456  evaluation reward: 18.27\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3844: Policy loss: 0.006759. Value loss: 0.550756. Entropy: 0.890952.\n",
      "Iteration 3845: Policy loss: -0.009209. Value loss: 0.202745. Entropy: 0.904184.\n",
      "Iteration 3846: Policy loss: -0.019636. Value loss: 0.120783. Entropy: 0.899397.\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3847: Policy loss: 0.010216. Value loss: 0.610488. Entropy: 0.873996.\n",
      "Iteration 3848: Policy loss: -0.006250. Value loss: 0.263059. Entropy: 0.875936.\n",
      "Iteration 3849: Policy loss: -0.019385. Value loss: 0.146997. Entropy: 0.882097.\n",
      "episode: 2056   score: 23.0  epsilon: 1.0    steps: 8  evaluation reward: 18.27\n",
      "episode: 2057   score: 28.0  epsilon: 1.0    steps: 400  evaluation reward: 18.44\n",
      "episode: 2058   score: 29.0  epsilon: 1.0    steps: 936  evaluation reward: 18.49\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3850: Policy loss: 0.012449. Value loss: 0.717409. Entropy: 0.853379.\n",
      "Iteration 3851: Policy loss: -0.007942. Value loss: 0.340514. Entropy: 0.857595.\n",
      "Iteration 3852: Policy loss: -0.017154. Value loss: 0.217804. Entropy: 0.849242.\n",
      "episode: 2059   score: 28.0  epsilon: 1.0    steps: 248  evaluation reward: 18.49\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3853: Policy loss: 0.005309. Value loss: 0.437697. Entropy: 0.849384.\n",
      "Iteration 3854: Policy loss: -0.009453. Value loss: 0.206099. Entropy: 0.846888.\n",
      "Iteration 3855: Policy loss: -0.021973. Value loss: 0.128637. Entropy: 0.847919.\n",
      "episode: 2060   score: 18.0  epsilon: 1.0    steps: 32  evaluation reward: 18.28\n",
      "episode: 2061   score: 14.0  epsilon: 1.0    steps: 496  evaluation reward: 18.23\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3856: Policy loss: 0.002998. Value loss: 0.368046. Entropy: 0.892399.\n",
      "Iteration 3857: Policy loss: -0.012830. Value loss: 0.137254. Entropy: 0.900001.\n",
      "Iteration 3858: Policy loss: -0.019046. Value loss: 0.070049. Entropy: 0.893865.\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3859: Policy loss: 0.008432. Value loss: 0.498964. Entropy: 0.917725.\n",
      "Iteration 3860: Policy loss: -0.013214. Value loss: 0.184584. Entropy: 0.916012.\n",
      "Iteration 3861: Policy loss: -0.029212. Value loss: 0.101366. Entropy: 0.903581.\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3862: Policy loss: 0.008584. Value loss: 0.328974. Entropy: 0.809018.\n",
      "Iteration 3863: Policy loss: -0.009665. Value loss: 0.111703. Entropy: 0.831774.\n",
      "Iteration 3864: Policy loss: -0.018896. Value loss: 0.049227. Entropy: 0.822687.\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3865: Policy loss: 0.013173. Value loss: 0.549647. Entropy: 0.913031.\n",
      "Iteration 3866: Policy loss: -0.011002. Value loss: 0.268955. Entropy: 0.898604.\n",
      "Iteration 3867: Policy loss: -0.022397. Value loss: 0.132736. Entropy: 0.899849.\n",
      "episode: 2062   score: 22.0  epsilon: 1.0    steps: 728  evaluation reward: 18.38\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3868: Policy loss: 0.008461. Value loss: 0.746634. Entropy: 0.864586.\n",
      "Iteration 3869: Policy loss: -0.005069. Value loss: 0.304177. Entropy: 0.871045.\n",
      "Iteration 3870: Policy loss: -0.018684. Value loss: 0.141899. Entropy: 0.848539.\n",
      "episode: 2063   score: 22.0  epsilon: 1.0    steps: 192  evaluation reward: 18.35\n",
      "episode: 2064   score: 19.0  epsilon: 1.0    steps: 280  evaluation reward: 18.36\n",
      "episode: 2065   score: 30.0  epsilon: 1.0    steps: 416  evaluation reward: 18.48\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3871: Policy loss: 0.009978. Value loss: 0.712583. Entropy: 0.917394.\n",
      "Iteration 3872: Policy loss: 0.003927. Value loss: 0.277221. Entropy: 0.904481.\n",
      "Iteration 3873: Policy loss: -0.009144. Value loss: 0.175918. Entropy: 0.898788.\n",
      "episode: 2066   score: 13.0  epsilon: 1.0    steps: 32  evaluation reward: 18.41\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3874: Policy loss: 0.014481. Value loss: 0.579884. Entropy: 0.914131.\n",
      "Iteration 3875: Policy loss: -0.003176. Value loss: 0.306728. Entropy: 0.904689.\n",
      "Iteration 3876: Policy loss: -0.015940. Value loss: 0.203866. Entropy: 0.903504.\n",
      "episode: 2067   score: 19.0  epsilon: 1.0    steps: 600  evaluation reward: 18.5\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3877: Policy loss: 0.002411. Value loss: 0.714842. Entropy: 0.841354.\n",
      "Iteration 3878: Policy loss: -0.000962. Value loss: 0.357567. Entropy: 0.843574.\n",
      "Iteration 3879: Policy loss: -0.013368. Value loss: 0.207416. Entropy: 0.844781.\n",
      "episode: 2068   score: 32.0  epsilon: 1.0    steps: 120  evaluation reward: 18.68\n",
      "episode: 2069   score: 23.0  epsilon: 1.0    steps: 744  evaluation reward: 18.74\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3880: Policy loss: 0.014672. Value loss: 0.573138. Entropy: 0.940615.\n",
      "Iteration 3881: Policy loss: -0.016617. Value loss: 0.275076. Entropy: 0.919426.\n",
      "Iteration 3882: Policy loss: -0.028061. Value loss: 0.165746. Entropy: 0.917238.\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3883: Policy loss: 0.022684. Value loss: 0.437239. Entropy: 0.892166.\n",
      "Iteration 3884: Policy loss: -0.002470. Value loss: 0.180243. Entropy: 0.867693.\n",
      "Iteration 3885: Policy loss: -0.016596. Value loss: 0.113652. Entropy: 0.884689.\n",
      "episode: 2070   score: 9.0  epsilon: 1.0    steps: 728  evaluation reward: 18.63\n",
      "episode: 2071   score: 11.0  epsilon: 1.0    steps: 880  evaluation reward: 18.49\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3886: Policy loss: 0.010534. Value loss: 0.715309. Entropy: 0.910520.\n",
      "Iteration 3887: Policy loss: -0.007306. Value loss: 0.350162. Entropy: 0.883819.\n",
      "Iteration 3888: Policy loss: -0.023958. Value loss: 0.222788. Entropy: 0.887909.\n",
      "episode: 2072   score: 14.0  epsilon: 1.0    steps: 240  evaluation reward: 18.38\n",
      "episode: 2073   score: 13.0  epsilon: 1.0    steps: 272  evaluation reward: 18.35\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3889: Policy loss: 0.000962. Value loss: 0.379052. Entropy: 0.863035.\n",
      "Iteration 3890: Policy loss: -0.005317. Value loss: 0.153305. Entropy: 0.865615.\n",
      "Iteration 3891: Policy loss: -0.018877. Value loss: 0.091651. Entropy: 0.865633.\n",
      "episode: 2074   score: 12.0  epsilon: 1.0    steps: 176  evaluation reward: 18.28\n",
      "episode: 2075   score: 14.0  epsilon: 1.0    steps: 968  evaluation reward: 18.17\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3892: Policy loss: 0.008239. Value loss: 0.505582. Entropy: 0.864444.\n",
      "Iteration 3893: Policy loss: -0.010803. Value loss: 0.206784. Entropy: 0.886584.\n",
      "Iteration 3894: Policy loss: -0.019978. Value loss: 0.129716. Entropy: 0.881400.\n",
      "episode: 2076   score: 13.0  epsilon: 1.0    steps: 944  evaluation reward: 18.14\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3895: Policy loss: 0.008094. Value loss: 0.269077. Entropy: 0.869576.\n",
      "Iteration 3896: Policy loss: -0.013693. Value loss: 0.102942. Entropy: 0.873196.\n",
      "Iteration 3897: Policy loss: -0.026754. Value loss: 0.057433. Entropy: 0.865851.\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3898: Policy loss: 0.006610. Value loss: 0.262438. Entropy: 0.900091.\n",
      "Iteration 3899: Policy loss: -0.014407. Value loss: 0.107488. Entropy: 0.905491.\n",
      "Iteration 3900: Policy loss: -0.016824. Value loss: 0.076702. Entropy: 0.909193.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3901: Policy loss: 0.003602. Value loss: 0.335957. Entropy: 0.926542.\n",
      "Iteration 3902: Policy loss: -0.013245. Value loss: 0.148012. Entropy: 0.923112.\n",
      "Iteration 3903: Policy loss: -0.029523. Value loss: 0.102310. Entropy: 0.915652.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3904: Policy loss: 0.011417. Value loss: 0.760512. Entropy: 0.878490.\n",
      "Iteration 3905: Policy loss: 0.002115. Value loss: 0.332130. Entropy: 0.858413.\n",
      "Iteration 3906: Policy loss: -0.013852. Value loss: 0.177241. Entropy: 0.872460.\n",
      "episode: 2077   score: 31.0  epsilon: 1.0    steps: 64  evaluation reward: 18.31\n",
      "episode: 2078   score: 19.0  epsilon: 1.0    steps: 632  evaluation reward: 18.28\n",
      "episode: 2079   score: 15.0  epsilon: 1.0    steps: 648  evaluation reward: 18.31\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3907: Policy loss: 0.012530. Value loss: 0.382505. Entropy: 0.928106.\n",
      "Iteration 3908: Policy loss: -0.011385. Value loss: 0.113941. Entropy: 0.911995.\n",
      "Iteration 3909: Policy loss: -0.023216. Value loss: 0.044364. Entropy: 0.917381.\n",
      "episode: 2080   score: 14.0  epsilon: 1.0    steps: 280  evaluation reward: 18.12\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3910: Policy loss: 0.009600. Value loss: 0.250470. Entropy: 0.879487.\n",
      "Iteration 3911: Policy loss: -0.012436. Value loss: 0.067079. Entropy: 0.886468.\n",
      "Iteration 3912: Policy loss: -0.026703. Value loss: 0.036071. Entropy: 0.873036.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3913: Policy loss: 0.010701. Value loss: 0.619894. Entropy: 0.908927.\n",
      "Iteration 3914: Policy loss: -0.009823. Value loss: 0.246676. Entropy: 0.879115.\n",
      "Iteration 3915: Policy loss: -0.021005. Value loss: 0.128754. Entropy: 0.872453.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3916: Policy loss: 0.008084. Value loss: 0.379040. Entropy: 0.882722.\n",
      "Iteration 3917: Policy loss: -0.017984. Value loss: 0.150215. Entropy: 0.876831.\n",
      "Iteration 3918: Policy loss: -0.026465. Value loss: 0.086250. Entropy: 0.872367.\n",
      "episode: 2081   score: 15.0  epsilon: 1.0    steps: 40  evaluation reward: 18.05\n",
      "episode: 2082   score: 22.0  epsilon: 1.0    steps: 128  evaluation reward: 18.12\n",
      "episode: 2083   score: 27.0  epsilon: 1.0    steps: 128  evaluation reward: 18.24\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3919: Policy loss: 0.007839. Value loss: 0.297577. Entropy: 0.846442.\n",
      "Iteration 3920: Policy loss: -0.012323. Value loss: 0.099248. Entropy: 0.835673.\n",
      "Iteration 3921: Policy loss: -0.026829. Value loss: 0.062169. Entropy: 0.847653.\n",
      "episode: 2084   score: 24.0  epsilon: 1.0    steps: 320  evaluation reward: 18.27\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3922: Policy loss: 0.007149. Value loss: 0.457237. Entropy: 0.836911.\n",
      "Iteration 3923: Policy loss: -0.007262. Value loss: 0.192462. Entropy: 0.853325.\n",
      "Iteration 3924: Policy loss: -0.016506. Value loss: 0.090939. Entropy: 0.836166.\n",
      "episode: 2085   score: 13.0  epsilon: 1.0    steps: 824  evaluation reward: 18.17\n",
      "episode: 2086   score: 18.0  epsilon: 1.0    steps: 864  evaluation reward: 18.15\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3925: Policy loss: 0.006456. Value loss: 0.475159. Entropy: 0.854794.\n",
      "Iteration 3926: Policy loss: -0.012289. Value loss: 0.193529. Entropy: 0.829172.\n",
      "Iteration 3927: Policy loss: -0.026915. Value loss: 0.094413. Entropy: 0.835618.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3928: Policy loss: 0.009653. Value loss: 0.244288. Entropy: 0.853338.\n",
      "Iteration 3929: Policy loss: -0.010588. Value loss: 0.110510. Entropy: 0.848107.\n",
      "Iteration 3930: Policy loss: -0.024101. Value loss: 0.058395. Entropy: 0.835395.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3931: Policy loss: 0.007231. Value loss: 0.660707. Entropy: 0.840760.\n",
      "Iteration 3932: Policy loss: -0.007290. Value loss: 0.194058. Entropy: 0.855325.\n",
      "Iteration 3933: Policy loss: -0.017512. Value loss: 0.101010. Entropy: 0.856040.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3934: Policy loss: 0.019686. Value loss: 0.346866. Entropy: 0.879771.\n",
      "Iteration 3935: Policy loss: -0.012842. Value loss: 0.127237. Entropy: 0.870860.\n",
      "Iteration 3936: Policy loss: -0.029799. Value loss: 0.074987. Entropy: 0.866251.\n",
      "episode: 2087   score: 26.0  epsilon: 1.0    steps: 480  evaluation reward: 18.1\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3937: Policy loss: 0.005378. Value loss: 0.478190. Entropy: 0.834502.\n",
      "Iteration 3938: Policy loss: -0.006444. Value loss: 0.177967. Entropy: 0.840107.\n",
      "Iteration 3939: Policy loss: -0.018862. Value loss: 0.091985. Entropy: 0.847551.\n",
      "episode: 2088   score: 26.0  epsilon: 1.0    steps: 88  evaluation reward: 18.14\n",
      "episode: 2089   score: 19.0  epsilon: 1.0    steps: 392  evaluation reward: 18.22\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3940: Policy loss: 0.012111. Value loss: 0.799558. Entropy: 0.885405.\n",
      "Iteration 3941: Policy loss: -0.000633. Value loss: 0.265353. Entropy: 0.888296.\n",
      "Iteration 3942: Policy loss: -0.014589. Value loss: 0.148830. Entropy: 0.886180.\n",
      "episode: 2090   score: 11.0  epsilon: 1.0    steps: 352  evaluation reward: 18.17\n",
      "episode: 2091   score: 26.0  epsilon: 1.0    steps: 992  evaluation reward: 18.26\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3943: Policy loss: 0.010107. Value loss: 0.509603. Entropy: 0.901245.\n",
      "Iteration 3944: Policy loss: -0.012550. Value loss: 0.162802. Entropy: 0.893392.\n",
      "Iteration 3945: Policy loss: -0.021171. Value loss: 0.084489. Entropy: 0.895073.\n",
      "episode: 2092   score: 13.0  epsilon: 1.0    steps: 152  evaluation reward: 18.23\n",
      "episode: 2093   score: 29.0  epsilon: 1.0    steps: 1016  evaluation reward: 18.33\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3946: Policy loss: 0.008590. Value loss: 0.499317. Entropy: 0.802042.\n",
      "Iteration 3947: Policy loss: -0.013812. Value loss: 0.188859. Entropy: 0.803777.\n",
      "Iteration 3948: Policy loss: -0.024009. Value loss: 0.097566. Entropy: 0.787376.\n",
      "episode: 2094   score: 30.0  epsilon: 1.0    steps: 80  evaluation reward: 18.53\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3949: Policy loss: 0.008049. Value loss: 0.344242. Entropy: 0.882246.\n",
      "Iteration 3950: Policy loss: -0.004138. Value loss: 0.190825. Entropy: 0.874926.\n",
      "Iteration 3951: Policy loss: -0.016333. Value loss: 0.144532. Entropy: 0.880706.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3952: Policy loss: 0.013892. Value loss: 0.240283. Entropy: 0.849758.\n",
      "Iteration 3953: Policy loss: -0.014801. Value loss: 0.089476. Entropy: 0.854615.\n",
      "Iteration 3954: Policy loss: -0.026380. Value loss: 0.056873. Entropy: 0.866073.\n",
      "episode: 2095   score: 13.0  epsilon: 1.0    steps: 304  evaluation reward: 18.54\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3955: Policy loss: 0.006806. Value loss: 0.247360. Entropy: 0.849374.\n",
      "Iteration 3956: Policy loss: -0.012624. Value loss: 0.101192. Entropy: 0.852502.\n",
      "Iteration 3957: Policy loss: -0.023985. Value loss: 0.064930. Entropy: 0.854304.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3958: Policy loss: 0.009830. Value loss: 0.484905. Entropy: 0.886850.\n",
      "Iteration 3959: Policy loss: -0.006576. Value loss: 0.144792. Entropy: 0.874122.\n",
      "Iteration 3960: Policy loss: -0.019711. Value loss: 0.093709. Entropy: 0.879181.\n",
      "episode: 2096   score: 12.0  epsilon: 1.0    steps: 592  evaluation reward: 18.52\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3961: Policy loss: 0.010210. Value loss: 0.852702. Entropy: 0.862006.\n",
      "Iteration 3962: Policy loss: -0.007118. Value loss: 0.359786. Entropy: 0.846297.\n",
      "Iteration 3963: Policy loss: -0.019161. Value loss: 0.162358. Entropy: 0.849008.\n",
      "episode: 2097   score: 22.0  epsilon: 1.0    steps: 864  evaluation reward: 18.61\n",
      "episode: 2098   score: 12.0  epsilon: 1.0    steps: 984  evaluation reward: 18.54\n",
      "episode: 2099   score: 22.0  epsilon: 1.0    steps: 1000  evaluation reward: 18.61\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3964: Policy loss: 0.007764. Value loss: 0.617076. Entropy: 0.910665.\n",
      "Iteration 3965: Policy loss: -0.008895. Value loss: 0.231835. Entropy: 0.912638.\n",
      "Iteration 3966: Policy loss: -0.023411. Value loss: 0.145785. Entropy: 0.899815.\n",
      "episode: 2100   score: 20.0  epsilon: 1.0    steps: 40  evaluation reward: 18.57\n",
      "now time :  2019-03-06 13:52:43.798803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2101   score: 25.0  epsilon: 1.0    steps: 72  evaluation reward: 18.62\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3967: Policy loss: 0.018311. Value loss: 0.390480. Entropy: 0.898930.\n",
      "Iteration 3968: Policy loss: -0.005855. Value loss: 0.153841. Entropy: 0.908001.\n",
      "Iteration 3969: Policy loss: -0.019912. Value loss: 0.078982. Entropy: 0.906487.\n",
      "episode: 2102   score: 18.0  epsilon: 1.0    steps: 144  evaluation reward: 18.6\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3970: Policy loss: 0.012260. Value loss: 0.422439. Entropy: 0.926624.\n",
      "Iteration 3971: Policy loss: -0.011698. Value loss: 0.166865. Entropy: 0.908721.\n",
      "Iteration 3972: Policy loss: -0.026482. Value loss: 0.102312. Entropy: 0.909436.\n",
      "episode: 2103   score: 12.0  epsilon: 1.0    steps: 192  evaluation reward: 18.58\n",
      "episode: 2104   score: 7.0  epsilon: 1.0    steps: 720  evaluation reward: 18.52\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3973: Policy loss: 0.012910. Value loss: 0.446345. Entropy: 0.916111.\n",
      "Iteration 3974: Policy loss: -0.004706. Value loss: 0.185227. Entropy: 0.916346.\n",
      "Iteration 3975: Policy loss: -0.022429. Value loss: 0.093088. Entropy: 0.898445.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3976: Policy loss: 0.005222. Value loss: 0.408567. Entropy: 0.933511.\n",
      "Iteration 3977: Policy loss: -0.015591. Value loss: 0.158346. Entropy: 0.935662.\n",
      "Iteration 3978: Policy loss: -0.026697. Value loss: 0.098941. Entropy: 0.927968.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3979: Policy loss: 0.005470. Value loss: 0.276648. Entropy: 0.931691.\n",
      "Iteration 3980: Policy loss: -0.018939. Value loss: 0.112005. Entropy: 0.936054.\n",
      "Iteration 3981: Policy loss: -0.026051. Value loss: 0.061872. Entropy: 0.931402.\n",
      "episode: 2105   score: 16.0  epsilon: 1.0    steps: 1024  evaluation reward: 18.53\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3982: Policy loss: 0.008281. Value loss: 0.485697. Entropy: 0.917182.\n",
      "Iteration 3983: Policy loss: -0.012684. Value loss: 0.191460. Entropy: 0.908946.\n",
      "Iteration 3984: Policy loss: -0.015751. Value loss: 0.088473. Entropy: 0.921738.\n",
      "episode: 2106   score: 13.0  epsilon: 1.0    steps: 216  evaluation reward: 18.46\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3985: Policy loss: 0.010380. Value loss: 0.379080. Entropy: 0.890218.\n",
      "Iteration 3986: Policy loss: -0.007143. Value loss: 0.161570. Entropy: 0.875129.\n",
      "Iteration 3987: Policy loss: -0.011338. Value loss: 0.080016. Entropy: 0.886215.\n",
      "episode: 2107   score: 21.0  epsilon: 1.0    steps: 624  evaluation reward: 18.49\n",
      "episode: 2108   score: 23.0  epsilon: 1.0    steps: 816  evaluation reward: 18.61\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3988: Policy loss: 0.019681. Value loss: 0.808178. Entropy: 0.878756.\n",
      "Iteration 3989: Policy loss: 0.000827. Value loss: 0.266352. Entropy: 0.880095.\n",
      "Iteration 3990: Policy loss: -0.017074. Value loss: 0.110253. Entropy: 0.883262.\n",
      "episode: 2109   score: 19.0  epsilon: 1.0    steps: 176  evaluation reward: 18.55\n",
      "episode: 2110   score: 26.0  epsilon: 1.0    steps: 736  evaluation reward: 18.62\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3991: Policy loss: 0.014551. Value loss: 0.471901. Entropy: 0.896039.\n",
      "Iteration 3992: Policy loss: -0.013100. Value loss: 0.175000. Entropy: 0.897162.\n",
      "Iteration 3993: Policy loss: -0.016442. Value loss: 0.101629. Entropy: 0.882749.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3994: Policy loss: 0.012524. Value loss: 0.343224. Entropy: 0.852439.\n",
      "Iteration 3995: Policy loss: -0.010908. Value loss: 0.142596. Entropy: 0.856301.\n",
      "Iteration 3996: Policy loss: -0.022064. Value loss: 0.084109. Entropy: 0.859354.\n",
      "episode: 2111   score: 17.0  epsilon: 1.0    steps: 64  evaluation reward: 18.69\n",
      "episode: 2112   score: 24.0  epsilon: 1.0    steps: 960  evaluation reward: 18.79\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3997: Policy loss: 0.008273. Value loss: 0.305536. Entropy: 0.854528.\n",
      "Iteration 3998: Policy loss: -0.016392. Value loss: 0.123183. Entropy: 0.857947.\n",
      "Iteration 3999: Policy loss: -0.025811. Value loss: 0.074814. Entropy: 0.860487.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 4000: Policy loss: 0.004743. Value loss: 0.487150. Entropy: 0.894718.\n",
      "Iteration 4001: Policy loss: -0.011211. Value loss: 0.249314. Entropy: 0.888039.\n",
      "Iteration 4002: Policy loss: -0.017911. Value loss: 0.141780. Entropy: 0.870789.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4003: Policy loss: 0.015352. Value loss: 0.463395. Entropy: 0.860812.\n",
      "Iteration 4004: Policy loss: -0.012956. Value loss: 0.170095. Entropy: 0.876816.\n",
      "Iteration 4005: Policy loss: -0.024086. Value loss: 0.087840. Entropy: 0.868591.\n",
      "episode: 2113   score: 13.0  epsilon: 1.0    steps: 672  evaluation reward: 18.82\n",
      "episode: 2114   score: 15.0  epsilon: 1.0    steps: 1016  evaluation reward: 18.79\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4006: Policy loss: 0.006840. Value loss: 0.464361. Entropy: 0.938704.\n",
      "Iteration 4007: Policy loss: -0.010918. Value loss: 0.170408. Entropy: 0.929970.\n",
      "Iteration 4008: Policy loss: -0.031473. Value loss: 0.085277. Entropy: 0.933254.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4009: Policy loss: 0.008529. Value loss: 0.207870. Entropy: 0.843512.\n",
      "Iteration 4010: Policy loss: -0.008798. Value loss: 0.060513. Entropy: 0.815792.\n",
      "Iteration 4011: Policy loss: -0.021114. Value loss: 0.032713. Entropy: 0.835575.\n",
      "episode: 2115   score: 20.0  epsilon: 1.0    steps: 456  evaluation reward: 18.84\n",
      "episode: 2116   score: 22.0  epsilon: 1.0    steps: 816  evaluation reward: 18.88\n",
      "episode: 2117   score: 25.0  epsilon: 1.0    steps: 896  evaluation reward: 18.9\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4012: Policy loss: 0.007479. Value loss: 0.436590. Entropy: 0.993279.\n",
      "Iteration 4013: Policy loss: -0.018659. Value loss: 0.168801. Entropy: 0.979463.\n",
      "Iteration 4014: Policy loss: -0.030993. Value loss: 0.089839. Entropy: 0.979646.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4015: Policy loss: 0.004900. Value loss: 0.513858. Entropy: 0.911326.\n",
      "Iteration 4016: Policy loss: -0.013633. Value loss: 0.242591. Entropy: 0.921580.\n",
      "Iteration 4017: Policy loss: -0.019985. Value loss: 0.103675. Entropy: 0.914303.\n",
      "episode: 2118   score: 23.0  epsilon: 1.0    steps: 488  evaluation reward: 18.98\n",
      "episode: 2119   score: 21.0  epsilon: 1.0    steps: 880  evaluation reward: 19.08\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4018: Policy loss: 0.015654. Value loss: 0.703680. Entropy: 0.922262.\n",
      "Iteration 4019: Policy loss: -0.006294. Value loss: 0.262277. Entropy: 0.917192.\n",
      "Iteration 4020: Policy loss: -0.012521. Value loss: 0.131434. Entropy: 0.908702.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4021: Policy loss: 0.005731. Value loss: 0.194659. Entropy: 0.849897.\n",
      "Iteration 4022: Policy loss: -0.013891. Value loss: 0.057558. Entropy: 0.848134.\n",
      "Iteration 4023: Policy loss: -0.025442. Value loss: 0.031840. Entropy: 0.860125.\n",
      "episode: 2120   score: 27.0  epsilon: 1.0    steps: 928  evaluation reward: 19.23\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4024: Policy loss: 0.004768. Value loss: 0.442293. Entropy: 0.886565.\n",
      "Iteration 4025: Policy loss: -0.005330. Value loss: 0.157208. Entropy: 0.869129.\n",
      "Iteration 4026: Policy loss: -0.027325. Value loss: 0.078413. Entropy: 0.883050.\n",
      "episode: 2121   score: 12.0  epsilon: 1.0    steps: 600  evaluation reward: 19.27\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4027: Policy loss: 0.008685. Value loss: 0.362087. Entropy: 0.909758.\n",
      "Iteration 4028: Policy loss: -0.008740. Value loss: 0.139882. Entropy: 0.899210.\n",
      "Iteration 4029: Policy loss: -0.024996. Value loss: 0.086020. Entropy: 0.896121.\n",
      "episode: 2122   score: 13.0  epsilon: 1.0    steps: 504  evaluation reward: 19.15\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4030: Policy loss: 0.013638. Value loss: 0.549709. Entropy: 0.928444.\n",
      "Iteration 4031: Policy loss: -0.006310. Value loss: 0.208697. Entropy: 0.933939.\n",
      "Iteration 4032: Policy loss: -0.010896. Value loss: 0.116761. Entropy: 0.917546.\n",
      "episode: 2123   score: 25.0  epsilon: 1.0    steps: 784  evaluation reward: 19.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2124   score: 15.0  epsilon: 1.0    steps: 928  evaluation reward: 19.22\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4033: Policy loss: 0.007731. Value loss: 0.423205. Entropy: 0.914553.\n",
      "Iteration 4034: Policy loss: -0.013617. Value loss: 0.162490. Entropy: 0.905874.\n",
      "Iteration 4035: Policy loss: -0.024701. Value loss: 0.092203. Entropy: 0.912957.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4036: Policy loss: 0.009303. Value loss: 0.530136. Entropy: 0.911894.\n",
      "Iteration 4037: Policy loss: -0.008916. Value loss: 0.235066. Entropy: 0.911325.\n",
      "Iteration 4038: Policy loss: -0.022055. Value loss: 0.156655. Entropy: 0.912048.\n",
      "episode: 2125   score: 25.0  epsilon: 1.0    steps: 96  evaluation reward: 19.24\n",
      "episode: 2126   score: 3.0  epsilon: 1.0    steps: 160  evaluation reward: 19.06\n",
      "episode: 2127   score: 25.0  epsilon: 1.0    steps: 760  evaluation reward: 19.09\n",
      "episode: 2128   score: 19.0  epsilon: 1.0    steps: 760  evaluation reward: 19.19\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4039: Policy loss: 0.012396. Value loss: 0.757590. Entropy: 0.880679.\n",
      "Iteration 4040: Policy loss: -0.007580. Value loss: 0.287050. Entropy: 0.873619.\n",
      "Iteration 4041: Policy loss: -0.017299. Value loss: 0.148861. Entropy: 0.882329.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4042: Policy loss: 0.007317. Value loss: 0.345285. Entropy: 0.883639.\n",
      "Iteration 4043: Policy loss: -0.009295. Value loss: 0.114371. Entropy: 0.894624.\n",
      "Iteration 4044: Policy loss: -0.015829. Value loss: 0.069804. Entropy: 0.885301.\n",
      "episode: 2129   score: 15.0  epsilon: 1.0    steps: 80  evaluation reward: 19.2\n",
      "episode: 2130   score: 13.0  epsilon: 1.0    steps: 544  evaluation reward: 19.26\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4045: Policy loss: 0.010480. Value loss: 0.371169. Entropy: 0.864067.\n",
      "Iteration 4046: Policy loss: -0.011486. Value loss: 0.167570. Entropy: 0.854118.\n",
      "Iteration 4047: Policy loss: -0.022079. Value loss: 0.106729. Entropy: 0.858491.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4048: Policy loss: 0.009176. Value loss: 0.274951. Entropy: 0.874825.\n",
      "Iteration 4049: Policy loss: -0.013424. Value loss: 0.102693. Entropy: 0.868407.\n",
      "Iteration 4050: Policy loss: -0.021946. Value loss: 0.062131. Entropy: 0.862018.\n",
      "episode: 2131   score: 12.0  epsilon: 1.0    steps: 1000  evaluation reward: 19.2\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4051: Policy loss: 0.011662. Value loss: 0.574233. Entropy: 0.884369.\n",
      "Iteration 4052: Policy loss: -0.011843. Value loss: 0.302050. Entropy: 0.872823.\n",
      "Iteration 4053: Policy loss: -0.022214. Value loss: 0.181170. Entropy: 0.864866.\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4054: Policy loss: 0.002209. Value loss: 0.573558. Entropy: 0.914047.\n",
      "Iteration 4055: Policy loss: -0.012283. Value loss: 0.185182. Entropy: 0.935956.\n",
      "Iteration 4056: Policy loss: -0.022511. Value loss: 0.094845. Entropy: 0.923018.\n",
      "episode: 2132   score: 15.0  epsilon: 1.0    steps: 392  evaluation reward: 19.21\n",
      "episode: 2133   score: 16.0  epsilon: 1.0    steps: 752  evaluation reward: 19.19\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4057: Policy loss: 0.012047. Value loss: 0.419358. Entropy: 0.884927.\n",
      "Iteration 4058: Policy loss: -0.008201. Value loss: 0.142681. Entropy: 0.892403.\n",
      "Iteration 4059: Policy loss: -0.025893. Value loss: 0.068616. Entropy: 0.889543.\n",
      "episode: 2134   score: 9.0  epsilon: 1.0    steps: 376  evaluation reward: 19.18\n",
      "episode: 2135   score: 20.0  epsilon: 1.0    steps: 880  evaluation reward: 19.26\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4060: Policy loss: 0.009706. Value loss: 0.936594. Entropy: 0.853228.\n",
      "Iteration 4061: Policy loss: -0.006891. Value loss: 0.340138. Entropy: 0.850413.\n",
      "Iteration 4062: Policy loss: -0.017568. Value loss: 0.194671. Entropy: 0.852099.\n",
      "episode: 2136   score: 21.0  epsilon: 1.0    steps: 728  evaluation reward: 19.32\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4063: Policy loss: 0.011957. Value loss: 0.562345. Entropy: 0.845789.\n",
      "Iteration 4064: Policy loss: -0.010929. Value loss: 0.209673. Entropy: 0.861780.\n",
      "Iteration 4065: Policy loss: -0.016556. Value loss: 0.116348. Entropy: 0.844417.\n",
      "episode: 2137   score: 25.0  epsilon: 1.0    steps: 272  evaluation reward: 19.41\n",
      "episode: 2138   score: 28.0  epsilon: 1.0    steps: 1016  evaluation reward: 19.42\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4066: Policy loss: 0.013649. Value loss: 0.533850. Entropy: 0.900582.\n",
      "Iteration 4067: Policy loss: -0.004345. Value loss: 0.203723. Entropy: 0.924909.\n",
      "Iteration 4068: Policy loss: -0.020226. Value loss: 0.110529. Entropy: 0.904783.\n",
      "episode: 2139   score: 6.0  epsilon: 1.0    steps: 360  evaluation reward: 19.23\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4069: Policy loss: 0.009732. Value loss: 0.460149. Entropy: 0.870391.\n",
      "Iteration 4070: Policy loss: -0.015297. Value loss: 0.168764. Entropy: 0.878183.\n",
      "Iteration 4071: Policy loss: -0.026329. Value loss: 0.094726. Entropy: 0.870128.\n",
      "episode: 2140   score: 11.0  epsilon: 1.0    steps: 208  evaluation reward: 19.18\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4072: Policy loss: 0.013346. Value loss: 0.580213. Entropy: 0.829500.\n",
      "Iteration 4073: Policy loss: -0.012976. Value loss: 0.189334. Entropy: 0.842663.\n",
      "Iteration 4074: Policy loss: -0.022716. Value loss: 0.093257. Entropy: 0.839256.\n",
      "episode: 2141   score: 15.0  epsilon: 1.0    steps: 352  evaluation reward: 19.14\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4075: Policy loss: 0.003912. Value loss: 0.273115. Entropy: 0.874199.\n",
      "Iteration 4076: Policy loss: -0.017742. Value loss: 0.096792. Entropy: 0.879041.\n",
      "Iteration 4077: Policy loss: -0.025936. Value loss: 0.066634. Entropy: 0.867514.\n",
      "episode: 2142   score: 12.0  epsilon: 1.0    steps: 344  evaluation reward: 19.13\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4078: Policy loss: 0.008350. Value loss: 0.222935. Entropy: 0.934964.\n",
      "Iteration 4079: Policy loss: -0.019493. Value loss: 0.086920. Entropy: 0.932029.\n",
      "Iteration 4080: Policy loss: -0.034646. Value loss: 0.050003. Entropy: 0.923680.\n",
      "episode: 2143   score: 14.0  epsilon: 1.0    steps: 792  evaluation reward: 19.13\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4081: Policy loss: 0.007788. Value loss: 0.478134. Entropy: 0.930435.\n",
      "Iteration 4082: Policy loss: -0.012604. Value loss: 0.161537. Entropy: 0.926828.\n",
      "Iteration 4083: Policy loss: -0.028984. Value loss: 0.092001. Entropy: 0.934289.\n",
      "episode: 2144   score: 13.0  epsilon: 1.0    steps: 88  evaluation reward: 19.0\n",
      "episode: 2145   score: 16.0  epsilon: 1.0    steps: 376  evaluation reward: 18.93\n",
      "episode: 2146   score: 12.0  epsilon: 1.0    steps: 784  evaluation reward: 18.77\n",
      "episode: 2147   score: 11.0  epsilon: 1.0    steps: 856  evaluation reward: 18.66\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4084: Policy loss: 0.008699. Value loss: 0.348706. Entropy: 0.827951.\n",
      "Iteration 4085: Policy loss: -0.015059. Value loss: 0.123898. Entropy: 0.840279.\n",
      "Iteration 4086: Policy loss: -0.021401. Value loss: 0.064680. Entropy: 0.833798.\n",
      "episode: 2148   score: 12.0  epsilon: 1.0    steps: 360  evaluation reward: 18.59\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4087: Policy loss: 0.008374. Value loss: 0.248786. Entropy: 0.890275.\n",
      "Iteration 4088: Policy loss: -0.015244. Value loss: 0.084142. Entropy: 0.895499.\n",
      "Iteration 4089: Policy loss: -0.026482. Value loss: 0.048067. Entropy: 0.877498.\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4090: Policy loss: 0.006246. Value loss: 0.210128. Entropy: 0.854115.\n",
      "Iteration 4091: Policy loss: -0.015263. Value loss: 0.078689. Entropy: 0.856213.\n",
      "Iteration 4092: Policy loss: -0.029250. Value loss: 0.035220. Entropy: 0.852838.\n",
      "episode: 2149   score: 13.0  epsilon: 1.0    steps: 848  evaluation reward: 18.59\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4093: Policy loss: 0.009040. Value loss: 0.228991. Entropy: 0.860864.\n",
      "Iteration 4094: Policy loss: -0.012046. Value loss: 0.076409. Entropy: 0.867747.\n",
      "Iteration 4095: Policy loss: -0.017879. Value loss: 0.044069. Entropy: 0.861692.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2150   score: 13.0  epsilon: 1.0    steps: 504  evaluation reward: 18.5\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4096: Policy loss: 0.007344. Value loss: 0.382421. Entropy: 0.836449.\n",
      "Iteration 4097: Policy loss: -0.007943. Value loss: 0.147614. Entropy: 0.839181.\n",
      "Iteration 4098: Policy loss: -0.016544. Value loss: 0.082266. Entropy: 0.837705.\n",
      "now time :  2019-03-06 13:55:32.344455\n",
      "episode: 2151   score: 15.0  epsilon: 1.0    steps: 56  evaluation reward: 18.39\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4099: Policy loss: 0.005754. Value loss: 0.312264. Entropy: 0.730186.\n",
      "Iteration 4100: Policy loss: -0.004992. Value loss: 0.127430. Entropy: 0.754450.\n",
      "Iteration 4101: Policy loss: -0.020282. Value loss: 0.060879. Entropy: 0.762127.\n",
      "episode: 2152   score: 17.0  epsilon: 1.0    steps: 920  evaluation reward: 18.42\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4102: Policy loss: 0.007328. Value loss: 0.227062. Entropy: 0.833885.\n",
      "Iteration 4103: Policy loss: -0.010388. Value loss: 0.086824. Entropy: 0.821070.\n",
      "Iteration 4104: Policy loss: -0.026930. Value loss: 0.069323. Entropy: 0.818528.\n",
      "episode: 2153   score: 19.0  epsilon: 1.0    steps: 752  evaluation reward: 18.42\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4105: Policy loss: 0.015578. Value loss: 0.479513. Entropy: 0.865465.\n",
      "Iteration 4106: Policy loss: -0.004929. Value loss: 0.154414. Entropy: 0.849883.\n",
      "Iteration 4107: Policy loss: -0.018841. Value loss: 0.079458. Entropy: 0.855329.\n",
      "episode: 2154   score: 16.0  epsilon: 1.0    steps: 216  evaluation reward: 18.21\n",
      "episode: 2155   score: 20.0  epsilon: 1.0    steps: 504  evaluation reward: 18.18\n",
      "episode: 2156   score: 16.0  epsilon: 1.0    steps: 568  evaluation reward: 18.11\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4108: Policy loss: 0.007618. Value loss: 0.522276. Entropy: 0.929831.\n",
      "Iteration 4109: Policy loss: -0.010809. Value loss: 0.241074. Entropy: 0.936044.\n",
      "Iteration 4110: Policy loss: -0.024142. Value loss: 0.151692. Entropy: 0.916241.\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4111: Policy loss: 0.003585. Value loss: 0.252557. Entropy: 0.881990.\n",
      "Iteration 4112: Policy loss: -0.017510. Value loss: 0.092089. Entropy: 0.875470.\n",
      "Iteration 4113: Policy loss: -0.027319. Value loss: 0.048492. Entropy: 0.873916.\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4114: Policy loss: 0.008141. Value loss: 0.326651. Entropy: 0.937120.\n",
      "Iteration 4115: Policy loss: -0.013103. Value loss: 0.125020. Entropy: 0.927620.\n",
      "Iteration 4116: Policy loss: -0.025795. Value loss: 0.079345. Entropy: 0.925228.\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4117: Policy loss: 0.006130. Value loss: 0.326634. Entropy: 0.959948.\n",
      "Iteration 4118: Policy loss: -0.012070. Value loss: 0.128550. Entropy: 0.963246.\n",
      "Iteration 4119: Policy loss: -0.027253. Value loss: 0.075322. Entropy: 0.966886.\n",
      "episode: 2157   score: 17.0  epsilon: 1.0    steps: 56  evaluation reward: 18.0\n",
      "episode: 2158   score: 20.0  epsilon: 1.0    steps: 944  evaluation reward: 17.91\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4120: Policy loss: 0.004313. Value loss: 0.480900. Entropy: 0.900908.\n",
      "Iteration 4121: Policy loss: -0.004823. Value loss: 0.215191. Entropy: 0.910559.\n",
      "Iteration 4122: Policy loss: -0.020501. Value loss: 0.115048. Entropy: 0.912986.\n",
      "episode: 2159   score: 21.0  epsilon: 1.0    steps: 208  evaluation reward: 17.84\n",
      "episode: 2160   score: 15.0  epsilon: 1.0    steps: 816  evaluation reward: 17.81\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4123: Policy loss: 0.008004. Value loss: 0.364504. Entropy: 0.904559.\n",
      "Iteration 4124: Policy loss: -0.019690. Value loss: 0.128117. Entropy: 0.896500.\n",
      "Iteration 4125: Policy loss: -0.028047. Value loss: 0.076133. Entropy: 0.901528.\n",
      "episode: 2161   score: 15.0  epsilon: 1.0    steps: 600  evaluation reward: 17.82\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4126: Policy loss: 0.010874. Value loss: 0.351329. Entropy: 0.912336.\n",
      "Iteration 4127: Policy loss: -0.014642. Value loss: 0.146389. Entropy: 0.905781.\n",
      "Iteration 4128: Policy loss: -0.024673. Value loss: 0.084135. Entropy: 0.912864.\n",
      "episode: 2162   score: 21.0  epsilon: 1.0    steps: 720  evaluation reward: 17.81\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4129: Policy loss: 0.020888. Value loss: 0.573890. Entropy: 0.926404.\n",
      "Iteration 4130: Policy loss: -0.005466. Value loss: 0.196143. Entropy: 0.922817.\n",
      "Iteration 4131: Policy loss: -0.022429. Value loss: 0.101344. Entropy: 0.925186.\n",
      "episode: 2163   score: 21.0  epsilon: 1.0    steps: 48  evaluation reward: 17.8\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4132: Policy loss: 0.009063. Value loss: 0.376190. Entropy: 0.944222.\n",
      "Iteration 4133: Policy loss: -0.009811. Value loss: 0.146776. Entropy: 0.918323.\n",
      "Iteration 4134: Policy loss: -0.021649. Value loss: 0.090666. Entropy: 0.905386.\n",
      "episode: 2164   score: 20.0  epsilon: 1.0    steps: 488  evaluation reward: 17.81\n",
      "episode: 2165   score: 8.0  epsilon: 1.0    steps: 952  evaluation reward: 17.59\n",
      "episode: 2166   score: 9.0  epsilon: 1.0    steps: 960  evaluation reward: 17.55\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4135: Policy loss: 0.015149. Value loss: 0.735977. Entropy: 0.959823.\n",
      "Iteration 4136: Policy loss: -0.000563. Value loss: 0.310417. Entropy: 0.946717.\n",
      "Iteration 4137: Policy loss: -0.015980. Value loss: 0.190462. Entropy: 0.939651.\n",
      "episode: 2167   score: 10.0  epsilon: 1.0    steps: 56  evaluation reward: 17.46\n",
      "episode: 2168   score: 19.0  epsilon: 1.0    steps: 504  evaluation reward: 17.33\n",
      "episode: 2169   score: 10.0  epsilon: 1.0    steps: 840  evaluation reward: 17.2\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4138: Policy loss: 0.010351. Value loss: 0.424683. Entropy: 0.907187.\n",
      "Iteration 4139: Policy loss: -0.003478. Value loss: 0.167379. Entropy: 0.911973.\n",
      "Iteration 4140: Policy loss: -0.016513. Value loss: 0.086073. Entropy: 0.905975.\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4141: Policy loss: 0.006420. Value loss: 0.276720. Entropy: 0.836756.\n",
      "Iteration 4142: Policy loss: -0.015743. Value loss: 0.073947. Entropy: 0.837363.\n",
      "Iteration 4143: Policy loss: -0.027962. Value loss: 0.039457. Entropy: 0.840279.\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4144: Policy loss: 0.004693. Value loss: 0.678493. Entropy: 0.915838.\n",
      "Iteration 4145: Policy loss: -0.009508. Value loss: 0.264670. Entropy: 0.916000.\n",
      "Iteration 4146: Policy loss: -0.017508. Value loss: 0.140928. Entropy: 0.907240.\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4147: Policy loss: 0.007665. Value loss: 0.372120. Entropy: 0.867822.\n",
      "Iteration 4148: Policy loss: -0.010329. Value loss: 0.138878. Entropy: 0.859009.\n",
      "Iteration 4149: Policy loss: -0.023285. Value loss: 0.073216. Entropy: 0.872959.\n",
      "episode: 2170   score: 14.0  epsilon: 1.0    steps: 200  evaluation reward: 17.25\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4150: Policy loss: 0.004442. Value loss: 0.569953. Entropy: 0.900135.\n",
      "Iteration 4151: Policy loss: -0.007277. Value loss: 0.289325. Entropy: 0.891284.\n",
      "Iteration 4152: Policy loss: -0.017898. Value loss: 0.185051. Entropy: 0.908628.\n",
      "episode: 2171   score: 12.0  epsilon: 1.0    steps: 616  evaluation reward: 17.26\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4153: Policy loss: 0.007303. Value loss: 0.699640. Entropy: 0.897362.\n",
      "Iteration 4154: Policy loss: -0.007973. Value loss: 0.267200. Entropy: 0.880256.\n",
      "Iteration 4155: Policy loss: -0.017588. Value loss: 0.167532. Entropy: 0.897484.\n",
      "episode: 2172   score: 27.0  epsilon: 1.0    steps: 120  evaluation reward: 17.39\n",
      "episode: 2173   score: 17.0  epsilon: 1.0    steps: 160  evaluation reward: 17.43\n",
      "episode: 2174   score: 21.0  epsilon: 1.0    steps: 568  evaluation reward: 17.52\n",
      "episode: 2175   score: 14.0  epsilon: 1.0    steps: 888  evaluation reward: 17.52\n",
      "episode: 2176   score: 17.0  epsilon: 1.0    steps: 984  evaluation reward: 17.56\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4156: Policy loss: 0.011418. Value loss: 0.479992. Entropy: 0.886966.\n",
      "Iteration 4157: Policy loss: -0.006715. Value loss: 0.151934. Entropy: 0.877192.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4158: Policy loss: -0.018877. Value loss: 0.080706. Entropy: 0.884586.\n",
      "episode: 2177   score: 17.0  epsilon: 1.0    steps: 656  evaluation reward: 17.42\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4159: Policy loss: 0.006797. Value loss: 0.258489. Entropy: 0.884616.\n",
      "Iteration 4160: Policy loss: -0.013113. Value loss: 0.103541. Entropy: 0.891073.\n",
      "Iteration 4161: Policy loss: -0.024082. Value loss: 0.053762. Entropy: 0.877926.\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4162: Policy loss: 0.012696. Value loss: 0.245497. Entropy: 0.885307.\n",
      "Iteration 4163: Policy loss: -0.012301. Value loss: 0.108517. Entropy: 0.890806.\n",
      "Iteration 4164: Policy loss: -0.026918. Value loss: 0.061764. Entropy: 0.879811.\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4165: Policy loss: 0.009502. Value loss: 0.238903. Entropy: 0.876310.\n",
      "Iteration 4166: Policy loss: -0.013437. Value loss: 0.110971. Entropy: 0.866215.\n",
      "Iteration 4167: Policy loss: -0.025788. Value loss: 0.068143. Entropy: 0.877420.\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4168: Policy loss: 0.009056. Value loss: 0.466920. Entropy: 0.871477.\n",
      "Iteration 4169: Policy loss: -0.005244. Value loss: 0.172286. Entropy: 0.853463.\n",
      "Iteration 4170: Policy loss: -0.017866. Value loss: 0.091690. Entropy: 0.867621.\n",
      "episode: 2178   score: 11.0  epsilon: 1.0    steps: 368  evaluation reward: 17.34\n",
      "episode: 2179   score: 13.0  epsilon: 1.0    steps: 464  evaluation reward: 17.32\n",
      "episode: 2180   score: 20.0  epsilon: 1.0    steps: 584  evaluation reward: 17.38\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4171: Policy loss: 0.012513. Value loss: 0.472809. Entropy: 0.886411.\n",
      "Iteration 4172: Policy loss: -0.008070. Value loss: 0.185924. Entropy: 0.901719.\n",
      "Iteration 4173: Policy loss: -0.020559. Value loss: 0.114323. Entropy: 0.900904.\n",
      "episode: 2181   score: 12.0  epsilon: 1.0    steps: 272  evaluation reward: 17.35\n",
      "episode: 2182   score: 16.0  epsilon: 1.0    steps: 848  evaluation reward: 17.29\n",
      "episode: 2183   score: 11.0  epsilon: 1.0    steps: 928  evaluation reward: 17.13\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4174: Policy loss: 0.011781. Value loss: 0.624941. Entropy: 0.888253.\n",
      "Iteration 4175: Policy loss: -0.006064. Value loss: 0.151625. Entropy: 0.880348.\n",
      "Iteration 4176: Policy loss: -0.016967. Value loss: 0.080384. Entropy: 0.883813.\n",
      "episode: 2184   score: 25.0  epsilon: 1.0    steps: 848  evaluation reward: 17.14\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4177: Policy loss: 0.008607. Value loss: 0.492306. Entropy: 0.819742.\n",
      "Iteration 4178: Policy loss: -0.011285. Value loss: 0.152556. Entropy: 0.833909.\n",
      "Iteration 4179: Policy loss: -0.021395. Value loss: 0.087045. Entropy: 0.836692.\n",
      "episode: 2185   score: 19.0  epsilon: 1.0    steps: 280  evaluation reward: 17.2\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4180: Policy loss: 0.015549. Value loss: 0.297245. Entropy: 0.888568.\n",
      "Iteration 4181: Policy loss: -0.010826. Value loss: 0.137146. Entropy: 0.876863.\n",
      "Iteration 4182: Policy loss: -0.027604. Value loss: 0.078470. Entropy: 0.877112.\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4183: Policy loss: 0.006022. Value loss: 0.381937. Entropy: 0.874591.\n",
      "Iteration 4184: Policy loss: -0.009192. Value loss: 0.145176. Entropy: 0.886780.\n",
      "Iteration 4185: Policy loss: -0.027277. Value loss: 0.081368. Entropy: 0.883826.\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4186: Policy loss: 0.012628. Value loss: 0.327214. Entropy: 0.851597.\n",
      "Iteration 4187: Policy loss: -0.008699. Value loss: 0.105664. Entropy: 0.849385.\n",
      "Iteration 4188: Policy loss: -0.024402. Value loss: 0.058751. Entropy: 0.850128.\n",
      "episode: 2186   score: 11.0  epsilon: 1.0    steps: 728  evaluation reward: 17.13\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4189: Policy loss: 0.014400. Value loss: 0.928430. Entropy: 0.899425.\n",
      "Iteration 4190: Policy loss: -0.000474. Value loss: 0.327069. Entropy: 0.886566.\n",
      "Iteration 4191: Policy loss: -0.009674. Value loss: 0.159518. Entropy: 0.893425.\n",
      "episode: 2187   score: 10.0  epsilon: 1.0    steps: 328  evaluation reward: 16.97\n",
      "episode: 2188   score: 19.0  epsilon: 1.0    steps: 576  evaluation reward: 16.9\n",
      "episode: 2189   score: 13.0  epsilon: 1.0    steps: 1016  evaluation reward: 16.84\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4192: Policy loss: 0.015804. Value loss: 0.435509. Entropy: 0.886258.\n",
      "Iteration 4193: Policy loss: -0.002021. Value loss: 0.153868. Entropy: 0.871268.\n",
      "Iteration 4194: Policy loss: -0.020012. Value loss: 0.095806. Entropy: 0.866110.\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4195: Policy loss: 0.005298. Value loss: 0.396476. Entropy: 0.838108.\n",
      "Iteration 4196: Policy loss: -0.009629. Value loss: 0.114032. Entropy: 0.830296.\n",
      "Iteration 4197: Policy loss: -0.024719. Value loss: 0.056160. Entropy: 0.823827.\n",
      "episode: 2190   score: 13.0  epsilon: 1.0    steps: 264  evaluation reward: 16.86\n",
      "episode: 2191   score: 14.0  epsilon: 1.0    steps: 456  evaluation reward: 16.74\n",
      "episode: 2192   score: 23.0  epsilon: 1.0    steps: 640  evaluation reward: 16.84\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4198: Policy loss: 0.010231. Value loss: 0.652800. Entropy: 0.891348.\n",
      "Iteration 4199: Policy loss: -0.005325. Value loss: 0.244967. Entropy: 0.862063.\n",
      "Iteration 4200: Policy loss: -0.016896. Value loss: 0.114082. Entropy: 0.855775.\n",
      "episode: 2193   score: 33.0  epsilon: 1.0    steps: 576  evaluation reward: 16.88\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4201: Policy loss: 0.004504. Value loss: 0.374066. Entropy: 0.822032.\n",
      "Iteration 4202: Policy loss: -0.011109. Value loss: 0.173762. Entropy: 0.828915.\n",
      "Iteration 4203: Policy loss: -0.017713. Value loss: 0.109809. Entropy: 0.826516.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4204: Policy loss: 0.010553. Value loss: 0.430881. Entropy: 0.758667.\n",
      "Iteration 4205: Policy loss: -0.007871. Value loss: 0.158638. Entropy: 0.745694.\n",
      "Iteration 4206: Policy loss: -0.021557. Value loss: 0.096129. Entropy: 0.754266.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4207: Policy loss: 0.010446. Value loss: 0.380082. Entropy: 0.819029.\n",
      "Iteration 4208: Policy loss: -0.008825. Value loss: 0.119501. Entropy: 0.821591.\n",
      "Iteration 4209: Policy loss: -0.025321. Value loss: 0.062632. Entropy: 0.819047.\n",
      "episode: 2194   score: 16.0  epsilon: 1.0    steps: 576  evaluation reward: 16.74\n",
      "episode: 2195   score: 14.0  epsilon: 1.0    steps: 624  evaluation reward: 16.75\n",
      "episode: 2196   score: 16.0  epsilon: 1.0    steps: 776  evaluation reward: 16.79\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4210: Policy loss: 0.004748. Value loss: 0.353220. Entropy: 0.851392.\n",
      "Iteration 4211: Policy loss: -0.013934. Value loss: 0.117984. Entropy: 0.834770.\n",
      "Iteration 4212: Policy loss: -0.022916. Value loss: 0.069048. Entropy: 0.837999.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4213: Policy loss: 0.012177. Value loss: 0.687683. Entropy: 0.805231.\n",
      "Iteration 4214: Policy loss: -0.000126. Value loss: 0.324017. Entropy: 0.822438.\n",
      "Iteration 4215: Policy loss: -0.012393. Value loss: 0.170537. Entropy: 0.817601.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4216: Policy loss: 0.008395. Value loss: 0.631351. Entropy: 0.882285.\n",
      "Iteration 4217: Policy loss: -0.010463. Value loss: 0.237824. Entropy: 0.874374.\n",
      "Iteration 4218: Policy loss: -0.020785. Value loss: 0.118196. Entropy: 0.896911.\n",
      "episode: 2197   score: 14.0  epsilon: 1.0    steps: 144  evaluation reward: 16.71\n",
      "episode: 2198   score: 22.0  epsilon: 1.0    steps: 168  evaluation reward: 16.81\n",
      "episode: 2199   score: 18.0  epsilon: 1.0    steps: 1024  evaluation reward: 16.77\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4219: Policy loss: 0.003921. Value loss: 0.359473. Entropy: 0.859029.\n",
      "Iteration 4220: Policy loss: -0.007416. Value loss: 0.135441. Entropy: 0.846802.\n",
      "Iteration 4221: Policy loss: -0.017368. Value loss: 0.075749. Entropy: 0.842979.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4222: Policy loss: 0.019458. Value loss: 0.472452. Entropy: 0.891424.\n",
      "Iteration 4223: Policy loss: -0.011597. Value loss: 0.265337. Entropy: 0.895893.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4224: Policy loss: -0.021640. Value loss: 0.165465. Entropy: 0.899705.\n",
      "episode: 2200   score: 30.0  epsilon: 1.0    steps: 408  evaluation reward: 16.87\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4225: Policy loss: 0.011920. Value loss: 0.413327. Entropy: 0.888190.\n",
      "Iteration 4226: Policy loss: -0.006106. Value loss: 0.149023. Entropy: 0.897965.\n",
      "Iteration 4227: Policy loss: -0.015860. Value loss: 0.086599. Entropy: 0.905756.\n",
      "now time :  2019-03-06 13:58:17.977568\n",
      "episode: 2201   score: 10.0  epsilon: 1.0    steps: 240  evaluation reward: 16.72\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4228: Policy loss: 0.005777. Value loss: 0.683861. Entropy: 0.862730.\n",
      "Iteration 4229: Policy loss: -0.013322. Value loss: 0.211381. Entropy: 0.878692.\n",
      "Iteration 4230: Policy loss: -0.017892. Value loss: 0.112787. Entropy: 0.866792.\n",
      "episode: 2202   score: 14.0  epsilon: 1.0    steps: 136  evaluation reward: 16.68\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4231: Policy loss: 0.005656. Value loss: 0.603946. Entropy: 0.935403.\n",
      "Iteration 4232: Policy loss: -0.006974. Value loss: 0.210906. Entropy: 0.924309.\n",
      "Iteration 4233: Policy loss: -0.017825. Value loss: 0.108594. Entropy: 0.934226.\n",
      "episode: 2203   score: 36.0  epsilon: 1.0    steps: 320  evaluation reward: 16.92\n",
      "episode: 2204   score: 27.0  epsilon: 1.0    steps: 656  evaluation reward: 17.12\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4234: Policy loss: 0.007179. Value loss: 0.478047. Entropy: 0.861482.\n",
      "Iteration 4235: Policy loss: -0.012640. Value loss: 0.215239. Entropy: 0.859367.\n",
      "Iteration 4236: Policy loss: -0.019206. Value loss: 0.118140. Entropy: 0.847267.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4237: Policy loss: 0.006712. Value loss: 0.361218. Entropy: 0.941242.\n",
      "Iteration 4238: Policy loss: -0.013804. Value loss: 0.218979. Entropy: 0.937979.\n",
      "Iteration 4239: Policy loss: -0.024234. Value loss: 0.120279. Entropy: 0.923342.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4240: Policy loss: 0.008592. Value loss: 0.380589. Entropy: 0.912292.\n",
      "Iteration 4241: Policy loss: -0.015081. Value loss: 0.157689. Entropy: 0.901754.\n",
      "Iteration 4242: Policy loss: -0.023481. Value loss: 0.084727. Entropy: 0.902831.\n",
      "episode: 2205   score: 17.0  epsilon: 1.0    steps: 136  evaluation reward: 17.13\n",
      "episode: 2206   score: 19.0  epsilon: 1.0    steps: 192  evaluation reward: 17.19\n",
      "episode: 2207   score: 23.0  epsilon: 1.0    steps: 904  evaluation reward: 17.21\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4243: Policy loss: 0.014019. Value loss: 0.476436. Entropy: 0.931939.\n",
      "Iteration 4244: Policy loss: -0.007274. Value loss: 0.221284. Entropy: 0.918857.\n",
      "Iteration 4245: Policy loss: -0.021902. Value loss: 0.146582. Entropy: 0.916754.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4246: Policy loss: 0.011039. Value loss: 0.430809. Entropy: 0.833219.\n",
      "Iteration 4247: Policy loss: 0.001095. Value loss: 0.115915. Entropy: 0.815191.\n",
      "Iteration 4248: Policy loss: -0.011587. Value loss: 0.060258. Entropy: 0.836387.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4249: Policy loss: 0.008768. Value loss: 0.393956. Entropy: 0.802255.\n",
      "Iteration 4250: Policy loss: -0.006900. Value loss: 0.222875. Entropy: 0.800310.\n",
      "Iteration 4251: Policy loss: -0.021488. Value loss: 0.153128. Entropy: 0.804168.\n",
      "episode: 2208   score: 37.0  epsilon: 1.0    steps: 328  evaluation reward: 17.35\n",
      "episode: 2209   score: 22.0  epsilon: 1.0    steps: 776  evaluation reward: 17.38\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4252: Policy loss: 0.004068. Value loss: 0.723546. Entropy: 0.877139.\n",
      "Iteration 4253: Policy loss: -0.011855. Value loss: 0.338393. Entropy: 0.876377.\n",
      "Iteration 4254: Policy loss: -0.022526. Value loss: 0.173340. Entropy: 0.872810.\n",
      "episode: 2210   score: 14.0  epsilon: 1.0    steps: 264  evaluation reward: 17.26\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4255: Policy loss: 0.018864. Value loss: 0.417987. Entropy: 0.922853.\n",
      "Iteration 4256: Policy loss: -0.005680. Value loss: 0.178489. Entropy: 0.901119.\n",
      "Iteration 4257: Policy loss: -0.023393. Value loss: 0.109941. Entropy: 0.902339.\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4258: Policy loss: 0.010173. Value loss: 0.329587. Entropy: 0.917786.\n",
      "Iteration 4259: Policy loss: -0.013938. Value loss: 0.133469. Entropy: 0.924215.\n",
      "Iteration 4260: Policy loss: -0.026668. Value loss: 0.070811. Entropy: 0.921558.\n",
      "episode: 2211   score: 25.0  epsilon: 1.0    steps: 144  evaluation reward: 17.34\n",
      "episode: 2212   score: 26.0  epsilon: 1.0    steps: 488  evaluation reward: 17.36\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4261: Policy loss: 0.009343. Value loss: 0.462066. Entropy: 0.915326.\n",
      "Iteration 4262: Policy loss: -0.002549. Value loss: 0.195158. Entropy: 0.921957.\n",
      "Iteration 4263: Policy loss: -0.024283. Value loss: 0.123072. Entropy: 0.910003.\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4264: Policy loss: 0.013297. Value loss: 0.402621. Entropy: 0.870308.\n",
      "Iteration 4265: Policy loss: -0.009572. Value loss: 0.144025. Entropy: 0.869478.\n",
      "Iteration 4266: Policy loss: -0.016587. Value loss: 0.076957. Entropy: 0.865765.\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4267: Policy loss: 0.006903. Value loss: 0.569509. Entropy: 0.906983.\n",
      "Iteration 4268: Policy loss: -0.009638. Value loss: 0.221641. Entropy: 0.912047.\n",
      "Iteration 4269: Policy loss: -0.023652. Value loss: 0.117132. Entropy: 0.906181.\n",
      "episode: 2213   score: 23.0  epsilon: 1.0    steps: 128  evaluation reward: 17.46\n",
      "episode: 2214   score: 24.0  epsilon: 1.0    steps: 248  evaluation reward: 17.55\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4270: Policy loss: 0.013523. Value loss: 0.616812. Entropy: 0.910933.\n",
      "Iteration 4271: Policy loss: 0.002319. Value loss: 0.181463. Entropy: 0.914282.\n",
      "Iteration 4272: Policy loss: -0.019154. Value loss: 0.099942. Entropy: 0.914980.\n",
      "episode: 2215   score: 13.0  epsilon: 1.0    steps: 208  evaluation reward: 17.48\n",
      "episode: 2216   score: 20.0  epsilon: 1.0    steps: 456  evaluation reward: 17.46\n",
      "episode: 2217   score: 20.0  epsilon: 1.0    steps: 688  evaluation reward: 17.41\n",
      "episode: 2218   score: 34.0  epsilon: 1.0    steps: 848  evaluation reward: 17.52\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4273: Policy loss: 0.010826. Value loss: 0.559904. Entropy: 0.882226.\n",
      "Iteration 4274: Policy loss: -0.012371. Value loss: 0.201743. Entropy: 0.885472.\n",
      "Iteration 4275: Policy loss: -0.016633. Value loss: 0.121248. Entropy: 0.883069.\n",
      "episode: 2219   score: 9.0  epsilon: 1.0    steps: 272  evaluation reward: 17.4\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4276: Policy loss: 0.006956. Value loss: 0.307626. Entropy: 0.886452.\n",
      "Iteration 4277: Policy loss: -0.012836. Value loss: 0.100632. Entropy: 0.903220.\n",
      "Iteration 4278: Policy loss: -0.025717. Value loss: 0.067690. Entropy: 0.900675.\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4279: Policy loss: 0.006129. Value loss: 0.530844. Entropy: 0.930270.\n",
      "Iteration 4280: Policy loss: -0.007913. Value loss: 0.247919. Entropy: 0.928232.\n",
      "Iteration 4281: Policy loss: -0.021393. Value loss: 0.123680. Entropy: 0.915909.\n",
      "episode: 2220   score: 22.0  epsilon: 1.0    steps: 800  evaluation reward: 17.35\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4282: Policy loss: 0.011727. Value loss: 0.500264. Entropy: 0.892025.\n",
      "Iteration 4283: Policy loss: -0.012798. Value loss: 0.212386. Entropy: 0.901682.\n",
      "Iteration 4284: Policy loss: -0.024097. Value loss: 0.122567. Entropy: 0.896345.\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4285: Policy loss: 0.013408. Value loss: 0.379150. Entropy: 0.966163.\n",
      "Iteration 4286: Policy loss: -0.010700. Value loss: 0.160031. Entropy: 0.968966.\n",
      "Iteration 4287: Policy loss: -0.024935. Value loss: 0.089550. Entropy: 0.976488.\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4288: Policy loss: 0.016707. Value loss: 0.627080. Entropy: 0.901381.\n",
      "Iteration 4289: Policy loss: -0.013610. Value loss: 0.263943. Entropy: 0.886692.\n",
      "Iteration 4290: Policy loss: -0.019499. Value loss: 0.131536. Entropy: 0.891785.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2221   score: 16.0  epsilon: 1.0    steps: 280  evaluation reward: 17.39\n",
      "episode: 2222   score: 19.0  epsilon: 1.0    steps: 320  evaluation reward: 17.45\n",
      "episode: 2223   score: 14.0  epsilon: 1.0    steps: 480  evaluation reward: 17.34\n",
      "episode: 2224   score: 16.0  epsilon: 1.0    steps: 904  evaluation reward: 17.35\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4291: Policy loss: 0.011930. Value loss: 0.699116. Entropy: 0.937307.\n",
      "Iteration 4292: Policy loss: -0.005668. Value loss: 0.336390. Entropy: 0.930890.\n",
      "Iteration 4293: Policy loss: -0.017655. Value loss: 0.208395. Entropy: 0.919652.\n",
      "episode: 2225   score: 17.0  epsilon: 1.0    steps: 920  evaluation reward: 17.27\n",
      "episode: 2226   score: 20.0  epsilon: 1.0    steps: 944  evaluation reward: 17.44\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4294: Policy loss: 0.011717. Value loss: 0.913229. Entropy: 0.891985.\n",
      "Iteration 4295: Policy loss: -0.002331. Value loss: 0.432983. Entropy: 0.886555.\n",
      "Iteration 4296: Policy loss: -0.008621. Value loss: 0.293775. Entropy: 0.887924.\n",
      "episode: 2227   score: 26.0  epsilon: 1.0    steps: 56  evaluation reward: 17.45\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4297: Policy loss: 0.005686. Value loss: 0.501649. Entropy: 0.822146.\n",
      "Iteration 4298: Policy loss: -0.011031. Value loss: 0.314000. Entropy: 0.829901.\n",
      "Iteration 4299: Policy loss: -0.017604. Value loss: 0.199509. Entropy: 0.804689.\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4300: Policy loss: 0.018217. Value loss: 0.339038. Entropy: 0.821270.\n",
      "Iteration 4301: Policy loss: -0.013369. Value loss: 0.143247. Entropy: 0.836830.\n",
      "Iteration 4302: Policy loss: -0.025656. Value loss: 0.081395. Entropy: 0.833321.\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4303: Policy loss: 0.008242. Value loss: 0.242818. Entropy: 0.903318.\n",
      "Iteration 4304: Policy loss: -0.014877. Value loss: 0.076669. Entropy: 0.905437.\n",
      "Iteration 4305: Policy loss: -0.026632. Value loss: 0.042592. Entropy: 0.893461.\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4306: Policy loss: 0.007274. Value loss: 0.620681. Entropy: 0.887150.\n",
      "Iteration 4307: Policy loss: -0.009772. Value loss: 0.320655. Entropy: 0.870282.\n",
      "Iteration 4308: Policy loss: -0.015554. Value loss: 0.178681. Entropy: 0.884739.\n",
      "episode: 2228   score: 11.0  epsilon: 1.0    steps: 384  evaluation reward: 17.37\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4309: Policy loss: 0.011794. Value loss: 0.360469. Entropy: 0.944711.\n",
      "Iteration 4310: Policy loss: -0.010492. Value loss: 0.125617. Entropy: 0.937436.\n",
      "Iteration 4311: Policy loss: -0.025954. Value loss: 0.079156. Entropy: 0.945363.\n",
      "episode: 2229   score: 11.0  epsilon: 1.0    steps: 488  evaluation reward: 17.33\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4312: Policy loss: 0.013639. Value loss: 0.510906. Entropy: 0.980094.\n",
      "Iteration 4313: Policy loss: -0.005230. Value loss: 0.208110. Entropy: 0.978916.\n",
      "Iteration 4314: Policy loss: -0.022330. Value loss: 0.137115. Entropy: 0.972460.\n",
      "episode: 2230   score: 24.0  epsilon: 1.0    steps: 616  evaluation reward: 17.44\n",
      "episode: 2231   score: 22.0  epsilon: 1.0    steps: 688  evaluation reward: 17.54\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4315: Policy loss: 0.014921. Value loss: 0.403000. Entropy: 0.891297.\n",
      "Iteration 4316: Policy loss: 0.004371. Value loss: 0.122759. Entropy: 0.879963.\n",
      "Iteration 4317: Policy loss: -0.020018. Value loss: 0.059940. Entropy: 0.887807.\n",
      "episode: 2232   score: 27.0  epsilon: 1.0    steps: 16  evaluation reward: 17.66\n",
      "episode: 2233   score: 18.0  epsilon: 1.0    steps: 288  evaluation reward: 17.68\n",
      "episode: 2234   score: 21.0  epsilon: 1.0    steps: 784  evaluation reward: 17.8\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4318: Policy loss: 0.010856. Value loss: 0.356535. Entropy: 0.881049.\n",
      "Iteration 4319: Policy loss: -0.008125. Value loss: 0.152299. Entropy: 0.867598.\n",
      "Iteration 4320: Policy loss: -0.020843. Value loss: 0.090302. Entropy: 0.880630.\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4321: Policy loss: 0.009960. Value loss: 0.452778. Entropy: 0.908425.\n",
      "Iteration 4322: Policy loss: -0.009682. Value loss: 0.205923. Entropy: 0.898118.\n",
      "Iteration 4323: Policy loss: -0.021263. Value loss: 0.110584. Entropy: 0.908350.\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4324: Policy loss: 0.019048. Value loss: 0.449926. Entropy: 0.844970.\n",
      "Iteration 4325: Policy loss: 0.000001. Value loss: 0.188341. Entropy: 0.832776.\n",
      "Iteration 4326: Policy loss: -0.013610. Value loss: 0.116900. Entropy: 0.829171.\n",
      "episode: 2235   score: 27.0  epsilon: 1.0    steps: 752  evaluation reward: 17.87\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4327: Policy loss: 0.008660. Value loss: 0.666782. Entropy: 0.890842.\n",
      "Iteration 4328: Policy loss: -0.007915. Value loss: 0.218836. Entropy: 0.898039.\n",
      "Iteration 4329: Policy loss: -0.014295. Value loss: 0.118680. Entropy: 0.892846.\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4330: Policy loss: 0.008207. Value loss: 0.554325. Entropy: 0.912722.\n",
      "Iteration 4331: Policy loss: -0.006620. Value loss: 0.212403. Entropy: 0.903239.\n",
      "Iteration 4332: Policy loss: -0.013571. Value loss: 0.117289. Entropy: 0.917540.\n",
      "episode: 2236   score: 23.0  epsilon: 1.0    steps: 176  evaluation reward: 17.89\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4333: Policy loss: 0.008878. Value loss: 0.490688. Entropy: 0.967380.\n",
      "Iteration 4334: Policy loss: -0.009036. Value loss: 0.162739. Entropy: 0.960763.\n",
      "Iteration 4335: Policy loss: -0.019601. Value loss: 0.080727. Entropy: 0.971346.\n",
      "episode: 2237   score: 21.0  epsilon: 1.0    steps: 464  evaluation reward: 17.85\n",
      "episode: 2238   score: 14.0  epsilon: 1.0    steps: 472  evaluation reward: 17.71\n",
      "episode: 2239   score: 14.0  epsilon: 1.0    steps: 640  evaluation reward: 17.79\n",
      "episode: 2240   score: 18.0  epsilon: 1.0    steps: 752  evaluation reward: 17.86\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4336: Policy loss: 0.009855. Value loss: 0.379324. Entropy: 0.929638.\n",
      "Iteration 4337: Policy loss: -0.009456. Value loss: 0.141045. Entropy: 0.928523.\n",
      "Iteration 4338: Policy loss: -0.021123. Value loss: 0.086775. Entropy: 0.926664.\n",
      "episode: 2241   score: 23.0  epsilon: 1.0    steps: 208  evaluation reward: 17.94\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4339: Policy loss: 0.008689. Value loss: 0.369279. Entropy: 0.838576.\n",
      "Iteration 4340: Policy loss: -0.006229. Value loss: 0.192759. Entropy: 0.832701.\n",
      "Iteration 4341: Policy loss: -0.018099. Value loss: 0.136489. Entropy: 0.841363.\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4342: Policy loss: 0.008227. Value loss: 0.362340. Entropy: 0.855955.\n",
      "Iteration 4343: Policy loss: -0.014882. Value loss: 0.158610. Entropy: 0.833937.\n",
      "Iteration 4344: Policy loss: -0.027134. Value loss: 0.087247. Entropy: 0.838511.\n",
      "episode: 2242   score: 22.0  epsilon: 1.0    steps: 96  evaluation reward: 18.04\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4345: Policy loss: 0.007667. Value loss: 0.396187. Entropy: 0.922069.\n",
      "Iteration 4346: Policy loss: -0.011942. Value loss: 0.167220. Entropy: 0.918234.\n",
      "Iteration 4347: Policy loss: -0.022231. Value loss: 0.113516. Entropy: 0.908426.\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4348: Policy loss: 0.007813. Value loss: 0.532188. Entropy: 0.851381.\n",
      "Iteration 4349: Policy loss: -0.009721. Value loss: 0.211729. Entropy: 0.851210.\n",
      "Iteration 4350: Policy loss: -0.012959. Value loss: 0.130723. Entropy: 0.845390.\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4351: Policy loss: 0.004793. Value loss: 0.656599. Entropy: 0.911672.\n",
      "Iteration 4352: Policy loss: -0.011238. Value loss: 0.205264. Entropy: 0.904481.\n",
      "Iteration 4353: Policy loss: -0.023589. Value loss: 0.097847. Entropy: 0.912066.\n",
      "episode: 2243   score: 12.0  epsilon: 1.0    steps: 56  evaluation reward: 18.02\n",
      "episode: 2244   score: 16.0  epsilon: 1.0    steps: 216  evaluation reward: 18.05\n",
      "episode: 2245   score: 15.0  epsilon: 1.0    steps: 552  evaluation reward: 18.04\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4354: Policy loss: 0.002795. Value loss: 0.566120. Entropy: 0.856697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4355: Policy loss: -0.007114. Value loss: 0.215219. Entropy: 0.865584.\n",
      "Iteration 4356: Policy loss: -0.016540. Value loss: 0.113842. Entropy: 0.874118.\n",
      "episode: 2246   score: 22.0  epsilon: 1.0    steps: 856  evaluation reward: 18.14\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4357: Policy loss: 0.012799. Value loss: 0.594737. Entropy: 0.898306.\n",
      "Iteration 4358: Policy loss: -0.006543. Value loss: 0.199964. Entropy: 0.886851.\n",
      "Iteration 4359: Policy loss: -0.020259. Value loss: 0.105738. Entropy: 0.899941.\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4360: Policy loss: 0.008949. Value loss: 0.601198. Entropy: 0.923141.\n",
      "Iteration 4361: Policy loss: -0.008630. Value loss: 0.247834. Entropy: 0.914181.\n",
      "Iteration 4362: Policy loss: -0.023603. Value loss: 0.127907. Entropy: 0.909804.\n",
      "episode: 2247   score: 26.0  epsilon: 1.0    steps: 352  evaluation reward: 18.29\n",
      "episode: 2248   score: 23.0  epsilon: 1.0    steps: 408  evaluation reward: 18.4\n",
      "episode: 2249   score: 34.0  epsilon: 1.0    steps: 512  evaluation reward: 18.61\n",
      "episode: 2250   score: 13.0  epsilon: 1.0    steps: 616  evaluation reward: 18.61\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4363: Policy loss: 0.005162. Value loss: 0.500361. Entropy: 0.877168.\n",
      "Iteration 4364: Policy loss: -0.008378. Value loss: 0.247447. Entropy: 0.861045.\n",
      "Iteration 4365: Policy loss: -0.019261. Value loss: 0.144392. Entropy: 0.862886.\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4366: Policy loss: 0.011954. Value loss: 0.469369. Entropy: 0.832120.\n",
      "Iteration 4367: Policy loss: -0.011118. Value loss: 0.179638. Entropy: 0.830365.\n",
      "Iteration 4368: Policy loss: -0.015827. Value loss: 0.094756. Entropy: 0.816092.\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4369: Policy loss: 0.007538. Value loss: 0.561661. Entropy: 0.885941.\n",
      "Iteration 4370: Policy loss: -0.010100. Value loss: 0.210966. Entropy: 0.880994.\n",
      "Iteration 4371: Policy loss: -0.020272. Value loss: 0.095292. Entropy: 0.866316.\n",
      "now time :  2019-03-06 14:01:23.341970\n",
      "episode: 2251   score: 18.0  epsilon: 1.0    steps: 576  evaluation reward: 18.64\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4372: Policy loss: 0.012948. Value loss: 0.602977. Entropy: 0.870536.\n",
      "Iteration 4373: Policy loss: -0.013493. Value loss: 0.221223. Entropy: 0.865024.\n",
      "Iteration 4374: Policy loss: -0.019844. Value loss: 0.105650. Entropy: 0.865399.\n",
      "episode: 2252   score: 21.0  epsilon: 1.0    steps: 816  evaluation reward: 18.68\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4375: Policy loss: 0.014060. Value loss: 0.465499. Entropy: 0.973822.\n",
      "Iteration 4376: Policy loss: -0.010740. Value loss: 0.159745. Entropy: 0.970827.\n",
      "Iteration 4377: Policy loss: -0.022974. Value loss: 0.093375. Entropy: 0.961524.\n",
      "episode: 2253   score: 9.0  epsilon: 1.0    steps: 24  evaluation reward: 18.58\n",
      "episode: 2254   score: 13.0  epsilon: 1.0    steps: 616  evaluation reward: 18.55\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4378: Policy loss: 0.010359. Value loss: 0.519873. Entropy: 0.870400.\n",
      "Iteration 4379: Policy loss: -0.001185. Value loss: 0.190755. Entropy: 0.876940.\n",
      "Iteration 4380: Policy loss: -0.014123. Value loss: 0.096265. Entropy: 0.875510.\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4381: Policy loss: 0.009813. Value loss: 0.682007. Entropy: 0.905639.\n",
      "Iteration 4382: Policy loss: -0.004490. Value loss: 0.247544. Entropy: 0.892298.\n",
      "Iteration 4383: Policy loss: -0.018757. Value loss: 0.131753. Entropy: 0.904962.\n",
      "episode: 2255   score: 16.0  epsilon: 1.0    steps: 8  evaluation reward: 18.51\n",
      "episode: 2256   score: 32.0  epsilon: 1.0    steps: 616  evaluation reward: 18.67\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4384: Policy loss: 0.012588. Value loss: 0.369892. Entropy: 0.904756.\n",
      "Iteration 4385: Policy loss: -0.004452. Value loss: 0.131760. Entropy: 0.930282.\n",
      "Iteration 4386: Policy loss: -0.020401. Value loss: 0.071236. Entropy: 0.924247.\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4387: Policy loss: 0.011450. Value loss: 0.388297. Entropy: 0.933738.\n",
      "Iteration 4388: Policy loss: -0.010942. Value loss: 0.152437. Entropy: 0.916200.\n",
      "Iteration 4389: Policy loss: -0.023658. Value loss: 0.079045. Entropy: 0.929277.\n",
      "episode: 2257   score: 36.0  epsilon: 1.0    steps: 168  evaluation reward: 18.86\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4390: Policy loss: 0.019181. Value loss: 0.350278. Entropy: 0.944800.\n",
      "Iteration 4391: Policy loss: -0.009690. Value loss: 0.138391. Entropy: 0.948490.\n",
      "Iteration 4392: Policy loss: -0.021781. Value loss: 0.089591. Entropy: 0.930680.\n",
      "episode: 2258   score: 30.0  epsilon: 1.0    steps: 504  evaluation reward: 18.96\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4393: Policy loss: 0.012031. Value loss: 0.398558. Entropy: 0.951103.\n",
      "Iteration 4394: Policy loss: -0.013928. Value loss: 0.163581. Entropy: 0.932617.\n",
      "Iteration 4395: Policy loss: -0.025486. Value loss: 0.096790. Entropy: 0.939495.\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4396: Policy loss: 0.012458. Value loss: 0.671625. Entropy: 0.934398.\n",
      "Iteration 4397: Policy loss: -0.010841. Value loss: 0.344725. Entropy: 0.926816.\n",
      "Iteration 4398: Policy loss: -0.014555. Value loss: 0.191989. Entropy: 0.931912.\n",
      "episode: 2259   score: 23.0  epsilon: 1.0    steps: 752  evaluation reward: 18.98\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4399: Policy loss: 0.014352. Value loss: 0.678653. Entropy: 0.929959.\n",
      "Iteration 4400: Policy loss: 0.000407. Value loss: 0.278356. Entropy: 0.935852.\n",
      "Iteration 4401: Policy loss: -0.014300. Value loss: 0.141017. Entropy: 0.942329.\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4402: Policy loss: 0.003618. Value loss: 0.642165. Entropy: 0.919103.\n",
      "Iteration 4403: Policy loss: -0.006877. Value loss: 0.309200. Entropy: 0.923754.\n",
      "Iteration 4404: Policy loss: -0.019175. Value loss: 0.157351. Entropy: 0.945372.\n",
      "episode: 2260   score: 32.0  epsilon: 1.0    steps: 344  evaluation reward: 19.15\n",
      "episode: 2261   score: 28.0  epsilon: 1.0    steps: 872  evaluation reward: 19.28\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4405: Policy loss: 0.013148. Value loss: 0.961368. Entropy: 0.992597.\n",
      "Iteration 4406: Policy loss: -0.000215. Value loss: 0.412414. Entropy: 0.980955.\n",
      "Iteration 4407: Policy loss: -0.017021. Value loss: 0.218199. Entropy: 0.990778.\n",
      "episode: 2262   score: 33.0  epsilon: 1.0    steps: 8  evaluation reward: 19.4\n",
      "episode: 2263   score: 24.0  epsilon: 1.0    steps: 128  evaluation reward: 19.43\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4408: Policy loss: 0.006529. Value loss: 0.483985. Entropy: 0.882091.\n",
      "Iteration 4409: Policy loss: -0.007405. Value loss: 0.194439. Entropy: 0.893046.\n",
      "Iteration 4410: Policy loss: -0.014928. Value loss: 0.104654. Entropy: 0.892476.\n",
      "episode: 2264   score: 25.0  epsilon: 1.0    steps: 888  evaluation reward: 19.48\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4411: Policy loss: 0.010155. Value loss: 0.468535. Entropy: 0.918722.\n",
      "Iteration 4412: Policy loss: -0.009055. Value loss: 0.163146. Entropy: 0.925334.\n",
      "Iteration 4413: Policy loss: -0.020235. Value loss: 0.108714. Entropy: 0.924011.\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4414: Policy loss: 0.008453. Value loss: 0.598707. Entropy: 0.968932.\n",
      "Iteration 4415: Policy loss: -0.012367. Value loss: 0.286497. Entropy: 0.956833.\n",
      "Iteration 4416: Policy loss: -0.023009. Value loss: 0.153449. Entropy: 0.961665.\n",
      "episode: 2265   score: 22.0  epsilon: 1.0    steps: 824  evaluation reward: 19.62\n",
      "episode: 2266   score: 38.0  epsilon: 1.0    steps: 904  evaluation reward: 19.91\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4417: Policy loss: 0.012197. Value loss: 0.765689. Entropy: 1.012313.\n",
      "Iteration 4418: Policy loss: -0.006734. Value loss: 0.365838. Entropy: 1.015765.\n",
      "Iteration 4419: Policy loss: -0.014609. Value loss: 0.212177. Entropy: 1.005989.\n",
      "episode: 2267   score: 18.0  epsilon: 1.0    steps: 936  evaluation reward: 19.99\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4420: Policy loss: 0.006272. Value loss: 0.576934. Entropy: 0.930817.\n",
      "Iteration 4421: Policy loss: -0.009739. Value loss: 0.219359. Entropy: 0.911153.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4422: Policy loss: -0.023852. Value loss: 0.113864. Entropy: 0.916619.\n",
      "episode: 2268   score: 12.0  epsilon: 1.0    steps: 192  evaluation reward: 19.92\n",
      "episode: 2269   score: 15.0  epsilon: 1.0    steps: 360  evaluation reward: 19.97\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4423: Policy loss: 0.004509. Value loss: 0.648676. Entropy: 1.018124.\n",
      "Iteration 4424: Policy loss: -0.017291. Value loss: 0.377885. Entropy: 1.031083.\n",
      "Iteration 4425: Policy loss: -0.023483. Value loss: 0.265003. Entropy: 1.011627.\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4426: Policy loss: 0.007404. Value loss: 0.540783. Entropy: 0.928126.\n",
      "Iteration 4427: Policy loss: -0.012699. Value loss: 0.218992. Entropy: 0.924330.\n",
      "Iteration 4428: Policy loss: -0.019608. Value loss: 0.121997. Entropy: 0.925551.\n",
      "episode: 2270   score: 19.0  epsilon: 1.0    steps: 576  evaluation reward: 20.02\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4429: Policy loss: 0.010190. Value loss: 0.600817. Entropy: 1.005507.\n",
      "Iteration 4430: Policy loss: -0.010105. Value loss: 0.251199. Entropy: 0.983195.\n",
      "Iteration 4431: Policy loss: -0.021864. Value loss: 0.143905. Entropy: 0.988057.\n",
      "episode: 2271   score: 21.0  epsilon: 1.0    steps: 320  evaluation reward: 20.11\n",
      "episode: 2272   score: 17.0  epsilon: 1.0    steps: 624  evaluation reward: 20.01\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4432: Policy loss: 0.005271. Value loss: 0.512752. Entropy: 0.969042.\n",
      "Iteration 4433: Policy loss: -0.015190. Value loss: 0.221132. Entropy: 0.956304.\n",
      "Iteration 4434: Policy loss: -0.024562. Value loss: 0.119210. Entropy: 0.971220.\n",
      "episode: 2273   score: 12.0  epsilon: 1.0    steps: 608  evaluation reward: 19.96\n",
      "episode: 2274   score: 13.0  epsilon: 1.0    steps: 904  evaluation reward: 19.88\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4435: Policy loss: 0.010441. Value loss: 0.540538. Entropy: 0.958743.\n",
      "Iteration 4436: Policy loss: -0.007855. Value loss: 0.221025. Entropy: 0.945122.\n",
      "Iteration 4437: Policy loss: -0.023523. Value loss: 0.141516. Entropy: 0.938744.\n",
      "episode: 2275   score: 15.0  epsilon: 1.0    steps: 456  evaluation reward: 19.89\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4438: Policy loss: 0.011751. Value loss: 0.483369. Entropy: 0.966110.\n",
      "Iteration 4439: Policy loss: -0.011817. Value loss: 0.169035. Entropy: 0.956583.\n",
      "Iteration 4440: Policy loss: -0.019251. Value loss: 0.101624. Entropy: 0.946580.\n",
      "episode: 2276   score: 18.0  epsilon: 1.0    steps: 1024  evaluation reward: 19.9\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4441: Policy loss: 0.009727. Value loss: 0.483404. Entropy: 0.931546.\n",
      "Iteration 4442: Policy loss: -0.007614. Value loss: 0.157595. Entropy: 0.942211.\n",
      "Iteration 4443: Policy loss: -0.021558. Value loss: 0.087624. Entropy: 0.937239.\n",
      "episode: 2277   score: 20.0  epsilon: 1.0    steps: 560  evaluation reward: 19.93\n",
      "episode: 2278   score: 13.0  epsilon: 1.0    steps: 584  evaluation reward: 19.95\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4444: Policy loss: 0.011802. Value loss: 0.321888. Entropy: 0.924524.\n",
      "Iteration 4445: Policy loss: -0.012142. Value loss: 0.107175. Entropy: 0.915031.\n",
      "Iteration 4446: Policy loss: -0.022578. Value loss: 0.059126. Entropy: 0.920550.\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4447: Policy loss: 0.007344. Value loss: 0.337437. Entropy: 0.962324.\n",
      "Iteration 4448: Policy loss: -0.009015. Value loss: 0.130707. Entropy: 0.963813.\n",
      "Iteration 4449: Policy loss: -0.015311. Value loss: 0.077098. Entropy: 0.961627.\n",
      "episode: 2279   score: 14.0  epsilon: 1.0    steps: 80  evaluation reward: 19.96\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4450: Policy loss: 0.007716. Value loss: 0.364025. Entropy: 0.910381.\n",
      "Iteration 4451: Policy loss: -0.009992. Value loss: 0.129893. Entropy: 0.907545.\n",
      "Iteration 4452: Policy loss: -0.020631. Value loss: 0.081660. Entropy: 0.903785.\n",
      "episode: 2280   score: 16.0  epsilon: 1.0    steps: 848  evaluation reward: 19.92\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4453: Policy loss: 0.005201. Value loss: 0.432069. Entropy: 1.039144.\n",
      "Iteration 4454: Policy loss: -0.012989. Value loss: 0.176450. Entropy: 1.055104.\n",
      "Iteration 4455: Policy loss: -0.024087. Value loss: 0.100426. Entropy: 1.040521.\n",
      "episode: 2281   score: 17.0  epsilon: 1.0    steps: 320  evaluation reward: 19.97\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4456: Policy loss: 0.002127. Value loss: 0.542031. Entropy: 0.920942.\n",
      "Iteration 4457: Policy loss: -0.013016. Value loss: 0.200731. Entropy: 0.914193.\n",
      "Iteration 4458: Policy loss: -0.022041. Value loss: 0.120123. Entropy: 0.916566.\n",
      "episode: 2282   score: 16.0  epsilon: 1.0    steps: 704  evaluation reward: 19.97\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4459: Policy loss: 0.010408. Value loss: 0.623509. Entropy: 0.999044.\n",
      "Iteration 4460: Policy loss: -0.012480. Value loss: 0.232395. Entropy: 0.999630.\n",
      "Iteration 4461: Policy loss: -0.024915. Value loss: 0.141947. Entropy: 0.994153.\n",
      "episode: 2283   score: 10.0  epsilon: 1.0    steps: 56  evaluation reward: 19.96\n",
      "episode: 2284   score: 21.0  epsilon: 1.0    steps: 864  evaluation reward: 19.92\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4462: Policy loss: 0.009254. Value loss: 0.205199. Entropy: 0.874321.\n",
      "Iteration 4463: Policy loss: -0.007528. Value loss: 0.076268. Entropy: 0.869385.\n",
      "Iteration 4464: Policy loss: -0.022396. Value loss: 0.042912. Entropy: 0.873194.\n",
      "episode: 2285   score: 20.0  epsilon: 1.0    steps: 232  evaluation reward: 19.93\n",
      "episode: 2286   score: 19.0  epsilon: 1.0    steps: 560  evaluation reward: 20.01\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4465: Policy loss: 0.007751. Value loss: 0.470838. Entropy: 0.951496.\n",
      "Iteration 4466: Policy loss: -0.015774. Value loss: 0.202194. Entropy: 0.948355.\n",
      "Iteration 4467: Policy loss: -0.026333. Value loss: 0.117481. Entropy: 0.942834.\n",
      "episode: 2287   score: 16.0  epsilon: 1.0    steps: 464  evaluation reward: 20.07\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4468: Policy loss: 0.015214. Value loss: 0.401619. Entropy: 0.950902.\n",
      "Iteration 4469: Policy loss: -0.006888. Value loss: 0.166794. Entropy: 0.935078.\n",
      "Iteration 4470: Policy loss: -0.023480. Value loss: 0.093704. Entropy: 0.947759.\n",
      "episode: 2288   score: 10.0  epsilon: 1.0    steps: 184  evaluation reward: 19.98\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4471: Policy loss: 0.017575. Value loss: 0.413865. Entropy: 0.880278.\n",
      "Iteration 4472: Policy loss: -0.015523. Value loss: 0.153920. Entropy: 0.880376.\n",
      "Iteration 4473: Policy loss: -0.026917. Value loss: 0.084735. Entropy: 0.885413.\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4474: Policy loss: 0.007767. Value loss: 0.817114. Entropy: 0.957356.\n",
      "Iteration 4475: Policy loss: -0.008961. Value loss: 0.316583. Entropy: 0.921228.\n",
      "Iteration 4476: Policy loss: -0.017118. Value loss: 0.139709. Entropy: 0.922998.\n",
      "episode: 2289   score: 8.0  epsilon: 1.0    steps: 464  evaluation reward: 19.93\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4477: Policy loss: 0.007991. Value loss: 0.504136. Entropy: 0.923195.\n",
      "Iteration 4478: Policy loss: -0.010037. Value loss: 0.181192. Entropy: 0.915886.\n",
      "Iteration 4479: Policy loss: -0.023475. Value loss: 0.108858. Entropy: 0.920629.\n",
      "episode: 2290   score: 19.0  epsilon: 1.0    steps: 352  evaluation reward: 19.99\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4480: Policy loss: 0.004759. Value loss: 0.484974. Entropy: 0.928500.\n",
      "Iteration 4481: Policy loss: -0.012584. Value loss: 0.179803. Entropy: 0.945461.\n",
      "Iteration 4482: Policy loss: -0.022199. Value loss: 0.094364. Entropy: 0.934174.\n",
      "episode: 2291   score: 15.0  epsilon: 1.0    steps: 296  evaluation reward: 20.0\n",
      "episode: 2292   score: 24.0  epsilon: 1.0    steps: 592  evaluation reward: 20.01\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4483: Policy loss: 0.005439. Value loss: 0.522791. Entropy: 0.997719.\n",
      "Iteration 4484: Policy loss: -0.010934. Value loss: 0.196749. Entropy: 0.992298.\n",
      "Iteration 4485: Policy loss: -0.018918. Value loss: 0.107572. Entropy: 1.008695.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2293   score: 25.0  epsilon: 1.0    steps: 648  evaluation reward: 19.93\n",
      "episode: 2294   score: 16.0  epsilon: 1.0    steps: 728  evaluation reward: 19.93\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4486: Policy loss: 0.007398. Value loss: 0.398114. Entropy: 0.982060.\n",
      "Iteration 4487: Policy loss: -0.008758. Value loss: 0.187851. Entropy: 0.977148.\n",
      "Iteration 4488: Policy loss: -0.021589. Value loss: 0.117515. Entropy: 0.977461.\n",
      "episode: 2295   score: 20.0  epsilon: 1.0    steps: 608  evaluation reward: 19.99\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4489: Policy loss: 0.007265. Value loss: 0.343190. Entropy: 0.966626.\n",
      "Iteration 4490: Policy loss: -0.017093. Value loss: 0.121656. Entropy: 0.978008.\n",
      "Iteration 4491: Policy loss: -0.029327. Value loss: 0.069829. Entropy: 0.975393.\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4492: Policy loss: 0.012173. Value loss: 0.429650. Entropy: 0.924448.\n",
      "Iteration 4493: Policy loss: -0.008010. Value loss: 0.125443. Entropy: 0.910991.\n",
      "Iteration 4494: Policy loss: -0.017185. Value loss: 0.058988. Entropy: 0.942352.\n",
      "episode: 2296   score: 22.0  epsilon: 1.0    steps: 600  evaluation reward: 20.05\n",
      "episode: 2297   score: 10.0  epsilon: 1.0    steps: 656  evaluation reward: 20.01\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4495: Policy loss: 0.007028. Value loss: 0.617402. Entropy: 0.925214.\n",
      "Iteration 4496: Policy loss: -0.007117. Value loss: 0.222007. Entropy: 0.913377.\n",
      "Iteration 4497: Policy loss: -0.017460. Value loss: 0.085296. Entropy: 0.907780.\n",
      "episode: 2298   score: 24.0  epsilon: 1.0    steps: 952  evaluation reward: 20.03\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4498: Policy loss: 0.009536. Value loss: 0.402381. Entropy: 0.987827.\n",
      "Iteration 4499: Policy loss: -0.010475. Value loss: 0.136876. Entropy: 1.019854.\n",
      "Iteration 4500: Policy loss: -0.018085. Value loss: 0.078076. Entropy: 1.024321.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4501: Policy loss: 0.007884. Value loss: 0.349397. Entropy: 0.993497.\n",
      "Iteration 4502: Policy loss: -0.010628. Value loss: 0.172133. Entropy: 0.986552.\n",
      "Iteration 4503: Policy loss: -0.024145. Value loss: 0.107450. Entropy: 0.978944.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4504: Policy loss: 0.008981. Value loss: 0.617575. Entropy: 0.926551.\n",
      "Iteration 4505: Policy loss: -0.007611. Value loss: 0.236239. Entropy: 0.937306.\n",
      "Iteration 4506: Policy loss: -0.018159. Value loss: 0.129722. Entropy: 0.930164.\n",
      "episode: 2299   score: 21.0  epsilon: 1.0    steps: 392  evaluation reward: 20.06\n",
      "episode: 2300   score: 30.0  epsilon: 1.0    steps: 416  evaluation reward: 20.06\n",
      "now time :  2019-03-06 14:04:16.332085\n",
      "episode: 2301   score: 19.0  epsilon: 1.0    steps: 752  evaluation reward: 20.15\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4507: Policy loss: 0.006705. Value loss: 1.034574. Entropy: 1.001831.\n",
      "Iteration 4508: Policy loss: 0.002881. Value loss: 0.508156. Entropy: 0.972451.\n",
      "Iteration 4509: Policy loss: -0.013254. Value loss: 0.331684. Entropy: 0.995082.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4510: Policy loss: 0.007431. Value loss: 0.623255. Entropy: 0.974279.\n",
      "Iteration 4511: Policy loss: -0.013393. Value loss: 0.231271. Entropy: 0.965655.\n",
      "Iteration 4512: Policy loss: -0.023659. Value loss: 0.108916. Entropy: 0.969463.\n",
      "episode: 2302   score: 17.0  epsilon: 1.0    steps: 376  evaluation reward: 20.18\n",
      "episode: 2303   score: 23.0  epsilon: 1.0    steps: 376  evaluation reward: 20.05\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4513: Policy loss: 0.004337. Value loss: 0.376846. Entropy: 0.979775.\n",
      "Iteration 4514: Policy loss: -0.010862. Value loss: 0.143226. Entropy: 0.970522.\n",
      "Iteration 4515: Policy loss: -0.026533. Value loss: 0.092791. Entropy: 0.966776.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4516: Policy loss: 0.010606. Value loss: 0.671539. Entropy: 0.993145.\n",
      "Iteration 4517: Policy loss: -0.010431. Value loss: 0.272577. Entropy: 0.987770.\n",
      "Iteration 4518: Policy loss: -0.025083. Value loss: 0.146985. Entropy: 0.992636.\n",
      "episode: 2304   score: 34.0  epsilon: 1.0    steps: 24  evaluation reward: 20.12\n",
      "episode: 2305   score: 20.0  epsilon: 1.0    steps: 688  evaluation reward: 20.15\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4519: Policy loss: 0.008900. Value loss: 0.621607. Entropy: 0.996173.\n",
      "Iteration 4520: Policy loss: -0.001458. Value loss: 0.239801. Entropy: 0.987642.\n",
      "Iteration 4521: Policy loss: -0.012823. Value loss: 0.145599. Entropy: 0.993801.\n",
      "episode: 2306   score: 10.0  epsilon: 1.0    steps: 240  evaluation reward: 20.06\n",
      "episode: 2307   score: 11.0  epsilon: 1.0    steps: 248  evaluation reward: 19.94\n",
      "episode: 2308   score: 18.0  epsilon: 1.0    steps: 376  evaluation reward: 19.75\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4522: Policy loss: 0.012873. Value loss: 0.565538. Entropy: 1.010551.\n",
      "Iteration 4523: Policy loss: -0.005000. Value loss: 0.215660. Entropy: 1.017097.\n",
      "Iteration 4524: Policy loss: -0.024792. Value loss: 0.132388. Entropy: 1.008536.\n",
      "episode: 2309   score: 11.0  epsilon: 1.0    steps: 128  evaluation reward: 19.64\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4525: Policy loss: 0.010977. Value loss: 0.311877. Entropy: 1.012822.\n",
      "Iteration 4526: Policy loss: -0.018304. Value loss: 0.142564. Entropy: 1.018469.\n",
      "Iteration 4527: Policy loss: -0.027936. Value loss: 0.082410. Entropy: 1.000964.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4528: Policy loss: 0.004184. Value loss: 0.411616. Entropy: 0.967900.\n",
      "Iteration 4529: Policy loss: -0.012899. Value loss: 0.186535. Entropy: 0.967357.\n",
      "Iteration 4530: Policy loss: -0.024564. Value loss: 0.110644. Entropy: 0.964392.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4531: Policy loss: 0.002855. Value loss: 0.346813. Entropy: 0.942131.\n",
      "Iteration 4532: Policy loss: -0.013301. Value loss: 0.155012. Entropy: 0.946932.\n",
      "Iteration 4533: Policy loss: -0.026367. Value loss: 0.091295. Entropy: 0.940516.\n",
      "episode: 2310   score: 11.0  epsilon: 1.0    steps: 344  evaluation reward: 19.61\n",
      "episode: 2311   score: 17.0  epsilon: 1.0    steps: 760  evaluation reward: 19.53\n",
      "episode: 2312   score: 17.0  epsilon: 1.0    steps: 920  evaluation reward: 19.44\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4534: Policy loss: 0.008926. Value loss: 0.341708. Entropy: 0.978789.\n",
      "Iteration 4535: Policy loss: -0.015488. Value loss: 0.107117. Entropy: 0.982023.\n",
      "Iteration 4536: Policy loss: -0.030948. Value loss: 0.063911. Entropy: 0.976220.\n",
      "episode: 2313   score: 9.0  epsilon: 1.0    steps: 112  evaluation reward: 19.3\n",
      "episode: 2314   score: 11.0  epsilon: 1.0    steps: 1008  evaluation reward: 19.17\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4537: Policy loss: 0.012120. Value loss: 0.355589. Entropy: 0.945491.\n",
      "Iteration 4538: Policy loss: -0.008630. Value loss: 0.131043. Entropy: 0.941050.\n",
      "Iteration 4539: Policy loss: -0.022195. Value loss: 0.080682. Entropy: 0.940077.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4540: Policy loss: 0.012764. Value loss: 0.481066. Entropy: 0.929751.\n",
      "Iteration 4541: Policy loss: -0.011998. Value loss: 0.180440. Entropy: 0.932738.\n",
      "Iteration 4542: Policy loss: -0.023591. Value loss: 0.095377. Entropy: 0.932946.\n",
      "episode: 2315   score: 19.0  epsilon: 1.0    steps: 200  evaluation reward: 19.23\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4543: Policy loss: 0.009629. Value loss: 0.598300. Entropy: 0.917137.\n",
      "Iteration 4544: Policy loss: -0.003036. Value loss: 0.240584. Entropy: 0.911803.\n",
      "Iteration 4545: Policy loss: -0.010200. Value loss: 0.129567. Entropy: 0.915258.\n",
      "episode: 2316   score: 17.0  epsilon: 1.0    steps: 224  evaluation reward: 19.2\n",
      "episode: 2317   score: 8.0  epsilon: 1.0    steps: 808  evaluation reward: 19.08\n",
      "episode: 2318   score: 26.0  epsilon: 1.0    steps: 904  evaluation reward: 19.0\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4546: Policy loss: 0.008267. Value loss: 0.462927. Entropy: 0.949261.\n",
      "Iteration 4547: Policy loss: -0.013302. Value loss: 0.231431. Entropy: 0.940881.\n",
      "Iteration 4548: Policy loss: -0.023309. Value loss: 0.148693. Entropy: 0.942686.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4549: Policy loss: 0.011027. Value loss: 0.311238. Entropy: 0.891405.\n",
      "Iteration 4550: Policy loss: -0.009952. Value loss: 0.121542. Entropy: 0.878006.\n",
      "Iteration 4551: Policy loss: -0.027379. Value loss: 0.069686. Entropy: 0.878808.\n",
      "episode: 2319   score: 16.0  epsilon: 1.0    steps: 400  evaluation reward: 19.07\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4552: Policy loss: 0.003490. Value loss: 0.308421. Entropy: 0.863743.\n",
      "Iteration 4553: Policy loss: -0.015194. Value loss: 0.095449. Entropy: 0.855518.\n",
      "Iteration 4554: Policy loss: -0.025123. Value loss: 0.055555. Entropy: 0.868816.\n",
      "episode: 2320   score: 13.0  epsilon: 1.0    steps: 88  evaluation reward: 18.98\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4555: Policy loss: 0.010748. Value loss: 0.282675. Entropy: 0.917488.\n",
      "Iteration 4556: Policy loss: -0.012646. Value loss: 0.130011. Entropy: 0.925974.\n",
      "Iteration 4557: Policy loss: -0.028384. Value loss: 0.093320. Entropy: 0.917498.\n",
      "episode: 2321   score: 17.0  epsilon: 1.0    steps: 312  evaluation reward: 18.99\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4558: Policy loss: 0.007932. Value loss: 0.478944. Entropy: 0.866581.\n",
      "Iteration 4559: Policy loss: -0.008666. Value loss: 0.191561. Entropy: 0.866166.\n",
      "Iteration 4560: Policy loss: -0.015272. Value loss: 0.095113. Entropy: 0.865044.\n",
      "episode: 2322   score: 24.0  epsilon: 1.0    steps: 488  evaluation reward: 19.04\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4561: Policy loss: 0.009363. Value loss: 0.605180. Entropy: 0.902751.\n",
      "Iteration 4562: Policy loss: -0.001787. Value loss: 0.242329. Entropy: 0.893286.\n",
      "Iteration 4563: Policy loss: -0.018206. Value loss: 0.132971. Entropy: 0.885794.\n",
      "episode: 2323   score: 21.0  epsilon: 1.0    steps: 696  evaluation reward: 19.11\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4564: Policy loss: 0.015633. Value loss: 0.455392. Entropy: 0.832897.\n",
      "Iteration 4565: Policy loss: -0.005799. Value loss: 0.129572. Entropy: 0.829167.\n",
      "Iteration 4566: Policy loss: -0.016835. Value loss: 0.071993. Entropy: 0.837687.\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4567: Policy loss: 0.009884. Value loss: 0.628861. Entropy: 0.953445.\n",
      "Iteration 4568: Policy loss: 0.000119. Value loss: 0.183476. Entropy: 0.952854.\n",
      "Iteration 4569: Policy loss: -0.010534. Value loss: 0.079073. Entropy: 0.961255.\n",
      "episode: 2324   score: 26.0  epsilon: 1.0    steps: 424  evaluation reward: 19.21\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4570: Policy loss: 0.012587. Value loss: 0.620578. Entropy: 0.956615.\n",
      "Iteration 4571: Policy loss: -0.010589. Value loss: 0.232609. Entropy: 0.957305.\n",
      "Iteration 4572: Policy loss: -0.022632. Value loss: 0.139349. Entropy: 0.970947.\n",
      "episode: 2325   score: 27.0  epsilon: 1.0    steps: 336  evaluation reward: 19.31\n",
      "episode: 2326   score: 26.0  epsilon: 1.0    steps: 544  evaluation reward: 19.37\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4573: Policy loss: 0.008548. Value loss: 0.512743. Entropy: 0.941124.\n",
      "Iteration 4574: Policy loss: -0.003071. Value loss: 0.172977. Entropy: 0.948912.\n",
      "Iteration 4575: Policy loss: -0.018580. Value loss: 0.086238. Entropy: 0.942814.\n",
      "episode: 2327   score: 26.0  epsilon: 1.0    steps: 1016  evaluation reward: 19.37\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4576: Policy loss: 0.012135. Value loss: 0.658896. Entropy: 0.975516.\n",
      "Iteration 4577: Policy loss: -0.007470. Value loss: 0.279253. Entropy: 0.970907.\n",
      "Iteration 4578: Policy loss: -0.021013. Value loss: 0.179364. Entropy: 0.985871.\n",
      "episode: 2328   score: 11.0  epsilon: 1.0    steps: 8  evaluation reward: 19.37\n",
      "episode: 2329   score: 22.0  epsilon: 1.0    steps: 736  evaluation reward: 19.48\n",
      "episode: 2330   score: 22.0  epsilon: 1.0    steps: 864  evaluation reward: 19.46\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4579: Policy loss: 0.014987. Value loss: 0.408443. Entropy: 0.906119.\n",
      "Iteration 4580: Policy loss: -0.013155. Value loss: 0.174616. Entropy: 0.894632.\n",
      "Iteration 4581: Policy loss: -0.024041. Value loss: 0.108769. Entropy: 0.894026.\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4582: Policy loss: 0.006498. Value loss: 0.339940. Entropy: 0.924159.\n",
      "Iteration 4583: Policy loss: -0.011270. Value loss: 0.140372. Entropy: 0.913481.\n",
      "Iteration 4584: Policy loss: -0.027758. Value loss: 0.067663. Entropy: 0.911105.\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4585: Policy loss: 0.017055. Value loss: 0.599045. Entropy: 0.896384.\n",
      "Iteration 4586: Policy loss: 0.002828. Value loss: 0.169006. Entropy: 0.910406.\n",
      "Iteration 4587: Policy loss: -0.015437. Value loss: 0.082236. Entropy: 0.924975.\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4588: Policy loss: 0.005959. Value loss: 0.525647. Entropy: 0.916020.\n",
      "Iteration 4589: Policy loss: -0.016797. Value loss: 0.149203. Entropy: 0.906077.\n",
      "Iteration 4590: Policy loss: -0.025046. Value loss: 0.088695. Entropy: 0.902402.\n",
      "episode: 2331   score: 13.0  epsilon: 1.0    steps: 256  evaluation reward: 19.37\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4591: Policy loss: 0.011457. Value loss: 0.644325. Entropy: 0.912385.\n",
      "Iteration 4592: Policy loss: -0.009141. Value loss: 0.239036. Entropy: 0.923471.\n",
      "Iteration 4593: Policy loss: -0.015398. Value loss: 0.137048. Entropy: 0.930279.\n",
      "episode: 2332   score: 14.0  epsilon: 1.0    steps: 144  evaluation reward: 19.24\n",
      "episode: 2333   score: 21.0  epsilon: 1.0    steps: 512  evaluation reward: 19.27\n",
      "episode: 2334   score: 20.0  epsilon: 1.0    steps: 784  evaluation reward: 19.26\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4594: Policy loss: 0.006746. Value loss: 0.527818. Entropy: 0.975336.\n",
      "Iteration 4595: Policy loss: -0.007758. Value loss: 0.218743. Entropy: 0.968520.\n",
      "Iteration 4596: Policy loss: -0.022108. Value loss: 0.122544. Entropy: 0.965950.\n",
      "episode: 2335   score: 36.0  epsilon: 1.0    steps: 392  evaluation reward: 19.35\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4597: Policy loss: 0.011868. Value loss: 0.389438. Entropy: 0.852468.\n",
      "Iteration 4598: Policy loss: -0.004260. Value loss: 0.128546. Entropy: 0.860522.\n",
      "Iteration 4599: Policy loss: -0.018230. Value loss: 0.076354. Entropy: 0.852047.\n",
      "episode: 2336   score: 23.0  epsilon: 1.0    steps: 712  evaluation reward: 19.35\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4600: Policy loss: 0.013839. Value loss: 0.565524. Entropy: 0.933232.\n",
      "Iteration 4601: Policy loss: -0.008530. Value loss: 0.235360. Entropy: 0.927949.\n",
      "Iteration 4602: Policy loss: -0.020621. Value loss: 0.116585. Entropy: 0.927967.\n",
      "episode: 2337   score: 22.0  epsilon: 1.0    steps: 176  evaluation reward: 19.36\n",
      "episode: 2338   score: 24.0  epsilon: 1.0    steps: 424  evaluation reward: 19.46\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4603: Policy loss: 0.007361. Value loss: 0.351737. Entropy: 0.897274.\n",
      "Iteration 4604: Policy loss: -0.007732. Value loss: 0.173739. Entropy: 0.889533.\n",
      "Iteration 4605: Policy loss: -0.021960. Value loss: 0.129937. Entropy: 0.900977.\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4606: Policy loss: 0.006808. Value loss: 0.532786. Entropy: 0.894897.\n",
      "Iteration 4607: Policy loss: -0.009849. Value loss: 0.216495. Entropy: 0.896142.\n",
      "Iteration 4608: Policy loss: -0.014454. Value loss: 0.108001. Entropy: 0.907527.\n",
      "episode: 2339   score: 21.0  epsilon: 1.0    steps: 904  evaluation reward: 19.53\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4609: Policy loss: 0.013918. Value loss: 0.561533. Entropy: 0.943545.\n",
      "Iteration 4610: Policy loss: -0.006633. Value loss: 0.187217. Entropy: 0.930602.\n",
      "Iteration 4611: Policy loss: -0.021880. Value loss: 0.099738. Entropy: 0.939750.\n",
      "episode: 2340   score: 14.0  epsilon: 1.0    steps: 1024  evaluation reward: 19.49\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4612: Policy loss: 0.003828. Value loss: 0.566327. Entropy: 0.928647.\n",
      "Iteration 4613: Policy loss: -0.016206. Value loss: 0.277716. Entropy: 0.915181.\n",
      "Iteration 4614: Policy loss: -0.020642. Value loss: 0.170486. Entropy: 0.920161.\n",
      "Training network. lr: 0.000215. clip: 0.085872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4615: Policy loss: 0.012111. Value loss: 0.622189. Entropy: 0.923378.\n",
      "Iteration 4616: Policy loss: -0.000887. Value loss: 0.237237. Entropy: 0.936227.\n",
      "Iteration 4617: Policy loss: -0.013641. Value loss: 0.118766. Entropy: 0.941806.\n",
      "episode: 2341   score: 29.0  epsilon: 1.0    steps: 208  evaluation reward: 19.55\n",
      "episode: 2342   score: 24.0  epsilon: 1.0    steps: 928  evaluation reward: 19.57\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4618: Policy loss: 0.009318. Value loss: 0.633787. Entropy: 0.959962.\n",
      "Iteration 4619: Policy loss: -0.012157. Value loss: 0.254341. Entropy: 0.969355.\n",
      "Iteration 4620: Policy loss: -0.019986. Value loss: 0.127782. Entropy: 0.962595.\n",
      "episode: 2343   score: 25.0  epsilon: 1.0    steps: 848  evaluation reward: 19.7\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4621: Policy loss: 0.011693. Value loss: 0.462601. Entropy: 0.984547.\n",
      "Iteration 4622: Policy loss: -0.011467. Value loss: 0.168562. Entropy: 0.990382.\n",
      "Iteration 4623: Policy loss: -0.021345. Value loss: 0.092123. Entropy: 0.984049.\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4624: Policy loss: 0.006743. Value loss: 0.510113. Entropy: 0.953911.\n",
      "Iteration 4625: Policy loss: 0.029812. Value loss: 0.205572. Entropy: 0.946760.\n",
      "Iteration 4626: Policy loss: -0.015711. Value loss: 0.124272. Entropy: 0.959008.\n",
      "episode: 2344   score: 21.0  epsilon: 1.0    steps: 168  evaluation reward: 19.75\n",
      "episode: 2345   score: 20.0  epsilon: 1.0    steps: 352  evaluation reward: 19.8\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4627: Policy loss: 0.010736. Value loss: 0.382904. Entropy: 0.946568.\n",
      "Iteration 4628: Policy loss: -0.010741. Value loss: 0.147045. Entropy: 0.947133.\n",
      "Iteration 4629: Policy loss: -0.019554. Value loss: 0.078667. Entropy: 0.944390.\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4630: Policy loss: 0.006832. Value loss: 0.463839. Entropy: 0.925385.\n",
      "Iteration 4631: Policy loss: -0.015790. Value loss: 0.186842. Entropy: 0.930496.\n",
      "Iteration 4632: Policy loss: -0.028024. Value loss: 0.102619. Entropy: 0.919164.\n",
      "episode: 2346   score: 21.0  epsilon: 1.0    steps: 312  evaluation reward: 19.79\n",
      "episode: 2347   score: 29.0  epsilon: 1.0    steps: 392  evaluation reward: 19.82\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4633: Policy loss: 0.006969. Value loss: 0.715374. Entropy: 0.964804.\n",
      "Iteration 4634: Policy loss: -0.002931. Value loss: 0.273725. Entropy: 0.957662.\n",
      "Iteration 4635: Policy loss: -0.018591. Value loss: 0.165781. Entropy: 0.955294.\n",
      "episode: 2348   score: 20.0  epsilon: 1.0    steps: 720  evaluation reward: 19.79\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4636: Policy loss: 0.013872. Value loss: 0.679990. Entropy: 0.971180.\n",
      "Iteration 4637: Policy loss: -0.005316. Value loss: 0.324401. Entropy: 0.956910.\n",
      "Iteration 4638: Policy loss: -0.019637. Value loss: 0.185794. Entropy: 0.961809.\n",
      "episode: 2349   score: 16.0  epsilon: 1.0    steps: 824  evaluation reward: 19.61\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4639: Policy loss: 0.007699. Value loss: 0.744337. Entropy: 0.953622.\n",
      "Iteration 4640: Policy loss: -0.006777. Value loss: 0.361531. Entropy: 0.960822.\n",
      "Iteration 4641: Policy loss: -0.024248. Value loss: 0.238115. Entropy: 0.946323.\n",
      "episode: 2350   score: 16.0  epsilon: 1.0    steps: 696  evaluation reward: 19.64\n",
      "now time :  2019-03-06 14:07:10.072029\n",
      "episode: 2351   score: 27.0  epsilon: 1.0    steps: 944  evaluation reward: 19.73\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4642: Policy loss: 0.006868. Value loss: 0.826149. Entropy: 0.924342.\n",
      "Iteration 4643: Policy loss: -0.007262. Value loss: 0.330093. Entropy: 0.926107.\n",
      "Iteration 4644: Policy loss: -0.017146. Value loss: 0.181795. Entropy: 0.917567.\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4645: Policy loss: 0.005367. Value loss: 0.466353. Entropy: 0.902636.\n",
      "Iteration 4646: Policy loss: -0.015904. Value loss: 0.185458. Entropy: 0.916258.\n",
      "Iteration 4647: Policy loss: -0.026917. Value loss: 0.121376. Entropy: 0.904376.\n",
      "episode: 2352   score: 24.0  epsilon: 1.0    steps: 680  evaluation reward: 19.76\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4648: Policy loss: 0.014617. Value loss: 0.552229. Entropy: 0.918038.\n",
      "Iteration 4649: Policy loss: -0.001279. Value loss: 0.270061. Entropy: 0.913036.\n",
      "Iteration 4650: Policy loss: -0.016444. Value loss: 0.158034. Entropy: 0.918967.\n",
      "episode: 2353   score: 13.0  epsilon: 1.0    steps: 480  evaluation reward: 19.8\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4651: Policy loss: 0.014430. Value loss: 0.641395. Entropy: 0.894021.\n",
      "Iteration 4652: Policy loss: -0.007341. Value loss: 0.303336. Entropy: 0.896992.\n",
      "Iteration 4653: Policy loss: -0.017240. Value loss: 0.179215. Entropy: 0.901240.\n",
      "episode: 2354   score: 13.0  epsilon: 1.0    steps: 736  evaluation reward: 19.8\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4654: Policy loss: 0.019879. Value loss: 0.577393. Entropy: 0.974091.\n",
      "Iteration 4655: Policy loss: -0.001633. Value loss: 0.232393. Entropy: 0.966675.\n",
      "Iteration 4656: Policy loss: -0.016600. Value loss: 0.126048. Entropy: 0.981992.\n",
      "episode: 2355   score: 29.0  epsilon: 1.0    steps: 552  evaluation reward: 19.93\n",
      "episode: 2356   score: 25.0  epsilon: 1.0    steps: 712  evaluation reward: 19.86\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4657: Policy loss: 0.011555. Value loss: 0.450190. Entropy: 0.997464.\n",
      "Iteration 4658: Policy loss: -0.001823. Value loss: 0.186928. Entropy: 0.982967.\n",
      "Iteration 4659: Policy loss: -0.016695. Value loss: 0.109934. Entropy: 0.975862.\n",
      "episode: 2357   score: 22.0  epsilon: 1.0    steps: 776  evaluation reward: 19.72\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4660: Policy loss: 0.004554. Value loss: 0.429896. Entropy: 0.881165.\n",
      "Iteration 4661: Policy loss: -0.006382. Value loss: 0.179838. Entropy: 0.882973.\n",
      "Iteration 4662: Policy loss: -0.022399. Value loss: 0.097165. Entropy: 0.888697.\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4663: Policy loss: 0.009835. Value loss: 0.575956. Entropy: 0.917774.\n",
      "Iteration 4664: Policy loss: -0.010310. Value loss: 0.236611. Entropy: 0.919487.\n",
      "Iteration 4665: Policy loss: -0.025255. Value loss: 0.130265. Entropy: 0.913826.\n",
      "episode: 2358   score: 23.0  epsilon: 1.0    steps: 416  evaluation reward: 19.65\n",
      "episode: 2359   score: 16.0  epsilon: 1.0    steps: 856  evaluation reward: 19.58\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4666: Policy loss: 0.022052. Value loss: 0.644247. Entropy: 0.935241.\n",
      "Iteration 4667: Policy loss: -0.003979. Value loss: 0.205909. Entropy: 0.959233.\n",
      "Iteration 4668: Policy loss: -0.018941. Value loss: 0.134030. Entropy: 0.954722.\n",
      "episode: 2360   score: 14.0  epsilon: 1.0    steps: 936  evaluation reward: 19.4\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4669: Policy loss: 0.006394. Value loss: 0.379559. Entropy: 0.921877.\n",
      "Iteration 4670: Policy loss: -0.016171. Value loss: 0.130057. Entropy: 0.932373.\n",
      "Iteration 4671: Policy loss: -0.025922. Value loss: 0.075246. Entropy: 0.920833.\n",
      "episode: 2361   score: 29.0  epsilon: 1.0    steps: 520  evaluation reward: 19.41\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4672: Policy loss: 0.005875. Value loss: 0.461293. Entropy: 0.903090.\n",
      "Iteration 4673: Policy loss: -0.016072. Value loss: 0.181251. Entropy: 0.893010.\n",
      "Iteration 4674: Policy loss: -0.025196. Value loss: 0.089426. Entropy: 0.902062.\n",
      "episode: 2362   score: 12.0  epsilon: 1.0    steps: 216  evaluation reward: 19.2\n",
      "episode: 2363   score: 23.0  epsilon: 1.0    steps: 848  evaluation reward: 19.19\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4675: Policy loss: 0.008701. Value loss: 0.549052. Entropy: 0.959065.\n",
      "Iteration 4676: Policy loss: -0.014817. Value loss: 0.225640. Entropy: 0.956871.\n",
      "Iteration 4677: Policy loss: -0.021206. Value loss: 0.117487. Entropy: 0.959017.\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4678: Policy loss: 0.012875. Value loss: 0.571635. Entropy: 0.941640.\n",
      "Iteration 4679: Policy loss: -0.004846. Value loss: 0.259737. Entropy: 0.941336.\n",
      "Iteration 4680: Policy loss: -0.019377. Value loss: 0.140295. Entropy: 0.940008.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2364   score: 15.0  epsilon: 1.0    steps: 312  evaluation reward: 19.09\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4681: Policy loss: 0.006044. Value loss: 0.707151. Entropy: 0.910909.\n",
      "Iteration 4682: Policy loss: -0.012456. Value loss: 0.248986. Entropy: 0.875358.\n",
      "Iteration 4683: Policy loss: -0.016078. Value loss: 0.116738. Entropy: 0.875807.\n",
      "episode: 2365   score: 27.0  epsilon: 1.0    steps: 96  evaluation reward: 19.14\n",
      "episode: 2366   score: 10.0  epsilon: 1.0    steps: 776  evaluation reward: 18.86\n",
      "episode: 2367   score: 19.0  epsilon: 1.0    steps: 944  evaluation reward: 18.87\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4684: Policy loss: 0.015344. Value loss: 0.670428. Entropy: 0.961017.\n",
      "Iteration 4685: Policy loss: -0.006680. Value loss: 0.288393. Entropy: 0.958806.\n",
      "Iteration 4686: Policy loss: -0.018669. Value loss: 0.174078. Entropy: 0.961398.\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4687: Policy loss: 0.008817. Value loss: 0.308765. Entropy: 0.890779.\n",
      "Iteration 4688: Policy loss: -0.016029. Value loss: 0.114034. Entropy: 0.892494.\n",
      "Iteration 4689: Policy loss: -0.029625. Value loss: 0.062888. Entropy: 0.887132.\n",
      "episode: 2368   score: 21.0  epsilon: 1.0    steps: 880  evaluation reward: 18.96\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4690: Policy loss: 0.012764. Value loss: 0.344906. Entropy: 0.887834.\n",
      "Iteration 4691: Policy loss: -0.009301. Value loss: 0.136561. Entropy: 0.888178.\n",
      "Iteration 4692: Policy loss: -0.022804. Value loss: 0.064559. Entropy: 0.867183.\n",
      "episode: 2369   score: 18.0  epsilon: 1.0    steps: 520  evaluation reward: 18.99\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4693: Policy loss: 0.012328. Value loss: 0.809420. Entropy: 0.905056.\n",
      "Iteration 4694: Policy loss: -0.004548. Value loss: 0.410746. Entropy: 0.904206.\n",
      "Iteration 4695: Policy loss: -0.008331. Value loss: 0.229276. Entropy: 0.897705.\n",
      "episode: 2370   score: 6.0  epsilon: 1.0    steps: 160  evaluation reward: 18.86\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4696: Policy loss: 0.011138. Value loss: 0.478850. Entropy: 0.880824.\n",
      "Iteration 4697: Policy loss: -0.012354. Value loss: 0.163927. Entropy: 0.879510.\n",
      "Iteration 4698: Policy loss: -0.022353. Value loss: 0.094238. Entropy: 0.879310.\n",
      "episode: 2371   score: 25.0  epsilon: 1.0    steps: 856  evaluation reward: 18.9\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4699: Policy loss: 0.009829. Value loss: 0.369077. Entropy: 0.875686.\n",
      "Iteration 4700: Policy loss: -0.003490. Value loss: 0.178613. Entropy: 0.868772.\n",
      "Iteration 4701: Policy loss: -0.023909. Value loss: 0.097079. Entropy: 0.881869.\n",
      "episode: 2372   score: 24.0  epsilon: 1.0    steps: 304  evaluation reward: 18.97\n",
      "episode: 2373   score: 15.0  epsilon: 1.0    steps: 656  evaluation reward: 19.0\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4702: Policy loss: 0.003873. Value loss: 0.576920. Entropy: 0.861075.\n",
      "Iteration 4703: Policy loss: -0.006561. Value loss: 0.243130. Entropy: 0.858489.\n",
      "Iteration 4704: Policy loss: -0.013733. Value loss: 0.102768. Entropy: 0.852662.\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4705: Policy loss: 0.015371. Value loss: 0.388701. Entropy: 0.932971.\n",
      "Iteration 4706: Policy loss: -0.008639. Value loss: 0.147447. Entropy: 0.927133.\n",
      "Iteration 4707: Policy loss: -0.023188. Value loss: 0.104792. Entropy: 0.922891.\n",
      "episode: 2374   score: 24.0  epsilon: 1.0    steps: 600  evaluation reward: 19.11\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4708: Policy loss: 0.010376. Value loss: 0.486432. Entropy: 0.895142.\n",
      "Iteration 4709: Policy loss: -0.005581. Value loss: 0.149823. Entropy: 0.918385.\n",
      "Iteration 4710: Policy loss: -0.022100. Value loss: 0.081419. Entropy: 0.909853.\n",
      "episode: 2375   score: 11.0  epsilon: 1.0    steps: 128  evaluation reward: 19.07\n",
      "episode: 2376   score: 23.0  epsilon: 1.0    steps: 736  evaluation reward: 19.12\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4711: Policy loss: 0.014433. Value loss: 0.827070. Entropy: 0.936507.\n",
      "Iteration 4712: Policy loss: -0.005350. Value loss: 0.387079. Entropy: 0.950917.\n",
      "Iteration 4713: Policy loss: -0.016806. Value loss: 0.232900. Entropy: 0.944736.\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4714: Policy loss: 0.007181. Value loss: 0.434915. Entropy: 0.930878.\n",
      "Iteration 4715: Policy loss: -0.010501. Value loss: 0.192934. Entropy: 0.934432.\n",
      "Iteration 4716: Policy loss: -0.021989. Value loss: 0.107145. Entropy: 0.934272.\n",
      "episode: 2377   score: 16.0  epsilon: 1.0    steps: 496  evaluation reward: 19.08\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4717: Policy loss: 0.009860. Value loss: 0.563247. Entropy: 0.910739.\n",
      "Iteration 4718: Policy loss: -0.007045. Value loss: 0.237092. Entropy: 0.910124.\n",
      "Iteration 4719: Policy loss: -0.015617. Value loss: 0.129717. Entropy: 0.911747.\n",
      "episode: 2378   score: 28.0  epsilon: 1.0    steps: 632  evaluation reward: 19.23\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4720: Policy loss: 0.007342. Value loss: 0.538608. Entropy: 0.940208.\n",
      "Iteration 4721: Policy loss: -0.011755. Value loss: 0.177427. Entropy: 0.936009.\n",
      "Iteration 4722: Policy loss: -0.019660. Value loss: 0.090526. Entropy: 0.926600.\n",
      "episode: 2379   score: 17.0  epsilon: 1.0    steps: 216  evaluation reward: 19.26\n",
      "episode: 2380   score: 32.0  epsilon: 1.0    steps: 584  evaluation reward: 19.42\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4723: Policy loss: 0.016572. Value loss: 0.562631. Entropy: 0.913262.\n",
      "Iteration 4724: Policy loss: -0.000367. Value loss: 0.219298. Entropy: 0.894959.\n",
      "Iteration 4725: Policy loss: -0.015556. Value loss: 0.121125. Entropy: 0.890206.\n",
      "episode: 2381   score: 19.0  epsilon: 1.0    steps: 360  evaluation reward: 19.44\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4726: Policy loss: 0.005911. Value loss: 0.580249. Entropy: 0.823273.\n",
      "Iteration 4727: Policy loss: -0.010661. Value loss: 0.230751. Entropy: 0.827378.\n",
      "Iteration 4728: Policy loss: -0.020432. Value loss: 0.135047. Entropy: 0.818814.\n",
      "episode: 2382   score: 15.0  epsilon: 1.0    steps: 264  evaluation reward: 19.43\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4729: Policy loss: 0.004330. Value loss: 0.242545. Entropy: 0.870609.\n",
      "Iteration 4730: Policy loss: -0.010469. Value loss: 0.083342. Entropy: 0.867818.\n",
      "Iteration 4731: Policy loss: -0.027754. Value loss: 0.050602. Entropy: 0.870646.\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4732: Policy loss: 0.007649. Value loss: 0.377126. Entropy: 0.824633.\n",
      "Iteration 4733: Policy loss: -0.011651. Value loss: 0.179989. Entropy: 0.826736.\n",
      "Iteration 4734: Policy loss: -0.027882. Value loss: 0.101407. Entropy: 0.819252.\n",
      "episode: 2383   score: 35.0  epsilon: 1.0    steps: 848  evaluation reward: 19.68\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4735: Policy loss: 0.015297. Value loss: 0.998448. Entropy: 0.856827.\n",
      "Iteration 4736: Policy loss: -0.006979. Value loss: 0.532837. Entropy: 0.843221.\n",
      "Iteration 4737: Policy loss: -0.013665. Value loss: 0.238574. Entropy: 0.827453.\n",
      "episode: 2384   score: 26.0  epsilon: 1.0    steps: 104  evaluation reward: 19.73\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4738: Policy loss: 0.001427. Value loss: 0.463832. Entropy: 0.836543.\n",
      "Iteration 4739: Policy loss: -0.001093. Value loss: 0.213185. Entropy: 0.859278.\n",
      "Iteration 4740: Policy loss: -0.014902. Value loss: 0.112591. Entropy: 0.850399.\n",
      "episode: 2385   score: 19.0  epsilon: 1.0    steps: 848  evaluation reward: 19.72\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4741: Policy loss: -0.001371. Value loss: 0.428820. Entropy: 0.864928.\n",
      "Iteration 4742: Policy loss: -0.010036. Value loss: 0.130159. Entropy: 0.866242.\n",
      "Iteration 4743: Policy loss: -0.019346. Value loss: 0.070867. Entropy: 0.872222.\n",
      "episode: 2386   score: 14.0  epsilon: 1.0    steps: 120  evaluation reward: 19.67\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4744: Policy loss: 0.011430. Value loss: 0.531544. Entropy: 0.837490.\n",
      "Iteration 4745: Policy loss: -0.006468. Value loss: 0.239888. Entropy: 0.839183.\n",
      "Iteration 4746: Policy loss: -0.011806. Value loss: 0.139211. Entropy: 0.842978.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2387   score: 29.0  epsilon: 1.0    steps: 832  evaluation reward: 19.8\n",
      "episode: 2388   score: 28.0  epsilon: 1.0    steps: 896  evaluation reward: 19.98\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4747: Policy loss: 0.014525. Value loss: 0.740077. Entropy: 0.948974.\n",
      "Iteration 4748: Policy loss: 0.001986. Value loss: 0.252045. Entropy: 0.940068.\n",
      "Iteration 4749: Policy loss: -0.016321. Value loss: 0.160018. Entropy: 0.934889.\n",
      "episode: 2389   score: 21.0  epsilon: 1.0    steps: 288  evaluation reward: 20.11\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4750: Policy loss: 0.011480. Value loss: 0.310633. Entropy: 0.890378.\n",
      "Iteration 4751: Policy loss: -0.011137. Value loss: 0.148513. Entropy: 0.898951.\n",
      "Iteration 4752: Policy loss: -0.024287. Value loss: 0.094054. Entropy: 0.881421.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4753: Policy loss: 0.013418. Value loss: 0.510108. Entropy: 0.868202.\n",
      "Iteration 4754: Policy loss: -0.004395. Value loss: 0.186641. Entropy: 0.842249.\n",
      "Iteration 4755: Policy loss: -0.018957. Value loss: 0.082667. Entropy: 0.857356.\n",
      "episode: 2390   score: 24.0  epsilon: 1.0    steps: 1016  evaluation reward: 20.16\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4756: Policy loss: 0.005413. Value loss: 0.462853. Entropy: 0.946342.\n",
      "Iteration 4757: Policy loss: -0.012436. Value loss: 0.228431. Entropy: 0.946242.\n",
      "Iteration 4758: Policy loss: -0.020060. Value loss: 0.158339. Entropy: 0.942185.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4759: Policy loss: 0.010373. Value loss: 0.656778. Entropy: 0.905607.\n",
      "Iteration 4760: Policy loss: -0.010014. Value loss: 0.298225. Entropy: 0.903144.\n",
      "Iteration 4761: Policy loss: -0.015273. Value loss: 0.167113. Entropy: 0.908190.\n",
      "episode: 2391   score: 23.0  epsilon: 1.0    steps: 648  evaluation reward: 20.24\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4762: Policy loss: 0.007070. Value loss: 0.570130. Entropy: 0.915107.\n",
      "Iteration 4763: Policy loss: -0.014329. Value loss: 0.250642. Entropy: 0.905264.\n",
      "Iteration 4764: Policy loss: -0.023065. Value loss: 0.123240. Entropy: 0.907728.\n",
      "episode: 2392   score: 23.0  epsilon: 1.0    steps: 600  evaluation reward: 20.23\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4765: Policy loss: 0.008442. Value loss: 0.502849. Entropy: 0.946420.\n",
      "Iteration 4766: Policy loss: -0.004621. Value loss: 0.197462. Entropy: 0.930367.\n",
      "Iteration 4767: Policy loss: -0.015746. Value loss: 0.118179. Entropy: 0.941181.\n",
      "episode: 2393   score: 22.0  epsilon: 1.0    steps: 712  evaluation reward: 20.2\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4768: Policy loss: 0.013150. Value loss: 0.814942. Entropy: 0.819231.\n",
      "Iteration 4769: Policy loss: -0.000292. Value loss: 0.320460. Entropy: 0.841904.\n",
      "Iteration 4770: Policy loss: -0.014531. Value loss: 0.184244. Entropy: 0.835782.\n",
      "episode: 2394   score: 8.0  epsilon: 1.0    steps: 344  evaluation reward: 20.12\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4771: Policy loss: 0.010883. Value loss: 0.637690. Entropy: 0.865276.\n",
      "Iteration 4772: Policy loss: -0.003752. Value loss: 0.224217. Entropy: 0.861676.\n",
      "Iteration 4773: Policy loss: -0.018708. Value loss: 0.131966. Entropy: 0.868535.\n",
      "episode: 2395   score: 23.0  epsilon: 1.0    steps: 192  evaluation reward: 20.15\n",
      "episode: 2396   score: 38.0  epsilon: 1.0    steps: 408  evaluation reward: 20.31\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4774: Policy loss: 0.009758. Value loss: 0.357604. Entropy: 0.893528.\n",
      "Iteration 4775: Policy loss: -0.003456. Value loss: 0.151900. Entropy: 0.893990.\n",
      "Iteration 4776: Policy loss: -0.017742. Value loss: 0.066489. Entropy: 0.897744.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4777: Policy loss: 0.010639. Value loss: 0.655306. Entropy: 0.917925.\n",
      "Iteration 4778: Policy loss: -0.009501. Value loss: 0.220208. Entropy: 0.913298.\n",
      "Iteration 4779: Policy loss: -0.015754. Value loss: 0.128833. Entropy: 0.923077.\n",
      "episode: 2397   score: 36.0  epsilon: 1.0    steps: 680  evaluation reward: 20.57\n",
      "episode: 2398   score: 15.0  epsilon: 1.0    steps: 912  evaluation reward: 20.48\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4780: Policy loss: 0.012813. Value loss: 0.359298. Entropy: 0.876155.\n",
      "Iteration 4781: Policy loss: -0.004233. Value loss: 0.102213. Entropy: 0.875276.\n",
      "Iteration 4782: Policy loss: -0.015377. Value loss: 0.065984. Entropy: 0.875724.\n",
      "episode: 2399   score: 30.0  epsilon: 1.0    steps: 336  evaluation reward: 20.57\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4783: Policy loss: 0.013712. Value loss: 0.607765. Entropy: 0.925784.\n",
      "Iteration 4784: Policy loss: -0.006814. Value loss: 0.264716. Entropy: 0.922485.\n",
      "Iteration 4785: Policy loss: -0.016228. Value loss: 0.152186. Entropy: 0.907589.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4786: Policy loss: 0.006611. Value loss: 0.387127. Entropy: 0.870858.\n",
      "Iteration 4787: Policy loss: -0.017628. Value loss: 0.141227. Entropy: 0.877403.\n",
      "Iteration 4788: Policy loss: -0.024337. Value loss: 0.081035. Entropy: 0.878813.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4789: Policy loss: 0.009115. Value loss: 0.532759. Entropy: 0.924549.\n",
      "Iteration 4790: Policy loss: -0.006331. Value loss: 0.227918. Entropy: 0.919894.\n",
      "Iteration 4791: Policy loss: -0.016373. Value loss: 0.140470. Entropy: 0.919691.\n",
      "episode: 2400   score: 29.0  epsilon: 1.0    steps: 472  evaluation reward: 20.56\n",
      "now time :  2019-03-06 14:10:21.583255\n",
      "episode: 2401   score: 17.0  epsilon: 1.0    steps: 848  evaluation reward: 20.54\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4792: Policy loss: 0.008221. Value loss: 0.545201. Entropy: 0.913951.\n",
      "Iteration 4793: Policy loss: -0.009583. Value loss: 0.193482. Entropy: 0.913340.\n",
      "Iteration 4794: Policy loss: -0.023717. Value loss: 0.100362. Entropy: 0.922308.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4795: Policy loss: 0.015720. Value loss: 0.589134. Entropy: 0.963853.\n",
      "Iteration 4796: Policy loss: -0.008108. Value loss: 0.217847. Entropy: 0.955552.\n",
      "Iteration 4797: Policy loss: -0.015322. Value loss: 0.114067. Entropy: 0.958860.\n",
      "episode: 2402   score: 36.0  epsilon: 1.0    steps: 496  evaluation reward: 20.73\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4798: Policy loss: 0.018414. Value loss: 1.294821. Entropy: 0.931401.\n",
      "Iteration 4799: Policy loss: -0.002511. Value loss: 0.655134. Entropy: 0.928121.\n",
      "Iteration 4800: Policy loss: -0.004408. Value loss: 0.401039. Entropy: 0.933869.\n",
      "episode: 2403   score: 29.0  epsilon: 1.0    steps: 792  evaluation reward: 20.79\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4801: Policy loss: 0.009185. Value loss: 0.981160. Entropy: 0.943485.\n",
      "Iteration 4802: Policy loss: -0.004249. Value loss: 0.381892. Entropy: 0.952450.\n",
      "Iteration 4803: Policy loss: -0.015626. Value loss: 0.257664. Entropy: 0.946239.\n",
      "episode: 2404   score: 6.0  epsilon: 1.0    steps: 80  evaluation reward: 20.51\n",
      "episode: 2405   score: 19.0  epsilon: 1.0    steps: 520  evaluation reward: 20.5\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4804: Policy loss: 0.013167. Value loss: 0.761348. Entropy: 0.874401.\n",
      "Iteration 4805: Policy loss: -0.009390. Value loss: 0.304581. Entropy: 0.866647.\n",
      "Iteration 4806: Policy loss: -0.018913. Value loss: 0.174229. Entropy: 0.854064.\n",
      "episode: 2406   score: 44.0  epsilon: 1.0    steps: 472  evaluation reward: 20.84\n",
      "episode: 2407   score: 27.0  epsilon: 1.0    steps: 608  evaluation reward: 21.0\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4807: Policy loss: 0.007217. Value loss: 0.670944. Entropy: 0.840076.\n",
      "Iteration 4808: Policy loss: -0.011609. Value loss: 0.220829. Entropy: 0.841256.\n",
      "Iteration 4809: Policy loss: -0.023516. Value loss: 0.125418. Entropy: 0.837910.\n",
      "episode: 2408   score: 29.0  epsilon: 1.0    steps: 456  evaluation reward: 21.11\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4810: Policy loss: 0.005622. Value loss: 0.608223. Entropy: 0.833374.\n",
      "Iteration 4811: Policy loss: -0.005231. Value loss: 0.227143. Entropy: 0.824518.\n",
      "Iteration 4812: Policy loss: -0.019462. Value loss: 0.154932. Entropy: 0.836847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4813: Policy loss: 0.012455. Value loss: 0.536594. Entropy: 0.886547.\n",
      "Iteration 4814: Policy loss: -0.012988. Value loss: 0.237828. Entropy: 0.874760.\n",
      "Iteration 4815: Policy loss: -0.015027. Value loss: 0.165916. Entropy: 0.871585.\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4816: Policy loss: 0.018296. Value loss: 0.674858. Entropy: 0.932936.\n",
      "Iteration 4817: Policy loss: -0.005751. Value loss: 0.362423. Entropy: 0.945775.\n",
      "Iteration 4818: Policy loss: -0.013202. Value loss: 0.220926. Entropy: 0.950215.\n",
      "episode: 2409   score: 24.0  epsilon: 1.0    steps: 848  evaluation reward: 21.24\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4819: Policy loss: 0.011009. Value loss: 1.331087. Entropy: 0.900307.\n",
      "Iteration 4820: Policy loss: -0.005470. Value loss: 0.554562. Entropy: 0.885604.\n",
      "Iteration 4821: Policy loss: -0.016556. Value loss: 0.335889. Entropy: 0.901334.\n",
      "episode: 2410   score: 36.0  epsilon: 1.0    steps: 48  evaluation reward: 21.49\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4822: Policy loss: 0.011867. Value loss: 0.972613. Entropy: 0.835488.\n",
      "Iteration 4823: Policy loss: -0.000524. Value loss: 0.403591. Entropy: 0.823939.\n",
      "Iteration 4824: Policy loss: -0.011263. Value loss: 0.179221. Entropy: 0.839614.\n",
      "episode: 2411   score: 24.0  epsilon: 1.0    steps: 32  evaluation reward: 21.56\n",
      "episode: 2412   score: 34.0  epsilon: 1.0    steps: 392  evaluation reward: 21.73\n",
      "episode: 2413   score: 22.0  epsilon: 1.0    steps: 416  evaluation reward: 21.86\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4825: Policy loss: 0.010816. Value loss: 0.530874. Entropy: 0.839127.\n",
      "Iteration 4826: Policy loss: 0.002977. Value loss: 0.204251. Entropy: 0.849871.\n",
      "Iteration 4827: Policy loss: -0.014723. Value loss: 0.117605. Entropy: 0.860372.\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4828: Policy loss: 0.012062. Value loss: 0.538030. Entropy: 0.817545.\n",
      "Iteration 4829: Policy loss: -0.006428. Value loss: 0.218426. Entropy: 0.824403.\n",
      "Iteration 4830: Policy loss: -0.023276. Value loss: 0.141474. Entropy: 0.824372.\n",
      "episode: 2414   score: 22.0  epsilon: 1.0    steps: 688  evaluation reward: 21.97\n",
      "episode: 2415   score: 22.0  epsilon: 1.0    steps: 800  evaluation reward: 22.0\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4831: Policy loss: 0.022055. Value loss: 0.604368. Entropy: 0.915858.\n",
      "Iteration 4832: Policy loss: 0.001478. Value loss: 0.241155. Entropy: 0.896078.\n",
      "Iteration 4833: Policy loss: -0.016594. Value loss: 0.155580. Entropy: 0.899705.\n",
      "episode: 2416   score: 19.0  epsilon: 1.0    steps: 680  evaluation reward: 22.02\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4834: Policy loss: 0.004696. Value loss: 0.675255. Entropy: 0.846980.\n",
      "Iteration 4835: Policy loss: -0.009358. Value loss: 0.322677. Entropy: 0.837967.\n",
      "Iteration 4836: Policy loss: -0.019743. Value loss: 0.178224. Entropy: 0.842628.\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4837: Policy loss: 0.009379. Value loss: 0.820554. Entropy: 0.862365.\n",
      "Iteration 4838: Policy loss: -0.007137. Value loss: 0.358875. Entropy: 0.862652.\n",
      "Iteration 4839: Policy loss: -0.016180. Value loss: 0.225859. Entropy: 0.852398.\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4840: Policy loss: 0.012410. Value loss: 0.666506. Entropy: 0.840067.\n",
      "Iteration 4841: Policy loss: -0.009233. Value loss: 0.329576. Entropy: 0.858421.\n",
      "Iteration 4842: Policy loss: -0.016495. Value loss: 0.207790. Entropy: 0.850149.\n",
      "episode: 2417   score: 28.0  epsilon: 1.0    steps: 216  evaluation reward: 22.22\n",
      "episode: 2418   score: 20.0  epsilon: 1.0    steps: 576  evaluation reward: 22.16\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4843: Policy loss: 0.006976. Value loss: 0.889862. Entropy: 0.895801.\n",
      "Iteration 4844: Policy loss: -0.007278. Value loss: 0.370791. Entropy: 0.890140.\n",
      "Iteration 4845: Policy loss: -0.018517. Value loss: 0.209105. Entropy: 0.883495.\n",
      "episode: 2419   score: 18.0  epsilon: 1.0    steps: 432  evaluation reward: 22.18\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4846: Policy loss: 0.011257. Value loss: 0.536752. Entropy: 0.919467.\n",
      "Iteration 4847: Policy loss: -0.003405. Value loss: 0.212600. Entropy: 0.920982.\n",
      "Iteration 4848: Policy loss: -0.017770. Value loss: 0.118672. Entropy: 0.919383.\n",
      "episode: 2420   score: 18.0  epsilon: 1.0    steps: 744  evaluation reward: 22.23\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4849: Policy loss: 0.012365. Value loss: 0.416793. Entropy: 0.887580.\n",
      "Iteration 4850: Policy loss: -0.000294. Value loss: 0.163130. Entropy: 0.884653.\n",
      "Iteration 4851: Policy loss: -0.017140. Value loss: 0.088162. Entropy: 0.896231.\n",
      "episode: 2421   score: 27.0  epsilon: 1.0    steps: 96  evaluation reward: 22.33\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4852: Policy loss: 0.007906. Value loss: 0.676365. Entropy: 0.887582.\n",
      "Iteration 4853: Policy loss: -0.006404. Value loss: 0.366570. Entropy: 0.887270.\n",
      "Iteration 4854: Policy loss: -0.013826. Value loss: 0.230167. Entropy: 0.886484.\n",
      "episode: 2422   score: 26.0  epsilon: 1.0    steps: 408  evaluation reward: 22.35\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4855: Policy loss: 0.006708. Value loss: 0.256742. Entropy: 0.959449.\n",
      "Iteration 4856: Policy loss: -0.016239. Value loss: 0.083641. Entropy: 0.958720.\n",
      "Iteration 4857: Policy loss: -0.029598. Value loss: 0.050056. Entropy: 0.955955.\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4858: Policy loss: 0.014949. Value loss: 0.610158. Entropy: 0.856847.\n",
      "Iteration 4859: Policy loss: -0.005762. Value loss: 0.313260. Entropy: 0.850649.\n",
      "Iteration 4860: Policy loss: -0.014336. Value loss: 0.198344. Entropy: 0.837303.\n",
      "episode: 2423   score: 29.0  epsilon: 1.0    steps: 336  evaluation reward: 22.43\n",
      "episode: 2424   score: 13.0  epsilon: 1.0    steps: 832  evaluation reward: 22.3\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4861: Policy loss: 0.019964. Value loss: 0.672314. Entropy: 0.931309.\n",
      "Iteration 4862: Policy loss: -0.000866. Value loss: 0.262533. Entropy: 0.913851.\n",
      "Iteration 4863: Policy loss: -0.022720. Value loss: 0.130732. Entropy: 0.910209.\n",
      "episode: 2425   score: 26.0  epsilon: 1.0    steps: 312  evaluation reward: 22.29\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4864: Policy loss: 0.001048. Value loss: 0.362941. Entropy: 0.944788.\n",
      "Iteration 4865: Policy loss: -0.018562. Value loss: 0.139584. Entropy: 0.944122.\n",
      "Iteration 4866: Policy loss: -0.028633. Value loss: 0.078585. Entropy: 0.942665.\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4867: Policy loss: 0.010950. Value loss: 0.311736. Entropy: 0.917049.\n",
      "Iteration 4868: Policy loss: -0.014229. Value loss: 0.125813. Entropy: 0.919173.\n",
      "Iteration 4869: Policy loss: -0.025921. Value loss: 0.072354. Entropy: 0.919415.\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4870: Policy loss: 0.005930. Value loss: 0.452581. Entropy: 0.919395.\n",
      "Iteration 4871: Policy loss: -0.015862. Value loss: 0.176697. Entropy: 0.929722.\n",
      "Iteration 4872: Policy loss: -0.023019. Value loss: 0.097284. Entropy: 0.918697.\n",
      "episode: 2426   score: 17.0  epsilon: 1.0    steps: 168  evaluation reward: 22.2\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4873: Policy loss: 0.006171. Value loss: 0.643834. Entropy: 0.901855.\n",
      "Iteration 4874: Policy loss: -0.004100. Value loss: 0.257878. Entropy: 0.896372.\n",
      "Iteration 4875: Policy loss: -0.016508. Value loss: 0.138737. Entropy: 0.906311.\n",
      "episode: 2427   score: 33.0  epsilon: 1.0    steps: 56  evaluation reward: 22.27\n",
      "episode: 2428   score: 25.0  epsilon: 1.0    steps: 216  evaluation reward: 22.41\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4876: Policy loss: 0.013556. Value loss: 0.703621. Entropy: 0.920798.\n",
      "Iteration 4877: Policy loss: -0.001209. Value loss: 0.266686. Entropy: 0.920933.\n",
      "Iteration 4878: Policy loss: -0.014156. Value loss: 0.169068. Entropy: 0.910417.\n",
      "episode: 2429   score: 19.0  epsilon: 1.0    steps: 72  evaluation reward: 22.38\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4879: Policy loss: 0.006432. Value loss: 0.452262. Entropy: 0.902201.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4880: Policy loss: -0.009774. Value loss: 0.132913. Entropy: 0.904496.\n",
      "Iteration 4881: Policy loss: -0.020747. Value loss: 0.079762. Entropy: 0.915468.\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4882: Policy loss: 0.013687. Value loss: 0.493176. Entropy: 0.938467.\n",
      "Iteration 4883: Policy loss: -0.001302. Value loss: 0.202909. Entropy: 0.945216.\n",
      "Iteration 4884: Policy loss: -0.010397. Value loss: 0.097970. Entropy: 0.950008.\n",
      "episode: 2430   score: 37.0  epsilon: 1.0    steps: 80  evaluation reward: 22.53\n",
      "episode: 2431   score: 28.0  epsilon: 1.0    steps: 600  evaluation reward: 22.68\n",
      "episode: 2432   score: 21.0  epsilon: 1.0    steps: 952  evaluation reward: 22.75\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4885: Policy loss: 0.010883. Value loss: 0.369148. Entropy: 0.836403.\n",
      "Iteration 4886: Policy loss: -0.003938. Value loss: 0.132451. Entropy: 0.844248.\n",
      "Iteration 4887: Policy loss: -0.015702. Value loss: 0.085298. Entropy: 0.845090.\n",
      "episode: 2433   score: 24.0  epsilon: 1.0    steps: 680  evaluation reward: 22.78\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4888: Policy loss: 0.013140. Value loss: 0.390590. Entropy: 0.872424.\n",
      "Iteration 4889: Policy loss: -0.004445. Value loss: 0.132246. Entropy: 0.867351.\n",
      "Iteration 4890: Policy loss: -0.023765. Value loss: 0.068736. Entropy: 0.874208.\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4891: Policy loss: 0.003355. Value loss: 0.397946. Entropy: 0.927072.\n",
      "Iteration 4892: Policy loss: -0.018552. Value loss: 0.170112. Entropy: 0.913901.\n",
      "Iteration 4893: Policy loss: -0.026351. Value loss: 0.102931. Entropy: 0.909344.\n",
      "episode: 2434   score: 17.0  epsilon: 1.0    steps: 376  evaluation reward: 22.75\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4894: Policy loss: 0.006266. Value loss: 0.452349. Entropy: 0.934584.\n",
      "Iteration 4895: Policy loss: -0.006205. Value loss: 0.178590. Entropy: 0.934007.\n",
      "Iteration 4896: Policy loss: -0.018008. Value loss: 0.099572. Entropy: 0.935541.\n",
      "episode: 2435   score: 22.0  epsilon: 1.0    steps: 496  evaluation reward: 22.61\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4897: Policy loss: 0.011483. Value loss: 0.408170. Entropy: 0.949258.\n",
      "Iteration 4898: Policy loss: -0.011411. Value loss: 0.150923. Entropy: 0.942387.\n",
      "Iteration 4899: Policy loss: -0.019940. Value loss: 0.080835. Entropy: 0.942454.\n",
      "episode: 2436   score: 22.0  epsilon: 1.0    steps: 176  evaluation reward: 22.6\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4900: Policy loss: 0.008859. Value loss: 0.488890. Entropy: 0.853476.\n",
      "Iteration 4901: Policy loss: -0.010312. Value loss: 0.217236. Entropy: 0.866984.\n",
      "Iteration 4902: Policy loss: -0.022727. Value loss: 0.139062. Entropy: 0.866557.\n",
      "episode: 2437   score: 16.0  epsilon: 1.0    steps: 192  evaluation reward: 22.54\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4903: Policy loss: 0.007113. Value loss: 0.704255. Entropy: 0.861530.\n",
      "Iteration 4904: Policy loss: -0.006719. Value loss: 0.270707. Entropy: 0.851401.\n",
      "Iteration 4905: Policy loss: -0.019806. Value loss: 0.146435. Entropy: 0.865833.\n",
      "episode: 2438   score: 30.0  epsilon: 1.0    steps: 1016  evaluation reward: 22.6\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4906: Policy loss: 0.017486. Value loss: 0.520601. Entropy: 0.898827.\n",
      "Iteration 4907: Policy loss: 0.000707. Value loss: 0.163999. Entropy: 0.914319.\n",
      "Iteration 4908: Policy loss: -0.017758. Value loss: 0.095604. Entropy: 0.923459.\n",
      "episode: 2439   score: 28.0  epsilon: 1.0    steps: 352  evaluation reward: 22.67\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4909: Policy loss: 0.016336. Value loss: 0.570162. Entropy: 0.938149.\n",
      "Iteration 4910: Policy loss: -0.000600. Value loss: 0.252697. Entropy: 0.945594.\n",
      "Iteration 4911: Policy loss: -0.008578. Value loss: 0.143433. Entropy: 0.952531.\n",
      "episode: 2440   score: 25.0  epsilon: 1.0    steps: 328  evaluation reward: 22.78\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4912: Policy loss: 0.014494. Value loss: 0.619900. Entropy: 0.971714.\n",
      "Iteration 4913: Policy loss: -0.009078. Value loss: 0.324059. Entropy: 0.966480.\n",
      "Iteration 4914: Policy loss: -0.016321. Value loss: 0.233603. Entropy: 0.964083.\n",
      "episode: 2441   score: 15.0  epsilon: 1.0    steps: 208  evaluation reward: 22.64\n",
      "episode: 2442   score: 23.0  epsilon: 1.0    steps: 392  evaluation reward: 22.63\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4915: Policy loss: 0.011669. Value loss: 0.351769. Entropy: 0.909099.\n",
      "Iteration 4916: Policy loss: 0.003816. Value loss: 0.121165. Entropy: 0.907764.\n",
      "Iteration 4917: Policy loss: -0.013822. Value loss: 0.075495. Entropy: 0.904319.\n",
      "episode: 2443   score: 13.0  epsilon: 1.0    steps: 160  evaluation reward: 22.51\n",
      "episode: 2444   score: 28.0  epsilon: 1.0    steps: 208  evaluation reward: 22.58\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4918: Policy loss: 0.007428. Value loss: 0.363660. Entropy: 0.886938.\n",
      "Iteration 4919: Policy loss: -0.012172. Value loss: 0.142047. Entropy: 0.872241.\n",
      "Iteration 4920: Policy loss: -0.023988. Value loss: 0.083086. Entropy: 0.876948.\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4921: Policy loss: 0.010884. Value loss: 0.535751. Entropy: 0.950467.\n",
      "Iteration 4922: Policy loss: -0.009804. Value loss: 0.227996. Entropy: 0.954513.\n",
      "Iteration 4923: Policy loss: -0.018755. Value loss: 0.125641. Entropy: 0.966349.\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4924: Policy loss: 0.009871. Value loss: 0.461742. Entropy: 0.828762.\n",
      "Iteration 4925: Policy loss: -0.004524. Value loss: 0.176752. Entropy: 0.821078.\n",
      "Iteration 4926: Policy loss: -0.017316. Value loss: 0.118453. Entropy: 0.846404.\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4927: Policy loss: 0.006894. Value loss: 0.413561. Entropy: 0.915299.\n",
      "Iteration 4928: Policy loss: -0.012003. Value loss: 0.155001. Entropy: 0.902437.\n",
      "Iteration 4929: Policy loss: -0.022562. Value loss: 0.088532. Entropy: 0.921733.\n",
      "episode: 2445   score: 33.0  epsilon: 1.0    steps: 168  evaluation reward: 22.71\n",
      "episode: 2446   score: 19.0  epsilon: 1.0    steps: 320  evaluation reward: 22.69\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4930: Policy loss: 0.006683. Value loss: 0.290258. Entropy: 0.975716.\n",
      "Iteration 4931: Policy loss: -0.013201. Value loss: 0.104085. Entropy: 0.970958.\n",
      "Iteration 4932: Policy loss: -0.027013. Value loss: 0.067281. Entropy: 0.975430.\n",
      "episode: 2447   score: 24.0  epsilon: 1.0    steps: 904  evaluation reward: 22.64\n",
      "episode: 2448   score: 14.0  epsilon: 1.0    steps: 1000  evaluation reward: 22.58\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4933: Policy loss: 0.004101. Value loss: 0.543949. Entropy: 1.002517.\n",
      "Iteration 4934: Policy loss: -0.015251. Value loss: 0.258230. Entropy: 1.004290.\n",
      "Iteration 4935: Policy loss: -0.023249. Value loss: 0.162142. Entropy: 1.004859.\n",
      "episode: 2449   score: 13.0  epsilon: 1.0    steps: 256  evaluation reward: 22.55\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4936: Policy loss: 0.007619. Value loss: 0.363033. Entropy: 0.913215.\n",
      "Iteration 4937: Policy loss: -0.014957. Value loss: 0.097804. Entropy: 0.905631.\n",
      "Iteration 4938: Policy loss: -0.022840. Value loss: 0.059124. Entropy: 0.914367.\n",
      "episode: 2450   score: 20.0  epsilon: 1.0    steps: 56  evaluation reward: 22.59\n",
      "now time :  2019-03-06 14:13:30.226145\n",
      "episode: 2451   score: 22.0  epsilon: 1.0    steps: 928  evaluation reward: 22.54\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4939: Policy loss: 0.000547. Value loss: 0.499383. Entropy: 0.888347.\n",
      "Iteration 4940: Policy loss: -0.010816. Value loss: 0.244033. Entropy: 0.880636.\n",
      "Iteration 4941: Policy loss: -0.020345. Value loss: 0.166322. Entropy: 0.882442.\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4942: Policy loss: 0.013738. Value loss: 0.491128. Entropy: 0.853310.\n",
      "Iteration 4943: Policy loss: -0.001314. Value loss: 0.151694. Entropy: 0.860270.\n",
      "Iteration 4944: Policy loss: -0.007547. Value loss: 0.067319. Entropy: 0.870121.\n",
      "episode: 2452   score: 33.0  epsilon: 1.0    steps: 432  evaluation reward: 22.63\n",
      "Training network. lr: 0.000212. clip: 0.084950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4945: Policy loss: 0.004801. Value loss: 0.477018. Entropy: 0.927451.\n",
      "Iteration 4946: Policy loss: -0.010319. Value loss: 0.203131. Entropy: 0.927259.\n",
      "Iteration 4947: Policy loss: -0.020195. Value loss: 0.103866. Entropy: 0.918910.\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4948: Policy loss: 0.007434. Value loss: 0.402222. Entropy: 0.948068.\n",
      "Iteration 4949: Policy loss: -0.005260. Value loss: 0.134700. Entropy: 0.949468.\n",
      "Iteration 4950: Policy loss: -0.017570. Value loss: 0.091647. Entropy: 0.958982.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4951: Policy loss: 0.000360. Value loss: 0.683384. Entropy: 0.965307.\n",
      "Iteration 4952: Policy loss: -0.013070. Value loss: 0.231339. Entropy: 0.955259.\n",
      "Iteration 4953: Policy loss: -0.022617. Value loss: 0.102558. Entropy: 0.960890.\n",
      "episode: 2453   score: 22.0  epsilon: 1.0    steps: 16  evaluation reward: 22.72\n",
      "episode: 2454   score: 22.0  epsilon: 1.0    steps: 320  evaluation reward: 22.81\n",
      "episode: 2455   score: 18.0  epsilon: 1.0    steps: 944  evaluation reward: 22.7\n",
      "episode: 2456   score: 29.0  epsilon: 1.0    steps: 1016  evaluation reward: 22.74\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4954: Policy loss: 0.008399. Value loss: 0.542551. Entropy: 0.921694.\n",
      "Iteration 4955: Policy loss: -0.011376. Value loss: 0.222683. Entropy: 0.923939.\n",
      "Iteration 4956: Policy loss: -0.021485. Value loss: 0.142815. Entropy: 0.926996.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4957: Policy loss: 0.012710. Value loss: 0.263116. Entropy: 0.900079.\n",
      "Iteration 4958: Policy loss: -0.014649. Value loss: 0.086331. Entropy: 0.892962.\n",
      "Iteration 4959: Policy loss: -0.027769. Value loss: 0.052728. Entropy: 0.896508.\n",
      "episode: 2457   score: 19.0  epsilon: 1.0    steps: 968  evaluation reward: 22.71\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4960: Policy loss: 0.002169. Value loss: 0.521287. Entropy: 0.896931.\n",
      "Iteration 4961: Policy loss: -0.012729. Value loss: 0.192141. Entropy: 0.890497.\n",
      "Iteration 4962: Policy loss: -0.023327. Value loss: 0.135699. Entropy: 0.880327.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4963: Policy loss: 0.012845. Value loss: 0.590567. Entropy: 0.967809.\n",
      "Iteration 4964: Policy loss: -0.005559. Value loss: 0.297167. Entropy: 0.958683.\n",
      "Iteration 4965: Policy loss: -0.022943. Value loss: 0.179180. Entropy: 0.962750.\n",
      "episode: 2458   score: 35.0  epsilon: 1.0    steps: 680  evaluation reward: 22.83\n",
      "episode: 2459   score: 22.0  epsilon: 1.0    steps: 744  evaluation reward: 22.89\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4966: Policy loss: 0.008877. Value loss: 0.894167. Entropy: 0.951845.\n",
      "Iteration 4967: Policy loss: -0.006712. Value loss: 0.367363. Entropy: 0.938973.\n",
      "Iteration 4968: Policy loss: -0.015627. Value loss: 0.202783. Entropy: 0.951846.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4969: Policy loss: 0.002297. Value loss: 0.510490. Entropy: 0.872369.\n",
      "Iteration 4970: Policy loss: -0.006037. Value loss: 0.189282. Entropy: 0.876661.\n",
      "Iteration 4971: Policy loss: -0.015957. Value loss: 0.109797. Entropy: 0.867155.\n",
      "episode: 2460   score: 27.0  epsilon: 1.0    steps: 56  evaluation reward: 23.02\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4972: Policy loss: 0.004528. Value loss: 0.557987. Entropy: 0.883703.\n",
      "Iteration 4973: Policy loss: -0.012136. Value loss: 0.272732. Entropy: 0.887830.\n",
      "Iteration 4974: Policy loss: -0.014867. Value loss: 0.163707. Entropy: 0.874342.\n",
      "episode: 2461   score: 15.0  epsilon: 1.0    steps: 608  evaluation reward: 22.88\n",
      "episode: 2462   score: 19.0  epsilon: 1.0    steps: 760  evaluation reward: 22.95\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4975: Policy loss: 0.016795. Value loss: 0.513496. Entropy: 0.990103.\n",
      "Iteration 4976: Policy loss: -0.002676. Value loss: 0.226607. Entropy: 0.969505.\n",
      "Iteration 4977: Policy loss: -0.021576. Value loss: 0.119432. Entropy: 0.974520.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4978: Policy loss: 0.017621. Value loss: 0.560804. Entropy: 0.853815.\n",
      "Iteration 4979: Policy loss: -0.002736. Value loss: 0.191901. Entropy: 0.855012.\n",
      "Iteration 4980: Policy loss: -0.019029. Value loss: 0.101155. Entropy: 0.860214.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4981: Policy loss: 0.016347. Value loss: 0.747374. Entropy: 0.917873.\n",
      "Iteration 4982: Policy loss: -0.013147. Value loss: 0.298536. Entropy: 0.913695.\n",
      "Iteration 4983: Policy loss: -0.021606. Value loss: 0.158378. Entropy: 0.921021.\n",
      "episode: 2463   score: 30.0  epsilon: 1.0    steps: 528  evaluation reward: 23.02\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4984: Policy loss: 0.015804. Value loss: 0.703527. Entropy: 0.922058.\n",
      "Iteration 4985: Policy loss: -0.005604. Value loss: 0.289321. Entropy: 0.900963.\n",
      "Iteration 4986: Policy loss: -0.015245. Value loss: 0.134881. Entropy: 0.907289.\n",
      "episode: 2464   score: 39.0  epsilon: 1.0    steps: 544  evaluation reward: 23.26\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4987: Policy loss: 0.010048. Value loss: 1.204991. Entropy: 0.924147.\n",
      "Iteration 4988: Policy loss: -0.002227. Value loss: 0.513483. Entropy: 0.914363.\n",
      "Iteration 4989: Policy loss: -0.018435. Value loss: 0.282806. Entropy: 0.921274.\n",
      "episode: 2465   score: 17.0  epsilon: 1.0    steps: 144  evaluation reward: 23.16\n",
      "episode: 2466   score: 11.0  epsilon: 1.0    steps: 760  evaluation reward: 23.17\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4990: Policy loss: 0.016344. Value loss: 0.637885. Entropy: 0.876985.\n",
      "Iteration 4991: Policy loss: -0.006306. Value loss: 0.304323. Entropy: 0.868205.\n",
      "Iteration 4992: Policy loss: -0.017995. Value loss: 0.158166. Entropy: 0.863404.\n",
      "episode: 2467   score: 35.0  epsilon: 1.0    steps: 352  evaluation reward: 23.33\n",
      "episode: 2468   score: 30.0  epsilon: 1.0    steps: 472  evaluation reward: 23.42\n",
      "episode: 2469   score: 34.0  epsilon: 1.0    steps: 600  evaluation reward: 23.58\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4993: Policy loss: 0.008929. Value loss: 0.409687. Entropy: 0.903704.\n",
      "Iteration 4994: Policy loss: -0.007688. Value loss: 0.150103. Entropy: 0.908112.\n",
      "Iteration 4995: Policy loss: -0.016727. Value loss: 0.081810. Entropy: 0.908644.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4996: Policy loss: 0.008926. Value loss: 0.599755. Entropy: 0.889922.\n",
      "Iteration 4997: Policy loss: -0.009957. Value loss: 0.271390. Entropy: 0.896911.\n",
      "Iteration 4998: Policy loss: -0.018945. Value loss: 0.177143. Entropy: 0.898882.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4999: Policy loss: 0.010705. Value loss: 0.408434. Entropy: 0.922044.\n",
      "Iteration 5000: Policy loss: -0.004725. Value loss: 0.175177. Entropy: 0.912577.\n",
      "Iteration 5001: Policy loss: -0.022263. Value loss: 0.077114. Entropy: 0.919016.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5002: Policy loss: 0.006103. Value loss: 0.669488. Entropy: 0.902753.\n",
      "Iteration 5003: Policy loss: -0.003262. Value loss: 0.330194. Entropy: 0.889416.\n",
      "Iteration 5004: Policy loss: -0.014781. Value loss: 0.205139. Entropy: 0.883900.\n",
      "episode: 2470   score: 23.0  epsilon: 1.0    steps: 976  evaluation reward: 23.75\n",
      "episode: 2471   score: 29.0  epsilon: 1.0    steps: 992  evaluation reward: 23.79\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5005: Policy loss: 0.007512. Value loss: 0.895939. Entropy: 0.947144.\n",
      "Iteration 5006: Policy loss: -0.008680. Value loss: 0.395269. Entropy: 0.952702.\n",
      "Iteration 5007: Policy loss: -0.016773. Value loss: 0.270207. Entropy: 0.927352.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5008: Policy loss: 0.008782. Value loss: 0.814355. Entropy: 0.907198.\n",
      "Iteration 5009: Policy loss: -0.002256. Value loss: 0.377629. Entropy: 0.918077.\n",
      "Iteration 5010: Policy loss: -0.018668. Value loss: 0.196639. Entropy: 0.898286.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5011: Policy loss: 0.015435. Value loss: 0.965096. Entropy: 0.940744.\n",
      "Iteration 5012: Policy loss: -0.007430. Value loss: 0.399341. Entropy: 0.921418.\n",
      "Iteration 5013: Policy loss: -0.014421. Value loss: 0.194608. Entropy: 0.923051.\n",
      "episode: 2472   score: 39.0  epsilon: 1.0    steps: 792  evaluation reward: 23.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2473   score: 22.0  epsilon: 1.0    steps: 992  evaluation reward: 24.01\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5014: Policy loss: 0.013622. Value loss: 0.578495. Entropy: 0.950253.\n",
      "Iteration 5015: Policy loss: -0.011383. Value loss: 0.238840. Entropy: 0.945951.\n",
      "Iteration 5016: Policy loss: -0.023669. Value loss: 0.136202. Entropy: 0.958443.\n",
      "episode: 2474   score: 27.0  epsilon: 1.0    steps: 792  evaluation reward: 24.04\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5017: Policy loss: 0.012934. Value loss: 0.778497. Entropy: 1.020627.\n",
      "Iteration 5018: Policy loss: -0.004132. Value loss: 0.383792. Entropy: 1.013403.\n",
      "Iteration 5019: Policy loss: -0.016288. Value loss: 0.237718. Entropy: 1.017810.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5020: Policy loss: 0.012203. Value loss: 0.448582. Entropy: 0.953318.\n",
      "Iteration 5021: Policy loss: -0.003933. Value loss: 0.178040. Entropy: 0.932919.\n",
      "Iteration 5022: Policy loss: -0.019553. Value loss: 0.105452. Entropy: 0.943028.\n",
      "episode: 2475   score: 31.0  epsilon: 1.0    steps: 88  evaluation reward: 24.24\n",
      "episode: 2476   score: 27.0  epsilon: 1.0    steps: 184  evaluation reward: 24.28\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5023: Policy loss: 0.030527. Value loss: 0.759843. Entropy: 0.926789.\n",
      "Iteration 5024: Policy loss: -0.007625. Value loss: 0.360559. Entropy: 0.930637.\n",
      "Iteration 5025: Policy loss: -0.018357. Value loss: 0.217633. Entropy: 0.935310.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5026: Policy loss: 0.015122. Value loss: 0.514698. Entropy: 0.905598.\n",
      "Iteration 5027: Policy loss: -0.007044. Value loss: 0.217510. Entropy: 0.895811.\n",
      "Iteration 5028: Policy loss: -0.016682. Value loss: 0.103560. Entropy: 0.894786.\n",
      "episode: 2477   score: 42.0  epsilon: 1.0    steps: 872  evaluation reward: 24.54\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5029: Policy loss: 0.015963. Value loss: 0.835924. Entropy: 0.940871.\n",
      "Iteration 5030: Policy loss: -0.002791. Value loss: 0.353282. Entropy: 0.928826.\n",
      "Iteration 5031: Policy loss: -0.012954. Value loss: 0.203678. Entropy: 0.927312.\n",
      "episode: 2478   score: 25.0  epsilon: 1.0    steps: 168  evaluation reward: 24.51\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5032: Policy loss: 0.004966. Value loss: 0.494285. Entropy: 0.899806.\n",
      "Iteration 5033: Policy loss: -0.005153. Value loss: 0.179519. Entropy: 0.904099.\n",
      "Iteration 5034: Policy loss: -0.014560. Value loss: 0.080217. Entropy: 0.910047.\n",
      "episode: 2479   score: 26.0  epsilon: 1.0    steps: 288  evaluation reward: 24.6\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5035: Policy loss: 0.005887. Value loss: 0.595498. Entropy: 0.952934.\n",
      "Iteration 5036: Policy loss: -0.010421. Value loss: 0.273596. Entropy: 0.951570.\n",
      "Iteration 5037: Policy loss: -0.022412. Value loss: 0.171655. Entropy: 0.949154.\n",
      "episode: 2480   score: 13.0  epsilon: 1.0    steps: 808  evaluation reward: 24.41\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5038: Policy loss: 0.025706. Value loss: 0.669897. Entropy: 0.999380.\n",
      "Iteration 5039: Policy loss: -0.002778. Value loss: 0.309214. Entropy: 1.018884.\n",
      "Iteration 5040: Policy loss: -0.018193. Value loss: 0.159943. Entropy: 1.010060.\n",
      "episode: 2481   score: 20.0  epsilon: 1.0    steps: 48  evaluation reward: 24.42\n",
      "episode: 2482   score: 27.0  epsilon: 1.0    steps: 512  evaluation reward: 24.54\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5041: Policy loss: 0.012592. Value loss: 0.522902. Entropy: 0.917322.\n",
      "Iteration 5042: Policy loss: 0.001025. Value loss: 0.166008. Entropy: 0.909724.\n",
      "Iteration 5043: Policy loss: -0.015968. Value loss: 0.084038. Entropy: 0.911739.\n",
      "episode: 2483   score: 23.0  epsilon: 1.0    steps: 712  evaluation reward: 24.42\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5044: Policy loss: 0.013334. Value loss: 0.663168. Entropy: 0.898227.\n",
      "Iteration 5045: Policy loss: -0.010012. Value loss: 0.262983. Entropy: 0.887872.\n",
      "Iteration 5046: Policy loss: -0.016317. Value loss: 0.141933. Entropy: 0.886298.\n",
      "episode: 2484   score: 24.0  epsilon: 1.0    steps: 80  evaluation reward: 24.4\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5047: Policy loss: 0.012719. Value loss: 0.227383. Entropy: 0.914353.\n",
      "Iteration 5048: Policy loss: -0.010786. Value loss: 0.097189. Entropy: 0.911865.\n",
      "Iteration 5049: Policy loss: -0.023147. Value loss: 0.054065. Entropy: 0.902201.\n",
      "episode: 2485   score: 11.0  epsilon: 1.0    steps: 640  evaluation reward: 24.32\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5050: Policy loss: 0.017975. Value loss: 0.598886. Entropy: 0.955601.\n",
      "Iteration 5051: Policy loss: -0.004001. Value loss: 0.260276. Entropy: 0.930893.\n",
      "Iteration 5052: Policy loss: -0.012332. Value loss: 0.154377. Entropy: 0.926829.\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5053: Policy loss: 0.013445. Value loss: 0.722664. Entropy: 0.954184.\n",
      "Iteration 5054: Policy loss: -0.005946. Value loss: 0.336252. Entropy: 0.947059.\n",
      "Iteration 5055: Policy loss: -0.011615. Value loss: 0.207662. Entropy: 0.943317.\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5056: Policy loss: 0.012235. Value loss: 0.409794. Entropy: 0.975532.\n",
      "Iteration 5057: Policy loss: -0.005556. Value loss: 0.136805. Entropy: 0.960476.\n",
      "Iteration 5058: Policy loss: -0.023521. Value loss: 0.066812. Entropy: 0.973507.\n",
      "episode: 2486   score: 37.0  epsilon: 1.0    steps: 128  evaluation reward: 24.55\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5059: Policy loss: 0.016434. Value loss: 0.611219. Entropy: 0.948309.\n",
      "Iteration 5060: Policy loss: -0.000101. Value loss: 0.242414. Entropy: 0.960944.\n",
      "Iteration 5061: Policy loss: -0.023375. Value loss: 0.128729. Entropy: 0.952468.\n",
      "episode: 2487   score: 23.0  epsilon: 1.0    steps: 752  evaluation reward: 24.49\n",
      "episode: 2488   score: 28.0  epsilon: 1.0    steps: 856  evaluation reward: 24.49\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5062: Policy loss: 0.011992. Value loss: 0.743602. Entropy: 0.968580.\n",
      "Iteration 5063: Policy loss: -0.002013. Value loss: 0.304231. Entropy: 0.963168.\n",
      "Iteration 5064: Policy loss: -0.014483. Value loss: 0.173075. Entropy: 0.959606.\n",
      "episode: 2489   score: 22.0  epsilon: 1.0    steps: 48  evaluation reward: 24.5\n",
      "episode: 2490   score: 37.0  epsilon: 1.0    steps: 64  evaluation reward: 24.63\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5065: Policy loss: 0.010501. Value loss: 0.641496. Entropy: 0.921254.\n",
      "Iteration 5066: Policy loss: -0.003616. Value loss: 0.320440. Entropy: 0.940841.\n",
      "Iteration 5067: Policy loss: -0.018554. Value loss: 0.203875. Entropy: 0.924717.\n",
      "episode: 2491   score: 25.0  epsilon: 1.0    steps: 736  evaluation reward: 24.65\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5068: Policy loss: 0.014418. Value loss: 0.721351. Entropy: 0.962442.\n",
      "Iteration 5069: Policy loss: -0.003387. Value loss: 0.290685. Entropy: 0.931228.\n",
      "Iteration 5070: Policy loss: -0.017279. Value loss: 0.167617. Entropy: 0.932030.\n",
      "episode: 2492   score: 17.0  epsilon: 1.0    steps: 680  evaluation reward: 24.59\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5071: Policy loss: 0.008637. Value loss: 0.713335. Entropy: 0.953138.\n",
      "Iteration 5072: Policy loss: -0.004079. Value loss: 0.323961. Entropy: 0.964505.\n",
      "Iteration 5073: Policy loss: -0.015870. Value loss: 0.213143. Entropy: 0.940257.\n",
      "episode: 2493   score: 7.0  epsilon: 1.0    steps: 888  evaluation reward: 24.44\n",
      "episode: 2494   score: 29.0  epsilon: 1.0    steps: 992  evaluation reward: 24.65\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5074: Policy loss: 0.017572. Value loss: 0.625355. Entropy: 0.963904.\n",
      "Iteration 5075: Policy loss: -0.007602. Value loss: 0.271126. Entropy: 0.958783.\n",
      "Iteration 5076: Policy loss: -0.017853. Value loss: 0.158683. Entropy: 0.954036.\n",
      "episode: 2495   score: 16.0  epsilon: 1.0    steps: 472  evaluation reward: 24.58\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5077: Policy loss: 0.013252. Value loss: 0.333157. Entropy: 0.877376.\n",
      "Iteration 5078: Policy loss: -0.005629. Value loss: 0.147612. Entropy: 0.855452.\n",
      "Iteration 5079: Policy loss: -0.018002. Value loss: 0.071808. Entropy: 0.871981.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5080: Policy loss: 0.010568. Value loss: 0.276320. Entropy: 0.901581.\n",
      "Iteration 5081: Policy loss: -0.009292. Value loss: 0.126499. Entropy: 0.903291.\n",
      "Iteration 5082: Policy loss: -0.021821. Value loss: 0.071114. Entropy: 0.903928.\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5083: Policy loss: 0.012204. Value loss: 0.735614. Entropy: 0.870684.\n",
      "Iteration 5084: Policy loss: 0.001816. Value loss: 0.266888. Entropy: 0.857053.\n",
      "Iteration 5085: Policy loss: -0.014442. Value loss: 0.134683. Entropy: 0.858270.\n",
      "episode: 2496   score: 26.0  epsilon: 1.0    steps: 32  evaluation reward: 24.46\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5086: Policy loss: 0.008438. Value loss: 0.511289. Entropy: 0.982765.\n",
      "Iteration 5087: Policy loss: -0.008395. Value loss: 0.201408. Entropy: 0.977367.\n",
      "Iteration 5088: Policy loss: -0.014564. Value loss: 0.112327. Entropy: 0.978575.\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5089: Policy loss: 0.012428. Value loss: 0.593013. Entropy: 0.929418.\n",
      "Iteration 5090: Policy loss: -0.007119. Value loss: 0.205320. Entropy: 0.915973.\n",
      "Iteration 5091: Policy loss: -0.018423. Value loss: 0.088753. Entropy: 0.926804.\n",
      "episode: 2497   score: 28.0  epsilon: 1.0    steps: 608  evaluation reward: 24.38\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5092: Policy loss: 0.015972. Value loss: 0.900011. Entropy: 1.021438.\n",
      "Iteration 5093: Policy loss: 0.004609. Value loss: 0.353771. Entropy: 1.011356.\n",
      "Iteration 5094: Policy loss: -0.015039. Value loss: 0.191274. Entropy: 1.012079.\n",
      "episode: 2498   score: 29.0  epsilon: 1.0    steps: 64  evaluation reward: 24.52\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5095: Policy loss: 0.011493. Value loss: 0.448818. Entropy: 0.957837.\n",
      "Iteration 5096: Policy loss: -0.004024. Value loss: 0.162710. Entropy: 0.985956.\n",
      "Iteration 5097: Policy loss: -0.017057. Value loss: 0.089480. Entropy: 0.975085.\n",
      "episode: 2499   score: 26.0  epsilon: 1.0    steps: 120  evaluation reward: 24.48\n",
      "episode: 2500   score: 16.0  epsilon: 1.0    steps: 416  evaluation reward: 24.35\n",
      "now time :  2019-03-06 14:16:52.994317\n",
      "episode: 2501   score: 13.0  epsilon: 1.0    steps: 944  evaluation reward: 24.31\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5098: Policy loss: 0.017569. Value loss: 0.691342. Entropy: 0.987853.\n",
      "Iteration 5099: Policy loss: -0.002548. Value loss: 0.270397. Entropy: 0.986386.\n",
      "Iteration 5100: Policy loss: -0.012823. Value loss: 0.150979. Entropy: 0.970040.\n",
      "episode: 2502   score: 31.0  epsilon: 1.0    steps: 1000  evaluation reward: 24.26\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5101: Policy loss: 0.015770. Value loss: 0.840040. Entropy: 0.979186.\n",
      "Iteration 5102: Policy loss: 0.004786. Value loss: 0.350560. Entropy: 0.963501.\n",
      "Iteration 5103: Policy loss: -0.011968. Value loss: 0.229378. Entropy: 0.968078.\n",
      "episode: 2503   score: 33.0  epsilon: 1.0    steps: 144  evaluation reward: 24.3\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5104: Policy loss: 0.009405. Value loss: 0.511937. Entropy: 0.895964.\n",
      "Iteration 5105: Policy loss: -0.006943. Value loss: 0.239110. Entropy: 0.868607.\n",
      "Iteration 5106: Policy loss: -0.020153. Value loss: 0.155086. Entropy: 0.867193.\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5107: Policy loss: 0.010909. Value loss: 0.464328. Entropy: 0.922880.\n",
      "Iteration 5108: Policy loss: -0.009381. Value loss: 0.238917. Entropy: 0.912775.\n",
      "Iteration 5109: Policy loss: -0.022879. Value loss: 0.166822. Entropy: 0.919327.\n",
      "episode: 2504   score: 33.0  epsilon: 1.0    steps: 928  evaluation reward: 24.57\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5110: Policy loss: 0.007919. Value loss: 0.605756. Entropy: 0.890664.\n",
      "Iteration 5111: Policy loss: -0.009032. Value loss: 0.215018. Entropy: 0.898280.\n",
      "Iteration 5112: Policy loss: -0.016532. Value loss: 0.101458. Entropy: 0.889392.\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5113: Policy loss: 0.016173. Value loss: 0.693627. Entropy: 0.931859.\n",
      "Iteration 5114: Policy loss: 0.000860. Value loss: 0.292682. Entropy: 0.931335.\n",
      "Iteration 5115: Policy loss: -0.009127. Value loss: 0.192289. Entropy: 0.930395.\n",
      "episode: 2505   score: 17.0  epsilon: 1.0    steps: 24  evaluation reward: 24.55\n",
      "episode: 2506   score: 15.0  epsilon: 1.0    steps: 448  evaluation reward: 24.26\n",
      "episode: 2507   score: 19.0  epsilon: 1.0    steps: 888  evaluation reward: 24.18\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5116: Policy loss: 0.011846. Value loss: 0.668504. Entropy: 0.898075.\n",
      "Iteration 5117: Policy loss: -0.007009. Value loss: 0.333049. Entropy: 0.891338.\n",
      "Iteration 5118: Policy loss: -0.020785. Value loss: 0.176491. Entropy: 0.884861.\n",
      "episode: 2508   score: 17.0  epsilon: 1.0    steps: 264  evaluation reward: 24.06\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5119: Policy loss: 0.015634. Value loss: 0.479859. Entropy: 0.899848.\n",
      "Iteration 5120: Policy loss: -0.002889. Value loss: 0.214510. Entropy: 0.897195.\n",
      "Iteration 5121: Policy loss: -0.015872. Value loss: 0.117498. Entropy: 0.911760.\n",
      "episode: 2509   score: 26.0  epsilon: 1.0    steps: 440  evaluation reward: 24.08\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5122: Policy loss: 0.008330. Value loss: 0.383341. Entropy: 0.874391.\n",
      "Iteration 5123: Policy loss: -0.014072. Value loss: 0.196573. Entropy: 0.872640.\n",
      "Iteration 5124: Policy loss: -0.022572. Value loss: 0.134817. Entropy: 0.886545.\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5125: Policy loss: 0.014321. Value loss: 0.309141. Entropy: 0.919017.\n",
      "Iteration 5126: Policy loss: -0.007220. Value loss: 0.131383. Entropy: 0.926550.\n",
      "Iteration 5127: Policy loss: -0.018117. Value loss: 0.070360. Entropy: 0.916202.\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5128: Policy loss: 0.008699. Value loss: 0.302514. Entropy: 0.922593.\n",
      "Iteration 5129: Policy loss: -0.003096. Value loss: 0.100622. Entropy: 0.922701.\n",
      "Iteration 5130: Policy loss: -0.015996. Value loss: 0.054749. Entropy: 0.936406.\n",
      "episode: 2510   score: 22.0  epsilon: 1.0    steps: 24  evaluation reward: 23.94\n",
      "episode: 2511   score: 34.0  epsilon: 1.0    steps: 584  evaluation reward: 24.04\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5131: Policy loss: 0.007764. Value loss: 0.270838. Entropy: 0.970931.\n",
      "Iteration 5132: Policy loss: -0.014209. Value loss: 0.125905. Entropy: 0.955840.\n",
      "Iteration 5133: Policy loss: -0.019865. Value loss: 0.073984. Entropy: 0.953603.\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5134: Policy loss: 0.010036. Value loss: 0.359737. Entropy: 0.958446.\n",
      "Iteration 5135: Policy loss: -0.006896. Value loss: 0.162594. Entropy: 0.940041.\n",
      "Iteration 5136: Policy loss: -0.019505. Value loss: 0.091099. Entropy: 0.940331.\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5137: Policy loss: 0.007617. Value loss: 0.420117. Entropy: 0.949922.\n",
      "Iteration 5138: Policy loss: -0.009069. Value loss: 0.174727. Entropy: 0.954504.\n",
      "Iteration 5139: Policy loss: -0.017782. Value loss: 0.117044. Entropy: 0.952220.\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5140: Policy loss: 0.011989. Value loss: 0.730606. Entropy: 0.930167.\n",
      "Iteration 5141: Policy loss: -0.006853. Value loss: 0.265608. Entropy: 0.927669.\n",
      "Iteration 5142: Policy loss: -0.014553. Value loss: 0.146393. Entropy: 0.936668.\n",
      "episode: 2512   score: 26.0  epsilon: 1.0    steps: 72  evaluation reward: 23.96\n",
      "episode: 2513   score: 23.0  epsilon: 1.0    steps: 720  evaluation reward: 23.97\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5143: Policy loss: 0.017541. Value loss: 1.128434. Entropy: 0.966170.\n",
      "Iteration 5144: Policy loss: 0.002037. Value loss: 0.449740. Entropy: 0.954813.\n",
      "Iteration 5145: Policy loss: -0.012784. Value loss: 0.242648. Entropy: 0.957866.\n",
      "episode: 2514   score: 23.0  epsilon: 1.0    steps: 224  evaluation reward: 23.98\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5146: Policy loss: 0.005187. Value loss: 0.602149. Entropy: 0.991437.\n",
      "Iteration 5147: Policy loss: -0.009982. Value loss: 0.269953. Entropy: 0.978098.\n",
      "Iteration 5148: Policy loss: -0.019310. Value loss: 0.139090. Entropy: 0.989338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2515   score: 38.0  epsilon: 1.0    steps: 504  evaluation reward: 24.14\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5149: Policy loss: 0.014005. Value loss: 0.514133. Entropy: 0.998105.\n",
      "Iteration 5150: Policy loss: -0.008332. Value loss: 0.206620. Entropy: 0.998261.\n",
      "Iteration 5151: Policy loss: -0.021769. Value loss: 0.114699. Entropy: 0.995289.\n",
      "episode: 2516   score: 13.0  epsilon: 1.0    steps: 184  evaluation reward: 24.08\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5152: Policy loss: 0.013442. Value loss: 0.977485. Entropy: 0.942061.\n",
      "Iteration 5153: Policy loss: -0.000415. Value loss: 0.369187. Entropy: 0.932627.\n",
      "Iteration 5154: Policy loss: -0.008708. Value loss: 0.202557. Entropy: 0.933440.\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5155: Policy loss: 0.011656. Value loss: 0.715536. Entropy: 0.916498.\n",
      "Iteration 5156: Policy loss: 0.000893. Value loss: 0.320552. Entropy: 0.909932.\n",
      "Iteration 5157: Policy loss: -0.006414. Value loss: 0.162860. Entropy: 0.902536.\n",
      "episode: 2517   score: 39.0  epsilon: 1.0    steps: 448  evaluation reward: 24.19\n",
      "episode: 2518   score: 48.0  epsilon: 1.0    steps: 488  evaluation reward: 24.47\n",
      "episode: 2519   score: 17.0  epsilon: 1.0    steps: 936  evaluation reward: 24.46\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5158: Policy loss: 0.009082. Value loss: 0.992139. Entropy: 0.979027.\n",
      "Iteration 5159: Policy loss: -0.003165. Value loss: 0.349455. Entropy: 0.962142.\n",
      "Iteration 5160: Policy loss: -0.012857. Value loss: 0.187166. Entropy: 0.972570.\n",
      "episode: 2520   score: 13.0  epsilon: 1.0    steps: 192  evaluation reward: 24.41\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5161: Policy loss: 0.004837. Value loss: 0.398963. Entropy: 0.928455.\n",
      "Iteration 5162: Policy loss: -0.014422. Value loss: 0.180402. Entropy: 0.942489.\n",
      "Iteration 5163: Policy loss: -0.023985. Value loss: 0.106266. Entropy: 0.938707.\n",
      "episode: 2521   score: 36.0  epsilon: 1.0    steps: 376  evaluation reward: 24.5\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5164: Policy loss: 0.008835. Value loss: 0.559174. Entropy: 0.917661.\n",
      "Iteration 5165: Policy loss: -0.011964. Value loss: 0.204073. Entropy: 0.917323.\n",
      "Iteration 5166: Policy loss: -0.019689. Value loss: 0.093513. Entropy: 0.927218.\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5167: Policy loss: 0.015222. Value loss: 0.689296. Entropy: 0.972366.\n",
      "Iteration 5168: Policy loss: 0.001287. Value loss: 0.311975. Entropy: 0.977929.\n",
      "Iteration 5169: Policy loss: -0.014831. Value loss: 0.184463. Entropy: 0.965801.\n",
      "episode: 2522   score: 32.0  epsilon: 1.0    steps: 368  evaluation reward: 24.56\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5170: Policy loss: 0.012937. Value loss: 0.700612. Entropy: 1.007145.\n",
      "Iteration 5171: Policy loss: -0.009160. Value loss: 0.318995. Entropy: 1.009074.\n",
      "Iteration 5172: Policy loss: -0.016479. Value loss: 0.188400. Entropy: 1.006028.\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5173: Policy loss: 0.008982. Value loss: 0.610196. Entropy: 1.022573.\n",
      "Iteration 5174: Policy loss: -0.001462. Value loss: 0.247952. Entropy: 1.013889.\n",
      "Iteration 5175: Policy loss: -0.019872. Value loss: 0.151265. Entropy: 1.011705.\n",
      "episode: 2523   score: 28.0  epsilon: 1.0    steps: 8  evaluation reward: 24.55\n",
      "episode: 2524   score: 24.0  epsilon: 1.0    steps: 264  evaluation reward: 24.66\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5176: Policy loss: 0.012160. Value loss: 0.366274. Entropy: 0.975689.\n",
      "Iteration 5177: Policy loss: -0.005082. Value loss: 0.138001. Entropy: 0.961791.\n",
      "Iteration 5178: Policy loss: -0.021064. Value loss: 0.070518. Entropy: 0.968678.\n",
      "episode: 2525   score: 14.0  epsilon: 1.0    steps: 536  evaluation reward: 24.54\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5179: Policy loss: 0.015469. Value loss: 0.873755. Entropy: 0.961515.\n",
      "Iteration 5180: Policy loss: -0.011339. Value loss: 0.316385. Entropy: 0.953171.\n",
      "Iteration 5181: Policy loss: -0.021087. Value loss: 0.167598. Entropy: 0.958815.\n",
      "episode: 2526   score: 24.0  epsilon: 1.0    steps: 896  evaluation reward: 24.61\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5182: Policy loss: 0.005582. Value loss: 0.551644. Entropy: 0.967476.\n",
      "Iteration 5183: Policy loss: -0.010680. Value loss: 0.174759. Entropy: 0.978420.\n",
      "Iteration 5184: Policy loss: -0.025222. Value loss: 0.093289. Entropy: 0.956292.\n",
      "episode: 2527   score: 24.0  epsilon: 1.0    steps: 712  evaluation reward: 24.52\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5185: Policy loss: 0.007706. Value loss: 0.423963. Entropy: 0.967342.\n",
      "Iteration 5186: Policy loss: -0.011112. Value loss: 0.170607. Entropy: 0.955855.\n",
      "Iteration 5187: Policy loss: -0.020558. Value loss: 0.095428. Entropy: 0.965995.\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5188: Policy loss: 0.009775. Value loss: 0.507621. Entropy: 1.003339.\n",
      "Iteration 5189: Policy loss: -0.009732. Value loss: 0.201454. Entropy: 0.991220.\n",
      "Iteration 5190: Policy loss: -0.018982. Value loss: 0.124170. Entropy: 0.995154.\n",
      "episode: 2528   score: 21.0  epsilon: 1.0    steps: 424  evaluation reward: 24.48\n",
      "episode: 2529   score: 35.0  epsilon: 1.0    steps: 712  evaluation reward: 24.64\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5191: Policy loss: 0.020690. Value loss: 0.716304. Entropy: 0.974366.\n",
      "Iteration 5192: Policy loss: -0.004101. Value loss: 0.273728. Entropy: 0.967805.\n",
      "Iteration 5193: Policy loss: -0.018533. Value loss: 0.156935. Entropy: 0.955060.\n",
      "episode: 2530   score: 29.0  epsilon: 1.0    steps: 216  evaluation reward: 24.56\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5194: Policy loss: 0.008403. Value loss: 0.361790. Entropy: 0.919213.\n",
      "Iteration 5195: Policy loss: -0.007648. Value loss: 0.159054. Entropy: 0.898395.\n",
      "Iteration 5196: Policy loss: -0.020648. Value loss: 0.083432. Entropy: 0.901021.\n",
      "episode: 2531   score: 22.0  epsilon: 1.0    steps: 832  evaluation reward: 24.5\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5197: Policy loss: 0.010935. Value loss: 0.886966. Entropy: 1.002521.\n",
      "Iteration 5198: Policy loss: -0.006906. Value loss: 0.352578. Entropy: 1.022499.\n",
      "Iteration 5199: Policy loss: -0.017266. Value loss: 0.183963. Entropy: 1.015808.\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5200: Policy loss: 0.018141. Value loss: 0.564745. Entropy: 0.922092.\n",
      "Iteration 5201: Policy loss: -0.013039. Value loss: 0.252171. Entropy: 0.921975.\n",
      "Iteration 5202: Policy loss: -0.019731. Value loss: 0.151738. Entropy: 0.927746.\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5203: Policy loss: 0.006160. Value loss: 0.479654. Entropy: 1.029760.\n",
      "Iteration 5204: Policy loss: -0.016065. Value loss: 0.242982. Entropy: 1.008767.\n",
      "Iteration 5205: Policy loss: -0.025498. Value loss: 0.145852. Entropy: 1.013883.\n",
      "episode: 2532   score: 22.0  epsilon: 1.0    steps: 24  evaluation reward: 24.51\n",
      "episode: 2533   score: 27.0  epsilon: 1.0    steps: 424  evaluation reward: 24.54\n",
      "episode: 2534   score: 8.0  epsilon: 1.0    steps: 432  evaluation reward: 24.45\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5206: Policy loss: 0.008583. Value loss: 0.466239. Entropy: 0.973081.\n",
      "Iteration 5207: Policy loss: -0.006390. Value loss: 0.193810. Entropy: 0.967547.\n",
      "Iteration 5208: Policy loss: -0.017150. Value loss: 0.111963. Entropy: 0.967694.\n",
      "episode: 2535   score: 16.0  epsilon: 1.0    steps: 760  evaluation reward: 24.39\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5209: Policy loss: 0.013438. Value loss: 0.412360. Entropy: 0.941782.\n",
      "Iteration 5210: Policy loss: -0.007998. Value loss: 0.190039. Entropy: 0.948901.\n",
      "Iteration 5211: Policy loss: -0.017941. Value loss: 0.126826. Entropy: 0.945388.\n",
      "episode: 2536   score: 19.0  epsilon: 1.0    steps: 568  evaluation reward: 24.36\n",
      "episode: 2537   score: 25.0  epsilon: 1.0    steps: 712  evaluation reward: 24.45\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5212: Policy loss: 0.011595. Value loss: 0.326707. Entropy: 0.897068.\n",
      "Iteration 5213: Policy loss: -0.010263. Value loss: 0.109669. Entropy: 0.902197.\n",
      "Iteration 5214: Policy loss: -0.024387. Value loss: 0.059435. Entropy: 0.900341.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5215: Policy loss: 0.004687. Value loss: 0.422507. Entropy: 0.935053.\n",
      "Iteration 5216: Policy loss: -0.010942. Value loss: 0.167893. Entropy: 0.938646.\n",
      "Iteration 5217: Policy loss: -0.020960. Value loss: 0.088893. Entropy: 0.925876.\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5218: Policy loss: 0.010414. Value loss: 0.424536. Entropy: 0.882848.\n",
      "Iteration 5219: Policy loss: -0.006049. Value loss: 0.135555. Entropy: 0.875699.\n",
      "Iteration 5220: Policy loss: -0.016138. Value loss: 0.061133. Entropy: 0.873320.\n",
      "episode: 2538   score: 39.0  epsilon: 1.0    steps: 784  evaluation reward: 24.54\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5221: Policy loss: 0.010913. Value loss: 0.433284. Entropy: 0.897020.\n",
      "Iteration 5222: Policy loss: -0.009394. Value loss: 0.177795. Entropy: 0.894206.\n",
      "Iteration 5223: Policy loss: -0.024572. Value loss: 0.099998. Entropy: 0.891431.\n",
      "episode: 2539   score: 15.0  epsilon: 1.0    steps: 1016  evaluation reward: 24.41\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5224: Policy loss: 0.013177. Value loss: 0.601180. Entropy: 0.917972.\n",
      "Iteration 5225: Policy loss: -0.003301. Value loss: 0.239627. Entropy: 0.928200.\n",
      "Iteration 5226: Policy loss: -0.013942. Value loss: 0.133371. Entropy: 0.929910.\n",
      "episode: 2540   score: 21.0  epsilon: 1.0    steps: 120  evaluation reward: 24.37\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5227: Policy loss: 0.002578. Value loss: 0.459531. Entropy: 0.869250.\n",
      "Iteration 5228: Policy loss: -0.013874. Value loss: 0.246450. Entropy: 0.869026.\n",
      "Iteration 5229: Policy loss: -0.024785. Value loss: 0.161923. Entropy: 0.855974.\n",
      "episode: 2541   score: 16.0  epsilon: 1.0    steps: 288  evaluation reward: 24.38\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5230: Policy loss: 0.019951. Value loss: 0.283870. Entropy: 0.937209.\n",
      "Iteration 5231: Policy loss: -0.006439. Value loss: 0.098818. Entropy: 0.943216.\n",
      "Iteration 5232: Policy loss: -0.016258. Value loss: 0.059221. Entropy: 0.923574.\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5233: Policy loss: 0.008388. Value loss: 0.961047. Entropy: 0.942842.\n",
      "Iteration 5234: Policy loss: -0.009624. Value loss: 0.413052. Entropy: 0.922816.\n",
      "Iteration 5235: Policy loss: -0.018494. Value loss: 0.235934. Entropy: 0.930683.\n",
      "episode: 2542   score: 21.0  epsilon: 1.0    steps: 784  evaluation reward: 24.36\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5236: Policy loss: 0.009696. Value loss: 0.730326. Entropy: 0.904960.\n",
      "Iteration 5237: Policy loss: -0.003805. Value loss: 0.301396. Entropy: 0.910515.\n",
      "Iteration 5238: Policy loss: -0.018673. Value loss: 0.185808. Entropy: 0.903213.\n",
      "episode: 2543   score: 35.0  epsilon: 1.0    steps: 72  evaluation reward: 24.58\n",
      "episode: 2544   score: 26.0  epsilon: 1.0    steps: 352  evaluation reward: 24.56\n",
      "episode: 2545   score: 34.0  epsilon: 1.0    steps: 784  evaluation reward: 24.57\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5239: Policy loss: 0.004365. Value loss: 0.496082. Entropy: 0.849608.\n",
      "Iteration 5240: Policy loss: -0.010014. Value loss: 0.167705. Entropy: 0.847807.\n",
      "Iteration 5241: Policy loss: -0.020737. Value loss: 0.099979. Entropy: 0.866972.\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5242: Policy loss: 0.014243. Value loss: 0.531297. Entropy: 0.919414.\n",
      "Iteration 5243: Policy loss: -0.001509. Value loss: 0.254072. Entropy: 0.896766.\n",
      "Iteration 5244: Policy loss: -0.015018. Value loss: 0.133654. Entropy: 0.912099.\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5245: Policy loss: 0.011636. Value loss: 0.556258. Entropy: 0.917498.\n",
      "Iteration 5246: Policy loss: -0.010473. Value loss: 0.242713. Entropy: 0.936668.\n",
      "Iteration 5247: Policy loss: -0.023018. Value loss: 0.137512. Entropy: 0.921852.\n",
      "episode: 2546   score: 23.0  epsilon: 1.0    steps: 288  evaluation reward: 24.61\n",
      "episode: 2547   score: 25.0  epsilon: 1.0    steps: 672  evaluation reward: 24.62\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5248: Policy loss: 0.018108. Value loss: 0.753651. Entropy: 0.848786.\n",
      "Iteration 5249: Policy loss: -0.010215. Value loss: 0.324724. Entropy: 0.842609.\n",
      "Iteration 5250: Policy loss: -0.008147. Value loss: 0.174115. Entropy: 0.809789.\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5251: Policy loss: 0.004449. Value loss: 0.850081. Entropy: 0.907292.\n",
      "Iteration 5252: Policy loss: -0.005135. Value loss: 0.403196. Entropy: 0.883558.\n",
      "Iteration 5253: Policy loss: -0.014430. Value loss: 0.190957. Entropy: 0.877678.\n",
      "episode: 2548   score: 37.0  epsilon: 1.0    steps: 904  evaluation reward: 24.85\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5254: Policy loss: 0.017857. Value loss: 0.902338. Entropy: 0.911247.\n",
      "Iteration 5255: Policy loss: -0.000814. Value loss: 0.369374. Entropy: 0.904440.\n",
      "Iteration 5256: Policy loss: -0.019315. Value loss: 0.218748. Entropy: 0.900533.\n",
      "episode: 2549   score: 22.0  epsilon: 1.0    steps: 944  evaluation reward: 24.94\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5257: Policy loss: 0.004287. Value loss: 0.770531. Entropy: 0.954674.\n",
      "Iteration 5258: Policy loss: -0.011171. Value loss: 0.270420. Entropy: 0.959940.\n",
      "Iteration 5259: Policy loss: -0.023497. Value loss: 0.164127. Entropy: 0.956889.\n",
      "episode: 2550   score: 16.0  epsilon: 1.0    steps: 872  evaluation reward: 24.9\n",
      "now time :  2019-03-06 14:20:20.598145\n",
      "episode: 2551   score: 31.0  epsilon: 1.0    steps: 936  evaluation reward: 24.99\n",
      "episode: 2552   score: 22.0  epsilon: 1.0    steps: 1024  evaluation reward: 24.88\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5260: Policy loss: 0.017847. Value loss: 0.832722. Entropy: 0.934157.\n",
      "Iteration 5261: Policy loss: 0.001244. Value loss: 0.313764. Entropy: 0.946812.\n",
      "Iteration 5262: Policy loss: -0.015939. Value loss: 0.150669. Entropy: 0.937846.\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5263: Policy loss: 0.001646. Value loss: 0.442437. Entropy: 0.876584.\n",
      "Iteration 5264: Policy loss: -0.009305. Value loss: 0.207439. Entropy: 0.864269.\n",
      "Iteration 5265: Policy loss: -0.022161. Value loss: 0.112794. Entropy: 0.867278.\n",
      "episode: 2553   score: 32.0  epsilon: 1.0    steps: 584  evaluation reward: 24.98\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5266: Policy loss: 0.011551. Value loss: 0.553497. Entropy: 0.886140.\n",
      "Iteration 5267: Policy loss: -0.007390. Value loss: 0.206109. Entropy: 0.873139.\n",
      "Iteration 5268: Policy loss: -0.021382. Value loss: 0.126391. Entropy: 0.883918.\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5269: Policy loss: 0.015224. Value loss: 0.604718. Entropy: 0.872286.\n",
      "Iteration 5270: Policy loss: -0.003211. Value loss: 0.282862. Entropy: 0.874233.\n",
      "Iteration 5271: Policy loss: -0.019970. Value loss: 0.159954. Entropy: 0.864155.\n",
      "episode: 2554   score: 12.0  epsilon: 1.0    steps: 72  evaluation reward: 24.88\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5272: Policy loss: 0.009825. Value loss: 0.617226. Entropy: 0.915248.\n",
      "Iteration 5273: Policy loss: -0.005459. Value loss: 0.233753. Entropy: 0.912318.\n",
      "Iteration 5274: Policy loss: -0.013475. Value loss: 0.132338. Entropy: 0.919975.\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5275: Policy loss: 0.007689. Value loss: 0.658535. Entropy: 0.914792.\n",
      "Iteration 5276: Policy loss: -0.007750. Value loss: 0.199045. Entropy: 0.897016.\n",
      "Iteration 5277: Policy loss: -0.022373. Value loss: 0.103257. Entropy: 0.904176.\n",
      "episode: 2555   score: 39.0  epsilon: 1.0    steps: 584  evaluation reward: 25.09\n",
      "episode: 2556   score: 13.0  epsilon: 1.0    steps: 840  evaluation reward: 24.93\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5278: Policy loss: 0.007310. Value loss: 0.743265. Entropy: 0.946542.\n",
      "Iteration 5279: Policy loss: -0.009657. Value loss: 0.223431. Entropy: 0.933892.\n",
      "Iteration 5280: Policy loss: -0.019036. Value loss: 0.123147. Entropy: 0.934606.\n",
      "episode: 2557   score: 17.0  epsilon: 1.0    steps: 304  evaluation reward: 24.91\n",
      "episode: 2558   score: 28.0  epsilon: 1.0    steps: 360  evaluation reward: 24.84\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5281: Policy loss: 0.020191. Value loss: 0.444828. Entropy: 0.836413.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5282: Policy loss: -0.005884. Value loss: 0.146389. Entropy: 0.857954.\n",
      "Iteration 5283: Policy loss: -0.018261. Value loss: 0.089185. Entropy: 0.835902.\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5284: Policy loss: 0.006631. Value loss: 0.527846. Entropy: 0.949829.\n",
      "Iteration 5285: Policy loss: -0.006454. Value loss: 0.187713. Entropy: 0.947789.\n",
      "Iteration 5286: Policy loss: -0.013116. Value loss: 0.105339. Entropy: 0.955283.\n",
      "episode: 2559   score: 12.0  epsilon: 1.0    steps: 912  evaluation reward: 24.74\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5287: Policy loss: 0.010975. Value loss: 0.704713. Entropy: 0.910279.\n",
      "Iteration 5288: Policy loss: -0.003416. Value loss: 0.183597. Entropy: 0.916834.\n",
      "Iteration 5289: Policy loss: -0.015558. Value loss: 0.103632. Entropy: 0.910689.\n",
      "episode: 2560   score: 27.0  epsilon: 1.0    steps: 344  evaluation reward: 24.74\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5290: Policy loss: 0.010544. Value loss: 0.484248. Entropy: 0.905891.\n",
      "Iteration 5291: Policy loss: -0.011033. Value loss: 0.163991. Entropy: 0.913268.\n",
      "Iteration 5292: Policy loss: -0.017136. Value loss: 0.078439. Entropy: 0.909409.\n",
      "episode: 2561   score: 33.0  epsilon: 1.0    steps: 1024  evaluation reward: 24.92\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5293: Policy loss: 0.005319. Value loss: 0.753222. Entropy: 0.973822.\n",
      "Iteration 5294: Policy loss: -0.001227. Value loss: 0.348696. Entropy: 0.977585.\n",
      "Iteration 5295: Policy loss: -0.013739. Value loss: 0.193048. Entropy: 0.970952.\n",
      "episode: 2562   score: 45.0  epsilon: 1.0    steps: 560  evaluation reward: 25.18\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5296: Policy loss: 0.007496. Value loss: 0.323110. Entropy: 0.969514.\n",
      "Iteration 5297: Policy loss: -0.008382. Value loss: 0.120934. Entropy: 0.966148.\n",
      "Iteration 5298: Policy loss: -0.019144. Value loss: 0.070624. Entropy: 0.972367.\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5299: Policy loss: 0.006592. Value loss: 0.446871. Entropy: 0.918566.\n",
      "Iteration 5300: Policy loss: -0.005640. Value loss: 0.229460. Entropy: 0.926086.\n",
      "Iteration 5301: Policy loss: -0.016664. Value loss: 0.113332. Entropy: 0.909593.\n",
      "episode: 2563   score: 20.0  epsilon: 1.0    steps: 880  evaluation reward: 25.08\n",
      "episode: 2564   score: 17.0  epsilon: 1.0    steps: 896  evaluation reward: 24.86\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5302: Policy loss: 0.012510. Value loss: 0.566622. Entropy: 0.958276.\n",
      "Iteration 5303: Policy loss: -0.005231. Value loss: 0.221351. Entropy: 0.951652.\n",
      "Iteration 5304: Policy loss: -0.019021. Value loss: 0.119538. Entropy: 0.948707.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5305: Policy loss: 0.009129. Value loss: 0.648403. Entropy: 0.917928.\n",
      "Iteration 5306: Policy loss: 0.001634. Value loss: 0.270797. Entropy: 0.888337.\n",
      "Iteration 5307: Policy loss: -0.015546. Value loss: 0.142720. Entropy: 0.906325.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5308: Policy loss: 0.008847. Value loss: 0.549365. Entropy: 0.936585.\n",
      "Iteration 5309: Policy loss: -0.011382. Value loss: 0.268329. Entropy: 0.938056.\n",
      "Iteration 5310: Policy loss: -0.020766. Value loss: 0.152504. Entropy: 0.925997.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5311: Policy loss: 0.024153. Value loss: 1.108290. Entropy: 0.939730.\n",
      "Iteration 5312: Policy loss: 0.001653. Value loss: 0.434343. Entropy: 0.942035.\n",
      "Iteration 5313: Policy loss: -0.010324. Value loss: 0.249591. Entropy: 0.953936.\n",
      "episode: 2565   score: 40.0  epsilon: 1.0    steps: 48  evaluation reward: 25.09\n",
      "episode: 2566   score: 26.0  epsilon: 1.0    steps: 320  evaluation reward: 25.24\n",
      "episode: 2567   score: 33.0  epsilon: 1.0    steps: 784  evaluation reward: 25.22\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5314: Policy loss: 0.017270. Value loss: 0.495039. Entropy: 0.935926.\n",
      "Iteration 5315: Policy loss: -0.011407. Value loss: 0.221578. Entropy: 0.926305.\n",
      "Iteration 5316: Policy loss: -0.022147. Value loss: 0.160519. Entropy: 0.923114.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5317: Policy loss: 0.009561. Value loss: 0.733746. Entropy: 0.898486.\n",
      "Iteration 5318: Policy loss: -0.010631. Value loss: 0.357448. Entropy: 0.911196.\n",
      "Iteration 5319: Policy loss: -0.020834. Value loss: 0.216144. Entropy: 0.898508.\n",
      "episode: 2568   score: 34.0  epsilon: 1.0    steps: 472  evaluation reward: 25.26\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5320: Policy loss: 0.013047. Value loss: 0.520623. Entropy: 0.962738.\n",
      "Iteration 5321: Policy loss: -0.004233. Value loss: 0.228384. Entropy: 0.963940.\n",
      "Iteration 5322: Policy loss: -0.017039. Value loss: 0.111077. Entropy: 0.965571.\n",
      "episode: 2569   score: 27.0  epsilon: 1.0    steps: 400  evaluation reward: 25.19\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5323: Policy loss: 0.016668. Value loss: 0.811750. Entropy: 1.000941.\n",
      "Iteration 5324: Policy loss: -0.003691. Value loss: 0.404772. Entropy: 0.988967.\n",
      "Iteration 5325: Policy loss: -0.012734. Value loss: 0.237815. Entropy: 0.981599.\n",
      "episode: 2570   score: 28.0  epsilon: 1.0    steps: 304  evaluation reward: 25.24\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5326: Policy loss: 0.010217. Value loss: 0.466599. Entropy: 0.958819.\n",
      "Iteration 5327: Policy loss: -0.007117. Value loss: 0.167674. Entropy: 0.952253.\n",
      "Iteration 5328: Policy loss: -0.015723. Value loss: 0.074868. Entropy: 0.961297.\n",
      "episode: 2571   score: 23.0  epsilon: 1.0    steps: 312  evaluation reward: 25.18\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5329: Policy loss: 0.012758. Value loss: 0.623483. Entropy: 0.939964.\n",
      "Iteration 5330: Policy loss: -0.006150. Value loss: 0.231272. Entropy: 0.947235.\n",
      "Iteration 5331: Policy loss: -0.018891. Value loss: 0.152133. Entropy: 0.941915.\n",
      "episode: 2572   score: 11.0  epsilon: 1.0    steps: 328  evaluation reward: 24.9\n",
      "episode: 2573   score: 16.0  epsilon: 1.0    steps: 960  evaluation reward: 24.84\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5332: Policy loss: 0.010273. Value loss: 0.721420. Entropy: 0.939951.\n",
      "Iteration 5333: Policy loss: -0.007338. Value loss: 0.263678. Entropy: 0.947991.\n",
      "Iteration 5334: Policy loss: -0.020383. Value loss: 0.131680. Entropy: 0.927753.\n",
      "episode: 2574   score: 32.0  epsilon: 1.0    steps: 744  evaluation reward: 24.89\n",
      "episode: 2575   score: 23.0  epsilon: 1.0    steps: 960  evaluation reward: 24.81\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5335: Policy loss: 0.014360. Value loss: 0.443025. Entropy: 0.907447.\n",
      "Iteration 5336: Policy loss: -0.008271. Value loss: 0.172282. Entropy: 0.911484.\n",
      "Iteration 5337: Policy loss: -0.024498. Value loss: 0.102369. Entropy: 0.898847.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5338: Policy loss: 0.011674. Value loss: 0.476505. Entropy: 0.883810.\n",
      "Iteration 5339: Policy loss: 0.008431. Value loss: 0.132374. Entropy: 0.865540.\n",
      "Iteration 5340: Policy loss: -0.007959. Value loss: 0.075767. Entropy: 0.872665.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5341: Policy loss: 0.007830. Value loss: 0.344345. Entropy: 0.899357.\n",
      "Iteration 5342: Policy loss: -0.003336. Value loss: 0.126006. Entropy: 0.886451.\n",
      "Iteration 5343: Policy loss: -0.018274. Value loss: 0.070230. Entropy: 0.897519.\n",
      "episode: 2576   score: 21.0  epsilon: 1.0    steps: 808  evaluation reward: 24.75\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5344: Policy loss: 0.010610. Value loss: 0.828631. Entropy: 0.924759.\n",
      "Iteration 5345: Policy loss: -0.008649. Value loss: 0.348057. Entropy: 0.912263.\n",
      "Iteration 5346: Policy loss: -0.020214. Value loss: 0.209678. Entropy: 0.921701.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5347: Policy loss: 0.030412. Value loss: 0.572179. Entropy: 0.991525.\n",
      "Iteration 5348: Policy loss: -0.002073. Value loss: 0.197977. Entropy: 0.970277.\n",
      "Iteration 5349: Policy loss: -0.019951. Value loss: 0.117178. Entropy: 0.963156.\n",
      "episode: 2577   score: 12.0  epsilon: 1.0    steps: 208  evaluation reward: 24.45\n",
      "episode: 2578   score: 30.0  epsilon: 1.0    steps: 944  evaluation reward: 24.5\n",
      "Training network. lr: 0.000209. clip: 0.083725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5350: Policy loss: 0.013604. Value loss: 0.677520. Entropy: 0.918343.\n",
      "Iteration 5351: Policy loss: -0.005143. Value loss: 0.330050. Entropy: 0.937192.\n",
      "Iteration 5352: Policy loss: -0.020380. Value loss: 0.228887. Entropy: 0.930002.\n",
      "episode: 2579   score: 16.0  epsilon: 1.0    steps: 448  evaluation reward: 24.4\n",
      "episode: 2580   score: 14.0  epsilon: 1.0    steps: 456  evaluation reward: 24.41\n",
      "episode: 2581   score: 47.0  epsilon: 1.0    steps: 904  evaluation reward: 24.68\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5353: Policy loss: 0.025779. Value loss: 1.340302. Entropy: 0.951573.\n",
      "Iteration 5354: Policy loss: 0.011379. Value loss: 0.541788. Entropy: 0.949979.\n",
      "Iteration 5355: Policy loss: -0.003845. Value loss: 0.318727. Entropy: 0.936287.\n",
      "episode: 2582   score: 21.0  epsilon: 1.0    steps: 456  evaluation reward: 24.62\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5356: Policy loss: 0.006075. Value loss: 0.566622. Entropy: 0.840777.\n",
      "Iteration 5357: Policy loss: -0.010717. Value loss: 0.197688. Entropy: 0.851483.\n",
      "Iteration 5358: Policy loss: -0.016323. Value loss: 0.111600. Entropy: 0.851240.\n",
      "episode: 2583   score: 27.0  epsilon: 1.0    steps: 136  evaluation reward: 24.66\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5359: Policy loss: 0.002988. Value loss: 0.356399. Entropy: 0.913013.\n",
      "Iteration 5360: Policy loss: -0.016582. Value loss: 0.156058. Entropy: 0.916539.\n",
      "Iteration 5361: Policy loss: -0.022372. Value loss: 0.090614. Entropy: 0.918600.\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5362: Policy loss: 0.007351. Value loss: 0.497596. Entropy: 0.894792.\n",
      "Iteration 5363: Policy loss: -0.011351. Value loss: 0.211111. Entropy: 0.881064.\n",
      "Iteration 5364: Policy loss: -0.023162. Value loss: 0.103803. Entropy: 0.879453.\n",
      "episode: 2584   score: 16.0  epsilon: 1.0    steps: 576  evaluation reward: 24.58\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5365: Policy loss: 0.007078. Value loss: 0.605539. Entropy: 0.947750.\n",
      "Iteration 5366: Policy loss: -0.016276. Value loss: 0.251962. Entropy: 0.941858.\n",
      "Iteration 5367: Policy loss: -0.023446. Value loss: 0.147152. Entropy: 0.950152.\n",
      "episode: 2585   score: 15.0  epsilon: 1.0    steps: 848  evaluation reward: 24.62\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5368: Policy loss: 0.010906. Value loss: 0.538385. Entropy: 0.940751.\n",
      "Iteration 5369: Policy loss: -0.010268. Value loss: 0.261155. Entropy: 0.937057.\n",
      "Iteration 5370: Policy loss: -0.013604. Value loss: 0.180284. Entropy: 0.936659.\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5371: Policy loss: 0.013336. Value loss: 0.447722. Entropy: 0.904161.\n",
      "Iteration 5372: Policy loss: -0.003502. Value loss: 0.204500. Entropy: 0.904362.\n",
      "Iteration 5373: Policy loss: -0.014036. Value loss: 0.143734. Entropy: 0.902220.\n",
      "episode: 2586   score: 20.0  epsilon: 1.0    steps: 184  evaluation reward: 24.45\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5374: Policy loss: 0.011788. Value loss: 0.674898. Entropy: 0.956624.\n",
      "Iteration 5375: Policy loss: -0.005983. Value loss: 0.272394. Entropy: 0.951852.\n",
      "Iteration 5376: Policy loss: -0.015943. Value loss: 0.119963. Entropy: 0.956828.\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5377: Policy loss: 0.019978. Value loss: 0.704590. Entropy: 0.958562.\n",
      "Iteration 5378: Policy loss: -0.001147. Value loss: 0.206580. Entropy: 0.952304.\n",
      "Iteration 5379: Policy loss: -0.013872. Value loss: 0.105143. Entropy: 0.978067.\n",
      "episode: 2587   score: 30.0  epsilon: 1.0    steps: 176  evaluation reward: 24.52\n",
      "episode: 2588   score: 20.0  epsilon: 1.0    steps: 264  evaluation reward: 24.44\n",
      "episode: 2589   score: 33.0  epsilon: 1.0    steps: 872  evaluation reward: 24.55\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5380: Policy loss: 0.009185. Value loss: 1.134556. Entropy: 0.910230.\n",
      "Iteration 5381: Policy loss: 0.001842. Value loss: 0.442096. Entropy: 0.895805.\n",
      "Iteration 5382: Policy loss: -0.009239. Value loss: 0.237017. Entropy: 0.885531.\n",
      "episode: 2590   score: 30.0  epsilon: 1.0    steps: 32  evaluation reward: 24.48\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5383: Policy loss: 0.015439. Value loss: 0.426886. Entropy: 0.886252.\n",
      "Iteration 5384: Policy loss: -0.006267. Value loss: 0.162991. Entropy: 0.903742.\n",
      "Iteration 5385: Policy loss: -0.012895. Value loss: 0.099755. Entropy: 0.916359.\n",
      "episode: 2591   score: 31.0  epsilon: 1.0    steps: 128  evaluation reward: 24.54\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5386: Policy loss: 0.006234. Value loss: 0.718964. Entropy: 0.927338.\n",
      "Iteration 5387: Policy loss: -0.003863. Value loss: 0.370795. Entropy: 0.925037.\n",
      "Iteration 5388: Policy loss: -0.018293. Value loss: 0.237840. Entropy: 0.916319.\n",
      "episode: 2592   score: 18.0  epsilon: 1.0    steps: 664  evaluation reward: 24.55\n",
      "episode: 2593   score: 21.0  epsilon: 1.0    steps: 736  evaluation reward: 24.69\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5389: Policy loss: 0.019345. Value loss: 0.521527. Entropy: 0.895253.\n",
      "Iteration 5390: Policy loss: -0.004338. Value loss: 0.178544. Entropy: 0.875039.\n",
      "Iteration 5391: Policy loss: -0.019419. Value loss: 0.102354. Entropy: 0.878111.\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5392: Policy loss: 0.009088. Value loss: 0.510415. Entropy: 0.958424.\n",
      "Iteration 5393: Policy loss: -0.007141. Value loss: 0.255498. Entropy: 0.957459.\n",
      "Iteration 5394: Policy loss: -0.017068. Value loss: 0.127335. Entropy: 0.954696.\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5395: Policy loss: 0.009083. Value loss: 0.597629. Entropy: 0.956594.\n",
      "Iteration 5396: Policy loss: -0.004288. Value loss: 0.287514. Entropy: 0.952957.\n",
      "Iteration 5397: Policy loss: -0.019831. Value loss: 0.168009. Entropy: 0.951852.\n",
      "episode: 2594   score: 14.0  epsilon: 1.0    steps: 448  evaluation reward: 24.54\n",
      "episode: 2595   score: 27.0  epsilon: 1.0    steps: 736  evaluation reward: 24.65\n",
      "episode: 2596   score: 18.0  epsilon: 1.0    steps: 768  evaluation reward: 24.57\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5398: Policy loss: 0.020559. Value loss: 0.618954. Entropy: 0.944687.\n",
      "Iteration 5399: Policy loss: 0.000810. Value loss: 0.208427. Entropy: 0.951846.\n",
      "Iteration 5400: Policy loss: -0.010506. Value loss: 0.143824. Entropy: 0.951519.\n",
      "episode: 2597   score: 20.0  epsilon: 1.0    steps: 728  evaluation reward: 24.49\n",
      "episode: 2598   score: 20.0  epsilon: 1.0    steps: 768  evaluation reward: 24.4\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5401: Policy loss: 0.001704. Value loss: 0.835220. Entropy: 0.867875.\n",
      "Iteration 5402: Policy loss: -0.005168. Value loss: 0.351807. Entropy: 0.870805.\n",
      "Iteration 5403: Policy loss: -0.016981. Value loss: 0.196828. Entropy: 0.862650.\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5404: Policy loss: 0.012365. Value loss: 0.436276. Entropy: 0.896313.\n",
      "Iteration 5405: Policy loss: -0.009601. Value loss: 0.199591. Entropy: 0.895603.\n",
      "Iteration 5406: Policy loss: -0.021636. Value loss: 0.117656. Entropy: 0.894633.\n",
      "episode: 2599   score: 20.0  epsilon: 1.0    steps: 816  evaluation reward: 24.34\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5407: Policy loss: 0.007122. Value loss: 0.501203. Entropy: 0.962648.\n",
      "Iteration 5408: Policy loss: -0.013488. Value loss: 0.232786. Entropy: 0.960989.\n",
      "Iteration 5409: Policy loss: -0.023756. Value loss: 0.145668. Entropy: 0.967602.\n",
      "episode: 2600   score: 17.0  epsilon: 1.0    steps: 96  evaluation reward: 24.35\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5410: Policy loss: 0.005045. Value loss: 0.348759. Entropy: 0.951604.\n",
      "Iteration 5411: Policy loss: -0.018232. Value loss: 0.168394. Entropy: 0.942017.\n",
      "Iteration 5412: Policy loss: -0.026076. Value loss: 0.106810. Entropy: 0.949870.\n",
      "now time :  2019-03-06 14:23:35.334014\n",
      "episode: 2601   score: 18.0  epsilon: 1.0    steps: 296  evaluation reward: 24.4\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5413: Policy loss: 0.012964. Value loss: 0.529055. Entropy: 0.899295.\n",
      "Iteration 5414: Policy loss: -0.014342. Value loss: 0.176633. Entropy: 0.909077.\n",
      "Iteration 5415: Policy loss: -0.025178. Value loss: 0.093720. Entropy: 0.893706.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5416: Policy loss: 0.011112. Value loss: 0.649178. Entropy: 0.910448.\n",
      "Iteration 5417: Policy loss: -0.004765. Value loss: 0.311443. Entropy: 0.892641.\n",
      "Iteration 5418: Policy loss: -0.019587. Value loss: 0.180123. Entropy: 0.914656.\n",
      "episode: 2602   score: 15.0  epsilon: 1.0    steps: 864  evaluation reward: 24.24\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5419: Policy loss: 0.006492. Value loss: 0.611225. Entropy: 0.938740.\n",
      "Iteration 5420: Policy loss: -0.004595. Value loss: 0.213020. Entropy: 0.945541.\n",
      "Iteration 5421: Policy loss: -0.020553. Value loss: 0.119153. Entropy: 0.950697.\n",
      "episode: 2603   score: 17.0  epsilon: 1.0    steps: 8  evaluation reward: 24.08\n",
      "episode: 2604   score: 25.0  epsilon: 1.0    steps: 280  evaluation reward: 24.0\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5422: Policy loss: 0.011193. Value loss: 0.266456. Entropy: 0.963243.\n",
      "Iteration 5423: Policy loss: -0.003407. Value loss: 0.122043. Entropy: 0.964191.\n",
      "Iteration 5424: Policy loss: -0.019730. Value loss: 0.065562. Entropy: 0.970241.\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5425: Policy loss: 0.007085. Value loss: 0.625273. Entropy: 0.958694.\n",
      "Iteration 5426: Policy loss: -0.002786. Value loss: 0.283302. Entropy: 0.945196.\n",
      "Iteration 5427: Policy loss: -0.015279. Value loss: 0.142892. Entropy: 0.935169.\n",
      "episode: 2605   score: 14.0  epsilon: 1.0    steps: 48  evaluation reward: 23.97\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5428: Policy loss: 0.014496. Value loss: 0.324100. Entropy: 0.915856.\n",
      "Iteration 5429: Policy loss: -0.007677. Value loss: 0.092698. Entropy: 0.922109.\n",
      "Iteration 5430: Policy loss: -0.012436. Value loss: 0.048444. Entropy: 0.924898.\n",
      "episode: 2606   score: 18.0  epsilon: 1.0    steps: 104  evaluation reward: 24.0\n",
      "episode: 2607   score: 33.0  epsilon: 1.0    steps: 272  evaluation reward: 24.14\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5431: Policy loss: 0.011557. Value loss: 0.672855. Entropy: 0.949912.\n",
      "Iteration 5432: Policy loss: -0.009957. Value loss: 0.358556. Entropy: 0.957614.\n",
      "Iteration 5433: Policy loss: -0.018209. Value loss: 0.245391. Entropy: 0.941547.\n",
      "episode: 2608   score: 32.0  epsilon: 1.0    steps: 328  evaluation reward: 24.29\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5434: Policy loss: 0.008288. Value loss: 0.426065. Entropy: 0.934642.\n",
      "Iteration 5435: Policy loss: -0.008489. Value loss: 0.179513. Entropy: 0.921389.\n",
      "Iteration 5436: Policy loss: -0.020268. Value loss: 0.080411. Entropy: 0.924803.\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5437: Policy loss: 0.022107. Value loss: 0.598679. Entropy: 0.959570.\n",
      "Iteration 5438: Policy loss: 0.001927. Value loss: 0.202071. Entropy: 0.963450.\n",
      "Iteration 5439: Policy loss: -0.009117. Value loss: 0.078946. Entropy: 0.975572.\n",
      "episode: 2609   score: 26.0  epsilon: 1.0    steps: 16  evaluation reward: 24.29\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5440: Policy loss: 0.012290. Value loss: 0.363792. Entropy: 0.976242.\n",
      "Iteration 5441: Policy loss: -0.009689. Value loss: 0.149265. Entropy: 0.956124.\n",
      "Iteration 5442: Policy loss: -0.016307. Value loss: 0.063649. Entropy: 0.971683.\n",
      "episode: 2610   score: 19.0  epsilon: 1.0    steps: 32  evaluation reward: 24.26\n",
      "episode: 2611   score: 25.0  epsilon: 1.0    steps: 416  evaluation reward: 24.17\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5443: Policy loss: 0.011492. Value loss: 0.611933. Entropy: 0.976946.\n",
      "Iteration 5444: Policy loss: -0.004154. Value loss: 0.235533. Entropy: 0.974134.\n",
      "Iteration 5445: Policy loss: -0.016981. Value loss: 0.113278. Entropy: 0.971347.\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5446: Policy loss: 0.012933. Value loss: 0.870983. Entropy: 1.006455.\n",
      "Iteration 5447: Policy loss: -0.000801. Value loss: 0.357273. Entropy: 0.984897.\n",
      "Iteration 5448: Policy loss: -0.009628. Value loss: 0.188890. Entropy: 0.993739.\n",
      "episode: 2612   score: 20.0  epsilon: 1.0    steps: 72  evaluation reward: 24.11\n",
      "episode: 2613   score: 26.0  epsilon: 1.0    steps: 312  evaluation reward: 24.14\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5449: Policy loss: 0.012905. Value loss: 0.309461. Entropy: 0.959321.\n",
      "Iteration 5450: Policy loss: 0.000061. Value loss: 0.121567. Entropy: 0.935765.\n",
      "Iteration 5451: Policy loss: -0.017782. Value loss: 0.069608. Entropy: 0.937876.\n",
      "episode: 2614   score: 24.0  epsilon: 1.0    steps: 608  evaluation reward: 24.15\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5452: Policy loss: 0.009714. Value loss: 0.523048. Entropy: 0.998792.\n",
      "Iteration 5453: Policy loss: -0.002853. Value loss: 0.264989. Entropy: 1.001875.\n",
      "Iteration 5454: Policy loss: -0.015654. Value loss: 0.145599. Entropy: 1.000657.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5455: Policy loss: 0.006899. Value loss: 0.386345. Entropy: 0.965080.\n",
      "Iteration 5456: Policy loss: -0.015101. Value loss: 0.163040. Entropy: 0.953884.\n",
      "Iteration 5457: Policy loss: -0.023902. Value loss: 0.089320. Entropy: 0.946849.\n",
      "episode: 2615   score: 27.0  epsilon: 1.0    steps: 744  evaluation reward: 24.04\n",
      "episode: 2616   score: 23.0  epsilon: 1.0    steps: 1008  evaluation reward: 24.14\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5458: Policy loss: 0.012811. Value loss: 0.709436. Entropy: 1.029881.\n",
      "Iteration 5459: Policy loss: -0.004552. Value loss: 0.306635. Entropy: 1.016735.\n",
      "Iteration 5460: Policy loss: -0.012745. Value loss: 0.134441. Entropy: 1.024390.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5461: Policy loss: 0.013297. Value loss: 0.388655. Entropy: 0.974296.\n",
      "Iteration 5462: Policy loss: -0.010237. Value loss: 0.156866. Entropy: 0.961927.\n",
      "Iteration 5463: Policy loss: -0.021062. Value loss: 0.086509. Entropy: 0.959314.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5464: Policy loss: 0.007022. Value loss: 0.668404. Entropy: 0.976814.\n",
      "Iteration 5465: Policy loss: -0.007208. Value loss: 0.367677. Entropy: 0.975627.\n",
      "Iteration 5466: Policy loss: -0.018552. Value loss: 0.196140. Entropy: 0.965637.\n",
      "episode: 2617   score: 44.0  epsilon: 1.0    steps: 896  evaluation reward: 24.19\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5467: Policy loss: 0.013008. Value loss: 0.657292. Entropy: 0.972986.\n",
      "Iteration 5468: Policy loss: -0.005727. Value loss: 0.256181. Entropy: 0.959720.\n",
      "Iteration 5469: Policy loss: -0.017730. Value loss: 0.122769. Entropy: 0.975029.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5470: Policy loss: 0.010476. Value loss: 0.800964. Entropy: 0.965436.\n",
      "Iteration 5471: Policy loss: -0.006949. Value loss: 0.335978. Entropy: 0.942322.\n",
      "Iteration 5472: Policy loss: -0.017293. Value loss: 0.160843. Entropy: 0.950626.\n",
      "episode: 2618   score: 22.0  epsilon: 1.0    steps: 272  evaluation reward: 23.93\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5473: Policy loss: 0.007882. Value loss: 0.490611. Entropy: 1.006761.\n",
      "Iteration 5474: Policy loss: -0.009279. Value loss: 0.207487. Entropy: 1.009920.\n",
      "Iteration 5475: Policy loss: -0.016540. Value loss: 0.107414. Entropy: 1.005259.\n",
      "episode: 2619   score: 33.0  epsilon: 1.0    steps: 728  evaluation reward: 24.09\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5476: Policy loss: 0.016316. Value loss: 0.829882. Entropy: 0.993120.\n",
      "Iteration 5477: Policy loss: -0.002072. Value loss: 0.329453. Entropy: 0.994919.\n",
      "Iteration 5478: Policy loss: -0.017830. Value loss: 0.201675. Entropy: 0.989295.\n",
      "episode: 2620   score: 35.0  epsilon: 1.0    steps: 112  evaluation reward: 24.31\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5479: Policy loss: 0.014656. Value loss: 0.416829. Entropy: 0.997719.\n",
      "Iteration 5480: Policy loss: 0.003293. Value loss: 0.166590. Entropy: 1.008650.\n",
      "Iteration 5481: Policy loss: -0.008776. Value loss: 0.091481. Entropy: 1.002099.\n",
      "episode: 2621   score: 53.0  epsilon: 1.0    steps: 336  evaluation reward: 24.48\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5482: Policy loss: 0.017522. Value loss: 0.712203. Entropy: 1.008019.\n",
      "Iteration 5483: Policy loss: 0.002074. Value loss: 0.322915. Entropy: 0.988143.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5484: Policy loss: -0.011222. Value loss: 0.177099. Entropy: 0.988786.\n",
      "episode: 2622   score: 13.0  epsilon: 1.0    steps: 824  evaluation reward: 24.29\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5485: Policy loss: 0.014423. Value loss: 0.789376. Entropy: 0.955460.\n",
      "Iteration 5486: Policy loss: -0.001861. Value loss: 0.294848. Entropy: 0.948404.\n",
      "Iteration 5487: Policy loss: -0.015204. Value loss: 0.142317. Entropy: 0.941985.\n",
      "episode: 2623   score: 22.0  epsilon: 1.0    steps: 248  evaluation reward: 24.23\n",
      "episode: 2624   score: 27.0  epsilon: 1.0    steps: 904  evaluation reward: 24.26\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5488: Policy loss: 0.006940. Value loss: 1.102019. Entropy: 0.970111.\n",
      "Iteration 5489: Policy loss: -0.005268. Value loss: 0.450778. Entropy: 0.968286.\n",
      "Iteration 5490: Policy loss: -0.012580. Value loss: 0.248029. Entropy: 0.961506.\n",
      "episode: 2625   score: 18.0  epsilon: 1.0    steps: 800  evaluation reward: 24.3\n",
      "episode: 2626   score: 49.0  epsilon: 1.0    steps: 944  evaluation reward: 24.55\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5491: Policy loss: 0.008119. Value loss: 0.777720. Entropy: 0.990455.\n",
      "Iteration 5492: Policy loss: -0.011080. Value loss: 0.246636. Entropy: 0.984457.\n",
      "Iteration 5493: Policy loss: -0.017886. Value loss: 0.117408. Entropy: 0.985320.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5494: Policy loss: 0.008968. Value loss: 0.613177. Entropy: 0.954954.\n",
      "Iteration 5495: Policy loss: -0.004018. Value loss: 0.235548. Entropy: 0.947649.\n",
      "Iteration 5496: Policy loss: -0.016366. Value loss: 0.107095. Entropy: 0.959193.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5497: Policy loss: 0.015935. Value loss: 0.577012. Entropy: 1.011927.\n",
      "Iteration 5498: Policy loss: -0.010039. Value loss: 0.233828. Entropy: 1.012087.\n",
      "Iteration 5499: Policy loss: -0.024196. Value loss: 0.140014. Entropy: 1.000636.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5500: Policy loss: 0.014090. Value loss: 0.409217. Entropy: 1.038491.\n",
      "Iteration 5501: Policy loss: -0.006911. Value loss: 0.225828. Entropy: 1.019223.\n",
      "Iteration 5502: Policy loss: -0.016972. Value loss: 0.160603. Entropy: 1.020866.\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5503: Policy loss: 0.017510. Value loss: 0.553454. Entropy: 1.005067.\n",
      "Iteration 5504: Policy loss: -0.004664. Value loss: 0.273392. Entropy: 0.991984.\n",
      "Iteration 5505: Policy loss: -0.019261. Value loss: 0.174436. Entropy: 0.991958.\n",
      "episode: 2627   score: 22.0  epsilon: 1.0    steps: 416  evaluation reward: 24.53\n",
      "episode: 2628   score: 39.0  epsilon: 1.0    steps: 576  evaluation reward: 24.71\n",
      "episode: 2629   score: 17.0  epsilon: 1.0    steps: 896  evaluation reward: 24.53\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5506: Policy loss: 0.008831. Value loss: 0.455118. Entropy: 0.961037.\n",
      "Iteration 5507: Policy loss: -0.008766. Value loss: 0.153044. Entropy: 0.969661.\n",
      "Iteration 5508: Policy loss: -0.020046. Value loss: 0.081640. Entropy: 0.972314.\n",
      "episode: 2630   score: 22.0  epsilon: 1.0    steps: 368  evaluation reward: 24.46\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5509: Policy loss: 0.003394. Value loss: 0.602203. Entropy: 0.966695.\n",
      "Iteration 5510: Policy loss: -0.007964. Value loss: 0.304923. Entropy: 0.944741.\n",
      "Iteration 5511: Policy loss: -0.018203. Value loss: 0.206969. Entropy: 0.948060.\n",
      "episode: 2631   score: 19.0  epsilon: 1.0    steps: 704  evaluation reward: 24.43\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5512: Policy loss: 0.006790. Value loss: 0.524000. Entropy: 0.950699.\n",
      "Iteration 5513: Policy loss: -0.007983. Value loss: 0.221008. Entropy: 0.946990.\n",
      "Iteration 5514: Policy loss: -0.016557. Value loss: 0.108314. Entropy: 0.944909.\n",
      "episode: 2632   score: 24.0  epsilon: 1.0    steps: 88  evaluation reward: 24.45\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5515: Policy loss: 0.007006. Value loss: 0.340655. Entropy: 0.867338.\n",
      "Iteration 5516: Policy loss: -0.012987. Value loss: 0.114561. Entropy: 0.863998.\n",
      "Iteration 5517: Policy loss: -0.023770. Value loss: 0.068935. Entropy: 0.867372.\n",
      "episode: 2633   score: 24.0  epsilon: 1.0    steps: 632  evaluation reward: 24.42\n",
      "episode: 2634   score: 28.0  epsilon: 1.0    steps: 1000  evaluation reward: 24.62\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5518: Policy loss: 0.009019. Value loss: 0.298801. Entropy: 0.984346.\n",
      "Iteration 5519: Policy loss: -0.008241. Value loss: 0.109788. Entropy: 0.975381.\n",
      "Iteration 5520: Policy loss: -0.024589. Value loss: 0.068868. Entropy: 0.985031.\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5521: Policy loss: 0.011070. Value loss: 0.304738. Entropy: 0.903306.\n",
      "Iteration 5522: Policy loss: -0.006813. Value loss: 0.122119. Entropy: 0.903967.\n",
      "Iteration 5523: Policy loss: -0.013207. Value loss: 0.065876. Entropy: 0.898113.\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5524: Policy loss: 0.009223. Value loss: 0.506325. Entropy: 1.006728.\n",
      "Iteration 5525: Policy loss: -0.007130. Value loss: 0.294401. Entropy: 0.981998.\n",
      "Iteration 5526: Policy loss: -0.014865. Value loss: 0.163207. Entropy: 0.987191.\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5527: Policy loss: 0.016238. Value loss: 0.629224. Entropy: 0.886865.\n",
      "Iteration 5528: Policy loss: -0.002180. Value loss: 0.228276. Entropy: 0.867835.\n",
      "Iteration 5529: Policy loss: -0.011151. Value loss: 0.144692. Entropy: 0.884237.\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5530: Policy loss: 0.012239. Value loss: 0.785046. Entropy: 0.942650.\n",
      "Iteration 5531: Policy loss: 0.001594. Value loss: 0.309271. Entropy: 0.934229.\n",
      "Iteration 5532: Policy loss: -0.013200. Value loss: 0.187950. Entropy: 0.940422.\n",
      "episode: 2635   score: 23.0  epsilon: 1.0    steps: 536  evaluation reward: 24.69\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5533: Policy loss: 0.011996. Value loss: 0.963010. Entropy: 1.019428.\n",
      "Iteration 5534: Policy loss: -0.001244. Value loss: 0.511229. Entropy: 1.041960.\n",
      "Iteration 5535: Policy loss: -0.015404. Value loss: 0.338843. Entropy: 1.035049.\n",
      "episode: 2636   score: 33.0  epsilon: 1.0    steps: 248  evaluation reward: 24.83\n",
      "episode: 2637   score: 30.0  epsilon: 1.0    steps: 400  evaluation reward: 24.88\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5536: Policy loss: 0.009593. Value loss: 0.606334. Entropy: 1.019469.\n",
      "Iteration 5537: Policy loss: -0.003497. Value loss: 0.269996. Entropy: 1.020400.\n",
      "Iteration 5538: Policy loss: -0.011101. Value loss: 0.170644. Entropy: 1.008910.\n",
      "episode: 2638   score: 23.0  epsilon: 1.0    steps: 376  evaluation reward: 24.72\n",
      "episode: 2639   score: 31.0  epsilon: 1.0    steps: 496  evaluation reward: 24.88\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5539: Policy loss: 0.006242. Value loss: 0.439053. Entropy: 1.004723.\n",
      "Iteration 5540: Policy loss: -0.008773. Value loss: 0.204473. Entropy: 1.015102.\n",
      "Iteration 5541: Policy loss: -0.019059. Value loss: 0.126458. Entropy: 1.009109.\n",
      "episode: 2640   score: 18.0  epsilon: 1.0    steps: 96  evaluation reward: 24.85\n",
      "episode: 2641   score: 15.0  epsilon: 1.0    steps: 240  evaluation reward: 24.84\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5542: Policy loss: 0.014076. Value loss: 0.375685. Entropy: 0.930357.\n",
      "Iteration 5543: Policy loss: 0.000952. Value loss: 0.120878. Entropy: 0.949774.\n",
      "Iteration 5544: Policy loss: -0.012450. Value loss: 0.065334. Entropy: 0.924039.\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5545: Policy loss: 0.009608. Value loss: 0.698307. Entropy: 1.000394.\n",
      "Iteration 5546: Policy loss: -0.006215. Value loss: 0.388121. Entropy: 0.996305.\n",
      "Iteration 5547: Policy loss: -0.016654. Value loss: 0.233302. Entropy: 1.002008.\n",
      "episode: 2642   score: 43.0  epsilon: 1.0    steps: 792  evaluation reward: 25.06\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5548: Policy loss: 0.014483. Value loss: 0.730623. Entropy: 0.957171.\n",
      "Iteration 5549: Policy loss: -0.006164. Value loss: 0.381445. Entropy: 0.964736.\n",
      "Iteration 5550: Policy loss: -0.011265. Value loss: 0.197102. Entropy: 0.972227.\n",
      "Training network. lr: 0.000207. clip: 0.082960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5551: Policy loss: 0.007049. Value loss: 0.281697. Entropy: 0.952197.\n",
      "Iteration 5552: Policy loss: -0.006692. Value loss: 0.082719. Entropy: 0.952417.\n",
      "Iteration 5553: Policy loss: -0.022677. Value loss: 0.043540. Entropy: 0.937701.\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5554: Policy loss: 0.010062. Value loss: 0.569516. Entropy: 1.036177.\n",
      "Iteration 5555: Policy loss: -0.008975. Value loss: 0.241825. Entropy: 1.029056.\n",
      "Iteration 5556: Policy loss: -0.015786. Value loss: 0.133587. Entropy: 1.030980.\n",
      "episode: 2643   score: 11.0  epsilon: 1.0    steps: 120  evaluation reward: 24.82\n",
      "episode: 2644   score: 13.0  epsilon: 1.0    steps: 952  evaluation reward: 24.69\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5557: Policy loss: 0.000896. Value loss: 0.556331. Entropy: 1.037791.\n",
      "Iteration 5558: Policy loss: -0.003582. Value loss: 0.211380. Entropy: 1.043259.\n",
      "Iteration 5559: Policy loss: -0.016003. Value loss: 0.132839. Entropy: 1.036868.\n",
      "episode: 2645   score: 24.0  epsilon: 1.0    steps: 376  evaluation reward: 24.59\n",
      "episode: 2646   score: 26.0  epsilon: 1.0    steps: 656  evaluation reward: 24.62\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5560: Policy loss: 0.010649. Value loss: 0.635464. Entropy: 1.003230.\n",
      "Iteration 5561: Policy loss: -0.007818. Value loss: 0.260485. Entropy: 0.982853.\n",
      "Iteration 5562: Policy loss: -0.016546. Value loss: 0.145265. Entropy: 0.988927.\n",
      "episode: 2647   score: 11.0  epsilon: 1.0    steps: 936  evaluation reward: 24.48\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5563: Policy loss: 0.009939. Value loss: 0.467163. Entropy: 0.990808.\n",
      "Iteration 5564: Policy loss: -0.012466. Value loss: 0.211651. Entropy: 0.979480.\n",
      "Iteration 5565: Policy loss: -0.015269. Value loss: 0.120830. Entropy: 0.971837.\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5566: Policy loss: 0.011434. Value loss: 0.924011. Entropy: 1.043468.\n",
      "Iteration 5567: Policy loss: -0.003450. Value loss: 0.381098. Entropy: 1.047502.\n",
      "Iteration 5568: Policy loss: -0.016427. Value loss: 0.207967. Entropy: 1.055264.\n",
      "episode: 2648   score: 25.0  epsilon: 1.0    steps: 16  evaluation reward: 24.36\n",
      "episode: 2649   score: 40.0  epsilon: 1.0    steps: 520  evaluation reward: 24.54\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5569: Policy loss: 0.009338. Value loss: 0.532883. Entropy: 0.998803.\n",
      "Iteration 5570: Policy loss: -0.005108. Value loss: 0.194323. Entropy: 1.003459.\n",
      "Iteration 5571: Policy loss: -0.019105. Value loss: 0.123698. Entropy: 1.009101.\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5572: Policy loss: 0.014309. Value loss: 0.636669. Entropy: 0.986034.\n",
      "Iteration 5573: Policy loss: -0.002865. Value loss: 0.234324. Entropy: 0.971218.\n",
      "Iteration 5574: Policy loss: -0.013494. Value loss: 0.105473. Entropy: 0.975578.\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5575: Policy loss: 0.010202. Value loss: 0.475842. Entropy: 1.050959.\n",
      "Iteration 5576: Policy loss: -0.005028. Value loss: 0.284289. Entropy: 1.044341.\n",
      "Iteration 5577: Policy loss: -0.016654. Value loss: 0.187617. Entropy: 1.059102.\n",
      "episode: 2650   score: 13.0  epsilon: 1.0    steps: 160  evaluation reward: 24.51\n",
      "now time :  2019-03-06 14:27:07.690752\n",
      "episode: 2651   score: 8.0  epsilon: 1.0    steps: 392  evaluation reward: 24.28\n",
      "episode: 2652   score: 47.0  epsilon: 1.0    steps: 960  evaluation reward: 24.53\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5578: Policy loss: 0.011162. Value loss: 0.637554. Entropy: 1.026031.\n",
      "Iteration 5579: Policy loss: -0.001344. Value loss: 0.305766. Entropy: 1.031716.\n",
      "Iteration 5580: Policy loss: -0.007970. Value loss: 0.177289. Entropy: 1.033824.\n",
      "episode: 2653   score: 23.0  epsilon: 1.0    steps: 752  evaluation reward: 24.44\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5581: Policy loss: 0.004252. Value loss: 0.690393. Entropy: 1.049152.\n",
      "Iteration 5582: Policy loss: -0.012164. Value loss: 0.266844. Entropy: 1.023916.\n",
      "Iteration 5583: Policy loss: -0.018424. Value loss: 0.156143. Entropy: 1.036842.\n",
      "episode: 2654   score: 17.0  epsilon: 1.0    steps: 264  evaluation reward: 24.49\n",
      "episode: 2655   score: 29.0  epsilon: 1.0    steps: 824  evaluation reward: 24.39\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5584: Policy loss: 0.008056. Value loss: 0.552793. Entropy: 1.047402.\n",
      "Iteration 5585: Policy loss: -0.008140. Value loss: 0.256743. Entropy: 1.053411.\n",
      "Iteration 5586: Policy loss: -0.016186. Value loss: 0.156442. Entropy: 1.053420.\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5587: Policy loss: 0.007169. Value loss: 0.570084. Entropy: 1.044519.\n",
      "Iteration 5588: Policy loss: -0.006066. Value loss: 0.317652. Entropy: 1.030087.\n",
      "Iteration 5589: Policy loss: -0.016473. Value loss: 0.212820. Entropy: 1.040417.\n",
      "episode: 2656   score: 17.0  epsilon: 1.0    steps: 560  evaluation reward: 24.43\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5590: Policy loss: 0.011581. Value loss: 0.651766. Entropy: 1.015879.\n",
      "Iteration 5591: Policy loss: -0.001183. Value loss: 0.318371. Entropy: 1.012626.\n",
      "Iteration 5592: Policy loss: -0.008894. Value loss: 0.194484. Entropy: 1.001670.\n",
      "episode: 2657   score: 9.0  epsilon: 1.0    steps: 248  evaluation reward: 24.35\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5593: Policy loss: 0.011088. Value loss: 0.445769. Entropy: 0.991780.\n",
      "Iteration 5594: Policy loss: -0.010694. Value loss: 0.154699. Entropy: 0.992732.\n",
      "Iteration 5595: Policy loss: -0.026044. Value loss: 0.066719. Entropy: 0.996618.\n",
      "episode: 2658   score: 21.0  epsilon: 1.0    steps: 488  evaluation reward: 24.28\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5596: Policy loss: 0.006465. Value loss: 0.453673. Entropy: 1.003742.\n",
      "Iteration 5597: Policy loss: -0.013052. Value loss: 0.171527. Entropy: 1.006848.\n",
      "Iteration 5598: Policy loss: -0.023024. Value loss: 0.090133. Entropy: 0.995350.\n",
      "episode: 2659   score: 15.0  epsilon: 1.0    steps: 216  evaluation reward: 24.31\n",
      "episode: 2660   score: 20.0  epsilon: 1.0    steps: 544  evaluation reward: 24.24\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5599: Policy loss: 0.011379. Value loss: 0.480240. Entropy: 0.975895.\n",
      "Iteration 5600: Policy loss: -0.004585. Value loss: 0.172537. Entropy: 0.988389.\n",
      "Iteration 5601: Policy loss: -0.017167. Value loss: 0.106227. Entropy: 0.991288.\n",
      "episode: 2661   score: 18.0  epsilon: 1.0    steps: 552  evaluation reward: 24.09\n",
      "episode: 2662   score: 22.0  epsilon: 1.0    steps: 568  evaluation reward: 23.86\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5602: Policy loss: 0.013349. Value loss: 0.555347. Entropy: 1.026815.\n",
      "Iteration 5603: Policy loss: -0.001553. Value loss: 0.229564. Entropy: 1.021557.\n",
      "Iteration 5604: Policy loss: -0.018225. Value loss: 0.131529. Entropy: 1.011845.\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5605: Policy loss: 0.001800. Value loss: 0.443667. Entropy: 0.976901.\n",
      "Iteration 5606: Policy loss: -0.013867. Value loss: 0.180672. Entropy: 0.968812.\n",
      "Iteration 5607: Policy loss: -0.025918. Value loss: 0.107730. Entropy: 0.964022.\n",
      "episode: 2663   score: 15.0  epsilon: 1.0    steps: 32  evaluation reward: 23.81\n",
      "episode: 2664   score: 10.0  epsilon: 1.0    steps: 696  evaluation reward: 23.74\n",
      "episode: 2665   score: 9.0  epsilon: 1.0    steps: 824  evaluation reward: 23.43\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5608: Policy loss: 0.011161. Value loss: 0.564472. Entropy: 0.961952.\n",
      "Iteration 5609: Policy loss: -0.004653. Value loss: 0.228264. Entropy: 0.937504.\n",
      "Iteration 5610: Policy loss: -0.018265. Value loss: 0.148656. Entropy: 0.947486.\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5611: Policy loss: 0.010145. Value loss: 0.556036. Entropy: 0.889135.\n",
      "Iteration 5612: Policy loss: -0.001562. Value loss: 0.205430. Entropy: 0.879191.\n",
      "Iteration 5613: Policy loss: -0.016063. Value loss: 0.126941. Entropy: 0.890093.\n",
      "episode: 2666   score: 19.0  epsilon: 1.0    steps: 680  evaluation reward: 23.36\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5614: Policy loss: 0.007569. Value loss: 0.459815. Entropy: 0.941028.\n",
      "Iteration 5615: Policy loss: -0.010105. Value loss: 0.203194. Entropy: 0.946379.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5616: Policy loss: -0.023301. Value loss: 0.116357. Entropy: 0.940134.\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5617: Policy loss: 0.013162. Value loss: 0.302088. Entropy: 0.938868.\n",
      "Iteration 5618: Policy loss: -0.009167. Value loss: 0.120565. Entropy: 0.933231.\n",
      "Iteration 5619: Policy loss: -0.019275. Value loss: 0.064609. Entropy: 0.918931.\n",
      "episode: 2667   score: 15.0  epsilon: 1.0    steps: 24  evaluation reward: 23.18\n",
      "episode: 2668   score: 19.0  epsilon: 1.0    steps: 40  evaluation reward: 23.03\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5620: Policy loss: 0.011730. Value loss: 0.230811. Entropy: 0.885577.\n",
      "Iteration 5621: Policy loss: -0.011999. Value loss: 0.107760. Entropy: 0.886866.\n",
      "Iteration 5622: Policy loss: -0.021122. Value loss: 0.067428. Entropy: 0.882493.\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5623: Policy loss: 0.011174. Value loss: 0.679705. Entropy: 1.006182.\n",
      "Iteration 5624: Policy loss: -0.004489. Value loss: 0.334063. Entropy: 0.974553.\n",
      "Iteration 5625: Policy loss: -0.017132. Value loss: 0.210521. Entropy: 0.980107.\n",
      "episode: 2669   score: 24.0  epsilon: 1.0    steps: 456  evaluation reward: 23.0\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5626: Policy loss: 0.009215. Value loss: 0.491959. Entropy: 1.001767.\n",
      "Iteration 5627: Policy loss: -0.005672. Value loss: 0.195996. Entropy: 0.978432.\n",
      "Iteration 5628: Policy loss: -0.012218. Value loss: 0.103492. Entropy: 0.980619.\n",
      "episode: 2670   score: 16.0  epsilon: 1.0    steps: 608  evaluation reward: 22.88\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5629: Policy loss: 0.014301. Value loss: 0.346855. Entropy: 1.009596.\n",
      "Iteration 5630: Policy loss: 0.007676. Value loss: 0.101343. Entropy: 1.014891.\n",
      "Iteration 5631: Policy loss: -0.009981. Value loss: 0.060842. Entropy: 0.980610.\n",
      "episode: 2671   score: 21.0  epsilon: 1.0    steps: 296  evaluation reward: 22.86\n",
      "episode: 2672   score: 23.0  epsilon: 1.0    steps: 672  evaluation reward: 22.98\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5632: Policy loss: 0.006002. Value loss: 0.386248. Entropy: 0.935735.\n",
      "Iteration 5633: Policy loss: -0.012577. Value loss: 0.150429. Entropy: 0.912037.\n",
      "Iteration 5634: Policy loss: -0.018025. Value loss: 0.094036. Entropy: 0.930658.\n",
      "episode: 2673   score: 13.0  epsilon: 1.0    steps: 464  evaluation reward: 22.95\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5635: Policy loss: 0.010641. Value loss: 0.783395. Entropy: 0.979803.\n",
      "Iteration 5636: Policy loss: -0.004950. Value loss: 0.354020. Entropy: 0.993629.\n",
      "Iteration 5637: Policy loss: -0.014422. Value loss: 0.176843. Entropy: 0.971138.\n",
      "episode: 2674   score: 25.0  epsilon: 1.0    steps: 320  evaluation reward: 22.88\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5638: Policy loss: 0.011257. Value loss: 0.807804. Entropy: 0.928900.\n",
      "Iteration 5639: Policy loss: -0.004176. Value loss: 0.336305. Entropy: 0.917033.\n",
      "Iteration 5640: Policy loss: -0.016675. Value loss: 0.215705. Entropy: 0.927351.\n",
      "episode: 2675   score: 24.0  epsilon: 1.0    steps: 528  evaluation reward: 22.89\n",
      "episode: 2676   score: 41.0  epsilon: 1.0    steps: 792  evaluation reward: 23.09\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5641: Policy loss: 0.025601. Value loss: 1.009385. Entropy: 0.882977.\n",
      "Iteration 5642: Policy loss: 0.002588. Value loss: 0.430795. Entropy: 0.887464.\n",
      "Iteration 5643: Policy loss: -0.015666. Value loss: 0.205851. Entropy: 0.901300.\n",
      "episode: 2677   score: 13.0  epsilon: 1.0    steps: 744  evaluation reward: 23.1\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5644: Policy loss: 0.004546. Value loss: 0.575366. Entropy: 1.015734.\n",
      "Iteration 5645: Policy loss: -0.002550. Value loss: 0.272405. Entropy: 1.016309.\n",
      "Iteration 5646: Policy loss: -0.010035. Value loss: 0.139924. Entropy: 1.014846.\n",
      "episode: 2678   score: 16.0  epsilon: 1.0    steps: 408  evaluation reward: 22.96\n",
      "episode: 2679   score: 15.0  epsilon: 1.0    steps: 624  evaluation reward: 22.95\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5647: Policy loss: 0.014155. Value loss: 0.805030. Entropy: 0.947367.\n",
      "Iteration 5648: Policy loss: -0.003188. Value loss: 0.443614. Entropy: 0.936041.\n",
      "Iteration 5649: Policy loss: -0.008048. Value loss: 0.302618. Entropy: 0.935295.\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5650: Policy loss: 0.012083. Value loss: 0.574065. Entropy: 0.913658.\n",
      "Iteration 5651: Policy loss: -0.007190. Value loss: 0.245682. Entropy: 0.907452.\n",
      "Iteration 5652: Policy loss: -0.019038. Value loss: 0.144038. Entropy: 0.895673.\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5653: Policy loss: 0.011958. Value loss: 0.758466. Entropy: 0.998909.\n",
      "Iteration 5654: Policy loss: -0.007631. Value loss: 0.324721. Entropy: 1.007325.\n",
      "Iteration 5655: Policy loss: -0.019096. Value loss: 0.194743. Entropy: 0.988925.\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5656: Policy loss: 0.007528. Value loss: 0.452569. Entropy: 0.974808.\n",
      "Iteration 5657: Policy loss: -0.010441. Value loss: 0.189677. Entropy: 0.971902.\n",
      "Iteration 5658: Policy loss: -0.016899. Value loss: 0.088177. Entropy: 0.963430.\n",
      "episode: 2680   score: 23.0  epsilon: 1.0    steps: 640  evaluation reward: 23.04\n",
      "episode: 2681   score: 29.0  epsilon: 1.0    steps: 760  evaluation reward: 22.86\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5659: Policy loss: 0.014317. Value loss: 0.857650. Entropy: 0.987036.\n",
      "Iteration 5660: Policy loss: 0.008873. Value loss: 0.326307. Entropy: 0.979792.\n",
      "Iteration 5661: Policy loss: -0.013449. Value loss: 0.174567. Entropy: 0.977283.\n",
      "episode: 2682   score: 24.0  epsilon: 1.0    steps: 896  evaluation reward: 22.89\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5662: Policy loss: 0.015273. Value loss: 0.855895. Entropy: 0.959661.\n",
      "Iteration 5663: Policy loss: -0.000646. Value loss: 0.426224. Entropy: 0.945662.\n",
      "Iteration 5664: Policy loss: -0.014104. Value loss: 0.231277. Entropy: 0.946716.\n",
      "episode: 2683   score: 38.0  epsilon: 1.0    steps: 600  evaluation reward: 23.0\n",
      "episode: 2684   score: 19.0  epsilon: 1.0    steps: 760  evaluation reward: 23.03\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5665: Policy loss: 0.024641. Value loss: 0.999018. Entropy: 0.935268.\n",
      "Iteration 5666: Policy loss: 0.003247. Value loss: 0.362504. Entropy: 0.938978.\n",
      "Iteration 5667: Policy loss: -0.008524. Value loss: 0.180767. Entropy: 0.937060.\n",
      "episode: 2685   score: 29.0  epsilon: 1.0    steps: 32  evaluation reward: 23.17\n",
      "episode: 2686   score: 20.0  epsilon: 1.0    steps: 624  evaluation reward: 23.17\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5668: Policy loss: 0.005299. Value loss: 0.475215. Entropy: 0.896905.\n",
      "Iteration 5669: Policy loss: -0.003444. Value loss: 0.166741. Entropy: 0.889924.\n",
      "Iteration 5670: Policy loss: -0.021518. Value loss: 0.100409. Entropy: 0.894873.\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5671: Policy loss: 0.011981. Value loss: 0.517097. Entropy: 0.889498.\n",
      "Iteration 5672: Policy loss: -0.011484. Value loss: 0.220327. Entropy: 0.875327.\n",
      "Iteration 5673: Policy loss: -0.022681. Value loss: 0.131588. Entropy: 0.868764.\n",
      "episode: 2687   score: 37.0  epsilon: 1.0    steps: 328  evaluation reward: 23.24\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5674: Policy loss: 0.012193. Value loss: 0.537330. Entropy: 0.910971.\n",
      "Iteration 5675: Policy loss: 0.002454. Value loss: 0.222647. Entropy: 0.897537.\n",
      "Iteration 5676: Policy loss: -0.012325. Value loss: 0.126301. Entropy: 0.893216.\n",
      "episode: 2688   score: 17.0  epsilon: 1.0    steps: 720  evaluation reward: 23.21\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5677: Policy loss: 0.010899. Value loss: 0.974793. Entropy: 1.001806.\n",
      "Iteration 5678: Policy loss: -0.005046. Value loss: 0.322639. Entropy: 0.997379.\n",
      "Iteration 5679: Policy loss: -0.016387. Value loss: 0.177566. Entropy: 0.993843.\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5680: Policy loss: 0.009769. Value loss: 0.684812. Entropy: 0.973330.\n",
      "Iteration 5681: Policy loss: -0.006157. Value loss: 0.332468. Entropy: 0.985362.\n",
      "Iteration 5682: Policy loss: -0.016000. Value loss: 0.187385. Entropy: 0.964869.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2689   score: 21.0  epsilon: 1.0    steps: 1016  evaluation reward: 23.09\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5683: Policy loss: 0.008068. Value loss: 0.732481. Entropy: 1.045250.\n",
      "Iteration 5684: Policy loss: -0.003662. Value loss: 0.287250. Entropy: 1.027311.\n",
      "Iteration 5685: Policy loss: -0.023566. Value loss: 0.139819. Entropy: 1.036740.\n",
      "episode: 2690   score: 19.0  epsilon: 1.0    steps: 456  evaluation reward: 22.98\n",
      "episode: 2691   score: 24.0  epsilon: 1.0    steps: 592  evaluation reward: 22.91\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5686: Policy loss: 0.014072. Value loss: 0.784855. Entropy: 0.981632.\n",
      "Iteration 5687: Policy loss: -0.000316. Value loss: 0.351455. Entropy: 0.958261.\n",
      "Iteration 5688: Policy loss: -0.011065. Value loss: 0.231054. Entropy: 0.969749.\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5689: Policy loss: 0.015657. Value loss: 0.376996. Entropy: 0.862228.\n",
      "Iteration 5690: Policy loss: -0.007850. Value loss: 0.146046. Entropy: 0.868453.\n",
      "Iteration 5691: Policy loss: -0.020153. Value loss: 0.082970. Entropy: 0.849710.\n",
      "episode: 2692   score: 22.0  epsilon: 1.0    steps: 352  evaluation reward: 22.95\n",
      "episode: 2693   score: 22.0  epsilon: 1.0    steps: 872  evaluation reward: 22.96\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5692: Policy loss: 0.009278. Value loss: 0.595733. Entropy: 0.864082.\n",
      "Iteration 5693: Policy loss: -0.005952. Value loss: 0.201755. Entropy: 0.846944.\n",
      "Iteration 5694: Policy loss: -0.017887. Value loss: 0.105212. Entropy: 0.864506.\n",
      "episode: 2694   score: 29.0  epsilon: 1.0    steps: 848  evaluation reward: 23.11\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5695: Policy loss: 0.011324. Value loss: 0.691837. Entropy: 0.924918.\n",
      "Iteration 5696: Policy loss: -0.000941. Value loss: 0.271442. Entropy: 0.921413.\n",
      "Iteration 5697: Policy loss: -0.012371. Value loss: 0.158086. Entropy: 0.913836.\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5698: Policy loss: 0.008929. Value loss: 0.595968. Entropy: 0.911660.\n",
      "Iteration 5699: Policy loss: -0.004708. Value loss: 0.239856. Entropy: 0.898085.\n",
      "Iteration 5700: Policy loss: -0.008011. Value loss: 0.162623. Entropy: 0.899726.\n",
      "episode: 2695   score: 32.0  epsilon: 1.0    steps: 488  evaluation reward: 23.16\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5701: Policy loss: 0.008856. Value loss: 0.421258. Entropy: 0.861301.\n",
      "Iteration 5702: Policy loss: -0.007631. Value loss: 0.182897. Entropy: 0.846817.\n",
      "Iteration 5703: Policy loss: -0.019751. Value loss: 0.097017. Entropy: 0.842428.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5704: Policy loss: 0.011664. Value loss: 1.072618. Entropy: 0.898558.\n",
      "Iteration 5705: Policy loss: 0.000306. Value loss: 0.544193. Entropy: 0.890737.\n",
      "Iteration 5706: Policy loss: -0.013355. Value loss: 0.278178. Entropy: 0.888436.\n",
      "episode: 2696   score: 19.0  epsilon: 1.0    steps: 328  evaluation reward: 23.17\n",
      "episode: 2697   score: 13.0  epsilon: 1.0    steps: 720  evaluation reward: 23.1\n",
      "episode: 2698   score: 24.0  epsilon: 1.0    steps: 824  evaluation reward: 23.14\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5707: Policy loss: 0.024317. Value loss: 0.963859. Entropy: 0.921861.\n",
      "Iteration 5708: Policy loss: 0.014813. Value loss: 0.413005. Entropy: 0.903768.\n",
      "Iteration 5709: Policy loss: -0.007556. Value loss: 0.210422. Entropy: 0.899741.\n",
      "episode: 2699   score: 12.0  epsilon: 1.0    steps: 312  evaluation reward: 23.06\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5710: Policy loss: 0.011001. Value loss: 0.674048. Entropy: 0.830834.\n",
      "Iteration 5711: Policy loss: -0.005635. Value loss: 0.319422. Entropy: 0.836514.\n",
      "Iteration 5712: Policy loss: -0.017643. Value loss: 0.220110. Entropy: 0.835902.\n",
      "episode: 2700   score: 40.0  epsilon: 1.0    steps: 112  evaluation reward: 23.29\n",
      "now time :  2019-03-06 14:30:00.849752\n",
      "episode: 2701   score: 30.0  epsilon: 1.0    steps: 544  evaluation reward: 23.41\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5713: Policy loss: 0.008145. Value loss: 0.760787. Entropy: 0.841313.\n",
      "Iteration 5714: Policy loss: -0.006752. Value loss: 0.371133. Entropy: 0.842426.\n",
      "Iteration 5715: Policy loss: -0.014437. Value loss: 0.225950. Entropy: 0.822393.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5716: Policy loss: 0.003931. Value loss: 0.450710. Entropy: 0.857895.\n",
      "Iteration 5717: Policy loss: -0.008009. Value loss: 0.157238. Entropy: 0.872140.\n",
      "Iteration 5718: Policy loss: -0.021061. Value loss: 0.092892. Entropy: 0.870398.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5719: Policy loss: 0.018799. Value loss: 1.007202. Entropy: 0.897916.\n",
      "Iteration 5720: Policy loss: 0.000345. Value loss: 0.480463. Entropy: 0.908940.\n",
      "Iteration 5721: Policy loss: -0.011888. Value loss: 0.282033. Entropy: 0.901950.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5722: Policy loss: 0.014707. Value loss: 0.713217. Entropy: 0.933530.\n",
      "Iteration 5723: Policy loss: -0.012786. Value loss: 0.238160. Entropy: 0.897452.\n",
      "Iteration 5724: Policy loss: -0.025756. Value loss: 0.123771. Entropy: 0.901863.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5725: Policy loss: 0.007225. Value loss: 0.682628. Entropy: 0.950557.\n",
      "Iteration 5726: Policy loss: -0.007572. Value loss: 0.234519. Entropy: 0.942079.\n",
      "Iteration 5727: Policy loss: -0.019428. Value loss: 0.125785. Entropy: 0.936878.\n",
      "episode: 2702   score: 25.0  epsilon: 1.0    steps: 72  evaluation reward: 23.51\n",
      "episode: 2703   score: 17.0  epsilon: 1.0    steps: 296  evaluation reward: 23.51\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5728: Policy loss: 0.016223. Value loss: 0.533430. Entropy: 0.966106.\n",
      "Iteration 5729: Policy loss: -0.004654. Value loss: 0.180064. Entropy: 0.967055.\n",
      "Iteration 5730: Policy loss: -0.017143. Value loss: 0.106533. Entropy: 0.976492.\n",
      "episode: 2704   score: 16.0  epsilon: 1.0    steps: 568  evaluation reward: 23.42\n",
      "episode: 2705   score: 27.0  epsilon: 1.0    steps: 744  evaluation reward: 23.55\n",
      "episode: 2706   score: 27.0  epsilon: 1.0    steps: 880  evaluation reward: 23.64\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5731: Policy loss: 0.014954. Value loss: 0.598356. Entropy: 0.990272.\n",
      "Iteration 5732: Policy loss: -0.003651. Value loss: 0.201618. Entropy: 0.983355.\n",
      "Iteration 5733: Policy loss: -0.014980. Value loss: 0.109855. Entropy: 0.996852.\n",
      "episode: 2707   score: 19.0  epsilon: 1.0    steps: 760  evaluation reward: 23.5\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5734: Policy loss: 0.009270. Value loss: 0.702870. Entropy: 0.902413.\n",
      "Iteration 5735: Policy loss: 0.000939. Value loss: 0.326532. Entropy: 0.894098.\n",
      "Iteration 5736: Policy loss: -0.018669. Value loss: 0.160058. Entropy: 0.894872.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5737: Policy loss: 0.009113. Value loss: 0.640767. Entropy: 0.900935.\n",
      "Iteration 5738: Policy loss: -0.006184. Value loss: 0.266715. Entropy: 0.901265.\n",
      "Iteration 5739: Policy loss: -0.017280. Value loss: 0.151679. Entropy: 0.901684.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5740: Policy loss: 0.012163. Value loss: 0.627869. Entropy: 0.962398.\n",
      "Iteration 5741: Policy loss: -0.005974. Value loss: 0.192895. Entropy: 0.946034.\n",
      "Iteration 5742: Policy loss: -0.019444. Value loss: 0.122114. Entropy: 0.950512.\n",
      "episode: 2708   score: 30.0  epsilon: 1.0    steps: 128  evaluation reward: 23.48\n",
      "episode: 2709   score: 61.0  epsilon: 1.0    steps: 192  evaluation reward: 23.83\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5743: Policy loss: 0.008141. Value loss: 0.479182. Entropy: 0.894662.\n",
      "Iteration 5744: Policy loss: -0.007502. Value loss: 0.219467. Entropy: 0.906021.\n",
      "Iteration 5745: Policy loss: -0.020084. Value loss: 0.137584. Entropy: 0.895352.\n",
      "episode: 2710   score: 11.0  epsilon: 1.0    steps: 656  evaluation reward: 23.75\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5746: Policy loss: 0.013799. Value loss: 0.474073. Entropy: 0.904162.\n",
      "Iteration 5747: Policy loss: -0.008260. Value loss: 0.219812. Entropy: 0.894448.\n",
      "Iteration 5748: Policy loss: -0.016646. Value loss: 0.130271. Entropy: 0.899733.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5749: Policy loss: 0.008352. Value loss: 0.428316. Entropy: 0.935090.\n",
      "Iteration 5750: Policy loss: -0.005511. Value loss: 0.193479. Entropy: 0.932868.\n",
      "Iteration 5751: Policy loss: -0.019097. Value loss: 0.106502. Entropy: 0.923642.\n",
      "episode: 2711   score: 15.0  epsilon: 1.0    steps: 248  evaluation reward: 23.65\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5752: Policy loss: 0.007226. Value loss: 0.998216. Entropy: 0.898218.\n",
      "Iteration 5753: Policy loss: -0.004161. Value loss: 0.314265. Entropy: 0.903094.\n",
      "Iteration 5754: Policy loss: -0.015209. Value loss: 0.132831. Entropy: 0.914861.\n",
      "episode: 2712   score: 26.0  epsilon: 1.0    steps: 384  evaluation reward: 23.71\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5755: Policy loss: 0.014963. Value loss: 0.765442. Entropy: 0.927341.\n",
      "Iteration 5756: Policy loss: 0.001256. Value loss: 0.328476. Entropy: 0.932045.\n",
      "Iteration 5757: Policy loss: -0.017420. Value loss: 0.187634. Entropy: 0.928043.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5758: Policy loss: 0.012438. Value loss: 0.470701. Entropy: 0.929376.\n",
      "Iteration 5759: Policy loss: -0.006937. Value loss: 0.169839. Entropy: 0.925157.\n",
      "Iteration 5760: Policy loss: -0.021481. Value loss: 0.094880. Entropy: 0.927771.\n",
      "episode: 2713   score: 15.0  epsilon: 1.0    steps: 904  evaluation reward: 23.6\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5761: Policy loss: 0.016651. Value loss: 0.767532. Entropy: 0.979621.\n",
      "Iteration 5762: Policy loss: -0.001285. Value loss: 0.359329. Entropy: 0.974747.\n",
      "Iteration 5763: Policy loss: -0.014458. Value loss: 0.191679. Entropy: 0.947930.\n",
      "episode: 2714   score: 45.0  epsilon: 1.0    steps: 384  evaluation reward: 23.81\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5764: Policy loss: 0.006532. Value loss: 0.704197. Entropy: 0.881899.\n",
      "Iteration 5765: Policy loss: -0.003698. Value loss: 0.307291. Entropy: 0.890685.\n",
      "Iteration 5766: Policy loss: -0.010410. Value loss: 0.155263. Entropy: 0.884993.\n",
      "episode: 2715   score: 21.0  epsilon: 1.0    steps: 384  evaluation reward: 23.75\n",
      "episode: 2716   score: 38.0  epsilon: 1.0    steps: 984  evaluation reward: 23.9\n",
      "episode: 2717   score: 43.0  epsilon: 1.0    steps: 984  evaluation reward: 23.89\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5767: Policy loss: 0.017246. Value loss: 0.977187. Entropy: 0.910166.\n",
      "Iteration 5768: Policy loss: -0.000193. Value loss: 0.435972. Entropy: 0.898939.\n",
      "Iteration 5769: Policy loss: -0.004801. Value loss: 0.197370. Entropy: 0.881928.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5770: Policy loss: 0.015940. Value loss: 0.354071. Entropy: 0.821048.\n",
      "Iteration 5771: Policy loss: -0.003693. Value loss: 0.168692. Entropy: 0.822603.\n",
      "Iteration 5772: Policy loss: -0.016168. Value loss: 0.099662. Entropy: 0.812084.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5773: Policy loss: 0.014943. Value loss: 0.665501. Entropy: 0.859407.\n",
      "Iteration 5774: Policy loss: 0.001158. Value loss: 0.313325. Entropy: 0.870215.\n",
      "Iteration 5775: Policy loss: -0.011840. Value loss: 0.200839. Entropy: 0.859116.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5776: Policy loss: 0.013521. Value loss: 0.593917. Entropy: 0.902583.\n",
      "Iteration 5777: Policy loss: -0.011710. Value loss: 0.246448. Entropy: 0.898711.\n",
      "Iteration 5778: Policy loss: -0.021771. Value loss: 0.143583. Entropy: 0.881367.\n",
      "episode: 2718   score: 31.0  epsilon: 1.0    steps: 592  evaluation reward: 23.98\n",
      "episode: 2719   score: 38.0  epsilon: 1.0    steps: 752  evaluation reward: 24.03\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5779: Policy loss: 0.016452. Value loss: 0.677596. Entropy: 0.933138.\n",
      "Iteration 5780: Policy loss: 0.000560. Value loss: 0.266581. Entropy: 0.931220.\n",
      "Iteration 5781: Policy loss: -0.009213. Value loss: 0.134775. Entropy: 0.933204.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5782: Policy loss: 0.012834. Value loss: 0.735624. Entropy: 0.920114.\n",
      "Iteration 5783: Policy loss: -0.005772. Value loss: 0.374887. Entropy: 0.924087.\n",
      "Iteration 5784: Policy loss: -0.014533. Value loss: 0.222580. Entropy: 0.912662.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5785: Policy loss: 0.003336. Value loss: 0.350613. Entropy: 0.921054.\n",
      "Iteration 5786: Policy loss: -0.009685. Value loss: 0.143080. Entropy: 0.910292.\n",
      "Iteration 5787: Policy loss: -0.021712. Value loss: 0.096363. Entropy: 0.916072.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5788: Policy loss: 0.005708. Value loss: 0.582921. Entropy: 0.904711.\n",
      "Iteration 5789: Policy loss: -0.006861. Value loss: 0.312368. Entropy: 0.883246.\n",
      "Iteration 5790: Policy loss: -0.021317. Value loss: 0.160320. Entropy: 0.873332.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5791: Policy loss: 0.008856. Value loss: 1.105228. Entropy: 1.030188.\n",
      "Iteration 5792: Policy loss: -0.002952. Value loss: 0.444344. Entropy: 0.997685.\n",
      "Iteration 5793: Policy loss: -0.017656. Value loss: 0.230196. Entropy: 0.999365.\n",
      "episode: 2720   score: 23.0  epsilon: 1.0    steps: 328  evaluation reward: 23.91\n",
      "episode: 2721   score: 46.0  epsilon: 1.0    steps: 624  evaluation reward: 23.84\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5794: Policy loss: 0.011392. Value loss: 1.076250. Entropy: 0.935253.\n",
      "Iteration 5795: Policy loss: 0.004830. Value loss: 0.400769. Entropy: 0.946895.\n",
      "Iteration 5796: Policy loss: -0.004062. Value loss: 0.220589. Entropy: 0.941761.\n",
      "episode: 2722   score: 28.0  epsilon: 1.0    steps: 344  evaluation reward: 23.99\n",
      "episode: 2723   score: 13.0  epsilon: 1.0    steps: 520  evaluation reward: 23.9\n",
      "episode: 2724   score: 40.0  epsilon: 1.0    steps: 544  evaluation reward: 24.03\n",
      "episode: 2725   score: 36.0  epsilon: 1.0    steps: 752  evaluation reward: 24.21\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5797: Policy loss: 0.017415. Value loss: 1.135395. Entropy: 0.894099.\n",
      "Iteration 5798: Policy loss: -0.002140. Value loss: 0.462565. Entropy: 0.878783.\n",
      "Iteration 5799: Policy loss: -0.014436. Value loss: 0.261295. Entropy: 0.887413.\n",
      "episode: 2726   score: 38.0  epsilon: 1.0    steps: 152  evaluation reward: 24.1\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5800: Policy loss: 0.015648. Value loss: 0.368135. Entropy: 0.851294.\n",
      "Iteration 5801: Policy loss: -0.003537. Value loss: 0.210243. Entropy: 0.841896.\n",
      "Iteration 5802: Policy loss: -0.015123. Value loss: 0.157215. Entropy: 0.845268.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5803: Policy loss: 0.007044. Value loss: 0.650195. Entropy: 0.893645.\n",
      "Iteration 5804: Policy loss: -0.009691. Value loss: 0.302122. Entropy: 0.897977.\n",
      "Iteration 5805: Policy loss: -0.012871. Value loss: 0.126780. Entropy: 0.898693.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5806: Policy loss: 0.012376. Value loss: 0.285510. Entropy: 1.006995.\n",
      "Iteration 5807: Policy loss: -0.011621. Value loss: 0.101818. Entropy: 1.027272.\n",
      "Iteration 5808: Policy loss: -0.024561. Value loss: 0.049828. Entropy: 1.020536.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5809: Policy loss: 0.009953. Value loss: 0.442593. Entropy: 0.925410.\n",
      "Iteration 5810: Policy loss: -0.008263. Value loss: 0.193938. Entropy: 0.929991.\n",
      "Iteration 5811: Policy loss: -0.020040. Value loss: 0.092167. Entropy: 0.916388.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5812: Policy loss: 0.017048. Value loss: 0.870279. Entropy: 0.981308.\n",
      "Iteration 5813: Policy loss: 0.001633. Value loss: 0.323658. Entropy: 0.953631.\n",
      "Iteration 5814: Policy loss: -0.005750. Value loss: 0.168407. Entropy: 0.972604.\n",
      "episode: 2727   score: 25.0  epsilon: 1.0    steps: 672  evaluation reward: 24.13\n",
      "episode: 2728   score: 41.0  epsilon: 1.0    steps: 840  evaluation reward: 24.15\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5815: Policy loss: 0.012181. Value loss: 0.878046. Entropy: 0.941817.\n",
      "Iteration 5816: Policy loss: -0.003717. Value loss: 0.308468. Entropy: 0.951778.\n",
      "Iteration 5817: Policy loss: -0.015286. Value loss: 0.160229. Entropy: 0.953724.\n",
      "episode: 2729   score: 17.0  epsilon: 1.0    steps: 8  evaluation reward: 24.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2730   score: 22.0  epsilon: 1.0    steps: 968  evaluation reward: 24.15\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5818: Policy loss: 0.018762. Value loss: 0.767001. Entropy: 0.938084.\n",
      "Iteration 5819: Policy loss: 0.000582. Value loss: 0.361371. Entropy: 0.952686.\n",
      "Iteration 5820: Policy loss: -0.019246. Value loss: 0.218678. Entropy: 0.935936.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5821: Policy loss: 0.007929. Value loss: 0.717999. Entropy: 0.905859.\n",
      "Iteration 5822: Policy loss: -0.007731. Value loss: 0.302599. Entropy: 0.912348.\n",
      "Iteration 5823: Policy loss: -0.016245. Value loss: 0.171096. Entropy: 0.918392.\n",
      "episode: 2731   score: 23.0  epsilon: 1.0    steps: 136  evaluation reward: 24.19\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5824: Policy loss: 0.009615. Value loss: 0.671597. Entropy: 0.908704.\n",
      "Iteration 5825: Policy loss: -0.004207. Value loss: 0.285005. Entropy: 0.902251.\n",
      "Iteration 5826: Policy loss: -0.014659. Value loss: 0.128012. Entropy: 0.894018.\n",
      "episode: 2732   score: 32.0  epsilon: 1.0    steps: 568  evaluation reward: 24.27\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5827: Policy loss: 0.008650. Value loss: 0.648772. Entropy: 0.949279.\n",
      "Iteration 5828: Policy loss: -0.002904. Value loss: 0.237709. Entropy: 0.938531.\n",
      "Iteration 5829: Policy loss: -0.019493. Value loss: 0.126676. Entropy: 0.949971.\n",
      "episode: 2733   score: 38.0  epsilon: 1.0    steps: 616  evaluation reward: 24.41\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5830: Policy loss: 0.005997. Value loss: 0.415451. Entropy: 0.922959.\n",
      "Iteration 5831: Policy loss: -0.009882. Value loss: 0.156690. Entropy: 0.913273.\n",
      "Iteration 5832: Policy loss: -0.021448. Value loss: 0.083863. Entropy: 0.916502.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5833: Policy loss: 0.009107. Value loss: 0.528476. Entropy: 0.906251.\n",
      "Iteration 5834: Policy loss: -0.005549. Value loss: 0.216480. Entropy: 0.895872.\n",
      "Iteration 5835: Policy loss: -0.016823. Value loss: 0.113504. Entropy: 0.914393.\n",
      "episode: 2734   score: 37.0  epsilon: 1.0    steps: 200  evaluation reward: 24.5\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5836: Policy loss: 0.011187. Value loss: 0.522971. Entropy: 0.924284.\n",
      "Iteration 5837: Policy loss: -0.002274. Value loss: 0.220662. Entropy: 0.945972.\n",
      "Iteration 5838: Policy loss: -0.013919. Value loss: 0.098450. Entropy: 0.925181.\n",
      "episode: 2735   score: 21.0  epsilon: 1.0    steps: 824  evaluation reward: 24.48\n",
      "episode: 2736   score: 21.0  epsilon: 1.0    steps: 872  evaluation reward: 24.36\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5839: Policy loss: 0.010281. Value loss: 0.771107. Entropy: 0.991888.\n",
      "Iteration 5840: Policy loss: -0.004809. Value loss: 0.314430. Entropy: 0.971603.\n",
      "Iteration 5841: Policy loss: -0.019893. Value loss: 0.174710. Entropy: 0.972890.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5842: Policy loss: 0.013888. Value loss: 0.469582. Entropy: 0.871448.\n",
      "Iteration 5843: Policy loss: 0.004327. Value loss: 0.153390. Entropy: 0.896367.\n",
      "Iteration 5844: Policy loss: -0.014714. Value loss: 0.082035. Entropy: 0.882996.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5845: Policy loss: 0.024696. Value loss: 0.965521. Entropy: 0.946386.\n",
      "Iteration 5846: Policy loss: 0.004442. Value loss: 0.346730. Entropy: 0.981055.\n",
      "Iteration 5847: Policy loss: -0.010192. Value loss: 0.214378. Entropy: 0.960293.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5848: Policy loss: 0.013914. Value loss: 0.666359. Entropy: 0.954539.\n",
      "Iteration 5849: Policy loss: 0.008642. Value loss: 0.246997. Entropy: 0.957576.\n",
      "Iteration 5850: Policy loss: -0.008594. Value loss: 0.128349. Entropy: 0.962317.\n",
      "episode: 2737   score: 43.0  epsilon: 1.0    steps: 288  evaluation reward: 24.49\n",
      "episode: 2738   score: 19.0  epsilon: 1.0    steps: 744  evaluation reward: 24.45\n",
      "episode: 2739   score: 38.0  epsilon: 1.0    steps: 848  evaluation reward: 24.52\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5851: Policy loss: 0.014519. Value loss: 0.851103. Entropy: 0.996780.\n",
      "Iteration 5852: Policy loss: -0.001474. Value loss: 0.352476. Entropy: 1.006289.\n",
      "Iteration 5853: Policy loss: -0.014355. Value loss: 0.211904. Entropy: 1.012502.\n",
      "episode: 2740   score: 51.0  epsilon: 1.0    steps: 928  evaluation reward: 24.85\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5854: Policy loss: 0.010025. Value loss: 1.423580. Entropy: 0.968316.\n",
      "Iteration 5855: Policy loss: 0.010680. Value loss: 0.772771. Entropy: 0.938429.\n",
      "Iteration 5856: Policy loss: 0.003390. Value loss: 0.502634. Entropy: 0.921820.\n",
      "episode: 2741   score: 25.0  epsilon: 1.0    steps: 8  evaluation reward: 24.95\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5857: Policy loss: 0.004914. Value loss: 0.770262. Entropy: 0.901710.\n",
      "Iteration 5858: Policy loss: -0.006093. Value loss: 0.340081. Entropy: 0.909567.\n",
      "Iteration 5859: Policy loss: -0.018745. Value loss: 0.196424. Entropy: 0.921386.\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5860: Policy loss: 0.010176. Value loss: 0.399072. Entropy: 0.984877.\n",
      "Iteration 5861: Policy loss: -0.010650. Value loss: 0.198729. Entropy: 0.985207.\n",
      "Iteration 5862: Policy loss: -0.022531. Value loss: 0.111414. Entropy: 0.982058.\n",
      "episode: 2742   score: 27.0  epsilon: 1.0    steps: 520  evaluation reward: 24.79\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5863: Policy loss: 0.009286. Value loss: 0.680028. Entropy: 0.960161.\n",
      "Iteration 5864: Policy loss: -0.001567. Value loss: 0.272934. Entropy: 0.970081.\n",
      "Iteration 5865: Policy loss: -0.019382. Value loss: 0.202874. Entropy: 0.964904.\n",
      "episode: 2743   score: 35.0  epsilon: 1.0    steps: 480  evaluation reward: 25.03\n",
      "episode: 2744   score: 10.0  epsilon: 1.0    steps: 744  evaluation reward: 25.0\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5866: Policy loss: 0.009215. Value loss: 0.670994. Entropy: 0.907492.\n",
      "Iteration 5867: Policy loss: -0.002856. Value loss: 0.302200. Entropy: 0.910659.\n",
      "Iteration 5868: Policy loss: -0.015547. Value loss: 0.178475. Entropy: 0.921670.\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5869: Policy loss: 0.009690. Value loss: 0.765384. Entropy: 0.935641.\n",
      "Iteration 5870: Policy loss: 0.001466. Value loss: 0.353937. Entropy: 0.924440.\n",
      "Iteration 5871: Policy loss: -0.013037. Value loss: 0.205402. Entropy: 0.912008.\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5872: Policy loss: 0.012104. Value loss: 0.826057. Entropy: 0.988803.\n",
      "Iteration 5873: Policy loss: -0.006628. Value loss: 0.382673. Entropy: 0.966686.\n",
      "Iteration 5874: Policy loss: -0.021270. Value loss: 0.248039. Entropy: 0.960799.\n",
      "episode: 2745   score: 22.0  epsilon: 1.0    steps: 688  evaluation reward: 24.98\n",
      "episode: 2746   score: 35.0  epsilon: 1.0    steps: 856  evaluation reward: 25.07\n",
      "episode: 2747   score: 14.0  epsilon: 1.0    steps: 864  evaluation reward: 25.1\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5875: Policy loss: 0.020476. Value loss: 1.203904. Entropy: 0.978300.\n",
      "Iteration 5876: Policy loss: 0.004935. Value loss: 0.485650. Entropy: 0.966797.\n",
      "Iteration 5877: Policy loss: -0.010190. Value loss: 0.272431. Entropy: 0.967836.\n",
      "episode: 2748   score: 26.0  epsilon: 1.0    steps: 440  evaluation reward: 25.11\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5878: Policy loss: 0.009007. Value loss: 0.658768. Entropy: 0.934804.\n",
      "Iteration 5879: Policy loss: -0.008240. Value loss: 0.251424. Entropy: 0.918309.\n",
      "Iteration 5880: Policy loss: -0.021160. Value loss: 0.145540. Entropy: 0.924367.\n",
      "episode: 2749   score: 14.0  epsilon: 1.0    steps: 312  evaluation reward: 24.85\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5881: Policy loss: 0.013338. Value loss: 0.632660. Entropy: 0.928720.\n",
      "Iteration 5882: Policy loss: -0.005316. Value loss: 0.234893. Entropy: 0.908034.\n",
      "Iteration 5883: Policy loss: -0.020038. Value loss: 0.120044. Entropy: 0.908467.\n",
      "episode: 2750   score: 11.0  epsilon: 1.0    steps: 32  evaluation reward: 24.83\n",
      "now time :  2019-03-06 14:33:38.115669\n",
      "episode: 2751   score: 29.0  epsilon: 1.0    steps: 152  evaluation reward: 25.04\n",
      "episode: 2752   score: 13.0  epsilon: 1.0    steps: 800  evaluation reward: 24.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5884: Policy loss: 0.009313. Value loss: 0.695065. Entropy: 0.931210.\n",
      "Iteration 5885: Policy loss: -0.000753. Value loss: 0.296006. Entropy: 0.941197.\n",
      "Iteration 5886: Policy loss: -0.013142. Value loss: 0.207896. Entropy: 0.931031.\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5887: Policy loss: 0.006923. Value loss: 0.360644. Entropy: 0.903348.\n",
      "Iteration 5888: Policy loss: -0.014649. Value loss: 0.183015. Entropy: 0.907450.\n",
      "Iteration 5889: Policy loss: -0.021845. Value loss: 0.107670. Entropy: 0.901606.\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5890: Policy loss: 0.013889. Value loss: 0.578570. Entropy: 0.907335.\n",
      "Iteration 5891: Policy loss: -0.010879. Value loss: 0.260236. Entropy: 0.924907.\n",
      "Iteration 5892: Policy loss: -0.021105. Value loss: 0.170100. Entropy: 0.909412.\n",
      "episode: 2753   score: 14.0  epsilon: 1.0    steps: 784  evaluation reward: 24.61\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5893: Policy loss: 0.016047. Value loss: 0.566481. Entropy: 0.925922.\n",
      "Iteration 5894: Policy loss: -0.000211. Value loss: 0.182599. Entropy: 0.927790.\n",
      "Iteration 5895: Policy loss: -0.018191. Value loss: 0.111653. Entropy: 0.934166.\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5896: Policy loss: 0.004408. Value loss: 0.317742. Entropy: 0.998761.\n",
      "Iteration 5897: Policy loss: -0.017534. Value loss: 0.102674. Entropy: 1.007250.\n",
      "Iteration 5898: Policy loss: -0.028371. Value loss: 0.054879. Entropy: 0.996689.\n",
      "episode: 2754   score: 19.0  epsilon: 1.0    steps: 184  evaluation reward: 24.63\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5899: Policy loss: 0.012843. Value loss: 0.668962. Entropy: 0.929261.\n",
      "Iteration 5900: Policy loss: 0.002623. Value loss: 0.249381. Entropy: 0.936880.\n",
      "Iteration 5901: Policy loss: -0.013503. Value loss: 0.128612. Entropy: 0.930667.\n",
      "episode: 2755   score: 23.0  epsilon: 1.0    steps: 120  evaluation reward: 24.57\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5902: Policy loss: 0.011508. Value loss: 0.477119. Entropy: 1.007711.\n",
      "Iteration 5903: Policy loss: -0.014264. Value loss: 0.189353. Entropy: 1.001152.\n",
      "Iteration 5904: Policy loss: -0.021210. Value loss: 0.100062. Entropy: 0.986830.\n",
      "episode: 2756   score: 28.0  epsilon: 1.0    steps: 568  evaluation reward: 24.68\n",
      "episode: 2757   score: 23.0  epsilon: 1.0    steps: 760  evaluation reward: 24.82\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5905: Policy loss: 0.016623. Value loss: 0.748989. Entropy: 0.953066.\n",
      "Iteration 5906: Policy loss: -0.002495. Value loss: 0.294432. Entropy: 0.964496.\n",
      "Iteration 5907: Policy loss: -0.014354. Value loss: 0.172998. Entropy: 0.967381.\n",
      "episode: 2758   score: 31.0  epsilon: 1.0    steps: 720  evaluation reward: 24.92\n",
      "episode: 2759   score: 21.0  epsilon: 1.0    steps: 872  evaluation reward: 24.98\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5908: Policy loss: 0.011339. Value loss: 0.515146. Entropy: 0.960590.\n",
      "Iteration 5909: Policy loss: -0.005408. Value loss: 0.172453. Entropy: 0.967162.\n",
      "Iteration 5910: Policy loss: -0.014184. Value loss: 0.100781. Entropy: 0.956376.\n",
      "episode: 2760   score: 35.0  epsilon: 1.0    steps: 576  evaluation reward: 25.13\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5911: Policy loss: 0.013189. Value loss: 0.720857. Entropy: 0.932904.\n",
      "Iteration 5912: Policy loss: 0.000815. Value loss: 0.315800. Entropy: 0.910238.\n",
      "Iteration 5913: Policy loss: -0.012082. Value loss: 0.183973. Entropy: 0.926293.\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5914: Policy loss: 0.008059. Value loss: 0.373436. Entropy: 0.939482.\n",
      "Iteration 5915: Policy loss: -0.004738. Value loss: 0.179770. Entropy: 0.944369.\n",
      "Iteration 5916: Policy loss: -0.018508. Value loss: 0.114542. Entropy: 0.937924.\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5917: Policy loss: 0.007189. Value loss: 0.403378. Entropy: 0.997748.\n",
      "Iteration 5918: Policy loss: -0.008443. Value loss: 0.170211. Entropy: 0.998348.\n",
      "Iteration 5919: Policy loss: -0.021614. Value loss: 0.106212. Entropy: 0.991636.\n",
      "episode: 2761   score: 29.0  epsilon: 1.0    steps: 128  evaluation reward: 25.24\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5920: Policy loss: 0.011475. Value loss: 1.079660. Entropy: 0.957591.\n",
      "Iteration 5921: Policy loss: -0.002606. Value loss: 0.510370. Entropy: 0.948055.\n",
      "Iteration 5922: Policy loss: -0.017220. Value loss: 0.300374. Entropy: 0.951504.\n",
      "episode: 2762   score: 14.0  epsilon: 1.0    steps: 8  evaluation reward: 25.16\n",
      "episode: 2763   score: 18.0  epsilon: 1.0    steps: 616  evaluation reward: 25.19\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5923: Policy loss: 0.014651. Value loss: 0.730667. Entropy: 0.921672.\n",
      "Iteration 5924: Policy loss: -0.005223. Value loss: 0.287726. Entropy: 0.924713.\n",
      "Iteration 5925: Policy loss: -0.022512. Value loss: 0.170712. Entropy: 0.932726.\n",
      "episode: 2764   score: 15.0  epsilon: 1.0    steps: 736  evaluation reward: 25.24\n",
      "episode: 2765   score: 30.0  epsilon: 1.0    steps: 800  evaluation reward: 25.45\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5926: Policy loss: 0.028298. Value loss: 0.729715. Entropy: 0.969847.\n",
      "Iteration 5927: Policy loss: 0.004238. Value loss: 0.284328. Entropy: 0.962723.\n",
      "Iteration 5928: Policy loss: -0.013841. Value loss: 0.154375. Entropy: 0.976485.\n",
      "episode: 2766   score: 14.0  epsilon: 1.0    steps: 496  evaluation reward: 25.4\n",
      "episode: 2767   score: 21.0  epsilon: 1.0    steps: 512  evaluation reward: 25.46\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5929: Policy loss: 0.012682. Value loss: 0.418855. Entropy: 0.908589.\n",
      "Iteration 5930: Policy loss: -0.004489. Value loss: 0.154207. Entropy: 0.915256.\n",
      "Iteration 5931: Policy loss: -0.018752. Value loss: 0.081315. Entropy: 0.882835.\n",
      "episode: 2768   score: 17.0  epsilon: 1.0    steps: 336  evaluation reward: 25.44\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5932: Policy loss: 0.011407. Value loss: 0.615668. Entropy: 0.749124.\n",
      "Iteration 5933: Policy loss: -0.006252. Value loss: 0.202252. Entropy: 0.738954.\n",
      "Iteration 5934: Policy loss: -0.017474. Value loss: 0.092757. Entropy: 0.741659.\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5935: Policy loss: 0.010639. Value loss: 0.555492. Entropy: 0.820639.\n",
      "Iteration 5936: Policy loss: -0.004056. Value loss: 0.268367. Entropy: 0.835857.\n",
      "Iteration 5937: Policy loss: -0.018075. Value loss: 0.173623. Entropy: 0.817597.\n",
      "episode: 2769   score: 26.0  epsilon: 1.0    steps: 960  evaluation reward: 25.46\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5938: Policy loss: 0.012405. Value loss: 0.484924. Entropy: 0.894311.\n",
      "Iteration 5939: Policy loss: -0.006513. Value loss: 0.212201. Entropy: 0.907117.\n",
      "Iteration 5940: Policy loss: -0.018364. Value loss: 0.123045. Entropy: 0.899629.\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5941: Policy loss: 0.012415. Value loss: 0.357913. Entropy: 0.886873.\n",
      "Iteration 5942: Policy loss: -0.006485. Value loss: 0.152928. Entropy: 0.896195.\n",
      "Iteration 5943: Policy loss: -0.018588. Value loss: 0.081075. Entropy: 0.898025.\n",
      "episode: 2770   score: 18.0  epsilon: 1.0    steps: 744  evaluation reward: 25.48\n",
      "episode: 2771   score: 15.0  epsilon: 1.0    steps: 752  evaluation reward: 25.42\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5944: Policy loss: 0.020488. Value loss: 0.958086. Entropy: 0.861782.\n",
      "Iteration 5945: Policy loss: 0.005033. Value loss: 0.266601. Entropy: 0.859404.\n",
      "Iteration 5946: Policy loss: -0.014138. Value loss: 0.129110. Entropy: 0.872501.\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5947: Policy loss: 0.006150. Value loss: 0.353352. Entropy: 0.880875.\n",
      "Iteration 5948: Policy loss: -0.013861. Value loss: 0.143487. Entropy: 0.894802.\n",
      "Iteration 5949: Policy loss: -0.026862. Value loss: 0.091845. Entropy: 0.884391.\n",
      "episode: 2772   score: 14.0  epsilon: 1.0    steps: 32  evaluation reward: 25.33\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5950: Policy loss: 0.014671. Value loss: 0.610095. Entropy: 0.859182.\n",
      "Iteration 5951: Policy loss: -0.006866. Value loss: 0.348771. Entropy: 0.876661.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5952: Policy loss: -0.014720. Value loss: 0.220884. Entropy: 0.858049.\n",
      "episode: 2773   score: 39.0  epsilon: 1.0    steps: 664  evaluation reward: 25.59\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5953: Policy loss: 0.009088. Value loss: 0.918803. Entropy: 0.875249.\n",
      "Iteration 5954: Policy loss: -0.002769. Value loss: 0.388700. Entropy: 0.870289.\n",
      "Iteration 5955: Policy loss: -0.013630. Value loss: 0.185295. Entropy: 0.893543.\n",
      "episode: 2774   score: 33.0  epsilon: 1.0    steps: 312  evaluation reward: 25.67\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5956: Policy loss: 0.006388. Value loss: 0.728494. Entropy: 0.879322.\n",
      "Iteration 5957: Policy loss: -0.003646. Value loss: 0.351877. Entropy: 0.909075.\n",
      "Iteration 5958: Policy loss: -0.015010. Value loss: 0.213084. Entropy: 0.906478.\n",
      "episode: 2775   score: 29.0  epsilon: 1.0    steps: 160  evaluation reward: 25.72\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5959: Policy loss: 0.020407. Value loss: 0.389311. Entropy: 0.896976.\n",
      "Iteration 5960: Policy loss: -0.001605. Value loss: 0.134934. Entropy: 0.870213.\n",
      "Iteration 5961: Policy loss: -0.016445. Value loss: 0.078972. Entropy: 0.884994.\n",
      "episode: 2776   score: 18.0  epsilon: 1.0    steps: 696  evaluation reward: 25.49\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5962: Policy loss: 0.012179. Value loss: 0.710120. Entropy: 0.870729.\n",
      "Iteration 5963: Policy loss: -0.009669. Value loss: 0.351516. Entropy: 0.874691.\n",
      "Iteration 5964: Policy loss: -0.020691. Value loss: 0.205734. Entropy: 0.858992.\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5965: Policy loss: 0.005648. Value loss: 0.607954. Entropy: 0.820437.\n",
      "Iteration 5966: Policy loss: -0.002892. Value loss: 0.242213. Entropy: 0.823238.\n",
      "Iteration 5967: Policy loss: -0.013141. Value loss: 0.126953. Entropy: 0.811106.\n",
      "episode: 2777   score: 45.0  epsilon: 1.0    steps: 232  evaluation reward: 25.81\n",
      "episode: 2778   score: 10.0  epsilon: 1.0    steps: 728  evaluation reward: 25.75\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5968: Policy loss: 0.016395. Value loss: 1.392898. Entropy: 0.909677.\n",
      "Iteration 5969: Policy loss: -0.000283. Value loss: 0.540980. Entropy: 0.889516.\n",
      "Iteration 5970: Policy loss: -0.008463. Value loss: 0.265560. Entropy: 0.910084.\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5971: Policy loss: 0.008596. Value loss: 0.519595. Entropy: 0.917092.\n",
      "Iteration 5972: Policy loss: -0.012901. Value loss: 0.215124. Entropy: 0.910435.\n",
      "Iteration 5973: Policy loss: -0.028491. Value loss: 0.130046. Entropy: 0.917956.\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5974: Policy loss: 0.014324. Value loss: 0.701171. Entropy: 0.906154.\n",
      "Iteration 5975: Policy loss: -0.004190. Value loss: 0.323134. Entropy: 0.900971.\n",
      "Iteration 5976: Policy loss: -0.019169. Value loss: 0.189770. Entropy: 0.899924.\n",
      "episode: 2779   score: 22.0  epsilon: 1.0    steps: 16  evaluation reward: 25.82\n",
      "episode: 2780   score: 21.0  epsilon: 1.0    steps: 200  evaluation reward: 25.8\n",
      "episode: 2781   score: 58.0  epsilon: 1.0    steps: 520  evaluation reward: 26.09\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5977: Policy loss: 0.013080. Value loss: 0.898568. Entropy: 0.858464.\n",
      "Iteration 5978: Policy loss: 0.001194. Value loss: 0.341949. Entropy: 0.864906.\n",
      "Iteration 5979: Policy loss: -0.002851. Value loss: 0.223345. Entropy: 0.897018.\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5980: Policy loss: 0.009921. Value loss: 0.274124. Entropy: 0.894599.\n",
      "Iteration 5981: Policy loss: -0.013331. Value loss: 0.081444. Entropy: 0.899616.\n",
      "Iteration 5982: Policy loss: -0.022597. Value loss: 0.047284. Entropy: 0.903709.\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5983: Policy loss: 0.006808. Value loss: 0.397953. Entropy: 0.952237.\n",
      "Iteration 5984: Policy loss: -0.014094. Value loss: 0.193691. Entropy: 0.945958.\n",
      "Iteration 5985: Policy loss: -0.021834. Value loss: 0.126297. Entropy: 0.941959.\n",
      "episode: 2782   score: 31.0  epsilon: 1.0    steps: 32  evaluation reward: 26.16\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5986: Policy loss: 0.010025. Value loss: 0.552125. Entropy: 0.905335.\n",
      "Iteration 5987: Policy loss: -0.010586. Value loss: 0.264692. Entropy: 0.895708.\n",
      "Iteration 5988: Policy loss: -0.020798. Value loss: 0.136066. Entropy: 0.912930.\n",
      "episode: 2783   score: 25.0  epsilon: 1.0    steps: 464  evaluation reward: 26.03\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5989: Policy loss: 0.003994. Value loss: 0.584626. Entropy: 0.935514.\n",
      "Iteration 5990: Policy loss: -0.008495. Value loss: 0.228826. Entropy: 0.932980.\n",
      "Iteration 5991: Policy loss: -0.019310. Value loss: 0.138479. Entropy: 0.935448.\n",
      "episode: 2784   score: 13.0  epsilon: 1.0    steps: 808  evaluation reward: 25.97\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5992: Policy loss: 0.005015. Value loss: 1.027283. Entropy: 0.966879.\n",
      "Iteration 5993: Policy loss: -0.007804. Value loss: 0.497140. Entropy: 0.980772.\n",
      "Iteration 5994: Policy loss: -0.017387. Value loss: 0.321999. Entropy: 0.991460.\n",
      "episode: 2785   score: 24.0  epsilon: 1.0    steps: 664  evaluation reward: 25.92\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5995: Policy loss: 0.008398. Value loss: 0.958005. Entropy: 0.942596.\n",
      "Iteration 5996: Policy loss: -0.003617. Value loss: 0.333760. Entropy: 0.941268.\n",
      "Iteration 5997: Policy loss: -0.016316. Value loss: 0.161209. Entropy: 0.941446.\n",
      "episode: 2786   score: 36.0  epsilon: 1.0    steps: 280  evaluation reward: 26.08\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5998: Policy loss: 0.008775. Value loss: 0.714512. Entropy: 0.856376.\n",
      "Iteration 5999: Policy loss: -0.009439. Value loss: 0.317536. Entropy: 0.854499.\n",
      "Iteration 6000: Policy loss: -0.013862. Value loss: 0.180330. Entropy: 0.856175.\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6001: Policy loss: 0.012559. Value loss: 0.477852. Entropy: 0.900092.\n",
      "Iteration 6002: Policy loss: -0.005204. Value loss: 0.179853. Entropy: 0.897997.\n",
      "Iteration 6003: Policy loss: -0.016428. Value loss: 0.097276. Entropy: 0.889217.\n",
      "episode: 2787   score: 28.0  epsilon: 1.0    steps: 848  evaluation reward: 25.99\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6004: Policy loss: 0.012025. Value loss: 1.283365. Entropy: 0.975539.\n",
      "Iteration 6005: Policy loss: 0.004075. Value loss: 0.442627. Entropy: 0.959875.\n",
      "Iteration 6006: Policy loss: -0.012654. Value loss: 0.234183. Entropy: 0.989383.\n",
      "episode: 2788   score: 37.0  epsilon: 1.0    steps: 312  evaluation reward: 26.19\n",
      "episode: 2789   score: 49.0  epsilon: 1.0    steps: 584  evaluation reward: 26.47\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6007: Policy loss: 0.007780. Value loss: 0.462165. Entropy: 0.925135.\n",
      "Iteration 6008: Policy loss: -0.009617. Value loss: 0.168312. Entropy: 0.921557.\n",
      "Iteration 6009: Policy loss: -0.020373. Value loss: 0.088593. Entropy: 0.926377.\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6010: Policy loss: 0.011314. Value loss: 0.402140. Entropy: 0.956812.\n",
      "Iteration 6011: Policy loss: -0.007678. Value loss: 0.176845. Entropy: 0.960597.\n",
      "Iteration 6012: Policy loss: -0.020276. Value loss: 0.111283. Entropy: 0.951565.\n",
      "episode: 2790   score: 25.0  epsilon: 1.0    steps: 600  evaluation reward: 26.53\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6013: Policy loss: 0.006596. Value loss: 0.536417. Entropy: 0.939561.\n",
      "Iteration 6014: Policy loss: -0.004816. Value loss: 0.253011. Entropy: 0.935408.\n",
      "Iteration 6015: Policy loss: -0.013784. Value loss: 0.170633. Entropy: 0.941509.\n",
      "episode: 2791   score: 17.0  epsilon: 1.0    steps: 360  evaluation reward: 26.46\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6016: Policy loss: 0.009714. Value loss: 0.948895. Entropy: 0.960521.\n",
      "Iteration 6017: Policy loss: 0.002512. Value loss: 0.446656. Entropy: 0.939416.\n",
      "Iteration 6018: Policy loss: -0.013592. Value loss: 0.266226. Entropy: 0.948206.\n",
      "episode: 2792   score: 12.0  epsilon: 1.0    steps: 632  evaluation reward: 26.36\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6019: Policy loss: 0.026759. Value loss: 0.603002. Entropy: 0.954458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6020: Policy loss: 0.000563. Value loss: 0.274567. Entropy: 0.950024.\n",
      "Iteration 6021: Policy loss: -0.011985. Value loss: 0.151770. Entropy: 0.935090.\n",
      "episode: 2793   score: 27.0  epsilon: 1.0    steps: 48  evaluation reward: 26.41\n",
      "episode: 2794   score: 27.0  epsilon: 1.0    steps: 232  evaluation reward: 26.39\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6022: Policy loss: 0.007480. Value loss: 0.465365. Entropy: 0.988815.\n",
      "Iteration 6023: Policy loss: -0.010461. Value loss: 0.216958. Entropy: 0.966286.\n",
      "Iteration 6024: Policy loss: -0.024308. Value loss: 0.139524. Entropy: 0.974710.\n",
      "episode: 2795   score: 26.0  epsilon: 1.0    steps: 384  evaluation reward: 26.33\n",
      "episode: 2796   score: 13.0  epsilon: 1.0    steps: 400  evaluation reward: 26.27\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6025: Policy loss: 0.010778. Value loss: 0.635878. Entropy: 0.990306.\n",
      "Iteration 6026: Policy loss: -0.001156. Value loss: 0.284577. Entropy: 0.985531.\n",
      "Iteration 6027: Policy loss: -0.013474. Value loss: 0.163681. Entropy: 0.985381.\n",
      "episode: 2797   score: 11.0  epsilon: 1.0    steps: 656  evaluation reward: 26.25\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6028: Policy loss: 0.009759. Value loss: 0.619116. Entropy: 0.999827.\n",
      "Iteration 6029: Policy loss: -0.005842. Value loss: 0.278482. Entropy: 1.005370.\n",
      "Iteration 6030: Policy loss: -0.018630. Value loss: 0.135647. Entropy: 0.993166.\n",
      "episode: 2798   score: 23.0  epsilon: 1.0    steps: 8  evaluation reward: 26.24\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6031: Policy loss: 0.007280. Value loss: 0.349031. Entropy: 0.973031.\n",
      "Iteration 6032: Policy loss: -0.011708. Value loss: 0.148860. Entropy: 0.982221.\n",
      "Iteration 6033: Policy loss: -0.023648. Value loss: 0.086804. Entropy: 0.975729.\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6034: Policy loss: 0.008625. Value loss: 0.257337. Entropy: 0.976214.\n",
      "Iteration 6035: Policy loss: -0.013949. Value loss: 0.098526. Entropy: 0.967587.\n",
      "Iteration 6036: Policy loss: -0.020972. Value loss: 0.064558. Entropy: 0.977905.\n",
      "episode: 2799   score: 13.0  epsilon: 1.0    steps: 864  evaluation reward: 26.25\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6037: Policy loss: 0.003791. Value loss: 0.248437. Entropy: 0.964450.\n",
      "Iteration 6038: Policy loss: -0.011998. Value loss: 0.080487. Entropy: 0.981059.\n",
      "Iteration 6039: Policy loss: -0.024045. Value loss: 0.043981. Entropy: 0.978659.\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6040: Policy loss: 0.005850. Value loss: 0.530071. Entropy: 0.939949.\n",
      "Iteration 6041: Policy loss: -0.005859. Value loss: 0.224698. Entropy: 0.919184.\n",
      "Iteration 6042: Policy loss: -0.017338. Value loss: 0.099758. Entropy: 0.943431.\n",
      "episode: 2800   score: 22.0  epsilon: 1.0    steps: 848  evaluation reward: 26.07\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6043: Policy loss: 0.006319. Value loss: 0.710289. Entropy: 1.006406.\n",
      "Iteration 6044: Policy loss: -0.004336. Value loss: 0.369547. Entropy: 1.003077.\n",
      "Iteration 6045: Policy loss: -0.007665. Value loss: 0.217180. Entropy: 1.002162.\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6046: Policy loss: 0.011393. Value loss: 0.721691. Entropy: 1.034925.\n",
      "Iteration 6047: Policy loss: -0.007953. Value loss: 0.306228. Entropy: 1.024455.\n",
      "Iteration 6048: Policy loss: -0.015959. Value loss: 0.164916. Entropy: 1.030016.\n",
      "now time :  2019-03-06 14:37:12.467553\n",
      "episode: 2801   score: 24.0  epsilon: 1.0    steps: 936  evaluation reward: 26.01\n",
      "episode: 2802   score: 25.0  epsilon: 1.0    steps: 968  evaluation reward: 26.01\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6049: Policy loss: 0.015157. Value loss: 0.516446. Entropy: 0.997729.\n",
      "Iteration 6050: Policy loss: -0.002589. Value loss: 0.232110. Entropy: 0.993489.\n",
      "Iteration 6051: Policy loss: -0.011220. Value loss: 0.103731. Entropy: 0.987608.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6052: Policy loss: 0.008687. Value loss: 0.912096. Entropy: 1.020878.\n",
      "Iteration 6053: Policy loss: -0.004508. Value loss: 0.449151. Entropy: 1.028027.\n",
      "Iteration 6054: Policy loss: -0.017084. Value loss: 0.236691. Entropy: 1.009435.\n",
      "episode: 2803   score: 30.0  epsilon: 1.0    steps: 328  evaluation reward: 26.14\n",
      "episode: 2804   score: 38.0  epsilon: 1.0    steps: 728  evaluation reward: 26.36\n",
      "episode: 2805   score: 22.0  epsilon: 1.0    steps: 808  evaluation reward: 26.31\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6055: Policy loss: 0.006424. Value loss: 0.791276. Entropy: 0.996475.\n",
      "Iteration 6056: Policy loss: -0.010356. Value loss: 0.331542. Entropy: 0.968528.\n",
      "Iteration 6057: Policy loss: -0.020239. Value loss: 0.185445. Entropy: 0.963178.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6058: Policy loss: 0.007524. Value loss: 0.600499. Entropy: 1.002516.\n",
      "Iteration 6059: Policy loss: -0.002454. Value loss: 0.257269. Entropy: 0.984606.\n",
      "Iteration 6060: Policy loss: -0.014301. Value loss: 0.143039. Entropy: 0.984560.\n",
      "episode: 2806   score: 19.0  epsilon: 1.0    steps: 656  evaluation reward: 26.23\n",
      "episode: 2807   score: 37.0  epsilon: 1.0    steps: 712  evaluation reward: 26.41\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6061: Policy loss: 0.012942. Value loss: 0.763386. Entropy: 1.023820.\n",
      "Iteration 6062: Policy loss: -0.000957. Value loss: 0.255808. Entropy: 1.024314.\n",
      "Iteration 6063: Policy loss: -0.013141. Value loss: 0.141308. Entropy: 1.011407.\n",
      "episode: 2808   score: 23.0  epsilon: 1.0    steps: 168  evaluation reward: 26.34\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6064: Policy loss: 0.015561. Value loss: 0.565559. Entropy: 1.058422.\n",
      "Iteration 6065: Policy loss: -0.005138. Value loss: 0.308500. Entropy: 1.050839.\n",
      "Iteration 6066: Policy loss: -0.009432. Value loss: 0.189909. Entropy: 1.058155.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6067: Policy loss: 0.005556. Value loss: 0.401346. Entropy: 1.056247.\n",
      "Iteration 6068: Policy loss: -0.009307. Value loss: 0.188518. Entropy: 1.057054.\n",
      "Iteration 6069: Policy loss: -0.021714. Value loss: 0.114822. Entropy: 1.045893.\n",
      "episode: 2809   score: 16.0  epsilon: 1.0    steps: 504  evaluation reward: 25.89\n",
      "episode: 2810   score: 15.0  epsilon: 1.0    steps: 936  evaluation reward: 25.93\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6070: Policy loss: 0.014724. Value loss: 0.621509. Entropy: 1.020078.\n",
      "Iteration 6071: Policy loss: -0.003344. Value loss: 0.260944. Entropy: 1.008831.\n",
      "Iteration 6072: Policy loss: -0.019052. Value loss: 0.134317. Entropy: 1.002362.\n",
      "episode: 2811   score: 13.0  epsilon: 1.0    steps: 944  evaluation reward: 25.91\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6073: Policy loss: 0.013279. Value loss: 0.303246. Entropy: 0.958627.\n",
      "Iteration 6074: Policy loss: -0.014030. Value loss: 0.103110. Entropy: 0.945307.\n",
      "Iteration 6075: Policy loss: -0.027295. Value loss: 0.054196. Entropy: 0.956160.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6076: Policy loss: 0.015054. Value loss: 0.493688. Entropy: 0.986953.\n",
      "Iteration 6077: Policy loss: -0.006530. Value loss: 0.192571. Entropy: 1.004992.\n",
      "Iteration 6078: Policy loss: -0.019427. Value loss: 0.105235. Entropy: 0.987095.\n",
      "episode: 2812   score: 23.0  epsilon: 1.0    steps: 440  evaluation reward: 25.88\n",
      "episode: 2813   score: 21.0  epsilon: 1.0    steps: 720  evaluation reward: 25.94\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6079: Policy loss: 0.011131. Value loss: 0.623547. Entropy: 0.925320.\n",
      "Iteration 6080: Policy loss: -0.010309. Value loss: 0.243547. Entropy: 0.925770.\n",
      "Iteration 6081: Policy loss: -0.013360. Value loss: 0.140455. Entropy: 0.939651.\n",
      "episode: 2814   score: 14.0  epsilon: 1.0    steps: 432  evaluation reward: 25.63\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6082: Policy loss: 0.008316. Value loss: 0.516510. Entropy: 0.977162.\n",
      "Iteration 6083: Policy loss: -0.007608. Value loss: 0.198039. Entropy: 0.971081.\n",
      "Iteration 6084: Policy loss: -0.022059. Value loss: 0.114757. Entropy: 0.974710.\n",
      "episode: 2815   score: 21.0  epsilon: 1.0    steps: 792  evaluation reward: 25.63\n",
      "Training network. lr: 0.000204. clip: 0.081421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6085: Policy loss: 0.017388. Value loss: 0.660889. Entropy: 0.895499.\n",
      "Iteration 6086: Policy loss: -0.001955. Value loss: 0.250764. Entropy: 0.895722.\n",
      "Iteration 6087: Policy loss: -0.017460. Value loss: 0.140320. Entropy: 0.897449.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6088: Policy loss: 0.013068. Value loss: 0.730401. Entropy: 0.917966.\n",
      "Iteration 6089: Policy loss: -0.003962. Value loss: 0.341696. Entropy: 0.929607.\n",
      "Iteration 6090: Policy loss: -0.012297. Value loss: 0.214097. Entropy: 0.928712.\n",
      "episode: 2816   score: 18.0  epsilon: 1.0    steps: 824  evaluation reward: 25.43\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6091: Policy loss: 0.004494. Value loss: 0.468623. Entropy: 1.021340.\n",
      "Iteration 6092: Policy loss: -0.007539. Value loss: 0.200340. Entropy: 1.027399.\n",
      "Iteration 6093: Policy loss: -0.021117. Value loss: 0.119998. Entropy: 1.013253.\n",
      "episode: 2817   score: 28.0  epsilon: 1.0    steps: 224  evaluation reward: 25.28\n",
      "episode: 2818   score: 22.0  epsilon: 1.0    steps: 680  evaluation reward: 25.19\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6094: Policy loss: 0.013768. Value loss: 0.545843. Entropy: 0.978046.\n",
      "Iteration 6095: Policy loss: -0.006514. Value loss: 0.239050. Entropy: 0.978335.\n",
      "Iteration 6096: Policy loss: -0.016959. Value loss: 0.161345. Entropy: 0.975974.\n",
      "episode: 2819   score: 16.0  epsilon: 1.0    steps: 760  evaluation reward: 24.97\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6097: Policy loss: 0.005575. Value loss: 0.233040. Entropy: 0.945631.\n",
      "Iteration 6098: Policy loss: -0.011623. Value loss: 0.077393. Entropy: 0.951511.\n",
      "Iteration 6099: Policy loss: -0.022905. Value loss: 0.044241. Entropy: 0.944691.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6100: Policy loss: 0.019663. Value loss: 0.647282. Entropy: 0.995738.\n",
      "Iteration 6101: Policy loss: -0.001113. Value loss: 0.316631. Entropy: 1.001768.\n",
      "Iteration 6102: Policy loss: -0.015879. Value loss: 0.170635. Entropy: 0.990193.\n",
      "episode: 2820   score: 22.0  epsilon: 1.0    steps: 392  evaluation reward: 24.96\n",
      "episode: 2821   score: 26.0  epsilon: 1.0    steps: 688  evaluation reward: 24.76\n",
      "episode: 2822   score: 15.0  epsilon: 1.0    steps: 768  evaluation reward: 24.63\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6103: Policy loss: 0.020598. Value loss: 0.492329. Entropy: 0.951951.\n",
      "Iteration 6104: Policy loss: -0.005737. Value loss: 0.235348. Entropy: 0.942560.\n",
      "Iteration 6105: Policy loss: -0.017852. Value loss: 0.131586. Entropy: 0.939075.\n",
      "episode: 2823   score: 25.0  epsilon: 1.0    steps: 600  evaluation reward: 24.75\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6106: Policy loss: 0.009798. Value loss: 0.635781. Entropy: 0.840134.\n",
      "Iteration 6107: Policy loss: -0.002885. Value loss: 0.241286. Entropy: 0.850098.\n",
      "Iteration 6108: Policy loss: -0.014208. Value loss: 0.109366. Entropy: 0.843468.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6109: Policy loss: 0.019122. Value loss: 0.585883. Entropy: 0.851262.\n",
      "Iteration 6110: Policy loss: -0.003099. Value loss: 0.262188. Entropy: 0.844966.\n",
      "Iteration 6111: Policy loss: -0.014697. Value loss: 0.131001. Entropy: 0.847011.\n",
      "episode: 2824   score: 14.0  epsilon: 1.0    steps: 936  evaluation reward: 24.49\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6112: Policy loss: 0.009480. Value loss: 0.723138. Entropy: 0.959173.\n",
      "Iteration 6113: Policy loss: -0.001427. Value loss: 0.312039. Entropy: 0.939234.\n",
      "Iteration 6114: Policy loss: -0.009877. Value loss: 0.197748. Entropy: 0.943020.\n",
      "episode: 2825   score: 21.0  epsilon: 1.0    steps: 784  evaluation reward: 24.34\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6115: Policy loss: 0.010318. Value loss: 0.573397. Entropy: 0.960858.\n",
      "Iteration 6116: Policy loss: -0.004549. Value loss: 0.170498. Entropy: 0.956482.\n",
      "Iteration 6117: Policy loss: -0.018017. Value loss: 0.102044. Entropy: 0.970101.\n",
      "episode: 2826   score: 18.0  epsilon: 1.0    steps: 744  evaluation reward: 24.14\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6118: Policy loss: 0.010332. Value loss: 0.503129. Entropy: 0.990586.\n",
      "Iteration 6119: Policy loss: -0.006288. Value loss: 0.176634. Entropy: 0.985952.\n",
      "Iteration 6120: Policy loss: -0.017956. Value loss: 0.092684. Entropy: 0.988103.\n",
      "episode: 2827   score: 14.0  epsilon: 1.0    steps: 328  evaluation reward: 24.03\n",
      "episode: 2828   score: 28.0  epsilon: 1.0    steps: 704  evaluation reward: 23.9\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6121: Policy loss: 0.014855. Value loss: 0.303931. Entropy: 0.895443.\n",
      "Iteration 6122: Policy loss: -0.000491. Value loss: 0.092426. Entropy: 0.866726.\n",
      "Iteration 6123: Policy loss: -0.015101. Value loss: 0.059638. Entropy: 0.866890.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6124: Policy loss: 0.012916. Value loss: 0.908990. Entropy: 0.875547.\n",
      "Iteration 6125: Policy loss: -0.004826. Value loss: 0.375363. Entropy: 0.875612.\n",
      "Iteration 6126: Policy loss: -0.006336. Value loss: 0.184268. Entropy: 0.858301.\n",
      "episode: 2829   score: 27.0  epsilon: 1.0    steps: 608  evaluation reward: 24.0\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6127: Policy loss: 0.014517. Value loss: 0.576376. Entropy: 0.944800.\n",
      "Iteration 6128: Policy loss: -0.005625. Value loss: 0.196529. Entropy: 0.936151.\n",
      "Iteration 6129: Policy loss: -0.017367. Value loss: 0.097691. Entropy: 0.927782.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6130: Policy loss: 0.011290. Value loss: 0.602944. Entropy: 0.944368.\n",
      "Iteration 6131: Policy loss: -0.004744. Value loss: 0.249235. Entropy: 0.947876.\n",
      "Iteration 6132: Policy loss: -0.021514. Value loss: 0.149707. Entropy: 0.959596.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6133: Policy loss: 0.009467. Value loss: 0.504836. Entropy: 0.976234.\n",
      "Iteration 6134: Policy loss: -0.004289. Value loss: 0.162078. Entropy: 0.979026.\n",
      "Iteration 6135: Policy loss: -0.015415. Value loss: 0.068795. Entropy: 0.968972.\n",
      "episode: 2830   score: 29.0  epsilon: 1.0    steps: 264  evaluation reward: 24.07\n",
      "episode: 2831   score: 37.0  epsilon: 1.0    steps: 936  evaluation reward: 24.21\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6136: Policy loss: 0.013510. Value loss: 0.761681. Entropy: 0.969294.\n",
      "Iteration 6137: Policy loss: -0.002957. Value loss: 0.281536. Entropy: 0.945186.\n",
      "Iteration 6138: Policy loss: -0.016926. Value loss: 0.163496. Entropy: 0.956656.\n",
      "episode: 2832   score: 24.0  epsilon: 1.0    steps: 232  evaluation reward: 24.13\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6139: Policy loss: 0.009782. Value loss: 0.456863. Entropy: 0.966651.\n",
      "Iteration 6140: Policy loss: -0.002935. Value loss: 0.167524. Entropy: 0.966209.\n",
      "Iteration 6141: Policy loss: -0.014878. Value loss: 0.083629. Entropy: 0.959442.\n",
      "episode: 2833   score: 24.0  epsilon: 1.0    steps: 320  evaluation reward: 23.99\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6142: Policy loss: 0.009490. Value loss: 0.655076. Entropy: 0.967025.\n",
      "Iteration 6143: Policy loss: -0.005053. Value loss: 0.287873. Entropy: 0.939620.\n",
      "Iteration 6144: Policy loss: -0.018606. Value loss: 0.165950. Entropy: 0.934342.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6145: Policy loss: 0.012727. Value loss: 0.855526. Entropy: 0.863557.\n",
      "Iteration 6146: Policy loss: -0.003193. Value loss: 0.378696. Entropy: 0.869026.\n",
      "Iteration 6147: Policy loss: -0.015414. Value loss: 0.185371. Entropy: 0.880752.\n",
      "episode: 2834   score: 25.0  epsilon: 1.0    steps: 136  evaluation reward: 23.87\n",
      "episode: 2835   score: 27.0  epsilon: 1.0    steps: 480  evaluation reward: 23.93\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6148: Policy loss: 0.009157. Value loss: 0.654950. Entropy: 0.931587.\n",
      "Iteration 6149: Policy loss: -0.005477. Value loss: 0.231447. Entropy: 0.935719.\n",
      "Iteration 6150: Policy loss: -0.007876. Value loss: 0.159255. Entropy: 0.936324.\n",
      "episode: 2836   score: 16.0  epsilon: 1.0    steps: 48  evaluation reward: 23.88\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6151: Policy loss: 0.008015. Value loss: 0.474718. Entropy: 0.919269.\n",
      "Iteration 6152: Policy loss: -0.007321. Value loss: 0.171263. Entropy: 0.936400.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6153: Policy loss: -0.012759. Value loss: 0.085436. Entropy: 0.935581.\n",
      "episode: 2837   score: 40.0  epsilon: 1.0    steps: 272  evaluation reward: 23.85\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6154: Policy loss: 0.006900. Value loss: 0.572348. Entropy: 0.939859.\n",
      "Iteration 6155: Policy loss: -0.008272. Value loss: 0.172522. Entropy: 0.941424.\n",
      "Iteration 6156: Policy loss: -0.018163. Value loss: 0.107900. Entropy: 0.944831.\n",
      "episode: 2838   score: 26.0  epsilon: 1.0    steps: 1000  evaluation reward: 23.92\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6157: Policy loss: 0.010468. Value loss: 0.792735. Entropy: 0.961453.\n",
      "Iteration 6158: Policy loss: -0.008110. Value loss: 0.252738. Entropy: 0.956605.\n",
      "Iteration 6159: Policy loss: -0.022536. Value loss: 0.148574. Entropy: 0.966424.\n",
      "episode: 2839   score: 15.0  epsilon: 1.0    steps: 696  evaluation reward: 23.69\n",
      "episode: 2840   score: 26.0  epsilon: 1.0    steps: 880  evaluation reward: 23.44\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6160: Policy loss: 0.008716. Value loss: 0.474450. Entropy: 1.002062.\n",
      "Iteration 6161: Policy loss: -0.008077. Value loss: 0.214657. Entropy: 1.003037.\n",
      "Iteration 6162: Policy loss: -0.022998. Value loss: 0.126261. Entropy: 1.004157.\n",
      "episode: 2841   score: 22.0  epsilon: 1.0    steps: 608  evaluation reward: 23.41\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6163: Policy loss: 0.010482. Value loss: 0.515837. Entropy: 0.924583.\n",
      "Iteration 6164: Policy loss: -0.011098. Value loss: 0.200926. Entropy: 0.938699.\n",
      "Iteration 6165: Policy loss: -0.020651. Value loss: 0.110648. Entropy: 0.933994.\n",
      "episode: 2842   score: 20.0  epsilon: 1.0    steps: 760  evaluation reward: 23.34\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6166: Policy loss: 0.003985. Value loss: 0.542492. Entropy: 1.014581.\n",
      "Iteration 6167: Policy loss: -0.012640. Value loss: 0.241588. Entropy: 1.010628.\n",
      "Iteration 6168: Policy loss: -0.022328. Value loss: 0.146931. Entropy: 1.000880.\n",
      "episode: 2843   score: 14.0  epsilon: 1.0    steps: 536  evaluation reward: 23.13\n",
      "episode: 2844   score: 16.0  epsilon: 1.0    steps: 696  evaluation reward: 23.19\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6169: Policy loss: 0.016055. Value loss: 0.358268. Entropy: 1.003133.\n",
      "Iteration 6170: Policy loss: 0.006856. Value loss: 0.112657. Entropy: 0.995869.\n",
      "Iteration 6171: Policy loss: -0.011820. Value loss: 0.062398. Entropy: 1.001885.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6172: Policy loss: 0.002044. Value loss: 0.254035. Entropy: 0.912528.\n",
      "Iteration 6173: Policy loss: -0.019338. Value loss: 0.080540. Entropy: 0.916567.\n",
      "Iteration 6174: Policy loss: -0.025579. Value loss: 0.048896. Entropy: 0.917862.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6175: Policy loss: 0.012804. Value loss: 0.462419. Entropy: 0.987060.\n",
      "Iteration 6176: Policy loss: -0.003309. Value loss: 0.252481. Entropy: 0.951386.\n",
      "Iteration 6177: Policy loss: -0.019080. Value loss: 0.164258. Entropy: 0.962775.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6178: Policy loss: 0.018769. Value loss: 0.328720. Entropy: 0.929702.\n",
      "Iteration 6179: Policy loss: -0.006189. Value loss: 0.126636. Entropy: 0.935021.\n",
      "Iteration 6180: Policy loss: -0.017519. Value loss: 0.077753. Entropy: 0.921688.\n",
      "episode: 2845   score: 19.0  epsilon: 1.0    steps: 56  evaluation reward: 23.16\n",
      "episode: 2846   score: 13.0  epsilon: 1.0    steps: 112  evaluation reward: 22.94\n",
      "episode: 2847   score: 17.0  epsilon: 1.0    steps: 744  evaluation reward: 22.97\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6181: Policy loss: 0.018339. Value loss: 0.484235. Entropy: 0.916684.\n",
      "Iteration 6182: Policy loss: -0.002704. Value loss: 0.195643. Entropy: 0.912235.\n",
      "Iteration 6183: Policy loss: -0.012944. Value loss: 0.101628. Entropy: 0.900299.\n",
      "episode: 2848   score: 17.0  epsilon: 1.0    steps: 336  evaluation reward: 22.88\n",
      "episode: 2849   score: 24.0  epsilon: 1.0    steps: 640  evaluation reward: 22.98\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6184: Policy loss: 0.009811. Value loss: 0.473459. Entropy: 0.861198.\n",
      "Iteration 6185: Policy loss: -0.003288. Value loss: 0.176862. Entropy: 0.863844.\n",
      "Iteration 6186: Policy loss: -0.019923. Value loss: 0.114499. Entropy: 0.869553.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6187: Policy loss: 0.006780. Value loss: 0.260815. Entropy: 0.869361.\n",
      "Iteration 6188: Policy loss: -0.008482. Value loss: 0.077112. Entropy: 0.884469.\n",
      "Iteration 6189: Policy loss: -0.024101. Value loss: 0.042895. Entropy: 0.880692.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6190: Policy loss: 0.005318. Value loss: 0.704994. Entropy: 0.945826.\n",
      "Iteration 6191: Policy loss: -0.007396. Value loss: 0.360134. Entropy: 0.943719.\n",
      "Iteration 6192: Policy loss: -0.013277. Value loss: 0.209214. Entropy: 0.938464.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6193: Policy loss: 0.013645. Value loss: 0.433653. Entropy: 0.997255.\n",
      "Iteration 6194: Policy loss: -0.003495. Value loss: 0.164906. Entropy: 1.003738.\n",
      "Iteration 6195: Policy loss: -0.017702. Value loss: 0.084702. Entropy: 1.013523.\n",
      "episode: 2850   score: 25.0  epsilon: 1.0    steps: 104  evaluation reward: 23.12\n",
      "now time :  2019-03-06 14:40:21.393288\n",
      "episode: 2851   score: 23.0  epsilon: 1.0    steps: 592  evaluation reward: 23.06\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6196: Policy loss: 0.012453. Value loss: 0.490499. Entropy: 0.969863.\n",
      "Iteration 6197: Policy loss: -0.010428. Value loss: 0.218209. Entropy: 0.966061.\n",
      "Iteration 6198: Policy loss: -0.019930. Value loss: 0.104339. Entropy: 0.957929.\n",
      "episode: 2852   score: 12.0  epsilon: 1.0    steps: 248  evaluation reward: 23.05\n",
      "episode: 2853   score: 32.0  epsilon: 1.0    steps: 824  evaluation reward: 23.23\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6199: Policy loss: 0.021321. Value loss: 0.541537. Entropy: 0.962702.\n",
      "Iteration 6200: Policy loss: 0.008075. Value loss: 0.143999. Entropy: 0.967803.\n",
      "Iteration 6201: Policy loss: -0.009013. Value loss: 0.090721. Entropy: 0.974991.\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6202: Policy loss: 0.020161. Value loss: 0.617563. Entropy: 0.976425.\n",
      "Iteration 6203: Policy loss: 0.006314. Value loss: 0.183819. Entropy: 0.964622.\n",
      "Iteration 6204: Policy loss: -0.009581. Value loss: 0.098680. Entropy: 0.981190.\n",
      "episode: 2854   score: 14.0  epsilon: 1.0    steps: 40  evaluation reward: 23.18\n",
      "episode: 2855   score: 30.0  epsilon: 1.0    steps: 112  evaluation reward: 23.25\n",
      "episode: 2856   score: 19.0  epsilon: 1.0    steps: 240  evaluation reward: 23.16\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6205: Policy loss: 0.011322. Value loss: 0.434569. Entropy: 1.039291.\n",
      "Iteration 6206: Policy loss: -0.006786. Value loss: 0.253227. Entropy: 1.028244.\n",
      "Iteration 6207: Policy loss: -0.017297. Value loss: 0.177072. Entropy: 1.021414.\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6208: Policy loss: 0.009017. Value loss: 0.485074. Entropy: 0.978133.\n",
      "Iteration 6209: Policy loss: -0.006161. Value loss: 0.250134. Entropy: 0.957102.\n",
      "Iteration 6210: Policy loss: -0.015208. Value loss: 0.117097. Entropy: 0.955133.\n",
      "episode: 2857   score: 29.0  epsilon: 1.0    steps: 296  evaluation reward: 23.22\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6211: Policy loss: 0.005772. Value loss: 0.478416. Entropy: 0.933668.\n",
      "Iteration 6212: Policy loss: -0.010946. Value loss: 0.201254. Entropy: 0.929361.\n",
      "Iteration 6213: Policy loss: -0.023044. Value loss: 0.101286. Entropy: 0.923991.\n",
      "episode: 2858   score: 12.0  epsilon: 1.0    steps: 264  evaluation reward: 23.03\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6214: Policy loss: 0.007546. Value loss: 0.415120. Entropy: 1.035839.\n",
      "Iteration 6215: Policy loss: -0.005691. Value loss: 0.148783. Entropy: 1.024181.\n",
      "Iteration 6216: Policy loss: -0.019668. Value loss: 0.099564. Entropy: 1.010074.\n",
      "episode: 2859   score: 18.0  epsilon: 1.0    steps: 568  evaluation reward: 23.0\n",
      "episode: 2860   score: 16.0  epsilon: 1.0    steps: 840  evaluation reward: 22.81\n",
      "Training network. lr: 0.000202. clip: 0.080960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6217: Policy loss: 0.011846. Value loss: 0.516371. Entropy: 0.968523.\n",
      "Iteration 6218: Policy loss: -0.008649. Value loss: 0.227630. Entropy: 0.968957.\n",
      "Iteration 6219: Policy loss: -0.022593. Value loss: 0.118404. Entropy: 0.957074.\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6220: Policy loss: 0.009112. Value loss: 0.464366. Entropy: 1.006196.\n",
      "Iteration 6221: Policy loss: -0.006795. Value loss: 0.169038. Entropy: 1.003655.\n",
      "Iteration 6222: Policy loss: -0.017246. Value loss: 0.095362. Entropy: 1.002374.\n",
      "episode: 2861   score: 21.0  epsilon: 1.0    steps: 160  evaluation reward: 22.73\n",
      "episode: 2862   score: 18.0  epsilon: 1.0    steps: 856  evaluation reward: 22.77\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6223: Policy loss: 0.004580. Value loss: 0.371939. Entropy: 0.911884.\n",
      "Iteration 6224: Policy loss: 0.000898. Value loss: 0.126556. Entropy: 0.926654.\n",
      "Iteration 6225: Policy loss: -0.018990. Value loss: 0.054271. Entropy: 0.929276.\n",
      "episode: 2863   score: 18.0  epsilon: 1.0    steps: 424  evaluation reward: 22.77\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6226: Policy loss: 0.006569. Value loss: 0.371752. Entropy: 0.889152.\n",
      "Iteration 6227: Policy loss: -0.010267. Value loss: 0.099762. Entropy: 0.881618.\n",
      "Iteration 6228: Policy loss: -0.025672. Value loss: 0.060240. Entropy: 0.885947.\n",
      "episode: 2864   score: 19.0  epsilon: 1.0    steps: 632  evaluation reward: 22.81\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6229: Policy loss: 0.009454. Value loss: 0.353365. Entropy: 0.969931.\n",
      "Iteration 6230: Policy loss: -0.008657. Value loss: 0.140289. Entropy: 0.960853.\n",
      "Iteration 6231: Policy loss: -0.020204. Value loss: 0.082732. Entropy: 0.959176.\n",
      "episode: 2865   score: 16.0  epsilon: 1.0    steps: 336  evaluation reward: 22.67\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6232: Policy loss: 0.003857. Value loss: 0.235206. Entropy: 0.917328.\n",
      "Iteration 6233: Policy loss: -0.012620. Value loss: 0.105913. Entropy: 0.927137.\n",
      "Iteration 6234: Policy loss: -0.021576. Value loss: 0.061926. Entropy: 0.907654.\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6235: Policy loss: 0.009877. Value loss: 0.346577. Entropy: 0.887010.\n",
      "Iteration 6236: Policy loss: -0.007023. Value loss: 0.099315. Entropy: 0.894154.\n",
      "Iteration 6237: Policy loss: -0.017095. Value loss: 0.053068. Entropy: 0.883152.\n",
      "episode: 2866   score: 15.0  epsilon: 1.0    steps: 728  evaluation reward: 22.68\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6238: Policy loss: 0.003226. Value loss: 0.365661. Entropy: 0.935083.\n",
      "Iteration 6239: Policy loss: -0.005720. Value loss: 0.126511. Entropy: 0.947189.\n",
      "Iteration 6240: Policy loss: -0.020969. Value loss: 0.064466. Entropy: 0.932207.\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6241: Policy loss: 0.008151. Value loss: 0.280222. Entropy: 0.962754.\n",
      "Iteration 6242: Policy loss: -0.005081. Value loss: 0.122398. Entropy: 0.940727.\n",
      "Iteration 6243: Policy loss: -0.020781. Value loss: 0.069311. Entropy: 0.956825.\n",
      "episode: 2867   score: 33.0  epsilon: 1.0    steps: 464  evaluation reward: 22.8\n",
      "episode: 2868   score: 17.0  epsilon: 1.0    steps: 784  evaluation reward: 22.8\n",
      "episode: 2869   score: 22.0  epsilon: 1.0    steps: 928  evaluation reward: 22.76\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6244: Policy loss: 0.024887. Value loss: 0.838544. Entropy: 0.912174.\n",
      "Iteration 6245: Policy loss: 0.008390. Value loss: 0.240910. Entropy: 0.906204.\n",
      "Iteration 6246: Policy loss: -0.001958. Value loss: 0.127793. Entropy: 0.899816.\n",
      "episode: 2870   score: 18.0  epsilon: 1.0    steps: 752  evaluation reward: 22.76\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6247: Policy loss: 0.010657. Value loss: 0.377451. Entropy: 0.917767.\n",
      "Iteration 6248: Policy loss: -0.009405. Value loss: 0.111346. Entropy: 0.923269.\n",
      "Iteration 6249: Policy loss: -0.018906. Value loss: 0.062106. Entropy: 0.935942.\n",
      "episode: 2871   score: 26.0  epsilon: 1.0    steps: 32  evaluation reward: 22.87\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6250: Policy loss: 0.008995. Value loss: 0.455178. Entropy: 0.942050.\n",
      "Iteration 6251: Policy loss: -0.005088. Value loss: 0.251463. Entropy: 0.918185.\n",
      "Iteration 6252: Policy loss: -0.019076. Value loss: 0.142295. Entropy: 0.921910.\n",
      "episode: 2872   score: 24.0  epsilon: 1.0    steps: 256  evaluation reward: 22.97\n",
      "episode: 2873   score: 19.0  epsilon: 1.0    steps: 424  evaluation reward: 22.77\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6253: Policy loss: 0.010355. Value loss: 0.370350. Entropy: 0.855697.\n",
      "Iteration 6254: Policy loss: -0.007632. Value loss: 0.182326. Entropy: 0.850672.\n",
      "Iteration 6255: Policy loss: -0.020545. Value loss: 0.119598. Entropy: 0.836987.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6256: Policy loss: 0.014467. Value loss: 0.518643. Entropy: 0.899073.\n",
      "Iteration 6257: Policy loss: -0.001409. Value loss: 0.275250. Entropy: 0.907969.\n",
      "Iteration 6258: Policy loss: -0.018625. Value loss: 0.179834. Entropy: 0.895473.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6259: Policy loss: 0.005746. Value loss: 0.520049. Entropy: 0.878643.\n",
      "Iteration 6260: Policy loss: -0.002562. Value loss: 0.169229. Entropy: 0.881483.\n",
      "Iteration 6261: Policy loss: -0.013739. Value loss: 0.088665. Entropy: 0.883073.\n",
      "episode: 2874   score: 22.0  epsilon: 1.0    steps: 704  evaluation reward: 22.66\n",
      "episode: 2875   score: 18.0  epsilon: 1.0    steps: 768  evaluation reward: 22.55\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6262: Policy loss: 0.010100. Value loss: 0.623554. Entropy: 0.958089.\n",
      "Iteration 6263: Policy loss: -0.009275. Value loss: 0.241973. Entropy: 0.951337.\n",
      "Iteration 6264: Policy loss: -0.016605. Value loss: 0.133307. Entropy: 0.945244.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6265: Policy loss: 0.009556. Value loss: 0.420972. Entropy: 1.004692.\n",
      "Iteration 6266: Policy loss: -0.005582. Value loss: 0.151370. Entropy: 1.009428.\n",
      "Iteration 6267: Policy loss: -0.021227. Value loss: 0.072704. Entropy: 1.016453.\n",
      "episode: 2876   score: 16.0  epsilon: 1.0    steps: 392  evaluation reward: 22.53\n",
      "episode: 2877   score: 22.0  epsilon: 1.0    steps: 776  evaluation reward: 22.3\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6268: Policy loss: 0.011252. Value loss: 0.370586. Entropy: 0.943696.\n",
      "Iteration 6269: Policy loss: -0.006763. Value loss: 0.157740. Entropy: 0.950103.\n",
      "Iteration 6270: Policy loss: -0.016423. Value loss: 0.087219. Entropy: 0.938035.\n",
      "episode: 2878   score: 24.0  epsilon: 1.0    steps: 552  evaluation reward: 22.44\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6271: Policy loss: 0.011451. Value loss: 0.519893. Entropy: 1.024386.\n",
      "Iteration 6272: Policy loss: -0.004525. Value loss: 0.178729. Entropy: 1.023207.\n",
      "Iteration 6273: Policy loss: -0.018393. Value loss: 0.106757. Entropy: 1.020564.\n",
      "episode: 2879   score: 26.0  epsilon: 1.0    steps: 192  evaluation reward: 22.48\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6274: Policy loss: 0.024375. Value loss: 0.509210. Entropy: 0.906880.\n",
      "Iteration 6275: Policy loss: 0.002146. Value loss: 0.199240. Entropy: 0.857043.\n",
      "Iteration 6276: Policy loss: -0.009165. Value loss: 0.109109. Entropy: 0.890482.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6277: Policy loss: 0.007353. Value loss: 0.301053. Entropy: 0.864822.\n",
      "Iteration 6278: Policy loss: -0.010062. Value loss: 0.091757. Entropy: 0.853098.\n",
      "Iteration 6279: Policy loss: -0.022855. Value loss: 0.041681. Entropy: 0.877404.\n",
      "episode: 2880   score: 27.0  epsilon: 1.0    steps: 368  evaluation reward: 22.54\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6280: Policy loss: 0.004072. Value loss: 0.569863. Entropy: 0.975435.\n",
      "Iteration 6281: Policy loss: -0.008482. Value loss: 0.187648. Entropy: 0.970213.\n",
      "Iteration 6282: Policy loss: -0.022012. Value loss: 0.098529. Entropy: 0.976918.\n",
      "episode: 2881   score: 27.0  epsilon: 1.0    steps: 192  evaluation reward: 22.23\n",
      "episode: 2882   score: 16.0  epsilon: 1.0    steps: 544  evaluation reward: 22.08\n",
      "episode: 2883   score: 17.0  epsilon: 1.0    steps: 960  evaluation reward: 22.0\n",
      "Training network. lr: 0.000202. clip: 0.080803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6283: Policy loss: 0.004814. Value loss: 0.656987. Entropy: 0.828506.\n",
      "Iteration 6284: Policy loss: -0.001389. Value loss: 0.250767. Entropy: 0.816660.\n",
      "Iteration 6285: Policy loss: -0.011878. Value loss: 0.127116. Entropy: 0.817243.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6286: Policy loss: 0.011290. Value loss: 0.344046. Entropy: 0.896372.\n",
      "Iteration 6287: Policy loss: -0.002612. Value loss: 0.132854. Entropy: 0.883171.\n",
      "Iteration 6288: Policy loss: -0.016701. Value loss: 0.073705. Entropy: 0.880920.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6289: Policy loss: 0.007919. Value loss: 0.584459. Entropy: 0.804776.\n",
      "Iteration 6290: Policy loss: -0.004375. Value loss: 0.213415. Entropy: 0.791206.\n",
      "Iteration 6291: Policy loss: -0.015271. Value loss: 0.095258. Entropy: 0.797714.\n",
      "episode: 2884   score: 20.0  epsilon: 1.0    steps: 80  evaluation reward: 22.07\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6292: Policy loss: 0.010764. Value loss: 0.394171. Entropy: 0.904601.\n",
      "Iteration 6293: Policy loss: 0.000208. Value loss: 0.213838. Entropy: 0.898799.\n",
      "Iteration 6294: Policy loss: -0.014090. Value loss: 0.158166. Entropy: 0.900171.\n",
      "episode: 2885   score: 19.0  epsilon: 1.0    steps: 656  evaluation reward: 22.02\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6295: Policy loss: 0.000431. Value loss: 0.459864. Entropy: 0.801622.\n",
      "Iteration 6296: Policy loss: -0.006438. Value loss: 0.189223. Entropy: 0.823388.\n",
      "Iteration 6297: Policy loss: -0.016667. Value loss: 0.111188. Entropy: 0.811723.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6298: Policy loss: 0.006689. Value loss: 0.534745. Entropy: 0.931758.\n",
      "Iteration 6299: Policy loss: -0.005822. Value loss: 0.177795. Entropy: 0.944238.\n",
      "Iteration 6300: Policy loss: -0.011010. Value loss: 0.096284. Entropy: 0.936786.\n",
      "episode: 2886   score: 29.0  epsilon: 1.0    steps: 192  evaluation reward: 21.95\n",
      "episode: 2887   score: 37.0  epsilon: 1.0    steps: 504  evaluation reward: 22.04\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6301: Policy loss: 0.009780. Value loss: 0.680637. Entropy: 0.882952.\n",
      "Iteration 6302: Policy loss: -0.004284. Value loss: 0.345612. Entropy: 0.867411.\n",
      "Iteration 6303: Policy loss: -0.010033. Value loss: 0.173767. Entropy: 0.875351.\n",
      "episode: 2888   score: 20.0  epsilon: 1.0    steps: 832  evaluation reward: 21.87\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6304: Policy loss: 0.010417. Value loss: 0.619006. Entropy: 0.894209.\n",
      "Iteration 6305: Policy loss: -0.009330. Value loss: 0.218817. Entropy: 0.877337.\n",
      "Iteration 6306: Policy loss: -0.021233. Value loss: 0.093120. Entropy: 0.878122.\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6307: Policy loss: 0.014279. Value loss: 0.466009. Entropy: 0.844542.\n",
      "Iteration 6308: Policy loss: -0.002894. Value loss: 0.144481. Entropy: 0.847419.\n",
      "Iteration 6309: Policy loss: -0.015733. Value loss: 0.069424. Entropy: 0.858976.\n",
      "episode: 2889   score: 27.0  epsilon: 1.0    steps: 32  evaluation reward: 21.65\n",
      "episode: 2890   score: 26.0  epsilon: 1.0    steps: 584  evaluation reward: 21.66\n",
      "episode: 2891   score: 29.0  epsilon: 1.0    steps: 792  evaluation reward: 21.78\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6310: Policy loss: 0.017023. Value loss: 0.625378. Entropy: 0.935659.\n",
      "Iteration 6311: Policy loss: -0.004240. Value loss: 0.266112. Entropy: 0.914174.\n",
      "Iteration 6312: Policy loss: -0.012596. Value loss: 0.176920. Entropy: 0.919938.\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6313: Policy loss: 0.014924. Value loss: 0.259521. Entropy: 0.867396.\n",
      "Iteration 6314: Policy loss: -0.002979. Value loss: 0.073335. Entropy: 0.849101.\n",
      "Iteration 6315: Policy loss: -0.015126. Value loss: 0.035690. Entropy: 0.856302.\n",
      "episode: 2892   score: 24.0  epsilon: 1.0    steps: 488  evaluation reward: 21.9\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6316: Policy loss: 0.009525. Value loss: 0.502520. Entropy: 0.900232.\n",
      "Iteration 6317: Policy loss: -0.009217. Value loss: 0.210744. Entropy: 0.889671.\n",
      "Iteration 6318: Policy loss: -0.023979. Value loss: 0.128495. Entropy: 0.892540.\n",
      "episode: 2893   score: 23.0  epsilon: 1.0    steps: 176  evaluation reward: 21.86\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6319: Policy loss: 0.014384. Value loss: 0.413439. Entropy: 0.874679.\n",
      "Iteration 6320: Policy loss: -0.006343. Value loss: 0.188616. Entropy: 0.863973.\n",
      "Iteration 6321: Policy loss: -0.016284. Value loss: 0.112418. Entropy: 0.865238.\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6322: Policy loss: 0.004258. Value loss: 0.678163. Entropy: 0.929291.\n",
      "Iteration 6323: Policy loss: -0.005812. Value loss: 0.261371. Entropy: 0.920508.\n",
      "Iteration 6324: Policy loss: -0.013846. Value loss: 0.159919. Entropy: 0.918374.\n",
      "episode: 2894   score: 26.0  epsilon: 1.0    steps: 880  evaluation reward: 21.85\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6325: Policy loss: 0.014402. Value loss: 0.589116. Entropy: 0.886168.\n",
      "Iteration 6326: Policy loss: -0.004393. Value loss: 0.229012. Entropy: 0.884882.\n",
      "Iteration 6327: Policy loss: -0.012853. Value loss: 0.111744. Entropy: 0.868723.\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6328: Policy loss: 0.009395. Value loss: 0.355470. Entropy: 0.911346.\n",
      "Iteration 6329: Policy loss: -0.011133. Value loss: 0.139134. Entropy: 0.923799.\n",
      "Iteration 6330: Policy loss: -0.021141. Value loss: 0.082447. Entropy: 0.916013.\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6331: Policy loss: 0.021888. Value loss: 0.632453. Entropy: 0.963313.\n",
      "Iteration 6332: Policy loss: -0.001127. Value loss: 0.206101. Entropy: 0.965983.\n",
      "Iteration 6333: Policy loss: -0.018368. Value loss: 0.119734. Entropy: 0.956672.\n",
      "episode: 2895   score: 20.0  epsilon: 1.0    steps: 152  evaluation reward: 21.79\n",
      "episode: 2896   score: 31.0  epsilon: 1.0    steps: 304  evaluation reward: 21.97\n",
      "episode: 2897   score: 20.0  epsilon: 1.0    steps: 712  evaluation reward: 22.06\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6334: Policy loss: 0.009806. Value loss: 0.373303. Entropy: 0.887845.\n",
      "Iteration 6335: Policy loss: -0.003037. Value loss: 0.151617. Entropy: 0.894329.\n",
      "Iteration 6336: Policy loss: -0.016363. Value loss: 0.097849. Entropy: 0.900423.\n",
      "episode: 2898   score: 30.0  epsilon: 1.0    steps: 496  evaluation reward: 22.13\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6337: Policy loss: 0.009106. Value loss: 0.396125. Entropy: 0.901492.\n",
      "Iteration 6338: Policy loss: -0.011334. Value loss: 0.158211. Entropy: 0.888989.\n",
      "Iteration 6339: Policy loss: -0.014354. Value loss: 0.092357. Entropy: 0.898137.\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6340: Policy loss: 0.010347. Value loss: 0.637967. Entropy: 0.953788.\n",
      "Iteration 6341: Policy loss: -0.008156. Value loss: 0.257339. Entropy: 0.939125.\n",
      "Iteration 6342: Policy loss: -0.019617. Value loss: 0.158517. Entropy: 0.947030.\n",
      "episode: 2899   score: 31.0  epsilon: 1.0    steps: 72  evaluation reward: 22.31\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6343: Policy loss: 0.008167. Value loss: 0.211754. Entropy: 0.886263.\n",
      "Iteration 6344: Policy loss: -0.009392. Value loss: 0.092262. Entropy: 0.873769.\n",
      "Iteration 6345: Policy loss: -0.019319. Value loss: 0.058623. Entropy: 0.862389.\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6346: Policy loss: 0.009775. Value loss: 0.655394. Entropy: 0.927508.\n",
      "Iteration 6347: Policy loss: -0.003497. Value loss: 0.270564. Entropy: 0.925764.\n",
      "Iteration 6348: Policy loss: -0.014186. Value loss: 0.158376. Entropy: 0.910163.\n",
      "episode: 2900   score: 30.0  epsilon: 1.0    steps: 16  evaluation reward: 22.39\n",
      "now time :  2019-03-06 14:43:37.881569\n",
      "episode: 2901   score: 11.0  epsilon: 1.0    steps: 384  evaluation reward: 22.26\n",
      "episode: 2902   score: 24.0  epsilon: 1.0    steps: 936  evaluation reward: 22.25\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6349: Policy loss: 0.007055. Value loss: 0.733765. Entropy: 0.893933.\n",
      "Iteration 6350: Policy loss: -0.001561. Value loss: 0.344852. Entropy: 0.882933.\n",
      "Iteration 6351: Policy loss: -0.014295. Value loss: 0.201089. Entropy: 0.864912.\n",
      "episode: 2903   score: 34.0  epsilon: 1.0    steps: 264  evaluation reward: 22.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6352: Policy loss: 0.010438. Value loss: 0.317839. Entropy: 0.863497.\n",
      "Iteration 6353: Policy loss: -0.008533. Value loss: 0.170227. Entropy: 0.858777.\n",
      "Iteration 6354: Policy loss: -0.018583. Value loss: 0.114295. Entropy: 0.866396.\n",
      "episode: 2904   score: 21.0  epsilon: 1.0    steps: 944  evaluation reward: 22.12\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6355: Policy loss: 0.014693. Value loss: 0.714068. Entropy: 0.874369.\n",
      "Iteration 6356: Policy loss: -0.003444. Value loss: 0.328436. Entropy: 0.864915.\n",
      "Iteration 6357: Policy loss: -0.014663. Value loss: 0.192497. Entropy: 0.869155.\n",
      "episode: 2905   score: 22.0  epsilon: 1.0    steps: 968  evaluation reward: 22.12\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6358: Policy loss: 0.008488. Value loss: 0.337012. Entropy: 0.898089.\n",
      "Iteration 6359: Policy loss: -0.011153. Value loss: 0.138442. Entropy: 0.886360.\n",
      "Iteration 6360: Policy loss: -0.020912. Value loss: 0.085011. Entropy: 0.887469.\n",
      "episode: 2906   score: 20.0  epsilon: 1.0    steps: 240  evaluation reward: 22.13\n",
      "episode: 2907   score: 21.0  epsilon: 1.0    steps: 768  evaluation reward: 21.97\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6361: Policy loss: 0.011763. Value loss: 0.467314. Entropy: 0.889670.\n",
      "Iteration 6362: Policy loss: -0.007527. Value loss: 0.213059. Entropy: 0.897005.\n",
      "Iteration 6363: Policy loss: -0.018350. Value loss: 0.109243. Entropy: 0.876386.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6364: Policy loss: 0.011734. Value loss: 0.283594. Entropy: 0.818366.\n",
      "Iteration 6365: Policy loss: -0.012720. Value loss: 0.100928. Entropy: 0.802326.\n",
      "Iteration 6366: Policy loss: -0.021888. Value loss: 0.060167. Entropy: 0.793656.\n",
      "episode: 2908   score: 14.0  epsilon: 1.0    steps: 816  evaluation reward: 21.88\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6367: Policy loss: 0.012086. Value loss: 0.377238. Entropy: 0.820025.\n",
      "Iteration 6368: Policy loss: -0.009699. Value loss: 0.149522. Entropy: 0.812251.\n",
      "Iteration 6369: Policy loss: -0.020517. Value loss: 0.081732. Entropy: 0.809564.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6370: Policy loss: 0.010423. Value loss: 0.220330. Entropy: 0.836775.\n",
      "Iteration 6371: Policy loss: -0.005122. Value loss: 0.086666. Entropy: 0.827337.\n",
      "Iteration 6372: Policy loss: -0.012901. Value loss: 0.040768. Entropy: 0.825710.\n",
      "episode: 2909   score: 14.0  epsilon: 1.0    steps: 320  evaluation reward: 21.86\n",
      "episode: 2910   score: 12.0  epsilon: 1.0    steps: 536  evaluation reward: 21.83\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6373: Policy loss: 0.015662. Value loss: 0.518980. Entropy: 0.853325.\n",
      "Iteration 6374: Policy loss: -0.005794. Value loss: 0.202503. Entropy: 0.844581.\n",
      "Iteration 6375: Policy loss: -0.017181. Value loss: 0.088202. Entropy: 0.826060.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6376: Policy loss: 0.008932. Value loss: 0.325835. Entropy: 0.870024.\n",
      "Iteration 6377: Policy loss: -0.007988. Value loss: 0.160429. Entropy: 0.854984.\n",
      "Iteration 6378: Policy loss: -0.017623. Value loss: 0.094692. Entropy: 0.857380.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6379: Policy loss: 0.011278. Value loss: 0.496297. Entropy: 0.836504.\n",
      "Iteration 6380: Policy loss: -0.001550. Value loss: 0.157793. Entropy: 0.811796.\n",
      "Iteration 6381: Policy loss: -0.017158. Value loss: 0.068467. Entropy: 0.816152.\n",
      "episode: 2911   score: 31.0  epsilon: 1.0    steps: 408  evaluation reward: 22.01\n",
      "episode: 2912   score: 33.0  epsilon: 1.0    steps: 584  evaluation reward: 22.11\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6382: Policy loss: 0.010164. Value loss: 0.730361. Entropy: 0.891745.\n",
      "Iteration 6383: Policy loss: -0.007808. Value loss: 0.281634. Entropy: 0.888461.\n",
      "Iteration 6384: Policy loss: -0.017957. Value loss: 0.156051. Entropy: 0.881998.\n",
      "episode: 2913   score: 25.0  epsilon: 1.0    steps: 696  evaluation reward: 22.15\n",
      "episode: 2914   score: 23.0  epsilon: 1.0    steps: 768  evaluation reward: 22.24\n",
      "episode: 2915   score: 27.0  epsilon: 1.0    steps: 912  evaluation reward: 22.3\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6385: Policy loss: 0.009669. Value loss: 0.534903. Entropy: 0.828399.\n",
      "Iteration 6386: Policy loss: -0.000090. Value loss: 0.209847. Entropy: 0.844380.\n",
      "Iteration 6387: Policy loss: -0.011882. Value loss: 0.102187. Entropy: 0.832315.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6388: Policy loss: 0.010245. Value loss: 0.541322. Entropy: 0.820200.\n",
      "Iteration 6389: Policy loss: -0.000528. Value loss: 0.249756. Entropy: 0.839835.\n",
      "Iteration 6390: Policy loss: -0.012182. Value loss: 0.149702. Entropy: 0.838343.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6391: Policy loss: 0.018619. Value loss: 0.532041. Entropy: 0.919211.\n",
      "Iteration 6392: Policy loss: -0.005546. Value loss: 0.195745. Entropy: 0.906876.\n",
      "Iteration 6393: Policy loss: -0.016599. Value loss: 0.103696. Entropy: 0.899363.\n",
      "episode: 2916   score: 24.0  epsilon: 1.0    steps: 832  evaluation reward: 22.36\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6394: Policy loss: 0.005227. Value loss: 0.626237. Entropy: 0.865831.\n",
      "Iteration 6395: Policy loss: -0.010988. Value loss: 0.242210. Entropy: 0.855616.\n",
      "Iteration 6396: Policy loss: -0.013802. Value loss: 0.106264. Entropy: 0.867125.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6397: Policy loss: 0.014152. Value loss: 0.656690. Entropy: 0.939939.\n",
      "Iteration 6398: Policy loss: -0.001042. Value loss: 0.294889. Entropy: 0.922396.\n",
      "Iteration 6399: Policy loss: -0.005738. Value loss: 0.185308. Entropy: 0.928589.\n",
      "episode: 2917   score: 29.0  epsilon: 1.0    steps: 672  evaluation reward: 22.37\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6400: Policy loss: 0.008278. Value loss: 0.339945. Entropy: 0.893448.\n",
      "Iteration 6401: Policy loss: -0.005950. Value loss: 0.153061. Entropy: 0.894193.\n",
      "Iteration 6402: Policy loss: -0.016781. Value loss: 0.079581. Entropy: 0.903063.\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6403: Policy loss: 0.007869. Value loss: 0.494233. Entropy: 0.933684.\n",
      "Iteration 6404: Policy loss: -0.002547. Value loss: 0.183880. Entropy: 0.937474.\n",
      "Iteration 6405: Policy loss: -0.014753. Value loss: 0.101056. Entropy: 0.944460.\n",
      "episode: 2918   score: 17.0  epsilon: 1.0    steps: 472  evaluation reward: 22.32\n",
      "episode: 2919   score: 19.0  epsilon: 1.0    steps: 696  evaluation reward: 22.35\n",
      "episode: 2920   score: 25.0  epsilon: 1.0    steps: 896  evaluation reward: 22.38\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6406: Policy loss: 0.017914. Value loss: 0.934541. Entropy: 0.928498.\n",
      "Iteration 6407: Policy loss: 0.004065. Value loss: 0.360230. Entropy: 0.931846.\n",
      "Iteration 6408: Policy loss: -0.011833. Value loss: 0.199471. Entropy: 0.942097.\n",
      "episode: 2921   score: 23.0  epsilon: 1.0    steps: 360  evaluation reward: 22.35\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6409: Policy loss: 0.011171. Value loss: 0.493648. Entropy: 0.780286.\n",
      "Iteration 6410: Policy loss: -0.001501. Value loss: 0.208908. Entropy: 0.769337.\n",
      "Iteration 6411: Policy loss: -0.005567. Value loss: 0.117977. Entropy: 0.775838.\n",
      "episode: 2922   score: 40.0  epsilon: 1.0    steps: 568  evaluation reward: 22.6\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6412: Policy loss: 0.008809. Value loss: 0.375949. Entropy: 0.853617.\n",
      "Iteration 6413: Policy loss: -0.009891. Value loss: 0.177493. Entropy: 0.865226.\n",
      "Iteration 6414: Policy loss: -0.019450. Value loss: 0.090267. Entropy: 0.843711.\n",
      "episode: 2923   score: 42.0  epsilon: 1.0    steps: 1024  evaluation reward: 22.77\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6415: Policy loss: 0.016777. Value loss: 0.706296. Entropy: 0.868864.\n",
      "Iteration 6416: Policy loss: 0.000743. Value loss: 0.322112. Entropy: 0.846570.\n",
      "Iteration 6417: Policy loss: -0.011689. Value loss: 0.177700. Entropy: 0.859551.\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6418: Policy loss: 0.010815. Value loss: 0.657593. Entropy: 0.879633.\n",
      "Iteration 6419: Policy loss: -0.005107. Value loss: 0.264407. Entropy: 0.867570.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6420: Policy loss: -0.017078. Value loss: 0.121447. Entropy: 0.879445.\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6421: Policy loss: 0.010021. Value loss: 0.362794. Entropy: 0.883033.\n",
      "Iteration 6422: Policy loss: -0.007472. Value loss: 0.134360. Entropy: 0.896973.\n",
      "Iteration 6423: Policy loss: -0.021579. Value loss: 0.063794. Entropy: 0.898353.\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6424: Policy loss: 0.008458. Value loss: 0.833477. Entropy: 0.925866.\n",
      "Iteration 6425: Policy loss: -0.009415. Value loss: 0.399890. Entropy: 0.922857.\n",
      "Iteration 6426: Policy loss: -0.019639. Value loss: 0.218893. Entropy: 0.908247.\n",
      "episode: 2924   score: 22.0  epsilon: 1.0    steps: 8  evaluation reward: 22.85\n",
      "episode: 2925   score: 34.0  epsilon: 1.0    steps: 1008  evaluation reward: 22.98\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6427: Policy loss: 0.008290. Value loss: 0.538130. Entropy: 0.923107.\n",
      "Iteration 6428: Policy loss: -0.003415. Value loss: 0.187538. Entropy: 0.897360.\n",
      "Iteration 6429: Policy loss: -0.015203. Value loss: 0.093281. Entropy: 0.901287.\n",
      "episode: 2926   score: 26.0  epsilon: 1.0    steps: 840  evaluation reward: 23.06\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6430: Policy loss: 0.016573. Value loss: 0.569123. Entropy: 0.862976.\n",
      "Iteration 6431: Policy loss: -0.001904. Value loss: 0.223687. Entropy: 0.850333.\n",
      "Iteration 6432: Policy loss: -0.014249. Value loss: 0.116528. Entropy: 0.852394.\n",
      "episode: 2927   score: 30.0  epsilon: 1.0    steps: 712  evaluation reward: 23.22\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6433: Policy loss: 0.010581. Value loss: 0.573599. Entropy: 0.894478.\n",
      "Iteration 6434: Policy loss: -0.003353. Value loss: 0.254085. Entropy: 0.900221.\n",
      "Iteration 6435: Policy loss: -0.012726. Value loss: 0.140794. Entropy: 0.901982.\n",
      "episode: 2928   score: 33.0  epsilon: 1.0    steps: 728  evaluation reward: 23.27\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6436: Policy loss: 0.002942. Value loss: 0.663517. Entropy: 0.921213.\n",
      "Iteration 6437: Policy loss: -0.008122. Value loss: 0.279481. Entropy: 0.912539.\n",
      "Iteration 6438: Policy loss: -0.016450. Value loss: 0.135091. Entropy: 0.905234.\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6439: Policy loss: 0.004528. Value loss: 0.414552. Entropy: 0.844864.\n",
      "Iteration 6440: Policy loss: -0.001546. Value loss: 0.177832. Entropy: 0.853733.\n",
      "Iteration 6441: Policy loss: -0.017855. Value loss: 0.100592. Entropy: 0.859300.\n",
      "episode: 2929   score: 39.0  epsilon: 1.0    steps: 920  evaluation reward: 23.39\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6442: Policy loss: 0.004049. Value loss: 0.787730. Entropy: 0.845398.\n",
      "Iteration 6443: Policy loss: -0.005784. Value loss: 0.256495. Entropy: 0.845399.\n",
      "Iteration 6444: Policy loss: -0.013882. Value loss: 0.151810. Entropy: 0.845788.\n",
      "episode: 2930   score: 30.0  epsilon: 1.0    steps: 216  evaluation reward: 23.4\n",
      "episode: 2931   score: 30.0  epsilon: 1.0    steps: 264  evaluation reward: 23.33\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6445: Policy loss: 0.014522. Value loss: 0.549420. Entropy: 0.899070.\n",
      "Iteration 6446: Policy loss: -0.001556. Value loss: 0.243824. Entropy: 0.886234.\n",
      "Iteration 6447: Policy loss: -0.013138. Value loss: 0.149450. Entropy: 0.891021.\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6448: Policy loss: 0.010436. Value loss: 0.607553. Entropy: 0.895306.\n",
      "Iteration 6449: Policy loss: -0.009307. Value loss: 0.290782. Entropy: 0.905193.\n",
      "Iteration 6450: Policy loss: -0.014314. Value loss: 0.175152. Entropy: 0.892717.\n",
      "episode: 2932   score: 17.0  epsilon: 1.0    steps: 600  evaluation reward: 23.26\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6451: Policy loss: 0.013004. Value loss: 0.874848. Entropy: 0.894366.\n",
      "Iteration 6452: Policy loss: -0.007844. Value loss: 0.365301. Entropy: 0.867305.\n",
      "Iteration 6453: Policy loss: -0.018775. Value loss: 0.180771. Entropy: 0.899961.\n",
      "episode: 2933   score: 28.0  epsilon: 1.0    steps: 360  evaluation reward: 23.3\n",
      "episode: 2934   score: 21.0  epsilon: 1.0    steps: 1016  evaluation reward: 23.26\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6454: Policy loss: 0.009318. Value loss: 0.847778. Entropy: 0.846897.\n",
      "Iteration 6455: Policy loss: -0.000732. Value loss: 0.330228. Entropy: 0.854634.\n",
      "Iteration 6456: Policy loss: -0.013754. Value loss: 0.188928. Entropy: 0.832672.\n",
      "episode: 2935   score: 19.0  epsilon: 1.0    steps: 384  evaluation reward: 23.18\n",
      "episode: 2936   score: 10.0  epsilon: 1.0    steps: 936  evaluation reward: 23.12\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6457: Policy loss: 0.016578. Value loss: 0.795806. Entropy: 0.950211.\n",
      "Iteration 6458: Policy loss: -0.002168. Value loss: 0.296949. Entropy: 0.942520.\n",
      "Iteration 6459: Policy loss: -0.014431. Value loss: 0.184782. Entropy: 0.933153.\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6460: Policy loss: 0.012092. Value loss: 0.431039. Entropy: 0.867941.\n",
      "Iteration 6461: Policy loss: -0.005337. Value loss: 0.145398. Entropy: 0.858053.\n",
      "Iteration 6462: Policy loss: -0.020261. Value loss: 0.085106. Entropy: 0.856255.\n",
      "episode: 2937   score: 17.0  epsilon: 1.0    steps: 336  evaluation reward: 22.89\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6463: Policy loss: 0.009407. Value loss: 0.693509. Entropy: 0.872529.\n",
      "Iteration 6464: Policy loss: -0.007370. Value loss: 0.291450. Entropy: 0.875746.\n",
      "Iteration 6465: Policy loss: -0.016164. Value loss: 0.179302. Entropy: 0.864040.\n",
      "episode: 2938   score: 30.0  epsilon: 1.0    steps: 272  evaluation reward: 22.93\n",
      "episode: 2939   score: 20.0  epsilon: 1.0    steps: 472  evaluation reward: 22.98\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6466: Policy loss: 0.009591. Value loss: 0.383967. Entropy: 0.911895.\n",
      "Iteration 6467: Policy loss: -0.013842. Value loss: 0.207105. Entropy: 0.907866.\n",
      "Iteration 6468: Policy loss: -0.021931. Value loss: 0.151450. Entropy: 0.913099.\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6469: Policy loss: 0.015103. Value loss: 0.572912. Entropy: 0.829554.\n",
      "Iteration 6470: Policy loss: -0.005401. Value loss: 0.315996. Entropy: 0.828397.\n",
      "Iteration 6471: Policy loss: -0.015253. Value loss: 0.208974. Entropy: 0.814673.\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6472: Policy loss: 0.009254. Value loss: 0.600200. Entropy: 0.904411.\n",
      "Iteration 6473: Policy loss: -0.007800. Value loss: 0.330003. Entropy: 0.887525.\n",
      "Iteration 6474: Policy loss: -0.019162. Value loss: 0.222912. Entropy: 0.881718.\n",
      "episode: 2940   score: 10.0  epsilon: 1.0    steps: 976  evaluation reward: 22.82\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6475: Policy loss: 0.003509. Value loss: 0.740702. Entropy: 0.940261.\n",
      "Iteration 6476: Policy loss: -0.010046. Value loss: 0.367201. Entropy: 0.941796.\n",
      "Iteration 6477: Policy loss: -0.016712. Value loss: 0.235172. Entropy: 0.945058.\n",
      "episode: 2941   score: 22.0  epsilon: 1.0    steps: 128  evaluation reward: 22.82\n",
      "episode: 2942   score: 19.0  epsilon: 1.0    steps: 672  evaluation reward: 22.81\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6478: Policy loss: 0.013209. Value loss: 0.640797. Entropy: 0.862371.\n",
      "Iteration 6479: Policy loss: -0.010252. Value loss: 0.267155. Entropy: 0.848240.\n",
      "Iteration 6480: Policy loss: -0.023178. Value loss: 0.148170. Entropy: 0.846795.\n",
      "episode: 2943   score: 19.0  epsilon: 1.0    steps: 64  evaluation reward: 22.86\n",
      "episode: 2944   score: 22.0  epsilon: 1.0    steps: 792  evaluation reward: 22.92\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6481: Policy loss: 0.013418. Value loss: 0.904255. Entropy: 0.894842.\n",
      "Iteration 6482: Policy loss: -0.003534. Value loss: 0.473007. Entropy: 0.879705.\n",
      "Iteration 6483: Policy loss: -0.007722. Value loss: 0.325166. Entropy: 0.867446.\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6484: Policy loss: 0.006932. Value loss: 0.521365. Entropy: 0.840183.\n",
      "Iteration 6485: Policy loss: -0.007538. Value loss: 0.140380. Entropy: 0.841471.\n",
      "Iteration 6486: Policy loss: -0.016351. Value loss: 0.078392. Entropy: 0.855106.\n",
      "episode: 2945   score: 15.0  epsilon: 1.0    steps: 152  evaluation reward: 22.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2946   score: 39.0  epsilon: 1.0    steps: 384  evaluation reward: 23.14\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6487: Policy loss: 0.012311. Value loss: 0.513643. Entropy: 0.844339.\n",
      "Iteration 6488: Policy loss: -0.008490. Value loss: 0.211040. Entropy: 0.826217.\n",
      "Iteration 6489: Policy loss: -0.010666. Value loss: 0.104392. Entropy: 0.825013.\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6490: Policy loss: 0.019673. Value loss: 0.757324. Entropy: 0.966123.\n",
      "Iteration 6491: Policy loss: -0.002262. Value loss: 0.369918. Entropy: 0.949337.\n",
      "Iteration 6492: Policy loss: -0.008094. Value loss: 0.217859. Entropy: 0.955326.\n",
      "episode: 2947   score: 11.0  epsilon: 1.0    steps: 360  evaluation reward: 23.08\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6493: Policy loss: 0.012698. Value loss: 0.761781. Entropy: 0.927631.\n",
      "Iteration 6494: Policy loss: -0.008481. Value loss: 0.310974. Entropy: 0.925326.\n",
      "Iteration 6495: Policy loss: -0.014799. Value loss: 0.184454. Entropy: 0.935758.\n",
      "episode: 2948   score: 41.0  epsilon: 1.0    steps: 992  evaluation reward: 23.32\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6496: Policy loss: 0.006787. Value loss: 0.387772. Entropy: 1.061237.\n",
      "Iteration 6497: Policy loss: -0.010156. Value loss: 0.172859. Entropy: 1.073812.\n",
      "Iteration 6498: Policy loss: -0.022052. Value loss: 0.099547. Entropy: 1.055262.\n",
      "episode: 2949   score: 11.0  epsilon: 1.0    steps: 408  evaluation reward: 23.19\n",
      "episode: 2950   score: 16.0  epsilon: 1.0    steps: 544  evaluation reward: 23.1\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6499: Policy loss: 0.008200. Value loss: 0.620840. Entropy: 0.918706.\n",
      "Iteration 6500: Policy loss: 0.000414. Value loss: 0.314087. Entropy: 0.925425.\n",
      "Iteration 6501: Policy loss: -0.009494. Value loss: 0.153077. Entropy: 0.930976.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6502: Policy loss: 0.021573. Value loss: 0.455780. Entropy: 0.916389.\n",
      "Iteration 6503: Policy loss: -0.007821. Value loss: 0.152635. Entropy: 0.901932.\n",
      "Iteration 6504: Policy loss: -0.017150. Value loss: 0.078421. Entropy: 0.903512.\n",
      "now time :  2019-03-06 14:46:58.578949\n",
      "episode: 2951   score: 20.0  epsilon: 1.0    steps: 600  evaluation reward: 23.07\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6505: Policy loss: -0.000215. Value loss: 0.653378. Entropy: 0.965108.\n",
      "Iteration 6506: Policy loss: -0.007743. Value loss: 0.354105. Entropy: 0.951783.\n",
      "Iteration 6507: Policy loss: -0.016035. Value loss: 0.228269. Entropy: 0.945228.\n",
      "episode: 2952   score: 14.0  epsilon: 1.0    steps: 16  evaluation reward: 23.09\n",
      "episode: 2953   score: 42.0  epsilon: 1.0    steps: 856  evaluation reward: 23.19\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6508: Policy loss: 0.016106. Value loss: 0.547432. Entropy: 0.884904.\n",
      "Iteration 6509: Policy loss: -0.001661. Value loss: 0.217168. Entropy: 0.865179.\n",
      "Iteration 6510: Policy loss: -0.013619. Value loss: 0.128039. Entropy: 0.870379.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6511: Policy loss: 0.007475. Value loss: 0.313510. Entropy: 0.851988.\n",
      "Iteration 6512: Policy loss: -0.011458. Value loss: 0.123640. Entropy: 0.837815.\n",
      "Iteration 6513: Policy loss: -0.018183. Value loss: 0.078945. Entropy: 0.841026.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6514: Policy loss: 0.013129. Value loss: 0.313185. Entropy: 0.851310.\n",
      "Iteration 6515: Policy loss: -0.013478. Value loss: 0.132401. Entropy: 0.857061.\n",
      "Iteration 6516: Policy loss: -0.021936. Value loss: 0.085209. Entropy: 0.856628.\n",
      "episode: 2954   score: 18.0  epsilon: 1.0    steps: 776  evaluation reward: 23.23\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6517: Policy loss: 0.024267. Value loss: 0.596556. Entropy: 0.938833.\n",
      "Iteration 6518: Policy loss: 0.000206. Value loss: 0.236451. Entropy: 0.914704.\n",
      "Iteration 6519: Policy loss: -0.012483. Value loss: 0.132426. Entropy: 0.915132.\n",
      "episode: 2955   score: 25.0  epsilon: 1.0    steps: 672  evaluation reward: 23.18\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6520: Policy loss: 0.016674. Value loss: 0.567839. Entropy: 0.929685.\n",
      "Iteration 6521: Policy loss: 0.003891. Value loss: 0.239677. Entropy: 0.918787.\n",
      "Iteration 6522: Policy loss: -0.005843. Value loss: 0.141240. Entropy: 0.904329.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6523: Policy loss: 0.014591. Value loss: 1.002952. Entropy: 0.878827.\n",
      "Iteration 6524: Policy loss: 0.003948. Value loss: 0.343344. Entropy: 0.866269.\n",
      "Iteration 6525: Policy loss: -0.013252. Value loss: 0.180232. Entropy: 0.872294.\n",
      "episode: 2956   score: 10.0  epsilon: 1.0    steps: 72  evaluation reward: 23.09\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6526: Policy loss: 0.015606. Value loss: 0.779038. Entropy: 0.935094.\n",
      "Iteration 6527: Policy loss: -0.004933. Value loss: 0.319068. Entropy: 0.943859.\n",
      "Iteration 6528: Policy loss: -0.017625. Value loss: 0.135508. Entropy: 0.938709.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6529: Policy loss: 0.014014. Value loss: 0.901308. Entropy: 0.877197.\n",
      "Iteration 6530: Policy loss: 0.002057. Value loss: 0.318919. Entropy: 0.889227.\n",
      "Iteration 6531: Policy loss: -0.008519. Value loss: 0.158896. Entropy: 0.884897.\n",
      "episode: 2957   score: 50.0  epsilon: 1.0    steps: 128  evaluation reward: 23.3\n",
      "episode: 2958   score: 27.0  epsilon: 1.0    steps: 448  evaluation reward: 23.45\n",
      "episode: 2959   score: 42.0  epsilon: 1.0    steps: 928  evaluation reward: 23.69\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6532: Policy loss: 0.012350. Value loss: 1.089189. Entropy: 0.939540.\n",
      "Iteration 6533: Policy loss: -0.000444. Value loss: 0.550478. Entropy: 0.927749.\n",
      "Iteration 6534: Policy loss: -0.013972. Value loss: 0.326462. Entropy: 0.916984.\n",
      "episode: 2960   score: 42.0  epsilon: 1.0    steps: 968  evaluation reward: 23.95\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6535: Policy loss: 0.011423. Value loss: 0.744245. Entropy: 0.891485.\n",
      "Iteration 6536: Policy loss: -0.001818. Value loss: 0.291899. Entropy: 0.905326.\n",
      "Iteration 6537: Policy loss: -0.014602. Value loss: 0.159910. Entropy: 0.897015.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6538: Policy loss: 0.007932. Value loss: 0.528838. Entropy: 0.916311.\n",
      "Iteration 6539: Policy loss: -0.006214. Value loss: 0.302977. Entropy: 0.900463.\n",
      "Iteration 6540: Policy loss: -0.015549. Value loss: 0.195268. Entropy: 0.908963.\n",
      "episode: 2961   score: 26.0  epsilon: 1.0    steps: 1000  evaluation reward: 24.0\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6541: Policy loss: 0.012980. Value loss: 1.314032. Entropy: 0.925498.\n",
      "Iteration 6542: Policy loss: 0.008075. Value loss: 0.636097. Entropy: 0.935691.\n",
      "Iteration 6543: Policy loss: -0.005437. Value loss: 0.344374. Entropy: 0.919661.\n",
      "episode: 2962   score: 25.0  epsilon: 1.0    steps: 704  evaluation reward: 24.07\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6544: Policy loss: 0.013704. Value loss: 0.696543. Entropy: 0.913375.\n",
      "Iteration 6545: Policy loss: 0.006814. Value loss: 0.335896. Entropy: 0.923083.\n",
      "Iteration 6546: Policy loss: -0.006855. Value loss: 0.205051. Entropy: 0.920167.\n",
      "episode: 2963   score: 31.0  epsilon: 1.0    steps: 392  evaluation reward: 24.2\n",
      "episode: 2964   score: 10.0  epsilon: 1.0    steps: 720  evaluation reward: 24.11\n",
      "episode: 2965   score: 53.0  epsilon: 1.0    steps: 832  evaluation reward: 24.48\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6547: Policy loss: 0.004700. Value loss: 0.825043. Entropy: 0.872624.\n",
      "Iteration 6548: Policy loss: -0.006300. Value loss: 0.339918. Entropy: 0.870595.\n",
      "Iteration 6549: Policy loss: -0.015934. Value loss: 0.175384. Entropy: 0.868594.\n",
      "episode: 2966   score: 14.0  epsilon: 1.0    steps: 944  evaluation reward: 24.47\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6550: Policy loss: 0.014500. Value loss: 0.448798. Entropy: 0.894815.\n",
      "Iteration 6551: Policy loss: 0.001964. Value loss: 0.223306. Entropy: 0.880147.\n",
      "Iteration 6552: Policy loss: -0.014358. Value loss: 0.148614. Entropy: 0.862190.\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6553: Policy loss: 0.006304. Value loss: 0.396910. Entropy: 0.843416.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6554: Policy loss: -0.002393. Value loss: 0.208477. Entropy: 0.832810.\n",
      "Iteration 6555: Policy loss: -0.017923. Value loss: 0.138330. Entropy: 0.842905.\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6556: Policy loss: 0.016754. Value loss: 0.657665. Entropy: 0.993879.\n",
      "Iteration 6557: Policy loss: 0.002169. Value loss: 0.280109. Entropy: 0.960451.\n",
      "Iteration 6558: Policy loss: -0.014083. Value loss: 0.153007. Entropy: 0.975780.\n",
      "episode: 2967   score: 22.0  epsilon: 1.0    steps: 112  evaluation reward: 24.36\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6559: Policy loss: 0.007791. Value loss: 0.547061. Entropy: 0.755201.\n",
      "Iteration 6560: Policy loss: -0.005950. Value loss: 0.196501. Entropy: 0.747280.\n",
      "Iteration 6561: Policy loss: -0.013684. Value loss: 0.107973. Entropy: 0.750250.\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6562: Policy loss: 0.017245. Value loss: 0.476932. Entropy: 0.891415.\n",
      "Iteration 6563: Policy loss: -0.006423. Value loss: 0.201911. Entropy: 0.889521.\n",
      "Iteration 6564: Policy loss: -0.017570. Value loss: 0.114284. Entropy: 0.896112.\n",
      "episode: 2968   score: 33.0  epsilon: 1.0    steps: 872  evaluation reward: 24.52\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6565: Policy loss: 0.010058. Value loss: 1.363866. Entropy: 0.843544.\n",
      "Iteration 6566: Policy loss: 0.004301. Value loss: 0.562718. Entropy: 0.828063.\n",
      "Iteration 6567: Policy loss: -0.001242. Value loss: 0.250017. Entropy: 0.836472.\n",
      "episode: 2969   score: 22.0  epsilon: 1.0    steps: 1000  evaluation reward: 24.52\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6568: Policy loss: 0.013172. Value loss: 0.934333. Entropy: 0.892811.\n",
      "Iteration 6569: Policy loss: 0.009105. Value loss: 0.429804. Entropy: 0.891810.\n",
      "Iteration 6570: Policy loss: -0.006260. Value loss: 0.271063. Entropy: 0.897642.\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6571: Policy loss: 0.014849. Value loss: 0.562770. Entropy: 0.928353.\n",
      "Iteration 6572: Policy loss: -0.001698. Value loss: 0.183307. Entropy: 0.929431.\n",
      "Iteration 6573: Policy loss: -0.014274. Value loss: 0.113365. Entropy: 0.930524.\n",
      "episode: 2970   score: 25.0  epsilon: 1.0    steps: 360  evaluation reward: 24.59\n",
      "episode: 2971   score: 29.0  epsilon: 1.0    steps: 472  evaluation reward: 24.62\n",
      "episode: 2972   score: 21.0  epsilon: 1.0    steps: 1016  evaluation reward: 24.59\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6574: Policy loss: 0.013397. Value loss: 0.973553. Entropy: 0.906001.\n",
      "Iteration 6575: Policy loss: 0.001325. Value loss: 0.434264. Entropy: 0.890203.\n",
      "Iteration 6576: Policy loss: -0.011470. Value loss: 0.226326. Entropy: 0.888078.\n",
      "episode: 2973   score: 27.0  epsilon: 1.0    steps: 136  evaluation reward: 24.67\n",
      "episode: 2974   score: 52.0  epsilon: 1.0    steps: 552  evaluation reward: 24.97\n",
      "episode: 2975   score: 8.0  epsilon: 1.0    steps: 744  evaluation reward: 24.87\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6577: Policy loss: 0.015809. Value loss: 0.576627. Entropy: 0.875479.\n",
      "Iteration 6578: Policy loss: 0.005170. Value loss: 0.248278. Entropy: 0.851309.\n",
      "Iteration 6579: Policy loss: -0.008580. Value loss: 0.120768. Entropy: 0.848837.\n",
      "episode: 2976   score: 16.0  epsilon: 1.0    steps: 792  evaluation reward: 24.87\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6580: Policy loss: 0.006104. Value loss: 0.399996. Entropy: 0.854059.\n",
      "Iteration 6581: Policy loss: -0.009633. Value loss: 0.194564. Entropy: 0.823954.\n",
      "Iteration 6582: Policy loss: -0.018770. Value loss: 0.119775. Entropy: 0.817999.\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6583: Policy loss: 0.008843. Value loss: 0.307251. Entropy: 0.751512.\n",
      "Iteration 6584: Policy loss: -0.005142. Value loss: 0.121079. Entropy: 0.738168.\n",
      "Iteration 6585: Policy loss: -0.015406. Value loss: 0.069060. Entropy: 0.753061.\n",
      "episode: 2977   score: 11.0  epsilon: 1.0    steps: 520  evaluation reward: 24.76\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6586: Policy loss: 0.014297. Value loss: 0.637737. Entropy: 0.870247.\n",
      "Iteration 6587: Policy loss: -0.001990. Value loss: 0.265412. Entropy: 0.854988.\n",
      "Iteration 6588: Policy loss: -0.016656. Value loss: 0.166946. Entropy: 0.856006.\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6589: Policy loss: 0.007297. Value loss: 0.280759. Entropy: 0.877872.\n",
      "Iteration 6590: Policy loss: -0.011792. Value loss: 0.096588. Entropy: 0.890891.\n",
      "Iteration 6591: Policy loss: -0.020053. Value loss: 0.054754. Entropy: 0.884419.\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6592: Policy loss: 0.007163. Value loss: 0.716244. Entropy: 0.882726.\n",
      "Iteration 6593: Policy loss: 0.003469. Value loss: 0.292215. Entropy: 0.864389.\n",
      "Iteration 6594: Policy loss: -0.011790. Value loss: 0.162166. Entropy: 0.885105.\n",
      "episode: 2978   score: 10.0  epsilon: 1.0    steps: 248  evaluation reward: 24.62\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6595: Policy loss: 0.013502. Value loss: 0.941424. Entropy: 0.957760.\n",
      "Iteration 6596: Policy loss: -0.000251. Value loss: 0.402135. Entropy: 0.952166.\n",
      "Iteration 6597: Policy loss: -0.007982. Value loss: 0.190101. Entropy: 0.949510.\n",
      "episode: 2979   score: 24.0  epsilon: 1.0    steps: 120  evaluation reward: 24.6\n",
      "episode: 2980   score: 21.0  epsilon: 1.0    steps: 872  evaluation reward: 24.54\n",
      "episode: 2981   score: 25.0  epsilon: 1.0    steps: 1024  evaluation reward: 24.52\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6598: Policy loss: 0.007392. Value loss: 0.698902. Entropy: 0.829557.\n",
      "Iteration 6599: Policy loss: -0.003076. Value loss: 0.237162. Entropy: 0.828203.\n",
      "Iteration 6600: Policy loss: -0.010298. Value loss: 0.141829. Entropy: 0.825746.\n",
      "episode: 2982   score: 18.0  epsilon: 1.0    steps: 648  evaluation reward: 24.54\n",
      "episode: 2983   score: 23.0  epsilon: 1.0    steps: 856  evaluation reward: 24.6\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6601: Policy loss: 0.006745. Value loss: 0.617878. Entropy: 0.810626.\n",
      "Iteration 6602: Policy loss: -0.008707. Value loss: 0.248232. Entropy: 0.811936.\n",
      "Iteration 6603: Policy loss: -0.017622. Value loss: 0.137592. Entropy: 0.808350.\n",
      "episode: 2984   score: 30.0  epsilon: 1.0    steps: 864  evaluation reward: 24.7\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6604: Policy loss: 0.007948. Value loss: 0.374738. Entropy: 0.820395.\n",
      "Iteration 6605: Policy loss: -0.009665. Value loss: 0.162417. Entropy: 0.819109.\n",
      "Iteration 6606: Policy loss: -0.018558. Value loss: 0.095029. Entropy: 0.813878.\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6607: Policy loss: 0.010537. Value loss: 0.611195. Entropy: 0.796756.\n",
      "Iteration 6608: Policy loss: 0.000291. Value loss: 0.344718. Entropy: 0.787010.\n",
      "Iteration 6609: Policy loss: -0.007770. Value loss: 0.221384. Entropy: 0.791554.\n",
      "episode: 2985   score: 16.0  epsilon: 1.0    steps: 784  evaluation reward: 24.67\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6610: Policy loss: 0.009242. Value loss: 0.619456. Entropy: 0.873838.\n",
      "Iteration 6611: Policy loss: 0.000635. Value loss: 0.252644. Entropy: 0.864375.\n",
      "Iteration 6612: Policy loss: -0.016731. Value loss: 0.155029. Entropy: 0.848064.\n",
      "episode: 2986   score: 30.0  epsilon: 1.0    steps: 168  evaluation reward: 24.68\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6613: Policy loss: 0.007832. Value loss: 0.509874. Entropy: 0.779322.\n",
      "Iteration 6614: Policy loss: -0.005040. Value loss: 0.166834. Entropy: 0.789102.\n",
      "Iteration 6615: Policy loss: -0.016193. Value loss: 0.106431. Entropy: 0.780296.\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6616: Policy loss: 0.007722. Value loss: 0.500657. Entropy: 0.831272.\n",
      "Iteration 6617: Policy loss: -0.010538. Value loss: 0.199787. Entropy: 0.814627.\n",
      "Iteration 6618: Policy loss: -0.020043. Value loss: 0.117315. Entropy: 0.815146.\n",
      "episode: 2987   score: 20.0  epsilon: 1.0    steps: 320  evaluation reward: 24.51\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6619: Policy loss: 0.009501. Value loss: 0.359083. Entropy: 0.840627.\n",
      "Iteration 6620: Policy loss: -0.011656. Value loss: 0.150169. Entropy: 0.853454.\n",
      "Iteration 6621: Policy loss: -0.018477. Value loss: 0.088748. Entropy: 0.843792.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2988   score: 24.0  epsilon: 1.0    steps: 376  evaluation reward: 24.55\n",
      "episode: 2989   score: 16.0  epsilon: 1.0    steps: 824  evaluation reward: 24.44\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6622: Policy loss: 0.010735. Value loss: 0.648275. Entropy: 0.881469.\n",
      "Iteration 6623: Policy loss: -0.008063. Value loss: 0.250970. Entropy: 0.895128.\n",
      "Iteration 6624: Policy loss: -0.019817. Value loss: 0.168788. Entropy: 0.914116.\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6625: Policy loss: 0.001844. Value loss: 0.274492. Entropy: 0.828902.\n",
      "Iteration 6626: Policy loss: -0.012365. Value loss: 0.104029. Entropy: 0.837602.\n",
      "Iteration 6627: Policy loss: -0.018328. Value loss: 0.056438. Entropy: 0.832913.\n",
      "episode: 2990   score: 20.0  epsilon: 1.0    steps: 336  evaluation reward: 24.38\n",
      "episode: 2991   score: 28.0  epsilon: 1.0    steps: 920  evaluation reward: 24.37\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6628: Policy loss: 0.012277. Value loss: 0.618183. Entropy: 0.815529.\n",
      "Iteration 6629: Policy loss: 0.000860. Value loss: 0.168778. Entropy: 0.823502.\n",
      "Iteration 6630: Policy loss: -0.013137. Value loss: 0.089473. Entropy: 0.817189.\n",
      "episode: 2992   score: 23.0  epsilon: 1.0    steps: 224  evaluation reward: 24.36\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6631: Policy loss: 0.007131. Value loss: 0.222430. Entropy: 0.823362.\n",
      "Iteration 6632: Policy loss: -0.006926. Value loss: 0.073931. Entropy: 0.829243.\n",
      "Iteration 6633: Policy loss: -0.015027. Value loss: 0.043570. Entropy: 0.833290.\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6634: Policy loss: 0.004183. Value loss: 0.298846. Entropy: 0.897321.\n",
      "Iteration 6635: Policy loss: -0.011707. Value loss: 0.095696. Entropy: 0.884052.\n",
      "Iteration 6636: Policy loss: -0.019604. Value loss: 0.054181. Entropy: 0.878190.\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6637: Policy loss: 0.012402. Value loss: 0.444021. Entropy: 0.824149.\n",
      "Iteration 6638: Policy loss: -0.003145. Value loss: 0.165006. Entropy: 0.801733.\n",
      "Iteration 6639: Policy loss: -0.015939. Value loss: 0.097828. Entropy: 0.791789.\n",
      "episode: 2993   score: 29.0  epsilon: 1.0    steps: 128  evaluation reward: 24.42\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6640: Policy loss: 0.013233. Value loss: 0.643492. Entropy: 0.829119.\n",
      "Iteration 6641: Policy loss: -0.007310. Value loss: 0.271197. Entropy: 0.830580.\n",
      "Iteration 6642: Policy loss: -0.020710. Value loss: 0.148756. Entropy: 0.821294.\n",
      "episode: 2994   score: 20.0  epsilon: 1.0    steps: 864  evaluation reward: 24.36\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6643: Policy loss: 0.007918. Value loss: 0.596892. Entropy: 0.864792.\n",
      "Iteration 6644: Policy loss: 0.006004. Value loss: 0.222103. Entropy: 0.847209.\n",
      "Iteration 6645: Policy loss: -0.008384. Value loss: 0.095475. Entropy: 0.865158.\n",
      "episode: 2995   score: 30.0  epsilon: 1.0    steps: 88  evaluation reward: 24.46\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6646: Policy loss: 0.003865. Value loss: 0.245819. Entropy: 0.784812.\n",
      "Iteration 6647: Policy loss: -0.013942. Value loss: 0.089607. Entropy: 0.776934.\n",
      "Iteration 6648: Policy loss: -0.024579. Value loss: 0.049458. Entropy: 0.775684.\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6649: Policy loss: 0.012718. Value loss: 0.585724. Entropy: 0.896656.\n",
      "Iteration 6650: Policy loss: 0.007246. Value loss: 0.225647. Entropy: 0.895298.\n",
      "Iteration 6651: Policy loss: -0.005537. Value loss: 0.130690. Entropy: 0.896360.\n",
      "episode: 2996   score: 19.0  epsilon: 1.0    steps: 312  evaluation reward: 24.34\n",
      "episode: 2997   score: 16.0  epsilon: 1.0    steps: 768  evaluation reward: 24.3\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6652: Policy loss: 0.006318. Value loss: 0.483494. Entropy: 0.830760.\n",
      "Iteration 6653: Policy loss: -0.012009. Value loss: 0.195282. Entropy: 0.817198.\n",
      "Iteration 6654: Policy loss: -0.022559. Value loss: 0.116453. Entropy: 0.816965.\n",
      "episode: 2998   score: 20.0  epsilon: 1.0    steps: 440  evaluation reward: 24.2\n",
      "episode: 2999   score: 36.0  epsilon: 1.0    steps: 696  evaluation reward: 24.25\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6655: Policy loss: 0.011656. Value loss: 0.595450. Entropy: 0.789878.\n",
      "Iteration 6656: Policy loss: 0.008390. Value loss: 0.227608. Entropy: 0.802333.\n",
      "Iteration 6657: Policy loss: -0.005476. Value loss: 0.122303. Entropy: 0.801044.\n",
      "episode: 3000   score: 36.0  epsilon: 1.0    steps: 32  evaluation reward: 24.31\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6658: Policy loss: 0.009061. Value loss: 0.388374. Entropy: 0.746039.\n",
      "Iteration 6659: Policy loss: -0.002860. Value loss: 0.152864. Entropy: 0.758968.\n",
      "Iteration 6660: Policy loss: -0.016483. Value loss: 0.089092. Entropy: 0.745674.\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6661: Policy loss: 0.005547. Value loss: 0.300056. Entropy: 0.810549.\n",
      "Iteration 6662: Policy loss: -0.012276. Value loss: 0.109314. Entropy: 0.810316.\n",
      "Iteration 6663: Policy loss: -0.021622. Value loss: 0.062497. Entropy: 0.803022.\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6664: Policy loss: 0.012779. Value loss: 0.309248. Entropy: 0.872686.\n",
      "Iteration 6665: Policy loss: -0.006523. Value loss: 0.107759. Entropy: 0.878986.\n",
      "Iteration 6666: Policy loss: -0.019004. Value loss: 0.069461. Entropy: 0.865711.\n",
      "now time :  2019-03-06 14:50:26.136655\n",
      "episode: 3001   score: 24.0  epsilon: 1.0    steps: 712  evaluation reward: 24.44\n",
      "episode: 3002   score: 18.0  epsilon: 1.0    steps: 824  evaluation reward: 24.38\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6667: Policy loss: 0.021849. Value loss: 0.678462. Entropy: 0.852994.\n",
      "Iteration 6668: Policy loss: -0.004710. Value loss: 0.257423. Entropy: 0.843034.\n",
      "Iteration 6669: Policy loss: -0.016029. Value loss: 0.118115. Entropy: 0.832019.\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6670: Policy loss: 0.010314. Value loss: 0.752343. Entropy: 0.878113.\n",
      "Iteration 6671: Policy loss: 0.000838. Value loss: 0.315750. Entropy: 0.874632.\n",
      "Iteration 6672: Policy loss: -0.004001. Value loss: 0.145738. Entropy: 0.883409.\n",
      "episode: 3003   score: 39.0  epsilon: 1.0    steps: 280  evaluation reward: 24.43\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6673: Policy loss: 0.004782. Value loss: 0.947628. Entropy: 0.890728.\n",
      "Iteration 6674: Policy loss: -0.002790. Value loss: 0.418590. Entropy: 0.886242.\n",
      "Iteration 6675: Policy loss: -0.013141. Value loss: 0.233069. Entropy: 0.887569.\n",
      "episode: 3004   score: 24.0  epsilon: 1.0    steps: 576  evaluation reward: 24.46\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6676: Policy loss: 0.009534. Value loss: 0.568957. Entropy: 0.924892.\n",
      "Iteration 6677: Policy loss: -0.001839. Value loss: 0.239337. Entropy: 0.943739.\n",
      "Iteration 6678: Policy loss: -0.011244. Value loss: 0.114579. Entropy: 0.943865.\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6679: Policy loss: 0.007284. Value loss: 0.633053. Entropy: 0.937918.\n",
      "Iteration 6680: Policy loss: -0.001863. Value loss: 0.258248. Entropy: 0.920093.\n",
      "Iteration 6681: Policy loss: -0.015262. Value loss: 0.133434. Entropy: 0.924482.\n",
      "episode: 3005   score: 23.0  epsilon: 1.0    steps: 320  evaluation reward: 24.47\n",
      "episode: 3006   score: 28.0  epsilon: 1.0    steps: 352  evaluation reward: 24.55\n",
      "episode: 3007   score: 16.0  epsilon: 1.0    steps: 864  evaluation reward: 24.5\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6682: Policy loss: 0.019472. Value loss: 0.655616. Entropy: 1.009275.\n",
      "Iteration 6683: Policy loss: 0.000745. Value loss: 0.276594. Entropy: 1.016558.\n",
      "Iteration 6684: Policy loss: -0.018638. Value loss: 0.178928. Entropy: 1.011175.\n",
      "episode: 3008   score: 28.0  epsilon: 1.0    steps: 360  evaluation reward: 24.64\n",
      "episode: 3009   score: 37.0  epsilon: 1.0    steps: 848  evaluation reward: 24.87\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6685: Policy loss: 0.020553. Value loss: 0.729874. Entropy: 0.869696.\n",
      "Iteration 6686: Policy loss: 0.001155. Value loss: 0.290565. Entropy: 0.874937.\n",
      "Iteration 6687: Policy loss: -0.012672. Value loss: 0.174474. Entropy: 0.862523.\n",
      "episode: 3010   score: 14.0  epsilon: 1.0    steps: 848  evaluation reward: 24.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6688: Policy loss: 0.017158. Value loss: 0.742422. Entropy: 0.856960.\n",
      "Iteration 6689: Policy loss: 0.002939. Value loss: 0.396338. Entropy: 0.850033.\n",
      "Iteration 6690: Policy loss: -0.008994. Value loss: 0.278711. Entropy: 0.827178.\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6691: Policy loss: 0.008808. Value loss: 0.458111. Entropy: 0.910683.\n",
      "Iteration 6692: Policy loss: -0.003339. Value loss: 0.166387. Entropy: 0.903445.\n",
      "Iteration 6693: Policy loss: -0.017501. Value loss: 0.076169. Entropy: 0.894357.\n",
      "episode: 3011   score: 22.0  epsilon: 1.0    steps: 312  evaluation reward: 24.8\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6694: Policy loss: 0.006578. Value loss: 0.521107. Entropy: 0.796867.\n",
      "Iteration 6695: Policy loss: -0.003371. Value loss: 0.226183. Entropy: 0.785361.\n",
      "Iteration 6696: Policy loss: -0.014393. Value loss: 0.131117. Entropy: 0.775535.\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6697: Policy loss: 0.007429. Value loss: 0.402029. Entropy: 0.888916.\n",
      "Iteration 6698: Policy loss: -0.007600. Value loss: 0.137125. Entropy: 0.867938.\n",
      "Iteration 6699: Policy loss: -0.020394. Value loss: 0.082395. Entropy: 0.875630.\n",
      "episode: 3012   score: 20.0  epsilon: 1.0    steps: 336  evaluation reward: 24.67\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6700: Policy loss: 0.020170. Value loss: 0.523303. Entropy: 0.900889.\n",
      "Iteration 6701: Policy loss: -0.003823. Value loss: 0.231583. Entropy: 0.900195.\n",
      "Iteration 6702: Policy loss: -0.015829. Value loss: 0.125839. Entropy: 0.893760.\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6703: Policy loss: 0.013251. Value loss: 0.884209. Entropy: 0.846514.\n",
      "Iteration 6704: Policy loss: -0.004955. Value loss: 0.365393. Entropy: 0.854318.\n",
      "Iteration 6705: Policy loss: -0.011494. Value loss: 0.186673. Entropy: 0.854544.\n",
      "episode: 3013   score: 29.0  epsilon: 1.0    steps: 264  evaluation reward: 24.71\n",
      "episode: 3014   score: 20.0  epsilon: 1.0    steps: 352  evaluation reward: 24.68\n",
      "episode: 3015   score: 19.0  epsilon: 1.0    steps: 752  evaluation reward: 24.6\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6706: Policy loss: 0.007698. Value loss: 0.564695. Entropy: 0.798032.\n",
      "Iteration 6707: Policy loss: -0.008411. Value loss: 0.202401. Entropy: 0.790202.\n",
      "Iteration 6708: Policy loss: -0.017456. Value loss: 0.141062. Entropy: 0.792511.\n",
      "episode: 3016   score: 21.0  epsilon: 1.0    steps: 968  evaluation reward: 24.57\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6709: Policy loss: 0.009012. Value loss: 0.629914. Entropy: 0.843139.\n",
      "Iteration 6710: Policy loss: -0.011553. Value loss: 0.250105. Entropy: 0.848878.\n",
      "Iteration 6711: Policy loss: -0.021739. Value loss: 0.138399. Entropy: 0.859303.\n",
      "episode: 3017   score: 31.0  epsilon: 1.0    steps: 296  evaluation reward: 24.59\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6712: Policy loss: 0.004270. Value loss: 0.510616. Entropy: 0.812890.\n",
      "Iteration 6713: Policy loss: -0.010347. Value loss: 0.261820. Entropy: 0.818062.\n",
      "Iteration 6714: Policy loss: -0.017165. Value loss: 0.176711. Entropy: 0.818317.\n",
      "episode: 3018   score: 16.0  epsilon: 1.0    steps: 448  evaluation reward: 24.58\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6715: Policy loss: 0.012153. Value loss: 0.613976. Entropy: 0.775831.\n",
      "Iteration 6716: Policy loss: -0.004736. Value loss: 0.243692. Entropy: 0.775148.\n",
      "Iteration 6717: Policy loss: -0.013496. Value loss: 0.133867. Entropy: 0.782161.\n",
      "episode: 3019   score: 20.0  epsilon: 1.0    steps: 936  evaluation reward: 24.59\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6718: Policy loss: 0.006756. Value loss: 0.392815. Entropy: 0.878457.\n",
      "Iteration 6719: Policy loss: -0.008267. Value loss: 0.185435. Entropy: 0.887900.\n",
      "Iteration 6720: Policy loss: -0.019327. Value loss: 0.116604. Entropy: 0.873416.\n",
      "episode: 3020   score: 31.0  epsilon: 1.0    steps: 840  evaluation reward: 24.65\n",
      "episode: 3021   score: 16.0  epsilon: 1.0    steps: 968  evaluation reward: 24.58\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6721: Policy loss: 0.005425. Value loss: 0.444239. Entropy: 0.869313.\n",
      "Iteration 6722: Policy loss: -0.006956. Value loss: 0.132361. Entropy: 0.868608.\n",
      "Iteration 6723: Policy loss: -0.022417. Value loss: 0.088451. Entropy: 0.873465.\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6724: Policy loss: 0.011668. Value loss: 0.301442. Entropy: 0.791308.\n",
      "Iteration 6725: Policy loss: -0.009296. Value loss: 0.161772. Entropy: 0.789373.\n",
      "Iteration 6726: Policy loss: -0.018942. Value loss: 0.103343. Entropy: 0.786737.\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6727: Policy loss: 0.005826. Value loss: 0.633108. Entropy: 0.903453.\n",
      "Iteration 6728: Policy loss: -0.008048. Value loss: 0.369941. Entropy: 0.913687.\n",
      "Iteration 6729: Policy loss: -0.017190. Value loss: 0.225836. Entropy: 0.909498.\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6730: Policy loss: 0.017710. Value loss: 0.716840. Entropy: 0.964498.\n",
      "Iteration 6731: Policy loss: -0.002213. Value loss: 0.322083. Entropy: 0.958132.\n",
      "Iteration 6732: Policy loss: -0.011075. Value loss: 0.161655. Entropy: 0.949304.\n",
      "episode: 3022   score: 22.0  epsilon: 1.0    steps: 224  evaluation reward: 24.4\n",
      "episode: 3023   score: 25.0  epsilon: 1.0    steps: 552  evaluation reward: 24.23\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6733: Policy loss: 0.009141. Value loss: 0.610734. Entropy: 0.932073.\n",
      "Iteration 6734: Policy loss: -0.007053. Value loss: 0.206111. Entropy: 0.945549.\n",
      "Iteration 6735: Policy loss: -0.014328. Value loss: 0.098922. Entropy: 0.935174.\n",
      "episode: 3024   score: 27.0  epsilon: 1.0    steps: 328  evaluation reward: 24.28\n",
      "episode: 3025   score: 21.0  epsilon: 1.0    steps: 520  evaluation reward: 24.15\n",
      "episode: 3026   score: 17.0  epsilon: 1.0    steps: 976  evaluation reward: 24.06\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6736: Policy loss: 0.012966. Value loss: 0.398884. Entropy: 0.901735.\n",
      "Iteration 6737: Policy loss: -0.010505. Value loss: 0.126516. Entropy: 0.911333.\n",
      "Iteration 6738: Policy loss: -0.019460. Value loss: 0.069896. Entropy: 0.911044.\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6739: Policy loss: 0.013946. Value loss: 0.340655. Entropy: 0.843973.\n",
      "Iteration 6740: Policy loss: -0.007583. Value loss: 0.133044. Entropy: 0.831975.\n",
      "Iteration 6741: Policy loss: -0.020727. Value loss: 0.084336. Entropy: 0.853056.\n",
      "episode: 3027   score: 24.0  epsilon: 1.0    steps: 392  evaluation reward: 24.0\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6742: Policy loss: 0.006817. Value loss: 0.476757. Entropy: 0.846776.\n",
      "Iteration 6743: Policy loss: -0.004570. Value loss: 0.221985. Entropy: 0.851474.\n",
      "Iteration 6744: Policy loss: -0.015258. Value loss: 0.152750. Entropy: 0.838279.\n",
      "episode: 3028   score: 29.0  epsilon: 1.0    steps: 872  evaluation reward: 23.96\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6745: Policy loss: 0.011041. Value loss: 0.559651. Entropy: 0.866854.\n",
      "Iteration 6746: Policy loss: -0.004576. Value loss: 0.244550. Entropy: 0.867727.\n",
      "Iteration 6747: Policy loss: -0.015762. Value loss: 0.121843. Entropy: 0.865749.\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6748: Policy loss: 0.011829. Value loss: 0.449087. Entropy: 0.917846.\n",
      "Iteration 6749: Policy loss: -0.003279. Value loss: 0.242818. Entropy: 0.908006.\n",
      "Iteration 6750: Policy loss: -0.010860. Value loss: 0.159001. Entropy: 0.906177.\n",
      "episode: 3029   score: 24.0  epsilon: 1.0    steps: 184  evaluation reward: 23.81\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6751: Policy loss: 0.017201. Value loss: 0.494187. Entropy: 0.899484.\n",
      "Iteration 6752: Policy loss: -0.007672. Value loss: 0.210462. Entropy: 0.901204.\n",
      "Iteration 6753: Policy loss: -0.019758. Value loss: 0.117576. Entropy: 0.905552.\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6754: Policy loss: 0.011254. Value loss: 0.613612. Entropy: 0.896033.\n",
      "Iteration 6755: Policy loss: -0.010718. Value loss: 0.241342. Entropy: 0.878779.\n",
      "Iteration 6756: Policy loss: -0.018144. Value loss: 0.136589. Entropy: 0.885659.\n",
      "episode: 3030   score: 25.0  epsilon: 1.0    steps: 8  evaluation reward: 23.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6757: Policy loss: 0.014622. Value loss: 0.781504. Entropy: 0.904315.\n",
      "Iteration 6758: Policy loss: -0.004861. Value loss: 0.386178. Entropy: 0.901394.\n",
      "Iteration 6759: Policy loss: 0.005096. Value loss: 0.197665. Entropy: 0.896588.\n",
      "episode: 3031   score: 19.0  epsilon: 1.0    steps: 224  evaluation reward: 23.65\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6760: Policy loss: 0.012740. Value loss: 0.668736. Entropy: 0.929418.\n",
      "Iteration 6761: Policy loss: 0.005385. Value loss: 0.264787. Entropy: 0.916439.\n",
      "Iteration 6762: Policy loss: -0.012836. Value loss: 0.165621. Entropy: 0.924935.\n",
      "episode: 3032   score: 17.0  epsilon: 1.0    steps: 88  evaluation reward: 23.65\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6763: Policy loss: 0.012717. Value loss: 0.555367. Entropy: 0.867447.\n",
      "Iteration 6764: Policy loss: -0.003049. Value loss: 0.197554. Entropy: 0.877598.\n",
      "Iteration 6765: Policy loss: -0.015314. Value loss: 0.109062. Entropy: 0.875051.\n",
      "episode: 3033   score: 32.0  epsilon: 1.0    steps: 408  evaluation reward: 23.69\n",
      "episode: 3034   score: 30.0  epsilon: 1.0    steps: 856  evaluation reward: 23.78\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6766: Policy loss: 0.008278. Value loss: 0.495395. Entropy: 0.910691.\n",
      "Iteration 6767: Policy loss: -0.001235. Value loss: 0.178235. Entropy: 0.907089.\n",
      "Iteration 6768: Policy loss: -0.013666. Value loss: 0.107150. Entropy: 0.917026.\n",
      "episode: 3035   score: 29.0  epsilon: 1.0    steps: 8  evaluation reward: 23.88\n",
      "episode: 3036   score: 20.0  epsilon: 1.0    steps: 992  evaluation reward: 23.98\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6769: Policy loss: 0.012713. Value loss: 0.482806. Entropy: 0.789730.\n",
      "Iteration 6770: Policy loss: -0.003238. Value loss: 0.153784. Entropy: 0.809580.\n",
      "Iteration 6771: Policy loss: -0.015569. Value loss: 0.073566. Entropy: 0.792644.\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6772: Policy loss: 0.009541. Value loss: 0.361605. Entropy: 0.866821.\n",
      "Iteration 6773: Policy loss: -0.005792. Value loss: 0.150877. Entropy: 0.855709.\n",
      "Iteration 6774: Policy loss: -0.014790. Value loss: 0.090691. Entropy: 0.872560.\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6775: Policy loss: 0.015110. Value loss: 0.820732. Entropy: 0.865868.\n",
      "Iteration 6776: Policy loss: 0.004046. Value loss: 0.316581. Entropy: 0.851233.\n",
      "Iteration 6777: Policy loss: -0.006927. Value loss: 0.210323. Entropy: 0.850796.\n",
      "episode: 3037   score: 27.0  epsilon: 1.0    steps: 48  evaluation reward: 24.08\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6778: Policy loss: 0.006600. Value loss: 0.623998. Entropy: 0.852771.\n",
      "Iteration 6779: Policy loss: 0.000099. Value loss: 0.219024. Entropy: 0.849182.\n",
      "Iteration 6780: Policy loss: -0.015850. Value loss: 0.116447. Entropy: 0.868353.\n",
      "episode: 3038   score: 10.0  epsilon: 1.0    steps: 904  evaluation reward: 23.88\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6781: Policy loss: 0.020810. Value loss: 0.844620. Entropy: 0.964789.\n",
      "Iteration 6782: Policy loss: 0.003506. Value loss: 0.353001. Entropy: 0.955510.\n",
      "Iteration 6783: Policy loss: -0.011626. Value loss: 0.189520. Entropy: 0.941926.\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6784: Policy loss: 0.014832. Value loss: 0.873567. Entropy: 0.952053.\n",
      "Iteration 6785: Policy loss: -0.001338. Value loss: 0.343578. Entropy: 0.961043.\n",
      "Iteration 6786: Policy loss: -0.010861. Value loss: 0.200870. Entropy: 0.955902.\n",
      "episode: 3039   score: 30.0  epsilon: 1.0    steps: 392  evaluation reward: 23.98\n",
      "episode: 3040   score: 22.0  epsilon: 1.0    steps: 632  evaluation reward: 24.1\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6787: Policy loss: 0.008177. Value loss: 0.419339. Entropy: 0.880213.\n",
      "Iteration 6788: Policy loss: -0.013601. Value loss: 0.152184. Entropy: 0.865140.\n",
      "Iteration 6789: Policy loss: -0.020632. Value loss: 0.071321. Entropy: 0.862823.\n",
      "episode: 3041   score: 31.0  epsilon: 1.0    steps: 24  evaluation reward: 24.19\n",
      "episode: 3042   score: 23.0  epsilon: 1.0    steps: 64  evaluation reward: 24.23\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6790: Policy loss: 0.008603. Value loss: 0.371227. Entropy: 0.858016.\n",
      "Iteration 6791: Policy loss: -0.009185. Value loss: 0.154175. Entropy: 0.842965.\n",
      "Iteration 6792: Policy loss: -0.020073. Value loss: 0.084458. Entropy: 0.859643.\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6793: Policy loss: 0.005271. Value loss: 0.728015. Entropy: 0.870229.\n",
      "Iteration 6794: Policy loss: -0.004086. Value loss: 0.391127. Entropy: 0.846634.\n",
      "Iteration 6795: Policy loss: -0.010244. Value loss: 0.189851. Entropy: 0.840996.\n",
      "episode: 3043   score: 25.0  epsilon: 1.0    steps: 128  evaluation reward: 24.29\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6796: Policy loss: 0.015315. Value loss: 0.450452. Entropy: 0.818576.\n",
      "Iteration 6797: Policy loss: -0.008939. Value loss: 0.186598. Entropy: 0.816749.\n",
      "Iteration 6798: Policy loss: -0.021426. Value loss: 0.108777. Entropy: 0.836248.\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6799: Policy loss: 0.008538. Value loss: 0.328672. Entropy: 0.895743.\n",
      "Iteration 6800: Policy loss: -0.007173. Value loss: 0.166743. Entropy: 0.887584.\n",
      "Iteration 6801: Policy loss: -0.019074. Value loss: 0.082050. Entropy: 0.899944.\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6802: Policy loss: 0.005147. Value loss: 0.688804. Entropy: 0.925323.\n",
      "Iteration 6803: Policy loss: -0.006317. Value loss: 0.300193. Entropy: 0.926541.\n",
      "Iteration 6804: Policy loss: -0.017112. Value loss: 0.139292. Entropy: 0.921862.\n",
      "episode: 3044   score: 50.0  epsilon: 1.0    steps: 680  evaluation reward: 24.57\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6805: Policy loss: 0.011251. Value loss: 0.976582. Entropy: 0.949398.\n",
      "Iteration 6806: Policy loss: -0.002257. Value loss: 0.364226. Entropy: 0.926342.\n",
      "Iteration 6807: Policy loss: -0.008848. Value loss: 0.180565. Entropy: 0.937330.\n",
      "episode: 3045   score: 24.0  epsilon: 1.0    steps: 80  evaluation reward: 24.66\n",
      "episode: 3046   score: 17.0  epsilon: 1.0    steps: 872  evaluation reward: 24.44\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6808: Policy loss: 0.002139. Value loss: 0.557467. Entropy: 0.906922.\n",
      "Iteration 6809: Policy loss: -0.013848. Value loss: 0.201460. Entropy: 0.893753.\n",
      "Iteration 6810: Policy loss: -0.020746. Value loss: 0.108609. Entropy: 0.896584.\n",
      "episode: 3047   score: 17.0  epsilon: 1.0    steps: 560  evaluation reward: 24.5\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6811: Policy loss: 0.012469. Value loss: 0.995788. Entropy: 0.887241.\n",
      "Iteration 6812: Policy loss: 0.003229. Value loss: 0.393684. Entropy: 0.883698.\n",
      "Iteration 6813: Policy loss: -0.010174. Value loss: 0.213628. Entropy: 0.875253.\n",
      "episode: 3048   score: 22.0  epsilon: 1.0    steps: 344  evaluation reward: 24.31\n",
      "episode: 3049   score: 30.0  epsilon: 1.0    steps: 472  evaluation reward: 24.5\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6814: Policy loss: 0.010064. Value loss: 0.582519. Entropy: 0.888352.\n",
      "Iteration 6815: Policy loss: -0.004603. Value loss: 0.231714. Entropy: 0.877135.\n",
      "Iteration 6816: Policy loss: -0.013107. Value loss: 0.136499. Entropy: 0.879075.\n",
      "episode: 3050   score: 36.0  epsilon: 1.0    steps: 264  evaluation reward: 24.7\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6817: Policy loss: 0.007203. Value loss: 0.564551. Entropy: 0.846774.\n",
      "Iteration 6818: Policy loss: -0.006632. Value loss: 0.260691. Entropy: 0.839358.\n",
      "Iteration 6819: Policy loss: -0.016176. Value loss: 0.154843. Entropy: 0.836924.\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6820: Policy loss: 0.007994. Value loss: 0.521353. Entropy: 0.826426.\n",
      "Iteration 6821: Policy loss: -0.002016. Value loss: 0.222828. Entropy: 0.832178.\n",
      "Iteration 6822: Policy loss: -0.015621. Value loss: 0.125799. Entropy: 0.815895.\n",
      "now time :  2019-03-06 14:53:47.863725\n",
      "episode: 3051   score: 28.0  epsilon: 1.0    steps: 840  evaluation reward: 24.78\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6823: Policy loss: 0.009557. Value loss: 0.663241. Entropy: 0.923457.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6824: Policy loss: -0.006845. Value loss: 0.319595. Entropy: 0.915033.\n",
      "Iteration 6825: Policy loss: -0.016140. Value loss: 0.203784. Entropy: 0.907417.\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6826: Policy loss: 0.016255. Value loss: 0.587945. Entropy: 0.828110.\n",
      "Iteration 6827: Policy loss: 0.008004. Value loss: 0.207711. Entropy: 0.834549.\n",
      "Iteration 6828: Policy loss: -0.011588. Value loss: 0.103250. Entropy: 0.842347.\n",
      "episode: 3052   score: 21.0  epsilon: 1.0    steps: 496  evaluation reward: 24.85\n",
      "episode: 3053   score: 22.0  epsilon: 1.0    steps: 608  evaluation reward: 24.65\n",
      "episode: 3054   score: 25.0  epsilon: 1.0    steps: 912  evaluation reward: 24.72\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6829: Policy loss: 0.014078. Value loss: 0.665770. Entropy: 0.926082.\n",
      "Iteration 6830: Policy loss: -0.010885. Value loss: 0.271720. Entropy: 0.918732.\n",
      "Iteration 6831: Policy loss: -0.015420. Value loss: 0.153600. Entropy: 0.918313.\n",
      "episode: 3055   score: 12.0  epsilon: 1.0    steps: 760  evaluation reward: 24.59\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6832: Policy loss: 0.016660. Value loss: 0.771550. Entropy: 0.874888.\n",
      "Iteration 6833: Policy loss: -0.003261. Value loss: 0.308533. Entropy: 0.877980.\n",
      "Iteration 6834: Policy loss: -0.016067. Value loss: 0.188089. Entropy: 0.851022.\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6835: Policy loss: 0.015586. Value loss: 0.660953. Entropy: 0.869114.\n",
      "Iteration 6836: Policy loss: -0.003552. Value loss: 0.303694. Entropy: 0.874130.\n",
      "Iteration 6837: Policy loss: -0.012982. Value loss: 0.144320. Entropy: 0.861647.\n",
      "episode: 3056   score: 23.0  epsilon: 1.0    steps: 48  evaluation reward: 24.72\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6838: Policy loss: 0.005972. Value loss: 0.625382. Entropy: 0.855064.\n",
      "Iteration 6839: Policy loss: 0.003920. Value loss: 0.212263. Entropy: 0.867697.\n",
      "Iteration 6840: Policy loss: -0.016389. Value loss: 0.105770. Entropy: 0.851210.\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6841: Policy loss: 0.006618. Value loss: 0.556032. Entropy: 0.858998.\n",
      "Iteration 6842: Policy loss: -0.002913. Value loss: 0.246203. Entropy: 0.866148.\n",
      "Iteration 6843: Policy loss: -0.012373. Value loss: 0.119809. Entropy: 0.850529.\n",
      "episode: 3057   score: 34.0  epsilon: 1.0    steps: 720  evaluation reward: 24.56\n",
      "episode: 3058   score: 31.0  epsilon: 1.0    steps: 760  evaluation reward: 24.6\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6844: Policy loss: 0.008812. Value loss: 0.711521. Entropy: 0.907575.\n",
      "Iteration 6845: Policy loss: -0.003529. Value loss: 0.320289. Entropy: 0.916278.\n",
      "Iteration 6846: Policy loss: -0.016599. Value loss: 0.182832. Entropy: 0.908018.\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6847: Policy loss: 0.014565. Value loss: 0.375224. Entropy: 0.941244.\n",
      "Iteration 6848: Policy loss: -0.004675. Value loss: 0.180125. Entropy: 0.942903.\n",
      "Iteration 6849: Policy loss: -0.014908. Value loss: 0.098825. Entropy: 0.933413.\n",
      "episode: 3059   score: 20.0  epsilon: 1.0    steps: 632  evaluation reward: 24.38\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6850: Policy loss: 0.030208. Value loss: 0.929756. Entropy: 0.900808.\n",
      "Iteration 6851: Policy loss: -0.002816. Value loss: 0.495522. Entropy: 0.866615.\n",
      "Iteration 6852: Policy loss: -0.008676. Value loss: 0.286173. Entropy: 0.869221.\n",
      "episode: 3060   score: 23.0  epsilon: 1.0    steps: 680  evaluation reward: 24.19\n",
      "episode: 3061   score: 21.0  epsilon: 1.0    steps: 808  evaluation reward: 24.14\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6853: Policy loss: 0.013077. Value loss: 0.739299. Entropy: 0.848192.\n",
      "Iteration 6854: Policy loss: 0.004508. Value loss: 0.254561. Entropy: 0.857338.\n",
      "Iteration 6855: Policy loss: -0.008148. Value loss: 0.149609. Entropy: 0.845729.\n",
      "episode: 3062   score: 27.0  epsilon: 1.0    steps: 432  evaluation reward: 24.16\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6856: Policy loss: 0.018488. Value loss: 0.808417. Entropy: 0.826799.\n",
      "Iteration 6857: Policy loss: -0.002979. Value loss: 0.349470. Entropy: 0.829508.\n",
      "Iteration 6858: Policy loss: -0.016070. Value loss: 0.199079. Entropy: 0.840083.\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6859: Policy loss: 0.010647. Value loss: 0.507236. Entropy: 0.870908.\n",
      "Iteration 6860: Policy loss: -0.009147. Value loss: 0.199728. Entropy: 0.868167.\n",
      "Iteration 6861: Policy loss: -0.020202. Value loss: 0.110607. Entropy: 0.871329.\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6862: Policy loss: 0.011424. Value loss: 0.765923. Entropy: 0.937878.\n",
      "Iteration 6863: Policy loss: 0.001188. Value loss: 0.335823. Entropy: 0.927380.\n",
      "Iteration 6864: Policy loss: -0.012020. Value loss: 0.214528. Entropy: 0.923301.\n",
      "episode: 3063   score: 26.0  epsilon: 1.0    steps: 176  evaluation reward: 24.11\n",
      "episode: 3064   score: 37.0  epsilon: 1.0    steps: 432  evaluation reward: 24.38\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6865: Policy loss: 0.010003. Value loss: 0.536531. Entropy: 0.921448.\n",
      "Iteration 6866: Policy loss: -0.004299. Value loss: 0.227252. Entropy: 0.918306.\n",
      "Iteration 6867: Policy loss: -0.011264. Value loss: 0.140693. Entropy: 0.910218.\n",
      "episode: 3065   score: 38.0  epsilon: 1.0    steps: 888  evaluation reward: 24.23\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6868: Policy loss: 0.013367. Value loss: 0.844576. Entropy: 0.905550.\n",
      "Iteration 6869: Policy loss: -0.005410. Value loss: 0.360154. Entropy: 0.922251.\n",
      "Iteration 6870: Policy loss: -0.014739. Value loss: 0.212383. Entropy: 0.925608.\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6871: Policy loss: 0.005626. Value loss: 0.584977. Entropy: 0.878783.\n",
      "Iteration 6872: Policy loss: -0.003536. Value loss: 0.256372. Entropy: 0.868977.\n",
      "Iteration 6873: Policy loss: -0.011382. Value loss: 0.155458. Entropy: 0.877734.\n",
      "episode: 3066   score: 18.0  epsilon: 1.0    steps: 168  evaluation reward: 24.27\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6874: Policy loss: 0.007488. Value loss: 0.654748. Entropy: 0.882060.\n",
      "Iteration 6875: Policy loss: -0.007953. Value loss: 0.300397. Entropy: 0.875964.\n",
      "Iteration 6876: Policy loss: -0.016314. Value loss: 0.151168. Entropy: 0.875446.\n",
      "episode: 3067   score: 21.0  epsilon: 1.0    steps: 288  evaluation reward: 24.26\n",
      "episode: 3068   score: 22.0  epsilon: 1.0    steps: 776  evaluation reward: 24.15\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6877: Policy loss: 0.007017. Value loss: 0.568239. Entropy: 0.940801.\n",
      "Iteration 6878: Policy loss: -0.010596. Value loss: 0.263860. Entropy: 0.920807.\n",
      "Iteration 6879: Policy loss: -0.021790. Value loss: 0.177397. Entropy: 0.921677.\n",
      "episode: 3069   score: 37.0  epsilon: 1.0    steps: 696  evaluation reward: 24.3\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6880: Policy loss: 0.006030. Value loss: 0.495655. Entropy: 0.876564.\n",
      "Iteration 6881: Policy loss: 0.000423. Value loss: 0.214760. Entropy: 0.887866.\n",
      "Iteration 6882: Policy loss: -0.010862. Value loss: 0.102450. Entropy: 0.872695.\n",
      "episode: 3070   score: 26.0  epsilon: 1.0    steps: 96  evaluation reward: 24.31\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6883: Policy loss: 0.009430. Value loss: 0.395307. Entropy: 0.899665.\n",
      "Iteration 6884: Policy loss: -0.011495. Value loss: 0.134467. Entropy: 0.895790.\n",
      "Iteration 6885: Policy loss: -0.018786. Value loss: 0.077862. Entropy: 0.897154.\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6886: Policy loss: 0.011833. Value loss: 0.500172. Entropy: 0.921890.\n",
      "Iteration 6887: Policy loss: -0.011617. Value loss: 0.236364. Entropy: 0.912568.\n",
      "Iteration 6888: Policy loss: -0.016851. Value loss: 0.108832. Entropy: 0.909988.\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6889: Policy loss: 0.006609. Value loss: 0.302559. Entropy: 1.011732.\n",
      "Iteration 6890: Policy loss: -0.012773. Value loss: 0.112156. Entropy: 1.005009.\n",
      "Iteration 6891: Policy loss: -0.021966. Value loss: 0.057266. Entropy: 1.003561.\n",
      "episode: 3071   score: 23.0  epsilon: 1.0    steps: 48  evaluation reward: 24.25\n",
      "episode: 3072   score: 27.0  epsilon: 1.0    steps: 336  evaluation reward: 24.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6892: Policy loss: 0.003724. Value loss: 0.558454. Entropy: 0.920172.\n",
      "Iteration 6893: Policy loss: -0.010690. Value loss: 0.244960. Entropy: 0.917757.\n",
      "Iteration 6894: Policy loss: -0.020174. Value loss: 0.171922. Entropy: 0.911170.\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6895: Policy loss: 0.011985. Value loss: 0.439979. Entropy: 0.864415.\n",
      "Iteration 6896: Policy loss: -0.003596. Value loss: 0.142481. Entropy: 0.864281.\n",
      "Iteration 6897: Policy loss: -0.015691. Value loss: 0.068553. Entropy: 0.867901.\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6898: Policy loss: 0.005849. Value loss: 0.622428. Entropy: 0.953556.\n",
      "Iteration 6899: Policy loss: -0.007877. Value loss: 0.304724. Entropy: 0.948101.\n",
      "Iteration 6900: Policy loss: -0.017530. Value loss: 0.200508. Entropy: 0.952566.\n",
      "episode: 3073   score: 29.0  epsilon: 1.0    steps: 480  evaluation reward: 24.33\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6901: Policy loss: 0.013272. Value loss: 0.615690. Entropy: 0.915670.\n",
      "Iteration 6902: Policy loss: 0.000740. Value loss: 0.253320. Entropy: 0.882969.\n",
      "Iteration 6903: Policy loss: -0.010324. Value loss: 0.126530. Entropy: 0.890504.\n",
      "episode: 3074   score: 18.0  epsilon: 1.0    steps: 392  evaluation reward: 23.99\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6904: Policy loss: 0.010495. Value loss: 0.837069. Entropy: 0.873350.\n",
      "Iteration 6905: Policy loss: -0.007651. Value loss: 0.360566. Entropy: 0.842072.\n",
      "Iteration 6906: Policy loss: -0.017150. Value loss: 0.180175. Entropy: 0.852636.\n",
      "episode: 3075   score: 35.0  epsilon: 1.0    steps: 144  evaluation reward: 24.26\n",
      "episode: 3076   score: 21.0  epsilon: 1.0    steps: 232  evaluation reward: 24.31\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6907: Policy loss: 0.009846. Value loss: 1.057250. Entropy: 0.914213.\n",
      "Iteration 6908: Policy loss: 0.002234. Value loss: 0.430477. Entropy: 0.884226.\n",
      "Iteration 6909: Policy loss: -0.014631. Value loss: 0.285596. Entropy: 0.913078.\n",
      "episode: 3077   score: 32.0  epsilon: 1.0    steps: 176  evaluation reward: 24.52\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6910: Policy loss: 0.008108. Value loss: 0.428060. Entropy: 0.870000.\n",
      "Iteration 6911: Policy loss: -0.015091. Value loss: 0.146815. Entropy: 0.882750.\n",
      "Iteration 6912: Policy loss: -0.021947. Value loss: 0.080691. Entropy: 0.883596.\n",
      "episode: 3078   score: 17.0  epsilon: 1.0    steps: 336  evaluation reward: 24.59\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6913: Policy loss: 0.017099. Value loss: 0.500649. Entropy: 0.901455.\n",
      "Iteration 6914: Policy loss: -0.004136. Value loss: 0.201802. Entropy: 0.908930.\n",
      "Iteration 6915: Policy loss: -0.018547. Value loss: 0.123011. Entropy: 0.913730.\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6916: Policy loss: 0.008410. Value loss: 0.905001. Entropy: 0.880803.\n",
      "Iteration 6917: Policy loss: -0.001074. Value loss: 0.264178. Entropy: 0.911272.\n",
      "Iteration 6918: Policy loss: -0.005749. Value loss: 0.161350. Entropy: 0.927193.\n",
      "episode: 3079   score: 47.0  epsilon: 1.0    steps: 296  evaluation reward: 24.82\n",
      "episode: 3080   score: 35.0  epsilon: 1.0    steps: 672  evaluation reward: 24.96\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6919: Policy loss: 0.004554. Value loss: 0.642778. Entropy: 0.893717.\n",
      "Iteration 6920: Policy loss: -0.002218. Value loss: 0.263599. Entropy: 0.885071.\n",
      "Iteration 6921: Policy loss: -0.013752. Value loss: 0.131830. Entropy: 0.894972.\n",
      "episode: 3081   score: 29.0  epsilon: 1.0    steps: 80  evaluation reward: 25.0\n",
      "episode: 3082   score: 11.0  epsilon: 1.0    steps: 720  evaluation reward: 24.93\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6922: Policy loss: 0.008991. Value loss: 0.468743. Entropy: 0.929190.\n",
      "Iteration 6923: Policy loss: -0.007635. Value loss: 0.152617. Entropy: 0.919250.\n",
      "Iteration 6924: Policy loss: -0.014151. Value loss: 0.088215. Entropy: 0.931729.\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6925: Policy loss: 0.024742. Value loss: 0.731001. Entropy: 0.890034.\n",
      "Iteration 6926: Policy loss: -0.005199. Value loss: 0.286117. Entropy: 0.883407.\n",
      "Iteration 6927: Policy loss: -0.012250. Value loss: 0.138460. Entropy: 0.881534.\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6928: Policy loss: 0.007801. Value loss: 0.471730. Entropy: 0.942558.\n",
      "Iteration 6929: Policy loss: -0.010707. Value loss: 0.159174. Entropy: 0.942078.\n",
      "Iteration 6930: Policy loss: -0.020897. Value loss: 0.079502. Entropy: 0.931611.\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6931: Policy loss: 0.014617. Value loss: 0.702677. Entropy: 0.921341.\n",
      "Iteration 6932: Policy loss: -0.005130. Value loss: 0.229267. Entropy: 0.924384.\n",
      "Iteration 6933: Policy loss: -0.009386. Value loss: 0.139018. Entropy: 0.934993.\n",
      "episode: 3083   score: 33.0  epsilon: 1.0    steps: 248  evaluation reward: 25.03\n",
      "episode: 3084   score: 28.0  epsilon: 1.0    steps: 480  evaluation reward: 25.01\n",
      "episode: 3085   score: 31.0  epsilon: 1.0    steps: 640  evaluation reward: 25.16\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6934: Policy loss: 0.015151. Value loss: 1.154228. Entropy: 0.881210.\n",
      "Iteration 6935: Policy loss: -0.002891. Value loss: 0.688669. Entropy: 0.874804.\n",
      "Iteration 6936: Policy loss: -0.017195. Value loss: 0.448198. Entropy: 0.844824.\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6937: Policy loss: 0.008613. Value loss: 0.389624. Entropy: 0.865216.\n",
      "Iteration 6938: Policy loss: -0.005671. Value loss: 0.148063. Entropy: 0.860242.\n",
      "Iteration 6939: Policy loss: -0.018628. Value loss: 0.088066. Entropy: 0.844745.\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6940: Policy loss: 0.004806. Value loss: 0.524385. Entropy: 0.874177.\n",
      "Iteration 6941: Policy loss: -0.009487. Value loss: 0.230843. Entropy: 0.888415.\n",
      "Iteration 6942: Policy loss: -0.019378. Value loss: 0.144689. Entropy: 0.880601.\n",
      "episode: 3086   score: 27.0  epsilon: 1.0    steps: 528  evaluation reward: 25.13\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6943: Policy loss: 0.012607. Value loss: 0.597171. Entropy: 0.845446.\n",
      "Iteration 6944: Policy loss: 0.004726. Value loss: 0.232719. Entropy: 0.832300.\n",
      "Iteration 6945: Policy loss: -0.011537. Value loss: 0.141436. Entropy: 0.847444.\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6946: Policy loss: 0.015510. Value loss: 0.740748. Entropy: 0.898264.\n",
      "Iteration 6947: Policy loss: -0.006021. Value loss: 0.363788. Entropy: 0.890172.\n",
      "Iteration 6948: Policy loss: -0.013603. Value loss: 0.229745. Entropy: 0.879066.\n",
      "episode: 3087   score: 25.0  epsilon: 1.0    steps: 992  evaluation reward: 25.18\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6949: Policy loss: 0.002315. Value loss: 0.697226. Entropy: 0.901377.\n",
      "Iteration 6950: Policy loss: -0.003275. Value loss: 0.321644. Entropy: 0.877130.\n",
      "Iteration 6951: Policy loss: -0.015060. Value loss: 0.178679. Entropy: 0.882054.\n",
      "episode: 3088   score: 33.0  epsilon: 1.0    steps: 104  evaluation reward: 25.27\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6952: Policy loss: 0.006039. Value loss: 0.577940. Entropy: 0.822352.\n",
      "Iteration 6953: Policy loss: -0.006808. Value loss: 0.273928. Entropy: 0.813507.\n",
      "Iteration 6954: Policy loss: -0.015222. Value loss: 0.171684. Entropy: 0.823268.\n",
      "episode: 3089   score: 54.0  epsilon: 1.0    steps: 312  evaluation reward: 25.65\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6955: Policy loss: 0.017023. Value loss: 1.124138. Entropy: 0.857763.\n",
      "Iteration 6956: Policy loss: 0.009509. Value loss: 0.492707. Entropy: 0.866751.\n",
      "Iteration 6957: Policy loss: -0.008665. Value loss: 0.258358. Entropy: 0.874443.\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6958: Policy loss: 0.010057. Value loss: 1.069560. Entropy: 0.894007.\n",
      "Iteration 6959: Policy loss: -0.005560. Value loss: 0.423391. Entropy: 0.916575.\n",
      "Iteration 6960: Policy loss: -0.013438. Value loss: 0.213725. Entropy: 0.906960.\n",
      "episode: 3090   score: 55.0  epsilon: 1.0    steps: 168  evaluation reward: 26.0\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6961: Policy loss: 0.008371. Value loss: 0.437783. Entropy: 0.865337.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6962: Policy loss: -0.009085. Value loss: 0.166773. Entropy: 0.868457.\n",
      "Iteration 6963: Policy loss: -0.019520. Value loss: 0.091293. Entropy: 0.857394.\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6964: Policy loss: 0.014382. Value loss: 1.086417. Entropy: 0.842945.\n",
      "Iteration 6965: Policy loss: -0.009270. Value loss: 0.520578. Entropy: 0.864477.\n",
      "Iteration 6966: Policy loss: -0.020752. Value loss: 0.294742. Entropy: 0.872622.\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6967: Policy loss: 0.012663. Value loss: 1.039458. Entropy: 0.950874.\n",
      "Iteration 6968: Policy loss: -0.001536. Value loss: 0.589190. Entropy: 0.955787.\n",
      "Iteration 6969: Policy loss: -0.011987. Value loss: 0.376131. Entropy: 0.951052.\n",
      "episode: 3091   score: 37.0  epsilon: 1.0    steps: 152  evaluation reward: 26.09\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6970: Policy loss: 0.013064. Value loss: 0.907229. Entropy: 0.995605.\n",
      "Iteration 6971: Policy loss: 0.002191. Value loss: 0.486276. Entropy: 0.987121.\n",
      "Iteration 6972: Policy loss: -0.008677. Value loss: 0.322701. Entropy: 0.977441.\n",
      "episode: 3092   score: 36.0  epsilon: 1.0    steps: 16  evaluation reward: 26.22\n",
      "episode: 3093   score: 35.0  epsilon: 1.0    steps: 968  evaluation reward: 26.28\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6973: Policy loss: 0.006887. Value loss: 0.893061. Entropy: 0.953329.\n",
      "Iteration 6974: Policy loss: -0.001192. Value loss: 0.381631. Entropy: 0.956508.\n",
      "Iteration 6975: Policy loss: -0.010497. Value loss: 0.199462. Entropy: 0.948509.\n",
      "episode: 3094   score: 54.0  epsilon: 1.0    steps: 192  evaluation reward: 26.62\n",
      "episode: 3095   score: 24.0  epsilon: 1.0    steps: 1008  evaluation reward: 26.56\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6976: Policy loss: 0.009208. Value loss: 1.152973. Entropy: 0.885492.\n",
      "Iteration 6977: Policy loss: -0.002341. Value loss: 0.592880. Entropy: 0.872684.\n",
      "Iteration 6978: Policy loss: -0.013658. Value loss: 0.297044. Entropy: 0.868280.\n",
      "episode: 3096   score: 36.0  epsilon: 1.0    steps: 368  evaluation reward: 26.73\n",
      "episode: 3097   score: 28.0  epsilon: 1.0    steps: 896  evaluation reward: 26.85\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6979: Policy loss: 0.009376. Value loss: 0.618563. Entropy: 0.939529.\n",
      "Iteration 6980: Policy loss: -0.002288. Value loss: 0.254676. Entropy: 0.925594.\n",
      "Iteration 6981: Policy loss: -0.008870. Value loss: 0.142174. Entropy: 0.912986.\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6982: Policy loss: 0.014095. Value loss: 0.412927. Entropy: 0.865262.\n",
      "Iteration 6983: Policy loss: -0.001163. Value loss: 0.206406. Entropy: 0.855837.\n",
      "Iteration 6984: Policy loss: -0.014462. Value loss: 0.116803. Entropy: 0.866156.\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6985: Policy loss: 0.009233. Value loss: 0.466195. Entropy: 0.839751.\n",
      "Iteration 6986: Policy loss: -0.006588. Value loss: 0.195835. Entropy: 0.855550.\n",
      "Iteration 6987: Policy loss: -0.018186. Value loss: 0.122552. Entropy: 0.837515.\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6988: Policy loss: 0.013755. Value loss: 0.536042. Entropy: 0.877594.\n",
      "Iteration 6989: Policy loss: -0.005138. Value loss: 0.225427. Entropy: 0.865761.\n",
      "Iteration 6990: Policy loss: -0.017501. Value loss: 0.119566. Entropy: 0.881158.\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6991: Policy loss: 0.013849. Value loss: 0.570511. Entropy: 0.931104.\n",
      "Iteration 6992: Policy loss: -0.000225. Value loss: 0.283286. Entropy: 0.909023.\n",
      "Iteration 6993: Policy loss: -0.008982. Value loss: 0.181156. Entropy: 0.902409.\n",
      "episode: 3098   score: 38.0  epsilon: 1.0    steps: 864  evaluation reward: 27.03\n",
      "episode: 3099   score: 15.0  epsilon: 1.0    steps: 904  evaluation reward: 26.82\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6994: Policy loss: 0.010966. Value loss: 1.187732. Entropy: 0.888100.\n",
      "Iteration 6995: Policy loss: -0.002872. Value loss: 0.487880. Entropy: 0.908290.\n",
      "Iteration 6996: Policy loss: -0.012385. Value loss: 0.259836. Entropy: 0.903606.\n",
      "episode: 3100   score: 28.0  epsilon: 1.0    steps: 560  evaluation reward: 26.74\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6997: Policy loss: 0.013248. Value loss: 0.581024. Entropy: 0.894012.\n",
      "Iteration 6998: Policy loss: -0.003855. Value loss: 0.222548. Entropy: 0.904682.\n",
      "Iteration 6999: Policy loss: -0.014561. Value loss: 0.113556. Entropy: 0.900993.\n",
      "now time :  2019-03-06 14:57:34.625442\n",
      "episode: 3101   score: 28.0  epsilon: 1.0    steps: 624  evaluation reward: 26.78\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 7000: Policy loss: 0.010566. Value loss: 0.588358. Entropy: 0.925152.\n",
      "Iteration 7001: Policy loss: -0.005503. Value loss: 0.284244. Entropy: 0.919711.\n",
      "Iteration 7002: Policy loss: -0.017396. Value loss: 0.180002. Entropy: 0.916312.\n",
      "episode: 3102   score: 22.0  epsilon: 1.0    steps: 640  evaluation reward: 26.82\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7003: Policy loss: 0.010192. Value loss: 0.458885. Entropy: 0.974355.\n",
      "Iteration 7004: Policy loss: -0.003313. Value loss: 0.174660. Entropy: 0.959518.\n",
      "Iteration 7005: Policy loss: -0.019050. Value loss: 0.102372. Entropy: 0.962119.\n",
      "episode: 3103   score: 28.0  epsilon: 1.0    steps: 768  evaluation reward: 26.71\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7006: Policy loss: 0.013286. Value loss: 0.731441. Entropy: 0.884794.\n",
      "Iteration 7007: Policy loss: -0.003904. Value loss: 0.322764. Entropy: 0.880955.\n",
      "Iteration 7008: Policy loss: -0.011196. Value loss: 0.163281. Entropy: 0.875333.\n",
      "episode: 3104   score: 32.0  epsilon: 1.0    steps: 280  evaluation reward: 26.79\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7009: Policy loss: 0.019146. Value loss: 0.621027. Entropy: 0.880291.\n",
      "Iteration 7010: Policy loss: -0.005357. Value loss: 0.218931. Entropy: 0.889027.\n",
      "Iteration 7011: Policy loss: -0.013752. Value loss: 0.114886. Entropy: 0.890789.\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7012: Policy loss: 0.012660. Value loss: 0.422940. Entropy: 0.954323.\n",
      "Iteration 7013: Policy loss: -0.011460. Value loss: 0.150678. Entropy: 0.961690.\n",
      "Iteration 7014: Policy loss: -0.019605. Value loss: 0.080873. Entropy: 0.965839.\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7015: Policy loss: 0.023633. Value loss: 0.609142. Entropy: 0.939764.\n",
      "Iteration 7016: Policy loss: 0.007839. Value loss: 0.250479. Entropy: 0.938321.\n",
      "Iteration 7017: Policy loss: -0.007924. Value loss: 0.149978. Entropy: 0.953884.\n",
      "episode: 3105   score: 48.0  epsilon: 1.0    steps: 344  evaluation reward: 27.04\n",
      "episode: 3106   score: 23.0  epsilon: 1.0    steps: 472  evaluation reward: 26.99\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7018: Policy loss: 0.016257. Value loss: 0.513924. Entropy: 0.925010.\n",
      "Iteration 7019: Policy loss: -0.000324. Value loss: 0.192890. Entropy: 0.930263.\n",
      "Iteration 7020: Policy loss: -0.013705. Value loss: 0.108261. Entropy: 0.929737.\n",
      "episode: 3107   score: 29.0  epsilon: 1.0    steps: 912  evaluation reward: 27.12\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7021: Policy loss: 0.007814. Value loss: 0.414933. Entropy: 0.850040.\n",
      "Iteration 7022: Policy loss: -0.008121. Value loss: 0.182719. Entropy: 0.853839.\n",
      "Iteration 7023: Policy loss: -0.015483. Value loss: 0.095336. Entropy: 0.853022.\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7024: Policy loss: 0.012868. Value loss: 0.594798. Entropy: 0.893873.\n",
      "Iteration 7025: Policy loss: -0.006156. Value loss: 0.264162. Entropy: 0.880932.\n",
      "Iteration 7026: Policy loss: -0.014873. Value loss: 0.162291. Entropy: 0.863704.\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7027: Policy loss: 0.012594. Value loss: 0.844652. Entropy: 0.916473.\n",
      "Iteration 7028: Policy loss: 0.002186. Value loss: 0.354455. Entropy: 0.896820.\n",
      "Iteration 7029: Policy loss: -0.013928. Value loss: 0.193175. Entropy: 0.918175.\n",
      "episode: 3108   score: 33.0  epsilon: 1.0    steps: 792  evaluation reward: 27.17\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7030: Policy loss: 0.012155. Value loss: 0.980829. Entropy: 0.901506.\n",
      "Iteration 7031: Policy loss: -0.000794. Value loss: 0.334719. Entropy: 0.894885.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7032: Policy loss: -0.012661. Value loss: 0.171076. Entropy: 0.896026.\n",
      "episode: 3109   score: 33.0  epsilon: 1.0    steps: 184  evaluation reward: 27.13\n",
      "episode: 3110   score: 44.0  epsilon: 1.0    steps: 632  evaluation reward: 27.43\n",
      "episode: 3111   score: 42.0  epsilon: 1.0    steps: 888  evaluation reward: 27.63\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7033: Policy loss: 0.009887. Value loss: 0.764182. Entropy: 0.897060.\n",
      "Iteration 7034: Policy loss: -0.001039. Value loss: 0.239731. Entropy: 0.907877.\n",
      "Iteration 7035: Policy loss: -0.010566. Value loss: 0.120823. Entropy: 0.920003.\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7036: Policy loss: 0.009816. Value loss: 0.656654. Entropy: 0.895453.\n",
      "Iteration 7037: Policy loss: -0.001278. Value loss: 0.413015. Entropy: 0.908703.\n",
      "Iteration 7038: Policy loss: -0.014471. Value loss: 0.264502. Entropy: 0.884075.\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7039: Policy loss: 0.016982. Value loss: 0.794449. Entropy: 0.951681.\n",
      "Iteration 7040: Policy loss: 0.006350. Value loss: 0.455508. Entropy: 0.929064.\n",
      "Iteration 7041: Policy loss: -0.012259. Value loss: 0.338009. Entropy: 0.931740.\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7042: Policy loss: 0.011936. Value loss: 0.472766. Entropy: 0.936317.\n",
      "Iteration 7043: Policy loss: -0.009051. Value loss: 0.183435. Entropy: 0.933911.\n",
      "Iteration 7044: Policy loss: -0.013329. Value loss: 0.098567. Entropy: 0.934328.\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7045: Policy loss: 0.014860. Value loss: 1.044284. Entropy: 0.963601.\n",
      "Iteration 7046: Policy loss: -0.001447. Value loss: 0.532696. Entropy: 0.969894.\n",
      "Iteration 7047: Policy loss: -0.011236. Value loss: 0.305887. Entropy: 0.968761.\n",
      "episode: 3112   score: 30.0  epsilon: 1.0    steps: 904  evaluation reward: 27.73\n",
      "episode: 3113   score: 31.0  epsilon: 1.0    steps: 992  evaluation reward: 27.75\n",
      "episode: 3114   score: 39.0  epsilon: 1.0    steps: 1016  evaluation reward: 27.94\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7048: Policy loss: 0.007496. Value loss: 1.096372. Entropy: 0.984257.\n",
      "Iteration 7049: Policy loss: -0.001717. Value loss: 0.544848. Entropy: 0.983579.\n",
      "Iteration 7050: Policy loss: -0.010239. Value loss: 0.389134. Entropy: 0.968564.\n",
      "episode: 3115   score: 44.0  epsilon: 1.0    steps: 472  evaluation reward: 28.19\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7051: Policy loss: 0.007498. Value loss: 0.740847. Entropy: 0.947482.\n",
      "Iteration 7052: Policy loss: 0.000914. Value loss: 0.361688. Entropy: 0.940655.\n",
      "Iteration 7053: Policy loss: -0.010415. Value loss: 0.249633. Entropy: 0.946095.\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7054: Policy loss: 0.004861. Value loss: 0.388251. Entropy: 0.904648.\n",
      "Iteration 7055: Policy loss: -0.010335. Value loss: 0.184987. Entropy: 0.907787.\n",
      "Iteration 7056: Policy loss: -0.021190. Value loss: 0.118162. Entropy: 0.901583.\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7057: Policy loss: 0.013934. Value loss: 0.826838. Entropy: 0.945404.\n",
      "Iteration 7058: Policy loss: 0.003351. Value loss: 0.419957. Entropy: 0.942722.\n",
      "Iteration 7059: Policy loss: -0.014342. Value loss: 0.246525. Entropy: 0.931856.\n",
      "episode: 3116   score: 35.0  epsilon: 1.0    steps: 200  evaluation reward: 28.33\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7060: Policy loss: 0.012121. Value loss: 0.686323. Entropy: 0.934040.\n",
      "Iteration 7061: Policy loss: -0.003429. Value loss: 0.323704. Entropy: 0.928622.\n",
      "Iteration 7062: Policy loss: -0.017412. Value loss: 0.195909. Entropy: 0.927655.\n",
      "episode: 3117   score: 35.0  epsilon: 1.0    steps: 424  evaluation reward: 28.37\n",
      "episode: 3118   score: 27.0  epsilon: 1.0    steps: 744  evaluation reward: 28.48\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7063: Policy loss: 0.013300. Value loss: 1.090695. Entropy: 0.858210.\n",
      "Iteration 7064: Policy loss: 0.005200. Value loss: 0.500041. Entropy: 0.873761.\n",
      "Iteration 7065: Policy loss: -0.011171. Value loss: 0.269543. Entropy: 0.857360.\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7066: Policy loss: 0.017251. Value loss: 0.606252. Entropy: 0.940544.\n",
      "Iteration 7067: Policy loss: -0.000933. Value loss: 0.253166. Entropy: 0.931667.\n",
      "Iteration 7068: Policy loss: -0.009572. Value loss: 0.164495. Entropy: 0.932591.\n",
      "episode: 3119   score: 46.0  epsilon: 1.0    steps: 904  evaluation reward: 28.74\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7069: Policy loss: 0.013770. Value loss: 0.644153. Entropy: 0.870372.\n",
      "Iteration 7070: Policy loss: 0.001495. Value loss: 0.276385. Entropy: 0.887851.\n",
      "Iteration 7071: Policy loss: -0.014768. Value loss: 0.163758. Entropy: 0.871847.\n",
      "episode: 3120   score: 24.0  epsilon: 1.0    steps: 88  evaluation reward: 28.67\n",
      "episode: 3121   score: 20.0  epsilon: 1.0    steps: 888  evaluation reward: 28.71\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7072: Policy loss: 0.010656. Value loss: 0.554317. Entropy: 0.986770.\n",
      "Iteration 7073: Policy loss: -0.003588. Value loss: 0.224547. Entropy: 0.971872.\n",
      "Iteration 7074: Policy loss: -0.016579. Value loss: 0.140646. Entropy: 0.972268.\n",
      "episode: 3122   score: 22.0  epsilon: 1.0    steps: 328  evaluation reward: 28.71\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7075: Policy loss: 0.016509. Value loss: 0.726543. Entropy: 0.899464.\n",
      "Iteration 7076: Policy loss: -0.000532. Value loss: 0.304297. Entropy: 0.875112.\n",
      "Iteration 7077: Policy loss: -0.015007. Value loss: 0.211456. Entropy: 0.878144.\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7078: Policy loss: 0.015245. Value loss: 0.601837. Entropy: 0.928731.\n",
      "Iteration 7079: Policy loss: 0.004294. Value loss: 0.284362. Entropy: 0.944929.\n",
      "Iteration 7080: Policy loss: -0.013130. Value loss: 0.200266. Entropy: 0.933628.\n",
      "episode: 3123   score: 30.0  epsilon: 1.0    steps: 344  evaluation reward: 28.76\n",
      "episode: 3124   score: 22.0  epsilon: 1.0    steps: 712  evaluation reward: 28.71\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7081: Policy loss: 0.010518. Value loss: 0.444097. Entropy: 0.884065.\n",
      "Iteration 7082: Policy loss: -0.006135. Value loss: 0.179167. Entropy: 0.877976.\n",
      "Iteration 7083: Policy loss: -0.016351. Value loss: 0.093985. Entropy: 0.878316.\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7084: Policy loss: 0.013296. Value loss: 0.413413. Entropy: 0.862698.\n",
      "Iteration 7085: Policy loss: -0.005814. Value loss: 0.182253. Entropy: 0.872377.\n",
      "Iteration 7086: Policy loss: -0.015658. Value loss: 0.116979. Entropy: 0.871273.\n",
      "episode: 3125   score: 28.0  epsilon: 1.0    steps: 456  evaluation reward: 28.78\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7087: Policy loss: 0.007773. Value loss: 0.423544. Entropy: 0.866275.\n",
      "Iteration 7088: Policy loss: -0.001032. Value loss: 0.208147. Entropy: 0.876342.\n",
      "Iteration 7089: Policy loss: -0.021528. Value loss: 0.145903. Entropy: 0.883953.\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7090: Policy loss: 0.010558. Value loss: 0.519681. Entropy: 0.941665.\n",
      "Iteration 7091: Policy loss: -0.000219. Value loss: 0.196876. Entropy: 0.944471.\n",
      "Iteration 7092: Policy loss: -0.014814. Value loss: 0.095291. Entropy: 0.945569.\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7093: Policy loss: 0.014494. Value loss: 0.563391. Entropy: 0.952195.\n",
      "Iteration 7094: Policy loss: 0.000897. Value loss: 0.228444. Entropy: 0.931410.\n",
      "Iteration 7095: Policy loss: -0.008016. Value loss: 0.112803. Entropy: 0.942732.\n",
      "episode: 3126   score: 22.0  epsilon: 1.0    steps: 1024  evaluation reward: 28.83\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7096: Policy loss: 0.011732. Value loss: 0.737593. Entropy: 0.978056.\n",
      "Iteration 7097: Policy loss: -0.001846. Value loss: 0.312986. Entropy: 0.986585.\n",
      "Iteration 7098: Policy loss: -0.013851. Value loss: 0.199662. Entropy: 0.959741.\n",
      "episode: 3127   score: 33.0  epsilon: 1.0    steps: 16  evaluation reward: 28.92\n",
      "episode: 3128   score: 11.0  epsilon: 1.0    steps: 216  evaluation reward: 28.74\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7099: Policy loss: 0.008040. Value loss: 0.750679. Entropy: 0.978273.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7100: Policy loss: 0.005633. Value loss: 0.396798. Entropy: 0.970138.\n",
      "Iteration 7101: Policy loss: -0.010405. Value loss: 0.228726. Entropy: 0.964569.\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7102: Policy loss: 0.006545. Value loss: 0.607028. Entropy: 0.939854.\n",
      "Iteration 7103: Policy loss: -0.006640. Value loss: 0.212700. Entropy: 0.936513.\n",
      "Iteration 7104: Policy loss: -0.015098. Value loss: 0.108154. Entropy: 0.943845.\n",
      "episode: 3129   score: 37.0  epsilon: 1.0    steps: 584  evaluation reward: 28.87\n",
      "episode: 3130   score: 32.0  epsilon: 1.0    steps: 648  evaluation reward: 28.94\n",
      "episode: 3131   score: 36.0  epsilon: 1.0    steps: 1024  evaluation reward: 29.11\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7105: Policy loss: 0.016895. Value loss: 0.685470. Entropy: 0.953878.\n",
      "Iteration 7106: Policy loss: 0.000042. Value loss: 0.304462. Entropy: 0.968775.\n",
      "Iteration 7107: Policy loss: -0.014519. Value loss: 0.159824. Entropy: 0.957124.\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7108: Policy loss: 0.007060. Value loss: 0.443634. Entropy: 0.942785.\n",
      "Iteration 7109: Policy loss: -0.007371. Value loss: 0.196593. Entropy: 0.937070.\n",
      "Iteration 7110: Policy loss: -0.016367. Value loss: 0.106167. Entropy: 0.935688.\n",
      "episode: 3132   score: 26.0  epsilon: 1.0    steps: 856  evaluation reward: 29.2\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7111: Policy loss: 0.006515. Value loss: 0.735397. Entropy: 0.862559.\n",
      "Iteration 7112: Policy loss: -0.003915. Value loss: 0.282970. Entropy: 0.866854.\n",
      "Iteration 7113: Policy loss: -0.013216. Value loss: 0.189480. Entropy: 0.867014.\n",
      "episode: 3133   score: 40.0  epsilon: 1.0    steps: 960  evaluation reward: 29.28\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7114: Policy loss: 0.016805. Value loss: 0.662684. Entropy: 0.872995.\n",
      "Iteration 7115: Policy loss: 0.010092. Value loss: 0.217917. Entropy: 0.872258.\n",
      "Iteration 7116: Policy loss: -0.009135. Value loss: 0.094147. Entropy: 0.874615.\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7117: Policy loss: 0.013051. Value loss: 0.660323. Entropy: 0.968698.\n",
      "Iteration 7118: Policy loss: -0.007913. Value loss: 0.311752. Entropy: 0.956756.\n",
      "Iteration 7119: Policy loss: -0.013807. Value loss: 0.177400. Entropy: 0.957536.\n",
      "episode: 3134   score: 17.0  epsilon: 1.0    steps: 472  evaluation reward: 29.15\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7120: Policy loss: 0.004457. Value loss: 0.427029. Entropy: 0.981746.\n",
      "Iteration 7121: Policy loss: -0.005714. Value loss: 0.184758. Entropy: 0.975992.\n",
      "Iteration 7122: Policy loss: -0.019609. Value loss: 0.111589. Entropy: 0.968198.\n",
      "episode: 3135   score: 18.0  epsilon: 1.0    steps: 296  evaluation reward: 29.04\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7123: Policy loss: 0.008522. Value loss: 0.725695. Entropy: 0.951186.\n",
      "Iteration 7124: Policy loss: -0.006908. Value loss: 0.320410. Entropy: 0.941666.\n",
      "Iteration 7125: Policy loss: -0.016249. Value loss: 0.181463. Entropy: 0.925667.\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7126: Policy loss: 0.013691. Value loss: 0.534238. Entropy: 0.915981.\n",
      "Iteration 7127: Policy loss: -0.003019. Value loss: 0.221515. Entropy: 0.901041.\n",
      "Iteration 7128: Policy loss: -0.016798. Value loss: 0.134292. Entropy: 0.897337.\n",
      "episode: 3136   score: 16.0  epsilon: 1.0    steps: 144  evaluation reward: 29.0\n",
      "episode: 3137   score: 21.0  epsilon: 1.0    steps: 392  evaluation reward: 28.94\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7129: Policy loss: 0.014942. Value loss: 0.728160. Entropy: 0.925230.\n",
      "Iteration 7130: Policy loss: -0.002091. Value loss: 0.276494. Entropy: 0.923024.\n",
      "Iteration 7131: Policy loss: -0.010176. Value loss: 0.138191. Entropy: 0.934861.\n",
      "episode: 3138   score: 35.0  epsilon: 1.0    steps: 96  evaluation reward: 29.19\n",
      "episode: 3139   score: 27.0  epsilon: 1.0    steps: 688  evaluation reward: 29.16\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7132: Policy loss: 0.007712. Value loss: 0.458659. Entropy: 0.936288.\n",
      "Iteration 7133: Policy loss: 0.002435. Value loss: 0.164610. Entropy: 0.923788.\n",
      "Iteration 7134: Policy loss: -0.009622. Value loss: 0.077726. Entropy: 0.937563.\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7135: Policy loss: 0.011155. Value loss: 0.432729. Entropy: 0.933091.\n",
      "Iteration 7136: Policy loss: -0.009075. Value loss: 0.214590. Entropy: 0.922477.\n",
      "Iteration 7137: Policy loss: -0.020556. Value loss: 0.140955. Entropy: 0.920497.\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7138: Policy loss: 0.011787. Value loss: 0.531522. Entropy: 0.932486.\n",
      "Iteration 7139: Policy loss: -0.005339. Value loss: 0.288693. Entropy: 0.919314.\n",
      "Iteration 7140: Policy loss: -0.015378. Value loss: 0.196202. Entropy: 0.913201.\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7141: Policy loss: 0.009975. Value loss: 0.489937. Entropy: 0.923515.\n",
      "Iteration 7142: Policy loss: -0.004520. Value loss: 0.222718. Entropy: 0.918202.\n",
      "Iteration 7143: Policy loss: -0.017896. Value loss: 0.137979. Entropy: 0.915733.\n",
      "episode: 3140   score: 35.0  epsilon: 1.0    steps: 616  evaluation reward: 29.29\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7144: Policy loss: 0.011636. Value loss: 0.720773. Entropy: 0.929436.\n",
      "Iteration 7145: Policy loss: 0.001722. Value loss: 0.285975. Entropy: 0.915331.\n",
      "Iteration 7146: Policy loss: -0.014828. Value loss: 0.149699. Entropy: 0.929620.\n",
      "episode: 3141   score: 26.0  epsilon: 1.0    steps: 32  evaluation reward: 29.24\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7147: Policy loss: 0.010784. Value loss: 0.566142. Entropy: 0.866611.\n",
      "Iteration 7148: Policy loss: -0.010279. Value loss: 0.243177. Entropy: 0.838491.\n",
      "Iteration 7149: Policy loss: -0.016077. Value loss: 0.132840. Entropy: 0.846671.\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7150: Policy loss: 0.016349. Value loss: 0.728476. Entropy: 0.897786.\n",
      "Iteration 7151: Policy loss: 0.004394. Value loss: 0.318185. Entropy: 0.879087.\n",
      "Iteration 7152: Policy loss: -0.003853. Value loss: 0.152339. Entropy: 0.883038.\n",
      "episode: 3142   score: 28.0  epsilon: 1.0    steps: 200  evaluation reward: 29.29\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7153: Policy loss: 0.008629. Value loss: 0.623486. Entropy: 0.956627.\n",
      "Iteration 7154: Policy loss: -0.000151. Value loss: 0.279472. Entropy: 0.952383.\n",
      "Iteration 7155: Policy loss: -0.011686. Value loss: 0.149107. Entropy: 0.938611.\n",
      "episode: 3143   score: 24.0  epsilon: 1.0    steps: 376  evaluation reward: 29.28\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7156: Policy loss: 0.011667. Value loss: 0.828814. Entropy: 0.996468.\n",
      "Iteration 7157: Policy loss: 0.004848. Value loss: 0.323887. Entropy: 0.973054.\n",
      "Iteration 7158: Policy loss: -0.011204. Value loss: 0.159874. Entropy: 0.972995.\n",
      "episode: 3144   score: 41.0  epsilon: 1.0    steps: 544  evaluation reward: 29.19\n",
      "episode: 3145   score: 29.0  epsilon: 1.0    steps: 608  evaluation reward: 29.24\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7159: Policy loss: 0.010124. Value loss: 0.964927. Entropy: 0.915682.\n",
      "Iteration 7160: Policy loss: 0.000385. Value loss: 0.348132. Entropy: 0.911072.\n",
      "Iteration 7161: Policy loss: -0.008828. Value loss: 0.211715. Entropy: 0.917286.\n",
      "episode: 3146   score: 30.0  epsilon: 1.0    steps: 320  evaluation reward: 29.37\n",
      "episode: 3147   score: 45.0  epsilon: 1.0    steps: 928  evaluation reward: 29.65\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7162: Policy loss: 0.008826. Value loss: 0.722854. Entropy: 0.913028.\n",
      "Iteration 7163: Policy loss: 0.001666. Value loss: 0.336690. Entropy: 0.900657.\n",
      "Iteration 7164: Policy loss: -0.009217. Value loss: 0.195724. Entropy: 0.902647.\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7165: Policy loss: 0.009283. Value loss: 0.768371. Entropy: 0.921826.\n",
      "Iteration 7166: Policy loss: -0.005412. Value loss: 0.333727. Entropy: 0.911356.\n",
      "Iteration 7167: Policy loss: -0.014972. Value loss: 0.168752. Entropy: 0.909586.\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7168: Policy loss: 0.017413. Value loss: 0.719587. Entropy: 0.909150.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7169: Policy loss: -0.001466. Value loss: 0.292002. Entropy: 0.908707.\n",
      "Iteration 7170: Policy loss: -0.013882. Value loss: 0.176456. Entropy: 0.894294.\n",
      "episode: 3148   score: 36.0  epsilon: 1.0    steps: 816  evaluation reward: 29.79\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7171: Policy loss: 0.011232. Value loss: 0.729188. Entropy: 0.946836.\n",
      "Iteration 7172: Policy loss: -0.002001. Value loss: 0.346613. Entropy: 0.928700.\n",
      "Iteration 7173: Policy loss: -0.014830. Value loss: 0.191247. Entropy: 0.937409.\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7174: Policy loss: 0.007030. Value loss: 0.515577. Entropy: 0.921941.\n",
      "Iteration 7175: Policy loss: -0.005336. Value loss: 0.214370. Entropy: 0.923529.\n",
      "Iteration 7176: Policy loss: -0.016379. Value loss: 0.144424. Entropy: 0.917487.\n",
      "episode: 3149   score: 34.0  epsilon: 1.0    steps: 64  evaluation reward: 29.83\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7177: Policy loss: 0.016111. Value loss: 0.584886. Entropy: 0.932123.\n",
      "Iteration 7178: Policy loss: -0.003385. Value loss: 0.189820. Entropy: 0.935646.\n",
      "Iteration 7179: Policy loss: -0.014775. Value loss: 0.108768. Entropy: 0.919008.\n",
      "episode: 3150   score: 14.0  epsilon: 1.0    steps: 200  evaluation reward: 29.61\n",
      "now time :  2019-03-06 15:01:23.053342\n",
      "episode: 3151   score: 25.0  epsilon: 1.0    steps: 608  evaluation reward: 29.58\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7180: Policy loss: 0.009483. Value loss: 0.860022. Entropy: 0.933950.\n",
      "Iteration 7181: Policy loss: -0.000754. Value loss: 0.371786. Entropy: 0.923076.\n",
      "Iteration 7182: Policy loss: -0.013486. Value loss: 0.177682. Entropy: 0.927090.\n",
      "episode: 3152   score: 25.0  epsilon: 1.0    steps: 888  evaluation reward: 29.62\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7183: Policy loss: 0.012874. Value loss: 0.672092. Entropy: 0.924548.\n",
      "Iteration 7184: Policy loss: -0.004501. Value loss: 0.316595. Entropy: 0.931998.\n",
      "Iteration 7185: Policy loss: -0.012264. Value loss: 0.190994. Entropy: 0.919709.\n",
      "episode: 3153   score: 27.0  epsilon: 1.0    steps: 632  evaluation reward: 29.67\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7186: Policy loss: 0.014219. Value loss: 0.992737. Entropy: 0.885134.\n",
      "Iteration 7187: Policy loss: 0.000032. Value loss: 0.461458. Entropy: 0.873483.\n",
      "Iteration 7188: Policy loss: -0.011924. Value loss: 0.299936. Entropy: 0.873042.\n",
      "episode: 3154   score: 41.0  epsilon: 1.0    steps: 432  evaluation reward: 29.83\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7189: Policy loss: 0.014458. Value loss: 0.309926. Entropy: 0.869602.\n",
      "Iteration 7190: Policy loss: -0.005062. Value loss: 0.113808. Entropy: 0.881724.\n",
      "Iteration 7191: Policy loss: -0.015709. Value loss: 0.061744. Entropy: 0.890807.\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7192: Policy loss: 0.013710. Value loss: 0.631506. Entropy: 0.859145.\n",
      "Iteration 7193: Policy loss: -0.002296. Value loss: 0.229312. Entropy: 0.857749.\n",
      "Iteration 7194: Policy loss: -0.015520. Value loss: 0.125587. Entropy: 0.868429.\n",
      "episode: 3155   score: 39.0  epsilon: 1.0    steps: 816  evaluation reward: 30.1\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7195: Policy loss: 0.012041. Value loss: 0.438310. Entropy: 0.945073.\n",
      "Iteration 7196: Policy loss: -0.008321. Value loss: 0.147821. Entropy: 0.955657.\n",
      "Iteration 7197: Policy loss: -0.022874. Value loss: 0.077868. Entropy: 0.958855.\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7198: Policy loss: 0.009334. Value loss: 0.540973. Entropy: 0.968837.\n",
      "Iteration 7199: Policy loss: 0.003025. Value loss: 0.216141. Entropy: 0.975888.\n",
      "Iteration 7200: Policy loss: -0.009418. Value loss: 0.129547. Entropy: 0.955808.\n",
      "episode: 3156   score: 26.0  epsilon: 1.0    steps: 200  evaluation reward: 30.13\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7201: Policy loss: 0.018159. Value loss: 0.720951. Entropy: 0.936448.\n",
      "Iteration 7202: Policy loss: 0.001582. Value loss: 0.338113. Entropy: 0.953934.\n",
      "Iteration 7203: Policy loss: -0.013735. Value loss: 0.145298. Entropy: 0.945522.\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7204: Policy loss: 0.009438. Value loss: 1.039970. Entropy: 0.967545.\n",
      "Iteration 7205: Policy loss: -0.009137. Value loss: 0.565263. Entropy: 0.965445.\n",
      "Iteration 7206: Policy loss: -0.011717. Value loss: 0.315782. Entropy: 0.967233.\n",
      "episode: 3157   score: 25.0  epsilon: 1.0    steps: 440  evaluation reward: 30.04\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7207: Policy loss: 0.009493. Value loss: 0.757214. Entropy: 0.982311.\n",
      "Iteration 7208: Policy loss: -0.007325. Value loss: 0.369781. Entropy: 0.983793.\n",
      "Iteration 7209: Policy loss: -0.016083. Value loss: 0.239472. Entropy: 0.975319.\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7210: Policy loss: 0.011418. Value loss: 0.624173. Entropy: 0.977069.\n",
      "Iteration 7211: Policy loss: -0.003469. Value loss: 0.231878. Entropy: 0.977209.\n",
      "Iteration 7212: Policy loss: -0.016486. Value loss: 0.121751. Entropy: 0.990937.\n",
      "episode: 3158   score: 41.0  epsilon: 1.0    steps: 40  evaluation reward: 30.14\n",
      "episode: 3159   score: 40.0  epsilon: 1.0    steps: 312  evaluation reward: 30.34\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7213: Policy loss: 0.012563. Value loss: 0.864443. Entropy: 0.949395.\n",
      "Iteration 7214: Policy loss: -0.006705. Value loss: 0.298248. Entropy: 0.925146.\n",
      "Iteration 7215: Policy loss: -0.017659. Value loss: 0.166458. Entropy: 0.930555.\n",
      "episode: 3160   score: 39.0  epsilon: 1.0    steps: 280  evaluation reward: 30.5\n",
      "episode: 3161   score: 33.0  epsilon: 1.0    steps: 608  evaluation reward: 30.62\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7216: Policy loss: 0.015591. Value loss: 0.904200. Entropy: 0.937012.\n",
      "Iteration 7217: Policy loss: -0.001509. Value loss: 0.419301. Entropy: 0.912404.\n",
      "Iteration 7218: Policy loss: -0.006795. Value loss: 0.260088. Entropy: 0.922089.\n",
      "episode: 3162   score: 15.0  epsilon: 1.0    steps: 912  evaluation reward: 30.5\n",
      "episode: 3163   score: 36.0  epsilon: 1.0    steps: 952  evaluation reward: 30.6\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7219: Policy loss: 0.011680. Value loss: 1.021811. Entropy: 0.939808.\n",
      "Iteration 7220: Policy loss: 0.009711. Value loss: 0.374620. Entropy: 0.905733.\n",
      "Iteration 7221: Policy loss: -0.010075. Value loss: 0.196612. Entropy: 0.912655.\n",
      "episode: 3164   score: 21.0  epsilon: 1.0    steps: 784  evaluation reward: 30.44\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7222: Policy loss: 0.017810. Value loss: 0.441851. Entropy: 0.908616.\n",
      "Iteration 7223: Policy loss: -0.003829. Value loss: 0.141681. Entropy: 0.910651.\n",
      "Iteration 7224: Policy loss: -0.013450. Value loss: 0.059534. Entropy: 0.893574.\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7225: Policy loss: 0.006161. Value loss: 0.392394. Entropy: 0.870944.\n",
      "Iteration 7226: Policy loss: -0.004705. Value loss: 0.141624. Entropy: 0.869911.\n",
      "Iteration 7227: Policy loss: -0.012012. Value loss: 0.068619. Entropy: 0.872029.\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7228: Policy loss: 0.015653. Value loss: 0.697319. Entropy: 0.932812.\n",
      "Iteration 7229: Policy loss: -0.000893. Value loss: 0.341337. Entropy: 0.920659.\n",
      "Iteration 7230: Policy loss: -0.009521. Value loss: 0.201165. Entropy: 0.925905.\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7231: Policy loss: 0.010902. Value loss: 0.517389. Entropy: 0.864878.\n",
      "Iteration 7232: Policy loss: -0.003825. Value loss: 0.270039. Entropy: 0.866224.\n",
      "Iteration 7233: Policy loss: -0.012189. Value loss: 0.159373. Entropy: 0.883864.\n",
      "episode: 3165   score: 15.0  epsilon: 1.0    steps: 552  evaluation reward: 30.21\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7234: Policy loss: 0.010909. Value loss: 0.705365. Entropy: 0.952881.\n",
      "Iteration 7235: Policy loss: -0.008651. Value loss: 0.284321. Entropy: 0.955796.\n",
      "Iteration 7236: Policy loss: -0.018534. Value loss: 0.155464. Entropy: 0.953872.\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7237: Policy loss: 0.009691. Value loss: 0.550430. Entropy: 0.901046.\n",
      "Iteration 7238: Policy loss: -0.003143. Value loss: 0.199477. Entropy: 0.898227.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7239: Policy loss: -0.011969. Value loss: 0.097859. Entropy: 0.904338.\n",
      "episode: 3166   score: 14.0  epsilon: 1.0    steps: 192  evaluation reward: 30.17\n",
      "episode: 3167   score: 29.0  epsilon: 1.0    steps: 456  evaluation reward: 30.25\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7240: Policy loss: 0.022226. Value loss: 0.948722. Entropy: 0.908495.\n",
      "Iteration 7241: Policy loss: 0.004577. Value loss: 0.368859. Entropy: 0.891346.\n",
      "Iteration 7242: Policy loss: -0.007050. Value loss: 0.185325. Entropy: 0.903707.\n",
      "episode: 3168   score: 28.0  epsilon: 1.0    steps: 96  evaluation reward: 30.31\n",
      "episode: 3169   score: 32.0  epsilon: 1.0    steps: 496  evaluation reward: 30.26\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7243: Policy loss: 0.010461. Value loss: 0.686268. Entropy: 0.865820.\n",
      "Iteration 7244: Policy loss: -0.002077. Value loss: 0.303666. Entropy: 0.865974.\n",
      "Iteration 7245: Policy loss: -0.011564. Value loss: 0.211655. Entropy: 0.874390.\n",
      "episode: 3170   score: 43.0  epsilon: 1.0    steps: 408  evaluation reward: 30.43\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7246: Policy loss: 0.009219. Value loss: 0.622081. Entropy: 0.934195.\n",
      "Iteration 7247: Policy loss: -0.000940. Value loss: 0.273046. Entropy: 0.922297.\n",
      "Iteration 7248: Policy loss: -0.008869. Value loss: 0.150454. Entropy: 0.923078.\n",
      "episode: 3171   score: 25.0  epsilon: 1.0    steps: 512  evaluation reward: 30.45\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7249: Policy loss: 0.009406. Value loss: 0.531348. Entropy: 0.914984.\n",
      "Iteration 7250: Policy loss: -0.003664. Value loss: 0.271148. Entropy: 0.916930.\n",
      "Iteration 7251: Policy loss: -0.018136. Value loss: 0.149873. Entropy: 0.897473.\n",
      "episode: 3172   score: 36.0  epsilon: 1.0    steps: 72  evaluation reward: 30.54\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7252: Policy loss: 0.008489. Value loss: 0.551845. Entropy: 0.862972.\n",
      "Iteration 7253: Policy loss: 0.001530. Value loss: 0.290278. Entropy: 0.852678.\n",
      "Iteration 7254: Policy loss: -0.010696. Value loss: 0.186589. Entropy: 0.863666.\n",
      "episode: 3173   score: 15.0  epsilon: 1.0    steps: 912  evaluation reward: 30.4\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7255: Policy loss: 0.014647. Value loss: 0.546080. Entropy: 0.860353.\n",
      "Iteration 7256: Policy loss: -0.006679. Value loss: 0.243224. Entropy: 0.846241.\n",
      "Iteration 7257: Policy loss: -0.015789. Value loss: 0.151564. Entropy: 0.857451.\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7258: Policy loss: 0.010795. Value loss: 0.679968. Entropy: 0.904598.\n",
      "Iteration 7259: Policy loss: -0.005011. Value loss: 0.337505. Entropy: 0.888306.\n",
      "Iteration 7260: Policy loss: -0.011440. Value loss: 0.194523. Entropy: 0.872873.\n",
      "episode: 3174   score: 25.0  epsilon: 1.0    steps: 32  evaluation reward: 30.47\n",
      "episode: 3175   score: 15.0  epsilon: 1.0    steps: 632  evaluation reward: 30.27\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7261: Policy loss: 0.016501. Value loss: 0.642814. Entropy: 0.850599.\n",
      "Iteration 7262: Policy loss: -0.010213. Value loss: 0.240614. Entropy: 0.832921.\n",
      "Iteration 7263: Policy loss: -0.018542. Value loss: 0.144580. Entropy: 0.838905.\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7264: Policy loss: 0.014732. Value loss: 0.413567. Entropy: 0.907205.\n",
      "Iteration 7265: Policy loss: -0.006278. Value loss: 0.138462. Entropy: 0.910784.\n",
      "Iteration 7266: Policy loss: -0.017383. Value loss: 0.074198. Entropy: 0.911380.\n",
      "episode: 3176   score: 18.0  epsilon: 1.0    steps: 488  evaluation reward: 30.24\n",
      "episode: 3177   score: 25.0  epsilon: 1.0    steps: 704  evaluation reward: 30.17\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7267: Policy loss: 0.009799. Value loss: 0.633061. Entropy: 0.911875.\n",
      "Iteration 7268: Policy loss: 0.001874. Value loss: 0.242939. Entropy: 0.925741.\n",
      "Iteration 7269: Policy loss: -0.014419. Value loss: 0.119262. Entropy: 0.927220.\n",
      "episode: 3178   score: 29.0  epsilon: 1.0    steps: 16  evaluation reward: 30.29\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7270: Policy loss: 0.012619. Value loss: 0.701125. Entropy: 0.844010.\n",
      "Iteration 7271: Policy loss: -0.003315. Value loss: 0.256656. Entropy: 0.859868.\n",
      "Iteration 7272: Policy loss: -0.013938. Value loss: 0.128620. Entropy: 0.859000.\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7273: Policy loss: 0.015537. Value loss: 0.550250. Entropy: 0.910978.\n",
      "Iteration 7274: Policy loss: -0.006767. Value loss: 0.193169. Entropy: 0.912304.\n",
      "Iteration 7275: Policy loss: -0.020227. Value loss: 0.099304. Entropy: 0.902563.\n",
      "episode: 3179   score: 26.0  epsilon: 1.0    steps: 608  evaluation reward: 30.08\n",
      "episode: 3180   score: 30.0  epsilon: 1.0    steps: 808  evaluation reward: 30.03\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7276: Policy loss: 0.016238. Value loss: 0.524052. Entropy: 0.929112.\n",
      "Iteration 7277: Policy loss: 0.003403. Value loss: 0.222752. Entropy: 0.940617.\n",
      "Iteration 7278: Policy loss: -0.012805. Value loss: 0.118052. Entropy: 0.917465.\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7279: Policy loss: 0.008145. Value loss: 0.572517. Entropy: 0.954196.\n",
      "Iteration 7280: Policy loss: -0.005229. Value loss: 0.228609. Entropy: 0.958868.\n",
      "Iteration 7281: Policy loss: -0.014700. Value loss: 0.125854. Entropy: 0.968890.\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7282: Policy loss: 0.011407. Value loss: 0.476409. Entropy: 0.973450.\n",
      "Iteration 7283: Policy loss: -0.002631. Value loss: 0.206069. Entropy: 0.977962.\n",
      "Iteration 7284: Policy loss: -0.014047. Value loss: 0.127215. Entropy: 0.985599.\n",
      "episode: 3181   score: 36.0  epsilon: 1.0    steps: 816  evaluation reward: 30.1\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7285: Policy loss: 0.016295. Value loss: 0.834760. Entropy: 0.866707.\n",
      "Iteration 7286: Policy loss: 0.000526. Value loss: 0.334031. Entropy: 0.870075.\n",
      "Iteration 7287: Policy loss: -0.013008. Value loss: 0.124414. Entropy: 0.860552.\n",
      "episode: 3182   score: 26.0  epsilon: 1.0    steps: 16  evaluation reward: 30.25\n",
      "episode: 3183   score: 25.0  epsilon: 1.0    steps: 168  evaluation reward: 30.17\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7288: Policy loss: 0.007851. Value loss: 0.401343. Entropy: 0.984430.\n",
      "Iteration 7289: Policy loss: -0.007452. Value loss: 0.146722. Entropy: 0.983113.\n",
      "Iteration 7290: Policy loss: -0.015569. Value loss: 0.087799. Entropy: 0.978822.\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7291: Policy loss: 0.006520. Value loss: 0.551822. Entropy: 0.989281.\n",
      "Iteration 7292: Policy loss: -0.007732. Value loss: 0.181882. Entropy: 0.985871.\n",
      "Iteration 7293: Policy loss: -0.018006. Value loss: 0.092622. Entropy: 0.997266.\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7294: Policy loss: 0.006640. Value loss: 0.582519. Entropy: 0.991040.\n",
      "Iteration 7295: Policy loss: -0.002303. Value loss: 0.217867. Entropy: 0.994716.\n",
      "Iteration 7296: Policy loss: -0.010360. Value loss: 0.111287. Entropy: 0.987460.\n",
      "episode: 3184   score: 30.0  epsilon: 1.0    steps: 176  evaluation reward: 30.19\n",
      "episode: 3185   score: 29.0  epsilon: 1.0    steps: 960  evaluation reward: 30.17\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7297: Policy loss: 0.007908. Value loss: 0.690813. Entropy: 0.950448.\n",
      "Iteration 7298: Policy loss: -0.002844. Value loss: 0.268205. Entropy: 0.960450.\n",
      "Iteration 7299: Policy loss: -0.012842. Value loss: 0.157817. Entropy: 0.955475.\n",
      "episode: 3186   score: 36.0  epsilon: 1.0    steps: 328  evaluation reward: 30.26\n",
      "episode: 3187   score: 23.0  epsilon: 1.0    steps: 544  evaluation reward: 30.24\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7300: Policy loss: 0.009575. Value loss: 0.379182. Entropy: 0.969331.\n",
      "Iteration 7301: Policy loss: -0.004526. Value loss: 0.174945. Entropy: 0.985986.\n",
      "Iteration 7302: Policy loss: -0.016245. Value loss: 0.115239. Entropy: 0.971030.\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7303: Policy loss: 0.012360. Value loss: 0.474487. Entropy: 0.967946.\n",
      "Iteration 7304: Policy loss: 0.000728. Value loss: 0.200741. Entropy: 0.982893.\n",
      "Iteration 7305: Policy loss: -0.016695. Value loss: 0.123197. Entropy: 0.976681.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3188   score: 30.0  epsilon: 1.0    steps: 224  evaluation reward: 30.21\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7306: Policy loss: 0.002489. Value loss: 0.679436. Entropy: 1.021734.\n",
      "Iteration 7307: Policy loss: -0.010930. Value loss: 0.353565. Entropy: 1.032019.\n",
      "Iteration 7308: Policy loss: -0.014892. Value loss: 0.232037. Entropy: 1.028337.\n",
      "episode: 3189   score: 18.0  epsilon: 1.0    steps: 120  evaluation reward: 29.85\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7309: Policy loss: 0.023192. Value loss: 0.798487. Entropy: 0.958822.\n",
      "Iteration 7310: Policy loss: 0.002646. Value loss: 0.332251. Entropy: 0.975390.\n",
      "Iteration 7311: Policy loss: -0.013557. Value loss: 0.191065. Entropy: 0.969121.\n",
      "episode: 3190   score: 23.0  epsilon: 1.0    steps: 736  evaluation reward: 29.53\n",
      "episode: 3191   score: 12.0  epsilon: 1.0    steps: 1024  evaluation reward: 29.28\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7312: Policy loss: 0.015434. Value loss: 0.478777. Entropy: 0.996737.\n",
      "Iteration 7313: Policy loss: -0.004598. Value loss: 0.187317. Entropy: 0.973979.\n",
      "Iteration 7314: Policy loss: -0.015910. Value loss: 0.102755. Entropy: 0.990123.\n",
      "episode: 3192   score: 25.0  epsilon: 1.0    steps: 96  evaluation reward: 29.17\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7315: Policy loss: 0.011115. Value loss: 0.239164. Entropy: 0.923858.\n",
      "Iteration 7316: Policy loss: -0.000644. Value loss: 0.094230. Entropy: 0.942950.\n",
      "Iteration 7317: Policy loss: -0.014648. Value loss: 0.064392. Entropy: 0.919571.\n",
      "episode: 3193   score: 24.0  epsilon: 1.0    steps: 360  evaluation reward: 29.06\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7318: Policy loss: 0.012371. Value loss: 0.570337. Entropy: 0.944391.\n",
      "Iteration 7319: Policy loss: -0.006572. Value loss: 0.244122. Entropy: 0.937519.\n",
      "Iteration 7320: Policy loss: -0.013921. Value loss: 0.153913. Entropy: 0.930258.\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7321: Policy loss: 0.009839. Value loss: 0.388286. Entropy: 0.903967.\n",
      "Iteration 7322: Policy loss: -0.003221. Value loss: 0.115141. Entropy: 0.908153.\n",
      "Iteration 7323: Policy loss: -0.013504. Value loss: 0.068461. Entropy: 0.909850.\n",
      "episode: 3194   score: 38.0  epsilon: 1.0    steps: 632  evaluation reward: 28.9\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7324: Policy loss: 0.011166. Value loss: 0.684830. Entropy: 1.006129.\n",
      "Iteration 7325: Policy loss: -0.004258. Value loss: 0.323603. Entropy: 0.995756.\n",
      "Iteration 7326: Policy loss: -0.016352. Value loss: 0.185091. Entropy: 0.986793.\n",
      "episode: 3195   score: 21.0  epsilon: 1.0    steps: 120  evaluation reward: 28.87\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7327: Policy loss: 0.009373. Value loss: 0.561923. Entropy: 0.932133.\n",
      "Iteration 7328: Policy loss: -0.000779. Value loss: 0.250414. Entropy: 0.938718.\n",
      "Iteration 7329: Policy loss: -0.012249. Value loss: 0.134068. Entropy: 0.926376.\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7330: Policy loss: 0.010880. Value loss: 0.986649. Entropy: 0.885377.\n",
      "Iteration 7331: Policy loss: 0.000309. Value loss: 0.358215. Entropy: 0.874327.\n",
      "Iteration 7332: Policy loss: -0.011715. Value loss: 0.188003. Entropy: 0.895657.\n",
      "episode: 3196   score: 36.0  epsilon: 1.0    steps: 296  evaluation reward: 28.87\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7333: Policy loss: 0.013027. Value loss: 0.662221. Entropy: 1.040769.\n",
      "Iteration 7334: Policy loss: -0.000603. Value loss: 0.259050. Entropy: 1.021181.\n",
      "Iteration 7335: Policy loss: -0.010144. Value loss: 0.165855. Entropy: 1.034979.\n",
      "episode: 3197   score: 32.0  epsilon: 1.0    steps: 488  evaluation reward: 28.91\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7336: Policy loss: 0.013515. Value loss: 0.924854. Entropy: 0.993801.\n",
      "Iteration 7337: Policy loss: 0.000239. Value loss: 0.308306. Entropy: 1.000566.\n",
      "Iteration 7338: Policy loss: -0.014146. Value loss: 0.185989. Entropy: 1.011324.\n",
      "episode: 3198   score: 28.0  epsilon: 1.0    steps: 56  evaluation reward: 28.81\n",
      "episode: 3199   score: 30.0  epsilon: 1.0    steps: 808  evaluation reward: 28.96\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7339: Policy loss: 0.008251. Value loss: 0.679390. Entropy: 0.984204.\n",
      "Iteration 7340: Policy loss: 0.003567. Value loss: 0.279266. Entropy: 0.966243.\n",
      "Iteration 7341: Policy loss: -0.008812. Value loss: 0.153384. Entropy: 0.971875.\n",
      "episode: 3200   score: 31.0  epsilon: 1.0    steps: 336  evaluation reward: 28.99\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7342: Policy loss: 0.005099. Value loss: 0.791725. Entropy: 0.985576.\n",
      "Iteration 7343: Policy loss: -0.004404. Value loss: 0.362704. Entropy: 0.970566.\n",
      "Iteration 7344: Policy loss: -0.012634. Value loss: 0.210148. Entropy: 0.972560.\n",
      "now time :  2019-03-06 15:04:52.492059\n",
      "episode: 3201   score: 28.0  epsilon: 1.0    steps: 216  evaluation reward: 28.99\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7345: Policy loss: 0.007056. Value loss: 0.428778. Entropy: 0.982945.\n",
      "Iteration 7346: Policy loss: -0.005839. Value loss: 0.182310. Entropy: 0.979275.\n",
      "Iteration 7347: Policy loss: -0.014722. Value loss: 0.100603. Entropy: 0.982707.\n",
      "episode: 3202   score: 26.0  epsilon: 1.0    steps: 672  evaluation reward: 29.03\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7348: Policy loss: 0.010754. Value loss: 0.471410. Entropy: 1.010539.\n",
      "Iteration 7349: Policy loss: -0.006132. Value loss: 0.175548. Entropy: 0.996270.\n",
      "Iteration 7350: Policy loss: -0.012586. Value loss: 0.119128. Entropy: 1.000273.\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7351: Policy loss: 0.005864. Value loss: 0.477068. Entropy: 0.918150.\n",
      "Iteration 7352: Policy loss: -0.009197. Value loss: 0.153791. Entropy: 0.926242.\n",
      "Iteration 7353: Policy loss: -0.017150. Value loss: 0.071567. Entropy: 0.926619.\n",
      "episode: 3203   score: 18.0  epsilon: 1.0    steps: 224  evaluation reward: 28.93\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7354: Policy loss: 0.008826. Value loss: 0.623895. Entropy: 1.009851.\n",
      "Iteration 7355: Policy loss: -0.003448. Value loss: 0.255068. Entropy: 1.011180.\n",
      "Iteration 7356: Policy loss: -0.012402. Value loss: 0.156519. Entropy: 1.002630.\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7357: Policy loss: 0.011600. Value loss: 0.741161. Entropy: 0.937458.\n",
      "Iteration 7358: Policy loss: -0.009011. Value loss: 0.281455. Entropy: 0.942772.\n",
      "Iteration 7359: Policy loss: -0.017070. Value loss: 0.146005. Entropy: 0.932146.\n",
      "episode: 3204   score: 22.0  epsilon: 1.0    steps: 536  evaluation reward: 28.83\n",
      "episode: 3205   score: 40.0  epsilon: 1.0    steps: 808  evaluation reward: 28.75\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7360: Policy loss: 0.012505. Value loss: 0.962857. Entropy: 0.928522.\n",
      "Iteration 7361: Policy loss: -0.000921. Value loss: 0.398960. Entropy: 0.918976.\n",
      "Iteration 7362: Policy loss: -0.012671. Value loss: 0.245477. Entropy: 0.921190.\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7363: Policy loss: 0.016489. Value loss: 0.471636. Entropy: 0.981654.\n",
      "Iteration 7364: Policy loss: 0.002512. Value loss: 0.190353. Entropy: 0.987833.\n",
      "Iteration 7365: Policy loss: -0.011579. Value loss: 0.107498. Entropy: 0.981924.\n",
      "episode: 3206   score: 29.0  epsilon: 1.0    steps: 688  evaluation reward: 28.81\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7366: Policy loss: 0.016181. Value loss: 0.645842. Entropy: 0.980575.\n",
      "Iteration 7367: Policy loss: -0.000852. Value loss: 0.230038. Entropy: 0.973956.\n",
      "Iteration 7368: Policy loss: -0.016718. Value loss: 0.138522. Entropy: 0.957199.\n",
      "episode: 3207   score: 36.0  epsilon: 1.0    steps: 504  evaluation reward: 28.88\n",
      "episode: 3208   score: 22.0  epsilon: 1.0    steps: 792  evaluation reward: 28.77\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7369: Policy loss: 0.008303. Value loss: 0.650144. Entropy: 0.965982.\n",
      "Iteration 7370: Policy loss: -0.005191. Value loss: 0.314687. Entropy: 0.973338.\n",
      "Iteration 7371: Policy loss: -0.013168. Value loss: 0.205570. Entropy: 0.973259.\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7372: Policy loss: 0.007329. Value loss: 0.553226. Entropy: 0.939177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7373: Policy loss: 0.000035. Value loss: 0.239292. Entropy: 0.937023.\n",
      "Iteration 7374: Policy loss: -0.011911. Value loss: 0.121249. Entropy: 0.932542.\n",
      "episode: 3209   score: 42.0  epsilon: 1.0    steps: 600  evaluation reward: 28.86\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7375: Policy loss: 0.014460. Value loss: 0.727331. Entropy: 0.956383.\n",
      "Iteration 7376: Policy loss: 0.001507. Value loss: 0.400222. Entropy: 0.957211.\n",
      "Iteration 7377: Policy loss: -0.011819. Value loss: 0.252997. Entropy: 0.937792.\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7378: Policy loss: 0.007897. Value loss: 0.430988. Entropy: 1.003431.\n",
      "Iteration 7379: Policy loss: -0.000635. Value loss: 0.177841. Entropy: 1.008103.\n",
      "Iteration 7380: Policy loss: -0.012985. Value loss: 0.121915. Entropy: 1.020665.\n",
      "episode: 3210   score: 45.0  epsilon: 1.0    steps: 304  evaluation reward: 28.87\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7381: Policy loss: 0.012473. Value loss: 0.703140. Entropy: 0.998597.\n",
      "Iteration 7382: Policy loss: -0.004172. Value loss: 0.343798. Entropy: 0.993308.\n",
      "Iteration 7383: Policy loss: -0.013136. Value loss: 0.198156. Entropy: 0.974663.\n",
      "episode: 3211   score: 18.0  epsilon: 1.0    steps: 552  evaluation reward: 28.63\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7384: Policy loss: 0.012727. Value loss: 0.881035. Entropy: 1.005169.\n",
      "Iteration 7385: Policy loss: -0.003099. Value loss: 0.305912. Entropy: 0.982124.\n",
      "Iteration 7386: Policy loss: -0.009855. Value loss: 0.175496. Entropy: 0.989595.\n",
      "episode: 3212   score: 15.0  epsilon: 1.0    steps: 160  evaluation reward: 28.48\n",
      "episode: 3213   score: 30.0  epsilon: 1.0    steps: 232  evaluation reward: 28.47\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7387: Policy loss: 0.010186. Value loss: 0.571832. Entropy: 0.958062.\n",
      "Iteration 7388: Policy loss: -0.006048. Value loss: 0.234746. Entropy: 0.948419.\n",
      "Iteration 7389: Policy loss: -0.019476. Value loss: 0.135862. Entropy: 0.958625.\n",
      "episode: 3214   score: 24.0  epsilon: 1.0    steps: 752  evaluation reward: 28.32\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7390: Policy loss: 0.013125. Value loss: 0.626683. Entropy: 1.071080.\n",
      "Iteration 7391: Policy loss: -0.002219. Value loss: 0.245355. Entropy: 1.066041.\n",
      "Iteration 7392: Policy loss: -0.013582. Value loss: 0.133365. Entropy: 1.060022.\n",
      "episode: 3215   score: 11.0  epsilon: 1.0    steps: 256  evaluation reward: 27.99\n",
      "episode: 3216   score: 30.0  epsilon: 1.0    steps: 352  evaluation reward: 27.94\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7393: Policy loss: 0.008602. Value loss: 0.567714. Entropy: 0.988271.\n",
      "Iteration 7394: Policy loss: -0.010428. Value loss: 0.235378. Entropy: 0.996428.\n",
      "Iteration 7395: Policy loss: -0.017960. Value loss: 0.135800. Entropy: 0.986963.\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7396: Policy loss: 0.010664. Value loss: 0.402829. Entropy: 0.981944.\n",
      "Iteration 7397: Policy loss: -0.009516. Value loss: 0.190348. Entropy: 0.993061.\n",
      "Iteration 7398: Policy loss: -0.020229. Value loss: 0.118932. Entropy: 0.982792.\n",
      "episode: 3217   score: 32.0  epsilon: 1.0    steps: 24  evaluation reward: 27.91\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7399: Policy loss: 0.005533. Value loss: 0.461174. Entropy: 1.017159.\n",
      "Iteration 7400: Policy loss: -0.009141. Value loss: 0.222606. Entropy: 1.009499.\n",
      "Iteration 7401: Policy loss: -0.015680. Value loss: 0.099072. Entropy: 1.017844.\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7402: Policy loss: 0.010098. Value loss: 0.342022. Entropy: 1.087108.\n",
      "Iteration 7403: Policy loss: -0.006291. Value loss: 0.130708. Entropy: 1.074770.\n",
      "Iteration 7404: Policy loss: -0.019341. Value loss: 0.075445. Entropy: 1.075017.\n",
      "episode: 3218   score: 13.0  epsilon: 1.0    steps: 336  evaluation reward: 27.77\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7405: Policy loss: 0.005944. Value loss: 0.661610. Entropy: 1.034205.\n",
      "Iteration 7406: Policy loss: 0.000900. Value loss: 0.158957. Entropy: 1.019627.\n",
      "Iteration 7407: Policy loss: -0.013197. Value loss: 0.076725. Entropy: 1.033401.\n",
      "episode: 3219   score: 23.0  epsilon: 1.0    steps: 8  evaluation reward: 27.54\n",
      "episode: 3220   score: 21.0  epsilon: 1.0    steps: 568  evaluation reward: 27.51\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7408: Policy loss: 0.015645. Value loss: 0.542403. Entropy: 0.919476.\n",
      "Iteration 7409: Policy loss: 0.000759. Value loss: 0.187350. Entropy: 0.933531.\n",
      "Iteration 7410: Policy loss: -0.012024. Value loss: 0.088299. Entropy: 0.931341.\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7411: Policy loss: 0.012393. Value loss: 0.412075. Entropy: 1.025557.\n",
      "Iteration 7412: Policy loss: -0.001261. Value loss: 0.154315. Entropy: 1.046097.\n",
      "Iteration 7413: Policy loss: -0.016889. Value loss: 0.091696. Entropy: 1.033199.\n",
      "episode: 3221   score: 19.0  epsilon: 1.0    steps: 480  evaluation reward: 27.5\n",
      "episode: 3222   score: 25.0  epsilon: 1.0    steps: 728  evaluation reward: 27.53\n",
      "episode: 3223   score: 34.0  epsilon: 1.0    steps: 976  evaluation reward: 27.57\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7414: Policy loss: 0.017579. Value loss: 0.552037. Entropy: 1.012134.\n",
      "Iteration 7415: Policy loss: -0.003794. Value loss: 0.232812. Entropy: 0.994836.\n",
      "Iteration 7416: Policy loss: -0.013722. Value loss: 0.132140. Entropy: 1.002983.\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7417: Policy loss: 0.008648. Value loss: 0.372186. Entropy: 0.976766.\n",
      "Iteration 7418: Policy loss: -0.005023. Value loss: 0.139827. Entropy: 0.970966.\n",
      "Iteration 7419: Policy loss: -0.016081. Value loss: 0.078764. Entropy: 0.980258.\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7420: Policy loss: 0.015339. Value loss: 0.373975. Entropy: 1.039751.\n",
      "Iteration 7421: Policy loss: -0.006504. Value loss: 0.148458. Entropy: 1.023947.\n",
      "Iteration 7422: Policy loss: -0.013351. Value loss: 0.073464. Entropy: 1.019076.\n",
      "episode: 3224   score: 18.0  epsilon: 1.0    steps: 8  evaluation reward: 27.53\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7423: Policy loss: 0.013820. Value loss: 0.567090. Entropy: 0.983975.\n",
      "Iteration 7424: Policy loss: -0.001483. Value loss: 0.184791. Entropy: 0.985461.\n",
      "Iteration 7425: Policy loss: -0.012693. Value loss: 0.116761. Entropy: 0.986239.\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7426: Policy loss: 0.014516. Value loss: 0.317409. Entropy: 0.951562.\n",
      "Iteration 7427: Policy loss: -0.005208. Value loss: 0.113376. Entropy: 0.946045.\n",
      "Iteration 7428: Policy loss: -0.018177. Value loss: 0.068168. Entropy: 0.946473.\n",
      "episode: 3225   score: 34.0  epsilon: 1.0    steps: 760  evaluation reward: 27.59\n",
      "episode: 3226   score: 24.0  epsilon: 1.0    steps: 992  evaluation reward: 27.61\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7429: Policy loss: 0.013603. Value loss: 0.734043. Entropy: 0.971631.\n",
      "Iteration 7430: Policy loss: -0.004998. Value loss: 0.281570. Entropy: 0.953388.\n",
      "Iteration 7431: Policy loss: -0.014370. Value loss: 0.147242. Entropy: 0.955054.\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7432: Policy loss: 0.007340. Value loss: 0.346857. Entropy: 0.951194.\n",
      "Iteration 7433: Policy loss: -0.008845. Value loss: 0.108102. Entropy: 0.960483.\n",
      "Iteration 7434: Policy loss: -0.022036. Value loss: 0.060104. Entropy: 0.957282.\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7435: Policy loss: 0.010686. Value loss: 0.530147. Entropy: 0.957525.\n",
      "Iteration 7436: Policy loss: -0.005936. Value loss: 0.206283. Entropy: 0.956973.\n",
      "Iteration 7437: Policy loss: -0.016731. Value loss: 0.134261. Entropy: 0.957071.\n",
      "episode: 3227   score: 47.0  epsilon: 1.0    steps: 128  evaluation reward: 27.75\n",
      "episode: 3228   score: 28.0  epsilon: 1.0    steps: 224  evaluation reward: 27.92\n",
      "episode: 3229   score: 22.0  epsilon: 1.0    steps: 880  evaluation reward: 27.77\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7438: Policy loss: 0.009186. Value loss: 0.478594. Entropy: 0.993786.\n",
      "Iteration 7439: Policy loss: 0.003157. Value loss: 0.213024. Entropy: 0.973598.\n",
      "Iteration 7440: Policy loss: -0.006478. Value loss: 0.112519. Entropy: 0.977811.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3230   score: 29.0  epsilon: 1.0    steps: 296  evaluation reward: 27.74\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7441: Policy loss: 0.007736. Value loss: 0.552568. Entropy: 0.920428.\n",
      "Iteration 7442: Policy loss: -0.006220. Value loss: 0.266743. Entropy: 0.892762.\n",
      "Iteration 7443: Policy loss: -0.015178. Value loss: 0.152051. Entropy: 0.903703.\n",
      "episode: 3231   score: 29.0  epsilon: 1.0    steps: 616  evaluation reward: 27.67\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7444: Policy loss: 0.008480. Value loss: 0.359242. Entropy: 0.990540.\n",
      "Iteration 7445: Policy loss: -0.008110. Value loss: 0.135650. Entropy: 0.986546.\n",
      "Iteration 7446: Policy loss: -0.020515. Value loss: 0.085365. Entropy: 0.983183.\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7447: Policy loss: 0.018987. Value loss: 0.429234. Entropy: 0.940992.\n",
      "Iteration 7448: Policy loss: -0.007813. Value loss: 0.178426. Entropy: 0.961069.\n",
      "Iteration 7449: Policy loss: -0.020545. Value loss: 0.094263. Entropy: 0.945581.\n",
      "episode: 3232   score: 25.0  epsilon: 1.0    steps: 552  evaluation reward: 27.66\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7450: Policy loss: 0.015621. Value loss: 0.288450. Entropy: 0.982694.\n",
      "Iteration 7451: Policy loss: -0.009730. Value loss: 0.113829. Entropy: 0.984213.\n",
      "Iteration 7452: Policy loss: -0.020138. Value loss: 0.073769. Entropy: 0.975322.\n",
      "episode: 3233   score: 16.0  epsilon: 1.0    steps: 216  evaluation reward: 27.42\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7453: Policy loss: 0.009367. Value loss: 0.371555. Entropy: 0.968166.\n",
      "Iteration 7454: Policy loss: -0.010272. Value loss: 0.163631. Entropy: 0.980019.\n",
      "Iteration 7455: Policy loss: -0.018438. Value loss: 0.110547. Entropy: 0.978132.\n",
      "episode: 3234   score: 29.0  epsilon: 1.0    steps: 1008  evaluation reward: 27.54\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7456: Policy loss: 0.015764. Value loss: 0.580347. Entropy: 0.960504.\n",
      "Iteration 7457: Policy loss: -0.004171. Value loss: 0.315634. Entropy: 0.956977.\n",
      "Iteration 7458: Policy loss: -0.010396. Value loss: 0.185692. Entropy: 0.959990.\n",
      "episode: 3235   score: 17.0  epsilon: 1.0    steps: 912  evaluation reward: 27.53\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7459: Policy loss: 0.012383. Value loss: 0.593130. Entropy: 0.988077.\n",
      "Iteration 7460: Policy loss: 0.005255. Value loss: 0.199465. Entropy: 0.986072.\n",
      "Iteration 7461: Policy loss: -0.005845. Value loss: 0.114005. Entropy: 1.000929.\n",
      "episode: 3236   score: 24.0  epsilon: 1.0    steps: 728  evaluation reward: 27.61\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7462: Policy loss: 0.010699. Value loss: 0.797196. Entropy: 0.942327.\n",
      "Iteration 7463: Policy loss: 0.000167. Value loss: 0.391429. Entropy: 0.943173.\n",
      "Iteration 7464: Policy loss: -0.012145. Value loss: 0.258296. Entropy: 0.935411.\n",
      "episode: 3237   score: 22.0  epsilon: 1.0    steps: 648  evaluation reward: 27.62\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7465: Policy loss: 0.005465. Value loss: 0.615075. Entropy: 0.981751.\n",
      "Iteration 7466: Policy loss: -0.002284. Value loss: 0.358249. Entropy: 0.964160.\n",
      "Iteration 7467: Policy loss: -0.016210. Value loss: 0.232126. Entropy: 0.971183.\n",
      "episode: 3238   score: 20.0  epsilon: 1.0    steps: 344  evaluation reward: 27.47\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7468: Policy loss: 0.004564. Value loss: 0.616741. Entropy: 0.949108.\n",
      "Iteration 7469: Policy loss: -0.010278. Value loss: 0.271170. Entropy: 0.956422.\n",
      "Iteration 7470: Policy loss: -0.019824. Value loss: 0.140228. Entropy: 0.936755.\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7471: Policy loss: 0.014750. Value loss: 0.755056. Entropy: 0.946166.\n",
      "Iteration 7472: Policy loss: 0.012007. Value loss: 0.347280. Entropy: 0.933061.\n",
      "Iteration 7473: Policy loss: 0.001843. Value loss: 0.157882. Entropy: 0.948460.\n",
      "episode: 3239   score: 20.0  epsilon: 1.0    steps: 360  evaluation reward: 27.4\n",
      "episode: 3240   score: 31.0  epsilon: 1.0    steps: 456  evaluation reward: 27.36\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7474: Policy loss: 0.015819. Value loss: 0.544521. Entropy: 1.012908.\n",
      "Iteration 7475: Policy loss: -0.005397. Value loss: 0.210370. Entropy: 1.013637.\n",
      "Iteration 7476: Policy loss: -0.013781. Value loss: 0.108853. Entropy: 1.005319.\n",
      "episode: 3241   score: 34.0  epsilon: 1.0    steps: 440  evaluation reward: 27.44\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7477: Policy loss: 0.006738. Value loss: 0.450442. Entropy: 1.038062.\n",
      "Iteration 7478: Policy loss: -0.010010. Value loss: 0.211254. Entropy: 1.014502.\n",
      "Iteration 7479: Policy loss: -0.016971. Value loss: 0.110391. Entropy: 1.010973.\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7480: Policy loss: 0.009575. Value loss: 0.331713. Entropy: 0.949884.\n",
      "Iteration 7481: Policy loss: -0.007451. Value loss: 0.156256. Entropy: 0.929769.\n",
      "Iteration 7482: Policy loss: -0.017865. Value loss: 0.084693. Entropy: 0.934540.\n",
      "episode: 3242   score: 29.0  epsilon: 1.0    steps: 952  evaluation reward: 27.45\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7483: Policy loss: 0.011960. Value loss: 0.993636. Entropy: 0.925069.\n",
      "Iteration 7484: Policy loss: -0.005579. Value loss: 0.348755. Entropy: 0.915652.\n",
      "Iteration 7485: Policy loss: -0.015953. Value loss: 0.185229. Entropy: 0.926238.\n",
      "episode: 3243   score: 28.0  epsilon: 1.0    steps: 792  evaluation reward: 27.49\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7486: Policy loss: 0.002070. Value loss: 0.468627. Entropy: 0.939034.\n",
      "Iteration 7487: Policy loss: -0.008577. Value loss: 0.197040. Entropy: 0.935248.\n",
      "Iteration 7488: Policy loss: -0.020366. Value loss: 0.098287. Entropy: 0.943632.\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7489: Policy loss: 0.007569. Value loss: 0.430239. Entropy: 0.937292.\n",
      "Iteration 7490: Policy loss: -0.007878. Value loss: 0.127137. Entropy: 0.934104.\n",
      "Iteration 7491: Policy loss: -0.018256. Value loss: 0.074919. Entropy: 0.948712.\n",
      "episode: 3244   score: 24.0  epsilon: 1.0    steps: 184  evaluation reward: 27.32\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7492: Policy loss: 0.011303. Value loss: 0.670073. Entropy: 0.895570.\n",
      "Iteration 7493: Policy loss: -0.001498. Value loss: 0.207513. Entropy: 0.890030.\n",
      "Iteration 7494: Policy loss: -0.010719. Value loss: 0.116931. Entropy: 0.895840.\n",
      "episode: 3245   score: 17.0  epsilon: 1.0    steps: 160  evaluation reward: 27.2\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7495: Policy loss: 0.008717. Value loss: 0.542509. Entropy: 0.967195.\n",
      "Iteration 7496: Policy loss: -0.004131. Value loss: 0.220205. Entropy: 0.982832.\n",
      "Iteration 7497: Policy loss: -0.016595. Value loss: 0.129585. Entropy: 0.971063.\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7498: Policy loss: 0.010259. Value loss: 0.549695. Entropy: 0.972925.\n",
      "Iteration 7499: Policy loss: -0.010000. Value loss: 0.201986. Entropy: 0.966074.\n",
      "Iteration 7500: Policy loss: -0.015543. Value loss: 0.155890. Entropy: 0.965815.\n",
      "episode: 3246   score: 31.0  epsilon: 1.0    steps: 144  evaluation reward: 27.21\n",
      "episode: 3247   score: 28.0  epsilon: 1.0    steps: 576  evaluation reward: 27.04\n",
      "episode: 3248   score: 33.0  epsilon: 1.0    steps: 800  evaluation reward: 27.01\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7501: Policy loss: 0.006567. Value loss: 0.499025. Entropy: 0.972023.\n",
      "Iteration 7502: Policy loss: -0.008080. Value loss: 0.217521. Entropy: 0.978463.\n",
      "Iteration 7503: Policy loss: -0.014815. Value loss: 0.129722. Entropy: 0.973655.\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7504: Policy loss: 0.005959. Value loss: 0.615131. Entropy: 0.943851.\n",
      "Iteration 7505: Policy loss: -0.005538. Value loss: 0.323784. Entropy: 0.936443.\n",
      "Iteration 7506: Policy loss: -0.017732. Value loss: 0.235068. Entropy: 0.933679.\n",
      "episode: 3249   score: 43.0  epsilon: 1.0    steps: 168  evaluation reward: 27.1\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7507: Policy loss: 0.020571. Value loss: 0.495602. Entropy: 0.982039.\n",
      "Iteration 7508: Policy loss: 0.001354. Value loss: 0.258306. Entropy: 0.972603.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7509: Policy loss: -0.012205. Value loss: 0.174832. Entropy: 0.965493.\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7510: Policy loss: 0.018282. Value loss: 0.575343. Entropy: 0.932017.\n",
      "Iteration 7511: Policy loss: 0.006116. Value loss: 0.271955. Entropy: 0.923638.\n",
      "Iteration 7512: Policy loss: -0.009403. Value loss: 0.167319. Entropy: 0.923837.\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7513: Policy loss: 0.011331. Value loss: 0.845574. Entropy: 0.901700.\n",
      "Iteration 7514: Policy loss: -0.004001. Value loss: 0.393121. Entropy: 0.904894.\n",
      "Iteration 7515: Policy loss: -0.015350. Value loss: 0.209440. Entropy: 0.902714.\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7516: Policy loss: 0.004503. Value loss: 0.792525. Entropy: 0.961320.\n",
      "Iteration 7517: Policy loss: -0.004794. Value loss: 0.334391. Entropy: 0.964616.\n",
      "Iteration 7518: Policy loss: -0.013607. Value loss: 0.169819. Entropy: 0.939816.\n",
      "episode: 3250   score: 33.0  epsilon: 1.0    steps: 576  evaluation reward: 27.29\n",
      "now time :  2019-03-06 15:08:36.504406\n",
      "episode: 3251   score: 30.0  epsilon: 1.0    steps: 1024  evaluation reward: 27.34\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7519: Policy loss: 0.008028. Value loss: 1.121374. Entropy: 0.955876.\n",
      "Iteration 7520: Policy loss: -0.001140. Value loss: 0.380767. Entropy: 0.939230.\n",
      "Iteration 7521: Policy loss: -0.010076. Value loss: 0.184042. Entropy: 0.949084.\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7522: Policy loss: 0.017891. Value loss: 0.687906. Entropy: 0.957919.\n",
      "Iteration 7523: Policy loss: -0.004868. Value loss: 0.266795. Entropy: 0.964705.\n",
      "Iteration 7524: Policy loss: -0.012463. Value loss: 0.143961. Entropy: 0.973513.\n",
      "episode: 3252   score: 44.0  epsilon: 1.0    steps: 136  evaluation reward: 27.53\n",
      "episode: 3253   score: 23.0  epsilon: 1.0    steps: 512  evaluation reward: 27.49\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7525: Policy loss: 0.011875. Value loss: 0.503988. Entropy: 0.990099.\n",
      "Iteration 7526: Policy loss: 0.000238. Value loss: 0.200933. Entropy: 0.991543.\n",
      "Iteration 7527: Policy loss: -0.007494. Value loss: 0.098609. Entropy: 0.998485.\n",
      "episode: 3254   score: 43.0  epsilon: 1.0    steps: 712  evaluation reward: 27.51\n",
      "episode: 3255   score: 35.0  epsilon: 1.0    steps: 944  evaluation reward: 27.47\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7528: Policy loss: 0.019077. Value loss: 0.966872. Entropy: 0.941797.\n",
      "Iteration 7529: Policy loss: 0.006716. Value loss: 0.322564. Entropy: 0.935216.\n",
      "Iteration 7530: Policy loss: -0.005890. Value loss: 0.176755. Entropy: 0.922377.\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7531: Policy loss: 0.003582. Value loss: 0.573132. Entropy: 0.980960.\n",
      "Iteration 7532: Policy loss: -0.002264. Value loss: 0.244618. Entropy: 0.985280.\n",
      "Iteration 7533: Policy loss: -0.011104. Value loss: 0.146663. Entropy: 0.978288.\n",
      "episode: 3256   score: 32.0  epsilon: 1.0    steps: 304  evaluation reward: 27.53\n",
      "episode: 3257   score: 46.0  epsilon: 1.0    steps: 312  evaluation reward: 27.74\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7534: Policy loss: 0.015958. Value loss: 0.717961. Entropy: 1.012405.\n",
      "Iteration 7535: Policy loss: -0.002095. Value loss: 0.295903. Entropy: 0.980973.\n",
      "Iteration 7536: Policy loss: -0.013614. Value loss: 0.151582. Entropy: 0.978830.\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7537: Policy loss: 0.009490. Value loss: 0.486165. Entropy: 1.060650.\n",
      "Iteration 7538: Policy loss: -0.003335. Value loss: 0.233053. Entropy: 1.051158.\n",
      "Iteration 7539: Policy loss: -0.021955. Value loss: 0.146014. Entropy: 1.037710.\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7540: Policy loss: 0.012018. Value loss: 0.799288. Entropy: 1.032666.\n",
      "Iteration 7541: Policy loss: -0.005844. Value loss: 0.346202. Entropy: 1.027529.\n",
      "Iteration 7542: Policy loss: -0.013613. Value loss: 0.205810. Entropy: 1.025949.\n",
      "episode: 3258   score: 22.0  epsilon: 1.0    steps: 296  evaluation reward: 27.55\n",
      "episode: 3259   score: 19.0  epsilon: 1.0    steps: 480  evaluation reward: 27.34\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7543: Policy loss: 0.015359. Value loss: 0.715453. Entropy: 1.022356.\n",
      "Iteration 7544: Policy loss: -0.000126. Value loss: 0.317043. Entropy: 1.013971.\n",
      "Iteration 7545: Policy loss: -0.011978. Value loss: 0.159939. Entropy: 1.004635.\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7546: Policy loss: 0.008653. Value loss: 0.484751. Entropy: 0.973215.\n",
      "Iteration 7547: Policy loss: -0.007059. Value loss: 0.257500. Entropy: 0.959790.\n",
      "Iteration 7548: Policy loss: -0.015662. Value loss: 0.151653. Entropy: 0.956275.\n",
      "episode: 3260   score: 26.0  epsilon: 1.0    steps: 840  evaluation reward: 27.21\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7549: Policy loss: 0.006790. Value loss: 0.784950. Entropy: 0.968032.\n",
      "Iteration 7550: Policy loss: 0.001324. Value loss: 0.339504. Entropy: 0.952432.\n",
      "Iteration 7551: Policy loss: -0.014176. Value loss: 0.202993. Entropy: 0.950219.\n",
      "episode: 3261   score: 28.0  epsilon: 1.0    steps: 776  evaluation reward: 27.16\n",
      "episode: 3262   score: 18.0  epsilon: 1.0    steps: 864  evaluation reward: 27.19\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7552: Policy loss: 0.006198. Value loss: 0.746297. Entropy: 0.971967.\n",
      "Iteration 7553: Policy loss: -0.009286. Value loss: 0.339515. Entropy: 0.983168.\n",
      "Iteration 7554: Policy loss: -0.015345. Value loss: 0.168367. Entropy: 0.972541.\n",
      "episode: 3263   score: 22.0  epsilon: 1.0    steps: 592  evaluation reward: 27.05\n",
      "episode: 3264   score: 18.0  epsilon: 1.0    steps: 792  evaluation reward: 27.02\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7555: Policy loss: 0.014018. Value loss: 0.850423. Entropy: 0.974054.\n",
      "Iteration 7556: Policy loss: 0.004709. Value loss: 0.353231. Entropy: 0.976770.\n",
      "Iteration 7557: Policy loss: -0.011805. Value loss: 0.193955. Entropy: 0.961214.\n",
      "episode: 3265   score: 31.0  epsilon: 1.0    steps: 208  evaluation reward: 27.18\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7558: Policy loss: 0.003172. Value loss: 0.464483. Entropy: 0.953870.\n",
      "Iteration 7559: Policy loss: -0.006891. Value loss: 0.165289. Entropy: 0.952773.\n",
      "Iteration 7560: Policy loss: -0.014617. Value loss: 0.090148. Entropy: 0.955781.\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7561: Policy loss: 0.005045. Value loss: 0.327986. Entropy: 0.925547.\n",
      "Iteration 7562: Policy loss: -0.004393. Value loss: 0.173391. Entropy: 0.943076.\n",
      "Iteration 7563: Policy loss: -0.013238. Value loss: 0.103844. Entropy: 0.918780.\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7564: Policy loss: 0.004981. Value loss: 0.591798. Entropy: 0.959080.\n",
      "Iteration 7565: Policy loss: -0.006009. Value loss: 0.277724. Entropy: 0.948358.\n",
      "Iteration 7566: Policy loss: -0.017119. Value loss: 0.179441. Entropy: 0.949923.\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7567: Policy loss: 0.007767. Value loss: 0.597676. Entropy: 0.919403.\n",
      "Iteration 7568: Policy loss: 0.002888. Value loss: 0.224288. Entropy: 0.916478.\n",
      "Iteration 7569: Policy loss: -0.014939. Value loss: 0.116767. Entropy: 0.919487.\n",
      "episode: 3266   score: 27.0  epsilon: 1.0    steps: 440  evaluation reward: 27.31\n",
      "episode: 3267   score: 26.0  epsilon: 1.0    steps: 624  evaluation reward: 27.28\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7570: Policy loss: 0.012855. Value loss: 0.348891. Entropy: 1.019393.\n",
      "Iteration 7571: Policy loss: -0.004475. Value loss: 0.131905. Entropy: 1.025947.\n",
      "Iteration 7572: Policy loss: -0.018300. Value loss: 0.106478. Entropy: 1.015392.\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7573: Policy loss: 0.007176. Value loss: 0.389097. Entropy: 0.945698.\n",
      "Iteration 7574: Policy loss: -0.007407. Value loss: 0.166990. Entropy: 0.953144.\n",
      "Iteration 7575: Policy loss: -0.015367. Value loss: 0.102271. Entropy: 0.946432.\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7576: Policy loss: 0.013529. Value loss: 0.775958. Entropy: 0.942722.\n",
      "Iteration 7577: Policy loss: -0.004140. Value loss: 0.376454. Entropy: 0.905577.\n",
      "Iteration 7578: Policy loss: -0.012930. Value loss: 0.194494. Entropy: 0.915896.\n",
      "episode: 3268   score: 18.0  epsilon: 1.0    steps: 64  evaluation reward: 27.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3269   score: 20.0  epsilon: 1.0    steps: 136  evaluation reward: 27.06\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7579: Policy loss: 0.021222. Value loss: 0.878411. Entropy: 0.904873.\n",
      "Iteration 7580: Policy loss: 0.002831. Value loss: 0.366575. Entropy: 0.889865.\n",
      "Iteration 7581: Policy loss: -0.010597. Value loss: 0.235258. Entropy: 0.873999.\n",
      "episode: 3270   score: 23.0  epsilon: 1.0    steps: 96  evaluation reward: 26.86\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7582: Policy loss: 0.016038. Value loss: 0.405134. Entropy: 0.890160.\n",
      "Iteration 7583: Policy loss: -0.001059. Value loss: 0.171254. Entropy: 0.882052.\n",
      "Iteration 7584: Policy loss: -0.017292. Value loss: 0.086151. Entropy: 0.877582.\n",
      "episode: 3271   score: 30.0  epsilon: 1.0    steps: 368  evaluation reward: 26.91\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7585: Policy loss: 0.013415. Value loss: 1.045883. Entropy: 0.881531.\n",
      "Iteration 7586: Policy loss: 0.005123. Value loss: 0.459662. Entropy: 0.882510.\n",
      "Iteration 7587: Policy loss: -0.004062. Value loss: 0.249152. Entropy: 0.864328.\n",
      "episode: 3272   score: 23.0  epsilon: 1.0    steps: 464  evaluation reward: 26.78\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7588: Policy loss: 0.013104. Value loss: 0.590771. Entropy: 0.979565.\n",
      "Iteration 7589: Policy loss: -0.002546. Value loss: 0.281552. Entropy: 0.966259.\n",
      "Iteration 7590: Policy loss: -0.012518. Value loss: 0.156346. Entropy: 0.958597.\n",
      "episode: 3273   score: 51.0  epsilon: 1.0    steps: 496  evaluation reward: 27.14\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7591: Policy loss: 0.016931. Value loss: 0.431701. Entropy: 1.002909.\n",
      "Iteration 7592: Policy loss: -0.000418. Value loss: 0.204003. Entropy: 0.985124.\n",
      "Iteration 7593: Policy loss: -0.007932. Value loss: 0.133307. Entropy: 0.992267.\n",
      "episode: 3274   score: 20.0  epsilon: 1.0    steps: 680  evaluation reward: 27.09\n",
      "episode: 3275   score: 11.0  epsilon: 1.0    steps: 840  evaluation reward: 27.05\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7594: Policy loss: 0.013218. Value loss: 0.645865. Entropy: 0.935565.\n",
      "Iteration 7595: Policy loss: 0.006680. Value loss: 0.247694. Entropy: 0.932799.\n",
      "Iteration 7596: Policy loss: -0.013052. Value loss: 0.109605. Entropy: 0.919543.\n",
      "episode: 3276   score: 44.0  epsilon: 1.0    steps: 88  evaluation reward: 27.31\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7597: Policy loss: 0.005945. Value loss: 0.337348. Entropy: 1.004787.\n",
      "Iteration 7598: Policy loss: -0.010298. Value loss: 0.140124. Entropy: 0.990706.\n",
      "Iteration 7599: Policy loss: -0.020674. Value loss: 0.089000. Entropy: 1.002602.\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7600: Policy loss: 0.007260. Value loss: 0.507836. Entropy: 0.941713.\n",
      "Iteration 7601: Policy loss: -0.013551. Value loss: 0.207850. Entropy: 0.931057.\n",
      "Iteration 7602: Policy loss: -0.023310. Value loss: 0.116487. Entropy: 0.933690.\n",
      "episode: 3277   score: 15.0  epsilon: 1.0    steps: 168  evaluation reward: 27.21\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7603: Policy loss: 0.010926. Value loss: 0.731400. Entropy: 0.973912.\n",
      "Iteration 7604: Policy loss: -0.000159. Value loss: 0.294610. Entropy: 0.972872.\n",
      "Iteration 7605: Policy loss: -0.011762. Value loss: 0.187464. Entropy: 0.987173.\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7606: Policy loss: 0.018707. Value loss: 0.756209. Entropy: 1.028870.\n",
      "Iteration 7607: Policy loss: -0.001160. Value loss: 0.345495. Entropy: 1.035764.\n",
      "Iteration 7608: Policy loss: -0.013174. Value loss: 0.215509. Entropy: 1.016378.\n",
      "episode: 3278   score: 32.0  epsilon: 1.0    steps: 968  evaluation reward: 27.24\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7609: Policy loss: 0.023116. Value loss: 0.711006. Entropy: 0.989959.\n",
      "Iteration 7610: Policy loss: -0.000198. Value loss: 0.300859. Entropy: 0.972953.\n",
      "Iteration 7611: Policy loss: -0.012444. Value loss: 0.168571. Entropy: 0.963753.\n",
      "episode: 3279   score: 31.0  epsilon: 1.0    steps: 8  evaluation reward: 27.29\n",
      "episode: 3280   score: 19.0  epsilon: 1.0    steps: 448  evaluation reward: 27.18\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7612: Policy loss: 0.006183. Value loss: 0.732896. Entropy: 0.982531.\n",
      "Iteration 7613: Policy loss: -0.002460. Value loss: 0.262173. Entropy: 0.975640.\n",
      "Iteration 7614: Policy loss: -0.011353. Value loss: 0.123456. Entropy: 0.984415.\n",
      "episode: 3281   score: 25.0  epsilon: 1.0    steps: 176  evaluation reward: 27.07\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7615: Policy loss: 0.010135. Value loss: 0.340521. Entropy: 0.959626.\n",
      "Iteration 7616: Policy loss: -0.006344. Value loss: 0.169368. Entropy: 0.951400.\n",
      "Iteration 7617: Policy loss: -0.014308. Value loss: 0.105454. Entropy: 0.951933.\n",
      "episode: 3282   score: 17.0  epsilon: 1.0    steps: 384  evaluation reward: 26.98\n",
      "episode: 3283   score: 24.0  epsilon: 1.0    steps: 896  evaluation reward: 26.97\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7618: Policy loss: 0.010975. Value loss: 0.536046. Entropy: 1.006754.\n",
      "Iteration 7619: Policy loss: -0.004860. Value loss: 0.216106. Entropy: 1.015707.\n",
      "Iteration 7620: Policy loss: -0.011483. Value loss: 0.145377. Entropy: 1.012315.\n",
      "episode: 3284   score: 26.0  epsilon: 1.0    steps: 840  evaluation reward: 26.93\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7621: Policy loss: 0.009800. Value loss: 0.304694. Entropy: 0.941848.\n",
      "Iteration 7622: Policy loss: -0.004234. Value loss: 0.117174. Entropy: 0.919833.\n",
      "Iteration 7623: Policy loss: -0.019670. Value loss: 0.067642. Entropy: 0.932671.\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7624: Policy loss: 0.011952. Value loss: 0.245776. Entropy: 0.967964.\n",
      "Iteration 7625: Policy loss: -0.006597. Value loss: 0.107710. Entropy: 0.971747.\n",
      "Iteration 7626: Policy loss: -0.019954. Value loss: 0.064042. Entropy: 0.974759.\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7627: Policy loss: 0.003813. Value loss: 0.333914. Entropy: 0.975658.\n",
      "Iteration 7628: Policy loss: -0.009722. Value loss: 0.160458. Entropy: 0.988247.\n",
      "Iteration 7629: Policy loss: -0.016670. Value loss: 0.097387. Entropy: 0.975439.\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7630: Policy loss: 0.008267. Value loss: 0.451209. Entropy: 1.002069.\n",
      "Iteration 7631: Policy loss: -0.003918. Value loss: 0.193850. Entropy: 0.991082.\n",
      "Iteration 7632: Policy loss: -0.012212. Value loss: 0.090373. Entropy: 0.992791.\n",
      "episode: 3285   score: 26.0  epsilon: 1.0    steps: 472  evaluation reward: 26.9\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7633: Policy loss: 0.012961. Value loss: 0.593742. Entropy: 1.014761.\n",
      "Iteration 7634: Policy loss: -0.004095. Value loss: 0.224901. Entropy: 0.995322.\n",
      "Iteration 7635: Policy loss: -0.012057. Value loss: 0.111537. Entropy: 0.999216.\n",
      "episode: 3286   score: 29.0  epsilon: 1.0    steps: 480  evaluation reward: 26.83\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7636: Policy loss: 0.006776. Value loss: 0.671824. Entropy: 0.974007.\n",
      "Iteration 7637: Policy loss: -0.007726. Value loss: 0.266592. Entropy: 0.950342.\n",
      "Iteration 7638: Policy loss: -0.014365. Value loss: 0.145702. Entropy: 0.953105.\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7639: Policy loss: 0.011088. Value loss: 0.455313. Entropy: 1.008896.\n",
      "Iteration 7640: Policy loss: -0.004318. Value loss: 0.199094. Entropy: 1.022370.\n",
      "Iteration 7641: Policy loss: -0.020405. Value loss: 0.103472. Entropy: 1.011436.\n",
      "episode: 3287   score: 33.0  epsilon: 1.0    steps: 1016  evaluation reward: 26.93\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7642: Policy loss: 0.010337. Value loss: 0.756385. Entropy: 0.992674.\n",
      "Iteration 7643: Policy loss: -0.001413. Value loss: 0.280601. Entropy: 0.993638.\n",
      "Iteration 7644: Policy loss: -0.012462. Value loss: 0.137495. Entropy: 0.995630.\n",
      "episode: 3288   score: 17.0  epsilon: 1.0    steps: 48  evaluation reward: 26.8\n",
      "episode: 3289   score: 26.0  epsilon: 1.0    steps: 152  evaluation reward: 26.88\n",
      "episode: 3290   score: 27.0  epsilon: 1.0    steps: 232  evaluation reward: 26.92\n",
      "Training network. lr: 0.000192. clip: 0.076656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7645: Policy loss: 0.012658. Value loss: 0.651407. Entropy: 0.974486.\n",
      "Iteration 7646: Policy loss: -0.006723. Value loss: 0.283878. Entropy: 0.957563.\n",
      "Iteration 7647: Policy loss: -0.014349. Value loss: 0.172496. Entropy: 0.963912.\n",
      "episode: 3291   score: 38.0  epsilon: 1.0    steps: 224  evaluation reward: 27.18\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7648: Policy loss: 0.014843. Value loss: 0.709767. Entropy: 0.941790.\n",
      "Iteration 7649: Policy loss: -0.002079. Value loss: 0.267318. Entropy: 0.921286.\n",
      "Iteration 7650: Policy loss: -0.008686. Value loss: 0.181214. Entropy: 0.930807.\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7651: Policy loss: 0.011083. Value loss: 0.592891. Entropy: 0.961962.\n",
      "Iteration 7652: Policy loss: -0.002949. Value loss: 0.198843. Entropy: 0.955007.\n",
      "Iteration 7653: Policy loss: -0.015518. Value loss: 0.100399. Entropy: 0.976261.\n",
      "episode: 3292   score: 45.0  epsilon: 1.0    steps: 432  evaluation reward: 27.38\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7654: Policy loss: 0.012146. Value loss: 0.324733. Entropy: 0.985574.\n",
      "Iteration 7655: Policy loss: -0.006654. Value loss: 0.152588. Entropy: 0.989910.\n",
      "Iteration 7656: Policy loss: -0.018731. Value loss: 0.086367. Entropy: 0.982345.\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7657: Policy loss: 0.005928. Value loss: 0.394376. Entropy: 0.997044.\n",
      "Iteration 7658: Policy loss: -0.003767. Value loss: 0.135143. Entropy: 1.024414.\n",
      "Iteration 7659: Policy loss: -0.016870. Value loss: 0.081573. Entropy: 1.013467.\n",
      "episode: 3293   score: 24.0  epsilon: 1.0    steps: 808  evaluation reward: 27.38\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7660: Policy loss: 0.014084. Value loss: 1.072491. Entropy: 0.993723.\n",
      "Iteration 7661: Policy loss: -0.007644. Value loss: 0.694881. Entropy: 0.966981.\n",
      "Iteration 7662: Policy loss: -0.014586. Value loss: 0.510821. Entropy: 0.971736.\n",
      "episode: 3294   score: 11.0  epsilon: 1.0    steps: 264  evaluation reward: 27.11\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7663: Policy loss: 0.021536. Value loss: 0.813951. Entropy: 1.018239.\n",
      "Iteration 7664: Policy loss: 0.003612. Value loss: 0.304779. Entropy: 1.010092.\n",
      "Iteration 7665: Policy loss: -0.012461. Value loss: 0.133081. Entropy: 1.018832.\n",
      "episode: 3295   score: 18.0  epsilon: 1.0    steps: 32  evaluation reward: 27.08\n",
      "episode: 3296   score: 38.0  epsilon: 1.0    steps: 960  evaluation reward: 27.1\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7666: Policy loss: 0.015799. Value loss: 0.711249. Entropy: 0.946850.\n",
      "Iteration 7667: Policy loss: 0.000259. Value loss: 0.270327. Entropy: 0.951092.\n",
      "Iteration 7668: Policy loss: -0.007069. Value loss: 0.133233. Entropy: 0.948745.\n",
      "episode: 3297   score: 29.0  epsilon: 1.0    steps: 648  evaluation reward: 27.07\n",
      "episode: 3298   score: 28.0  epsilon: 1.0    steps: 880  evaluation reward: 27.07\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7669: Policy loss: 0.010101. Value loss: 0.388900. Entropy: 1.018407.\n",
      "Iteration 7670: Policy loss: -0.008083. Value loss: 0.198995. Entropy: 1.024003.\n",
      "Iteration 7671: Policy loss: -0.014310. Value loss: 0.119848. Entropy: 1.003776.\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7672: Policy loss: 0.011875. Value loss: 0.397145. Entropy: 0.984049.\n",
      "Iteration 7673: Policy loss: -0.011365. Value loss: 0.179138. Entropy: 0.984072.\n",
      "Iteration 7674: Policy loss: -0.017778. Value loss: 0.106274. Entropy: 0.979046.\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7675: Policy loss: 0.007148. Value loss: 0.555041. Entropy: 1.025332.\n",
      "Iteration 7676: Policy loss: -0.002122. Value loss: 0.232227. Entropy: 1.017172.\n",
      "Iteration 7677: Policy loss: -0.009484. Value loss: 0.131684. Entropy: 1.019194.\n",
      "episode: 3299   score: 45.0  epsilon: 1.0    steps: 696  evaluation reward: 27.22\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7678: Policy loss: 0.008410. Value loss: 0.708744. Entropy: 1.053610.\n",
      "Iteration 7679: Policy loss: 0.002329. Value loss: 0.324470. Entropy: 1.053245.\n",
      "Iteration 7680: Policy loss: -0.010051. Value loss: 0.184855. Entropy: 1.061292.\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7681: Policy loss: 0.010229. Value loss: 0.485364. Entropy: 1.038888.\n",
      "Iteration 7682: Policy loss: -0.010901. Value loss: 0.237407. Entropy: 1.032398.\n",
      "Iteration 7683: Policy loss: -0.014942. Value loss: 0.139176. Entropy: 1.028544.\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7684: Policy loss: 0.009537. Value loss: 0.493745. Entropy: 1.036220.\n",
      "Iteration 7685: Policy loss: -0.008898. Value loss: 0.233354. Entropy: 1.046469.\n",
      "Iteration 7686: Policy loss: -0.016349. Value loss: 0.151590. Entropy: 1.038506.\n",
      "episode: 3300   score: 26.0  epsilon: 1.0    steps: 568  evaluation reward: 27.17\n",
      "now time :  2019-03-06 15:12:08.659168\n",
      "episode: 3301   score: 19.0  epsilon: 1.0    steps: 760  evaluation reward: 27.08\n",
      "episode: 3302   score: 15.0  epsilon: 1.0    steps: 872  evaluation reward: 26.97\n",
      "episode: 3303   score: 18.0  epsilon: 1.0    steps: 912  evaluation reward: 26.97\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7687: Policy loss: 0.021286. Value loss: 1.192246. Entropy: 0.968765.\n",
      "Iteration 7688: Policy loss: 0.006450. Value loss: 0.466462. Entropy: 0.956549.\n",
      "Iteration 7689: Policy loss: -0.008802. Value loss: 0.224930. Entropy: 0.949117.\n",
      "episode: 3304   score: 32.0  epsilon: 1.0    steps: 152  evaluation reward: 27.07\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7690: Policy loss: 0.011292. Value loss: 0.475147. Entropy: 0.978281.\n",
      "Iteration 7691: Policy loss: -0.007035. Value loss: 0.245962. Entropy: 0.978682.\n",
      "Iteration 7692: Policy loss: -0.017821. Value loss: 0.151659. Entropy: 0.966579.\n",
      "episode: 3305   score: 31.0  epsilon: 1.0    steps: 408  evaluation reward: 26.98\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7693: Policy loss: 0.009354. Value loss: 0.348463. Entropy: 1.031178.\n",
      "Iteration 7694: Policy loss: -0.004554. Value loss: 0.189233. Entropy: 1.024005.\n",
      "Iteration 7695: Policy loss: -0.015118. Value loss: 0.123971. Entropy: 1.016853.\n",
      "episode: 3306   score: 20.0  epsilon: 1.0    steps: 688  evaluation reward: 26.89\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7696: Policy loss: 0.011176. Value loss: 0.470628. Entropy: 0.929538.\n",
      "Iteration 7697: Policy loss: -0.008243. Value loss: 0.182893. Entropy: 0.916597.\n",
      "Iteration 7698: Policy loss: -0.013794. Value loss: 0.094178. Entropy: 0.926472.\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7699: Policy loss: 0.008170. Value loss: 0.463254. Entropy: 1.002647.\n",
      "Iteration 7700: Policy loss: -0.004386. Value loss: 0.243845. Entropy: 0.986726.\n",
      "Iteration 7701: Policy loss: -0.015886. Value loss: 0.165870. Entropy: 0.988731.\n",
      "episode: 3307   score: 10.0  epsilon: 1.0    steps: 464  evaluation reward: 26.63\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7702: Policy loss: 0.009492. Value loss: 0.546019. Entropy: 0.948656.\n",
      "Iteration 7703: Policy loss: 0.000123. Value loss: 0.201902. Entropy: 0.938998.\n",
      "Iteration 7704: Policy loss: -0.009683. Value loss: 0.098168. Entropy: 0.932796.\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7705: Policy loss: 0.006213. Value loss: 0.515086. Entropy: 0.981333.\n",
      "Iteration 7706: Policy loss: -0.002696. Value loss: 0.190790. Entropy: 0.962842.\n",
      "Iteration 7707: Policy loss: -0.014261. Value loss: 0.116004. Entropy: 0.961524.\n",
      "episode: 3308   score: 25.0  epsilon: 1.0    steps: 288  evaluation reward: 26.66\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7708: Policy loss: 0.008177. Value loss: 0.472782. Entropy: 0.904621.\n",
      "Iteration 7709: Policy loss: -0.002718. Value loss: 0.192824. Entropy: 0.890269.\n",
      "Iteration 7710: Policy loss: -0.014664. Value loss: 0.118010. Entropy: 0.884922.\n",
      "episode: 3309   score: 20.0  epsilon: 1.0    steps: 720  evaluation reward: 26.44\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7711: Policy loss: 0.012831. Value loss: 0.839271. Entropy: 0.936612.\n",
      "Iteration 7712: Policy loss: 0.000938. Value loss: 0.320027. Entropy: 0.946190.\n",
      "Iteration 7713: Policy loss: -0.013167. Value loss: 0.193382. Entropy: 0.933205.\n",
      "episode: 3310   score: 15.0  epsilon: 1.0    steps: 32  evaluation reward: 26.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7714: Policy loss: 0.005834. Value loss: 0.653591. Entropy: 0.915352.\n",
      "Iteration 7715: Policy loss: -0.008386. Value loss: 0.314990. Entropy: 0.905564.\n",
      "Iteration 7716: Policy loss: -0.019305. Value loss: 0.141505. Entropy: 0.911882.\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7717: Policy loss: 0.009291. Value loss: 0.617164. Entropy: 0.966169.\n",
      "Iteration 7718: Policy loss: -0.008040. Value loss: 0.209528. Entropy: 0.963104.\n",
      "Iteration 7719: Policy loss: -0.021334. Value loss: 0.126992. Entropy: 0.952910.\n",
      "episode: 3311   score: 34.0  epsilon: 1.0    steps: 152  evaluation reward: 26.3\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7720: Policy loss: 0.011758. Value loss: 0.695973. Entropy: 0.920116.\n",
      "Iteration 7721: Policy loss: 0.004843. Value loss: 0.253831. Entropy: 0.916440.\n",
      "Iteration 7722: Policy loss: -0.006705. Value loss: 0.131657. Entropy: 0.930965.\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7723: Policy loss: 0.011780. Value loss: 0.967264. Entropy: 0.934652.\n",
      "Iteration 7724: Policy loss: -0.003013. Value loss: 0.531468. Entropy: 0.941497.\n",
      "Iteration 7725: Policy loss: -0.011688. Value loss: 0.313514. Entropy: 0.931910.\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7726: Policy loss: 0.018746. Value loss: 0.925745. Entropy: 0.896163.\n",
      "Iteration 7727: Policy loss: 0.001764. Value loss: 0.354216. Entropy: 0.901462.\n",
      "Iteration 7728: Policy loss: -0.012988. Value loss: 0.165681. Entropy: 0.894783.\n",
      "episode: 3312   score: 50.0  epsilon: 1.0    steps: 48  evaluation reward: 26.65\n",
      "episode: 3313   score: 46.0  epsilon: 1.0    steps: 152  evaluation reward: 26.81\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7729: Policy loss: 0.009618. Value loss: 0.789386. Entropy: 0.942257.\n",
      "Iteration 7730: Policy loss: -0.003061. Value loss: 0.341402. Entropy: 0.937597.\n",
      "Iteration 7731: Policy loss: -0.014234. Value loss: 0.209288. Entropy: 0.934603.\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7732: Policy loss: 0.014426. Value loss: 0.725970. Entropy: 0.953488.\n",
      "Iteration 7733: Policy loss: 0.004034. Value loss: 0.258225. Entropy: 0.966196.\n",
      "Iteration 7734: Policy loss: -0.013058. Value loss: 0.101750. Entropy: 0.961377.\n",
      "episode: 3314   score: 43.0  epsilon: 1.0    steps: 472  evaluation reward: 27.0\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7735: Policy loss: 0.009335. Value loss: 0.959641. Entropy: 0.931027.\n",
      "Iteration 7736: Policy loss: -0.005301. Value loss: 0.424187. Entropy: 0.927050.\n",
      "Iteration 7737: Policy loss: -0.012498. Value loss: 0.261967. Entropy: 0.918611.\n",
      "episode: 3315   score: 35.0  epsilon: 1.0    steps: 296  evaluation reward: 27.24\n",
      "episode: 3316   score: 50.0  epsilon: 1.0    steps: 376  evaluation reward: 27.44\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7738: Policy loss: 0.016740. Value loss: 0.304665. Entropy: 0.981619.\n",
      "Iteration 7739: Policy loss: -0.009551. Value loss: 0.107561. Entropy: 0.960984.\n",
      "Iteration 7740: Policy loss: -0.017867. Value loss: 0.074219. Entropy: 0.959093.\n",
      "episode: 3317   score: 17.0  epsilon: 1.0    steps: 504  evaluation reward: 27.29\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7741: Policy loss: 0.008993. Value loss: 0.544316. Entropy: 0.864549.\n",
      "Iteration 7742: Policy loss: -0.007208. Value loss: 0.240698. Entropy: 0.858959.\n",
      "Iteration 7743: Policy loss: -0.015897. Value loss: 0.136915. Entropy: 0.852103.\n",
      "episode: 3318   score: 40.0  epsilon: 1.0    steps: 280  evaluation reward: 27.56\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7744: Policy loss: 0.006242. Value loss: 0.889849. Entropy: 0.915772.\n",
      "Iteration 7745: Policy loss: -0.012599. Value loss: 0.387343. Entropy: 0.896787.\n",
      "Iteration 7746: Policy loss: -0.019646. Value loss: 0.219252. Entropy: 0.894888.\n",
      "episode: 3319   score: 13.0  epsilon: 1.0    steps: 264  evaluation reward: 27.46\n",
      "episode: 3320   score: 42.0  epsilon: 1.0    steps: 920  evaluation reward: 27.67\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7747: Policy loss: 0.011232. Value loss: 0.878460. Entropy: 0.886443.\n",
      "Iteration 7748: Policy loss: -0.007717. Value loss: 0.356755. Entropy: 0.882435.\n",
      "Iteration 7749: Policy loss: -0.011652. Value loss: 0.178155. Entropy: 0.880910.\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7750: Policy loss: 0.012567. Value loss: 0.230855. Entropy: 0.858491.\n",
      "Iteration 7751: Policy loss: -0.008230. Value loss: 0.085854. Entropy: 0.862892.\n",
      "Iteration 7752: Policy loss: -0.022547. Value loss: 0.046779. Entropy: 0.853556.\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7753: Policy loss: 0.007281. Value loss: 0.731492. Entropy: 0.874860.\n",
      "Iteration 7754: Policy loss: -0.006766. Value loss: 0.323278. Entropy: 0.853355.\n",
      "Iteration 7755: Policy loss: -0.016712. Value loss: 0.197237. Entropy: 0.874652.\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7756: Policy loss: 0.006992. Value loss: 0.753955. Entropy: 0.960924.\n",
      "Iteration 7757: Policy loss: -0.008589. Value loss: 0.291337. Entropy: 0.952255.\n",
      "Iteration 7758: Policy loss: -0.015293. Value loss: 0.171180. Entropy: 0.961163.\n",
      "episode: 3321   score: 34.0  epsilon: 1.0    steps: 408  evaluation reward: 27.82\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7759: Policy loss: 0.010878. Value loss: 0.496198. Entropy: 0.975326.\n",
      "Iteration 7760: Policy loss: -0.010374. Value loss: 0.221768. Entropy: 0.976635.\n",
      "Iteration 7761: Policy loss: -0.018096. Value loss: 0.116861. Entropy: 0.986020.\n",
      "episode: 3322   score: 31.0  epsilon: 1.0    steps: 616  evaluation reward: 27.88\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7762: Policy loss: 0.008718. Value loss: 0.764361. Entropy: 0.925530.\n",
      "Iteration 7763: Policy loss: 0.000177. Value loss: 0.289936. Entropy: 0.903953.\n",
      "Iteration 7764: Policy loss: -0.009380. Value loss: 0.120710. Entropy: 0.897193.\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7765: Policy loss: 0.009686. Value loss: 0.666489. Entropy: 0.944492.\n",
      "Iteration 7766: Policy loss: -0.001181. Value loss: 0.275110. Entropy: 0.947177.\n",
      "Iteration 7767: Policy loss: -0.015708. Value loss: 0.119465. Entropy: 0.948777.\n",
      "episode: 3323   score: 28.0  epsilon: 1.0    steps: 488  evaluation reward: 27.82\n",
      "episode: 3324   score: 32.0  epsilon: 1.0    steps: 824  evaluation reward: 27.96\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7768: Policy loss: 0.014743. Value loss: 1.142352. Entropy: 0.942745.\n",
      "Iteration 7769: Policy loss: -0.002249. Value loss: 0.486737. Entropy: 0.927834.\n",
      "Iteration 7770: Policy loss: -0.016687. Value loss: 0.255022. Entropy: 0.929916.\n",
      "episode: 3325   score: 37.0  epsilon: 1.0    steps: 472  evaluation reward: 27.99\n",
      "episode: 3326   score: 27.0  epsilon: 1.0    steps: 512  evaluation reward: 28.02\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7771: Policy loss: 0.017175. Value loss: 0.387256. Entropy: 0.872174.\n",
      "Iteration 7772: Policy loss: -0.002484. Value loss: 0.135145. Entropy: 0.894098.\n",
      "Iteration 7773: Policy loss: -0.013468. Value loss: 0.078874. Entropy: 0.883053.\n",
      "episode: 3327   score: 27.0  epsilon: 1.0    steps: 480  evaluation reward: 27.82\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7774: Policy loss: 0.015517. Value loss: 0.996016. Entropy: 0.955720.\n",
      "Iteration 7775: Policy loss: 0.001063. Value loss: 0.519080. Entropy: 0.944221.\n",
      "Iteration 7776: Policy loss: -0.012328. Value loss: 0.298170. Entropy: 0.938842.\n",
      "episode: 3328   score: 33.0  epsilon: 1.0    steps: 432  evaluation reward: 27.87\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7777: Policy loss: 0.008961. Value loss: 0.786027. Entropy: 0.923422.\n",
      "Iteration 7778: Policy loss: -0.002560. Value loss: 0.373857. Entropy: 0.914512.\n",
      "Iteration 7779: Policy loss: -0.010956. Value loss: 0.255774. Entropy: 0.918006.\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7780: Policy loss: 0.007647. Value loss: 0.413137. Entropy: 0.968414.\n",
      "Iteration 7781: Policy loss: -0.008554. Value loss: 0.191666. Entropy: 0.963463.\n",
      "Iteration 7782: Policy loss: -0.019956. Value loss: 0.130424. Entropy: 0.964599.\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7783: Policy loss: 0.012795. Value loss: 0.634052. Entropy: 0.967462.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7784: Policy loss: -0.004126. Value loss: 0.261124. Entropy: 0.957848.\n",
      "Iteration 7785: Policy loss: -0.012336. Value loss: 0.138541. Entropy: 0.958963.\n",
      "episode: 3329   score: 28.0  epsilon: 1.0    steps: 336  evaluation reward: 27.93\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7786: Policy loss: 0.009472. Value loss: 0.511941. Entropy: 1.036773.\n",
      "Iteration 7787: Policy loss: -0.000491. Value loss: 0.272462. Entropy: 1.037987.\n",
      "Iteration 7788: Policy loss: -0.007970. Value loss: 0.152914. Entropy: 1.038887.\n",
      "episode: 3330   score: 25.0  epsilon: 1.0    steps: 472  evaluation reward: 27.89\n",
      "episode: 3331   score: 23.0  epsilon: 1.0    steps: 608  evaluation reward: 27.83\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7789: Policy loss: 0.012508. Value loss: 0.849396. Entropy: 0.912385.\n",
      "Iteration 7790: Policy loss: -0.007067. Value loss: 0.397186. Entropy: 0.912765.\n",
      "Iteration 7791: Policy loss: -0.012235. Value loss: 0.244565. Entropy: 0.920697.\n",
      "episode: 3332   score: 21.0  epsilon: 1.0    steps: 768  evaluation reward: 27.79\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7792: Policy loss: 0.011142. Value loss: 0.434458. Entropy: 0.859555.\n",
      "Iteration 7793: Policy loss: -0.006347. Value loss: 0.178098. Entropy: 0.869512.\n",
      "Iteration 7794: Policy loss: -0.019028. Value loss: 0.116518. Entropy: 0.861857.\n",
      "episode: 3333   score: 21.0  epsilon: 1.0    steps: 440  evaluation reward: 27.84\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7795: Policy loss: 0.009037. Value loss: 0.318116. Entropy: 0.892045.\n",
      "Iteration 7796: Policy loss: -0.006412. Value loss: 0.107551. Entropy: 0.901227.\n",
      "Iteration 7797: Policy loss: -0.015330. Value loss: 0.073009. Entropy: 0.871483.\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7798: Policy loss: 0.018019. Value loss: 0.589044. Entropy: 0.901264.\n",
      "Iteration 7799: Policy loss: 0.002988. Value loss: 0.227144. Entropy: 0.905323.\n",
      "Iteration 7800: Policy loss: -0.011404. Value loss: 0.128749. Entropy: 0.913569.\n",
      "episode: 3334   score: 24.0  epsilon: 1.0    steps: 328  evaluation reward: 27.79\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7801: Policy loss: 0.003799. Value loss: 0.569969. Entropy: 0.898078.\n",
      "Iteration 7802: Policy loss: -0.002925. Value loss: 0.247069. Entropy: 0.904787.\n",
      "Iteration 7803: Policy loss: -0.014989. Value loss: 0.128518. Entropy: 0.893011.\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7804: Policy loss: 0.009326. Value loss: 0.630756. Entropy: 0.906208.\n",
      "Iteration 7805: Policy loss: -0.002231. Value loss: 0.261869. Entropy: 0.895361.\n",
      "Iteration 7806: Policy loss: -0.013817. Value loss: 0.131957. Entropy: 0.899526.\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7807: Policy loss: 0.007815. Value loss: 0.959559. Entropy: 0.964834.\n",
      "Iteration 7808: Policy loss: -0.001585. Value loss: 0.457395. Entropy: 0.963798.\n",
      "Iteration 7809: Policy loss: -0.013436. Value loss: 0.231120. Entropy: 0.965089.\n",
      "episode: 3335   score: 48.0  epsilon: 1.0    steps: 816  evaluation reward: 28.1\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7810: Policy loss: 0.012895. Value loss: 0.823876. Entropy: 1.006295.\n",
      "Iteration 7811: Policy loss: -0.008752. Value loss: 0.305745. Entropy: 0.992791.\n",
      "Iteration 7812: Policy loss: -0.009597. Value loss: 0.184838. Entropy: 0.993017.\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7813: Policy loss: 0.007988. Value loss: 0.726617. Entropy: 0.926957.\n",
      "Iteration 7814: Policy loss: -0.002758. Value loss: 0.261319. Entropy: 0.923174.\n",
      "Iteration 7815: Policy loss: -0.014147. Value loss: 0.146443. Entropy: 0.937202.\n",
      "episode: 3336   score: 25.0  epsilon: 1.0    steps: 936  evaluation reward: 28.11\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7816: Policy loss: 0.015373. Value loss: 1.125821. Entropy: 0.994752.\n",
      "Iteration 7817: Policy loss: 0.003413. Value loss: 0.468289. Entropy: 0.982554.\n",
      "Iteration 7818: Policy loss: -0.012751. Value loss: 0.264215. Entropy: 0.983584.\n",
      "episode: 3337   score: 38.0  epsilon: 1.0    steps: 40  evaluation reward: 28.27\n",
      "episode: 3338   score: 27.0  epsilon: 1.0    steps: 616  evaluation reward: 28.34\n",
      "episode: 3339   score: 56.0  epsilon: 1.0    steps: 816  evaluation reward: 28.7\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7819: Policy loss: 0.011178. Value loss: 0.809202. Entropy: 0.923985.\n",
      "Iteration 7820: Policy loss: -0.000681. Value loss: 0.239520. Entropy: 0.918346.\n",
      "Iteration 7821: Policy loss: -0.010610. Value loss: 0.128463. Entropy: 0.918491.\n",
      "episode: 3340   score: 44.0  epsilon: 1.0    steps: 936  evaluation reward: 28.83\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7822: Policy loss: 0.022919. Value loss: 1.043786. Entropy: 0.956979.\n",
      "Iteration 7823: Policy loss: -0.000642. Value loss: 0.463665. Entropy: 0.922576.\n",
      "Iteration 7824: Policy loss: -0.010208. Value loss: 0.225046. Entropy: 0.907870.\n",
      "episode: 3341   score: 42.0  epsilon: 1.0    steps: 192  evaluation reward: 28.91\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7825: Policy loss: 0.012273. Value loss: 0.355057. Entropy: 0.911254.\n",
      "Iteration 7826: Policy loss: -0.005968. Value loss: 0.111045. Entropy: 0.901087.\n",
      "Iteration 7827: Policy loss: -0.015738. Value loss: 0.069706. Entropy: 0.881978.\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7828: Policy loss: 0.011835. Value loss: 0.733463. Entropy: 0.896869.\n",
      "Iteration 7829: Policy loss: 0.003582. Value loss: 0.382046. Entropy: 0.908744.\n",
      "Iteration 7830: Policy loss: -0.010515. Value loss: 0.209139. Entropy: 0.906977.\n",
      "episode: 3342   score: 30.0  epsilon: 1.0    steps: 48  evaluation reward: 28.92\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7831: Policy loss: 0.006420. Value loss: 0.477012. Entropy: 0.936251.\n",
      "Iteration 7832: Policy loss: -0.006320. Value loss: 0.195960. Entropy: 0.926272.\n",
      "Iteration 7833: Policy loss: -0.014007. Value loss: 0.106935. Entropy: 0.940242.\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7834: Policy loss: 0.006442. Value loss: 0.562770. Entropy: 0.897862.\n",
      "Iteration 7835: Policy loss: -0.005299. Value loss: 0.216982. Entropy: 0.903183.\n",
      "Iteration 7836: Policy loss: -0.013874. Value loss: 0.135305. Entropy: 0.901913.\n",
      "episode: 3343   score: 22.0  epsilon: 1.0    steps: 112  evaluation reward: 28.86\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7837: Policy loss: 0.016067. Value loss: 0.820631. Entropy: 1.039382.\n",
      "Iteration 7838: Policy loss: 0.000483. Value loss: 0.373332. Entropy: 1.044501.\n",
      "Iteration 7839: Policy loss: -0.006758. Value loss: 0.208838. Entropy: 1.029760.\n",
      "episode: 3344   score: 24.0  epsilon: 1.0    steps: 712  evaluation reward: 28.86\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7840: Policy loss: 0.015072. Value loss: 0.656560. Entropy: 0.999364.\n",
      "Iteration 7841: Policy loss: 0.001921. Value loss: 0.247135. Entropy: 0.974191.\n",
      "Iteration 7842: Policy loss: -0.008309. Value loss: 0.138861. Entropy: 0.972477.\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7843: Policy loss: 0.011891. Value loss: 0.709946. Entropy: 0.953758.\n",
      "Iteration 7844: Policy loss: 0.002477. Value loss: 0.344729. Entropy: 0.960307.\n",
      "Iteration 7845: Policy loss: -0.009450. Value loss: 0.193324. Entropy: 0.957749.\n",
      "episode: 3345   score: 29.0  epsilon: 1.0    steps: 688  evaluation reward: 28.98\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7846: Policy loss: 0.013958. Value loss: 0.708072. Entropy: 0.983310.\n",
      "Iteration 7847: Policy loss: -0.000524. Value loss: 0.288316. Entropy: 0.971721.\n",
      "Iteration 7848: Policy loss: -0.012436. Value loss: 0.184171. Entropy: 0.972390.\n",
      "episode: 3346   score: 39.0  epsilon: 1.0    steps: 576  evaluation reward: 29.06\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7849: Policy loss: 0.008971. Value loss: 1.367346. Entropy: 0.948862.\n",
      "Iteration 7850: Policy loss: 0.002982. Value loss: 0.631607. Entropy: 0.937482.\n",
      "Iteration 7851: Policy loss: -0.000397. Value loss: 0.394626. Entropy: 0.951376.\n",
      "episode: 3347   score: 27.0  epsilon: 1.0    steps: 8  evaluation reward: 29.05\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7852: Policy loss: 0.008519. Value loss: 0.768896. Entropy: 0.977530.\n",
      "Iteration 7853: Policy loss: -0.007981. Value loss: 0.355360. Entropy: 0.962508.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7854: Policy loss: -0.014881. Value loss: 0.207505. Entropy: 0.949221.\n",
      "episode: 3348   score: 36.0  epsilon: 1.0    steps: 424  evaluation reward: 29.08\n",
      "episode: 3349   score: 35.0  epsilon: 1.0    steps: 496  evaluation reward: 29.0\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7855: Policy loss: 0.011527. Value loss: 0.398700. Entropy: 0.935689.\n",
      "Iteration 7856: Policy loss: -0.013375. Value loss: 0.181524. Entropy: 0.938246.\n",
      "Iteration 7857: Policy loss: -0.022886. Value loss: 0.115040. Entropy: 0.943730.\n",
      "episode: 3350   score: 28.0  epsilon: 1.0    steps: 912  evaluation reward: 28.95\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7858: Policy loss: 0.016460. Value loss: 0.608081. Entropy: 0.911187.\n",
      "Iteration 7859: Policy loss: -0.006985. Value loss: 0.246726. Entropy: 0.913334.\n",
      "Iteration 7860: Policy loss: -0.011463. Value loss: 0.134620. Entropy: 0.909447.\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7861: Policy loss: 0.010241. Value loss: 0.354606. Entropy: 0.938764.\n",
      "Iteration 7862: Policy loss: -0.007750. Value loss: 0.150067. Entropy: 0.936335.\n",
      "Iteration 7863: Policy loss: -0.019464. Value loss: 0.083729. Entropy: 0.933418.\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7864: Policy loss: 0.012393. Value loss: 0.624556. Entropy: 0.873102.\n",
      "Iteration 7865: Policy loss: 0.005909. Value loss: 0.201100. Entropy: 0.855261.\n",
      "Iteration 7866: Policy loss: -0.011943. Value loss: 0.106038. Entropy: 0.862457.\n",
      "now time :  2019-03-06 15:15:59.666324\n",
      "episode: 3351   score: 37.0  epsilon: 1.0    steps: 232  evaluation reward: 29.02\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7867: Policy loss: 0.004783. Value loss: 0.733271. Entropy: 1.015100.\n",
      "Iteration 7868: Policy loss: -0.007047. Value loss: 0.314929. Entropy: 1.001016.\n",
      "Iteration 7869: Policy loss: -0.018891. Value loss: 0.190903. Entropy: 0.994749.\n",
      "episode: 3352   score: 30.0  epsilon: 1.0    steps: 512  evaluation reward: 28.88\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7870: Policy loss: 0.011795. Value loss: 0.592035. Entropy: 0.895405.\n",
      "Iteration 7871: Policy loss: 0.003341. Value loss: 0.221932. Entropy: 0.894955.\n",
      "Iteration 7872: Policy loss: -0.017269. Value loss: 0.121504. Entropy: 0.896115.\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7873: Policy loss: 0.014110. Value loss: 0.786172. Entropy: 0.939857.\n",
      "Iteration 7874: Policy loss: -0.006466. Value loss: 0.393084. Entropy: 0.929814.\n",
      "Iteration 7875: Policy loss: -0.016061. Value loss: 0.212968. Entropy: 0.915030.\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7876: Policy loss: 0.011277. Value loss: 0.870253. Entropy: 0.970246.\n",
      "Iteration 7877: Policy loss: 0.000288. Value loss: 0.363075. Entropy: 0.962915.\n",
      "Iteration 7878: Policy loss: -0.010058. Value loss: 0.192782. Entropy: 0.972168.\n",
      "episode: 3353   score: 28.0  epsilon: 1.0    steps: 88  evaluation reward: 28.93\n",
      "episode: 3354   score: 29.0  epsilon: 1.0    steps: 128  evaluation reward: 28.79\n",
      "episode: 3355   score: 18.0  epsilon: 1.0    steps: 520  evaluation reward: 28.62\n",
      "episode: 3356   score: 37.0  epsilon: 1.0    steps: 624  evaluation reward: 28.67\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7879: Policy loss: 0.003918. Value loss: 0.704726. Entropy: 0.928621.\n",
      "Iteration 7880: Policy loss: -0.005545. Value loss: 0.259277. Entropy: 0.934520.\n",
      "Iteration 7881: Policy loss: -0.010805. Value loss: 0.153042. Entropy: 0.933941.\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7882: Policy loss: 0.017889. Value loss: 0.656939. Entropy: 0.908786.\n",
      "Iteration 7883: Policy loss: 0.000396. Value loss: 0.260018. Entropy: 0.910611.\n",
      "Iteration 7884: Policy loss: -0.012929. Value loss: 0.128210. Entropy: 0.910824.\n",
      "episode: 3357   score: 26.0  epsilon: 1.0    steps: 112  evaluation reward: 28.47\n",
      "episode: 3358   score: 30.0  epsilon: 1.0    steps: 1008  evaluation reward: 28.55\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7885: Policy loss: 0.022329. Value loss: 0.456447. Entropy: 0.952299.\n",
      "Iteration 7886: Policy loss: -0.000722. Value loss: 0.198198. Entropy: 0.947703.\n",
      "Iteration 7887: Policy loss: -0.016079. Value loss: 0.115871. Entropy: 0.935527.\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7888: Policy loss: 0.009546. Value loss: 0.327009. Entropy: 0.890865.\n",
      "Iteration 7889: Policy loss: -0.009902. Value loss: 0.114726. Entropy: 0.891034.\n",
      "Iteration 7890: Policy loss: -0.022456. Value loss: 0.071367. Entropy: 0.884027.\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7891: Policy loss: 0.012996. Value loss: 0.704373. Entropy: 0.947695.\n",
      "Iteration 7892: Policy loss: -0.003200. Value loss: 0.385524. Entropy: 0.932174.\n",
      "Iteration 7893: Policy loss: -0.015783. Value loss: 0.212747. Entropy: 0.928265.\n",
      "episode: 3359   score: 26.0  epsilon: 1.0    steps: 384  evaluation reward: 28.62\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7894: Policy loss: 0.016052. Value loss: 0.545426. Entropy: 0.921117.\n",
      "Iteration 7895: Policy loss: -0.000490. Value loss: 0.200338. Entropy: 0.927039.\n",
      "Iteration 7896: Policy loss: -0.010435. Value loss: 0.108696. Entropy: 0.924888.\n",
      "episode: 3360   score: 13.0  epsilon: 1.0    steps: 736  evaluation reward: 28.49\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7897: Policy loss: 0.010149. Value loss: 0.586728. Entropy: 0.922863.\n",
      "Iteration 7898: Policy loss: -0.005289. Value loss: 0.225351. Entropy: 0.924829.\n",
      "Iteration 7899: Policy loss: -0.013202. Value loss: 0.121634. Entropy: 0.917228.\n",
      "episode: 3361   score: 37.0  epsilon: 1.0    steps: 984  evaluation reward: 28.58\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7900: Policy loss: 0.010268. Value loss: 0.600039. Entropy: 0.961677.\n",
      "Iteration 7901: Policy loss: -0.004320. Value loss: 0.274971. Entropy: 0.955637.\n",
      "Iteration 7902: Policy loss: -0.014129. Value loss: 0.164125. Entropy: 0.964906.\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7903: Policy loss: 0.011399. Value loss: 0.636289. Entropy: 0.971121.\n",
      "Iteration 7904: Policy loss: -0.001914. Value loss: 0.295027. Entropy: 0.980492.\n",
      "Iteration 7905: Policy loss: -0.016732. Value loss: 0.161648. Entropy: 0.987776.\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7906: Policy loss: 0.010179. Value loss: 0.818616. Entropy: 0.943126.\n",
      "Iteration 7907: Policy loss: -0.003673. Value loss: 0.370143. Entropy: 0.938176.\n",
      "Iteration 7908: Policy loss: -0.016827. Value loss: 0.196351. Entropy: 0.945693.\n",
      "episode: 3362   score: 22.0  epsilon: 1.0    steps: 424  evaluation reward: 28.62\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7909: Policy loss: 0.009061. Value loss: 0.637731. Entropy: 1.056781.\n",
      "Iteration 7910: Policy loss: -0.003197. Value loss: 0.267835. Entropy: 1.043778.\n",
      "Iteration 7911: Policy loss: -0.013381. Value loss: 0.167383. Entropy: 1.048101.\n",
      "episode: 3363   score: 35.0  epsilon: 1.0    steps: 256  evaluation reward: 28.75\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7912: Policy loss: 0.012238. Value loss: 0.834256. Entropy: 1.008588.\n",
      "Iteration 7913: Policy loss: 0.003518. Value loss: 0.309436. Entropy: 1.005464.\n",
      "Iteration 7914: Policy loss: -0.013097. Value loss: 0.180678. Entropy: 1.013296.\n",
      "episode: 3364   score: 45.0  epsilon: 1.0    steps: 104  evaluation reward: 29.02\n",
      "episode: 3365   score: 30.0  epsilon: 1.0    steps: 144  evaluation reward: 29.01\n",
      "episode: 3366   score: 13.0  epsilon: 1.0    steps: 688  evaluation reward: 28.87\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7915: Policy loss: 0.007347. Value loss: 0.493701. Entropy: 0.978779.\n",
      "Iteration 7916: Policy loss: -0.008571. Value loss: 0.249402. Entropy: 0.989545.\n",
      "Iteration 7917: Policy loss: -0.021425. Value loss: 0.156807. Entropy: 0.983884.\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7918: Policy loss: 0.008459. Value loss: 0.578071. Entropy: 1.049608.\n",
      "Iteration 7919: Policy loss: -0.003143. Value loss: 0.213642. Entropy: 1.035575.\n",
      "Iteration 7920: Policy loss: -0.013648. Value loss: 0.110224. Entropy: 1.042309.\n",
      "episode: 3367   score: 14.0  epsilon: 1.0    steps: 144  evaluation reward: 28.75\n",
      "episode: 3368   score: 34.0  epsilon: 1.0    steps: 312  evaluation reward: 28.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7921: Policy loss: 0.007274. Value loss: 0.436000. Entropy: 1.022740.\n",
      "Iteration 7922: Policy loss: -0.001369. Value loss: 0.215669. Entropy: 1.026970.\n",
      "Iteration 7923: Policy loss: -0.014624. Value loss: 0.145318. Entropy: 1.014653.\n",
      "episode: 3369   score: 24.0  epsilon: 1.0    steps: 176  evaluation reward: 28.95\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7924: Policy loss: 0.007842. Value loss: 0.465519. Entropy: 1.046203.\n",
      "Iteration 7925: Policy loss: -0.002320. Value loss: 0.202933. Entropy: 1.027998.\n",
      "Iteration 7926: Policy loss: -0.017605. Value loss: 0.114495. Entropy: 1.031116.\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7927: Policy loss: 0.012472. Value loss: 0.307844. Entropy: 1.046779.\n",
      "Iteration 7928: Policy loss: -0.009990. Value loss: 0.134538. Entropy: 1.022893.\n",
      "Iteration 7929: Policy loss: -0.022046. Value loss: 0.079944. Entropy: 1.028968.\n",
      "episode: 3370   score: 13.0  epsilon: 1.0    steps: 80  evaluation reward: 28.85\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7930: Policy loss: 0.008782. Value loss: 0.351293. Entropy: 1.034302.\n",
      "Iteration 7931: Policy loss: -0.008895. Value loss: 0.153597. Entropy: 1.024923.\n",
      "Iteration 7932: Policy loss: -0.013412. Value loss: 0.090771. Entropy: 1.037824.\n",
      "episode: 3371   score: 18.0  epsilon: 1.0    steps: 528  evaluation reward: 28.73\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7933: Policy loss: 0.011898. Value loss: 0.461139. Entropy: 1.021113.\n",
      "Iteration 7934: Policy loss: -0.000398. Value loss: 0.168817. Entropy: 1.012470.\n",
      "Iteration 7935: Policy loss: -0.013269. Value loss: 0.101399. Entropy: 1.020827.\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7936: Policy loss: 0.005092. Value loss: 0.447588. Entropy: 0.998085.\n",
      "Iteration 7937: Policy loss: -0.006867. Value loss: 0.175840. Entropy: 0.994358.\n",
      "Iteration 7938: Policy loss: -0.015479. Value loss: 0.107245. Entropy: 0.974743.\n",
      "episode: 3372   score: 12.0  epsilon: 1.0    steps: 336  evaluation reward: 28.62\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7939: Policy loss: 0.013832. Value loss: 0.753889. Entropy: 0.980461.\n",
      "Iteration 7940: Policy loss: -0.000809. Value loss: 0.258316. Entropy: 0.979406.\n",
      "Iteration 7941: Policy loss: -0.010766. Value loss: 0.145503. Entropy: 0.983421.\n",
      "episode: 3373   score: 25.0  epsilon: 1.0    steps: 824  evaluation reward: 28.36\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7942: Policy loss: 0.020064. Value loss: 0.698897. Entropy: 0.996570.\n",
      "Iteration 7943: Policy loss: 0.002267. Value loss: 0.215608. Entropy: 1.000424.\n",
      "Iteration 7944: Policy loss: -0.008774. Value loss: 0.118744. Entropy: 0.995565.\n",
      "episode: 3374   score: 30.0  epsilon: 1.0    steps: 544  evaluation reward: 28.46\n",
      "episode: 3375   score: 37.0  epsilon: 1.0    steps: 664  evaluation reward: 28.72\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7945: Policy loss: 0.013421. Value loss: 0.784900. Entropy: 1.073760.\n",
      "Iteration 7946: Policy loss: 0.005392. Value loss: 0.353122. Entropy: 1.071382.\n",
      "Iteration 7947: Policy loss: -0.011956. Value loss: 0.183242. Entropy: 1.076972.\n",
      "episode: 3376   score: 32.0  epsilon: 1.0    steps: 440  evaluation reward: 28.6\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7948: Policy loss: 0.012745. Value loss: 0.444859. Entropy: 1.024821.\n",
      "Iteration 7949: Policy loss: 0.000123. Value loss: 0.171340. Entropy: 1.035107.\n",
      "Iteration 7950: Policy loss: -0.009475. Value loss: 0.112343. Entropy: 1.018378.\n",
      "episode: 3377   score: 28.0  epsilon: 1.0    steps: 712  evaluation reward: 28.73\n",
      "episode: 3378   score: 30.0  epsilon: 1.0    steps: 960  evaluation reward: 28.71\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7951: Policy loss: 0.012854. Value loss: 0.634232. Entropy: 1.008447.\n",
      "Iteration 7952: Policy loss: -0.001807. Value loss: 0.310775. Entropy: 1.013050.\n",
      "Iteration 7953: Policy loss: -0.012497. Value loss: 0.141404. Entropy: 1.014951.\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7954: Policy loss: 0.004332. Value loss: 0.301229. Entropy: 1.014310.\n",
      "Iteration 7955: Policy loss: -0.001757. Value loss: 0.145800. Entropy: 1.008611.\n",
      "Iteration 7956: Policy loss: -0.017718. Value loss: 0.098586. Entropy: 1.009113.\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7957: Policy loss: 0.002263. Value loss: 0.424778. Entropy: 1.052898.\n",
      "Iteration 7958: Policy loss: -0.013630. Value loss: 0.236649. Entropy: 1.040027.\n",
      "Iteration 7959: Policy loss: -0.019130. Value loss: 0.155253. Entropy: 1.031191.\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7960: Policy loss: 0.007916. Value loss: 0.529860. Entropy: 1.057215.\n",
      "Iteration 7961: Policy loss: -0.005199. Value loss: 0.220187. Entropy: 1.038783.\n",
      "Iteration 7962: Policy loss: -0.016810. Value loss: 0.111346. Entropy: 1.050365.\n",
      "episode: 3379   score: 33.0  epsilon: 1.0    steps: 584  evaluation reward: 28.73\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7963: Policy loss: 0.016263. Value loss: 0.690888. Entropy: 1.003754.\n",
      "Iteration 7964: Policy loss: 0.001957. Value loss: 0.244521. Entropy: 0.989346.\n",
      "Iteration 7965: Policy loss: -0.012549. Value loss: 0.136158. Entropy: 0.995267.\n",
      "episode: 3380   score: 26.0  epsilon: 1.0    steps: 40  evaluation reward: 28.8\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7966: Policy loss: 0.016430. Value loss: 0.286218. Entropy: 1.024467.\n",
      "Iteration 7967: Policy loss: -0.007951. Value loss: 0.142351. Entropy: 1.012527.\n",
      "Iteration 7968: Policy loss: -0.018074. Value loss: 0.098947. Entropy: 1.004485.\n",
      "episode: 3381   score: 28.0  epsilon: 1.0    steps: 736  evaluation reward: 28.83\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7969: Policy loss: 0.007426. Value loss: 0.270996. Entropy: 0.981160.\n",
      "Iteration 7970: Policy loss: -0.013142. Value loss: 0.107069. Entropy: 0.975049.\n",
      "Iteration 7971: Policy loss: -0.022271. Value loss: 0.064678. Entropy: 0.977604.\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7972: Policy loss: 0.014808. Value loss: 0.519811. Entropy: 1.000036.\n",
      "Iteration 7973: Policy loss: -0.000958. Value loss: 0.234635. Entropy: 0.968774.\n",
      "Iteration 7974: Policy loss: -0.017927. Value loss: 0.127687. Entropy: 0.988656.\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7975: Policy loss: 0.011545. Value loss: 0.555472. Entropy: 1.017244.\n",
      "Iteration 7976: Policy loss: -0.002875. Value loss: 0.232736. Entropy: 1.032548.\n",
      "Iteration 7977: Policy loss: -0.006688. Value loss: 0.139890. Entropy: 1.033009.\n",
      "episode: 3382   score: 29.0  epsilon: 1.0    steps: 544  evaluation reward: 28.95\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7978: Policy loss: 0.008838. Value loss: 0.775512. Entropy: 1.025946.\n",
      "Iteration 7979: Policy loss: -0.005450. Value loss: 0.357071. Entropy: 1.010798.\n",
      "Iteration 7980: Policy loss: -0.014572. Value loss: 0.219911. Entropy: 1.004048.\n",
      "episode: 3383   score: 40.0  epsilon: 1.0    steps: 728  evaluation reward: 29.11\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7981: Policy loss: 0.021885. Value loss: 1.131583. Entropy: 0.979516.\n",
      "Iteration 7982: Policy loss: 0.000669. Value loss: 0.418508. Entropy: 0.987303.\n",
      "Iteration 7983: Policy loss: -0.009342. Value loss: 0.205505. Entropy: 0.983774.\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7984: Policy loss: 0.010426. Value loss: 0.609521. Entropy: 0.926100.\n",
      "Iteration 7985: Policy loss: 0.000070. Value loss: 0.283566. Entropy: 0.939078.\n",
      "Iteration 7986: Policy loss: -0.007293. Value loss: 0.156630. Entropy: 0.923102.\n",
      "episode: 3384   score: 51.0  epsilon: 1.0    steps: 96  evaluation reward: 29.36\n",
      "episode: 3385   score: 41.0  epsilon: 1.0    steps: 640  evaluation reward: 29.51\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7987: Policy loss: 0.004568. Value loss: 0.703884. Entropy: 1.019482.\n",
      "Iteration 7988: Policy loss: -0.005473. Value loss: 0.225394. Entropy: 0.995625.\n",
      "Iteration 7989: Policy loss: -0.018856. Value loss: 0.125426. Entropy: 1.003353.\n",
      "episode: 3386   score: 43.0  epsilon: 1.0    steps: 296  evaluation reward: 29.65\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7990: Policy loss: 0.013293. Value loss: 0.634945. Entropy: 1.004770.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7991: Policy loss: 0.007661. Value loss: 0.241958. Entropy: 1.019266.\n",
      "Iteration 7992: Policy loss: -0.008653. Value loss: 0.135760. Entropy: 1.012883.\n",
      "episode: 3387   score: 35.0  epsilon: 1.0    steps: 736  evaluation reward: 29.67\n",
      "episode: 3388   score: 35.0  epsilon: 1.0    steps: 1024  evaluation reward: 29.85\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7993: Policy loss: 0.006670. Value loss: 0.841583. Entropy: 1.046165.\n",
      "Iteration 7994: Policy loss: -0.010461. Value loss: 0.407623. Entropy: 1.038211.\n",
      "Iteration 7995: Policy loss: -0.015527. Value loss: 0.217726. Entropy: 1.028567.\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7996: Policy loss: 0.009753. Value loss: 0.241557. Entropy: 1.007398.\n",
      "Iteration 7997: Policy loss: -0.006369. Value loss: 0.074470. Entropy: 1.010808.\n",
      "Iteration 7998: Policy loss: -0.016772. Value loss: 0.045961. Entropy: 0.999958.\n",
      "episode: 3389   score: 34.0  epsilon: 1.0    steps: 672  evaluation reward: 29.93\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7999: Policy loss: 0.008573. Value loss: 1.069289. Entropy: 1.003648.\n",
      "Iteration 8000: Policy loss: 0.004410. Value loss: 0.447273. Entropy: 1.000870.\n",
      "Iteration 8001: Policy loss: -0.008285. Value loss: 0.271230. Entropy: 1.015743.\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8002: Policy loss: 0.007350. Value loss: 0.413649. Entropy: 0.963466.\n",
      "Iteration 8003: Policy loss: -0.007496. Value loss: 0.159828. Entropy: 0.963319.\n",
      "Iteration 8004: Policy loss: -0.017459. Value loss: 0.099340. Entropy: 0.955432.\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8005: Policy loss: 0.011723. Value loss: 0.671211. Entropy: 0.978362.\n",
      "Iteration 8006: Policy loss: -0.006014. Value loss: 0.269025. Entropy: 0.957521.\n",
      "Iteration 8007: Policy loss: -0.015842. Value loss: 0.154491. Entropy: 0.951697.\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8008: Policy loss: 0.016945. Value loss: 0.463218. Entropy: 1.074668.\n",
      "Iteration 8009: Policy loss: -0.003641. Value loss: 0.219073. Entropy: 1.060575.\n",
      "Iteration 8010: Policy loss: -0.017851. Value loss: 0.131050. Entropy: 1.057554.\n",
      "episode: 3390   score: 29.0  epsilon: 1.0    steps: 680  evaluation reward: 29.95\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8011: Policy loss: 0.010225. Value loss: 1.207466. Entropy: 1.032112.\n",
      "Iteration 8012: Policy loss: 0.002030. Value loss: 0.571405. Entropy: 1.006196.\n",
      "Iteration 8013: Policy loss: -0.009094. Value loss: 0.369459. Entropy: 1.015006.\n",
      "episode: 3391   score: 27.0  epsilon: 1.0    steps: 1024  evaluation reward: 29.84\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8014: Policy loss: 0.006751. Value loss: 0.819509. Entropy: 1.038582.\n",
      "Iteration 8015: Policy loss: -0.001228. Value loss: 0.403193. Entropy: 1.040453.\n",
      "Iteration 8016: Policy loss: -0.014552. Value loss: 0.261599. Entropy: 1.034762.\n",
      "episode: 3392   score: 33.0  epsilon: 1.0    steps: 824  evaluation reward: 29.72\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8017: Policy loss: 0.014887. Value loss: 0.727610. Entropy: 1.063546.\n",
      "Iteration 8018: Policy loss: -0.004014. Value loss: 0.310229. Entropy: 1.051638.\n",
      "Iteration 8019: Policy loss: -0.010357. Value loss: 0.168612. Entropy: 1.053374.\n",
      "episode: 3393   score: 29.0  epsilon: 1.0    steps: 96  evaluation reward: 29.77\n",
      "episode: 3394   score: 50.0  epsilon: 1.0    steps: 504  evaluation reward: 30.16\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8020: Policy loss: 0.009258. Value loss: 0.652992. Entropy: 1.041893.\n",
      "Iteration 8021: Policy loss: 0.003597. Value loss: 0.258382. Entropy: 1.038905.\n",
      "Iteration 8022: Policy loss: -0.004737. Value loss: 0.122556. Entropy: 1.051556.\n",
      "episode: 3395   score: 32.0  epsilon: 1.0    steps: 944  evaluation reward: 30.3\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8023: Policy loss: 0.009859. Value loss: 0.487430. Entropy: 1.087610.\n",
      "Iteration 8024: Policy loss: -0.000171. Value loss: 0.182526. Entropy: 1.086233.\n",
      "Iteration 8025: Policy loss: -0.014344. Value loss: 0.119514. Entropy: 1.087400.\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8026: Policy loss: 0.009809. Value loss: 0.431874. Entropy: 1.016294.\n",
      "Iteration 8027: Policy loss: -0.005784. Value loss: 0.218752. Entropy: 1.010172.\n",
      "Iteration 8028: Policy loss: -0.014316. Value loss: 0.153535. Entropy: 1.001783.\n",
      "episode: 3396   score: 30.0  epsilon: 1.0    steps: 1016  evaluation reward: 30.22\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8029: Policy loss: 0.007003. Value loss: 0.840521. Entropy: 1.062190.\n",
      "Iteration 8030: Policy loss: -0.002342. Value loss: 0.476776. Entropy: 1.049219.\n",
      "Iteration 8031: Policy loss: -0.012517. Value loss: 0.293007. Entropy: 1.054348.\n",
      "episode: 3397   score: 47.0  epsilon: 1.0    steps: 264  evaluation reward: 30.4\n",
      "episode: 3398   score: 16.0  epsilon: 1.0    steps: 304  evaluation reward: 30.28\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8032: Policy loss: 0.013739. Value loss: 0.846885. Entropy: 1.018301.\n",
      "Iteration 8033: Policy loss: 0.007254. Value loss: 0.314532. Entropy: 0.995969.\n",
      "Iteration 8034: Policy loss: -0.001973. Value loss: 0.148724. Entropy: 0.994416.\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8035: Policy loss: 0.008816. Value loss: 0.776979. Entropy: 0.950725.\n",
      "Iteration 8036: Policy loss: -0.002322. Value loss: 0.287621. Entropy: 0.957814.\n",
      "Iteration 8037: Policy loss: -0.019320. Value loss: 0.169665. Entropy: 0.948721.\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8038: Policy loss: 0.015027. Value loss: 0.636921. Entropy: 1.016448.\n",
      "Iteration 8039: Policy loss: 0.001343. Value loss: 0.281240. Entropy: 1.011065.\n",
      "Iteration 8040: Policy loss: -0.007597. Value loss: 0.139686. Entropy: 0.998973.\n",
      "episode: 3399   score: 34.0  epsilon: 1.0    steps: 24  evaluation reward: 30.17\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8041: Policy loss: 0.012617. Value loss: 0.477535. Entropy: 1.005058.\n",
      "Iteration 8042: Policy loss: -0.011742. Value loss: 0.198673. Entropy: 1.000515.\n",
      "Iteration 8043: Policy loss: -0.020221. Value loss: 0.120050. Entropy: 1.004731.\n",
      "episode: 3400   score: 26.0  epsilon: 1.0    steps: 56  evaluation reward: 30.17\n",
      "now time :  2019-03-06 15:19:44.866457\n",
      "episode: 3401   score: 26.0  epsilon: 1.0    steps: 432  evaluation reward: 30.24\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8044: Policy loss: 0.011281. Value loss: 0.412164. Entropy: 1.047566.\n",
      "Iteration 8045: Policy loss: -0.012232. Value loss: 0.148949. Entropy: 1.048914.\n",
      "Iteration 8046: Policy loss: -0.021775. Value loss: 0.099348. Entropy: 1.037976.\n",
      "episode: 3402   score: 25.0  epsilon: 1.0    steps: 8  evaluation reward: 30.34\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8047: Policy loss: 0.003604. Value loss: 0.232526. Entropy: 0.991347.\n",
      "Iteration 8048: Policy loss: -0.013637. Value loss: 0.110114. Entropy: 1.000139.\n",
      "Iteration 8049: Policy loss: -0.020615. Value loss: 0.054654. Entropy: 0.998605.\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8050: Policy loss: 0.015188. Value loss: 0.598417. Entropy: 0.990916.\n",
      "Iteration 8051: Policy loss: -0.004766. Value loss: 0.306538. Entropy: 0.970926.\n",
      "Iteration 8052: Policy loss: -0.013567. Value loss: 0.170253. Entropy: 0.977696.\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8053: Policy loss: 0.014923. Value loss: 0.418376. Entropy: 1.011299.\n",
      "Iteration 8054: Policy loss: -0.009167. Value loss: 0.172625. Entropy: 1.013334.\n",
      "Iteration 8055: Policy loss: -0.018970. Value loss: 0.109847. Entropy: 1.003399.\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8056: Policy loss: 0.016376. Value loss: 0.703718. Entropy: 1.019796.\n",
      "Iteration 8057: Policy loss: 0.001309. Value loss: 0.260594. Entropy: 1.032326.\n",
      "Iteration 8058: Policy loss: -0.013949. Value loss: 0.147035. Entropy: 1.021511.\n",
      "episode: 3403   score: 46.0  epsilon: 1.0    steps: 880  evaluation reward: 30.62\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8059: Policy loss: 0.009293. Value loss: 1.064711. Entropy: 0.987513.\n",
      "Iteration 8060: Policy loss: -0.003184. Value loss: 0.487159. Entropy: 0.984700.\n",
      "Iteration 8061: Policy loss: -0.007463. Value loss: 0.300079. Entropy: 0.985378.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3404   score: 30.0  epsilon: 1.0    steps: 944  evaluation reward: 30.6\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8062: Policy loss: 0.006196. Value loss: 0.871602. Entropy: 1.063262.\n",
      "Iteration 8063: Policy loss: 0.007970. Value loss: 0.378143. Entropy: 1.060254.\n",
      "Iteration 8064: Policy loss: 0.000954. Value loss: 0.212713. Entropy: 1.051662.\n",
      "episode: 3405   score: 18.0  epsilon: 1.0    steps: 144  evaluation reward: 30.47\n",
      "episode: 3406   score: 44.0  epsilon: 1.0    steps: 328  evaluation reward: 30.71\n",
      "episode: 3407   score: 38.0  epsilon: 1.0    steps: 800  evaluation reward: 30.99\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8065: Policy loss: 0.009731. Value loss: 0.841720. Entropy: 1.002589.\n",
      "Iteration 8066: Policy loss: -0.001246. Value loss: 0.396376. Entropy: 0.976248.\n",
      "Iteration 8067: Policy loss: -0.011881. Value loss: 0.231411. Entropy: 0.987715.\n",
      "episode: 3408   score: 31.0  epsilon: 1.0    steps: 432  evaluation reward: 31.05\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8068: Policy loss: 0.010021. Value loss: 0.634124. Entropy: 0.938960.\n",
      "Iteration 8069: Policy loss: 0.000054. Value loss: 0.262771. Entropy: 0.948288.\n",
      "Iteration 8070: Policy loss: -0.013884. Value loss: 0.135271. Entropy: 0.944789.\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8071: Policy loss: 0.015794. Value loss: 0.501410. Entropy: 0.980754.\n",
      "Iteration 8072: Policy loss: -0.001703. Value loss: 0.256148. Entropy: 0.964707.\n",
      "Iteration 8073: Policy loss: -0.014665. Value loss: 0.163511. Entropy: 0.967392.\n",
      "episode: 3409   score: 28.0  epsilon: 1.0    steps: 440  evaluation reward: 31.13\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8074: Policy loss: 0.018167. Value loss: 0.546050. Entropy: 0.965713.\n",
      "Iteration 8075: Policy loss: -0.003595. Value loss: 0.232362. Entropy: 0.961567.\n",
      "Iteration 8076: Policy loss: -0.013264. Value loss: 0.129898. Entropy: 0.971186.\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8077: Policy loss: 0.012733. Value loss: 0.593498. Entropy: 1.040521.\n",
      "Iteration 8078: Policy loss: -0.001848. Value loss: 0.286768. Entropy: 1.053726.\n",
      "Iteration 8079: Policy loss: -0.008962. Value loss: 0.147398. Entropy: 1.019562.\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8080: Policy loss: 0.012079. Value loss: 0.526673. Entropy: 1.031505.\n",
      "Iteration 8081: Policy loss: 0.000961. Value loss: 0.195366. Entropy: 1.019973.\n",
      "Iteration 8082: Policy loss: -0.007089. Value loss: 0.106117. Entropy: 1.009174.\n",
      "episode: 3410   score: 14.0  epsilon: 1.0    steps: 344  evaluation reward: 31.12\n",
      "episode: 3411   score: 38.0  epsilon: 1.0    steps: 384  evaluation reward: 31.16\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8083: Policy loss: 0.015116. Value loss: 0.740490. Entropy: 0.999508.\n",
      "Iteration 8084: Policy loss: -0.001739. Value loss: 0.300652. Entropy: 0.978522.\n",
      "Iteration 8085: Policy loss: -0.011626. Value loss: 0.157662. Entropy: 0.978004.\n",
      "episode: 3412   score: 16.0  epsilon: 1.0    steps: 520  evaluation reward: 30.82\n",
      "episode: 3413   score: 23.0  epsilon: 1.0    steps: 584  evaluation reward: 30.59\n",
      "episode: 3414   score: 20.0  epsilon: 1.0    steps: 704  evaluation reward: 30.36\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8086: Policy loss: 0.005882. Value loss: 0.536330. Entropy: 1.033666.\n",
      "Iteration 8087: Policy loss: -0.005559. Value loss: 0.221357. Entropy: 1.025549.\n",
      "Iteration 8088: Policy loss: -0.016003. Value loss: 0.126208. Entropy: 1.020258.\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8089: Policy loss: 0.008213. Value loss: 0.357363. Entropy: 0.997340.\n",
      "Iteration 8090: Policy loss: -0.006934. Value loss: 0.123460. Entropy: 0.993276.\n",
      "Iteration 8091: Policy loss: -0.016322. Value loss: 0.067770. Entropy: 0.991570.\n",
      "episode: 3415   score: 22.0  epsilon: 1.0    steps: 176  evaluation reward: 30.23\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8092: Policy loss: 0.010249. Value loss: 0.499169. Entropy: 0.969440.\n",
      "Iteration 8093: Policy loss: -0.002096. Value loss: 0.284129. Entropy: 0.969088.\n",
      "Iteration 8094: Policy loss: -0.013801. Value loss: 0.174471. Entropy: 0.963460.\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8095: Policy loss: 0.005288. Value loss: 0.574304. Entropy: 1.042504.\n",
      "Iteration 8096: Policy loss: -0.011071. Value loss: 0.333725. Entropy: 1.051028.\n",
      "Iteration 8097: Policy loss: -0.017271. Value loss: 0.156594. Entropy: 1.046862.\n",
      "episode: 3416   score: 34.0  epsilon: 1.0    steps: 880  evaluation reward: 30.07\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8098: Policy loss: 0.013760. Value loss: 0.594918. Entropy: 0.966832.\n",
      "Iteration 8099: Policy loss: 0.005183. Value loss: 0.281575. Entropy: 0.970228.\n",
      "Iteration 8100: Policy loss: -0.007351. Value loss: 0.167404. Entropy: 0.961388.\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8101: Policy loss: 0.012693. Value loss: 0.441726. Entropy: 0.924870.\n",
      "Iteration 8102: Policy loss: -0.003967. Value loss: 0.140152. Entropy: 0.930849.\n",
      "Iteration 8103: Policy loss: -0.017569. Value loss: 0.080733. Entropy: 0.927755.\n",
      "episode: 3417   score: 10.0  epsilon: 1.0    steps: 232  evaluation reward: 30.0\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8104: Policy loss: 0.014419. Value loss: 0.473602. Entropy: 0.934264.\n",
      "Iteration 8105: Policy loss: -0.004393. Value loss: 0.164081. Entropy: 0.922946.\n",
      "Iteration 8106: Policy loss: -0.012281. Value loss: 0.062349. Entropy: 0.927186.\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8107: Policy loss: 0.011055. Value loss: 0.963477. Entropy: 0.967275.\n",
      "Iteration 8108: Policy loss: -0.006325. Value loss: 0.459026. Entropy: 0.938668.\n",
      "Iteration 8109: Policy loss: -0.011932. Value loss: 0.242086. Entropy: 0.927624.\n",
      "episode: 3418   score: 43.0  epsilon: 1.0    steps: 336  evaluation reward: 30.03\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8110: Policy loss: 0.013094. Value loss: 0.707402. Entropy: 0.963539.\n",
      "Iteration 8111: Policy loss: -0.001833. Value loss: 0.284196. Entropy: 0.954528.\n",
      "Iteration 8112: Policy loss: -0.010442. Value loss: 0.169910. Entropy: 0.961275.\n",
      "episode: 3419   score: 35.0  epsilon: 1.0    steps: 280  evaluation reward: 30.25\n",
      "episode: 3420   score: 27.0  epsilon: 1.0    steps: 408  evaluation reward: 30.1\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8113: Policy loss: 0.015278. Value loss: 0.945381. Entropy: 0.980172.\n",
      "Iteration 8114: Policy loss: -0.000099. Value loss: 0.313875. Entropy: 0.980962.\n",
      "Iteration 8115: Policy loss: -0.013930. Value loss: 0.158773. Entropy: 0.988595.\n",
      "episode: 3421   score: 31.0  epsilon: 1.0    steps: 648  evaluation reward: 30.07\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8116: Policy loss: 0.010426. Value loss: 0.805437. Entropy: 0.960008.\n",
      "Iteration 8117: Policy loss: -0.005229. Value loss: 0.372647. Entropy: 0.951488.\n",
      "Iteration 8118: Policy loss: -0.018218. Value loss: 0.189994. Entropy: 0.930642.\n",
      "episode: 3422   score: 43.0  epsilon: 1.0    steps: 336  evaluation reward: 30.19\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8119: Policy loss: 0.011777. Value loss: 0.422734. Entropy: 0.957585.\n",
      "Iteration 8120: Policy loss: -0.005559. Value loss: 0.217489. Entropy: 0.945719.\n",
      "Iteration 8121: Policy loss: -0.016230. Value loss: 0.152737. Entropy: 0.932188.\n",
      "episode: 3423   score: 30.0  epsilon: 1.0    steps: 472  evaluation reward: 30.21\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8122: Policy loss: 0.008475. Value loss: 0.455541. Entropy: 0.935990.\n",
      "Iteration 8123: Policy loss: -0.005182. Value loss: 0.160684. Entropy: 0.938485.\n",
      "Iteration 8124: Policy loss: -0.014240. Value loss: 0.086551. Entropy: 0.934107.\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8125: Policy loss: 0.011133. Value loss: 0.705013. Entropy: 0.990985.\n",
      "Iteration 8126: Policy loss: -0.003282. Value loss: 0.317853. Entropy: 0.997949.\n",
      "Iteration 8127: Policy loss: -0.009603. Value loss: 0.179213. Entropy: 0.983925.\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8128: Policy loss: 0.006770. Value loss: 0.349863. Entropy: 0.978792.\n",
      "Iteration 8129: Policy loss: -0.004170. Value loss: 0.115498. Entropy: 0.987102.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8130: Policy loss: -0.009120. Value loss: 0.065270. Entropy: 0.987090.\n",
      "episode: 3424   score: 30.0  epsilon: 1.0    steps: 128  evaluation reward: 30.19\n",
      "episode: 3425   score: 33.0  epsilon: 1.0    steps: 224  evaluation reward: 30.15\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8131: Policy loss: 0.015481. Value loss: 0.601280. Entropy: 1.003700.\n",
      "Iteration 8132: Policy loss: -0.006962. Value loss: 0.299638. Entropy: 1.014573.\n",
      "Iteration 8133: Policy loss: -0.012561. Value loss: 0.170932. Entropy: 1.001551.\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8134: Policy loss: 0.010081. Value loss: 0.536940. Entropy: 0.987543.\n",
      "Iteration 8135: Policy loss: -0.004480. Value loss: 0.231817. Entropy: 0.975824.\n",
      "Iteration 8136: Policy loss: -0.013546. Value loss: 0.148521. Entropy: 0.994135.\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8137: Policy loss: 0.020964. Value loss: 0.785935. Entropy: 0.956966.\n",
      "Iteration 8138: Policy loss: -0.005394. Value loss: 0.278850. Entropy: 0.938990.\n",
      "Iteration 8139: Policy loss: -0.017603. Value loss: 0.155629. Entropy: 0.946135.\n",
      "episode: 3426   score: 31.0  epsilon: 1.0    steps: 352  evaluation reward: 30.19\n",
      "episode: 3427   score: 25.0  epsilon: 1.0    steps: 968  evaluation reward: 30.17\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8140: Policy loss: 0.012148. Value loss: 0.533543. Entropy: 1.013030.\n",
      "Iteration 8141: Policy loss: -0.000166. Value loss: 0.190677. Entropy: 1.020637.\n",
      "Iteration 8142: Policy loss: -0.016040. Value loss: 0.119551. Entropy: 1.009258.\n",
      "episode: 3428   score: 21.0  epsilon: 1.0    steps: 536  evaluation reward: 30.05\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8143: Policy loss: 0.004507. Value loss: 0.792332. Entropy: 0.985615.\n",
      "Iteration 8144: Policy loss: -0.008294. Value loss: 0.376918. Entropy: 0.985874.\n",
      "Iteration 8145: Policy loss: -0.012727. Value loss: 0.203769. Entropy: 0.977062.\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8146: Policy loss: 0.017615. Value loss: 0.384483. Entropy: 1.009631.\n",
      "Iteration 8147: Policy loss: -0.005220. Value loss: 0.153811. Entropy: 1.011293.\n",
      "Iteration 8148: Policy loss: -0.016183. Value loss: 0.081347. Entropy: 1.014977.\n",
      "episode: 3429   score: 37.0  epsilon: 1.0    steps: 304  evaluation reward: 30.14\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8149: Policy loss: 0.015724. Value loss: 1.013853. Entropy: 0.931765.\n",
      "Iteration 8150: Policy loss: -0.001646. Value loss: 0.452731. Entropy: 0.939728.\n",
      "Iteration 8151: Policy loss: -0.008368. Value loss: 0.269282. Entropy: 0.949369.\n",
      "episode: 3430   score: 33.0  epsilon: 1.0    steps: 120  evaluation reward: 30.22\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8152: Policy loss: 0.015440. Value loss: 0.419839. Entropy: 1.028027.\n",
      "Iteration 8153: Policy loss: -0.004468. Value loss: 0.185444. Entropy: 1.025203.\n",
      "Iteration 8154: Policy loss: -0.015722. Value loss: 0.106315. Entropy: 1.031959.\n",
      "episode: 3431   score: 18.0  epsilon: 1.0    steps: 168  evaluation reward: 30.17\n",
      "episode: 3432   score: 41.0  epsilon: 1.0    steps: 512  evaluation reward: 30.37\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8155: Policy loss: 0.004411. Value loss: 0.635276. Entropy: 0.982921.\n",
      "Iteration 8156: Policy loss: -0.008526. Value loss: 0.254744. Entropy: 0.969112.\n",
      "Iteration 8157: Policy loss: -0.012480. Value loss: 0.134316. Entropy: 0.968008.\n",
      "episode: 3433   score: 17.0  epsilon: 1.0    steps: 960  evaluation reward: 30.33\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8158: Policy loss: 0.006730. Value loss: 0.730942. Entropy: 0.918434.\n",
      "Iteration 8159: Policy loss: -0.005611. Value loss: 0.257155. Entropy: 0.927931.\n",
      "Iteration 8160: Policy loss: -0.014954. Value loss: 0.161610. Entropy: 0.926376.\n",
      "episode: 3434   score: 38.0  epsilon: 1.0    steps: 64  evaluation reward: 30.47\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8161: Policy loss: 0.012178. Value loss: 0.536276. Entropy: 1.001872.\n",
      "Iteration 8162: Policy loss: -0.008253. Value loss: 0.270039. Entropy: 1.017961.\n",
      "Iteration 8163: Policy loss: -0.018790. Value loss: 0.176870. Entropy: 1.013605.\n",
      "episode: 3435   score: 19.0  epsilon: 1.0    steps: 600  evaluation reward: 30.18\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8164: Policy loss: 0.014749. Value loss: 0.550414. Entropy: 1.032863.\n",
      "Iteration 8165: Policy loss: 0.001754. Value loss: 0.223929. Entropy: 1.043308.\n",
      "Iteration 8166: Policy loss: -0.006146. Value loss: 0.135529. Entropy: 1.045092.\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8167: Policy loss: 0.008225. Value loss: 0.592171. Entropy: 1.019047.\n",
      "Iteration 8168: Policy loss: -0.006170. Value loss: 0.249123. Entropy: 1.004593.\n",
      "Iteration 8169: Policy loss: -0.014394. Value loss: 0.147102. Entropy: 1.003310.\n",
      "episode: 3436   score: 12.0  epsilon: 1.0    steps: 64  evaluation reward: 30.05\n",
      "episode: 3437   score: 35.0  epsilon: 1.0    steps: 288  evaluation reward: 30.02\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8170: Policy loss: 0.012063. Value loss: 0.435977. Entropy: 0.974589.\n",
      "Iteration 8171: Policy loss: -0.004362. Value loss: 0.161613. Entropy: 0.959770.\n",
      "Iteration 8172: Policy loss: -0.018227. Value loss: 0.072970. Entropy: 0.974169.\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8173: Policy loss: 0.006454. Value loss: 0.705607. Entropy: 0.992662.\n",
      "Iteration 8174: Policy loss: 0.002354. Value loss: 0.260640. Entropy: 1.001721.\n",
      "Iteration 8175: Policy loss: -0.014265. Value loss: 0.152630. Entropy: 0.994584.\n",
      "episode: 3438   score: 22.0  epsilon: 1.0    steps: 632  evaluation reward: 29.97\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8176: Policy loss: 0.011979. Value loss: 0.596188. Entropy: 1.037321.\n",
      "Iteration 8177: Policy loss: -0.001887. Value loss: 0.218473. Entropy: 1.028352.\n",
      "Iteration 8178: Policy loss: -0.010540. Value loss: 0.131573. Entropy: 1.016393.\n",
      "episode: 3439   score: 31.0  epsilon: 1.0    steps: 144  evaluation reward: 29.72\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8179: Policy loss: 0.008018. Value loss: 0.581983. Entropy: 1.053578.\n",
      "Iteration 8180: Policy loss: -0.010956. Value loss: 0.227041. Entropy: 1.037729.\n",
      "Iteration 8181: Policy loss: -0.021318. Value loss: 0.111104. Entropy: 1.033584.\n",
      "episode: 3440   score: 18.0  epsilon: 1.0    steps: 872  evaluation reward: 29.46\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8182: Policy loss: 0.010888. Value loss: 1.100233. Entropy: 1.009844.\n",
      "Iteration 8183: Policy loss: 0.005273. Value loss: 0.484732. Entropy: 0.986700.\n",
      "Iteration 8184: Policy loss: -0.005083. Value loss: 0.265109. Entropy: 0.985133.\n",
      "episode: 3441   score: 28.0  epsilon: 1.0    steps: 280  evaluation reward: 29.32\n",
      "episode: 3442   score: 25.0  epsilon: 1.0    steps: 312  evaluation reward: 29.27\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8185: Policy loss: 0.007383. Value loss: 0.731513. Entropy: 0.924549.\n",
      "Iteration 8186: Policy loss: -0.009129. Value loss: 0.241929. Entropy: 0.912457.\n",
      "Iteration 8187: Policy loss: -0.018796. Value loss: 0.125004. Entropy: 0.915500.\n",
      "episode: 3443   score: 13.0  epsilon: 1.0    steps: 360  evaluation reward: 29.18\n",
      "episode: 3444   score: 22.0  epsilon: 1.0    steps: 920  evaluation reward: 29.16\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8188: Policy loss: 0.012072. Value loss: 0.641312. Entropy: 0.949048.\n",
      "Iteration 8189: Policy loss: 0.000762. Value loss: 0.242862. Entropy: 0.929366.\n",
      "Iteration 8190: Policy loss: -0.008221. Value loss: 0.127894. Entropy: 0.938578.\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8191: Policy loss: 0.010813. Value loss: 0.391856. Entropy: 0.969041.\n",
      "Iteration 8192: Policy loss: -0.002399. Value loss: 0.153539. Entropy: 0.954473.\n",
      "Iteration 8193: Policy loss: -0.012592. Value loss: 0.070540. Entropy: 0.960373.\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8194: Policy loss: 0.008850. Value loss: 0.602824. Entropy: 0.966739.\n",
      "Iteration 8195: Policy loss: -0.009340. Value loss: 0.321532. Entropy: 0.957157.\n",
      "Iteration 8196: Policy loss: -0.015581. Value loss: 0.154276. Entropy: 0.956705.\n",
      "episode: 3445   score: 33.0  epsilon: 1.0    steps: 352  evaluation reward: 29.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8197: Policy loss: 0.014876. Value loss: 0.329553. Entropy: 0.960322.\n",
      "Iteration 8198: Policy loss: -0.000139. Value loss: 0.138839. Entropy: 0.955621.\n",
      "Iteration 8199: Policy loss: -0.010312. Value loss: 0.084678. Entropy: 0.945230.\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8200: Policy loss: 0.016817. Value loss: 0.455107. Entropy: 0.935245.\n",
      "Iteration 8201: Policy loss: -0.005589. Value loss: 0.223958. Entropy: 0.944091.\n",
      "Iteration 8202: Policy loss: -0.014981. Value loss: 0.139593. Entropy: 0.943265.\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8203: Policy loss: 0.012114. Value loss: 0.666780. Entropy: 0.992323.\n",
      "Iteration 8204: Policy loss: -0.001242. Value loss: 0.238409. Entropy: 1.018242.\n",
      "Iteration 8205: Policy loss: -0.010351. Value loss: 0.149737. Entropy: 1.012655.\n",
      "episode: 3446   score: 18.0  epsilon: 1.0    steps: 112  evaluation reward: 28.99\n",
      "episode: 3447   score: 39.0  epsilon: 1.0    steps: 920  evaluation reward: 29.11\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8206: Policy loss: 0.009993. Value loss: 0.817568. Entropy: 0.973342.\n",
      "Iteration 8207: Policy loss: -0.001248. Value loss: 0.337140. Entropy: 0.957962.\n",
      "Iteration 8208: Policy loss: -0.010816. Value loss: 0.192417. Entropy: 0.961278.\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8209: Policy loss: 0.007042. Value loss: 0.464268. Entropy: 1.005654.\n",
      "Iteration 8210: Policy loss: -0.005030. Value loss: 0.197179. Entropy: 0.995153.\n",
      "Iteration 8211: Policy loss: -0.010648. Value loss: 0.114720. Entropy: 0.996994.\n",
      "episode: 3448   score: 19.0  epsilon: 1.0    steps: 296  evaluation reward: 28.94\n",
      "episode: 3449   score: 22.0  epsilon: 1.0    steps: 648  evaluation reward: 28.81\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8212: Policy loss: 0.013273. Value loss: 1.273342. Entropy: 0.926896.\n",
      "Iteration 8213: Policy loss: 0.004876. Value loss: 0.424069. Entropy: 0.911673.\n",
      "Iteration 8214: Policy loss: -0.008466. Value loss: 0.219196. Entropy: 0.914282.\n",
      "episode: 3450   score: 34.0  epsilon: 1.0    steps: 16  evaluation reward: 28.87\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8215: Policy loss: 0.013595. Value loss: 0.572283. Entropy: 0.968821.\n",
      "Iteration 8216: Policy loss: -0.001604. Value loss: 0.215657. Entropy: 0.976107.\n",
      "Iteration 8217: Policy loss: -0.011636. Value loss: 0.117322. Entropy: 0.954346.\n",
      "now time :  2019-03-06 15:23:24.922352\n",
      "episode: 3451   score: 47.0  epsilon: 1.0    steps: 272  evaluation reward: 28.97\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8218: Policy loss: 0.010225. Value loss: 0.384168. Entropy: 1.007442.\n",
      "Iteration 8219: Policy loss: -0.004632. Value loss: 0.191976. Entropy: 1.020340.\n",
      "Iteration 8220: Policy loss: -0.013522. Value loss: 0.125982. Entropy: 1.007434.\n",
      "episode: 3452   score: 23.0  epsilon: 1.0    steps: 432  evaluation reward: 28.9\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8221: Policy loss: 0.007613. Value loss: 0.714596. Entropy: 0.970347.\n",
      "Iteration 8222: Policy loss: 0.003134. Value loss: 0.303650. Entropy: 0.973598.\n",
      "Iteration 8223: Policy loss: -0.012062. Value loss: 0.172290. Entropy: 0.968022.\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8224: Policy loss: 0.021984. Value loss: 1.677429. Entropy: 0.950846.\n",
      "Iteration 8225: Policy loss: 0.003322. Value loss: 0.970908. Entropy: 0.928270.\n",
      "Iteration 8226: Policy loss: -0.005816. Value loss: 0.645961. Entropy: 0.926834.\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8227: Policy loss: 0.012928. Value loss: 0.575345. Entropy: 0.999759.\n",
      "Iteration 8228: Policy loss: -0.006951. Value loss: 0.223472. Entropy: 0.983872.\n",
      "Iteration 8229: Policy loss: -0.012550. Value loss: 0.127549. Entropy: 0.985910.\n",
      "episode: 3453   score: 51.0  epsilon: 1.0    steps: 168  evaluation reward: 29.13\n",
      "episode: 3454   score: 20.0  epsilon: 1.0    steps: 752  evaluation reward: 29.04\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8230: Policy loss: 0.011517. Value loss: 0.748426. Entropy: 0.990036.\n",
      "Iteration 8231: Policy loss: -0.002786. Value loss: 0.348447. Entropy: 1.009472.\n",
      "Iteration 8232: Policy loss: -0.008647. Value loss: 0.204873. Entropy: 0.989989.\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8233: Policy loss: 0.013746. Value loss: 0.635913. Entropy: 1.006573.\n",
      "Iteration 8234: Policy loss: 0.000837. Value loss: 0.287562. Entropy: 0.991934.\n",
      "Iteration 8235: Policy loss: -0.010940. Value loss: 0.132721. Entropy: 0.993334.\n",
      "episode: 3455   score: 32.0  epsilon: 1.0    steps: 224  evaluation reward: 29.18\n",
      "episode: 3456   score: 18.0  epsilon: 1.0    steps: 352  evaluation reward: 28.99\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8236: Policy loss: 0.009289. Value loss: 0.547785. Entropy: 1.030865.\n",
      "Iteration 8237: Policy loss: -0.004815. Value loss: 0.191976. Entropy: 1.034733.\n",
      "Iteration 8238: Policy loss: -0.013223. Value loss: 0.118445. Entropy: 1.035252.\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8239: Policy loss: 0.008811. Value loss: 0.657752. Entropy: 0.982291.\n",
      "Iteration 8240: Policy loss: -0.005404. Value loss: 0.253604. Entropy: 0.977071.\n",
      "Iteration 8241: Policy loss: -0.015010. Value loss: 0.154786. Entropy: 0.969964.\n",
      "episode: 3457   score: 18.0  epsilon: 1.0    steps: 184  evaluation reward: 28.91\n",
      "episode: 3458   score: 32.0  epsilon: 1.0    steps: 336  evaluation reward: 28.93\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8242: Policy loss: 0.013533. Value loss: 0.507809. Entropy: 0.899170.\n",
      "Iteration 8243: Policy loss: 0.004311. Value loss: 0.224270. Entropy: 0.914919.\n",
      "Iteration 8244: Policy loss: -0.001890. Value loss: 0.126909. Entropy: 0.911138.\n",
      "episode: 3459   score: 42.0  epsilon: 1.0    steps: 672  evaluation reward: 29.09\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8245: Policy loss: 0.009989. Value loss: 0.579869. Entropy: 0.986409.\n",
      "Iteration 8246: Policy loss: -0.004970. Value loss: 0.243550. Entropy: 0.990568.\n",
      "Iteration 8247: Policy loss: -0.013607. Value loss: 0.149621. Entropy: 0.975840.\n",
      "episode: 3460   score: 49.0  epsilon: 1.0    steps: 1000  evaluation reward: 29.45\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8248: Policy loss: 0.018530. Value loss: 0.856977. Entropy: 1.005429.\n",
      "Iteration 8249: Policy loss: 0.010003. Value loss: 0.363260. Entropy: 1.014027.\n",
      "Iteration 8250: Policy loss: 0.002267. Value loss: 0.202124. Entropy: 0.991793.\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8251: Policy loss: 0.009124. Value loss: 0.455686. Entropy: 1.036110.\n",
      "Iteration 8252: Policy loss: -0.009318. Value loss: 0.190137. Entropy: 1.033983.\n",
      "Iteration 8253: Policy loss: -0.017459. Value loss: 0.089978. Entropy: 1.027499.\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8254: Policy loss: 0.015350. Value loss: 0.671291. Entropy: 1.077382.\n",
      "Iteration 8255: Policy loss: 0.000255. Value loss: 0.337751. Entropy: 1.065165.\n",
      "Iteration 8256: Policy loss: -0.010912. Value loss: 0.194803. Entropy: 1.051111.\n",
      "episode: 3461   score: 28.0  epsilon: 1.0    steps: 960  evaluation reward: 29.36\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8257: Policy loss: 0.008492. Value loss: 0.716944. Entropy: 1.028804.\n",
      "Iteration 8258: Policy loss: -0.006619. Value loss: 0.298489. Entropy: 0.995823.\n",
      "Iteration 8259: Policy loss: -0.016028. Value loss: 0.167385. Entropy: 0.995027.\n",
      "episode: 3462   score: 30.0  epsilon: 1.0    steps: 232  evaluation reward: 29.44\n",
      "episode: 3463   score: 26.0  epsilon: 1.0    steps: 272  evaluation reward: 29.35\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8260: Policy loss: 0.005344. Value loss: 0.667348. Entropy: 1.000703.\n",
      "Iteration 8261: Policy loss: -0.013256. Value loss: 0.277530. Entropy: 1.003509.\n",
      "Iteration 8262: Policy loss: -0.024231. Value loss: 0.172056. Entropy: 1.000083.\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8263: Policy loss: 0.010332. Value loss: 0.400220. Entropy: 1.013420.\n",
      "Iteration 8264: Policy loss: -0.005891. Value loss: 0.165913. Entropy: 1.010906.\n",
      "Iteration 8265: Policy loss: -0.018420. Value loss: 0.091395. Entropy: 1.018190.\n",
      "Training network. lr: 0.000187. clip: 0.074665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8266: Policy loss: 0.013624. Value loss: 0.410086. Entropy: 1.077446.\n",
      "Iteration 8267: Policy loss: -0.003364. Value loss: 0.187264. Entropy: 1.065703.\n",
      "Iteration 8268: Policy loss: -0.016656. Value loss: 0.130774. Entropy: 1.063404.\n",
      "episode: 3464   score: 32.0  epsilon: 1.0    steps: 424  evaluation reward: 29.22\n",
      "episode: 3465   score: 41.0  epsilon: 1.0    steps: 728  evaluation reward: 29.33\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8269: Policy loss: 0.009552. Value loss: 0.767536. Entropy: 0.973046.\n",
      "Iteration 8270: Policy loss: 0.001672. Value loss: 0.318030. Entropy: 0.950796.\n",
      "Iteration 8271: Policy loss: -0.011343. Value loss: 0.162677. Entropy: 0.959853.\n",
      "episode: 3466   score: 21.0  epsilon: 1.0    steps: 112  evaluation reward: 29.41\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8272: Policy loss: 0.004557. Value loss: 0.850645. Entropy: 1.039345.\n",
      "Iteration 8273: Policy loss: -0.001356. Value loss: 0.465674. Entropy: 1.024329.\n",
      "Iteration 8274: Policy loss: -0.012078. Value loss: 0.249771. Entropy: 1.019417.\n",
      "episode: 3467   score: 31.0  epsilon: 1.0    steps: 224  evaluation reward: 29.58\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8275: Policy loss: 0.007273. Value loss: 0.615730. Entropy: 0.966559.\n",
      "Iteration 8276: Policy loss: -0.007777. Value loss: 0.247061. Entropy: 0.967213.\n",
      "Iteration 8277: Policy loss: -0.010943. Value loss: 0.137267. Entropy: 0.973702.\n",
      "episode: 3468   score: 30.0  epsilon: 1.0    steps: 296  evaluation reward: 29.54\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8278: Policy loss: 0.006194. Value loss: 0.517971. Entropy: 0.975549.\n",
      "Iteration 8279: Policy loss: -0.001860. Value loss: 0.224796. Entropy: 0.977524.\n",
      "Iteration 8280: Policy loss: -0.012912. Value loss: 0.144340. Entropy: 0.978001.\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8281: Policy loss: 0.009636. Value loss: 0.644915. Entropy: 1.003257.\n",
      "Iteration 8282: Policy loss: -0.007606. Value loss: 0.382534. Entropy: 0.996732.\n",
      "Iteration 8283: Policy loss: -0.012123. Value loss: 0.237790. Entropy: 0.980012.\n",
      "episode: 3469   score: 27.0  epsilon: 1.0    steps: 968  evaluation reward: 29.57\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8284: Policy loss: 0.006313. Value loss: 0.688861. Entropy: 0.998472.\n",
      "Iteration 8285: Policy loss: -0.010769. Value loss: 0.275855. Entropy: 0.978157.\n",
      "Iteration 8286: Policy loss: -0.016232. Value loss: 0.144477. Entropy: 0.983492.\n",
      "episode: 3470   score: 32.0  epsilon: 1.0    steps: 480  evaluation reward: 29.76\n",
      "episode: 3471   score: 33.0  epsilon: 1.0    steps: 520  evaluation reward: 29.91\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8287: Policy loss: 0.008047. Value loss: 0.505399. Entropy: 0.974494.\n",
      "Iteration 8288: Policy loss: 0.001034. Value loss: 0.200325. Entropy: 0.966317.\n",
      "Iteration 8289: Policy loss: -0.008319. Value loss: 0.128201. Entropy: 0.986678.\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8290: Policy loss: 0.008203. Value loss: 0.931442. Entropy: 0.997853.\n",
      "Iteration 8291: Policy loss: -0.005200. Value loss: 0.425492. Entropy: 0.977652.\n",
      "Iteration 8292: Policy loss: -0.012395. Value loss: 0.264595. Entropy: 0.991366.\n",
      "episode: 3472   score: 22.0  epsilon: 1.0    steps: 448  evaluation reward: 30.01\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8293: Policy loss: 0.019360. Value loss: 0.684705. Entropy: 0.961740.\n",
      "Iteration 8294: Policy loss: -0.003467. Value loss: 0.207934. Entropy: 0.947681.\n",
      "Iteration 8295: Policy loss: -0.018222. Value loss: 0.107253. Entropy: 0.952435.\n",
      "episode: 3473   score: 27.0  epsilon: 1.0    steps: 896  evaluation reward: 30.03\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8296: Policy loss: 0.016861. Value loss: 0.775707. Entropy: 0.960440.\n",
      "Iteration 8297: Policy loss: 0.000577. Value loss: 0.307599. Entropy: 0.941205.\n",
      "Iteration 8298: Policy loss: -0.011640. Value loss: 0.162914. Entropy: 0.958905.\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8299: Policy loss: 0.011166. Value loss: 0.390981. Entropy: 1.030225.\n",
      "Iteration 8300: Policy loss: 0.000722. Value loss: 0.188076. Entropy: 1.007560.\n",
      "Iteration 8301: Policy loss: -0.016228. Value loss: 0.095957. Entropy: 1.010822.\n",
      "episode: 3474   score: 28.0  epsilon: 1.0    steps: 104  evaluation reward: 30.01\n",
      "episode: 3475   score: 32.0  epsilon: 1.0    steps: 800  evaluation reward: 29.96\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8302: Policy loss: 0.013877. Value loss: 0.802504. Entropy: 0.939777.\n",
      "Iteration 8303: Policy loss: 0.002352. Value loss: 0.371564. Entropy: 0.939641.\n",
      "Iteration 8304: Policy loss: -0.008916. Value loss: 0.210958. Entropy: 0.936425.\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8305: Policy loss: 0.004206. Value loss: 0.710454. Entropy: 1.017386.\n",
      "Iteration 8306: Policy loss: -0.001845. Value loss: 0.355545. Entropy: 0.991582.\n",
      "Iteration 8307: Policy loss: -0.015485. Value loss: 0.209084. Entropy: 1.000388.\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8308: Policy loss: 0.015000. Value loss: 0.993352. Entropy: 0.889942.\n",
      "Iteration 8309: Policy loss: 0.006361. Value loss: 0.372545. Entropy: 0.873362.\n",
      "Iteration 8310: Policy loss: -0.004240. Value loss: 0.203549. Entropy: 0.871829.\n",
      "episode: 3476   score: 11.0  epsilon: 1.0    steps: 152  evaluation reward: 29.75\n",
      "episode: 3477   score: 40.0  epsilon: 1.0    steps: 584  evaluation reward: 29.87\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8311: Policy loss: 0.006614. Value loss: 0.683082. Entropy: 0.951251.\n",
      "Iteration 8312: Policy loss: 0.000424. Value loss: 0.235794. Entropy: 0.957022.\n",
      "Iteration 8313: Policy loss: -0.014820. Value loss: 0.163783. Entropy: 0.947358.\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8314: Policy loss: 0.013889. Value loss: 0.497819. Entropy: 1.018764.\n",
      "Iteration 8315: Policy loss: -0.003171. Value loss: 0.173239. Entropy: 1.007349.\n",
      "Iteration 8316: Policy loss: -0.017500. Value loss: 0.104362. Entropy: 1.019412.\n",
      "episode: 3478   score: 36.0  epsilon: 1.0    steps: 8  evaluation reward: 29.93\n",
      "episode: 3479   score: 30.0  epsilon: 1.0    steps: 360  evaluation reward: 29.9\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8317: Policy loss: 0.007754. Value loss: 0.596527. Entropy: 0.979813.\n",
      "Iteration 8318: Policy loss: -0.006957. Value loss: 0.277165. Entropy: 0.993296.\n",
      "Iteration 8319: Policy loss: -0.013136. Value loss: 0.170545. Entropy: 0.992771.\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8320: Policy loss: 0.008762. Value loss: 0.479430. Entropy: 1.009310.\n",
      "Iteration 8321: Policy loss: -0.004558. Value loss: 0.168115. Entropy: 0.973747.\n",
      "Iteration 8322: Policy loss: -0.014890. Value loss: 0.102425. Entropy: 0.981679.\n",
      "episode: 3480   score: 40.0  epsilon: 1.0    steps: 232  evaluation reward: 30.04\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8323: Policy loss: 0.016402. Value loss: 0.635679. Entropy: 0.969913.\n",
      "Iteration 8324: Policy loss: -0.001114. Value loss: 0.315842. Entropy: 0.967192.\n",
      "Iteration 8325: Policy loss: -0.016709. Value loss: 0.195251. Entropy: 0.960307.\n",
      "episode: 3481   score: 32.0  epsilon: 1.0    steps: 176  evaluation reward: 30.08\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8326: Policy loss: 0.017239. Value loss: 0.470267. Entropy: 0.977381.\n",
      "Iteration 8327: Policy loss: -0.009551. Value loss: 0.205541. Entropy: 0.984545.\n",
      "Iteration 8328: Policy loss: -0.017496. Value loss: 0.120843. Entropy: 0.971271.\n",
      "episode: 3482   score: 19.0  epsilon: 1.0    steps: 672  evaluation reward: 29.98\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8329: Policy loss: 0.010206. Value loss: 0.981926. Entropy: 0.998714.\n",
      "Iteration 8330: Policy loss: 0.002983. Value loss: 0.346829. Entropy: 0.998650.\n",
      "Iteration 8331: Policy loss: -0.011694. Value loss: 0.225132. Entropy: 0.986418.\n",
      "episode: 3483   score: 39.0  epsilon: 1.0    steps: 80  evaluation reward: 29.97\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8332: Policy loss: 0.005942. Value loss: 0.687004. Entropy: 0.960874.\n",
      "Iteration 8333: Policy loss: -0.004510. Value loss: 0.338749. Entropy: 0.961435.\n",
      "Iteration 8334: Policy loss: -0.012836. Value loss: 0.197150. Entropy: 0.946778.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3484   score: 21.0  epsilon: 1.0    steps: 200  evaluation reward: 29.67\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8335: Policy loss: 0.012291. Value loss: 0.640888. Entropy: 0.934179.\n",
      "Iteration 8336: Policy loss: 0.008568. Value loss: 0.240375. Entropy: 0.909999.\n",
      "Iteration 8337: Policy loss: -0.011375. Value loss: 0.136783. Entropy: 0.899817.\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8338: Policy loss: 0.011425. Value loss: 0.453160. Entropy: 0.879519.\n",
      "Iteration 8339: Policy loss: -0.000447. Value loss: 0.202471. Entropy: 0.893413.\n",
      "Iteration 8340: Policy loss: -0.014083. Value loss: 0.116469. Entropy: 0.874090.\n",
      "episode: 3485   score: 33.0  epsilon: 1.0    steps: 104  evaluation reward: 29.59\n",
      "episode: 3486   score: 21.0  epsilon: 1.0    steps: 920  evaluation reward: 29.37\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8341: Policy loss: 0.016046. Value loss: 0.407710. Entropy: 0.986596.\n",
      "Iteration 8342: Policy loss: -0.005267. Value loss: 0.157391. Entropy: 0.989189.\n",
      "Iteration 8343: Policy loss: -0.018296. Value loss: 0.103019. Entropy: 0.984874.\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8344: Policy loss: 0.011974. Value loss: 0.418287. Entropy: 0.972076.\n",
      "Iteration 8345: Policy loss: -0.006400. Value loss: 0.164688. Entropy: 0.960467.\n",
      "Iteration 8346: Policy loss: -0.009898. Value loss: 0.102475. Entropy: 0.969841.\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8347: Policy loss: 0.020424. Value loss: 1.076319. Entropy: 0.944847.\n",
      "Iteration 8348: Policy loss: -0.002180. Value loss: 0.511999. Entropy: 0.947075.\n",
      "Iteration 8349: Policy loss: -0.010751. Value loss: 0.328819. Entropy: 0.943774.\n",
      "episode: 3487   score: 34.0  epsilon: 1.0    steps: 192  evaluation reward: 29.36\n",
      "episode: 3488   score: 26.0  epsilon: 1.0    steps: 904  evaluation reward: 29.27\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8350: Policy loss: 0.010364. Value loss: 0.535293. Entropy: 0.908971.\n",
      "Iteration 8351: Policy loss: -0.002024. Value loss: 0.267761. Entropy: 0.901592.\n",
      "Iteration 8352: Policy loss: -0.012521. Value loss: 0.180273. Entropy: 0.898889.\n",
      "episode: 3489   score: 17.0  epsilon: 1.0    steps: 8  evaluation reward: 29.1\n",
      "episode: 3490   score: 30.0  epsilon: 1.0    steps: 760  evaluation reward: 29.11\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8353: Policy loss: 0.014785. Value loss: 0.437794. Entropy: 0.932194.\n",
      "Iteration 8354: Policy loss: -0.005824. Value loss: 0.196238. Entropy: 0.904361.\n",
      "Iteration 8355: Policy loss: -0.017957. Value loss: 0.120205. Entropy: 0.911635.\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8356: Policy loss: 0.010865. Value loss: 0.272767. Entropy: 0.910218.\n",
      "Iteration 8357: Policy loss: -0.008042. Value loss: 0.111995. Entropy: 0.887432.\n",
      "Iteration 8358: Policy loss: -0.014623. Value loss: 0.052109. Entropy: 0.889765.\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8359: Policy loss: 0.029224. Value loss: 0.964324. Entropy: 0.924331.\n",
      "Iteration 8360: Policy loss: 0.008663. Value loss: 0.383903. Entropy: 0.899602.\n",
      "Iteration 8361: Policy loss: -0.009264. Value loss: 0.217242. Entropy: 0.903928.\n",
      "episode: 3491   score: 33.0  epsilon: 1.0    steps: 8  evaluation reward: 29.17\n",
      "episode: 3492   score: 29.0  epsilon: 1.0    steps: 112  evaluation reward: 29.13\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8362: Policy loss: 0.015187. Value loss: 0.428203. Entropy: 0.958959.\n",
      "Iteration 8363: Policy loss: -0.005673. Value loss: 0.221942. Entropy: 0.952323.\n",
      "Iteration 8364: Policy loss: -0.016437. Value loss: 0.134265. Entropy: 0.949995.\n",
      "episode: 3493   score: 20.0  epsilon: 1.0    steps: 568  evaluation reward: 29.04\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8365: Policy loss: 0.003850. Value loss: 0.474400. Entropy: 0.938855.\n",
      "Iteration 8366: Policy loss: -0.009098. Value loss: 0.198867. Entropy: 0.942890.\n",
      "Iteration 8367: Policy loss: -0.021475. Value loss: 0.123491. Entropy: 0.947158.\n",
      "episode: 3494   score: 27.0  epsilon: 1.0    steps: 648  evaluation reward: 28.81\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8368: Policy loss: 0.005949. Value loss: 0.435272. Entropy: 0.910016.\n",
      "Iteration 8369: Policy loss: -0.011973. Value loss: 0.182144. Entropy: 0.892981.\n",
      "Iteration 8370: Policy loss: -0.020310. Value loss: 0.099482. Entropy: 0.902121.\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8371: Policy loss: 0.004719. Value loss: 0.572942. Entropy: 0.850280.\n",
      "Iteration 8372: Policy loss: -0.006164. Value loss: 0.283053. Entropy: 0.856878.\n",
      "Iteration 8373: Policy loss: -0.010725. Value loss: 0.139151. Entropy: 0.848432.\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8374: Policy loss: 0.009071. Value loss: 0.747596. Entropy: 0.967831.\n",
      "Iteration 8375: Policy loss: -0.006357. Value loss: 0.255519. Entropy: 0.955358.\n",
      "Iteration 8376: Policy loss: -0.017570. Value loss: 0.124938. Entropy: 0.977530.\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8377: Policy loss: 0.011786. Value loss: 0.788653. Entropy: 0.973926.\n",
      "Iteration 8378: Policy loss: -0.001839. Value loss: 0.341235. Entropy: 0.942841.\n",
      "Iteration 8379: Policy loss: -0.013776. Value loss: 0.189949. Entropy: 0.949772.\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8380: Policy loss: 0.023191. Value loss: 0.670353. Entropy: 0.989238.\n",
      "Iteration 8381: Policy loss: 0.007563. Value loss: 0.256878. Entropy: 0.974279.\n",
      "Iteration 8382: Policy loss: -0.008488. Value loss: 0.150548. Entropy: 0.968758.\n",
      "episode: 3495   score: 32.0  epsilon: 1.0    steps: 400  evaluation reward: 28.81\n",
      "episode: 3496   score: 39.0  epsilon: 1.0    steps: 816  evaluation reward: 28.9\n",
      "episode: 3497   score: 28.0  epsilon: 1.0    steps: 896  evaluation reward: 28.71\n",
      "episode: 3498   score: 37.0  epsilon: 1.0    steps: 912  evaluation reward: 28.92\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8383: Policy loss: 0.029113. Value loss: 1.043047. Entropy: 0.846616.\n",
      "Iteration 8384: Policy loss: 0.010759. Value loss: 0.360737. Entropy: 0.826386.\n",
      "Iteration 8385: Policy loss: -0.009371. Value loss: 0.216171. Entropy: 0.846265.\n",
      "episode: 3499   score: 12.0  epsilon: 1.0    steps: 808  evaluation reward: 28.7\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8386: Policy loss: 0.010374. Value loss: 0.568038. Entropy: 0.986449.\n",
      "Iteration 8387: Policy loss: 0.003921. Value loss: 0.261759. Entropy: 0.974003.\n",
      "Iteration 8388: Policy loss: -0.006686. Value loss: 0.156950. Entropy: 0.966514.\n",
      "episode: 3500   score: 30.0  epsilon: 1.0    steps: 656  evaluation reward: 28.74\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8389: Policy loss: 0.005278. Value loss: 0.666744. Entropy: 0.854273.\n",
      "Iteration 8390: Policy loss: -0.005673. Value loss: 0.211980. Entropy: 0.850624.\n",
      "Iteration 8391: Policy loss: -0.012956. Value loss: 0.124385. Entropy: 0.856032.\n",
      "now time :  2019-03-06 15:27:04.310463\n",
      "episode: 3501   score: 45.0  epsilon: 1.0    steps: 96  evaluation reward: 28.93\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8392: Policy loss: 0.004308. Value loss: 0.345058. Entropy: 0.879727.\n",
      "Iteration 8393: Policy loss: -0.002718. Value loss: 0.149560. Entropy: 0.875095.\n",
      "Iteration 8394: Policy loss: -0.017489. Value loss: 0.092745. Entropy: 0.882379.\n",
      "episode: 3502   score: 35.0  epsilon: 1.0    steps: 904  evaluation reward: 29.03\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8395: Policy loss: 0.015600. Value loss: 0.481919. Entropy: 0.901039.\n",
      "Iteration 8396: Policy loss: 0.004319. Value loss: 0.189585. Entropy: 0.859747.\n",
      "Iteration 8397: Policy loss: -0.008040. Value loss: 0.098919. Entropy: 0.877534.\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8398: Policy loss: 0.007225. Value loss: 0.503189. Entropy: 0.928048.\n",
      "Iteration 8399: Policy loss: -0.006754. Value loss: 0.153375. Entropy: 0.907026.\n",
      "Iteration 8400: Policy loss: -0.011612. Value loss: 0.091113. Entropy: 0.910818.\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8401: Policy loss: 0.008862. Value loss: 0.308144. Entropy: 0.966248.\n",
      "Iteration 8402: Policy loss: -0.009562. Value loss: 0.097488. Entropy: 0.952863.\n",
      "Iteration 8403: Policy loss: -0.023169. Value loss: 0.057840. Entropy: 0.947121.\n",
      "episode: 3503   score: 13.0  epsilon: 1.0    steps: 88  evaluation reward: 28.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8404: Policy loss: 0.016443. Value loss: 0.432737. Entropy: 0.933240.\n",
      "Iteration 8405: Policy loss: -0.002189. Value loss: 0.212895. Entropy: 0.914560.\n",
      "Iteration 8406: Policy loss: -0.014468. Value loss: 0.146306. Entropy: 0.915281.\n",
      "episode: 3504   score: 11.0  epsilon: 1.0    steps: 672  evaluation reward: 28.51\n",
      "episode: 3505   score: 17.0  epsilon: 1.0    steps: 952  evaluation reward: 28.5\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8407: Policy loss: 0.011755. Value loss: 0.783575. Entropy: 0.889920.\n",
      "Iteration 8408: Policy loss: 0.003929. Value loss: 0.312236. Entropy: 0.873117.\n",
      "Iteration 8409: Policy loss: -0.007202. Value loss: 0.144580. Entropy: 0.877787.\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8410: Policy loss: 0.009306. Value loss: 0.619939. Entropy: 0.854009.\n",
      "Iteration 8411: Policy loss: -0.001829. Value loss: 0.299818. Entropy: 0.867574.\n",
      "Iteration 8412: Policy loss: -0.013375. Value loss: 0.151352. Entropy: 0.879598.\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8413: Policy loss: 0.014134. Value loss: 0.502133. Entropy: 0.896631.\n",
      "Iteration 8414: Policy loss: -0.003134. Value loss: 0.141947. Entropy: 0.901636.\n",
      "Iteration 8415: Policy loss: -0.016726. Value loss: 0.067677. Entropy: 0.899447.\n",
      "episode: 3506   score: 31.0  epsilon: 1.0    steps: 384  evaluation reward: 28.37\n",
      "episode: 3507   score: 32.0  epsilon: 1.0    steps: 592  evaluation reward: 28.31\n",
      "episode: 3508   score: 33.0  epsilon: 1.0    steps: 952  evaluation reward: 28.33\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8416: Policy loss: 0.016748. Value loss: 0.627459. Entropy: 0.892948.\n",
      "Iteration 8417: Policy loss: -0.003055. Value loss: 0.220041. Entropy: 0.875045.\n",
      "Iteration 8418: Policy loss: -0.009051. Value loss: 0.137838. Entropy: 0.875894.\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8419: Policy loss: 0.009742. Value loss: 0.285096. Entropy: 0.944909.\n",
      "Iteration 8420: Policy loss: -0.002995. Value loss: 0.085462. Entropy: 0.930756.\n",
      "Iteration 8421: Policy loss: -0.004680. Value loss: 0.057164. Entropy: 0.956404.\n",
      "episode: 3509   score: 22.0  epsilon: 1.0    steps: 856  evaluation reward: 28.27\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8422: Policy loss: 0.005321. Value loss: 0.845949. Entropy: 0.844900.\n",
      "Iteration 8423: Policy loss: -0.004466. Value loss: 0.336968. Entropy: 0.837278.\n",
      "Iteration 8424: Policy loss: -0.014448. Value loss: 0.199772. Entropy: 0.849641.\n",
      "episode: 3510   score: 52.0  epsilon: 1.0    steps: 264  evaluation reward: 28.65\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8425: Policy loss: 0.010155. Value loss: 0.559995. Entropy: 0.980087.\n",
      "Iteration 8426: Policy loss: -0.003409. Value loss: 0.290024. Entropy: 0.985068.\n",
      "Iteration 8427: Policy loss: -0.011892. Value loss: 0.166547. Entropy: 0.983585.\n",
      "episode: 3511   score: 34.0  epsilon: 1.0    steps: 456  evaluation reward: 28.61\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8428: Policy loss: 0.008589. Value loss: 0.674150. Entropy: 0.973663.\n",
      "Iteration 8429: Policy loss: -0.000968. Value loss: 0.309895. Entropy: 0.962619.\n",
      "Iteration 8430: Policy loss: -0.015543. Value loss: 0.171209. Entropy: 0.963299.\n",
      "episode: 3512   score: 16.0  epsilon: 1.0    steps: 224  evaluation reward: 28.61\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8431: Policy loss: 0.013473. Value loss: 0.710444. Entropy: 0.985171.\n",
      "Iteration 8432: Policy loss: -0.000693. Value loss: 0.345455. Entropy: 0.987920.\n",
      "Iteration 8433: Policy loss: -0.012342. Value loss: 0.169708. Entropy: 0.995469.\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8434: Policy loss: 0.013768. Value loss: 0.685867. Entropy: 0.999983.\n",
      "Iteration 8435: Policy loss: -0.005574. Value loss: 0.291618. Entropy: 0.988654.\n",
      "Iteration 8436: Policy loss: -0.018482. Value loss: 0.150035. Entropy: 0.983124.\n",
      "episode: 3513   score: 17.0  epsilon: 1.0    steps: 72  evaluation reward: 28.55\n",
      "episode: 3514   score: 34.0  epsilon: 1.0    steps: 280  evaluation reward: 28.69\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8437: Policy loss: 0.032878. Value loss: 0.942424. Entropy: 0.923054.\n",
      "Iteration 8438: Policy loss: 0.007604. Value loss: 0.520508. Entropy: 0.918992.\n",
      "Iteration 8439: Policy loss: -0.005775. Value loss: 0.327397. Entropy: 0.890593.\n",
      "episode: 3515   score: 26.0  epsilon: 1.0    steps: 816  evaluation reward: 28.73\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8440: Policy loss: 0.006681. Value loss: 0.502099. Entropy: 0.999341.\n",
      "Iteration 8441: Policy loss: -0.005104. Value loss: 0.199125. Entropy: 1.000290.\n",
      "Iteration 8442: Policy loss: -0.014183. Value loss: 0.112594. Entropy: 0.987235.\n",
      "episode: 3516   score: 26.0  epsilon: 1.0    steps: 552  evaluation reward: 28.65\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8443: Policy loss: 0.005889. Value loss: 0.307287. Entropy: 1.023190.\n",
      "Iteration 8444: Policy loss: -0.008685. Value loss: 0.103416. Entropy: 1.032307.\n",
      "Iteration 8445: Policy loss: -0.020000. Value loss: 0.069106. Entropy: 1.034412.\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8446: Policy loss: 0.005103. Value loss: 0.917721. Entropy: 1.006673.\n",
      "Iteration 8447: Policy loss: -0.004718. Value loss: 0.460413. Entropy: 0.987851.\n",
      "Iteration 8448: Policy loss: -0.004030. Value loss: 0.226792. Entropy: 0.989182.\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8449: Policy loss: 0.014090. Value loss: 0.475045. Entropy: 1.057109.\n",
      "Iteration 8450: Policy loss: -0.002779. Value loss: 0.211113. Entropy: 1.048304.\n",
      "Iteration 8451: Policy loss: -0.014392. Value loss: 0.114614. Entropy: 1.040764.\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8452: Policy loss: 0.004193. Value loss: 0.892341. Entropy: 0.984800.\n",
      "Iteration 8453: Policy loss: -0.000829. Value loss: 0.461686. Entropy: 0.972594.\n",
      "Iteration 8454: Policy loss: -0.008124. Value loss: 0.254106. Entropy: 0.972030.\n",
      "episode: 3517   score: 34.0  epsilon: 1.0    steps: 104  evaluation reward: 28.89\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8455: Policy loss: 0.009705. Value loss: 0.779926. Entropy: 0.956939.\n",
      "Iteration 8456: Policy loss: -0.005659. Value loss: 0.338962. Entropy: 0.953704.\n",
      "Iteration 8457: Policy loss: -0.016892. Value loss: 0.195059. Entropy: 0.943713.\n",
      "episode: 3518   score: 37.0  epsilon: 1.0    steps: 56  evaluation reward: 28.83\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8458: Policy loss: 0.012894. Value loss: 0.783079. Entropy: 0.975723.\n",
      "Iteration 8459: Policy loss: 0.005222. Value loss: 0.351070. Entropy: 0.966228.\n",
      "Iteration 8460: Policy loss: -0.008338. Value loss: 0.192339. Entropy: 0.958596.\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8461: Policy loss: 0.015226. Value loss: 0.825307. Entropy: 0.973126.\n",
      "Iteration 8462: Policy loss: -0.000578. Value loss: 0.322471. Entropy: 0.975886.\n",
      "Iteration 8463: Policy loss: -0.011493. Value loss: 0.197230. Entropy: 0.979701.\n",
      "episode: 3519   score: 27.0  epsilon: 1.0    steps: 248  evaluation reward: 28.75\n",
      "episode: 3520   score: 21.0  epsilon: 1.0    steps: 560  evaluation reward: 28.69\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8464: Policy loss: 0.015037. Value loss: 0.947100. Entropy: 0.941318.\n",
      "Iteration 8465: Policy loss: 0.002709. Value loss: 0.330087. Entropy: 0.922136.\n",
      "Iteration 8466: Policy loss: -0.010567. Value loss: 0.166495. Entropy: 0.920568.\n",
      "episode: 3521   score: 59.0  epsilon: 1.0    steps: 40  evaluation reward: 28.97\n",
      "episode: 3522   score: 37.0  epsilon: 1.0    steps: 176  evaluation reward: 28.91\n",
      "episode: 3523   score: 24.0  epsilon: 1.0    steps: 536  evaluation reward: 28.85\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8467: Policy loss: 0.007075. Value loss: 0.454460. Entropy: 0.951827.\n",
      "Iteration 8468: Policy loss: -0.000700. Value loss: 0.170512. Entropy: 0.934306.\n",
      "Iteration 8469: Policy loss: -0.011604. Value loss: 0.094543. Entropy: 0.947676.\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8470: Policy loss: 0.006322. Value loss: 0.586999. Entropy: 0.995037.\n",
      "Iteration 8471: Policy loss: -0.007912. Value loss: 0.298084. Entropy: 0.999089.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8472: Policy loss: -0.017946. Value loss: 0.171166. Entropy: 0.986867.\n",
      "episode: 3524   score: 12.0  epsilon: 1.0    steps: 448  evaluation reward: 28.67\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8473: Policy loss: 0.011788. Value loss: 0.724650. Entropy: 0.804689.\n",
      "Iteration 8474: Policy loss: 0.002653. Value loss: 0.256597. Entropy: 0.784120.\n",
      "Iteration 8475: Policy loss: -0.014581. Value loss: 0.151433. Entropy: 0.787788.\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8476: Policy loss: 0.008300. Value loss: 0.653821. Entropy: 0.924864.\n",
      "Iteration 8477: Policy loss: -0.002220. Value loss: 0.346122. Entropy: 0.913114.\n",
      "Iteration 8478: Policy loss: -0.010467. Value loss: 0.222136. Entropy: 0.925982.\n",
      "episode: 3525   score: 57.0  epsilon: 1.0    steps: 152  evaluation reward: 28.91\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8479: Policy loss: 0.014582. Value loss: 0.690597. Entropy: 0.955083.\n",
      "Iteration 8480: Policy loss: -0.010050. Value loss: 0.256166. Entropy: 0.954098.\n",
      "Iteration 8481: Policy loss: -0.020653. Value loss: 0.156816. Entropy: 0.946036.\n",
      "episode: 3526   score: 12.0  epsilon: 1.0    steps: 344  evaluation reward: 28.72\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8482: Policy loss: 0.011817. Value loss: 0.531944. Entropy: 0.966544.\n",
      "Iteration 8483: Policy loss: -0.008037. Value loss: 0.188092. Entropy: 0.972567.\n",
      "Iteration 8484: Policy loss: -0.016667. Value loss: 0.107700. Entropy: 0.969580.\n",
      "episode: 3527   score: 35.0  epsilon: 1.0    steps: 928  evaluation reward: 28.82\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8485: Policy loss: 0.018084. Value loss: 0.773176. Entropy: 0.912459.\n",
      "Iteration 8486: Policy loss: -0.002969. Value loss: 0.319195. Entropy: 0.907365.\n",
      "Iteration 8487: Policy loss: -0.013096. Value loss: 0.204758. Entropy: 0.913453.\n",
      "episode: 3528   score: 20.0  epsilon: 1.0    steps: 760  evaluation reward: 28.81\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8488: Policy loss: 0.005780. Value loss: 0.474666. Entropy: 0.978643.\n",
      "Iteration 8489: Policy loss: -0.000556. Value loss: 0.185034. Entropy: 0.995986.\n",
      "Iteration 8490: Policy loss: -0.014431. Value loss: 0.095236. Entropy: 0.991025.\n",
      "episode: 3529   score: 31.0  epsilon: 1.0    steps: 960  evaluation reward: 28.75\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8491: Policy loss: 0.011440. Value loss: 0.982258. Entropy: 0.979116.\n",
      "Iteration 8492: Policy loss: -0.002181. Value loss: 0.452663. Entropy: 0.956840.\n",
      "Iteration 8493: Policy loss: -0.012912. Value loss: 0.267575. Entropy: 0.981846.\n",
      "episode: 3530   score: 10.0  epsilon: 1.0    steps: 328  evaluation reward: 28.52\n",
      "episode: 3531   score: 15.0  epsilon: 1.0    steps: 512  evaluation reward: 28.49\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8494: Policy loss: 0.010129. Value loss: 0.825189. Entropy: 0.934671.\n",
      "Iteration 8495: Policy loss: -0.005156. Value loss: 0.375847. Entropy: 0.938010.\n",
      "Iteration 8496: Policy loss: -0.013626. Value loss: 0.229294. Entropy: 0.932863.\n",
      "episode: 3532   score: 35.0  epsilon: 1.0    steps: 520  evaluation reward: 28.43\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8497: Policy loss: 0.007942. Value loss: 0.483936. Entropy: 1.032357.\n",
      "Iteration 8498: Policy loss: 0.002058. Value loss: 0.240435. Entropy: 1.018925.\n",
      "Iteration 8499: Policy loss: -0.012240. Value loss: 0.162001. Entropy: 1.006110.\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8500: Policy loss: 0.013028. Value loss: 0.414114. Entropy: 0.979064.\n",
      "Iteration 8501: Policy loss: -0.005047. Value loss: 0.204143. Entropy: 0.974359.\n",
      "Iteration 8502: Policy loss: -0.010537. Value loss: 0.106944. Entropy: 0.974548.\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8503: Policy loss: 0.014767. Value loss: 0.736326. Entropy: 0.960725.\n",
      "Iteration 8504: Policy loss: -0.005894. Value loss: 0.286086. Entropy: 0.957088.\n",
      "Iteration 8505: Policy loss: -0.009030. Value loss: 0.159771. Entropy: 0.950580.\n",
      "episode: 3533   score: 43.0  epsilon: 1.0    steps: 168  evaluation reward: 28.69\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8506: Policy loss: 0.016263. Value loss: 0.772655. Entropy: 0.972104.\n",
      "Iteration 8507: Policy loss: -0.001581. Value loss: 0.305212. Entropy: 0.974701.\n",
      "Iteration 8508: Policy loss: -0.012355. Value loss: 0.168947. Entropy: 0.977870.\n",
      "episode: 3534   score: 9.0  epsilon: 1.0    steps: 584  evaluation reward: 28.4\n",
      "episode: 3535   score: 25.0  epsilon: 1.0    steps: 608  evaluation reward: 28.46\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8509: Policy loss: 0.006590. Value loss: 0.608192. Entropy: 0.991277.\n",
      "Iteration 8510: Policy loss: 0.001377. Value loss: 0.248245. Entropy: 0.980560.\n",
      "Iteration 8511: Policy loss: -0.011719. Value loss: 0.124579. Entropy: 0.988534.\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8512: Policy loss: 0.012371. Value loss: 0.870774. Entropy: 0.929205.\n",
      "Iteration 8513: Policy loss: -0.003615. Value loss: 0.426248. Entropy: 0.927954.\n",
      "Iteration 8514: Policy loss: -0.015693. Value loss: 0.243718. Entropy: 0.920769.\n",
      "episode: 3536   score: 40.0  epsilon: 1.0    steps: 16  evaluation reward: 28.74\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8515: Policy loss: 0.016085. Value loss: 0.674266. Entropy: 0.976237.\n",
      "Iteration 8516: Policy loss: -0.001525. Value loss: 0.276939. Entropy: 0.992076.\n",
      "Iteration 8517: Policy loss: -0.014455. Value loss: 0.156558. Entropy: 0.987682.\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8518: Policy loss: 0.010229. Value loss: 0.835335. Entropy: 0.977324.\n",
      "Iteration 8519: Policy loss: -0.006189. Value loss: 0.356267. Entropy: 0.990216.\n",
      "Iteration 8520: Policy loss: -0.013956. Value loss: 0.189195. Entropy: 0.990635.\n",
      "episode: 3537   score: 25.0  epsilon: 1.0    steps: 24  evaluation reward: 28.64\n",
      "episode: 3538   score: 35.0  epsilon: 1.0    steps: 184  evaluation reward: 28.77\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8521: Policy loss: 0.013687. Value loss: 0.802294. Entropy: 1.082882.\n",
      "Iteration 8522: Policy loss: 0.005862. Value loss: 0.282670. Entropy: 1.076237.\n",
      "Iteration 8523: Policy loss: -0.007230. Value loss: 0.177120. Entropy: 1.075498.\n",
      "episode: 3539   score: 11.0  epsilon: 1.0    steps: 128  evaluation reward: 28.57\n",
      "episode: 3540   score: 35.0  epsilon: 1.0    steps: 576  evaluation reward: 28.74\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8524: Policy loss: 0.011981. Value loss: 0.527846. Entropy: 1.035669.\n",
      "Iteration 8525: Policy loss: -0.005700. Value loss: 0.257730. Entropy: 1.029369.\n",
      "Iteration 8526: Policy loss: -0.015633. Value loss: 0.137571. Entropy: 1.010740.\n",
      "episode: 3541   score: 34.0  epsilon: 1.0    steps: 536  evaluation reward: 28.8\n",
      "episode: 3542   score: 23.0  epsilon: 1.0    steps: 944  evaluation reward: 28.78\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8527: Policy loss: 0.012769. Value loss: 0.801130. Entropy: 0.961021.\n",
      "Iteration 8528: Policy loss: -0.002835. Value loss: 0.383029. Entropy: 0.955656.\n",
      "Iteration 8529: Policy loss: -0.012905. Value loss: 0.232723. Entropy: 0.965574.\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8530: Policy loss: 0.015538. Value loss: 0.403406. Entropy: 0.984614.\n",
      "Iteration 8531: Policy loss: -0.005256. Value loss: 0.154021. Entropy: 0.979626.\n",
      "Iteration 8532: Policy loss: -0.021396. Value loss: 0.078386. Entropy: 0.977238.\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8533: Policy loss: 0.021553. Value loss: 0.861095. Entropy: 1.026123.\n",
      "Iteration 8534: Policy loss: 0.003431. Value loss: 0.407847. Entropy: 1.015400.\n",
      "Iteration 8535: Policy loss: -0.004291. Value loss: 0.253898. Entropy: 1.007173.\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8536: Policy loss: 0.014298. Value loss: 0.454213. Entropy: 1.036723.\n",
      "Iteration 8537: Policy loss: -0.004491. Value loss: 0.172217. Entropy: 1.053809.\n",
      "Iteration 8538: Policy loss: -0.019428. Value loss: 0.110721. Entropy: 1.047284.\n",
      "episode: 3543   score: 30.0  epsilon: 1.0    steps: 64  evaluation reward: 28.95\n",
      "episode: 3544   score: 31.0  epsilon: 1.0    steps: 384  evaluation reward: 29.04\n",
      "Training network. lr: 0.000185. clip: 0.073891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8539: Policy loss: 0.012817. Value loss: 0.784166. Entropy: 1.016189.\n",
      "Iteration 8540: Policy loss: -0.004077. Value loss: 0.399666. Entropy: 1.003637.\n",
      "Iteration 8541: Policy loss: -0.008356. Value loss: 0.259945. Entropy: 1.008276.\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8542: Policy loss: 0.013371. Value loss: 0.644930. Entropy: 1.008636.\n",
      "Iteration 8543: Policy loss: 0.003548. Value loss: 0.245097. Entropy: 1.021612.\n",
      "Iteration 8544: Policy loss: -0.006762. Value loss: 0.140394. Entropy: 1.033472.\n",
      "episode: 3545   score: 20.0  epsilon: 1.0    steps: 736  evaluation reward: 28.91\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8545: Policy loss: 0.017371. Value loss: 0.727172. Entropy: 1.034252.\n",
      "Iteration 8546: Policy loss: 0.002578. Value loss: 0.259668. Entropy: 1.028399.\n",
      "Iteration 8547: Policy loss: -0.011925. Value loss: 0.138231. Entropy: 1.025430.\n",
      "episode: 3546   score: 24.0  epsilon: 1.0    steps: 368  evaluation reward: 28.97\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8548: Policy loss: 0.005897. Value loss: 0.892325. Entropy: 1.025245.\n",
      "Iteration 8549: Policy loss: 0.003520. Value loss: 0.420530. Entropy: 1.004575.\n",
      "Iteration 8550: Policy loss: -0.004851. Value loss: 0.271789. Entropy: 0.997785.\n",
      "episode: 3547   score: 17.0  epsilon: 1.0    steps: 424  evaluation reward: 28.75\n",
      "episode: 3548   score: 26.0  epsilon: 1.0    steps: 968  evaluation reward: 28.82\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8551: Policy loss: 0.012717. Value loss: 0.923323. Entropy: 0.988367.\n",
      "Iteration 8552: Policy loss: 0.003763. Value loss: 0.448306. Entropy: 0.974138.\n",
      "Iteration 8553: Policy loss: -0.010704. Value loss: 0.267507. Entropy: 0.987739.\n",
      "episode: 3549   score: 32.0  epsilon: 1.0    steps: 320  evaluation reward: 28.92\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8554: Policy loss: 0.020246. Value loss: 0.577583. Entropy: 1.032735.\n",
      "Iteration 8555: Policy loss: -0.000225. Value loss: 0.251871. Entropy: 1.040423.\n",
      "Iteration 8556: Policy loss: -0.016468. Value loss: 0.187045. Entropy: 1.045110.\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8557: Policy loss: 0.005529. Value loss: 0.583750. Entropy: 1.006160.\n",
      "Iteration 8558: Policy loss: -0.008187. Value loss: 0.311692. Entropy: 1.010555.\n",
      "Iteration 8559: Policy loss: -0.016829. Value loss: 0.188006. Entropy: 1.004368.\n",
      "episode: 3550   score: 38.0  epsilon: 1.0    steps: 232  evaluation reward: 28.96\n",
      "now time :  2019-03-06 15:30:36.497013\n",
      "episode: 3551   score: 14.0  epsilon: 1.0    steps: 248  evaluation reward: 28.63\n",
      "episode: 3552   score: 22.0  epsilon: 1.0    steps: 736  evaluation reward: 28.62\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8560: Policy loss: 0.020081. Value loss: 0.643555. Entropy: 1.053751.\n",
      "Iteration 8561: Policy loss: 0.009877. Value loss: 0.270927. Entropy: 1.040451.\n",
      "Iteration 8562: Policy loss: -0.009297. Value loss: 0.176709. Entropy: 1.047327.\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8563: Policy loss: 0.014214. Value loss: 0.620839. Entropy: 1.015765.\n",
      "Iteration 8564: Policy loss: -0.001082. Value loss: 0.258027. Entropy: 1.026607.\n",
      "Iteration 8565: Policy loss: -0.013411. Value loss: 0.179976. Entropy: 1.005888.\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8566: Policy loss: 0.010145. Value loss: 0.612449. Entropy: 0.988590.\n",
      "Iteration 8567: Policy loss: 0.001062. Value loss: 0.260049. Entropy: 0.959608.\n",
      "Iteration 8568: Policy loss: -0.010644. Value loss: 0.156017. Entropy: 0.962200.\n",
      "episode: 3553   score: 24.0  epsilon: 1.0    steps: 272  evaluation reward: 28.35\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8569: Policy loss: 0.007962. Value loss: 0.509638. Entropy: 0.950677.\n",
      "Iteration 8570: Policy loss: -0.000955. Value loss: 0.173436. Entropy: 0.947804.\n",
      "Iteration 8571: Policy loss: -0.014944. Value loss: 0.089498. Entropy: 0.961743.\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8572: Policy loss: 0.006570. Value loss: 0.678069. Entropy: 1.000760.\n",
      "Iteration 8573: Policy loss: -0.001874. Value loss: 0.349178. Entropy: 0.993914.\n",
      "Iteration 8574: Policy loss: -0.015001. Value loss: 0.225640. Entropy: 0.977726.\n",
      "episode: 3554   score: 24.0  epsilon: 1.0    steps: 1000  evaluation reward: 28.39\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8575: Policy loss: 0.018892. Value loss: 0.470369. Entropy: 1.016720.\n",
      "Iteration 8576: Policy loss: 0.000724. Value loss: 0.192799. Entropy: 1.005186.\n",
      "Iteration 8577: Policy loss: -0.009088. Value loss: 0.120679. Entropy: 1.008775.\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8578: Policy loss: 0.009180. Value loss: 0.541267. Entropy: 1.016766.\n",
      "Iteration 8579: Policy loss: -0.002111. Value loss: 0.174939. Entropy: 1.023861.\n",
      "Iteration 8580: Policy loss: -0.016479. Value loss: 0.106868. Entropy: 1.017704.\n",
      "episode: 3555   score: 33.0  epsilon: 1.0    steps: 224  evaluation reward: 28.4\n",
      "episode: 3556   score: 27.0  epsilon: 1.0    steps: 816  evaluation reward: 28.49\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8581: Policy loss: 0.008660. Value loss: 0.797720. Entropy: 1.004798.\n",
      "Iteration 8582: Policy loss: -0.004538. Value loss: 0.313628. Entropy: 0.988903.\n",
      "Iteration 8583: Policy loss: -0.014311. Value loss: 0.163552. Entropy: 0.984164.\n",
      "episode: 3557   score: 31.0  epsilon: 1.0    steps: 104  evaluation reward: 28.62\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8584: Policy loss: 0.005038. Value loss: 0.541336. Entropy: 1.021393.\n",
      "Iteration 8585: Policy loss: -0.007038. Value loss: 0.233759. Entropy: 1.003574.\n",
      "Iteration 8586: Policy loss: -0.011784. Value loss: 0.119643. Entropy: 1.005661.\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8587: Policy loss: 0.010782. Value loss: 0.530165. Entropy: 1.010482.\n",
      "Iteration 8588: Policy loss: 0.000704. Value loss: 0.224114. Entropy: 1.004774.\n",
      "Iteration 8589: Policy loss: -0.012106. Value loss: 0.119744. Entropy: 1.000668.\n",
      "episode: 3558   score: 31.0  epsilon: 1.0    steps: 232  evaluation reward: 28.61\n",
      "episode: 3559   score: 35.0  epsilon: 1.0    steps: 904  evaluation reward: 28.54\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8590: Policy loss: 0.014738. Value loss: 0.442330. Entropy: 1.054358.\n",
      "Iteration 8591: Policy loss: -0.002958. Value loss: 0.166134. Entropy: 1.057656.\n",
      "Iteration 8592: Policy loss: -0.016938. Value loss: 0.098940. Entropy: 1.046774.\n",
      "episode: 3560   score: 24.0  epsilon: 1.0    steps: 88  evaluation reward: 28.29\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8593: Policy loss: 0.015676. Value loss: 0.549450. Entropy: 1.011361.\n",
      "Iteration 8594: Policy loss: 0.000532. Value loss: 0.267539. Entropy: 1.011137.\n",
      "Iteration 8595: Policy loss: -0.009646. Value loss: 0.150790. Entropy: 1.007094.\n",
      "episode: 3561   score: 39.0  epsilon: 1.0    steps: 96  evaluation reward: 28.4\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8596: Policy loss: 0.011432. Value loss: 0.652415. Entropy: 1.006958.\n",
      "Iteration 8597: Policy loss: 0.003465. Value loss: 0.270353. Entropy: 1.003437.\n",
      "Iteration 8598: Policy loss: -0.012322. Value loss: 0.148665. Entropy: 0.998232.\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8599: Policy loss: 0.020221. Value loss: 0.629093. Entropy: 0.969849.\n",
      "Iteration 8600: Policy loss: 0.004441. Value loss: 0.272175. Entropy: 0.970832.\n",
      "Iteration 8601: Policy loss: -0.009533. Value loss: 0.149011. Entropy: 0.967926.\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8602: Policy loss: 0.006861. Value loss: 0.694256. Entropy: 0.982333.\n",
      "Iteration 8603: Policy loss: -0.002358. Value loss: 0.283135. Entropy: 0.989720.\n",
      "Iteration 8604: Policy loss: -0.012330. Value loss: 0.173834. Entropy: 0.989298.\n",
      "episode: 3562   score: 17.0  epsilon: 1.0    steps: 640  evaluation reward: 28.27\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8605: Policy loss: 0.015817. Value loss: 0.659647. Entropy: 1.025878.\n",
      "Iteration 8606: Policy loss: -0.006553. Value loss: 0.280876. Entropy: 1.033448.\n",
      "Iteration 8607: Policy loss: -0.010880. Value loss: 0.186118. Entropy: 1.025183.\n",
      "episode: 3563   score: 30.0  epsilon: 1.0    steps: 72  evaluation reward: 28.31\n",
      "episode: 3564   score: 27.0  epsilon: 1.0    steps: 256  evaluation reward: 28.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3565   score: 16.0  epsilon: 1.0    steps: 352  evaluation reward: 28.01\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8608: Policy loss: 0.011134. Value loss: 0.523998. Entropy: 1.000699.\n",
      "Iteration 8609: Policy loss: -0.002975. Value loss: 0.278724. Entropy: 1.008239.\n",
      "Iteration 8610: Policy loss: -0.013475. Value loss: 0.172735. Entropy: 0.990937.\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8611: Policy loss: 0.004137. Value loss: 0.480991. Entropy: 1.001356.\n",
      "Iteration 8612: Policy loss: -0.007644. Value loss: 0.190317. Entropy: 0.975862.\n",
      "Iteration 8613: Policy loss: -0.017753. Value loss: 0.106675. Entropy: 0.967996.\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8614: Policy loss: 0.011568. Value loss: 0.570336. Entropy: 1.026339.\n",
      "Iteration 8615: Policy loss: -0.005282. Value loss: 0.255239. Entropy: 1.026691.\n",
      "Iteration 8616: Policy loss: -0.010741. Value loss: 0.145734. Entropy: 1.012229.\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8617: Policy loss: 0.008048. Value loss: 0.655455. Entropy: 1.027280.\n",
      "Iteration 8618: Policy loss: -0.001486. Value loss: 0.300871. Entropy: 1.037477.\n",
      "Iteration 8619: Policy loss: -0.011941. Value loss: 0.190428. Entropy: 1.020389.\n",
      "episode: 3566   score: 39.0  epsilon: 1.0    steps: 16  evaluation reward: 28.19\n",
      "episode: 3567   score: 22.0  epsilon: 1.0    steps: 216  evaluation reward: 28.1\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8620: Policy loss: 0.011437. Value loss: 0.576766. Entropy: 1.065498.\n",
      "Iteration 8621: Policy loss: -0.011541. Value loss: 0.257423. Entropy: 1.049997.\n",
      "Iteration 8622: Policy loss: -0.018222. Value loss: 0.169821. Entropy: 1.044526.\n",
      "episode: 3568   score: 17.0  epsilon: 1.0    steps: 536  evaluation reward: 27.97\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8623: Policy loss: 0.009880. Value loss: 0.515373. Entropy: 1.026881.\n",
      "Iteration 8624: Policy loss: 0.001951. Value loss: 0.193137. Entropy: 1.029909.\n",
      "Iteration 8625: Policy loss: -0.012244. Value loss: 0.091868. Entropy: 1.037881.\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8626: Policy loss: 0.017306. Value loss: 0.736509. Entropy: 1.015403.\n",
      "Iteration 8627: Policy loss: 0.000762. Value loss: 0.283329. Entropy: 1.008984.\n",
      "Iteration 8628: Policy loss: -0.004473. Value loss: 0.161272. Entropy: 1.021078.\n",
      "episode: 3569   score: 36.0  epsilon: 1.0    steps: 288  evaluation reward: 28.06\n",
      "episode: 3570   score: 25.0  epsilon: 1.0    steps: 712  evaluation reward: 27.99\n",
      "episode: 3571   score: 42.0  epsilon: 1.0    steps: 776  evaluation reward: 28.08\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8629: Policy loss: 0.005888. Value loss: 0.709881. Entropy: 1.012567.\n",
      "Iteration 8630: Policy loss: -0.000495. Value loss: 0.325168. Entropy: 1.011968.\n",
      "Iteration 8631: Policy loss: -0.009848. Value loss: 0.251454. Entropy: 1.005749.\n",
      "episode: 3572   score: 32.0  epsilon: 1.0    steps: 576  evaluation reward: 28.18\n",
      "episode: 3573   score: 8.0  epsilon: 1.0    steps: 784  evaluation reward: 27.99\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8632: Policy loss: 0.013338. Value loss: 1.119797. Entropy: 1.000714.\n",
      "Iteration 8633: Policy loss: 0.009910. Value loss: 0.504423. Entropy: 0.964084.\n",
      "Iteration 8634: Policy loss: -0.005584. Value loss: 0.265158. Entropy: 0.988780.\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8635: Policy loss: 0.013221. Value loss: 0.850022. Entropy: 1.051654.\n",
      "Iteration 8636: Policy loss: -0.006078. Value loss: 0.361823. Entropy: 1.041140.\n",
      "Iteration 8637: Policy loss: -0.014842. Value loss: 0.226471. Entropy: 1.042236.\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8638: Policy loss: 0.011456. Value loss: 0.656102. Entropy: 1.030131.\n",
      "Iteration 8639: Policy loss: -0.001769. Value loss: 0.279180. Entropy: 1.021836.\n",
      "Iteration 8640: Policy loss: -0.007592. Value loss: 0.128666. Entropy: 1.025538.\n",
      "episode: 3574   score: 23.0  epsilon: 1.0    steps: 88  evaluation reward: 27.94\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8641: Policy loss: 0.008274. Value loss: 0.554992. Entropy: 1.015238.\n",
      "Iteration 8642: Policy loss: -0.003333. Value loss: 0.246903. Entropy: 1.017014.\n",
      "Iteration 8643: Policy loss: -0.015714. Value loss: 0.141832. Entropy: 1.006206.\n",
      "episode: 3575   score: 26.0  epsilon: 1.0    steps: 592  evaluation reward: 27.88\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8644: Policy loss: 0.014173. Value loss: 0.838555. Entropy: 1.009487.\n",
      "Iteration 8645: Policy loss: -0.004457. Value loss: 0.454589. Entropy: 1.009288.\n",
      "Iteration 8646: Policy loss: -0.009616. Value loss: 0.288300. Entropy: 1.001600.\n",
      "episode: 3576   score: 38.0  epsilon: 1.0    steps: 944  evaluation reward: 28.15\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8647: Policy loss: 0.005876. Value loss: 0.439652. Entropy: 1.036258.\n",
      "Iteration 8648: Policy loss: -0.003543. Value loss: 0.220980. Entropy: 1.027343.\n",
      "Iteration 8649: Policy loss: -0.014041. Value loss: 0.133778. Entropy: 1.034911.\n",
      "episode: 3577   score: 28.0  epsilon: 1.0    steps: 784  evaluation reward: 28.03\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8650: Policy loss: 0.016662. Value loss: 0.916942. Entropy: 1.008663.\n",
      "Iteration 8651: Policy loss: 0.003929. Value loss: 0.433097. Entropy: 1.021536.\n",
      "Iteration 8652: Policy loss: -0.006017. Value loss: 0.228673. Entropy: 1.023891.\n",
      "episode: 3578   score: 14.0  epsilon: 1.0    steps: 744  evaluation reward: 27.81\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8653: Policy loss: 0.014300. Value loss: 0.642515. Entropy: 1.080010.\n",
      "Iteration 8654: Policy loss: 0.002443. Value loss: 0.235718. Entropy: 1.069637.\n",
      "Iteration 8655: Policy loss: -0.011499. Value loss: 0.146324. Entropy: 1.064246.\n",
      "episode: 3579   score: 25.0  epsilon: 1.0    steps: 32  evaluation reward: 27.76\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8656: Policy loss: 0.009312. Value loss: 0.832947. Entropy: 1.047026.\n",
      "Iteration 8657: Policy loss: -0.003476. Value loss: 0.296902. Entropy: 1.052924.\n",
      "Iteration 8658: Policy loss: -0.016544. Value loss: 0.181863. Entropy: 1.047931.\n",
      "episode: 3580   score: 24.0  epsilon: 1.0    steps: 824  evaluation reward: 27.6\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8659: Policy loss: 0.006084. Value loss: 0.604714. Entropy: 1.032875.\n",
      "Iteration 8660: Policy loss: -0.011363. Value loss: 0.236548. Entropy: 1.033756.\n",
      "Iteration 8661: Policy loss: -0.021011. Value loss: 0.123561. Entropy: 1.019856.\n",
      "episode: 3581   score: 19.0  epsilon: 1.0    steps: 696  evaluation reward: 27.47\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8662: Policy loss: 0.014607. Value loss: 0.496190. Entropy: 0.993592.\n",
      "Iteration 8663: Policy loss: 0.000800. Value loss: 0.240348. Entropy: 0.994564.\n",
      "Iteration 8664: Policy loss: -0.012404. Value loss: 0.142375. Entropy: 0.989597.\n",
      "episode: 3582   score: 28.0  epsilon: 1.0    steps: 296  evaluation reward: 27.56\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8665: Policy loss: 0.007861. Value loss: 0.641933. Entropy: 0.994640.\n",
      "Iteration 8666: Policy loss: -0.003833. Value loss: 0.339988. Entropy: 0.985399.\n",
      "Iteration 8667: Policy loss: -0.010582. Value loss: 0.185265. Entropy: 0.973455.\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8668: Policy loss: 0.013876. Value loss: 0.502503. Entropy: 0.884194.\n",
      "Iteration 8669: Policy loss: -0.002613. Value loss: 0.128964. Entropy: 0.903444.\n",
      "Iteration 8670: Policy loss: -0.014507. Value loss: 0.067998. Entropy: 0.905797.\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8671: Policy loss: 0.015785. Value loss: 0.690653. Entropy: 1.008119.\n",
      "Iteration 8672: Policy loss: -0.001771. Value loss: 0.273554. Entropy: 1.008772.\n",
      "Iteration 8673: Policy loss: -0.010282. Value loss: 0.119672. Entropy: 1.003948.\n",
      "episode: 3583   score: 25.0  epsilon: 1.0    steps: 152  evaluation reward: 27.42\n",
      "episode: 3584   score: 34.0  epsilon: 1.0    steps: 512  evaluation reward: 27.55\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8674: Policy loss: 0.013484. Value loss: 0.556366. Entropy: 1.052436.\n",
      "Iteration 8675: Policy loss: -0.000974. Value loss: 0.253182. Entropy: 1.032985.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8676: Policy loss: -0.011540. Value loss: 0.168831. Entropy: 1.045890.\n",
      "episode: 3585   score: 21.0  epsilon: 1.0    steps: 144  evaluation reward: 27.43\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8677: Policy loss: 0.015178. Value loss: 0.710279. Entropy: 0.948318.\n",
      "Iteration 8678: Policy loss: 0.004866. Value loss: 0.342215. Entropy: 0.929603.\n",
      "Iteration 8679: Policy loss: -0.001827. Value loss: 0.185320. Entropy: 0.933204.\n",
      "episode: 3586   score: 26.0  epsilon: 1.0    steps: 664  evaluation reward: 27.48\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8680: Policy loss: 0.013582. Value loss: 0.902888. Entropy: 0.976765.\n",
      "Iteration 8681: Policy loss: -0.003353. Value loss: 0.329133. Entropy: 0.973292.\n",
      "Iteration 8682: Policy loss: -0.011684. Value loss: 0.193953. Entropy: 0.981995.\n",
      "episode: 3587   score: 29.0  epsilon: 1.0    steps: 96  evaluation reward: 27.43\n",
      "episode: 3588   score: 26.0  epsilon: 1.0    steps: 920  evaluation reward: 27.43\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8683: Policy loss: 0.010465. Value loss: 0.415061. Entropy: 0.967304.\n",
      "Iteration 8684: Policy loss: 0.000207. Value loss: 0.152905. Entropy: 0.987193.\n",
      "Iteration 8685: Policy loss: -0.010262. Value loss: 0.088028. Entropy: 0.982355.\n",
      "episode: 3589   score: 22.0  epsilon: 1.0    steps: 160  evaluation reward: 27.48\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8686: Policy loss: 0.007224. Value loss: 0.417329. Entropy: 0.956752.\n",
      "Iteration 8687: Policy loss: -0.006349. Value loss: 0.156515. Entropy: 0.953186.\n",
      "Iteration 8688: Policy loss: -0.018399. Value loss: 0.091895. Entropy: 0.957759.\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8689: Policy loss: 0.010585. Value loss: 0.463533. Entropy: 1.037410.\n",
      "Iteration 8690: Policy loss: -0.003241. Value loss: 0.184606. Entropy: 1.043445.\n",
      "Iteration 8691: Policy loss: -0.011495. Value loss: 0.096790. Entropy: 1.035471.\n",
      "episode: 3590   score: 25.0  epsilon: 1.0    steps: 864  evaluation reward: 27.43\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8692: Policy loss: 0.011298. Value loss: 0.684475. Entropy: 1.013269.\n",
      "Iteration 8693: Policy loss: 0.004061. Value loss: 0.273028. Entropy: 1.004931.\n",
      "Iteration 8694: Policy loss: -0.015266. Value loss: 0.141829. Entropy: 1.015289.\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8695: Policy loss: 0.005869. Value loss: 0.481081. Entropy: 1.015443.\n",
      "Iteration 8696: Policy loss: -0.004561. Value loss: 0.186654. Entropy: 1.010751.\n",
      "Iteration 8697: Policy loss: -0.015417. Value loss: 0.104280. Entropy: 1.012497.\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8698: Policy loss: 0.011426. Value loss: 0.516451. Entropy: 0.992938.\n",
      "Iteration 8699: Policy loss: -0.003958. Value loss: 0.201421. Entropy: 1.003782.\n",
      "Iteration 8700: Policy loss: -0.012600. Value loss: 0.112015. Entropy: 1.007877.\n",
      "episode: 3591   score: 27.0  epsilon: 1.0    steps: 32  evaluation reward: 27.37\n",
      "episode: 3592   score: 27.0  epsilon: 1.0    steps: 576  evaluation reward: 27.35\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8701: Policy loss: 0.010841. Value loss: 0.775903. Entropy: 0.950475.\n",
      "Iteration 8702: Policy loss: -0.006721. Value loss: 0.350431. Entropy: 0.945254.\n",
      "Iteration 8703: Policy loss: -0.011835. Value loss: 0.187391. Entropy: 0.939569.\n",
      "episode: 3593   score: 33.0  epsilon: 1.0    steps: 552  evaluation reward: 27.48\n",
      "episode: 3594   score: 23.0  epsilon: 1.0    steps: 848  evaluation reward: 27.44\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8704: Policy loss: 0.012689. Value loss: 0.506981. Entropy: 0.965395.\n",
      "Iteration 8705: Policy loss: -0.001610. Value loss: 0.210407. Entropy: 0.939996.\n",
      "Iteration 8706: Policy loss: -0.017275. Value loss: 0.147469. Entropy: 0.955939.\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8707: Policy loss: 0.012633. Value loss: 0.463678. Entropy: 0.998800.\n",
      "Iteration 8708: Policy loss: 0.000291. Value loss: 0.198199. Entropy: 0.993047.\n",
      "Iteration 8709: Policy loss: -0.011120. Value loss: 0.114234. Entropy: 0.996762.\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8710: Policy loss: 0.026230. Value loss: 0.713881. Entropy: 0.985902.\n",
      "Iteration 8711: Policy loss: -0.002636. Value loss: 0.325873. Entropy: 0.976105.\n",
      "Iteration 8712: Policy loss: -0.008477. Value loss: 0.185530. Entropy: 0.951720.\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8713: Policy loss: 0.010038. Value loss: 0.680017. Entropy: 0.899937.\n",
      "Iteration 8714: Policy loss: -0.006804. Value loss: 0.280441. Entropy: 0.896890.\n",
      "Iteration 8715: Policy loss: -0.017494. Value loss: 0.145573. Entropy: 0.901504.\n",
      "episode: 3595   score: 28.0  epsilon: 1.0    steps: 32  evaluation reward: 27.4\n",
      "episode: 3596   score: 22.0  epsilon: 1.0    steps: 408  evaluation reward: 27.23\n",
      "episode: 3597   score: 13.0  epsilon: 1.0    steps: 656  evaluation reward: 27.08\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8716: Policy loss: 0.016528. Value loss: 0.862008. Entropy: 0.975779.\n",
      "Iteration 8717: Policy loss: 0.006024. Value loss: 0.281409. Entropy: 0.951902.\n",
      "Iteration 8718: Policy loss: -0.006734. Value loss: 0.162937. Entropy: 0.958271.\n",
      "episode: 3598   score: 42.0  epsilon: 1.0    steps: 96  evaluation reward: 27.13\n",
      "episode: 3599   score: 35.0  epsilon: 1.0    steps: 152  evaluation reward: 27.36\n",
      "episode: 3600   score: 17.0  epsilon: 1.0    steps: 200  evaluation reward: 27.23\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8719: Policy loss: 0.005868. Value loss: 0.250582. Entropy: 0.938772.\n",
      "Iteration 8720: Policy loss: -0.011051. Value loss: 0.098708. Entropy: 0.942105.\n",
      "Iteration 8721: Policy loss: -0.021599. Value loss: 0.061821. Entropy: 0.932171.\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8722: Policy loss: 0.004068. Value loss: 0.330355. Entropy: 0.877124.\n",
      "Iteration 8723: Policy loss: -0.010741. Value loss: 0.156909. Entropy: 0.866354.\n",
      "Iteration 8724: Policy loss: -0.019589. Value loss: 0.095667. Entropy: 0.874509.\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8725: Policy loss: 0.009258. Value loss: 0.382838. Entropy: 0.967666.\n",
      "Iteration 8726: Policy loss: -0.006878. Value loss: 0.167219. Entropy: 0.967828.\n",
      "Iteration 8727: Policy loss: -0.012997. Value loss: 0.106907. Entropy: 0.956780.\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8728: Policy loss: 0.010374. Value loss: 0.507289. Entropy: 1.008506.\n",
      "Iteration 8729: Policy loss: -0.005885. Value loss: 0.240349. Entropy: 1.013445.\n",
      "Iteration 8730: Policy loss: -0.012282. Value loss: 0.136278. Entropy: 1.006495.\n",
      "now time :  2019-03-06 15:34:15.216735\n",
      "episode: 3601   score: 11.0  epsilon: 1.0    steps: 1016  evaluation reward: 26.89\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8731: Policy loss: 0.017399. Value loss: 0.647827. Entropy: 0.992664.\n",
      "Iteration 8732: Policy loss: 0.003745. Value loss: 0.304947. Entropy: 0.970150.\n",
      "Iteration 8733: Policy loss: -0.011067. Value loss: 0.175254. Entropy: 0.973516.\n",
      "episode: 3602   score: 21.0  epsilon: 1.0    steps: 104  evaluation reward: 26.75\n",
      "episode: 3603   score: 12.0  epsilon: 1.0    steps: 928  evaluation reward: 26.74\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8734: Policy loss: 0.009604. Value loss: 0.502571. Entropy: 0.965996.\n",
      "Iteration 8735: Policy loss: -0.007187. Value loss: 0.221937. Entropy: 0.961789.\n",
      "Iteration 8736: Policy loss: -0.014233. Value loss: 0.129206. Entropy: 0.964343.\n",
      "episode: 3604   score: 40.0  epsilon: 1.0    steps: 768  evaluation reward: 27.03\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8737: Policy loss: 0.019608. Value loss: 0.682165. Entropy: 0.873683.\n",
      "Iteration 8738: Policy loss: 0.001898. Value loss: 0.269256. Entropy: 0.863776.\n",
      "Iteration 8739: Policy loss: -0.011235. Value loss: 0.160250. Entropy: 0.875525.\n",
      "episode: 3605   score: 24.0  epsilon: 1.0    steps: 120  evaluation reward: 27.1\n",
      "episode: 3606   score: 27.0  epsilon: 1.0    steps: 944  evaluation reward: 27.06\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8740: Policy loss: 0.011967. Value loss: 0.541125. Entropy: 0.877900.\n",
      "Iteration 8741: Policy loss: -0.005044. Value loss: 0.262001. Entropy: 0.868215.\n",
      "Iteration 8742: Policy loss: -0.008540. Value loss: 0.189305. Entropy: 0.848314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3607   score: 13.0  epsilon: 1.0    steps: 656  evaluation reward: 26.87\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8743: Policy loss: 0.008007. Value loss: 0.613813. Entropy: 0.928632.\n",
      "Iteration 8744: Policy loss: -0.004064. Value loss: 0.311022. Entropy: 0.927463.\n",
      "Iteration 8745: Policy loss: -0.015966. Value loss: 0.195098. Entropy: 0.924256.\n",
      "episode: 3608   score: 29.0  epsilon: 1.0    steps: 312  evaluation reward: 26.83\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8746: Policy loss: 0.015871. Value loss: 0.553333. Entropy: 0.947710.\n",
      "Iteration 8747: Policy loss: 0.008473. Value loss: 0.328924. Entropy: 0.916902.\n",
      "Iteration 8748: Policy loss: -0.012013. Value loss: 0.220891. Entropy: 0.930480.\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8749: Policy loss: 0.011146. Value loss: 0.524387. Entropy: 0.869187.\n",
      "Iteration 8750: Policy loss: -0.000748. Value loss: 0.229895. Entropy: 0.885970.\n",
      "Iteration 8751: Policy loss: -0.017760. Value loss: 0.136265. Entropy: 0.879797.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8752: Policy loss: 0.006680. Value loss: 0.469947. Entropy: 0.985498.\n",
      "Iteration 8753: Policy loss: -0.003765. Value loss: 0.228192. Entropy: 0.974922.\n",
      "Iteration 8754: Policy loss: -0.013462. Value loss: 0.117325. Entropy: 0.965538.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8755: Policy loss: 0.010030. Value loss: 0.380771. Entropy: 0.955113.\n",
      "Iteration 8756: Policy loss: 0.001568. Value loss: 0.179921. Entropy: 0.967201.\n",
      "Iteration 8757: Policy loss: -0.017005. Value loss: 0.098890. Entropy: 0.959988.\n",
      "episode: 3609   score: 36.0  epsilon: 1.0    steps: 144  evaluation reward: 26.97\n",
      "episode: 3610   score: 19.0  epsilon: 1.0    steps: 448  evaluation reward: 26.64\n",
      "episode: 3611   score: 23.0  epsilon: 1.0    steps: 576  evaluation reward: 26.53\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8758: Policy loss: 0.012567. Value loss: 0.639578. Entropy: 0.905054.\n",
      "Iteration 8759: Policy loss: -0.004917. Value loss: 0.268807. Entropy: 0.890117.\n",
      "Iteration 8760: Policy loss: -0.007948. Value loss: 0.174227. Entropy: 0.882897.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8761: Policy loss: 0.012507. Value loss: 0.200216. Entropy: 0.906436.\n",
      "Iteration 8762: Policy loss: -0.005568. Value loss: 0.076949. Entropy: 0.907693.\n",
      "Iteration 8763: Policy loss: -0.014347. Value loss: 0.040296. Entropy: 0.900966.\n",
      "episode: 3612   score: 23.0  epsilon: 1.0    steps: 8  evaluation reward: 26.6\n",
      "episode: 3613   score: 17.0  epsilon: 1.0    steps: 552  evaluation reward: 26.6\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8764: Policy loss: 0.007382. Value loss: 0.625400. Entropy: 0.887509.\n",
      "Iteration 8765: Policy loss: 0.002007. Value loss: 0.237334. Entropy: 0.890173.\n",
      "Iteration 8766: Policy loss: -0.009607. Value loss: 0.127465. Entropy: 0.873720.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8767: Policy loss: 0.008765. Value loss: 0.439448. Entropy: 0.905725.\n",
      "Iteration 8768: Policy loss: -0.005222. Value loss: 0.170160. Entropy: 0.903742.\n",
      "Iteration 8769: Policy loss: -0.013954. Value loss: 0.087868. Entropy: 0.907363.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8770: Policy loss: 0.005706. Value loss: 0.353947. Entropy: 0.877850.\n",
      "Iteration 8771: Policy loss: -0.007887. Value loss: 0.116341. Entropy: 0.872182.\n",
      "Iteration 8772: Policy loss: -0.016396. Value loss: 0.070794. Entropy: 0.878136.\n",
      "episode: 3614   score: 34.0  epsilon: 1.0    steps: 728  evaluation reward: 26.6\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8773: Policy loss: 0.021815. Value loss: 0.479178. Entropy: 0.904964.\n",
      "Iteration 8774: Policy loss: 0.003236. Value loss: 0.160401. Entropy: 0.889449.\n",
      "Iteration 8775: Policy loss: -0.006700. Value loss: 0.090714. Entropy: 0.908252.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8776: Policy loss: 0.009895. Value loss: 0.619276. Entropy: 0.953350.\n",
      "Iteration 8777: Policy loss: -0.000150. Value loss: 0.290137. Entropy: 0.949976.\n",
      "Iteration 8778: Policy loss: -0.005918. Value loss: 0.167118. Entropy: 0.942435.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8779: Policy loss: 0.012029. Value loss: 0.450651. Entropy: 0.959971.\n",
      "Iteration 8780: Policy loss: -0.002870. Value loss: 0.137492. Entropy: 0.966385.\n",
      "Iteration 8781: Policy loss: -0.016525. Value loss: 0.081659. Entropy: 0.962412.\n",
      "episode: 3615   score: 35.0  epsilon: 1.0    steps: 16  evaluation reward: 26.69\n",
      "episode: 3616   score: 25.0  epsilon: 1.0    steps: 72  evaluation reward: 26.68\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8782: Policy loss: 0.006501. Value loss: 0.830034. Entropy: 0.978358.\n",
      "Iteration 8783: Policy loss: 0.000877. Value loss: 0.390583. Entropy: 0.969706.\n",
      "Iteration 8784: Policy loss: -0.012075. Value loss: 0.259190. Entropy: 0.966022.\n",
      "episode: 3617   score: 23.0  epsilon: 1.0    steps: 488  evaluation reward: 26.57\n",
      "episode: 3618   score: 20.0  epsilon: 1.0    steps: 752  evaluation reward: 26.4\n",
      "episode: 3619   score: 45.0  epsilon: 1.0    steps: 968  evaluation reward: 26.58\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8785: Policy loss: 0.009051. Value loss: 0.612142. Entropy: 1.040118.\n",
      "Iteration 8786: Policy loss: -0.002063. Value loss: 0.220650. Entropy: 1.033626.\n",
      "Iteration 8787: Policy loss: -0.013970. Value loss: 0.132608. Entropy: 1.032544.\n",
      "episode: 3620   score: 18.0  epsilon: 1.0    steps: 544  evaluation reward: 26.55\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8788: Policy loss: 0.009573. Value loss: 0.490458. Entropy: 1.008072.\n",
      "Iteration 8789: Policy loss: -0.006365. Value loss: 0.229825. Entropy: 0.991089.\n",
      "Iteration 8790: Policy loss: -0.018376. Value loss: 0.141746. Entropy: 0.979817.\n",
      "episode: 3621   score: 19.0  epsilon: 1.0    steps: 136  evaluation reward: 26.15\n",
      "episode: 3622   score: 6.0  epsilon: 1.0    steps: 760  evaluation reward: 25.84\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8791: Policy loss: 0.011860. Value loss: 0.638351. Entropy: 0.925788.\n",
      "Iteration 8792: Policy loss: -0.005580. Value loss: 0.309679. Entropy: 0.931917.\n",
      "Iteration 8793: Policy loss: -0.015243. Value loss: 0.206435. Entropy: 0.920636.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8794: Policy loss: 0.008613. Value loss: 0.455960. Entropy: 0.941843.\n",
      "Iteration 8795: Policy loss: -0.008126. Value loss: 0.204067. Entropy: 0.933243.\n",
      "Iteration 8796: Policy loss: -0.017115. Value loss: 0.095679. Entropy: 0.945239.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8797: Policy loss: 0.015768. Value loss: 0.498388. Entropy: 0.898068.\n",
      "Iteration 8798: Policy loss: -0.005638. Value loss: 0.174473. Entropy: 0.912010.\n",
      "Iteration 8799: Policy loss: -0.015638. Value loss: 0.095610. Entropy: 0.907173.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8800: Policy loss: 0.013983. Value loss: 0.432191. Entropy: 0.913320.\n",
      "Iteration 8801: Policy loss: 0.001416. Value loss: 0.189801. Entropy: 0.920562.\n",
      "Iteration 8802: Policy loss: -0.009624. Value loss: 0.097398. Entropy: 0.927891.\n",
      "episode: 3623   score: 11.0  epsilon: 1.0    steps: 784  evaluation reward: 25.71\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8803: Policy loss: 0.015456. Value loss: 0.732694. Entropy: 0.950506.\n",
      "Iteration 8804: Policy loss: 0.000567. Value loss: 0.336379. Entropy: 0.966529.\n",
      "Iteration 8805: Policy loss: -0.010844. Value loss: 0.228070. Entropy: 0.951860.\n",
      "episode: 3624   score: 30.0  epsilon: 1.0    steps: 48  evaluation reward: 25.89\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8806: Policy loss: 0.018234. Value loss: 0.387256. Entropy: 0.951494.\n",
      "Iteration 8807: Policy loss: -0.004181. Value loss: 0.174233. Entropy: 0.957051.\n",
      "Iteration 8808: Policy loss: -0.018653. Value loss: 0.093973. Entropy: 0.944254.\n",
      "episode: 3625   score: 27.0  epsilon: 1.0    steps: 56  evaluation reward: 25.59\n",
      "episode: 3626   score: 19.0  epsilon: 1.0    steps: 464  evaluation reward: 25.66\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8809: Policy loss: 0.004367. Value loss: 0.668026. Entropy: 0.891343.\n",
      "Iteration 8810: Policy loss: -0.001111. Value loss: 0.218350. Entropy: 0.894651.\n",
      "Iteration 8811: Policy loss: -0.011942. Value loss: 0.107026. Entropy: 0.901109.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3627   score: 22.0  epsilon: 1.0    steps: 552  evaluation reward: 25.53\n",
      "episode: 3628   score: 21.0  epsilon: 1.0    steps: 840  evaluation reward: 25.54\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8812: Policy loss: 0.010446. Value loss: 0.637891. Entropy: 0.895227.\n",
      "Iteration 8813: Policy loss: -0.008887. Value loss: 0.218936. Entropy: 0.906111.\n",
      "Iteration 8814: Policy loss: -0.019798. Value loss: 0.113997. Entropy: 0.915033.\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8815: Policy loss: 0.014002. Value loss: 0.691196. Entropy: 0.987832.\n",
      "Iteration 8816: Policy loss: 0.005564. Value loss: 0.305767. Entropy: 0.965021.\n",
      "Iteration 8817: Policy loss: 0.000186. Value loss: 0.184600. Entropy: 0.996178.\n",
      "episode: 3629   score: 36.0  epsilon: 1.0    steps: 864  evaluation reward: 25.59\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8818: Policy loss: 0.014687. Value loss: 0.462819. Entropy: 0.973454.\n",
      "Iteration 8819: Policy loss: -0.001973. Value loss: 0.232554. Entropy: 0.964338.\n",
      "Iteration 8820: Policy loss: -0.013308. Value loss: 0.129927. Entropy: 0.982140.\n",
      "episode: 3630   score: 32.0  epsilon: 1.0    steps: 296  evaluation reward: 25.81\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8821: Policy loss: 0.002034. Value loss: 0.372279. Entropy: 0.977104.\n",
      "Iteration 8822: Policy loss: -0.008596. Value loss: 0.191951. Entropy: 0.993182.\n",
      "Iteration 8823: Policy loss: -0.017444. Value loss: 0.114905. Entropy: 0.973116.\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8824: Policy loss: 0.004539. Value loss: 0.628401. Entropy: 0.925976.\n",
      "Iteration 8825: Policy loss: -0.008180. Value loss: 0.237747. Entropy: 0.911657.\n",
      "Iteration 8826: Policy loss: -0.015969. Value loss: 0.137223. Entropy: 0.917287.\n",
      "episode: 3631   score: 17.0  epsilon: 1.0    steps: 304  evaluation reward: 25.83\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8827: Policy loss: 0.008146. Value loss: 0.512114. Entropy: 0.994413.\n",
      "Iteration 8828: Policy loss: -0.004334. Value loss: 0.170044. Entropy: 1.008193.\n",
      "Iteration 8829: Policy loss: -0.014710. Value loss: 0.085410. Entropy: 1.009580.\n",
      "episode: 3632   score: 13.0  epsilon: 1.0    steps: 128  evaluation reward: 25.61\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8830: Policy loss: 0.013021. Value loss: 0.457555. Entropy: 0.980774.\n",
      "Iteration 8831: Policy loss: -0.004249. Value loss: 0.173551. Entropy: 0.990779.\n",
      "Iteration 8832: Policy loss: -0.015932. Value loss: 0.095020. Entropy: 0.994615.\n",
      "episode: 3633   score: 26.0  epsilon: 1.0    steps: 480  evaluation reward: 25.44\n",
      "episode: 3634   score: 19.0  epsilon: 1.0    steps: 984  evaluation reward: 25.54\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8833: Policy loss: 0.014790. Value loss: 0.583289. Entropy: 0.908543.\n",
      "Iteration 8834: Policy loss: 0.007275. Value loss: 0.181454. Entropy: 0.939434.\n",
      "Iteration 8835: Policy loss: -0.010106. Value loss: 0.080315. Entropy: 0.912522.\n",
      "episode: 3635   score: 34.0  epsilon: 1.0    steps: 856  evaluation reward: 25.63\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8836: Policy loss: 0.005694. Value loss: 0.341044. Entropy: 0.951379.\n",
      "Iteration 8837: Policy loss: -0.011989. Value loss: 0.144340. Entropy: 0.945260.\n",
      "Iteration 8838: Policy loss: -0.019111. Value loss: 0.067670. Entropy: 0.943073.\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8839: Policy loss: 0.009485. Value loss: 0.614123. Entropy: 0.934603.\n",
      "Iteration 8840: Policy loss: 0.001810. Value loss: 0.192009. Entropy: 0.904779.\n",
      "Iteration 8841: Policy loss: -0.012863. Value loss: 0.126169. Entropy: 0.937221.\n",
      "episode: 3636   score: 19.0  epsilon: 1.0    steps: 256  evaluation reward: 25.42\n",
      "episode: 3637   score: 33.0  epsilon: 1.0    steps: 824  evaluation reward: 25.5\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8842: Policy loss: 0.010118. Value loss: 0.708794. Entropy: 1.019526.\n",
      "Iteration 8843: Policy loss: 0.005446. Value loss: 0.282583. Entropy: 1.013691.\n",
      "Iteration 8844: Policy loss: -0.011125. Value loss: 0.147632. Entropy: 1.013141.\n",
      "episode: 3638   score: 26.0  epsilon: 1.0    steps: 1024  evaluation reward: 25.41\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8845: Policy loss: 0.016763. Value loss: 0.440086. Entropy: 0.926691.\n",
      "Iteration 8846: Policy loss: 0.004963. Value loss: 0.153471. Entropy: 0.929032.\n",
      "Iteration 8847: Policy loss: -0.008502. Value loss: 0.079093. Entropy: 0.928427.\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8848: Policy loss: 0.013345. Value loss: 0.719550. Entropy: 1.008307.\n",
      "Iteration 8849: Policy loss: 0.007003. Value loss: 0.305906. Entropy: 1.002787.\n",
      "Iteration 8850: Policy loss: -0.002893. Value loss: 0.206853. Entropy: 0.997823.\n",
      "episode: 3639   score: 22.0  epsilon: 1.0    steps: 112  evaluation reward: 25.52\n",
      "episode: 3640   score: 21.0  epsilon: 1.0    steps: 792  evaluation reward: 25.38\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8851: Policy loss: 0.008062. Value loss: 0.498719. Entropy: 0.987548.\n",
      "Iteration 8852: Policy loss: -0.009098. Value loss: 0.202507. Entropy: 0.976169.\n",
      "Iteration 8853: Policy loss: -0.020275. Value loss: 0.100072. Entropy: 0.978499.\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8854: Policy loss: 0.005124. Value loss: 0.611334. Entropy: 0.893718.\n",
      "Iteration 8855: Policy loss: -0.007971. Value loss: 0.217315. Entropy: 0.877487.\n",
      "Iteration 8856: Policy loss: -0.015774. Value loss: 0.109673. Entropy: 0.886288.\n",
      "episode: 3641   score: 17.0  epsilon: 1.0    steps: 680  evaluation reward: 25.21\n",
      "episode: 3642   score: 19.0  epsilon: 1.0    steps: 872  evaluation reward: 25.17\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8857: Policy loss: 0.006036. Value loss: 0.829993. Entropy: 0.978525.\n",
      "Iteration 8858: Policy loss: 0.003661. Value loss: 0.354527. Entropy: 0.962437.\n",
      "Iteration 8859: Policy loss: -0.013747. Value loss: 0.218362. Entropy: 0.957168.\n",
      "episode: 3643   score: 25.0  epsilon: 1.0    steps: 280  evaluation reward: 25.12\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8860: Policy loss: 0.010031. Value loss: 0.469145. Entropy: 0.970794.\n",
      "Iteration 8861: Policy loss: -0.008526. Value loss: 0.207814. Entropy: 0.971242.\n",
      "Iteration 8862: Policy loss: -0.013046. Value loss: 0.112517. Entropy: 0.963268.\n",
      "episode: 3644   score: 13.0  epsilon: 1.0    steps: 200  evaluation reward: 24.94\n",
      "episode: 3645   score: 19.0  epsilon: 1.0    steps: 560  evaluation reward: 24.93\n",
      "episode: 3646   score: 14.0  epsilon: 1.0    steps: 704  evaluation reward: 24.83\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8863: Policy loss: 0.016681. Value loss: 0.346892. Entropy: 0.950352.\n",
      "Iteration 8864: Policy loss: -0.009969. Value loss: 0.132426. Entropy: 0.954697.\n",
      "Iteration 8865: Policy loss: -0.019229. Value loss: 0.072302. Entropy: 0.952196.\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8866: Policy loss: 0.014632. Value loss: 0.348824. Entropy: 0.945780.\n",
      "Iteration 8867: Policy loss: -0.002970. Value loss: 0.164422. Entropy: 0.940786.\n",
      "Iteration 8868: Policy loss: -0.013971. Value loss: 0.104370. Entropy: 0.936808.\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8869: Policy loss: 0.009363. Value loss: 0.484260. Entropy: 0.958573.\n",
      "Iteration 8870: Policy loss: -0.005150. Value loss: 0.243471. Entropy: 0.963522.\n",
      "Iteration 8871: Policy loss: -0.013614. Value loss: 0.122026. Entropy: 0.969172.\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8872: Policy loss: 0.008855. Value loss: 0.592717. Entropy: 0.984820.\n",
      "Iteration 8873: Policy loss: -0.002114. Value loss: 0.212141. Entropy: 0.980889.\n",
      "Iteration 8874: Policy loss: -0.013361. Value loss: 0.120695. Entropy: 0.988527.\n",
      "episode: 3647   score: 28.0  epsilon: 1.0    steps: 544  evaluation reward: 24.94\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8875: Policy loss: 0.008963. Value loss: 0.592184. Entropy: 1.026761.\n",
      "Iteration 8876: Policy loss: -0.006057. Value loss: 0.302132. Entropy: 1.021784.\n",
      "Iteration 8877: Policy loss: -0.013364. Value loss: 0.175902. Entropy: 1.017468.\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8878: Policy loss: 0.017832. Value loss: 0.509346. Entropy: 0.912029.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8879: Policy loss: -0.006096. Value loss: 0.170109. Entropy: 0.920041.\n",
      "Iteration 8880: Policy loss: -0.014348. Value loss: 0.084872. Entropy: 0.918893.\n",
      "episode: 3648   score: 17.0  epsilon: 1.0    steps: 232  evaluation reward: 24.85\n",
      "episode: 3649   score: 36.0  epsilon: 1.0    steps: 480  evaluation reward: 24.89\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8881: Policy loss: 0.009405. Value loss: 0.523180. Entropy: 0.957089.\n",
      "Iteration 8882: Policy loss: -0.006802. Value loss: 0.234952. Entropy: 0.944518.\n",
      "Iteration 8883: Policy loss: -0.013762. Value loss: 0.142685. Entropy: 0.946801.\n",
      "episode: 3650   score: 24.0  epsilon: 1.0    steps: 336  evaluation reward: 24.75\n",
      "now time :  2019-03-06 15:37:25.893472\n",
      "episode: 3651   score: 20.0  epsilon: 1.0    steps: 616  evaluation reward: 24.81\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8884: Policy loss: 0.008227. Value loss: 0.384271. Entropy: 0.993419.\n",
      "Iteration 8885: Policy loss: -0.008042. Value loss: 0.171086. Entropy: 1.002593.\n",
      "Iteration 8886: Policy loss: -0.013185. Value loss: 0.102714. Entropy: 0.996620.\n",
      "episode: 3652   score: 21.0  epsilon: 1.0    steps: 128  evaluation reward: 24.8\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8887: Policy loss: 0.009076. Value loss: 0.299955. Entropy: 0.934741.\n",
      "Iteration 8888: Policy loss: -0.006939. Value loss: 0.170200. Entropy: 0.955712.\n",
      "Iteration 8889: Policy loss: -0.016502. Value loss: 0.132573. Entropy: 0.947816.\n",
      "episode: 3653   score: 30.0  epsilon: 1.0    steps: 208  evaluation reward: 24.86\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8890: Policy loss: 0.005022. Value loss: 0.672330. Entropy: 0.915006.\n",
      "Iteration 8891: Policy loss: -0.008024. Value loss: 0.337073. Entropy: 0.906690.\n",
      "Iteration 8892: Policy loss: -0.012895. Value loss: 0.175209. Entropy: 0.928893.\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8893: Policy loss: 0.009592. Value loss: 0.505363. Entropy: 0.893074.\n",
      "Iteration 8894: Policy loss: -0.005375. Value loss: 0.191310. Entropy: 0.899754.\n",
      "Iteration 8895: Policy loss: -0.017622. Value loss: 0.102004. Entropy: 0.915688.\n",
      "episode: 3654   score: 34.0  epsilon: 1.0    steps: 448  evaluation reward: 24.96\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8896: Policy loss: 0.011390. Value loss: 0.589311. Entropy: 0.929212.\n",
      "Iteration 8897: Policy loss: -0.003555. Value loss: 0.210039. Entropy: 0.924523.\n",
      "Iteration 8898: Policy loss: -0.015667. Value loss: 0.125062. Entropy: 0.932723.\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8899: Policy loss: 0.016611. Value loss: 0.332872. Entropy: 1.020026.\n",
      "Iteration 8900: Policy loss: 0.004811. Value loss: 0.133604. Entropy: 1.016525.\n",
      "Iteration 8901: Policy loss: -0.013858. Value loss: 0.061150. Entropy: 1.007571.\n",
      "episode: 3655   score: 26.0  epsilon: 1.0    steps: 504  evaluation reward: 24.89\n",
      "episode: 3656   score: 17.0  epsilon: 1.0    steps: 672  evaluation reward: 24.79\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8902: Policy loss: 0.010661. Value loss: 0.635398. Entropy: 1.030239.\n",
      "Iteration 8903: Policy loss: -0.006376. Value loss: 0.291971. Entropy: 1.035799.\n",
      "Iteration 8904: Policy loss: -0.015023. Value loss: 0.182996. Entropy: 1.034666.\n",
      "episode: 3657   score: 17.0  epsilon: 1.0    steps: 344  evaluation reward: 24.65\n",
      "episode: 3658   score: 25.0  epsilon: 1.0    steps: 1000  evaluation reward: 24.59\n",
      "episode: 3659   score: 26.0  epsilon: 1.0    steps: 1000  evaluation reward: 24.5\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8905: Policy loss: 0.005102. Value loss: 0.476870. Entropy: 1.074331.\n",
      "Iteration 8906: Policy loss: -0.003540. Value loss: 0.193165. Entropy: 1.071666.\n",
      "Iteration 8907: Policy loss: -0.012488. Value loss: 0.131503. Entropy: 1.072607.\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8908: Policy loss: 0.006999. Value loss: 0.456331. Entropy: 0.995911.\n",
      "Iteration 8909: Policy loss: -0.004777. Value loss: 0.156816. Entropy: 0.995185.\n",
      "Iteration 8910: Policy loss: -0.015353. Value loss: 0.097090. Entropy: 1.003736.\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8911: Policy loss: 0.005570. Value loss: 0.639331. Entropy: 1.021370.\n",
      "Iteration 8912: Policy loss: -0.003028. Value loss: 0.312814. Entropy: 1.001979.\n",
      "Iteration 8913: Policy loss: -0.016178. Value loss: 0.182801. Entropy: 1.014886.\n",
      "episode: 3660   score: 17.0  epsilon: 1.0    steps: 464  evaluation reward: 24.43\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8914: Policy loss: 0.006697. Value loss: 0.520592. Entropy: 1.035835.\n",
      "Iteration 8915: Policy loss: -0.004569. Value loss: 0.210794. Entropy: 1.035702.\n",
      "Iteration 8916: Policy loss: -0.014467. Value loss: 0.125453. Entropy: 1.022495.\n",
      "episode: 3661   score: 21.0  epsilon: 1.0    steps: 832  evaluation reward: 24.25\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8917: Policy loss: 0.016238. Value loss: 0.628862. Entropy: 1.009419.\n",
      "Iteration 8918: Policy loss: 0.001594. Value loss: 0.271613. Entropy: 1.021024.\n",
      "Iteration 8919: Policy loss: -0.007030. Value loss: 0.151332. Entropy: 1.000471.\n",
      "episode: 3662   score: 35.0  epsilon: 1.0    steps: 120  evaluation reward: 24.43\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8920: Policy loss: 0.016548. Value loss: 0.433816. Entropy: 1.018305.\n",
      "Iteration 8921: Policy loss: -0.006199. Value loss: 0.193327. Entropy: 1.011944.\n",
      "Iteration 8922: Policy loss: -0.013642. Value loss: 0.108124. Entropy: 1.006994.\n",
      "episode: 3663   score: 20.0  epsilon: 1.0    steps: 600  evaluation reward: 24.33\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8923: Policy loss: 0.014227. Value loss: 0.313957. Entropy: 0.981377.\n",
      "Iteration 8924: Policy loss: -0.011059. Value loss: 0.127495. Entropy: 0.962231.\n",
      "Iteration 8925: Policy loss: -0.020626. Value loss: 0.068676. Entropy: 0.971495.\n",
      "episode: 3664   score: 20.0  epsilon: 1.0    steps: 776  evaluation reward: 24.26\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8926: Policy loss: 0.011212. Value loss: 0.652391. Entropy: 0.963302.\n",
      "Iteration 8927: Policy loss: 0.000536. Value loss: 0.317740. Entropy: 0.941961.\n",
      "Iteration 8928: Policy loss: -0.012228. Value loss: 0.169506. Entropy: 0.935257.\n",
      "episode: 3665   score: 20.0  epsilon: 1.0    steps: 960  evaluation reward: 24.3\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8929: Policy loss: 0.022394. Value loss: 0.768973. Entropy: 0.880961.\n",
      "Iteration 8930: Policy loss: 0.004354. Value loss: 0.289997. Entropy: 0.893483.\n",
      "Iteration 8931: Policy loss: -0.002895. Value loss: 0.162359. Entropy: 0.887382.\n",
      "episode: 3666   score: 26.0  epsilon: 1.0    steps: 200  evaluation reward: 24.17\n",
      "episode: 3667   score: 22.0  epsilon: 1.0    steps: 736  evaluation reward: 24.17\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8932: Policy loss: 0.011497. Value loss: 0.383506. Entropy: 0.965551.\n",
      "Iteration 8933: Policy loss: -0.001649. Value loss: 0.146864. Entropy: 0.969344.\n",
      "Iteration 8934: Policy loss: -0.013346. Value loss: 0.073527. Entropy: 0.956503.\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8935: Policy loss: 0.009816. Value loss: 0.413436. Entropy: 0.981800.\n",
      "Iteration 8936: Policy loss: -0.010351. Value loss: 0.150526. Entropy: 0.968118.\n",
      "Iteration 8937: Policy loss: -0.016634. Value loss: 0.087372. Entropy: 0.982536.\n",
      "episode: 3668   score: 19.0  epsilon: 1.0    steps: 968  evaluation reward: 24.19\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8938: Policy loss: 0.011207. Value loss: 0.559010. Entropy: 0.925402.\n",
      "Iteration 8939: Policy loss: -0.000077. Value loss: 0.206210. Entropy: 0.921185.\n",
      "Iteration 8940: Policy loss: -0.014088. Value loss: 0.126456. Entropy: 0.904948.\n",
      "episode: 3669   score: 21.0  epsilon: 1.0    steps: 584  evaluation reward: 24.04\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8941: Policy loss: 0.006535. Value loss: 0.429029. Entropy: 0.914874.\n",
      "Iteration 8942: Policy loss: -0.002109. Value loss: 0.205296. Entropy: 0.901676.\n",
      "Iteration 8943: Policy loss: -0.008435. Value loss: 0.123430. Entropy: 0.903940.\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8944: Policy loss: 0.014806. Value loss: 0.576388. Entropy: 1.012790.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8945: Policy loss: 0.003785. Value loss: 0.218854. Entropy: 0.992675.\n",
      "Iteration 8946: Policy loss: -0.009197. Value loss: 0.117425. Entropy: 0.994418.\n",
      "episode: 3670   score: 33.0  epsilon: 1.0    steps: 408  evaluation reward: 24.12\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8947: Policy loss: 0.016479. Value loss: 0.514644. Entropy: 0.982996.\n",
      "Iteration 8948: Policy loss: 0.006332. Value loss: 0.163292. Entropy: 0.985094.\n",
      "Iteration 8949: Policy loss: -0.009676. Value loss: 0.079193. Entropy: 0.991076.\n",
      "episode: 3671   score: 30.0  epsilon: 1.0    steps: 1024  evaluation reward: 24.0\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8950: Policy loss: 0.015254. Value loss: 0.762794. Entropy: 0.971774.\n",
      "Iteration 8951: Policy loss: 0.002450. Value loss: 0.343281. Entropy: 0.970975.\n",
      "Iteration 8952: Policy loss: -0.006596. Value loss: 0.175743. Entropy: 0.957067.\n",
      "episode: 3672   score: 15.0  epsilon: 1.0    steps: 168  evaluation reward: 23.83\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8953: Policy loss: 0.014772. Value loss: 0.644310. Entropy: 0.895528.\n",
      "Iteration 8954: Policy loss: 0.000922. Value loss: 0.279101. Entropy: 0.871941.\n",
      "Iteration 8955: Policy loss: -0.013162. Value loss: 0.179111. Entropy: 0.874418.\n",
      "episode: 3673   score: 31.0  epsilon: 1.0    steps: 152  evaluation reward: 24.06\n",
      "episode: 3674   score: 26.0  epsilon: 1.0    steps: 608  evaluation reward: 24.09\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8956: Policy loss: 0.010730. Value loss: 0.545075. Entropy: 0.979759.\n",
      "Iteration 8957: Policy loss: -0.004526. Value loss: 0.289997. Entropy: 0.992310.\n",
      "Iteration 8958: Policy loss: -0.010795. Value loss: 0.199176. Entropy: 0.989021.\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8959: Policy loss: 0.011796. Value loss: 0.465735. Entropy: 0.917894.\n",
      "Iteration 8960: Policy loss: -0.004312. Value loss: 0.198702. Entropy: 0.909016.\n",
      "Iteration 8961: Policy loss: -0.016358. Value loss: 0.124051. Entropy: 0.911013.\n",
      "episode: 3675   score: 26.0  epsilon: 1.0    steps: 728  evaluation reward: 24.09\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8962: Policy loss: 0.017650. Value loss: 0.741125. Entropy: 0.972656.\n",
      "Iteration 8963: Policy loss: -0.001460. Value loss: 0.263501. Entropy: 0.931568.\n",
      "Iteration 8964: Policy loss: -0.017541. Value loss: 0.144064. Entropy: 0.952489.\n",
      "episode: 3676   score: 31.0  epsilon: 1.0    steps: 80  evaluation reward: 24.02\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8965: Policy loss: 0.013113. Value loss: 0.234777. Entropy: 0.998322.\n",
      "Iteration 8966: Policy loss: -0.005927. Value loss: 0.085202. Entropy: 0.993376.\n",
      "Iteration 8967: Policy loss: -0.019994. Value loss: 0.044458. Entropy: 0.991543.\n",
      "episode: 3677   score: 28.0  epsilon: 1.0    steps: 112  evaluation reward: 24.02\n",
      "episode: 3678   score: 21.0  epsilon: 1.0    steps: 456  evaluation reward: 24.09\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8968: Policy loss: 0.014225. Value loss: 0.515063. Entropy: 0.989620.\n",
      "Iteration 8969: Policy loss: -0.004245. Value loss: 0.225267. Entropy: 0.969114.\n",
      "Iteration 8970: Policy loss: -0.016631. Value loss: 0.130349. Entropy: 0.978490.\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8971: Policy loss: 0.013801. Value loss: 0.441058. Entropy: 1.023843.\n",
      "Iteration 8972: Policy loss: 0.000660. Value loss: 0.147314. Entropy: 1.013009.\n",
      "Iteration 8973: Policy loss: -0.006581. Value loss: 0.078892. Entropy: 1.012473.\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8974: Policy loss: 0.008864. Value loss: 0.613046. Entropy: 0.934676.\n",
      "Iteration 8975: Policy loss: -0.004512. Value loss: 0.262626. Entropy: 0.929206.\n",
      "Iteration 8976: Policy loss: -0.017640. Value loss: 0.156314. Entropy: 0.919025.\n",
      "episode: 3679   score: 20.0  epsilon: 1.0    steps: 280  evaluation reward: 24.04\n",
      "episode: 3680   score: 19.0  epsilon: 1.0    steps: 576  evaluation reward: 23.99\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8977: Policy loss: 0.013636. Value loss: 0.454630. Entropy: 0.999725.\n",
      "Iteration 8978: Policy loss: -0.000774. Value loss: 0.200251. Entropy: 0.993352.\n",
      "Iteration 8979: Policy loss: -0.011350. Value loss: 0.112506. Entropy: 0.983818.\n",
      "episode: 3681   score: 23.0  epsilon: 1.0    steps: 72  evaluation reward: 24.03\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8980: Policy loss: 0.012919. Value loss: 0.523946. Entropy: 0.927787.\n",
      "Iteration 8981: Policy loss: -0.006028. Value loss: 0.259729. Entropy: 0.926359.\n",
      "Iteration 8982: Policy loss: -0.010934. Value loss: 0.164051. Entropy: 0.915709.\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8983: Policy loss: 0.016678. Value loss: 0.386545. Entropy: 0.914043.\n",
      "Iteration 8984: Policy loss: -0.005716. Value loss: 0.141468. Entropy: 0.909049.\n",
      "Iteration 8985: Policy loss: -0.014033. Value loss: 0.068437. Entropy: 0.899495.\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8986: Policy loss: 0.009573. Value loss: 0.581024. Entropy: 0.926843.\n",
      "Iteration 8987: Policy loss: -0.005276. Value loss: 0.209553. Entropy: 0.897608.\n",
      "Iteration 8988: Policy loss: -0.017766. Value loss: 0.094133. Entropy: 0.896096.\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8989: Policy loss: 0.020042. Value loss: 0.512948. Entropy: 0.892981.\n",
      "Iteration 8990: Policy loss: -0.001737. Value loss: 0.193839. Entropy: 0.914469.\n",
      "Iteration 8991: Policy loss: -0.014053. Value loss: 0.086094. Entropy: 0.909507.\n",
      "episode: 3682   score: 42.0  epsilon: 1.0    steps: 96  evaluation reward: 24.17\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8992: Policy loss: 0.011540. Value loss: 0.664065. Entropy: 0.996908.\n",
      "Iteration 8993: Policy loss: -0.002545. Value loss: 0.305675. Entropy: 0.983995.\n",
      "Iteration 8994: Policy loss: -0.007164. Value loss: 0.169925. Entropy: 1.002394.\n",
      "episode: 3683   score: 31.0  epsilon: 1.0    steps: 256  evaluation reward: 24.23\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8995: Policy loss: 0.018997. Value loss: 0.672096. Entropy: 0.990658.\n",
      "Iteration 8996: Policy loss: 0.011883. Value loss: 0.258697. Entropy: 0.988980.\n",
      "Iteration 8997: Policy loss: -0.007999. Value loss: 0.151487. Entropy: 0.986867.\n",
      "episode: 3684   score: 26.0  epsilon: 1.0    steps: 264  evaluation reward: 24.15\n",
      "episode: 3685   score: 41.0  epsilon: 1.0    steps: 392  evaluation reward: 24.35\n",
      "episode: 3686   score: 34.0  epsilon: 1.0    steps: 488  evaluation reward: 24.43\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8998: Policy loss: 0.010413. Value loss: 0.811334. Entropy: 0.964481.\n",
      "Iteration 8999: Policy loss: -0.000648. Value loss: 0.394248. Entropy: 0.963366.\n",
      "Iteration 9000: Policy loss: -0.008362. Value loss: 0.266972. Entropy: 0.954802.\n",
      "episode: 3687   score: 24.0  epsilon: 1.0    steps: 496  evaluation reward: 24.38\n",
      "episode: 3688   score: 28.0  epsilon: 1.0    steps: 976  evaluation reward: 24.4\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9001: Policy loss: 0.015285. Value loss: 0.922184. Entropy: 0.924943.\n",
      "Iteration 9002: Policy loss: -0.000601. Value loss: 0.507451. Entropy: 0.918682.\n",
      "Iteration 9003: Policy loss: -0.008608. Value loss: 0.364893. Entropy: 0.919895.\n",
      "episode: 3689   score: 26.0  epsilon: 1.0    steps: 808  evaluation reward: 24.44\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9004: Policy loss: 0.010736. Value loss: 0.392505. Entropy: 0.973144.\n",
      "Iteration 9005: Policy loss: -0.006246. Value loss: 0.188038. Entropy: 0.968796.\n",
      "Iteration 9006: Policy loss: -0.016880. Value loss: 0.111741. Entropy: 0.977142.\n",
      "episode: 3690   score: 6.0  epsilon: 1.0    steps: 568  evaluation reward: 24.25\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9007: Policy loss: 0.012076. Value loss: 0.721354. Entropy: 1.036889.\n",
      "Iteration 9008: Policy loss: -0.003714. Value loss: 0.382843. Entropy: 1.028381.\n",
      "Iteration 9009: Policy loss: -0.014299. Value loss: 0.225909. Entropy: 1.029085.\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9010: Policy loss: 0.009199. Value loss: 0.337383. Entropy: 0.995876.\n",
      "Iteration 9011: Policy loss: -0.005024. Value loss: 0.141870. Entropy: 0.995041.\n",
      "Iteration 9012: Policy loss: -0.018191. Value loss: 0.077511. Entropy: 0.985138.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9013: Policy loss: 0.013350. Value loss: 0.589880. Entropy: 0.983152.\n",
      "Iteration 9014: Policy loss: -0.006532. Value loss: 0.212098. Entropy: 1.000438.\n",
      "Iteration 9015: Policy loss: -0.018807. Value loss: 0.107384. Entropy: 0.987745.\n",
      "episode: 3691   score: 7.0  epsilon: 1.0    steps: 32  evaluation reward: 24.05\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9016: Policy loss: 0.013640. Value loss: 0.537896. Entropy: 1.001923.\n",
      "Iteration 9017: Policy loss: -0.005636. Value loss: 0.213197. Entropy: 1.008900.\n",
      "Iteration 9018: Policy loss: -0.017296. Value loss: 0.129533. Entropy: 0.996678.\n",
      "episode: 3692   score: 19.0  epsilon: 1.0    steps: 808  evaluation reward: 23.97\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9019: Policy loss: 0.010515. Value loss: 0.614579. Entropy: 1.024168.\n",
      "Iteration 9020: Policy loss: -0.001048. Value loss: 0.215622. Entropy: 1.024131.\n",
      "Iteration 9021: Policy loss: -0.011643. Value loss: 0.132457. Entropy: 1.007180.\n",
      "episode: 3693   score: 21.0  epsilon: 1.0    steps: 8  evaluation reward: 23.85\n",
      "episode: 3694   score: 15.0  epsilon: 1.0    steps: 176  evaluation reward: 23.77\n",
      "episode: 3695   score: 31.0  epsilon: 1.0    steps: 336  evaluation reward: 23.8\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9022: Policy loss: 0.008920. Value loss: 0.616241. Entropy: 0.934140.\n",
      "Iteration 9023: Policy loss: -0.001463. Value loss: 0.183036. Entropy: 0.933439.\n",
      "Iteration 9024: Policy loss: -0.013102. Value loss: 0.119332. Entropy: 0.934751.\n",
      "episode: 3696   score: 25.0  epsilon: 1.0    steps: 120  evaluation reward: 23.83\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9025: Policy loss: 0.004566. Value loss: 0.331096. Entropy: 0.931315.\n",
      "Iteration 9026: Policy loss: -0.007983. Value loss: 0.173235. Entropy: 0.945649.\n",
      "Iteration 9027: Policy loss: -0.019103. Value loss: 0.116482. Entropy: 0.932069.\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9028: Policy loss: 0.015382. Value loss: 0.412440. Entropy: 0.961102.\n",
      "Iteration 9029: Policy loss: 0.000640. Value loss: 0.207019. Entropy: 0.974270.\n",
      "Iteration 9030: Policy loss: -0.012997. Value loss: 0.135407. Entropy: 0.971540.\n",
      "episode: 3697   score: 41.0  epsilon: 1.0    steps: 888  evaluation reward: 24.11\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9031: Policy loss: 0.016187. Value loss: 0.465656. Entropy: 0.948812.\n",
      "Iteration 9032: Policy loss: 0.005908. Value loss: 0.239202. Entropy: 0.925683.\n",
      "Iteration 9033: Policy loss: -0.008581. Value loss: 0.145053. Entropy: 0.931217.\n",
      "episode: 3698   score: 16.0  epsilon: 1.0    steps: 672  evaluation reward: 23.85\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9034: Policy loss: 0.011624. Value loss: 0.709402. Entropy: 1.006821.\n",
      "Iteration 9035: Policy loss: -0.000542. Value loss: 0.325426. Entropy: 0.979564.\n",
      "Iteration 9036: Policy loss: -0.007441. Value loss: 0.196355. Entropy: 0.981691.\n",
      "episode: 3699   score: 13.0  epsilon: 1.0    steps: 600  evaluation reward: 23.63\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9037: Policy loss: 0.019880. Value loss: 0.619292. Entropy: 0.959990.\n",
      "Iteration 9038: Policy loss: 0.000481. Value loss: 0.232292. Entropy: 0.942450.\n",
      "Iteration 9039: Policy loss: -0.012034. Value loss: 0.142533. Entropy: 0.939896.\n",
      "episode: 3700   score: 18.0  epsilon: 1.0    steps: 1008  evaluation reward: 23.64\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9040: Policy loss: 0.016428. Value loss: 0.541736. Entropy: 0.982407.\n",
      "Iteration 9041: Policy loss: 0.012474. Value loss: 0.199089. Entropy: 0.981129.\n",
      "Iteration 9042: Policy loss: -0.008231. Value loss: 0.100918. Entropy: 0.974673.\n",
      "now time :  2019-03-06 15:40:48.348406\n",
      "episode: 3701   score: 38.0  epsilon: 1.0    steps: 864  evaluation reward: 23.91\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9043: Policy loss: 0.020851. Value loss: 0.739403. Entropy: 0.961030.\n",
      "Iteration 9044: Policy loss: 0.011895. Value loss: 0.301986. Entropy: 0.950241.\n",
      "Iteration 9045: Policy loss: -0.008930. Value loss: 0.180888. Entropy: 0.961284.\n",
      "episode: 3702   score: 23.0  epsilon: 1.0    steps: 800  evaluation reward: 23.93\n",
      "episode: 3703   score: 21.0  epsilon: 1.0    steps: 968  evaluation reward: 24.02\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9046: Policy loss: 0.014581. Value loss: 0.546133. Entropy: 1.017988.\n",
      "Iteration 9047: Policy loss: -0.002706. Value loss: 0.228427. Entropy: 1.004042.\n",
      "Iteration 9048: Policy loss: -0.017536. Value loss: 0.136520. Entropy: 1.007404.\n",
      "episode: 3704   score: 23.0  epsilon: 1.0    steps: 776  evaluation reward: 23.85\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9049: Policy loss: 0.006239. Value loss: 0.244086. Entropy: 0.981616.\n",
      "Iteration 9050: Policy loss: 0.001103. Value loss: 0.107979. Entropy: 0.977315.\n",
      "Iteration 9051: Policy loss: -0.011104. Value loss: 0.045411. Entropy: 0.984774.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9052: Policy loss: 0.017643. Value loss: 0.496285. Entropy: 0.891772.\n",
      "Iteration 9053: Policy loss: 0.001492. Value loss: 0.127042. Entropy: 0.896703.\n",
      "Iteration 9054: Policy loss: -0.010047. Value loss: 0.070963. Entropy: 0.901071.\n",
      "episode: 3705   score: 22.0  epsilon: 1.0    steps: 176  evaluation reward: 23.83\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9055: Policy loss: 0.007642. Value loss: 0.279936. Entropy: 1.008512.\n",
      "Iteration 9056: Policy loss: -0.007139. Value loss: 0.112916. Entropy: 1.018086.\n",
      "Iteration 9057: Policy loss: -0.016369. Value loss: 0.070063. Entropy: 1.019094.\n",
      "episode: 3706   score: 21.0  epsilon: 1.0    steps: 336  evaluation reward: 23.77\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9058: Policy loss: 0.008278. Value loss: 0.562289. Entropy: 0.980990.\n",
      "Iteration 9059: Policy loss: 0.001678. Value loss: 0.206662. Entropy: 0.983262.\n",
      "Iteration 9060: Policy loss: -0.015555. Value loss: 0.110426. Entropy: 0.991991.\n",
      "episode: 3707   score: 20.0  epsilon: 1.0    steps: 296  evaluation reward: 23.84\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9061: Policy loss: 0.011762. Value loss: 0.359516. Entropy: 0.943148.\n",
      "Iteration 9062: Policy loss: -0.004352. Value loss: 0.134689. Entropy: 0.919164.\n",
      "Iteration 9063: Policy loss: -0.017623. Value loss: 0.086979. Entropy: 0.917448.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9064: Policy loss: 0.022677. Value loss: 0.770202. Entropy: 0.867017.\n",
      "Iteration 9065: Policy loss: 0.002542. Value loss: 0.283929. Entropy: 0.865981.\n",
      "Iteration 9066: Policy loss: -0.008124. Value loss: 0.129494. Entropy: 0.879780.\n",
      "episode: 3708   score: 17.0  epsilon: 1.0    steps: 472  evaluation reward: 23.72\n",
      "episode: 3709   score: 30.0  epsilon: 1.0    steps: 688  evaluation reward: 23.66\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9067: Policy loss: 0.011256. Value loss: 0.713620. Entropy: 0.960769.\n",
      "Iteration 9068: Policy loss: -0.009422. Value loss: 0.258154. Entropy: 0.952887.\n",
      "Iteration 9069: Policy loss: -0.014311. Value loss: 0.141661. Entropy: 0.965715.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9070: Policy loss: 0.005615. Value loss: 0.415024. Entropy: 0.986652.\n",
      "Iteration 9071: Policy loss: -0.001417. Value loss: 0.190188. Entropy: 0.980001.\n",
      "Iteration 9072: Policy loss: -0.018789. Value loss: 0.106564. Entropy: 0.979206.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9073: Policy loss: 0.021356. Value loss: 0.535334. Entropy: 0.949491.\n",
      "Iteration 9074: Policy loss: 0.005416. Value loss: 0.193645. Entropy: 0.950234.\n",
      "Iteration 9075: Policy loss: -0.011224. Value loss: 0.084941. Entropy: 0.957046.\n",
      "episode: 3710   score: 25.0  epsilon: 1.0    steps: 64  evaluation reward: 23.72\n",
      "episode: 3711   score: 34.0  epsilon: 1.0    steps: 632  evaluation reward: 23.83\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9076: Policy loss: 0.012318. Value loss: 0.566074. Entropy: 0.901661.\n",
      "Iteration 9077: Policy loss: -0.001265. Value loss: 0.231491. Entropy: 0.915821.\n",
      "Iteration 9078: Policy loss: -0.011218. Value loss: 0.114766. Entropy: 0.918479.\n",
      "episode: 3712   score: 19.0  epsilon: 1.0    steps: 392  evaluation reward: 23.79\n",
      "episode: 3713   score: 38.0  epsilon: 1.0    steps: 904  evaluation reward: 24.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9079: Policy loss: 0.007104. Value loss: 0.531750. Entropy: 0.880035.\n",
      "Iteration 9080: Policy loss: 0.001732. Value loss: 0.224499. Entropy: 0.881405.\n",
      "Iteration 9081: Policy loss: -0.007074. Value loss: 0.097695. Entropy: 0.866267.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9082: Policy loss: 0.008463. Value loss: 0.408642. Entropy: 0.966816.\n",
      "Iteration 9083: Policy loss: -0.009030. Value loss: 0.216005. Entropy: 0.956696.\n",
      "Iteration 9084: Policy loss: -0.014702. Value loss: 0.143734. Entropy: 0.947704.\n",
      "episode: 3714   score: 18.0  epsilon: 1.0    steps: 896  evaluation reward: 23.84\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9085: Policy loss: 0.013143. Value loss: 0.688702. Entropy: 0.960586.\n",
      "Iteration 9086: Policy loss: 0.004678. Value loss: 0.343670. Entropy: 0.936682.\n",
      "Iteration 9087: Policy loss: -0.005277. Value loss: 0.184216. Entropy: 0.924611.\n",
      "episode: 3715   score: 28.0  epsilon: 1.0    steps: 248  evaluation reward: 23.77\n",
      "episode: 3716   score: 23.0  epsilon: 1.0    steps: 536  evaluation reward: 23.75\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9088: Policy loss: 0.010637. Value loss: 0.444109. Entropy: 0.951269.\n",
      "Iteration 9089: Policy loss: -0.004577. Value loss: 0.162698. Entropy: 0.942640.\n",
      "Iteration 9090: Policy loss: -0.019569. Value loss: 0.096849. Entropy: 0.946921.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9091: Policy loss: 0.007349. Value loss: 0.501405. Entropy: 0.930364.\n",
      "Iteration 9092: Policy loss: -0.003981. Value loss: 0.245340. Entropy: 0.917220.\n",
      "Iteration 9093: Policy loss: -0.011738. Value loss: 0.131548. Entropy: 0.924089.\n",
      "episode: 3717   score: 23.0  epsilon: 1.0    steps: 208  evaluation reward: 23.75\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9094: Policy loss: 0.014372. Value loss: 0.488901. Entropy: 0.945352.\n",
      "Iteration 9095: Policy loss: -0.004125. Value loss: 0.195223. Entropy: 0.907546.\n",
      "Iteration 9096: Policy loss: -0.014179. Value loss: 0.109128. Entropy: 0.906041.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9097: Policy loss: 0.010656. Value loss: 0.725674. Entropy: 0.946925.\n",
      "Iteration 9098: Policy loss: -0.003663. Value loss: 0.323152. Entropy: 0.958918.\n",
      "Iteration 9099: Policy loss: -0.015333. Value loss: 0.148579. Entropy: 0.950798.\n",
      "episode: 3718   score: 23.0  epsilon: 1.0    steps: 416  evaluation reward: 23.78\n",
      "episode: 3719   score: 19.0  epsilon: 1.0    steps: 960  evaluation reward: 23.52\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9100: Policy loss: 0.015603. Value loss: 0.756197. Entropy: 0.938319.\n",
      "Iteration 9101: Policy loss: 0.000542. Value loss: 0.318083. Entropy: 0.936696.\n",
      "Iteration 9102: Policy loss: -0.010463. Value loss: 0.175751. Entropy: 0.930184.\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9103: Policy loss: 0.015458. Value loss: 0.599042. Entropy: 0.922613.\n",
      "Iteration 9104: Policy loss: -0.000270. Value loss: 0.201927. Entropy: 0.917109.\n",
      "Iteration 9105: Policy loss: -0.015913. Value loss: 0.104709. Entropy: 0.905758.\n",
      "episode: 3720   score: 35.0  epsilon: 1.0    steps: 464  evaluation reward: 23.69\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9106: Policy loss: 0.009270. Value loss: 1.127982. Entropy: 0.889136.\n",
      "Iteration 9107: Policy loss: 0.000095. Value loss: 0.482069. Entropy: 0.880533.\n",
      "Iteration 9108: Policy loss: -0.008768. Value loss: 0.205074. Entropy: 0.882086.\n",
      "episode: 3721   score: 41.0  epsilon: 1.0    steps: 936  evaluation reward: 23.91\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9109: Policy loss: 0.017481. Value loss: 0.476042. Entropy: 0.966251.\n",
      "Iteration 9110: Policy loss: 0.000178. Value loss: 0.201123. Entropy: 0.939842.\n",
      "Iteration 9111: Policy loss: -0.012232. Value loss: 0.113263. Entropy: 0.939954.\n",
      "episode: 3722   score: 25.0  epsilon: 1.0    steps: 80  evaluation reward: 24.1\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9112: Policy loss: 0.009188. Value loss: 0.418910. Entropy: 0.961307.\n",
      "Iteration 9113: Policy loss: -0.002314. Value loss: 0.166314. Entropy: 0.965333.\n",
      "Iteration 9114: Policy loss: -0.013444. Value loss: 0.108109. Entropy: 0.963711.\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9115: Policy loss: 0.013863. Value loss: 0.605754. Entropy: 0.945394.\n",
      "Iteration 9116: Policy loss: 0.001616. Value loss: 0.278613. Entropy: 0.937317.\n",
      "Iteration 9117: Policy loss: -0.012423. Value loss: 0.176914. Entropy: 0.934868.\n",
      "episode: 3723   score: 32.0  epsilon: 1.0    steps: 216  evaluation reward: 24.31\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9118: Policy loss: 0.006429. Value loss: 0.564796. Entropy: 0.979386.\n",
      "Iteration 9119: Policy loss: -0.002661. Value loss: 0.233773. Entropy: 0.960914.\n",
      "Iteration 9120: Policy loss: -0.014197. Value loss: 0.099311. Entropy: 0.969881.\n",
      "episode: 3724   score: 21.0  epsilon: 1.0    steps: 664  evaluation reward: 24.22\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9121: Policy loss: 0.007167. Value loss: 0.610872. Entropy: 0.956814.\n",
      "Iteration 9122: Policy loss: -0.008316. Value loss: 0.198646. Entropy: 0.968207.\n",
      "Iteration 9123: Policy loss: -0.011165. Value loss: 0.110500. Entropy: 0.961599.\n",
      "episode: 3725   score: 31.0  epsilon: 1.0    steps: 16  evaluation reward: 24.26\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9124: Policy loss: 0.012812. Value loss: 0.246311. Entropy: 0.961747.\n",
      "Iteration 9125: Policy loss: -0.001205. Value loss: 0.105284. Entropy: 0.969219.\n",
      "Iteration 9126: Policy loss: -0.014151. Value loss: 0.072827. Entropy: 0.949752.\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9127: Policy loss: 0.013850. Value loss: 0.647504. Entropy: 0.955332.\n",
      "Iteration 9128: Policy loss: -0.007274. Value loss: 0.281608. Entropy: 0.964442.\n",
      "Iteration 9129: Policy loss: -0.012516. Value loss: 0.162459. Entropy: 0.959578.\n",
      "episode: 3726   score: 33.0  epsilon: 1.0    steps: 624  evaluation reward: 24.4\n",
      "episode: 3727   score: 32.0  epsilon: 1.0    steps: 776  evaluation reward: 24.5\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9130: Policy loss: 0.013973. Value loss: 0.657034. Entropy: 0.958765.\n",
      "Iteration 9131: Policy loss: -0.000117. Value loss: 0.273317. Entropy: 0.966547.\n",
      "Iteration 9132: Policy loss: -0.008321. Value loss: 0.170589. Entropy: 0.953004.\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9133: Policy loss: 0.010738. Value loss: 0.728057. Entropy: 0.955026.\n",
      "Iteration 9134: Policy loss: 0.002268. Value loss: 0.351426. Entropy: 0.947136.\n",
      "Iteration 9135: Policy loss: -0.008706. Value loss: 0.213657. Entropy: 0.922543.\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9136: Policy loss: 0.013663. Value loss: 0.403417. Entropy: 0.966025.\n",
      "Iteration 9137: Policy loss: -0.005624. Value loss: 0.158910. Entropy: 0.944803.\n",
      "Iteration 9138: Policy loss: -0.016658. Value loss: 0.101576. Entropy: 0.956859.\n",
      "episode: 3728   score: 34.0  epsilon: 1.0    steps: 864  evaluation reward: 24.63\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9139: Policy loss: 0.012353. Value loss: 0.620834. Entropy: 0.898815.\n",
      "Iteration 9140: Policy loss: 0.005826. Value loss: 0.216854. Entropy: 0.898529.\n",
      "Iteration 9141: Policy loss: -0.012045. Value loss: 0.119693. Entropy: 0.899486.\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9142: Policy loss: 0.006116. Value loss: 0.771497. Entropy: 0.919769.\n",
      "Iteration 9143: Policy loss: -0.005010. Value loss: 0.345831. Entropy: 0.892460.\n",
      "Iteration 9144: Policy loss: -0.013742. Value loss: 0.199161. Entropy: 0.891718.\n",
      "episode: 3729   score: 37.0  epsilon: 1.0    steps: 8  evaluation reward: 24.64\n",
      "episode: 3730   score: 26.0  epsilon: 1.0    steps: 120  evaluation reward: 24.58\n",
      "episode: 3731   score: 40.0  epsilon: 1.0    steps: 480  evaluation reward: 24.81\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9145: Policy loss: 0.012561. Value loss: 0.549951. Entropy: 0.923594.\n",
      "Iteration 9146: Policy loss: -0.004920. Value loss: 0.309419. Entropy: 0.910407.\n",
      "Iteration 9147: Policy loss: -0.012167. Value loss: 0.217464. Entropy: 0.908276.\n",
      "episode: 3732   score: 15.0  epsilon: 1.0    steps: 840  evaluation reward: 24.83\n",
      "Training network. lr: 0.000180. clip: 0.072048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9148: Policy loss: 0.015440. Value loss: 0.653621. Entropy: 0.976096.\n",
      "Iteration 9149: Policy loss: 0.005638. Value loss: 0.333117. Entropy: 0.941772.\n",
      "Iteration 9150: Policy loss: -0.004808. Value loss: 0.220090. Entropy: 0.950506.\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9151: Policy loss: 0.009968. Value loss: 0.495907. Entropy: 0.914529.\n",
      "Iteration 9152: Policy loss: -0.003077. Value loss: 0.247498. Entropy: 0.881938.\n",
      "Iteration 9153: Policy loss: -0.014066. Value loss: 0.157520. Entropy: 0.895407.\n",
      "episode: 3733   score: 38.0  epsilon: 1.0    steps: 984  evaluation reward: 24.95\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9154: Policy loss: 0.015628. Value loss: 0.609226. Entropy: 0.941573.\n",
      "Iteration 9155: Policy loss: -0.006042. Value loss: 0.232884. Entropy: 0.908744.\n",
      "Iteration 9156: Policy loss: -0.009802. Value loss: 0.113023. Entropy: 0.912757.\n",
      "episode: 3734   score: 33.0  epsilon: 1.0    steps: 48  evaluation reward: 25.09\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9157: Policy loss: 0.011764. Value loss: 0.386759. Entropy: 0.916149.\n",
      "Iteration 9158: Policy loss: -0.008301. Value loss: 0.163182. Entropy: 0.921121.\n",
      "Iteration 9159: Policy loss: -0.012133. Value loss: 0.096204. Entropy: 0.915262.\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9160: Policy loss: 0.014937. Value loss: 0.630462. Entropy: 0.953806.\n",
      "Iteration 9161: Policy loss: -0.001087. Value loss: 0.282611. Entropy: 0.947197.\n",
      "Iteration 9162: Policy loss: -0.013821. Value loss: 0.179388. Entropy: 0.934631.\n",
      "episode: 3735   score: 26.0  epsilon: 1.0    steps: 536  evaluation reward: 25.01\n",
      "episode: 3736   score: 18.0  epsilon: 1.0    steps: 808  evaluation reward: 25.0\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9163: Policy loss: 0.019559. Value loss: 0.613029. Entropy: 0.962273.\n",
      "Iteration 9164: Policy loss: -0.001237. Value loss: 0.250033. Entropy: 0.974271.\n",
      "Iteration 9165: Policy loss: -0.014362. Value loss: 0.153303. Entropy: 0.978880.\n",
      "episode: 3737   score: 28.0  epsilon: 1.0    steps: 232  evaluation reward: 24.95\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9166: Policy loss: 0.010071. Value loss: 0.472590. Entropy: 0.930381.\n",
      "Iteration 9167: Policy loss: -0.001537. Value loss: 0.234899. Entropy: 0.916938.\n",
      "Iteration 9168: Policy loss: -0.016409. Value loss: 0.157824. Entropy: 0.920012.\n",
      "episode: 3738   score: 14.0  epsilon: 1.0    steps: 176  evaluation reward: 24.83\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9169: Policy loss: 0.007207. Value loss: 0.310028. Entropy: 0.945035.\n",
      "Iteration 9170: Policy loss: -0.005601. Value loss: 0.107893. Entropy: 0.925629.\n",
      "Iteration 9171: Policy loss: -0.018330. Value loss: 0.057457. Entropy: 0.920471.\n",
      "episode: 3739   score: 24.0  epsilon: 1.0    steps: 584  evaluation reward: 24.85\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9172: Policy loss: 0.012928. Value loss: 0.544010. Entropy: 0.891766.\n",
      "Iteration 9173: Policy loss: 0.001325. Value loss: 0.293002. Entropy: 0.897518.\n",
      "Iteration 9174: Policy loss: -0.008819. Value loss: 0.158081. Entropy: 0.886808.\n",
      "episode: 3740   score: 16.0  epsilon: 1.0    steps: 216  evaluation reward: 24.8\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9175: Policy loss: 0.008204. Value loss: 0.553351. Entropy: 0.871489.\n",
      "Iteration 9176: Policy loss: -0.001620. Value loss: 0.203167. Entropy: 0.851403.\n",
      "Iteration 9177: Policy loss: -0.009578. Value loss: 0.113768. Entropy: 0.862212.\n",
      "episode: 3741   score: 37.0  epsilon: 1.0    steps: 720  evaluation reward: 25.0\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9178: Policy loss: 0.011658. Value loss: 0.830046. Entropy: 0.890408.\n",
      "Iteration 9179: Policy loss: -0.001102. Value loss: 0.368948. Entropy: 0.872861.\n",
      "Iteration 9180: Policy loss: -0.010255. Value loss: 0.226768. Entropy: 0.867005.\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9181: Policy loss: 0.010521. Value loss: 0.374085. Entropy: 0.877768.\n",
      "Iteration 9182: Policy loss: -0.001657. Value loss: 0.127927. Entropy: 0.892969.\n",
      "Iteration 9183: Policy loss: -0.014575. Value loss: 0.084450. Entropy: 0.886809.\n",
      "episode: 3742   score: 24.0  epsilon: 1.0    steps: 432  evaluation reward: 25.05\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9184: Policy loss: 0.016256. Value loss: 0.665295. Entropy: 0.880418.\n",
      "Iteration 9185: Policy loss: -0.003244. Value loss: 0.279755. Entropy: 0.863842.\n",
      "Iteration 9186: Policy loss: -0.010156. Value loss: 0.135044. Entropy: 0.873128.\n",
      "episode: 3743   score: 27.0  epsilon: 1.0    steps: 1016  evaluation reward: 25.07\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9187: Policy loss: 0.019622. Value loss: 0.516926. Entropy: 0.933072.\n",
      "Iteration 9188: Policy loss: 0.004426. Value loss: 0.234288. Entropy: 0.919973.\n",
      "Iteration 9189: Policy loss: -0.008300. Value loss: 0.128960. Entropy: 0.941329.\n",
      "episode: 3744   score: 21.0  epsilon: 1.0    steps: 536  evaluation reward: 25.15\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9190: Policy loss: 0.006777. Value loss: 0.506739. Entropy: 0.938670.\n",
      "Iteration 9191: Policy loss: -0.003326. Value loss: 0.158094. Entropy: 0.929269.\n",
      "Iteration 9192: Policy loss: -0.010906. Value loss: 0.080254. Entropy: 0.922545.\n",
      "episode: 3745   score: 26.0  epsilon: 1.0    steps: 944  evaluation reward: 25.22\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9193: Policy loss: 0.022983. Value loss: 0.750293. Entropy: 0.832248.\n",
      "Iteration 9194: Policy loss: 0.000362. Value loss: 0.267655. Entropy: 0.822149.\n",
      "Iteration 9195: Policy loss: -0.007585. Value loss: 0.115813. Entropy: 0.833825.\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9196: Policy loss: 0.012647. Value loss: 0.458103. Entropy: 0.873940.\n",
      "Iteration 9197: Policy loss: -0.005360. Value loss: 0.228599. Entropy: 0.883805.\n",
      "Iteration 9198: Policy loss: -0.014341. Value loss: 0.147833. Entropy: 0.860663.\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9199: Policy loss: 0.008518. Value loss: 0.567561. Entropy: 0.902411.\n",
      "Iteration 9200: Policy loss: 0.002601. Value loss: 0.214626. Entropy: 0.891458.\n",
      "Iteration 9201: Policy loss: -0.003569. Value loss: 0.128455. Entropy: 0.872871.\n",
      "episode: 3746   score: 34.0  epsilon: 1.0    steps: 144  evaluation reward: 25.42\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9202: Policy loss: 0.015564. Value loss: 0.774571. Entropy: 0.978332.\n",
      "Iteration 9203: Policy loss: 0.002411. Value loss: 0.337089. Entropy: 0.962319.\n",
      "Iteration 9204: Policy loss: -0.009954. Value loss: 0.182406. Entropy: 0.950559.\n",
      "episode: 3747   score: 14.0  epsilon: 1.0    steps: 104  evaluation reward: 25.28\n",
      "episode: 3748   score: 31.0  epsilon: 1.0    steps: 408  evaluation reward: 25.42\n",
      "episode: 3749   score: 44.0  epsilon: 1.0    steps: 672  evaluation reward: 25.5\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9205: Policy loss: 0.010203. Value loss: 0.865063. Entropy: 0.893944.\n",
      "Iteration 9206: Policy loss: 0.003703. Value loss: 0.547838. Entropy: 0.884899.\n",
      "Iteration 9207: Policy loss: -0.010400. Value loss: 0.399397. Entropy: 0.886316.\n",
      "episode: 3750   score: 26.0  epsilon: 1.0    steps: 96  evaluation reward: 25.52\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9208: Policy loss: 0.008170. Value loss: 0.411933. Entropy: 0.840259.\n",
      "Iteration 9209: Policy loss: -0.002364. Value loss: 0.190708. Entropy: 0.832434.\n",
      "Iteration 9210: Policy loss: -0.010132. Value loss: 0.113726. Entropy: 0.855272.\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9211: Policy loss: 0.008045. Value loss: 0.521439. Entropy: 0.833997.\n",
      "Iteration 9212: Policy loss: -0.001042. Value loss: 0.256997. Entropy: 0.823520.\n",
      "Iteration 9213: Policy loss: -0.008569. Value loss: 0.139277. Entropy: 0.838657.\n",
      "now time :  2019-03-06 15:44:20.578289\n",
      "episode: 3751   score: 24.0  epsilon: 1.0    steps: 176  evaluation reward: 25.56\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9214: Policy loss: 0.007692. Value loss: 0.296595. Entropy: 0.902888.\n",
      "Iteration 9215: Policy loss: -0.008629. Value loss: 0.153178. Entropy: 0.892562.\n",
      "Iteration 9216: Policy loss: -0.016484. Value loss: 0.098526. Entropy: 0.899403.\n",
      "Training network. lr: 0.000179. clip: 0.071744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9217: Policy loss: 0.020797. Value loss: 0.708341. Entropy: 0.901199.\n",
      "Iteration 9218: Policy loss: 0.008367. Value loss: 0.301321. Entropy: 0.881976.\n",
      "Iteration 9219: Policy loss: -0.006166. Value loss: 0.158563. Entropy: 0.906270.\n",
      "episode: 3752   score: 20.0  epsilon: 1.0    steps: 504  evaluation reward: 25.55\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9220: Policy loss: 0.020154. Value loss: 1.052219. Entropy: 0.880938.\n",
      "Iteration 9221: Policy loss: 0.006980. Value loss: 0.351531. Entropy: 0.885071.\n",
      "Iteration 9222: Policy loss: -0.008236. Value loss: 0.186022. Entropy: 0.890897.\n",
      "episode: 3753   score: 46.0  epsilon: 1.0    steps: 176  evaluation reward: 25.71\n",
      "episode: 3754   score: 16.0  epsilon: 1.0    steps: 184  evaluation reward: 25.53\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9223: Policy loss: 0.020531. Value loss: 0.625617. Entropy: 0.807609.\n",
      "Iteration 9224: Policy loss: 0.003422. Value loss: 0.277763. Entropy: 0.815939.\n",
      "Iteration 9225: Policy loss: -0.008208. Value loss: 0.146766. Entropy: 0.824979.\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9226: Policy loss: 0.012092. Value loss: 0.539885. Entropy: 0.887663.\n",
      "Iteration 9227: Policy loss: -0.009546. Value loss: 0.319794. Entropy: 0.879820.\n",
      "Iteration 9228: Policy loss: -0.015277. Value loss: 0.214391. Entropy: 0.871507.\n",
      "episode: 3755   score: 20.0  epsilon: 1.0    steps: 288  evaluation reward: 25.47\n",
      "episode: 3756   score: 24.0  epsilon: 1.0    steps: 552  evaluation reward: 25.54\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9229: Policy loss: 0.016491. Value loss: 0.773395. Entropy: 0.868664.\n",
      "Iteration 9230: Policy loss: 0.005342. Value loss: 0.356793. Entropy: 0.856541.\n",
      "Iteration 9231: Policy loss: -0.007210. Value loss: 0.212356. Entropy: 0.852917.\n",
      "episode: 3757   score: 31.0  epsilon: 1.0    steps: 24  evaluation reward: 25.68\n",
      "episode: 3758   score: 30.0  epsilon: 1.0    steps: 928  evaluation reward: 25.73\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9232: Policy loss: 0.006719. Value loss: 0.463674. Entropy: 0.914489.\n",
      "Iteration 9233: Policy loss: -0.000180. Value loss: 0.210626. Entropy: 0.899334.\n",
      "Iteration 9234: Policy loss: -0.011115. Value loss: 0.143618. Entropy: 0.909105.\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9235: Policy loss: 0.012627. Value loss: 0.400597. Entropy: 0.923204.\n",
      "Iteration 9236: Policy loss: -0.000734. Value loss: 0.150965. Entropy: 0.916913.\n",
      "Iteration 9237: Policy loss: -0.012184. Value loss: 0.077026. Entropy: 0.924387.\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9238: Policy loss: 0.012424. Value loss: 0.556463. Entropy: 0.888121.\n",
      "Iteration 9239: Policy loss: -0.001045. Value loss: 0.188759. Entropy: 0.889846.\n",
      "Iteration 9240: Policy loss: -0.011804. Value loss: 0.102219. Entropy: 0.889833.\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9241: Policy loss: 0.019939. Value loss: 0.501868. Entropy: 0.954273.\n",
      "Iteration 9242: Policy loss: -0.001441. Value loss: 0.157803. Entropy: 0.928729.\n",
      "Iteration 9243: Policy loss: -0.014931. Value loss: 0.075648. Entropy: 0.928967.\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9244: Policy loss: 0.017714. Value loss: 0.684487. Entropy: 0.945737.\n",
      "Iteration 9245: Policy loss: 0.008703. Value loss: 0.309063. Entropy: 0.930034.\n",
      "Iteration 9246: Policy loss: 0.002275. Value loss: 0.159584. Entropy: 0.933825.\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9247: Policy loss: 0.013176. Value loss: 0.791340. Entropy: 0.943987.\n",
      "Iteration 9248: Policy loss: 0.008069. Value loss: 0.358229. Entropy: 0.923331.\n",
      "Iteration 9249: Policy loss: -0.006044. Value loss: 0.196305. Entropy: 0.916254.\n",
      "episode: 3759   score: 28.0  epsilon: 1.0    steps: 664  evaluation reward: 25.75\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9250: Policy loss: 0.012351. Value loss: 1.047816. Entropy: 0.930150.\n",
      "Iteration 9251: Policy loss: -0.002786. Value loss: 0.515096. Entropy: 0.949170.\n",
      "Iteration 9252: Policy loss: -0.011360. Value loss: 0.273731. Entropy: 0.942516.\n",
      "episode: 3760   score: 31.0  epsilon: 1.0    steps: 136  evaluation reward: 25.89\n",
      "episode: 3761   score: 19.0  epsilon: 1.0    steps: 216  evaluation reward: 25.87\n",
      "episode: 3762   score: 14.0  epsilon: 1.0    steps: 552  evaluation reward: 25.66\n",
      "episode: 3763   score: 34.0  epsilon: 1.0    steps: 728  evaluation reward: 25.8\n",
      "episode: 3764   score: 26.0  epsilon: 1.0    steps: 840  evaluation reward: 25.86\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9253: Policy loss: 0.016131. Value loss: 0.634210. Entropy: 0.900152.\n",
      "Iteration 9254: Policy loss: 0.005768. Value loss: 0.222350. Entropy: 0.902349.\n",
      "Iteration 9255: Policy loss: -0.011309. Value loss: 0.149237. Entropy: 0.900155.\n",
      "episode: 3765   score: 52.0  epsilon: 1.0    steps: 696  evaluation reward: 26.18\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9256: Policy loss: 0.015559. Value loss: 0.521714. Entropy: 0.850494.\n",
      "Iteration 9257: Policy loss: -0.000492. Value loss: 0.284277. Entropy: 0.847091.\n",
      "Iteration 9258: Policy loss: -0.008515. Value loss: 0.177250. Entropy: 0.855275.\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9259: Policy loss: 0.003273. Value loss: 0.651035. Entropy: 0.913601.\n",
      "Iteration 9260: Policy loss: -0.000711. Value loss: 0.366256. Entropy: 0.884023.\n",
      "Iteration 9261: Policy loss: -0.008284. Value loss: 0.259659. Entropy: 0.892735.\n",
      "episode: 3766   score: 33.0  epsilon: 1.0    steps: 248  evaluation reward: 26.25\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9262: Policy loss: 0.012399. Value loss: 0.306947. Entropy: 0.827208.\n",
      "Iteration 9263: Policy loss: -0.007764. Value loss: 0.133730. Entropy: 0.824645.\n",
      "Iteration 9264: Policy loss: -0.015711. Value loss: 0.083823. Entropy: 0.816191.\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9265: Policy loss: 0.009853. Value loss: 0.740119. Entropy: 0.921304.\n",
      "Iteration 9266: Policy loss: 0.002159. Value loss: 0.345020. Entropy: 0.917467.\n",
      "Iteration 9267: Policy loss: -0.008998. Value loss: 0.201572. Entropy: 0.914670.\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9268: Policy loss: 0.016864. Value loss: 0.768586. Entropy: 1.017381.\n",
      "Iteration 9269: Policy loss: -0.000668. Value loss: 0.371159. Entropy: 1.020169.\n",
      "Iteration 9270: Policy loss: -0.009817. Value loss: 0.268948. Entropy: 1.018401.\n",
      "episode: 3767   score: 12.0  epsilon: 1.0    steps: 400  evaluation reward: 26.15\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9271: Policy loss: 0.010840. Value loss: 0.788544. Entropy: 1.017883.\n",
      "Iteration 9272: Policy loss: -0.001214. Value loss: 0.351965. Entropy: 1.011861.\n",
      "Iteration 9273: Policy loss: -0.010352. Value loss: 0.233644. Entropy: 0.996986.\n",
      "episode: 3768   score: 20.0  epsilon: 1.0    steps: 80  evaluation reward: 26.16\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9274: Policy loss: 0.015074. Value loss: 0.450418. Entropy: 0.962057.\n",
      "Iteration 9275: Policy loss: -0.000981. Value loss: 0.148890. Entropy: 0.958170.\n",
      "Iteration 9276: Policy loss: -0.017379. Value loss: 0.093951. Entropy: 0.965160.\n",
      "episode: 3769   score: 23.0  epsilon: 1.0    steps: 352  evaluation reward: 26.18\n",
      "episode: 3770   score: 14.0  epsilon: 1.0    steps: 600  evaluation reward: 25.99\n",
      "episode: 3771   score: 31.0  epsilon: 1.0    steps: 936  evaluation reward: 26.0\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9277: Policy loss: 0.016777. Value loss: 0.948608. Entropy: 0.864242.\n",
      "Iteration 9278: Policy loss: 0.002112. Value loss: 0.330764. Entropy: 0.859987.\n",
      "Iteration 9279: Policy loss: -0.008374. Value loss: 0.165884. Entropy: 0.872288.\n",
      "episode: 3772   score: 19.0  epsilon: 1.0    steps: 592  evaluation reward: 26.04\n",
      "episode: 3773   score: 22.0  epsilon: 1.0    steps: 696  evaluation reward: 25.95\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9280: Policy loss: 0.032273. Value loss: 0.668075. Entropy: 0.867909.\n",
      "Iteration 9281: Policy loss: 0.000100. Value loss: 0.235722. Entropy: 0.865417.\n",
      "Iteration 9282: Policy loss: -0.010952. Value loss: 0.135724. Entropy: 0.869490.\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9283: Policy loss: 0.013133. Value loss: 0.703153. Entropy: 0.833435.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9284: Policy loss: 0.000593. Value loss: 0.321943. Entropy: 0.826326.\n",
      "Iteration 9285: Policy loss: -0.004180. Value loss: 0.223631. Entropy: 0.814487.\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9286: Policy loss: 0.019775. Value loss: 0.691700. Entropy: 0.864542.\n",
      "Iteration 9287: Policy loss: 0.002682. Value loss: 0.283014. Entropy: 0.852793.\n",
      "Iteration 9288: Policy loss: -0.012438. Value loss: 0.137451. Entropy: 0.862979.\n",
      "episode: 3774   score: 37.0  epsilon: 1.0    steps: 760  evaluation reward: 26.06\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9289: Policy loss: 0.008940. Value loss: 0.397307. Entropy: 0.903358.\n",
      "Iteration 9290: Policy loss: -0.008911. Value loss: 0.154674. Entropy: 0.905209.\n",
      "Iteration 9291: Policy loss: -0.015222. Value loss: 0.088323. Entropy: 0.910098.\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9292: Policy loss: 0.009914. Value loss: 0.891661. Entropy: 0.873244.\n",
      "Iteration 9293: Policy loss: -0.002304. Value loss: 0.429197. Entropy: 0.844449.\n",
      "Iteration 9294: Policy loss: -0.009530. Value loss: 0.213847. Entropy: 0.857833.\n",
      "episode: 3775   score: 14.0  epsilon: 1.0    steps: 248  evaluation reward: 25.94\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9295: Policy loss: 0.019831. Value loss: 0.582255. Entropy: 0.862765.\n",
      "Iteration 9296: Policy loss: 0.003171. Value loss: 0.246550. Entropy: 0.877500.\n",
      "Iteration 9297: Policy loss: -0.007986. Value loss: 0.145107. Entropy: 0.881988.\n",
      "episode: 3776   score: 27.0  epsilon: 1.0    steps: 576  evaluation reward: 25.9\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9298: Policy loss: 0.018751. Value loss: 1.059215. Entropy: 0.840702.\n",
      "Iteration 9299: Policy loss: 0.006367. Value loss: 0.383279. Entropy: 0.838988.\n",
      "Iteration 9300: Policy loss: -0.002475. Value loss: 0.230338. Entropy: 0.856674.\n",
      "episode: 3777   score: 42.0  epsilon: 1.0    steps: 176  evaluation reward: 26.04\n",
      "episode: 3778   score: 25.0  epsilon: 1.0    steps: 688  evaluation reward: 26.08\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9301: Policy loss: 0.007849. Value loss: 0.719960. Entropy: 0.890502.\n",
      "Iteration 9302: Policy loss: -0.000787. Value loss: 0.312083. Entropy: 0.891940.\n",
      "Iteration 9303: Policy loss: -0.012743. Value loss: 0.189240. Entropy: 0.884434.\n",
      "episode: 3779   score: 28.0  epsilon: 1.0    steps: 24  evaluation reward: 26.16\n",
      "episode: 3780   score: 25.0  epsilon: 1.0    steps: 456  evaluation reward: 26.22\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9304: Policy loss: 0.011082. Value loss: 0.468566. Entropy: 0.902467.\n",
      "Iteration 9305: Policy loss: 0.004845. Value loss: 0.232092. Entropy: 0.893275.\n",
      "Iteration 9306: Policy loss: -0.011362. Value loss: 0.138352. Entropy: 0.890134.\n",
      "episode: 3781   score: 33.0  epsilon: 1.0    steps: 368  evaluation reward: 26.32\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9307: Policy loss: 0.014634. Value loss: 0.344728. Entropy: 0.905739.\n",
      "Iteration 9308: Policy loss: 0.003877. Value loss: 0.123964. Entropy: 0.915743.\n",
      "Iteration 9309: Policy loss: -0.012637. Value loss: 0.081618. Entropy: 0.904866.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9310: Policy loss: 0.011681. Value loss: 0.601556. Entropy: 0.880268.\n",
      "Iteration 9311: Policy loss: -0.009161. Value loss: 0.210576. Entropy: 0.886536.\n",
      "Iteration 9312: Policy loss: -0.016991. Value loss: 0.104048. Entropy: 0.879393.\n",
      "episode: 3782   score: 21.0  epsilon: 1.0    steps: 248  evaluation reward: 26.11\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9313: Policy loss: 0.013077. Value loss: 0.625306. Entropy: 0.838177.\n",
      "Iteration 9314: Policy loss: -0.000444. Value loss: 0.233478. Entropy: 0.846435.\n",
      "Iteration 9315: Policy loss: -0.006845. Value loss: 0.146554. Entropy: 0.866396.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9316: Policy loss: 0.004977. Value loss: 0.394831. Entropy: 0.869584.\n",
      "Iteration 9317: Policy loss: -0.010743. Value loss: 0.114488. Entropy: 0.875595.\n",
      "Iteration 9318: Policy loss: -0.017906. Value loss: 0.056743. Entropy: 0.879845.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9319: Policy loss: 0.009433. Value loss: 0.356621. Entropy: 0.932628.\n",
      "Iteration 9320: Policy loss: -0.005530. Value loss: 0.164957. Entropy: 0.949223.\n",
      "Iteration 9321: Policy loss: -0.017287. Value loss: 0.091756. Entropy: 0.938806.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9322: Policy loss: 0.013006. Value loss: 0.360438. Entropy: 0.987348.\n",
      "Iteration 9323: Policy loss: -0.003911. Value loss: 0.166415. Entropy: 0.979552.\n",
      "Iteration 9324: Policy loss: -0.013177. Value loss: 0.093229. Entropy: 0.980717.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9325: Policy loss: 0.012913. Value loss: 0.919583. Entropy: 0.967688.\n",
      "Iteration 9326: Policy loss: 0.002148. Value loss: 0.392916. Entropy: 0.967965.\n",
      "Iteration 9327: Policy loss: -0.015336. Value loss: 0.204015. Entropy: 0.976638.\n",
      "episode: 3783   score: 22.0  epsilon: 1.0    steps: 520  evaluation reward: 26.02\n",
      "episode: 3784   score: 27.0  epsilon: 1.0    steps: 784  evaluation reward: 26.03\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9328: Policy loss: 0.014215. Value loss: 0.549376. Entropy: 0.936854.\n",
      "Iteration 9329: Policy loss: 0.004048. Value loss: 0.235325. Entropy: 0.942237.\n",
      "Iteration 9330: Policy loss: -0.005518. Value loss: 0.137482. Entropy: 0.951054.\n",
      "episode: 3785   score: 31.0  epsilon: 1.0    steps: 704  evaluation reward: 25.93\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9331: Policy loss: 0.017505. Value loss: 0.789601. Entropy: 0.959412.\n",
      "Iteration 9332: Policy loss: 0.004986. Value loss: 0.343191. Entropy: 0.948377.\n",
      "Iteration 9333: Policy loss: -0.002268. Value loss: 0.162188. Entropy: 0.965215.\n",
      "episode: 3786   score: 36.0  epsilon: 1.0    steps: 264  evaluation reward: 25.95\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9334: Policy loss: 0.005785. Value loss: 0.542899. Entropy: 0.982617.\n",
      "Iteration 9335: Policy loss: 0.002630. Value loss: 0.199743. Entropy: 0.983124.\n",
      "Iteration 9336: Policy loss: -0.013547. Value loss: 0.118864. Entropy: 0.990189.\n",
      "episode: 3787   score: 29.0  epsilon: 1.0    steps: 128  evaluation reward: 26.0\n",
      "episode: 3788   score: 51.0  epsilon: 1.0    steps: 656  evaluation reward: 26.23\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9337: Policy loss: 0.007832. Value loss: 0.489378. Entropy: 0.897815.\n",
      "Iteration 9338: Policy loss: -0.000820. Value loss: 0.194896. Entropy: 0.875575.\n",
      "Iteration 9339: Policy loss: -0.010224. Value loss: 0.096984. Entropy: 0.884543.\n",
      "episode: 3789   score: 35.0  epsilon: 1.0    steps: 24  evaluation reward: 26.32\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9340: Policy loss: 0.018348. Value loss: 0.722698. Entropy: 0.941693.\n",
      "Iteration 9341: Policy loss: 0.003202. Value loss: 0.381382. Entropy: 0.968653.\n",
      "Iteration 9342: Policy loss: -0.011722. Value loss: 0.253521. Entropy: 0.964228.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9343: Policy loss: 0.013137. Value loss: 0.779725. Entropy: 0.999050.\n",
      "Iteration 9344: Policy loss: -0.005235. Value loss: 0.384380. Entropy: 0.989465.\n",
      "Iteration 9345: Policy loss: -0.010048. Value loss: 0.200155. Entropy: 0.990603.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9346: Policy loss: 0.016622. Value loss: 0.670754. Entropy: 0.926457.\n",
      "Iteration 9347: Policy loss: 0.002795. Value loss: 0.255933. Entropy: 0.942867.\n",
      "Iteration 9348: Policy loss: -0.010205. Value loss: 0.130124. Entropy: 0.951223.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9349: Policy loss: 0.013991. Value loss: 0.534563. Entropy: 0.906479.\n",
      "Iteration 9350: Policy loss: 0.000444. Value loss: 0.263961. Entropy: 0.899649.\n",
      "Iteration 9351: Policy loss: -0.011875. Value loss: 0.164179. Entropy: 0.888348.\n",
      "episode: 3790   score: 24.0  epsilon: 1.0    steps: 472  evaluation reward: 26.5\n",
      "episode: 3791   score: 40.0  epsilon: 1.0    steps: 616  evaluation reward: 26.83\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9352: Policy loss: 0.010746. Value loss: 0.719949. Entropy: 1.056596.\n",
      "Iteration 9353: Policy loss: -0.001622. Value loss: 0.309874. Entropy: 1.044181.\n",
      "Iteration 9354: Policy loss: -0.007398. Value loss: 0.196100. Entropy: 1.052472.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3792   score: 18.0  epsilon: 1.0    steps: 384  evaluation reward: 26.82\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9355: Policy loss: 0.003439. Value loss: 0.566411. Entropy: 0.966028.\n",
      "Iteration 9356: Policy loss: -0.004150. Value loss: 0.274021. Entropy: 0.973757.\n",
      "Iteration 9357: Policy loss: -0.012554. Value loss: 0.170054. Entropy: 0.968309.\n",
      "episode: 3793   score: 30.0  epsilon: 1.0    steps: 424  evaluation reward: 26.91\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9358: Policy loss: 0.009030. Value loss: 0.804761. Entropy: 0.920980.\n",
      "Iteration 9359: Policy loss: -0.001625. Value loss: 0.326817. Entropy: 0.916075.\n",
      "Iteration 9360: Policy loss: -0.008229. Value loss: 0.145830. Entropy: 0.908789.\n",
      "episode: 3794   score: 23.0  epsilon: 1.0    steps: 408  evaluation reward: 26.99\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9361: Policy loss: 0.010843. Value loss: 0.693245. Entropy: 0.854609.\n",
      "Iteration 9362: Policy loss: -0.002840. Value loss: 0.348365. Entropy: 0.848293.\n",
      "Iteration 9363: Policy loss: -0.009505. Value loss: 0.234083. Entropy: 0.841028.\n",
      "episode: 3795   score: 30.0  epsilon: 1.0    steps: 832  evaluation reward: 26.98\n",
      "episode: 3796   score: 27.0  epsilon: 1.0    steps: 1024  evaluation reward: 27.0\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9364: Policy loss: 0.011268. Value loss: 0.289943. Entropy: 0.898376.\n",
      "Iteration 9365: Policy loss: -0.002377. Value loss: 0.114928. Entropy: 0.897192.\n",
      "Iteration 9366: Policy loss: -0.016931. Value loss: 0.067427. Entropy: 0.902596.\n",
      "episode: 3797   score: 25.0  epsilon: 1.0    steps: 808  evaluation reward: 26.84\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9367: Policy loss: 0.007288. Value loss: 0.331318. Entropy: 0.905468.\n",
      "Iteration 9368: Policy loss: -0.005943. Value loss: 0.092883. Entropy: 0.906127.\n",
      "Iteration 9369: Policy loss: -0.011244. Value loss: 0.048885. Entropy: 0.906489.\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9370: Policy loss: 0.008159. Value loss: 0.424088. Entropy: 0.889370.\n",
      "Iteration 9371: Policy loss: -0.007028. Value loss: 0.164309. Entropy: 0.884572.\n",
      "Iteration 9372: Policy loss: -0.016152. Value loss: 0.087494. Entropy: 0.893028.\n",
      "episode: 3798   score: 15.0  epsilon: 1.0    steps: 144  evaluation reward: 26.83\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9373: Policy loss: 0.010962. Value loss: 0.427607. Entropy: 0.946199.\n",
      "Iteration 9374: Policy loss: -0.001953. Value loss: 0.193866. Entropy: 0.951147.\n",
      "Iteration 9375: Policy loss: -0.012955. Value loss: 0.129057. Entropy: 0.946064.\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9376: Policy loss: 0.018697. Value loss: 0.585559. Entropy: 0.958994.\n",
      "Iteration 9377: Policy loss: 0.002786. Value loss: 0.233885. Entropy: 0.963200.\n",
      "Iteration 9378: Policy loss: -0.001912. Value loss: 0.146707. Entropy: 0.946396.\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9379: Policy loss: 0.015368. Value loss: 0.818984. Entropy: 0.920506.\n",
      "Iteration 9380: Policy loss: -0.000162. Value loss: 0.278993. Entropy: 0.910691.\n",
      "Iteration 9381: Policy loss: -0.010525. Value loss: 0.129019. Entropy: 0.918415.\n",
      "episode: 3799   score: 16.0  epsilon: 1.0    steps: 520  evaluation reward: 26.86\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9382: Policy loss: 0.012741. Value loss: 0.865165. Entropy: 0.914524.\n",
      "Iteration 9383: Policy loss: -0.001481. Value loss: 0.332500. Entropy: 0.876976.\n",
      "Iteration 9384: Policy loss: -0.015086. Value loss: 0.180200. Entropy: 0.875946.\n",
      "episode: 3800   score: 23.0  epsilon: 1.0    steps: 128  evaluation reward: 26.91\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9385: Policy loss: 0.005148. Value loss: 0.475314. Entropy: 0.874807.\n",
      "Iteration 9386: Policy loss: -0.007215. Value loss: 0.207666. Entropy: 0.882227.\n",
      "Iteration 9387: Policy loss: -0.016373. Value loss: 0.117472. Entropy: 0.880621.\n",
      "now time :  2019-03-06 15:48:01.518264\n",
      "episode: 3801   score: 35.0  epsilon: 1.0    steps: 256  evaluation reward: 26.88\n",
      "episode: 3802   score: 27.0  epsilon: 1.0    steps: 696  evaluation reward: 26.92\n",
      "episode: 3803   score: 19.0  epsilon: 1.0    steps: 920  evaluation reward: 26.9\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9388: Policy loss: 0.011666. Value loss: 0.778370. Entropy: 0.816881.\n",
      "Iteration 9389: Policy loss: 0.007771. Value loss: 0.278110. Entropy: 0.802943.\n",
      "Iteration 9390: Policy loss: -0.009730. Value loss: 0.142856. Entropy: 0.816177.\n",
      "episode: 3804   score: 23.0  epsilon: 1.0    steps: 832  evaluation reward: 26.9\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9391: Policy loss: 0.006880. Value loss: 0.417217. Entropy: 0.927978.\n",
      "Iteration 9392: Policy loss: -0.000935. Value loss: 0.172610. Entropy: 0.933538.\n",
      "Iteration 9393: Policy loss: -0.016320. Value loss: 0.122817. Entropy: 0.931655.\n",
      "episode: 3805   score: 17.0  epsilon: 1.0    steps: 56  evaluation reward: 26.85\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9394: Policy loss: 0.008974. Value loss: 0.386848. Entropy: 0.857131.\n",
      "Iteration 9395: Policy loss: -0.001091. Value loss: 0.157690. Entropy: 0.852483.\n",
      "Iteration 9396: Policy loss: -0.000682. Value loss: 0.080261. Entropy: 0.864565.\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9397: Policy loss: 0.017561. Value loss: 0.754452. Entropy: 0.899723.\n",
      "Iteration 9398: Policy loss: 0.002793. Value loss: 0.297535. Entropy: 0.896100.\n",
      "Iteration 9399: Policy loss: -0.008466. Value loss: 0.129319. Entropy: 0.885389.\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9400: Policy loss: 0.019169. Value loss: 0.464622. Entropy: 0.874208.\n",
      "Iteration 9401: Policy loss: 0.007398. Value loss: 0.112524. Entropy: 0.873239.\n",
      "Iteration 9402: Policy loss: -0.008212. Value loss: 0.058152. Entropy: 0.880662.\n",
      "episode: 3806   score: 55.0  epsilon: 1.0    steps: 688  evaluation reward: 27.19\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9403: Policy loss: 0.019626. Value loss: 0.502500. Entropy: 0.921546.\n",
      "Iteration 9404: Policy loss: 0.005033. Value loss: 0.209951. Entropy: 0.915035.\n",
      "Iteration 9405: Policy loss: -0.009094. Value loss: 0.117580. Entropy: 0.925424.\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9406: Policy loss: 0.016033. Value loss: 0.685931. Entropy: 0.922678.\n",
      "Iteration 9407: Policy loss: -0.002164. Value loss: 0.332522. Entropy: 0.910043.\n",
      "Iteration 9408: Policy loss: -0.010804. Value loss: 0.191997. Entropy: 0.917238.\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9409: Policy loss: 0.007372. Value loss: 0.661950. Entropy: 0.905834.\n",
      "Iteration 9410: Policy loss: -0.004320. Value loss: 0.275324. Entropy: 0.909842.\n",
      "Iteration 9411: Policy loss: -0.009595. Value loss: 0.145648. Entropy: 0.894956.\n",
      "episode: 3807   score: 25.0  epsilon: 1.0    steps: 736  evaluation reward: 27.24\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9412: Policy loss: 0.010534. Value loss: 0.579794. Entropy: 0.945947.\n",
      "Iteration 9413: Policy loss: -0.005142. Value loss: 0.251752. Entropy: 0.942072.\n",
      "Iteration 9414: Policy loss: -0.012520. Value loss: 0.156660. Entropy: 0.939392.\n",
      "episode: 3808   score: 29.0  epsilon: 1.0    steps: 632  evaluation reward: 27.36\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9415: Policy loss: 0.011134. Value loss: 0.949580. Entropy: 0.934526.\n",
      "Iteration 9416: Policy loss: 0.001717. Value loss: 0.415537. Entropy: 0.907887.\n",
      "Iteration 9417: Policy loss: -0.010851. Value loss: 0.215430. Entropy: 0.907750.\n",
      "episode: 3809   score: 33.0  epsilon: 1.0    steps: 72  evaluation reward: 27.39\n",
      "episode: 3810   score: 45.0  epsilon: 1.0    steps: 1024  evaluation reward: 27.59\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9418: Policy loss: 0.014782. Value loss: 0.509476. Entropy: 0.913280.\n",
      "Iteration 9419: Policy loss: 0.004409. Value loss: 0.219900. Entropy: 0.893266.\n",
      "Iteration 9420: Policy loss: -0.007342. Value loss: 0.116331. Entropy: 0.898651.\n",
      "episode: 3811   score: 32.0  epsilon: 1.0    steps: 144  evaluation reward: 27.57\n",
      "episode: 3812   score: 21.0  epsilon: 1.0    steps: 416  evaluation reward: 27.59\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9421: Policy loss: 0.014805. Value loss: 0.663355. Entropy: 0.957777.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9422: Policy loss: -0.000544. Value loss: 0.323217. Entropy: 0.946247.\n",
      "Iteration 9423: Policy loss: -0.001883. Value loss: 0.193364. Entropy: 0.951093.\n",
      "episode: 3813   score: 31.0  epsilon: 1.0    steps: 952  evaluation reward: 27.52\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9424: Policy loss: 0.017955. Value loss: 0.637136. Entropy: 0.894556.\n",
      "Iteration 9425: Policy loss: 0.009436. Value loss: 0.293663. Entropy: 0.904422.\n",
      "Iteration 9426: Policy loss: -0.006884. Value loss: 0.172581. Entropy: 0.900164.\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9427: Policy loss: 0.005201. Value loss: 0.347610. Entropy: 0.945189.\n",
      "Iteration 9428: Policy loss: -0.006746. Value loss: 0.185642. Entropy: 0.940770.\n",
      "Iteration 9429: Policy loss: -0.016055. Value loss: 0.127150. Entropy: 0.937647.\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9430: Policy loss: 0.011788. Value loss: 0.276701. Entropy: 0.913673.\n",
      "Iteration 9431: Policy loss: 0.001249. Value loss: 0.115501. Entropy: 0.925598.\n",
      "Iteration 9432: Policy loss: -0.016332. Value loss: 0.074476. Entropy: 0.919788.\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9433: Policy loss: 0.008218. Value loss: 0.457009. Entropy: 0.907207.\n",
      "Iteration 9434: Policy loss: -0.003000. Value loss: 0.230072. Entropy: 0.908960.\n",
      "Iteration 9435: Policy loss: -0.015579. Value loss: 0.149875. Entropy: 0.909012.\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9436: Policy loss: 0.013916. Value loss: 0.706089. Entropy: 0.932363.\n",
      "Iteration 9437: Policy loss: 0.003648. Value loss: 0.313954. Entropy: 0.931980.\n",
      "Iteration 9438: Policy loss: -0.006597. Value loss: 0.176382. Entropy: 0.927433.\n",
      "episode: 3814   score: 38.0  epsilon: 1.0    steps: 248  evaluation reward: 27.72\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9439: Policy loss: 0.005220. Value loss: 0.614699. Entropy: 1.002918.\n",
      "Iteration 9440: Policy loss: 0.002180. Value loss: 0.282681. Entropy: 0.997208.\n",
      "Iteration 9441: Policy loss: -0.012037. Value loss: 0.187393. Entropy: 0.995395.\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9442: Policy loss: 0.010988. Value loss: 0.833888. Entropy: 0.904661.\n",
      "Iteration 9443: Policy loss: 0.006005. Value loss: 0.301101. Entropy: 0.903188.\n",
      "Iteration 9444: Policy loss: -0.008382. Value loss: 0.169953. Entropy: 0.910209.\n",
      "episode: 3815   score: 31.0  epsilon: 1.0    steps: 376  evaluation reward: 27.75\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9445: Policy loss: 0.019750. Value loss: 0.820181. Entropy: 0.959564.\n",
      "Iteration 9446: Policy loss: 0.003507. Value loss: 0.335711. Entropy: 0.961799.\n",
      "Iteration 9447: Policy loss: -0.008957. Value loss: 0.162610. Entropy: 0.944764.\n",
      "episode: 3816   score: 25.0  epsilon: 1.0    steps: 240  evaluation reward: 27.77\n",
      "episode: 3817   score: 21.0  epsilon: 1.0    steps: 248  evaluation reward: 27.75\n",
      "episode: 3818   score: 33.0  epsilon: 1.0    steps: 424  evaluation reward: 27.85\n",
      "episode: 3819   score: 24.0  epsilon: 1.0    steps: 800  evaluation reward: 27.9\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9448: Policy loss: 0.013076. Value loss: 0.591072. Entropy: 0.858565.\n",
      "Iteration 9449: Policy loss: 0.007006. Value loss: 0.249097. Entropy: 0.858751.\n",
      "Iteration 9450: Policy loss: -0.001588. Value loss: 0.157201. Entropy: 0.865027.\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9451: Policy loss: 0.007876. Value loss: 0.567928. Entropy: 0.862027.\n",
      "Iteration 9452: Policy loss: -0.003326. Value loss: 0.217789. Entropy: 0.847191.\n",
      "Iteration 9453: Policy loss: -0.011119. Value loss: 0.117598. Entropy: 0.850717.\n",
      "episode: 3820   score: 36.0  epsilon: 1.0    steps: 240  evaluation reward: 27.91\n",
      "episode: 3821   score: 42.0  epsilon: 1.0    steps: 912  evaluation reward: 27.92\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9454: Policy loss: 0.010587. Value loss: 0.773036. Entropy: 0.825770.\n",
      "Iteration 9455: Policy loss: -0.003958. Value loss: 0.360504. Entropy: 0.825183.\n",
      "Iteration 9456: Policy loss: -0.012384. Value loss: 0.223635. Entropy: 0.824996.\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9457: Policy loss: 0.009067. Value loss: 0.519036. Entropy: 0.920878.\n",
      "Iteration 9458: Policy loss: -0.007920. Value loss: 0.198364. Entropy: 0.919106.\n",
      "Iteration 9459: Policy loss: -0.014534. Value loss: 0.113776. Entropy: 0.917643.\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9460: Policy loss: 0.010553. Value loss: 0.736528. Entropy: 0.979010.\n",
      "Iteration 9461: Policy loss: 0.006132. Value loss: 0.344351. Entropy: 0.971140.\n",
      "Iteration 9462: Policy loss: -0.012478. Value loss: 0.163968. Entropy: 0.959418.\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9463: Policy loss: 0.011741. Value loss: 0.524499. Entropy: 0.983406.\n",
      "Iteration 9464: Policy loss: -0.009731. Value loss: 0.207946. Entropy: 0.965851.\n",
      "Iteration 9465: Policy loss: -0.013817. Value loss: 0.134508. Entropy: 0.959060.\n",
      "episode: 3822   score: 11.0  epsilon: 1.0    steps: 88  evaluation reward: 27.78\n",
      "episode: 3823   score: 17.0  epsilon: 1.0    steps: 544  evaluation reward: 27.63\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9466: Policy loss: 0.009897. Value loss: 0.602635. Entropy: 0.863078.\n",
      "Iteration 9467: Policy loss: 0.005507. Value loss: 0.194389. Entropy: 0.866710.\n",
      "Iteration 9468: Policy loss: -0.010942. Value loss: 0.106036. Entropy: 0.867315.\n",
      "episode: 3824   score: 23.0  epsilon: 1.0    steps: 808  evaluation reward: 27.65\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9469: Policy loss: 0.013754. Value loss: 0.854718. Entropy: 0.899385.\n",
      "Iteration 9470: Policy loss: 0.010728. Value loss: 0.423037. Entropy: 0.887022.\n",
      "Iteration 9471: Policy loss: -0.003665. Value loss: 0.244059. Entropy: 0.874551.\n",
      "episode: 3825   score: 43.0  epsilon: 1.0    steps: 256  evaluation reward: 27.77\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9472: Policy loss: 0.009266. Value loss: 0.550776. Entropy: 0.887000.\n",
      "Iteration 9473: Policy loss: -0.005844. Value loss: 0.181085. Entropy: 0.891240.\n",
      "Iteration 9474: Policy loss: -0.012614. Value loss: 0.104633. Entropy: 0.886291.\n",
      "episode: 3826   score: 17.0  epsilon: 1.0    steps: 664  evaluation reward: 27.61\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9475: Policy loss: 0.010183. Value loss: 0.977684. Entropy: 0.814331.\n",
      "Iteration 9476: Policy loss: 0.000264. Value loss: 0.416624. Entropy: 0.816528.\n",
      "Iteration 9477: Policy loss: -0.010927. Value loss: 0.191949. Entropy: 0.826067.\n",
      "episode: 3827   score: 31.0  epsilon: 1.0    steps: 96  evaluation reward: 27.6\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9478: Policy loss: 0.006705. Value loss: 0.456976. Entropy: 0.931732.\n"
     ]
    }
   ],
   "source": [
    "### Loop through all environments and run PPO on them\n",
    "\n",
    "env_names = ['Breakout-v0', 'Phoenix-v0', 'Asteroids-v0', 'SpaceInvaders-v0', 'MsPacman-v0', 'Asterix-v0', 'Atlantis-v0', 'Alien-v0', 'Amidar-v0', 'Assault-v0', 'BankHeist-v0']\n",
    "for a in range(len(env_names)):\n",
    "    name = env_names[a]\n",
    "    print(\"\\n\\n\\n ------- STARTING TRAINING FOR %s ------- \\n\\n\\n\" % (name))\n",
    "    \n",
    "    envs = []\n",
    "    for i in range(num_envs):\n",
    "        envs.append(GameEnv(name))\n",
    "    #env.render()\n",
    "    \n",
    "\n",
    "    number_lives = envs[0].life\n",
    "    state_size = envs[0].observation_space.shape\n",
    "    if (name == 'SpaceInvaders-v0' or name == 'Breakout-v0'):\n",
    "        action_size = 4\n",
    "    else:\n",
    "        action_size = envs[0].action_space.n\n",
    "    rewards, episodes = [], []\n",
    "\n",
    "    vis_env_idx = 0\n",
    "    vis_env = envs[vis_env_idx]\n",
    "    e = 0\n",
    "    frame = 0\n",
    "    max_eval = -np.inf\n",
    "    reset_count = 0\n",
    "\n",
    "\n",
    "    agent = Agent(action_size)\n",
    "    torch.save(agent.policy_net.state_dict(), \"./save_model/\" + name + \"_best\")\n",
    "    evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "    frame = 0\n",
    "    memory_size = 0\n",
    "    reset_max = 10\n",
    "    \n",
    "    print(\"Determing min/max rewards of environment\")\n",
    "    [low, high] = score_range = get_score_range(name)\n",
    "    print(\"Min: %d. Max: %d.\" % (low, high))\n",
    "\n",
    "    while (frame < 10000000):\n",
    "        step = 0\n",
    "        assert(num_envs * env_mem_size == train_frame)\n",
    "        frame_next_vals = []\n",
    "        \n",
    "        for j in range(env_mem_size):\n",
    "            \n",
    "            curr_states = np.stack([envs[i].history[HISTORY_SIZE-1,:,:] for i in range(num_envs)])\n",
    "            next_states = []\n",
    "            net_in = np.stack([envs[i].history[:HISTORY_SIZE,:,:] for i in range(num_envs)])\n",
    "            step += num_envs\n",
    "            frame += num_envs\n",
    "            actions, values = agent.get_action(np.float32(net_in) / 255.)\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                next_states.append(next_state)\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                \"\"\"\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "                \"\"\"\n",
    "                \n",
    "                frame_next_state = get_frame(next_states[i])\n",
    "                env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "                terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "                env.life = env.info['ale.lives']\n",
    "                r = (env.reward / high) * 20.0 #np.log(max(env.reward+1, 1))#((env.reward - low) / (high - low)) * 30\n",
    "                agent.memory.push(i, deepcopy(curr_states[i]), actions[i], r, terminal_state, values[i], 0, 0)\n",
    "                \n",
    "                if (j == env_mem_size-1):\n",
    "                    net_in = np.stack([envs[k].history[1:,:,:] for k in range(num_envs)])\n",
    "                    _, frame_next_vals = agent.get_action(np.float32(net_in) / 255.)\n",
    "                \n",
    "                env.score += env.reward\n",
    "                env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "        \n",
    "                if (env.done):\n",
    "                    if (e % 50 == 0):\n",
    "                        print('now time : ', datetime.now())\n",
    "                        rewards.append(np.mean(evaluation_reward))\n",
    "                        episodes.append(e)\n",
    "                        pylab.plot(episodes, rewards, 'b')\n",
    "                        pylab.savefig(\"./save_graph/\" + name + \"_ppo.png\")\n",
    "                        torch.save(agent.policy_net, \"./save_model/\" + name + \"_ppo\")\n",
    "\n",
    "                        if np.mean(evaluation_reward) > max_eval:\n",
    "                            torch.save(agent.policy_net.state_dict(), \"./save_model/\"  + name + \"_ppo_best\")\n",
    "                            max_eval = float(np.mean(evaluation_reward))\n",
    "                            reset_count = 0\n",
    "                        elif e > 5000:\n",
    "                            reset_count += 1\n",
    "                            \"\"\"\n",
    "                            if (reset_count == reset_max):\n",
    "                                print(\"Training went nowhere, starting again at best model\")\n",
    "                                agent.policy_net.load_state_dict(torch.load(\"./save_model/spaceinvaders_ppo_best\"))\n",
    "                                agent.update_target_net()\n",
    "                                reset_count = 0\n",
    "                            \"\"\"\n",
    "                    e += 1\n",
    "                    evaluation_reward.append(env.score)\n",
    "                    print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                      \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "                    env.done = False\n",
    "                    env.score = 0\n",
    "                    env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "                    env.state = env.reset()\n",
    "                    env.life = number_lives\n",
    "                    get_init_state(env.history, env.state)\n",
    "            \n",
    "        agent.train_policy_net(frame, frame_next_vals)\n",
    "        agent.update_target_net()\n",
    "    print(\"FINISHED TRAINING FOR %s\" % (name))\n",
    "    pylab.figure()\n",
    "    \n",
    "    for i in range(len(envs)):\n",
    "        envs[i]._env.close()\n",
    "    del envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best(name):\n",
    "    env = GameEnv(name)\n",
    "    print(\"\\n\\n\\n ------- TESTING BEST MODEL FOR %s ------- \\n\\n\\n\" % (name))\n",
    "    number_lives = env.life\n",
    "    \n",
    "    if (name == 'SpaceInvaders-v0'):\n",
    "        action_size = 4\n",
    "    else:\n",
    "        action_size = env.action_space.n\n",
    "    rewards, episodes = [], []\n",
    "    \n",
    "    e = 0\n",
    "    frame = 0\n",
    "\n",
    "    agent = Agent(action_size)\n",
    "    agent.policy_net.load_state_dict(torch.load(\"./save_model/\" + name + \"_ppo_best\"))\n",
    "    agent.update_target_net()\n",
    "    agent.policy_net.eval()\n",
    "    evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "\n",
    "    for i in range(100):\n",
    "        env.done = False\n",
    "        env.score = 0\n",
    "        env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "        env.state = env.reset()\n",
    "        env.life = number_lives\n",
    "        get_init_state(env.history, env.state)\n",
    "        step = 0\n",
    "        while not env.done:\n",
    "            curr_state = env.history[HISTORY_SIZE-1,:,:]\n",
    "            net_in = env.history[:HISTORY_SIZE,:,:]\n",
    "            action, value = agent.get_action(np.float32(net_in) / 255.)\n",
    "            \n",
    "            next_state, env.reward, env.done, env.info = env.step(action)\n",
    "            env._env.render()\n",
    "            \n",
    "            frame_next_state = get_frame(next_state)\n",
    "            \n",
    "            env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "            terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "            env.life = env.info['ale.lives']\n",
    "            \n",
    "            \n",
    "            env.score += env.reward\n",
    "            env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "            step += 1\n",
    "        \n",
    "\n",
    "        evaluation_reward.append(env.score)\n",
    "        print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                      \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],1)[0].unsqueeze(1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " ------- TESTING BEST MODEL FOR MsPacman-v0 ------- \n",
      "\n",
      "\n",
      "\n",
      "episode: 0   score: 1270.0  epsilon: 1.0    steps: 965  evaluation reward: 1270.0\n",
      "episode: 0   score: 1450.0  epsilon: 1.0    steps: 1162  evaluation reward: 1360.0\n",
      "episode: 0   score: 1270.0  epsilon: 1.0    steps: 746  evaluation reward: 1330.0\n",
      "episode: 0   score: 1790.0  epsilon: 1.0    steps: 1058  evaluation reward: 1445.0\n",
      "episode: 0   score: 1160.0  epsilon: 1.0    steps: 1112  evaluation reward: 1388.0\n",
      "episode: 0   score: 1350.0  epsilon: 1.0    steps: 1073  evaluation reward: 1381.6666666666667\n",
      "episode: 0   score: 1500.0  epsilon: 1.0    steps: 1265  evaluation reward: 1398.5714285714287\n",
      "episode: 0   score: 1270.0  epsilon: 1.0    steps: 960  evaluation reward: 1382.5\n",
      "episode: 0   score: 1670.0  epsilon: 1.0    steps: 1405  evaluation reward: 1414.4444444444443\n",
      "episode: 0   score: 1230.0  epsilon: 1.0    steps: 1457  evaluation reward: 1396.0\n",
      "episode: 0   score: 1600.0  epsilon: 1.0    steps: 1223  evaluation reward: 1414.5454545454545\n",
      "episode: 0   score: 1940.0  epsilon: 1.0    steps: 1265  evaluation reward: 1458.3333333333333\n",
      "episode: 0   score: 1950.0  epsilon: 1.0    steps: 982  evaluation reward: 1496.1538461538462\n",
      "episode: 0   score: 1460.0  epsilon: 1.0    steps: 1157  evaluation reward: 1493.5714285714287\n",
      "episode: 0   score: 1430.0  epsilon: 1.0    steps: 1037  evaluation reward: 1489.3333333333333\n",
      "episode: 0   score: 1420.0  epsilon: 1.0    steps: 1048  evaluation reward: 1485.0\n",
      "episode: 0   score: 870.0  epsilon: 1.0    steps: 592  evaluation reward: 1448.8235294117646\n",
      "episode: 0   score: 1590.0  epsilon: 1.0    steps: 1071  evaluation reward: 1456.6666666666667\n",
      "episode: 0   score: 770.0  epsilon: 1.0    steps: 892  evaluation reward: 1420.5263157894738\n",
      "episode: 0   score: 1340.0  epsilon: 1.0    steps: 980  evaluation reward: 1416.5\n",
      "episode: 0   score: 1570.0  epsilon: 1.0    steps: 1165  evaluation reward: 1423.8095238095239\n",
      "episode: 0   score: 740.0  epsilon: 1.0    steps: 584  evaluation reward: 1392.7272727272727\n",
      "episode: 0   score: 860.0  epsilon: 1.0    steps: 1396  evaluation reward: 1369.5652173913043\n",
      "episode: 0   score: 1470.0  epsilon: 1.0    steps: 1022  evaluation reward: 1373.75\n",
      "episode: 0   score: 1650.0  epsilon: 1.0    steps: 1447  evaluation reward: 1384.8\n",
      "episode: 0   score: 1470.0  epsilon: 1.0    steps: 1196  evaluation reward: 1388.076923076923\n",
      "episode: 0   score: 1530.0  epsilon: 1.0    steps: 1181  evaluation reward: 1393.3333333333333\n",
      "episode: 0   score: 1470.0  epsilon: 1.0    steps: 1059  evaluation reward: 1396.0714285714287\n",
      "episode: 0   score: 2420.0  epsilon: 1.0    steps: 1168  evaluation reward: 1431.3793103448277\n",
      "episode: 0   score: 930.0  epsilon: 1.0    steps: 925  evaluation reward: 1414.6666666666667\n",
      "episode: 0   score: 1250.0  epsilon: 1.0    steps: 1156  evaluation reward: 1409.3548387096773\n"
     ]
    }
   ],
   "source": [
    "test_best('MsPacman-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
