{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "from env import GameEnv\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "envs = []\n",
    "for i in range(num_envs):\n",
    "    envs.append(GameEnv('SpaceInvadersDeterministic-v4'))\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = envs[0].life\n",
    "state_size = envs[0].observation_space.shape\n",
    "action_size = envs[0].action_space.n\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "torch.save(agent.policy_net.state_dict(), \"./save_model/spaceinvaders_ppo_best\")\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n",
    "reset_max = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],1)[0].unsqueeze(1))\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000250. clip: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:255: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:256: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:257: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Policy loss: -1.094032. Value loss: 6.568430. Entropy: 1.787648.\n",
      "Iteration 2: Policy loss: -1.059619. Value loss: 5.662950. Entropy: 1.787355.\n",
      "Iteration 3: Policy loss: -1.087144. Value loss: 5.936832. Entropy: 1.787368.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 4: Policy loss: -3.762961. Value loss: 23.942623. Entropy: 1.753100.\n",
      "Iteration 5: Policy loss: -3.626564. Value loss: 21.021532. Entropy: 1.765004.\n",
      "Iteration 6: Policy loss: -3.676214. Value loss: 18.459532. Entropy: 1.769020.\n",
      "now time :  2019-02-26 12:27:11.380270\n",
      "episode: 1   score: 105.0  epsilon: 1.0    steps: 255  evaluation reward: 105.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2   score: 35.0  epsilon: 1.0    steps: 896  evaluation reward: 70.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 7: Policy loss: -0.501706. Value loss: 21.333206. Entropy: 1.764121.\n",
      "Iteration 8: Policy loss: -0.357049. Value loss: 19.093555. Entropy: 1.760776.\n",
      "Iteration 9: Policy loss: -0.408432. Value loss: 18.803070. Entropy: 1.770087.\n",
      "episode: 3   score: 65.0  epsilon: 1.0    steps: 2  evaluation reward: 68.33333333333333\n",
      "episode: 4   score: 80.0  epsilon: 1.0    steps: 910  evaluation reward: 71.25\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 10: Policy loss: -1.910898. Value loss: 37.708187. Entropy: 1.745175.\n",
      "Iteration 11: Policy loss: -2.176072. Value loss: 38.048820. Entropy: 1.751735.\n",
      "Iteration 12: Policy loss: -1.972138. Value loss: 35.902405. Entropy: 1.746001.\n",
      "episode: 5   score: 125.0  epsilon: 1.0    steps: 380  evaluation reward: 82.0\n",
      "episode: 6   score: 120.0  epsilon: 1.0    steps: 646  evaluation reward: 88.33333333333333\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 13: Policy loss: -3.919967. Value loss: 43.589821. Entropy: 1.720170.\n",
      "Iteration 14: Policy loss: -3.956326. Value loss: 38.351189. Entropy: 1.728888.\n",
      "Iteration 15: Policy loss: -3.945714. Value loss: 32.370884. Entropy: 1.721680.\n",
      "episode: 7   score: 140.0  epsilon: 1.0    steps: 402  evaluation reward: 95.71428571428571\n",
      "episode: 8   score: 120.0  epsilon: 1.0    steps: 541  evaluation reward: 98.75\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 16: Policy loss: -0.443037. Value loss: 27.030287. Entropy: 1.654595.\n",
      "Iteration 17: Policy loss: -0.350171. Value loss: 19.387470. Entropy: 1.666796.\n",
      "Iteration 18: Policy loss: -0.442781. Value loss: 17.865414. Entropy: 1.665475.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 19: Policy loss: -4.209280. Value loss: 377.956757. Entropy: 1.634101.\n",
      "Iteration 20: Policy loss: -3.679299. Value loss: 277.682770. Entropy: 1.570777.\n",
      "Iteration 21: Policy loss: -3.818136. Value loss: 223.997940. Entropy: 1.612485.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 22: Policy loss: 0.875051. Value loss: 36.998138. Entropy: 1.648212.\n",
      "Iteration 23: Policy loss: 0.445439. Value loss: 23.713249. Entropy: 1.662119.\n",
      "Iteration 24: Policy loss: 0.893741. Value loss: 20.844755. Entropy: 1.652943.\n",
      "episode: 9   score: 120.0  epsilon: 1.0    steps: 871  evaluation reward: 101.11111111111111\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 25: Policy loss: 0.993941. Value loss: 41.968204. Entropy: 1.635186.\n",
      "Iteration 26: Policy loss: 0.792420. Value loss: 34.698204. Entropy: 1.622223.\n",
      "Iteration 27: Policy loss: 0.689505. Value loss: 33.906471. Entropy: 1.613713.\n",
      "episode: 10   score: 170.0  epsilon: 1.0    steps: 214  evaluation reward: 108.0\n",
      "episode: 11   score: 90.0  epsilon: 1.0    steps: 356  evaluation reward: 106.36363636363636\n",
      "episode: 12   score: 110.0  epsilon: 1.0    steps: 621  evaluation reward: 106.66666666666667\n",
      "episode: 13   score: 120.0  epsilon: 1.0    steps: 662  evaluation reward: 107.6923076923077\n",
      "episode: 14   score: 385.0  epsilon: 1.0    steps: 935  evaluation reward: 127.5\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 28: Policy loss: 0.849915. Value loss: 38.727669. Entropy: 1.628098.\n",
      "Iteration 29: Policy loss: 1.141092. Value loss: 31.108158. Entropy: 1.645301.\n",
      "Iteration 30: Policy loss: 1.179381. Value loss: 27.511501. Entropy: 1.633561.\n",
      "episode: 15   score: 180.0  epsilon: 1.0    steps: 441  evaluation reward: 131.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 31: Policy loss: -1.926132. Value loss: 47.062508. Entropy: 1.559766.\n",
      "Iteration 32: Policy loss: -1.872680. Value loss: 34.493626. Entropy: 1.562020.\n",
      "Iteration 33: Policy loss: -1.947375. Value loss: 32.636719. Entropy: 1.533051.\n",
      "episode: 16   score: 285.0  epsilon: 1.0    steps: 126  evaluation reward: 140.625\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 34: Policy loss: -0.170868. Value loss: 41.082020. Entropy: 1.477435.\n",
      "Iteration 35: Policy loss: -0.289769. Value loss: 39.579411. Entropy: 1.474935.\n",
      "Iteration 36: Policy loss: -0.352760. Value loss: 33.512146. Entropy: 1.471774.\n",
      "episode: 17   score: 85.0  epsilon: 1.0    steps: 704  evaluation reward: 137.35294117647058\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 37: Policy loss: -2.446547. Value loss: 332.493317. Entropy: 1.398741.\n",
      "Iteration 38: Policy loss: -0.070100. Value loss: 194.031097. Entropy: 1.213968.\n",
      "Iteration 39: Policy loss: -1.643373. Value loss: 225.174042. Entropy: 1.153046.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 40: Policy loss: 0.140590. Value loss: 39.054569. Entropy: 1.227451.\n",
      "Iteration 41: Policy loss: 0.120703. Value loss: 27.823666. Entropy: 1.235110.\n",
      "Iteration 42: Policy loss: 0.137701. Value loss: 25.988981. Entropy: 1.212812.\n",
      "episode: 18   score: 165.0  epsilon: 1.0    steps: 866  evaluation reward: 138.88888888888889\n",
      "episode: 19   score: 105.0  epsilon: 1.0    steps: 922  evaluation reward: 137.10526315789474\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 43: Policy loss: -2.334996. Value loss: 67.575119. Entropy: 1.253552.\n",
      "Iteration 44: Policy loss: -2.507641. Value loss: 54.491718. Entropy: 1.270622.\n",
      "Iteration 45: Policy loss: -2.416465. Value loss: 41.659462. Entropy: 1.266629.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 46: Policy loss: 2.999182. Value loss: 74.753761. Entropy: 1.202525.\n",
      "Iteration 47: Policy loss: 3.267269. Value loss: 53.781292. Entropy: 1.264855.\n",
      "Iteration 48: Policy loss: 2.627737. Value loss: 45.642357. Entropy: 1.297710.\n",
      "episode: 20   score: 255.0  epsilon: 1.0    steps: 384  evaluation reward: 143.0\n",
      "episode: 21   score: 160.0  epsilon: 1.0    steps: 548  evaluation reward: 143.8095238095238\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 49: Policy loss: -0.762239. Value loss: 259.957611. Entropy: 1.410169.\n",
      "Iteration 50: Policy loss: -0.596912. Value loss: 252.809555. Entropy: 1.374754.\n",
      "Iteration 51: Policy loss: -1.418622. Value loss: 239.507950. Entropy: 1.349813.\n",
      "episode: 22   score: 190.0  epsilon: 1.0    steps: 125  evaluation reward: 145.9090909090909\n",
      "episode: 23   score: 570.0  epsilon: 1.0    steps: 157  evaluation reward: 164.34782608695653\n",
      "episode: 24   score: 425.0  epsilon: 1.0    steps: 412  evaluation reward: 175.20833333333334\n",
      "episode: 25   score: 105.0  epsilon: 1.0    steps: 703  evaluation reward: 172.4\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 52: Policy loss: 0.512903. Value loss: 53.368481. Entropy: 1.114115.\n",
      "Iteration 53: Policy loss: 0.230052. Value loss: 36.440338. Entropy: 1.150067.\n",
      "Iteration 54: Policy loss: 0.651484. Value loss: 29.758133. Entropy: 1.126242.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 55: Policy loss: 1.828156. Value loss: 75.661339. Entropy: 1.039239.\n",
      "Iteration 56: Policy loss: 2.023129. Value loss: 65.020920. Entropy: 1.076314.\n",
      "Iteration 57: Policy loss: 1.835525. Value loss: 52.531902. Entropy: 1.052031.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 58: Policy loss: 2.501588. Value loss: 47.456032. Entropy: 1.300931.\n",
      "Iteration 59: Policy loss: 2.386642. Value loss: 37.179352. Entropy: 1.302971.\n",
      "Iteration 60: Policy loss: 2.390301. Value loss: 34.836910. Entropy: 1.329750.\n",
      "episode: 26   score: 105.0  epsilon: 1.0    steps: 833  evaluation reward: 169.80769230769232\n",
      "episode: 27   score: 180.0  epsilon: 1.0    steps: 1003  evaluation reward: 170.1851851851852\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 61: Policy loss: 1.856069. Value loss: 34.510029. Entropy: 1.365131.\n",
      "Iteration 62: Policy loss: 1.687822. Value loss: 29.879564. Entropy: 1.355245.\n",
      "Iteration 63: Policy loss: 2.103557. Value loss: 25.040329. Entropy: 1.367324.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 64: Policy loss: 0.630637. Value loss: 23.369928. Entropy: 1.255297.\n",
      "Iteration 65: Policy loss: 1.025237. Value loss: 18.758568. Entropy: 1.276227.\n",
      "Iteration 66: Policy loss: 0.615760. Value loss: 16.455790. Entropy: 1.255458.\n",
      "episode: 28   score: 155.0  epsilon: 1.0    steps: 170  evaluation reward: 169.64285714285714\n",
      "episode: 29   score: 180.0  epsilon: 1.0    steps: 603  evaluation reward: 170.0\n",
      "Training network. lr: 0.000250. clip: 0.099853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 67: Policy loss: -0.374049. Value loss: 57.317833. Entropy: 1.153395.\n",
      "Iteration 68: Policy loss: -0.356640. Value loss: 46.394497. Entropy: 1.207495.\n",
      "Iteration 69: Policy loss: 0.085022. Value loss: 50.242630. Entropy: 1.173969.\n",
      "episode: 30   score: 165.0  epsilon: 1.0    steps: 272  evaluation reward: 169.83333333333334\n",
      "episode: 31   score: 110.0  epsilon: 1.0    steps: 409  evaluation reward: 167.90322580645162\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 70: Policy loss: 1.645714. Value loss: 238.341461. Entropy: 1.154018.\n",
      "Iteration 71: Policy loss: 1.606470. Value loss: 180.611511. Entropy: 1.136458.\n",
      "Iteration 72: Policy loss: 0.898455. Value loss: 228.929352. Entropy: 1.173779.\n",
      "episode: 32   score: 210.0  epsilon: 1.0    steps: 76  evaluation reward: 169.21875\n",
      "episode: 33   score: 460.0  epsilon: 1.0    steps: 733  evaluation reward: 178.03030303030303\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 73: Policy loss: 0.793839. Value loss: 38.408730. Entropy: 1.110812.\n",
      "Iteration 74: Policy loss: 0.565368. Value loss: 27.025370. Entropy: 1.144695.\n",
      "Iteration 75: Policy loss: 0.583725. Value loss: 25.399471. Entropy: 1.162910.\n",
      "episode: 34   score: 105.0  epsilon: 1.0    steps: 1007  evaluation reward: 175.88235294117646\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 76: Policy loss: -1.947752. Value loss: 227.757416. Entropy: 1.221300.\n",
      "Iteration 77: Policy loss: -2.032320. Value loss: 147.190857. Entropy: 1.125222.\n",
      "Iteration 78: Policy loss: -1.375332. Value loss: 111.762405. Entropy: 1.101218.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 79: Policy loss: 2.265770. Value loss: 62.079433. Entropy: 0.946307.\n",
      "Iteration 80: Policy loss: 2.038564. Value loss: 39.122532. Entropy: 1.056903.\n",
      "Iteration 81: Policy loss: 1.812625. Value loss: 31.187653. Entropy: 0.998811.\n",
      "episode: 35   score: 105.0  epsilon: 1.0    steps: 219  evaluation reward: 173.85714285714286\n",
      "episode: 36   score: 155.0  epsilon: 1.0    steps: 772  evaluation reward: 173.33333333333334\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 82: Policy loss: 0.395604. Value loss: 20.272789. Entropy: 1.086894.\n",
      "Iteration 83: Policy loss: 0.696003. Value loss: 14.179317. Entropy: 1.116382.\n",
      "Iteration 84: Policy loss: 0.290224. Value loss: 15.105446. Entropy: 1.098484.\n",
      "episode: 37   score: 75.0  epsilon: 1.0    steps: 416  evaluation reward: 170.67567567567568\n",
      "episode: 38   score: 135.0  epsilon: 1.0    steps: 640  evaluation reward: 169.73684210526315\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 85: Policy loss: 1.622258. Value loss: 28.164570. Entropy: 1.052802.\n",
      "Iteration 86: Policy loss: 2.010831. Value loss: 21.783766. Entropy: 0.968374.\n",
      "Iteration 87: Policy loss: 2.046525. Value loss: 18.114939. Entropy: 1.023664.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 88: Policy loss: 0.884862. Value loss: 48.325493. Entropy: 0.829527.\n",
      "Iteration 89: Policy loss: 0.928712. Value loss: 35.489391. Entropy: 0.894137.\n",
      "Iteration 90: Policy loss: 0.883229. Value loss: 31.744648. Entropy: 0.895487.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 91: Policy loss: -0.550545. Value loss: 49.254513. Entropy: 0.955272.\n",
      "Iteration 92: Policy loss: -0.233994. Value loss: 41.352516. Entropy: 0.978419.\n",
      "Iteration 93: Policy loss: -0.321476. Value loss: 34.373760. Entropy: 0.956549.\n",
      "episode: 39   score: 210.0  epsilon: 1.0    steps: 14  evaluation reward: 170.76923076923077\n",
      "episode: 40   score: 50.0  epsilon: 1.0    steps: 434  evaluation reward: 167.75\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 94: Policy loss: 1.332260. Value loss: 23.498365. Entropy: 1.125322.\n",
      "Iteration 95: Policy loss: 1.185473. Value loss: 19.241137. Entropy: 1.118191.\n",
      "Iteration 96: Policy loss: 1.251670. Value loss: 18.430635. Entropy: 1.120543.\n",
      "episode: 41   score: 105.0  epsilon: 1.0    steps: 222  evaluation reward: 166.21951219512195\n",
      "episode: 42   score: 270.0  epsilon: 1.0    steps: 710  evaluation reward: 168.6904761904762\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 97: Policy loss: -1.517035. Value loss: 33.947891. Entropy: 0.937537.\n",
      "Iteration 98: Policy loss: -2.062099. Value loss: 28.095716. Entropy: 0.965831.\n",
      "Iteration 99: Policy loss: -1.632068. Value loss: 21.839993. Entropy: 1.006822.\n",
      "episode: 43   score: 515.0  epsilon: 1.0    steps: 365  evaluation reward: 176.74418604651163\n",
      "episode: 44   score: 180.0  epsilon: 1.0    steps: 823  evaluation reward: 176.8181818181818\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 100: Policy loss: -1.248951. Value loss: 28.584991. Entropy: 0.955297.\n",
      "Iteration 101: Policy loss: -1.179283. Value loss: 25.327692. Entropy: 0.902198.\n",
      "Iteration 102: Policy loss: -1.093908. Value loss: 22.842461. Entropy: 0.963756.\n",
      "episode: 45   score: 315.0  epsilon: 1.0    steps: 914  evaluation reward: 179.88888888888889\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 103: Policy loss: 0.600072. Value loss: 19.749556. Entropy: 0.799796.\n",
      "Iteration 104: Policy loss: 0.524606. Value loss: 16.918686. Entropy: 0.791783.\n",
      "Iteration 105: Policy loss: 0.542733. Value loss: 17.081825. Entropy: 0.786451.\n",
      "episode: 46   score: 135.0  epsilon: 1.0    steps: 531  evaluation reward: 178.91304347826087\n",
      "episode: 47   score: 80.0  epsilon: 1.0    steps: 736  evaluation reward: 176.80851063829786\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 106: Policy loss: 0.205914. Value loss: 29.033672. Entropy: 1.034728.\n",
      "Iteration 107: Policy loss: 0.074751. Value loss: 23.162294. Entropy: 1.018964.\n",
      "Iteration 108: Policy loss: 0.099986. Value loss: 22.053957. Entropy: 1.025709.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 109: Policy loss: -2.998904. Value loss: 393.473907. Entropy: 0.841749.\n",
      "Iteration 110: Policy loss: -2.416413. Value loss: 221.961411. Entropy: 0.866427.\n",
      "Iteration 111: Policy loss: -2.713289. Value loss: 214.177917. Entropy: 0.856973.\n",
      "episode: 48   score: 320.0  epsilon: 1.0    steps: 23  evaluation reward: 179.79166666666666\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 112: Policy loss: -0.589268. Value loss: 35.548668. Entropy: 0.849933.\n",
      "Iteration 113: Policy loss: -0.839454. Value loss: 20.541588. Entropy: 0.844227.\n",
      "Iteration 114: Policy loss: -0.713317. Value loss: 18.949987. Entropy: 0.842615.\n",
      "episode: 49   score: 210.0  epsilon: 1.0    steps: 428  evaluation reward: 180.40816326530611\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 115: Policy loss: -0.192718. Value loss: 43.773781. Entropy: 0.715907.\n",
      "Iteration 116: Policy loss: -0.341557. Value loss: 27.121759. Entropy: 0.755892.\n",
      "Iteration 117: Policy loss: -0.258295. Value loss: 25.278358. Entropy: 0.715730.\n",
      "episode: 50   score: 160.0  epsilon: 1.0    steps: 840  evaluation reward: 180.0\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 118: Policy loss: -0.725676. Value loss: 49.636509. Entropy: 0.731943.\n",
      "Iteration 119: Policy loss: -0.979901. Value loss: 33.708809. Entropy: 0.648739.\n",
      "Iteration 120: Policy loss: -0.943321. Value loss: 31.235146. Entropy: 0.724100.\n",
      "now time :  2019-02-26 12:29:22.498620\n",
      "episode: 51   score: 265.0  epsilon: 1.0    steps: 214  evaluation reward: 181.66666666666666\n",
      "episode: 52   score: 210.0  epsilon: 1.0    steps: 959  evaluation reward: 182.21153846153845\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 121: Policy loss: -0.783783. Value loss: 28.336124. Entropy: 0.679093.\n",
      "Iteration 122: Policy loss: -0.684451. Value loss: 22.671679. Entropy: 0.666608.\n",
      "Iteration 123: Policy loss: -0.879553. Value loss: 21.606785. Entropy: 0.669764.\n",
      "episode: 53   score: 260.0  epsilon: 1.0    steps: 329  evaluation reward: 183.67924528301887\n",
      "episode: 54   score: 210.0  epsilon: 1.0    steps: 626  evaluation reward: 184.16666666666666\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 124: Policy loss: 1.190691. Value loss: 42.720444. Entropy: 0.572464.\n",
      "Iteration 125: Policy loss: 1.134163. Value loss: 34.391842. Entropy: 0.583161.\n",
      "Iteration 126: Policy loss: 0.994772. Value loss: 29.526114. Entropy: 0.522569.\n",
      "episode: 55   score: 210.0  epsilon: 1.0    steps: 701  evaluation reward: 184.63636363636363\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 127: Policy loss: -0.070562. Value loss: 24.890280. Entropy: 0.481871.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 128: Policy loss: -0.100772. Value loss: 20.411129. Entropy: 0.454290.\n",
      "Iteration 129: Policy loss: 0.035658. Value loss: 20.497225. Entropy: 0.464881.\n",
      "episode: 56   score: 185.0  epsilon: 1.0    steps: 55  evaluation reward: 184.64285714285714\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 130: Policy loss: 1.379640. Value loss: 28.818233. Entropy: 0.481112.\n",
      "Iteration 131: Policy loss: 1.105922. Value loss: 21.068638. Entropy: 0.445000.\n",
      "Iteration 132: Policy loss: 1.059253. Value loss: 19.317085. Entropy: 0.456872.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 133: Policy loss: -0.343825. Value loss: 17.710852. Entropy: 0.468456.\n",
      "Iteration 134: Policy loss: -0.481948. Value loss: 11.700258. Entropy: 0.470345.\n",
      "Iteration 135: Policy loss: -0.431312. Value loss: 10.790524. Entropy: 0.458852.\n",
      "episode: 57   score: 180.0  epsilon: 1.0    steps: 891  evaluation reward: 184.56140350877192\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 136: Policy loss: -3.618014. Value loss: 320.350830. Entropy: 0.329951.\n",
      "Iteration 137: Policy loss: -3.219073. Value loss: 215.257355. Entropy: 0.312357.\n",
      "Iteration 138: Policy loss: -3.985089. Value loss: 246.532364. Entropy: 0.342578.\n",
      "episode: 58   score: 180.0  epsilon: 1.0    steps: 218  evaluation reward: 184.48275862068965\n",
      "episode: 59   score: 355.0  epsilon: 1.0    steps: 1008  evaluation reward: 187.3728813559322\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 139: Policy loss: 0.145520. Value loss: 42.674953. Entropy: 0.294736.\n",
      "Iteration 140: Policy loss: 0.099710. Value loss: 25.497591. Entropy: 0.386267.\n",
      "Iteration 141: Policy loss: 0.017253. Value loss: 19.839537. Entropy: 0.394643.\n",
      "episode: 60   score: 180.0  epsilon: 1.0    steps: 374  evaluation reward: 187.25\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 142: Policy loss: 2.357973. Value loss: 36.270199. Entropy: 0.245997.\n",
      "Iteration 143: Policy loss: 1.990758. Value loss: 28.517431. Entropy: 0.241919.\n",
      "Iteration 144: Policy loss: 2.229830. Value loss: 22.396915. Entropy: 0.374803.\n",
      "episode: 61   score: 210.0  epsilon: 1.0    steps: 551  evaluation reward: 187.62295081967213\n",
      "episode: 62   score: 210.0  epsilon: 1.0    steps: 764  evaluation reward: 187.98387096774192\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 145: Policy loss: 1.626797. Value loss: 33.352184. Entropy: 0.443273.\n",
      "Iteration 146: Policy loss: 1.593100. Value loss: 30.430191. Entropy: 0.474336.\n",
      "Iteration 147: Policy loss: 1.595743. Value loss: 25.202477. Entropy: 0.531814.\n",
      "episode: 63   score: 210.0  epsilon: 1.0    steps: 104  evaluation reward: 188.33333333333334\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 148: Policy loss: 2.154563. Value loss: 24.290113. Entropy: 0.664715.\n",
      "Iteration 149: Policy loss: 2.272409. Value loss: 19.681879. Entropy: 0.674177.\n",
      "Iteration 150: Policy loss: 2.215515. Value loss: 18.005724. Entropy: 0.803536.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 151: Policy loss: 1.447244. Value loss: 11.755996. Entropy: 0.855399.\n",
      "Iteration 152: Policy loss: 1.443861. Value loss: 7.580010. Entropy: 0.859754.\n",
      "Iteration 153: Policy loss: 1.469090. Value loss: 6.558481. Entropy: 0.868095.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 154: Policy loss: 0.270445. Value loss: 25.147139. Entropy: 0.952670.\n",
      "Iteration 155: Policy loss: 0.448771. Value loss: 15.674295. Entropy: 0.910648.\n",
      "Iteration 156: Policy loss: 0.572291. Value loss: 12.248252. Entropy: 0.969083.\n",
      "episode: 64   score: 225.0  epsilon: 1.0    steps: 461  evaluation reward: 188.90625\n",
      "episode: 65   score: 320.0  epsilon: 1.0    steps: 1017  evaluation reward: 190.92307692307693\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 157: Policy loss: -0.931496. Value loss: 301.116272. Entropy: 0.991779.\n",
      "Iteration 158: Policy loss: -0.172791. Value loss: 230.393860. Entropy: 0.912993.\n",
      "Iteration 159: Policy loss: -0.527782. Value loss: 219.178314. Entropy: 1.005373.\n",
      "episode: 66   score: 180.0  epsilon: 1.0    steps: 197  evaluation reward: 190.75757575757575\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 160: Policy loss: -2.479367. Value loss: 27.933897. Entropy: 0.842057.\n",
      "Iteration 161: Policy loss: -2.641930. Value loss: 20.509546. Entropy: 0.925044.\n",
      "Iteration 162: Policy loss: -2.747720. Value loss: 18.155998. Entropy: 0.996404.\n",
      "episode: 67   score: 185.0  epsilon: 1.0    steps: 263  evaluation reward: 190.67164179104478\n",
      "episode: 68   score: 185.0  epsilon: 1.0    steps: 608  evaluation reward: 190.58823529411765\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 163: Policy loss: 0.679544. Value loss: 22.610910. Entropy: 0.934826.\n",
      "Iteration 164: Policy loss: 0.590429. Value loss: 17.136457. Entropy: 0.937596.\n",
      "Iteration 165: Policy loss: 0.636804. Value loss: 16.013771. Entropy: 0.907141.\n",
      "episode: 69   score: 180.0  epsilon: 1.0    steps: 675  evaluation reward: 190.43478260869566\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 166: Policy loss: 1.329594. Value loss: 18.133722. Entropy: 0.950640.\n",
      "Iteration 167: Policy loss: 1.400327. Value loss: 16.344511. Entropy: 0.956726.\n",
      "Iteration 168: Policy loss: 1.360535. Value loss: 13.918682. Entropy: 0.951613.\n",
      "episode: 70   score: 165.0  epsilon: 1.0    steps: 17  evaluation reward: 190.07142857142858\n",
      "episode: 71   score: 260.0  epsilon: 1.0    steps: 828  evaluation reward: 191.05633802816902\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 169: Policy loss: 0.501316. Value loss: 11.288168. Entropy: 0.893984.\n",
      "Iteration 170: Policy loss: 0.633005. Value loss: 10.384886. Entropy: 0.883748.\n",
      "Iteration 171: Policy loss: 0.635665. Value loss: 8.363164. Entropy: 0.918762.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 172: Policy loss: -0.424298. Value loss: 18.616129. Entropy: 0.820133.\n",
      "Iteration 173: Policy loss: -0.350768. Value loss: 14.065960. Entropy: 0.780128.\n",
      "Iteration 174: Policy loss: -0.348725. Value loss: 13.223107. Entropy: 0.781533.\n",
      "episode: 72   score: 180.0  epsilon: 1.0    steps: 506  evaluation reward: 190.90277777777777\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 175: Policy loss: 0.932194. Value loss: 20.242678. Entropy: 0.837604.\n",
      "Iteration 176: Policy loss: 0.925956. Value loss: 15.756317. Entropy: 0.841528.\n",
      "Iteration 177: Policy loss: 0.817190. Value loss: 12.159403. Entropy: 0.857557.\n",
      "episode: 73   score: 155.0  epsilon: 1.0    steps: 216  evaluation reward: 190.41095890410958\n",
      "episode: 74   score: 105.0  epsilon: 1.0    steps: 611  evaluation reward: 189.25675675675674\n",
      "episode: 75   score: 135.0  epsilon: 1.0    steps: 908  evaluation reward: 188.53333333333333\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 178: Policy loss: -2.225033. Value loss: 316.054474. Entropy: 0.538416.\n",
      "Iteration 179: Policy loss: -2.146856. Value loss: 217.171692. Entropy: 0.574398.\n",
      "Iteration 180: Policy loss: -2.149964. Value loss: 155.415482. Entropy: 0.503850.\n",
      "episode: 76   score: 210.0  epsilon: 1.0    steps: 292  evaluation reward: 188.81578947368422\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 181: Policy loss: 0.375372. Value loss: 22.471678. Entropy: 0.568849.\n",
      "Iteration 182: Policy loss: 0.399876. Value loss: 16.247091. Entropy: 0.570558.\n",
      "Iteration 183: Policy loss: 0.328544. Value loss: 14.932484. Entropy: 0.590902.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 184: Policy loss: -1.634250. Value loss: 33.229160. Entropy: 1.082295.\n",
      "Iteration 185: Policy loss: -1.415920. Value loss: 30.076050. Entropy: 1.131864.\n",
      "Iteration 186: Policy loss: -1.444752. Value loss: 31.744432. Entropy: 1.088056.\n",
      "episode: 77   score: 240.0  epsilon: 1.0    steps: 732  evaluation reward: 189.4805194805195\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 187: Policy loss: -0.587781. Value loss: 34.832756. Entropy: 0.917631.\n",
      "Iteration 188: Policy loss: -0.624449. Value loss: 24.569902. Entropy: 0.967251.\n",
      "Iteration 189: Policy loss: -0.499378. Value loss: 21.082722. Entropy: 0.976002.\n",
      "episode: 78   score: 260.0  epsilon: 1.0    steps: 89  evaluation reward: 190.3846153846154\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 190: Policy loss: 1.225950. Value loss: 10.900727. Entropy: 0.805817.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 191: Policy loss: 1.321886. Value loss: 8.135585. Entropy: 0.691685.\n",
      "Iteration 192: Policy loss: 1.208769. Value loss: 8.007512. Entropy: 0.714133.\n",
      "episode: 79   score: 520.0  epsilon: 1.0    steps: 889  evaluation reward: 194.55696202531647\n",
      "episode: 80   score: 105.0  epsilon: 1.0    steps: 915  evaluation reward: 193.4375\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 193: Policy loss: -0.114598. Value loss: 22.619146. Entropy: 0.620992.\n",
      "Iteration 194: Policy loss: 0.149672. Value loss: 16.985306. Entropy: 0.664565.\n",
      "Iteration 195: Policy loss: 0.031528. Value loss: 15.849419. Entropy: 0.657354.\n",
      "episode: 81   score: 210.0  epsilon: 1.0    steps: 493  evaluation reward: 193.64197530864197\n",
      "episode: 82   score: 160.0  epsilon: 1.0    steps: 637  evaluation reward: 193.23170731707316\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 196: Policy loss: 1.487036. Value loss: 40.822403. Entropy: 0.649981.\n",
      "Iteration 197: Policy loss: 1.482673. Value loss: 29.011583. Entropy: 0.621810.\n",
      "Iteration 198: Policy loss: 1.400162. Value loss: 26.823992. Entropy: 0.599164.\n",
      "episode: 83   score: 210.0  epsilon: 1.0    steps: 145  evaluation reward: 193.43373493975903\n",
      "episode: 84   score: 210.0  epsilon: 1.0    steps: 369  evaluation reward: 193.63095238095238\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 199: Policy loss: 1.241250. Value loss: 14.048252. Entropy: 0.588239.\n",
      "Iteration 200: Policy loss: 1.414945. Value loss: 11.009000. Entropy: 0.546701.\n",
      "Iteration 201: Policy loss: 1.129781. Value loss: 11.372838. Entropy: 0.518103.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 202: Policy loss: 0.963024. Value loss: 16.230181. Entropy: 0.407278.\n",
      "Iteration 203: Policy loss: 1.002971. Value loss: 12.726169. Entropy: 0.349926.\n",
      "Iteration 204: Policy loss: 1.104269. Value loss: 10.738721. Entropy: 0.382276.\n",
      "episode: 85   score: 180.0  epsilon: 1.0    steps: 767  evaluation reward: 193.47058823529412\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 205: Policy loss: -0.516298. Value loss: 182.744659. Entropy: 0.413385.\n",
      "Iteration 206: Policy loss: -1.286522. Value loss: 114.173294. Entropy: 0.392287.\n",
      "Iteration 207: Policy loss: -1.236743. Value loss: 110.877556. Entropy: 0.390934.\n",
      "episode: 86   score: 210.0  epsilon: 1.0    steps: 128  evaluation reward: 193.6627906976744\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 208: Policy loss: 0.511236. Value loss: 20.608194. Entropy: 0.401128.\n",
      "Iteration 209: Policy loss: 0.521610. Value loss: 13.038816. Entropy: 0.461466.\n",
      "Iteration 210: Policy loss: 0.608545. Value loss: 12.344203. Entropy: 0.428291.\n",
      "episode: 87   score: 180.0  epsilon: 1.0    steps: 1000  evaluation reward: 193.50574712643677\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 211: Policy loss: -0.717364. Value loss: 19.013380. Entropy: 0.439955.\n",
      "Iteration 212: Policy loss: -0.719300. Value loss: 14.237238. Entropy: 0.332541.\n",
      "Iteration 213: Policy loss: -0.741675. Value loss: 12.768672. Entropy: 0.398169.\n",
      "episode: 88   score: 155.0  epsilon: 1.0    steps: 796  evaluation reward: 193.0681818181818\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 214: Policy loss: -0.800548. Value loss: 28.771410. Entropy: 0.494773.\n",
      "Iteration 215: Policy loss: -0.583604. Value loss: 23.054461. Entropy: 0.436432.\n",
      "Iteration 216: Policy loss: -0.844453. Value loss: 21.808722. Entropy: 0.465637.\n",
      "episode: 89   score: 210.0  epsilon: 1.0    steps: 155  evaluation reward: 193.25842696629215\n",
      "episode: 90   score: 180.0  epsilon: 1.0    steps: 538  evaluation reward: 193.11111111111111\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 217: Policy loss: -0.462578. Value loss: 16.525665. Entropy: 0.514722.\n",
      "Iteration 218: Policy loss: -0.299879. Value loss: 12.392811. Entropy: 0.511243.\n",
      "Iteration 219: Policy loss: -0.262674. Value loss: 12.296957. Entropy: 0.537680.\n",
      "episode: 91   score: 210.0  epsilon: 1.0    steps: 304  evaluation reward: 193.2967032967033\n",
      "episode: 92   score: 485.0  epsilon: 1.0    steps: 429  evaluation reward: 196.4673913043478\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 220: Policy loss: -0.518643. Value loss: 17.233326. Entropy: 0.717391.\n",
      "Iteration 221: Policy loss: -0.323535. Value loss: 15.444268. Entropy: 0.725032.\n",
      "Iteration 222: Policy loss: -0.455974. Value loss: 12.367105. Entropy: 0.710712.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 223: Policy loss: -0.286693. Value loss: 19.887054. Entropy: 0.657072.\n",
      "Iteration 224: Policy loss: -0.209717. Value loss: 17.314730. Entropy: 0.670244.\n",
      "Iteration 225: Policy loss: -0.418268. Value loss: 14.076417. Entropy: 0.690485.\n",
      "episode: 93   score: 105.0  epsilon: 1.0    steps: 3  evaluation reward: 195.48387096774192\n",
      "episode: 94   score: 180.0  epsilon: 1.0    steps: 757  evaluation reward: 195.31914893617022\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 226: Policy loss: -1.441194. Value loss: 22.798420. Entropy: 0.876865.\n",
      "Iteration 227: Policy loss: -1.450867. Value loss: 17.068172. Entropy: 0.898249.\n",
      "Iteration 228: Policy loss: -1.344308. Value loss: 14.979346. Entropy: 0.876295.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 229: Policy loss: -1.518638. Value loss: 331.644501. Entropy: 0.512419.\n",
      "Iteration 230: Policy loss: -1.982609. Value loss: 290.038971. Entropy: 0.603633.\n",
      "Iteration 231: Policy loss: -0.180054. Value loss: 173.847305. Entropy: 0.509098.\n",
      "episode: 95   score: 180.0  epsilon: 1.0    steps: 570  evaluation reward: 195.1578947368421\n",
      "episode: 96   score: 155.0  epsilon: 1.0    steps: 900  evaluation reward: 194.73958333333334\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 232: Policy loss: 0.191536. Value loss: 39.173820. Entropy: 0.759048.\n",
      "Iteration 233: Policy loss: 0.134153. Value loss: 29.132296. Entropy: 0.735649.\n",
      "Iteration 234: Policy loss: -0.132721. Value loss: 30.331991. Entropy: 0.751223.\n",
      "episode: 97   score: 210.0  epsilon: 1.0    steps: 233  evaluation reward: 194.89690721649484\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 235: Policy loss: -0.148348. Value loss: 43.532589. Entropy: 0.981741.\n",
      "Iteration 236: Policy loss: -0.446461. Value loss: 37.732170. Entropy: 1.023650.\n",
      "Iteration 237: Policy loss: -0.110456. Value loss: 30.042582. Entropy: 1.055843.\n",
      "episode: 98   score: 410.0  epsilon: 1.0    steps: 351  evaluation reward: 197.09183673469389\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 238: Policy loss: 0.751664. Value loss: 39.502495. Entropy: 1.108672.\n",
      "Iteration 239: Policy loss: 0.711158. Value loss: 35.426823. Entropy: 1.118557.\n",
      "Iteration 240: Policy loss: 0.686351. Value loss: 32.095245. Entropy: 1.150518.\n",
      "episode: 99   score: 110.0  epsilon: 1.0    steps: 47  evaluation reward: 196.21212121212122\n",
      "episode: 100   score: 325.0  epsilon: 1.0    steps: 454  evaluation reward: 197.5\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 241: Policy loss: 1.691670. Value loss: 43.305561. Entropy: 0.846183.\n",
      "Iteration 242: Policy loss: 1.891652. Value loss: 32.331463. Entropy: 0.873203.\n",
      "Iteration 243: Policy loss: 1.749144. Value loss: 28.371336. Entropy: 0.868640.\n",
      "now time :  2019-02-26 12:31:41.365462\n",
      "episode: 101   score: 120.0  epsilon: 1.0    steps: 750  evaluation reward: 197.65\n",
      "episode: 102   score: 345.0  epsilon: 1.0    steps: 781  evaluation reward: 200.75\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 244: Policy loss: 2.778047. Value loss: 29.896570. Entropy: 0.722279.\n",
      "Iteration 245: Policy loss: 3.183760. Value loss: 24.144684. Entropy: 0.713161.\n",
      "Iteration 246: Policy loss: 2.554737. Value loss: 19.768406. Entropy: 0.653931.\n",
      "episode: 103   score: 155.0  epsilon: 1.0    steps: 1005  evaluation reward: 201.65\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 247: Policy loss: 0.512112. Value loss: 43.884251. Entropy: 0.649176.\n",
      "Iteration 248: Policy loss: 0.322427. Value loss: 43.318390. Entropy: 0.602999.\n",
      "Iteration 249: Policy loss: 0.280648. Value loss: 32.990528. Entropy: 0.625390.\n",
      "episode: 104   score: 180.0  epsilon: 1.0    steps: 548  evaluation reward: 202.65\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 250: Policy loss: 1.854225. Value loss: 27.539717. Entropy: 0.972592.\n",
      "Iteration 251: Policy loss: 1.913566. Value loss: 21.673527. Entropy: 0.947704.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 252: Policy loss: 1.911385. Value loss: 17.369148. Entropy: 0.971937.\n",
      "episode: 105   score: 75.0  epsilon: 1.0    steps: 166  evaluation reward: 202.15\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 253: Policy loss: 2.334609. Value loss: 29.768791. Entropy: 0.871741.\n",
      "Iteration 254: Policy loss: 2.279338. Value loss: 18.676300. Entropy: 0.856928.\n",
      "Iteration 255: Policy loss: 2.320926. Value loss: 17.512388. Entropy: 0.871839.\n",
      "episode: 106   score: 155.0  epsilon: 1.0    steps: 82  evaluation reward: 202.5\n",
      "episode: 107   score: 110.0  epsilon: 1.0    steps: 433  evaluation reward: 202.2\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 256: Policy loss: -2.251868. Value loss: 48.598141. Entropy: 0.901539.\n",
      "Iteration 257: Policy loss: -2.409133. Value loss: 34.160103. Entropy: 0.918745.\n",
      "Iteration 258: Policy loss: -1.999194. Value loss: 28.673079. Entropy: 0.913760.\n",
      "episode: 108   score: 250.0  epsilon: 1.0    steps: 307  evaluation reward: 203.5\n",
      "episode: 109   score: 110.0  epsilon: 1.0    steps: 731  evaluation reward: 203.4\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 259: Policy loss: 1.140308. Value loss: 42.982548. Entropy: 0.753935.\n",
      "Iteration 260: Policy loss: 0.778271. Value loss: 36.988251. Entropy: 0.767559.\n",
      "Iteration 261: Policy loss: 1.037920. Value loss: 29.422400. Entropy: 0.772717.\n",
      "episode: 110   score: 105.0  epsilon: 1.0    steps: 956  evaluation reward: 202.75\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 262: Policy loss: 1.356032. Value loss: 31.986803. Entropy: 0.931611.\n",
      "Iteration 263: Policy loss: 1.244748. Value loss: 18.229757. Entropy: 0.934248.\n",
      "Iteration 264: Policy loss: 1.284055. Value loss: 17.162569. Entropy: 0.938833.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 265: Policy loss: 0.531571. Value loss: 23.268671. Entropy: 1.129462.\n",
      "Iteration 266: Policy loss: 0.751497. Value loss: 16.945538. Entropy: 1.132107.\n",
      "Iteration 267: Policy loss: 0.512013. Value loss: 16.114401. Entropy: 1.136730.\n",
      "episode: 111   score: 90.0  epsilon: 1.0    steps: 17  evaluation reward: 202.75\n",
      "episode: 112   score: 120.0  epsilon: 1.0    steps: 173  evaluation reward: 202.85\n",
      "episode: 113   score: 315.0  epsilon: 1.0    steps: 774  evaluation reward: 204.8\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 268: Policy loss: 1.836273. Value loss: 24.260744. Entropy: 1.042279.\n",
      "Iteration 269: Policy loss: 1.632980. Value loss: 15.512823. Entropy: 1.057447.\n",
      "Iteration 270: Policy loss: 1.763671. Value loss: 13.241982. Entropy: 1.046237.\n",
      "episode: 114   score: 105.0  epsilon: 1.0    steps: 730  evaluation reward: 202.0\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 271: Policy loss: -2.048398. Value loss: 40.709919. Entropy: 0.884999.\n",
      "Iteration 272: Policy loss: -1.931932. Value loss: 28.560938. Entropy: 0.899093.\n",
      "Iteration 273: Policy loss: -1.947426. Value loss: 25.862751. Entropy: 0.938077.\n",
      "episode: 115   score: 105.0  epsilon: 1.0    steps: 323  evaluation reward: 201.25\n",
      "episode: 116   score: 155.0  epsilon: 1.0    steps: 432  evaluation reward: 199.95\n",
      "episode: 117   score: 260.0  epsilon: 1.0    steps: 591  evaluation reward: 201.7\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 274: Policy loss: 1.441839. Value loss: 40.798519. Entropy: 1.070357.\n",
      "Iteration 275: Policy loss: 1.296418. Value loss: 32.759296. Entropy: 1.092579.\n",
      "Iteration 276: Policy loss: 1.387477. Value loss: 24.568554. Entropy: 1.107514.\n",
      "episode: 118   score: 75.0  epsilon: 1.0    steps: 909  evaluation reward: 200.8\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 277: Policy loss: -0.062521. Value loss: 15.595907. Entropy: 1.023885.\n",
      "Iteration 278: Policy loss: -0.190561. Value loss: 11.401366. Entropy: 0.998491.\n",
      "Iteration 279: Policy loss: -0.137058. Value loss: 8.493452. Entropy: 1.038555.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 280: Policy loss: 0.561894. Value loss: 27.160404. Entropy: 0.869790.\n",
      "Iteration 281: Policy loss: 0.352422. Value loss: 17.518229. Entropy: 0.858061.\n",
      "Iteration 282: Policy loss: 0.533987. Value loss: 13.145481. Entropy: 0.831991.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 283: Policy loss: 1.142020. Value loss: 21.472498. Entropy: 0.926104.\n",
      "Iteration 284: Policy loss: 1.012228. Value loss: 14.279155. Entropy: 0.898353.\n",
      "Iteration 285: Policy loss: 1.122083. Value loss: 11.618860. Entropy: 0.953011.\n",
      "episode: 119   score: 180.0  epsilon: 1.0    steps: 62  evaluation reward: 201.55\n",
      "episode: 120   score: 210.0  epsilon: 1.0    steps: 256  evaluation reward: 201.1\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 286: Policy loss: 0.891838. Value loss: 33.573696. Entropy: 0.562394.\n",
      "Iteration 287: Policy loss: 0.657146. Value loss: 28.293238. Entropy: 0.639617.\n",
      "Iteration 288: Policy loss: 0.721022. Value loss: 21.127733. Entropy: 0.618707.\n",
      "episode: 121   score: 75.0  epsilon: 1.0    steps: 411  evaluation reward: 200.25\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 289: Policy loss: -0.322542. Value loss: 25.752089. Entropy: 0.684530.\n",
      "Iteration 290: Policy loss: -0.392414. Value loss: 16.113052. Entropy: 0.697337.\n",
      "Iteration 291: Policy loss: -0.525123. Value loss: 12.457918. Entropy: 0.700277.\n",
      "episode: 122   score: 210.0  epsilon: 1.0    steps: 374  evaluation reward: 200.45\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 292: Policy loss: -0.295893. Value loss: 28.356339. Entropy: 0.596410.\n",
      "Iteration 293: Policy loss: -0.144636. Value loss: 21.615923. Entropy: 0.584089.\n",
      "Iteration 294: Policy loss: -0.190377. Value loss: 18.974777. Entropy: 0.616528.\n",
      "episode: 123   score: 210.0  epsilon: 1.0    steps: 514  evaluation reward: 196.85\n",
      "episode: 124   score: 210.0  epsilon: 1.0    steps: 960  evaluation reward: 194.7\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 295: Policy loss: 0.715590. Value loss: 18.044083. Entropy: 1.007230.\n",
      "Iteration 296: Policy loss: 0.565076. Value loss: 11.375173. Entropy: 0.972172.\n",
      "Iteration 297: Policy loss: 0.650163. Value loss: 10.089333. Entropy: 0.963052.\n",
      "episode: 125   score: 305.0  epsilon: 1.0    steps: 667  evaluation reward: 196.7\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 298: Policy loss: -1.442117. Value loss: 33.429588. Entropy: 0.832072.\n",
      "Iteration 299: Policy loss: -1.467232. Value loss: 22.007538. Entropy: 0.800434.\n",
      "Iteration 300: Policy loss: -1.421060. Value loss: 19.357979. Entropy: 0.830867.\n",
      "episode: 126   score: 365.0  epsilon: 1.0    steps: 857  evaluation reward: 199.3\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 301: Policy loss: 0.151109. Value loss: 17.705978. Entropy: 0.920232.\n",
      "Iteration 302: Policy loss: 0.174666. Value loss: 10.768950. Entropy: 0.892100.\n",
      "Iteration 303: Policy loss: 0.082646. Value loss: 10.246559. Entropy: 0.913355.\n",
      "episode: 127   score: 105.0  epsilon: 1.0    steps: 414  evaluation reward: 198.55\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 304: Policy loss: 1.073804. Value loss: 14.599109. Entropy: 0.973698.\n",
      "Iteration 305: Policy loss: 1.209887. Value loss: 11.805366. Entropy: 0.998926.\n",
      "Iteration 306: Policy loss: 0.955659. Value loss: 8.913961. Entropy: 1.012852.\n",
      "episode: 128   score: 185.0  epsilon: 1.0    steps: 161  evaluation reward: 198.85\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 307: Policy loss: 0.145521. Value loss: 16.765110. Entropy: 0.856713.\n",
      "Iteration 308: Policy loss: 0.238170. Value loss: 11.422845. Entropy: 0.875297.\n",
      "Iteration 309: Policy loss: 0.032272. Value loss: 11.278799. Entropy: 0.872230.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 310: Policy loss: -0.553474. Value loss: 30.475782. Entropy: 0.872532.\n",
      "Iteration 311: Policy loss: -0.689031. Value loss: 21.815376. Entropy: 0.863642.\n",
      "Iteration 312: Policy loss: -0.453635. Value loss: 20.490685. Entropy: 0.850685.\n",
      "episode: 129   score: 210.0  epsilon: 1.0    steps: 342  evaluation reward: 199.15\n",
      "episode: 130   score: 210.0  epsilon: 1.0    steps: 610  evaluation reward: 199.6\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 313: Policy loss: -1.904577. Value loss: 304.933777. Entropy: 0.966157.\n",
      "Iteration 314: Policy loss: -1.814002. Value loss: 194.082916. Entropy: 0.841167.\n",
      "Iteration 315: Policy loss: -0.391544. Value loss: 125.917145. Entropy: 0.934354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 131   score: 80.0  epsilon: 1.0    steps: 148  evaluation reward: 199.3\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 316: Policy loss: 1.190094. Value loss: 16.201731. Entropy: 0.603407.\n",
      "Iteration 317: Policy loss: 1.182451. Value loss: 7.632030. Entropy: 0.693849.\n",
      "Iteration 318: Policy loss: 1.187920. Value loss: 6.448758. Entropy: 0.669992.\n",
      "episode: 132   score: 545.0  epsilon: 1.0    steps: 65  evaluation reward: 202.65\n",
      "episode: 133   score: 260.0  epsilon: 1.0    steps: 936  evaluation reward: 200.65\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 319: Policy loss: 0.369271. Value loss: 27.780775. Entropy: 0.641300.\n",
      "Iteration 320: Policy loss: 0.592883. Value loss: 24.362944. Entropy: 0.633898.\n",
      "Iteration 321: Policy loss: 0.504433. Value loss: 20.884628. Entropy: 0.643890.\n",
      "episode: 134   score: 210.0  epsilon: 1.0    steps: 497  evaluation reward: 201.7\n",
      "episode: 135   score: 280.0  epsilon: 1.0    steps: 674  evaluation reward: 203.45\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 322: Policy loss: 1.764314. Value loss: 20.733400. Entropy: 0.734106.\n",
      "Iteration 323: Policy loss: 1.863502. Value loss: 16.659031. Entropy: 0.720047.\n",
      "Iteration 324: Policy loss: 1.839032. Value loss: 15.431688. Entropy: 0.736095.\n",
      "episode: 136   score: 105.0  epsilon: 1.0    steps: 352  evaluation reward: 202.95\n",
      "episode: 137   score: 260.0  epsilon: 1.0    steps: 803  evaluation reward: 204.8\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 325: Policy loss: 0.575666. Value loss: 14.286991. Entropy: 0.834992.\n",
      "Iteration 326: Policy loss: 0.619634. Value loss: 10.314367. Entropy: 0.827021.\n",
      "Iteration 327: Policy loss: 0.491464. Value loss: 10.424047. Entropy: 0.814254.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 328: Policy loss: 0.756348. Value loss: 15.959706. Entropy: 0.740959.\n",
      "Iteration 329: Policy loss: 0.586518. Value loss: 13.488502. Entropy: 0.722430.\n",
      "Iteration 330: Policy loss: 0.664419. Value loss: 12.550021. Entropy: 0.751638.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 331: Policy loss: -1.979220. Value loss: 30.410210. Entropy: 0.791962.\n",
      "Iteration 332: Policy loss: -1.960101. Value loss: 24.485323. Entropy: 0.731629.\n",
      "Iteration 333: Policy loss: -1.839512. Value loss: 20.571508. Entropy: 0.804556.\n",
      "episode: 138   score: 210.0  epsilon: 1.0    steps: 162  evaluation reward: 205.55\n",
      "episode: 139   score: 210.0  epsilon: 1.0    steps: 533  evaluation reward: 205.55\n",
      "episode: 140   score: 120.0  epsilon: 1.0    steps: 715  evaluation reward: 206.25\n",
      "episode: 141   score: 105.0  epsilon: 1.0    steps: 896  evaluation reward: 206.25\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 334: Policy loss: -4.991576. Value loss: 289.408783. Entropy: 0.757895.\n",
      "Iteration 335: Policy loss: -3.359176. Value loss: 158.351440. Entropy: 0.658837.\n",
      "Iteration 336: Policy loss: -4.411828. Value loss: 126.291603. Entropy: 0.704529.\n",
      "episode: 142   score: 105.0  epsilon: 1.0    steps: 295  evaluation reward: 204.6\n",
      "episode: 143   score: 155.0  epsilon: 1.0    steps: 422  evaluation reward: 201.0\n",
      "episode: 144   score: 210.0  epsilon: 1.0    steps: 979  evaluation reward: 201.3\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 337: Policy loss: 0.679106. Value loss: 34.382545. Entropy: 0.347077.\n",
      "Iteration 338: Policy loss: 0.426482. Value loss: 27.956697. Entropy: 0.326717.\n",
      "Iteration 339: Policy loss: 0.074914. Value loss: 27.228449. Entropy: 0.362857.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 340: Policy loss: 0.349514. Value loss: 49.616859. Entropy: 0.531834.\n",
      "Iteration 341: Policy loss: 0.454237. Value loss: 36.355961. Entropy: 0.531113.\n",
      "Iteration 342: Policy loss: 0.719711. Value loss: 28.085487. Entropy: 0.531761.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 343: Policy loss: 2.918087. Value loss: 35.274487. Entropy: 1.014934.\n",
      "Iteration 344: Policy loss: 2.133858. Value loss: 18.496050. Entropy: 0.947102.\n",
      "Iteration 345: Policy loss: 2.094893. Value loss: 16.765312. Entropy: 0.937494.\n",
      "episode: 145   score: 110.0  epsilon: 1.0    steps: 147  evaluation reward: 199.25\n",
      "episode: 146   score: 105.0  epsilon: 1.0    steps: 700  evaluation reward: 198.95\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 346: Policy loss: 1.534147. Value loss: 35.563957. Entropy: 0.859483.\n",
      "Iteration 347: Policy loss: 1.403649. Value loss: 21.453615. Entropy: 1.010029.\n",
      "Iteration 348: Policy loss: 1.609312. Value loss: 20.675331. Entropy: 0.991415.\n",
      "episode: 147   score: 460.0  epsilon: 1.0    steps: 112  evaluation reward: 202.75\n",
      "episode: 148   score: 105.0  epsilon: 1.0    steps: 458  evaluation reward: 200.6\n",
      "episode: 149   score: 155.0  epsilon: 1.0    steps: 851  evaluation reward: 200.05\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 349: Policy loss: -0.980007. Value loss: 58.763142. Entropy: 0.753952.\n",
      "Iteration 350: Policy loss: -0.806089. Value loss: 53.105259. Entropy: 0.766339.\n",
      "Iteration 351: Policy loss: -1.010059. Value loss: 42.009136. Entropy: 0.762843.\n",
      "episode: 150   score: 170.0  epsilon: 1.0    steps: 347  evaluation reward: 200.15\n",
      "now time :  2019-02-26 12:33:48.073325\n",
      "episode: 151   score: 235.0  epsilon: 1.0    steps: 560  evaluation reward: 199.85\n",
      "episode: 152   score: 180.0  epsilon: 1.0    steps: 906  evaluation reward: 199.55\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 352: Policy loss: -0.318393. Value loss: 51.265167. Entropy: 0.776168.\n",
      "Iteration 353: Policy loss: -0.718333. Value loss: 37.436508. Entropy: 0.754162.\n",
      "Iteration 354: Policy loss: -0.679515. Value loss: 30.351072. Entropy: 0.809237.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 355: Policy loss: 1.893415. Value loss: 64.099716. Entropy: 0.508376.\n",
      "Iteration 356: Policy loss: 2.081699. Value loss: 43.032444. Entropy: 0.561683.\n",
      "Iteration 357: Policy loss: 2.008868. Value loss: 46.606934. Entropy: 0.526929.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 358: Policy loss: 5.301409. Value loss: 88.195480. Entropy: 0.863924.\n",
      "Iteration 359: Policy loss: 5.088323. Value loss: 65.506882. Entropy: 0.924097.\n",
      "Iteration 360: Policy loss: 3.691736. Value loss: 59.926403. Entropy: 0.687967.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 361: Policy loss: 1.316237. Value loss: 38.901756. Entropy: 0.967802.\n",
      "Iteration 362: Policy loss: 1.877942. Value loss: 24.819044. Entropy: 0.905123.\n",
      "Iteration 363: Policy loss: 1.477710. Value loss: 23.874422. Entropy: 0.924195.\n",
      "episode: 153   score: 230.0  epsilon: 1.0    steps: 228  evaluation reward: 199.25\n",
      "episode: 154   score: 180.0  epsilon: 1.0    steps: 415  evaluation reward: 198.95\n",
      "episode: 155   score: 135.0  epsilon: 1.0    steps: 611  evaluation reward: 198.2\n",
      "episode: 156   score: 165.0  epsilon: 1.0    steps: 903  evaluation reward: 198.0\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 364: Policy loss: 0.938871. Value loss: 52.929840. Entropy: 0.970886.\n",
      "Iteration 365: Policy loss: 1.251040. Value loss: 35.090576. Entropy: 0.961684.\n",
      "Iteration 366: Policy loss: 1.149814. Value loss: 28.794868. Entropy: 1.059184.\n",
      "episode: 157   score: 135.0  epsilon: 1.0    steps: 277  evaluation reward: 197.55\n",
      "episode: 158   score: 215.0  epsilon: 1.0    steps: 751  evaluation reward: 197.9\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 367: Policy loss: 3.240774. Value loss: 39.891155. Entropy: 0.766442.\n",
      "Iteration 368: Policy loss: 3.549210. Value loss: 27.091345. Entropy: 0.826869.\n",
      "Iteration 369: Policy loss: 3.261055. Value loss: 26.850939. Entropy: 0.722507.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 370: Policy loss: 1.633482. Value loss: 45.472717. Entropy: 0.792872.\n",
      "Iteration 371: Policy loss: 1.859221. Value loss: 29.125708. Entropy: 0.734877.\n",
      "Iteration 372: Policy loss: 1.732535. Value loss: 25.775927. Entropy: 0.790301.\n",
      "episode: 159   score: 230.0  epsilon: 1.0    steps: 43  evaluation reward: 196.65\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 373: Policy loss: -0.657463. Value loss: 56.660789. Entropy: 1.054461.\n",
      "Iteration 374: Policy loss: -0.676587. Value loss: 43.776875. Entropy: 1.042213.\n",
      "Iteration 375: Policy loss: -0.783833. Value loss: 37.334282. Entropy: 1.065517.\n",
      "episode: 160   score: 105.0  epsilon: 1.0    steps: 225  evaluation reward: 195.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 161   score: 230.0  epsilon: 1.0    steps: 858  evaluation reward: 196.1\n",
      "episode: 162   score: 180.0  epsilon: 1.0    steps: 897  evaluation reward: 195.8\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 376: Policy loss: 1.100583. Value loss: 23.121008. Entropy: 0.935528.\n",
      "Iteration 377: Policy loss: 0.981534. Value loss: 18.602787. Entropy: 0.955684.\n",
      "Iteration 378: Policy loss: 1.151357. Value loss: 14.238213. Entropy: 0.968996.\n",
      "episode: 163   score: 110.0  epsilon: 1.0    steps: 572  evaluation reward: 194.8\n",
      "episode: 164   score: 55.0  epsilon: 1.0    steps: 732  evaluation reward: 193.1\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 379: Policy loss: 2.178570. Value loss: 46.976063. Entropy: 0.798756.\n",
      "Iteration 380: Policy loss: 2.289909. Value loss: 35.869354. Entropy: 0.790697.\n",
      "Iteration 381: Policy loss: 2.096302. Value loss: 27.059549. Entropy: 0.832144.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 382: Policy loss: 1.527689. Value loss: 23.611355. Entropy: 0.845332.\n",
      "Iteration 383: Policy loss: 1.398033. Value loss: 18.262196. Entropy: 0.832803.\n",
      "Iteration 384: Policy loss: 1.483319. Value loss: 18.855757. Entropy: 0.850071.\n",
      "episode: 165   score: 180.0  epsilon: 1.0    steps: 354  evaluation reward: 191.7\n",
      "episode: 166   score: 30.0  epsilon: 1.0    steps: 913  evaluation reward: 190.2\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 385: Policy loss: 1.454179. Value loss: 29.054193. Entropy: 1.012669.\n",
      "Iteration 386: Policy loss: 1.719039. Value loss: 25.888287. Entropy: 1.066656.\n",
      "Iteration 387: Policy loss: 1.383021. Value loss: 22.341154. Entropy: 1.036445.\n",
      "episode: 167   score: 210.0  epsilon: 1.0    steps: 443  evaluation reward: 190.45\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 388: Policy loss: -0.214738. Value loss: 22.402437. Entropy: 1.059089.\n",
      "Iteration 389: Policy loss: -0.269222. Value loss: 13.931301. Entropy: 0.976879.\n",
      "Iteration 390: Policy loss: -0.235860. Value loss: 11.519420. Entropy: 0.975032.\n",
      "episode: 168   score: 155.0  epsilon: 1.0    steps: 74  evaluation reward: 190.15\n",
      "episode: 169   score: 105.0  epsilon: 1.0    steps: 184  evaluation reward: 189.4\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 391: Policy loss: 2.348395. Value loss: 29.644850. Entropy: 0.900176.\n",
      "Iteration 392: Policy loss: 2.226294. Value loss: 21.206774. Entropy: 0.869357.\n",
      "Iteration 393: Policy loss: 2.328640. Value loss: 17.358904. Entropy: 0.852632.\n",
      "episode: 170   score: 105.0  epsilon: 1.0    steps: 580  evaluation reward: 188.8\n",
      "episode: 171   score: 110.0  epsilon: 1.0    steps: 751  evaluation reward: 187.3\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 394: Policy loss: 1.328731. Value loss: 23.257664. Entropy: 0.869963.\n",
      "Iteration 395: Policy loss: 1.501299. Value loss: 15.357129. Entropy: 0.865593.\n",
      "Iteration 396: Policy loss: 1.291510. Value loss: 13.136212. Entropy: 0.868233.\n",
      "episode: 172   score: 65.0  epsilon: 1.0    steps: 1012  evaluation reward: 186.15\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 397: Policy loss: -0.032595. Value loss: 31.931124. Entropy: 0.833683.\n",
      "Iteration 398: Policy loss: -0.043119. Value loss: 23.971365. Entropy: 0.906159.\n",
      "Iteration 399: Policy loss: -0.150037. Value loss: 21.568680. Entropy: 0.917346.\n",
      "episode: 173   score: 260.0  epsilon: 1.0    steps: 823  evaluation reward: 187.2\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 400: Policy loss: 0.497523. Value loss: 14.926414. Entropy: 1.066699.\n",
      "Iteration 401: Policy loss: 0.164628. Value loss: 11.010792. Entropy: 1.116468.\n",
      "Iteration 402: Policy loss: 0.197678. Value loss: 8.809989. Entropy: 1.077519.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 403: Policy loss: -3.283744. Value loss: 310.442932. Entropy: 0.884081.\n",
      "Iteration 404: Policy loss: -2.335643. Value loss: 149.416443. Entropy: 0.862842.\n",
      "Iteration 405: Policy loss: -2.474352. Value loss: 134.840652. Entropy: 0.878528.\n",
      "episode: 174   score: 385.0  epsilon: 1.0    steps: 303  evaluation reward: 190.0\n",
      "episode: 175   score: 210.0  epsilon: 1.0    steps: 486  evaluation reward: 190.75\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 406: Policy loss: -0.463617. Value loss: 42.472641. Entropy: 0.845299.\n",
      "Iteration 407: Policy loss: -0.637449. Value loss: 28.807671. Entropy: 0.853056.\n",
      "Iteration 408: Policy loss: -0.637386. Value loss: 25.505386. Entropy: 0.878897.\n",
      "episode: 176   score: 210.0  epsilon: 1.0    steps: 234  evaluation reward: 190.75\n",
      "episode: 177   score: 110.0  epsilon: 1.0    steps: 582  evaluation reward: 189.45\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 409: Policy loss: -0.578516. Value loss: 31.706823. Entropy: 0.935034.\n",
      "Iteration 410: Policy loss: -0.391495. Value loss: 25.350288. Entropy: 0.943373.\n",
      "Iteration 411: Policy loss: -0.599524. Value loss: 23.742567. Entropy: 0.946553.\n",
      "episode: 178   score: 225.0  epsilon: 1.0    steps: 65  evaluation reward: 189.1\n",
      "episode: 179   score: 105.0  epsilon: 1.0    steps: 830  evaluation reward: 184.95\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 412: Policy loss: 0.450420. Value loss: 29.737545. Entropy: 0.908944.\n",
      "Iteration 413: Policy loss: 0.940325. Value loss: 23.637846. Entropy: 0.861302.\n",
      "Iteration 414: Policy loss: 0.460473. Value loss: 23.053679. Entropy: 0.883387.\n",
      "episode: 180   score: 210.0  epsilon: 1.0    steps: 650  evaluation reward: 186.0\n",
      "episode: 181   score: 135.0  epsilon: 1.0    steps: 971  evaluation reward: 185.25\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 415: Policy loss: 0.232440. Value loss: 29.943558. Entropy: 0.754265.\n",
      "Iteration 416: Policy loss: 0.306601. Value loss: 28.747898. Entropy: 0.788802.\n",
      "Iteration 417: Policy loss: 0.305581. Value loss: 21.647030. Entropy: 0.760696.\n",
      "episode: 182   score: 75.0  epsilon: 1.0    steps: 249  evaluation reward: 184.4\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 418: Policy loss: -1.217321. Value loss: 41.546150. Entropy: 0.789616.\n",
      "Iteration 419: Policy loss: -1.233851. Value loss: 28.213772. Entropy: 0.762821.\n",
      "Iteration 420: Policy loss: -1.219423. Value loss: 23.130018. Entropy: 0.844490.\n",
      "episode: 183   score: 135.0  epsilon: 1.0    steps: 308  evaluation reward: 183.65\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 421: Policy loss: 0.570220. Value loss: 21.160812. Entropy: 0.908846.\n",
      "Iteration 422: Policy loss: 0.575146. Value loss: 17.479631. Entropy: 0.928578.\n",
      "Iteration 423: Policy loss: 0.529016. Value loss: 14.824401. Entropy: 0.933020.\n",
      "episode: 184   score: 125.0  epsilon: 1.0    steps: 407  evaluation reward: 182.8\n",
      "episode: 185   score: 125.0  epsilon: 1.0    steps: 633  evaluation reward: 182.25\n",
      "episode: 186   score: 105.0  epsilon: 1.0    steps: 855  evaluation reward: 181.2\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 424: Policy loss: 0.439764. Value loss: 30.021767. Entropy: 0.891161.\n",
      "Iteration 425: Policy loss: 0.282980. Value loss: 20.498255. Entropy: 0.840874.\n",
      "Iteration 426: Policy loss: 0.656577. Value loss: 14.754762. Entropy: 0.897495.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 427: Policy loss: 0.172081. Value loss: 28.866226. Entropy: 0.704746.\n",
      "Iteration 428: Policy loss: 0.126249. Value loss: 21.991510. Entropy: 0.692231.\n",
      "Iteration 429: Policy loss: 0.114912. Value loss: 21.658216. Entropy: 0.745684.\n",
      "episode: 187   score: 75.0  epsilon: 1.0    steps: 156  evaluation reward: 180.15\n",
      "episode: 188   score: 90.0  epsilon: 1.0    steps: 326  evaluation reward: 179.5\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 430: Policy loss: 1.143023. Value loss: 30.445818. Entropy: 0.860602.\n",
      "Iteration 431: Policy loss: 1.124554. Value loss: 22.687885. Entropy: 0.863688.\n",
      "Iteration 432: Policy loss: 1.281526. Value loss: 18.710846. Entropy: 0.846826.\n",
      "episode: 189   score: 220.0  epsilon: 1.0    steps: 44  evaluation reward: 179.6\n",
      "episode: 190   score: 210.0  epsilon: 1.0    steps: 729  evaluation reward: 179.9\n",
      "episode: 191   score: 80.0  epsilon: 1.0    steps: 884  evaluation reward: 178.6\n",
      "episode: 192   score: 155.0  epsilon: 1.0    steps: 936  evaluation reward: 175.3\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 433: Policy loss: 0.693748. Value loss: 32.739044. Entropy: 0.888904.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 434: Policy loss: 0.685018. Value loss: 23.438894. Entropy: 0.940887.\n",
      "Iteration 435: Policy loss: 0.601201. Value loss: 21.345741. Entropy: 0.909577.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 436: Policy loss: 0.156238. Value loss: 20.034294. Entropy: 0.844988.\n",
      "Iteration 437: Policy loss: 0.137889. Value loss: 16.058397. Entropy: 0.834918.\n",
      "Iteration 438: Policy loss: 0.110493. Value loss: 14.556019. Entropy: 0.853298.\n",
      "episode: 193   score: 50.0  epsilon: 1.0    steps: 357  evaluation reward: 174.75\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 439: Policy loss: -0.947008. Value loss: 27.475838. Entropy: 0.907396.\n",
      "Iteration 440: Policy loss: -0.862115. Value loss: 19.089130. Entropy: 0.930528.\n",
      "Iteration 441: Policy loss: -0.882099. Value loss: 16.341553. Entropy: 0.938067.\n",
      "episode: 194   score: 150.0  epsilon: 1.0    steps: 569  evaluation reward: 174.45\n",
      "episode: 195   score: 50.0  epsilon: 1.0    steps: 965  evaluation reward: 173.15\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 442: Policy loss: -0.357340. Value loss: 13.898726. Entropy: 1.015751.\n",
      "Iteration 443: Policy loss: -0.300525. Value loss: 6.178898. Entropy: 1.029467.\n",
      "Iteration 444: Policy loss: -0.366032. Value loss: 6.123773. Entropy: 1.014204.\n",
      "episode: 196   score: 260.0  epsilon: 1.0    steps: 498  evaluation reward: 174.2\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 445: Policy loss: 0.601841. Value loss: 28.127041. Entropy: 0.706806.\n",
      "Iteration 446: Policy loss: 0.271179. Value loss: 19.593763. Entropy: 0.674070.\n",
      "Iteration 447: Policy loss: 0.544593. Value loss: 19.028097. Entropy: 0.708101.\n",
      "episode: 197   score: 145.0  epsilon: 1.0    steps: 59  evaluation reward: 173.55\n",
      "episode: 198   score: 210.0  epsilon: 1.0    steps: 201  evaluation reward: 171.55\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 448: Policy loss: -0.043275. Value loss: 196.689774. Entropy: 0.966819.\n",
      "Iteration 449: Policy loss: 0.383667. Value loss: 158.901871. Entropy: 0.954741.\n",
      "Iteration 450: Policy loss: 0.183659. Value loss: 134.346100. Entropy: 0.962490.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 451: Policy loss: -0.687798. Value loss: 30.560427. Entropy: 0.741237.\n",
      "Iteration 452: Policy loss: -0.716921. Value loss: 27.325260. Entropy: 0.733217.\n",
      "Iteration 453: Policy loss: -0.674366. Value loss: 24.268875. Entropy: 0.754353.\n",
      "episode: 199   score: 410.0  epsilon: 1.0    steps: 679  evaluation reward: 174.55\n",
      "episode: 200   score: 180.0  epsilon: 1.0    steps: 841  evaluation reward: 173.1\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 454: Policy loss: 1.063711. Value loss: 15.463346. Entropy: 0.896726.\n",
      "Iteration 455: Policy loss: 1.099119. Value loss: 12.131937. Entropy: 0.862004.\n",
      "Iteration 456: Policy loss: 0.829152. Value loss: 10.613924. Entropy: 0.884828.\n",
      "now time :  2019-02-26 12:35:48.106032\n",
      "episode: 201   score: 105.0  epsilon: 1.0    steps: 537  evaluation reward: 172.95\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 457: Policy loss: 0.208101. Value loss: 18.932945. Entropy: 0.807962.\n",
      "Iteration 458: Policy loss: 0.251367. Value loss: 15.260134. Entropy: 0.760320.\n",
      "Iteration 459: Policy loss: 0.006790. Value loss: 13.584116. Entropy: 0.815194.\n",
      "episode: 202   score: 210.0  epsilon: 1.0    steps: 992  evaluation reward: 171.6\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 460: Policy loss: -0.708110. Value loss: 22.282555. Entropy: 0.680768.\n",
      "Iteration 461: Policy loss: -0.972743. Value loss: 20.782927. Entropy: 0.683935.\n",
      "Iteration 462: Policy loss: -0.924599. Value loss: 22.089794. Entropy: 0.675495.\n",
      "episode: 203   score: 250.0  epsilon: 1.0    steps: 287  evaluation reward: 172.55\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 463: Policy loss: 0.978705. Value loss: 18.150578. Entropy: 0.646250.\n",
      "Iteration 464: Policy loss: 0.976727. Value loss: 10.714368. Entropy: 0.662209.\n",
      "Iteration 465: Policy loss: 0.903525. Value loss: 8.562984. Entropy: 0.608816.\n",
      "episode: 204   score: 180.0  epsilon: 1.0    steps: 90  evaluation reward: 172.55\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 466: Policy loss: 0.481001. Value loss: 19.718916. Entropy: 0.591499.\n",
      "Iteration 467: Policy loss: 0.576584. Value loss: 16.783146. Entropy: 0.532617.\n",
      "Iteration 468: Policy loss: 0.603614. Value loss: 15.600362. Entropy: 0.541848.\n",
      "episode: 205   score: 285.0  epsilon: 1.0    steps: 459  evaluation reward: 174.65\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 469: Policy loss: 0.556654. Value loss: 26.274162. Entropy: 0.472283.\n",
      "Iteration 470: Policy loss: 0.655959. Value loss: 17.891731. Entropy: 0.496726.\n",
      "Iteration 471: Policy loss: 0.487992. Value loss: 16.112608. Entropy: 0.509760.\n",
      "episode: 206   score: 265.0  epsilon: 1.0    steps: 213  evaluation reward: 175.75\n",
      "episode: 207   score: 155.0  epsilon: 1.0    steps: 880  evaluation reward: 176.2\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 472: Policy loss: 1.341779. Value loss: 14.792909. Entropy: 0.606208.\n",
      "Iteration 473: Policy loss: 1.458874. Value loss: 8.870699. Entropy: 0.536722.\n",
      "Iteration 474: Policy loss: 1.289863. Value loss: 7.406345. Entropy: 0.648487.\n",
      "episode: 208   score: 180.0  epsilon: 1.0    steps: 566  evaluation reward: 175.5\n",
      "episode: 209   score: 240.0  epsilon: 1.0    steps: 764  evaluation reward: 176.8\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 475: Policy loss: 0.618672. Value loss: 15.022923. Entropy: 0.710224.\n",
      "Iteration 476: Policy loss: 0.735714. Value loss: 11.173347. Entropy: 0.712717.\n",
      "Iteration 477: Policy loss: 0.502443. Value loss: 9.768908. Entropy: 0.698531.\n",
      "episode: 210   score: 105.0  epsilon: 1.0    steps: 290  evaluation reward: 176.8\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 478: Policy loss: 1.159521. Value loss: 17.220680. Entropy: 0.718175.\n",
      "Iteration 479: Policy loss: 1.080132. Value loss: 14.220143. Entropy: 0.784458.\n",
      "Iteration 480: Policy loss: 1.090857. Value loss: 12.739180. Entropy: 0.761194.\n",
      "episode: 211   score: 180.0  epsilon: 1.0    steps: 915  evaluation reward: 177.7\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 481: Policy loss: 1.473626. Value loss: 9.501762. Entropy: 0.882522.\n",
      "Iteration 482: Policy loss: 1.503875. Value loss: 8.612669. Entropy: 0.892325.\n",
      "Iteration 483: Policy loss: 1.567062. Value loss: 7.744895. Entropy: 0.894301.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 484: Policy loss: 0.353756. Value loss: 16.926523. Entropy: 0.861752.\n",
      "Iteration 485: Policy loss: 0.318145. Value loss: 13.100296. Entropy: 0.865863.\n",
      "Iteration 486: Policy loss: 0.421862. Value loss: 9.929245. Entropy: 0.900456.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 487: Policy loss: 0.828340. Value loss: 10.271940. Entropy: 0.713862.\n",
      "Iteration 488: Policy loss: 0.820122. Value loss: 10.123721. Entropy: 0.708333.\n",
      "Iteration 489: Policy loss: 0.911084. Value loss: 9.469331. Entropy: 0.786175.\n",
      "episode: 212   score: 260.0  epsilon: 1.0    steps: 48  evaluation reward: 179.1\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 490: Policy loss: 0.288751. Value loss: 14.477822. Entropy: 0.731787.\n",
      "Iteration 491: Policy loss: 0.242336. Value loss: 10.966237. Entropy: 0.743534.\n",
      "Iteration 492: Policy loss: 0.290087. Value loss: 11.178134. Entropy: 0.756953.\n",
      "episode: 213   score: 210.0  epsilon: 1.0    steps: 136  evaluation reward: 178.05\n",
      "episode: 214   score: 265.0  epsilon: 1.0    steps: 448  evaluation reward: 179.65\n",
      "episode: 215   score: 210.0  epsilon: 1.0    steps: 623  evaluation reward: 180.7\n",
      "episode: 216   score: 210.0  epsilon: 1.0    steps: 802  evaluation reward: 181.25\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 493: Policy loss: -0.582038. Value loss: 15.004343. Entropy: 0.730418.\n",
      "Iteration 494: Policy loss: -0.649456. Value loss: 10.997252. Entropy: 0.710293.\n",
      "Iteration 495: Policy loss: -0.505755. Value loss: 10.253805. Entropy: 0.734612.\n",
      "episode: 217   score: 210.0  epsilon: 1.0    steps: 341  evaluation reward: 180.75\n",
      "episode: 218   score: 210.0  epsilon: 1.0    steps: 687  evaluation reward: 182.1\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 496: Policy loss: -2.158806. Value loss: 255.727051. Entropy: 0.706628.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 497: Policy loss: -2.869219. Value loss: 144.810547. Entropy: 0.780263.\n",
      "Iteration 498: Policy loss: -2.581054. Value loss: 123.001953. Entropy: 0.749960.\n",
      "episode: 219   score: 355.0  epsilon: 1.0    steps: 956  evaluation reward: 183.85\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 499: Policy loss: -0.033314. Value loss: 13.755231. Entropy: 0.872719.\n",
      "Iteration 500: Policy loss: 0.007322. Value loss: 11.209599. Entropy: 0.871224.\n",
      "Iteration 501: Policy loss: -0.093113. Value loss: 10.645125. Entropy: 0.862429.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 502: Policy loss: -0.025862. Value loss: 10.893949. Entropy: 0.884854.\n",
      "Iteration 503: Policy loss: -0.101424. Value loss: 7.984505. Entropy: 0.889052.\n",
      "Iteration 504: Policy loss: -0.064026. Value loss: 6.341643. Entropy: 0.904426.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 505: Policy loss: 0.824595. Value loss: 15.914540. Entropy: 0.707347.\n",
      "Iteration 506: Policy loss: 0.990628. Value loss: 8.044593. Entropy: 0.684487.\n",
      "Iteration 507: Policy loss: 1.119468. Value loss: 6.646885. Entropy: 0.739336.\n",
      "episode: 220   score: 180.0  epsilon: 1.0    steps: 97  evaluation reward: 183.55\n",
      "episode: 221   score: 105.0  epsilon: 1.0    steps: 139  evaluation reward: 183.85\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 508: Policy loss: 1.039860. Value loss: 15.911711. Entropy: 0.647886.\n",
      "Iteration 509: Policy loss: 0.932552. Value loss: 10.786162. Entropy: 0.692394.\n",
      "Iteration 510: Policy loss: 0.943958. Value loss: 9.453294. Entropy: 0.643099.\n",
      "episode: 222   score: 135.0  epsilon: 1.0    steps: 469  evaluation reward: 183.1\n",
      "episode: 223   score: 135.0  epsilon: 1.0    steps: 832  evaluation reward: 182.35\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 511: Policy loss: 2.703703. Value loss: 19.216692. Entropy: 0.731076.\n",
      "Iteration 512: Policy loss: 2.698404. Value loss: 12.053772. Entropy: 0.737197.\n",
      "Iteration 513: Policy loss: 2.569165. Value loss: 11.298774. Entropy: 0.716277.\n",
      "episode: 224   score: 155.0  epsilon: 1.0    steps: 528  evaluation reward: 181.8\n",
      "episode: 225   score: 105.0  epsilon: 1.0    steps: 995  evaluation reward: 179.8\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 514: Policy loss: 1.295335. Value loss: 23.637028. Entropy: 0.788104.\n",
      "Iteration 515: Policy loss: 1.482157. Value loss: 14.595723. Entropy: 0.843176.\n",
      "Iteration 516: Policy loss: 1.584495. Value loss: 14.404167. Entropy: 0.822744.\n",
      "episode: 226   score: 125.0  epsilon: 1.0    steps: 317  evaluation reward: 177.4\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 517: Policy loss: -0.792485. Value loss: 14.908692. Entropy: 1.093510.\n",
      "Iteration 518: Policy loss: -0.602743. Value loss: 9.077403. Entropy: 1.098216.\n",
      "Iteration 519: Policy loss: -0.747692. Value loss: 9.988810. Entropy: 1.115519.\n",
      "episode: 227   score: 160.0  epsilon: 1.0    steps: 646  evaluation reward: 177.95\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 520: Policy loss: -0.773041. Value loss: 28.406637. Entropy: 0.966600.\n",
      "Iteration 521: Policy loss: -0.615861. Value loss: 18.423222. Entropy: 0.965512.\n",
      "Iteration 522: Policy loss: -0.763079. Value loss: 17.914867. Entropy: 0.913750.\n",
      "episode: 228   score: 140.0  epsilon: 1.0    steps: 117  evaluation reward: 177.5\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 523: Policy loss: -0.273951. Value loss: 27.684681. Entropy: 1.075986.\n",
      "Iteration 524: Policy loss: 0.123361. Value loss: 18.239281. Entropy: 1.084051.\n",
      "Iteration 525: Policy loss: -0.218731. Value loss: 14.158373. Entropy: 0.987561.\n",
      "episode: 229   score: 90.0  epsilon: 1.0    steps: 446  evaluation reward: 176.3\n",
      "episode: 230   score: 145.0  epsilon: 1.0    steps: 856  evaluation reward: 175.65\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 526: Policy loss: -1.800461. Value loss: 39.003529. Entropy: 0.937806.\n",
      "Iteration 527: Policy loss: -1.891959. Value loss: 29.081264. Entropy: 0.887543.\n",
      "Iteration 528: Policy loss: -1.706897. Value loss: 22.688179. Entropy: 0.882607.\n",
      "episode: 231   score: 210.0  epsilon: 1.0    steps: 986  evaluation reward: 176.95\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 529: Policy loss: -1.087549. Value loss: 42.712872. Entropy: 0.893979.\n",
      "Iteration 530: Policy loss: -0.667803. Value loss: 29.483030. Entropy: 0.775484.\n",
      "Iteration 531: Policy loss: -1.130027. Value loss: 26.372000. Entropy: 0.790814.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 532: Policy loss: -0.031417. Value loss: 34.299408. Entropy: 0.914821.\n",
      "Iteration 533: Policy loss: 0.323403. Value loss: 24.573145. Entropy: 0.910976.\n",
      "Iteration 534: Policy loss: 0.349555. Value loss: 17.451773. Entropy: 0.825085.\n",
      "episode: 232   score: 290.0  epsilon: 1.0    steps: 200  evaluation reward: 174.4\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 535: Policy loss: -1.092768. Value loss: 44.990383. Entropy: 0.859267.\n",
      "Iteration 536: Policy loss: -1.170014. Value loss: 23.869135. Entropy: 0.788535.\n",
      "Iteration 537: Policy loss: -1.203165. Value loss: 19.085398. Entropy: 0.812961.\n",
      "episode: 233   score: 255.0  epsilon: 1.0    steps: 294  evaluation reward: 174.35\n",
      "episode: 234   score: 305.0  epsilon: 1.0    steps: 548  evaluation reward: 175.3\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 538: Policy loss: 1.065139. Value loss: 42.200497. Entropy: 0.792842.\n",
      "Iteration 539: Policy loss: 0.928160. Value loss: 27.481121. Entropy: 0.790107.\n",
      "Iteration 540: Policy loss: 0.871769. Value loss: 24.564774. Entropy: 0.822509.\n",
      "episode: 235   score: 85.0  epsilon: 1.0    steps: 419  evaluation reward: 173.35\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 541: Policy loss: 1.468063. Value loss: 31.019224. Entropy: 0.827678.\n",
      "Iteration 542: Policy loss: 1.614584. Value loss: 23.396376. Entropy: 0.859207.\n",
      "Iteration 543: Policy loss: 1.467111. Value loss: 22.095118. Entropy: 0.890359.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 544: Policy loss: -4.519810. Value loss: 320.556335. Entropy: 0.864054.\n",
      "Iteration 545: Policy loss: -2.808937. Value loss: 109.206467. Entropy: 0.908364.\n",
      "Iteration 546: Policy loss: -3.191466. Value loss: 90.342896. Entropy: 0.888967.\n",
      "episode: 236   score: 235.0  epsilon: 1.0    steps: 10  evaluation reward: 174.65\n",
      "episode: 237   score: 560.0  epsilon: 1.0    steps: 668  evaluation reward: 177.65\n",
      "episode: 238   score: 225.0  epsilon: 1.0    steps: 965  evaluation reward: 177.8\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 547: Policy loss: -1.646886. Value loss: 259.106110. Entropy: 0.885799.\n",
      "Iteration 548: Policy loss: -1.894069. Value loss: 143.452240. Entropy: 0.855595.\n",
      "Iteration 549: Policy loss: -1.779064. Value loss: 109.017052. Entropy: 0.888060.\n",
      "episode: 239   score: 125.0  epsilon: 1.0    steps: 206  evaluation reward: 176.95\n",
      "episode: 240   score: 90.0  epsilon: 1.0    steps: 460  evaluation reward: 176.65\n",
      "episode: 241   score: 115.0  epsilon: 1.0    steps: 517  evaluation reward: 176.75\n",
      "episode: 242   score: 250.0  epsilon: 1.0    steps: 817  evaluation reward: 178.2\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 550: Policy loss: 0.612319. Value loss: 37.280746. Entropy: 0.989096.\n",
      "Iteration 551: Policy loss: 0.834761. Value loss: 28.024374. Entropy: 0.982355.\n",
      "Iteration 552: Policy loss: 0.808607. Value loss: 25.925508. Entropy: 0.988462.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 553: Policy loss: -2.306902. Value loss: 210.366928. Entropy: 0.890380.\n",
      "Iteration 554: Policy loss: -2.121497. Value loss: 108.825241. Entropy: 0.931689.\n",
      "Iteration 555: Policy loss: -2.381046. Value loss: 110.523705. Entropy: 0.951669.\n",
      "episode: 243   score: 430.0  epsilon: 1.0    steps: 358  evaluation reward: 180.95\n",
      "episode: 244   score: 60.0  epsilon: 1.0    steps: 689  evaluation reward: 179.45\n",
      "episode: 245   score: 65.0  epsilon: 1.0    steps: 1014  evaluation reward: 179.0\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 556: Policy loss: 0.822473. Value loss: 27.128073. Entropy: 1.172542.\n",
      "Iteration 557: Policy loss: 0.851035. Value loss: 21.767401. Entropy: 1.199212.\n",
      "Iteration 558: Policy loss: 0.899461. Value loss: 19.122084. Entropy: 1.198674.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 559: Policy loss: -0.247459. Value loss: 48.653835. Entropy: 1.163217.\n",
      "Iteration 560: Policy loss: -0.154402. Value loss: 40.285690. Entropy: 1.200026.\n",
      "Iteration 561: Policy loss: -0.066101. Value loss: 35.820118. Entropy: 1.183124.\n",
      "episode: 246   score: 90.0  epsilon: 1.0    steps: 181  evaluation reward: 178.85\n",
      "episode: 247   score: 60.0  epsilon: 1.0    steps: 821  evaluation reward: 174.85\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 562: Policy loss: 0.940456. Value loss: 46.449745. Entropy: 1.126326.\n",
      "Iteration 563: Policy loss: 0.849038. Value loss: 31.073532. Entropy: 1.126516.\n",
      "Iteration 564: Policy loss: 0.441706. Value loss: 28.903900. Entropy: 1.143933.\n",
      "episode: 248   score: 405.0  epsilon: 1.0    steps: 113  evaluation reward: 177.85\n",
      "episode: 249   score: 160.0  epsilon: 1.0    steps: 580  evaluation reward: 177.9\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 565: Policy loss: 1.728884. Value loss: 90.193153. Entropy: 1.126497.\n",
      "Iteration 566: Policy loss: 1.497396. Value loss: 39.234020. Entropy: 1.104366.\n",
      "Iteration 567: Policy loss: 1.625624. Value loss: 29.688799. Entropy: 1.128828.\n",
      "episode: 250   score: 210.0  epsilon: 1.0    steps: 440  evaluation reward: 178.3\n",
      "now time :  2019-02-26 12:37:53.263691\n",
      "episode: 251   score: 40.0  epsilon: 1.0    steps: 680  evaluation reward: 176.35\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 568: Policy loss: 1.546787. Value loss: 39.898487. Entropy: 1.167690.\n",
      "Iteration 569: Policy loss: 1.584622. Value loss: 28.234524. Entropy: 1.105216.\n",
      "Iteration 570: Policy loss: 1.587309. Value loss: 27.037447. Entropy: 1.130730.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 571: Policy loss: -2.182301. Value loss: 54.429966. Entropy: 1.286700.\n",
      "Iteration 572: Policy loss: -2.106601. Value loss: 46.510166. Entropy: 1.279216.\n",
      "Iteration 573: Policy loss: -2.027459. Value loss: 41.362213. Entropy: 1.278385.\n",
      "episode: 252   score: 10.0  epsilon: 1.0    steps: 236  evaluation reward: 174.65\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 574: Policy loss: 0.883838. Value loss: 62.782639. Entropy: 1.047746.\n",
      "Iteration 575: Policy loss: 1.113791. Value loss: 36.843300. Entropy: 1.017623.\n",
      "Iteration 576: Policy loss: 0.928022. Value loss: 27.301998. Entropy: 0.963310.\n",
      "episode: 253   score: 220.0  epsilon: 1.0    steps: 274  evaluation reward: 174.55\n",
      "episode: 254   score: 30.0  epsilon: 1.0    steps: 688  evaluation reward: 173.05\n",
      "episode: 255   score: 180.0  epsilon: 1.0    steps: 929  evaluation reward: 173.5\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 577: Policy loss: 0.463787. Value loss: 34.872379. Entropy: 0.952381.\n",
      "Iteration 578: Policy loss: 0.246405. Value loss: 27.246428. Entropy: 0.960898.\n",
      "Iteration 579: Policy loss: 0.246661. Value loss: 24.362923. Entropy: 0.938688.\n",
      "episode: 256   score: 175.0  epsilon: 1.0    steps: 620  evaluation reward: 173.6\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 580: Policy loss: 0.235696. Value loss: 28.462309. Entropy: 1.154743.\n",
      "Iteration 581: Policy loss: 0.190934. Value loss: 23.703959. Entropy: 1.143553.\n",
      "Iteration 582: Policy loss: 0.303234. Value loss: 22.491226. Entropy: 1.161079.\n",
      "episode: 257   score: 315.0  epsilon: 1.0    steps: 843  evaluation reward: 175.4\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 583: Policy loss: 0.686114. Value loss: 19.674248. Entropy: 1.150657.\n",
      "Iteration 584: Policy loss: 0.831429. Value loss: 15.193243. Entropy: 1.178329.\n",
      "Iteration 585: Policy loss: 0.767098. Value loss: 13.405725. Entropy: 1.186941.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 586: Policy loss: 0.475635. Value loss: 28.324200. Entropy: 1.080932.\n",
      "Iteration 587: Policy loss: 0.614121. Value loss: 24.829834. Entropy: 1.051021.\n",
      "Iteration 588: Policy loss: 0.532944. Value loss: 22.918026. Entropy: 1.085143.\n",
      "episode: 258   score: 190.0  epsilon: 1.0    steps: 252  evaluation reward: 175.15\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 589: Policy loss: 0.532517. Value loss: 47.022816. Entropy: 1.025427.\n",
      "Iteration 590: Policy loss: 0.166743. Value loss: 32.780819. Entropy: 1.009865.\n",
      "Iteration 591: Policy loss: 0.486869. Value loss: 28.922083. Entropy: 1.021507.\n",
      "episode: 259   score: 265.0  epsilon: 1.0    steps: 29  evaluation reward: 175.5\n",
      "episode: 260   score: 80.0  epsilon: 1.0    steps: 331  evaluation reward: 175.25\n",
      "episode: 261   score: 210.0  epsilon: 1.0    steps: 418  evaluation reward: 175.05\n",
      "episode: 262   score: 160.0  epsilon: 1.0    steps: 741  evaluation reward: 174.85\n",
      "episode: 263   score: 125.0  epsilon: 1.0    steps: 926  evaluation reward: 175.0\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 592: Policy loss: 1.018678. Value loss: 19.340536. Entropy: 1.192536.\n",
      "Iteration 593: Policy loss: 1.180172. Value loss: 14.391294. Entropy: 1.198476.\n",
      "Iteration 594: Policy loss: 0.816148. Value loss: 11.775794. Entropy: 1.178340.\n",
      "episode: 264   score: 65.0  epsilon: 1.0    steps: 551  evaluation reward: 175.1\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 595: Policy loss: 1.491317. Value loss: 24.939419. Entropy: 1.246655.\n",
      "Iteration 596: Policy loss: 1.202378. Value loss: 19.937199. Entropy: 1.191133.\n",
      "Iteration 597: Policy loss: 1.275580. Value loss: 17.426067. Entropy: 1.267978.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 598: Policy loss: -0.594845. Value loss: 36.012920. Entropy: 1.296654.\n",
      "Iteration 599: Policy loss: -0.457975. Value loss: 21.453072. Entropy: 1.347342.\n",
      "Iteration 600: Policy loss: -0.572355. Value loss: 17.784430. Entropy: 1.333532.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 601: Policy loss: 1.510266. Value loss: 28.892040. Entropy: 1.143905.\n",
      "Iteration 602: Policy loss: 1.645862. Value loss: 21.243687. Entropy: 1.156307.\n",
      "Iteration 603: Policy loss: 1.394440. Value loss: 18.665632. Entropy: 1.252640.\n",
      "episode: 265   score: 200.0  epsilon: 1.0    steps: 888  evaluation reward: 175.3\n",
      "episode: 266   score: 35.0  epsilon: 1.0    steps: 994  evaluation reward: 175.35\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 604: Policy loss: 1.859043. Value loss: 48.889694. Entropy: 1.196608.\n",
      "Iteration 605: Policy loss: 2.128546. Value loss: 37.792336. Entropy: 1.182678.\n",
      "Iteration 606: Policy loss: 2.024694. Value loss: 28.243263. Entropy: 1.133219.\n",
      "episode: 267   score: 210.0  epsilon: 1.0    steps: 111  evaluation reward: 175.35\n",
      "episode: 268   score: 120.0  epsilon: 1.0    steps: 165  evaluation reward: 175.0\n",
      "episode: 269   score: 110.0  epsilon: 1.0    steps: 345  evaluation reward: 175.05\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 607: Policy loss: 0.618506. Value loss: 25.579035. Entropy: 1.385063.\n",
      "Iteration 608: Policy loss: 0.648030. Value loss: 17.727455. Entropy: 1.365498.\n",
      "Iteration 609: Policy loss: 0.431975. Value loss: 19.030436. Entropy: 1.360031.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 610: Policy loss: -0.625973. Value loss: 19.310463. Entropy: 1.390468.\n",
      "Iteration 611: Policy loss: -0.639262. Value loss: 15.477700. Entropy: 1.429354.\n",
      "Iteration 612: Policy loss: -0.460434. Value loss: 12.325869. Entropy: 1.418052.\n",
      "episode: 270   score: 360.0  epsilon: 1.0    steps: 492  evaluation reward: 177.6\n",
      "episode: 271   score: 50.0  epsilon: 1.0    steps: 556  evaluation reward: 177.0\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 613: Policy loss: -4.665788. Value loss: 320.826111. Entropy: 1.192024.\n",
      "Iteration 614: Policy loss: -4.149050. Value loss: 196.592453. Entropy: 1.257095.\n",
      "Iteration 615: Policy loss: -4.203698. Value loss: 180.549774. Entropy: 1.248360.\n",
      "episode: 272   score: 45.0  epsilon: 1.0    steps: 771  evaluation reward: 176.8\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 616: Policy loss: -1.504854. Value loss: 27.640665. Entropy: 1.310635.\n",
      "Iteration 617: Policy loss: -1.518138. Value loss: 18.517584. Entropy: 1.280523.\n",
      "Iteration 618: Policy loss: -1.562860. Value loss: 15.035837. Entropy: 1.304205.\n",
      "episode: 273   score: 210.0  epsilon: 1.0    steps: 654  evaluation reward: 176.3\n",
      "episode: 274   score: 155.0  epsilon: 1.0    steps: 1019  evaluation reward: 174.0\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 619: Policy loss: 0.916700. Value loss: 26.572618. Entropy: 1.126110.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 620: Policy loss: 1.279294. Value loss: 17.873552. Entropy: 1.151036.\n",
      "Iteration 621: Policy loss: 0.920216. Value loss: 14.364750. Entropy: 1.161243.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 622: Policy loss: -6.356101. Value loss: 471.963959. Entropy: 1.090229.\n",
      "Iteration 623: Policy loss: -5.312871. Value loss: 291.055511. Entropy: 1.017005.\n",
      "Iteration 624: Policy loss: -5.780021. Value loss: 268.739410. Entropy: 1.116791.\n",
      "episode: 275   score: 415.0  epsilon: 1.0    steps: 172  evaluation reward: 176.05\n",
      "episode: 276   score: 235.0  epsilon: 1.0    steps: 372  evaluation reward: 176.3\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 625: Policy loss: 0.230960. Value loss: 116.726196. Entropy: 1.197583.\n",
      "Iteration 626: Policy loss: 0.838236. Value loss: 73.674355. Entropy: 1.183872.\n",
      "Iteration 627: Policy loss: 0.313165. Value loss: 56.506966. Entropy: 1.190513.\n",
      "episode: 277   score: 140.0  epsilon: 1.0    steps: 476  evaluation reward: 176.6\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 628: Policy loss: 1.912247. Value loss: 32.275990. Entropy: 1.109506.\n",
      "Iteration 629: Policy loss: 1.804178. Value loss: 24.173122. Entropy: 1.034020.\n",
      "Iteration 630: Policy loss: 1.752053. Value loss: 20.115780. Entropy: 1.071732.\n",
      "episode: 278   score: 180.0  epsilon: 1.0    steps: 843  evaluation reward: 176.15\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 631: Policy loss: -5.744862. Value loss: 331.771606. Entropy: 1.104061.\n",
      "Iteration 632: Policy loss: -4.974518. Value loss: 186.028061. Entropy: 1.082774.\n",
      "Iteration 633: Policy loss: -4.914011. Value loss: 101.624001. Entropy: 1.041944.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 634: Policy loss: 0.478785. Value loss: 59.000294. Entropy: 0.936380.\n",
      "Iteration 635: Policy loss: 0.784967. Value loss: 38.378174. Entropy: 0.934250.\n",
      "Iteration 636: Policy loss: 0.443471. Value loss: 33.286919. Entropy: 0.928406.\n",
      "episode: 279   score: 490.0  epsilon: 1.0    steps: 9  evaluation reward: 180.0\n",
      "episode: 280   score: 100.0  epsilon: 1.0    steps: 173  evaluation reward: 178.9\n",
      "episode: 281   score: 510.0  epsilon: 1.0    steps: 633  evaluation reward: 182.65\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 637: Policy loss: 1.542568. Value loss: 62.302654. Entropy: 0.973251.\n",
      "Iteration 638: Policy loss: 1.351341. Value loss: 36.370590. Entropy: 1.004873.\n",
      "Iteration 639: Policy loss: 0.956739. Value loss: 32.694233. Entropy: 1.078350.\n",
      "episode: 282   score: 295.0  epsilon: 1.0    steps: 976  evaluation reward: 184.85\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 640: Policy loss: 2.444716. Value loss: 97.480064. Entropy: 1.134371.\n",
      "Iteration 641: Policy loss: 2.761415. Value loss: 72.940575. Entropy: 1.174759.\n",
      "Iteration 642: Policy loss: 2.423168. Value loss: 64.827888. Entropy: 1.134625.\n",
      "episode: 283   score: 145.0  epsilon: 1.0    steps: 481  evaluation reward: 184.95\n",
      "episode: 284   score: 440.0  epsilon: 1.0    steps: 763  evaluation reward: 188.1\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 643: Policy loss: 1.786071. Value loss: 66.450912. Entropy: 1.109491.\n",
      "Iteration 644: Policy loss: 1.495038. Value loss: 47.103012. Entropy: 1.090381.\n",
      "Iteration 645: Policy loss: 1.666989. Value loss: 45.157036. Entropy: 1.135367.\n",
      "episode: 285   score: 190.0  epsilon: 1.0    steps: 287  evaluation reward: 188.75\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 646: Policy loss: 0.098761. Value loss: 36.392223. Entropy: 1.151826.\n",
      "Iteration 647: Policy loss: 0.129725. Value loss: 27.765331. Entropy: 1.137375.\n",
      "Iteration 648: Policy loss: 0.305399. Value loss: 21.916471. Entropy: 1.129670.\n",
      "episode: 286   score: 55.0  epsilon: 1.0    steps: 213  evaluation reward: 188.25\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 649: Policy loss: 0.255521. Value loss: 56.226063. Entropy: 1.013789.\n",
      "Iteration 650: Policy loss: 0.301452. Value loss: 45.487522. Entropy: 1.007456.\n",
      "Iteration 651: Policy loss: 0.210065. Value loss: 42.266525. Entropy: 1.025846.\n",
      "episode: 287   score: 145.0  epsilon: 1.0    steps: 98  evaluation reward: 188.95\n",
      "episode: 288   score: 115.0  epsilon: 1.0    steps: 827  evaluation reward: 189.2\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 652: Policy loss: 0.616342. Value loss: 50.388760. Entropy: 1.093076.\n",
      "Iteration 653: Policy loss: 0.915710. Value loss: 34.372643. Entropy: 1.095444.\n",
      "Iteration 654: Policy loss: 0.389763. Value loss: 27.469204. Entropy: 1.167095.\n",
      "episode: 289   score: 80.0  epsilon: 1.0    steps: 334  evaluation reward: 187.8\n",
      "episode: 290   score: 150.0  epsilon: 1.0    steps: 524  evaluation reward: 187.2\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 655: Policy loss: 0.285933. Value loss: 43.255852. Entropy: 1.250267.\n",
      "Iteration 656: Policy loss: 0.238937. Value loss: 30.964546. Entropy: 1.246787.\n",
      "Iteration 657: Policy loss: 0.127275. Value loss: 25.729176. Entropy: 1.241886.\n",
      "episode: 291   score: 125.0  epsilon: 1.0    steps: 455  evaluation reward: 187.65\n",
      "episode: 292   score: 235.0  epsilon: 1.0    steps: 957  evaluation reward: 188.45\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 658: Policy loss: 3.089009. Value loss: 50.682896. Entropy: 1.056825.\n",
      "Iteration 659: Policy loss: 3.045502. Value loss: 35.915379. Entropy: 1.045161.\n",
      "Iteration 660: Policy loss: 2.823369. Value loss: 29.462118. Entropy: 1.068461.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 661: Policy loss: 1.031598. Value loss: 34.438488. Entropy: 1.129057.\n",
      "Iteration 662: Policy loss: 1.035064. Value loss: 25.422510. Entropy: 1.109833.\n",
      "Iteration 663: Policy loss: 1.158989. Value loss: 21.279694. Entropy: 1.109292.\n",
      "episode: 293   score: 15.0  epsilon: 1.0    steps: 359  evaluation reward: 188.1\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 664: Policy loss: -0.385289. Value loss: 54.086185. Entropy: 1.130559.\n",
      "Iteration 665: Policy loss: -0.254007. Value loss: 41.478806. Entropy: 1.094036.\n",
      "Iteration 666: Policy loss: -0.546152. Value loss: 31.737755. Entropy: 1.130400.\n",
      "episode: 294   score: 75.0  epsilon: 1.0    steps: 115  evaluation reward: 187.35\n",
      "episode: 295   score: 135.0  epsilon: 1.0    steps: 800  evaluation reward: 188.2\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 667: Policy loss: -1.187940. Value loss: 213.743683. Entropy: 0.948577.\n",
      "Iteration 668: Policy loss: -1.957002. Value loss: 167.644791. Entropy: 0.876619.\n",
      "Iteration 669: Policy loss: -1.987645. Value loss: 131.477036. Entropy: 0.905303.\n",
      "episode: 296   score: 190.0  epsilon: 1.0    steps: 201  evaluation reward: 187.5\n",
      "episode: 297   score: 235.0  epsilon: 1.0    steps: 624  evaluation reward: 188.4\n",
      "episode: 298   score: 210.0  epsilon: 1.0    steps: 720  evaluation reward: 188.4\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 670: Policy loss: 1.065021. Value loss: 45.814957. Entropy: 0.942148.\n",
      "Iteration 671: Policy loss: 0.826131. Value loss: 32.804943. Entropy: 0.952309.\n",
      "Iteration 672: Policy loss: 1.000712. Value loss: 27.229715. Entropy: 0.949667.\n",
      "episode: 299   score: 180.0  epsilon: 1.0    steps: 1018  evaluation reward: 186.1\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 673: Policy loss: -0.996319. Value loss: 44.119320. Entropy: 0.940893.\n",
      "Iteration 674: Policy loss: -1.203372. Value loss: 31.057533. Entropy: 0.926472.\n",
      "Iteration 675: Policy loss: -0.999949. Value loss: 26.004381. Entropy: 0.926377.\n",
      "episode: 300   score: 105.0  epsilon: 1.0    steps: 113  evaluation reward: 185.35\n",
      "now time :  2019-02-26 12:39:56.245544\n",
      "episode: 301   score: 490.0  epsilon: 1.0    steps: 507  evaluation reward: 189.2\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 676: Policy loss: 0.267480. Value loss: 31.998463. Entropy: 0.955931.\n",
      "Iteration 677: Policy loss: 0.077655. Value loss: 21.359882. Entropy: 0.966094.\n",
      "Iteration 678: Policy loss: 0.395853. Value loss: 18.921551. Entropy: 0.969733.\n",
      "episode: 302   score: 70.0  epsilon: 1.0    steps: 344  evaluation reward: 187.8\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 679: Policy loss: 0.713523. Value loss: 54.681984. Entropy: 0.866115.\n",
      "Iteration 680: Policy loss: 0.870532. Value loss: 41.435226. Entropy: 0.885601.\n",
      "Iteration 681: Policy loss: 0.611382. Value loss: 37.170033. Entropy: 0.865057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 303   score: 185.0  epsilon: 1.0    steps: 818  evaluation reward: 187.15\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 682: Policy loss: 0.199533. Value loss: 38.971970. Entropy: 0.875023.\n",
      "Iteration 683: Policy loss: 0.358227. Value loss: 23.479279. Entropy: 0.895578.\n",
      "Iteration 684: Policy loss: 0.539025. Value loss: 20.305292. Entropy: 0.913318.\n",
      "episode: 304   score: 185.0  epsilon: 1.0    steps: 222  evaluation reward: 187.2\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 685: Policy loss: -1.131722. Value loss: 45.407246. Entropy: 0.995499.\n",
      "Iteration 686: Policy loss: -1.151071. Value loss: 31.908087. Entropy: 1.016736.\n",
      "Iteration 687: Policy loss: -1.187189. Value loss: 31.037338. Entropy: 1.003963.\n",
      "episode: 305   score: 75.0  epsilon: 1.0    steps: 919  evaluation reward: 185.1\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 688: Policy loss: 0.790027. Value loss: 42.425541. Entropy: 1.001015.\n",
      "Iteration 689: Policy loss: 0.717023. Value loss: 30.271191. Entropy: 0.987075.\n",
      "Iteration 690: Policy loss: 0.891300. Value loss: 24.311668. Entropy: 0.953872.\n",
      "episode: 306   score: 60.0  epsilon: 1.0    steps: 17  evaluation reward: 183.05\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 691: Policy loss: -1.385371. Value loss: 39.743538. Entropy: 0.989513.\n",
      "Iteration 692: Policy loss: -1.249271. Value loss: 26.581673. Entropy: 0.991147.\n",
      "Iteration 693: Policy loss: -1.308703. Value loss: 21.999866. Entropy: 0.964130.\n",
      "episode: 307   score: 250.0  epsilon: 1.0    steps: 633  evaluation reward: 184.0\n",
      "episode: 308   score: 235.0  epsilon: 1.0    steps: 733  evaluation reward: 184.55\n",
      "episode: 309   score: 75.0  epsilon: 1.0    steps: 827  evaluation reward: 182.9\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 694: Policy loss: 1.043911. Value loss: 29.133020. Entropy: 0.964079.\n",
      "Iteration 695: Policy loss: 0.859241. Value loss: 18.564678. Entropy: 0.976406.\n",
      "Iteration 696: Policy loss: 0.890780. Value loss: 17.725309. Entropy: 0.969285.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 697: Policy loss: -1.150495. Value loss: 39.179672. Entropy: 0.855364.\n",
      "Iteration 698: Policy loss: -1.092034. Value loss: 32.833755. Entropy: 0.861406.\n",
      "Iteration 699: Policy loss: -1.211668. Value loss: 32.608250. Entropy: 0.883670.\n",
      "episode: 310   score: 335.0  epsilon: 1.0    steps: 337  evaluation reward: 185.2\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 700: Policy loss: 0.758744. Value loss: 30.741089. Entropy: 0.940877.\n",
      "Iteration 701: Policy loss: 0.730063. Value loss: 22.214542. Entropy: 0.936128.\n",
      "Iteration 702: Policy loss: 0.550012. Value loss: 20.389771. Entropy: 0.943909.\n",
      "episode: 311   score: 155.0  epsilon: 1.0    steps: 193  evaluation reward: 184.95\n",
      "episode: 312   score: 105.0  epsilon: 1.0    steps: 916  evaluation reward: 183.4\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 703: Policy loss: 0.348125. Value loss: 20.431953. Entropy: 0.952658.\n",
      "Iteration 704: Policy loss: 0.483552. Value loss: 13.436932. Entropy: 0.956863.\n",
      "Iteration 705: Policy loss: 0.295558. Value loss: 12.019992. Entropy: 0.970055.\n",
      "episode: 313   score: 110.0  epsilon: 1.0    steps: 41  evaluation reward: 182.4\n",
      "episode: 314   score: 310.0  epsilon: 1.0    steps: 439  evaluation reward: 182.85\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 706: Policy loss: 1.147058. Value loss: 17.906260. Entropy: 0.945257.\n",
      "Iteration 707: Policy loss: 1.279198. Value loss: 12.756924. Entropy: 0.931385.\n",
      "Iteration 708: Policy loss: 1.292729. Value loss: 11.505234. Entropy: 0.898729.\n",
      "episode: 315   score: 105.0  epsilon: 1.0    steps: 767  evaluation reward: 181.8\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 709: Policy loss: -0.077688. Value loss: 27.327694. Entropy: 0.845425.\n",
      "Iteration 710: Policy loss: -0.005563. Value loss: 18.435963. Entropy: 0.871731.\n",
      "Iteration 711: Policy loss: 0.053486. Value loss: 17.957661. Entropy: 0.872067.\n",
      "episode: 316   score: 110.0  epsilon: 1.0    steps: 548  evaluation reward: 180.8\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 712: Policy loss: 0.336612. Value loss: 10.467981. Entropy: 0.963333.\n",
      "Iteration 713: Policy loss: 0.363329. Value loss: 6.800554. Entropy: 0.940156.\n",
      "Iteration 714: Policy loss: 0.425360. Value loss: 5.506265. Entropy: 0.959085.\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 715: Policy loss: 0.912856. Value loss: 19.314466. Entropy: 0.943952.\n",
      "Iteration 716: Policy loss: 0.804456. Value loss: 16.679865. Entropy: 0.960680.\n",
      "Iteration 717: Policy loss: 1.140427. Value loss: 14.224998. Entropy: 0.929706.\n",
      "episode: 317   score: 115.0  epsilon: 1.0    steps: 1019  evaluation reward: 179.85\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 718: Policy loss: 2.308339. Value loss: 19.298449. Entropy: 1.012941.\n",
      "Iteration 719: Policy loss: 2.457257. Value loss: 13.743608. Entropy: 1.019928.\n",
      "Iteration 720: Policy loss: 2.373477. Value loss: 12.305449. Entropy: 1.012653.\n",
      "episode: 318   score: 110.0  epsilon: 1.0    steps: 85  evaluation reward: 178.85\n",
      "episode: 319   score: 105.0  epsilon: 1.0    steps: 208  evaluation reward: 176.35\n",
      "episode: 320   score: 185.0  epsilon: 1.0    steps: 262  evaluation reward: 176.4\n",
      "episode: 321   score: 110.0  epsilon: 1.0    steps: 500  evaluation reward: 176.45\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 721: Policy loss: 1.613782. Value loss: 13.245864. Entropy: 0.891192.\n",
      "Iteration 722: Policy loss: 1.615881. Value loss: 8.799537. Entropy: 0.844517.\n",
      "Iteration 723: Policy loss: 1.335905. Value loss: 7.302950. Entropy: 0.868843.\n",
      "episode: 322   score: 305.0  epsilon: 1.0    steps: 867  evaluation reward: 178.15\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 724: Policy loss: 2.768354. Value loss: 31.534492. Entropy: 0.668641.\n",
      "Iteration 725: Policy loss: 2.645619. Value loss: 20.129440. Entropy: 0.621244.\n",
      "Iteration 726: Policy loss: 2.542338. Value loss: 17.909214. Entropy: 0.629426.\n",
      "episode: 323   score: 105.0  epsilon: 1.0    steps: 580  evaluation reward: 177.85\n",
      "episode: 324   score: 110.0  epsilon: 1.0    steps: 680  evaluation reward: 177.4\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 727: Policy loss: 0.861390. Value loss: 18.414415. Entropy: 0.750129.\n",
      "Iteration 728: Policy loss: 0.820208. Value loss: 17.899399. Entropy: 0.789934.\n",
      "Iteration 729: Policy loss: 0.706998. Value loss: 14.992547. Entropy: 0.741594.\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 730: Policy loss: 0.554871. Value loss: 21.550053. Entropy: 0.914936.\n",
      "Iteration 731: Policy loss: 0.859739. Value loss: 18.253843. Entropy: 0.923600.\n",
      "Iteration 732: Policy loss: 0.637628. Value loss: 19.210981. Entropy: 0.928017.\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 733: Policy loss: 1.600106. Value loss: 5.530803. Entropy: 1.071487.\n",
      "Iteration 734: Policy loss: 1.564916. Value loss: 3.883231. Entropy: 1.085518.\n",
      "Iteration 735: Policy loss: 1.511064. Value loss: 4.092460. Entropy: 1.121406.\n",
      "episode: 325   score: 110.0  epsilon: 1.0    steps: 323  evaluation reward: 177.45\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 736: Policy loss: -0.045368. Value loss: 9.125732. Entropy: 1.008458.\n",
      "Iteration 737: Policy loss: 0.012428. Value loss: 6.264162. Entropy: 1.057076.\n",
      "Iteration 738: Policy loss: -0.113847. Value loss: 5.913581. Entropy: 1.072818.\n",
      "episode: 326   score: 110.0  epsilon: 1.0    steps: 135  evaluation reward: 177.3\n",
      "episode: 327   score: 110.0  epsilon: 1.0    steps: 431  evaluation reward: 176.8\n",
      "episode: 328   score: 105.0  epsilon: 1.0    steps: 867  evaluation reward: 176.45\n",
      "episode: 329   score: 145.0  epsilon: 1.0    steps: 913  evaluation reward: 177.0\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 739: Policy loss: 0.575686. Value loss: 14.719177. Entropy: 0.791999.\n",
      "Iteration 740: Policy loss: 0.652737. Value loss: 12.864774. Entropy: 0.825994.\n",
      "Iteration 741: Policy loss: 0.459572. Value loss: 8.822381. Entropy: 0.805962.\n",
      "episode: 330   score: 105.0  epsilon: 1.0    steps: 689  evaluation reward: 176.6\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 742: Policy loss: 0.405463. Value loss: 26.972097. Entropy: 0.703300.\n",
      "Iteration 743: Policy loss: 0.242997. Value loss: 23.684853. Entropy: 0.694974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 744: Policy loss: 0.245105. Value loss: 21.039413. Entropy: 0.730159.\n",
      "episode: 331   score: 115.0  epsilon: 1.0    steps: 586  evaluation reward: 175.65\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 745: Policy loss: 1.653847. Value loss: 20.628984. Entropy: 0.951563.\n",
      "Iteration 746: Policy loss: 1.426474. Value loss: 16.929083. Entropy: 0.960754.\n",
      "Iteration 747: Policy loss: 1.570952. Value loss: 15.804911. Entropy: 0.971307.\n",
      "episode: 332   score: 255.0  epsilon: 1.0    steps: 35  evaluation reward: 175.3\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 748: Policy loss: 0.030440. Value loss: 9.031096. Entropy: 1.124690.\n",
      "Iteration 749: Policy loss: -0.024782. Value loss: 7.545158. Entropy: 1.139948.\n",
      "Iteration 750: Policy loss: -0.019261. Value loss: 7.025337. Entropy: 1.139237.\n",
      "episode: 333   score: 75.0  epsilon: 1.0    steps: 428  evaluation reward: 173.5\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 751: Policy loss: 1.099890. Value loss: 7.073374. Entropy: 1.136604.\n",
      "Iteration 752: Policy loss: 1.082889. Value loss: 4.901340. Entropy: 1.105444.\n",
      "Iteration 753: Policy loss: 1.066641. Value loss: 4.689485. Entropy: 1.105832.\n",
      "episode: 334   score: 105.0  epsilon: 1.0    steps: 136  evaluation reward: 171.5\n",
      "episode: 335   score: 140.0  epsilon: 1.0    steps: 354  evaluation reward: 172.05\n",
      "episode: 336   score: 75.0  epsilon: 1.0    steps: 721  evaluation reward: 170.45\n",
      "episode: 337   score: 110.0  epsilon: 1.0    steps: 959  evaluation reward: 165.95\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 754: Policy loss: -0.254393. Value loss: 17.321918. Entropy: 0.931179.\n",
      "Iteration 755: Policy loss: -0.099410. Value loss: 14.503468. Entropy: 0.944292.\n",
      "Iteration 756: Policy loss: -0.256379. Value loss: 11.940177. Entropy: 0.947373.\n",
      "episode: 338   score: 150.0  epsilon: 1.0    steps: 894  evaluation reward: 165.2\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 757: Policy loss: 0.000193. Value loss: 15.890802. Entropy: 0.910421.\n",
      "Iteration 758: Policy loss: -0.064663. Value loss: 13.843732. Entropy: 0.894347.\n",
      "Iteration 759: Policy loss: -0.275527. Value loss: 14.232519. Entropy: 0.900524.\n",
      "episode: 339   score: 105.0  epsilon: 1.0    steps: 630  evaluation reward: 165.0\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 760: Policy loss: 1.811316. Value loss: 18.390739. Entropy: 0.970869.\n",
      "Iteration 761: Policy loss: 1.694401. Value loss: 15.235990. Entropy: 0.968539.\n",
      "Iteration 762: Policy loss: 1.863819. Value loss: 12.982690. Entropy: 0.946533.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 763: Policy loss: -0.537221. Value loss: 5.816493. Entropy: 1.076201.\n",
      "Iteration 764: Policy loss: -0.724227. Value loss: 5.775056. Entropy: 1.089019.\n",
      "Iteration 765: Policy loss: -0.618451. Value loss: 4.271871. Entropy: 1.069974.\n",
      "episode: 340   score: 180.0  epsilon: 1.0    steps: 73  evaluation reward: 165.9\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 766: Policy loss: -1.069715. Value loss: 14.169073. Entropy: 1.148306.\n",
      "Iteration 767: Policy loss: -1.008421. Value loss: 13.884319. Entropy: 1.155725.\n",
      "Iteration 768: Policy loss: -1.104251. Value loss: 11.837560. Entropy: 1.132759.\n",
      "episode: 341   score: 320.0  epsilon: 1.0    steps: 434  evaluation reward: 167.95\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 769: Policy loss: -9.171594. Value loss: 920.431641. Entropy: 1.017165.\n",
      "Iteration 770: Policy loss: -7.840515. Value loss: 625.385437. Entropy: 1.028064.\n",
      "Iteration 771: Policy loss: -7.970429. Value loss: 575.685242. Entropy: 1.098701.\n",
      "episode: 342   score: 160.0  epsilon: 1.0    steps: 187  evaluation reward: 167.05\n",
      "episode: 343   score: 110.0  epsilon: 1.0    steps: 303  evaluation reward: 163.85\n",
      "episode: 344   score: 370.0  epsilon: 1.0    steps: 762  evaluation reward: 166.95\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 772: Policy loss: 1.769701. Value loss: 30.889267. Entropy: 0.890085.\n",
      "Iteration 773: Policy loss: 1.602240. Value loss: 18.752317. Entropy: 0.928954.\n",
      "Iteration 774: Policy loss: 1.736096. Value loss: 17.861181. Entropy: 0.950460.\n",
      "episode: 345   score: 140.0  epsilon: 1.0    steps: 933  evaluation reward: 167.7\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 775: Policy loss: -0.858464. Value loss: 23.285679. Entropy: 0.952575.\n",
      "Iteration 776: Policy loss: -0.624426. Value loss: 18.842424. Entropy: 0.958691.\n",
      "Iteration 777: Policy loss: -0.800031. Value loss: 17.400688. Entropy: 0.966104.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 778: Policy loss: -0.793334. Value loss: 23.845005. Entropy: 1.082868.\n",
      "Iteration 779: Policy loss: -0.956659. Value loss: 19.621372. Entropy: 1.075460.\n",
      "Iteration 780: Policy loss: -0.844101. Value loss: 18.009851. Entropy: 1.093413.\n",
      "episode: 346   score: 180.0  epsilon: 1.0    steps: 552  evaluation reward: 168.6\n",
      "episode: 347   score: 460.0  epsilon: 1.0    steps: 887  evaluation reward: 172.6\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 781: Policy loss: 0.220443. Value loss: 10.653060. Entropy: 1.218031.\n",
      "Iteration 782: Policy loss: 0.151944. Value loss: 8.832603. Entropy: 1.216362.\n",
      "Iteration 783: Policy loss: 0.256021. Value loss: 6.520411. Entropy: 1.212929.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 784: Policy loss: 0.194288. Value loss: 13.735572. Entropy: 1.107518.\n",
      "Iteration 785: Policy loss: 0.337509. Value loss: 9.718736. Entropy: 1.107073.\n",
      "Iteration 786: Policy loss: 0.216824. Value loss: 8.747808. Entropy: 1.121055.\n",
      "episode: 348   score: 180.0  epsilon: 1.0    steps: 58  evaluation reward: 170.35\n",
      "episode: 349   score: 155.0  epsilon: 1.0    steps: 461  evaluation reward: 170.3\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 787: Policy loss: 0.948164. Value loss: 16.803213. Entropy: 1.016494.\n",
      "Iteration 788: Policy loss: 0.736015. Value loss: 12.300333. Entropy: 1.035190.\n",
      "Iteration 789: Policy loss: 0.984429. Value loss: 12.565353. Entropy: 1.044597.\n",
      "episode: 350   score: 180.0  epsilon: 1.0    steps: 218  evaluation reward: 170.0\n",
      "now time :  2019-02-26 12:42:04.776566\n",
      "episode: 351   score: 185.0  epsilon: 1.0    steps: 331  evaluation reward: 171.45\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 790: Policy loss: -0.358638. Value loss: 28.957190. Entropy: 0.987379.\n",
      "Iteration 791: Policy loss: -0.684378. Value loss: 24.683342. Entropy: 1.015324.\n",
      "Iteration 792: Policy loss: -0.566683. Value loss: 22.365936. Entropy: 1.021690.\n",
      "episode: 352   score: 165.0  epsilon: 1.0    steps: 661  evaluation reward: 173.0\n",
      "episode: 353   score: 210.0  epsilon: 1.0    steps: 996  evaluation reward: 172.9\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 793: Policy loss: -0.731465. Value loss: 16.591969. Entropy: 0.980839.\n",
      "Iteration 794: Policy loss: -0.763456. Value loss: 14.383173. Entropy: 1.020172.\n",
      "Iteration 795: Policy loss: -0.803553. Value loss: 11.110126. Entropy: 1.012221.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 796: Policy loss: -0.015668. Value loss: 15.777867. Entropy: 1.109696.\n",
      "Iteration 797: Policy loss: -0.031470. Value loss: 11.991792. Entropy: 1.127503.\n",
      "Iteration 798: Policy loss: -0.076457. Value loss: 10.874197. Entropy: 1.105055.\n",
      "episode: 354   score: 180.0  epsilon: 1.0    steps: 587  evaluation reward: 174.4\n",
      "episode: 355   score: 105.0  epsilon: 1.0    steps: 778  evaluation reward: 173.65\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 799: Policy loss: -1.989861. Value loss: 203.056839. Entropy: 1.064073.\n",
      "Iteration 800: Policy loss: -1.813805. Value loss: 155.205078. Entropy: 0.999209.\n",
      "Iteration 801: Policy loss: -1.715658. Value loss: 135.177460. Entropy: 1.020976.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 802: Policy loss: -0.687203. Value loss: 23.620543. Entropy: 1.061822.\n",
      "Iteration 803: Policy loss: -0.145091. Value loss: 13.413624. Entropy: 1.008743.\n",
      "Iteration 804: Policy loss: -0.622327. Value loss: 12.824758. Entropy: 0.993345.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 805: Policy loss: -0.394583. Value loss: 19.245371. Entropy: 1.131796.\n",
      "Iteration 806: Policy loss: -0.715487. Value loss: 16.980854. Entropy: 1.103812.\n",
      "Iteration 807: Policy loss: -0.757931. Value loss: 14.798086. Entropy: 1.143030.\n",
      "episode: 356   score: 210.0  epsilon: 1.0    steps: 10  evaluation reward: 174.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 808: Policy loss: 0.137146. Value loss: 16.915133. Entropy: 1.036311.\n",
      "Iteration 809: Policy loss: 0.116427. Value loss: 12.730200. Entropy: 1.048073.\n",
      "Iteration 810: Policy loss: 0.161240. Value loss: 11.768647. Entropy: 1.028202.\n",
      "episode: 357   score: 180.0  epsilon: 1.0    steps: 141  evaluation reward: 172.65\n",
      "episode: 358   score: 210.0  epsilon: 1.0    steps: 272  evaluation reward: 172.85\n",
      "episode: 359   score: 180.0  epsilon: 1.0    steps: 706  evaluation reward: 172.0\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 811: Policy loss: 0.361530. Value loss: 16.650620. Entropy: 0.963433.\n",
      "Iteration 812: Policy loss: 0.449559. Value loss: 12.920420. Entropy: 0.919009.\n",
      "Iteration 813: Policy loss: 0.337339. Value loss: 11.455174. Entropy: 0.948294.\n",
      "episode: 360   score: 495.0  epsilon: 1.0    steps: 425  evaluation reward: 176.15\n",
      "episode: 361   score: 180.0  epsilon: 1.0    steps: 919  evaluation reward: 175.85\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 814: Policy loss: 0.345385. Value loss: 15.116996. Entropy: 0.871826.\n",
      "Iteration 815: Policy loss: 0.225845. Value loss: 14.107061. Entropy: 0.870229.\n",
      "Iteration 816: Policy loss: 0.357397. Value loss: 12.036996. Entropy: 0.860370.\n",
      "episode: 362   score: 180.0  epsilon: 1.0    steps: 638  evaluation reward: 176.05\n",
      "episode: 363   score: 210.0  epsilon: 1.0    steps: 895  evaluation reward: 176.9\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 817: Policy loss: 0.556508. Value loss: 27.033323. Entropy: 1.001723.\n",
      "Iteration 818: Policy loss: 0.318157. Value loss: 20.354216. Entropy: 0.986707.\n",
      "Iteration 819: Policy loss: 0.531829. Value loss: 20.372129. Entropy: 0.999217.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 820: Policy loss: -0.936937. Value loss: 7.647384. Entropy: 1.016165.\n",
      "Iteration 821: Policy loss: -0.930347. Value loss: 6.741716. Entropy: 1.017169.\n",
      "Iteration 822: Policy loss: -0.862055. Value loss: 5.457522. Entropy: 1.025378.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 823: Policy loss: -0.032788. Value loss: 11.144884. Entropy: 0.969957.\n",
      "Iteration 824: Policy loss: -0.262593. Value loss: 6.884085. Entropy: 0.989794.\n",
      "Iteration 825: Policy loss: -0.251049. Value loss: 6.366829. Entropy: 0.977327.\n",
      "episode: 364   score: 210.0  epsilon: 1.0    steps: 79  evaluation reward: 178.35\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 826: Policy loss: 0.723901. Value loss: 7.293743. Entropy: 1.023704.\n",
      "Iteration 827: Policy loss: 0.712882. Value loss: 5.451694. Entropy: 1.027023.\n",
      "Iteration 828: Policy loss: 0.621241. Value loss: 4.807663. Entropy: 1.026135.\n",
      "episode: 365   score: 180.0  epsilon: 1.0    steps: 179  evaluation reward: 178.15\n",
      "episode: 366   score: 210.0  epsilon: 1.0    steps: 339  evaluation reward: 179.9\n",
      "episode: 367   score: 135.0  epsilon: 1.0    steps: 727  evaluation reward: 179.15\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 829: Policy loss: 0.950953. Value loss: 14.586239. Entropy: 0.868184.\n",
      "Iteration 830: Policy loss: 0.969067. Value loss: 11.348157. Entropy: 0.858456.\n",
      "Iteration 831: Policy loss: 0.834094. Value loss: 9.837392. Entropy: 0.885561.\n",
      "episode: 368   score: 180.0  epsilon: 1.0    steps: 458  evaluation reward: 179.75\n",
      "episode: 369   score: 180.0  epsilon: 1.0    steps: 957  evaluation reward: 180.45\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 832: Policy loss: 0.603642. Value loss: 18.050793. Entropy: 0.798688.\n",
      "Iteration 833: Policy loss: 0.423896. Value loss: 15.990955. Entropy: 0.784095.\n",
      "Iteration 834: Policy loss: 0.352239. Value loss: 13.154507. Entropy: 0.804916.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 835: Policy loss: -0.799403. Value loss: 19.330803. Entropy: 0.830428.\n",
      "Iteration 836: Policy loss: -0.859392. Value loss: 15.771949. Entropy: 0.825660.\n",
      "Iteration 837: Policy loss: -0.847774. Value loss: 16.143995. Entropy: 0.805545.\n",
      "episode: 370   score: 210.0  epsilon: 1.0    steps: 561  evaluation reward: 178.95\n",
      "episode: 371   score: 180.0  epsilon: 1.0    steps: 818  evaluation reward: 180.25\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 838: Policy loss: 0.055857. Value loss: 8.291999. Entropy: 0.937674.\n",
      "Iteration 839: Policy loss: 0.072420. Value loss: 8.238451. Entropy: 0.896488.\n",
      "Iteration 840: Policy loss: 0.060120. Value loss: 7.474951. Entropy: 0.911047.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 841: Policy loss: 0.725742. Value loss: 5.010476. Entropy: 0.959243.\n",
      "Iteration 842: Policy loss: 0.695142. Value loss: 3.517398. Entropy: 0.967333.\n",
      "Iteration 843: Policy loss: 0.746353. Value loss: 3.462467. Entropy: 0.963198.\n",
      "episode: 372   score: 180.0  epsilon: 1.0    steps: 117  evaluation reward: 181.6\n",
      "episode: 373   score: 105.0  epsilon: 1.0    steps: 198  evaluation reward: 180.55\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 844: Policy loss: -0.017161. Value loss: 12.748405. Entropy: 0.913786.\n",
      "Iteration 845: Policy loss: 0.177703. Value loss: 8.913363. Entropy: 0.894788.\n",
      "Iteration 846: Policy loss: 0.036833. Value loss: 8.943533. Entropy: 0.913770.\n",
      "episode: 374   score: 180.0  epsilon: 1.0    steps: 377  evaluation reward: 180.8\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 847: Policy loss: -0.247750. Value loss: 9.693092. Entropy: 0.889536.\n",
      "Iteration 848: Policy loss: -0.204246. Value loss: 6.861110. Entropy: 0.882825.\n",
      "Iteration 849: Policy loss: -0.047520. Value loss: 6.281038. Entropy: 0.892166.\n",
      "episode: 375   score: 180.0  epsilon: 1.0    steps: 496  evaluation reward: 178.45\n",
      "episode: 376   score: 210.0  epsilon: 1.0    steps: 649  evaluation reward: 178.2\n",
      "episode: 377   score: 210.0  epsilon: 1.0    steps: 1008  evaluation reward: 178.9\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 850: Policy loss: -0.930468. Value loss: 22.029823. Entropy: 0.791701.\n",
      "Iteration 851: Policy loss: -1.063281. Value loss: 19.947203. Entropy: 0.766399.\n",
      "Iteration 852: Policy loss: -0.946138. Value loss: 18.805191. Entropy: 0.783836.\n",
      "episode: 378   score: 105.0  epsilon: 1.0    steps: 580  evaluation reward: 178.15\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 853: Policy loss: -0.764513. Value loss: 15.365153. Entropy: 0.795574.\n",
      "Iteration 854: Policy loss: -0.620620. Value loss: 10.436665. Entropy: 0.741752.\n",
      "Iteration 855: Policy loss: -0.692530. Value loss: 9.392278. Entropy: 0.760214.\n",
      "episode: 379   score: 210.0  epsilon: 1.0    steps: 888  evaluation reward: 175.35\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 856: Policy loss: -0.596023. Value loss: 16.156727. Entropy: 0.825212.\n",
      "Iteration 857: Policy loss: -0.620111. Value loss: 12.422164. Entropy: 0.837311.\n",
      "Iteration 858: Policy loss: -0.594636. Value loss: 9.833381. Entropy: 0.859506.\n",
      "episode: 380   score: 105.0  epsilon: 1.0    steps: 217  evaluation reward: 175.4\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 859: Policy loss: -0.000853. Value loss: 5.237092. Entropy: 0.968003.\n",
      "Iteration 860: Policy loss: 0.029519. Value loss: 3.560965. Entropy: 0.987991.\n",
      "Iteration 861: Policy loss: -0.002526. Value loss: 2.978462. Entropy: 0.972423.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 862: Policy loss: -0.718992. Value loss: 8.205698. Entropy: 0.916266.\n",
      "Iteration 863: Policy loss: -0.745007. Value loss: 6.391089. Entropy: 0.916927.\n",
      "Iteration 864: Policy loss: -0.775603. Value loss: 4.909129. Entropy: 0.936771.\n",
      "episode: 381   score: 210.0  epsilon: 1.0    steps: 106  evaluation reward: 172.4\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 865: Policy loss: -0.226258. Value loss: 21.477819. Entropy: 0.916760.\n",
      "Iteration 866: Policy loss: -0.314949. Value loss: 14.340856. Entropy: 0.908061.\n",
      "Iteration 867: Policy loss: -0.300490. Value loss: 11.718464. Entropy: 0.886003.\n",
      "episode: 382   score: 180.0  epsilon: 1.0    steps: 287  evaluation reward: 171.25\n",
      "episode: 383   score: 210.0  epsilon: 1.0    steps: 700  evaluation reward: 171.9\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 868: Policy loss: -0.608370. Value loss: 10.291459. Entropy: 0.807799.\n",
      "Iteration 869: Policy loss: -0.516224. Value loss: 7.309935. Entropy: 0.807938.\n",
      "Iteration 870: Policy loss: -0.553536. Value loss: 6.813879. Entropy: 0.801177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 384   score: 210.0  epsilon: 1.0    steps: 431  evaluation reward: 169.6\n",
      "episode: 385   score: 210.0  epsilon: 1.0    steps: 931  evaluation reward: 169.8\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 871: Policy loss: -1.025433. Value loss: 16.493488. Entropy: 0.737045.\n",
      "Iteration 872: Policy loss: -1.038777. Value loss: 13.030171. Entropy: 0.726725.\n",
      "Iteration 873: Policy loss: -1.063892. Value loss: 13.269789. Entropy: 0.757343.\n",
      "episode: 386   score: 210.0  epsilon: 1.0    steps: 521  evaluation reward: 171.35\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 874: Policy loss: 0.042608. Value loss: 10.231205. Entropy: 0.778889.\n",
      "Iteration 875: Policy loss: 0.126452. Value loss: 7.422169. Entropy: 0.780808.\n",
      "Iteration 876: Policy loss: 0.103560. Value loss: 6.176163. Entropy: 0.793503.\n",
      "episode: 387   score: 180.0  epsilon: 1.0    steps: 798  evaluation reward: 171.7\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 877: Policy loss: 0.431102. Value loss: 14.234047. Entropy: 0.858442.\n",
      "Iteration 878: Policy loss: 0.436013. Value loss: 11.735085. Entropy: 0.863399.\n",
      "Iteration 879: Policy loss: 0.595576. Value loss: 9.279596. Entropy: 0.876309.\n",
      "episode: 388   score: 180.0  epsilon: 1.0    steps: 147  evaluation reward: 172.35\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 880: Policy loss: 0.406954. Value loss: 8.882928. Entropy: 1.027020.\n",
      "Iteration 881: Policy loss: 0.569591. Value loss: 4.138205. Entropy: 1.010574.\n",
      "Iteration 882: Policy loss: 0.483584. Value loss: 6.491698. Entropy: 1.033641.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 883: Policy loss: 0.231179. Value loss: 9.575535. Entropy: 0.958482.\n",
      "Iteration 884: Policy loss: 0.257945. Value loss: 5.659067. Entropy: 0.979817.\n",
      "Iteration 885: Policy loss: 0.231545. Value loss: 5.135931. Entropy: 0.965214.\n",
      "episode: 389   score: 180.0  epsilon: 1.0    steps: 95  evaluation reward: 173.35\n",
      "episode: 390   score: 180.0  epsilon: 1.0    steps: 334  evaluation reward: 173.65\n",
      "episode: 391   score: 180.0  epsilon: 1.0    steps: 742  evaluation reward: 174.2\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 886: Policy loss: 0.820655. Value loss: 13.385676. Entropy: 0.955821.\n",
      "Iteration 887: Policy loss: 0.825361. Value loss: 11.180897. Entropy: 0.967756.\n",
      "Iteration 888: Policy loss: 0.848993. Value loss: 8.862270. Entropy: 0.974856.\n",
      "episode: 392   score: 180.0  epsilon: 1.0    steps: 482  evaluation reward: 173.65\n",
      "episode: 393   score: 210.0  epsilon: 1.0    steps: 980  evaluation reward: 175.6\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 889: Policy loss: -0.129339. Value loss: 19.423985. Entropy: 1.034867.\n",
      "Iteration 890: Policy loss: -0.209359. Value loss: 15.532789. Entropy: 1.032137.\n",
      "Iteration 891: Policy loss: -0.086123. Value loss: 15.584935. Entropy: 1.028373.\n",
      "episode: 394   score: 210.0  epsilon: 1.0    steps: 572  evaluation reward: 176.95\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 892: Policy loss: -0.506031. Value loss: 225.428879. Entropy: 0.868907.\n",
      "Iteration 893: Policy loss: -0.101129. Value loss: 144.003693. Entropy: 0.834807.\n",
      "Iteration 894: Policy loss: -0.525113. Value loss: 64.821754. Entropy: 0.882319.\n",
      "episode: 395   score: 50.0  epsilon: 1.0    steps: 100  evaluation reward: 176.1\n",
      "episode: 396   score: 105.0  epsilon: 1.0    steps: 166  evaluation reward: 175.25\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 895: Policy loss: 0.472336. Value loss: 29.181749. Entropy: 0.958628.\n",
      "Iteration 896: Policy loss: 0.531674. Value loss: 22.188122. Entropy: 1.024317.\n",
      "Iteration 897: Policy loss: 0.326327. Value loss: 21.562128. Entropy: 0.991610.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 898: Policy loss: -0.590445. Value loss: 18.856382. Entropy: 1.027180.\n",
      "Iteration 899: Policy loss: -0.432198. Value loss: 14.580244. Entropy: 1.019315.\n",
      "Iteration 900: Policy loss: -0.509624. Value loss: 12.461237. Entropy: 1.017427.\n",
      "episode: 397   score: 445.0  epsilon: 1.0    steps: 873  evaluation reward: 177.35\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 901: Policy loss: -1.916450. Value loss: 164.894348. Entropy: 0.980903.\n",
      "Iteration 902: Policy loss: -1.702441. Value loss: 154.689178. Entropy: 1.001826.\n",
      "Iteration 903: Policy loss: -2.117693. Value loss: 172.893356. Entropy: 0.975571.\n",
      "episode: 398   score: 105.0  epsilon: 1.0    steps: 1000  evaluation reward: 176.3\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 904: Policy loss: -1.264443. Value loss: 311.811615. Entropy: 0.954782.\n",
      "Iteration 905: Policy loss: -0.035763. Value loss: 174.258881. Entropy: 0.895390.\n",
      "Iteration 906: Policy loss: -1.457293. Value loss: 282.462891. Entropy: 0.917047.\n",
      "episode: 399   score: 180.0  epsilon: 1.0    steps: 257  evaluation reward: 176.3\n",
      "episode: 400   score: 180.0  epsilon: 1.0    steps: 675  evaluation reward: 177.05\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 907: Policy loss: -1.187416. Value loss: 36.649731. Entropy: 0.877079.\n",
      "Iteration 908: Policy loss: -1.292780. Value loss: 27.411678. Entropy: 0.885291.\n",
      "Iteration 909: Policy loss: -1.159179. Value loss: 22.746696. Entropy: 0.883966.\n",
      "now time :  2019-02-26 12:44:24.470206\n",
      "episode: 401   score: 80.0  epsilon: 1.0    steps: 881  evaluation reward: 172.95\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 910: Policy loss: -1.858051. Value loss: 33.848988. Entropy: 1.025022.\n",
      "Iteration 911: Policy loss: -1.976583. Value loss: 26.834911. Entropy: 1.021499.\n",
      "Iteration 912: Policy loss: -1.884340. Value loss: 23.801012. Entropy: 1.032120.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 913: Policy loss: -1.977194. Value loss: 28.577091. Entropy: 1.030754.\n",
      "Iteration 914: Policy loss: -1.922426. Value loss: 18.675215. Entropy: 1.018607.\n",
      "Iteration 915: Policy loss: -1.900023. Value loss: 15.795447. Entropy: 1.024533.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 916: Policy loss: 2.404243. Value loss: 66.690636. Entropy: 1.032570.\n",
      "Iteration 917: Policy loss: 2.539506. Value loss: 25.773388. Entropy: 1.048320.\n",
      "Iteration 918: Policy loss: 2.338225. Value loss: 18.868322. Entropy: 1.055101.\n",
      "episode: 402   score: 240.0  epsilon: 1.0    steps: 83  evaluation reward: 174.65\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 919: Policy loss: 1.482714. Value loss: 17.323332. Entropy: 1.148931.\n",
      "Iteration 920: Policy loss: 1.663026. Value loss: 10.382455. Entropy: 1.131936.\n",
      "Iteration 921: Policy loss: 1.562316. Value loss: 8.103966. Entropy: 1.121327.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 922: Policy loss: -1.106208. Value loss: 28.467707. Entropy: 1.103514.\n",
      "Iteration 923: Policy loss: -0.644246. Value loss: 20.351246. Entropy: 1.065302.\n",
      "Iteration 924: Policy loss: -0.804338. Value loss: 18.011377. Entropy: 1.076128.\n",
      "episode: 403   score: 180.0  epsilon: 1.0    steps: 291  evaluation reward: 174.6\n",
      "episode: 404   score: 565.0  epsilon: 1.0    steps: 385  evaluation reward: 178.4\n",
      "episode: 405   score: 290.0  epsilon: 1.0    steps: 541  evaluation reward: 180.55\n",
      "episode: 406   score: 210.0  epsilon: 1.0    steps: 759  evaluation reward: 182.05\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 925: Policy loss: -0.598569. Value loss: 26.127577. Entropy: 0.945365.\n",
      "Iteration 926: Policy loss: -1.190232. Value loss: 20.513668. Entropy: 0.904854.\n",
      "Iteration 927: Policy loss: -1.053983. Value loss: 15.934382. Entropy: 0.889810.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 928: Policy loss: 0.812401. Value loss: 20.424362. Entropy: 0.904126.\n",
      "Iteration 929: Policy loss: 0.800330. Value loss: 16.204311. Entropy: 0.924690.\n",
      "Iteration 930: Policy loss: 0.802980. Value loss: 15.910854. Entropy: 0.933019.\n",
      "episode: 407   score: 610.0  epsilon: 1.0    steps: 138  evaluation reward: 185.65\n",
      "episode: 408   score: 155.0  epsilon: 1.0    steps: 791  evaluation reward: 184.85\n",
      "episode: 409   score: 245.0  epsilon: 1.0    steps: 941  evaluation reward: 186.55\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 931: Policy loss: -0.629640. Value loss: 10.790597. Entropy: 0.955337.\n",
      "Iteration 932: Policy loss: -0.544461. Value loss: 9.089095. Entropy: 0.937656.\n",
      "Iteration 933: Policy loss: -0.722620. Value loss: 8.736708. Entropy: 0.952534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 934: Policy loss: -0.033432. Value loss: 11.683496. Entropy: 1.020974.\n",
      "Iteration 935: Policy loss: 0.086071. Value loss: 10.623855. Entropy: 1.025875.\n",
      "Iteration 936: Policy loss: -0.132412. Value loss: 12.130360. Entropy: 1.028537.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 937: Policy loss: 0.005007. Value loss: 9.300933. Entropy: 1.070382.\n",
      "Iteration 938: Policy loss: -0.081080. Value loss: 8.099711. Entropy: 1.080618.\n",
      "Iteration 939: Policy loss: -0.051248. Value loss: 5.914804. Entropy: 1.074592.\n",
      "episode: 410   score: 210.0  epsilon: 1.0    steps: 20  evaluation reward: 185.3\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 940: Policy loss: 0.314940. Value loss: 5.438137. Entropy: 0.989615.\n",
      "Iteration 941: Policy loss: 0.314456. Value loss: 4.625469. Entropy: 0.983555.\n",
      "Iteration 942: Policy loss: 0.534730. Value loss: 4.194338. Entropy: 0.994154.\n",
      "episode: 411   score: 210.0  epsilon: 1.0    steps: 342  evaluation reward: 185.85\n",
      "episode: 412   score: 210.0  epsilon: 1.0    steps: 448  evaluation reward: 186.9\n",
      "episode: 413   score: 180.0  epsilon: 1.0    steps: 590  evaluation reward: 187.6\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 943: Policy loss: -0.441429. Value loss: 16.115206. Entropy: 0.910115.\n",
      "Iteration 944: Policy loss: -0.329608. Value loss: 13.930762. Entropy: 0.906639.\n",
      "Iteration 945: Policy loss: -0.335470. Value loss: 13.535668. Entropy: 0.910218.\n",
      "episode: 414   score: 180.0  epsilon: 1.0    steps: 669  evaluation reward: 186.3\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 946: Policy loss: -0.482636. Value loss: 9.109586. Entropy: 0.867704.\n",
      "Iteration 947: Policy loss: -0.482421. Value loss: 7.518703. Entropy: 0.871372.\n",
      "Iteration 948: Policy loss: -0.531333. Value loss: 7.669862. Entropy: 0.866010.\n",
      "episode: 415   score: 180.0  epsilon: 1.0    steps: 176  evaluation reward: 187.05\n",
      "episode: 416   score: 210.0  epsilon: 1.0    steps: 842  evaluation reward: 188.05\n",
      "episode: 417   score: 155.0  epsilon: 1.0    steps: 980  evaluation reward: 188.45\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 949: Policy loss: 0.363510. Value loss: 19.887657. Entropy: 0.859768.\n",
      "Iteration 950: Policy loss: 0.450540. Value loss: 18.157688. Entropy: 0.869316.\n",
      "Iteration 951: Policy loss: 0.310279. Value loss: 16.123558. Entropy: 0.849946.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 952: Policy loss: 0.357305. Value loss: 5.852779. Entropy: 0.980657.\n",
      "Iteration 953: Policy loss: 0.382609. Value loss: 4.681915. Entropy: 0.973784.\n",
      "Iteration 954: Policy loss: 0.315717. Value loss: 4.100082. Entropy: 0.967228.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 955: Policy loss: -0.097975. Value loss: 10.061312. Entropy: 1.028528.\n",
      "Iteration 956: Policy loss: -0.259237. Value loss: 10.636663. Entropy: 1.023933.\n",
      "Iteration 957: Policy loss: -0.111800. Value loss: 9.094828. Entropy: 1.046349.\n",
      "episode: 418   score: 180.0  epsilon: 1.0    steps: 58  evaluation reward: 189.15\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 958: Policy loss: -0.090754. Value loss: 3.409635. Entropy: 1.057991.\n",
      "Iteration 959: Policy loss: -0.035990. Value loss: 2.672035. Entropy: 1.060915.\n",
      "Iteration 960: Policy loss: -0.095681. Value loss: 2.934160. Entropy: 1.052851.\n",
      "episode: 419   score: 180.0  epsilon: 1.0    steps: 380  evaluation reward: 189.9\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 961: Policy loss: 0.118682. Value loss: 16.608864. Entropy: 0.946407.\n",
      "Iteration 962: Policy loss: 0.204319. Value loss: 11.833348. Entropy: 0.950661.\n",
      "Iteration 963: Policy loss: 0.155386. Value loss: 12.256205. Entropy: 0.940614.\n",
      "episode: 420   score: 210.0  epsilon: 1.0    steps: 437  evaluation reward: 190.15\n",
      "episode: 421   score: 180.0  epsilon: 1.0    steps: 513  evaluation reward: 190.85\n",
      "episode: 422   score: 210.0  epsilon: 1.0    steps: 732  evaluation reward: 189.9\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 964: Policy loss: 0.790120. Value loss: 10.282768. Entropy: 0.833156.\n",
      "Iteration 965: Policy loss: 0.790495. Value loss: 8.072134. Entropy: 0.833091.\n",
      "Iteration 966: Policy loss: 0.893078. Value loss: 7.561761. Entropy: 0.844424.\n",
      "episode: 423   score: 155.0  epsilon: 1.0    steps: 225  evaluation reward: 190.4\n",
      "episode: 424   score: 180.0  epsilon: 1.0    steps: 1018  evaluation reward: 191.1\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 967: Policy loss: 0.703438. Value loss: 22.538349. Entropy: 0.849250.\n",
      "Iteration 968: Policy loss: 0.875183. Value loss: 18.587543. Entropy: 0.880087.\n",
      "Iteration 969: Policy loss: 0.844071. Value loss: 16.277088. Entropy: 0.974871.\n",
      "episode: 425   score: 210.0  epsilon: 1.0    steps: 831  evaluation reward: 192.1\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 970: Policy loss: -0.143203. Value loss: 15.755231. Entropy: 0.945574.\n",
      "Iteration 971: Policy loss: -0.272792. Value loss: 13.330687. Entropy: 0.971182.\n",
      "Iteration 972: Policy loss: -0.200316. Value loss: 13.730711. Entropy: 0.938038.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 973: Policy loss: 0.784896. Value loss: 9.444145. Entropy: 1.157815.\n",
      "Iteration 974: Policy loss: 0.641911. Value loss: 6.890187. Entropy: 1.197041.\n",
      "Iteration 975: Policy loss: 0.716173. Value loss: 6.480367. Entropy: 1.166161.\n",
      "episode: 426   score: 210.0  epsilon: 1.0    steps: 109  evaluation reward: 193.1\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 976: Policy loss: 1.241642. Value loss: 13.940996. Entropy: 1.110235.\n",
      "Iteration 977: Policy loss: 1.117060. Value loss: 8.173258. Entropy: 1.099266.\n",
      "Iteration 978: Policy loss: 1.124808. Value loss: 7.561870. Entropy: 1.146944.\n",
      "episode: 427   score: 105.0  epsilon: 1.0    steps: 456  evaluation reward: 193.05\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 979: Policy loss: 0.436234. Value loss: 7.836667. Entropy: 1.080445.\n",
      "Iteration 980: Policy loss: 0.586730. Value loss: 5.946124. Entropy: 1.112059.\n",
      "Iteration 981: Policy loss: 0.434965. Value loss: 6.022139. Entropy: 1.143119.\n",
      "episode: 428   score: 210.0  epsilon: 1.0    steps: 315  evaluation reward: 194.1\n",
      "episode: 429   score: 165.0  epsilon: 1.0    steps: 534  evaluation reward: 194.3\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 982: Policy loss: 0.934638. Value loss: 9.559263. Entropy: 1.043113.\n",
      "Iteration 983: Policy loss: 0.816540. Value loss: 8.946504. Entropy: 1.021973.\n",
      "Iteration 984: Policy loss: 0.972663. Value loss: 8.418843. Entropy: 1.012494.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 985: Policy loss: 1.464967. Value loss: 18.254974. Entropy: 1.171347.\n",
      "Iteration 986: Policy loss: 1.118973. Value loss: 12.283619. Entropy: 1.127026.\n",
      "Iteration 987: Policy loss: 1.520230. Value loss: 9.954394. Entropy: 1.159094.\n",
      "episode: 430   score: 155.0  epsilon: 1.0    steps: 146  evaluation reward: 194.8\n",
      "episode: 431   score: 120.0  epsilon: 1.0    steps: 851  evaluation reward: 194.85\n",
      "episode: 432   score: 135.0  epsilon: 1.0    steps: 911  evaluation reward: 193.65\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 988: Policy loss: 1.125976. Value loss: 11.878947. Entropy: 0.990898.\n",
      "Iteration 989: Policy loss: 1.150159. Value loss: 8.379086. Entropy: 0.969277.\n",
      "Iteration 990: Policy loss: 1.106238. Value loss: 6.354464. Entropy: 0.999482.\n",
      "episode: 433   score: 50.0  epsilon: 1.0    steps: 308  evaluation reward: 193.4\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 991: Policy loss: 0.137330. Value loss: 21.959024. Entropy: 1.135157.\n",
      "Iteration 992: Policy loss: 0.265322. Value loss: 13.937312. Entropy: 1.145272.\n",
      "Iteration 993: Policy loss: 0.305347. Value loss: 12.660582. Entropy: 1.152643.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 994: Policy loss: -2.260240. Value loss: 24.877754. Entropy: 1.201239.\n",
      "Iteration 995: Policy loss: -2.219332. Value loss: 18.348593. Entropy: 1.209403.\n",
      "Iteration 996: Policy loss: -2.078663. Value loss: 15.944651. Entropy: 1.196881.\n",
      "episode: 434   score: 285.0  epsilon: 1.0    steps: 691  evaluation reward: 195.2\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 997: Policy loss: -0.685399. Value loss: 17.758713. Entropy: 1.182899.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 998: Policy loss: -0.614410. Value loss: 11.697846. Entropy: 1.176036.\n",
      "Iteration 999: Policy loss: -0.629233. Value loss: 11.170350. Entropy: 1.198759.\n",
      "episode: 435   score: 210.0  epsilon: 1.0    steps: 387  evaluation reward: 195.9\n",
      "episode: 436   score: 210.0  epsilon: 1.0    steps: 609  evaluation reward: 197.25\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 1000: Policy loss: -0.163238. Value loss: 15.001289. Entropy: 1.122367.\n",
      "Iteration 1001: Policy loss: -0.230630. Value loss: 9.560289. Entropy: 1.131785.\n",
      "Iteration 1002: Policy loss: -0.263302. Value loss: 8.473472. Entropy: 1.133854.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1003: Policy loss: 1.808013. Value loss: 13.695273. Entropy: 1.191929.\n",
      "Iteration 1004: Policy loss: 1.770518. Value loss: 9.736426. Entropy: 1.175255.\n",
      "Iteration 1005: Policy loss: 1.933730. Value loss: 8.373353. Entropy: 1.203704.\n",
      "episode: 437   score: 120.0  epsilon: 1.0    steps: 173  evaluation reward: 197.35\n",
      "episode: 438   score: 185.0  epsilon: 1.0    steps: 893  evaluation reward: 197.7\n",
      "episode: 439   score: 155.0  epsilon: 1.0    steps: 956  evaluation reward: 198.2\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1006: Policy loss: 0.775211. Value loss: 16.356359. Entropy: 1.169316.\n",
      "Iteration 1007: Policy loss: 0.791302. Value loss: 11.160149. Entropy: 1.136678.\n",
      "Iteration 1008: Policy loss: 0.792821. Value loss: 10.343935. Entropy: 1.151503.\n",
      "episode: 440   score: 155.0  epsilon: 1.0    steps: 356  evaluation reward: 197.95\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1009: Policy loss: -0.543928. Value loss: 20.483976. Entropy: 1.121096.\n",
      "Iteration 1010: Policy loss: -0.439561. Value loss: 12.991422. Entropy: 1.071450.\n",
      "Iteration 1011: Policy loss: -0.602779. Value loss: 10.663110. Entropy: 1.117773.\n",
      "episode: 441   score: 370.0  epsilon: 1.0    steps: 44  evaluation reward: 198.45\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1012: Policy loss: -0.393024. Value loss: 23.578588. Entropy: 1.072713.\n",
      "Iteration 1013: Policy loss: -0.123512. Value loss: 16.099245. Entropy: 1.072377.\n",
      "Iteration 1014: Policy loss: -0.329621. Value loss: 13.959955. Entropy: 1.066819.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1015: Policy loss: 1.876722. Value loss: 13.776368. Entropy: 1.146912.\n",
      "Iteration 1016: Policy loss: 1.693345. Value loss: 9.947819. Entropy: 1.170066.\n",
      "Iteration 1017: Policy loss: 1.551818. Value loss: 7.723700. Entropy: 1.165782.\n",
      "episode: 442   score: 120.0  epsilon: 1.0    steps: 412  evaluation reward: 198.05\n",
      "episode: 443   score: 240.0  epsilon: 1.0    steps: 752  evaluation reward: 199.35\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1018: Policy loss: -0.575181. Value loss: 267.385376. Entropy: 1.095886.\n",
      "Iteration 1019: Policy loss: 0.538526. Value loss: 201.125061. Entropy: 0.983378.\n",
      "Iteration 1020: Policy loss: -0.464893. Value loss: 198.994583. Entropy: 1.054372.\n",
      "episode: 444   score: 75.0  epsilon: 1.0    steps: 65  evaluation reward: 196.4\n",
      "episode: 445   score: 180.0  epsilon: 1.0    steps: 604  evaluation reward: 196.8\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1021: Policy loss: 1.982194. Value loss: 16.100199. Entropy: 0.888532.\n",
      "Iteration 1022: Policy loss: 1.955528. Value loss: 10.702715. Entropy: 0.905607.\n",
      "Iteration 1023: Policy loss: 2.102082. Value loss: 8.607772. Entropy: 0.881632.\n",
      "episode: 446   score: 155.0  epsilon: 1.0    steps: 202  evaluation reward: 196.55\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1024: Policy loss: 1.039765. Value loss: 24.745754. Entropy: 0.970861.\n",
      "Iteration 1025: Policy loss: 0.822459. Value loss: 18.401754. Entropy: 0.971740.\n",
      "Iteration 1026: Policy loss: 0.973802. Value loss: 14.928458. Entropy: 0.955179.\n",
      "episode: 447   score: 155.0  epsilon: 1.0    steps: 840  evaluation reward: 193.5\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1027: Policy loss: 0.475325. Value loss: 18.476288. Entropy: 0.911419.\n",
      "Iteration 1028: Policy loss: 0.769592. Value loss: 10.818943. Entropy: 0.928748.\n",
      "Iteration 1029: Policy loss: 0.349008. Value loss: 10.472106. Entropy: 0.939742.\n",
      "episode: 448   score: 205.0  epsilon: 1.0    steps: 343  evaluation reward: 193.75\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1030: Policy loss: 0.634551. Value loss: 12.633333. Entropy: 1.014082.\n",
      "Iteration 1031: Policy loss: 0.845373. Value loss: 9.951245. Entropy: 1.003279.\n",
      "Iteration 1032: Policy loss: 0.850893. Value loss: 9.041987. Entropy: 1.004483.\n",
      "episode: 449   score: 105.0  epsilon: 1.0    steps: 766  evaluation reward: 193.25\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1033: Policy loss: -0.044308. Value loss: 17.196686. Entropy: 1.013388.\n",
      "Iteration 1034: Policy loss: -0.115187. Value loss: 13.851733. Entropy: 0.990920.\n",
      "Iteration 1035: Policy loss: 0.038763. Value loss: 11.558676. Entropy: 0.977357.\n",
      "episode: 450   score: 210.0  epsilon: 1.0    steps: 479  evaluation reward: 193.55\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1036: Policy loss: 0.601422. Value loss: 15.905592. Entropy: 0.946171.\n",
      "Iteration 1037: Policy loss: 0.688975. Value loss: 9.274418. Entropy: 0.953412.\n",
      "Iteration 1038: Policy loss: 0.394278. Value loss: 9.454449. Entropy: 0.957755.\n",
      "now time :  2019-02-26 12:46:52.046316\n",
      "episode: 451   score: 155.0  epsilon: 1.0    steps: 109  evaluation reward: 193.25\n",
      "episode: 452   score: 105.0  epsilon: 1.0    steps: 513  evaluation reward: 192.65\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1039: Policy loss: -0.613130. Value loss: 12.998742. Entropy: 0.903113.\n",
      "Iteration 1040: Policy loss: -0.661749. Value loss: 9.397043. Entropy: 0.915062.\n",
      "Iteration 1041: Policy loss: -0.637920. Value loss: 7.828768. Entropy: 0.882275.\n",
      "episode: 453   score: 155.0  epsilon: 1.0    steps: 237  evaluation reward: 192.1\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1042: Policy loss: 0.383024. Value loss: 16.053431. Entropy: 0.965891.\n",
      "Iteration 1043: Policy loss: 0.717056. Value loss: 12.065367. Entropy: 0.993169.\n",
      "Iteration 1044: Policy loss: 0.679439. Value loss: 10.727498. Entropy: 0.976156.\n",
      "episode: 454   score: 210.0  epsilon: 1.0    steps: 893  evaluation reward: 192.4\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1045: Policy loss: -0.667300. Value loss: 10.484401. Entropy: 0.992189.\n",
      "Iteration 1046: Policy loss: -0.753502. Value loss: 6.760651. Entropy: 1.005955.\n",
      "Iteration 1047: Policy loss: -0.720414. Value loss: 6.957665. Entropy: 1.001944.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1048: Policy loss: -0.527934. Value loss: 14.200175. Entropy: 0.945062.\n",
      "Iteration 1049: Policy loss: -0.511940. Value loss: 10.431562. Entropy: 0.905271.\n",
      "Iteration 1050: Policy loss: -0.518141. Value loss: 9.129424. Entropy: 0.898712.\n",
      "episode: 455   score: 210.0  epsilon: 1.0    steps: 298  evaluation reward: 193.45\n",
      "episode: 456   score: 485.0  epsilon: 1.0    steps: 917  evaluation reward: 196.2\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1051: Policy loss: -0.842082. Value loss: 11.986654. Entropy: 0.885558.\n",
      "Iteration 1052: Policy loss: -0.984856. Value loss: 9.892685. Entropy: 0.901113.\n",
      "Iteration 1053: Policy loss: -0.803874. Value loss: 8.106347. Entropy: 0.909693.\n",
      "episode: 457   score: 155.0  epsilon: 1.0    steps: 507  evaluation reward: 195.95\n",
      "episode: 458   score: 155.0  epsilon: 1.0    steps: 676  evaluation reward: 195.4\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1054: Policy loss: 0.343277. Value loss: 10.440886. Entropy: 1.030562.\n",
      "Iteration 1055: Policy loss: 0.506840. Value loss: 7.423352. Entropy: 1.030317.\n",
      "Iteration 1056: Policy loss: 0.195156. Value loss: 6.620271. Entropy: 1.102109.\n",
      "episode: 459   score: 165.0  epsilon: 1.0    steps: 534  evaluation reward: 195.25\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1057: Policy loss: -3.173423. Value loss: 186.102951. Entropy: 0.876632.\n",
      "Iteration 1058: Policy loss: -3.124415. Value loss: 139.475494. Entropy: 0.824367.\n",
      "Iteration 1059: Policy loss: -3.135042. Value loss: 110.208405. Entropy: 0.801797.\n",
      "episode: 460   score: 180.0  epsilon: 1.0    steps: 32  evaluation reward: 192.1\n",
      "Training network. lr: 0.000242. clip: 0.096784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1060: Policy loss: 1.032392. Value loss: 10.998137. Entropy: 0.915581.\n",
      "Iteration 1061: Policy loss: 0.888903. Value loss: 7.338737. Entropy: 0.919114.\n",
      "Iteration 1062: Policy loss: 0.916811. Value loss: 6.500465. Entropy: 0.910907.\n",
      "episode: 461   score: 155.0  epsilon: 1.0    steps: 142  evaluation reward: 191.85\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1063: Policy loss: 0.751853. Value loss: 20.129177. Entropy: 0.778445.\n",
      "Iteration 1064: Policy loss: 0.590345. Value loss: 15.078423. Entropy: 0.776416.\n",
      "Iteration 1065: Policy loss: 0.569882. Value loss: 12.831829. Entropy: 0.793290.\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1066: Policy loss: 1.174552. Value loss: 9.694540. Entropy: 0.925325.\n",
      "Iteration 1067: Policy loss: 1.158807. Value loss: 6.870113. Entropy: 0.970062.\n",
      "Iteration 1068: Policy loss: 1.277643. Value loss: 6.561716. Entropy: 0.946955.\n",
      "episode: 462   score: 180.0  epsilon: 1.0    steps: 364  evaluation reward: 191.85\n",
      "episode: 463   score: 155.0  epsilon: 1.0    steps: 958  evaluation reward: 191.3\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1069: Policy loss: -0.317431. Value loss: 16.530554. Entropy: 0.882095.\n",
      "Iteration 1070: Policy loss: -0.313042. Value loss: 13.118721. Entropy: 0.901519.\n",
      "Iteration 1071: Policy loss: -0.282546. Value loss: 11.319460. Entropy: 0.877566.\n",
      "episode: 464   score: 210.0  epsilon: 1.0    steps: 737  evaluation reward: 191.3\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1072: Policy loss: -0.417687. Value loss: 13.833962. Entropy: 0.911944.\n",
      "Iteration 1073: Policy loss: -0.345885. Value loss: 9.880668. Entropy: 0.923149.\n",
      "Iteration 1074: Policy loss: -0.333381. Value loss: 10.147929. Entropy: 0.933707.\n",
      "episode: 465   score: 155.0  epsilon: 1.0    steps: 410  evaluation reward: 191.05\n",
      "episode: 466   score: 180.0  epsilon: 1.0    steps: 591  evaluation reward: 190.75\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1075: Policy loss: 0.721088. Value loss: 20.414371. Entropy: 0.876611.\n",
      "Iteration 1076: Policy loss: 0.438348. Value loss: 13.017747. Entropy: 0.892127.\n",
      "Iteration 1077: Policy loss: 0.605194. Value loss: 12.392876. Entropy: 0.906427.\n",
      "episode: 467   score: 135.0  epsilon: 1.0    steps: 51  evaluation reward: 190.75\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1078: Policy loss: 0.847043. Value loss: 11.238588. Entropy: 0.885782.\n",
      "Iteration 1079: Policy loss: 0.938712. Value loss: 8.309167. Entropy: 0.939301.\n",
      "Iteration 1080: Policy loss: 0.709175. Value loss: 8.150753. Entropy: 0.898108.\n",
      "episode: 468   score: 165.0  epsilon: 1.0    steps: 176  evaluation reward: 190.6\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1081: Policy loss: -0.175767. Value loss: 9.266849. Entropy: 0.952579.\n",
      "Iteration 1082: Policy loss: -0.219451. Value loss: 7.133745. Entropy: 0.949158.\n",
      "Iteration 1083: Policy loss: -0.234988. Value loss: 7.295945. Entropy: 0.959757.\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1084: Policy loss: -3.917743. Value loss: 246.543213. Entropy: 0.961269.\n",
      "Iteration 1085: Policy loss: -3.679278. Value loss: 148.173889. Entropy: 0.928115.\n",
      "Iteration 1086: Policy loss: -3.654395. Value loss: 109.820595. Entropy: 0.906170.\n",
      "episode: 469   score: 180.0  epsilon: 1.0    steps: 1015  evaluation reward: 190.6\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1087: Policy loss: -1.928473. Value loss: 366.227081. Entropy: 0.817942.\n",
      "Iteration 1088: Policy loss: -2.375631. Value loss: 205.585266. Entropy: 0.785043.\n",
      "Iteration 1089: Policy loss: -2.249680. Value loss: 193.441315. Entropy: 0.764215.\n",
      "episode: 470   score: 325.0  epsilon: 1.0    steps: 261  evaluation reward: 191.75\n",
      "episode: 471   score: 310.0  epsilon: 1.0    steps: 765  evaluation reward: 193.05\n",
      "episode: 472   score: 660.0  epsilon: 1.0    steps: 881  evaluation reward: 197.85\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1090: Policy loss: 3.985102. Value loss: 63.292511. Entropy: 0.429450.\n",
      "Iteration 1091: Policy loss: 3.382898. Value loss: 27.469311. Entropy: 0.430482.\n",
      "Iteration 1092: Policy loss: 3.368519. Value loss: 18.811459. Entropy: 0.477174.\n",
      "episode: 473   score: 160.0  epsilon: 1.0    steps: 455  evaluation reward: 198.4\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1093: Policy loss: 3.816375. Value loss: 35.970043. Entropy: 0.512579.\n",
      "Iteration 1094: Policy loss: 3.632272. Value loss: 21.408220. Entropy: 0.481797.\n",
      "Iteration 1095: Policy loss: 3.812195. Value loss: 18.395658. Entropy: 0.508650.\n",
      "episode: 474   score: 135.0  epsilon: 1.0    steps: 68  evaluation reward: 197.95\n",
      "episode: 475   score: 155.0  epsilon: 1.0    steps: 576  evaluation reward: 197.7\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1096: Policy loss: 4.054836. Value loss: 33.672863. Entropy: 0.609942.\n",
      "Iteration 1097: Policy loss: 4.026467. Value loss: 20.183969. Entropy: 0.602800.\n",
      "Iteration 1098: Policy loss: 3.907238. Value loss: 16.437769. Entropy: 0.585654.\n",
      "episode: 476   score: 135.0  epsilon: 1.0    steps: 209  evaluation reward: 196.95\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1099: Policy loss: 1.875250. Value loss: 15.010004. Entropy: 0.760507.\n",
      "Iteration 1100: Policy loss: 1.858304. Value loss: 11.017419. Entropy: 0.775053.\n",
      "Iteration 1101: Policy loss: 1.969144. Value loss: 9.090547. Entropy: 0.828523.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1102: Policy loss: -0.582055. Value loss: 22.023315. Entropy: 0.920811.\n",
      "Iteration 1103: Policy loss: -0.617823. Value loss: 16.745564. Entropy: 0.887633.\n",
      "Iteration 1104: Policy loss: -0.409215. Value loss: 13.981839. Entropy: 0.911926.\n",
      "episode: 477   score: 50.0  epsilon: 1.0    steps: 93  evaluation reward: 195.35\n",
      "episode: 478   score: 105.0  epsilon: 1.0    steps: 283  evaluation reward: 195.35\n",
      "episode: 479   score: 105.0  epsilon: 1.0    steps: 906  evaluation reward: 194.3\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1105: Policy loss: 0.445725. Value loss: 13.904569. Entropy: 0.797374.\n",
      "Iteration 1106: Policy loss: 0.611806. Value loss: 8.609421. Entropy: 0.784054.\n",
      "Iteration 1107: Policy loss: 0.490059. Value loss: 9.900814. Entropy: 0.773484.\n",
      "episode: 480   score: 105.0  epsilon: 1.0    steps: 482  evaluation reward: 194.3\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1108: Policy loss: 0.941531. Value loss: 18.370026. Entropy: 0.951602.\n",
      "Iteration 1109: Policy loss: 0.942530. Value loss: 10.473918. Entropy: 0.950623.\n",
      "Iteration 1110: Policy loss: 1.017840. Value loss: 9.611865. Entropy: 0.964098.\n",
      "episode: 481   score: 140.0  epsilon: 1.0    steps: 662  evaluation reward: 193.6\n",
      "episode: 482   score: 180.0  epsilon: 1.0    steps: 786  evaluation reward: 193.6\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1111: Policy loss: -0.576703. Value loss: 16.607800. Entropy: 0.804444.\n",
      "Iteration 1112: Policy loss: -0.576252. Value loss: 14.498670. Entropy: 0.776886.\n",
      "Iteration 1113: Policy loss: -0.564722. Value loss: 11.643488. Entropy: 0.725276.\n",
      "episode: 483   score: 155.0  epsilon: 1.0    steps: 619  evaluation reward: 193.05\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1114: Policy loss: -0.155876. Value loss: 17.694048. Entropy: 1.133805.\n",
      "Iteration 1115: Policy loss: -0.091728. Value loss: 12.130904. Entropy: 1.054031.\n",
      "Iteration 1116: Policy loss: -0.260951. Value loss: 11.820503. Entropy: 1.106251.\n",
      "episode: 484   score: 160.0  epsilon: 1.0    steps: 240  evaluation reward: 192.55\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1117: Policy loss: -0.642607. Value loss: 9.546006. Entropy: 1.025006.\n",
      "Iteration 1118: Policy loss: -0.872910. Value loss: 9.257527. Entropy: 1.025665.\n",
      "Iteration 1119: Policy loss: -0.739327. Value loss: 8.193418. Entropy: 1.020718.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1120: Policy loss: 0.142908. Value loss: 8.378085. Entropy: 1.004827.\n",
      "Iteration 1121: Policy loss: 0.143721. Value loss: 7.457138. Entropy: 0.957343.\n",
      "Iteration 1122: Policy loss: 0.086105. Value loss: 5.940406. Entropy: 0.961743.\n",
      "episode: 485   score: 135.0  epsilon: 1.0    steps: 118  evaluation reward: 191.8\n",
      "episode: 486   score: 155.0  epsilon: 1.0    steps: 334  evaluation reward: 191.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 487   score: 160.0  epsilon: 1.0    steps: 928  evaluation reward: 191.05\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1123: Policy loss: 1.247193. Value loss: 10.147388. Entropy: 1.057870.\n",
      "Iteration 1124: Policy loss: 1.257364. Value loss: 7.021204. Entropy: 1.088825.\n",
      "Iteration 1125: Policy loss: 1.203314. Value loss: 6.411729. Entropy: 1.068564.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1126: Policy loss: -2.449863. Value loss: 272.327332. Entropy: 0.852639.\n",
      "Iteration 1127: Policy loss: -2.762070. Value loss: 185.786774. Entropy: 0.830648.\n",
      "Iteration 1128: Policy loss: -2.386631. Value loss: 132.509277. Entropy: 0.868059.\n",
      "episode: 488   score: 210.0  epsilon: 1.0    steps: 424  evaluation reward: 191.35\n",
      "episode: 489   score: 410.0  epsilon: 1.0    steps: 735  evaluation reward: 193.65\n",
      "episode: 490   score: 135.0  epsilon: 1.0    steps: 815  evaluation reward: 193.2\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1129: Policy loss: -0.116010. Value loss: 27.882870. Entropy: 0.992992.\n",
      "Iteration 1130: Policy loss: 0.050306. Value loss: 17.245493. Entropy: 1.019552.\n",
      "Iteration 1131: Policy loss: -0.176912. Value loss: 14.147236. Entropy: 1.019332.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1132: Policy loss: -1.088646. Value loss: 15.492446. Entropy: 1.008830.\n",
      "Iteration 1133: Policy loss: -0.979421. Value loss: 13.959215. Entropy: 1.039001.\n",
      "Iteration 1134: Policy loss: -1.039009. Value loss: 11.942934. Entropy: 0.975691.\n",
      "episode: 491   score: 210.0  epsilon: 1.0    steps: 571  evaluation reward: 193.5\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1135: Policy loss: -1.108934. Value loss: 24.118986. Entropy: 1.085013.\n",
      "Iteration 1136: Policy loss: -1.081775. Value loss: 15.767317. Entropy: 1.124801.\n",
      "Iteration 1137: Policy loss: -0.933922. Value loss: 16.750734. Entropy: 1.136354.\n",
      "episode: 492   score: 210.0  epsilon: 1.0    steps: 195  evaluation reward: 193.8\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1138: Policy loss: 1.374580. Value loss: 12.272151. Entropy: 1.038093.\n",
      "Iteration 1139: Policy loss: 1.440623. Value loss: 7.429847. Entropy: 0.972852.\n",
      "Iteration 1140: Policy loss: 1.436162. Value loss: 6.205587. Entropy: 0.944241.\n",
      "episode: 493   score: 180.0  epsilon: 1.0    steps: 375  evaluation reward: 193.5\n",
      "episode: 494   score: 110.0  epsilon: 1.0    steps: 758  evaluation reward: 192.5\n",
      "episode: 495   score: 135.0  epsilon: 1.0    steps: 954  evaluation reward: 193.35\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1141: Policy loss: -0.119138. Value loss: 19.994087. Entropy: 0.987524.\n",
      "Iteration 1142: Policy loss: -0.220918. Value loss: 15.455101. Entropy: 0.881549.\n",
      "Iteration 1143: Policy loss: -0.192858. Value loss: 14.230790. Entropy: 0.904610.\n",
      "episode: 496   score: 180.0  epsilon: 1.0    steps: 35  evaluation reward: 194.1\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1144: Policy loss: 0.612137. Value loss: 10.450433. Entropy: 0.698299.\n",
      "Iteration 1145: Policy loss: 0.658609. Value loss: 7.254055. Entropy: 0.698935.\n",
      "Iteration 1146: Policy loss: 0.631354. Value loss: 5.894741. Entropy: 0.713113.\n",
      "episode: 497   score: 210.0  epsilon: 1.0    steps: 475  evaluation reward: 191.75\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1147: Policy loss: 0.032837. Value loss: 16.134941. Entropy: 0.918019.\n",
      "Iteration 1148: Policy loss: 0.130461. Value loss: 12.069627. Entropy: 0.913997.\n",
      "Iteration 1149: Policy loss: 0.013792. Value loss: 11.936005. Entropy: 0.909602.\n",
      "episode: 498   score: 180.0  epsilon: 1.0    steps: 808  evaluation reward: 192.5\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1150: Policy loss: -0.135589. Value loss: 12.823836. Entropy: 0.818683.\n",
      "Iteration 1151: Policy loss: -0.145429. Value loss: 9.615377. Entropy: 0.749190.\n",
      "Iteration 1152: Policy loss: -0.098601. Value loss: 10.912226. Entropy: 0.809572.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1153: Policy loss: 0.751094. Value loss: 9.596910. Entropy: 0.994311.\n",
      "Iteration 1154: Policy loss: 0.583106. Value loss: 6.490670. Entropy: 0.888665.\n",
      "Iteration 1155: Policy loss: 0.655018. Value loss: 5.600209. Entropy: 0.904925.\n",
      "episode: 499   score: 160.0  epsilon: 1.0    steps: 218  evaluation reward: 192.3\n",
      "episode: 500   score: 135.0  epsilon: 1.0    steps: 534  evaluation reward: 191.85\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1156: Policy loss: 0.559007. Value loss: 10.023831. Entropy: 0.887621.\n",
      "Iteration 1157: Policy loss: 0.491951. Value loss: 7.958586. Entropy: 0.893807.\n",
      "Iteration 1158: Policy loss: 0.527168. Value loss: 6.244101. Entropy: 0.891212.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1159: Policy loss: -0.486278. Value loss: 13.764480. Entropy: 0.918913.\n",
      "Iteration 1160: Policy loss: -0.576526. Value loss: 10.260149. Entropy: 0.911846.\n",
      "Iteration 1161: Policy loss: -0.283906. Value loss: 8.611852. Entropy: 0.912448.\n",
      "now time :  2019-02-26 12:49:13.137393\n",
      "episode: 501   score: 155.0  epsilon: 1.0    steps: 67  evaluation reward: 192.6\n",
      "episode: 502   score: 155.0  epsilon: 1.0    steps: 289  evaluation reward: 191.75\n",
      "episode: 503   score: 180.0  epsilon: 1.0    steps: 661  evaluation reward: 191.75\n",
      "episode: 504   score: 210.0  epsilon: 1.0    steps: 934  evaluation reward: 188.2\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1162: Policy loss: -0.180894. Value loss: 9.893152. Entropy: 0.862198.\n",
      "Iteration 1163: Policy loss: -0.161812. Value loss: 9.862812. Entropy: 0.837062.\n",
      "Iteration 1164: Policy loss: -0.217722. Value loss: 9.835068. Entropy: 0.870911.\n",
      "episode: 505   score: 155.0  epsilon: 1.0    steps: 499  evaluation reward: 186.85\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1165: Policy loss: 0.199307. Value loss: 17.243835. Entropy: 0.926535.\n",
      "Iteration 1166: Policy loss: 0.100205. Value loss: 15.528084. Entropy: 0.938345.\n",
      "Iteration 1167: Policy loss: 0.382148. Value loss: 13.680546. Entropy: 0.967814.\n",
      "episode: 506   score: 155.0  epsilon: 1.0    steps: 849  evaluation reward: 186.3\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1168: Policy loss: -0.615209. Value loss: 21.518789. Entropy: 0.834592.\n",
      "Iteration 1169: Policy loss: -0.680506. Value loss: 19.209166. Entropy: 0.774238.\n",
      "Iteration 1170: Policy loss: -0.777044. Value loss: 18.368755. Entropy: 0.826984.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1171: Policy loss: -0.112932. Value loss: 5.721878. Entropy: 0.783351.\n",
      "Iteration 1172: Policy loss: -0.184796. Value loss: 3.899231. Entropy: 0.722606.\n",
      "Iteration 1173: Policy loss: -0.111383. Value loss: 3.491072. Entropy: 0.746024.\n",
      "episode: 507   score: 160.0  epsilon: 1.0    steps: 556  evaluation reward: 181.8\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1174: Policy loss: 0.095881. Value loss: 13.575625. Entropy: 0.789953.\n",
      "Iteration 1175: Policy loss: -0.044101. Value loss: 10.041049. Entropy: 0.752499.\n",
      "Iteration 1176: Policy loss: 0.113754. Value loss: 7.372052. Entropy: 0.789214.\n",
      "episode: 508   score: 105.0  epsilon: 1.0    steps: 688  evaluation reward: 181.3\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1177: Policy loss: 1.089193. Value loss: 13.043547. Entropy: 0.751884.\n",
      "Iteration 1178: Policy loss: 1.199384. Value loss: 9.573833. Entropy: 0.767978.\n",
      "Iteration 1179: Policy loss: 1.260255. Value loss: 7.442005. Entropy: 0.810906.\n",
      "episode: 509   score: 180.0  epsilon: 1.0    steps: 105  evaluation reward: 180.65\n",
      "episode: 510   score: 120.0  epsilon: 1.0    steps: 948  evaluation reward: 179.75\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1180: Policy loss: 0.101300. Value loss: 15.763424. Entropy: 0.871110.\n",
      "Iteration 1181: Policy loss: 0.239260. Value loss: 11.158724. Entropy: 0.892688.\n",
      "Iteration 1182: Policy loss: -0.052513. Value loss: 9.366155. Entropy: 0.921438.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1183: Policy loss: 1.132800. Value loss: 10.526233. Entropy: 0.778471.\n",
      "Iteration 1184: Policy loss: 1.078704. Value loss: 7.519596. Entropy: 0.820614.\n",
      "Iteration 1185: Policy loss: 1.135518. Value loss: 5.516703. Entropy: 0.836357.\n",
      "episode: 511   score: 180.0  epsilon: 1.0    steps: 474  evaluation reward: 179.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1186: Policy loss: -0.870614. Value loss: 209.306335. Entropy: 0.865506.\n",
      "Iteration 1187: Policy loss: -1.149569. Value loss: 76.695930. Entropy: 0.726525.\n",
      "Iteration 1188: Policy loss: -1.337688. Value loss: 52.358418. Entropy: 0.761310.\n",
      "episode: 512   score: 500.0  epsilon: 1.0    steps: 249  evaluation reward: 182.35\n",
      "episode: 513   score: 180.0  epsilon: 1.0    steps: 769  evaluation reward: 182.35\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1189: Policy loss: 1.735553. Value loss: 12.139707. Entropy: 0.741844.\n",
      "Iteration 1190: Policy loss: 1.866455. Value loss: 6.632883. Entropy: 0.769863.\n",
      "Iteration 1191: Policy loss: 2.051028. Value loss: 4.911047. Entropy: 0.780433.\n",
      "episode: 514   score: 155.0  epsilon: 1.0    steps: 588  evaluation reward: 182.1\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1192: Policy loss: -0.009432. Value loss: 133.017197. Entropy: 0.798083.\n",
      "Iteration 1193: Policy loss: 0.509619. Value loss: 134.258362. Entropy: 0.716319.\n",
      "Iteration 1194: Policy loss: 0.381611. Value loss: 149.584366. Entropy: 0.718442.\n",
      "episode: 515   score: 155.0  epsilon: 1.0    steps: 720  evaluation reward: 181.85\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1195: Policy loss: -0.074922. Value loss: 14.777800. Entropy: 0.745072.\n",
      "Iteration 1196: Policy loss: 0.007276. Value loss: 12.093042. Entropy: 0.754237.\n",
      "Iteration 1197: Policy loss: -0.031288. Value loss: 10.474152. Entropy: 0.765117.\n",
      "episode: 516   score: 210.0  epsilon: 1.0    steps: 999  evaluation reward: 181.85\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1198: Policy loss: -2.274865. Value loss: 31.178989. Entropy: 0.747686.\n",
      "Iteration 1199: Policy loss: -2.239112. Value loss: 16.947321. Entropy: 0.701321.\n",
      "Iteration 1200: Policy loss: -2.292549. Value loss: 15.215823. Entropy: 0.702044.\n",
      "episode: 517   score: 210.0  epsilon: 1.0    steps: 75  evaluation reward: 182.4\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1201: Policy loss: 1.545419. Value loss: 21.848930. Entropy: 0.716766.\n",
      "Iteration 1202: Policy loss: 1.636966. Value loss: 7.779480. Entropy: 0.737391.\n",
      "Iteration 1203: Policy loss: 1.566036. Value loss: 6.952930. Entropy: 0.736380.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1204: Policy loss: -2.314158. Value loss: 223.007599. Entropy: 0.750204.\n",
      "Iteration 1205: Policy loss: -2.910980. Value loss: 168.307297. Entropy: 0.765626.\n",
      "Iteration 1206: Policy loss: -1.652776. Value loss: 77.760254. Entropy: 0.797956.\n",
      "episode: 518   score: 470.0  epsilon: 1.0    steps: 293  evaluation reward: 185.3\n",
      "episode: 519   score: 210.0  epsilon: 1.0    steps: 403  evaluation reward: 185.6\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1207: Policy loss: 2.210290. Value loss: 41.205135. Entropy: 0.766816.\n",
      "Iteration 1208: Policy loss: 2.604614. Value loss: 18.048172. Entropy: 0.735614.\n",
      "Iteration 1209: Policy loss: 2.218822. Value loss: 12.798691. Entropy: 0.784768.\n",
      "episode: 520   score: 180.0  epsilon: 1.0    steps: 242  evaluation reward: 185.3\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1210: Policy loss: 0.169786. Value loss: 34.051865. Entropy: 0.736335.\n",
      "Iteration 1211: Policy loss: 0.227471. Value loss: 19.740757. Entropy: 0.707054.\n",
      "Iteration 1212: Policy loss: 0.143586. Value loss: 16.239162. Entropy: 0.728763.\n",
      "episode: 521   score: 210.0  epsilon: 1.0    steps: 543  evaluation reward: 185.6\n",
      "episode: 522   score: 440.0  epsilon: 1.0    steps: 882  evaluation reward: 187.9\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1213: Policy loss: -1.107567. Value loss: 22.862309. Entropy: 0.754902.\n",
      "Iteration 1214: Policy loss: -1.290134. Value loss: 16.243040. Entropy: 0.771614.\n",
      "Iteration 1215: Policy loss: -1.162097. Value loss: 14.242801. Entropy: 0.758732.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1216: Policy loss: -2.237924. Value loss: 267.653107. Entropy: 0.589363.\n",
      "Iteration 1217: Policy loss: -2.937098. Value loss: 236.080292. Entropy: 0.581881.\n",
      "Iteration 1218: Policy loss: -2.372385. Value loss: 154.952316. Entropy: 0.629892.\n",
      "episode: 523   score: 265.0  epsilon: 1.0    steps: 691  evaluation reward: 189.0\n",
      "episode: 524   score: 155.0  epsilon: 1.0    steps: 908  evaluation reward: 188.75\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1219: Policy loss: 0.482144. Value loss: 22.505093. Entropy: 0.795445.\n",
      "Iteration 1220: Policy loss: 0.611542. Value loss: 14.366615. Entropy: 0.774934.\n",
      "Iteration 1221: Policy loss: 0.480691. Value loss: 11.774187. Entropy: 0.736595.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1222: Policy loss: -0.069412. Value loss: 21.317633. Entropy: 0.776274.\n",
      "Iteration 1223: Policy loss: 0.025150. Value loss: 14.920185. Entropy: 0.778109.\n",
      "Iteration 1224: Policy loss: 0.017567. Value loss: 11.185611. Entropy: 0.800293.\n",
      "episode: 525   score: 285.0  epsilon: 1.0    steps: 26  evaluation reward: 189.5\n",
      "episode: 526   score: 210.0  epsilon: 1.0    steps: 490  evaluation reward: 189.5\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1225: Policy loss: 0.126483. Value loss: 15.541582. Entropy: 0.830208.\n",
      "Iteration 1226: Policy loss: 0.317540. Value loss: 11.138968. Entropy: 0.833268.\n",
      "Iteration 1227: Policy loss: 0.215719. Value loss: 10.961717. Entropy: 0.842772.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1228: Policy loss: 0.855902. Value loss: 104.190292. Entropy: 0.694572.\n",
      "Iteration 1229: Policy loss: 0.710967. Value loss: 139.596146. Entropy: 0.691844.\n",
      "Iteration 1230: Policy loss: 0.855464. Value loss: 104.495316. Entropy: 0.723079.\n",
      "episode: 527   score: 355.0  epsilon: 1.0    steps: 156  evaluation reward: 192.0\n",
      "episode: 528   score: 495.0  epsilon: 1.0    steps: 374  evaluation reward: 194.85\n",
      "episode: 529   score: 155.0  epsilon: 1.0    steps: 586  evaluation reward: 194.75\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1231: Policy loss: 0.935351. Value loss: 14.783555. Entropy: 0.836512.\n",
      "Iteration 1232: Policy loss: 1.018067. Value loss: 12.167838. Entropy: 0.846176.\n",
      "Iteration 1233: Policy loss: 0.846759. Value loss: 11.610669. Entropy: 0.833052.\n",
      "episode: 530   score: 180.0  epsilon: 1.0    steps: 817  evaluation reward: 195.0\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1234: Policy loss: -0.434541. Value loss: 13.793165. Entropy: 0.699519.\n",
      "Iteration 1235: Policy loss: -0.388716. Value loss: 10.248313. Entropy: 0.702899.\n",
      "Iteration 1236: Policy loss: -0.533980. Value loss: 9.680228. Entropy: 0.715369.\n",
      "episode: 531   score: 210.0  epsilon: 1.0    steps: 959  evaluation reward: 195.9\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1237: Policy loss: -1.290823. Value loss: 19.552162. Entropy: 0.706895.\n",
      "Iteration 1238: Policy loss: -1.353418. Value loss: 16.258165. Entropy: 0.715592.\n",
      "Iteration 1239: Policy loss: -1.045259. Value loss: 13.061406. Entropy: 0.711675.\n",
      "episode: 532   score: 105.0  epsilon: 1.0    steps: 45  evaluation reward: 195.6\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1240: Policy loss: 0.155343. Value loss: 9.715546. Entropy: 0.809885.\n",
      "Iteration 1241: Policy loss: 0.354051. Value loss: 6.885875. Entropy: 0.845293.\n",
      "Iteration 1242: Policy loss: 0.259247. Value loss: 5.047402. Entropy: 0.859169.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1243: Policy loss: 1.440752. Value loss: 8.226762. Entropy: 0.912951.\n",
      "Iteration 1244: Policy loss: 1.322576. Value loss: 6.218040. Entropy: 0.961800.\n",
      "Iteration 1245: Policy loss: 1.355983. Value loss: 6.425551. Entropy: 0.946767.\n",
      "episode: 533   score: 180.0  epsilon: 1.0    steps: 413  evaluation reward: 196.9\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1246: Policy loss: 0.439016. Value loss: 16.679911. Entropy: 0.815495.\n",
      "Iteration 1247: Policy loss: 0.466833. Value loss: 7.259482. Entropy: 0.840540.\n",
      "Iteration 1248: Policy loss: 0.423422. Value loss: 4.776333. Entropy: 0.835982.\n",
      "episode: 534   score: 210.0  epsilon: 1.0    steps: 218  evaluation reward: 196.15\n",
      "episode: 535   score: 155.0  epsilon: 1.0    steps: 624  evaluation reward: 195.6\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1249: Policy loss: -1.199083. Value loss: 219.746155. Entropy: 0.748775.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1250: Policy loss: -2.292176. Value loss: 231.789093. Entropy: 0.753788.\n",
      "Iteration 1251: Policy loss: -1.080633. Value loss: 144.329163. Entropy: 0.717520.\n",
      "episode: 536   score: 355.0  epsilon: 1.0    steps: 280  evaluation reward: 197.05\n",
      "episode: 537   score: 390.0  epsilon: 1.0    steps: 738  evaluation reward: 199.75\n",
      "episode: 538   score: 210.0  epsilon: 1.0    steps: 868  evaluation reward: 200.0\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1252: Policy loss: -0.174668. Value loss: 25.745150. Entropy: 0.776736.\n",
      "Iteration 1253: Policy loss: -0.205782. Value loss: 15.185195. Entropy: 0.776732.\n",
      "Iteration 1254: Policy loss: -0.086241. Value loss: 10.956415. Entropy: 0.792446.\n",
      "episode: 539   score: 105.0  epsilon: 1.0    steps: 69  evaluation reward: 199.5\n",
      "episode: 540   score: 160.0  epsilon: 1.0    steps: 989  evaluation reward: 199.55\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1255: Policy loss: 2.068122. Value loss: 16.927010. Entropy: 0.757598.\n",
      "Iteration 1256: Policy loss: 1.932323. Value loss: 11.581244. Entropy: 0.788260.\n",
      "Iteration 1257: Policy loss: 2.008210. Value loss: 11.074059. Entropy: 0.775985.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1258: Policy loss: -0.634716. Value loss: 13.794722. Entropy: 0.889160.\n",
      "Iteration 1259: Policy loss: -0.581067. Value loss: 12.153124. Entropy: 0.880091.\n",
      "Iteration 1260: Policy loss: -0.510615. Value loss: 11.031901. Entropy: 0.882224.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1261: Policy loss: -0.368510. Value loss: 15.830595. Entropy: 0.875148.\n",
      "Iteration 1262: Policy loss: -0.174000. Value loss: 12.177082. Entropy: 0.883377.\n",
      "Iteration 1263: Policy loss: -0.276328. Value loss: 11.111225. Entropy: 0.774394.\n",
      "episode: 541   score: 120.0  epsilon: 1.0    steps: 430  evaluation reward: 197.05\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1264: Policy loss: 1.718873. Value loss: 19.236792. Entropy: 0.778148.\n",
      "Iteration 1265: Policy loss: 1.764236. Value loss: 10.005539. Entropy: 0.829571.\n",
      "Iteration 1266: Policy loss: 1.445974. Value loss: 8.907293. Entropy: 0.878663.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1267: Policy loss: -2.874157. Value loss: 240.202148. Entropy: 0.631048.\n",
      "Iteration 1268: Policy loss: -2.648447. Value loss: 169.006836. Entropy: 0.494576.\n",
      "Iteration 1269: Policy loss: -2.414273. Value loss: 145.393784. Entropy: 0.564882.\n",
      "episode: 542   score: 210.0  epsilon: 1.0    steps: 161  evaluation reward: 197.95\n",
      "episode: 543   score: 210.0  epsilon: 1.0    steps: 343  evaluation reward: 197.65\n",
      "episode: 544   score: 210.0  epsilon: 1.0    steps: 555  evaluation reward: 199.0\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1270: Policy loss: -1.599835. Value loss: 27.257999. Entropy: 0.707758.\n",
      "Iteration 1271: Policy loss: -1.544225. Value loss: 18.367382. Entropy: 0.686703.\n",
      "Iteration 1272: Policy loss: -1.488456. Value loss: 13.690039. Entropy: 0.745453.\n",
      "episode: 545   score: 210.0  epsilon: 1.0    steps: 120  evaluation reward: 199.3\n",
      "episode: 546   score: 380.0  epsilon: 1.0    steps: 660  evaluation reward: 201.55\n",
      "episode: 547   score: 210.0  epsilon: 1.0    steps: 857  evaluation reward: 202.1\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1273: Policy loss: -0.243159. Value loss: 15.884421. Entropy: 0.867611.\n",
      "Iteration 1274: Policy loss: -0.268833. Value loss: 10.806430. Entropy: 0.851046.\n",
      "Iteration 1275: Policy loss: -0.242038. Value loss: 9.406322. Entropy: 0.859288.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1276: Policy loss: 0.364630. Value loss: 10.308200. Entropy: 0.769827.\n",
      "Iteration 1277: Policy loss: 0.311135. Value loss: 7.601677. Entropy: 0.784263.\n",
      "Iteration 1278: Policy loss: 0.387866. Value loss: 7.238092. Entropy: 0.806085.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1279: Policy loss: 0.303714. Value loss: 18.306892. Entropy: 1.013820.\n",
      "Iteration 1280: Policy loss: 0.332256. Value loss: 13.738914. Entropy: 1.017102.\n",
      "Iteration 1281: Policy loss: 0.616276. Value loss: 10.463415. Entropy: 0.989540.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1282: Policy loss: -0.053499. Value loss: 11.658703. Entropy: 0.994495.\n",
      "Iteration 1283: Policy loss: 0.044451. Value loss: 8.713791. Entropy: 0.993816.\n",
      "Iteration 1284: Policy loss: -0.028964. Value loss: 7.455514. Entropy: 1.037331.\n",
      "episode: 548   score: 105.0  epsilon: 1.0    steps: 116  evaluation reward: 201.1\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1285: Policy loss: 0.755035. Value loss: 23.161892. Entropy: 0.830604.\n",
      "Iteration 1286: Policy loss: 0.970689. Value loss: 13.377153. Entropy: 0.819733.\n",
      "Iteration 1287: Policy loss: 0.800046. Value loss: 10.612523. Entropy: 0.871247.\n",
      "episode: 549   score: 135.0  epsilon: 1.0    steps: 375  evaluation reward: 201.4\n",
      "episode: 550   score: 300.0  epsilon: 1.0    steps: 496  evaluation reward: 202.3\n",
      "now time :  2019-02-26 12:51:38.831926\n",
      "episode: 551   score: 135.0  epsilon: 1.0    steps: 577  evaluation reward: 202.1\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1288: Policy loss: 2.131979. Value loss: 32.041775. Entropy: 0.936268.\n",
      "Iteration 1289: Policy loss: 2.060333. Value loss: 17.038931. Entropy: 0.987385.\n",
      "Iteration 1290: Policy loss: 1.847261. Value loss: 13.396278. Entropy: 1.009811.\n",
      "episode: 552   score: 285.0  epsilon: 1.0    steps: 236  evaluation reward: 203.9\n",
      "episode: 553   score: 210.0  epsilon: 1.0    steps: 731  evaluation reward: 204.45\n",
      "episode: 554   score: 395.0  epsilon: 1.0    steps: 904  evaluation reward: 206.3\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1291: Policy loss: 0.073490. Value loss: 21.201000. Entropy: 1.033306.\n",
      "Iteration 1292: Policy loss: 0.157335. Value loss: 17.295164. Entropy: 1.009750.\n",
      "Iteration 1293: Policy loss: 0.110379. Value loss: 14.973062. Entropy: 1.009452.\n",
      "episode: 555   score: 210.0  epsilon: 1.0    steps: 816  evaluation reward: 206.3\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1294: Policy loss: -0.821763. Value loss: 22.728422. Entropy: 1.125381.\n",
      "Iteration 1295: Policy loss: -0.845043. Value loss: 20.153290. Entropy: 1.117340.\n",
      "Iteration 1296: Policy loss: -0.864839. Value loss: 16.451204. Entropy: 1.099743.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1297: Policy loss: 0.221238. Value loss: 7.340267. Entropy: 1.229563.\n",
      "Iteration 1298: Policy loss: 0.226726. Value loss: 5.673615. Entropy: 1.205063.\n",
      "Iteration 1299: Policy loss: 0.203607. Value loss: 5.165701. Entropy: 1.254092.\n",
      "episode: 556   score: 75.0  epsilon: 1.0    steps: 372  evaluation reward: 202.2\n",
      "episode: 557   score: 110.0  epsilon: 1.0    steps: 501  evaluation reward: 201.75\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1300: Policy loss: 0.272556. Value loss: 28.446394. Entropy: 1.177688.\n",
      "Iteration 1301: Policy loss: 0.529045. Value loss: 18.471006. Entropy: 1.152245.\n",
      "Iteration 1302: Policy loss: 0.387686. Value loss: 14.514472. Entropy: 1.172575.\n",
      "episode: 558   score: 75.0  epsilon: 1.0    steps: 235  evaluation reward: 200.95\n",
      "episode: 559   score: 180.0  epsilon: 1.0    steps: 523  evaluation reward: 201.1\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1303: Policy loss: 0.974265. Value loss: 12.942095. Entropy: 1.121483.\n",
      "Iteration 1304: Policy loss: 1.036653. Value loss: 8.562240. Entropy: 1.085376.\n",
      "Iteration 1305: Policy loss: 0.961130. Value loss: 6.361201. Entropy: 1.114743.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1306: Policy loss: -1.044362. Value loss: 21.937601. Entropy: 0.899104.\n",
      "Iteration 1307: Policy loss: -1.054274. Value loss: 14.451054. Entropy: 0.909139.\n",
      "Iteration 1308: Policy loss: -1.265504. Value loss: 13.408813. Entropy: 0.918865.\n",
      "episode: 560   score: 300.0  epsilon: 1.0    steps: 32  evaluation reward: 202.3\n",
      "episode: 561   score: 210.0  epsilon: 1.0    steps: 1009  evaluation reward: 202.85\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1309: Policy loss: -0.423568. Value loss: 22.113756. Entropy: 1.128009.\n",
      "Iteration 1310: Policy loss: -0.470235. Value loss: 16.529030. Entropy: 1.114380.\n",
      "Iteration 1311: Policy loss: -0.279736. Value loss: 13.909783. Entropy: 1.116557.\n",
      "episode: 562   score: 210.0  epsilon: 1.0    steps: 675  evaluation reward: 203.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 563   score: 155.0  epsilon: 1.0    steps: 842  evaluation reward: 203.15\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1312: Policy loss: 0.254200. Value loss: 25.018066. Entropy: 1.165870.\n",
      "Iteration 1313: Policy loss: 0.251259. Value loss: 18.922510. Entropy: 1.134781.\n",
      "Iteration 1314: Policy loss: 0.445401. Value loss: 14.481759. Entropy: 1.177143.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1315: Policy loss: -2.238944. Value loss: 271.003845. Entropy: 0.768658.\n",
      "Iteration 1316: Policy loss: -1.543033. Value loss: 125.113991. Entropy: 0.900708.\n",
      "Iteration 1317: Policy loss: -1.931308. Value loss: 84.826813. Entropy: 0.831455.\n",
      "episode: 564   score: 105.0  epsilon: 1.0    steps: 83  evaluation reward: 202.1\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1318: Policy loss: 1.037854. Value loss: 29.631046. Entropy: 1.131828.\n",
      "Iteration 1319: Policy loss: 1.131469. Value loss: 22.313091. Entropy: 1.149633.\n",
      "Iteration 1320: Policy loss: 1.183512. Value loss: 18.855942. Entropy: 1.140579.\n",
      "episode: 565   score: 380.0  epsilon: 1.0    steps: 358  evaluation reward: 204.35\n",
      "episode: 566   score: 155.0  epsilon: 1.0    steps: 409  evaluation reward: 204.1\n",
      "episode: 567   score: 210.0  epsilon: 1.0    steps: 566  evaluation reward: 204.85\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1321: Policy loss: 0.798167. Value loss: 53.801086. Entropy: 1.109938.\n",
      "Iteration 1322: Policy loss: 1.062268. Value loss: 32.096527. Entropy: 1.097609.\n",
      "Iteration 1323: Policy loss: 0.900718. Value loss: 27.871529. Entropy: 1.095048.\n",
      "episode: 568   score: 210.0  epsilon: 1.0    steps: 164  evaluation reward: 205.3\n",
      "episode: 569   score: 75.0  epsilon: 1.0    steps: 689  evaluation reward: 204.25\n",
      "episode: 570   score: 185.0  epsilon: 1.0    steps: 1000  evaluation reward: 202.85\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1324: Policy loss: 0.136315. Value loss: 25.029406. Entropy: 1.101790.\n",
      "Iteration 1325: Policy loss: 0.227431. Value loss: 13.207364. Entropy: 1.118296.\n",
      "Iteration 1326: Policy loss: 0.471964. Value loss: 14.477522. Entropy: 1.084727.\n",
      "episode: 571   score: 135.0  epsilon: 1.0    steps: 777  evaluation reward: 201.1\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1327: Policy loss: -1.619743. Value loss: 36.365185. Entropy: 1.038109.\n",
      "Iteration 1328: Policy loss: -1.797168. Value loss: 27.796091. Entropy: 1.070417.\n",
      "Iteration 1329: Policy loss: -1.510909. Value loss: 23.804296. Entropy: 1.096692.\n",
      "episode: 572   score: 135.0  epsilon: 1.0    steps: 24  evaluation reward: 195.85\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1330: Policy loss: -2.114558. Value loss: 45.044983. Entropy: 1.145122.\n",
      "Iteration 1331: Policy loss: -1.868232. Value loss: 33.664055. Entropy: 1.191737.\n",
      "Iteration 1332: Policy loss: -2.017563. Value loss: 27.525547. Entropy: 1.162398.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1333: Policy loss: 2.087884. Value loss: 35.958092. Entropy: 1.084368.\n",
      "Iteration 1334: Policy loss: 2.093601. Value loss: 19.726330. Entropy: 1.068444.\n",
      "Iteration 1335: Policy loss: 2.173732. Value loss: 14.489704. Entropy: 1.029892.\n",
      "episode: 573   score: 130.0  epsilon: 1.0    steps: 612  evaluation reward: 195.55\n",
      "episode: 574   score: 110.0  epsilon: 1.0    steps: 995  evaluation reward: 195.3\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1336: Policy loss: 2.504759. Value loss: 45.327255. Entropy: 1.109934.\n",
      "Iteration 1337: Policy loss: 2.268113. Value loss: 26.230410. Entropy: 1.093283.\n",
      "Iteration 1338: Policy loss: 2.386333. Value loss: 20.439695. Entropy: 1.090109.\n",
      "episode: 575   score: 220.0  epsilon: 1.0    steps: 291  evaluation reward: 195.95\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1339: Policy loss: -1.464698. Value loss: 38.157356. Entropy: 1.086070.\n",
      "Iteration 1340: Policy loss: -1.305586. Value loss: 25.708675. Entropy: 1.062131.\n",
      "Iteration 1341: Policy loss: -1.331686. Value loss: 19.260458. Entropy: 1.054480.\n",
      "episode: 576   score: 260.0  epsilon: 1.0    steps: 488  evaluation reward: 197.2\n",
      "episode: 577   score: 240.0  epsilon: 1.0    steps: 748  evaluation reward: 199.1\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1342: Policy loss: 1.714288. Value loss: 52.660988. Entropy: 0.812470.\n",
      "Iteration 1343: Policy loss: 1.450104. Value loss: 26.481121. Entropy: 0.860298.\n",
      "Iteration 1344: Policy loss: 1.892916. Value loss: 26.023293. Entropy: 0.857488.\n",
      "episode: 578   score: 200.0  epsilon: 1.0    steps: 109  evaluation reward: 200.05\n",
      "episode: 579   score: 350.0  epsilon: 1.0    steps: 155  evaluation reward: 202.5\n",
      "episode: 580   score: 235.0  epsilon: 1.0    steps: 822  evaluation reward: 203.8\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1345: Policy loss: -0.346378. Value loss: 33.198368. Entropy: 1.072602.\n",
      "Iteration 1346: Policy loss: -0.580213. Value loss: 21.836563. Entropy: 1.106219.\n",
      "Iteration 1347: Policy loss: -0.339628. Value loss: 16.043287. Entropy: 1.045142.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1348: Policy loss: 0.473067. Value loss: 39.687595. Entropy: 0.889337.\n",
      "Iteration 1349: Policy loss: 0.732826. Value loss: 26.233366. Entropy: 0.886051.\n",
      "Iteration 1350: Policy loss: 0.508985. Value loss: 22.791544. Entropy: 0.866826.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1351: Policy loss: 0.109345. Value loss: 45.912827. Entropy: 0.971732.\n",
      "Iteration 1352: Policy loss: 0.170022. Value loss: 26.366817. Entropy: 0.939110.\n",
      "Iteration 1353: Policy loss: -0.182039. Value loss: 21.684036. Entropy: 0.968603.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1354: Policy loss: 1.017276. Value loss: 57.945786. Entropy: 0.923961.\n",
      "Iteration 1355: Policy loss: 1.108229. Value loss: 36.307171. Entropy: 0.973330.\n",
      "Iteration 1356: Policy loss: 1.251401. Value loss: 29.736025. Entropy: 0.928936.\n",
      "episode: 581   score: 110.0  epsilon: 1.0    steps: 238  evaluation reward: 203.5\n",
      "episode: 582   score: 295.0  epsilon: 1.0    steps: 549  evaluation reward: 204.65\n",
      "episode: 583   score: 290.0  epsilon: 1.0    steps: 1020  evaluation reward: 206.0\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1357: Policy loss: 1.186487. Value loss: 44.709599. Entropy: 0.863838.\n",
      "Iteration 1358: Policy loss: 0.775747. Value loss: 32.964710. Entropy: 0.804812.\n",
      "Iteration 1359: Policy loss: 0.935795. Value loss: 26.840725. Entropy: 0.818561.\n",
      "episode: 584   score: 135.0  epsilon: 1.0    steps: 116  evaluation reward: 205.75\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1360: Policy loss: 3.105713. Value loss: 42.186539. Entropy: 0.649008.\n",
      "Iteration 1361: Policy loss: 3.129584. Value loss: 21.709255. Entropy: 0.807489.\n",
      "Iteration 1362: Policy loss: 2.975599. Value loss: 15.628314. Entropy: 0.774394.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1363: Policy loss: 0.648612. Value loss: 17.260870. Entropy: 0.794872.\n",
      "Iteration 1364: Policy loss: 0.704832. Value loss: 12.362092. Entropy: 0.742942.\n",
      "Iteration 1365: Policy loss: 0.735917. Value loss: 10.250446. Entropy: 0.743774.\n",
      "episode: 585   score: 290.0  epsilon: 1.0    steps: 437  evaluation reward: 207.3\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1366: Policy loss: -1.329995. Value loss: 21.848904. Entropy: 0.928607.\n",
      "Iteration 1367: Policy loss: -1.396510. Value loss: 17.041616. Entropy: 0.954084.\n",
      "Iteration 1368: Policy loss: -1.241645. Value loss: 14.267878. Entropy: 0.893615.\n",
      "episode: 586   score: 295.0  epsilon: 1.0    steps: 689  evaluation reward: 208.7\n",
      "episode: 587   score: 290.0  epsilon: 1.0    steps: 849  evaluation reward: 210.0\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1369: Policy loss: 2.238173. Value loss: 22.181087. Entropy: 0.835701.\n",
      "Iteration 1370: Policy loss: 2.338989. Value loss: 13.597361. Entropy: 0.874829.\n",
      "Iteration 1371: Policy loss: 2.198610. Value loss: 12.794333. Entropy: 0.882354.\n",
      "episode: 588   score: 155.0  epsilon: 1.0    steps: 218  evaluation reward: 209.45\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1372: Policy loss: 1.262911. Value loss: 21.895041. Entropy: 0.836484.\n",
      "Iteration 1373: Policy loss: 1.034658. Value loss: 14.862270. Entropy: 0.849739.\n",
      "Iteration 1374: Policy loss: 0.961393. Value loss: 12.083683. Entropy: 0.829277.\n",
      "episode: 589   score: 135.0  epsilon: 1.0    steps: 56  evaluation reward: 206.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 590   score: 210.0  epsilon: 1.0    steps: 594  evaluation reward: 207.45\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1375: Policy loss: 0.180647. Value loss: 20.427515. Entropy: 0.845950.\n",
      "Iteration 1376: Policy loss: -0.066163. Value loss: 14.359523. Entropy: 0.827542.\n",
      "Iteration 1377: Policy loss: 0.074366. Value loss: 12.817825. Entropy: 0.875112.\n",
      "episode: 591   score: 210.0  epsilon: 1.0    steps: 933  evaluation reward: 207.45\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1378: Policy loss: -0.402633. Value loss: 26.980284. Entropy: 0.908135.\n",
      "Iteration 1379: Policy loss: -0.630051. Value loss: 18.911873. Entropy: 0.898230.\n",
      "Iteration 1380: Policy loss: -0.453342. Value loss: 15.259083. Entropy: 0.926020.\n",
      "episode: 592   score: 105.0  epsilon: 1.0    steps: 394  evaluation reward: 206.4\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1381: Policy loss: -1.333618. Value loss: 35.361946. Entropy: 1.126266.\n",
      "Iteration 1382: Policy loss: -1.145853. Value loss: 28.133137. Entropy: 1.137753.\n",
      "Iteration 1383: Policy loss: -1.463238. Value loss: 22.381718. Entropy: 1.118981.\n",
      "episode: 593   score: 605.0  epsilon: 1.0    steps: 324  evaluation reward: 210.65\n",
      "episode: 594   score: 120.0  epsilon: 1.0    steps: 644  evaluation reward: 210.75\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1384: Policy loss: 0.000429. Value loss: 29.104204. Entropy: 1.050282.\n",
      "Iteration 1385: Policy loss: -0.045422. Value loss: 20.592739. Entropy: 1.001162.\n",
      "Iteration 1386: Policy loss: -0.175114. Value loss: 16.144930. Entropy: 1.031435.\n",
      "episode: 595   score: 235.0  epsilon: 1.0    steps: 840  evaluation reward: 211.75\n",
      "episode: 596   score: 105.0  epsilon: 1.0    steps: 973  evaluation reward: 211.0\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1387: Policy loss: 2.963853. Value loss: 34.431801. Entropy: 0.937712.\n",
      "Iteration 1388: Policy loss: 2.872005. Value loss: 23.409903. Entropy: 0.905148.\n",
      "Iteration 1389: Policy loss: 3.166504. Value loss: 20.733768. Entropy: 0.908786.\n",
      "episode: 597   score: 135.0  epsilon: 1.0    steps: 557  evaluation reward: 210.25\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1390: Policy loss: -2.427006. Value loss: 290.808472. Entropy: 1.021242.\n",
      "Iteration 1391: Policy loss: -2.047658. Value loss: 192.824173. Entropy: 0.910656.\n",
      "Iteration 1392: Policy loss: -2.404697. Value loss: 216.535599. Entropy: 0.955897.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1393: Policy loss: -0.519238. Value loss: 41.850349. Entropy: 0.820027.\n",
      "Iteration 1394: Policy loss: -0.438078. Value loss: 31.762823. Entropy: 0.879149.\n",
      "Iteration 1395: Policy loss: -0.460786. Value loss: 26.807381. Entropy: 0.888679.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1396: Policy loss: -2.569573. Value loss: 249.577179. Entropy: 0.849720.\n",
      "Iteration 1397: Policy loss: -2.586135. Value loss: 166.182755. Entropy: 0.820646.\n",
      "Iteration 1398: Policy loss: -2.702222. Value loss: 132.095337. Entropy: 0.924515.\n",
      "episode: 598   score: 330.0  epsilon: 1.0    steps: 119  evaluation reward: 211.75\n",
      "episode: 599   score: 35.0  epsilon: 1.0    steps: 904  evaluation reward: 210.5\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1399: Policy loss: 2.346755. Value loss: 89.035637. Entropy: 0.940374.\n",
      "Iteration 1400: Policy loss: 2.034009. Value loss: 49.335140. Entropy: 0.951607.\n",
      "Iteration 1401: Policy loss: 1.848896. Value loss: 41.276539. Entropy: 0.973346.\n",
      "episode: 600   score: 225.0  epsilon: 1.0    steps: 219  evaluation reward: 211.4\n",
      "now time :  2019-02-26 12:53:49.431654\n",
      "episode: 601   score: 510.0  epsilon: 1.0    steps: 482  evaluation reward: 214.95\n",
      "episode: 602   score: 445.0  epsilon: 1.0    steps: 750  evaluation reward: 217.85\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1402: Policy loss: 2.729982. Value loss: 61.707302. Entropy: 0.827417.\n",
      "Iteration 1403: Policy loss: 3.416243. Value loss: 44.391380. Entropy: 0.812770.\n",
      "Iteration 1404: Policy loss: 2.874803. Value loss: 33.898941. Entropy: 0.798793.\n",
      "episode: 603   score: 245.0  epsilon: 1.0    steps: 895  evaluation reward: 218.5\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1405: Policy loss: -0.530935. Value loss: 169.675674. Entropy: 0.900958.\n",
      "Iteration 1406: Policy loss: -0.335362. Value loss: 84.421814. Entropy: 0.800725.\n",
      "Iteration 1407: Policy loss: 0.242479. Value loss: 26.403374. Entropy: 0.826622.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1408: Policy loss: 2.339659. Value loss: 179.674973. Entropy: 0.726674.\n",
      "Iteration 1409: Policy loss: 2.034722. Value loss: 199.270416. Entropy: 0.723937.\n",
      "Iteration 1410: Policy loss: 2.072493. Value loss: 169.163452. Entropy: 0.730737.\n",
      "episode: 604   score: 480.0  epsilon: 1.0    steps: 582  evaluation reward: 221.2\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1411: Policy loss: 4.037748. Value loss: 59.077621. Entropy: 0.799718.\n",
      "Iteration 1412: Policy loss: 3.970007. Value loss: 32.571983. Entropy: 0.849732.\n",
      "Iteration 1413: Policy loss: 4.005348. Value loss: 24.789852. Entropy: 0.816778.\n",
      "episode: 605   score: 250.0  epsilon: 1.0    steps: 354  evaluation reward: 222.15\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1414: Policy loss: 2.563345. Value loss: 24.600937. Entropy: 0.807049.\n",
      "Iteration 1415: Policy loss: 2.615621. Value loss: 18.035223. Entropy: 0.805172.\n",
      "Iteration 1416: Policy loss: 2.350886. Value loss: 12.798352. Entropy: 0.799109.\n",
      "episode: 606   score: 350.0  epsilon: 1.0    steps: 909  evaluation reward: 224.1\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1417: Policy loss: -0.471211. Value loss: 17.168133. Entropy: 0.738840.\n",
      "Iteration 1418: Policy loss: -0.239891. Value loss: 9.659590. Entropy: 0.741577.\n",
      "Iteration 1419: Policy loss: -0.422803. Value loss: 10.194378. Entropy: 0.717698.\n",
      "episode: 607   score: 210.0  epsilon: 1.0    steps: 105  evaluation reward: 224.6\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1420: Policy loss: 1.952891. Value loss: 33.835995. Entropy: 0.844742.\n",
      "Iteration 1421: Policy loss: 1.832633. Value loss: 23.184362. Entropy: 0.841931.\n",
      "Iteration 1422: Policy loss: 1.850512. Value loss: 18.672871. Entropy: 0.799116.\n",
      "episode: 608   score: 180.0  epsilon: 1.0    steps: 389  evaluation reward: 225.35\n",
      "episode: 609   score: 210.0  epsilon: 1.0    steps: 673  evaluation reward: 225.65\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1423: Policy loss: -0.626129. Value loss: 17.134235. Entropy: 0.845687.\n",
      "Iteration 1424: Policy loss: -0.641446. Value loss: 11.419851. Entropy: 0.886288.\n",
      "Iteration 1425: Policy loss: -0.665988. Value loss: 9.496301. Entropy: 0.853014.\n",
      "episode: 610   score: 260.0  epsilon: 1.0    steps: 174  evaluation reward: 227.05\n",
      "episode: 611   score: 155.0  epsilon: 1.0    steps: 818  evaluation reward: 226.8\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1426: Policy loss: 1.237346. Value loss: 20.480625. Entropy: 0.758094.\n",
      "Iteration 1427: Policy loss: 1.348443. Value loss: 12.111523. Entropy: 0.791855.\n",
      "Iteration 1428: Policy loss: 1.214319. Value loss: 12.176826. Entropy: 0.785866.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1429: Policy loss: 1.422790. Value loss: 16.476021. Entropy: 0.885611.\n",
      "Iteration 1430: Policy loss: 1.012279. Value loss: 13.591999. Entropy: 0.924327.\n",
      "Iteration 1431: Policy loss: 1.443982. Value loss: 11.394451. Entropy: 0.928619.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1432: Policy loss: -0.310527. Value loss: 19.823858. Entropy: 0.849953.\n",
      "Iteration 1433: Policy loss: -0.359844. Value loss: 14.908381. Entropy: 0.832729.\n",
      "Iteration 1434: Policy loss: -0.137548. Value loss: 12.096066. Entropy: 0.882334.\n",
      "episode: 612   score: 155.0  epsilon: 1.0    steps: 264  evaluation reward: 223.35\n",
      "episode: 613   score: 210.0  epsilon: 1.0    steps: 979  evaluation reward: 223.65\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1435: Policy loss: 2.051645. Value loss: 12.703922. Entropy: 0.982797.\n",
      "Iteration 1436: Policy loss: 2.125427. Value loss: 8.556741. Entropy: 1.025829.\n",
      "Iteration 1437: Policy loss: 2.096065. Value loss: 6.458851. Entropy: 1.013309.\n",
      "episode: 614   score: 105.0  epsilon: 1.0    steps: 200  evaluation reward: 223.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 615   score: 300.0  epsilon: 1.0    steps: 523  evaluation reward: 224.6\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1438: Policy loss: -2.041078. Value loss: 240.566681. Entropy: 0.867251.\n",
      "Iteration 1439: Policy loss: -1.563635. Value loss: 131.532227. Entropy: 0.829863.\n",
      "Iteration 1440: Policy loss: -1.343656. Value loss: 88.775070. Entropy: 0.930272.\n",
      "episode: 616   score: 210.0  epsilon: 1.0    steps: 58  evaluation reward: 224.6\n",
      "episode: 617   score: 135.0  epsilon: 1.0    steps: 423  evaluation reward: 223.85\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1441: Policy loss: -0.002983. Value loss: 19.080410. Entropy: 0.851455.\n",
      "Iteration 1442: Policy loss: 0.102643. Value loss: 14.715319. Entropy: 0.855634.\n",
      "Iteration 1443: Policy loss: 0.055114. Value loss: 14.254767. Entropy: 0.865817.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1444: Policy loss: -0.341877. Value loss: 26.474606. Entropy: 0.965547.\n",
      "Iteration 1445: Policy loss: -0.358871. Value loss: 22.434402. Entropy: 0.897830.\n",
      "Iteration 1446: Policy loss: -0.573609. Value loss: 19.006546. Entropy: 0.918886.\n",
      "episode: 618   score: 265.0  epsilon: 1.0    steps: 665  evaluation reward: 221.8\n",
      "episode: 619   score: 460.0  epsilon: 1.0    steps: 831  evaluation reward: 224.3\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1447: Policy loss: -1.475827. Value loss: 215.762878. Entropy: 0.954815.\n",
      "Iteration 1448: Policy loss: -1.744273. Value loss: 100.118065. Entropy: 1.023206.\n",
      "Iteration 1449: Policy loss: -2.213982. Value loss: 61.289215. Entropy: 1.041354.\n",
      "episode: 620   score: 105.0  epsilon: 1.0    steps: 558  evaluation reward: 223.55\n",
      "episode: 621   score: 155.0  epsilon: 1.0    steps: 910  evaluation reward: 223.0\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1450: Policy loss: 0.184883. Value loss: 21.272089. Entropy: 0.781138.\n",
      "Iteration 1451: Policy loss: 0.062425. Value loss: 13.243086. Entropy: 0.778101.\n",
      "Iteration 1452: Policy loss: 0.042095. Value loss: 9.876900. Entropy: 0.772430.\n",
      "episode: 622   score: 135.0  epsilon: 1.0    steps: 111  evaluation reward: 219.95\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1453: Policy loss: 0.335770. Value loss: 20.790913. Entropy: 0.843864.\n",
      "Iteration 1454: Policy loss: 0.382326. Value loss: 13.654908. Entropy: 0.832862.\n",
      "Iteration 1455: Policy loss: 0.462263. Value loss: 10.947428. Entropy: 0.837133.\n",
      "episode: 623   score: 185.0  epsilon: 1.0    steps: 233  evaluation reward: 219.15\n",
      "episode: 624   score: 460.0  epsilon: 1.0    steps: 335  evaluation reward: 222.2\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1456: Policy loss: 0.314639. Value loss: 15.585839. Entropy: 0.932624.\n",
      "Iteration 1457: Policy loss: 0.325422. Value loss: 11.196589. Entropy: 0.948030.\n",
      "Iteration 1458: Policy loss: 0.338269. Value loss: 11.412478. Entropy: 0.959436.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1459: Policy loss: -2.344682. Value loss: 285.947327. Entropy: 0.759222.\n",
      "Iteration 1460: Policy loss: -2.529022. Value loss: 147.373459. Entropy: 0.650220.\n",
      "Iteration 1461: Policy loss: -1.212796. Value loss: 67.705589. Entropy: 0.627306.\n",
      "episode: 625   score: 75.0  epsilon: 1.0    steps: 585  evaluation reward: 220.1\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1462: Policy loss: 3.307598. Value loss: 85.712891. Entropy: 0.856153.\n",
      "Iteration 1463: Policy loss: 3.247230. Value loss: 35.317173. Entropy: 0.902326.\n",
      "Iteration 1464: Policy loss: 2.493070. Value loss: 30.487768. Entropy: 0.876276.\n",
      "episode: 626   score: 50.0  epsilon: 1.0    steps: 126  evaluation reward: 218.5\n",
      "episode: 627   score: 460.0  epsilon: 1.0    steps: 388  evaluation reward: 219.55\n",
      "episode: 628   score: 225.0  epsilon: 1.0    steps: 866  evaluation reward: 216.85\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1465: Policy loss: 1.631108. Value loss: 64.055061. Entropy: 1.045499.\n",
      "Iteration 1466: Policy loss: 1.212116. Value loss: 37.856796. Entropy: 1.058731.\n",
      "Iteration 1467: Policy loss: 1.324012. Value loss: 32.024357. Entropy: 1.087311.\n",
      "episode: 629   score: 260.0  epsilon: 1.0    steps: 726  evaluation reward: 217.9\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1468: Policy loss: 0.071380. Value loss: 36.869846. Entropy: 1.169521.\n",
      "Iteration 1469: Policy loss: 0.309018. Value loss: 24.426334. Entropy: 1.123917.\n",
      "Iteration 1470: Policy loss: 0.329898. Value loss: 20.848114. Entropy: 1.132082.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1471: Policy loss: -1.674105. Value loss: 72.641357. Entropy: 1.063481.\n",
      "Iteration 1472: Policy loss: -1.373809. Value loss: 44.171276. Entropy: 1.052822.\n",
      "Iteration 1473: Policy loss: -1.469361. Value loss: 36.419121. Entropy: 1.036999.\n",
      "episode: 630   score: 195.0  epsilon: 1.0    steps: 566  evaluation reward: 218.05\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1474: Policy loss: -0.297579. Value loss: 57.179806. Entropy: 1.235703.\n",
      "Iteration 1475: Policy loss: -0.079991. Value loss: 42.506023. Entropy: 1.206419.\n",
      "Iteration 1476: Policy loss: 0.118810. Value loss: 34.770443. Entropy: 1.204628.\n",
      "episode: 631   score: 205.0  epsilon: 1.0    steps: 286  evaluation reward: 218.0\n",
      "episode: 632   score: 485.0  epsilon: 1.0    steps: 935  evaluation reward: 221.8\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1477: Policy loss: 2.854189. Value loss: 63.805286. Entropy: 1.267827.\n",
      "Iteration 1478: Policy loss: 2.662793. Value loss: 37.810040. Entropy: 1.254199.\n",
      "Iteration 1479: Policy loss: 2.716781. Value loss: 31.080015. Entropy: 1.274422.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1480: Policy loss: 1.418220. Value loss: 65.882156. Entropy: 1.104140.\n",
      "Iteration 1481: Policy loss: 1.603254. Value loss: 53.111477. Entropy: 1.123151.\n",
      "Iteration 1482: Policy loss: 2.087817. Value loss: 55.286266. Entropy: 1.152351.\n",
      "episode: 633   score: 385.0  epsilon: 1.0    steps: 201  evaluation reward: 223.85\n",
      "episode: 634   score: 170.0  epsilon: 1.0    steps: 827  evaluation reward: 223.45\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1483: Policy loss: 2.656747. Value loss: 53.087788. Entropy: 1.270468.\n",
      "Iteration 1484: Policy loss: 2.753757. Value loss: 32.143562. Entropy: 1.299983.\n",
      "Iteration 1485: Policy loss: 2.633913. Value loss: 26.667301. Entropy: 1.311682.\n",
      "episode: 635   score: 230.0  epsilon: 1.0    steps: 65  evaluation reward: 224.2\n",
      "episode: 636   score: 65.0  epsilon: 1.0    steps: 553  evaluation reward: 221.3\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1486: Policy loss: -0.946803. Value loss: 337.914062. Entropy: 1.338574.\n",
      "Iteration 1487: Policy loss: -1.259332. Value loss: 260.522644. Entropy: 1.280001.\n",
      "Iteration 1488: Policy loss: -0.379397. Value loss: 151.917709. Entropy: 1.316937.\n",
      "episode: 637   score: 540.0  epsilon: 1.0    steps: 411  evaluation reward: 222.8\n",
      "episode: 638   score: 280.0  epsilon: 1.0    steps: 700  evaluation reward: 223.5\n",
      "episode: 639   score: 105.0  epsilon: 1.0    steps: 998  evaluation reward: 223.5\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1489: Policy loss: 2.283166. Value loss: 34.597279. Entropy: 1.125822.\n",
      "Iteration 1490: Policy loss: 2.262304. Value loss: 25.552355. Entropy: 1.098127.\n",
      "Iteration 1491: Policy loss: 2.162887. Value loss: 23.168589. Entropy: 1.107970.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1492: Policy loss: -1.126107. Value loss: 35.673145. Entropy: 0.915248.\n",
      "Iteration 1493: Policy loss: -0.614104. Value loss: 24.069721. Entropy: 0.931656.\n",
      "Iteration 1494: Policy loss: -0.808685. Value loss: 21.172403. Entropy: 0.965563.\n",
      "episode: 640   score: 225.0  epsilon: 1.0    steps: 375  evaluation reward: 224.15\n",
      "episode: 641   score: 55.0  epsilon: 1.0    steps: 615  evaluation reward: 223.5\n",
      "episode: 642   score: 75.0  epsilon: 1.0    steps: 887  evaluation reward: 222.15\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1495: Policy loss: 1.093709. Value loss: 65.639465. Entropy: 1.070356.\n",
      "Iteration 1496: Policy loss: 1.802174. Value loss: 47.539730. Entropy: 1.065400.\n",
      "Iteration 1497: Policy loss: 1.289085. Value loss: 34.645798. Entropy: 1.126123.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1498: Policy loss: 0.307783. Value loss: 65.677185. Entropy: 0.981361.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1499: Policy loss: 0.562985. Value loss: 37.888386. Entropy: 0.960751.\n",
      "Iteration 1500: Policy loss: 0.531327. Value loss: 28.042667. Entropy: 0.958089.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1501: Policy loss: 1.470808. Value loss: 50.234436. Entropy: 0.845951.\n",
      "Iteration 1502: Policy loss: 1.499507. Value loss: 30.314114. Entropy: 0.864336.\n",
      "Iteration 1503: Policy loss: 1.695602. Value loss: 23.645315. Entropy: 0.876364.\n",
      "episode: 643   score: 240.0  epsilon: 1.0    steps: 119  evaluation reward: 222.45\n",
      "episode: 644   score: 150.0  epsilon: 1.0    steps: 424  evaluation reward: 221.85\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1504: Policy loss: 2.683555. Value loss: 40.944687. Entropy: 0.993898.\n",
      "Iteration 1505: Policy loss: 2.976480. Value loss: 26.296568. Entropy: 1.057872.\n",
      "Iteration 1506: Policy loss: 2.812706. Value loss: 18.046619. Entropy: 1.034498.\n",
      "episode: 645   score: 80.0  epsilon: 1.0    steps: 273  evaluation reward: 220.55\n",
      "episode: 646   score: 240.0  epsilon: 1.0    steps: 698  evaluation reward: 219.15\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1507: Policy loss: 2.222043. Value loss: 27.336454. Entropy: 1.054221.\n",
      "Iteration 1508: Policy loss: 1.948334. Value loss: 16.168161. Entropy: 1.064191.\n",
      "Iteration 1509: Policy loss: 1.920052. Value loss: 13.032103. Entropy: 1.073308.\n",
      "episode: 647   score: 105.0  epsilon: 1.0    steps: 865  evaluation reward: 218.1\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1510: Policy loss: 0.889878. Value loss: 39.452057. Entropy: 0.930615.\n",
      "Iteration 1511: Policy loss: 0.732962. Value loss: 26.945137. Entropy: 0.935098.\n",
      "Iteration 1512: Policy loss: 0.916026. Value loss: 23.899904. Entropy: 0.926384.\n",
      "episode: 648   score: 285.0  epsilon: 1.0    steps: 172  evaluation reward: 219.9\n",
      "episode: 649   score: 215.0  epsilon: 1.0    steps: 918  evaluation reward: 220.7\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1513: Policy loss: 1.932138. Value loss: 51.508183. Entropy: 1.076177.\n",
      "Iteration 1514: Policy loss: 2.069689. Value loss: 37.863480. Entropy: 1.132619.\n",
      "Iteration 1515: Policy loss: 2.096101. Value loss: 31.851665. Entropy: 1.060470.\n",
      "episode: 650   score: 185.0  epsilon: 1.0    steps: 521  evaluation reward: 219.55\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1516: Policy loss: 0.702804. Value loss: 38.912777. Entropy: 1.018654.\n",
      "Iteration 1517: Policy loss: 0.584701. Value loss: 23.937243. Entropy: 1.043742.\n",
      "Iteration 1518: Policy loss: 0.538657. Value loss: 20.030912. Entropy: 1.054562.\n",
      "now time :  2019-02-26 12:56:01.568273\n",
      "episode: 651   score: 135.0  epsilon: 1.0    steps: 78  evaluation reward: 219.55\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1519: Policy loss: -2.933316. Value loss: 44.156155. Entropy: 1.132813.\n",
      "Iteration 1520: Policy loss: -2.914653. Value loss: 33.655991. Entropy: 1.113385.\n",
      "Iteration 1521: Policy loss: -2.947131. Value loss: 29.118078. Entropy: 1.104222.\n",
      "episode: 652   score: 150.0  epsilon: 1.0    steps: 679  evaluation reward: 218.2\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1522: Policy loss: 0.190226. Value loss: 38.592422. Entropy: 1.122593.\n",
      "Iteration 1523: Policy loss: 0.324365. Value loss: 26.680899. Entropy: 1.143812.\n",
      "Iteration 1524: Policy loss: 0.155717. Value loss: 24.652517. Entropy: 1.151450.\n",
      "episode: 653   score: 160.0  epsilon: 1.0    steps: 311  evaluation reward: 217.7\n",
      "episode: 654   score: 340.0  epsilon: 1.0    steps: 425  evaluation reward: 217.15\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1525: Policy loss: 1.562043. Value loss: 37.217770. Entropy: 1.119521.\n",
      "Iteration 1526: Policy loss: 2.020845. Value loss: 23.567270. Entropy: 1.103404.\n",
      "Iteration 1527: Policy loss: 1.606031. Value loss: 18.477713. Entropy: 1.065961.\n",
      "episode: 655   score: 110.0  epsilon: 1.0    steps: 172  evaluation reward: 216.15\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1528: Policy loss: 2.275418. Value loss: 38.772587. Entropy: 1.074768.\n",
      "Iteration 1529: Policy loss: 2.104217. Value loss: 29.630157. Entropy: 0.992764.\n",
      "Iteration 1530: Policy loss: 2.098692. Value loss: 23.336870. Entropy: 1.005629.\n",
      "episode: 656   score: 285.0  epsilon: 1.0    steps: 815  evaluation reward: 218.25\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1531: Policy loss: 0.426323. Value loss: 41.701500. Entropy: 1.119210.\n",
      "Iteration 1532: Policy loss: 0.189141. Value loss: 29.675011. Entropy: 1.124279.\n",
      "Iteration 1533: Policy loss: 0.562602. Value loss: 22.604568. Entropy: 1.129480.\n",
      "episode: 657   score: 135.0  epsilon: 1.0    steps: 101  evaluation reward: 218.5\n",
      "episode: 658   score: 80.0  epsilon: 1.0    steps: 676  evaluation reward: 218.55\n",
      "episode: 659   score: 270.0  epsilon: 1.0    steps: 981  evaluation reward: 219.45\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1534: Policy loss: -0.426685. Value loss: 51.314186. Entropy: 1.209567.\n",
      "Iteration 1535: Policy loss: -0.114791. Value loss: 31.560181. Entropy: 1.194439.\n",
      "Iteration 1536: Policy loss: -0.316105. Value loss: 27.892458. Entropy: 1.219536.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1537: Policy loss: 0.887150. Value loss: 23.951811. Entropy: 1.225366.\n",
      "Iteration 1538: Policy loss: 0.793805. Value loss: 19.679485. Entropy: 1.177597.\n",
      "Iteration 1539: Policy loss: 0.860714. Value loss: 17.683212. Entropy: 1.190119.\n",
      "episode: 660   score: 155.0  epsilon: 1.0    steps: 456  evaluation reward: 218.0\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1540: Policy loss: -1.205450. Value loss: 54.189339. Entropy: 1.107970.\n",
      "Iteration 1541: Policy loss: -1.112472. Value loss: 43.228714. Entropy: 1.102508.\n",
      "Iteration 1542: Policy loss: -0.967138. Value loss: 31.328993. Entropy: 1.120917.\n",
      "episode: 661   score: 155.0  epsilon: 1.0    steps: 838  evaluation reward: 217.45\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1543: Policy loss: -1.149494. Value loss: 54.864811. Entropy: 1.203369.\n",
      "Iteration 1544: Policy loss: -1.301378. Value loss: 35.505405. Entropy: 1.208225.\n",
      "Iteration 1545: Policy loss: -1.377562. Value loss: 27.460632. Entropy: 1.184184.\n",
      "episode: 662   score: 205.0  epsilon: 1.0    steps: 260  evaluation reward: 217.4\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1546: Policy loss: -0.127704. Value loss: 39.217724. Entropy: 1.203185.\n",
      "Iteration 1547: Policy loss: 0.116493. Value loss: 26.294809. Entropy: 1.187853.\n",
      "Iteration 1548: Policy loss: -0.100114. Value loss: 23.318996. Entropy: 1.195019.\n",
      "episode: 663   score: 200.0  epsilon: 1.0    steps: 713  evaluation reward: 217.85\n",
      "episode: 664   score: 55.0  epsilon: 1.0    steps: 904  evaluation reward: 217.35\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1549: Policy loss: 0.946139. Value loss: 43.566925. Entropy: 1.239241.\n",
      "Iteration 1550: Policy loss: 1.024313. Value loss: 33.605888. Entropy: 1.232153.\n",
      "Iteration 1551: Policy loss: 0.632389. Value loss: 27.320944. Entropy: 1.226152.\n",
      "episode: 665   score: 360.0  epsilon: 1.0    steps: 182  evaluation reward: 217.15\n",
      "episode: 666   score: 490.0  epsilon: 1.0    steps: 528  evaluation reward: 220.5\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1552: Policy loss: -1.785034. Value loss: 279.681305. Entropy: 1.250787.\n",
      "Iteration 1553: Policy loss: -2.022763. Value loss: 312.580475. Entropy: 1.219866.\n",
      "Iteration 1554: Policy loss: -1.807652. Value loss: 183.090881. Entropy: 1.159243.\n",
      "episode: 667   score: 560.0  epsilon: 1.0    steps: 93  evaluation reward: 224.0\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1555: Policy loss: 1.430366. Value loss: 73.090675. Entropy: 1.104116.\n",
      "Iteration 1556: Policy loss: 1.591193. Value loss: 42.385616. Entropy: 1.075761.\n",
      "Iteration 1557: Policy loss: 1.302702. Value loss: 35.598560. Entropy: 1.078713.\n",
      "episode: 668   score: 125.0  epsilon: 1.0    steps: 809  evaluation reward: 223.15\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1558: Policy loss: 1.804825. Value loss: 62.736519. Entropy: 1.072737.\n",
      "Iteration 1559: Policy loss: 1.803552. Value loss: 43.324047. Entropy: 1.077593.\n",
      "Iteration 1560: Policy loss: 1.835826. Value loss: 37.763218. Entropy: 1.069696.\n",
      "episode: 669   score: 220.0  epsilon: 1.0    steps: 277  evaluation reward: 224.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 670   score: 305.0  epsilon: 1.0    steps: 471  evaluation reward: 225.8\n",
      "episode: 671   score: 105.0  epsilon: 1.0    steps: 674  evaluation reward: 225.5\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1561: Policy loss: -0.321187. Value loss: 33.412102. Entropy: 1.034179.\n",
      "Iteration 1562: Policy loss: -0.406290. Value loss: 26.312763. Entropy: 1.032219.\n",
      "Iteration 1563: Policy loss: -0.545655. Value loss: 24.035101. Entropy: 1.021787.\n",
      "episode: 672   score: 135.0  epsilon: 1.0    steps: 900  evaluation reward: 225.5\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1564: Policy loss: -0.518783. Value loss: 26.940498. Entropy: 1.009220.\n",
      "Iteration 1565: Policy loss: -0.267651. Value loss: 18.300024. Entropy: 1.000599.\n",
      "Iteration 1566: Policy loss: -0.364980. Value loss: 15.861204. Entropy: 0.995887.\n",
      "episode: 673   score: 185.0  epsilon: 1.0    steps: 156  evaluation reward: 226.05\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1567: Policy loss: 0.049835. Value loss: 43.749596. Entropy: 1.048139.\n",
      "Iteration 1568: Policy loss: 0.155497. Value loss: 26.778643. Entropy: 1.039526.\n",
      "Iteration 1569: Policy loss: -0.175791. Value loss: 20.762110. Entropy: 1.052006.\n",
      "episode: 674   score: 135.0  epsilon: 1.0    steps: 338  evaluation reward: 226.3\n",
      "episode: 675   score: 255.0  epsilon: 1.0    steps: 633  evaluation reward: 226.65\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1570: Policy loss: 1.445380. Value loss: 46.499004. Entropy: 1.063620.\n",
      "Iteration 1571: Policy loss: 1.928316. Value loss: 39.947105. Entropy: 1.067939.\n",
      "Iteration 1572: Policy loss: 1.802496. Value loss: 31.925226. Entropy: 1.058049.\n",
      "episode: 676   score: 185.0  epsilon: 1.0    steps: 857  evaluation reward: 225.9\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1573: Policy loss: -0.030445. Value loss: 35.689598. Entropy: 1.062226.\n",
      "Iteration 1574: Policy loss: -0.012676. Value loss: 22.783373. Entropy: 1.105864.\n",
      "Iteration 1575: Policy loss: -0.016337. Value loss: 20.505098. Entropy: 1.078688.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1576: Policy loss: -3.839869. Value loss: 61.409279. Entropy: 1.025460.\n",
      "Iteration 1577: Policy loss: -3.902727. Value loss: 31.923883. Entropy: 1.032200.\n",
      "Iteration 1578: Policy loss: -3.758286. Value loss: 26.484617. Entropy: 1.043256.\n",
      "episode: 677   score: 20.0  epsilon: 1.0    steps: 632  evaluation reward: 223.7\n",
      "episode: 678   score: 75.0  epsilon: 1.0    steps: 956  evaluation reward: 222.45\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1579: Policy loss: 1.796276. Value loss: 42.260315. Entropy: 1.053284.\n",
      "Iteration 1580: Policy loss: 2.229811. Value loss: 26.228386. Entropy: 1.052088.\n",
      "Iteration 1581: Policy loss: 1.845696. Value loss: 20.950758. Entropy: 1.060966.\n",
      "episode: 679   score: 155.0  epsilon: 1.0    steps: 187  evaluation reward: 220.5\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1582: Policy loss: -2.255165. Value loss: 62.001842. Entropy: 1.186549.\n",
      "Iteration 1583: Policy loss: -2.414948. Value loss: 41.977444. Entropy: 1.189286.\n",
      "Iteration 1584: Policy loss: -2.697368. Value loss: 36.971279. Entropy: 1.186089.\n",
      "episode: 680   score: 480.0  epsilon: 1.0    steps: 104  evaluation reward: 222.95\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1585: Policy loss: -0.904017. Value loss: 55.027889. Entropy: 1.155268.\n",
      "Iteration 1586: Policy loss: -1.228679. Value loss: 39.775719. Entropy: 1.123266.\n",
      "Iteration 1587: Policy loss: -0.922032. Value loss: 32.730721. Entropy: 1.127726.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1588: Policy loss: -2.338076. Value loss: 72.588058. Entropy: 1.122041.\n",
      "Iteration 1589: Policy loss: -2.211185. Value loss: 40.604698. Entropy: 1.131455.\n",
      "Iteration 1590: Policy loss: -2.506439. Value loss: 34.091057. Entropy: 1.132746.\n",
      "episode: 681   score: 445.0  epsilon: 1.0    steps: 730  evaluation reward: 226.3\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1591: Policy loss: 0.234861. Value loss: 42.805180. Entropy: 1.108519.\n",
      "Iteration 1592: Policy loss: 0.308242. Value loss: 29.270973. Entropy: 1.078766.\n",
      "Iteration 1593: Policy loss: 0.413826. Value loss: 23.806860. Entropy: 1.080241.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1594: Policy loss: -2.996539. Value loss: 267.791565. Entropy: 1.053977.\n",
      "Iteration 1595: Policy loss: -2.205326. Value loss: 134.106186. Entropy: 0.932298.\n",
      "Iteration 1596: Policy loss: -3.421238. Value loss: 86.092247. Entropy: 0.934881.\n",
      "episode: 682   score: 370.0  epsilon: 1.0    steps: 862  evaluation reward: 227.05\n",
      "episode: 683   score: 170.0  epsilon: 1.0    steps: 909  evaluation reward: 225.85\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1597: Policy loss: 3.971423. Value loss: 76.344398. Entropy: 0.838279.\n",
      "Iteration 1598: Policy loss: 4.003126. Value loss: 52.640045. Entropy: 0.895153.\n",
      "Iteration 1599: Policy loss: 3.824554. Value loss: 37.950840. Entropy: 0.894043.\n",
      "episode: 684   score: 455.0  epsilon: 1.0    steps: 218  evaluation reward: 229.05\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1600: Policy loss: 3.704175. Value loss: 25.901718. Entropy: 0.980407.\n",
      "Iteration 1601: Policy loss: 3.668506. Value loss: 15.747622. Entropy: 1.043667.\n",
      "Iteration 1602: Policy loss: 3.233799. Value loss: 14.163623. Entropy: 1.135606.\n",
      "episode: 685   score: 410.0  epsilon: 1.0    steps: 358  evaluation reward: 230.25\n",
      "episode: 686   score: 735.0  epsilon: 1.0    steps: 392  evaluation reward: 234.65\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1603: Policy loss: 3.897031. Value loss: 45.552670. Entropy: 1.168300.\n",
      "Iteration 1604: Policy loss: 3.986463. Value loss: 27.163624. Entropy: 1.157543.\n",
      "Iteration 1605: Policy loss: 3.956208. Value loss: 24.718550. Entropy: 1.176803.\n",
      "episode: 687   score: 265.0  epsilon: 1.0    steps: 585  evaluation reward: 234.4\n",
      "episode: 688   score: 120.0  epsilon: 1.0    steps: 689  evaluation reward: 234.05\n",
      "episode: 689   score: 75.0  epsilon: 1.0    steps: 895  evaluation reward: 233.45\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1606: Policy loss: 1.370542. Value loss: 34.156429. Entropy: 1.147020.\n",
      "Iteration 1607: Policy loss: 1.518142. Value loss: 21.537548. Entropy: 1.125358.\n",
      "Iteration 1608: Policy loss: 1.623360. Value loss: 15.477725. Entropy: 1.131882.\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1609: Policy loss: -0.763100. Value loss: 31.982323. Entropy: 1.123885.\n",
      "Iteration 1610: Policy loss: -0.635743. Value loss: 21.082144. Entropy: 1.167322.\n",
      "Iteration 1611: Policy loss: -0.544657. Value loss: 19.270391. Entropy: 1.168178.\n",
      "episode: 690   score: 350.0  epsilon: 1.0    steps: 97  evaluation reward: 234.85\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1612: Policy loss: 1.667318. Value loss: 19.784399. Entropy: 1.249094.\n",
      "Iteration 1613: Policy loss: 1.573166. Value loss: 13.415694. Entropy: 1.253189.\n",
      "Iteration 1614: Policy loss: 1.667536. Value loss: 10.295687. Entropy: 1.251994.\n",
      "episode: 691   score: 180.0  epsilon: 1.0    steps: 937  evaluation reward: 234.55\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1615: Policy loss: -0.068642. Value loss: 20.727480. Entropy: 1.273010.\n",
      "Iteration 1616: Policy loss: -0.011697. Value loss: 12.349498. Entropy: 1.309012.\n",
      "Iteration 1617: Policy loss: -0.066944. Value loss: 10.949292. Entropy: 1.302747.\n",
      "episode: 692   score: 230.0  epsilon: 1.0    steps: 225  evaluation reward: 235.8\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1618: Policy loss: -0.200703. Value loss: 36.358074. Entropy: 1.125171.\n",
      "Iteration 1619: Policy loss: -0.134107. Value loss: 30.218378. Entropy: 1.116267.\n",
      "Iteration 1620: Policy loss: -0.203431. Value loss: 26.141960. Entropy: 1.116700.\n",
      "episode: 693   score: 80.0  epsilon: 1.0    steps: 532  evaluation reward: 230.55\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1621: Policy loss: -0.136203. Value loss: 23.450230. Entropy: 1.218485.\n",
      "Iteration 1622: Policy loss: -0.217861. Value loss: 14.688493. Entropy: 1.180067.\n",
      "Iteration 1623: Policy loss: -0.109543. Value loss: 12.506128. Entropy: 1.181320.\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1624: Policy loss: -0.804251. Value loss: 29.806149. Entropy: 1.102248.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1625: Policy loss: -0.875435. Value loss: 21.031429. Entropy: 1.107937.\n",
      "Iteration 1626: Policy loss: -0.613873. Value loss: 17.285608. Entropy: 1.116277.\n",
      "episode: 694   score: 275.0  epsilon: 1.0    steps: 343  evaluation reward: 232.1\n",
      "episode: 695   score: 200.0  epsilon: 1.0    steps: 660  evaluation reward: 231.75\n",
      "episode: 696   score: 140.0  epsilon: 1.0    steps: 790  evaluation reward: 232.1\n",
      "episode: 697   score: 105.0  epsilon: 1.0    steps: 965  evaluation reward: 231.8\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1627: Policy loss: 2.267570. Value loss: 24.258530. Entropy: 1.239579.\n",
      "Iteration 1628: Policy loss: 2.009513. Value loss: 14.485106. Entropy: 1.222010.\n",
      "Iteration 1629: Policy loss: 2.011754. Value loss: 10.495629. Entropy: 1.216516.\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1630: Policy loss: -0.839688. Value loss: 29.400782. Entropy: 1.018559.\n",
      "Iteration 1631: Policy loss: -0.761229. Value loss: 20.015207. Entropy: 0.957410.\n",
      "Iteration 1632: Policy loss: -0.883439. Value loss: 17.078096. Entropy: 1.016215.\n",
      "episode: 698   score: 210.0  epsilon: 1.0    steps: 28  evaluation reward: 230.6\n",
      "episode: 699   score: 285.0  epsilon: 1.0    steps: 443  evaluation reward: 233.1\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1633: Policy loss: 1.733362. Value loss: 18.838018. Entropy: 1.102143.\n",
      "Iteration 1634: Policy loss: 1.700733. Value loss: 11.114394. Entropy: 1.117060.\n",
      "Iteration 1635: Policy loss: 1.662502. Value loss: 11.586734. Entropy: 1.109821.\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1636: Policy loss: -1.148631. Value loss: 18.752178. Entropy: 1.106279.\n",
      "Iteration 1637: Policy loss: -1.179949. Value loss: 15.624494. Entropy: 1.067075.\n",
      "Iteration 1638: Policy loss: -1.155496. Value loss: 13.284741. Entropy: 1.084821.\n",
      "episode: 700   score: 210.0  epsilon: 1.0    steps: 619  evaluation reward: 232.95\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1639: Policy loss: -1.061624. Value loss: 22.547564. Entropy: 0.859800.\n",
      "Iteration 1640: Policy loss: -1.067399. Value loss: 14.663866. Entropy: 0.913632.\n",
      "Iteration 1641: Policy loss: -0.979739. Value loss: 11.968311. Entropy: 0.905890.\n",
      "now time :  2019-02-26 12:58:19.162674\n",
      "episode: 701   score: 290.0  epsilon: 1.0    steps: 178  evaluation reward: 230.75\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1642: Policy loss: 0.860988. Value loss: 16.050941. Entropy: 0.818955.\n",
      "Iteration 1643: Policy loss: 1.146367. Value loss: 7.461604. Entropy: 0.789868.\n",
      "Iteration 1644: Policy loss: 0.853144. Value loss: 5.984158. Entropy: 0.773012.\n",
      "episode: 702   score: 210.0  epsilon: 1.0    steps: 859  evaluation reward: 228.4\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1645: Policy loss: -5.027323. Value loss: 340.351349. Entropy: 0.745900.\n",
      "Iteration 1646: Policy loss: -4.298799. Value loss: 235.449142. Entropy: 0.641414.\n",
      "Iteration 1647: Policy loss: -5.027985. Value loss: 183.201248. Entropy: 0.653035.\n",
      "episode: 703   score: 210.0  epsilon: 1.0    steps: 259  evaluation reward: 228.05\n",
      "episode: 704   score: 355.0  epsilon: 1.0    steps: 906  evaluation reward: 226.8\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1648: Policy loss: 0.205492. Value loss: 8.441418. Entropy: 0.820952.\n",
      "Iteration 1649: Policy loss: 0.369341. Value loss: 6.179616. Entropy: 0.826241.\n",
      "Iteration 1650: Policy loss: 0.312322. Value loss: 5.911021. Entropy: 0.864390.\n",
      "episode: 705   score: 155.0  epsilon: 1.0    steps: 67  evaluation reward: 225.85\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1651: Policy loss: 0.792044. Value loss: 26.077324. Entropy: 0.801459.\n",
      "Iteration 1652: Policy loss: 0.703785. Value loss: 17.160072. Entropy: 0.868960.\n",
      "Iteration 1653: Policy loss: 0.738881. Value loss: 14.445921. Entropy: 0.873399.\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1654: Policy loss: -0.778402. Value loss: 13.323292. Entropy: 0.889553.\n",
      "Iteration 1655: Policy loss: -0.883245. Value loss: 8.722078. Entropy: 0.869634.\n",
      "Iteration 1656: Policy loss: -0.655788. Value loss: 6.944385. Entropy: 0.906633.\n",
      "episode: 706   score: 105.0  epsilon: 1.0    steps: 197  evaluation reward: 223.4\n",
      "episode: 707   score: 270.0  epsilon: 1.0    steps: 462  evaluation reward: 224.0\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1657: Policy loss: -0.990109. Value loss: 31.754770. Entropy: 0.883083.\n",
      "Iteration 1658: Policy loss: -0.816156. Value loss: 15.216720. Entropy: 0.912311.\n",
      "Iteration 1659: Policy loss: -1.077639. Value loss: 12.637260. Entropy: 0.881637.\n",
      "episode: 708   score: 210.0  epsilon: 1.0    steps: 554  evaluation reward: 224.3\n",
      "episode: 709   score: 105.0  epsilon: 1.0    steps: 878  evaluation reward: 223.25\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1660: Policy loss: -0.845009. Value loss: 13.715498. Entropy: 0.829400.\n",
      "Iteration 1661: Policy loss: -0.773102. Value loss: 11.324065. Entropy: 0.806296.\n",
      "Iteration 1662: Policy loss: -1.005337. Value loss: 9.207863. Entropy: 0.791067.\n",
      "episode: 710   score: 440.0  epsilon: 1.0    steps: 701  evaluation reward: 225.05\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1663: Policy loss: 1.021962. Value loss: 18.087858. Entropy: 0.613555.\n",
      "Iteration 1664: Policy loss: 1.099494. Value loss: 12.976521. Entropy: 0.646152.\n",
      "Iteration 1665: Policy loss: 0.997839. Value loss: 11.780577. Entropy: 0.641023.\n",
      "episode: 711   score: 210.0  epsilon: 1.0    steps: 329  evaluation reward: 225.6\n",
      "episode: 712   score: 180.0  epsilon: 1.0    steps: 955  evaluation reward: 225.85\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1666: Policy loss: -0.839842. Value loss: 15.396684. Entropy: 0.884292.\n",
      "Iteration 1667: Policy loss: -0.934566. Value loss: 12.527457. Entropy: 0.917788.\n",
      "Iteration 1668: Policy loss: -0.896775. Value loss: 10.450240. Entropy: 0.928030.\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1669: Policy loss: -0.571405. Value loss: 20.710978. Entropy: 0.807032.\n",
      "Iteration 1670: Policy loss: -0.298379. Value loss: 12.709098. Entropy: 0.859179.\n",
      "Iteration 1671: Policy loss: -0.568032. Value loss: 10.378130. Entropy: 0.841658.\n",
      "episode: 713   score: 135.0  epsilon: 1.0    steps: 409  evaluation reward: 225.1\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1672: Policy loss: -0.521178. Value loss: 18.033775. Entropy: 0.859052.\n",
      "Iteration 1673: Policy loss: -0.836506. Value loss: 12.066504. Entropy: 0.815655.\n",
      "Iteration 1674: Policy loss: -0.514511. Value loss: 9.879490. Entropy: 0.829283.\n",
      "episode: 714   score: 265.0  epsilon: 1.0    steps: 17  evaluation reward: 226.7\n",
      "episode: 715   score: 210.0  epsilon: 1.0    steps: 221  evaluation reward: 225.8\n",
      "episode: 716   score: 120.0  epsilon: 1.0    steps: 872  evaluation reward: 224.9\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1675: Policy loss: 0.371663. Value loss: 20.619276. Entropy: 0.812702.\n",
      "Iteration 1676: Policy loss: 0.366849. Value loss: 12.633085. Entropy: 0.747592.\n",
      "Iteration 1677: Policy loss: 0.282051. Value loss: 10.437647. Entropy: 0.768523.\n",
      "episode: 717   score: 105.0  epsilon: 1.0    steps: 292  evaluation reward: 224.6\n",
      "episode: 718   score: 155.0  epsilon: 1.0    steps: 568  evaluation reward: 223.5\n",
      "episode: 719   score: 135.0  epsilon: 1.0    steps: 681  evaluation reward: 220.25\n",
      "episode: 720   score: 105.0  epsilon: 1.0    steps: 902  evaluation reward: 220.25\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1678: Policy loss: 0.906355. Value loss: 19.084190. Entropy: 0.901753.\n",
      "Iteration 1679: Policy loss: 0.879112. Value loss: 13.032104. Entropy: 0.917468.\n",
      "Iteration 1680: Policy loss: 0.904544. Value loss: 11.956990. Entropy: 0.890520.\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1681: Policy loss: 1.565901. Value loss: 25.142990. Entropy: 0.691379.\n",
      "Iteration 1682: Policy loss: 1.567657. Value loss: 17.287287. Entropy: 0.608451.\n",
      "Iteration 1683: Policy loss: 1.706579. Value loss: 17.183092. Entropy: 0.577464.\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1684: Policy loss: 1.583714. Value loss: 11.660345. Entropy: 0.923012.\n",
      "Iteration 1685: Policy loss: 1.633041. Value loss: 7.282294. Entropy: 0.952743.\n",
      "Iteration 1686: Policy loss: 1.560628. Value loss: 7.674613. Entropy: 0.966619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1687: Policy loss: -0.673835. Value loss: 12.046342. Entropy: 0.731099.\n",
      "Iteration 1688: Policy loss: -0.712121. Value loss: 8.774471. Entropy: 0.725841.\n",
      "Iteration 1689: Policy loss: -0.719627. Value loss: 9.431079. Entropy: 0.721567.\n",
      "episode: 721   score: 135.0  epsilon: 1.0    steps: 226  evaluation reward: 220.05\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1690: Policy loss: -3.925382. Value loss: 254.696808. Entropy: 0.410820.\n",
      "Iteration 1691: Policy loss: -3.533398. Value loss: 169.711502. Entropy: 0.300053.\n",
      "Iteration 1692: Policy loss: -4.905250. Value loss: 197.679352. Entropy: 0.309886.\n",
      "episode: 722   score: 270.0  epsilon: 1.0    steps: 498  evaluation reward: 221.4\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1693: Policy loss: 3.494462. Value loss: 42.655220. Entropy: 0.199075.\n",
      "Iteration 1694: Policy loss: 3.897582. Value loss: 27.585079. Entropy: 0.209461.\n",
      "Iteration 1695: Policy loss: 3.658527. Value loss: 18.785070. Entropy: 0.260904.\n",
      "episode: 723   score: 210.0  epsilon: 1.0    steps: 358  evaluation reward: 221.65\n",
      "episode: 724   score: 180.0  epsilon: 1.0    steps: 732  evaluation reward: 218.85\n",
      "episode: 725   score: 180.0  epsilon: 1.0    steps: 805  evaluation reward: 219.9\n",
      "episode: 726   score: 180.0  epsilon: 1.0    steps: 945  evaluation reward: 221.2\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1696: Policy loss: 4.184216. Value loss: 50.320911. Entropy: 0.653689.\n",
      "Iteration 1697: Policy loss: 3.944317. Value loss: 24.080814. Entropy: 0.600891.\n",
      "Iteration 1698: Policy loss: 3.739791. Value loss: 20.669598. Entropy: 0.782155.\n",
      "episode: 727   score: 485.0  epsilon: 1.0    steps: 8  evaluation reward: 221.45\n",
      "episode: 728   score: 210.0  epsilon: 1.0    steps: 520  evaluation reward: 221.3\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1699: Policy loss: 1.609978. Value loss: 9.784767. Entropy: 1.017067.\n",
      "Iteration 1700: Policy loss: 1.722164. Value loss: 8.918825. Entropy: 1.060929.\n",
      "Iteration 1701: Policy loss: 1.810074. Value loss: 8.239277. Entropy: 0.998007.\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1702: Policy loss: -0.011411. Value loss: 20.580336. Entropy: 0.924050.\n",
      "Iteration 1703: Policy loss: -0.034978. Value loss: 17.328249. Entropy: 0.929551.\n",
      "Iteration 1704: Policy loss: 0.099321. Value loss: 14.211167. Entropy: 0.938149.\n",
      "episode: 729   score: 105.0  epsilon: 1.0    steps: 421  evaluation reward: 219.75\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1705: Policy loss: -2.939406. Value loss: 140.495941. Entropy: 1.094546.\n",
      "Iteration 1706: Policy loss: -2.918536. Value loss: 41.685322. Entropy: 1.112222.\n",
      "Iteration 1707: Policy loss: -3.034683. Value loss: 20.840361. Entropy: 1.092084.\n",
      "episode: 730   score: 75.0  epsilon: 1.0    steps: 39  evaluation reward: 218.55\n",
      "episode: 731   score: 335.0  epsilon: 1.0    steps: 227  evaluation reward: 219.85\n",
      "episode: 732   score: 75.0  epsilon: 1.0    steps: 551  evaluation reward: 215.75\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1708: Policy loss: 2.579880. Value loss: 36.407280. Entropy: 0.839328.\n",
      "Iteration 1709: Policy loss: 2.451493. Value loss: 10.827255. Entropy: 0.855936.\n",
      "Iteration 1710: Policy loss: 2.456302. Value loss: 8.877195. Entropy: 0.913510.\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1711: Policy loss: -2.393608. Value loss: 276.052765. Entropy: 0.646646.\n",
      "Iteration 1712: Policy loss: -2.390241. Value loss: 169.641052. Entropy: 0.577656.\n",
      "Iteration 1713: Policy loss: -2.622961. Value loss: 145.910049. Entropy: 0.596381.\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1714: Policy loss: 0.567430. Value loss: 35.296921. Entropy: 0.684468.\n",
      "Iteration 1715: Policy loss: 0.274579. Value loss: 22.933186. Entropy: 0.729100.\n",
      "Iteration 1716: Policy loss: 0.351931. Value loss: 17.998489. Entropy: 0.705392.\n",
      "episode: 733   score: 380.0  epsilon: 1.0    steps: 691  evaluation reward: 215.7\n",
      "episode: 734   score: 260.0  epsilon: 1.0    steps: 893  evaluation reward: 216.6\n",
      "episode: 735   score: 210.0  epsilon: 1.0    steps: 919  evaluation reward: 216.4\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1717: Policy loss: 1.450900. Value loss: 36.879387. Entropy: 0.889738.\n",
      "Iteration 1718: Policy loss: 1.231171. Value loss: 15.392623. Entropy: 0.881746.\n",
      "Iteration 1719: Policy loss: 1.458304. Value loss: 14.768025. Entropy: 0.905784.\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1720: Policy loss: 1.258389. Value loss: 21.841253. Entropy: 0.780605.\n",
      "Iteration 1721: Policy loss: 1.128212. Value loss: 13.500748. Entropy: 0.832722.\n",
      "Iteration 1722: Policy loss: 1.073757. Value loss: 10.676955. Entropy: 0.799281.\n",
      "episode: 736   score: 330.0  epsilon: 1.0    steps: 304  evaluation reward: 219.05\n",
      "episode: 737   score: 180.0  epsilon: 1.0    steps: 450  evaluation reward: 215.45\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1723: Policy loss: 0.324364. Value loss: 26.620707. Entropy: 0.892627.\n",
      "Iteration 1724: Policy loss: 0.435435. Value loss: 19.036898. Entropy: 0.961581.\n",
      "Iteration 1725: Policy loss: 0.109889. Value loss: 16.806179. Entropy: 0.925886.\n",
      "episode: 738   score: 135.0  epsilon: 1.0    steps: 49  evaluation reward: 214.0\n",
      "episode: 739   score: 155.0  epsilon: 1.0    steps: 238  evaluation reward: 214.5\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1726: Policy loss: 2.481220. Value loss: 26.391567. Entropy: 0.807992.\n",
      "Iteration 1727: Policy loss: 2.371928. Value loss: 14.536358. Entropy: 0.877731.\n",
      "Iteration 1728: Policy loss: 2.568073. Value loss: 12.820621. Entropy: 0.832898.\n",
      "episode: 740   score: 230.0  epsilon: 1.0    steps: 535  evaluation reward: 214.55\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1729: Policy loss: -3.202679. Value loss: 267.419647. Entropy: 0.856635.\n",
      "Iteration 1730: Policy loss: -3.261983. Value loss: 192.898727. Entropy: 0.724925.\n",
      "Iteration 1731: Policy loss: -3.120401. Value loss: 138.626114. Entropy: 0.738479.\n",
      "episode: 741   score: 105.0  epsilon: 1.0    steps: 740  evaluation reward: 215.05\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1732: Policy loss: 3.156479. Value loss: 54.657921. Entropy: 0.607447.\n",
      "Iteration 1733: Policy loss: 3.339195. Value loss: 37.360809. Entropy: 0.507302.\n",
      "Iteration 1734: Policy loss: 3.297929. Value loss: 31.395529. Entropy: 0.587043.\n",
      "episode: 742   score: 135.0  epsilon: 1.0    steps: 356  evaluation reward: 215.65\n",
      "episode: 743   score: 125.0  epsilon: 1.0    steps: 969  evaluation reward: 214.5\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1735: Policy loss: 0.121890. Value loss: 55.751560. Entropy: 0.907142.\n",
      "Iteration 1736: Policy loss: 0.095081. Value loss: 39.060631. Entropy: 0.904388.\n",
      "Iteration 1737: Policy loss: 0.063161. Value loss: 30.338001. Entropy: 0.891634.\n",
      "episode: 744   score: 380.0  epsilon: 1.0    steps: 826  evaluation reward: 216.8\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1738: Policy loss: -0.851999. Value loss: 48.329361. Entropy: 0.927130.\n",
      "Iteration 1739: Policy loss: -0.976645. Value loss: 28.290054. Entropy: 0.893340.\n",
      "Iteration 1740: Policy loss: -0.891133. Value loss: 19.280407. Entropy: 0.913910.\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1741: Policy loss: 0.341493. Value loss: 310.920563. Entropy: 0.715352.\n",
      "Iteration 1742: Policy loss: 0.918191. Value loss: 207.498627. Entropy: 0.673149.\n",
      "Iteration 1743: Policy loss: 1.056223. Value loss: 110.366463. Entropy: 0.667589.\n",
      "episode: 745   score: 180.0  epsilon: 1.0    steps: 193  evaluation reward: 217.8\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1744: Policy loss: -0.184494. Value loss: 59.023228. Entropy: 0.616533.\n",
      "Iteration 1745: Policy loss: -0.377944. Value loss: 40.210255. Entropy: 0.597845.\n",
      "Iteration 1746: Policy loss: -0.028908. Value loss: 35.882591. Entropy: 0.606567.\n",
      "episode: 746   score: 225.0  epsilon: 1.0    steps: 596  evaluation reward: 217.65\n",
      "episode: 747   score: 140.0  epsilon: 1.0    steps: 714  evaluation reward: 218.0\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1747: Policy loss: -4.392783. Value loss: 243.241226. Entropy: 0.639024.\n",
      "Iteration 1748: Policy loss: -3.677156. Value loss: 122.860374. Entropy: 0.607378.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1749: Policy loss: -4.066023. Value loss: 116.825409. Entropy: 0.630893.\n",
      "episode: 748   score: 460.0  epsilon: 1.0    steps: 50  evaluation reward: 219.75\n",
      "episode: 749   score: 125.0  epsilon: 1.0    steps: 320  evaluation reward: 218.85\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1750: Policy loss: 2.775236. Value loss: 62.867756. Entropy: 0.595703.\n",
      "Iteration 1751: Policy loss: 2.696617. Value loss: 36.527176. Entropy: 0.685970.\n",
      "Iteration 1752: Policy loss: 2.646499. Value loss: 30.790228. Entropy: 0.840031.\n",
      "episode: 750   score: 220.0  epsilon: 1.0    steps: 906  evaluation reward: 219.2\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1753: Policy loss: 2.367167. Value loss: 44.803761. Entropy: 0.923444.\n",
      "Iteration 1754: Policy loss: 2.425138. Value loss: 30.788416. Entropy: 0.960375.\n",
      "Iteration 1755: Policy loss: 2.291775. Value loss: 24.452574. Entropy: 0.948977.\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1756: Policy loss: 3.374216. Value loss: 41.294392. Entropy: 0.779039.\n",
      "Iteration 1757: Policy loss: 3.321777. Value loss: 26.000788. Entropy: 0.603197.\n",
      "Iteration 1758: Policy loss: 3.337182. Value loss: 21.274481. Entropy: 0.643739.\n",
      "now time :  2019-02-26 13:00:29.372449\n",
      "episode: 751   score: 50.0  epsilon: 1.0    steps: 113  evaluation reward: 218.35\n",
      "episode: 752   score: 140.0  epsilon: 1.0    steps: 192  evaluation reward: 218.25\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1759: Policy loss: 0.591981. Value loss: 41.477100. Entropy: 0.616191.\n",
      "Iteration 1760: Policy loss: 0.941899. Value loss: 32.618423. Entropy: 0.603041.\n",
      "Iteration 1761: Policy loss: 1.359599. Value loss: 24.302580. Entropy: 0.512914.\n",
      "episode: 753   score: 755.0  epsilon: 1.0    steps: 396  evaluation reward: 224.2\n",
      "episode: 754   score: 100.0  epsilon: 1.0    steps: 532  evaluation reward: 221.8\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1762: Policy loss: -0.276058. Value loss: 35.888733. Entropy: 0.443814.\n",
      "Iteration 1763: Policy loss: -0.657557. Value loss: 30.635902. Entropy: 0.478591.\n",
      "Iteration 1764: Policy loss: -0.223434. Value loss: 23.956911. Entropy: 0.456193.\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1765: Policy loss: 0.534904. Value loss: 37.852219. Entropy: 0.420252.\n",
      "Iteration 1766: Policy loss: 0.566416. Value loss: 22.353241. Entropy: 0.428250.\n",
      "Iteration 1767: Policy loss: 0.421994. Value loss: 17.811687. Entropy: 0.449810.\n",
      "episode: 755   score: 270.0  epsilon: 1.0    steps: 771  evaluation reward: 223.4\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1768: Policy loss: 1.587791. Value loss: 39.164948. Entropy: 0.595524.\n",
      "Iteration 1769: Policy loss: 1.286602. Value loss: 24.879395. Entropy: 0.584475.\n",
      "Iteration 1770: Policy loss: 1.479287. Value loss: 17.838142. Entropy: 0.611746.\n",
      "episode: 756   score: 45.0  epsilon: 1.0    steps: 182  evaluation reward: 221.0\n",
      "episode: 757   score: 265.0  epsilon: 1.0    steps: 366  evaluation reward: 222.3\n",
      "episode: 758   score: 315.0  epsilon: 1.0    steps: 724  evaluation reward: 224.65\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1771: Policy loss: 0.265779. Value loss: 58.215336. Entropy: 0.523349.\n",
      "Iteration 1772: Policy loss: 0.186959. Value loss: 41.062000. Entropy: 0.560274.\n",
      "Iteration 1773: Policy loss: -0.047880. Value loss: 35.464909. Entropy: 0.540568.\n",
      "episode: 759   score: 280.0  epsilon: 1.0    steps: 939  evaluation reward: 224.75\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1774: Policy loss: -0.988189. Value loss: 47.955070. Entropy: 0.489130.\n",
      "Iteration 1775: Policy loss: -1.039339. Value loss: 28.110567. Entropy: 0.498278.\n",
      "Iteration 1776: Policy loss: -1.665448. Value loss: 26.309626. Entropy: 0.489696.\n",
      "episode: 760   score: 165.0  epsilon: 1.0    steps: 555  evaluation reward: 224.85\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1777: Policy loss: 0.760452. Value loss: 37.740128. Entropy: 0.468941.\n",
      "Iteration 1778: Policy loss: 0.890987. Value loss: 27.330124. Entropy: 0.438031.\n",
      "Iteration 1779: Policy loss: 0.621612. Value loss: 23.468351. Entropy: 0.440222.\n",
      "episode: 761   score: 260.0  epsilon: 1.0    steps: 31  evaluation reward: 225.9\n",
      "episode: 762   score: 255.0  epsilon: 1.0    steps: 417  evaluation reward: 226.4\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1780: Policy loss: 1.337060. Value loss: 29.018890. Entropy: 0.463803.\n",
      "Iteration 1781: Policy loss: 1.447149. Value loss: 16.613308. Entropy: 0.436488.\n",
      "Iteration 1782: Policy loss: 1.289929. Value loss: 12.169286. Entropy: 0.425479.\n",
      "episode: 763   score: 120.0  epsilon: 1.0    steps: 289  evaluation reward: 225.6\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1783: Policy loss: 0.118469. Value loss: 50.291451. Entropy: 0.407280.\n",
      "Iteration 1784: Policy loss: 0.221625. Value loss: 33.598087. Entropy: 0.431345.\n",
      "Iteration 1785: Policy loss: -0.010371. Value loss: 28.806950. Entropy: 0.404572.\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1786: Policy loss: 0.912018. Value loss: 43.844193. Entropy: 0.449558.\n",
      "Iteration 1787: Policy loss: 0.825118. Value loss: 27.732985. Entropy: 0.489297.\n",
      "Iteration 1788: Policy loss: 0.832981. Value loss: 22.006557. Entropy: 0.431180.\n",
      "episode: 764   score: 230.0  epsilon: 1.0    steps: 681  evaluation reward: 227.35\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1789: Policy loss: 1.770540. Value loss: 61.627007. Entropy: 0.430111.\n",
      "Iteration 1790: Policy loss: 1.573860. Value loss: 33.793789. Entropy: 0.451127.\n",
      "Iteration 1791: Policy loss: 1.626049. Value loss: 26.712446. Entropy: 0.454651.\n",
      "episode: 765   score: 125.0  epsilon: 1.0    steps: 431  evaluation reward: 225.0\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1792: Policy loss: -3.899823. Value loss: 304.486023. Entropy: 0.325340.\n",
      "Iteration 1793: Policy loss: -2.627138. Value loss: 150.940201. Entropy: 0.262321.\n",
      "Iteration 1794: Policy loss: -2.874826. Value loss: 127.770653. Entropy: 0.173895.\n",
      "episode: 766   score: 150.0  epsilon: 1.0    steps: 19  evaluation reward: 221.6\n",
      "episode: 767   score: 505.0  epsilon: 1.0    steps: 130  evaluation reward: 221.05\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1795: Policy loss: -0.915195. Value loss: 67.165520. Entropy: 0.460557.\n",
      "Iteration 1796: Policy loss: -0.487072. Value loss: 38.918999. Entropy: 0.448418.\n",
      "Iteration 1797: Policy loss: -0.618695. Value loss: 33.800705. Entropy: 0.444032.\n",
      "episode: 768   score: 290.0  epsilon: 1.0    steps: 606  evaluation reward: 222.7\n",
      "episode: 769   score: 365.0  epsilon: 1.0    steps: 997  evaluation reward: 224.15\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1798: Policy loss: 0.927100. Value loss: 44.374512. Entropy: 0.444738.\n",
      "Iteration 1799: Policy loss: 0.959830. Value loss: 30.572844. Entropy: 0.463455.\n",
      "Iteration 1800: Policy loss: 0.869582. Value loss: 25.951565. Entropy: 0.446153.\n",
      "episode: 770   score: 500.0  epsilon: 1.0    steps: 786  evaluation reward: 226.1\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1801: Policy loss: -0.114950. Value loss: 36.788113. Entropy: 0.315756.\n",
      "Iteration 1802: Policy loss: -0.292011. Value loss: 23.174126. Entropy: 0.324885.\n",
      "Iteration 1803: Policy loss: -0.414916. Value loss: 17.173096. Entropy: 0.295835.\n",
      "episode: 771   score: 180.0  epsilon: 1.0    steps: 233  evaluation reward: 226.85\n",
      "episode: 772   score: 200.0  epsilon: 1.0    steps: 288  evaluation reward: 227.5\n",
      "episode: 773   score: 210.0  epsilon: 1.0    steps: 734  evaluation reward: 227.75\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1804: Policy loss: 1.385506. Value loss: 47.350422. Entropy: 0.305399.\n",
      "Iteration 1805: Policy loss: 1.297684. Value loss: 31.117432. Entropy: 0.314931.\n",
      "Iteration 1806: Policy loss: 1.475213. Value loss: 27.352522. Entropy: 0.305695.\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1807: Policy loss: -1.774812. Value loss: 65.035881. Entropy: 0.238090.\n",
      "Iteration 1808: Policy loss: -1.845248. Value loss: 42.099678. Entropy: 0.301474.\n",
      "Iteration 1809: Policy loss: -1.750160. Value loss: 39.550861. Entropy: 0.244783.\n",
      "episode: 774   score: 195.0  epsilon: 1.0    steps: 481  evaluation reward: 228.35\n",
      "Training network. lr: 0.000236. clip: 0.094480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1810: Policy loss: -1.941224. Value loss: 241.812622. Entropy: 0.253425.\n",
      "Iteration 1811: Policy loss: -1.283008. Value loss: 159.414886. Entropy: 0.229384.\n",
      "Iteration 1812: Policy loss: -1.756278. Value loss: 162.342911. Entropy: 0.191980.\n",
      "episode: 775   score: 80.0  epsilon: 1.0    steps: 297  evaluation reward: 226.6\n",
      "episode: 776   score: 210.0  epsilon: 1.0    steps: 926  evaluation reward: 226.85\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1813: Policy loss: 1.087275. Value loss: 54.720272. Entropy: 0.407839.\n",
      "Iteration 1814: Policy loss: 0.920385. Value loss: 25.449926. Entropy: 0.384704.\n",
      "Iteration 1815: Policy loss: 0.983053. Value loss: 20.108599. Entropy: 0.425955.\n",
      "episode: 777   score: 240.0  epsilon: 1.0    steps: 32  evaluation reward: 229.05\n",
      "episode: 778   score: 155.0  epsilon: 1.0    steps: 216  evaluation reward: 229.85\n",
      "episode: 779   score: 210.0  epsilon: 1.0    steps: 618  evaluation reward: 230.4\n",
      "episode: 780   score: 135.0  epsilon: 1.0    steps: 704  evaluation reward: 226.95\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1816: Policy loss: -1.036281. Value loss: 62.363190. Entropy: 0.450824.\n",
      "Iteration 1817: Policy loss: -0.930424. Value loss: 40.332203. Entropy: 0.382793.\n",
      "Iteration 1818: Policy loss: -0.931492. Value loss: 35.856190. Entropy: 0.418933.\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1819: Policy loss: -3.149396. Value loss: 51.102745. Entropy: 0.284264.\n",
      "Iteration 1820: Policy loss: -3.184139. Value loss: 32.584114. Entropy: 0.266545.\n",
      "Iteration 1821: Policy loss: -3.207748. Value loss: 32.181606. Entropy: 0.288883.\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1822: Policy loss: 0.057656. Value loss: 62.485771. Entropy: 0.202187.\n",
      "Iteration 1823: Policy loss: -0.667396. Value loss: 54.083706. Entropy: 0.203176.\n",
      "Iteration 1824: Policy loss: -0.113200. Value loss: 45.757294. Entropy: 0.171383.\n",
      "episode: 781   score: 240.0  epsilon: 1.0    steps: 494  evaluation reward: 224.9\n",
      "episode: 782   score: 155.0  epsilon: 1.0    steps: 763  evaluation reward: 222.75\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1825: Policy loss: 1.406993. Value loss: 31.797031. Entropy: 0.287586.\n",
      "Iteration 1826: Policy loss: 1.536051. Value loss: 24.186378. Entropy: 0.445437.\n",
      "Iteration 1827: Policy loss: 1.384167. Value loss: 24.917805. Entropy: 0.423584.\n",
      "episode: 783   score: 135.0  epsilon: 1.0    steps: 545  evaluation reward: 222.4\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1828: Policy loss: 1.836522. Value loss: 20.209480. Entropy: 0.409880.\n",
      "Iteration 1829: Policy loss: 1.747956. Value loss: 10.648473. Entropy: 0.552312.\n",
      "Iteration 1830: Policy loss: 1.725899. Value loss: 10.210970. Entropy: 0.657308.\n",
      "episode: 784   score: 210.0  epsilon: 1.0    steps: 39  evaluation reward: 219.95\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1831: Policy loss: -0.648193. Value loss: 37.427685. Entropy: 0.428328.\n",
      "Iteration 1832: Policy loss: -1.105815. Value loss: 23.981411. Entropy: 0.433442.\n",
      "Iteration 1833: Policy loss: -0.797017. Value loss: 19.907454. Entropy: 0.457531.\n",
      "episode: 785   score: 285.0  epsilon: 1.0    steps: 319  evaluation reward: 218.7\n",
      "episode: 786   score: 640.0  epsilon: 1.0    steps: 850  evaluation reward: 217.75\n",
      "episode: 787   score: 270.0  epsilon: 1.0    steps: 934  evaluation reward: 217.8\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1834: Policy loss: 3.021189. Value loss: 43.529484. Entropy: 0.758185.\n",
      "Iteration 1835: Policy loss: 3.254025. Value loss: 22.959728. Entropy: 0.756249.\n",
      "Iteration 1836: Policy loss: 3.062813. Value loss: 20.181786. Entropy: 0.743496.\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1837: Policy loss: -0.696610. Value loss: 22.714336. Entropy: 0.397088.\n",
      "Iteration 1838: Policy loss: -0.803304. Value loss: 15.610883. Entropy: 0.351957.\n",
      "Iteration 1839: Policy loss: -0.803654. Value loss: 13.622540. Entropy: 0.434513.\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1840: Policy loss: 0.869756. Value loss: 22.638832. Entropy: 0.496299.\n",
      "Iteration 1841: Policy loss: 1.078790. Value loss: 20.989510. Entropy: 0.468687.\n",
      "Iteration 1842: Policy loss: 0.938486. Value loss: 16.480173. Entropy: 0.519953.\n",
      "episode: 788   score: 310.0  epsilon: 1.0    steps: 145  evaluation reward: 219.7\n",
      "episode: 789   score: 150.0  epsilon: 1.0    steps: 756  evaluation reward: 220.45\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1843: Policy loss: 0.453642. Value loss: 29.002375. Entropy: 0.408813.\n",
      "Iteration 1844: Policy loss: 0.566088. Value loss: 19.048351. Entropy: 0.466296.\n",
      "Iteration 1845: Policy loss: 0.630528. Value loss: 16.302944. Entropy: 0.430644.\n",
      "episode: 790   score: 210.0  epsilon: 1.0    steps: 387  evaluation reward: 219.05\n",
      "episode: 791   score: 180.0  epsilon: 1.0    steps: 525  evaluation reward: 219.05\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1846: Policy loss: -2.542301. Value loss: 185.271118. Entropy: 0.429511.\n",
      "Iteration 1847: Policy loss: -3.006231. Value loss: 104.253227. Entropy: 0.391459.\n",
      "Iteration 1848: Policy loss: -2.561805. Value loss: 42.762096. Entropy: 0.380329.\n",
      "episode: 792   score: 380.0  epsilon: 1.0    steps: 128  evaluation reward: 220.55\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1849: Policy loss: 3.914926. Value loss: 70.800232. Entropy: 0.234407.\n",
      "Iteration 1850: Policy loss: 4.141316. Value loss: 37.672470. Entropy: 0.451178.\n",
      "Iteration 1851: Policy loss: 3.476727. Value loss: 27.873386. Entropy: 0.525177.\n",
      "episode: 793   score: 210.0  epsilon: 1.0    steps: 382  evaluation reward: 221.85\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1852: Policy loss: 1.166921. Value loss: 38.464333. Entropy: 1.065324.\n",
      "Iteration 1853: Policy loss: 1.251828. Value loss: 28.965851. Entropy: 1.024396.\n",
      "Iteration 1854: Policy loss: 1.150769. Value loss: 19.266914. Entropy: 1.042270.\n",
      "episode: 794   score: 110.0  epsilon: 1.0    steps: 706  evaluation reward: 220.2\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1855: Policy loss: 0.083515. Value loss: 64.174011. Entropy: 0.912859.\n",
      "Iteration 1856: Policy loss: 0.013910. Value loss: 47.410702. Entropy: 0.897804.\n",
      "Iteration 1857: Policy loss: -0.124093. Value loss: 40.074554. Entropy: 0.911744.\n",
      "episode: 795   score: 195.0  epsilon: 1.0    steps: 176  evaluation reward: 220.15\n",
      "episode: 796   score: 245.0  epsilon: 1.0    steps: 782  evaluation reward: 221.2\n",
      "episode: 797   score: 250.0  epsilon: 1.0    steps: 899  evaluation reward: 222.65\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1858: Policy loss: 2.472514. Value loss: 40.076435. Entropy: 0.862582.\n",
      "Iteration 1859: Policy loss: 2.335741. Value loss: 23.883335. Entropy: 0.858238.\n",
      "Iteration 1860: Policy loss: 2.275238. Value loss: 19.204996. Entropy: 0.853319.\n",
      "episode: 798   score: 60.0  epsilon: 1.0    steps: 3  evaluation reward: 221.15\n",
      "episode: 799   score: 270.0  epsilon: 1.0    steps: 471  evaluation reward: 221.0\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1861: Policy loss: 1.009351. Value loss: 40.014961. Entropy: 0.896204.\n",
      "Iteration 1862: Policy loss: 0.656937. Value loss: 30.804312. Entropy: 0.871047.\n",
      "Iteration 1863: Policy loss: 0.800181. Value loss: 26.743237. Entropy: 0.901876.\n",
      "episode: 800   score: 85.0  epsilon: 1.0    steps: 349  evaluation reward: 219.75\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1864: Policy loss: -0.512998. Value loss: 32.653004. Entropy: 0.866140.\n",
      "Iteration 1865: Policy loss: -0.816710. Value loss: 23.521912. Entropy: 0.829515.\n",
      "Iteration 1866: Policy loss: -0.666096. Value loss: 20.281885. Entropy: 0.825921.\n",
      "now time :  2019-02-26 13:02:33.941762\n",
      "episode: 801   score: 160.0  epsilon: 1.0    steps: 527  evaluation reward: 218.45\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1867: Policy loss: -2.315453. Value loss: 288.099457. Entropy: 0.934922.\n",
      "Iteration 1868: Policy loss: -2.217544. Value loss: 137.342438. Entropy: 0.991541.\n",
      "Iteration 1869: Policy loss: -1.571184. Value loss: 119.156052. Entropy: 0.994434.\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1870: Policy loss: -0.078559. Value loss: 66.653084. Entropy: 0.767773.\n",
      "Iteration 1871: Policy loss: -0.023397. Value loss: 51.120586. Entropy: 0.764372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1872: Policy loss: -0.496429. Value loss: 43.581467. Entropy: 0.721432.\n",
      "episode: 802   score: 145.0  epsilon: 1.0    steps: 126  evaluation reward: 217.8\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1873: Policy loss: -1.435789. Value loss: 61.940807. Entropy: 1.022902.\n",
      "Iteration 1874: Policy loss: -1.115221. Value loss: 40.345119. Entropy: 1.047972.\n",
      "Iteration 1875: Policy loss: -1.118605. Value loss: 31.423044. Entropy: 1.025760.\n",
      "episode: 803   score: 300.0  epsilon: 1.0    steps: 253  evaluation reward: 218.7\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1876: Policy loss: -3.714795. Value loss: 60.878227. Entropy: 0.991863.\n",
      "Iteration 1877: Policy loss: -3.457717. Value loss: 39.995323. Entropy: 0.974983.\n",
      "Iteration 1878: Policy loss: -3.709589. Value loss: 31.510607. Entropy: 0.925961.\n",
      "episode: 804   score: 180.0  epsilon: 1.0    steps: 366  evaluation reward: 216.95\n",
      "episode: 805   score: 350.0  epsilon: 1.0    steps: 809  evaluation reward: 218.9\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1879: Policy loss: -1.663503. Value loss: 434.528381. Entropy: 0.981393.\n",
      "Iteration 1880: Policy loss: -0.867124. Value loss: 317.752991. Entropy: 0.967772.\n",
      "Iteration 1881: Policy loss: -2.248312. Value loss: 265.225830. Entropy: 0.952823.\n",
      "episode: 806   score: 510.0  epsilon: 1.0    steps: 901  evaluation reward: 222.95\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1882: Policy loss: 1.718525. Value loss: 45.019760. Entropy: 1.109224.\n",
      "Iteration 1883: Policy loss: 2.265587. Value loss: 39.274071. Entropy: 1.058431.\n",
      "Iteration 1884: Policy loss: 2.466526. Value loss: 32.951271. Entropy: 1.052731.\n",
      "episode: 807   score: 360.0  epsilon: 1.0    steps: 415  evaluation reward: 223.85\n",
      "episode: 808   score: 180.0  epsilon: 1.0    steps: 526  evaluation reward: 223.55\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1885: Policy loss: 0.941801. Value loss: 55.019047. Entropy: 1.022980.\n",
      "Iteration 1886: Policy loss: 0.846182. Value loss: 39.371685. Entropy: 1.069831.\n",
      "Iteration 1887: Policy loss: 0.769246. Value loss: 34.144127. Entropy: 1.034747.\n",
      "episode: 809   score: 95.0  epsilon: 1.0    steps: 10  evaluation reward: 223.45\n",
      "episode: 810   score: 925.0  epsilon: 1.0    steps: 665  evaluation reward: 228.3\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1888: Policy loss: 1.754244. Value loss: 56.495014. Entropy: 0.981287.\n",
      "Iteration 1889: Policy loss: 1.659047. Value loss: 39.365078. Entropy: 0.978327.\n",
      "Iteration 1890: Policy loss: 2.244910. Value loss: 30.063786. Entropy: 0.810304.\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1891: Policy loss: 2.174227. Value loss: 43.623684. Entropy: 0.788562.\n",
      "Iteration 1892: Policy loss: 2.150500. Value loss: 31.105618. Entropy: 0.701740.\n",
      "Iteration 1893: Policy loss: 2.098575. Value loss: 24.881136. Entropy: 0.752500.\n",
      "episode: 811   score: 145.0  epsilon: 1.0    steps: 175  evaluation reward: 227.65\n",
      "episode: 812   score: 145.0  epsilon: 1.0    steps: 257  evaluation reward: 227.3\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1894: Policy loss: 1.042945. Value loss: 30.910294. Entropy: 0.891387.\n",
      "Iteration 1895: Policy loss: 0.636528. Value loss: 24.524542. Entropy: 0.910818.\n",
      "Iteration 1896: Policy loss: 0.933426. Value loss: 21.909891. Entropy: 0.895997.\n",
      "episode: 813   score: 195.0  epsilon: 1.0    steps: 126  evaluation reward: 227.9\n",
      "episode: 814   score: 70.0  epsilon: 1.0    steps: 414  evaluation reward: 225.95\n",
      "episode: 815   score: 175.0  epsilon: 1.0    steps: 968  evaluation reward: 225.6\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1897: Policy loss: -1.707167. Value loss: 255.016312. Entropy: 0.623303.\n",
      "Iteration 1898: Policy loss: -1.291258. Value loss: 172.503082. Entropy: 0.657762.\n",
      "Iteration 1899: Policy loss: -1.731118. Value loss: 195.425903. Entropy: 0.567406.\n",
      "episode: 816   score: 220.0  epsilon: 1.0    steps: 513  evaluation reward: 226.6\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1900: Policy loss: 1.692807. Value loss: 43.421616. Entropy: 0.477931.\n",
      "Iteration 1901: Policy loss: 1.374497. Value loss: 29.493301. Entropy: 0.669955.\n",
      "Iteration 1902: Policy loss: 1.506837. Value loss: 25.840139. Entropy: 0.690806.\n",
      "episode: 817   score: 135.0  epsilon: 1.0    steps: 184  evaluation reward: 226.9\n",
      "episode: 818   score: 155.0  epsilon: 1.0    steps: 695  evaluation reward: 226.9\n",
      "episode: 819   score: 470.0  epsilon: 1.0    steps: 864  evaluation reward: 230.25\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1903: Policy loss: 4.943844. Value loss: 103.247429. Entropy: 0.649389.\n",
      "Iteration 1904: Policy loss: 5.455202. Value loss: 70.557449. Entropy: 0.669915.\n",
      "Iteration 1905: Policy loss: 4.806135. Value loss: 60.786762. Entropy: 0.641000.\n",
      "episode: 820   score: 155.0  epsilon: 1.0    steps: 370  evaluation reward: 230.75\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1906: Policy loss: 0.003316. Value loss: 48.180702. Entropy: 0.677429.\n",
      "Iteration 1907: Policy loss: 0.004679. Value loss: 30.018591. Entropy: 0.656686.\n",
      "Iteration 1908: Policy loss: 0.243923. Value loss: 22.526251. Entropy: 0.596290.\n",
      "episode: 821   score: 210.0  epsilon: 1.0    steps: 401  evaluation reward: 231.5\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1909: Policy loss: 0.136489. Value loss: 35.523922. Entropy: 0.810988.\n",
      "Iteration 1910: Policy loss: -0.157209. Value loss: 28.728050. Entropy: 0.809048.\n",
      "Iteration 1911: Policy loss: 0.255560. Value loss: 21.262327. Entropy: 0.795300.\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1912: Policy loss: 0.623782. Value loss: 23.233891. Entropy: 0.784708.\n",
      "Iteration 1913: Policy loss: 0.597730. Value loss: 14.410548. Entropy: 0.801297.\n",
      "Iteration 1914: Policy loss: 0.645409. Value loss: 11.868658. Entropy: 0.794680.\n",
      "episode: 822   score: 75.0  epsilon: 1.0    steps: 137  evaluation reward: 229.55\n",
      "episode: 823   score: 155.0  epsilon: 1.0    steps: 968  evaluation reward: 229.0\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1915: Policy loss: 0.701709. Value loss: 11.475374. Entropy: 0.760342.\n",
      "Iteration 1916: Policy loss: 0.924664. Value loss: 8.833952. Entropy: 0.792493.\n",
      "Iteration 1917: Policy loss: 0.609857. Value loss: 8.888652. Entropy: 0.807888.\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1918: Policy loss: -3.489681. Value loss: 243.408081. Entropy: 0.440841.\n",
      "Iteration 1919: Policy loss: -3.505306. Value loss: 188.232208. Entropy: 0.402863.\n",
      "Iteration 1920: Policy loss: -3.503747. Value loss: 166.600418. Entropy: 0.387695.\n",
      "episode: 824   score: 290.0  epsilon: 1.0    steps: 600  evaluation reward: 230.1\n",
      "episode: 825   score: 210.0  epsilon: 1.0    steps: 702  evaluation reward: 230.4\n",
      "episode: 826   score: 180.0  epsilon: 1.0    steps: 893  evaluation reward: 230.4\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1921: Policy loss: -0.255964. Value loss: 35.817551. Entropy: 0.633458.\n",
      "Iteration 1922: Policy loss: -0.458417. Value loss: 21.271486. Entropy: 0.650768.\n",
      "Iteration 1923: Policy loss: -0.457265. Value loss: 21.005785. Entropy: 0.694811.\n",
      "episode: 827   score: 300.0  epsilon: 1.0    steps: 73  evaluation reward: 228.55\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1924: Policy loss: -2.005988. Value loss: 232.989761. Entropy: 0.686033.\n",
      "Iteration 1925: Policy loss: -2.615635. Value loss: 199.947525. Entropy: 0.603757.\n",
      "Iteration 1926: Policy loss: -2.571326. Value loss: 127.111153. Entropy: 0.549836.\n",
      "episode: 828   score: 440.0  epsilon: 1.0    steps: 336  evaluation reward: 230.85\n",
      "episode: 829   score: 410.0  epsilon: 1.0    steps: 496  evaluation reward: 233.9\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1927: Policy loss: 0.874823. Value loss: 30.012432. Entropy: 0.702256.\n",
      "Iteration 1928: Policy loss: 1.125742. Value loss: 23.240149. Entropy: 0.763606.\n",
      "Iteration 1929: Policy loss: 0.887607. Value loss: 22.969524. Entropy: 0.763337.\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1930: Policy loss: -0.005950. Value loss: 223.411484. Entropy: 0.739358.\n",
      "Iteration 1931: Policy loss: 0.002896. Value loss: 172.104736. Entropy: 0.692331.\n",
      "Iteration 1932: Policy loss: 0.577673. Value loss: 133.260681. Entropy: 0.703179.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 830   score: 155.0  epsilon: 1.0    steps: 1013  evaluation reward: 234.7\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1933: Policy loss: 3.493282. Value loss: 55.385120. Entropy: 0.577141.\n",
      "Iteration 1934: Policy loss: 3.693843. Value loss: 29.503822. Entropy: 0.553455.\n",
      "Iteration 1935: Policy loss: 3.545416. Value loss: 23.241152. Entropy: 0.602404.\n",
      "episode: 831   score: 440.0  epsilon: 1.0    steps: 150  evaluation reward: 235.75\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1936: Policy loss: 2.955927. Value loss: 44.844292. Entropy: 0.697165.\n",
      "Iteration 1937: Policy loss: 3.016835. Value loss: 30.753458. Entropy: 0.627177.\n",
      "Iteration 1938: Policy loss: 3.162304. Value loss: 26.364586. Entropy: 0.671633.\n",
      "episode: 832   score: 105.0  epsilon: 1.0    steps: 489  evaluation reward: 236.05\n",
      "episode: 833   score: 215.0  epsilon: 1.0    steps: 755  evaluation reward: 234.4\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1939: Policy loss: 2.294163. Value loss: 36.749519. Entropy: 0.563971.\n",
      "Iteration 1940: Policy loss: 1.879342. Value loss: 21.377630. Entropy: 0.561440.\n",
      "Iteration 1941: Policy loss: 1.818720. Value loss: 20.809538. Entropy: 0.563102.\n",
      "episode: 834   score: 225.0  epsilon: 1.0    steps: 624  evaluation reward: 234.05\n",
      "episode: 835   score: 135.0  epsilon: 1.0    steps: 780  evaluation reward: 233.3\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1942: Policy loss: -0.593262. Value loss: 49.520794. Entropy: 0.639562.\n",
      "Iteration 1943: Policy loss: -0.090909. Value loss: 33.629856. Entropy: 0.647782.\n",
      "Iteration 1944: Policy loss: -0.405919. Value loss: 27.723888. Entropy: 0.668029.\n",
      "episode: 836   score: 210.0  epsilon: 1.0    steps: 22  evaluation reward: 232.1\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1945: Policy loss: -0.759290. Value loss: 202.375275. Entropy: 0.597907.\n",
      "Iteration 1946: Policy loss: -1.251475. Value loss: 183.486664. Entropy: 0.569687.\n",
      "Iteration 1947: Policy loss: -0.513678. Value loss: 125.358078. Entropy: 0.585728.\n",
      "episode: 837   score: 210.0  epsilon: 1.0    steps: 291  evaluation reward: 232.4\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1948: Policy loss: 0.094935. Value loss: 37.002197. Entropy: 0.587113.\n",
      "Iteration 1949: Policy loss: 0.258691. Value loss: 26.574516. Entropy: 0.596701.\n",
      "Iteration 1950: Policy loss: 0.205468. Value loss: 23.431030. Entropy: 0.603490.\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1951: Policy loss: -0.926148. Value loss: 71.920837. Entropy: 0.627461.\n",
      "Iteration 1952: Policy loss: -0.845170. Value loss: 40.165649. Entropy: 0.699409.\n",
      "Iteration 1953: Policy loss: -1.301839. Value loss: 34.203236. Entropy: 0.691207.\n",
      "episode: 838   score: 185.0  epsilon: 1.0    steps: 904  evaluation reward: 232.9\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1954: Policy loss: 0.575205. Value loss: 45.883999. Entropy: 0.731819.\n",
      "Iteration 1955: Policy loss: 0.886225. Value loss: 32.726154. Entropy: 0.733713.\n",
      "Iteration 1956: Policy loss: 0.389815. Value loss: 24.063166. Entropy: 0.712711.\n",
      "episode: 839   score: 460.0  epsilon: 1.0    steps: 231  evaluation reward: 235.95\n",
      "episode: 840   score: 240.0  epsilon: 1.0    steps: 396  evaluation reward: 236.05\n",
      "episode: 841   score: 210.0  epsilon: 1.0    steps: 848  evaluation reward: 237.1\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1957: Policy loss: -0.501468. Value loss: 211.993408. Entropy: 0.745398.\n",
      "Iteration 1958: Policy loss: -1.402531. Value loss: 225.688309. Entropy: 0.725489.\n",
      "Iteration 1959: Policy loss: -0.391688. Value loss: 143.750458. Entropy: 0.701032.\n",
      "episode: 842   score: 225.0  epsilon: 1.0    steps: 746  evaluation reward: 238.0\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1960: Policy loss: 0.770812. Value loss: 18.835396. Entropy: 0.782811.\n",
      "Iteration 1961: Policy loss: 0.543660. Value loss: 12.183987. Entropy: 0.754455.\n",
      "Iteration 1962: Policy loss: 0.583513. Value loss: 9.791984. Entropy: 0.767798.\n",
      "episode: 843   score: 370.0  epsilon: 1.0    steps: 322  evaluation reward: 240.45\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1963: Policy loss: -1.807388. Value loss: 28.469217. Entropy: 0.668262.\n",
      "Iteration 1964: Policy loss: -1.804860. Value loss: 17.644089. Entropy: 0.635386.\n",
      "Iteration 1965: Policy loss: -1.818694. Value loss: 14.651789. Entropy: 0.631353.\n",
      "episode: 844   score: 220.0  epsilon: 1.0    steps: 27  evaluation reward: 238.85\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1966: Policy loss: 1.570518. Value loss: 34.669598. Entropy: 0.654656.\n",
      "Iteration 1967: Policy loss: 1.614756. Value loss: 19.287096. Entropy: 0.680805.\n",
      "Iteration 1968: Policy loss: 1.436580. Value loss: 14.657662. Entropy: 0.812571.\n",
      "episode: 845   score: 335.0  epsilon: 1.0    steps: 955  evaluation reward: 240.4\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1969: Policy loss: -1.332389. Value loss: 225.404373. Entropy: 0.923500.\n",
      "Iteration 1970: Policy loss: -0.928494. Value loss: 111.166100. Entropy: 0.846641.\n",
      "Iteration 1971: Policy loss: -0.378474. Value loss: 70.470703. Entropy: 0.865680.\n",
      "episode: 846   score: 215.0  epsilon: 1.0    steps: 533  evaluation reward: 240.3\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1972: Policy loss: -2.818455. Value loss: 43.796791. Entropy: 1.041998.\n",
      "Iteration 1973: Policy loss: -2.825676. Value loss: 22.796560. Entropy: 1.045707.\n",
      "Iteration 1974: Policy loss: -2.715149. Value loss: 19.592741. Entropy: 1.042873.\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1975: Policy loss: 2.376805. Value loss: 86.128082. Entropy: 0.869014.\n",
      "Iteration 1976: Policy loss: 2.648112. Value loss: 38.693584. Entropy: 0.907517.\n",
      "Iteration 1977: Policy loss: 1.953515. Value loss: 33.875675. Entropy: 0.942017.\n",
      "episode: 847   score: 210.0  epsilon: 1.0    steps: 186  evaluation reward: 241.0\n",
      "episode: 848   score: 290.0  epsilon: 1.0    steps: 842  evaluation reward: 239.3\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1978: Policy loss: -0.946184. Value loss: 239.639328. Entropy: 0.942326.\n",
      "Iteration 1979: Policy loss: -0.702963. Value loss: 158.060135. Entropy: 0.894807.\n",
      "Iteration 1980: Policy loss: -1.033493. Value loss: 161.230392. Entropy: 0.870171.\n",
      "episode: 849   score: 285.0  epsilon: 1.0    steps: 403  evaluation reward: 240.9\n",
      "episode: 850   score: 155.0  epsilon: 1.0    steps: 630  evaluation reward: 240.25\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1981: Policy loss: -1.303168. Value loss: 32.209690. Entropy: 0.663685.\n",
      "Iteration 1982: Policy loss: -1.301084. Value loss: 22.758373. Entropy: 0.664006.\n",
      "Iteration 1983: Policy loss: -1.306456. Value loss: 20.409790. Entropy: 0.654364.\n",
      "now time :  2019-02-26 13:04:45.276895\n",
      "episode: 851   score: 215.0  epsilon: 1.0    steps: 117  evaluation reward: 241.9\n",
      "episode: 852   score: 240.0  epsilon: 1.0    steps: 691  evaluation reward: 242.9\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1984: Policy loss: 1.466384. Value loss: 48.773869. Entropy: 0.759785.\n",
      "Iteration 1985: Policy loss: 1.413765. Value loss: 33.299068. Entropy: 0.814405.\n",
      "Iteration 1986: Policy loss: 1.180622. Value loss: 30.353252. Entropy: 0.799388.\n",
      "episode: 853   score: 320.0  epsilon: 1.0    steps: 285  evaluation reward: 238.55\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1987: Policy loss: 0.352611. Value loss: 32.712460. Entropy: 0.681346.\n",
      "Iteration 1988: Policy loss: 0.251593. Value loss: 24.074438. Entropy: 0.688409.\n",
      "Iteration 1989: Policy loss: 0.309835. Value loss: 20.925085. Entropy: 0.671955.\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1990: Policy loss: -0.291965. Value loss: 28.163212. Entropy: 0.655939.\n",
      "Iteration 1991: Policy loss: -0.375283. Value loss: 21.852831. Entropy: 0.648634.\n",
      "Iteration 1992: Policy loss: -0.382348. Value loss: 19.571905. Entropy: 0.642083.\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1993: Policy loss: -2.484473. Value loss: 279.985260. Entropy: 0.745905.\n",
      "Iteration 1994: Policy loss: -1.367997. Value loss: 154.001389. Entropy: 0.679001.\n",
      "Iteration 1995: Policy loss: -2.040408. Value loss: 155.552902. Entropy: 0.669270.\n",
      "episode: 854   score: 210.0  epsilon: 1.0    steps: 213  evaluation reward: 239.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1996: Policy loss: 2.059394. Value loss: 59.245068. Entropy: 0.748718.\n",
      "Iteration 1997: Policy loss: 2.273056. Value loss: 35.222099. Entropy: 0.803460.\n",
      "Iteration 1998: Policy loss: 2.296624. Value loss: 30.952427. Entropy: 0.830147.\n",
      "episode: 855   score: 105.0  epsilon: 1.0    steps: 116  evaluation reward: 238.0\n",
      "episode: 856   score: 180.0  epsilon: 1.0    steps: 597  evaluation reward: 239.35\n",
      "episode: 857   score: 185.0  epsilon: 1.0    steps: 818  evaluation reward: 238.55\n",
      "episode: 858   score: 660.0  epsilon: 1.0    steps: 942  evaluation reward: 242.0\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1999: Policy loss: 2.138669. Value loss: 52.366707. Entropy: 0.863845.\n",
      "Iteration 2000: Policy loss: 2.494306. Value loss: 38.720352. Entropy: 0.885465.\n",
      "Iteration 2001: Policy loss: 2.308179. Value loss: 29.487810. Entropy: 0.887828.\n",
      "episode: 859   score: 120.0  epsilon: 1.0    steps: 352  evaluation reward: 240.4\n",
      "episode: 860   score: 225.0  epsilon: 1.0    steps: 416  evaluation reward: 241.0\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2002: Policy loss: -0.342571. Value loss: 33.648659. Entropy: 0.601741.\n",
      "Iteration 2003: Policy loss: -0.576604. Value loss: 24.454620. Entropy: 0.553349.\n",
      "Iteration 2004: Policy loss: -0.503553. Value loss: 20.181644. Entropy: 0.577066.\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2005: Policy loss: 0.002330. Value loss: 54.476223. Entropy: 0.582884.\n",
      "Iteration 2006: Policy loss: -0.135983. Value loss: 33.460537. Entropy: 0.590876.\n",
      "Iteration 2007: Policy loss: 0.308833. Value loss: 26.535498. Entropy: 0.592036.\n",
      "episode: 861   score: 315.0  epsilon: 1.0    steps: 732  evaluation reward: 241.55\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2008: Policy loss: 4.033789. Value loss: 70.726952. Entropy: 0.746327.\n",
      "Iteration 2009: Policy loss: 3.750562. Value loss: 40.785236. Entropy: 0.838521.\n",
      "Iteration 2010: Policy loss: 3.710320. Value loss: 33.427418. Entropy: 0.743071.\n",
      "episode: 862   score: 120.0  epsilon: 1.0    steps: 194  evaluation reward: 240.2\n",
      "episode: 863   score: 160.0  epsilon: 1.0    steps: 808  evaluation reward: 240.6\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2011: Policy loss: 0.554319. Value loss: 25.774956. Entropy: 0.485754.\n",
      "Iteration 2012: Policy loss: 0.462192. Value loss: 18.547531. Entropy: 0.466080.\n",
      "Iteration 2013: Policy loss: 0.602029. Value loss: 13.749794. Entropy: 0.492765.\n",
      "episode: 864   score: 230.0  epsilon: 1.0    steps: 632  evaluation reward: 240.6\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2014: Policy loss: -1.687894. Value loss: 155.056244. Entropy: 0.469742.\n",
      "Iteration 2015: Policy loss: -0.843266. Value loss: 44.414509. Entropy: 0.421429.\n",
      "Iteration 2016: Policy loss: -2.179527. Value loss: 35.287258. Entropy: 0.428671.\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2017: Policy loss: 3.251980. Value loss: 80.950325. Entropy: 0.593244.\n",
      "Iteration 2018: Policy loss: 3.576179. Value loss: 42.006691. Entropy: 0.535557.\n",
      "Iteration 2019: Policy loss: 3.872892. Value loss: 34.391972. Entropy: 0.550066.\n",
      "episode: 865   score: 180.0  epsilon: 1.0    steps: 56  evaluation reward: 241.15\n",
      "episode: 866   score: 220.0  epsilon: 1.0    steps: 915  evaluation reward: 241.85\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2020: Policy loss: 4.214394. Value loss: 42.182247. Entropy: 0.645901.\n",
      "Iteration 2021: Policy loss: 4.064283. Value loss: 23.966991. Entropy: 0.678114.\n",
      "Iteration 2022: Policy loss: 4.169022. Value loss: 20.471371. Entropy: 0.672168.\n",
      "episode: 867   score: 485.0  epsilon: 1.0    steps: 459  evaluation reward: 241.65\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2023: Policy loss: -1.378002. Value loss: 39.306595. Entropy: 0.572959.\n",
      "Iteration 2024: Policy loss: -1.439822. Value loss: 26.246706. Entropy: 0.578225.\n",
      "Iteration 2025: Policy loss: -1.453352. Value loss: 23.535418. Entropy: 0.582199.\n",
      "episode: 868   score: 180.0  epsilon: 1.0    steps: 867  evaluation reward: 240.55\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2026: Policy loss: 4.146623. Value loss: 23.751717. Entropy: 0.664805.\n",
      "Iteration 2027: Policy loss: 4.097099. Value loss: 15.995810. Entropy: 0.750927.\n",
      "Iteration 2028: Policy loss: 4.102298. Value loss: 13.273092. Entropy: 0.756006.\n",
      "episode: 869   score: 235.0  epsilon: 1.0    steps: 325  evaluation reward: 239.25\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2029: Policy loss: 0.284644. Value loss: 27.468092. Entropy: 0.676105.\n",
      "Iteration 2030: Policy loss: 0.125105. Value loss: 19.374922. Entropy: 0.652878.\n",
      "Iteration 2031: Policy loss: 0.395930. Value loss: 18.430803. Entropy: 0.672151.\n",
      "episode: 870   score: 260.0  epsilon: 1.0    steps: 732  evaluation reward: 236.85\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2032: Policy loss: -0.031329. Value loss: 23.339369. Entropy: 0.713933.\n",
      "Iteration 2033: Policy loss: 0.196136. Value loss: 17.475756. Entropy: 0.720104.\n",
      "Iteration 2034: Policy loss: 0.034742. Value loss: 14.402475. Entropy: 0.692520.\n",
      "episode: 871   score: 210.0  epsilon: 1.0    steps: 549  evaluation reward: 237.15\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2035: Policy loss: 0.725021. Value loss: 63.167709. Entropy: 0.732227.\n",
      "Iteration 2036: Policy loss: 0.588725. Value loss: 55.047062. Entropy: 0.760920.\n",
      "Iteration 2037: Policy loss: 0.684663. Value loss: 51.093483. Entropy: 0.769542.\n",
      "episode: 872   score: 210.0  epsilon: 1.0    steps: 99  evaluation reward: 237.25\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2038: Policy loss: -0.606962. Value loss: 19.230284. Entropy: 0.688078.\n",
      "Iteration 2039: Policy loss: -0.516904. Value loss: 14.916494. Entropy: 0.720851.\n",
      "Iteration 2040: Policy loss: -0.586516. Value loss: 13.747394. Entropy: 0.717162.\n",
      "episode: 873   score: 335.0  epsilon: 1.0    steps: 221  evaluation reward: 238.5\n",
      "episode: 874   score: 460.0  epsilon: 1.0    steps: 1002  evaluation reward: 241.15\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2041: Policy loss: 1.019952. Value loss: 16.431278. Entropy: 0.651720.\n",
      "Iteration 2042: Policy loss: 0.932934. Value loss: 12.003464. Entropy: 0.636660.\n",
      "Iteration 2043: Policy loss: 0.889258. Value loss: 9.784598. Entropy: 0.676934.\n",
      "episode: 875   score: 105.0  epsilon: 1.0    steps: 298  evaluation reward: 241.4\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2044: Policy loss: -1.256549. Value loss: 22.238018. Entropy: 0.636909.\n",
      "Iteration 2045: Policy loss: -1.053465. Value loss: 20.176666. Entropy: 0.609348.\n",
      "Iteration 2046: Policy loss: -1.325518. Value loss: 15.928658. Entropy: 0.607281.\n",
      "episode: 876   score: 285.0  epsilon: 1.0    steps: 396  evaluation reward: 242.15\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2047: Policy loss: -0.885991. Value loss: 22.795532. Entropy: 0.532187.\n",
      "Iteration 2048: Policy loss: -1.468719. Value loss: 20.878712. Entropy: 0.568056.\n",
      "Iteration 2049: Policy loss: -1.068829. Value loss: 15.658747. Entropy: 0.529457.\n",
      "episode: 877   score: 180.0  epsilon: 1.0    steps: 767  evaluation reward: 241.55\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2050: Policy loss: 1.586456. Value loss: 20.968517. Entropy: 0.614109.\n",
      "Iteration 2051: Policy loss: 1.719688. Value loss: 16.101873. Entropy: 0.705409.\n",
      "Iteration 2052: Policy loss: 1.651653. Value loss: 13.046089. Entropy: 0.663142.\n",
      "episode: 878   score: 300.0  epsilon: 1.0    steps: 823  evaluation reward: 243.0\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2053: Policy loss: 1.444730. Value loss: 16.094206. Entropy: 0.660105.\n",
      "Iteration 2054: Policy loss: 1.420791. Value loss: 9.572853. Entropy: 0.672071.\n",
      "Iteration 2055: Policy loss: 1.403006. Value loss: 8.306808. Entropy: 0.653490.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2056: Policy loss: -2.269029. Value loss: 210.223770. Entropy: 0.549028.\n",
      "Iteration 2057: Policy loss: -1.402146. Value loss: 58.863998. Entropy: 0.532395.\n",
      "Iteration 2058: Policy loss: -2.575885. Value loss: 63.558372. Entropy: 0.483886.\n",
      "episode: 879   score: 490.0  epsilon: 1.0    steps: 632  evaluation reward: 245.8\n",
      "Training network. lr: 0.000234. clip: 0.093705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2059: Policy loss: 1.160418. Value loss: 45.023956. Entropy: 0.681939.\n",
      "Iteration 2060: Policy loss: 0.716431. Value loss: 35.549557. Entropy: 0.683526.\n",
      "Iteration 2061: Policy loss: 1.114015. Value loss: 27.411850. Entropy: 0.698455.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2062: Policy loss: 2.207444. Value loss: 36.869732. Entropy: 0.663849.\n",
      "Iteration 2063: Policy loss: 2.530267. Value loss: 22.720463. Entropy: 0.672217.\n",
      "Iteration 2064: Policy loss: 2.516617. Value loss: 16.771582. Entropy: 0.619695.\n",
      "episode: 880   score: 320.0  epsilon: 1.0    steps: 32  evaluation reward: 247.65\n",
      "episode: 881   score: 265.0  epsilon: 1.0    steps: 989  evaluation reward: 247.9\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2065: Policy loss: 1.326175. Value loss: 32.149227. Entropy: 0.598219.\n",
      "Iteration 2066: Policy loss: 1.337972. Value loss: 22.005419. Entropy: 0.627605.\n",
      "Iteration 2067: Policy loss: 1.583149. Value loss: 21.979647. Entropy: 0.624625.\n",
      "episode: 882   score: 325.0  epsilon: 1.0    steps: 363  evaluation reward: 249.6\n",
      "episode: 883   score: 105.0  epsilon: 1.0    steps: 655  evaluation reward: 249.3\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2068: Policy loss: 0.004011. Value loss: 23.384441. Entropy: 0.680470.\n",
      "Iteration 2069: Policy loss: -0.061071. Value loss: 18.951550. Entropy: 0.658610.\n",
      "Iteration 2070: Policy loss: 0.047139. Value loss: 16.061577. Entropy: 0.679454.\n",
      "episode: 884   score: 290.0  epsilon: 1.0    steps: 470  evaluation reward: 250.1\n",
      "episode: 885   score: 120.0  epsilon: 1.0    steps: 790  evaluation reward: 248.45\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2071: Policy loss: 0.455173. Value loss: 39.197517. Entropy: 0.518393.\n",
      "Iteration 2072: Policy loss: 0.464432. Value loss: 20.653524. Entropy: 0.469279.\n",
      "Iteration 2073: Policy loss: 0.477956. Value loss: 17.197859. Entropy: 0.483493.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2074: Policy loss: -0.929450. Value loss: 28.444324. Entropy: 0.291075.\n",
      "Iteration 2075: Policy loss: -0.862014. Value loss: 17.151804. Entropy: 0.313157.\n",
      "Iteration 2076: Policy loss: -0.818308. Value loss: 12.469597. Entropy: 0.321918.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2077: Policy loss: -0.814023. Value loss: 17.901440. Entropy: 0.419753.\n",
      "Iteration 2078: Policy loss: -1.006042. Value loss: 15.168588. Entropy: 0.402995.\n",
      "Iteration 2079: Policy loss: -0.932824. Value loss: 11.564919. Entropy: 0.449238.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2080: Policy loss: 1.758673. Value loss: 19.920938. Entropy: 0.522510.\n",
      "Iteration 2081: Policy loss: 1.653864. Value loss: 10.288836. Entropy: 0.537073.\n",
      "Iteration 2082: Policy loss: 1.593524. Value loss: 8.610701. Entropy: 0.551066.\n",
      "episode: 886   score: 210.0  epsilon: 1.0    steps: 64  evaluation reward: 244.15\n",
      "episode: 887   score: 470.0  epsilon: 1.0    steps: 163  evaluation reward: 246.15\n",
      "episode: 888   score: 105.0  epsilon: 1.0    steps: 688  evaluation reward: 244.1\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2083: Policy loss: 1.365939. Value loss: 21.013496. Entropy: 0.673157.\n",
      "Iteration 2084: Policy loss: 1.248464. Value loss: 12.790930. Entropy: 0.684713.\n",
      "Iteration 2085: Policy loss: 1.160262. Value loss: 10.115947. Entropy: 0.689162.\n",
      "episode: 889   score: 175.0  epsilon: 1.0    steps: 633  evaluation reward: 244.35\n",
      "episode: 890   score: 105.0  epsilon: 1.0    steps: 811  evaluation reward: 243.3\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2086: Policy loss: 3.418991. Value loss: 16.993990. Entropy: 0.695540.\n",
      "Iteration 2087: Policy loss: 3.439984. Value loss: 8.878080. Entropy: 0.681743.\n",
      "Iteration 2088: Policy loss: 3.444174. Value loss: 6.477755. Entropy: 0.679054.\n",
      "episode: 891   score: 110.0  epsilon: 1.0    steps: 387  evaluation reward: 242.6\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2089: Policy loss: 0.128799. Value loss: 17.354197. Entropy: 0.524658.\n",
      "Iteration 2090: Policy loss: 0.184928. Value loss: 14.256951. Entropy: 0.493993.\n",
      "Iteration 2091: Policy loss: 0.197484. Value loss: 12.169133. Entropy: 0.507544.\n",
      "episode: 892   score: 115.0  epsilon: 1.0    steps: 956  evaluation reward: 239.95\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2092: Policy loss: 0.698966. Value loss: 19.521187. Entropy: 0.417185.\n",
      "Iteration 2093: Policy loss: 0.952451. Value loss: 13.276129. Entropy: 0.378697.\n",
      "Iteration 2094: Policy loss: 0.614227. Value loss: 11.285619. Entropy: 0.394083.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2095: Policy loss: 0.028477. Value loss: 16.344288. Entropy: 0.326445.\n",
      "Iteration 2096: Policy loss: 0.261096. Value loss: 12.881834. Entropy: 0.378484.\n",
      "Iteration 2097: Policy loss: 0.213304. Value loss: 9.789735. Entropy: 0.346783.\n",
      "episode: 893   score: 380.0  epsilon: 1.0    steps: 339  evaluation reward: 241.65\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2098: Policy loss: -3.745328. Value loss: 279.694458. Entropy: 0.548822.\n",
      "Iteration 2099: Policy loss: -2.901931. Value loss: 161.202423. Entropy: 0.506987.\n",
      "Iteration 2100: Policy loss: -3.348552. Value loss: 163.868378. Entropy: 0.503350.\n",
      "episode: 894   score: 210.0  epsilon: 1.0    steps: 123  evaluation reward: 242.65\n",
      "episode: 895   score: 190.0  epsilon: 1.0    steps: 743  evaluation reward: 242.6\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2101: Policy loss: 0.208702. Value loss: 19.852217. Entropy: 0.400857.\n",
      "Iteration 2102: Policy loss: 0.027914. Value loss: 14.954727. Entropy: 0.442182.\n",
      "Iteration 2103: Policy loss: 0.116463. Value loss: 13.236335. Entropy: 0.444991.\n",
      "episode: 896   score: 135.0  epsilon: 1.0    steps: 519  evaluation reward: 241.5\n",
      "episode: 897   score: 180.0  epsilon: 1.0    steps: 861  evaluation reward: 240.8\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2104: Policy loss: 0.019117. Value loss: 20.177471. Entropy: 0.409148.\n",
      "Iteration 2105: Policy loss: 0.079559. Value loss: 13.544927. Entropy: 0.342017.\n",
      "Iteration 2106: Policy loss: -0.085643. Value loss: 10.780119. Entropy: 0.354316.\n",
      "episode: 898   score: 275.0  epsilon: 1.0    steps: 210  evaluation reward: 242.95\n",
      "episode: 899   score: 210.0  epsilon: 1.0    steps: 476  evaluation reward: 242.35\n",
      "episode: 900   score: 105.0  epsilon: 1.0    steps: 969  evaluation reward: 242.55\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2107: Policy loss: -0.025992. Value loss: 16.488483. Entropy: 0.328091.\n",
      "Iteration 2108: Policy loss: 0.049042. Value loss: 12.790503. Entropy: 0.278130.\n",
      "Iteration 2109: Policy loss: 0.148424. Value loss: 11.505218. Entropy: 0.307214.\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2110: Policy loss: -0.268871. Value loss: 8.191198. Entropy: 0.153449.\n",
      "Iteration 2111: Policy loss: -0.265126. Value loss: 6.712592. Entropy: 0.131281.\n",
      "Iteration 2112: Policy loss: -0.291511. Value loss: 6.191462. Entropy: 0.133724.\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2113: Policy loss: -0.228124. Value loss: 7.460977. Entropy: 0.170124.\n",
      "Iteration 2114: Policy loss: -0.316009. Value loss: 6.726409. Entropy: 0.163091.\n",
      "Iteration 2115: Policy loss: -0.284375. Value loss: 6.137593. Entropy: 0.161474.\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2116: Policy loss: -0.515622. Value loss: 7.129842. Entropy: 0.294574.\n",
      "Iteration 2117: Policy loss: -0.393014. Value loss: 6.281601. Entropy: 0.283475.\n",
      "Iteration 2118: Policy loss: -0.472198. Value loss: 6.157746. Entropy: 0.265358.\n",
      "now time :  2019-02-26 13:07:20.277012\n",
      "episode: 901   score: 180.0  epsilon: 1.0    steps: 262  evaluation reward: 242.75\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2119: Policy loss: 0.429372. Value loss: 12.701133. Entropy: 0.467921.\n",
      "Iteration 2120: Policy loss: 0.322229. Value loss: 8.334671. Entropy: 0.538138.\n",
      "Iteration 2121: Policy loss: 0.427800. Value loss: 8.941153. Entropy: 0.512884.\n",
      "episode: 902   score: 180.0  epsilon: 1.0    steps: 41  evaluation reward: 243.1\n",
      "episode: 903   score: 155.0  epsilon: 1.0    steps: 554  evaluation reward: 241.65\n",
      "episode: 904   score: 210.0  epsilon: 1.0    steps: 704  evaluation reward: 241.95\n",
      "Training network. lr: 0.000234. clip: 0.093558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2122: Policy loss: 0.144983. Value loss: 15.641424. Entropy: 0.629536.\n",
      "Iteration 2123: Policy loss: 0.371753. Value loss: 11.478615. Entropy: 0.544508.\n",
      "Iteration 2124: Policy loss: 0.457594. Value loss: 10.472921. Entropy: 0.574925.\n",
      "episode: 905   score: 210.0  epsilon: 1.0    steps: 777  evaluation reward: 240.55\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2125: Policy loss: -0.419458. Value loss: 12.145477. Entropy: 0.302038.\n",
      "Iteration 2126: Policy loss: -0.535957. Value loss: 10.433533. Entropy: 0.295201.\n",
      "Iteration 2127: Policy loss: -0.406082. Value loss: 9.531811. Entropy: 0.307350.\n",
      "episode: 906   score: 180.0  epsilon: 1.0    steps: 133  evaluation reward: 237.25\n",
      "episode: 907   score: 210.0  epsilon: 1.0    steps: 402  evaluation reward: 235.75\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2128: Policy loss: -0.463126. Value loss: 5.199169. Entropy: 0.266860.\n",
      "Iteration 2129: Policy loss: -0.468141. Value loss: 4.698680. Entropy: 0.260660.\n",
      "Iteration 2130: Policy loss: -0.482129. Value loss: 4.713974. Entropy: 0.265450.\n",
      "episode: 908   score: 260.0  epsilon: 1.0    steps: 922  evaluation reward: 236.55\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2131: Policy loss: -0.362493. Value loss: 5.271958. Entropy: 0.151571.\n",
      "Iteration 2132: Policy loss: -0.363895. Value loss: 4.006482. Entropy: 0.144025.\n",
      "Iteration 2133: Policy loss: -0.281741. Value loss: 3.964675. Entropy: 0.136479.\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2134: Policy loss: -0.545239. Value loss: 7.240568. Entropy: 0.169203.\n",
      "Iteration 2135: Policy loss: -0.763341. Value loss: 4.240953. Entropy: 0.152887.\n",
      "Iteration 2136: Policy loss: -0.500537. Value loss: 5.944207. Entropy: 0.155148.\n",
      "episode: 909   score: 210.0  epsilon: 1.0    steps: 361  evaluation reward: 237.7\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2137: Policy loss: 0.103510. Value loss: 10.083433. Entropy: 0.415271.\n",
      "Iteration 2138: Policy loss: 0.100684. Value loss: 5.863581. Entropy: 0.416782.\n",
      "Iteration 2139: Policy loss: -0.007779. Value loss: 5.954601. Entropy: 0.427209.\n",
      "episode: 910   score: 180.0  epsilon: 1.0    steps: 605  evaluation reward: 230.25\n",
      "episode: 911   score: 180.0  epsilon: 1.0    steps: 755  evaluation reward: 230.6\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2140: Policy loss: -0.985893. Value loss: 13.774464. Entropy: 0.376783.\n",
      "Iteration 2141: Policy loss: -0.805491. Value loss: 10.862726. Entropy: 0.358480.\n",
      "Iteration 2142: Policy loss: -0.965306. Value loss: 9.678149. Entropy: 0.342141.\n",
      "episode: 912   score: 105.0  epsilon: 1.0    steps: 129  evaluation reward: 230.2\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2143: Policy loss: -0.951126. Value loss: 10.297631. Entropy: 0.444430.\n",
      "Iteration 2144: Policy loss: -1.013410. Value loss: 6.632494. Entropy: 0.396141.\n",
      "Iteration 2145: Policy loss: -0.961874. Value loss: 6.243045. Entropy: 0.400695.\n",
      "episode: 913   score: 270.0  epsilon: 1.0    steps: 17  evaluation reward: 230.95\n",
      "episode: 914   score: 180.0  epsilon: 1.0    steps: 453  evaluation reward: 232.05\n",
      "episode: 915   score: 260.0  epsilon: 1.0    steps: 875  evaluation reward: 232.9\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2146: Policy loss: -0.126047. Value loss: 16.042988. Entropy: 0.301747.\n",
      "Iteration 2147: Policy loss: -0.141836. Value loss: 12.339545. Entropy: 0.305121.\n",
      "Iteration 2148: Policy loss: -0.149111. Value loss: 9.624275. Entropy: 0.299954.\n",
      "episode: 916   score: 180.0  epsilon: 1.0    steps: 973  evaluation reward: 232.5\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2149: Policy loss: -0.452887. Value loss: 4.830679. Entropy: 0.120111.\n",
      "Iteration 2150: Policy loss: -0.387208. Value loss: 5.129740. Entropy: 0.130473.\n",
      "Iteration 2151: Policy loss: -0.451234. Value loss: 4.484514. Entropy: 0.108570.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2152: Policy loss: -0.203543. Value loss: 5.369530. Entropy: 0.090625.\n",
      "Iteration 2153: Policy loss: -0.397152. Value loss: 4.237995. Entropy: 0.102442.\n",
      "Iteration 2154: Policy loss: -0.167610. Value loss: 3.559708. Entropy: 0.101842.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2155: Policy loss: -0.335563. Value loss: 6.384463. Entropy: 0.331026.\n",
      "Iteration 2156: Policy loss: -0.327752. Value loss: 5.442306. Entropy: 0.305182.\n",
      "Iteration 2157: Policy loss: -0.374473. Value loss: 4.834736. Entropy: 0.342378.\n",
      "episode: 917   score: 180.0  epsilon: 1.0    steps: 284  evaluation reward: 232.95\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2158: Policy loss: -0.742016. Value loss: 12.720332. Entropy: 0.438276.\n",
      "Iteration 2159: Policy loss: -0.687372. Value loss: 9.632051. Entropy: 0.444471.\n",
      "Iteration 2160: Policy loss: -0.736393. Value loss: 8.473928. Entropy: 0.425540.\n",
      "episode: 918   score: 210.0  epsilon: 1.0    steps: 230  evaluation reward: 233.5\n",
      "episode: 919   score: 210.0  epsilon: 1.0    steps: 541  evaluation reward: 230.9\n",
      "episode: 920   score: 180.0  epsilon: 1.0    steps: 670  evaluation reward: 231.15\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2161: Policy loss: -3.730311. Value loss: 276.569885. Entropy: 0.468399.\n",
      "Iteration 2162: Policy loss: -3.315042. Value loss: 142.931076. Entropy: 0.231641.\n",
      "Iteration 2163: Policy loss: -3.931284. Value loss: 141.512207. Entropy: 0.209959.\n",
      "episode: 921   score: 180.0  epsilon: 1.0    steps: 43  evaluation reward: 230.85\n",
      "episode: 922   score: 210.0  epsilon: 1.0    steps: 500  evaluation reward: 232.2\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2164: Policy loss: 0.374972. Value loss: 31.538240. Entropy: 0.129427.\n",
      "Iteration 2165: Policy loss: 0.038621. Value loss: 20.448950. Entropy: 0.135151.\n",
      "Iteration 2166: Policy loss: 0.335615. Value loss: 16.633181. Entropy: 0.135357.\n",
      "episode: 923   score: 180.0  epsilon: 1.0    steps: 798  evaluation reward: 232.45\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2167: Policy loss: 0.459127. Value loss: 6.164609. Entropy: 0.068182.\n",
      "Iteration 2168: Policy loss: 0.371169. Value loss: 5.747810. Entropy: 0.063360.\n",
      "Iteration 2169: Policy loss: 0.346191. Value loss: 4.883302. Entropy: 0.066201.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2170: Policy loss: 1.672056. Value loss: 12.288823. Entropy: 0.097798.\n",
      "Iteration 2171: Policy loss: 1.623020. Value loss: 7.777651. Entropy: 0.100526.\n",
      "Iteration 2172: Policy loss: 1.446057. Value loss: 5.274422. Entropy: 0.112681.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2173: Policy loss: 0.929030. Value loss: 9.096772. Entropy: 0.143630.\n",
      "Iteration 2174: Policy loss: 0.960150. Value loss: 6.458348. Entropy: 0.173033.\n",
      "Iteration 2175: Policy loss: 0.807862. Value loss: 5.588809. Entropy: 0.195603.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2176: Policy loss: -1.181615. Value loss: 11.209330. Entropy: 0.316441.\n",
      "Iteration 2177: Policy loss: -1.204879. Value loss: 6.385397. Entropy: 0.331850.\n",
      "Iteration 2178: Policy loss: -1.108024. Value loss: 5.881506. Entropy: 0.287735.\n",
      "episode: 924   score: 225.0  epsilon: 1.0    steps: 362  evaluation reward: 231.8\n",
      "episode: 925   score: 210.0  epsilon: 1.0    steps: 604  evaluation reward: 231.8\n",
      "episode: 926   score: 155.0  epsilon: 1.0    steps: 689  evaluation reward: 231.55\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2179: Policy loss: 1.397975. Value loss: 26.823179. Entropy: 0.344540.\n",
      "Iteration 2180: Policy loss: 1.649812. Value loss: 15.696447. Entropy: 0.340696.\n",
      "Iteration 2181: Policy loss: 1.571330. Value loss: 12.320503. Entropy: 0.347035.\n",
      "episode: 927   score: 180.0  epsilon: 1.0    steps: 94  evaluation reward: 230.35\n",
      "episode: 928   score: 210.0  epsilon: 1.0    steps: 187  evaluation reward: 228.05\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2182: Policy loss: -0.624214. Value loss: 29.802048. Entropy: 0.278078.\n",
      "Iteration 2183: Policy loss: -0.308850. Value loss: 14.951856. Entropy: 0.246998.\n",
      "Iteration 2184: Policy loss: -0.377242. Value loss: 13.135859. Entropy: 0.255750.\n",
      "episode: 929   score: 135.0  epsilon: 1.0    steps: 405  evaluation reward: 225.3\n",
      "episode: 930   score: 210.0  epsilon: 1.0    steps: 843  evaluation reward: 225.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 931   score: 620.0  epsilon: 1.0    steps: 922  evaluation reward: 227.65\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2185: Policy loss: -0.003084. Value loss: 28.389328. Entropy: 0.136751.\n",
      "Iteration 2186: Policy loss: 0.021397. Value loss: 15.925108. Entropy: 0.127457.\n",
      "Iteration 2187: Policy loss: -0.060650. Value loss: 15.704276. Entropy: 0.139121.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2188: Policy loss: 0.731687. Value loss: 10.546285. Entropy: 0.062879.\n",
      "Iteration 2189: Policy loss: 0.613728. Value loss: 9.416876. Entropy: 0.066704.\n",
      "Iteration 2190: Policy loss: 0.656514. Value loss: 7.542037. Entropy: 0.076876.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2191: Policy loss: 0.022201. Value loss: 5.267183. Entropy: 0.056365.\n",
      "Iteration 2192: Policy loss: 0.133983. Value loss: 3.991392. Entropy: 0.047958.\n",
      "Iteration 2193: Policy loss: -0.013594. Value loss: 3.903472. Entropy: 0.045504.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2194: Policy loss: 0.472118. Value loss: 3.216939. Entropy: 0.133676.\n",
      "Iteration 2195: Policy loss: 0.413858. Value loss: 2.600108. Entropy: 0.152227.\n",
      "Iteration 2196: Policy loss: 0.442798. Value loss: 3.047425. Entropy: 0.154081.\n",
      "episode: 932   score: 210.0  epsilon: 1.0    steps: 741  evaluation reward: 228.7\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2197: Policy loss: -0.860498. Value loss: 12.727215. Entropy: 0.260539.\n",
      "Iteration 2198: Policy loss: -0.864874. Value loss: 8.281476. Entropy: 0.224606.\n",
      "Iteration 2199: Policy loss: -0.838201. Value loss: 9.883649. Entropy: 0.244604.\n",
      "episode: 933   score: 180.0  epsilon: 1.0    steps: 238  evaluation reward: 228.35\n",
      "episode: 934   score: 180.0  epsilon: 1.0    steps: 265  evaluation reward: 227.9\n",
      "episode: 935   score: 180.0  epsilon: 1.0    steps: 527  evaluation reward: 228.35\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2200: Policy loss: -0.325358. Value loss: 4.168970. Entropy: 0.299140.\n",
      "Iteration 2201: Policy loss: -0.526296. Value loss: 2.951882. Entropy: 0.272759.\n",
      "Iteration 2202: Policy loss: -0.245162. Value loss: 2.576259. Entropy: 0.261752.\n",
      "episode: 936   score: 180.0  epsilon: 1.0    steps: 17  evaluation reward: 228.05\n",
      "episode: 937   score: 155.0  epsilon: 1.0    steps: 442  evaluation reward: 227.5\n",
      "episode: 938   score: 180.0  epsilon: 1.0    steps: 894  evaluation reward: 227.45\n",
      "episode: 939   score: 180.0  epsilon: 1.0    steps: 973  evaluation reward: 224.65\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2203: Policy loss: -0.614523. Value loss: 12.206676. Entropy: 0.169368.\n",
      "Iteration 2204: Policy loss: -0.641039. Value loss: 8.900654. Entropy: 0.190356.\n",
      "Iteration 2205: Policy loss: -0.681723. Value loss: 8.285150. Entropy: 0.215017.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2206: Policy loss: -0.540827. Value loss: 8.345375. Entropy: 0.079175.\n",
      "Iteration 2207: Policy loss: -0.560525. Value loss: 7.564173. Entropy: 0.077278.\n",
      "Iteration 2208: Policy loss: -0.611143. Value loss: 7.859419. Entropy: 0.074861.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2209: Policy loss: -1.947845. Value loss: 129.329346. Entropy: 0.029015.\n",
      "Iteration 2210: Policy loss: -1.973804. Value loss: 84.849823. Entropy: 0.023082.\n",
      "Iteration 2211: Policy loss: -2.143919. Value loss: 70.716789. Entropy: 0.032014.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2212: Policy loss: 0.350208. Value loss: 12.364486. Entropy: 0.100261.\n",
      "Iteration 2213: Policy loss: 0.247590. Value loss: 6.164023. Entropy: 0.097162.\n",
      "Iteration 2214: Policy loss: 0.516684. Value loss: 5.351514. Entropy: 0.109309.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2215: Policy loss: -0.257977. Value loss: 15.484042. Entropy: 0.149890.\n",
      "Iteration 2216: Policy loss: 0.152264. Value loss: 11.477474. Entropy: 0.142797.\n",
      "Iteration 2217: Policy loss: 0.080836. Value loss: 10.326652. Entropy: 0.146870.\n",
      "episode: 940   score: 210.0  epsilon: 1.0    steps: 317  evaluation reward: 224.35\n",
      "episode: 941   score: 180.0  epsilon: 1.0    steps: 578  evaluation reward: 224.05\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2218: Policy loss: 0.492652. Value loss: 19.398716. Entropy: 0.233732.\n",
      "Iteration 2219: Policy loss: 0.664751. Value loss: 11.306624. Entropy: 0.282641.\n",
      "Iteration 2220: Policy loss: 0.463467. Value loss: 10.516674. Entropy: 0.211618.\n",
      "episode: 942   score: 180.0  epsilon: 1.0    steps: 50  evaluation reward: 223.6\n",
      "episode: 943   score: 180.0  epsilon: 1.0    steps: 161  evaluation reward: 221.7\n",
      "episode: 944   score: 210.0  epsilon: 1.0    steps: 488  evaluation reward: 221.6\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2221: Policy loss: 1.202198. Value loss: 27.461172. Entropy: 0.164898.\n",
      "Iteration 2222: Policy loss: 1.169002. Value loss: 17.630602. Entropy: 0.153775.\n",
      "Iteration 2223: Policy loss: 1.123374. Value loss: 17.012484. Entropy: 0.153932.\n",
      "episode: 945   score: 500.0  epsilon: 1.0    steps: 676  evaluation reward: 223.25\n",
      "episode: 946   score: 155.0  epsilon: 1.0    steps: 801  evaluation reward: 222.65\n",
      "episode: 947   score: 210.0  epsilon: 1.0    steps: 913  evaluation reward: 222.65\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2224: Policy loss: 0.462055. Value loss: 10.106093. Entropy: 0.109598.\n",
      "Iteration 2225: Policy loss: 0.529104. Value loss: 7.870459. Entropy: 0.143280.\n",
      "Iteration 2226: Policy loss: 0.357109. Value loss: 7.154353. Entropy: 0.155370.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2227: Policy loss: 0.909311. Value loss: 6.475948. Entropy: 0.060535.\n",
      "Iteration 2228: Policy loss: 0.887316. Value loss: 4.870170. Entropy: 0.093547.\n",
      "Iteration 2229: Policy loss: 0.837119. Value loss: 5.189136. Entropy: 0.245262.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2230: Policy loss: -0.371808. Value loss: 9.536615. Entropy: 0.220867.\n",
      "Iteration 2231: Policy loss: -0.382723. Value loss: 9.115411. Entropy: 0.164117.\n",
      "Iteration 2232: Policy loss: -0.425384. Value loss: 7.867282. Entropy: 0.255324.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2233: Policy loss: -0.128857. Value loss: 15.638219. Entropy: 0.182137.\n",
      "Iteration 2234: Policy loss: -0.059582. Value loss: 12.740005. Entropy: 0.185473.\n",
      "Iteration 2235: Policy loss: -0.141073. Value loss: 10.830503. Entropy: 0.194417.\n",
      "episode: 948   score: 165.0  epsilon: 1.0    steps: 495  evaluation reward: 221.4\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2236: Policy loss: -0.368286. Value loss: 47.545029. Entropy: 0.153714.\n",
      "Iteration 2237: Policy loss: -0.322210. Value loss: 32.657299. Entropy: 0.224564.\n",
      "Iteration 2238: Policy loss: -0.557666. Value loss: 26.849073. Entropy: 0.288406.\n",
      "episode: 949   score: 180.0  epsilon: 1.0    steps: 86  evaluation reward: 220.35\n",
      "episode: 950   score: 210.0  epsilon: 1.0    steps: 264  evaluation reward: 220.9\n",
      "now time :  2019-02-26 13:09:38.267848\n",
      "episode: 951   score: 135.0  epsilon: 1.0    steps: 905  evaluation reward: 220.1\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2239: Policy loss: 0.111400. Value loss: 12.270374. Entropy: 0.219028.\n",
      "Iteration 2240: Policy loss: 0.328924. Value loss: 10.115996. Entropy: 0.269570.\n",
      "Iteration 2241: Policy loss: 0.036144. Value loss: 9.295078. Entropy: 0.301216.\n",
      "episode: 952   score: 270.0  epsilon: 1.0    steps: 214  evaluation reward: 220.4\n",
      "episode: 953   score: 210.0  epsilon: 1.0    steps: 713  evaluation reward: 219.3\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2242: Policy loss: 1.891645. Value loss: 15.166359. Entropy: 0.255832.\n",
      "Iteration 2243: Policy loss: 1.717550. Value loss: 11.277918. Entropy: 0.399401.\n",
      "Iteration 2244: Policy loss: 1.950959. Value loss: 10.822443. Entropy: 0.301249.\n",
      "episode: 954   score: 240.0  epsilon: 1.0    steps: 892  evaluation reward: 219.6\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2245: Policy loss: 1.671036. Value loss: 9.516117. Entropy: 0.284950.\n",
      "Iteration 2246: Policy loss: 1.649578. Value loss: 6.158006. Entropy: 0.256957.\n",
      "Iteration 2247: Policy loss: 1.627035. Value loss: 4.586279. Entropy: 0.250235.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2248: Policy loss: 0.210490. Value loss: 12.625099. Entropy: 0.147151.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2249: Policy loss: 0.265741. Value loss: 8.027597. Entropy: 0.158766.\n",
      "Iteration 2250: Policy loss: 0.266925. Value loss: 8.168193. Entropy: 0.148996.\n",
      "episode: 955   score: 365.0  epsilon: 1.0    steps: 587  evaluation reward: 222.2\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2251: Policy loss: 0.371944. Value loss: 20.353098. Entropy: 0.327018.\n",
      "Iteration 2252: Policy loss: 0.218327. Value loss: 14.113194. Entropy: 0.348334.\n",
      "Iteration 2253: Policy loss: 0.282345. Value loss: 12.123341. Entropy: 0.366254.\n",
      "episode: 956   score: 135.0  epsilon: 1.0    steps: 283  evaluation reward: 221.75\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2254: Policy loss: -3.152433. Value loss: 253.016190. Entropy: 0.368955.\n",
      "Iteration 2255: Policy loss: -2.846454. Value loss: 219.682465. Entropy: 0.384032.\n",
      "Iteration 2256: Policy loss: -3.739532. Value loss: 199.592056. Entropy: 0.385241.\n",
      "episode: 957   score: 185.0  epsilon: 1.0    steps: 121  evaluation reward: 221.75\n",
      "episode: 958   score: 410.0  epsilon: 1.0    steps: 409  evaluation reward: 219.25\n",
      "episode: 959   score: 155.0  epsilon: 1.0    steps: 737  evaluation reward: 219.6\n",
      "episode: 960   score: 210.0  epsilon: 1.0    steps: 977  evaluation reward: 219.45\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2257: Policy loss: 2.465217. Value loss: 58.838112. Entropy: 0.380909.\n",
      "Iteration 2258: Policy loss: 2.117160. Value loss: 38.519028. Entropy: 0.343180.\n",
      "Iteration 2259: Policy loss: 2.778863. Value loss: 28.039124. Entropy: 0.389267.\n",
      "episode: 961   score: 60.0  epsilon: 1.0    steps: 593  evaluation reward: 216.9\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2260: Policy loss: 0.279256. Value loss: 26.408819. Entropy: 0.321394.\n",
      "Iteration 2261: Policy loss: 0.247389. Value loss: 22.046198. Entropy: 0.405571.\n",
      "Iteration 2262: Policy loss: 0.372954. Value loss: 16.350214. Entropy: 0.288486.\n",
      "episode: 962   score: 210.0  epsilon: 1.0    steps: 173  evaluation reward: 217.8\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2263: Policy loss: -0.182687. Value loss: 39.081177. Entropy: 0.331152.\n",
      "Iteration 2264: Policy loss: 0.018119. Value loss: 25.167738. Entropy: 0.344122.\n",
      "Iteration 2265: Policy loss: -0.042466. Value loss: 22.410217. Entropy: 0.269384.\n",
      "episode: 963   score: 135.0  epsilon: 1.0    steps: 746  evaluation reward: 217.55\n",
      "episode: 964   score: 210.0  epsilon: 1.0    steps: 796  evaluation reward: 217.35\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2266: Policy loss: 0.673779. Value loss: 20.074688. Entropy: 0.279436.\n",
      "Iteration 2267: Policy loss: 0.758626. Value loss: 13.599489. Entropy: 0.372422.\n",
      "Iteration 2268: Policy loss: 0.841422. Value loss: 12.053359. Entropy: 0.386991.\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2269: Policy loss: 1.397838. Value loss: 22.525553. Entropy: 0.382306.\n",
      "Iteration 2270: Policy loss: 1.407411. Value loss: 16.275534. Entropy: 0.287285.\n",
      "Iteration 2271: Policy loss: 1.370217. Value loss: 17.653481. Entropy: 0.352587.\n",
      "episode: 965   score: 155.0  epsilon: 1.0    steps: 414  evaluation reward: 217.1\n",
      "episode: 966   score: 135.0  epsilon: 1.0    steps: 1020  evaluation reward: 216.25\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2272: Policy loss: -1.242230. Value loss: 27.142757. Entropy: 0.611235.\n",
      "Iteration 2273: Policy loss: -1.685874. Value loss: 20.244122. Entropy: 0.591207.\n",
      "Iteration 2274: Policy loss: -1.123762. Value loss: 17.849312. Entropy: 0.581895.\n",
      "episode: 967   score: 235.0  epsilon: 1.0    steps: 344  evaluation reward: 213.75\n",
      "episode: 968   score: 180.0  epsilon: 1.0    steps: 600  evaluation reward: 213.75\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2275: Policy loss: 0.941270. Value loss: 23.364859. Entropy: 0.573861.\n",
      "Iteration 2276: Policy loss: 1.044077. Value loss: 13.908321. Entropy: 0.637884.\n",
      "Iteration 2277: Policy loss: 0.800440. Value loss: 12.292904. Entropy: 0.623820.\n",
      "episode: 969   score: 110.0  epsilon: 1.0    steps: 130  evaluation reward: 212.5\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2278: Policy loss: 0.546106. Value loss: 28.408840. Entropy: 0.513970.\n",
      "Iteration 2279: Policy loss: 0.420301. Value loss: 17.363384. Entropy: 0.499683.\n",
      "Iteration 2280: Policy loss: 0.365780. Value loss: 14.948647. Entropy: 0.496317.\n",
      "episode: 970   score: 290.0  epsilon: 1.0    steps: 20  evaluation reward: 212.8\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2281: Policy loss: 0.047467. Value loss: 29.200764. Entropy: 0.289021.\n",
      "Iteration 2282: Policy loss: 0.097332. Value loss: 21.449236. Entropy: 0.294453.\n",
      "Iteration 2283: Policy loss: 0.336881. Value loss: 17.772797. Entropy: 0.167859.\n",
      "episode: 971   score: 120.0  epsilon: 1.0    steps: 443  evaluation reward: 211.9\n",
      "episode: 972   score: 180.0  epsilon: 1.0    steps: 801  evaluation reward: 211.6\n",
      "episode: 973   score: 80.0  epsilon: 1.0    steps: 979  evaluation reward: 209.05\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2284: Policy loss: 0.291325. Value loss: 32.154217. Entropy: 0.359539.\n",
      "Iteration 2285: Policy loss: 0.461830. Value loss: 18.490740. Entropy: 0.395584.\n",
      "Iteration 2286: Policy loss: 0.297459. Value loss: 15.434823. Entropy: 0.336112.\n",
      "episode: 974   score: 125.0  epsilon: 1.0    steps: 293  evaluation reward: 205.7\n",
      "episode: 975   score: 205.0  epsilon: 1.0    steps: 689  evaluation reward: 206.7\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2287: Policy loss: 0.978424. Value loss: 33.256878. Entropy: 0.284223.\n",
      "Iteration 2288: Policy loss: 0.590825. Value loss: 25.412395. Entropy: 0.316428.\n",
      "Iteration 2289: Policy loss: 0.676280. Value loss: 21.695419. Entropy: 0.296733.\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2290: Policy loss: 1.525939. Value loss: 19.500877. Entropy: 0.219478.\n",
      "Iteration 2291: Policy loss: 1.284044. Value loss: 13.706306. Entropy: 0.237630.\n",
      "Iteration 2292: Policy loss: 1.340800. Value loss: 12.727409. Entropy: 0.229558.\n",
      "episode: 976   score: 155.0  epsilon: 1.0    steps: 251  evaluation reward: 205.4\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2293: Policy loss: 0.379530. Value loss: 28.261099. Entropy: 0.308466.\n",
      "Iteration 2294: Policy loss: 0.600378. Value loss: 19.584044. Entropy: 0.226085.\n",
      "Iteration 2295: Policy loss: 0.507158. Value loss: 17.726746. Entropy: 0.235690.\n",
      "episode: 977   score: 210.0  epsilon: 1.0    steps: 63  evaluation reward: 205.7\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2296: Policy loss: 1.465654. Value loss: 31.327818. Entropy: 0.485302.\n",
      "Iteration 2297: Policy loss: 1.409277. Value loss: 15.627835. Entropy: 0.327170.\n",
      "Iteration 2298: Policy loss: 1.336716. Value loss: 11.282619. Entropy: 0.377343.\n",
      "episode: 978   score: 240.0  epsilon: 1.0    steps: 529  evaluation reward: 205.1\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2299: Policy loss: 1.436935. Value loss: 20.886280. Entropy: 0.418078.\n",
      "Iteration 2300: Policy loss: 1.474604. Value loss: 13.997465. Entropy: 0.512762.\n",
      "Iteration 2301: Policy loss: 1.347682. Value loss: 12.067348. Entropy: 0.724813.\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2302: Policy loss: 1.289377. Value loss: 37.269482. Entropy: 0.804652.\n",
      "Iteration 2303: Policy loss: 1.052181. Value loss: 26.014671. Entropy: 0.844945.\n",
      "Iteration 2304: Policy loss: 1.212307. Value loss: 22.026905. Entropy: 0.861621.\n",
      "episode: 979   score: 215.0  epsilon: 1.0    steps: 958  evaluation reward: 202.35\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2305: Policy loss: -0.024355. Value loss: 23.131598. Entropy: 0.940835.\n",
      "Iteration 2306: Policy loss: -0.201303. Value loss: 13.341726. Entropy: 0.941694.\n",
      "Iteration 2307: Policy loss: 0.075712. Value loss: 13.678654. Entropy: 0.934743.\n",
      "episode: 980   score: 280.0  epsilon: 1.0    steps: 444  evaluation reward: 201.95\n",
      "episode: 981   score: 265.0  epsilon: 1.0    steps: 768  evaluation reward: 201.95\n",
      "episode: 982   score: 265.0  epsilon: 1.0    steps: 796  evaluation reward: 201.35\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2308: Policy loss: 0.882823. Value loss: 33.998650. Entropy: 0.896000.\n",
      "Iteration 2309: Policy loss: 1.242719. Value loss: 20.810602. Entropy: 0.875978.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2310: Policy loss: 1.245649. Value loss: 18.490248. Entropy: 0.911175.\n",
      "episode: 983   score: 275.0  epsilon: 1.0    steps: 280  evaluation reward: 203.05\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2311: Policy loss: 2.952392. Value loss: 28.026291. Entropy: 0.693844.\n",
      "Iteration 2312: Policy loss: 3.078306. Value loss: 17.082567. Entropy: 0.689212.\n",
      "Iteration 2313: Policy loss: 3.072373. Value loss: 15.625852. Entropy: 0.682251.\n",
      "episode: 984   score: 240.0  epsilon: 1.0    steps: 206  evaluation reward: 202.55\n",
      "episode: 985   score: 170.0  epsilon: 1.0    steps: 606  evaluation reward: 203.05\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2314: Policy loss: 1.652220. Value loss: 31.761036. Entropy: 0.455542.\n",
      "Iteration 2315: Policy loss: 1.294827. Value loss: 19.405153. Entropy: 0.477048.\n",
      "Iteration 2316: Policy loss: 1.614235. Value loss: 16.601763. Entropy: 0.477437.\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2317: Policy loss: 1.125315. Value loss: 32.873554. Entropy: 0.459704.\n",
      "Iteration 2318: Policy loss: 0.854300. Value loss: 22.046003. Entropy: 0.423401.\n",
      "Iteration 2319: Policy loss: 1.113853. Value loss: 17.366598. Entropy: 0.458016.\n",
      "episode: 986   score: 105.0  epsilon: 1.0    steps: 377  evaluation reward: 202.0\n",
      "episode: 987   score: 195.0  epsilon: 1.0    steps: 946  evaluation reward: 199.25\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2320: Policy loss: -1.742632. Value loss: 36.032890. Entropy: 0.601846.\n",
      "Iteration 2321: Policy loss: -1.864518. Value loss: 19.465876. Entropy: 0.653259.\n",
      "Iteration 2322: Policy loss: -1.619345. Value loss: 17.549858. Entropy: 0.686719.\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2323: Policy loss: -3.132794. Value loss: 43.833878. Entropy: 0.856474.\n",
      "Iteration 2324: Policy loss: -3.010965. Value loss: 20.377199. Entropy: 0.893064.\n",
      "Iteration 2325: Policy loss: -3.047267. Value loss: 15.815260. Entropy: 0.881639.\n",
      "episode: 988   score: 250.0  epsilon: 1.0    steps: 888  evaluation reward: 200.7\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2326: Policy loss: -0.037684. Value loss: 47.629368. Entropy: 0.919008.\n",
      "Iteration 2327: Policy loss: 0.362904. Value loss: 23.408728. Entropy: 0.906489.\n",
      "Iteration 2328: Policy loss: 0.001746. Value loss: 18.918051. Entropy: 0.890495.\n",
      "episode: 989   score: 250.0  epsilon: 1.0    steps: 453  evaluation reward: 201.45\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2329: Policy loss: 0.494019. Value loss: 74.014450. Entropy: 0.716136.\n",
      "Iteration 2330: Policy loss: 0.410802. Value loss: 41.313854. Entropy: 0.685492.\n",
      "Iteration 2331: Policy loss: 0.118763. Value loss: 31.413229. Entropy: 0.748920.\n",
      "episode: 990   score: 300.0  epsilon: 1.0    steps: 673  evaluation reward: 203.4\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2332: Policy loss: 0.804945. Value loss: 55.435200. Entropy: 0.651627.\n",
      "Iteration 2333: Policy loss: 0.410495. Value loss: 29.507399. Entropy: 0.634592.\n",
      "Iteration 2334: Policy loss: 0.680582. Value loss: 24.636360. Entropy: 0.637927.\n",
      "episode: 991   score: 705.0  epsilon: 1.0    steps: 10  evaluation reward: 209.35\n",
      "episode: 992   score: 155.0  epsilon: 1.0    steps: 355  evaluation reward: 209.75\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2335: Policy loss: 0.137598. Value loss: 53.918888. Entropy: 0.617463.\n",
      "Iteration 2336: Policy loss: 0.686843. Value loss: 34.220409. Entropy: 0.646002.\n",
      "Iteration 2337: Policy loss: 0.146976. Value loss: 23.335381. Entropy: 0.664342.\n",
      "episode: 993   score: 75.0  epsilon: 1.0    steps: 454  evaluation reward: 206.7\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2338: Policy loss: 2.198178. Value loss: 73.697006. Entropy: 0.739356.\n",
      "Iteration 2339: Policy loss: 2.138228. Value loss: 38.780849. Entropy: 0.758790.\n",
      "Iteration 2340: Policy loss: 2.205228. Value loss: 32.187679. Entropy: 0.719554.\n",
      "episode: 994   score: 385.0  epsilon: 1.0    steps: 135  evaluation reward: 208.45\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2341: Policy loss: -1.094218. Value loss: 81.793945. Entropy: 0.715655.\n",
      "Iteration 2342: Policy loss: -1.148258. Value loss: 42.922249. Entropy: 0.742834.\n",
      "Iteration 2343: Policy loss: -1.027104. Value loss: 33.749706. Entropy: 0.739217.\n",
      "episode: 995   score: 135.0  epsilon: 1.0    steps: 372  evaluation reward: 207.9\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2344: Policy loss: 0.290707. Value loss: 64.012527. Entropy: 0.769059.\n",
      "Iteration 2345: Policy loss: 0.051071. Value loss: 41.042797. Entropy: 0.755481.\n",
      "Iteration 2346: Policy loss: 0.058027. Value loss: 34.569244. Entropy: 0.774648.\n",
      "episode: 996   score: 570.0  epsilon: 1.0    steps: 615  evaluation reward: 212.25\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2347: Policy loss: 1.168418. Value loss: 32.599579. Entropy: 0.850074.\n",
      "Iteration 2348: Policy loss: 1.423575. Value loss: 23.510559. Entropy: 0.842204.\n",
      "Iteration 2349: Policy loss: 1.456570. Value loss: 19.995712. Entropy: 0.849430.\n",
      "episode: 997   score: 300.0  epsilon: 1.0    steps: 917  evaluation reward: 213.45\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2350: Policy loss: 1.062104. Value loss: 38.885521. Entropy: 0.905734.\n",
      "Iteration 2351: Policy loss: 0.728071. Value loss: 23.228455. Entropy: 0.892811.\n",
      "Iteration 2352: Policy loss: 1.030100. Value loss: 17.864511. Entropy: 0.918692.\n",
      "episode: 998   score: 275.0  epsilon: 1.0    steps: 68  evaluation reward: 213.45\n",
      "episode: 999   score: 105.0  epsilon: 1.0    steps: 663  evaluation reward: 212.4\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2353: Policy loss: 0.561152. Value loss: 57.523293. Entropy: 0.652145.\n",
      "Iteration 2354: Policy loss: 0.602156. Value loss: 35.350445. Entropy: 0.638146.\n",
      "Iteration 2355: Policy loss: 0.509079. Value loss: 26.405586. Entropy: 0.631924.\n",
      "episode: 1000   score: 240.0  epsilon: 1.0    steps: 162  evaluation reward: 213.75\n",
      "now time :  2019-02-26 13:11:52.982945\n",
      "episode: 1001   score: 420.0  epsilon: 1.0    steps: 797  evaluation reward: 216.15\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2356: Policy loss: 2.087857. Value loss: 55.487007. Entropy: 0.493227.\n",
      "Iteration 2357: Policy loss: 1.963486. Value loss: 30.511133. Entropy: 0.463947.\n",
      "Iteration 2358: Policy loss: 1.863806. Value loss: 23.174128. Entropy: 0.522590.\n",
      "episode: 1002   score: 240.0  epsilon: 1.0    steps: 342  evaluation reward: 216.75\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2359: Policy loss: 2.676732. Value loss: 53.242851. Entropy: 0.550563.\n",
      "Iteration 2360: Policy loss: 2.987220. Value loss: 33.974720. Entropy: 0.618158.\n",
      "Iteration 2361: Policy loss: 2.715662. Value loss: 27.201506. Entropy: 0.540504.\n",
      "episode: 1003   score: 420.0  epsilon: 1.0    steps: 450  evaluation reward: 219.4\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2362: Policy loss: 0.286771. Value loss: 33.751255. Entropy: 0.460872.\n",
      "Iteration 2363: Policy loss: 0.248240. Value loss: 22.550726. Entropy: 0.444862.\n",
      "Iteration 2364: Policy loss: 0.309614. Value loss: 17.015558. Entropy: 0.453269.\n",
      "episode: 1004   score: 185.0  epsilon: 1.0    steps: 646  evaluation reward: 219.15\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2365: Policy loss: 1.268878. Value loss: 56.305794. Entropy: 0.537120.\n",
      "Iteration 2366: Policy loss: 1.419360. Value loss: 29.180750. Entropy: 0.567350.\n",
      "Iteration 2367: Policy loss: 1.504479. Value loss: 26.879805. Entropy: 0.580252.\n",
      "episode: 1005   score: 210.0  epsilon: 1.0    steps: 115  evaluation reward: 219.15\n",
      "episode: 1006   score: 210.0  epsilon: 1.0    steps: 534  evaluation reward: 219.45\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2368: Policy loss: -1.618477. Value loss: 45.792538. Entropy: 0.522114.\n",
      "Iteration 2369: Policy loss: -1.359052. Value loss: 24.889502. Entropy: 0.523322.\n",
      "Iteration 2370: Policy loss: -1.661615. Value loss: 22.479612. Entropy: 0.553903.\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2371: Policy loss: 3.179417. Value loss: 280.980408. Entropy: 0.595597.\n",
      "Iteration 2372: Policy loss: 4.024439. Value loss: 128.268127. Entropy: 0.573894.\n",
      "Iteration 2373: Policy loss: 4.518098. Value loss: 89.898224. Entropy: 0.638043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1007   score: 140.0  epsilon: 1.0    steps: 796  evaluation reward: 218.75\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2374: Policy loss: 1.308556. Value loss: 37.495998. Entropy: 0.815529.\n",
      "Iteration 2375: Policy loss: 1.353734. Value loss: 26.910820. Entropy: 0.849331.\n",
      "Iteration 2376: Policy loss: 1.305030. Value loss: 21.709814. Entropy: 0.827069.\n",
      "episode: 1008   score: 135.0  epsilon: 1.0    steps: 315  evaluation reward: 217.5\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2377: Policy loss: 3.599595. Value loss: 52.866333. Entropy: 0.826645.\n",
      "Iteration 2378: Policy loss: 3.215151. Value loss: 35.240326. Entropy: 0.801955.\n",
      "Iteration 2379: Policy loss: 3.281137. Value loss: 31.176695. Entropy: 0.840964.\n",
      "episode: 1009   score: 495.0  epsilon: 1.0    steps: 197  evaluation reward: 220.35\n",
      "episode: 1010   score: 135.0  epsilon: 1.0    steps: 623  evaluation reward: 219.9\n",
      "episode: 1011   score: 175.0  epsilon: 1.0    steps: 676  evaluation reward: 219.85\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2380: Policy loss: 3.707604. Value loss: 49.148628. Entropy: 0.512896.\n",
      "Iteration 2381: Policy loss: 3.732002. Value loss: 35.227898. Entropy: 0.560703.\n",
      "Iteration 2382: Policy loss: 3.590701. Value loss: 26.730663. Entropy: 0.538112.\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2383: Policy loss: 1.724480. Value loss: 31.125877. Entropy: 0.566218.\n",
      "Iteration 2384: Policy loss: 2.035172. Value loss: 21.903267. Entropy: 0.551077.\n",
      "Iteration 2385: Policy loss: 2.051942. Value loss: 17.770399. Entropy: 0.536653.\n",
      "episode: 1012   score: 120.0  epsilon: 1.0    steps: 869  evaluation reward: 220.0\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2386: Policy loss: -7.336704. Value loss: 544.727722. Entropy: 0.590177.\n",
      "Iteration 2387: Policy loss: -6.274765. Value loss: 304.612244. Entropy: 0.524506.\n",
      "Iteration 2388: Policy loss: -6.253335. Value loss: 208.596176. Entropy: 0.625506.\n",
      "episode: 1013   score: 265.0  epsilon: 1.0    steps: 437  evaluation reward: 219.95\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2389: Policy loss: 1.099508. Value loss: 65.140770. Entropy: 0.634732.\n",
      "Iteration 2390: Policy loss: 1.035610. Value loss: 42.414497. Entropy: 0.644658.\n",
      "Iteration 2391: Policy loss: 1.424575. Value loss: 30.404060. Entropy: 0.666205.\n",
      "episode: 1014   score: 730.0  epsilon: 1.0    steps: 925  evaluation reward: 225.45\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2392: Policy loss: -0.515788. Value loss: 34.007320. Entropy: 0.430280.\n",
      "Iteration 2393: Policy loss: -0.399461. Value loss: 24.478827. Entropy: 0.472043.\n",
      "Iteration 2394: Policy loss: -0.373207. Value loss: 19.718178. Entropy: 0.428771.\n",
      "episode: 1015   score: 510.0  epsilon: 1.0    steps: 36  evaluation reward: 227.95\n",
      "episode: 1016   score: 210.0  epsilon: 1.0    steps: 376  evaluation reward: 228.25\n",
      "episode: 1017   score: 210.0  epsilon: 1.0    steps: 707  evaluation reward: 228.55\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2395: Policy loss: 1.266699. Value loss: 43.465870. Entropy: 0.366690.\n",
      "Iteration 2396: Policy loss: 1.463872. Value loss: 23.469259. Entropy: 0.424960.\n",
      "Iteration 2397: Policy loss: 1.375088. Value loss: 18.668840. Entropy: 0.390627.\n",
      "episode: 1018   score: 210.0  epsilon: 1.0    steps: 236  evaluation reward: 228.55\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2398: Policy loss: 1.434649. Value loss: 39.910892. Entropy: 0.396495.\n",
      "Iteration 2399: Policy loss: 1.711217. Value loss: 26.309826. Entropy: 0.387169.\n",
      "Iteration 2400: Policy loss: 1.501122. Value loss: 23.761900. Entropy: 0.386514.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2401: Policy loss: -0.446641. Value loss: 27.243340. Entropy: 0.415420.\n",
      "Iteration 2402: Policy loss: -0.518674. Value loss: 16.276695. Entropy: 0.423004.\n",
      "Iteration 2403: Policy loss: -0.484821. Value loss: 13.754031. Entropy: 0.422043.\n",
      "episode: 1019   score: 225.0  epsilon: 1.0    steps: 519  evaluation reward: 228.7\n",
      "episode: 1020   score: 180.0  epsilon: 1.0    steps: 807  evaluation reward: 228.7\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2404: Policy loss: -1.785886. Value loss: 237.555283. Entropy: 0.516405.\n",
      "Iteration 2405: Policy loss: -2.711908. Value loss: 241.326309. Entropy: 0.521542.\n",
      "Iteration 2406: Policy loss: -1.200707. Value loss: 100.074203. Entropy: 0.477075.\n",
      "episode: 1021   score: 105.0  epsilon: 1.0    steps: 298  evaluation reward: 227.95\n",
      "episode: 1022   score: 180.0  epsilon: 1.0    steps: 465  evaluation reward: 227.65\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2407: Policy loss: 0.252035. Value loss: 30.783230. Entropy: 0.458541.\n",
      "Iteration 2408: Policy loss: 0.393644. Value loss: 23.221325. Entropy: 0.470072.\n",
      "Iteration 2409: Policy loss: 0.240806. Value loss: 18.767187. Entropy: 0.469006.\n",
      "episode: 1023   score: 150.0  epsilon: 1.0    steps: 19  evaluation reward: 227.35\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2410: Policy loss: -0.403053. Value loss: 23.068975. Entropy: 0.455770.\n",
      "Iteration 2411: Policy loss: -0.208643. Value loss: 16.038071. Entropy: 0.481499.\n",
      "Iteration 2412: Policy loss: -0.255092. Value loss: 12.058101. Entropy: 0.467296.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2413: Policy loss: 0.352214. Value loss: 43.797600. Entropy: 0.429813.\n",
      "Iteration 2414: Policy loss: 0.819911. Value loss: 27.913206. Entropy: 0.430085.\n",
      "Iteration 2415: Policy loss: 0.545906. Value loss: 22.576567. Entropy: 0.409994.\n",
      "episode: 1024   score: 410.0  epsilon: 1.0    steps: 664  evaluation reward: 229.2\n",
      "episode: 1025   score: 80.0  epsilon: 1.0    steps: 882  evaluation reward: 227.9\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2416: Policy loss: 2.509195. Value loss: 31.057711. Entropy: 0.672222.\n",
      "Iteration 2417: Policy loss: 2.724908. Value loss: 16.444891. Entropy: 0.725617.\n",
      "Iteration 2418: Policy loss: 2.373337. Value loss: 13.379830. Entropy: 0.710338.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2419: Policy loss: 1.156276. Value loss: 50.175674. Entropy: 0.583648.\n",
      "Iteration 2420: Policy loss: 1.055104. Value loss: 27.799408. Entropy: 0.575581.\n",
      "Iteration 2421: Policy loss: 1.124817. Value loss: 20.685400. Entropy: 0.596920.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2422: Policy loss: -1.207823. Value loss: 32.925976. Entropy: 0.649955.\n",
      "Iteration 2423: Policy loss: -1.139096. Value loss: 23.545607. Entropy: 0.663637.\n",
      "Iteration 2424: Policy loss: -0.758319. Value loss: 15.622539. Entropy: 0.665736.\n",
      "episode: 1026   score: 135.0  epsilon: 1.0    steps: 111  evaluation reward: 227.7\n",
      "episode: 1027   score: 300.0  epsilon: 1.0    steps: 182  evaluation reward: 228.9\n",
      "episode: 1028   score: 15.0  epsilon: 1.0    steps: 692  evaluation reward: 226.95\n",
      "episode: 1029   score: 280.0  epsilon: 1.0    steps: 971  evaluation reward: 228.4\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2425: Policy loss: -1.325814. Value loss: 203.989944. Entropy: 0.575151.\n",
      "Iteration 2426: Policy loss: -0.733625. Value loss: 82.880272. Entropy: 0.605643.\n",
      "Iteration 2427: Policy loss: -1.184976. Value loss: 56.700066. Entropy: 0.563661.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2428: Policy loss: -2.430365. Value loss: 41.352081. Entropy: 0.614038.\n",
      "Iteration 2429: Policy loss: -2.631597. Value loss: 28.779161. Entropy: 0.604316.\n",
      "Iteration 2430: Policy loss: -2.299605. Value loss: 22.641342. Entropy: 0.598767.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2431: Policy loss: -2.627939. Value loss: 57.776741. Entropy: 0.467392.\n",
      "Iteration 2432: Policy loss: -2.573093. Value loss: 28.190899. Entropy: 0.479855.\n",
      "Iteration 2433: Policy loss: -2.735099. Value loss: 21.598965. Entropy: 0.472661.\n",
      "episode: 1030   score: 370.0  epsilon: 1.0    steps: 401  evaluation reward: 230.0\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2434: Policy loss: -1.502878. Value loss: 238.761841. Entropy: 0.529635.\n",
      "Iteration 2435: Policy loss: -1.190216. Value loss: 138.439774. Entropy: 0.505931.\n",
      "Iteration 2436: Policy loss: -1.166585. Value loss: 104.073639. Entropy: 0.522730.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2437: Policy loss: 0.277415. Value loss: 26.774109. Entropy: 0.505738.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2438: Policy loss: 0.335836. Value loss: 14.213782. Entropy: 0.493087.\n",
      "Iteration 2439: Policy loss: 0.376142. Value loss: 11.066486. Entropy: 0.471368.\n",
      "episode: 1031   score: 545.0  epsilon: 1.0    steps: 351  evaluation reward: 229.25\n",
      "episode: 1032   score: 655.0  epsilon: 1.0    steps: 589  evaluation reward: 233.7\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2440: Policy loss: 0.723191. Value loss: 187.087860. Entropy: 0.420463.\n",
      "Iteration 2441: Policy loss: 0.581151. Value loss: 100.023987. Entropy: 0.422657.\n",
      "Iteration 2442: Policy loss: -0.241384. Value loss: 117.017998. Entropy: 0.422389.\n",
      "episode: 1033   score: 530.0  epsilon: 1.0    steps: 815  evaluation reward: 237.2\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2443: Policy loss: 1.164596. Value loss: 48.488770. Entropy: 0.472288.\n",
      "Iteration 2444: Policy loss: 1.365269. Value loss: 26.865570. Entropy: 0.499868.\n",
      "Iteration 2445: Policy loss: 1.238146. Value loss: 20.791712. Entropy: 0.510744.\n",
      "episode: 1034   score: 210.0  epsilon: 1.0    steps: 143  evaluation reward: 237.5\n",
      "episode: 1035   score: 180.0  epsilon: 1.0    steps: 702  evaluation reward: 237.5\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2446: Policy loss: 0.100155. Value loss: 31.763071. Entropy: 0.365444.\n",
      "Iteration 2447: Policy loss: 0.015315. Value loss: 21.283672. Entropy: 0.374878.\n",
      "Iteration 2448: Policy loss: 0.115500. Value loss: 16.739222. Entropy: 0.330810.\n",
      "episode: 1036   score: 260.0  epsilon: 1.0    steps: 28  evaluation reward: 238.3\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2449: Policy loss: -0.162726. Value loss: 20.041517. Entropy: 0.260078.\n",
      "Iteration 2450: Policy loss: -0.130081. Value loss: 13.499330. Entropy: 0.251358.\n",
      "Iteration 2451: Policy loss: -0.508622. Value loss: 11.560027. Entropy: 0.287690.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2452: Policy loss: -0.394283. Value loss: 27.937767. Entropy: 0.296050.\n",
      "Iteration 2453: Policy loss: -0.482038. Value loss: 12.829886. Entropy: 0.292384.\n",
      "Iteration 2454: Policy loss: -0.545595. Value loss: 12.384768. Entropy: 0.315401.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2455: Policy loss: 1.388772. Value loss: 25.190229. Entropy: 0.368315.\n",
      "Iteration 2456: Policy loss: 1.522393. Value loss: 13.002921. Entropy: 0.331955.\n",
      "Iteration 2457: Policy loss: 1.614108. Value loss: 9.046633. Entropy: 0.353960.\n",
      "episode: 1037   score: 225.0  epsilon: 1.0    steps: 455  evaluation reward: 239.0\n",
      "episode: 1038   score: 555.0  epsilon: 1.0    steps: 977  evaluation reward: 242.75\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2458: Policy loss: -1.719208. Value loss: 270.459473. Entropy: 0.552513.\n",
      "Iteration 2459: Policy loss: -2.232574. Value loss: 181.590164. Entropy: 0.573943.\n",
      "Iteration 2460: Policy loss: -2.161252. Value loss: 111.434814. Entropy: 0.534728.\n",
      "episode: 1039   score: 210.0  epsilon: 1.0    steps: 268  evaluation reward: 243.05\n",
      "episode: 1040   score: 210.0  epsilon: 1.0    steps: 568  evaluation reward: 243.05\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2461: Policy loss: 0.588254. Value loss: 20.329342. Entropy: 0.541372.\n",
      "Iteration 2462: Policy loss: 0.480067. Value loss: 12.681954. Entropy: 0.548171.\n",
      "Iteration 2463: Policy loss: 0.434939. Value loss: 11.206616. Entropy: 0.542939.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2464: Policy loss: -2.521720. Value loss: 30.500847. Entropy: 0.444857.\n",
      "Iteration 2465: Policy loss: -2.654914. Value loss: 21.693390. Entropy: 0.447856.\n",
      "Iteration 2466: Policy loss: -2.458991. Value loss: 18.838161. Entropy: 0.452068.\n",
      "episode: 1041   score: 210.0  epsilon: 1.0    steps: 663  evaluation reward: 243.35\n",
      "episode: 1042   score: 260.0  epsilon: 1.0    steps: 774  evaluation reward: 244.15\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2467: Policy loss: 0.036075. Value loss: 21.935984. Entropy: 0.413823.\n",
      "Iteration 2468: Policy loss: -0.121669. Value loss: 11.599065. Entropy: 0.373504.\n",
      "Iteration 2469: Policy loss: 0.208197. Value loss: 9.155957. Entropy: 0.419050.\n",
      "episode: 1043   score: 465.0  epsilon: 1.0    steps: 152  evaluation reward: 247.0\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2470: Policy loss: 2.072252. Value loss: 34.153934. Entropy: 0.187813.\n",
      "Iteration 2471: Policy loss: 2.758076. Value loss: 13.733578. Entropy: 0.152870.\n",
      "Iteration 2472: Policy loss: 2.158283. Value loss: 11.072248. Entropy: 0.071895.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2473: Policy loss: -3.218052. Value loss: 219.897858. Entropy: 0.152946.\n",
      "Iteration 2474: Policy loss: -2.576160. Value loss: 121.616570. Entropy: 0.135482.\n",
      "Iteration 2475: Policy loss: -3.092049. Value loss: 71.563011. Entropy: 0.117646.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2476: Policy loss: -0.717696. Value loss: 220.475647. Entropy: 0.141021.\n",
      "Iteration 2477: Policy loss: -1.277164. Value loss: 200.299377. Entropy: 0.127930.\n",
      "Iteration 2478: Policy loss: -0.445290. Value loss: 153.716385. Entropy: 0.205557.\n",
      "episode: 1044   score: 410.0  epsilon: 1.0    steps: 353  evaluation reward: 249.0\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2479: Policy loss: -0.558985. Value loss: 40.291603. Entropy: 0.274245.\n",
      "Iteration 2480: Policy loss: -0.418119. Value loss: 25.502430. Entropy: 0.305557.\n",
      "Iteration 2481: Policy loss: -0.303597. Value loss: 22.117678. Entropy: 0.323239.\n",
      "episode: 1045   score: 105.0  epsilon: 1.0    steps: 147  evaluation reward: 245.05\n",
      "episode: 1046   score: 260.0  epsilon: 1.0    steps: 637  evaluation reward: 246.1\n",
      "episode: 1047   score: 445.0  epsilon: 1.0    steps: 964  evaluation reward: 248.45\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2482: Policy loss: 2.919636. Value loss: 24.701050. Entropy: 0.286499.\n",
      "Iteration 2483: Policy loss: 2.980102. Value loss: 12.683082. Entropy: 0.288270.\n",
      "Iteration 2484: Policy loss: 3.063684. Value loss: 10.620569. Entropy: 0.294933.\n",
      "episode: 1048   score: 210.0  epsilon: 1.0    steps: 752  evaluation reward: 248.9\n",
      "episode: 1049   score: 180.0  epsilon: 1.0    steps: 825  evaluation reward: 248.9\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2485: Policy loss: 0.003126. Value loss: 35.109818. Entropy: 0.319055.\n",
      "Iteration 2486: Policy loss: 0.143828. Value loss: 19.492992. Entropy: 0.363752.\n",
      "Iteration 2487: Policy loss: -0.049946. Value loss: 16.245708. Entropy: 0.383043.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2488: Policy loss: 2.253924. Value loss: 40.490162. Entropy: 0.531122.\n",
      "Iteration 2489: Policy loss: 2.475593. Value loss: 25.062397. Entropy: 0.525332.\n",
      "Iteration 2490: Policy loss: 2.223806. Value loss: 20.237600. Entropy: 0.542048.\n",
      "episode: 1050   score: 80.0  epsilon: 1.0    steps: 636  evaluation reward: 247.6\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2491: Policy loss: 4.243741. Value loss: 29.458971. Entropy: 0.410298.\n",
      "Iteration 2492: Policy loss: 4.203052. Value loss: 14.385909. Entropy: 0.424287.\n",
      "Iteration 2493: Policy loss: 4.229035. Value loss: 10.353390. Entropy: 0.423105.\n",
      "now time :  2019-02-26 13:14:29.284544\n",
      "episode: 1051   score: 265.0  epsilon: 1.0    steps: 44  evaluation reward: 248.9\n",
      "episode: 1052   score: 130.0  epsilon: 1.0    steps: 322  evaluation reward: 247.5\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2494: Policy loss: 0.944220. Value loss: 41.322159. Entropy: 0.458014.\n",
      "Iteration 2495: Policy loss: 0.890413. Value loss: 25.310101. Entropy: 0.468226.\n",
      "Iteration 2496: Policy loss: 0.946492. Value loss: 20.214081. Entropy: 0.479773.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2497: Policy loss: 0.674255. Value loss: 27.525579. Entropy: 0.534012.\n",
      "Iteration 2498: Policy loss: 1.006763. Value loss: 21.951180. Entropy: 0.545743.\n",
      "Iteration 2499: Policy loss: 0.777406. Value loss: 17.571339. Entropy: 0.544814.\n",
      "episode: 1053   score: 400.0  epsilon: 1.0    steps: 385  evaluation reward: 249.4\n",
      "episode: 1054   score: 105.0  epsilon: 1.0    steps: 1021  evaluation reward: 248.05\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2500: Policy loss: -0.245963. Value loss: 47.372959. Entropy: 0.624216.\n",
      "Iteration 2501: Policy loss: 0.188090. Value loss: 23.818989. Entropy: 0.662963.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2502: Policy loss: 0.196643. Value loss: 16.159082. Entropy: 0.675585.\n",
      "episode: 1055   score: 70.0  epsilon: 1.0    steps: 650  evaluation reward: 245.1\n",
      "episode: 1056   score: 250.0  epsilon: 1.0    steps: 831  evaluation reward: 246.25\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2503: Policy loss: 0.212027. Value loss: 45.640972. Entropy: 0.520016.\n",
      "Iteration 2504: Policy loss: 0.217784. Value loss: 22.862068. Entropy: 0.488754.\n",
      "Iteration 2505: Policy loss: 0.210003. Value loss: 18.937296. Entropy: 0.500271.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2506: Policy loss: 1.282356. Value loss: 55.050133. Entropy: 0.454208.\n",
      "Iteration 2507: Policy loss: 1.387764. Value loss: 32.111546. Entropy: 0.460017.\n",
      "Iteration 2508: Policy loss: 1.178184. Value loss: 24.353481. Entropy: 0.471908.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2509: Policy loss: 2.211497. Value loss: 44.860241. Entropy: 0.671053.\n",
      "Iteration 2510: Policy loss: 1.330361. Value loss: 29.186554. Entropy: 0.645130.\n",
      "Iteration 2511: Policy loss: 1.956961. Value loss: 24.491440. Entropy: 0.650498.\n",
      "episode: 1057   score: 455.0  epsilon: 1.0    steps: 254  evaluation reward: 248.95\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2512: Policy loss: 0.382151. Value loss: 32.156097. Entropy: 0.582286.\n",
      "Iteration 2513: Policy loss: 0.275986. Value loss: 22.194389. Entropy: 0.589777.\n",
      "Iteration 2514: Policy loss: 0.288577. Value loss: 16.948555. Entropy: 0.596248.\n",
      "episode: 1058   score: 305.0  epsilon: 1.0    steps: 107  evaluation reward: 247.9\n",
      "episode: 1059   score: 395.0  epsilon: 1.0    steps: 373  evaluation reward: 250.3\n",
      "episode: 1060   score: 110.0  epsilon: 1.0    steps: 396  evaluation reward: 249.3\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2515: Policy loss: 1.394270. Value loss: 36.035145. Entropy: 0.407727.\n",
      "Iteration 2516: Policy loss: 1.780995. Value loss: 22.125582. Entropy: 0.393121.\n",
      "Iteration 2517: Policy loss: 1.389871. Value loss: 19.233692. Entropy: 0.376568.\n",
      "episode: 1061   score: 130.0  epsilon: 1.0    steps: 807  evaluation reward: 250.0\n",
      "episode: 1062   score: 200.0  epsilon: 1.0    steps: 910  evaluation reward: 249.9\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2518: Policy loss: 0.435910. Value loss: 19.613710. Entropy: 0.254961.\n",
      "Iteration 2519: Policy loss: 0.384307. Value loss: 11.804907. Entropy: 0.264835.\n",
      "Iteration 2520: Policy loss: 0.399237. Value loss: 10.855906. Entropy: 0.284164.\n",
      "episode: 1063   score: 340.0  epsilon: 1.0    steps: 538  evaluation reward: 251.95\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2521: Policy loss: -1.209833. Value loss: 36.009064. Entropy: 0.342333.\n",
      "Iteration 2522: Policy loss: -1.322951. Value loss: 24.492428. Entropy: 0.330142.\n",
      "Iteration 2523: Policy loss: -1.333384. Value loss: 20.247732. Entropy: 0.329105.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2524: Policy loss: 2.277060. Value loss: 22.386751. Entropy: 0.446180.\n",
      "Iteration 2525: Policy loss: 2.384629. Value loss: 14.046468. Entropy: 0.424670.\n",
      "Iteration 2526: Policy loss: 2.153249. Value loss: 11.498448. Entropy: 0.453338.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2527: Policy loss: 0.137415. Value loss: 28.963804. Entropy: 0.523873.\n",
      "Iteration 2528: Policy loss: 0.399472. Value loss: 17.300442. Entropy: 0.534359.\n",
      "Iteration 2529: Policy loss: 0.199160. Value loss: 15.012401. Entropy: 0.549418.\n",
      "episode: 1064   score: 375.0  epsilon: 1.0    steps: 655  evaluation reward: 253.6\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2530: Policy loss: -2.213536. Value loss: 177.717331. Entropy: 0.422281.\n",
      "Iteration 2531: Policy loss: -2.216939. Value loss: 136.015381. Entropy: 0.444299.\n",
      "Iteration 2532: Policy loss: -2.319162. Value loss: 106.934586. Entropy: 0.461523.\n",
      "episode: 1065   score: 200.0  epsilon: 1.0    steps: 928  evaluation reward: 254.05\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2533: Policy loss: -3.695067. Value loss: 206.705688. Entropy: 0.433504.\n",
      "Iteration 2534: Policy loss: -3.502314. Value loss: 104.719223. Entropy: 0.471143.\n",
      "Iteration 2535: Policy loss: -4.181687. Value loss: 77.473289. Entropy: 0.478197.\n",
      "episode: 1066   score: 260.0  epsilon: 1.0    steps: 26  evaluation reward: 255.3\n",
      "episode: 1067   score: 290.0  epsilon: 1.0    steps: 279  evaluation reward: 255.85\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2536: Policy loss: 1.270903. Value loss: 37.325329. Entropy: 0.468915.\n",
      "Iteration 2537: Policy loss: 1.192239. Value loss: 20.556805. Entropy: 0.445616.\n",
      "Iteration 2538: Policy loss: 1.374615. Value loss: 16.376553. Entropy: 0.446756.\n",
      "episode: 1068   score: 275.0  epsilon: 1.0    steps: 536  evaluation reward: 256.8\n",
      "episode: 1069   score: 495.0  epsilon: 1.0    steps: 889  evaluation reward: 260.65\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2539: Policy loss: -0.367743. Value loss: 67.734161. Entropy: 0.435394.\n",
      "Iteration 2540: Policy loss: -0.445093. Value loss: 28.978661. Entropy: 0.453460.\n",
      "Iteration 2541: Policy loss: -0.256329. Value loss: 21.035295. Entropy: 0.412339.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2542: Policy loss: -2.324896. Value loss: 119.314926. Entropy: 0.273462.\n",
      "Iteration 2543: Policy loss: -2.305103. Value loss: 27.242994. Entropy: 0.267515.\n",
      "Iteration 2544: Policy loss: -2.955255. Value loss: 23.846716. Entropy: 0.283808.\n",
      "episode: 1070   score: 460.0  epsilon: 1.0    steps: 159  evaluation reward: 262.35\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2545: Policy loss: 2.490932. Value loss: 79.152237. Entropy: 0.504318.\n",
      "Iteration 2546: Policy loss: 3.163571. Value loss: 16.966934. Entropy: 0.522450.\n",
      "Iteration 2547: Policy loss: 2.413558. Value loss: 10.792690. Entropy: 0.490259.\n",
      "episode: 1071   score: 190.0  epsilon: 1.0    steps: 384  evaluation reward: 263.05\n",
      "episode: 1072   score: 150.0  epsilon: 1.0    steps: 675  evaluation reward: 262.75\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2548: Policy loss: -1.298279. Value loss: 39.329826. Entropy: 0.525504.\n",
      "Iteration 2549: Policy loss: -0.960959. Value loss: 29.247459. Entropy: 0.513358.\n",
      "Iteration 2550: Policy loss: -1.310117. Value loss: 26.314316. Entropy: 0.505458.\n",
      "episode: 1073   score: 205.0  epsilon: 1.0    steps: 5  evaluation reward: 264.0\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2551: Policy loss: -2.457788. Value loss: 26.854328. Entropy: 0.340819.\n",
      "Iteration 2552: Policy loss: -2.330562. Value loss: 17.019096. Entropy: 0.315951.\n",
      "Iteration 2553: Policy loss: -2.234967. Value loss: 14.834603. Entropy: 0.335107.\n",
      "episode: 1074   score: 750.0  epsilon: 1.0    steps: 431  evaluation reward: 270.25\n",
      "episode: 1075   score: 230.0  epsilon: 1.0    steps: 515  evaluation reward: 270.5\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2554: Policy loss: 0.696091. Value loss: 25.149075. Entropy: 0.369148.\n",
      "Iteration 2555: Policy loss: 0.672503. Value loss: 15.816922. Entropy: 0.377384.\n",
      "Iteration 2556: Policy loss: 0.572058. Value loss: 15.347633. Entropy: 0.391548.\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2557: Policy loss: 1.492782. Value loss: 30.129986. Entropy: 0.454886.\n",
      "Iteration 2558: Policy loss: 1.829983. Value loss: 14.466569. Entropy: 0.427272.\n",
      "Iteration 2559: Policy loss: 1.783258. Value loss: 12.167052. Entropy: 0.433032.\n",
      "episode: 1076   score: 210.0  epsilon: 1.0    steps: 129  evaluation reward: 271.05\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2560: Policy loss: 0.736968. Value loss: 21.734833. Entropy: 0.415647.\n",
      "Iteration 2561: Policy loss: 1.223114. Value loss: 13.644609. Entropy: 0.429631.\n",
      "Iteration 2562: Policy loss: 0.746252. Value loss: 11.827661. Entropy: 0.425933.\n",
      "episode: 1077   score: 295.0  epsilon: 1.0    steps: 784  evaluation reward: 271.9\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2563: Policy loss: -4.099360. Value loss: 241.292908. Entropy: 0.497441.\n",
      "Iteration 2564: Policy loss: -4.545627. Value loss: 214.758591. Entropy: 0.487372.\n",
      "Iteration 2565: Policy loss: -4.141885. Value loss: 165.818466. Entropy: 0.485652.\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2566: Policy loss: -1.379021. Value loss: 69.045776. Entropy: 0.444666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2567: Policy loss: -1.422301. Value loss: 44.463882. Entropy: 0.499967.\n",
      "Iteration 2568: Policy loss: -1.418653. Value loss: 33.290657. Entropy: 0.453538.\n",
      "episode: 1078   score: 325.0  epsilon: 1.0    steps: 348  evaluation reward: 272.75\n",
      "episode: 1079   score: 220.0  epsilon: 1.0    steps: 427  evaluation reward: 272.8\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2569: Policy loss: 1.949825. Value loss: 34.974438. Entropy: 0.506779.\n",
      "Iteration 2570: Policy loss: 1.936129. Value loss: 19.009113. Entropy: 0.428042.\n",
      "Iteration 2571: Policy loss: 1.775492. Value loss: 15.003953. Entropy: 0.447247.\n",
      "episode: 1080   score: 190.0  epsilon: 1.0    steps: 514  evaluation reward: 271.9\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2572: Policy loss: 1.362781. Value loss: 26.705202. Entropy: 0.444312.\n",
      "Iteration 2573: Policy loss: 1.601865. Value loss: 16.238417. Entropy: 0.448368.\n",
      "Iteration 2574: Policy loss: 1.502978. Value loss: 14.197544. Entropy: 0.448937.\n",
      "episode: 1081   score: 265.0  epsilon: 1.0    steps: 2  evaluation reward: 271.9\n",
      "episode: 1082   score: 210.0  epsilon: 1.0    steps: 205  evaluation reward: 271.35\n",
      "episode: 1083   score: 590.0  epsilon: 1.0    steps: 908  evaluation reward: 274.5\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2575: Policy loss: 0.285601. Value loss: 27.832424. Entropy: 0.308630.\n",
      "Iteration 2576: Policy loss: 0.459088. Value loss: 17.676649. Entropy: 0.298723.\n",
      "Iteration 2577: Policy loss: 0.257178. Value loss: 14.832567. Entropy: 0.284578.\n",
      "episode: 1084   score: 620.0  epsilon: 1.0    steps: 715  evaluation reward: 278.3\n",
      "episode: 1085   score: 130.0  epsilon: 1.0    steps: 829  evaluation reward: 277.9\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2578: Policy loss: 1.455061. Value loss: 28.605907. Entropy: 0.318228.\n",
      "Iteration 2579: Policy loss: 1.327098. Value loss: 17.950333. Entropy: 0.303623.\n",
      "Iteration 2580: Policy loss: 1.324178. Value loss: 16.692810. Entropy: 0.305369.\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2581: Policy loss: 0.257811. Value loss: 14.208506. Entropy: 0.361693.\n",
      "Iteration 2582: Policy loss: 0.166778. Value loss: 8.550106. Entropy: 0.357372.\n",
      "Iteration 2583: Policy loss: 0.264898. Value loss: 8.296515. Entropy: 0.361957.\n",
      "episode: 1086   score: 205.0  epsilon: 1.0    steps: 408  evaluation reward: 278.9\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2584: Policy loss: -2.089645. Value loss: 27.798218. Entropy: 0.433774.\n",
      "Iteration 2585: Policy loss: -1.935362. Value loss: 20.453142. Entropy: 0.410912.\n",
      "Iteration 2586: Policy loss: -2.035744. Value loss: 18.437933. Entropy: 0.415931.\n",
      "episode: 1087   score: 205.0  epsilon: 1.0    steps: 108  evaluation reward: 279.0\n",
      "episode: 1088   score: 75.0  epsilon: 1.0    steps: 878  evaluation reward: 277.25\n",
      "episode: 1089   score: 170.0  epsilon: 1.0    steps: 1009  evaluation reward: 276.45\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2587: Policy loss: 2.408302. Value loss: 37.377342. Entropy: 0.477893.\n",
      "Iteration 2588: Policy loss: 2.236742. Value loss: 27.094793. Entropy: 0.499569.\n",
      "Iteration 2589: Policy loss: 2.556611. Value loss: 22.728954. Entropy: 0.546051.\n",
      "episode: 1090   score: 245.0  epsilon: 1.0    steps: 212  evaluation reward: 275.9\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2590: Policy loss: 0.144594. Value loss: 20.120291. Entropy: 0.393703.\n",
      "Iteration 2591: Policy loss: -0.208388. Value loss: 12.739370. Entropy: 0.390982.\n",
      "Iteration 2592: Policy loss: -0.116459. Value loss: 10.178694. Entropy: 0.425488.\n",
      "episode: 1091   score: 240.0  epsilon: 1.0    steps: 274  evaluation reward: 271.25\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2593: Policy loss: -1.081594. Value loss: 27.754921. Entropy: 0.338030.\n",
      "Iteration 2594: Policy loss: -1.137109. Value loss: 21.318693. Entropy: 0.355361.\n",
      "Iteration 2595: Policy loss: -1.006749. Value loss: 17.525799. Entropy: 0.424556.\n",
      "episode: 1092   score: 545.0  epsilon: 1.0    steps: 541  evaluation reward: 275.15\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2596: Policy loss: -0.479490. Value loss: 23.984085. Entropy: 0.496530.\n",
      "Iteration 2597: Policy loss: -0.430695. Value loss: 16.562698. Entropy: 0.459724.\n",
      "Iteration 2598: Policy loss: -0.330408. Value loss: 12.857138. Entropy: 0.486353.\n",
      "episode: 1093   score: 220.0  epsilon: 1.0    steps: 412  evaluation reward: 276.6\n",
      "episode: 1094   score: 255.0  epsilon: 1.0    steps: 690  evaluation reward: 275.3\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2599: Policy loss: 1.330389. Value loss: 27.649582. Entropy: 0.483460.\n",
      "Iteration 2600: Policy loss: 1.532342. Value loss: 17.391739. Entropy: 0.512606.\n",
      "Iteration 2601: Policy loss: 1.433537. Value loss: 13.893685. Entropy: 0.540387.\n",
      "episode: 1095   score: 200.0  epsilon: 1.0    steps: 887  evaluation reward: 275.95\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2602: Policy loss: 0.000951. Value loss: 30.349115. Entropy: 0.609289.\n",
      "Iteration 2603: Policy loss: -0.042776. Value loss: 20.666721. Entropy: 0.552280.\n",
      "Iteration 2604: Policy loss: -0.204524. Value loss: 15.435760. Entropy: 0.556377.\n",
      "episode: 1096   score: 215.0  epsilon: 1.0    steps: 125  evaluation reward: 272.4\n",
      "episode: 1097   score: 120.0  epsilon: 1.0    steps: 378  evaluation reward: 270.6\n",
      "episode: 1098   score: 210.0  epsilon: 1.0    steps: 909  evaluation reward: 269.95\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2605: Policy loss: 1.520917. Value loss: 27.723682. Entropy: 0.423836.\n",
      "Iteration 2606: Policy loss: 1.566360. Value loss: 18.343100. Entropy: 0.391392.\n",
      "Iteration 2607: Policy loss: 1.597947. Value loss: 16.442043. Entropy: 0.411446.\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2608: Policy loss: 0.505627. Value loss: 18.100473. Entropy: 0.383928.\n",
      "Iteration 2609: Policy loss: 0.324698. Value loss: 13.807922. Entropy: 0.377569.\n",
      "Iteration 2610: Policy loss: 0.388827. Value loss: 10.408491. Entropy: 0.394266.\n",
      "episode: 1099   score: 165.0  epsilon: 1.0    steps: 528  evaluation reward: 270.55\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2611: Policy loss: -1.062508. Value loss: 39.150181. Entropy: 0.393439.\n",
      "Iteration 2612: Policy loss: -0.991250. Value loss: 22.619999. Entropy: 0.384867.\n",
      "Iteration 2613: Policy loss: -1.055914. Value loss: 18.741470. Entropy: 0.364576.\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2614: Policy loss: -1.415372. Value loss: 34.521202. Entropy: 0.477399.\n",
      "Iteration 2615: Policy loss: -1.427912. Value loss: 18.387178. Entropy: 0.466411.\n",
      "Iteration 2616: Policy loss: -1.491185. Value loss: 15.161865. Entropy: 0.476336.\n",
      "episode: 1100   score: 280.0  epsilon: 1.0    steps: 693  evaluation reward: 270.95\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2617: Policy loss: 1.707549. Value loss: 29.618185. Entropy: 0.536929.\n",
      "Iteration 2618: Policy loss: 1.890565. Value loss: 17.998055. Entropy: 0.520399.\n",
      "Iteration 2619: Policy loss: 1.453944. Value loss: 12.144684. Entropy: 0.528674.\n",
      "now time :  2019-02-26 13:16:49.892121\n",
      "episode: 1101   score: 175.0  epsilon: 1.0    steps: 110  evaluation reward: 268.5\n",
      "episode: 1102   score: 335.0  epsilon: 1.0    steps: 145  evaluation reward: 269.45\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2620: Policy loss: -2.981873. Value loss: 264.185242. Entropy: 0.452834.\n",
      "Iteration 2621: Policy loss: -2.805321. Value loss: 100.148491. Entropy: 0.472343.\n",
      "Iteration 2622: Policy loss: -2.257031. Value loss: 67.405594. Entropy: 0.458036.\n",
      "episode: 1103   score: 255.0  epsilon: 1.0    steps: 866  evaluation reward: 267.8\n",
      "episode: 1104   score: 540.0  epsilon: 1.0    steps: 1014  evaluation reward: 271.35\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2623: Policy loss: -1.175050. Value loss: 277.562531. Entropy: 0.426379.\n",
      "Iteration 2624: Policy loss: -0.247615. Value loss: 127.306557. Entropy: 0.381269.\n",
      "Iteration 2625: Policy loss: -0.613222. Value loss: 90.634178. Entropy: 0.461023.\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2626: Policy loss: -1.526744. Value loss: 47.781796. Entropy: 0.446235.\n",
      "Iteration 2627: Policy loss: -1.625874. Value loss: 32.617413. Entropy: 0.465973.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2628: Policy loss: -1.882595. Value loss: 29.553719. Entropy: 0.437971.\n",
      "episode: 1105   score: 345.0  epsilon: 1.0    steps: 321  evaluation reward: 272.7\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2629: Policy loss: 1.251305. Value loss: 30.265930. Entropy: 0.525082.\n",
      "Iteration 2630: Policy loss: 1.733644. Value loss: 18.092102. Entropy: 0.530175.\n",
      "Iteration 2631: Policy loss: 1.533573. Value loss: 15.163594. Entropy: 0.508286.\n",
      "episode: 1106   score: 460.0  epsilon: 1.0    steps: 447  evaluation reward: 275.2\n",
      "episode: 1107   score: 435.0  epsilon: 1.0    steps: 603  evaluation reward: 278.15\n",
      "episode: 1108   score: 245.0  epsilon: 1.0    steps: 687  evaluation reward: 279.25\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2632: Policy loss: 0.171768. Value loss: 30.270229. Entropy: 0.447448.\n",
      "Iteration 2633: Policy loss: -0.030293. Value loss: 15.754935. Entropy: 0.453132.\n",
      "Iteration 2634: Policy loss: 0.264177. Value loss: 13.187986. Entropy: 0.435547.\n",
      "episode: 1109   score: 100.0  epsilon: 1.0    steps: 15  evaluation reward: 275.3\n",
      "episode: 1110   score: 135.0  epsilon: 1.0    steps: 993  evaluation reward: 275.3\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2635: Policy loss: 1.311831. Value loss: 31.069063. Entropy: 0.343016.\n",
      "Iteration 2636: Policy loss: 1.366048. Value loss: 19.123310. Entropy: 0.363540.\n",
      "Iteration 2637: Policy loss: 1.580263. Value loss: 16.426994. Entropy: 0.358879.\n",
      "episode: 1111   score: 265.0  epsilon: 1.0    steps: 170  evaluation reward: 276.2\n",
      "episode: 1112   score: 220.0  epsilon: 1.0    steps: 855  evaluation reward: 277.2\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2638: Policy loss: 1.195956. Value loss: 28.103212. Entropy: 0.385003.\n",
      "Iteration 2639: Policy loss: 1.148083. Value loss: 20.406433. Entropy: 0.307435.\n",
      "Iteration 2640: Policy loss: 1.065464. Value loss: 17.018700. Entropy: 0.383137.\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2641: Policy loss: -5.730617. Value loss: 230.014145. Entropy: 0.425544.\n",
      "Iteration 2642: Policy loss: -5.524939. Value loss: 118.918839. Entropy: 0.388098.\n",
      "Iteration 2643: Policy loss: -6.252553. Value loss: 129.272156. Entropy: 0.360142.\n",
      "episode: 1113   score: 240.0  epsilon: 1.0    steps: 316  evaluation reward: 276.95\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2644: Policy loss: 1.027349. Value loss: 64.160873. Entropy: 0.437486.\n",
      "Iteration 2645: Policy loss: 0.898102. Value loss: 34.110336. Entropy: 0.406423.\n",
      "Iteration 2646: Policy loss: 0.730803. Value loss: 29.741446. Entropy: 0.408367.\n",
      "episode: 1114   score: 220.0  epsilon: 1.0    steps: 431  evaluation reward: 271.85\n",
      "episode: 1115   score: 215.0  epsilon: 1.0    steps: 759  evaluation reward: 268.9\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2647: Policy loss: 2.119724. Value loss: 57.889874. Entropy: 0.223698.\n",
      "Iteration 2648: Policy loss: 2.086629. Value loss: 33.596935. Entropy: 0.255648.\n",
      "Iteration 2649: Policy loss: 1.849380. Value loss: 33.658531. Entropy: 0.305772.\n",
      "episode: 1116   score: 195.0  epsilon: 1.0    steps: 20  evaluation reward: 268.75\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2650: Policy loss: 1.640661. Value loss: 36.271877. Entropy: 0.461265.\n",
      "Iteration 2651: Policy loss: 1.509963. Value loss: 25.469494. Entropy: 0.570568.\n",
      "Iteration 2652: Policy loss: 1.480470. Value loss: 23.593679. Entropy: 0.520785.\n",
      "episode: 1117   score: 105.0  epsilon: 1.0    steps: 336  evaluation reward: 267.7\n",
      "episode: 1118   score: 460.0  epsilon: 1.0    steps: 528  evaluation reward: 270.2\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2653: Policy loss: 1.008026. Value loss: 37.722298. Entropy: 0.484988.\n",
      "Iteration 2654: Policy loss: 0.692390. Value loss: 27.504269. Entropy: 0.487366.\n",
      "Iteration 2655: Policy loss: 0.766826. Value loss: 20.348423. Entropy: 0.512620.\n",
      "episode: 1119   score: 105.0  epsilon: 1.0    steps: 732  evaluation reward: 269.0\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2656: Policy loss: 1.429133. Value loss: 40.196945. Entropy: 0.604807.\n",
      "Iteration 2657: Policy loss: 1.479777. Value loss: 26.930744. Entropy: 0.600522.\n",
      "Iteration 2658: Policy loss: 1.593098. Value loss: 24.999607. Entropy: 0.584268.\n",
      "episode: 1120   score: 285.0  epsilon: 1.0    steps: 207  evaluation reward: 270.05\n",
      "episode: 1121   score: 220.0  epsilon: 1.0    steps: 783  evaluation reward: 271.2\n",
      "episode: 1122   score: 265.0  epsilon: 1.0    steps: 908  evaluation reward: 272.05\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2659: Policy loss: -0.374888. Value loss: 29.042891. Entropy: 0.578699.\n",
      "Iteration 2660: Policy loss: -0.081221. Value loss: 16.136194. Entropy: 0.564103.\n",
      "Iteration 2661: Policy loss: -0.362365. Value loss: 13.335770. Entropy: 0.569214.\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2662: Policy loss: 1.028710. Value loss: 43.017948. Entropy: 0.363215.\n",
      "Iteration 2663: Policy loss: 0.649957. Value loss: 30.220123. Entropy: 0.348534.\n",
      "Iteration 2664: Policy loss: 0.938712. Value loss: 25.057199. Entropy: 0.379667.\n",
      "episode: 1123   score: 210.0  epsilon: 1.0    steps: 48  evaluation reward: 272.65\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2665: Policy loss: 2.340046. Value loss: 33.817978. Entropy: 0.474728.\n",
      "Iteration 2666: Policy loss: 2.537700. Value loss: 18.607031. Entropy: 0.478684.\n",
      "Iteration 2667: Policy loss: 2.288856. Value loss: 15.076550. Entropy: 0.477305.\n",
      "episode: 1124   score: 60.0  epsilon: 1.0    steps: 832  evaluation reward: 269.15\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2668: Policy loss: -1.814237. Value loss: 208.886108. Entropy: 0.424670.\n",
      "Iteration 2669: Policy loss: -1.543195. Value loss: 137.467209. Entropy: 0.402553.\n",
      "Iteration 2670: Policy loss: -1.527434. Value loss: 111.116776. Entropy: 0.442839.\n",
      "episode: 1125   score: 295.0  epsilon: 1.0    steps: 386  evaluation reward: 271.3\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2671: Policy loss: -0.288463. Value loss: 33.854645. Entropy: 0.374844.\n",
      "Iteration 2672: Policy loss: -0.462096. Value loss: 22.933210. Entropy: 0.355888.\n",
      "Iteration 2673: Policy loss: -0.254936. Value loss: 18.047647. Entropy: 0.358138.\n",
      "episode: 1126   score: 260.0  epsilon: 1.0    steps: 366  evaluation reward: 272.55\n",
      "episode: 1127   score: 275.0  epsilon: 1.0    steps: 594  evaluation reward: 272.3\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2674: Policy loss: 1.957745. Value loss: 43.698830. Entropy: 0.387362.\n",
      "Iteration 2675: Policy loss: 2.219567. Value loss: 25.875223. Entropy: 0.383274.\n",
      "Iteration 2676: Policy loss: 1.693610. Value loss: 18.410229. Entropy: 0.396848.\n",
      "episode: 1128   score: 285.0  epsilon: 1.0    steps: 674  evaluation reward: 275.0\n",
      "episode: 1129   score: 30.0  epsilon: 1.0    steps: 884  evaluation reward: 272.5\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2677: Policy loss: 3.195278. Value loss: 29.947193. Entropy: 0.589563.\n",
      "Iteration 2678: Policy loss: 3.521127. Value loss: 11.541980. Entropy: 0.615058.\n",
      "Iteration 2679: Policy loss: 3.264523. Value loss: 9.281506. Entropy: 0.584858.\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2680: Policy loss: 3.702426. Value loss: 32.359665. Entropy: 0.540010.\n",
      "Iteration 2681: Policy loss: 3.275900. Value loss: 18.178482. Entropy: 0.473823.\n",
      "Iteration 2682: Policy loss: 3.221112. Value loss: 16.091301. Entropy: 0.540958.\n",
      "episode: 1130   score: 345.0  epsilon: 1.0    steps: 241  evaluation reward: 272.25\n",
      "episode: 1131   score: 495.0  epsilon: 1.0    steps: 929  evaluation reward: 271.75\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2683: Policy loss: 0.457405. Value loss: 26.661188. Entropy: 0.504408.\n",
      "Iteration 2684: Policy loss: 0.776766. Value loss: 18.840212. Entropy: 0.489939.\n",
      "Iteration 2685: Policy loss: 0.623350. Value loss: 15.676918. Entropy: 0.511530.\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2686: Policy loss: 0.368958. Value loss: 25.382662. Entropy: 0.357397.\n",
      "Iteration 2687: Policy loss: 0.134892. Value loss: 15.189495. Entropy: 0.325364.\n",
      "Iteration 2688: Policy loss: 0.610336. Value loss: 11.688451. Entropy: 0.330702.\n",
      "episode: 1132   score: 305.0  epsilon: 1.0    steps: 71  evaluation reward: 268.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2689: Policy loss: -0.109741. Value loss: 22.062237. Entropy: 0.337546.\n",
      "Iteration 2690: Policy loss: 0.019269. Value loss: 16.610226. Entropy: 0.357432.\n",
      "Iteration 2691: Policy loss: -0.168237. Value loss: 16.206587. Entropy: 0.372025.\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2692: Policy loss: -1.037794. Value loss: 28.457348. Entropy: 0.345785.\n",
      "Iteration 2693: Policy loss: -1.042592. Value loss: 17.973328. Entropy: 0.301555.\n",
      "Iteration 2694: Policy loss: -1.049241. Value loss: 15.879011. Entropy: 0.332941.\n",
      "episode: 1133   score: 210.0  epsilon: 1.0    steps: 735  evaluation reward: 265.05\n",
      "episode: 1134   score: 225.0  epsilon: 1.0    steps: 798  evaluation reward: 265.2\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2695: Policy loss: -1.346569. Value loss: 254.995300. Entropy: 0.261764.\n",
      "Iteration 2696: Policy loss: -0.099847. Value loss: 136.716721. Entropy: 0.289440.\n",
      "Iteration 2697: Policy loss: -0.897433. Value loss: 133.273331. Entropy: 0.239322.\n",
      "episode: 1135   score: 180.0  epsilon: 1.0    steps: 223  evaluation reward: 265.2\n",
      "episode: 1136   score: 290.0  epsilon: 1.0    steps: 371  evaluation reward: 265.5\n",
      "episode: 1137   score: 210.0  epsilon: 1.0    steps: 458  evaluation reward: 265.35\n",
      "episode: 1138   score: 215.0  epsilon: 1.0    steps: 1014  evaluation reward: 261.95\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2698: Policy loss: 3.166339. Value loss: 45.881973. Entropy: 0.351605.\n",
      "Iteration 2699: Policy loss: 3.490796. Value loss: 29.984367. Entropy: 0.430055.\n",
      "Iteration 2700: Policy loss: 3.257921. Value loss: 25.424911. Entropy: 0.455078.\n",
      "episode: 1139   score: 460.0  epsilon: 1.0    steps: 521  evaluation reward: 264.45\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2701: Policy loss: 2.629460. Value loss: 21.469318. Entropy: 0.328512.\n",
      "Iteration 2702: Policy loss: 2.462400. Value loss: 14.546355. Entropy: 0.558873.\n",
      "Iteration 2703: Policy loss: 2.555294. Value loss: 11.889961. Entropy: 0.570969.\n",
      "episode: 1140   score: 105.0  epsilon: 1.0    steps: 772  evaluation reward: 263.4\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2704: Policy loss: 1.468578. Value loss: 27.306883. Entropy: 0.413874.\n",
      "Iteration 2705: Policy loss: 1.576785. Value loss: 18.326628. Entropy: 0.284015.\n",
      "Iteration 2706: Policy loss: 1.428186. Value loss: 17.454855. Entropy: 0.308983.\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2707: Policy loss: -0.094552. Value loss: 29.817173. Entropy: 0.220358.\n",
      "Iteration 2708: Policy loss: 0.295069. Value loss: 17.014530. Entropy: 0.202494.\n",
      "Iteration 2709: Policy loss: -0.221076. Value loss: 15.690854. Entropy: 0.226575.\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2710: Policy loss: 0.976516. Value loss: 30.980408. Entropy: 0.156989.\n",
      "Iteration 2711: Policy loss: 0.635107. Value loss: 18.933483. Entropy: 0.168367.\n",
      "Iteration 2712: Policy loss: 0.903522. Value loss: 15.759166. Entropy: 0.157252.\n",
      "episode: 1141   score: 275.0  epsilon: 1.0    steps: 102  evaluation reward: 264.05\n",
      "episode: 1142   score: 180.0  epsilon: 1.0    steps: 147  evaluation reward: 263.25\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2713: Policy loss: 2.963488. Value loss: 22.057152. Entropy: 0.223775.\n",
      "Iteration 2714: Policy loss: 2.903934. Value loss: 12.723173. Entropy: 0.296927.\n",
      "Iteration 2715: Policy loss: 2.820337. Value loss: 11.606857. Entropy: 0.273912.\n",
      "episode: 1143   score: 210.0  epsilon: 1.0    steps: 680  evaluation reward: 260.7\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2716: Policy loss: 0.123918. Value loss: 26.205896. Entropy: 0.215253.\n",
      "Iteration 2717: Policy loss: 0.190713. Value loss: 20.194643. Entropy: 0.220103.\n",
      "Iteration 2718: Policy loss: 0.258535. Value loss: 17.777430. Entropy: 0.221002.\n",
      "episode: 1144   score: 210.0  epsilon: 1.0    steps: 330  evaluation reward: 258.7\n",
      "episode: 1145   score: 180.0  epsilon: 1.0    steps: 420  evaluation reward: 259.45\n",
      "episode: 1146   score: 190.0  epsilon: 1.0    steps: 549  evaluation reward: 258.75\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2719: Policy loss: 0.620563. Value loss: 29.201399. Entropy: 0.253283.\n",
      "Iteration 2720: Policy loss: 0.992787. Value loss: 15.724428. Entropy: 0.318256.\n",
      "Iteration 2721: Policy loss: 0.449778. Value loss: 13.345057. Entropy: 0.312269.\n",
      "episode: 1147   score: 180.0  epsilon: 1.0    steps: 879  evaluation reward: 256.1\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2722: Policy loss: 1.979234. Value loss: 19.779446. Entropy: 0.293391.\n",
      "Iteration 2723: Policy loss: 2.058534. Value loss: 11.709043. Entropy: 0.307813.\n",
      "Iteration 2724: Policy loss: 1.989418. Value loss: 9.315615. Entropy: 0.303544.\n",
      "episode: 1148   score: 325.0  epsilon: 1.0    steps: 983  evaluation reward: 257.25\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2725: Policy loss: 2.050395. Value loss: 21.720156. Entropy: 0.420170.\n",
      "Iteration 2726: Policy loss: 2.147268. Value loss: 12.027325. Entropy: 0.417139.\n",
      "Iteration 2727: Policy loss: 2.129200. Value loss: 9.739029. Entropy: 0.438414.\n",
      "episode: 1149   score: 110.0  epsilon: 1.0    steps: 186  evaluation reward: 256.55\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2728: Policy loss: 2.215474. Value loss: 22.298124. Entropy: 0.386228.\n",
      "Iteration 2729: Policy loss: 2.164090. Value loss: 13.649137. Entropy: 0.351933.\n",
      "Iteration 2730: Policy loss: 2.006902. Value loss: 11.478755. Entropy: 0.405637.\n",
      "episode: 1150   score: 105.0  epsilon: 1.0    steps: 657  evaluation reward: 256.8\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2731: Policy loss: -0.593643. Value loss: 23.269920. Entropy: 0.261766.\n",
      "Iteration 2732: Policy loss: -0.745386. Value loss: 15.145185. Entropy: 0.293471.\n",
      "Iteration 2733: Policy loss: -0.489428. Value loss: 13.711390. Entropy: 0.285723.\n",
      "now time :  2019-02-26 13:18:59.913675\n",
      "episode: 1151   score: 80.0  epsilon: 1.0    steps: 285  evaluation reward: 254.95\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2734: Policy loss: 0.401206. Value loss: 30.211216. Entropy: 0.485391.\n",
      "Iteration 2735: Policy loss: 0.360517. Value loss: 21.713030. Entropy: 0.448884.\n",
      "Iteration 2736: Policy loss: 0.444374. Value loss: 16.873425. Entropy: 0.506977.\n",
      "episode: 1152   score: 210.0  epsilon: 1.0    steps: 634  evaluation reward: 255.75\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2737: Policy loss: -2.087967. Value loss: 276.703094. Entropy: 0.417120.\n",
      "Iteration 2738: Policy loss: -2.471685. Value loss: 159.043427. Entropy: 0.397589.\n",
      "Iteration 2739: Policy loss: -1.262659. Value loss: 90.409004. Entropy: 0.396366.\n",
      "episode: 1153   score: 290.0  epsilon: 1.0    steps: 414  evaluation reward: 254.65\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2740: Policy loss: 1.579312. Value loss: 32.422245. Entropy: 0.352873.\n",
      "Iteration 2741: Policy loss: 1.497523. Value loss: 19.248678. Entropy: 0.345052.\n",
      "Iteration 2742: Policy loss: 1.675034. Value loss: 16.260382. Entropy: 0.362777.\n",
      "episode: 1154   score: 515.0  epsilon: 1.0    steps: 111  evaluation reward: 258.75\n",
      "episode: 1155   score: 135.0  epsilon: 1.0    steps: 160  evaluation reward: 259.4\n",
      "episode: 1156   score: 295.0  epsilon: 1.0    steps: 850  evaluation reward: 259.85\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2743: Policy loss: 1.076395. Value loss: 68.586159. Entropy: 0.384430.\n",
      "Iteration 2744: Policy loss: 1.314211. Value loss: 41.263180. Entropy: 0.405379.\n",
      "Iteration 2745: Policy loss: 1.007267. Value loss: 34.984619. Entropy: 0.431786.\n",
      "episode: 1157   score: 225.0  epsilon: 1.0    steps: 981  evaluation reward: 257.55\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2746: Policy loss: 1.479015. Value loss: 35.401234. Entropy: 0.434197.\n",
      "Iteration 2747: Policy loss: 1.353135. Value loss: 20.684481. Entropy: 0.471212.\n",
      "Iteration 2748: Policy loss: 1.349645. Value loss: 15.723160. Entropy: 0.479624.\n",
      "episode: 1158   score: 175.0  epsilon: 1.0    steps: 698  evaluation reward: 256.25\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2749: Policy loss: 2.863498. Value loss: 41.794708. Entropy: 0.461593.\n",
      "Iteration 2750: Policy loss: 2.839715. Value loss: 25.024317. Entropy: 0.472156.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2751: Policy loss: 2.906044. Value loss: 19.169172. Entropy: 0.538533.\n",
      "episode: 1159   score: 145.0  epsilon: 1.0    steps: 169  evaluation reward: 253.75\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2752: Policy loss: 0.335826. Value loss: 22.001699. Entropy: 0.786891.\n",
      "Iteration 2753: Policy loss: 0.247425. Value loss: 14.579674. Entropy: 0.755266.\n",
      "Iteration 2754: Policy loss: 0.348698. Value loss: 12.102354. Entropy: 0.756864.\n",
      "episode: 1160   score: 195.0  epsilon: 1.0    steps: 351  evaluation reward: 254.6\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2755: Policy loss: -1.720709. Value loss: 51.092899. Entropy: 0.656760.\n",
      "Iteration 2756: Policy loss: -2.402290. Value loss: 29.690002. Entropy: 0.591446.\n",
      "Iteration 2757: Policy loss: -1.971863. Value loss: 24.360291. Entropy: 0.605505.\n",
      "episode: 1161   score: 290.0  epsilon: 1.0    steps: 601  evaluation reward: 256.2\n",
      "episode: 1162   score: 195.0  epsilon: 1.0    steps: 863  evaluation reward: 256.15\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2758: Policy loss: 0.617367. Value loss: 43.385082. Entropy: 0.708702.\n",
      "Iteration 2759: Policy loss: 0.612053. Value loss: 26.104713. Entropy: 0.748123.\n",
      "Iteration 2760: Policy loss: 0.480529. Value loss: 20.448774. Entropy: 0.737940.\n",
      "episode: 1163   score: 190.0  epsilon: 1.0    steps: 30  evaluation reward: 254.65\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2761: Policy loss: 0.830082. Value loss: 34.851200. Entropy: 0.712545.\n",
      "Iteration 2762: Policy loss: 0.950740. Value loss: 21.996239. Entropy: 0.723968.\n",
      "Iteration 2763: Policy loss: 0.756323. Value loss: 17.865808. Entropy: 0.726685.\n",
      "episode: 1164   score: 340.0  epsilon: 1.0    steps: 435  evaluation reward: 254.3\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2764: Policy loss: 1.709317. Value loss: 38.962662. Entropy: 0.554658.\n",
      "Iteration 2765: Policy loss: 1.699466. Value loss: 23.075050. Entropy: 0.513806.\n",
      "Iteration 2766: Policy loss: 1.945691. Value loss: 18.592144. Entropy: 0.514853.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2767: Policy loss: 0.341096. Value loss: 222.546356. Entropy: 0.565288.\n",
      "Iteration 2768: Policy loss: 0.355747. Value loss: 171.843262. Entropy: 0.454053.\n",
      "Iteration 2769: Policy loss: 0.365032. Value loss: 63.528385. Entropy: 0.456040.\n",
      "episode: 1165   score: 45.0  epsilon: 1.0    steps: 55  evaluation reward: 252.75\n",
      "episode: 1166   score: 270.0  epsilon: 1.0    steps: 957  evaluation reward: 252.85\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2770: Policy loss: -0.730365. Value loss: 47.299511. Entropy: 0.530559.\n",
      "Iteration 2771: Policy loss: -0.875246. Value loss: 31.254131. Entropy: 0.516354.\n",
      "Iteration 2772: Policy loss: -0.745867. Value loss: 27.316175. Entropy: 0.514091.\n",
      "episode: 1167   score: 520.0  epsilon: 1.0    steps: 733  evaluation reward: 255.15\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2773: Policy loss: -0.761467. Value loss: 46.741848. Entropy: 0.370280.\n",
      "Iteration 2774: Policy loss: -0.436428. Value loss: 21.180731. Entropy: 0.341209.\n",
      "Iteration 2775: Policy loss: -0.431361. Value loss: 16.382795. Entropy: 0.370802.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2776: Policy loss: 0.228235. Value loss: 41.318451. Entropy: 0.502241.\n",
      "Iteration 2777: Policy loss: 0.001504. Value loss: 26.447994. Entropy: 0.529066.\n",
      "Iteration 2778: Policy loss: 0.341806. Value loss: 20.352541. Entropy: 0.506441.\n",
      "episode: 1168   score: 280.0  epsilon: 1.0    steps: 200  evaluation reward: 255.2\n",
      "episode: 1169   score: 195.0  epsilon: 1.0    steps: 477  evaluation reward: 252.2\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2779: Policy loss: -0.725440. Value loss: 38.089314. Entropy: 0.447226.\n",
      "Iteration 2780: Policy loss: -0.277163. Value loss: 21.424622. Entropy: 0.438602.\n",
      "Iteration 2781: Policy loss: -0.648250. Value loss: 19.196148. Entropy: 0.452916.\n",
      "episode: 1170   score: 470.0  epsilon: 1.0    steps: 368  evaluation reward: 252.3\n",
      "episode: 1171   score: 255.0  epsilon: 1.0    steps: 528  evaluation reward: 252.95\n",
      "episode: 1172   score: 335.0  epsilon: 1.0    steps: 890  evaluation reward: 254.8\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2782: Policy loss: 1.508175. Value loss: 38.890575. Entropy: 0.349452.\n",
      "Iteration 2783: Policy loss: 1.655759. Value loss: 23.392548. Entropy: 0.338485.\n",
      "Iteration 2784: Policy loss: 1.243029. Value loss: 17.825020. Entropy: 0.385622.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2785: Policy loss: -2.144925. Value loss: 27.010057. Entropy: 0.183889.\n",
      "Iteration 2786: Policy loss: -2.026706. Value loss: 18.425735. Entropy: 0.179683.\n",
      "Iteration 2787: Policy loss: -2.103155. Value loss: 17.690409. Entropy: 0.182900.\n",
      "episode: 1173   score: 240.0  epsilon: 1.0    steps: 95  evaluation reward: 255.15\n",
      "episode: 1174   score: 200.0  epsilon: 1.0    steps: 764  evaluation reward: 249.65\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2788: Policy loss: 1.483337. Value loss: 25.033974. Entropy: 0.343448.\n",
      "Iteration 2789: Policy loss: 1.476589. Value loss: 16.414183. Entropy: 0.318642.\n",
      "Iteration 2790: Policy loss: 1.643373. Value loss: 13.697481. Entropy: 0.304215.\n",
      "episode: 1175   score: 345.0  epsilon: 1.0    steps: 968  evaluation reward: 250.8\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2791: Policy loss: 0.880231. Value loss: 26.959084. Entropy: 0.295178.\n",
      "Iteration 2792: Policy loss: 0.853697. Value loss: 14.763284. Entropy: 0.292889.\n",
      "Iteration 2793: Policy loss: 0.673133. Value loss: 12.712494. Entropy: 0.322950.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2794: Policy loss: 0.987942. Value loss: 20.983534. Entropy: 0.142492.\n",
      "Iteration 2795: Policy loss: 0.744383. Value loss: 12.090947. Entropy: 0.160509.\n",
      "Iteration 2796: Policy loss: 1.062489. Value loss: 10.697955. Entropy: 0.162104.\n",
      "episode: 1176   score: 225.0  epsilon: 1.0    steps: 255  evaluation reward: 250.95\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2797: Policy loss: -1.352072. Value loss: 36.726124. Entropy: 0.260333.\n",
      "Iteration 2798: Policy loss: -1.073757. Value loss: 24.437056. Entropy: 0.264464.\n",
      "Iteration 2799: Policy loss: -1.375254. Value loss: 20.770037. Entropy: 0.230657.\n",
      "episode: 1177   score: 75.0  epsilon: 1.0    steps: 657  evaluation reward: 248.75\n",
      "episode: 1178   score: 200.0  epsilon: 1.0    steps: 840  evaluation reward: 247.5\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2800: Policy loss: 1.908045. Value loss: 33.387829. Entropy: 0.474235.\n",
      "Iteration 2801: Policy loss: 1.908067. Value loss: 23.417448. Entropy: 0.473078.\n",
      "Iteration 2802: Policy loss: 2.187732. Value loss: 17.563410. Entropy: 0.429587.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2803: Policy loss: 1.257820. Value loss: 28.832582. Entropy: 0.343910.\n",
      "Iteration 2804: Policy loss: 1.339571. Value loss: 21.600325. Entropy: 0.370221.\n",
      "Iteration 2805: Policy loss: 1.312421. Value loss: 17.704746. Entropy: 0.379169.\n",
      "episode: 1179   score: 205.0  epsilon: 1.0    steps: 117  evaluation reward: 247.35\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2806: Policy loss: -1.367230. Value loss: 52.993031. Entropy: 0.400646.\n",
      "Iteration 2807: Policy loss: -1.537359. Value loss: 32.451405. Entropy: 0.412769.\n",
      "Iteration 2808: Policy loss: -1.260000. Value loss: 27.805090. Entropy: 0.444163.\n",
      "episode: 1180   score: 370.0  epsilon: 1.0    steps: 386  evaluation reward: 249.15\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2809: Policy loss: 1.965594. Value loss: 22.362078. Entropy: 0.325655.\n",
      "Iteration 2810: Policy loss: 1.996833. Value loss: 12.357865. Entropy: 0.341300.\n",
      "Iteration 2811: Policy loss: 1.854771. Value loss: 9.979204. Entropy: 0.356717.\n",
      "episode: 1181   score: 480.0  epsilon: 1.0    steps: 283  evaluation reward: 251.3\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2812: Policy loss: -0.228480. Value loss: 32.346874. Entropy: 0.452883.\n",
      "Iteration 2813: Policy loss: -0.164677. Value loss: 20.765205. Entropy: 0.453976.\n",
      "Iteration 2814: Policy loss: -0.087656. Value loss: 17.967321. Entropy: 0.447511.\n",
      "episode: 1182   score: 230.0  epsilon: 1.0    steps: 701  evaluation reward: 251.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2815: Policy loss: 0.592289. Value loss: 35.006107. Entropy: 0.388277.\n",
      "Iteration 2816: Policy loss: 0.863799. Value loss: 20.904135. Entropy: 0.432565.\n",
      "Iteration 2817: Policy loss: 0.878595. Value loss: 14.070838. Entropy: 0.455264.\n",
      "episode: 1183   score: 460.0  epsilon: 1.0    steps: 564  evaluation reward: 250.2\n",
      "episode: 1184   score: 215.0  epsilon: 1.0    steps: 853  evaluation reward: 246.15\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2818: Policy loss: 0.907194. Value loss: 40.084076. Entropy: 0.654945.\n",
      "Iteration 2819: Policy loss: 0.634081. Value loss: 29.769474. Entropy: 0.612965.\n",
      "Iteration 2820: Policy loss: 0.812464. Value loss: 23.095398. Entropy: 0.658658.\n",
      "episode: 1185   score: 315.0  epsilon: 1.0    steps: 159  evaluation reward: 248.0\n",
      "episode: 1186   score: 455.0  epsilon: 1.0    steps: 926  evaluation reward: 250.5\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2821: Policy loss: -0.665778. Value loss: 27.838726. Entropy: 0.347373.\n",
      "Iteration 2822: Policy loss: -0.696466. Value loss: 18.409128. Entropy: 0.356799.\n",
      "Iteration 2823: Policy loss: -0.625839. Value loss: 16.139894. Entropy: 0.377553.\n",
      "episode: 1187   score: 185.0  epsilon: 1.0    steps: 63  evaluation reward: 250.3\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2824: Policy loss: -0.192359. Value loss: 21.370941. Entropy: 0.384374.\n",
      "Iteration 2825: Policy loss: -0.337495. Value loss: 14.775483. Entropy: 0.387136.\n",
      "Iteration 2826: Policy loss: -0.249098. Value loss: 14.714539. Entropy: 0.359711.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2827: Policy loss: -0.727017. Value loss: 15.921669. Entropy: 0.397772.\n",
      "Iteration 2828: Policy loss: -1.108489. Value loss: 14.637976. Entropy: 0.392450.\n",
      "Iteration 2829: Policy loss: -0.556122. Value loss: 11.893221. Entropy: 0.389556.\n",
      "episode: 1188   score: 310.0  epsilon: 1.0    steps: 465  evaluation reward: 252.65\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2830: Policy loss: -2.708063. Value loss: 314.200653. Entropy: 0.586135.\n",
      "Iteration 2831: Policy loss: -2.572232. Value loss: 255.190506. Entropy: 0.534618.\n",
      "Iteration 2832: Policy loss: -2.341487. Value loss: 171.932892. Entropy: 0.481291.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2833: Policy loss: -1.515663. Value loss: 53.462940. Entropy: 0.585382.\n",
      "Iteration 2834: Policy loss: -1.300764. Value loss: 27.735634. Entropy: 0.593437.\n",
      "Iteration 2835: Policy loss: -1.443131. Value loss: 22.145603. Entropy: 0.578149.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2836: Policy loss: -1.096207. Value loss: 51.462215. Entropy: 0.642056.\n",
      "Iteration 2837: Policy loss: -1.075004. Value loss: 31.963703. Entropy: 0.649180.\n",
      "Iteration 2838: Policy loss: -0.858341. Value loss: 27.612844. Entropy: 0.659896.\n",
      "episode: 1189   score: 315.0  epsilon: 1.0    steps: 579  evaluation reward: 254.1\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2839: Policy loss: 1.630710. Value loss: 46.825581. Entropy: 0.597477.\n",
      "Iteration 2840: Policy loss: 1.504192. Value loss: 30.032953. Entropy: 0.612865.\n",
      "Iteration 2841: Policy loss: 1.512963. Value loss: 21.695950. Entropy: 0.589027.\n",
      "episode: 1190   score: 375.0  epsilon: 1.0    steps: 302  evaluation reward: 255.4\n",
      "episode: 1191   score: 350.0  epsilon: 1.0    steps: 716  evaluation reward: 256.5\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2842: Policy loss: 1.881498. Value loss: 25.348907. Entropy: 0.564031.\n",
      "Iteration 2843: Policy loss: 1.445133. Value loss: 18.075159. Entropy: 0.584286.\n",
      "Iteration 2844: Policy loss: 1.814871. Value loss: 11.574222. Entropy: 0.606401.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2845: Policy loss: -0.088899. Value loss: 29.716526. Entropy: 0.430450.\n",
      "Iteration 2846: Policy loss: -0.343382. Value loss: 20.192970. Entropy: 0.391490.\n",
      "Iteration 2847: Policy loss: -0.265655. Value loss: 19.418945. Entropy: 0.453802.\n",
      "episode: 1192   score: 380.0  epsilon: 1.0    steps: 76  evaluation reward: 254.85\n",
      "episode: 1193   score: 215.0  epsilon: 1.0    steps: 496  evaluation reward: 254.8\n",
      "episode: 1194   score: 510.0  epsilon: 1.0    steps: 885  evaluation reward: 257.35\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2848: Policy loss: -0.255157. Value loss: 36.060310. Entropy: 0.491419.\n",
      "Iteration 2849: Policy loss: -0.000658. Value loss: 24.394196. Entropy: 0.489653.\n",
      "Iteration 2850: Policy loss: -0.189263. Value loss: 20.942963. Entropy: 0.480792.\n",
      "episode: 1195   score: 605.0  epsilon: 1.0    steps: 980  evaluation reward: 261.4\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2851: Policy loss: 0.306014. Value loss: 18.511660. Entropy: 0.300431.\n",
      "Iteration 2852: Policy loss: 0.404094. Value loss: 12.016356. Entropy: 0.343108.\n",
      "Iteration 2853: Policy loss: 0.306392. Value loss: 10.699477. Entropy: 0.356682.\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2854: Policy loss: -1.445257. Value loss: 43.649662. Entropy: 0.343795.\n",
      "Iteration 2855: Policy loss: -1.344613. Value loss: 26.514853. Entropy: 0.333602.\n",
      "Iteration 2856: Policy loss: -1.482627. Value loss: 24.897594. Entropy: 0.322429.\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2857: Policy loss: -0.424856. Value loss: 27.225142. Entropy: 0.403476.\n",
      "Iteration 2858: Policy loss: -0.293684. Value loss: 19.292915. Entropy: 0.418111.\n",
      "Iteration 2859: Policy loss: -0.388445. Value loss: 17.331482. Entropy: 0.390436.\n",
      "episode: 1196   score: 600.0  epsilon: 1.0    steps: 254  evaluation reward: 265.25\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2860: Policy loss: 1.466104. Value loss: 29.504181. Entropy: 0.546610.\n",
      "Iteration 2861: Policy loss: 1.610817. Value loss: 21.005445. Entropy: 0.561558.\n",
      "Iteration 2862: Policy loss: 1.318766. Value loss: 18.230616. Entropy: 0.543247.\n",
      "episode: 1197   score: 320.0  epsilon: 1.0    steps: 623  evaluation reward: 267.25\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2863: Policy loss: 2.906310. Value loss: 37.415775. Entropy: 0.466571.\n",
      "Iteration 2864: Policy loss: 2.809630. Value loss: 23.990833. Entropy: 0.445371.\n",
      "Iteration 2865: Policy loss: 2.748615. Value loss: 18.866316. Entropy: 0.477551.\n",
      "episode: 1198   score: 245.0  epsilon: 1.0    steps: 118  evaluation reward: 267.6\n",
      "episode: 1199   score: 220.0  epsilon: 1.0    steps: 821  evaluation reward: 268.15\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2866: Policy loss: -1.147576. Value loss: 45.289204. Entropy: 0.433003.\n",
      "Iteration 2867: Policy loss: -1.148504. Value loss: 24.321796. Entropy: 0.444041.\n",
      "Iteration 2868: Policy loss: -1.104589. Value loss: 19.408966. Entropy: 0.552934.\n",
      "episode: 1200   score: 410.0  epsilon: 1.0    steps: 737  evaluation reward: 269.45\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2869: Policy loss: 1.706481. Value loss: 24.364204. Entropy: 0.533056.\n",
      "Iteration 2870: Policy loss: 1.947238. Value loss: 11.678033. Entropy: 0.519044.\n",
      "Iteration 2871: Policy loss: 1.613987. Value loss: 12.264104. Entropy: 0.536252.\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2872: Policy loss: -1.746400. Value loss: 54.444145. Entropy: 0.423552.\n",
      "Iteration 2873: Policy loss: -2.106536. Value loss: 32.630836. Entropy: 0.361690.\n",
      "Iteration 2874: Policy loss: -1.896622. Value loss: 24.719643. Entropy: 0.361614.\n",
      "now time :  2019-02-26 13:21:43.554789\n",
      "episode: 1201   score: 510.0  epsilon: 1.0    steps: 303  evaluation reward: 272.8\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2875: Policy loss: -0.115654. Value loss: 40.599564. Entropy: 0.413947.\n",
      "Iteration 2876: Policy loss: -0.369442. Value loss: 27.054220. Entropy: 0.409872.\n",
      "Iteration 2877: Policy loss: -0.227430. Value loss: 22.954718. Entropy: 0.415496.\n",
      "episode: 1202   score: 365.0  epsilon: 1.0    steps: 447  evaluation reward: 273.1\n",
      "episode: 1203   score: 325.0  epsilon: 1.0    steps: 998  evaluation reward: 273.8\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2878: Policy loss: 1.308886. Value loss: 29.432058. Entropy: 0.423241.\n",
      "Iteration 2879: Policy loss: 1.235996. Value loss: 16.904791. Entropy: 0.430507.\n",
      "Iteration 2880: Policy loss: 1.203405. Value loss: 14.420469. Entropy: 0.448447.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1204   score: 280.0  epsilon: 1.0    steps: 222  evaluation reward: 271.2\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2881: Policy loss: 0.203833. Value loss: 27.823561. Entropy: 0.440726.\n",
      "Iteration 2882: Policy loss: 0.555640. Value loss: 18.656727. Entropy: 0.419970.\n",
      "Iteration 2883: Policy loss: 0.446668. Value loss: 14.666582. Entropy: 0.422256.\n",
      "episode: 1205   score: 220.0  epsilon: 1.0    steps: 18  evaluation reward: 269.95\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2884: Policy loss: -0.903610. Value loss: 305.173401. Entropy: 0.400092.\n",
      "Iteration 2885: Policy loss: -0.564637. Value loss: 164.144058. Entropy: 0.316441.\n",
      "Iteration 2886: Policy loss: -0.190821. Value loss: 96.429207. Entropy: 0.296261.\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2887: Policy loss: -0.647410. Value loss: 67.382118. Entropy: 0.330669.\n",
      "Iteration 2888: Policy loss: -0.696636. Value loss: 47.239082. Entropy: 0.369963.\n",
      "Iteration 2889: Policy loss: -0.710221. Value loss: 31.450232. Entropy: 0.350558.\n",
      "episode: 1206   score: 215.0  epsilon: 1.0    steps: 340  evaluation reward: 267.5\n",
      "episode: 1207   score: 550.0  epsilon: 1.0    steps: 736  evaluation reward: 268.65\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2890: Policy loss: 4.272243. Value loss: 69.458954. Entropy: 0.350751.\n",
      "Iteration 2891: Policy loss: 3.964038. Value loss: 38.693851. Entropy: 0.351833.\n",
      "Iteration 2892: Policy loss: 3.777457. Value loss: 32.879616. Entropy: 0.366262.\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2893: Policy loss: -0.385750. Value loss: 220.000351. Entropy: 0.234065.\n",
      "Iteration 2894: Policy loss: 0.200053. Value loss: 99.907440. Entropy: 0.148932.\n",
      "Iteration 2895: Policy loss: 0.119918. Value loss: 72.174911. Entropy: 0.140744.\n",
      "episode: 1208   score: 365.0  epsilon: 1.0    steps: 613  evaluation reward: 269.85\n",
      "episode: 1209   score: 665.0  epsilon: 1.0    steps: 864  evaluation reward: 275.5\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2896: Policy loss: 0.154396. Value loss: 33.899330. Entropy: 0.244223.\n",
      "Iteration 2897: Policy loss: 0.288556. Value loss: 24.626492. Entropy: 0.235577.\n",
      "Iteration 2898: Policy loss: 0.448280. Value loss: 21.501043. Entropy: 0.224701.\n",
      "episode: 1210   score: 225.0  epsilon: 1.0    steps: 925  evaluation reward: 276.4\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2899: Policy loss: -0.169716. Value loss: 17.838821. Entropy: 0.286572.\n",
      "Iteration 2900: Policy loss: -0.099323. Value loss: 15.644711. Entropy: 0.273679.\n",
      "Iteration 2901: Policy loss: -0.077343. Value loss: 10.397527. Entropy: 0.290344.\n",
      "episode: 1211   score: 240.0  epsilon: 1.0    steps: 144  evaluation reward: 276.15\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2902: Policy loss: 1.641098. Value loss: 50.948421. Entropy: 0.382932.\n",
      "Iteration 2903: Policy loss: 1.729899. Value loss: 31.420998. Entropy: 0.353566.\n",
      "Iteration 2904: Policy loss: 1.533657. Value loss: 23.108213. Entropy: 0.359465.\n",
      "episode: 1212   score: 340.0  epsilon: 1.0    steps: 490  evaluation reward: 277.35\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2905: Policy loss: -2.989191. Value loss: 253.382980. Entropy: 0.297696.\n",
      "Iteration 2906: Policy loss: -2.993355. Value loss: 153.816772. Entropy: 0.323625.\n",
      "Iteration 2907: Policy loss: -3.820788. Value loss: 108.831871. Entropy: 0.286133.\n",
      "episode: 1213   score: 550.0  epsilon: 1.0    steps: 107  evaluation reward: 280.45\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2908: Policy loss: 1.779596. Value loss: 43.850807. Entropy: 0.244612.\n",
      "Iteration 2909: Policy loss: 1.657150. Value loss: 26.327765. Entropy: 0.249370.\n",
      "Iteration 2910: Policy loss: 1.714823. Value loss: 20.687700. Entropy: 0.266093.\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2911: Policy loss: 2.331916. Value loss: 47.493984. Entropy: 0.324655.\n",
      "Iteration 2912: Policy loss: 2.545855. Value loss: 29.172039. Entropy: 0.326245.\n",
      "Iteration 2913: Policy loss: 2.277145. Value loss: 25.747070. Entropy: 0.335313.\n",
      "episode: 1214   score: 335.0  epsilon: 1.0    steps: 275  evaluation reward: 281.6\n",
      "episode: 1215   score: 230.0  epsilon: 1.0    steps: 534  evaluation reward: 281.75\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2914: Policy loss: 0.814630. Value loss: 37.180725. Entropy: 0.302512.\n",
      "Iteration 2915: Policy loss: 0.686898. Value loss: 24.295218. Entropy: 0.298029.\n",
      "Iteration 2916: Policy loss: 0.583174. Value loss: 20.191879. Entropy: 0.315267.\n",
      "episode: 1216   score: 350.0  epsilon: 1.0    steps: 696  evaluation reward: 283.3\n",
      "episode: 1217   score: 240.0  epsilon: 1.0    steps: 908  evaluation reward: 284.65\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2917: Policy loss: 0.511732. Value loss: 37.730980. Entropy: 0.182105.\n",
      "Iteration 2918: Policy loss: 0.638352. Value loss: 20.654280. Entropy: 0.189646.\n",
      "Iteration 2919: Policy loss: 0.780796. Value loss: 16.185133. Entropy: 0.191738.\n",
      "episode: 1218   score: 250.0  epsilon: 1.0    steps: 221  evaluation reward: 282.55\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2920: Policy loss: -0.533750. Value loss: 51.903622. Entropy: 0.162875.\n",
      "Iteration 2921: Policy loss: -0.290149. Value loss: 33.665474. Entropy: 0.158261.\n",
      "Iteration 2922: Policy loss: -0.627222. Value loss: 28.716274. Entropy: 0.172070.\n",
      "episode: 1219   score: 205.0  epsilon: 1.0    steps: 407  evaluation reward: 283.55\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2923: Policy loss: -0.352314. Value loss: 147.215591. Entropy: 0.237590.\n",
      "Iteration 2924: Policy loss: -0.016243. Value loss: 50.546173. Entropy: 0.255978.\n",
      "Iteration 2925: Policy loss: 0.180932. Value loss: 55.556499. Entropy: 0.254896.\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2926: Policy loss: 0.815477. Value loss: 35.259430. Entropy: 0.202488.\n",
      "Iteration 2927: Policy loss: 0.760515. Value loss: 20.886120. Entropy: 0.194434.\n",
      "Iteration 2928: Policy loss: 0.747633. Value loss: 18.381201. Entropy: 0.223612.\n",
      "episode: 1220   score: 220.0  epsilon: 1.0    steps: 52  evaluation reward: 282.9\n",
      "episode: 1221   score: 75.0  epsilon: 1.0    steps: 247  evaluation reward: 281.45\n",
      "episode: 1222   score: 710.0  epsilon: 1.0    steps: 770  evaluation reward: 285.9\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2929: Policy loss: 1.300230. Value loss: 37.121063. Entropy: 0.251250.\n",
      "Iteration 2930: Policy loss: 1.267982. Value loss: 20.951157. Entropy: 0.261123.\n",
      "Iteration 2931: Policy loss: 1.172432. Value loss: 18.456518. Entropy: 0.237776.\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2932: Policy loss: 0.077984. Value loss: 29.851604. Entropy: 0.214365.\n",
      "Iteration 2933: Policy loss: 0.233768. Value loss: 18.523289. Entropy: 0.235979.\n",
      "Iteration 2934: Policy loss: 0.296375. Value loss: 16.415939. Entropy: 0.230903.\n",
      "episode: 1223   score: 315.0  epsilon: 1.0    steps: 538  evaluation reward: 286.95\n",
      "episode: 1224   score: 230.0  epsilon: 1.0    steps: 960  evaluation reward: 288.65\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2935: Policy loss: 0.356106. Value loss: 19.811453. Entropy: 0.344115.\n",
      "Iteration 2936: Policy loss: 0.130171. Value loss: 13.467460. Entropy: 0.332308.\n",
      "Iteration 2937: Policy loss: 0.310896. Value loss: 12.696208. Entropy: 0.361532.\n",
      "episode: 1225   score: 365.0  epsilon: 1.0    steps: 282  evaluation reward: 289.35\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2938: Policy loss: 0.749630. Value loss: 18.105705. Entropy: 0.228313.\n",
      "Iteration 2939: Policy loss: 0.438889. Value loss: 11.269690. Entropy: 0.210280.\n",
      "Iteration 2940: Policy loss: 0.640128. Value loss: 8.991125. Entropy: 0.224548.\n",
      "episode: 1226   score: 320.0  epsilon: 1.0    steps: 745  evaluation reward: 289.95\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2941: Policy loss: 0.536174. Value loss: 41.457851. Entropy: 0.215989.\n",
      "Iteration 2942: Policy loss: 0.481347. Value loss: 28.045225. Entropy: 0.218477.\n",
      "Iteration 2943: Policy loss: 0.518405. Value loss: 24.226273. Entropy: 0.243728.\n",
      "episode: 1227   score: 205.0  epsilon: 1.0    steps: 820  evaluation reward: 289.25\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2944: Policy loss: 0.421459. Value loss: 32.500427. Entropy: 0.240930.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2945: Policy loss: 0.066068. Value loss: 19.298328. Entropy: 0.219111.\n",
      "Iteration 2946: Policy loss: 0.298025. Value loss: 18.638599. Entropy: 0.232104.\n",
      "episode: 1228   score: 190.0  epsilon: 1.0    steps: 158  evaluation reward: 288.3\n",
      "episode: 1229   score: 370.0  epsilon: 1.0    steps: 466  evaluation reward: 291.7\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2947: Policy loss: -1.213589. Value loss: 34.982826. Entropy: 0.252010.\n",
      "Iteration 2948: Policy loss: -1.161341. Value loss: 23.716103. Entropy: 0.225231.\n",
      "Iteration 2949: Policy loss: -1.284958. Value loss: 20.323792. Entropy: 0.212977.\n",
      "episode: 1230   score: 325.0  epsilon: 1.0    steps: 77  evaluation reward: 291.5\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2950: Policy loss: 1.228065. Value loss: 30.115664. Entropy: 0.202295.\n",
      "Iteration 2951: Policy loss: 1.326921. Value loss: 18.029827. Entropy: 0.207661.\n",
      "Iteration 2952: Policy loss: 1.101540. Value loss: 15.772848. Entropy: 0.286997.\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2953: Policy loss: 2.203814. Value loss: 21.279488. Entropy: 0.367022.\n",
      "Iteration 2954: Policy loss: 2.029971. Value loss: 16.225180. Entropy: 0.386314.\n",
      "Iteration 2955: Policy loss: 2.150203. Value loss: 13.714658. Entropy: 0.432162.\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2956: Policy loss: 1.107646. Value loss: 22.931644. Entropy: 0.470073.\n",
      "Iteration 2957: Policy loss: 1.141151. Value loss: 13.858263. Entropy: 0.489906.\n",
      "Iteration 2958: Policy loss: 1.167495. Value loss: 12.538001. Entropy: 0.508151.\n",
      "episode: 1231   score: 275.0  epsilon: 1.0    steps: 280  evaluation reward: 289.3\n",
      "episode: 1232   score: 185.0  epsilon: 1.0    steps: 809  evaluation reward: 288.1\n",
      "episode: 1233   score: 360.0  epsilon: 1.0    steps: 1000  evaluation reward: 289.6\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2959: Policy loss: -0.465239. Value loss: 27.233591. Entropy: 0.381082.\n",
      "Iteration 2960: Policy loss: -0.320292. Value loss: 16.518995. Entropy: 0.328694.\n",
      "Iteration 2961: Policy loss: -0.535194. Value loss: 16.421545. Entropy: 0.349064.\n",
      "episode: 1234   score: 180.0  epsilon: 1.0    steps: 146  evaluation reward: 289.15\n",
      "episode: 1235   score: 220.0  epsilon: 1.0    steps: 483  evaluation reward: 289.55\n",
      "episode: 1236   score: 345.0  epsilon: 1.0    steps: 663  evaluation reward: 290.1\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2962: Policy loss: 0.120986. Value loss: 14.674403. Entropy: 0.215718.\n",
      "Iteration 2963: Policy loss: 0.220844. Value loss: 10.722601. Entropy: 0.191236.\n",
      "Iteration 2964: Policy loss: 0.149044. Value loss: 8.787890. Entropy: 0.201208.\n",
      "episode: 1237   score: 205.0  epsilon: 1.0    steps: 62  evaluation reward: 290.05\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2965: Policy loss: 0.486333. Value loss: 24.707714. Entropy: 0.347341.\n",
      "Iteration 2966: Policy loss: 0.403355. Value loss: 16.642786. Entropy: 0.318433.\n",
      "Iteration 2967: Policy loss: 0.704414. Value loss: 13.021937. Entropy: 0.339031.\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2968: Policy loss: 0.967628. Value loss: 28.645767. Entropy: 0.365674.\n",
      "Iteration 2969: Policy loss: 0.943489. Value loss: 15.413674. Entropy: 0.372929.\n",
      "Iteration 2970: Policy loss: 0.998320. Value loss: 12.674663. Entropy: 0.356680.\n",
      "episode: 1238   score: 600.0  epsilon: 1.0    steps: 623  evaluation reward: 293.9\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2971: Policy loss: -1.478745. Value loss: 56.443226. Entropy: 0.413052.\n",
      "Iteration 2972: Policy loss: -1.545323. Value loss: 33.949627. Entropy: 0.385049.\n",
      "Iteration 2973: Policy loss: -1.285440. Value loss: 23.137203. Entropy: 0.394613.\n",
      "episode: 1239   score: 225.0  epsilon: 1.0    steps: 215  evaluation reward: 291.55\n",
      "episode: 1240   score: 235.0  epsilon: 1.0    steps: 929  evaluation reward: 292.85\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2974: Policy loss: 0.232431. Value loss: 47.544083. Entropy: 0.241518.\n",
      "Iteration 2975: Policy loss: 0.115334. Value loss: 29.070791. Entropy: 0.242267.\n",
      "Iteration 2976: Policy loss: 0.634226. Value loss: 23.988197. Entropy: 0.248924.\n",
      "episode: 1241   score: 185.0  epsilon: 1.0    steps: 442  evaluation reward: 291.95\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2977: Policy loss: 3.887675. Value loss: 46.074764. Entropy: 0.257132.\n",
      "Iteration 2978: Policy loss: 3.275327. Value loss: 23.945169. Entropy: 0.331768.\n",
      "Iteration 2979: Policy loss: 3.953201. Value loss: 24.951490. Entropy: 0.267646.\n",
      "episode: 1242   score: 245.0  epsilon: 1.0    steps: 872  evaluation reward: 292.6\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2980: Policy loss: 1.261610. Value loss: 39.101913. Entropy: 0.343604.\n",
      "Iteration 2981: Policy loss: 1.199367. Value loss: 18.729969. Entropy: 0.316829.\n",
      "Iteration 2982: Policy loss: 1.418515. Value loss: 15.177127. Entropy: 0.292128.\n",
      "episode: 1243   score: 285.0  epsilon: 1.0    steps: 115  evaluation reward: 293.35\n",
      "episode: 1244   score: 155.0  epsilon: 1.0    steps: 622  evaluation reward: 292.8\n",
      "episode: 1245   score: 245.0  epsilon: 1.0    steps: 738  evaluation reward: 293.45\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2983: Policy loss: 2.916631. Value loss: 40.013443. Entropy: 0.423757.\n",
      "Iteration 2984: Policy loss: 2.923237. Value loss: 19.211889. Entropy: 0.420957.\n",
      "Iteration 2985: Policy loss: 2.795819. Value loss: 14.338053. Entropy: 0.437296.\n",
      "episode: 1246   score: 135.0  epsilon: 1.0    steps: 228  evaluation reward: 292.9\n",
      "episode: 1247   score: 110.0  epsilon: 1.0    steps: 938  evaluation reward: 292.2\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2986: Policy loss: 1.373992. Value loss: 16.276024. Entropy: 0.291510.\n",
      "Iteration 2987: Policy loss: 1.391469. Value loss: 10.367712. Entropy: 0.302693.\n",
      "Iteration 2988: Policy loss: 1.281840. Value loss: 10.161030. Entropy: 0.315068.\n",
      "episode: 1248   score: 420.0  epsilon: 1.0    steps: 291  evaluation reward: 293.15\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2989: Policy loss: -0.460602. Value loss: 16.828123. Entropy: 0.231354.\n",
      "Iteration 2990: Policy loss: -0.327740. Value loss: 11.454279. Entropy: 0.227340.\n",
      "Iteration 2991: Policy loss: -0.298500. Value loss: 11.760183. Entropy: 0.249906.\n",
      "episode: 1249   score: 105.0  epsilon: 1.0    steps: 812  evaluation reward: 293.1\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2992: Policy loss: 0.845347. Value loss: 27.151791. Entropy: 0.346276.\n",
      "Iteration 2993: Policy loss: 0.882820. Value loss: 17.184093. Entropy: 0.324742.\n",
      "Iteration 2994: Policy loss: 0.702591. Value loss: 14.156216. Entropy: 0.336093.\n",
      "episode: 1250   score: 230.0  epsilon: 1.0    steps: 490  evaluation reward: 294.35\n",
      "now time :  2019-02-26 13:24:03.164022\n",
      "episode: 1251   score: 110.0  epsilon: 1.0    steps: 701  evaluation reward: 294.65\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2995: Policy loss: 2.047325. Value loss: 20.314491. Entropy: 0.362858.\n",
      "Iteration 2996: Policy loss: 1.784094. Value loss: 13.530129. Entropy: 0.345505.\n",
      "Iteration 2997: Policy loss: 1.982412. Value loss: 10.795938. Entropy: 0.389715.\n",
      "episode: 1252   score: 95.0  epsilon: 1.0    steps: 207  evaluation reward: 293.5\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2998: Policy loss: 1.080700. Value loss: 20.839687. Entropy: 0.202248.\n",
      "Iteration 2999: Policy loss: 0.991801. Value loss: 10.994509. Entropy: 0.214814.\n",
      "Iteration 3000: Policy loss: 1.079316. Value loss: 9.477955. Entropy: 0.235248.\n",
      "episode: 1253   score: 215.0  epsilon: 1.0    steps: 582  evaluation reward: 292.75\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3001: Policy loss: -1.005787. Value loss: 38.525734. Entropy: 0.372243.\n",
      "Iteration 3002: Policy loss: -0.922812. Value loss: 21.945976. Entropy: 0.367467.\n",
      "Iteration 3003: Policy loss: -0.771974. Value loss: 16.343918. Entropy: 0.352188.\n",
      "episode: 1254   score: 195.0  epsilon: 1.0    steps: 878  evaluation reward: 289.55\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3004: Policy loss: -1.113933. Value loss: 29.362823. Entropy: 0.461459.\n",
      "Iteration 3005: Policy loss: -1.498500. Value loss: 19.936468. Entropy: 0.494839.\n",
      "Iteration 3006: Policy loss: -1.145289. Value loss: 14.958382. Entropy: 0.481135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1255   score: 300.0  epsilon: 1.0    steps: 285  evaluation reward: 291.2\n",
      "episode: 1256   score: 195.0  epsilon: 1.0    steps: 735  evaluation reward: 290.2\n",
      "episode: 1257   score: 315.0  epsilon: 1.0    steps: 981  evaluation reward: 291.1\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3007: Policy loss: -0.861262. Value loss: 28.108791. Entropy: 0.280731.\n",
      "Iteration 3008: Policy loss: -1.006473. Value loss: 17.896538. Entropy: 0.292160.\n",
      "Iteration 3009: Policy loss: -0.801275. Value loss: 14.612124. Entropy: 0.304765.\n",
      "episode: 1258   score: 180.0  epsilon: 1.0    steps: 212  evaluation reward: 291.15\n",
      "episode: 1259   score: 135.0  epsilon: 1.0    steps: 386  evaluation reward: 291.05\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3010: Policy loss: 0.447557. Value loss: 20.673946. Entropy: 0.244339.\n",
      "Iteration 3011: Policy loss: 0.128267. Value loss: 13.799458. Entropy: 0.209047.\n",
      "Iteration 3012: Policy loss: 0.307414. Value loss: 10.154296. Entropy: 0.238947.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3013: Policy loss: -0.792074. Value loss: 23.884226. Entropy: 0.269832.\n",
      "Iteration 3014: Policy loss: -1.008888. Value loss: 18.505375. Entropy: 0.282603.\n",
      "Iteration 3015: Policy loss: -0.834192. Value loss: 16.026112. Entropy: 0.298442.\n",
      "episode: 1260   score: 435.0  epsilon: 1.0    steps: 121  evaluation reward: 293.45\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3016: Policy loss: 0.241254. Value loss: 36.442959. Entropy: 0.588570.\n",
      "Iteration 3017: Policy loss: 0.157341. Value loss: 22.759151. Entropy: 0.583168.\n",
      "Iteration 3018: Policy loss: 0.295473. Value loss: 20.814701. Entropy: 0.598225.\n",
      "episode: 1261   score: 170.0  epsilon: 1.0    steps: 779  evaluation reward: 292.25\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3019: Policy loss: 2.274736. Value loss: 41.647472. Entropy: 0.308944.\n",
      "Iteration 3020: Policy loss: 2.131660. Value loss: 23.742611. Entropy: 0.298873.\n",
      "Iteration 3021: Policy loss: 2.222375. Value loss: 21.853800. Entropy: 0.315113.\n",
      "episode: 1262   score: 195.0  epsilon: 1.0    steps: 268  evaluation reward: 292.25\n",
      "episode: 1263   score: 190.0  epsilon: 1.0    steps: 912  evaluation reward: 292.25\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3022: Policy loss: 1.464580. Value loss: 10.549409. Entropy: 0.188042.\n",
      "Iteration 3023: Policy loss: 1.514740. Value loss: 9.312794. Entropy: 0.192907.\n",
      "Iteration 3024: Policy loss: 1.490324. Value loss: 6.525997. Entropy: 0.215866.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3025: Policy loss: -0.826161. Value loss: 23.683815. Entropy: 0.305583.\n",
      "Iteration 3026: Policy loss: -0.710264. Value loss: 15.575505. Entropy: 0.308088.\n",
      "Iteration 3027: Policy loss: -0.948712. Value loss: 12.933337. Entropy: 0.321554.\n",
      "episode: 1264   score: 255.0  epsilon: 1.0    steps: 508  evaluation reward: 291.4\n",
      "episode: 1265   score: 165.0  epsilon: 1.0    steps: 612  evaluation reward: 292.6\n",
      "episode: 1266   score: 215.0  epsilon: 1.0    steps: 686  evaluation reward: 292.05\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3028: Policy loss: 2.026749. Value loss: 25.195129. Entropy: 0.530306.\n",
      "Iteration 3029: Policy loss: 2.128690. Value loss: 17.274900. Entropy: 0.490080.\n",
      "Iteration 3030: Policy loss: 1.943209. Value loss: 14.843356. Entropy: 0.516297.\n",
      "episode: 1267   score: 200.0  epsilon: 1.0    steps: 97  evaluation reward: 288.85\n",
      "episode: 1268   score: 210.0  epsilon: 1.0    steps: 159  evaluation reward: 288.15\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3031: Policy loss: -1.088755. Value loss: 17.328499. Entropy: 0.229751.\n",
      "Iteration 3032: Policy loss: -1.156126. Value loss: 13.113713. Entropy: 0.248845.\n",
      "Iteration 3033: Policy loss: -1.135913. Value loss: 10.743234. Entropy: 0.219226.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3034: Policy loss: 0.450720. Value loss: 17.497885. Entropy: 0.264341.\n",
      "Iteration 3035: Policy loss: 0.535724. Value loss: 13.896238. Entropy: 0.274200.\n",
      "Iteration 3036: Policy loss: 0.371821. Value loss: 10.036504. Entropy: 0.282533.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3037: Policy loss: 0.950123. Value loss: 24.123882. Entropy: 0.523409.\n",
      "Iteration 3038: Policy loss: 0.964612. Value loss: 17.662197. Entropy: 0.528089.\n",
      "Iteration 3039: Policy loss: 0.960233. Value loss: 15.283952. Entropy: 0.540195.\n",
      "episode: 1269   score: 230.0  epsilon: 1.0    steps: 366  evaluation reward: 288.5\n",
      "episode: 1270   score: 320.0  epsilon: 1.0    steps: 880  evaluation reward: 287.0\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3040: Policy loss: -0.435546. Value loss: 192.734467. Entropy: 0.636000.\n",
      "Iteration 3041: Policy loss: -0.159404. Value loss: 88.507858. Entropy: 0.546552.\n",
      "Iteration 3042: Policy loss: -0.716967. Value loss: 88.610123. Entropy: 0.564444.\n",
      "episode: 1271   score: 185.0  epsilon: 1.0    steps: 502  evaluation reward: 286.3\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3043: Policy loss: -0.826589. Value loss: 36.717758. Entropy: 0.456790.\n",
      "Iteration 3044: Policy loss: -0.783498. Value loss: 23.060453. Entropy: 0.426766.\n",
      "Iteration 3045: Policy loss: -0.975327. Value loss: 17.257391. Entropy: 0.447739.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3046: Policy loss: -0.270043. Value loss: 256.584534. Entropy: 0.581539.\n",
      "Iteration 3047: Policy loss: -0.668082. Value loss: 163.549713. Entropy: 0.547206.\n",
      "Iteration 3048: Policy loss: -0.641643. Value loss: 115.286018. Entropy: 0.528256.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3049: Policy loss: 0.790981. Value loss: 62.764584. Entropy: 0.798815.\n",
      "Iteration 3050: Policy loss: 0.895420. Value loss: 35.138428. Entropy: 0.868149.\n",
      "Iteration 3051: Policy loss: 0.323201. Value loss: 22.855127. Entropy: 0.921464.\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3052: Policy loss: 2.100700. Value loss: 60.581478. Entropy: 0.880045.\n",
      "Iteration 3053: Policy loss: 1.819535. Value loss: 35.325127. Entropy: 0.841216.\n",
      "Iteration 3054: Policy loss: 2.838267. Value loss: 24.338039. Entropy: 0.839673.\n",
      "episode: 1272   score: 485.0  epsilon: 1.0    steps: 114  evaluation reward: 287.8\n",
      "episode: 1273   score: 395.0  epsilon: 1.0    steps: 911  evaluation reward: 289.35\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3055: Policy loss: 1.988740. Value loss: 23.329042. Entropy: 0.753037.\n",
      "Iteration 3056: Policy loss: 1.814721. Value loss: 18.083691. Entropy: 0.781211.\n",
      "Iteration 3057: Policy loss: 2.130143. Value loss: 13.808152. Entropy: 0.759071.\n",
      "episode: 1274   score: 535.0  epsilon: 1.0    steps: 181  evaluation reward: 292.7\n",
      "episode: 1275   score: 410.0  epsilon: 1.0    steps: 631  evaluation reward: 293.35\n",
      "episode: 1276   score: 410.0  epsilon: 1.0    steps: 711  evaluation reward: 295.2\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3058: Policy loss: -0.064795. Value loss: 38.535194. Entropy: 0.808049.\n",
      "Iteration 3059: Policy loss: 0.145879. Value loss: 21.171900. Entropy: 0.779197.\n",
      "Iteration 3060: Policy loss: -0.062890. Value loss: 18.609926. Entropy: 0.788736.\n",
      "episode: 1277   score: 145.0  epsilon: 1.0    steps: 350  evaluation reward: 295.9\n",
      "episode: 1278   score: 195.0  epsilon: 1.0    steps: 397  evaluation reward: 295.85\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3061: Policy loss: -1.443691. Value loss: 34.906605. Entropy: 0.786974.\n",
      "Iteration 3062: Policy loss: -1.582385. Value loss: 21.472969. Entropy: 0.793970.\n",
      "Iteration 3063: Policy loss: -1.179873. Value loss: 16.467909. Entropy: 0.783810.\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3064: Policy loss: 1.667118. Value loss: 35.500996. Entropy: 0.703475.\n",
      "Iteration 3065: Policy loss: 1.534565. Value loss: 17.525412. Entropy: 0.716748.\n",
      "Iteration 3066: Policy loss: 1.567475. Value loss: 14.026077. Entropy: 0.705612.\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3067: Policy loss: 3.243009. Value loss: 51.437885. Entropy: 0.757285.\n",
      "Iteration 3068: Policy loss: 3.294484. Value loss: 31.925983. Entropy: 0.785061.\n",
      "Iteration 3069: Policy loss: 3.317613. Value loss: 27.628395. Entropy: 0.845939.\n",
      "episode: 1279   score: 130.0  epsilon: 1.0    steps: 234  evaluation reward: 295.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1280   score: 100.0  epsilon: 1.0    steps: 676  evaluation reward: 292.4\n",
      "episode: 1281   score: 390.0  epsilon: 1.0    steps: 857  evaluation reward: 291.5\n",
      "episode: 1282   score: 230.0  epsilon: 1.0    steps: 902  evaluation reward: 291.5\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3070: Policy loss: 1.098296. Value loss: 36.025547. Entropy: 0.716109.\n",
      "Iteration 3071: Policy loss: 1.087731. Value loss: 21.875546. Entropy: 0.682727.\n",
      "Iteration 3072: Policy loss: 0.983153. Value loss: 17.975864. Entropy: 0.662271.\n",
      "episode: 1283   score: 105.0  epsilon: 1.0    steps: 397  evaluation reward: 287.95\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3073: Policy loss: -2.845424. Value loss: 55.091675. Entropy: 0.784409.\n",
      "Iteration 3074: Policy loss: -2.630213. Value loss: 32.241028. Entropy: 0.753630.\n",
      "Iteration 3075: Policy loss: -2.677572. Value loss: 26.387604. Entropy: 0.751730.\n",
      "episode: 1284   score: 75.0  epsilon: 1.0    steps: 767  evaluation reward: 286.55\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3076: Policy loss: 0.071119. Value loss: 54.923618. Entropy: 0.741175.\n",
      "Iteration 3077: Policy loss: 0.436459. Value loss: 35.794422. Entropy: 0.758767.\n",
      "Iteration 3078: Policy loss: 0.214640. Value loss: 25.943899. Entropy: 0.751740.\n",
      "episode: 1285   score: 340.0  epsilon: 1.0    steps: 16  evaluation reward: 286.8\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3079: Policy loss: -0.250420. Value loss: 45.140469. Entropy: 0.877909.\n",
      "Iteration 3080: Policy loss: -0.762934. Value loss: 25.059755. Entropy: 0.881152.\n",
      "Iteration 3081: Policy loss: 0.252750. Value loss: 19.644747. Entropy: 0.862122.\n",
      "episode: 1286   score: 75.0  epsilon: 1.0    steps: 449  evaluation reward: 283.0\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3082: Policy loss: 4.010369. Value loss: 60.526150. Entropy: 0.880522.\n",
      "Iteration 3083: Policy loss: 4.786924. Value loss: 35.365837. Entropy: 0.884823.\n",
      "Iteration 3084: Policy loss: 4.245375. Value loss: 27.390152. Entropy: 0.916710.\n",
      "episode: 1287   score: 270.0  epsilon: 1.0    steps: 259  evaluation reward: 283.85\n",
      "episode: 1288   score: 440.0  epsilon: 1.0    steps: 528  evaluation reward: 285.15\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3085: Policy loss: 1.952052. Value loss: 42.535507. Entropy: 0.854846.\n",
      "Iteration 3086: Policy loss: 2.211930. Value loss: 23.424826. Entropy: 0.837252.\n",
      "Iteration 3087: Policy loss: 2.113037. Value loss: 16.789238. Entropy: 0.850331.\n",
      "episode: 1289   score: 95.0  epsilon: 1.0    steps: 770  evaluation reward: 282.95\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3088: Policy loss: -0.838752. Value loss: 55.882862. Entropy: 0.927208.\n",
      "Iteration 3089: Policy loss: -0.838258. Value loss: 32.335464. Entropy: 0.933415.\n",
      "Iteration 3090: Policy loss: -1.009112. Value loss: 24.688679. Entropy: 0.909183.\n",
      "episode: 1290   score: 370.0  epsilon: 1.0    steps: 238  evaluation reward: 282.9\n",
      "episode: 1291   score: 275.0  epsilon: 1.0    steps: 931  evaluation reward: 282.15\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3091: Policy loss: -0.069720. Value loss: 39.678139. Entropy: 0.899528.\n",
      "Iteration 3092: Policy loss: -0.119856. Value loss: 25.798769. Entropy: 0.875206.\n",
      "Iteration 3093: Policy loss: 0.106061. Value loss: 20.358170. Entropy: 0.912366.\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3094: Policy loss: 1.954569. Value loss: 31.801363. Entropy: 0.858802.\n",
      "Iteration 3095: Policy loss: 2.152610. Value loss: 18.772438. Entropy: 0.803487.\n",
      "Iteration 3096: Policy loss: 1.938846. Value loss: 16.145779. Entropy: 0.844430.\n",
      "episode: 1292   score: 85.0  epsilon: 1.0    steps: 610  evaluation reward: 279.2\n",
      "episode: 1293   score: 175.0  epsilon: 1.0    steps: 868  evaluation reward: 278.8\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3097: Policy loss: 1.486699. Value loss: 54.603424. Entropy: 0.967932.\n",
      "Iteration 3098: Policy loss: 2.179708. Value loss: 33.327278. Entropy: 0.953377.\n",
      "Iteration 3099: Policy loss: 1.796509. Value loss: 29.496037. Entropy: 0.964186.\n",
      "episode: 1294   score: 280.0  epsilon: 1.0    steps: 458  evaluation reward: 276.5\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3100: Policy loss: 0.994609. Value loss: 29.652184. Entropy: 0.762936.\n",
      "Iteration 3101: Policy loss: 0.689188. Value loss: 19.498619. Entropy: 0.720505.\n",
      "Iteration 3102: Policy loss: 1.093611. Value loss: 12.376786. Entropy: 0.760182.\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3103: Policy loss: 1.972104. Value loss: 34.094769. Entropy: 0.841261.\n",
      "Iteration 3104: Policy loss: 2.083470. Value loss: 21.999056. Entropy: 0.853445.\n",
      "Iteration 3105: Policy loss: 2.162967. Value loss: 16.956167. Entropy: 0.811319.\n",
      "episode: 1295   score: 355.0  epsilon: 1.0    steps: 124  evaluation reward: 274.0\n",
      "episode: 1296   score: 180.0  epsilon: 1.0    steps: 266  evaluation reward: 269.8\n",
      "episode: 1297   score: 495.0  epsilon: 1.0    steps: 662  evaluation reward: 271.55\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3106: Policy loss: 1.851040. Value loss: 32.908707. Entropy: 0.921213.\n",
      "Iteration 3107: Policy loss: 1.791803. Value loss: 23.153799. Entropy: 0.896209.\n",
      "Iteration 3108: Policy loss: 1.639573. Value loss: 21.255098. Entropy: 0.930421.\n",
      "episode: 1298   score: 210.0  epsilon: 1.0    steps: 998  evaluation reward: 271.2\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3109: Policy loss: 2.405365. Value loss: 47.776287. Entropy: 0.911167.\n",
      "Iteration 3110: Policy loss: 2.758055. Value loss: 30.129379. Entropy: 0.912520.\n",
      "Iteration 3111: Policy loss: 2.473437. Value loss: 23.525217. Entropy: 0.868859.\n",
      "episode: 1299   score: 110.0  epsilon: 1.0    steps: 379  evaluation reward: 270.1\n",
      "episode: 1300   score: 150.0  epsilon: 1.0    steps: 868  evaluation reward: 267.5\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3112: Policy loss: 1.898982. Value loss: 33.571018. Entropy: 0.917232.\n",
      "Iteration 3113: Policy loss: 1.590218. Value loss: 21.034403. Entropy: 0.902885.\n",
      "Iteration 3114: Policy loss: 1.461530. Value loss: 14.927622. Entropy: 0.897917.\n",
      "now time :  2019-02-26 13:26:23.460728\n",
      "episode: 1301   score: 360.0  epsilon: 1.0    steps: 243  evaluation reward: 266.0\n",
      "episode: 1302   score: 190.0  epsilon: 1.0    steps: 481  evaluation reward: 264.25\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3115: Policy loss: 0.601335. Value loss: 22.526440. Entropy: 0.810206.\n",
      "Iteration 3116: Policy loss: 0.498662. Value loss: 14.098786. Entropy: 0.784389.\n",
      "Iteration 3117: Policy loss: 0.502166. Value loss: 11.653262. Entropy: 0.766444.\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3118: Policy loss: -0.297532. Value loss: 37.238327. Entropy: 0.741122.\n",
      "Iteration 3119: Policy loss: -0.129927. Value loss: 21.558256. Entropy: 0.734526.\n",
      "Iteration 3120: Policy loss: -0.228643. Value loss: 18.583046. Entropy: 0.750902.\n",
      "episode: 1303   score: 250.0  epsilon: 1.0    steps: 582  evaluation reward: 263.5\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3121: Policy loss: 0.452838. Value loss: 30.729870. Entropy: 0.886074.\n",
      "Iteration 3122: Policy loss: 0.657087. Value loss: 17.200197. Entropy: 0.885358.\n",
      "Iteration 3123: Policy loss: 0.750899. Value loss: 14.145176. Entropy: 0.846809.\n",
      "episode: 1304   score: 210.0  epsilon: 1.0    steps: 732  evaluation reward: 262.8\n",
      "episode: 1305   score: 155.0  epsilon: 1.0    steps: 975  evaluation reward: 262.15\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3124: Policy loss: 2.322087. Value loss: 18.431389. Entropy: 0.797897.\n",
      "Iteration 3125: Policy loss: 2.106891. Value loss: 13.292234. Entropy: 0.811172.\n",
      "Iteration 3126: Policy loss: 2.291454. Value loss: 9.740056. Entropy: 0.825673.\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3127: Policy loss: 0.517473. Value loss: 29.991650. Entropy: 0.678940.\n",
      "Iteration 3128: Policy loss: 0.722246. Value loss: 17.563688. Entropy: 0.674037.\n",
      "Iteration 3129: Policy loss: 0.599221. Value loss: 14.144762. Entropy: 0.663645.\n",
      "episode: 1306   score: 190.0  epsilon: 1.0    steps: 773  evaluation reward: 261.9\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3130: Policy loss: 3.794009. Value loss: 21.721836. Entropy: 0.912306.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3131: Policy loss: 3.727169. Value loss: 12.011443. Entropy: 0.902255.\n",
      "Iteration 3132: Policy loss: 3.608222. Value loss: 11.574549. Entropy: 0.939129.\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3133: Policy loss: 1.810600. Value loss: 28.256300. Entropy: 0.827180.\n",
      "Iteration 3134: Policy loss: 2.077802. Value loss: 16.660896. Entropy: 0.801288.\n",
      "Iteration 3135: Policy loss: 1.846941. Value loss: 13.066233. Entropy: 0.808594.\n",
      "episode: 1307   score: 345.0  epsilon: 1.0    steps: 80  evaluation reward: 259.85\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3136: Policy loss: 1.998812. Value loss: 26.725208. Entropy: 0.702071.\n",
      "Iteration 3137: Policy loss: 1.892511. Value loss: 15.096488. Entropy: 0.700095.\n",
      "Iteration 3138: Policy loss: 1.997638. Value loss: 12.049831. Entropy: 0.728160.\n",
      "episode: 1308   score: 285.0  epsilon: 1.0    steps: 418  evaluation reward: 259.05\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3139: Policy loss: 0.345649. Value loss: 21.057262. Entropy: 0.590970.\n",
      "Iteration 3140: Policy loss: 0.171940. Value loss: 15.210636. Entropy: 0.624790.\n",
      "Iteration 3141: Policy loss: 0.087906. Value loss: 12.026292. Entropy: 0.608926.\n",
      "episode: 1309   score: 195.0  epsilon: 1.0    steps: 597  evaluation reward: 254.35\n",
      "episode: 1310   score: 125.0  epsilon: 1.0    steps: 653  evaluation reward: 253.35\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3142: Policy loss: -1.555574. Value loss: 227.938797. Entropy: 0.886666.\n",
      "Iteration 3143: Policy loss: -0.486410. Value loss: 83.466591. Entropy: 0.816827.\n",
      "Iteration 3144: Policy loss: -1.355378. Value loss: 57.891499. Entropy: 0.856072.\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3145: Policy loss: -1.793985. Value loss: 29.233953. Entropy: 0.847587.\n",
      "Iteration 3146: Policy loss: -1.796437. Value loss: 19.092407. Entropy: 0.834215.\n",
      "Iteration 3147: Policy loss: -1.998813. Value loss: 14.326395. Entropy: 0.851527.\n",
      "episode: 1311   score: 170.0  epsilon: 1.0    steps: 791  evaluation reward: 252.65\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3148: Policy loss: 0.572503. Value loss: 23.547382. Entropy: 0.606809.\n",
      "Iteration 3149: Policy loss: 0.510050. Value loss: 10.932949. Entropy: 0.588480.\n",
      "Iteration 3150: Policy loss: 0.717234. Value loss: 9.016352. Entropy: 0.615212.\n",
      "episode: 1312   score: 295.0  epsilon: 1.0    steps: 975  evaluation reward: 252.2\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3151: Policy loss: -3.047137. Value loss: 205.437088. Entropy: 0.513510.\n",
      "Iteration 3152: Policy loss: -3.133974. Value loss: 138.110092. Entropy: 0.514832.\n",
      "Iteration 3153: Policy loss: -2.931288. Value loss: 114.413063. Entropy: 0.526857.\n",
      "episode: 1313   score: 680.0  epsilon: 1.0    steps: 138  evaluation reward: 253.5\n",
      "episode: 1314   score: 330.0  epsilon: 1.0    steps: 347  evaluation reward: 253.45\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3154: Policy loss: -2.513383. Value loss: 164.184692. Entropy: 0.656805.\n",
      "Iteration 3155: Policy loss: -3.173656. Value loss: 118.987244. Entropy: 0.677264.\n",
      "Iteration 3156: Policy loss: -2.739133. Value loss: 84.925255. Entropy: 0.696168.\n",
      "episode: 1315   score: 210.0  epsilon: 1.0    steps: 27  evaluation reward: 253.25\n",
      "episode: 1316   score: 410.0  epsilon: 1.0    steps: 479  evaluation reward: 253.85\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3157: Policy loss: 0.200632. Value loss: 15.071472. Entropy: 0.539793.\n",
      "Iteration 3158: Policy loss: 0.164379. Value loss: 11.987849. Entropy: 0.544804.\n",
      "Iteration 3159: Policy loss: 0.110237. Value loss: 10.621075. Entropy: 0.565421.\n",
      "episode: 1317   score: 65.0  epsilon: 1.0    steps: 249  evaluation reward: 252.1\n",
      "episode: 1318   score: 105.0  epsilon: 1.0    steps: 958  evaluation reward: 250.65\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3160: Policy loss: -1.346639. Value loss: 87.690193. Entropy: 0.687182.\n",
      "Iteration 3161: Policy loss: -2.080757. Value loss: 28.105736. Entropy: 0.635882.\n",
      "Iteration 3162: Policy loss: -1.862955. Value loss: 20.560587. Entropy: 0.591320.\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3163: Policy loss: 1.369327. Value loss: 34.524914. Entropy: 0.581905.\n",
      "Iteration 3164: Policy loss: 1.412609. Value loss: 17.910915. Entropy: 0.561911.\n",
      "Iteration 3165: Policy loss: 1.544251. Value loss: 14.197329. Entropy: 0.584878.\n",
      "episode: 1319   score: 105.0  epsilon: 1.0    steps: 8  evaluation reward: 249.65\n",
      "episode: 1320   score: 290.0  epsilon: 1.0    steps: 733  evaluation reward: 250.35\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3166: Policy loss: -0.168849. Value loss: 32.388939. Entropy: 0.716738.\n",
      "Iteration 3167: Policy loss: -0.143530. Value loss: 19.227020. Entropy: 0.694300.\n",
      "Iteration 3168: Policy loss: -0.113416. Value loss: 15.501424. Entropy: 0.720682.\n",
      "episode: 1321   score: 240.0  epsilon: 1.0    steps: 881  evaluation reward: 252.0\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3169: Policy loss: 0.366041. Value loss: 25.584152. Entropy: 0.659545.\n",
      "Iteration 3170: Policy loss: 0.527400. Value loss: 16.198683. Entropy: 0.641998.\n",
      "Iteration 3171: Policy loss: 0.464978. Value loss: 12.188804. Entropy: 0.643157.\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3172: Policy loss: -1.298814. Value loss: 189.441925. Entropy: 0.542446.\n",
      "Iteration 3173: Policy loss: -0.672459. Value loss: 81.886436. Entropy: 0.594690.\n",
      "Iteration 3174: Policy loss: -1.065644. Value loss: 63.714920. Entropy: 0.589005.\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3175: Policy loss: -0.331974. Value loss: 39.493370. Entropy: 0.352459.\n",
      "Iteration 3176: Policy loss: -0.240825. Value loss: 24.456039. Entropy: 0.344280.\n",
      "Iteration 3177: Policy loss: -0.586327. Value loss: 23.304550. Entropy: 0.341566.\n",
      "episode: 1322   score: 420.0  epsilon: 1.0    steps: 513  evaluation reward: 249.1\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3178: Policy loss: 0.486197. Value loss: 20.666182. Entropy: 0.430634.\n",
      "Iteration 3179: Policy loss: 0.659865. Value loss: 13.806039. Entropy: 0.425028.\n",
      "Iteration 3180: Policy loss: 0.570334. Value loss: 10.957861. Entropy: 0.462163.\n",
      "episode: 1323   score: 310.0  epsilon: 1.0    steps: 291  evaluation reward: 249.05\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3181: Policy loss: 2.595691. Value loss: 27.836388. Entropy: 0.467532.\n",
      "Iteration 3182: Policy loss: 2.426353. Value loss: 18.537611. Entropy: 0.487818.\n",
      "Iteration 3183: Policy loss: 2.618019. Value loss: 15.333763. Entropy: 0.503089.\n",
      "episode: 1324   score: 265.0  epsilon: 1.0    steps: 22  evaluation reward: 249.4\n",
      "episode: 1325   score: 295.0  epsilon: 1.0    steps: 1013  evaluation reward: 248.7\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3184: Policy loss: -0.998277. Value loss: 148.969391. Entropy: 0.568785.\n",
      "Iteration 3185: Policy loss: -1.403608. Value loss: 65.636398. Entropy: 0.568315.\n",
      "Iteration 3186: Policy loss: -0.666372. Value loss: 71.710503. Entropy: 0.597425.\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3187: Policy loss: -1.024709. Value loss: 31.939138. Entropy: 0.605329.\n",
      "Iteration 3188: Policy loss: -0.925553. Value loss: 13.193312. Entropy: 0.628941.\n",
      "Iteration 3189: Policy loss: -0.999878. Value loss: 10.419412. Entropy: 0.587767.\n",
      "episode: 1326   score: 265.0  epsilon: 1.0    steps: 705  evaluation reward: 248.15\n",
      "episode: 1327   score: 410.0  epsilon: 1.0    steps: 827  evaluation reward: 250.2\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3190: Policy loss: 2.441029. Value loss: 47.628712. Entropy: 0.639887.\n",
      "Iteration 3191: Policy loss: 2.585666. Value loss: 20.780365. Entropy: 0.597802.\n",
      "Iteration 3192: Policy loss: 2.646460. Value loss: 14.610326. Entropy: 0.613627.\n",
      "episode: 1328   score: 640.0  epsilon: 1.0    steps: 511  evaluation reward: 254.7\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3193: Policy loss: -0.243193. Value loss: 32.439606. Entropy: 0.522451.\n",
      "Iteration 3194: Policy loss: -0.236787. Value loss: 20.595936. Entropy: 0.514634.\n",
      "Iteration 3195: Policy loss: -0.171504. Value loss: 15.716338. Entropy: 0.525979.\n",
      "episode: 1329   score: 390.0  epsilon: 1.0    steps: 184  evaluation reward: 254.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3196: Policy loss: 1.760134. Value loss: 29.363588. Entropy: 0.565931.\n",
      "Iteration 3197: Policy loss: 1.663014. Value loss: 14.385849. Entropy: 0.591730.\n",
      "Iteration 3198: Policy loss: 1.811888. Value loss: 10.725096. Entropy: 0.581039.\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3199: Policy loss: -8.852673. Value loss: 497.616821. Entropy: 0.300922.\n",
      "Iteration 3200: Policy loss: -8.847976. Value loss: 308.920013. Entropy: 0.181237.\n",
      "Iteration 3201: Policy loss: -8.510406. Value loss: 281.545502. Entropy: 0.166322.\n",
      "episode: 1330   score: 290.0  epsilon: 1.0    steps: 574  evaluation reward: 254.55\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3202: Policy loss: 2.114472. Value loss: 48.107994. Entropy: 0.280033.\n",
      "Iteration 3203: Policy loss: 2.264899. Value loss: 33.516045. Entropy: 0.331449.\n",
      "Iteration 3204: Policy loss: 2.236218. Value loss: 22.245909. Entropy: 0.420650.\n",
      "episode: 1331   score: 285.0  epsilon: 1.0    steps: 100  evaluation reward: 254.65\n",
      "episode: 1332   score: 380.0  epsilon: 1.0    steps: 958  evaluation reward: 256.6\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3205: Policy loss: 1.981898. Value loss: 22.013134. Entropy: 0.224268.\n",
      "Iteration 3206: Policy loss: 2.006420. Value loss: 13.083842. Entropy: 0.244980.\n",
      "Iteration 3207: Policy loss: 1.960629. Value loss: 9.273537. Entropy: 0.248631.\n",
      "episode: 1333   score: 415.0  epsilon: 1.0    steps: 876  evaluation reward: 257.15\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3208: Policy loss: -0.190354. Value loss: 24.628176. Entropy: 0.267251.\n",
      "Iteration 3209: Policy loss: -0.352419. Value loss: 17.232609. Entropy: 0.267684.\n",
      "Iteration 3210: Policy loss: -0.384137. Value loss: 12.617702. Entropy: 0.265126.\n",
      "episode: 1334   score: 170.0  epsilon: 1.0    steps: 392  evaluation reward: 257.05\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3211: Policy loss: 0.738897. Value loss: 18.020432. Entropy: 0.309971.\n",
      "Iteration 3212: Policy loss: 0.655037. Value loss: 11.979740. Entropy: 0.318987.\n",
      "Iteration 3213: Policy loss: 0.648302. Value loss: 8.558854. Entropy: 0.337114.\n",
      "episode: 1335   score: 300.0  epsilon: 1.0    steps: 381  evaluation reward: 257.85\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3214: Policy loss: 2.665572. Value loss: 22.217960. Entropy: 0.533202.\n",
      "Iteration 3215: Policy loss: 2.609866. Value loss: 10.386648. Entropy: 0.525232.\n",
      "Iteration 3216: Policy loss: 2.617915. Value loss: 7.936745. Entropy: 0.500635.\n",
      "episode: 1336   score: 175.0  epsilon: 1.0    steps: 212  evaluation reward: 256.15\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3217: Policy loss: 0.773692. Value loss: 15.742193. Entropy: 0.465109.\n",
      "Iteration 3218: Policy loss: 0.240960. Value loss: 8.822597. Entropy: 0.469710.\n",
      "Iteration 3219: Policy loss: 0.500415. Value loss: 7.290004. Entropy: 0.461895.\n",
      "episode: 1337   score: 105.0  epsilon: 1.0    steps: 95  evaluation reward: 255.15\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3220: Policy loss: 0.109600. Value loss: 18.754057. Entropy: 0.442053.\n",
      "Iteration 3221: Policy loss: 0.215775. Value loss: 12.213112. Entropy: 0.398178.\n",
      "Iteration 3222: Policy loss: 0.414569. Value loss: 8.715858. Entropy: 0.418623.\n",
      "episode: 1338   score: 210.0  epsilon: 1.0    steps: 540  evaluation reward: 251.25\n",
      "episode: 1339   score: 420.0  epsilon: 1.0    steps: 763  evaluation reward: 253.2\n",
      "episode: 1340   score: 210.0  epsilon: 1.0    steps: 1009  evaluation reward: 252.95\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3223: Policy loss: 0.807507. Value loss: 20.312107. Entropy: 0.486320.\n",
      "Iteration 3224: Policy loss: 0.905218. Value loss: 11.565289. Entropy: 0.476558.\n",
      "Iteration 3225: Policy loss: 0.869603. Value loss: 10.132777. Entropy: 0.489008.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3226: Policy loss: -0.774009. Value loss: 149.015060. Entropy: 0.283034.\n",
      "Iteration 3227: Policy loss: -0.719871. Value loss: 53.727917. Entropy: 0.391113.\n",
      "Iteration 3228: Policy loss: -0.828052. Value loss: 44.577728. Entropy: 0.298604.\n",
      "episode: 1341   score: 355.0  epsilon: 1.0    steps: 431  evaluation reward: 254.65\n",
      "episode: 1342   score: 180.0  epsilon: 1.0    steps: 799  evaluation reward: 254.0\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3229: Policy loss: 0.940824. Value loss: 25.127907. Entropy: 0.426487.\n",
      "Iteration 3230: Policy loss: 1.404840. Value loss: 16.022089. Entropy: 0.435428.\n",
      "Iteration 3231: Policy loss: 1.214614. Value loss: 10.610625. Entropy: 0.454042.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3232: Policy loss: -1.371718. Value loss: 223.039413. Entropy: 0.489287.\n",
      "Iteration 3233: Policy loss: -1.807333. Value loss: 162.162506. Entropy: 0.491177.\n",
      "Iteration 3234: Policy loss: -1.749273. Value loss: 95.227982. Entropy: 0.454048.\n",
      "episode: 1343   score: 180.0  epsilon: 1.0    steps: 374  evaluation reward: 252.95\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3235: Policy loss: 0.736585. Value loss: 20.560379. Entropy: 0.270170.\n",
      "Iteration 3236: Policy loss: 0.895026. Value loss: 14.743106. Entropy: 0.278948.\n",
      "Iteration 3237: Policy loss: 1.023245. Value loss: 13.346704. Entropy: 0.270427.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3238: Policy loss: -2.131219. Value loss: 318.948181. Entropy: 0.280876.\n",
      "Iteration 3239: Policy loss: -1.526699. Value loss: 202.830109. Entropy: 0.256687.\n",
      "Iteration 3240: Policy loss: -1.571098. Value loss: 169.961884. Entropy: 0.265887.\n",
      "episode: 1344   score: 195.0  epsilon: 1.0    steps: 149  evaluation reward: 253.35\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3241: Policy loss: -0.233499. Value loss: 26.170099. Entropy: 0.205081.\n",
      "Iteration 3242: Policy loss: -0.328458. Value loss: 14.569633. Entropy: 0.210565.\n",
      "Iteration 3243: Policy loss: -0.218961. Value loss: 11.650213. Entropy: 0.216601.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3244: Policy loss: -1.578217. Value loss: 23.271391. Entropy: 0.263188.\n",
      "Iteration 3245: Policy loss: -1.379052. Value loss: 14.142363. Entropy: 0.288715.\n",
      "Iteration 3246: Policy loss: -1.567005. Value loss: 12.008875. Entropy: 0.257749.\n",
      "episode: 1345   score: 505.0  epsilon: 1.0    steps: 623  evaluation reward: 255.95\n",
      "episode: 1346   score: 480.0  epsilon: 1.0    steps: 989  evaluation reward: 259.4\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3247: Policy loss: -0.350993. Value loss: 23.067602. Entropy: 0.388721.\n",
      "Iteration 3248: Policy loss: -0.283153. Value loss: 17.126247. Entropy: 0.400004.\n",
      "Iteration 3249: Policy loss: -0.347424. Value loss: 14.667776. Entropy: 0.395705.\n",
      "episode: 1347   score: 285.0  epsilon: 1.0    steps: 510  evaluation reward: 261.15\n",
      "episode: 1348   score: 335.0  epsilon: 1.0    steps: 710  evaluation reward: 260.3\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3250: Policy loss: 0.480665. Value loss: 18.845697. Entropy: 0.254591.\n",
      "Iteration 3251: Policy loss: 0.515709. Value loss: 12.344420. Entropy: 0.252263.\n",
      "Iteration 3252: Policy loss: 0.629408. Value loss: 8.194473. Entropy: 0.275732.\n",
      "episode: 1349   score: 265.0  epsilon: 1.0    steps: 788  evaluation reward: 261.9\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3253: Policy loss: 0.414413. Value loss: 11.292830. Entropy: 0.388884.\n",
      "Iteration 3254: Policy loss: 0.302579. Value loss: 8.656527. Entropy: 0.393725.\n",
      "Iteration 3255: Policy loss: 0.345425. Value loss: 7.332800. Entropy: 0.418380.\n",
      "episode: 1350   score: 365.0  epsilon: 1.0    steps: 17  evaluation reward: 263.25\n",
      "now time :  2019-02-26 13:29:03.392441\n",
      "episode: 1351   score: 180.0  epsilon: 1.0    steps: 364  evaluation reward: 263.95\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3256: Policy loss: 1.455295. Value loss: 18.849058. Entropy: 0.398786.\n",
      "Iteration 3257: Policy loss: 1.383963. Value loss: 11.997149. Entropy: 0.400104.\n",
      "Iteration 3258: Policy loss: 1.447154. Value loss: 10.468731. Entropy: 0.415247.\n",
      "episode: 1352   score: 210.0  epsilon: 1.0    steps: 213  evaluation reward: 265.1\n",
      "episode: 1353   score: 75.0  epsilon: 1.0    steps: 513  evaluation reward: 263.7\n",
      "Training network. lr: 0.000225. clip: 0.090019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3259: Policy loss: -1.449932. Value loss: 11.339739. Entropy: 0.420132.\n",
      "Iteration 3260: Policy loss: -1.524654. Value loss: 7.105929. Entropy: 0.447225.\n",
      "Iteration 3261: Policy loss: -1.393548. Value loss: 5.992872. Entropy: 0.427707.\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3262: Policy loss: -0.051116. Value loss: 36.693169. Entropy: 0.433469.\n",
      "Iteration 3263: Policy loss: -0.768900. Value loss: 17.556616. Entropy: 0.431394.\n",
      "Iteration 3264: Policy loss: -0.087717. Value loss: 17.859558. Entropy: 0.464030.\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3265: Policy loss: -4.608569. Value loss: 383.028046. Entropy: 0.449020.\n",
      "Iteration 3266: Policy loss: -5.016473. Value loss: 270.599640. Entropy: 0.403398.\n",
      "Iteration 3267: Policy loss: -4.589665. Value loss: 160.225021. Entropy: 0.380828.\n",
      "episode: 1354   score: 415.0  epsilon: 1.0    steps: 686  evaluation reward: 265.9\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3268: Policy loss: 0.352853. Value loss: 32.849602. Entropy: 0.189460.\n",
      "Iteration 3269: Policy loss: 0.350863. Value loss: 16.187014. Entropy: 0.189862.\n",
      "Iteration 3270: Policy loss: 0.251621. Value loss: 13.743773. Entropy: 0.195578.\n",
      "episode: 1355   score: 230.0  epsilon: 1.0    steps: 436  evaluation reward: 265.2\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3271: Policy loss: 0.704389. Value loss: 18.644808. Entropy: 0.221771.\n",
      "Iteration 3272: Policy loss: 0.609352. Value loss: 13.814219. Entropy: 0.204403.\n",
      "Iteration 3273: Policy loss: 0.605511. Value loss: 12.544012. Entropy: 0.201249.\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3274: Policy loss: -0.579442. Value loss: 31.840366. Entropy: 0.313900.\n",
      "Iteration 3275: Policy loss: -0.597365. Value loss: 19.491194. Entropy: 0.294199.\n",
      "Iteration 3276: Policy loss: -0.431634. Value loss: 15.169669. Entropy: 0.312886.\n",
      "episode: 1356   score: 245.0  epsilon: 1.0    steps: 329  evaluation reward: 265.7\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3277: Policy loss: 0.496549. Value loss: 251.330154. Entropy: 0.429854.\n",
      "Iteration 3278: Policy loss: 0.976699. Value loss: 107.067619. Entropy: 0.430343.\n",
      "Iteration 3279: Policy loss: 0.511463. Value loss: 91.557762. Entropy: 0.490778.\n",
      "episode: 1357   score: 230.0  epsilon: 1.0    steps: 177  evaluation reward: 264.85\n",
      "episode: 1358   score: 265.0  epsilon: 1.0    steps: 614  evaluation reward: 265.7\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3280: Policy loss: 0.942242. Value loss: 28.535099. Entropy: 0.277088.\n",
      "Iteration 3281: Policy loss: 0.670336. Value loss: 20.751406. Entropy: 0.286251.\n",
      "Iteration 3282: Policy loss: 0.815153. Value loss: 16.539637. Entropy: 0.323092.\n",
      "episode: 1359   score: 560.0  epsilon: 1.0    steps: 813  evaluation reward: 269.95\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3283: Policy loss: -3.067760. Value loss: 51.307541. Entropy: 0.287417.\n",
      "Iteration 3284: Policy loss: -2.973185. Value loss: 30.338278. Entropy: 0.254251.\n",
      "Iteration 3285: Policy loss: -3.082972. Value loss: 24.297678. Entropy: 0.242085.\n",
      "episode: 1360   score: 605.0  epsilon: 1.0    steps: 966  evaluation reward: 271.65\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3286: Policy loss: -0.179809. Value loss: 65.716728. Entropy: 0.278273.\n",
      "Iteration 3287: Policy loss: -0.430487. Value loss: 32.981651. Entropy: 0.281133.\n",
      "Iteration 3288: Policy loss: -0.165865. Value loss: 30.535465. Entropy: 0.270084.\n",
      "episode: 1361   score: 895.0  epsilon: 1.0    steps: 76  evaluation reward: 278.9\n",
      "episode: 1362   score: 270.0  epsilon: 1.0    steps: 433  evaluation reward: 279.65\n",
      "episode: 1363   score: 250.0  epsilon: 1.0    steps: 761  evaluation reward: 280.25\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3289: Policy loss: 0.557694. Value loss: 22.767748. Entropy: 0.283623.\n",
      "Iteration 3290: Policy loss: 0.489222. Value loss: 17.924929. Entropy: 0.287639.\n",
      "Iteration 3291: Policy loss: 0.365185. Value loss: 15.634453. Entropy: 0.309337.\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3292: Policy loss: -1.135724. Value loss: 17.870840. Entropy: 0.161181.\n",
      "Iteration 3293: Policy loss: -0.888266. Value loss: 11.225589. Entropy: 0.138870.\n",
      "Iteration 3294: Policy loss: -1.045205. Value loss: 8.153500. Entropy: 0.116662.\n",
      "episode: 1364   score: 200.0  epsilon: 1.0    steps: 603  evaluation reward: 279.7\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3295: Policy loss: 0.603236. Value loss: 39.104286. Entropy: 0.289768.\n",
      "Iteration 3296: Policy loss: 0.961534. Value loss: 30.890799. Entropy: 0.252516.\n",
      "Iteration 3297: Policy loss: 1.262688. Value loss: 24.376551. Entropy: 0.287385.\n",
      "episode: 1365   score: 245.0  epsilon: 1.0    steps: 288  evaluation reward: 280.5\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3298: Policy loss: -5.551127. Value loss: 493.071960. Entropy: 0.247405.\n",
      "Iteration 3299: Policy loss: -5.403282. Value loss: 324.185211. Entropy: 0.188478.\n",
      "Iteration 3300: Policy loss: -5.563347. Value loss: 323.265228. Entropy: 0.212521.\n",
      "episode: 1366   score: 225.0  epsilon: 1.0    steps: 858  evaluation reward: 280.6\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3301: Policy loss: 0.457269. Value loss: 42.276264. Entropy: 0.095719.\n",
      "Iteration 3302: Policy loss: 0.407652. Value loss: 31.321676. Entropy: 0.093328.\n",
      "Iteration 3303: Policy loss: 0.329698. Value loss: 26.862886. Entropy: 0.125423.\n",
      "episode: 1367   score: 320.0  epsilon: 1.0    steps: 198  evaluation reward: 281.8\n",
      "episode: 1368   score: 270.0  epsilon: 1.0    steps: 1003  evaluation reward: 282.4\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3304: Policy loss: 2.270329. Value loss: 58.298706. Entropy: 0.154029.\n",
      "Iteration 3305: Policy loss: 2.539073. Value loss: 34.759895. Entropy: 0.165827.\n",
      "Iteration 3306: Policy loss: 1.507064. Value loss: 26.382095. Entropy: 0.176235.\n",
      "episode: 1369   score: 220.0  epsilon: 1.0    steps: 3  evaluation reward: 282.3\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3307: Policy loss: 1.150966. Value loss: 19.058826. Entropy: 0.105018.\n",
      "Iteration 3308: Policy loss: 1.200337. Value loss: 13.214438. Entropy: 0.092834.\n",
      "Iteration 3309: Policy loss: 1.244641. Value loss: 10.909481. Entropy: 0.108957.\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3310: Policy loss: 0.202796. Value loss: 22.832609. Entropy: 0.177718.\n",
      "Iteration 3311: Policy loss: -0.026484. Value loss: 16.422907. Entropy: 0.192328.\n",
      "Iteration 3312: Policy loss: 0.048502. Value loss: 14.781374. Entropy: 0.183232.\n",
      "episode: 1370   score: 105.0  epsilon: 1.0    steps: 248  evaluation reward: 280.15\n",
      "episode: 1371   score: 545.0  epsilon: 1.0    steps: 412  evaluation reward: 283.75\n",
      "episode: 1372   score: 505.0  epsilon: 1.0    steps: 734  evaluation reward: 283.95\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3313: Policy loss: 0.264219. Value loss: 22.124275. Entropy: 0.334246.\n",
      "Iteration 3314: Policy loss: -0.068449. Value loss: 14.854071. Entropy: 0.316084.\n",
      "Iteration 3315: Policy loss: 0.089863. Value loss: 12.178543. Entropy: 0.328639.\n",
      "episode: 1373   score: 240.0  epsilon: 1.0    steps: 288  evaluation reward: 282.4\n",
      "episode: 1374   score: 205.0  epsilon: 1.0    steps: 552  evaluation reward: 279.1\n",
      "episode: 1375   score: 205.0  epsilon: 1.0    steps: 883  evaluation reward: 277.05\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3316: Policy loss: 0.475989. Value loss: 21.476007. Entropy: 0.211808.\n",
      "Iteration 3317: Policy loss: 0.418996. Value loss: 13.274746. Entropy: 0.179732.\n",
      "Iteration 3318: Policy loss: 0.343132. Value loss: 12.719566. Entropy: 0.200329.\n",
      "episode: 1376   score: 195.0  epsilon: 1.0    steps: 1001  evaluation reward: 274.9\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3319: Policy loss: 0.386591. Value loss: 12.959742. Entropy: 0.090224.\n",
      "Iteration 3320: Policy loss: 0.234321. Value loss: 10.367167. Entropy: 0.069739.\n",
      "Iteration 3321: Policy loss: 0.404215. Value loss: 9.572758. Entropy: 0.077111.\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3322: Policy loss: 2.276404. Value loss: 14.471135. Entropy: 0.220143.\n",
      "Iteration 3323: Policy loss: 2.375344. Value loss: 7.835551. Entropy: 0.206752.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3324: Policy loss: 2.274737. Value loss: 9.546555. Entropy: 0.217118.\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3325: Policy loss: -1.217020. Value loss: 150.940704. Entropy: 0.110854.\n",
      "Iteration 3326: Policy loss: -0.791852. Value loss: 41.023605. Entropy: 0.114710.\n",
      "Iteration 3327: Policy loss: -1.158013. Value loss: 79.255379. Entropy: 0.110095.\n",
      "episode: 1377   score: 420.0  epsilon: 1.0    steps: 14  evaluation reward: 277.65\n",
      "episode: 1378   score: 205.0  epsilon: 1.0    steps: 237  evaluation reward: 277.75\n",
      "episode: 1379   score: 155.0  epsilon: 1.0    steps: 402  evaluation reward: 278.0\n",
      "episode: 1380   score: 205.0  epsilon: 1.0    steps: 713  evaluation reward: 279.05\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3328: Policy loss: -1.706344. Value loss: 29.825468. Entropy: 0.191351.\n",
      "Iteration 3329: Policy loss: -1.920383. Value loss: 22.525742. Entropy: 0.197335.\n",
      "Iteration 3330: Policy loss: -1.645586. Value loss: 19.781557. Entropy: 0.185690.\n",
      "episode: 1381   score: 220.0  epsilon: 1.0    steps: 347  evaluation reward: 277.35\n",
      "episode: 1382   score: 220.0  epsilon: 1.0    steps: 611  evaluation reward: 277.25\n",
      "episode: 1383   score: 220.0  epsilon: 1.0    steps: 812  evaluation reward: 278.4\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3331: Policy loss: 1.133134. Value loss: 20.538021. Entropy: 0.135725.\n",
      "Iteration 3332: Policy loss: 0.855918. Value loss: 15.248491. Entropy: 0.131786.\n",
      "Iteration 3333: Policy loss: 1.100751. Value loss: 12.248169. Entropy: 0.122429.\n",
      "episode: 1384   score: 245.0  epsilon: 1.0    steps: 1006  evaluation reward: 280.1\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3334: Policy loss: -1.108977. Value loss: 14.412516. Entropy: 0.112383.\n",
      "Iteration 3335: Policy loss: -1.099304. Value loss: 11.731975. Entropy: 0.112979.\n",
      "Iteration 3336: Policy loss: -1.143325. Value loss: 11.796004. Entropy: 0.123625.\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3337: Policy loss: 1.274769. Value loss: 13.307945. Entropy: 0.239114.\n",
      "Iteration 3338: Policy loss: 0.998392. Value loss: 11.059535. Entropy: 0.242677.\n",
      "Iteration 3339: Policy loss: 1.284205. Value loss: 11.175267. Entropy: 0.202286.\n",
      "episode: 1385   score: 185.0  epsilon: 1.0    steps: 125  evaluation reward: 278.55\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3340: Policy loss: 4.364381. Value loss: 36.076378. Entropy: 0.189177.\n",
      "Iteration 3341: Policy loss: 4.252544. Value loss: 16.437290. Entropy: 0.153281.\n",
      "Iteration 3342: Policy loss: 4.666555. Value loss: 18.331470. Entropy: 0.174210.\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3343: Policy loss: 0.404802. Value loss: 20.467491. Entropy: 0.075672.\n",
      "Iteration 3344: Policy loss: 0.546779. Value loss: 14.125566. Entropy: 0.081977.\n",
      "Iteration 3345: Policy loss: 0.540015. Value loss: 12.843596. Entropy: 0.075161.\n",
      "episode: 1386   score: 220.0  epsilon: 1.0    steps: 164  evaluation reward: 280.0\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3346: Policy loss: -1.576149. Value loss: 19.254969. Entropy: 0.065127.\n",
      "Iteration 3347: Policy loss: -1.408456. Value loss: 12.855848. Entropy: 0.073937.\n",
      "Iteration 3348: Policy loss: -1.558638. Value loss: 11.102913. Entropy: 0.064269.\n",
      "episode: 1387   score: 240.0  epsilon: 1.0    steps: 353  evaluation reward: 279.7\n",
      "episode: 1388   score: 240.0  epsilon: 1.0    steps: 540  evaluation reward: 277.7\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3349: Policy loss: -3.626913. Value loss: 274.443420. Entropy: 0.333969.\n",
      "Iteration 3350: Policy loss: -3.887874. Value loss: 209.692047. Entropy: 0.287206.\n",
      "Iteration 3351: Policy loss: -3.518884. Value loss: 121.956284. Entropy: 0.318879.\n",
      "episode: 1389   score: 220.0  epsilon: 1.0    steps: 933  evaluation reward: 278.95\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3352: Policy loss: -0.060479. Value loss: 35.439144. Entropy: 0.339770.\n",
      "Iteration 3353: Policy loss: -0.158340. Value loss: 18.459356. Entropy: 0.312303.\n",
      "Iteration 3354: Policy loss: -0.233233. Value loss: 16.826620. Entropy: 0.362253.\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3355: Policy loss: 2.239183. Value loss: 35.037071. Entropy: 0.158170.\n",
      "Iteration 3356: Policy loss: 2.336490. Value loss: 21.323614. Entropy: 0.192500.\n",
      "Iteration 3357: Policy loss: 2.216162. Value loss: 16.364443. Entropy: 0.187231.\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3358: Policy loss: 0.605571. Value loss: 74.383354. Entropy: 0.190338.\n",
      "Iteration 3359: Policy loss: 0.462635. Value loss: 59.043026. Entropy: 0.165564.\n",
      "Iteration 3360: Policy loss: 0.467663. Value loss: 56.186657. Entropy: 0.158355.\n",
      "episode: 1390   score: 515.0  epsilon: 1.0    steps: 392  evaluation reward: 280.4\n",
      "episode: 1391   score: 605.0  epsilon: 1.0    steps: 830  evaluation reward: 283.7\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3361: Policy loss: -0.671515. Value loss: 29.528458. Entropy: 0.348274.\n",
      "Iteration 3362: Policy loss: -0.550766. Value loss: 17.657570. Entropy: 0.371597.\n",
      "Iteration 3363: Policy loss: -0.745390. Value loss: 14.002256. Entropy: 0.341997.\n",
      "episode: 1392   score: 250.0  epsilon: 1.0    steps: 36  evaluation reward: 285.35\n",
      "episode: 1393   score: 190.0  epsilon: 1.0    steps: 326  evaluation reward: 285.5\n",
      "episode: 1394   score: 220.0  epsilon: 1.0    steps: 595  evaluation reward: 284.9\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3364: Policy loss: 0.758700. Value loss: 27.683365. Entropy: 0.277231.\n",
      "Iteration 3365: Policy loss: 0.888034. Value loss: 14.915855. Entropy: 0.282388.\n",
      "Iteration 3366: Policy loss: 0.773586. Value loss: 14.571671. Entropy: 0.275759.\n",
      "episode: 1395   score: 280.0  epsilon: 1.0    steps: 201  evaluation reward: 284.15\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3367: Policy loss: -2.285971. Value loss: 240.717972. Entropy: 0.253316.\n",
      "Iteration 3368: Policy loss: -2.143525. Value loss: 214.773590. Entropy: 0.283133.\n",
      "Iteration 3369: Policy loss: -1.745976. Value loss: 108.463089. Entropy: 0.228312.\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3370: Policy loss: 0.189596. Value loss: 30.772554. Entropy: 0.184037.\n",
      "Iteration 3371: Policy loss: 0.214587. Value loss: 23.121374. Entropy: 0.186322.\n",
      "Iteration 3372: Policy loss: 0.283363. Value loss: 18.299749. Entropy: 0.184921.\n",
      "episode: 1396   score: 885.0  epsilon: 1.0    steps: 678  evaluation reward: 291.2\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3373: Policy loss: 2.388465. Value loss: 30.956553. Entropy: 0.249741.\n",
      "Iteration 3374: Policy loss: 2.603768. Value loss: 18.391602. Entropy: 0.253738.\n",
      "Iteration 3375: Policy loss: 2.486103. Value loss: 13.552102. Entropy: 0.251846.\n",
      "episode: 1397   score: 220.0  epsilon: 1.0    steps: 885  evaluation reward: 288.45\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3376: Policy loss: 0.638895. Value loss: 28.874458. Entropy: 0.063807.\n",
      "Iteration 3377: Policy loss: 0.546556. Value loss: 22.115894. Entropy: 0.073504.\n",
      "Iteration 3378: Policy loss: 0.585062. Value loss: 19.161749. Entropy: 0.069909.\n",
      "episode: 1398   score: 195.0  epsilon: 1.0    steps: 39  evaluation reward: 288.3\n",
      "episode: 1399   score: 250.0  epsilon: 1.0    steps: 509  evaluation reward: 289.7\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3379: Policy loss: 0.981064. Value loss: 38.359814. Entropy: 0.168746.\n",
      "Iteration 3380: Policy loss: 0.880561. Value loss: 18.118853. Entropy: 0.156720.\n",
      "Iteration 3381: Policy loss: 1.006573. Value loss: 13.229955. Entropy: 0.155421.\n",
      "episode: 1400   score: 205.0  epsilon: 1.0    steps: 178  evaluation reward: 290.25\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3382: Policy loss: 1.487755. Value loss: 18.996077. Entropy: 0.235303.\n",
      "Iteration 3383: Policy loss: 1.273270. Value loss: 13.190219. Entropy: 0.226396.\n",
      "Iteration 3384: Policy loss: 1.597161. Value loss: 11.335160. Entropy: 0.228371.\n",
      "now time :  2019-02-26 13:31:30.243972\n",
      "episode: 1401   score: 295.0  epsilon: 1.0    steps: 262  evaluation reward: 289.6\n",
      "episode: 1402   score: 205.0  epsilon: 1.0    steps: 620  evaluation reward: 289.75\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3385: Policy loss: 0.551138. Value loss: 21.961485. Entropy: 0.139217.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3386: Policy loss: 0.875120. Value loss: 16.358236. Entropy: 0.140428.\n",
      "Iteration 3387: Policy loss: 0.743713. Value loss: 15.157722. Entropy: 0.140387.\n",
      "episode: 1403   score: 545.0  epsilon: 1.0    steps: 906  evaluation reward: 292.7\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3388: Policy loss: 0.081230. Value loss: 15.967778. Entropy: 0.182493.\n",
      "Iteration 3389: Policy loss: 0.092133. Value loss: 11.955717. Entropy: 0.179618.\n",
      "Iteration 3390: Policy loss: 0.113569. Value loss: 10.101035. Entropy: 0.181249.\n",
      "episode: 1404   score: 105.0  epsilon: 1.0    steps: 193  evaluation reward: 291.65\n",
      "episode: 1405   score: 205.0  epsilon: 1.0    steps: 860  evaluation reward: 292.15\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3391: Policy loss: 0.423895. Value loss: 21.732441. Entropy: 0.242106.\n",
      "Iteration 3392: Policy loss: 0.388050. Value loss: 14.485948. Entropy: 0.270382.\n",
      "Iteration 3393: Policy loss: 0.550825. Value loss: 11.183221. Entropy: 0.282846.\n",
      "episode: 1406   score: 195.0  epsilon: 1.0    steps: 37  evaluation reward: 292.2\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3394: Policy loss: -0.222495. Value loss: 22.939425. Entropy: 0.267533.\n",
      "Iteration 3395: Policy loss: -0.006648. Value loss: 11.441236. Entropy: 0.205872.\n",
      "Iteration 3396: Policy loss: 0.155589. Value loss: 10.833383. Entropy: 0.213210.\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3397: Policy loss: 0.931013. Value loss: 36.436279. Entropy: 0.237966.\n",
      "Iteration 3398: Policy loss: 0.079635. Value loss: 21.971731. Entropy: 0.223158.\n",
      "Iteration 3399: Policy loss: 0.245622. Value loss: 18.781382. Entropy: 0.211950.\n",
      "episode: 1407   score: 250.0  epsilon: 1.0    steps: 600  evaluation reward: 291.25\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3400: Policy loss: 1.758498. Value loss: 31.594465. Entropy: 0.402554.\n",
      "Iteration 3401: Policy loss: 3.823814. Value loss: 21.159033. Entropy: 0.428379.\n",
      "Iteration 3402: Policy loss: 1.729892. Value loss: 14.489540. Entropy: 0.379234.\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3403: Policy loss: 1.613069. Value loss: 46.642982. Entropy: 0.316543.\n",
      "Iteration 3404: Policy loss: 1.579556. Value loss: 21.361795. Entropy: 0.299540.\n",
      "Iteration 3405: Policy loss: 1.395585. Value loss: 16.150139. Entropy: 0.294199.\n",
      "episode: 1408   score: 240.0  epsilon: 1.0    steps: 961  evaluation reward: 290.8\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3406: Policy loss: 0.270426. Value loss: 26.476160. Entropy: 0.171621.\n",
      "Iteration 3407: Policy loss: 0.174680. Value loss: 17.686615. Entropy: 0.172784.\n",
      "Iteration 3408: Policy loss: 0.246274. Value loss: 14.995159. Entropy: 0.183238.\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3409: Policy loss: -0.407270. Value loss: 42.078247. Entropy: 0.146215.\n",
      "Iteration 3410: Policy loss: -0.513539. Value loss: 28.938049. Entropy: 0.130148.\n",
      "Iteration 3411: Policy loss: -0.414083. Value loss: 21.876352. Entropy: 0.139335.\n",
      "episode: 1409   score: 220.0  epsilon: 1.0    steps: 102  evaluation reward: 291.05\n",
      "episode: 1410   score: 410.0  epsilon: 1.0    steps: 394  evaluation reward: 293.9\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3412: Policy loss: -0.793451. Value loss: 36.665791. Entropy: 0.127886.\n",
      "Iteration 3413: Policy loss: -0.457338. Value loss: 23.153488. Entropy: 0.138542.\n",
      "Iteration 3414: Policy loss: -0.785813. Value loss: 19.405869. Entropy: 0.134128.\n",
      "episode: 1411   score: 310.0  epsilon: 1.0    steps: 229  evaluation reward: 295.3\n",
      "episode: 1412   score: 425.0  epsilon: 1.0    steps: 283  evaluation reward: 296.6\n",
      "episode: 1413   score: 305.0  epsilon: 1.0    steps: 816  evaluation reward: 292.85\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3415: Policy loss: 0.289403. Value loss: 30.007627. Entropy: 0.183230.\n",
      "Iteration 3416: Policy loss: 0.128909. Value loss: 16.005482. Entropy: 0.165187.\n",
      "Iteration 3417: Policy loss: 0.413081. Value loss: 12.452416. Entropy: 0.154633.\n",
      "episode: 1414   score: 735.0  epsilon: 1.0    steps: 689  evaluation reward: 296.9\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3418: Policy loss: -0.430525. Value loss: 24.997658. Entropy: 0.104437.\n",
      "Iteration 3419: Policy loss: -0.279584. Value loss: 14.521206. Entropy: 0.107146.\n",
      "Iteration 3420: Policy loss: -0.392065. Value loss: 12.426921. Entropy: 0.099485.\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3421: Policy loss: -2.795880. Value loss: 297.734009. Entropy: 0.116237.\n",
      "Iteration 3422: Policy loss: -1.821078. Value loss: 134.911087. Entropy: 0.110838.\n",
      "Iteration 3423: Policy loss: -3.259451. Value loss: 118.260368. Entropy: 0.087063.\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3424: Policy loss: -0.526604. Value loss: 37.359329. Entropy: 0.091748.\n",
      "Iteration 3425: Policy loss: -0.647880. Value loss: 21.487339. Entropy: 0.103539.\n",
      "Iteration 3426: Policy loss: -0.665724. Value loss: 17.357225. Entropy: 0.106197.\n",
      "episode: 1415   score: 280.0  epsilon: 1.0    steps: 513  evaluation reward: 297.6\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3427: Policy loss: -1.511251. Value loss: 39.200546. Entropy: 0.084680.\n",
      "Iteration 3428: Policy loss: -1.746914. Value loss: 24.968212. Entropy: 0.109191.\n",
      "Iteration 3429: Policy loss: -1.420141. Value loss: 19.103687. Entropy: 0.100376.\n",
      "episode: 1416   score: 195.0  epsilon: 1.0    steps: 312  evaluation reward: 295.45\n",
      "episode: 1417   score: 320.0  epsilon: 1.0    steps: 949  evaluation reward: 298.0\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3430: Policy loss: 0.842997. Value loss: 25.820189. Entropy: 0.117589.\n",
      "Iteration 3431: Policy loss: 1.102046. Value loss: 15.319541. Entropy: 0.107291.\n",
      "Iteration 3432: Policy loss: 1.082318. Value loss: 12.830080. Entropy: 0.122654.\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3433: Policy loss: -0.126787. Value loss: 29.959581. Entropy: 0.089537.\n",
      "Iteration 3434: Policy loss: -0.412411. Value loss: 19.953335. Entropy: 0.093334.\n",
      "Iteration 3435: Policy loss: -0.379181. Value loss: 15.263244. Entropy: 0.095813.\n",
      "episode: 1418   score: 615.0  epsilon: 1.0    steps: 398  evaluation reward: 303.1\n",
      "episode: 1419   score: 295.0  epsilon: 1.0    steps: 827  evaluation reward: 305.0\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3436: Policy loss: 0.728673. Value loss: 29.248257. Entropy: 0.109460.\n",
      "Iteration 3437: Policy loss: 0.720544. Value loss: 15.852917. Entropy: 0.118185.\n",
      "Iteration 3438: Policy loss: 0.609393. Value loss: 14.221457. Entropy: 0.104459.\n",
      "episode: 1420   score: 265.0  epsilon: 1.0    steps: 200  evaluation reward: 304.75\n",
      "episode: 1421   score: 170.0  epsilon: 1.0    steps: 616  evaluation reward: 304.05\n",
      "episode: 1422   score: 265.0  epsilon: 1.0    steps: 708  evaluation reward: 302.5\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3439: Policy loss: 0.373212. Value loss: 24.742237. Entropy: 0.213801.\n",
      "Iteration 3440: Policy loss: 0.290715. Value loss: 12.753772. Entropy: 0.201044.\n",
      "Iteration 3441: Policy loss: 0.546705. Value loss: 10.384520. Entropy: 0.199290.\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3442: Policy loss: -2.961655. Value loss: 248.139526. Entropy: 0.071175.\n",
      "Iteration 3443: Policy loss: -3.072233. Value loss: 124.198700. Entropy: 0.047971.\n",
      "Iteration 3444: Policy loss: -3.391876. Value loss: 110.870720. Entropy: 0.058198.\n",
      "episode: 1423   score: 195.0  epsilon: 1.0    steps: 978  evaluation reward: 301.35\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3445: Policy loss: -0.171414. Value loss: 24.848106. Entropy: 0.127354.\n",
      "Iteration 3446: Policy loss: -0.406799. Value loss: 17.525528. Entropy: 0.149536.\n",
      "Iteration 3447: Policy loss: -0.335286. Value loss: 13.627379. Entropy: 0.139033.\n",
      "episode: 1424   score: 660.0  epsilon: 1.0    steps: 49  evaluation reward: 305.3\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3448: Policy loss: 0.465881. Value loss: 17.704060. Entropy: 0.056291.\n",
      "Iteration 3449: Policy loss: 0.542287. Value loss: 12.901383. Entropy: 0.058101.\n",
      "Iteration 3450: Policy loss: 0.546587. Value loss: 10.963205. Entropy: 0.051737.\n",
      "episode: 1425   score: 195.0  epsilon: 1.0    steps: 415  evaluation reward: 304.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3451: Policy loss: 0.190491. Value loss: 13.293644. Entropy: 0.061757.\n",
      "Iteration 3452: Policy loss: 0.107907. Value loss: 9.159985. Entropy: 0.062285.\n",
      "Iteration 3453: Policy loss: 0.135803. Value loss: 8.442396. Entropy: 0.064865.\n",
      "episode: 1426   score: 195.0  epsilon: 1.0    steps: 229  evaluation reward: 303.6\n",
      "episode: 1427   score: 195.0  epsilon: 1.0    steps: 737  evaluation reward: 301.45\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3454: Policy loss: 2.570655. Value loss: 29.517149. Entropy: 0.105443.\n",
      "Iteration 3455: Policy loss: 2.427852. Value loss: 16.635798. Entropy: 0.061650.\n",
      "Iteration 3456: Policy loss: 2.369313. Value loss: 14.325904. Entropy: 0.055723.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3457: Policy loss: -0.678087. Value loss: 34.922802. Entropy: 0.081854.\n",
      "Iteration 3458: Policy loss: -0.221538. Value loss: 18.129570. Entropy: 0.107730.\n",
      "Iteration 3459: Policy loss: -0.531828. Value loss: 16.018297. Entropy: 0.089410.\n",
      "episode: 1428   score: 225.0  epsilon: 1.0    steps: 545  evaluation reward: 297.3\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3460: Policy loss: -0.066586. Value loss: 33.668697. Entropy: 0.046535.\n",
      "Iteration 3461: Policy loss: -0.134741. Value loss: 21.156042. Entropy: 0.054421.\n",
      "Iteration 3462: Policy loss: -0.342325. Value loss: 18.397543. Entropy: 0.052796.\n",
      "episode: 1429   score: 180.0  epsilon: 1.0    steps: 64  evaluation reward: 295.2\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3463: Policy loss: -0.514748. Value loss: 39.711658. Entropy: 0.081336.\n",
      "Iteration 3464: Policy loss: -0.325210. Value loss: 22.567797. Entropy: 0.093232.\n",
      "Iteration 3465: Policy loss: -0.392612. Value loss: 17.497648. Entropy: 0.103541.\n",
      "episode: 1430   score: 210.0  epsilon: 1.0    steps: 422  evaluation reward: 294.4\n",
      "episode: 1431   score: 295.0  epsilon: 1.0    steps: 993  evaluation reward: 294.5\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3466: Policy loss: -0.541254. Value loss: 31.495562. Entropy: 0.167312.\n",
      "Iteration 3467: Policy loss: -0.573993. Value loss: 18.923834. Entropy: 0.163085.\n",
      "Iteration 3468: Policy loss: -0.566460. Value loss: 13.391097. Entropy: 0.160495.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3469: Policy loss: -0.029337. Value loss: 34.162468. Entropy: 0.102083.\n",
      "Iteration 3470: Policy loss: -0.123916. Value loss: 24.322296. Entropy: 0.100201.\n",
      "Iteration 3471: Policy loss: 0.264260. Value loss: 20.781904. Entropy: 0.104192.\n",
      "episode: 1432   score: 175.0  epsilon: 1.0    steps: 136  evaluation reward: 292.45\n",
      "episode: 1433   score: 545.0  epsilon: 1.0    steps: 267  evaluation reward: 293.75\n",
      "episode: 1434   score: 265.0  epsilon: 1.0    steps: 699  evaluation reward: 294.7\n",
      "episode: 1435   score: 400.0  epsilon: 1.0    steps: 835  evaluation reward: 295.7\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3472: Policy loss: 0.877462. Value loss: 27.115088. Entropy: 0.250664.\n",
      "Iteration 3473: Policy loss: 0.676226. Value loss: 18.545488. Entropy: 0.248016.\n",
      "Iteration 3474: Policy loss: 1.016898. Value loss: 12.751663. Entropy: 0.259591.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3475: Policy loss: 1.101352. Value loss: 24.353970. Entropy: 0.166174.\n",
      "Iteration 3476: Policy loss: 1.239517. Value loss: 12.222447. Entropy: 0.156904.\n",
      "Iteration 3477: Policy loss: 1.052858. Value loss: 10.347642. Entropy: 0.163072.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3478: Policy loss: 1.699257. Value loss: 27.737402. Entropy: 0.128807.\n",
      "Iteration 3479: Policy loss: 1.587679. Value loss: 17.330746. Entropy: 0.136440.\n",
      "Iteration 3480: Policy loss: 1.824088. Value loss: 14.911412. Entropy: 0.115657.\n",
      "episode: 1436   score: 275.0  epsilon: 1.0    steps: 9  evaluation reward: 296.7\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3481: Policy loss: 0.319842. Value loss: 31.903137. Entropy: 0.251581.\n",
      "Iteration 3482: Policy loss: 0.629536. Value loss: 20.382107. Entropy: 0.211897.\n",
      "Iteration 3483: Policy loss: 0.555570. Value loss: 17.164274. Entropy: 0.240274.\n",
      "episode: 1437   score: 405.0  epsilon: 1.0    steps: 636  evaluation reward: 299.7\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3484: Policy loss: 1.601732. Value loss: 31.394672. Entropy: 0.053781.\n",
      "Iteration 3485: Policy loss: 1.318506. Value loss: 18.008743. Entropy: 0.062399.\n",
      "Iteration 3486: Policy loss: 1.856168. Value loss: 13.288749. Entropy: 0.095580.\n",
      "episode: 1438   score: 240.0  epsilon: 1.0    steps: 295  evaluation reward: 300.0\n",
      "episode: 1439   score: 255.0  epsilon: 1.0    steps: 394  evaluation reward: 298.35\n",
      "episode: 1440   score: 225.0  epsilon: 1.0    steps: 875  evaluation reward: 298.5\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3487: Policy loss: 2.821869. Value loss: 32.487209. Entropy: 0.173266.\n",
      "Iteration 3488: Policy loss: 2.805893. Value loss: 24.950338. Entropy: 0.174388.\n",
      "Iteration 3489: Policy loss: 2.869492. Value loss: 21.711475. Entropy: 0.182567.\n",
      "episode: 1441   score: 345.0  epsilon: 1.0    steps: 932  evaluation reward: 298.4\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3490: Policy loss: 1.176585. Value loss: 13.804449. Entropy: 0.248243.\n",
      "Iteration 3491: Policy loss: 1.394310. Value loss: 9.764480. Entropy: 0.240636.\n",
      "Iteration 3492: Policy loss: 1.641983. Value loss: 8.249437. Entropy: 0.124882.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3493: Policy loss: -1.522883. Value loss: 36.160271. Entropy: 0.088569.\n",
      "Iteration 3494: Policy loss: -1.360899. Value loss: 19.257595. Entropy: 0.087251.\n",
      "Iteration 3495: Policy loss: -1.332166. Value loss: 18.844730. Entropy: 0.087167.\n",
      "episode: 1442   score: 25.0  epsilon: 1.0    steps: 443  evaluation reward: 296.85\n",
      "episode: 1443   score: 260.0  epsilon: 1.0    steps: 643  evaluation reward: 297.65\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3496: Policy loss: 1.173147. Value loss: 21.587412. Entropy: 0.121379.\n",
      "Iteration 3497: Policy loss: 1.208363. Value loss: 15.005033. Entropy: 0.122098.\n",
      "Iteration 3498: Policy loss: 1.003104. Value loss: 12.056452. Entropy: 0.113280.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3499: Policy loss: 2.207996. Value loss: 29.171329. Entropy: 0.094986.\n",
      "Iteration 3500: Policy loss: 2.104745. Value loss: 15.367275. Entropy: 0.125402.\n",
      "Iteration 3501: Policy loss: 2.102597. Value loss: 11.286950. Entropy: 0.270181.\n",
      "episode: 1444   score: 140.0  epsilon: 1.0    steps: 537  evaluation reward: 297.1\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3502: Policy loss: 1.261335. Value loss: 29.534580. Entropy: 0.201349.\n",
      "Iteration 3503: Policy loss: 1.364625. Value loss: 17.545753. Entropy: 0.231318.\n",
      "Iteration 3504: Policy loss: 1.074290. Value loss: 14.299791. Entropy: 0.217059.\n",
      "episode: 1445   score: 265.0  epsilon: 1.0    steps: 25  evaluation reward: 294.7\n",
      "episode: 1446   score: 340.0  epsilon: 1.0    steps: 139  evaluation reward: 293.3\n",
      "episode: 1447   score: 135.0  epsilon: 1.0    steps: 975  evaluation reward: 291.8\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3505: Policy loss: -0.118616. Value loss: 33.746140. Entropy: 0.245001.\n",
      "Iteration 3506: Policy loss: 0.307671. Value loss: 21.193407. Entropy: 0.232212.\n",
      "Iteration 3507: Policy loss: 0.392310. Value loss: 18.494839. Entropy: 0.228270.\n",
      "episode: 1448   score: 225.0  epsilon: 1.0    steps: 804  evaluation reward: 290.7\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3508: Policy loss: -2.119870. Value loss: 31.056402. Entropy: 0.222197.\n",
      "Iteration 3509: Policy loss: -2.072530. Value loss: 22.469517. Entropy: 0.208387.\n",
      "Iteration 3510: Policy loss: -2.040491. Value loss: 18.544594. Entropy: 0.216784.\n",
      "episode: 1449   score: 200.0  epsilon: 1.0    steps: 455  evaluation reward: 290.05\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3511: Policy loss: -0.787211. Value loss: 26.744127. Entropy: 0.216625.\n",
      "Iteration 3512: Policy loss: -0.856933. Value loss: 20.751184. Entropy: 0.212990.\n",
      "Iteration 3513: Policy loss: -0.944042. Value loss: 18.302059. Entropy: 0.227818.\n",
      "episode: 1450   score: 250.0  epsilon: 1.0    steps: 286  evaluation reward: 288.9\n",
      "now time :  2019-02-26 13:33:55.013446\n",
      "episode: 1451   score: 145.0  epsilon: 1.0    steps: 558  evaluation reward: 288.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1452   score: 220.0  epsilon: 1.0    steps: 730  evaluation reward: 288.65\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3514: Policy loss: 1.977071. Value loss: 22.055330. Entropy: 0.264695.\n",
      "Iteration 3515: Policy loss: 2.209653. Value loss: 12.639461. Entropy: 0.252650.\n",
      "Iteration 3516: Policy loss: 2.047094. Value loss: 11.415161. Entropy: 0.254501.\n",
      "episode: 1453   score: 70.0  epsilon: 1.0    steps: 820  evaluation reward: 288.6\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3517: Policy loss: -2.425020. Value loss: 28.678041. Entropy: 0.179121.\n",
      "Iteration 3518: Policy loss: -2.830363. Value loss: 18.558033. Entropy: 0.189830.\n",
      "Iteration 3519: Policy loss: -2.420207. Value loss: 18.093582. Entropy: 0.163668.\n",
      "episode: 1454   score: 215.0  epsilon: 1.0    steps: 54  evaluation reward: 286.6\n",
      "episode: 1455   score: 140.0  epsilon: 1.0    steps: 496  evaluation reward: 285.7\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3520: Policy loss: 0.711108. Value loss: 23.750376. Entropy: 0.264845.\n",
      "Iteration 3521: Policy loss: 0.773887. Value loss: 15.540218. Entropy: 0.279155.\n",
      "Iteration 3522: Policy loss: 0.820425. Value loss: 14.022546. Entropy: 0.279861.\n",
      "episode: 1456   score: 70.0  epsilon: 1.0    steps: 744  evaluation reward: 283.95\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3523: Policy loss: 1.861793. Value loss: 24.701696. Entropy: 0.239936.\n",
      "Iteration 3524: Policy loss: 1.844437. Value loss: 12.876614. Entropy: 0.262756.\n",
      "Iteration 3525: Policy loss: 1.791048. Value loss: 13.192052. Entropy: 0.248236.\n",
      "episode: 1457   score: 145.0  epsilon: 1.0    steps: 640  evaluation reward: 283.1\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3526: Policy loss: 0.059373. Value loss: 36.622208. Entropy: 0.217719.\n",
      "Iteration 3527: Policy loss: 0.160104. Value loss: 19.312643. Entropy: 0.205521.\n",
      "Iteration 3528: Policy loss: 0.292769. Value loss: 15.205504. Entropy: 0.202073.\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3529: Policy loss: -1.946880. Value loss: 24.311619. Entropy: 0.191514.\n",
      "Iteration 3530: Policy loss: -1.936272. Value loss: 16.610254. Entropy: 0.198430.\n",
      "Iteration 3531: Policy loss: -2.041429. Value loss: 15.620209. Entropy: 0.206059.\n",
      "episode: 1458   score: 350.0  epsilon: 1.0    steps: 173  evaluation reward: 283.95\n",
      "episode: 1459   score: 235.0  epsilon: 1.0    steps: 331  evaluation reward: 280.7\n",
      "episode: 1460   score: 245.0  epsilon: 1.0    steps: 866  evaluation reward: 277.1\n",
      "episode: 1461   score: 250.0  epsilon: 1.0    steps: 952  evaluation reward: 270.65\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3532: Policy loss: -1.352549. Value loss: 21.563364. Entropy: 0.222547.\n",
      "Iteration 3533: Policy loss: -1.038074. Value loss: 15.193239. Entropy: 0.225951.\n",
      "Iteration 3534: Policy loss: -1.371878. Value loss: 12.813588. Entropy: 0.209491.\n",
      "episode: 1462   score: 165.0  epsilon: 1.0    steps: 481  evaluation reward: 269.6\n",
      "episode: 1463   score: 200.0  epsilon: 1.0    steps: 754  evaluation reward: 269.1\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3535: Policy loss: -1.460968. Value loss: 37.078785. Entropy: 0.182732.\n",
      "Iteration 3536: Policy loss: -1.924143. Value loss: 29.566839. Entropy: 0.190243.\n",
      "Iteration 3537: Policy loss: -1.600641. Value loss: 28.962767. Entropy: 0.169799.\n",
      "episode: 1464   score: 225.0  epsilon: 1.0    steps: 24  evaluation reward: 269.35\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3538: Policy loss: 0.420385. Value loss: 22.689772. Entropy: 0.137780.\n",
      "Iteration 3539: Policy loss: 0.404256. Value loss: 14.573259. Entropy: 0.135978.\n",
      "Iteration 3540: Policy loss: 0.419550. Value loss: 11.639687. Entropy: 0.133762.\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3541: Policy loss: 0.538259. Value loss: 21.717131. Entropy: 0.128203.\n",
      "Iteration 3542: Policy loss: 0.517517. Value loss: 14.414117. Entropy: 0.129912.\n",
      "Iteration 3543: Policy loss: 0.334262. Value loss: 11.305885. Entropy: 0.140603.\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3544: Policy loss: 0.078840. Value loss: 21.953306. Entropy: 0.120634.\n",
      "Iteration 3545: Policy loss: 0.060389. Value loss: 14.453675. Entropy: 0.122752.\n",
      "Iteration 3546: Policy loss: 0.010980. Value loss: 13.054386. Entropy: 0.127688.\n",
      "episode: 1465   score: 165.0  epsilon: 1.0    steps: 326  evaluation reward: 268.55\n",
      "episode: 1466   score: 205.0  epsilon: 1.0    steps: 861  evaluation reward: 268.35\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3547: Policy loss: 0.061258. Value loss: 19.361652. Entropy: 0.152500.\n",
      "Iteration 3548: Policy loss: -0.258084. Value loss: 12.813520. Entropy: 0.159899.\n",
      "Iteration 3549: Policy loss: -0.061283. Value loss: 11.419891. Entropy: 0.162091.\n",
      "episode: 1467   score: 220.0  epsilon: 1.0    steps: 460  evaluation reward: 267.35\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3550: Policy loss: -0.838425. Value loss: 31.782867. Entropy: 0.212207.\n",
      "Iteration 3551: Policy loss: -0.969314. Value loss: 21.215530. Entropy: 0.252721.\n",
      "Iteration 3552: Policy loss: -1.055835. Value loss: 16.419741. Entropy: 0.236030.\n",
      "episode: 1468   score: 295.0  epsilon: 1.0    steps: 132  evaluation reward: 267.6\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3553: Policy loss: 1.935186. Value loss: 10.311031. Entropy: 0.332026.\n",
      "Iteration 3554: Policy loss: 1.785640. Value loss: 6.119860. Entropy: 0.399758.\n",
      "Iteration 3555: Policy loss: 1.797567. Value loss: 5.320640. Entropy: 0.464609.\n",
      "episode: 1469   score: 255.0  epsilon: 1.0    steps: 622  evaluation reward: 267.95\n",
      "episode: 1470   score: 195.0  epsilon: 1.0    steps: 1023  evaluation reward: 268.85\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3556: Policy loss: 0.482779. Value loss: 42.972225. Entropy: 0.383523.\n",
      "Iteration 3557: Policy loss: 0.547515. Value loss: 21.393116. Entropy: 0.409228.\n",
      "Iteration 3558: Policy loss: 0.774657. Value loss: 15.545290. Entropy: 0.392599.\n",
      "episode: 1471   score: 275.0  epsilon: 1.0    steps: 679  evaluation reward: 266.15\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3559: Policy loss: -0.718106. Value loss: 23.340521. Entropy: 0.434525.\n",
      "Iteration 3560: Policy loss: -0.371625. Value loss: 14.912751. Entropy: 0.453236.\n",
      "Iteration 3561: Policy loss: -0.684968. Value loss: 10.604494. Entropy: 0.452691.\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3562: Policy loss: 0.764230. Value loss: 43.151501. Entropy: 0.368695.\n",
      "Iteration 3563: Policy loss: 0.618177. Value loss: 22.119074. Entropy: 0.405283.\n",
      "Iteration 3564: Policy loss: 0.816395. Value loss: 15.686835. Entropy: 0.408525.\n",
      "episode: 1472   score: 285.0  epsilon: 1.0    steps: 273  evaluation reward: 263.95\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3565: Policy loss: 1.577392. Value loss: 21.016108. Entropy: 0.511912.\n",
      "Iteration 3566: Policy loss: 1.444762. Value loss: 13.679757. Entropy: 0.529490.\n",
      "Iteration 3567: Policy loss: 1.311832. Value loss: 10.647988. Entropy: 0.525747.\n",
      "episode: 1473   score: 280.0  epsilon: 1.0    steps: 9  evaluation reward: 264.35\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3568: Policy loss: 3.124188. Value loss: 28.718664. Entropy: 0.640825.\n",
      "Iteration 3569: Policy loss: 3.218710. Value loss: 15.182722. Entropy: 0.637979.\n",
      "Iteration 3570: Policy loss: 3.279133. Value loss: 10.657999. Entropy: 0.639419.\n",
      "episode: 1474   score: 310.0  epsilon: 1.0    steps: 851  evaluation reward: 265.4\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3571: Policy loss: 0.853348. Value loss: 34.193802. Entropy: 0.456919.\n",
      "Iteration 3572: Policy loss: 1.182733. Value loss: 15.726512. Entropy: 0.444424.\n",
      "Iteration 3573: Policy loss: 0.894585. Value loss: 13.103527. Entropy: 0.417019.\n",
      "episode: 1475   score: 65.0  epsilon: 1.0    steps: 112  evaluation reward: 264.0\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3574: Policy loss: -0.859058. Value loss: 224.533951. Entropy: 0.312139.\n",
      "Iteration 3575: Policy loss: -1.462708. Value loss: 236.498947. Entropy: 0.248984.\n",
      "Iteration 3576: Policy loss: -1.408019. Value loss: 163.523544. Entropy: 0.253200.\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3577: Policy loss: -1.501504. Value loss: 43.199013. Entropy: 0.205141.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3578: Policy loss: -1.823323. Value loss: 23.259022. Entropy: 0.195227.\n",
      "Iteration 3579: Policy loss: -1.780225. Value loss: 19.466089. Entropy: 0.200529.\n",
      "episode: 1476   score: 325.0  epsilon: 1.0    steps: 479  evaluation reward: 265.3\n",
      "episode: 1477   score: 280.0  epsilon: 1.0    steps: 653  evaluation reward: 263.9\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3580: Policy loss: -2.415237. Value loss: 41.198334. Entropy: 0.251721.\n",
      "Iteration 3581: Policy loss: -2.317340. Value loss: 22.789652. Entropy: 0.271008.\n",
      "Iteration 3582: Policy loss: -2.638889. Value loss: 19.050220. Entropy: 0.264275.\n",
      "episode: 1478   score: 250.0  epsilon: 1.0    steps: 331  evaluation reward: 264.35\n",
      "episode: 1479   score: 435.0  epsilon: 1.0    steps: 573  evaluation reward: 267.15\n",
      "episode: 1480   score: 365.0  epsilon: 1.0    steps: 984  evaluation reward: 268.75\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3583: Policy loss: -2.808780. Value loss: 303.420624. Entropy: 0.335425.\n",
      "Iteration 3584: Policy loss: -2.270622. Value loss: 183.636368. Entropy: 0.304723.\n",
      "Iteration 3585: Policy loss: -2.037757. Value loss: 178.377228. Entropy: 0.312818.\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3586: Policy loss: 2.387404. Value loss: 17.840952. Entropy: 0.151041.\n",
      "Iteration 3587: Policy loss: 2.384115. Value loss: 10.923391. Entropy: 0.152432.\n",
      "Iteration 3588: Policy loss: 2.413145. Value loss: 8.632450. Entropy: 0.132241.\n",
      "episode: 1481   score: 775.0  epsilon: 1.0    steps: 191  evaluation reward: 274.3\n",
      "episode: 1482   score: 135.0  epsilon: 1.0    steps: 498  evaluation reward: 273.45\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3589: Policy loss: 1.774866. Value loss: 26.645226. Entropy: 0.228659.\n",
      "Iteration 3590: Policy loss: 1.921699. Value loss: 19.936871. Entropy: 0.222339.\n",
      "Iteration 3591: Policy loss: 1.592972. Value loss: 16.594091. Entropy: 0.223545.\n",
      "episode: 1483   score: 260.0  epsilon: 1.0    steps: 870  evaluation reward: 273.85\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3592: Policy loss: 1.258794. Value loss: 20.257360. Entropy: 0.472700.\n",
      "Iteration 3593: Policy loss: 0.953893. Value loss: 13.317047. Entropy: 0.462884.\n",
      "Iteration 3594: Policy loss: 1.212180. Value loss: 9.982893. Entropy: 0.431804.\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3595: Policy loss: 1.568910. Value loss: 42.141323. Entropy: 0.326366.\n",
      "Iteration 3596: Policy loss: 1.429222. Value loss: 23.078205. Entropy: 0.327328.\n",
      "Iteration 3597: Policy loss: 1.447852. Value loss: 15.638677. Entropy: 0.348436.\n",
      "episode: 1484   score: 265.0  epsilon: 1.0    steps: 22  evaluation reward: 274.05\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3598: Policy loss: -0.149145. Value loss: 17.789925. Entropy: 0.188299.\n",
      "Iteration 3599: Policy loss: 0.003919. Value loss: 11.367011. Entropy: 0.185406.\n",
      "Iteration 3600: Policy loss: -0.024167. Value loss: 10.564267. Entropy: 0.210105.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3601: Policy loss: -1.722939. Value loss: 31.364801. Entropy: 0.187038.\n",
      "Iteration 3602: Policy loss: -1.489732. Value loss: 17.485670. Entropy: 0.194670.\n",
      "Iteration 3603: Policy loss: -1.652507. Value loss: 14.414083. Entropy: 0.174349.\n",
      "episode: 1485   score: 220.0  epsilon: 1.0    steps: 279  evaluation reward: 274.4\n",
      "episode: 1486   score: 190.0  epsilon: 1.0    steps: 899  evaluation reward: 274.1\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3604: Policy loss: -2.526702. Value loss: 189.912537. Entropy: 0.325959.\n",
      "Iteration 3605: Policy loss: -3.019938. Value loss: 98.739037. Entropy: 0.303358.\n",
      "Iteration 3606: Policy loss: -2.832886. Value loss: 44.524193. Entropy: 0.302416.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3607: Policy loss: 0.889442. Value loss: 58.778660. Entropy: 0.236149.\n",
      "Iteration 3608: Policy loss: 1.302083. Value loss: 35.310001. Entropy: 0.160037.\n",
      "Iteration 3609: Policy loss: 1.116079. Value loss: 27.836184. Entropy: 0.194430.\n",
      "episode: 1487   score: 255.0  epsilon: 1.0    steps: 611  evaluation reward: 274.25\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3610: Policy loss: 0.396427. Value loss: 59.435417. Entropy: 0.175263.\n",
      "Iteration 3611: Policy loss: 0.564081. Value loss: 34.022720. Entropy: 0.183616.\n",
      "Iteration 3612: Policy loss: 0.523846. Value loss: 27.509010. Entropy: 0.182806.\n",
      "episode: 1488   score: 670.0  epsilon: 1.0    steps: 653  evaluation reward: 278.55\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3613: Policy loss: 1.320937. Value loss: 41.453621. Entropy: 0.184452.\n",
      "Iteration 3614: Policy loss: 1.768215. Value loss: 24.104319. Entropy: 0.192698.\n",
      "Iteration 3615: Policy loss: 1.537016. Value loss: 18.652630. Entropy: 0.200813.\n",
      "episode: 1489   score: 335.0  epsilon: 1.0    steps: 225  evaluation reward: 279.7\n",
      "episode: 1490   score: 325.0  epsilon: 1.0    steps: 415  evaluation reward: 277.8\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3616: Policy loss: -0.301759. Value loss: 37.339428. Entropy: 0.104478.\n",
      "Iteration 3617: Policy loss: -0.656528. Value loss: 19.949097. Entropy: 0.100084.\n",
      "Iteration 3618: Policy loss: -0.301659. Value loss: 17.875032. Entropy: 0.095640.\n",
      "episode: 1491   score: 320.0  epsilon: 1.0    steps: 793  evaluation reward: 274.95\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3619: Policy loss: 0.255821. Value loss: 24.122829. Entropy: 0.180330.\n",
      "Iteration 3620: Policy loss: 0.096291. Value loss: 15.658166. Entropy: 0.166783.\n",
      "Iteration 3621: Policy loss: 0.314452. Value loss: 13.660044. Entropy: 0.149894.\n",
      "episode: 1492   score: 210.0  epsilon: 1.0    steps: 274  evaluation reward: 274.55\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3622: Policy loss: -1.611582. Value loss: 31.173531. Entropy: 0.300326.\n",
      "Iteration 3623: Policy loss: -1.788171. Value loss: 20.959379. Entropy: 0.301215.\n",
      "Iteration 3624: Policy loss: -1.922397. Value loss: 15.647979. Entropy: 0.296673.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3625: Policy loss: -0.493968. Value loss: 23.046152. Entropy: 0.143007.\n",
      "Iteration 3626: Policy loss: -0.390956. Value loss: 16.377762. Entropy: 0.137905.\n",
      "Iteration 3627: Policy loss: -0.291165. Value loss: 12.006823. Entropy: 0.149579.\n",
      "episode: 1493   score: 430.0  epsilon: 1.0    steps: 113  evaluation reward: 276.95\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3628: Policy loss: -1.766784. Value loss: 46.274651. Entropy: 0.238043.\n",
      "Iteration 3629: Policy loss: -1.901128. Value loss: 31.404907. Entropy: 0.259104.\n",
      "Iteration 3630: Policy loss: -1.922033. Value loss: 23.580690. Entropy: 0.252705.\n",
      "episode: 1494   score: 290.0  epsilon: 1.0    steps: 723  evaluation reward: 277.65\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3631: Policy loss: -0.830800. Value loss: 30.760805. Entropy: 0.058427.\n",
      "Iteration 3632: Policy loss: -0.718093. Value loss: 19.438993. Entropy: 0.053762.\n",
      "Iteration 3633: Policy loss: -0.758566. Value loss: 15.181902. Entropy: 0.050794.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3634: Policy loss: 0.106270. Value loss: 35.905849. Entropy: 0.011793.\n",
      "Iteration 3635: Policy loss: 0.233162. Value loss: 20.945517. Entropy: 0.005782.\n",
      "Iteration 3636: Policy loss: -0.081628. Value loss: 18.994040. Entropy: 0.008045.\n",
      "episode: 1495   score: 355.0  epsilon: 1.0    steps: 233  evaluation reward: 278.4\n",
      "episode: 1496   score: 220.0  epsilon: 1.0    steps: 289  evaluation reward: 271.75\n",
      "episode: 1497   score: 280.0  epsilon: 1.0    steps: 426  evaluation reward: 272.35\n",
      "episode: 1498   score: 445.0  epsilon: 1.0    steps: 578  evaluation reward: 274.85\n",
      "episode: 1499   score: 620.0  epsilon: 1.0    steps: 968  evaluation reward: 278.55\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3637: Policy loss: -2.311172. Value loss: 217.654755. Entropy: 0.163591.\n",
      "Iteration 3638: Policy loss: -2.803555. Value loss: 141.314148. Entropy: 0.158579.\n",
      "Iteration 3639: Policy loss: -2.408842. Value loss: 151.473251. Entropy: 0.142653.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3640: Policy loss: -0.420658. Value loss: 14.490789. Entropy: 0.237677.\n",
      "Iteration 3641: Policy loss: -0.313138. Value loss: 11.417206. Entropy: 0.242365.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3642: Policy loss: -0.242172. Value loss: 9.621587. Entropy: 0.227654.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3643: Policy loss: -0.247327. Value loss: 226.803787. Entropy: 0.134414.\n",
      "Iteration 3644: Policy loss: 0.346189. Value loss: 200.520416. Entropy: 0.061200.\n",
      "Iteration 3645: Policy loss: 0.348343. Value loss: 132.500259. Entropy: 0.058940.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3646: Policy loss: -0.046340. Value loss: 32.129341. Entropy: 0.103603.\n",
      "Iteration 3647: Policy loss: 0.319090. Value loss: 21.490744. Entropy: 0.110812.\n",
      "Iteration 3648: Policy loss: -0.074305. Value loss: 16.807722. Entropy: 0.108793.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3649: Policy loss: 2.268788. Value loss: 85.124260. Entropy: 0.058471.\n",
      "Iteration 3650: Policy loss: 2.677365. Value loss: 38.775421. Entropy: 0.048935.\n",
      "Iteration 3651: Policy loss: 2.369014. Value loss: 31.547062. Entropy: 0.137572.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3652: Policy loss: 5.023569. Value loss: 73.008186. Entropy: 0.089647.\n",
      "Iteration 3653: Policy loss: 4.778692. Value loss: 38.386295. Entropy: 0.078772.\n",
      "Iteration 3654: Policy loss: 4.789337. Value loss: 28.620213. Entropy: 0.086513.\n",
      "episode: 1500   score: 315.0  epsilon: 1.0    steps: 892  evaluation reward: 279.65\n",
      "now time :  2019-02-26 13:36:35.745842\n",
      "episode: 1501   score: 245.0  epsilon: 1.0    steps: 911  evaluation reward: 279.15\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3655: Policy loss: 0.846709. Value loss: 42.321136. Entropy: 0.186533.\n",
      "Iteration 3656: Policy loss: 0.469211. Value loss: 22.369104. Entropy: 0.203979.\n",
      "Iteration 3657: Policy loss: 0.809872. Value loss: 17.488060. Entropy: 0.212066.\n",
      "episode: 1502   score: 340.0  epsilon: 1.0    steps: 341  evaluation reward: 280.5\n",
      "episode: 1503   score: 320.0  epsilon: 1.0    steps: 606  evaluation reward: 278.25\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3658: Policy loss: 4.023129. Value loss: 68.619492. Entropy: 0.196914.\n",
      "Iteration 3659: Policy loss: 4.340610. Value loss: 29.214350. Entropy: 0.232091.\n",
      "Iteration 3660: Policy loss: 4.358895. Value loss: 24.526226. Entropy: 0.208858.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3661: Policy loss: -0.623752. Value loss: 39.305737. Entropy: 0.289131.\n",
      "Iteration 3662: Policy loss: -0.618156. Value loss: 23.817066. Entropy: 0.303210.\n",
      "Iteration 3663: Policy loss: -0.602924. Value loss: 19.223740. Entropy: 0.288946.\n",
      "episode: 1504   score: 280.0  epsilon: 1.0    steps: 146  evaluation reward: 280.0\n",
      "episode: 1505   score: 440.0  epsilon: 1.0    steps: 454  evaluation reward: 282.35\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3664: Policy loss: 2.080057. Value loss: 44.677834. Entropy: 0.436954.\n",
      "Iteration 3665: Policy loss: 1.881143. Value loss: 30.447828. Entropy: 0.439127.\n",
      "Iteration 3666: Policy loss: 2.160152. Value loss: 25.849298. Entropy: 0.453756.\n",
      "episode: 1506   score: 665.0  epsilon: 1.0    steps: 738  evaluation reward: 287.05\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3667: Policy loss: 3.150089. Value loss: 30.071228. Entropy: 0.382196.\n",
      "Iteration 3668: Policy loss: 3.056929. Value loss: 15.893565. Entropy: 0.393611.\n",
      "Iteration 3669: Policy loss: 2.921094. Value loss: 14.283879. Entropy: 0.353975.\n",
      "episode: 1507   score: 90.0  epsilon: 1.0    steps: 906  evaluation reward: 285.45\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3670: Policy loss: 0.766714. Value loss: 26.991812. Entropy: 0.269603.\n",
      "Iteration 3671: Policy loss: 0.763917. Value loss: 19.725393. Entropy: 0.271974.\n",
      "Iteration 3672: Policy loss: 0.826960. Value loss: 13.643085. Entropy: 0.271414.\n",
      "episode: 1508   score: 545.0  epsilon: 1.0    steps: 42  evaluation reward: 288.5\n",
      "episode: 1509   score: 210.0  epsilon: 1.0    steps: 603  evaluation reward: 288.4\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3673: Policy loss: 1.159821. Value loss: 38.076817. Entropy: 0.159802.\n",
      "Iteration 3674: Policy loss: 0.574764. Value loss: 22.425192. Entropy: 0.149360.\n",
      "Iteration 3675: Policy loss: 0.586186. Value loss: 16.649111. Entropy: 0.158090.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3676: Policy loss: 1.622552. Value loss: 41.534878. Entropy: 0.233453.\n",
      "Iteration 3677: Policy loss: 1.707790. Value loss: 25.988140. Entropy: 0.236050.\n",
      "Iteration 3678: Policy loss: 1.906582. Value loss: 20.208462. Entropy: 0.240143.\n",
      "episode: 1510   score: 270.0  epsilon: 1.0    steps: 843  evaluation reward: 287.0\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3679: Policy loss: 1.200112. Value loss: 31.232819. Entropy: 0.274104.\n",
      "Iteration 3680: Policy loss: 1.194505. Value loss: 19.140610. Entropy: 0.267451.\n",
      "Iteration 3681: Policy loss: 1.020957. Value loss: 15.418989. Entropy: 0.276445.\n",
      "episode: 1511   score: 260.0  epsilon: 1.0    steps: 327  evaluation reward: 286.5\n",
      "episode: 1512   score: 335.0  epsilon: 1.0    steps: 471  evaluation reward: 285.6\n",
      "episode: 1513   score: 195.0  epsilon: 1.0    steps: 724  evaluation reward: 284.5\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3682: Policy loss: 0.330914. Value loss: 23.413404. Entropy: 0.338070.\n",
      "Iteration 3683: Policy loss: 0.396568. Value loss: 14.473394. Entropy: 0.340030.\n",
      "Iteration 3684: Policy loss: 0.551353. Value loss: 12.398667. Entropy: 0.344148.\n",
      "episode: 1514   score: 245.0  epsilon: 1.0    steps: 201  evaluation reward: 279.6\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3685: Policy loss: 1.563040. Value loss: 42.267166. Entropy: 0.262174.\n",
      "Iteration 3686: Policy loss: 1.443405. Value loss: 21.337852. Entropy: 0.256225.\n",
      "Iteration 3687: Policy loss: 1.765827. Value loss: 19.676678. Entropy: 0.253910.\n",
      "episode: 1515   score: 80.0  epsilon: 1.0    steps: 600  evaluation reward: 277.6\n",
      "episode: 1516   score: 260.0  epsilon: 1.0    steps: 937  evaluation reward: 278.25\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3688: Policy loss: 1.322685. Value loss: 37.068943. Entropy: 0.293383.\n",
      "Iteration 3689: Policy loss: 1.338664. Value loss: 25.432663. Entropy: 0.289140.\n",
      "Iteration 3690: Policy loss: 1.240254. Value loss: 22.598381. Entropy: 0.283378.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3691: Policy loss: 0.316720. Value loss: 27.122765. Entropy: 0.407558.\n",
      "Iteration 3692: Policy loss: 0.149201. Value loss: 18.222172. Entropy: 0.397494.\n",
      "Iteration 3693: Policy loss: 0.531158. Value loss: 14.563544. Entropy: 0.407023.\n",
      "episode: 1517   score: 205.0  epsilon: 1.0    steps: 117  evaluation reward: 277.1\n",
      "episode: 1518   score: 115.0  epsilon: 1.0    steps: 711  evaluation reward: 272.1\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3694: Policy loss: -0.318679. Value loss: 22.890530. Entropy: 0.280002.\n",
      "Iteration 3695: Policy loss: -0.065595. Value loss: 14.038431. Entropy: 0.274074.\n",
      "Iteration 3696: Policy loss: -0.149512. Value loss: 13.576655. Entropy: 0.264869.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3697: Policy loss: 1.347699. Value loss: 23.910583. Entropy: 0.136710.\n",
      "Iteration 3698: Policy loss: 1.604182. Value loss: 16.926083. Entropy: 0.139205.\n",
      "Iteration 3699: Policy loss: 1.226960. Value loss: 14.507355. Entropy: 0.134895.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3700: Policy loss: -0.907848. Value loss: 24.809612. Entropy: 0.212813.\n",
      "Iteration 3701: Policy loss: -0.756739. Value loss: 14.278125. Entropy: 0.213379.\n",
      "Iteration 3702: Policy loss: -0.919739. Value loss: 12.340503. Entropy: 0.201230.\n",
      "episode: 1519   score: 295.0  epsilon: 1.0    steps: 437  evaluation reward: 272.1\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3703: Policy loss: -2.241564. Value loss: 33.771610. Entropy: 0.319758.\n",
      "Iteration 3704: Policy loss: -1.993448. Value loss: 14.585732. Entropy: 0.298905.\n",
      "Iteration 3705: Policy loss: -2.054932. Value loss: 11.398534. Entropy: 0.306347.\n",
      "episode: 1520   score: 270.0  epsilon: 1.0    steps: 223  evaluation reward: 272.15\n",
      "episode: 1521   score: 230.0  epsilon: 1.0    steps: 311  evaluation reward: 272.75\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3706: Policy loss: -0.083832. Value loss: 35.864571. Entropy: 0.196906.\n",
      "Iteration 3707: Policy loss: 0.285978. Value loss: 20.166107. Entropy: 0.167098.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3708: Policy loss: -0.045451. Value loss: 15.980776. Entropy: 0.195034.\n",
      "episode: 1522   score: 400.0  epsilon: 1.0    steps: 770  evaluation reward: 274.1\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3709: Policy loss: 0.611244. Value loss: 15.338656. Entropy: 0.220860.\n",
      "Iteration 3710: Policy loss: 0.485223. Value loss: 9.261387. Entropy: 0.221617.\n",
      "Iteration 3711: Policy loss: 0.341080. Value loss: 9.892632. Entropy: 0.229776.\n",
      "episode: 1523   score: 215.0  epsilon: 1.0    steps: 578  evaluation reward: 274.3\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3712: Policy loss: -0.198284. Value loss: 44.474934. Entropy: 0.134253.\n",
      "Iteration 3713: Policy loss: -0.583486. Value loss: 24.136442. Entropy: 0.127039.\n",
      "Iteration 3714: Policy loss: -0.484641. Value loss: 19.954475. Entropy: 0.137250.\n",
      "episode: 1524   score: 275.0  epsilon: 1.0    steps: 927  evaluation reward: 270.45\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3715: Policy loss: -1.328747. Value loss: 30.444735. Entropy: 0.250896.\n",
      "Iteration 3716: Policy loss: -1.162098. Value loss: 19.604389. Entropy: 0.262502.\n",
      "Iteration 3717: Policy loss: -0.788476. Value loss: 17.272589. Entropy: 0.269990.\n",
      "episode: 1525   score: 295.0  epsilon: 1.0    steps: 11  evaluation reward: 271.45\n",
      "episode: 1526   score: 55.0  epsilon: 1.0    steps: 310  evaluation reward: 270.05\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3718: Policy loss: 0.781751. Value loss: 28.182991. Entropy: 0.086813.\n",
      "Iteration 3719: Policy loss: 0.708324. Value loss: 17.573400. Entropy: 0.118231.\n",
      "Iteration 3720: Policy loss: 0.712067. Value loss: 14.873106. Entropy: 0.129653.\n",
      "episode: 1527   score: 105.0  epsilon: 1.0    steps: 606  evaluation reward: 269.15\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3721: Policy loss: 1.337776. Value loss: 28.646257. Entropy: 0.161162.\n",
      "Iteration 3722: Policy loss: 1.142252. Value loss: 20.026146. Entropy: 0.209849.\n",
      "Iteration 3723: Policy loss: 1.343515. Value loss: 15.340376. Entropy: 0.211845.\n",
      "episode: 1528   score: 315.0  epsilon: 1.0    steps: 648  evaluation reward: 270.05\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3724: Policy loss: -0.159472. Value loss: 34.192101. Entropy: 0.281897.\n",
      "Iteration 3725: Policy loss: -0.018256. Value loss: 20.697584. Entropy: 0.251868.\n",
      "Iteration 3726: Policy loss: 0.047678. Value loss: 17.490856. Entropy: 0.249165.\n",
      "episode: 1529   score: 105.0  epsilon: 1.0    steps: 273  evaluation reward: 269.3\n",
      "episode: 1530   score: 305.0  epsilon: 1.0    steps: 444  evaluation reward: 270.25\n",
      "episode: 1531   score: 195.0  epsilon: 1.0    steps: 831  evaluation reward: 269.25\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3727: Policy loss: 2.104682. Value loss: 22.874853. Entropy: 0.249339.\n",
      "Iteration 3728: Policy loss: 2.258989. Value loss: 13.947031. Entropy: 0.256527.\n",
      "Iteration 3729: Policy loss: 2.095785. Value loss: 13.177359. Entropy: 0.226700.\n",
      "episode: 1532   score: 75.0  epsilon: 1.0    steps: 573  evaluation reward: 268.25\n",
      "episode: 1533   score: 105.0  epsilon: 1.0    steps: 744  evaluation reward: 263.85\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3730: Policy loss: -0.188025. Value loss: 27.661743. Entropy: 0.118729.\n",
      "Iteration 3731: Policy loss: -0.184160. Value loss: 18.996069. Entropy: 0.124720.\n",
      "Iteration 3732: Policy loss: -0.275408. Value loss: 15.179511. Entropy: 0.105955.\n",
      "episode: 1534   score: 275.0  epsilon: 1.0    steps: 181  evaluation reward: 263.95\n",
      "episode: 1535   score: 265.0  epsilon: 1.0    steps: 962  evaluation reward: 262.6\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3733: Policy loss: 3.676896. Value loss: 23.894617. Entropy: 0.135781.\n",
      "Iteration 3734: Policy loss: 3.782373. Value loss: 15.165966. Entropy: 0.144811.\n",
      "Iteration 3735: Policy loss: 3.710263. Value loss: 12.679092. Entropy: 0.162220.\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3736: Policy loss: -3.286019. Value loss: 193.237457. Entropy: 0.245656.\n",
      "Iteration 3737: Policy loss: -3.664572. Value loss: 107.849907. Entropy: 0.226287.\n",
      "Iteration 3738: Policy loss: -3.846052. Value loss: 79.229469. Entropy: 0.199167.\n",
      "episode: 1536   score: 115.0  epsilon: 1.0    steps: 257  evaluation reward: 261.0\n",
      "episode: 1537   score: 105.0  epsilon: 1.0    steps: 761  evaluation reward: 258.0\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3739: Policy loss: 2.871635. Value loss: 32.298306. Entropy: 0.205862.\n",
      "Iteration 3740: Policy loss: 2.607249. Value loss: 19.383612. Entropy: 0.220320.\n",
      "Iteration 3741: Policy loss: 2.636354. Value loss: 18.576683. Entropy: 0.214481.\n",
      "episode: 1538   score: 135.0  epsilon: 1.0    steps: 455  evaluation reward: 256.95\n",
      "episode: 1539   score: 100.0  epsilon: 1.0    steps: 959  evaluation reward: 255.4\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3742: Policy loss: 1.173779. Value loss: 259.376038. Entropy: 0.228865.\n",
      "Iteration 3743: Policy loss: 0.981704. Value loss: 172.922821. Entropy: 0.254527.\n",
      "Iteration 3744: Policy loss: 1.129739. Value loss: 155.985474. Entropy: 0.239091.\n",
      "episode: 1540   score: 565.0  epsilon: 1.0    steps: 4  evaluation reward: 258.8\n",
      "episode: 1541   score: 340.0  epsilon: 1.0    steps: 594  evaluation reward: 258.75\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3745: Policy loss: -0.923367. Value loss: 20.263248. Entropy: 0.173139.\n",
      "Iteration 3746: Policy loss: -0.994247. Value loss: 13.671798. Entropy: 0.152113.\n",
      "Iteration 3747: Policy loss: -0.946988. Value loss: 13.085212. Entropy: 0.159807.\n",
      "episode: 1542   score: 215.0  epsilon: 1.0    steps: 182  evaluation reward: 260.65\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3748: Policy loss: -0.429517. Value loss: 49.141037. Entropy: 0.270271.\n",
      "Iteration 3749: Policy loss: -0.576838. Value loss: 20.630230. Entropy: 0.242745.\n",
      "Iteration 3750: Policy loss: -0.324647. Value loss: 15.741106. Entropy: 0.270609.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3751: Policy loss: 1.111196. Value loss: 37.304310. Entropy: 0.204170.\n",
      "Iteration 3752: Policy loss: 0.857868. Value loss: 17.314659. Entropy: 0.278501.\n",
      "Iteration 3753: Policy loss: 0.884542. Value loss: 12.664689. Entropy: 0.279559.\n",
      "episode: 1543   score: 60.0  epsilon: 1.0    steps: 633  evaluation reward: 258.65\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3754: Policy loss: 3.695569. Value loss: 29.912971. Entropy: 0.345021.\n",
      "Iteration 3755: Policy loss: 3.782733. Value loss: 14.804353. Entropy: 0.354748.\n",
      "Iteration 3756: Policy loss: 4.007865. Value loss: 13.553280. Entropy: 0.371265.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3757: Policy loss: 0.438656. Value loss: 37.755379. Entropy: 0.356934.\n",
      "Iteration 3758: Policy loss: 0.933768. Value loss: 22.213667. Entropy: 0.334222.\n",
      "Iteration 3759: Policy loss: 0.435732. Value loss: 22.426167. Entropy: 0.365243.\n",
      "episode: 1544   score: 180.0  epsilon: 1.0    steps: 683  evaluation reward: 259.05\n",
      "episode: 1545   score: 365.0  epsilon: 1.0    steps: 798  evaluation reward: 260.05\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3760: Policy loss: -0.104032. Value loss: 37.735474. Entropy: 0.247158.\n",
      "Iteration 3761: Policy loss: 0.318113. Value loss: 27.601562. Entropy: 0.214220.\n",
      "Iteration 3762: Policy loss: 0.341327. Value loss: 25.900982. Entropy: 0.237146.\n",
      "episode: 1546   score: 210.0  epsilon: 1.0    steps: 93  evaluation reward: 258.75\n",
      "episode: 1547   score: 210.0  epsilon: 1.0    steps: 394  evaluation reward: 259.5\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3763: Policy loss: 0.994213. Value loss: 32.084087. Entropy: 0.165253.\n",
      "Iteration 3764: Policy loss: 1.019675. Value loss: 17.754961. Entropy: 0.166044.\n",
      "Iteration 3765: Policy loss: 0.878572. Value loss: 14.892965. Entropy: 0.135756.\n",
      "episode: 1548   score: 255.0  epsilon: 1.0    steps: 251  evaluation reward: 259.8\n",
      "episode: 1549   score: 380.0  epsilon: 1.0    steps: 960  evaluation reward: 261.6\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3766: Policy loss: 0.019752. Value loss: 23.244835. Entropy: 0.300413.\n",
      "Iteration 3767: Policy loss: -0.073132. Value loss: 12.267816. Entropy: 0.279650.\n",
      "Iteration 3768: Policy loss: -0.182515. Value loss: 10.086632. Entropy: 0.288088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1550   score: 285.0  epsilon: 1.0    steps: 264  evaluation reward: 261.95\n",
      "now time :  2019-02-26 13:38:45.956803\n",
      "episode: 1551   score: 135.0  epsilon: 1.0    steps: 815  evaluation reward: 261.85\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3769: Policy loss: 0.135395. Value loss: 32.172546. Entropy: 0.316797.\n",
      "Iteration 3770: Policy loss: 0.308135. Value loss: 19.165638. Entropy: 0.307874.\n",
      "Iteration 3771: Policy loss: 0.384717. Value loss: 16.618204. Entropy: 0.307040.\n",
      "episode: 1552   score: 100.0  epsilon: 1.0    steps: 62  evaluation reward: 260.65\n",
      "episode: 1553   score: 155.0  epsilon: 1.0    steps: 518  evaluation reward: 261.5\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3772: Policy loss: 1.046761. Value loss: 36.352909. Entropy: 0.229490.\n",
      "Iteration 3773: Policy loss: 0.767591. Value loss: 23.206211. Entropy: 0.226823.\n",
      "Iteration 3774: Policy loss: 0.787717. Value loss: 20.053158. Entropy: 0.229384.\n",
      "episode: 1554   score: 80.0  epsilon: 1.0    steps: 256  evaluation reward: 260.15\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3775: Policy loss: 0.537437. Value loss: 213.150177. Entropy: 0.206015.\n",
      "Iteration 3776: Policy loss: -0.142033. Value loss: 94.336884. Entropy: 0.196196.\n",
      "Iteration 3777: Policy loss: 0.280240. Value loss: 59.911263. Entropy: 0.202595.\n",
      "episode: 1555   score: 105.0  epsilon: 1.0    steps: 822  evaluation reward: 259.8\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3778: Policy loss: 0.828822. Value loss: 25.938795. Entropy: 0.289755.\n",
      "Iteration 3779: Policy loss: 0.776703. Value loss: 14.191699. Entropy: 0.286136.\n",
      "Iteration 3780: Policy loss: 0.715819. Value loss: 12.328907. Entropy: 0.289268.\n",
      "episode: 1556   score: 180.0  epsilon: 1.0    steps: 458  evaluation reward: 260.9\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3781: Policy loss: 0.790478. Value loss: 30.718962. Entropy: 0.174898.\n",
      "Iteration 3782: Policy loss: 0.662228. Value loss: 20.670069. Entropy: 0.195453.\n",
      "Iteration 3783: Policy loss: 0.523543. Value loss: 14.775348. Entropy: 0.194874.\n",
      "episode: 1557   score: 210.0  epsilon: 1.0    steps: 279  evaluation reward: 261.55\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3784: Policy loss: 2.474367. Value loss: 19.136860. Entropy: 0.179466.\n",
      "Iteration 3785: Policy loss: 2.342424. Value loss: 12.080634. Entropy: 0.181056.\n",
      "Iteration 3786: Policy loss: 2.511549. Value loss: 10.603552. Entropy: 0.192820.\n",
      "episode: 1558   score: 210.0  epsilon: 1.0    steps: 537  evaluation reward: 260.15\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3787: Policy loss: 0.599374. Value loss: 21.849277. Entropy: 0.148813.\n",
      "Iteration 3788: Policy loss: 0.603765. Value loss: 13.828206. Entropy: 0.155975.\n",
      "Iteration 3789: Policy loss: 0.675051. Value loss: 11.493432. Entropy: 0.151932.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3790: Policy loss: 0.144951. Value loss: 25.357416. Entropy: 0.219616.\n",
      "Iteration 3791: Policy loss: 0.221300. Value loss: 12.173127. Entropy: 0.225456.\n",
      "Iteration 3792: Policy loss: 0.277619. Value loss: 10.483813. Entropy: 0.230435.\n",
      "episode: 1559   score: 210.0  epsilon: 1.0    steps: 689  evaluation reward: 259.9\n",
      "episode: 1560   score: 490.0  epsilon: 1.0    steps: 915  evaluation reward: 262.35\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3793: Policy loss: -0.248878. Value loss: 226.555832. Entropy: 0.154207.\n",
      "Iteration 3794: Policy loss: 0.037621. Value loss: 172.045563. Entropy: 0.218175.\n",
      "Iteration 3795: Policy loss: -0.616782. Value loss: 135.832047. Entropy: 0.192680.\n",
      "episode: 1561   score: 240.0  epsilon: 1.0    steps: 19  evaluation reward: 262.25\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3796: Policy loss: 0.078017. Value loss: 25.044039. Entropy: 0.218531.\n",
      "Iteration 3797: Policy loss: 0.151262. Value loss: 15.652066. Entropy: 0.229600.\n",
      "Iteration 3798: Policy loss: 0.169764. Value loss: 10.877060. Entropy: 0.236758.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3799: Policy loss: 1.579585. Value loss: 28.092045. Entropy: 0.262835.\n",
      "Iteration 3800: Policy loss: 1.178521. Value loss: 12.934174. Entropy: 0.277683.\n",
      "Iteration 3801: Policy loss: 1.442833. Value loss: 8.851661. Entropy: 0.391152.\n",
      "episode: 1562   score: 535.0  epsilon: 1.0    steps: 203  evaluation reward: 265.95\n",
      "episode: 1563   score: 230.0  epsilon: 1.0    steps: 778  evaluation reward: 266.25\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3802: Policy loss: 1.288080. Value loss: 28.322819. Entropy: 0.278147.\n",
      "Iteration 3803: Policy loss: 1.373164. Value loss: 14.135200. Entropy: 0.309446.\n",
      "Iteration 3804: Policy loss: 1.142514. Value loss: 11.996019. Entropy: 0.311129.\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3805: Policy loss: 0.072225. Value loss: 24.453218. Entropy: 0.273949.\n",
      "Iteration 3806: Policy loss: 0.144472. Value loss: 13.605044. Entropy: 0.247050.\n",
      "Iteration 3807: Policy loss: 0.103040. Value loss: 11.270332. Entropy: 0.237582.\n",
      "episode: 1564   score: 90.0  epsilon: 1.0    steps: 24  evaluation reward: 264.9\n",
      "episode: 1565   score: 180.0  epsilon: 1.0    steps: 274  evaluation reward: 265.05\n",
      "episode: 1566   score: 295.0  epsilon: 1.0    steps: 391  evaluation reward: 265.95\n",
      "episode: 1567   score: 180.0  epsilon: 1.0    steps: 525  evaluation reward: 265.55\n",
      "episode: 1568   score: 115.0  epsilon: 1.0    steps: 690  evaluation reward: 263.75\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3808: Policy loss: 2.898951. Value loss: 17.547451. Entropy: 0.403439.\n",
      "Iteration 3809: Policy loss: 2.920696. Value loss: 8.300614. Entropy: 0.376323.\n",
      "Iteration 3810: Policy loss: 2.932923. Value loss: 6.636362. Entropy: 0.386971.\n",
      "episode: 1569   score: 165.0  epsilon: 1.0    steps: 986  evaluation reward: 262.85\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3811: Policy loss: -1.666241. Value loss: 30.350794. Entropy: 0.347179.\n",
      "Iteration 3812: Policy loss: -1.631023. Value loss: 20.069735. Entropy: 0.301880.\n",
      "Iteration 3813: Policy loss: -1.784040. Value loss: 16.644911. Entropy: 0.304481.\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3814: Policy loss: 1.449468. Value loss: 17.880934. Entropy: 0.375909.\n",
      "Iteration 3815: Policy loss: 1.546204. Value loss: 9.207743. Entropy: 0.407088.\n",
      "Iteration 3816: Policy loss: 1.433943. Value loss: 10.339507. Entropy: 0.397684.\n",
      "episode: 1570   score: 205.0  epsilon: 1.0    steps: 199  evaluation reward: 262.95\n",
      "episode: 1571   score: 65.0  epsilon: 1.0    steps: 270  evaluation reward: 260.85\n",
      "episode: 1572   score: 80.0  epsilon: 1.0    steps: 707  evaluation reward: 258.8\n",
      "episode: 1573   score: 135.0  epsilon: 1.0    steps: 818  evaluation reward: 257.35\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3817: Policy loss: -0.770046. Value loss: 17.717846. Entropy: 0.280698.\n",
      "Iteration 3818: Policy loss: -0.970138. Value loss: 10.322116. Entropy: 0.284597.\n",
      "Iteration 3819: Policy loss: -0.755809. Value loss: 8.192436. Entropy: 0.300291.\n",
      "episode: 1574   score: 225.0  epsilon: 1.0    steps: 604  evaluation reward: 256.5\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3820: Policy loss: 0.238064. Value loss: 32.140446. Entropy: 0.297562.\n",
      "Iteration 3821: Policy loss: 0.139796. Value loss: 20.898838. Entropy: 0.296738.\n",
      "Iteration 3822: Policy loss: 0.136718. Value loss: 16.727650. Entropy: 0.265456.\n",
      "episode: 1575   score: 180.0  epsilon: 1.0    steps: 28  evaluation reward: 257.65\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3823: Policy loss: -0.078619. Value loss: 27.647490. Entropy: 0.406915.\n",
      "Iteration 3824: Policy loss: 0.019175. Value loss: 15.787910. Entropy: 0.409476.\n",
      "Iteration 3825: Policy loss: 0.095102. Value loss: 13.495206. Entropy: 0.420598.\n",
      "episode: 1576   score: 90.0  epsilon: 1.0    steps: 814  evaluation reward: 255.3\n",
      "episode: 1577   score: 180.0  epsilon: 1.0    steps: 996  evaluation reward: 254.3\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3826: Policy loss: 1.694937. Value loss: 15.274164. Entropy: 0.213634.\n",
      "Iteration 3827: Policy loss: 1.715914. Value loss: 9.917550. Entropy: 0.247973.\n",
      "Iteration 3828: Policy loss: 1.635227. Value loss: 7.862883. Entropy: 0.258832.\n",
      "episode: 1578   score: 215.0  epsilon: 1.0    steps: 482  evaluation reward: 253.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3829: Policy loss: 0.305742. Value loss: 16.793684. Entropy: 0.337647.\n",
      "Iteration 3830: Policy loss: 0.465713. Value loss: 9.843289. Entropy: 0.316702.\n",
      "Iteration 3831: Policy loss: 0.267997. Value loss: 7.564712. Entropy: 0.322111.\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3832: Policy loss: -1.599959. Value loss: 20.155142. Entropy: 0.205276.\n",
      "Iteration 3833: Policy loss: -1.626586. Value loss: 12.687953. Entropy: 0.229731.\n",
      "Iteration 3834: Policy loss: -1.524165. Value loss: 11.063786. Entropy: 0.217740.\n",
      "episode: 1579   score: 205.0  epsilon: 1.0    steps: 247  evaluation reward: 251.65\n",
      "episode: 1580   score: 195.0  epsilon: 1.0    steps: 620  evaluation reward: 249.95\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3835: Policy loss: -1.563531. Value loss: 28.770050. Entropy: 0.270088.\n",
      "Iteration 3836: Policy loss: -1.218867. Value loss: 18.720926. Entropy: 0.271002.\n",
      "Iteration 3837: Policy loss: -1.343923. Value loss: 14.597936. Entropy: 0.286933.\n",
      "episode: 1581   score: 105.0  epsilon: 1.0    steps: 971  evaluation reward: 243.25\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3838: Policy loss: -0.739112. Value loss: 33.111637. Entropy: 0.290290.\n",
      "Iteration 3839: Policy loss: -0.740599. Value loss: 17.840706. Entropy: 0.310206.\n",
      "Iteration 3840: Policy loss: -0.724101. Value loss: 14.538894. Entropy: 0.300061.\n",
      "episode: 1582   score: 550.0  epsilon: 1.0    steps: 47  evaluation reward: 247.4\n",
      "episode: 1583   score: 280.0  epsilon: 1.0    steps: 321  evaluation reward: 247.6\n",
      "episode: 1584   score: 130.0  epsilon: 1.0    steps: 843  evaluation reward: 246.25\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3841: Policy loss: -1.512474. Value loss: 86.246628. Entropy: 0.161603.\n",
      "Iteration 3842: Policy loss: -0.962611. Value loss: 14.424589. Entropy: 0.160381.\n",
      "Iteration 3843: Policy loss: -1.228440. Value loss: 45.950943. Entropy: 0.168907.\n",
      "episode: 1585   score: 315.0  epsilon: 1.0    steps: 750  evaluation reward: 247.2\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3844: Policy loss: 0.717292. Value loss: 21.135010. Entropy: 0.252216.\n",
      "Iteration 3845: Policy loss: 0.343468. Value loss: 15.799876. Entropy: 0.281241.\n",
      "Iteration 3846: Policy loss: 0.431311. Value loss: 12.859511. Entropy: 0.265443.\n",
      "episode: 1586   score: 110.0  epsilon: 1.0    steps: 522  evaluation reward: 246.4\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3847: Policy loss: -0.073442. Value loss: 13.888852. Entropy: 0.364344.\n",
      "Iteration 3848: Policy loss: -0.107540. Value loss: 10.301366. Entropy: 0.373037.\n",
      "Iteration 3849: Policy loss: -0.147432. Value loss: 8.066349. Entropy: 0.382026.\n",
      "episode: 1587   score: 105.0  epsilon: 1.0    steps: 354  evaluation reward: 244.9\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3850: Policy loss: -0.380887. Value loss: 14.792068. Entropy: 0.306861.\n",
      "Iteration 3851: Policy loss: -0.598268. Value loss: 12.467988. Entropy: 0.281336.\n",
      "Iteration 3852: Policy loss: -0.277910. Value loss: 11.054472. Entropy: 0.314085.\n",
      "episode: 1588   score: 205.0  epsilon: 1.0    steps: 451  evaluation reward: 240.25\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3853: Policy loss: -1.723813. Value loss: 237.628326. Entropy: 0.283450.\n",
      "Iteration 3854: Policy loss: -1.438984. Value loss: 160.549347. Entropy: 0.283099.\n",
      "Iteration 3855: Policy loss: -1.466603. Value loss: 160.507584. Entropy: 0.278811.\n",
      "episode: 1589   score: 150.0  epsilon: 1.0    steps: 121  evaluation reward: 238.4\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3856: Policy loss: -1.813760. Value loss: 38.848373. Entropy: 0.186108.\n",
      "Iteration 3857: Policy loss: -1.715245. Value loss: 25.566601. Entropy: 0.171168.\n",
      "Iteration 3858: Policy loss: -1.764039. Value loss: 21.655300. Entropy: 0.168384.\n",
      "episode: 1590   score: 120.0  epsilon: 1.0    steps: 339  evaluation reward: 236.35\n",
      "episode: 1591   score: 185.0  epsilon: 1.0    steps: 782  evaluation reward: 235.0\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3859: Policy loss: 0.821870. Value loss: 19.087389. Entropy: 0.210665.\n",
      "Iteration 3860: Policy loss: 0.898658. Value loss: 12.545943. Entropy: 0.197344.\n",
      "Iteration 3861: Policy loss: 0.616208. Value loss: 10.443516. Entropy: 0.208512.\n",
      "episode: 1592   score: 155.0  epsilon: 1.0    steps: 687  evaluation reward: 234.45\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3862: Policy loss: 2.455280. Value loss: 21.887772. Entropy: 0.246161.\n",
      "Iteration 3863: Policy loss: 2.221104. Value loss: 15.172883. Entropy: 0.230726.\n",
      "Iteration 3864: Policy loss: 2.456290. Value loss: 10.232529. Entropy: 0.240445.\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3865: Policy loss: -0.048323. Value loss: 19.276703. Entropy: 0.324629.\n",
      "Iteration 3866: Policy loss: -0.066051. Value loss: 11.048903. Entropy: 0.326790.\n",
      "Iteration 3867: Policy loss: -0.069501. Value loss: 8.388352. Entropy: 0.318234.\n",
      "episode: 1593   score: 125.0  epsilon: 1.0    steps: 584  evaluation reward: 231.4\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3868: Policy loss: 3.034911. Value loss: 33.234245. Entropy: 0.426316.\n",
      "Iteration 3869: Policy loss: 3.003275. Value loss: 16.285706. Entropy: 0.457119.\n",
      "Iteration 3870: Policy loss: 2.788120. Value loss: 14.343519. Entropy: 0.455842.\n",
      "episode: 1594   score: 370.0  epsilon: 1.0    steps: 173  evaluation reward: 232.2\n",
      "episode: 1595   score: 185.0  epsilon: 1.0    steps: 508  evaluation reward: 230.5\n",
      "episode: 1596   score: 610.0  epsilon: 1.0    steps: 957  evaluation reward: 234.4\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3871: Policy loss: 1.392101. Value loss: 27.755423. Entropy: 0.351245.\n",
      "Iteration 3872: Policy loss: 1.394743. Value loss: 15.531251. Entropy: 0.349123.\n",
      "Iteration 3873: Policy loss: 1.578337. Value loss: 13.500368. Entropy: 0.388517.\n",
      "episode: 1597   score: 50.0  epsilon: 1.0    steps: 823  evaluation reward: 232.1\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3874: Policy loss: -0.363524. Value loss: 192.024536. Entropy: 0.184634.\n",
      "Iteration 3875: Policy loss: -1.545431. Value loss: 174.063751. Entropy: 0.197389.\n",
      "Iteration 3876: Policy loss: -0.244693. Value loss: 78.155373. Entropy: 0.181156.\n",
      "episode: 1598   score: 105.0  epsilon: 1.0    steps: 696  evaluation reward: 228.7\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3877: Policy loss: 0.555638. Value loss: 25.240416. Entropy: 0.270451.\n",
      "Iteration 3878: Policy loss: 0.529245. Value loss: 17.591688. Entropy: 0.274421.\n",
      "Iteration 3879: Policy loss: 0.674685. Value loss: 15.140946. Entropy: 0.272700.\n",
      "episode: 1599   score: 155.0  epsilon: 1.0    steps: 291  evaluation reward: 224.05\n",
      "episode: 1600   score: 105.0  epsilon: 1.0    steps: 936  evaluation reward: 221.95\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3880: Policy loss: 0.770207. Value loss: 19.340481. Entropy: 0.395708.\n",
      "Iteration 3881: Policy loss: 0.825640. Value loss: 12.211618. Entropy: 0.397648.\n",
      "Iteration 3882: Policy loss: 0.874590. Value loss: 10.226144. Entropy: 0.373040.\n",
      "now time :  2019-02-26 13:40:57.810306\n",
      "episode: 1601   score: 465.0  epsilon: 1.0    steps: 44  evaluation reward: 224.15\n",
      "episode: 1602   score: 60.0  epsilon: 1.0    steps: 393  evaluation reward: 221.35\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3883: Policy loss: 0.332880. Value loss: 32.070087. Entropy: 0.295030.\n",
      "Iteration 3884: Policy loss: 0.298208. Value loss: 19.362589. Entropy: 0.291414.\n",
      "Iteration 3885: Policy loss: 0.265952. Value loss: 18.058521. Entropy: 0.292770.\n",
      "episode: 1603   score: 105.0  epsilon: 1.0    steps: 137  evaluation reward: 219.2\n",
      "episode: 1604   score: 100.0  epsilon: 1.0    steps: 693  evaluation reward: 217.4\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3886: Policy loss: -0.578414. Value loss: 31.688700. Entropy: 0.199316.\n",
      "Iteration 3887: Policy loss: -0.610949. Value loss: 16.309786. Entropy: 0.217087.\n",
      "Iteration 3888: Policy loss: -0.451165. Value loss: 12.170544. Entropy: 0.214356.\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3889: Policy loss: -0.885831. Value loss: 24.246655. Entropy: 0.207337.\n",
      "Iteration 3890: Policy loss: -0.863398. Value loss: 16.265621. Entropy: 0.219896.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3891: Policy loss: -0.928824. Value loss: 13.075006. Entropy: 0.199052.\n",
      "episode: 1605   score: 75.0  epsilon: 1.0    steps: 83  evaluation reward: 213.75\n",
      "episode: 1606   score: 210.0  epsilon: 1.0    steps: 558  evaluation reward: 209.2\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3892: Policy loss: -0.051510. Value loss: 20.957813. Entropy: 0.284149.\n",
      "Iteration 3893: Policy loss: -0.409697. Value loss: 14.015271. Entropy: 0.264501.\n",
      "Iteration 3894: Policy loss: -0.217371. Value loss: 11.035592. Entropy: 0.307459.\n",
      "episode: 1607   score: 215.0  epsilon: 1.0    steps: 355  evaluation reward: 210.45\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3895: Policy loss: -1.987573. Value loss: 31.814426. Entropy: 0.288151.\n",
      "Iteration 3896: Policy loss: -2.272815. Value loss: 22.876772. Entropy: 0.293534.\n",
      "Iteration 3897: Policy loss: -2.160172. Value loss: 15.561296. Entropy: 0.309155.\n",
      "episode: 1608   score: 315.0  epsilon: 1.0    steps: 844  evaluation reward: 208.15\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3898: Policy loss: -0.406121. Value loss: 26.093472. Entropy: 0.153799.\n",
      "Iteration 3899: Policy loss: -0.292199. Value loss: 16.251553. Entropy: 0.147969.\n",
      "Iteration 3900: Policy loss: -0.186789. Value loss: 14.809487. Entropy: 0.151446.\n",
      "episode: 1609   score: 270.0  epsilon: 1.0    steps: 438  evaluation reward: 208.75\n",
      "episode: 1610   score: 250.0  epsilon: 1.0    steps: 987  evaluation reward: 208.55\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3901: Policy loss: 0.005353. Value loss: 24.819405. Entropy: 0.171712.\n",
      "Iteration 3902: Policy loss: -0.051233. Value loss: 14.161327. Entropy: 0.176537.\n",
      "Iteration 3903: Policy loss: 0.059853. Value loss: 12.071131. Entropy: 0.179590.\n",
      "episode: 1611   score: 280.0  epsilon: 1.0    steps: 201  evaluation reward: 208.75\n",
      "episode: 1612   score: 255.0  epsilon: 1.0    steps: 735  evaluation reward: 207.95\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3904: Policy loss: -0.075736. Value loss: 27.395454. Entropy: 0.205346.\n",
      "Iteration 3905: Policy loss: 0.117733. Value loss: 15.450706. Entropy: 0.193379.\n",
      "Iteration 3906: Policy loss: 0.061105. Value loss: 12.872900. Entropy: 0.181893.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3907: Policy loss: 1.337190. Value loss: 24.343138. Entropy: 0.324559.\n",
      "Iteration 3908: Policy loss: 1.298890. Value loss: 14.489524. Entropy: 0.248811.\n",
      "Iteration 3909: Policy loss: 1.067505. Value loss: 12.933391. Entropy: 0.227094.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3910: Policy loss: -0.517821. Value loss: 17.999598. Entropy: 0.261345.\n",
      "Iteration 3911: Policy loss: -0.290795. Value loss: 12.442973. Entropy: 0.283324.\n",
      "Iteration 3912: Policy loss: -0.648911. Value loss: 11.207174. Entropy: 0.285527.\n",
      "episode: 1613   score: 275.0  epsilon: 1.0    steps: 571  evaluation reward: 208.75\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3913: Policy loss: -1.124288. Value loss: 278.269165. Entropy: 0.339170.\n",
      "Iteration 3914: Policy loss: -0.780388. Value loss: 208.022308. Entropy: 0.344135.\n",
      "Iteration 3915: Policy loss: -1.264768. Value loss: 186.636536. Entropy: 0.370007.\n",
      "episode: 1614   score: 205.0  epsilon: 1.0    steps: 286  evaluation reward: 208.35\n",
      "episode: 1615   score: 210.0  epsilon: 1.0    steps: 453  evaluation reward: 209.65\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3916: Policy loss: 1.378367. Value loss: 26.201971. Entropy: 0.226734.\n",
      "Iteration 3917: Policy loss: 1.362721. Value loss: 17.024433. Entropy: 0.227580.\n",
      "Iteration 3918: Policy loss: 1.472010. Value loss: 13.820538. Entropy: 0.229834.\n",
      "episode: 1616   score: 300.0  epsilon: 1.0    steps: 56  evaluation reward: 210.05\n",
      "episode: 1617   score: 235.0  epsilon: 1.0    steps: 736  evaluation reward: 210.35\n",
      "episode: 1618   score: 135.0  epsilon: 1.0    steps: 913  evaluation reward: 210.55\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3919: Policy loss: -0.761367. Value loss: 22.148361. Entropy: 0.241522.\n",
      "Iteration 3920: Policy loss: -0.833075. Value loss: 18.519526. Entropy: 0.253639.\n",
      "Iteration 3921: Policy loss: -0.895030. Value loss: 16.061888. Entropy: 0.232080.\n",
      "episode: 1619   score: 285.0  epsilon: 1.0    steps: 784  evaluation reward: 210.45\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3922: Policy loss: -1.500409. Value loss: 253.066635. Entropy: 0.164572.\n",
      "Iteration 3923: Policy loss: -0.990104. Value loss: 150.858353. Entropy: 0.154681.\n",
      "Iteration 3924: Policy loss: -1.875034. Value loss: 160.186844. Entropy: 0.151719.\n",
      "episode: 1620   score: 440.0  epsilon: 1.0    steps: 131  evaluation reward: 212.15\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3925: Policy loss: 1.799228. Value loss: 23.189005. Entropy: 0.293334.\n",
      "Iteration 3926: Policy loss: 1.852445. Value loss: 17.912092. Entropy: 0.268661.\n",
      "Iteration 3927: Policy loss: 1.921162. Value loss: 15.169107. Entropy: 0.279853.\n",
      "episode: 1621   score: 415.0  epsilon: 1.0    steps: 626  evaluation reward: 214.0\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3928: Policy loss: -0.212150. Value loss: 38.207661. Entropy: 0.169798.\n",
      "Iteration 3929: Policy loss: -0.706919. Value loss: 21.057196. Entropy: 0.179751.\n",
      "Iteration 3930: Policy loss: -0.532465. Value loss: 14.968493. Entropy: 0.193972.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3931: Policy loss: 1.887573. Value loss: 47.805420. Entropy: 0.287664.\n",
      "Iteration 3932: Policy loss: 2.280489. Value loss: 29.804436. Entropy: 0.292925.\n",
      "Iteration 3933: Policy loss: 1.723930. Value loss: 23.189505. Entropy: 0.303383.\n",
      "episode: 1622   score: 145.0  epsilon: 1.0    steps: 252  evaluation reward: 211.45\n",
      "episode: 1623   score: 150.0  epsilon: 1.0    steps: 399  evaluation reward: 210.8\n",
      "episode: 1624   score: 180.0  epsilon: 1.0    steps: 971  evaluation reward: 209.85\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3934: Policy loss: 3.746899. Value loss: 50.648651. Entropy: 0.217131.\n",
      "Iteration 3935: Policy loss: 3.973763. Value loss: 27.520435. Entropy: 0.258269.\n",
      "Iteration 3936: Policy loss: 3.691729. Value loss: 22.773266. Entropy: 0.245298.\n",
      "episode: 1625   score: 180.0  epsilon: 1.0    steps: 368  evaluation reward: 208.7\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3937: Policy loss: 1.278072. Value loss: 29.639238. Entropy: 0.134091.\n",
      "Iteration 3938: Policy loss: 1.276592. Value loss: 17.528307. Entropy: 0.141568.\n",
      "Iteration 3939: Policy loss: 0.974335. Value loss: 13.029493. Entropy: 0.138495.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3940: Policy loss: -0.040712. Value loss: 28.775524. Entropy: 0.259653.\n",
      "Iteration 3941: Policy loss: 0.098403. Value loss: 18.445791. Entropy: 0.274894.\n",
      "Iteration 3942: Policy loss: 0.191765. Value loss: 14.008450. Entropy: 0.265279.\n",
      "episode: 1626   score: 305.0  epsilon: 1.0    steps: 112  evaluation reward: 211.2\n",
      "episode: 1627   score: 260.0  epsilon: 1.0    steps: 795  evaluation reward: 212.75\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3943: Policy loss: 0.913840. Value loss: 22.129448. Entropy: 0.168928.\n",
      "Iteration 3944: Policy loss: 0.922152. Value loss: 15.688189. Entropy: 0.170460.\n",
      "Iteration 3945: Policy loss: 0.967858. Value loss: 13.356406. Entropy: 0.171826.\n",
      "episode: 1628   score: 55.0  epsilon: 1.0    steps: 135  evaluation reward: 210.15\n",
      "episode: 1629   score: 60.0  epsilon: 1.0    steps: 380  evaluation reward: 209.7\n",
      "episode: 1630   score: 230.0  epsilon: 1.0    steps: 502  evaluation reward: 208.95\n",
      "episode: 1631   score: 125.0  epsilon: 1.0    steps: 543  evaluation reward: 208.25\n",
      "episode: 1632   score: 110.0  epsilon: 1.0    steps: 958  evaluation reward: 208.6\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3946: Policy loss: 2.310907. Value loss: 16.751640. Entropy: 0.149479.\n",
      "Iteration 3947: Policy loss: 2.346792. Value loss: 11.263259. Entropy: 0.145722.\n",
      "Iteration 3948: Policy loss: 2.302150. Value loss: 10.600968. Entropy: 0.147796.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3949: Policy loss: -1.648022. Value loss: 41.666622. Entropy: 0.129385.\n",
      "Iteration 3950: Policy loss: -1.683820. Value loss: 31.839279. Entropy: 0.139199.\n",
      "Iteration 3951: Policy loss: -1.624385. Value loss: 24.362543. Entropy: 0.137148.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3952: Policy loss: -0.887297. Value loss: 40.823978. Entropy: 0.115070.\n",
      "Iteration 3953: Policy loss: -0.883693. Value loss: 27.594193. Entropy: 0.074280.\n",
      "Iteration 3954: Policy loss: -0.971265. Value loss: 22.497408. Entropy: 0.090220.\n",
      "episode: 1633   score: 105.0  epsilon: 1.0    steps: 101  evaluation reward: 208.6\n",
      "episode: 1634   score: 80.0  epsilon: 1.0    steps: 496  evaluation reward: 206.65\n",
      "episode: 1635   score: 530.0  epsilon: 1.0    steps: 725  evaluation reward: 209.3\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3955: Policy loss: 2.697236. Value loss: 30.571947. Entropy: 0.109397.\n",
      "Iteration 3956: Policy loss: 2.847620. Value loss: 18.051216. Entropy: 0.118830.\n",
      "Iteration 3957: Policy loss: 2.492115. Value loss: 13.098247. Entropy: 0.116770.\n",
      "episode: 1636   score: 130.0  epsilon: 1.0    steps: 536  evaluation reward: 209.45\n",
      "episode: 1637   score: 235.0  epsilon: 1.0    steps: 788  evaluation reward: 210.75\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3958: Policy loss: -0.261241. Value loss: 38.512814. Entropy: 0.147291.\n",
      "Iteration 3959: Policy loss: -0.263134. Value loss: 23.980772. Entropy: 0.150236.\n",
      "Iteration 3960: Policy loss: -0.319780. Value loss: 20.257504. Entropy: 0.152590.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3961: Policy loss: 4.516238. Value loss: 36.337723. Entropy: 0.039218.\n",
      "Iteration 3962: Policy loss: 4.254220. Value loss: 23.641184. Entropy: 0.057481.\n",
      "Iteration 3963: Policy loss: 4.373477. Value loss: 20.380495. Entropy: 0.046154.\n",
      "episode: 1638   score: 75.0  epsilon: 1.0    steps: 108  evaluation reward: 210.15\n",
      "episode: 1639   score: 105.0  epsilon: 1.0    steps: 332  evaluation reward: 210.2\n",
      "episode: 1640   score: 125.0  epsilon: 1.0    steps: 476  evaluation reward: 205.8\n",
      "episode: 1641   score: 270.0  epsilon: 1.0    steps: 1001  evaluation reward: 205.1\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3964: Policy loss: 2.370114. Value loss: 27.155924. Entropy: 0.108267.\n",
      "Iteration 3965: Policy loss: 2.296633. Value loss: 15.712137. Entropy: 0.137545.\n",
      "Iteration 3966: Policy loss: 2.493664. Value loss: 14.575300. Entropy: 0.145772.\n",
      "episode: 1642   score: 120.0  epsilon: 1.0    steps: 789  evaluation reward: 204.15\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3967: Policy loss: -0.780998. Value loss: 22.509153. Entropy: 0.244507.\n",
      "Iteration 3968: Policy loss: -0.491785. Value loss: 14.546988. Entropy: 0.251521.\n",
      "Iteration 3969: Policy loss: -0.648500. Value loss: 13.696859. Entropy: 0.259421.\n",
      "episode: 1643   score: 125.0  epsilon: 1.0    steps: 574  evaluation reward: 204.8\n",
      "episode: 1644   score: 205.0  epsilon: 1.0    steps: 732  evaluation reward: 205.05\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3970: Policy loss: 3.300261. Value loss: 32.100883. Entropy: 0.142464.\n",
      "Iteration 3971: Policy loss: 3.334721. Value loss: 21.156536. Entropy: 0.146255.\n",
      "Iteration 3972: Policy loss: 3.256356. Value loss: 16.499069. Entropy: 0.136780.\n",
      "episode: 1645   score: 80.0  epsilon: 1.0    steps: 113  evaluation reward: 202.2\n",
      "episode: 1646   score: 85.0  epsilon: 1.0    steps: 471  evaluation reward: 200.95\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3973: Policy loss: -0.378225. Value loss: 15.565280. Entropy: 0.218492.\n",
      "Iteration 3974: Policy loss: -0.274019. Value loss: 10.873150. Entropy: 0.217025.\n",
      "Iteration 3975: Policy loss: -0.175441. Value loss: 9.854149. Entropy: 0.212276.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3976: Policy loss: -0.976848. Value loss: 21.508921. Entropy: 0.190909.\n",
      "Iteration 3977: Policy loss: -1.112105. Value loss: 16.389952. Entropy: 0.173939.\n",
      "Iteration 3978: Policy loss: -1.059080. Value loss: 13.971565. Entropy: 0.180308.\n",
      "episode: 1647   score: 190.0  epsilon: 1.0    steps: 978  evaluation reward: 200.75\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3979: Policy loss: -1.040712. Value loss: 19.948614. Entropy: 0.194233.\n",
      "Iteration 3980: Policy loss: -0.854828. Value loss: 12.364133. Entropy: 0.195018.\n",
      "Iteration 3981: Policy loss: -0.951436. Value loss: 11.901212. Entropy: 0.202835.\n",
      "episode: 1648   score: 205.0  epsilon: 1.0    steps: 365  evaluation reward: 200.25\n",
      "episode: 1649   score: 215.0  epsilon: 1.0    steps: 786  evaluation reward: 198.6\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3982: Policy loss: 1.694865. Value loss: 9.898666. Entropy: 0.099958.\n",
      "Iteration 3983: Policy loss: 1.469918. Value loss: 8.100493. Entropy: 0.100589.\n",
      "Iteration 3984: Policy loss: 1.611504. Value loss: 6.002968. Entropy: 0.107432.\n",
      "episode: 1650   score: 245.0  epsilon: 1.0    steps: 202  evaluation reward: 198.2\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3985: Policy loss: 1.103977. Value loss: 15.980651. Entropy: 0.129341.\n",
      "Iteration 3986: Policy loss: 1.409332. Value loss: 11.391403. Entropy: 0.132117.\n",
      "Iteration 3987: Policy loss: 1.261340. Value loss: 9.303239. Entropy: 0.129337.\n",
      "now time :  2019-02-26 13:43:00.735836\n",
      "episode: 1651   score: 150.0  epsilon: 1.0    steps: 520  evaluation reward: 198.35\n",
      "episode: 1652   score: 150.0  epsilon: 1.0    steps: 678  evaluation reward: 198.85\n",
      "episode: 1653   score: 55.0  epsilon: 1.0    steps: 991  evaluation reward: 197.85\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3988: Policy loss: 0.909601. Value loss: 20.486734. Entropy: 0.190731.\n",
      "Iteration 3989: Policy loss: 0.871298. Value loss: 17.482483. Entropy: 0.206652.\n",
      "Iteration 3990: Policy loss: 1.070029. Value loss: 13.785899. Entropy: 0.215711.\n",
      "episode: 1654   score: 180.0  epsilon: 1.0    steps: 28  evaluation reward: 198.85\n",
      "episode: 1655   score: 55.0  epsilon: 1.0    steps: 377  evaluation reward: 198.35\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3991: Policy loss: 1.348978. Value loss: 16.699263. Entropy: 0.103256.\n",
      "Iteration 3992: Policy loss: 1.337800. Value loss: 11.489431. Entropy: 0.110046.\n",
      "Iteration 3993: Policy loss: 1.290651. Value loss: 10.597149. Entropy: 0.099556.\n",
      "episode: 1656   score: 170.0  epsilon: 1.0    steps: 884  evaluation reward: 198.25\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3994: Policy loss: -0.657299. Value loss: 14.600697. Entropy: 0.098490.\n",
      "Iteration 3995: Policy loss: -0.810296. Value loss: 11.941917. Entropy: 0.094333.\n",
      "Iteration 3996: Policy loss: -0.528867. Value loss: 10.954422. Entropy: 0.093534.\n",
      "episode: 1657   score: 240.0  epsilon: 1.0    steps: 429  evaluation reward: 198.55\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3997: Policy loss: -0.043885. Value loss: 9.197588. Entropy: 0.099065.\n",
      "Iteration 3998: Policy loss: 0.013400. Value loss: 6.669828. Entropy: 0.101308.\n",
      "Iteration 3999: Policy loss: -0.153270. Value loss: 6.031758. Entropy: 0.105587.\n",
      "episode: 1658   score: 180.0  epsilon: 1.0    steps: 220  evaluation reward: 198.25\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 4000: Policy loss: -0.062985. Value loss: 20.691957. Entropy: 0.081062.\n",
      "Iteration 4001: Policy loss: -0.145771. Value loss: 14.021708. Entropy: 0.108510.\n",
      "Iteration 4002: Policy loss: -0.283246. Value loss: 11.183314. Entropy: 0.091469.\n",
      "episode: 1659   score: 155.0  epsilon: 1.0    steps: 553  evaluation reward: 197.7\n",
      "episode: 1660   score: 230.0  epsilon: 1.0    steps: 992  evaluation reward: 195.1\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4003: Policy loss: -1.579652. Value loss: 15.346019. Entropy: 0.118842.\n",
      "Iteration 4004: Policy loss: -1.584641. Value loss: 9.871970. Entropy: 0.126686.\n",
      "Iteration 4005: Policy loss: -1.495001. Value loss: 7.915516. Entropy: 0.123120.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4006: Policy loss: 0.098082. Value loss: 21.105568. Entropy: 0.206790.\n",
      "Iteration 4007: Policy loss: 0.106390. Value loss: 11.952027. Entropy: 0.212571.\n",
      "Iteration 4008: Policy loss: 0.142663. Value loss: 9.904394. Entropy: 0.218657.\n",
      "episode: 1661   score: 230.0  epsilon: 1.0    steps: 89  evaluation reward: 195.0\n",
      "episode: 1662   score: 310.0  epsilon: 1.0    steps: 700  evaluation reward: 192.75\n",
      "episode: 1663   score: 180.0  epsilon: 1.0    steps: 895  evaluation reward: 192.25\n",
      "Training network. lr: 0.000219. clip: 0.087715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4009: Policy loss: -0.434667. Value loss: 19.148506. Entropy: 0.125453.\n",
      "Iteration 4010: Policy loss: -0.777641. Value loss: 14.365764. Entropy: 0.112538.\n",
      "Iteration 4011: Policy loss: -0.660092. Value loss: 11.808970. Entropy: 0.126076.\n",
      "episode: 1664   score: 165.0  epsilon: 1.0    steps: 470  evaluation reward: 193.0\n",
      "episode: 1665   score: 105.0  epsilon: 1.0    steps: 599  evaluation reward: 192.25\n",
      "episode: 1666   score: 65.0  epsilon: 1.0    steps: 1010  evaluation reward: 189.95\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4012: Policy loss: 1.600743. Value loss: 17.493425. Entropy: 0.378607.\n",
      "Iteration 4013: Policy loss: 1.545107. Value loss: 11.380888. Entropy: 0.486691.\n",
      "Iteration 4014: Policy loss: 1.607961. Value loss: 9.041997. Entropy: 0.476556.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4015: Policy loss: 0.772930. Value loss: 26.121996. Entropy: 0.483861.\n",
      "Iteration 4016: Policy loss: 0.925399. Value loss: 13.924390. Entropy: 0.488540.\n",
      "Iteration 4017: Policy loss: 0.959340. Value loss: 12.081025. Entropy: 0.495323.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4018: Policy loss: 0.573584. Value loss: 37.433918. Entropy: 0.477509.\n",
      "Iteration 4019: Policy loss: 0.549158. Value loss: 17.393692. Entropy: 0.485681.\n",
      "Iteration 4020: Policy loss: 0.777917. Value loss: 14.443348. Entropy: 0.485655.\n",
      "episode: 1667   score: 160.0  epsilon: 1.0    steps: 181  evaluation reward: 189.75\n",
      "episode: 1668   score: 540.0  epsilon: 1.0    steps: 304  evaluation reward: 194.0\n",
      "episode: 1669   score: 75.0  epsilon: 1.0    steps: 777  evaluation reward: 193.1\n",
      "episode: 1670   score: 80.0  epsilon: 1.0    steps: 1008  evaluation reward: 191.85\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4021: Policy loss: -1.466635. Value loss: 211.088760. Entropy: 0.370058.\n",
      "Iteration 4022: Policy loss: -1.108635. Value loss: 179.447174. Entropy: 0.298174.\n",
      "Iteration 4023: Policy loss: -0.734170. Value loss: 97.208885. Entropy: 0.320569.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4024: Policy loss: 0.874259. Value loss: 30.767035. Entropy: 0.249885.\n",
      "Iteration 4025: Policy loss: 0.931549. Value loss: 17.369059. Entropy: 0.242132.\n",
      "Iteration 4026: Policy loss: 1.036668. Value loss: 13.775041. Entropy: 0.262179.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4027: Policy loss: -1.004004. Value loss: 32.903690. Entropy: 0.258833.\n",
      "Iteration 4028: Policy loss: -0.920587. Value loss: 22.145475. Entropy: 0.264907.\n",
      "Iteration 4029: Policy loss: -0.832284. Value loss: 20.396717. Entropy: 0.289006.\n",
      "episode: 1671   score: 210.0  epsilon: 1.0    steps: 642  evaluation reward: 193.3\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4030: Policy loss: -0.896833. Value loss: 16.805656. Entropy: 0.185153.\n",
      "Iteration 4031: Policy loss: -1.121268. Value loss: 10.217271. Entropy: 0.186467.\n",
      "Iteration 4032: Policy loss: -0.950603. Value loss: 8.240939. Entropy: 0.195731.\n",
      "episode: 1672   score: 270.0  epsilon: 1.0    steps: 48  evaluation reward: 195.2\n",
      "episode: 1673   score: 255.0  epsilon: 1.0    steps: 418  evaluation reward: 196.4\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4033: Policy loss: -0.233743. Value loss: 13.816914. Entropy: 0.133577.\n",
      "Iteration 4034: Policy loss: -0.258548. Value loss: 9.853316. Entropy: 0.150830.\n",
      "Iteration 4035: Policy loss: -0.215423. Value loss: 6.851953. Entropy: 0.144075.\n",
      "episode: 1674   score: 150.0  epsilon: 1.0    steps: 308  evaluation reward: 195.65\n",
      "episode: 1675   score: 260.0  epsilon: 1.0    steps: 560  evaluation reward: 196.45\n",
      "episode: 1676   score: 185.0  epsilon: 1.0    steps: 1009  evaluation reward: 197.4\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4036: Policy loss: 0.084563. Value loss: 24.808832. Entropy: 0.339294.\n",
      "Iteration 4037: Policy loss: 0.254412. Value loss: 14.970001. Entropy: 0.335943.\n",
      "Iteration 4038: Policy loss: 0.178719. Value loss: 12.814497. Entropy: 0.332747.\n",
      "episode: 1677   score: 220.0  epsilon: 1.0    steps: 860  evaluation reward: 197.8\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4039: Policy loss: -0.485244. Value loss: 29.211594. Entropy: 0.413401.\n",
      "Iteration 4040: Policy loss: -0.334098. Value loss: 20.611492. Entropy: 0.415525.\n",
      "Iteration 4041: Policy loss: -0.403463. Value loss: 16.676458. Entropy: 0.413168.\n",
      "episode: 1678   score: 245.0  epsilon: 1.0    steps: 164  evaluation reward: 198.1\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4042: Policy loss: 0.518147. Value loss: 11.666934. Entropy: 0.384333.\n",
      "Iteration 4043: Policy loss: 0.468504. Value loss: 7.889954. Entropy: 0.356627.\n",
      "Iteration 4044: Policy loss: 0.484449. Value loss: 7.901981. Entropy: 0.380581.\n",
      "episode: 1679   score: 35.0  epsilon: 1.0    steps: 289  evaluation reward: 196.4\n",
      "episode: 1680   score: 170.0  epsilon: 1.0    steps: 650  evaluation reward: 196.15\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4045: Policy loss: -1.137310. Value loss: 30.816635. Entropy: 0.358562.\n",
      "Iteration 4046: Policy loss: -0.755460. Value loss: 20.868696. Entropy: 0.369159.\n",
      "Iteration 4047: Policy loss: -1.050173. Value loss: 16.922255. Entropy: 0.376519.\n",
      "episode: 1681   score: 115.0  epsilon: 1.0    steps: 918  evaluation reward: 196.25\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4048: Policy loss: -0.392355. Value loss: 15.916312. Entropy: 0.292549.\n",
      "Iteration 4049: Policy loss: -0.135335. Value loss: 10.569831. Entropy: 0.293174.\n",
      "Iteration 4050: Policy loss: -0.210986. Value loss: 10.218188. Entropy: 0.288937.\n",
      "episode: 1682   score: 235.0  epsilon: 1.0    steps: 95  evaluation reward: 193.1\n",
      "episode: 1683   score: 40.0  epsilon: 1.0    steps: 363  evaluation reward: 190.7\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4051: Policy loss: -3.176360. Value loss: 307.772339. Entropy: 0.291786.\n",
      "Iteration 4052: Policy loss: -3.454254. Value loss: 228.012070. Entropy: 0.202206.\n",
      "Iteration 4053: Policy loss: -2.676402. Value loss: 139.008682. Entropy: 0.169626.\n",
      "episode: 1684   score: 200.0  epsilon: 1.0    steps: 400  evaluation reward: 191.4\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4054: Policy loss: -1.278676. Value loss: 34.028534. Entropy: 0.172898.\n",
      "Iteration 4055: Policy loss: -1.257799. Value loss: 22.914486. Entropy: 0.138324.\n",
      "Iteration 4056: Policy loss: -1.251902. Value loss: 16.748411. Entropy: 0.169045.\n",
      "episode: 1685   score: 185.0  epsilon: 1.0    steps: 156  evaluation reward: 190.1\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4057: Policy loss: -0.238137. Value loss: 22.273531. Entropy: 0.184393.\n",
      "Iteration 4058: Policy loss: -0.539059. Value loss: 13.175641. Entropy: 0.117509.\n",
      "Iteration 4059: Policy loss: -0.412841. Value loss: 11.561887. Entropy: 0.155521.\n",
      "episode: 1686   score: 400.0  epsilon: 1.0    steps: 534  evaluation reward: 193.0\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4060: Policy loss: 2.803884. Value loss: 23.221083. Entropy: 0.244299.\n",
      "Iteration 4061: Policy loss: 2.104302. Value loss: 12.962735. Entropy: 0.274621.\n",
      "Iteration 4062: Policy loss: 2.492226. Value loss: 10.215474. Entropy: 0.322538.\n",
      "episode: 1687   score: 110.0  epsilon: 1.0    steps: 909  evaluation reward: 193.05\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4063: Policy loss: 0.496249. Value loss: 22.548819. Entropy: 0.179997.\n",
      "Iteration 4064: Policy loss: 0.586435. Value loss: 13.980612. Entropy: 0.186638.\n",
      "Iteration 4065: Policy loss: 0.350438. Value loss: 11.759841. Entropy: 0.223377.\n",
      "episode: 1688   score: 80.0  epsilon: 1.0    steps: 169  evaluation reward: 191.8\n",
      "episode: 1689   score: 180.0  epsilon: 1.0    steps: 373  evaluation reward: 192.1\n",
      "episode: 1690   score: 260.0  epsilon: 1.0    steps: 727  evaluation reward: 193.5\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4066: Policy loss: -0.723050. Value loss: 19.546167. Entropy: 0.283475.\n",
      "Iteration 4067: Policy loss: -0.813376. Value loss: 13.503941. Entropy: 0.297703.\n",
      "Iteration 4068: Policy loss: -0.715940. Value loss: 12.279913. Entropy: 0.280752.\n",
      "episode: 1691   score: 105.0  epsilon: 1.0    steps: 564  evaluation reward: 192.7\n",
      "episode: 1692   score: 505.0  epsilon: 1.0    steps: 827  evaluation reward: 196.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4069: Policy loss: -0.693861. Value loss: 19.899605. Entropy: 0.327034.\n",
      "Iteration 4070: Policy loss: -0.633094. Value loss: 12.154847. Entropy: 0.330613.\n",
      "Iteration 4071: Policy loss: -0.638956. Value loss: 8.152069. Entropy: 0.322681.\n",
      "episode: 1693   score: 210.0  epsilon: 1.0    steps: 56  evaluation reward: 197.05\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4072: Policy loss: 2.242862. Value loss: 25.694790. Entropy: 0.291980.\n",
      "Iteration 4073: Policy loss: 2.373662. Value loss: 18.971207. Entropy: 0.273437.\n",
      "Iteration 4074: Policy loss: 2.320723. Value loss: 15.911024. Entropy: 0.293576.\n",
      "episode: 1694   score: 285.0  epsilon: 1.0    steps: 395  evaluation reward: 196.2\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4075: Policy loss: -0.045041. Value loss: 26.658865. Entropy: 0.430292.\n",
      "Iteration 4076: Policy loss: 0.064225. Value loss: 21.375977. Entropy: 0.445840.\n",
      "Iteration 4077: Policy loss: -0.022159. Value loss: 17.501247. Entropy: 0.424406.\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4078: Policy loss: 0.678527. Value loss: 25.272221. Entropy: 0.258461.\n",
      "Iteration 4079: Policy loss: 0.955051. Value loss: 16.469404. Entropy: 0.332216.\n",
      "Iteration 4080: Policy loss: 0.828188. Value loss: 14.519842. Entropy: 0.265748.\n",
      "episode: 1695   score: 105.0  epsilon: 1.0    steps: 87  evaluation reward: 195.4\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4081: Policy loss: 1.530859. Value loss: 33.012558. Entropy: 0.369655.\n",
      "Iteration 4082: Policy loss: 1.635957. Value loss: 19.212845. Entropy: 0.360580.\n",
      "Iteration 4083: Policy loss: 1.512908. Value loss: 14.169415. Entropy: 0.344087.\n",
      "episode: 1696   score: 130.0  epsilon: 1.0    steps: 283  evaluation reward: 190.6\n",
      "episode: 1697   score: 105.0  epsilon: 1.0    steps: 571  evaluation reward: 191.15\n"
     ]
    }
   ],
   "source": [
    "vis_env_idx = 0\n",
    "vis_env = envs[vis_env_idx]\n",
    "e = 0\n",
    "frame = 0\n",
    "max_eval = -np.inf\n",
    "reset_count = 0\n",
    "\n",
    "while (frame < 10000000):\n",
    "    step = 0\n",
    "    assert(num_envs * env_mem_size == train_frame)\n",
    "    frame_next_vals = []\n",
    "    for i in range(num_envs):\n",
    "        env = envs[i]\n",
    "        #history = env.history\n",
    "        #life = env.life\n",
    "        #state, reward, done, info = [env.state, env.reward, env.done, env.info]\n",
    "        for j in range(env_mem_size):\n",
    "            step += 1\n",
    "            frame += 1\n",
    "            \n",
    "            curr_state = env.history[HISTORY_SIZE-1,:,:]\n",
    "            action, value = agent.get_action(np.float32(env.history[:HISTORY_SIZE,:,:]) / 255.)\n",
    "            \n",
    "            next_state, env.reward, env.done, env.info = env.step(action)\n",
    "            \n",
    "            if (i == vis_env_idx):\n",
    "                vis_env._env.render()\n",
    "            \n",
    "            frame_next_state = get_frame(next_state)\n",
    "            env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "            terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "            \n",
    "            env.life = env.info['ale.lives']\n",
    "            r = env.reward\n",
    "            \n",
    "            agent.memory.push(i, deepcopy(curr_state), action, r, terminal_state, value, 0, 0)\n",
    "            if (j == env_mem_size-1):\n",
    "                _, frame_next_val = agent.get_action(np.float32(env.history[1:,:,:]) / 255.)\n",
    "                frame_next_vals.append(frame_next_val)\n",
    "            env.score += r\n",
    "            env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "            \n",
    "            if (env.done):\n",
    "                if (e % 50 == 0):\n",
    "                    print('now time : ', datetime.now())\n",
    "                    rewards.append(np.mean(evaluation_reward))\n",
    "                    episodes.append(e)\n",
    "                    pylab.plot(episodes, rewards, 'b')\n",
    "                    pylab.savefig(\"./save_graph/spaceinvaders_ppo.png\")\n",
    "                    torch.save(agent.policy_net, \"./save_model/spaceinvaders_ppo\")\n",
    "                    \n",
    "                    if np.mean(evaluation_reward) > max_eval:\n",
    "                        torch.save(agent.policy_net.state_dict(), \"./save_model/spaceinvaders_ppo_best\")\n",
    "                        max_eval = float(np.mean(evaluation_reward))\n",
    "                        reset_count = 0\n",
    "                    elif e > 5000:\n",
    "                        reset_count += 1\n",
    "                        \"\"\"\n",
    "                        if (reset_count == reset_max):\n",
    "                            print(\"Training went nowhere, starting again at best model\")\n",
    "                            agent.policy_net.load_state_dict(torch.load(\"./save_model/spaceinvaders_ppo_best\"))\n",
    "                            agent.update_target_net()\n",
    "                            reset_count = 0\n",
    "                        \"\"\"\n",
    "                e += 1\n",
    "                evaluation_reward.append(env.score)\n",
    "                print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "                \n",
    "                env.done = False\n",
    "                env.score = 0\n",
    "                env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "                env.state = env.reset()\n",
    "                env.life = number_lives\n",
    "                get_init_state(env.history, env.state)\n",
    "                \n",
    "                \n",
    "                \n",
    "    agent.train_policy_net(frame, frame_next_vals)\n",
    "    agent.update_target_net()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/spaceinvaders_ppo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],1)[0].unsqueeze(1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000250. clip: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:255: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:256: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:257: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Policy loss: -0.134242. Value loss: 0.025672. Entropy: 1.384760.\n",
      "Iteration 2: Policy loss: -0.134622. Value loss: 0.024417. Entropy: 1.382654.\n",
      "Iteration 3: Policy loss: -0.138663. Value loss: 0.025462. Entropy: 1.380519.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 4: Policy loss: -0.166682. Value loss: 0.051994. Entropy: 1.374561.\n",
      "Iteration 5: Policy loss: -0.153367. Value loss: 0.041330. Entropy: 1.375694.\n",
      "Iteration 6: Policy loss: -0.167738. Value loss: 0.047337. Entropy: 1.376785.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 7: Policy loss: -0.256462. Value loss: 0.727763. Entropy: 1.370962.\n",
      "Iteration 8: Policy loss: -0.249125. Value loss: 0.479165. Entropy: 1.374830.\n",
      "Iteration 9: Policy loss: -0.237247. Value loss: 0.508010. Entropy: 1.382150.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 10: Policy loss: -0.563716. Value loss: 2.374101. Entropy: 1.376569.\n",
      "Iteration 11: Policy loss: -0.543814. Value loss: 2.030069. Entropy: 1.379307.\n",
      "Iteration 12: Policy loss: -0.566954. Value loss: 1.876679. Entropy: 1.380044.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 13: Policy loss: -0.337421. Value loss: 2.447332. Entropy: 1.373016.\n",
      "Iteration 14: Policy loss: -0.408392. Value loss: 2.332986. Entropy: 1.377833.\n",
      "Iteration 15: Policy loss: -0.392529. Value loss: 2.149341. Entropy: 1.374973.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 16: Policy loss: -0.218130. Value loss: 0.989801. Entropy: 1.376110.\n",
      "Iteration 17: Policy loss: -0.232407. Value loss: 0.903971. Entropy: 1.371503.\n",
      "Iteration 18: Policy loss: -0.263298. Value loss: 0.835594. Entropy: 1.373009.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 19: Policy loss: -0.005378. Value loss: 0.680149. Entropy: 1.376527.\n",
      "Iteration 20: Policy loss: 0.007001. Value loss: 0.388243. Entropy: 1.372749.\n",
      "Iteration 21: Policy loss: -0.001339. Value loss: 0.428446. Entropy: 1.374038.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 22: Policy loss: -0.044410. Value loss: 0.574210. Entropy: 1.369309.\n",
      "Iteration 23: Policy loss: -0.104588. Value loss: 0.804823. Entropy: 1.365573.\n",
      "Iteration 24: Policy loss: -0.121444. Value loss: 0.816057. Entropy: 1.365613.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 25: Policy loss: -0.364618. Value loss: 2.418900. Entropy: 1.371554.\n",
      "Iteration 26: Policy loss: -0.382875. Value loss: 2.166423. Entropy: 1.369383.\n",
      "Iteration 27: Policy loss: -0.418865. Value loss: 2.527520. Entropy: 1.369931.\n",
      "now time :  2019-02-26 18:28:38.548289\n",
      "episode: 1   score: 7700.0  epsilon: 1.0    steps: 348  evaluation reward: 7700.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 28: Policy loss: -0.035380. Value loss: 1.883276. Entropy: 1.367360.\n",
      "Iteration 29: Policy loss: -0.036834. Value loss: 1.759964. Entropy: 1.370840.\n",
      "Iteration 30: Policy loss: -0.036598. Value loss: 1.746900. Entropy: 1.371950.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 31: Policy loss: -0.058636. Value loss: 1.006419. Entropy: 1.369001.\n",
      "Iteration 32: Policy loss: -0.033320. Value loss: 0.862457. Entropy: 1.364149.\n",
      "Iteration 33: Policy loss: -0.065974. Value loss: 0.884775. Entropy: 1.361571.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 34: Policy loss: 0.000143. Value loss: 1.397101. Entropy: 1.370030.\n",
      "Iteration 35: Policy loss: -0.000328. Value loss: 1.279787. Entropy: 1.359337.\n",
      "Iteration 36: Policy loss: -0.010333. Value loss: 1.356722. Entropy: 1.366242.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 37: Policy loss: 0.151668. Value loss: 0.326707. Entropy: 1.362033.\n",
      "Iteration 38: Policy loss: 0.120222. Value loss: 0.633723. Entropy: 1.359322.\n",
      "Iteration 39: Policy loss: 0.096740. Value loss: 0.657801. Entropy: 1.369568.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 40: Policy loss: 0.263479. Value loss: 0.925697. Entropy: 1.371442.\n",
      "Iteration 41: Policy loss: 0.242011. Value loss: 1.152072. Entropy: 1.373991.\n",
      "Iteration 42: Policy loss: 0.275679. Value loss: 0.905793. Entropy: 1.370134.\n",
      "episode: 2   score: 19700.0  epsilon: 1.0    steps: 213  evaluation reward: 13700.0\n",
      "episode: 3   score: 19800.0  epsilon: 1.0    steps: 747  evaluation reward: 15733.333333333334\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 43: Policy loss: -0.206675. Value loss: 2.724686. Entropy: 1.375890.\n",
      "Iteration 44: Policy loss: -0.177189. Value loss: 2.282678. Entropy: 1.359556.\n",
      "Iteration 45: Policy loss: -0.208517. Value loss: 2.374061. Entropy: 1.365482.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 46: Policy loss: -0.052535. Value loss: 0.860957. Entropy: 1.364740.\n",
      "Iteration 47: Policy loss: -0.044913. Value loss: 0.771527. Entropy: 1.358306.\n",
      "Iteration 48: Policy loss: -0.064552. Value loss: 0.630981. Entropy: 1.362592.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 49: Policy loss: 0.033956. Value loss: 0.702614. Entropy: 1.351950.\n",
      "Iteration 50: Policy loss: 0.002342. Value loss: 0.834315. Entropy: 1.352887.\n",
      "Iteration 51: Policy loss: -0.047984. Value loss: 0.951180. Entropy: 1.353769.\n",
      "episode: 4   score: 17500.0  epsilon: 1.0    steps: 463  evaluation reward: 16175.0\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 52: Policy loss: 0.102354. Value loss: 1.262624. Entropy: 1.351104.\n",
      "Iteration 53: Policy loss: 0.082071. Value loss: 1.185876. Entropy: 1.344647.\n",
      "Iteration 54: Policy loss: 0.086521. Value loss: 1.046437. Entropy: 1.343283.\n",
      "episode: 5   score: 27200.0  epsilon: 1.0    steps: 916  evaluation reward: 18380.0\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 55: Policy loss: -0.275639. Value loss: 1.830601. Entropy: 1.338831.\n",
      "Iteration 56: Policy loss: -0.222343. Value loss: 1.727149. Entropy: 1.325506.\n",
      "Iteration 57: Policy loss: -0.302193. Value loss: 1.473941. Entropy: 1.328483.\n",
      "episode: 6   score: 22100.0  epsilon: 1.0    steps: 523  evaluation reward: 19000.0\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 58: Policy loss: 0.346007. Value loss: 0.567518. Entropy: 1.312028.\n",
      "Iteration 59: Policy loss: 0.345520. Value loss: 0.450806. Entropy: 1.322274.\n",
      "Iteration 60: Policy loss: 0.354244. Value loss: 0.410623. Entropy: 1.321767.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 61: Policy loss: 0.147363. Value loss: 0.635574. Entropy: 1.317868.\n",
      "Iteration 62: Policy loss: 0.140720. Value loss: 0.693985. Entropy: 1.332772.\n",
      "Iteration 63: Policy loss: 0.129345. Value loss: 0.602723. Entropy: 1.319486.\n",
      "episode: 7   score: 31700.0  epsilon: 1.0    steps: 798  evaluation reward: 20814.285714285714\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 64: Policy loss: -0.188279. Value loss: 1.415527. Entropy: 1.325181.\n",
      "Iteration 65: Policy loss: -0.194193. Value loss: 1.211572. Entropy: 1.332697.\n",
      "Iteration 66: Policy loss: -0.211223. Value loss: 1.177130. Entropy: 1.329612.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 67: Policy loss: -0.123286. Value loss: 1.883530. Entropy: 1.343618.\n",
      "Iteration 68: Policy loss: -0.071217. Value loss: 1.229707. Entropy: 1.332411.\n",
      "Iteration 69: Policy loss: -0.117177. Value loss: 1.205740. Entropy: 1.326370.\n",
      "episode: 8   score: 39400.0  epsilon: 1.0    steps: 78  evaluation reward: 23137.5\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 70: Policy loss: 0.189820. Value loss: 1.153423. Entropy: 1.328754.\n",
      "Iteration 71: Policy loss: 0.305144. Value loss: 0.638589. Entropy: 1.317543.\n",
      "Iteration 72: Policy loss: 0.260406. Value loss: 0.648923. Entropy: 1.315011.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 73: Policy loss: -0.323597. Value loss: 2.410517. Entropy: 1.298001.\n",
      "Iteration 74: Policy loss: -0.232053. Value loss: 1.877670. Entropy: 1.306429.\n",
      "Iteration 75: Policy loss: -0.268054. Value loss: 2.078341. Entropy: 1.308005.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 76: Policy loss: 0.267789. Value loss: 0.534690. Entropy: 1.290469.\n",
      "Iteration 77: Policy loss: 0.283844. Value loss: 0.388711. Entropy: 1.292114.\n",
      "Iteration 78: Policy loss: 0.251278. Value loss: 0.336781. Entropy: 1.297447.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 79: Policy loss: -0.115699. Value loss: 1.857984. Entropy: 1.274607.\n",
      "Iteration 80: Policy loss: -0.017624. Value loss: 1.334265. Entropy: 1.286561.\n",
      "Iteration 81: Policy loss: -0.066483. Value loss: 1.052274. Entropy: 1.283230.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 82: Policy loss: 0.089969. Value loss: 0.966442. Entropy: 1.284803.\n",
      "Iteration 83: Policy loss: 0.065274. Value loss: 0.917917. Entropy: 1.281547.\n",
      "Iteration 84: Policy loss: 0.105854. Value loss: 0.750232. Entropy: 1.285162.\n",
      "episode: 9   score: 21800.0  epsilon: 1.0    steps: 378  evaluation reward: 22988.88888888889\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 85: Policy loss: 0.268218. Value loss: 0.913028. Entropy: 1.285013.\n",
      "Iteration 86: Policy loss: 0.241994. Value loss: 0.777983. Entropy: 1.287925.\n",
      "Iteration 87: Policy loss: 0.234302. Value loss: 0.738064. Entropy: 1.279687.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 88: Policy loss: 0.048712. Value loss: 0.858994. Entropy: 1.303121.\n",
      "Iteration 89: Policy loss: 0.044057. Value loss: 0.564432. Entropy: 1.307566.\n",
      "Iteration 90: Policy loss: 0.040870. Value loss: 0.549256. Entropy: 1.304551.\n",
      "episode: 10   score: 22800.0  epsilon: 1.0    steps: 153  evaluation reward: 22970.0\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 91: Policy loss: 0.005436. Value loss: 0.615767. Entropy: 1.287207.\n",
      "Iteration 92: Policy loss: 0.017937. Value loss: 0.448422. Entropy: 1.304350.\n",
      "Iteration 93: Policy loss: -0.012785. Value loss: 0.461256. Entropy: 1.299510.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 94: Policy loss: -0.016429. Value loss: 1.256985. Entropy: 1.307291.\n",
      "Iteration 95: Policy loss: -0.009922. Value loss: 1.074603. Entropy: 1.296330.\n",
      "Iteration 96: Policy loss: 0.003630. Value loss: 0.796453. Entropy: 1.293783.\n",
      "episode: 11   score: 14700.0  epsilon: 1.0    steps: 589  evaluation reward: 22218.18181818182\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 97: Policy loss: 0.282472. Value loss: 0.724878. Entropy: 1.307948.\n",
      "Iteration 98: Policy loss: 0.253145. Value loss: 0.803634. Entropy: 1.328351.\n",
      "Iteration 99: Policy loss: 0.272439. Value loss: 0.551466. Entropy: 1.315614.\n",
      "episode: 12   score: 28200.0  epsilon: 1.0    steps: 642  evaluation reward: 22716.666666666668\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 100: Policy loss: -0.234213. Value loss: 2.052508. Entropy: 1.311731.\n",
      "Iteration 101: Policy loss: -0.212712. Value loss: 1.824195. Entropy: 1.297956.\n",
      "Iteration 102: Policy loss: -0.263188. Value loss: 1.797038. Entropy: 1.297062.\n",
      "episode: 13   score: 22400.0  epsilon: 1.0    steps: 392  evaluation reward: 22692.30769230769\n",
      "episode: 14   score: 18000.0  epsilon: 1.0    steps: 965  evaluation reward: 22357.14285714286\n",
      "Training network. lr: 0.000249. clip: 0.099696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 103: Policy loss: 0.104650. Value loss: 0.487069. Entropy: 1.310694.\n",
      "Iteration 104: Policy loss: 0.121857. Value loss: 0.390418. Entropy: 1.308963.\n",
      "Iteration 105: Policy loss: 0.107936. Value loss: 0.398923. Entropy: 1.303240.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 106: Policy loss: -0.019600. Value loss: 0.810460. Entropy: 1.291222.\n",
      "Iteration 107: Policy loss: -0.043799. Value loss: 0.813314. Entropy: 1.305367.\n",
      "Iteration 108: Policy loss: -0.041227. Value loss: 0.788016. Entropy: 1.295246.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 109: Policy loss: -0.053537. Value loss: 1.527595. Entropy: 1.300061.\n",
      "Iteration 110: Policy loss: -0.172127. Value loss: 1.685091. Entropy: 1.297742.\n",
      "Iteration 111: Policy loss: -0.111701. Value loss: 1.285365. Entropy: 1.288221.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 112: Policy loss: 0.182973. Value loss: 0.846592. Entropy: 1.276863.\n",
      "Iteration 113: Policy loss: 0.123608. Value loss: 0.738449. Entropy: 1.267223.\n",
      "Iteration 114: Policy loss: 0.158275. Value loss: 0.496647. Entropy: 1.272006.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 115: Policy loss: -0.222162. Value loss: 1.810855. Entropy: 1.260025.\n",
      "Iteration 116: Policy loss: -0.275154. Value loss: 1.629239. Entropy: 1.244413.\n",
      "Iteration 117: Policy loss: -0.294521. Value loss: 1.518205. Entropy: 1.250029.\n",
      "episode: 15   score: 17000.0  epsilon: 1.0    steps: 49  evaluation reward: 22000.0\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 118: Policy loss: -0.212441. Value loss: 1.153143. Entropy: 1.249287.\n",
      "Iteration 119: Policy loss: -0.177706. Value loss: 0.827762. Entropy: 1.250613.\n",
      "Iteration 120: Policy loss: -0.169635. Value loss: 0.730123. Entropy: 1.244563.\n",
      "episode: 16   score: 29600.0  epsilon: 1.0    steps: 874  evaluation reward: 22475.0\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 121: Policy loss: 0.159460. Value loss: 1.592253. Entropy: 1.245400.\n",
      "Iteration 122: Policy loss: 0.133137. Value loss: 1.389232. Entropy: 1.230398.\n",
      "Iteration 123: Policy loss: 0.140527. Value loss: 1.197404. Entropy: 1.232581.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 124: Policy loss: 0.131218. Value loss: 0.748518. Entropy: 1.265509.\n",
      "Iteration 125: Policy loss: 0.089064. Value loss: 0.607684. Entropy: 1.264234.\n",
      "Iteration 126: Policy loss: 0.101871. Value loss: 0.513232. Entropy: 1.254126.\n",
      "episode: 17   score: 18900.0  epsilon: 1.0    steps: 148  evaluation reward: 22264.70588235294\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 127: Policy loss: 0.239537. Value loss: 0.539801. Entropy: 1.259595.\n",
      "Iteration 128: Policy loss: 0.216701. Value loss: 0.446758. Entropy: 1.261543.\n",
      "Iteration 129: Policy loss: 0.231914. Value loss: 0.460698. Entropy: 1.259481.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 130: Policy loss: 0.150902. Value loss: 0.649961. Entropy: 1.263702.\n",
      "Iteration 131: Policy loss: 0.151933. Value loss: 0.629757. Entropy: 1.275769.\n",
      "Iteration 132: Policy loss: 0.105372. Value loss: 0.753841. Entropy: 1.270319.\n",
      "episode: 18   score: 16300.0  epsilon: 1.0    steps: 265  evaluation reward: 21933.333333333332\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 133: Policy loss: 0.064728. Value loss: 0.858148. Entropy: 1.287185.\n",
      "Iteration 134: Policy loss: 0.080237. Value loss: 0.636959. Entropy: 1.293762.\n",
      "Iteration 135: Policy loss: 0.055050. Value loss: 0.749933. Entropy: 1.283901.\n",
      "episode: 19   score: 22000.0  epsilon: 1.0    steps: 763  evaluation reward: 21936.842105263157\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 136: Policy loss: -0.105163. Value loss: 1.531454. Entropy: 1.262079.\n",
      "Iteration 137: Policy loss: -0.113426. Value loss: 1.217598. Entropy: 1.256848.\n",
      "Iteration 138: Policy loss: -0.082540. Value loss: 0.992972. Entropy: 1.265413.\n",
      "episode: 20   score: 8200.0  epsilon: 1.0    steps: 90  evaluation reward: 21250.0\n",
      "episode: 21   score: 14400.0  epsilon: 1.0    steps: 470  evaluation reward: 20923.809523809523\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 139: Policy loss: -0.111905. Value loss: 0.928430. Entropy: 1.260661.\n",
      "Iteration 140: Policy loss: -0.132818. Value loss: 0.577418. Entropy: 1.244027.\n",
      "Iteration 141: Policy loss: -0.135003. Value loss: 0.614389. Entropy: 1.234156.\n",
      "episode: 22   score: 16600.0  epsilon: 1.0    steps: 899  evaluation reward: 20727.272727272728\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 142: Policy loss: -0.011835. Value loss: 1.132346. Entropy: 1.269647.\n",
      "Iteration 143: Policy loss: -0.003485. Value loss: 0.977195. Entropy: 1.272933.\n",
      "Iteration 144: Policy loss: 0.000872. Value loss: 0.980062. Entropy: 1.272542.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 145: Policy loss: -0.175239. Value loss: 1.424164. Entropy: 1.266447.\n",
      "Iteration 146: Policy loss: -0.240519. Value loss: 1.326836. Entropy: 1.262999.\n",
      "Iteration 147: Policy loss: -0.191971. Value loss: 1.116179. Entropy: 1.254915.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 148: Policy loss: -0.231361. Value loss: 1.858826. Entropy: 1.270677.\n",
      "Iteration 149: Policy loss: -0.221485. Value loss: 1.679555. Entropy: 1.257463.\n",
      "Iteration 150: Policy loss: -0.246462. Value loss: 1.617842. Entropy: 1.253185.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 151: Policy loss: 0.049834. Value loss: 2.189571. Entropy: 1.248492.\n",
      "Iteration 152: Policy loss: 0.064905. Value loss: 1.882213. Entropy: 1.253928.\n",
      "Iteration 153: Policy loss: -0.003215. Value loss: 1.633047. Entropy: 1.247441.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 154: Policy loss: 0.019725. Value loss: 1.159588. Entropy: 1.228572.\n",
      "Iteration 155: Policy loss: -0.010242. Value loss: 1.096632. Entropy: 1.223235.\n",
      "Iteration 156: Policy loss: -0.022795. Value loss: 0.949814. Entropy: 1.225282.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 157: Policy loss: -0.060399. Value loss: 2.235929. Entropy: 1.194476.\n",
      "Iteration 158: Policy loss: -0.037480. Value loss: 1.746140. Entropy: 1.199817.\n",
      "Iteration 159: Policy loss: -0.074723. Value loss: 1.681425. Entropy: 1.179422.\n",
      "episode: 23   score: 31100.0  epsilon: 1.0    steps: 541  evaluation reward: 21178.260869565216\n",
      "episode: 24   score: 17400.0  epsilon: 1.0    steps: 867  evaluation reward: 21020.833333333332\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 160: Policy loss: 0.249992. Value loss: 0.907723. Entropy: 1.238500.\n",
      "Iteration 161: Policy loss: 0.252128. Value loss: 0.665747. Entropy: 1.248567.\n",
      "Iteration 162: Policy loss: 0.215561. Value loss: 0.561358. Entropy: 1.240099.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 163: Policy loss: 0.052201. Value loss: 0.659171. Entropy: 1.241730.\n",
      "Iteration 164: Policy loss: 0.056856. Value loss: 0.526180. Entropy: 1.248983.\n",
      "Iteration 165: Policy loss: 0.039226. Value loss: 0.534391. Entropy: 1.244428.\n",
      "episode: 25   score: 20400.0  epsilon: 1.0    steps: 165  evaluation reward: 20996.0\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 166: Policy loss: -0.229779. Value loss: 1.429668. Entropy: 1.282551.\n",
      "Iteration 167: Policy loss: -0.230095. Value loss: 1.338783. Entropy: 1.274564.\n",
      "Iteration 168: Policy loss: -0.214812. Value loss: 1.278463. Entropy: 1.277076.\n",
      "episode: 26   score: 21800.0  epsilon: 1.0    steps: 285  evaluation reward: 21026.923076923078\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 169: Policy loss: -0.018461. Value loss: 0.910960. Entropy: 1.264223.\n",
      "Iteration 170: Policy loss: -0.045014. Value loss: 0.787751. Entropy: 1.260487.\n",
      "Iteration 171: Policy loss: -0.055855. Value loss: 0.590654. Entropy: 1.259597.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 172: Policy loss: -0.091244. Value loss: 1.398936. Entropy: 1.260930.\n",
      "Iteration 173: Policy loss: -0.086726. Value loss: 1.065631. Entropy: 1.250345.\n",
      "Iteration 174: Policy loss: -0.082103. Value loss: 0.713698. Entropy: 1.245256.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 175: Policy loss: -0.050010. Value loss: 0.772323. Entropy: 1.259520.\n",
      "Iteration 176: Policy loss: -0.047915. Value loss: 0.430823. Entropy: 1.251579.\n",
      "Iteration 177: Policy loss: -0.026329. Value loss: 0.447836. Entropy: 1.251050.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 27   score: 15100.0  epsilon: 1.0    steps: 652  evaluation reward: 20807.40740740741\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 178: Policy loss: 0.024522. Value loss: 1.066570. Entropy: 1.251285.\n",
      "Iteration 179: Policy loss: 0.002568. Value loss: 0.774038. Entropy: 1.244492.\n",
      "Iteration 180: Policy loss: 0.035311. Value loss: 0.542031. Entropy: 1.251379.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 181: Policy loss: 0.348849. Value loss: 0.860963. Entropy: 1.250753.\n",
      "Iteration 182: Policy loss: 0.360043. Value loss: 0.543085. Entropy: 1.267107.\n",
      "Iteration 183: Policy loss: 0.340807. Value loss: 0.445778. Entropy: 1.261358.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 184: Policy loss: -0.182871. Value loss: 1.296346. Entropy: 1.253929.\n",
      "Iteration 185: Policy loss: -0.199198. Value loss: 1.305865. Entropy: 1.246943.\n",
      "Iteration 186: Policy loss: -0.209038. Value loss: 1.193934. Entropy: 1.243795.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 187: Policy loss: 0.187177. Value loss: 1.747922. Entropy: 1.229990.\n",
      "Iteration 188: Policy loss: 0.166177. Value loss: 1.490834. Entropy: 1.241176.\n",
      "Iteration 189: Policy loss: 0.157351. Value loss: 1.211300. Entropy: 1.225667.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 190: Policy loss: -0.027777. Value loss: 1.360953. Entropy: 1.213908.\n",
      "Iteration 191: Policy loss: -0.021151. Value loss: 1.024137. Entropy: 1.208828.\n",
      "Iteration 192: Policy loss: -0.039356. Value loss: 0.918242. Entropy: 1.208631.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 193: Policy loss: 0.160974. Value loss: 0.972360. Entropy: 1.227727.\n",
      "Iteration 194: Policy loss: 0.129330. Value loss: 0.622117. Entropy: 1.237204.\n",
      "Iteration 195: Policy loss: 0.150176. Value loss: 0.618030. Entropy: 1.228160.\n",
      "episode: 28   score: 30200.0  epsilon: 1.0    steps: 405  evaluation reward: 21142.85714285714\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 196: Policy loss: -0.121538. Value loss: 0.735195. Entropy: 1.252509.\n",
      "Iteration 197: Policy loss: -0.128379. Value loss: 0.581204. Entropy: 1.254438.\n",
      "Iteration 198: Policy loss: -0.118969. Value loss: 0.487212. Entropy: 1.248842.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 199: Policy loss: -0.180329. Value loss: 0.920394. Entropy: 1.216559.\n",
      "Iteration 200: Policy loss: -0.218271. Value loss: 0.690321. Entropy: 1.222403.\n",
      "Iteration 201: Policy loss: -0.219665. Value loss: 0.656242. Entropy: 1.206663.\n",
      "episode: 29   score: 26900.0  epsilon: 1.0    steps: 967  evaluation reward: 21341.379310344826\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 202: Policy loss: 0.216477. Value loss: 0.833777. Entropy: 1.213017.\n",
      "Iteration 203: Policy loss: 0.288782. Value loss: 0.556800. Entropy: 1.220835.\n",
      "Iteration 204: Policy loss: 0.255013. Value loss: 0.530424. Entropy: 1.208437.\n",
      "episode: 30   score: 37200.0  epsilon: 1.0    steps: 15  evaluation reward: 21870.0\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 205: Policy loss: -0.245106. Value loss: 1.005975. Entropy: 1.267397.\n",
      "Iteration 206: Policy loss: -0.206225. Value loss: 0.702353. Entropy: 1.262423.\n",
      "Iteration 207: Policy loss: -0.261868. Value loss: 0.649845. Entropy: 1.251042.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 208: Policy loss: -0.240120. Value loss: 1.145005. Entropy: 1.247048.\n",
      "Iteration 209: Policy loss: -0.265831. Value loss: 1.031742. Entropy: 1.238864.\n",
      "Iteration 210: Policy loss: -0.253171. Value loss: 0.829652. Entropy: 1.229637.\n",
      "episode: 31   score: 19300.0  epsilon: 1.0    steps: 816  evaluation reward: 21787.09677419355\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 211: Policy loss: -0.095345. Value loss: 1.084911. Entropy: 1.251727.\n",
      "Iteration 212: Policy loss: -0.096941. Value loss: 0.971607. Entropy: 1.249707.\n",
      "Iteration 213: Policy loss: -0.107386. Value loss: 0.911690. Entropy: 1.243662.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 214: Policy loss: 0.191352. Value loss: 0.504672. Entropy: 1.215433.\n",
      "Iteration 215: Policy loss: 0.190692. Value loss: 0.392216. Entropy: 1.226244.\n",
      "Iteration 216: Policy loss: 0.189899. Value loss: 0.321113. Entropy: 1.223816.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 217: Policy loss: 0.062868. Value loss: 0.606992. Entropy: 1.211003.\n",
      "Iteration 218: Policy loss: 0.093463. Value loss: 0.407304. Entropy: 1.224143.\n",
      "Iteration 219: Policy loss: 0.077213. Value loss: 0.346960. Entropy: 1.219299.\n",
      "episode: 32   score: 26000.0  epsilon: 1.0    steps: 214  evaluation reward: 21918.75\n",
      "episode: 33   score: 19800.0  epsilon: 1.0    steps: 310  evaluation reward: 21854.545454545456\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 220: Policy loss: -0.015909. Value loss: 0.739415. Entropy: 1.267245.\n",
      "Iteration 221: Policy loss: -0.027694. Value loss: 0.650258. Entropy: 1.263698.\n",
      "Iteration 222: Policy loss: 0.003913. Value loss: 0.458682. Entropy: 1.263983.\n",
      "episode: 34   score: 24200.0  epsilon: 1.0    steps: 642  evaluation reward: 21923.529411764706\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 223: Policy loss: -0.290030. Value loss: 1.364465. Entropy: 1.274331.\n",
      "Iteration 224: Policy loss: -0.243602. Value loss: 0.878377. Entropy: 1.261286.\n",
      "Iteration 225: Policy loss: -0.249468. Value loss: 0.732534. Entropy: 1.248552.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 226: Policy loss: -0.004326. Value loss: 0.532229. Entropy: 1.223971.\n",
      "Iteration 227: Policy loss: -0.025069. Value loss: 0.370619. Entropy: 1.226848.\n",
      "Iteration 228: Policy loss: -0.009548. Value loss: 0.350684. Entropy: 1.224367.\n",
      "episode: 35   score: 27200.0  epsilon: 1.0    steps: 633  evaluation reward: 22074.285714285714\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 229: Policy loss: 0.075539. Value loss: 1.068061. Entropy: 1.247005.\n",
      "Iteration 230: Policy loss: 0.055850. Value loss: 0.957821. Entropy: 1.252789.\n",
      "Iteration 231: Policy loss: 0.076171. Value loss: 0.736897. Entropy: 1.234754.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 232: Policy loss: -0.195781. Value loss: 1.297354. Entropy: 1.189867.\n",
      "Iteration 233: Policy loss: -0.127343. Value loss: 0.703974. Entropy: 1.191930.\n",
      "Iteration 234: Policy loss: -0.151394. Value loss: 0.571758. Entropy: 1.173663.\n"
     ]
    }
   ],
   "source": [
    "### Loop through all environments and run PPO on them\n",
    "\n",
    "env_names = ['Atlantis-v0', 'Alien-v0', 'Amidar-v0', 'Assault-v0', 'Asterix-v0', 'Asteroids-v0', 'BankHeist-v0', 'MsPacman-v0']\n",
    "\n",
    "for a in range(len(env_names)):\n",
    "    \n",
    "    name = env_names[a]\n",
    "    \n",
    "    envs = []\n",
    "    for i in range(num_envs):\n",
    "        envs.append(GameEnv(name))\n",
    "    #env.render()\n",
    "\n",
    "    number_lives = envs[0].life\n",
    "    state_size = envs[0].observation_space.shape\n",
    "    action_size = envs[0].action_space.n\n",
    "    rewards, episodes = [], []\n",
    "\n",
    "    vis_env_idx = 0\n",
    "    vis_env = envs[vis_env_idx]\n",
    "    e = 0\n",
    "    frame = 0\n",
    "    max_eval = -np.inf\n",
    "    reset_count = 0\n",
    "\n",
    "\n",
    "    agent = Agent(action_size)\n",
    "    torch.save(agent.policy_net.state_dict(), \"./save_model/\" + name + \"_best\")\n",
    "    evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "    frame = 0\n",
    "    memory_size = 0\n",
    "    reset_max = 10\n",
    "    \n",
    "    print(\"Determing min/max rewards of environment\")\n",
    "    [low, high] = score_range = get_score_range(name)\n",
    "    print(\"Min: %d. Max: %d.\" % (low, high))\n",
    "\n",
    "    while (frame < 10000000):\n",
    "        step = 0\n",
    "        assert(num_envs * env_mem_size == train_frame)\n",
    "        frame_next_vals = []\n",
    "        for i in range(num_envs):\n",
    "            env = envs[i]\n",
    "            #history = env.history\n",
    "            #life = env.life\n",
    "            #state, reward, done, info = [env.state, env.reward, env.done, env.info]\n",
    "            for j in range(env_mem_size):\n",
    "                step += 1\n",
    "                frame += 1\n",
    "\n",
    "                curr_state = env.history[HISTORY_SIZE-1,:,:]\n",
    "                action, value = agent.get_action(np.float32(env.history[:HISTORY_SIZE,:,:]) / 255.)\n",
    "\n",
    "                next_state, env.reward, env.done, env.info = env.step(action)\n",
    "\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "\n",
    "                frame_next_state = get_frame(next_state)\n",
    "                env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "                terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "\n",
    "                env.life = env.info['ale.lives']\n",
    "                r = ((env.reward - low) / (high - low)) * 10\n",
    "\n",
    "                agent.memory.push(i, deepcopy(curr_state), action, r, terminal_state, value, 0, 0)\n",
    "                if (j == env_mem_size-1):\n",
    "                    _, frame_next_val = agent.get_action(np.float32(env.history[1:,:,:]) / 255.)\n",
    "                    frame_next_vals.append(frame_next_val)\n",
    "                env.score += env.reward\n",
    "                env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "\n",
    "                if (env.done):\n",
    "                    if (e % 50 == 0):\n",
    "                        print('now time : ', datetime.now())\n",
    "                        rewards.append(np.mean(evaluation_reward))\n",
    "                        episodes.append(e)\n",
    "                        pylab.plot(episodes, rewards, 'b')\n",
    "                        pylab.savefig(\"./save_graph/\" + name + \"_ppo.png\")\n",
    "                        torch.save(agent.policy_net, \"./save_model/\" + name + \"_ppo\")\n",
    "\n",
    "                        if np.mean(evaluation_reward) > max_eval:\n",
    "                            torch.save(agent.policy_net.state_dict(), \"./save_model/\"  + name + \"_ppo_best\")\n",
    "                            max_eval = float(np.mean(evaluation_reward))\n",
    "                            reset_count = 0\n",
    "                        elif e > 5000:\n",
    "                            reset_count += 1\n",
    "                            \"\"\"\n",
    "                            if (reset_count == reset_max):\n",
    "                                print(\"Training went nowhere, starting again at best model\")\n",
    "                                agent.policy_net.load_state_dict(torch.load(\"./save_model/spaceinvaders_ppo_best\"))\n",
    "                                agent.update_target_net()\n",
    "                                reset_count = 0\n",
    "                            \"\"\"\n",
    "                    e += 1\n",
    "                    evaluation_reward.append(env.score)\n",
    "                    print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                      \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "                    env.done = False\n",
    "                    env.score = 0\n",
    "                    env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "                    env.state = env.reset()\n",
    "                    env.life = number_lives\n",
    "                    get_init_state(env.history, env.state)\n",
    "\n",
    "\n",
    "\n",
    "        agent.train_policy_net(frame, frame_next_vals)\n",
    "        agent.update_target_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
