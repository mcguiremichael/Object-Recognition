{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "from env import GameEnv\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/anaconda3/envs/py36/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "envs = []\n",
    "for i in range(num_envs):\n",
    "    envs.append(GameEnv('SpaceInvadersDeterministic-v4'))\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = envs[0].life\n",
    "state_size = envs[0].observation_space.shape\n",
    "action_size = envs[0].action_space.n\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "torch.save(agent.policy_net.state_dict(), \"./save_model/spaceinvaders_ppo_best\")\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n",
    "reset_max = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [16, 4, 8, 8], but got input of size [4, 84, 84] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-77aed6047ce3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mcurr_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mHISTORY_SIZE\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mHISTORY_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#x = F.leaky_relu(self.bn3(self.conv3(x)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [16, 4, 8, 8], but got input of size [4, 84, 84] instead"
     ]
    }
   ],
   "source": [
    "vis_env_idx = 0\n",
    "vis_env = envs[vis_env_idx]\n",
    "e = 0\n",
    "frame = 0\n",
    "max_eval = -np.inf\n",
    "reset_count = 0\n",
    "\n",
    "while (frame < 10000000):\n",
    "    step = 0\n",
    "    assert(num_envs * env_mem_size == train_frame)\n",
    "    frame_next_vals = []\n",
    "    for i in range(num_envs):\n",
    "        env = envs[i]\n",
    "        #history = env.history\n",
    "        #life = env.life\n",
    "        #state, reward, done, info = [env.state, env.reward, env.done, env.info]\n",
    "        for j in range(env_mem_size):\n",
    "            step += 1\n",
    "            frame += 1\n",
    "            \n",
    "            curr_state = env.history[HISTORY_SIZE-1,:,:]\n",
    "            action, value = agent.get_action(np.float32(env.history[:HISTORY_SIZE,:,:]) / 255.)\n",
    "            \n",
    "            next_state, env.reward, env.done, env.info = env.step(action)\n",
    "            \n",
    "            if (i == vis_env_idx):\n",
    "                vis_env._env.render()\n",
    "            \n",
    "            frame_next_state = get_frame(next_state)\n",
    "            env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "            terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "            \n",
    "            env.life = env.info['ale.lives']\n",
    "            r = env.reward\n",
    "            \n",
    "            agent.memory.push(i, deepcopy(curr_state), action, r, terminal_state, value, 0, 0)\n",
    "            if (j == env_mem_size-1):\n",
    "                _, frame_next_val = agent.get_action(np.float32(env.history[1:,:,:]) / 255.)\n",
    "                frame_next_vals.append(frame_next_val)\n",
    "            env.score += r\n",
    "            env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "            \n",
    "            if (env.done):\n",
    "                if (e % 50 == 0):\n",
    "                    print('now time : ', datetime.now())\n",
    "                    rewards.append(np.mean(evaluation_reward))\n",
    "                    episodes.append(e)\n",
    "                    pylab.plot(episodes, rewards, 'b')\n",
    "                    pylab.savefig(\"./save_graph/spaceinvaders_ppo.png\")\n",
    "                    torch.save(agent.policy_net, \"./save_model/spaceinvaders_ppo\")\n",
    "                    \n",
    "                    if np.mean(evaluation_reward) > max_eval:\n",
    "                        torch.save(agent.policy_net.state_dict(), \"./save_model/spaceinvaders_ppo_best\")\n",
    "                        max_eval = float(np.mean(evaluation_reward))\n",
    "                        reset_count = 0\n",
    "                    elif e > 5000:\n",
    "                        reset_count += 1\n",
    "                        \"\"\"\n",
    "                        if (reset_count == reset_max):\n",
    "                            print(\"Training went nowhere, starting again at best model\")\n",
    "                            agent.policy_net.load_state_dict(torch.load(\"./save_model/spaceinvaders_ppo_best\"))\n",
    "                            agent.update_target_net()\n",
    "                            reset_count = 0\n",
    "                        \"\"\"\n",
    "                e += 1\n",
    "                evaluation_reward.append(env.score)\n",
    "                print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "                \n",
    "                env.done = False\n",
    "                env.score = 0\n",
    "                env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "                env.state = env.reset()\n",
    "                env.life = number_lives\n",
    "                get_init_state(env.history, env.state)\n",
    "                \n",
    "                \n",
    "                \n",
    "    agent.train_policy_net(frame, frame_next_vals)\n",
    "    agent.update_target_net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/spaceinvaders_ppo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " ------- STARTING TRAINING FOR SpaceInvaders-v0 ------- \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/anaconda3/envs/py36/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determing min/max rewards of environment\n",
      "Min: 0. Max: 200.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],1)[0].unsqueeze(1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000250. clip: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:260: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:261: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:262: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Policy loss: 0.000286. Value loss: 1.222049. Entropy: 1.385591.\n",
      "Iteration 2: Policy loss: 0.000138. Value loss: 0.763694. Entropy: 1.384290.\n",
      "Iteration 3: Policy loss: -0.001526. Value loss: 0.645708. Entropy: 1.384588.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 4: Policy loss: 0.000514. Value loss: 1.511997. Entropy: 1.381829.\n",
      "Iteration 5: Policy loss: -0.000044. Value loss: 0.797335. Entropy: 1.382019.\n",
      "Iteration 6: Policy loss: -0.002469. Value loss: 0.498602. Entropy: 1.382235.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 7: Policy loss: -0.000772. Value loss: 1.050353. Entropy: 1.378190.\n",
      "Iteration 8: Policy loss: -0.000993. Value loss: 0.600291. Entropy: 1.373755.\n",
      "Iteration 9: Policy loss: -0.002539. Value loss: 0.334053. Entropy: 1.372794.\n",
      "now time :  2019-02-28 10:54:34.430414\n",
      "episode: 1   score: 110.0  epsilon: 1.0    steps: 912  evaluation reward: 110.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/sigai/.local/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 10: Policy loss: -0.001303. Value loss: 0.652697. Entropy: 1.373592.\n",
      "Iteration 11: Policy loss: -0.001422. Value loss: 0.266825. Entropy: 1.369634.\n",
      "Iteration 12: Policy loss: -0.001310. Value loss: 0.164508. Entropy: 1.368400.\n",
      "episode: 2   score: 75.0  epsilon: 1.0    steps: 440  evaluation reward: 92.5\n",
      "episode: 3   score: 55.0  epsilon: 1.0    steps: 664  evaluation reward: 80.0\n",
      "episode: 4   score: 80.0  epsilon: 1.0    steps: 952  evaluation reward: 80.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 13: Policy loss: 0.000085. Value loss: 0.988260. Entropy: 1.367490.\n",
      "Iteration 14: Policy loss: -0.000047. Value loss: 0.443936. Entropy: 1.370726.\n",
      "Iteration 15: Policy loss: -0.001396. Value loss: 0.342720. Entropy: 1.373903.\n",
      "episode: 5   score: 65.0  epsilon: 1.0    steps: 192  evaluation reward: 77.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 16: Policy loss: 0.000355. Value loss: 0.829658. Entropy: 1.373774.\n",
      "Iteration 17: Policy loss: -0.002786. Value loss: 0.507070. Entropy: 1.374722.\n",
      "Iteration 18: Policy loss: -0.000086. Value loss: 0.398591. Entropy: 1.377701.\n",
      "episode: 6   score: 190.0  epsilon: 1.0    steps: 544  evaluation reward: 95.83333333333333\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 19: Policy loss: -0.000197. Value loss: 0.782019. Entropy: 1.373287.\n",
      "Iteration 20: Policy loss: -0.004621. Value loss: 0.430810. Entropy: 1.373608.\n",
      "Iteration 21: Policy loss: -0.003407. Value loss: 0.370671. Entropy: 1.375693.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 22: Policy loss: 0.001414. Value loss: 1.203010. Entropy: 1.371583.\n",
      "Iteration 23: Policy loss: -0.001549. Value loss: 0.490842. Entropy: 1.365250.\n",
      "Iteration 24: Policy loss: -0.002779. Value loss: 0.303714. Entropy: 1.365470.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 25: Policy loss: 0.000808. Value loss: 0.853428. Entropy: 1.368175.\n",
      "Iteration 26: Policy loss: 0.000570. Value loss: 0.421294. Entropy: 1.363290.\n",
      "Iteration 27: Policy loss: -0.000857. Value loss: 0.274517. Entropy: 1.363795.\n",
      "episode: 7   score: 245.0  epsilon: 1.0    steps: 88  evaluation reward: 117.14285714285714\n",
      "episode: 8   score: 110.0  epsilon: 1.0    steps: 544  evaluation reward: 116.25\n",
      "episode: 9   score: 60.0  epsilon: 1.0    steps: 632  evaluation reward: 110.0\n",
      "episode: 10   score: 220.0  epsilon: 1.0    steps: 704  evaluation reward: 121.0\n",
      "episode: 11   score: 35.0  epsilon: 1.0    steps: 728  evaluation reward: 113.18181818181819\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 28: Policy loss: 0.000532. Value loss: 0.981383. Entropy: 1.359846.\n",
      "Iteration 29: Policy loss: 0.003035. Value loss: 0.340750. Entropy: 1.364912.\n",
      "Iteration 30: Policy loss: -0.000087. Value loss: 0.314336. Entropy: 1.363163.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 31: Policy loss: -0.001056. Value loss: 0.935795. Entropy: 1.363718.\n",
      "Iteration 32: Policy loss: -0.003755. Value loss: 0.435625. Entropy: 1.369257.\n",
      "Iteration 33: Policy loss: -0.003983. Value loss: 0.311272. Entropy: 1.358379.\n",
      "episode: 12   score: 210.0  epsilon: 1.0    steps: 464  evaluation reward: 121.25\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 34: Policy loss: -0.003678. Value loss: 0.839342. Entropy: 1.366889.\n",
      "Iteration 35: Policy loss: -0.009953. Value loss: 0.461878. Entropy: 1.371936.\n",
      "Iteration 36: Policy loss: -0.006712. Value loss: 0.359625. Entropy: 1.369493.\n",
      "episode: 13   score: 35.0  epsilon: 1.0    steps: 736  evaluation reward: 114.61538461538461\n",
      "episode: 14   score: 55.0  epsilon: 1.0    steps: 840  evaluation reward: 110.35714285714286\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 37: Policy loss: 0.002047. Value loss: 0.735677. Entropy: 1.367058.\n",
      "Iteration 38: Policy loss: 0.001043. Value loss: 0.453123. Entropy: 1.371151.\n",
      "Iteration 39: Policy loss: 0.000187. Value loss: 0.307737. Entropy: 1.368162.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 40: Policy loss: -0.001118. Value loss: 1.051704. Entropy: 1.373664.\n",
      "Iteration 41: Policy loss: -0.001826. Value loss: 0.494152. Entropy: 1.376824.\n",
      "Iteration 42: Policy loss: 0.000655. Value loss: 0.281537. Entropy: 1.371328.\n",
      "episode: 15   score: 175.0  epsilon: 1.0    steps: 336  evaluation reward: 114.66666666666667\n",
      "episode: 16   score: 275.0  epsilon: 1.0    steps: 592  evaluation reward: 124.6875\n",
      "episode: 17   score: 230.0  epsilon: 1.0    steps: 816  evaluation reward: 130.88235294117646\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 43: Policy loss: -0.002811. Value loss: 0.776921. Entropy: 1.377148.\n",
      "Iteration 44: Policy loss: -0.005380. Value loss: 0.342804. Entropy: 1.377993.\n",
      "Iteration 45: Policy loss: -0.004780. Value loss: 0.206088. Entropy: 1.377755.\n",
      "episode: 18   score: 135.0  epsilon: 1.0    steps: 488  evaluation reward: 131.11111111111111\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 46: Policy loss: 0.000655. Value loss: 1.254599. Entropy: 1.378326.\n",
      "Iteration 47: Policy loss: -0.001141. Value loss: 0.647133. Entropy: 1.380345.\n",
      "Iteration 48: Policy loss: -0.001156. Value loss: 0.447882. Entropy: 1.380526.\n",
      "episode: 19   score: 195.0  epsilon: 1.0    steps: 320  evaluation reward: 134.47368421052633\n",
      "episode: 20   score: 235.0  epsilon: 1.0    steps: 912  evaluation reward: 139.5\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 49: Policy loss: 0.000011. Value loss: 1.267214. Entropy: 1.381976.\n",
      "Iteration 50: Policy loss: 0.001191. Value loss: 0.507853. Entropy: 1.383503.\n",
      "Iteration 51: Policy loss: -0.000736. Value loss: 0.325651. Entropy: 1.379621.\n",
      "episode: 21   score: 80.0  epsilon: 1.0    steps: 656  evaluation reward: 136.66666666666666\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 52: Policy loss: 0.000637. Value loss: 0.824633. Entropy: 1.381341.\n",
      "Iteration 53: Policy loss: -0.001131. Value loss: 0.392127. Entropy: 1.383298.\n",
      "Iteration 54: Policy loss: -0.003698. Value loss: 0.279845. Entropy: 1.383098.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 55: Policy loss: -0.000114. Value loss: 1.168996. Entropy: 1.383698.\n",
      "Iteration 56: Policy loss: 0.000047. Value loss: 0.545750. Entropy: 1.382283.\n",
      "Iteration 57: Policy loss: 0.000917. Value loss: 0.359230. Entropy: 1.382154.\n",
      "episode: 22   score: 65.0  epsilon: 1.0    steps: 328  evaluation reward: 133.4090909090909\n",
      "episode: 23   score: 50.0  epsilon: 1.0    steps: 392  evaluation reward: 129.7826086956522\n",
      "episode: 24   score: 55.0  epsilon: 1.0    steps: 456  evaluation reward: 126.66666666666667\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 58: Policy loss: -0.002107. Value loss: 0.664076. Entropy: 1.384385.\n",
      "Iteration 59: Policy loss: 0.002472. Value loss: 0.340642. Entropy: 1.384086.\n",
      "Iteration 60: Policy loss: 0.001424. Value loss: 0.289711. Entropy: 1.382612.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 61: Policy loss: 0.000744. Value loss: 1.195526. Entropy: 1.379780.\n",
      "Iteration 62: Policy loss: -0.002842. Value loss: 0.507748. Entropy: 1.377400.\n",
      "Iteration 63: Policy loss: 0.000173. Value loss: 0.334666. Entropy: 1.374627.\n",
      "episode: 25   score: 135.0  epsilon: 1.0    steps: 136  evaluation reward: 127.0\n",
      "episode: 26   score: 155.0  epsilon: 1.0    steps: 176  evaluation reward: 128.07692307692307\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 64: Policy loss: -0.001637. Value loss: 0.715657. Entropy: 1.381634.\n",
      "Iteration 65: Policy loss: -0.005060. Value loss: 0.359688. Entropy: 1.382941.\n",
      "Iteration 66: Policy loss: -0.008511. Value loss: 0.220573. Entropy: 1.383636.\n",
      "episode: 27   score: 55.0  epsilon: 1.0    steps: 624  evaluation reward: 125.37037037037037\n",
      "episode: 28   score: 215.0  epsilon: 1.0    steps: 912  evaluation reward: 128.57142857142858\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 67: Policy loss: -0.000851. Value loss: 0.858549. Entropy: 1.380052.\n",
      "Iteration 68: Policy loss: -0.002526. Value loss: 0.339316. Entropy: 1.377039.\n",
      "Iteration 69: Policy loss: -0.002475. Value loss: 0.203110. Entropy: 1.378328.\n",
      "episode: 29   score: 460.0  epsilon: 1.0    steps: 792  evaluation reward: 140.0\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 70: Policy loss: -0.000274. Value loss: 1.044215. Entropy: 1.370011.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 71: Policy loss: -0.001968. Value loss: 0.307152. Entropy: 1.366166.\n",
      "Iteration 72: Policy loss: -0.001882. Value loss: 0.190961. Entropy: 1.366615.\n",
      "episode: 30   score: 165.0  epsilon: 1.0    steps: 240  evaluation reward: 140.83333333333334\n",
      "episode: 31   score: 55.0  epsilon: 1.0    steps: 832  evaluation reward: 138.06451612903226\n",
      "episode: 32   score: 135.0  epsilon: 1.0    steps: 1016  evaluation reward: 137.96875\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 73: Policy loss: -0.001251. Value loss: 0.681423. Entropy: 1.357835.\n",
      "Iteration 74: Policy loss: -0.000527. Value loss: 0.405220. Entropy: 1.358865.\n",
      "Iteration 75: Policy loss: -0.003212. Value loss: 0.345408. Entropy: 1.352594.\n",
      "episode: 33   score: 75.0  epsilon: 1.0    steps: 80  evaluation reward: 136.06060606060606\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 76: Policy loss: 0.000558. Value loss: 0.782845. Entropy: 1.347416.\n",
      "Iteration 77: Policy loss: -0.000054. Value loss: 0.286762. Entropy: 1.339251.\n",
      "Iteration 78: Policy loss: 0.001297. Value loss: 0.188024. Entropy: 1.356163.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 79: Policy loss: -0.000888. Value loss: 1.381890. Entropy: 1.351484.\n",
      "Iteration 80: Policy loss: -0.003642. Value loss: 0.617172. Entropy: 1.354681.\n",
      "Iteration 81: Policy loss: -0.003331. Value loss: 0.349202. Entropy: 1.359270.\n",
      "episode: 34   score: 35.0  epsilon: 1.0    steps: 512  evaluation reward: 133.08823529411765\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 82: Policy loss: -0.001516. Value loss: 0.627303. Entropy: 1.356127.\n",
      "Iteration 83: Policy loss: -0.003954. Value loss: 0.255751. Entropy: 1.355785.\n",
      "Iteration 84: Policy loss: -0.007175. Value loss: 0.236148. Entropy: 1.358761.\n",
      "episode: 35   score: 110.0  epsilon: 1.0    steps: 512  evaluation reward: 132.42857142857142\n",
      "episode: 36   score: 155.0  epsilon: 1.0    steps: 984  evaluation reward: 133.05555555555554\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 85: Policy loss: -0.000380. Value loss: 1.223958. Entropy: 1.355197.\n",
      "Iteration 86: Policy loss: -0.000759. Value loss: 0.552873. Entropy: 1.345288.\n",
      "Iteration 87: Policy loss: -0.005339. Value loss: 0.288940. Entropy: 1.339836.\n",
      "episode: 37   score: 195.0  epsilon: 1.0    steps: 920  evaluation reward: 134.72972972972974\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 88: Policy loss: -0.000980. Value loss: 1.164494. Entropy: 1.337258.\n",
      "Iteration 89: Policy loss: -0.000333. Value loss: 0.385290. Entropy: 1.346050.\n",
      "Iteration 90: Policy loss: -0.001612. Value loss: 0.249123. Entropy: 1.340477.\n",
      "episode: 38   score: 430.0  epsilon: 1.0    steps: 400  evaluation reward: 142.5\n",
      "episode: 39   score: 135.0  epsilon: 1.0    steps: 1016  evaluation reward: 142.30769230769232\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 91: Policy loss: 0.000279. Value loss: 0.804373. Entropy: 1.341285.\n",
      "Iteration 92: Policy loss: -0.002583. Value loss: 0.361942. Entropy: 1.341949.\n",
      "Iteration 93: Policy loss: -0.000887. Value loss: 0.291182. Entropy: 1.347177.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 94: Policy loss: -0.001675. Value loss: 0.974845. Entropy: 1.346342.\n",
      "Iteration 95: Policy loss: 0.000682. Value loss: 0.351182. Entropy: 1.359134.\n",
      "Iteration 96: Policy loss: -0.003089. Value loss: 0.277871. Entropy: 1.360657.\n",
      "episode: 40   score: 315.0  epsilon: 1.0    steps: 256  evaluation reward: 146.625\n",
      "episode: 41   score: 65.0  epsilon: 1.0    steps: 360  evaluation reward: 144.6341463414634\n",
      "episode: 42   score: 105.0  epsilon: 1.0    steps: 528  evaluation reward: 143.6904761904762\n",
      "episode: 43   score: 360.0  epsilon: 1.0    steps: 1000  evaluation reward: 148.72093023255815\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 97: Policy loss: -0.004380. Value loss: 0.618367. Entropy: 1.365689.\n",
      "Iteration 98: Policy loss: -0.005412. Value loss: 0.237031. Entropy: 1.371516.\n",
      "Iteration 99: Policy loss: -0.004669. Value loss: 0.165660. Entropy: 1.370817.\n",
      "episode: 44   score: 45.0  epsilon: 1.0    steps: 984  evaluation reward: 146.36363636363637\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 100: Policy loss: -0.000454. Value loss: 0.881833. Entropy: 1.372552.\n",
      "Iteration 101: Policy loss: -0.000114. Value loss: 0.400093. Entropy: 1.375808.\n",
      "Iteration 102: Policy loss: -0.000052. Value loss: 0.230840. Entropy: 1.373815.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 103: Policy loss: -0.001530. Value loss: 0.995422. Entropy: 1.365333.\n",
      "Iteration 104: Policy loss: -0.002885. Value loss: 0.482413. Entropy: 1.367916.\n",
      "Iteration 105: Policy loss: -0.003152. Value loss: 0.317355. Entropy: 1.365542.\n",
      "episode: 45   score: 45.0  epsilon: 1.0    steps: 1008  evaluation reward: 144.11111111111111\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 106: Policy loss: 0.000387. Value loss: 0.733106. Entropy: 1.365752.\n",
      "Iteration 107: Policy loss: -0.000699. Value loss: 0.282364. Entropy: 1.368273.\n",
      "Iteration 108: Policy loss: 0.000670. Value loss: 0.192997. Entropy: 1.369714.\n",
      "episode: 46   score: 55.0  epsilon: 1.0    steps: 280  evaluation reward: 142.17391304347825\n",
      "episode: 47   score: 60.0  epsilon: 1.0    steps: 824  evaluation reward: 140.4255319148936\n",
      "episode: 48   score: 165.0  epsilon: 1.0    steps: 864  evaluation reward: 140.9375\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 109: Policy loss: -0.000381. Value loss: 0.691269. Entropy: 1.369630.\n",
      "Iteration 110: Policy loss: -0.000016. Value loss: 0.315383. Entropy: 1.366833.\n",
      "Iteration 111: Policy loss: 0.000108. Value loss: 0.165443. Entropy: 1.365183.\n",
      "episode: 49   score: 160.0  epsilon: 1.0    steps: 664  evaluation reward: 141.3265306122449\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 112: Policy loss: -0.000433. Value loss: 0.739886. Entropy: 1.364134.\n",
      "Iteration 113: Policy loss: -0.001500. Value loss: 0.407814. Entropy: 1.359959.\n",
      "Iteration 114: Policy loss: -0.003789. Value loss: 0.290826. Entropy: 1.360587.\n",
      "episode: 50   score: 335.0  epsilon: 1.0    steps: 144  evaluation reward: 145.2\n",
      "now time :  2019-02-28 10:55:50.869477\n",
      "episode: 51   score: 135.0  epsilon: 1.0    steps: 776  evaluation reward: 145.0\n",
      "episode: 52   score: 185.0  epsilon: 1.0    steps: 800  evaluation reward: 145.76923076923077\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 115: Policy loss: -0.000918. Value loss: 0.966661. Entropy: 1.360747.\n",
      "Iteration 116: Policy loss: -0.000145. Value loss: 0.405912. Entropy: 1.360771.\n",
      "Iteration 117: Policy loss: -0.002588. Value loss: 0.368084. Entropy: 1.360314.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 118: Policy loss: 0.000065. Value loss: 1.235127. Entropy: 1.357983.\n",
      "Iteration 119: Policy loss: -0.000224. Value loss: 0.761730. Entropy: 1.354611.\n",
      "Iteration 120: Policy loss: -0.001148. Value loss: 0.479050. Entropy: 1.352103.\n",
      "episode: 53   score: 75.0  epsilon: 1.0    steps: 712  evaluation reward: 144.43396226415095\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 121: Policy loss: -0.000520. Value loss: 0.842131. Entropy: 1.352508.\n",
      "Iteration 122: Policy loss: -0.002061. Value loss: 0.452272. Entropy: 1.351790.\n",
      "Iteration 123: Policy loss: -0.003986. Value loss: 0.291210. Entropy: 1.349922.\n",
      "episode: 54   score: 150.0  epsilon: 1.0    steps: 160  evaluation reward: 144.53703703703704\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 124: Policy loss: -0.001071. Value loss: 0.551204. Entropy: 1.347558.\n",
      "Iteration 125: Policy loss: -0.000763. Value loss: 0.224953. Entropy: 1.354028.\n",
      "Iteration 126: Policy loss: -0.003144. Value loss: 0.166490. Entropy: 1.353510.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 127: Policy loss: 0.000141. Value loss: 0.852704. Entropy: 1.352704.\n",
      "Iteration 128: Policy loss: -0.000275. Value loss: 0.276252. Entropy: 1.349164.\n",
      "Iteration 129: Policy loss: -0.002540. Value loss: 0.270589. Entropy: 1.347374.\n",
      "episode: 55   score: 420.0  epsilon: 1.0    steps: 40  evaluation reward: 149.54545454545453\n",
      "episode: 56   score: 100.0  epsilon: 1.0    steps: 72  evaluation reward: 148.66071428571428\n",
      "episode: 57   score: 70.0  epsilon: 1.0    steps: 184  evaluation reward: 147.28070175438597\n",
      "episode: 58   score: 145.0  epsilon: 1.0    steps: 616  evaluation reward: 147.24137931034483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 130: Policy loss: 0.000529. Value loss: 0.518195. Entropy: 1.337850.\n",
      "Iteration 131: Policy loss: -0.000903. Value loss: 0.113338. Entropy: 1.337968.\n",
      "Iteration 132: Policy loss: -0.000621. Value loss: 0.106030. Entropy: 1.341355.\n",
      "episode: 59   score: 5.0  epsilon: 1.0    steps: 456  evaluation reward: 144.83050847457628\n",
      "episode: 60   score: 385.0  epsilon: 1.0    steps: 680  evaluation reward: 148.83333333333334\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 133: Policy loss: 0.001146. Value loss: 1.080609. Entropy: 1.341153.\n",
      "Iteration 134: Policy loss: -0.000639. Value loss: 0.417108. Entropy: 1.335413.\n",
      "Iteration 135: Policy loss: -0.000100. Value loss: 0.344107. Entropy: 1.336276.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 136: Policy loss: 0.002177. Value loss: 0.949127. Entropy: 1.342453.\n",
      "Iteration 137: Policy loss: 0.001035. Value loss: 0.397984. Entropy: 1.331897.\n",
      "Iteration 138: Policy loss: -0.001671. Value loss: 0.313935. Entropy: 1.332659.\n",
      "episode: 61   score: 330.0  epsilon: 1.0    steps: 472  evaluation reward: 151.80327868852459\n",
      "episode: 62   score: 30.0  epsilon: 1.0    steps: 848  evaluation reward: 149.83870967741936\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 139: Policy loss: -0.000269. Value loss: 1.152726. Entropy: 1.328061.\n",
      "Iteration 140: Policy loss: -0.002043. Value loss: 0.297908. Entropy: 1.331584.\n",
      "Iteration 141: Policy loss: -0.004732. Value loss: 0.169320. Entropy: 1.334852.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 142: Policy loss: 0.000070. Value loss: 1.210233. Entropy: 1.341840.\n",
      "Iteration 143: Policy loss: -0.001797. Value loss: 0.428190. Entropy: 1.344493.\n",
      "Iteration 144: Policy loss: -0.002312. Value loss: 0.262898. Entropy: 1.344123.\n",
      "episode: 63   score: 160.0  epsilon: 1.0    steps: 592  evaluation reward: 150.0\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 145: Policy loss: -0.000081. Value loss: 1.002323. Entropy: 1.342735.\n",
      "Iteration 146: Policy loss: -0.003482. Value loss: 0.415999. Entropy: 1.352658.\n",
      "Iteration 147: Policy loss: -0.002536. Value loss: 0.311186. Entropy: 1.351050.\n",
      "episode: 64   score: 115.0  epsilon: 1.0    steps: 8  evaluation reward: 149.453125\n",
      "episode: 65   score: 115.0  epsilon: 1.0    steps: 984  evaluation reward: 148.92307692307693\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 148: Policy loss: 0.000454. Value loss: 0.864663. Entropy: 1.357302.\n",
      "Iteration 149: Policy loss: -0.000207. Value loss: 0.277824. Entropy: 1.360859.\n",
      "Iteration 150: Policy loss: -0.001373. Value loss: 0.169900. Entropy: 1.354372.\n",
      "episode: 66   score: 295.0  epsilon: 1.0    steps: 312  evaluation reward: 151.13636363636363\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 151: Policy loss: -0.000224. Value loss: 0.945405. Entropy: 1.348978.\n",
      "Iteration 152: Policy loss: -0.000078. Value loss: 0.300479. Entropy: 1.345874.\n",
      "Iteration 153: Policy loss: -0.002928. Value loss: 0.234555. Entropy: 1.350581.\n",
      "episode: 67   score: 10.0  epsilon: 1.0    steps: 248  evaluation reward: 149.02985074626866\n",
      "episode: 68   score: 205.0  epsilon: 1.0    steps: 320  evaluation reward: 149.85294117647058\n",
      "episode: 69   score: 235.0  epsilon: 1.0    steps: 336  evaluation reward: 151.08695652173913\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 154: Policy loss: 0.000480. Value loss: 1.210351. Entropy: 1.350426.\n",
      "Iteration 155: Policy loss: 0.000064. Value loss: 0.692019. Entropy: 1.351810.\n",
      "Iteration 156: Policy loss: -0.001994. Value loss: 0.339996. Entropy: 1.350903.\n",
      "episode: 70   score: 50.0  epsilon: 1.0    steps: 544  evaluation reward: 149.64285714285714\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 157: Policy loss: -0.000759. Value loss: 0.902913. Entropy: 1.339617.\n",
      "Iteration 158: Policy loss: -0.004839. Value loss: 0.515044. Entropy: 1.340276.\n",
      "Iteration 159: Policy loss: -0.001244. Value loss: 0.330315. Entropy: 1.338125.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 160: Policy loss: -0.000327. Value loss: 0.858929. Entropy: 1.329158.\n",
      "Iteration 161: Policy loss: -0.004685. Value loss: 0.538184. Entropy: 1.324682.\n",
      "Iteration 162: Policy loss: -0.002391. Value loss: 0.288215. Entropy: 1.331740.\n",
      "episode: 71   score: 90.0  epsilon: 1.0    steps: 728  evaluation reward: 148.80281690140845\n",
      "episode: 72   score: 135.0  epsilon: 1.0    steps: 960  evaluation reward: 148.61111111111111\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 163: Policy loss: 0.000374. Value loss: 0.756695. Entropy: 1.330228.\n",
      "Iteration 164: Policy loss: 0.000678. Value loss: 0.264024. Entropy: 1.323138.\n",
      "Iteration 165: Policy loss: 0.000847. Value loss: 0.150867. Entropy: 1.335610.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 166: Policy loss: 0.000050. Value loss: 0.723978. Entropy: 1.328645.\n",
      "Iteration 167: Policy loss: -0.001717. Value loss: 0.196731. Entropy: 1.321725.\n",
      "Iteration 168: Policy loss: -0.001528. Value loss: 0.176785. Entropy: 1.320659.\n",
      "episode: 73   score: 40.0  epsilon: 1.0    steps: 168  evaluation reward: 147.12328767123287\n",
      "episode: 74   score: 155.0  epsilon: 1.0    steps: 912  evaluation reward: 147.22972972972974\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 169: Policy loss: -0.001958. Value loss: 0.646222. Entropy: 1.316906.\n",
      "Iteration 170: Policy loss: -0.001156. Value loss: 0.370714. Entropy: 1.310952.\n",
      "Iteration 171: Policy loss: -0.006662. Value loss: 0.332685. Entropy: 1.307682.\n",
      "episode: 75   score: 455.0  epsilon: 1.0    steps: 184  evaluation reward: 151.33333333333334\n",
      "episode: 76   score: 20.0  epsilon: 1.0    steps: 688  evaluation reward: 149.60526315789474\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 172: Policy loss: 0.000310. Value loss: 1.026565. Entropy: 1.304298.\n",
      "Iteration 173: Policy loss: -0.000007. Value loss: 0.437264. Entropy: 1.319254.\n",
      "Iteration 174: Policy loss: -0.002909. Value loss: 0.262265. Entropy: 1.322957.\n",
      "episode: 77   score: 40.0  epsilon: 1.0    steps: 736  evaluation reward: 148.1818181818182\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 175: Policy loss: 0.000810. Value loss: 0.871929. Entropy: 1.323181.\n",
      "Iteration 176: Policy loss: 0.001047. Value loss: 0.489491. Entropy: 1.324546.\n",
      "Iteration 177: Policy loss: 0.000609. Value loss: 0.309896. Entropy: 1.311159.\n",
      "episode: 78   score: 35.0  epsilon: 1.0    steps: 528  evaluation reward: 146.73076923076923\n",
      "episode: 79   score: 175.0  epsilon: 1.0    steps: 784  evaluation reward: 147.08860759493672\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 178: Policy loss: 0.001765. Value loss: 0.890674. Entropy: 1.311790.\n",
      "Iteration 179: Policy loss: 0.000829. Value loss: 0.336166. Entropy: 1.313590.\n",
      "Iteration 180: Policy loss: 0.001058. Value loss: 0.220093. Entropy: 1.314367.\n",
      "episode: 80   score: 300.0  epsilon: 1.0    steps: 216  evaluation reward: 149.0\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 181: Policy loss: 0.002509. Value loss: 0.953303. Entropy: 1.301765.\n",
      "Iteration 182: Policy loss: 0.002119. Value loss: 0.327028. Entropy: 1.296253.\n",
      "Iteration 183: Policy loss: -0.000769. Value loss: 0.282971. Entropy: 1.289501.\n",
      "episode: 81   score: 515.0  epsilon: 1.0    steps: 376  evaluation reward: 153.5185185185185\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 184: Policy loss: 0.000346. Value loss: 0.827857. Entropy: 1.293468.\n",
      "Iteration 185: Policy loss: -0.001515. Value loss: 0.406394. Entropy: 1.280303.\n",
      "Iteration 186: Policy loss: -0.001595. Value loss: 0.326249. Entropy: 1.286251.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 187: Policy loss: -0.000997. Value loss: 0.556255. Entropy: 1.250322.\n",
      "Iteration 188: Policy loss: -0.003109. Value loss: 0.228637. Entropy: 1.246798.\n",
      "Iteration 189: Policy loss: -0.005032. Value loss: 0.159295. Entropy: 1.248819.\n",
      "episode: 82   score: 45.0  epsilon: 1.0    steps: 256  evaluation reward: 152.1951219512195\n",
      "episode: 83   score: 65.0  epsilon: 1.0    steps: 408  evaluation reward: 151.14457831325302\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 190: Policy loss: 0.000434. Value loss: 1.130411. Entropy: 1.245078.\n",
      "Iteration 191: Policy loss: -0.008307. Value loss: 0.387593. Entropy: 1.261029.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 192: Policy loss: -0.003038. Value loss: 0.246436. Entropy: 1.252399.\n",
      "episode: 84   score: 80.0  epsilon: 1.0    steps: 120  evaluation reward: 150.29761904761904\n",
      "episode: 85   score: 300.0  epsilon: 1.0    steps: 552  evaluation reward: 152.05882352941177\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 193: Policy loss: 0.000030. Value loss: 0.831781. Entropy: 1.269186.\n",
      "Iteration 194: Policy loss: -0.004146. Value loss: 0.348322. Entropy: 1.281192.\n",
      "Iteration 195: Policy loss: -0.004356. Value loss: 0.261501. Entropy: 1.273729.\n",
      "episode: 86   score: 250.0  epsilon: 1.0    steps: 384  evaluation reward: 153.19767441860466\n",
      "episode: 87   score: 125.0  epsilon: 1.0    steps: 456  evaluation reward: 152.8735632183908\n",
      "episode: 88   score: 415.0  epsilon: 1.0    steps: 744  evaluation reward: 155.85227272727272\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 196: Policy loss: 0.000418. Value loss: 0.587817. Entropy: 1.259564.\n",
      "Iteration 197: Policy loss: -0.000474. Value loss: 0.270949. Entropy: 1.250426.\n",
      "Iteration 198: Policy loss: -0.002877. Value loss: 0.193068. Entropy: 1.248236.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 199: Policy loss: -0.001758. Value loss: 1.019884. Entropy: 1.246774.\n",
      "Iteration 200: Policy loss: -0.001680. Value loss: 0.341478. Entropy: 1.254459.\n",
      "Iteration 201: Policy loss: -0.001439. Value loss: 0.212158. Entropy: 1.265012.\n",
      "episode: 89   score: 165.0  epsilon: 1.0    steps: 120  evaluation reward: 155.95505617977528\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 202: Policy loss: 0.000745. Value loss: 0.820666. Entropy: 1.247451.\n",
      "Iteration 203: Policy loss: 0.001392. Value loss: 0.312394. Entropy: 1.242765.\n",
      "Iteration 204: Policy loss: 0.002431. Value loss: 0.198740. Entropy: 1.255229.\n",
      "episode: 90   score: 65.0  epsilon: 1.0    steps: 568  evaluation reward: 154.94444444444446\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 205: Policy loss: 0.001354. Value loss: 1.199844. Entropy: 1.245344.\n",
      "Iteration 206: Policy loss: -0.003769. Value loss: 0.486581. Entropy: 1.229501.\n",
      "Iteration 207: Policy loss: 0.000223. Value loss: 0.310074. Entropy: 1.258159.\n",
      "episode: 91   score: 75.0  epsilon: 1.0    steps: 72  evaluation reward: 154.06593406593407\n",
      "episode: 92   score: 60.0  epsilon: 1.0    steps: 504  evaluation reward: 153.04347826086956\n",
      "episode: 93   score: 80.0  epsilon: 1.0    steps: 560  evaluation reward: 152.25806451612902\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 208: Policy loss: 0.000484. Value loss: 0.885163. Entropy: 1.262803.\n",
      "Iteration 209: Policy loss: -0.001573. Value loss: 0.274098. Entropy: 1.262668.\n",
      "Iteration 210: Policy loss: -0.002075. Value loss: 0.191055. Entropy: 1.263142.\n",
      "episode: 94   score: 235.0  epsilon: 1.0    steps: 48  evaluation reward: 153.13829787234042\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 211: Policy loss: -0.000150. Value loss: 1.023454. Entropy: 1.269745.\n",
      "Iteration 212: Policy loss: -0.000775. Value loss: 0.564313. Entropy: 1.270685.\n",
      "Iteration 213: Policy loss: -0.000073. Value loss: 0.396267. Entropy: 1.266166.\n",
      "episode: 95   score: 30.0  epsilon: 1.0    steps: 480  evaluation reward: 151.8421052631579\n",
      "episode: 96   score: 40.0  epsilon: 1.0    steps: 736  evaluation reward: 150.67708333333334\n",
      "episode: 97   score: 235.0  epsilon: 1.0    steps: 768  evaluation reward: 151.5463917525773\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 214: Policy loss: -0.000345. Value loss: 0.840788. Entropy: 1.261406.\n",
      "Iteration 215: Policy loss: 0.000206. Value loss: 0.365089. Entropy: 1.268794.\n",
      "Iteration 216: Policy loss: -0.002891. Value loss: 0.223525. Entropy: 1.265618.\n",
      "episode: 98   score: 30.0  epsilon: 1.0    steps: 472  evaluation reward: 150.30612244897958\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 217: Policy loss: -0.001412. Value loss: 0.873903. Entropy: 1.263013.\n",
      "Iteration 218: Policy loss: -0.003133. Value loss: 0.426671. Entropy: 1.255606.\n",
      "Iteration 219: Policy loss: -0.001763. Value loss: 0.257780. Entropy: 1.258159.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 220: Policy loss: -0.000570. Value loss: 0.758704. Entropy: 1.246692.\n",
      "Iteration 221: Policy loss: -0.002007. Value loss: 0.273937. Entropy: 1.252955.\n",
      "Iteration 222: Policy loss: -0.005331. Value loss: 0.201075. Entropy: 1.250713.\n",
      "episode: 99   score: 140.0  epsilon: 1.0    steps: 952  evaluation reward: 150.2020202020202\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 223: Policy loss: 0.000333. Value loss: 1.053482. Entropy: 1.248008.\n",
      "Iteration 224: Policy loss: -0.001146. Value loss: 0.334091. Entropy: 1.247435.\n",
      "Iteration 225: Policy loss: 0.000492. Value loss: 0.222488. Entropy: 1.248434.\n",
      "episode: 100   score: 85.0  epsilon: 1.0    steps: 104  evaluation reward: 149.55\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 226: Policy loss: 0.000697. Value loss: 0.919358. Entropy: 1.259447.\n",
      "Iteration 227: Policy loss: -0.001593. Value loss: 0.399924. Entropy: 1.262612.\n",
      "Iteration 228: Policy loss: -0.000147. Value loss: 0.328091. Entropy: 1.257676.\n",
      "now time :  2019-02-28 10:57:13.510300\n",
      "episode: 101   score: 580.0  epsilon: 1.0    steps: 672  evaluation reward: 154.25\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 229: Policy loss: -0.000061. Value loss: 1.341482. Entropy: 1.255298.\n",
      "Iteration 230: Policy loss: -0.003398. Value loss: 0.403578. Entropy: 1.238707.\n",
      "Iteration 231: Policy loss: -0.002585. Value loss: 0.203481. Entropy: 1.247231.\n",
      "episode: 102   score: 225.0  epsilon: 1.0    steps: 552  evaluation reward: 155.75\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 232: Policy loss: -0.001552. Value loss: 0.983636. Entropy: 1.233891.\n",
      "Iteration 233: Policy loss: -0.001832. Value loss: 0.282890. Entropy: 1.238439.\n",
      "Iteration 234: Policy loss: -0.003064. Value loss: 0.197092. Entropy: 1.236348.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 235: Policy loss: -0.002625. Value loss: 0.782122. Entropy: 1.213863.\n",
      "Iteration 236: Policy loss: -0.004357. Value loss: 0.233588. Entropy: 1.215192.\n",
      "Iteration 237: Policy loss: -0.008773. Value loss: 0.166308. Entropy: 1.209892.\n",
      "episode: 103   score: 40.0  epsilon: 1.0    steps: 296  evaluation reward: 155.6\n",
      "episode: 104   score: 160.0  epsilon: 1.0    steps: 496  evaluation reward: 156.4\n",
      "episode: 105   score: 240.0  epsilon: 1.0    steps: 720  evaluation reward: 158.15\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 238: Policy loss: 0.000032. Value loss: 0.989702. Entropy: 1.210369.\n",
      "Iteration 239: Policy loss: 0.000391. Value loss: 0.362281. Entropy: 1.211375.\n",
      "Iteration 240: Policy loss: -0.000272. Value loss: 0.271815. Entropy: 1.201306.\n",
      "episode: 106   score: 355.0  epsilon: 1.0    steps: 96  evaluation reward: 159.8\n",
      "episode: 107   score: 5.0  epsilon: 1.0    steps: 464  evaluation reward: 157.4\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 241: Policy loss: -0.001518. Value loss: 0.654991. Entropy: 1.233659.\n",
      "Iteration 242: Policy loss: -0.000869. Value loss: 0.281469. Entropy: 1.239031.\n",
      "Iteration 243: Policy loss: -0.001302. Value loss: 0.233508. Entropy: 1.237587.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 244: Policy loss: -0.001304. Value loss: 0.839899. Entropy: 1.205108.\n",
      "Iteration 245: Policy loss: -0.000948. Value loss: 0.300877. Entropy: 1.216346.\n",
      "Iteration 246: Policy loss: -0.004176. Value loss: 0.222377. Entropy: 1.226185.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 247: Policy loss: 0.001399. Value loss: 0.750016. Entropy: 1.249727.\n",
      "Iteration 248: Policy loss: -0.002353. Value loss: 0.255308. Entropy: 1.209973.\n",
      "Iteration 249: Policy loss: -0.004430. Value loss: 0.238330. Entropy: 1.206471.\n",
      "episode: 108   score: 105.0  epsilon: 1.0    steps: 248  evaluation reward: 157.35\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 250: Policy loss: -0.000318. Value loss: 0.507838. Entropy: 1.196139.\n",
      "Iteration 251: Policy loss: -0.000768. Value loss: 0.304154. Entropy: 1.195697.\n",
      "Iteration 252: Policy loss: -0.003577. Value loss: 0.217422. Entropy: 1.199205.\n",
      "episode: 109   score: 185.0  epsilon: 1.0    steps: 32  evaluation reward: 158.6\n",
      "episode: 110   score: 205.0  epsilon: 1.0    steps: 504  evaluation reward: 158.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 111   score: 530.0  epsilon: 1.0    steps: 984  evaluation reward: 163.4\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 253: Policy loss: 0.000473. Value loss: 1.006435. Entropy: 1.192834.\n",
      "Iteration 254: Policy loss: -0.000979. Value loss: 0.314719. Entropy: 1.171571.\n",
      "Iteration 255: Policy loss: 0.002728. Value loss: 0.217527. Entropy: 1.192246.\n",
      "episode: 112   score: 235.0  epsilon: 1.0    steps: 776  evaluation reward: 163.65\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 256: Policy loss: 0.001200. Value loss: 0.764650. Entropy: 1.172814.\n",
      "Iteration 257: Policy loss: -0.000485. Value loss: 0.265355. Entropy: 1.150887.\n",
      "Iteration 258: Policy loss: 0.000393. Value loss: 0.165204. Entropy: 1.162391.\n",
      "episode: 113   score: 210.0  epsilon: 1.0    steps: 336  evaluation reward: 165.4\n",
      "episode: 114   score: 455.0  epsilon: 1.0    steps: 872  evaluation reward: 169.4\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 259: Policy loss: -0.001972. Value loss: 0.878471. Entropy: 1.181261.\n",
      "Iteration 260: Policy loss: -0.000592. Value loss: 0.302864. Entropy: 1.172084.\n",
      "Iteration 261: Policy loss: -0.002760. Value loss: 0.209676. Entropy: 1.186368.\n",
      "episode: 115   score: 230.0  epsilon: 1.0    steps: 568  evaluation reward: 169.95\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 262: Policy loss: -0.001610. Value loss: 0.735944. Entropy: 1.218861.\n",
      "Iteration 263: Policy loss: -0.002216. Value loss: 0.251846. Entropy: 1.227600.\n",
      "Iteration 264: Policy loss: -0.003397. Value loss: 0.223715. Entropy: 1.223034.\n",
      "episode: 116   score: 130.0  epsilon: 1.0    steps: 432  evaluation reward: 168.5\n",
      "episode: 117   score: 15.0  epsilon: 1.0    steps: 968  evaluation reward: 166.35\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 265: Policy loss: 0.001025. Value loss: 0.709193. Entropy: 1.227817.\n",
      "Iteration 266: Policy loss: -0.000392. Value loss: 0.282104. Entropy: 1.226850.\n",
      "Iteration 267: Policy loss: -0.000873. Value loss: 0.200837. Entropy: 1.233036.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 268: Policy loss: 0.000997. Value loss: 1.145845. Entropy: 1.262316.\n",
      "Iteration 269: Policy loss: -0.003322. Value loss: 0.286911. Entropy: 1.254794.\n",
      "Iteration 270: Policy loss: -0.005100. Value loss: 0.190044. Entropy: 1.253736.\n",
      "episode: 118   score: 200.0  epsilon: 1.0    steps: 360  evaluation reward: 167.0\n",
      "episode: 119   score: 185.0  epsilon: 1.0    steps: 824  evaluation reward: 166.9\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 271: Policy loss: 0.000397. Value loss: 0.734662. Entropy: 1.243996.\n",
      "Iteration 272: Policy loss: -0.001828. Value loss: 0.261671. Entropy: 1.241362.\n",
      "Iteration 273: Policy loss: -0.001978. Value loss: 0.171053. Entropy: 1.228397.\n",
      "episode: 120   score: 245.0  epsilon: 1.0    steps: 408  evaluation reward: 167.0\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 274: Policy loss: 0.002259. Value loss: 0.772154. Entropy: 1.247723.\n",
      "Iteration 275: Policy loss: -0.001331. Value loss: 0.313501. Entropy: 1.226066.\n",
      "Iteration 276: Policy loss: -0.005311. Value loss: 0.240438. Entropy: 1.232553.\n",
      "episode: 121   score: 60.0  epsilon: 1.0    steps: 640  evaluation reward: 166.8\n",
      "episode: 122   score: 240.0  epsilon: 1.0    steps: 944  evaluation reward: 168.55\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 277: Policy loss: 0.000106. Value loss: 0.957083. Entropy: 1.207558.\n",
      "Iteration 278: Policy loss: -0.001357. Value loss: 0.278461. Entropy: 1.205970.\n",
      "Iteration 279: Policy loss: -0.000826. Value loss: 0.185407. Entropy: 1.214287.\n",
      "episode: 123   score: 175.0  epsilon: 1.0    steps: 160  evaluation reward: 169.8\n",
      "episode: 124   score: 65.0  epsilon: 1.0    steps: 344  evaluation reward: 169.9\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 280: Policy loss: -0.000826. Value loss: 0.728704. Entropy: 1.215706.\n",
      "Iteration 281: Policy loss: -0.003806. Value loss: 0.249703. Entropy: 1.206271.\n",
      "Iteration 282: Policy loss: -0.003290. Value loss: 0.150132. Entropy: 1.196831.\n",
      "episode: 125   score: 335.0  epsilon: 1.0    steps: 976  evaluation reward: 171.9\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 283: Policy loss: 0.000051. Value loss: 0.887927. Entropy: 1.229929.\n",
      "Iteration 284: Policy loss: -0.003931. Value loss: 0.405848. Entropy: 1.204426.\n",
      "Iteration 285: Policy loss: -0.001734. Value loss: 0.243968. Entropy: 1.200162.\n",
      "episode: 126   score: 120.0  epsilon: 1.0    steps: 672  evaluation reward: 171.55\n",
      "episode: 127   score: 90.0  epsilon: 1.0    steps: 1000  evaluation reward: 171.9\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 286: Policy loss: -0.000701. Value loss: 1.328719. Entropy: 1.212439.\n",
      "Iteration 287: Policy loss: -0.000293. Value loss: 0.500297. Entropy: 1.209400.\n",
      "Iteration 288: Policy loss: -0.001432. Value loss: 0.353142. Entropy: 1.196471.\n",
      "episode: 128   score: 55.0  epsilon: 1.0    steps: 360  evaluation reward: 170.3\n",
      "episode: 129   score: 165.0  epsilon: 1.0    steps: 832  evaluation reward: 167.35\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 289: Policy loss: -0.000035. Value loss: 0.852075. Entropy: 1.212245.\n",
      "Iteration 290: Policy loss: -0.002848. Value loss: 0.356906. Entropy: 1.227036.\n",
      "Iteration 291: Policy loss: -0.000258. Value loss: 0.229798. Entropy: 1.228871.\n",
      "episode: 130   score: 190.0  epsilon: 1.0    steps: 944  evaluation reward: 167.6\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 292: Policy loss: -0.002274. Value loss: 1.123402. Entropy: 1.232825.\n",
      "Iteration 293: Policy loss: -0.003240. Value loss: 0.416062. Entropy: 1.258719.\n",
      "Iteration 294: Policy loss: -0.002872. Value loss: 0.286985. Entropy: 1.257950.\n",
      "episode: 131   score: 75.0  epsilon: 1.0    steps: 576  evaluation reward: 167.8\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 295: Policy loss: -0.001136. Value loss: 1.187404. Entropy: 1.245114.\n",
      "Iteration 296: Policy loss: -0.003747. Value loss: 0.437600. Entropy: 1.229692.\n",
      "Iteration 297: Policy loss: -0.003839. Value loss: 0.197836. Entropy: 1.244658.\n",
      "episode: 132   score: 225.0  epsilon: 1.0    steps: 616  evaluation reward: 168.7\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 298: Policy loss: -0.000703. Value loss: 0.977433. Entropy: 1.213868.\n",
      "Iteration 299: Policy loss: -0.000821. Value loss: 0.432540. Entropy: 1.227096.\n",
      "Iteration 300: Policy loss: -0.002662. Value loss: 0.296908. Entropy: 1.208175.\n",
      "episode: 133   score: 180.0  epsilon: 1.0    steps: 88  evaluation reward: 169.75\n",
      "episode: 134   score: 65.0  epsilon: 1.0    steps: 192  evaluation reward: 170.05\n",
      "episode: 135   score: 245.0  epsilon: 1.0    steps: 216  evaluation reward: 171.4\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 301: Policy loss: -0.000202. Value loss: 1.036785. Entropy: 1.192264.\n",
      "Iteration 302: Policy loss: -0.003046. Value loss: 0.471631. Entropy: 1.179243.\n",
      "Iteration 303: Policy loss: -0.002243. Value loss: 0.291864. Entropy: 1.170483.\n",
      "episode: 136   score: 165.0  epsilon: 1.0    steps: 744  evaluation reward: 171.5\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 304: Policy loss: 0.001008. Value loss: 0.856122. Entropy: 1.183952.\n",
      "Iteration 305: Policy loss: 0.000143. Value loss: 0.429460. Entropy: 1.224538.\n",
      "Iteration 306: Policy loss: -0.002060. Value loss: 0.242800. Entropy: 1.206369.\n",
      "episode: 137   score: 140.0  epsilon: 1.0    steps: 848  evaluation reward: 170.95\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 307: Policy loss: -0.001220. Value loss: 0.908791. Entropy: 1.192642.\n",
      "Iteration 308: Policy loss: -0.000724. Value loss: 0.277687. Entropy: 1.181107.\n",
      "Iteration 309: Policy loss: -0.003634. Value loss: 0.170541. Entropy: 1.191933.\n",
      "episode: 138   score: 240.0  epsilon: 1.0    steps: 248  evaluation reward: 169.05\n",
      "episode: 139   score: 85.0  epsilon: 1.0    steps: 352  evaluation reward: 168.55\n",
      "episode: 140   score: 110.0  epsilon: 1.0    steps: 752  evaluation reward: 166.5\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 310: Policy loss: -0.003816. Value loss: 0.725120. Entropy: 1.173773.\n",
      "Iteration 311: Policy loss: -0.002054. Value loss: 0.382019. Entropy: 1.162491.\n",
      "Iteration 312: Policy loss: -0.004391. Value loss: 0.357441. Entropy: 1.171405.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 313: Policy loss: 0.001698. Value loss: 1.116754. Entropy: 1.136402.\n",
      "Iteration 314: Policy loss: 0.002262. Value loss: 0.393227. Entropy: 1.128906.\n",
      "Iteration 315: Policy loss: 0.003017. Value loss: 0.268646. Entropy: 1.132143.\n",
      "episode: 141   score: 95.0  epsilon: 1.0    steps: 104  evaluation reward: 166.8\n",
      "episode: 142   score: 235.0  epsilon: 1.0    steps: 608  evaluation reward: 168.1\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 316: Policy loss: 0.000509. Value loss: 0.799417. Entropy: 1.155182.\n",
      "Iteration 317: Policy loss: -0.001385. Value loss: 0.430270. Entropy: 1.139876.\n",
      "Iteration 318: Policy loss: 0.000419. Value loss: 0.266887. Entropy: 1.143153.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 319: Policy loss: 0.003320. Value loss: 1.071587. Entropy: 1.121362.\n",
      "Iteration 320: Policy loss: -0.004040. Value loss: 0.474875. Entropy: 1.164177.\n",
      "Iteration 321: Policy loss: -0.001975. Value loss: 0.300314. Entropy: 1.161351.\n",
      "episode: 143   score: 105.0  epsilon: 1.0    steps: 560  evaluation reward: 165.55\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 322: Policy loss: -0.000227. Value loss: 0.615313. Entropy: 1.207464.\n",
      "Iteration 323: Policy loss: -0.001538. Value loss: 0.292786. Entropy: 1.196635.\n",
      "Iteration 324: Policy loss: -0.001882. Value loss: 0.246658. Entropy: 1.194961.\n",
      "episode: 144   score: 90.0  epsilon: 1.0    steps: 264  evaluation reward: 166.0\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 325: Policy loss: 0.000954. Value loss: 0.601621. Entropy: 1.232766.\n",
      "Iteration 326: Policy loss: -0.001389. Value loss: 0.294952. Entropy: 1.243162.\n",
      "Iteration 327: Policy loss: 0.000485. Value loss: 0.194264. Entropy: 1.243030.\n",
      "episode: 145   score: 270.0  epsilon: 1.0    steps: 368  evaluation reward: 168.25\n",
      "episode: 146   score: 65.0  epsilon: 1.0    steps: 584  evaluation reward: 168.35\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 328: Policy loss: -0.001113. Value loss: 0.868051. Entropy: 1.223289.\n",
      "Iteration 329: Policy loss: -0.001061. Value loss: 0.408868. Entropy: 1.244046.\n",
      "Iteration 330: Policy loss: -0.002911. Value loss: 0.253050. Entropy: 1.231239.\n",
      "episode: 147   score: 225.0  epsilon: 1.0    steps: 408  evaluation reward: 170.0\n",
      "episode: 148   score: 70.0  epsilon: 1.0    steps: 432  evaluation reward: 169.05\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 331: Policy loss: 0.000941. Value loss: 0.952657. Entropy: 1.229690.\n",
      "Iteration 332: Policy loss: -0.000644. Value loss: 0.407348. Entropy: 1.244814.\n",
      "Iteration 333: Policy loss: -0.000789. Value loss: 0.210498. Entropy: 1.239643.\n",
      "episode: 149   score: 260.0  epsilon: 1.0    steps: 64  evaluation reward: 170.05\n",
      "episode: 150   score: 490.0  epsilon: 1.0    steps: 568  evaluation reward: 171.6\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 334: Policy loss: -0.003555. Value loss: 1.236978. Entropy: 1.258842.\n",
      "Iteration 335: Policy loss: -0.004959. Value loss: 0.548692. Entropy: 1.259624.\n",
      "Iteration 336: Policy loss: -0.006168. Value loss: 0.600551. Entropy: 1.261831.\n",
      "now time :  2019-02-28 10:58:31.094505\n",
      "episode: 151   score: 240.0  epsilon: 1.0    steps: 344  evaluation reward: 172.65\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 337: Policy loss: -0.000867. Value loss: 1.317128. Entropy: 1.248688.\n",
      "Iteration 338: Policy loss: -0.004152. Value loss: 0.393891. Entropy: 1.253627.\n",
      "Iteration 339: Policy loss: -0.007073. Value loss: 0.237639. Entropy: 1.241966.\n",
      "episode: 152   score: 20.0  epsilon: 1.0    steps: 224  evaluation reward: 171.0\n",
      "episode: 153   score: 65.0  epsilon: 1.0    steps: 696  evaluation reward: 170.9\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 340: Policy loss: -0.000064. Value loss: 0.751408. Entropy: 1.235445.\n",
      "Iteration 341: Policy loss: 0.000212. Value loss: 0.362399. Entropy: 1.243157.\n",
      "Iteration 342: Policy loss: -0.002810. Value loss: 0.216992. Entropy: 1.217717.\n",
      "episode: 154   score: 70.0  epsilon: 1.0    steps: 368  evaluation reward: 170.1\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 343: Policy loss: 0.001577. Value loss: 0.947143. Entropy: 1.245740.\n",
      "Iteration 344: Policy loss: 0.001458. Value loss: 0.322325. Entropy: 1.242654.\n",
      "Iteration 345: Policy loss: -0.000986. Value loss: 0.202224. Entropy: 1.228980.\n",
      "episode: 155   score: 200.0  epsilon: 1.0    steps: 336  evaluation reward: 167.9\n",
      "episode: 156   score: 10.0  epsilon: 1.0    steps: 360  evaluation reward: 167.0\n",
      "episode: 157   score: 210.0  epsilon: 1.0    steps: 416  evaluation reward: 168.4\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 346: Policy loss: 0.001735. Value loss: 0.689885. Entropy: 1.209304.\n",
      "Iteration 347: Policy loss: -0.000166. Value loss: 0.219240. Entropy: 1.232711.\n",
      "Iteration 348: Policy loss: 0.000574. Value loss: 0.192619. Entropy: 1.226546.\n",
      "episode: 158   score: 35.0  epsilon: 1.0    steps: 488  evaluation reward: 167.3\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 349: Policy loss: 0.000972. Value loss: 0.791172. Entropy: 1.218467.\n",
      "Iteration 350: Policy loss: 0.000012. Value loss: 0.311100. Entropy: 1.217292.\n",
      "Iteration 351: Policy loss: -0.001267. Value loss: 0.233835. Entropy: 1.223150.\n",
      "episode: 159   score: 190.0  epsilon: 1.0    steps: 704  evaluation reward: 169.15\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 352: Policy loss: 0.000015. Value loss: 0.909725. Entropy: 1.204182.\n",
      "Iteration 353: Policy loss: -0.001151. Value loss: 0.406220. Entropy: 1.206395.\n",
      "Iteration 354: Policy loss: -0.000429. Value loss: 0.240914. Entropy: 1.226737.\n",
      "episode: 160   score: 30.0  epsilon: 1.0    steps: 112  evaluation reward: 165.6\n",
      "episode: 161   score: 120.0  epsilon: 1.0    steps: 184  evaluation reward: 163.5\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 355: Policy loss: 0.000632. Value loss: 0.825583. Entropy: 1.235461.\n",
      "Iteration 356: Policy loss: -0.000614. Value loss: 0.335848. Entropy: 1.226279.\n",
      "Iteration 357: Policy loss: -0.000161. Value loss: 0.145820. Entropy: 1.253049.\n",
      "episode: 162   score: 260.0  epsilon: 1.0    steps: 416  evaluation reward: 165.8\n",
      "episode: 163   score: 485.0  epsilon: 1.0    steps: 784  evaluation reward: 169.05\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 358: Policy loss: 0.001483. Value loss: 0.860168. Entropy: 1.235702.\n",
      "Iteration 359: Policy loss: 0.000099. Value loss: 0.310067. Entropy: 1.229896.\n",
      "Iteration 360: Policy loss: -0.001976. Value loss: 0.247025. Entropy: 1.217883.\n",
      "episode: 164   score: 145.0  epsilon: 1.0    steps: 592  evaluation reward: 169.35\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 361: Policy loss: -0.000875. Value loss: 1.189750. Entropy: 1.208969.\n",
      "Iteration 362: Policy loss: -0.001425. Value loss: 0.616152. Entropy: 1.212086.\n",
      "Iteration 363: Policy loss: -0.002718. Value loss: 0.367766. Entropy: 1.202157.\n",
      "episode: 165   score: 120.0  epsilon: 1.0    steps: 768  evaluation reward: 169.4\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 364: Policy loss: 0.002137. Value loss: 0.889541. Entropy: 1.207106.\n",
      "Iteration 365: Policy loss: 0.001980. Value loss: 0.233598. Entropy: 1.214182.\n",
      "Iteration 366: Policy loss: 0.000012. Value loss: 0.208510. Entropy: 1.240897.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 367: Policy loss: -0.001830. Value loss: 0.971060. Entropy: 1.200887.\n",
      "Iteration 368: Policy loss: 0.000141. Value loss: 0.372961. Entropy: 1.231671.\n",
      "Iteration 369: Policy loss: -0.002783. Value loss: 0.254119. Entropy: 1.223888.\n",
      "episode: 166   score: 125.0  epsilon: 1.0    steps: 400  evaluation reward: 167.7\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 370: Policy loss: 0.001511. Value loss: 1.086012. Entropy: 1.238506.\n",
      "Iteration 371: Policy loss: -0.002391. Value loss: 0.322087. Entropy: 1.249188.\n",
      "Iteration 372: Policy loss: -0.000742. Value loss: 0.209656. Entropy: 1.241365.\n",
      "episode: 167   score: 150.0  epsilon: 1.0    steps: 544  evaluation reward: 169.1\n",
      "episode: 168   score: 235.0  epsilon: 1.0    steps: 872  evaluation reward: 169.4\n",
      "episode: 169   score: 250.0  epsilon: 1.0    steps: 960  evaluation reward: 169.55\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 373: Policy loss: 0.001784. Value loss: 1.094970. Entropy: 1.268939.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 374: Policy loss: -0.002126. Value loss: 0.323854. Entropy: 1.252575.\n",
      "Iteration 375: Policy loss: 0.000572. Value loss: 0.157204. Entropy: 1.248767.\n",
      "episode: 170   score: 95.0  epsilon: 1.0    steps: 88  evaluation reward: 170.0\n",
      "episode: 171   score: 60.0  epsilon: 1.0    steps: 336  evaluation reward: 169.7\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 376: Policy loss: 0.000467. Value loss: 0.669806. Entropy: 1.288777.\n",
      "Iteration 377: Policy loss: -0.004846. Value loss: 0.311671. Entropy: 1.297116.\n",
      "Iteration 378: Policy loss: -0.003149. Value loss: 0.292329. Entropy: 1.281451.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 379: Policy loss: 0.004133. Value loss: 0.864706. Entropy: 1.306637.\n",
      "Iteration 380: Policy loss: 0.000414. Value loss: 0.420613. Entropy: 1.322720.\n",
      "Iteration 381: Policy loss: 0.000654. Value loss: 0.299733. Entropy: 1.319386.\n",
      "episode: 172   score: 665.0  epsilon: 1.0    steps: 72  evaluation reward: 175.0\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 382: Policy loss: 0.002341. Value loss: 0.804189. Entropy: 1.332194.\n",
      "Iteration 383: Policy loss: 0.005110. Value loss: 0.232291. Entropy: 1.336611.\n",
      "Iteration 384: Policy loss: -0.001412. Value loss: 0.132618. Entropy: 1.318938.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 385: Policy loss: -0.000977. Value loss: 1.029181. Entropy: 1.354024.\n",
      "Iteration 386: Policy loss: 0.002003. Value loss: 0.439541. Entropy: 1.346894.\n",
      "Iteration 387: Policy loss: -0.001984. Value loss: 0.234307. Entropy: 1.343435.\n",
      "episode: 173   score: 285.0  epsilon: 1.0    steps: 896  evaluation reward: 177.45\n",
      "episode: 174   score: 190.0  epsilon: 1.0    steps: 912  evaluation reward: 177.8\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 388: Policy loss: 0.004674. Value loss: 0.989444. Entropy: 1.337993.\n",
      "Iteration 389: Policy loss: 0.000986. Value loss: 0.305797. Entropy: 1.353339.\n",
      "Iteration 390: Policy loss: -0.002014. Value loss: 0.224925. Entropy: 1.345467.\n",
      "episode: 175   score: 215.0  epsilon: 1.0    steps: 936  evaluation reward: 175.4\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 391: Policy loss: 0.000056. Value loss: 1.095755. Entropy: 1.348979.\n",
      "Iteration 392: Policy loss: -0.003704. Value loss: 0.573986. Entropy: 1.336055.\n",
      "Iteration 393: Policy loss: -0.006544. Value loss: 0.331845. Entropy: 1.337378.\n",
      "episode: 176   score: 155.0  epsilon: 1.0    steps: 360  evaluation reward: 176.75\n",
      "episode: 177   score: 140.0  epsilon: 1.0    steps: 792  evaluation reward: 177.75\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 394: Policy loss: 0.000193. Value loss: 1.151918. Entropy: 1.328798.\n",
      "Iteration 395: Policy loss: -0.004568. Value loss: 0.496031. Entropy: 1.347161.\n",
      "Iteration 396: Policy loss: -0.002032. Value loss: 0.309697. Entropy: 1.339770.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 397: Policy loss: -0.001861. Value loss: 0.943143. Entropy: 1.347065.\n",
      "Iteration 398: Policy loss: -0.001939. Value loss: 0.369466. Entropy: 1.352112.\n",
      "Iteration 399: Policy loss: -0.005270. Value loss: 0.213047. Entropy: 1.346745.\n",
      "episode: 178   score: 30.0  epsilon: 1.0    steps: 24  evaluation reward: 177.7\n",
      "episode: 179   score: 340.0  epsilon: 1.0    steps: 672  evaluation reward: 179.35\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 400: Policy loss: 0.002375. Value loss: 1.059747. Entropy: 1.339716.\n",
      "Iteration 401: Policy loss: 0.001693. Value loss: 0.314132. Entropy: 1.354425.\n",
      "Iteration 402: Policy loss: -0.003029. Value loss: 0.221652. Entropy: 1.353094.\n",
      "episode: 180   score: 405.0  epsilon: 1.0    steps: 368  evaluation reward: 180.4\n",
      "episode: 181   score: 105.0  epsilon: 1.0    steps: 544  evaluation reward: 176.3\n",
      "episode: 182   score: 30.0  epsilon: 1.0    steps: 800  evaluation reward: 176.15\n",
      "episode: 183   score: 180.0  epsilon: 1.0    steps: 992  evaluation reward: 177.3\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 403: Policy loss: 0.001130. Value loss: 1.038099. Entropy: 1.337686.\n",
      "Iteration 404: Policy loss: 0.002705. Value loss: 0.512986. Entropy: 1.351517.\n",
      "Iteration 405: Policy loss: -0.001564. Value loss: 0.357618. Entropy: 1.343263.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 406: Policy loss: 0.002573. Value loss: 0.948561. Entropy: 1.340417.\n",
      "Iteration 407: Policy loss: 0.001976. Value loss: 0.365359. Entropy: 1.332289.\n",
      "Iteration 408: Policy loss: 0.002513. Value loss: 0.228934. Entropy: 1.337370.\n",
      "episode: 184   score: 70.0  epsilon: 1.0    steps: 728  evaluation reward: 177.2\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 409: Policy loss: 0.001071. Value loss: 0.574219. Entropy: 1.315682.\n",
      "Iteration 410: Policy loss: -0.000178. Value loss: 0.267318. Entropy: 1.332749.\n",
      "Iteration 411: Policy loss: -0.000513. Value loss: 0.164529. Entropy: 1.330914.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 412: Policy loss: 0.001249. Value loss: 0.984479. Entropy: 1.356662.\n",
      "Iteration 413: Policy loss: -0.004445. Value loss: 0.396625. Entropy: 1.336585.\n",
      "Iteration 414: Policy loss: -0.006345. Value loss: 0.282732. Entropy: 1.338059.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 415: Policy loss: -0.001993. Value loss: 0.704775. Entropy: 1.345923.\n",
      "Iteration 416: Policy loss: 0.000730. Value loss: 0.277735. Entropy: 1.355896.\n",
      "Iteration 417: Policy loss: 0.001143. Value loss: 0.225388. Entropy: 1.349697.\n",
      "episode: 185   score: 120.0  epsilon: 1.0    steps: 32  evaluation reward: 175.4\n",
      "episode: 186   score: 230.0  epsilon: 1.0    steps: 392  evaluation reward: 175.2\n",
      "episode: 187   score: 230.0  epsilon: 1.0    steps: 944  evaluation reward: 176.25\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 418: Policy loss: -0.001154. Value loss: 1.176615. Entropy: 1.343870.\n",
      "Iteration 419: Policy loss: -0.004785. Value loss: 0.487329. Entropy: 1.343381.\n",
      "Iteration 420: Policy loss: -0.001447. Value loss: 0.287191. Entropy: 1.339091.\n",
      "episode: 188   score: 75.0  epsilon: 1.0    steps: 568  evaluation reward: 172.85\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 421: Policy loss: 0.000930. Value loss: 0.811071. Entropy: 1.349469.\n",
      "Iteration 422: Policy loss: -0.001646. Value loss: 0.393406. Entropy: 1.361769.\n",
      "Iteration 423: Policy loss: -0.001106. Value loss: 0.260306. Entropy: 1.358864.\n",
      "episode: 189   score: 260.0  epsilon: 1.0    steps: 48  evaluation reward: 173.8\n",
      "episode: 190   score: 190.0  epsilon: 1.0    steps: 920  evaluation reward: 175.05\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 424: Policy loss: -0.000730. Value loss: 0.958925. Entropy: 1.372514.\n",
      "Iteration 425: Policy loss: 0.000896. Value loss: 0.393241. Entropy: 1.371461.\n",
      "Iteration 426: Policy loss: -0.001024. Value loss: 0.263491. Entropy: 1.372696.\n",
      "episode: 191   score: 85.0  epsilon: 1.0    steps: 168  evaluation reward: 175.15\n",
      "episode: 192   score: 335.0  epsilon: 1.0    steps: 928  evaluation reward: 177.9\n",
      "episode: 193   score: 345.0  epsilon: 1.0    steps: 984  evaluation reward: 180.55\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 427: Policy loss: 0.001677. Value loss: 1.253933. Entropy: 1.379858.\n",
      "Iteration 428: Policy loss: 0.000053. Value loss: 0.562217. Entropy: 1.371237.\n",
      "Iteration 429: Policy loss: -0.002031. Value loss: 0.335065. Entropy: 1.372462.\n",
      "episode: 194   score: 130.0  epsilon: 1.0    steps: 472  evaluation reward: 179.5\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 430: Policy loss: -0.000277. Value loss: 0.546075. Entropy: 1.374633.\n",
      "Iteration 431: Policy loss: -0.000521. Value loss: 0.335214. Entropy: 1.377658.\n",
      "Iteration 432: Policy loss: 0.000050. Value loss: 0.227534. Entropy: 1.372724.\n",
      "episode: 195   score: 50.0  epsilon: 1.0    steps: 8  evaluation reward: 179.7\n",
      "episode: 196   score: 80.0  epsilon: 1.0    steps: 808  evaluation reward: 180.1\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 433: Policy loss: -0.003724. Value loss: 1.170484. Entropy: 1.381914.\n",
      "Iteration 434: Policy loss: -0.004204. Value loss: 0.583324. Entropy: 1.380263.\n",
      "Iteration 435: Policy loss: -0.005196. Value loss: 0.372678. Entropy: 1.380107.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 436: Policy loss: 0.001958. Value loss: 0.946251. Entropy: 1.382045.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 437: Policy loss: -0.001550. Value loss: 0.346096. Entropy: 1.382690.\n",
      "Iteration 438: Policy loss: -0.001846. Value loss: 0.272740. Entropy: 1.380611.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 439: Policy loss: -0.001222. Value loss: 0.813801. Entropy: 1.381625.\n",
      "Iteration 440: Policy loss: 0.001537. Value loss: 0.400102. Entropy: 1.381539.\n",
      "Iteration 441: Policy loss: -0.002603. Value loss: 0.319707. Entropy: 1.383786.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 442: Policy loss: 0.000838. Value loss: 1.048621. Entropy: 1.383499.\n",
      "Iteration 443: Policy loss: 0.001516. Value loss: 0.484455. Entropy: 1.382848.\n",
      "Iteration 444: Policy loss: -0.000511. Value loss: 0.285935. Entropy: 1.383954.\n",
      "episode: 197   score: 80.0  epsilon: 1.0    steps: 16  evaluation reward: 178.55\n",
      "episode: 198   score: 155.0  epsilon: 1.0    steps: 720  evaluation reward: 179.8\n",
      "episode: 199   score: 25.0  epsilon: 1.0    steps: 928  evaluation reward: 178.65\n",
      "episode: 200   score: 145.0  epsilon: 1.0    steps: 952  evaluation reward: 179.25\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 445: Policy loss: 0.002109. Value loss: 0.737767. Entropy: 1.380443.\n",
      "Iteration 446: Policy loss: 0.001604. Value loss: 0.315665. Entropy: 1.384178.\n",
      "Iteration 447: Policy loss: 0.000588. Value loss: 0.162089. Entropy: 1.384890.\n",
      "now time :  2019-02-28 10:59:52.157957\n",
      "episode: 201   score: 250.0  epsilon: 1.0    steps: 104  evaluation reward: 175.95\n",
      "episode: 202   score: 350.0  epsilon: 1.0    steps: 168  evaluation reward: 177.2\n",
      "episode: 203   score: 135.0  epsilon: 1.0    steps: 872  evaluation reward: 178.15\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 448: Policy loss: 0.000632. Value loss: 1.076127. Entropy: 1.384129.\n",
      "Iteration 449: Policy loss: 0.000356. Value loss: 0.567413. Entropy: 1.382396.\n",
      "Iteration 450: Policy loss: 0.000528. Value loss: 0.501716. Entropy: 1.383317.\n",
      "episode: 204   score: 460.0  epsilon: 1.0    steps: 16  evaluation reward: 181.15\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 451: Policy loss: -0.000267. Value loss: 1.396637. Entropy: 1.383744.\n",
      "Iteration 452: Policy loss: 0.000139. Value loss: 0.703953. Entropy: 1.380862.\n",
      "Iteration 453: Policy loss: -0.001497. Value loss: 0.391640. Entropy: 1.380929.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 454: Policy loss: -0.002938. Value loss: 1.333706. Entropy: 1.381182.\n",
      "Iteration 455: Policy loss: -0.002758. Value loss: 0.674061. Entropy: 1.380951.\n",
      "Iteration 456: Policy loss: -0.001226. Value loss: 0.392235. Entropy: 1.379605.\n",
      "episode: 205   score: 35.0  epsilon: 1.0    steps: 1000  evaluation reward: 179.1\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 457: Policy loss: -0.001129. Value loss: 1.021833. Entropy: 1.377935.\n",
      "Iteration 458: Policy loss: -0.005666. Value loss: 0.561491. Entropy: 1.375242.\n",
      "Iteration 459: Policy loss: -0.001733. Value loss: 0.377188. Entropy: 1.378005.\n",
      "episode: 206   score: 65.0  epsilon: 1.0    steps: 968  evaluation reward: 176.2\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 460: Policy loss: -0.000508. Value loss: 0.744364. Entropy: 1.378462.\n",
      "Iteration 461: Policy loss: -0.001689. Value loss: 0.427765. Entropy: 1.378581.\n",
      "Iteration 462: Policy loss: -0.000827. Value loss: 0.269434. Entropy: 1.379117.\n",
      "episode: 207   score: 75.0  epsilon: 1.0    steps: 376  evaluation reward: 176.9\n",
      "episode: 208   score: 120.0  epsilon: 1.0    steps: 784  evaluation reward: 177.05\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 463: Policy loss: 0.002316. Value loss: 1.187031. Entropy: 1.379578.\n",
      "Iteration 464: Policy loss: 0.000486. Value loss: 0.446214. Entropy: 1.378901.\n",
      "Iteration 465: Policy loss: 0.000465. Value loss: 0.327090. Entropy: 1.376603.\n",
      "episode: 209   score: 210.0  epsilon: 1.0    steps: 464  evaluation reward: 177.3\n",
      "episode: 210   score: 120.0  epsilon: 1.0    steps: 624  evaluation reward: 176.45\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 466: Policy loss: -0.000017. Value loss: 1.107513. Entropy: 1.370737.\n",
      "Iteration 467: Policy loss: -0.000492. Value loss: 0.517808. Entropy: 1.367978.\n",
      "Iteration 468: Policy loss: -0.000956. Value loss: 0.293475. Entropy: 1.371511.\n",
      "episode: 211   score: 75.0  epsilon: 1.0    steps: 208  evaluation reward: 171.9\n",
      "episode: 212   score: 155.0  epsilon: 1.0    steps: 600  evaluation reward: 171.1\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 469: Policy loss: -0.002337. Value loss: 1.182009. Entropy: 1.369094.\n",
      "Iteration 470: Policy loss: -0.000678. Value loss: 0.564155. Entropy: 1.369610.\n",
      "Iteration 471: Policy loss: -0.002835. Value loss: 0.412994. Entropy: 1.376781.\n",
      "episode: 213   score: 50.0  epsilon: 1.0    steps: 880  evaluation reward: 169.5\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 472: Policy loss: -0.002779. Value loss: 0.785537. Entropy: 1.373922.\n",
      "Iteration 473: Policy loss: -0.002741. Value loss: 0.433179. Entropy: 1.372085.\n",
      "Iteration 474: Policy loss: -0.003336. Value loss: 0.372098. Entropy: 1.373083.\n",
      "episode: 214   score: 105.0  epsilon: 1.0    steps: 984  evaluation reward: 166.0\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 475: Policy loss: -0.001594. Value loss: 0.950875. Entropy: 1.374714.\n",
      "Iteration 476: Policy loss: -0.002281. Value loss: 0.385348. Entropy: 1.376551.\n",
      "Iteration 477: Policy loss: -0.005641. Value loss: 0.250454. Entropy: 1.376899.\n",
      "episode: 215   score: 460.0  epsilon: 1.0    steps: 464  evaluation reward: 168.3\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 478: Policy loss: 0.000595. Value loss: 1.056617. Entropy: 1.375933.\n",
      "Iteration 479: Policy loss: -0.001062. Value loss: 0.514867. Entropy: 1.379474.\n",
      "Iteration 480: Policy loss: -0.002723. Value loss: 0.300505. Entropy: 1.380519.\n",
      "episode: 216   score: 105.0  epsilon: 1.0    steps: 64  evaluation reward: 168.05\n",
      "episode: 217   score: 105.0  epsilon: 1.0    steps: 408  evaluation reward: 168.95\n",
      "episode: 218   score: 85.0  epsilon: 1.0    steps: 504  evaluation reward: 167.8\n",
      "episode: 219   score: 175.0  epsilon: 1.0    steps: 648  evaluation reward: 167.7\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 481: Policy loss: 0.000633. Value loss: 1.139710. Entropy: 1.372362.\n",
      "Iteration 482: Policy loss: -0.001275. Value loss: 0.516634. Entropy: 1.379018.\n",
      "Iteration 483: Policy loss: -0.001321. Value loss: 0.342237. Entropy: 1.378087.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 484: Policy loss: -0.001787. Value loss: 1.211878. Entropy: 1.382362.\n",
      "Iteration 485: Policy loss: -0.006366. Value loss: 0.558904. Entropy: 1.381326.\n",
      "Iteration 486: Policy loss: -0.004954. Value loss: 0.350560. Entropy: 1.380217.\n",
      "episode: 220   score: 50.0  epsilon: 1.0    steps: 688  evaluation reward: 165.75\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 487: Policy loss: 0.001537. Value loss: 1.498460. Entropy: 1.380460.\n",
      "Iteration 488: Policy loss: 0.002502. Value loss: 0.727576. Entropy: 1.377406.\n",
      "Iteration 489: Policy loss: 0.001264. Value loss: 0.337198. Entropy: 1.378876.\n",
      "episode: 221   score: 160.0  epsilon: 1.0    steps: 624  evaluation reward: 166.75\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 490: Policy loss: 0.000305. Value loss: 0.713959. Entropy: 1.374360.\n",
      "Iteration 491: Policy loss: -0.001602. Value loss: 0.367723. Entropy: 1.375667.\n",
      "Iteration 492: Policy loss: -0.003016. Value loss: 0.258700. Entropy: 1.378278.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 493: Policy loss: -0.000349. Value loss: 0.797893. Entropy: 1.377427.\n",
      "Iteration 494: Policy loss: -0.000489. Value loss: 0.399572. Entropy: 1.376060.\n",
      "Iteration 495: Policy loss: -0.001688. Value loss: 0.279470. Entropy: 1.374784.\n",
      "episode: 222   score: 120.0  epsilon: 1.0    steps: 128  evaluation reward: 165.55\n",
      "episode: 223   score: 105.0  epsilon: 1.0    steps: 672  evaluation reward: 164.85\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 496: Policy loss: -0.000204. Value loss: 0.587976. Entropy: 1.379072.\n",
      "Iteration 497: Policy loss: -0.000891. Value loss: 0.271073. Entropy: 1.382624.\n",
      "Iteration 498: Policy loss: -0.002905. Value loss: 0.236888. Entropy: 1.381177.\n",
      "episode: 224   score: 105.0  epsilon: 1.0    steps: 16  evaluation reward: 165.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 225   score: 180.0  epsilon: 1.0    steps: 400  evaluation reward: 163.7\n",
      "episode: 226   score: 155.0  epsilon: 1.0    steps: 904  evaluation reward: 164.05\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 499: Policy loss: -0.003920. Value loss: 0.658022. Entropy: 1.377845.\n",
      "Iteration 500: Policy loss: -0.004209. Value loss: 0.342358. Entropy: 1.375747.\n",
      "Iteration 501: Policy loss: -0.004654. Value loss: 0.295927. Entropy: 1.374444.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 502: Policy loss: -0.003378. Value loss: 1.224157. Entropy: 1.371393.\n",
      "Iteration 503: Policy loss: -0.002440. Value loss: 0.465626. Entropy: 1.374895.\n",
      "Iteration 504: Policy loss: -0.003081. Value loss: 0.318167. Entropy: 1.374768.\n",
      "episode: 227   score: 215.0  epsilon: 1.0    steps: 896  evaluation reward: 165.3\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 505: Policy loss: -0.000556. Value loss: 0.948508. Entropy: 1.370398.\n",
      "Iteration 506: Policy loss: 0.000848. Value loss: 0.584517. Entropy: 1.373212.\n",
      "Iteration 507: Policy loss: 0.000177. Value loss: 0.472027. Entropy: 1.376020.\n",
      "episode: 228   score: 140.0  epsilon: 1.0    steps: 408  evaluation reward: 166.15\n",
      "episode: 229   score: 45.0  epsilon: 1.0    steps: 888  evaluation reward: 164.95\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 508: Policy loss: 0.002350. Value loss: 1.035986. Entropy: 1.377525.\n",
      "Iteration 509: Policy loss: -0.001128. Value loss: 0.332308. Entropy: 1.380914.\n",
      "Iteration 510: Policy loss: 0.000519. Value loss: 0.285659. Entropy: 1.381844.\n",
      "episode: 230   score: 370.0  epsilon: 1.0    steps: 296  evaluation reward: 166.75\n",
      "episode: 231   score: 80.0  epsilon: 1.0    steps: 824  evaluation reward: 166.8\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 511: Policy loss: 0.001093. Value loss: 0.974683. Entropy: 1.376566.\n",
      "Iteration 512: Policy loss: -0.001289. Value loss: 0.384073. Entropy: 1.375024.\n",
      "Iteration 513: Policy loss: 0.001280. Value loss: 0.264499. Entropy: 1.373798.\n",
      "episode: 232   score: 35.0  epsilon: 1.0    steps: 512  evaluation reward: 164.9\n",
      "episode: 233   score: 10.0  epsilon: 1.0    steps: 1000  evaluation reward: 163.2\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 514: Policy loss: 0.000112. Value loss: 0.738429. Entropy: 1.377553.\n",
      "Iteration 515: Policy loss: -0.003359. Value loss: 0.310548. Entropy: 1.381441.\n",
      "Iteration 516: Policy loss: -0.004983. Value loss: 0.207375. Entropy: 1.382353.\n",
      "episode: 234   score: 255.0  epsilon: 1.0    steps: 576  evaluation reward: 165.1\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 517: Policy loss: -0.001575. Value loss: 1.018232. Entropy: 1.382274.\n",
      "Iteration 518: Policy loss: -0.004465. Value loss: 0.439903. Entropy: 1.383059.\n",
      "Iteration 519: Policy loss: -0.004589. Value loss: 0.344003. Entropy: 1.381957.\n",
      "episode: 235   score: 115.0  epsilon: 1.0    steps: 696  evaluation reward: 163.8\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 520: Policy loss: -0.002880. Value loss: 0.823582. Entropy: 1.380919.\n",
      "Iteration 521: Policy loss: -0.004517. Value loss: 0.308409. Entropy: 1.381846.\n",
      "Iteration 522: Policy loss: -0.004754. Value loss: 0.234711. Entropy: 1.381665.\n",
      "episode: 236   score: 185.0  epsilon: 1.0    steps: 392  evaluation reward: 164.0\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 523: Policy loss: -0.000529. Value loss: 0.796539. Entropy: 1.378805.\n",
      "Iteration 524: Policy loss: 0.000708. Value loss: 0.363113. Entropy: 1.375558.\n",
      "Iteration 525: Policy loss: -0.000715. Value loss: 0.274811. Entropy: 1.382432.\n",
      "episode: 237   score: 115.0  epsilon: 1.0    steps: 296  evaluation reward: 163.75\n",
      "episode: 238   score: 130.0  epsilon: 1.0    steps: 408  evaluation reward: 162.65\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 526: Policy loss: -0.000134. Value loss: 0.892405. Entropy: 1.380067.\n",
      "Iteration 527: Policy loss: 0.000739. Value loss: 0.478758. Entropy: 1.376012.\n",
      "Iteration 528: Policy loss: -0.000353. Value loss: 0.342161. Entropy: 1.381810.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 529: Policy loss: 0.001173. Value loss: 0.719375. Entropy: 1.380101.\n",
      "Iteration 530: Policy loss: 0.000132. Value loss: 0.380165. Entropy: 1.381920.\n",
      "Iteration 531: Policy loss: -0.002379. Value loss: 0.304491. Entropy: 1.383863.\n",
      "episode: 239   score: 165.0  epsilon: 1.0    steps: 448  evaluation reward: 163.45\n",
      "episode: 240   score: 155.0  epsilon: 1.0    steps: 592  evaluation reward: 163.9\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 532: Policy loss: -0.001335. Value loss: 0.849510. Entropy: 1.379327.\n",
      "Iteration 533: Policy loss: -0.001894. Value loss: 0.470212. Entropy: 1.377952.\n",
      "Iteration 534: Policy loss: -0.005276. Value loss: 0.268173. Entropy: 1.377201.\n",
      "episode: 241   score: 285.0  epsilon: 1.0    steps: 208  evaluation reward: 165.8\n",
      "episode: 242   score: 435.0  epsilon: 1.0    steps: 928  evaluation reward: 167.8\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 535: Policy loss: 0.002930. Value loss: 1.022405. Entropy: 1.378184.\n",
      "Iteration 536: Policy loss: -0.000098. Value loss: 0.518061. Entropy: 1.372266.\n",
      "Iteration 537: Policy loss: -0.003480. Value loss: 0.272999. Entropy: 1.375248.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 538: Policy loss: -0.002970. Value loss: 0.880828. Entropy: 1.377865.\n",
      "Iteration 539: Policy loss: -0.002056. Value loss: 0.414992. Entropy: 1.379685.\n",
      "Iteration 540: Policy loss: -0.007115. Value loss: 0.271010. Entropy: 1.380386.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 541: Policy loss: 0.000816. Value loss: 0.525000. Entropy: 1.382015.\n",
      "Iteration 542: Policy loss: -0.002568. Value loss: 0.261002. Entropy: 1.380358.\n",
      "Iteration 543: Policy loss: -0.002197. Value loss: 0.156458. Entropy: 1.379058.\n",
      "episode: 243   score: 160.0  epsilon: 1.0    steps: 216  evaluation reward: 168.35\n",
      "episode: 244   score: 95.0  epsilon: 1.0    steps: 320  evaluation reward: 168.4\n",
      "episode: 245   score: 155.0  epsilon: 1.0    steps: 688  evaluation reward: 167.25\n",
      "episode: 246   score: 205.0  epsilon: 1.0    steps: 752  evaluation reward: 168.65\n",
      "episode: 247   score: 180.0  epsilon: 1.0    steps: 976  evaluation reward: 168.2\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 544: Policy loss: 0.000282. Value loss: 1.026725. Entropy: 1.379492.\n",
      "Iteration 545: Policy loss: -0.002752. Value loss: 0.311285. Entropy: 1.376841.\n",
      "Iteration 546: Policy loss: -0.004208. Value loss: 0.316500. Entropy: 1.377814.\n",
      "episode: 248   score: 55.0  epsilon: 1.0    steps: 312  evaluation reward: 168.05\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 547: Policy loss: -0.002341. Value loss: 1.280937. Entropy: 1.378362.\n",
      "Iteration 548: Policy loss: 0.004001. Value loss: 0.533338. Entropy: 1.372741.\n",
      "Iteration 549: Policy loss: -0.002141. Value loss: 0.426597. Entropy: 1.374414.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 550: Policy loss: 0.002143. Value loss: 1.286195. Entropy: 1.376686.\n",
      "Iteration 551: Policy loss: -0.000696. Value loss: 0.745121. Entropy: 1.379596.\n",
      "Iteration 552: Policy loss: -0.000625. Value loss: 0.436909. Entropy: 1.378228.\n",
      "episode: 249   score: 5.0  epsilon: 1.0    steps: 208  evaluation reward: 165.5\n",
      "episode: 250   score: 20.0  epsilon: 1.0    steps: 616  evaluation reward: 160.8\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 553: Policy loss: 0.000993. Value loss: 1.136750. Entropy: 1.379571.\n",
      "Iteration 554: Policy loss: 0.001826. Value loss: 0.368384. Entropy: 1.383290.\n",
      "Iteration 555: Policy loss: -0.000669. Value loss: 0.253425. Entropy: 1.382195.\n",
      "now time :  2019-02-28 11:01:10.487018\n",
      "episode: 251   score: 15.0  epsilon: 1.0    steps: 112  evaluation reward: 158.55\n",
      "episode: 252   score: 215.0  epsilon: 1.0    steps: 384  evaluation reward: 160.5\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 556: Policy loss: 0.000165. Value loss: 0.815839. Entropy: 1.380614.\n",
      "Iteration 557: Policy loss: -0.003178. Value loss: 0.359424. Entropy: 1.380099.\n",
      "Iteration 558: Policy loss: -0.004714. Value loss: 0.237068. Entropy: 1.380069.\n",
      "episode: 253   score: 160.0  epsilon: 1.0    steps: 640  evaluation reward: 161.45\n",
      "episode: 254   score: 195.0  epsilon: 1.0    steps: 688  evaluation reward: 162.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 559: Policy loss: 0.001046. Value loss: 0.789505. Entropy: 1.374625.\n",
      "Iteration 560: Policy loss: 0.003708. Value loss: 0.348092. Entropy: 1.373293.\n",
      "Iteration 561: Policy loss: 0.000541. Value loss: 0.302961. Entropy: 1.370317.\n",
      "episode: 255   score: 180.0  epsilon: 1.0    steps: 400  evaluation reward: 162.5\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 562: Policy loss: -0.002642. Value loss: 0.646548. Entropy: 1.374341.\n",
      "Iteration 563: Policy loss: -0.005103. Value loss: 0.306671. Entropy: 1.375334.\n",
      "Iteration 564: Policy loss: -0.003396. Value loss: 0.198019. Entropy: 1.371187.\n",
      "episode: 256   score: 135.0  epsilon: 1.0    steps: 96  evaluation reward: 163.75\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 565: Policy loss: -0.003891. Value loss: 0.904185. Entropy: 1.369268.\n",
      "Iteration 566: Policy loss: -0.003303. Value loss: 0.511294. Entropy: 1.368344.\n",
      "Iteration 567: Policy loss: -0.005577. Value loss: 0.353717. Entropy: 1.367091.\n",
      "episode: 257   score: 80.0  epsilon: 1.0    steps: 752  evaluation reward: 162.45\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 568: Policy loss: -0.000834. Value loss: 0.826055. Entropy: 1.366518.\n",
      "Iteration 569: Policy loss: 0.002368. Value loss: 0.457796. Entropy: 1.365808.\n",
      "Iteration 570: Policy loss: 0.001091. Value loss: 0.351569. Entropy: 1.371616.\n",
      "episode: 258   score: 105.0  epsilon: 1.0    steps: 112  evaluation reward: 163.15\n",
      "episode: 259   score: 300.0  epsilon: 1.0    steps: 792  evaluation reward: 164.25\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 571: Policy loss: -0.003668. Value loss: 0.617227. Entropy: 1.361055.\n",
      "Iteration 572: Policy loss: -0.002703. Value loss: 0.272434. Entropy: 1.362458.\n",
      "Iteration 573: Policy loss: -0.007154. Value loss: 0.161108. Entropy: 1.358258.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 574: Policy loss: 0.000196. Value loss: 0.705751. Entropy: 1.362929.\n",
      "Iteration 575: Policy loss: -0.001830. Value loss: 0.286448. Entropy: 1.366106.\n",
      "Iteration 576: Policy loss: -0.002868. Value loss: 0.260696. Entropy: 1.362628.\n",
      "episode: 260   score: 225.0  epsilon: 1.0    steps: 608  evaluation reward: 166.2\n",
      "episode: 261   score: 140.0  epsilon: 1.0    steps: 920  evaluation reward: 166.4\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 577: Policy loss: 0.000339. Value loss: 0.619538. Entropy: 1.356481.\n",
      "Iteration 578: Policy loss: -0.002937. Value loss: 0.196753. Entropy: 1.359472.\n",
      "Iteration 579: Policy loss: -0.005172. Value loss: 0.148851. Entropy: 1.354459.\n",
      "episode: 262   score: 105.0  epsilon: 1.0    steps: 16  evaluation reward: 164.85\n",
      "episode: 263   score: 180.0  epsilon: 1.0    steps: 856  evaluation reward: 161.8\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 580: Policy loss: -0.000160. Value loss: 0.545675. Entropy: 1.369354.\n",
      "Iteration 581: Policy loss: 0.003730. Value loss: 0.223147. Entropy: 1.356549.\n",
      "Iteration 582: Policy loss: 0.000763. Value loss: 0.203086. Entropy: 1.365821.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 583: Policy loss: 0.002278. Value loss: 0.617887. Entropy: 1.373767.\n",
      "Iteration 584: Policy loss: -0.000063. Value loss: 0.350326. Entropy: 1.373581.\n",
      "Iteration 585: Policy loss: -0.000640. Value loss: 0.248948. Entropy: 1.367568.\n",
      "episode: 264   score: 30.0  epsilon: 1.0    steps: 112  evaluation reward: 160.65\n",
      "episode: 265   score: 120.0  epsilon: 1.0    steps: 312  evaluation reward: 160.65\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 586: Policy loss: 0.000397. Value loss: 0.706170. Entropy: 1.361371.\n",
      "Iteration 587: Policy loss: -0.001684. Value loss: 0.357662. Entropy: 1.359923.\n",
      "Iteration 588: Policy loss: -0.001000. Value loss: 0.313909. Entropy: 1.361736.\n",
      "episode: 266   score: 180.0  epsilon: 1.0    steps: 232  evaluation reward: 161.2\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 589: Policy loss: 0.000433. Value loss: 0.714831. Entropy: 1.355060.\n",
      "Iteration 590: Policy loss: -0.000302. Value loss: 0.242628. Entropy: 1.353284.\n",
      "Iteration 591: Policy loss: 0.001132. Value loss: 0.155456. Entropy: 1.366961.\n",
      "episode: 267   score: 295.0  epsilon: 1.0    steps: 32  evaluation reward: 162.65\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 592: Policy loss: -0.000170. Value loss: 0.680154. Entropy: 1.366174.\n",
      "Iteration 593: Policy loss: -0.001911. Value loss: 0.503832. Entropy: 1.364165.\n",
      "Iteration 594: Policy loss: 0.000594. Value loss: 0.403259. Entropy: 1.365420.\n",
      "episode: 268   score: 105.0  epsilon: 1.0    steps: 152  evaluation reward: 161.35\n",
      "episode: 269   score: 75.0  epsilon: 1.0    steps: 368  evaluation reward: 159.6\n",
      "episode: 270   score: 135.0  epsilon: 1.0    steps: 768  evaluation reward: 160.0\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 595: Policy loss: 0.001889. Value loss: 0.631371. Entropy: 1.363115.\n",
      "Iteration 596: Policy loss: -0.002004. Value loss: 0.212269. Entropy: 1.363950.\n",
      "Iteration 597: Policy loss: -0.000687. Value loss: 0.180081. Entropy: 1.360418.\n",
      "episode: 271   score: 210.0  epsilon: 1.0    steps: 752  evaluation reward: 161.5\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 598: Policy loss: 0.000054. Value loss: 0.714321. Entropy: 1.357103.\n",
      "Iteration 599: Policy loss: 0.000076. Value loss: 0.466612. Entropy: 1.357474.\n",
      "Iteration 600: Policy loss: -0.002616. Value loss: 0.296322. Entropy: 1.353595.\n",
      "episode: 272   score: 160.0  epsilon: 1.0    steps: 112  evaluation reward: 156.45\n",
      "episode: 273   score: 75.0  epsilon: 1.0    steps: 344  evaluation reward: 154.35\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 601: Policy loss: 0.003261. Value loss: 0.581520. Entropy: 1.354573.\n",
      "Iteration 602: Policy loss: -0.001848. Value loss: 0.296007. Entropy: 1.357187.\n",
      "Iteration 603: Policy loss: -0.002860. Value loss: 0.231438. Entropy: 1.354889.\n",
      "episode: 274   score: 110.0  epsilon: 1.0    steps: 616  evaluation reward: 153.55\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 604: Policy loss: 0.000096. Value loss: 0.692869. Entropy: 1.345669.\n",
      "Iteration 605: Policy loss: -0.002536. Value loss: 0.364839. Entropy: 1.347430.\n",
      "Iteration 606: Policy loss: 0.001594. Value loss: 0.215174. Entropy: 1.344222.\n",
      "episode: 275   score: 180.0  epsilon: 1.0    steps: 424  evaluation reward: 153.2\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 607: Policy loss: 0.000629. Value loss: 0.638795. Entropy: 1.344529.\n",
      "Iteration 608: Policy loss: 0.000134. Value loss: 0.354448. Entropy: 1.342728.\n",
      "Iteration 609: Policy loss: -0.002510. Value loss: 0.229023. Entropy: 1.334422.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 610: Policy loss: 0.000776. Value loss: 0.528676. Entropy: 1.329476.\n",
      "Iteration 611: Policy loss: -0.001190. Value loss: 0.301435. Entropy: 1.339507.\n",
      "Iteration 612: Policy loss: -0.005359. Value loss: 0.224854. Entropy: 1.347779.\n",
      "episode: 276   score: 160.0  epsilon: 1.0    steps: 368  evaluation reward: 153.25\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 613: Policy loss: -0.001307. Value loss: 0.608470. Entropy: 1.345448.\n",
      "Iteration 614: Policy loss: -0.002209. Value loss: 0.261708. Entropy: 1.349439.\n",
      "Iteration 615: Policy loss: -0.005476. Value loss: 0.179322. Entropy: 1.355832.\n",
      "episode: 277   score: 105.0  epsilon: 1.0    steps: 56  evaluation reward: 152.9\n",
      "episode: 278   score: 105.0  epsilon: 1.0    steps: 256  evaluation reward: 153.65\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 616: Policy loss: -0.001563. Value loss: 0.643882. Entropy: 1.351731.\n",
      "Iteration 617: Policy loss: 0.000187. Value loss: 0.335191. Entropy: 1.355165.\n",
      "Iteration 618: Policy loss: -0.002900. Value loss: 0.167260. Entropy: 1.355086.\n",
      "episode: 279   score: 165.0  epsilon: 1.0    steps: 496  evaluation reward: 151.9\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 619: Policy loss: 0.001761. Value loss: 1.188237. Entropy: 1.333317.\n",
      "Iteration 620: Policy loss: -0.000542. Value loss: 0.536902. Entropy: 1.339445.\n",
      "Iteration 621: Policy loss: -0.001770. Value loss: 0.465445. Entropy: 1.334862.\n",
      "episode: 280   score: 335.0  epsilon: 1.0    steps: 776  evaluation reward: 151.2\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 622: Policy loss: 0.002103. Value loss: 1.222743. Entropy: 1.336618.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 623: Policy loss: 0.000794. Value loss: 0.548118. Entropy: 1.334084.\n",
      "Iteration 624: Policy loss: -0.001581. Value loss: 0.356963. Entropy: 1.334742.\n",
      "episode: 281   score: 330.0  epsilon: 1.0    steps: 824  evaluation reward: 153.45\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 625: Policy loss: -0.000850. Value loss: 0.560053. Entropy: 1.332955.\n",
      "Iteration 626: Policy loss: -0.000344. Value loss: 0.320324. Entropy: 1.323729.\n",
      "Iteration 627: Policy loss: -0.004009. Value loss: 0.206762. Entropy: 1.318530.\n",
      "episode: 282   score: 180.0  epsilon: 1.0    steps: 592  evaluation reward: 154.95\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 628: Policy loss: 0.001298. Value loss: 0.645914. Entropy: 1.315797.\n",
      "Iteration 629: Policy loss: -0.000518. Value loss: 0.284427. Entropy: 1.310604.\n",
      "Iteration 630: Policy loss: -0.000169. Value loss: 0.200117. Entropy: 1.311901.\n",
      "episode: 283   score: 105.0  epsilon: 1.0    steps: 248  evaluation reward: 154.2\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 631: Policy loss: 0.000738. Value loss: 0.786049. Entropy: 1.313480.\n",
      "Iteration 632: Policy loss: 0.002052. Value loss: 0.345504. Entropy: 1.315794.\n",
      "Iteration 633: Policy loss: -0.002728. Value loss: 0.173915. Entropy: 1.330359.\n",
      "episode: 284   score: 225.0  epsilon: 1.0    steps: 224  evaluation reward: 155.75\n",
      "episode: 285   score: 105.0  epsilon: 1.0    steps: 656  evaluation reward: 155.6\n",
      "episode: 286   score: 60.0  epsilon: 1.0    steps: 888  evaluation reward: 153.9\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 634: Policy loss: 0.001394. Value loss: 0.670879. Entropy: 1.326000.\n",
      "Iteration 635: Policy loss: -0.001296. Value loss: 0.341312. Entropy: 1.330742.\n",
      "Iteration 636: Policy loss: -0.000498. Value loss: 0.236282. Entropy: 1.320674.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 637: Policy loss: 0.001666. Value loss: 0.608412. Entropy: 1.321473.\n",
      "Iteration 638: Policy loss: 0.001078. Value loss: 0.307031. Entropy: 1.324321.\n",
      "Iteration 639: Policy loss: 0.000273. Value loss: 0.255371. Entropy: 1.328225.\n",
      "episode: 287   score: 155.0  epsilon: 1.0    steps: 960  evaluation reward: 153.15\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 640: Policy loss: 0.001823. Value loss: 0.626312. Entropy: 1.322979.\n",
      "Iteration 641: Policy loss: -0.002882. Value loss: 0.304108. Entropy: 1.334735.\n",
      "Iteration 642: Policy loss: -0.005766. Value loss: 0.252241. Entropy: 1.340281.\n",
      "episode: 288   score: 30.0  epsilon: 1.0    steps: 320  evaluation reward: 152.7\n",
      "episode: 289   score: 50.0  epsilon: 1.0    steps: 888  evaluation reward: 150.6\n",
      "episode: 290   score: 30.0  epsilon: 1.0    steps: 984  evaluation reward: 149.0\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 643: Policy loss: 0.001316. Value loss: 0.434003. Entropy: 1.336473.\n",
      "Iteration 644: Policy loss: -0.000383. Value loss: 0.146740. Entropy: 1.333733.\n",
      "Iteration 645: Policy loss: -0.001853. Value loss: 0.154993. Entropy: 1.334281.\n",
      "episode: 291   score: 105.0  epsilon: 1.0    steps: 168  evaluation reward: 149.2\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 646: Policy loss: -0.000156. Value loss: 0.730663. Entropy: 1.338162.\n",
      "Iteration 647: Policy loss: -0.002091. Value loss: 0.397038. Entropy: 1.332132.\n",
      "Iteration 648: Policy loss: -0.004309. Value loss: 0.294395. Entropy: 1.345207.\n",
      "episode: 292   score: 380.0  epsilon: 1.0    steps: 56  evaluation reward: 149.65\n",
      "episode: 293   score: 660.0  epsilon: 1.0    steps: 584  evaluation reward: 152.8\n",
      "episode: 294   score: 335.0  epsilon: 1.0    steps: 872  evaluation reward: 154.85\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 649: Policy loss: -0.001087. Value loss: 0.570098. Entropy: 1.331819.\n",
      "Iteration 650: Policy loss: 0.000903. Value loss: 0.312257. Entropy: 1.337718.\n",
      "Iteration 651: Policy loss: -0.004015. Value loss: 0.211640. Entropy: 1.347305.\n",
      "episode: 295   score: 50.0  epsilon: 1.0    steps: 944  evaluation reward: 154.85\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 652: Policy loss: 0.001474. Value loss: 0.587227. Entropy: 1.328157.\n",
      "Iteration 653: Policy loss: 0.000716. Value loss: 0.241984. Entropy: 1.330852.\n",
      "Iteration 654: Policy loss: -0.006078. Value loss: 0.199431. Entropy: 1.335797.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 655: Policy loss: -0.002429. Value loss: 0.827233. Entropy: 1.325600.\n",
      "Iteration 656: Policy loss: -0.008507. Value loss: 0.367372. Entropy: 1.316236.\n",
      "Iteration 657: Policy loss: -0.008433. Value loss: 0.219318. Entropy: 1.317913.\n",
      "episode: 296   score: 105.0  epsilon: 1.0    steps: 352  evaluation reward: 155.1\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 658: Policy loss: -0.000097. Value loss: 0.559530. Entropy: 1.309242.\n",
      "Iteration 659: Policy loss: -0.005098. Value loss: 0.275629. Entropy: 1.322376.\n",
      "Iteration 660: Policy loss: -0.003039. Value loss: 0.159499. Entropy: 1.321767.\n",
      "episode: 297   score: 180.0  epsilon: 1.0    steps: 552  evaluation reward: 156.1\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 661: Policy loss: 0.000703. Value loss: 0.838510. Entropy: 1.328733.\n",
      "Iteration 662: Policy loss: 0.002251. Value loss: 0.332563. Entropy: 1.330636.\n",
      "Iteration 663: Policy loss: 0.000426. Value loss: 0.227328. Entropy: 1.331516.\n",
      "episode: 298   score: 155.0  epsilon: 1.0    steps: 288  evaluation reward: 156.1\n",
      "episode: 299   score: 210.0  epsilon: 1.0    steps: 1000  evaluation reward: 157.95\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 664: Policy loss: -0.000285. Value loss: 0.395656. Entropy: 1.334602.\n",
      "Iteration 665: Policy loss: -0.002370. Value loss: 0.257307. Entropy: 1.321248.\n",
      "Iteration 666: Policy loss: -0.001304. Value loss: 0.236850. Entropy: 1.316319.\n",
      "episode: 300   score: 155.0  epsilon: 1.0    steps: 240  evaluation reward: 158.05\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 667: Policy loss: 0.002086. Value loss: 0.840977. Entropy: 1.325346.\n",
      "Iteration 668: Policy loss: -0.007438. Value loss: 0.269267. Entropy: 1.321201.\n",
      "Iteration 669: Policy loss: -0.003721. Value loss: 0.138631. Entropy: 1.321005.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 670: Policy loss: -0.000777. Value loss: 0.560131. Entropy: 1.306414.\n",
      "Iteration 671: Policy loss: -0.003669. Value loss: 0.189423. Entropy: 1.308044.\n",
      "Iteration 672: Policy loss: -0.004364. Value loss: 0.125329. Entropy: 1.307604.\n",
      "now time :  2019-02-28 11:02:34.988777\n",
      "episode: 301   score: 180.0  epsilon: 1.0    steps: 160  evaluation reward: 157.35\n",
      "episode: 302   score: 430.0  epsilon: 1.0    steps: 792  evaluation reward: 158.15\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 673: Policy loss: 0.004391. Value loss: 0.863412. Entropy: 1.316338.\n",
      "Iteration 674: Policy loss: -0.000282. Value loss: 0.358758. Entropy: 1.332731.\n",
      "Iteration 675: Policy loss: -0.000771. Value loss: 0.275790. Entropy: 1.322992.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 676: Policy loss: -0.000894. Value loss: 0.985605. Entropy: 1.311000.\n",
      "Iteration 677: Policy loss: -0.004476. Value loss: 0.408696. Entropy: 1.303695.\n",
      "Iteration 678: Policy loss: -0.003799. Value loss: 0.213620. Entropy: 1.305318.\n",
      "episode: 303   score: 210.0  epsilon: 1.0    steps: 520  evaluation reward: 158.9\n",
      "episode: 304   score: 45.0  epsilon: 1.0    steps: 760  evaluation reward: 154.75\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 679: Policy loss: 0.000004. Value loss: 0.667200. Entropy: 1.326636.\n",
      "Iteration 680: Policy loss: -0.001328. Value loss: 0.270687. Entropy: 1.326802.\n",
      "Iteration 681: Policy loss: -0.001570. Value loss: 0.198151. Entropy: 1.330173.\n",
      "episode: 305   score: 310.0  epsilon: 1.0    steps: 160  evaluation reward: 157.5\n",
      "episode: 306   score: 410.0  epsilon: 1.0    steps: 624  evaluation reward: 160.95\n",
      "episode: 307   score: 155.0  epsilon: 1.0    steps: 784  evaluation reward: 161.75\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 682: Policy loss: 0.000671. Value loss: 0.837058. Entropy: 1.340873.\n",
      "Iteration 683: Policy loss: -0.000694. Value loss: 0.330075. Entropy: 1.333803.\n",
      "Iteration 684: Policy loss: -0.002761. Value loss: 0.226661. Entropy: 1.327502.\n",
      "episode: 308   score: 135.0  epsilon: 1.0    steps: 192  evaluation reward: 161.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 685: Policy loss: -0.000775. Value loss: 0.626401. Entropy: 1.348105.\n",
      "Iteration 686: Policy loss: -0.001441. Value loss: 0.289914. Entropy: 1.345137.\n",
      "Iteration 687: Policy loss: -0.001302. Value loss: 0.250950. Entropy: 1.342915.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 688: Policy loss: -0.002560. Value loss: 0.665067. Entropy: 1.345379.\n",
      "Iteration 689: Policy loss: -0.003046. Value loss: 0.237568. Entropy: 1.349173.\n",
      "Iteration 690: Policy loss: -0.004434. Value loss: 0.170077. Entropy: 1.344705.\n",
      "episode: 309   score: 210.0  epsilon: 1.0    steps: 232  evaluation reward: 161.9\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 691: Policy loss: 0.001160. Value loss: 0.421941. Entropy: 1.350475.\n",
      "Iteration 692: Policy loss: -0.002144. Value loss: 0.162802. Entropy: 1.340665.\n",
      "Iteration 693: Policy loss: 0.000390. Value loss: 0.121355. Entropy: 1.345542.\n",
      "episode: 310   score: 50.0  epsilon: 1.0    steps: 632  evaluation reward: 161.2\n",
      "episode: 311   score: 55.0  epsilon: 1.0    steps: 976  evaluation reward: 161.0\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 694: Policy loss: 0.000166. Value loss: 0.690981. Entropy: 1.341731.\n",
      "Iteration 695: Policy loss: -0.005490. Value loss: 0.310835. Entropy: 1.343334.\n",
      "Iteration 696: Policy loss: -0.002731. Value loss: 0.263843. Entropy: 1.344263.\n",
      "episode: 312   score: 155.0  epsilon: 1.0    steps: 472  evaluation reward: 161.0\n",
      "episode: 313   score: 155.0  epsilon: 1.0    steps: 816  evaluation reward: 162.05\n",
      "episode: 314   score: 160.0  epsilon: 1.0    steps: 848  evaluation reward: 162.6\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 697: Policy loss: -0.001987. Value loss: 0.823136. Entropy: 1.337575.\n",
      "Iteration 698: Policy loss: -0.002368. Value loss: 0.322846. Entropy: 1.342682.\n",
      "Iteration 699: Policy loss: -0.004389. Value loss: 0.292955. Entropy: 1.340900.\n",
      "episode: 315   score: 315.0  epsilon: 1.0    steps: 192  evaluation reward: 161.15\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 700: Policy loss: 0.000300. Value loss: 0.481734. Entropy: 1.330689.\n",
      "Iteration 701: Policy loss: -0.003619. Value loss: 0.288794. Entropy: 1.330974.\n",
      "Iteration 702: Policy loss: -0.003706. Value loss: 0.235923. Entropy: 1.332114.\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 703: Policy loss: 0.000183. Value loss: 0.666500. Entropy: 1.353759.\n",
      "Iteration 704: Policy loss: 0.000490. Value loss: 0.278604. Entropy: 1.353947.\n",
      "Iteration 705: Policy loss: -0.003167. Value loss: 0.192917. Entropy: 1.352213.\n",
      "episode: 316   score: 210.0  epsilon: 1.0    steps: 48  evaluation reward: 162.2\n",
      "episode: 317   score: 50.0  epsilon: 1.0    steps: 136  evaluation reward: 161.65\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 706: Policy loss: 0.002294. Value loss: 0.872373. Entropy: 1.355203.\n",
      "Iteration 707: Policy loss: 0.001671. Value loss: 0.322165. Entropy: 1.364100.\n",
      "Iteration 708: Policy loss: 0.003222. Value loss: 0.182771. Entropy: 1.364828.\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 709: Policy loss: 0.002782. Value loss: 0.567420. Entropy: 1.355747.\n",
      "Iteration 710: Policy loss: -0.001593. Value loss: 0.243460. Entropy: 1.359150.\n",
      "Iteration 711: Policy loss: -0.003767. Value loss: 0.192631. Entropy: 1.357668.\n",
      "episode: 318   score: 130.0  epsilon: 1.0    steps: 912  evaluation reward: 162.1\n",
      "episode: 319   score: 155.0  epsilon: 1.0    steps: 928  evaluation reward: 161.9\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 712: Policy loss: 0.000294. Value loss: 1.248173. Entropy: 1.362300.\n",
      "Iteration 713: Policy loss: -0.002814. Value loss: 0.373420. Entropy: 1.356086.\n",
      "Iteration 714: Policy loss: -0.002615. Value loss: 0.295130. Entropy: 1.363577.\n",
      "episode: 320   score: 105.0  epsilon: 1.0    steps: 136  evaluation reward: 162.45\n",
      "episode: 321   score: 55.0  epsilon: 1.0    steps: 320  evaluation reward: 161.4\n",
      "episode: 322   score: 255.0  epsilon: 1.0    steps: 504  evaluation reward: 162.75\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 715: Policy loss: 0.001437. Value loss: 0.715136. Entropy: 1.368660.\n",
      "Iteration 716: Policy loss: -0.002531. Value loss: 0.286352. Entropy: 1.363732.\n",
      "Iteration 717: Policy loss: -0.005853. Value loss: 0.190245. Entropy: 1.356906.\n",
      "episode: 323   score: 210.0  epsilon: 1.0    steps: 696  evaluation reward: 163.8\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 718: Policy loss: -0.002148. Value loss: 0.799020. Entropy: 1.356205.\n",
      "Iteration 719: Policy loss: -0.001882. Value loss: 0.315801. Entropy: 1.347537.\n",
      "Iteration 720: Policy loss: -0.004963. Value loss: 0.222714. Entropy: 1.344115.\n",
      "episode: 324   score: 145.0  epsilon: 1.0    steps: 696  evaluation reward: 164.2\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 721: Policy loss: 0.000211. Value loss: 0.952477. Entropy: 1.351179.\n",
      "Iteration 722: Policy loss: -0.001427. Value loss: 0.322512. Entropy: 1.347639.\n",
      "Iteration 723: Policy loss: -0.000479. Value loss: 0.242014. Entropy: 1.348008.\n",
      "episode: 325   score: 75.0  epsilon: 1.0    steps: 216  evaluation reward: 163.15\n",
      "episode: 326   score: 105.0  epsilon: 1.0    steps: 920  evaluation reward: 162.65\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 724: Policy loss: -0.000291. Value loss: 0.781154. Entropy: 1.353667.\n",
      "Iteration 725: Policy loss: -0.004725. Value loss: 0.260297. Entropy: 1.359078.\n",
      "Iteration 726: Policy loss: -0.001632. Value loss: 0.152376. Entropy: 1.361415.\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 727: Policy loss: 0.000819. Value loss: 0.793699. Entropy: 1.371164.\n",
      "Iteration 728: Policy loss: -0.000701. Value loss: 0.413151. Entropy: 1.374973.\n",
      "Iteration 729: Policy loss: -0.000318. Value loss: 0.243015. Entropy: 1.368353.\n",
      "episode: 327   score: 155.0  epsilon: 1.0    steps: 864  evaluation reward: 162.05\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 730: Policy loss: -0.000533. Value loss: 0.683334. Entropy: 1.369462.\n",
      "Iteration 731: Policy loss: -0.001626. Value loss: 0.358949. Entropy: 1.374558.\n",
      "Iteration 732: Policy loss: -0.004254. Value loss: 0.229491. Entropy: 1.371482.\n",
      "episode: 328   score: 210.0  epsilon: 1.0    steps: 448  evaluation reward: 162.75\n",
      "episode: 329   score: 50.0  epsilon: 1.0    steps: 696  evaluation reward: 162.8\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 733: Policy loss: 0.001002. Value loss: 0.677561. Entropy: 1.375436.\n",
      "Iteration 734: Policy loss: 0.000111. Value loss: 0.314255. Entropy: 1.375554.\n",
      "Iteration 735: Policy loss: -0.003422. Value loss: 0.200672. Entropy: 1.377542.\n",
      "episode: 330   score: 60.0  epsilon: 1.0    steps: 88  evaluation reward: 159.7\n",
      "episode: 331   score: 440.0  epsilon: 1.0    steps: 848  evaluation reward: 163.3\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 736: Policy loss: 0.000363. Value loss: 1.170759. Entropy: 1.380175.\n",
      "Iteration 737: Policy loss: -0.004696. Value loss: 0.419706. Entropy: 1.380844.\n",
      "Iteration 738: Policy loss: -0.004206. Value loss: 0.288330. Entropy: 1.382577.\n",
      "episode: 332   score: 85.0  epsilon: 1.0    steps: 160  evaluation reward: 163.8\n",
      "episode: 333   score: 640.0  epsilon: 1.0    steps: 768  evaluation reward: 170.1\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 739: Policy loss: -0.000544. Value loss: 0.546814. Entropy: 1.382918.\n",
      "Iteration 740: Policy loss: -0.000536. Value loss: 0.273605. Entropy: 1.383918.\n",
      "Iteration 741: Policy loss: -0.001560. Value loss: 0.196305. Entropy: 1.383752.\n",
      "episode: 334   score: 85.0  epsilon: 1.0    steps: 840  evaluation reward: 168.4\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 742: Policy loss: -0.001750. Value loss: 1.095115. Entropy: 1.383367.\n",
      "Iteration 743: Policy loss: -0.006494. Value loss: 0.494139. Entropy: 1.382702.\n",
      "Iteration 744: Policy loss: -0.008057. Value loss: 0.372559. Entropy: 1.383210.\n",
      "episode: 335   score: 50.0  epsilon: 1.0    steps: 144  evaluation reward: 167.75\n",
      "episode: 336   score: 60.0  epsilon: 1.0    steps: 936  evaluation reward: 166.5\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 745: Policy loss: -0.001207. Value loss: 0.834394. Entropy: 1.381592.\n",
      "Iteration 746: Policy loss: 0.000141. Value loss: 0.377567. Entropy: 1.381402.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 747: Policy loss: -0.000359. Value loss: 0.363945. Entropy: 1.383755.\n",
      "episode: 337   score: 0.0  epsilon: 1.0    steps: 48  evaluation reward: 165.35\n",
      "episode: 338   score: 240.0  epsilon: 1.0    steps: 264  evaluation reward: 166.45\n",
      "episode: 339   score: 65.0  epsilon: 1.0    steps: 536  evaluation reward: 165.45\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 748: Policy loss: 0.001340. Value loss: 0.872315. Entropy: 1.383852.\n",
      "Iteration 749: Policy loss: 0.000688. Value loss: 0.397908. Entropy: 1.381920.\n",
      "Iteration 750: Policy loss: -0.001594. Value loss: 0.276667. Entropy: 1.382097.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 751: Policy loss: 0.001424. Value loss: 0.765044. Entropy: 1.379500.\n",
      "Iteration 752: Policy loss: -0.001809. Value loss: 0.311080. Entropy: 1.377003.\n",
      "Iteration 753: Policy loss: -0.001099. Value loss: 0.233982. Entropy: 1.372923.\n",
      "episode: 340   score: 105.0  epsilon: 1.0    steps: 624  evaluation reward: 164.95\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 754: Policy loss: -0.002359. Value loss: 0.815443. Entropy: 1.374080.\n",
      "Iteration 755: Policy loss: -0.004189. Value loss: 0.363176. Entropy: 1.370603.\n",
      "Iteration 756: Policy loss: -0.004672. Value loss: 0.240923. Entropy: 1.370857.\n",
      "episode: 341   score: 90.0  epsilon: 1.0    steps: 976  evaluation reward: 163.0\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 757: Policy loss: -0.000323. Value loss: 0.756653. Entropy: 1.371456.\n",
      "Iteration 758: Policy loss: -0.001130. Value loss: 0.196257. Entropy: 1.370492.\n",
      "Iteration 759: Policy loss: -0.001280. Value loss: 0.153432. Entropy: 1.369690.\n",
      "episode: 342   score: 110.0  epsilon: 1.0    steps: 440  evaluation reward: 159.75\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 760: Policy loss: -0.000015. Value loss: 0.518886. Entropy: 1.369782.\n",
      "Iteration 761: Policy loss: -0.001718. Value loss: 0.144686. Entropy: 1.360960.\n",
      "Iteration 762: Policy loss: -0.007181. Value loss: 0.057667. Entropy: 1.363684.\n",
      "episode: 343   score: 120.0  epsilon: 1.0    steps: 656  evaluation reward: 159.35\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 763: Policy loss: -0.001463. Value loss: 0.869375. Entropy: 1.362027.\n",
      "Iteration 764: Policy loss: -0.004357. Value loss: 0.327813. Entropy: 1.363617.\n",
      "Iteration 765: Policy loss: -0.004804. Value loss: 0.277264. Entropy: 1.367313.\n",
      "episode: 344   score: 105.0  epsilon: 1.0    steps: 392  evaluation reward: 159.45\n",
      "episode: 345   score: 210.0  epsilon: 1.0    steps: 528  evaluation reward: 160.0\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 766: Policy loss: 0.001620. Value loss: 0.770506. Entropy: 1.365171.\n",
      "Iteration 767: Policy loss: -0.000451. Value loss: 0.326376. Entropy: 1.371639.\n",
      "Iteration 768: Policy loss: -0.002591. Value loss: 0.157160. Entropy: 1.364832.\n",
      "episode: 346   score: 75.0  epsilon: 1.0    steps: 496  evaluation reward: 158.7\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 769: Policy loss: -0.000231. Value loss: 0.767613. Entropy: 1.353837.\n",
      "Iteration 770: Policy loss: -0.003131. Value loss: 0.353008. Entropy: 1.358667.\n",
      "Iteration 771: Policy loss: -0.004159. Value loss: 0.248910. Entropy: 1.358471.\n",
      "episode: 347   score: 410.0  epsilon: 1.0    steps: 368  evaluation reward: 161.0\n",
      "episode: 348   score: 295.0  epsilon: 1.0    steps: 936  evaluation reward: 163.4\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 772: Policy loss: 0.001579. Value loss: 0.604889. Entropy: 1.349518.\n",
      "Iteration 773: Policy loss: -0.005380. Value loss: 0.237008. Entropy: 1.349441.\n",
      "Iteration 774: Policy loss: -0.004491. Value loss: 0.148811. Entropy: 1.343620.\n",
      "episode: 349   score: 125.0  epsilon: 1.0    steps: 936  evaluation reward: 164.6\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 775: Policy loss: -0.000118. Value loss: 0.557804. Entropy: 1.351944.\n",
      "Iteration 776: Policy loss: -0.002179. Value loss: 0.165156. Entropy: 1.364903.\n",
      "Iteration 777: Policy loss: -0.000778. Value loss: 0.120350. Entropy: 1.363517.\n",
      "episode: 350   score: 110.0  epsilon: 1.0    steps: 280  evaluation reward: 165.5\n",
      "now time :  2019-02-28 11:03:51.459387\n",
      "episode: 351   score: 30.0  epsilon: 1.0    steps: 448  evaluation reward: 165.65\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 778: Policy loss: -0.000161. Value loss: 0.709852. Entropy: 1.355804.\n",
      "Iteration 779: Policy loss: -0.001063. Value loss: 0.441444. Entropy: 1.358003.\n",
      "Iteration 780: Policy loss: -0.003886. Value loss: 0.290910. Entropy: 1.358539.\n",
      "episode: 352   score: 105.0  epsilon: 1.0    steps: 432  evaluation reward: 164.55\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 781: Policy loss: 0.000789. Value loss: 0.548770. Entropy: 1.356093.\n",
      "Iteration 782: Policy loss: -0.000467. Value loss: 0.313668. Entropy: 1.355075.\n",
      "Iteration 783: Policy loss: -0.003676. Value loss: 0.169498. Entropy: 1.357799.\n",
      "episode: 353   score: 45.0  epsilon: 1.0    steps: 264  evaluation reward: 163.4\n",
      "episode: 354   score: 90.0  epsilon: 1.0    steps: 528  evaluation reward: 162.35\n",
      "episode: 355   score: 100.0  epsilon: 1.0    steps: 984  evaluation reward: 161.55\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 784: Policy loss: 0.000172. Value loss: 1.016405. Entropy: 1.374852.\n",
      "Iteration 785: Policy loss: -0.003354. Value loss: 0.334653. Entropy: 1.373561.\n",
      "Iteration 786: Policy loss: -0.001932. Value loss: 0.336402. Entropy: 1.374998.\n",
      "episode: 356   score: 30.0  epsilon: 1.0    steps: 864  evaluation reward: 160.5\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 787: Policy loss: 0.000595. Value loss: 0.782464. Entropy: 1.374611.\n",
      "Iteration 788: Policy loss: -0.000968. Value loss: 0.395217. Entropy: 1.376047.\n",
      "Iteration 789: Policy loss: -0.001157. Value loss: 0.258186. Entropy: 1.375417.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 790: Policy loss: -0.001301. Value loss: 1.014959. Entropy: 1.372809.\n",
      "Iteration 791: Policy loss: -0.002252. Value loss: 0.434851. Entropy: 1.375094.\n",
      "Iteration 792: Policy loss: -0.003934. Value loss: 0.334369. Entropy: 1.380206.\n",
      "episode: 357   score: 155.0  epsilon: 1.0    steps: 472  evaluation reward: 161.25\n",
      "episode: 358   score: 215.0  epsilon: 1.0    steps: 648  evaluation reward: 162.35\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 793: Policy loss: -0.000089. Value loss: 1.161171. Entropy: 1.381538.\n",
      "Iteration 794: Policy loss: -0.000769. Value loss: 0.423411. Entropy: 1.381065.\n",
      "Iteration 795: Policy loss: -0.001978. Value loss: 0.308068. Entropy: 1.381960.\n",
      "episode: 359   score: 70.0  epsilon: 1.0    steps: 40  evaluation reward: 160.05\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 796: Policy loss: 0.000974. Value loss: 1.692565. Entropy: 1.380049.\n",
      "Iteration 797: Policy loss: -0.001660. Value loss: 0.547466. Entropy: 1.381231.\n",
      "Iteration 798: Policy loss: -0.002512. Value loss: 0.385874. Entropy: 1.376992.\n",
      "episode: 360   score: 105.0  epsilon: 1.0    steps: 856  evaluation reward: 158.85\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 799: Policy loss: -0.001537. Value loss: 0.581299. Entropy: 1.377449.\n",
      "Iteration 800: Policy loss: -0.004221. Value loss: 0.256119. Entropy: 1.375120.\n",
      "Iteration 801: Policy loss: -0.001176. Value loss: 0.143322. Entropy: 1.375598.\n",
      "episode: 361   score: 205.0  epsilon: 1.0    steps: 472  evaluation reward: 159.5\n",
      "episode: 362   score: 55.0  epsilon: 1.0    steps: 592  evaluation reward: 159.0\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 802: Policy loss: 0.000494. Value loss: 1.072061. Entropy: 1.376091.\n",
      "Iteration 803: Policy loss: -0.001361. Value loss: 0.477868. Entropy: 1.376433.\n",
      "Iteration 804: Policy loss: -0.004463. Value loss: 0.271267. Entropy: 1.371014.\n",
      "episode: 363   score: 175.0  epsilon: 1.0    steps: 256  evaluation reward: 158.95\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 805: Policy loss: 0.000511. Value loss: 0.904561. Entropy: 1.374208.\n",
      "Iteration 806: Policy loss: -0.000722. Value loss: 0.342581. Entropy: 1.371753.\n",
      "Iteration 807: Policy loss: -0.002664. Value loss: 0.196293. Entropy: 1.367629.\n",
      "episode: 364   score: 30.0  epsilon: 1.0    steps: 168  evaluation reward: 158.95\n",
      "episode: 365   score: 80.0  epsilon: 1.0    steps: 800  evaluation reward: 158.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 808: Policy loss: -0.001260. Value loss: 0.951850. Entropy: 1.367219.\n",
      "Iteration 809: Policy loss: -0.000461. Value loss: 0.394570. Entropy: 1.366755.\n",
      "Iteration 810: Policy loss: -0.002247. Value loss: 0.239307. Entropy: 1.369269.\n",
      "episode: 366   score: 95.0  epsilon: 1.0    steps: 736  evaluation reward: 157.7\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 811: Policy loss: 0.000049. Value loss: 0.710114. Entropy: 1.367905.\n",
      "Iteration 812: Policy loss: -0.002033. Value loss: 0.277859. Entropy: 1.366133.\n",
      "Iteration 813: Policy loss: -0.003441. Value loss: 0.177242. Entropy: 1.372905.\n",
      "episode: 367   score: 50.0  epsilon: 1.0    steps: 600  evaluation reward: 155.25\n",
      "episode: 368   score: 95.0  epsilon: 1.0    steps: 672  evaluation reward: 155.15\n",
      "episode: 369   score: 310.0  epsilon: 1.0    steps: 760  evaluation reward: 157.5\n",
      "episode: 370   score: 585.0  epsilon: 1.0    steps: 984  evaluation reward: 162.0\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 814: Policy loss: -0.000537. Value loss: 0.681891. Entropy: 1.365035.\n",
      "Iteration 815: Policy loss: 0.000165. Value loss: 0.226909. Entropy: 1.373087.\n",
      "Iteration 816: Policy loss: -0.006801. Value loss: 0.181645. Entropy: 1.370239.\n",
      "episode: 371   score: 60.0  epsilon: 1.0    steps: 808  evaluation reward: 160.5\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 817: Policy loss: 0.001431. Value loss: 0.834294. Entropy: 1.372140.\n",
      "Iteration 818: Policy loss: -0.001153. Value loss: 0.369527. Entropy: 1.370109.\n",
      "Iteration 819: Policy loss: -0.005034. Value loss: 0.269819. Entropy: 1.370989.\n",
      "episode: 372   score: 115.0  epsilon: 1.0    steps: 144  evaluation reward: 160.05\n",
      "episode: 373   score: 80.0  epsilon: 1.0    steps: 944  evaluation reward: 160.1\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 820: Policy loss: -0.000325. Value loss: 0.630491. Entropy: 1.377571.\n",
      "Iteration 821: Policy loss: -0.002100. Value loss: 0.325816. Entropy: 1.379126.\n",
      "Iteration 822: Policy loss: -0.002503. Value loss: 0.183668. Entropy: 1.377360.\n",
      "episode: 374   score: 165.0  epsilon: 1.0    steps: 632  evaluation reward: 160.65\n",
      "episode: 375   score: 20.0  epsilon: 1.0    steps: 728  evaluation reward: 159.05\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 823: Policy loss: 0.001125. Value loss: 0.900074. Entropy: 1.375085.\n",
      "Iteration 824: Policy loss: -0.004559. Value loss: 0.496388. Entropy: 1.370759.\n",
      "Iteration 825: Policy loss: -0.007071. Value loss: 0.374608. Entropy: 1.369028.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 826: Policy loss: 0.000867. Value loss: 0.774881. Entropy: 1.366919.\n",
      "Iteration 827: Policy loss: -0.001181. Value loss: 0.286856. Entropy: 1.369020.\n",
      "Iteration 828: Policy loss: -0.004167. Value loss: 0.170356. Entropy: 1.367977.\n",
      "episode: 376   score: 155.0  epsilon: 1.0    steps: 720  evaluation reward: 159.0\n",
      "episode: 377   score: 155.0  epsilon: 1.0    steps: 800  evaluation reward: 159.5\n",
      "episode: 378   score: 45.0  epsilon: 1.0    steps: 1024  evaluation reward: 158.9\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 829: Policy loss: 0.000427. Value loss: 0.668383. Entropy: 1.361954.\n",
      "Iteration 830: Policy loss: -0.002941. Value loss: 0.279872. Entropy: 1.358842.\n",
      "Iteration 831: Policy loss: -0.000272. Value loss: 0.167791. Entropy: 1.360460.\n",
      "episode: 379   score: 80.0  epsilon: 1.0    steps: 744  evaluation reward: 158.05\n",
      "episode: 380   score: 205.0  epsilon: 1.0    steps: 1008  evaluation reward: 156.75\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 832: Policy loss: 0.000618. Value loss: 0.971076. Entropy: 1.368151.\n",
      "Iteration 833: Policy loss: -0.002375. Value loss: 0.429433. Entropy: 1.368941.\n",
      "Iteration 834: Policy loss: -0.002654. Value loss: 0.355288. Entropy: 1.368421.\n",
      "episode: 381   score: 180.0  epsilon: 1.0    steps: 152  evaluation reward: 155.25\n",
      "episode: 382   score: 60.0  epsilon: 1.0    steps: 872  evaluation reward: 154.05\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 835: Policy loss: 0.001004. Value loss: 1.003068. Entropy: 1.370352.\n",
      "Iteration 836: Policy loss: -0.002558. Value loss: 0.356633. Entropy: 1.372321.\n",
      "Iteration 837: Policy loss: -0.004416. Value loss: 0.278757. Entropy: 1.372291.\n",
      "episode: 383   score: 65.0  epsilon: 1.0    steps: 768  evaluation reward: 153.65\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 838: Policy loss: 0.001216. Value loss: 0.911978. Entropy: 1.372221.\n",
      "Iteration 839: Policy loss: -0.002033. Value loss: 0.430107. Entropy: 1.370703.\n",
      "Iteration 840: Policy loss: -0.003408. Value loss: 0.234424. Entropy: 1.365898.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 841: Policy loss: -0.000568. Value loss: 0.822576. Entropy: 1.372552.\n",
      "Iteration 842: Policy loss: -0.001126. Value loss: 0.313942. Entropy: 1.370263.\n",
      "Iteration 843: Policy loss: -0.003889. Value loss: 0.216728. Entropy: 1.371749.\n",
      "episode: 384   score: 70.0  epsilon: 1.0    steps: 888  evaluation reward: 152.1\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 844: Policy loss: -0.000938. Value loss: 0.760767. Entropy: 1.368676.\n",
      "Iteration 845: Policy loss: -0.003766. Value loss: 0.333618. Entropy: 1.364421.\n",
      "Iteration 846: Policy loss: -0.003074. Value loss: 0.197465. Entropy: 1.367840.\n",
      "episode: 385   score: 115.0  epsilon: 1.0    steps: 664  evaluation reward: 152.2\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 847: Policy loss: -0.004456. Value loss: 0.748769. Entropy: 1.359458.\n",
      "Iteration 848: Policy loss: -0.003921. Value loss: 0.397985. Entropy: 1.363501.\n",
      "Iteration 849: Policy loss: -0.004528. Value loss: 0.243681. Entropy: 1.364833.\n",
      "episode: 386   score: 185.0  epsilon: 1.0    steps: 96  evaluation reward: 153.45\n",
      "episode: 387   score: 205.0  epsilon: 1.0    steps: 976  evaluation reward: 153.95\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 850: Policy loss: -0.003430. Value loss: 1.044841. Entropy: 1.357761.\n",
      "Iteration 851: Policy loss: -0.002254. Value loss: 0.473062. Entropy: 1.358634.\n",
      "Iteration 852: Policy loss: -0.003886. Value loss: 0.303933. Entropy: 1.361036.\n",
      "episode: 388   score: 405.0  epsilon: 1.0    steps: 984  evaluation reward: 157.7\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 853: Policy loss: 0.000243. Value loss: 0.813346. Entropy: 1.359395.\n",
      "Iteration 854: Policy loss: -0.001874. Value loss: 0.297102. Entropy: 1.365871.\n",
      "Iteration 855: Policy loss: -0.002361. Value loss: 0.211557. Entropy: 1.362185.\n",
      "episode: 389   score: 235.0  epsilon: 1.0    steps: 504  evaluation reward: 159.55\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 856: Policy loss: -0.002402. Value loss: 0.855536. Entropy: 1.355964.\n",
      "Iteration 857: Policy loss: -0.004438. Value loss: 0.329669. Entropy: 1.358242.\n",
      "Iteration 858: Policy loss: -0.004479. Value loss: 0.190178. Entropy: 1.359459.\n",
      "episode: 390   score: 270.0  epsilon: 1.0    steps: 528  evaluation reward: 161.95\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 859: Policy loss: -0.000965. Value loss: 0.964960. Entropy: 1.367822.\n",
      "Iteration 860: Policy loss: -0.004302. Value loss: 0.384363. Entropy: 1.368415.\n",
      "Iteration 861: Policy loss: -0.004855. Value loss: 0.200972. Entropy: 1.364849.\n",
      "episode: 391   score: 270.0  epsilon: 1.0    steps: 544  evaluation reward: 163.6\n",
      "episode: 392   score: 155.0  epsilon: 1.0    steps: 648  evaluation reward: 161.35\n",
      "episode: 393   score: 85.0  epsilon: 1.0    steps: 888  evaluation reward: 155.6\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 862: Policy loss: -0.000047. Value loss: 0.841659. Entropy: 1.359318.\n",
      "Iteration 863: Policy loss: -0.000967. Value loss: 0.264239. Entropy: 1.363285.\n",
      "Iteration 864: Policy loss: -0.007363. Value loss: 0.187688. Entropy: 1.366328.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 865: Policy loss: 0.001951. Value loss: 0.603072. Entropy: 1.366605.\n",
      "Iteration 866: Policy loss: 0.004921. Value loss: 0.176355. Entropy: 1.365752.\n",
      "Iteration 867: Policy loss: 0.000873. Value loss: 0.162023. Entropy: 1.359425.\n",
      "episode: 394   score: 240.0  epsilon: 1.0    steps: 104  evaluation reward: 154.65\n",
      "episode: 395   score: 115.0  epsilon: 1.0    steps: 144  evaluation reward: 155.3\n",
      "episode: 396   score: 150.0  epsilon: 1.0    steps: 248  evaluation reward: 155.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 397   score: 45.0  epsilon: 1.0    steps: 784  evaluation reward: 154.4\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 868: Policy loss: -0.003115. Value loss: 0.542910. Entropy: 1.362395.\n",
      "Iteration 869: Policy loss: -0.005983. Value loss: 0.276367. Entropy: 1.364335.\n",
      "Iteration 870: Policy loss: -0.005644. Value loss: 0.193613. Entropy: 1.357212.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 871: Policy loss: -0.000528. Value loss: 0.689646. Entropy: 1.363378.\n",
      "Iteration 872: Policy loss: -0.002021. Value loss: 0.306134. Entropy: 1.370276.\n",
      "Iteration 873: Policy loss: -0.004426. Value loss: 0.192729. Entropy: 1.368963.\n",
      "episode: 398   score: 50.0  epsilon: 1.0    steps: 344  evaluation reward: 153.35\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 874: Policy loss: -0.002911. Value loss: 0.855241. Entropy: 1.376957.\n",
      "Iteration 875: Policy loss: -0.002506. Value loss: 0.320400. Entropy: 1.374833.\n",
      "Iteration 876: Policy loss: -0.003843. Value loss: 0.201214. Entropy: 1.374110.\n",
      "episode: 399   score: 80.0  epsilon: 1.0    steps: 168  evaluation reward: 152.05\n",
      "episode: 400   score: 135.0  epsilon: 1.0    steps: 584  evaluation reward: 151.85\n",
      "now time :  2019-02-28 11:05:03.989516\n",
      "episode: 401   score: 55.0  epsilon: 1.0    steps: 840  evaluation reward: 150.6\n",
      "episode: 402   score: 145.0  epsilon: 1.0    steps: 1024  evaluation reward: 147.75\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 877: Policy loss: 0.002537. Value loss: 0.733289. Entropy: 1.372793.\n",
      "Iteration 878: Policy loss: 0.000072. Value loss: 0.289182. Entropy: 1.367482.\n",
      "Iteration 879: Policy loss: -0.004787. Value loss: 0.256734. Entropy: 1.368070.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 880: Policy loss: -0.002586. Value loss: 0.923296. Entropy: 1.358025.\n",
      "Iteration 881: Policy loss: -0.002865. Value loss: 0.426376. Entropy: 1.358486.\n",
      "Iteration 882: Policy loss: -0.005437. Value loss: 0.271727. Entropy: 1.361290.\n",
      "episode: 403   score: 590.0  epsilon: 1.0    steps: 864  evaluation reward: 151.55\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 883: Policy loss: -0.000383. Value loss: 0.672100. Entropy: 1.368783.\n",
      "Iteration 884: Policy loss: -0.002338. Value loss: 0.298304. Entropy: 1.367580.\n",
      "Iteration 885: Policy loss: -0.006652. Value loss: 0.179084. Entropy: 1.364069.\n",
      "episode: 404   score: 155.0  epsilon: 1.0    steps: 32  evaluation reward: 152.65\n",
      "episode: 405   score: 255.0  epsilon: 1.0    steps: 688  evaluation reward: 152.1\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 886: Policy loss: -0.001011. Value loss: 0.912028. Entropy: 1.367934.\n",
      "Iteration 887: Policy loss: -0.004011. Value loss: 0.288648. Entropy: 1.362115.\n",
      "Iteration 888: Policy loss: -0.006705. Value loss: 0.146228. Entropy: 1.364899.\n",
      "episode: 406   score: 75.0  epsilon: 1.0    steps: 952  evaluation reward: 148.75\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 889: Policy loss: -0.000421. Value loss: 1.214342. Entropy: 1.358080.\n",
      "Iteration 890: Policy loss: 0.003006. Value loss: 0.362021. Entropy: 1.366279.\n",
      "Iteration 891: Policy loss: -0.002932. Value loss: 0.213068. Entropy: 1.364174.\n",
      "episode: 407   score: 65.0  epsilon: 1.0    steps: 600  evaluation reward: 147.85\n",
      "episode: 408   score: 165.0  epsilon: 1.0    steps: 880  evaluation reward: 148.15\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 892: Policy loss: -0.002967. Value loss: 0.942842. Entropy: 1.364468.\n",
      "Iteration 893: Policy loss: -0.001623. Value loss: 0.296608. Entropy: 1.363242.\n",
      "Iteration 894: Policy loss: -0.000079. Value loss: 0.238077. Entropy: 1.362871.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 895: Policy loss: -0.000119. Value loss: 1.177025. Entropy: 1.366322.\n",
      "Iteration 896: Policy loss: -0.002602. Value loss: 0.566885. Entropy: 1.360957.\n",
      "Iteration 897: Policy loss: -0.008387. Value loss: 0.345524. Entropy: 1.362526.\n",
      "episode: 409   score: 240.0  epsilon: 1.0    steps: 688  evaluation reward: 148.45\n",
      "episode: 410   score: 95.0  epsilon: 1.0    steps: 768  evaluation reward: 148.9\n",
      "episode: 411   score: 150.0  epsilon: 1.0    steps: 880  evaluation reward: 149.85\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 898: Policy loss: -0.000405. Value loss: 0.911078. Entropy: 1.360037.\n",
      "Iteration 899: Policy loss: -0.005874. Value loss: 0.331096. Entropy: 1.361164.\n",
      "Iteration 900: Policy loss: -0.006007. Value loss: 0.209981. Entropy: 1.361947.\n",
      "episode: 412   score: 65.0  epsilon: 1.0    steps: 944  evaluation reward: 148.95\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 901: Policy loss: -0.001485. Value loss: 1.244093. Entropy: 1.361829.\n",
      "Iteration 902: Policy loss: -0.000079. Value loss: 0.466373. Entropy: 1.362244.\n",
      "Iteration 903: Policy loss: -0.002957. Value loss: 0.283498. Entropy: 1.363747.\n",
      "episode: 413   score: 145.0  epsilon: 1.0    steps: 1000  evaluation reward: 148.85\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 904: Policy loss: -0.001486. Value loss: 1.219312. Entropy: 1.370332.\n",
      "Iteration 905: Policy loss: -0.006853. Value loss: 0.398513. Entropy: 1.365005.\n",
      "Iteration 906: Policy loss: -0.008115. Value loss: 0.303982. Entropy: 1.364031.\n",
      "episode: 414   score: 205.0  epsilon: 1.0    steps: 328  evaluation reward: 149.3\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 907: Policy loss: 0.001456. Value loss: 0.761027. Entropy: 1.359815.\n",
      "Iteration 908: Policy loss: -0.002932. Value loss: 0.346611. Entropy: 1.356841.\n",
      "Iteration 909: Policy loss: -0.002976. Value loss: 0.215039. Entropy: 1.356126.\n",
      "episode: 415   score: 45.0  epsilon: 1.0    steps: 568  evaluation reward: 146.6\n",
      "episode: 416   score: 60.0  epsilon: 1.0    steps: 768  evaluation reward: 145.1\n",
      "episode: 417   score: 185.0  epsilon: 1.0    steps: 768  evaluation reward: 146.45\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 910: Policy loss: -0.002467. Value loss: 1.260702. Entropy: 1.347790.\n",
      "Iteration 911: Policy loss: -0.004941. Value loss: 0.594533. Entropy: 1.342426.\n",
      "Iteration 912: Policy loss: -0.006431. Value loss: 0.346803. Entropy: 1.341421.\n",
      "episode: 418   score: 240.0  epsilon: 1.0    steps: 200  evaluation reward: 147.55\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 913: Policy loss: -0.000354. Value loss: 1.174205. Entropy: 1.347124.\n",
      "Iteration 914: Policy loss: -0.002591. Value loss: 0.405729. Entropy: 1.337487.\n",
      "Iteration 915: Policy loss: -0.003877. Value loss: 0.314167. Entropy: 1.335624.\n",
      "episode: 419   score: 75.0  epsilon: 1.0    steps: 240  evaluation reward: 146.75\n",
      "episode: 420   score: 180.0  epsilon: 1.0    steps: 848  evaluation reward: 147.5\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 916: Policy loss: 0.000630. Value loss: 1.408404. Entropy: 1.337279.\n",
      "Iteration 917: Policy loss: -0.001701. Value loss: 0.525634. Entropy: 1.346841.\n",
      "Iteration 918: Policy loss: -0.005069. Value loss: 0.243775. Entropy: 1.342303.\n",
      "episode: 421   score: 85.0  epsilon: 1.0    steps: 256  evaluation reward: 147.8\n",
      "episode: 422   score: 120.0  epsilon: 1.0    steps: 264  evaluation reward: 146.45\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 919: Policy loss: -0.002089. Value loss: 0.512429. Entropy: 1.340337.\n",
      "Iteration 920: Policy loss: -0.002226. Value loss: 0.182972. Entropy: 1.345826.\n",
      "Iteration 921: Policy loss: -0.005679. Value loss: 0.147092. Entropy: 1.343956.\n",
      "episode: 423   score: 85.0  epsilon: 1.0    steps: 552  evaluation reward: 145.2\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 922: Policy loss: -0.000482. Value loss: 1.065890. Entropy: 1.344796.\n",
      "Iteration 923: Policy loss: -0.000627. Value loss: 0.431348. Entropy: 1.343122.\n",
      "Iteration 924: Policy loss: -0.006331. Value loss: 0.247481. Entropy: 1.346240.\n",
      "episode: 424   score: 120.0  epsilon: 1.0    steps: 960  evaluation reward: 144.95\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 925: Policy loss: 0.000548. Value loss: 1.174027. Entropy: 1.336695.\n",
      "Iteration 926: Policy loss: -0.002800. Value loss: 0.393966. Entropy: 1.351536.\n",
      "Iteration 927: Policy loss: -0.007434. Value loss: 0.200194. Entropy: 1.353613.\n",
      "episode: 425   score: 65.0  epsilon: 1.0    steps: 80  evaluation reward: 144.85\n",
      "Training network. lr: 0.000243. clip: 0.097244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 928: Policy loss: -0.000200. Value loss: 1.088633. Entropy: 1.355265.\n",
      "Iteration 929: Policy loss: -0.001267. Value loss: 0.457591. Entropy: 1.340577.\n",
      "Iteration 930: Policy loss: -0.003059. Value loss: 0.328105. Entropy: 1.346573.\n",
      "episode: 426   score: 190.0  epsilon: 1.0    steps: 704  evaluation reward: 145.7\n",
      "episode: 427   score: 95.0  epsilon: 1.0    steps: 928  evaluation reward: 145.1\n",
      "episode: 428   score: 65.0  epsilon: 1.0    steps: 1024  evaluation reward: 143.65\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 931: Policy loss: 0.002905. Value loss: 0.904168. Entropy: 1.347106.\n",
      "Iteration 932: Policy loss: -0.000389. Value loss: 0.373455. Entropy: 1.349141.\n",
      "Iteration 933: Policy loss: -0.003406. Value loss: 0.255681. Entropy: 1.348151.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 934: Policy loss: -0.000778. Value loss: 1.099221. Entropy: 1.353529.\n",
      "Iteration 935: Policy loss: -0.001188. Value loss: 0.476076. Entropy: 1.355538.\n",
      "Iteration 936: Policy loss: -0.008618. Value loss: 0.311576. Entropy: 1.351514.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 937: Policy loss: 0.000455. Value loss: 1.054284. Entropy: 1.356189.\n",
      "Iteration 938: Policy loss: -0.002919. Value loss: 0.442899. Entropy: 1.355729.\n",
      "Iteration 939: Policy loss: -0.007666. Value loss: 0.285616. Entropy: 1.356009.\n",
      "episode: 429   score: 280.0  epsilon: 1.0    steps: 16  evaluation reward: 145.95\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 940: Policy loss: 0.001341. Value loss: 1.101374. Entropy: 1.362474.\n",
      "Iteration 941: Policy loss: -0.000978. Value loss: 0.352345. Entropy: 1.355804.\n",
      "Iteration 942: Policy loss: -0.007901. Value loss: 0.216855. Entropy: 1.358319.\n",
      "episode: 430   score: 115.0  epsilon: 1.0    steps: 32  evaluation reward: 146.5\n",
      "episode: 431   score: 255.0  epsilon: 1.0    steps: 544  evaluation reward: 144.65\n",
      "episode: 432   score: 90.0  epsilon: 1.0    steps: 824  evaluation reward: 144.7\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 943: Policy loss: 0.002622. Value loss: 0.993779. Entropy: 1.357599.\n",
      "Iteration 944: Policy loss: 0.001011. Value loss: 0.252012. Entropy: 1.358537.\n",
      "Iteration 945: Policy loss: -0.001508. Value loss: 0.144152. Entropy: 1.355222.\n",
      "episode: 433   score: 240.0  epsilon: 1.0    steps: 256  evaluation reward: 140.7\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 946: Policy loss: 0.002434. Value loss: 0.528109. Entropy: 1.355161.\n",
      "Iteration 947: Policy loss: -0.001965. Value loss: 0.247476. Entropy: 1.352832.\n",
      "Iteration 948: Policy loss: -0.002000. Value loss: 0.175659. Entropy: 1.353227.\n",
      "episode: 434   score: 130.0  epsilon: 1.0    steps: 920  evaluation reward: 141.15\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 949: Policy loss: 0.000806. Value loss: 1.023345. Entropy: 1.351929.\n",
      "Iteration 950: Policy loss: -0.000693. Value loss: 0.399639. Entropy: 1.354971.\n",
      "Iteration 951: Policy loss: -0.002250. Value loss: 0.186265. Entropy: 1.352113.\n",
      "episode: 435   score: 200.0  epsilon: 1.0    steps: 304  evaluation reward: 142.65\n",
      "episode: 436   score: 40.0  epsilon: 1.0    steps: 816  evaluation reward: 142.45\n",
      "episode: 437   score: 35.0  epsilon: 1.0    steps: 872  evaluation reward: 142.8\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 952: Policy loss: 0.003146. Value loss: 0.850037. Entropy: 1.350913.\n",
      "Iteration 953: Policy loss: -0.002354. Value loss: 0.293318. Entropy: 1.352339.\n",
      "Iteration 954: Policy loss: -0.003694. Value loss: 0.127192. Entropy: 1.356513.\n",
      "episode: 438   score: 490.0  epsilon: 1.0    steps: 184  evaluation reward: 145.3\n",
      "episode: 439   score: 10.0  epsilon: 1.0    steps: 576  evaluation reward: 144.75\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 955: Policy loss: 0.000613. Value loss: 0.604698. Entropy: 1.346638.\n",
      "Iteration 956: Policy loss: -0.000226. Value loss: 0.445887. Entropy: 1.347341.\n",
      "Iteration 957: Policy loss: -0.002286. Value loss: 0.270294. Entropy: 1.345857.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 958: Policy loss: -0.000962. Value loss: 0.871871. Entropy: 1.346704.\n",
      "Iteration 959: Policy loss: -0.000780. Value loss: 0.300634. Entropy: 1.346464.\n",
      "Iteration 960: Policy loss: -0.002848. Value loss: 0.215085. Entropy: 1.337820.\n",
      "episode: 440   score: 265.0  epsilon: 1.0    steps: 488  evaluation reward: 146.35\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 961: Policy loss: -0.000574. Value loss: 0.853462. Entropy: 1.346282.\n",
      "Iteration 962: Policy loss: -0.004845. Value loss: 0.308371. Entropy: 1.344842.\n",
      "Iteration 963: Policy loss: -0.002738. Value loss: 0.205788. Entropy: 1.344332.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 964: Policy loss: -0.000691. Value loss: 0.875860. Entropy: 1.349796.\n",
      "Iteration 965: Policy loss: -0.002406. Value loss: 0.334233. Entropy: 1.346441.\n",
      "Iteration 966: Policy loss: -0.006998. Value loss: 0.180797. Entropy: 1.345979.\n",
      "episode: 441   score: 85.0  epsilon: 1.0    steps: 544  evaluation reward: 146.3\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 967: Policy loss: 0.001961. Value loss: 0.699427. Entropy: 1.349764.\n",
      "Iteration 968: Policy loss: -0.000731. Value loss: 0.340793. Entropy: 1.343902.\n",
      "Iteration 969: Policy loss: -0.002787. Value loss: 0.227006. Entropy: 1.344549.\n",
      "episode: 442   score: 590.0  epsilon: 1.0    steps: 448  evaluation reward: 151.1\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 970: Policy loss: -0.002364. Value loss: 1.123482. Entropy: 1.335068.\n",
      "Iteration 971: Policy loss: -0.000280. Value loss: 0.449632. Entropy: 1.347897.\n",
      "Iteration 972: Policy loss: -0.001735. Value loss: 0.243592. Entropy: 1.346650.\n",
      "episode: 443   score: 285.0  epsilon: 1.0    steps: 168  evaluation reward: 152.75\n",
      "episode: 444   score: 165.0  epsilon: 1.0    steps: 176  evaluation reward: 153.35\n",
      "episode: 445   score: 210.0  epsilon: 1.0    steps: 992  evaluation reward: 153.35\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 973: Policy loss: 0.003157. Value loss: 0.992589. Entropy: 1.348062.\n",
      "Iteration 974: Policy loss: -0.004611. Value loss: 0.404948. Entropy: 1.342398.\n",
      "Iteration 975: Policy loss: -0.003494. Value loss: 0.313102. Entropy: 1.334665.\n",
      "episode: 446   score: 185.0  epsilon: 1.0    steps: 96  evaluation reward: 154.45\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 976: Policy loss: 0.001430. Value loss: 0.795915. Entropy: 1.347937.\n",
      "Iteration 977: Policy loss: -0.001450. Value loss: 0.396263. Entropy: 1.340077.\n",
      "Iteration 978: Policy loss: -0.005482. Value loss: 0.245288. Entropy: 1.338406.\n",
      "episode: 447   score: 510.0  epsilon: 1.0    steps: 144  evaluation reward: 155.45\n",
      "episode: 448   score: 225.0  epsilon: 1.0    steps: 536  evaluation reward: 154.75\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 979: Policy loss: -0.000517. Value loss: 1.097838. Entropy: 1.329763.\n",
      "Iteration 980: Policy loss: -0.000989. Value loss: 0.435317. Entropy: 1.331276.\n",
      "Iteration 981: Policy loss: -0.004993. Value loss: 0.330575. Entropy: 1.329215.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 982: Policy loss: 0.000352. Value loss: 0.521101. Entropy: 1.326316.\n",
      "Iteration 983: Policy loss: -0.000020. Value loss: 0.316018. Entropy: 1.320490.\n",
      "Iteration 984: Policy loss: -0.000580. Value loss: 0.231754. Entropy: 1.318024.\n",
      "episode: 449   score: 105.0  epsilon: 1.0    steps: 944  evaluation reward: 154.55\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 985: Policy loss: -0.001801. Value loss: 0.733501. Entropy: 1.305605.\n",
      "Iteration 986: Policy loss: -0.003233. Value loss: 0.330514. Entropy: 1.308041.\n",
      "Iteration 987: Policy loss: -0.000708. Value loss: 0.225291. Entropy: 1.312994.\n",
      "episode: 450   score: 125.0  epsilon: 1.0    steps: 120  evaluation reward: 154.7\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 988: Policy loss: 0.002466. Value loss: 0.976870. Entropy: 1.311419.\n",
      "Iteration 989: Policy loss: -0.003116. Value loss: 0.620451. Entropy: 1.306159.\n",
      "Iteration 990: Policy loss: -0.006067. Value loss: 0.409570. Entropy: 1.310257.\n",
      "now time :  2019-02-28 11:06:25.742219\n",
      "episode: 451   score: 65.0  epsilon: 1.0    steps: 296  evaluation reward: 155.05\n",
      "episode: 452   score: 225.0  epsilon: 1.0    steps: 888  evaluation reward: 156.25\n",
      "Training network. lr: 0.000243. clip: 0.097088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 991: Policy loss: 0.002702. Value loss: 0.720495. Entropy: 1.289119.\n",
      "Iteration 992: Policy loss: -0.003955. Value loss: 0.259900. Entropy: 1.297074.\n",
      "Iteration 993: Policy loss: -0.003166. Value loss: 0.181295. Entropy: 1.291531.\n",
      "episode: 453   score: 140.0  epsilon: 1.0    steps: 280  evaluation reward: 157.2\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 994: Policy loss: 0.001022. Value loss: 0.774993. Entropy: 1.287341.\n",
      "Iteration 995: Policy loss: -0.001926. Value loss: 0.370072. Entropy: 1.291892.\n",
      "Iteration 996: Policy loss: -0.005284. Value loss: 0.205804. Entropy: 1.291524.\n",
      "episode: 454   score: 15.0  epsilon: 1.0    steps: 352  evaluation reward: 156.45\n",
      "episode: 455   score: 200.0  epsilon: 1.0    steps: 672  evaluation reward: 157.45\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 997: Policy loss: 0.003618. Value loss: 1.248585. Entropy: 1.300207.\n",
      "Iteration 998: Policy loss: 0.002590. Value loss: 0.352318. Entropy: 1.295792.\n",
      "Iteration 999: Policy loss: -0.000471. Value loss: 0.179208. Entropy: 1.301537.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 1000: Policy loss: 0.000883. Value loss: 0.854433. Entropy: 1.290870.\n",
      "Iteration 1001: Policy loss: -0.000621. Value loss: 0.417146. Entropy: 1.292046.\n",
      "Iteration 1002: Policy loss: -0.002528. Value loss: 0.288198. Entropy: 1.302499.\n",
      "episode: 456   score: 105.0  epsilon: 1.0    steps: 80  evaluation reward: 158.2\n",
      "episode: 457   score: 865.0  epsilon: 1.0    steps: 464  evaluation reward: 165.3\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1003: Policy loss: 0.001330. Value loss: 1.115549. Entropy: 1.291206.\n",
      "Iteration 1004: Policy loss: 0.001210. Value loss: 0.571334. Entropy: 1.302026.\n",
      "Iteration 1005: Policy loss: -0.001526. Value loss: 0.306791. Entropy: 1.300247.\n",
      "episode: 458   score: 80.0  epsilon: 1.0    steps: 384  evaluation reward: 163.95\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1006: Policy loss: 0.002329. Value loss: 1.088645. Entropy: 1.313892.\n",
      "Iteration 1007: Policy loss: -0.000038. Value loss: 0.346064. Entropy: 1.314989.\n",
      "Iteration 1008: Policy loss: -0.003452. Value loss: 0.222408. Entropy: 1.323684.\n",
      "episode: 459   score: 135.0  epsilon: 1.0    steps: 112  evaluation reward: 164.6\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1009: Policy loss: 0.001511. Value loss: 0.744453. Entropy: 1.316686.\n",
      "Iteration 1010: Policy loss: -0.001499. Value loss: 0.307074. Entropy: 1.332360.\n",
      "Iteration 1011: Policy loss: -0.004322. Value loss: 0.213488. Entropy: 1.334307.\n",
      "episode: 460   score: 55.0  epsilon: 1.0    steps: 464  evaluation reward: 164.1\n",
      "episode: 461   score: 490.0  epsilon: 1.0    steps: 736  evaluation reward: 166.95\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1012: Policy loss: 0.000693. Value loss: 1.294086. Entropy: 1.326072.\n",
      "Iteration 1013: Policy loss: -0.003471. Value loss: 0.349905. Entropy: 1.331363.\n",
      "Iteration 1014: Policy loss: -0.003946. Value loss: 0.187621. Entropy: 1.334792.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1015: Policy loss: -0.001527. Value loss: 0.784421. Entropy: 1.331698.\n",
      "Iteration 1016: Policy loss: -0.003842. Value loss: 0.333073. Entropy: 1.343672.\n",
      "Iteration 1017: Policy loss: -0.004546. Value loss: 0.245728. Entropy: 1.341697.\n",
      "episode: 462   score: 140.0  epsilon: 1.0    steps: 312  evaluation reward: 167.8\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1018: Policy loss: 0.001523. Value loss: 0.857408. Entropy: 1.338849.\n",
      "Iteration 1019: Policy loss: -0.002759. Value loss: 0.311591. Entropy: 1.334594.\n",
      "Iteration 1020: Policy loss: -0.002835. Value loss: 0.183338. Entropy: 1.346098.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1021: Policy loss: 0.000088. Value loss: 1.285565. Entropy: 1.352674.\n",
      "Iteration 1022: Policy loss: -0.002832. Value loss: 0.398136. Entropy: 1.355149.\n",
      "Iteration 1023: Policy loss: -0.005128. Value loss: 0.235191. Entropy: 1.357073.\n",
      "episode: 463   score: 595.0  epsilon: 1.0    steps: 344  evaluation reward: 172.0\n",
      "episode: 464   score: 55.0  epsilon: 1.0    steps: 672  evaluation reward: 172.25\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1024: Policy loss: -0.001121. Value loss: 0.892371. Entropy: 1.344690.\n",
      "Iteration 1025: Policy loss: -0.001332. Value loss: 0.391409. Entropy: 1.348792.\n",
      "Iteration 1026: Policy loss: -0.003555. Value loss: 0.227996. Entropy: 1.351318.\n",
      "episode: 465   score: 290.0  epsilon: 1.0    steps: 24  evaluation reward: 174.35\n",
      "episode: 466   score: 175.0  epsilon: 1.0    steps: 472  evaluation reward: 175.15\n",
      "episode: 467   score: 310.0  epsilon: 1.0    steps: 496  evaluation reward: 177.75\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1027: Policy loss: -0.000006. Value loss: 0.943528. Entropy: 1.351277.\n",
      "Iteration 1028: Policy loss: -0.003109. Value loss: 0.332261. Entropy: 1.354700.\n",
      "Iteration 1029: Policy loss: -0.005952. Value loss: 0.209829. Entropy: 1.352525.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1030: Policy loss: 0.002330. Value loss: 0.824721. Entropy: 1.361299.\n",
      "Iteration 1031: Policy loss: 0.003793. Value loss: 0.440879. Entropy: 1.362419.\n",
      "Iteration 1032: Policy loss: -0.004318. Value loss: 0.371510. Entropy: 1.358829.\n",
      "episode: 468   score: 65.0  epsilon: 1.0    steps: 136  evaluation reward: 177.45\n",
      "episode: 469   score: 260.0  epsilon: 1.0    steps: 592  evaluation reward: 176.95\n",
      "episode: 470   score: 215.0  epsilon: 1.0    steps: 896  evaluation reward: 173.25\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1033: Policy loss: -0.000098. Value loss: 0.778799. Entropy: 1.366072.\n",
      "Iteration 1034: Policy loss: -0.000065. Value loss: 0.281426. Entropy: 1.358520.\n",
      "Iteration 1035: Policy loss: -0.005309. Value loss: 0.209578. Entropy: 1.363191.\n",
      "episode: 471   score: 185.0  epsilon: 1.0    steps: 176  evaluation reward: 174.5\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1036: Policy loss: 0.000336. Value loss: 0.553304. Entropy: 1.355538.\n",
      "Iteration 1037: Policy loss: -0.000909. Value loss: 0.235311. Entropy: 1.357808.\n",
      "Iteration 1038: Policy loss: -0.002629. Value loss: 0.166244. Entropy: 1.357311.\n",
      "episode: 472   score: 105.0  epsilon: 1.0    steps: 112  evaluation reward: 174.4\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1039: Policy loss: 0.001677. Value loss: 1.063583. Entropy: 1.358268.\n",
      "Iteration 1040: Policy loss: 0.001379. Value loss: 0.369946. Entropy: 1.364707.\n",
      "Iteration 1041: Policy loss: -0.005937. Value loss: 0.452840. Entropy: 1.362647.\n",
      "episode: 473   score: 335.0  epsilon: 1.0    steps: 872  evaluation reward: 176.95\n",
      "episode: 474   score: 90.0  epsilon: 1.0    steps: 880  evaluation reward: 176.2\n",
      "episode: 475   score: 155.0  epsilon: 1.0    steps: 920  evaluation reward: 177.55\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1042: Policy loss: -0.001088. Value loss: 0.891097. Entropy: 1.363933.\n",
      "Iteration 1043: Policy loss: -0.002459. Value loss: 0.379862. Entropy: 1.357205.\n",
      "Iteration 1044: Policy loss: -0.007671. Value loss: 0.266838. Entropy: 1.353435.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1045: Policy loss: -0.002071. Value loss: 1.168053. Entropy: 1.346564.\n",
      "Iteration 1046: Policy loss: -0.000200. Value loss: 0.563854. Entropy: 1.350432.\n",
      "Iteration 1047: Policy loss: -0.005682. Value loss: 0.398895. Entropy: 1.348059.\n",
      "episode: 476   score: 90.0  epsilon: 1.0    steps: 296  evaluation reward: 176.9\n",
      "episode: 477   score: 110.0  epsilon: 1.0    steps: 464  evaluation reward: 176.45\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1048: Policy loss: 0.000610. Value loss: 0.611648. Entropy: 1.358371.\n",
      "Iteration 1049: Policy loss: -0.000858. Value loss: 0.275083. Entropy: 1.342527.\n",
      "Iteration 1050: Policy loss: -0.004368. Value loss: 0.195253. Entropy: 1.351103.\n",
      "episode: 478   score: 320.0  epsilon: 1.0    steps: 88  evaluation reward: 179.2\n",
      "episode: 479   score: 185.0  epsilon: 1.0    steps: 176  evaluation reward: 180.25\n",
      "episode: 480   score: 20.0  epsilon: 1.0    steps: 424  evaluation reward: 178.4\n",
      "episode: 481   score: 65.0  epsilon: 1.0    steps: 800  evaluation reward: 177.25\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1051: Policy loss: 0.000960. Value loss: 0.678353. Entropy: 1.359124.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1052: Policy loss: -0.001468. Value loss: 0.272619. Entropy: 1.356586.\n",
      "Iteration 1053: Policy loss: -0.002085. Value loss: 0.196204. Entropy: 1.364029.\n",
      "episode: 482   score: 50.0  epsilon: 1.0    steps: 736  evaluation reward: 177.15\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1054: Policy loss: -0.003457. Value loss: 1.121577. Entropy: 1.361055.\n",
      "Iteration 1055: Policy loss: -0.000862. Value loss: 0.505578. Entropy: 1.367444.\n",
      "Iteration 1056: Policy loss: -0.001633. Value loss: 0.363580. Entropy: 1.359291.\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1057: Policy loss: -0.001230. Value loss: 0.740962. Entropy: 1.369733.\n",
      "Iteration 1058: Policy loss: -0.002336. Value loss: 0.301921. Entropy: 1.365581.\n",
      "Iteration 1059: Policy loss: -0.003117. Value loss: 0.234890. Entropy: 1.365055.\n",
      "episode: 483   score: 210.0  epsilon: 1.0    steps: 200  evaluation reward: 178.6\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1060: Policy loss: -0.002752. Value loss: 0.666613. Entropy: 1.367019.\n",
      "Iteration 1061: Policy loss: -0.001755. Value loss: 0.398299. Entropy: 1.366603.\n",
      "Iteration 1062: Policy loss: -0.006380. Value loss: 0.216336. Entropy: 1.367301.\n",
      "episode: 484   score: 155.0  epsilon: 1.0    steps: 184  evaluation reward: 179.45\n",
      "episode: 485   score: 90.0  epsilon: 1.0    steps: 248  evaluation reward: 179.2\n",
      "episode: 486   score: 5.0  epsilon: 1.0    steps: 296  evaluation reward: 177.4\n",
      "episode: 487   score: 60.0  epsilon: 1.0    steps: 696  evaluation reward: 175.95\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1063: Policy loss: 0.000351. Value loss: 0.578847. Entropy: 1.366362.\n",
      "Iteration 1064: Policy loss: -0.005559. Value loss: 0.279459. Entropy: 1.366324.\n",
      "Iteration 1065: Policy loss: -0.004496. Value loss: 0.223219. Entropy: 1.369932.\n",
      "episode: 488   score: 50.0  epsilon: 1.0    steps: 88  evaluation reward: 172.4\n",
      "episode: 489   score: 60.0  epsilon: 1.0    steps: 592  evaluation reward: 170.65\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1066: Policy loss: -0.000238. Value loss: 1.132140. Entropy: 1.361132.\n",
      "Iteration 1067: Policy loss: 0.000921. Value loss: 0.455374. Entropy: 1.360357.\n",
      "Iteration 1068: Policy loss: -0.003403. Value loss: 0.269603. Entropy: 1.371269.\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1069: Policy loss: -0.001194. Value loss: 0.632591. Entropy: 1.356204.\n",
      "Iteration 1070: Policy loss: -0.002407. Value loss: 0.276228. Entropy: 1.358758.\n",
      "Iteration 1071: Policy loss: -0.003664. Value loss: 0.211025. Entropy: 1.355761.\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1072: Policy loss: -0.001911. Value loss: 0.727423. Entropy: 1.354690.\n",
      "Iteration 1073: Policy loss: -0.002547. Value loss: 0.276105. Entropy: 1.345735.\n",
      "Iteration 1074: Policy loss: -0.008055. Value loss: 0.176722. Entropy: 1.349758.\n",
      "episode: 490   score: 40.0  epsilon: 1.0    steps: 392  evaluation reward: 168.35\n",
      "episode: 491   score: 30.0  epsilon: 1.0    steps: 864  evaluation reward: 165.95\n",
      "episode: 492   score: 180.0  epsilon: 1.0    steps: 920  evaluation reward: 166.2\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1075: Policy loss: -0.001391. Value loss: 0.904549. Entropy: 1.337735.\n",
      "Iteration 1076: Policy loss: -0.003431. Value loss: 0.300262. Entropy: 1.341781.\n",
      "Iteration 1077: Policy loss: -0.006287. Value loss: 0.138137. Entropy: 1.343212.\n",
      "episode: 493   score: 100.0  epsilon: 1.0    steps: 128  evaluation reward: 166.35\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1078: Policy loss: 0.002477. Value loss: 0.610652. Entropy: 1.337035.\n",
      "Iteration 1079: Policy loss: 0.002147. Value loss: 0.332379. Entropy: 1.340915.\n",
      "Iteration 1080: Policy loss: -0.005648. Value loss: 0.287032. Entropy: 1.341114.\n",
      "episode: 494   score: 205.0  epsilon: 1.0    steps: 432  evaluation reward: 166.0\n",
      "episode: 495   score: 70.0  epsilon: 1.0    steps: 488  evaluation reward: 165.55\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1081: Policy loss: 0.000328. Value loss: 0.751315. Entropy: 1.344437.\n",
      "Iteration 1082: Policy loss: -0.001655. Value loss: 0.203962. Entropy: 1.342813.\n",
      "Iteration 1083: Policy loss: -0.001032. Value loss: 0.218446. Entropy: 1.340811.\n",
      "episode: 496   score: 205.0  epsilon: 1.0    steps: 416  evaluation reward: 166.1\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1084: Policy loss: 0.000337. Value loss: 0.705215. Entropy: 1.342206.\n",
      "Iteration 1085: Policy loss: -0.004982. Value loss: 0.411093. Entropy: 1.334999.\n",
      "Iteration 1086: Policy loss: -0.005367. Value loss: 0.302008. Entropy: 1.338161.\n",
      "episode: 497   score: 60.0  epsilon: 1.0    steps: 120  evaluation reward: 166.25\n",
      "episode: 498   score: 90.0  epsilon: 1.0    steps: 856  evaluation reward: 166.65\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1087: Policy loss: 0.001297. Value loss: 0.823125. Entropy: 1.329809.\n",
      "Iteration 1088: Policy loss: 0.000148. Value loss: 0.220842. Entropy: 1.324408.\n",
      "Iteration 1089: Policy loss: -0.000349. Value loss: 0.137345. Entropy: 1.334252.\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1090: Policy loss: 0.000298. Value loss: 0.529714. Entropy: 1.338739.\n",
      "Iteration 1091: Policy loss: -0.002993. Value loss: 0.257645. Entropy: 1.327798.\n",
      "Iteration 1092: Policy loss: -0.002472. Value loss: 0.163860. Entropy: 1.336812.\n",
      "episode: 499   score: 165.0  epsilon: 1.0    steps: 64  evaluation reward: 167.5\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1093: Policy loss: 0.000397. Value loss: 0.965527. Entropy: 1.340238.\n",
      "Iteration 1094: Policy loss: -0.002780. Value loss: 0.334523. Entropy: 1.344210.\n",
      "Iteration 1095: Policy loss: -0.002779. Value loss: 0.187944. Entropy: 1.343186.\n",
      "episode: 500   score: 110.0  epsilon: 1.0    steps: 800  evaluation reward: 167.25\n",
      "now time :  2019-02-28 11:07:42.692160\n",
      "episode: 501   score: 215.0  epsilon: 1.0    steps: 952  evaluation reward: 168.85\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1096: Policy loss: 0.000539. Value loss: 0.483271. Entropy: 1.350063.\n",
      "Iteration 1097: Policy loss: -0.001770. Value loss: 0.162855. Entropy: 1.337970.\n",
      "Iteration 1098: Policy loss: -0.006422. Value loss: 0.118824. Entropy: 1.329109.\n",
      "episode: 502   score: 375.0  epsilon: 1.0    steps: 712  evaluation reward: 171.15\n",
      "episode: 503   score: 150.0  epsilon: 1.0    steps: 808  evaluation reward: 166.75\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1099: Policy loss: -0.000067. Value loss: 0.930229. Entropy: 1.325588.\n",
      "Iteration 1100: Policy loss: -0.002501. Value loss: 0.291266. Entropy: 1.317251.\n",
      "Iteration 1101: Policy loss: -0.005095. Value loss: 0.163738. Entropy: 1.316662.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1102: Policy loss: 0.000662. Value loss: 0.870645. Entropy: 1.328685.\n",
      "Iteration 1103: Policy loss: -0.000343. Value loss: 0.406864. Entropy: 1.330136.\n",
      "Iteration 1104: Policy loss: -0.002075. Value loss: 0.287293. Entropy: 1.339188.\n",
      "episode: 504   score: 265.0  epsilon: 1.0    steps: 200  evaluation reward: 167.85\n",
      "episode: 505   score: 20.0  epsilon: 1.0    steps: 656  evaluation reward: 165.5\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1105: Policy loss: -0.003810. Value loss: 0.886164. Entropy: 1.316105.\n",
      "Iteration 1106: Policy loss: -0.002237. Value loss: 0.290835. Entropy: 1.305684.\n",
      "Iteration 1107: Policy loss: -0.005093. Value loss: 0.199067. Entropy: 1.313002.\n",
      "episode: 506   score: 210.0  epsilon: 1.0    steps: 304  evaluation reward: 166.85\n",
      "episode: 507   score: 255.0  epsilon: 1.0    steps: 624  evaluation reward: 168.75\n",
      "episode: 508   score: 15.0  epsilon: 1.0    steps: 1008  evaluation reward: 167.25\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1108: Policy loss: -0.001031. Value loss: 0.746729. Entropy: 1.323509.\n",
      "Iteration 1109: Policy loss: -0.009247. Value loss: 0.297994. Entropy: 1.295380.\n",
      "Iteration 1110: Policy loss: -0.007088. Value loss: 0.225524. Entropy: 1.290683.\n",
      "episode: 509   score: 225.0  epsilon: 1.0    steps: 864  evaluation reward: 167.1\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1111: Policy loss: -0.000773. Value loss: 0.873362. Entropy: 1.296629.\n",
      "Iteration 1112: Policy loss: -0.002418. Value loss: 0.250214. Entropy: 1.303394.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1113: Policy loss: -0.006946. Value loss: 0.231835. Entropy: 1.316263.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1114: Policy loss: 0.000168. Value loss: 0.712648. Entropy: 1.294327.\n",
      "Iteration 1115: Policy loss: -0.002924. Value loss: 0.292921. Entropy: 1.291376.\n",
      "Iteration 1116: Policy loss: -0.007690. Value loss: 0.209032. Entropy: 1.284816.\n",
      "episode: 510   score: 215.0  epsilon: 1.0    steps: 592  evaluation reward: 168.3\n",
      "episode: 511   score: 30.0  epsilon: 1.0    steps: 680  evaluation reward: 167.1\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1117: Policy loss: -0.003313. Value loss: 0.984109. Entropy: 1.276011.\n",
      "Iteration 1118: Policy loss: -0.007915. Value loss: 0.452388. Entropy: 1.268295.\n",
      "Iteration 1119: Policy loss: -0.008796. Value loss: 0.222017. Entropy: 1.276121.\n",
      "episode: 512   score: 150.0  epsilon: 1.0    steps: 80  evaluation reward: 167.95\n",
      "episode: 513   score: 110.0  epsilon: 1.0    steps: 808  evaluation reward: 167.6\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1120: Policy loss: 0.001106. Value loss: 1.006994. Entropy: 1.307746.\n",
      "Iteration 1121: Policy loss: 0.002514. Value loss: 0.434204. Entropy: 1.298404.\n",
      "Iteration 1122: Policy loss: -0.001010. Value loss: 0.427654. Entropy: 1.290860.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1123: Policy loss: 0.002132. Value loss: 0.756027. Entropy: 1.313830.\n",
      "Iteration 1124: Policy loss: -0.001024. Value loss: 0.286231. Entropy: 1.301887.\n",
      "Iteration 1125: Policy loss: -0.004399. Value loss: 0.210174. Entropy: 1.316128.\n",
      "episode: 514   score: 470.0  epsilon: 1.0    steps: 448  evaluation reward: 170.25\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1126: Policy loss: -0.000375. Value loss: 0.927243. Entropy: 1.314874.\n",
      "Iteration 1127: Policy loss: -0.003001. Value loss: 0.386735. Entropy: 1.301871.\n",
      "Iteration 1128: Policy loss: -0.004160. Value loss: 0.230764. Entropy: 1.313290.\n",
      "episode: 515   score: 70.0  epsilon: 1.0    steps: 104  evaluation reward: 170.5\n",
      "episode: 516   score: 260.0  epsilon: 1.0    steps: 224  evaluation reward: 172.5\n",
      "episode: 517   score: 245.0  epsilon: 1.0    steps: 240  evaluation reward: 173.1\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1129: Policy loss: 0.002317. Value loss: 1.188193. Entropy: 1.332222.\n",
      "Iteration 1130: Policy loss: 0.001648. Value loss: 0.295767. Entropy: 1.322261.\n",
      "Iteration 1131: Policy loss: -0.003105. Value loss: 0.231550. Entropy: 1.318262.\n",
      "episode: 518   score: 335.0  epsilon: 1.0    steps: 296  evaluation reward: 174.05\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1132: Policy loss: -0.000231. Value loss: 0.987125. Entropy: 1.307703.\n",
      "Iteration 1133: Policy loss: -0.005352. Value loss: 0.357285. Entropy: 1.317591.\n",
      "Iteration 1134: Policy loss: -0.007117. Value loss: 0.216934. Entropy: 1.318193.\n",
      "episode: 519   score: 145.0  epsilon: 1.0    steps: 256  evaluation reward: 174.75\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1135: Policy loss: -0.004198. Value loss: 0.997356. Entropy: 1.320848.\n",
      "Iteration 1136: Policy loss: -0.005612. Value loss: 0.379120. Entropy: 1.322161.\n",
      "Iteration 1137: Policy loss: -0.005452. Value loss: 0.222216. Entropy: 1.322052.\n",
      "episode: 520   score: 35.0  epsilon: 1.0    steps: 760  evaluation reward: 173.3\n",
      "episode: 521   score: 405.0  epsilon: 1.0    steps: 864  evaluation reward: 176.5\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1138: Policy loss: -0.002234. Value loss: 0.996049. Entropy: 1.318620.\n",
      "Iteration 1139: Policy loss: -0.007908. Value loss: 0.448450. Entropy: 1.313073.\n",
      "Iteration 1140: Policy loss: -0.005797. Value loss: 0.360021. Entropy: 1.318836.\n",
      "episode: 522   score: 185.0  epsilon: 1.0    steps: 96  evaluation reward: 177.15\n",
      "episode: 523   score: 80.0  epsilon: 1.0    steps: 360  evaluation reward: 177.1\n",
      "episode: 524   score: 135.0  epsilon: 1.0    steps: 648  evaluation reward: 177.25\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1141: Policy loss: 0.004781. Value loss: 1.277914. Entropy: 1.331899.\n",
      "Iteration 1142: Policy loss: 0.001937. Value loss: 0.560680. Entropy: 1.347448.\n",
      "Iteration 1143: Policy loss: -0.001310. Value loss: 0.375932. Entropy: 1.336569.\n",
      "episode: 525   score: 40.0  epsilon: 1.0    steps: 16  evaluation reward: 177.0\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1144: Policy loss: 0.001646. Value loss: 0.889211. Entropy: 1.334276.\n",
      "Iteration 1145: Policy loss: -0.002147. Value loss: 0.320281. Entropy: 1.326565.\n",
      "Iteration 1146: Policy loss: -0.002394. Value loss: 0.248034. Entropy: 1.325099.\n",
      "episode: 526   score: 145.0  epsilon: 1.0    steps: 968  evaluation reward: 176.55\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1147: Policy loss: -0.000721. Value loss: 0.996360. Entropy: 1.323226.\n",
      "Iteration 1148: Policy loss: -0.002288. Value loss: 0.421223. Entropy: 1.323682.\n",
      "Iteration 1149: Policy loss: -0.006453. Value loss: 0.282747. Entropy: 1.322876.\n",
      "episode: 527   score: 80.0  epsilon: 1.0    steps: 856  evaluation reward: 176.4\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1150: Policy loss: -0.001761. Value loss: 1.376207. Entropy: 1.330921.\n",
      "Iteration 1151: Policy loss: -0.005750. Value loss: 0.601073. Entropy: 1.336886.\n",
      "Iteration 1152: Policy loss: -0.001649. Value loss: 0.238497. Entropy: 1.338543.\n",
      "episode: 528   score: 135.0  epsilon: 1.0    steps: 464  evaluation reward: 177.1\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1153: Policy loss: 0.003808. Value loss: 1.137778. Entropy: 1.337387.\n",
      "Iteration 1154: Policy loss: 0.002519. Value loss: 0.572902. Entropy: 1.333361.\n",
      "Iteration 1155: Policy loss: -0.000472. Value loss: 0.300260. Entropy: 1.341124.\n",
      "episode: 529   score: 135.0  epsilon: 1.0    steps: 624  evaluation reward: 175.65\n",
      "episode: 530   score: 210.0  epsilon: 1.0    steps: 672  evaluation reward: 176.6\n",
      "episode: 531   score: 190.0  epsilon: 1.0    steps: 752  evaluation reward: 175.95\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1156: Policy loss: -0.000349. Value loss: 1.150604. Entropy: 1.336347.\n",
      "Iteration 1157: Policy loss: -0.002566. Value loss: 0.475789. Entropy: 1.333649.\n",
      "Iteration 1158: Policy loss: -0.004332. Value loss: 0.283115. Entropy: 1.335132.\n",
      "episode: 532   score: 235.0  epsilon: 1.0    steps: 312  evaluation reward: 177.4\n",
      "episode: 533   score: 155.0  epsilon: 1.0    steps: 848  evaluation reward: 176.55\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1159: Policy loss: 0.003236. Value loss: 0.578236. Entropy: 1.342832.\n",
      "Iteration 1160: Policy loss: 0.000441. Value loss: 0.289871. Entropy: 1.343966.\n",
      "Iteration 1161: Policy loss: -0.003973. Value loss: 0.210282. Entropy: 1.345592.\n",
      "episode: 534   score: 80.0  epsilon: 1.0    steps: 344  evaluation reward: 176.05\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1162: Policy loss: -0.000681. Value loss: 0.745152. Entropy: 1.350794.\n",
      "Iteration 1163: Policy loss: 0.001315. Value loss: 0.337140. Entropy: 1.340003.\n",
      "Iteration 1164: Policy loss: -0.001332. Value loss: 0.251181. Entropy: 1.344090.\n",
      "episode: 535   score: 180.0  epsilon: 1.0    steps: 240  evaluation reward: 175.85\n",
      "episode: 536   score: 50.0  epsilon: 1.0    steps: 704  evaluation reward: 175.95\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1165: Policy loss: 0.000803. Value loss: 0.756996. Entropy: 1.346181.\n",
      "Iteration 1166: Policy loss: -0.001275. Value loss: 0.256712. Entropy: 1.346636.\n",
      "Iteration 1167: Policy loss: -0.003867. Value loss: 0.203610. Entropy: 1.345648.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1168: Policy loss: -0.001760. Value loss: 0.633898. Entropy: 1.354063.\n",
      "Iteration 1169: Policy loss: -0.005809. Value loss: 0.263158. Entropy: 1.355988.\n",
      "Iteration 1170: Policy loss: -0.004833. Value loss: 0.178101. Entropy: 1.348716.\n",
      "episode: 537   score: 410.0  epsilon: 1.0    steps: 840  evaluation reward: 179.7\n",
      "episode: 538   score: 55.0  epsilon: 1.0    steps: 984  evaluation reward: 175.35\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1171: Policy loss: -0.001992. Value loss: 1.350797. Entropy: 1.359537.\n",
      "Iteration 1172: Policy loss: -0.000570. Value loss: 0.476762. Entropy: 1.352988.\n",
      "Iteration 1173: Policy loss: -0.007887. Value loss: 0.362346. Entropy: 1.352475.\n",
      "episode: 539   score: 120.0  epsilon: 1.0    steps: 72  evaluation reward: 176.45\n",
      "episode: 540   score: 30.0  epsilon: 1.0    steps: 128  evaluation reward: 174.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1174: Policy loss: -0.000729. Value loss: 0.583836. Entropy: 1.361619.\n",
      "Iteration 1175: Policy loss: -0.001497. Value loss: 0.380203. Entropy: 1.362444.\n",
      "Iteration 1176: Policy loss: -0.001489. Value loss: 0.160695. Entropy: 1.357457.\n",
      "episode: 541   score: 105.0  epsilon: 1.0    steps: 912  evaluation reward: 174.3\n",
      "episode: 542   score: 110.0  epsilon: 1.0    steps: 928  evaluation reward: 169.5\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1177: Policy loss: 0.002407. Value loss: 1.094679. Entropy: 1.357050.\n",
      "Iteration 1178: Policy loss: 0.000418. Value loss: 0.419157. Entropy: 1.353254.\n",
      "Iteration 1179: Policy loss: -0.000747. Value loss: 0.303961. Entropy: 1.348730.\n",
      "episode: 543   score: 110.0  epsilon: 1.0    steps: 224  evaluation reward: 167.75\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1180: Policy loss: -0.001556. Value loss: 0.594263. Entropy: 1.349389.\n",
      "Iteration 1181: Policy loss: -0.004123. Value loss: 0.194449. Entropy: 1.351685.\n",
      "Iteration 1182: Policy loss: -0.001494. Value loss: 0.129608. Entropy: 1.349157.\n",
      "episode: 544   score: 85.0  epsilon: 1.0    steps: 536  evaluation reward: 166.95\n",
      "episode: 545   score: 60.0  epsilon: 1.0    steps: 648  evaluation reward: 165.45\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1183: Policy loss: 0.003294. Value loss: 0.969543. Entropy: 1.343653.\n",
      "Iteration 1184: Policy loss: -0.000298. Value loss: 0.401350. Entropy: 1.341648.\n",
      "Iteration 1185: Policy loss: -0.000534. Value loss: 0.172825. Entropy: 1.340863.\n",
      "episode: 546   score: 155.0  epsilon: 1.0    steps: 176  evaluation reward: 165.15\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1186: Policy loss: 0.000800. Value loss: 0.592277. Entropy: 1.349962.\n",
      "Iteration 1187: Policy loss: -0.005176. Value loss: 0.270693. Entropy: 1.357551.\n",
      "Iteration 1188: Policy loss: -0.005278. Value loss: 0.202310. Entropy: 1.356762.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1189: Policy loss: -0.001637. Value loss: 1.135867. Entropy: 1.354766.\n",
      "Iteration 1190: Policy loss: -0.003103. Value loss: 0.434940. Entropy: 1.355247.\n",
      "Iteration 1191: Policy loss: -0.009960. Value loss: 0.219792. Entropy: 1.355162.\n",
      "episode: 547   score: 35.0  epsilon: 1.0    steps: 120  evaluation reward: 160.4\n",
      "episode: 548   score: 15.0  epsilon: 1.0    steps: 432  evaluation reward: 158.3\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1192: Policy loss: 0.001099. Value loss: 0.882986. Entropy: 1.358015.\n",
      "Iteration 1193: Policy loss: 0.001637. Value loss: 0.254731. Entropy: 1.359291.\n",
      "Iteration 1194: Policy loss: -0.004795. Value loss: 0.147550. Entropy: 1.357246.\n",
      "episode: 549   score: 105.0  epsilon: 1.0    steps: 80  evaluation reward: 158.3\n",
      "episode: 550   score: 655.0  epsilon: 1.0    steps: 720  evaluation reward: 163.6\n",
      "now time :  2019-02-28 11:08:54.577958\n",
      "episode: 551   score: 260.0  epsilon: 1.0    steps: 920  evaluation reward: 165.55\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1195: Policy loss: -0.000166. Value loss: 0.769858. Entropy: 1.339879.\n",
      "Iteration 1196: Policy loss: 0.001286. Value loss: 0.329891. Entropy: 1.344333.\n",
      "Iteration 1197: Policy loss: -0.003045. Value loss: 0.183871. Entropy: 1.342889.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1198: Policy loss: -0.000722. Value loss: 0.798288. Entropy: 1.346168.\n",
      "Iteration 1199: Policy loss: 0.001723. Value loss: 0.324909. Entropy: 1.337277.\n",
      "Iteration 1200: Policy loss: 0.001005. Value loss: 0.185860. Entropy: 1.339497.\n",
      "episode: 552   score: 85.0  epsilon: 1.0    steps: 904  evaluation reward: 164.15\n",
      "episode: 553   score: 180.0  epsilon: 1.0    steps: 952  evaluation reward: 164.55\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1201: Policy loss: 0.001008. Value loss: 1.264948. Entropy: 1.339751.\n",
      "Iteration 1202: Policy loss: -0.002042. Value loss: 0.511696. Entropy: 1.347035.\n",
      "Iteration 1203: Policy loss: -0.003694. Value loss: 0.346548. Entropy: 1.345445.\n",
      "episode: 554   score: 135.0  epsilon: 1.0    steps: 16  evaluation reward: 165.75\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1204: Policy loss: 0.000968. Value loss: 0.880616. Entropy: 1.347375.\n",
      "Iteration 1205: Policy loss: -0.002263. Value loss: 0.342303. Entropy: 1.353333.\n",
      "Iteration 1206: Policy loss: -0.003415. Value loss: 0.191859. Entropy: 1.349934.\n",
      "episode: 555   score: 120.0  epsilon: 1.0    steps: 528  evaluation reward: 164.95\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1207: Policy loss: -0.001208. Value loss: 1.168167. Entropy: 1.354412.\n",
      "Iteration 1208: Policy loss: -0.002130. Value loss: 0.312472. Entropy: 1.354093.\n",
      "Iteration 1209: Policy loss: -0.003001. Value loss: 0.170961. Entropy: 1.352297.\n",
      "episode: 556   score: 65.0  epsilon: 1.0    steps: 128  evaluation reward: 164.55\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1210: Policy loss: -0.001630. Value loss: 0.635390. Entropy: 1.354516.\n",
      "Iteration 1211: Policy loss: -0.005356. Value loss: 0.268955. Entropy: 1.350248.\n",
      "Iteration 1212: Policy loss: -0.007442. Value loss: 0.205165. Entropy: 1.352532.\n",
      "episode: 557   score: 45.0  epsilon: 1.0    steps: 528  evaluation reward: 156.35\n",
      "episode: 558   score: 85.0  epsilon: 1.0    steps: 552  evaluation reward: 156.4\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1213: Policy loss: 0.000773. Value loss: 0.709112. Entropy: 1.346859.\n",
      "Iteration 1214: Policy loss: 0.001801. Value loss: 0.372852. Entropy: 1.347980.\n",
      "Iteration 1215: Policy loss: -0.000417. Value loss: 0.166567. Entropy: 1.346920.\n",
      "episode: 559   score: 110.0  epsilon: 1.0    steps: 264  evaluation reward: 156.15\n",
      "episode: 560   score: 470.0  epsilon: 1.0    steps: 720  evaluation reward: 160.3\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1216: Policy loss: 0.001894. Value loss: 0.942820. Entropy: 1.352533.\n",
      "Iteration 1217: Policy loss: -0.003692. Value loss: 0.288034. Entropy: 1.358156.\n",
      "Iteration 1218: Policy loss: -0.003880. Value loss: 0.191633. Entropy: 1.354387.\n",
      "episode: 561   score: 120.0  epsilon: 1.0    steps: 1024  evaluation reward: 156.6\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1219: Policy loss: 0.003755. Value loss: 1.079429. Entropy: 1.364150.\n",
      "Iteration 1220: Policy loss: 0.000632. Value loss: 0.425050. Entropy: 1.370331.\n",
      "Iteration 1221: Policy loss: -0.001261. Value loss: 0.280319. Entropy: 1.368447.\n",
      "episode: 562   score: 80.0  epsilon: 1.0    steps: 16  evaluation reward: 156.0\n",
      "episode: 563   score: 30.0  epsilon: 1.0    steps: 552  evaluation reward: 150.35\n",
      "episode: 564   score: 105.0  epsilon: 1.0    steps: 584  evaluation reward: 150.85\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1222: Policy loss: -0.000759. Value loss: 0.858081. Entropy: 1.355711.\n",
      "Iteration 1223: Policy loss: -0.000468. Value loss: 0.271999. Entropy: 1.353917.\n",
      "Iteration 1224: Policy loss: -0.001287. Value loss: 0.202376. Entropy: 1.360518.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1225: Policy loss: 0.000354. Value loss: 0.918377. Entropy: 1.355677.\n",
      "Iteration 1226: Policy loss: -0.001163. Value loss: 0.310780. Entropy: 1.349146.\n",
      "Iteration 1227: Policy loss: -0.006323. Value loss: 0.236923. Entropy: 1.353212.\n",
      "episode: 565   score: 105.0  epsilon: 1.0    steps: 584  evaluation reward: 149.0\n",
      "episode: 566   score: 90.0  epsilon: 1.0    steps: 688  evaluation reward: 148.15\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1228: Policy loss: 0.002278. Value loss: 0.807504. Entropy: 1.351263.\n",
      "Iteration 1229: Policy loss: 0.000434. Value loss: 0.339558. Entropy: 1.353163.\n",
      "Iteration 1230: Policy loss: -0.003801. Value loss: 0.238152. Entropy: 1.354849.\n",
      "episode: 567   score: 260.0  epsilon: 1.0    steps: 984  evaluation reward: 147.65\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1231: Policy loss: 0.002346. Value loss: 0.821288. Entropy: 1.348230.\n",
      "Iteration 1232: Policy loss: -0.002111. Value loss: 0.336622. Entropy: 1.351377.\n",
      "Iteration 1233: Policy loss: -0.005252. Value loss: 0.198326. Entropy: 1.360754.\n",
      "episode: 568   score: 210.0  epsilon: 1.0    steps: 288  evaluation reward: 149.1\n",
      "episode: 569   score: 35.0  epsilon: 1.0    steps: 576  evaluation reward: 146.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1234: Policy loss: 0.004302. Value loss: 0.804820. Entropy: 1.361942.\n",
      "Iteration 1235: Policy loss: -0.000481. Value loss: 0.277509. Entropy: 1.366618.\n",
      "Iteration 1236: Policy loss: -0.000315. Value loss: 0.211536. Entropy: 1.367739.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1237: Policy loss: 0.001526. Value loss: 0.943721. Entropy: 1.362615.\n",
      "Iteration 1238: Policy loss: -0.001619. Value loss: 0.448170. Entropy: 1.364224.\n",
      "Iteration 1239: Policy loss: -0.003443. Value loss: 0.231017. Entropy: 1.365386.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1240: Policy loss: -0.003334. Value loss: 0.868149. Entropy: 1.360299.\n",
      "Iteration 1241: Policy loss: -0.003202. Value loss: 0.360511. Entropy: 1.362844.\n",
      "Iteration 1242: Policy loss: -0.005978. Value loss: 0.250494. Entropy: 1.362018.\n",
      "episode: 570   score: 430.0  epsilon: 1.0    steps: 176  evaluation reward: 149.0\n",
      "episode: 571   score: 105.0  epsilon: 1.0    steps: 536  evaluation reward: 148.2\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1243: Policy loss: 0.000841. Value loss: 1.006525. Entropy: 1.354313.\n",
      "Iteration 1244: Policy loss: 0.000273. Value loss: 0.469061. Entropy: 1.351925.\n",
      "Iteration 1245: Policy loss: -0.004056. Value loss: 0.257341. Entropy: 1.358384.\n",
      "episode: 572   score: 290.0  epsilon: 1.0    steps: 728  evaluation reward: 150.05\n",
      "episode: 573   score: 135.0  epsilon: 1.0    steps: 824  evaluation reward: 148.05\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1246: Policy loss: 0.000035. Value loss: 0.779754. Entropy: 1.358496.\n",
      "Iteration 1247: Policy loss: -0.004687. Value loss: 0.297258. Entropy: 1.360243.\n",
      "Iteration 1248: Policy loss: -0.006261. Value loss: 0.235539. Entropy: 1.359042.\n",
      "episode: 574   score: 95.0  epsilon: 1.0    steps: 976  evaluation reward: 148.1\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1249: Policy loss: 0.000732. Value loss: 0.560094. Entropy: 1.361480.\n",
      "Iteration 1250: Policy loss: -0.003343. Value loss: 0.305925. Entropy: 1.356072.\n",
      "Iteration 1251: Policy loss: -0.007628. Value loss: 0.171902. Entropy: 1.365070.\n",
      "episode: 575   score: 180.0  epsilon: 1.0    steps: 576  evaluation reward: 148.35\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1252: Policy loss: 0.003363. Value loss: 0.712090. Entropy: 1.363037.\n",
      "Iteration 1253: Policy loss: -0.006361. Value loss: 0.396558. Entropy: 1.355331.\n",
      "Iteration 1254: Policy loss: 0.000386. Value loss: 0.282948. Entropy: 1.355069.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1255: Policy loss: -0.001563. Value loss: 1.010824. Entropy: 1.340096.\n",
      "Iteration 1256: Policy loss: -0.000505. Value loss: 0.385527. Entropy: 1.345738.\n",
      "Iteration 1257: Policy loss: -0.003472. Value loss: 0.261372. Entropy: 1.346246.\n",
      "episode: 576   score: 255.0  epsilon: 1.0    steps: 96  evaluation reward: 150.0\n",
      "episode: 577   score: 300.0  epsilon: 1.0    steps: 272  evaluation reward: 151.9\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1258: Policy loss: 0.001614. Value loss: 0.684857. Entropy: 1.359143.\n",
      "Iteration 1259: Policy loss: 0.001186. Value loss: 0.227375. Entropy: 1.359953.\n",
      "Iteration 1260: Policy loss: -0.002694. Value loss: 0.153058. Entropy: 1.358479.\n",
      "episode: 578   score: 105.0  epsilon: 1.0    steps: 696  evaluation reward: 149.75\n",
      "episode: 579   score: 170.0  epsilon: 1.0    steps: 944  evaluation reward: 149.6\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1261: Policy loss: 0.000884. Value loss: 0.644876. Entropy: 1.362238.\n",
      "Iteration 1262: Policy loss: -0.002492. Value loss: 0.274105. Entropy: 1.359094.\n",
      "Iteration 1263: Policy loss: -0.004771. Value loss: 0.170718. Entropy: 1.361123.\n",
      "episode: 580   score: 105.0  epsilon: 1.0    steps: 160  evaluation reward: 150.45\n",
      "episode: 581   score: 270.0  epsilon: 1.0    steps: 216  evaluation reward: 152.5\n",
      "episode: 582   score: 35.0  epsilon: 1.0    steps: 312  evaluation reward: 152.35\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1264: Policy loss: -0.002695. Value loss: 0.695575. Entropy: 1.363277.\n",
      "Iteration 1265: Policy loss: -0.003372. Value loss: 0.331307. Entropy: 1.368721.\n",
      "Iteration 1266: Policy loss: -0.004322. Value loss: 0.218345. Entropy: 1.364693.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1267: Policy loss: -0.001630. Value loss: 1.066397. Entropy: 1.348591.\n",
      "Iteration 1268: Policy loss: -0.002651. Value loss: 0.499519. Entropy: 1.348503.\n",
      "Iteration 1269: Policy loss: -0.005533. Value loss: 0.276611. Entropy: 1.351085.\n",
      "episode: 583   score: 45.0  epsilon: 1.0    steps: 136  evaluation reward: 150.7\n",
      "episode: 584   score: 105.0  epsilon: 1.0    steps: 704  evaluation reward: 150.2\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1270: Policy loss: -0.000475. Value loss: 0.723709. Entropy: 1.357914.\n",
      "Iteration 1271: Policy loss: -0.004375. Value loss: 0.322101. Entropy: 1.354432.\n",
      "Iteration 1272: Policy loss: -0.004129. Value loss: 0.138447. Entropy: 1.353517.\n",
      "episode: 585   score: 575.0  epsilon: 1.0    steps: 344  evaluation reward: 155.05\n",
      "episode: 586   score: 20.0  epsilon: 1.0    steps: 728  evaluation reward: 155.2\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1273: Policy loss: -0.002441. Value loss: 0.776475. Entropy: 1.343519.\n",
      "Iteration 1274: Policy loss: -0.003837. Value loss: 0.386479. Entropy: 1.348252.\n",
      "Iteration 1275: Policy loss: -0.007736. Value loss: 0.225761. Entropy: 1.354201.\n",
      "episode: 587   score: 125.0  epsilon: 1.0    steps: 248  evaluation reward: 155.85\n",
      "episode: 588   score: 110.0  epsilon: 1.0    steps: 384  evaluation reward: 156.45\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1276: Policy loss: 0.001596. Value loss: 0.847879. Entropy: 1.345737.\n",
      "Iteration 1277: Policy loss: -0.003670. Value loss: 0.460828. Entropy: 1.348901.\n",
      "Iteration 1278: Policy loss: -0.006973. Value loss: 0.256300. Entropy: 1.341728.\n",
      "episode: 589   score: 60.0  epsilon: 1.0    steps: 376  evaluation reward: 156.45\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1279: Policy loss: -0.003365. Value loss: 1.119578. Entropy: 1.344544.\n",
      "Iteration 1280: Policy loss: -0.001686. Value loss: 0.548036. Entropy: 1.350274.\n",
      "Iteration 1281: Policy loss: -0.004537. Value loss: 0.300750. Entropy: 1.340245.\n",
      "episode: 590   score: 60.0  epsilon: 1.0    steps: 176  evaluation reward: 156.65\n",
      "episode: 591   score: 90.0  epsilon: 1.0    steps: 448  evaluation reward: 157.25\n",
      "episode: 592   score: 80.0  epsilon: 1.0    steps: 888  evaluation reward: 156.25\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1282: Policy loss: -0.000916. Value loss: 0.707445. Entropy: 1.345983.\n",
      "Iteration 1283: Policy loss: -0.001947. Value loss: 0.354578. Entropy: 1.336028.\n",
      "Iteration 1284: Policy loss: -0.008696. Value loss: 0.201879. Entropy: 1.336814.\n",
      "episode: 593   score: 15.0  epsilon: 1.0    steps: 608  evaluation reward: 155.4\n",
      "episode: 594   score: 390.0  epsilon: 1.0    steps: 912  evaluation reward: 157.25\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1285: Policy loss: -0.002214. Value loss: 0.597595. Entropy: 1.333365.\n",
      "Iteration 1286: Policy loss: -0.001589. Value loss: 0.353475. Entropy: 1.331659.\n",
      "Iteration 1287: Policy loss: -0.003620. Value loss: 0.240857. Entropy: 1.331984.\n",
      "episode: 595   score: 120.0  epsilon: 1.0    steps: 464  evaluation reward: 157.75\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1288: Policy loss: 0.000449. Value loss: 0.651748. Entropy: 1.336822.\n",
      "Iteration 1289: Policy loss: -0.001325. Value loss: 0.242347. Entropy: 1.337164.\n",
      "Iteration 1290: Policy loss: -0.004282. Value loss: 0.177957. Entropy: 1.333604.\n",
      "episode: 596   score: 20.0  epsilon: 1.0    steps: 528  evaluation reward: 155.9\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1291: Policy loss: 0.002725. Value loss: 1.111456. Entropy: 1.328723.\n",
      "Iteration 1292: Policy loss: -0.000441. Value loss: 0.445309. Entropy: 1.334397.\n",
      "Iteration 1293: Policy loss: -0.006101. Value loss: 0.251269. Entropy: 1.337664.\n",
      "episode: 597   score: 30.0  epsilon: 1.0    steps: 392  evaluation reward: 155.6\n",
      "episode: 598   score: 90.0  epsilon: 1.0    steps: 688  evaluation reward: 155.6\n",
      "Training network. lr: 0.000240. clip: 0.096166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1294: Policy loss: -0.000931. Value loss: 0.677435. Entropy: 1.314547.\n",
      "Iteration 1295: Policy loss: -0.009079. Value loss: 0.221457. Entropy: 1.316043.\n",
      "Iteration 1296: Policy loss: -0.009257. Value loss: 0.148009. Entropy: 1.317224.\n",
      "episode: 599   score: 45.0  epsilon: 1.0    steps: 680  evaluation reward: 154.4\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1297: Policy loss: -0.002923. Value loss: 0.810211. Entropy: 1.326381.\n",
      "Iteration 1298: Policy loss: -0.002526. Value loss: 0.373924. Entropy: 1.318807.\n",
      "Iteration 1299: Policy loss: -0.003613. Value loss: 0.245648. Entropy: 1.318836.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1300: Policy loss: 0.000909. Value loss: 0.772111. Entropy: 1.320058.\n",
      "Iteration 1301: Policy loss: -0.002435. Value loss: 0.330619. Entropy: 1.311717.\n",
      "Iteration 1302: Policy loss: -0.006932. Value loss: 0.227337. Entropy: 1.308321.\n",
      "episode: 600   score: 275.0  epsilon: 1.0    steps: 328  evaluation reward: 156.05\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1303: Policy loss: 0.001026. Value loss: 0.833464. Entropy: 1.319082.\n",
      "Iteration 1304: Policy loss: -0.000937. Value loss: 0.358301. Entropy: 1.320818.\n",
      "Iteration 1305: Policy loss: -0.001592. Value loss: 0.202687. Entropy: 1.312171.\n",
      "now time :  2019-02-28 11:10:13.991504\n",
      "episode: 601   score: 160.0  epsilon: 1.0    steps: 88  evaluation reward: 155.5\n",
      "episode: 602   score: 105.0  epsilon: 1.0    steps: 136  evaluation reward: 152.8\n",
      "episode: 603   score: 105.0  epsilon: 1.0    steps: 144  evaluation reward: 152.35\n",
      "episode: 604   score: 125.0  epsilon: 1.0    steps: 208  evaluation reward: 150.95\n",
      "episode: 605   score: 190.0  epsilon: 1.0    steps: 376  evaluation reward: 152.65\n",
      "episode: 606   score: 135.0  epsilon: 1.0    steps: 1024  evaluation reward: 151.9\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1306: Policy loss: -0.003895. Value loss: 0.558519. Entropy: 1.348999.\n",
      "Iteration 1307: Policy loss: -0.004828. Value loss: 0.290951. Entropy: 1.350510.\n",
      "Iteration 1308: Policy loss: -0.006383. Value loss: 0.175931. Entropy: 1.347343.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1309: Policy loss: 0.002611. Value loss: 1.094369. Entropy: 1.347942.\n",
      "Iteration 1310: Policy loss: -0.002676. Value loss: 0.534414. Entropy: 1.353979.\n",
      "Iteration 1311: Policy loss: -0.003355. Value loss: 0.354418. Entropy: 1.350451.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1312: Policy loss: 0.002095. Value loss: 0.712068. Entropy: 1.332313.\n",
      "Iteration 1313: Policy loss: -0.005086. Value loss: 0.348760. Entropy: 1.343849.\n",
      "Iteration 1314: Policy loss: -0.004067. Value loss: 0.215313. Entropy: 1.341967.\n",
      "episode: 607   score: 40.0  epsilon: 1.0    steps: 296  evaluation reward: 149.75\n",
      "episode: 608   score: 55.0  epsilon: 1.0    steps: 456  evaluation reward: 150.15\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1315: Policy loss: -0.000631. Value loss: 0.726243. Entropy: 1.352550.\n",
      "Iteration 1316: Policy loss: -0.001257. Value loss: 0.306734. Entropy: 1.344657.\n",
      "Iteration 1317: Policy loss: -0.006452. Value loss: 0.222478. Entropy: 1.349788.\n",
      "episode: 609   score: 60.0  epsilon: 1.0    steps: 928  evaluation reward: 148.5\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1318: Policy loss: -0.001470. Value loss: 0.816504. Entropy: 1.342422.\n",
      "Iteration 1319: Policy loss: 0.000039. Value loss: 0.307112. Entropy: 1.355988.\n",
      "Iteration 1320: Policy loss: -0.004577. Value loss: 0.201119. Entropy: 1.343715.\n",
      "episode: 610   score: 120.0  epsilon: 1.0    steps: 48  evaluation reward: 147.55\n",
      "episode: 611   score: 135.0  epsilon: 1.0    steps: 296  evaluation reward: 148.6\n",
      "episode: 612   score: 180.0  epsilon: 1.0    steps: 440  evaluation reward: 148.9\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1321: Policy loss: 0.004772. Value loss: 0.999230. Entropy: 1.367173.\n",
      "Iteration 1322: Policy loss: 0.002704. Value loss: 0.254683. Entropy: 1.370556.\n",
      "Iteration 1323: Policy loss: 0.000345. Value loss: 0.148236. Entropy: 1.369620.\n",
      "episode: 613   score: 300.0  epsilon: 1.0    steps: 496  evaluation reward: 150.8\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1324: Policy loss: -0.000668. Value loss: 1.085553. Entropy: 1.368058.\n",
      "Iteration 1325: Policy loss: -0.000712. Value loss: 0.511926. Entropy: 1.358131.\n",
      "Iteration 1326: Policy loss: -0.012212. Value loss: 0.438854. Entropy: 1.359433.\n",
      "episode: 614   score: 35.0  epsilon: 1.0    steps: 32  evaluation reward: 146.45\n",
      "episode: 615   score: 90.0  epsilon: 1.0    steps: 800  evaluation reward: 146.65\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1327: Policy loss: -0.000384. Value loss: 1.252028. Entropy: 1.353152.\n",
      "Iteration 1328: Policy loss: -0.000221. Value loss: 0.418707. Entropy: 1.350130.\n",
      "Iteration 1329: Policy loss: -0.002488. Value loss: 0.235512. Entropy: 1.350193.\n",
      "episode: 616   score: 415.0  epsilon: 1.0    steps: 96  evaluation reward: 148.2\n",
      "episode: 617   score: 105.0  epsilon: 1.0    steps: 920  evaluation reward: 146.8\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1330: Policy loss: 0.000360. Value loss: 0.867596. Entropy: 1.360098.\n",
      "Iteration 1331: Policy loss: -0.003578. Value loss: 0.450467. Entropy: 1.367611.\n",
      "Iteration 1332: Policy loss: -0.002982. Value loss: 0.319486. Entropy: 1.366460.\n",
      "episode: 618   score: 120.0  epsilon: 1.0    steps: 280  evaluation reward: 144.65\n",
      "episode: 619   score: 75.0  epsilon: 1.0    steps: 432  evaluation reward: 143.95\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1333: Policy loss: 0.002103. Value loss: 0.788501. Entropy: 1.361397.\n",
      "Iteration 1334: Policy loss: 0.001835. Value loss: 0.275659. Entropy: 1.365166.\n",
      "Iteration 1335: Policy loss: -0.001428. Value loss: 0.226274. Entropy: 1.363320.\n",
      "episode: 620   score: 125.0  epsilon: 1.0    steps: 480  evaluation reward: 144.85\n",
      "episode: 621   score: 15.0  epsilon: 1.0    steps: 616  evaluation reward: 140.95\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1336: Policy loss: -0.000996. Value loss: 0.822179. Entropy: 1.372547.\n",
      "Iteration 1337: Policy loss: -0.002860. Value loss: 0.322799. Entropy: 1.369930.\n",
      "Iteration 1338: Policy loss: -0.004701. Value loss: 0.189948. Entropy: 1.369592.\n",
      "episode: 622   score: 90.0  epsilon: 1.0    steps: 776  evaluation reward: 140.0\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1339: Policy loss: -0.002684. Value loss: 0.681479. Entropy: 1.369418.\n",
      "Iteration 1340: Policy loss: -0.003191. Value loss: 0.473671. Entropy: 1.369276.\n",
      "Iteration 1341: Policy loss: -0.007046. Value loss: 0.346957. Entropy: 1.370329.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1342: Policy loss: -0.002706. Value loss: 0.724973. Entropy: 1.367768.\n",
      "Iteration 1343: Policy loss: -0.002155. Value loss: 0.336460. Entropy: 1.366467.\n",
      "Iteration 1344: Policy loss: -0.009561. Value loss: 0.225421. Entropy: 1.366112.\n",
      "episode: 623   score: 195.0  epsilon: 1.0    steps: 8  evaluation reward: 141.15\n",
      "episode: 624   score: 155.0  epsilon: 1.0    steps: 256  evaluation reward: 141.35\n",
      "episode: 625   score: 120.0  epsilon: 1.0    steps: 552  evaluation reward: 142.15\n",
      "episode: 626   score: 180.0  epsilon: 1.0    steps: 664  evaluation reward: 142.5\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1345: Policy loss: 0.000425. Value loss: 0.512466. Entropy: 1.362240.\n",
      "Iteration 1346: Policy loss: -0.000931. Value loss: 0.223669. Entropy: 1.362640.\n",
      "Iteration 1347: Policy loss: -0.002631. Value loss: 0.138280. Entropy: 1.358967.\n",
      "episode: 627   score: 145.0  epsilon: 1.0    steps: 576  evaluation reward: 143.15\n",
      "episode: 628   score: 75.0  epsilon: 1.0    steps: 824  evaluation reward: 142.55\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1348: Policy loss: 0.002446. Value loss: 1.019307. Entropy: 1.361401.\n",
      "Iteration 1349: Policy loss: -0.002045. Value loss: 0.608594. Entropy: 1.356087.\n",
      "Iteration 1350: Policy loss: -0.005435. Value loss: 0.383431. Entropy: 1.357964.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1351: Policy loss: 0.004015. Value loss: 0.969128. Entropy: 1.345520.\n",
      "Iteration 1352: Policy loss: -0.003159. Value loss: 0.347707. Entropy: 1.344859.\n",
      "Iteration 1353: Policy loss: -0.002889. Value loss: 0.213936. Entropy: 1.348712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 629   score: 80.0  epsilon: 1.0    steps: 248  evaluation reward: 142.0\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1354: Policy loss: -0.000396. Value loss: 1.252330. Entropy: 1.356126.\n",
      "Iteration 1355: Policy loss: -0.002690. Value loss: 0.413567. Entropy: 1.353830.\n",
      "Iteration 1356: Policy loss: -0.007207. Value loss: 0.253690. Entropy: 1.353251.\n",
      "episode: 630   score: 85.0  epsilon: 1.0    steps: 600  evaluation reward: 140.75\n",
      "episode: 631   score: 245.0  epsilon: 1.0    steps: 968  evaluation reward: 141.3\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1357: Policy loss: -0.001759. Value loss: 0.937263. Entropy: 1.358657.\n",
      "Iteration 1358: Policy loss: -0.006117. Value loss: 0.298684. Entropy: 1.355706.\n",
      "Iteration 1359: Policy loss: -0.009658. Value loss: 0.198668. Entropy: 1.359261.\n",
      "episode: 632   score: 130.0  epsilon: 1.0    steps: 696  evaluation reward: 140.25\n",
      "episode: 633   score: 90.0  epsilon: 1.0    steps: 864  evaluation reward: 139.6\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1360: Policy loss: 0.001639. Value loss: 1.011219. Entropy: 1.366001.\n",
      "Iteration 1361: Policy loss: -0.002788. Value loss: 0.451098. Entropy: 1.359551.\n",
      "Iteration 1362: Policy loss: -0.007424. Value loss: 0.270798. Entropy: 1.362818.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1363: Policy loss: 0.000386. Value loss: 1.224326. Entropy: 1.355652.\n",
      "Iteration 1364: Policy loss: -0.005671. Value loss: 0.374901. Entropy: 1.362233.\n",
      "Iteration 1365: Policy loss: -0.012448. Value loss: 0.223426. Entropy: 1.360639.\n",
      "episode: 634   score: 340.0  epsilon: 1.0    steps: 488  evaluation reward: 142.2\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1366: Policy loss: -0.002150. Value loss: 1.092266. Entropy: 1.355591.\n",
      "Iteration 1367: Policy loss: -0.003643. Value loss: 0.504883. Entropy: 1.352780.\n",
      "Iteration 1368: Policy loss: -0.007431. Value loss: 0.298207. Entropy: 1.355594.\n",
      "episode: 635   score: 205.0  epsilon: 1.0    steps: 712  evaluation reward: 142.45\n",
      "episode: 636   score: 135.0  epsilon: 1.0    steps: 720  evaluation reward: 143.3\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1369: Policy loss: 0.001575. Value loss: 1.016275. Entropy: 1.359266.\n",
      "Iteration 1370: Policy loss: -0.004652. Value loss: 0.423141. Entropy: 1.346485.\n",
      "Iteration 1371: Policy loss: -0.005627. Value loss: 0.211542. Entropy: 1.347232.\n",
      "episode: 637   score: 95.0  epsilon: 1.0    steps: 776  evaluation reward: 140.15\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1372: Policy loss: -0.001730. Value loss: 0.829842. Entropy: 1.354872.\n",
      "Iteration 1373: Policy loss: -0.005682. Value loss: 0.355361. Entropy: 1.348863.\n",
      "Iteration 1374: Policy loss: -0.009290. Value loss: 0.220763. Entropy: 1.349377.\n",
      "episode: 638   score: 45.0  epsilon: 1.0    steps: 456  evaluation reward: 140.05\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1375: Policy loss: -0.003171. Value loss: 0.622902. Entropy: 1.353048.\n",
      "Iteration 1376: Policy loss: -0.001962. Value loss: 0.225405. Entropy: 1.350849.\n",
      "Iteration 1377: Policy loss: -0.008877. Value loss: 0.132470. Entropy: 1.348451.\n",
      "episode: 639   score: 135.0  epsilon: 1.0    steps: 760  evaluation reward: 140.2\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1378: Policy loss: -0.000559. Value loss: 0.837659. Entropy: 1.352396.\n",
      "Iteration 1379: Policy loss: -0.004420. Value loss: 0.314162. Entropy: 1.348367.\n",
      "Iteration 1380: Policy loss: -0.008806. Value loss: 0.206491. Entropy: 1.347255.\n",
      "episode: 640   score: 225.0  epsilon: 1.0    steps: 288  evaluation reward: 142.15\n",
      "episode: 641   score: 215.0  epsilon: 1.0    steps: 352  evaluation reward: 143.25\n",
      "episode: 642   score: 50.0  epsilon: 1.0    steps: 880  evaluation reward: 142.65\n",
      "episode: 643   score: 230.0  epsilon: 1.0    steps: 896  evaluation reward: 143.85\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1381: Policy loss: 0.001909. Value loss: 1.079924. Entropy: 1.342049.\n",
      "Iteration 1382: Policy loss: -0.002776. Value loss: 0.388186. Entropy: 1.346139.\n",
      "Iteration 1383: Policy loss: -0.007619. Value loss: 0.248961. Entropy: 1.348604.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1384: Policy loss: 0.000772. Value loss: 0.997032. Entropy: 1.338321.\n",
      "Iteration 1385: Policy loss: -0.000047. Value loss: 0.374199. Entropy: 1.346759.\n",
      "Iteration 1386: Policy loss: -0.009284. Value loss: 0.318406. Entropy: 1.342953.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1387: Policy loss: 0.000485. Value loss: 0.759655. Entropy: 1.351048.\n",
      "Iteration 1388: Policy loss: -0.004197. Value loss: 0.259071. Entropy: 1.342400.\n",
      "Iteration 1389: Policy loss: -0.002770. Value loss: 0.182236. Entropy: 1.349159.\n",
      "episode: 644   score: 130.0  epsilon: 1.0    steps: 240  evaluation reward: 144.3\n",
      "episode: 645   score: 30.0  epsilon: 1.0    steps: 648  evaluation reward: 144.0\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1390: Policy loss: -0.000145. Value loss: 0.937042. Entropy: 1.355270.\n",
      "Iteration 1391: Policy loss: -0.002145. Value loss: 0.320620. Entropy: 1.357791.\n",
      "Iteration 1392: Policy loss: -0.008578. Value loss: 0.188589. Entropy: 1.349256.\n",
      "episode: 646   score: 240.0  epsilon: 1.0    steps: 984  evaluation reward: 144.85\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1393: Policy loss: -0.000529. Value loss: 1.345367. Entropy: 1.342708.\n",
      "Iteration 1394: Policy loss: -0.004171. Value loss: 0.542206. Entropy: 1.348347.\n",
      "Iteration 1395: Policy loss: -0.006477. Value loss: 0.329082. Entropy: 1.353701.\n",
      "episode: 647   score: 370.0  epsilon: 1.0    steps: 768  evaluation reward: 148.2\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1396: Policy loss: 0.001001. Value loss: 0.736429. Entropy: 1.341478.\n",
      "Iteration 1397: Policy loss: 0.000831. Value loss: 0.421852. Entropy: 1.342058.\n",
      "Iteration 1398: Policy loss: -0.005166. Value loss: 0.275188. Entropy: 1.344547.\n",
      "episode: 648   score: 40.0  epsilon: 1.0    steps: 704  evaluation reward: 148.45\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1399: Policy loss: -0.001667. Value loss: 1.006094. Entropy: 1.349469.\n",
      "Iteration 1400: Policy loss: -0.003269. Value loss: 0.442083. Entropy: 1.345431.\n",
      "Iteration 1401: Policy loss: -0.006178. Value loss: 0.252239. Entropy: 1.347832.\n",
      "episode: 649   score: 50.0  epsilon: 1.0    steps: 96  evaluation reward: 147.9\n",
      "episode: 650   score: 195.0  epsilon: 1.0    steps: 136  evaluation reward: 143.3\n",
      "now time :  2019-02-28 11:11:24.739674\n",
      "episode: 651   score: 150.0  epsilon: 1.0    steps: 744  evaluation reward: 142.2\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1402: Policy loss: -0.001129. Value loss: 0.991531. Entropy: 1.333294.\n",
      "Iteration 1403: Policy loss: 0.000982. Value loss: 0.367451. Entropy: 1.344872.\n",
      "Iteration 1404: Policy loss: -0.005309. Value loss: 0.235534. Entropy: 1.347130.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1405: Policy loss: 0.001140. Value loss: 0.739377. Entropy: 1.341957.\n",
      "Iteration 1406: Policy loss: -0.000701. Value loss: 0.319110. Entropy: 1.336709.\n",
      "Iteration 1407: Policy loss: -0.004738. Value loss: 0.195633. Entropy: 1.347526.\n",
      "episode: 652   score: 95.0  epsilon: 1.0    steps: 752  evaluation reward: 142.3\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1408: Policy loss: -0.001732. Value loss: 0.821984. Entropy: 1.343182.\n",
      "Iteration 1409: Policy loss: -0.001583. Value loss: 0.392156. Entropy: 1.343683.\n",
      "Iteration 1410: Policy loss: -0.006010. Value loss: 0.262021. Entropy: 1.349887.\n",
      "episode: 653   score: 300.0  epsilon: 1.0    steps: 352  evaluation reward: 143.5\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1411: Policy loss: 0.003253. Value loss: 1.034968. Entropy: 1.357106.\n",
      "Iteration 1412: Policy loss: -0.001247. Value loss: 0.411090. Entropy: 1.358898.\n",
      "Iteration 1413: Policy loss: -0.001728. Value loss: 0.234766. Entropy: 1.357333.\n",
      "episode: 654   score: 150.0  epsilon: 1.0    steps: 800  evaluation reward: 143.65\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1414: Policy loss: -0.001902. Value loss: 1.396526. Entropy: 1.352842.\n",
      "Iteration 1415: Policy loss: -0.002761. Value loss: 0.495483. Entropy: 1.349383.\n",
      "Iteration 1416: Policy loss: -0.006257. Value loss: 0.329739. Entropy: 1.358643.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 655   score: 245.0  epsilon: 1.0    steps: 832  evaluation reward: 144.9\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1417: Policy loss: -0.002997. Value loss: 0.633337. Entropy: 1.372467.\n",
      "Iteration 1418: Policy loss: -0.003857. Value loss: 0.253013. Entropy: 1.374032.\n",
      "Iteration 1419: Policy loss: -0.006299. Value loss: 0.162075. Entropy: 1.373029.\n",
      "episode: 656   score: 155.0  epsilon: 1.0    steps: 64  evaluation reward: 145.8\n",
      "episode: 657   score: 150.0  epsilon: 1.0    steps: 104  evaluation reward: 146.85\n",
      "episode: 658   score: 155.0  epsilon: 1.0    steps: 544  evaluation reward: 147.55\n",
      "episode: 659   score: 155.0  epsilon: 1.0    steps: 568  evaluation reward: 148.0\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1420: Policy loss: 0.002744. Value loss: 0.526709. Entropy: 1.369523.\n",
      "Iteration 1421: Policy loss: -0.002052. Value loss: 0.287119. Entropy: 1.368962.\n",
      "Iteration 1422: Policy loss: -0.005941. Value loss: 0.182950. Entropy: 1.374452.\n",
      "episode: 660   score: 45.0  epsilon: 1.0    steps: 456  evaluation reward: 143.75\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1423: Policy loss: -0.003736. Value loss: 0.854071. Entropy: 1.367282.\n",
      "Iteration 1424: Policy loss: -0.004453. Value loss: 0.452998. Entropy: 1.371352.\n",
      "Iteration 1425: Policy loss: -0.006086. Value loss: 0.280308. Entropy: 1.368826.\n",
      "episode: 661   score: 90.0  epsilon: 1.0    steps: 192  evaluation reward: 143.45\n",
      "episode: 662   score: 240.0  epsilon: 1.0    steps: 664  evaluation reward: 145.05\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1426: Policy loss: -0.000121. Value loss: 1.000230. Entropy: 1.365643.\n",
      "Iteration 1427: Policy loss: -0.002901. Value loss: 0.383267. Entropy: 1.370379.\n",
      "Iteration 1428: Policy loss: -0.006225. Value loss: 0.255774. Entropy: 1.370455.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1429: Policy loss: -0.000340. Value loss: 0.557136. Entropy: 1.375747.\n",
      "Iteration 1430: Policy loss: 0.000737. Value loss: 0.228781. Entropy: 1.374831.\n",
      "Iteration 1431: Policy loss: -0.004949. Value loss: 0.158225. Entropy: 1.372615.\n",
      "episode: 663   score: 75.0  epsilon: 1.0    steps: 160  evaluation reward: 145.5\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1432: Policy loss: 0.001747. Value loss: 1.009942. Entropy: 1.376441.\n",
      "Iteration 1433: Policy loss: 0.000135. Value loss: 0.542410. Entropy: 1.374091.\n",
      "Iteration 1434: Policy loss: 0.001231. Value loss: 0.269279. Entropy: 1.376834.\n",
      "episode: 664   score: 90.0  epsilon: 1.0    steps: 696  evaluation reward: 145.35\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1435: Policy loss: -0.005192. Value loss: 1.080620. Entropy: 1.376985.\n",
      "Iteration 1436: Policy loss: -0.003892. Value loss: 0.427654. Entropy: 1.376653.\n",
      "Iteration 1437: Policy loss: -0.009636. Value loss: 0.209084. Entropy: 1.375001.\n",
      "episode: 665   score: 120.0  epsilon: 1.0    steps: 16  evaluation reward: 145.5\n",
      "episode: 666   score: 120.0  epsilon: 1.0    steps: 144  evaluation reward: 145.8\n",
      "episode: 667   score: 75.0  epsilon: 1.0    steps: 336  evaluation reward: 143.95\n",
      "episode: 668   score: 155.0  epsilon: 1.0    steps: 520  evaluation reward: 143.4\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1438: Policy loss: 0.003600. Value loss: 0.581577. Entropy: 1.375304.\n",
      "Iteration 1439: Policy loss: -0.002759. Value loss: 0.214614. Entropy: 1.370127.\n",
      "Iteration 1440: Policy loss: -0.004610. Value loss: 0.119722. Entropy: 1.374525.\n",
      "episode: 669   score: 135.0  epsilon: 1.0    steps: 232  evaluation reward: 144.4\n",
      "episode: 670   score: 80.0  epsilon: 1.0    steps: 432  evaluation reward: 140.9\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1441: Policy loss: 0.000662. Value loss: 0.974964. Entropy: 1.351816.\n",
      "Iteration 1442: Policy loss: -0.003293. Value loss: 0.451361. Entropy: 1.353999.\n",
      "Iteration 1443: Policy loss: -0.007184. Value loss: 0.276652. Entropy: 1.352572.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1444: Policy loss: 0.001306. Value loss: 0.768766. Entropy: 1.348941.\n",
      "Iteration 1445: Policy loss: 0.001657. Value loss: 0.403435. Entropy: 1.343778.\n",
      "Iteration 1446: Policy loss: -0.001449. Value loss: 0.293950. Entropy: 1.345018.\n",
      "episode: 671   score: 110.0  epsilon: 1.0    steps: 352  evaluation reward: 140.95\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1447: Policy loss: 0.000935. Value loss: 0.846618. Entropy: 1.346157.\n",
      "Iteration 1448: Policy loss: 0.000070. Value loss: 0.302072. Entropy: 1.348834.\n",
      "Iteration 1449: Policy loss: -0.002302. Value loss: 0.208083. Entropy: 1.349554.\n",
      "episode: 672   score: 30.0  epsilon: 1.0    steps: 48  evaluation reward: 138.35\n",
      "episode: 673   score: 35.0  epsilon: 1.0    steps: 384  evaluation reward: 137.35\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1450: Policy loss: 0.000804. Value loss: 1.233436. Entropy: 1.359417.\n",
      "Iteration 1451: Policy loss: -0.004172. Value loss: 0.505251. Entropy: 1.356170.\n",
      "Iteration 1452: Policy loss: -0.004207. Value loss: 0.378298. Entropy: 1.352313.\n",
      "episode: 674   score: 35.0  epsilon: 1.0    steps: 416  evaluation reward: 136.75\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1453: Policy loss: 0.002659. Value loss: 0.763296. Entropy: 1.345215.\n",
      "Iteration 1454: Policy loss: -0.000254. Value loss: 0.359577. Entropy: 1.347815.\n",
      "Iteration 1455: Policy loss: -0.003698. Value loss: 0.228795. Entropy: 1.352238.\n",
      "episode: 675   score: 155.0  epsilon: 1.0    steps: 120  evaluation reward: 136.5\n",
      "episode: 676   score: 60.0  epsilon: 1.0    steps: 424  evaluation reward: 134.55\n",
      "episode: 677   score: 65.0  epsilon: 1.0    steps: 648  evaluation reward: 132.2\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1456: Policy loss: 0.000013. Value loss: 0.791539. Entropy: 1.337701.\n",
      "Iteration 1457: Policy loss: -0.000663. Value loss: 0.306965. Entropy: 1.346390.\n",
      "Iteration 1458: Policy loss: -0.001172. Value loss: 0.268526. Entropy: 1.341077.\n",
      "episode: 678   score: 200.0  epsilon: 1.0    steps: 40  evaluation reward: 133.15\n",
      "episode: 679   score: 230.0  epsilon: 1.0    steps: 304  evaluation reward: 133.75\n",
      "episode: 680   score: 75.0  epsilon: 1.0    steps: 504  evaluation reward: 133.45\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1459: Policy loss: 0.002780. Value loss: 1.346994. Entropy: 1.354338.\n",
      "Iteration 1460: Policy loss: 0.000276. Value loss: 0.616295. Entropy: 1.355845.\n",
      "Iteration 1461: Policy loss: -0.002291. Value loss: 0.456827. Entropy: 1.352011.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1462: Policy loss: -0.001682. Value loss: 0.565822. Entropy: 1.344256.\n",
      "Iteration 1463: Policy loss: -0.003460. Value loss: 0.241663. Entropy: 1.343954.\n",
      "Iteration 1464: Policy loss: -0.003964. Value loss: 0.165130. Entropy: 1.346005.\n",
      "episode: 681   score: 60.0  epsilon: 1.0    steps: 744  evaluation reward: 131.35\n",
      "episode: 682   score: 50.0  epsilon: 1.0    steps: 872  evaluation reward: 131.5\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1465: Policy loss: -0.001294. Value loss: 1.006979. Entropy: 1.360171.\n",
      "Iteration 1466: Policy loss: -0.001268. Value loss: 0.422941. Entropy: 1.354370.\n",
      "Iteration 1467: Policy loss: -0.002754. Value loss: 0.267658. Entropy: 1.354479.\n",
      "episode: 683   score: 95.0  epsilon: 1.0    steps: 216  evaluation reward: 132.0\n",
      "episode: 684   score: 145.0  epsilon: 1.0    steps: 1024  evaluation reward: 132.4\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1468: Policy loss: 0.001280. Value loss: 0.871606. Entropy: 1.357722.\n",
      "Iteration 1469: Policy loss: -0.002345. Value loss: 0.297396. Entropy: 1.358242.\n",
      "Iteration 1470: Policy loss: -0.005212. Value loss: 0.241974. Entropy: 1.360822.\n",
      "episode: 685   score: 355.0  epsilon: 1.0    steps: 136  evaluation reward: 130.2\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1471: Policy loss: -0.001352. Value loss: 0.930701. Entropy: 1.365478.\n",
      "Iteration 1472: Policy loss: -0.004267. Value loss: 0.398250. Entropy: 1.361594.\n",
      "Iteration 1473: Policy loss: -0.009048. Value loss: 0.202781. Entropy: 1.364558.\n",
      "episode: 686   score: 50.0  epsilon: 1.0    steps: 312  evaluation reward: 130.5\n",
      "episode: 687   score: 55.0  epsilon: 1.0    steps: 648  evaluation reward: 129.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1474: Policy loss: 0.002204. Value loss: 0.682544. Entropy: 1.359805.\n",
      "Iteration 1475: Policy loss: -0.000407. Value loss: 0.304604. Entropy: 1.357856.\n",
      "Iteration 1476: Policy loss: -0.005331. Value loss: 0.229893. Entropy: 1.358539.\n",
      "episode: 688   score: 90.0  epsilon: 1.0    steps: 208  evaluation reward: 129.6\n",
      "episode: 689   score: 65.0  epsilon: 1.0    steps: 944  evaluation reward: 129.65\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1477: Policy loss: 0.000690. Value loss: 0.626612. Entropy: 1.358550.\n",
      "Iteration 1478: Policy loss: -0.000540. Value loss: 0.290469. Entropy: 1.354664.\n",
      "Iteration 1479: Policy loss: -0.007977. Value loss: 0.229008. Entropy: 1.360906.\n",
      "episode: 690   score: 150.0  epsilon: 1.0    steps: 128  evaluation reward: 130.55\n",
      "episode: 691   score: 50.0  epsilon: 1.0    steps: 448  evaluation reward: 130.15\n",
      "episode: 692   score: 55.0  epsilon: 1.0    steps: 624  evaluation reward: 129.9\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1480: Policy loss: 0.000462. Value loss: 0.640171. Entropy: 1.346231.\n",
      "Iteration 1481: Policy loss: -0.001101. Value loss: 0.277266. Entropy: 1.352127.\n",
      "Iteration 1482: Policy loss: 0.000191. Value loss: 0.213279. Entropy: 1.355401.\n",
      "episode: 693   score: 475.0  epsilon: 1.0    steps: 216  evaluation reward: 134.5\n",
      "episode: 694   score: 80.0  epsilon: 1.0    steps: 496  evaluation reward: 131.4\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1483: Policy loss: 0.002917. Value loss: 0.952655. Entropy: 1.338343.\n",
      "Iteration 1484: Policy loss: -0.001254. Value loss: 0.466624. Entropy: 1.340804.\n",
      "Iteration 1485: Policy loss: -0.005208. Value loss: 0.296107. Entropy: 1.342279.\n",
      "episode: 695   score: 55.0  epsilon: 1.0    steps: 152  evaluation reward: 130.75\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1486: Policy loss: 0.000973. Value loss: 1.101754. Entropy: 1.319647.\n",
      "Iteration 1487: Policy loss: 0.000706. Value loss: 0.409290. Entropy: 1.326742.\n",
      "Iteration 1488: Policy loss: -0.001774. Value loss: 0.283407. Entropy: 1.328932.\n",
      "episode: 696   score: 5.0  epsilon: 1.0    steps: 920  evaluation reward: 130.6\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1489: Policy loss: 0.001916. Value loss: 0.959477. Entropy: 1.356426.\n",
      "Iteration 1490: Policy loss: -0.000158. Value loss: 0.389987. Entropy: 1.349819.\n",
      "Iteration 1491: Policy loss: -0.002820. Value loss: 0.239533. Entropy: 1.356314.\n",
      "episode: 697   score: 105.0  epsilon: 1.0    steps: 184  evaluation reward: 131.35\n",
      "episode: 698   score: 60.0  epsilon: 1.0    steps: 424  evaluation reward: 131.05\n",
      "episode: 699   score: 150.0  epsilon: 1.0    steps: 752  evaluation reward: 132.1\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1492: Policy loss: 0.000673. Value loss: 0.765319. Entropy: 1.350212.\n",
      "Iteration 1493: Policy loss: -0.001559. Value loss: 0.275751. Entropy: 1.335540.\n",
      "Iteration 1494: Policy loss: -0.001070. Value loss: 0.191408. Entropy: 1.342444.\n",
      "episode: 700   score: 60.0  epsilon: 1.0    steps: 808  evaluation reward: 129.95\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1495: Policy loss: -0.002150. Value loss: 0.819150. Entropy: 1.343874.\n",
      "Iteration 1496: Policy loss: -0.003392. Value loss: 0.477645. Entropy: 1.346449.\n",
      "Iteration 1497: Policy loss: -0.010314. Value loss: 0.333715. Entropy: 1.352952.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1498: Policy loss: 0.003270. Value loss: 0.892801. Entropy: 1.339879.\n",
      "Iteration 1499: Policy loss: 0.000416. Value loss: 0.308755. Entropy: 1.332404.\n",
      "Iteration 1500: Policy loss: -0.002415. Value loss: 0.204447. Entropy: 1.333997.\n",
      "now time :  2019-02-28 11:12:36.119123\n",
      "episode: 701   score: 150.0  epsilon: 1.0    steps: 416  evaluation reward: 129.85\n",
      "episode: 702   score: 375.0  epsilon: 1.0    steps: 816  evaluation reward: 132.55\n",
      "episode: 703   score: 105.0  epsilon: 1.0    steps: 952  evaluation reward: 132.55\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1501: Policy loss: 0.001527. Value loss: 1.198689. Entropy: 1.354030.\n",
      "Iteration 1502: Policy loss: -0.002504. Value loss: 0.440697. Entropy: 1.350251.\n",
      "Iteration 1503: Policy loss: -0.006394. Value loss: 0.317501. Entropy: 1.350538.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1504: Policy loss: -0.000371. Value loss: 0.820517. Entropy: 1.355199.\n",
      "Iteration 1505: Policy loss: -0.004397. Value loss: 0.436870. Entropy: 1.350972.\n",
      "Iteration 1506: Policy loss: -0.006920. Value loss: 0.317146. Entropy: 1.353897.\n",
      "episode: 704   score: 410.0  epsilon: 1.0    steps: 208  evaluation reward: 135.4\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1507: Policy loss: -0.001184. Value loss: 1.219568. Entropy: 1.357533.\n",
      "Iteration 1508: Policy loss: -0.005798. Value loss: 0.471779. Entropy: 1.363265.\n",
      "Iteration 1509: Policy loss: -0.003716. Value loss: 0.291826. Entropy: 1.354062.\n",
      "episode: 705   score: 90.0  epsilon: 1.0    steps: 480  evaluation reward: 134.4\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1510: Policy loss: -0.000363. Value loss: 1.179244. Entropy: 1.343494.\n",
      "Iteration 1511: Policy loss: 0.002337. Value loss: 0.463697. Entropy: 1.330662.\n",
      "Iteration 1512: Policy loss: -0.002730. Value loss: 0.234968. Entropy: 1.329492.\n",
      "episode: 706   score: 10.0  epsilon: 1.0    steps: 216  evaluation reward: 133.15\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1513: Policy loss: 0.002887. Value loss: 0.559945. Entropy: 1.341629.\n",
      "Iteration 1514: Policy loss: -0.000727. Value loss: 0.204894. Entropy: 1.345536.\n",
      "Iteration 1515: Policy loss: -0.002807. Value loss: 0.156924. Entropy: 1.341305.\n",
      "episode: 707   score: 125.0  epsilon: 1.0    steps: 80  evaluation reward: 134.0\n",
      "episode: 708   score: 225.0  epsilon: 1.0    steps: 592  evaluation reward: 135.7\n",
      "episode: 709   score: 420.0  epsilon: 1.0    steps: 600  evaluation reward: 139.3\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1516: Policy loss: -0.000126. Value loss: 0.815243. Entropy: 1.347543.\n",
      "Iteration 1517: Policy loss: -0.003298. Value loss: 0.380601. Entropy: 1.346455.\n",
      "Iteration 1518: Policy loss: -0.007025. Value loss: 0.236126. Entropy: 1.351293.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1519: Policy loss: -0.001174. Value loss: 0.964908. Entropy: 1.318736.\n",
      "Iteration 1520: Policy loss: -0.001475. Value loss: 0.398479. Entropy: 1.330761.\n",
      "Iteration 1521: Policy loss: -0.001840. Value loss: 0.233787. Entropy: 1.330619.\n",
      "episode: 710   score: 130.0  epsilon: 1.0    steps: 16  evaluation reward: 139.4\n",
      "episode: 711   score: 135.0  epsilon: 1.0    steps: 224  evaluation reward: 139.4\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1522: Policy loss: 0.000643. Value loss: 0.474852. Entropy: 1.323467.\n",
      "Iteration 1523: Policy loss: -0.000091. Value loss: 0.241198. Entropy: 1.326181.\n",
      "Iteration 1524: Policy loss: -0.003645. Value loss: 0.187956. Entropy: 1.325934.\n",
      "episode: 712   score: 50.0  epsilon: 1.0    steps: 896  evaluation reward: 138.1\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1525: Policy loss: 0.000829. Value loss: 0.871076. Entropy: 1.296504.\n",
      "Iteration 1526: Policy loss: -0.004682. Value loss: 0.237318. Entropy: 1.309369.\n",
      "Iteration 1527: Policy loss: -0.001481. Value loss: 0.165130. Entropy: 1.310670.\n",
      "episode: 713   score: 180.0  epsilon: 1.0    steps: 288  evaluation reward: 136.9\n",
      "episode: 714   score: 155.0  epsilon: 1.0    steps: 840  evaluation reward: 138.1\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1528: Policy loss: 0.001937. Value loss: 0.721872. Entropy: 1.317714.\n",
      "Iteration 1529: Policy loss: -0.003670. Value loss: 0.312415. Entropy: 1.321910.\n",
      "Iteration 1530: Policy loss: -0.006433. Value loss: 0.137866. Entropy: 1.323003.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1531: Policy loss: 0.003012. Value loss: 0.979523. Entropy: 1.329183.\n",
      "Iteration 1532: Policy loss: -0.001675. Value loss: 0.487062. Entropy: 1.325646.\n",
      "Iteration 1533: Policy loss: -0.003308. Value loss: 0.350773. Entropy: 1.328004.\n",
      "episode: 715   score: 120.0  epsilon: 1.0    steps: 64  evaluation reward: 138.4\n",
      "episode: 716   score: 460.0  epsilon: 1.0    steps: 184  evaluation reward: 138.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1534: Policy loss: -0.000023. Value loss: 0.818349. Entropy: 1.324992.\n",
      "Iteration 1535: Policy loss: 0.002523. Value loss: 0.382264. Entropy: 1.326054.\n",
      "Iteration 1536: Policy loss: -0.003798. Value loss: 0.241669. Entropy: 1.329968.\n",
      "episode: 717   score: 190.0  epsilon: 1.0    steps: 144  evaluation reward: 139.7\n",
      "episode: 718   score: 15.0  epsilon: 1.0    steps: 480  evaluation reward: 138.65\n",
      "episode: 719   score: 140.0  epsilon: 1.0    steps: 680  evaluation reward: 139.3\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1537: Policy loss: 0.005005. Value loss: 0.587623. Entropy: 1.321669.\n",
      "Iteration 1538: Policy loss: -0.000920. Value loss: 0.188714. Entropy: 1.349552.\n",
      "Iteration 1539: Policy loss: -0.002284. Value loss: 0.133977. Entropy: 1.346380.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1540: Policy loss: -0.001463. Value loss: 1.171591. Entropy: 1.338230.\n",
      "Iteration 1541: Policy loss: 0.000397. Value loss: 0.543787. Entropy: 1.329341.\n",
      "Iteration 1542: Policy loss: -0.001657. Value loss: 0.348306. Entropy: 1.333884.\n",
      "episode: 720   score: 80.0  epsilon: 1.0    steps: 16  evaluation reward: 138.85\n",
      "episode: 721   score: 55.0  epsilon: 1.0    steps: 848  evaluation reward: 139.25\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1543: Policy loss: -0.000756. Value loss: 0.815493. Entropy: 1.306872.\n",
      "Iteration 1544: Policy loss: -0.003041. Value loss: 0.350141. Entropy: 1.304825.\n",
      "Iteration 1545: Policy loss: -0.005300. Value loss: 0.207924. Entropy: 1.309624.\n",
      "episode: 722   score: 135.0  epsilon: 1.0    steps: 48  evaluation reward: 139.7\n",
      "episode: 723   score: 15.0  epsilon: 1.0    steps: 88  evaluation reward: 137.9\n",
      "episode: 724   score: 35.0  epsilon: 1.0    steps: 704  evaluation reward: 136.7\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1546: Policy loss: 0.001177. Value loss: 0.689126. Entropy: 1.319900.\n",
      "Iteration 1547: Policy loss: -0.001594. Value loss: 0.292318. Entropy: 1.325628.\n",
      "Iteration 1548: Policy loss: -0.006029. Value loss: 0.189944. Entropy: 1.319899.\n",
      "episode: 725   score: 180.0  epsilon: 1.0    steps: 312  evaluation reward: 137.3\n",
      "episode: 726   score: 75.0  epsilon: 1.0    steps: 1024  evaluation reward: 136.25\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1549: Policy loss: 0.002256. Value loss: 0.475961. Entropy: 1.323686.\n",
      "Iteration 1550: Policy loss: -0.001693. Value loss: 0.196629. Entropy: 1.322047.\n",
      "Iteration 1551: Policy loss: -0.004180. Value loss: 0.145906. Entropy: 1.328457.\n",
      "episode: 727   score: 120.0  epsilon: 1.0    steps: 456  evaluation reward: 136.0\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1552: Policy loss: 0.000330. Value loss: 0.672535. Entropy: 1.318904.\n",
      "Iteration 1553: Policy loss: -0.004900. Value loss: 0.238351. Entropy: 1.317066.\n",
      "Iteration 1554: Policy loss: -0.004906. Value loss: 0.171214. Entropy: 1.320909.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1555: Policy loss: -0.000639. Value loss: 1.115019. Entropy: 1.317276.\n",
      "Iteration 1556: Policy loss: -0.001546. Value loss: 0.456080. Entropy: 1.312175.\n",
      "Iteration 1557: Policy loss: -0.008367. Value loss: 0.285942. Entropy: 1.312045.\n",
      "episode: 728   score: 125.0  epsilon: 1.0    steps: 352  evaluation reward: 136.5\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1558: Policy loss: 0.000399. Value loss: 0.732961. Entropy: 1.316799.\n",
      "Iteration 1559: Policy loss: -0.003166. Value loss: 0.328337. Entropy: 1.315253.\n",
      "Iteration 1560: Policy loss: -0.004196. Value loss: 0.173267. Entropy: 1.321618.\n",
      "episode: 729   score: 135.0  epsilon: 1.0    steps: 168  evaluation reward: 137.05\n",
      "episode: 730   score: 35.0  epsilon: 1.0    steps: 400  evaluation reward: 136.55\n",
      "episode: 731   score: 215.0  epsilon: 1.0    steps: 944  evaluation reward: 136.25\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1561: Policy loss: 0.001951. Value loss: 0.486682. Entropy: 1.341197.\n",
      "Iteration 1562: Policy loss: -0.004076. Value loss: 0.200404. Entropy: 1.335866.\n",
      "Iteration 1563: Policy loss: -0.003418. Value loss: 0.151504. Entropy: 1.334292.\n",
      "episode: 732   score: 105.0  epsilon: 1.0    steps: 96  evaluation reward: 136.0\n",
      "episode: 733   score: 155.0  epsilon: 1.0    steps: 216  evaluation reward: 136.65\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1564: Policy loss: -0.003157. Value loss: 0.777495. Entropy: 1.316004.\n",
      "Iteration 1565: Policy loss: -0.000794. Value loss: 0.433372. Entropy: 1.316558.\n",
      "Iteration 1566: Policy loss: -0.007390. Value loss: 0.311339. Entropy: 1.315651.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1567: Policy loss: -0.004526. Value loss: 0.828570. Entropy: 1.314403.\n",
      "Iteration 1568: Policy loss: 0.000670. Value loss: 0.289518. Entropy: 1.307615.\n",
      "Iteration 1569: Policy loss: -0.000987. Value loss: 0.180142. Entropy: 1.313968.\n",
      "episode: 734   score: 385.0  epsilon: 1.0    steps: 176  evaluation reward: 137.1\n",
      "episode: 735   score: 365.0  epsilon: 1.0    steps: 760  evaluation reward: 138.7\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1570: Policy loss: -0.000022. Value loss: 0.934069. Entropy: 1.310679.\n",
      "Iteration 1571: Policy loss: 0.001342. Value loss: 0.421144. Entropy: 1.311293.\n",
      "Iteration 1572: Policy loss: -0.005729. Value loss: 0.365592. Entropy: 1.312547.\n",
      "episode: 736   score: 30.0  epsilon: 1.0    steps: 632  evaluation reward: 137.65\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1573: Policy loss: 0.001769. Value loss: 0.887815. Entropy: 1.297481.\n",
      "Iteration 1574: Policy loss: -0.000279. Value loss: 0.291996. Entropy: 1.300313.\n",
      "Iteration 1575: Policy loss: -0.006213. Value loss: 0.143726. Entropy: 1.297907.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1576: Policy loss: 0.002543. Value loss: 0.665446. Entropy: 1.304706.\n",
      "Iteration 1577: Policy loss: -0.002976. Value loss: 0.313889. Entropy: 1.289722.\n",
      "Iteration 1578: Policy loss: -0.006105. Value loss: 0.202727. Entropy: 1.288156.\n",
      "episode: 737   score: 125.0  epsilon: 1.0    steps: 32  evaluation reward: 137.95\n",
      "episode: 738   score: 110.0  epsilon: 1.0    steps: 376  evaluation reward: 138.6\n",
      "episode: 739   score: 160.0  epsilon: 1.0    steps: 1008  evaluation reward: 138.85\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1579: Policy loss: -0.000684. Value loss: 0.649306. Entropy: 1.317138.\n",
      "Iteration 1580: Policy loss: -0.005143. Value loss: 0.313880. Entropy: 1.307560.\n",
      "Iteration 1581: Policy loss: -0.005921. Value loss: 0.206090. Entropy: 1.316420.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1582: Policy loss: -0.000340. Value loss: 0.716055. Entropy: 1.322263.\n",
      "Iteration 1583: Policy loss: -0.001220. Value loss: 0.239394. Entropy: 1.316165.\n",
      "Iteration 1584: Policy loss: -0.005121. Value loss: 0.213237. Entropy: 1.315314.\n",
      "episode: 740   score: 65.0  epsilon: 1.0    steps: 496  evaluation reward: 137.25\n",
      "episode: 741   score: 240.0  epsilon: 1.0    steps: 528  evaluation reward: 137.5\n",
      "episode: 742   score: 135.0  epsilon: 1.0    steps: 592  evaluation reward: 138.35\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1585: Policy loss: 0.000124. Value loss: 0.801132. Entropy: 1.314555.\n",
      "Iteration 1586: Policy loss: -0.002673. Value loss: 0.266944. Entropy: 1.317523.\n",
      "Iteration 1587: Policy loss: -0.004864. Value loss: 0.160674. Entropy: 1.315636.\n",
      "episode: 743   score: 315.0  epsilon: 1.0    steps: 160  evaluation reward: 139.2\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1588: Policy loss: 0.003280. Value loss: 0.663311. Entropy: 1.265938.\n",
      "Iteration 1589: Policy loss: -0.003139. Value loss: 0.227862. Entropy: 1.271701.\n",
      "Iteration 1590: Policy loss: -0.007947. Value loss: 0.172668. Entropy: 1.273024.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1591: Policy loss: 0.002738. Value loss: 0.923895. Entropy: 1.311771.\n",
      "Iteration 1592: Policy loss: -0.002789. Value loss: 0.311949. Entropy: 1.322640.\n",
      "Iteration 1593: Policy loss: -0.004763. Value loss: 0.194247. Entropy: 1.322594.\n",
      "episode: 744   score: 105.0  epsilon: 1.0    steps: 640  evaluation reward: 138.95\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1594: Policy loss: -0.002067. Value loss: 0.854424. Entropy: 1.297499.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1595: Policy loss: -0.003129. Value loss: 0.388333. Entropy: 1.298643.\n",
      "Iteration 1596: Policy loss: -0.012512. Value loss: 0.216197. Entropy: 1.299206.\n",
      "episode: 745   score: 210.0  epsilon: 1.0    steps: 40  evaluation reward: 140.75\n",
      "episode: 746   score: 125.0  epsilon: 1.0    steps: 568  evaluation reward: 139.6\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1597: Policy loss: -0.001233. Value loss: 0.674232. Entropy: 1.295094.\n",
      "Iteration 1598: Policy loss: -0.002748. Value loss: 0.320214. Entropy: 1.304870.\n",
      "Iteration 1599: Policy loss: -0.006904. Value loss: 0.344363. Entropy: 1.304366.\n",
      "episode: 747   score: 210.0  epsilon: 1.0    steps: 720  evaluation reward: 138.0\n",
      "episode: 748   score: 155.0  epsilon: 1.0    steps: 832  evaluation reward: 139.15\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1600: Policy loss: 0.000904. Value loss: 0.678397. Entropy: 1.312959.\n",
      "Iteration 1601: Policy loss: -0.000688. Value loss: 0.246891. Entropy: 1.319874.\n",
      "Iteration 1602: Policy loss: -0.001421. Value loss: 0.188161. Entropy: 1.308852.\n",
      "episode: 749   score: 120.0  epsilon: 1.0    steps: 336  evaluation reward: 139.85\n",
      "episode: 750   score: 240.0  epsilon: 1.0    steps: 504  evaluation reward: 140.3\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1603: Policy loss: 0.000837. Value loss: 0.660482. Entropy: 1.313020.\n",
      "Iteration 1604: Policy loss: 0.002068. Value loss: 0.244176. Entropy: 1.321500.\n",
      "Iteration 1605: Policy loss: -0.002438. Value loss: 0.204253. Entropy: 1.318955.\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1606: Policy loss: -0.001299. Value loss: 0.820294. Entropy: 1.320800.\n",
      "Iteration 1607: Policy loss: -0.003215. Value loss: 0.335591. Entropy: 1.314615.\n",
      "Iteration 1608: Policy loss: -0.003098. Value loss: 0.253060. Entropy: 1.315062.\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1609: Policy loss: 0.000408. Value loss: 0.844588. Entropy: 1.313125.\n",
      "Iteration 1610: Policy loss: -0.004678. Value loss: 0.385134. Entropy: 1.312096.\n",
      "Iteration 1611: Policy loss: -0.007406. Value loss: 0.239574. Entropy: 1.315028.\n",
      "now time :  2019-02-28 11:13:56.573695\n",
      "episode: 751   score: 155.0  epsilon: 1.0    steps: 392  evaluation reward: 140.35\n",
      "episode: 752   score: 120.0  epsilon: 1.0    steps: 624  evaluation reward: 140.6\n",
      "episode: 753   score: 210.0  epsilon: 1.0    steps: 984  evaluation reward: 139.7\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1612: Policy loss: -0.002264. Value loss: 0.672609. Entropy: 1.335211.\n",
      "Iteration 1613: Policy loss: -0.006102. Value loss: 0.289671. Entropy: 1.338030.\n",
      "Iteration 1614: Policy loss: -0.009119. Value loss: 0.234984. Entropy: 1.336157.\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1615: Policy loss: -0.000319. Value loss: 0.554984. Entropy: 1.336756.\n",
      "Iteration 1616: Policy loss: -0.005430. Value loss: 0.267825. Entropy: 1.333152.\n",
      "Iteration 1617: Policy loss: -0.005166. Value loss: 0.188918. Entropy: 1.330047.\n",
      "episode: 754   score: 110.0  epsilon: 1.0    steps: 552  evaluation reward: 139.3\n",
      "episode: 755   score: 135.0  epsilon: 1.0    steps: 872  evaluation reward: 138.2\n",
      "episode: 756   score: 125.0  epsilon: 1.0    steps: 912  evaluation reward: 137.9\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1618: Policy loss: 0.005074. Value loss: 0.664810. Entropy: 1.351613.\n",
      "Iteration 1619: Policy loss: 0.002536. Value loss: 0.226393. Entropy: 1.350529.\n",
      "Iteration 1620: Policy loss: -0.001752. Value loss: 0.156089. Entropy: 1.353665.\n",
      "episode: 757   score: 440.0  epsilon: 1.0    steps: 56  evaluation reward: 140.8\n",
      "episode: 758   score: 360.0  epsilon: 1.0    steps: 288  evaluation reward: 142.85\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1621: Policy loss: 0.000488. Value loss: 0.687445. Entropy: 1.336692.\n",
      "Iteration 1622: Policy loss: -0.005069. Value loss: 0.301887. Entropy: 1.333998.\n",
      "Iteration 1623: Policy loss: -0.005280. Value loss: 0.204076. Entropy: 1.333959.\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1624: Policy loss: 0.003110. Value loss: 0.720765. Entropy: 1.321766.\n",
      "Iteration 1625: Policy loss: 0.000573. Value loss: 0.276760. Entropy: 1.316947.\n",
      "Iteration 1626: Policy loss: -0.002416. Value loss: 0.222056. Entropy: 1.320365.\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1627: Policy loss: -0.000842. Value loss: 1.025647. Entropy: 1.342137.\n",
      "Iteration 1628: Policy loss: 0.001727. Value loss: 0.489661. Entropy: 1.342985.\n",
      "Iteration 1629: Policy loss: -0.003527. Value loss: 0.355917. Entropy: 1.338073.\n",
      "episode: 759   score: 55.0  epsilon: 1.0    steps: 32  evaluation reward: 141.85\n",
      "episode: 760   score: 155.0  epsilon: 1.0    steps: 592  evaluation reward: 142.95\n",
      "episode: 761   score: 110.0  epsilon: 1.0    steps: 856  evaluation reward: 143.15\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1630: Policy loss: 0.001569. Value loss: 0.629775. Entropy: 1.347699.\n",
      "Iteration 1631: Policy loss: -0.002124. Value loss: 0.280014. Entropy: 1.346109.\n",
      "Iteration 1632: Policy loss: -0.003876. Value loss: 0.191113. Entropy: 1.346924.\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1633: Policy loss: 0.001493. Value loss: 0.606627. Entropy: 1.341865.\n",
      "Iteration 1634: Policy loss: 0.002771. Value loss: 0.295962. Entropy: 1.350072.\n",
      "Iteration 1635: Policy loss: -0.003521. Value loss: 0.182340. Entropy: 1.345459.\n",
      "episode: 762   score: 110.0  epsilon: 1.0    steps: 184  evaluation reward: 141.85\n",
      "episode: 763   score: 260.0  epsilon: 1.0    steps: 640  evaluation reward: 143.7\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1636: Policy loss: 0.003294. Value loss: 0.535655. Entropy: 1.344713.\n",
      "Iteration 1637: Policy loss: -0.001136. Value loss: 0.265604. Entropy: 1.343669.\n",
      "Iteration 1638: Policy loss: -0.007196. Value loss: 0.185755. Entropy: 1.348276.\n",
      "episode: 764   score: 380.0  epsilon: 1.0    steps: 56  evaluation reward: 146.6\n",
      "episode: 765   score: 75.0  epsilon: 1.0    steps: 248  evaluation reward: 146.15\n",
      "episode: 766   score: 275.0  epsilon: 1.0    steps: 360  evaluation reward: 147.7\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1639: Policy loss: -0.000225. Value loss: 0.639566. Entropy: 1.347773.\n",
      "Iteration 1640: Policy loss: 0.004526. Value loss: 0.363525. Entropy: 1.344689.\n",
      "Iteration 1641: Policy loss: -0.000983. Value loss: 0.298954. Entropy: 1.347474.\n",
      "episode: 767   score: 210.0  epsilon: 1.0    steps: 952  evaluation reward: 149.05\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1642: Policy loss: 0.000115. Value loss: 0.681138. Entropy: 1.348783.\n",
      "Iteration 1643: Policy loss: -0.000232. Value loss: 0.304565. Entropy: 1.345984.\n",
      "Iteration 1644: Policy loss: -0.003724. Value loss: 0.218577. Entropy: 1.345493.\n",
      "episode: 768   score: 135.0  epsilon: 1.0    steps: 696  evaluation reward: 148.85\n",
      "episode: 769   score: 55.0  epsilon: 1.0    steps: 920  evaluation reward: 148.05\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1645: Policy loss: -0.000537. Value loss: 0.764912. Entropy: 1.337986.\n",
      "Iteration 1646: Policy loss: -0.006009. Value loss: 0.359852. Entropy: 1.341906.\n",
      "Iteration 1647: Policy loss: -0.010544. Value loss: 0.200881. Entropy: 1.341576.\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1648: Policy loss: 0.000522. Value loss: 0.701733. Entropy: 1.343534.\n",
      "Iteration 1649: Policy loss: -0.004899. Value loss: 0.267827. Entropy: 1.345250.\n",
      "Iteration 1650: Policy loss: -0.004301. Value loss: 0.184621. Entropy: 1.337878.\n",
      "episode: 770   score: 110.0  epsilon: 1.0    steps: 344  evaluation reward: 148.35\n",
      "episode: 771   score: 110.0  epsilon: 1.0    steps: 480  evaluation reward: 148.35\n",
      "episode: 772   score: 40.0  epsilon: 1.0    steps: 888  evaluation reward: 148.45\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1651: Policy loss: -0.001539. Value loss: 0.563005. Entropy: 1.352287.\n",
      "Iteration 1652: Policy loss: -0.001892. Value loss: 0.245429. Entropy: 1.350299.\n",
      "Iteration 1653: Policy loss: -0.005700. Value loss: 0.192047. Entropy: 1.348857.\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1654: Policy loss: -0.000122. Value loss: 0.777277. Entropy: 1.344491.\n",
      "Iteration 1655: Policy loss: -0.008746. Value loss: 0.319695. Entropy: 1.338078.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1656: Policy loss: -0.003938. Value loss: 0.223098. Entropy: 1.340202.\n",
      "episode: 773   score: 150.0  epsilon: 1.0    steps: 48  evaluation reward: 149.6\n",
      "episode: 774   score: 155.0  epsilon: 1.0    steps: 480  evaluation reward: 150.8\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1657: Policy loss: -0.000390. Value loss: 0.550783. Entropy: 1.360564.\n",
      "Iteration 1658: Policy loss: -0.003378. Value loss: 0.216169. Entropy: 1.361124.\n",
      "Iteration 1659: Policy loss: -0.007884. Value loss: 0.136875. Entropy: 1.362440.\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1660: Policy loss: 0.001378. Value loss: 0.585791. Entropy: 1.343158.\n",
      "Iteration 1661: Policy loss: -0.002169. Value loss: 0.280802. Entropy: 1.340505.\n",
      "Iteration 1662: Policy loss: -0.006526. Value loss: 0.192432. Entropy: 1.338882.\n",
      "episode: 775   score: 180.0  epsilon: 1.0    steps: 272  evaluation reward: 151.05\n",
      "episode: 776   score: 185.0  epsilon: 1.0    steps: 392  evaluation reward: 152.3\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1663: Policy loss: 0.005491. Value loss: 0.583644. Entropy: 1.347496.\n",
      "Iteration 1664: Policy loss: 0.000871. Value loss: 0.285042. Entropy: 1.346451.\n",
      "Iteration 1665: Policy loss: -0.004771. Value loss: 0.264601. Entropy: 1.349334.\n",
      "episode: 777   score: 215.0  epsilon: 1.0    steps: 664  evaluation reward: 153.8\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1666: Policy loss: 0.003213. Value loss: 0.972430. Entropy: 1.337408.\n",
      "Iteration 1667: Policy loss: 0.001655. Value loss: 0.381483. Entropy: 1.328703.\n",
      "Iteration 1668: Policy loss: -0.002149. Value loss: 0.220203. Entropy: 1.332777.\n",
      "episode: 778   score: 65.0  epsilon: 1.0    steps: 256  evaluation reward: 152.45\n",
      "episode: 779   score: 180.0  epsilon: 1.0    steps: 328  evaluation reward: 151.95\n",
      "episode: 780   score: 50.0  epsilon: 1.0    steps: 400  evaluation reward: 151.7\n",
      "episode: 781   score: 135.0  epsilon: 1.0    steps: 504  evaluation reward: 152.45\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1669: Policy loss: 0.001680. Value loss: 0.898665. Entropy: 1.356717.\n",
      "Iteration 1670: Policy loss: 0.000267. Value loss: 0.349769. Entropy: 1.358078.\n",
      "Iteration 1671: Policy loss: -0.001229. Value loss: 0.209705. Entropy: 1.357789.\n",
      "episode: 782   score: 235.0  epsilon: 1.0    steps: 56  evaluation reward: 154.3\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1672: Policy loss: -0.002764. Value loss: 0.663153. Entropy: 1.334822.\n",
      "Iteration 1673: Policy loss: -0.004071. Value loss: 0.334387. Entropy: 1.332085.\n",
      "Iteration 1674: Policy loss: -0.005548. Value loss: 0.165779. Entropy: 1.331928.\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1675: Policy loss: 0.002609. Value loss: 0.854790. Entropy: 1.316625.\n",
      "Iteration 1676: Policy loss: 0.001342. Value loss: 0.311933. Entropy: 1.319551.\n",
      "Iteration 1677: Policy loss: -0.001566. Value loss: 0.210046. Entropy: 1.319454.\n",
      "episode: 783   score: 20.0  epsilon: 1.0    steps: 80  evaluation reward: 153.55\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1678: Policy loss: 0.001055. Value loss: 0.951576. Entropy: 1.333111.\n",
      "Iteration 1679: Policy loss: -0.000177. Value loss: 0.403199. Entropy: 1.338568.\n",
      "Iteration 1680: Policy loss: -0.006575. Value loss: 0.253326. Entropy: 1.335179.\n",
      "episode: 784   score: 210.0  epsilon: 1.0    steps: 504  evaluation reward: 154.2\n",
      "episode: 785   score: 180.0  epsilon: 1.0    steps: 856  evaluation reward: 152.45\n",
      "episode: 786   score: 155.0  epsilon: 1.0    steps: 888  evaluation reward: 153.5\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1681: Policy loss: 0.000602. Value loss: 0.712207. Entropy: 1.359790.\n",
      "Iteration 1682: Policy loss: -0.000487. Value loss: 0.227199. Entropy: 1.358620.\n",
      "Iteration 1683: Policy loss: -0.000859. Value loss: 0.153015. Entropy: 1.359462.\n",
      "episode: 787   score: 105.0  epsilon: 1.0    steps: 168  evaluation reward: 154.0\n",
      "episode: 788   score: 180.0  epsilon: 1.0    steps: 232  evaluation reward: 154.9\n",
      "episode: 789   score: 35.0  epsilon: 1.0    steps: 344  evaluation reward: 154.6\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1684: Policy loss: -0.003724. Value loss: 0.763820. Entropy: 1.340733.\n",
      "Iteration 1685: Policy loss: -0.004658. Value loss: 0.439659. Entropy: 1.339060.\n",
      "Iteration 1686: Policy loss: -0.011624. Value loss: 0.369399. Entropy: 1.337775.\n",
      "episode: 790   score: 30.0  epsilon: 1.0    steps: 24  evaluation reward: 153.4\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1687: Policy loss: -0.000943. Value loss: 0.637379. Entropy: 1.336345.\n",
      "Iteration 1688: Policy loss: -0.000995. Value loss: 0.256362. Entropy: 1.328071.\n",
      "Iteration 1689: Policy loss: -0.005778. Value loss: 0.179815. Entropy: 1.327871.\n",
      "episode: 791   score: 80.0  epsilon: 1.0    steps: 712  evaluation reward: 153.7\n",
      "episode: 792   score: 210.0  epsilon: 1.0    steps: 768  evaluation reward: 155.25\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1690: Policy loss: -0.001835. Value loss: 0.977880. Entropy: 1.320319.\n",
      "Iteration 1691: Policy loss: 0.000827. Value loss: 0.339505. Entropy: 1.325565.\n",
      "Iteration 1692: Policy loss: -0.006035. Value loss: 0.184365. Entropy: 1.312152.\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1693: Policy loss: 0.002941. Value loss: 0.506103. Entropy: 1.348764.\n",
      "Iteration 1694: Policy loss: -0.001421. Value loss: 0.250588. Entropy: 1.342631.\n",
      "Iteration 1695: Policy loss: -0.005212. Value loss: 0.175057. Entropy: 1.335235.\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1696: Policy loss: 0.004477. Value loss: 0.710943. Entropy: 1.351646.\n",
      "Iteration 1697: Policy loss: 0.000254. Value loss: 0.238984. Entropy: 1.346343.\n",
      "Iteration 1698: Policy loss: -0.007619. Value loss: 0.176215. Entropy: 1.348429.\n",
      "episode: 793   score: 210.0  epsilon: 1.0    steps: 64  evaluation reward: 152.6\n",
      "episode: 794   score: 120.0  epsilon: 1.0    steps: 128  evaluation reward: 153.0\n",
      "episode: 795   score: 50.0  epsilon: 1.0    steps: 872  evaluation reward: 152.95\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1699: Policy loss: 0.000523. Value loss: 0.727185. Entropy: 1.302547.\n",
      "Iteration 1700: Policy loss: -0.007911. Value loss: 0.309123. Entropy: 1.294865.\n",
      "Iteration 1701: Policy loss: -0.009735. Value loss: 0.169344. Entropy: 1.296961.\n",
      "episode: 796   score: 180.0  epsilon: 1.0    steps: 40  evaluation reward: 154.7\n",
      "episode: 797   score: 220.0  epsilon: 1.0    steps: 792  evaluation reward: 155.85\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1702: Policy loss: 0.002288. Value loss: 1.036917. Entropy: 1.294456.\n",
      "Iteration 1703: Policy loss: -0.002670. Value loss: 0.486509. Entropy: 1.276767.\n",
      "Iteration 1704: Policy loss: -0.008753. Value loss: 0.298369. Entropy: 1.297500.\n",
      "episode: 798   score: 165.0  epsilon: 1.0    steps: 136  evaluation reward: 156.9\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1705: Policy loss: -0.000817. Value loss: 0.758833. Entropy: 1.300851.\n",
      "Iteration 1706: Policy loss: -0.003414. Value loss: 0.311074. Entropy: 1.305266.\n",
      "Iteration 1707: Policy loss: -0.006936. Value loss: 0.175702. Entropy: 1.313921.\n",
      "episode: 799   score: 100.0  epsilon: 1.0    steps: 176  evaluation reward: 156.4\n",
      "episode: 800   score: 55.0  epsilon: 1.0    steps: 232  evaluation reward: 156.35\n",
      "now time :  2019-02-28 11:15:06.265231\n",
      "episode: 801   score: 280.0  epsilon: 1.0    steps: 312  evaluation reward: 157.65\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1708: Policy loss: 0.000422. Value loss: 0.863425. Entropy: 1.291101.\n",
      "Iteration 1709: Policy loss: -0.000067. Value loss: 0.383964. Entropy: 1.262313.\n",
      "Iteration 1710: Policy loss: -0.004056. Value loss: 0.240958. Entropy: 1.275262.\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1711: Policy loss: -0.000189. Value loss: 0.966644. Entropy: 1.317537.\n",
      "Iteration 1712: Policy loss: -0.004053. Value loss: 0.344020. Entropy: 1.307868.\n",
      "Iteration 1713: Policy loss: -0.005713. Value loss: 0.201403. Entropy: 1.316677.\n",
      "episode: 802   score: 50.0  epsilon: 1.0    steps: 976  evaluation reward: 154.4\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1714: Policy loss: 0.002737. Value loss: 0.668315. Entropy: 1.300784.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1715: Policy loss: -0.002507. Value loss: 0.234068. Entropy: 1.302372.\n",
      "Iteration 1716: Policy loss: -0.006419. Value loss: 0.158776. Entropy: 1.298220.\n",
      "episode: 803   score: 80.0  epsilon: 1.0    steps: 96  evaluation reward: 154.15\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1717: Policy loss: -0.001366. Value loss: 0.850625. Entropy: 1.303010.\n",
      "Iteration 1718: Policy loss: 0.001285. Value loss: 0.322301. Entropy: 1.295494.\n",
      "Iteration 1719: Policy loss: -0.002303. Value loss: 0.193806. Entropy: 1.294569.\n",
      "episode: 804   score: 170.0  epsilon: 1.0    steps: 1024  evaluation reward: 151.75\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1720: Policy loss: 0.000653. Value loss: 0.919566. Entropy: 1.319631.\n",
      "Iteration 1721: Policy loss: -0.000355. Value loss: 0.355458. Entropy: 1.320275.\n",
      "Iteration 1722: Policy loss: -0.008320. Value loss: 0.251089. Entropy: 1.318902.\n",
      "episode: 805   score: 85.0  epsilon: 1.0    steps: 288  evaluation reward: 151.7\n",
      "episode: 806   score: 175.0  epsilon: 1.0    steps: 776  evaluation reward: 153.35\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1723: Policy loss: -0.001486. Value loss: 0.691787. Entropy: 1.250081.\n",
      "Iteration 1724: Policy loss: -0.001162. Value loss: 0.272664. Entropy: 1.255167.\n",
      "Iteration 1725: Policy loss: -0.009070. Value loss: 0.175225. Entropy: 1.256113.\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1726: Policy loss: 0.004005. Value loss: 0.782418. Entropy: 1.257544.\n",
      "Iteration 1727: Policy loss: 0.004930. Value loss: 0.282605. Entropy: 1.252159.\n",
      "Iteration 1728: Policy loss: -0.003252. Value loss: 0.197005. Entropy: 1.252048.\n",
      "episode: 807   score: 430.0  epsilon: 1.0    steps: 496  evaluation reward: 156.4\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1729: Policy loss: -0.000971. Value loss: 0.833617. Entropy: 1.275758.\n",
      "Iteration 1730: Policy loss: -0.003617. Value loss: 0.487913. Entropy: 1.285134.\n",
      "Iteration 1731: Policy loss: -0.004980. Value loss: 0.451790. Entropy: 1.292426.\n",
      "episode: 808   score: 160.0  epsilon: 1.0    steps: 112  evaluation reward: 155.75\n",
      "episode: 809   score: 275.0  epsilon: 1.0    steps: 712  evaluation reward: 154.3\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1732: Policy loss: 0.001042. Value loss: 0.515443. Entropy: 1.245939.\n",
      "Iteration 1733: Policy loss: -0.001781. Value loss: 0.308186. Entropy: 1.249095.\n",
      "Iteration 1734: Policy loss: -0.010819. Value loss: 0.190285. Entropy: 1.252339.\n",
      "episode: 810   score: 265.0  epsilon: 1.0    steps: 280  evaluation reward: 155.65\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1735: Policy loss: -0.001696. Value loss: 0.802795. Entropy: 1.263388.\n",
      "Iteration 1736: Policy loss: -0.004233. Value loss: 0.319427. Entropy: 1.288455.\n",
      "Iteration 1737: Policy loss: -0.009038. Value loss: 0.182018. Entropy: 1.272500.\n",
      "episode: 811   score: 185.0  epsilon: 1.0    steps: 288  evaluation reward: 156.15\n",
      "episode: 812   score: 150.0  epsilon: 1.0    steps: 488  evaluation reward: 157.15\n",
      "episode: 813   score: 100.0  epsilon: 1.0    steps: 912  evaluation reward: 156.35\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1738: Policy loss: 0.006290. Value loss: 0.791783. Entropy: 1.279044.\n",
      "Iteration 1739: Policy loss: 0.005100. Value loss: 0.342179. Entropy: 1.295994.\n",
      "Iteration 1740: Policy loss: 0.000493. Value loss: 0.214956. Entropy: 1.297611.\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1741: Policy loss: 0.001867. Value loss: 0.980828. Entropy: 1.267546.\n",
      "Iteration 1742: Policy loss: -0.001945. Value loss: 0.499795. Entropy: 1.273237.\n",
      "Iteration 1743: Policy loss: -0.005472. Value loss: 0.286127. Entropy: 1.284649.\n",
      "episode: 814   score: 275.0  epsilon: 1.0    steps: 544  evaluation reward: 157.55\n",
      "episode: 815   score: 120.0  epsilon: 1.0    steps: 776  evaluation reward: 157.55\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1744: Policy loss: -0.000724. Value loss: 0.666663. Entropy: 1.324688.\n",
      "Iteration 1745: Policy loss: -0.003747. Value loss: 0.292215. Entropy: 1.323921.\n",
      "Iteration 1746: Policy loss: -0.005164. Value loss: 0.185329. Entropy: 1.317860.\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1747: Policy loss: 0.000837. Value loss: 1.026747. Entropy: 1.320957.\n",
      "Iteration 1748: Policy loss: -0.001589. Value loss: 0.457078. Entropy: 1.323597.\n",
      "Iteration 1749: Policy loss: -0.003698. Value loss: 0.285149. Entropy: 1.317167.\n",
      "episode: 816   score: 85.0  epsilon: 1.0    steps: 448  evaluation reward: 153.8\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1750: Policy loss: 0.001015. Value loss: 0.965657. Entropy: 1.348647.\n",
      "Iteration 1751: Policy loss: -0.000051. Value loss: 0.409041. Entropy: 1.347086.\n",
      "Iteration 1752: Policy loss: -0.007572. Value loss: 0.305695. Entropy: 1.350911.\n",
      "episode: 817   score: 170.0  epsilon: 1.0    steps: 272  evaluation reward: 153.6\n",
      "episode: 818   score: 305.0  epsilon: 1.0    steps: 328  evaluation reward: 156.5\n",
      "episode: 819   score: 60.0  epsilon: 1.0    steps: 472  evaluation reward: 155.7\n",
      "episode: 820   score: 200.0  epsilon: 1.0    steps: 984  evaluation reward: 156.9\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1753: Policy loss: 0.001988. Value loss: 1.083443. Entropy: 1.329677.\n",
      "Iteration 1754: Policy loss: -0.001621. Value loss: 0.330135. Entropy: 1.345611.\n",
      "Iteration 1755: Policy loss: -0.003974. Value loss: 0.226500. Entropy: 1.347104.\n",
      "episode: 821   score: 185.0  epsilon: 1.0    steps: 448  evaluation reward: 158.2\n",
      "episode: 822   score: 80.0  epsilon: 1.0    steps: 808  evaluation reward: 157.65\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1756: Policy loss: 0.000629. Value loss: 1.045339. Entropy: 1.339492.\n",
      "Iteration 1757: Policy loss: -0.003214. Value loss: 0.472131. Entropy: 1.340423.\n",
      "Iteration 1758: Policy loss: -0.007981. Value loss: 0.305387. Entropy: 1.342990.\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1759: Policy loss: 0.000195. Value loss: 1.388939. Entropy: 1.350111.\n",
      "Iteration 1760: Policy loss: 0.003464. Value loss: 0.477458. Entropy: 1.347509.\n",
      "Iteration 1761: Policy loss: 0.002931. Value loss: 0.321282. Entropy: 1.343388.\n",
      "episode: 823   score: 65.0  epsilon: 1.0    steps: 704  evaluation reward: 158.15\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1762: Policy loss: -0.001578. Value loss: 0.861016. Entropy: 1.327833.\n",
      "Iteration 1763: Policy loss: -0.000890. Value loss: 0.334187. Entropy: 1.332005.\n",
      "Iteration 1764: Policy loss: -0.003669. Value loss: 0.235980. Entropy: 1.336344.\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1765: Policy loss: -0.001223. Value loss: 0.840688. Entropy: 1.341202.\n",
      "Iteration 1766: Policy loss: -0.000872. Value loss: 0.255834. Entropy: 1.339845.\n",
      "Iteration 1767: Policy loss: -0.005082. Value loss: 0.188406. Entropy: 1.350516.\n",
      "episode: 824   score: 215.0  epsilon: 1.0    steps: 272  evaluation reward: 159.95\n",
      "episode: 825   score: 85.0  epsilon: 1.0    steps: 1024  evaluation reward: 159.0\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1768: Policy loss: -0.000385. Value loss: 1.294379. Entropy: 1.349887.\n",
      "Iteration 1769: Policy loss: -0.001504. Value loss: 0.435956. Entropy: 1.358564.\n",
      "Iteration 1770: Policy loss: -0.003081. Value loss: 0.250208. Entropy: 1.356299.\n",
      "episode: 826   score: 35.0  epsilon: 1.0    steps: 336  evaluation reward: 158.6\n",
      "episode: 827   score: 110.0  epsilon: 1.0    steps: 464  evaluation reward: 158.5\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1771: Policy loss: 0.000522. Value loss: 0.913038. Entropy: 1.350712.\n",
      "Iteration 1772: Policy loss: -0.001666. Value loss: 0.504261. Entropy: 1.339358.\n",
      "Iteration 1773: Policy loss: -0.005504. Value loss: 0.331613. Entropy: 1.337842.\n",
      "episode: 828   score: 275.0  epsilon: 1.0    steps: 784  evaluation reward: 160.0\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1774: Policy loss: -0.000066. Value loss: 0.818956. Entropy: 1.338705.\n",
      "Iteration 1775: Policy loss: -0.003115. Value loss: 0.298147. Entropy: 1.337217.\n",
      "Iteration 1776: Policy loss: -0.004743. Value loss: 0.172013. Entropy: 1.335898.\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1777: Policy loss: -0.000638. Value loss: 0.669984. Entropy: 1.347685.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1778: Policy loss: 0.001480. Value loss: 0.284956. Entropy: 1.338204.\n",
      "Iteration 1779: Policy loss: -0.001647. Value loss: 0.184551. Entropy: 1.327842.\n",
      "episode: 829   score: 260.0  epsilon: 1.0    steps: 472  evaluation reward: 161.25\n",
      "episode: 830   score: 425.0  epsilon: 1.0    steps: 592  evaluation reward: 165.15\n",
      "episode: 831   score: 30.0  epsilon: 1.0    steps: 904  evaluation reward: 163.3\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1780: Policy loss: 0.001810. Value loss: 0.634982. Entropy: 1.338495.\n",
      "Iteration 1781: Policy loss: -0.001966. Value loss: 0.224945. Entropy: 1.337461.\n",
      "Iteration 1782: Policy loss: -0.005415. Value loss: 0.148659. Entropy: 1.335186.\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1783: Policy loss: 0.000821. Value loss: 0.965967. Entropy: 1.327319.\n",
      "Iteration 1784: Policy loss: -0.001085. Value loss: 0.391883. Entropy: 1.326980.\n",
      "Iteration 1785: Policy loss: -0.005657. Value loss: 0.247772. Entropy: 1.323154.\n",
      "episode: 832   score: 140.0  epsilon: 1.0    steps: 712  evaluation reward: 163.65\n",
      "episode: 833   score: 200.0  epsilon: 1.0    steps: 824  evaluation reward: 164.1\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1786: Policy loss: 0.002593. Value loss: 1.078092. Entropy: 1.342674.\n",
      "Iteration 1787: Policy loss: 0.000047. Value loss: 0.383977. Entropy: 1.347990.\n",
      "Iteration 1788: Policy loss: -0.003832. Value loss: 0.275088. Entropy: 1.343593.\n",
      "episode: 834   score: 245.0  epsilon: 1.0    steps: 704  evaluation reward: 162.7\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1789: Policy loss: 0.002151. Value loss: 0.586425. Entropy: 1.319287.\n",
      "Iteration 1790: Policy loss: -0.001274. Value loss: 0.247831. Entropy: 1.322807.\n",
      "Iteration 1791: Policy loss: -0.001477. Value loss: 0.181351. Entropy: 1.326392.\n",
      "episode: 835   score: 60.0  epsilon: 1.0    steps: 584  evaluation reward: 159.65\n",
      "episode: 836   score: 485.0  epsilon: 1.0    steps: 976  evaluation reward: 164.2\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1792: Policy loss: 0.004210. Value loss: 0.867568. Entropy: 1.341635.\n",
      "Iteration 1793: Policy loss: 0.001701. Value loss: 0.312599. Entropy: 1.336539.\n",
      "Iteration 1794: Policy loss: -0.003643. Value loss: 0.280404. Entropy: 1.337230.\n",
      "episode: 837   score: 20.0  epsilon: 1.0    steps: 544  evaluation reward: 163.15\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1795: Policy loss: 0.000972. Value loss: 1.357857. Entropy: 1.328532.\n",
      "Iteration 1796: Policy loss: 0.000014. Value loss: 0.566136. Entropy: 1.318665.\n",
      "Iteration 1797: Policy loss: 0.004877. Value loss: 0.420833. Entropy: 1.325427.\n",
      "episode: 838   score: 70.0  epsilon: 1.0    steps: 376  evaluation reward: 162.75\n",
      "episode: 839   score: 320.0  epsilon: 1.0    steps: 696  evaluation reward: 164.35\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1798: Policy loss: 0.000528. Value loss: 0.626666. Entropy: 1.335237.\n",
      "Iteration 1799: Policy loss: -0.001263. Value loss: 0.292847. Entropy: 1.336371.\n",
      "Iteration 1800: Policy loss: -0.002389. Value loss: 0.242504. Entropy: 1.330776.\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1801: Policy loss: 0.003979. Value loss: 0.877001. Entropy: 1.346489.\n",
      "Iteration 1802: Policy loss: -0.000819. Value loss: 0.444873. Entropy: 1.350724.\n",
      "Iteration 1803: Policy loss: -0.004860. Value loss: 0.330133. Entropy: 1.350847.\n",
      "episode: 840   score: 75.0  epsilon: 1.0    steps: 320  evaluation reward: 164.45\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1804: Policy loss: 0.000394. Value loss: 1.167411. Entropy: 1.337639.\n",
      "Iteration 1805: Policy loss: -0.002215. Value loss: 0.386287. Entropy: 1.342324.\n",
      "Iteration 1806: Policy loss: -0.006565. Value loss: 0.244186. Entropy: 1.338330.\n",
      "episode: 841   score: 100.0  epsilon: 1.0    steps: 128  evaluation reward: 163.05\n",
      "episode: 842   score: 330.0  epsilon: 1.0    steps: 432  evaluation reward: 165.0\n",
      "episode: 843   score: 65.0  epsilon: 1.0    steps: 592  evaluation reward: 162.5\n",
      "episode: 844   score: 110.0  epsilon: 1.0    steps: 592  evaluation reward: 162.55\n",
      "episode: 845   score: 50.0  epsilon: 1.0    steps: 608  evaluation reward: 160.95\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1807: Policy loss: 0.000169. Value loss: 1.227817. Entropy: 1.335522.\n",
      "Iteration 1808: Policy loss: -0.000532. Value loss: 0.789119. Entropy: 1.331002.\n",
      "Iteration 1809: Policy loss: -0.007214. Value loss: 0.451146. Entropy: 1.331190.\n",
      "episode: 846   score: 250.0  epsilon: 1.0    steps: 528  evaluation reward: 162.2\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1810: Policy loss: 0.001722. Value loss: 1.013704. Entropy: 1.329264.\n",
      "Iteration 1811: Policy loss: -0.000404. Value loss: 0.388749. Entropy: 1.327531.\n",
      "Iteration 1812: Policy loss: -0.007627. Value loss: 0.233897. Entropy: 1.331148.\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1813: Policy loss: -0.001208. Value loss: 0.706988. Entropy: 1.325927.\n",
      "Iteration 1814: Policy loss: -0.000076. Value loss: 0.377345. Entropy: 1.329093.\n",
      "Iteration 1815: Policy loss: -0.005040. Value loss: 0.224915. Entropy: 1.330453.\n",
      "episode: 847   score: 415.0  epsilon: 1.0    steps: 64  evaluation reward: 164.25\n",
      "episode: 848   score: 20.0  epsilon: 1.0    steps: 272  evaluation reward: 162.9\n",
      "episode: 849   score: 65.0  epsilon: 1.0    steps: 424  evaluation reward: 162.35\n",
      "episode: 850   score: 100.0  epsilon: 1.0    steps: 768  evaluation reward: 160.95\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1816: Policy loss: 0.002120. Value loss: 0.774975. Entropy: 1.318531.\n",
      "Iteration 1817: Policy loss: 0.000659. Value loss: 0.465113. Entropy: 1.314530.\n",
      "Iteration 1818: Policy loss: -0.004468. Value loss: 0.302356. Entropy: 1.319647.\n",
      "now time :  2019-02-28 11:16:27.332083\n",
      "episode: 851   score: 55.0  epsilon: 1.0    steps: 408  evaluation reward: 159.95\n",
      "episode: 852   score: 180.0  epsilon: 1.0    steps: 816  evaluation reward: 160.55\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1819: Policy loss: 0.001424. Value loss: 0.703221. Entropy: 1.330657.\n",
      "Iteration 1820: Policy loss: -0.002856. Value loss: 0.394725. Entropy: 1.339879.\n",
      "Iteration 1821: Policy loss: -0.005064. Value loss: 0.309059. Entropy: 1.334457.\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1822: Policy loss: -0.001322. Value loss: 0.984804. Entropy: 1.343067.\n",
      "Iteration 1823: Policy loss: -0.000471. Value loss: 0.478932. Entropy: 1.343428.\n",
      "Iteration 1824: Policy loss: -0.003043. Value loss: 0.305784. Entropy: 1.346240.\n",
      "episode: 853   score: 180.0  epsilon: 1.0    steps: 168  evaluation reward: 160.25\n",
      "episode: 854   score: 35.0  epsilon: 1.0    steps: 168  evaluation reward: 159.5\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1825: Policy loss: 0.000775. Value loss: 0.818555. Entropy: 1.304656.\n",
      "Iteration 1826: Policy loss: -0.000825. Value loss: 0.389937. Entropy: 1.295351.\n",
      "Iteration 1827: Policy loss: -0.009645. Value loss: 0.311020. Entropy: 1.300317.\n",
      "episode: 855   score: 185.0  epsilon: 1.0    steps: 960  evaluation reward: 160.0\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1828: Policy loss: -0.000008. Value loss: 0.956288. Entropy: 1.322557.\n",
      "Iteration 1829: Policy loss: -0.007725. Value loss: 0.362903. Entropy: 1.307680.\n",
      "Iteration 1830: Policy loss: -0.008821. Value loss: 0.235891. Entropy: 1.311574.\n",
      "episode: 856   score: 295.0  epsilon: 1.0    steps: 712  evaluation reward: 161.7\n",
      "episode: 857   score: 110.0  epsilon: 1.0    steps: 808  evaluation reward: 158.4\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1831: Policy loss: 0.000783. Value loss: 0.667686. Entropy: 1.321041.\n",
      "Iteration 1832: Policy loss: -0.001242. Value loss: 0.297122. Entropy: 1.315309.\n",
      "Iteration 1833: Policy loss: -0.007362. Value loss: 0.180041. Entropy: 1.322829.\n",
      "episode: 858   score: 110.0  epsilon: 1.0    steps: 296  evaluation reward: 155.9\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1834: Policy loss: -0.001870. Value loss: 0.745873. Entropy: 1.319824.\n",
      "Iteration 1835: Policy loss: -0.005361. Value loss: 0.320933. Entropy: 1.313316.\n",
      "Iteration 1836: Policy loss: -0.011259. Value loss: 0.234468. Entropy: 1.316094.\n",
      "episode: 859   score: 105.0  epsilon: 1.0    steps: 256  evaluation reward: 156.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1837: Policy loss: -0.001806. Value loss: 0.854738. Entropy: 1.320468.\n",
      "Iteration 1838: Policy loss: -0.006289. Value loss: 0.439933. Entropy: 1.316164.\n",
      "Iteration 1839: Policy loss: -0.005270. Value loss: 0.258902. Entropy: 1.316316.\n",
      "episode: 860   score: 145.0  epsilon: 1.0    steps: 288  evaluation reward: 156.3\n",
      "episode: 861   score: 485.0  epsilon: 1.0    steps: 808  evaluation reward: 160.05\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1840: Policy loss: 0.000900. Value loss: 0.615209. Entropy: 1.314622.\n",
      "Iteration 1841: Policy loss: -0.004730. Value loss: 0.242442. Entropy: 1.313397.\n",
      "Iteration 1842: Policy loss: -0.003452. Value loss: 0.216473. Entropy: 1.319900.\n",
      "episode: 862   score: 35.0  epsilon: 1.0    steps: 280  evaluation reward: 159.3\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1843: Policy loss: 0.000255. Value loss: 0.693370. Entropy: 1.310022.\n",
      "Iteration 1844: Policy loss: -0.005561. Value loss: 0.316507. Entropy: 1.315984.\n",
      "Iteration 1845: Policy loss: -0.009467. Value loss: 0.238128. Entropy: 1.315738.\n",
      "episode: 863   score: 400.0  epsilon: 1.0    steps: 24  evaluation reward: 160.7\n",
      "episode: 864   score: 110.0  epsilon: 1.0    steps: 424  evaluation reward: 158.0\n",
      "episode: 865   score: 65.0  epsilon: 1.0    steps: 624  evaluation reward: 157.9\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1846: Policy loss: 0.005143. Value loss: 0.666492. Entropy: 1.302544.\n",
      "Iteration 1847: Policy loss: -0.000107. Value loss: 0.235824. Entropy: 1.299463.\n",
      "Iteration 1848: Policy loss: -0.009026. Value loss: 0.163866. Entropy: 1.298731.\n",
      "episode: 866   score: 30.0  epsilon: 1.0    steps: 80  evaluation reward: 155.45\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1849: Policy loss: 0.001484. Value loss: 0.942911. Entropy: 1.301487.\n",
      "Iteration 1850: Policy loss: 0.003001. Value loss: 0.410761. Entropy: 1.298938.\n",
      "Iteration 1851: Policy loss: -0.005344. Value loss: 0.271719. Entropy: 1.300134.\n",
      "episode: 867   score: 50.0  epsilon: 1.0    steps: 936  evaluation reward: 153.85\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1852: Policy loss: 0.000349. Value loss: 0.726455. Entropy: 1.315634.\n",
      "Iteration 1853: Policy loss: -0.003852. Value loss: 0.299051. Entropy: 1.315912.\n",
      "Iteration 1854: Policy loss: -0.011756. Value loss: 0.224680. Entropy: 1.311812.\n",
      "episode: 868   score: 155.0  epsilon: 1.0    steps: 280  evaluation reward: 154.05\n",
      "episode: 869   score: 45.0  epsilon: 1.0    steps: 464  evaluation reward: 153.95\n",
      "episode: 870   score: 185.0  epsilon: 1.0    steps: 832  evaluation reward: 154.7\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1855: Policy loss: -0.000728. Value loss: 1.183757. Entropy: 1.308211.\n",
      "Iteration 1856: Policy loss: -0.005275. Value loss: 0.402891. Entropy: 1.314821.\n",
      "Iteration 1857: Policy loss: -0.007925. Value loss: 0.278843. Entropy: 1.304855.\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1858: Policy loss: 0.002971. Value loss: 0.951703. Entropy: 1.327340.\n",
      "Iteration 1859: Policy loss: -0.004481. Value loss: 0.436229. Entropy: 1.331743.\n",
      "Iteration 1860: Policy loss: -0.009362. Value loss: 0.242492. Entropy: 1.330703.\n",
      "episode: 871   score: 135.0  epsilon: 1.0    steps: 320  evaluation reward: 154.95\n",
      "episode: 872   score: 155.0  epsilon: 1.0    steps: 784  evaluation reward: 156.1\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1861: Policy loss: 0.001839. Value loss: 0.852928. Entropy: 1.313884.\n",
      "Iteration 1862: Policy loss: -0.001980. Value loss: 0.323009. Entropy: 1.307019.\n",
      "Iteration 1863: Policy loss: -0.008610. Value loss: 0.248534. Entropy: 1.307121.\n",
      "episode: 873   score: 195.0  epsilon: 1.0    steps: 64  evaluation reward: 156.55\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1864: Policy loss: -0.002599. Value loss: 0.730365. Entropy: 1.291772.\n",
      "Iteration 1865: Policy loss: -0.000380. Value loss: 0.296043. Entropy: 1.300403.\n",
      "Iteration 1866: Policy loss: -0.007510. Value loss: 0.198417. Entropy: 1.291220.\n",
      "episode: 874   score: 90.0  epsilon: 1.0    steps: 936  evaluation reward: 155.9\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1867: Policy loss: -0.002440. Value loss: 1.022026. Entropy: 1.297219.\n",
      "Iteration 1868: Policy loss: -0.004437. Value loss: 0.421206. Entropy: 1.294322.\n",
      "Iteration 1869: Policy loss: -0.006119. Value loss: 0.319914. Entropy: 1.297826.\n",
      "episode: 875   score: 170.0  epsilon: 1.0    steps: 480  evaluation reward: 155.8\n",
      "episode: 876   score: 375.0  epsilon: 1.0    steps: 992  evaluation reward: 157.7\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1870: Policy loss: 0.001595. Value loss: 1.036514. Entropy: 1.279349.\n",
      "Iteration 1871: Policy loss: -0.000676. Value loss: 0.530483. Entropy: 1.284095.\n",
      "Iteration 1872: Policy loss: -0.008332. Value loss: 0.469058. Entropy: 1.284946.\n",
      "episode: 877   score: 500.0  epsilon: 1.0    steps: 64  evaluation reward: 160.55\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1873: Policy loss: 0.002388. Value loss: 0.866040. Entropy: 1.291367.\n",
      "Iteration 1874: Policy loss: -0.000601. Value loss: 0.398635. Entropy: 1.285625.\n",
      "Iteration 1875: Policy loss: -0.007145. Value loss: 0.282483. Entropy: 1.282217.\n",
      "episode: 878   score: 90.0  epsilon: 1.0    steps: 528  evaluation reward: 160.8\n",
      "episode: 879   score: 155.0  epsilon: 1.0    steps: 656  evaluation reward: 160.55\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1876: Policy loss: 0.004407. Value loss: 0.781298. Entropy: 1.289763.\n",
      "Iteration 1877: Policy loss: 0.001912. Value loss: 0.447584. Entropy: 1.277526.\n",
      "Iteration 1878: Policy loss: -0.003266. Value loss: 0.429020. Entropy: 1.278572.\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1879: Policy loss: 0.002235. Value loss: 1.044301. Entropy: 1.286036.\n",
      "Iteration 1880: Policy loss: 0.000526. Value loss: 0.458745. Entropy: 1.299114.\n",
      "Iteration 1881: Policy loss: -0.005878. Value loss: 0.243851. Entropy: 1.293773.\n",
      "episode: 880   score: 490.0  epsilon: 1.0    steps: 664  evaluation reward: 164.95\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1882: Policy loss: -0.001119. Value loss: 0.692731. Entropy: 1.314922.\n",
      "Iteration 1883: Policy loss: -0.002084. Value loss: 0.299395. Entropy: 1.322042.\n",
      "Iteration 1884: Policy loss: -0.007381. Value loss: 0.240172. Entropy: 1.314423.\n",
      "episode: 881   score: 110.0  epsilon: 1.0    steps: 1024  evaluation reward: 164.7\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1885: Policy loss: 0.002064. Value loss: 0.877700. Entropy: 1.329218.\n",
      "Iteration 1886: Policy loss: -0.002694. Value loss: 0.334435. Entropy: 1.324629.\n",
      "Iteration 1887: Policy loss: -0.007608. Value loss: 0.227756. Entropy: 1.329740.\n",
      "episode: 882   score: 175.0  epsilon: 1.0    steps: 248  evaluation reward: 164.1\n",
      "episode: 883   score: 175.0  epsilon: 1.0    steps: 864  evaluation reward: 165.65\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1888: Policy loss: 0.001538. Value loss: 1.162128. Entropy: 1.322295.\n",
      "Iteration 1889: Policy loss: -0.003192. Value loss: 0.590642. Entropy: 1.320096.\n",
      "Iteration 1890: Policy loss: -0.007886. Value loss: 0.359644. Entropy: 1.325041.\n",
      "episode: 884   score: 205.0  epsilon: 1.0    steps: 400  evaluation reward: 165.6\n",
      "episode: 885   score: 165.0  epsilon: 1.0    steps: 608  evaluation reward: 165.45\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1891: Policy loss: 0.000070. Value loss: 0.560823. Entropy: 1.326080.\n",
      "Iteration 1892: Policy loss: -0.002586. Value loss: 0.185682. Entropy: 1.322225.\n",
      "Iteration 1893: Policy loss: -0.006684. Value loss: 0.155067. Entropy: 1.321857.\n",
      "episode: 886   score: 275.0  epsilon: 1.0    steps: 256  evaluation reward: 166.65\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1894: Policy loss: 0.001587. Value loss: 0.818390. Entropy: 1.329899.\n",
      "Iteration 1895: Policy loss: -0.003951. Value loss: 0.356913. Entropy: 1.332991.\n",
      "Iteration 1896: Policy loss: -0.010773. Value loss: 0.239785. Entropy: 1.326554.\n",
      "episode: 887   score: 50.0  epsilon: 1.0    steps: 264  evaluation reward: 166.1\n",
      "episode: 888   score: 945.0  epsilon: 1.0    steps: 672  evaluation reward: 173.75\n",
      "Training network. lr: 0.000236. clip: 0.094323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1897: Policy loss: 0.004667. Value loss: 0.824210. Entropy: 1.313796.\n",
      "Iteration 1898: Policy loss: -0.000555. Value loss: 0.401758. Entropy: 1.321953.\n",
      "Iteration 1899: Policy loss: -0.001906. Value loss: 0.304641. Entropy: 1.318436.\n",
      "episode: 889   score: 160.0  epsilon: 1.0    steps: 176  evaluation reward: 175.0\n",
      "episode: 890   score: 50.0  epsilon: 1.0    steps: 312  evaluation reward: 175.2\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1900: Policy loss: 0.001016. Value loss: 0.512544. Entropy: 1.310236.\n",
      "Iteration 1901: Policy loss: -0.001912. Value loss: 0.285804. Entropy: 1.295277.\n",
      "Iteration 1902: Policy loss: -0.007892. Value loss: 0.186546. Entropy: 1.289031.\n",
      "episode: 891   score: 125.0  epsilon: 1.0    steps: 648  evaluation reward: 175.65\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1903: Policy loss: -0.000818. Value loss: 0.828988. Entropy: 1.280023.\n",
      "Iteration 1904: Policy loss: -0.001954. Value loss: 0.350869. Entropy: 1.288988.\n",
      "Iteration 1905: Policy loss: -0.004604. Value loss: 0.253426. Entropy: 1.291960.\n",
      "episode: 892   score: 215.0  epsilon: 1.0    steps: 168  evaluation reward: 175.7\n",
      "episode: 893   score: 210.0  epsilon: 1.0    steps: 960  evaluation reward: 175.7\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1906: Policy loss: 0.002354. Value loss: 0.442500. Entropy: 1.288713.\n",
      "Iteration 1907: Policy loss: -0.003081. Value loss: 0.216979. Entropy: 1.281464.\n",
      "Iteration 1908: Policy loss: -0.009703. Value loss: 0.159113. Entropy: 1.282330.\n",
      "episode: 894   score: 200.0  epsilon: 1.0    steps: 384  evaluation reward: 176.5\n",
      "episode: 895   score: 155.0  epsilon: 1.0    steps: 928  evaluation reward: 177.55\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1909: Policy loss: 0.000575. Value loss: 0.530875. Entropy: 1.295643.\n",
      "Iteration 1910: Policy loss: -0.002583. Value loss: 0.239236. Entropy: 1.292906.\n",
      "Iteration 1911: Policy loss: -0.005151. Value loss: 0.164324. Entropy: 1.287615.\n",
      "episode: 896   score: 80.0  epsilon: 1.0    steps: 416  evaluation reward: 176.55\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1912: Policy loss: 0.000704. Value loss: 0.788834. Entropy: 1.297279.\n",
      "Iteration 1913: Policy loss: -0.002998. Value loss: 0.261222. Entropy: 1.294074.\n",
      "Iteration 1914: Policy loss: -0.007283. Value loss: 0.172980. Entropy: 1.302774.\n",
      "episode: 897   score: 215.0  epsilon: 1.0    steps: 328  evaluation reward: 176.5\n",
      "episode: 898   score: 175.0  epsilon: 1.0    steps: 712  evaluation reward: 176.6\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1915: Policy loss: 0.000852. Value loss: 0.901775. Entropy: 1.316582.\n",
      "Iteration 1916: Policy loss: -0.003454. Value loss: 0.512574. Entropy: 1.322465.\n",
      "Iteration 1917: Policy loss: -0.008436. Value loss: 0.331728. Entropy: 1.319124.\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1918: Policy loss: 0.001903. Value loss: 0.901349. Entropy: 1.281208.\n",
      "Iteration 1919: Policy loss: 0.000793. Value loss: 0.373549. Entropy: 1.297118.\n",
      "Iteration 1920: Policy loss: -0.004686. Value loss: 0.273917. Entropy: 1.282119.\n",
      "episode: 899   score: 125.0  epsilon: 1.0    steps: 64  evaluation reward: 176.85\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1921: Policy loss: -0.002529. Value loss: 0.759044. Entropy: 1.320460.\n",
      "Iteration 1922: Policy loss: -0.004490. Value loss: 0.259544. Entropy: 1.321539.\n",
      "Iteration 1923: Policy loss: -0.009704. Value loss: 0.182459. Entropy: 1.321222.\n",
      "episode: 900   score: 180.0  epsilon: 1.0    steps: 120  evaluation reward: 178.1\n",
      "now time :  2019-02-28 11:17:43.681055\n",
      "episode: 901   score: 30.0  epsilon: 1.0    steps: 472  evaluation reward: 175.6\n",
      "episode: 902   score: 135.0  epsilon: 1.0    steps: 568  evaluation reward: 176.45\n",
      "episode: 903   score: 70.0  epsilon: 1.0    steps: 752  evaluation reward: 176.35\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1924: Policy loss: -0.001498. Value loss: 0.586392. Entropy: 1.337245.\n",
      "Iteration 1925: Policy loss: -0.006828. Value loss: 0.165456. Entropy: 1.337054.\n",
      "Iteration 1926: Policy loss: -0.011041. Value loss: 0.108169. Entropy: 1.332252.\n",
      "episode: 904   score: 180.0  epsilon: 1.0    steps: 800  evaluation reward: 176.45\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1927: Policy loss: 0.002484. Value loss: 1.191211. Entropy: 1.338964.\n",
      "Iteration 1928: Policy loss: 0.000619. Value loss: 0.487367. Entropy: 1.330113.\n",
      "Iteration 1929: Policy loss: -0.004911. Value loss: 0.297071. Entropy: 1.334679.\n",
      "episode: 905   score: 120.0  epsilon: 1.0    steps: 616  evaluation reward: 176.8\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1930: Policy loss: -0.000371. Value loss: 0.455841. Entropy: 1.315691.\n",
      "Iteration 1931: Policy loss: -0.001965. Value loss: 0.194767. Entropy: 1.326789.\n",
      "Iteration 1932: Policy loss: -0.004975. Value loss: 0.173837. Entropy: 1.325599.\n",
      "episode: 906   score: 50.0  epsilon: 1.0    steps: 456  evaluation reward: 175.55\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1933: Policy loss: 0.001862. Value loss: 0.662757. Entropy: 1.325822.\n",
      "Iteration 1934: Policy loss: -0.002387. Value loss: 0.262223. Entropy: 1.317901.\n",
      "Iteration 1935: Policy loss: -0.007952. Value loss: 0.191866. Entropy: 1.317731.\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1936: Policy loss: -0.001148. Value loss: 0.655357. Entropy: 1.342986.\n",
      "Iteration 1937: Policy loss: -0.006859. Value loss: 0.255499. Entropy: 1.341464.\n",
      "Iteration 1938: Policy loss: -0.011202. Value loss: 0.172610. Entropy: 1.343306.\n",
      "episode: 907   score: 580.0  epsilon: 1.0    steps: 632  evaluation reward: 177.05\n",
      "episode: 908   score: 125.0  epsilon: 1.0    steps: 1016  evaluation reward: 176.7\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1939: Policy loss: 0.003101. Value loss: 0.958265. Entropy: 1.332648.\n",
      "Iteration 1940: Policy loss: -0.004383. Value loss: 0.343885. Entropy: 1.333889.\n",
      "Iteration 1941: Policy loss: -0.006435. Value loss: 0.202559. Entropy: 1.329833.\n",
      "episode: 909   score: 110.0  epsilon: 1.0    steps: 56  evaluation reward: 175.05\n",
      "episode: 910   score: 165.0  epsilon: 1.0    steps: 400  evaluation reward: 174.05\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1942: Policy loss: 0.002512. Value loss: 0.810782. Entropy: 1.320043.\n",
      "Iteration 1943: Policy loss: 0.000578. Value loss: 0.300397. Entropy: 1.317822.\n",
      "Iteration 1944: Policy loss: -0.006444. Value loss: 0.189806. Entropy: 1.319907.\n",
      "episode: 911   score: 190.0  epsilon: 1.0    steps: 240  evaluation reward: 174.1\n",
      "episode: 912   score: 250.0  epsilon: 1.0    steps: 504  evaluation reward: 175.1\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1945: Policy loss: 0.002216. Value loss: 0.611696. Entropy: 1.308733.\n",
      "Iteration 1946: Policy loss: -0.000195. Value loss: 0.297727. Entropy: 1.307788.\n",
      "Iteration 1947: Policy loss: -0.006145. Value loss: 0.197607. Entropy: 1.313273.\n",
      "episode: 913   score: 170.0  epsilon: 1.0    steps: 944  evaluation reward: 175.8\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1948: Policy loss: 0.002853. Value loss: 0.813268. Entropy: 1.293814.\n",
      "Iteration 1949: Policy loss: -0.000516. Value loss: 0.289969. Entropy: 1.304165.\n",
      "Iteration 1950: Policy loss: -0.004641. Value loss: 0.202868. Entropy: 1.302854.\n",
      "episode: 914   score: 140.0  epsilon: 1.0    steps: 288  evaluation reward: 174.45\n",
      "episode: 915   score: 125.0  epsilon: 1.0    steps: 352  evaluation reward: 174.5\n",
      "episode: 916   score: 355.0  epsilon: 1.0    steps: 472  evaluation reward: 177.2\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1951: Policy loss: 0.000640. Value loss: 0.586231. Entropy: 1.327643.\n",
      "Iteration 1952: Policy loss: -0.003333. Value loss: 0.228960. Entropy: 1.323990.\n",
      "Iteration 1953: Policy loss: -0.004774. Value loss: 0.169269. Entropy: 1.326279.\n",
      "episode: 917   score: 75.0  epsilon: 1.0    steps: 168  evaluation reward: 176.25\n",
      "episode: 918   score: 180.0  epsilon: 1.0    steps: 584  evaluation reward: 175.0\n",
      "episode: 919   score: 55.0  epsilon: 1.0    steps: 712  evaluation reward: 174.95\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1954: Policy loss: 0.000734. Value loss: 0.594576. Entropy: 1.316003.\n",
      "Iteration 1955: Policy loss: 0.001642. Value loss: 0.281367. Entropy: 1.310272.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1956: Policy loss: -0.005234. Value loss: 0.189724. Entropy: 1.310729.\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1957: Policy loss: -0.001584. Value loss: 0.462215. Entropy: 1.310633.\n",
      "Iteration 1958: Policy loss: -0.002091. Value loss: 0.247389. Entropy: 1.304306.\n",
      "Iteration 1959: Policy loss: -0.012918. Value loss: 0.168764. Entropy: 1.309525.\n",
      "episode: 920   score: 130.0  epsilon: 1.0    steps: 360  evaluation reward: 174.25\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1960: Policy loss: 0.000752. Value loss: 0.651202. Entropy: 1.284698.\n",
      "Iteration 1961: Policy loss: -0.001591. Value loss: 0.364336. Entropy: 1.280014.\n",
      "Iteration 1962: Policy loss: -0.004456. Value loss: 0.253100. Entropy: 1.285594.\n",
      "episode: 921   score: 55.0  epsilon: 1.0    steps: 120  evaluation reward: 172.95\n",
      "episode: 922   score: 105.0  epsilon: 1.0    steps: 800  evaluation reward: 173.2\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1963: Policy loss: -0.001868. Value loss: 0.882620. Entropy: 1.323298.\n",
      "Iteration 1964: Policy loss: -0.000964. Value loss: 0.361049. Entropy: 1.316741.\n",
      "Iteration 1965: Policy loss: -0.005877. Value loss: 0.245437. Entropy: 1.316253.\n",
      "episode: 923   score: 100.0  epsilon: 1.0    steps: 264  evaluation reward: 173.55\n",
      "episode: 924   score: 105.0  epsilon: 1.0    steps: 464  evaluation reward: 172.45\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1966: Policy loss: 0.001866. Value loss: 1.076151. Entropy: 1.323510.\n",
      "Iteration 1967: Policy loss: -0.001683. Value loss: 0.383103. Entropy: 1.326139.\n",
      "Iteration 1968: Policy loss: -0.004840. Value loss: 0.202483. Entropy: 1.325596.\n",
      "episode: 925   score: 115.0  epsilon: 1.0    steps: 64  evaluation reward: 172.75\n",
      "episode: 926   score: 15.0  epsilon: 1.0    steps: 464  evaluation reward: 172.55\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1969: Policy loss: -0.004183. Value loss: 0.580794. Entropy: 1.305161.\n",
      "Iteration 1970: Policy loss: -0.006154. Value loss: 0.270198. Entropy: 1.306210.\n",
      "Iteration 1971: Policy loss: -0.010611. Value loss: 0.179126. Entropy: 1.307036.\n",
      "episode: 927   score: 130.0  epsilon: 1.0    steps: 400  evaluation reward: 172.75\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1972: Policy loss: 0.000262. Value loss: 0.813006. Entropy: 1.293179.\n",
      "Iteration 1973: Policy loss: -0.000698. Value loss: 0.353559. Entropy: 1.304350.\n",
      "Iteration 1974: Policy loss: -0.003837. Value loss: 0.313924. Entropy: 1.305007.\n",
      "episode: 928   score: 115.0  epsilon: 1.0    steps: 528  evaluation reward: 171.15\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1975: Policy loss: 0.003123. Value loss: 0.804675. Entropy: 1.306629.\n",
      "Iteration 1976: Policy loss: -0.001218. Value loss: 0.311745. Entropy: 1.314191.\n",
      "Iteration 1977: Policy loss: -0.004369. Value loss: 0.230938. Entropy: 1.316155.\n",
      "episode: 929   score: 105.0  epsilon: 1.0    steps: 168  evaluation reward: 169.6\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1978: Policy loss: -0.001032. Value loss: 0.625358. Entropy: 1.332110.\n",
      "Iteration 1979: Policy loss: -0.001922. Value loss: 0.225146. Entropy: 1.331774.\n",
      "Iteration 1980: Policy loss: -0.000825. Value loss: 0.138303. Entropy: 1.328517.\n",
      "episode: 930   score: 445.0  epsilon: 1.0    steps: 64  evaluation reward: 169.8\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1981: Policy loss: 0.003534. Value loss: 0.496025. Entropy: 1.332521.\n",
      "Iteration 1982: Policy loss: -0.005873. Value loss: 0.233943. Entropy: 1.336753.\n",
      "Iteration 1983: Policy loss: -0.010950. Value loss: 0.171039. Entropy: 1.335471.\n",
      "episode: 931   score: 110.0  epsilon: 1.0    steps: 264  evaluation reward: 170.6\n",
      "episode: 932   score: 150.0  epsilon: 1.0    steps: 288  evaluation reward: 170.7\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1984: Policy loss: 0.001380. Value loss: 0.926493. Entropy: 1.331360.\n",
      "Iteration 1985: Policy loss: -0.001662. Value loss: 0.446829. Entropy: 1.326272.\n",
      "Iteration 1986: Policy loss: -0.004353. Value loss: 0.233828. Entropy: 1.324906.\n",
      "episode: 933   score: 495.0  epsilon: 1.0    steps: 472  evaluation reward: 173.65\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1987: Policy loss: 0.001121. Value loss: 0.690143. Entropy: 1.286033.\n",
      "Iteration 1988: Policy loss: -0.003969. Value loss: 0.333424. Entropy: 1.284885.\n",
      "Iteration 1989: Policy loss: -0.007063. Value loss: 0.190638. Entropy: 1.285634.\n",
      "episode: 934   score: 215.0  epsilon: 1.0    steps: 184  evaluation reward: 173.35\n",
      "episode: 935   score: 235.0  epsilon: 1.0    steps: 392  evaluation reward: 175.1\n",
      "episode: 936   score: 115.0  epsilon: 1.0    steps: 1008  evaluation reward: 171.4\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1990: Policy loss: -0.001442. Value loss: 0.658450. Entropy: 1.276716.\n",
      "Iteration 1991: Policy loss: -0.003870. Value loss: 0.368872. Entropy: 1.279642.\n",
      "Iteration 1992: Policy loss: -0.009279. Value loss: 0.232376. Entropy: 1.279087.\n",
      "episode: 937   score: 155.0  epsilon: 1.0    steps: 312  evaluation reward: 172.75\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1993: Policy loss: -0.001570. Value loss: 1.020718. Entropy: 1.297562.\n",
      "Iteration 1994: Policy loss: -0.000627. Value loss: 0.483925. Entropy: 1.293189.\n",
      "Iteration 1995: Policy loss: -0.005030. Value loss: 0.270785. Entropy: 1.286556.\n",
      "episode: 938   score: 155.0  epsilon: 1.0    steps: 32  evaluation reward: 173.6\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1996: Policy loss: 0.001560. Value loss: 0.587029. Entropy: 1.273898.\n",
      "Iteration 1997: Policy loss: -0.001393. Value loss: 0.282987. Entropy: 1.278178.\n",
      "Iteration 1998: Policy loss: -0.006225. Value loss: 0.193413. Entropy: 1.274981.\n",
      "episode: 939   score: 235.0  epsilon: 1.0    steps: 72  evaluation reward: 172.75\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1999: Policy loss: 0.001852. Value loss: 0.618075. Entropy: 1.281750.\n",
      "Iteration 2000: Policy loss: -0.002766. Value loss: 0.235165. Entropy: 1.270357.\n",
      "Iteration 2001: Policy loss: -0.003706. Value loss: 0.147323. Entropy: 1.279233.\n",
      "episode: 940   score: 120.0  epsilon: 1.0    steps: 344  evaluation reward: 173.2\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2002: Policy loss: -0.001880. Value loss: 0.715914. Entropy: 1.299218.\n",
      "Iteration 2003: Policy loss: -0.006104. Value loss: 0.317770. Entropy: 1.315172.\n",
      "Iteration 2004: Policy loss: -0.006755. Value loss: 0.239711. Entropy: 1.307759.\n",
      "episode: 941   score: 150.0  epsilon: 1.0    steps: 544  evaluation reward: 173.7\n",
      "episode: 942   score: 120.0  epsilon: 1.0    steps: 880  evaluation reward: 171.6\n",
      "episode: 943   score: 510.0  epsilon: 1.0    steps: 984  evaluation reward: 176.05\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2005: Policy loss: -0.000568. Value loss: 0.735404. Entropy: 1.323884.\n",
      "Iteration 2006: Policy loss: -0.002833. Value loss: 0.308724. Entropy: 1.321342.\n",
      "Iteration 2007: Policy loss: -0.005976. Value loss: 0.166126. Entropy: 1.320485.\n",
      "episode: 944   score: 110.0  epsilon: 1.0    steps: 136  evaluation reward: 176.05\n",
      "episode: 945   score: 355.0  epsilon: 1.0    steps: 680  evaluation reward: 179.1\n",
      "episode: 946   score: 80.0  epsilon: 1.0    steps: 856  evaluation reward: 177.4\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2008: Policy loss: -0.000144. Value loss: 0.679430. Entropy: 1.302668.\n",
      "Iteration 2009: Policy loss: -0.000408. Value loss: 0.256994. Entropy: 1.304657.\n",
      "Iteration 2010: Policy loss: -0.005115. Value loss: 0.194061. Entropy: 1.299817.\n",
      "episode: 947   score: 100.0  epsilon: 1.0    steps: 632  evaluation reward: 174.25\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2011: Policy loss: 0.002837. Value loss: 1.098287. Entropy: 1.246982.\n",
      "Iteration 2012: Policy loss: -0.000124. Value loss: 0.520995. Entropy: 1.245070.\n",
      "Iteration 2013: Policy loss: -0.000694. Value loss: 0.384822. Entropy: 1.257796.\n",
      "episode: 948   score: 65.0  epsilon: 1.0    steps: 608  evaluation reward: 174.7\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2014: Policy loss: 0.000979. Value loss: 0.569863. Entropy: 1.305761.\n",
      "Iteration 2015: Policy loss: -0.005238. Value loss: 0.245172. Entropy: 1.307600.\n",
      "Iteration 2016: Policy loss: -0.006678. Value loss: 0.205035. Entropy: 1.307299.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 949   score: 210.0  epsilon: 1.0    steps: 336  evaluation reward: 176.15\n",
      "episode: 950   score: 100.0  epsilon: 1.0    steps: 952  evaluation reward: 176.15\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2017: Policy loss: 0.002718. Value loss: 0.452572. Entropy: 1.289137.\n",
      "Iteration 2018: Policy loss: -0.006222. Value loss: 0.241229. Entropy: 1.289296.\n",
      "Iteration 2019: Policy loss: -0.001783. Value loss: 0.151413. Entropy: 1.285908.\n",
      "now time :  2019-02-28 11:18:53.792028\n",
      "episode: 951   score: 110.0  epsilon: 1.0    steps: 744  evaluation reward: 176.7\n",
      "episode: 952   score: 100.0  epsilon: 1.0    steps: 776  evaluation reward: 175.9\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2020: Policy loss: 0.001319. Value loss: 0.542458. Entropy: 1.263084.\n",
      "Iteration 2021: Policy loss: -0.003041. Value loss: 0.241717. Entropy: 1.253908.\n",
      "Iteration 2022: Policy loss: -0.007907. Value loss: 0.125287. Entropy: 1.258021.\n",
      "episode: 953   score: 155.0  epsilon: 1.0    steps: 720  evaluation reward: 175.65\n",
      "episode: 954   score: 155.0  epsilon: 1.0    steps: 880  evaluation reward: 176.85\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2023: Policy loss: 0.003392. Value loss: 0.649904. Entropy: 1.237681.\n",
      "Iteration 2024: Policy loss: 0.000261. Value loss: 0.273655. Entropy: 1.245331.\n",
      "Iteration 2025: Policy loss: -0.004115. Value loss: 0.163904. Entropy: 1.251433.\n",
      "episode: 955   score: 110.0  epsilon: 1.0    steps: 616  evaluation reward: 176.1\n",
      "episode: 956   score: 55.0  epsilon: 1.0    steps: 984  evaluation reward: 173.7\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2026: Policy loss: 0.001316. Value loss: 0.401556. Entropy: 1.277695.\n",
      "Iteration 2027: Policy loss: -0.005735. Value loss: 0.153405. Entropy: 1.280648.\n",
      "Iteration 2028: Policy loss: -0.008102. Value loss: 0.111517. Entropy: 1.276353.\n",
      "episode: 957   score: 180.0  epsilon: 1.0    steps: 736  evaluation reward: 174.4\n",
      "episode: 958   score: 30.0  epsilon: 1.0    steps: 760  evaluation reward: 173.6\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2029: Policy loss: 0.001584. Value loss: 0.692153. Entropy: 1.284726.\n",
      "Iteration 2030: Policy loss: -0.002752. Value loss: 0.241602. Entropy: 1.288595.\n",
      "Iteration 2031: Policy loss: -0.006906. Value loss: 0.149852. Entropy: 1.302657.\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2032: Policy loss: 0.001299. Value loss: 0.804199. Entropy: 1.239188.\n",
      "Iteration 2033: Policy loss: -0.001016. Value loss: 0.284177. Entropy: 1.236996.\n",
      "Iteration 2034: Policy loss: -0.000146. Value loss: 0.144777. Entropy: 1.242953.\n",
      "episode: 959   score: 35.0  epsilon: 1.0    steps: 640  evaluation reward: 172.9\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2035: Policy loss: -0.000978. Value loss: 0.568232. Entropy: 1.291788.\n",
      "Iteration 2036: Policy loss: -0.002386. Value loss: 0.274009. Entropy: 1.298169.\n",
      "Iteration 2037: Policy loss: -0.005617. Value loss: 0.162106. Entropy: 1.300998.\n",
      "episode: 960   score: 200.0  epsilon: 1.0    steps: 360  evaluation reward: 173.45\n",
      "episode: 961   score: 50.0  epsilon: 1.0    steps: 792  evaluation reward: 169.1\n",
      "episode: 962   score: 210.0  epsilon: 1.0    steps: 888  evaluation reward: 170.85\n",
      "episode: 963   score: 70.0  epsilon: 1.0    steps: 960  evaluation reward: 167.55\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2038: Policy loss: 0.004139. Value loss: 0.508456. Entropy: 1.325666.\n",
      "Iteration 2039: Policy loss: -0.001485. Value loss: 0.156035. Entropy: 1.326723.\n",
      "Iteration 2040: Policy loss: -0.004224. Value loss: 0.106278. Entropy: 1.333131.\n",
      "episode: 964   score: 155.0  epsilon: 1.0    steps: 888  evaluation reward: 168.0\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2041: Policy loss: 0.002810. Value loss: 0.504134. Entropy: 1.269863.\n",
      "Iteration 2042: Policy loss: -0.000941. Value loss: 0.256316. Entropy: 1.280115.\n",
      "Iteration 2043: Policy loss: -0.005300. Value loss: 0.195485. Entropy: 1.273600.\n",
      "episode: 965   score: 155.0  epsilon: 1.0    steps: 128  evaluation reward: 168.9\n",
      "episode: 966   score: 120.0  epsilon: 1.0    steps: 712  evaluation reward: 169.8\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2044: Policy loss: -0.000587. Value loss: 0.778832. Entropy: 1.259606.\n",
      "Iteration 2045: Policy loss: -0.004502. Value loss: 0.309330. Entropy: 1.264188.\n",
      "Iteration 2046: Policy loss: -0.004757. Value loss: 0.206801. Entropy: 1.250859.\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2047: Policy loss: 0.001171. Value loss: 0.503655. Entropy: 1.297598.\n",
      "Iteration 2048: Policy loss: -0.005807. Value loss: 0.203215. Entropy: 1.291473.\n",
      "Iteration 2049: Policy loss: -0.004750. Value loss: 0.172606. Entropy: 1.299262.\n",
      "episode: 967   score: 105.0  epsilon: 1.0    steps: 984  evaluation reward: 170.35\n",
      "episode: 968   score: 50.0  epsilon: 1.0    steps: 992  evaluation reward: 169.3\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2050: Policy loss: 0.001792. Value loss: 0.755277. Entropy: 1.326009.\n",
      "Iteration 2051: Policy loss: -0.002163. Value loss: 0.385972. Entropy: 1.325332.\n",
      "Iteration 2052: Policy loss: -0.001916. Value loss: 0.291408. Entropy: 1.328773.\n",
      "episode: 969   score: 210.0  epsilon: 1.0    steps: 640  evaluation reward: 170.95\n",
      "episode: 970   score: 155.0  epsilon: 1.0    steps: 936  evaluation reward: 170.65\n",
      "episode: 971   score: 210.0  epsilon: 1.0    steps: 1024  evaluation reward: 171.4\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2053: Policy loss: 0.003125. Value loss: 0.648974. Entropy: 1.337156.\n",
      "Iteration 2054: Policy loss: 0.003736. Value loss: 0.292615. Entropy: 1.335011.\n",
      "Iteration 2055: Policy loss: -0.002578. Value loss: 0.204205. Entropy: 1.339560.\n",
      "episode: 972   score: 240.0  epsilon: 1.0    steps: 664  evaluation reward: 172.25\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2056: Policy loss: 0.002370. Value loss: 0.939778. Entropy: 1.297659.\n",
      "Iteration 2057: Policy loss: -0.002835. Value loss: 0.421892. Entropy: 1.295022.\n",
      "Iteration 2058: Policy loss: -0.008004. Value loss: 0.291712. Entropy: 1.298729.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2059: Policy loss: -0.001387. Value loss: 0.716478. Entropy: 1.325250.\n",
      "Iteration 2060: Policy loss: -0.002033. Value loss: 0.332866. Entropy: 1.325401.\n",
      "Iteration 2061: Policy loss: -0.011186. Value loss: 0.222610. Entropy: 1.326244.\n",
      "episode: 973   score: 110.0  epsilon: 1.0    steps: 72  evaluation reward: 171.4\n",
      "episode: 974   score: 175.0  epsilon: 1.0    steps: 456  evaluation reward: 172.25\n",
      "episode: 975   score: 30.0  epsilon: 1.0    steps: 648  evaluation reward: 170.85\n",
      "episode: 976   score: 50.0  epsilon: 1.0    steps: 856  evaluation reward: 167.6\n",
      "episode: 977   score: 50.0  epsilon: 1.0    steps: 960  evaluation reward: 163.1\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2062: Policy loss: -0.000935. Value loss: 0.470915. Entropy: 1.342776.\n",
      "Iteration 2063: Policy loss: -0.000151. Value loss: 0.264233. Entropy: 1.337759.\n",
      "Iteration 2064: Policy loss: -0.004100. Value loss: 0.238144. Entropy: 1.340622.\n",
      "episode: 978   score: 105.0  epsilon: 1.0    steps: 784  evaluation reward: 163.25\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2065: Policy loss: -0.003908. Value loss: 0.395213. Entropy: 1.325404.\n",
      "Iteration 2066: Policy loss: -0.003139. Value loss: 0.161677. Entropy: 1.330055.\n",
      "Iteration 2067: Policy loss: -0.006582. Value loss: 0.134179. Entropy: 1.338145.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2068: Policy loss: -0.000124. Value loss: 0.510076. Entropy: 1.328958.\n",
      "Iteration 2069: Policy loss: 0.002348. Value loss: 0.270361. Entropy: 1.332636.\n",
      "Iteration 2070: Policy loss: -0.005160. Value loss: 0.215467. Entropy: 1.326750.\n",
      "episode: 979   score: 30.0  epsilon: 1.0    steps: 592  evaluation reward: 162.0\n",
      "episode: 980   score: 75.0  epsilon: 1.0    steps: 800  evaluation reward: 157.85\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2071: Policy loss: 0.002497. Value loss: 0.876935. Entropy: 1.329272.\n",
      "Iteration 2072: Policy loss: 0.001842. Value loss: 0.348301. Entropy: 1.326891.\n",
      "Iteration 2073: Policy loss: -0.002552. Value loss: 0.202503. Entropy: 1.327333.\n",
      "episode: 981   score: 180.0  epsilon: 1.0    steps: 56  evaluation reward: 158.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2074: Policy loss: -0.000560. Value loss: 0.583572. Entropy: 1.306960.\n",
      "Iteration 2075: Policy loss: -0.011164. Value loss: 0.206945. Entropy: 1.300143.\n",
      "Iteration 2076: Policy loss: -0.005572. Value loss: 0.189575. Entropy: 1.311672.\n",
      "episode: 982   score: 225.0  epsilon: 1.0    steps: 264  evaluation reward: 159.05\n",
      "episode: 983   score: 110.0  epsilon: 1.0    steps: 496  evaluation reward: 158.4\n",
      "episode: 984   score: 75.0  epsilon: 1.0    steps: 648  evaluation reward: 157.1\n",
      "episode: 985   score: 50.0  epsilon: 1.0    steps: 976  evaluation reward: 155.95\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2077: Policy loss: 0.003525. Value loss: 0.484385. Entropy: 1.321927.\n",
      "Iteration 2078: Policy loss: 0.001715. Value loss: 0.281212. Entropy: 1.320262.\n",
      "Iteration 2079: Policy loss: -0.005887. Value loss: 0.178483. Entropy: 1.320707.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2080: Policy loss: 0.001698. Value loss: 0.374478. Entropy: 1.329200.\n",
      "Iteration 2081: Policy loss: 0.004245. Value loss: 0.174833. Entropy: 1.324689.\n",
      "Iteration 2082: Policy loss: -0.005020. Value loss: 0.127489. Entropy: 1.328660.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2083: Policy loss: -0.001316. Value loss: 0.558387. Entropy: 1.317250.\n",
      "Iteration 2084: Policy loss: -0.004734. Value loss: 0.262144. Entropy: 1.320917.\n",
      "Iteration 2085: Policy loss: -0.008136. Value loss: 0.177822. Entropy: 1.321850.\n",
      "episode: 986   score: 155.0  epsilon: 1.0    steps: 72  evaluation reward: 154.75\n",
      "episode: 987   score: 80.0  epsilon: 1.0    steps: 832  evaluation reward: 155.05\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2086: Policy loss: 0.003085. Value loss: 0.668388. Entropy: 1.329386.\n",
      "Iteration 2087: Policy loss: -0.004490. Value loss: 0.284443. Entropy: 1.329148.\n",
      "Iteration 2088: Policy loss: -0.004162. Value loss: 0.187920. Entropy: 1.332719.\n",
      "episode: 988   score: 135.0  epsilon: 1.0    steps: 344  evaluation reward: 146.95\n",
      "episode: 989   score: 155.0  epsilon: 1.0    steps: 496  evaluation reward: 146.9\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2089: Policy loss: -0.000729. Value loss: 0.417599. Entropy: 1.338883.\n",
      "Iteration 2090: Policy loss: -0.001348. Value loss: 0.157305. Entropy: 1.334826.\n",
      "Iteration 2091: Policy loss: -0.009408. Value loss: 0.099032. Entropy: 1.336859.\n",
      "episode: 990   score: 80.0  epsilon: 1.0    steps: 680  evaluation reward: 147.2\n",
      "episode: 991   score: 110.0  epsilon: 1.0    steps: 848  evaluation reward: 147.05\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2092: Policy loss: 0.000176. Value loss: 0.517576. Entropy: 1.331384.\n",
      "Iteration 2093: Policy loss: -0.000093. Value loss: 0.218994. Entropy: 1.326741.\n",
      "Iteration 2094: Policy loss: -0.006698. Value loss: 0.141616. Entropy: 1.331802.\n",
      "episode: 992   score: 75.0  epsilon: 1.0    steps: 848  evaluation reward: 145.65\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2095: Policy loss: 0.001569. Value loss: 0.681387. Entropy: 1.331001.\n",
      "Iteration 2096: Policy loss: -0.003688. Value loss: 0.320811. Entropy: 1.334849.\n",
      "Iteration 2097: Policy loss: -0.006730. Value loss: 0.230809. Entropy: 1.331219.\n",
      "episode: 993   score: 105.0  epsilon: 1.0    steps: 8  evaluation reward: 144.6\n",
      "episode: 994   score: 125.0  epsilon: 1.0    steps: 96  evaluation reward: 143.85\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2098: Policy loss: -0.000385. Value loss: 0.591205. Entropy: 1.311363.\n",
      "Iteration 2099: Policy loss: 0.000361. Value loss: 0.291902. Entropy: 1.315028.\n",
      "Iteration 2100: Policy loss: -0.004294. Value loss: 0.139513. Entropy: 1.311879.\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2101: Policy loss: 0.002761. Value loss: 0.568974. Entropy: 1.314550.\n",
      "Iteration 2102: Policy loss: -0.000292. Value loss: 0.239889. Entropy: 1.312960.\n",
      "Iteration 2103: Policy loss: -0.006500. Value loss: 0.176947. Entropy: 1.312811.\n",
      "episode: 995   score: 65.0  epsilon: 1.0    steps: 520  evaluation reward: 142.95\n",
      "episode: 996   score: 230.0  epsilon: 1.0    steps: 600  evaluation reward: 144.45\n",
      "episode: 997   score: 135.0  epsilon: 1.0    steps: 960  evaluation reward: 143.65\n",
      "episode: 998   score: 30.0  epsilon: 1.0    steps: 984  evaluation reward: 142.2\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2104: Policy loss: -0.001032. Value loss: 0.684103. Entropy: 1.355470.\n",
      "Iteration 2105: Policy loss: -0.003383. Value loss: 0.223227. Entropy: 1.356053.\n",
      "Iteration 2106: Policy loss: -0.004457. Value loss: 0.164371. Entropy: 1.354984.\n",
      "episode: 999   score: 125.0  epsilon: 1.0    steps: 904  evaluation reward: 142.2\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2107: Policy loss: 0.001370. Value loss: 0.640244. Entropy: 1.335224.\n",
      "Iteration 2108: Policy loss: 0.000172. Value loss: 0.255145. Entropy: 1.331717.\n",
      "Iteration 2109: Policy loss: -0.003414. Value loss: 0.155405. Entropy: 1.335565.\n",
      "episode: 1000   score: 215.0  epsilon: 1.0    steps: 776  evaluation reward: 142.55\n",
      "now time :  2019-02-28 11:19:59.330486\n",
      "episode: 1001   score: 150.0  epsilon: 1.0    steps: 776  evaluation reward: 143.75\n",
      "episode: 1002   score: 120.0  epsilon: 1.0    steps: 824  evaluation reward: 143.6\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2110: Policy loss: -0.000034. Value loss: 0.637502. Entropy: 1.318717.\n",
      "Iteration 2111: Policy loss: -0.000670. Value loss: 0.187798. Entropy: 1.316382.\n",
      "Iteration 2112: Policy loss: -0.009355. Value loss: 0.127318. Entropy: 1.320667.\n",
      "episode: 1003   score: 110.0  epsilon: 1.0    steps: 688  evaluation reward: 144.0\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2113: Policy loss: 0.002052. Value loss: 0.432306. Entropy: 1.310800.\n",
      "Iteration 2114: Policy loss: -0.003163. Value loss: 0.238064. Entropy: 1.312994.\n",
      "Iteration 2115: Policy loss: -0.008920. Value loss: 0.158558. Entropy: 1.315893.\n",
      "episode: 1004   score: 120.0  epsilon: 1.0    steps: 848  evaluation reward: 143.4\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2116: Policy loss: 0.000996. Value loss: 0.474087. Entropy: 1.312916.\n",
      "Iteration 2117: Policy loss: -0.008312. Value loss: 0.227878. Entropy: 1.305738.\n",
      "Iteration 2118: Policy loss: -0.012225. Value loss: 0.199057. Entropy: 1.304435.\n",
      "episode: 1005   score: 110.0  epsilon: 1.0    steps: 880  evaluation reward: 143.3\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2119: Policy loss: -0.002654. Value loss: 0.295147. Entropy: 1.334986.\n",
      "Iteration 2120: Policy loss: -0.006591. Value loss: 0.157585. Entropy: 1.329808.\n",
      "Iteration 2121: Policy loss: -0.010888. Value loss: 0.101449. Entropy: 1.328666.\n",
      "episode: 1006   score: 60.0  epsilon: 1.0    steps: 328  evaluation reward: 143.4\n",
      "episode: 1007   score: 160.0  epsilon: 1.0    steps: 664  evaluation reward: 139.2\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2122: Policy loss: 0.001375. Value loss: 0.460175. Entropy: 1.319155.\n",
      "Iteration 2123: Policy loss: -0.003125. Value loss: 0.210055. Entropy: 1.318984.\n",
      "Iteration 2124: Policy loss: -0.001877. Value loss: 0.141685. Entropy: 1.313553.\n",
      "episode: 1008   score: 180.0  epsilon: 1.0    steps: 512  evaluation reward: 139.75\n",
      "episode: 1009   score: 50.0  epsilon: 1.0    steps: 520  evaluation reward: 139.15\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2125: Policy loss: -0.002102. Value loss: 0.491038. Entropy: 1.287842.\n",
      "Iteration 2126: Policy loss: -0.003482. Value loss: 0.250643. Entropy: 1.276316.\n",
      "Iteration 2127: Policy loss: -0.007318. Value loss: 0.160905. Entropy: 1.280568.\n",
      "episode: 1010   score: 160.0  epsilon: 1.0    steps: 832  evaluation reward: 139.1\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2128: Policy loss: -0.002190. Value loss: 0.633083. Entropy: 1.284824.\n",
      "Iteration 2129: Policy loss: 0.000349. Value loss: 0.243076. Entropy: 1.279915.\n",
      "Iteration 2130: Policy loss: -0.006393. Value loss: 0.193527. Entropy: 1.276539.\n",
      "episode: 1011   score: 210.0  epsilon: 1.0    steps: 368  evaluation reward: 139.3\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2131: Policy loss: 0.002721. Value loss: 0.575801. Entropy: 1.319543.\n",
      "Iteration 2132: Policy loss: -0.001672. Value loss: 0.282553. Entropy: 1.320880.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2133: Policy loss: -0.005778. Value loss: 0.219109. Entropy: 1.318961.\n",
      "episode: 1012   score: 180.0  epsilon: 1.0    steps: 864  evaluation reward: 138.6\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2134: Policy loss: 0.002448. Value loss: 0.403256. Entropy: 1.282125.\n",
      "Iteration 2135: Policy loss: -0.001496. Value loss: 0.183495. Entropy: 1.279609.\n",
      "Iteration 2136: Policy loss: -0.009051. Value loss: 0.149384. Entropy: 1.282229.\n",
      "episode: 1013   score: 135.0  epsilon: 1.0    steps: 464  evaluation reward: 138.25\n",
      "episode: 1014   score: 195.0  epsilon: 1.0    steps: 576  evaluation reward: 138.8\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2137: Policy loss: 0.001314. Value loss: 0.442820. Entropy: 1.302869.\n",
      "Iteration 2138: Policy loss: -0.004115. Value loss: 0.195727. Entropy: 1.302696.\n",
      "Iteration 2139: Policy loss: -0.009280. Value loss: 0.152301. Entropy: 1.304235.\n",
      "episode: 1015   score: 135.0  epsilon: 1.0    steps: 8  evaluation reward: 138.9\n",
      "episode: 1016   score: 210.0  epsilon: 1.0    steps: 312  evaluation reward: 137.45\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2140: Policy loss: -0.000252. Value loss: 0.521444. Entropy: 1.212199.\n",
      "Iteration 2141: Policy loss: -0.004888. Value loss: 0.243209. Entropy: 1.215897.\n",
      "Iteration 2142: Policy loss: -0.009320. Value loss: 0.175942. Entropy: 1.219463.\n",
      "episode: 1017   score: 210.0  epsilon: 1.0    steps: 808  evaluation reward: 138.8\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2143: Policy loss: 0.001802. Value loss: 0.913285. Entropy: 1.256996.\n",
      "Iteration 2144: Policy loss: -0.000395. Value loss: 0.335213. Entropy: 1.262893.\n",
      "Iteration 2145: Policy loss: -0.006452. Value loss: 0.240697. Entropy: 1.262025.\n",
      "episode: 1018   score: 100.0  epsilon: 1.0    steps: 56  evaluation reward: 138.0\n",
      "episode: 1019   score: 480.0  epsilon: 1.0    steps: 688  evaluation reward: 142.25\n",
      "episode: 1020   score: 190.0  epsilon: 1.0    steps: 976  evaluation reward: 142.85\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2146: Policy loss: 0.009104. Value loss: 0.651981. Entropy: 1.289688.\n",
      "Iteration 2147: Policy loss: 0.000102. Value loss: 0.201168. Entropy: 1.299439.\n",
      "Iteration 2148: Policy loss: -0.001478. Value loss: 0.182678. Entropy: 1.306644.\n",
      "episode: 1021   score: 100.0  epsilon: 1.0    steps: 688  evaluation reward: 143.3\n",
      "episode: 1022   score: 110.0  epsilon: 1.0    steps: 1024  evaluation reward: 143.35\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2149: Policy loss: 0.000522. Value loss: 0.576962. Entropy: 1.253904.\n",
      "Iteration 2150: Policy loss: -0.000940. Value loss: 0.306433. Entropy: 1.264258.\n",
      "Iteration 2151: Policy loss: -0.012783. Value loss: 0.197063. Entropy: 1.259200.\n",
      "episode: 1023   score: 170.0  epsilon: 1.0    steps: 696  evaluation reward: 144.05\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2152: Policy loss: 0.001660. Value loss: 0.591835. Entropy: 1.253308.\n",
      "Iteration 2153: Policy loss: -0.002464. Value loss: 0.227202. Entropy: 1.256273.\n",
      "Iteration 2154: Policy loss: -0.002132. Value loss: 0.157438. Entropy: 1.255137.\n",
      "episode: 1024   score: 200.0  epsilon: 1.0    steps: 320  evaluation reward: 145.0\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2155: Policy loss: 0.002326. Value loss: 1.068952. Entropy: 1.291793.\n",
      "Iteration 2156: Policy loss: -0.001886. Value loss: 0.343591. Entropy: 1.282797.\n",
      "Iteration 2157: Policy loss: -0.005518. Value loss: 0.195271. Entropy: 1.279951.\n",
      "episode: 1025   score: 95.0  epsilon: 1.0    steps: 88  evaluation reward: 144.8\n",
      "episode: 1026   score: 180.0  epsilon: 1.0    steps: 392  evaluation reward: 146.45\n",
      "episode: 1027   score: 180.0  epsilon: 1.0    steps: 592  evaluation reward: 146.95\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2158: Policy loss: 0.000269. Value loss: 0.806150. Entropy: 1.288472.\n",
      "Iteration 2159: Policy loss: -0.006149. Value loss: 0.508144. Entropy: 1.280877.\n",
      "Iteration 2160: Policy loss: -0.009559. Value loss: 0.332903. Entropy: 1.279657.\n",
      "episode: 1028   score: 50.0  epsilon: 1.0    steps: 808  evaluation reward: 146.3\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2161: Policy loss: 0.002692. Value loss: 1.245453. Entropy: 1.199979.\n",
      "Iteration 2162: Policy loss: -0.005286. Value loss: 0.524926. Entropy: 1.193959.\n",
      "Iteration 2163: Policy loss: -0.003496. Value loss: 0.286033. Entropy: 1.202180.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2164: Policy loss: 0.001520. Value loss: 0.881727. Entropy: 1.265110.\n",
      "Iteration 2165: Policy loss: -0.003584. Value loss: 0.335855. Entropy: 1.272143.\n",
      "Iteration 2166: Policy loss: -0.009175. Value loss: 0.198663. Entropy: 1.261353.\n",
      "episode: 1029   score: 85.0  epsilon: 1.0    steps: 296  evaluation reward: 146.1\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2167: Policy loss: -0.000592. Value loss: 0.779455. Entropy: 1.280590.\n",
      "Iteration 2168: Policy loss: -0.002885. Value loss: 0.353969. Entropy: 1.274962.\n",
      "Iteration 2169: Policy loss: -0.002445. Value loss: 0.249556. Entropy: 1.282535.\n",
      "episode: 1030   score: 210.0  epsilon: 1.0    steps: 400  evaluation reward: 143.75\n",
      "episode: 1031   score: 100.0  epsilon: 1.0    steps: 592  evaluation reward: 143.65\n",
      "episode: 1032   score: 140.0  epsilon: 1.0    steps: 632  evaluation reward: 143.55\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2170: Policy loss: -0.003524. Value loss: 1.157154. Entropy: 1.256136.\n",
      "Iteration 2171: Policy loss: -0.001634. Value loss: 0.449708. Entropy: 1.260649.\n",
      "Iteration 2172: Policy loss: -0.008416. Value loss: 0.306023. Entropy: 1.264886.\n",
      "episode: 1033   score: 230.0  epsilon: 1.0    steps: 456  evaluation reward: 140.9\n",
      "episode: 1034   score: 415.0  epsilon: 1.0    steps: 472  evaluation reward: 142.9\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2173: Policy loss: -0.000244. Value loss: 0.736703. Entropy: 1.233468.\n",
      "Iteration 2174: Policy loss: -0.004173. Value loss: 0.325875. Entropy: 1.249823.\n",
      "Iteration 2175: Policy loss: -0.007967. Value loss: 0.300412. Entropy: 1.245951.\n",
      "episode: 1035   score: 250.0  epsilon: 1.0    steps: 184  evaluation reward: 143.05\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2176: Policy loss: 0.005077. Value loss: 0.797116. Entropy: 1.188240.\n",
      "Iteration 2177: Policy loss: -0.004071. Value loss: 0.323236. Entropy: 1.200463.\n",
      "Iteration 2178: Policy loss: -0.006667. Value loss: 0.180913. Entropy: 1.208927.\n",
      "episode: 1036   score: 65.0  epsilon: 1.0    steps: 224  evaluation reward: 142.55\n",
      "episode: 1037   score: 60.0  epsilon: 1.0    steps: 504  evaluation reward: 141.6\n",
      "episode: 1038   score: 395.0  epsilon: 1.0    steps: 808  evaluation reward: 144.0\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2179: Policy loss: -0.001898. Value loss: 0.722411. Entropy: 1.243653.\n",
      "Iteration 2180: Policy loss: -0.001552. Value loss: 0.290468. Entropy: 1.245673.\n",
      "Iteration 2181: Policy loss: -0.004964. Value loss: 0.154973. Entropy: 1.240980.\n",
      "episode: 1039   score: 140.0  epsilon: 1.0    steps: 552  evaluation reward: 143.05\n",
      "episode: 1040   score: 160.0  epsilon: 1.0    steps: 960  evaluation reward: 143.45\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2182: Policy loss: 0.003373. Value loss: 0.628085. Entropy: 1.224445.\n",
      "Iteration 2183: Policy loss: -0.001374. Value loss: 0.358779. Entropy: 1.220960.\n",
      "Iteration 2184: Policy loss: -0.001961. Value loss: 0.247790. Entropy: 1.223768.\n",
      "episode: 1041   score: 30.0  epsilon: 1.0    steps: 840  evaluation reward: 142.25\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2185: Policy loss: -0.002302. Value loss: 0.829660. Entropy: 1.225330.\n",
      "Iteration 2186: Policy loss: -0.003231. Value loss: 0.322962. Entropy: 1.219778.\n",
      "Iteration 2187: Policy loss: -0.010165. Value loss: 0.199387. Entropy: 1.226895.\n",
      "episode: 1042   score: 135.0  epsilon: 1.0    steps: 392  evaluation reward: 142.4\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2188: Policy loss: 0.001882. Value loss: 0.918941. Entropy: 1.255380.\n",
      "Iteration 2189: Policy loss: -0.001743. Value loss: 0.408157. Entropy: 1.254717.\n",
      "Iteration 2190: Policy loss: -0.006900. Value loss: 0.265733. Entropy: 1.251721.\n",
      "episode: 1043   score: 260.0  epsilon: 1.0    steps: 832  evaluation reward: 139.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2191: Policy loss: 0.000009. Value loss: 0.624387. Entropy: 1.268852.\n",
      "Iteration 2192: Policy loss: -0.006446. Value loss: 0.223863. Entropy: 1.260405.\n",
      "Iteration 2193: Policy loss: -0.003156. Value loss: 0.112947. Entropy: 1.267585.\n",
      "episode: 1044   score: 115.0  epsilon: 1.0    steps: 144  evaluation reward: 139.95\n",
      "episode: 1045   score: 60.0  epsilon: 1.0    steps: 392  evaluation reward: 137.0\n",
      "episode: 1046   score: 75.0  epsilon: 1.0    steps: 752  evaluation reward: 136.95\n",
      "episode: 1047   score: 235.0  epsilon: 1.0    steps: 880  evaluation reward: 138.3\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2194: Policy loss: -0.002336. Value loss: 0.745587. Entropy: 1.288085.\n",
      "Iteration 2195: Policy loss: -0.003255. Value loss: 0.267918. Entropy: 1.294272.\n",
      "Iteration 2196: Policy loss: -0.005605. Value loss: 0.196001. Entropy: 1.290133.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2197: Policy loss: -0.001842. Value loss: 0.772669. Entropy: 1.198811.\n",
      "Iteration 2198: Policy loss: -0.005357. Value loss: 0.455608. Entropy: 1.194727.\n",
      "Iteration 2199: Policy loss: -0.007702. Value loss: 0.249140. Entropy: 1.197701.\n",
      "episode: 1048   score: 135.0  epsilon: 1.0    steps: 120  evaluation reward: 139.0\n",
      "episode: 1049   score: 200.0  epsilon: 1.0    steps: 440  evaluation reward: 138.9\n",
      "episode: 1050   score: 140.0  epsilon: 1.0    steps: 672  evaluation reward: 139.3\n",
      "now time :  2019-02-28 11:21:04.795307\n",
      "episode: 1051   score: 110.0  epsilon: 1.0    steps: 824  evaluation reward: 139.3\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2200: Policy loss: 0.006559. Value loss: 0.611484. Entropy: 1.252209.\n",
      "Iteration 2201: Policy loss: -0.004358. Value loss: 0.224517. Entropy: 1.253729.\n",
      "Iteration 2202: Policy loss: -0.003543. Value loss: 0.165676. Entropy: 1.255695.\n",
      "episode: 1052   score: 85.0  epsilon: 1.0    steps: 1016  evaluation reward: 139.15\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2203: Policy loss: 0.001848. Value loss: 1.026460. Entropy: 1.216336.\n",
      "Iteration 2204: Policy loss: -0.004115. Value loss: 0.386627. Entropy: 1.211035.\n",
      "Iteration 2205: Policy loss: -0.005093. Value loss: 0.264186. Entropy: 1.210558.\n",
      "episode: 1053   score: 60.0  epsilon: 1.0    steps: 184  evaluation reward: 138.2\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2206: Policy loss: 0.004166. Value loss: 1.015096. Entropy: 1.250303.\n",
      "Iteration 2207: Policy loss: 0.002217. Value loss: 0.458632. Entropy: 1.249005.\n",
      "Iteration 2208: Policy loss: -0.008689. Value loss: 0.301739. Entropy: 1.247591.\n",
      "episode: 1054   score: 155.0  epsilon: 1.0    steps: 496  evaluation reward: 138.2\n",
      "episode: 1055   score: 50.0  epsilon: 1.0    steps: 888  evaluation reward: 137.6\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2209: Policy loss: 0.001264. Value loss: 0.731583. Entropy: 1.213882.\n",
      "Iteration 2210: Policy loss: -0.004148. Value loss: 0.268042. Entropy: 1.234440.\n",
      "Iteration 2211: Policy loss: -0.002564. Value loss: 0.159876. Entropy: 1.213489.\n",
      "episode: 1056   score: 150.0  epsilon: 1.0    steps: 384  evaluation reward: 138.55\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2212: Policy loss: 0.005436. Value loss: 0.801352. Entropy: 1.264159.\n",
      "Iteration 2213: Policy loss: -0.000447. Value loss: 0.447861. Entropy: 1.249597.\n",
      "Iteration 2214: Policy loss: -0.000132. Value loss: 0.276101. Entropy: 1.257405.\n",
      "episode: 1057   score: 45.0  epsilon: 1.0    steps: 384  evaluation reward: 137.2\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2215: Policy loss: 0.004601. Value loss: 0.818784. Entropy: 1.216235.\n",
      "Iteration 2216: Policy loss: -0.004288. Value loss: 0.348294. Entropy: 1.220843.\n",
      "Iteration 2217: Policy loss: -0.003698. Value loss: 0.245290. Entropy: 1.221080.\n",
      "episode: 1058   score: 55.0  epsilon: 1.0    steps: 248  evaluation reward: 137.45\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2218: Policy loss: 0.000101. Value loss: 0.666737. Entropy: 1.278719.\n",
      "Iteration 2219: Policy loss: -0.004677. Value loss: 0.230983. Entropy: 1.278983.\n",
      "Iteration 2220: Policy loss: -0.009576. Value loss: 0.150369. Entropy: 1.280163.\n",
      "episode: 1059   score: 210.0  epsilon: 1.0    steps: 376  evaluation reward: 139.2\n",
      "episode: 1060   score: 65.0  epsilon: 1.0    steps: 480  evaluation reward: 137.85\n",
      "episode: 1061   score: 350.0  epsilon: 1.0    steps: 744  evaluation reward: 140.85\n",
      "episode: 1062   score: 305.0  epsilon: 1.0    steps: 880  evaluation reward: 141.8\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2221: Policy loss: 0.000121. Value loss: 0.646333. Entropy: 1.240968.\n",
      "Iteration 2222: Policy loss: -0.004006. Value loss: 0.233082. Entropy: 1.242875.\n",
      "Iteration 2223: Policy loss: -0.005731. Value loss: 0.176080. Entropy: 1.252666.\n",
      "episode: 1063   score: 145.0  epsilon: 1.0    steps: 8  evaluation reward: 142.55\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2224: Policy loss: 0.000833. Value loss: 0.788577. Entropy: 1.190724.\n",
      "Iteration 2225: Policy loss: -0.001251. Value loss: 0.416855. Entropy: 1.186726.\n",
      "Iteration 2226: Policy loss: -0.004422. Value loss: 0.293919. Entropy: 1.175916.\n",
      "episode: 1064   score: 80.0  epsilon: 1.0    steps: 240  evaluation reward: 141.8\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2227: Policy loss: 0.001170. Value loss: 0.737567. Entropy: 1.220695.\n",
      "Iteration 2228: Policy loss: -0.003314. Value loss: 0.301779. Entropy: 1.231820.\n",
      "Iteration 2229: Policy loss: -0.006227. Value loss: 0.167290. Entropy: 1.230615.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2230: Policy loss: 0.003625. Value loss: 0.598576. Entropy: 1.288136.\n",
      "Iteration 2231: Policy loss: -0.003042. Value loss: 0.243045. Entropy: 1.277053.\n",
      "Iteration 2232: Policy loss: -0.004829. Value loss: 0.181707. Entropy: 1.284002.\n",
      "episode: 1065   score: 185.0  epsilon: 1.0    steps: 960  evaluation reward: 142.1\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2233: Policy loss: 0.001483. Value loss: 0.648994. Entropy: 1.265199.\n",
      "Iteration 2234: Policy loss: -0.007573. Value loss: 0.248808. Entropy: 1.269419.\n",
      "Iteration 2235: Policy loss: -0.012561. Value loss: 0.186167. Entropy: 1.269805.\n",
      "episode: 1066   score: 130.0  epsilon: 1.0    steps: 64  evaluation reward: 142.2\n",
      "episode: 1067   score: 210.0  epsilon: 1.0    steps: 456  evaluation reward: 143.25\n",
      "episode: 1068   score: 260.0  epsilon: 1.0    steps: 768  evaluation reward: 145.35\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2236: Policy loss: 0.002897. Value loss: 0.523006. Entropy: 1.256896.\n",
      "Iteration 2237: Policy loss: -0.005583. Value loss: 0.244568. Entropy: 1.261158.\n",
      "Iteration 2238: Policy loss: -0.008241. Value loss: 0.168633. Entropy: 1.263976.\n",
      "episode: 1069   score: 185.0  epsilon: 1.0    steps: 512  evaluation reward: 145.1\n",
      "episode: 1070   score: 260.0  epsilon: 1.0    steps: 976  evaluation reward: 146.15\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2239: Policy loss: -0.001817. Value loss: 0.697920. Entropy: 1.161101.\n",
      "Iteration 2240: Policy loss: 0.001193. Value loss: 0.239376. Entropy: 1.179559.\n",
      "Iteration 2241: Policy loss: -0.010771. Value loss: 0.175576. Entropy: 1.174100.\n",
      "episode: 1071   score: 180.0  epsilon: 1.0    steps: 328  evaluation reward: 145.85\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2242: Policy loss: -0.000478. Value loss: 0.477417. Entropy: 1.128774.\n",
      "Iteration 2243: Policy loss: -0.004353. Value loss: 0.201872. Entropy: 1.156159.\n",
      "Iteration 2244: Policy loss: -0.005684. Value loss: 0.122076. Entropy: 1.137345.\n",
      "episode: 1072   score: 75.0  epsilon: 1.0    steps: 16  evaluation reward: 144.2\n",
      "episode: 1073   score: 125.0  epsilon: 1.0    steps: 952  evaluation reward: 144.35\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2245: Policy loss: -0.001554. Value loss: 0.473418. Entropy: 1.208474.\n",
      "Iteration 2246: Policy loss: -0.004355. Value loss: 0.219785. Entropy: 1.202830.\n",
      "Iteration 2247: Policy loss: -0.010043. Value loss: 0.141633. Entropy: 1.209671.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2248: Policy loss: 0.000050. Value loss: 0.742566. Entropy: 1.271945.\n",
      "Iteration 2249: Policy loss: -0.009239. Value loss: 0.262616. Entropy: 1.270644.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2250: Policy loss: -0.012383. Value loss: 0.159802. Entropy: 1.265557.\n",
      "episode: 1074   score: 185.0  epsilon: 1.0    steps: 112  evaluation reward: 144.45\n",
      "episode: 1075   score: 75.0  epsilon: 1.0    steps: 272  evaluation reward: 144.9\n",
      "episode: 1076   score: 305.0  epsilon: 1.0    steps: 320  evaluation reward: 147.45\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2251: Policy loss: 0.000604. Value loss: 0.487497. Entropy: 1.279967.\n",
      "Iteration 2252: Policy loss: 0.003074. Value loss: 0.186318. Entropy: 1.275977.\n",
      "Iteration 2253: Policy loss: -0.007464. Value loss: 0.130735. Entropy: 1.275476.\n",
      "episode: 1077   score: 110.0  epsilon: 1.0    steps: 232  evaluation reward: 148.05\n",
      "episode: 1078   score: 160.0  epsilon: 1.0    steps: 272  evaluation reward: 148.6\n",
      "episode: 1079   score: 195.0  epsilon: 1.0    steps: 880  evaluation reward: 150.25\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2254: Policy loss: 0.003903. Value loss: 0.470680. Entropy: 1.229092.\n",
      "Iteration 2255: Policy loss: 0.001633. Value loss: 0.204196. Entropy: 1.221346.\n",
      "Iteration 2256: Policy loss: -0.004504. Value loss: 0.166642. Entropy: 1.228280.\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2257: Policy loss: -0.001796. Value loss: 0.811673. Entropy: 1.190204.\n",
      "Iteration 2258: Policy loss: -0.009124. Value loss: 0.343543. Entropy: 1.189784.\n",
      "Iteration 2259: Policy loss: -0.009862. Value loss: 0.258255. Entropy: 1.201886.\n",
      "episode: 1080   score: 120.0  epsilon: 1.0    steps: 320  evaluation reward: 150.7\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2260: Policy loss: -0.000670. Value loss: 0.673604. Entropy: 1.265449.\n",
      "Iteration 2261: Policy loss: -0.000873. Value loss: 0.307248. Entropy: 1.278897.\n",
      "Iteration 2262: Policy loss: -0.004074. Value loss: 0.214615. Entropy: 1.279163.\n",
      "episode: 1081   score: 80.0  epsilon: 1.0    steps: 24  evaluation reward: 149.7\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2263: Policy loss: 0.003037. Value loss: 0.596703. Entropy: 1.244181.\n",
      "Iteration 2264: Policy loss: -0.001856. Value loss: 0.192635. Entropy: 1.240267.\n",
      "Iteration 2265: Policy loss: -0.003375. Value loss: 0.133089. Entropy: 1.248840.\n",
      "episode: 1082   score: 105.0  epsilon: 1.0    steps: 344  evaluation reward: 148.5\n",
      "episode: 1083   score: 185.0  epsilon: 1.0    steps: 448  evaluation reward: 149.25\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2266: Policy loss: 0.001061. Value loss: 0.745641. Entropy: 1.270400.\n",
      "Iteration 2267: Policy loss: 0.004750. Value loss: 0.405144. Entropy: 1.260510.\n",
      "Iteration 2268: Policy loss: -0.002713. Value loss: 0.296427. Entropy: 1.262767.\n",
      "episode: 1084   score: 75.0  epsilon: 1.0    steps: 760  evaluation reward: 149.25\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2269: Policy loss: 0.001663. Value loss: 0.784959. Entropy: 1.245881.\n",
      "Iteration 2270: Policy loss: 0.000453. Value loss: 0.380238. Entropy: 1.255123.\n",
      "Iteration 2271: Policy loss: -0.004881. Value loss: 0.247759. Entropy: 1.257837.\n",
      "episode: 1085   score: 215.0  epsilon: 1.0    steps: 504  evaluation reward: 150.9\n",
      "episode: 1086   score: 225.0  epsilon: 1.0    steps: 904  evaluation reward: 151.6\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2272: Policy loss: 0.003268. Value loss: 0.647543. Entropy: 1.272270.\n",
      "Iteration 2273: Policy loss: -0.001377. Value loss: 0.231390. Entropy: 1.285521.\n",
      "Iteration 2274: Policy loss: -0.004657. Value loss: 0.157231. Entropy: 1.278521.\n",
      "episode: 1087   score: 255.0  epsilon: 1.0    steps: 176  evaluation reward: 153.35\n",
      "episode: 1088   score: 105.0  epsilon: 1.0    steps: 536  evaluation reward: 153.05\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2275: Policy loss: 0.000524. Value loss: 0.801865. Entropy: 1.280349.\n",
      "Iteration 2276: Policy loss: -0.001644. Value loss: 0.412789. Entropy: 1.281291.\n",
      "Iteration 2277: Policy loss: -0.006117. Value loss: 0.263974. Entropy: 1.277854.\n",
      "episode: 1089   score: 520.0  epsilon: 1.0    steps: 752  evaluation reward: 156.7\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2278: Policy loss: 0.000649. Value loss: 0.572570. Entropy: 1.221848.\n",
      "Iteration 2279: Policy loss: -0.003531. Value loss: 0.301317. Entropy: 1.220492.\n",
      "Iteration 2280: Policy loss: -0.005649. Value loss: 0.190871. Entropy: 1.214870.\n",
      "episode: 1090   score: 70.0  epsilon: 1.0    steps: 488  evaluation reward: 156.6\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2281: Policy loss: 0.001800. Value loss: 0.577371. Entropy: 1.257932.\n",
      "Iteration 2282: Policy loss: -0.003224. Value loss: 0.235321. Entropy: 1.251042.\n",
      "Iteration 2283: Policy loss: -0.006942. Value loss: 0.146183. Entropy: 1.257528.\n",
      "episode: 1091   score: 80.0  epsilon: 1.0    steps: 216  evaluation reward: 156.3\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2284: Policy loss: 0.000026. Value loss: 0.919830. Entropy: 1.277539.\n",
      "Iteration 2285: Policy loss: -0.002269. Value loss: 0.412826. Entropy: 1.278329.\n",
      "Iteration 2286: Policy loss: -0.011795. Value loss: 0.260510. Entropy: 1.279271.\n",
      "episode: 1092   score: 210.0  epsilon: 1.0    steps: 728  evaluation reward: 157.65\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2287: Policy loss: 0.000300. Value loss: 0.548785. Entropy: 1.229783.\n",
      "Iteration 2288: Policy loss: -0.004601. Value loss: 0.226908. Entropy: 1.229777.\n",
      "Iteration 2289: Policy loss: -0.009062. Value loss: 0.133126. Entropy: 1.232396.\n",
      "episode: 1093   score: 180.0  epsilon: 1.0    steps: 376  evaluation reward: 158.4\n",
      "episode: 1094   score: 155.0  epsilon: 1.0    steps: 904  evaluation reward: 158.7\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2290: Policy loss: -0.000580. Value loss: 0.471226. Entropy: 1.275098.\n",
      "Iteration 2291: Policy loss: 0.003120. Value loss: 0.189157. Entropy: 1.251127.\n",
      "Iteration 2292: Policy loss: -0.009526. Value loss: 0.172232. Entropy: 1.269787.\n",
      "episode: 1095   score: 210.0  epsilon: 1.0    steps: 448  evaluation reward: 160.15\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2293: Policy loss: -0.000074. Value loss: 0.573406. Entropy: 1.247180.\n",
      "Iteration 2294: Policy loss: -0.004488. Value loss: 0.279433. Entropy: 1.232209.\n",
      "Iteration 2295: Policy loss: -0.007683. Value loss: 0.189054. Entropy: 1.236197.\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2296: Policy loss: 0.000792. Value loss: 0.383803. Entropy: 1.252618.\n",
      "Iteration 2297: Policy loss: -0.007414. Value loss: 0.185250. Entropy: 1.243111.\n",
      "Iteration 2298: Policy loss: -0.009216. Value loss: 0.119436. Entropy: 1.242068.\n",
      "episode: 1096   score: 180.0  epsilon: 1.0    steps: 112  evaluation reward: 159.65\n",
      "episode: 1097   score: 75.0  epsilon: 1.0    steps: 416  evaluation reward: 159.05\n",
      "episode: 1098   score: 80.0  epsilon: 1.0    steps: 840  evaluation reward: 159.55\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2299: Policy loss: 0.000206. Value loss: 0.553446. Entropy: 1.283572.\n",
      "Iteration 2300: Policy loss: 0.002365. Value loss: 0.122553. Entropy: 1.289269.\n",
      "Iteration 2301: Policy loss: -0.000800. Value loss: 0.118144. Entropy: 1.289216.\n",
      "episode: 1099   score: 345.0  epsilon: 1.0    steps: 456  evaluation reward: 161.75\n",
      "episode: 1100   score: 395.0  epsilon: 1.0    steps: 544  evaluation reward: 163.55\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2302: Policy loss: 0.000487. Value loss: 0.641694. Entropy: 1.241764.\n",
      "Iteration 2303: Policy loss: -0.008092. Value loss: 0.278345. Entropy: 1.233739.\n",
      "Iteration 2304: Policy loss: -0.002891. Value loss: 0.171956. Entropy: 1.228378.\n",
      "now time :  2019-02-28 11:22:20.982978\n",
      "episode: 1101   score: 245.0  epsilon: 1.0    steps: 728  evaluation reward: 164.5\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2305: Policy loss: -0.000517. Value loss: 0.714749. Entropy: 1.217879.\n",
      "Iteration 2306: Policy loss: -0.000366. Value loss: 0.287842. Entropy: 1.212119.\n",
      "Iteration 2307: Policy loss: -0.013102. Value loss: 0.161237. Entropy: 1.222252.\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2308: Policy loss: -0.001469. Value loss: 0.544513. Entropy: 1.282323.\n",
      "Iteration 2309: Policy loss: -0.006545. Value loss: 0.309141. Entropy: 1.273661.\n",
      "Iteration 2310: Policy loss: -0.008188. Value loss: 0.197170. Entropy: 1.273578.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1102   score: 180.0  epsilon: 1.0    steps: 616  evaluation reward: 165.1\n",
      "episode: 1103   score: 50.0  epsilon: 1.0    steps: 928  evaluation reward: 164.5\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2311: Policy loss: -0.000164. Value loss: 0.475623. Entropy: 1.295171.\n",
      "Iteration 2312: Policy loss: -0.003246. Value loss: 0.196589. Entropy: 1.295372.\n",
      "Iteration 2313: Policy loss: -0.007347. Value loss: 0.133277. Entropy: 1.291646.\n",
      "episode: 1104   score: 165.0  epsilon: 1.0    steps: 152  evaluation reward: 164.95\n",
      "episode: 1105   score: 105.0  epsilon: 1.0    steps: 576  evaluation reward: 164.9\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2314: Policy loss: 0.000976. Value loss: 0.529897. Entropy: 1.248711.\n",
      "Iteration 2315: Policy loss: -0.004861. Value loss: 0.218406. Entropy: 1.244027.\n",
      "Iteration 2316: Policy loss: -0.008667. Value loss: 0.154751. Entropy: 1.242978.\n",
      "episode: 1106   score: 235.0  epsilon: 1.0    steps: 752  evaluation reward: 166.65\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2317: Policy loss: 0.003415. Value loss: 0.558296. Entropy: 1.233051.\n",
      "Iteration 2318: Policy loss: -0.004111. Value loss: 0.208756. Entropy: 1.225019.\n",
      "Iteration 2319: Policy loss: -0.011658. Value loss: 0.157214. Entropy: 1.235317.\n",
      "episode: 1107   score: 280.0  epsilon: 1.0    steps: 432  evaluation reward: 167.85\n",
      "episode: 1108   score: 210.0  epsilon: 1.0    steps: 944  evaluation reward: 168.15\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2320: Policy loss: -0.005078. Value loss: 0.522900. Entropy: 1.275278.\n",
      "Iteration 2321: Policy loss: -0.004271. Value loss: 0.217345. Entropy: 1.251866.\n",
      "Iteration 2322: Policy loss: -0.011158. Value loss: 0.171943. Entropy: 1.258779.\n",
      "episode: 1109   score: 315.0  epsilon: 1.0    steps: 376  evaluation reward: 170.8\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2323: Policy loss: 0.003006. Value loss: 0.526425. Entropy: 1.225961.\n",
      "Iteration 2324: Policy loss: -0.005642. Value loss: 0.245743. Entropy: 1.245339.\n",
      "Iteration 2325: Policy loss: -0.006678. Value loss: 0.140561. Entropy: 1.218590.\n",
      "episode: 1110   score: 130.0  epsilon: 1.0    steps: 112  evaluation reward: 170.5\n",
      "episode: 1111   score: 180.0  epsilon: 1.0    steps: 352  evaluation reward: 170.2\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2326: Policy loss: 0.004979. Value loss: 0.585484. Entropy: 1.201896.\n",
      "Iteration 2327: Policy loss: 0.000856. Value loss: 0.226333. Entropy: 1.212901.\n",
      "Iteration 2328: Policy loss: -0.004388. Value loss: 0.127653. Entropy: 1.213223.\n",
      "episode: 1112   score: 220.0  epsilon: 1.0    steps: 776  evaluation reward: 170.6\n",
      "episode: 1113   score: 90.0  epsilon: 1.0    steps: 912  evaluation reward: 170.15\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2329: Policy loss: -0.000879. Value loss: 0.506940. Entropy: 1.253050.\n",
      "Iteration 2330: Policy loss: -0.004728. Value loss: 0.204052. Entropy: 1.249308.\n",
      "Iteration 2331: Policy loss: -0.009776. Value loss: 0.147189. Entropy: 1.249024.\n",
      "episode: 1114   score: 230.0  epsilon: 1.0    steps: 904  evaluation reward: 170.5\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2332: Policy loss: 0.001782. Value loss: 0.418768. Entropy: 1.243508.\n",
      "Iteration 2333: Policy loss: -0.004671. Value loss: 0.207610. Entropy: 1.234746.\n",
      "Iteration 2334: Policy loss: -0.005934. Value loss: 0.162385. Entropy: 1.228665.\n",
      "episode: 1115   score: 160.0  epsilon: 1.0    steps: 120  evaluation reward: 170.75\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2335: Policy loss: -0.001224. Value loss: 0.553749. Entropy: 1.211205.\n",
      "Iteration 2336: Policy loss: -0.002593. Value loss: 0.236426. Entropy: 1.233245.\n",
      "Iteration 2337: Policy loss: -0.008035. Value loss: 0.142336. Entropy: 1.233458.\n",
      "episode: 1116   score: 165.0  epsilon: 1.0    steps: 384  evaluation reward: 170.3\n",
      "episode: 1117   score: 155.0  epsilon: 1.0    steps: 880  evaluation reward: 169.75\n",
      "episode: 1118   score: 105.0  epsilon: 1.0    steps: 888  evaluation reward: 169.8\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2338: Policy loss: 0.001822. Value loss: 0.743119. Entropy: 1.206779.\n",
      "Iteration 2339: Policy loss: 0.002121. Value loss: 0.247247. Entropy: 1.206035.\n",
      "Iteration 2340: Policy loss: 0.000122. Value loss: 0.161367. Entropy: 1.202242.\n",
      "episode: 1119   score: 95.0  epsilon: 1.0    steps: 880  evaluation reward: 165.95\n",
      "episode: 1120   score: 120.0  epsilon: 1.0    steps: 880  evaluation reward: 165.25\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2341: Policy loss: 0.002319. Value loss: 0.683053. Entropy: 1.215540.\n",
      "Iteration 2342: Policy loss: -0.005609. Value loss: 0.265956. Entropy: 1.228873.\n",
      "Iteration 2343: Policy loss: -0.006450. Value loss: 0.192356. Entropy: 1.202605.\n",
      "episode: 1121   score: 225.0  epsilon: 1.0    steps: 896  evaluation reward: 166.5\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2344: Policy loss: -0.000644. Value loss: 0.823007. Entropy: 1.239343.\n",
      "Iteration 2345: Policy loss: -0.000312. Value loss: 0.326943. Entropy: 1.236118.\n",
      "Iteration 2346: Policy loss: -0.013224. Value loss: 0.206775. Entropy: 1.247508.\n",
      "episode: 1122   score: 80.0  epsilon: 1.0    steps: 8  evaluation reward: 166.2\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2347: Policy loss: -0.000308. Value loss: 0.657250. Entropy: 1.244217.\n",
      "Iteration 2348: Policy loss: -0.006339. Value loss: 0.182201. Entropy: 1.218523.\n",
      "Iteration 2349: Policy loss: -0.007753. Value loss: 0.118521. Entropy: 1.220901.\n",
      "episode: 1123   score: 80.0  epsilon: 1.0    steps: 944  evaluation reward: 165.3\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2350: Policy loss: -0.000013. Value loss: 0.431179. Entropy: 1.281412.\n",
      "Iteration 2351: Policy loss: -0.004223. Value loss: 0.176369. Entropy: 1.292217.\n",
      "Iteration 2352: Policy loss: -0.008055. Value loss: 0.120885. Entropy: 1.288592.\n",
      "episode: 1124   score: 260.0  epsilon: 1.0    steps: 200  evaluation reward: 165.9\n",
      "episode: 1125   score: 225.0  epsilon: 1.0    steps: 992  evaluation reward: 167.2\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2353: Policy loss: -0.000873. Value loss: 0.312691. Entropy: 1.290573.\n",
      "Iteration 2354: Policy loss: -0.005029. Value loss: 0.151978. Entropy: 1.287906.\n",
      "Iteration 2355: Policy loss: -0.011973. Value loss: 0.093783. Entropy: 1.293609.\n",
      "episode: 1126   score: 160.0  epsilon: 1.0    steps: 432  evaluation reward: 167.0\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2356: Policy loss: -0.001010. Value loss: 0.537301. Entropy: 1.211777.\n",
      "Iteration 2357: Policy loss: -0.006031. Value loss: 0.182175. Entropy: 1.211598.\n",
      "Iteration 2358: Policy loss: -0.012053. Value loss: 0.160866. Entropy: 1.213971.\n",
      "episode: 1127   score: 105.0  epsilon: 1.0    steps: 448  evaluation reward: 166.25\n",
      "episode: 1128   score: 270.0  epsilon: 1.0    steps: 832  evaluation reward: 168.45\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2359: Policy loss: 0.002496. Value loss: 1.024425. Entropy: 1.240566.\n",
      "Iteration 2360: Policy loss: -0.001251. Value loss: 0.440474. Entropy: 1.249369.\n",
      "Iteration 2361: Policy loss: -0.007875. Value loss: 0.266718. Entropy: 1.233200.\n",
      "episode: 1129   score: 420.0  epsilon: 1.0    steps: 888  evaluation reward: 171.8\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2362: Policy loss: 0.001767. Value loss: 0.580676. Entropy: 1.230264.\n",
      "Iteration 2363: Policy loss: -0.004117. Value loss: 0.294878. Entropy: 1.226149.\n",
      "Iteration 2364: Policy loss: -0.009373. Value loss: 0.183767. Entropy: 1.224389.\n",
      "episode: 1130   score: 85.0  epsilon: 1.0    steps: 256  evaluation reward: 170.55\n",
      "episode: 1131   score: 90.0  epsilon: 1.0    steps: 768  evaluation reward: 170.45\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2365: Policy loss: -0.001820. Value loss: 0.602595. Entropy: 1.243655.\n",
      "Iteration 2366: Policy loss: 0.003276. Value loss: 0.237568. Entropy: 1.262142.\n",
      "Iteration 2367: Policy loss: -0.005061. Value loss: 0.143045. Entropy: 1.259585.\n",
      "episode: 1132   score: 240.0  epsilon: 1.0    steps: 680  evaluation reward: 171.45\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2368: Policy loss: 0.004064. Value loss: 0.856078. Entropy: 1.250448.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2369: Policy loss: -0.001047. Value loss: 0.341808. Entropy: 1.250067.\n",
      "Iteration 2370: Policy loss: -0.007383. Value loss: 0.227436. Entropy: 1.255263.\n",
      "episode: 1133   score: 215.0  epsilon: 1.0    steps: 272  evaluation reward: 171.3\n",
      "episode: 1134   score: 80.0  epsilon: 1.0    steps: 272  evaluation reward: 167.95\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2371: Policy loss: 0.002183. Value loss: 1.030706. Entropy: 1.282869.\n",
      "Iteration 2372: Policy loss: 0.005611. Value loss: 0.367807. Entropy: 1.286081.\n",
      "Iteration 2373: Policy loss: -0.003383. Value loss: 0.217569. Entropy: 1.293463.\n",
      "episode: 1135   score: 135.0  epsilon: 1.0    steps: 240  evaluation reward: 166.8\n",
      "episode: 1136   score: 355.0  epsilon: 1.0    steps: 344  evaluation reward: 169.7\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2374: Policy loss: -0.000906. Value loss: 0.477510. Entropy: 1.266204.\n",
      "Iteration 2375: Policy loss: -0.002735. Value loss: 0.274481. Entropy: 1.262409.\n",
      "Iteration 2376: Policy loss: -0.006048. Value loss: 0.185104. Entropy: 1.261045.\n",
      "episode: 1137   score: 80.0  epsilon: 1.0    steps: 488  evaluation reward: 169.9\n",
      "episode: 1138   score: 155.0  epsilon: 1.0    steps: 560  evaluation reward: 167.5\n",
      "episode: 1139   score: 120.0  epsilon: 1.0    steps: 624  evaluation reward: 167.3\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2377: Policy loss: 0.001633. Value loss: 0.429104. Entropy: 1.221680.\n",
      "Iteration 2378: Policy loss: -0.001843. Value loss: 0.180101. Entropy: 1.211393.\n",
      "Iteration 2379: Policy loss: -0.007248. Value loss: 0.149681. Entropy: 1.221196.\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2380: Policy loss: -0.002368. Value loss: 0.535977. Entropy: 1.203986.\n",
      "Iteration 2381: Policy loss: -0.004592. Value loss: 0.324149. Entropy: 1.211877.\n",
      "Iteration 2382: Policy loss: -0.002746. Value loss: 0.251589. Entropy: 1.221015.\n",
      "episode: 1140   score: 120.0  epsilon: 1.0    steps: 240  evaluation reward: 166.9\n",
      "episode: 1141   score: 165.0  epsilon: 1.0    steps: 360  evaluation reward: 168.25\n",
      "episode: 1142   score: 80.0  epsilon: 1.0    steps: 808  evaluation reward: 167.7\n",
      "episode: 1143   score: 180.0  epsilon: 1.0    steps: 872  evaluation reward: 166.9\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2383: Policy loss: 0.002315. Value loss: 0.447246. Entropy: 1.303799.\n",
      "Iteration 2384: Policy loss: 0.000378. Value loss: 0.172805. Entropy: 1.308364.\n",
      "Iteration 2385: Policy loss: -0.006251. Value loss: 0.098690. Entropy: 1.312352.\n",
      "episode: 1144   score: 155.0  epsilon: 1.0    steps: 248  evaluation reward: 167.3\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2386: Policy loss: -0.000512. Value loss: 0.500278. Entropy: 1.217418.\n",
      "Iteration 2387: Policy loss: -0.002453. Value loss: 0.176647. Entropy: 1.214465.\n",
      "Iteration 2388: Policy loss: -0.006724. Value loss: 0.102223. Entropy: 1.209474.\n",
      "episode: 1145   score: 90.0  epsilon: 1.0    steps: 400  evaluation reward: 167.6\n",
      "episode: 1146   score: 50.0  epsilon: 1.0    steps: 896  evaluation reward: 167.35\n",
      "episode: 1147   score: 150.0  epsilon: 1.0    steps: 904  evaluation reward: 166.5\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2389: Policy loss: 0.002389. Value loss: 0.377442. Entropy: 1.248487.\n",
      "Iteration 2390: Policy loss: -0.003341. Value loss: 0.183780. Entropy: 1.249671.\n",
      "Iteration 2391: Policy loss: -0.005162. Value loss: 0.123657. Entropy: 1.247814.\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2392: Policy loss: -0.001195. Value loss: 0.647694. Entropy: 1.238011.\n",
      "Iteration 2393: Policy loss: -0.004350. Value loss: 0.216352. Entropy: 1.246354.\n",
      "Iteration 2394: Policy loss: -0.003911. Value loss: 0.148167. Entropy: 1.254507.\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2395: Policy loss: 0.002726. Value loss: 0.622532. Entropy: 1.275296.\n",
      "Iteration 2396: Policy loss: -0.000772. Value loss: 0.332677. Entropy: 1.274512.\n",
      "Iteration 2397: Policy loss: -0.005973. Value loss: 0.221065. Entropy: 1.278874.\n",
      "episode: 1148   score: 135.0  epsilon: 1.0    steps: 16  evaluation reward: 166.5\n",
      "episode: 1149   score: 135.0  epsilon: 1.0    steps: 208  evaluation reward: 165.85\n",
      "episode: 1150   score: 225.0  epsilon: 1.0    steps: 1000  evaluation reward: 166.7\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2398: Policy loss: -0.000203. Value loss: 0.684842. Entropy: 1.325256.\n",
      "Iteration 2399: Policy loss: -0.001711. Value loss: 0.246527. Entropy: 1.327560.\n",
      "Iteration 2400: Policy loss: -0.004618. Value loss: 0.162379. Entropy: 1.324483.\n",
      "now time :  2019-02-28 11:23:30.921890\n",
      "episode: 1151   score: 180.0  epsilon: 1.0    steps: 952  evaluation reward: 167.4\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2401: Policy loss: 0.001874. Value loss: 0.784766. Entropy: 1.260401.\n",
      "Iteration 2402: Policy loss: -0.003874. Value loss: 0.301586. Entropy: 1.262106.\n",
      "Iteration 2403: Policy loss: -0.007517. Value loss: 0.163374. Entropy: 1.257918.\n",
      "episode: 1152   score: 135.0  epsilon: 1.0    steps: 528  evaluation reward: 167.9\n",
      "episode: 1153   score: 125.0  epsilon: 1.0    steps: 552  evaluation reward: 168.55\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2404: Policy loss: 0.001665. Value loss: 0.471074. Entropy: 1.204298.\n",
      "Iteration 2405: Policy loss: -0.005626. Value loss: 0.190021. Entropy: 1.198981.\n",
      "Iteration 2406: Policy loss: -0.010693. Value loss: 0.135710. Entropy: 1.210639.\n",
      "episode: 1154   score: 310.0  epsilon: 1.0    steps: 840  evaluation reward: 170.1\n",
      "episode: 1155   score: 65.0  epsilon: 1.0    steps: 936  evaluation reward: 170.25\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2407: Policy loss: 0.001971. Value loss: 0.442155. Entropy: 1.176864.\n",
      "Iteration 2408: Policy loss: -0.003415. Value loss: 0.210362. Entropy: 1.161916.\n",
      "Iteration 2409: Policy loss: -0.003499. Value loss: 0.151090. Entropy: 1.161350.\n",
      "episode: 1156   score: 125.0  epsilon: 1.0    steps: 440  evaluation reward: 170.0\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2410: Policy loss: 0.002738. Value loss: 1.090669. Entropy: 1.205113.\n",
      "Iteration 2411: Policy loss: 0.002844. Value loss: 0.417378. Entropy: 1.220295.\n",
      "Iteration 2412: Policy loss: -0.003932. Value loss: 0.248529. Entropy: 1.221143.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2413: Policy loss: -0.001037. Value loss: 0.620470. Entropy: 1.223799.\n",
      "Iteration 2414: Policy loss: -0.007710. Value loss: 0.222599. Entropy: 1.214381.\n",
      "Iteration 2415: Policy loss: -0.014559. Value loss: 0.176518. Entropy: 1.204676.\n",
      "episode: 1157   score: 380.0  epsilon: 1.0    steps: 224  evaluation reward: 173.35\n",
      "episode: 1158   score: 210.0  epsilon: 1.0    steps: 512  evaluation reward: 174.9\n",
      "episode: 1159   score: 90.0  epsilon: 1.0    steps: 936  evaluation reward: 173.7\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2416: Policy loss: 0.003874. Value loss: 0.505877. Entropy: 1.311568.\n",
      "Iteration 2417: Policy loss: -0.003539. Value loss: 0.203483. Entropy: 1.315524.\n",
      "Iteration 2418: Policy loss: -0.008403. Value loss: 0.135053. Entropy: 1.317030.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2419: Policy loss: -0.001587. Value loss: 0.488565. Entropy: 1.203918.\n",
      "Iteration 2420: Policy loss: -0.006302. Value loss: 0.233549. Entropy: 1.205251.\n",
      "Iteration 2421: Policy loss: -0.010374. Value loss: 0.160676. Entropy: 1.207083.\n",
      "episode: 1160   score: 210.0  epsilon: 1.0    steps: 176  evaluation reward: 175.15\n",
      "episode: 1161   score: 275.0  epsilon: 1.0    steps: 984  evaluation reward: 174.4\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2422: Policy loss: 0.005599. Value loss: 0.610293. Entropy: 1.261962.\n",
      "Iteration 2423: Policy loss: -0.005198. Value loss: 0.256952. Entropy: 1.262568.\n",
      "Iteration 2424: Policy loss: -0.008383. Value loss: 0.175444. Entropy: 1.267857.\n",
      "episode: 1162   score: 155.0  epsilon: 1.0    steps: 144  evaluation reward: 172.9\n",
      "episode: 1163   score: 240.0  epsilon: 1.0    steps: 744  evaluation reward: 173.85\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2425: Policy loss: 0.003018. Value loss: 0.556561. Entropy: 1.218717.\n",
      "Iteration 2426: Policy loss: -0.002963. Value loss: 0.238808. Entropy: 1.199521.\n",
      "Iteration 2427: Policy loss: -0.006756. Value loss: 0.210280. Entropy: 1.206977.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1164   score: 105.0  epsilon: 1.0    steps: 784  evaluation reward: 174.1\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2428: Policy loss: 0.001041. Value loss: 0.459163. Entropy: 1.139717.\n",
      "Iteration 2429: Policy loss: -0.005870. Value loss: 0.225984. Entropy: 1.152168.\n",
      "Iteration 2430: Policy loss: -0.008758. Value loss: 0.154809. Entropy: 1.147865.\n",
      "episode: 1165   score: 180.0  epsilon: 1.0    steps: 480  evaluation reward: 174.05\n",
      "episode: 1166   score: 315.0  epsilon: 1.0    steps: 832  evaluation reward: 175.9\n",
      "episode: 1167   score: 125.0  epsilon: 1.0    steps: 1016  evaluation reward: 175.05\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2431: Policy loss: 0.000325. Value loss: 0.770926. Entropy: 1.242528.\n",
      "Iteration 2432: Policy loss: -0.002768. Value loss: 0.249712. Entropy: 1.237276.\n",
      "Iteration 2433: Policy loss: -0.006589. Value loss: 0.173792. Entropy: 1.250345.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2434: Policy loss: 0.000038. Value loss: 0.298702. Entropy: 1.172785.\n",
      "Iteration 2435: Policy loss: -0.002283. Value loss: 0.176315. Entropy: 1.200072.\n",
      "Iteration 2436: Policy loss: -0.003115. Value loss: 0.113754. Entropy: 1.180252.\n",
      "episode: 1168   score: 180.0  epsilon: 1.0    steps: 696  evaluation reward: 174.25\n",
      "episode: 1169   score: 210.0  epsilon: 1.0    steps: 992  evaluation reward: 174.5\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2437: Policy loss: 0.000376. Value loss: 0.556061. Entropy: 1.201111.\n",
      "Iteration 2438: Policy loss: -0.004469. Value loss: 0.267851. Entropy: 1.224164.\n",
      "Iteration 2439: Policy loss: -0.007399. Value loss: 0.162065. Entropy: 1.201788.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2440: Policy loss: 0.000392. Value loss: 0.747778. Entropy: 1.249597.\n",
      "Iteration 2441: Policy loss: -0.005805. Value loss: 0.301927. Entropy: 1.251374.\n",
      "Iteration 2442: Policy loss: -0.010118. Value loss: 0.224197. Entropy: 1.257417.\n",
      "episode: 1170   score: 315.0  epsilon: 1.0    steps: 104  evaluation reward: 175.05\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2443: Policy loss: 0.001322. Value loss: 0.821398. Entropy: 1.162454.\n",
      "Iteration 2444: Policy loss: 0.002691. Value loss: 0.338316. Entropy: 1.162274.\n",
      "Iteration 2445: Policy loss: -0.006458. Value loss: 0.230031. Entropy: 1.162138.\n",
      "episode: 1171   score: 120.0  epsilon: 1.0    steps: 528  evaluation reward: 174.45\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2446: Policy loss: 0.006692. Value loss: 0.573982. Entropy: 1.252443.\n",
      "Iteration 2447: Policy loss: -0.001134. Value loss: 0.192733. Entropy: 1.250377.\n",
      "Iteration 2448: Policy loss: -0.007671. Value loss: 0.135169. Entropy: 1.252825.\n",
      "episode: 1172   score: 375.0  epsilon: 1.0    steps: 416  evaluation reward: 177.45\n",
      "episode: 1173   score: 105.0  epsilon: 1.0    steps: 480  evaluation reward: 177.25\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2449: Policy loss: -0.000246. Value loss: 0.578618. Entropy: 1.253194.\n",
      "Iteration 2450: Policy loss: 0.000174. Value loss: 0.256141. Entropy: 1.252114.\n",
      "Iteration 2451: Policy loss: -0.007645. Value loss: 0.184832. Entropy: 1.260999.\n",
      "episode: 1174   score: 155.0  epsilon: 1.0    steps: 368  evaluation reward: 176.95\n",
      "episode: 1175   score: 245.0  epsilon: 1.0    steps: 896  evaluation reward: 178.65\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2452: Policy loss: 0.003026. Value loss: 0.601186. Entropy: 1.207893.\n",
      "Iteration 2453: Policy loss: -0.002592. Value loss: 0.252575. Entropy: 1.210587.\n",
      "Iteration 2454: Policy loss: -0.006414. Value loss: 0.147944. Entropy: 1.211241.\n",
      "episode: 1176   score: 130.0  epsilon: 1.0    steps: 808  evaluation reward: 176.9\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2455: Policy loss: 0.002896. Value loss: 0.470596. Entropy: 1.245716.\n",
      "Iteration 2456: Policy loss: -0.005036. Value loss: 0.161286. Entropy: 1.229637.\n",
      "Iteration 2457: Policy loss: -0.005403. Value loss: 0.124981. Entropy: 1.238007.\n",
      "episode: 1177   score: 65.0  epsilon: 1.0    steps: 472  evaluation reward: 176.45\n",
      "episode: 1178   score: 255.0  epsilon: 1.0    steps: 904  evaluation reward: 177.4\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2458: Policy loss: 0.000008. Value loss: 0.273998. Entropy: 1.260526.\n",
      "Iteration 2459: Policy loss: -0.010353. Value loss: 0.130904. Entropy: 1.247394.\n",
      "Iteration 2460: Policy loss: -0.008342. Value loss: 0.091327. Entropy: 1.250412.\n",
      "episode: 1179   score: 135.0  epsilon: 1.0    steps: 752  evaluation reward: 176.8\n",
      "episode: 1180   score: 30.0  epsilon: 1.0    steps: 920  evaluation reward: 175.9\n",
      "episode: 1181   score: 285.0  epsilon: 1.0    steps: 952  evaluation reward: 177.95\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2461: Policy loss: 0.001775. Value loss: 0.918653. Entropy: 1.159230.\n",
      "Iteration 2462: Policy loss: -0.002603. Value loss: 0.237634. Entropy: 1.143109.\n",
      "Iteration 2463: Policy loss: -0.010221. Value loss: 0.135873. Entropy: 1.165972.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2464: Policy loss: 0.007611. Value loss: 0.763929. Entropy: 1.146916.\n",
      "Iteration 2465: Policy loss: -0.001238. Value loss: 0.338506. Entropy: 1.177940.\n",
      "Iteration 2466: Policy loss: -0.002909. Value loss: 0.260119. Entropy: 1.155926.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2467: Policy loss: 0.000431. Value loss: 0.515379. Entropy: 1.226879.\n",
      "Iteration 2468: Policy loss: -0.003674. Value loss: 0.293790. Entropy: 1.227426.\n",
      "Iteration 2469: Policy loss: -0.010734. Value loss: 0.212685. Entropy: 1.220366.\n",
      "episode: 1182   score: 150.0  epsilon: 1.0    steps: 632  evaluation reward: 178.4\n",
      "episode: 1183   score: 380.0  epsilon: 1.0    steps: 992  evaluation reward: 180.35\n",
      "episode: 1184   score: 50.0  epsilon: 1.0    steps: 1016  evaluation reward: 180.1\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2470: Policy loss: -0.000941. Value loss: 0.601398. Entropy: 1.307260.\n",
      "Iteration 2471: Policy loss: -0.004882. Value loss: 0.192418. Entropy: 1.312801.\n",
      "Iteration 2472: Policy loss: -0.013102. Value loss: 0.132144. Entropy: 1.314418.\n",
      "episode: 1185   score: 175.0  epsilon: 1.0    steps: 248  evaluation reward: 179.7\n",
      "episode: 1186   score: 180.0  epsilon: 1.0    steps: 256  evaluation reward: 179.25\n",
      "episode: 1187   score: 120.0  epsilon: 1.0    steps: 496  evaluation reward: 177.9\n",
      "episode: 1188   score: 85.0  epsilon: 1.0    steps: 680  evaluation reward: 177.7\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2473: Policy loss: -0.000976. Value loss: 0.330097. Entropy: 1.204629.\n",
      "Iteration 2474: Policy loss: -0.003015. Value loss: 0.234790. Entropy: 1.209426.\n",
      "Iteration 2475: Policy loss: -0.007143. Value loss: 0.184785. Entropy: 1.206130.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2476: Policy loss: 0.003710. Value loss: 0.394159. Entropy: 0.852268.\n",
      "Iteration 2477: Policy loss: -0.002473. Value loss: 0.212033. Entropy: 0.872486.\n",
      "Iteration 2478: Policy loss: -0.006941. Value loss: 0.160053. Entropy: 0.874246.\n",
      "episode: 1189   score: 110.0  epsilon: 1.0    steps: 168  evaluation reward: 173.6\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2479: Policy loss: -0.001047. Value loss: 0.634074. Entropy: 1.207798.\n",
      "Iteration 2480: Policy loss: -0.004923. Value loss: 0.316516. Entropy: 1.194321.\n",
      "Iteration 2481: Policy loss: -0.006830. Value loss: 0.194279. Entropy: 1.207768.\n",
      "episode: 1190   score: 155.0  epsilon: 1.0    steps: 176  evaluation reward: 174.45\n",
      "episode: 1191   score: 135.0  epsilon: 1.0    steps: 776  evaluation reward: 175.0\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2482: Policy loss: 0.001182. Value loss: 0.499222. Entropy: 1.238448.\n",
      "Iteration 2483: Policy loss: -0.003408. Value loss: 0.233686. Entropy: 1.224755.\n",
      "Iteration 2484: Policy loss: -0.007163. Value loss: 0.150138. Entropy: 1.222947.\n",
      "episode: 1192   score: 180.0  epsilon: 1.0    steps: 296  evaluation reward: 174.7\n",
      "episode: 1193   score: 155.0  epsilon: 1.0    steps: 896  evaluation reward: 174.45\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2485: Policy loss: -0.000171. Value loss: 0.751081. Entropy: 1.187493.\n",
      "Iteration 2486: Policy loss: -0.005809. Value loss: 0.342022. Entropy: 1.185191.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2487: Policy loss: -0.006041. Value loss: 0.204342. Entropy: 1.186713.\n",
      "episode: 1194   score: 180.0  epsilon: 1.0    steps: 320  evaluation reward: 174.7\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2488: Policy loss: 0.000701. Value loss: 0.558322. Entropy: 1.127030.\n",
      "Iteration 2489: Policy loss: -0.004058. Value loss: 0.251644. Entropy: 1.125635.\n",
      "Iteration 2490: Policy loss: -0.008949. Value loss: 0.149678. Entropy: 1.124546.\n",
      "episode: 1195   score: 180.0  epsilon: 1.0    steps: 48  evaluation reward: 174.4\n",
      "episode: 1196   score: 210.0  epsilon: 1.0    steps: 160  evaluation reward: 174.7\n",
      "episode: 1197   score: 225.0  epsilon: 1.0    steps: 520  evaluation reward: 176.2\n",
      "episode: 1198   score: 75.0  epsilon: 1.0    steps: 952  evaluation reward: 176.15\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2491: Policy loss: 0.004505. Value loss: 0.497994. Entropy: 0.965617.\n",
      "Iteration 2492: Policy loss: -0.003042. Value loss: 0.204847. Entropy: 0.965706.\n",
      "Iteration 2493: Policy loss: -0.005456. Value loss: 0.126006. Entropy: 0.971820.\n",
      "episode: 1199   score: 65.0  epsilon: 1.0    steps: 128  evaluation reward: 173.35\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2494: Policy loss: -0.002023. Value loss: 0.525210. Entropy: 0.989398.\n",
      "Iteration 2495: Policy loss: -0.006419. Value loss: 0.253588. Entropy: 1.009085.\n",
      "Iteration 2496: Policy loss: -0.007874. Value loss: 0.162952. Entropy: 0.988079.\n",
      "episode: 1200   score: 80.0  epsilon: 1.0    steps: 544  evaluation reward: 170.2\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2497: Policy loss: 0.002511. Value loss: 0.425906. Entropy: 1.134865.\n",
      "Iteration 2498: Policy loss: -0.004857. Value loss: 0.158609. Entropy: 1.134339.\n",
      "Iteration 2499: Policy loss: -0.007126. Value loss: 0.123507. Entropy: 1.130139.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2500: Policy loss: -0.000732. Value loss: 0.382247. Entropy: 1.196813.\n",
      "Iteration 2501: Policy loss: -0.006127. Value loss: 0.214151. Entropy: 1.195137.\n",
      "Iteration 2502: Policy loss: -0.007899. Value loss: 0.138192. Entropy: 1.203820.\n",
      "now time :  2019-02-28 11:24:44.017484\n",
      "episode: 1201   score: 285.0  epsilon: 1.0    steps: 336  evaluation reward: 170.6\n",
      "episode: 1202   score: 185.0  epsilon: 1.0    steps: 376  evaluation reward: 170.65\n",
      "episode: 1203   score: 170.0  epsilon: 1.0    steps: 448  evaluation reward: 171.85\n",
      "episode: 1204   score: 180.0  epsilon: 1.0    steps: 1008  evaluation reward: 172.0\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2503: Policy loss: 0.005759. Value loss: 0.457680. Entropy: 1.211347.\n",
      "Iteration 2504: Policy loss: -0.008262. Value loss: 0.183776. Entropy: 1.208866.\n",
      "Iteration 2505: Policy loss: -0.010022. Value loss: 0.132872. Entropy: 1.224282.\n",
      "episode: 1205   score: 155.0  epsilon: 1.0    steps: 16  evaluation reward: 172.5\n",
      "episode: 1206   score: 125.0  epsilon: 1.0    steps: 432  evaluation reward: 171.4\n",
      "episode: 1207   score: 210.0  epsilon: 1.0    steps: 664  evaluation reward: 170.7\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2506: Policy loss: 0.003918. Value loss: 0.462056. Entropy: 1.020880.\n",
      "Iteration 2507: Policy loss: -0.004000. Value loss: 0.262922. Entropy: 1.036461.\n",
      "Iteration 2508: Policy loss: -0.005584. Value loss: 0.140020. Entropy: 1.025612.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2509: Policy loss: 0.001822. Value loss: 0.382662. Entropy: 1.066620.\n",
      "Iteration 2510: Policy loss: 0.000213. Value loss: 0.151159. Entropy: 1.091654.\n",
      "Iteration 2511: Policy loss: -0.007677. Value loss: 0.103514. Entropy: 1.075366.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2512: Policy loss: 0.001648. Value loss: 0.462312. Entropy: 1.218528.\n",
      "Iteration 2513: Policy loss: -0.011335. Value loss: 0.182217. Entropy: 1.219003.\n",
      "Iteration 2514: Policy loss: -0.008298. Value loss: 0.132577. Entropy: 1.227776.\n",
      "episode: 1208   score: 80.0  epsilon: 1.0    steps: 280  evaluation reward: 169.4\n",
      "episode: 1209   score: 105.0  epsilon: 1.0    steps: 776  evaluation reward: 167.3\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2515: Policy loss: 0.001674. Value loss: 0.472274. Entropy: 1.295713.\n",
      "Iteration 2516: Policy loss: -0.008440. Value loss: 0.204779. Entropy: 1.294457.\n",
      "Iteration 2517: Policy loss: -0.011896. Value loss: 0.135122. Entropy: 1.303324.\n",
      "episode: 1210   score: 260.0  epsilon: 1.0    steps: 352  evaluation reward: 168.6\n",
      "episode: 1211   score: 145.0  epsilon: 1.0    steps: 672  evaluation reward: 168.25\n",
      "episode: 1212   score: 125.0  epsilon: 1.0    steps: 752  evaluation reward: 167.3\n",
      "episode: 1213   score: 155.0  epsilon: 1.0    steps: 824  evaluation reward: 167.95\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2518: Policy loss: 0.003546. Value loss: 0.414752. Entropy: 1.152307.\n",
      "Iteration 2519: Policy loss: -0.001526. Value loss: 0.177958. Entropy: 1.150195.\n",
      "Iteration 2520: Policy loss: -0.004996. Value loss: 0.122745. Entropy: 1.147276.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2521: Policy loss: 0.004440. Value loss: 0.441040. Entropy: 0.968455.\n",
      "Iteration 2522: Policy loss: -0.001219. Value loss: 0.196722. Entropy: 0.961640.\n",
      "Iteration 2523: Policy loss: -0.006090. Value loss: 0.136661. Entropy: 0.966742.\n",
      "episode: 1214   score: 105.0  epsilon: 1.0    steps: 304  evaluation reward: 166.7\n",
      "episode: 1215   score: 225.0  epsilon: 1.0    steps: 480  evaluation reward: 167.35\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2524: Policy loss: 0.004492. Value loss: 0.511229. Entropy: 1.113754.\n",
      "Iteration 2525: Policy loss: -0.001686. Value loss: 0.193368. Entropy: 1.117559.\n",
      "Iteration 2526: Policy loss: -0.009329. Value loss: 0.144974. Entropy: 1.117579.\n",
      "episode: 1216   score: 185.0  epsilon: 1.0    steps: 112  evaluation reward: 167.55\n",
      "episode: 1217   score: 80.0  epsilon: 1.0    steps: 920  evaluation reward: 166.8\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2527: Policy loss: -0.002418. Value loss: 0.405125. Entropy: 1.139294.\n",
      "Iteration 2528: Policy loss: -0.007209. Value loss: 0.177702. Entropy: 1.159724.\n",
      "Iteration 2529: Policy loss: -0.013258. Value loss: 0.120976. Entropy: 1.149014.\n",
      "episode: 1218   score: 210.0  epsilon: 1.0    steps: 376  evaluation reward: 167.85\n",
      "episode: 1219   score: 65.0  epsilon: 1.0    steps: 576  evaluation reward: 167.55\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2530: Policy loss: 0.001467. Value loss: 0.574759. Entropy: 1.162872.\n",
      "Iteration 2531: Policy loss: 0.000396. Value loss: 0.203876. Entropy: 1.184770.\n",
      "Iteration 2532: Policy loss: -0.006239. Value loss: 0.124126. Entropy: 1.165512.\n",
      "episode: 1220   score: 210.0  epsilon: 1.0    steps: 376  evaluation reward: 168.45\n",
      "episode: 1221   score: 120.0  epsilon: 1.0    steps: 544  evaluation reward: 167.4\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2533: Policy loss: 0.003494. Value loss: 0.309716. Entropy: 1.109059.\n",
      "Iteration 2534: Policy loss: -0.003569. Value loss: 0.116267. Entropy: 1.098150.\n",
      "Iteration 2535: Policy loss: -0.006403. Value loss: 0.092472. Entropy: 1.094649.\n",
      "episode: 1222   score: 225.0  epsilon: 1.0    steps: 528  evaluation reward: 168.85\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2536: Policy loss: 0.005521. Value loss: 0.280627. Entropy: 1.100808.\n",
      "Iteration 2537: Policy loss: -0.004508. Value loss: 0.129257. Entropy: 1.102363.\n",
      "Iteration 2538: Policy loss: -0.006006. Value loss: 0.093801. Entropy: 1.116552.\n",
      "episode: 1223   score: 120.0  epsilon: 1.0    steps: 880  evaluation reward: 169.25\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2539: Policy loss: 0.001451. Value loss: 1.156120. Entropy: 1.144602.\n",
      "Iteration 2540: Policy loss: 0.005210. Value loss: 0.464821. Entropy: 1.143193.\n",
      "Iteration 2541: Policy loss: -0.000279. Value loss: 0.327063. Entropy: 1.132581.\n",
      "episode: 1224   score: 105.0  epsilon: 1.0    steps: 456  evaluation reward: 167.7\n",
      "episode: 1225   score: 250.0  epsilon: 1.0    steps: 488  evaluation reward: 167.95\n",
      "episode: 1226   score: 110.0  epsilon: 1.0    steps: 496  evaluation reward: 167.45\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2542: Policy loss: 0.000148. Value loss: 0.718691. Entropy: 1.184490.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2543: Policy loss: -0.001941. Value loss: 0.321496. Entropy: 1.170330.\n",
      "Iteration 2544: Policy loss: -0.009177. Value loss: 0.246094. Entropy: 1.177321.\n",
      "episode: 1227   score: 105.0  epsilon: 1.0    steps: 504  evaluation reward: 167.45\n",
      "episode: 1228   score: 320.0  epsilon: 1.0    steps: 864  evaluation reward: 167.95\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2545: Policy loss: 0.002650. Value loss: 0.425830. Entropy: 1.043717.\n",
      "Iteration 2546: Policy loss: -0.002223. Value loss: 0.170986. Entropy: 1.049377.\n",
      "Iteration 2547: Policy loss: -0.008015. Value loss: 0.119544. Entropy: 1.035729.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2548: Policy loss: 0.001697. Value loss: 0.515254. Entropy: 1.119382.\n",
      "Iteration 2549: Policy loss: -0.006086. Value loss: 0.174971. Entropy: 1.142637.\n",
      "Iteration 2550: Policy loss: -0.009060. Value loss: 0.163501. Entropy: 1.131822.\n",
      "episode: 1229   score: 210.0  epsilon: 1.0    steps: 208  evaluation reward: 165.85\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2551: Policy loss: 0.005843. Value loss: 0.685262. Entropy: 1.232776.\n",
      "Iteration 2552: Policy loss: -0.007787. Value loss: 0.406236. Entropy: 1.238791.\n",
      "Iteration 2553: Policy loss: -0.007694. Value loss: 0.249859. Entropy: 1.236331.\n",
      "episode: 1230   score: 20.0  epsilon: 1.0    steps: 320  evaluation reward: 165.2\n",
      "episode: 1231   score: 65.0  epsilon: 1.0    steps: 624  evaluation reward: 164.95\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2554: Policy loss: 0.002105. Value loss: 0.809540. Entropy: 1.226342.\n",
      "Iteration 2555: Policy loss: -0.003433. Value loss: 0.298202. Entropy: 1.238231.\n",
      "Iteration 2556: Policy loss: -0.003960. Value loss: 0.162091. Entropy: 1.235101.\n",
      "episode: 1232   score: 150.0  epsilon: 1.0    steps: 864  evaluation reward: 164.05\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2557: Policy loss: -0.001540. Value loss: 0.499726. Entropy: 1.118201.\n",
      "Iteration 2558: Policy loss: -0.007147. Value loss: 0.193587. Entropy: 1.129055.\n",
      "Iteration 2559: Policy loss: -0.004947. Value loss: 0.137133. Entropy: 1.120070.\n",
      "episode: 1233   score: 120.0  epsilon: 1.0    steps: 8  evaluation reward: 163.1\n",
      "episode: 1234   score: 135.0  epsilon: 1.0    steps: 32  evaluation reward: 163.65\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2560: Policy loss: 0.001731. Value loss: 0.369622. Entropy: 1.190395.\n",
      "Iteration 2561: Policy loss: -0.003561. Value loss: 0.162154. Entropy: 1.187735.\n",
      "Iteration 2562: Policy loss: -0.009683. Value loss: 0.130732. Entropy: 1.179910.\n",
      "episode: 1235   score: 440.0  epsilon: 1.0    steps: 136  evaluation reward: 166.7\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2563: Policy loss: 0.004209. Value loss: 0.306709. Entropy: 1.070818.\n",
      "Iteration 2564: Policy loss: 0.000466. Value loss: 0.178235. Entropy: 1.085084.\n",
      "Iteration 2565: Policy loss: -0.005038. Value loss: 0.117515. Entropy: 1.081960.\n",
      "episode: 1236   score: 150.0  epsilon: 1.0    steps: 152  evaluation reward: 164.65\n",
      "episode: 1237   score: 495.0  epsilon: 1.0    steps: 544  evaluation reward: 168.8\n",
      "episode: 1238   score: 155.0  epsilon: 1.0    steps: 664  evaluation reward: 168.8\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2566: Policy loss: 0.004108. Value loss: 0.377709. Entropy: 1.198599.\n",
      "Iteration 2567: Policy loss: -0.004505. Value loss: 0.189481. Entropy: 1.202670.\n",
      "Iteration 2568: Policy loss: -0.008793. Value loss: 0.139592. Entropy: 1.190629.\n",
      "episode: 1239   score: 125.0  epsilon: 1.0    steps: 232  evaluation reward: 168.85\n",
      "episode: 1240   score: 120.0  epsilon: 1.0    steps: 544  evaluation reward: 168.85\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2569: Policy loss: 0.000273. Value loss: 0.226438. Entropy: 1.052729.\n",
      "Iteration 2570: Policy loss: -0.005285. Value loss: 0.100458. Entropy: 1.099155.\n",
      "Iteration 2571: Policy loss: -0.009496. Value loss: 0.083186. Entropy: 1.066581.\n",
      "episode: 1241   score: 105.0  epsilon: 1.0    steps: 624  evaluation reward: 168.25\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2572: Policy loss: 0.000458. Value loss: 0.332407. Entropy: 1.059988.\n",
      "Iteration 2573: Policy loss: -0.001901. Value loss: 0.159258. Entropy: 1.082288.\n",
      "Iteration 2574: Policy loss: -0.008784. Value loss: 0.108486. Entropy: 1.076165.\n",
      "episode: 1242   score: 210.0  epsilon: 1.0    steps: 904  evaluation reward: 169.55\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2575: Policy loss: 0.000239. Value loss: 0.391812. Entropy: 1.157507.\n",
      "Iteration 2576: Policy loss: 0.000194. Value loss: 0.170294. Entropy: 1.155599.\n",
      "Iteration 2577: Policy loss: -0.007899. Value loss: 0.110357. Entropy: 1.162908.\n",
      "episode: 1243   score: 135.0  epsilon: 1.0    steps: 240  evaluation reward: 169.1\n",
      "episode: 1244   score: 120.0  epsilon: 1.0    steps: 560  evaluation reward: 168.75\n",
      "episode: 1245   score: 135.0  epsilon: 1.0    steps: 640  evaluation reward: 169.2\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2578: Policy loss: 0.002124. Value loss: 0.253200. Entropy: 1.175997.\n",
      "Iteration 2579: Policy loss: -0.003019. Value loss: 0.129080. Entropy: 1.159752.\n",
      "Iteration 2580: Policy loss: -0.008312. Value loss: 0.084510. Entropy: 1.163990.\n",
      "episode: 1246   score: 110.0  epsilon: 1.0    steps: 360  evaluation reward: 169.8\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2581: Policy loss: -0.001491. Value loss: 0.366160. Entropy: 0.826836.\n",
      "Iteration 2582: Policy loss: -0.005064. Value loss: 0.197136. Entropy: 0.815839.\n",
      "Iteration 2583: Policy loss: -0.008158. Value loss: 0.144553. Entropy: 0.838226.\n",
      "episode: 1247   score: 105.0  epsilon: 1.0    steps: 80  evaluation reward: 169.35\n",
      "episode: 1248   score: 125.0  epsilon: 1.0    steps: 544  evaluation reward: 169.25\n",
      "episode: 1249   score: 155.0  epsilon: 1.0    steps: 616  evaluation reward: 169.45\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2584: Policy loss: 0.000902. Value loss: 0.275591. Entropy: 1.073742.\n",
      "Iteration 2585: Policy loss: -0.001582. Value loss: 0.148911. Entropy: 1.084426.\n",
      "Iteration 2586: Policy loss: -0.003998. Value loss: 0.127038. Entropy: 1.097121.\n",
      "episode: 1250   score: 135.0  epsilon: 1.0    steps: 432  evaluation reward: 168.55\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2587: Policy loss: 0.002740. Value loss: 0.414362. Entropy: 1.016595.\n",
      "Iteration 2588: Policy loss: -0.002692. Value loss: 0.151744. Entropy: 0.993829.\n",
      "Iteration 2589: Policy loss: -0.005904. Value loss: 0.129282. Entropy: 1.006763.\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2590: Policy loss: 0.002026. Value loss: 0.378109. Entropy: 1.151094.\n",
      "Iteration 2591: Policy loss: -0.004735. Value loss: 0.208963. Entropy: 1.137781.\n",
      "Iteration 2592: Policy loss: -0.005769. Value loss: 0.139430. Entropy: 1.150444.\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2593: Policy loss: 0.000732. Value loss: 1.096652. Entropy: 1.262984.\n",
      "Iteration 2594: Policy loss: 0.001055. Value loss: 0.523275. Entropy: 1.268025.\n",
      "Iteration 2595: Policy loss: -0.002649. Value loss: 0.281189. Entropy: 1.262718.\n",
      "now time :  2019-02-28 11:25:51.186970\n",
      "episode: 1251   score: 385.0  epsilon: 1.0    steps: 32  evaluation reward: 170.6\n",
      "episode: 1252   score: 210.0  epsilon: 1.0    steps: 32  evaluation reward: 171.35\n",
      "episode: 1253   score: 80.0  epsilon: 1.0    steps: 768  evaluation reward: 170.9\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2596: Policy loss: 0.007479. Value loss: 0.843813. Entropy: 1.248945.\n",
      "Iteration 2597: Policy loss: -0.001230. Value loss: 0.303160. Entropy: 1.249617.\n",
      "Iteration 2598: Policy loss: -0.004350. Value loss: 0.169424. Entropy: 1.257124.\n",
      "episode: 1254   score: 290.0  epsilon: 1.0    steps: 488  evaluation reward: 170.7\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2599: Policy loss: 0.000666. Value loss: 0.678289. Entropy: 1.082777.\n",
      "Iteration 2600: Policy loss: -0.002978. Value loss: 0.296988. Entropy: 1.088522.\n",
      "Iteration 2601: Policy loss: -0.006880. Value loss: 0.165094. Entropy: 1.092672.\n",
      "episode: 1255   score: 210.0  epsilon: 1.0    steps: 800  evaluation reward: 172.15\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2602: Policy loss: -0.001047. Value loss: 0.844092. Entropy: 1.179226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2603: Policy loss: -0.001392. Value loss: 0.324416. Entropy: 1.201594.\n",
      "Iteration 2604: Policy loss: -0.013039. Value loss: 0.168627. Entropy: 1.216808.\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2605: Policy loss: 0.000144. Value loss: 0.254324. Entropy: 1.210647.\n",
      "Iteration 2606: Policy loss: -0.004454. Value loss: 0.113295. Entropy: 1.218742.\n",
      "Iteration 2607: Policy loss: -0.013054. Value loss: 0.101484. Entropy: 1.223998.\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2608: Policy loss: -0.001693. Value loss: 0.419016. Entropy: 1.233606.\n",
      "Iteration 2609: Policy loss: -0.007205. Value loss: 0.249823. Entropy: 1.217410.\n",
      "Iteration 2610: Policy loss: -0.008863. Value loss: 0.141290. Entropy: 1.219609.\n",
      "episode: 1256   score: 210.0  epsilon: 1.0    steps: 40  evaluation reward: 173.0\n",
      "episode: 1257   score: 120.0  epsilon: 1.0    steps: 520  evaluation reward: 170.4\n",
      "episode: 1258   score: 205.0  epsilon: 1.0    steps: 696  evaluation reward: 170.35\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2611: Policy loss: -0.000068. Value loss: 0.383278. Entropy: 1.266289.\n",
      "Iteration 2612: Policy loss: -0.004234. Value loss: 0.152129. Entropy: 1.264627.\n",
      "Iteration 2613: Policy loss: -0.011100. Value loss: 0.121515. Entropy: 1.260654.\n",
      "episode: 1259   score: 155.0  epsilon: 1.0    steps: 312  evaluation reward: 171.0\n",
      "episode: 1260   score: 120.0  epsilon: 1.0    steps: 648  evaluation reward: 170.1\n",
      "episode: 1261   score: 210.0  epsilon: 1.0    steps: 736  evaluation reward: 169.45\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2614: Policy loss: 0.000900. Value loss: 0.471372. Entropy: 1.060941.\n",
      "Iteration 2615: Policy loss: -0.004611. Value loss: 0.233942. Entropy: 1.056000.\n",
      "Iteration 2616: Policy loss: -0.007869. Value loss: 0.132677. Entropy: 1.050630.\n",
      "episode: 1262   score: 360.0  epsilon: 1.0    steps: 736  evaluation reward: 171.5\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2617: Policy loss: -0.002081. Value loss: 0.413061. Entropy: 0.926133.\n",
      "Iteration 2618: Policy loss: -0.004407. Value loss: 0.183992. Entropy: 0.947818.\n",
      "Iteration 2619: Policy loss: -0.009934. Value loss: 0.130302. Entropy: 0.908145.\n",
      "episode: 1263   score: 210.0  epsilon: 1.0    steps: 448  evaluation reward: 171.2\n",
      "episode: 1264   score: 105.0  epsilon: 1.0    steps: 520  evaluation reward: 171.2\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2620: Policy loss: 0.003212. Value loss: 0.329834. Entropy: 1.039020.\n",
      "Iteration 2621: Policy loss: 0.003092. Value loss: 0.211108. Entropy: 1.049147.\n",
      "Iteration 2622: Policy loss: -0.002087. Value loss: 0.132777. Entropy: 1.025023.\n",
      "episode: 1265   score: 125.0  epsilon: 1.0    steps: 512  evaluation reward: 170.65\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2623: Policy loss: 0.006454. Value loss: 0.370757. Entropy: 1.025266.\n",
      "Iteration 2624: Policy loss: -0.003132. Value loss: 0.142430. Entropy: 1.043748.\n",
      "Iteration 2625: Policy loss: -0.006818. Value loss: 0.104848. Entropy: 1.021015.\n",
      "episode: 1266   score: 210.0  epsilon: 1.0    steps: 272  evaluation reward: 169.6\n",
      "episode: 1267   score: 55.0  epsilon: 1.0    steps: 488  evaluation reward: 168.9\n",
      "episode: 1268   score: 210.0  epsilon: 1.0    steps: 800  evaluation reward: 169.2\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2626: Policy loss: -0.001494. Value loss: 0.368888. Entropy: 1.166126.\n",
      "Iteration 2627: Policy loss: 0.000245. Value loss: 0.169933. Entropy: 1.162456.\n",
      "Iteration 2628: Policy loss: -0.008231. Value loss: 0.143135. Entropy: 1.167541.\n",
      "episode: 1269   score: 180.0  epsilon: 1.0    steps: 776  evaluation reward: 168.9\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2629: Policy loss: 0.000894. Value loss: 0.352209. Entropy: 0.948722.\n",
      "Iteration 2630: Policy loss: -0.001807. Value loss: 0.182452. Entropy: 0.943433.\n",
      "Iteration 2631: Policy loss: -0.006773. Value loss: 0.138035. Entropy: 0.959026.\n",
      "episode: 1270   score: 100.0  epsilon: 1.0    steps: 376  evaluation reward: 166.75\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2632: Policy loss: -0.002512. Value loss: 0.375904. Entropy: 1.094141.\n",
      "Iteration 2633: Policy loss: -0.006288. Value loss: 0.164807. Entropy: 1.121904.\n",
      "Iteration 2634: Policy loss: -0.008502. Value loss: 0.124290. Entropy: 1.098907.\n",
      "episode: 1271   score: 210.0  epsilon: 1.0    steps: 624  evaluation reward: 167.65\n",
      "episode: 1272   score: 155.0  epsilon: 1.0    steps: 656  evaluation reward: 165.45\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2635: Policy loss: -0.000277. Value loss: 0.333614. Entropy: 1.127153.\n",
      "Iteration 2636: Policy loss: -0.003594. Value loss: 0.171530. Entropy: 1.117535.\n",
      "Iteration 2637: Policy loss: -0.008110. Value loss: 0.104470. Entropy: 1.129504.\n",
      "episode: 1273   score: 125.0  epsilon: 1.0    steps: 800  evaluation reward: 165.65\n",
      "episode: 1274   score: 120.0  epsilon: 1.0    steps: 936  evaluation reward: 165.3\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2638: Policy loss: 0.002882. Value loss: 0.824327. Entropy: 1.067687.\n",
      "Iteration 2639: Policy loss: 0.003858. Value loss: 0.320070. Entropy: 1.057372.\n",
      "Iteration 2640: Policy loss: -0.001792. Value loss: 0.195189. Entropy: 1.045534.\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2641: Policy loss: 0.007949. Value loss: 0.838339. Entropy: 1.090269.\n",
      "Iteration 2642: Policy loss: 0.002637. Value loss: 0.373481. Entropy: 1.098185.\n",
      "Iteration 2643: Policy loss: -0.006770. Value loss: 0.208955. Entropy: 1.105613.\n",
      "episode: 1275   score: 180.0  epsilon: 1.0    steps: 432  evaluation reward: 164.65\n",
      "episode: 1276   score: 155.0  epsilon: 1.0    steps: 672  evaluation reward: 164.9\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2644: Policy loss: 0.002785. Value loss: 0.875840. Entropy: 1.170769.\n",
      "Iteration 2645: Policy loss: -0.002333. Value loss: 0.296681. Entropy: 1.165533.\n",
      "Iteration 2646: Policy loss: -0.004180. Value loss: 0.188097. Entropy: 1.180192.\n",
      "episode: 1277   score: 460.0  epsilon: 1.0    steps: 40  evaluation reward: 168.85\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2647: Policy loss: 0.002764. Value loss: 0.663542. Entropy: 1.035160.\n",
      "Iteration 2648: Policy loss: 0.001795. Value loss: 0.280625. Entropy: 1.027419.\n",
      "Iteration 2649: Policy loss: -0.004447. Value loss: 0.206168. Entropy: 1.044832.\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2650: Policy loss: 0.004291. Value loss: 0.521422. Entropy: 1.167614.\n",
      "Iteration 2651: Policy loss: 0.001835. Value loss: 0.277316. Entropy: 1.168706.\n",
      "Iteration 2652: Policy loss: -0.003473. Value loss: 0.179352. Entropy: 1.175112.\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2653: Policy loss: -0.000365. Value loss: 0.553987. Entropy: 1.296843.\n",
      "Iteration 2654: Policy loss: 0.000199. Value loss: 0.267292. Entropy: 1.290486.\n",
      "Iteration 2655: Policy loss: -0.006800. Value loss: 0.156608. Entropy: 1.294688.\n",
      "episode: 1278   score: 140.0  epsilon: 1.0    steps: 608  evaluation reward: 167.7\n",
      "episode: 1279   score: 540.0  epsilon: 1.0    steps: 712  evaluation reward: 171.75\n",
      "episode: 1280   score: 210.0  epsilon: 1.0    steps: 912  evaluation reward: 173.55\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2656: Policy loss: 0.002202. Value loss: 0.792501. Entropy: 1.301868.\n",
      "Iteration 2657: Policy loss: 0.003025. Value loss: 0.292330. Entropy: 1.295072.\n",
      "Iteration 2658: Policy loss: -0.001202. Value loss: 0.194515. Entropy: 1.292806.\n",
      "episode: 1281   score: 255.0  epsilon: 1.0    steps: 640  evaluation reward: 173.25\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2659: Policy loss: -0.001687. Value loss: 0.172958. Entropy: 1.069658.\n",
      "Iteration 2660: Policy loss: -0.003780. Value loss: 0.107325. Entropy: 1.058658.\n",
      "Iteration 2661: Policy loss: -0.007192. Value loss: 0.089710. Entropy: 1.063778.\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2662: Policy loss: -0.003480. Value loss: 0.382209. Entropy: 1.082052.\n",
      "Iteration 2663: Policy loss: -0.007553. Value loss: 0.175252. Entropy: 1.090315.\n",
      "Iteration 2664: Policy loss: -0.011784. Value loss: 0.131748. Entropy: 1.094606.\n",
      "episode: 1282   score: 240.0  epsilon: 1.0    steps: 144  evaluation reward: 174.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1283   score: 215.0  epsilon: 1.0    steps: 208  evaluation reward: 172.5\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2665: Policy loss: -0.001740. Value loss: 0.400436. Entropy: 1.260533.\n",
      "Iteration 2666: Policy loss: -0.004091. Value loss: 0.185406. Entropy: 1.263634.\n",
      "Iteration 2667: Policy loss: -0.009314. Value loss: 0.107271. Entropy: 1.264649.\n",
      "episode: 1284   score: 90.0  epsilon: 1.0    steps: 72  evaluation reward: 172.9\n",
      "episode: 1285   score: 270.0  epsilon: 1.0    steps: 864  evaluation reward: 173.85\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2668: Policy loss: 0.002489. Value loss: 0.293182. Entropy: 1.031769.\n",
      "Iteration 2669: Policy loss: -0.000012. Value loss: 0.125691. Entropy: 1.048906.\n",
      "Iteration 2670: Policy loss: -0.006235. Value loss: 0.106757. Entropy: 1.050905.\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2671: Policy loss: 0.002361. Value loss: 0.263088. Entropy: 1.146973.\n",
      "Iteration 2672: Policy loss: -0.004291. Value loss: 0.155170. Entropy: 1.137689.\n",
      "Iteration 2673: Policy loss: -0.008494. Value loss: 0.101052. Entropy: 1.154383.\n",
      "episode: 1286   score: 210.0  epsilon: 1.0    steps: 656  evaluation reward: 174.15\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2674: Policy loss: 0.006787. Value loss: 0.392552. Entropy: 1.192731.\n",
      "Iteration 2675: Policy loss: 0.000207. Value loss: 0.159091. Entropy: 1.179967.\n",
      "Iteration 2676: Policy loss: -0.004676. Value loss: 0.109149. Entropy: 1.187773.\n",
      "episode: 1287   score: 210.0  epsilon: 1.0    steps: 32  evaluation reward: 175.05\n",
      "episode: 1288   score: 210.0  epsilon: 1.0    steps: 920  evaluation reward: 176.3\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2677: Policy loss: 0.000086. Value loss: 0.336239. Entropy: 1.123852.\n",
      "Iteration 2678: Policy loss: -0.002290. Value loss: 0.158395. Entropy: 1.092747.\n",
      "Iteration 2679: Policy loss: -0.008840. Value loss: 0.112691. Entropy: 1.116566.\n",
      "episode: 1289   score: 310.0  epsilon: 1.0    steps: 264  evaluation reward: 178.3\n",
      "episode: 1290   score: 210.0  epsilon: 1.0    steps: 352  evaluation reward: 178.85\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2680: Policy loss: -0.001799. Value loss: 0.318600. Entropy: 1.061547.\n",
      "Iteration 2681: Policy loss: -0.001127. Value loss: 0.175498. Entropy: 1.067997.\n",
      "Iteration 2682: Policy loss: -0.005198. Value loss: 0.106748. Entropy: 1.061957.\n",
      "episode: 1291   score: 180.0  epsilon: 1.0    steps: 688  evaluation reward: 179.3\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2683: Policy loss: 0.001073. Value loss: 0.190959. Entropy: 0.931920.\n",
      "Iteration 2684: Policy loss: -0.001210. Value loss: 0.082580. Entropy: 0.948728.\n",
      "Iteration 2685: Policy loss: -0.007200. Value loss: 0.056151. Entropy: 0.939272.\n",
      "episode: 1292   score: 210.0  epsilon: 1.0    steps: 16  evaluation reward: 179.6\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2686: Policy loss: 0.005861. Value loss: 0.304820. Entropy: 1.070835.\n",
      "Iteration 2687: Policy loss: 0.003341. Value loss: 0.120832. Entropy: 1.067559.\n",
      "Iteration 2688: Policy loss: -0.001620. Value loss: 0.085117. Entropy: 1.067346.\n",
      "episode: 1293   score: 105.0  epsilon: 1.0    steps: 272  evaluation reward: 179.1\n",
      "episode: 1294   score: 105.0  epsilon: 1.0    steps: 352  evaluation reward: 178.35\n",
      "episode: 1295   score: 215.0  epsilon: 1.0    steps: 864  evaluation reward: 178.7\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2689: Policy loss: -0.001029. Value loss: 0.538245. Entropy: 1.130023.\n",
      "Iteration 2690: Policy loss: -0.005137. Value loss: 0.226172. Entropy: 1.127569.\n",
      "Iteration 2691: Policy loss: -0.010687. Value loss: 0.223053. Entropy: 1.127890.\n",
      "episode: 1296   score: 210.0  epsilon: 1.0    steps: 136  evaluation reward: 178.7\n",
      "episode: 1297   score: 180.0  epsilon: 1.0    steps: 568  evaluation reward: 178.25\n",
      "episode: 1298   score: 100.0  epsilon: 1.0    steps: 952  evaluation reward: 178.5\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2692: Policy loss: -0.000288. Value loss: 0.270949. Entropy: 0.964462.\n",
      "Iteration 2693: Policy loss: -0.002470. Value loss: 0.136847. Entropy: 0.913745.\n",
      "Iteration 2694: Policy loss: 0.000862. Value loss: 0.081071. Entropy: 0.930376.\n",
      "episode: 1299   score: 85.0  epsilon: 1.0    steps: 64  evaluation reward: 178.7\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2695: Policy loss: -0.000080. Value loss: 0.412715. Entropy: 0.968731.\n",
      "Iteration 2696: Policy loss: -0.003757. Value loss: 0.174906. Entropy: 0.988878.\n",
      "Iteration 2697: Policy loss: -0.006637. Value loss: 0.115408. Entropy: 0.977767.\n",
      "episode: 1300   score: 210.0  epsilon: 1.0    steps: 960  evaluation reward: 180.0\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2698: Policy loss: 0.001915. Value loss: 0.497089. Entropy: 0.910991.\n",
      "Iteration 2699: Policy loss: -0.000362. Value loss: 0.197868. Entropy: 0.910387.\n",
      "Iteration 2700: Policy loss: -0.001583. Value loss: 0.153102. Entropy: 0.928073.\n",
      "now time :  2019-02-28 11:27:07.863734\n",
      "episode: 1301   score: 65.0  epsilon: 1.0    steps: 536  evaluation reward: 177.8\n",
      "episode: 1302   score: 155.0  epsilon: 1.0    steps: 976  evaluation reward: 177.5\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2701: Policy loss: 0.001471. Value loss: 0.466281. Entropy: 1.240098.\n",
      "Iteration 2702: Policy loss: -0.004595. Value loss: 0.213939. Entropy: 1.242099.\n",
      "Iteration 2703: Policy loss: -0.006909. Value loss: 0.146627. Entropy: 1.234997.\n",
      "episode: 1303   score: 210.0  epsilon: 1.0    steps: 1008  evaluation reward: 177.9\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2704: Policy loss: -0.002073. Value loss: 0.585753. Entropy: 1.084539.\n",
      "Iteration 2705: Policy loss: -0.005237. Value loss: 0.263966. Entropy: 1.081437.\n",
      "Iteration 2706: Policy loss: -0.006496. Value loss: 0.201954. Entropy: 1.092540.\n",
      "episode: 1304   score: 215.0  epsilon: 1.0    steps: 472  evaluation reward: 178.25\n",
      "episode: 1305   score: 195.0  epsilon: 1.0    steps: 1016  evaluation reward: 178.65\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2707: Policy loss: 0.002475. Value loss: 0.498425. Entropy: 1.212822.\n",
      "Iteration 2708: Policy loss: -0.001759. Value loss: 0.197743. Entropy: 1.208130.\n",
      "Iteration 2709: Policy loss: -0.006092. Value loss: 0.114833. Entropy: 1.201168.\n",
      "episode: 1306   score: 210.0  epsilon: 1.0    steps: 144  evaluation reward: 179.5\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2710: Policy loss: 0.001422. Value loss: 0.743198. Entropy: 1.079026.\n",
      "Iteration 2711: Policy loss: -0.007166. Value loss: 0.587394. Entropy: 1.071783.\n",
      "Iteration 2712: Policy loss: -0.006626. Value loss: 0.316519. Entropy: 1.070500.\n",
      "episode: 1307   score: 120.0  epsilon: 1.0    steps: 608  evaluation reward: 178.6\n",
      "episode: 1308   score: 105.0  epsilon: 1.0    steps: 896  evaluation reward: 178.85\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2713: Policy loss: 0.000953. Value loss: 0.489016. Entropy: 1.067755.\n",
      "Iteration 2714: Policy loss: -0.004550. Value loss: 0.225123. Entropy: 1.088449.\n",
      "Iteration 2715: Policy loss: -0.007005. Value loss: 0.141720. Entropy: 1.085588.\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2716: Policy loss: -0.000364. Value loss: 0.393990. Entropy: 1.153567.\n",
      "Iteration 2717: Policy loss: -0.007088. Value loss: 0.189628. Entropy: 1.147500.\n",
      "Iteration 2718: Policy loss: -0.015442. Value loss: 0.135139. Entropy: 1.153016.\n",
      "episode: 1309   score: 410.0  epsilon: 1.0    steps: 408  evaluation reward: 181.9\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2719: Policy loss: 0.005458. Value loss: 0.741271. Entropy: 1.165557.\n",
      "Iteration 2720: Policy loss: -0.000451. Value loss: 0.272343. Entropy: 1.155696.\n",
      "Iteration 2721: Policy loss: -0.005922. Value loss: 0.169762. Entropy: 1.174697.\n",
      "episode: 1310   score: 135.0  epsilon: 1.0    steps: 176  evaluation reward: 180.65\n",
      "episode: 1311   score: 180.0  epsilon: 1.0    steps: 952  evaluation reward: 181.0\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2722: Policy loss: 0.002062. Value loss: 0.510390. Entropy: 1.214611.\n",
      "Iteration 2723: Policy loss: 0.001486. Value loss: 0.214150. Entropy: 1.202729.\n",
      "Iteration 2724: Policy loss: -0.011364. Value loss: 0.181219. Entropy: 1.215806.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1312   score: 210.0  epsilon: 1.0    steps: 648  evaluation reward: 181.85\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2725: Policy loss: -0.001785. Value loss: 0.522143. Entropy: 0.987743.\n",
      "Iteration 2726: Policy loss: -0.000821. Value loss: 0.261085. Entropy: 0.996644.\n",
      "Iteration 2727: Policy loss: -0.004758. Value loss: 0.178424. Entropy: 0.991958.\n",
      "episode: 1313   score: 300.0  epsilon: 1.0    steps: 48  evaluation reward: 183.3\n",
      "episode: 1314   score: 75.0  epsilon: 1.0    steps: 224  evaluation reward: 183.0\n",
      "episode: 1315   score: 210.0  epsilon: 1.0    steps: 648  evaluation reward: 182.85\n",
      "episode: 1316   score: 240.0  epsilon: 1.0    steps: 880  evaluation reward: 183.4\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2728: Policy loss: 0.002012. Value loss: 0.427513. Entropy: 0.980130.\n",
      "Iteration 2729: Policy loss: -0.002068. Value loss: 0.224996. Entropy: 0.976731.\n",
      "Iteration 2730: Policy loss: 0.002252. Value loss: 0.182339. Entropy: 0.981935.\n",
      "episode: 1317   score: 105.0  epsilon: 1.0    steps: 128  evaluation reward: 183.65\n",
      "episode: 1318   score: 215.0  epsilon: 1.0    steps: 232  evaluation reward: 183.7\n",
      "episode: 1319   score: 125.0  epsilon: 1.0    steps: 840  evaluation reward: 184.3\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2731: Policy loss: 0.002988. Value loss: 0.545337. Entropy: 0.731982.\n",
      "Iteration 2732: Policy loss: -0.003517. Value loss: 0.214172. Entropy: 0.742769.\n",
      "Iteration 2733: Policy loss: -0.005006. Value loss: 0.145580. Entropy: 0.732012.\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2734: Policy loss: 0.001441. Value loss: 0.404187. Entropy: 0.781305.\n",
      "Iteration 2735: Policy loss: -0.006105. Value loss: 0.195484. Entropy: 0.799971.\n",
      "Iteration 2736: Policy loss: -0.006838. Value loss: 0.143717. Entropy: 0.817959.\n",
      "episode: 1320   score: 135.0  epsilon: 1.0    steps: 400  evaluation reward: 183.55\n",
      "episode: 1321   score: 130.0  epsilon: 1.0    steps: 656  evaluation reward: 183.65\n",
      "episode: 1322   score: 125.0  epsilon: 1.0    steps: 968  evaluation reward: 182.65\n",
      "episode: 1323   score: 125.0  epsilon: 1.0    steps: 1008  evaluation reward: 182.7\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2737: Policy loss: 0.001476. Value loss: 0.447885. Entropy: 1.196379.\n",
      "Iteration 2738: Policy loss: -0.001398. Value loss: 0.247314. Entropy: 1.215025.\n",
      "Iteration 2739: Policy loss: -0.007494. Value loss: 0.177910. Entropy: 1.214824.\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2740: Policy loss: 0.002994. Value loss: 0.454222. Entropy: 1.009632.\n",
      "Iteration 2741: Policy loss: -0.003777. Value loss: 0.174990. Entropy: 0.998104.\n",
      "Iteration 2742: Policy loss: -0.006999. Value loss: 0.131519. Entropy: 0.985707.\n",
      "episode: 1324   score: 155.0  epsilon: 1.0    steps: 112  evaluation reward: 183.2\n",
      "episode: 1325   score: 120.0  epsilon: 1.0    steps: 336  evaluation reward: 181.9\n",
      "episode: 1326   score: 210.0  epsilon: 1.0    steps: 920  evaluation reward: 182.9\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2743: Policy loss: -0.000256. Value loss: 0.302091. Entropy: 0.958138.\n",
      "Iteration 2744: Policy loss: -0.004230. Value loss: 0.122417. Entropy: 0.956581.\n",
      "Iteration 2745: Policy loss: -0.010124. Value loss: 0.097842. Entropy: 0.952127.\n",
      "episode: 1327   score: 20.0  epsilon: 1.0    steps: 920  evaluation reward: 182.05\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2746: Policy loss: -0.000049. Value loss: 0.323736. Entropy: 1.050333.\n",
      "Iteration 2747: Policy loss: -0.003912. Value loss: 0.165695. Entropy: 1.070953.\n",
      "Iteration 2748: Policy loss: -0.006768. Value loss: 0.134146. Entropy: 1.046407.\n",
      "episode: 1328   score: 165.0  epsilon: 1.0    steps: 672  evaluation reward: 180.5\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2749: Policy loss: 0.000154. Value loss: 0.399751. Entropy: 1.155310.\n",
      "Iteration 2750: Policy loss: -0.001858. Value loss: 0.150908. Entropy: 1.177593.\n",
      "Iteration 2751: Policy loss: -0.007309. Value loss: 0.100451. Entropy: 1.151341.\n",
      "episode: 1329   score: 240.0  epsilon: 1.0    steps: 928  evaluation reward: 180.8\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2752: Policy loss: 0.000468. Value loss: 0.239517. Entropy: 1.036156.\n",
      "Iteration 2753: Policy loss: -0.003212. Value loss: 0.123636. Entropy: 1.055367.\n",
      "Iteration 2754: Policy loss: -0.010459. Value loss: 0.096011. Entropy: 1.056422.\n",
      "episode: 1330   score: 215.0  epsilon: 1.0    steps: 472  evaluation reward: 182.75\n",
      "episode: 1331   score: 135.0  epsilon: 1.0    steps: 952  evaluation reward: 183.45\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2755: Policy loss: 0.004038. Value loss: 0.918159. Entropy: 1.121418.\n",
      "Iteration 2756: Policy loss: 0.003924. Value loss: 0.324945. Entropy: 1.137304.\n",
      "Iteration 2757: Policy loss: -0.001398. Value loss: 0.255040. Entropy: 1.144785.\n",
      "episode: 1332   score: 105.0  epsilon: 1.0    steps: 328  evaluation reward: 183.0\n",
      "episode: 1333   score: 440.0  epsilon: 1.0    steps: 552  evaluation reward: 186.2\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2758: Policy loss: 0.001507. Value loss: 0.628916. Entropy: 0.953735.\n",
      "Iteration 2759: Policy loss: -0.004024. Value loss: 0.329074. Entropy: 0.963385.\n",
      "Iteration 2760: Policy loss: -0.005639. Value loss: 0.219337. Entropy: 0.959052.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2761: Policy loss: 0.004214. Value loss: 0.500986. Entropy: 0.921015.\n",
      "Iteration 2762: Policy loss: -0.003993. Value loss: 0.201909. Entropy: 0.931305.\n",
      "Iteration 2763: Policy loss: -0.003518. Value loss: 0.150297. Entropy: 0.952188.\n",
      "episode: 1334   score: 255.0  epsilon: 1.0    steps: 208  evaluation reward: 187.4\n",
      "episode: 1335   score: 210.0  epsilon: 1.0    steps: 728  evaluation reward: 185.1\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2764: Policy loss: 0.006424. Value loss: 0.444617. Entropy: 1.152542.\n",
      "Iteration 2765: Policy loss: -0.005345. Value loss: 0.175478. Entropy: 1.132922.\n",
      "Iteration 2766: Policy loss: -0.008166. Value loss: 0.141454. Entropy: 1.147227.\n",
      "episode: 1336   score: 180.0  epsilon: 1.0    steps: 120  evaluation reward: 185.4\n",
      "episode: 1337   score: 155.0  epsilon: 1.0    steps: 176  evaluation reward: 182.0\n",
      "episode: 1338   score: 105.0  epsilon: 1.0    steps: 624  evaluation reward: 181.5\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2767: Policy loss: 0.001342. Value loss: 0.429402. Entropy: 0.994781.\n",
      "Iteration 2768: Policy loss: 0.001300. Value loss: 0.201290. Entropy: 1.016361.\n",
      "Iteration 2769: Policy loss: -0.007288. Value loss: 0.119916. Entropy: 1.014717.\n",
      "episode: 1339   score: 155.0  epsilon: 1.0    steps: 312  evaluation reward: 181.8\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2770: Policy loss: -0.002953. Value loss: 0.366285. Entropy: 0.938723.\n",
      "Iteration 2771: Policy loss: -0.004884. Value loss: 0.216976. Entropy: 0.920208.\n",
      "Iteration 2772: Policy loss: -0.011666. Value loss: 0.116325. Entropy: 0.941780.\n",
      "episode: 1340   score: 315.0  epsilon: 1.0    steps: 24  evaluation reward: 183.75\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2773: Policy loss: 0.001762. Value loss: 0.517253. Entropy: 1.105415.\n",
      "Iteration 2774: Policy loss: -0.003970. Value loss: 0.285587. Entropy: 1.107554.\n",
      "Iteration 2775: Policy loss: -0.005462. Value loss: 0.188574. Entropy: 1.103833.\n",
      "episode: 1341   score: 155.0  epsilon: 1.0    steps: 552  evaluation reward: 184.25\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2776: Policy loss: 0.001491. Value loss: 0.343858. Entropy: 1.202340.\n",
      "Iteration 2777: Policy loss: -0.006136. Value loss: 0.161835. Entropy: 1.194846.\n",
      "Iteration 2778: Policy loss: -0.011424. Value loss: 0.131731. Entropy: 1.192339.\n",
      "episode: 1342   score: 210.0  epsilon: 1.0    steps: 528  evaluation reward: 184.25\n",
      "episode: 1343   score: 180.0  epsilon: 1.0    steps: 704  evaluation reward: 184.7\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2779: Policy loss: 0.006410. Value loss: 0.504973. Entropy: 1.183670.\n",
      "Iteration 2780: Policy loss: -0.004098. Value loss: 0.182437. Entropy: 1.193965.\n",
      "Iteration 2781: Policy loss: -0.005802. Value loss: 0.125000. Entropy: 1.192719.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2782: Policy loss: 0.002243. Value loss: 0.858271. Entropy: 1.014449.\n",
      "Iteration 2783: Policy loss: -0.001711. Value loss: 0.302477. Entropy: 1.034239.\n",
      "Iteration 2784: Policy loss: -0.001665. Value loss: 0.214917. Entropy: 1.041187.\n",
      "episode: 1344   score: 245.0  epsilon: 1.0    steps: 1024  evaluation reward: 185.95\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2785: Policy loss: -0.000023. Value loss: 0.506636. Entropy: 1.170026.\n",
      "Iteration 2786: Policy loss: -0.002854. Value loss: 0.208397. Entropy: 1.186180.\n",
      "Iteration 2787: Policy loss: -0.006462. Value loss: 0.167804. Entropy: 1.174232.\n",
      "episode: 1345   score: 180.0  epsilon: 1.0    steps: 104  evaluation reward: 186.4\n",
      "episode: 1346   score: 90.0  epsilon: 1.0    steps: 520  evaluation reward: 186.2\n",
      "episode: 1347   score: 150.0  epsilon: 1.0    steps: 624  evaluation reward: 186.65\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2788: Policy loss: 0.002551. Value loss: 0.604595. Entropy: 1.225888.\n",
      "Iteration 2789: Policy loss: 0.003530. Value loss: 0.276955. Entropy: 1.231125.\n",
      "Iteration 2790: Policy loss: -0.002581. Value loss: 0.136028. Entropy: 1.231943.\n",
      "episode: 1348   score: 440.0  epsilon: 1.0    steps: 248  evaluation reward: 189.8\n",
      "episode: 1349   score: 245.0  epsilon: 1.0    steps: 824  evaluation reward: 190.7\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2791: Policy loss: 0.000178. Value loss: 0.252643. Entropy: 0.932215.\n",
      "Iteration 2792: Policy loss: -0.002276. Value loss: 0.143104. Entropy: 0.918306.\n",
      "Iteration 2793: Policy loss: -0.004593. Value loss: 0.116949. Entropy: 0.929411.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2794: Policy loss: 0.002329. Value loss: 0.989477. Entropy: 1.093324.\n",
      "Iteration 2795: Policy loss: -0.003077. Value loss: 0.333519. Entropy: 1.085716.\n",
      "Iteration 2796: Policy loss: -0.005650. Value loss: 0.284740. Entropy: 1.088828.\n",
      "episode: 1350   score: 20.0  epsilon: 1.0    steps: 304  evaluation reward: 189.55\n",
      "now time :  2019-02-28 11:28:17.707855\n",
      "episode: 1351   score: 180.0  epsilon: 1.0    steps: 696  evaluation reward: 187.5\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2797: Policy loss: 0.006179. Value loss: 0.605572. Entropy: 1.167223.\n",
      "Iteration 2798: Policy loss: 0.002468. Value loss: 0.232074. Entropy: 1.197050.\n",
      "Iteration 2799: Policy loss: -0.004018. Value loss: 0.141165. Entropy: 1.183849.\n",
      "episode: 1352   score: 440.0  epsilon: 1.0    steps: 248  evaluation reward: 189.8\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2800: Policy loss: 0.002633. Value loss: 0.401584. Entropy: 1.091953.\n",
      "Iteration 2801: Policy loss: 0.003022. Value loss: 0.210008. Entropy: 1.085589.\n",
      "Iteration 2802: Policy loss: -0.005154. Value loss: 0.127269. Entropy: 1.099562.\n",
      "episode: 1353   score: 430.0  epsilon: 1.0    steps: 776  evaluation reward: 193.3\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2803: Policy loss: -0.000390. Value loss: 0.344733. Entropy: 1.105585.\n",
      "Iteration 2804: Policy loss: 0.002815. Value loss: 0.137157. Entropy: 1.118748.\n",
      "Iteration 2805: Policy loss: -0.002861. Value loss: 0.113494. Entropy: 1.116272.\n",
      "episode: 1354   score: 115.0  epsilon: 1.0    steps: 144  evaluation reward: 191.55\n",
      "episode: 1355   score: 240.0  epsilon: 1.0    steps: 752  evaluation reward: 191.85\n",
      "episode: 1356   score: 210.0  epsilon: 1.0    steps: 952  evaluation reward: 191.85\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2806: Policy loss: 0.002078. Value loss: 0.570024. Entropy: 1.167298.\n",
      "Iteration 2807: Policy loss: -0.001341. Value loss: 0.288718. Entropy: 1.166055.\n",
      "Iteration 2808: Policy loss: -0.007295. Value loss: 0.171528. Entropy: 1.174985.\n",
      "episode: 1357   score: 135.0  epsilon: 1.0    steps: 272  evaluation reward: 192.0\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2809: Policy loss: -0.000883. Value loss: 0.347054. Entropy: 0.933208.\n",
      "Iteration 2810: Policy loss: -0.000548. Value loss: 0.212265. Entropy: 0.960692.\n",
      "Iteration 2811: Policy loss: -0.008759. Value loss: 0.132849. Entropy: 0.952422.\n",
      "episode: 1358   score: 180.0  epsilon: 1.0    steps: 344  evaluation reward: 191.75\n",
      "episode: 1359   score: 260.0  epsilon: 1.0    steps: 520  evaluation reward: 192.8\n",
      "episode: 1360   score: 105.0  epsilon: 1.0    steps: 824  evaluation reward: 192.65\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2812: Policy loss: 0.008103. Value loss: 0.333714. Entropy: 1.094329.\n",
      "Iteration 2813: Policy loss: 0.002343. Value loss: 0.197943. Entropy: 1.113216.\n",
      "Iteration 2814: Policy loss: -0.001974. Value loss: 0.121223. Entropy: 1.118625.\n",
      "episode: 1361   score: 105.0  epsilon: 1.0    steps: 88  evaluation reward: 191.6\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2815: Policy loss: 0.003325. Value loss: 0.289246. Entropy: 0.950053.\n",
      "Iteration 2816: Policy loss: -0.003026. Value loss: 0.171617. Entropy: 0.978872.\n",
      "Iteration 2817: Policy loss: -0.005590. Value loss: 0.118400. Entropy: 0.969307.\n",
      "episode: 1362   score: 155.0  epsilon: 1.0    steps: 648  evaluation reward: 189.55\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2818: Policy loss: 0.003389. Value loss: 0.311171. Entropy: 1.104760.\n",
      "Iteration 2819: Policy loss: -0.009092. Value loss: 0.171696. Entropy: 1.102128.\n",
      "Iteration 2820: Policy loss: -0.010975. Value loss: 0.143485. Entropy: 1.094231.\n",
      "episode: 1363   score: 80.0  epsilon: 1.0    steps: 400  evaluation reward: 188.25\n",
      "episode: 1364   score: 210.0  epsilon: 1.0    steps: 536  evaluation reward: 189.3\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2821: Policy loss: -0.000284. Value loss: 0.544279. Entropy: 1.197720.\n",
      "Iteration 2822: Policy loss: -0.007696. Value loss: 0.237864. Entropy: 1.201329.\n",
      "Iteration 2823: Policy loss: -0.013835. Value loss: 0.148434. Entropy: 1.203818.\n",
      "episode: 1365   score: 155.0  epsilon: 1.0    steps: 344  evaluation reward: 189.6\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2824: Policy loss: 0.003746. Value loss: 0.440192. Entropy: 1.027525.\n",
      "Iteration 2825: Policy loss: -0.001561. Value loss: 0.217757. Entropy: 1.036810.\n",
      "Iteration 2826: Policy loss: -0.003379. Value loss: 0.164107. Entropy: 1.033617.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2827: Policy loss: -0.002424. Value loss: 0.452430. Entropy: 1.142895.\n",
      "Iteration 2828: Policy loss: -0.003869. Value loss: 0.183962. Entropy: 1.115921.\n",
      "Iteration 2829: Policy loss: -0.013633. Value loss: 0.117065. Entropy: 1.134273.\n",
      "episode: 1366   score: 180.0  epsilon: 1.0    steps: 360  evaluation reward: 189.3\n",
      "episode: 1367   score: 120.0  epsilon: 1.0    steps: 752  evaluation reward: 189.95\n",
      "episode: 1368   score: 340.0  epsilon: 1.0    steps: 960  evaluation reward: 191.25\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2830: Policy loss: 0.001821. Value loss: 0.669627. Entropy: 1.243861.\n",
      "Iteration 2831: Policy loss: -0.001295. Value loss: 0.235540. Entropy: 1.237678.\n",
      "Iteration 2832: Policy loss: -0.000562. Value loss: 0.121524. Entropy: 1.234594.\n",
      "episode: 1369   score: 285.0  epsilon: 1.0    steps: 208  evaluation reward: 192.3\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2833: Policy loss: 0.001975. Value loss: 0.433387. Entropy: 1.033898.\n",
      "Iteration 2834: Policy loss: -0.002210. Value loss: 0.177883. Entropy: 1.036566.\n",
      "Iteration 2835: Policy loss: -0.006717. Value loss: 0.122857. Entropy: 1.036397.\n",
      "episode: 1370   score: 210.0  epsilon: 1.0    steps: 552  evaluation reward: 193.4\n",
      "episode: 1371   score: 165.0  epsilon: 1.0    steps: 792  evaluation reward: 192.95\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2836: Policy loss: 0.005631. Value loss: 0.495710. Entropy: 1.000419.\n",
      "Iteration 2837: Policy loss: -0.003588. Value loss: 0.147716. Entropy: 0.991864.\n",
      "Iteration 2838: Policy loss: -0.005825. Value loss: 0.107087. Entropy: 1.019291.\n",
      "episode: 1372   score: 210.0  epsilon: 1.0    steps: 512  evaluation reward: 193.5\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2839: Policy loss: -0.000682. Value loss: 0.389519. Entropy: 1.081334.\n",
      "Iteration 2840: Policy loss: 0.001175. Value loss: 0.153299. Entropy: 1.087315.\n",
      "Iteration 2841: Policy loss: -0.006774. Value loss: 0.110791. Entropy: 1.076671.\n",
      "episode: 1373   score: 210.0  epsilon: 1.0    steps: 816  evaluation reward: 194.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2842: Policy loss: 0.002673. Value loss: 0.325480. Entropy: 1.114906.\n",
      "Iteration 2843: Policy loss: -0.005267. Value loss: 0.172732. Entropy: 1.110585.\n",
      "Iteration 2844: Policy loss: -0.011240. Value loss: 0.109496. Entropy: 1.097995.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2845: Policy loss: 0.000080. Value loss: 0.535498. Entropy: 1.188345.\n",
      "Iteration 2846: Policy loss: -0.010671. Value loss: 0.272179. Entropy: 1.202419.\n",
      "Iteration 2847: Policy loss: -0.008407. Value loss: 0.175218. Entropy: 1.186834.\n",
      "episode: 1374   score: 185.0  epsilon: 1.0    steps: 216  evaluation reward: 195.0\n",
      "episode: 1375   score: 180.0  epsilon: 1.0    steps: 240  evaluation reward: 195.0\n",
      "episode: 1376   score: 215.0  epsilon: 1.0    steps: 368  evaluation reward: 195.6\n",
      "episode: 1377   score: 240.0  epsilon: 1.0    steps: 736  evaluation reward: 193.4\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2848: Policy loss: 0.001712. Value loss: 0.393925. Entropy: 1.212081.\n",
      "Iteration 2849: Policy loss: -0.001907. Value loss: 0.178489. Entropy: 1.225144.\n",
      "Iteration 2850: Policy loss: -0.003128. Value loss: 0.157344. Entropy: 1.228376.\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2851: Policy loss: -0.002334. Value loss: 0.369576. Entropy: 0.856900.\n",
      "Iteration 2852: Policy loss: -0.005474. Value loss: 0.200992. Entropy: 0.858315.\n",
      "Iteration 2853: Policy loss: -0.011335. Value loss: 0.131977. Entropy: 0.861216.\n",
      "episode: 1378   score: 210.0  epsilon: 1.0    steps: 608  evaluation reward: 194.1\n",
      "episode: 1379   score: 180.0  epsilon: 1.0    steps: 752  evaluation reward: 190.5\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2854: Policy loss: 0.003008. Value loss: 0.842625. Entropy: 1.150718.\n",
      "Iteration 2855: Policy loss: -0.006853. Value loss: 0.492970. Entropy: 1.130856.\n",
      "Iteration 2856: Policy loss: -0.007233. Value loss: 0.205848. Entropy: 1.134087.\n",
      "episode: 1380   score: 125.0  epsilon: 1.0    steps: 224  evaluation reward: 189.65\n",
      "episode: 1381   score: 100.0  epsilon: 1.0    steps: 360  evaluation reward: 188.1\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2857: Policy loss: 0.001092. Value loss: 0.303843. Entropy: 1.080031.\n",
      "Iteration 2858: Policy loss: -0.004195. Value loss: 0.143747. Entropy: 1.068901.\n",
      "Iteration 2859: Policy loss: -0.005102. Value loss: 0.117335. Entropy: 1.084836.\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2860: Policy loss: 0.005216. Value loss: 0.437873. Entropy: 0.972784.\n",
      "Iteration 2861: Policy loss: -0.003628. Value loss: 0.236955. Entropy: 0.978992.\n",
      "Iteration 2862: Policy loss: -0.008986. Value loss: 0.147619. Entropy: 0.991337.\n",
      "episode: 1382   score: 370.0  epsilon: 1.0    steps: 448  evaluation reward: 189.4\n",
      "episode: 1383   score: 80.0  epsilon: 1.0    steps: 616  evaluation reward: 188.05\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2863: Policy loss: 0.000917. Value loss: 0.500684. Entropy: 1.224875.\n",
      "Iteration 2864: Policy loss: -0.006095. Value loss: 0.227849. Entropy: 1.213041.\n",
      "Iteration 2865: Policy loss: -0.012630. Value loss: 0.142955. Entropy: 1.216846.\n",
      "episode: 1384   score: 155.0  epsilon: 1.0    steps: 288  evaluation reward: 188.7\n",
      "episode: 1385   score: 320.0  epsilon: 1.0    steps: 360  evaluation reward: 189.2\n",
      "episode: 1386   score: 170.0  epsilon: 1.0    steps: 392  evaluation reward: 188.8\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2866: Policy loss: 0.001912. Value loss: 0.574470. Entropy: 1.068079.\n",
      "Iteration 2867: Policy loss: 0.000384. Value loss: 0.282213. Entropy: 1.071129.\n",
      "Iteration 2868: Policy loss: -0.001530. Value loss: 0.175248. Entropy: 1.072023.\n",
      "episode: 1387   score: 210.0  epsilon: 1.0    steps: 16  evaluation reward: 188.8\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2869: Policy loss: -0.005763. Value loss: 0.455741. Entropy: 0.861907.\n",
      "Iteration 2870: Policy loss: -0.009008. Value loss: 0.222415. Entropy: 0.854316.\n",
      "Iteration 2871: Policy loss: -0.012005. Value loss: 0.155708. Entropy: 0.866701.\n",
      "episode: 1388   score: 455.0  epsilon: 1.0    steps: 104  evaluation reward: 191.25\n",
      "episode: 1389   score: 105.0  epsilon: 1.0    steps: 472  evaluation reward: 189.2\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2872: Policy loss: 0.004174. Value loss: 0.359598. Entropy: 1.114785.\n",
      "Iteration 2873: Policy loss: -0.002732. Value loss: 0.144165. Entropy: 1.126250.\n",
      "Iteration 2874: Policy loss: -0.007249. Value loss: 0.100345. Entropy: 1.132745.\n",
      "episode: 1390   score: 180.0  epsilon: 1.0    steps: 904  evaluation reward: 188.9\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2875: Policy loss: 0.000259. Value loss: 0.537046. Entropy: 1.038172.\n",
      "Iteration 2876: Policy loss: -0.006013. Value loss: 0.245620. Entropy: 1.026620.\n",
      "Iteration 2877: Policy loss: -0.008073. Value loss: 0.115713. Entropy: 1.027105.\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2878: Policy loss: 0.002842. Value loss: 0.621858. Entropy: 1.190868.\n",
      "Iteration 2879: Policy loss: -0.005038. Value loss: 0.218510. Entropy: 1.207648.\n",
      "Iteration 2880: Policy loss: -0.011748. Value loss: 0.141726. Entropy: 1.200865.\n",
      "episode: 1391   score: 105.0  epsilon: 1.0    steps: 64  evaluation reward: 188.15\n",
      "episode: 1392   score: 310.0  epsilon: 1.0    steps: 624  evaluation reward: 189.15\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2881: Policy loss: 0.003468. Value loss: 0.477388. Entropy: 1.153139.\n",
      "Iteration 2882: Policy loss: -0.001060. Value loss: 0.229355. Entropy: 1.154488.\n",
      "Iteration 2883: Policy loss: -0.001854. Value loss: 0.154412. Entropy: 1.146926.\n",
      "episode: 1393   score: 180.0  epsilon: 1.0    steps: 696  evaluation reward: 189.9\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2884: Policy loss: 0.005046. Value loss: 0.535477. Entropy: 1.084878.\n",
      "Iteration 2885: Policy loss: -0.007853. Value loss: 0.204333. Entropy: 1.094902.\n",
      "Iteration 2886: Policy loss: -0.014690. Value loss: 0.136828. Entropy: 1.114815.\n",
      "episode: 1394   score: 170.0  epsilon: 1.0    steps: 744  evaluation reward: 190.55\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2887: Policy loss: 0.001277. Value loss: 0.332836. Entropy: 1.169181.\n",
      "Iteration 2888: Policy loss: -0.005686. Value loss: 0.119069. Entropy: 1.180392.\n",
      "Iteration 2889: Policy loss: -0.009877. Value loss: 0.098417. Entropy: 1.184147.\n",
      "episode: 1395   score: 340.0  epsilon: 1.0    steps: 744  evaluation reward: 191.8\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2890: Policy loss: -0.001581. Value loss: 0.460105. Entropy: 1.184106.\n",
      "Iteration 2891: Policy loss: -0.005118. Value loss: 0.156880. Entropy: 1.188692.\n",
      "Iteration 2892: Policy loss: -0.011075. Value loss: 0.108261. Entropy: 1.183490.\n",
      "episode: 1396   score: 265.0  epsilon: 1.0    steps: 304  evaluation reward: 192.35\n",
      "episode: 1397   score: 250.0  epsilon: 1.0    steps: 944  evaluation reward: 193.05\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2893: Policy loss: 0.003765. Value loss: 0.407696. Entropy: 1.162845.\n",
      "Iteration 2894: Policy loss: -0.002247. Value loss: 0.220567. Entropy: 1.168428.\n",
      "Iteration 2895: Policy loss: -0.005308. Value loss: 0.162143. Entropy: 1.161404.\n",
      "episode: 1398   score: 180.0  epsilon: 1.0    steps: 672  evaluation reward: 193.85\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2896: Policy loss: 0.000272. Value loss: 0.447632. Entropy: 1.101455.\n",
      "Iteration 2897: Policy loss: -0.005501. Value loss: 0.211583. Entropy: 1.106396.\n",
      "Iteration 2898: Policy loss: -0.009745. Value loss: 0.139750. Entropy: 1.104517.\n",
      "episode: 1399   score: 315.0  epsilon: 1.0    steps: 512  evaluation reward: 196.15\n",
      "episode: 1400   score: 245.0  epsilon: 1.0    steps: 840  evaluation reward: 196.5\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2899: Policy loss: 0.002696. Value loss: 0.739979. Entropy: 1.092974.\n",
      "Iteration 2900: Policy loss: -0.000419. Value loss: 0.281875. Entropy: 1.097284.\n",
      "Iteration 2901: Policy loss: -0.003693. Value loss: 0.173906. Entropy: 1.106295.\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2902: Policy loss: 0.000448. Value loss: 0.557389. Entropy: 1.105016.\n",
      "Iteration 2903: Policy loss: -0.006315. Value loss: 0.265804. Entropy: 1.139801.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2904: Policy loss: -0.010891. Value loss: 0.135064. Entropy: 1.112640.\n",
      "now time :  2019-02-28 11:29:35.301455\n",
      "episode: 1401   score: 210.0  epsilon: 1.0    steps: 88  evaluation reward: 197.95\n",
      "episode: 1402   score: 425.0  epsilon: 1.0    steps: 136  evaluation reward: 200.65\n",
      "episode: 1403   score: 180.0  epsilon: 1.0    steps: 416  evaluation reward: 200.35\n",
      "episode: 1404   score: 150.0  epsilon: 1.0    steps: 808  evaluation reward: 199.7\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2905: Policy loss: -0.001320. Value loss: 0.469207. Entropy: 1.163862.\n",
      "Iteration 2906: Policy loss: -0.002812. Value loss: 0.193867. Entropy: 1.182479.\n",
      "Iteration 2907: Policy loss: -0.009989. Value loss: 0.158448. Entropy: 1.170676.\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2908: Policy loss: 0.004531. Value loss: 0.668384. Entropy: 0.974724.\n",
      "Iteration 2909: Policy loss: -0.005416. Value loss: 0.361790. Entropy: 0.987171.\n",
      "Iteration 2910: Policy loss: -0.010389. Value loss: 0.265595. Entropy: 0.985954.\n",
      "episode: 1405   score: 395.0  epsilon: 1.0    steps: 344  evaluation reward: 201.7\n",
      "episode: 1406   score: 155.0  epsilon: 1.0    steps: 672  evaluation reward: 201.15\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2911: Policy loss: -0.003955. Value loss: 0.685418. Entropy: 1.153894.\n",
      "Iteration 2912: Policy loss: -0.003300. Value loss: 0.206138. Entropy: 1.179672.\n",
      "Iteration 2913: Policy loss: -0.008705. Value loss: 0.140171. Entropy: 1.161448.\n",
      "episode: 1407   score: 135.0  epsilon: 1.0    steps: 496  evaluation reward: 201.3\n",
      "episode: 1408   score: 45.0  epsilon: 1.0    steps: 800  evaluation reward: 200.7\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2914: Policy loss: -0.000312. Value loss: 0.399477. Entropy: 1.031771.\n",
      "Iteration 2915: Policy loss: -0.001668. Value loss: 0.173845. Entropy: 1.057274.\n",
      "Iteration 2916: Policy loss: -0.007197. Value loss: 0.120288. Entropy: 1.024409.\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2917: Policy loss: 0.002947. Value loss: 0.391731. Entropy: 1.080645.\n",
      "Iteration 2918: Policy loss: -0.004488. Value loss: 0.215292. Entropy: 1.082075.\n",
      "Iteration 2919: Policy loss: -0.005249. Value loss: 0.143850. Entropy: 1.080028.\n",
      "episode: 1409   score: 210.0  epsilon: 1.0    steps: 88  evaluation reward: 198.7\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2920: Policy loss: 0.002833. Value loss: 0.312194. Entropy: 1.133602.\n",
      "Iteration 2921: Policy loss: 0.000162. Value loss: 0.132747. Entropy: 1.145020.\n",
      "Iteration 2922: Policy loss: -0.006119. Value loss: 0.114708. Entropy: 1.143833.\n",
      "episode: 1410   score: 135.0  epsilon: 1.0    steps: 192  evaluation reward: 198.7\n",
      "episode: 1411   score: 250.0  epsilon: 1.0    steps: 936  evaluation reward: 199.4\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2923: Policy loss: -0.003395. Value loss: 0.378539. Entropy: 1.096865.\n",
      "Iteration 2924: Policy loss: -0.004341. Value loss: 0.157731. Entropy: 1.102393.\n",
      "Iteration 2925: Policy loss: -0.012556. Value loss: 0.117156. Entropy: 1.101810.\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2926: Policy loss: -0.000249. Value loss: 0.275856. Entropy: 1.102548.\n",
      "Iteration 2927: Policy loss: -0.004395. Value loss: 0.125401. Entropy: 1.110777.\n",
      "Iteration 2928: Policy loss: -0.009408. Value loss: 0.087562. Entropy: 1.100837.\n",
      "episode: 1412   score: 215.0  epsilon: 1.0    steps: 488  evaluation reward: 199.45\n",
      "episode: 1413   score: 155.0  epsilon: 1.0    steps: 976  evaluation reward: 198.0\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2929: Policy loss: -0.000276. Value loss: 0.580839. Entropy: 1.243846.\n",
      "Iteration 2930: Policy loss: -0.008074. Value loss: 0.230558. Entropy: 1.250086.\n",
      "Iteration 2931: Policy loss: -0.010832. Value loss: 0.158614. Entropy: 1.247889.\n",
      "episode: 1414   score: 210.0  epsilon: 1.0    steps: 120  evaluation reward: 199.35\n",
      "episode: 1415   score: 225.0  epsilon: 1.0    steps: 168  evaluation reward: 199.5\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2932: Policy loss: -0.002256. Value loss: 0.241131. Entropy: 0.956907.\n",
      "Iteration 2933: Policy loss: -0.004530. Value loss: 0.140071. Entropy: 0.962022.\n",
      "Iteration 2934: Policy loss: -0.006884. Value loss: 0.094985. Entropy: 0.957933.\n",
      "episode: 1416   score: 180.0  epsilon: 1.0    steps: 168  evaluation reward: 198.9\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2935: Policy loss: -0.000920. Value loss: 0.488604. Entropy: 0.992311.\n",
      "Iteration 2936: Policy loss: -0.003744. Value loss: 0.205726. Entropy: 0.932291.\n",
      "Iteration 2937: Policy loss: -0.010205. Value loss: 0.159050. Entropy: 0.986689.\n",
      "episode: 1417   score: 215.0  epsilon: 1.0    steps: 72  evaluation reward: 200.0\n",
      "episode: 1418   score: 155.0  epsilon: 1.0    steps: 944  evaluation reward: 199.4\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2938: Policy loss: 0.000345. Value loss: 0.388090. Entropy: 1.039671.\n",
      "Iteration 2939: Policy loss: -0.002716. Value loss: 0.170795. Entropy: 1.036672.\n",
      "Iteration 2940: Policy loss: -0.010479. Value loss: 0.118800. Entropy: 1.045094.\n",
      "episode: 1419   score: 185.0  epsilon: 1.0    steps: 128  evaluation reward: 200.0\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2941: Policy loss: 0.001699. Value loss: 0.403152. Entropy: 1.030457.\n",
      "Iteration 2942: Policy loss: -0.004379. Value loss: 0.199819. Entropy: 1.029322.\n",
      "Iteration 2943: Policy loss: -0.009023. Value loss: 0.132343. Entropy: 1.000047.\n",
      "episode: 1420   score: 435.0  epsilon: 1.0    steps: 64  evaluation reward: 203.0\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2944: Policy loss: 0.000490. Value loss: 0.416144. Entropy: 1.074706.\n",
      "Iteration 2945: Policy loss: -0.005162. Value loss: 0.186059. Entropy: 1.062781.\n",
      "Iteration 2946: Policy loss: -0.008074. Value loss: 0.154313. Entropy: 1.044136.\n",
      "episode: 1421   score: 155.0  epsilon: 1.0    steps: 336  evaluation reward: 203.25\n",
      "episode: 1422   score: 230.0  epsilon: 1.0    steps: 432  evaluation reward: 204.3\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2947: Policy loss: -0.003175. Value loss: 0.287642. Entropy: 1.117424.\n",
      "Iteration 2948: Policy loss: -0.002190. Value loss: 0.139665. Entropy: 1.128643.\n",
      "Iteration 2949: Policy loss: -0.007341. Value loss: 0.087437. Entropy: 1.143374.\n",
      "episode: 1423   score: 215.0  epsilon: 1.0    steps: 968  evaluation reward: 205.2\n",
      "episode: 1424   score: 180.0  epsilon: 1.0    steps: 1008  evaluation reward: 205.45\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2950: Policy loss: 0.003713. Value loss: 0.544630. Entropy: 0.950373.\n",
      "Iteration 2951: Policy loss: -0.004123. Value loss: 0.249106. Entropy: 0.954542.\n",
      "Iteration 2952: Policy loss: -0.005847. Value loss: 0.131881. Entropy: 0.939948.\n",
      "episode: 1425   score: 155.0  epsilon: 1.0    steps: 128  evaluation reward: 205.8\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2953: Policy loss: 0.002749. Value loss: 0.413264. Entropy: 1.021034.\n",
      "Iteration 2954: Policy loss: -0.003578. Value loss: 0.190631. Entropy: 1.023791.\n",
      "Iteration 2955: Policy loss: -0.006563. Value loss: 0.153151. Entropy: 1.027713.\n",
      "episode: 1426   score: 210.0  epsilon: 1.0    steps: 536  evaluation reward: 205.8\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2956: Policy loss: 0.000902. Value loss: 0.411724. Entropy: 0.934898.\n",
      "Iteration 2957: Policy loss: -0.002867. Value loss: 0.155656. Entropy: 0.915621.\n",
      "Iteration 2958: Policy loss: -0.008496. Value loss: 0.109928. Entropy: 0.927805.\n",
      "episode: 1427   score: 110.0  epsilon: 1.0    steps: 376  evaluation reward: 206.7\n",
      "episode: 1428   score: 205.0  epsilon: 1.0    steps: 488  evaluation reward: 207.1\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2959: Policy loss: 0.000897. Value loss: 0.281880. Entropy: 1.053589.\n",
      "Iteration 2960: Policy loss: -0.004479. Value loss: 0.144727. Entropy: 1.050350.\n",
      "Iteration 2961: Policy loss: -0.010229. Value loss: 0.084345. Entropy: 1.049765.\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2962: Policy loss: -0.002955. Value loss: 0.297365. Entropy: 0.929104.\n",
      "Iteration 2963: Policy loss: 0.001000. Value loss: 0.147904. Entropy: 0.942234.\n",
      "Iteration 2964: Policy loss: -0.006154. Value loss: 0.102329. Entropy: 0.968589.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1429   score: 260.0  epsilon: 1.0    steps: 984  evaluation reward: 207.3\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2965: Policy loss: 0.004218. Value loss: 0.946223. Entropy: 1.243033.\n",
      "Iteration 2966: Policy loss: -0.000439. Value loss: 0.459771. Entropy: 1.235768.\n",
      "Iteration 2967: Policy loss: 0.000861. Value loss: 0.267526. Entropy: 1.248371.\n",
      "episode: 1430   score: 155.0  epsilon: 1.0    steps: 352  evaluation reward: 206.7\n",
      "episode: 1431   score: 135.0  epsilon: 1.0    steps: 488  evaluation reward: 206.7\n",
      "episode: 1432   score: 215.0  epsilon: 1.0    steps: 920  evaluation reward: 207.8\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2968: Policy loss: 0.006963. Value loss: 0.648414. Entropy: 1.186883.\n",
      "Iteration 2969: Policy loss: 0.003255. Value loss: 0.267639. Entropy: 1.189155.\n",
      "Iteration 2970: Policy loss: -0.002481. Value loss: 0.177370. Entropy: 1.180650.\n",
      "episode: 1433   score: 285.0  epsilon: 1.0    steps: 80  evaluation reward: 206.25\n",
      "episode: 1434   score: 210.0  epsilon: 1.0    steps: 376  evaluation reward: 205.8\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2971: Policy loss: 0.001985. Value loss: 0.247863. Entropy: 0.838645.\n",
      "Iteration 2972: Policy loss: -0.003116. Value loss: 0.133149. Entropy: 0.830286.\n",
      "Iteration 2973: Policy loss: -0.006284. Value loss: 0.087045. Entropy: 0.843988.\n",
      "episode: 1435   score: 410.0  epsilon: 1.0    steps: 576  evaluation reward: 207.8\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2974: Policy loss: 0.001653. Value loss: 0.249674. Entropy: 0.781357.\n",
      "Iteration 2975: Policy loss: -0.001252. Value loss: 0.109940. Entropy: 0.784612.\n",
      "Iteration 2976: Policy loss: -0.008143. Value loss: 0.092575. Entropy: 0.772266.\n",
      "episode: 1436   score: 105.0  epsilon: 1.0    steps: 848  evaluation reward: 207.05\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2977: Policy loss: 0.002367. Value loss: 0.376259. Entropy: 1.078942.\n",
      "Iteration 2978: Policy loss: -0.000005. Value loss: 0.196383. Entropy: 1.110713.\n",
      "Iteration 2979: Policy loss: -0.006340. Value loss: 0.120937. Entropy: 1.100950.\n",
      "episode: 1437   score: 445.0  epsilon: 1.0    steps: 904  evaluation reward: 209.95\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2980: Policy loss: -0.000186. Value loss: 0.415929. Entropy: 1.166432.\n",
      "Iteration 2981: Policy loss: -0.003430. Value loss: 0.123517. Entropy: 1.168293.\n",
      "Iteration 2982: Policy loss: -0.013808. Value loss: 0.066860. Entropy: 1.164743.\n",
      "episode: 1438   score: 355.0  epsilon: 1.0    steps: 696  evaluation reward: 212.45\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2983: Policy loss: 0.000317. Value loss: 0.646957. Entropy: 1.112033.\n",
      "Iteration 2984: Policy loss: 0.002212. Value loss: 0.287672. Entropy: 1.116547.\n",
      "Iteration 2985: Policy loss: -0.004259. Value loss: 0.250405. Entropy: 1.112848.\n",
      "episode: 1439   score: 110.0  epsilon: 1.0    steps: 152  evaluation reward: 212.0\n",
      "episode: 1440   score: 210.0  epsilon: 1.0    steps: 248  evaluation reward: 210.95\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2986: Policy loss: 0.001971. Value loss: 0.452456. Entropy: 1.010539.\n",
      "Iteration 2987: Policy loss: -0.003572. Value loss: 0.135877. Entropy: 1.011624.\n",
      "Iteration 2988: Policy loss: -0.011499. Value loss: 0.099660. Entropy: 1.017811.\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2989: Policy loss: 0.001671. Value loss: 0.329610. Entropy: 0.883748.\n",
      "Iteration 2990: Policy loss: -0.004255. Value loss: 0.166525. Entropy: 0.921023.\n",
      "Iteration 2991: Policy loss: -0.010399. Value loss: 0.089647. Entropy: 0.896881.\n",
      "episode: 1441   score: 75.0  epsilon: 1.0    steps: 144  evaluation reward: 210.15\n",
      "episode: 1442   score: 110.0  epsilon: 1.0    steps: 208  evaluation reward: 209.15\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2992: Policy loss: -0.001350. Value loss: 0.246031. Entropy: 1.105496.\n",
      "Iteration 2993: Policy loss: -0.007745. Value loss: 0.107990. Entropy: 1.118089.\n",
      "Iteration 2994: Policy loss: -0.010304. Value loss: 0.082557. Entropy: 1.121766.\n",
      "episode: 1443   score: 105.0  epsilon: 1.0    steps: 264  evaluation reward: 208.4\n",
      "episode: 1444   score: 240.0  epsilon: 1.0    steps: 456  evaluation reward: 208.35\n",
      "episode: 1445   score: 290.0  epsilon: 1.0    steps: 688  evaluation reward: 209.45\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2995: Policy loss: 0.000409. Value loss: 0.188508. Entropy: 0.995777.\n",
      "Iteration 2996: Policy loss: -0.004205. Value loss: 0.091107. Entropy: 0.961799.\n",
      "Iteration 2997: Policy loss: -0.005927. Value loss: 0.076625. Entropy: 0.945541.\n",
      "episode: 1446   score: 210.0  epsilon: 1.0    steps: 80  evaluation reward: 210.65\n",
      "episode: 1447   score: 180.0  epsilon: 1.0    steps: 120  evaluation reward: 210.95\n",
      "episode: 1448   score: 110.0  epsilon: 1.0    steps: 680  evaluation reward: 207.65\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2998: Policy loss: -0.000122. Value loss: 0.283517. Entropy: 0.770449.\n",
      "Iteration 2999: Policy loss: -0.002257. Value loss: 0.174331. Entropy: 0.801053.\n",
      "Iteration 3000: Policy loss: -0.004125. Value loss: 0.140237. Entropy: 0.799896.\n",
      "episode: 1449   score: 100.0  epsilon: 1.0    steps: 184  evaluation reward: 206.2\n",
      "episode: 1450   score: 80.0  epsilon: 1.0    steps: 944  evaluation reward: 206.8\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3001: Policy loss: 0.000612. Value loss: 0.218296. Entropy: 0.657322.\n",
      "Iteration 3002: Policy loss: -0.010595. Value loss: 0.088684. Entropy: 0.675360.\n",
      "Iteration 3003: Policy loss: -0.014517. Value loss: 0.072920. Entropy: 0.708109.\n",
      "now time :  2019-02-28 11:30:47.751835\n",
      "episode: 1451   score: 105.0  epsilon: 1.0    steps: 512  evaluation reward: 206.05\n",
      "episode: 1452   score: 120.0  epsilon: 1.0    steps: 776  evaluation reward: 202.85\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3004: Policy loss: 0.001606. Value loss: 0.501901. Entropy: 0.966265.\n",
      "Iteration 3005: Policy loss: -0.001714. Value loss: 0.183366. Entropy: 0.936414.\n",
      "Iteration 3006: Policy loss: -0.004695. Value loss: 0.134062. Entropy: 0.980980.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3007: Policy loss: 0.004945. Value loss: 0.312352. Entropy: 0.924953.\n",
      "Iteration 3008: Policy loss: -0.003006. Value loss: 0.164216. Entropy: 0.926161.\n",
      "Iteration 3009: Policy loss: -0.006480. Value loss: 0.095165. Entropy: 0.907075.\n",
      "episode: 1453   score: 210.0  epsilon: 1.0    steps: 808  evaluation reward: 200.65\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3010: Policy loss: -0.000540. Value loss: 0.322402. Entropy: 1.115770.\n",
      "Iteration 3011: Policy loss: -0.007561. Value loss: 0.112188. Entropy: 1.106727.\n",
      "Iteration 3012: Policy loss: -0.007647. Value loss: 0.089244. Entropy: 1.109525.\n",
      "episode: 1454   score: 215.0  epsilon: 1.0    steps: 824  evaluation reward: 201.65\n",
      "episode: 1455   score: 210.0  epsilon: 1.0    steps: 928  evaluation reward: 201.35\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3013: Policy loss: 0.002139. Value loss: 0.333814. Entropy: 1.127716.\n",
      "Iteration 3014: Policy loss: -0.001886. Value loss: 0.141640. Entropy: 1.137469.\n",
      "Iteration 3015: Policy loss: -0.004628. Value loss: 0.073011. Entropy: 1.135019.\n",
      "episode: 1456   score: 180.0  epsilon: 1.0    steps: 120  evaluation reward: 201.05\n",
      "episode: 1457   score: 210.0  epsilon: 1.0    steps: 688  evaluation reward: 201.8\n",
      "episode: 1458   score: 210.0  epsilon: 1.0    steps: 984  evaluation reward: 202.1\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3016: Policy loss: 0.002933. Value loss: 0.432355. Entropy: 1.044856.\n",
      "Iteration 3017: Policy loss: -0.004397. Value loss: 0.179388. Entropy: 1.037483.\n",
      "Iteration 3018: Policy loss: -0.000793. Value loss: 0.140588. Entropy: 1.043945.\n",
      "episode: 1459   score: 305.0  epsilon: 1.0    steps: 280  evaluation reward: 202.55\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3019: Policy loss: 0.004384. Value loss: 0.345790. Entropy: 0.783445.\n",
      "Iteration 3020: Policy loss: -0.003797. Value loss: 0.159114. Entropy: 0.811919.\n",
      "Iteration 3021: Policy loss: -0.002950. Value loss: 0.116511. Entropy: 0.809398.\n",
      "episode: 1460   score: 210.0  epsilon: 1.0    steps: 864  evaluation reward: 203.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3022: Policy loss: -0.000181. Value loss: 0.186226. Entropy: 0.980697.\n",
      "Iteration 3023: Policy loss: -0.003854. Value loss: 0.090089. Entropy: 0.988187.\n",
      "Iteration 3024: Policy loss: -0.004239. Value loss: 0.065857. Entropy: 0.987363.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3025: Policy loss: -0.001395. Value loss: 0.151987. Entropy: 1.039802.\n",
      "Iteration 3026: Policy loss: -0.002938. Value loss: 0.094859. Entropy: 1.038723.\n",
      "Iteration 3027: Policy loss: -0.009791. Value loss: 0.085926. Entropy: 1.040756.\n",
      "episode: 1461   score: 150.0  epsilon: 1.0    steps: 80  evaluation reward: 204.05\n",
      "episode: 1462   score: 210.0  epsilon: 1.0    steps: 152  evaluation reward: 204.6\n",
      "episode: 1463   score: 210.0  epsilon: 1.0    steps: 400  evaluation reward: 205.9\n",
      "episode: 1464   score: 180.0  epsilon: 1.0    steps: 896  evaluation reward: 205.6\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3028: Policy loss: 0.000516. Value loss: 0.229685. Entropy: 1.003961.\n",
      "Iteration 3029: Policy loss: -0.007594. Value loss: 0.119544. Entropy: 1.019540.\n",
      "Iteration 3030: Policy loss: -0.008223. Value loss: 0.087897. Entropy: 1.025338.\n",
      "episode: 1465   score: 210.0  epsilon: 1.0    steps: 280  evaluation reward: 206.15\n",
      "episode: 1466   score: 210.0  epsilon: 1.0    steps: 672  evaluation reward: 206.45\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3031: Policy loss: 0.002525. Value loss: 0.291127. Entropy: 0.824826.\n",
      "Iteration 3032: Policy loss: -0.001120. Value loss: 0.167859. Entropy: 0.819314.\n",
      "Iteration 3033: Policy loss: -0.005704. Value loss: 0.124257. Entropy: 0.813179.\n",
      "episode: 1467   score: 520.0  epsilon: 1.0    steps: 872  evaluation reward: 210.45\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3034: Policy loss: 0.008389. Value loss: 0.377856. Entropy: 0.961463.\n",
      "Iteration 3035: Policy loss: -0.001698. Value loss: 0.205734. Entropy: 0.985298.\n",
      "Iteration 3036: Policy loss: -0.005053. Value loss: 0.131722. Entropy: 0.974289.\n",
      "episode: 1468   score: 180.0  epsilon: 1.0    steps: 920  evaluation reward: 208.85\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3037: Policy loss: 0.002303. Value loss: 0.197082. Entropy: 0.946160.\n",
      "Iteration 3038: Policy loss: -0.006478. Value loss: 0.153400. Entropy: 0.958185.\n",
      "Iteration 3039: Policy loss: -0.007912. Value loss: 0.089716. Entropy: 0.954498.\n",
      "episode: 1469   score: 210.0  epsilon: 1.0    steps: 248  evaluation reward: 208.1\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3040: Policy loss: -0.001245. Value loss: 0.230950. Entropy: 1.128068.\n",
      "Iteration 3041: Policy loss: -0.002235. Value loss: 0.124830. Entropy: 1.128724.\n",
      "Iteration 3042: Policy loss: -0.007436. Value loss: 0.094425. Entropy: 1.130714.\n",
      "episode: 1470   score: 215.0  epsilon: 1.0    steps: 240  evaluation reward: 208.15\n",
      "episode: 1471   score: 135.0  epsilon: 1.0    steps: 304  evaluation reward: 207.85\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3043: Policy loss: 0.003459. Value loss: 0.158961. Entropy: 1.040606.\n",
      "Iteration 3044: Policy loss: -0.003317. Value loss: 0.087812. Entropy: 1.063433.\n",
      "Iteration 3045: Policy loss: -0.003875. Value loss: 0.061198. Entropy: 1.059517.\n",
      "episode: 1472   score: 210.0  epsilon: 1.0    steps: 56  evaluation reward: 207.85\n",
      "episode: 1473   score: 240.0  epsilon: 1.0    steps: 816  evaluation reward: 208.15\n",
      "episode: 1474   score: 210.0  epsilon: 1.0    steps: 1008  evaluation reward: 208.4\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3046: Policy loss: 0.002624. Value loss: 0.253229. Entropy: 1.031433.\n",
      "Iteration 3047: Policy loss: -0.002236. Value loss: 0.095417. Entropy: 1.043674.\n",
      "Iteration 3048: Policy loss: -0.009913. Value loss: 0.079751. Entropy: 1.024459.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3049: Policy loss: 0.000080. Value loss: 0.178711. Entropy: 0.801957.\n",
      "Iteration 3050: Policy loss: -0.003080. Value loss: 0.127101. Entropy: 0.806883.\n",
      "Iteration 3051: Policy loss: -0.003586. Value loss: 0.071676. Entropy: 0.811079.\n",
      "episode: 1475   score: 210.0  epsilon: 1.0    steps: 432  evaluation reward: 208.7\n",
      "episode: 1476   score: 135.0  epsilon: 1.0    steps: 896  evaluation reward: 207.9\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3052: Policy loss: -0.001368. Value loss: 0.290967. Entropy: 1.150613.\n",
      "Iteration 3053: Policy loss: -0.004298. Value loss: 0.100684. Entropy: 1.129995.\n",
      "Iteration 3054: Policy loss: -0.006423. Value loss: 0.086970. Entropy: 1.156842.\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3055: Policy loss: -0.002215. Value loss: 0.487559. Entropy: 1.073809.\n",
      "Iteration 3056: Policy loss: -0.000018. Value loss: 0.156319. Entropy: 1.042215.\n",
      "Iteration 3057: Policy loss: -0.004279. Value loss: 0.101825. Entropy: 1.068189.\n",
      "episode: 1477   score: 270.0  epsilon: 1.0    steps: 176  evaluation reward: 208.2\n",
      "episode: 1478   score: 215.0  epsilon: 1.0    steps: 424  evaluation reward: 208.25\n",
      "episode: 1479   score: 135.0  epsilon: 1.0    steps: 1008  evaluation reward: 207.8\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3058: Policy loss: -0.000358. Value loss: 0.322625. Entropy: 1.039968.\n",
      "Iteration 3059: Policy loss: -0.007731. Value loss: 0.128698. Entropy: 1.081724.\n",
      "Iteration 3060: Policy loss: -0.009523. Value loss: 0.068359. Entropy: 1.058517.\n",
      "episode: 1480   score: 180.0  epsilon: 1.0    steps: 368  evaluation reward: 208.35\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3061: Policy loss: 0.000446. Value loss: 0.221007. Entropy: 1.033655.\n",
      "Iteration 3062: Policy loss: -0.002012. Value loss: 0.091549. Entropy: 1.037187.\n",
      "Iteration 3063: Policy loss: -0.007585. Value loss: 0.072501. Entropy: 1.042699.\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3064: Policy loss: 0.000592. Value loss: 0.393750. Entropy: 1.049469.\n",
      "Iteration 3065: Policy loss: -0.001714. Value loss: 0.204694. Entropy: 1.062206.\n",
      "Iteration 3066: Policy loss: -0.007741. Value loss: 0.113784. Entropy: 1.047230.\n",
      "episode: 1481   score: 245.0  epsilon: 1.0    steps: 552  evaluation reward: 209.8\n",
      "episode: 1482   score: 355.0  epsilon: 1.0    steps: 664  evaluation reward: 209.65\n",
      "episode: 1483   score: 210.0  epsilon: 1.0    steps: 720  evaluation reward: 210.95\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3067: Policy loss: -0.000139. Value loss: 0.357028. Entropy: 1.155785.\n",
      "Iteration 3068: Policy loss: -0.002644. Value loss: 0.146010. Entropy: 1.169780.\n",
      "Iteration 3069: Policy loss: -0.007289. Value loss: 0.102831. Entropy: 1.160094.\n",
      "episode: 1484   score: 120.0  epsilon: 1.0    steps: 480  evaluation reward: 210.6\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3070: Policy loss: -0.000563. Value loss: 0.362660. Entropy: 0.910261.\n",
      "Iteration 3071: Policy loss: -0.004164. Value loss: 0.147638. Entropy: 0.914543.\n",
      "Iteration 3072: Policy loss: -0.002723. Value loss: 0.097127. Entropy: 0.930951.\n",
      "episode: 1485   score: 210.0  epsilon: 1.0    steps: 456  evaluation reward: 209.5\n",
      "episode: 1486   score: 110.0  epsilon: 1.0    steps: 936  evaluation reward: 208.9\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3073: Policy loss: 0.001829. Value loss: 0.321584. Entropy: 1.112909.\n",
      "Iteration 3074: Policy loss: -0.001717. Value loss: 0.183664. Entropy: 1.139047.\n",
      "Iteration 3075: Policy loss: -0.012200. Value loss: 0.144519. Entropy: 1.131692.\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3076: Policy loss: 0.005719. Value loss: 0.325004. Entropy: 0.863297.\n",
      "Iteration 3077: Policy loss: -0.003000. Value loss: 0.120088. Entropy: 0.864546.\n",
      "Iteration 3078: Policy loss: -0.007164. Value loss: 0.079736. Entropy: 0.867368.\n",
      "episode: 1487   score: 210.0  epsilon: 1.0    steps: 344  evaluation reward: 208.9\n",
      "episode: 1488   score: 45.0  epsilon: 1.0    steps: 480  evaluation reward: 204.8\n",
      "episode: 1489   score: 180.0  epsilon: 1.0    steps: 920  evaluation reward: 205.55\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3079: Policy loss: 0.000031. Value loss: 0.475741. Entropy: 1.228671.\n",
      "Iteration 3080: Policy loss: -0.002138. Value loss: 0.233831. Entropy: 1.235095.\n",
      "Iteration 3081: Policy loss: -0.007122. Value loss: 0.132316. Entropy: 1.236978.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3082: Policy loss: 0.000761. Value loss: 0.214959. Entropy: 0.904182.\n",
      "Iteration 3083: Policy loss: -0.003918. Value loss: 0.095962. Entropy: 0.909448.\n",
      "Iteration 3084: Policy loss: -0.006430. Value loss: 0.074259. Entropy: 0.895967.\n",
      "episode: 1490   score: 255.0  epsilon: 1.0    steps: 688  evaluation reward: 206.3\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3085: Policy loss: -0.000201. Value loss: 0.444147. Entropy: 1.228241.\n",
      "Iteration 3086: Policy loss: -0.003033. Value loss: 0.148844. Entropy: 1.230170.\n",
      "Iteration 3087: Policy loss: -0.011460. Value loss: 0.123830. Entropy: 1.227246.\n",
      "episode: 1491   score: 225.0  epsilon: 1.0    steps: 120  evaluation reward: 207.5\n",
      "episode: 1492   score: 155.0  epsilon: 1.0    steps: 888  evaluation reward: 205.95\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3088: Policy loss: 0.005724. Value loss: 0.278818. Entropy: 1.097336.\n",
      "Iteration 3089: Policy loss: -0.000367. Value loss: 0.130861. Entropy: 1.109802.\n",
      "Iteration 3090: Policy loss: -0.004354. Value loss: 0.111542. Entropy: 1.101614.\n",
      "episode: 1493   score: 210.0  epsilon: 1.0    steps: 432  evaluation reward: 206.25\n",
      "episode: 1494   score: 120.0  epsilon: 1.0    steps: 888  evaluation reward: 205.75\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3091: Policy loss: 0.001564. Value loss: 0.340850. Entropy: 1.116077.\n",
      "Iteration 3092: Policy loss: -0.007115. Value loss: 0.142378. Entropy: 1.125372.\n",
      "Iteration 3093: Policy loss: -0.010962. Value loss: 0.099596. Entropy: 1.127028.\n",
      "episode: 1495   score: 215.0  epsilon: 1.0    steps: 264  evaluation reward: 204.5\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3094: Policy loss: 0.003472. Value loss: 0.359105. Entropy: 1.017491.\n",
      "Iteration 3095: Policy loss: -0.000857. Value loss: 0.121383. Entropy: 1.016015.\n",
      "Iteration 3096: Policy loss: -0.004081. Value loss: 0.075575. Entropy: 1.044283.\n",
      "episode: 1496   score: 80.0  epsilon: 1.0    steps: 288  evaluation reward: 202.65\n",
      "episode: 1497   score: 365.0  epsilon: 1.0    steps: 624  evaluation reward: 203.8\n",
      "episode: 1498   score: 50.0  epsilon: 1.0    steps: 944  evaluation reward: 202.5\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3097: Policy loss: 0.001070. Value loss: 0.385448. Entropy: 1.116449.\n",
      "Iteration 3098: Policy loss: -0.001848. Value loss: 0.205809. Entropy: 1.109026.\n",
      "Iteration 3099: Policy loss: -0.006001. Value loss: 0.128444. Entropy: 1.112325.\n",
      "episode: 1499   score: 230.0  epsilon: 1.0    steps: 136  evaluation reward: 201.65\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3100: Policy loss: 0.000387. Value loss: 0.234817. Entropy: 0.854281.\n",
      "Iteration 3101: Policy loss: -0.002494. Value loss: 0.137360. Entropy: 0.871541.\n",
      "Iteration 3102: Policy loss: -0.005176. Value loss: 0.110692. Entropy: 0.853615.\n",
      "episode: 1500   score: 180.0  epsilon: 1.0    steps: 536  evaluation reward: 201.0\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3103: Policy loss: -0.000416. Value loss: 0.250613. Entropy: 0.948530.\n",
      "Iteration 3104: Policy loss: -0.007145. Value loss: 0.110252. Entropy: 0.956360.\n",
      "Iteration 3105: Policy loss: -0.013985. Value loss: 0.073437. Entropy: 0.942397.\n",
      "now time :  2019-02-28 11:32:01.072346\n",
      "episode: 1501   score: 150.0  epsilon: 1.0    steps: 88  evaluation reward: 200.4\n",
      "episode: 1502   score: 230.0  epsilon: 1.0    steps: 360  evaluation reward: 198.45\n",
      "episode: 1503   score: 210.0  epsilon: 1.0    steps: 792  evaluation reward: 198.75\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3106: Policy loss: 0.001435. Value loss: 0.305269. Entropy: 1.032585.\n",
      "Iteration 3107: Policy loss: -0.005306. Value loss: 0.174158. Entropy: 1.032347.\n",
      "Iteration 3108: Policy loss: -0.006657. Value loss: 0.133918. Entropy: 1.024886.\n",
      "episode: 1504   score: 155.0  epsilon: 1.0    steps: 776  evaluation reward: 198.8\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3109: Policy loss: -0.001089. Value loss: 0.274933. Entropy: 1.002912.\n",
      "Iteration 3110: Policy loss: 0.000920. Value loss: 0.144251. Entropy: 0.997882.\n",
      "Iteration 3111: Policy loss: -0.006827. Value loss: 0.064932. Entropy: 1.017810.\n",
      "episode: 1505   score: 135.0  epsilon: 1.0    steps: 40  evaluation reward: 196.2\n",
      "episode: 1506   score: 210.0  epsilon: 1.0    steps: 920  evaluation reward: 196.75\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3112: Policy loss: -0.002674. Value loss: 0.289795. Entropy: 0.935838.\n",
      "Iteration 3113: Policy loss: -0.008024. Value loss: 0.119067. Entropy: 0.980336.\n",
      "Iteration 3114: Policy loss: -0.002982. Value loss: 0.076439. Entropy: 0.970506.\n",
      "episode: 1507   score: 155.0  epsilon: 1.0    steps: 184  evaluation reward: 196.95\n",
      "episode: 1508   score: 90.0  epsilon: 1.0    steps: 840  evaluation reward: 197.4\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3115: Policy loss: 0.000289. Value loss: 0.267131. Entropy: 1.072261.\n",
      "Iteration 3116: Policy loss: -0.000271. Value loss: 0.115583. Entropy: 1.085530.\n",
      "Iteration 3117: Policy loss: -0.005027. Value loss: 0.073367. Entropy: 1.092782.\n",
      "episode: 1509   score: 135.0  epsilon: 1.0    steps: 1016  evaluation reward: 196.65\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3118: Policy loss: 0.002822. Value loss: 0.332858. Entropy: 1.024537.\n",
      "Iteration 3119: Policy loss: -0.002030. Value loss: 0.127450. Entropy: 1.030334.\n",
      "Iteration 3120: Policy loss: -0.003943. Value loss: 0.089729. Entropy: 1.040801.\n",
      "episode: 1510   score: 155.0  epsilon: 1.0    steps: 408  evaluation reward: 196.85\n",
      "episode: 1511   score: 215.0  epsilon: 1.0    steps: 968  evaluation reward: 196.5\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3121: Policy loss: 0.002107. Value loss: 0.290387. Entropy: 1.125721.\n",
      "Iteration 3122: Policy loss: -0.006942. Value loss: 0.090724. Entropy: 1.126544.\n",
      "Iteration 3123: Policy loss: -0.008084. Value loss: 0.064714. Entropy: 1.123367.\n",
      "episode: 1512   score: 130.0  epsilon: 1.0    steps: 56  evaluation reward: 195.65\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3124: Policy loss: 0.000641. Value loss: 0.873772. Entropy: 1.094254.\n",
      "Iteration 3125: Policy loss: -0.001297. Value loss: 0.313391. Entropy: 1.068890.\n",
      "Iteration 3126: Policy loss: 0.004663. Value loss: 0.190463. Entropy: 1.078348.\n",
      "episode: 1513   score: 180.0  epsilon: 1.0    steps: 304  evaluation reward: 195.9\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3127: Policy loss: 0.003350. Value loss: 0.523046. Entropy: 1.015137.\n",
      "Iteration 3128: Policy loss: 0.004281. Value loss: 0.202712. Entropy: 1.012126.\n",
      "Iteration 3129: Policy loss: -0.002166. Value loss: 0.154013. Entropy: 1.009713.\n",
      "episode: 1514   score: 210.0  epsilon: 1.0    steps: 136  evaluation reward: 195.9\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3130: Policy loss: -0.000453. Value loss: 0.441782. Entropy: 1.095554.\n",
      "Iteration 3131: Policy loss: -0.001705. Value loss: 0.204814. Entropy: 1.108206.\n",
      "Iteration 3132: Policy loss: -0.010470. Value loss: 0.156403. Entropy: 1.099669.\n",
      "episode: 1515   score: 305.0  epsilon: 1.0    steps: 400  evaluation reward: 196.7\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3133: Policy loss: 0.005601. Value loss: 0.382053. Entropy: 1.165845.\n",
      "Iteration 3134: Policy loss: -0.001624. Value loss: 0.130136. Entropy: 1.172242.\n",
      "Iteration 3135: Policy loss: -0.008119. Value loss: 0.094640. Entropy: 1.178790.\n",
      "episode: 1516   score: 135.0  epsilon: 1.0    steps: 408  evaluation reward: 196.25\n",
      "episode: 1517   score: 110.0  epsilon: 1.0    steps: 488  evaluation reward: 195.2\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3136: Policy loss: 0.000288. Value loss: 0.337533. Entropy: 1.119301.\n",
      "Iteration 3137: Policy loss: -0.001099. Value loss: 0.134485. Entropy: 1.117574.\n",
      "Iteration 3138: Policy loss: -0.006371. Value loss: 0.084273. Entropy: 1.122591.\n",
      "episode: 1518   score: 415.0  epsilon: 1.0    steps: 712  evaluation reward: 197.8\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3139: Policy loss: 0.003836. Value loss: 0.272735. Entropy: 1.048083.\n",
      "Iteration 3140: Policy loss: -0.004673. Value loss: 0.135338. Entropy: 1.044435.\n",
      "Iteration 3141: Policy loss: -0.010414. Value loss: 0.093165. Entropy: 1.027123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1519   score: 100.0  epsilon: 1.0    steps: 272  evaluation reward: 196.95\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3142: Policy loss: 0.000753. Value loss: 0.627931. Entropy: 1.168783.\n",
      "Iteration 3143: Policy loss: 0.003303. Value loss: 0.276884. Entropy: 1.194111.\n",
      "Iteration 3144: Policy loss: -0.009842. Value loss: 0.162611. Entropy: 1.166379.\n",
      "episode: 1520   score: 215.0  epsilon: 1.0    steps: 408  evaluation reward: 194.75\n",
      "episode: 1521   score: 285.0  epsilon: 1.0    steps: 480  evaluation reward: 196.05\n",
      "episode: 1522   score: 125.0  epsilon: 1.0    steps: 712  evaluation reward: 195.0\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3145: Policy loss: -0.003949. Value loss: 0.238251. Entropy: 1.060853.\n",
      "Iteration 3146: Policy loss: -0.005736. Value loss: 0.109323. Entropy: 1.059068.\n",
      "Iteration 3147: Policy loss: -0.005815. Value loss: 0.093135. Entropy: 1.071560.\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3148: Policy loss: -0.001314. Value loss: 0.259964. Entropy: 1.033323.\n",
      "Iteration 3149: Policy loss: -0.001994. Value loss: 0.120938. Entropy: 1.009964.\n",
      "Iteration 3150: Policy loss: -0.004693. Value loss: 0.095708. Entropy: 1.020770.\n",
      "episode: 1523   score: 395.0  epsilon: 1.0    steps: 704  evaluation reward: 196.8\n",
      "episode: 1524   score: 210.0  epsilon: 1.0    steps: 848  evaluation reward: 197.1\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3151: Policy loss: 0.001014. Value loss: 0.424666. Entropy: 1.044969.\n",
      "Iteration 3152: Policy loss: -0.005199. Value loss: 0.160896. Entropy: 1.062196.\n",
      "Iteration 3153: Policy loss: -0.009329. Value loss: 0.114085. Entropy: 1.061904.\n",
      "episode: 1525   score: 210.0  epsilon: 1.0    steps: 832  evaluation reward: 197.65\n",
      "episode: 1526   score: 65.0  epsilon: 1.0    steps: 976  evaluation reward: 196.2\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3154: Policy loss: 0.000773. Value loss: 0.492421. Entropy: 1.094602.\n",
      "Iteration 3155: Policy loss: -0.006923. Value loss: 0.161110. Entropy: 1.100717.\n",
      "Iteration 3156: Policy loss: -0.010818. Value loss: 0.093660. Entropy: 1.086568.\n",
      "episode: 1527   score: 295.0  epsilon: 1.0    steps: 984  evaluation reward: 198.05\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3157: Policy loss: 0.007321. Value loss: 0.267236. Entropy: 1.078374.\n",
      "Iteration 3158: Policy loss: -0.005169. Value loss: 0.091342. Entropy: 1.082960.\n",
      "Iteration 3159: Policy loss: -0.003025. Value loss: 0.075797. Entropy: 1.104242.\n",
      "episode: 1528   score: 80.0  epsilon: 1.0    steps: 856  evaluation reward: 196.8\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3160: Policy loss: -0.000704. Value loss: 0.156531. Entropy: 1.115751.\n",
      "Iteration 3161: Policy loss: -0.009731. Value loss: 0.052088. Entropy: 1.113861.\n",
      "Iteration 3162: Policy loss: -0.009850. Value loss: 0.039565. Entropy: 1.097304.\n",
      "episode: 1529   score: 105.0  epsilon: 1.0    steps: 976  evaluation reward: 195.25\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3163: Policy loss: 0.002732. Value loss: 0.355304. Entropy: 1.153002.\n",
      "Iteration 3164: Policy loss: -0.004255. Value loss: 0.136997. Entropy: 1.144092.\n",
      "Iteration 3165: Policy loss: -0.010470. Value loss: 0.096078. Entropy: 1.134522.\n",
      "episode: 1530   score: 270.0  epsilon: 1.0    steps: 184  evaluation reward: 196.4\n",
      "episode: 1531   score: 240.0  epsilon: 1.0    steps: 336  evaluation reward: 197.45\n",
      "episode: 1532   score: 225.0  epsilon: 1.0    steps: 728  evaluation reward: 197.55\n",
      "episode: 1533   score: 180.0  epsilon: 1.0    steps: 912  evaluation reward: 196.5\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3166: Policy loss: 0.007635. Value loss: 0.730597. Entropy: 1.169105.\n",
      "Iteration 3167: Policy loss: -0.001461. Value loss: 0.367267. Entropy: 1.175545.\n",
      "Iteration 3168: Policy loss: -0.002429. Value loss: 0.248177. Entropy: 1.166178.\n",
      "episode: 1534   score: 105.0  epsilon: 1.0    steps: 224  evaluation reward: 195.45\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3169: Policy loss: 0.001664. Value loss: 0.310649. Entropy: 0.937623.\n",
      "Iteration 3170: Policy loss: -0.001194. Value loss: 0.181885. Entropy: 0.911186.\n",
      "Iteration 3171: Policy loss: -0.004195. Value loss: 0.160362. Entropy: 0.925367.\n",
      "episode: 1535   score: 415.0  epsilon: 1.0    steps: 384  evaluation reward: 195.5\n",
      "episode: 1536   score: 125.0  epsilon: 1.0    steps: 968  evaluation reward: 195.7\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3172: Policy loss: 0.002005. Value loss: 0.414531. Entropy: 1.075876.\n",
      "Iteration 3173: Policy loss: 0.004761. Value loss: 0.249704. Entropy: 1.070094.\n",
      "Iteration 3174: Policy loss: -0.003477. Value loss: 0.182451. Entropy: 1.087155.\n",
      "episode: 1537   score: 90.0  epsilon: 1.0    steps: 304  evaluation reward: 192.15\n",
      "episode: 1538   score: 105.0  epsilon: 1.0    steps: 800  evaluation reward: 189.65\n",
      "episode: 1539   score: 90.0  epsilon: 1.0    steps: 808  evaluation reward: 189.45\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3175: Policy loss: -0.002291. Value loss: 0.290190. Entropy: 1.006835.\n",
      "Iteration 3176: Policy loss: -0.004814. Value loss: 0.150898. Entropy: 0.989747.\n",
      "Iteration 3177: Policy loss: -0.006696. Value loss: 0.092628. Entropy: 0.996765.\n",
      "episode: 1540   score: 180.0  epsilon: 1.0    steps: 312  evaluation reward: 189.15\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3178: Policy loss: 0.001258. Value loss: 0.434179. Entropy: 0.938204.\n",
      "Iteration 3179: Policy loss: -0.002340. Value loss: 0.177585. Entropy: 0.953706.\n",
      "Iteration 3180: Policy loss: -0.007189. Value loss: 0.121683. Entropy: 0.967372.\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3181: Policy loss: 0.002664. Value loss: 0.616073. Entropy: 1.103754.\n",
      "Iteration 3182: Policy loss: -0.002672. Value loss: 0.242015. Entropy: 1.105355.\n",
      "Iteration 3183: Policy loss: -0.002877. Value loss: 0.166487. Entropy: 1.117066.\n",
      "episode: 1541   score: 180.0  epsilon: 1.0    steps: 352  evaluation reward: 190.2\n",
      "episode: 1542   score: 285.0  epsilon: 1.0    steps: 1016  evaluation reward: 191.95\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3184: Policy loss: 0.003870. Value loss: 0.563757. Entropy: 1.194727.\n",
      "Iteration 3185: Policy loss: 0.003644. Value loss: 0.243789. Entropy: 1.193714.\n",
      "Iteration 3186: Policy loss: -0.002242. Value loss: 0.151139. Entropy: 1.191925.\n",
      "episode: 1543   score: 210.0  epsilon: 1.0    steps: 200  evaluation reward: 193.0\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3187: Policy loss: -0.000662. Value loss: 0.269592. Entropy: 1.117171.\n",
      "Iteration 3188: Policy loss: -0.003537. Value loss: 0.154610. Entropy: 1.098681.\n",
      "Iteration 3189: Policy loss: -0.008437. Value loss: 0.110995. Entropy: 1.109352.\n",
      "episode: 1544   score: 80.0  epsilon: 1.0    steps: 64  evaluation reward: 191.4\n",
      "episode: 1545   score: 210.0  epsilon: 1.0    steps: 168  evaluation reward: 190.6\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3190: Policy loss: -0.002466. Value loss: 0.460974. Entropy: 1.066994.\n",
      "Iteration 3191: Policy loss: -0.004531. Value loss: 0.251013. Entropy: 1.085745.\n",
      "Iteration 3192: Policy loss: -0.005534. Value loss: 0.198546. Entropy: 1.086521.\n",
      "episode: 1546   score: 170.0  epsilon: 1.0    steps: 216  evaluation reward: 190.2\n",
      "episode: 1547   score: 90.0  epsilon: 1.0    steps: 392  evaluation reward: 189.3\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3193: Policy loss: 0.003296. Value loss: 0.303736. Entropy: 1.031285.\n",
      "Iteration 3194: Policy loss: -0.000571. Value loss: 0.151595. Entropy: 1.020181.\n",
      "Iteration 3195: Policy loss: -0.006112. Value loss: 0.111328. Entropy: 1.023735.\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3196: Policy loss: 0.008468. Value loss: 0.253590. Entropy: 1.085283.\n",
      "Iteration 3197: Policy loss: -0.002154. Value loss: 0.116434. Entropy: 1.092976.\n",
      "Iteration 3198: Policy loss: -0.007976. Value loss: 0.078775. Entropy: 1.095964.\n",
      "episode: 1548   score: 410.0  epsilon: 1.0    steps: 112  evaluation reward: 192.3\n",
      "episode: 1549   score: 110.0  epsilon: 1.0    steps: 480  evaluation reward: 192.4\n",
      "episode: 1550   score: 225.0  epsilon: 1.0    steps: 800  evaluation reward: 193.85\n",
      "Training network. lr: 0.000226. clip: 0.090332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3199: Policy loss: 0.006730. Value loss: 0.505161. Entropy: 1.264863.\n",
      "Iteration 3200: Policy loss: -0.004413. Value loss: 0.197122. Entropy: 1.256385.\n",
      "Iteration 3201: Policy loss: -0.003537. Value loss: 0.114934. Entropy: 1.260308.\n",
      "now time :  2019-02-28 11:33:11.366713\n",
      "episode: 1551   score: 210.0  epsilon: 1.0    steps: 480  evaluation reward: 194.9\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3202: Policy loss: 0.000425. Value loss: 0.135641. Entropy: 0.968035.\n",
      "Iteration 3203: Policy loss: -0.011308. Value loss: 0.071903. Entropy: 0.976823.\n",
      "Iteration 3204: Policy loss: -0.008964. Value loss: 0.060888. Entropy: 0.957264.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3205: Policy loss: 0.001829. Value loss: 0.764505. Entropy: 1.210417.\n",
      "Iteration 3206: Policy loss: -0.000855. Value loss: 0.276710. Entropy: 1.205506.\n",
      "Iteration 3207: Policy loss: -0.004153. Value loss: 0.136273. Entropy: 1.223545.\n",
      "episode: 1552   score: 235.0  epsilon: 1.0    steps: 48  evaluation reward: 196.05\n",
      "episode: 1553   score: 110.0  epsilon: 1.0    steps: 136  evaluation reward: 195.05\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3208: Policy loss: 0.001537. Value loss: 0.625352. Entropy: 1.095465.\n",
      "Iteration 3209: Policy loss: -0.006697. Value loss: 0.207196. Entropy: 1.079182.\n",
      "Iteration 3210: Policy loss: -0.008597. Value loss: 0.119651. Entropy: 1.082832.\n",
      "episode: 1554   score: 180.0  epsilon: 1.0    steps: 232  evaluation reward: 194.7\n",
      "episode: 1555   score: 315.0  epsilon: 1.0    steps: 304  evaluation reward: 195.75\n",
      "episode: 1556   score: 80.0  epsilon: 1.0    steps: 408  evaluation reward: 194.75\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3211: Policy loss: 0.000828. Value loss: 0.527000. Entropy: 1.074207.\n",
      "Iteration 3212: Policy loss: -0.000916. Value loss: 0.156413. Entropy: 1.114958.\n",
      "Iteration 3213: Policy loss: -0.010738. Value loss: 0.108629. Entropy: 1.099154.\n",
      "episode: 1557   score: 155.0  epsilon: 1.0    steps: 128  evaluation reward: 194.2\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3214: Policy loss: -0.001160. Value loss: 0.380300. Entropy: 0.981760.\n",
      "Iteration 3215: Policy loss: -0.004879. Value loss: 0.238106. Entropy: 1.002129.\n",
      "Iteration 3216: Policy loss: -0.010904. Value loss: 0.186990. Entropy: 0.979511.\n",
      "episode: 1558   score: 105.0  epsilon: 1.0    steps: 1016  evaluation reward: 193.15\n",
      "episode: 1559   score: 155.0  epsilon: 1.0    steps: 1024  evaluation reward: 191.65\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3217: Policy loss: -0.001149. Value loss: 0.340019. Entropy: 1.017010.\n",
      "Iteration 3218: Policy loss: -0.003999. Value loss: 0.167807. Entropy: 1.018419.\n",
      "Iteration 3219: Policy loss: -0.006715. Value loss: 0.130832. Entropy: 1.045140.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3220: Policy loss: -0.003432. Value loss: 0.501995. Entropy: 1.171983.\n",
      "Iteration 3221: Policy loss: -0.006114. Value loss: 0.170547. Entropy: 1.174307.\n",
      "Iteration 3222: Policy loss: -0.011620. Value loss: 0.101902. Entropy: 1.187509.\n",
      "episode: 1560   score: 65.0  epsilon: 1.0    steps: 64  evaluation reward: 190.2\n",
      "episode: 1561   score: 155.0  epsilon: 1.0    steps: 392  evaluation reward: 190.25\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3223: Policy loss: 0.002862. Value loss: 0.528683. Entropy: 1.211457.\n",
      "Iteration 3224: Policy loss: -0.004319. Value loss: 0.184093. Entropy: 1.200465.\n",
      "Iteration 3225: Policy loss: -0.006127. Value loss: 0.111399. Entropy: 1.209715.\n",
      "episode: 1562   score: 290.0  epsilon: 1.0    steps: 216  evaluation reward: 191.05\n",
      "episode: 1563   score: 265.0  epsilon: 1.0    steps: 936  evaluation reward: 191.6\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3226: Policy loss: 0.005857. Value loss: 0.388962. Entropy: 1.030094.\n",
      "Iteration 3227: Policy loss: -0.007793. Value loss: 0.186523. Entropy: 1.053125.\n",
      "Iteration 3228: Policy loss: -0.009900. Value loss: 0.117667. Entropy: 1.056152.\n",
      "episode: 1564   score: 315.0  epsilon: 1.0    steps: 520  evaluation reward: 192.95\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3229: Policy loss: -0.000206. Value loss: 0.309667. Entropy: 1.141984.\n",
      "Iteration 3230: Policy loss: -0.006408. Value loss: 0.167203. Entropy: 1.147669.\n",
      "Iteration 3231: Policy loss: -0.012485. Value loss: 0.118477. Entropy: 1.127239.\n",
      "episode: 1565   score: 210.0  epsilon: 1.0    steps: 72  evaluation reward: 192.95\n",
      "episode: 1566   score: 170.0  epsilon: 1.0    steps: 184  evaluation reward: 192.55\n",
      "episode: 1567   score: 120.0  epsilon: 1.0    steps: 480  evaluation reward: 188.55\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3232: Policy loss: 0.003694. Value loss: 0.960055. Entropy: 1.183618.\n",
      "Iteration 3233: Policy loss: 0.004060. Value loss: 0.363676. Entropy: 1.193467.\n",
      "Iteration 3234: Policy loss: -0.004241. Value loss: 0.361807. Entropy: 1.169389.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3235: Policy loss: 0.002059. Value loss: 0.388361. Entropy: 0.917604.\n",
      "Iteration 3236: Policy loss: -0.004949. Value loss: 0.151331. Entropy: 0.977209.\n",
      "Iteration 3237: Policy loss: -0.010715. Value loss: 0.094880. Entropy: 0.956577.\n",
      "episode: 1568   score: 410.0  epsilon: 1.0    steps: 128  evaluation reward: 190.85\n",
      "episode: 1569   score: 125.0  epsilon: 1.0    steps: 640  evaluation reward: 190.0\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3238: Policy loss: 0.001705. Value loss: 0.515808. Entropy: 1.200094.\n",
      "Iteration 3239: Policy loss: 0.000842. Value loss: 0.520294. Entropy: 1.205133.\n",
      "Iteration 3240: Policy loss: -0.004892. Value loss: 0.274917. Entropy: 1.207061.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3241: Policy loss: 0.003132. Value loss: 0.323195. Entropy: 1.092223.\n",
      "Iteration 3242: Policy loss: -0.001423. Value loss: 0.135639. Entropy: 1.091866.\n",
      "Iteration 3243: Policy loss: -0.008447. Value loss: 0.095531. Entropy: 1.066410.\n",
      "episode: 1570   score: 105.0  epsilon: 1.0    steps: 616  evaluation reward: 188.9\n",
      "episode: 1571   score: 75.0  epsilon: 1.0    steps: 904  evaluation reward: 188.3\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3244: Policy loss: 0.000494. Value loss: 0.373801. Entropy: 1.293755.\n",
      "Iteration 3245: Policy loss: -0.006675. Value loss: 0.166488. Entropy: 1.287840.\n",
      "Iteration 3246: Policy loss: -0.010119. Value loss: 0.124841. Entropy: 1.283290.\n",
      "episode: 1572   score: 165.0  epsilon: 1.0    steps: 856  evaluation reward: 187.85\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3247: Policy loss: 0.001070. Value loss: 0.201148. Entropy: 1.125289.\n",
      "Iteration 3248: Policy loss: -0.005202. Value loss: 0.099767. Entropy: 1.142008.\n",
      "Iteration 3249: Policy loss: -0.007430. Value loss: 0.066186. Entropy: 1.114549.\n",
      "episode: 1573   score: 415.0  epsilon: 1.0    steps: 592  evaluation reward: 189.6\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3250: Policy loss: 0.006846. Value loss: 0.482960. Entropy: 1.168889.\n",
      "Iteration 3251: Policy loss: -0.004806. Value loss: 0.225152. Entropy: 1.180988.\n",
      "Iteration 3252: Policy loss: -0.007250. Value loss: 0.165772. Entropy: 1.181318.\n",
      "episode: 1574   score: 180.0  epsilon: 1.0    steps: 1000  evaluation reward: 189.3\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3253: Policy loss: -0.002003. Value loss: 0.588438. Entropy: 1.154511.\n",
      "Iteration 3254: Policy loss: -0.004913. Value loss: 0.238929. Entropy: 1.161854.\n",
      "Iteration 3255: Policy loss: -0.007661. Value loss: 0.129889. Entropy: 1.178742.\n",
      "episode: 1575   score: 125.0  epsilon: 1.0    steps: 392  evaluation reward: 188.45\n",
      "episode: 1576   score: 160.0  epsilon: 1.0    steps: 728  evaluation reward: 188.7\n",
      "episode: 1577   score: 210.0  epsilon: 1.0    steps: 800  evaluation reward: 188.1\n",
      "episode: 1578   score: 235.0  epsilon: 1.0    steps: 912  evaluation reward: 188.3\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3256: Policy loss: -0.001129. Value loss: 0.215221. Entropy: 1.159193.\n",
      "Iteration 3257: Policy loss: -0.008336. Value loss: 0.130122. Entropy: 1.180827.\n",
      "Iteration 3258: Policy loss: -0.010705. Value loss: 0.122221. Entropy: 1.185211.\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3259: Policy loss: 0.001050. Value loss: 0.253437. Entropy: 0.970964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3260: Policy loss: -0.006640. Value loss: 0.130144. Entropy: 1.000992.\n",
      "Iteration 3261: Policy loss: -0.007484. Value loss: 0.078056. Entropy: 0.980741.\n",
      "episode: 1579   score: 90.0  epsilon: 1.0    steps: 624  evaluation reward: 187.85\n",
      "episode: 1580   score: 240.0  epsilon: 1.0    steps: 904  evaluation reward: 188.45\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3262: Policy loss: 0.000018. Value loss: 0.329305. Entropy: 1.133596.\n",
      "Iteration 3263: Policy loss: 0.001967. Value loss: 0.147876. Entropy: 1.145369.\n",
      "Iteration 3264: Policy loss: -0.005819. Value loss: 0.107389. Entropy: 1.147061.\n",
      "episode: 1581   score: 210.0  epsilon: 1.0    steps: 448  evaluation reward: 188.1\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3265: Policy loss: 0.002863. Value loss: 0.264493. Entropy: 1.010847.\n",
      "Iteration 3266: Policy loss: -0.001691. Value loss: 0.117035. Entropy: 1.037885.\n",
      "Iteration 3267: Policy loss: -0.002488. Value loss: 0.082094. Entropy: 1.034982.\n",
      "episode: 1582   score: 130.0  epsilon: 1.0    steps: 592  evaluation reward: 185.85\n",
      "episode: 1583   score: 185.0  epsilon: 1.0    steps: 704  evaluation reward: 185.6\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3268: Policy loss: -0.001931. Value loss: 0.408086. Entropy: 1.161757.\n",
      "Iteration 3269: Policy loss: -0.003117. Value loss: 0.162001. Entropy: 1.158056.\n",
      "Iteration 3270: Policy loss: -0.007133. Value loss: 0.114181. Entropy: 1.153483.\n",
      "episode: 1584   score: 155.0  epsilon: 1.0    steps: 80  evaluation reward: 185.95\n",
      "episode: 1585   score: 135.0  epsilon: 1.0    steps: 296  evaluation reward: 185.2\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3271: Policy loss: -0.000905. Value loss: 0.174502. Entropy: 1.083332.\n",
      "Iteration 3272: Policy loss: -0.008185. Value loss: 0.095865. Entropy: 1.084090.\n",
      "Iteration 3273: Policy loss: -0.011574. Value loss: 0.069726. Entropy: 1.095195.\n",
      "episode: 1586   score: 155.0  epsilon: 1.0    steps: 952  evaluation reward: 185.65\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3274: Policy loss: 0.001314. Value loss: 0.341061. Entropy: 0.981499.\n",
      "Iteration 3275: Policy loss: 0.000335. Value loss: 0.161375. Entropy: 0.950909.\n",
      "Iteration 3276: Policy loss: -0.003762. Value loss: 0.099872. Entropy: 0.945575.\n",
      "episode: 1587   score: 125.0  epsilon: 1.0    steps: 504  evaluation reward: 184.8\n",
      "episode: 1588   score: 105.0  epsilon: 1.0    steps: 640  evaluation reward: 185.4\n",
      "episode: 1589   score: 105.0  epsilon: 1.0    steps: 696  evaluation reward: 184.65\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3277: Policy loss: -0.001128. Value loss: 0.262780. Entropy: 1.235630.\n",
      "Iteration 3278: Policy loss: 0.003121. Value loss: 0.114668. Entropy: 1.234089.\n",
      "Iteration 3279: Policy loss: -0.007897. Value loss: 0.071828. Entropy: 1.241141.\n",
      "episode: 1590   score: 180.0  epsilon: 1.0    steps: 136  evaluation reward: 183.9\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3280: Policy loss: -0.002942. Value loss: 0.211973. Entropy: 0.952624.\n",
      "Iteration 3281: Policy loss: -0.001598. Value loss: 0.113340. Entropy: 0.954682.\n",
      "Iteration 3282: Policy loss: -0.008594. Value loss: 0.083979. Entropy: 0.970088.\n",
      "episode: 1591   score: 125.0  epsilon: 1.0    steps: 992  evaluation reward: 182.9\n",
      "episode: 1592   score: 110.0  epsilon: 1.0    steps: 1008  evaluation reward: 182.45\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3283: Policy loss: 0.001591. Value loss: 0.360965. Entropy: 1.225562.\n",
      "Iteration 3284: Policy loss: -0.004014. Value loss: 0.163544. Entropy: 1.225191.\n",
      "Iteration 3285: Policy loss: -0.009659. Value loss: 0.116907. Entropy: 1.223892.\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3286: Policy loss: 0.001662. Value loss: 0.249008. Entropy: 1.114802.\n",
      "Iteration 3287: Policy loss: -0.002463. Value loss: 0.122051. Entropy: 1.116717.\n",
      "Iteration 3288: Policy loss: -0.009175. Value loss: 0.078232. Entropy: 1.119812.\n",
      "episode: 1593   score: 80.0  epsilon: 1.0    steps: 136  evaluation reward: 181.15\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3289: Policy loss: -0.002241. Value loss: 0.228127. Entropy: 1.219858.\n",
      "Iteration 3290: Policy loss: -0.006070. Value loss: 0.073005. Entropy: 1.233600.\n",
      "Iteration 3291: Policy loss: -0.013573. Value loss: 0.060735. Entropy: 1.239471.\n",
      "episode: 1594   score: 300.0  epsilon: 1.0    steps: 192  evaluation reward: 182.95\n",
      "episode: 1595   score: 215.0  epsilon: 1.0    steps: 456  evaluation reward: 182.95\n",
      "episode: 1596   score: 210.0  epsilon: 1.0    steps: 608  evaluation reward: 184.25\n",
      "episode: 1597   score: 135.0  epsilon: 1.0    steps: 680  evaluation reward: 181.95\n",
      "episode: 1598   score: 15.0  epsilon: 1.0    steps: 888  evaluation reward: 181.6\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3292: Policy loss: 0.001594. Value loss: 0.246625. Entropy: 1.185366.\n",
      "Iteration 3293: Policy loss: 0.002718. Value loss: 0.123956. Entropy: 1.180220.\n",
      "Iteration 3294: Policy loss: -0.008844. Value loss: 0.104162. Entropy: 1.192521.\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3295: Policy loss: 0.000702. Value loss: 0.437807. Entropy: 0.974862.\n",
      "Iteration 3296: Policy loss: -0.002300. Value loss: 0.161573. Entropy: 0.993624.\n",
      "Iteration 3297: Policy loss: -0.008589. Value loss: 0.122844. Entropy: 0.984169.\n",
      "episode: 1599   score: 105.0  epsilon: 1.0    steps: 200  evaluation reward: 180.35\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3298: Policy loss: 0.006023. Value loss: 0.455269. Entropy: 1.256959.\n",
      "Iteration 3299: Policy loss: 0.001060. Value loss: 0.169210. Entropy: 1.259971.\n",
      "Iteration 3300: Policy loss: -0.005013. Value loss: 0.118784. Entropy: 1.260140.\n",
      "episode: 1600   score: 335.0  epsilon: 1.0    steps: 144  evaluation reward: 181.9\n",
      "now time :  2019-02-28 11:34:23.296890\n",
      "episode: 1601   score: 30.0  epsilon: 1.0    steps: 760  evaluation reward: 180.7\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3301: Policy loss: 0.007111. Value loss: 0.376171. Entropy: 1.062477.\n",
      "Iteration 3302: Policy loss: -0.003925. Value loss: 0.163375. Entropy: 1.067451.\n",
      "Iteration 3303: Policy loss: -0.005067. Value loss: 0.099775. Entropy: 1.060354.\n",
      "episode: 1602   score: 210.0  epsilon: 1.0    steps: 168  evaluation reward: 180.5\n",
      "episode: 1603   score: 80.0  epsilon: 1.0    steps: 616  evaluation reward: 179.2\n",
      "episode: 1604   score: 105.0  epsilon: 1.0    steps: 720  evaluation reward: 178.7\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3304: Policy loss: 0.003725. Value loss: 0.322556. Entropy: 1.068885.\n",
      "Iteration 3305: Policy loss: -0.000097. Value loss: 0.158377. Entropy: 1.059133.\n",
      "Iteration 3306: Policy loss: -0.003393. Value loss: 0.140322. Entropy: 1.057134.\n",
      "episode: 1605   score: 180.0  epsilon: 1.0    steps: 328  evaluation reward: 179.15\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3307: Policy loss: 0.000243. Value loss: 0.343170. Entropy: 1.073916.\n",
      "Iteration 3308: Policy loss: -0.003349. Value loss: 0.187585. Entropy: 1.065317.\n",
      "Iteration 3309: Policy loss: -0.003866. Value loss: 0.122457. Entropy: 1.108789.\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3310: Policy loss: -0.001973. Value loss: 0.294592. Entropy: 1.204164.\n",
      "Iteration 3311: Policy loss: -0.009476. Value loss: 0.157820. Entropy: 1.192832.\n",
      "Iteration 3312: Policy loss: -0.008499. Value loss: 0.098441. Entropy: 1.193667.\n",
      "episode: 1606   score: 210.0  epsilon: 1.0    steps: 16  evaluation reward: 179.15\n",
      "episode: 1607   score: 155.0  epsilon: 1.0    steps: 576  evaluation reward: 179.15\n",
      "episode: 1608   score: 135.0  epsilon: 1.0    steps: 592  evaluation reward: 179.6\n",
      "episode: 1609   score: 180.0  epsilon: 1.0    steps: 744  evaluation reward: 180.05\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3313: Policy loss: -0.001055. Value loss: 0.320692. Entropy: 1.151635.\n",
      "Iteration 3314: Policy loss: -0.008474. Value loss: 0.125680. Entropy: 1.132915.\n",
      "Iteration 3315: Policy loss: -0.008696. Value loss: 0.074051. Entropy: 1.136365.\n",
      "episode: 1610   score: 210.0  epsilon: 1.0    steps: 328  evaluation reward: 180.6\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3316: Policy loss: 0.007401. Value loss: 0.345294. Entropy: 1.062836.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3317: Policy loss: -0.002949. Value loss: 0.116649. Entropy: 1.060730.\n",
      "Iteration 3318: Policy loss: -0.007105. Value loss: 0.089353. Entropy: 1.056739.\n",
      "episode: 1611   score: 210.0  epsilon: 1.0    steps: 24  evaluation reward: 180.55\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3319: Policy loss: 0.002242. Value loss: 0.183096. Entropy: 1.132808.\n",
      "Iteration 3320: Policy loss: -0.003518. Value loss: 0.096212. Entropy: 1.151816.\n",
      "Iteration 3321: Policy loss: -0.004602. Value loss: 0.062122. Entropy: 1.140330.\n",
      "episode: 1612   score: 80.0  epsilon: 1.0    steps: 616  evaluation reward: 180.05\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3322: Policy loss: 0.002821. Value loss: 0.409015. Entropy: 1.097686.\n",
      "Iteration 3323: Policy loss: -0.001472. Value loss: 0.170649. Entropy: 1.102223.\n",
      "Iteration 3324: Policy loss: -0.009705. Value loss: 0.141543. Entropy: 1.105790.\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3325: Policy loss: -0.000903. Value loss: 0.323798. Entropy: 1.077003.\n",
      "Iteration 3326: Policy loss: 0.000255. Value loss: 0.108881. Entropy: 1.084178.\n",
      "Iteration 3327: Policy loss: -0.006586. Value loss: 0.075922. Entropy: 1.082849.\n",
      "episode: 1613   score: 180.0  epsilon: 1.0    steps: 200  evaluation reward: 180.05\n",
      "episode: 1614   score: 120.0  epsilon: 1.0    steps: 704  evaluation reward: 179.15\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3328: Policy loss: 0.002579. Value loss: 0.336441. Entropy: 1.303355.\n",
      "Iteration 3329: Policy loss: -0.003123. Value loss: 0.126109. Entropy: 1.310724.\n",
      "Iteration 3330: Policy loss: -0.007811. Value loss: 0.092997. Entropy: 1.312276.\n",
      "episode: 1615   score: 180.0  epsilon: 1.0    steps: 304  evaluation reward: 177.9\n",
      "episode: 1616   score: 215.0  epsilon: 1.0    steps: 456  evaluation reward: 178.7\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3331: Policy loss: 0.002567. Value loss: 0.372206. Entropy: 1.075709.\n",
      "Iteration 3332: Policy loss: -0.003739. Value loss: 0.131522. Entropy: 1.084734.\n",
      "Iteration 3333: Policy loss: -0.008425. Value loss: 0.094459. Entropy: 1.078903.\n",
      "episode: 1617   score: 125.0  epsilon: 1.0    steps: 736  evaluation reward: 178.85\n",
      "episode: 1618   score: 195.0  epsilon: 1.0    steps: 1024  evaluation reward: 176.65\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3334: Policy loss: 0.001659. Value loss: 0.405146. Entropy: 1.043726.\n",
      "Iteration 3335: Policy loss: 0.000772. Value loss: 0.166295. Entropy: 1.062814.\n",
      "Iteration 3336: Policy loss: -0.003092. Value loss: 0.102800. Entropy: 1.080388.\n",
      "episode: 1619   score: 265.0  epsilon: 1.0    steps: 40  evaluation reward: 178.3\n",
      "episode: 1620   score: 245.0  epsilon: 1.0    steps: 80  evaluation reward: 178.6\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3337: Policy loss: 0.001793. Value loss: 0.410885. Entropy: 1.083158.\n",
      "Iteration 3338: Policy loss: -0.003116. Value loss: 0.187425. Entropy: 1.102818.\n",
      "Iteration 3339: Policy loss: -0.003278. Value loss: 0.139396. Entropy: 1.116008.\n",
      "episode: 1621   score: 180.0  epsilon: 1.0    steps: 632  evaluation reward: 177.55\n",
      "episode: 1622   score: 125.0  epsilon: 1.0    steps: 992  evaluation reward: 177.55\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3340: Policy loss: -0.002143. Value loss: 0.414132. Entropy: 0.832455.\n",
      "Iteration 3341: Policy loss: -0.006551. Value loss: 0.143852. Entropy: 0.831935.\n",
      "Iteration 3342: Policy loss: -0.010157. Value loss: 0.127242. Entropy: 0.848397.\n",
      "episode: 1623   score: 210.0  epsilon: 1.0    steps: 224  evaluation reward: 175.7\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3343: Policy loss: 0.004810. Value loss: 0.707464. Entropy: 1.176375.\n",
      "Iteration 3344: Policy loss: 0.002439. Value loss: 0.223712. Entropy: 1.171120.\n",
      "Iteration 3345: Policy loss: -0.005385. Value loss: 0.173794. Entropy: 1.179393.\n",
      "episode: 1624   score: 110.0  epsilon: 1.0    steps: 632  evaluation reward: 174.7\n",
      "episode: 1625   score: 125.0  epsilon: 1.0    steps: 928  evaluation reward: 173.85\n",
      "episode: 1626   score: 125.0  epsilon: 1.0    steps: 960  evaluation reward: 174.45\n",
      "episode: 1627   score: 105.0  epsilon: 1.0    steps: 960  evaluation reward: 172.55\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3346: Policy loss: 0.003897. Value loss: 0.456234. Entropy: 1.022915.\n",
      "Iteration 3347: Policy loss: -0.000423. Value loss: 0.215371. Entropy: 1.021084.\n",
      "Iteration 3348: Policy loss: -0.005882. Value loss: 0.133475. Entropy: 1.014369.\n",
      "episode: 1628   score: 20.0  epsilon: 1.0    steps: 696  evaluation reward: 171.95\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3349: Policy loss: -0.000341. Value loss: 0.416130. Entropy: 1.096158.\n",
      "Iteration 3350: Policy loss: 0.000010. Value loss: 0.203414. Entropy: 1.114951.\n",
      "Iteration 3351: Policy loss: -0.006725. Value loss: 0.148480. Entropy: 1.105757.\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3352: Policy loss: -0.001313. Value loss: 0.368812. Entropy: 0.978439.\n",
      "Iteration 3353: Policy loss: -0.000444. Value loss: 0.126828. Entropy: 0.989007.\n",
      "Iteration 3354: Policy loss: -0.004533. Value loss: 0.105656. Entropy: 1.019529.\n",
      "episode: 1629   score: 125.0  epsilon: 1.0    steps: 552  evaluation reward: 172.15\n",
      "episode: 1630   score: 225.0  epsilon: 1.0    steps: 728  evaluation reward: 171.7\n",
      "episode: 1631   score: 80.0  epsilon: 1.0    steps: 832  evaluation reward: 170.1\n",
      "episode: 1632   score: 125.0  epsilon: 1.0    steps: 936  evaluation reward: 169.1\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3355: Policy loss: 0.001013. Value loss: 0.502673. Entropy: 1.235339.\n",
      "Iteration 3356: Policy loss: -0.003845. Value loss: 0.176617. Entropy: 1.215731.\n",
      "Iteration 3357: Policy loss: -0.006651. Value loss: 0.113667. Entropy: 1.232810.\n",
      "episode: 1633   score: 115.0  epsilon: 1.0    steps: 144  evaluation reward: 168.45\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3358: Policy loss: 0.004107. Value loss: 0.469584. Entropy: 1.059058.\n",
      "Iteration 3359: Policy loss: -0.001652. Value loss: 0.149165. Entropy: 1.098245.\n",
      "Iteration 3360: Policy loss: -0.007622. Value loss: 0.107460. Entropy: 1.084512.\n",
      "episode: 1634   score: 260.0  epsilon: 1.0    steps: 896  evaluation reward: 170.0\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3361: Policy loss: 0.005271. Value loss: 0.871229. Entropy: 1.218329.\n",
      "Iteration 3362: Policy loss: -0.000278. Value loss: 0.354972. Entropy: 1.228259.\n",
      "Iteration 3363: Policy loss: -0.004228. Value loss: 0.206482. Entropy: 1.214676.\n",
      "episode: 1635   score: 135.0  epsilon: 1.0    steps: 880  evaluation reward: 167.2\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3364: Policy loss: 0.006780. Value loss: 0.700721. Entropy: 1.126366.\n",
      "Iteration 3365: Policy loss: -0.002720. Value loss: 0.197217. Entropy: 1.144034.\n",
      "Iteration 3366: Policy loss: -0.006590. Value loss: 0.152790. Entropy: 1.135468.\n",
      "episode: 1636   score: 195.0  epsilon: 1.0    steps: 40  evaluation reward: 167.9\n",
      "episode: 1637   score: 100.0  epsilon: 1.0    steps: 936  evaluation reward: 168.0\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3367: Policy loss: 0.005155. Value loss: 0.748315. Entropy: 1.189409.\n",
      "Iteration 3368: Policy loss: -0.001518. Value loss: 0.348579. Entropy: 1.184630.\n",
      "Iteration 3369: Policy loss: -0.003683. Value loss: 0.250408. Entropy: 1.178740.\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3370: Policy loss: 0.003668. Value loss: 0.547427. Entropy: 1.191971.\n",
      "Iteration 3371: Policy loss: -0.000553. Value loss: 0.205016. Entropy: 1.197006.\n",
      "Iteration 3372: Policy loss: -0.004497. Value loss: 0.129776. Entropy: 1.200442.\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3373: Policy loss: 0.004488. Value loss: 0.551684. Entropy: 1.160877.\n",
      "Iteration 3374: Policy loss: 0.003506. Value loss: 0.194862. Entropy: 1.159630.\n",
      "Iteration 3375: Policy loss: -0.009013. Value loss: 0.125485. Entropy: 1.167695.\n",
      "episode: 1638   score: 215.0  epsilon: 1.0    steps: 768  evaluation reward: 169.1\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3376: Policy loss: 0.003337. Value loss: 0.772598. Entropy: 1.276628.\n",
      "Iteration 3377: Policy loss: -0.003104. Value loss: 0.402306. Entropy: 1.278792.\n",
      "Iteration 3378: Policy loss: -0.003593. Value loss: 0.366533. Entropy: 1.280061.\n",
      "episode: 1639   score: 100.0  epsilon: 1.0    steps: 80  evaluation reward: 169.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1640   score: 340.0  epsilon: 1.0    steps: 184  evaluation reward: 170.8\n",
      "episode: 1641   score: 75.0  epsilon: 1.0    steps: 384  evaluation reward: 169.75\n",
      "episode: 1642   score: 195.0  epsilon: 1.0    steps: 656  evaluation reward: 168.85\n",
      "episode: 1643   score: 140.0  epsilon: 1.0    steps: 976  evaluation reward: 168.15\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3379: Policy loss: 0.005797. Value loss: 0.580047. Entropy: 1.143886.\n",
      "Iteration 3380: Policy loss: -0.001575. Value loss: 0.449080. Entropy: 1.174261.\n",
      "Iteration 3381: Policy loss: -0.004633. Value loss: 0.298541. Entropy: 1.145336.\n",
      "episode: 1644   score: 235.0  epsilon: 1.0    steps: 176  evaluation reward: 169.7\n",
      "episode: 1645   score: 430.0  epsilon: 1.0    steps: 288  evaluation reward: 171.9\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3382: Policy loss: 0.003936. Value loss: 0.335098. Entropy: 1.069512.\n",
      "Iteration 3383: Policy loss: 0.000534. Value loss: 0.227004. Entropy: 1.062955.\n",
      "Iteration 3384: Policy loss: -0.004544. Value loss: 0.151407. Entropy: 1.069019.\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3385: Policy loss: -0.001247. Value loss: 0.401953. Entropy: 0.967648.\n",
      "Iteration 3386: Policy loss: -0.004617. Value loss: 0.284910. Entropy: 0.965835.\n",
      "Iteration 3387: Policy loss: -0.010918. Value loss: 0.160930. Entropy: 0.996340.\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3388: Policy loss: 0.004011. Value loss: 0.682990. Entropy: 1.195192.\n",
      "Iteration 3389: Policy loss: -0.002169. Value loss: 0.294459. Entropy: 1.181562.\n",
      "Iteration 3390: Policy loss: -0.005534. Value loss: 0.203497. Entropy: 1.192760.\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3391: Policy loss: 0.002843. Value loss: 0.599507. Entropy: 1.262502.\n",
      "Iteration 3392: Policy loss: -0.002939. Value loss: 0.217022. Entropy: 1.260581.\n",
      "Iteration 3393: Policy loss: -0.011278. Value loss: 0.137625. Entropy: 1.258991.\n",
      "episode: 1646   score: 165.0  epsilon: 1.0    steps: 304  evaluation reward: 171.85\n",
      "episode: 1647   score: 210.0  epsilon: 1.0    steps: 480  evaluation reward: 173.05\n",
      "episode: 1648   score: 150.0  epsilon: 1.0    steps: 624  evaluation reward: 170.45\n",
      "episode: 1649   score: 255.0  epsilon: 1.0    steps: 920  evaluation reward: 171.9\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3394: Policy loss: -0.001664. Value loss: 0.807738. Entropy: 1.272249.\n",
      "Iteration 3395: Policy loss: -0.003894. Value loss: 0.369064. Entropy: 1.273778.\n",
      "Iteration 3396: Policy loss: -0.010053. Value loss: 0.193844. Entropy: 1.280764.\n",
      "episode: 1650   score: 200.0  epsilon: 1.0    steps: 1000  evaluation reward: 171.65\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3397: Policy loss: 0.002818. Value loss: 0.600375. Entropy: 1.089816.\n",
      "Iteration 3398: Policy loss: -0.000539. Value loss: 0.283907. Entropy: 1.105265.\n",
      "Iteration 3399: Policy loss: 0.001464. Value loss: 0.181619. Entropy: 1.110475.\n",
      "now time :  2019-02-28 11:35:34.633703\n",
      "episode: 1651   score: 145.0  epsilon: 1.0    steps: 576  evaluation reward: 171.0\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3400: Policy loss: -0.001053. Value loss: 0.704784. Entropy: 1.150176.\n",
      "Iteration 3401: Policy loss: -0.001731. Value loss: 0.376372. Entropy: 1.172487.\n",
      "Iteration 3402: Policy loss: -0.006926. Value loss: 0.281355. Entropy: 1.171590.\n",
      "episode: 1652   score: 410.0  epsilon: 1.0    steps: 512  evaluation reward: 172.75\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3403: Policy loss: -0.000326. Value loss: 0.543061. Entropy: 1.117280.\n",
      "Iteration 3404: Policy loss: -0.006849. Value loss: 0.225425. Entropy: 1.112318.\n",
      "Iteration 3405: Policy loss: -0.008731. Value loss: 0.148618. Entropy: 1.119695.\n",
      "episode: 1653   score: 275.0  epsilon: 1.0    steps: 816  evaluation reward: 174.4\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3406: Policy loss: 0.002563. Value loss: 0.515324. Entropy: 1.175331.\n",
      "Iteration 3407: Policy loss: -0.004769. Value loss: 0.216725. Entropy: 1.173894.\n",
      "Iteration 3408: Policy loss: -0.010706. Value loss: 0.153782. Entropy: 1.166716.\n",
      "episode: 1654   score: 105.0  epsilon: 1.0    steps: 56  evaluation reward: 173.65\n",
      "episode: 1655   score: 180.0  epsilon: 1.0    steps: 400  evaluation reward: 172.3\n",
      "episode: 1656   score: 155.0  epsilon: 1.0    steps: 424  evaluation reward: 173.05\n",
      "episode: 1657   score: 150.0  epsilon: 1.0    steps: 776  evaluation reward: 173.0\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3409: Policy loss: -0.000564. Value loss: 0.270690. Entropy: 1.194170.\n",
      "Iteration 3410: Policy loss: -0.001700. Value loss: 0.144727. Entropy: 1.184817.\n",
      "Iteration 3411: Policy loss: -0.007500. Value loss: 0.140189. Entropy: 1.184561.\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3412: Policy loss: -0.000410. Value loss: 0.504563. Entropy: 0.949929.\n",
      "Iteration 3413: Policy loss: -0.000577. Value loss: 0.196553. Entropy: 0.968285.\n",
      "Iteration 3414: Policy loss: -0.009191. Value loss: 0.137530. Entropy: 0.990578.\n",
      "episode: 1658   score: 180.0  epsilon: 1.0    steps: 880  evaluation reward: 173.75\n",
      "episode: 1659   score: 125.0  epsilon: 1.0    steps: 1000  evaluation reward: 173.45\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3415: Policy loss: 0.001733. Value loss: 0.496873. Entropy: 1.222871.\n",
      "Iteration 3416: Policy loss: -0.001024. Value loss: 0.180958. Entropy: 1.223536.\n",
      "Iteration 3417: Policy loss: -0.003387. Value loss: 0.137602. Entropy: 1.223740.\n",
      "episode: 1660   score: 55.0  epsilon: 1.0    steps: 360  evaluation reward: 173.35\n",
      "episode: 1661   score: 165.0  epsilon: 1.0    steps: 992  evaluation reward: 173.45\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3418: Policy loss: -0.001391. Value loss: 0.445441. Entropy: 1.169832.\n",
      "Iteration 3419: Policy loss: 0.000169. Value loss: 0.213608. Entropy: 1.171900.\n",
      "Iteration 3420: Policy loss: -0.007915. Value loss: 0.169546. Entropy: 1.157381.\n",
      "episode: 1662   score: 150.0  epsilon: 1.0    steps: 736  evaluation reward: 172.05\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3421: Policy loss: 0.004890. Value loss: 0.602839. Entropy: 1.127686.\n",
      "Iteration 3422: Policy loss: -0.001886. Value loss: 0.244262. Entropy: 1.110887.\n",
      "Iteration 3423: Policy loss: -0.008403. Value loss: 0.165692. Entropy: 1.124642.\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3424: Policy loss: 0.001378. Value loss: 0.418921. Entropy: 1.181765.\n",
      "Iteration 3425: Policy loss: -0.005647. Value loss: 0.186582. Entropy: 1.190197.\n",
      "Iteration 3426: Policy loss: -0.012732. Value loss: 0.113880. Entropy: 1.170291.\n",
      "episode: 1663   score: 60.0  epsilon: 1.0    steps: 280  evaluation reward: 170.0\n",
      "episode: 1664   score: 240.0  epsilon: 1.0    steps: 424  evaluation reward: 169.25\n",
      "episode: 1665   score: 430.0  epsilon: 1.0    steps: 872  evaluation reward: 171.45\n",
      "episode: 1666   score: 105.0  epsilon: 1.0    steps: 936  evaluation reward: 170.8\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3427: Policy loss: 0.001812. Value loss: 0.566498. Entropy: 1.210239.\n",
      "Iteration 3428: Policy loss: -0.008278. Value loss: 0.204037. Entropy: 1.223062.\n",
      "Iteration 3429: Policy loss: -0.012154. Value loss: 0.161635. Entropy: 1.223248.\n",
      "episode: 1667   score: 90.0  epsilon: 1.0    steps: 584  evaluation reward: 170.5\n",
      "episode: 1668   score: 285.0  epsilon: 1.0    steps: 696  evaluation reward: 169.25\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3430: Policy loss: -0.000196. Value loss: 0.359029. Entropy: 1.078325.\n",
      "Iteration 3431: Policy loss: -0.003162. Value loss: 0.190062. Entropy: 1.099056.\n",
      "Iteration 3432: Policy loss: -0.008490. Value loss: 0.143030. Entropy: 1.088678.\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3433: Policy loss: 0.005386. Value loss: 0.461350. Entropy: 1.043311.\n",
      "Iteration 3434: Policy loss: -0.001928. Value loss: 0.210116. Entropy: 1.056403.\n",
      "Iteration 3435: Policy loss: -0.004457. Value loss: 0.135370. Entropy: 1.058725.\n",
      "episode: 1669   score: 210.0  epsilon: 1.0    steps: 16  evaluation reward: 170.1\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3436: Policy loss: 0.004958. Value loss: 0.406042. Entropy: 1.081818.\n",
      "Iteration 3437: Policy loss: -0.002587. Value loss: 0.172934. Entropy: 1.095862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3438: Policy loss: -0.012190. Value loss: 0.116836. Entropy: 1.103320.\n",
      "episode: 1670   score: 210.0  epsilon: 1.0    steps: 144  evaluation reward: 171.15\n",
      "episode: 1671   score: 155.0  epsilon: 1.0    steps: 912  evaluation reward: 171.95\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3439: Policy loss: -0.002930. Value loss: 0.384771. Entropy: 1.231881.\n",
      "Iteration 3440: Policy loss: -0.005144. Value loss: 0.120660. Entropy: 1.244924.\n",
      "Iteration 3441: Policy loss: -0.007744. Value loss: 0.072340. Entropy: 1.233932.\n",
      "episode: 1672   score: 120.0  epsilon: 1.0    steps: 384  evaluation reward: 171.5\n",
      "episode: 1673   score: 155.0  epsilon: 1.0    steps: 632  evaluation reward: 168.9\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3442: Policy loss: 0.001044. Value loss: 0.250109. Entropy: 1.174413.\n",
      "Iteration 3443: Policy loss: -0.003751. Value loss: 0.138943. Entropy: 1.186875.\n",
      "Iteration 3444: Policy loss: -0.008598. Value loss: 0.088729. Entropy: 1.174120.\n",
      "episode: 1674   score: 210.0  epsilon: 1.0    steps: 176  evaluation reward: 169.2\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3445: Policy loss: 0.000178. Value loss: 0.320283. Entropy: 1.042557.\n",
      "Iteration 3446: Policy loss: -0.002008. Value loss: 0.167925. Entropy: 1.094970.\n",
      "Iteration 3447: Policy loss: -0.011494. Value loss: 0.100836. Entropy: 1.089437.\n",
      "episode: 1675   score: 135.0  epsilon: 1.0    steps: 344  evaluation reward: 169.3\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3448: Policy loss: 0.005017. Value loss: 0.346943. Entropy: 1.217114.\n",
      "Iteration 3449: Policy loss: -0.001694. Value loss: 0.141790. Entropy: 1.212927.\n",
      "Iteration 3450: Policy loss: -0.007322. Value loss: 0.110366. Entropy: 1.206115.\n",
      "episode: 1676   score: 220.0  epsilon: 1.0    steps: 8  evaluation reward: 169.9\n",
      "episode: 1677   score: 180.0  epsilon: 1.0    steps: 360  evaluation reward: 169.6\n",
      "episode: 1678   score: 315.0  epsilon: 1.0    steps: 640  evaluation reward: 170.4\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3451: Policy loss: 0.001646. Value loss: 0.306531. Entropy: 1.163218.\n",
      "Iteration 3452: Policy loss: -0.002914. Value loss: 0.165855. Entropy: 1.163991.\n",
      "Iteration 3453: Policy loss: -0.008926. Value loss: 0.103104. Entropy: 1.160226.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3454: Policy loss: 0.000641. Value loss: 0.377294. Entropy: 1.010941.\n",
      "Iteration 3455: Policy loss: -0.005660. Value loss: 0.158049. Entropy: 0.996019.\n",
      "Iteration 3456: Policy loss: -0.008047. Value loss: 0.112919. Entropy: 1.016766.\n",
      "episode: 1679   score: 120.0  epsilon: 1.0    steps: 96  evaluation reward: 170.7\n",
      "episode: 1680   score: 90.0  epsilon: 1.0    steps: 480  evaluation reward: 169.2\n",
      "episode: 1681   score: 240.0  epsilon: 1.0    steps: 512  evaluation reward: 169.5\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3457: Policy loss: -0.000088. Value loss: 0.319159. Entropy: 1.204333.\n",
      "Iteration 3458: Policy loss: -0.006426. Value loss: 0.176362. Entropy: 1.222590.\n",
      "Iteration 3459: Policy loss: -0.007405. Value loss: 0.129757. Entropy: 1.224794.\n",
      "episode: 1682   score: 215.0  epsilon: 1.0    steps: 736  evaluation reward: 170.35\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3460: Policy loss: 0.002589. Value loss: 0.344965. Entropy: 1.038138.\n",
      "Iteration 3461: Policy loss: -0.002938. Value loss: 0.186673. Entropy: 1.052277.\n",
      "Iteration 3462: Policy loss: -0.006671. Value loss: 0.128424. Entropy: 1.056419.\n",
      "episode: 1683   score: 155.0  epsilon: 1.0    steps: 128  evaluation reward: 170.05\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3463: Policy loss: -0.002313. Value loss: 0.392749. Entropy: 1.215181.\n",
      "Iteration 3464: Policy loss: -0.007814. Value loss: 0.191931. Entropy: 1.217418.\n",
      "Iteration 3465: Policy loss: -0.012742. Value loss: 0.110772. Entropy: 1.221693.\n",
      "episode: 1684   score: 55.0  epsilon: 1.0    steps: 432  evaluation reward: 169.05\n",
      "episode: 1685   score: 135.0  epsilon: 1.0    steps: 712  evaluation reward: 169.05\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3466: Policy loss: 0.002834. Value loss: 0.475981. Entropy: 1.214239.\n",
      "Iteration 3467: Policy loss: -0.003571. Value loss: 0.245035. Entropy: 1.220709.\n",
      "Iteration 3468: Policy loss: -0.010573. Value loss: 0.150133. Entropy: 1.218486.\n",
      "episode: 1686   score: 135.0  epsilon: 1.0    steps: 160  evaluation reward: 168.85\n",
      "episode: 1687   score: 290.0  epsilon: 1.0    steps: 504  evaluation reward: 170.5\n",
      "episode: 1688   score: 150.0  epsilon: 1.0    steps: 552  evaluation reward: 170.95\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3469: Policy loss: 0.002694. Value loss: 0.283491. Entropy: 1.054839.\n",
      "Iteration 3470: Policy loss: -0.002934. Value loss: 0.131858. Entropy: 1.067586.\n",
      "Iteration 3471: Policy loss: -0.002778. Value loss: 0.089710. Entropy: 1.025089.\n",
      "episode: 1689   score: 245.0  epsilon: 1.0    steps: 720  evaluation reward: 172.35\n",
      "episode: 1690   score: 155.0  epsilon: 1.0    steps: 992  evaluation reward: 172.1\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3472: Policy loss: -0.002125. Value loss: 0.258281. Entropy: 1.058818.\n",
      "Iteration 3473: Policy loss: -0.004229. Value loss: 0.160951. Entropy: 1.073078.\n",
      "Iteration 3474: Policy loss: -0.006347. Value loss: 0.116463. Entropy: 1.065893.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3475: Policy loss: 0.003473. Value loss: 0.349530. Entropy: 1.042235.\n",
      "Iteration 3476: Policy loss: -0.002906. Value loss: 0.149822. Entropy: 1.023781.\n",
      "Iteration 3477: Policy loss: -0.005218. Value loss: 0.100352. Entropy: 1.040356.\n",
      "episode: 1691   score: 210.0  epsilon: 1.0    steps: 344  evaluation reward: 172.95\n",
      "episode: 1692   score: 180.0  epsilon: 1.0    steps: 672  evaluation reward: 173.65\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3478: Policy loss: 0.004825. Value loss: 0.407953. Entropy: 1.120678.\n",
      "Iteration 3479: Policy loss: -0.000660. Value loss: 0.161079. Entropy: 1.132490.\n",
      "Iteration 3480: Policy loss: -0.008973. Value loss: 0.106122. Entropy: 1.126730.\n",
      "episode: 1693   score: 135.0  epsilon: 1.0    steps: 160  evaluation reward: 174.2\n",
      "episode: 1694   score: 180.0  epsilon: 1.0    steps: 496  evaluation reward: 173.0\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3481: Policy loss: 0.003321. Value loss: 0.362967. Entropy: 1.012283.\n",
      "Iteration 3482: Policy loss: 0.002081. Value loss: 0.167213. Entropy: 1.027785.\n",
      "Iteration 3483: Policy loss: -0.006378. Value loss: 0.119522. Entropy: 1.046201.\n",
      "episode: 1695   score: 155.0  epsilon: 1.0    steps: 192  evaluation reward: 172.4\n",
      "episode: 1696   score: 135.0  epsilon: 1.0    steps: 608  evaluation reward: 171.65\n",
      "episode: 1697   score: 150.0  epsilon: 1.0    steps: 864  evaluation reward: 171.8\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3484: Policy loss: -0.002496. Value loss: 0.334524. Entropy: 1.121160.\n",
      "Iteration 3485: Policy loss: -0.006151. Value loss: 0.142040. Entropy: 1.131639.\n",
      "Iteration 3486: Policy loss: -0.011180. Value loss: 0.100885. Entropy: 1.127315.\n",
      "episode: 1698   score: 210.0  epsilon: 1.0    steps: 648  evaluation reward: 173.75\n",
      "episode: 1699   score: 125.0  epsilon: 1.0    steps: 744  evaluation reward: 173.95\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3487: Policy loss: -0.001703. Value loss: 0.269742. Entropy: 1.020449.\n",
      "Iteration 3488: Policy loss: -0.006572. Value loss: 0.141795. Entropy: 1.055272.\n",
      "Iteration 3489: Policy loss: -0.009906. Value loss: 0.125290. Entropy: 1.046301.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3490: Policy loss: 0.001904. Value loss: 0.320487. Entropy: 1.126605.\n",
      "Iteration 3491: Policy loss: -0.003587. Value loss: 0.157998. Entropy: 1.134974.\n",
      "Iteration 3492: Policy loss: -0.005338. Value loss: 0.132764. Entropy: 1.128018.\n",
      "episode: 1700   score: 165.0  epsilon: 1.0    steps: 208  evaluation reward: 172.25\n",
      "now time :  2019-02-28 11:36:41.490260\n",
      "episode: 1701   score: 120.0  epsilon: 1.0    steps: 312  evaluation reward: 173.15\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3493: Policy loss: 0.008382. Value loss: 0.462280. Entropy: 1.204103.\n",
      "Iteration 3494: Policy loss: -0.002135. Value loss: 0.193598. Entropy: 1.191430.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3495: Policy loss: -0.005079. Value loss: 0.137035. Entropy: 1.197548.\n",
      "episode: 1702   score: 125.0  epsilon: 1.0    steps: 768  evaluation reward: 172.3\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3496: Policy loss: 0.000631. Value loss: 0.391856. Entropy: 1.175730.\n",
      "Iteration 3497: Policy loss: -0.005180. Value loss: 0.188867. Entropy: 1.192509.\n",
      "Iteration 3498: Policy loss: -0.011210. Value loss: 0.103833. Entropy: 1.186079.\n",
      "episode: 1703   score: 210.0  epsilon: 1.0    steps: 424  evaluation reward: 173.6\n",
      "episode: 1704   score: 155.0  epsilon: 1.0    steps: 944  evaluation reward: 174.1\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3499: Policy loss: 0.001387. Value loss: 0.428607. Entropy: 1.149793.\n",
      "Iteration 3500: Policy loss: -0.001778. Value loss: 0.190704. Entropy: 1.170422.\n",
      "Iteration 3501: Policy loss: -0.005241. Value loss: 0.091081. Entropy: 1.150065.\n",
      "episode: 1705   score: 185.0  epsilon: 1.0    steps: 208  evaluation reward: 174.15\n",
      "episode: 1706   score: 210.0  epsilon: 1.0    steps: 400  evaluation reward: 174.15\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3502: Policy loss: -0.000268. Value loss: 0.359384. Entropy: 1.150152.\n",
      "Iteration 3503: Policy loss: -0.004439. Value loss: 0.124621. Entropy: 1.141387.\n",
      "Iteration 3504: Policy loss: -0.010892. Value loss: 0.086419. Entropy: 1.151354.\n",
      "episode: 1707   score: 125.0  epsilon: 1.0    steps: 336  evaluation reward: 173.85\n",
      "episode: 1708   score: 120.0  epsilon: 1.0    steps: 816  evaluation reward: 173.7\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3505: Policy loss: -0.000662. Value loss: 0.606628. Entropy: 1.232165.\n",
      "Iteration 3506: Policy loss: -0.003816. Value loss: 0.286614. Entropy: 1.233003.\n",
      "Iteration 3507: Policy loss: -0.009647. Value loss: 0.207320. Entropy: 1.227266.\n",
      "episode: 1709   score: 260.0  epsilon: 1.0    steps: 648  evaluation reward: 174.5\n",
      "episode: 1710   score: 135.0  epsilon: 1.0    steps: 848  evaluation reward: 173.75\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3508: Policy loss: 0.003764. Value loss: 0.385261. Entropy: 1.087586.\n",
      "Iteration 3509: Policy loss: -0.002336. Value loss: 0.203479. Entropy: 1.098776.\n",
      "Iteration 3510: Policy loss: -0.007844. Value loss: 0.136358. Entropy: 1.089710.\n",
      "episode: 1711   score: 135.0  epsilon: 1.0    steps: 856  evaluation reward: 173.0\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3511: Policy loss: 0.000871. Value loss: 0.430249. Entropy: 1.207886.\n",
      "Iteration 3512: Policy loss: -0.003998. Value loss: 0.119530. Entropy: 1.198246.\n",
      "Iteration 3513: Policy loss: -0.007839. Value loss: 0.084250. Entropy: 1.213323.\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3514: Policy loss: 0.001835. Value loss: 0.646728. Entropy: 1.134104.\n",
      "Iteration 3515: Policy loss: -0.006453. Value loss: 0.229276. Entropy: 1.146652.\n",
      "Iteration 3516: Policy loss: -0.011825. Value loss: 0.118301. Entropy: 1.142784.\n",
      "episode: 1712   score: 210.0  epsilon: 1.0    steps: 32  evaluation reward: 174.3\n",
      "episode: 1713   score: 180.0  epsilon: 1.0    steps: 128  evaluation reward: 174.3\n",
      "episode: 1714   score: 105.0  epsilon: 1.0    steps: 472  evaluation reward: 174.15\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3517: Policy loss: 0.003077. Value loss: 0.672754. Entropy: 1.201650.\n",
      "Iteration 3518: Policy loss: -0.007209. Value loss: 0.270815. Entropy: 1.199030.\n",
      "Iteration 3519: Policy loss: -0.008770. Value loss: 0.227659. Entropy: 1.193154.\n",
      "episode: 1715   score: 125.0  epsilon: 1.0    steps: 968  evaluation reward: 173.6\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3520: Policy loss: 0.007104. Value loss: 1.091411. Entropy: 1.151301.\n",
      "Iteration 3521: Policy loss: 0.003648. Value loss: 0.382491. Entropy: 1.142458.\n",
      "Iteration 3522: Policy loss: 0.004280. Value loss: 0.246413. Entropy: 1.148889.\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3523: Policy loss: -0.001139. Value loss: 0.563847. Entropy: 1.197038.\n",
      "Iteration 3524: Policy loss: -0.005805. Value loss: 0.238230. Entropy: 1.182815.\n",
      "Iteration 3525: Policy loss: -0.011089. Value loss: 0.148766. Entropy: 1.199648.\n",
      "episode: 1716   score: 110.0  epsilon: 1.0    steps: 256  evaluation reward: 172.55\n",
      "episode: 1717   score: 55.0  epsilon: 1.0    steps: 320  evaluation reward: 171.85\n",
      "episode: 1718   score: 35.0  epsilon: 1.0    steps: 528  evaluation reward: 170.25\n",
      "episode: 1719   score: 240.0  epsilon: 1.0    steps: 872  evaluation reward: 170.0\n",
      "episode: 1720   score: 260.0  epsilon: 1.0    steps: 880  evaluation reward: 170.15\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3526: Policy loss: 0.004527. Value loss: 0.898606. Entropy: 1.241691.\n",
      "Iteration 3527: Policy loss: -0.001196. Value loss: 0.392535. Entropy: 1.246348.\n",
      "Iteration 3528: Policy loss: -0.009266. Value loss: 0.265106. Entropy: 1.244460.\n",
      "episode: 1721   score: 565.0  epsilon: 1.0    steps: 616  evaluation reward: 174.0\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3529: Policy loss: 0.002356. Value loss: 0.402480. Entropy: 0.922547.\n",
      "Iteration 3530: Policy loss: -0.000655. Value loss: 0.228340. Entropy: 0.952782.\n",
      "Iteration 3531: Policy loss: -0.008286. Value loss: 0.162683. Entropy: 0.957080.\n",
      "episode: 1722   score: 105.0  epsilon: 1.0    steps: 96  evaluation reward: 173.8\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3532: Policy loss: -0.001855. Value loss: 0.531126. Entropy: 1.201959.\n",
      "Iteration 3533: Policy loss: -0.001470. Value loss: 0.259432. Entropy: 1.210197.\n",
      "Iteration 3534: Policy loss: -0.011495. Value loss: 0.175793. Entropy: 1.210034.\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3535: Policy loss: 0.005931. Value loss: 0.617561. Entropy: 1.147523.\n",
      "Iteration 3536: Policy loss: 0.001340. Value loss: 0.271801. Entropy: 1.154979.\n",
      "Iteration 3537: Policy loss: -0.005702. Value loss: 0.195816. Entropy: 1.136024.\n",
      "episode: 1723   score: 135.0  epsilon: 1.0    steps: 600  evaluation reward: 173.05\n",
      "episode: 1724   score: 150.0  epsilon: 1.0    steps: 640  evaluation reward: 173.45\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3538: Policy loss: 0.003133. Value loss: 0.614836. Entropy: 1.171536.\n",
      "Iteration 3539: Policy loss: 0.000193. Value loss: 0.226351. Entropy: 1.188837.\n",
      "Iteration 3540: Policy loss: -0.006469. Value loss: 0.149890. Entropy: 1.182147.\n",
      "episode: 1725   score: 240.0  epsilon: 1.0    steps: 88  evaluation reward: 174.6\n",
      "episode: 1726   score: 105.0  epsilon: 1.0    steps: 128  evaluation reward: 174.4\n",
      "episode: 1727   score: 125.0  epsilon: 1.0    steps: 488  evaluation reward: 174.6\n",
      "episode: 1728   score: 295.0  epsilon: 1.0    steps: 1016  evaluation reward: 177.35\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3541: Policy loss: 0.002796. Value loss: 0.437912. Entropy: 1.165386.\n",
      "Iteration 3542: Policy loss: -0.000959. Value loss: 0.213379. Entropy: 1.162558.\n",
      "Iteration 3543: Policy loss: -0.003241. Value loss: 0.171826. Entropy: 1.163161.\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3544: Policy loss: -0.000779. Value loss: 0.397965. Entropy: 1.058207.\n",
      "Iteration 3545: Policy loss: -0.004332. Value loss: 0.194104. Entropy: 1.062072.\n",
      "Iteration 3546: Policy loss: -0.007627. Value loss: 0.145060. Entropy: 1.075571.\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3547: Policy loss: -0.002052. Value loss: 0.259069. Entropy: 1.173679.\n",
      "Iteration 3548: Policy loss: -0.007316. Value loss: 0.159825. Entropy: 1.164049.\n",
      "Iteration 3549: Policy loss: -0.012005. Value loss: 0.135923. Entropy: 1.182567.\n",
      "episode: 1729   score: 260.0  epsilon: 1.0    steps: 168  evaluation reward: 178.7\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3550: Policy loss: -0.001454. Value loss: 0.500702. Entropy: 1.284625.\n",
      "Iteration 3551: Policy loss: -0.003700. Value loss: 0.234482. Entropy: 1.280401.\n",
      "Iteration 3552: Policy loss: -0.006102. Value loss: 0.148902. Entropy: 1.289577.\n",
      "episode: 1730   score: 90.0  epsilon: 1.0    steps: 880  evaluation reward: 177.35\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3553: Policy loss: 0.002976. Value loss: 0.567878. Entropy: 1.245304.\n",
      "Iteration 3554: Policy loss: -0.000931. Value loss: 0.252072. Entropy: 1.247031.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3555: Policy loss: -0.007262. Value loss: 0.171582. Entropy: 1.249376.\n",
      "episode: 1731   score: 210.0  epsilon: 1.0    steps: 528  evaluation reward: 178.65\n",
      "episode: 1732   score: 165.0  epsilon: 1.0    steps: 824  evaluation reward: 179.05\n",
      "episode: 1733   score: 410.0  epsilon: 1.0    steps: 1008  evaluation reward: 182.0\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3556: Policy loss: 0.000870. Value loss: 0.595472. Entropy: 1.232334.\n",
      "Iteration 3557: Policy loss: -0.007530. Value loss: 0.230360. Entropy: 1.237109.\n",
      "Iteration 3558: Policy loss: -0.009260. Value loss: 0.160344. Entropy: 1.246895.\n",
      "episode: 1734   score: 225.0  epsilon: 1.0    steps: 288  evaluation reward: 181.65\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3559: Policy loss: 0.002522. Value loss: 0.604139. Entropy: 1.170772.\n",
      "Iteration 3560: Policy loss: -0.004524. Value loss: 0.301450. Entropy: 1.165966.\n",
      "Iteration 3561: Policy loss: -0.004975. Value loss: 0.227379. Entropy: 1.162426.\n",
      "episode: 1735   score: 215.0  epsilon: 1.0    steps: 184  evaluation reward: 182.45\n",
      "episode: 1736   score: 130.0  epsilon: 1.0    steps: 808  evaluation reward: 181.8\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3562: Policy loss: 0.000463. Value loss: 0.295789. Entropy: 1.199187.\n",
      "Iteration 3563: Policy loss: -0.004812. Value loss: 0.145550. Entropy: 1.198263.\n",
      "Iteration 3564: Policy loss: -0.010738. Value loss: 0.115340. Entropy: 1.188520.\n",
      "episode: 1737   score: 260.0  epsilon: 1.0    steps: 960  evaluation reward: 183.4\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3565: Policy loss: -0.002600. Value loss: 0.284528. Entropy: 1.137437.\n",
      "Iteration 3566: Policy loss: -0.003988. Value loss: 0.145131. Entropy: 1.153367.\n",
      "Iteration 3567: Policy loss: -0.010613. Value loss: 0.095594. Entropy: 1.144201.\n",
      "episode: 1738   score: 125.0  epsilon: 1.0    steps: 768  evaluation reward: 182.5\n",
      "episode: 1739   score: 180.0  epsilon: 1.0    steps: 816  evaluation reward: 183.3\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3568: Policy loss: 0.000958. Value loss: 0.233686. Entropy: 1.165573.\n",
      "Iteration 3569: Policy loss: -0.000923. Value loss: 0.100887. Entropy: 1.180141.\n",
      "Iteration 3570: Policy loss: -0.008860. Value loss: 0.084063. Entropy: 1.175819.\n",
      "episode: 1740   score: 55.0  epsilon: 1.0    steps: 24  evaluation reward: 180.45\n",
      "episode: 1741   score: 210.0  epsilon: 1.0    steps: 184  evaluation reward: 181.8\n",
      "episode: 1742   score: 155.0  epsilon: 1.0    steps: 888  evaluation reward: 181.4\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3571: Policy loss: 0.002249. Value loss: 0.299388. Entropy: 1.081781.\n",
      "Iteration 3572: Policy loss: -0.002111. Value loss: 0.217736. Entropy: 1.083573.\n",
      "Iteration 3573: Policy loss: -0.003967. Value loss: 0.163039. Entropy: 1.085798.\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3574: Policy loss: 0.003299. Value loss: 0.414584. Entropy: 0.958326.\n",
      "Iteration 3575: Policy loss: -0.001355. Value loss: 0.187643. Entropy: 0.985874.\n",
      "Iteration 3576: Policy loss: -0.001658. Value loss: 0.145256. Entropy: 0.968591.\n",
      "episode: 1743   score: 120.0  epsilon: 1.0    steps: 64  evaluation reward: 181.2\n",
      "episode: 1744   score: 180.0  epsilon: 1.0    steps: 296  evaluation reward: 180.65\n",
      "episode: 1745   score: 80.0  epsilon: 1.0    steps: 752  evaluation reward: 177.15\n",
      "episode: 1746   score: 80.0  epsilon: 1.0    steps: 776  evaluation reward: 176.3\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3577: Policy loss: 0.000839. Value loss: 0.433687. Entropy: 1.263252.\n",
      "Iteration 3578: Policy loss: -0.006963. Value loss: 0.179237. Entropy: 1.258955.\n",
      "Iteration 3579: Policy loss: -0.006222. Value loss: 0.123586. Entropy: 1.251918.\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3580: Policy loss: 0.002450. Value loss: 0.263210. Entropy: 0.923517.\n",
      "Iteration 3581: Policy loss: 0.000268. Value loss: 0.098074. Entropy: 0.936194.\n",
      "Iteration 3582: Policy loss: -0.005390. Value loss: 0.057640. Entropy: 0.910607.\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3583: Policy loss: 0.001326. Value loss: 0.273874. Entropy: 1.252135.\n",
      "Iteration 3584: Policy loss: -0.009605. Value loss: 0.154468. Entropy: 1.256780.\n",
      "Iteration 3585: Policy loss: -0.010556. Value loss: 0.110903. Entropy: 1.257549.\n",
      "episode: 1747   score: 110.0  epsilon: 1.0    steps: 344  evaluation reward: 175.3\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3586: Policy loss: 0.000985. Value loss: 0.371332. Entropy: 1.253584.\n",
      "Iteration 3587: Policy loss: -0.007394. Value loss: 0.160840. Entropy: 1.248870.\n",
      "Iteration 3588: Policy loss: -0.009229. Value loss: 0.098643. Entropy: 1.248163.\n",
      "episode: 1748   score: 105.0  epsilon: 1.0    steps: 136  evaluation reward: 174.85\n",
      "episode: 1749   score: 210.0  epsilon: 1.0    steps: 240  evaluation reward: 174.4\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3589: Policy loss: 0.003615. Value loss: 0.416007. Entropy: 1.199274.\n",
      "Iteration 3590: Policy loss: -0.002784. Value loss: 0.207289. Entropy: 1.208444.\n",
      "Iteration 3591: Policy loss: -0.013080. Value loss: 0.145328. Entropy: 1.225082.\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3592: Policy loss: 0.001313. Value loss: 0.453401. Entropy: 1.110677.\n",
      "Iteration 3593: Policy loss: -0.002073. Value loss: 0.156411. Entropy: 1.106462.\n",
      "Iteration 3594: Policy loss: -0.008217. Value loss: 0.134931. Entropy: 1.119925.\n",
      "episode: 1750   score: 155.0  epsilon: 1.0    steps: 504  evaluation reward: 173.95\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3595: Policy loss: 0.001083. Value loss: 0.455673. Entropy: 1.285002.\n",
      "Iteration 3596: Policy loss: 0.006541. Value loss: 0.185572. Entropy: 1.275278.\n",
      "Iteration 3597: Policy loss: -0.005516. Value loss: 0.137279. Entropy: 1.289576.\n",
      "now time :  2019-02-28 11:37:57.096676\n",
      "episode: 1751   score: 210.0  epsilon: 1.0    steps: 88  evaluation reward: 174.6\n",
      "episode: 1752   score: 290.0  epsilon: 1.0    steps: 960  evaluation reward: 173.4\n",
      "episode: 1753   score: 365.0  epsilon: 1.0    steps: 1008  evaluation reward: 174.3\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3598: Policy loss: 0.002200. Value loss: 0.303413. Entropy: 1.169117.\n",
      "Iteration 3599: Policy loss: -0.007056. Value loss: 0.147186. Entropy: 1.174735.\n",
      "Iteration 3600: Policy loss: -0.004967. Value loss: 0.102170. Entropy: 1.178171.\n",
      "episode: 1754   score: 210.0  epsilon: 1.0    steps: 144  evaluation reward: 175.35\n",
      "episode: 1755   score: 210.0  epsilon: 1.0    steps: 384  evaluation reward: 175.65\n",
      "episode: 1756   score: 155.0  epsilon: 1.0    steps: 392  evaluation reward: 175.65\n",
      "episode: 1757   score: 110.0  epsilon: 1.0    steps: 792  evaluation reward: 175.25\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3601: Policy loss: -0.002143. Value loss: 0.276627. Entropy: 1.109948.\n",
      "Iteration 3602: Policy loss: -0.004174. Value loss: 0.131285. Entropy: 1.105283.\n",
      "Iteration 3603: Policy loss: -0.007183. Value loss: 0.110126. Entropy: 1.111353.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3604: Policy loss: -0.001593. Value loss: 0.417765. Entropy: 0.918652.\n",
      "Iteration 3605: Policy loss: -0.003466. Value loss: 0.222774. Entropy: 0.924348.\n",
      "Iteration 3606: Policy loss: -0.013188. Value loss: 0.171412. Entropy: 0.934506.\n",
      "episode: 1758   score: 65.0  epsilon: 1.0    steps: 272  evaluation reward: 174.1\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3607: Policy loss: 0.003417. Value loss: 0.656556. Entropy: 1.183422.\n",
      "Iteration 3608: Policy loss: -0.006358. Value loss: 0.253242. Entropy: 1.206094.\n",
      "Iteration 3609: Policy loss: -0.012281. Value loss: 0.166437. Entropy: 1.189247.\n",
      "episode: 1759   score: 30.0  epsilon: 1.0    steps: 704  evaluation reward: 173.15\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3610: Policy loss: -0.002137. Value loss: 0.342665. Entropy: 1.161172.\n",
      "Iteration 3611: Policy loss: -0.008809. Value loss: 0.146674. Entropy: 1.164957.\n",
      "Iteration 3612: Policy loss: -0.015388. Value loss: 0.109681. Entropy: 1.149558.\n",
      "episode: 1760   score: 230.0  epsilon: 1.0    steps: 880  evaluation reward: 174.9\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3613: Policy loss: 0.001055. Value loss: 0.390012. Entropy: 1.248943.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3614: Policy loss: -0.005441. Value loss: 0.173728. Entropy: 1.253505.\n",
      "Iteration 3615: Policy loss: -0.011186. Value loss: 0.122017. Entropy: 1.250794.\n",
      "episode: 1761   score: 150.0  epsilon: 1.0    steps: 224  evaluation reward: 174.75\n",
      "episode: 1762   score: 225.0  epsilon: 1.0    steps: 456  evaluation reward: 175.5\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3616: Policy loss: 0.003682. Value loss: 0.387901. Entropy: 1.196748.\n",
      "Iteration 3617: Policy loss: -0.005420. Value loss: 0.184445. Entropy: 1.204869.\n",
      "Iteration 3618: Policy loss: -0.011145. Value loss: 0.114163. Entropy: 1.214288.\n",
      "episode: 1763   score: 240.0  epsilon: 1.0    steps: 88  evaluation reward: 177.3\n",
      "episode: 1764   score: 120.0  epsilon: 1.0    steps: 376  evaluation reward: 176.1\n",
      "episode: 1765   score: 220.0  epsilon: 1.0    steps: 728  evaluation reward: 174.0\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3619: Policy loss: -0.002003. Value loss: 0.329598. Entropy: 1.010892.\n",
      "Iteration 3620: Policy loss: -0.007306. Value loss: 0.129346. Entropy: 1.000261.\n",
      "Iteration 3621: Policy loss: -0.010710. Value loss: 0.080009. Entropy: 1.015032.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3622: Policy loss: 0.000563. Value loss: 0.321790. Entropy: 0.961805.\n",
      "Iteration 3623: Policy loss: -0.003486. Value loss: 0.141347. Entropy: 0.925401.\n",
      "Iteration 3624: Policy loss: -0.010155. Value loss: 0.108992. Entropy: 0.949275.\n",
      "episode: 1766   score: 105.0  epsilon: 1.0    steps: 360  evaluation reward: 174.0\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3625: Policy loss: 0.004960. Value loss: 0.734579. Entropy: 1.222880.\n",
      "Iteration 3626: Policy loss: 0.010088. Value loss: 0.245598. Entropy: 1.215597.\n",
      "Iteration 3627: Policy loss: -0.003316. Value loss: 0.155470. Entropy: 1.223514.\n",
      "episode: 1767   score: 180.0  epsilon: 1.0    steps: 256  evaluation reward: 174.9\n",
      "episode: 1768   score: 155.0  epsilon: 1.0    steps: 368  evaluation reward: 173.6\n",
      "episode: 1769   score: 135.0  epsilon: 1.0    steps: 704  evaluation reward: 172.85\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3628: Policy loss: -0.001652. Value loss: 0.429029. Entropy: 1.176003.\n",
      "Iteration 3629: Policy loss: -0.003914. Value loss: 0.253541. Entropy: 1.169297.\n",
      "Iteration 3630: Policy loss: -0.008912. Value loss: 0.182478. Entropy: 1.177256.\n",
      "episode: 1770   score: 210.0  epsilon: 1.0    steps: 544  evaluation reward: 172.85\n",
      "episode: 1771   score: 460.0  epsilon: 1.0    steps: 864  evaluation reward: 175.9\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3631: Policy loss: 0.004777. Value loss: 0.526429. Entropy: 1.082849.\n",
      "Iteration 3632: Policy loss: -0.003601. Value loss: 0.173128. Entropy: 1.079951.\n",
      "Iteration 3633: Policy loss: -0.007990. Value loss: 0.139656. Entropy: 1.074540.\n",
      "episode: 1772   score: 210.0  epsilon: 1.0    steps: 96  evaluation reward: 176.8\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3634: Policy loss: 0.004624. Value loss: 0.560580. Entropy: 1.121743.\n",
      "Iteration 3635: Policy loss: -0.001676. Value loss: 0.261047. Entropy: 1.140167.\n",
      "Iteration 3636: Policy loss: -0.005029. Value loss: 0.152734. Entropy: 1.121769.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3637: Policy loss: -0.001301. Value loss: 0.484948. Entropy: 1.136133.\n",
      "Iteration 3638: Policy loss: -0.000508. Value loss: 0.174561. Entropy: 1.141025.\n",
      "Iteration 3639: Policy loss: -0.010472. Value loss: 0.126411. Entropy: 1.138418.\n",
      "episode: 1773   score: 290.0  epsilon: 1.0    steps: 96  evaluation reward: 178.15\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3640: Policy loss: 0.001237. Value loss: 0.541027. Entropy: 1.244972.\n",
      "Iteration 3641: Policy loss: -0.008311. Value loss: 0.189740. Entropy: 1.255299.\n",
      "Iteration 3642: Policy loss: -0.015562. Value loss: 0.139207. Entropy: 1.248588.\n",
      "episode: 1774   score: 105.0  epsilon: 1.0    steps: 200  evaluation reward: 177.1\n",
      "episode: 1775   score: 210.0  epsilon: 1.0    steps: 608  evaluation reward: 177.85\n",
      "episode: 1776   score: 140.0  epsilon: 1.0    steps: 776  evaluation reward: 177.05\n",
      "episode: 1777   score: 155.0  epsilon: 1.0    steps: 848  evaluation reward: 176.8\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3643: Policy loss: -0.000578. Value loss: 0.519837. Entropy: 1.253207.\n",
      "Iteration 3644: Policy loss: -0.003457. Value loss: 0.232424. Entropy: 1.254443.\n",
      "Iteration 3645: Policy loss: -0.010306. Value loss: 0.163085. Entropy: 1.254375.\n",
      "episode: 1778   score: 395.0  epsilon: 1.0    steps: 760  evaluation reward: 177.6\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3646: Policy loss: 0.001957. Value loss: 0.791862. Entropy: 0.983292.\n",
      "Iteration 3647: Policy loss: -0.000445. Value loss: 0.439336. Entropy: 0.968843.\n",
      "Iteration 3648: Policy loss: 0.001026. Value loss: 0.325050. Entropy: 0.989321.\n",
      "episode: 1779   score: 155.0  epsilon: 1.0    steps: 584  evaluation reward: 177.95\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3649: Policy loss: 0.001787. Value loss: 0.342493. Entropy: 1.138457.\n",
      "Iteration 3650: Policy loss: -0.002412. Value loss: 0.173695. Entropy: 1.141597.\n",
      "Iteration 3651: Policy loss: -0.003252. Value loss: 0.133258. Entropy: 1.152152.\n",
      "episode: 1780   score: 125.0  epsilon: 1.0    steps: 544  evaluation reward: 178.3\n",
      "episode: 1781   score: 155.0  epsilon: 1.0    steps: 1024  evaluation reward: 177.45\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3652: Policy loss: -0.001372. Value loss: 0.615948. Entropy: 1.152014.\n",
      "Iteration 3653: Policy loss: -0.001813. Value loss: 0.260226. Entropy: 1.155657.\n",
      "Iteration 3654: Policy loss: -0.012341. Value loss: 0.169661. Entropy: 1.171486.\n",
      "episode: 1782   score: 120.0  epsilon: 1.0    steps: 440  evaluation reward: 176.5\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3655: Policy loss: 0.001507. Value loss: 0.221551. Entropy: 1.109296.\n",
      "Iteration 3656: Policy loss: -0.004963. Value loss: 0.108747. Entropy: 1.105097.\n",
      "Iteration 3657: Policy loss: -0.009418. Value loss: 0.061908. Entropy: 1.101175.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3658: Policy loss: 0.003581. Value loss: 0.690838. Entropy: 1.173993.\n",
      "Iteration 3659: Policy loss: -0.001440. Value loss: 0.342009. Entropy: 1.171829.\n",
      "Iteration 3660: Policy loss: -0.010679. Value loss: 0.213243. Entropy: 1.173372.\n",
      "episode: 1783   score: 250.0  epsilon: 1.0    steps: 632  evaluation reward: 177.45\n",
      "episode: 1784   score: 110.0  epsilon: 1.0    steps: 816  evaluation reward: 178.0\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3661: Policy loss: 0.002262. Value loss: 0.384119. Entropy: 1.219209.\n",
      "Iteration 3662: Policy loss: -0.007593. Value loss: 0.137194. Entropy: 1.217212.\n",
      "Iteration 3663: Policy loss: -0.008172. Value loss: 0.130546. Entropy: 1.216630.\n",
      "episode: 1785   score: 135.0  epsilon: 1.0    steps: 416  evaluation reward: 178.0\n",
      "episode: 1786   score: 95.0  epsilon: 1.0    steps: 464  evaluation reward: 177.6\n",
      "episode: 1787   score: 160.0  epsilon: 1.0    steps: 664  evaluation reward: 176.3\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3664: Policy loss: 0.002352. Value loss: 0.344382. Entropy: 1.101576.\n",
      "Iteration 3665: Policy loss: -0.001102. Value loss: 0.178849. Entropy: 1.095635.\n",
      "Iteration 3666: Policy loss: -0.006890. Value loss: 0.127638. Entropy: 1.095584.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3667: Policy loss: 0.009036. Value loss: 0.284209. Entropy: 1.053499.\n",
      "Iteration 3668: Policy loss: -0.003790. Value loss: 0.115052. Entropy: 1.078004.\n",
      "Iteration 3669: Policy loss: -0.006603. Value loss: 0.079459. Entropy: 1.062214.\n",
      "episode: 1788   score: 210.0  epsilon: 1.0    steps: 8  evaluation reward: 176.9\n",
      "episode: 1789   score: 245.0  epsilon: 1.0    steps: 160  evaluation reward: 176.9\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3670: Policy loss: 0.000438. Value loss: 0.876334. Entropy: 1.258365.\n",
      "Iteration 3671: Policy loss: -0.005365. Value loss: 0.336127. Entropy: 1.257733.\n",
      "Iteration 3672: Policy loss: -0.009302. Value loss: 0.202071. Entropy: 1.262419.\n",
      "episode: 1790   score: 195.0  epsilon: 1.0    steps: 232  evaluation reward: 177.3\n",
      "Training network. lr: 0.000222. clip: 0.088793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3673: Policy loss: 0.000276. Value loss: 0.459375. Entropy: 1.039181.\n",
      "Iteration 3674: Policy loss: -0.004438. Value loss: 0.179339. Entropy: 1.051869.\n",
      "Iteration 3675: Policy loss: -0.011163. Value loss: 0.121242. Entropy: 1.041191.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3676: Policy loss: 0.001226. Value loss: 0.978065. Entropy: 1.197445.\n",
      "Iteration 3677: Policy loss: -0.003815. Value loss: 0.392159. Entropy: 1.202302.\n",
      "Iteration 3678: Policy loss: -0.006941. Value loss: 0.218294. Entropy: 1.196681.\n",
      "episode: 1791   score: 130.0  epsilon: 1.0    steps: 336  evaluation reward: 176.5\n",
      "episode: 1792   score: 210.0  epsilon: 1.0    steps: 880  evaluation reward: 176.8\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3679: Policy loss: 0.001728. Value loss: 0.586342. Entropy: 1.244392.\n",
      "Iteration 3680: Policy loss: -0.003255. Value loss: 0.331352. Entropy: 1.240584.\n",
      "Iteration 3681: Policy loss: -0.009042. Value loss: 0.321018. Entropy: 1.237737.\n",
      "episode: 1793   score: 210.0  epsilon: 1.0    steps: 80  evaluation reward: 177.55\n",
      "episode: 1794   score: 465.0  epsilon: 1.0    steps: 168  evaluation reward: 180.4\n",
      "episode: 1795   score: 435.0  epsilon: 1.0    steps: 904  evaluation reward: 183.2\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3682: Policy loss: 0.000102. Value loss: 0.422531. Entropy: 1.135843.\n",
      "Iteration 3683: Policy loss: -0.000691. Value loss: 0.137370. Entropy: 1.153453.\n",
      "Iteration 3684: Policy loss: -0.007385. Value loss: 0.103116. Entropy: 1.141480.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3685: Policy loss: 0.005071. Value loss: 0.686061. Entropy: 1.074121.\n",
      "Iteration 3686: Policy loss: -0.002609. Value loss: 0.326035. Entropy: 1.106976.\n",
      "Iteration 3687: Policy loss: -0.007486. Value loss: 0.216119. Entropy: 1.119338.\n",
      "episode: 1796   score: 230.0  epsilon: 1.0    steps: 520  evaluation reward: 184.15\n",
      "episode: 1797   score: 350.0  epsilon: 1.0    steps: 648  evaluation reward: 186.15\n",
      "episode: 1798   score: 215.0  epsilon: 1.0    steps: 800  evaluation reward: 186.2\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3688: Policy loss: 0.001471. Value loss: 0.369118. Entropy: 1.234406.\n",
      "Iteration 3689: Policy loss: -0.002877. Value loss: 0.152743. Entropy: 1.237689.\n",
      "Iteration 3690: Policy loss: -0.004907. Value loss: 0.120045. Entropy: 1.233628.\n",
      "episode: 1799   score: 30.0  epsilon: 1.0    steps: 168  evaluation reward: 185.25\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3691: Policy loss: -0.000400. Value loss: 0.402544. Entropy: 1.082047.\n",
      "Iteration 3692: Policy loss: -0.007314. Value loss: 0.203379. Entropy: 1.095755.\n",
      "Iteration 3693: Policy loss: -0.008213. Value loss: 0.131977. Entropy: 1.097252.\n",
      "episode: 1800   score: 105.0  epsilon: 1.0    steps: 248  evaluation reward: 184.65\n",
      "now time :  2019-02-28 11:39:07.899207\n",
      "episode: 1801   score: 180.0  epsilon: 1.0    steps: 920  evaluation reward: 185.25\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3694: Policy loss: 0.000932. Value loss: 0.407267. Entropy: 1.115224.\n",
      "Iteration 3695: Policy loss: -0.003279. Value loss: 0.181552. Entropy: 1.119579.\n",
      "Iteration 3696: Policy loss: -0.009036. Value loss: 0.146601. Entropy: 1.129751.\n",
      "episode: 1802   score: 210.0  epsilon: 1.0    steps: 32  evaluation reward: 186.1\n",
      "episode: 1803   score: 180.0  epsilon: 1.0    steps: 216  evaluation reward: 185.8\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3697: Policy loss: 0.000751. Value loss: 0.234313. Entropy: 1.069606.\n",
      "Iteration 3698: Policy loss: -0.006342. Value loss: 0.130694. Entropy: 1.054148.\n",
      "Iteration 3699: Policy loss: -0.005522. Value loss: 0.104569. Entropy: 1.091703.\n",
      "episode: 1804   score: 90.0  epsilon: 1.0    steps: 680  evaluation reward: 185.15\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3700: Policy loss: -0.002259. Value loss: 0.442760. Entropy: 1.209559.\n",
      "Iteration 3701: Policy loss: -0.002320. Value loss: 0.232292. Entropy: 1.220979.\n",
      "Iteration 3702: Policy loss: -0.008140. Value loss: 0.170241. Entropy: 1.208323.\n",
      "episode: 1805   score: 125.0  epsilon: 1.0    steps: 392  evaluation reward: 184.55\n",
      "episode: 1806   score: 130.0  epsilon: 1.0    steps: 696  evaluation reward: 183.75\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3703: Policy loss: -0.001456. Value loss: 0.469943. Entropy: 1.166236.\n",
      "Iteration 3704: Policy loss: -0.004438. Value loss: 0.209595. Entropy: 1.175095.\n",
      "Iteration 3705: Policy loss: -0.008701. Value loss: 0.128814. Entropy: 1.167570.\n",
      "episode: 1807   score: 210.0  epsilon: 1.0    steps: 200  evaluation reward: 184.6\n",
      "episode: 1808   score: 180.0  epsilon: 1.0    steps: 384  evaluation reward: 185.2\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3706: Policy loss: -0.000781. Value loss: 0.253172. Entropy: 1.109269.\n",
      "Iteration 3707: Policy loss: -0.005346. Value loss: 0.146084. Entropy: 1.119152.\n",
      "Iteration 3708: Policy loss: -0.011183. Value loss: 0.099447. Entropy: 1.115340.\n",
      "episode: 1809   score: 210.0  epsilon: 1.0    steps: 592  evaluation reward: 184.7\n",
      "episode: 1810   score: 90.0  epsilon: 1.0    steps: 728  evaluation reward: 184.25\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3709: Policy loss: -0.000983. Value loss: 0.331829. Entropy: 1.141981.\n",
      "Iteration 3710: Policy loss: -0.010234. Value loss: 0.157583. Entropy: 1.159412.\n",
      "Iteration 3711: Policy loss: -0.013835. Value loss: 0.106787. Entropy: 1.147904.\n",
      "episode: 1811   score: 230.0  epsilon: 1.0    steps: 672  evaluation reward: 185.2\n",
      "episode: 1812   score: 135.0  epsilon: 1.0    steps: 672  evaluation reward: 184.45\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3712: Policy loss: -0.000454. Value loss: 0.250123. Entropy: 1.145259.\n",
      "Iteration 3713: Policy loss: -0.006719. Value loss: 0.124659. Entropy: 1.145079.\n",
      "Iteration 3714: Policy loss: -0.007614. Value loss: 0.100327. Entropy: 1.149659.\n",
      "episode: 1813   score: 135.0  epsilon: 1.0    steps: 392  evaluation reward: 184.0\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3715: Policy loss: 0.000283. Value loss: 0.280824. Entropy: 1.166643.\n",
      "Iteration 3716: Policy loss: -0.005997. Value loss: 0.138851. Entropy: 1.176754.\n",
      "Iteration 3717: Policy loss: -0.009941. Value loss: 0.080477. Entropy: 1.179565.\n",
      "episode: 1814   score: 160.0  epsilon: 1.0    steps: 96  evaluation reward: 184.55\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3718: Policy loss: 0.000616. Value loss: 0.435055. Entropy: 1.187338.\n",
      "Iteration 3719: Policy loss: -0.002933. Value loss: 0.164175. Entropy: 1.183881.\n",
      "Iteration 3720: Policy loss: -0.006163. Value loss: 0.149442. Entropy: 1.185588.\n",
      "episode: 1815   score: 185.0  epsilon: 1.0    steps: 120  evaluation reward: 185.15\n",
      "episode: 1816   score: 80.0  epsilon: 1.0    steps: 608  evaluation reward: 184.85\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3721: Policy loss: 0.002705. Value loss: 0.218617. Entropy: 1.212659.\n",
      "Iteration 3722: Policy loss: -0.007068. Value loss: 0.105140. Entropy: 1.214241.\n",
      "Iteration 3723: Policy loss: -0.011438. Value loss: 0.089396. Entropy: 1.205906.\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3724: Policy loss: 0.002582. Value loss: 0.317104. Entropy: 1.190369.\n",
      "Iteration 3725: Policy loss: -0.005265. Value loss: 0.115902. Entropy: 1.183847.\n",
      "Iteration 3726: Policy loss: -0.011633. Value loss: 0.080111. Entropy: 1.175221.\n",
      "episode: 1817   score: 185.0  epsilon: 1.0    steps: 296  evaluation reward: 186.15\n",
      "episode: 1818   score: 125.0  epsilon: 1.0    steps: 920  evaluation reward: 187.05\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3727: Policy loss: 0.001637. Value loss: 0.339127. Entropy: 1.199642.\n",
      "Iteration 3728: Policy loss: -0.005272. Value loss: 0.127042. Entropy: 1.208720.\n",
      "Iteration 3729: Policy loss: -0.011292. Value loss: 0.083100. Entropy: 1.211166.\n",
      "episode: 1819   score: 210.0  epsilon: 1.0    steps: 384  evaluation reward: 186.75\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3730: Policy loss: 0.002432. Value loss: 0.273780. Entropy: 1.148671.\n",
      "Iteration 3731: Policy loss: -0.006191. Value loss: 0.179317. Entropy: 1.129182.\n",
      "Iteration 3732: Policy loss: -0.009414. Value loss: 0.103904. Entropy: 1.143815.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3733: Policy loss: 0.002418. Value loss: 0.239138. Entropy: 1.184422.\n",
      "Iteration 3734: Policy loss: -0.007297. Value loss: 0.092812. Entropy: 1.192322.\n",
      "Iteration 3735: Policy loss: -0.009495. Value loss: 0.059722. Entropy: 1.194072.\n",
      "episode: 1820   score: 105.0  epsilon: 1.0    steps: 64  evaluation reward: 185.2\n",
      "episode: 1821   score: 235.0  epsilon: 1.0    steps: 800  evaluation reward: 181.9\n",
      "episode: 1822   score: 125.0  epsilon: 1.0    steps: 944  evaluation reward: 182.1\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3736: Policy loss: 0.002876. Value loss: 0.382786. Entropy: 1.210971.\n",
      "Iteration 3737: Policy loss: 0.000515. Value loss: 0.117699. Entropy: 1.207609.\n",
      "Iteration 3738: Policy loss: -0.006644. Value loss: 0.089553. Entropy: 1.213346.\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3739: Policy loss: -0.001535. Value loss: 0.377500. Entropy: 1.147359.\n",
      "Iteration 3740: Policy loss: 0.000577. Value loss: 0.181879. Entropy: 1.131250.\n",
      "Iteration 3741: Policy loss: -0.012141. Value loss: 0.110606. Entropy: 1.137701.\n",
      "episode: 1823   score: 215.0  epsilon: 1.0    steps: 320  evaluation reward: 182.9\n",
      "episode: 1824   score: 255.0  epsilon: 1.0    steps: 568  evaluation reward: 183.95\n",
      "episode: 1825   score: 240.0  epsilon: 1.0    steps: 712  evaluation reward: 183.95\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3742: Policy loss: 0.000048. Value loss: 0.538496. Entropy: 1.241134.\n",
      "Iteration 3743: Policy loss: 0.003523. Value loss: 0.242748. Entropy: 1.222779.\n",
      "Iteration 3744: Policy loss: -0.007024. Value loss: 0.164239. Entropy: 1.243471.\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3745: Policy loss: -0.001604. Value loss: 0.452266. Entropy: 1.144525.\n",
      "Iteration 3746: Policy loss: -0.005123. Value loss: 0.232091. Entropy: 1.148520.\n",
      "Iteration 3747: Policy loss: -0.008258. Value loss: 0.129105. Entropy: 1.156421.\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3748: Policy loss: 0.001141. Value loss: 0.521064. Entropy: 1.174754.\n",
      "Iteration 3749: Policy loss: -0.000864. Value loss: 0.175302. Entropy: 1.178961.\n",
      "Iteration 3750: Policy loss: -0.008861. Value loss: 0.108906. Entropy: 1.171273.\n",
      "episode: 1826   score: 45.0  epsilon: 1.0    steps: 328  evaluation reward: 183.35\n",
      "episode: 1827   score: 160.0  epsilon: 1.0    steps: 800  evaluation reward: 183.7\n",
      "episode: 1828   score: 445.0  epsilon: 1.0    steps: 896  evaluation reward: 185.2\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3751: Policy loss: -0.000354. Value loss: 0.854802. Entropy: 1.290849.\n",
      "Iteration 3752: Policy loss: -0.001720. Value loss: 0.403462. Entropy: 1.286355.\n",
      "Iteration 3753: Policy loss: -0.004492. Value loss: 0.368570. Entropy: 1.289004.\n",
      "episode: 1829   score: 370.0  epsilon: 1.0    steps: 248  evaluation reward: 186.3\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3754: Policy loss: 0.003653. Value loss: 0.474344. Entropy: 1.077621.\n",
      "Iteration 3755: Policy loss: -0.003299. Value loss: 0.283213. Entropy: 1.120707.\n",
      "Iteration 3756: Policy loss: -0.005445. Value loss: 0.251241. Entropy: 1.093783.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3757: Policy loss: -0.002482. Value loss: 0.797983. Entropy: 1.240171.\n",
      "Iteration 3758: Policy loss: -0.002037. Value loss: 0.359119. Entropy: 1.240776.\n",
      "Iteration 3759: Policy loss: -0.005430. Value loss: 0.238040. Entropy: 1.240544.\n",
      "episode: 1830   score: 140.0  epsilon: 1.0    steps: 472  evaluation reward: 186.8\n",
      "episode: 1831   score: 190.0  epsilon: 1.0    steps: 936  evaluation reward: 186.6\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3760: Policy loss: 0.000637. Value loss: 0.538679. Entropy: 1.242001.\n",
      "Iteration 3761: Policy loss: -0.000469. Value loss: 0.233379. Entropy: 1.261464.\n",
      "Iteration 3762: Policy loss: -0.010036. Value loss: 0.189936. Entropy: 1.255693.\n",
      "episode: 1832   score: 100.0  epsilon: 1.0    steps: 888  evaluation reward: 185.95\n",
      "episode: 1833   score: 215.0  epsilon: 1.0    steps: 976  evaluation reward: 184.0\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3763: Policy loss: -0.001102. Value loss: 0.419582. Entropy: 1.187889.\n",
      "Iteration 3764: Policy loss: -0.005033. Value loss: 0.235590. Entropy: 1.183777.\n",
      "Iteration 3765: Policy loss: -0.010011. Value loss: 0.164577. Entropy: 1.179367.\n",
      "episode: 1834   score: 195.0  epsilon: 1.0    steps: 104  evaluation reward: 183.7\n",
      "episode: 1835   score: 125.0  epsilon: 1.0    steps: 440  evaluation reward: 182.8\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3766: Policy loss: 0.009699. Value loss: 0.635881. Entropy: 1.206063.\n",
      "Iteration 3767: Policy loss: 0.001609. Value loss: 0.284941. Entropy: 1.212947.\n",
      "Iteration 3768: Policy loss: -0.007166. Value loss: 0.219524. Entropy: 1.211273.\n",
      "episode: 1836   score: 170.0  epsilon: 1.0    steps: 512  evaluation reward: 183.2\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3769: Policy loss: 0.006705. Value loss: 0.514700. Entropy: 1.172420.\n",
      "Iteration 3770: Policy loss: 0.000543. Value loss: 0.239093. Entropy: 1.184127.\n",
      "Iteration 3771: Policy loss: -0.006354. Value loss: 0.175615. Entropy: 1.166230.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3772: Policy loss: 0.000860. Value loss: 0.407838. Entropy: 1.209166.\n",
      "Iteration 3773: Policy loss: -0.008989. Value loss: 0.192186. Entropy: 1.212196.\n",
      "Iteration 3774: Policy loss: -0.014464. Value loss: 0.140809. Entropy: 1.225374.\n",
      "episode: 1837   score: 225.0  epsilon: 1.0    steps: 80  evaluation reward: 182.85\n",
      "episode: 1838   score: 120.0  epsilon: 1.0    steps: 832  evaluation reward: 182.8\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3775: Policy loss: 0.001506. Value loss: 0.558027. Entropy: 1.265345.\n",
      "Iteration 3776: Policy loss: -0.003719. Value loss: 0.211548. Entropy: 1.260433.\n",
      "Iteration 3777: Policy loss: -0.013720. Value loss: 0.171443. Entropy: 1.265751.\n",
      "episode: 1839   score: 120.0  epsilon: 1.0    steps: 608  evaluation reward: 182.2\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3778: Policy loss: 0.003682. Value loss: 0.548372. Entropy: 1.153454.\n",
      "Iteration 3779: Policy loss: -0.002150. Value loss: 0.315871. Entropy: 1.155304.\n",
      "Iteration 3780: Policy loss: -0.007939. Value loss: 0.189985. Entropy: 1.160312.\n",
      "episode: 1840   score: 240.0  epsilon: 1.0    steps: 344  evaluation reward: 184.05\n",
      "episode: 1841   score: 165.0  epsilon: 1.0    steps: 776  evaluation reward: 183.6\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3781: Policy loss: -0.001024. Value loss: 0.371862. Entropy: 1.207345.\n",
      "Iteration 3782: Policy loss: -0.009444. Value loss: 0.181163. Entropy: 1.207691.\n",
      "Iteration 3783: Policy loss: -0.009068. Value loss: 0.130716. Entropy: 1.212924.\n",
      "episode: 1842   score: 225.0  epsilon: 1.0    steps: 112  evaluation reward: 184.3\n",
      "episode: 1843   score: 195.0  epsilon: 1.0    steps: 168  evaluation reward: 185.05\n",
      "episode: 1844   score: 175.0  epsilon: 1.0    steps: 656  evaluation reward: 185.0\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3784: Policy loss: 0.002074. Value loss: 0.353736. Entropy: 1.147946.\n",
      "Iteration 3785: Policy loss: -0.002280. Value loss: 0.187433. Entropy: 1.156110.\n",
      "Iteration 3786: Policy loss: -0.005555. Value loss: 0.125676. Entropy: 1.136208.\n",
      "episode: 1845   score: 190.0  epsilon: 1.0    steps: 632  evaluation reward: 186.1\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3787: Policy loss: 0.001689. Value loss: 0.216976. Entropy: 1.094721.\n",
      "Iteration 3788: Policy loss: -0.004915. Value loss: 0.143582. Entropy: 1.109193.\n",
      "Iteration 3789: Policy loss: -0.012943. Value loss: 0.089810. Entropy: 1.083858.\n",
      "episode: 1846   score: 215.0  epsilon: 1.0    steps: 848  evaluation reward: 187.45\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3790: Policy loss: 0.001020. Value loss: 0.303008. Entropy: 1.197516.\n",
      "Iteration 3791: Policy loss: -0.005594. Value loss: 0.110967. Entropy: 1.176829.\n",
      "Iteration 3792: Policy loss: -0.012791. Value loss: 0.089341. Entropy: 1.184301.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3793: Policy loss: 0.003625. Value loss: 0.587845. Entropy: 1.239950.\n",
      "Iteration 3794: Policy loss: 0.006403. Value loss: 0.249723. Entropy: 1.254332.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3795: Policy loss: -0.001244. Value loss: 0.188201. Entropy: 1.253059.\n",
      "episode: 1847   score: 105.0  epsilon: 1.0    steps: 512  evaluation reward: 187.4\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3796: Policy loss: 0.004084. Value loss: 0.884965. Entropy: 1.269662.\n",
      "Iteration 3797: Policy loss: -0.003662. Value loss: 0.333934. Entropy: 1.270410.\n",
      "Iteration 3798: Policy loss: -0.008881. Value loss: 0.201821. Entropy: 1.266114.\n",
      "episode: 1848   score: 145.0  epsilon: 1.0    steps: 576  evaluation reward: 187.8\n",
      "episode: 1849   score: 255.0  epsilon: 1.0    steps: 688  evaluation reward: 188.25\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3799: Policy loss: 0.001538. Value loss: 0.440977. Entropy: 1.226239.\n",
      "Iteration 3800: Policy loss: -0.004714. Value loss: 0.238382. Entropy: 1.215071.\n",
      "Iteration 3801: Policy loss: -0.008197. Value loss: 0.157620. Entropy: 1.213645.\n",
      "episode: 1850   score: 185.0  epsilon: 1.0    steps: 448  evaluation reward: 188.55\n",
      "now time :  2019-02-28 11:40:25.653524\n",
      "episode: 1851   score: 105.0  epsilon: 1.0    steps: 568  evaluation reward: 187.5\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3802: Policy loss: -0.002813. Value loss: 0.478222. Entropy: 1.172467.\n",
      "Iteration 3803: Policy loss: -0.003622. Value loss: 0.220905. Entropy: 1.154829.\n",
      "Iteration 3804: Policy loss: -0.012151. Value loss: 0.150937. Entropy: 1.174261.\n",
      "episode: 1852   score: 370.0  epsilon: 1.0    steps: 272  evaluation reward: 188.3\n",
      "episode: 1853   score: 210.0  epsilon: 1.0    steps: 320  evaluation reward: 186.75\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3805: Policy loss: 0.003013. Value loss: 0.407769. Entropy: 1.193254.\n",
      "Iteration 3806: Policy loss: -0.002566. Value loss: 0.165323. Entropy: 1.198371.\n",
      "Iteration 3807: Policy loss: -0.003411. Value loss: 0.123017. Entropy: 1.216065.\n",
      "episode: 1854   score: 230.0  epsilon: 1.0    steps: 48  evaluation reward: 186.95\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3808: Policy loss: 0.000683. Value loss: 0.400807. Entropy: 1.171892.\n",
      "Iteration 3809: Policy loss: -0.004733. Value loss: 0.156603. Entropy: 1.192850.\n",
      "Iteration 3810: Policy loss: -0.009128. Value loss: 0.118274. Entropy: 1.190045.\n",
      "episode: 1855   score: 80.0  epsilon: 1.0    steps: 72  evaluation reward: 185.65\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3811: Policy loss: -0.000822. Value loss: 0.304761. Entropy: 1.135375.\n",
      "Iteration 3812: Policy loss: -0.010337. Value loss: 0.135642. Entropy: 1.153217.\n",
      "Iteration 3813: Policy loss: -0.020262. Value loss: 0.093828. Entropy: 1.126817.\n",
      "episode: 1856   score: 150.0  epsilon: 1.0    steps: 456  evaluation reward: 185.6\n",
      "episode: 1857   score: 180.0  epsilon: 1.0    steps: 648  evaluation reward: 186.3\n",
      "episode: 1858   score: 230.0  epsilon: 1.0    steps: 760  evaluation reward: 187.95\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3814: Policy loss: 0.003820. Value loss: 0.469533. Entropy: 1.192877.\n",
      "Iteration 3815: Policy loss: 0.000075. Value loss: 0.188502. Entropy: 1.206657.\n",
      "Iteration 3816: Policy loss: -0.003546. Value loss: 0.150495. Entropy: 1.197575.\n",
      "episode: 1859   score: 160.0  epsilon: 1.0    steps: 800  evaluation reward: 189.25\n",
      "episode: 1860   score: 200.0  epsilon: 1.0    steps: 904  evaluation reward: 188.95\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3817: Policy loss: -0.002410. Value loss: 0.195176. Entropy: 1.126786.\n",
      "Iteration 3818: Policy loss: -0.006177. Value loss: 0.098493. Entropy: 1.114423.\n",
      "Iteration 3819: Policy loss: -0.009871. Value loss: 0.070565. Entropy: 1.108974.\n",
      "episode: 1861   score: 125.0  epsilon: 1.0    steps: 120  evaluation reward: 188.7\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3820: Policy loss: 0.001157. Value loss: 0.267693. Entropy: 1.163323.\n",
      "Iteration 3821: Policy loss: -0.005455. Value loss: 0.114564. Entropy: 1.165393.\n",
      "Iteration 3822: Policy loss: -0.011311. Value loss: 0.084679. Entropy: 1.166259.\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3823: Policy loss: 0.002222. Value loss: 0.231610. Entropy: 1.156107.\n",
      "Iteration 3824: Policy loss: -0.005828. Value loss: 0.110979. Entropy: 1.172328.\n",
      "Iteration 3825: Policy loss: -0.010501. Value loss: 0.085276. Entropy: 1.173241.\n",
      "episode: 1862   score: 255.0  epsilon: 1.0    steps: 440  evaluation reward: 189.0\n",
      "episode: 1863   score: 105.0  epsilon: 1.0    steps: 1000  evaluation reward: 187.65\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3826: Policy loss: 0.004517. Value loss: 0.743176. Entropy: 1.254806.\n",
      "Iteration 3827: Policy loss: -0.003271. Value loss: 0.218121. Entropy: 1.261320.\n",
      "Iteration 3828: Policy loss: -0.012214. Value loss: 0.129269. Entropy: 1.265936.\n",
      "episode: 1864   score: 135.0  epsilon: 1.0    steps: 664  evaluation reward: 187.8\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3829: Policy loss: 0.002420. Value loss: 0.465977. Entropy: 1.221700.\n",
      "Iteration 3830: Policy loss: -0.000121. Value loss: 0.241409. Entropy: 1.231609.\n",
      "Iteration 3831: Policy loss: -0.006328. Value loss: 0.158061. Entropy: 1.231431.\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3832: Policy loss: -0.000718. Value loss: 0.418092. Entropy: 1.224406.\n",
      "Iteration 3833: Policy loss: -0.000823. Value loss: 0.159294. Entropy: 1.207735.\n",
      "Iteration 3834: Policy loss: -0.004777. Value loss: 0.100892. Entropy: 1.217709.\n",
      "episode: 1865   score: 165.0  epsilon: 1.0    steps: 496  evaluation reward: 187.25\n",
      "episode: 1866   score: 200.0  epsilon: 1.0    steps: 1008  evaluation reward: 188.2\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3835: Policy loss: 0.004682. Value loss: 0.612522. Entropy: 1.291996.\n",
      "Iteration 3836: Policy loss: -0.005286. Value loss: 0.199753. Entropy: 1.283646.\n",
      "Iteration 3837: Policy loss: -0.008782. Value loss: 0.096321. Entropy: 1.287509.\n",
      "episode: 1867   score: 190.0  epsilon: 1.0    steps: 224  evaluation reward: 188.3\n",
      "episode: 1868   score: 180.0  epsilon: 1.0    steps: 984  evaluation reward: 188.55\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3838: Policy loss: 0.001795. Value loss: 0.702498. Entropy: 1.202963.\n",
      "Iteration 3839: Policy loss: 0.002908. Value loss: 0.247011. Entropy: 1.203475.\n",
      "Iteration 3840: Policy loss: -0.002123. Value loss: 0.214915. Entropy: 1.197172.\n",
      "episode: 1869   score: 455.0  epsilon: 1.0    steps: 512  evaluation reward: 191.75\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3841: Policy loss: 0.002287. Value loss: 0.475027. Entropy: 1.194749.\n",
      "Iteration 3842: Policy loss: 0.004253. Value loss: 0.216998. Entropy: 1.199951.\n",
      "Iteration 3843: Policy loss: -0.006320. Value loss: 0.158548. Entropy: 1.202965.\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3844: Policy loss: -0.000162. Value loss: 0.569799. Entropy: 1.260482.\n",
      "Iteration 3845: Policy loss: -0.004666. Value loss: 0.314228. Entropy: 1.270696.\n",
      "Iteration 3846: Policy loss: -0.005547. Value loss: 0.176851. Entropy: 1.255151.\n",
      "episode: 1870   score: 80.0  epsilon: 1.0    steps: 80  evaluation reward: 190.45\n",
      "episode: 1871   score: 200.0  epsilon: 1.0    steps: 96  evaluation reward: 187.85\n",
      "episode: 1872   score: 105.0  epsilon: 1.0    steps: 272  evaluation reward: 186.8\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3847: Policy loss: 0.005892. Value loss: 0.706896. Entropy: 1.263824.\n",
      "Iteration 3848: Policy loss: -0.002590. Value loss: 0.421438. Entropy: 1.267585.\n",
      "Iteration 3849: Policy loss: -0.007410. Value loss: 0.206368. Entropy: 1.252747.\n",
      "episode: 1873   score: 425.0  epsilon: 1.0    steps: 152  evaluation reward: 188.15\n",
      "episode: 1874   score: 220.0  epsilon: 1.0    steps: 1016  evaluation reward: 189.3\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3850: Policy loss: -0.001480. Value loss: 0.143785. Entropy: 1.151440.\n",
      "Iteration 3851: Policy loss: -0.005800. Value loss: 0.083641. Entropy: 1.149366.\n",
      "Iteration 3852: Policy loss: -0.011984. Value loss: 0.056137. Entropy: 1.143794.\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3853: Policy loss: 0.002912. Value loss: 0.262105. Entropy: 1.235139.\n",
      "Iteration 3854: Policy loss: -0.005445. Value loss: 0.114629. Entropy: 1.234716.\n",
      "Iteration 3855: Policy loss: -0.010260. Value loss: 0.081473. Entropy: 1.230077.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1875   score: 170.0  epsilon: 1.0    steps: 240  evaluation reward: 188.9\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3856: Policy loss: 0.004750. Value loss: 0.411492. Entropy: 1.242201.\n",
      "Iteration 3857: Policy loss: -0.007791. Value loss: 0.179661. Entropy: 1.233829.\n",
      "Iteration 3858: Policy loss: -0.010251. Value loss: 0.120403. Entropy: 1.237140.\n",
      "episode: 1876   score: 155.0  epsilon: 1.0    steps: 64  evaluation reward: 189.05\n",
      "episode: 1877   score: 155.0  epsilon: 1.0    steps: 488  evaluation reward: 189.05\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3859: Policy loss: -0.002844. Value loss: 0.502824. Entropy: 1.165790.\n",
      "Iteration 3860: Policy loss: -0.010626. Value loss: 0.205618. Entropy: 1.170525.\n",
      "Iteration 3861: Policy loss: -0.011585. Value loss: 0.147830. Entropy: 1.181963.\n",
      "episode: 1878   score: 180.0  epsilon: 1.0    steps: 160  evaluation reward: 186.9\n",
      "episode: 1879   score: 185.0  epsilon: 1.0    steps: 864  evaluation reward: 187.2\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3862: Policy loss: -0.000565. Value loss: 0.472654. Entropy: 1.219352.\n",
      "Iteration 3863: Policy loss: -0.009177. Value loss: 0.178370. Entropy: 1.224002.\n",
      "Iteration 3864: Policy loss: -0.014709. Value loss: 0.103563. Entropy: 1.217494.\n",
      "episode: 1880   score: 310.0  epsilon: 1.0    steps: 784  evaluation reward: 189.05\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3865: Policy loss: 0.009074. Value loss: 0.519240. Entropy: 1.118148.\n",
      "Iteration 3866: Policy loss: -0.002946. Value loss: 0.186911. Entropy: 1.129499.\n",
      "Iteration 3867: Policy loss: -0.008918. Value loss: 0.137797. Entropy: 1.142607.\n",
      "episode: 1881   score: 80.0  epsilon: 1.0    steps: 48  evaluation reward: 188.3\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3868: Policy loss: 0.004083. Value loss: 0.486346. Entropy: 1.185733.\n",
      "Iteration 3869: Policy loss: -0.004054. Value loss: 0.206234. Entropy: 1.179489.\n",
      "Iteration 3870: Policy loss: -0.009062. Value loss: 0.126240. Entropy: 1.185636.\n",
      "episode: 1882   score: 365.0  epsilon: 1.0    steps: 248  evaluation reward: 190.75\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3871: Policy loss: 0.003060. Value loss: 0.291845. Entropy: 1.223262.\n",
      "Iteration 3872: Policy loss: -0.005400. Value loss: 0.132409. Entropy: 1.213006.\n",
      "Iteration 3873: Policy loss: -0.013392. Value loss: 0.092835. Entropy: 1.220031.\n",
      "episode: 1883   score: 370.0  epsilon: 1.0    steps: 608  evaluation reward: 191.95\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3874: Policy loss: -0.001707. Value loss: 0.293024. Entropy: 1.221440.\n",
      "Iteration 3875: Policy loss: -0.007546. Value loss: 0.094033. Entropy: 1.209100.\n",
      "Iteration 3876: Policy loss: -0.010417. Value loss: 0.086838. Entropy: 1.213558.\n",
      "episode: 1884   score: 140.0  epsilon: 1.0    steps: 144  evaluation reward: 192.25\n",
      "episode: 1885   score: 180.0  epsilon: 1.0    steps: 424  evaluation reward: 192.7\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3877: Policy loss: 0.000940. Value loss: 0.372828. Entropy: 1.253254.\n",
      "Iteration 3878: Policy loss: -0.007341. Value loss: 0.177848. Entropy: 1.248235.\n",
      "Iteration 3879: Policy loss: -0.011037. Value loss: 0.132064. Entropy: 1.232512.\n",
      "episode: 1886   score: 230.0  epsilon: 1.0    steps: 264  evaluation reward: 194.05\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3880: Policy loss: 0.001772. Value loss: 0.324703. Entropy: 1.153798.\n",
      "Iteration 3881: Policy loss: -0.006503. Value loss: 0.139965. Entropy: 1.146659.\n",
      "Iteration 3882: Policy loss: -0.011687. Value loss: 0.100093. Entropy: 1.162275.\n",
      "episode: 1887   score: 155.0  epsilon: 1.0    steps: 40  evaluation reward: 194.0\n",
      "episode: 1888   score: 185.0  epsilon: 1.0    steps: 296  evaluation reward: 193.75\n",
      "episode: 1889   score: 150.0  epsilon: 1.0    steps: 672  evaluation reward: 192.8\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3883: Policy loss: 0.003722. Value loss: 0.381633. Entropy: 1.190903.\n",
      "Iteration 3884: Policy loss: -0.003641. Value loss: 0.214773. Entropy: 1.186224.\n",
      "Iteration 3885: Policy loss: -0.007095. Value loss: 0.161495. Entropy: 1.175357.\n",
      "episode: 1890   score: 70.0  epsilon: 1.0    steps: 600  evaluation reward: 191.55\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3886: Policy loss: 0.004391. Value loss: 0.411001. Entropy: 1.078001.\n",
      "Iteration 3887: Policy loss: -0.006992. Value loss: 0.235486. Entropy: 1.078690.\n",
      "Iteration 3888: Policy loss: -0.012046. Value loss: 0.175076. Entropy: 1.077617.\n",
      "episode: 1891   score: 260.0  epsilon: 1.0    steps: 16  evaluation reward: 192.85\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3889: Policy loss: 0.000016. Value loss: 0.342885. Entropy: 1.160792.\n",
      "Iteration 3890: Policy loss: -0.001748. Value loss: 0.141872. Entropy: 1.166180.\n",
      "Iteration 3891: Policy loss: -0.007991. Value loss: 0.104984. Entropy: 1.166684.\n",
      "episode: 1892   score: 110.0  epsilon: 1.0    steps: 144  evaluation reward: 191.85\n",
      "episode: 1893   score: 255.0  epsilon: 1.0    steps: 896  evaluation reward: 192.3\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3892: Policy loss: 0.001858. Value loss: 0.340397. Entropy: 1.175228.\n",
      "Iteration 3893: Policy loss: 0.000484. Value loss: 0.165905. Entropy: 1.182374.\n",
      "Iteration 3894: Policy loss: -0.005302. Value loss: 0.139822. Entropy: 1.191011.\n",
      "episode: 1894   score: 230.0  epsilon: 1.0    steps: 464  evaluation reward: 189.95\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3895: Policy loss: 0.005992. Value loss: 0.561549. Entropy: 1.245509.\n",
      "Iteration 3896: Policy loss: 0.005211. Value loss: 0.240371. Entropy: 1.260558.\n",
      "Iteration 3897: Policy loss: -0.002147. Value loss: 0.163752. Entropy: 1.248369.\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3898: Policy loss: 0.003322. Value loss: 0.497168. Entropy: 1.254476.\n",
      "Iteration 3899: Policy loss: -0.004586. Value loss: 0.186770. Entropy: 1.260572.\n",
      "Iteration 3900: Policy loss: -0.007498. Value loss: 0.135926. Entropy: 1.258636.\n",
      "episode: 1895   score: 125.0  epsilon: 1.0    steps: 224  evaluation reward: 186.85\n",
      "episode: 1896   score: 245.0  epsilon: 1.0    steps: 688  evaluation reward: 187.0\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3901: Policy loss: -0.000104. Value loss: 0.440414. Entropy: 1.239306.\n",
      "Iteration 3902: Policy loss: -0.006720. Value loss: 0.198915. Entropy: 1.240187.\n",
      "Iteration 3903: Policy loss: -0.013974. Value loss: 0.140939. Entropy: 1.245932.\n",
      "episode: 1897   score: 175.0  epsilon: 1.0    steps: 512  evaluation reward: 185.25\n",
      "episode: 1898   score: 55.0  epsilon: 1.0    steps: 520  evaluation reward: 183.65\n",
      "episode: 1899   score: 110.0  epsilon: 1.0    steps: 904  evaluation reward: 184.45\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3904: Policy loss: 0.000609. Value loss: 0.416786. Entropy: 1.205942.\n",
      "Iteration 3905: Policy loss: -0.008535. Value loss: 0.193785. Entropy: 1.208116.\n",
      "Iteration 3906: Policy loss: -0.008826. Value loss: 0.114482. Entropy: 1.211914.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3907: Policy loss: -0.001670. Value loss: 0.244539. Entropy: 1.158087.\n",
      "Iteration 3908: Policy loss: -0.000709. Value loss: 0.143742. Entropy: 1.179121.\n",
      "Iteration 3909: Policy loss: -0.010416. Value loss: 0.094027. Entropy: 1.171014.\n",
      "episode: 1900   score: 160.0  epsilon: 1.0    steps: 768  evaluation reward: 185.0\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3910: Policy loss: 0.000773. Value loss: 0.331684. Entropy: 1.252177.\n",
      "Iteration 3911: Policy loss: -0.007113. Value loss: 0.160352. Entropy: 1.237262.\n",
      "Iteration 3912: Policy loss: -0.010482. Value loss: 0.127048. Entropy: 1.243880.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3913: Policy loss: -0.001290. Value loss: 0.364048. Entropy: 1.226882.\n",
      "Iteration 3914: Policy loss: -0.007505. Value loss: 0.137638. Entropy: 1.220075.\n",
      "Iteration 3915: Policy loss: -0.013291. Value loss: 0.096284. Entropy: 1.230417.\n",
      "now time :  2019-02-28 11:41:47.355542\n",
      "episode: 1901   score: 155.0  epsilon: 1.0    steps: 16  evaluation reward: 184.75\n",
      "episode: 1902   score: 240.0  epsilon: 1.0    steps: 944  evaluation reward: 185.05\n",
      "episode: 1903   score: 610.0  epsilon: 1.0    steps: 992  evaluation reward: 189.35\n",
      "Training network. lr: 0.000220. clip: 0.088028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3916: Policy loss: 0.002186. Value loss: 1.160649. Entropy: 1.258114.\n",
      "Iteration 3917: Policy loss: 0.002723. Value loss: 0.519727. Entropy: 1.256294.\n",
      "Iteration 3918: Policy loss: -0.006899. Value loss: 0.346512. Entropy: 1.257146.\n",
      "episode: 1904   score: 90.0  epsilon: 1.0    steps: 176  evaluation reward: 189.35\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3919: Policy loss: 0.004795. Value loss: 0.442489. Entropy: 1.218318.\n",
      "Iteration 3920: Policy loss: -0.002904. Value loss: 0.211697. Entropy: 1.217895.\n",
      "Iteration 3921: Policy loss: -0.008074. Value loss: 0.134013. Entropy: 1.224160.\n",
      "episode: 1905   score: 410.0  epsilon: 1.0    steps: 432  evaluation reward: 192.2\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3922: Policy loss: -0.001148. Value loss: 0.370151. Entropy: 1.159202.\n",
      "Iteration 3923: Policy loss: -0.006955. Value loss: 0.218539. Entropy: 1.177886.\n",
      "Iteration 3924: Policy loss: -0.012219. Value loss: 0.140137. Entropy: 1.165099.\n",
      "episode: 1906   score: 125.0  epsilon: 1.0    steps: 1008  evaluation reward: 192.15\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3925: Policy loss: 0.009369. Value loss: 0.432281. Entropy: 1.160925.\n",
      "Iteration 3926: Policy loss: 0.000309. Value loss: 0.189588. Entropy: 1.170866.\n",
      "Iteration 3927: Policy loss: -0.006037. Value loss: 0.131881. Entropy: 1.177156.\n",
      "episode: 1907   score: 440.0  epsilon: 1.0    steps: 256  evaluation reward: 194.45\n",
      "episode: 1908   score: 260.0  epsilon: 1.0    steps: 720  evaluation reward: 195.25\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3928: Policy loss: -0.000442. Value loss: 0.392644. Entropy: 1.244965.\n",
      "Iteration 3929: Policy loss: -0.006800. Value loss: 0.166571. Entropy: 1.234198.\n",
      "Iteration 3930: Policy loss: -0.007868. Value loss: 0.127559. Entropy: 1.244737.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3931: Policy loss: 0.011317. Value loss: 0.758442. Entropy: 1.198882.\n",
      "Iteration 3932: Policy loss: 0.004567. Value loss: 0.380128. Entropy: 1.208595.\n",
      "Iteration 3933: Policy loss: -0.005900. Value loss: 0.238520. Entropy: 1.193976.\n",
      "episode: 1909   score: 145.0  epsilon: 1.0    steps: 16  evaluation reward: 194.6\n",
      "episode: 1910   score: 225.0  epsilon: 1.0    steps: 272  evaluation reward: 195.95\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3934: Policy loss: 0.004906. Value loss: 0.555542. Entropy: 1.186078.\n",
      "Iteration 3935: Policy loss: -0.001273. Value loss: 0.282615. Entropy: 1.163436.\n",
      "Iteration 3936: Policy loss: -0.008885. Value loss: 0.251053. Entropy: 1.182270.\n",
      "episode: 1911   score: 270.0  epsilon: 1.0    steps: 520  evaluation reward: 196.35\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3937: Policy loss: 0.003207. Value loss: 0.329925. Entropy: 1.213968.\n",
      "Iteration 3938: Policy loss: -0.001297. Value loss: 0.112586. Entropy: 1.206851.\n",
      "Iteration 3939: Policy loss: -0.012033. Value loss: 0.102857. Entropy: 1.211254.\n",
      "episode: 1912   score: 110.0  epsilon: 1.0    steps: 848  evaluation reward: 196.1\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3940: Policy loss: 0.004414. Value loss: 0.740996. Entropy: 1.251819.\n",
      "Iteration 3941: Policy loss: 0.000497. Value loss: 0.338598. Entropy: 1.249425.\n",
      "Iteration 3942: Policy loss: -0.009657. Value loss: 0.209802. Entropy: 1.257900.\n",
      "episode: 1913   score: 215.0  epsilon: 1.0    steps: 216  evaluation reward: 196.9\n",
      "episode: 1914   score: 90.0  epsilon: 1.0    steps: 384  evaluation reward: 196.2\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3943: Policy loss: 0.007032. Value loss: 0.439627. Entropy: 1.269637.\n",
      "Iteration 3944: Policy loss: 0.000077. Value loss: 0.226631. Entropy: 1.271340.\n",
      "Iteration 3945: Policy loss: -0.005575. Value loss: 0.167024. Entropy: 1.272882.\n",
      "episode: 1915   score: 545.0  epsilon: 1.0    steps: 96  evaluation reward: 199.8\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3946: Policy loss: 0.007328. Value loss: 0.421533. Entropy: 1.160880.\n",
      "Iteration 3947: Policy loss: -0.000405. Value loss: 0.178436. Entropy: 1.171624.\n",
      "Iteration 3948: Policy loss: -0.004725. Value loss: 0.124354. Entropy: 1.177914.\n",
      "episode: 1916   score: 140.0  epsilon: 1.0    steps: 232  evaluation reward: 200.4\n",
      "episode: 1917   score: 215.0  epsilon: 1.0    steps: 1008  evaluation reward: 200.7\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3949: Policy loss: 0.001971. Value loss: 0.442610. Entropy: 1.293449.\n",
      "Iteration 3950: Policy loss: -0.004641. Value loss: 0.180040. Entropy: 1.278629.\n",
      "Iteration 3951: Policy loss: -0.010386. Value loss: 0.117789. Entropy: 1.284888.\n",
      "episode: 1918   score: 265.0  epsilon: 1.0    steps: 168  evaluation reward: 202.1\n",
      "episode: 1919   score: 115.0  epsilon: 1.0    steps: 776  evaluation reward: 201.15\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3952: Policy loss: 0.003004. Value loss: 0.399811. Entropy: 1.178484.\n",
      "Iteration 3953: Policy loss: 0.000760. Value loss: 0.200802. Entropy: 1.207331.\n",
      "Iteration 3954: Policy loss: -0.003639. Value loss: 0.110758. Entropy: 1.194788.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3955: Policy loss: 0.000335. Value loss: 0.370881. Entropy: 1.127294.\n",
      "Iteration 3956: Policy loss: -0.004841. Value loss: 0.186065. Entropy: 1.151973.\n",
      "Iteration 3957: Policy loss: -0.011730. Value loss: 0.138113. Entropy: 1.165220.\n",
      "episode: 1920   score: 180.0  epsilon: 1.0    steps: 416  evaluation reward: 201.9\n",
      "episode: 1921   score: 210.0  epsilon: 1.0    steps: 744  evaluation reward: 201.65\n",
      "episode: 1922   score: 125.0  epsilon: 1.0    steps: 944  evaluation reward: 201.65\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3958: Policy loss: -0.001860. Value loss: 0.397519. Entropy: 1.248563.\n",
      "Iteration 3959: Policy loss: -0.006955. Value loss: 0.168038. Entropy: 1.242884.\n",
      "Iteration 3960: Policy loss: -0.014950. Value loss: 0.117678. Entropy: 1.246007.\n",
      "episode: 1923   score: 120.0  epsilon: 1.0    steps: 432  evaluation reward: 200.7\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3961: Policy loss: 0.009236. Value loss: 0.279868. Entropy: 1.121648.\n",
      "Iteration 3962: Policy loss: -0.008645. Value loss: 0.138584. Entropy: 1.113025.\n",
      "Iteration 3963: Policy loss: -0.012043. Value loss: 0.114838. Entropy: 1.108482.\n",
      "episode: 1924   score: 210.0  epsilon: 1.0    steps: 912  evaluation reward: 200.25\n",
      "episode: 1925   score: 210.0  epsilon: 1.0    steps: 952  evaluation reward: 199.95\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3964: Policy loss: 0.001201. Value loss: 0.553605. Entropy: 1.204193.\n",
      "Iteration 3965: Policy loss: 0.000918. Value loss: 0.241931. Entropy: 1.205669.\n",
      "Iteration 3966: Policy loss: -0.010559. Value loss: 0.153189. Entropy: 1.194032.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3967: Policy loss: 0.002869. Value loss: 0.485252. Entropy: 1.206803.\n",
      "Iteration 3968: Policy loss: -0.000111. Value loss: 0.193157. Entropy: 1.187728.\n",
      "Iteration 3969: Policy loss: -0.003143. Value loss: 0.108786. Entropy: 1.192028.\n",
      "episode: 1926   score: 105.0  epsilon: 1.0    steps: 24  evaluation reward: 200.55\n",
      "episode: 1927   score: 270.0  epsilon: 1.0    steps: 152  evaluation reward: 201.65\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3970: Policy loss: -0.001754. Value loss: 0.354384. Entropy: 1.214998.\n",
      "Iteration 3971: Policy loss: -0.002246. Value loss: 0.214477. Entropy: 1.225990.\n",
      "Iteration 3972: Policy loss: -0.008266. Value loss: 0.132711. Entropy: 1.220749.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3973: Policy loss: 0.000866. Value loss: 0.391607. Entropy: 1.160350.\n",
      "Iteration 3974: Policy loss: 0.001576. Value loss: 0.176369. Entropy: 1.164965.\n",
      "Iteration 3975: Policy loss: -0.007864. Value loss: 0.113508. Entropy: 1.159910.\n",
      "episode: 1928   score: 260.0  epsilon: 1.0    steps: 16  evaluation reward: 199.8\n",
      "episode: 1929   score: 110.0  epsilon: 1.0    steps: 336  evaluation reward: 197.2\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3976: Policy loss: -0.001859. Value loss: 0.433458. Entropy: 1.225711.\n",
      "Iteration 3977: Policy loss: 0.000727. Value loss: 0.167946. Entropy: 1.232904.\n",
      "Iteration 3978: Policy loss: -0.007022. Value loss: 0.095631. Entropy: 1.229499.\n",
      "episode: 1930   score: 210.0  epsilon: 1.0    steps: 16  evaluation reward: 197.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1931   score: 180.0  epsilon: 1.0    steps: 992  evaluation reward: 197.8\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3979: Policy loss: 0.004592. Value loss: 0.388608. Entropy: 1.125445.\n",
      "Iteration 3980: Policy loss: -0.003295. Value loss: 0.254880. Entropy: 1.126695.\n",
      "Iteration 3981: Policy loss: -0.010845. Value loss: 0.166936. Entropy: 1.141471.\n",
      "episode: 1932   score: 290.0  epsilon: 1.0    steps: 440  evaluation reward: 199.7\n",
      "episode: 1933   score: 210.0  epsilon: 1.0    steps: 928  evaluation reward: 199.65\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3982: Policy loss: -0.001629. Value loss: 0.431528. Entropy: 1.225965.\n",
      "Iteration 3983: Policy loss: 0.000386. Value loss: 0.188512. Entropy: 1.222856.\n",
      "Iteration 3984: Policy loss: -0.008391. Value loss: 0.155959. Entropy: 1.216903.\n",
      "episode: 1934   score: 210.0  epsilon: 1.0    steps: 912  evaluation reward: 199.8\n",
      "episode: 1935   score: 215.0  epsilon: 1.0    steps: 952  evaluation reward: 200.7\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3985: Policy loss: 0.000587. Value loss: 0.329908. Entropy: 1.123431.\n",
      "Iteration 3986: Policy loss: -0.005256. Value loss: 0.177707. Entropy: 1.127643.\n",
      "Iteration 3987: Policy loss: -0.013318. Value loss: 0.123050. Entropy: 1.130898.\n",
      "episode: 1936   score: 205.0  epsilon: 1.0    steps: 184  evaluation reward: 201.05\n",
      "episode: 1937   score: 155.0  epsilon: 1.0    steps: 440  evaluation reward: 200.35\n",
      "episode: 1938   score: 120.0  epsilon: 1.0    steps: 760  evaluation reward: 200.35\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3988: Policy loss: 0.002949. Value loss: 0.205577. Entropy: 1.180692.\n",
      "Iteration 3989: Policy loss: -0.004115. Value loss: 0.134480. Entropy: 1.168993.\n",
      "Iteration 3990: Policy loss: -0.005236. Value loss: 0.100765. Entropy: 1.161167.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3991: Policy loss: 0.006521. Value loss: 0.422808. Entropy: 1.034416.\n",
      "Iteration 3992: Policy loss: -0.003229. Value loss: 0.165891. Entropy: 1.039609.\n",
      "Iteration 3993: Policy loss: -0.004704. Value loss: 0.126986. Entropy: 1.036050.\n",
      "episode: 1939   score: 185.0  epsilon: 1.0    steps: 360  evaluation reward: 201.0\n",
      "episode: 1940   score: 45.0  epsilon: 1.0    steps: 1016  evaluation reward: 199.05\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3994: Policy loss: 0.002317. Value loss: 0.520740. Entropy: 1.248070.\n",
      "Iteration 3995: Policy loss: -0.000380. Value loss: 0.214143. Entropy: 1.232476.\n",
      "Iteration 3996: Policy loss: -0.008340. Value loss: 0.155428. Entropy: 1.238704.\n",
      "episode: 1941   score: 80.0  epsilon: 1.0    steps: 808  evaluation reward: 198.2\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3997: Policy loss: 0.002340. Value loss: 0.367903. Entropy: 1.193109.\n",
      "Iteration 3998: Policy loss: -0.003021. Value loss: 0.173218. Entropy: 1.168583.\n",
      "Iteration 3999: Policy loss: -0.009143. Value loss: 0.150884. Entropy: 1.186941.\n",
      "episode: 1942   score: 210.0  epsilon: 1.0    steps: 232  evaluation reward: 198.05\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 4000: Policy loss: 0.001583. Value loss: 0.581729. Entropy: 1.193157.\n",
      "Iteration 4001: Policy loss: -0.002139. Value loss: 0.300273. Entropy: 1.160618.\n",
      "Iteration 4002: Policy loss: -0.010321. Value loss: 0.245565. Entropy: 1.174062.\n",
      "episode: 1943   score: 265.0  epsilon: 1.0    steps: 448  evaluation reward: 198.75\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4003: Policy loss: 0.002977. Value loss: 0.498622. Entropy: 1.183984.\n",
      "Iteration 4004: Policy loss: -0.005296. Value loss: 0.258467. Entropy: 1.177314.\n",
      "Iteration 4005: Policy loss: -0.011009. Value loss: 0.182636. Entropy: 1.176079.\n",
      "episode: 1944   score: 135.0  epsilon: 1.0    steps: 464  evaluation reward: 198.35\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4006: Policy loss: 0.002517. Value loss: 0.427074. Entropy: 1.225916.\n",
      "Iteration 4007: Policy loss: -0.001935. Value loss: 0.196689. Entropy: 1.219171.\n",
      "Iteration 4008: Policy loss: -0.008945. Value loss: 0.150625. Entropy: 1.221823.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4009: Policy loss: 0.001906. Value loss: 0.395428. Entropy: 1.216261.\n",
      "Iteration 4010: Policy loss: -0.008948. Value loss: 0.190852. Entropy: 1.221362.\n",
      "Iteration 4011: Policy loss: -0.015992. Value loss: 0.128324. Entropy: 1.224271.\n",
      "episode: 1945   score: 230.0  epsilon: 1.0    steps: 336  evaluation reward: 198.75\n",
      "episode: 1946   score: 180.0  epsilon: 1.0    steps: 1008  evaluation reward: 198.4\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4012: Policy loss: 0.005623. Value loss: 0.449874. Entropy: 1.221963.\n",
      "Iteration 4013: Policy loss: -0.003089. Value loss: 0.246960. Entropy: 1.226467.\n",
      "Iteration 4014: Policy loss: -0.010168. Value loss: 0.171829. Entropy: 1.208829.\n",
      "episode: 1947   score: 315.0  epsilon: 1.0    steps: 72  evaluation reward: 200.5\n",
      "episode: 1948   score: 195.0  epsilon: 1.0    steps: 208  evaluation reward: 201.0\n",
      "episode: 1949   score: 290.0  epsilon: 1.0    steps: 424  evaluation reward: 201.35\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4015: Policy loss: 0.003023. Value loss: 0.351939. Entropy: 1.088686.\n",
      "Iteration 4016: Policy loss: -0.004281. Value loss: 0.193587. Entropy: 1.113102.\n",
      "Iteration 4017: Policy loss: -0.004419. Value loss: 0.126408. Entropy: 1.113367.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4018: Policy loss: 0.003500. Value loss: 0.360294. Entropy: 1.071662.\n",
      "Iteration 4019: Policy loss: 0.001849. Value loss: 0.216711. Entropy: 1.067771.\n",
      "Iteration 4020: Policy loss: -0.003950. Value loss: 0.161317. Entropy: 1.078918.\n",
      "episode: 1950   score: 285.0  epsilon: 1.0    steps: 120  evaluation reward: 202.35\n",
      "now time :  2019-02-28 11:43:04.370467\n",
      "episode: 1951   score: 210.0  epsilon: 1.0    steps: 640  evaluation reward: 203.4\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4021: Policy loss: 0.003609. Value loss: 0.494509. Entropy: 1.212880.\n",
      "Iteration 4022: Policy loss: -0.004048. Value loss: 0.252267. Entropy: 1.211815.\n",
      "Iteration 4023: Policy loss: -0.014712. Value loss: 0.166122. Entropy: 1.196935.\n",
      "episode: 1952   score: 190.0  epsilon: 1.0    steps: 600  evaluation reward: 201.6\n",
      "episode: 1953   score: 135.0  epsilon: 1.0    steps: 896  evaluation reward: 200.85\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4024: Policy loss: 0.001157. Value loss: 0.274247. Entropy: 1.124415.\n",
      "Iteration 4025: Policy loss: -0.006156. Value loss: 0.133669. Entropy: 1.121064.\n",
      "Iteration 4026: Policy loss: -0.007119. Value loss: 0.085870. Entropy: 1.129207.\n",
      "episode: 1954   score: 150.0  epsilon: 1.0    steps: 352  evaluation reward: 200.05\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4027: Policy loss: 0.000607. Value loss: 0.273821. Entropy: 1.197434.\n",
      "Iteration 4028: Policy loss: -0.003050. Value loss: 0.145029. Entropy: 1.193825.\n",
      "Iteration 4029: Policy loss: -0.007435. Value loss: 0.092110. Entropy: 1.196067.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4030: Policy loss: 0.001220. Value loss: 0.486181. Entropy: 1.228464.\n",
      "Iteration 4031: Policy loss: -0.000717. Value loss: 0.192472. Entropy: 1.223815.\n",
      "Iteration 4032: Policy loss: -0.015474. Value loss: 0.138832. Entropy: 1.227138.\n",
      "episode: 1955   score: 215.0  epsilon: 1.0    steps: 256  evaluation reward: 201.4\n",
      "episode: 1956   score: 130.0  epsilon: 1.0    steps: 696  evaluation reward: 201.2\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4033: Policy loss: 0.001474. Value loss: 0.403506. Entropy: 1.225232.\n",
      "Iteration 4034: Policy loss: -0.007404. Value loss: 0.139308. Entropy: 1.218035.\n",
      "Iteration 4035: Policy loss: -0.008843. Value loss: 0.092320. Entropy: 1.231928.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4036: Policy loss: 0.002537. Value loss: 0.406756. Entropy: 1.136292.\n",
      "Iteration 4037: Policy loss: -0.004722. Value loss: 0.178676. Entropy: 1.129119.\n",
      "Iteration 4038: Policy loss: -0.006747. Value loss: 0.134019. Entropy: 1.125656.\n",
      "episode: 1957   score: 315.0  epsilon: 1.0    steps: 320  evaluation reward: 202.55\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4039: Policy loss: 0.008818. Value loss: 0.676757. Entropy: 1.250992.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4040: Policy loss: -0.002780. Value loss: 0.280594. Entropy: 1.240640.\n",
      "Iteration 4041: Policy loss: -0.006241. Value loss: 0.204831. Entropy: 1.241088.\n",
      "episode: 1958   score: 230.0  epsilon: 1.0    steps: 256  evaluation reward: 202.55\n",
      "episode: 1959   score: 225.0  epsilon: 1.0    steps: 408  evaluation reward: 203.2\n",
      "episode: 1960   score: 240.0  epsilon: 1.0    steps: 976  evaluation reward: 203.6\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4042: Policy loss: -0.000309. Value loss: 0.398325. Entropy: 1.187351.\n",
      "Iteration 4043: Policy loss: -0.003976. Value loss: 0.173269. Entropy: 1.189807.\n",
      "Iteration 4044: Policy loss: -0.014186. Value loss: 0.121435. Entropy: 1.185416.\n",
      "episode: 1961   score: 165.0  epsilon: 1.0    steps: 400  evaluation reward: 204.0\n",
      "episode: 1962   score: 430.0  epsilon: 1.0    steps: 944  evaluation reward: 205.75\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4045: Policy loss: 0.000168. Value loss: 0.675083. Entropy: 1.064833.\n",
      "Iteration 4046: Policy loss: 0.000974. Value loss: 0.318510. Entropy: 1.071141.\n",
      "Iteration 4047: Policy loss: -0.003840. Value loss: 0.202191. Entropy: 1.069824.\n",
      "episode: 1963   score: 245.0  epsilon: 1.0    steps: 336  evaluation reward: 207.15\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4048: Policy loss: 0.001982. Value loss: 0.618333. Entropy: 1.096932.\n",
      "Iteration 4049: Policy loss: -0.000629. Value loss: 0.245449. Entropy: 1.101793.\n",
      "Iteration 4050: Policy loss: -0.010447. Value loss: 0.162710. Entropy: 1.095519.\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4051: Policy loss: 0.003328. Value loss: 0.525061. Entropy: 1.140123.\n",
      "Iteration 4052: Policy loss: -0.002607. Value loss: 0.244546. Entropy: 1.129424.\n",
      "Iteration 4053: Policy loss: -0.008585. Value loss: 0.172581. Entropy: 1.139815.\n",
      "episode: 1964   score: 125.0  epsilon: 1.0    steps: 344  evaluation reward: 207.05\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4054: Policy loss: 0.000707. Value loss: 0.360094. Entropy: 1.175635.\n",
      "Iteration 4055: Policy loss: -0.005880. Value loss: 0.170984. Entropy: 1.173302.\n",
      "Iteration 4056: Policy loss: -0.014591. Value loss: 0.110363. Entropy: 1.175960.\n",
      "episode: 1965   score: 345.0  epsilon: 1.0    steps: 48  evaluation reward: 208.85\n",
      "episode: 1966   score: 135.0  epsilon: 1.0    steps: 80  evaluation reward: 208.2\n",
      "episode: 1967   score: 245.0  epsilon: 1.0    steps: 672  evaluation reward: 208.75\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4057: Policy loss: -0.000340. Value loss: 0.560265. Entropy: 1.132043.\n",
      "Iteration 4058: Policy loss: -0.001688. Value loss: 0.183970. Entropy: 1.139104.\n",
      "Iteration 4059: Policy loss: -0.008081. Value loss: 0.310385. Entropy: 1.134627.\n",
      "episode: 1968   score: 155.0  epsilon: 1.0    steps: 616  evaluation reward: 208.5\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4060: Policy loss: 0.002023. Value loss: 0.789594. Entropy: 1.097294.\n",
      "Iteration 4061: Policy loss: -0.000080. Value loss: 0.486062. Entropy: 1.099614.\n",
      "Iteration 4062: Policy loss: -0.008268. Value loss: 0.392652. Entropy: 1.084061.\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4063: Policy loss: 0.000852. Value loss: 0.957258. Entropy: 1.177688.\n",
      "Iteration 4064: Policy loss: 0.003548. Value loss: 0.360424. Entropy: 1.190069.\n",
      "Iteration 4065: Policy loss: -0.005534. Value loss: 0.207060. Entropy: 1.180450.\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4066: Policy loss: 0.003421. Value loss: 0.425569. Entropy: 1.175967.\n",
      "Iteration 4067: Policy loss: -0.004894. Value loss: 0.162931. Entropy: 1.170545.\n",
      "Iteration 4068: Policy loss: -0.010354. Value loss: 0.128195. Entropy: 1.174715.\n",
      "episode: 1969   score: 450.0  epsilon: 1.0    steps: 40  evaluation reward: 208.45\n",
      "episode: 1970   score: 130.0  epsilon: 1.0    steps: 48  evaluation reward: 208.95\n",
      "episode: 1971   score: 45.0  epsilon: 1.0    steps: 680  evaluation reward: 207.4\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4069: Policy loss: 0.002609. Value loss: 0.426067. Entropy: 1.183753.\n",
      "Iteration 4070: Policy loss: 0.000649. Value loss: 0.261847. Entropy: 1.177067.\n",
      "Iteration 4071: Policy loss: -0.004061. Value loss: 0.181008. Entropy: 1.179355.\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4072: Policy loss: 0.002260. Value loss: 0.645465. Entropy: 1.137521.\n",
      "Iteration 4073: Policy loss: 0.002307. Value loss: 0.279116. Entropy: 1.150049.\n",
      "Iteration 4074: Policy loss: -0.004590. Value loss: 0.143761. Entropy: 1.156269.\n",
      "episode: 1972   score: 270.0  epsilon: 1.0    steps: 288  evaluation reward: 209.05\n",
      "episode: 1973   score: 565.0  epsilon: 1.0    steps: 456  evaluation reward: 210.45\n",
      "episode: 1974   score: 415.0  epsilon: 1.0    steps: 656  evaluation reward: 212.4\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4075: Policy loss: 0.006390. Value loss: 0.478474. Entropy: 1.174966.\n",
      "Iteration 4076: Policy loss: 0.002057. Value loss: 0.232311. Entropy: 1.166652.\n",
      "Iteration 4077: Policy loss: -0.002607. Value loss: 0.198015. Entropy: 1.166187.\n",
      "episode: 1975   score: 410.0  epsilon: 1.0    steps: 384  evaluation reward: 214.8\n",
      "episode: 1976   score: 460.0  epsilon: 1.0    steps: 968  evaluation reward: 217.85\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4078: Policy loss: 0.003608. Value loss: 0.727039. Entropy: 1.038284.\n",
      "Iteration 4079: Policy loss: 0.000775. Value loss: 0.363827. Entropy: 1.021853.\n",
      "Iteration 4080: Policy loss: -0.009808. Value loss: 0.262351. Entropy: 1.035498.\n",
      "episode: 1977   score: 210.0  epsilon: 1.0    steps: 800  evaluation reward: 218.4\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4081: Policy loss: 0.007065. Value loss: 0.644746. Entropy: 1.176694.\n",
      "Iteration 4082: Policy loss: -0.003260. Value loss: 0.335878. Entropy: 1.195988.\n",
      "Iteration 4083: Policy loss: -0.005478. Value loss: 0.259119. Entropy: 1.193920.\n",
      "episode: 1978   score: 105.0  epsilon: 1.0    steps: 192  evaluation reward: 217.65\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4084: Policy loss: 0.000114. Value loss: 0.305506. Entropy: 1.111179.\n",
      "Iteration 4085: Policy loss: 0.000206. Value loss: 0.181990. Entropy: 1.107324.\n",
      "Iteration 4086: Policy loss: -0.009427. Value loss: 0.140603. Entropy: 1.107000.\n",
      "episode: 1979   score: 180.0  epsilon: 1.0    steps: 232  evaluation reward: 217.6\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4087: Policy loss: 0.000538. Value loss: 0.331513. Entropy: 1.199642.\n",
      "Iteration 4088: Policy loss: -0.003457. Value loss: 0.173506. Entropy: 1.201749.\n",
      "Iteration 4089: Policy loss: -0.006075. Value loss: 0.134414. Entropy: 1.192379.\n",
      "episode: 1980   score: 105.0  epsilon: 1.0    steps: 976  evaluation reward: 215.55\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4090: Policy loss: -0.000860. Value loss: 0.445668. Entropy: 1.235236.\n",
      "Iteration 4091: Policy loss: -0.002188. Value loss: 0.268580. Entropy: 1.215045.\n",
      "Iteration 4092: Policy loss: -0.010120. Value loss: 0.182651. Entropy: 1.236948.\n",
      "episode: 1981   score: 110.0  epsilon: 1.0    steps: 176  evaluation reward: 215.85\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4093: Policy loss: 0.002106. Value loss: 0.617140. Entropy: 1.214599.\n",
      "Iteration 4094: Policy loss: -0.003512. Value loss: 0.282225. Entropy: 1.225785.\n",
      "Iteration 4095: Policy loss: -0.011060. Value loss: 0.199792. Entropy: 1.223958.\n",
      "episode: 1982   score: 265.0  epsilon: 1.0    steps: 776  evaluation reward: 214.85\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4096: Policy loss: 0.000303. Value loss: 0.502276. Entropy: 1.219946.\n",
      "Iteration 4097: Policy loss: -0.006928. Value loss: 0.231192. Entropy: 1.218152.\n",
      "Iteration 4098: Policy loss: -0.015599. Value loss: 0.157973. Entropy: 1.233791.\n",
      "episode: 1983   score: 420.0  epsilon: 1.0    steps: 136  evaluation reward: 215.35\n",
      "episode: 1984   score: 290.0  epsilon: 1.0    steps: 136  evaluation reward: 216.85\n",
      "episode: 1985   score: 330.0  epsilon: 1.0    steps: 712  evaluation reward: 218.35\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4099: Policy loss: 0.003447. Value loss: 0.435941. Entropy: 1.209650.\n",
      "Iteration 4100: Policy loss: -0.002716. Value loss: 0.202566. Entropy: 1.209196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4101: Policy loss: -0.004497. Value loss: 0.120975. Entropy: 1.217266.\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4102: Policy loss: -0.001465. Value loss: 0.301849. Entropy: 1.149184.\n",
      "Iteration 4103: Policy loss: -0.001267. Value loss: 0.146936. Entropy: 1.160625.\n",
      "Iteration 4104: Policy loss: -0.011104. Value loss: 0.087083. Entropy: 1.152055.\n",
      "episode: 1986   score: 215.0  epsilon: 1.0    steps: 256  evaluation reward: 218.2\n",
      "episode: 1987   score: 210.0  epsilon: 1.0    steps: 560  evaluation reward: 218.75\n",
      "episode: 1988   score: 185.0  epsilon: 1.0    steps: 848  evaluation reward: 218.75\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4105: Policy loss: -0.000835. Value loss: 0.647624. Entropy: 1.267611.\n",
      "Iteration 4106: Policy loss: -0.000891. Value loss: 0.220839. Entropy: 1.257390.\n",
      "Iteration 4107: Policy loss: -0.007037. Value loss: 0.181673. Entropy: 1.257969.\n",
      "episode: 1989   score: 160.0  epsilon: 1.0    steps: 104  evaluation reward: 218.85\n",
      "episode: 1990   score: 90.0  epsilon: 1.0    steps: 1000  evaluation reward: 219.05\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4108: Policy loss: 0.000543. Value loss: 0.270293. Entropy: 1.065163.\n",
      "Iteration 4109: Policy loss: -0.005418. Value loss: 0.112957. Entropy: 1.062180.\n",
      "Iteration 4110: Policy loss: -0.009837. Value loss: 0.075260. Entropy: 1.062817.\n",
      "episode: 1991   score: 35.0  epsilon: 1.0    steps: 672  evaluation reward: 216.8\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4111: Policy loss: 0.000546. Value loss: 0.481782. Entropy: 1.227453.\n",
      "Iteration 4112: Policy loss: -0.000365. Value loss: 0.222631. Entropy: 1.209814.\n",
      "Iteration 4113: Policy loss: -0.009212. Value loss: 0.142640. Entropy: 1.214836.\n",
      "episode: 1992   score: 155.0  epsilon: 1.0    steps: 72  evaluation reward: 217.25\n",
      "episode: 1993   score: 60.0  epsilon: 1.0    steps: 704  evaluation reward: 215.3\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4114: Policy loss: 0.003308. Value loss: 0.259697. Entropy: 1.131127.\n",
      "Iteration 4115: Policy loss: -0.001157. Value loss: 0.128824. Entropy: 1.120438.\n",
      "Iteration 4116: Policy loss: -0.007931. Value loss: 0.110365. Entropy: 1.135884.\n",
      "episode: 1994   score: 180.0  epsilon: 1.0    steps: 576  evaluation reward: 214.8\n",
      "episode: 1995   score: 155.0  epsilon: 1.0    steps: 704  evaluation reward: 215.1\n",
      "episode: 1996   score: 80.0  epsilon: 1.0    steps: 952  evaluation reward: 213.45\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4117: Policy loss: -0.001023. Value loss: 0.380224. Entropy: 1.090681.\n",
      "Iteration 4118: Policy loss: -0.005787. Value loss: 0.179548. Entropy: 1.073377.\n",
      "Iteration 4119: Policy loss: -0.004804. Value loss: 0.112928. Entropy: 1.074039.\n",
      "episode: 1997   score: 250.0  epsilon: 1.0    steps: 1008  evaluation reward: 214.2\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4120: Policy loss: -0.000939. Value loss: 0.353461. Entropy: 1.009751.\n",
      "Iteration 4121: Policy loss: -0.002263. Value loss: 0.119245. Entropy: 1.003206.\n",
      "Iteration 4122: Policy loss: -0.011196. Value loss: 0.111461. Entropy: 1.037952.\n",
      "episode: 1998   score: 80.0  epsilon: 1.0    steps: 496  evaluation reward: 214.45\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4123: Policy loss: 0.000775. Value loss: 0.424955. Entropy: 1.217831.\n",
      "Iteration 4124: Policy loss: -0.007368. Value loss: 0.169677. Entropy: 1.214427.\n",
      "Iteration 4125: Policy loss: -0.009881. Value loss: 0.132544. Entropy: 1.218583.\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4126: Policy loss: 0.001615. Value loss: 0.420339. Entropy: 1.114917.\n",
      "Iteration 4127: Policy loss: -0.008347. Value loss: 0.182445. Entropy: 1.100830.\n",
      "Iteration 4128: Policy loss: -0.007533. Value loss: 0.105318. Entropy: 1.104996.\n",
      "episode: 1999   score: 180.0  epsilon: 1.0    steps: 16  evaluation reward: 215.15\n",
      "episode: 2000   score: 120.0  epsilon: 1.0    steps: 448  evaluation reward: 214.75\n",
      "now time :  2019-02-28 11:44:22.842558\n",
      "episode: 2001   score: 210.0  epsilon: 1.0    steps: 776  evaluation reward: 215.3\n",
      "episode: 2002   score: 180.0  epsilon: 1.0    steps: 856  evaluation reward: 214.7\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4129: Policy loss: 0.003376. Value loss: 0.460510. Entropy: 1.210705.\n",
      "Iteration 4130: Policy loss: -0.005562. Value loss: 0.217573. Entropy: 1.212806.\n",
      "Iteration 4131: Policy loss: -0.011356. Value loss: 0.130410. Entropy: 1.177252.\n",
      "episode: 2003   score: 90.0  epsilon: 1.0    steps: 664  evaluation reward: 209.5\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4132: Policy loss: -0.002327. Value loss: 0.263269. Entropy: 1.071884.\n",
      "Iteration 4133: Policy loss: -0.009831. Value loss: 0.122775. Entropy: 1.049692.\n",
      "Iteration 4134: Policy loss: -0.016667. Value loss: 0.085461. Entropy: 1.067411.\n",
      "episode: 2004   score: 305.0  epsilon: 1.0    steps: 616  evaluation reward: 211.65\n",
      "episode: 2005   score: 80.0  epsilon: 1.0    steps: 1000  evaluation reward: 208.35\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4135: Policy loss: -0.000322. Value loss: 0.350436. Entropy: 1.224397.\n",
      "Iteration 4136: Policy loss: -0.005125. Value loss: 0.185322. Entropy: 1.221519.\n",
      "Iteration 4137: Policy loss: -0.009864. Value loss: 0.092013. Entropy: 1.210426.\n",
      "episode: 2006   score: 395.0  epsilon: 1.0    steps: 384  evaluation reward: 211.05\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4138: Policy loss: 0.002436. Value loss: 0.189508. Entropy: 1.057435.\n",
      "Iteration 4139: Policy loss: -0.008744. Value loss: 0.107625. Entropy: 1.039244.\n",
      "Iteration 4140: Policy loss: -0.013127. Value loss: 0.077513. Entropy: 1.053524.\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4141: Policy loss: 0.005285. Value loss: 0.440077. Entropy: 1.170901.\n",
      "Iteration 4142: Policy loss: 0.000092. Value loss: 0.193327. Entropy: 1.160979.\n",
      "Iteration 4143: Policy loss: -0.008475. Value loss: 0.125258. Entropy: 1.171949.\n",
      "episode: 2007   score: 155.0  epsilon: 1.0    steps: 32  evaluation reward: 208.2\n",
      "episode: 2008   score: 280.0  epsilon: 1.0    steps: 224  evaluation reward: 208.4\n",
      "episode: 2009   score: 210.0  epsilon: 1.0    steps: 576  evaluation reward: 209.05\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4144: Policy loss: 0.000628. Value loss: 0.342939. Entropy: 1.209682.\n",
      "Iteration 4145: Policy loss: -0.001961. Value loss: 0.131132. Entropy: 1.195549.\n",
      "Iteration 4146: Policy loss: -0.008190. Value loss: 0.119001. Entropy: 1.216207.\n",
      "episode: 2010   score: 185.0  epsilon: 1.0    steps: 904  evaluation reward: 208.65\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4147: Policy loss: -0.002012. Value loss: 0.384861. Entropy: 1.097503.\n",
      "Iteration 4148: Policy loss: -0.006834. Value loss: 0.196765. Entropy: 1.091819.\n",
      "Iteration 4149: Policy loss: -0.013634. Value loss: 0.111907. Entropy: 1.099465.\n",
      "episode: 2011   score: 195.0  epsilon: 1.0    steps: 968  evaluation reward: 207.9\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4150: Policy loss: -0.001931. Value loss: 0.551897. Entropy: 1.216226.\n",
      "Iteration 4151: Policy loss: 0.002363. Value loss: 0.226211. Entropy: 1.206588.\n",
      "Iteration 4152: Policy loss: -0.007447. Value loss: 0.159388. Entropy: 1.209258.\n",
      "episode: 2012   score: 90.0  epsilon: 1.0    steps: 360  evaluation reward: 207.7\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4153: Policy loss: -0.000520. Value loss: 0.509059. Entropy: 1.217668.\n",
      "Iteration 4154: Policy loss: -0.003369. Value loss: 0.266009. Entropy: 1.210568.\n",
      "Iteration 4155: Policy loss: -0.010241. Value loss: 0.167566. Entropy: 1.220206.\n",
      "episode: 2013   score: 290.0  epsilon: 1.0    steps: 848  evaluation reward: 208.45\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4156: Policy loss: 0.000522. Value loss: 0.455309. Entropy: 1.127512.\n",
      "Iteration 4157: Policy loss: -0.005644. Value loss: 0.196092. Entropy: 1.144417.\n",
      "Iteration 4158: Policy loss: -0.011736. Value loss: 0.139888. Entropy: 1.136666.\n",
      "episode: 2014   score: 180.0  epsilon: 1.0    steps: 72  evaluation reward: 209.35\n",
      "episode: 2015   score: 235.0  epsilon: 1.0    steps: 720  evaluation reward: 206.25\n",
      "episode: 2016   score: 135.0  epsilon: 1.0    steps: 912  evaluation reward: 206.2\n",
      "Training network. lr: 0.000218. clip: 0.087254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4159: Policy loss: -0.000414. Value loss: 0.265203. Entropy: 1.145553.\n",
      "Iteration 4160: Policy loss: -0.004165. Value loss: 0.131320. Entropy: 1.154055.\n",
      "Iteration 4161: Policy loss: -0.008977. Value loss: 0.091495. Entropy: 1.164698.\n",
      "episode: 2017   score: 105.0  epsilon: 1.0    steps: 16  evaluation reward: 205.1\n",
      "episode: 2018   score: 320.0  epsilon: 1.0    steps: 520  evaluation reward: 205.65\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4162: Policy loss: 0.000880. Value loss: 0.223426. Entropy: 1.074050.\n",
      "Iteration 4163: Policy loss: 0.002731. Value loss: 0.106002. Entropy: 1.061554.\n",
      "Iteration 4164: Policy loss: -0.006381. Value loss: 0.088201. Entropy: 1.077466.\n",
      "episode: 2019   score: 125.0  epsilon: 1.0    steps: 656  evaluation reward: 205.75\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4165: Policy loss: 0.004479. Value loss: 0.391821. Entropy: 1.025317.\n",
      "Iteration 4166: Policy loss: -0.000614. Value loss: 0.176060. Entropy: 1.059421.\n",
      "Iteration 4167: Policy loss: -0.007874. Value loss: 0.122714. Entropy: 1.022988.\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4168: Policy loss: -0.001300. Value loss: 0.475241. Entropy: 1.139415.\n",
      "Iteration 4169: Policy loss: -0.003243. Value loss: 0.152054. Entropy: 1.131667.\n",
      "Iteration 4170: Policy loss: -0.006264. Value loss: 0.107155. Entropy: 1.142194.\n",
      "episode: 2020   score: 110.0  epsilon: 1.0    steps: 56  evaluation reward: 205.05\n",
      "episode: 2021   score: 235.0  epsilon: 1.0    steps: 696  evaluation reward: 205.3\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4171: Policy loss: 0.002888. Value loss: 1.123966. Entropy: 1.237819.\n",
      "Iteration 4172: Policy loss: -0.001457. Value loss: 0.445660. Entropy: 1.232686.\n",
      "Iteration 4173: Policy loss: -0.003470. Value loss: 0.277385. Entropy: 1.233489.\n",
      "episode: 2022   score: 155.0  epsilon: 1.0    steps: 144  evaluation reward: 205.6\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4174: Policy loss: 0.002582. Value loss: 0.643718. Entropy: 1.185062.\n",
      "Iteration 4175: Policy loss: -0.001562. Value loss: 0.233171. Entropy: 1.202211.\n",
      "Iteration 4176: Policy loss: -0.008244. Value loss: 0.186775. Entropy: 1.185621.\n",
      "episode: 2023   score: 210.0  epsilon: 1.0    steps: 152  evaluation reward: 206.5\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4177: Policy loss: -0.001045. Value loss: 0.345529. Entropy: 1.169097.\n",
      "Iteration 4178: Policy loss: -0.002677. Value loss: 0.164334. Entropy: 1.160971.\n",
      "Iteration 4179: Policy loss: -0.010752. Value loss: 0.101036. Entropy: 1.182765.\n",
      "episode: 2024   score: 105.0  epsilon: 1.0    steps: 344  evaluation reward: 205.45\n",
      "episode: 2025   score: 165.0  epsilon: 1.0    steps: 912  evaluation reward: 205.0\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4180: Policy loss: 0.002517. Value loss: 0.243433. Entropy: 1.149745.\n",
      "Iteration 4181: Policy loss: -0.004790. Value loss: 0.130719. Entropy: 1.155615.\n",
      "Iteration 4182: Policy loss: -0.005835. Value loss: 0.085904. Entropy: 1.157950.\n",
      "episode: 2026   score: 440.0  epsilon: 1.0    steps: 112  evaluation reward: 208.35\n",
      "episode: 2027   score: 90.0  epsilon: 1.0    steps: 512  evaluation reward: 206.55\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4183: Policy loss: -0.001192. Value loss: 0.260616. Entropy: 1.134501.\n",
      "Iteration 4184: Policy loss: -0.001452. Value loss: 0.165797. Entropy: 1.133013.\n",
      "Iteration 4185: Policy loss: -0.007067. Value loss: 0.105107. Entropy: 1.131446.\n",
      "episode: 2028   score: 295.0  epsilon: 1.0    steps: 848  evaluation reward: 206.9\n",
      "episode: 2029   score: 135.0  epsilon: 1.0    steps: 968  evaluation reward: 207.15\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4186: Policy loss: 0.000600. Value loss: 0.342330. Entropy: 1.047181.\n",
      "Iteration 4187: Policy loss: 0.002797. Value loss: 0.114548. Entropy: 1.054344.\n",
      "Iteration 4188: Policy loss: -0.013793. Value loss: 0.091683. Entropy: 1.045523.\n",
      "episode: 2030   score: 150.0  epsilon: 1.0    steps: 264  evaluation reward: 206.55\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4189: Policy loss: -0.000007. Value loss: 0.125133. Entropy: 1.080016.\n",
      "Iteration 4190: Policy loss: -0.000210. Value loss: 0.070978. Entropy: 1.051938.\n",
      "Iteration 4191: Policy loss: -0.004954. Value loss: 0.056637. Entropy: 1.084174.\n",
      "episode: 2031   score: 150.0  epsilon: 1.0    steps: 384  evaluation reward: 206.25\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4192: Policy loss: 0.001843. Value loss: 0.306545. Entropy: 1.159107.\n",
      "Iteration 4193: Policy loss: -0.003296. Value loss: 0.146427. Entropy: 1.147321.\n",
      "Iteration 4194: Policy loss: -0.012340. Value loss: 0.103485. Entropy: 1.140509.\n",
      "episode: 2032   score: 120.0  epsilon: 1.0    steps: 320  evaluation reward: 204.55\n",
      "episode: 2033   score: 265.0  epsilon: 1.0    steps: 928  evaluation reward: 205.1\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4195: Policy loss: 0.002274. Value loss: 0.299957. Entropy: 1.173376.\n",
      "Iteration 4196: Policy loss: -0.002849. Value loss: 0.211247. Entropy: 1.159921.\n",
      "Iteration 4197: Policy loss: -0.010534. Value loss: 0.160677. Entropy: 1.185126.\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4198: Policy loss: 0.002304. Value loss: 0.239357. Entropy: 1.091051.\n",
      "Iteration 4199: Policy loss: -0.002745. Value loss: 0.148796. Entropy: 1.108412.\n",
      "Iteration 4200: Policy loss: -0.000388. Value loss: 0.097321. Entropy: 1.108615.\n",
      "episode: 2034   score: 150.0  epsilon: 1.0    steps: 192  evaluation reward: 204.5\n",
      "episode: 2035   score: 110.0  epsilon: 1.0    steps: 520  evaluation reward: 203.45\n",
      "episode: 2036   score: 210.0  epsilon: 1.0    steps: 592  evaluation reward: 203.5\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4201: Policy loss: 0.001908. Value loss: 0.193644. Entropy: 1.208532.\n",
      "Iteration 4202: Policy loss: -0.008723. Value loss: 0.083135. Entropy: 1.203282.\n",
      "Iteration 4203: Policy loss: -0.014249. Value loss: 0.066709. Entropy: 1.209011.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4204: Policy loss: 0.000309. Value loss: 0.164970. Entropy: 0.889123.\n",
      "Iteration 4205: Policy loss: -0.000535. Value loss: 0.091188. Entropy: 0.895768.\n",
      "Iteration 4206: Policy loss: -0.006781. Value loss: 0.066406. Entropy: 0.881477.\n",
      "episode: 2037   score: 230.0  epsilon: 1.0    steps: 280  evaluation reward: 204.25\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4207: Policy loss: 0.000865. Value loss: 0.243832. Entropy: 1.278309.\n",
      "Iteration 4208: Policy loss: -0.004511. Value loss: 0.134126. Entropy: 1.269629.\n",
      "Iteration 4209: Policy loss: -0.010827. Value loss: 0.108426. Entropy: 1.279347.\n",
      "episode: 2038   score: 210.0  epsilon: 1.0    steps: 280  evaluation reward: 205.15\n",
      "episode: 2039   score: 225.0  epsilon: 1.0    steps: 800  evaluation reward: 205.55\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4210: Policy loss: 0.001025. Value loss: 0.293485. Entropy: 1.113326.\n",
      "Iteration 4211: Policy loss: -0.012562. Value loss: 0.133803. Entropy: 1.097832.\n",
      "Iteration 4212: Policy loss: -0.011273. Value loss: 0.102398. Entropy: 1.119609.\n",
      "episode: 2040   score: 155.0  epsilon: 1.0    steps: 208  evaluation reward: 206.65\n",
      "episode: 2041   score: 140.0  epsilon: 1.0    steps: 592  evaluation reward: 207.25\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4213: Policy loss: 0.002246. Value loss: 0.363554. Entropy: 1.098381.\n",
      "Iteration 4214: Policy loss: -0.000769. Value loss: 0.143834. Entropy: 1.111629.\n",
      "Iteration 4215: Policy loss: -0.009949. Value loss: 0.110442. Entropy: 1.105334.\n",
      "episode: 2042   score: 155.0  epsilon: 1.0    steps: 312  evaluation reward: 206.7\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4216: Policy loss: 0.008498. Value loss: 0.934430. Entropy: 1.107764.\n",
      "Iteration 4217: Policy loss: -0.006285. Value loss: 0.523913. Entropy: 1.117676.\n",
      "Iteration 4218: Policy loss: -0.001033. Value loss: 0.375304. Entropy: 1.116630.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4219: Policy loss: 0.003932. Value loss: 0.408050. Entropy: 1.126754.\n",
      "Iteration 4220: Policy loss: 0.001076. Value loss: 0.209635. Entropy: 1.119291.\n",
      "Iteration 4221: Policy loss: 0.000358. Value loss: 0.126491. Entropy: 1.139187.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2043   score: 210.0  epsilon: 1.0    steps: 600  evaluation reward: 206.15\n",
      "episode: 2044   score: 150.0  epsilon: 1.0    steps: 960  evaluation reward: 206.3\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4222: Policy loss: 0.003156. Value loss: 0.478317. Entropy: 1.252066.\n",
      "Iteration 4223: Policy loss: 0.000841. Value loss: 0.211136. Entropy: 1.250282.\n",
      "Iteration 4224: Policy loss: -0.010254. Value loss: 0.176464. Entropy: 1.251277.\n",
      "episode: 2045   score: 535.0  epsilon: 1.0    steps: 208  evaluation reward: 209.35\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4225: Policy loss: 0.002783. Value loss: 0.509731. Entropy: 1.110202.\n",
      "Iteration 4226: Policy loss: 0.003637. Value loss: 0.215188. Entropy: 1.120080.\n",
      "Iteration 4227: Policy loss: -0.008045. Value loss: 0.098103. Entropy: 1.144962.\n",
      "episode: 2046   score: 80.0  epsilon: 1.0    steps: 64  evaluation reward: 208.35\n",
      "episode: 2047   score: 400.0  epsilon: 1.0    steps: 544  evaluation reward: 209.2\n",
      "episode: 2048   score: 500.0  epsilon: 1.0    steps: 624  evaluation reward: 212.25\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4228: Policy loss: 0.003225. Value loss: 0.598473. Entropy: 1.150794.\n",
      "Iteration 4229: Policy loss: -0.000300. Value loss: 0.324337. Entropy: 1.147964.\n",
      "Iteration 4230: Policy loss: -0.005210. Value loss: 0.319152. Entropy: 1.157457.\n",
      "episode: 2049   score: 180.0  epsilon: 1.0    steps: 816  evaluation reward: 211.15\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4231: Policy loss: 0.003193. Value loss: 0.380115. Entropy: 1.058651.\n",
      "Iteration 4232: Policy loss: -0.006101. Value loss: 0.197020. Entropy: 1.061091.\n",
      "Iteration 4233: Policy loss: -0.014260. Value loss: 0.148224. Entropy: 1.053200.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4234: Policy loss: 0.001455. Value loss: 0.640276. Entropy: 1.171343.\n",
      "Iteration 4235: Policy loss: -0.002384. Value loss: 0.329958. Entropy: 1.163136.\n",
      "Iteration 4236: Policy loss: -0.008112. Value loss: 0.201061. Entropy: 1.177118.\n",
      "episode: 2050   score: 260.0  epsilon: 1.0    steps: 112  evaluation reward: 210.9\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4237: Policy loss: -0.002879. Value loss: 0.381420. Entropy: 1.223629.\n",
      "Iteration 4238: Policy loss: -0.008997. Value loss: 0.178433. Entropy: 1.214265.\n",
      "Iteration 4239: Policy loss: -0.013912. Value loss: 0.147867. Entropy: 1.210240.\n",
      "now time :  2019-02-28 11:45:43.000870\n",
      "episode: 2051   score: 195.0  epsilon: 1.0    steps: 600  evaluation reward: 210.75\n",
      "episode: 2052   score: 125.0  epsilon: 1.0    steps: 784  evaluation reward: 210.1\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4240: Policy loss: 0.000068. Value loss: 0.444089. Entropy: 1.231000.\n",
      "Iteration 4241: Policy loss: -0.005589. Value loss: 0.176368. Entropy: 1.216035.\n",
      "Iteration 4242: Policy loss: -0.007036. Value loss: 0.148717. Entropy: 1.225403.\n",
      "episode: 2053   score: 125.0  epsilon: 1.0    steps: 440  evaluation reward: 210.0\n",
      "episode: 2054   score: 165.0  epsilon: 1.0    steps: 656  evaluation reward: 210.15\n",
      "episode: 2055   score: 130.0  epsilon: 1.0    steps: 848  evaluation reward: 209.3\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4243: Policy loss: 0.001041. Value loss: 0.230128. Entropy: 1.105267.\n",
      "Iteration 4244: Policy loss: -0.005460. Value loss: 0.146048. Entropy: 1.101655.\n",
      "Iteration 4245: Policy loss: -0.007560. Value loss: 0.108358. Entropy: 1.103422.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4246: Policy loss: 0.002213. Value loss: 0.594445. Entropy: 1.126032.\n",
      "Iteration 4247: Policy loss: -0.003559. Value loss: 0.315848. Entropy: 1.129579.\n",
      "Iteration 4248: Policy loss: -0.004209. Value loss: 0.178765. Entropy: 1.124610.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4249: Policy loss: -0.000329. Value loss: 0.627781. Entropy: 1.170637.\n",
      "Iteration 4250: Policy loss: -0.005785. Value loss: 0.294530. Entropy: 1.167686.\n",
      "Iteration 4251: Policy loss: -0.008652. Value loss: 0.197371. Entropy: 1.164418.\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4252: Policy loss: 0.004932. Value loss: 0.661360. Entropy: 1.219343.\n",
      "Iteration 4253: Policy loss: -0.003586. Value loss: 0.363337. Entropy: 1.225950.\n",
      "Iteration 4254: Policy loss: -0.005978. Value loss: 0.264617. Entropy: 1.219324.\n",
      "episode: 2056   score: 420.0  epsilon: 1.0    steps: 72  evaluation reward: 212.2\n",
      "episode: 2057   score: 500.0  epsilon: 1.0    steps: 664  evaluation reward: 214.05\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4255: Policy loss: -0.003394. Value loss: 0.372313. Entropy: 1.221154.\n",
      "Iteration 4256: Policy loss: -0.009616. Value loss: 0.213937. Entropy: 1.232923.\n",
      "Iteration 4257: Policy loss: -0.012866. Value loss: 0.122614. Entropy: 1.227196.\n",
      "episode: 2058   score: 155.0  epsilon: 1.0    steps: 344  evaluation reward: 213.3\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4258: Policy loss: 0.001658. Value loss: 0.563137. Entropy: 1.206185.\n",
      "Iteration 4259: Policy loss: -0.004040. Value loss: 0.223821. Entropy: 1.192100.\n",
      "Iteration 4260: Policy loss: -0.008929. Value loss: 0.164959. Entropy: 1.194213.\n",
      "episode: 2059   score: 260.0  epsilon: 1.0    steps: 456  evaluation reward: 213.65\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4261: Policy loss: -0.000467. Value loss: 0.600851. Entropy: 1.215903.\n",
      "Iteration 4262: Policy loss: -0.006246. Value loss: 0.238423. Entropy: 1.204813.\n",
      "Iteration 4263: Policy loss: -0.012215. Value loss: 0.158921. Entropy: 1.225216.\n",
      "episode: 2060   score: 380.0  epsilon: 1.0    steps: 464  evaluation reward: 215.05\n",
      "episode: 2061   score: 260.0  epsilon: 1.0    steps: 680  evaluation reward: 216.0\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4264: Policy loss: 0.001102. Value loss: 0.623342. Entropy: 1.187975.\n",
      "Iteration 4265: Policy loss: -0.000767. Value loss: 0.309005. Entropy: 1.177064.\n",
      "Iteration 4266: Policy loss: -0.004061. Value loss: 0.244807. Entropy: 1.190920.\n",
      "episode: 2062   score: 170.0  epsilon: 1.0    steps: 48  evaluation reward: 213.4\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4267: Policy loss: 0.003194. Value loss: 0.465395. Entropy: 1.150167.\n",
      "Iteration 4268: Policy loss: -0.007232. Value loss: 0.231948. Entropy: 1.145798.\n",
      "Iteration 4269: Policy loss: -0.011756. Value loss: 0.187280. Entropy: 1.143208.\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4270: Policy loss: 0.002641. Value loss: 0.726609. Entropy: 1.195230.\n",
      "Iteration 4271: Policy loss: -0.001271. Value loss: 0.224352. Entropy: 1.197821.\n",
      "Iteration 4272: Policy loss: -0.008785. Value loss: 0.195992. Entropy: 1.188512.\n",
      "episode: 2063   score: 300.0  epsilon: 1.0    steps: 392  evaluation reward: 213.95\n",
      "episode: 2064   score: 430.0  epsilon: 1.0    steps: 448  evaluation reward: 217.0\n",
      "episode: 2065   score: 180.0  epsilon: 1.0    steps: 784  evaluation reward: 215.35\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4273: Policy loss: 0.000499. Value loss: 0.956639. Entropy: 1.208987.\n",
      "Iteration 4274: Policy loss: -0.005103. Value loss: 0.390758. Entropy: 1.227483.\n",
      "Iteration 4275: Policy loss: -0.007189. Value loss: 0.267536. Entropy: 1.225681.\n",
      "episode: 2066   score: 280.0  epsilon: 1.0    steps: 680  evaluation reward: 216.8\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4276: Policy loss: 0.003039. Value loss: 0.558774. Entropy: 1.146341.\n",
      "Iteration 4277: Policy loss: -0.001616. Value loss: 0.266214. Entropy: 1.138318.\n",
      "Iteration 4278: Policy loss: -0.004101. Value loss: 0.197763. Entropy: 1.143273.\n",
      "episode: 2067   score: 130.0  epsilon: 1.0    steps: 712  evaluation reward: 215.65\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4279: Policy loss: 0.001841. Value loss: 0.252908. Entropy: 1.230976.\n",
      "Iteration 4280: Policy loss: -0.002428. Value loss: 0.134826. Entropy: 1.230562.\n",
      "Iteration 4281: Policy loss: -0.008817. Value loss: 0.093524. Entropy: 1.230901.\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4282: Policy loss: 0.002542. Value loss: 0.416698. Entropy: 1.099345.\n",
      "Iteration 4283: Policy loss: -0.008299. Value loss: 0.197894. Entropy: 1.109000.\n",
      "Iteration 4284: Policy loss: -0.009989. Value loss: 0.150863. Entropy: 1.122272.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2068   score: 275.0  epsilon: 1.0    steps: 592  evaluation reward: 216.85\n",
      "episode: 2069   score: 110.0  epsilon: 1.0    steps: 760  evaluation reward: 213.45\n",
      "episode: 2070   score: 200.0  epsilon: 1.0    steps: 896  evaluation reward: 214.15\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4285: Policy loss: 0.001327. Value loss: 0.388712. Entropy: 1.239011.\n",
      "Iteration 4286: Policy loss: -0.003044. Value loss: 0.141756. Entropy: 1.253736.\n",
      "Iteration 4287: Policy loss: -0.007224. Value loss: 0.104643. Entropy: 1.247569.\n",
      "episode: 2071   score: 210.0  epsilon: 1.0    steps: 496  evaluation reward: 215.8\n",
      "episode: 2072   score: 265.0  epsilon: 1.0    steps: 944  evaluation reward: 215.75\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4288: Policy loss: 0.000856. Value loss: 0.471737. Entropy: 1.070927.\n",
      "Iteration 4289: Policy loss: -0.002105. Value loss: 0.239450. Entropy: 1.061016.\n",
      "Iteration 4290: Policy loss: -0.004999. Value loss: 0.144419. Entropy: 1.060395.\n",
      "episode: 2073   score: 120.0  epsilon: 1.0    steps: 680  evaluation reward: 211.3\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4291: Policy loss: 0.003660. Value loss: 0.518447. Entropy: 1.197527.\n",
      "Iteration 4292: Policy loss: -0.001686. Value loss: 0.276882. Entropy: 1.199974.\n",
      "Iteration 4293: Policy loss: -0.002326. Value loss: 0.209493. Entropy: 1.194605.\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4294: Policy loss: 0.000930. Value loss: 0.454002. Entropy: 1.139071.\n",
      "Iteration 4295: Policy loss: 0.000015. Value loss: 0.171115. Entropy: 1.137982.\n",
      "Iteration 4296: Policy loss: -0.008476. Value loss: 0.140159. Entropy: 1.125039.\n",
      "episode: 2074   score: 320.0  epsilon: 1.0    steps: 592  evaluation reward: 210.35\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4297: Policy loss: 0.004373. Value loss: 0.291450. Entropy: 1.235601.\n",
      "Iteration 4298: Policy loss: -0.003363. Value loss: 0.143968. Entropy: 1.217200.\n",
      "Iteration 4299: Policy loss: -0.005525. Value loss: 0.132926. Entropy: 1.226661.\n",
      "episode: 2075   score: 65.0  epsilon: 1.0    steps: 656  evaluation reward: 206.9\n",
      "episode: 2076   score: 155.0  epsilon: 1.0    steps: 872  evaluation reward: 203.85\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4300: Policy loss: 0.000058. Value loss: 0.344935. Entropy: 1.158242.\n",
      "Iteration 4301: Policy loss: -0.006522. Value loss: 0.185070. Entropy: 1.139374.\n",
      "Iteration 4302: Policy loss: -0.011707. Value loss: 0.130477. Entropy: 1.148333.\n",
      "episode: 2077   score: 160.0  epsilon: 1.0    steps: 424  evaluation reward: 203.35\n",
      "episode: 2078   score: 315.0  epsilon: 1.0    steps: 640  evaluation reward: 205.45\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4303: Policy loss: 0.001262. Value loss: 0.501105. Entropy: 1.194566.\n",
      "Iteration 4304: Policy loss: -0.006302. Value loss: 0.244858. Entropy: 1.200218.\n",
      "Iteration 4305: Policy loss: -0.009211. Value loss: 0.183169. Entropy: 1.206472.\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4306: Policy loss: 0.004880. Value loss: 0.390210. Entropy: 1.154729.\n",
      "Iteration 4307: Policy loss: -0.004656. Value loss: 0.187295. Entropy: 1.139306.\n",
      "Iteration 4308: Policy loss: -0.012104. Value loss: 0.143319. Entropy: 1.148764.\n",
      "episode: 2079   score: 345.0  epsilon: 1.0    steps: 136  evaluation reward: 207.1\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4309: Policy loss: 0.001175. Value loss: 0.524574. Entropy: 1.192213.\n",
      "Iteration 4310: Policy loss: -0.005294. Value loss: 0.240968. Entropy: 1.203722.\n",
      "Iteration 4311: Policy loss: -0.010107. Value loss: 0.176151. Entropy: 1.196346.\n",
      "episode: 2080   score: 225.0  epsilon: 1.0    steps: 112  evaluation reward: 208.3\n",
      "episode: 2081   score: 285.0  epsilon: 1.0    steps: 528  evaluation reward: 210.05\n",
      "episode: 2082   score: 135.0  epsilon: 1.0    steps: 856  evaluation reward: 208.75\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4312: Policy loss: 0.002234. Value loss: 0.257695. Entropy: 1.168917.\n",
      "Iteration 4313: Policy loss: -0.005770. Value loss: 0.151787. Entropy: 1.162638.\n",
      "Iteration 4314: Policy loss: -0.008945. Value loss: 0.113184. Entropy: 1.167181.\n",
      "episode: 2083   score: 140.0  epsilon: 1.0    steps: 32  evaluation reward: 205.95\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4315: Policy loss: -0.000571. Value loss: 0.284582. Entropy: 1.088064.\n",
      "Iteration 4316: Policy loss: -0.004199. Value loss: 0.173124. Entropy: 1.075610.\n",
      "Iteration 4317: Policy loss: -0.010916. Value loss: 0.106941. Entropy: 1.095371.\n",
      "episode: 2084   score: 210.0  epsilon: 1.0    steps: 680  evaluation reward: 205.15\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4318: Policy loss: 0.000257. Value loss: 0.494305. Entropy: 1.216410.\n",
      "Iteration 4319: Policy loss: -0.005117. Value loss: 0.179737. Entropy: 1.212170.\n",
      "Iteration 4320: Policy loss: -0.011809. Value loss: 0.132611. Entropy: 1.217074.\n",
      "episode: 2085   score: 265.0  epsilon: 1.0    steps: 576  evaluation reward: 204.5\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4321: Policy loss: 0.009425. Value loss: 0.636244. Entropy: 1.134010.\n",
      "Iteration 4322: Policy loss: -0.001726. Value loss: 0.278565. Entropy: 1.148018.\n",
      "Iteration 4323: Policy loss: -0.009175. Value loss: 0.143570. Entropy: 1.138260.\n",
      "episode: 2086   score: 275.0  epsilon: 1.0    steps: 288  evaluation reward: 205.1\n",
      "episode: 2087   score: 165.0  epsilon: 1.0    steps: 840  evaluation reward: 204.65\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4324: Policy loss: 0.002149. Value loss: 0.421970. Entropy: 1.113698.\n",
      "Iteration 4325: Policy loss: -0.003562. Value loss: 0.182065. Entropy: 1.129949.\n",
      "Iteration 4326: Policy loss: -0.011716. Value loss: 0.144915. Entropy: 1.102474.\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4327: Policy loss: -0.001407. Value loss: 0.303259. Entropy: 1.145813.\n",
      "Iteration 4328: Policy loss: -0.005080. Value loss: 0.177413. Entropy: 1.145853.\n",
      "Iteration 4329: Policy loss: -0.012092. Value loss: 0.097272. Entropy: 1.127887.\n",
      "episode: 2088   score: 230.0  epsilon: 1.0    steps: 760  evaluation reward: 205.1\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4330: Policy loss: 0.004024. Value loss: 0.851884. Entropy: 1.275952.\n",
      "Iteration 4331: Policy loss: 0.001307. Value loss: 0.298466. Entropy: 1.273744.\n",
      "Iteration 4332: Policy loss: -0.006834. Value loss: 0.209772. Entropy: 1.260657.\n",
      "episode: 2089   score: 120.0  epsilon: 1.0    steps: 312  evaluation reward: 204.7\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4333: Policy loss: 0.001202. Value loss: 0.400079. Entropy: 1.131805.\n",
      "Iteration 4334: Policy loss: -0.005882. Value loss: 0.183259. Entropy: 1.128240.\n",
      "Iteration 4335: Policy loss: -0.012583. Value loss: 0.169338. Entropy: 1.130783.\n",
      "episode: 2090   score: 430.0  epsilon: 1.0    steps: 256  evaluation reward: 208.1\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4336: Policy loss: 0.002138. Value loss: 0.436539. Entropy: 1.227160.\n",
      "Iteration 4337: Policy loss: -0.007610. Value loss: 0.199678. Entropy: 1.217938.\n",
      "Iteration 4338: Policy loss: -0.011037. Value loss: 0.123626. Entropy: 1.226406.\n",
      "episode: 2091   score: 290.0  epsilon: 1.0    steps: 296  evaluation reward: 210.65\n",
      "episode: 2092   score: 590.0  epsilon: 1.0    steps: 504  evaluation reward: 215.0\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4339: Policy loss: -0.002348. Value loss: 0.393156. Entropy: 1.235305.\n",
      "Iteration 4340: Policy loss: -0.005834. Value loss: 0.172353. Entropy: 1.237824.\n",
      "Iteration 4341: Policy loss: -0.014582. Value loss: 0.131336. Entropy: 1.241216.\n",
      "episode: 2093   score: 295.0  epsilon: 1.0    steps: 32  evaluation reward: 217.35\n",
      "episode: 2094   score: 290.0  epsilon: 1.0    steps: 688  evaluation reward: 218.45\n",
      "episode: 2095   score: 215.0  epsilon: 1.0    steps: 848  evaluation reward: 219.05\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4342: Policy loss: -0.002164. Value loss: 0.270740. Entropy: 1.116005.\n",
      "Iteration 4343: Policy loss: -0.008084. Value loss: 0.114474. Entropy: 1.117627.\n",
      "Iteration 4344: Policy loss: -0.012339. Value loss: 0.074372. Entropy: 1.126061.\n",
      "episode: 2096   score: 80.0  epsilon: 1.0    steps: 376  evaluation reward: 219.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2097   score: 180.0  epsilon: 1.0    steps: 800  evaluation reward: 218.35\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4345: Policy loss: -0.001771. Value loss: 0.319339. Entropy: 1.082403.\n",
      "Iteration 4346: Policy loss: -0.004887. Value loss: 0.167570. Entropy: 1.081522.\n",
      "Iteration 4347: Policy loss: -0.011997. Value loss: 0.118184. Entropy: 1.080193.\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4348: Policy loss: 0.003691. Value loss: 0.293848. Entropy: 1.058756.\n",
      "Iteration 4349: Policy loss: -0.000736. Value loss: 0.168564. Entropy: 1.054152.\n",
      "Iteration 4350: Policy loss: -0.003523. Value loss: 0.118875. Entropy: 1.054814.\n",
      "episode: 2098   score: 185.0  epsilon: 1.0    steps: 952  evaluation reward: 219.4\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4351: Policy loss: 0.003274. Value loss: 0.578425. Entropy: 1.168058.\n",
      "Iteration 4352: Policy loss: -0.004703. Value loss: 0.237596. Entropy: 1.181554.\n",
      "Iteration 4353: Policy loss: -0.013821. Value loss: 0.156579. Entropy: 1.162697.\n",
      "episode: 2099   score: 360.0  epsilon: 1.0    steps: 720  evaluation reward: 221.2\n",
      "episode: 2100   score: 110.0  epsilon: 1.0    steps: 784  evaluation reward: 221.1\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4354: Policy loss: 0.001436. Value loss: 0.313420. Entropy: 1.141465.\n",
      "Iteration 4355: Policy loss: -0.004529. Value loss: 0.163560. Entropy: 1.167623.\n",
      "Iteration 4356: Policy loss: -0.005799. Value loss: 0.103932. Entropy: 1.155223.\n",
      "now time :  2019-02-28 11:47:07.257509\n",
      "episode: 2101   score: 180.0  epsilon: 1.0    steps: 184  evaluation reward: 220.8\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4357: Policy loss: 0.000889. Value loss: 0.309777. Entropy: 1.136496.\n",
      "Iteration 4358: Policy loss: -0.008689. Value loss: 0.176722. Entropy: 1.124622.\n",
      "Iteration 4359: Policy loss: -0.012702. Value loss: 0.135558. Entropy: 1.122595.\n",
      "episode: 2102   score: 125.0  epsilon: 1.0    steps: 688  evaluation reward: 220.25\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4360: Policy loss: -0.001074. Value loss: 0.825846. Entropy: 1.165129.\n",
      "Iteration 4361: Policy loss: 0.003267. Value loss: 0.544464. Entropy: 1.177807.\n",
      "Iteration 4362: Policy loss: -0.006725. Value loss: 0.467856. Entropy: 1.179547.\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4363: Policy loss: -0.001920. Value loss: 0.502756. Entropy: 1.109544.\n",
      "Iteration 4364: Policy loss: -0.002689. Value loss: 0.199440. Entropy: 1.111851.\n",
      "Iteration 4365: Policy loss: -0.012669. Value loss: 0.132588. Entropy: 1.119215.\n",
      "episode: 2103   score: 465.0  epsilon: 1.0    steps: 168  evaluation reward: 224.0\n",
      "episode: 2104   score: 105.0  epsilon: 1.0    steps: 240  evaluation reward: 222.0\n",
      "episode: 2105   score: 105.0  epsilon: 1.0    steps: 816  evaluation reward: 222.25\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4366: Policy loss: 0.003752. Value loss: 0.452508. Entropy: 1.258334.\n",
      "Iteration 4367: Policy loss: 0.002059. Value loss: 0.223803. Entropy: 1.269580.\n",
      "Iteration 4368: Policy loss: -0.008298. Value loss: 0.157320. Entropy: 1.260076.\n",
      "episode: 2106   score: 210.0  epsilon: 1.0    steps: 72  evaluation reward: 220.4\n",
      "episode: 2107   score: 250.0  epsilon: 1.0    steps: 944  evaluation reward: 221.35\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4369: Policy loss: 0.003330. Value loss: 0.366645. Entropy: 1.067020.\n",
      "Iteration 4370: Policy loss: -0.007073. Value loss: 0.177365. Entropy: 1.099256.\n",
      "Iteration 4371: Policy loss: -0.009916. Value loss: 0.134182. Entropy: 1.089032.\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4372: Policy loss: 0.002901. Value loss: 0.286899. Entropy: 1.137265.\n",
      "Iteration 4373: Policy loss: -0.004254. Value loss: 0.131261. Entropy: 1.126734.\n",
      "Iteration 4374: Policy loss: -0.012379. Value loss: 0.095489. Entropy: 1.128230.\n",
      "episode: 2108   score: 410.0  epsilon: 1.0    steps: 680  evaluation reward: 222.65\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4375: Policy loss: 0.003246. Value loss: 0.253186. Entropy: 1.180812.\n",
      "Iteration 4376: Policy loss: -0.005352. Value loss: 0.099423. Entropy: 1.163512.\n",
      "Iteration 4377: Policy loss: -0.007873. Value loss: 0.069198. Entropy: 1.176620.\n",
      "episode: 2109   score: 105.0  epsilon: 1.0    steps: 544  evaluation reward: 221.6\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4378: Policy loss: 0.004589. Value loss: 0.763630. Entropy: 1.153405.\n",
      "Iteration 4379: Policy loss: -0.001140. Value loss: 0.376611. Entropy: 1.154940.\n",
      "Iteration 4380: Policy loss: -0.008383. Value loss: 0.227293. Entropy: 1.154501.\n",
      "episode: 2110   score: 320.0  epsilon: 1.0    steps: 760  evaluation reward: 222.95\n",
      "episode: 2111   score: 180.0  epsilon: 1.0    steps: 832  evaluation reward: 222.8\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4381: Policy loss: -0.000555. Value loss: 0.381218. Entropy: 1.202201.\n",
      "Iteration 4382: Policy loss: -0.009606. Value loss: 0.173338. Entropy: 1.208658.\n",
      "Iteration 4383: Policy loss: -0.011543. Value loss: 0.125660. Entropy: 1.216130.\n",
      "episode: 2112   score: 180.0  epsilon: 1.0    steps: 32  evaluation reward: 223.7\n",
      "episode: 2113   score: 170.0  epsilon: 1.0    steps: 48  evaluation reward: 222.5\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4384: Policy loss: 0.007989. Value loss: 0.689709. Entropy: 1.064007.\n",
      "Iteration 4385: Policy loss: 0.002123. Value loss: 0.334380. Entropy: 1.043627.\n",
      "Iteration 4386: Policy loss: -0.001884. Value loss: 0.225579. Entropy: 1.088750.\n",
      "episode: 2114   score: 550.0  epsilon: 1.0    steps: 624  evaluation reward: 226.2\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4387: Policy loss: 0.003338. Value loss: 0.706263. Entropy: 1.224558.\n",
      "Iteration 4388: Policy loss: -0.001835. Value loss: 0.313573. Entropy: 1.224246.\n",
      "Iteration 4389: Policy loss: -0.009080. Value loss: 0.195807. Entropy: 1.232049.\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4390: Policy loss: 0.004972. Value loss: 0.413922. Entropy: 1.055642.\n",
      "Iteration 4391: Policy loss: -0.000784. Value loss: 0.187369. Entropy: 1.053687.\n",
      "Iteration 4392: Policy loss: -0.006621. Value loss: 0.130953. Entropy: 1.055308.\n",
      "episode: 2115   score: 345.0  epsilon: 1.0    steps: 680  evaluation reward: 227.3\n",
      "episode: 2116   score: 135.0  epsilon: 1.0    steps: 840  evaluation reward: 227.3\n",
      "episode: 2117   score: 155.0  epsilon: 1.0    steps: 976  evaluation reward: 227.8\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4393: Policy loss: 0.002939. Value loss: 0.489722. Entropy: 1.242162.\n",
      "Iteration 4394: Policy loss: -0.000723. Value loss: 0.240213. Entropy: 1.237663.\n",
      "Iteration 4395: Policy loss: -0.008373. Value loss: 0.157136. Entropy: 1.243844.\n",
      "episode: 2118   score: 180.0  epsilon: 1.0    steps: 784  evaluation reward: 226.4\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4396: Policy loss: 0.004160. Value loss: 0.338952. Entropy: 1.089664.\n",
      "Iteration 4397: Policy loss: -0.000082. Value loss: 0.178530. Entropy: 1.090071.\n",
      "Iteration 4398: Policy loss: -0.002696. Value loss: 0.149184. Entropy: 1.081010.\n",
      "episode: 2119   score: 560.0  epsilon: 1.0    steps: 272  evaluation reward: 230.75\n",
      "episode: 2120   score: 235.0  epsilon: 1.0    steps: 592  evaluation reward: 232.0\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4399: Policy loss: 0.000583. Value loss: 0.334339. Entropy: 1.146605.\n",
      "Iteration 4400: Policy loss: -0.003582. Value loss: 0.128582. Entropy: 1.178570.\n",
      "Iteration 4401: Policy loss: -0.012269. Value loss: 0.110194. Entropy: 1.156728.\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4402: Policy loss: 0.003521. Value loss: 0.365078. Entropy: 1.097701.\n",
      "Iteration 4403: Policy loss: 0.001180. Value loss: 0.146650. Entropy: 1.110070.\n",
      "Iteration 4404: Policy loss: -0.006683. Value loss: 0.100866. Entropy: 1.099159.\n",
      "episode: 2121   score: 350.0  epsilon: 1.0    steps: 192  evaluation reward: 233.15\n",
      "episode: 2122   score: 145.0  epsilon: 1.0    steps: 592  evaluation reward: 233.05\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4405: Policy loss: -0.001751. Value loss: 0.188666. Entropy: 1.188222.\n",
      "Iteration 4406: Policy loss: -0.006279. Value loss: 0.139318. Entropy: 1.199657.\n",
      "Iteration 4407: Policy loss: -0.007654. Value loss: 0.098301. Entropy: 1.180746.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2123   score: 180.0  epsilon: 1.0    steps: 296  evaluation reward: 232.75\n",
      "episode: 2124   score: 125.0  epsilon: 1.0    steps: 744  evaluation reward: 232.95\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4408: Policy loss: 0.004326. Value loss: 0.156788. Entropy: 1.083164.\n",
      "Iteration 4409: Policy loss: -0.003209. Value loss: 0.097910. Entropy: 1.105498.\n",
      "Iteration 4410: Policy loss: -0.005288. Value loss: 0.084441. Entropy: 1.089868.\n",
      "episode: 2125   score: 225.0  epsilon: 1.0    steps: 544  evaluation reward: 233.55\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4411: Policy loss: 0.004305. Value loss: 0.316119. Entropy: 1.080277.\n",
      "Iteration 4412: Policy loss: -0.004743. Value loss: 0.133884. Entropy: 1.079658.\n",
      "Iteration 4413: Policy loss: -0.010063. Value loss: 0.085613. Entropy: 1.083554.\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4414: Policy loss: -0.002098. Value loss: 0.234125. Entropy: 1.092173.\n",
      "Iteration 4415: Policy loss: -0.008443. Value loss: 0.141589. Entropy: 1.083767.\n",
      "Iteration 4416: Policy loss: -0.015890. Value loss: 0.104158. Entropy: 1.083651.\n",
      "episode: 2126   score: 185.0  epsilon: 1.0    steps: 200  evaluation reward: 231.0\n",
      "episode: 2127   score: 225.0  epsilon: 1.0    steps: 440  evaluation reward: 232.35\n",
      "episode: 2128   score: 260.0  epsilon: 1.0    steps: 920  evaluation reward: 232.0\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4417: Policy loss: -0.002297. Value loss: 0.250119. Entropy: 1.211373.\n",
      "Iteration 4418: Policy loss: 0.003158. Value loss: 0.110438. Entropy: 1.209482.\n",
      "Iteration 4419: Policy loss: -0.009356. Value loss: 0.072704. Entropy: 1.209365.\n",
      "episode: 2129   score: 230.0  epsilon: 1.0    steps: 512  evaluation reward: 232.95\n",
      "episode: 2130   score: 210.0  epsilon: 1.0    steps: 544  evaluation reward: 233.55\n",
      "episode: 2131   score: 155.0  epsilon: 1.0    steps: 648  evaluation reward: 233.6\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4420: Policy loss: 0.001571. Value loss: 0.166467. Entropy: 1.088239.\n",
      "Iteration 4421: Policy loss: -0.003167. Value loss: 0.084996. Entropy: 1.119420.\n",
      "Iteration 4422: Policy loss: -0.006668. Value loss: 0.066821. Entropy: 1.091812.\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4423: Policy loss: -0.001214. Value loss: 0.100775. Entropy: 0.983320.\n",
      "Iteration 4424: Policy loss: -0.006649. Value loss: 0.069466. Entropy: 0.991604.\n",
      "Iteration 4425: Policy loss: -0.008502. Value loss: 0.057195. Entropy: 0.974820.\n",
      "episode: 2132   score: 210.0  epsilon: 1.0    steps: 72  evaluation reward: 234.5\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4426: Policy loss: 0.002603. Value loss: 0.224200. Entropy: 1.214609.\n",
      "Iteration 4427: Policy loss: -0.001350. Value loss: 0.135584. Entropy: 1.210913.\n",
      "Iteration 4428: Policy loss: -0.007421. Value loss: 0.089617. Entropy: 1.200328.\n",
      "episode: 2133   score: 105.0  epsilon: 1.0    steps: 160  evaluation reward: 232.9\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4429: Policy loss: 0.004106. Value loss: 0.178890. Entropy: 1.142865.\n",
      "Iteration 4430: Policy loss: -0.005621. Value loss: 0.098072. Entropy: 1.132600.\n",
      "Iteration 4431: Policy loss: -0.010355. Value loss: 0.075924. Entropy: 1.131800.\n",
      "episode: 2134   score: 125.0  epsilon: 1.0    steps: 176  evaluation reward: 232.65\n",
      "episode: 2135   score: 210.0  epsilon: 1.0    steps: 712  evaluation reward: 233.65\n",
      "episode: 2136   score: 155.0  epsilon: 1.0    steps: 736  evaluation reward: 233.1\n",
      "episode: 2137   score: 180.0  epsilon: 1.0    steps: 952  evaluation reward: 232.6\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4432: Policy loss: 0.001116. Value loss: 0.192138. Entropy: 1.207006.\n",
      "Iteration 4433: Policy loss: -0.000320. Value loss: 0.094315. Entropy: 1.199421.\n",
      "Iteration 4434: Policy loss: -0.005952. Value loss: 0.066969. Entropy: 1.212740.\n",
      "episode: 2138   score: 210.0  epsilon: 1.0    steps: 1016  evaluation reward: 232.6\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4435: Policy loss: -0.002764. Value loss: 0.267323. Entropy: 1.016373.\n",
      "Iteration 4436: Policy loss: -0.007908. Value loss: 0.165858. Entropy: 1.021079.\n",
      "Iteration 4437: Policy loss: -0.009154. Value loss: 0.111855. Entropy: 1.016884.\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4438: Policy loss: -0.001914. Value loss: 0.334961. Entropy: 1.188185.\n",
      "Iteration 4439: Policy loss: -0.002956. Value loss: 0.150993. Entropy: 1.181843.\n",
      "Iteration 4440: Policy loss: -0.007788. Value loss: 0.096222. Entropy: 1.184016.\n",
      "episode: 2139   score: 155.0  epsilon: 1.0    steps: 440  evaluation reward: 231.9\n",
      "episode: 2140   score: 100.0  epsilon: 1.0    steps: 696  evaluation reward: 231.35\n",
      "episode: 2141   score: 210.0  epsilon: 1.0    steps: 920  evaluation reward: 232.05\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4441: Policy loss: 0.002622. Value loss: 0.356220. Entropy: 1.217937.\n",
      "Iteration 4442: Policy loss: -0.007993. Value loss: 0.150964. Entropy: 1.224572.\n",
      "Iteration 4443: Policy loss: -0.008897. Value loss: 0.106777. Entropy: 1.222936.\n",
      "episode: 2142   score: 285.0  epsilon: 1.0    steps: 160  evaluation reward: 233.35\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4444: Policy loss: 0.004966. Value loss: 0.318028. Entropy: 1.049583.\n",
      "Iteration 4445: Policy loss: -0.000310. Value loss: 0.184762. Entropy: 1.081600.\n",
      "Iteration 4446: Policy loss: -0.005236. Value loss: 0.126541. Entropy: 1.068706.\n",
      "episode: 2143   score: 155.0  epsilon: 1.0    steps: 704  evaluation reward: 232.8\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4447: Policy loss: -0.002878. Value loss: 0.304666. Entropy: 1.143762.\n",
      "Iteration 4448: Policy loss: -0.003611. Value loss: 0.157643. Entropy: 1.126167.\n",
      "Iteration 4449: Policy loss: -0.004813. Value loss: 0.105977. Entropy: 1.143434.\n",
      "episode: 2144   score: 440.0  epsilon: 1.0    steps: 808  evaluation reward: 235.7\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4450: Policy loss: -0.002027. Value loss: 0.724118. Entropy: 1.049145.\n",
      "Iteration 4451: Policy loss: -0.004268. Value loss: 0.267583. Entropy: 1.064176.\n",
      "Iteration 4452: Policy loss: -0.009566. Value loss: 0.178646. Entropy: 1.054880.\n",
      "episode: 2145   score: 210.0  epsilon: 1.0    steps: 240  evaluation reward: 232.45\n",
      "episode: 2146   score: 180.0  epsilon: 1.0    steps: 928  evaluation reward: 233.45\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4453: Policy loss: -0.001977. Value loss: 0.325709. Entropy: 1.211825.\n",
      "Iteration 4454: Policy loss: -0.005785. Value loss: 0.164601. Entropy: 1.216451.\n",
      "Iteration 4455: Policy loss: -0.009104. Value loss: 0.124490. Entropy: 1.215717.\n",
      "episode: 2147   score: 155.0  epsilon: 1.0    steps: 840  evaluation reward: 231.0\n",
      "episode: 2148   score: 480.0  epsilon: 1.0    steps: 920  evaluation reward: 230.8\n",
      "episode: 2149   score: 120.0  epsilon: 1.0    steps: 984  evaluation reward: 230.2\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4456: Policy loss: 0.002971. Value loss: 0.481688. Entropy: 1.147909.\n",
      "Iteration 4457: Policy loss: 0.000552. Value loss: 0.204283. Entropy: 1.146528.\n",
      "Iteration 4458: Policy loss: -0.004418. Value loss: 0.117621. Entropy: 1.158908.\n",
      "episode: 2150   score: 180.0  epsilon: 1.0    steps: 768  evaluation reward: 229.4\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4459: Policy loss: 0.002290. Value loss: 0.197201. Entropy: 1.090468.\n",
      "Iteration 4460: Policy loss: -0.003569. Value loss: 0.095461. Entropy: 1.069714.\n",
      "Iteration 4461: Policy loss: -0.009935. Value loss: 0.077667. Entropy: 1.112517.\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4462: Policy loss: 0.000274. Value loss: 0.368533. Entropy: 1.131468.\n",
      "Iteration 4463: Policy loss: -0.003979. Value loss: 0.205792. Entropy: 1.131867.\n",
      "Iteration 4464: Policy loss: -0.007608. Value loss: 0.147770. Entropy: 1.123015.\n",
      "now time :  2019-02-28 11:48:25.584134\n",
      "episode: 2151   score: 210.0  epsilon: 1.0    steps: 160  evaluation reward: 229.55\n",
      "episode: 2152   score: 225.0  epsilon: 1.0    steps: 776  evaluation reward: 230.55\n",
      "episode: 2153   score: 105.0  epsilon: 1.0    steps: 856  evaluation reward: 230.35\n",
      "episode: 2154   score: 210.0  epsilon: 1.0    steps: 896  evaluation reward: 230.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2155   score: 245.0  epsilon: 1.0    steps: 992  evaluation reward: 231.95\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4465: Policy loss: 0.000612. Value loss: 0.186224. Entropy: 1.110901.\n",
      "Iteration 4466: Policy loss: -0.008043. Value loss: 0.103401. Entropy: 1.109765.\n",
      "Iteration 4467: Policy loss: -0.011621. Value loss: 0.069353. Entropy: 1.090835.\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4468: Policy loss: -0.000607. Value loss: 0.164695. Entropy: 1.025967.\n",
      "Iteration 4469: Policy loss: -0.000473. Value loss: 0.084917. Entropy: 1.038958.\n",
      "Iteration 4470: Policy loss: -0.008883. Value loss: 0.072015. Entropy: 1.031981.\n",
      "episode: 2156   score: 155.0  epsilon: 1.0    steps: 560  evaluation reward: 229.3\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4471: Policy loss: 0.004162. Value loss: 0.279813. Entropy: 1.129211.\n",
      "Iteration 4472: Policy loss: -0.000855. Value loss: 0.131872. Entropy: 1.142217.\n",
      "Iteration 4473: Policy loss: -0.009488. Value loss: 0.101597. Entropy: 1.152477.\n",
      "episode: 2157   score: 255.0  epsilon: 1.0    steps: 504  evaluation reward: 226.85\n",
      "episode: 2158   score: 65.0  epsilon: 1.0    steps: 880  evaluation reward: 225.95\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4474: Policy loss: 0.001694. Value loss: 0.195467. Entropy: 1.203252.\n",
      "Iteration 4475: Policy loss: -0.004863. Value loss: 0.102479. Entropy: 1.189679.\n",
      "Iteration 4476: Policy loss: -0.012332. Value loss: 0.069239. Entropy: 1.202453.\n",
      "episode: 2159   score: 180.0  epsilon: 1.0    steps: 1024  evaluation reward: 225.15\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4477: Policy loss: 0.004109. Value loss: 0.227748. Entropy: 1.159075.\n",
      "Iteration 4478: Policy loss: -0.013617. Value loss: 0.129053. Entropy: 1.148501.\n",
      "Iteration 4479: Policy loss: -0.010763. Value loss: 0.086131. Entropy: 1.135804.\n",
      "episode: 2160   score: 105.0  epsilon: 1.0    steps: 536  evaluation reward: 222.4\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4480: Policy loss: 0.002804. Value loss: 0.486561. Entropy: 1.163937.\n",
      "Iteration 4481: Policy loss: 0.003060. Value loss: 0.241758. Entropy: 1.152959.\n",
      "Iteration 4482: Policy loss: -0.000305. Value loss: 0.158128. Entropy: 1.159251.\n",
      "episode: 2161   score: 240.0  epsilon: 1.0    steps: 464  evaluation reward: 222.2\n",
      "episode: 2162   score: 290.0  epsilon: 1.0    steps: 616  evaluation reward: 223.4\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4483: Policy loss: -0.000715. Value loss: 0.207265. Entropy: 1.182754.\n",
      "Iteration 4484: Policy loss: -0.006181. Value loss: 0.081791. Entropy: 1.176115.\n",
      "Iteration 4485: Policy loss: -0.012902. Value loss: 0.060677. Entropy: 1.185278.\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4486: Policy loss: 0.006319. Value loss: 0.870263. Entropy: 1.169586.\n",
      "Iteration 4487: Policy loss: 0.005648. Value loss: 0.465263. Entropy: 1.190275.\n",
      "Iteration 4488: Policy loss: 0.000916. Value loss: 0.272659. Entropy: 1.171293.\n",
      "episode: 2163   score: 105.0  epsilon: 1.0    steps: 24  evaluation reward: 221.45\n",
      "episode: 2164   score: 225.0  epsilon: 1.0    steps: 832  evaluation reward: 219.4\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4489: Policy loss: 0.006578. Value loss: 0.435612. Entropy: 1.185236.\n",
      "Iteration 4490: Policy loss: 0.002259. Value loss: 0.162041. Entropy: 1.210007.\n",
      "Iteration 4491: Policy loss: -0.008036. Value loss: 0.094379. Entropy: 1.193840.\n",
      "episode: 2165   score: 120.0  epsilon: 1.0    steps: 64  evaluation reward: 218.8\n",
      "episode: 2166   score: 135.0  epsilon: 1.0    steps: 72  evaluation reward: 217.35\n",
      "episode: 2167   score: 530.0  epsilon: 1.0    steps: 192  evaluation reward: 221.35\n",
      "episode: 2168   score: 150.0  epsilon: 1.0    steps: 776  evaluation reward: 220.1\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4492: Policy loss: 0.004327. Value loss: 0.440722. Entropy: 1.161123.\n",
      "Iteration 4493: Policy loss: -0.000982. Value loss: 0.164279. Entropy: 1.161679.\n",
      "Iteration 4494: Policy loss: -0.007489. Value loss: 0.105286. Entropy: 1.163219.\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4495: Policy loss: 0.003584. Value loss: 0.233211. Entropy: 1.037114.\n",
      "Iteration 4496: Policy loss: 0.003572. Value loss: 0.115334. Entropy: 1.040694.\n",
      "Iteration 4497: Policy loss: -0.007024. Value loss: 0.087120. Entropy: 1.041791.\n",
      "episode: 2169   score: 210.0  epsilon: 1.0    steps: 712  evaluation reward: 221.1\n",
      "episode: 2170   score: 45.0  epsilon: 1.0    steps: 776  evaluation reward: 219.55\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4498: Policy loss: 0.010712. Value loss: 0.652547. Entropy: 1.278662.\n",
      "Iteration 4499: Policy loss: -0.001161. Value loss: 0.254175. Entropy: 1.267104.\n",
      "Iteration 4500: Policy loss: -0.004279. Value loss: 0.179886. Entropy: 1.271814.\n",
      "episode: 2171   score: 240.0  epsilon: 1.0    steps: 928  evaluation reward: 219.85\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4501: Policy loss: 0.003393. Value loss: 0.239817. Entropy: 1.101082.\n",
      "Iteration 4502: Policy loss: -0.004627. Value loss: 0.123383. Entropy: 1.097138.\n",
      "Iteration 4503: Policy loss: -0.012033. Value loss: 0.094334. Entropy: 1.120759.\n",
      "episode: 2172   score: 110.0  epsilon: 1.0    steps: 112  evaluation reward: 218.3\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4504: Policy loss: 0.001341. Value loss: 0.340452. Entropy: 1.249229.\n",
      "Iteration 4505: Policy loss: -0.007690. Value loss: 0.164755. Entropy: 1.250243.\n",
      "Iteration 4506: Policy loss: -0.011098. Value loss: 0.103586. Entropy: 1.261247.\n",
      "episode: 2173   score: 110.0  epsilon: 1.0    steps: 8  evaluation reward: 218.2\n",
      "episode: 2174   score: 210.0  epsilon: 1.0    steps: 376  evaluation reward: 217.1\n",
      "episode: 2175   score: 100.0  epsilon: 1.0    steps: 704  evaluation reward: 217.45\n",
      "episode: 2176   score: 215.0  epsilon: 1.0    steps: 1024  evaluation reward: 218.05\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4507: Policy loss: 0.001281. Value loss: 0.335130. Entropy: 1.201829.\n",
      "Iteration 4508: Policy loss: -0.000098. Value loss: 0.181721. Entropy: 1.188501.\n",
      "Iteration 4509: Policy loss: -0.007721. Value loss: 0.115458. Entropy: 1.195384.\n",
      "episode: 2177   score: 230.0  epsilon: 1.0    steps: 992  evaluation reward: 218.75\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4510: Policy loss: -0.002215. Value loss: 0.208540. Entropy: 1.037913.\n",
      "Iteration 4511: Policy loss: -0.002412. Value loss: 0.121083. Entropy: 1.022901.\n",
      "Iteration 4512: Policy loss: -0.009617. Value loss: 0.087004. Entropy: 1.028448.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4513: Policy loss: -0.003719. Value loss: 0.215341. Entropy: 1.201181.\n",
      "Iteration 4514: Policy loss: -0.004910. Value loss: 0.062879. Entropy: 1.199649.\n",
      "Iteration 4515: Policy loss: -0.012444. Value loss: 0.050755. Entropy: 1.198881.\n",
      "episode: 2178   score: 110.0  epsilon: 1.0    steps: 104  evaluation reward: 216.7\n",
      "episode: 2179   score: 120.0  epsilon: 1.0    steps: 672  evaluation reward: 214.45\n",
      "episode: 2180   score: 105.0  epsilon: 1.0    steps: 792  evaluation reward: 213.25\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4516: Policy loss: 0.003038. Value loss: 0.218676. Entropy: 1.184370.\n",
      "Iteration 4517: Policy loss: -0.004938. Value loss: 0.124712. Entropy: 1.189616.\n",
      "Iteration 4518: Policy loss: -0.007984. Value loss: 0.095692. Entropy: 1.187434.\n",
      "episode: 2181   score: 210.0  epsilon: 1.0    steps: 64  evaluation reward: 212.5\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4519: Policy loss: -0.001596. Value loss: 0.271700. Entropy: 1.053910.\n",
      "Iteration 4520: Policy loss: -0.011022. Value loss: 0.142153. Entropy: 1.063478.\n",
      "Iteration 4521: Policy loss: -0.015013. Value loss: 0.097567. Entropy: 1.033542.\n",
      "episode: 2182   score: 210.0  epsilon: 1.0    steps: 584  evaluation reward: 213.25\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4522: Policy loss: -0.000095. Value loss: 0.168375. Entropy: 1.160141.\n",
      "Iteration 4523: Policy loss: -0.003380. Value loss: 0.083148. Entropy: 1.161868.\n",
      "Iteration 4524: Policy loss: -0.009766. Value loss: 0.052604. Entropy: 1.158717.\n",
      "episode: 2183   score: 90.0  epsilon: 1.0    steps: 888  evaluation reward: 212.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4525: Policy loss: -0.002933. Value loss: 0.147156. Entropy: 1.183154.\n",
      "Iteration 4526: Policy loss: -0.005039. Value loss: 0.091292. Entropy: 1.161746.\n",
      "Iteration 4527: Policy loss: -0.009570. Value loss: 0.058340. Entropy: 1.181210.\n",
      "episode: 2184   score: 290.0  epsilon: 1.0    steps: 176  evaluation reward: 213.55\n",
      "episode: 2185   score: 110.0  epsilon: 1.0    steps: 200  evaluation reward: 212.0\n",
      "episode: 2186   score: 210.0  epsilon: 1.0    steps: 352  evaluation reward: 211.35\n",
      "episode: 2187   score: 215.0  epsilon: 1.0    steps: 352  evaluation reward: 211.85\n",
      "episode: 2188   score: 180.0  epsilon: 1.0    steps: 904  evaluation reward: 211.35\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4528: Policy loss: -0.000894. Value loss: 0.127638. Entropy: 1.117127.\n",
      "Iteration 4529: Policy loss: -0.003683. Value loss: 0.085545. Entropy: 1.132381.\n",
      "Iteration 4530: Policy loss: -0.002870. Value loss: 0.065947. Entropy: 1.117768.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4531: Policy loss: 0.002216. Value loss: 0.129772. Entropy: 0.941133.\n",
      "Iteration 4532: Policy loss: -0.000958. Value loss: 0.076126. Entropy: 0.910882.\n",
      "Iteration 4533: Policy loss: -0.008634. Value loss: 0.062191. Entropy: 0.939248.\n",
      "episode: 2189   score: 210.0  epsilon: 1.0    steps: 144  evaluation reward: 212.25\n",
      "episode: 2190   score: 180.0  epsilon: 1.0    steps: 808  evaluation reward: 209.75\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4534: Policy loss: -0.000367. Value loss: 0.189506. Entropy: 1.215931.\n",
      "Iteration 4535: Policy loss: -0.003773. Value loss: 0.089724. Entropy: 1.198447.\n",
      "Iteration 4536: Policy loss: -0.007025. Value loss: 0.062888. Entropy: 1.219985.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4537: Policy loss: -0.002130. Value loss: 0.125187. Entropy: 0.974765.\n",
      "Iteration 4538: Policy loss: -0.002992. Value loss: 0.067433. Entropy: 0.973052.\n",
      "Iteration 4539: Policy loss: -0.012523. Value loss: 0.059748. Entropy: 0.970161.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4540: Policy loss: 0.003980. Value loss: 1.016395. Entropy: 1.314950.\n",
      "Iteration 4541: Policy loss: -0.003608. Value loss: 0.531322. Entropy: 1.308837.\n",
      "Iteration 4542: Policy loss: 0.003571. Value loss: 0.220752. Entropy: 1.313125.\n",
      "episode: 2191   score: 155.0  epsilon: 1.0    steps: 496  evaluation reward: 208.4\n",
      "episode: 2192   score: 210.0  epsilon: 1.0    steps: 584  evaluation reward: 204.6\n",
      "episode: 2193   score: 180.0  epsilon: 1.0    steps: 1016  evaluation reward: 203.45\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4543: Policy loss: 0.002950. Value loss: 0.277495. Entropy: 1.230517.\n",
      "Iteration 4544: Policy loss: -0.003832. Value loss: 0.151138. Entropy: 1.223024.\n",
      "Iteration 4545: Policy loss: -0.010456. Value loss: 0.108949. Entropy: 1.229542.\n",
      "episode: 2194   score: 445.0  epsilon: 1.0    steps: 368  evaluation reward: 205.0\n",
      "episode: 2195   score: 215.0  epsilon: 1.0    steps: 584  evaluation reward: 205.0\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4546: Policy loss: 0.002240. Value loss: 0.322618. Entropy: 1.100572.\n",
      "Iteration 4547: Policy loss: -0.000721. Value loss: 0.187295. Entropy: 1.104884.\n",
      "Iteration 4548: Policy loss: -0.005430. Value loss: 0.130865. Entropy: 1.108153.\n",
      "episode: 2196   score: 335.0  epsilon: 1.0    steps: 504  evaluation reward: 207.55\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4549: Policy loss: -0.001020. Value loss: 0.231162. Entropy: 1.120126.\n",
      "Iteration 4550: Policy loss: -0.001614. Value loss: 0.133877. Entropy: 1.116243.\n",
      "Iteration 4551: Policy loss: -0.011711. Value loss: 0.100304. Entropy: 1.107080.\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4552: Policy loss: -0.002817. Value loss: 0.192208. Entropy: 1.062633.\n",
      "Iteration 4553: Policy loss: -0.008723. Value loss: 0.065186. Entropy: 1.042109.\n",
      "Iteration 4554: Policy loss: -0.013176. Value loss: 0.060980. Entropy: 1.051417.\n",
      "episode: 2197   score: 180.0  epsilon: 1.0    steps: 544  evaluation reward: 207.55\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4555: Policy loss: 0.002045. Value loss: 0.197404. Entropy: 1.236698.\n",
      "Iteration 4556: Policy loss: -0.003303. Value loss: 0.080523. Entropy: 1.246448.\n",
      "Iteration 4557: Policy loss: -0.009292. Value loss: 0.072578. Entropy: 1.241498.\n",
      "episode: 2198   score: 210.0  epsilon: 1.0    steps: 224  evaluation reward: 207.8\n",
      "episode: 2199   score: 365.0  epsilon: 1.0    steps: 832  evaluation reward: 207.85\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4558: Policy loss: -0.000767. Value loss: 0.164437. Entropy: 1.143064.\n",
      "Iteration 4559: Policy loss: -0.006098. Value loss: 0.099527. Entropy: 1.132966.\n",
      "Iteration 4560: Policy loss: -0.011492. Value loss: 0.078077. Entropy: 1.136049.\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4561: Policy loss: 0.007005. Value loss: 0.275592. Entropy: 1.050385.\n",
      "Iteration 4562: Policy loss: -0.001858. Value loss: 0.115829. Entropy: 1.057257.\n",
      "Iteration 4563: Policy loss: -0.005645. Value loss: 0.066900. Entropy: 1.027884.\n",
      "episode: 2200   score: 155.0  epsilon: 1.0    steps: 48  evaluation reward: 208.3\n",
      "now time :  2019-02-28 11:49:37.694778\n",
      "episode: 2201   score: 425.0  epsilon: 1.0    steps: 448  evaluation reward: 210.75\n",
      "episode: 2202   score: 295.0  epsilon: 1.0    steps: 720  evaluation reward: 212.45\n",
      "episode: 2203   score: 210.0  epsilon: 1.0    steps: 944  evaluation reward: 209.9\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4564: Policy loss: 0.003702. Value loss: 0.316038. Entropy: 1.160600.\n",
      "Iteration 4565: Policy loss: -0.005083. Value loss: 0.150211. Entropy: 1.164046.\n",
      "Iteration 4566: Policy loss: -0.014294. Value loss: 0.099491. Entropy: 1.167025.\n",
      "episode: 2204   score: 210.0  epsilon: 1.0    steps: 528  evaluation reward: 210.95\n",
      "episode: 2205   score: 120.0  epsilon: 1.0    steps: 920  evaluation reward: 211.1\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4567: Policy loss: -0.002402. Value loss: 0.184357. Entropy: 1.014354.\n",
      "Iteration 4568: Policy loss: -0.006192. Value loss: 0.108753. Entropy: 1.042071.\n",
      "Iteration 4569: Policy loss: -0.013104. Value loss: 0.066987. Entropy: 1.001219.\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4570: Policy loss: 0.000019. Value loss: 0.188675. Entropy: 1.033734.\n",
      "Iteration 4571: Policy loss: 0.000906. Value loss: 0.093053. Entropy: 1.042424.\n",
      "Iteration 4572: Policy loss: -0.007987. Value loss: 0.079390. Entropy: 1.042028.\n",
      "episode: 2206   score: 80.0  epsilon: 1.0    steps: 376  evaluation reward: 209.8\n",
      "episode: 2207   score: 125.0  epsilon: 1.0    steps: 760  evaluation reward: 208.55\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4573: Policy loss: 0.003390. Value loss: 0.318379. Entropy: 1.248529.\n",
      "Iteration 4574: Policy loss: 0.001425. Value loss: 0.138391. Entropy: 1.245859.\n",
      "Iteration 4575: Policy loss: -0.008275. Value loss: 0.086730. Entropy: 1.245388.\n",
      "episode: 2208   score: 420.0  epsilon: 1.0    steps: 88  evaluation reward: 208.65\n",
      "episode: 2209   score: 135.0  epsilon: 1.0    steps: 776  evaluation reward: 208.95\n",
      "episode: 2210   score: 120.0  epsilon: 1.0    steps: 904  evaluation reward: 206.95\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4576: Policy loss: -0.000256. Value loss: 0.223125. Entropy: 0.946373.\n",
      "Iteration 4577: Policy loss: -0.001356. Value loss: 0.137226. Entropy: 0.971494.\n",
      "Iteration 4578: Policy loss: -0.005040. Value loss: 0.095102. Entropy: 0.973369.\n",
      "episode: 2211   score: 260.0  epsilon: 1.0    steps: 392  evaluation reward: 207.75\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4579: Policy loss: -0.004402. Value loss: 0.337511. Entropy: 1.065675.\n",
      "Iteration 4580: Policy loss: -0.007855. Value loss: 0.163372. Entropy: 1.069535.\n",
      "Iteration 4581: Policy loss: -0.007720. Value loss: 0.116974. Entropy: 1.062972.\n",
      "episode: 2212   score: 180.0  epsilon: 1.0    steps: 920  evaluation reward: 207.75\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4582: Policy loss: 0.000151. Value loss: 0.279942. Entropy: 1.084796.\n",
      "Iteration 4583: Policy loss: -0.004230. Value loss: 0.152601. Entropy: 1.061961.\n",
      "Iteration 4584: Policy loss: -0.009641. Value loss: 0.128982. Entropy: 1.078091.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2213   score: 120.0  epsilon: 1.0    steps: 384  evaluation reward: 207.25\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4585: Policy loss: -0.000411. Value loss: 0.136483. Entropy: 1.132328.\n",
      "Iteration 4586: Policy loss: -0.007915. Value loss: 0.073003. Entropy: 1.132553.\n",
      "Iteration 4587: Policy loss: -0.011044. Value loss: 0.055949. Entropy: 1.120632.\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4588: Policy loss: 0.000099. Value loss: 0.157968. Entropy: 1.086442.\n",
      "Iteration 4589: Policy loss: -0.004116. Value loss: 0.067870. Entropy: 1.069150.\n",
      "Iteration 4590: Policy loss: -0.011293. Value loss: 0.052887. Entropy: 1.081526.\n",
      "episode: 2214   score: 210.0  epsilon: 1.0    steps: 608  evaluation reward: 203.85\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4591: Policy loss: 0.002150. Value loss: 0.473272. Entropy: 1.245147.\n",
      "Iteration 4592: Policy loss: -0.003300. Value loss: 0.235633. Entropy: 1.249520.\n",
      "Iteration 4593: Policy loss: -0.009327. Value loss: 0.155416. Entropy: 1.258274.\n",
      "episode: 2215   score: 485.0  epsilon: 1.0    steps: 712  evaluation reward: 205.25\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4594: Policy loss: 0.003835. Value loss: 0.282040. Entropy: 1.195089.\n",
      "Iteration 4595: Policy loss: -0.003087. Value loss: 0.125516. Entropy: 1.188829.\n",
      "Iteration 4596: Policy loss: -0.008886. Value loss: 0.068785. Entropy: 1.189697.\n",
      "episode: 2216   score: 210.0  epsilon: 1.0    steps: 200  evaluation reward: 206.0\n",
      "episode: 2217   score: 225.0  epsilon: 1.0    steps: 400  evaluation reward: 206.7\n",
      "episode: 2218   score: 290.0  epsilon: 1.0    steps: 616  evaluation reward: 207.8\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4597: Policy loss: -0.002529. Value loss: 0.275404. Entropy: 1.111998.\n",
      "Iteration 4598: Policy loss: -0.002080. Value loss: 0.120046. Entropy: 1.124492.\n",
      "Iteration 4599: Policy loss: -0.008128. Value loss: 0.098048. Entropy: 1.119871.\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4600: Policy loss: 0.001440. Value loss: 0.294395. Entropy: 0.965855.\n",
      "Iteration 4601: Policy loss: -0.003453. Value loss: 0.136508. Entropy: 0.951526.\n",
      "Iteration 4602: Policy loss: -0.006993. Value loss: 0.088641. Entropy: 0.978204.\n",
      "episode: 2219   score: 215.0  epsilon: 1.0    steps: 720  evaluation reward: 204.35\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4603: Policy loss: -0.000016. Value loss: 0.383886. Entropy: 1.175002.\n",
      "Iteration 4604: Policy loss: -0.002315. Value loss: 0.144004. Entropy: 1.196276.\n",
      "Iteration 4605: Policy loss: -0.012525. Value loss: 0.095222. Entropy: 1.180473.\n",
      "episode: 2220   score: 105.0  epsilon: 1.0    steps: 416  evaluation reward: 203.05\n",
      "episode: 2221   score: 395.0  epsilon: 1.0    steps: 672  evaluation reward: 203.5\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4606: Policy loss: 0.001609. Value loss: 0.397273. Entropy: 1.209161.\n",
      "Iteration 4607: Policy loss: -0.005064. Value loss: 0.201706. Entropy: 1.201489.\n",
      "Iteration 4608: Policy loss: -0.009510. Value loss: 0.119635. Entropy: 1.193359.\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4609: Policy loss: 0.000082. Value loss: 0.214992. Entropy: 1.067998.\n",
      "Iteration 4610: Policy loss: -0.003916. Value loss: 0.110005. Entropy: 1.060365.\n",
      "Iteration 4611: Policy loss: -0.010143. Value loss: 0.074657. Entropy: 1.075593.\n",
      "episode: 2222   score: 210.0  epsilon: 1.0    steps: 112  evaluation reward: 204.15\n",
      "episode: 2223   score: 120.0  epsilon: 1.0    steps: 304  evaluation reward: 203.55\n",
      "episode: 2224   score: 235.0  epsilon: 1.0    steps: 904  evaluation reward: 204.65\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4612: Policy loss: 0.000812. Value loss: 0.804831. Entropy: 1.210082.\n",
      "Iteration 4613: Policy loss: 0.005462. Value loss: 0.397317. Entropy: 1.190853.\n",
      "Iteration 4614: Policy loss: -0.004784. Value loss: 0.265481. Entropy: 1.201749.\n",
      "episode: 2225   score: 590.0  epsilon: 1.0    steps: 536  evaluation reward: 208.3\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4615: Policy loss: 0.001181. Value loss: 0.416519. Entropy: 1.001479.\n",
      "Iteration 4616: Policy loss: -0.003255. Value loss: 0.243681. Entropy: 0.986548.\n",
      "Iteration 4617: Policy loss: -0.005792. Value loss: 0.165311. Entropy: 0.999627.\n",
      "episode: 2226   score: 295.0  epsilon: 1.0    steps: 664  evaluation reward: 209.4\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4618: Policy loss: 0.005328. Value loss: 0.282770. Entropy: 1.098838.\n",
      "Iteration 4619: Policy loss: -0.001405. Value loss: 0.159152. Entropy: 1.096301.\n",
      "Iteration 4620: Policy loss: -0.006191. Value loss: 0.104064. Entropy: 1.088570.\n",
      "episode: 2227   score: 210.0  epsilon: 1.0    steps: 96  evaluation reward: 209.25\n",
      "episode: 2228   score: 135.0  epsilon: 1.0    steps: 952  evaluation reward: 208.0\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4621: Policy loss: 0.002217. Value loss: 0.318551. Entropy: 1.070871.\n",
      "Iteration 4622: Policy loss: -0.003224. Value loss: 0.142162. Entropy: 1.077884.\n",
      "Iteration 4623: Policy loss: -0.004178. Value loss: 0.122826. Entropy: 1.085496.\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4624: Policy loss: 0.001654. Value loss: 0.361477. Entropy: 1.057437.\n",
      "Iteration 4625: Policy loss: -0.003892. Value loss: 0.183869. Entropy: 1.069284.\n",
      "Iteration 4626: Policy loss: -0.012014. Value loss: 0.136474. Entropy: 1.076762.\n",
      "episode: 2229   score: 155.0  epsilon: 1.0    steps: 368  evaluation reward: 207.25\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4627: Policy loss: 0.001146. Value loss: 0.242456. Entropy: 1.177345.\n",
      "Iteration 4628: Policy loss: -0.007576. Value loss: 0.106747. Entropy: 1.172817.\n",
      "Iteration 4629: Policy loss: -0.004758. Value loss: 0.078327. Entropy: 1.171458.\n",
      "episode: 2230   score: 135.0  epsilon: 1.0    steps: 80  evaluation reward: 206.5\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4630: Policy loss: 0.000418. Value loss: 0.358727. Entropy: 1.111812.\n",
      "Iteration 4631: Policy loss: -0.004327. Value loss: 0.168209. Entropy: 1.140382.\n",
      "Iteration 4632: Policy loss: -0.011355. Value loss: 0.107166. Entropy: 1.126895.\n",
      "episode: 2231   score: 265.0  epsilon: 1.0    steps: 424  evaluation reward: 207.6\n",
      "episode: 2232   score: 315.0  epsilon: 1.0    steps: 672  evaluation reward: 208.65\n",
      "episode: 2233   score: 210.0  epsilon: 1.0    steps: 928  evaluation reward: 209.7\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4633: Policy loss: 0.002414. Value loss: 0.257749. Entropy: 1.121152.\n",
      "Iteration 4634: Policy loss: -0.004062. Value loss: 0.119300. Entropy: 1.127728.\n",
      "Iteration 4635: Policy loss: -0.009512. Value loss: 0.108455. Entropy: 1.125901.\n",
      "episode: 2234   score: 260.0  epsilon: 1.0    steps: 48  evaluation reward: 211.05\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4636: Policy loss: 0.002167. Value loss: 0.229911. Entropy: 0.902516.\n",
      "Iteration 4637: Policy loss: -0.002669. Value loss: 0.125182. Entropy: 0.914014.\n",
      "Iteration 4638: Policy loss: -0.006913. Value loss: 0.108485. Entropy: 0.910797.\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4639: Policy loss: -0.000019. Value loss: 0.293983. Entropy: 1.187102.\n",
      "Iteration 4640: Policy loss: -0.005445. Value loss: 0.191375. Entropy: 1.187304.\n",
      "Iteration 4641: Policy loss: -0.011018. Value loss: 0.132632. Entropy: 1.197829.\n",
      "episode: 2235   score: 490.0  epsilon: 1.0    steps: 760  evaluation reward: 213.85\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4642: Policy loss: 0.003626. Value loss: 1.028477. Entropy: 1.079374.\n",
      "Iteration 4643: Policy loss: -0.006389. Value loss: 0.415890. Entropy: 1.070961.\n",
      "Iteration 4644: Policy loss: -0.005847. Value loss: 0.269791. Entropy: 1.043010.\n",
      "episode: 2236   score: 105.0  epsilon: 1.0    steps: 8  evaluation reward: 213.35\n",
      "episode: 2237   score: 180.0  epsilon: 1.0    steps: 744  evaluation reward: 213.35\n",
      "episode: 2238   score: 425.0  epsilon: 1.0    steps: 1008  evaluation reward: 215.5\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4645: Policy loss: 0.000041. Value loss: 0.317487. Entropy: 1.110234.\n",
      "Iteration 4646: Policy loss: -0.001314. Value loss: 0.170603. Entropy: 1.110240.\n",
      "Iteration 4647: Policy loss: -0.010920. Value loss: 0.124991. Entropy: 1.122665.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4648: Policy loss: 0.004011. Value loss: 0.484751. Entropy: 1.051421.\n",
      "Iteration 4649: Policy loss: 0.000892. Value loss: 0.215547. Entropy: 1.060176.\n",
      "Iteration 4650: Policy loss: -0.002643. Value loss: 0.156665. Entropy: 1.058939.\n",
      "episode: 2239   score: 335.0  epsilon: 1.0    steps: 80  evaluation reward: 217.3\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4651: Policy loss: 0.002925. Value loss: 0.489848. Entropy: 1.144253.\n",
      "Iteration 4652: Policy loss: -0.000291. Value loss: 0.277133. Entropy: 1.131928.\n",
      "Iteration 4653: Policy loss: -0.005496. Value loss: 0.177928. Entropy: 1.134448.\n",
      "episode: 2240   score: 540.0  epsilon: 1.0    steps: 440  evaluation reward: 221.7\n",
      "episode: 2241   score: 470.0  epsilon: 1.0    steps: 816  evaluation reward: 224.3\n",
      "episode: 2242   score: 155.0  epsilon: 1.0    steps: 968  evaluation reward: 223.0\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4654: Policy loss: 0.001322. Value loss: 0.377771. Entropy: 1.021696.\n",
      "Iteration 4655: Policy loss: -0.004814. Value loss: 0.189060. Entropy: 1.046726.\n",
      "Iteration 4656: Policy loss: -0.007692. Value loss: 0.139322. Entropy: 1.031639.\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4657: Policy loss: -0.000486. Value loss: 0.503739. Entropy: 0.966096.\n",
      "Iteration 4658: Policy loss: 0.001270. Value loss: 0.192941. Entropy: 0.965107.\n",
      "Iteration 4659: Policy loss: -0.005090. Value loss: 0.139797. Entropy: 0.960091.\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4660: Policy loss: -0.001728. Value loss: 0.327104. Entropy: 1.068143.\n",
      "Iteration 4661: Policy loss: -0.006991. Value loss: 0.142915. Entropy: 1.075404.\n",
      "Iteration 4662: Policy loss: -0.006968. Value loss: 0.088517. Entropy: 1.082444.\n",
      "episode: 2243   score: 245.0  epsilon: 1.0    steps: 368  evaluation reward: 223.9\n",
      "episode: 2244   score: 155.0  epsilon: 1.0    steps: 512  evaluation reward: 221.05\n",
      "episode: 2245   score: 405.0  epsilon: 1.0    steps: 1000  evaluation reward: 223.0\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4663: Policy loss: 0.001655. Value loss: 0.378061. Entropy: 1.202682.\n",
      "Iteration 4664: Policy loss: -0.001355. Value loss: 0.195277. Entropy: 1.191374.\n",
      "Iteration 4665: Policy loss: -0.008269. Value loss: 0.171240. Entropy: 1.187259.\n",
      "episode: 2246   score: 260.0  epsilon: 1.0    steps: 408  evaluation reward: 223.8\n",
      "episode: 2247   score: 180.0  epsilon: 1.0    steps: 1008  evaluation reward: 224.05\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4666: Policy loss: 0.005237. Value loss: 0.255018. Entropy: 0.977524.\n",
      "Iteration 4667: Policy loss: -0.000892. Value loss: 0.139869. Entropy: 0.980391.\n",
      "Iteration 4668: Policy loss: -0.006242. Value loss: 0.093483. Entropy: 0.977004.\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4669: Policy loss: 0.000576. Value loss: 0.235384. Entropy: 1.076510.\n",
      "Iteration 4670: Policy loss: -0.002462. Value loss: 0.124017. Entropy: 1.046860.\n",
      "Iteration 4671: Policy loss: -0.007825. Value loss: 0.115671. Entropy: 1.081417.\n",
      "episode: 2248   score: 565.0  epsilon: 1.0    steps: 128  evaluation reward: 224.9\n",
      "episode: 2249   score: 240.0  epsilon: 1.0    steps: 928  evaluation reward: 226.1\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4672: Policy loss: -0.000425. Value loss: 0.192853. Entropy: 1.025329.\n",
      "Iteration 4673: Policy loss: -0.003961. Value loss: 0.107919. Entropy: 1.030906.\n",
      "Iteration 4674: Policy loss: -0.010330. Value loss: 0.090320. Entropy: 1.029753.\n",
      "episode: 2250   score: 100.0  epsilon: 1.0    steps: 48  evaluation reward: 225.3\n",
      "now time :  2019-02-28 11:50:58.261358\n",
      "episode: 2251   score: 245.0  epsilon: 1.0    steps: 664  evaluation reward: 225.65\n",
      "episode: 2252   score: 135.0  epsilon: 1.0    steps: 944  evaluation reward: 224.75\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4675: Policy loss: -0.001435. Value loss: 0.185839. Entropy: 1.010952.\n",
      "Iteration 4676: Policy loss: 0.000399. Value loss: 0.095412. Entropy: 1.031204.\n",
      "Iteration 4677: Policy loss: -0.007305. Value loss: 0.069258. Entropy: 1.026454.\n",
      "episode: 2253   score: 180.0  epsilon: 1.0    steps: 728  evaluation reward: 225.5\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4678: Policy loss: -0.000587. Value loss: 0.197497. Entropy: 1.058574.\n",
      "Iteration 4679: Policy loss: -0.002280. Value loss: 0.136208. Entropy: 1.060745.\n",
      "Iteration 4680: Policy loss: -0.010432. Value loss: 0.118617. Entropy: 1.064570.\n",
      "episode: 2254   score: 215.0  epsilon: 1.0    steps: 304  evaluation reward: 225.55\n",
      "episode: 2255   score: 215.0  epsilon: 1.0    steps: 416  evaluation reward: 225.25\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4681: Policy loss: 0.002046. Value loss: 0.153415. Entropy: 0.975575.\n",
      "Iteration 4682: Policy loss: -0.004738. Value loss: 0.094106. Entropy: 0.967172.\n",
      "Iteration 4683: Policy loss: -0.006991. Value loss: 0.079824. Entropy: 0.965390.\n",
      "episode: 2256   score: 155.0  epsilon: 1.0    steps: 752  evaluation reward: 225.25\n",
      "episode: 2257   score: 210.0  epsilon: 1.0    steps: 944  evaluation reward: 224.8\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4684: Policy loss: 0.003165. Value loss: 0.160256. Entropy: 0.964725.\n",
      "Iteration 4685: Policy loss: -0.004783. Value loss: 0.101421. Entropy: 0.941151.\n",
      "Iteration 4686: Policy loss: -0.010409. Value loss: 0.081887. Entropy: 0.961041.\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4687: Policy loss: 0.003151. Value loss: 0.279306. Entropy: 1.035766.\n",
      "Iteration 4688: Policy loss: -0.000870. Value loss: 0.138804. Entropy: 1.033951.\n",
      "Iteration 4689: Policy loss: -0.005175. Value loss: 0.093366. Entropy: 1.029293.\n",
      "episode: 2258   score: 215.0  epsilon: 1.0    steps: 72  evaluation reward: 226.3\n",
      "episode: 2259   score: 180.0  epsilon: 1.0    steps: 928  evaluation reward: 226.3\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4690: Policy loss: 0.001517. Value loss: 0.196258. Entropy: 1.157158.\n",
      "Iteration 4691: Policy loss: -0.002144. Value loss: 0.103650. Entropy: 1.165133.\n",
      "Iteration 4692: Policy loss: -0.001549. Value loss: 0.080333. Entropy: 1.164430.\n",
      "episode: 2260   score: 180.0  epsilon: 1.0    steps: 176  evaluation reward: 227.05\n",
      "episode: 2261   score: 210.0  epsilon: 1.0    steps: 856  evaluation reward: 226.75\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4693: Policy loss: 0.000336. Value loss: 0.235820. Entropy: 1.035318.\n",
      "Iteration 4694: Policy loss: -0.004925. Value loss: 0.130624. Entropy: 1.031886.\n",
      "Iteration 4695: Policy loss: -0.008110. Value loss: 0.085192. Entropy: 1.028896.\n",
      "episode: 2262   score: 245.0  epsilon: 1.0    steps: 288  evaluation reward: 226.3\n",
      "episode: 2263   score: 210.0  epsilon: 1.0    steps: 944  evaluation reward: 227.35\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4696: Policy loss: 0.001358. Value loss: 0.179529. Entropy: 1.057397.\n",
      "Iteration 4697: Policy loss: -0.002788. Value loss: 0.111322. Entropy: 1.042503.\n",
      "Iteration 4698: Policy loss: -0.007882. Value loss: 0.070132. Entropy: 1.035280.\n",
      "episode: 2264   score: 210.0  epsilon: 1.0    steps: 256  evaluation reward: 227.2\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4699: Policy loss: 0.001821. Value loss: 0.713213. Entropy: 1.067057.\n",
      "Iteration 4700: Policy loss: 0.003825. Value loss: 0.375131. Entropy: 1.078588.\n",
      "Iteration 4701: Policy loss: 0.004068. Value loss: 0.254933. Entropy: 1.084640.\n",
      "episode: 2265   score: 135.0  epsilon: 1.0    steps: 192  evaluation reward: 227.35\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4702: Policy loss: 0.001139. Value loss: 0.253781. Entropy: 1.067762.\n",
      "Iteration 4703: Policy loss: -0.006457. Value loss: 0.143436. Entropy: 1.046366.\n",
      "Iteration 4704: Policy loss: -0.008947. Value loss: 0.111112. Entropy: 1.056562.\n",
      "episode: 2266   score: 210.0  epsilon: 1.0    steps: 368  evaluation reward: 228.1\n",
      "episode: 2267   score: 210.0  epsilon: 1.0    steps: 1000  evaluation reward: 224.9\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4705: Policy loss: 0.004561. Value loss: 0.337911. Entropy: 1.093578.\n",
      "Iteration 4706: Policy loss: -0.003292. Value loss: 0.167446. Entropy: 1.087894.\n",
      "Iteration 4707: Policy loss: -0.007369. Value loss: 0.128974. Entropy: 1.087246.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2268   score: 180.0  epsilon: 1.0    steps: 744  evaluation reward: 225.2\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4708: Policy loss: 0.001949. Value loss: 0.227561. Entropy: 0.952493.\n",
      "Iteration 4709: Policy loss: -0.005782. Value loss: 0.099842. Entropy: 0.951780.\n",
      "Iteration 4710: Policy loss: -0.009060. Value loss: 0.085038. Entropy: 0.967907.\n",
      "episode: 2269   score: 490.0  epsilon: 1.0    steps: 112  evaluation reward: 228.0\n",
      "episode: 2270   score: 215.0  epsilon: 1.0    steps: 144  evaluation reward: 229.7\n",
      "episode: 2271   score: 180.0  epsilon: 1.0    steps: 560  evaluation reward: 229.1\n",
      "episode: 2272   score: 180.0  epsilon: 1.0    steps: 568  evaluation reward: 229.8\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4711: Policy loss: 0.001535. Value loss: 0.097554. Entropy: 0.958496.\n",
      "Iteration 4712: Policy loss: -0.004322. Value loss: 0.064363. Entropy: 0.946010.\n",
      "Iteration 4713: Policy loss: -0.007895. Value loss: 0.047035. Entropy: 0.958882.\n",
      "episode: 2273   score: 100.0  epsilon: 1.0    steps: 744  evaluation reward: 229.7\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4714: Policy loss: 0.004224. Value loss: 0.236468. Entropy: 0.965546.\n",
      "Iteration 4715: Policy loss: -0.001800. Value loss: 0.112486. Entropy: 0.967025.\n",
      "Iteration 4716: Policy loss: -0.004672. Value loss: 0.081752. Entropy: 0.956025.\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4717: Policy loss: 0.001127. Value loss: 0.149131. Entropy: 1.059013.\n",
      "Iteration 4718: Policy loss: -0.003803. Value loss: 0.100972. Entropy: 1.031445.\n",
      "Iteration 4719: Policy loss: -0.009269. Value loss: 0.076114. Entropy: 1.070860.\n",
      "episode: 2274   score: 155.0  epsilon: 1.0    steps: 696  evaluation reward: 229.15\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4720: Policy loss: -0.003771. Value loss: 0.145815. Entropy: 1.049660.\n",
      "Iteration 4721: Policy loss: -0.003853. Value loss: 0.085929. Entropy: 1.042430.\n",
      "Iteration 4722: Policy loss: -0.010769. Value loss: 0.059678. Entropy: 1.070580.\n",
      "episode: 2275   score: 215.0  epsilon: 1.0    steps: 224  evaluation reward: 230.3\n",
      "episode: 2276   score: 180.0  epsilon: 1.0    steps: 272  evaluation reward: 229.95\n",
      "episode: 2277   score: 210.0  epsilon: 1.0    steps: 392  evaluation reward: 229.75\n",
      "episode: 2278   score: 270.0  epsilon: 1.0    steps: 976  evaluation reward: 231.35\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4723: Policy loss: -0.000194. Value loss: 0.164652. Entropy: 1.081183.\n",
      "Iteration 4724: Policy loss: -0.000915. Value loss: 0.111461. Entropy: 1.067874.\n",
      "Iteration 4725: Policy loss: -0.008767. Value loss: 0.082112. Entropy: 1.074334.\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4726: Policy loss: -0.000161. Value loss: 0.177655. Entropy: 0.964629.\n",
      "Iteration 4727: Policy loss: -0.005500. Value loss: 0.103994. Entropy: 0.971620.\n",
      "Iteration 4728: Policy loss: -0.005293. Value loss: 0.089438. Entropy: 0.961070.\n",
      "episode: 2279   score: 230.0  epsilon: 1.0    steps: 56  evaluation reward: 232.45\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4729: Policy loss: -0.000077. Value loss: 0.164138. Entropy: 1.105486.\n",
      "Iteration 4730: Policy loss: -0.006442. Value loss: 0.114152. Entropy: 1.104320.\n",
      "Iteration 4731: Policy loss: -0.008138. Value loss: 0.073846. Entropy: 1.100826.\n",
      "episode: 2280   score: 235.0  epsilon: 1.0    steps: 584  evaluation reward: 233.75\n",
      "episode: 2281   score: 180.0  epsilon: 1.0    steps: 664  evaluation reward: 233.45\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4732: Policy loss: -0.000031. Value loss: 0.125468. Entropy: 1.002215.\n",
      "Iteration 4733: Policy loss: -0.005609. Value loss: 0.086924. Entropy: 1.013936.\n",
      "Iteration 4734: Policy loss: -0.010529. Value loss: 0.059796. Entropy: 0.999791.\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4735: Policy loss: 0.002849. Value loss: 0.131106. Entropy: 1.088191.\n",
      "Iteration 4736: Policy loss: -0.006514. Value loss: 0.053430. Entropy: 1.089388.\n",
      "Iteration 4737: Policy loss: -0.008286. Value loss: 0.044738. Entropy: 1.078347.\n",
      "episode: 2282   score: 210.0  epsilon: 1.0    steps: 800  evaluation reward: 233.45\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4738: Policy loss: 0.004145. Value loss: 0.534138. Entropy: 1.162818.\n",
      "Iteration 4739: Policy loss: -0.003025. Value loss: 0.259430. Entropy: 1.142022.\n",
      "Iteration 4740: Policy loss: -0.005016. Value loss: 0.123340. Entropy: 1.139167.\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4741: Policy loss: 0.001393. Value loss: 0.312025. Entropy: 1.015648.\n",
      "Iteration 4742: Policy loss: -0.007253. Value loss: 0.121252. Entropy: 1.032346.\n",
      "Iteration 4743: Policy loss: -0.015467. Value loss: 0.090364. Entropy: 1.034760.\n",
      "episode: 2283   score: 180.0  epsilon: 1.0    steps: 264  evaluation reward: 234.35\n",
      "episode: 2284   score: 245.0  epsilon: 1.0    steps: 544  evaluation reward: 233.9\n",
      "episode: 2285   score: 120.0  epsilon: 1.0    steps: 744  evaluation reward: 234.0\n",
      "episode: 2286   score: 240.0  epsilon: 1.0    steps: 856  evaluation reward: 234.3\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4744: Policy loss: 0.006013. Value loss: 0.749052. Entropy: 1.178263.\n",
      "Iteration 4745: Policy loss: 0.007956. Value loss: 0.338224. Entropy: 1.178064.\n",
      "Iteration 4746: Policy loss: 0.002738. Value loss: 0.333669. Entropy: 1.185321.\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4747: Policy loss: 0.003661. Value loss: 0.407349. Entropy: 0.907668.\n",
      "Iteration 4748: Policy loss: -0.000581. Value loss: 0.208618. Entropy: 0.902355.\n",
      "Iteration 4749: Policy loss: -0.004046. Value loss: 0.166439. Entropy: 0.909829.\n",
      "episode: 2287   score: 705.0  epsilon: 1.0    steps: 264  evaluation reward: 239.2\n",
      "episode: 2288   score: 470.0  epsilon: 1.0    steps: 808  evaluation reward: 242.1\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4750: Policy loss: -0.000018. Value loss: 0.527087. Entropy: 1.238015.\n",
      "Iteration 4751: Policy loss: 0.002603. Value loss: 0.306047. Entropy: 1.241292.\n",
      "Iteration 4752: Policy loss: 0.000226. Value loss: 0.267416. Entropy: 1.249092.\n",
      "episode: 2289   score: 135.0  epsilon: 1.0    steps: 992  evaluation reward: 241.35\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4753: Policy loss: 0.000696. Value loss: 0.591990. Entropy: 0.921140.\n",
      "Iteration 4754: Policy loss: -0.000828. Value loss: 0.260371. Entropy: 0.924138.\n",
      "Iteration 4755: Policy loss: -0.009956. Value loss: 0.206585. Entropy: 0.931368.\n",
      "episode: 2290   score: 210.0  epsilon: 1.0    steps: 8  evaluation reward: 241.65\n",
      "episode: 2291   score: 155.0  epsilon: 1.0    steps: 72  evaluation reward: 241.65\n",
      "episode: 2292   score: 180.0  epsilon: 1.0    steps: 904  evaluation reward: 241.35\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4756: Policy loss: 0.002188. Value loss: 0.234024. Entropy: 1.093392.\n",
      "Iteration 4757: Policy loss: -0.001914. Value loss: 0.150605. Entropy: 1.110584.\n",
      "Iteration 4758: Policy loss: -0.004358. Value loss: 0.118453. Entropy: 1.093251.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4759: Policy loss: 0.000512. Value loss: 0.234061. Entropy: 1.049872.\n",
      "Iteration 4760: Policy loss: 0.001449. Value loss: 0.134434. Entropy: 1.034069.\n",
      "Iteration 4761: Policy loss: -0.003459. Value loss: 0.107817. Entropy: 1.054132.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4762: Policy loss: 0.001923. Value loss: 0.402447. Entropy: 1.167367.\n",
      "Iteration 4763: Policy loss: -0.003444. Value loss: 0.187859. Entropy: 1.175900.\n",
      "Iteration 4764: Policy loss: -0.009663. Value loss: 0.143795. Entropy: 1.167022.\n",
      "episode: 2293   score: 240.0  epsilon: 1.0    steps: 136  evaluation reward: 241.95\n",
      "episode: 2294   score: 515.0  epsilon: 1.0    steps: 504  evaluation reward: 242.65\n",
      "episode: 2295   score: 210.0  epsilon: 1.0    steps: 648  evaluation reward: 242.6\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4765: Policy loss: 0.000001. Value loss: 0.328991. Entropy: 1.063203.\n",
      "Iteration 4766: Policy loss: -0.006967. Value loss: 0.176339. Entropy: 1.037616.\n",
      "Iteration 4767: Policy loss: -0.004505. Value loss: 0.136956. Entropy: 1.060545.\n",
      "episode: 2296   score: 180.0  epsilon: 1.0    steps: 16  evaluation reward: 241.05\n",
      "episode: 2297   score: 210.0  epsilon: 1.0    steps: 120  evaluation reward: 241.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2298   score: 210.0  epsilon: 1.0    steps: 456  evaluation reward: 241.35\n",
      "episode: 2299   score: 125.0  epsilon: 1.0    steps: 752  evaluation reward: 238.95\n",
      "episode: 2300   score: 285.0  epsilon: 1.0    steps: 848  evaluation reward: 240.25\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4768: Policy loss: 0.001481. Value loss: 0.228065. Entropy: 0.953028.\n",
      "Iteration 4769: Policy loss: 0.006435. Value loss: 0.140593. Entropy: 0.929651.\n",
      "Iteration 4770: Policy loss: -0.006685. Value loss: 0.111304. Entropy: 0.939685.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4771: Policy loss: 0.006473. Value loss: 0.227518. Entropy: 0.966597.\n",
      "Iteration 4772: Policy loss: -0.001326. Value loss: 0.135456. Entropy: 0.947103.\n",
      "Iteration 4773: Policy loss: 0.000641. Value loss: 0.110332. Entropy: 0.968463.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4774: Policy loss: 0.002582. Value loss: 0.319791. Entropy: 0.932696.\n",
      "Iteration 4775: Policy loss: -0.004378. Value loss: 0.152545. Entropy: 0.936102.\n",
      "Iteration 4776: Policy loss: -0.005838. Value loss: 0.118006. Entropy: 0.949191.\n",
      "now time :  2019-02-28 11:52:11.716896\n",
      "episode: 2301   score: 155.0  epsilon: 1.0    steps: 128  evaluation reward: 237.55\n",
      "episode: 2302   score: 210.0  epsilon: 1.0    steps: 904  evaluation reward: 236.7\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4777: Policy loss: -0.000723. Value loss: 0.178966. Entropy: 1.028517.\n",
      "Iteration 4778: Policy loss: -0.006920. Value loss: 0.119592. Entropy: 1.067553.\n",
      "Iteration 4779: Policy loss: -0.013017. Value loss: 0.075361. Entropy: 1.031776.\n",
      "episode: 2303   score: 210.0  epsilon: 1.0    steps: 640  evaluation reward: 236.7\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4780: Policy loss: 0.000472. Value loss: 0.200438. Entropy: 1.119933.\n",
      "Iteration 4781: Policy loss: -0.000873. Value loss: 0.119516. Entropy: 1.121234.\n",
      "Iteration 4782: Policy loss: -0.008230. Value loss: 0.094757. Entropy: 1.127539.\n",
      "episode: 2304   score: 240.0  epsilon: 1.0    steps: 744  evaluation reward: 237.0\n",
      "episode: 2305   score: 210.0  epsilon: 1.0    steps: 976  evaluation reward: 237.9\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4783: Policy loss: -0.000966. Value loss: 0.234067. Entropy: 1.140939.\n",
      "Iteration 4784: Policy loss: -0.008139. Value loss: 0.132733. Entropy: 1.133170.\n",
      "Iteration 4785: Policy loss: -0.010457. Value loss: 0.090004. Entropy: 1.142480.\n",
      "episode: 2306   score: 120.0  epsilon: 1.0    steps: 320  evaluation reward: 238.3\n",
      "episode: 2307   score: 155.0  epsilon: 1.0    steps: 616  evaluation reward: 238.6\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4786: Policy loss: 0.001438. Value loss: 0.161584. Entropy: 1.116267.\n",
      "Iteration 4787: Policy loss: 0.001042. Value loss: 0.106003. Entropy: 1.108166.\n",
      "Iteration 4788: Policy loss: -0.006958. Value loss: 0.077724. Entropy: 1.103021.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4789: Policy loss: 0.001542. Value loss: 0.200302. Entropy: 1.012244.\n",
      "Iteration 4790: Policy loss: -0.005910. Value loss: 0.087587. Entropy: 1.006466.\n",
      "Iteration 4791: Policy loss: -0.009426. Value loss: 0.060351. Entropy: 1.006078.\n",
      "episode: 2308   score: 185.0  epsilon: 1.0    steps: 104  evaluation reward: 236.25\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4792: Policy loss: 0.001592. Value loss: 0.143698. Entropy: 1.066521.\n",
      "Iteration 4793: Policy loss: -0.002181. Value loss: 0.081300. Entropy: 1.026449.\n",
      "Iteration 4794: Policy loss: -0.010812. Value loss: 0.069864. Entropy: 1.072993.\n",
      "episode: 2309   score: 150.0  epsilon: 1.0    steps: 992  evaluation reward: 236.4\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4795: Policy loss: 0.002266. Value loss: 0.199439. Entropy: 1.176445.\n",
      "Iteration 4796: Policy loss: -0.006510. Value loss: 0.108919. Entropy: 1.146242.\n",
      "Iteration 4797: Policy loss: -0.009373. Value loss: 0.088406. Entropy: 1.172738.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4798: Policy loss: 0.004144. Value loss: 0.320750. Entropy: 1.193116.\n",
      "Iteration 4799: Policy loss: -0.001751. Value loss: 0.149588. Entropy: 1.190555.\n",
      "Iteration 4800: Policy loss: -0.006111. Value loss: 0.093340. Entropy: 1.195070.\n",
      "episode: 2310   score: 215.0  epsilon: 1.0    steps: 656  evaluation reward: 237.35\n",
      "episode: 2311   score: 240.0  epsilon: 1.0    steps: 776  evaluation reward: 237.15\n",
      "episode: 2312   score: 515.0  epsilon: 1.0    steps: 928  evaluation reward: 240.5\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4801: Policy loss: 0.003991. Value loss: 0.524438. Entropy: 1.138751.\n",
      "Iteration 4802: Policy loss: 0.000536. Value loss: 0.252722. Entropy: 1.151881.\n",
      "Iteration 4803: Policy loss: -0.002994. Value loss: 0.213317. Entropy: 1.153035.\n",
      "episode: 2313   score: 210.0  epsilon: 1.0    steps: 464  evaluation reward: 241.4\n",
      "episode: 2314   score: 240.0  epsilon: 1.0    steps: 880  evaluation reward: 241.7\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4804: Policy loss: 0.002772. Value loss: 0.539042. Entropy: 0.988052.\n",
      "Iteration 4805: Policy loss: -0.001787. Value loss: 0.261263. Entropy: 0.973116.\n",
      "Iteration 4806: Policy loss: -0.010095. Value loss: 0.164839. Entropy: 0.994845.\n",
      "episode: 2315   score: 410.0  epsilon: 1.0    steps: 144  evaluation reward: 240.95\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4807: Policy loss: -0.000780. Value loss: 0.420790. Entropy: 1.087214.\n",
      "Iteration 4808: Policy loss: -0.002305. Value loss: 0.225949. Entropy: 1.091050.\n",
      "Iteration 4809: Policy loss: -0.007697. Value loss: 0.170042. Entropy: 1.107885.\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4810: Policy loss: -0.001435. Value loss: 0.254596. Entropy: 1.032484.\n",
      "Iteration 4811: Policy loss: -0.008847. Value loss: 0.133284. Entropy: 1.030618.\n",
      "Iteration 4812: Policy loss: -0.014237. Value loss: 0.088013. Entropy: 1.022623.\n",
      "episode: 2316   score: 155.0  epsilon: 1.0    steps: 520  evaluation reward: 240.4\n",
      "episode: 2317   score: 155.0  epsilon: 1.0    steps: 664  evaluation reward: 239.7\n",
      "episode: 2318   score: 155.0  epsilon: 1.0    steps: 944  evaluation reward: 238.35\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4813: Policy loss: 0.001237. Value loss: 0.479458. Entropy: 1.124705.\n",
      "Iteration 4814: Policy loss: -0.004096. Value loss: 0.218047. Entropy: 1.128648.\n",
      "Iteration 4815: Policy loss: -0.009462. Value loss: 0.142392. Entropy: 1.126964.\n",
      "episode: 2319   score: 425.0  epsilon: 1.0    steps: 528  evaluation reward: 240.45\n",
      "episode: 2320   score: 155.0  epsilon: 1.0    steps: 672  evaluation reward: 240.95\n",
      "episode: 2321   score: 300.0  epsilon: 1.0    steps: 776  evaluation reward: 240.0\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4816: Policy loss: 0.003859. Value loss: 0.357652. Entropy: 1.052228.\n",
      "Iteration 4817: Policy loss: -0.001699. Value loss: 0.208135. Entropy: 1.070791.\n",
      "Iteration 4818: Policy loss: -0.003897. Value loss: 0.132010. Entropy: 1.047585.\n",
      "episode: 2322   score: 240.0  epsilon: 1.0    steps: 576  evaluation reward: 240.3\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4819: Policy loss: 0.001945. Value loss: 0.342913. Entropy: 0.973707.\n",
      "Iteration 4820: Policy loss: -0.000146. Value loss: 0.175105. Entropy: 1.004776.\n",
      "Iteration 4821: Policy loss: -0.002523. Value loss: 0.123297. Entropy: 0.995771.\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4822: Policy loss: 0.001547. Value loss: 0.441247. Entropy: 0.975887.\n",
      "Iteration 4823: Policy loss: -0.003492. Value loss: 0.161884. Entropy: 0.966550.\n",
      "Iteration 4824: Policy loss: -0.007484. Value loss: 0.113178. Entropy: 0.970885.\n",
      "episode: 2323   score: 210.0  epsilon: 1.0    steps: 120  evaluation reward: 241.2\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4825: Policy loss: 0.001960. Value loss: 0.238954. Entropy: 1.153363.\n",
      "Iteration 4826: Policy loss: -0.001261. Value loss: 0.118955. Entropy: 1.142740.\n",
      "Iteration 4827: Policy loss: -0.005256. Value loss: 0.083753. Entropy: 1.139144.\n",
      "episode: 2324   score: 180.0  epsilon: 1.0    steps: 344  evaluation reward: 240.65\n",
      "episode: 2325   score: 165.0  epsilon: 1.0    steps: 448  evaluation reward: 236.4\n",
      "episode: 2326   score: 210.0  epsilon: 1.0    steps: 616  evaluation reward: 235.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4828: Policy loss: -0.001441. Value loss: 0.242515. Entropy: 1.169582.\n",
      "Iteration 4829: Policy loss: -0.004070. Value loss: 0.137590. Entropy: 1.176031.\n",
      "Iteration 4830: Policy loss: -0.007015. Value loss: 0.107048. Entropy: 1.174515.\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4831: Policy loss: 0.002672. Value loss: 0.280542. Entropy: 0.958003.\n",
      "Iteration 4832: Policy loss: -0.005973. Value loss: 0.139358. Entropy: 0.957092.\n",
      "Iteration 4833: Policy loss: -0.011499. Value loss: 0.103756. Entropy: 0.952482.\n",
      "episode: 2327   score: 250.0  epsilon: 1.0    steps: 240  evaluation reward: 235.95\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4834: Policy loss: 0.001070. Value loss: 0.298442. Entropy: 1.226607.\n",
      "Iteration 4835: Policy loss: -0.005065. Value loss: 0.123276. Entropy: 1.231098.\n",
      "Iteration 4836: Policy loss: -0.011977. Value loss: 0.103050. Entropy: 1.227541.\n",
      "episode: 2328   score: 285.0  epsilon: 1.0    steps: 16  evaluation reward: 237.45\n",
      "episode: 2329   score: 240.0  epsilon: 1.0    steps: 128  evaluation reward: 238.3\n",
      "episode: 2330   score: 225.0  epsilon: 1.0    steps: 784  evaluation reward: 239.2\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4837: Policy loss: -0.002984. Value loss: 0.129854. Entropy: 0.922394.\n",
      "Iteration 4838: Policy loss: -0.004841. Value loss: 0.081609. Entropy: 0.918690.\n",
      "Iteration 4839: Policy loss: -0.009126. Value loss: 0.054736. Entropy: 0.936172.\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4840: Policy loss: -0.003862. Value loss: 0.191802. Entropy: 1.093860.\n",
      "Iteration 4841: Policy loss: -0.005530. Value loss: 0.090845. Entropy: 1.110758.\n",
      "Iteration 4842: Policy loss: -0.011355. Value loss: 0.078470. Entropy: 1.102229.\n",
      "episode: 2331   score: 210.0  epsilon: 1.0    steps: 360  evaluation reward: 238.65\n",
      "episode: 2332   score: 240.0  epsilon: 1.0    steps: 672  evaluation reward: 237.9\n",
      "episode: 2333   score: 210.0  epsilon: 1.0    steps: 1008  evaluation reward: 237.9\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4843: Policy loss: 0.002231. Value loss: 0.173569. Entropy: 1.068937.\n",
      "Iteration 4844: Policy loss: -0.004003. Value loss: 0.083324. Entropy: 1.055564.\n",
      "Iteration 4845: Policy loss: -0.009968. Value loss: 0.073070. Entropy: 1.057382.\n",
      "episode: 2334   score: 185.0  epsilon: 1.0    steps: 64  evaluation reward: 237.15\n",
      "episode: 2335   score: 105.0  epsilon: 1.0    steps: 72  evaluation reward: 233.3\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4846: Policy loss: -0.001837. Value loss: 0.193610. Entropy: 0.926609.\n",
      "Iteration 4847: Policy loss: -0.006001. Value loss: 0.112925. Entropy: 0.931553.\n",
      "Iteration 4848: Policy loss: -0.008455. Value loss: 0.085833. Entropy: 0.918180.\n",
      "episode: 2336   score: 210.0  epsilon: 1.0    steps: 232  evaluation reward: 234.35\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4849: Policy loss: -0.003031. Value loss: 0.185135. Entropy: 1.119777.\n",
      "Iteration 4850: Policy loss: -0.000193. Value loss: 0.090773. Entropy: 1.113800.\n",
      "Iteration 4851: Policy loss: -0.008478. Value loss: 0.060183. Entropy: 1.122774.\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4852: Policy loss: 0.001031. Value loss: 0.103984. Entropy: 0.931223.\n",
      "Iteration 4853: Policy loss: -0.003927. Value loss: 0.057517. Entropy: 0.908347.\n",
      "Iteration 4854: Policy loss: -0.007106. Value loss: 0.046237. Entropy: 0.925297.\n",
      "episode: 2337   score: 225.0  epsilon: 1.0    steps: 464  evaluation reward: 234.8\n",
      "episode: 2338   score: 135.0  epsilon: 1.0    steps: 552  evaluation reward: 231.9\n",
      "episode: 2339   score: 210.0  epsilon: 1.0    steps: 832  evaluation reward: 230.65\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4855: Policy loss: -0.003261. Value loss: 0.597530. Entropy: 1.258289.\n",
      "Iteration 4856: Policy loss: 0.006786. Value loss: 0.302557. Entropy: 1.237753.\n",
      "Iteration 4857: Policy loss: -0.007206. Value loss: 0.328621. Entropy: 1.264055.\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4858: Policy loss: 0.002794. Value loss: 0.209711. Entropy: 0.995407.\n",
      "Iteration 4859: Policy loss: 0.000208. Value loss: 0.098064. Entropy: 1.003841.\n",
      "Iteration 4860: Policy loss: -0.006997. Value loss: 0.073279. Entropy: 0.999851.\n",
      "episode: 2340   score: 210.0  epsilon: 1.0    steps: 184  evaluation reward: 227.35\n",
      "episode: 2341   score: 150.0  epsilon: 1.0    steps: 504  evaluation reward: 224.15\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4861: Policy loss: 0.004330. Value loss: 0.403938. Entropy: 1.240252.\n",
      "Iteration 4862: Policy loss: -0.001700. Value loss: 0.203910. Entropy: 1.250965.\n",
      "Iteration 4863: Policy loss: -0.007324. Value loss: 0.165671. Entropy: 1.244564.\n",
      "episode: 2342   score: 505.0  epsilon: 1.0    steps: 888  evaluation reward: 227.65\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4864: Policy loss: 0.004078. Value loss: 0.248901. Entropy: 0.987280.\n",
      "Iteration 4865: Policy loss: -0.003246. Value loss: 0.130277. Entropy: 0.982066.\n",
      "Iteration 4866: Policy loss: -0.009512. Value loss: 0.084188. Entropy: 1.024177.\n",
      "episode: 2343   score: 335.0  epsilon: 1.0    steps: 536  evaluation reward: 228.55\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4867: Policy loss: -0.000006. Value loss: 0.157425. Entropy: 1.059765.\n",
      "Iteration 4868: Policy loss: -0.002387. Value loss: 0.081323. Entropy: 1.041211.\n",
      "Iteration 4869: Policy loss: -0.008901. Value loss: 0.063758. Entropy: 1.056889.\n",
      "episode: 2344   score: 180.0  epsilon: 1.0    steps: 672  evaluation reward: 228.8\n",
      "episode: 2345   score: 220.0  epsilon: 1.0    steps: 800  evaluation reward: 226.95\n",
      "episode: 2346   score: 245.0  epsilon: 1.0    steps: 976  evaluation reward: 226.8\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4870: Policy loss: -0.000028. Value loss: 0.222716. Entropy: 1.148842.\n",
      "Iteration 4871: Policy loss: -0.003291. Value loss: 0.090812. Entropy: 1.136263.\n",
      "Iteration 4872: Policy loss: -0.007182. Value loss: 0.061856. Entropy: 1.146076.\n",
      "episode: 2347   score: 225.0  epsilon: 1.0    steps: 648  evaluation reward: 227.25\n",
      "episode: 2348   score: 180.0  epsilon: 1.0    steps: 920  evaluation reward: 223.4\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4873: Policy loss: -0.000100. Value loss: 0.080993. Entropy: 0.950279.\n",
      "Iteration 4874: Policy loss: -0.001367. Value loss: 0.050994. Entropy: 0.956200.\n",
      "Iteration 4875: Policy loss: -0.004602. Value loss: 0.040927. Entropy: 0.950629.\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4876: Policy loss: 0.001465. Value loss: 0.149663. Entropy: 1.011736.\n",
      "Iteration 4877: Policy loss: -0.003817. Value loss: 0.065800. Entropy: 0.995261.\n",
      "Iteration 4878: Policy loss: -0.006718. Value loss: 0.051619. Entropy: 1.008839.\n",
      "episode: 2349   score: 260.0  epsilon: 1.0    steps: 768  evaluation reward: 223.6\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4879: Policy loss: 0.000764. Value loss: 0.211008. Entropy: 1.141638.\n",
      "Iteration 4880: Policy loss: -0.009417. Value loss: 0.126435. Entropy: 1.142663.\n",
      "Iteration 4881: Policy loss: -0.008451. Value loss: 0.109759. Entropy: 1.135350.\n",
      "episode: 2350   score: 180.0  epsilon: 1.0    steps: 904  evaluation reward: 224.4\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4882: Policy loss: -0.003409. Value loss: 0.306971. Entropy: 1.091074.\n",
      "Iteration 4883: Policy loss: -0.003623. Value loss: 0.123989. Entropy: 1.069408.\n",
      "Iteration 4884: Policy loss: -0.012385. Value loss: 0.091437. Entropy: 1.071343.\n",
      "now time :  2019-02-28 11:53:30.662823\n",
      "episode: 2351   score: 210.0  epsilon: 1.0    steps: 392  evaluation reward: 224.05\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4885: Policy loss: 0.001586. Value loss: 0.379956. Entropy: 1.155500.\n",
      "Iteration 4886: Policy loss: -0.004017. Value loss: 0.189172. Entropy: 1.140510.\n",
      "Iteration 4887: Policy loss: -0.009595. Value loss: 0.096412. Entropy: 1.135841.\n",
      "episode: 2352   score: 180.0  epsilon: 1.0    steps: 344  evaluation reward: 224.5\n",
      "episode: 2353   score: 370.0  epsilon: 1.0    steps: 1016  evaluation reward: 226.4\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4888: Policy loss: -0.000119. Value loss: 0.371125. Entropy: 1.142130.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4889: Policy loss: -0.006096. Value loss: 0.173250. Entropy: 1.159200.\n",
      "Iteration 4890: Policy loss: -0.008434. Value loss: 0.110718. Entropy: 1.147243.\n",
      "episode: 2354   score: 285.0  epsilon: 1.0    steps: 824  evaluation reward: 227.1\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4891: Policy loss: 0.003717. Value loss: 0.627423. Entropy: 1.107727.\n",
      "Iteration 4892: Policy loss: 0.005244. Value loss: 0.246718. Entropy: 1.114467.\n",
      "Iteration 4893: Policy loss: -0.001690. Value loss: 0.146916. Entropy: 1.121567.\n",
      "episode: 2355   score: 135.0  epsilon: 1.0    steps: 424  evaluation reward: 226.3\n",
      "episode: 2356   score: 290.0  epsilon: 1.0    steps: 944  evaluation reward: 227.65\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4894: Policy loss: 0.000781. Value loss: 0.328296. Entropy: 1.052584.\n",
      "Iteration 4895: Policy loss: -0.006797. Value loss: 0.136041. Entropy: 1.017880.\n",
      "Iteration 4896: Policy loss: -0.008499. Value loss: 0.092080. Entropy: 1.032302.\n",
      "episode: 2357   score: 150.0  epsilon: 1.0    steps: 168  evaluation reward: 227.05\n",
      "episode: 2358   score: 320.0  epsilon: 1.0    steps: 544  evaluation reward: 228.1\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4897: Policy loss: -0.001397. Value loss: 0.297806. Entropy: 0.956954.\n",
      "Iteration 4898: Policy loss: -0.001660. Value loss: 0.128249. Entropy: 0.942771.\n",
      "Iteration 4899: Policy loss: -0.002756. Value loss: 0.115600. Entropy: 0.981045.\n",
      "episode: 2359   score: 475.0  epsilon: 1.0    steps: 1016  evaluation reward: 231.05\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4900: Policy loss: 0.005248. Value loss: 0.273272. Entropy: 0.984573.\n",
      "Iteration 4901: Policy loss: 0.000673. Value loss: 0.148872. Entropy: 1.003110.\n",
      "Iteration 4902: Policy loss: -0.009907. Value loss: 0.114616. Entropy: 0.996049.\n",
      "episode: 2360   score: 100.0  epsilon: 1.0    steps: 384  evaluation reward: 230.25\n",
      "episode: 2361   score: 120.0  epsilon: 1.0    steps: 864  evaluation reward: 229.35\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4903: Policy loss: 0.002166. Value loss: 0.211898. Entropy: 1.142478.\n",
      "Iteration 4904: Policy loss: -0.001787. Value loss: 0.120843. Entropy: 1.160108.\n",
      "Iteration 4905: Policy loss: -0.002806. Value loss: 0.095198. Entropy: 1.155297.\n",
      "episode: 2362   score: 215.0  epsilon: 1.0    steps: 880  evaluation reward: 229.05\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4906: Policy loss: -0.000512. Value loss: 0.362193. Entropy: 0.975415.\n",
      "Iteration 4907: Policy loss: -0.005743. Value loss: 0.197402. Entropy: 0.967746.\n",
      "Iteration 4908: Policy loss: -0.010675. Value loss: 0.127628. Entropy: 0.989595.\n",
      "episode: 2363   score: 120.0  epsilon: 1.0    steps: 1000  evaluation reward: 228.15\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4909: Policy loss: -0.002461. Value loss: 0.335381. Entropy: 1.098735.\n",
      "Iteration 4910: Policy loss: -0.001091. Value loss: 0.181690. Entropy: 1.086214.\n",
      "Iteration 4911: Policy loss: -0.009682. Value loss: 0.144589. Entropy: 1.078975.\n",
      "episode: 2364   score: 125.0  epsilon: 1.0    steps: 88  evaluation reward: 227.3\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4912: Policy loss: 0.001823. Value loss: 0.314350. Entropy: 1.033063.\n",
      "Iteration 4913: Policy loss: 0.000006. Value loss: 0.130935. Entropy: 1.020491.\n",
      "Iteration 4914: Policy loss: -0.004175. Value loss: 0.084326. Entropy: 1.027946.\n",
      "episode: 2365   score: 155.0  epsilon: 1.0    steps: 560  evaluation reward: 227.5\n",
      "episode: 2366   score: 235.0  epsilon: 1.0    steps: 696  evaluation reward: 227.75\n",
      "episode: 2367   score: 310.0  epsilon: 1.0    steps: 960  evaluation reward: 228.75\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4915: Policy loss: 0.000804. Value loss: 0.165144. Entropy: 1.084739.\n",
      "Iteration 4916: Policy loss: -0.001001. Value loss: 0.093121. Entropy: 1.101291.\n",
      "Iteration 4917: Policy loss: -0.008403. Value loss: 0.070538. Entropy: 1.102139.\n",
      "episode: 2368   score: 210.0  epsilon: 1.0    steps: 264  evaluation reward: 229.05\n",
      "episode: 2369   score: 155.0  epsilon: 1.0    steps: 624  evaluation reward: 225.7\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4918: Policy loss: 0.001521. Value loss: 0.202403. Entropy: 0.886716.\n",
      "Iteration 4919: Policy loss: -0.000201. Value loss: 0.111193. Entropy: 0.907029.\n",
      "Iteration 4920: Policy loss: -0.005610. Value loss: 0.092480. Entropy: 0.911425.\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4921: Policy loss: 0.001022. Value loss: 0.240776. Entropy: 1.000777.\n",
      "Iteration 4922: Policy loss: -0.001847. Value loss: 0.134162. Entropy: 1.001348.\n",
      "Iteration 4923: Policy loss: -0.010267. Value loss: 0.106911. Entropy: 1.011694.\n",
      "episode: 2370   score: 180.0  epsilon: 1.0    steps: 120  evaluation reward: 225.35\n",
      "episode: 2371   score: 245.0  epsilon: 1.0    steps: 536  evaluation reward: 226.0\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4924: Policy loss: 0.001351. Value loss: 0.243703. Entropy: 1.071890.\n",
      "Iteration 4925: Policy loss: -0.007168. Value loss: 0.163480. Entropy: 1.069891.\n",
      "Iteration 4926: Policy loss: -0.005880. Value loss: 0.118061. Entropy: 1.074097.\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4927: Policy loss: 0.001263. Value loss: 0.263501. Entropy: 0.983557.\n",
      "Iteration 4928: Policy loss: -0.006820. Value loss: 0.104892. Entropy: 0.998133.\n",
      "Iteration 4929: Policy loss: -0.007308. Value loss: 0.092926. Entropy: 1.008866.\n",
      "episode: 2372   score: 195.0  epsilon: 1.0    steps: 200  evaluation reward: 226.15\n",
      "episode: 2373   score: 110.0  epsilon: 1.0    steps: 256  evaluation reward: 226.25\n",
      "episode: 2374   score: 180.0  epsilon: 1.0    steps: 536  evaluation reward: 226.5\n",
      "episode: 2375   score: 120.0  epsilon: 1.0    steps: 584  evaluation reward: 225.55\n",
      "episode: 2376   score: 230.0  epsilon: 1.0    steps: 1016  evaluation reward: 226.05\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4930: Policy loss: 0.001215. Value loss: 0.453158. Entropy: 1.248844.\n",
      "Iteration 4931: Policy loss: -0.000527. Value loss: 0.239979. Entropy: 1.251520.\n",
      "Iteration 4932: Policy loss: -0.005572. Value loss: 0.175772. Entropy: 1.265699.\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4933: Policy loss: 0.002883. Value loss: 0.455634. Entropy: 0.809534.\n",
      "Iteration 4934: Policy loss: -0.004641. Value loss: 0.229319. Entropy: 0.803495.\n",
      "Iteration 4935: Policy loss: -0.010014. Value loss: 0.142087. Entropy: 0.805145.\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4936: Policy loss: 0.002558. Value loss: 0.392663. Entropy: 1.067433.\n",
      "Iteration 4937: Policy loss: -0.001882. Value loss: 0.195413. Entropy: 1.065051.\n",
      "Iteration 4938: Policy loss: -0.007241. Value loss: 0.147328. Entropy: 1.077786.\n",
      "episode: 2377   score: 270.0  epsilon: 1.0    steps: 944  evaluation reward: 226.65\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4939: Policy loss: 0.000444. Value loss: 0.435380. Entropy: 1.096706.\n",
      "Iteration 4940: Policy loss: -0.004468. Value loss: 0.165760. Entropy: 1.082721.\n",
      "Iteration 4941: Policy loss: -0.008843. Value loss: 0.140338. Entropy: 1.080708.\n",
      "episode: 2378   score: 295.0  epsilon: 1.0    steps: 632  evaluation reward: 226.9\n",
      "episode: 2379   score: 80.0  epsilon: 1.0    steps: 936  evaluation reward: 225.4\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4942: Policy loss: 0.000924. Value loss: 0.303101. Entropy: 1.194257.\n",
      "Iteration 4943: Policy loss: -0.009159. Value loss: 0.163831. Entropy: 1.192108.\n",
      "Iteration 4944: Policy loss: -0.010110. Value loss: 0.148187. Entropy: 1.181090.\n",
      "episode: 2380   score: 215.0  epsilon: 1.0    steps: 144  evaluation reward: 225.2\n",
      "episode: 2381   score: 300.0  epsilon: 1.0    steps: 640  evaluation reward: 226.4\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4945: Policy loss: 0.002195. Value loss: 0.338466. Entropy: 1.099975.\n",
      "Iteration 4946: Policy loss: -0.004758. Value loss: 0.175814. Entropy: 1.132274.\n",
      "Iteration 4947: Policy loss: -0.003141. Value loss: 0.101111. Entropy: 1.115298.\n",
      "episode: 2382   score: 225.0  epsilon: 1.0    steps: 16  evaluation reward: 226.55\n",
      "episode: 2383   score: 145.0  epsilon: 1.0    steps: 32  evaluation reward: 226.2\n",
      "Training network. lr: 0.000212. clip: 0.084950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4948: Policy loss: -0.000397. Value loss: 0.204386. Entropy: 1.006564.\n",
      "Iteration 4949: Policy loss: -0.003879. Value loss: 0.108573. Entropy: 1.003976.\n",
      "Iteration 4950: Policy loss: -0.004572. Value loss: 0.090634. Entropy: 1.009753.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4951: Policy loss: 0.003086. Value loss: 0.285920. Entropy: 0.936759.\n",
      "Iteration 4952: Policy loss: -0.002070. Value loss: 0.106549. Entropy: 0.946663.\n",
      "Iteration 4953: Policy loss: -0.006374. Value loss: 0.080601. Entropy: 0.945268.\n",
      "episode: 2384   score: 105.0  epsilon: 1.0    steps: 88  evaluation reward: 224.8\n",
      "episode: 2385   score: 210.0  epsilon: 1.0    steps: 432  evaluation reward: 225.7\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4954: Policy loss: 0.006236. Value loss: 0.310371. Entropy: 1.143196.\n",
      "Iteration 4955: Policy loss: -0.001063. Value loss: 0.134028. Entropy: 1.123294.\n",
      "Iteration 4956: Policy loss: -0.007448. Value loss: 0.098190. Entropy: 1.141445.\n",
      "episode: 2386   score: 105.0  epsilon: 1.0    steps: 488  evaluation reward: 224.35\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4957: Policy loss: 0.002074. Value loss: 0.174185. Entropy: 1.085396.\n",
      "Iteration 4958: Policy loss: -0.003784. Value loss: 0.093596. Entropy: 1.100352.\n",
      "Iteration 4959: Policy loss: -0.006555. Value loss: 0.069713. Entropy: 1.067872.\n",
      "episode: 2387   score: 240.0  epsilon: 1.0    steps: 64  evaluation reward: 219.7\n",
      "episode: 2388   score: 155.0  epsilon: 1.0    steps: 488  evaluation reward: 216.55\n",
      "episode: 2389   score: 180.0  epsilon: 1.0    steps: 544  evaluation reward: 217.0\n",
      "episode: 2390   score: 210.0  epsilon: 1.0    steps: 752  evaluation reward: 217.0\n",
      "episode: 2391   score: 255.0  epsilon: 1.0    steps: 896  evaluation reward: 218.0\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4960: Policy loss: -0.003603. Value loss: 0.197681. Entropy: 1.145899.\n",
      "Iteration 4961: Policy loss: -0.006472. Value loss: 0.119472. Entropy: 1.156255.\n",
      "Iteration 4962: Policy loss: -0.011271. Value loss: 0.087971. Entropy: 1.143512.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4963: Policy loss: 0.000527. Value loss: 0.277645. Entropy: 0.743437.\n",
      "Iteration 4964: Policy loss: -0.006942. Value loss: 0.146686. Entropy: 0.745732.\n",
      "Iteration 4965: Policy loss: -0.013087. Value loss: 0.104421. Entropy: 0.768664.\n",
      "episode: 2392   score: 210.0  epsilon: 1.0    steps: 712  evaluation reward: 218.3\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4966: Policy loss: 0.001155. Value loss: 0.330315. Entropy: 1.166570.\n",
      "Iteration 4967: Policy loss: -0.007748. Value loss: 0.172182. Entropy: 1.155829.\n",
      "Iteration 4968: Policy loss: -0.008933. Value loss: 0.123861. Entropy: 1.147487.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4969: Policy loss: 0.000067. Value loss: 0.355811. Entropy: 1.051485.\n",
      "Iteration 4970: Policy loss: -0.010445. Value loss: 0.159303. Entropy: 1.031383.\n",
      "Iteration 4971: Policy loss: -0.016131. Value loss: 0.118751. Entropy: 1.058051.\n",
      "episode: 2393   score: 155.0  epsilon: 1.0    steps: 328  evaluation reward: 217.45\n",
      "episode: 2394   score: 180.0  epsilon: 1.0    steps: 376  evaluation reward: 214.1\n",
      "episode: 2395   score: 280.0  epsilon: 1.0    steps: 512  evaluation reward: 214.8\n",
      "episode: 2396   score: 155.0  epsilon: 1.0    steps: 888  evaluation reward: 214.55\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4972: Policy loss: 0.001999. Value loss: 0.422981. Entropy: 1.228121.\n",
      "Iteration 4973: Policy loss: -0.003780. Value loss: 0.236910. Entropy: 1.224065.\n",
      "Iteration 4974: Policy loss: -0.008955. Value loss: 0.159417. Entropy: 1.229252.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4975: Policy loss: 0.001163. Value loss: 0.390167. Entropy: 0.893348.\n",
      "Iteration 4976: Policy loss: 0.003541. Value loss: 0.172667. Entropy: 0.924852.\n",
      "Iteration 4977: Policy loss: -0.008648. Value loss: 0.115459. Entropy: 0.906333.\n",
      "episode: 2397   score: 210.0  epsilon: 1.0    steps: 912  evaluation reward: 214.55\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4978: Policy loss: -0.004267. Value loss: 0.667531. Entropy: 1.100080.\n",
      "Iteration 4979: Policy loss: -0.007939. Value loss: 0.250289. Entropy: 1.101849.\n",
      "Iteration 4980: Policy loss: -0.015584. Value loss: 0.149084. Entropy: 1.103116.\n",
      "episode: 2398   score: 65.0  epsilon: 1.0    steps: 288  evaluation reward: 213.1\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4981: Policy loss: 0.000746. Value loss: 0.449369. Entropy: 1.159970.\n",
      "Iteration 4982: Policy loss: -0.005027. Value loss: 0.233614. Entropy: 1.167005.\n",
      "Iteration 4983: Policy loss: -0.010700. Value loss: 0.155478. Entropy: 1.167060.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4984: Policy loss: 0.001609. Value loss: 0.414383. Entropy: 1.116408.\n",
      "Iteration 4985: Policy loss: -0.001695. Value loss: 0.218176. Entropy: 1.136829.\n",
      "Iteration 4986: Policy loss: -0.011045. Value loss: 0.122084. Entropy: 1.136423.\n",
      "episode: 2399   score: 560.0  epsilon: 1.0    steps: 832  evaluation reward: 217.45\n",
      "episode: 2400   score: 215.0  epsilon: 1.0    steps: 1008  evaluation reward: 216.75\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4987: Policy loss: -0.002276. Value loss: 0.589303. Entropy: 1.214344.\n",
      "Iteration 4988: Policy loss: -0.002853. Value loss: 0.199641. Entropy: 1.214866.\n",
      "Iteration 4989: Policy loss: -0.010478. Value loss: 0.096695. Entropy: 1.218206.\n",
      "now time :  2019-02-28 11:54:45.916031\n",
      "episode: 2401   score: 225.0  epsilon: 1.0    steps: 16  evaluation reward: 217.45\n",
      "episode: 2402   score: 105.0  epsilon: 1.0    steps: 296  evaluation reward: 216.4\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4990: Policy loss: 0.005033. Value loss: 0.514901. Entropy: 1.003504.\n",
      "Iteration 4991: Policy loss: 0.001372. Value loss: 0.253431. Entropy: 1.023175.\n",
      "Iteration 4992: Policy loss: -0.008389. Value loss: 0.138671. Entropy: 1.029240.\n",
      "episode: 2403   score: 500.0  epsilon: 1.0    steps: 96  evaluation reward: 219.3\n",
      "episode: 2404   score: 210.0  epsilon: 1.0    steps: 320  evaluation reward: 219.0\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4993: Policy loss: 0.004402. Value loss: 0.432845. Entropy: 1.014890.\n",
      "Iteration 4994: Policy loss: -0.001705. Value loss: 0.206007. Entropy: 1.012629.\n",
      "Iteration 4995: Policy loss: -0.011399. Value loss: 0.154404. Entropy: 0.999016.\n",
      "episode: 2405   score: 335.0  epsilon: 1.0    steps: 520  evaluation reward: 220.25\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4996: Policy loss: 0.002034. Value loss: 0.319420. Entropy: 1.067784.\n",
      "Iteration 4997: Policy loss: -0.003381. Value loss: 0.145021. Entropy: 1.058515.\n",
      "Iteration 4998: Policy loss: -0.007182. Value loss: 0.093877. Entropy: 1.065381.\n",
      "episode: 2406   score: 180.0  epsilon: 1.0    steps: 936  evaluation reward: 220.85\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4999: Policy loss: 0.000420. Value loss: 0.347183. Entropy: 1.041231.\n",
      "Iteration 5000: Policy loss: 0.002057. Value loss: 0.174888. Entropy: 1.022868.\n",
      "Iteration 5001: Policy loss: -0.003451. Value loss: 0.129258. Entropy: 1.031288.\n",
      "episode: 2407   score: 60.0  epsilon: 1.0    steps: 216  evaluation reward: 219.9\n",
      "episode: 2408   score: 280.0  epsilon: 1.0    steps: 320  evaluation reward: 220.85\n",
      "episode: 2409   score: 180.0  epsilon: 1.0    steps: 400  evaluation reward: 221.15\n",
      "episode: 2410   score: 215.0  epsilon: 1.0    steps: 744  evaluation reward: 221.15\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5002: Policy loss: -0.000834. Value loss: 0.221422. Entropy: 1.193361.\n",
      "Iteration 5003: Policy loss: -0.002432. Value loss: 0.124564. Entropy: 1.185218.\n",
      "Iteration 5004: Policy loss: -0.006061. Value loss: 0.094069. Entropy: 1.185528.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5005: Policy loss: -0.000402. Value loss: 0.351665. Entropy: 0.940255.\n",
      "Iteration 5006: Policy loss: -0.003241. Value loss: 0.145660. Entropy: 0.980085.\n",
      "Iteration 5007: Policy loss: -0.007350. Value loss: 0.119010. Entropy: 0.951224.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5008: Policy loss: -0.000029. Value loss: 0.396952. Entropy: 1.146881.\n",
      "Iteration 5009: Policy loss: 0.000423. Value loss: 0.199226. Entropy: 1.165531.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5010: Policy loss: -0.009834. Value loss: 0.139024. Entropy: 1.155841.\n",
      "episode: 2411   score: 475.0  epsilon: 1.0    steps: 488  evaluation reward: 223.5\n",
      "episode: 2412   score: 215.0  epsilon: 1.0    steps: 528  evaluation reward: 220.5\n",
      "episode: 2413   score: 65.0  epsilon: 1.0    steps: 640  evaluation reward: 219.05\n",
      "episode: 2414   score: 365.0  epsilon: 1.0    steps: 656  evaluation reward: 220.3\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5011: Policy loss: -0.001926. Value loss: 0.798971. Entropy: 1.111579.\n",
      "Iteration 5012: Policy loss: 0.006663. Value loss: 0.396555. Entropy: 1.102578.\n",
      "Iteration 5013: Policy loss: -0.002001. Value loss: 0.300258. Entropy: 1.104289.\n",
      "episode: 2415   score: 210.0  epsilon: 1.0    steps: 264  evaluation reward: 218.3\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5014: Policy loss: 0.001153. Value loss: 0.394182. Entropy: 0.959859.\n",
      "Iteration 5015: Policy loss: -0.002688. Value loss: 0.212522. Entropy: 0.976735.\n",
      "Iteration 5016: Policy loss: -0.005993. Value loss: 0.145243. Entropy: 0.956810.\n",
      "episode: 2416   score: 210.0  epsilon: 1.0    steps: 608  evaluation reward: 218.85\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5017: Policy loss: 0.000701. Value loss: 0.403339. Entropy: 1.049395.\n",
      "Iteration 5018: Policy loss: -0.004565. Value loss: 0.161246. Entropy: 1.044676.\n",
      "Iteration 5019: Policy loss: -0.008751. Value loss: 0.114904. Entropy: 1.056358.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5020: Policy loss: 0.006659. Value loss: 0.372823. Entropy: 1.005815.\n",
      "Iteration 5021: Policy loss: -0.003611. Value loss: 0.160992. Entropy: 1.020186.\n",
      "Iteration 5022: Policy loss: -0.010935. Value loss: 0.103121. Entropy: 0.999340.\n",
      "episode: 2417   score: 210.0  epsilon: 1.0    steps: 624  evaluation reward: 219.4\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5023: Policy loss: 0.000044. Value loss: 0.428773. Entropy: 1.256334.\n",
      "Iteration 5024: Policy loss: -0.005508. Value loss: 0.175171. Entropy: 1.254566.\n",
      "Iteration 5025: Policy loss: -0.011559. Value loss: 0.102900. Entropy: 1.248868.\n",
      "episode: 2418   score: 315.0  epsilon: 1.0    steps: 80  evaluation reward: 221.0\n",
      "episode: 2419   score: 315.0  epsilon: 1.0    steps: 888  evaluation reward: 219.9\n",
      "episode: 2420   score: 240.0  epsilon: 1.0    steps: 928  evaluation reward: 220.75\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5026: Policy loss: -0.000194. Value loss: 0.195561. Entropy: 1.104880.\n",
      "Iteration 5027: Policy loss: -0.007601. Value loss: 0.113319. Entropy: 1.108452.\n",
      "Iteration 5028: Policy loss: -0.009697. Value loss: 0.080449. Entropy: 1.112477.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5029: Policy loss: 0.001565. Value loss: 0.272892. Entropy: 1.104548.\n",
      "Iteration 5030: Policy loss: -0.006517. Value loss: 0.148211. Entropy: 1.113224.\n",
      "Iteration 5031: Policy loss: -0.013763. Value loss: 0.086082. Entropy: 1.116828.\n",
      "episode: 2421   score: 215.0  epsilon: 1.0    steps: 488  evaluation reward: 219.9\n",
      "episode: 2422   score: 210.0  epsilon: 1.0    steps: 600  evaluation reward: 219.6\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5032: Policy loss: -0.001541. Value loss: 0.337743. Entropy: 1.097514.\n",
      "Iteration 5033: Policy loss: -0.008412. Value loss: 0.135622. Entropy: 1.098536.\n",
      "Iteration 5034: Policy loss: -0.012861. Value loss: 0.087039. Entropy: 1.098801.\n",
      "episode: 2423   score: 305.0  epsilon: 1.0    steps: 144  evaluation reward: 220.55\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5035: Policy loss: -0.004374. Value loss: 0.143247. Entropy: 1.053363.\n",
      "Iteration 5036: Policy loss: -0.006503. Value loss: 0.090058. Entropy: 1.039302.\n",
      "Iteration 5037: Policy loss: -0.013237. Value loss: 0.068902. Entropy: 1.043018.\n",
      "episode: 2424   score: 330.0  epsilon: 1.0    steps: 536  evaluation reward: 222.05\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5038: Policy loss: -0.001883. Value loss: 0.204004. Entropy: 1.171129.\n",
      "Iteration 5039: Policy loss: -0.005762. Value loss: 0.102777. Entropy: 1.169194.\n",
      "Iteration 5040: Policy loss: -0.009237. Value loss: 0.097383. Entropy: 1.169917.\n",
      "episode: 2425   score: 240.0  epsilon: 1.0    steps: 208  evaluation reward: 222.8\n",
      "episode: 2426   score: 210.0  epsilon: 1.0    steps: 888  evaluation reward: 222.8\n",
      "episode: 2427   score: 220.0  epsilon: 1.0    steps: 968  evaluation reward: 222.5\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5041: Policy loss: 0.001177. Value loss: 0.169152. Entropy: 0.964893.\n",
      "Iteration 5042: Policy loss: -0.005095. Value loss: 0.104223. Entropy: 0.965909.\n",
      "Iteration 5043: Policy loss: -0.011659. Value loss: 0.079973. Entropy: 0.953868.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5044: Policy loss: 0.009123. Value loss: 0.438024. Entropy: 0.987182.\n",
      "Iteration 5045: Policy loss: -0.000812. Value loss: 0.129512. Entropy: 1.004866.\n",
      "Iteration 5046: Policy loss: -0.003278. Value loss: 0.073003. Entropy: 0.995171.\n",
      "episode: 2428   score: 225.0  epsilon: 1.0    steps: 512  evaluation reward: 221.9\n",
      "episode: 2429   score: 210.0  epsilon: 1.0    steps: 600  evaluation reward: 221.6\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5047: Policy loss: -0.001290. Value loss: 0.237966. Entropy: 1.070674.\n",
      "Iteration 5048: Policy loss: -0.004507. Value loss: 0.119155. Entropy: 1.075079.\n",
      "Iteration 5049: Policy loss: -0.011252. Value loss: 0.084133. Entropy: 1.081113.\n",
      "episode: 2430   score: 470.0  epsilon: 1.0    steps: 328  evaluation reward: 224.05\n",
      "episode: 2431   score: 260.0  epsilon: 1.0    steps: 512  evaluation reward: 224.55\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5050: Policy loss: -0.000343. Value loss: 0.285332. Entropy: 0.955828.\n",
      "Iteration 5051: Policy loss: -0.004225. Value loss: 0.172277. Entropy: 0.947575.\n",
      "Iteration 5052: Policy loss: -0.008740. Value loss: 0.113485. Entropy: 0.962294.\n",
      "episode: 2432   score: 210.0  epsilon: 1.0    steps: 8  evaluation reward: 224.25\n",
      "episode: 2433   score: 210.0  epsilon: 1.0    steps: 904  evaluation reward: 224.25\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5053: Policy loss: 0.001815. Value loss: 0.142200. Entropy: 0.961495.\n",
      "Iteration 5054: Policy loss: -0.007255. Value loss: 0.099373. Entropy: 0.961728.\n",
      "Iteration 5055: Policy loss: -0.010729. Value loss: 0.079047. Entropy: 0.958779.\n",
      "episode: 2434   score: 110.0  epsilon: 1.0    steps: 192  evaluation reward: 223.5\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5056: Policy loss: -0.000377. Value loss: 0.248148. Entropy: 0.942851.\n",
      "Iteration 5057: Policy loss: -0.005979. Value loss: 0.115134. Entropy: 0.962858.\n",
      "Iteration 5058: Policy loss: -0.010301. Value loss: 0.078265. Entropy: 0.958007.\n",
      "episode: 2435   score: 210.0  epsilon: 1.0    steps: 376  evaluation reward: 224.55\n",
      "episode: 2436   score: 285.0  epsilon: 1.0    steps: 408  evaluation reward: 225.3\n",
      "episode: 2437   score: 180.0  epsilon: 1.0    steps: 800  evaluation reward: 224.85\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5059: Policy loss: 0.001621. Value loss: 0.198529. Entropy: 0.954300.\n",
      "Iteration 5060: Policy loss: -0.004565. Value loss: 0.107001. Entropy: 0.943345.\n",
      "Iteration 5061: Policy loss: -0.002789. Value loss: 0.081409. Entropy: 0.960739.\n",
      "episode: 2438   score: 160.0  epsilon: 1.0    steps: 576  evaluation reward: 225.1\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5062: Policy loss: 0.000979. Value loss: 0.218073. Entropy: 0.979521.\n",
      "Iteration 5063: Policy loss: -0.003142. Value loss: 0.110636. Entropy: 0.950601.\n",
      "Iteration 5064: Policy loss: -0.011165. Value loss: 0.083992. Entropy: 0.984743.\n",
      "episode: 2439   score: 215.0  epsilon: 1.0    steps: 832  evaluation reward: 225.15\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5065: Policy loss: 0.001254. Value loss: 0.116386. Entropy: 0.948451.\n",
      "Iteration 5066: Policy loss: -0.003618. Value loss: 0.059604. Entropy: 0.980231.\n",
      "Iteration 5067: Policy loss: -0.011987. Value loss: 0.041013. Entropy: 0.962501.\n",
      "episode: 2440   score: 210.0  epsilon: 1.0    steps: 248  evaluation reward: 225.15\n",
      "episode: 2441   score: 210.0  epsilon: 1.0    steps: 264  evaluation reward: 225.75\n",
      "episode: 2442   score: 180.0  epsilon: 1.0    steps: 712  evaluation reward: 222.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5068: Policy loss: 0.005155. Value loss: 0.216336. Entropy: 1.048087.\n",
      "Iteration 5069: Policy loss: 0.000107. Value loss: 0.127403. Entropy: 1.055177.\n",
      "Iteration 5070: Policy loss: -0.007636. Value loss: 0.113916. Entropy: 1.050346.\n",
      "episode: 2443   score: 125.0  epsilon: 1.0    steps: 512  evaluation reward: 220.4\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5071: Policy loss: 0.003516. Value loss: 0.172053. Entropy: 1.021794.\n",
      "Iteration 5072: Policy loss: -0.003937. Value loss: 0.098719. Entropy: 1.039135.\n",
      "Iteration 5073: Policy loss: -0.005514. Value loss: 0.079200. Entropy: 1.017911.\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5074: Policy loss: -0.000075. Value loss: 0.170141. Entropy: 1.075956.\n",
      "Iteration 5075: Policy loss: -0.012022. Value loss: 0.104278. Entropy: 1.087101.\n",
      "Iteration 5076: Policy loss: -0.014965. Value loss: 0.081608. Entropy: 1.067583.\n",
      "episode: 2444   score: 285.0  epsilon: 1.0    steps: 952  evaluation reward: 221.45\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5077: Policy loss: 0.000591. Value loss: 0.215481. Entropy: 1.083265.\n",
      "Iteration 5078: Policy loss: -0.006098. Value loss: 0.082813. Entropy: 1.093273.\n",
      "Iteration 5079: Policy loss: -0.012951. Value loss: 0.073400. Entropy: 1.102744.\n",
      "episode: 2445   score: 180.0  epsilon: 1.0    steps: 488  evaluation reward: 221.05\n",
      "episode: 2446   score: 210.0  epsilon: 1.0    steps: 912  evaluation reward: 220.7\n",
      "episode: 2447   score: 345.0  epsilon: 1.0    steps: 936  evaluation reward: 221.9\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5080: Policy loss: -0.000103. Value loss: 0.417796. Entropy: 1.157345.\n",
      "Iteration 5081: Policy loss: -0.000717. Value loss: 0.194468. Entropy: 1.146681.\n",
      "Iteration 5082: Policy loss: -0.006119. Value loss: 0.106066. Entropy: 1.160402.\n",
      "episode: 2448   score: 210.0  epsilon: 1.0    steps: 520  evaluation reward: 222.2\n",
      "episode: 2449   score: 215.0  epsilon: 1.0    steps: 632  evaluation reward: 221.75\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5083: Policy loss: 0.000558. Value loss: 0.253451. Entropy: 1.047508.\n",
      "Iteration 5084: Policy loss: -0.001476. Value loss: 0.164647. Entropy: 1.041426.\n",
      "Iteration 5085: Policy loss: -0.005952. Value loss: 0.107484. Entropy: 1.054321.\n",
      "episode: 2450   score: 310.0  epsilon: 1.0    steps: 1008  evaluation reward: 223.05\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5086: Policy loss: 0.001745. Value loss: 0.430195. Entropy: 1.041262.\n",
      "Iteration 5087: Policy loss: -0.001021. Value loss: 0.175022. Entropy: 1.038151.\n",
      "Iteration 5088: Policy loss: -0.009395. Value loss: 0.139101. Entropy: 1.041498.\n",
      "now time :  2019-02-28 11:55:58.885768\n",
      "episode: 2451   score: 420.0  epsilon: 1.0    steps: 864  evaluation reward: 225.15\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5089: Policy loss: 0.000773. Value loss: 0.341159. Entropy: 0.972008.\n",
      "Iteration 5090: Policy loss: -0.008071. Value loss: 0.190638. Entropy: 0.993181.\n",
      "Iteration 5091: Policy loss: -0.013749. Value loss: 0.125526. Entropy: 0.992694.\n",
      "episode: 2452   score: 210.0  epsilon: 1.0    steps: 928  evaluation reward: 225.45\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5092: Policy loss: -0.002212. Value loss: 0.233119. Entropy: 1.151915.\n",
      "Iteration 5093: Policy loss: -0.003694. Value loss: 0.148005. Entropy: 1.148910.\n",
      "Iteration 5094: Policy loss: -0.010303. Value loss: 0.123326. Entropy: 1.130287.\n",
      "episode: 2453   score: 180.0  epsilon: 1.0    steps: 520  evaluation reward: 223.55\n",
      "episode: 2454   score: 225.0  epsilon: 1.0    steps: 840  evaluation reward: 222.95\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5095: Policy loss: 0.001372. Value loss: 0.222419. Entropy: 1.137871.\n",
      "Iteration 5096: Policy loss: 0.000160. Value loss: 0.110929. Entropy: 1.132538.\n",
      "Iteration 5097: Policy loss: -0.012613. Value loss: 0.072217. Entropy: 1.140092.\n",
      "episode: 2455   score: 225.0  epsilon: 1.0    steps: 224  evaluation reward: 223.85\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5098: Policy loss: 0.001897. Value loss: 0.301258. Entropy: 1.076000.\n",
      "Iteration 5099: Policy loss: -0.004154. Value loss: 0.146715. Entropy: 1.065155.\n",
      "Iteration 5100: Policy loss: -0.001176. Value loss: 0.105868. Entropy: 1.062032.\n",
      "episode: 2456   score: 210.0  epsilon: 1.0    steps: 920  evaluation reward: 223.05\n",
      "episode: 2457   score: 230.0  epsilon: 1.0    steps: 1000  evaluation reward: 223.85\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5101: Policy loss: 0.004463. Value loss: 0.311324. Entropy: 1.082843.\n",
      "Iteration 5102: Policy loss: -0.003523. Value loss: 0.156084. Entropy: 1.094907.\n",
      "Iteration 5103: Policy loss: -0.008943. Value loss: 0.115820. Entropy: 1.087188.\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5104: Policy loss: 0.001674. Value loss: 0.302252. Entropy: 1.040573.\n",
      "Iteration 5105: Policy loss: -0.009813. Value loss: 0.167755. Entropy: 1.038210.\n",
      "Iteration 5106: Policy loss: -0.008128. Value loss: 0.115577. Entropy: 1.055505.\n",
      "episode: 2458   score: 210.0  epsilon: 1.0    steps: 224  evaluation reward: 222.75\n",
      "episode: 2459   score: 210.0  epsilon: 1.0    steps: 344  evaluation reward: 220.1\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5107: Policy loss: 0.006528. Value loss: 0.423199. Entropy: 1.123526.\n",
      "Iteration 5108: Policy loss: -0.003854. Value loss: 0.203205. Entropy: 1.123791.\n",
      "Iteration 5109: Policy loss: -0.009042. Value loss: 0.162337. Entropy: 1.125103.\n",
      "episode: 2460   score: 135.0  epsilon: 1.0    steps: 144  evaluation reward: 220.45\n",
      "episode: 2461   score: 210.0  epsilon: 1.0    steps: 624  evaluation reward: 221.35\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5110: Policy loss: 0.001303. Value loss: 0.263273. Entropy: 0.944380.\n",
      "Iteration 5111: Policy loss: -0.003309. Value loss: 0.150054. Entropy: 0.947677.\n",
      "Iteration 5112: Policy loss: -0.011626. Value loss: 0.102113. Entropy: 0.950127.\n",
      "episode: 2462   score: 390.0  epsilon: 1.0    steps: 544  evaluation reward: 223.1\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5113: Policy loss: 0.000302. Value loss: 0.222783. Entropy: 1.124804.\n",
      "Iteration 5114: Policy loss: -0.003689. Value loss: 0.107255. Entropy: 1.116014.\n",
      "Iteration 5115: Policy loss: -0.008554. Value loss: 0.074459. Entropy: 1.118395.\n",
      "episode: 2463   score: 285.0  epsilon: 1.0    steps: 56  evaluation reward: 224.75\n",
      "episode: 2464   score: 135.0  epsilon: 1.0    steps: 264  evaluation reward: 224.85\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5116: Policy loss: -0.001163. Value loss: 0.143644. Entropy: 1.021066.\n",
      "Iteration 5117: Policy loss: -0.006594. Value loss: 0.081885. Entropy: 1.020795.\n",
      "Iteration 5118: Policy loss: -0.013551. Value loss: 0.065166. Entropy: 1.016997.\n",
      "episode: 2465   score: 215.0  epsilon: 1.0    steps: 592  evaluation reward: 225.45\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5119: Policy loss: 0.000188. Value loss: 0.203843. Entropy: 1.086098.\n",
      "Iteration 5120: Policy loss: -0.004031. Value loss: 0.128443. Entropy: 1.085844.\n",
      "Iteration 5121: Policy loss: -0.012623. Value loss: 0.117207. Entropy: 1.091706.\n",
      "episode: 2466   score: 260.0  epsilon: 1.0    steps: 424  evaluation reward: 225.7\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5122: Policy loss: -0.002500. Value loss: 0.478023. Entropy: 1.004811.\n",
      "Iteration 5123: Policy loss: -0.000658. Value loss: 0.274289. Entropy: 1.003543.\n",
      "Iteration 5124: Policy loss: -0.004722. Value loss: 0.181873. Entropy: 1.007168.\n",
      "episode: 2467   score: 210.0  epsilon: 1.0    steps: 592  evaluation reward: 224.7\n",
      "episode: 2468   score: 155.0  epsilon: 1.0    steps: 688  evaluation reward: 224.15\n",
      "episode: 2469   score: 210.0  epsilon: 1.0    steps: 928  evaluation reward: 224.7\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5125: Policy loss: -0.002006. Value loss: 0.298045. Entropy: 1.184551.\n",
      "Iteration 5126: Policy loss: -0.005987. Value loss: 0.140347. Entropy: 1.188548.\n",
      "Iteration 5127: Policy loss: -0.007943. Value loss: 0.101276. Entropy: 1.185192.\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5128: Policy loss: -0.002681. Value loss: 0.225722. Entropy: 0.961346.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5129: Policy loss: -0.006368. Value loss: 0.108818. Entropy: 0.929003.\n",
      "Iteration 5130: Policy loss: -0.012226. Value loss: 0.078963. Entropy: 0.955459.\n",
      "episode: 2470   score: 555.0  epsilon: 1.0    steps: 832  evaluation reward: 228.45\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5131: Policy loss: 0.005272. Value loss: 0.389115. Entropy: 1.273751.\n",
      "Iteration 5132: Policy loss: -0.000241. Value loss: 0.191859. Entropy: 1.266476.\n",
      "Iteration 5133: Policy loss: -0.007315. Value loss: 0.125458. Entropy: 1.268777.\n",
      "episode: 2471   score: 135.0  epsilon: 1.0    steps: 224  evaluation reward: 227.35\n",
      "episode: 2472   score: 215.0  epsilon: 1.0    steps: 536  evaluation reward: 227.55\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5134: Policy loss: 0.001845. Value loss: 0.207579. Entropy: 0.996889.\n",
      "Iteration 5135: Policy loss: -0.002184. Value loss: 0.119898. Entropy: 1.000967.\n",
      "Iteration 5136: Policy loss: -0.006789. Value loss: 0.080777. Entropy: 1.006735.\n",
      "episode: 2473   score: 210.0  epsilon: 1.0    steps: 112  evaluation reward: 228.55\n",
      "episode: 2474   score: 210.0  epsilon: 1.0    steps: 736  evaluation reward: 228.85\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5137: Policy loss: 0.001317. Value loss: 0.296124. Entropy: 0.948543.\n",
      "Iteration 5138: Policy loss: -0.001521. Value loss: 0.140083. Entropy: 0.999377.\n",
      "Iteration 5139: Policy loss: -0.010947. Value loss: 0.095407. Entropy: 0.975937.\n",
      "episode: 2475   score: 210.0  epsilon: 1.0    steps: 584  evaluation reward: 229.75\n",
      "episode: 2476   score: 180.0  epsilon: 1.0    steps: 712  evaluation reward: 229.25\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5140: Policy loss: -0.000296. Value loss: 0.407088. Entropy: 1.116864.\n",
      "Iteration 5141: Policy loss: -0.001981. Value loss: 0.187484. Entropy: 1.124349.\n",
      "Iteration 5142: Policy loss: -0.010608. Value loss: 0.157324. Entropy: 1.120538.\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5143: Policy loss: 0.002199. Value loss: 0.287698. Entropy: 0.911026.\n",
      "Iteration 5144: Policy loss: -0.005072. Value loss: 0.142589. Entropy: 0.926087.\n",
      "Iteration 5145: Policy loss: -0.013778. Value loss: 0.092993. Entropy: 0.935294.\n",
      "episode: 2477   score: 180.0  epsilon: 1.0    steps: 360  evaluation reward: 228.35\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5146: Policy loss: 0.002646. Value loss: 0.243306. Entropy: 1.216787.\n",
      "Iteration 5147: Policy loss: -0.008283. Value loss: 0.117528. Entropy: 1.214843.\n",
      "Iteration 5148: Policy loss: -0.010350. Value loss: 0.089702. Entropy: 1.217039.\n",
      "episode: 2478   score: 310.0  epsilon: 1.0    steps: 312  evaluation reward: 228.5\n",
      "episode: 2479   score: 155.0  epsilon: 1.0    steps: 528  evaluation reward: 229.25\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5149: Policy loss: 0.002830. Value loss: 0.143981. Entropy: 1.038172.\n",
      "Iteration 5150: Policy loss: -0.000626. Value loss: 0.086715. Entropy: 1.032233.\n",
      "Iteration 5151: Policy loss: -0.007359. Value loss: 0.055377. Entropy: 1.043743.\n",
      "episode: 2480   score: 215.0  epsilon: 1.0    steps: 40  evaluation reward: 229.25\n",
      "episode: 2481   score: 210.0  epsilon: 1.0    steps: 160  evaluation reward: 228.35\n",
      "episode: 2482   score: 100.0  epsilon: 1.0    steps: 768  evaluation reward: 227.1\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5152: Policy loss: 0.002044. Value loss: 0.678070. Entropy: 1.027473.\n",
      "Iteration 5153: Policy loss: 0.001966. Value loss: 0.512100. Entropy: 1.049033.\n",
      "Iteration 5154: Policy loss: -0.001805. Value loss: 0.323799. Entropy: 1.025456.\n",
      "episode: 2483   score: 460.0  epsilon: 1.0    steps: 896  evaluation reward: 230.25\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5155: Policy loss: 0.000215. Value loss: 0.249250. Entropy: 0.948052.\n",
      "Iteration 5156: Policy loss: -0.006424. Value loss: 0.114527. Entropy: 0.929593.\n",
      "Iteration 5157: Policy loss: -0.008362. Value loss: 0.100324. Entropy: 0.928205.\n",
      "episode: 2484   score: 210.0  epsilon: 1.0    steps: 56  evaluation reward: 231.3\n",
      "episode: 2485   score: 210.0  epsilon: 1.0    steps: 296  evaluation reward: 231.3\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5158: Policy loss: 0.002453. Value loss: 0.703198. Entropy: 1.086378.\n",
      "Iteration 5159: Policy loss: 0.004315. Value loss: 0.315001. Entropy: 1.069703.\n",
      "Iteration 5160: Policy loss: 0.002242. Value loss: 0.186930. Entropy: 1.078870.\n",
      "episode: 2486   score: 155.0  epsilon: 1.0    steps: 944  evaluation reward: 231.8\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5161: Policy loss: 0.003970. Value loss: 0.318350. Entropy: 1.021591.\n",
      "Iteration 5162: Policy loss: -0.005887. Value loss: 0.171870. Entropy: 1.023704.\n",
      "Iteration 5163: Policy loss: -0.005702. Value loss: 0.134020. Entropy: 1.030372.\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5164: Policy loss: 0.004098. Value loss: 0.366083. Entropy: 1.150030.\n",
      "Iteration 5165: Policy loss: -0.003397. Value loss: 0.203459. Entropy: 1.144451.\n",
      "Iteration 5166: Policy loss: -0.005701. Value loss: 0.142770. Entropy: 1.140850.\n",
      "episode: 2487   score: 100.0  epsilon: 1.0    steps: 344  evaluation reward: 230.4\n",
      "episode: 2488   score: 210.0  epsilon: 1.0    steps: 448  evaluation reward: 230.95\n",
      "episode: 2489   score: 205.0  epsilon: 1.0    steps: 480  evaluation reward: 231.2\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5167: Policy loss: 0.001472. Value loss: 0.361484. Entropy: 1.196554.\n",
      "Iteration 5168: Policy loss: -0.001882. Value loss: 0.169587. Entropy: 1.205149.\n",
      "Iteration 5169: Policy loss: -0.009548. Value loss: 0.163654. Entropy: 1.207807.\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5170: Policy loss: -0.000673. Value loss: 0.255113. Entropy: 0.890776.\n",
      "Iteration 5171: Policy loss: -0.003784. Value loss: 0.148792. Entropy: 0.896050.\n",
      "Iteration 5172: Policy loss: -0.015666. Value loss: 0.121417. Entropy: 0.914564.\n",
      "episode: 2490   score: 240.0  epsilon: 1.0    steps: 120  evaluation reward: 231.5\n",
      "episode: 2491   score: 530.0  epsilon: 1.0    steps: 768  evaluation reward: 234.25\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5173: Policy loss: 0.000921. Value loss: 0.338974. Entropy: 1.121097.\n",
      "Iteration 5174: Policy loss: -0.002141. Value loss: 0.195585. Entropy: 1.140197.\n",
      "Iteration 5175: Policy loss: -0.009420. Value loss: 0.129343. Entropy: 1.128412.\n",
      "episode: 2492   score: 210.0  epsilon: 1.0    steps: 736  evaluation reward: 234.25\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5176: Policy loss: 0.003071. Value loss: 0.176653. Entropy: 1.011908.\n",
      "Iteration 5177: Policy loss: -0.000664. Value loss: 0.086591. Entropy: 1.031730.\n",
      "Iteration 5178: Policy loss: -0.008711. Value loss: 0.065948. Entropy: 1.007722.\n",
      "episode: 2493   score: 210.0  epsilon: 1.0    steps: 192  evaluation reward: 234.8\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5179: Policy loss: 0.002785. Value loss: 0.200445. Entropy: 1.070309.\n",
      "Iteration 5180: Policy loss: -0.006245. Value loss: 0.098733. Entropy: 1.089909.\n",
      "Iteration 5181: Policy loss: -0.009303. Value loss: 0.071763. Entropy: 1.083342.\n",
      "episode: 2494   score: 380.0  epsilon: 1.0    steps: 912  evaluation reward: 236.8\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5182: Policy loss: 0.003583. Value loss: 0.855766. Entropy: 1.113296.\n",
      "Iteration 5183: Policy loss: 0.000539. Value loss: 0.506665. Entropy: 1.129549.\n",
      "Iteration 5184: Policy loss: 0.002212. Value loss: 0.293203. Entropy: 1.124752.\n",
      "episode: 2495   score: 215.0  epsilon: 1.0    steps: 952  evaluation reward: 236.15\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5185: Policy loss: 0.000180. Value loss: 0.393583. Entropy: 1.171714.\n",
      "Iteration 5186: Policy loss: 0.000041. Value loss: 0.198914. Entropy: 1.157563.\n",
      "Iteration 5187: Policy loss: -0.008336. Value loss: 0.121751. Entropy: 1.180735.\n",
      "episode: 2496   score: 465.0  epsilon: 1.0    steps: 528  evaluation reward: 239.25\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5188: Policy loss: 0.007568. Value loss: 0.310962. Entropy: 1.166313.\n",
      "Iteration 5189: Policy loss: -0.002350. Value loss: 0.157057. Entropy: 1.171509.\n",
      "Iteration 5190: Policy loss: -0.008455. Value loss: 0.115566. Entropy: 1.157863.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2497   score: 260.0  epsilon: 1.0    steps: 1008  evaluation reward: 239.75\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5191: Policy loss: 0.004655. Value loss: 0.310764. Entropy: 1.142440.\n",
      "Iteration 5192: Policy loss: 0.001168. Value loss: 0.142649. Entropy: 1.141174.\n",
      "Iteration 5193: Policy loss: -0.010287. Value loss: 0.106750. Entropy: 1.143909.\n",
      "episode: 2498   score: 440.0  epsilon: 1.0    steps: 152  evaluation reward: 243.5\n",
      "episode: 2499   score: 160.0  epsilon: 1.0    steps: 152  evaluation reward: 239.5\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5194: Policy loss: 0.001669. Value loss: 0.767335. Entropy: 1.167140.\n",
      "Iteration 5195: Policy loss: 0.007241. Value loss: 0.359048. Entropy: 1.182105.\n",
      "Iteration 5196: Policy loss: -0.002034. Value loss: 0.321369. Entropy: 1.155654.\n",
      "episode: 2500   score: 410.0  epsilon: 1.0    steps: 608  evaluation reward: 241.45\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5197: Policy loss: -0.001075. Value loss: 0.316480. Entropy: 1.017540.\n",
      "Iteration 5198: Policy loss: 0.002062. Value loss: 0.182178. Entropy: 1.039975.\n",
      "Iteration 5199: Policy loss: -0.002441. Value loss: 0.149205. Entropy: 1.030619.\n",
      "now time :  2019-02-28 11:57:19.187725\n",
      "episode: 2501   score: 315.0  epsilon: 1.0    steps: 784  evaluation reward: 242.35\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5200: Policy loss: -0.002490. Value loss: 0.236420. Entropy: 1.064584.\n",
      "Iteration 5201: Policy loss: -0.001574. Value loss: 0.118107. Entropy: 1.046159.\n",
      "Iteration 5202: Policy loss: -0.008018. Value loss: 0.083216. Entropy: 1.064458.\n",
      "episode: 2502   score: 65.0  epsilon: 1.0    steps: 120  evaluation reward: 241.95\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5203: Policy loss: 0.001778. Value loss: 0.560876. Entropy: 1.141831.\n",
      "Iteration 5204: Policy loss: -0.004653. Value loss: 0.297589. Entropy: 1.131839.\n",
      "Iteration 5205: Policy loss: -0.008644. Value loss: 0.222694. Entropy: 1.143103.\n",
      "episode: 2503   score: 330.0  epsilon: 1.0    steps: 248  evaluation reward: 240.25\n",
      "episode: 2504   score: 215.0  epsilon: 1.0    steps: 568  evaluation reward: 240.3\n",
      "episode: 2505   score: 210.0  epsilon: 1.0    steps: 880  evaluation reward: 239.05\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5206: Policy loss: 0.004510. Value loss: 0.731627. Entropy: 1.163545.\n",
      "Iteration 5207: Policy loss: 0.000022. Value loss: 0.369763. Entropy: 1.158187.\n",
      "Iteration 5208: Policy loss: -0.005941. Value loss: 0.252627. Entropy: 1.178523.\n",
      "episode: 2506   score: 210.0  epsilon: 1.0    steps: 640  evaluation reward: 239.35\n",
      "episode: 2507   score: 115.0  epsilon: 1.0    steps: 768  evaluation reward: 239.9\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5209: Policy loss: 0.003781. Value loss: 0.365473. Entropy: 1.103442.\n",
      "Iteration 5210: Policy loss: -0.001243. Value loss: 0.170838. Entropy: 1.108064.\n",
      "Iteration 5211: Policy loss: -0.005887. Value loss: 0.132859. Entropy: 1.108858.\n",
      "episode: 2508   score: 245.0  epsilon: 1.0    steps: 112  evaluation reward: 239.55\n",
      "episode: 2509   score: 80.0  epsilon: 1.0    steps: 160  evaluation reward: 238.55\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5212: Policy loss: 0.003304. Value loss: 0.542630. Entropy: 1.091767.\n",
      "Iteration 5213: Policy loss: 0.000709. Value loss: 0.351915. Entropy: 1.089942.\n",
      "Iteration 5214: Policy loss: -0.003777. Value loss: 0.288074. Entropy: 1.084340.\n",
      "episode: 2510   score: 155.0  epsilon: 1.0    steps: 256  evaluation reward: 237.95\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5215: Policy loss: 0.000934. Value loss: 0.372265. Entropy: 1.033322.\n",
      "Iteration 5216: Policy loss: -0.001335. Value loss: 0.199858. Entropy: 1.042684.\n",
      "Iteration 5217: Policy loss: -0.004413. Value loss: 0.151568. Entropy: 1.040724.\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5218: Policy loss: 0.003785. Value loss: 0.366465. Entropy: 1.088044.\n",
      "Iteration 5219: Policy loss: -0.002607. Value loss: 0.204165. Entropy: 1.083970.\n",
      "Iteration 5220: Policy loss: -0.008157. Value loss: 0.141986. Entropy: 1.085686.\n",
      "episode: 2511   score: 65.0  epsilon: 1.0    steps: 64  evaluation reward: 233.85\n",
      "episode: 2512   score: 180.0  epsilon: 1.0    steps: 528  evaluation reward: 233.5\n",
      "episode: 2513   score: 190.0  epsilon: 1.0    steps: 736  evaluation reward: 234.75\n",
      "episode: 2514   score: 540.0  epsilon: 1.0    steps: 1024  evaluation reward: 236.5\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5221: Policy loss: 0.001860. Value loss: 0.311604. Entropy: 1.139644.\n",
      "Iteration 5222: Policy loss: -0.007081. Value loss: 0.158091. Entropy: 1.144293.\n",
      "Iteration 5223: Policy loss: -0.008993. Value loss: 0.120778. Entropy: 1.144777.\n",
      "episode: 2515   score: 155.0  epsilon: 1.0    steps: 592  evaluation reward: 235.95\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5224: Policy loss: 0.001008. Value loss: 0.222025. Entropy: 0.989361.\n",
      "Iteration 5225: Policy loss: -0.006637. Value loss: 0.124778. Entropy: 1.020697.\n",
      "Iteration 5226: Policy loss: -0.010291. Value loss: 0.097543. Entropy: 0.998145.\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5227: Policy loss: 0.000212. Value loss: 0.206717. Entropy: 1.070481.\n",
      "Iteration 5228: Policy loss: -0.005312. Value loss: 0.078030. Entropy: 1.063414.\n",
      "Iteration 5229: Policy loss: -0.010494. Value loss: 0.067623. Entropy: 1.081125.\n",
      "episode: 2516   score: 210.0  epsilon: 1.0    steps: 720  evaluation reward: 235.95\n",
      "episode: 2517   score: 250.0  epsilon: 1.0    steps: 792  evaluation reward: 236.35\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5230: Policy loss: 0.001183. Value loss: 0.266730. Entropy: 1.005233.\n",
      "Iteration 5231: Policy loss: -0.004930. Value loss: 0.156389. Entropy: 0.986900.\n",
      "Iteration 5232: Policy loss: -0.009804. Value loss: 0.115604. Entropy: 0.976256.\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5233: Policy loss: 0.001869. Value loss: 0.609294. Entropy: 1.146772.\n",
      "Iteration 5234: Policy loss: 0.007038. Value loss: 0.310222. Entropy: 1.151911.\n",
      "Iteration 5235: Policy loss: -0.001395. Value loss: 0.277055. Entropy: 1.157724.\n",
      "episode: 2518   score: 315.0  epsilon: 1.0    steps: 152  evaluation reward: 236.35\n",
      "episode: 2519   score: 165.0  epsilon: 1.0    steps: 656  evaluation reward: 234.85\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5236: Policy loss: 0.006302. Value loss: 0.615640. Entropy: 1.215189.\n",
      "Iteration 5237: Policy loss: 0.004064. Value loss: 0.347269. Entropy: 1.222982.\n",
      "Iteration 5238: Policy loss: -0.004508. Value loss: 0.263624. Entropy: 1.211203.\n",
      "episode: 2520   score: 425.0  epsilon: 1.0    steps: 1000  evaluation reward: 236.7\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5239: Policy loss: 0.002161. Value loss: 0.694580. Entropy: 1.090443.\n",
      "Iteration 5240: Policy loss: 0.001472. Value loss: 0.338174. Entropy: 1.114248.\n",
      "Iteration 5241: Policy loss: -0.003829. Value loss: 0.243597. Entropy: 1.076112.\n",
      "episode: 2521   score: 180.0  epsilon: 1.0    steps: 544  evaluation reward: 236.35\n",
      "episode: 2522   score: 215.0  epsilon: 1.0    steps: 776  evaluation reward: 236.4\n",
      "episode: 2523   score: 460.0  epsilon: 1.0    steps: 992  evaluation reward: 237.95\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5242: Policy loss: -0.000453. Value loss: 0.429571. Entropy: 1.171598.\n",
      "Iteration 5243: Policy loss: -0.003070. Value loss: 0.215543. Entropy: 1.167039.\n",
      "Iteration 5244: Policy loss: -0.009881. Value loss: 0.154056. Entropy: 1.175688.\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5245: Policy loss: -0.000705. Value loss: 0.406902. Entropy: 1.051891.\n",
      "Iteration 5246: Policy loss: -0.004284. Value loss: 0.208304. Entropy: 1.057810.\n",
      "Iteration 5247: Policy loss: -0.007704. Value loss: 0.157160. Entropy: 1.054254.\n",
      "episode: 2524   score: 180.0  epsilon: 1.0    steps: 640  evaluation reward: 236.45\n",
      "episode: 2525   score: 80.0  epsilon: 1.0    steps: 864  evaluation reward: 234.85\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5248: Policy loss: 0.004280. Value loss: 0.587238. Entropy: 1.217229.\n",
      "Iteration 5249: Policy loss: 0.000069. Value loss: 0.370217. Entropy: 1.213918.\n",
      "Iteration 5250: Policy loss: -0.001989. Value loss: 0.301145. Entropy: 1.199988.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2526   score: 210.0  epsilon: 1.0    steps: 880  evaluation reward: 234.85\n",
      "episode: 2527   score: 60.0  epsilon: 1.0    steps: 1016  evaluation reward: 233.25\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5251: Policy loss: 0.008371. Value loss: 0.566654. Entropy: 1.077175.\n",
      "Iteration 5252: Policy loss: 0.002120. Value loss: 0.242826. Entropy: 1.082974.\n",
      "Iteration 5253: Policy loss: -0.008013. Value loss: 0.174058. Entropy: 1.071349.\n",
      "episode: 2528   score: 280.0  epsilon: 1.0    steps: 520  evaluation reward: 233.8\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5254: Policy loss: -0.002721. Value loss: 0.297507. Entropy: 1.172815.\n",
      "Iteration 5255: Policy loss: -0.001332. Value loss: 0.199302. Entropy: 1.163072.\n",
      "Iteration 5256: Policy loss: -0.008685. Value loss: 0.114042. Entropy: 1.173658.\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5257: Policy loss: 0.002142. Value loss: 0.201231. Entropy: 1.109357.\n",
      "Iteration 5258: Policy loss: -0.006282. Value loss: 0.087785. Entropy: 1.100416.\n",
      "Iteration 5259: Policy loss: -0.005662. Value loss: 0.071651. Entropy: 1.124782.\n",
      "episode: 2529   score: 135.0  epsilon: 1.0    steps: 8  evaluation reward: 233.05\n",
      "episode: 2530   score: 220.0  epsilon: 1.0    steps: 640  evaluation reward: 230.55\n",
      "episode: 2531   score: 105.0  epsilon: 1.0    steps: 944  evaluation reward: 229.0\n",
      "episode: 2532   score: 695.0  epsilon: 1.0    steps: 976  evaluation reward: 233.85\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5260: Policy loss: -0.000149. Value loss: 0.402382. Entropy: 1.224287.\n",
      "Iteration 5261: Policy loss: -0.005220. Value loss: 0.195239. Entropy: 1.217388.\n",
      "Iteration 5262: Policy loss: -0.011976. Value loss: 0.162510. Entropy: 1.229596.\n",
      "episode: 2533   score: 105.0  epsilon: 1.0    steps: 344  evaluation reward: 232.8\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5263: Policy loss: 0.002894. Value loss: 0.234901. Entropy: 1.042143.\n",
      "Iteration 5264: Policy loss: -0.000615. Value loss: 0.120251. Entropy: 1.068301.\n",
      "Iteration 5265: Policy loss: -0.008029. Value loss: 0.090075. Entropy: 1.055981.\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5266: Policy loss: 0.004750. Value loss: 0.349749. Entropy: 1.124518.\n",
      "Iteration 5267: Policy loss: -0.002722. Value loss: 0.150532. Entropy: 1.138683.\n",
      "Iteration 5268: Policy loss: -0.006463. Value loss: 0.133489. Entropy: 1.134249.\n",
      "episode: 2534   score: 225.0  epsilon: 1.0    steps: 88  evaluation reward: 233.95\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5269: Policy loss: -0.000033. Value loss: 0.113580. Entropy: 1.039562.\n",
      "Iteration 5270: Policy loss: -0.011926. Value loss: 0.061298. Entropy: 1.062495.\n",
      "Iteration 5271: Policy loss: -0.012685. Value loss: 0.051136. Entropy: 1.055023.\n",
      "episode: 2535   score: 240.0  epsilon: 1.0    steps: 784  evaluation reward: 234.25\n",
      "episode: 2536   score: 315.0  epsilon: 1.0    steps: 1008  evaluation reward: 234.55\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5272: Policy loss: -0.001944. Value loss: 0.179841. Entropy: 1.199497.\n",
      "Iteration 5273: Policy loss: -0.005844. Value loss: 0.082167. Entropy: 1.189473.\n",
      "Iteration 5274: Policy loss: -0.009607. Value loss: 0.087919. Entropy: 1.187917.\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5275: Policy loss: -0.002265. Value loss: 0.147323. Entropy: 1.119424.\n",
      "Iteration 5276: Policy loss: -0.006697. Value loss: 0.089877. Entropy: 1.116789.\n",
      "Iteration 5277: Policy loss: -0.010364. Value loss: 0.069767. Entropy: 1.119905.\n",
      "episode: 2537   score: 410.0  epsilon: 1.0    steps: 168  evaluation reward: 236.85\n",
      "episode: 2538   score: 210.0  epsilon: 1.0    steps: 176  evaluation reward: 237.35\n",
      "episode: 2539   score: 210.0  epsilon: 1.0    steps: 192  evaluation reward: 237.3\n",
      "episode: 2540   score: 155.0  epsilon: 1.0    steps: 248  evaluation reward: 236.75\n",
      "episode: 2541   score: 210.0  epsilon: 1.0    steps: 392  evaluation reward: 236.75\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5278: Policy loss: 0.001286. Value loss: 0.212346. Entropy: 1.111538.\n",
      "Iteration 5279: Policy loss: -0.005584. Value loss: 0.142003. Entropy: 1.113407.\n",
      "Iteration 5280: Policy loss: -0.007040. Value loss: 0.129680. Entropy: 1.121578.\n",
      "episode: 2542   score: 105.0  epsilon: 1.0    steps: 984  evaluation reward: 236.0\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5281: Policy loss: 0.004076. Value loss: 0.119755. Entropy: 0.883116.\n",
      "Iteration 5282: Policy loss: -0.003324. Value loss: 0.088467. Entropy: 0.883883.\n",
      "Iteration 5283: Policy loss: -0.006766. Value loss: 0.062336. Entropy: 0.880726.\n",
      "episode: 2543   score: 210.0  epsilon: 1.0    steps: 624  evaluation reward: 236.85\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5284: Policy loss: 0.000725. Value loss: 0.125697. Entropy: 1.119153.\n",
      "Iteration 5285: Policy loss: -0.004088. Value loss: 0.067392. Entropy: 1.089337.\n",
      "Iteration 5286: Policy loss: -0.007877. Value loss: 0.051818. Entropy: 1.119540.\n",
      "episode: 2544   score: 80.0  epsilon: 1.0    steps: 232  evaluation reward: 234.8\n",
      "episode: 2545   score: 105.0  epsilon: 1.0    steps: 392  evaluation reward: 234.05\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5287: Policy loss: 0.003602. Value loss: 0.159613. Entropy: 1.060304.\n",
      "Iteration 5288: Policy loss: -0.009061. Value loss: 0.096753. Entropy: 1.081805.\n",
      "Iteration 5289: Policy loss: -0.009890. Value loss: 0.059795. Entropy: 1.057455.\n",
      "episode: 2546   score: 210.0  epsilon: 1.0    steps: 392  evaluation reward: 234.05\n",
      "episode: 2547   score: 215.0  epsilon: 1.0    steps: 560  evaluation reward: 232.75\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5290: Policy loss: 0.000727. Value loss: 0.281064. Entropy: 1.169242.\n",
      "Iteration 5291: Policy loss: -0.003464. Value loss: 0.132201. Entropy: 1.169856.\n",
      "Iteration 5292: Policy loss: -0.011698. Value loss: 0.091527. Entropy: 1.166038.\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5293: Policy loss: 0.002405. Value loss: 0.214937. Entropy: 0.945106.\n",
      "Iteration 5294: Policy loss: -0.001888. Value loss: 0.089066. Entropy: 0.925396.\n",
      "Iteration 5295: Policy loss: -0.008115. Value loss: 0.065594. Entropy: 0.930684.\n",
      "episode: 2548   score: 210.0  epsilon: 1.0    steps: 72  evaluation reward: 232.75\n",
      "episode: 2549   score: 255.0  epsilon: 1.0    steps: 640  evaluation reward: 233.15\n",
      "episode: 2550   score: 240.0  epsilon: 1.0    steps: 768  evaluation reward: 232.45\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5296: Policy loss: 0.000692. Value loss: 0.118200. Entropy: 1.109231.\n",
      "Iteration 5297: Policy loss: -0.004946. Value loss: 0.066884. Entropy: 1.107094.\n",
      "Iteration 5298: Policy loss: -0.009668. Value loss: 0.055774. Entropy: 1.107719.\n",
      "now time :  2019-02-28 11:58:30.021987\n",
      "episode: 2551   score: 180.0  epsilon: 1.0    steps: 24  evaluation reward: 230.05\n",
      "episode: 2552   score: 210.0  epsilon: 1.0    steps: 728  evaluation reward: 230.05\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5299: Policy loss: 0.002701. Value loss: 0.180700. Entropy: 0.986439.\n",
      "Iteration 5300: Policy loss: -0.002814. Value loss: 0.105107. Entropy: 1.016234.\n",
      "Iteration 5301: Policy loss: -0.008662. Value loss: 0.079246. Entropy: 0.981033.\n",
      "episode: 2553   score: 210.0  epsilon: 1.0    steps: 760  evaluation reward: 230.35\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5302: Policy loss: 0.001858. Value loss: 0.142382. Entropy: 1.031986.\n",
      "Iteration 5303: Policy loss: -0.003020. Value loss: 0.089695. Entropy: 1.061756.\n",
      "Iteration 5304: Policy loss: -0.006166. Value loss: 0.075656. Entropy: 1.059646.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5305: Policy loss: 0.001369. Value loss: 0.161061. Entropy: 1.105450.\n",
      "Iteration 5306: Policy loss: -0.003400. Value loss: 0.072499. Entropy: 1.091510.\n",
      "Iteration 5307: Policy loss: -0.007899. Value loss: 0.060877. Entropy: 1.112895.\n",
      "episode: 2554   score: 225.0  epsilon: 1.0    steps: 72  evaluation reward: 230.35\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5308: Policy loss: 0.002991. Value loss: 0.103841. Entropy: 1.116261.\n",
      "Iteration 5309: Policy loss: -0.001885. Value loss: 0.050322. Entropy: 1.105812.\n",
      "Iteration 5310: Policy loss: -0.011723. Value loss: 0.040216. Entropy: 1.112076.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2555   score: 240.0  epsilon: 1.0    steps: 200  evaluation reward: 230.5\n",
      "episode: 2556   score: 210.0  epsilon: 1.0    steps: 584  evaluation reward: 230.5\n",
      "episode: 2557   score: 80.0  epsilon: 1.0    steps: 680  evaluation reward: 229.0\n",
      "episode: 2558   score: 210.0  epsilon: 1.0    steps: 824  evaluation reward: 229.0\n",
      "episode: 2559   score: 215.0  epsilon: 1.0    steps: 912  evaluation reward: 229.05\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5311: Policy loss: 0.001877. Value loss: 0.166179. Entropy: 1.226921.\n",
      "Iteration 5312: Policy loss: -0.005874. Value loss: 0.103841. Entropy: 1.232088.\n",
      "Iteration 5313: Policy loss: -0.014658. Value loss: 0.101684. Entropy: 1.226968.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5314: Policy loss: 0.002428. Value loss: 0.259768. Entropy: 0.914609.\n",
      "Iteration 5315: Policy loss: -0.003615. Value loss: 0.154687. Entropy: 0.932195.\n",
      "Iteration 5316: Policy loss: -0.007159. Value loss: 0.103951. Entropy: 0.936207.\n",
      "episode: 2560   score: 210.0  epsilon: 1.0    steps: 136  evaluation reward: 229.8\n",
      "episode: 2561   score: 260.0  epsilon: 1.0    steps: 256  evaluation reward: 230.3\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5317: Policy loss: 0.004177. Value loss: 0.224419. Entropy: 1.139015.\n",
      "Iteration 5318: Policy loss: -0.004840. Value loss: 0.105452. Entropy: 1.143600.\n",
      "Iteration 5319: Policy loss: -0.001976. Value loss: 0.091542. Entropy: 1.145864.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5320: Policy loss: -0.000341. Value loss: 0.095656. Entropy: 0.959753.\n",
      "Iteration 5321: Policy loss: -0.007547. Value loss: 0.064184. Entropy: 0.965720.\n",
      "Iteration 5322: Policy loss: -0.011186. Value loss: 0.049374. Entropy: 0.970485.\n",
      "episode: 2562   score: 215.0  epsilon: 1.0    steps: 64  evaluation reward: 228.55\n",
      "episode: 2563   score: 65.0  epsilon: 1.0    steps: 72  evaluation reward: 226.35\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5323: Policy loss: 0.004639. Value loss: 0.124178. Entropy: 1.215065.\n",
      "Iteration 5324: Policy loss: -0.001763. Value loss: 0.065111. Entropy: 1.196901.\n",
      "Iteration 5325: Policy loss: -0.008643. Value loss: 0.050399. Entropy: 1.214746.\n",
      "episode: 2564   score: 210.0  epsilon: 1.0    steps: 944  evaluation reward: 227.1\n",
      "episode: 2565   score: 210.0  epsilon: 1.0    steps: 968  evaluation reward: 227.05\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5326: Policy loss: 0.000101. Value loss: 0.125713. Entropy: 1.117247.\n",
      "Iteration 5327: Policy loss: -0.009292. Value loss: 0.066266. Entropy: 1.120426.\n",
      "Iteration 5328: Policy loss: -0.006659. Value loss: 0.056409. Entropy: 1.127736.\n",
      "episode: 2566   score: 210.0  epsilon: 1.0    steps: 432  evaluation reward: 226.55\n",
      "episode: 2567   score: 215.0  epsilon: 1.0    steps: 576  evaluation reward: 226.6\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5329: Policy loss: -0.001269. Value loss: 0.185659. Entropy: 1.190198.\n",
      "Iteration 5330: Policy loss: -0.007403. Value loss: 0.077982. Entropy: 1.186134.\n",
      "Iteration 5331: Policy loss: -0.010706. Value loss: 0.066412. Entropy: 1.197752.\n",
      "episode: 2568   score: 180.0  epsilon: 1.0    steps: 32  evaluation reward: 226.85\n",
      "episode: 2569   score: 210.0  epsilon: 1.0    steps: 1024  evaluation reward: 226.85\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5332: Policy loss: 0.002269. Value loss: 0.207188. Entropy: 0.924434.\n",
      "Iteration 5333: Policy loss: -0.008109. Value loss: 0.125597. Entropy: 0.919575.\n",
      "Iteration 5334: Policy loss: -0.009254. Value loss: 0.083259. Entropy: 0.927445.\n",
      "episode: 2570   score: 125.0  epsilon: 1.0    steps: 88  evaluation reward: 222.55\n",
      "episode: 2571   score: 105.0  epsilon: 1.0    steps: 1000  evaluation reward: 222.25\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5335: Policy loss: -0.000315. Value loss: 0.163562. Entropy: 1.147594.\n",
      "Iteration 5336: Policy loss: -0.004666. Value loss: 0.081960. Entropy: 1.115120.\n",
      "Iteration 5337: Policy loss: -0.009392. Value loss: 0.074607. Entropy: 1.140727.\n",
      "episode: 2572   score: 155.0  epsilon: 1.0    steps: 136  evaluation reward: 221.65\n",
      "episode: 2573   score: 80.0  epsilon: 1.0    steps: 488  evaluation reward: 220.35\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5338: Policy loss: 0.001104. Value loss: 0.160986. Entropy: 1.017980.\n",
      "Iteration 5339: Policy loss: -0.000558. Value loss: 0.081230. Entropy: 1.029304.\n",
      "Iteration 5340: Policy loss: -0.005417. Value loss: 0.062918. Entropy: 1.023387.\n",
      "episode: 2574   score: 210.0  epsilon: 1.0    steps: 344  evaluation reward: 220.35\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5341: Policy loss: 0.002714. Value loss: 0.176716. Entropy: 1.086334.\n",
      "Iteration 5342: Policy loss: 0.000105. Value loss: 0.110993. Entropy: 1.086449.\n",
      "Iteration 5343: Policy loss: -0.003640. Value loss: 0.069534. Entropy: 1.088018.\n",
      "episode: 2575   score: 105.0  epsilon: 1.0    steps: 128  evaluation reward: 219.3\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5344: Policy loss: -0.000173. Value loss: 0.304724. Entropy: 1.097169.\n",
      "Iteration 5345: Policy loss: -0.001726. Value loss: 0.166715. Entropy: 1.099270.\n",
      "Iteration 5346: Policy loss: -0.013106. Value loss: 0.105144. Entropy: 1.086525.\n",
      "episode: 2576   score: 180.0  epsilon: 1.0    steps: 416  evaluation reward: 219.3\n",
      "episode: 2577   score: 260.0  epsilon: 1.0    steps: 456  evaluation reward: 220.1\n",
      "episode: 2578   score: 210.0  epsilon: 1.0    steps: 904  evaluation reward: 219.1\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5347: Policy loss: 0.001967. Value loss: 0.402554. Entropy: 1.219816.\n",
      "Iteration 5348: Policy loss: -0.003626. Value loss: 0.221050. Entropy: 1.221701.\n",
      "Iteration 5349: Policy loss: -0.008408. Value loss: 0.145431. Entropy: 1.214040.\n",
      "episode: 2579   score: 210.0  epsilon: 1.0    steps: 272  evaluation reward: 219.65\n",
      "episode: 2580   score: 155.0  epsilon: 1.0    steps: 416  evaluation reward: 219.05\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5350: Policy loss: -0.000281. Value loss: 0.379651. Entropy: 1.046830.\n",
      "Iteration 5351: Policy loss: -0.000716. Value loss: 0.242420. Entropy: 1.073896.\n",
      "Iteration 5352: Policy loss: -0.008395. Value loss: 0.186424. Entropy: 1.032253.\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5353: Policy loss: 0.002642. Value loss: 0.293954. Entropy: 1.021809.\n",
      "Iteration 5354: Policy loss: -0.005682. Value loss: 0.185466. Entropy: 1.028796.\n",
      "Iteration 5355: Policy loss: -0.009761. Value loss: 0.157631. Entropy: 1.027620.\n",
      "episode: 2581   score: 275.0  epsilon: 1.0    steps: 120  evaluation reward: 219.7\n",
      "episode: 2582   score: 180.0  epsilon: 1.0    steps: 392  evaluation reward: 220.5\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5356: Policy loss: 0.006327. Value loss: 0.396119. Entropy: 1.098549.\n",
      "Iteration 5357: Policy loss: -0.001734. Value loss: 0.192794. Entropy: 1.099417.\n",
      "Iteration 5358: Policy loss: -0.007040. Value loss: 0.158728. Entropy: 1.094529.\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5359: Policy loss: -0.002062. Value loss: 0.306049. Entropy: 1.206007.\n",
      "Iteration 5360: Policy loss: -0.007672. Value loss: 0.182101. Entropy: 1.222620.\n",
      "Iteration 5361: Policy loss: -0.010744. Value loss: 0.125581. Entropy: 1.227499.\n",
      "episode: 2583   score: 475.0  epsilon: 1.0    steps: 296  evaluation reward: 220.65\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5362: Policy loss: -0.000755. Value loss: 0.315850. Entropy: 1.174771.\n",
      "Iteration 5363: Policy loss: -0.004504. Value loss: 0.166428. Entropy: 1.164396.\n",
      "Iteration 5364: Policy loss: -0.005058. Value loss: 0.132362. Entropy: 1.181396.\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5365: Policy loss: 0.001073. Value loss: 0.203146. Entropy: 1.205502.\n",
      "Iteration 5366: Policy loss: -0.003009. Value loss: 0.127277. Entropy: 1.206866.\n",
      "Iteration 5367: Policy loss: -0.011709. Value loss: 0.101448. Entropy: 1.221226.\n",
      "episode: 2584   score: 210.0  epsilon: 1.0    steps: 720  evaluation reward: 220.65\n",
      "episode: 2585   score: 215.0  epsilon: 1.0    steps: 760  evaluation reward: 220.7\n",
      "episode: 2586   score: 260.0  epsilon: 1.0    steps: 888  evaluation reward: 221.75\n",
      "Training network. lr: 0.000209. clip: 0.083568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5368: Policy loss: 0.001338. Value loss: 0.266879. Entropy: 1.218491.\n",
      "Iteration 5369: Policy loss: -0.004451. Value loss: 0.131698. Entropy: 1.214640.\n",
      "Iteration 5370: Policy loss: -0.010557. Value loss: 0.097440. Entropy: 1.208637.\n",
      "episode: 2587   score: 240.0  epsilon: 1.0    steps: 272  evaluation reward: 223.15\n",
      "episode: 2588   score: 320.0  epsilon: 1.0    steps: 416  evaluation reward: 224.25\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5371: Policy loss: 0.003546. Value loss: 0.128818. Entropy: 1.068481.\n",
      "Iteration 5372: Policy loss: -0.006878. Value loss: 0.083964. Entropy: 1.089044.\n",
      "Iteration 5373: Policy loss: -0.010154. Value loss: 0.068159. Entropy: 1.096841.\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5374: Policy loss: -0.000031. Value loss: 0.159398. Entropy: 1.069943.\n",
      "Iteration 5375: Policy loss: -0.005476. Value loss: 0.090234. Entropy: 1.070771.\n",
      "Iteration 5376: Policy loss: -0.012637. Value loss: 0.064876. Entropy: 1.068917.\n",
      "episode: 2589   score: 265.0  epsilon: 1.0    steps: 416  evaluation reward: 224.85\n",
      "episode: 2590   score: 260.0  epsilon: 1.0    steps: 856  evaluation reward: 225.05\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5377: Policy loss: -0.001380. Value loss: 0.146402. Entropy: 1.131858.\n",
      "Iteration 5378: Policy loss: -0.004953. Value loss: 0.090448. Entropy: 1.113220.\n",
      "Iteration 5379: Policy loss: -0.010699. Value loss: 0.075333. Entropy: 1.135784.\n",
      "episode: 2591   score: 185.0  epsilon: 1.0    steps: 624  evaluation reward: 221.6\n",
      "episode: 2592   score: 240.0  epsilon: 1.0    steps: 848  evaluation reward: 221.9\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5380: Policy loss: 0.002588. Value loss: 0.186315. Entropy: 1.006532.\n",
      "Iteration 5381: Policy loss: -0.005564. Value loss: 0.117994. Entropy: 1.025211.\n",
      "Iteration 5382: Policy loss: -0.004763. Value loss: 0.085844. Entropy: 1.023356.\n",
      "episode: 2593   score: 210.0  epsilon: 1.0    steps: 32  evaluation reward: 221.9\n",
      "episode: 2594   score: 210.0  epsilon: 1.0    steps: 976  evaluation reward: 220.2\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5383: Policy loss: 0.001775. Value loss: 0.224718. Entropy: 1.080205.\n",
      "Iteration 5384: Policy loss: -0.003707. Value loss: 0.137214. Entropy: 1.087848.\n",
      "Iteration 5385: Policy loss: -0.005194. Value loss: 0.112478. Entropy: 1.092744.\n",
      "episode: 2595   score: 210.0  epsilon: 1.0    steps: 664  evaluation reward: 220.15\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5386: Policy loss: 0.001268. Value loss: 0.159201. Entropy: 1.028479.\n",
      "Iteration 5387: Policy loss: -0.004604. Value loss: 0.076533. Entropy: 1.047803.\n",
      "Iteration 5388: Policy loss: -0.009754. Value loss: 0.069410. Entropy: 1.032348.\n",
      "episode: 2596   score: 210.0  epsilon: 1.0    steps: 592  evaluation reward: 217.6\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5389: Policy loss: 0.007620. Value loss: 0.627730. Entropy: 1.145941.\n",
      "Iteration 5390: Policy loss: 0.001941. Value loss: 0.254379. Entropy: 1.163713.\n",
      "Iteration 5391: Policy loss: -0.000190. Value loss: 0.224894. Entropy: 1.149893.\n",
      "episode: 2597   score: 270.0  epsilon: 1.0    steps: 48  evaluation reward: 217.7\n",
      "episode: 2598   score: 210.0  epsilon: 1.0    steps: 728  evaluation reward: 215.4\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5392: Policy loss: 0.000056. Value loss: 0.286908. Entropy: 1.148270.\n",
      "Iteration 5393: Policy loss: -0.007782. Value loss: 0.188690. Entropy: 1.133892.\n",
      "Iteration 5394: Policy loss: -0.012882. Value loss: 0.142362. Entropy: 1.148597.\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5395: Policy loss: 0.002731. Value loss: 0.206992. Entropy: 1.033500.\n",
      "Iteration 5396: Policy loss: -0.010100. Value loss: 0.118010. Entropy: 1.043208.\n",
      "Iteration 5397: Policy loss: -0.005481. Value loss: 0.075773. Entropy: 1.053684.\n",
      "episode: 2599   score: 225.0  epsilon: 1.0    steps: 224  evaluation reward: 216.05\n",
      "episode: 2600   score: 425.0  epsilon: 1.0    steps: 864  evaluation reward: 216.2\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5398: Policy loss: -0.002872. Value loss: 0.416790. Entropy: 1.306490.\n",
      "Iteration 5399: Policy loss: -0.010409. Value loss: 0.178558. Entropy: 1.306350.\n",
      "Iteration 5400: Policy loss: -0.015662. Value loss: 0.150718. Entropy: 1.307645.\n",
      "now time :  2019-02-28 11:59:43.773290\n",
      "episode: 2601   score: 135.0  epsilon: 1.0    steps: 24  evaluation reward: 214.4\n",
      "episode: 2602   score: 260.0  epsilon: 1.0    steps: 248  evaluation reward: 216.35\n",
      "episode: 2603   score: 250.0  epsilon: 1.0    steps: 448  evaluation reward: 215.55\n",
      "episode: 2604   score: 200.0  epsilon: 1.0    steps: 1016  evaluation reward: 215.4\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5401: Policy loss: 0.002208. Value loss: 0.183033. Entropy: 1.014305.\n",
      "Iteration 5402: Policy loss: -0.004500. Value loss: 0.103037. Entropy: 1.007182.\n",
      "Iteration 5403: Policy loss: -0.003703. Value loss: 0.084440. Entropy: 1.029185.\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5404: Policy loss: 0.002070. Value loss: 0.207491. Entropy: 1.037594.\n",
      "Iteration 5405: Policy loss: -0.003513. Value loss: 0.123513. Entropy: 1.067396.\n",
      "Iteration 5406: Policy loss: -0.010090. Value loss: 0.092531. Entropy: 1.044362.\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5407: Policy loss: 0.006622. Value loss: 0.377824. Entropy: 1.116705.\n",
      "Iteration 5408: Policy loss: -0.005507. Value loss: 0.149963. Entropy: 1.149862.\n",
      "Iteration 5409: Policy loss: -0.011624. Value loss: 0.103071. Entropy: 1.125877.\n",
      "episode: 2605   score: 180.0  epsilon: 1.0    steps: 616  evaluation reward: 215.1\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5410: Policy loss: 0.000511. Value loss: 0.325830. Entropy: 1.107940.\n",
      "Iteration 5411: Policy loss: -0.003485. Value loss: 0.141900. Entropy: 1.095800.\n",
      "Iteration 5412: Policy loss: -0.008223. Value loss: 0.107745. Entropy: 1.094765.\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5413: Policy loss: 0.002718. Value loss: 0.259323. Entropy: 1.238773.\n",
      "Iteration 5414: Policy loss: -0.007315. Value loss: 0.119269. Entropy: 1.231064.\n",
      "Iteration 5415: Policy loss: -0.009430. Value loss: 0.074738. Entropy: 1.235096.\n",
      "episode: 2606   score: 210.0  epsilon: 1.0    steps: 352  evaluation reward: 215.1\n",
      "episode: 2607   score: 150.0  epsilon: 1.0    steps: 472  evaluation reward: 215.45\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5416: Policy loss: 0.003209. Value loss: 0.647963. Entropy: 1.282833.\n",
      "Iteration 5417: Policy loss: 0.001386. Value loss: 0.205701. Entropy: 1.280235.\n",
      "Iteration 5418: Policy loss: -0.004408. Value loss: 0.196330. Entropy: 1.284282.\n",
      "episode: 2608   score: 360.0  epsilon: 1.0    steps: 160  evaluation reward: 216.6\n",
      "episode: 2609   score: 390.0  epsilon: 1.0    steps: 368  evaluation reward: 219.7\n",
      "episode: 2610   score: 105.0  epsilon: 1.0    steps: 664  evaluation reward: 219.2\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5419: Policy loss: 0.002348. Value loss: 0.270371. Entropy: 1.126244.\n",
      "Iteration 5420: Policy loss: -0.002332. Value loss: 0.139035. Entropy: 1.117805.\n",
      "Iteration 5421: Policy loss: -0.010766. Value loss: 0.131642. Entropy: 1.137762.\n",
      "episode: 2611   score: 240.0  epsilon: 1.0    steps: 504  evaluation reward: 220.95\n",
      "episode: 2612   score: 215.0  epsilon: 1.0    steps: 624  evaluation reward: 221.3\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5422: Policy loss: -0.001190. Value loss: 0.273164. Entropy: 1.043980.\n",
      "Iteration 5423: Policy loss: -0.010338. Value loss: 0.164431. Entropy: 1.039961.\n",
      "Iteration 5424: Policy loss: -0.014356. Value loss: 0.135358. Entropy: 1.044279.\n",
      "episode: 2613   score: 120.0  epsilon: 1.0    steps: 576  evaluation reward: 220.6\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5425: Policy loss: -0.004430. Value loss: 0.152198. Entropy: 1.054324.\n",
      "Iteration 5426: Policy loss: -0.009950. Value loss: 0.113742. Entropy: 1.086809.\n",
      "Iteration 5427: Policy loss: -0.012506. Value loss: 0.090767. Entropy: 1.073305.\n",
      "episode: 2614   score: 520.0  epsilon: 1.0    steps: 240  evaluation reward: 220.4\n",
      "episode: 2615   score: 135.0  epsilon: 1.0    steps: 672  evaluation reward: 220.2\n",
      "Training network. lr: 0.000209. clip: 0.083420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5428: Policy loss: -0.001347. Value loss: 0.150638. Entropy: 1.045026.\n",
      "Iteration 5429: Policy loss: -0.002843. Value loss: 0.078809. Entropy: 1.028677.\n",
      "Iteration 5430: Policy loss: -0.009282. Value loss: 0.072893. Entropy: 1.038443.\n",
      "episode: 2616   score: 120.0  epsilon: 1.0    steps: 32  evaluation reward: 219.3\n",
      "episode: 2617   score: 210.0  epsilon: 1.0    steps: 680  evaluation reward: 218.9\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5431: Policy loss: 0.001958. Value loss: 0.148015. Entropy: 0.970703.\n",
      "Iteration 5432: Policy loss: -0.004153. Value loss: 0.085482. Entropy: 0.979679.\n",
      "Iteration 5433: Policy loss: -0.010692. Value loss: 0.060229. Entropy: 0.957435.\n",
      "episode: 2618   score: 150.0  epsilon: 1.0    steps: 328  evaluation reward: 217.25\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5434: Policy loss: 0.002195. Value loss: 0.350356. Entropy: 1.083582.\n",
      "Iteration 5435: Policy loss: -0.002838. Value loss: 0.170484. Entropy: 1.085251.\n",
      "Iteration 5436: Policy loss: -0.013160. Value loss: 0.123834. Entropy: 1.058661.\n",
      "episode: 2619   score: 210.0  epsilon: 1.0    steps: 736  evaluation reward: 217.7\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5437: Policy loss: -0.000284. Value loss: 0.184994. Entropy: 1.088469.\n",
      "Iteration 5438: Policy loss: -0.002249. Value loss: 0.085829. Entropy: 1.108397.\n",
      "Iteration 5439: Policy loss: -0.008482. Value loss: 0.063633. Entropy: 1.089158.\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5440: Policy loss: -0.001992. Value loss: 0.128400. Entropy: 1.111740.\n",
      "Iteration 5441: Policy loss: -0.005370. Value loss: 0.093737. Entropy: 1.106787.\n",
      "Iteration 5442: Policy loss: -0.009300. Value loss: 0.073140. Entropy: 1.107225.\n",
      "episode: 2620   score: 215.0  epsilon: 1.0    steps: 376  evaluation reward: 215.6\n",
      "episode: 2621   score: 180.0  epsilon: 1.0    steps: 584  evaluation reward: 215.6\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5443: Policy loss: 0.000110. Value loss: 0.251947. Entropy: 1.179518.\n",
      "Iteration 5444: Policy loss: -0.006423. Value loss: 0.119440. Entropy: 1.191028.\n",
      "Iteration 5445: Policy loss: -0.010641. Value loss: 0.101452. Entropy: 1.187193.\n",
      "episode: 2622   score: 210.0  epsilon: 1.0    steps: 608  evaluation reward: 215.55\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5446: Policy loss: 0.004512. Value loss: 0.238158. Entropy: 1.063842.\n",
      "Iteration 5447: Policy loss: -0.005737. Value loss: 0.118357. Entropy: 1.053650.\n",
      "Iteration 5448: Policy loss: -0.011811. Value loss: 0.092605. Entropy: 1.067264.\n",
      "episode: 2623   score: 260.0  epsilon: 1.0    steps: 136  evaluation reward: 213.55\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5449: Policy loss: 0.004442. Value loss: 0.580938. Entropy: 1.076661.\n",
      "Iteration 5450: Policy loss: -0.000924. Value loss: 0.281219. Entropy: 1.079434.\n",
      "Iteration 5451: Policy loss: -0.005667. Value loss: 0.194384. Entropy: 1.085177.\n",
      "episode: 2624   score: 390.0  epsilon: 1.0    steps: 584  evaluation reward: 215.65\n",
      "episode: 2625   score: 210.0  epsilon: 1.0    steps: 816  evaluation reward: 216.95\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5452: Policy loss: 0.000562. Value loss: 0.614228. Entropy: 1.048397.\n",
      "Iteration 5453: Policy loss: -0.002836. Value loss: 0.366017. Entropy: 1.043209.\n",
      "Iteration 5454: Policy loss: -0.001694. Value loss: 0.249187. Entropy: 1.036209.\n",
      "episode: 2626   score: 180.0  epsilon: 1.0    steps: 104  evaluation reward: 216.65\n",
      "episode: 2627   score: 460.0  epsilon: 1.0    steps: 112  evaluation reward: 220.65\n",
      "episode: 2628   score: 335.0  epsilon: 1.0    steps: 696  evaluation reward: 221.2\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5455: Policy loss: 0.003725. Value loss: 0.490554. Entropy: 1.005252.\n",
      "Iteration 5456: Policy loss: -0.000028. Value loss: 0.383147. Entropy: 0.979141.\n",
      "Iteration 5457: Policy loss: -0.004678. Value loss: 0.300734. Entropy: 0.997975.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5458: Policy loss: 0.006307. Value loss: 0.443085. Entropy: 1.092210.\n",
      "Iteration 5459: Policy loss: -0.002031. Value loss: 0.258431. Entropy: 1.072874.\n",
      "Iteration 5460: Policy loss: -0.003988. Value loss: 0.215576. Entropy: 1.087077.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5461: Policy loss: 0.001825. Value loss: 0.318352. Entropy: 1.129528.\n",
      "Iteration 5462: Policy loss: -0.002195. Value loss: 0.179529. Entropy: 1.132173.\n",
      "Iteration 5463: Policy loss: -0.005225. Value loss: 0.153441. Entropy: 1.134026.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5464: Policy loss: 0.002587. Value loss: 0.223631. Entropy: 1.018071.\n",
      "Iteration 5465: Policy loss: -0.005357. Value loss: 0.105739. Entropy: 1.007882.\n",
      "Iteration 5466: Policy loss: -0.011190. Value loss: 0.071147. Entropy: 1.020564.\n",
      "episode: 2629   score: 155.0  epsilon: 1.0    steps: 504  evaluation reward: 221.4\n",
      "episode: 2630   score: 210.0  epsilon: 1.0    steps: 968  evaluation reward: 221.3\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5467: Policy loss: -0.000579. Value loss: 0.445703. Entropy: 1.239674.\n",
      "Iteration 5468: Policy loss: -0.004706. Value loss: 0.230088. Entropy: 1.244369.\n",
      "Iteration 5469: Policy loss: -0.009761. Value loss: 0.172570. Entropy: 1.236011.\n",
      "episode: 2631   score: 285.0  epsilon: 1.0    steps: 200  evaluation reward: 223.1\n",
      "episode: 2632   score: 220.0  epsilon: 1.0    steps: 376  evaluation reward: 218.35\n",
      "episode: 2633   score: 205.0  epsilon: 1.0    steps: 496  evaluation reward: 219.35\n",
      "episode: 2634   score: 360.0  epsilon: 1.0    steps: 512  evaluation reward: 220.7\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5470: Policy loss: 0.000796. Value loss: 0.521496. Entropy: 1.068284.\n",
      "Iteration 5471: Policy loss: -0.000918. Value loss: 0.340955. Entropy: 1.077680.\n",
      "Iteration 5472: Policy loss: -0.004003. Value loss: 0.207459. Entropy: 1.073447.\n",
      "episode: 2635   score: 170.0  epsilon: 1.0    steps: 720  evaluation reward: 220.0\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5473: Policy loss: -0.000037. Value loss: 0.272132. Entropy: 0.813528.\n",
      "Iteration 5474: Policy loss: -0.008273. Value loss: 0.140200. Entropy: 0.849745.\n",
      "Iteration 5475: Policy loss: -0.004694. Value loss: 0.108749. Entropy: 0.837870.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5476: Policy loss: 0.002335. Value loss: 0.208420. Entropy: 1.054438.\n",
      "Iteration 5477: Policy loss: -0.007446. Value loss: 0.112294. Entropy: 1.052883.\n",
      "Iteration 5478: Policy loss: -0.011560. Value loss: 0.093147. Entropy: 1.064892.\n",
      "episode: 2636   score: 120.0  epsilon: 1.0    steps: 512  evaluation reward: 218.05\n",
      "episode: 2637   score: 610.0  epsilon: 1.0    steps: 784  evaluation reward: 220.05\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5479: Policy loss: 0.001495. Value loss: 0.608978. Entropy: 1.099324.\n",
      "Iteration 5480: Policy loss: 0.000697. Value loss: 0.301211. Entropy: 1.107465.\n",
      "Iteration 5481: Policy loss: -0.004331. Value loss: 0.225159. Entropy: 1.099146.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5482: Policy loss: 0.003601. Value loss: 0.542545. Entropy: 1.100177.\n",
      "Iteration 5483: Policy loss: -0.001676. Value loss: 0.244266. Entropy: 1.081428.\n",
      "Iteration 5484: Policy loss: 0.001881. Value loss: 0.271917. Entropy: 1.067431.\n",
      "episode: 2638   score: 210.0  epsilon: 1.0    steps: 760  evaluation reward: 220.05\n",
      "episode: 2639   score: 415.0  epsilon: 1.0    steps: 936  evaluation reward: 222.1\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5485: Policy loss: 0.004711. Value loss: 0.358012. Entropy: 1.181240.\n",
      "Iteration 5486: Policy loss: 0.001765. Value loss: 0.211604. Entropy: 1.180337.\n",
      "Iteration 5487: Policy loss: -0.005589. Value loss: 0.161353. Entropy: 1.178869.\n",
      "episode: 2640   score: 210.0  epsilon: 1.0    steps: 392  evaluation reward: 222.65\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5488: Policy loss: 0.006531. Value loss: 0.394688. Entropy: 1.094722.\n",
      "Iteration 5489: Policy loss: -0.000229. Value loss: 0.178009. Entropy: 1.083845.\n",
      "Iteration 5490: Policy loss: -0.006505. Value loss: 0.143299. Entropy: 1.078457.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5491: Policy loss: 0.010554. Value loss: 0.398691. Entropy: 1.121096.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5492: Policy loss: -0.002178. Value loss: 0.151831. Entropy: 1.125117.\n",
      "Iteration 5493: Policy loss: -0.009754. Value loss: 0.096005. Entropy: 1.110528.\n",
      "episode: 2641   score: 405.0  epsilon: 1.0    steps: 200  evaluation reward: 224.6\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5494: Policy loss: 0.001989. Value loss: 0.661824. Entropy: 1.021438.\n",
      "Iteration 5495: Policy loss: -0.001633. Value loss: 0.357019. Entropy: 1.032638.\n",
      "Iteration 5496: Policy loss: -0.005961. Value loss: 0.285728. Entropy: 1.045770.\n",
      "episode: 2642   score: 180.0  epsilon: 1.0    steps: 544  evaluation reward: 225.35\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5497: Policy loss: -0.000980. Value loss: 0.484533. Entropy: 1.137987.\n",
      "Iteration 5498: Policy loss: -0.006931. Value loss: 0.289217. Entropy: 1.123938.\n",
      "Iteration 5499: Policy loss: -0.011614. Value loss: 0.225796. Entropy: 1.142329.\n",
      "episode: 2643   score: 260.0  epsilon: 1.0    steps: 448  evaluation reward: 225.85\n",
      "episode: 2644   score: 320.0  epsilon: 1.0    steps: 496  evaluation reward: 228.25\n",
      "episode: 2645   score: 470.0  epsilon: 1.0    steps: 744  evaluation reward: 231.9\n",
      "episode: 2646   score: 205.0  epsilon: 1.0    steps: 920  evaluation reward: 231.85\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5500: Policy loss: 0.004380. Value loss: 0.375638. Entropy: 1.080479.\n",
      "Iteration 5501: Policy loss: -0.000518. Value loss: 0.203144. Entropy: 1.090577.\n",
      "Iteration 5502: Policy loss: -0.004958. Value loss: 0.137469. Entropy: 1.101689.\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5503: Policy loss: 0.000034. Value loss: 0.223678. Entropy: 0.835379.\n",
      "Iteration 5504: Policy loss: -0.006928. Value loss: 0.114536. Entropy: 0.836475.\n",
      "Iteration 5505: Policy loss: -0.009654. Value loss: 0.082855. Entropy: 0.833299.\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5506: Policy loss: 0.002235. Value loss: 0.295532. Entropy: 1.216844.\n",
      "Iteration 5507: Policy loss: -0.008546. Value loss: 0.188841. Entropy: 1.223911.\n",
      "Iteration 5508: Policy loss: -0.013157. Value loss: 0.110269. Entropy: 1.211296.\n",
      "episode: 2647   score: 535.0  epsilon: 1.0    steps: 624  evaluation reward: 235.05\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5509: Policy loss: 0.001841. Value loss: 0.213212. Entropy: 1.069112.\n",
      "Iteration 5510: Policy loss: -0.007841. Value loss: 0.081648. Entropy: 1.055897.\n",
      "Iteration 5511: Policy loss: -0.015721. Value loss: 0.061064. Entropy: 1.061720.\n",
      "episode: 2648   score: 295.0  epsilon: 1.0    steps: 904  evaluation reward: 235.9\n",
      "episode: 2649   score: 460.0  epsilon: 1.0    steps: 1024  evaluation reward: 237.95\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5512: Policy loss: -0.000481. Value loss: 0.209084. Entropy: 1.134488.\n",
      "Iteration 5513: Policy loss: -0.005376. Value loss: 0.117206. Entropy: 1.120295.\n",
      "Iteration 5514: Policy loss: -0.007828. Value loss: 0.083329. Entropy: 1.133608.\n",
      "episode: 2650   score: 185.0  epsilon: 1.0    steps: 816  evaluation reward: 237.4\n",
      "now time :  2019-02-28 12:01:07.470753\n",
      "episode: 2651   score: 155.0  epsilon: 1.0    steps: 1024  evaluation reward: 237.15\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5515: Policy loss: -0.000460. Value loss: 0.201261. Entropy: 1.081898.\n",
      "Iteration 5516: Policy loss: -0.002299. Value loss: 0.102745. Entropy: 1.104586.\n",
      "Iteration 5517: Policy loss: -0.010020. Value loss: 0.079395. Entropy: 1.092789.\n",
      "episode: 2652   score: 240.0  epsilon: 1.0    steps: 56  evaluation reward: 237.45\n",
      "episode: 2653   score: 215.0  epsilon: 1.0    steps: 296  evaluation reward: 237.5\n",
      "episode: 2654   score: 255.0  epsilon: 1.0    steps: 488  evaluation reward: 237.8\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5518: Policy loss: 0.003701. Value loss: 0.183926. Entropy: 0.869503.\n",
      "Iteration 5519: Policy loss: -0.003439. Value loss: 0.100233. Entropy: 0.880375.\n",
      "Iteration 5520: Policy loss: -0.006027. Value loss: 0.083448. Entropy: 0.882024.\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5521: Policy loss: 0.007475. Value loss: 0.321570. Entropy: 0.855781.\n",
      "Iteration 5522: Policy loss: -0.003204. Value loss: 0.135358. Entropy: 0.878759.\n",
      "Iteration 5523: Policy loss: -0.005265. Value loss: 0.100326. Entropy: 0.890928.\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5524: Policy loss: 0.002706. Value loss: 0.496428. Entropy: 1.051044.\n",
      "Iteration 5525: Policy loss: -0.004621. Value loss: 0.195566. Entropy: 1.070208.\n",
      "Iteration 5526: Policy loss: -0.013046. Value loss: 0.131839. Entropy: 1.059358.\n",
      "episode: 2655   score: 270.0  epsilon: 1.0    steps: 664  evaluation reward: 238.1\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5527: Policy loss: -0.001887. Value loss: 0.411831. Entropy: 1.213194.\n",
      "Iteration 5528: Policy loss: -0.003504. Value loss: 0.232201. Entropy: 1.197312.\n",
      "Iteration 5529: Policy loss: -0.010586. Value loss: 0.193076. Entropy: 1.188022.\n",
      "episode: 2656   score: 260.0  epsilon: 1.0    steps: 232  evaluation reward: 238.6\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5530: Policy loss: -0.003510. Value loss: 0.348321. Entropy: 1.106613.\n",
      "Iteration 5531: Policy loss: -0.009227. Value loss: 0.174191. Entropy: 1.108968.\n",
      "Iteration 5532: Policy loss: -0.014124. Value loss: 0.143348. Entropy: 1.121530.\n",
      "episode: 2657   score: 165.0  epsilon: 1.0    steps: 272  evaluation reward: 239.45\n",
      "episode: 2658   score: 180.0  epsilon: 1.0    steps: 376  evaluation reward: 239.15\n",
      "episode: 2659   score: 280.0  epsilon: 1.0    steps: 632  evaluation reward: 239.8\n",
      "episode: 2660   score: 230.0  epsilon: 1.0    steps: 968  evaluation reward: 240.0\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5533: Policy loss: 0.001221. Value loss: 0.350590. Entropy: 1.145326.\n",
      "Iteration 5534: Policy loss: -0.000840. Value loss: 0.198666. Entropy: 1.151976.\n",
      "Iteration 5535: Policy loss: -0.006651. Value loss: 0.145818. Entropy: 1.149841.\n",
      "episode: 2661   score: 270.0  epsilon: 1.0    steps: 264  evaluation reward: 240.1\n",
      "episode: 2662   score: 215.0  epsilon: 1.0    steps: 544  evaluation reward: 240.1\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5536: Policy loss: 0.002797. Value loss: 0.200449. Entropy: 0.907616.\n",
      "Iteration 5537: Policy loss: -0.005746. Value loss: 0.121085. Entropy: 0.897409.\n",
      "Iteration 5538: Policy loss: -0.010313. Value loss: 0.096916. Entropy: 0.911840.\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5539: Policy loss: 0.003067. Value loss: 0.235226. Entropy: 0.979261.\n",
      "Iteration 5540: Policy loss: -0.002851. Value loss: 0.129691. Entropy: 0.998814.\n",
      "Iteration 5541: Policy loss: -0.008948. Value loss: 0.096452. Entropy: 1.008136.\n",
      "episode: 2663   score: 105.0  epsilon: 1.0    steps: 600  evaluation reward: 240.5\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5542: Policy loss: 0.000788. Value loss: 0.380792. Entropy: 1.153574.\n",
      "Iteration 5543: Policy loss: -0.001641. Value loss: 0.170782. Entropy: 1.164394.\n",
      "Iteration 5544: Policy loss: -0.010090. Value loss: 0.130625. Entropy: 1.152908.\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5545: Policy loss: 0.000653. Value loss: 0.201167. Entropy: 1.086302.\n",
      "Iteration 5546: Policy loss: -0.002681. Value loss: 0.100008. Entropy: 1.110579.\n",
      "Iteration 5547: Policy loss: -0.010101. Value loss: 0.081138. Entropy: 1.081367.\n",
      "episode: 2664   score: 135.0  epsilon: 1.0    steps: 232  evaluation reward: 239.75\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5548: Policy loss: 0.000751. Value loss: 0.327933. Entropy: 1.161810.\n",
      "Iteration 5549: Policy loss: -0.004532. Value loss: 0.149141. Entropy: 1.176629.\n",
      "Iteration 5550: Policy loss: -0.009021. Value loss: 0.106127. Entropy: 1.166317.\n",
      "episode: 2665   score: 240.0  epsilon: 1.0    steps: 64  evaluation reward: 240.05\n",
      "episode: 2666   score: 240.0  epsilon: 1.0    steps: 280  evaluation reward: 240.35\n",
      "episode: 2667   score: 265.0  epsilon: 1.0    steps: 328  evaluation reward: 240.85\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5551: Policy loss: 0.000421. Value loss: 0.371105. Entropy: 1.054303.\n",
      "Iteration 5552: Policy loss: -0.006823. Value loss: 0.187339. Entropy: 1.042175.\n",
      "Iteration 5553: Policy loss: -0.012154. Value loss: 0.142425. Entropy: 1.064797.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2668   score: 475.0  epsilon: 1.0    steps: 424  evaluation reward: 243.8\n",
      "episode: 2669   score: 235.0  epsilon: 1.0    steps: 424  evaluation reward: 244.05\n",
      "episode: 2670   score: 195.0  epsilon: 1.0    steps: 872  evaluation reward: 244.75\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5554: Policy loss: 0.003433. Value loss: 0.228128. Entropy: 0.984967.\n",
      "Iteration 5555: Policy loss: -0.004832. Value loss: 0.113707. Entropy: 0.961515.\n",
      "Iteration 5556: Policy loss: -0.011981. Value loss: 0.105950. Entropy: 0.960430.\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5557: Policy loss: 0.002340. Value loss: 0.241329. Entropy: 0.863714.\n",
      "Iteration 5558: Policy loss: -0.003660. Value loss: 0.121355. Entropy: 0.877637.\n",
      "Iteration 5559: Policy loss: -0.010955. Value loss: 0.089426. Entropy: 0.860076.\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5560: Policy loss: 0.005525. Value loss: 0.823739. Entropy: 1.151801.\n",
      "Iteration 5561: Policy loss: 0.001239. Value loss: 0.395852. Entropy: 1.135617.\n",
      "Iteration 5562: Policy loss: -0.004458. Value loss: 0.292372. Entropy: 1.147232.\n",
      "episode: 2671   score: 220.0  epsilon: 1.0    steps: 192  evaluation reward: 245.9\n",
      "episode: 2672   score: 125.0  epsilon: 1.0    steps: 216  evaluation reward: 245.6\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5563: Policy loss: 0.001360. Value loss: 0.492087. Entropy: 1.125376.\n",
      "Iteration 5564: Policy loss: -0.007441. Value loss: 0.220286. Entropy: 1.118061.\n",
      "Iteration 5565: Policy loss: -0.009804. Value loss: 0.179985. Entropy: 1.131891.\n",
      "episode: 2673   score: 140.0  epsilon: 1.0    steps: 808  evaluation reward: 246.2\n",
      "episode: 2674   score: 515.0  epsilon: 1.0    steps: 920  evaluation reward: 249.25\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5566: Policy loss: 0.004100. Value loss: 0.161757. Entropy: 1.101234.\n",
      "Iteration 5567: Policy loss: 0.001352. Value loss: 0.084030. Entropy: 1.107127.\n",
      "Iteration 5568: Policy loss: -0.010558. Value loss: 0.067095. Entropy: 1.108625.\n",
      "episode: 2675   score: 105.0  epsilon: 1.0    steps: 496  evaluation reward: 249.25\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5569: Policy loss: 0.004973. Value loss: 0.674484. Entropy: 0.997407.\n",
      "Iteration 5570: Policy loss: 0.001659. Value loss: 0.452552. Entropy: 1.002509.\n",
      "Iteration 5571: Policy loss: -0.000145. Value loss: 0.398784. Entropy: 1.004063.\n",
      "episode: 2676   score: 255.0  epsilon: 1.0    steps: 528  evaluation reward: 250.0\n",
      "episode: 2677   score: 495.0  epsilon: 1.0    steps: 624  evaluation reward: 252.35\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5572: Policy loss: 0.002097. Value loss: 0.297269. Entropy: 1.012320.\n",
      "Iteration 5573: Policy loss: 0.001176. Value loss: 0.170866. Entropy: 1.044832.\n",
      "Iteration 5574: Policy loss: -0.006519. Value loss: 0.121197. Entropy: 1.040277.\n",
      "episode: 2678   score: 215.0  epsilon: 1.0    steps: 728  evaluation reward: 252.4\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5575: Policy loss: 0.001877. Value loss: 0.197101. Entropy: 0.900461.\n",
      "Iteration 5576: Policy loss: -0.001848. Value loss: 0.110017. Entropy: 0.904880.\n",
      "Iteration 5577: Policy loss: -0.005831. Value loss: 0.085411. Entropy: 0.900840.\n",
      "episode: 2679   score: 210.0  epsilon: 1.0    steps: 32  evaluation reward: 252.4\n",
      "episode: 2680   score: 170.0  epsilon: 1.0    steps: 1016  evaluation reward: 252.55\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5578: Policy loss: 0.005056. Value loss: 0.316960. Entropy: 1.022535.\n",
      "Iteration 5579: Policy loss: 0.000614. Value loss: 0.197273. Entropy: 1.028531.\n",
      "Iteration 5580: Policy loss: -0.004528. Value loss: 0.142522. Entropy: 1.027756.\n",
      "episode: 2681   score: 155.0  epsilon: 1.0    steps: 312  evaluation reward: 251.35\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5581: Policy loss: 0.001900. Value loss: 0.375165. Entropy: 0.957770.\n",
      "Iteration 5582: Policy loss: -0.007377. Value loss: 0.144560. Entropy: 0.961619.\n",
      "Iteration 5583: Policy loss: -0.008470. Value loss: 0.101174. Entropy: 0.960412.\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5584: Policy loss: 0.001849. Value loss: 0.472715. Entropy: 0.981602.\n",
      "Iteration 5585: Policy loss: -0.001767. Value loss: 0.214290. Entropy: 0.996183.\n",
      "Iteration 5586: Policy loss: -0.010190. Value loss: 0.139748. Entropy: 0.994458.\n",
      "episode: 2682   score: 225.0  epsilon: 1.0    steps: 56  evaluation reward: 251.8\n",
      "episode: 2683   score: 390.0  epsilon: 1.0    steps: 632  evaluation reward: 250.95\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5587: Policy loss: 0.000304. Value loss: 0.415147. Entropy: 1.037892.\n",
      "Iteration 5588: Policy loss: -0.001067. Value loss: 0.214643. Entropy: 1.002992.\n",
      "Iteration 5589: Policy loss: -0.004133. Value loss: 0.157933. Entropy: 1.019703.\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5590: Policy loss: 0.003951. Value loss: 0.372857. Entropy: 1.011392.\n",
      "Iteration 5591: Policy loss: -0.006385. Value loss: 0.155898. Entropy: 1.014192.\n",
      "Iteration 5592: Policy loss: -0.013151. Value loss: 0.115891. Entropy: 1.025729.\n",
      "episode: 2684   score: 180.0  epsilon: 1.0    steps: 320  evaluation reward: 250.65\n",
      "episode: 2685   score: 295.0  epsilon: 1.0    steps: 864  evaluation reward: 251.45\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5593: Policy loss: 0.005515. Value loss: 0.753672. Entropy: 1.012261.\n",
      "Iteration 5594: Policy loss: -0.005822. Value loss: 0.308155. Entropy: 1.015144.\n",
      "Iteration 5595: Policy loss: -0.009114. Value loss: 0.223950. Entropy: 1.023823.\n",
      "episode: 2686   score: 295.0  epsilon: 1.0    steps: 576  evaluation reward: 251.8\n",
      "episode: 2687   score: 380.0  epsilon: 1.0    steps: 1000  evaluation reward: 253.2\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5596: Policy loss: 0.002545. Value loss: 0.470550. Entropy: 0.938348.\n",
      "Iteration 5597: Policy loss: -0.004841. Value loss: 0.204392. Entropy: 0.930244.\n",
      "Iteration 5598: Policy loss: -0.010810. Value loss: 0.152597. Entropy: 0.919786.\n",
      "episode: 2688   score: 265.0  epsilon: 1.0    steps: 376  evaluation reward: 252.65\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5599: Policy loss: -0.000791. Value loss: 0.307443. Entropy: 0.962377.\n",
      "Iteration 5600: Policy loss: -0.006719. Value loss: 0.190465. Entropy: 0.962675.\n",
      "Iteration 5601: Policy loss: -0.009723. Value loss: 0.140302. Entropy: 0.954222.\n",
      "episode: 2689   score: 320.0  epsilon: 1.0    steps: 784  evaluation reward: 253.2\n",
      "episode: 2690   score: 270.0  epsilon: 1.0    steps: 1016  evaluation reward: 253.3\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5602: Policy loss: -0.001627. Value loss: 0.256013. Entropy: 0.994979.\n",
      "Iteration 5603: Policy loss: -0.007851. Value loss: 0.143963. Entropy: 1.015395.\n",
      "Iteration 5604: Policy loss: -0.007762. Value loss: 0.107990. Entropy: 1.003323.\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5605: Policy loss: 0.000722. Value loss: 0.278373. Entropy: 0.877596.\n",
      "Iteration 5606: Policy loss: 0.001305. Value loss: 0.144757. Entropy: 0.898965.\n",
      "Iteration 5607: Policy loss: -0.008454. Value loss: 0.127911. Entropy: 0.884790.\n",
      "episode: 2691   score: 155.0  epsilon: 1.0    steps: 792  evaluation reward: 253.0\n",
      "episode: 2692   score: 320.0  epsilon: 1.0    steps: 920  evaluation reward: 253.8\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5608: Policy loss: 0.000896. Value loss: 0.375435. Entropy: 1.184862.\n",
      "Iteration 5609: Policy loss: 0.001598. Value loss: 0.203678. Entropy: 1.178524.\n",
      "Iteration 5610: Policy loss: -0.012077. Value loss: 0.150573. Entropy: 1.187256.\n",
      "episode: 2693   score: 265.0  epsilon: 1.0    steps: 432  evaluation reward: 254.35\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5611: Policy loss: 0.001239. Value loss: 0.308975. Entropy: 0.893222.\n",
      "Iteration 5612: Policy loss: -0.000249. Value loss: 0.170329. Entropy: 0.915393.\n",
      "Iteration 5613: Policy loss: -0.005758. Value loss: 0.129877. Entropy: 0.919518.\n",
      "episode: 2694   score: 180.0  epsilon: 1.0    steps: 912  evaluation reward: 254.05\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5614: Policy loss: 0.004233. Value loss: 0.601320. Entropy: 1.095221.\n",
      "Iteration 5615: Policy loss: -0.000981. Value loss: 0.271102. Entropy: 1.083486.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5616: Policy loss: -0.003828. Value loss: 0.211160. Entropy: 1.076724.\n",
      "episode: 2695   score: 155.0  epsilon: 1.0    steps: 216  evaluation reward: 253.5\n",
      "episode: 2696   score: 560.0  epsilon: 1.0    steps: 344  evaluation reward: 257.0\n",
      "episode: 2697   score: 230.0  epsilon: 1.0    steps: 720  evaluation reward: 256.6\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5617: Policy loss: 0.006465. Value loss: 0.510989. Entropy: 0.942785.\n",
      "Iteration 5618: Policy loss: 0.003391. Value loss: 0.247873. Entropy: 0.959760.\n",
      "Iteration 5619: Policy loss: -0.006116. Value loss: 0.199333. Entropy: 0.957349.\n",
      "episode: 2698   score: 415.0  epsilon: 1.0    steps: 816  evaluation reward: 258.65\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5620: Policy loss: 0.000430. Value loss: 0.342280. Entropy: 0.964649.\n",
      "Iteration 5621: Policy loss: 0.000732. Value loss: 0.184751. Entropy: 0.978143.\n",
      "Iteration 5622: Policy loss: -0.006383. Value loss: 0.152424. Entropy: 0.966108.\n",
      "episode: 2699   score: 210.0  epsilon: 1.0    steps: 472  evaluation reward: 258.5\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5623: Policy loss: 0.003935. Value loss: 0.328775. Entropy: 0.901954.\n",
      "Iteration 5624: Policy loss: -0.002694. Value loss: 0.200093. Entropy: 0.910497.\n",
      "Iteration 5625: Policy loss: -0.007318. Value loss: 0.138255. Entropy: 0.903204.\n",
      "episode: 2700   score: 230.0  epsilon: 1.0    steps: 512  evaluation reward: 256.55\n",
      "now time :  2019-02-28 12:02:27.795864\n",
      "episode: 2701   score: 200.0  epsilon: 1.0    steps: 1024  evaluation reward: 257.2\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5626: Policy loss: 0.006651. Value loss: 0.194384. Entropy: 1.079351.\n",
      "Iteration 5627: Policy loss: -0.001992. Value loss: 0.091943. Entropy: 1.087186.\n",
      "Iteration 5628: Policy loss: -0.009285. Value loss: 0.070484. Entropy: 1.077757.\n",
      "episode: 2702   score: 240.0  epsilon: 1.0    steps: 856  evaluation reward: 257.0\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5629: Policy loss: 0.000341. Value loss: 0.224135. Entropy: 0.951809.\n",
      "Iteration 5630: Policy loss: -0.005014. Value loss: 0.117186. Entropy: 0.994812.\n",
      "Iteration 5631: Policy loss: -0.010941. Value loss: 0.094464. Entropy: 0.974140.\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5632: Policy loss: 0.001772. Value loss: 0.454908. Entropy: 0.955291.\n",
      "Iteration 5633: Policy loss: 0.005351. Value loss: 0.230523. Entropy: 0.940812.\n",
      "Iteration 5634: Policy loss: -0.008763. Value loss: 0.133335. Entropy: 0.947361.\n",
      "episode: 2703   score: 320.0  epsilon: 1.0    steps: 888  evaluation reward: 257.7\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5635: Policy loss: 0.002122. Value loss: 0.640436. Entropy: 1.152282.\n",
      "Iteration 5636: Policy loss: 0.004309. Value loss: 0.251858. Entropy: 1.141633.\n",
      "Iteration 5637: Policy loss: -0.004574. Value loss: 0.205845. Entropy: 1.147905.\n",
      "episode: 2704   score: 215.0  epsilon: 1.0    steps: 568  evaluation reward: 257.85\n",
      "episode: 2705   score: 210.0  epsilon: 1.0    steps: 592  evaluation reward: 258.15\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5638: Policy loss: 0.003260. Value loss: 0.404311. Entropy: 1.065570.\n",
      "Iteration 5639: Policy loss: -0.001115. Value loss: 0.189694. Entropy: 1.037132.\n",
      "Iteration 5640: Policy loss: -0.007313. Value loss: 0.124741. Entropy: 1.057435.\n",
      "episode: 2706   score: 450.0  epsilon: 1.0    steps: 528  evaluation reward: 260.55\n",
      "episode: 2707   score: 185.0  epsilon: 1.0    steps: 720  evaluation reward: 260.9\n",
      "episode: 2708   score: 165.0  epsilon: 1.0    steps: 1024  evaluation reward: 258.95\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5641: Policy loss: 0.003459. Value loss: 0.160133. Entropy: 1.061330.\n",
      "Iteration 5642: Policy loss: -0.004395. Value loss: 0.096246. Entropy: 1.065702.\n",
      "Iteration 5643: Policy loss: -0.006692. Value loss: 0.078021. Entropy: 1.061703.\n",
      "episode: 2709   score: 500.0  epsilon: 1.0    steps: 248  evaluation reward: 260.05\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5644: Policy loss: 0.002154. Value loss: 0.191594. Entropy: 0.919993.\n",
      "Iteration 5645: Policy loss: -0.000055. Value loss: 0.100025. Entropy: 0.955890.\n",
      "Iteration 5646: Policy loss: -0.005320. Value loss: 0.080405. Entropy: 0.937342.\n",
      "episode: 2710   score: 80.0  epsilon: 1.0    steps: 88  evaluation reward: 259.8\n",
      "episode: 2711   score: 210.0  epsilon: 1.0    steps: 608  evaluation reward: 259.5\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5647: Policy loss: -0.005205. Value loss: 0.276733. Entropy: 0.984239.\n",
      "Iteration 5648: Policy loss: 0.002544. Value loss: 0.147274. Entropy: 0.978916.\n",
      "Iteration 5649: Policy loss: -0.011325. Value loss: 0.108365. Entropy: 0.995719.\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5650: Policy loss: 0.000409. Value loss: 0.166257. Entropy: 0.917337.\n",
      "Iteration 5651: Policy loss: -0.004541. Value loss: 0.116687. Entropy: 0.954828.\n",
      "Iteration 5652: Policy loss: -0.011532. Value loss: 0.062218. Entropy: 0.938781.\n",
      "episode: 2712   score: 180.0  epsilon: 1.0    steps: 784  evaluation reward: 259.15\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5653: Policy loss: 0.003777. Value loss: 0.641977. Entropy: 1.137479.\n",
      "Iteration 5654: Policy loss: 0.007936. Value loss: 0.246256. Entropy: 1.150962.\n",
      "Iteration 5655: Policy loss: -0.001117. Value loss: 0.224244. Entropy: 1.138556.\n",
      "episode: 2713   score: 215.0  epsilon: 1.0    steps: 704  evaluation reward: 260.1\n",
      "episode: 2714   score: 160.0  epsilon: 1.0    steps: 904  evaluation reward: 256.5\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5656: Policy loss: 0.005197. Value loss: 0.332154. Entropy: 1.115149.\n",
      "Iteration 5657: Policy loss: -0.000912. Value loss: 0.204321. Entropy: 1.131647.\n",
      "Iteration 5658: Policy loss: -0.008139. Value loss: 0.153368. Entropy: 1.122554.\n",
      "episode: 2715   score: 210.0  epsilon: 1.0    steps: 280  evaluation reward: 257.25\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5659: Policy loss: -0.004673. Value loss: 0.162008. Entropy: 1.002109.\n",
      "Iteration 5660: Policy loss: -0.004664. Value loss: 0.108130. Entropy: 1.025488.\n",
      "Iteration 5661: Policy loss: -0.013216. Value loss: 0.088574. Entropy: 1.013011.\n",
      "episode: 2716   score: 280.0  epsilon: 1.0    steps: 312  evaluation reward: 258.85\n",
      "episode: 2717   score: 320.0  epsilon: 1.0    steps: 720  evaluation reward: 259.95\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5662: Policy loss: -0.003955. Value loss: 0.268806. Entropy: 1.057698.\n",
      "Iteration 5663: Policy loss: -0.002397. Value loss: 0.126046. Entropy: 1.066105.\n",
      "Iteration 5664: Policy loss: -0.009650. Value loss: 0.103636. Entropy: 1.062844.\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5665: Policy loss: 0.000787. Value loss: 0.241761. Entropy: 0.968940.\n",
      "Iteration 5666: Policy loss: -0.002639. Value loss: 0.123255. Entropy: 0.972082.\n",
      "Iteration 5667: Policy loss: -0.010704. Value loss: 0.084593. Entropy: 0.959991.\n",
      "episode: 2718   score: 180.0  epsilon: 1.0    steps: 16  evaluation reward: 260.25\n",
      "episode: 2719   score: 290.0  epsilon: 1.0    steps: 328  evaluation reward: 261.05\n",
      "episode: 2720   score: 170.0  epsilon: 1.0    steps: 656  evaluation reward: 260.6\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5668: Policy loss: 0.001506. Value loss: 0.331048. Entropy: 1.018926.\n",
      "Iteration 5669: Policy loss: -0.003823. Value loss: 0.153324. Entropy: 1.010576.\n",
      "Iteration 5670: Policy loss: -0.005037. Value loss: 0.121514. Entropy: 1.020145.\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5671: Policy loss: 0.001160. Value loss: 0.141251. Entropy: 0.992137.\n",
      "Iteration 5672: Policy loss: -0.001977. Value loss: 0.095022. Entropy: 0.975916.\n",
      "Iteration 5673: Policy loss: -0.008888. Value loss: 0.082278. Entropy: 0.988689.\n",
      "episode: 2721   score: 180.0  epsilon: 1.0    steps: 8  evaluation reward: 260.6\n",
      "episode: 2722   score: 230.0  epsilon: 1.0    steps: 488  evaluation reward: 260.8\n",
      "episode: 2723   score: 240.0  epsilon: 1.0    steps: 544  evaluation reward: 260.6\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5674: Policy loss: 0.004073. Value loss: 0.247067. Entropy: 1.129080.\n",
      "Iteration 5675: Policy loss: -0.002726. Value loss: 0.162281. Entropy: 1.134319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5676: Policy loss: -0.006852. Value loss: 0.120772. Entropy: 1.138310.\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5677: Policy loss: 0.001882. Value loss: 0.194483. Entropy: 0.864522.\n",
      "Iteration 5678: Policy loss: -0.002346. Value loss: 0.086024. Entropy: 0.863822.\n",
      "Iteration 5679: Policy loss: -0.008856. Value loss: 0.063235. Entropy: 0.871391.\n",
      "episode: 2724   score: 210.0  epsilon: 1.0    steps: 816  evaluation reward: 258.8\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5680: Policy loss: 0.000797. Value loss: 0.276330. Entropy: 1.044468.\n",
      "Iteration 5681: Policy loss: -0.005454. Value loss: 0.117076. Entropy: 1.023476.\n",
      "Iteration 5682: Policy loss: -0.016348. Value loss: 0.100280. Entropy: 1.069551.\n",
      "episode: 2725   score: 215.0  epsilon: 1.0    steps: 560  evaluation reward: 258.85\n",
      "episode: 2726   score: 305.0  epsilon: 1.0    steps: 856  evaluation reward: 260.1\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5683: Policy loss: 0.006313. Value loss: 0.618616. Entropy: 1.042448.\n",
      "Iteration 5684: Policy loss: -0.001703. Value loss: 0.289497. Entropy: 1.054122.\n",
      "Iteration 5685: Policy loss: 0.000302. Value loss: 0.185343. Entropy: 1.057631.\n",
      "episode: 2727   score: 210.0  epsilon: 1.0    steps: 600  evaluation reward: 257.6\n",
      "episode: 2728   score: 210.0  epsilon: 1.0    steps: 848  evaluation reward: 256.35\n",
      "episode: 2729   score: 180.0  epsilon: 1.0    steps: 944  evaluation reward: 256.6\n",
      "episode: 2730   score: 565.0  epsilon: 1.0    steps: 1024  evaluation reward: 260.15\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5686: Policy loss: -0.000455. Value loss: 0.257580. Entropy: 1.031742.\n",
      "Iteration 5687: Policy loss: -0.006569. Value loss: 0.127272. Entropy: 1.030778.\n",
      "Iteration 5688: Policy loss: -0.010489. Value loss: 0.104946. Entropy: 1.045581.\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5689: Policy loss: 0.000701. Value loss: 0.271004. Entropy: 0.731028.\n",
      "Iteration 5690: Policy loss: 0.000750. Value loss: 0.160616. Entropy: 0.714969.\n",
      "Iteration 5691: Policy loss: -0.007632. Value loss: 0.115861. Entropy: 0.719229.\n",
      "episode: 2731   score: 420.0  epsilon: 1.0    steps: 512  evaluation reward: 261.5\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5692: Policy loss: -0.003976. Value loss: 0.424600. Entropy: 1.023587.\n",
      "Iteration 5693: Policy loss: -0.004854. Value loss: 0.143332. Entropy: 1.000954.\n",
      "Iteration 5694: Policy loss: -0.009976. Value loss: 0.117665. Entropy: 1.030564.\n",
      "episode: 2732   score: 110.0  epsilon: 1.0    steps: 696  evaluation reward: 260.4\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5695: Policy loss: 0.001771. Value loss: 0.285351. Entropy: 0.979997.\n",
      "Iteration 5696: Policy loss: -0.002577. Value loss: 0.152114. Entropy: 0.971581.\n",
      "Iteration 5697: Policy loss: -0.008610. Value loss: 0.114645. Entropy: 0.984725.\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5698: Policy loss: 0.001474. Value loss: 0.220677. Entropy: 1.016770.\n",
      "Iteration 5699: Policy loss: -0.002417. Value loss: 0.088861. Entropy: 1.006244.\n",
      "Iteration 5700: Policy loss: -0.005844. Value loss: 0.073535. Entropy: 1.007087.\n",
      "episode: 2733   score: 270.0  epsilon: 1.0    steps: 240  evaluation reward: 261.05\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5701: Policy loss: 0.002352. Value loss: 0.495177. Entropy: 1.214742.\n",
      "Iteration 5702: Policy loss: -0.008758. Value loss: 0.234403. Entropy: 1.213938.\n",
      "Iteration 5703: Policy loss: -0.009543. Value loss: 0.168225. Entropy: 1.213875.\n",
      "episode: 2734   score: 290.0  epsilon: 1.0    steps: 272  evaluation reward: 260.35\n",
      "episode: 2735   score: 245.0  epsilon: 1.0    steps: 840  evaluation reward: 261.1\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5704: Policy loss: -0.000254. Value loss: 0.379928. Entropy: 1.150495.\n",
      "Iteration 5705: Policy loss: -0.003558. Value loss: 0.164604. Entropy: 1.148970.\n",
      "Iteration 5706: Policy loss: -0.010313. Value loss: 0.103309. Entropy: 1.148630.\n",
      "episode: 2736   score: 275.0  epsilon: 1.0    steps: 120  evaluation reward: 262.65\n",
      "episode: 2737   score: 295.0  epsilon: 1.0    steps: 136  evaluation reward: 259.5\n",
      "episode: 2738   score: 160.0  epsilon: 1.0    steps: 808  evaluation reward: 259.0\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5707: Policy loss: -0.001356. Value loss: 0.320575. Entropy: 0.931595.\n",
      "Iteration 5708: Policy loss: 0.002529. Value loss: 0.172597. Entropy: 0.944351.\n",
      "Iteration 5709: Policy loss: -0.011576. Value loss: 0.125011. Entropy: 0.934472.\n",
      "episode: 2739   score: 110.0  epsilon: 1.0    steps: 480  evaluation reward: 255.95\n",
      "episode: 2740   score: 185.0  epsilon: 1.0    steps: 504  evaluation reward: 255.7\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5710: Policy loss: -0.002279. Value loss: 0.267299. Entropy: 1.033245.\n",
      "Iteration 5711: Policy loss: -0.006379. Value loss: 0.138436. Entropy: 1.052541.\n",
      "Iteration 5712: Policy loss: -0.007624. Value loss: 0.097973. Entropy: 1.024441.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5713: Policy loss: 0.001648. Value loss: 0.290797. Entropy: 0.910913.\n",
      "Iteration 5714: Policy loss: -0.003633. Value loss: 0.141656. Entropy: 0.899095.\n",
      "Iteration 5715: Policy loss: -0.007891. Value loss: 0.120748. Entropy: 0.914673.\n",
      "episode: 2741   score: 180.0  epsilon: 1.0    steps: 952  evaluation reward: 253.45\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5716: Policy loss: 0.005945. Value loss: 0.243844. Entropy: 1.068760.\n",
      "Iteration 5717: Policy loss: -0.008622. Value loss: 0.121142. Entropy: 1.062533.\n",
      "Iteration 5718: Policy loss: -0.010491. Value loss: 0.088207. Entropy: 1.066816.\n",
      "episode: 2742   score: 210.0  epsilon: 1.0    steps: 800  evaluation reward: 253.75\n",
      "episode: 2743   score: 210.0  epsilon: 1.0    steps: 896  evaluation reward: 253.25\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5719: Policy loss: 0.001423. Value loss: 0.301642. Entropy: 1.159703.\n",
      "Iteration 5720: Policy loss: -0.002846. Value loss: 0.152741. Entropy: 1.152926.\n",
      "Iteration 5721: Policy loss: -0.009727. Value loss: 0.130394. Entropy: 1.158299.\n",
      "episode: 2744   score: 295.0  epsilon: 1.0    steps: 672  evaluation reward: 253.0\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5722: Policy loss: -0.003037. Value loss: 0.288488. Entropy: 1.064546.\n",
      "Iteration 5723: Policy loss: -0.007851. Value loss: 0.159916. Entropy: 1.072761.\n",
      "Iteration 5724: Policy loss: -0.009939. Value loss: 0.103691. Entropy: 1.070069.\n",
      "episode: 2745   score: 480.0  epsilon: 1.0    steps: 544  evaluation reward: 253.1\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5725: Policy loss: 0.000589. Value loss: 0.163790. Entropy: 1.034232.\n",
      "Iteration 5726: Policy loss: -0.002693. Value loss: 0.089117. Entropy: 1.062012.\n",
      "Iteration 5727: Policy loss: -0.007222. Value loss: 0.058521. Entropy: 1.060372.\n",
      "episode: 2746   score: 260.0  epsilon: 1.0    steps: 568  evaluation reward: 253.65\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5728: Policy loss: 0.007360. Value loss: 0.186158. Entropy: 1.006250.\n",
      "Iteration 5729: Policy loss: -0.002096. Value loss: 0.087151. Entropy: 1.041703.\n",
      "Iteration 5730: Policy loss: -0.003958. Value loss: 0.065753. Entropy: 1.026881.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5731: Policy loss: 0.006106. Value loss: 0.209694. Entropy: 1.091862.\n",
      "Iteration 5732: Policy loss: -0.001501. Value loss: 0.121970. Entropy: 1.081092.\n",
      "Iteration 5733: Policy loss: -0.008898. Value loss: 0.077767. Entropy: 1.098631.\n",
      "episode: 2747   score: 290.0  epsilon: 1.0    steps: 192  evaluation reward: 251.2\n",
      "episode: 2748   score: 235.0  epsilon: 1.0    steps: 968  evaluation reward: 250.6\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5734: Policy loss: 0.000799. Value loss: 0.283923. Entropy: 1.051385.\n",
      "Iteration 5735: Policy loss: -0.008574. Value loss: 0.162849. Entropy: 1.044572.\n",
      "Iteration 5736: Policy loss: -0.015384. Value loss: 0.129794. Entropy: 1.045117.\n",
      "episode: 2749   score: 290.0  epsilon: 1.0    steps: 672  evaluation reward: 248.9\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5737: Policy loss: -0.003079. Value loss: 0.186597. Entropy: 1.151643.\n",
      "Iteration 5738: Policy loss: -0.007189. Value loss: 0.102839. Entropy: 1.147836.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5739: Policy loss: -0.017732. Value loss: 0.080312. Entropy: 1.142750.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5740: Policy loss: 0.000462. Value loss: 0.283313. Entropy: 1.118068.\n",
      "Iteration 5741: Policy loss: -0.002525. Value loss: 0.120279. Entropy: 1.082295.\n",
      "Iteration 5742: Policy loss: -0.011542. Value loss: 0.097398. Entropy: 1.090813.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5743: Policy loss: 0.007972. Value loss: 0.937901. Entropy: 1.175616.\n",
      "Iteration 5744: Policy loss: 0.005941. Value loss: 0.464076. Entropy: 1.173471.\n",
      "Iteration 5745: Policy loss: 0.004398. Value loss: 0.265517. Entropy: 1.187358.\n",
      "episode: 2750   score: 370.0  epsilon: 1.0    steps: 200  evaluation reward: 250.75\n",
      "now time :  2019-02-28 12:03:54.207323\n",
      "episode: 2751   score: 460.0  epsilon: 1.0    steps: 624  evaluation reward: 253.8\n",
      "episode: 2752   score: 395.0  epsilon: 1.0    steps: 928  evaluation reward: 255.35\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5746: Policy loss: 0.000620. Value loss: 0.227514. Entropy: 1.169517.\n",
      "Iteration 5747: Policy loss: -0.003273. Value loss: 0.102455. Entropy: 1.163029.\n",
      "Iteration 5748: Policy loss: -0.006420. Value loss: 0.077503. Entropy: 1.182472.\n",
      "episode: 2753   score: 210.0  epsilon: 1.0    steps: 72  evaluation reward: 255.3\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5749: Policy loss: 0.000369. Value loss: 0.252653. Entropy: 0.921016.\n",
      "Iteration 5750: Policy loss: -0.004848. Value loss: 0.142557. Entropy: 0.872208.\n",
      "Iteration 5751: Policy loss: -0.009476. Value loss: 0.106435. Entropy: 0.884752.\n",
      "episode: 2754   score: 705.0  epsilon: 1.0    steps: 464  evaluation reward: 259.8\n",
      "episode: 2755   score: 215.0  epsilon: 1.0    steps: 1024  evaluation reward: 259.25\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5752: Policy loss: -0.004745. Value loss: 0.288491. Entropy: 1.107404.\n",
      "Iteration 5753: Policy loss: -0.011367. Value loss: 0.161673. Entropy: 1.114606.\n",
      "Iteration 5754: Policy loss: -0.010768. Value loss: 0.119684. Entropy: 1.129542.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5755: Policy loss: 0.002008. Value loss: 0.202400. Entropy: 0.938473.\n",
      "Iteration 5756: Policy loss: -0.006644. Value loss: 0.124473. Entropy: 0.952325.\n",
      "Iteration 5757: Policy loss: -0.009542. Value loss: 0.075974. Entropy: 0.949918.\n",
      "episode: 2756   score: 120.0  epsilon: 1.0    steps: 152  evaluation reward: 257.85\n",
      "episode: 2757   score: 285.0  epsilon: 1.0    steps: 632  evaluation reward: 259.05\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5758: Policy loss: 0.002292. Value loss: 0.376310. Entropy: 1.231574.\n",
      "Iteration 5759: Policy loss: -0.002209. Value loss: 0.182952. Entropy: 1.220942.\n",
      "Iteration 5760: Policy loss: -0.004832. Value loss: 0.125636. Entropy: 1.225646.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5761: Policy loss: -0.000498. Value loss: 0.168569. Entropy: 0.977015.\n",
      "Iteration 5762: Policy loss: -0.004989. Value loss: 0.092411. Entropy: 0.987915.\n",
      "Iteration 5763: Policy loss: -0.010349. Value loss: 0.078151. Entropy: 0.984314.\n",
      "episode: 2758   score: 210.0  epsilon: 1.0    steps: 352  evaluation reward: 259.35\n",
      "episode: 2759   score: 210.0  epsilon: 1.0    steps: 464  evaluation reward: 258.65\n",
      "episode: 2760   score: 210.0  epsilon: 1.0    steps: 520  evaluation reward: 258.45\n",
      "episode: 2761   score: 580.0  epsilon: 1.0    steps: 888  evaluation reward: 261.55\n",
      "episode: 2762   score: 215.0  epsilon: 1.0    steps: 928  evaluation reward: 261.55\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5764: Policy loss: 0.002535. Value loss: 0.277392. Entropy: 1.079241.\n",
      "Iteration 5765: Policy loss: -0.000485. Value loss: 0.154548. Entropy: 1.096759.\n",
      "Iteration 5766: Policy loss: -0.003552. Value loss: 0.112770. Entropy: 1.086255.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5767: Policy loss: -0.001143. Value loss: 0.160714. Entropy: 0.792759.\n",
      "Iteration 5768: Policy loss: -0.003984. Value loss: 0.088228. Entropy: 0.824962.\n",
      "Iteration 5769: Policy loss: -0.009003. Value loss: 0.071669. Entropy: 0.814962.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5770: Policy loss: -0.001210. Value loss: 0.354717. Entropy: 1.199141.\n",
      "Iteration 5771: Policy loss: -0.000520. Value loss: 0.207525. Entropy: 1.194209.\n",
      "Iteration 5772: Policy loss: -0.008156. Value loss: 0.141268. Entropy: 1.189852.\n",
      "episode: 2763   score: 290.0  epsilon: 1.0    steps: 72  evaluation reward: 263.4\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5773: Policy loss: 0.000482. Value loss: 0.289370. Entropy: 0.896834.\n",
      "Iteration 5774: Policy loss: -0.004979. Value loss: 0.158024. Entropy: 0.910769.\n",
      "Iteration 5775: Policy loss: -0.010000. Value loss: 0.096636. Entropy: 0.914073.\n",
      "episode: 2764   score: 210.0  epsilon: 1.0    steps: 680  evaluation reward: 264.15\n",
      "episode: 2765   score: 245.0  epsilon: 1.0    steps: 1016  evaluation reward: 264.2\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5776: Policy loss: 0.007164. Value loss: 0.260528. Entropy: 1.228376.\n",
      "Iteration 5777: Policy loss: 0.000758. Value loss: 0.112710. Entropy: 1.236626.\n",
      "Iteration 5778: Policy loss: -0.011902. Value loss: 0.079467. Entropy: 1.229884.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5779: Policy loss: 0.000421. Value loss: 0.118481. Entropy: 1.064723.\n",
      "Iteration 5780: Policy loss: -0.008401. Value loss: 0.055111. Entropy: 1.048219.\n",
      "Iteration 5781: Policy loss: -0.008466. Value loss: 0.047829. Entropy: 1.071407.\n",
      "episode: 2766   score: 210.0  epsilon: 1.0    steps: 72  evaluation reward: 263.9\n",
      "episode: 2767   score: 215.0  epsilon: 1.0    steps: 416  evaluation reward: 263.4\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5782: Policy loss: -0.002978. Value loss: 0.253867. Entropy: 1.143708.\n",
      "Iteration 5783: Policy loss: -0.008518. Value loss: 0.159536. Entropy: 1.147228.\n",
      "Iteration 5784: Policy loss: -0.010825. Value loss: 0.116784. Entropy: 1.150017.\n",
      "episode: 2768   score: 180.0  epsilon: 1.0    steps: 376  evaluation reward: 260.45\n",
      "episode: 2769   score: 325.0  epsilon: 1.0    steps: 544  evaluation reward: 261.35\n",
      "episode: 2770   score: 410.0  epsilon: 1.0    steps: 592  evaluation reward: 263.5\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5785: Policy loss: 0.001829. Value loss: 0.214365. Entropy: 1.057302.\n",
      "Iteration 5786: Policy loss: -0.001881. Value loss: 0.118083. Entropy: 1.058724.\n",
      "Iteration 5787: Policy loss: -0.008361. Value loss: 0.087437. Entropy: 1.041138.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5788: Policy loss: -0.003302. Value loss: 0.180523. Entropy: 0.832995.\n",
      "Iteration 5789: Policy loss: -0.008606. Value loss: 0.099933. Entropy: 0.805448.\n",
      "Iteration 5790: Policy loss: -0.015523. Value loss: 0.071551. Entropy: 0.822011.\n",
      "episode: 2771   score: 290.0  epsilon: 1.0    steps: 240  evaluation reward: 264.2\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5791: Policy loss: 0.004835. Value loss: 0.125764. Entropy: 1.159952.\n",
      "Iteration 5792: Policy loss: -0.003693. Value loss: 0.062376. Entropy: 1.164533.\n",
      "Iteration 5793: Policy loss: -0.007423. Value loss: 0.050356. Entropy: 1.170209.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5794: Policy loss: 0.000401. Value loss: 0.181039. Entropy: 1.157804.\n",
      "Iteration 5795: Policy loss: -0.009751. Value loss: 0.091040. Entropy: 1.165281.\n",
      "Iteration 5796: Policy loss: -0.012338. Value loss: 0.076089. Entropy: 1.163309.\n",
      "episode: 2772   score: 245.0  epsilon: 1.0    steps: 88  evaluation reward: 265.4\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5797: Policy loss: -0.000525. Value loss: 0.298328. Entropy: 1.259281.\n",
      "Iteration 5798: Policy loss: -0.008913. Value loss: 0.140128. Entropy: 1.247806.\n",
      "Iteration 5799: Policy loss: -0.004541. Value loss: 0.097158. Entropy: 1.257845.\n",
      "episode: 2773   score: 210.0  epsilon: 1.0    steps: 368  evaluation reward: 266.1\n",
      "episode: 2774   score: 215.0  epsilon: 1.0    steps: 448  evaluation reward: 263.1\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5800: Policy loss: -0.000820. Value loss: 0.286067. Entropy: 1.149055.\n",
      "Iteration 5801: Policy loss: -0.002775. Value loss: 0.146935. Entropy: 1.152405.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5802: Policy loss: -0.010755. Value loss: 0.108426. Entropy: 1.161700.\n",
      "episode: 2775   score: 240.0  epsilon: 1.0    steps: 160  evaluation reward: 264.45\n",
      "episode: 2776   score: 215.0  epsilon: 1.0    steps: 168  evaluation reward: 264.05\n",
      "episode: 2777   score: 220.0  epsilon: 1.0    steps: 496  evaluation reward: 261.3\n",
      "episode: 2778   score: 290.0  epsilon: 1.0    steps: 536  evaluation reward: 262.05\n",
      "episode: 2779   score: 345.0  epsilon: 1.0    steps: 584  evaluation reward: 263.4\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5803: Policy loss: 0.001716. Value loss: 0.240913. Entropy: 0.973510.\n",
      "Iteration 5804: Policy loss: -0.004053. Value loss: 0.127286. Entropy: 0.981254.\n",
      "Iteration 5805: Policy loss: -0.008371. Value loss: 0.095484. Entropy: 0.970890.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5806: Policy loss: 0.003066. Value loss: 0.118472. Entropy: 0.851631.\n",
      "Iteration 5807: Policy loss: -0.005664. Value loss: 0.083441. Entropy: 0.851036.\n",
      "Iteration 5808: Policy loss: -0.008144. Value loss: 0.065751. Entropy: 0.861924.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5809: Policy loss: 0.000795. Value loss: 0.206684. Entropy: 0.864394.\n",
      "Iteration 5810: Policy loss: -0.010393. Value loss: 0.109243. Entropy: 0.883965.\n",
      "Iteration 5811: Policy loss: -0.012984. Value loss: 0.093865. Entropy: 0.878742.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5812: Policy loss: 0.003141. Value loss: 0.373246. Entropy: 1.310659.\n",
      "Iteration 5813: Policy loss: -0.008495. Value loss: 0.208089. Entropy: 1.307192.\n",
      "Iteration 5814: Policy loss: -0.011173. Value loss: 0.148870. Entropy: 1.306226.\n",
      "episode: 2780   score: 245.0  epsilon: 1.0    steps: 488  evaluation reward: 264.15\n",
      "episode: 2781   score: 210.0  epsilon: 1.0    steps: 776  evaluation reward: 264.7\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5815: Policy loss: -0.000920. Value loss: 0.302232. Entropy: 1.300821.\n",
      "Iteration 5816: Policy loss: -0.004687. Value loss: 0.172310. Entropy: 1.301312.\n",
      "Iteration 5817: Policy loss: -0.009340. Value loss: 0.091513. Entropy: 1.309017.\n",
      "episode: 2782   score: 210.0  epsilon: 1.0    steps: 168  evaluation reward: 264.55\n",
      "episode: 2783   score: 260.0  epsilon: 1.0    steps: 368  evaluation reward: 263.25\n",
      "episode: 2784   score: 240.0  epsilon: 1.0    steps: 880  evaluation reward: 263.85\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5818: Policy loss: -0.001709. Value loss: 0.164351. Entropy: 1.009204.\n",
      "Iteration 5819: Policy loss: -0.002266. Value loss: 0.097323. Entropy: 0.987612.\n",
      "Iteration 5820: Policy loss: -0.009283. Value loss: 0.079993. Entropy: 1.010621.\n",
      "episode: 2785   score: 330.0  epsilon: 1.0    steps: 496  evaluation reward: 264.2\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5821: Policy loss: -0.000328. Value loss: 0.196888. Entropy: 0.935436.\n",
      "Iteration 5822: Policy loss: -0.010272. Value loss: 0.095285. Entropy: 0.939405.\n",
      "Iteration 5823: Policy loss: -0.012993. Value loss: 0.077696. Entropy: 0.946925.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5824: Policy loss: -0.002092. Value loss: 0.194972. Entropy: 0.936820.\n",
      "Iteration 5825: Policy loss: -0.009830. Value loss: 0.139914. Entropy: 0.935459.\n",
      "Iteration 5826: Policy loss: -0.014348. Value loss: 0.085560. Entropy: 0.929637.\n",
      "episode: 2786   score: 305.0  epsilon: 1.0    steps: 856  evaluation reward: 264.3\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5827: Policy loss: -0.000326. Value loss: 0.316379. Entropy: 1.223572.\n",
      "Iteration 5828: Policy loss: -0.007507. Value loss: 0.113539. Entropy: 1.209588.\n",
      "Iteration 5829: Policy loss: -0.011827. Value loss: 0.092348. Entropy: 1.214342.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5830: Policy loss: 0.000285. Value loss: 0.257347. Entropy: 1.143545.\n",
      "Iteration 5831: Policy loss: -0.008592. Value loss: 0.151151. Entropy: 1.139554.\n",
      "Iteration 5832: Policy loss: -0.007521. Value loss: 0.078756. Entropy: 1.151593.\n",
      "episode: 2787   score: 230.0  epsilon: 1.0    steps: 368  evaluation reward: 262.8\n",
      "episode: 2788   score: 300.0  epsilon: 1.0    steps: 392  evaluation reward: 263.15\n",
      "episode: 2789   score: 215.0  epsilon: 1.0    steps: 832  evaluation reward: 262.1\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5833: Policy loss: 0.003163. Value loss: 0.504588. Entropy: 1.163045.\n",
      "Iteration 5834: Policy loss: 0.002718. Value loss: 0.210218. Entropy: 1.172476.\n",
      "Iteration 5835: Policy loss: -0.007117. Value loss: 0.166970. Entropy: 1.174976.\n",
      "episode: 2790   score: 645.0  epsilon: 1.0    steps: 560  evaluation reward: 265.85\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5836: Policy loss: 0.006151. Value loss: 0.571594. Entropy: 0.959961.\n",
      "Iteration 5837: Policy loss: 0.001198. Value loss: 0.290607. Entropy: 0.963033.\n",
      "Iteration 5838: Policy loss: 0.001186. Value loss: 0.170810. Entropy: 0.944241.\n",
      "episode: 2791   score: 260.0  epsilon: 1.0    steps: 384  evaluation reward: 266.9\n",
      "episode: 2792   score: 335.0  epsilon: 1.0    steps: 912  evaluation reward: 267.05\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5839: Policy loss: 0.002777. Value loss: 0.345234. Entropy: 1.016865.\n",
      "Iteration 5840: Policy loss: -0.000787. Value loss: 0.182312. Entropy: 1.026108.\n",
      "Iteration 5841: Policy loss: -0.006262. Value loss: 0.135455. Entropy: 1.012447.\n",
      "episode: 2793   score: 145.0  epsilon: 1.0    steps: 24  evaluation reward: 265.85\n",
      "episode: 2794   score: 180.0  epsilon: 1.0    steps: 680  evaluation reward: 265.85\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5842: Policy loss: 0.000634. Value loss: 0.327639. Entropy: 0.939830.\n",
      "Iteration 5843: Policy loss: -0.006187. Value loss: 0.162860. Entropy: 0.962859.\n",
      "Iteration 5844: Policy loss: -0.011129. Value loss: 0.119813. Entropy: 0.954454.\n",
      "episode: 2795   score: 165.0  epsilon: 1.0    steps: 840  evaluation reward: 265.95\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5845: Policy loss: 0.005382. Value loss: 0.292589. Entropy: 1.027614.\n",
      "Iteration 5846: Policy loss: -0.004230. Value loss: 0.130533. Entropy: 1.040968.\n",
      "Iteration 5847: Policy loss: -0.005379. Value loss: 0.120917. Entropy: 1.031404.\n",
      "episode: 2796   score: 155.0  epsilon: 1.0    steps: 136  evaluation reward: 261.9\n",
      "episode: 2797   score: 265.0  epsilon: 1.0    steps: 600  evaluation reward: 262.25\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5848: Policy loss: -0.001106. Value loss: 0.311729. Entropy: 0.932274.\n",
      "Iteration 5849: Policy loss: -0.008979. Value loss: 0.147394. Entropy: 0.932601.\n",
      "Iteration 5850: Policy loss: -0.012739. Value loss: 0.115671. Entropy: 0.923349.\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5851: Policy loss: 0.003381. Value loss: 0.377789. Entropy: 0.993796.\n",
      "Iteration 5852: Policy loss: -0.003203. Value loss: 0.192187. Entropy: 1.004581.\n",
      "Iteration 5853: Policy loss: -0.007055. Value loss: 0.154799. Entropy: 1.025369.\n",
      "episode: 2798   score: 320.0  epsilon: 1.0    steps: 104  evaluation reward: 261.3\n",
      "episode: 2799   score: 100.0  epsilon: 1.0    steps: 920  evaluation reward: 260.2\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5854: Policy loss: -0.000710. Value loss: 0.181286. Entropy: 1.132917.\n",
      "Iteration 5855: Policy loss: -0.003584. Value loss: 0.094011. Entropy: 1.115324.\n",
      "Iteration 5856: Policy loss: -0.009843. Value loss: 0.061869. Entropy: 1.111477.\n",
      "episode: 2800   score: 210.0  epsilon: 1.0    steps: 648  evaluation reward: 260.0\n",
      "now time :  2019-02-28 12:05:14.840838\n",
      "episode: 2801   score: 215.0  epsilon: 1.0    steps: 704  evaluation reward: 260.15\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5857: Policy loss: 0.000690. Value loss: 0.318626. Entropy: 0.989276.\n",
      "Iteration 5858: Policy loss: -0.002269. Value loss: 0.133366. Entropy: 0.987553.\n",
      "Iteration 5859: Policy loss: -0.007694. Value loss: 0.104382. Entropy: 1.036291.\n",
      "episode: 2802   score: 285.0  epsilon: 1.0    steps: 416  evaluation reward: 260.6\n",
      "episode: 2803   score: 135.0  epsilon: 1.0    steps: 552  evaluation reward: 258.75\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5860: Policy loss: 0.003645. Value loss: 0.791868. Entropy: 0.946342.\n",
      "Iteration 5861: Policy loss: 0.000678. Value loss: 0.324830. Entropy: 0.921638.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5862: Policy loss: 0.002405. Value loss: 0.227855. Entropy: 0.953943.\n",
      "episode: 2804   score: 225.0  epsilon: 1.0    steps: 872  evaluation reward: 258.85\n",
      "episode: 2805   score: 120.0  epsilon: 1.0    steps: 880  evaluation reward: 257.95\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5863: Policy loss: -0.000114. Value loss: 0.162628. Entropy: 1.013819.\n",
      "Iteration 5864: Policy loss: -0.004847. Value loss: 0.072654. Entropy: 1.026659.\n",
      "Iteration 5865: Policy loss: -0.008556. Value loss: 0.055937. Entropy: 1.030482.\n",
      "episode: 2806   score: 415.0  epsilon: 1.0    steps: 960  evaluation reward: 257.6\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5866: Policy loss: -0.001030. Value loss: 0.293687. Entropy: 0.945384.\n",
      "Iteration 5867: Policy loss: -0.006950. Value loss: 0.138323. Entropy: 0.942663.\n",
      "Iteration 5868: Policy loss: -0.010731. Value loss: 0.095319. Entropy: 0.946417.\n",
      "episode: 2807   score: 125.0  epsilon: 1.0    steps: 920  evaluation reward: 257.0\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5869: Policy loss: 0.001296. Value loss: 0.437409. Entropy: 1.137712.\n",
      "Iteration 5870: Policy loss: -0.006970. Value loss: 0.200651. Entropy: 1.124957.\n",
      "Iteration 5871: Policy loss: -0.006981. Value loss: 0.148597. Entropy: 1.130192.\n",
      "episode: 2808   score: 365.0  epsilon: 1.0    steps: 872  evaluation reward: 259.0\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5872: Policy loss: 0.001633. Value loss: 0.526883. Entropy: 1.071581.\n",
      "Iteration 5873: Policy loss: -0.000933. Value loss: 0.187174. Entropy: 1.072987.\n",
      "Iteration 5874: Policy loss: -0.009381. Value loss: 0.155733. Entropy: 1.068943.\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5875: Policy loss: 0.000985. Value loss: 0.256543. Entropy: 1.049221.\n",
      "Iteration 5876: Policy loss: -0.001662. Value loss: 0.127947. Entropy: 1.040607.\n",
      "Iteration 5877: Policy loss: -0.003645. Value loss: 0.087421. Entropy: 1.040942.\n",
      "episode: 2809   score: 150.0  epsilon: 1.0    steps: 464  evaluation reward: 255.5\n",
      "episode: 2810   score: 350.0  epsilon: 1.0    steps: 792  evaluation reward: 258.2\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5878: Policy loss: 0.005826. Value loss: 0.385519. Entropy: 1.127137.\n",
      "Iteration 5879: Policy loss: 0.004377. Value loss: 0.143230. Entropy: 1.120813.\n",
      "Iteration 5880: Policy loss: -0.005895. Value loss: 0.109571. Entropy: 1.130694.\n",
      "episode: 2811   score: 225.0  epsilon: 1.0    steps: 24  evaluation reward: 258.35\n",
      "episode: 2812   score: 365.0  epsilon: 1.0    steps: 864  evaluation reward: 260.2\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5881: Policy loss: 0.005618. Value loss: 0.265511. Entropy: 0.899103.\n",
      "Iteration 5882: Policy loss: -0.006007. Value loss: 0.115483. Entropy: 0.906417.\n",
      "Iteration 5883: Policy loss: -0.008634. Value loss: 0.069975. Entropy: 0.917946.\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5884: Policy loss: 0.004060. Value loss: 0.284490. Entropy: 1.097409.\n",
      "Iteration 5885: Policy loss: -0.002171. Value loss: 0.159703. Entropy: 1.102988.\n",
      "Iteration 5886: Policy loss: -0.010246. Value loss: 0.115592. Entropy: 1.115027.\n",
      "episode: 2813   score: 230.0  epsilon: 1.0    steps: 72  evaluation reward: 260.35\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5887: Policy loss: -0.000528. Value loss: 0.468038. Entropy: 0.976851.\n",
      "Iteration 5888: Policy loss: -0.010388. Value loss: 0.212882. Entropy: 0.981601.\n",
      "Iteration 5889: Policy loss: -0.014236. Value loss: 0.118470. Entropy: 0.985290.\n",
      "episode: 2814   score: 360.0  epsilon: 1.0    steps: 896  evaluation reward: 262.35\n",
      "episode: 2815   score: 105.0  epsilon: 1.0    steps: 1024  evaluation reward: 261.3\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5890: Policy loss: 0.000169. Value loss: 0.409243. Entropy: 1.196382.\n",
      "Iteration 5891: Policy loss: -0.005379. Value loss: 0.134004. Entropy: 1.176660.\n",
      "Iteration 5892: Policy loss: -0.010472. Value loss: 0.106835. Entropy: 1.192657.\n",
      "episode: 2816   score: 265.0  epsilon: 1.0    steps: 328  evaluation reward: 261.15\n",
      "episode: 2817   score: 290.0  epsilon: 1.0    steps: 752  evaluation reward: 260.85\n",
      "episode: 2818   score: 210.0  epsilon: 1.0    steps: 888  evaluation reward: 261.15\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5893: Policy loss: 0.002531. Value loss: 0.344380. Entropy: 0.968861.\n",
      "Iteration 5894: Policy loss: -0.002753. Value loss: 0.202273. Entropy: 0.977605.\n",
      "Iteration 5895: Policy loss: -0.006043. Value loss: 0.134094. Entropy: 0.988867.\n",
      "episode: 2819   score: 235.0  epsilon: 1.0    steps: 344  evaluation reward: 260.6\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5896: Policy loss: 0.001619. Value loss: 0.275280. Entropy: 0.912772.\n",
      "Iteration 5897: Policy loss: -0.001734. Value loss: 0.151988. Entropy: 0.925094.\n",
      "Iteration 5898: Policy loss: -0.004184. Value loss: 0.131283. Entropy: 0.904328.\n",
      "episode: 2820   score: 335.0  epsilon: 1.0    steps: 152  evaluation reward: 262.25\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5899: Policy loss: 0.001044. Value loss: 0.423621. Entropy: 1.039337.\n",
      "Iteration 5900: Policy loss: -0.002888. Value loss: 0.207683. Entropy: 1.038236.\n",
      "Iteration 5901: Policy loss: -0.009565. Value loss: 0.159791. Entropy: 1.046410.\n",
      "episode: 2821   score: 180.0  epsilon: 1.0    steps: 840  evaluation reward: 262.25\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5902: Policy loss: -0.000170. Value loss: 0.294941. Entropy: 0.924217.\n",
      "Iteration 5903: Policy loss: -0.003616. Value loss: 0.189059. Entropy: 0.933177.\n",
      "Iteration 5904: Policy loss: -0.010723. Value loss: 0.115774. Entropy: 0.961387.\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5905: Policy loss: 0.003673. Value loss: 0.441492. Entropy: 1.154704.\n",
      "Iteration 5906: Policy loss: -0.003980. Value loss: 0.175049. Entropy: 1.168322.\n",
      "Iteration 5907: Policy loss: -0.008955. Value loss: 0.151310. Entropy: 1.156588.\n",
      "episode: 2822   score: 140.0  epsilon: 1.0    steps: 72  evaluation reward: 261.35\n",
      "episode: 2823   score: 215.0  epsilon: 1.0    steps: 640  evaluation reward: 261.1\n",
      "episode: 2824   score: 195.0  epsilon: 1.0    steps: 704  evaluation reward: 260.95\n",
      "episode: 2825   score: 425.0  epsilon: 1.0    steps: 984  evaluation reward: 263.05\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5908: Policy loss: 0.007056. Value loss: 0.551390. Entropy: 1.050191.\n",
      "Iteration 5909: Policy loss: -0.002841. Value loss: 0.290293. Entropy: 1.091765.\n",
      "Iteration 5910: Policy loss: -0.005504. Value loss: 0.205950. Entropy: 1.072259.\n",
      "episode: 2826   score: 270.0  epsilon: 1.0    steps: 896  evaluation reward: 262.7\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5911: Policy loss: 0.005274. Value loss: 0.445177. Entropy: 0.895256.\n",
      "Iteration 5912: Policy loss: -0.001978. Value loss: 0.207678. Entropy: 0.922241.\n",
      "Iteration 5913: Policy loss: -0.011108. Value loss: 0.160682. Entropy: 0.892302.\n",
      "episode: 2827   score: 260.0  epsilon: 1.0    steps: 288  evaluation reward: 263.2\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5914: Policy loss: 0.000136. Value loss: 0.349186. Entropy: 1.042342.\n",
      "Iteration 5915: Policy loss: -0.000726. Value loss: 0.196791. Entropy: 1.002631.\n",
      "Iteration 5916: Policy loss: -0.008194. Value loss: 0.147515. Entropy: 1.044076.\n",
      "episode: 2828   score: 395.0  epsilon: 1.0    steps: 256  evaluation reward: 265.05\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5917: Policy loss: -0.002173. Value loss: 0.420950. Entropy: 0.994070.\n",
      "Iteration 5918: Policy loss: -0.003723. Value loss: 0.256619. Entropy: 1.003621.\n",
      "Iteration 5919: Policy loss: -0.008129. Value loss: 0.166233. Entropy: 1.013299.\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5920: Policy loss: 0.001452. Value loss: 0.337850. Entropy: 1.136907.\n",
      "Iteration 5921: Policy loss: -0.002385. Value loss: 0.138312. Entropy: 1.159615.\n",
      "Iteration 5922: Policy loss: -0.005722. Value loss: 0.128116. Entropy: 1.151620.\n",
      "episode: 2829   score: 215.0  epsilon: 1.0    steps: 952  evaluation reward: 265.4\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5923: Policy loss: 0.002826. Value loss: 0.351761. Entropy: 1.196111.\n",
      "Iteration 5924: Policy loss: -0.004857. Value loss: 0.184816. Entropy: 1.192288.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5925: Policy loss: -0.009737. Value loss: 0.094563. Entropy: 1.194897.\n",
      "episode: 2830   score: 210.0  epsilon: 1.0    steps: 344  evaluation reward: 261.85\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5926: Policy loss: 0.007060. Value loss: 0.707462. Entropy: 1.195698.\n",
      "Iteration 5927: Policy loss: 0.001168. Value loss: 0.272476. Entropy: 1.202683.\n",
      "Iteration 5928: Policy loss: -0.000272. Value loss: 0.217801. Entropy: 1.200408.\n",
      "episode: 2831   score: 260.0  epsilon: 1.0    steps: 32  evaluation reward: 260.25\n",
      "episode: 2832   score: 250.0  epsilon: 1.0    steps: 184  evaluation reward: 261.65\n",
      "episode: 2833   score: 265.0  epsilon: 1.0    steps: 232  evaluation reward: 261.6\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5929: Policy loss: 0.001185. Value loss: 0.219822. Entropy: 1.051885.\n",
      "Iteration 5930: Policy loss: -0.008738. Value loss: 0.119909. Entropy: 1.048384.\n",
      "Iteration 5931: Policy loss: -0.010422. Value loss: 0.106625. Entropy: 1.031251.\n",
      "episode: 2834   score: 380.0  epsilon: 1.0    steps: 152  evaluation reward: 262.5\n",
      "episode: 2835   score: 265.0  epsilon: 1.0    steps: 192  evaluation reward: 262.7\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5932: Policy loss: -0.000752. Value loss: 0.355399. Entropy: 0.986659.\n",
      "Iteration 5933: Policy loss: -0.002775. Value loss: 0.142560. Entropy: 0.978451.\n",
      "Iteration 5934: Policy loss: -0.005031. Value loss: 0.119952. Entropy: 1.002700.\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5935: Policy loss: 0.000899. Value loss: 0.259997. Entropy: 0.853352.\n",
      "Iteration 5936: Policy loss: -0.004552. Value loss: 0.140159. Entropy: 0.870733.\n",
      "Iteration 5937: Policy loss: -0.008727. Value loss: 0.102951. Entropy: 0.865601.\n",
      "episode: 2836   score: 135.0  epsilon: 1.0    steps: 152  evaluation reward: 261.3\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5938: Policy loss: 0.000403. Value loss: 0.326361. Entropy: 1.080411.\n",
      "Iteration 5939: Policy loss: -0.003585. Value loss: 0.184296. Entropy: 1.074630.\n",
      "Iteration 5940: Policy loss: -0.005448. Value loss: 0.117351. Entropy: 1.083141.\n",
      "episode: 2837   score: 215.0  epsilon: 1.0    steps: 304  evaluation reward: 260.5\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5941: Policy loss: 0.004769. Value loss: 0.367309. Entropy: 1.184280.\n",
      "Iteration 5942: Policy loss: -0.003005. Value loss: 0.182096. Entropy: 1.195178.\n",
      "Iteration 5943: Policy loss: -0.013586. Value loss: 0.137736. Entropy: 1.184057.\n",
      "episode: 2838   score: 550.0  epsilon: 1.0    steps: 80  evaluation reward: 264.4\n",
      "episode: 2839   score: 225.0  epsilon: 1.0    steps: 424  evaluation reward: 265.55\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5944: Policy loss: 0.003184. Value loss: 0.267087. Entropy: 1.031104.\n",
      "Iteration 5945: Policy loss: -0.001467. Value loss: 0.139130. Entropy: 1.046921.\n",
      "Iteration 5946: Policy loss: -0.006290. Value loss: 0.100339. Entropy: 1.045019.\n",
      "episode: 2840   score: 320.0  epsilon: 1.0    steps: 784  evaluation reward: 266.9\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5947: Policy loss: -0.000368. Value loss: 0.222539. Entropy: 1.079345.\n",
      "Iteration 5948: Policy loss: -0.006196. Value loss: 0.117962. Entropy: 1.106290.\n",
      "Iteration 5949: Policy loss: -0.011046. Value loss: 0.084481. Entropy: 1.103851.\n",
      "episode: 2841   score: 250.0  epsilon: 1.0    steps: 120  evaluation reward: 267.6\n",
      "episode: 2842   score: 315.0  epsilon: 1.0    steps: 688  evaluation reward: 268.65\n",
      "episode: 2843   score: 285.0  epsilon: 1.0    steps: 1024  evaluation reward: 269.4\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5950: Policy loss: 0.001286. Value loss: 0.252721. Entropy: 1.096796.\n",
      "Iteration 5951: Policy loss: -0.007555. Value loss: 0.174787. Entropy: 1.093651.\n",
      "Iteration 5952: Policy loss: -0.009902. Value loss: 0.115731. Entropy: 1.103112.\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5953: Policy loss: 0.000327. Value loss: 0.151518. Entropy: 0.915719.\n",
      "Iteration 5954: Policy loss: -0.008173. Value loss: 0.087272. Entropy: 0.926074.\n",
      "Iteration 5955: Policy loss: -0.010014. Value loss: 0.069616. Entropy: 0.913455.\n",
      "episode: 2844   score: 210.0  epsilon: 1.0    steps: 344  evaluation reward: 268.55\n",
      "episode: 2845   score: 215.0  epsilon: 1.0    steps: 648  evaluation reward: 265.9\n",
      "episode: 2846   score: 185.0  epsilon: 1.0    steps: 736  evaluation reward: 265.15\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5956: Policy loss: 0.001778. Value loss: 0.223013. Entropy: 1.145543.\n",
      "Iteration 5957: Policy loss: -0.005630. Value loss: 0.136492. Entropy: 1.150742.\n",
      "Iteration 5958: Policy loss: -0.010154. Value loss: 0.117186. Entropy: 1.142343.\n",
      "episode: 2847   score: 145.0  epsilon: 1.0    steps: 320  evaluation reward: 263.7\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5959: Policy loss: 0.000904. Value loss: 0.182399. Entropy: 0.795722.\n",
      "Iteration 5960: Policy loss: -0.004891. Value loss: 0.142726. Entropy: 0.843802.\n",
      "Iteration 5961: Policy loss: -0.009674. Value loss: 0.091094. Entropy: 0.821259.\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5962: Policy loss: -0.002347. Value loss: 0.147888. Entropy: 1.114823.\n",
      "Iteration 5963: Policy loss: -0.007433. Value loss: 0.091532. Entropy: 1.103848.\n",
      "Iteration 5964: Policy loss: -0.011216. Value loss: 0.056797. Entropy: 1.107724.\n",
      "episode: 2848   score: 265.0  epsilon: 1.0    steps: 528  evaluation reward: 264.0\n",
      "episode: 2849   score: 225.0  epsilon: 1.0    steps: 904  evaluation reward: 263.35\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5965: Policy loss: 0.000404. Value loss: 0.190561. Entropy: 1.153210.\n",
      "Iteration 5966: Policy loss: -0.005974. Value loss: 0.113322. Entropy: 1.141567.\n",
      "Iteration 5967: Policy loss: -0.008999. Value loss: 0.085915. Entropy: 1.156415.\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5968: Policy loss: 0.002085. Value loss: 0.335216. Entropy: 1.096804.\n",
      "Iteration 5969: Policy loss: -0.008454. Value loss: 0.158715. Entropy: 1.113665.\n",
      "Iteration 5970: Policy loss: -0.013082. Value loss: 0.085846. Entropy: 1.092301.\n",
      "episode: 2850   score: 370.0  epsilon: 1.0    steps: 24  evaluation reward: 263.35\n",
      "now time :  2019-02-28 12:06:37.158910\n",
      "episode: 2851   score: 230.0  epsilon: 1.0    steps: 608  evaluation reward: 261.05\n",
      "episode: 2852   score: 210.0  epsilon: 1.0    steps: 936  evaluation reward: 259.2\n",
      "episode: 2853   score: 275.0  epsilon: 1.0    steps: 992  evaluation reward: 259.85\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5971: Policy loss: 0.007157. Value loss: 0.338255. Entropy: 1.201984.\n",
      "Iteration 5972: Policy loss: -0.007207. Value loss: 0.164222. Entropy: 1.204276.\n",
      "Iteration 5973: Policy loss: -0.010144. Value loss: 0.102927. Entropy: 1.199656.\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5974: Policy loss: -0.000143. Value loss: 0.295488. Entropy: 0.890862.\n",
      "Iteration 5975: Policy loss: -0.007911. Value loss: 0.145919. Entropy: 0.877527.\n",
      "Iteration 5976: Policy loss: -0.010004. Value loss: 0.114482. Entropy: 0.863558.\n",
      "episode: 2854   score: 250.0  epsilon: 1.0    steps: 1024  evaluation reward: 255.3\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5977: Policy loss: -0.000868. Value loss: 0.185059. Entropy: 1.134619.\n",
      "Iteration 5978: Policy loss: -0.005997. Value loss: 0.091226. Entropy: 1.121305.\n",
      "Iteration 5979: Policy loss: -0.005375. Value loss: 0.065689. Entropy: 1.134175.\n",
      "episode: 2855   score: 260.0  epsilon: 1.0    steps: 776  evaluation reward: 255.75\n",
      "episode: 2856   score: 325.0  epsilon: 1.0    steps: 856  evaluation reward: 257.8\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5980: Policy loss: 0.000960. Value loss: 0.338394. Entropy: 1.105474.\n",
      "Iteration 5981: Policy loss: -0.006563. Value loss: 0.168658. Entropy: 1.088514.\n",
      "Iteration 5982: Policy loss: -0.007882. Value loss: 0.113806. Entropy: 1.075040.\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5983: Policy loss: 0.001121. Value loss: 0.200923. Entropy: 1.037775.\n",
      "Iteration 5984: Policy loss: -0.001234. Value loss: 0.093065. Entropy: 1.056648.\n",
      "Iteration 5985: Policy loss: -0.008640. Value loss: 0.065944. Entropy: 1.036031.\n",
      "episode: 2857   score: 320.0  epsilon: 1.0    steps: 424  evaluation reward: 258.15\n",
      "episode: 2858   score: 155.0  epsilon: 1.0    steps: 504  evaluation reward: 257.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5986: Policy loss: 0.001504. Value loss: 0.324335. Entropy: 1.167762.\n",
      "Iteration 5987: Policy loss: 0.003182. Value loss: 0.132005. Entropy: 1.173423.\n",
      "Iteration 5988: Policy loss: -0.007470. Value loss: 0.094723. Entropy: 1.172518.\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5989: Policy loss: 0.003929. Value loss: 0.154410. Entropy: 0.900909.\n",
      "Iteration 5990: Policy loss: -0.006917. Value loss: 0.067720. Entropy: 0.876932.\n",
      "Iteration 5991: Policy loss: -0.009116. Value loss: 0.062392. Entropy: 0.905743.\n",
      "episode: 2859   score: 185.0  epsilon: 1.0    steps: 144  evaluation reward: 257.35\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5992: Policy loss: 0.002105. Value loss: 0.280570. Entropy: 1.134702.\n",
      "Iteration 5993: Policy loss: -0.003963. Value loss: 0.112884. Entropy: 1.101055.\n",
      "Iteration 5994: Policy loss: -0.008649. Value loss: 0.084064. Entropy: 1.144722.\n",
      "episode: 2860   score: 270.0  epsilon: 1.0    steps: 232  evaluation reward: 257.95\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5995: Policy loss: 0.004221. Value loss: 0.404853. Entropy: 1.205966.\n",
      "Iteration 5996: Policy loss: -0.001844. Value loss: 0.166424. Entropy: 1.212349.\n",
      "Iteration 5997: Policy loss: -0.008697. Value loss: 0.099838. Entropy: 1.212645.\n",
      "episode: 2861   score: 210.0  epsilon: 1.0    steps: 984  evaluation reward: 254.25\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5998: Policy loss: 0.001753. Value loss: 0.351125. Entropy: 1.223359.\n",
      "Iteration 5999: Policy loss: -0.004006. Value loss: 0.187711. Entropy: 1.212183.\n",
      "Iteration 6000: Policy loss: -0.008856. Value loss: 0.161723. Entropy: 1.221717.\n",
      "episode: 2862   score: 240.0  epsilon: 1.0    steps: 8  evaluation reward: 254.5\n",
      "episode: 2863   score: 105.0  epsilon: 1.0    steps: 24  evaluation reward: 252.65\n",
      "episode: 2864   score: 210.0  epsilon: 1.0    steps: 752  evaluation reward: 252.65\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6001: Policy loss: 0.000747. Value loss: 0.151641. Entropy: 1.049949.\n",
      "Iteration 6002: Policy loss: -0.006569. Value loss: 0.089912. Entropy: 1.053231.\n",
      "Iteration 6003: Policy loss: -0.011561. Value loss: 0.063256. Entropy: 1.038658.\n",
      "episode: 2865   score: 490.0  epsilon: 1.0    steps: 424  evaluation reward: 255.1\n",
      "episode: 2866   score: 225.0  epsilon: 1.0    steps: 640  evaluation reward: 255.25\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6004: Policy loss: -0.000098. Value loss: 0.158086. Entropy: 1.010487.\n",
      "Iteration 6005: Policy loss: -0.008136. Value loss: 0.102135. Entropy: 1.023904.\n",
      "Iteration 6006: Policy loss: -0.009776. Value loss: 0.099297. Entropy: 1.021440.\n",
      "episode: 2867   score: 320.0  epsilon: 1.0    steps: 720  evaluation reward: 256.3\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6007: Policy loss: 0.000090. Value loss: 0.139951. Entropy: 0.808806.\n",
      "Iteration 6008: Policy loss: -0.007930. Value loss: 0.066402. Entropy: 0.829284.\n",
      "Iteration 6009: Policy loss: -0.011615. Value loss: 0.055716. Entropy: 0.816021.\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6010: Policy loss: -0.002200. Value loss: 0.155129. Entropy: 1.040262.\n",
      "Iteration 6011: Policy loss: -0.005159. Value loss: 0.074311. Entropy: 1.059017.\n",
      "Iteration 6012: Policy loss: -0.011630. Value loss: 0.080563. Entropy: 1.057802.\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6013: Policy loss: 0.002358. Value loss: 0.210685. Entropy: 1.191757.\n",
      "Iteration 6014: Policy loss: -0.005014. Value loss: 0.086466. Entropy: 1.198304.\n",
      "Iteration 6015: Policy loss: -0.008280. Value loss: 0.067179. Entropy: 1.183531.\n",
      "episode: 2868   score: 210.0  epsilon: 1.0    steps: 40  evaluation reward: 256.6\n",
      "episode: 2869   score: 125.0  epsilon: 1.0    steps: 656  evaluation reward: 254.6\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6016: Policy loss: 0.002142. Value loss: 0.252162. Entropy: 1.119998.\n",
      "Iteration 6017: Policy loss: -0.008616. Value loss: 0.105995. Entropy: 1.126867.\n",
      "Iteration 6018: Policy loss: -0.011212. Value loss: 0.075974. Entropy: 1.135094.\n",
      "episode: 2870   score: 365.0  epsilon: 1.0    steps: 104  evaluation reward: 254.15\n",
      "episode: 2871   score: 240.0  epsilon: 1.0    steps: 960  evaluation reward: 253.65\n",
      "episode: 2872   score: 255.0  epsilon: 1.0    steps: 976  evaluation reward: 253.75\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6019: Policy loss: 0.002509. Value loss: 0.234938. Entropy: 1.067977.\n",
      "Iteration 6020: Policy loss: -0.003001. Value loss: 0.102464. Entropy: 1.092674.\n",
      "Iteration 6021: Policy loss: -0.003473. Value loss: 0.067731. Entropy: 1.078475.\n",
      "episode: 2873   score: 405.0  epsilon: 1.0    steps: 760  evaluation reward: 255.7\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6022: Policy loss: -0.000603. Value loss: 0.296183. Entropy: 1.044926.\n",
      "Iteration 6023: Policy loss: 0.002666. Value loss: 0.129756. Entropy: 1.021043.\n",
      "Iteration 6024: Policy loss: -0.006690. Value loss: 0.084577. Entropy: 1.010156.\n",
      "episode: 2874   score: 245.0  epsilon: 1.0    steps: 584  evaluation reward: 256.0\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6025: Policy loss: -0.000732. Value loss: 0.173370. Entropy: 0.902516.\n",
      "Iteration 6026: Policy loss: -0.006413. Value loss: 0.088843. Entropy: 0.929232.\n",
      "Iteration 6027: Policy loss: -0.010896. Value loss: 0.061218. Entropy: 0.934258.\n",
      "episode: 2875   score: 285.0  epsilon: 1.0    steps: 576  evaluation reward: 256.45\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6028: Policy loss: -0.001005. Value loss: 0.218381. Entropy: 1.046393.\n",
      "Iteration 6029: Policy loss: -0.004943. Value loss: 0.103548. Entropy: 1.027331.\n",
      "Iteration 6030: Policy loss: -0.009592. Value loss: 0.088716. Entropy: 1.042556.\n",
      "episode: 2876   score: 210.0  epsilon: 1.0    steps: 896  evaluation reward: 256.4\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6031: Policy loss: 0.001736. Value loss: 0.204882. Entropy: 1.151944.\n",
      "Iteration 6032: Policy loss: -0.003098. Value loss: 0.084244. Entropy: 1.152356.\n",
      "Iteration 6033: Policy loss: -0.004261. Value loss: 0.048776. Entropy: 1.150698.\n",
      "episode: 2877   score: 225.0  epsilon: 1.0    steps: 8  evaluation reward: 256.45\n",
      "episode: 2878   score: 195.0  epsilon: 1.0    steps: 432  evaluation reward: 255.5\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6034: Policy loss: 0.002246. Value loss: 0.112708. Entropy: 1.078109.\n",
      "Iteration 6035: Policy loss: -0.003047. Value loss: 0.063894. Entropy: 1.059011.\n",
      "Iteration 6036: Policy loss: -0.003837. Value loss: 0.048391. Entropy: 1.083346.\n",
      "episode: 2879   score: 260.0  epsilon: 1.0    steps: 8  evaluation reward: 254.65\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6037: Policy loss: 0.001178. Value loss: 0.298371. Entropy: 1.021306.\n",
      "Iteration 6038: Policy loss: -0.004186. Value loss: 0.161841. Entropy: 0.998851.\n",
      "Iteration 6039: Policy loss: -0.005261. Value loss: 0.107627. Entropy: 1.017834.\n",
      "episode: 2880   score: 215.0  epsilon: 1.0    steps: 40  evaluation reward: 254.35\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6040: Policy loss: 0.000892. Value loss: 0.199115. Entropy: 1.047408.\n",
      "Iteration 6041: Policy loss: -0.003563. Value loss: 0.112943. Entropy: 1.047644.\n",
      "Iteration 6042: Policy loss: -0.010420. Value loss: 0.084996. Entropy: 1.060651.\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6043: Policy loss: -0.001470. Value loss: 0.611929. Entropy: 1.203581.\n",
      "Iteration 6044: Policy loss: -0.000371. Value loss: 0.295636. Entropy: 1.198292.\n",
      "Iteration 6045: Policy loss: -0.006787. Value loss: 0.226690. Entropy: 1.195780.\n",
      "episode: 2881   score: 235.0  epsilon: 1.0    steps: 800  evaluation reward: 254.6\n",
      "episode: 2882   score: 105.0  epsilon: 1.0    steps: 864  evaluation reward: 253.55\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6046: Policy loss: 0.006329. Value loss: 0.629539. Entropy: 1.236665.\n",
      "Iteration 6047: Policy loss: -0.002349. Value loss: 0.307067. Entropy: 1.233237.\n",
      "Iteration 6048: Policy loss: -0.004156. Value loss: 0.222691. Entropy: 1.236283.\n",
      "episode: 2883   score: 120.0  epsilon: 1.0    steps: 272  evaluation reward: 252.15\n",
      "episode: 2884   score: 220.0  epsilon: 1.0    steps: 352  evaluation reward: 251.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2885   score: 155.0  epsilon: 1.0    steps: 448  evaluation reward: 250.2\n",
      "episode: 2886   score: 320.0  epsilon: 1.0    steps: 504  evaluation reward: 250.35\n",
      "episode: 2887   score: 565.0  epsilon: 1.0    steps: 848  evaluation reward: 253.7\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6049: Policy loss: 0.000801. Value loss: 0.445579. Entropy: 1.061063.\n",
      "Iteration 6050: Policy loss: -0.001308. Value loss: 0.214091. Entropy: 1.042053.\n",
      "Iteration 6051: Policy loss: -0.006349. Value loss: 0.196517. Entropy: 1.064472.\n",
      "episode: 2888   score: 320.0  epsilon: 1.0    steps: 80  evaluation reward: 253.9\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6052: Policy loss: -0.001839. Value loss: 0.461887. Entropy: 0.752401.\n",
      "Iteration 6053: Policy loss: -0.003181. Value loss: 0.267827. Entropy: 0.799476.\n",
      "Iteration 6054: Policy loss: -0.006178. Value loss: 0.186457. Entropy: 0.774344.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6055: Policy loss: 0.000500. Value loss: 0.273771. Entropy: 0.866723.\n",
      "Iteration 6056: Policy loss: -0.006607. Value loss: 0.138232. Entropy: 0.901203.\n",
      "Iteration 6057: Policy loss: -0.011746. Value loss: 0.090263. Entropy: 0.895353.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6058: Policy loss: 0.003818. Value loss: 0.780827. Entropy: 1.092537.\n",
      "Iteration 6059: Policy loss: 0.002377. Value loss: 0.413098. Entropy: 1.097559.\n",
      "Iteration 6060: Policy loss: -0.001238. Value loss: 0.294916. Entropy: 1.094246.\n",
      "episode: 2889   score: 180.0  epsilon: 1.0    steps: 232  evaluation reward: 253.55\n",
      "episode: 2890   score: 225.0  epsilon: 1.0    steps: 672  evaluation reward: 249.35\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6061: Policy loss: 0.004696. Value loss: 0.567626. Entropy: 1.274850.\n",
      "Iteration 6062: Policy loss: 0.002898. Value loss: 0.270324. Entropy: 1.277430.\n",
      "Iteration 6063: Policy loss: -0.005964. Value loss: 0.225808. Entropy: 1.273059.\n",
      "episode: 2891   score: 180.0  epsilon: 1.0    steps: 128  evaluation reward: 248.55\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6064: Policy loss: 0.002415. Value loss: 0.271118. Entropy: 1.097660.\n",
      "Iteration 6065: Policy loss: -0.001739. Value loss: 0.158308. Entropy: 1.104211.\n",
      "Iteration 6066: Policy loss: -0.012025. Value loss: 0.114917. Entropy: 1.086538.\n",
      "episode: 2892   score: 300.0  epsilon: 1.0    steps: 720  evaluation reward: 248.2\n",
      "episode: 2893   score: 240.0  epsilon: 1.0    steps: 1000  evaluation reward: 249.15\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6067: Policy loss: 0.002493. Value loss: 0.404074. Entropy: 1.131081.\n",
      "Iteration 6068: Policy loss: -0.002543. Value loss: 0.147514. Entropy: 1.121639.\n",
      "Iteration 6069: Policy loss: -0.007152. Value loss: 0.131757. Entropy: 1.125638.\n",
      "episode: 2894   score: 320.0  epsilon: 1.0    steps: 352  evaluation reward: 250.55\n",
      "episode: 2895   score: 325.0  epsilon: 1.0    steps: 616  evaluation reward: 252.15\n",
      "episode: 2896   score: 650.0  epsilon: 1.0    steps: 664  evaluation reward: 257.1\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6070: Policy loss: -0.000414. Value loss: 0.245364. Entropy: 0.983892.\n",
      "Iteration 6071: Policy loss: -0.004465. Value loss: 0.189846. Entropy: 1.009624.\n",
      "Iteration 6072: Policy loss: -0.011158. Value loss: 0.143721. Entropy: 0.997603.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6073: Policy loss: -0.001353. Value loss: 0.150795. Entropy: 0.953028.\n",
      "Iteration 6074: Policy loss: -0.002118. Value loss: 0.090217. Entropy: 0.971467.\n",
      "Iteration 6075: Policy loss: -0.009174. Value loss: 0.081404. Entropy: 0.959769.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6076: Policy loss: -0.000164. Value loss: 0.388062. Entropy: 1.100100.\n",
      "Iteration 6077: Policy loss: -0.008333. Value loss: 0.180639. Entropy: 1.102416.\n",
      "Iteration 6078: Policy loss: -0.014388. Value loss: 0.129383. Entropy: 1.094755.\n",
      "episode: 2897   score: 225.0  epsilon: 1.0    steps: 712  evaluation reward: 256.7\n",
      "episode: 2898   score: 180.0  epsilon: 1.0    steps: 968  evaluation reward: 255.3\n",
      "episode: 2899   score: 300.0  epsilon: 1.0    steps: 976  evaluation reward: 257.3\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6079: Policy loss: 0.002935. Value loss: 0.296699. Entropy: 1.103316.\n",
      "Iteration 6080: Policy loss: -0.004781. Value loss: 0.132822. Entropy: 1.101727.\n",
      "Iteration 6081: Policy loss: -0.009207. Value loss: 0.113983. Entropy: 1.118147.\n",
      "episode: 2900   score: 150.0  epsilon: 1.0    steps: 696  evaluation reward: 256.7\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6082: Policy loss: 0.002623. Value loss: 0.370467. Entropy: 1.032075.\n",
      "Iteration 6083: Policy loss: -0.001102. Value loss: 0.167869. Entropy: 1.065518.\n",
      "Iteration 6084: Policy loss: -0.008538. Value loss: 0.131156. Entropy: 1.060436.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6085: Policy loss: 0.001117. Value loss: 0.478211. Entropy: 1.077654.\n",
      "Iteration 6086: Policy loss: 0.002723. Value loss: 0.276566. Entropy: 1.074989.\n",
      "Iteration 6087: Policy loss: -0.007632. Value loss: 0.199796. Entropy: 1.086710.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6088: Policy loss: 0.003533. Value loss: 0.461847. Entropy: 1.004953.\n",
      "Iteration 6089: Policy loss: -0.000335. Value loss: 0.223268. Entropy: 1.000879.\n",
      "Iteration 6090: Policy loss: -0.012138. Value loss: 0.155656. Entropy: 1.007634.\n",
      "now time :  2019-02-28 12:08:03.254705\n",
      "episode: 2901   score: 320.0  epsilon: 1.0    steps: 8  evaluation reward: 257.75\n",
      "episode: 2902   score: 295.0  epsilon: 1.0    steps: 424  evaluation reward: 257.85\n",
      "episode: 2903   score: 260.0  epsilon: 1.0    steps: 480  evaluation reward: 259.1\n",
      "episode: 2904   score: 285.0  epsilon: 1.0    steps: 552  evaluation reward: 259.7\n",
      "episode: 2905   score: 105.0  epsilon: 1.0    steps: 984  evaluation reward: 259.55\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6091: Policy loss: 0.005087. Value loss: 0.269548. Entropy: 1.172938.\n",
      "Iteration 6092: Policy loss: 0.001531. Value loss: 0.172824. Entropy: 1.173322.\n",
      "Iteration 6093: Policy loss: -0.003779. Value loss: 0.134104. Entropy: 1.192314.\n",
      "episode: 2906   score: 180.0  epsilon: 1.0    steps: 344  evaluation reward: 257.2\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6094: Policy loss: 0.001165. Value loss: 0.318756. Entropy: 0.888719.\n",
      "Iteration 6095: Policy loss: -0.002963. Value loss: 0.213755. Entropy: 0.861254.\n",
      "Iteration 6096: Policy loss: -0.006867. Value loss: 0.158464. Entropy: 0.880165.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6097: Policy loss: -0.000832. Value loss: 0.369329. Entropy: 1.053559.\n",
      "Iteration 6098: Policy loss: -0.004620. Value loss: 0.209484. Entropy: 1.073138.\n",
      "Iteration 6099: Policy loss: -0.008044. Value loss: 0.164643. Entropy: 1.079612.\n",
      "episode: 2907   score: 230.0  epsilon: 1.0    steps: 816  evaluation reward: 258.25\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6100: Policy loss: -0.000251. Value loss: 0.658908. Entropy: 0.955387.\n",
      "Iteration 6101: Policy loss: -0.000251. Value loss: 0.337463. Entropy: 0.915869.\n",
      "Iteration 6102: Policy loss: -0.004348. Value loss: 0.246618. Entropy: 0.942448.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6103: Policy loss: 0.002804. Value loss: 0.984076. Entropy: 1.137750.\n",
      "Iteration 6104: Policy loss: 0.004969. Value loss: 0.536467. Entropy: 1.142006.\n",
      "Iteration 6105: Policy loss: -0.002053. Value loss: 0.341269. Entropy: 1.147988.\n",
      "episode: 2908   score: 230.0  epsilon: 1.0    steps: 944  evaluation reward: 256.9\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6106: Policy loss: 0.005071. Value loss: 0.707893. Entropy: 1.225740.\n",
      "Iteration 6107: Policy loss: -0.001490. Value loss: 0.330712. Entropy: 1.232026.\n",
      "Iteration 6108: Policy loss: -0.008941. Value loss: 0.243927. Entropy: 1.229800.\n",
      "episode: 2909   score: 145.0  epsilon: 1.0    steps: 528  evaluation reward: 256.85\n",
      "episode: 2910   score: 525.0  epsilon: 1.0    steps: 640  evaluation reward: 258.6\n",
      "episode: 2911   score: 305.0  epsilon: 1.0    steps: 720  evaluation reward: 259.4\n",
      "episode: 2912   score: 350.0  epsilon: 1.0    steps: 760  evaluation reward: 259.25\n",
      "Training network. lr: 0.000203. clip: 0.081264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6109: Policy loss: 0.001114. Value loss: 0.392898. Entropy: 1.138191.\n",
      "Iteration 6110: Policy loss: -0.002667. Value loss: 0.246530. Entropy: 1.155622.\n",
      "Iteration 6111: Policy loss: -0.008592. Value loss: 0.196690. Entropy: 1.123855.\n",
      "episode: 2913   score: 390.0  epsilon: 1.0    steps: 680  evaluation reward: 260.85\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6112: Policy loss: 0.000206. Value loss: 0.399757. Entropy: 0.912848.\n",
      "Iteration 6113: Policy loss: -0.001975. Value loss: 0.308146. Entropy: 0.892383.\n",
      "Iteration 6114: Policy loss: -0.006218. Value loss: 0.193886. Entropy: 0.921977.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6115: Policy loss: 0.000600. Value loss: 0.416927. Entropy: 1.063373.\n",
      "Iteration 6116: Policy loss: -0.003550. Value loss: 0.201875. Entropy: 1.052203.\n",
      "Iteration 6117: Policy loss: -0.007018. Value loss: 0.170670. Entropy: 1.071709.\n",
      "episode: 2914   score: 170.0  epsilon: 1.0    steps: 304  evaluation reward: 258.95\n",
      "episode: 2915   score: 685.0  epsilon: 1.0    steps: 424  evaluation reward: 264.75\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6118: Policy loss: 0.004059. Value loss: 0.659270. Entropy: 1.052942.\n",
      "Iteration 6119: Policy loss: 0.001704. Value loss: 0.391226. Entropy: 1.057541.\n",
      "Iteration 6120: Policy loss: -0.002859. Value loss: 0.330820. Entropy: 1.058659.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6121: Policy loss: 0.002648. Value loss: 0.312991. Entropy: 1.080409.\n",
      "Iteration 6122: Policy loss: -0.005233. Value loss: 0.166489. Entropy: 1.075893.\n",
      "Iteration 6123: Policy loss: -0.008995. Value loss: 0.133968. Entropy: 1.074574.\n",
      "episode: 2916   score: 160.0  epsilon: 1.0    steps: 104  evaluation reward: 263.7\n",
      "episode: 2917   score: 180.0  epsilon: 1.0    steps: 688  evaluation reward: 262.6\n",
      "episode: 2918   score: 185.0  epsilon: 1.0    steps: 808  evaluation reward: 262.35\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6124: Policy loss: 0.003859. Value loss: 0.399737. Entropy: 1.161326.\n",
      "Iteration 6125: Policy loss: -0.000955. Value loss: 0.295262. Entropy: 1.142935.\n",
      "Iteration 6126: Policy loss: -0.004767. Value loss: 0.230899. Entropy: 1.146153.\n",
      "episode: 2919   score: 195.0  epsilon: 1.0    steps: 976  evaluation reward: 261.95\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6127: Policy loss: 0.003416. Value loss: 0.257416. Entropy: 1.008901.\n",
      "Iteration 6128: Policy loss: 0.003038. Value loss: 0.176247. Entropy: 1.040970.\n",
      "Iteration 6129: Policy loss: -0.007503. Value loss: 0.150561. Entropy: 1.010302.\n",
      "episode: 2920   score: 185.0  epsilon: 1.0    steps: 368  evaluation reward: 260.45\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6130: Policy loss: 0.002203. Value loss: 0.342586. Entropy: 1.154793.\n",
      "Iteration 6131: Policy loss: -0.004817. Value loss: 0.155511. Entropy: 1.162890.\n",
      "Iteration 6132: Policy loss: -0.012111. Value loss: 0.131587. Entropy: 1.162013.\n",
      "episode: 2921   score: 130.0  epsilon: 1.0    steps: 128  evaluation reward: 259.95\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6133: Policy loss: 0.003998. Value loss: 0.626448. Entropy: 1.059995.\n",
      "Iteration 6134: Policy loss: 0.002829. Value loss: 0.287586. Entropy: 1.078891.\n",
      "Iteration 6135: Policy loss: -0.004459. Value loss: 0.244950. Entropy: 1.076102.\n",
      "episode: 2922   score: 445.0  epsilon: 1.0    steps: 248  evaluation reward: 263.0\n",
      "episode: 2923   score: 210.0  epsilon: 1.0    steps: 376  evaluation reward: 262.95\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6136: Policy loss: 0.003627. Value loss: 0.480830. Entropy: 1.085013.\n",
      "Iteration 6137: Policy loss: -0.006542. Value loss: 0.230339. Entropy: 1.093903.\n",
      "Iteration 6138: Policy loss: -0.014144. Value loss: 0.140343. Entropy: 1.094153.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6139: Policy loss: 0.002062. Value loss: 0.291702. Entropy: 1.104269.\n",
      "Iteration 6140: Policy loss: -0.006412. Value loss: 0.152518. Entropy: 1.114885.\n",
      "Iteration 6141: Policy loss: -0.010421. Value loss: 0.112736. Entropy: 1.104055.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6142: Policy loss: 0.002045. Value loss: 0.369936. Entropy: 1.125012.\n",
      "Iteration 6143: Policy loss: -0.002926. Value loss: 0.174354. Entropy: 1.122155.\n",
      "Iteration 6144: Policy loss: -0.005575. Value loss: 0.140595. Entropy: 1.133871.\n",
      "episode: 2924   score: 540.0  epsilon: 1.0    steps: 8  evaluation reward: 266.4\n",
      "episode: 2925   score: 220.0  epsilon: 1.0    steps: 88  evaluation reward: 264.35\n",
      "episode: 2926   score: 170.0  epsilon: 1.0    steps: 136  evaluation reward: 263.35\n",
      "episode: 2927   score: 215.0  epsilon: 1.0    steps: 664  evaluation reward: 262.9\n",
      "episode: 2928   score: 260.0  epsilon: 1.0    steps: 680  evaluation reward: 261.55\n",
      "episode: 2929   score: 315.0  epsilon: 1.0    steps: 992  evaluation reward: 262.55\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6145: Policy loss: -0.000739. Value loss: 0.212641. Entropy: 1.092025.\n",
      "Iteration 6146: Policy loss: -0.002124. Value loss: 0.142667. Entropy: 1.085034.\n",
      "Iteration 6147: Policy loss: -0.005237. Value loss: 0.101131. Entropy: 1.102949.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6148: Policy loss: -0.001212. Value loss: 0.254773. Entropy: 0.932171.\n",
      "Iteration 6149: Policy loss: -0.003671. Value loss: 0.164374. Entropy: 0.938037.\n",
      "Iteration 6150: Policy loss: -0.007913. Value loss: 0.143454. Entropy: 0.926008.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6151: Policy loss: 0.003881. Value loss: 0.248303. Entropy: 0.971700.\n",
      "Iteration 6152: Policy loss: -0.000430. Value loss: 0.125566. Entropy: 0.974432.\n",
      "Iteration 6153: Policy loss: -0.008366. Value loss: 0.093537. Entropy: 0.986734.\n",
      "episode: 2930   score: 210.0  epsilon: 1.0    steps: 208  evaluation reward: 262.55\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6154: Policy loss: 0.008249. Value loss: 0.218920. Entropy: 1.163617.\n",
      "Iteration 6155: Policy loss: 0.000873. Value loss: 0.140733. Entropy: 1.191961.\n",
      "Iteration 6156: Policy loss: -0.004117. Value loss: 0.094899. Entropy: 1.178892.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6157: Policy loss: 0.006189. Value loss: 0.365102. Entropy: 1.210719.\n",
      "Iteration 6158: Policy loss: 0.001399. Value loss: 0.193312. Entropy: 1.217034.\n",
      "Iteration 6159: Policy loss: -0.002863. Value loss: 0.139492. Entropy: 1.219988.\n",
      "episode: 2931   score: 285.0  epsilon: 1.0    steps: 432  evaluation reward: 262.8\n",
      "episode: 2932   score: 160.0  epsilon: 1.0    steps: 584  evaluation reward: 261.9\n",
      "episode: 2933   score: 225.0  epsilon: 1.0    steps: 792  evaluation reward: 261.5\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6160: Policy loss: -0.001047. Value loss: 0.373636. Entropy: 1.222765.\n",
      "Iteration 6161: Policy loss: 0.001848. Value loss: 0.197557. Entropy: 1.217453.\n",
      "Iteration 6162: Policy loss: -0.009473. Value loss: 0.161120. Entropy: 1.209307.\n",
      "episode: 2934   score: 305.0  epsilon: 1.0    steps: 280  evaluation reward: 260.75\n",
      "episode: 2935   score: 300.0  epsilon: 1.0    steps: 448  evaluation reward: 261.1\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6163: Policy loss: 0.001964. Value loss: 0.210610. Entropy: 0.975891.\n",
      "Iteration 6164: Policy loss: -0.000878. Value loss: 0.127203. Entropy: 0.965343.\n",
      "Iteration 6165: Policy loss: -0.008351. Value loss: 0.100875. Entropy: 0.976261.\n",
      "episode: 2936   score: 270.0  epsilon: 1.0    steps: 304  evaluation reward: 262.45\n",
      "episode: 2937   score: 210.0  epsilon: 1.0    steps: 776  evaluation reward: 262.4\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6166: Policy loss: 0.004912. Value loss: 0.246131. Entropy: 1.040446.\n",
      "Iteration 6167: Policy loss: -0.002450. Value loss: 0.150405. Entropy: 1.042378.\n",
      "Iteration 6168: Policy loss: -0.006517. Value loss: 0.118136. Entropy: 1.050406.\n",
      "episode: 2938   score: 90.0  epsilon: 1.0    steps: 424  evaluation reward: 257.8\n",
      "episode: 2939   score: 365.0  epsilon: 1.0    steps: 848  evaluation reward: 259.2\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6169: Policy loss: 0.002662. Value loss: 0.114883. Entropy: 0.937124.\n",
      "Iteration 6170: Policy loss: -0.003814. Value loss: 0.077122. Entropy: 0.957349.\n",
      "Iteration 6171: Policy loss: -0.011431. Value loss: 0.057750. Entropy: 0.944033.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6172: Policy loss: -0.002626. Value loss: 0.267325. Entropy: 0.951444.\n",
      "Iteration 6173: Policy loss: -0.006080. Value loss: 0.154569. Entropy: 0.948364.\n",
      "Iteration 6174: Policy loss: -0.014201. Value loss: 0.103724. Entropy: 0.965461.\n",
      "episode: 2940   score: 215.0  epsilon: 1.0    steps: 376  evaluation reward: 258.15\n",
      "episode: 2941   score: 185.0  epsilon: 1.0    steps: 656  evaluation reward: 257.5\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6175: Policy loss: -0.001587. Value loss: 0.288166. Entropy: 1.229182.\n",
      "Iteration 6176: Policy loss: -0.006777. Value loss: 0.167310. Entropy: 1.227046.\n",
      "Iteration 6177: Policy loss: -0.014707. Value loss: 0.131466. Entropy: 1.225855.\n",
      "episode: 2942   score: 65.0  epsilon: 1.0    steps: 1024  evaluation reward: 255.0\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6178: Policy loss: -0.000934. Value loss: 0.228034. Entropy: 1.072194.\n",
      "Iteration 6179: Policy loss: -0.006305. Value loss: 0.112983. Entropy: 1.046643.\n",
      "Iteration 6180: Policy loss: -0.012884. Value loss: 0.087943. Entropy: 1.060696.\n",
      "episode: 2943   score: 200.0  epsilon: 1.0    steps: 656  evaluation reward: 254.15\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6181: Policy loss: 0.000654. Value loss: 0.209378. Entropy: 1.207794.\n",
      "Iteration 6182: Policy loss: -0.004459. Value loss: 0.132093. Entropy: 1.197573.\n",
      "Iteration 6183: Policy loss: -0.007610. Value loss: 0.101305. Entropy: 1.212651.\n",
      "episode: 2944   score: 225.0  epsilon: 1.0    steps: 400  evaluation reward: 254.3\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6184: Policy loss: 0.000421. Value loss: 0.234950. Entropy: 0.940554.\n",
      "Iteration 6185: Policy loss: -0.007028. Value loss: 0.147600. Entropy: 0.988413.\n",
      "Iteration 6186: Policy loss: -0.011857. Value loss: 0.097752. Entropy: 0.981595.\n",
      "episode: 2945   score: 210.0  epsilon: 1.0    steps: 320  evaluation reward: 254.25\n",
      "episode: 2946   score: 230.0  epsilon: 1.0    steps: 488  evaluation reward: 254.7\n",
      "episode: 2947   score: 210.0  epsilon: 1.0    steps: 600  evaluation reward: 255.35\n",
      "episode: 2948   score: 435.0  epsilon: 1.0    steps: 888  evaluation reward: 257.05\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6187: Policy loss: 0.002219. Value loss: 0.267124. Entropy: 1.081676.\n",
      "Iteration 6188: Policy loss: -0.005421. Value loss: 0.196296. Entropy: 1.080022.\n",
      "Iteration 6189: Policy loss: -0.008580. Value loss: 0.179612. Entropy: 1.107173.\n",
      "episode: 2949   score: 455.0  epsilon: 1.0    steps: 232  evaluation reward: 259.35\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6190: Policy loss: 0.003364. Value loss: 0.756257. Entropy: 0.826583.\n",
      "Iteration 6191: Policy loss: 0.004852. Value loss: 0.447235. Entropy: 0.796636.\n",
      "Iteration 6192: Policy loss: 0.006111. Value loss: 0.300720. Entropy: 0.832013.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6193: Policy loss: 0.008030. Value loss: 0.797943. Entropy: 1.072433.\n",
      "Iteration 6194: Policy loss: 0.006728. Value loss: 0.359086. Entropy: 1.071020.\n",
      "Iteration 6195: Policy loss: -0.000106. Value loss: 0.263309. Entropy: 1.082248.\n",
      "episode: 2950   score: 175.0  epsilon: 1.0    steps: 968  evaluation reward: 257.4\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6196: Policy loss: 0.005987. Value loss: 0.353058. Entropy: 1.064699.\n",
      "Iteration 6197: Policy loss: -0.002957. Value loss: 0.150418. Entropy: 1.057984.\n",
      "Iteration 6198: Policy loss: -0.010495. Value loss: 0.133236. Entropy: 1.050285.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6199: Policy loss: 0.005437. Value loss: 0.408258. Entropy: 1.061368.\n",
      "Iteration 6200: Policy loss: -0.002565. Value loss: 0.208812. Entropy: 1.048408.\n",
      "Iteration 6201: Policy loss: -0.003681. Value loss: 0.164415. Entropy: 1.051002.\n",
      "now time :  2019-02-28 12:09:24.469234\n",
      "episode: 2951   score: 155.0  epsilon: 1.0    steps: 736  evaluation reward: 256.65\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6202: Policy loss: 0.002046. Value loss: 0.775916. Entropy: 1.159410.\n",
      "Iteration 6203: Policy loss: -0.001374. Value loss: 0.431989. Entropy: 1.149304.\n",
      "Iteration 6204: Policy loss: -0.008540. Value loss: 0.322708. Entropy: 1.153073.\n",
      "episode: 2952   score: 710.0  epsilon: 1.0    steps: 248  evaluation reward: 261.65\n",
      "episode: 2953   score: 490.0  epsilon: 1.0    steps: 256  evaluation reward: 263.8\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6205: Policy loss: 0.001454. Value loss: 0.384228. Entropy: 1.042249.\n",
      "Iteration 6206: Policy loss: 0.001396. Value loss: 0.220949. Entropy: 1.069124.\n",
      "Iteration 6207: Policy loss: -0.006417. Value loss: 0.162032. Entropy: 1.056095.\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6208: Policy loss: 0.002193. Value loss: 0.319080. Entropy: 1.065504.\n",
      "Iteration 6209: Policy loss: -0.001743. Value loss: 0.181967. Entropy: 1.052405.\n",
      "Iteration 6210: Policy loss: -0.006892. Value loss: 0.134497. Entropy: 1.048847.\n",
      "episode: 2954   score: 345.0  epsilon: 1.0    steps: 24  evaluation reward: 264.75\n",
      "episode: 2955   score: 440.0  epsilon: 1.0    steps: 736  evaluation reward: 266.55\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6211: Policy loss: 0.002453. Value loss: 0.219568. Entropy: 1.090374.\n",
      "Iteration 6212: Policy loss: -0.008378. Value loss: 0.121577. Entropy: 1.079571.\n",
      "Iteration 6213: Policy loss: -0.010818. Value loss: 0.101275. Entropy: 1.087689.\n",
      "episode: 2956   score: 290.0  epsilon: 1.0    steps: 536  evaluation reward: 266.2\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6214: Policy loss: 0.006354. Value loss: 0.518164. Entropy: 1.049309.\n",
      "Iteration 6215: Policy loss: -0.000281. Value loss: 0.277274. Entropy: 1.058535.\n",
      "Iteration 6216: Policy loss: -0.001933. Value loss: 0.228050. Entropy: 1.038799.\n",
      "episode: 2957   score: 285.0  epsilon: 1.0    steps: 296  evaluation reward: 265.85\n",
      "episode: 2958   score: 135.0  epsilon: 1.0    steps: 384  evaluation reward: 265.65\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6217: Policy loss: 0.004392. Value loss: 0.497091. Entropy: 1.098063.\n",
      "Iteration 6218: Policy loss: -0.001134. Value loss: 0.225893. Entropy: 1.083966.\n",
      "Iteration 6219: Policy loss: -0.003776. Value loss: 0.222058. Entropy: 1.110542.\n",
      "episode: 2959   score: 210.0  epsilon: 1.0    steps: 752  evaluation reward: 265.9\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6220: Policy loss: -0.001319. Value loss: 0.244839. Entropy: 0.918375.\n",
      "Iteration 6221: Policy loss: -0.001587. Value loss: 0.122968. Entropy: 0.919068.\n",
      "Iteration 6222: Policy loss: -0.008508. Value loss: 0.080943. Entropy: 0.924396.\n",
      "episode: 2960   score: 185.0  epsilon: 1.0    steps: 48  evaluation reward: 265.05\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6223: Policy loss: -0.001378. Value loss: 0.193124. Entropy: 0.826495.\n",
      "Iteration 6224: Policy loss: -0.005426. Value loss: 0.123020. Entropy: 0.861394.\n",
      "Iteration 6225: Policy loss: -0.011079. Value loss: 0.103629. Entropy: 0.852197.\n",
      "episode: 2961   score: 185.0  epsilon: 1.0    steps: 144  evaluation reward: 264.8\n",
      "episode: 2962   score: 195.0  epsilon: 1.0    steps: 328  evaluation reward: 264.35\n",
      "episode: 2963   score: 535.0  epsilon: 1.0    steps: 392  evaluation reward: 268.65\n",
      "episode: 2964   score: 135.0  epsilon: 1.0    steps: 728  evaluation reward: 267.9\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6226: Policy loss: 0.004922. Value loss: 0.361041. Entropy: 1.121387.\n",
      "Iteration 6227: Policy loss: -0.000597. Value loss: 0.167711. Entropy: 1.128078.\n",
      "Iteration 6228: Policy loss: -0.004534. Value loss: 0.134584. Entropy: 1.121517.\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6229: Policy loss: 0.002137. Value loss: 0.150445. Entropy: 0.884232.\n",
      "Iteration 6230: Policy loss: -0.000656. Value loss: 0.116821. Entropy: 0.900240.\n",
      "Iteration 6231: Policy loss: -0.004979. Value loss: 0.071689. Entropy: 0.868056.\n",
      "episode: 2965   score: 125.0  epsilon: 1.0    steps: 40  evaluation reward: 264.25\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6232: Policy loss: 0.001407. Value loss: 0.118354. Entropy: 1.016390.\n",
      "Iteration 6233: Policy loss: -0.002333. Value loss: 0.076255. Entropy: 0.992408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6234: Policy loss: -0.006403. Value loss: 0.060390. Entropy: 1.006398.\n",
      "episode: 2966   score: 215.0  epsilon: 1.0    steps: 56  evaluation reward: 264.15\n",
      "episode: 2967   score: 245.0  epsilon: 1.0    steps: 816  evaluation reward: 263.4\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6235: Policy loss: 0.001384. Value loss: 0.236475. Entropy: 1.085348.\n",
      "Iteration 6236: Policy loss: -0.004909. Value loss: 0.147898. Entropy: 1.103213.\n",
      "Iteration 6237: Policy loss: -0.005721. Value loss: 0.111503. Entropy: 1.091823.\n",
      "episode: 2968   score: 175.0  epsilon: 1.0    steps: 680  evaluation reward: 263.05\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6238: Policy loss: 0.002129. Value loss: 0.264652. Entropy: 1.075840.\n",
      "Iteration 6239: Policy loss: -0.007425. Value loss: 0.132978. Entropy: 1.066363.\n",
      "Iteration 6240: Policy loss: -0.013570. Value loss: 0.098673. Entropy: 1.065206.\n",
      "episode: 2969   score: 330.0  epsilon: 1.0    steps: 432  evaluation reward: 265.1\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6241: Policy loss: 0.003435. Value loss: 0.216679. Entropy: 1.037561.\n",
      "Iteration 6242: Policy loss: -0.001619. Value loss: 0.113977. Entropy: 1.049920.\n",
      "Iteration 6243: Policy loss: -0.011352. Value loss: 0.076907. Entropy: 1.049855.\n",
      "episode: 2970   score: 255.0  epsilon: 1.0    steps: 88  evaluation reward: 264.0\n",
      "episode: 2971   score: 180.0  epsilon: 1.0    steps: 280  evaluation reward: 263.4\n",
      "episode: 2972   score: 240.0  epsilon: 1.0    steps: 456  evaluation reward: 263.25\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6244: Policy loss: 0.002980. Value loss: 0.183711. Entropy: 1.040133.\n",
      "Iteration 6245: Policy loss: -0.003927. Value loss: 0.118046. Entropy: 1.033542.\n",
      "Iteration 6246: Policy loss: -0.008776. Value loss: 0.081113. Entropy: 1.027483.\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6247: Policy loss: -0.000062. Value loss: 0.111676. Entropy: 1.048276.\n",
      "Iteration 6248: Policy loss: -0.005767. Value loss: 0.081619. Entropy: 1.051970.\n",
      "Iteration 6249: Policy loss: -0.007476. Value loss: 0.049279. Entropy: 1.037372.\n",
      "episode: 2973   score: 245.0  epsilon: 1.0    steps: 32  evaluation reward: 261.65\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6250: Policy loss: 0.000890. Value loss: 0.700420. Entropy: 1.149899.\n",
      "Iteration 6251: Policy loss: 0.001260. Value loss: 0.370942. Entropy: 1.177563.\n",
      "Iteration 6252: Policy loss: -0.000827. Value loss: 0.267305. Entropy: 1.163429.\n",
      "episode: 2974   score: 120.0  epsilon: 1.0    steps: 72  evaluation reward: 260.4\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6253: Policy loss: 0.000119. Value loss: 0.173234. Entropy: 1.053611.\n",
      "Iteration 6254: Policy loss: -0.006644. Value loss: 0.103424. Entropy: 1.047172.\n",
      "Iteration 6255: Policy loss: -0.012506. Value loss: 0.078541. Entropy: 1.063657.\n",
      "episode: 2975   score: 210.0  epsilon: 1.0    steps: 136  evaluation reward: 259.65\n",
      "episode: 2976   score: 210.0  epsilon: 1.0    steps: 400  evaluation reward: 259.65\n",
      "episode: 2977   score: 180.0  epsilon: 1.0    steps: 592  evaluation reward: 259.2\n",
      "episode: 2978   score: 300.0  epsilon: 1.0    steps: 768  evaluation reward: 260.25\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6256: Policy loss: 0.002207. Value loss: 0.349059. Entropy: 1.093615.\n",
      "Iteration 6257: Policy loss: -0.000610. Value loss: 0.213727. Entropy: 1.092123.\n",
      "Iteration 6258: Policy loss: -0.007769. Value loss: 0.155789. Entropy: 1.102670.\n",
      "episode: 2979   score: 415.0  epsilon: 1.0    steps: 152  evaluation reward: 261.8\n",
      "episode: 2980   score: 225.0  epsilon: 1.0    steps: 216  evaluation reward: 261.9\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6259: Policy loss: -0.001958. Value loss: 0.240850. Entropy: 0.817803.\n",
      "Iteration 6260: Policy loss: -0.006032. Value loss: 0.134015. Entropy: 0.824165.\n",
      "Iteration 6261: Policy loss: -0.010817. Value loss: 0.116371. Entropy: 0.804400.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6262: Policy loss: 0.003297. Value loss: 0.287653. Entropy: 0.965542.\n",
      "Iteration 6263: Policy loss: -0.001818. Value loss: 0.182198. Entropy: 0.963590.\n",
      "Iteration 6264: Policy loss: -0.009035. Value loss: 0.146247. Entropy: 0.951787.\n",
      "episode: 2981   score: 185.0  epsilon: 1.0    steps: 936  evaluation reward: 261.4\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6265: Policy loss: 0.001158. Value loss: 0.745534. Entropy: 1.202309.\n",
      "Iteration 6266: Policy loss: 0.005920. Value loss: 0.241086. Entropy: 1.193236.\n",
      "Iteration 6267: Policy loss: -0.003063. Value loss: 0.171472. Entropy: 1.216685.\n",
      "episode: 2982   score: 210.0  epsilon: 1.0    steps: 944  evaluation reward: 262.45\n",
      "episode: 2983   score: 215.0  epsilon: 1.0    steps: 1016  evaluation reward: 263.4\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6268: Policy loss: 0.001915. Value loss: 0.301981. Entropy: 1.197992.\n",
      "Iteration 6269: Policy loss: -0.002626. Value loss: 0.193656. Entropy: 1.190421.\n",
      "Iteration 6270: Policy loss: -0.005322. Value loss: 0.138328. Entropy: 1.196343.\n",
      "episode: 2984   score: 210.0  epsilon: 1.0    steps: 64  evaluation reward: 263.3\n",
      "episode: 2985   score: 235.0  epsilon: 1.0    steps: 872  evaluation reward: 264.1\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6271: Policy loss: 0.001318. Value loss: 0.310248. Entropy: 1.100545.\n",
      "Iteration 6272: Policy loss: -0.002645. Value loss: 0.188501. Entropy: 1.107794.\n",
      "Iteration 6273: Policy loss: -0.006004. Value loss: 0.165753. Entropy: 1.105574.\n",
      "episode: 2986   score: 340.0  epsilon: 1.0    steps: 144  evaluation reward: 264.3\n",
      "episode: 2987   score: 210.0  epsilon: 1.0    steps: 584  evaluation reward: 260.75\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6274: Policy loss: 0.001803. Value loss: 0.254579. Entropy: 1.039680.\n",
      "Iteration 6275: Policy loss: -0.002806. Value loss: 0.167742. Entropy: 1.019652.\n",
      "Iteration 6276: Policy loss: -0.011953. Value loss: 0.112283. Entropy: 1.051718.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6277: Policy loss: 0.001941. Value loss: 0.189550. Entropy: 1.039306.\n",
      "Iteration 6278: Policy loss: -0.004519. Value loss: 0.077454. Entropy: 1.062223.\n",
      "Iteration 6279: Policy loss: -0.006787. Value loss: 0.063701. Entropy: 1.060606.\n",
      "episode: 2988   score: 545.0  epsilon: 1.0    steps: 80  evaluation reward: 263.0\n",
      "episode: 2989   score: 180.0  epsilon: 1.0    steps: 792  evaluation reward: 263.0\n",
      "episode: 2990   score: 210.0  epsilon: 1.0    steps: 976  evaluation reward: 262.85\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6280: Policy loss: -0.000094. Value loss: 0.180735. Entropy: 1.132078.\n",
      "Iteration 6281: Policy loss: -0.002748. Value loss: 0.113794. Entropy: 1.133486.\n",
      "Iteration 6282: Policy loss: -0.008171. Value loss: 0.090604. Entropy: 1.113164.\n",
      "episode: 2991   score: 170.0  epsilon: 1.0    steps: 288  evaluation reward: 262.75\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6283: Policy loss: -0.000298. Value loss: 0.152753. Entropy: 1.057140.\n",
      "Iteration 6284: Policy loss: -0.002043. Value loss: 0.091068. Entropy: 1.058519.\n",
      "Iteration 6285: Policy loss: -0.007285. Value loss: 0.065628. Entropy: 1.053076.\n",
      "episode: 2992   score: 210.0  epsilon: 1.0    steps: 464  evaluation reward: 261.85\n",
      "episode: 2993   score: 225.0  epsilon: 1.0    steps: 744  evaluation reward: 261.7\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6286: Policy loss: 0.003189. Value loss: 0.144243. Entropy: 1.149366.\n",
      "Iteration 6287: Policy loss: -0.007284. Value loss: 0.075723. Entropy: 1.150328.\n",
      "Iteration 6288: Policy loss: -0.013317. Value loss: 0.060292. Entropy: 1.147513.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6289: Policy loss: 0.003103. Value loss: 0.176418. Entropy: 0.993156.\n",
      "Iteration 6290: Policy loss: -0.002135. Value loss: 0.071301. Entropy: 1.017655.\n",
      "Iteration 6291: Policy loss: -0.007864. Value loss: 0.057645. Entropy: 1.007268.\n",
      "episode: 2994   score: 240.0  epsilon: 1.0    steps: 72  evaluation reward: 260.9\n",
      "episode: 2995   score: 240.0  epsilon: 1.0    steps: 520  evaluation reward: 260.05\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6292: Policy loss: -0.002320. Value loss: 0.148600. Entropy: 1.140605.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6293: Policy loss: -0.005465. Value loss: 0.090738. Entropy: 1.142429.\n",
      "Iteration 6294: Policy loss: -0.014008. Value loss: 0.065921. Entropy: 1.163326.\n",
      "episode: 2996   score: 180.0  epsilon: 1.0    steps: 128  evaluation reward: 255.35\n",
      "episode: 2997   score: 120.0  epsilon: 1.0    steps: 968  evaluation reward: 254.3\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6295: Policy loss: 0.000540. Value loss: 0.155833. Entropy: 1.075570.\n",
      "Iteration 6296: Policy loss: -0.003791. Value loss: 0.084458. Entropy: 1.083399.\n",
      "Iteration 6297: Policy loss: -0.007273. Value loss: 0.086448. Entropy: 1.090438.\n",
      "episode: 2998   score: 230.0  epsilon: 1.0    steps: 160  evaluation reward: 254.8\n",
      "episode: 2999   score: 210.0  epsilon: 1.0    steps: 976  evaluation reward: 253.9\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6298: Policy loss: 0.001527. Value loss: 0.163053. Entropy: 1.081183.\n",
      "Iteration 6299: Policy loss: -0.006104. Value loss: 0.100620. Entropy: 1.072776.\n",
      "Iteration 6300: Policy loss: -0.006678. Value loss: 0.078646. Entropy: 1.087677.\n",
      "episode: 3000   score: 210.0  epsilon: 1.0    steps: 480  evaluation reward: 254.5\n",
      "now time :  2019-02-28 12:10:36.676479\n",
      "episode: 3001   score: 165.0  epsilon: 1.0    steps: 952  evaluation reward: 252.95\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6301: Policy loss: 0.000932. Value loss: 0.176854. Entropy: 1.037161.\n",
      "Iteration 6302: Policy loss: -0.008718. Value loss: 0.119407. Entropy: 1.031672.\n",
      "Iteration 6303: Policy loss: -0.012545. Value loss: 0.090804. Entropy: 1.040351.\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6304: Policy loss: 0.001021. Value loss: 0.173793. Entropy: 1.031813.\n",
      "Iteration 6305: Policy loss: -0.005231. Value loss: 0.108810. Entropy: 1.027207.\n",
      "Iteration 6306: Policy loss: -0.007716. Value loss: 0.089045. Entropy: 1.027197.\n",
      "episode: 3002   score: 265.0  epsilon: 1.0    steps: 32  evaluation reward: 252.65\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6307: Policy loss: -0.002170. Value loss: 0.142444. Entropy: 1.100329.\n",
      "Iteration 6308: Policy loss: -0.005607. Value loss: 0.072253. Entropy: 1.106263.\n",
      "Iteration 6309: Policy loss: -0.010970. Value loss: 0.046420. Entropy: 1.099201.\n",
      "episode: 3003   score: 180.0  epsilon: 1.0    steps: 400  evaluation reward: 251.85\n",
      "episode: 3004   score: 125.0  epsilon: 1.0    steps: 872  evaluation reward: 250.25\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6310: Policy loss: 0.001869. Value loss: 0.206392. Entropy: 1.182677.\n",
      "Iteration 6311: Policy loss: -0.002136. Value loss: 0.080226. Entropy: 1.169001.\n",
      "Iteration 6312: Policy loss: -0.006165. Value loss: 0.074877. Entropy: 1.168755.\n",
      "episode: 3005   score: 240.0  epsilon: 1.0    steps: 632  evaluation reward: 251.6\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6313: Policy loss: 0.002871. Value loss: 0.246751. Entropy: 1.050039.\n",
      "Iteration 6314: Policy loss: -0.007117. Value loss: 0.129713. Entropy: 1.057654.\n",
      "Iteration 6315: Policy loss: -0.012034. Value loss: 0.112818. Entropy: 1.067335.\n",
      "episode: 3006   score: 200.0  epsilon: 1.0    steps: 336  evaluation reward: 251.8\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6316: Policy loss: 0.002816. Value loss: 0.226638. Entropy: 1.099309.\n",
      "Iteration 6317: Policy loss: -0.009355. Value loss: 0.119752. Entropy: 1.099141.\n",
      "Iteration 6318: Policy loss: -0.011633. Value loss: 0.098008. Entropy: 1.111844.\n",
      "episode: 3007   score: 210.0  epsilon: 1.0    steps: 256  evaluation reward: 251.6\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6319: Policy loss: 0.001632. Value loss: 0.257119. Entropy: 1.115388.\n",
      "Iteration 6320: Policy loss: -0.007300. Value loss: 0.150824. Entropy: 1.119475.\n",
      "Iteration 6321: Policy loss: -0.011054. Value loss: 0.101078. Entropy: 1.116493.\n",
      "episode: 3008   score: 285.0  epsilon: 1.0    steps: 64  evaluation reward: 252.15\n",
      "episode: 3009   score: 425.0  epsilon: 1.0    steps: 264  evaluation reward: 254.95\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6322: Policy loss: -0.000967. Value loss: 0.228513. Entropy: 1.006214.\n",
      "Iteration 6323: Policy loss: -0.005869. Value loss: 0.121721. Entropy: 1.022669.\n",
      "Iteration 6324: Policy loss: -0.009779. Value loss: 0.081563. Entropy: 1.028255.\n",
      "episode: 3010   score: 255.0  epsilon: 1.0    steps: 816  evaluation reward: 252.25\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6325: Policy loss: -0.004164. Value loss: 0.208597. Entropy: 1.124921.\n",
      "Iteration 6326: Policy loss: -0.006161. Value loss: 0.124899. Entropy: 1.136319.\n",
      "Iteration 6327: Policy loss: -0.011825. Value loss: 0.088351. Entropy: 1.137926.\n",
      "episode: 3011   score: 185.0  epsilon: 1.0    steps: 680  evaluation reward: 251.05\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6328: Policy loss: 0.002045. Value loss: 0.153560. Entropy: 1.089205.\n",
      "Iteration 6329: Policy loss: -0.008004. Value loss: 0.094472. Entropy: 1.087859.\n",
      "Iteration 6330: Policy loss: -0.011891. Value loss: 0.064224. Entropy: 1.092431.\n",
      "episode: 3012   score: 285.0  epsilon: 1.0    steps: 512  evaluation reward: 250.4\n",
      "episode: 3013   score: 210.0  epsilon: 1.0    steps: 936  evaluation reward: 248.6\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6331: Policy loss: 0.001613. Value loss: 0.182678. Entropy: 1.213267.\n",
      "Iteration 6332: Policy loss: -0.003286. Value loss: 0.105018. Entropy: 1.217524.\n",
      "Iteration 6333: Policy loss: -0.004875. Value loss: 0.088571. Entropy: 1.215480.\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6334: Policy loss: -0.001486. Value loss: 0.174254. Entropy: 1.041538.\n",
      "Iteration 6335: Policy loss: -0.007273. Value loss: 0.099926. Entropy: 1.067619.\n",
      "Iteration 6336: Policy loss: -0.008666. Value loss: 0.070901. Entropy: 1.081771.\n",
      "episode: 3014   score: 280.0  epsilon: 1.0    steps: 328  evaluation reward: 249.7\n",
      "episode: 3015   score: 215.0  epsilon: 1.0    steps: 672  evaluation reward: 245.0\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6337: Policy loss: 0.000392. Value loss: 0.166689. Entropy: 1.160970.\n",
      "Iteration 6338: Policy loss: -0.006377. Value loss: 0.086649. Entropy: 1.177455.\n",
      "Iteration 6339: Policy loss: -0.007549. Value loss: 0.070275. Entropy: 1.161114.\n",
      "episode: 3016   score: 210.0  epsilon: 1.0    steps: 872  evaluation reward: 245.5\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6340: Policy loss: -0.000097. Value loss: 0.152476. Entropy: 1.035587.\n",
      "Iteration 6341: Policy loss: -0.009488. Value loss: 0.085239. Entropy: 0.992594.\n",
      "Iteration 6342: Policy loss: -0.012342. Value loss: 0.060865. Entropy: 1.014597.\n",
      "episode: 3017   score: 350.0  epsilon: 1.0    steps: 288  evaluation reward: 247.2\n",
      "episode: 3018   score: 210.0  epsilon: 1.0    steps: 664  evaluation reward: 247.45\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6343: Policy loss: 0.001406. Value loss: 0.293894. Entropy: 1.111199.\n",
      "Iteration 6344: Policy loss: -0.004509. Value loss: 0.136146. Entropy: 1.100367.\n",
      "Iteration 6345: Policy loss: -0.012057. Value loss: 0.101170. Entropy: 1.109991.\n",
      "episode: 3019   score: 270.0  epsilon: 1.0    steps: 680  evaluation reward: 248.2\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6346: Policy loss: 0.005413. Value loss: 0.265015. Entropy: 1.061564.\n",
      "Iteration 6347: Policy loss: -0.009531. Value loss: 0.121531. Entropy: 1.043403.\n",
      "Iteration 6348: Policy loss: -0.010104. Value loss: 0.098802. Entropy: 1.071957.\n",
      "episode: 3020   score: 360.0  epsilon: 1.0    steps: 112  evaluation reward: 249.95\n",
      "episode: 3021   score: 240.0  epsilon: 1.0    steps: 680  evaluation reward: 251.05\n",
      "episode: 3022   score: 210.0  epsilon: 1.0    steps: 768  evaluation reward: 248.7\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6349: Policy loss: 0.000185. Value loss: 0.139967. Entropy: 0.990781.\n",
      "Iteration 6350: Policy loss: -0.003087. Value loss: 0.103130. Entropy: 0.979117.\n",
      "Iteration 6351: Policy loss: -0.008367. Value loss: 0.074569. Entropy: 0.984060.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6352: Policy loss: 0.004141. Value loss: 0.426728. Entropy: 0.988157.\n",
      "Iteration 6353: Policy loss: 0.001101. Value loss: 0.209704. Entropy: 1.002445.\n",
      "Iteration 6354: Policy loss: -0.000729. Value loss: 0.150851. Entropy: 0.993794.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3023   score: 225.0  epsilon: 1.0    steps: 424  evaluation reward: 248.85\n",
      "episode: 3024   score: 185.0  epsilon: 1.0    steps: 720  evaluation reward: 245.3\n",
      "episode: 3025   score: 210.0  epsilon: 1.0    steps: 1000  evaluation reward: 245.2\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6355: Policy loss: 0.002580. Value loss: 0.409629. Entropy: 1.160843.\n",
      "Iteration 6356: Policy loss: 0.000880. Value loss: 0.228968. Entropy: 1.168916.\n",
      "Iteration 6357: Policy loss: -0.006335. Value loss: 0.191618. Entropy: 1.160034.\n",
      "episode: 3026   score: 210.0  epsilon: 1.0    steps: 552  evaluation reward: 245.6\n",
      "episode: 3027   score: 155.0  epsilon: 1.0    steps: 616  evaluation reward: 245.0\n",
      "episode: 3028   score: 410.0  epsilon: 1.0    steps: 880  evaluation reward: 246.5\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6358: Policy loss: 0.004088. Value loss: 0.260267. Entropy: 0.943807.\n",
      "Iteration 6359: Policy loss: -0.000388. Value loss: 0.145929. Entropy: 0.957558.\n",
      "Iteration 6360: Policy loss: -0.004397. Value loss: 0.132637. Entropy: 0.931950.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6361: Policy loss: 0.000873. Value loss: 0.269507. Entropy: 0.945189.\n",
      "Iteration 6362: Policy loss: -0.005185. Value loss: 0.197298. Entropy: 0.941290.\n",
      "Iteration 6363: Policy loss: -0.006704. Value loss: 0.196367. Entropy: 0.937888.\n",
      "episode: 3029   score: 140.0  epsilon: 1.0    steps: 208  evaluation reward: 244.75\n",
      "episode: 3030   score: 215.0  epsilon: 1.0    steps: 848  evaluation reward: 244.8\n",
      "episode: 3031   score: 135.0  epsilon: 1.0    steps: 1008  evaluation reward: 243.3\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6364: Policy loss: -0.000003. Value loss: 0.327835. Entropy: 0.993628.\n",
      "Iteration 6365: Policy loss: -0.007585. Value loss: 0.197133. Entropy: 0.982687.\n",
      "Iteration 6366: Policy loss: -0.012264. Value loss: 0.151395. Entropy: 0.967966.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6367: Policy loss: -0.000065. Value loss: 0.231194. Entropy: 1.050028.\n",
      "Iteration 6368: Policy loss: -0.005231. Value loss: 0.142893. Entropy: 1.020664.\n",
      "Iteration 6369: Policy loss: -0.010759. Value loss: 0.114267. Entropy: 1.042681.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6370: Policy loss: -0.000931. Value loss: 0.212592. Entropy: 1.206587.\n",
      "Iteration 6371: Policy loss: -0.007801. Value loss: 0.122358. Entropy: 1.219544.\n",
      "Iteration 6372: Policy loss: -0.015532. Value loss: 0.114580. Entropy: 1.211550.\n",
      "episode: 3032   score: 210.0  epsilon: 1.0    steps: 216  evaluation reward: 243.8\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6373: Policy loss: 0.004975. Value loss: 0.174908. Entropy: 1.129475.\n",
      "Iteration 6374: Policy loss: -0.004775. Value loss: 0.102826. Entropy: 1.128223.\n",
      "Iteration 6375: Policy loss: -0.008370. Value loss: 0.076947. Entropy: 1.121906.\n",
      "episode: 3033   score: 285.0  epsilon: 1.0    steps: 496  evaluation reward: 244.4\n",
      "episode: 3034   score: 210.0  epsilon: 1.0    steps: 512  evaluation reward: 243.45\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6376: Policy loss: 0.002203. Value loss: 0.246004. Entropy: 1.205247.\n",
      "Iteration 6377: Policy loss: -0.004595. Value loss: 0.130992. Entropy: 1.204333.\n",
      "Iteration 6378: Policy loss: -0.015230. Value loss: 0.103424. Entropy: 1.192560.\n",
      "episode: 3035   score: 255.0  epsilon: 1.0    steps: 352  evaluation reward: 243.0\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6379: Policy loss: 0.000640. Value loss: 0.221759. Entropy: 1.039856.\n",
      "Iteration 6380: Policy loss: -0.006860. Value loss: 0.100011. Entropy: 1.024900.\n",
      "Iteration 6381: Policy loss: -0.008962. Value loss: 0.091157. Entropy: 1.035745.\n",
      "episode: 3036   score: 210.0  epsilon: 1.0    steps: 64  evaluation reward: 242.4\n",
      "episode: 3037   score: 210.0  epsilon: 1.0    steps: 80  evaluation reward: 242.4\n",
      "episode: 3038   score: 240.0  epsilon: 1.0    steps: 312  evaluation reward: 243.9\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6382: Policy loss: -0.002038. Value loss: 0.201254. Entropy: 1.004324.\n",
      "Iteration 6383: Policy loss: -0.008401. Value loss: 0.111959. Entropy: 1.008666.\n",
      "Iteration 6384: Policy loss: -0.011396. Value loss: 0.080479. Entropy: 0.995379.\n",
      "episode: 3039   score: 255.0  epsilon: 1.0    steps: 288  evaluation reward: 242.8\n",
      "episode: 3040   score: 180.0  epsilon: 1.0    steps: 320  evaluation reward: 242.45\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6385: Policy loss: 0.001227. Value loss: 0.201264. Entropy: 1.008104.\n",
      "Iteration 6386: Policy loss: -0.003519. Value loss: 0.145259. Entropy: 1.021697.\n",
      "Iteration 6387: Policy loss: -0.010967. Value loss: 0.114738. Entropy: 1.008330.\n",
      "episode: 3041   score: 150.0  epsilon: 1.0    steps: 888  evaluation reward: 242.1\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6388: Policy loss: 0.001485. Value loss: 0.131346. Entropy: 0.892072.\n",
      "Iteration 6389: Policy loss: -0.002332. Value loss: 0.085409. Entropy: 0.923295.\n",
      "Iteration 6390: Policy loss: -0.007828. Value loss: 0.063833. Entropy: 0.908908.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6391: Policy loss: 0.003463. Value loss: 0.167075. Entropy: 1.152104.\n",
      "Iteration 6392: Policy loss: -0.008156. Value loss: 0.102691. Entropy: 1.137236.\n",
      "Iteration 6393: Policy loss: -0.012647. Value loss: 0.083832. Entropy: 1.137990.\n",
      "episode: 3042   score: 210.0  epsilon: 1.0    steps: 872  evaluation reward: 243.55\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6394: Policy loss: 0.001851. Value loss: 0.182233. Entropy: 1.241074.\n",
      "Iteration 6395: Policy loss: -0.000936. Value loss: 0.095485. Entropy: 1.235309.\n",
      "Iteration 6396: Policy loss: -0.008079. Value loss: 0.060301. Entropy: 1.233978.\n",
      "episode: 3043   score: 210.0  epsilon: 1.0    steps: 448  evaluation reward: 243.65\n",
      "episode: 3044   score: 180.0  epsilon: 1.0    steps: 672  evaluation reward: 243.2\n",
      "episode: 3045   score: 185.0  epsilon: 1.0    steps: 912  evaluation reward: 242.95\n",
      "episode: 3046   score: 230.0  epsilon: 1.0    steps: 920  evaluation reward: 242.95\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6397: Policy loss: 0.001473. Value loss: 0.321829. Entropy: 1.168686.\n",
      "Iteration 6398: Policy loss: -0.004366. Value loss: 0.141661. Entropy: 1.171345.\n",
      "Iteration 6399: Policy loss: -0.010795. Value loss: 0.094931. Entropy: 1.147332.\n",
      "episode: 3047   score: 300.0  epsilon: 1.0    steps: 64  evaluation reward: 243.85\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6400: Policy loss: 0.000099. Value loss: 0.121219. Entropy: 0.794366.\n",
      "Iteration 6401: Policy loss: 0.000075. Value loss: 0.089156. Entropy: 0.832654.\n",
      "Iteration 6402: Policy loss: -0.003885. Value loss: 0.075246. Entropy: 0.792240.\n",
      "episode: 3048   score: 230.0  epsilon: 1.0    steps: 104  evaluation reward: 241.8\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6403: Policy loss: 0.004338. Value loss: 0.162286. Entropy: 1.117648.\n",
      "Iteration 6404: Policy loss: 0.001542. Value loss: 0.083944. Entropy: 1.095652.\n",
      "Iteration 6405: Policy loss: -0.006905. Value loss: 0.059892. Entropy: 1.121058.\n",
      "episode: 3049   score: 210.0  epsilon: 1.0    steps: 720  evaluation reward: 239.35\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6406: Policy loss: 0.004476. Value loss: 0.197347. Entropy: 0.983838.\n",
      "Iteration 6407: Policy loss: -0.003187. Value loss: 0.114526. Entropy: 0.992852.\n",
      "Iteration 6408: Policy loss: -0.007410. Value loss: 0.086592. Entropy: 0.988677.\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6409: Policy loss: 0.002048. Value loss: 0.185593. Entropy: 1.108732.\n",
      "Iteration 6410: Policy loss: -0.008408. Value loss: 0.119641. Entropy: 1.147862.\n",
      "Iteration 6411: Policy loss: -0.015517. Value loss: 0.081887. Entropy: 1.116452.\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6412: Policy loss: 0.001188. Value loss: 0.427056. Entropy: 1.298417.\n",
      "Iteration 6413: Policy loss: -0.004556. Value loss: 0.176504. Entropy: 1.286754.\n",
      "Iteration 6414: Policy loss: -0.009269. Value loss: 0.133770. Entropy: 1.297770.\n",
      "episode: 3050   score: 225.0  epsilon: 1.0    steps: 112  evaluation reward: 239.85\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6415: Policy loss: -0.001540. Value loss: 0.476749. Entropy: 1.237002.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6416: Policy loss: -0.005962. Value loss: 0.236250. Entropy: 1.234530.\n",
      "Iteration 6417: Policy loss: -0.008325. Value loss: 0.186248. Entropy: 1.243917.\n",
      "now time :  2019-02-28 12:12:00.159851\n",
      "episode: 3051   score: 225.0  epsilon: 1.0    steps: 8  evaluation reward: 240.55\n",
      "episode: 3052   score: 395.0  epsilon: 1.0    steps: 440  evaluation reward: 237.4\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6418: Policy loss: 0.000246. Value loss: 0.532409. Entropy: 1.169160.\n",
      "Iteration 6419: Policy loss: -0.002150. Value loss: 0.200451. Entropy: 1.149727.\n",
      "Iteration 6420: Policy loss: -0.005536. Value loss: 0.138876. Entropy: 1.160567.\n",
      "episode: 3053   score: 160.0  epsilon: 1.0    steps: 72  evaluation reward: 234.1\n",
      "episode: 3054   score: 270.0  epsilon: 1.0    steps: 216  evaluation reward: 233.35\n",
      "episode: 3055   score: 315.0  epsilon: 1.0    steps: 312  evaluation reward: 232.1\n",
      "episode: 3056   score: 385.0  epsilon: 1.0    steps: 984  evaluation reward: 233.05\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6421: Policy loss: 0.000839. Value loss: 0.336913. Entropy: 0.947955.\n",
      "Iteration 6422: Policy loss: -0.003463. Value loss: 0.162946. Entropy: 0.951535.\n",
      "Iteration 6423: Policy loss: -0.010740. Value loss: 0.129672. Entropy: 0.966156.\n",
      "episode: 3057   score: 300.0  epsilon: 1.0    steps: 664  evaluation reward: 233.2\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6424: Policy loss: 0.000331. Value loss: 0.153187. Entropy: 0.921594.\n",
      "Iteration 6425: Policy loss: -0.009100. Value loss: 0.095362. Entropy: 0.921298.\n",
      "Iteration 6426: Policy loss: -0.009524. Value loss: 0.084810. Entropy: 0.937698.\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6427: Policy loss: 0.000488. Value loss: 0.133948. Entropy: 0.884562.\n",
      "Iteration 6428: Policy loss: -0.006237. Value loss: 0.088730. Entropy: 0.916559.\n",
      "Iteration 6429: Policy loss: -0.006529. Value loss: 0.056943. Entropy: 0.904831.\n",
      "episode: 3058   score: 185.0  epsilon: 1.0    steps: 736  evaluation reward: 233.7\n",
      "episode: 3059   score: 240.0  epsilon: 1.0    steps: 904  evaluation reward: 234.0\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6430: Policy loss: -0.001898. Value loss: 0.127733. Entropy: 1.148615.\n",
      "Iteration 6431: Policy loss: -0.001874. Value loss: 0.072402. Entropy: 1.157692.\n",
      "Iteration 6432: Policy loss: -0.007896. Value loss: 0.047317. Entropy: 1.149705.\n",
      "episode: 3060   score: 215.0  epsilon: 1.0    steps: 256  evaluation reward: 234.3\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6433: Policy loss: 0.003098. Value loss: 0.263517. Entropy: 1.021713.\n",
      "Iteration 6434: Policy loss: 0.001477. Value loss: 0.165622. Entropy: 1.048712.\n",
      "Iteration 6435: Policy loss: -0.005749. Value loss: 0.132243. Entropy: 1.033623.\n",
      "episode: 3061   score: 240.0  epsilon: 1.0    steps: 1000  evaluation reward: 234.85\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6436: Policy loss: 0.002525. Value loss: 0.404493. Entropy: 1.199374.\n",
      "Iteration 6437: Policy loss: -0.000770. Value loss: 0.206954. Entropy: 1.201820.\n",
      "Iteration 6438: Policy loss: -0.007661. Value loss: 0.158571. Entropy: 1.200096.\n",
      "episode: 3062   score: 245.0  epsilon: 1.0    steps: 8  evaluation reward: 235.35\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6439: Policy loss: 0.000428. Value loss: 0.200491. Entropy: 0.968568.\n",
      "Iteration 6440: Policy loss: -0.001380. Value loss: 0.092324. Entropy: 0.970400.\n",
      "Iteration 6441: Policy loss: -0.005691. Value loss: 0.059850. Entropy: 0.974388.\n",
      "episode: 3063   score: 195.0  epsilon: 1.0    steps: 400  evaluation reward: 231.95\n",
      "episode: 3064   score: 360.0  epsilon: 1.0    steps: 536  evaluation reward: 234.2\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6442: Policy loss: 0.000118. Value loss: 0.389051. Entropy: 1.168650.\n",
      "Iteration 6443: Policy loss: -0.004653. Value loss: 0.233379. Entropy: 1.189869.\n",
      "Iteration 6444: Policy loss: -0.009482. Value loss: 0.165366. Entropy: 1.186934.\n",
      "episode: 3065   score: 150.0  epsilon: 1.0    steps: 64  evaluation reward: 234.45\n",
      "episode: 3066   score: 225.0  epsilon: 1.0    steps: 952  evaluation reward: 234.55\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6445: Policy loss: 0.002097. Value loss: 0.240315. Entropy: 0.867624.\n",
      "Iteration 6446: Policy loss: -0.004776. Value loss: 0.121162. Entropy: 0.858779.\n",
      "Iteration 6447: Policy loss: -0.006902. Value loss: 0.097126. Entropy: 0.874764.\n",
      "episode: 3067   score: 240.0  epsilon: 1.0    steps: 88  evaluation reward: 234.5\n",
      "episode: 3068   score: 240.0  epsilon: 1.0    steps: 488  evaluation reward: 235.15\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6448: Policy loss: 0.002932. Value loss: 0.754550. Entropy: 0.988525.\n",
      "Iteration 6449: Policy loss: 0.006447. Value loss: 0.304205. Entropy: 1.005364.\n",
      "Iteration 6450: Policy loss: -0.001048. Value loss: 0.196875. Entropy: 0.991314.\n",
      "episode: 3069   score: 210.0  epsilon: 1.0    steps: 648  evaluation reward: 233.95\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6451: Policy loss: 0.003670. Value loss: 0.439124. Entropy: 1.036993.\n",
      "Iteration 6452: Policy loss: -0.002549. Value loss: 0.221295. Entropy: 1.041628.\n",
      "Iteration 6453: Policy loss: -0.004724. Value loss: 0.159314. Entropy: 1.028352.\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6454: Policy loss: -0.001584. Value loss: 0.310627. Entropy: 1.031181.\n",
      "Iteration 6455: Policy loss: -0.004006. Value loss: 0.154171. Entropy: 1.034277.\n",
      "Iteration 6456: Policy loss: -0.014474. Value loss: 0.120941. Entropy: 1.056942.\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6457: Policy loss: 0.000689. Value loss: 0.245318. Entropy: 1.196159.\n",
      "Iteration 6458: Policy loss: -0.001986. Value loss: 0.156136. Entropy: 1.197008.\n",
      "Iteration 6459: Policy loss: -0.004507. Value loss: 0.105556. Entropy: 1.220181.\n",
      "episode: 3070   score: 180.0  epsilon: 1.0    steps: 128  evaluation reward: 233.2\n",
      "episode: 3071   score: 620.0  epsilon: 1.0    steps: 544  evaluation reward: 237.6\n",
      "episode: 3072   score: 215.0  epsilon: 1.0    steps: 832  evaluation reward: 237.35\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6460: Policy loss: -0.000062. Value loss: 0.178763. Entropy: 1.227194.\n",
      "Iteration 6461: Policy loss: -0.006645. Value loss: 0.106589. Entropy: 1.241303.\n",
      "Iteration 6462: Policy loss: -0.013645. Value loss: 0.096689. Entropy: 1.235427.\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6463: Policy loss: -0.000947. Value loss: 0.137542. Entropy: 1.040042.\n",
      "Iteration 6464: Policy loss: -0.003920. Value loss: 0.063810. Entropy: 1.025851.\n",
      "Iteration 6465: Policy loss: -0.011053. Value loss: 0.044355. Entropy: 1.034417.\n",
      "episode: 3073   score: 225.0  epsilon: 1.0    steps: 24  evaluation reward: 237.15\n",
      "episode: 3074   score: 360.0  epsilon: 1.0    steps: 728  evaluation reward: 239.55\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6466: Policy loss: 0.001378. Value loss: 0.242208. Entropy: 1.052616.\n",
      "Iteration 6467: Policy loss: -0.007488. Value loss: 0.124415. Entropy: 1.070950.\n",
      "Iteration 6468: Policy loss: -0.014586. Value loss: 0.099478. Entropy: 1.086186.\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6469: Policy loss: 0.003979. Value loss: 0.216451. Entropy: 1.027051.\n",
      "Iteration 6470: Policy loss: -0.003781. Value loss: 0.117894. Entropy: 1.024280.\n",
      "Iteration 6471: Policy loss: -0.004841. Value loss: 0.096658. Entropy: 1.036114.\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6472: Policy loss: 0.003485. Value loss: 0.291198. Entropy: 1.268991.\n",
      "Iteration 6473: Policy loss: -0.004083. Value loss: 0.132672. Entropy: 1.266524.\n",
      "Iteration 6474: Policy loss: -0.010367. Value loss: 0.105506. Entropy: 1.273067.\n",
      "episode: 3075   score: 120.0  epsilon: 1.0    steps: 16  evaluation reward: 238.65\n",
      "episode: 3076   score: 260.0  epsilon: 1.0    steps: 360  evaluation reward: 239.15\n",
      "episode: 3077   score: 220.0  epsilon: 1.0    steps: 616  evaluation reward: 239.55\n",
      "episode: 3078   score: 310.0  epsilon: 1.0    steps: 808  evaluation reward: 239.65\n",
      "episode: 3079   score: 210.0  epsilon: 1.0    steps: 1016  evaluation reward: 237.6\n",
      "episode: 3080   score: 280.0  epsilon: 1.0    steps: 1024  evaluation reward: 238.15\n",
      "Training network. lr: 0.000200. clip: 0.080195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6475: Policy loss: -0.000148. Value loss: 0.255485. Entropy: 1.142878.\n",
      "Iteration 6476: Policy loss: -0.002976. Value loss: 0.171227. Entropy: 1.137406.\n",
      "Iteration 6477: Policy loss: -0.005536. Value loss: 0.166708. Entropy: 1.139427.\n",
      "episode: 3081   score: 180.0  epsilon: 1.0    steps: 464  evaluation reward: 238.1\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6478: Policy loss: 0.000412. Value loss: 0.203649. Entropy: 0.839065.\n",
      "Iteration 6479: Policy loss: -0.003868. Value loss: 0.135926. Entropy: 0.841166.\n",
      "Iteration 6480: Policy loss: -0.005534. Value loss: 0.099012. Entropy: 0.855067.\n",
      "episode: 3082   score: 360.0  epsilon: 1.0    steps: 584  evaluation reward: 239.6\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6481: Policy loss: 0.000620. Value loss: 0.199428. Entropy: 0.972800.\n",
      "Iteration 6482: Policy loss: -0.003941. Value loss: 0.118860. Entropy: 0.951275.\n",
      "Iteration 6483: Policy loss: -0.011275. Value loss: 0.100735. Entropy: 0.976536.\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6484: Policy loss: 0.001998. Value loss: 0.186065. Entropy: 1.044651.\n",
      "Iteration 6485: Policy loss: -0.006786. Value loss: 0.116500. Entropy: 1.068988.\n",
      "Iteration 6486: Policy loss: -0.011098. Value loss: 0.082105. Entropy: 1.064317.\n",
      "episode: 3083   score: 210.0  epsilon: 1.0    steps: 928  evaluation reward: 239.55\n",
      "episode: 3084   score: 210.0  epsilon: 1.0    steps: 992  evaluation reward: 239.55\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6487: Policy loss: 0.000110. Value loss: 0.148062. Entropy: 1.063645.\n",
      "Iteration 6488: Policy loss: -0.004811. Value loss: 0.093480. Entropy: 1.061970.\n",
      "Iteration 6489: Policy loss: -0.009297. Value loss: 0.081104. Entropy: 1.052355.\n",
      "episode: 3085   score: 225.0  epsilon: 1.0    steps: 312  evaluation reward: 239.45\n",
      "episode: 3086   score: 210.0  epsilon: 1.0    steps: 472  evaluation reward: 238.15\n",
      "episode: 3087   score: 210.0  epsilon: 1.0    steps: 960  evaluation reward: 238.15\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6490: Policy loss: 0.000344. Value loss: 0.143864. Entropy: 1.101358.\n",
      "Iteration 6491: Policy loss: -0.001165. Value loss: 0.098031. Entropy: 1.117639.\n",
      "Iteration 6492: Policy loss: -0.007315. Value loss: 0.084388. Entropy: 1.106461.\n",
      "episode: 3088   score: 240.0  epsilon: 1.0    steps: 368  evaluation reward: 235.1\n",
      "episode: 3089   score: 225.0  epsilon: 1.0    steps: 480  evaluation reward: 235.55\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6493: Policy loss: -0.001213. Value loss: 0.175066. Entropy: 1.005563.\n",
      "Iteration 6494: Policy loss: -0.004019. Value loss: 0.120899. Entropy: 0.998630.\n",
      "Iteration 6495: Policy loss: -0.007481. Value loss: 0.113981. Entropy: 0.996388.\n",
      "episode: 3090   score: 210.0  epsilon: 1.0    steps: 992  evaluation reward: 235.55\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6496: Policy loss: -0.001673. Value loss: 0.157406. Entropy: 0.968185.\n",
      "Iteration 6497: Policy loss: -0.005247. Value loss: 0.081626. Entropy: 0.972880.\n",
      "Iteration 6498: Policy loss: -0.008078. Value loss: 0.058446. Entropy: 0.970738.\n",
      "episode: 3091   score: 90.0  epsilon: 1.0    steps: 8  evaluation reward: 234.75\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6499: Policy loss: 0.002066. Value loss: 0.189978. Entropy: 0.914158.\n",
      "Iteration 6500: Policy loss: -0.002675. Value loss: 0.101669. Entropy: 0.923478.\n",
      "Iteration 6501: Policy loss: -0.007298. Value loss: 0.069065. Entropy: 0.935776.\n",
      "episode: 3092   score: 210.0  epsilon: 1.0    steps: 792  evaluation reward: 234.75\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6502: Policy loss: -0.000258. Value loss: 0.294671. Entropy: 1.197741.\n",
      "Iteration 6503: Policy loss: -0.002171. Value loss: 0.144215. Entropy: 1.196763.\n",
      "Iteration 6504: Policy loss: -0.009982. Value loss: 0.121597. Entropy: 1.210128.\n",
      "episode: 3093   score: 180.0  epsilon: 1.0    steps: 504  evaluation reward: 234.3\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6505: Policy loss: -0.002184. Value loss: 0.150175. Entropy: 1.029362.\n",
      "Iteration 6506: Policy loss: -0.005751. Value loss: 0.083144. Entropy: 1.007720.\n",
      "Iteration 6507: Policy loss: -0.011886. Value loss: 0.071356. Entropy: 1.005761.\n",
      "episode: 3094   score: 245.0  epsilon: 1.0    steps: 224  evaluation reward: 234.35\n",
      "episode: 3095   score: 280.0  epsilon: 1.0    steps: 1024  evaluation reward: 234.75\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6508: Policy loss: 0.002041. Value loss: 0.243749. Entropy: 1.137498.\n",
      "Iteration 6509: Policy loss: -0.003962. Value loss: 0.152707. Entropy: 1.143466.\n",
      "Iteration 6510: Policy loss: -0.008676. Value loss: 0.110079. Entropy: 1.145541.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6511: Policy loss: 0.001356. Value loss: 0.190982. Entropy: 1.059986.\n",
      "Iteration 6512: Policy loss: -0.006444. Value loss: 0.120278. Entropy: 1.048566.\n",
      "Iteration 6513: Policy loss: -0.010867. Value loss: 0.101706. Entropy: 1.070101.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6514: Policy loss: 0.004340. Value loss: 0.477031. Entropy: 1.166688.\n",
      "Iteration 6515: Policy loss: 0.005325. Value loss: 0.318714. Entropy: 1.166903.\n",
      "Iteration 6516: Policy loss: 0.001610. Value loss: 0.280254. Entropy: 1.161838.\n",
      "episode: 3096   score: 280.0  epsilon: 1.0    steps: 80  evaluation reward: 235.75\n",
      "episode: 3097   score: 245.0  epsilon: 1.0    steps: 96  evaluation reward: 237.0\n",
      "episode: 3098   score: 240.0  epsilon: 1.0    steps: 656  evaluation reward: 237.1\n",
      "episode: 3099   score: 490.0  epsilon: 1.0    steps: 1008  evaluation reward: 239.9\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6517: Policy loss: 0.001838. Value loss: 0.355595. Entropy: 1.002546.\n",
      "Iteration 6518: Policy loss: -0.004639. Value loss: 0.213173. Entropy: 1.025411.\n",
      "Iteration 6519: Policy loss: -0.007672. Value loss: 0.176224. Entropy: 1.019579.\n",
      "episode: 3100   score: 260.0  epsilon: 1.0    steps: 880  evaluation reward: 240.4\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6520: Policy loss: -0.000721. Value loss: 0.222377. Entropy: 1.036454.\n",
      "Iteration 6521: Policy loss: -0.000886. Value loss: 0.150895. Entropy: 1.004918.\n",
      "Iteration 6522: Policy loss: -0.010412. Value loss: 0.113820. Entropy: 1.046171.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6523: Policy loss: 0.002751. Value loss: 0.280069. Entropy: 0.996786.\n",
      "Iteration 6524: Policy loss: -0.002912. Value loss: 0.118130. Entropy: 0.991824.\n",
      "Iteration 6525: Policy loss: -0.006702. Value loss: 0.078007. Entropy: 0.994321.\n",
      "now time :  2019-02-28 12:13:19.198658\n",
      "episode: 3101   score: 295.0  epsilon: 1.0    steps: 640  evaluation reward: 241.7\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6526: Policy loss: 0.004807. Value loss: 0.331819. Entropy: 1.217954.\n",
      "Iteration 6527: Policy loss: -0.003057. Value loss: 0.143094. Entropy: 1.208704.\n",
      "Iteration 6528: Policy loss: -0.008974. Value loss: 0.113105. Entropy: 1.215214.\n",
      "episode: 3102   score: 180.0  epsilon: 1.0    steps: 320  evaluation reward: 240.85\n",
      "episode: 3103   score: 180.0  epsilon: 1.0    steps: 400  evaluation reward: 240.85\n",
      "episode: 3104   score: 320.0  epsilon: 1.0    steps: 512  evaluation reward: 242.8\n",
      "episode: 3105   score: 650.0  epsilon: 1.0    steps: 688  evaluation reward: 246.9\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6529: Policy loss: 0.001563. Value loss: 0.626579. Entropy: 1.106895.\n",
      "Iteration 6530: Policy loss: -0.002201. Value loss: 0.346012. Entropy: 1.073774.\n",
      "Iteration 6531: Policy loss: -0.004172. Value loss: 0.299932. Entropy: 1.112828.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6532: Policy loss: -0.001671. Value loss: 0.286572. Entropy: 0.916553.\n",
      "Iteration 6533: Policy loss: -0.001959. Value loss: 0.161203. Entropy: 0.913486.\n",
      "Iteration 6534: Policy loss: -0.007800. Value loss: 0.111251. Entropy: 0.906366.\n",
      "episode: 3106   score: 210.0  epsilon: 1.0    steps: 104  evaluation reward: 247.0\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6535: Policy loss: 0.001973. Value loss: 0.360531. Entropy: 0.977131.\n",
      "Iteration 6536: Policy loss: -0.006561. Value loss: 0.181777. Entropy: 0.992764.\n",
      "Iteration 6537: Policy loss: -0.007354. Value loss: 0.109743. Entropy: 0.961222.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3107   score: 235.0  epsilon: 1.0    steps: 240  evaluation reward: 247.25\n",
      "episode: 3108   score: 330.0  epsilon: 1.0    steps: 792  evaluation reward: 247.7\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6538: Policy loss: 0.000296. Value loss: 0.236012. Entropy: 1.122324.\n",
      "Iteration 6539: Policy loss: -0.004719. Value loss: 0.121380. Entropy: 1.108409.\n",
      "Iteration 6540: Policy loss: -0.006950. Value loss: 0.101019. Entropy: 1.117257.\n",
      "episode: 3109   score: 215.0  epsilon: 1.0    steps: 488  evaluation reward: 245.6\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6541: Policy loss: 0.001647. Value loss: 0.186782. Entropy: 1.066058.\n",
      "Iteration 6542: Policy loss: -0.003244. Value loss: 0.103579. Entropy: 1.045113.\n",
      "Iteration 6543: Policy loss: -0.011429. Value loss: 0.079797. Entropy: 1.061569.\n",
      "episode: 3110   score: 265.0  epsilon: 1.0    steps: 408  evaluation reward: 245.7\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6544: Policy loss: 0.001853. Value loss: 0.329247. Entropy: 1.127737.\n",
      "Iteration 6545: Policy loss: -0.003315. Value loss: 0.179636. Entropy: 1.124707.\n",
      "Iteration 6546: Policy loss: -0.008022. Value loss: 0.145367. Entropy: 1.155341.\n",
      "episode: 3111   score: 240.0  epsilon: 1.0    steps: 72  evaluation reward: 246.25\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6547: Policy loss: -0.000536. Value loss: 0.205175. Entropy: 1.041391.\n",
      "Iteration 6548: Policy loss: -0.007628. Value loss: 0.110726. Entropy: 1.027834.\n",
      "Iteration 6549: Policy loss: -0.012024. Value loss: 0.089743. Entropy: 1.049077.\n",
      "episode: 3112   score: 305.0  epsilon: 1.0    steps: 672  evaluation reward: 246.45\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6550: Policy loss: -0.001647. Value loss: 0.116272. Entropy: 1.122868.\n",
      "Iteration 6551: Policy loss: -0.006370. Value loss: 0.072111. Entropy: 1.123448.\n",
      "Iteration 6552: Policy loss: -0.010858. Value loss: 0.046626. Entropy: 1.113690.\n",
      "episode: 3113   score: 225.0  epsilon: 1.0    steps: 632  evaluation reward: 246.6\n",
      "episode: 3114   score: 330.0  epsilon: 1.0    steps: 896  evaluation reward: 247.1\n",
      "episode: 3115   score: 210.0  epsilon: 1.0    steps: 944  evaluation reward: 247.05\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6553: Policy loss: 0.000042. Value loss: 0.131118. Entropy: 1.072064.\n",
      "Iteration 6554: Policy loss: -0.005328. Value loss: 0.075432. Entropy: 1.087835.\n",
      "Iteration 6555: Policy loss: -0.008113. Value loss: 0.057018. Entropy: 1.077184.\n",
      "episode: 3116   score: 210.0  epsilon: 1.0    steps: 824  evaluation reward: 247.05\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6556: Policy loss: 0.004445. Value loss: 0.109087. Entropy: 1.018010.\n",
      "Iteration 6557: Policy loss: -0.004370. Value loss: 0.072610. Entropy: 1.004066.\n",
      "Iteration 6558: Policy loss: -0.008645. Value loss: 0.061615. Entropy: 1.029780.\n",
      "episode: 3117   score: 260.0  epsilon: 1.0    steps: 264  evaluation reward: 246.15\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6559: Policy loss: -0.001914. Value loss: 0.085675. Entropy: 1.064281.\n",
      "Iteration 6560: Policy loss: -0.003627. Value loss: 0.064513. Entropy: 1.060573.\n",
      "Iteration 6561: Policy loss: -0.005220. Value loss: 0.049791. Entropy: 1.055580.\n",
      "episode: 3118   score: 240.0  epsilon: 1.0    steps: 368  evaluation reward: 246.45\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6562: Policy loss: -0.001677. Value loss: 0.168591. Entropy: 1.035297.\n",
      "Iteration 6563: Policy loss: -0.001184. Value loss: 0.076857. Entropy: 1.030463.\n",
      "Iteration 6564: Policy loss: -0.008272. Value loss: 0.063492. Entropy: 1.023407.\n",
      "episode: 3119   score: 320.0  epsilon: 1.0    steps: 904  evaluation reward: 246.95\n",
      "episode: 3120   score: 185.0  epsilon: 1.0    steps: 936  evaluation reward: 245.2\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6565: Policy loss: -0.000847. Value loss: 0.124114. Entropy: 1.081176.\n",
      "Iteration 6566: Policy loss: -0.009415. Value loss: 0.080514. Entropy: 1.073230.\n",
      "Iteration 6567: Policy loss: -0.007954. Value loss: 0.059275. Entropy: 1.070419.\n",
      "episode: 3121   score: 225.0  epsilon: 1.0    steps: 352  evaluation reward: 245.05\n",
      "episode: 3122   score: 245.0  epsilon: 1.0    steps: 704  evaluation reward: 245.4\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6568: Policy loss: 0.002233. Value loss: 0.136547. Entropy: 1.098720.\n",
      "Iteration 6569: Policy loss: -0.003878. Value loss: 0.099689. Entropy: 1.108584.\n",
      "Iteration 6570: Policy loss: -0.009723. Value loss: 0.081562. Entropy: 1.095789.\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6571: Policy loss: 0.000424. Value loss: 0.117059. Entropy: 0.987428.\n",
      "Iteration 6572: Policy loss: -0.009236. Value loss: 0.072674. Entropy: 1.003531.\n",
      "Iteration 6573: Policy loss: -0.011021. Value loss: 0.062335. Entropy: 1.004959.\n",
      "episode: 3123   score: 215.0  epsilon: 1.0    steps: 472  evaluation reward: 245.3\n",
      "episode: 3124   score: 210.0  epsilon: 1.0    steps: 544  evaluation reward: 245.55\n",
      "episode: 3125   score: 215.0  epsilon: 1.0    steps: 776  evaluation reward: 245.6\n",
      "episode: 3126   score: 210.0  epsilon: 1.0    steps: 992  evaluation reward: 245.6\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6574: Policy loss: -0.002359. Value loss: 0.180567. Entropy: 1.117100.\n",
      "Iteration 6575: Policy loss: -0.006864. Value loss: 0.130820. Entropy: 1.122784.\n",
      "Iteration 6576: Policy loss: -0.010675. Value loss: 0.118919. Entropy: 1.123367.\n",
      "episode: 3127   score: 110.0  epsilon: 1.0    steps: 656  evaluation reward: 245.15\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6577: Policy loss: 0.001579. Value loss: 0.108982. Entropy: 0.837506.\n",
      "Iteration 6578: Policy loss: -0.002472. Value loss: 0.076799. Entropy: 0.864806.\n",
      "Iteration 6579: Policy loss: -0.002859. Value loss: 0.067468. Entropy: 0.852146.\n",
      "episode: 3128   score: 180.0  epsilon: 1.0    steps: 376  evaluation reward: 242.85\n",
      "episode: 3129   score: 240.0  epsilon: 1.0    steps: 688  evaluation reward: 243.85\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6580: Policy loss: -0.000656. Value loss: 0.183506. Entropy: 1.116460.\n",
      "Iteration 6581: Policy loss: 0.000952. Value loss: 0.119750. Entropy: 1.120708.\n",
      "Iteration 6582: Policy loss: -0.004025. Value loss: 0.074998. Entropy: 1.111126.\n",
      "episode: 3130   score: 210.0  epsilon: 1.0    steps: 120  evaluation reward: 243.8\n",
      "episode: 3131   score: 125.0  epsilon: 1.0    steps: 536  evaluation reward: 243.7\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6583: Policy loss: 0.003010. Value loss: 0.159310. Entropy: 0.889806.\n",
      "Iteration 6584: Policy loss: -0.003812. Value loss: 0.104180. Entropy: 0.895199.\n",
      "Iteration 6585: Policy loss: -0.005799. Value loss: 0.080219. Entropy: 0.879967.\n",
      "episode: 3132   score: 210.0  epsilon: 1.0    steps: 552  evaluation reward: 243.7\n",
      "episode: 3133   score: 135.0  epsilon: 1.0    steps: 752  evaluation reward: 242.2\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6586: Policy loss: -0.000188. Value loss: 0.131470. Entropy: 1.073546.\n",
      "Iteration 6587: Policy loss: -0.007047. Value loss: 0.079392. Entropy: 1.071273.\n",
      "Iteration 6588: Policy loss: -0.013449. Value loss: 0.067857. Entropy: 1.077405.\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6589: Policy loss: 0.000112. Value loss: 0.107745. Entropy: 0.976719.\n",
      "Iteration 6590: Policy loss: -0.006604. Value loss: 0.072408. Entropy: 0.996482.\n",
      "Iteration 6591: Policy loss: -0.011861. Value loss: 0.055165. Entropy: 0.993803.\n",
      "episode: 3134   score: 135.0  epsilon: 1.0    steps: 256  evaluation reward: 241.45\n",
      "episode: 3135   score: 245.0  epsilon: 1.0    steps: 448  evaluation reward: 241.35\n",
      "episode: 3136   score: 120.0  epsilon: 1.0    steps: 672  evaluation reward: 240.45\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6592: Policy loss: 0.001559. Value loss: 0.182112. Entropy: 1.219481.\n",
      "Iteration 6593: Policy loss: -0.008965. Value loss: 0.168297. Entropy: 1.211068.\n",
      "Iteration 6594: Policy loss: -0.009994. Value loss: 0.119230. Entropy: 1.210032.\n",
      "episode: 3137   score: 210.0  epsilon: 1.0    steps: 840  evaluation reward: 240.45\n",
      "episode: 3138   score: 310.0  epsilon: 1.0    steps: 928  evaluation reward: 241.15\n",
      "Training network. lr: 0.000200. clip: 0.079881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6595: Policy loss: -0.002923. Value loss: 0.107544. Entropy: 0.883350.\n",
      "Iteration 6596: Policy loss: -0.008312. Value loss: 0.070418. Entropy: 0.880668.\n",
      "Iteration 6597: Policy loss: -0.012928. Value loss: 0.049538. Entropy: 0.878861.\n",
      "episode: 3139   score: 210.0  epsilon: 1.0    steps: 592  evaluation reward: 240.7\n",
      "episode: 3140   score: 250.0  epsilon: 1.0    steps: 648  evaluation reward: 241.4\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6598: Policy loss: 0.002715. Value loss: 0.132721. Entropy: 1.057269.\n",
      "Iteration 6599: Policy loss: -0.001777. Value loss: 0.067534. Entropy: 1.053133.\n",
      "Iteration 6600: Policy loss: -0.002685. Value loss: 0.067158. Entropy: 1.067686.\n",
      "episode: 3141   score: 210.0  epsilon: 1.0    steps: 416  evaluation reward: 242.0\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6601: Policy loss: 0.002288. Value loss: 0.092244. Entropy: 0.859925.\n",
      "Iteration 6602: Policy loss: -0.003227. Value loss: 0.062916. Entropy: 0.881289.\n",
      "Iteration 6603: Policy loss: -0.007439. Value loss: 0.058095. Entropy: 0.847210.\n",
      "episode: 3142   score: 210.0  epsilon: 1.0    steps: 80  evaluation reward: 242.0\n",
      "episode: 3143   score: 105.0  epsilon: 1.0    steps: 832  evaluation reward: 240.95\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6604: Policy loss: 0.000116. Value loss: 0.165805. Entropy: 1.057403.\n",
      "Iteration 6605: Policy loss: -0.004017. Value loss: 0.094516. Entropy: 1.052223.\n",
      "Iteration 6606: Policy loss: -0.011133. Value loss: 0.091078. Entropy: 1.070035.\n",
      "episode: 3144   score: 210.0  epsilon: 1.0    steps: 992  evaluation reward: 241.25\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6607: Policy loss: -0.000758. Value loss: 0.134919. Entropy: 0.954990.\n",
      "Iteration 6608: Policy loss: -0.009452. Value loss: 0.070278. Entropy: 0.988357.\n",
      "Iteration 6609: Policy loss: -0.015104. Value loss: 0.048851. Entropy: 0.940414.\n",
      "episode: 3145   score: 250.0  epsilon: 1.0    steps: 160  evaluation reward: 241.9\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6610: Policy loss: 0.001340. Value loss: 0.156612. Entropy: 1.009959.\n",
      "Iteration 6611: Policy loss: -0.006717. Value loss: 0.084942. Entropy: 1.028158.\n",
      "Iteration 6612: Policy loss: -0.012028. Value loss: 0.059953. Entropy: 1.014793.\n",
      "episode: 3146   score: 180.0  epsilon: 1.0    steps: 704  evaluation reward: 241.4\n",
      "episode: 3147   score: 240.0  epsilon: 1.0    steps: 736  evaluation reward: 240.8\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6613: Policy loss: 0.007287. Value loss: 0.512402. Entropy: 1.130073.\n",
      "Iteration 6614: Policy loss: 0.006742. Value loss: 0.171043. Entropy: 1.142001.\n",
      "Iteration 6615: Policy loss: 0.003130. Value loss: 0.114185. Entropy: 1.143381.\n",
      "episode: 3148   score: 320.0  epsilon: 1.0    steps: 544  evaluation reward: 241.7\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6616: Policy loss: 0.002201. Value loss: 0.147441. Entropy: 1.016012.\n",
      "Iteration 6617: Policy loss: -0.006133. Value loss: 0.086242. Entropy: 1.001432.\n",
      "Iteration 6618: Policy loss: -0.007478. Value loss: 0.066904. Entropy: 1.018893.\n",
      "episode: 3149   score: 310.0  epsilon: 1.0    steps: 616  evaluation reward: 242.7\n",
      "episode: 3150   score: 240.0  epsilon: 1.0    steps: 760  evaluation reward: 242.85\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6619: Policy loss: 0.000296. Value loss: 0.192939. Entropy: 1.111494.\n",
      "Iteration 6620: Policy loss: -0.006552. Value loss: 0.105813. Entropy: 1.102399.\n",
      "Iteration 6621: Policy loss: -0.009624. Value loss: 0.071181. Entropy: 1.104217.\n",
      "now time :  2019-02-28 12:14:31.029298\n",
      "episode: 3151   score: 150.0  epsilon: 1.0    steps: 504  evaluation reward: 242.1\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6622: Policy loss: 0.002669. Value loss: 0.140715. Entropy: 0.920469.\n",
      "Iteration 6623: Policy loss: -0.004451. Value loss: 0.068738. Entropy: 0.903244.\n",
      "Iteration 6624: Policy loss: -0.010743. Value loss: 0.054557. Entropy: 0.876316.\n",
      "episode: 3152   score: 300.0  epsilon: 1.0    steps: 672  evaluation reward: 241.15\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6625: Policy loss: 0.003761. Value loss: 0.304069. Entropy: 1.115152.\n",
      "Iteration 6626: Policy loss: -0.002466. Value loss: 0.162379. Entropy: 1.113821.\n",
      "Iteration 6627: Policy loss: -0.007767. Value loss: 0.110550. Entropy: 1.128312.\n",
      "episode: 3153   score: 220.0  epsilon: 1.0    steps: 808  evaluation reward: 241.75\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6628: Policy loss: 0.001300. Value loss: 0.089769. Entropy: 1.008771.\n",
      "Iteration 6629: Policy loss: -0.005290. Value loss: 0.053072. Entropy: 0.997097.\n",
      "Iteration 6630: Policy loss: -0.006316. Value loss: 0.038617. Entropy: 1.000716.\n",
      "episode: 3154   score: 545.0  epsilon: 1.0    steps: 368  evaluation reward: 244.5\n",
      "episode: 3155   score: 245.0  epsilon: 1.0    steps: 528  evaluation reward: 243.8\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6631: Policy loss: 0.001255. Value loss: 0.174360. Entropy: 1.085363.\n",
      "Iteration 6632: Policy loss: -0.002189. Value loss: 0.098461. Entropy: 1.093505.\n",
      "Iteration 6633: Policy loss: -0.009070. Value loss: 0.071881. Entropy: 1.090333.\n",
      "episode: 3156   score: 210.0  epsilon: 1.0    steps: 360  evaluation reward: 242.05\n",
      "episode: 3157   score: 250.0  epsilon: 1.0    steps: 448  evaluation reward: 241.55\n",
      "episode: 3158   score: 290.0  epsilon: 1.0    steps: 720  evaluation reward: 242.6\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6634: Policy loss: -0.000372. Value loss: 0.224315. Entropy: 1.024401.\n",
      "Iteration 6635: Policy loss: -0.003063. Value loss: 0.159878. Entropy: 1.019850.\n",
      "Iteration 6636: Policy loss: -0.008297. Value loss: 0.125936. Entropy: 1.041587.\n",
      "episode: 3159   score: 210.0  epsilon: 1.0    steps: 544  evaluation reward: 242.3\n",
      "episode: 3160   score: 210.0  epsilon: 1.0    steps: 1000  evaluation reward: 242.25\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6637: Policy loss: 0.000628. Value loss: 0.178795. Entropy: 0.885257.\n",
      "Iteration 6638: Policy loss: -0.002148. Value loss: 0.111702. Entropy: 0.878718.\n",
      "Iteration 6639: Policy loss: -0.008411. Value loss: 0.084375. Entropy: 0.867096.\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6640: Policy loss: -0.001278. Value loss: 0.120552. Entropy: 0.896959.\n",
      "Iteration 6641: Policy loss: 0.000891. Value loss: 0.082371. Entropy: 0.890902.\n",
      "Iteration 6642: Policy loss: -0.005578. Value loss: 0.076826. Entropy: 0.913376.\n",
      "episode: 3161   score: 120.0  epsilon: 1.0    steps: 280  evaluation reward: 241.05\n",
      "episode: 3162   score: 210.0  epsilon: 1.0    steps: 328  evaluation reward: 240.7\n",
      "episode: 3163   score: 240.0  epsilon: 1.0    steps: 760  evaluation reward: 241.15\n",
      "episode: 3164   score: 180.0  epsilon: 1.0    steps: 904  evaluation reward: 239.35\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6643: Policy loss: 0.000200. Value loss: 0.167872. Entropy: 1.115174.\n",
      "Iteration 6644: Policy loss: -0.002125. Value loss: 0.100989. Entropy: 1.101677.\n",
      "Iteration 6645: Policy loss: -0.007165. Value loss: 0.088773. Entropy: 1.117740.\n",
      "episode: 3165   score: 210.0  epsilon: 1.0    steps: 680  evaluation reward: 239.95\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6646: Policy loss: 0.002033. Value loss: 0.163900. Entropy: 0.787660.\n",
      "Iteration 6647: Policy loss: -0.004031. Value loss: 0.097238. Entropy: 0.780580.\n",
      "Iteration 6648: Policy loss: -0.008431. Value loss: 0.078984. Entropy: 0.790834.\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6649: Policy loss: -0.000223. Value loss: 0.164170. Entropy: 1.045874.\n",
      "Iteration 6650: Policy loss: -0.003727. Value loss: 0.098984. Entropy: 1.065898.\n",
      "Iteration 6651: Policy loss: -0.007409. Value loss: 0.061676. Entropy: 1.060271.\n",
      "episode: 3166   score: 245.0  epsilon: 1.0    steps: 160  evaluation reward: 240.15\n",
      "episode: 3167   score: 135.0  epsilon: 1.0    steps: 752  evaluation reward: 239.1\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6652: Policy loss: -0.002097. Value loss: 0.234360. Entropy: 1.167936.\n",
      "Iteration 6653: Policy loss: -0.002569. Value loss: 0.140178. Entropy: 1.182262.\n",
      "Iteration 6654: Policy loss: -0.013279. Value loss: 0.108037. Entropy: 1.164111.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3168   score: 135.0  epsilon: 1.0    steps: 576  evaluation reward: 238.05\n",
      "episode: 3169   score: 315.0  epsilon: 1.0    steps: 608  evaluation reward: 239.1\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6655: Policy loss: 0.000777. Value loss: 0.173507. Entropy: 1.079863.\n",
      "Iteration 6656: Policy loss: -0.004971. Value loss: 0.110098. Entropy: 1.088675.\n",
      "Iteration 6657: Policy loss: -0.008208. Value loss: 0.084512. Entropy: 1.097745.\n",
      "episode: 3170   score: 210.0  epsilon: 1.0    steps: 288  evaluation reward: 239.4\n",
      "episode: 3171   score: 325.0  epsilon: 1.0    steps: 400  evaluation reward: 236.45\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6658: Policy loss: 0.000550. Value loss: 0.322772. Entropy: 0.976481.\n",
      "Iteration 6659: Policy loss: -0.002113. Value loss: 0.194896. Entropy: 0.959239.\n",
      "Iteration 6660: Policy loss: -0.006518. Value loss: 0.161574. Entropy: 0.962025.\n",
      "episode: 3172   score: 275.0  epsilon: 1.0    steps: 72  evaluation reward: 237.05\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6661: Policy loss: 0.001097. Value loss: 0.194389. Entropy: 0.994691.\n",
      "Iteration 6662: Policy loss: -0.006497. Value loss: 0.121512. Entropy: 0.988207.\n",
      "Iteration 6663: Policy loss: -0.010051. Value loss: 0.094121. Entropy: 0.997140.\n",
      "episode: 3173   score: 210.0  epsilon: 1.0    steps: 720  evaluation reward: 236.9\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6664: Policy loss: 0.001035. Value loss: 0.107829. Entropy: 1.053781.\n",
      "Iteration 6665: Policy loss: -0.005003. Value loss: 0.078779. Entropy: 1.042808.\n",
      "Iteration 6666: Policy loss: -0.006757. Value loss: 0.053638. Entropy: 1.031165.\n",
      "episode: 3174   score: 155.0  epsilon: 1.0    steps: 648  evaluation reward: 234.85\n",
      "episode: 3175   score: 180.0  epsilon: 1.0    steps: 736  evaluation reward: 235.45\n",
      "episode: 3176   score: 210.0  epsilon: 1.0    steps: 824  evaluation reward: 234.95\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6667: Policy loss: -0.000739. Value loss: 0.090086. Entropy: 1.039212.\n",
      "Iteration 6668: Policy loss: -0.000203. Value loss: 0.052220. Entropy: 1.062755.\n",
      "Iteration 6669: Policy loss: -0.005764. Value loss: 0.047670. Entropy: 1.035498.\n",
      "episode: 3177   score: 155.0  epsilon: 1.0    steps: 72  evaluation reward: 234.3\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6670: Policy loss: 0.000696. Value loss: 0.127456. Entropy: 0.894544.\n",
      "Iteration 6671: Policy loss: -0.004982. Value loss: 0.078663. Entropy: 0.907764.\n",
      "Iteration 6672: Policy loss: -0.010802. Value loss: 0.060155. Entropy: 0.894033.\n",
      "episode: 3178   score: 245.0  epsilon: 1.0    steps: 656  evaluation reward: 233.65\n",
      "episode: 3179   score: 455.0  epsilon: 1.0    steps: 832  evaluation reward: 236.1\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6673: Policy loss: 0.000939. Value loss: 0.183216. Entropy: 1.171512.\n",
      "Iteration 6674: Policy loss: -0.005381. Value loss: 0.113715. Entropy: 1.167181.\n",
      "Iteration 6675: Policy loss: -0.013872. Value loss: 0.090229. Entropy: 1.168563.\n",
      "episode: 3180   score: 135.0  epsilon: 1.0    steps: 712  evaluation reward: 234.65\n",
      "episode: 3181   score: 210.0  epsilon: 1.0    steps: 1000  evaluation reward: 234.95\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6676: Policy loss: 0.000982. Value loss: 0.098733. Entropy: 0.911798.\n",
      "Iteration 6677: Policy loss: -0.001855. Value loss: 0.067131. Entropy: 0.887590.\n",
      "Iteration 6678: Policy loss: -0.012899. Value loss: 0.059131. Entropy: 0.918125.\n",
      "episode: 3182   score: 210.0  epsilon: 1.0    steps: 656  evaluation reward: 233.45\n",
      "episode: 3183   score: 255.0  epsilon: 1.0    steps: 776  evaluation reward: 233.9\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6679: Policy loss: 0.001151. Value loss: 0.109588. Entropy: 1.076543.\n",
      "Iteration 6680: Policy loss: -0.008133. Value loss: 0.082151. Entropy: 1.058037.\n",
      "Iteration 6681: Policy loss: -0.011658. Value loss: 0.066798. Entropy: 1.055050.\n",
      "episode: 3184   score: 180.0  epsilon: 1.0    steps: 264  evaluation reward: 233.6\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6682: Policy loss: -0.001294. Value loss: 0.125996. Entropy: 0.910733.\n",
      "Iteration 6683: Policy loss: -0.000670. Value loss: 0.089449. Entropy: 0.879690.\n",
      "Iteration 6684: Policy loss: -0.006702. Value loss: 0.076456. Entropy: 0.908089.\n",
      "episode: 3185   score: 210.0  epsilon: 1.0    steps: 64  evaluation reward: 233.45\n",
      "episode: 3186   score: 210.0  epsilon: 1.0    steps: 712  evaluation reward: 233.45\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6685: Policy loss: -0.000851. Value loss: 0.129820. Entropy: 1.005355.\n",
      "Iteration 6686: Policy loss: -0.005114. Value loss: 0.067769. Entropy: 1.016072.\n",
      "Iteration 6687: Policy loss: -0.010671. Value loss: 0.060422. Entropy: 1.003416.\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6688: Policy loss: -0.000276. Value loss: 0.121214. Entropy: 0.901192.\n",
      "Iteration 6689: Policy loss: -0.005820. Value loss: 0.071396. Entropy: 0.928635.\n",
      "Iteration 6690: Policy loss: -0.007021. Value loss: 0.049257. Entropy: 0.919788.\n",
      "episode: 3187   score: 210.0  epsilon: 1.0    steps: 344  evaluation reward: 233.45\n",
      "episode: 3188   score: 210.0  epsilon: 1.0    steps: 472  evaluation reward: 233.15\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6691: Policy loss: 0.001413. Value loss: 0.162372. Entropy: 1.082379.\n",
      "Iteration 6692: Policy loss: -0.005329. Value loss: 0.125619. Entropy: 1.088786.\n",
      "Iteration 6693: Policy loss: -0.010477. Value loss: 0.099745. Entropy: 1.093692.\n",
      "episode: 3189   score: 245.0  epsilon: 1.0    steps: 88  evaluation reward: 233.35\n",
      "episode: 3190   score: 180.0  epsilon: 1.0    steps: 488  evaluation reward: 233.05\n",
      "episode: 3191   score: 210.0  epsilon: 1.0    steps: 816  evaluation reward: 234.25\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6694: Policy loss: 0.000641. Value loss: 0.102096. Entropy: 0.898014.\n",
      "Iteration 6695: Policy loss: -0.002391. Value loss: 0.063220. Entropy: 0.886056.\n",
      "Iteration 6696: Policy loss: -0.009477. Value loss: 0.061993. Entropy: 0.889473.\n",
      "episode: 3192   score: 180.0  epsilon: 1.0    steps: 504  evaluation reward: 233.95\n",
      "episode: 3193   score: 345.0  epsilon: 1.0    steps: 992  evaluation reward: 235.6\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6697: Policy loss: -0.002744. Value loss: 0.134109. Entropy: 0.980030.\n",
      "Iteration 6698: Policy loss: -0.004306. Value loss: 0.093886. Entropy: 0.953449.\n",
      "Iteration 6699: Policy loss: -0.009439. Value loss: 0.067372. Entropy: 0.953272.\n",
      "episode: 3194   score: 210.0  epsilon: 1.0    steps: 776  evaluation reward: 235.25\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6700: Policy loss: -0.000839. Value loss: 0.101495. Entropy: 0.929287.\n",
      "Iteration 6701: Policy loss: -0.003271. Value loss: 0.066442. Entropy: 0.916484.\n",
      "Iteration 6702: Policy loss: -0.006068. Value loss: 0.058803. Entropy: 0.930228.\n",
      "episode: 3195   score: 180.0  epsilon: 1.0    steps: 912  evaluation reward: 234.25\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6703: Policy loss: -0.001171. Value loss: 0.087394. Entropy: 0.961056.\n",
      "Iteration 6704: Policy loss: -0.007905. Value loss: 0.055231. Entropy: 0.980131.\n",
      "Iteration 6705: Policy loss: -0.011647. Value loss: 0.045294. Entropy: 0.977653.\n",
      "episode: 3196   score: 210.0  epsilon: 1.0    steps: 136  evaluation reward: 233.55\n",
      "episode: 3197   score: 210.0  epsilon: 1.0    steps: 976  evaluation reward: 233.2\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6706: Policy loss: 0.003173. Value loss: 0.154793. Entropy: 0.892851.\n",
      "Iteration 6707: Policy loss: -0.002220. Value loss: 0.101302. Entropy: 0.884928.\n",
      "Iteration 6708: Policy loss: -0.003160. Value loss: 0.082113. Entropy: 0.879790.\n",
      "episode: 3198   score: 210.0  epsilon: 1.0    steps: 528  evaluation reward: 232.9\n",
      "episode: 3199   score: 250.0  epsilon: 1.0    steps: 760  evaluation reward: 230.5\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6709: Policy loss: 0.002431. Value loss: 0.099230. Entropy: 1.033516.\n",
      "Iteration 6710: Policy loss: 0.001592. Value loss: 0.057748. Entropy: 1.022240.\n",
      "Iteration 6711: Policy loss: -0.009224. Value loss: 0.046568. Entropy: 1.033077.\n",
      "Training network. lr: 0.000199. clip: 0.079421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6712: Policy loss: -0.000986. Value loss: 0.076793. Entropy: 0.930854.\n",
      "Iteration 6713: Policy loss: -0.008322. Value loss: 0.046272. Entropy: 0.937302.\n",
      "Iteration 6714: Policy loss: -0.009178. Value loss: 0.039014. Entropy: 0.913733.\n",
      "episode: 3200   score: 240.0  epsilon: 1.0    steps: 144  evaluation reward: 230.3\n",
      "now time :  2019-02-28 12:15:38.629922\n",
      "episode: 3201   score: 215.0  epsilon: 1.0    steps: 568  evaluation reward: 229.5\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6715: Policy loss: 0.001381. Value loss: 0.129346. Entropy: 1.102507.\n",
      "Iteration 6716: Policy loss: -0.003836. Value loss: 0.077815. Entropy: 1.100967.\n",
      "Iteration 6717: Policy loss: -0.007944. Value loss: 0.062357. Entropy: 1.112944.\n",
      "episode: 3202   score: 210.0  epsilon: 1.0    steps: 464  evaluation reward: 229.8\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6718: Policy loss: -0.001438. Value loss: 0.152248. Entropy: 0.873818.\n",
      "Iteration 6719: Policy loss: -0.010012. Value loss: 0.079512. Entropy: 0.897948.\n",
      "Iteration 6720: Policy loss: -0.010788. Value loss: 0.067300. Entropy: 0.875266.\n",
      "episode: 3203   score: 180.0  epsilon: 1.0    steps: 120  evaluation reward: 229.8\n",
      "episode: 3204   score: 215.0  epsilon: 1.0    steps: 600  evaluation reward: 228.75\n",
      "episode: 3205   score: 180.0  epsilon: 1.0    steps: 920  evaluation reward: 224.05\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6721: Policy loss: 0.003499. Value loss: 0.164626. Entropy: 1.053212.\n",
      "Iteration 6722: Policy loss: -0.004180. Value loss: 0.101853. Entropy: 1.041749.\n",
      "Iteration 6723: Policy loss: -0.009468. Value loss: 0.073656. Entropy: 1.047606.\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6724: Policy loss: 0.003078. Value loss: 0.158404. Entropy: 0.937823.\n",
      "Iteration 6725: Policy loss: -0.005721. Value loss: 0.089787. Entropy: 0.928295.\n",
      "Iteration 6726: Policy loss: -0.011795. Value loss: 0.065950. Entropy: 0.925388.\n",
      "episode: 3206   score: 340.0  epsilon: 1.0    steps: 48  evaluation reward: 225.35\n",
      "episode: 3207   score: 260.0  epsilon: 1.0    steps: 320  evaluation reward: 225.6\n",
      "episode: 3208   score: 150.0  epsilon: 1.0    steps: 672  evaluation reward: 223.8\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6727: Policy loss: 0.001351. Value loss: 0.229276. Entropy: 0.945916.\n",
      "Iteration 6728: Policy loss: -0.003237. Value loss: 0.136972. Entropy: 0.960446.\n",
      "Iteration 6729: Policy loss: -0.007410. Value loss: 0.117272. Entropy: 0.959192.\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6730: Policy loss: 0.000985. Value loss: 0.110606. Entropy: 0.913465.\n",
      "Iteration 6731: Policy loss: -0.000082. Value loss: 0.082295. Entropy: 0.897886.\n",
      "Iteration 6732: Policy loss: -0.008068. Value loss: 0.062445. Entropy: 0.888917.\n",
      "episode: 3209   score: 260.0  epsilon: 1.0    steps: 200  evaluation reward: 224.25\n",
      "episode: 3210   score: 210.0  epsilon: 1.0    steps: 704  evaluation reward: 223.7\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6733: Policy loss: 0.003353. Value loss: 0.095650. Entropy: 1.050707.\n",
      "Iteration 6734: Policy loss: -0.001083. Value loss: 0.068331. Entropy: 1.061190.\n",
      "Iteration 6735: Policy loss: -0.005081. Value loss: 0.051539. Entropy: 1.053161.\n",
      "episode: 3211   score: 210.0  epsilon: 1.0    steps: 232  evaluation reward: 223.4\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6736: Policy loss: 0.001739. Value loss: 0.093952. Entropy: 0.916940.\n",
      "Iteration 6737: Policy loss: -0.002264. Value loss: 0.059457. Entropy: 0.926676.\n",
      "Iteration 6738: Policy loss: -0.003958. Value loss: 0.049177. Entropy: 0.917439.\n",
      "episode: 3212   score: 210.0  epsilon: 1.0    steps: 680  evaluation reward: 222.45\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6739: Policy loss: 0.003116. Value loss: 0.154538. Entropy: 1.093817.\n",
      "Iteration 6740: Policy loss: -0.002039. Value loss: 0.082934. Entropy: 1.103920.\n",
      "Iteration 6741: Policy loss: -0.010623. Value loss: 0.059479. Entropy: 1.108367.\n",
      "episode: 3213   score: 215.0  epsilon: 1.0    steps: 256  evaluation reward: 222.35\n",
      "episode: 3214   score: 135.0  epsilon: 1.0    steps: 808  evaluation reward: 220.4\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6742: Policy loss: -0.001181. Value loss: 0.168032. Entropy: 1.049111.\n",
      "Iteration 6743: Policy loss: -0.005354. Value loss: 0.091765. Entropy: 1.068790.\n",
      "Iteration 6744: Policy loss: -0.011681. Value loss: 0.058353. Entropy: 1.059789.\n",
      "episode: 3215   score: 315.0  epsilon: 1.0    steps: 256  evaluation reward: 221.45\n",
      "episode: 3216   score: 265.0  epsilon: 1.0    steps: 496  evaluation reward: 222.0\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6745: Policy loss: 0.001040. Value loss: 0.509658. Entropy: 0.993843.\n",
      "Iteration 6746: Policy loss: -0.001814. Value loss: 0.366110. Entropy: 1.006939.\n",
      "Iteration 6747: Policy loss: 0.000302. Value loss: 0.142662. Entropy: 0.996090.\n",
      "episode: 3217   score: 180.0  epsilon: 1.0    steps: 208  evaluation reward: 221.2\n",
      "episode: 3218   score: 545.0  epsilon: 1.0    steps: 912  evaluation reward: 224.25\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6748: Policy loss: 0.003758. Value loss: 0.203723. Entropy: 0.985655.\n",
      "Iteration 6749: Policy loss: -0.000731. Value loss: 0.124497. Entropy: 0.971083.\n",
      "Iteration 6750: Policy loss: -0.003758. Value loss: 0.105510. Entropy: 0.969077.\n",
      "episode: 3219   score: 225.0  epsilon: 1.0    steps: 232  evaluation reward: 223.3\n",
      "episode: 3220   score: 210.0  epsilon: 1.0    steps: 472  evaluation reward: 223.55\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6751: Policy loss: 0.002384. Value loss: 0.238106. Entropy: 0.928071.\n",
      "Iteration 6752: Policy loss: -0.003090. Value loss: 0.173632. Entropy: 0.935538.\n",
      "Iteration 6753: Policy loss: -0.008537. Value loss: 0.126889. Entropy: 0.923708.\n",
      "episode: 3221   score: 210.0  epsilon: 1.0    steps: 856  evaluation reward: 223.4\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6754: Policy loss: 0.003083. Value loss: 0.164685. Entropy: 0.922680.\n",
      "Iteration 6755: Policy loss: -0.004117. Value loss: 0.108328. Entropy: 0.931614.\n",
      "Iteration 6756: Policy loss: -0.005247. Value loss: 0.079144. Entropy: 0.934371.\n",
      "episode: 3222   score: 250.0  epsilon: 1.0    steps: 360  evaluation reward: 223.45\n",
      "episode: 3223   score: 180.0  epsilon: 1.0    steps: 520  evaluation reward: 223.1\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6757: Policy loss: 0.005788. Value loss: 0.594745. Entropy: 0.986701.\n",
      "Iteration 6758: Policy loss: -0.001204. Value loss: 0.360068. Entropy: 0.997037.\n",
      "Iteration 6759: Policy loss: -0.003863. Value loss: 0.223561. Entropy: 0.988670.\n",
      "episode: 3224   score: 210.0  epsilon: 1.0    steps: 152  evaluation reward: 223.1\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6760: Policy loss: 0.000533. Value loss: 0.212367. Entropy: 0.895931.\n",
      "Iteration 6761: Policy loss: -0.002018. Value loss: 0.111745. Entropy: 0.885530.\n",
      "Iteration 6762: Policy loss: -0.004481. Value loss: 0.085898. Entropy: 0.895488.\n",
      "episode: 3225   score: 180.0  epsilon: 1.0    steps: 384  evaluation reward: 222.75\n",
      "episode: 3226   score: 230.0  epsilon: 1.0    steps: 840  evaluation reward: 222.95\n",
      "episode: 3227   score: 440.0  epsilon: 1.0    steps: 960  evaluation reward: 226.25\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6763: Policy loss: 0.002797. Value loss: 0.408133. Entropy: 1.122166.\n",
      "Iteration 6764: Policy loss: 0.001185. Value loss: 0.301535. Entropy: 1.153989.\n",
      "Iteration 6765: Policy loss: -0.000949. Value loss: 0.225259. Entropy: 1.143609.\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6766: Policy loss: 0.002583. Value loss: 0.204186. Entropy: 0.791502.\n",
      "Iteration 6767: Policy loss: -0.004895. Value loss: 0.112019. Entropy: 0.788061.\n",
      "Iteration 6768: Policy loss: -0.013029. Value loss: 0.076876. Entropy: 0.778832.\n",
      "episode: 3228   score: 210.0  epsilon: 1.0    steps: 296  evaluation reward: 226.55\n",
      "episode: 3229   score: 210.0  epsilon: 1.0    steps: 504  evaluation reward: 226.25\n",
      "episode: 3230   score: 225.0  epsilon: 1.0    steps: 696  evaluation reward: 226.4\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6769: Policy loss: 0.002282. Value loss: 0.315157. Entropy: 1.183706.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6770: Policy loss: 0.003239. Value loss: 0.240761. Entropy: 1.190555.\n",
      "Iteration 6771: Policy loss: -0.001017. Value loss: 0.196664. Entropy: 1.195369.\n",
      "episode: 3231   score: 410.0  epsilon: 1.0    steps: 712  evaluation reward: 229.25\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6772: Policy loss: 0.000874. Value loss: 0.150582. Entropy: 0.812562.\n",
      "Iteration 6773: Policy loss: -0.005375. Value loss: 0.078401. Entropy: 0.804930.\n",
      "Iteration 6774: Policy loss: -0.005683. Value loss: 0.059888. Entropy: 0.805376.\n",
      "episode: 3232   score: 210.0  epsilon: 1.0    steps: 176  evaluation reward: 229.25\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6775: Policy loss: -0.001253. Value loss: 0.162700. Entropy: 1.088054.\n",
      "Iteration 6776: Policy loss: -0.006257. Value loss: 0.099457. Entropy: 1.072156.\n",
      "Iteration 6777: Policy loss: -0.011905. Value loss: 0.102922. Entropy: 1.088286.\n",
      "episode: 3233   score: 180.0  epsilon: 1.0    steps: 248  evaluation reward: 229.7\n",
      "episode: 3234   score: 240.0  epsilon: 1.0    steps: 816  evaluation reward: 230.75\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6778: Policy loss: 0.004987. Value loss: 0.170650. Entropy: 0.970463.\n",
      "Iteration 6779: Policy loss: 0.003275. Value loss: 0.102098. Entropy: 0.966368.\n",
      "Iteration 6780: Policy loss: -0.001366. Value loss: 0.081662. Entropy: 0.975801.\n",
      "episode: 3235   score: 180.0  epsilon: 1.0    steps: 552  evaluation reward: 230.1\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6781: Policy loss: 0.001938. Value loss: 0.133489. Entropy: 1.023716.\n",
      "Iteration 6782: Policy loss: -0.009664. Value loss: 0.077889. Entropy: 1.032926.\n",
      "Iteration 6783: Policy loss: -0.012443. Value loss: 0.053725. Entropy: 1.014273.\n",
      "episode: 3236   score: 210.0  epsilon: 1.0    steps: 328  evaluation reward: 231.0\n",
      "episode: 3237   score: 240.0  epsilon: 1.0    steps: 472  evaluation reward: 231.3\n",
      "episode: 3238   score: 210.0  epsilon: 1.0    steps: 744  evaluation reward: 230.3\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6784: Policy loss: 0.004128. Value loss: 0.278237. Entropy: 1.047293.\n",
      "Iteration 6785: Policy loss: 0.001457. Value loss: 0.161728. Entropy: 1.053649.\n",
      "Iteration 6786: Policy loss: -0.003552. Value loss: 0.108883. Entropy: 1.041281.\n",
      "episode: 3239   score: 120.0  epsilon: 1.0    steps: 240  evaluation reward: 229.4\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6787: Policy loss: -0.001647. Value loss: 0.172642. Entropy: 0.878554.\n",
      "Iteration 6788: Policy loss: -0.001827. Value loss: 0.119679. Entropy: 0.875959.\n",
      "Iteration 6789: Policy loss: -0.009798. Value loss: 0.101912. Entropy: 0.902522.\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6790: Policy loss: -0.000572. Value loss: 0.316786. Entropy: 1.144915.\n",
      "Iteration 6791: Policy loss: -0.004594. Value loss: 0.158202. Entropy: 1.152394.\n",
      "Iteration 6792: Policy loss: -0.010124. Value loss: 0.111220. Entropy: 1.146095.\n",
      "episode: 3240   score: 330.0  epsilon: 1.0    steps: 520  evaluation reward: 230.2\n",
      "episode: 3241   score: 210.0  epsilon: 1.0    steps: 672  evaluation reward: 230.2\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6793: Policy loss: 0.000674. Value loss: 0.245850. Entropy: 0.940566.\n",
      "Iteration 6794: Policy loss: -0.003718. Value loss: 0.128075. Entropy: 0.967753.\n",
      "Iteration 6795: Policy loss: -0.009940. Value loss: 0.092713. Entropy: 0.944253.\n",
      "episode: 3242   score: 210.0  epsilon: 1.0    steps: 144  evaluation reward: 230.2\n",
      "episode: 3243   score: 180.0  epsilon: 1.0    steps: 328  evaluation reward: 230.95\n",
      "episode: 3244   score: 185.0  epsilon: 1.0    steps: 1024  evaluation reward: 230.7\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6796: Policy loss: 0.000105. Value loss: 0.210240. Entropy: 0.967335.\n",
      "Iteration 6797: Policy loss: -0.005968. Value loss: 0.134887. Entropy: 0.972046.\n",
      "Iteration 6798: Policy loss: -0.008671. Value loss: 0.100315. Entropy: 0.960234.\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6799: Policy loss: 0.004497. Value loss: 0.245273. Entropy: 1.060219.\n",
      "Iteration 6800: Policy loss: -0.003494. Value loss: 0.135096. Entropy: 1.080189.\n",
      "Iteration 6801: Policy loss: -0.008188. Value loss: 0.089314. Entropy: 1.062322.\n",
      "episode: 3245   score: 285.0  epsilon: 1.0    steps: 48  evaluation reward: 231.05\n",
      "episode: 3246   score: 210.0  epsilon: 1.0    steps: 936  evaluation reward: 231.35\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6802: Policy loss: 0.000959. Value loss: 0.209814. Entropy: 1.038961.\n",
      "Iteration 6803: Policy loss: -0.001818. Value loss: 0.156471. Entropy: 1.026692.\n",
      "Iteration 6804: Policy loss: -0.010560. Value loss: 0.127078. Entropy: 1.018186.\n",
      "episode: 3247   score: 90.0  epsilon: 1.0    steps: 208  evaluation reward: 229.85\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6805: Policy loss: 0.000631. Value loss: 0.142098. Entropy: 0.901682.\n",
      "Iteration 6806: Policy loss: 0.002248. Value loss: 0.079816. Entropy: 0.901123.\n",
      "Iteration 6807: Policy loss: -0.005028. Value loss: 0.067752. Entropy: 0.882806.\n",
      "episode: 3248   score: 215.0  epsilon: 1.0    steps: 328  evaluation reward: 228.8\n",
      "episode: 3249   score: 545.0  epsilon: 1.0    steps: 568  evaluation reward: 231.15\n",
      "episode: 3250   score: 225.0  epsilon: 1.0    steps: 848  evaluation reward: 231.0\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6808: Policy loss: 0.001537. Value loss: 0.175377. Entropy: 1.050015.\n",
      "Iteration 6809: Policy loss: -0.002256. Value loss: 0.104412. Entropy: 1.041451.\n",
      "Iteration 6810: Policy loss: -0.005395. Value loss: 0.082812. Entropy: 1.051951.\n",
      "now time :  2019-02-28 12:16:47.384121\n",
      "episode: 3251   score: 180.0  epsilon: 1.0    steps: 32  evaluation reward: 231.3\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6811: Policy loss: -0.000519. Value loss: 0.117321. Entropy: 0.815934.\n",
      "Iteration 6812: Policy loss: -0.006831. Value loss: 0.069321. Entropy: 0.822223.\n",
      "Iteration 6813: Policy loss: -0.011264. Value loss: 0.053376. Entropy: 0.822221.\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6814: Policy loss: 0.004945. Value loss: 0.609982. Entropy: 1.143503.\n",
      "Iteration 6815: Policy loss: 0.012245. Value loss: 0.215251. Entropy: 1.159300.\n",
      "Iteration 6816: Policy loss: 0.000541. Value loss: 0.111600. Entropy: 1.141935.\n",
      "episode: 3252   score: 180.0  epsilon: 1.0    steps: 176  evaluation reward: 230.1\n",
      "episode: 3253   score: 535.0  epsilon: 1.0    steps: 256  evaluation reward: 233.25\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6817: Policy loss: -0.000341. Value loss: 0.166233. Entropy: 0.973452.\n",
      "Iteration 6818: Policy loss: -0.004145. Value loss: 0.091526. Entropy: 0.998187.\n",
      "Iteration 6819: Policy loss: -0.011590. Value loss: 0.065235. Entropy: 0.977735.\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6820: Policy loss: -0.003597. Value loss: 0.116768. Entropy: 1.124699.\n",
      "Iteration 6821: Policy loss: -0.010235. Value loss: 0.066321. Entropy: 1.124056.\n",
      "Iteration 6822: Policy loss: -0.016514. Value loss: 0.047067. Entropy: 1.126074.\n",
      "episode: 3254   score: 180.0  epsilon: 1.0    steps: 48  evaluation reward: 229.6\n",
      "episode: 3255   score: 260.0  epsilon: 1.0    steps: 104  evaluation reward: 229.75\n",
      "episode: 3256   score: 275.0  epsilon: 1.0    steps: 992  evaluation reward: 230.4\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6823: Policy loss: -0.001509. Value loss: 0.122853. Entropy: 0.981134.\n",
      "Iteration 6824: Policy loss: -0.003210. Value loss: 0.071340. Entropy: 1.003946.\n",
      "Iteration 6825: Policy loss: -0.008855. Value loss: 0.061800. Entropy: 0.992328.\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6826: Policy loss: 0.000339. Value loss: 0.122248. Entropy: 1.135082.\n",
      "Iteration 6827: Policy loss: -0.007125. Value loss: 0.077935. Entropy: 1.135439.\n",
      "Iteration 6828: Policy loss: -0.010622. Value loss: 0.067057. Entropy: 1.128211.\n",
      "episode: 3257   score: 225.0  epsilon: 1.0    steps: 640  evaluation reward: 230.15\n",
      "episode: 3258   score: 210.0  epsilon: 1.0    steps: 736  evaluation reward: 229.35\n",
      "episode: 3259   score: 360.0  epsilon: 1.0    steps: 784  evaluation reward: 230.85\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6829: Policy loss: -0.000451. Value loss: 0.115749. Entropy: 1.151474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6830: Policy loss: -0.004599. Value loss: 0.076520. Entropy: 1.164457.\n",
      "Iteration 6831: Policy loss: -0.009284. Value loss: 0.065921. Entropy: 1.150126.\n",
      "episode: 3260   score: 235.0  epsilon: 1.0    steps: 952  evaluation reward: 231.1\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6832: Policy loss: -0.001764. Value loss: 0.103252. Entropy: 0.838621.\n",
      "Iteration 6833: Policy loss: -0.005017. Value loss: 0.075917. Entropy: 0.841432.\n",
      "Iteration 6834: Policy loss: -0.009494. Value loss: 0.057222. Entropy: 0.853134.\n",
      "episode: 3261   score: 210.0  epsilon: 1.0    steps: 80  evaluation reward: 232.0\n",
      "episode: 3262   score: 240.0  epsilon: 1.0    steps: 864  evaluation reward: 232.3\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6835: Policy loss: -0.000067. Value loss: 0.147189. Entropy: 1.047356.\n",
      "Iteration 6836: Policy loss: -0.003323. Value loss: 0.092009. Entropy: 1.060730.\n",
      "Iteration 6837: Policy loss: -0.005114. Value loss: 0.098826. Entropy: 1.064236.\n",
      "episode: 3263   score: 215.0  epsilon: 1.0    steps: 760  evaluation reward: 232.05\n",
      "episode: 3264   score: 90.0  epsilon: 1.0    steps: 872  evaluation reward: 231.15\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6838: Policy loss: 0.000186. Value loss: 0.069800. Entropy: 0.942122.\n"
     ]
    }
   ],
   "source": [
    "### Loop through all environments and run PPO on them\n",
    "\n",
    "env_names = ['SpaceInvaders-v0', 'MsPacman-v0', 'Asterix-v0', 'Asteroids-v0', 'Atlantis-v0', 'Alien-v0', 'Amidar-v0', 'Assault-v0', 'BankHeist-v0']\n",
    "\n",
    "for a in range(len(env_names)):\n",
    "    name = env_names[a]\n",
    "    print(\"\\n\\n\\n ------- STARTING TRAINING FOR %s ------- \\n\\n\\n\" % (name))\n",
    "    \n",
    "    envs = []\n",
    "    for i in range(num_envs):\n",
    "        envs.append(GameEnv(name))\n",
    "    #env.render()\n",
    "    \n",
    "\n",
    "    number_lives = envs[0].life\n",
    "    state_size = envs[0].observation_space.shape\n",
    "    if (name == 'SpaceInvaders-v0'):\n",
    "        action_size = 4\n",
    "    else:\n",
    "        action_size = envs[0].action_space.n\n",
    "    rewards, episodes = [], []\n",
    "\n",
    "    vis_env_idx = 0\n",
    "    vis_env = envs[vis_env_idx]\n",
    "    e = 0\n",
    "    frame = 0\n",
    "    max_eval = -np.inf\n",
    "    reset_count = 0\n",
    "\n",
    "\n",
    "    agent = Agent(action_size)\n",
    "    torch.save(agent.policy_net.state_dict(), \"./save_model/\" + name + \"_best\")\n",
    "    evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "    frame = 0\n",
    "    memory_size = 0\n",
    "    reset_max = 10\n",
    "    \n",
    "    print(\"Determing min/max rewards of environment\")\n",
    "    [low, high] = score_range = get_score_range(name)\n",
    "    print(\"Min: %d. Max: %d.\" % (low, high))\n",
    "\n",
    "    while (frame < 10000000):\n",
    "        step = 0\n",
    "        assert(num_envs * env_mem_size == train_frame)\n",
    "        frame_next_vals = []\n",
    "        \n",
    "        for j in range(env_mem_size):\n",
    "            \n",
    "            curr_states = np.stack([envs[i].history[HISTORY_SIZE-1,:,:] for i in range(num_envs)])\n",
    "            next_states = []\n",
    "            net_in = np.stack([envs[i].history[:HISTORY_SIZE,:,:] for i in range(num_envs)])\n",
    "            step += num_envs\n",
    "            frame += num_envs\n",
    "            actions, values = agent.get_action(np.float32(net_in) / 255.)\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                next_states.append(next_state)\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                \"\"\"\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "                \"\"\"\n",
    "                \n",
    "                frame_next_state = get_frame(next_states[i])\n",
    "                env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "                terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "                env.life = env.info['ale.lives']\n",
    "                r = env.reward #np.log(max(env.reward+1, 1))#((env.reward - low) / (high - low)) * 30\n",
    "                agent.memory.push(i, deepcopy(curr_states[i]), actions[i], r, terminal_state, values[i], 0, 0)\n",
    "                \n",
    "                if (j == env_mem_size-1):\n",
    "                    net_in = np.stack([envs[k].history[1:,:,:] for k in range(num_envs)])\n",
    "                    _, frame_next_vals = agent.get_action(np.float32(net_in) / 255.)\n",
    "                \n",
    "                env.score += env.reward\n",
    "                env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "        \n",
    "                if (env.done):\n",
    "                    if (e % 50 == 0):\n",
    "                        print('now time : ', datetime.now())\n",
    "                        rewards.append(np.mean(evaluation_reward))\n",
    "                        episodes.append(e)\n",
    "                        pylab.plot(episodes, rewards, 'b')\n",
    "                        pylab.savefig(\"./save_graph/\" + name + \"_ppo.png\")\n",
    "                        torch.save(agent.policy_net, \"./save_model/\" + name + \"_ppo\")\n",
    "\n",
    "                        if np.mean(evaluation_reward) > max_eval:\n",
    "                            torch.save(agent.policy_net.state_dict(), \"./save_model/\"  + name + \"_ppo_best\")\n",
    "                            max_eval = float(np.mean(evaluation_reward))\n",
    "                            reset_count = 0\n",
    "                        elif e > 5000:\n",
    "                            reset_count += 1\n",
    "                            \"\"\"\n",
    "                            if (reset_count == reset_max):\n",
    "                                print(\"Training went nowhere, starting again at best model\")\n",
    "                                agent.policy_net.load_state_dict(torch.load(\"./save_model/spaceinvaders_ppo_best\"))\n",
    "                                agent.update_target_net()\n",
    "                                reset_count = 0\n",
    "                            \"\"\"\n",
    "                    e += 1\n",
    "                    evaluation_reward.append(env.score)\n",
    "                    print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                      \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "                    env.done = False\n",
    "                    env.score = 0\n",
    "                    env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "                    env.state = env.reset()\n",
    "                    env.life = number_lives\n",
    "                    get_init_state(env.history, env.state)\n",
    "            \n",
    "        agent.train_policy_net(frame, frame_next_vals)\n",
    "        agent.update_target_net()\n",
    "    print(\"FINISHED TRAINING FOR %s\" % (name))\n",
    "    pylab.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
