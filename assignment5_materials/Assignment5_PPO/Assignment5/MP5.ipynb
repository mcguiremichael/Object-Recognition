{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvadersDeterministic-v4')\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lifes(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 6\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],0)[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 210.0   memory length: 1069   epsilon: 1.0    steps: 1069     evaluation reward: 210.0\n",
      "episode: 1   score: 65.0   memory length: 1700   epsilon: 1.0    steps: 631     evaluation reward: 137.5\n",
      "episode: 2   score: 190.0   memory length: 2674   epsilon: 1.0    steps: 974     evaluation reward: 155.0\n",
      "episode: 3   score: 155.0   memory length: 3397   epsilon: 1.0    steps: 723     evaluation reward: 155.0\n",
      "episode: 4   score: 440.0   memory length: 4786   epsilon: 1.0    steps: 1389     evaluation reward: 212.0\n",
      "episode: 5   score: 210.0   memory length: 5555   epsilon: 1.0    steps: 769     evaluation reward: 211.66666666666666\n",
      "episode: 6   score: 230.0   memory length: 6366   epsilon: 1.0    steps: 811     evaluation reward: 214.28571428571428\n",
      "episode: 7   score: 215.0   memory length: 7243   epsilon: 1.0    steps: 877     evaluation reward: 214.375\n",
      "episode: 8   score: 140.0   memory length: 7878   epsilon: 1.0    steps: 635     evaluation reward: 206.11111111111111\n",
      "episode: 9   score: 110.0   memory length: 8493   epsilon: 1.0    steps: 615     evaluation reward: 196.5\n",
      "episode: 10   score: 75.0   memory length: 8992   epsilon: 1.0    steps: 499     evaluation reward: 185.45454545454547\n",
      "episode: 11   score: 225.0   memory length: 9923   epsilon: 1.0    steps: 931     evaluation reward: 188.75\n",
      "Training network\n",
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:167: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:168: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:169: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy loss: 0.002120. Value loss: 9.616186. Entropy: 1.774370.\n",
      "Iteration 2\n",
      "Policy loss: 0.001829. Value loss: 7.188891. Entropy: 1.768062.\n",
      "Iteration 3\n",
      "Policy loss: -0.000299. Value loss: 5.595581. Entropy: 1.767132.\n",
      "episode: 12   score: 65.0   memory length: 10467   epsilon: 1.0    steps: 544     evaluation reward: 179.23076923076923\n",
      "episode: 13   score: 305.0   memory length: 11214   epsilon: 1.0    steps: 747     evaluation reward: 188.21428571428572\n",
      "episode: 14   score: 5.0   memory length: 11670   epsilon: 1.0    steps: 456     evaluation reward: 176.0\n",
      "episode: 15   score: 420.0   memory length: 12985   epsilon: 1.0    steps: 1315     evaluation reward: 191.25\n",
      "episode: 16   score: 210.0   memory length: 13756   epsilon: 1.0    steps: 771     evaluation reward: 192.35294117647058\n",
      "episode: 17   score: 155.0   memory length: 14601   epsilon: 1.0    steps: 845     evaluation reward: 190.27777777777777\n",
      "episode: 18   score: 135.0   memory length: 15372   epsilon: 1.0    steps: 771     evaluation reward: 187.3684210526316\n",
      "episode: 19   score: 190.0   memory length: 16287   epsilon: 1.0    steps: 915     evaluation reward: 187.5\n",
      "episode: 20   score: 55.0   memory length: 16990   epsilon: 1.0    steps: 703     evaluation reward: 181.1904761904762\n",
      "episode: 21   score: 105.0   memory length: 17499   epsilon: 1.0    steps: 509     evaluation reward: 177.72727272727272\n",
      "episode: 22   score: 35.0   memory length: 18191   epsilon: 1.0    steps: 692     evaluation reward: 171.52173913043478\n",
      "episode: 23   score: 185.0   memory length: 18982   epsilon: 1.0    steps: 791     evaluation reward: 172.08333333333334\n",
      "episode: 24   score: 155.0   memory length: 19754   epsilon: 1.0    steps: 772     evaluation reward: 171.4\n",
      "episode: 25   score: 45.0   memory length: 20244   epsilon: 1.0    steps: 490     evaluation reward: 166.53846153846155\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.010957. Value loss: 7.111510. Entropy: 1.756138.\n",
      "Iteration 2\n",
      "Policy loss: 0.008842. Value loss: 5.653242. Entropy: 1.754972.\n",
      "Iteration 3\n",
      "Policy loss: 0.005476. Value loss: 4.773868. Entropy: 1.755234.\n",
      "episode: 26   score: 180.0   memory length: 21020   epsilon: 1.0    steps: 776     evaluation reward: 167.03703703703704\n",
      "episode: 27   score: 210.0   memory length: 21791   epsilon: 1.0    steps: 771     evaluation reward: 168.57142857142858\n",
      "episode: 28   score: 65.0   memory length: 22383   epsilon: 1.0    steps: 592     evaluation reward: 165.0\n",
      "episode: 29   score: 325.0   memory length: 23088   epsilon: 1.0    steps: 705     evaluation reward: 170.33333333333334\n",
      "episode: 30   score: 50.0   memory length: 23491   epsilon: 1.0    steps: 403     evaluation reward: 166.4516129032258\n",
      "episode: 31   score: 315.0   memory length: 24343   epsilon: 1.0    steps: 852     evaluation reward: 171.09375\n",
      "episode: 32   score: 210.0   memory length: 25144   epsilon: 1.0    steps: 801     evaluation reward: 172.27272727272728\n",
      "episode: 33   score: 100.0   memory length: 25715   epsilon: 1.0    steps: 571     evaluation reward: 170.14705882352942\n",
      "episode: 34   score: 35.0   memory length: 26304   epsilon: 1.0    steps: 589     evaluation reward: 166.28571428571428\n",
      "episode: 35   score: 195.0   memory length: 27085   epsilon: 1.0    steps: 781     evaluation reward: 167.08333333333334\n",
      "episode: 36   score: 315.0   memory length: 28199   epsilon: 1.0    steps: 1114     evaluation reward: 171.0810810810811\n",
      "episode: 37   score: 440.0   memory length: 29388   epsilon: 1.0    steps: 1189     evaluation reward: 178.1578947368421\n",
      "episode: 38   score: 310.0   memory length: 30258   epsilon: 1.0    steps: 870     evaluation reward: 181.53846153846155\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.007571. Value loss: 6.124619. Entropy: 1.745809.\n",
      "Iteration 2\n",
      "Policy loss: 0.008332. Value loss: 5.099982. Entropy: 1.744509.\n",
      "Iteration 3\n",
      "Policy loss: 0.007110. Value loss: 4.442729. Entropy: 1.744233.\n",
      "episode: 39   score: 120.0   memory length: 30943   epsilon: 1.0    steps: 685     evaluation reward: 180.0\n",
      "episode: 40   score: 370.0   memory length: 31877   epsilon: 1.0    steps: 934     evaluation reward: 184.6341463414634\n",
      "episode: 41   score: 215.0   memory length: 32729   epsilon: 1.0    steps: 852     evaluation reward: 185.35714285714286\n",
      "episode: 42   score: 150.0   memory length: 33502   epsilon: 1.0    steps: 773     evaluation reward: 184.53488372093022\n",
      "episode: 43   score: 105.0   memory length: 34161   epsilon: 1.0    steps: 659     evaluation reward: 182.72727272727272\n",
      "episode: 44   score: 125.0   memory length: 34721   epsilon: 1.0    steps: 560     evaluation reward: 181.44444444444446\n",
      "episode: 45   score: 135.0   memory length: 35383   epsilon: 1.0    steps: 662     evaluation reward: 180.43478260869566\n",
      "episode: 46   score: 75.0   memory length: 36048   epsilon: 1.0    steps: 665     evaluation reward: 178.19148936170214\n",
      "episode: 47   score: 180.0   memory length: 36687   epsilon: 1.0    steps: 639     evaluation reward: 178.22916666666666\n",
      "episode: 48   score: 100.0   memory length: 37067   epsilon: 1.0    steps: 380     evaluation reward: 176.6326530612245\n",
      "episode: 49   score: 80.0   memory length: 37717   epsilon: 1.0    steps: 650     evaluation reward: 174.7\n",
      "episode: 50   score: 90.0   memory length: 38352   epsilon: 1.0    steps: 635     evaluation reward: 173.0392156862745\n",
      "episode: 51   score: 110.0   memory length: 38989   epsilon: 1.0    steps: 637     evaluation reward: 171.82692307692307\n",
      "episode: 52   score: 90.0   memory length: 39386   epsilon: 1.0    steps: 397     evaluation reward: 170.28301886792454\n",
      "episode: 53   score: 65.0   memory length: 39949   epsilon: 1.0    steps: 563     evaluation reward: 168.33333333333334\n",
      "episode: 54   score: 105.0   memory length: 40684   epsilon: 1.0    steps: 735     evaluation reward: 167.1818181818182\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.010579. Value loss: 4.983078. Entropy: 1.738913.\n",
      "Iteration 2\n",
      "Policy loss: 0.007615. Value loss: 4.316041. Entropy: 1.737696.\n",
      "Iteration 3\n",
      "Policy loss: 0.007616. Value loss: 3.867550. Entropy: 1.735591.\n",
      "episode: 55   score: 45.0   memory length: 41079   epsilon: 1.0    steps: 395     evaluation reward: 165.0\n",
      "episode: 56   score: 180.0   memory length: 41800   epsilon: 1.0    steps: 721     evaluation reward: 165.26315789473685\n",
      "episode: 57   score: 155.0   memory length: 42329   epsilon: 1.0    steps: 529     evaluation reward: 165.08620689655172\n",
      "episode: 58   score: 165.0   memory length: 43092   epsilon: 1.0    steps: 763     evaluation reward: 165.08474576271186\n",
      "episode: 59   score: 55.0   memory length: 43515   epsilon: 1.0    steps: 423     evaluation reward: 163.25\n",
      "episode: 60   score: 110.0   memory length: 44196   epsilon: 1.0    steps: 681     evaluation reward: 162.37704918032787\n",
      "episode: 61   score: 195.0   memory length: 44983   epsilon: 1.0    steps: 787     evaluation reward: 162.90322580645162\n",
      "episode: 62   score: 110.0   memory length: 45730   epsilon: 1.0    steps: 747     evaluation reward: 162.06349206349208\n",
      "episode: 63   score: 45.0   memory length: 46351   epsilon: 1.0    steps: 621     evaluation reward: 160.234375\n",
      "episode: 64   score: 35.0   memory length: 46742   epsilon: 1.0    steps: 391     evaluation reward: 158.30769230769232\n",
      "episode: 65   score: 195.0   memory length: 47636   epsilon: 1.0    steps: 894     evaluation reward: 158.86363636363637\n",
      "episode: 66   score: 75.0   memory length: 48064   epsilon: 1.0    steps: 428     evaluation reward: 157.61194029850745\n",
      "episode: 67   score: 125.0   memory length: 49006   epsilon: 1.0    steps: 942     evaluation reward: 157.13235294117646\n",
      "episode: 68   score: 105.0   memory length: 49627   epsilon: 1.0    steps: 621     evaluation reward: 156.3768115942029\n",
      "now time :  2018-12-18 13:08:34.700441\n",
      "episode: 69   score: 135.0   memory length: 50258   epsilon: 1.0    steps: 631     evaluation reward: 156.07142857142858\n",
      "episode: 70   score: 50.0   memory length: 50873   epsilon: 1.0    steps: 615     evaluation reward: 154.57746478873239\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.009070. Value loss: 4.064096. Entropy: 1.727820.\n",
      "Iteration 2\n",
      "Policy loss: 0.007154. Value loss: 3.685624. Entropy: 1.720962.\n",
      "Iteration 3\n",
      "Policy loss: 0.005225. Value loss: 3.378132. Entropy: 1.718390.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 71   score: 70.0   memory length: 51444   epsilon: 1.0    steps: 571     evaluation reward: 153.40277777777777\n",
      "episode: 72   score: 90.0   memory length: 51983   epsilon: 1.0    steps: 539     evaluation reward: 152.53424657534248\n",
      "episode: 73   score: 100.0   memory length: 52556   epsilon: 1.0    steps: 573     evaluation reward: 151.82432432432432\n",
      "episode: 74   score: 185.0   memory length: 53273   epsilon: 1.0    steps: 717     evaluation reward: 152.26666666666668\n",
      "episode: 75   score: 140.0   memory length: 54062   epsilon: 1.0    steps: 789     evaluation reward: 152.10526315789474\n",
      "episode: 76   score: 390.0   memory length: 55224   epsilon: 1.0    steps: 1162     evaluation reward: 155.19480519480518\n",
      "episode: 77   score: 135.0   memory length: 55850   epsilon: 1.0    steps: 626     evaluation reward: 154.93589743589743\n",
      "episode: 78   score: 140.0   memory length: 56542   epsilon: 1.0    steps: 692     evaluation reward: 154.74683544303798\n",
      "episode: 79   score: 90.0   memory length: 57113   epsilon: 1.0    steps: 571     evaluation reward: 153.9375\n",
      "episode: 80   score: 605.0   memory length: 58530   epsilon: 1.0    steps: 1417     evaluation reward: 159.50617283950618\n",
      "episode: 81   score: 155.0   memory length: 59391   epsilon: 1.0    steps: 861     evaluation reward: 159.4512195121951\n",
      "episode: 82   score: 135.0   memory length: 60132   epsilon: 1.0    steps: 741     evaluation reward: 159.15662650602408\n",
      "episode: 83   score: 50.0   memory length: 60540   epsilon: 1.0    steps: 408     evaluation reward: 157.85714285714286\n",
      "episode: 84   score: 95.0   memory length: 61029   epsilon: 1.0    steps: 489     evaluation reward: 157.11764705882354\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.008860. Value loss: 4.159213. Entropy: 1.696803.\n",
      "Iteration 2\n",
      "Policy loss: 0.010163. Value loss: 3.640246. Entropy: 1.692335.\n",
      "Iteration 3\n",
      "Policy loss: 0.007409. Value loss: 3.380644. Entropy: 1.690451.\n",
      "episode: 85   score: 30.0   memory length: 61557   epsilon: 1.0    steps: 528     evaluation reward: 155.63953488372093\n",
      "episode: 86   score: 105.0   memory length: 62223   epsilon: 1.0    steps: 666     evaluation reward: 155.05747126436782\n",
      "episode: 87   score: 115.0   memory length: 62814   epsilon: 1.0    steps: 591     evaluation reward: 154.60227272727272\n",
      "episode: 88   score: 45.0   memory length: 63225   epsilon: 1.0    steps: 411     evaluation reward: 153.37078651685394\n",
      "episode: 89   score: 105.0   memory length: 63858   epsilon: 1.0    steps: 633     evaluation reward: 152.83333333333334\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        if render_breakout:\n",
    "            env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        action, value = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        #r = np.clip(reward, -1, 1)\n",
    "        r = reward\n",
    "        \n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state, value, 0, 0)\n",
    "        # Start training after random sample generation\n",
    "        if(frame % train_frame == 0):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network\n",
    "            agent.update_target_net()\n",
    "        score += r\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > 40 and len(evaluation_reward) > 350:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn\")\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
