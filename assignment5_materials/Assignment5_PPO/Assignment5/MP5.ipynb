{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.backends.cudnn.benchmarks = True\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "from env import GameEnv\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/anaconda3/envs/py36/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "envs = []\n",
    "for i in range(num_envs):\n",
    "    envs.append(GameEnv('SpaceInvadersDeterministic-v4'))\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = envs[0].life\n",
    "state_size = envs[0].observation_space.shape\n",
    "action_size = envs[0].action_space.n\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent = Agent(action_size, mode='PPO_MHDPA')\n",
    "torch.save(agent.policy_net.state_dict(), \"./save_model/spaceinvaders_ppo_best\")\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n",
    "reset_max = 10\n",
    "\n",
    "\n",
    "### Loop through all environments and run PPO on them\n",
    "\n",
    "#env_names = ['Breakout-v0', 'Phoenix-v0', 'Asteroids-v0', 'SpaceInvaders-v0', 'MsPacman-v0', 'Asterix-v0', 'Atlantis-v0', 'Alien-v0', 'Amidar-v0', 'Assault-v0', 'BankHeist-v0']\n",
    "env_names = ['SpaceInvaders-v4']\n",
    "for a in range(len(env_names)):\n",
    "    name = env_names[a]\n",
    "    print(\"\\n\\n\\n ------- STARTING TRAINING FOR %s ------- \\n\\n\\n\" % (name))\n",
    "    \n",
    "    envs = []\n",
    "    for i in range(num_envs):\n",
    "        envs.append(GameEnv(name))\n",
    "    #env.render()\n",
    "    \n",
    "\n",
    "    number_lives = envs[0].life\n",
    "    state_size = envs[0].observation_space.shape\n",
    "    if (name == 'SpaceInvaders-v0' or name == 'Breakout-v0'):\n",
    "        action_size = 4\n",
    "    else:\n",
    "        action_size = envs[0].action_space.n\n",
    "    rewards, episodes = [], []\n",
    "\n",
    "    vis_env_idx = 0\n",
    "    vis_env = envs[vis_env_idx]\n",
    "    e = 0\n",
    "    frame = 0\n",
    "    max_eval = -np.inf\n",
    "    reset_count = 0\n",
    "\n",
    "\n",
    "    agent = Agent(action_size, mode='PPO_MHDPA')\n",
    "    torch.save(agent.policy_net.state_dict(), \"./save_model/\" + name + \"_best\")\n",
    "    evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "    frame = 0\n",
    "    memory_size = 0\n",
    "    reset_max = 10\n",
    "    \n",
    "    print(\"Determing min/max rewards of environment\")\n",
    "    [low, high] = score_range = get_score_range(name)\n",
    "    print(\"Min: %d. Max: %d.\" % (low, high))\n",
    "\n",
    "    while (frame < 10000000):\n",
    "        step = 0\n",
    "        assert(num_envs * env_mem_size == train_frame)\n",
    "        frame_next_vals = []\n",
    "        \n",
    "        for j in range(env_mem_size):\n",
    "            \n",
    "            curr_states = np.stack([envs[i].history[HISTORY_SIZE-1,:,:] for i in range(num_envs)])\n",
    "            next_states = []\n",
    "            net_in = np.stack([envs[i].history[:HISTORY_SIZE,:,:] for i in range(num_envs)])\n",
    "            step += num_envs\n",
    "            frame += num_envs\n",
    "            actions, values, _ = agent.get_action(np.float32(net_in) / 255.)\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                next_states.append(next_state)\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                \"\"\"\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "                \"\"\"\n",
    "                \n",
    "                frame_next_state = get_frame(next_states[i])\n",
    "                env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "                terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "                env.life = env.info['ale.lives']\n",
    "                r = (env.reward / high) * 20.0 #np.log(max(env.reward+1, 1))#((env.reward - low) / (high - low)) * 30\n",
    "                agent.memory.push(i, deepcopy(curr_states[i]), actions[i], r, terminal_state, values[i], 0, 0)\n",
    "                \n",
    "                if (j == env_mem_size-1):\n",
    "                    net_in = np.stack([envs[k].history[1:,:,:] for k in range(num_envs)])\n",
    "                    _, frame_next_vals, _ = agent.get_action(np.float32(net_in) / 255.)\n",
    "                \n",
    "                env.score += env.reward\n",
    "                env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "        \n",
    "                if (env.done):\n",
    "                    if (e % 50 == 0):\n",
    "                        print('now time : ', datetime.now())\n",
    "                        rewards.append(np.mean(evaluation_reward))\n",
    "                        episodes.append(e)\n",
    "                        pylab.plot(episodes, rewards, 'b')\n",
    "                        pylab.savefig(\"./save_graph/\" + name + \"_ppo.png\")\n",
    "                        torch.save(agent.policy_net, \"./save_model/\" + name + \"_ppo\")\n",
    "\n",
    "                        if np.mean(evaluation_reward) > max_eval:\n",
    "                            torch.save(agent.policy_net.state_dict(), \"./save_model/\"  + name + \"_ppo_best\")\n",
    "                            max_eval = float(np.mean(evaluation_reward))\n",
    "                            reset_count = 0\n",
    "                        elif e > 5000:\n",
    "                            reset_count += 1\n",
    "                            \"\"\"\n",
    "                            if (reset_count == reset_max):\n",
    "                                print(\"Training went nowhere, starting again at best model\")\n",
    "                                agent.policy_net.load_state_dict(torch.load(\"./save_model/spaceinvaders_ppo_best\"))\n",
    "                                agent.update_target_net()\n",
    "                                reset_count = 0\n",
    "                            \"\"\"\n",
    "                    e += 1\n",
    "                    evaluation_reward.append(env.score)\n",
    "                    print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                      \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "                    env.done = False\n",
    "                    env.score = 0\n",
    "                    env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "                    env.state = env.reset()\n",
    "                    env.life = number_lives\n",
    "                    get_init_state(env.history, env.state)\n",
    "            \n",
    "        agent.train_policy_net(frame, frame_next_vals)\n",
    "        agent.update_target_net()\n",
    "    print(\"FINISHED TRAINING FOR %s\" % (name))\n",
    "    pylab.figure()\n",
    "    \n",
    "    for i in range(len(envs)):\n",
    "        envs[i]._env.close()\n",
    "    del envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best(name):\n",
    "    env = GameEnv(name)\n",
    "    print(\"\\n\\n\\n ------- TESTING BEST MODEL FOR %s ------- \\n\\n\\n\" % (name))\n",
    "    number_lives = env.life\n",
    "    \n",
    "    if (name == 'SpaceInvaders-v0'):\n",
    "        action_size = 4\n",
    "    else:\n",
    "        action_size = env.action_space.n\n",
    "    rewards, episodes = [], []\n",
    "    \n",
    "    e = 0\n",
    "    frame = 0\n",
    "\n",
    "    agent = Agent(action_size)\n",
    "    agent.policy_net.load_state_dict(torch.load(\"./save_model/\" + name + \"_ppo_best\"))\n",
    "    agent.update_target_net()\n",
    "    agent.policy_net.eval()\n",
    "    evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "\n",
    "    for i in range(100):\n",
    "        env.done = False\n",
    "        env.score = 0\n",
    "        env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "        env.state = env.reset()\n",
    "        env.life = number_lives\n",
    "        get_init_state(env.history, env.state)\n",
    "        step = 0\n",
    "        while not env.done:\n",
    "            curr_state = env.history[HISTORY_SIZE-1,:,:]\n",
    "            net_in = env.history[:HISTORY_SIZE,:,:]\n",
    "            action, value, _ = agent.get_action(np.float32(net_in) / 255.)\n",
    "            \n",
    "            next_state, env.reward, env.done, env.info = env.step(action)\n",
    "            env._env.render()\n",
    "            \n",
    "            frame_next_state = get_frame(next_state)\n",
    "            \n",
    "            env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "            terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "            env.life = env.info['ale.lives']\n",
    "            \n",
    "            \n",
    "            env.score += env.reward\n",
    "            env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "            step += 1\n",
    "        \n",
    "\n",
    "        evaluation_reward.append(env.score)\n",
    "        print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                      \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_best('MsPacman-v0')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional LSTM agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " ------- STARTING TRAINING FOR SpaceInvaders-v4 ------- \n",
      "\n",
      "\n",
      "\n",
      "Determing min/max rewards of environment\n",
      "Min: 0. Max: 200.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/sigai/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:124: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],1)[0].unsqueeze(1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000250. clip: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:140: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],1)[0].unsqueeze(1))\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:284: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:285: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/sigai/Documents/Projects/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:286: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Policy loss: -0.003072. Value loss: 0.022652. Entropy: 0.299232.\n",
      "Iteration 2: Policy loss: -0.003580. Value loss: 0.004498. Entropy: 0.298123.\n",
      "Iteration 3: Policy loss: -0.001997. Value loss: 0.003115. Entropy: 0.299286.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 4: Policy loss: -0.260096. Value loss: 0.115091. Entropy: 0.296358.\n",
      "Iteration 5: Policy loss: -0.265775. Value loss: 0.115090. Entropy: 0.299206.\n",
      "Iteration 6: Policy loss: -0.264243. Value loss: 0.107152. Entropy: 0.296674.\n",
      "now time :  2019-09-05 14:14:58.883665\n",
      "episode: 1   score: 30.0  epsilon: 1.0    steps: 1016  evaluation reward: 30.0\n",
      "Training network. lr: 0.000250. clip: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sigai/anaconda3/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/sigai/anaconda3/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7: Policy loss: -0.127577. Value loss: 0.097120. Entropy: 0.299122.\n",
      "Iteration 8: Policy loss: -0.129910. Value loss: 0.098277. Entropy: 0.299415.\n",
      "Iteration 9: Policy loss: -0.128063. Value loss: 0.101629. Entropy: 0.298915.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 10: Policy loss: -0.182123. Value loss: 0.134210. Entropy: 0.301229.\n",
      "Iteration 11: Policy loss: -0.193441. Value loss: 0.141604. Entropy: 0.300440.\n",
      "Iteration 12: Policy loss: -0.192024. Value loss: 0.141548. Entropy: 0.300822.\n",
      "episode: 2   score: 105.0  epsilon: 1.0    steps: 936  evaluation reward: 67.5\n",
      "episode: 3   score: 110.0  epsilon: 1.0    steps: 952  evaluation reward: 81.66666666666667\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 13: Policy loss: -0.365810. Value loss: 0.377602. Entropy: 0.299323.\n",
      "Iteration 14: Policy loss: -0.352187. Value loss: 0.360414. Entropy: 0.298534.\n",
      "Iteration 15: Policy loss: -0.374952. Value loss: 0.368787. Entropy: 0.297435.\n",
      "episode: 4   score: 110.0  epsilon: 1.0    steps: 504  evaluation reward: 88.75\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 16: Policy loss: -0.237277. Value loss: 0.181994. Entropy: 0.298853.\n",
      "Iteration 17: Policy loss: -0.232078. Value loss: 0.179477. Entropy: 0.298564.\n",
      "Iteration 18: Policy loss: -0.231116. Value loss: 0.178750. Entropy: 0.298643.\n",
      "episode: 5   score: 80.0  epsilon: 1.0    steps: 408  evaluation reward: 87.0\n",
      "episode: 6   score: 210.0  epsilon: 1.0    steps: 544  evaluation reward: 107.5\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 19: Policy loss: -0.037952. Value loss: 0.122789. Entropy: 0.300722.\n",
      "Iteration 20: Policy loss: -0.041312. Value loss: 0.122668. Entropy: 0.300710.\n",
      "Iteration 21: Policy loss: -0.042337. Value loss: 0.121992. Entropy: 0.300667.\n",
      "episode: 7   score: 90.0  epsilon: 1.0    steps: 480  evaluation reward: 105.0\n",
      "episode: 8   score: 225.0  epsilon: 1.0    steps: 504  evaluation reward: 120.0\n",
      "episode: 9   score: 385.0  epsilon: 1.0    steps: 504  evaluation reward: 149.44444444444446\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 22: Policy loss: -0.086745. Value loss: 0.113195. Entropy: 0.301877.\n",
      "Iteration 23: Policy loss: -0.086906. Value loss: 0.112703. Entropy: 0.301876.\n",
      "Iteration 24: Policy loss: -0.084493. Value loss: 0.112595. Entropy: 0.302054.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 25: Policy loss: -0.194158. Value loss: 0.142501. Entropy: 0.301107.\n",
      "Iteration 26: Policy loss: -0.194411. Value loss: 0.139795. Entropy: 0.301385.\n",
      "Iteration 27: Policy loss: -0.197500. Value loss: 0.140481. Entropy: 0.301165.\n",
      "episode: 10   score: 110.0  epsilon: 1.0    steps: 368  evaluation reward: 145.5\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 28: Policy loss: -0.095236. Value loss: 0.188406. Entropy: 0.300650.\n",
      "Iteration 29: Policy loss: -0.094881. Value loss: 0.188368. Entropy: 0.300550.\n",
      "Iteration 30: Policy loss: -0.095869. Value loss: 0.186479. Entropy: 0.299207.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 31: Policy loss: -0.477467. Value loss: 0.464676. Entropy: 0.302130.\n",
      "Iteration 32: Policy loss: -0.483297. Value loss: 0.463316. Entropy: 0.301309.\n",
      "Iteration 33: Policy loss: -0.476813. Value loss: 0.457241. Entropy: 0.301022.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 34: Policy loss: -0.035653. Value loss: 0.171021. Entropy: 0.305190.\n",
      "Iteration 35: Policy loss: -0.033321. Value loss: 0.169681. Entropy: 0.305670.\n",
      "Iteration 36: Policy loss: -0.038599. Value loss: 0.177560. Entropy: 0.305232.\n",
      "episode: 11   score: 210.0  epsilon: 1.0    steps: 48  evaluation reward: 151.36363636363637\n",
      "episode: 12   score: 110.0  epsilon: 1.0    steps: 488  evaluation reward: 147.91666666666666\n",
      "episode: 13   score: 155.0  epsilon: 1.0    steps: 552  evaluation reward: 148.46153846153845\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 37: Policy loss: -0.121409. Value loss: 0.198220. Entropy: 0.302795.\n",
      "Iteration 38: Policy loss: -0.116735. Value loss: 0.158138. Entropy: 0.303506.\n",
      "Iteration 39: Policy loss: -0.108115. Value loss: 0.146237. Entropy: 0.304538.\n",
      "episode: 14   score: 275.0  epsilon: 1.0    steps: 520  evaluation reward: 157.5\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 40: Policy loss: -0.020071. Value loss: 0.184759. Entropy: 0.305642.\n",
      "Iteration 41: Policy loss: -0.020150. Value loss: 0.167751. Entropy: 0.305652.\n",
      "Iteration 42: Policy loss: -0.021182. Value loss: 0.160683. Entropy: 0.305721.\n",
      "episode: 15   score: 180.0  epsilon: 1.0    steps: 120  evaluation reward: 159.0\n",
      "episode: 16   score: 460.0  epsilon: 1.0    steps: 752  evaluation reward: 177.8125\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 43: Policy loss: -0.109943. Value loss: 0.194268. Entropy: 0.306542.\n",
      "Iteration 44: Policy loss: -0.112065. Value loss: 0.171720. Entropy: 0.306904.\n",
      "Iteration 45: Policy loss: -0.111422. Value loss: 0.168772. Entropy: 0.306927.\n",
      "episode: 17   score: 240.0  epsilon: 1.0    steps: 104  evaluation reward: 181.47058823529412\n",
      "episode: 18   score: 210.0  epsilon: 1.0    steps: 984  evaluation reward: 183.05555555555554\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 46: Policy loss: -0.097040. Value loss: 0.176730. Entropy: 0.306586.\n",
      "Iteration 47: Policy loss: -0.098143. Value loss: 0.163632. Entropy: 0.306269.\n",
      "Iteration 48: Policy loss: -0.101159. Value loss: 0.156598. Entropy: 0.306477.\n",
      "Training network. lr: 0.000250. clip: 0.100000\n",
      "Iteration 49: Policy loss: 0.009686. Value loss: 0.151239. Entropy: 0.306598.\n",
      "Iteration 50: Policy loss: 0.005768. Value loss: 0.131588. Entropy: 0.306171.\n",
      "Iteration 51: Policy loss: 0.011589. Value loss: 0.124153. Entropy: 0.305808.\n",
      "episode: 19   score: 105.0  epsilon: 1.0    steps: 8  evaluation reward: 178.94736842105263\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 52: Policy loss: -0.233768. Value loss: 0.206355. Entropy: 0.309139.\n",
      "Iteration 53: Policy loss: -0.232396. Value loss: 0.173073. Entropy: 0.308340.\n",
      "Iteration 54: Policy loss: -0.229045. Value loss: 0.167584. Entropy: 0.307600.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 55: Policy loss: -0.019733. Value loss: 0.106264. Entropy: 0.301728.\n",
      "Iteration 56: Policy loss: -0.018531. Value loss: 0.076206. Entropy: 0.301616.\n",
      "Iteration 57: Policy loss: -0.019848. Value loss: 0.070413. Entropy: 0.301487.\n",
      "episode: 20   score: 185.0  epsilon: 1.0    steps: 200  evaluation reward: 179.25\n",
      "episode: 21   score: 110.0  epsilon: 1.0    steps: 608  evaluation reward: 175.95238095238096\n",
      "episode: 22   score: 155.0  epsilon: 1.0    steps: 824  evaluation reward: 175.0\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 58: Policy loss: -0.063538. Value loss: 0.151848. Entropy: 0.304633.\n",
      "Iteration 59: Policy loss: -0.066818. Value loss: 0.123130. Entropy: 0.305137.\n",
      "Iteration 60: Policy loss: -0.067062. Value loss: 0.084880. Entropy: 0.305273.\n",
      "episode: 23   score: 240.0  epsilon: 1.0    steps: 96  evaluation reward: 177.82608695652175\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 61: Policy loss: -0.732138. Value loss: 0.784808. Entropy: 0.302060.\n",
      "Iteration 62: Policy loss: -0.706602. Value loss: 0.651676. Entropy: 0.303976.\n",
      "Iteration 63: Policy loss: -0.705078. Value loss: 0.525049. Entropy: 0.303525.\n",
      "episode: 24   score: 260.0  epsilon: 1.0    steps: 728  evaluation reward: 181.25\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 64: Policy loss: -0.090953. Value loss: 0.221746. Entropy: 0.305625.\n",
      "Iteration 65: Policy loss: -0.104358. Value loss: 0.151624. Entropy: 0.304978.\n",
      "Iteration 66: Policy loss: -0.099896. Value loss: 0.121437. Entropy: 0.305302.\n",
      "episode: 25   score: 440.0  epsilon: 1.0    steps: 992  evaluation reward: 191.6\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 67: Policy loss: 0.151903. Value loss: 0.099811. Entropy: 0.303947.\n",
      "Iteration 68: Policy loss: 0.143872. Value loss: 0.051891. Entropy: 0.303731.\n",
      "Iteration 69: Policy loss: 0.149207. Value loss: 0.037658. Entropy: 0.304266.\n",
      "episode: 26   score: 80.0  epsilon: 1.0    steps: 576  evaluation reward: 187.30769230769232\n",
      "episode: 27   score: 155.0  epsilon: 1.0    steps: 688  evaluation reward: 186.11111111111111\n",
      "Training network. lr: 0.000250. clip: 0.099853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70: Policy loss: -0.076122. Value loss: 0.137759. Entropy: 0.301880.\n",
      "Iteration 71: Policy loss: -0.069569. Value loss: 0.101255. Entropy: 0.301401.\n",
      "Iteration 72: Policy loss: -0.075448. Value loss: 0.082537. Entropy: 0.300758.\n",
      "episode: 28   score: 110.0  epsilon: 1.0    steps: 864  evaluation reward: 183.39285714285714\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 73: Policy loss: 0.059979. Value loss: 0.093275. Entropy: 0.303562.\n",
      "Iteration 74: Policy loss: 0.065201. Value loss: 0.058164. Entropy: 0.303229.\n",
      "Iteration 75: Policy loss: 0.062696. Value loss: 0.045372. Entropy: 0.302738.\n",
      "episode: 29   score: 155.0  epsilon: 1.0    steps: 456  evaluation reward: 182.41379310344828\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 76: Policy loss: 0.189335. Value loss: 0.147428. Entropy: 0.304712.\n",
      "Iteration 77: Policy loss: 0.191607. Value loss: 0.091933. Entropy: 0.304422.\n",
      "Iteration 78: Policy loss: 0.195197. Value loss: 0.079831. Entropy: 0.304911.\n",
      "episode: 30   score: 515.0  epsilon: 1.0    steps: 56  evaluation reward: 193.5\n",
      "episode: 31   score: 155.0  epsilon: 1.0    steps: 488  evaluation reward: 192.25806451612902\n",
      "episode: 32   score: 75.0  epsilon: 1.0    steps: 888  evaluation reward: 188.59375\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 79: Policy loss: 0.057923. Value loss: 0.093981. Entropy: 0.302736.\n",
      "Iteration 80: Policy loss: 0.059175. Value loss: 0.067413. Entropy: 0.302495.\n",
      "Iteration 81: Policy loss: 0.057394. Value loss: 0.049440. Entropy: 0.302920.\n",
      "episode: 33   score: 155.0  epsilon: 1.0    steps: 1008  evaluation reward: 187.57575757575756\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 82: Policy loss: -0.010506. Value loss: 0.119382. Entropy: 0.302984.\n",
      "Iteration 83: Policy loss: -0.007899. Value loss: 0.077942. Entropy: 0.302912.\n",
      "Iteration 84: Policy loss: -0.015908. Value loss: 0.068702. Entropy: 0.303526.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 85: Policy loss: -0.021186. Value loss: 0.108200. Entropy: 0.304949.\n",
      "Iteration 86: Policy loss: -0.030687. Value loss: 0.078535. Entropy: 0.305297.\n",
      "Iteration 87: Policy loss: -0.027952. Value loss: 0.058748. Entropy: 0.304547.\n",
      "episode: 34   score: 380.0  epsilon: 1.0    steps: 976  evaluation reward: 193.23529411764707\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 88: Policy loss: -0.229659. Value loss: 0.139057. Entropy: 0.304025.\n",
      "Iteration 89: Policy loss: -0.231837. Value loss: 0.095315. Entropy: 0.304833.\n",
      "Iteration 90: Policy loss: -0.237998. Value loss: 0.076137. Entropy: 0.304603.\n",
      "episode: 35   score: 255.0  epsilon: 1.0    steps: 312  evaluation reward: 195.0\n",
      "episode: 36   score: 110.0  epsilon: 1.0    steps: 600  evaluation reward: 192.63888888888889\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 91: Policy loss: 0.046689. Value loss: 0.116659. Entropy: 0.306353.\n",
      "Iteration 92: Policy loss: 0.038984. Value loss: 0.081810. Entropy: 0.306075.\n",
      "Iteration 93: Policy loss: 0.038325. Value loss: 0.072946. Entropy: 0.306137.\n",
      "episode: 37   score: 210.0  epsilon: 1.0    steps: 592  evaluation reward: 193.1081081081081\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 94: Policy loss: -0.007885. Value loss: 0.212500. Entropy: 0.303952.\n",
      "Iteration 95: Policy loss: -0.009368. Value loss: 0.147287. Entropy: 0.303669.\n",
      "Iteration 96: Policy loss: -0.005717. Value loss: 0.132349. Entropy: 0.303583.\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 97: Policy loss: -0.022203. Value loss: 0.081740. Entropy: 0.300671.\n",
      "Iteration 98: Policy loss: -0.020532. Value loss: 0.046083. Entropy: 0.300357.\n",
      "Iteration 99: Policy loss: -0.021184. Value loss: 0.035830. Entropy: 0.300126.\n",
      "episode: 38   score: 225.0  epsilon: 1.0    steps: 448  evaluation reward: 193.94736842105263\n",
      "episode: 39   score: 210.0  epsilon: 1.0    steps: 456  evaluation reward: 194.35897435897436\n",
      "episode: 40   score: 105.0  epsilon: 1.0    steps: 736  evaluation reward: 192.125\n",
      "Training network. lr: 0.000250. clip: 0.099853\n",
      "Iteration 100: Policy loss: 0.203230. Value loss: 0.191839. Entropy: 0.301912.\n",
      "Iteration 101: Policy loss: 0.202585. Value loss: 0.120786. Entropy: 0.302104.\n",
      "Iteration 102: Policy loss: 0.205345. Value loss: 0.088199. Entropy: 0.302258.\n",
      "episode: 41   score: 180.0  epsilon: 1.0    steps: 680  evaluation reward: 191.82926829268294\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 103: Policy loss: 0.052422. Value loss: 0.059546. Entropy: 0.301890.\n",
      "Iteration 104: Policy loss: 0.048195. Value loss: 0.044389. Entropy: 0.302247.\n",
      "Iteration 105: Policy loss: 0.046526. Value loss: 0.037731. Entropy: 0.302134.\n",
      "episode: 42   score: 125.0  epsilon: 1.0    steps: 432  evaluation reward: 190.23809523809524\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 106: Policy loss: 0.156351. Value loss: 0.173209. Entropy: 0.302610.\n",
      "Iteration 107: Policy loss: 0.151125. Value loss: 0.103882. Entropy: 0.303076.\n",
      "Iteration 108: Policy loss: 0.139060. Value loss: 0.085924. Entropy: 0.302916.\n",
      "episode: 43   score: 210.0  epsilon: 1.0    steps: 232  evaluation reward: 190.69767441860466\n",
      "episode: 44   score: 50.0  epsilon: 1.0    steps: 712  evaluation reward: 187.5\n",
      "episode: 45   score: 155.0  epsilon: 1.0    steps: 768  evaluation reward: 186.77777777777777\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 109: Policy loss: 0.201382. Value loss: 0.106712. Entropy: 0.303073.\n",
      "Iteration 110: Policy loss: 0.208246. Value loss: 0.078064. Entropy: 0.303083.\n",
      "Iteration 111: Policy loss: 0.201283. Value loss: 0.062748. Entropy: 0.303060.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 112: Policy loss: 0.192125. Value loss: 0.089231. Entropy: 0.307838.\n",
      "Iteration 113: Policy loss: 0.192813. Value loss: 0.056305. Entropy: 0.307554.\n",
      "Iteration 114: Policy loss: 0.193462. Value loss: 0.045740. Entropy: 0.307461.\n",
      "episode: 46   score: 105.0  epsilon: 1.0    steps: 488  evaluation reward: 185.0\n",
      "episode: 47   score: 215.0  epsilon: 1.0    steps: 608  evaluation reward: 185.63829787234042\n",
      "episode: 48   score: 50.0  epsilon: 1.0    steps: 912  evaluation reward: 182.8125\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 115: Policy loss: 0.051228. Value loss: 0.135828. Entropy: 0.305888.\n",
      "Iteration 116: Policy loss: 0.049440. Value loss: 0.075208. Entropy: 0.305488.\n",
      "Iteration 117: Policy loss: 0.041585. Value loss: 0.065006. Entropy: 0.305560.\n",
      "episode: 49   score: 210.0  epsilon: 1.0    steps: 1000  evaluation reward: 183.3673469387755\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 118: Policy loss: -0.127007. Value loss: 0.083920. Entropy: 0.303295.\n",
      "Iteration 119: Policy loss: -0.130073. Value loss: 0.053074. Entropy: 0.303285.\n",
      "Iteration 120: Policy loss: -0.126959. Value loss: 0.046525. Entropy: 0.303204.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 121: Policy loss: -0.382913. Value loss: 0.370381. Entropy: 0.302959.\n",
      "Iteration 122: Policy loss: -0.366493. Value loss: 0.244121. Entropy: 0.304106.\n",
      "Iteration 123: Policy loss: -0.389974. Value loss: 0.221305. Entropy: 0.302698.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 124: Policy loss: -0.114931. Value loss: 0.113511. Entropy: 0.303666.\n",
      "Iteration 125: Policy loss: -0.126690. Value loss: 0.057235. Entropy: 0.304146.\n",
      "Iteration 126: Policy loss: -0.125001. Value loss: 0.035489. Entropy: 0.303111.\n",
      "episode: 50   score: 450.0  epsilon: 1.0    steps: 72  evaluation reward: 188.7\n",
      "now time :  2019-09-05 14:22:25.272288\n",
      "episode: 51   score: 125.0  epsilon: 1.0    steps: 456  evaluation reward: 187.45098039215685\n",
      "episode: 52   score: 135.0  epsilon: 1.0    steps: 512  evaluation reward: 186.44230769230768\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 127: Policy loss: 0.377287. Value loss: 0.143182. Entropy: 0.303667.\n",
      "Iteration 128: Policy loss: 0.364490. Value loss: 0.076814. Entropy: 0.303863.\n",
      "Iteration 129: Policy loss: 0.368463. Value loss: 0.055720. Entropy: 0.303823.\n",
      "episode: 53   score: 200.0  epsilon: 1.0    steps: 328  evaluation reward: 186.69811320754718\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 130: Policy loss: -0.115797. Value loss: 0.073387. Entropy: 0.302109.\n",
      "Iteration 131: Policy loss: -0.117886. Value loss: 0.047327. Entropy: 0.301358.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 132: Policy loss: -0.118019. Value loss: 0.041037. Entropy: 0.301752.\n",
      "episode: 54   score: 155.0  epsilon: 1.0    steps: 664  evaluation reward: 186.11111111111111\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 133: Policy loss: -0.000096. Value loss: 0.085713. Entropy: 0.303117.\n",
      "Iteration 134: Policy loss: 0.003984. Value loss: 0.044598. Entropy: 0.303275.\n",
      "Iteration 135: Policy loss: 0.001568. Value loss: 0.031010. Entropy: 0.303298.\n",
      "episode: 55   score: 180.0  epsilon: 1.0    steps: 200  evaluation reward: 186.0\n",
      "episode: 56   score: 105.0  epsilon: 1.0    steps: 224  evaluation reward: 184.55357142857142\n",
      "episode: 57   score: 235.0  epsilon: 1.0    steps: 800  evaluation reward: 185.43859649122808\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 136: Policy loss: 0.086186. Value loss: 0.110051. Entropy: 0.301385.\n",
      "Iteration 137: Policy loss: 0.075962. Value loss: 0.048434. Entropy: 0.301680.\n",
      "Iteration 138: Policy loss: 0.085940. Value loss: 0.034758. Entropy: 0.301731.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 139: Policy loss: -0.061970. Value loss: 0.116270. Entropy: 0.302568.\n",
      "Iteration 140: Policy loss: -0.065139. Value loss: 0.080259. Entropy: 0.303445.\n",
      "Iteration 141: Policy loss: -0.069719. Value loss: 0.067549. Entropy: 0.302763.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 142: Policy loss: -0.437368. Value loss: 0.335577. Entropy: 0.301012.\n",
      "Iteration 143: Policy loss: -0.416943. Value loss: 0.287341. Entropy: 0.299027.\n",
      "Iteration 144: Policy loss: -0.440290. Value loss: 0.233243. Entropy: 0.300698.\n",
      "episode: 58   score: 135.0  epsilon: 1.0    steps: 152  evaluation reward: 184.56896551724137\n",
      "episode: 59   score: 135.0  epsilon: 1.0    steps: 680  evaluation reward: 183.72881355932202\n",
      "episode: 60   score: 210.0  epsilon: 1.0    steps: 816  evaluation reward: 184.16666666666666\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 145: Policy loss: -0.042455. Value loss: 0.167285. Entropy: 0.300052.\n",
      "Iteration 146: Policy loss: -0.041587. Value loss: 0.113055. Entropy: 0.299891.\n",
      "Iteration 147: Policy loss: -0.040387. Value loss: 0.086677. Entropy: 0.299778.\n",
      "Training network. lr: 0.000249. clip: 0.099696\n",
      "Iteration 148: Policy loss: -0.182958. Value loss: 0.147021. Entropy: 0.300423.\n",
      "Iteration 149: Policy loss: -0.178189. Value loss: 0.076920. Entropy: 0.299648.\n",
      "Iteration 150: Policy loss: -0.189947. Value loss: 0.069300. Entropy: 0.300213.\n",
      "episode: 61   score: 315.0  epsilon: 1.0    steps: 352  evaluation reward: 186.31147540983608\n",
      "episode: 62   score: 135.0  epsilon: 1.0    steps: 544  evaluation reward: 185.48387096774192\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 151: Policy loss: 0.274097. Value loss: 0.190557. Entropy: 0.297993.\n",
      "Iteration 152: Policy loss: 0.272845. Value loss: 0.093854. Entropy: 0.297276.\n",
      "Iteration 153: Policy loss: 0.277078. Value loss: 0.070717. Entropy: 0.298097.\n",
      "episode: 63   score: 155.0  epsilon: 1.0    steps: 392  evaluation reward: 185.0\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 154: Policy loss: 0.056710. Value loss: 0.147586. Entropy: 0.300323.\n",
      "Iteration 155: Policy loss: 0.055177. Value loss: 0.081489. Entropy: 0.300182.\n",
      "Iteration 156: Policy loss: 0.060537. Value loss: 0.061035. Entropy: 0.300223.\n",
      "episode: 64   score: 40.0  epsilon: 1.0    steps: 776  evaluation reward: 182.734375\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 157: Policy loss: 0.151489. Value loss: 0.107060. Entropy: 0.299325.\n",
      "Iteration 158: Policy loss: 0.152687. Value loss: 0.060040. Entropy: 0.299110.\n",
      "Iteration 159: Policy loss: 0.146927. Value loss: 0.046972. Entropy: 0.299431.\n",
      "episode: 65   score: 255.0  epsilon: 1.0    steps: 288  evaluation reward: 183.84615384615384\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 160: Policy loss: 0.138652. Value loss: 0.056968. Entropy: 0.301642.\n",
      "Iteration 161: Policy loss: 0.135046. Value loss: 0.029720. Entropy: 0.301542.\n",
      "Iteration 162: Policy loss: 0.136433. Value loss: 0.026359. Entropy: 0.301404.\n",
      "episode: 66   score: 530.0  epsilon: 1.0    steps: 888  evaluation reward: 189.0909090909091\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 163: Policy loss: -0.074459. Value loss: 0.136720. Entropy: 0.301237.\n",
      "Iteration 164: Policy loss: -0.075033. Value loss: 0.075817. Entropy: 0.301568.\n",
      "Iteration 165: Policy loss: -0.075437. Value loss: 0.059157. Entropy: 0.300763.\n",
      "episode: 67   score: 210.0  epsilon: 1.0    steps: 48  evaluation reward: 189.40298507462686\n",
      "episode: 68   score: 240.0  epsilon: 1.0    steps: 736  evaluation reward: 190.14705882352942\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 166: Policy loss: 0.107883. Value loss: 0.080750. Entropy: 0.300407.\n",
      "Iteration 167: Policy loss: 0.105200. Value loss: 0.043762. Entropy: 0.298836.\n",
      "Iteration 168: Policy loss: 0.098491. Value loss: 0.032583. Entropy: 0.299907.\n",
      "episode: 69   score: 155.0  epsilon: 1.0    steps: 560  evaluation reward: 189.63768115942028\n",
      "episode: 70   score: 180.0  epsilon: 1.0    steps: 640  evaluation reward: 189.5\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 169: Policy loss: 0.323641. Value loss: 0.144211. Entropy: 0.301785.\n",
      "Iteration 170: Policy loss: 0.321280. Value loss: 0.075983. Entropy: 0.300357.\n",
      "Iteration 171: Policy loss: 0.320215. Value loss: 0.056402. Entropy: 0.301925.\n",
      "episode: 71   score: 210.0  epsilon: 1.0    steps: 568  evaluation reward: 189.7887323943662\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 172: Policy loss: 0.121887. Value loss: 0.092537. Entropy: 0.297819.\n",
      "Iteration 173: Policy loss: 0.117514. Value loss: 0.063091. Entropy: 0.298467.\n",
      "Iteration 174: Policy loss: 0.115886. Value loss: 0.047927. Entropy: 0.297493.\n",
      "episode: 72   score: 180.0  epsilon: 1.0    steps: 944  evaluation reward: 189.65277777777777\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 175: Policy loss: 0.093011. Value loss: 0.086850. Entropy: 0.303425.\n",
      "Iteration 176: Policy loss: 0.098100. Value loss: 0.047995. Entropy: 0.304510.\n",
      "Iteration 177: Policy loss: 0.094590. Value loss: 0.039444. Entropy: 0.302596.\n",
      "episode: 73   score: 155.0  epsilon: 1.0    steps: 448  evaluation reward: 189.17808219178082\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 178: Policy loss: 0.085400. Value loss: 0.048101. Entropy: 0.304478.\n",
      "Iteration 179: Policy loss: 0.083826. Value loss: 0.033220. Entropy: 0.303398.\n",
      "Iteration 180: Policy loss: 0.086281. Value loss: 0.025055. Entropy: 0.304318.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 181: Policy loss: 0.015908. Value loss: 0.094222. Entropy: 0.305906.\n",
      "Iteration 182: Policy loss: 0.010111. Value loss: 0.064589. Entropy: 0.305023.\n",
      "Iteration 183: Policy loss: 0.010532. Value loss: 0.052739. Entropy: 0.305650.\n",
      "episode: 74   score: 155.0  epsilon: 1.0    steps: 184  evaluation reward: 188.71621621621622\n",
      "episode: 75   score: 180.0  epsilon: 1.0    steps: 272  evaluation reward: 188.6\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 184: Policy loss: -0.066127. Value loss: 0.086991. Entropy: 0.303733.\n",
      "Iteration 185: Policy loss: -0.062926. Value loss: 0.057297. Entropy: 0.303119.\n",
      "Iteration 186: Policy loss: -0.061200. Value loss: 0.044022. Entropy: 0.303278.\n",
      "episode: 76   score: 185.0  epsilon: 1.0    steps: 272  evaluation reward: 188.55263157894737\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 187: Policy loss: 0.091225. Value loss: 0.224197. Entropy: 0.304839.\n",
      "Iteration 188: Policy loss: 0.089917. Value loss: 0.129468. Entropy: 0.305735.\n",
      "Iteration 189: Policy loss: 0.090247. Value loss: 0.106194. Entropy: 0.306158.\n",
      "episode: 77   score: 210.0  epsilon: 1.0    steps: 144  evaluation reward: 188.83116883116884\n",
      "episode: 78   score: 210.0  epsilon: 1.0    steps: 240  evaluation reward: 189.10256410256412\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 190: Policy loss: 0.089007. Value loss: 0.125722. Entropy: 0.301079.\n",
      "Iteration 191: Policy loss: 0.092857. Value loss: 0.064912. Entropy: 0.301686.\n",
      "Iteration 192: Policy loss: 0.095818. Value loss: 0.047716. Entropy: 0.301255.\n",
      "episode: 79   score: 105.0  epsilon: 1.0    steps: 288  evaluation reward: 188.0379746835443\n",
      "Training network. lr: 0.000249. clip: 0.099548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 193: Policy loss: 0.273945. Value loss: 0.177352. Entropy: 0.306840.\n",
      "Iteration 194: Policy loss: 0.269715. Value loss: 0.107727. Entropy: 0.306228.\n",
      "Iteration 195: Policy loss: 0.265975. Value loss: 0.071253. Entropy: 0.305879.\n",
      "episode: 80   score: 155.0  epsilon: 1.0    steps: 264  evaluation reward: 187.625\n",
      "episode: 81   score: 260.0  epsilon: 1.0    steps: 336  evaluation reward: 188.5185185185185\n",
      "episode: 82   score: 210.0  epsilon: 1.0    steps: 736  evaluation reward: 188.78048780487805\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 196: Policy loss: 0.292155. Value loss: 0.142949. Entropy: 0.302272.\n",
      "Iteration 197: Policy loss: 0.299017. Value loss: 0.065318. Entropy: 0.302921.\n",
      "Iteration 198: Policy loss: 0.296714. Value loss: 0.048246. Entropy: 0.303208.\n",
      "Training network. lr: 0.000249. clip: 0.099548\n",
      "Iteration 199: Policy loss: -0.285775. Value loss: 0.365460. Entropy: 0.305739.\n",
      "Iteration 200: Policy loss: -0.304001. Value loss: 0.288381. Entropy: 0.306277.\n",
      "Iteration 201: Policy loss: -0.311668. Value loss: 0.242628. Entropy: 0.307587.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 202: Policy loss: 0.116264. Value loss: 0.136308. Entropy: 0.309483.\n",
      "Iteration 203: Policy loss: 0.119000. Value loss: 0.068840. Entropy: 0.309269.\n",
      "Iteration 204: Policy loss: 0.113411. Value loss: 0.050824. Entropy: 0.308563.\n",
      "episode: 83   score: 210.0  epsilon: 1.0    steps: 48  evaluation reward: 189.03614457831324\n",
      "episode: 84   score: 380.0  epsilon: 1.0    steps: 856  evaluation reward: 191.3095238095238\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 205: Policy loss: 0.111102. Value loss: 0.078916. Entropy: 0.306558.\n",
      "Iteration 206: Policy loss: 0.113988. Value loss: 0.033095. Entropy: 0.304861.\n",
      "Iteration 207: Policy loss: 0.109256. Value loss: 0.029653. Entropy: 0.304222.\n",
      "episode: 85   score: 140.0  epsilon: 1.0    steps: 136  evaluation reward: 190.7058823529412\n",
      "episode: 86   score: 180.0  epsilon: 1.0    steps: 848  evaluation reward: 190.58139534883722\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 208: Policy loss: 0.036746. Value loss: 0.088915. Entropy: 0.308690.\n",
      "Iteration 209: Policy loss: 0.033860. Value loss: 0.046603. Entropy: 0.307266.\n",
      "Iteration 210: Policy loss: 0.035959. Value loss: 0.033312. Entropy: 0.308010.\n",
      "episode: 87   score: 160.0  epsilon: 1.0    steps: 616  evaluation reward: 190.22988505747125\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 211: Policy loss: 0.115016. Value loss: 0.120562. Entropy: 0.306926.\n",
      "Iteration 212: Policy loss: 0.115041. Value loss: 0.070172. Entropy: 0.305828.\n",
      "Iteration 213: Policy loss: 0.120462. Value loss: 0.055746. Entropy: 0.305835.\n",
      "episode: 88   score: 180.0  epsilon: 1.0    steps: 904  evaluation reward: 190.11363636363637\n",
      "episode: 89   score: 210.0  epsilon: 1.0    steps: 952  evaluation reward: 190.3370786516854\n",
      "episode: 90   score: 165.0  epsilon: 1.0    steps: 1000  evaluation reward: 190.05555555555554\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 214: Policy loss: 0.195289. Value loss: 0.135114. Entropy: 0.303482.\n",
      "Iteration 215: Policy loss: 0.198313. Value loss: 0.067193. Entropy: 0.303447.\n",
      "Iteration 216: Policy loss: 0.187691. Value loss: 0.057634. Entropy: 0.303613.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 217: Policy loss: -0.221470. Value loss: 0.305283. Entropy: 0.303132.\n",
      "Iteration 218: Policy loss: -0.204711. Value loss: 0.219681. Entropy: 0.301837.\n",
      "Iteration 219: Policy loss: -0.222949. Value loss: 0.172434. Entropy: 0.302438.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 220: Policy loss: 0.335467. Value loss: 0.193048. Entropy: 0.308055.\n",
      "Iteration 221: Policy loss: 0.335113. Value loss: 0.124409. Entropy: 0.309364.\n",
      "Iteration 222: Policy loss: 0.328390. Value loss: 0.096260. Entropy: 0.309711.\n",
      "episode: 91   score: 355.0  epsilon: 1.0    steps: 328  evaluation reward: 191.86813186813185\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 223: Policy loss: 0.210422. Value loss: 0.164499. Entropy: 0.304046.\n",
      "Iteration 224: Policy loss: 0.209692. Value loss: 0.113694. Entropy: 0.303964.\n",
      "Iteration 225: Policy loss: 0.208947. Value loss: 0.070126. Entropy: 0.304207.\n",
      "episode: 92   score: 165.0  epsilon: 1.0    steps: 184  evaluation reward: 191.57608695652175\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 226: Policy loss: 0.038298. Value loss: 0.080282. Entropy: 0.303491.\n",
      "Iteration 227: Policy loss: 0.035536. Value loss: 0.045937. Entropy: 0.302976.\n",
      "Iteration 228: Policy loss: 0.037073. Value loss: 0.028301. Entropy: 0.304374.\n",
      "episode: 93   score: 180.0  epsilon: 1.0    steps: 8  evaluation reward: 191.4516129032258\n",
      "episode: 94   score: 180.0  epsilon: 1.0    steps: 152  evaluation reward: 191.32978723404256\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 229: Policy loss: -0.659872. Value loss: 0.667417. Entropy: 0.302693.\n",
      "Iteration 230: Policy loss: -0.654885. Value loss: 0.491672. Entropy: 0.302709.\n",
      "Iteration 231: Policy loss: -0.626211. Value loss: 0.371521. Entropy: 0.301264.\n",
      "episode: 95   score: 210.0  epsilon: 1.0    steps: 144  evaluation reward: 191.52631578947367\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 232: Policy loss: 0.197810. Value loss: 0.145873. Entropy: 0.301924.\n",
      "Iteration 233: Policy loss: 0.201833. Value loss: 0.095686. Entropy: 0.301894.\n",
      "Iteration 234: Policy loss: 0.205764. Value loss: 0.079939. Entropy: 0.302016.\n",
      "episode: 96   score: 380.0  epsilon: 1.0    steps: 400  evaluation reward: 193.48958333333334\n",
      "episode: 97   score: 410.0  epsilon: 1.0    steps: 648  evaluation reward: 195.72164948453607\n",
      "episode: 98   score: 225.0  epsilon: 1.0    steps: 688  evaluation reward: 196.0204081632653\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 235: Policy loss: -0.086875. Value loss: 0.099511. Entropy: 0.303286.\n",
      "Iteration 236: Policy loss: -0.087522. Value loss: 0.063778. Entropy: 0.302166.\n",
      "Iteration 237: Policy loss: -0.085387. Value loss: 0.049572. Entropy: 0.301850.\n",
      "episode: 99   score: 105.0  epsilon: 1.0    steps: 104  evaluation reward: 195.1010101010101\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 238: Policy loss: -0.075952. Value loss: 0.032421. Entropy: 0.301521.\n",
      "Iteration 239: Policy loss: -0.078183. Value loss: 0.023279. Entropy: 0.301294.\n",
      "Iteration 240: Policy loss: -0.075134. Value loss: 0.018527. Entropy: 0.300445.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 241: Policy loss: -0.083857. Value loss: 0.153655. Entropy: 0.302362.\n",
      "Iteration 242: Policy loss: -0.089431. Value loss: 0.118589. Entropy: 0.303192.\n",
      "Iteration 243: Policy loss: -0.089454. Value loss: 0.092294. Entropy: 0.302793.\n",
      "episode: 100   score: 75.0  epsilon: 1.0    steps: 48  evaluation reward: 193.9\n",
      "now time :  2019-09-05 14:29:40.634175\n",
      "episode: 101   score: 110.0  epsilon: 1.0    steps: 96  evaluation reward: 194.7\n",
      "episode: 102   score: 180.0  epsilon: 1.0    steps: 504  evaluation reward: 195.45\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 244: Policy loss: -0.069778. Value loss: 0.060902. Entropy: 0.300857.\n",
      "Iteration 245: Policy loss: -0.068402. Value loss: 0.033949. Entropy: 0.301252.\n",
      "Iteration 246: Policy loss: -0.077043. Value loss: 0.026833. Entropy: 0.301232.\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 247: Policy loss: 0.204859. Value loss: 0.094750. Entropy: 0.303117.\n",
      "Iteration 248: Policy loss: 0.196643. Value loss: 0.056499. Entropy: 0.302105.\n",
      "Iteration 249: Policy loss: 0.198276. Value loss: 0.044682. Entropy: 0.301340.\n",
      "episode: 103   score: 180.0  epsilon: 1.0    steps: 736  evaluation reward: 196.15\n",
      "Training network. lr: 0.000248. clip: 0.099392\n",
      "Iteration 250: Policy loss: -0.010131. Value loss: 0.120015. Entropy: 0.304397.\n",
      "Iteration 251: Policy loss: -0.009891. Value loss: 0.069373. Entropy: 0.303208.\n",
      "Iteration 252: Policy loss: -0.008046. Value loss: 0.053622. Entropy: 0.303996.\n",
      "episode: 104   score: 110.0  epsilon: 1.0    steps: 672  evaluation reward: 196.15\n",
      "episode: 105   score: 210.0  epsilon: 1.0    steps: 1024  evaluation reward: 197.45\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 253: Policy loss: -0.176444. Value loss: 0.388704. Entropy: 0.304605.\n",
      "Iteration 254: Policy loss: -0.176645. Value loss: 0.268739. Entropy: 0.304862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 255: Policy loss: -0.151396. Value loss: 0.203915. Entropy: 0.303627.\n",
      "episode: 106   score: 180.0  epsilon: 1.0    steps: 392  evaluation reward: 197.15\n",
      "episode: 107   score: 110.0  epsilon: 1.0    steps: 928  evaluation reward: 197.35\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 256: Policy loss: -0.080352. Value loss: 0.107484. Entropy: 0.301900.\n",
      "Iteration 257: Policy loss: -0.079008. Value loss: 0.050604. Entropy: 0.302630.\n",
      "Iteration 258: Policy loss: -0.093881. Value loss: 0.037557. Entropy: 0.302018.\n",
      "episode: 108   score: 110.0  epsilon: 1.0    steps: 432  evaluation reward: 196.2\n",
      "episode: 109   score: 455.0  epsilon: 1.0    steps: 856  evaluation reward: 196.9\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 259: Policy loss: -0.147414. Value loss: 0.095688. Entropy: 0.300330.\n",
      "Iteration 260: Policy loss: -0.148294. Value loss: 0.063489. Entropy: 0.300346.\n",
      "Iteration 261: Policy loss: -0.150122. Value loss: 0.046794. Entropy: 0.300337.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 262: Policy loss: -0.050410. Value loss: 0.093270. Entropy: 0.299991.\n",
      "Iteration 263: Policy loss: -0.052503. Value loss: 0.040984. Entropy: 0.299812.\n",
      "Iteration 264: Policy loss: -0.048809. Value loss: 0.029066. Entropy: 0.300027.\n",
      "episode: 110   score: 240.0  epsilon: 1.0    steps: 480  evaluation reward: 198.2\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 265: Policy loss: -0.062305. Value loss: 0.087054. Entropy: 0.300087.\n",
      "Iteration 266: Policy loss: -0.067331. Value loss: 0.047804. Entropy: 0.301146.\n",
      "Iteration 267: Policy loss: -0.063732. Value loss: 0.042750. Entropy: 0.301485.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 268: Policy loss: -0.076168. Value loss: 0.079350. Entropy: 0.300552.\n",
      "Iteration 269: Policy loss: -0.082772. Value loss: 0.046983. Entropy: 0.299724.\n",
      "Iteration 270: Policy loss: -0.077847. Value loss: 0.037076. Entropy: 0.300214.\n",
      "episode: 111   score: 180.0  epsilon: 1.0    steps: 328  evaluation reward: 197.9\n",
      "episode: 112   score: 155.0  epsilon: 1.0    steps: 952  evaluation reward: 198.35\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 271: Policy loss: -0.063385. Value loss: 0.083303. Entropy: 0.299893.\n",
      "Iteration 272: Policy loss: -0.070081. Value loss: 0.052446. Entropy: 0.300866.\n",
      "Iteration 273: Policy loss: -0.067376. Value loss: 0.039854. Entropy: 0.301027.\n",
      "episode: 113   score: 135.0  epsilon: 1.0    steps: 136  evaluation reward: 198.15\n",
      "episode: 114   score: 105.0  epsilon: 1.0    steps: 368  evaluation reward: 196.45\n",
      "episode: 115   score: 120.0  epsilon: 1.0    steps: 400  evaluation reward: 195.85\n",
      "episode: 116   score: 135.0  epsilon: 1.0    steps: 544  evaluation reward: 192.6\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 274: Policy loss: 0.146117. Value loss: 0.044458. Entropy: 0.300895.\n",
      "Iteration 275: Policy loss: 0.139920. Value loss: 0.018470. Entropy: 0.300240.\n",
      "Iteration 276: Policy loss: 0.140293. Value loss: 0.011373. Entropy: 0.300300.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 277: Policy loss: -0.143914. Value loss: 0.113876. Entropy: 0.300196.\n",
      "Iteration 278: Policy loss: -0.148984. Value loss: 0.070963. Entropy: 0.300970.\n",
      "Iteration 279: Policy loss: -0.145841. Value loss: 0.054908. Entropy: 0.300220.\n",
      "episode: 117   score: 210.0  epsilon: 1.0    steps: 136  evaluation reward: 192.3\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 280: Policy loss: -0.007587. Value loss: 0.061132. Entropy: 0.301655.\n",
      "Iteration 281: Policy loss: -0.009132. Value loss: 0.035401. Entropy: 0.301229.\n",
      "Iteration 282: Policy loss: -0.009924. Value loss: 0.029533. Entropy: 0.301764.\n",
      "episode: 118   score: 75.0  epsilon: 1.0    steps: 736  evaluation reward: 190.95\n",
      "episode: 119   score: 210.0  epsilon: 1.0    steps: 800  evaluation reward: 192.0\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 283: Policy loss: 0.032942. Value loss: 0.050014. Entropy: 0.303061.\n",
      "Iteration 284: Policy loss: 0.031436. Value loss: 0.031545. Entropy: 0.302725.\n",
      "Iteration 285: Policy loss: 0.030111. Value loss: 0.027853. Entropy: 0.302807.\n",
      "episode: 120   score: 120.0  epsilon: 1.0    steps: 776  evaluation reward: 191.35\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 286: Policy loss: -0.196832. Value loss: 0.084846. Entropy: 0.300779.\n",
      "Iteration 287: Policy loss: -0.206717. Value loss: 0.063245. Entropy: 0.300278.\n",
      "Iteration 288: Policy loss: -0.197789. Value loss: 0.044889. Entropy: 0.300551.\n",
      "episode: 121   score: 105.0  epsilon: 1.0    steps: 216  evaluation reward: 191.3\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 289: Policy loss: -0.109429. Value loss: 0.078443. Entropy: 0.304407.\n",
      "Iteration 290: Policy loss: -0.108506. Value loss: 0.052226. Entropy: 0.304364.\n",
      "Iteration 291: Policy loss: -0.111085. Value loss: 0.037950. Entropy: 0.304450.\n",
      "episode: 122   score: 155.0  epsilon: 1.0    steps: 152  evaluation reward: 191.3\n",
      "episode: 123   score: 110.0  epsilon: 1.0    steps: 240  evaluation reward: 190.0\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 292: Policy loss: 0.025486. Value loss: 0.083048. Entropy: 0.302576.\n",
      "Iteration 293: Policy loss: 0.027895. Value loss: 0.046650. Entropy: 0.304038.\n",
      "Iteration 294: Policy loss: 0.019724. Value loss: 0.030599. Entropy: 0.304055.\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 295: Policy loss: 0.101031. Value loss: 0.092835. Entropy: 0.306500.\n",
      "Iteration 296: Policy loss: 0.095864. Value loss: 0.063496. Entropy: 0.306318.\n",
      "Iteration 297: Policy loss: 0.103532. Value loss: 0.047512. Entropy: 0.306045.\n",
      "episode: 124   score: 285.0  epsilon: 1.0    steps: 312  evaluation reward: 190.25\n",
      "episode: 125   score: 180.0  epsilon: 1.0    steps: 544  evaluation reward: 187.65\n",
      "Training network. lr: 0.000248. clip: 0.099235\n",
      "Iteration 298: Policy loss: 0.148087. Value loss: 0.110796. Entropy: 0.308947.\n",
      "Iteration 299: Policy loss: 0.150703. Value loss: 0.060841. Entropy: 0.308423.\n",
      "Iteration 300: Policy loss: 0.154360. Value loss: 0.050430. Entropy: 0.308738.\n",
      "episode: 126   score: 75.0  epsilon: 1.0    steps: 480  evaluation reward: 187.6\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 301: Policy loss: -0.047900. Value loss: 0.112040. Entropy: 0.309953.\n",
      "Iteration 302: Policy loss: -0.042224. Value loss: 0.071009. Entropy: 0.309729.\n",
      "Iteration 303: Policy loss: -0.048098. Value loss: 0.055342. Entropy: 0.308232.\n",
      "episode: 127   score: 180.0  epsilon: 1.0    steps: 400  evaluation reward: 187.85\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 304: Policy loss: -0.036457. Value loss: 0.130022. Entropy: 0.309128.\n",
      "Iteration 305: Policy loss: -0.038804. Value loss: 0.071889. Entropy: 0.309054.\n",
      "Iteration 306: Policy loss: -0.041290. Value loss: 0.057314. Entropy: 0.308857.\n",
      "episode: 128   score: 210.0  epsilon: 1.0    steps: 832  evaluation reward: 188.85\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 307: Policy loss: -0.383235. Value loss: 0.367859. Entropy: 0.309370.\n",
      "Iteration 308: Policy loss: -0.409607. Value loss: 0.270654. Entropy: 0.309681.\n",
      "Iteration 309: Policy loss: -0.375768. Value loss: 0.200989. Entropy: 0.310310.\n",
      "episode: 129   score: 335.0  epsilon: 1.0    steps: 256  evaluation reward: 190.65\n",
      "episode: 130   score: 155.0  epsilon: 1.0    steps: 304  evaluation reward: 187.05\n",
      "episode: 131   score: 240.0  epsilon: 1.0    steps: 336  evaluation reward: 187.9\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 310: Policy loss: -0.087844. Value loss: 0.071195. Entropy: 0.307939.\n",
      "Iteration 311: Policy loss: -0.094067. Value loss: 0.037128. Entropy: 0.308772.\n",
      "Iteration 312: Policy loss: -0.089919. Value loss: 0.028494. Entropy: 0.308818.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 313: Policy loss: -0.007934. Value loss: 0.159731. Entropy: 0.307235.\n",
      "Iteration 314: Policy loss: -0.008603. Value loss: 0.109276. Entropy: 0.307606.\n",
      "Iteration 315: Policy loss: -0.003440. Value loss: 0.093171. Entropy: 0.307744.\n",
      "episode: 132   score: 210.0  epsilon: 1.0    steps: 880  evaluation reward: 189.25\n",
      "episode: 133   score: 335.0  epsilon: 1.0    steps: 944  evaluation reward: 191.05\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 316: Policy loss: -0.082059. Value loss: 0.205533. Entropy: 0.309433.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 317: Policy loss: -0.086123. Value loss: 0.171853. Entropy: 0.307447.\n",
      "Iteration 318: Policy loss: -0.090028. Value loss: 0.158286. Entropy: 0.308561.\n",
      "episode: 134   score: 135.0  epsilon: 1.0    steps: 536  evaluation reward: 188.6\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 319: Policy loss: 0.060408. Value loss: 0.025223. Entropy: 0.311062.\n",
      "Iteration 320: Policy loss: 0.064124. Value loss: 0.011927. Entropy: 0.310658.\n",
      "Iteration 321: Policy loss: 0.060911. Value loss: 0.010168. Entropy: 0.310868.\n",
      "episode: 135   score: 155.0  epsilon: 1.0    steps: 744  evaluation reward: 187.6\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 322: Policy loss: 0.176940. Value loss: 0.097887. Entropy: 0.310519.\n",
      "Iteration 323: Policy loss: 0.175646. Value loss: 0.060738. Entropy: 0.310161.\n",
      "Iteration 324: Policy loss: 0.168918. Value loss: 0.043239. Entropy: 0.310416.\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 325: Policy loss: 0.092346. Value loss: 0.094472. Entropy: 0.306715.\n",
      "Iteration 326: Policy loss: 0.086375. Value loss: 0.052541. Entropy: 0.306217.\n",
      "Iteration 327: Policy loss: 0.091177. Value loss: 0.043111. Entropy: 0.306671.\n",
      "episode: 136   score: 50.0  epsilon: 1.0    steps: 96  evaluation reward: 187.0\n",
      "episode: 137   score: 210.0  epsilon: 1.0    steps: 392  evaluation reward: 187.0\n",
      "episode: 138   score: 155.0  epsilon: 1.0    steps: 968  evaluation reward: 186.3\n",
      "episode: 139   score: 210.0  epsilon: 1.0    steps: 1016  evaluation reward: 186.3\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 328: Policy loss: -0.042969. Value loss: 0.051681. Entropy: 0.306226.\n",
      "Iteration 329: Policy loss: -0.039724. Value loss: 0.028563. Entropy: 0.305599.\n",
      "Iteration 330: Policy loss: -0.044103. Value loss: 0.023301. Entropy: 0.305790.\n",
      "episode: 140   score: 240.0  epsilon: 1.0    steps: 912  evaluation reward: 187.65\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 331: Policy loss: -0.077786. Value loss: 0.058994. Entropy: 0.301209.\n",
      "Iteration 332: Policy loss: -0.082074. Value loss: 0.042892. Entropy: 0.300779.\n",
      "Iteration 333: Policy loss: -0.084887. Value loss: 0.033203. Entropy: 0.301316.\n",
      "episode: 141   score: 105.0  epsilon: 1.0    steps: 680  evaluation reward: 186.9\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 334: Policy loss: -0.056259. Value loss: 0.130596. Entropy: 0.300668.\n",
      "Iteration 335: Policy loss: -0.060103. Value loss: 0.087391. Entropy: 0.301183.\n",
      "Iteration 336: Policy loss: -0.064269. Value loss: 0.062893. Entropy: 0.301258.\n",
      "episode: 142   score: 180.0  epsilon: 1.0    steps: 192  evaluation reward: 187.45\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 337: Policy loss: 0.021935. Value loss: 0.066638. Entropy: 0.303571.\n",
      "Iteration 338: Policy loss: 0.018005. Value loss: 0.036873. Entropy: 0.303474.\n",
      "Iteration 339: Policy loss: 0.017090. Value loss: 0.023374. Entropy: 0.303676.\n",
      "episode: 143   score: 75.0  epsilon: 1.0    steps: 96  evaluation reward: 186.1\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 340: Policy loss: -0.029739. Value loss: 0.061429. Entropy: 0.304588.\n",
      "Iteration 341: Policy loss: -0.041160. Value loss: 0.033190. Entropy: 0.304490.\n",
      "Iteration 342: Policy loss: -0.034692. Value loss: 0.027805. Entropy: 0.304346.\n",
      "episode: 144   score: 155.0  epsilon: 1.0    steps: 72  evaluation reward: 187.15\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 343: Policy loss: -0.123001. Value loss: 0.083207. Entropy: 0.301747.\n",
      "Iteration 344: Policy loss: -0.124519. Value loss: 0.053200. Entropy: 0.301463.\n",
      "Iteration 345: Policy loss: -0.121450. Value loss: 0.041522. Entropy: 0.301779.\n",
      "episode: 145   score: 155.0  epsilon: 1.0    steps: 320  evaluation reward: 187.15\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 346: Policy loss: -0.163726. Value loss: 0.082550. Entropy: 0.299909.\n",
      "Iteration 347: Policy loss: -0.166903. Value loss: 0.057057. Entropy: 0.300254.\n",
      "Iteration 348: Policy loss: -0.166177. Value loss: 0.040472. Entropy: 0.300226.\n",
      "episode: 146   score: 210.0  epsilon: 1.0    steps: 432  evaluation reward: 188.2\n",
      "episode: 147   score: 255.0  epsilon: 1.0    steps: 560  evaluation reward: 188.6\n",
      "episode: 148   score: 390.0  epsilon: 1.0    steps: 1000  evaluation reward: 192.0\n",
      "Training network. lr: 0.000248. clip: 0.099088\n",
      "Iteration 349: Policy loss: -0.412903. Value loss: 0.336813. Entropy: 0.300464.\n",
      "Iteration 350: Policy loss: -0.405227. Value loss: 0.271064. Entropy: 0.300527.\n",
      "Iteration 351: Policy loss: -0.406773. Value loss: 0.199817. Entropy: 0.300864.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 352: Policy loss: -0.130393. Value loss: 0.078482. Entropy: 0.302139.\n",
      "Iteration 353: Policy loss: -0.129309. Value loss: 0.035576. Entropy: 0.301952.\n",
      "Iteration 354: Policy loss: -0.135285. Value loss: 0.029239. Entropy: 0.302137.\n",
      "episode: 149   score: 410.0  epsilon: 1.0    steps: 168  evaluation reward: 194.0\n",
      "episode: 150   score: 140.0  epsilon: 1.0    steps: 240  evaluation reward: 190.9\n",
      "now time :  2019-09-05 14:36:35.914317\n",
      "episode: 151   score: 110.0  epsilon: 1.0    steps: 1016  evaluation reward: 190.75\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 355: Policy loss: -0.359887. Value loss: 0.260221. Entropy: 0.302221.\n",
      "Iteration 356: Policy loss: -0.366460. Value loss: 0.143656. Entropy: 0.302147.\n",
      "Iteration 357: Policy loss: -0.351493. Value loss: 0.117828. Entropy: 0.301659.\n",
      "episode: 152   score: 410.0  epsilon: 1.0    steps: 616  evaluation reward: 193.5\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 358: Policy loss: -0.217013. Value loss: 0.136843. Entropy: 0.299181.\n",
      "Iteration 359: Policy loss: -0.207266. Value loss: 0.089389. Entropy: 0.299297.\n",
      "Iteration 360: Policy loss: -0.215740. Value loss: 0.080496. Entropy: 0.298462.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 361: Policy loss: -0.169917. Value loss: 0.056491. Entropy: 0.302824.\n",
      "Iteration 362: Policy loss: -0.175525. Value loss: 0.036082. Entropy: 0.301780.\n",
      "Iteration 363: Policy loss: -0.173035. Value loss: 0.030070. Entropy: 0.301486.\n",
      "episode: 153   score: 210.0  epsilon: 1.0    steps: 968  evaluation reward: 193.6\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 364: Policy loss: -0.015914. Value loss: 0.111226. Entropy: 0.303558.\n",
      "Iteration 365: Policy loss: -0.012942. Value loss: 0.067485. Entropy: 0.303908.\n",
      "Iteration 366: Policy loss: -0.011608. Value loss: 0.058611. Entropy: 0.303524.\n",
      "episode: 154   score: 155.0  epsilon: 1.0    steps: 936  evaluation reward: 193.6\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 367: Policy loss: 0.113878. Value loss: 0.152496. Entropy: 0.301337.\n",
      "Iteration 368: Policy loss: 0.107154. Value loss: 0.089705. Entropy: 0.300277.\n",
      "Iteration 369: Policy loss: 0.109459. Value loss: 0.066889. Entropy: 0.300575.\n",
      "episode: 155   score: 480.0  epsilon: 1.0    steps: 912  evaluation reward: 196.6\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 370: Policy loss: -0.211826. Value loss: 0.134887. Entropy: 0.301984.\n",
      "Iteration 371: Policy loss: -0.210108. Value loss: 0.083278. Entropy: 0.301591.\n",
      "Iteration 372: Policy loss: -0.212821. Value loss: 0.069145. Entropy: 0.300495.\n",
      "episode: 156   score: 275.0  epsilon: 1.0    steps: 680  evaluation reward: 198.3\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 373: Policy loss: -0.031563. Value loss: 0.167097. Entropy: 0.303982.\n",
      "Iteration 374: Policy loss: -0.029442. Value loss: 0.088051. Entropy: 0.303950.\n",
      "Iteration 375: Policy loss: -0.032294. Value loss: 0.062853. Entropy: 0.303317.\n",
      "episode: 157   score: 155.0  epsilon: 1.0    steps: 232  evaluation reward: 197.5\n",
      "episode: 158   score: 265.0  epsilon: 1.0    steps: 984  evaluation reward: 198.8\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 376: Policy loss: 0.069548. Value loss: 0.093634. Entropy: 0.301080.\n",
      "Iteration 377: Policy loss: 0.069841. Value loss: 0.053467. Entropy: 0.300358.\n",
      "Iteration 378: Policy loss: 0.072549. Value loss: 0.041533. Entropy: 0.300754.\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 379: Policy loss: -0.409724. Value loss: 0.314455. Entropy: 0.300877.\n",
      "Iteration 380: Policy loss: -0.396653. Value loss: 0.171543. Entropy: 0.301761.\n",
      "Iteration 381: Policy loss: -0.433700. Value loss: 0.115867. Entropy: 0.302938.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 159   score: 270.0  epsilon: 1.0    steps: 232  evaluation reward: 200.15\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 382: Policy loss: -0.161313. Value loss: 0.368421. Entropy: 0.304058.\n",
      "Iteration 383: Policy loss: -0.197960. Value loss: 0.234876. Entropy: 0.305501.\n",
      "Iteration 384: Policy loss: -0.197468. Value loss: 0.185866. Entropy: 0.305080.\n",
      "episode: 160   score: 380.0  epsilon: 1.0    steps: 328  evaluation reward: 201.85\n",
      "episode: 161   score: 440.0  epsilon: 1.0    steps: 496  evaluation reward: 203.1\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 385: Policy loss: 0.084137. Value loss: 0.114610. Entropy: 0.301088.\n",
      "Iteration 386: Policy loss: 0.091531. Value loss: 0.052758. Entropy: 0.300881.\n",
      "Iteration 387: Policy loss: 0.083467. Value loss: 0.035825. Entropy: 0.300521.\n",
      "episode: 162   score: 210.0  epsilon: 1.0    steps: 480  evaluation reward: 203.85\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 388: Policy loss: 0.448084. Value loss: 0.125863. Entropy: 0.304422.\n",
      "Iteration 389: Policy loss: 0.449078. Value loss: 0.072379. Entropy: 0.303972.\n",
      "Iteration 390: Policy loss: 0.444529. Value loss: 0.053756. Entropy: 0.304629.\n",
      "episode: 163   score: 180.0  epsilon: 1.0    steps: 312  evaluation reward: 204.1\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 391: Policy loss: 0.164989. Value loss: 0.116010. Entropy: 0.309278.\n",
      "Iteration 392: Policy loss: 0.156791. Value loss: 0.063689. Entropy: 0.308351.\n",
      "Iteration 393: Policy loss: 0.158047. Value loss: 0.056212. Entropy: 0.307690.\n",
      "episode: 164   score: 210.0  epsilon: 1.0    steps: 304  evaluation reward: 205.8\n",
      "episode: 165   score: 180.0  epsilon: 1.0    steps: 848  evaluation reward: 205.05\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 394: Policy loss: 0.196611. Value loss: 0.099814. Entropy: 0.304738.\n",
      "Iteration 395: Policy loss: 0.192219. Value loss: 0.065105. Entropy: 0.303297.\n",
      "Iteration 396: Policy loss: 0.192581. Value loss: 0.058932. Entropy: 0.304049.\n",
      "episode: 166   score: 210.0  epsilon: 1.0    steps: 728  evaluation reward: 201.85\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 397: Policy loss: -0.007221. Value loss: 0.083645. Entropy: 0.304219.\n",
      "Iteration 398: Policy loss: -0.010524. Value loss: 0.059741. Entropy: 0.304640.\n",
      "Iteration 399: Policy loss: -0.008700. Value loss: 0.048768. Entropy: 0.304033.\n",
      "episode: 167   score: 210.0  epsilon: 1.0    steps: 320  evaluation reward: 201.85\n",
      "Training network. lr: 0.000247. clip: 0.098931\n",
      "Iteration 400: Policy loss: -0.092547. Value loss: 0.097607. Entropy: 0.307969.\n",
      "Iteration 401: Policy loss: -0.087748. Value loss: 0.060959. Entropy: 0.307642.\n",
      "Iteration 402: Policy loss: -0.086342. Value loss: 0.044775. Entropy: 0.307746.\n",
      "episode: 168   score: 140.0  epsilon: 1.0    steps: 704  evaluation reward: 200.85\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 403: Policy loss: -0.034169. Value loss: 0.132744. Entropy: 0.304881.\n",
      "Iteration 404: Policy loss: -0.037807. Value loss: 0.084799. Entropy: 0.303812.\n",
      "Iteration 405: Policy loss: -0.037987. Value loss: 0.068143. Entropy: 0.303667.\n",
      "episode: 169   score: 210.0  epsilon: 1.0    steps: 64  evaluation reward: 201.4\n",
      "episode: 170   score: 265.0  epsilon: 1.0    steps: 928  evaluation reward: 202.25\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 406: Policy loss: 0.092794. Value loss: 0.090670. Entropy: 0.303232.\n",
      "Iteration 407: Policy loss: 0.091990. Value loss: 0.051635. Entropy: 0.302673.\n",
      "Iteration 408: Policy loss: 0.094969. Value loss: 0.035590. Entropy: 0.303324.\n",
      "episode: 171   score: 210.0  epsilon: 1.0    steps: 832  evaluation reward: 202.25\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 409: Policy loss: 0.077486. Value loss: 0.137069. Entropy: 0.308549.\n",
      "Iteration 410: Policy loss: 0.072812. Value loss: 0.092564. Entropy: 0.309537.\n",
      "Iteration 411: Policy loss: 0.066395. Value loss: 0.077352. Entropy: 0.308613.\n",
      "episode: 172   score: 180.0  epsilon: 1.0    steps: 880  evaluation reward: 202.25\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 412: Policy loss: -0.036292. Value loss: 0.071637. Entropy: 0.308836.\n",
      "Iteration 413: Policy loss: -0.039160. Value loss: 0.037612. Entropy: 0.308742.\n",
      "Iteration 414: Policy loss: -0.043031. Value loss: 0.030564. Entropy: 0.309716.\n",
      "episode: 173   score: 120.0  epsilon: 1.0    steps: 576  evaluation reward: 201.9\n",
      "episode: 174   score: 180.0  epsilon: 1.0    steps: 728  evaluation reward: 202.15\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 415: Policy loss: -0.184483. Value loss: 0.334207. Entropy: 0.305910.\n",
      "Iteration 416: Policy loss: -0.171572. Value loss: 0.203335. Entropy: 0.306035.\n",
      "Iteration 417: Policy loss: -0.191848. Value loss: 0.157034. Entropy: 0.304856.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 418: Policy loss: -0.070338. Value loss: 0.063553. Entropy: 0.303425.\n",
      "Iteration 419: Policy loss: -0.072476. Value loss: 0.042069. Entropy: 0.303755.\n",
      "Iteration 420: Policy loss: -0.069790. Value loss: 0.037126. Entropy: 0.303850.\n",
      "episode: 175   score: 380.0  epsilon: 1.0    steps: 168  evaluation reward: 204.15\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 421: Policy loss: -0.004740. Value loss: 0.106625. Entropy: 0.308541.\n",
      "Iteration 422: Policy loss: -0.004339. Value loss: 0.054642. Entropy: 0.308158.\n",
      "Iteration 423: Policy loss: -0.004155. Value loss: 0.042758. Entropy: 0.308561.\n",
      "episode: 176   score: 210.0  epsilon: 1.0    steps: 648  evaluation reward: 204.4\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 424: Policy loss: 0.129964. Value loss: 0.084945. Entropy: 0.306697.\n",
      "Iteration 425: Policy loss: 0.131365. Value loss: 0.044592. Entropy: 0.305191.\n",
      "Iteration 426: Policy loss: 0.131260. Value loss: 0.034879. Entropy: 0.305976.\n",
      "episode: 177   score: 210.0  epsilon: 1.0    steps: 632  evaluation reward: 204.4\n",
      "episode: 178   score: 260.0  epsilon: 1.0    steps: 704  evaluation reward: 204.9\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 427: Policy loss: 0.027492. Value loss: 0.098290. Entropy: 0.306754.\n",
      "Iteration 428: Policy loss: 0.016078. Value loss: 0.067799. Entropy: 0.306631.\n",
      "Iteration 429: Policy loss: 0.026246. Value loss: 0.047984. Entropy: 0.306249.\n",
      "episode: 179   score: 180.0  epsilon: 1.0    steps: 304  evaluation reward: 205.65\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 430: Policy loss: 0.097033. Value loss: 0.063491. Entropy: 0.313632.\n",
      "Iteration 431: Policy loss: 0.097593. Value loss: 0.037059. Entropy: 0.313183.\n",
      "Iteration 432: Policy loss: 0.095176. Value loss: 0.030576. Entropy: 0.314191.\n",
      "episode: 180   score: 210.0  epsilon: 1.0    steps: 344  evaluation reward: 206.2\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 433: Policy loss: -0.002056. Value loss: 0.112935. Entropy: 0.306390.\n",
      "Iteration 434: Policy loss: 0.000713. Value loss: 0.068569. Entropy: 0.306473.\n",
      "Iteration 435: Policy loss: 0.002288. Value loss: 0.059116. Entropy: 0.305863.\n",
      "episode: 181   score: 180.0  epsilon: 1.0    steps: 152  evaluation reward: 205.4\n",
      "episode: 182   score: 105.0  epsilon: 1.0    steps: 200  evaluation reward: 204.35\n",
      "episode: 183   score: 155.0  epsilon: 1.0    steps: 248  evaluation reward: 203.8\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 436: Policy loss: -0.015026. Value loss: 0.020150. Entropy: 0.313719.\n",
      "Iteration 437: Policy loss: -0.018281. Value loss: 0.013471. Entropy: 0.313184.\n",
      "Iteration 438: Policy loss: -0.017180. Value loss: 0.010126. Entropy: 0.312637.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 439: Policy loss: -0.033794. Value loss: 0.079748. Entropy: 0.311122.\n",
      "Iteration 440: Policy loss: -0.035426. Value loss: 0.053659. Entropy: 0.309468.\n",
      "Iteration 441: Policy loss: -0.036773. Value loss: 0.045635. Entropy: 0.309647.\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 442: Policy loss: -0.170786. Value loss: 0.051559. Entropy: 0.307062.\n",
      "Iteration 443: Policy loss: -0.170173. Value loss: 0.020082. Entropy: 0.307388.\n",
      "Iteration 444: Policy loss: -0.169456. Value loss: 0.013500. Entropy: 0.307733.\n",
      "episode: 184   score: 210.0  epsilon: 1.0    steps: 272  evaluation reward: 202.1\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 445: Policy loss: 0.048895. Value loss: 0.069367. Entropy: 0.310836.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 446: Policy loss: 0.051993. Value loss: 0.038382. Entropy: 0.310445.\n",
      "Iteration 447: Policy loss: 0.047904. Value loss: 0.035645. Entropy: 0.310614.\n",
      "episode: 185   score: 160.0  epsilon: 1.0    steps: 16  evaluation reward: 202.3\n",
      "episode: 186   score: 75.0  epsilon: 1.0    steps: 224  evaluation reward: 201.25\n",
      "episode: 187   score: 210.0  epsilon: 1.0    steps: 568  evaluation reward: 201.75\n",
      "Training network. lr: 0.000247. clip: 0.098774\n",
      "Iteration 448: Policy loss: -0.410388. Value loss: 0.380311. Entropy: 0.312355.\n",
      "Iteration 449: Policy loss: -0.406879. Value loss: 0.331196. Entropy: 0.311127.\n",
      "Iteration 450: Policy loss: -0.418655. Value loss: 0.327167. Entropy: 0.311873.\n",
      "episode: 188   score: 410.0  epsilon: 1.0    steps: 96  evaluation reward: 204.05\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 451: Policy loss: -0.004777. Value loss: 0.060357. Entropy: 0.312691.\n",
      "Iteration 452: Policy loss: -0.003765. Value loss: 0.040119. Entropy: 0.314112.\n",
      "Iteration 453: Policy loss: -0.009789. Value loss: 0.033323. Entropy: 0.314605.\n",
      "episode: 189   score: 260.0  epsilon: 1.0    steps: 896  evaluation reward: 204.55\n",
      "episode: 190   score: 180.0  epsilon: 1.0    steps: 936  evaluation reward: 204.7\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 454: Policy loss: 0.130954. Value loss: 0.093446. Entropy: 0.308824.\n",
      "Iteration 455: Policy loss: 0.130030. Value loss: 0.052124. Entropy: 0.308290.\n",
      "Iteration 456: Policy loss: 0.121349. Value loss: 0.040341. Entropy: 0.308383.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 457: Policy loss: -0.182853. Value loss: 0.063232. Entropy: 0.308133.\n",
      "Iteration 458: Policy loss: -0.179843. Value loss: 0.029168. Entropy: 0.307349.\n",
      "Iteration 459: Policy loss: -0.183754. Value loss: 0.021364. Entropy: 0.308792.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 460: Policy loss: -0.413207. Value loss: 0.361900. Entropy: 0.307848.\n",
      "Iteration 461: Policy loss: -0.421786. Value loss: 0.328979. Entropy: 0.309070.\n",
      "Iteration 462: Policy loss: -0.418171. Value loss: 0.328223. Entropy: 0.308301.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 463: Policy loss: -0.301114. Value loss: 0.422036. Entropy: 0.309952.\n",
      "Iteration 464: Policy loss: -0.237721. Value loss: 0.268107. Entropy: 0.309888.\n",
      "Iteration 465: Policy loss: -0.308963. Value loss: 0.270254. Entropy: 0.310339.\n",
      "episode: 191   score: 715.0  epsilon: 1.0    steps: 272  evaluation reward: 208.3\n",
      "episode: 192   score: 180.0  epsilon: 1.0    steps: 776  evaluation reward: 208.45\n",
      "episode: 193   score: 270.0  epsilon: 1.0    steps: 984  evaluation reward: 209.35\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 466: Policy loss: 0.334854. Value loss: 0.101297. Entropy: 0.307543.\n",
      "Iteration 467: Policy loss: 0.326441. Value loss: 0.052150. Entropy: 0.305567.\n",
      "Iteration 468: Policy loss: 0.326827. Value loss: 0.042009. Entropy: 0.305992.\n",
      "episode: 194   score: 180.0  epsilon: 1.0    steps: 32  evaluation reward: 209.35\n",
      "episode: 195   score: 155.0  epsilon: 1.0    steps: 488  evaluation reward: 208.8\n",
      "episode: 196   score: 370.0  epsilon: 1.0    steps: 944  evaluation reward: 208.7\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 469: Policy loss: 0.116818. Value loss: 0.038684. Entropy: 0.313838.\n",
      "Iteration 470: Policy loss: 0.121983. Value loss: 0.021532. Entropy: 0.311839.\n",
      "Iteration 471: Policy loss: 0.116093. Value loss: 0.016523. Entropy: 0.313126.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 472: Policy loss: -0.023703. Value loss: 0.100041. Entropy: 0.307196.\n",
      "Iteration 473: Policy loss: -0.024603. Value loss: 0.076666. Entropy: 0.307000.\n",
      "Iteration 474: Policy loss: -0.021941. Value loss: 0.065355. Entropy: 0.306607.\n",
      "episode: 197   score: 180.0  epsilon: 1.0    steps: 544  evaluation reward: 206.4\n",
      "episode: 198   score: 210.0  epsilon: 1.0    steps: 592  evaluation reward: 206.25\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 475: Policy loss: 0.037630. Value loss: 0.052391. Entropy: 0.305381.\n",
      "Iteration 476: Policy loss: 0.039764. Value loss: 0.034897. Entropy: 0.306043.\n",
      "Iteration 477: Policy loss: 0.033422. Value loss: 0.029742. Entropy: 0.305396.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 478: Policy loss: 0.030747. Value loss: 0.024289. Entropy: 0.305483.\n",
      "Iteration 479: Policy loss: 0.033000. Value loss: 0.013456. Entropy: 0.305339.\n",
      "Iteration 480: Policy loss: 0.033159. Value loss: 0.012065. Entropy: 0.304688.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 481: Policy loss: 0.011563. Value loss: 0.076122. Entropy: 0.306020.\n",
      "Iteration 482: Policy loss: 0.013240. Value loss: 0.049737. Entropy: 0.306478.\n",
      "Iteration 483: Policy loss: 0.013512. Value loss: 0.039331. Entropy: 0.305638.\n",
      "episode: 199   score: 155.0  epsilon: 1.0    steps: 608  evaluation reward: 206.75\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 484: Policy loss: 0.227451. Value loss: 0.082773. Entropy: 0.309756.\n",
      "Iteration 485: Policy loss: 0.219370. Value loss: 0.042885. Entropy: 0.309259.\n",
      "Iteration 486: Policy loss: 0.215786. Value loss: 0.036241. Entropy: 0.309950.\n",
      "episode: 200   score: 155.0  epsilon: 1.0    steps: 320  evaluation reward: 207.55\n",
      "now time :  2019-09-05 14:44:45.114613\n",
      "episode: 201   score: 210.0  epsilon: 1.0    steps: 568  evaluation reward: 208.55\n",
      "episode: 202   score: 180.0  epsilon: 1.0    steps: 816  evaluation reward: 208.55\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 487: Policy loss: -0.054012. Value loss: 0.103152. Entropy: 0.310161.\n",
      "Iteration 488: Policy loss: -0.058320. Value loss: 0.057970. Entropy: 0.310148.\n",
      "Iteration 489: Policy loss: -0.050469. Value loss: 0.039804. Entropy: 0.310033.\n",
      "episode: 203   score: 210.0  epsilon: 1.0    steps: 280  evaluation reward: 208.85\n",
      "episode: 204   score: 105.0  epsilon: 1.0    steps: 712  evaluation reward: 208.8\n",
      "episode: 205   score: 260.0  epsilon: 1.0    steps: 784  evaluation reward: 209.3\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 490: Policy loss: -0.193289. Value loss: 0.102657. Entropy: 0.305769.\n",
      "Iteration 491: Policy loss: -0.195394. Value loss: 0.067057. Entropy: 0.305872.\n",
      "Iteration 492: Policy loss: -0.196664. Value loss: 0.045657. Entropy: 0.306521.\n",
      "episode: 206   score: 135.0  epsilon: 1.0    steps: 712  evaluation reward: 208.85\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 493: Policy loss: 0.109791. Value loss: 0.062147. Entropy: 0.307839.\n",
      "Iteration 494: Policy loss: 0.104286. Value loss: 0.036211. Entropy: 0.308194.\n",
      "Iteration 495: Policy loss: 0.104007. Value loss: 0.027655. Entropy: 0.306737.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 496: Policy loss: 0.050446. Value loss: 0.035862. Entropy: 0.306207.\n",
      "Iteration 497: Policy loss: 0.048996. Value loss: 0.017975. Entropy: 0.305986.\n",
      "Iteration 498: Policy loss: 0.048705. Value loss: 0.014711. Entropy: 0.305900.\n",
      "Training network. lr: 0.000247. clip: 0.098627\n",
      "Iteration 499: Policy loss: -0.058900. Value loss: 0.037010. Entropy: 0.303251.\n",
      "Iteration 500: Policy loss: -0.063470. Value loss: 0.030218. Entropy: 0.302670.\n",
      "Iteration 501: Policy loss: -0.063191. Value loss: 0.020099. Entropy: 0.302943.\n",
      "episode: 207   score: 155.0  epsilon: 1.0    steps: 712  evaluation reward: 209.3\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 502: Policy loss: -0.112281. Value loss: 0.059215. Entropy: 0.305666.\n",
      "Iteration 503: Policy loss: -0.112991. Value loss: 0.034306. Entropy: 0.305828.\n",
      "Iteration 504: Policy loss: -0.115149. Value loss: 0.028674. Entropy: 0.305412.\n",
      "episode: 208   score: 210.0  epsilon: 1.0    steps: 656  evaluation reward: 210.3\n",
      "episode: 209   score: 105.0  epsilon: 1.0    steps: 736  evaluation reward: 206.8\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 505: Policy loss: -0.174157. Value loss: 0.098358. Entropy: 0.303935.\n",
      "Iteration 506: Policy loss: -0.171606. Value loss: 0.061377. Entropy: 0.303840.\n",
      "Iteration 507: Policy loss: -0.175328. Value loss: 0.051527. Entropy: 0.303618.\n",
      "episode: 210   score: 155.0  epsilon: 1.0    steps: 368  evaluation reward: 205.95\n",
      "episode: 211   score: 240.0  epsilon: 1.0    steps: 528  evaluation reward: 206.55\n",
      "episode: 212   score: 135.0  epsilon: 1.0    steps: 704  evaluation reward: 206.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 508: Policy loss: 0.143334. Value loss: 0.073930. Entropy: 0.310891.\n",
      "Iteration 509: Policy loss: 0.135727. Value loss: 0.034330. Entropy: 0.309867.\n",
      "Iteration 510: Policy loss: 0.139621. Value loss: 0.027317. Entropy: 0.310562.\n",
      "episode: 213   score: 240.0  epsilon: 1.0    steps: 408  evaluation reward: 207.4\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 511: Policy loss: 0.237362. Value loss: 0.179286. Entropy: 0.309028.\n",
      "Iteration 512: Policy loss: 0.245715. Value loss: 0.102672. Entropy: 0.308951.\n",
      "Iteration 513: Policy loss: 0.239417. Value loss: 0.080930. Entropy: 0.308315.\n",
      "episode: 214   score: 50.0  epsilon: 1.0    steps: 704  evaluation reward: 206.85\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 514: Policy loss: 0.043187. Value loss: 0.052896. Entropy: 0.303742.\n",
      "Iteration 515: Policy loss: 0.039772. Value loss: 0.039807. Entropy: 0.303993.\n",
      "Iteration 516: Policy loss: 0.039053. Value loss: 0.033090. Entropy: 0.302971.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 517: Policy loss: -0.108371. Value loss: 0.059435. Entropy: 0.304537.\n",
      "Iteration 518: Policy loss: -0.109679. Value loss: 0.031904. Entropy: 0.304958.\n",
      "Iteration 519: Policy loss: -0.109005. Value loss: 0.024838. Entropy: 0.304398.\n",
      "episode: 215   score: 110.0  epsilon: 1.0    steps: 608  evaluation reward: 206.75\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 520: Policy loss: -0.019094. Value loss: 0.071706. Entropy: 0.300738.\n",
      "Iteration 521: Policy loss: -0.022672. Value loss: 0.030155. Entropy: 0.299984.\n",
      "Iteration 522: Policy loss: -0.022880. Value loss: 0.021672. Entropy: 0.300768.\n",
      "episode: 216   score: 180.0  epsilon: 1.0    steps: 96  evaluation reward: 207.2\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 523: Policy loss: -0.148532. Value loss: 0.123802. Entropy: 0.307138.\n",
      "Iteration 524: Policy loss: -0.148958. Value loss: 0.065045. Entropy: 0.306770.\n",
      "Iteration 525: Policy loss: -0.141964. Value loss: 0.042294. Entropy: 0.306939.\n",
      "episode: 217   score: 315.0  epsilon: 1.0    steps: 24  evaluation reward: 208.25\n",
      "episode: 218   score: 210.0  epsilon: 1.0    steps: 896  evaluation reward: 209.6\n",
      "episode: 219   score: 180.0  epsilon: 1.0    steps: 992  evaluation reward: 209.3\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 526: Policy loss: -0.208352. Value loss: 0.111144. Entropy: 0.302915.\n",
      "Iteration 527: Policy loss: -0.202328. Value loss: 0.061592. Entropy: 0.302899.\n",
      "Iteration 528: Policy loss: -0.207358. Value loss: 0.047204. Entropy: 0.303162.\n",
      "episode: 220   score: 180.0  epsilon: 1.0    steps: 640  evaluation reward: 209.9\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 529: Policy loss: -0.096529. Value loss: 0.064627. Entropy: 0.305251.\n",
      "Iteration 530: Policy loss: -0.104148. Value loss: 0.044071. Entropy: 0.304494.\n",
      "Iteration 531: Policy loss: -0.102217. Value loss: 0.036653. Entropy: 0.304174.\n",
      "episode: 221   score: 260.0  epsilon: 1.0    steps: 184  evaluation reward: 211.45\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 532: Policy loss: -0.013259. Value loss: 0.097543. Entropy: 0.304006.\n",
      "Iteration 533: Policy loss: -0.009100. Value loss: 0.055159. Entropy: 0.303830.\n",
      "Iteration 534: Policy loss: -0.026415. Value loss: 0.045233. Entropy: 0.304569.\n",
      "episode: 222   score: 410.0  epsilon: 1.0    steps: 96  evaluation reward: 214.0\n",
      "episode: 223   score: 120.0  epsilon: 1.0    steps: 224  evaluation reward: 214.1\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 535: Policy loss: -0.410508. Value loss: 0.341021. Entropy: 0.305316.\n",
      "Iteration 536: Policy loss: -0.360386. Value loss: 0.212488. Entropy: 0.304685.\n",
      "Iteration 537: Policy loss: -0.360611. Value loss: 0.153516. Entropy: 0.304998.\n",
      "episode: 224   score: 355.0  epsilon: 1.0    steps: 672  evaluation reward: 214.8\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 538: Policy loss: 0.008788. Value loss: 0.070227. Entropy: 0.307801.\n",
      "Iteration 539: Policy loss: 0.009130. Value loss: 0.046535. Entropy: 0.307401.\n",
      "Iteration 540: Policy loss: 0.003924. Value loss: 0.040412. Entropy: 0.307610.\n",
      "episode: 225   score: 155.0  epsilon: 1.0    steps: 312  evaluation reward: 214.55\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 541: Policy loss: -0.050173. Value loss: 0.036102. Entropy: 0.305611.\n",
      "Iteration 542: Policy loss: -0.053166. Value loss: 0.021233. Entropy: 0.305402.\n",
      "Iteration 543: Policy loss: -0.054207. Value loss: 0.018381. Entropy: 0.304848.\n",
      "episode: 226   score: 105.0  epsilon: 1.0    steps: 648  evaluation reward: 214.85\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 544: Policy loss: -0.212141. Value loss: 0.113668. Entropy: 0.304401.\n",
      "Iteration 545: Policy loss: -0.216001. Value loss: 0.080356. Entropy: 0.303780.\n",
      "Iteration 546: Policy loss: -0.221402. Value loss: 0.063840. Entropy: 0.304244.\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 547: Policy loss: 0.081216. Value loss: 0.055575. Entropy: 0.305671.\n",
      "Iteration 548: Policy loss: 0.084607. Value loss: 0.030125. Entropy: 0.305893.\n",
      "Iteration 549: Policy loss: 0.078811. Value loss: 0.024217. Entropy: 0.304511.\n",
      "episode: 227   score: 285.0  epsilon: 1.0    steps: 368  evaluation reward: 215.9\n",
      "episode: 228   score: 210.0  epsilon: 1.0    steps: 768  evaluation reward: 215.9\n",
      "Training network. lr: 0.000246. clip: 0.098470\n",
      "Iteration 550: Policy loss: 0.087705. Value loss: 0.121640. Entropy: 0.310290.\n",
      "Iteration 551: Policy loss: 0.087804. Value loss: 0.075764. Entropy: 0.310673.\n",
      "Iteration 552: Policy loss: 0.089943. Value loss: 0.058349. Entropy: 0.310702.\n",
      "episode: 229   score: 120.0  epsilon: 1.0    steps: 384  evaluation reward: 213.75\n",
      "episode: 230   score: 210.0  epsilon: 1.0    steps: 448  evaluation reward: 214.3\n",
      "episode: 231   score: 330.0  epsilon: 1.0    steps: 568  evaluation reward: 215.2\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 553: Policy loss: 0.019485. Value loss: 0.114570. Entropy: 0.310612.\n",
      "Iteration 554: Policy loss: 0.023195. Value loss: 0.072028. Entropy: 0.310966.\n",
      "Iteration 555: Policy loss: 0.018100. Value loss: 0.056333. Entropy: 0.312372.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 556: Policy loss: -0.062956. Value loss: 0.103236. Entropy: 0.313522.\n",
      "Iteration 557: Policy loss: -0.061830. Value loss: 0.062725. Entropy: 0.314062.\n",
      "Iteration 558: Policy loss: -0.071961. Value loss: 0.054364. Entropy: 0.314103.\n",
      "episode: 232   score: 185.0  epsilon: 1.0    steps: 296  evaluation reward: 214.95\n",
      "episode: 233   score: 210.0  epsilon: 1.0    steps: 656  evaluation reward: 213.7\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 559: Policy loss: -0.372973. Value loss: 0.344624. Entropy: 0.310056.\n",
      "Iteration 560: Policy loss: -0.397302. Value loss: 0.289312. Entropy: 0.308238.\n",
      "Iteration 561: Policy loss: -0.420058. Value loss: 0.241504. Entropy: 0.309563.\n",
      "episode: 234   score: 385.0  epsilon: 1.0    steps: 984  evaluation reward: 216.2\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 562: Policy loss: 0.093728. Value loss: 0.073463. Entropy: 0.310469.\n",
      "Iteration 563: Policy loss: 0.090132. Value loss: 0.037272. Entropy: 0.309141.\n",
      "Iteration 564: Policy loss: 0.088370. Value loss: 0.028270. Entropy: 0.308961.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 565: Policy loss: 0.302449. Value loss: 0.156621. Entropy: 0.309753.\n",
      "Iteration 566: Policy loss: 0.289561. Value loss: 0.073649. Entropy: 0.310690.\n",
      "Iteration 567: Policy loss: 0.291201. Value loss: 0.058478. Entropy: 0.310168.\n",
      "episode: 235   score: 135.0  epsilon: 1.0    steps: 352  evaluation reward: 216.0\n",
      "episode: 236   score: 75.0  epsilon: 1.0    steps: 776  evaluation reward: 216.25\n",
      "episode: 237   score: 180.0  epsilon: 1.0    steps: 976  evaluation reward: 215.95\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 568: Policy loss: 0.145709. Value loss: 0.173285. Entropy: 0.310086.\n",
      "Iteration 569: Policy loss: 0.146622. Value loss: 0.087787. Entropy: 0.308501.\n",
      "Iteration 570: Policy loss: 0.141347. Value loss: 0.068547. Entropy: 0.307480.\n",
      "episode: 238   score: 245.0  epsilon: 1.0    steps: 720  evaluation reward: 216.85\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 571: Policy loss: -0.090276. Value loss: 0.104535. Entropy: 0.314939.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 572: Policy loss: -0.085634. Value loss: 0.059892. Entropy: 0.315327.\n",
      "Iteration 573: Policy loss: -0.088751. Value loss: 0.051303. Entropy: 0.314566.\n",
      "episode: 239   score: 180.0  epsilon: 1.0    steps: 136  evaluation reward: 216.55\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 574: Policy loss: 0.064143. Value loss: 0.093035. Entropy: 0.310301.\n",
      "Iteration 575: Policy loss: 0.060098. Value loss: 0.053096. Entropy: 0.308524.\n",
      "Iteration 576: Policy loss: 0.058230. Value loss: 0.039798. Entropy: 0.310218.\n",
      "episode: 240   score: 180.0  epsilon: 1.0    steps: 968  evaluation reward: 215.95\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 577: Policy loss: -0.069184. Value loss: 0.090246. Entropy: 0.307852.\n",
      "Iteration 578: Policy loss: -0.069222. Value loss: 0.049696. Entropy: 0.309294.\n",
      "Iteration 579: Policy loss: -0.074391. Value loss: 0.041001. Entropy: 0.307432.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 580: Policy loss: -0.006535. Value loss: 0.029273. Entropy: 0.311442.\n",
      "Iteration 581: Policy loss: -0.008780. Value loss: 0.017317. Entropy: 0.308747.\n",
      "Iteration 582: Policy loss: -0.007696. Value loss: 0.015392. Entropy: 0.308692.\n",
      "episode: 241   score: 210.0  epsilon: 1.0    steps: 704  evaluation reward: 217.0\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 583: Policy loss: -0.146698. Value loss: 0.447577. Entropy: 0.310898.\n",
      "Iteration 584: Policy loss: -0.162489. Value loss: 0.339077. Entropy: 0.309632.\n",
      "Iteration 585: Policy loss: -0.167834. Value loss: 0.282386. Entropy: 0.311163.\n",
      "episode: 242   score: 135.0  epsilon: 1.0    steps: 488  evaluation reward: 216.55\n",
      "episode: 243   score: 365.0  epsilon: 1.0    steps: 720  evaluation reward: 219.45\n",
      "episode: 244   score: 155.0  epsilon: 1.0    steps: 976  evaluation reward: 219.45\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 586: Policy loss: 0.312546. Value loss: 0.099381. Entropy: 0.307635.\n",
      "Iteration 587: Policy loss: 0.310127. Value loss: 0.050495. Entropy: 0.309204.\n",
      "Iteration 588: Policy loss: 0.307380. Value loss: 0.041845. Entropy: 0.309878.\n",
      "episode: 245   score: 380.0  epsilon: 1.0    steps: 136  evaluation reward: 221.7\n",
      "episode: 246   score: 155.0  epsilon: 1.0    steps: 904  evaluation reward: 221.15\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 589: Policy loss: 0.060912. Value loss: 0.056933. Entropy: 0.310609.\n",
      "Iteration 590: Policy loss: 0.057236. Value loss: 0.033383. Entropy: 0.310407.\n",
      "Iteration 591: Policy loss: 0.054524. Value loss: 0.030528. Entropy: 0.311142.\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 592: Policy loss: 0.109255. Value loss: 0.083397. Entropy: 0.309465.\n",
      "Iteration 593: Policy loss: 0.107538. Value loss: 0.046168. Entropy: 0.307304.\n",
      "Iteration 594: Policy loss: 0.109366. Value loss: 0.042416. Entropy: 0.307261.\n",
      "episode: 247   score: 105.0  epsilon: 1.0    steps: 840  evaluation reward: 219.65\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 595: Policy loss: -0.177261. Value loss: 0.135812. Entropy: 0.307235.\n",
      "Iteration 596: Policy loss: -0.173711. Value loss: 0.079050. Entropy: 0.306993.\n",
      "Iteration 597: Policy loss: -0.181426. Value loss: 0.055748. Entropy: 0.305397.\n",
      "episode: 248   score: 180.0  epsilon: 1.0    steps: 176  evaluation reward: 217.55\n",
      "episode: 249   score: 75.0  epsilon: 1.0    steps: 736  evaluation reward: 214.2\n",
      "Training network. lr: 0.000246. clip: 0.098313\n",
      "Iteration 598: Policy loss: 0.028950. Value loss: 0.059255. Entropy: 0.311579.\n",
      "Iteration 599: Policy loss: 0.024959. Value loss: 0.030521. Entropy: 0.312699.\n",
      "Iteration 600: Policy loss: 0.026268. Value loss: 0.027437. Entropy: 0.312301.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 601: Policy loss: 0.050482. Value loss: 0.100901. Entropy: 0.315147.\n",
      "Iteration 602: Policy loss: 0.049859. Value loss: 0.049694. Entropy: 0.315158.\n",
      "Iteration 603: Policy loss: 0.048705. Value loss: 0.035884. Entropy: 0.315014.\n",
      "episode: 250   score: 345.0  epsilon: 1.0    steps: 576  evaluation reward: 216.25\n",
      "now time :  2019-09-05 14:52:06.171902\n",
      "episode: 251   score: 180.0  epsilon: 1.0    steps: 952  evaluation reward: 216.95\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 604: Policy loss: -0.358423. Value loss: 0.351751. Entropy: 0.312374.\n",
      "Iteration 605: Policy loss: -0.388558. Value loss: 0.254185. Entropy: 0.311789.\n",
      "Iteration 606: Policy loss: -0.380055. Value loss: 0.179465. Entropy: 0.311751.\n",
      "episode: 252   score: 180.0  epsilon: 1.0    steps: 168  evaluation reward: 214.65\n",
      "episode: 253   score: 180.0  epsilon: 1.0    steps: 480  evaluation reward: 214.35\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 607: Policy loss: -0.022563. Value loss: 0.066810. Entropy: 0.308989.\n",
      "Iteration 608: Policy loss: -0.028003. Value loss: 0.039769. Entropy: 0.311010.\n",
      "Iteration 609: Policy loss: -0.025723. Value loss: 0.032792. Entropy: 0.311839.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 610: Policy loss: 0.029972. Value loss: 0.114865. Entropy: 0.315780.\n",
      "Iteration 611: Policy loss: 0.025654. Value loss: 0.056262. Entropy: 0.314084.\n",
      "Iteration 612: Policy loss: 0.027175. Value loss: 0.045079. Entropy: 0.314266.\n",
      "episode: 254   score: 180.0  epsilon: 1.0    steps: 80  evaluation reward: 214.6\n",
      "episode: 255   score: 425.0  epsilon: 1.0    steps: 264  evaluation reward: 214.05\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 613: Policy loss: -0.021201. Value loss: 0.076558. Entropy: 0.309795.\n",
      "Iteration 614: Policy loss: -0.018717. Value loss: 0.038430. Entropy: 0.308836.\n",
      "Iteration 615: Policy loss: -0.016785. Value loss: 0.029504. Entropy: 0.310114.\n",
      "episode: 256   score: 260.0  epsilon: 1.0    steps: 488  evaluation reward: 213.9\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 616: Policy loss: -0.302250. Value loss: 0.346453. Entropy: 0.314383.\n",
      "Iteration 617: Policy loss: -0.286273. Value loss: 0.227577. Entropy: 0.314555.\n",
      "Iteration 618: Policy loss: -0.297781. Value loss: 0.179179. Entropy: 0.314227.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 619: Policy loss: -0.022339. Value loss: 0.046489. Entropy: 0.310434.\n",
      "Iteration 620: Policy loss: -0.023007. Value loss: 0.018079. Entropy: 0.309070.\n",
      "Iteration 621: Policy loss: -0.025838. Value loss: 0.013456. Entropy: 0.307876.\n",
      "episode: 257   score: 465.0  epsilon: 1.0    steps: 448  evaluation reward: 217.0\n",
      "episode: 258   score: 155.0  epsilon: 1.0    steps: 864  evaluation reward: 215.9\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 622: Policy loss: 0.253312. Value loss: 0.106052. Entropy: 0.313333.\n",
      "Iteration 623: Policy loss: 0.260284. Value loss: 0.062418. Entropy: 0.313652.\n",
      "Iteration 624: Policy loss: 0.259379. Value loss: 0.045652. Entropy: 0.314380.\n",
      "episode: 259   score: 135.0  epsilon: 1.0    steps: 448  evaluation reward: 214.55\n",
      "episode: 260   score: 210.0  epsilon: 1.0    steps: 592  evaluation reward: 212.85\n",
      "episode: 261   score: 210.0  epsilon: 1.0    steps: 808  evaluation reward: 210.55\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 625: Policy loss: 0.284761. Value loss: 0.165760. Entropy: 0.316352.\n",
      "Iteration 626: Policy loss: 0.289743. Value loss: 0.080834. Entropy: 0.317136.\n",
      "Iteration 627: Policy loss: 0.291569. Value loss: 0.052364. Entropy: 0.316424.\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 628: Policy loss: 0.114636. Value loss: 0.090234. Entropy: 0.319580.\n",
      "Iteration 629: Policy loss: 0.112580. Value loss: 0.060120. Entropy: 0.319206.\n",
      "Iteration 630: Policy loss: 0.112495. Value loss: 0.053513. Entropy: 0.318701.\n",
      "episode: 262   score: 180.0  epsilon: 1.0    steps: 664  evaluation reward: 210.25\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 631: Policy loss: 0.051963. Value loss: 0.189259. Entropy: 0.317467.\n",
      "Iteration 632: Policy loss: 0.034086. Value loss: 0.145907. Entropy: 0.317609.\n",
      "Iteration 633: Policy loss: 0.036577. Value loss: 0.116414. Entropy: 0.316607.\n",
      "episode: 263   score: 240.0  epsilon: 1.0    steps: 744  evaluation reward: 210.85\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 634: Policy loss: 0.191771. Value loss: 0.051632. Entropy: 0.317511.\n",
      "Iteration 635: Policy loss: 0.189940. Value loss: 0.024710. Entropy: 0.316498.\n",
      "Iteration 636: Policy loss: 0.187364. Value loss: 0.020971. Entropy: 0.315416.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 264   score: 460.0  epsilon: 1.0    steps: 1000  evaluation reward: 213.35\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 637: Policy loss: 0.435789. Value loss: 0.158224. Entropy: 0.316739.\n",
      "Iteration 638: Policy loss: 0.423871. Value loss: 0.075184. Entropy: 0.316123.\n",
      "Iteration 639: Policy loss: 0.421722. Value loss: 0.057418. Entropy: 0.314886.\n",
      "episode: 265   score: 140.0  epsilon: 1.0    steps: 856  evaluation reward: 212.95\n",
      "episode: 266   score: 110.0  epsilon: 1.0    steps: 896  evaluation reward: 211.95\n",
      "episode: 267   score: 210.0  epsilon: 1.0    steps: 904  evaluation reward: 211.95\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 640: Policy loss: 0.518990. Value loss: 0.193227. Entropy: 0.312773.\n",
      "Iteration 641: Policy loss: 0.496688. Value loss: 0.088088. Entropy: 0.311492.\n",
      "Iteration 642: Policy loss: 0.501727. Value loss: 0.071791. Entropy: 0.312908.\n",
      "episode: 268   score: 185.0  epsilon: 1.0    steps: 576  evaluation reward: 212.4\n",
      "episode: 269   score: 155.0  epsilon: 1.0    steps: 1016  evaluation reward: 211.85\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 643: Policy loss: 0.150590. Value loss: 0.131305. Entropy: 0.312325.\n",
      "Iteration 644: Policy loss: 0.147663. Value loss: 0.059179. Entropy: 0.311752.\n",
      "Iteration 645: Policy loss: 0.147519. Value loss: 0.040486. Entropy: 0.312262.\n",
      "episode: 270   score: 110.0  epsilon: 1.0    steps: 896  evaluation reward: 210.3\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 646: Policy loss: 0.337800. Value loss: 0.124544. Entropy: 0.312449.\n",
      "Iteration 647: Policy loss: 0.336452. Value loss: 0.063653. Entropy: 0.313718.\n",
      "Iteration 648: Policy loss: 0.333983. Value loss: 0.048805. Entropy: 0.312799.\n",
      "episode: 271   score: 110.0  epsilon: 1.0    steps: 1008  evaluation reward: 209.3\n",
      "Training network. lr: 0.000245. clip: 0.098166\n",
      "Iteration 649: Policy loss: 0.110689. Value loss: 0.069108. Entropy: 0.313624.\n",
      "Iteration 650: Policy loss: 0.110161. Value loss: 0.040513. Entropy: 0.314060.\n",
      "Iteration 651: Policy loss: 0.109716. Value loss: 0.025571. Entropy: 0.314210.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 652: Policy loss: 0.152013. Value loss: 0.054907. Entropy: 0.311001.\n",
      "Iteration 653: Policy loss: 0.151288. Value loss: 0.030926. Entropy: 0.310342.\n",
      "Iteration 654: Policy loss: 0.148866. Value loss: 0.024641. Entropy: 0.311125.\n",
      "episode: 272   score: 105.0  epsilon: 1.0    steps: 632  evaluation reward: 208.55\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 655: Policy loss: 0.206309. Value loss: 0.102652. Entropy: 0.305410.\n",
      "Iteration 656: Policy loss: 0.203114. Value loss: 0.049109. Entropy: 0.305100.\n",
      "Iteration 657: Policy loss: 0.204749. Value loss: 0.044352. Entropy: 0.304159.\n",
      "episode: 273   score: 105.0  epsilon: 1.0    steps: 120  evaluation reward: 208.4\n",
      "episode: 274   score: 210.0  epsilon: 1.0    steps: 120  evaluation reward: 208.7\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 658: Policy loss: 0.140216. Value loss: 0.038654. Entropy: 0.306543.\n",
      "Iteration 659: Policy loss: 0.142516. Value loss: 0.020565. Entropy: 0.305028.\n",
      "Iteration 660: Policy loss: 0.142198. Value loss: 0.015280. Entropy: 0.305192.\n",
      "episode: 275   score: 80.0  epsilon: 1.0    steps: 968  evaluation reward: 205.7\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 661: Policy loss: 0.280974. Value loss: 0.107166. Entropy: 0.307681.\n",
      "Iteration 662: Policy loss: 0.281444. Value loss: 0.057709. Entropy: 0.307909.\n",
      "Iteration 663: Policy loss: 0.278613. Value loss: 0.044441. Entropy: 0.307701.\n",
      "episode: 276   score: 180.0  epsilon: 1.0    steps: 208  evaluation reward: 205.4\n",
      "episode: 277   score: 180.0  epsilon: 1.0    steps: 376  evaluation reward: 205.1\n",
      "episode: 278   score: 180.0  epsilon: 1.0    steps: 1016  evaluation reward: 204.3\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 664: Policy loss: 0.035911. Value loss: 0.068046. Entropy: 0.314735.\n",
      "Iteration 665: Policy loss: 0.035062. Value loss: 0.036546. Entropy: 0.314281.\n",
      "Iteration 666: Policy loss: 0.033468. Value loss: 0.029629. Entropy: 0.313924.\n",
      "episode: 279   score: 50.0  epsilon: 1.0    steps: 424  evaluation reward: 203.0\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 667: Policy loss: 0.036431. Value loss: 0.074830. Entropy: 0.306700.\n",
      "Iteration 668: Policy loss: 0.033653. Value loss: 0.038237. Entropy: 0.306593.\n",
      "Iteration 669: Policy loss: 0.028537. Value loss: 0.025872. Entropy: 0.306437.\n",
      "episode: 280   score: 295.0  epsilon: 1.0    steps: 832  evaluation reward: 203.85\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 670: Policy loss: -0.439933. Value loss: 0.340494. Entropy: 0.306195.\n",
      "Iteration 671: Policy loss: -0.419004. Value loss: 0.217896. Entropy: 0.306091.\n",
      "Iteration 672: Policy loss: -0.447926. Value loss: 0.175596. Entropy: 0.307458.\n",
      "episode: 281   score: 110.0  epsilon: 1.0    steps: 88  evaluation reward: 203.15\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 673: Policy loss: -0.398545. Value loss: 0.281404. Entropy: 0.310916.\n",
      "Iteration 674: Policy loss: -0.368651. Value loss: 0.144828. Entropy: 0.310530.\n",
      "Iteration 675: Policy loss: -0.381623. Value loss: 0.131230. Entropy: 0.311368.\n",
      "episode: 282   score: 380.0  epsilon: 1.0    steps: 568  evaluation reward: 205.9\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 676: Policy loss: -0.022656. Value loss: 0.203045. Entropy: 0.307424.\n",
      "Iteration 677: Policy loss: -0.029205. Value loss: 0.118154. Entropy: 0.307954.\n",
      "Iteration 678: Policy loss: -0.030853. Value loss: 0.084285. Entropy: 0.308360.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 679: Policy loss: 0.065905. Value loss: 0.139691. Entropy: 0.307815.\n",
      "Iteration 680: Policy loss: 0.063393. Value loss: 0.079170. Entropy: 0.308719.\n",
      "Iteration 681: Policy loss: 0.064767. Value loss: 0.059348. Entropy: 0.308553.\n",
      "episode: 283   score: 180.0  epsilon: 1.0    steps: 376  evaluation reward: 206.15\n",
      "episode: 284   score: 165.0  epsilon: 1.0    steps: 488  evaluation reward: 205.7\n",
      "episode: 285   score: 105.0  epsilon: 1.0    steps: 840  evaluation reward: 205.15\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 682: Policy loss: 0.068612. Value loss: 0.183132. Entropy: 0.308692.\n",
      "Iteration 683: Policy loss: 0.057249. Value loss: 0.086186. Entropy: 0.309134.\n",
      "Iteration 684: Policy loss: 0.060997. Value loss: 0.061037. Entropy: 0.308801.\n",
      "episode: 286   score: 225.0  epsilon: 1.0    steps: 288  evaluation reward: 206.65\n",
      "episode: 287   score: 515.0  epsilon: 1.0    steps: 616  evaluation reward: 209.7\n",
      "episode: 288   score: 210.0  epsilon: 1.0    steps: 968  evaluation reward: 207.7\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 685: Policy loss: 0.079825. Value loss: 0.159288. Entropy: 0.310902.\n",
      "Iteration 686: Policy loss: 0.075513. Value loss: 0.090358. Entropy: 0.311165.\n",
      "Iteration 687: Policy loss: 0.077119. Value loss: 0.066253. Entropy: 0.311410.\n",
      "episode: 289   score: 135.0  epsilon: 1.0    steps: 296  evaluation reward: 206.45\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 688: Policy loss: 0.008038. Value loss: 0.104361. Entropy: 0.308772.\n",
      "Iteration 689: Policy loss: 0.003829. Value loss: 0.060047. Entropy: 0.308944.\n",
      "Iteration 690: Policy loss: 0.001794. Value loss: 0.043492. Entropy: 0.308578.\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 691: Policy loss: 0.158749. Value loss: 0.137310. Entropy: 0.311482.\n",
      "Iteration 692: Policy loss: 0.157234. Value loss: 0.076006. Entropy: 0.310526.\n",
      "Iteration 693: Policy loss: 0.152655. Value loss: 0.057760. Entropy: 0.310729.\n",
      "episode: 290   score: 120.0  epsilon: 1.0    steps: 272  evaluation reward: 205.85\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 694: Policy loss: 0.202034. Value loss: 0.085677. Entropy: 0.309062.\n",
      "Iteration 695: Policy loss: 0.199532. Value loss: 0.036024. Entropy: 0.307700.\n",
      "Iteration 696: Policy loss: 0.201783. Value loss: 0.029018. Entropy: 0.308229.\n",
      "episode: 291   score: 75.0  epsilon: 1.0    steps: 192  evaluation reward: 199.45\n",
      "episode: 292   score: 105.0  epsilon: 1.0    steps: 280  evaluation reward: 198.7\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 697: Policy loss: -0.315074. Value loss: 0.333033. Entropy: 0.306613.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 698: Policy loss: -0.334175. Value loss: 0.291601. Entropy: 0.305463.\n",
      "Iteration 699: Policy loss: -0.306716. Value loss: 0.235482. Entropy: 0.306462.\n",
      "episode: 293   score: 75.0  epsilon: 1.0    steps: 504  evaluation reward: 196.75\n",
      "episode: 294   score: 210.0  epsilon: 1.0    steps: 824  evaluation reward: 197.05\n",
      "Training network. lr: 0.000245. clip: 0.098009\n",
      "Iteration 700: Policy loss: -0.019522. Value loss: 0.122763. Entropy: 0.310567.\n",
      "Iteration 701: Policy loss: -0.018095. Value loss: 0.072068. Entropy: 0.312894.\n",
      "Iteration 702: Policy loss: -0.021566. Value loss: 0.055665. Entropy: 0.311574.\n",
      "episode: 295   score: 410.0  epsilon: 1.0    steps: 432  evaluation reward: 199.6\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 703: Policy loss: -0.010226. Value loss: 0.068783. Entropy: 0.310512.\n",
      "Iteration 704: Policy loss: -0.013442. Value loss: 0.046234. Entropy: 0.310803.\n",
      "Iteration 705: Policy loss: -0.017030. Value loss: 0.032986. Entropy: 0.310235.\n",
      "episode: 296   score: 210.0  epsilon: 1.0    steps: 352  evaluation reward: 198.0\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 706: Policy loss: 0.146966. Value loss: 0.075438. Entropy: 0.309742.\n",
      "Iteration 707: Policy loss: 0.148633. Value loss: 0.034407. Entropy: 0.309279.\n",
      "Iteration 708: Policy loss: 0.147591. Value loss: 0.025632. Entropy: 0.308812.\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 709: Policy loss: -0.023656. Value loss: 0.073135. Entropy: 0.308003.\n",
      "Iteration 710: Policy loss: -0.026344. Value loss: 0.030836. Entropy: 0.308162.\n",
      "Iteration 711: Policy loss: -0.034622. Value loss: 0.025874. Entropy: 0.308173.\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 712: Policy loss: 0.072296. Value loss: 0.078502. Entropy: 0.312899.\n",
      "Iteration 713: Policy loss: 0.077684. Value loss: 0.036892. Entropy: 0.312627.\n",
      "Iteration 714: Policy loss: 0.071564. Value loss: 0.033426. Entropy: 0.312150.\n",
      "episode: 297   score: 380.0  epsilon: 1.0    steps: 728  evaluation reward: 200.0\n",
      "episode: 298   score: 210.0  epsilon: 1.0    steps: 1016  evaluation reward: 200.0\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 715: Policy loss: 0.029511. Value loss: 0.052885. Entropy: 0.311300.\n",
      "Iteration 716: Policy loss: 0.028061. Value loss: 0.028873. Entropy: 0.311502.\n",
      "Iteration 717: Policy loss: 0.022590. Value loss: 0.018180. Entropy: 0.311188.\n",
      "episode: 299   score: 240.0  epsilon: 1.0    steps: 56  evaluation reward: 200.85\n",
      "episode: 300   score: 105.0  epsilon: 1.0    steps: 416  evaluation reward: 200.35\n",
      "now time :  2019-09-05 14:59:14.134474\n",
      "episode: 301   score: 290.0  epsilon: 1.0    steps: 936  evaluation reward: 201.15\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 718: Policy loss: -0.105814. Value loss: 0.085349. Entropy: 0.312615.\n",
      "Iteration 719: Policy loss: -0.101027. Value loss: 0.048252. Entropy: 0.312853.\n",
      "Iteration 720: Policy loss: -0.100133. Value loss: 0.034687. Entropy: 0.312493.\n",
      "episode: 302   score: 210.0  epsilon: 1.0    steps: 104  evaluation reward: 201.45\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 721: Policy loss: 0.132987. Value loss: 0.103454. Entropy: 0.310046.\n",
      "Iteration 722: Policy loss: 0.140193. Value loss: 0.053125. Entropy: 0.310852.\n",
      "Iteration 723: Policy loss: 0.138656. Value loss: 0.037282. Entropy: 0.309714.\n",
      "episode: 303   score: 240.0  epsilon: 1.0    steps: 472  evaluation reward: 201.75\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 724: Policy loss: 0.198321. Value loss: 0.058405. Entropy: 0.310045.\n",
      "Iteration 725: Policy loss: 0.195142. Value loss: 0.031509. Entropy: 0.310039.\n",
      "Iteration 726: Policy loss: 0.199028. Value loss: 0.024675. Entropy: 0.309912.\n",
      "episode: 304   score: 50.0  epsilon: 1.0    steps: 200  evaluation reward: 201.2\n",
      "episode: 305   score: 240.0  epsilon: 1.0    steps: 920  evaluation reward: 201.0\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 727: Policy loss: 0.156540. Value loss: 0.025450. Entropy: 0.310125.\n",
      "Iteration 728: Policy loss: 0.154091. Value loss: 0.012067. Entropy: 0.308713.\n",
      "Iteration 729: Policy loss: 0.154405. Value loss: 0.007981. Entropy: 0.308730.\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 730: Policy loss: -0.177387. Value loss: 0.074339. Entropy: 0.306963.\n",
      "Iteration 731: Policy loss: -0.179714. Value loss: 0.042416. Entropy: 0.307051.\n",
      "Iteration 732: Policy loss: -0.186992. Value loss: 0.031924. Entropy: 0.306806.\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 733: Policy loss: -0.142177. Value loss: 0.298914. Entropy: 0.305923.\n",
      "Iteration 734: Policy loss: -0.157297. Value loss: 0.214235. Entropy: 0.304027.\n",
      "Iteration 735: Policy loss: -0.130646. Value loss: 0.135769. Entropy: 0.302886.\n",
      "episode: 306   score: 180.0  epsilon: 1.0    steps: 24  evaluation reward: 201.45\n",
      "episode: 307   score: 320.0  epsilon: 1.0    steps: 144  evaluation reward: 203.1\n",
      "episode: 308   score: 120.0  epsilon: 1.0    steps: 440  evaluation reward: 202.2\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 736: Policy loss: -0.201767. Value loss: 0.344750. Entropy: 0.307279.\n",
      "Iteration 737: Policy loss: -0.209172. Value loss: 0.195208. Entropy: 0.307217.\n",
      "Iteration 738: Policy loss: -0.213998. Value loss: 0.152395. Entropy: 0.306873.\n",
      "episode: 309   score: 120.0  epsilon: 1.0    steps: 128  evaluation reward: 202.35\n",
      "episode: 310   score: 110.0  epsilon: 1.0    steps: 376  evaluation reward: 201.9\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 739: Policy loss: 0.075759. Value loss: 0.089252. Entropy: 0.303311.\n",
      "Iteration 740: Policy loss: 0.076425. Value loss: 0.045807. Entropy: 0.302816.\n",
      "Iteration 741: Policy loss: 0.077707. Value loss: 0.035032. Entropy: 0.302005.\n",
      "episode: 311   score: 240.0  epsilon: 1.0    steps: 304  evaluation reward: 201.9\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 742: Policy loss: 0.077389. Value loss: 0.086106. Entropy: 0.307176.\n",
      "Iteration 743: Policy loss: 0.080666. Value loss: 0.048574. Entropy: 0.307001.\n",
      "Iteration 744: Policy loss: 0.073175. Value loss: 0.040863. Entropy: 0.307142.\n",
      "episode: 312   score: 120.0  epsilon: 1.0    steps: 352  evaluation reward: 201.75\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 745: Policy loss: 0.037255. Value loss: 0.054806. Entropy: 0.308118.\n",
      "Iteration 746: Policy loss: 0.031374. Value loss: 0.027767. Entropy: 0.308811.\n",
      "Iteration 747: Policy loss: 0.036206. Value loss: 0.023513. Entropy: 0.307752.\n",
      "episode: 313   score: 80.0  epsilon: 1.0    steps: 200  evaluation reward: 200.15\n",
      "Training network. lr: 0.000245. clip: 0.097853\n",
      "Iteration 748: Policy loss: 0.002325. Value loss: 0.062667. Entropy: 0.306134.\n",
      "Iteration 749: Policy loss: -0.002036. Value loss: 0.030153. Entropy: 0.305987.\n",
      "Iteration 750: Policy loss: -0.001794. Value loss: 0.028495. Entropy: 0.304887.\n",
      "episode: 314   score: 460.0  epsilon: 1.0    steps: 592  evaluation reward: 204.25\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 751: Policy loss: 0.017849. Value loss: 0.091019. Entropy: 0.309593.\n",
      "Iteration 752: Policy loss: 0.016563. Value loss: 0.053145. Entropy: 0.309102.\n",
      "Iteration 753: Policy loss: 0.018959. Value loss: 0.040515. Entropy: 0.309026.\n",
      "episode: 315   score: 155.0  epsilon: 1.0    steps: 296  evaluation reward: 204.7\n",
      "episode: 316   score: 210.0  epsilon: 1.0    steps: 744  evaluation reward: 205.0\n",
      "episode: 317   score: 180.0  epsilon: 1.0    steps: 752  evaluation reward: 203.65\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 754: Policy loss: 0.137568. Value loss: 0.064211. Entropy: 0.310702.\n",
      "Iteration 755: Policy loss: 0.133976. Value loss: 0.029241. Entropy: 0.309281.\n",
      "Iteration 756: Policy loss: 0.133736. Value loss: 0.020146. Entropy: 0.309414.\n",
      "episode: 318   score: 135.0  epsilon: 1.0    steps: 600  evaluation reward: 202.9\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 757: Policy loss: 0.117929. Value loss: 0.063355. Entropy: 0.311197.\n",
      "Iteration 758: Policy loss: 0.113591. Value loss: 0.037413. Entropy: 0.311797.\n",
      "Iteration 759: Policy loss: 0.113233. Value loss: 0.030633. Entropy: 0.311298.\n",
      "episode: 319   score: 120.0  epsilon: 1.0    steps: 768  evaluation reward: 202.3\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 760: Policy loss: 0.097103. Value loss: 0.088011. Entropy: 0.308235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 761: Policy loss: 0.099468. Value loss: 0.050773. Entropy: 0.307322.\n",
      "Iteration 762: Policy loss: 0.092795. Value loss: 0.043434. Entropy: 0.307186.\n",
      "episode: 320   score: 185.0  epsilon: 1.0    steps: 480  evaluation reward: 202.35\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 763: Policy loss: -0.101951. Value loss: 0.038457. Entropy: 0.311380.\n",
      "Iteration 764: Policy loss: -0.110501. Value loss: 0.020821. Entropy: 0.312606.\n",
      "Iteration 765: Policy loss: -0.109399. Value loss: 0.015768. Entropy: 0.312464.\n",
      "episode: 321   score: 210.0  epsilon: 1.0    steps: 880  evaluation reward: 201.85\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 766: Policy loss: -0.167401. Value loss: 0.085403. Entropy: 0.307891.\n",
      "Iteration 767: Policy loss: -0.174135. Value loss: 0.048966. Entropy: 0.306655.\n",
      "Iteration 768: Policy loss: -0.173222. Value loss: 0.038282. Entropy: 0.305465.\n",
      "episode: 322   score: 120.0  epsilon: 1.0    steps: 680  evaluation reward: 198.95\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 769: Policy loss: -0.314629. Value loss: 0.365419. Entropy: 0.312523.\n",
      "Iteration 770: Policy loss: -0.295934. Value loss: 0.256292. Entropy: 0.313404.\n",
      "Iteration 771: Policy loss: -0.302730. Value loss: 0.221025. Entropy: 0.313024.\n",
      "episode: 323   score: 410.0  epsilon: 1.0    steps: 80  evaluation reward: 201.85\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 772: Policy loss: -0.181082. Value loss: 0.279909. Entropy: 0.314328.\n",
      "Iteration 773: Policy loss: -0.159875. Value loss: 0.167645. Entropy: 0.313961.\n",
      "Iteration 774: Policy loss: -0.185909. Value loss: 0.110358. Entropy: 0.314849.\n",
      "episode: 324   score: 210.0  epsilon: 1.0    steps: 496  evaluation reward: 200.4\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 775: Policy loss: 0.225080. Value loss: 0.159007. Entropy: 0.319642.\n",
      "Iteration 776: Policy loss: 0.230552. Value loss: 0.080437. Entropy: 0.318293.\n",
      "Iteration 777: Policy loss: 0.207137. Value loss: 0.053555. Entropy: 0.319241.\n",
      "episode: 325   score: 245.0  epsilon: 1.0    steps: 56  evaluation reward: 201.3\n",
      "episode: 326   score: 410.0  epsilon: 1.0    steps: 256  evaluation reward: 204.35\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 778: Policy loss: 0.098848. Value loss: 0.048465. Entropy: 0.319647.\n",
      "Iteration 779: Policy loss: 0.100762. Value loss: 0.023400. Entropy: 0.319925.\n",
      "Iteration 780: Policy loss: 0.098346. Value loss: 0.019166. Entropy: 0.319367.\n",
      "episode: 327   score: 210.0  epsilon: 1.0    steps: 344  evaluation reward: 203.6\n",
      "episode: 328   score: 155.0  epsilon: 1.0    steps: 640  evaluation reward: 203.05\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 781: Policy loss: -0.146532. Value loss: 0.099356. Entropy: 0.316095.\n",
      "Iteration 782: Policy loss: -0.143759. Value loss: 0.064952. Entropy: 0.316022.\n",
      "Iteration 783: Policy loss: -0.146185. Value loss: 0.049535. Entropy: 0.315628.\n",
      "episode: 329   score: 105.0  epsilon: 1.0    steps: 40  evaluation reward: 202.9\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 784: Policy loss: -0.016243. Value loss: 0.084021. Entropy: 0.313984.\n",
      "Iteration 785: Policy loss: -0.022750. Value loss: 0.042864. Entropy: 0.313714.\n",
      "Iteration 786: Policy loss: -0.023269. Value loss: 0.038182. Entropy: 0.314164.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 787: Policy loss: -0.066485. Value loss: 0.061977. Entropy: 0.319010.\n",
      "Iteration 788: Policy loss: -0.072137. Value loss: 0.030974. Entropy: 0.319907.\n",
      "Iteration 789: Policy loss: -0.079988. Value loss: 0.025136. Entropy: 0.318370.\n",
      "episode: 330   score: 210.0  epsilon: 1.0    steps: 312  evaluation reward: 202.9\n",
      "episode: 331   score: 210.0  epsilon: 1.0    steps: 528  evaluation reward: 201.7\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 790: Policy loss: 0.126004. Value loss: 0.056431. Entropy: 0.322610.\n",
      "Iteration 791: Policy loss: 0.127665. Value loss: 0.027039. Entropy: 0.320376.\n",
      "Iteration 792: Policy loss: 0.123560. Value loss: 0.022013. Entropy: 0.321119.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 793: Policy loss: 0.056475. Value loss: 0.065521. Entropy: 0.326156.\n",
      "Iteration 794: Policy loss: 0.054490. Value loss: 0.040673. Entropy: 0.327615.\n",
      "Iteration 795: Policy loss: 0.052843. Value loss: 0.033258. Entropy: 0.326267.\n",
      "episode: 332   score: 155.0  epsilon: 1.0    steps: 144  evaluation reward: 201.4\n",
      "episode: 333   score: 165.0  epsilon: 1.0    steps: 144  evaluation reward: 200.95\n",
      "episode: 334   score: 210.0  epsilon: 1.0    steps: 776  evaluation reward: 199.2\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 796: Policy loss: -0.014196. Value loss: 0.071320. Entropy: 0.323062.\n",
      "Iteration 797: Policy loss: -0.014631. Value loss: 0.045390. Entropy: 0.322479.\n",
      "Iteration 798: Policy loss: -0.013200. Value loss: 0.030177. Entropy: 0.321864.\n",
      "Training network. lr: 0.000244. clip: 0.097705\n",
      "Iteration 799: Policy loss: -0.108040. Value loss: 0.057907. Entropy: 0.325406.\n",
      "Iteration 800: Policy loss: -0.109741. Value loss: 0.031419. Entropy: 0.326014.\n",
      "Iteration 801: Policy loss: -0.110786. Value loss: 0.027807. Entropy: 0.326459.\n",
      "episode: 335   score: 180.0  epsilon: 1.0    steps: 64  evaluation reward: 199.65\n",
      "episode: 336   score: 240.0  epsilon: 1.0    steps: 832  evaluation reward: 201.3\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 802: Policy loss: 0.164654. Value loss: 0.040068. Entropy: 0.317766.\n",
      "Iteration 803: Policy loss: 0.161977. Value loss: 0.016823. Entropy: 0.320449.\n",
      "Iteration 804: Policy loss: 0.161365. Value loss: 0.013993. Entropy: 0.319206.\n",
      "episode: 337   score: 260.0  epsilon: 1.0    steps: 824  evaluation reward: 202.1\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 805: Policy loss: -0.060555. Value loss: 0.119990. Entropy: 0.327434.\n",
      "Iteration 806: Policy loss: -0.062584. Value loss: 0.090174. Entropy: 0.325938.\n",
      "Iteration 807: Policy loss: -0.072412. Value loss: 0.082988. Entropy: 0.325644.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 808: Policy loss: 0.153975. Value loss: 0.048448. Entropy: 0.323039.\n",
      "Iteration 809: Policy loss: 0.150449. Value loss: 0.027213. Entropy: 0.322989.\n",
      "Iteration 810: Policy loss: 0.150532. Value loss: 0.023409. Entropy: 0.321505.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 811: Policy loss: -0.208190. Value loss: 0.369565. Entropy: 0.322968.\n",
      "Iteration 812: Policy loss: -0.201690. Value loss: 0.276374. Entropy: 0.324863.\n",
      "Iteration 813: Policy loss: -0.178461. Value loss: 0.204273. Entropy: 0.325236.\n",
      "episode: 338   score: 240.0  epsilon: 1.0    steps: 32  evaluation reward: 202.05\n",
      "episode: 339   score: 155.0  epsilon: 1.0    steps: 512  evaluation reward: 201.8\n",
      "episode: 340   score: 210.0  epsilon: 1.0    steps: 984  evaluation reward: 202.1\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 814: Policy loss: 0.175365. Value loss: 0.057091. Entropy: 0.306614.\n",
      "Iteration 815: Policy loss: 0.174044. Value loss: 0.030669. Entropy: 0.306816.\n",
      "Iteration 816: Policy loss: 0.169590. Value loss: 0.025275. Entropy: 0.306739.\n",
      "episode: 341   score: 110.0  epsilon: 1.0    steps: 64  evaluation reward: 201.1\n",
      "episode: 342   score: 210.0  epsilon: 1.0    steps: 400  evaluation reward: 201.85\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 817: Policy loss: 0.166357. Value loss: 0.059945. Entropy: 0.321430.\n",
      "Iteration 818: Policy loss: 0.172181. Value loss: 0.033463. Entropy: 0.319870.\n",
      "Iteration 819: Policy loss: 0.169481. Value loss: 0.027532. Entropy: 0.320225.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 820: Policy loss: -0.114240. Value loss: 0.328037. Entropy: 0.313634.\n",
      "Iteration 821: Policy loss: -0.119999. Value loss: 0.227530. Entropy: 0.314421.\n",
      "Iteration 822: Policy loss: -0.083410. Value loss: 0.154844. Entropy: 0.311597.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 823: Policy loss: 0.159965. Value loss: 0.035925. Entropy: 0.308637.\n",
      "Iteration 824: Policy loss: 0.163615. Value loss: 0.017761. Entropy: 0.308753.\n",
      "Iteration 825: Policy loss: 0.162899. Value loss: 0.014941. Entropy: 0.308863.\n",
      "episode: 343   score: 155.0  epsilon: 1.0    steps: 168  evaluation reward: 199.75\n",
      "episode: 344   score: 260.0  epsilon: 1.0    steps: 464  evaluation reward: 200.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 826: Policy loss: 0.043008. Value loss: 0.020128. Entropy: 0.309589.\n",
      "Iteration 827: Policy loss: 0.043385. Value loss: 0.007859. Entropy: 0.309781.\n",
      "Iteration 828: Policy loss: 0.042307. Value loss: 0.007555. Entropy: 0.310032.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 829: Policy loss: -0.030747. Value loss: 0.199828. Entropy: 0.312241.\n",
      "Iteration 830: Policy loss: -0.022786. Value loss: 0.147987. Entropy: 0.313047.\n",
      "Iteration 831: Policy loss: -0.042524. Value loss: 0.157136. Entropy: 0.309871.\n",
      "episode: 345   score: 335.0  epsilon: 1.0    steps: 176  evaluation reward: 200.35\n",
      "episode: 346   score: 155.0  epsilon: 1.0    steps: 880  evaluation reward: 200.35\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 832: Policy loss: -0.006028. Value loss: 0.251443. Entropy: 0.310197.\n",
      "Iteration 833: Policy loss: -0.005790. Value loss: 0.190381. Entropy: 0.309764.\n",
      "Iteration 834: Policy loss: -0.010735. Value loss: 0.165938. Entropy: 0.309058.\n",
      "episode: 347   score: 210.0  epsilon: 1.0    steps: 160  evaluation reward: 201.4\n",
      "episode: 348   score: 190.0  epsilon: 1.0    steps: 296  evaluation reward: 201.5\n",
      "episode: 349   score: 920.0  epsilon: 1.0    steps: 760  evaluation reward: 209.95\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 835: Policy loss: -0.036915. Value loss: 0.054305. Entropy: 0.305641.\n",
      "Iteration 836: Policy loss: -0.037163. Value loss: 0.034247. Entropy: 0.304978.\n",
      "Iteration 837: Policy loss: -0.036598. Value loss: 0.027517. Entropy: 0.305393.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 838: Policy loss: 0.005736. Value loss: 0.062998. Entropy: 0.315575.\n",
      "Iteration 839: Policy loss: 0.005138. Value loss: 0.038727. Entropy: 0.316147.\n",
      "Iteration 840: Policy loss: 0.001167. Value loss: 0.033934. Entropy: 0.314973.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 841: Policy loss: -0.234342. Value loss: 0.313419. Entropy: 0.310878.\n",
      "Iteration 842: Policy loss: -0.263010. Value loss: 0.256543. Entropy: 0.311826.\n",
      "Iteration 843: Policy loss: -0.248853. Value loss: 0.187486. Entropy: 0.310897.\n",
      "episode: 350   score: 360.0  epsilon: 1.0    steps: 256  evaluation reward: 210.1\n",
      "now time :  2019-09-05 15:07:08.521721\n",
      "episode: 351   score: 180.0  epsilon: 1.0    steps: 928  evaluation reward: 210.1\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 844: Policy loss: 0.078613. Value loss: 0.057147. Entropy: 0.311390.\n",
      "Iteration 845: Policy loss: 0.078008. Value loss: 0.021259. Entropy: 0.311864.\n",
      "Iteration 846: Policy loss: 0.074261. Value loss: 0.014466. Entropy: 0.311721.\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 847: Policy loss: 0.120971. Value loss: 0.085441. Entropy: 0.310070.\n",
      "Iteration 848: Policy loss: 0.124891. Value loss: 0.036964. Entropy: 0.309647.\n",
      "Iteration 849: Policy loss: 0.123761. Value loss: 0.025964. Entropy: 0.308951.\n",
      "episode: 352   score: 530.0  epsilon: 1.0    steps: 464  evaluation reward: 213.6\n",
      "Training network. lr: 0.000244. clip: 0.097549\n",
      "Iteration 850: Policy loss: 0.026298. Value loss: 0.109979. Entropy: 0.312403.\n",
      "Iteration 851: Policy loss: 0.031466. Value loss: 0.063083. Entropy: 0.311652.\n",
      "Iteration 852: Policy loss: 0.023205. Value loss: 0.050424. Entropy: 0.312062.\n",
      "episode: 353   score: 210.0  epsilon: 1.0    steps: 88  evaluation reward: 213.9\n",
      "episode: 354   score: 145.0  epsilon: 1.0    steps: 408  evaluation reward: 213.55\n",
      "episode: 355   score: 210.0  epsilon: 1.0    steps: 936  evaluation reward: 211.4\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 853: Policy loss: 0.137195. Value loss: 0.074129. Entropy: 0.312453.\n",
      "Iteration 854: Policy loss: 0.132062. Value loss: 0.034538. Entropy: 0.312978.\n",
      "Iteration 855: Policy loss: 0.131827. Value loss: 0.024658. Entropy: 0.311939.\n",
      "episode: 356   score: 210.0  epsilon: 1.0    steps: 200  evaluation reward: 210.9\n",
      "episode: 357   score: 260.0  epsilon: 1.0    steps: 440  evaluation reward: 208.85\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 856: Policy loss: 0.217011. Value loss: 0.085201. Entropy: 0.314073.\n",
      "Iteration 857: Policy loss: 0.210976. Value loss: 0.050833. Entropy: 0.312835.\n",
      "Iteration 858: Policy loss: 0.205019. Value loss: 0.040947. Entropy: 0.312972.\n",
      "episode: 358   score: 30.0  epsilon: 1.0    steps: 608  evaluation reward: 207.6\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 859: Policy loss: 0.123238. Value loss: 0.073150. Entropy: 0.319224.\n",
      "Iteration 860: Policy loss: 0.128483. Value loss: 0.038888. Entropy: 0.319425.\n",
      "Iteration 861: Policy loss: 0.127078. Value loss: 0.031744. Entropy: 0.319054.\n",
      "episode: 359   score: 180.0  epsilon: 1.0    steps: 808  evaluation reward: 208.05\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 862: Policy loss: 0.089043. Value loss: 0.051721. Entropy: 0.314038.\n",
      "Iteration 863: Policy loss: 0.085851. Value loss: 0.028181. Entropy: 0.313045.\n",
      "Iteration 864: Policy loss: 0.086392. Value loss: 0.023025. Entropy: 0.312551.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 865: Policy loss: -0.066938. Value loss: 0.063166. Entropy: 0.310112.\n",
      "Iteration 866: Policy loss: -0.075005. Value loss: 0.040657. Entropy: 0.310510.\n",
      "Iteration 867: Policy loss: -0.073001. Value loss: 0.030829. Entropy: 0.309822.\n",
      "episode: 360   score: 105.0  epsilon: 1.0    steps: 224  evaluation reward: 207.0\n",
      "episode: 361   score: 265.0  epsilon: 1.0    steps: 376  evaluation reward: 207.55\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 868: Policy loss: 0.018666. Value loss: 0.040679. Entropy: 0.312213.\n",
      "Iteration 869: Policy loss: 0.019115. Value loss: 0.022153. Entropy: 0.311402.\n",
      "Iteration 870: Policy loss: 0.014595. Value loss: 0.018758. Entropy: 0.311297.\n",
      "episode: 362   score: 105.0  epsilon: 1.0    steps: 400  evaluation reward: 206.8\n",
      "episode: 363   score: 110.0  epsilon: 1.0    steps: 688  evaluation reward: 205.5\n",
      "episode: 364   score: 210.0  epsilon: 1.0    steps: 808  evaluation reward: 203.0\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 871: Policy loss: 0.116212. Value loss: 0.072775. Entropy: 0.314699.\n",
      "Iteration 872: Policy loss: 0.112039. Value loss: 0.038387. Entropy: 0.313154.\n",
      "Iteration 873: Policy loss: 0.112266. Value loss: 0.034621. Entropy: 0.312941.\n",
      "episode: 365   score: 210.0  epsilon: 1.0    steps: 448  evaluation reward: 203.7\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 874: Policy loss: -0.079393. Value loss: 0.059296. Entropy: 0.312234.\n",
      "Iteration 875: Policy loss: -0.081995. Value loss: 0.039955. Entropy: 0.312192.\n",
      "Iteration 876: Policy loss: -0.089016. Value loss: 0.034618. Entropy: 0.313184.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 877: Policy loss: -0.045801. Value loss: 0.056997. Entropy: 0.318046.\n",
      "Iteration 878: Policy loss: -0.050642. Value loss: 0.033180. Entropy: 0.316268.\n",
      "Iteration 879: Policy loss: -0.050372. Value loss: 0.028611. Entropy: 0.316340.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 880: Policy loss: 0.011451. Value loss: 0.051623. Entropy: 0.312117.\n",
      "Iteration 881: Policy loss: 0.010149. Value loss: 0.025777. Entropy: 0.311072.\n",
      "Iteration 882: Policy loss: 0.014753. Value loss: 0.021879. Entropy: 0.311142.\n",
      "episode: 366   score: 260.0  epsilon: 1.0    steps: 152  evaluation reward: 205.2\n",
      "episode: 367   score: 210.0  epsilon: 1.0    steps: 352  evaluation reward: 205.2\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 883: Policy loss: 0.087435. Value loss: 0.020197. Entropy: 0.314326.\n",
      "Iteration 884: Policy loss: 0.088720. Value loss: 0.011266. Entropy: 0.313959.\n",
      "Iteration 885: Policy loss: 0.085601. Value loss: 0.007215. Entropy: 0.314103.\n",
      "episode: 368   score: 150.0  epsilon: 1.0    steps: 272  evaluation reward: 204.85\n",
      "episode: 369   score: 210.0  epsilon: 1.0    steps: 864  evaluation reward: 205.4\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 886: Policy loss: 0.049666. Value loss: 0.051922. Entropy: 0.314669.\n",
      "Iteration 887: Policy loss: 0.045218. Value loss: 0.031374. Entropy: 0.313156.\n",
      "Iteration 888: Policy loss: 0.038767. Value loss: 0.024603. Entropy: 0.314119.\n",
      "episode: 370   score: 110.0  epsilon: 1.0    steps: 448  evaluation reward: 205.4\n",
      "episode: 371   score: 185.0  epsilon: 1.0    steps: 680  evaluation reward: 206.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 889: Policy loss: 0.064028. Value loss: 0.088296. Entropy: 0.312375.\n",
      "Iteration 890: Policy loss: 0.062724. Value loss: 0.048590. Entropy: 0.312227.\n",
      "Iteration 891: Policy loss: 0.060144. Value loss: 0.043817. Entropy: 0.311411.\n",
      "episode: 372   score: 210.0  epsilon: 1.0    steps: 400  evaluation reward: 207.2\n",
      "episode: 373   score: 180.0  epsilon: 1.0    steps: 760  evaluation reward: 207.95\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 892: Policy loss: -0.049782. Value loss: 0.083552. Entropy: 0.312183.\n",
      "Iteration 893: Policy loss: -0.042907. Value loss: 0.048511. Entropy: 0.310798.\n",
      "Iteration 894: Policy loss: -0.045363. Value loss: 0.040396. Entropy: 0.312579.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 895: Policy loss: 0.009009. Value loss: 0.063010. Entropy: 0.308551.\n",
      "Iteration 896: Policy loss: 0.009307. Value loss: 0.035756. Entropy: 0.309655.\n",
      "Iteration 897: Policy loss: 0.007052. Value loss: 0.025500. Entropy: 0.309890.\n",
      "Training network. lr: 0.000243. clip: 0.097392\n",
      "Iteration 898: Policy loss: -0.034559. Value loss: 0.039259. Entropy: 0.309994.\n",
      "Iteration 899: Policy loss: -0.033330. Value loss: 0.024061. Entropy: 0.310931.\n",
      "Iteration 900: Policy loss: -0.034349. Value loss: 0.021472. Entropy: 0.311064.\n",
      "episode: 374   score: 165.0  epsilon: 1.0    steps: 280  evaluation reward: 207.5\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 901: Policy loss: -0.071128. Value loss: 0.060372. Entropy: 0.305082.\n",
      "Iteration 902: Policy loss: -0.070664. Value loss: 0.026679. Entropy: 0.303421.\n",
      "Iteration 903: Policy loss: -0.067707. Value loss: 0.023547. Entropy: 0.303915.\n",
      "episode: 375   score: 105.0  epsilon: 1.0    steps: 144  evaluation reward: 207.75\n",
      "episode: 376   score: 260.0  epsilon: 1.0    steps: 920  evaluation reward: 208.55\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 904: Policy loss: 0.044820. Value loss: 0.076060. Entropy: 0.311946.\n",
      "Iteration 905: Policy loss: 0.037653. Value loss: 0.046909. Entropy: 0.310834.\n",
      "Iteration 906: Policy loss: 0.042491. Value loss: 0.034931. Entropy: 0.310697.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 907: Policy loss: 0.066195. Value loss: 0.062958. Entropy: 0.313579.\n",
      "Iteration 908: Policy loss: 0.062204. Value loss: 0.036820. Entropy: 0.314062.\n",
      "Iteration 909: Policy loss: 0.061640. Value loss: 0.029868. Entropy: 0.314155.\n",
      "episode: 377   score: 320.0  epsilon: 1.0    steps: 904  evaluation reward: 209.95\n",
      "episode: 378   score: 155.0  epsilon: 1.0    steps: 936  evaluation reward: 209.7\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 910: Policy loss: 0.121711. Value loss: 0.100247. Entropy: 0.314370.\n",
      "Iteration 911: Policy loss: 0.119756. Value loss: 0.069330. Entropy: 0.314063.\n",
      "Iteration 912: Policy loss: 0.116647. Value loss: 0.055699. Entropy: 0.313554.\n",
      "episode: 379   score: 270.0  epsilon: 1.0    steps: 96  evaluation reward: 211.9\n",
      "episode: 380   score: 260.0  epsilon: 1.0    steps: 128  evaluation reward: 211.55\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 913: Policy loss: 0.125383. Value loss: 0.034087. Entropy: 0.309457.\n",
      "Iteration 914: Policy loss: 0.125139. Value loss: 0.020106. Entropy: 0.310579.\n",
      "Iteration 915: Policy loss: 0.123440. Value loss: 0.017310. Entropy: 0.310711.\n",
      "episode: 381   score: 195.0  epsilon: 1.0    steps: 232  evaluation reward: 212.4\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 916: Policy loss: 0.102182. Value loss: 0.094361. Entropy: 0.323006.\n",
      "Iteration 917: Policy loss: 0.104039. Value loss: 0.054892. Entropy: 0.322239.\n",
      "Iteration 918: Policy loss: 0.101162. Value loss: 0.044946. Entropy: 0.322120.\n",
      "episode: 382   score: 180.0  epsilon: 1.0    steps: 584  evaluation reward: 210.4\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 919: Policy loss: 0.072475. Value loss: 0.067860. Entropy: 0.315907.\n",
      "Iteration 920: Policy loss: 0.065578. Value loss: 0.031974. Entropy: 0.315686.\n",
      "Iteration 921: Policy loss: 0.065830. Value loss: 0.033527. Entropy: 0.314806.\n",
      "episode: 383   score: 75.0  epsilon: 1.0    steps: 8  evaluation reward: 209.35\n",
      "episode: 384   score: 210.0  epsilon: 1.0    steps: 496  evaluation reward: 209.8\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 922: Policy loss: -0.052633. Value loss: 0.046829. Entropy: 0.315129.\n",
      "Iteration 923: Policy loss: -0.049539. Value loss: 0.021604. Entropy: 0.315318.\n",
      "Iteration 924: Policy loss: -0.055265. Value loss: 0.017049. Entropy: 0.314639.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 925: Policy loss: 0.090517. Value loss: 0.042820. Entropy: 0.316895.\n",
      "Iteration 926: Policy loss: 0.085030. Value loss: 0.024845. Entropy: 0.316605.\n",
      "Iteration 927: Policy loss: 0.088704. Value loss: 0.020143. Entropy: 0.316448.\n",
      "episode: 385   score: 260.0  epsilon: 1.0    steps: 768  evaluation reward: 211.35\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 928: Policy loss: -0.033802. Value loss: 0.084683. Entropy: 0.317477.\n",
      "Iteration 929: Policy loss: -0.039260. Value loss: 0.056885. Entropy: 0.317454.\n",
      "Iteration 930: Policy loss: -0.041450. Value loss: 0.036916. Entropy: 0.317048.\n",
      "episode: 386   score: 180.0  epsilon: 1.0    steps: 112  evaluation reward: 210.9\n",
      "episode: 387   score: 380.0  epsilon: 1.0    steps: 848  evaluation reward: 209.55\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 931: Policy loss: -0.192751. Value loss: 0.332440. Entropy: 0.317936.\n",
      "Iteration 932: Policy loss: -0.206445. Value loss: 0.193796. Entropy: 0.316062.\n",
      "Iteration 933: Policy loss: -0.200716. Value loss: 0.132664. Entropy: 0.316764.\n",
      "episode: 388   score: 210.0  epsilon: 1.0    steps: 768  evaluation reward: 209.55\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 934: Policy loss: -0.309134. Value loss: 0.306824. Entropy: 0.315557.\n",
      "Iteration 935: Policy loss: -0.326111. Value loss: 0.178182. Entropy: 0.316061.\n",
      "Iteration 936: Policy loss: -0.290641. Value loss: 0.089022. Entropy: 0.316777.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 937: Policy loss: 0.141729. Value loss: 0.116909. Entropy: 0.316747.\n",
      "Iteration 938: Policy loss: 0.139198. Value loss: 0.054155. Entropy: 0.316899.\n",
      "Iteration 939: Policy loss: 0.140428. Value loss: 0.034940. Entropy: 0.316344.\n",
      "episode: 389   score: 410.0  epsilon: 1.0    steps: 192  evaluation reward: 212.3\n",
      "episode: 390   score: 210.0  epsilon: 1.0    steps: 360  evaluation reward: 213.2\n",
      "episode: 391   score: 120.0  epsilon: 1.0    steps: 664  evaluation reward: 213.65\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 940: Policy loss: 0.308638. Value loss: 0.082552. Entropy: 0.316381.\n",
      "Iteration 941: Policy loss: 0.309086. Value loss: 0.028715. Entropy: 0.316572.\n",
      "Iteration 942: Policy loss: 0.307260. Value loss: 0.025449. Entropy: 0.315644.\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 943: Policy loss: 0.057826. Value loss: 0.060067. Entropy: 0.330968.\n",
      "Iteration 944: Policy loss: 0.058074. Value loss: 0.030343. Entropy: 0.330513.\n",
      "Iteration 945: Policy loss: 0.053986. Value loss: 0.023336. Entropy: 0.330061.\n",
      "episode: 392   score: 365.0  epsilon: 1.0    steps: 168  evaluation reward: 216.25\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 946: Policy loss: 0.124101. Value loss: 0.070564. Entropy: 0.320490.\n",
      "Iteration 947: Policy loss: 0.122320. Value loss: 0.032889. Entropy: 0.319951.\n",
      "Iteration 948: Policy loss: 0.119571. Value loss: 0.025804. Entropy: 0.319563.\n",
      "episode: 393   score: 180.0  epsilon: 1.0    steps: 208  evaluation reward: 217.3\n",
      "episode: 394   score: 290.0  epsilon: 1.0    steps: 264  evaluation reward: 218.1\n",
      "Training network. lr: 0.000243. clip: 0.097244\n",
      "Iteration 949: Policy loss: 0.060581. Value loss: 0.069310. Entropy: 0.321851.\n",
      "Iteration 950: Policy loss: 0.054033. Value loss: 0.035525. Entropy: 0.321688.\n",
      "Iteration 951: Policy loss: 0.054275. Value loss: 0.025474. Entropy: 0.322326.\n",
      "episode: 395   score: 210.0  epsilon: 1.0    steps: 176  evaluation reward: 216.1\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 952: Policy loss: 0.021786. Value loss: 0.068677. Entropy: 0.330169.\n",
      "Iteration 953: Policy loss: 0.019396. Value loss: 0.047337. Entropy: 0.329686.\n",
      "Iteration 954: Policy loss: 0.018693. Value loss: 0.040045. Entropy: 0.329871.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 396   score: 180.0  epsilon: 1.0    steps: 112  evaluation reward: 215.8\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 955: Policy loss: -0.032777. Value loss: 0.060411. Entropy: 0.323494.\n",
      "Iteration 956: Policy loss: -0.030748. Value loss: 0.041253. Entropy: 0.324102.\n",
      "Iteration 957: Policy loss: -0.035400. Value loss: 0.027471. Entropy: 0.324283.\n",
      "episode: 397   score: 110.0  epsilon: 1.0    steps: 816  evaluation reward: 213.1\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 958: Policy loss: 0.048075. Value loss: 0.047801. Entropy: 0.319483.\n",
      "Iteration 959: Policy loss: 0.046277. Value loss: 0.023106. Entropy: 0.318479.\n",
      "Iteration 960: Policy loss: 0.043414. Value loss: 0.018974. Entropy: 0.319972.\n",
      "episode: 398   score: 210.0  epsilon: 1.0    steps: 128  evaluation reward: 213.1\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 961: Policy loss: 0.162709. Value loss: 0.049892. Entropy: 0.321124.\n",
      "Iteration 962: Policy loss: 0.158515. Value loss: 0.022771. Entropy: 0.321763.\n",
      "Iteration 963: Policy loss: 0.158628. Value loss: 0.019129. Entropy: 0.320076.\n",
      "episode: 399   score: 260.0  epsilon: 1.0    steps: 312  evaluation reward: 213.3\n",
      "episode: 400   score: 210.0  epsilon: 1.0    steps: 704  evaluation reward: 214.35\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 964: Policy loss: -0.028810. Value loss: 0.066856. Entropy: 0.321579.\n",
      "Iteration 965: Policy loss: -0.035745. Value loss: 0.040788. Entropy: 0.321881.\n",
      "Iteration 966: Policy loss: -0.032328. Value loss: 0.032789. Entropy: 0.322093.\n",
      "now time :  2019-09-05 15:14:59.479419\n",
      "episode: 401   score: 460.0  epsilon: 1.0    steps: 760  evaluation reward: 216.05\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 967: Policy loss: -0.347788. Value loss: 0.333359. Entropy: 0.318658.\n",
      "Iteration 968: Policy loss: -0.366779. Value loss: 0.279962. Entropy: 0.319359.\n",
      "Iteration 969: Policy loss: -0.336592. Value loss: 0.224088. Entropy: 0.318619.\n",
      "episode: 402   score: 225.0  epsilon: 1.0    steps: 424  evaluation reward: 216.2\n",
      "episode: 403   score: 260.0  epsilon: 1.0    steps: 768  evaluation reward: 216.4\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 970: Policy loss: 0.029301. Value loss: 0.073857. Entropy: 0.313316.\n",
      "Iteration 971: Policy loss: 0.022329. Value loss: 0.042742. Entropy: 0.314910.\n",
      "Iteration 972: Policy loss: 0.021086. Value loss: 0.033533. Entropy: 0.314325.\n",
      "episode: 404   score: 210.0  epsilon: 1.0    steps: 832  evaluation reward: 218.0\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 973: Policy loss: -0.693972. Value loss: 0.597444. Entropy: 0.320211.\n",
      "Iteration 974: Policy loss: -0.697739. Value loss: 0.499843. Entropy: 0.321802.\n",
      "Iteration 975: Policy loss: -0.698859. Value loss: 0.357421. Entropy: 0.322835.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 976: Policy loss: 0.124680. Value loss: 0.077263. Entropy: 0.314880.\n",
      "Iteration 977: Policy loss: 0.122109. Value loss: 0.030882. Entropy: 0.314190.\n",
      "Iteration 978: Policy loss: 0.122857. Value loss: 0.025278. Entropy: 0.315100.\n",
      "episode: 405   score: 180.0  epsilon: 1.0    steps: 584  evaluation reward: 217.4\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 979: Policy loss: 0.262640. Value loss: 0.089913. Entropy: 0.322487.\n",
      "Iteration 980: Policy loss: 0.259501. Value loss: 0.042341. Entropy: 0.322919.\n",
      "Iteration 981: Policy loss: 0.258598. Value loss: 0.036771. Entropy: 0.323213.\n",
      "episode: 406   score: 425.0  epsilon: 1.0    steps: 208  evaluation reward: 219.85\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 982: Policy loss: -0.140201. Value loss: 0.323111. Entropy: 0.320466.\n",
      "Iteration 983: Policy loss: -0.128787. Value loss: 0.186272. Entropy: 0.320519.\n",
      "Iteration 984: Policy loss: -0.133509. Value loss: 0.114839. Entropy: 0.321076.\n",
      "episode: 407   score: 210.0  epsilon: 1.0    steps: 264  evaluation reward: 218.75\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 985: Policy loss: -0.223765. Value loss: 0.185489. Entropy: 0.316715.\n",
      "Iteration 986: Policy loss: -0.256272. Value loss: 0.108086. Entropy: 0.316289.\n",
      "Iteration 987: Policy loss: -0.264053. Value loss: 0.092252. Entropy: 0.317114.\n",
      "episode: 408   score: 530.0  epsilon: 1.0    steps: 888  evaluation reward: 222.85\n",
      "episode: 409   score: 210.0  epsilon: 1.0    steps: 928  evaluation reward: 223.75\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 988: Policy loss: 0.119848. Value loss: 0.169027. Entropy: 0.319418.\n",
      "Iteration 989: Policy loss: 0.110072. Value loss: 0.053234. Entropy: 0.318796.\n",
      "Iteration 990: Policy loss: 0.106640. Value loss: 0.036474. Entropy: 0.317412.\n",
      "episode: 410   score: 440.0  epsilon: 1.0    steps: 112  evaluation reward: 227.05\n",
      "episode: 411   score: 410.0  epsilon: 1.0    steps: 360  evaluation reward: 228.75\n",
      "episode: 412   score: 110.0  epsilon: 1.0    steps: 896  evaluation reward: 228.65\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 991: Policy loss: 0.169233. Value loss: 0.067567. Entropy: 0.308231.\n",
      "Iteration 992: Policy loss: 0.160760. Value loss: 0.029222. Entropy: 0.306104.\n",
      "Iteration 993: Policy loss: 0.165282. Value loss: 0.020964. Entropy: 0.306773.\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 994: Policy loss: 0.352180. Value loss: 0.088769. Entropy: 0.314874.\n",
      "Iteration 995: Policy loss: 0.342428. Value loss: 0.038913. Entropy: 0.310442.\n",
      "Iteration 996: Policy loss: 0.345046. Value loss: 0.029731. Entropy: 0.311486.\n",
      "episode: 413   score: 165.0  epsilon: 1.0    steps: 944  evaluation reward: 229.5\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 997: Policy loss: 0.553683. Value loss: 0.162669. Entropy: 0.313805.\n",
      "Iteration 998: Policy loss: 0.552213. Value loss: 0.088687. Entropy: 0.313542.\n",
      "Iteration 999: Policy loss: 0.534318. Value loss: 0.057045. Entropy: 0.310277.\n",
      "episode: 414   score: 180.0  epsilon: 1.0    steps: 880  evaluation reward: 226.7\n",
      "Training network. lr: 0.000243. clip: 0.097088\n",
      "Iteration 1000: Policy loss: 0.130093. Value loss: 0.071979. Entropy: 0.308836.\n",
      "Iteration 1001: Policy loss: 0.124804. Value loss: 0.035828. Entropy: 0.308554.\n",
      "Iteration 1002: Policy loss: 0.122234. Value loss: 0.026433. Entropy: 0.308472.\n",
      "episode: 415   score: 105.0  epsilon: 1.0    steps: 16  evaluation reward: 226.2\n",
      "episode: 416   score: 180.0  epsilon: 1.0    steps: 544  evaluation reward: 225.9\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1003: Policy loss: 0.040781. Value loss: 0.069447. Entropy: 0.319442.\n",
      "Iteration 1004: Policy loss: 0.037013. Value loss: 0.036913. Entropy: 0.319335.\n",
      "Iteration 1005: Policy loss: 0.037439. Value loss: 0.030467. Entropy: 0.318559.\n",
      "episode: 417   score: 105.0  epsilon: 1.0    steps: 520  evaluation reward: 225.15\n",
      "episode: 418   score: 110.0  epsilon: 1.0    steps: 848  evaluation reward: 224.9\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1006: Policy loss: 0.083937. Value loss: 0.083082. Entropy: 0.316407.\n",
      "Iteration 1007: Policy loss: 0.079429. Value loss: 0.048303. Entropy: 0.315760.\n",
      "Iteration 1008: Policy loss: 0.079986. Value loss: 0.037732. Entropy: 0.316144.\n",
      "episode: 419   score: 210.0  epsilon: 1.0    steps: 168  evaluation reward: 225.8\n",
      "episode: 420   score: 180.0  epsilon: 1.0    steps: 352  evaluation reward: 225.75\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1009: Policy loss: -0.069542. Value loss: 0.046766. Entropy: 0.308436.\n",
      "Iteration 1010: Policy loss: -0.075110. Value loss: 0.030464. Entropy: 0.311940.\n",
      "Iteration 1011: Policy loss: -0.073452. Value loss: 0.022495. Entropy: 0.311095.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1012: Policy loss: 0.078018. Value loss: 0.052811. Entropy: 0.311038.\n",
      "Iteration 1013: Policy loss: 0.076487. Value loss: 0.020813. Entropy: 0.309805.\n",
      "Iteration 1014: Policy loss: 0.078573. Value loss: 0.015134. Entropy: 0.310596.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1015: Policy loss: -0.025553. Value loss: 0.031216. Entropy: 0.307933.\n",
      "Iteration 1016: Policy loss: -0.031734. Value loss: 0.014784. Entropy: 0.308067.\n",
      "Iteration 1017: Policy loss: -0.029761. Value loss: 0.010724. Entropy: 0.308726.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1018: Policy loss: 0.132427. Value loss: 0.070550. Entropy: 0.321646.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1019: Policy loss: 0.128685. Value loss: 0.028273. Entropy: 0.319968.\n",
      "Iteration 1020: Policy loss: 0.129215. Value loss: 0.021116. Entropy: 0.320053.\n",
      "episode: 421   score: 180.0  epsilon: 1.0    steps: 360  evaluation reward: 225.45\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1021: Policy loss: 0.135895. Value loss: 0.086223. Entropy: 0.320001.\n",
      "Iteration 1022: Policy loss: 0.130853. Value loss: 0.039678. Entropy: 0.319431.\n",
      "Iteration 1023: Policy loss: 0.129467. Value loss: 0.029529. Entropy: 0.319325.\n",
      "episode: 422   score: 180.0  epsilon: 1.0    steps: 48  evaluation reward: 226.05\n",
      "episode: 423   score: 285.0  epsilon: 1.0    steps: 824  evaluation reward: 224.8\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1024: Policy loss: -0.164338. Value loss: 0.341830. Entropy: 0.317998.\n",
      "Iteration 1025: Policy loss: -0.175828. Value loss: 0.216803. Entropy: 0.318819.\n",
      "Iteration 1026: Policy loss: -0.164410. Value loss: 0.140316. Entropy: 0.316659.\n",
      "episode: 424   score: 210.0  epsilon: 1.0    steps: 128  evaluation reward: 224.8\n",
      "episode: 425   score: 210.0  epsilon: 1.0    steps: 544  evaluation reward: 224.45\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1027: Policy loss: 0.185642. Value loss: 0.071080. Entropy: 0.318294.\n",
      "Iteration 1028: Policy loss: 0.182310. Value loss: 0.033199. Entropy: 0.316305.\n",
      "Iteration 1029: Policy loss: 0.180007. Value loss: 0.026179. Entropy: 0.315193.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1030: Policy loss: -0.029853. Value loss: 0.102257. Entropy: 0.317953.\n",
      "Iteration 1031: Policy loss: -0.029052. Value loss: 0.047488. Entropy: 0.317751.\n",
      "Iteration 1032: Policy loss: -0.038246. Value loss: 0.039306. Entropy: 0.318000.\n",
      "episode: 426   score: 565.0  epsilon: 1.0    steps: 488  evaluation reward: 226.0\n",
      "episode: 427   score: 250.0  epsilon: 1.0    steps: 944  evaluation reward: 226.4\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1033: Policy loss: 0.333379. Value loss: 0.101753. Entropy: 0.303803.\n",
      "Iteration 1034: Policy loss: 0.323520. Value loss: 0.055731. Entropy: 0.305726.\n",
      "Iteration 1035: Policy loss: 0.313926. Value loss: 0.039469. Entropy: 0.304759.\n",
      "episode: 428   score: 110.0  epsilon: 1.0    steps: 328  evaluation reward: 225.95\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1036: Policy loss: -0.010009. Value loss: 0.017842. Entropy: 0.309221.\n",
      "Iteration 1037: Policy loss: -0.011637. Value loss: 0.010775. Entropy: 0.309063.\n",
      "Iteration 1038: Policy loss: -0.011892. Value loss: 0.008492. Entropy: 0.309304.\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1039: Policy loss: -0.039465. Value loss: 0.080881. Entropy: 0.314774.\n",
      "Iteration 1040: Policy loss: -0.042551. Value loss: 0.038418. Entropy: 0.314891.\n",
      "Iteration 1041: Policy loss: -0.050660. Value loss: 0.028427. Entropy: 0.316213.\n",
      "episode: 429   score: 335.0  epsilon: 1.0    steps: 400  evaluation reward: 228.25\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1042: Policy loss: 0.186708. Value loss: 0.189550. Entropy: 0.306852.\n",
      "Iteration 1043: Policy loss: 0.167805. Value loss: 0.142983. Entropy: 0.304568.\n",
      "Iteration 1044: Policy loss: 0.156159. Value loss: 0.134212. Entropy: 0.309729.\n",
      "episode: 430   score: 380.0  epsilon: 1.0    steps: 176  evaluation reward: 229.95\n",
      "episode: 431   score: 155.0  epsilon: 1.0    steps: 424  evaluation reward: 229.4\n",
      "episode: 432   score: 260.0  epsilon: 1.0    steps: 672  evaluation reward: 230.45\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1045: Policy loss: 0.022656. Value loss: 0.062649. Entropy: 0.310314.\n",
      "Iteration 1046: Policy loss: 0.023230. Value loss: 0.026181. Entropy: 0.310852.\n",
      "Iteration 1047: Policy loss: 0.024305. Value loss: 0.023445. Entropy: 0.311670.\n",
      "episode: 433   score: 210.0  epsilon: 1.0    steps: 312  evaluation reward: 230.9\n",
      "Training network. lr: 0.000242. clip: 0.096931\n",
      "Iteration 1048: Policy loss: -0.346369. Value loss: 0.323645. Entropy: 0.318199.\n",
      "Iteration 1049: Policy loss: -0.331699. Value loss: 0.296860. Entropy: 0.318527.\n",
      "Iteration 1050: Policy loss: -0.376243. Value loss: 0.300602. Entropy: 0.318774.\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1051: Policy loss: 0.092664. Value loss: 0.077771. Entropy: 0.315444.\n",
      "Iteration 1052: Policy loss: 0.086940. Value loss: 0.048839. Entropy: 0.315556.\n",
      "Iteration 1053: Policy loss: 0.088678. Value loss: 0.041638. Entropy: 0.316571.\n",
      "episode: 434   score: 180.0  epsilon: 1.0    steps: 664  evaluation reward: 230.6\n",
      "episode: 435   score: 380.0  epsilon: 1.0    steps: 720  evaluation reward: 232.6\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1054: Policy loss: 0.169665. Value loss: 0.044137. Entropy: 0.311650.\n",
      "Iteration 1055: Policy loss: 0.164384. Value loss: 0.014560. Entropy: 0.310524.\n",
      "Iteration 1056: Policy loss: 0.170279. Value loss: 0.011830. Entropy: 0.311172.\n",
      "episode: 436   score: 240.0  epsilon: 1.0    steps: 248  evaluation reward: 232.6\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1057: Policy loss: 0.092722. Value loss: 0.039435. Entropy: 0.322274.\n",
      "Iteration 1058: Policy loss: 0.086564. Value loss: 0.024561. Entropy: 0.322449.\n",
      "Iteration 1059: Policy loss: 0.087352. Value loss: 0.023573. Entropy: 0.322673.\n",
      "episode: 437   score: 180.0  epsilon: 1.0    steps: 848  evaluation reward: 231.8\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1060: Policy loss: 0.026470. Value loss: 0.060507. Entropy: 0.321460.\n",
      "Iteration 1061: Policy loss: 0.021143. Value loss: 0.033055. Entropy: 0.321235.\n",
      "Iteration 1062: Policy loss: 0.024071. Value loss: 0.025922. Entropy: 0.321045.\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1063: Policy loss: -0.114981. Value loss: 0.067561. Entropy: 0.316540.\n",
      "Iteration 1064: Policy loss: -0.123779. Value loss: 0.041634. Entropy: 0.315843.\n",
      "Iteration 1065: Policy loss: -0.120884. Value loss: 0.030644. Entropy: 0.316683.\n",
      "episode: 438   score: 210.0  epsilon: 1.0    steps: 248  evaluation reward: 231.5\n",
      "episode: 439   score: 260.0  epsilon: 1.0    steps: 904  evaluation reward: 232.55\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1066: Policy loss: 0.013138. Value loss: 0.046977. Entropy: 0.314804.\n",
      "Iteration 1067: Policy loss: 0.007476. Value loss: 0.029255. Entropy: 0.314478.\n",
      "Iteration 1068: Policy loss: 0.010167. Value loss: 0.021494. Entropy: 0.313833.\n",
      "episode: 440   score: 260.0  epsilon: 1.0    steps: 272  evaluation reward: 233.05\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1069: Policy loss: -0.291588. Value loss: 0.323408. Entropy: 0.314896.\n",
      "Iteration 1070: Policy loss: -0.312531. Value loss: 0.299109. Entropy: 0.316511.\n",
      "Iteration 1071: Policy loss: -0.295020. Value loss: 0.274874. Entropy: 0.317488.\n",
      "episode: 441   score: 300.0  epsilon: 1.0    steps: 792  evaluation reward: 234.95\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1072: Policy loss: 0.150769. Value loss: 0.081059. Entropy: 0.315558.\n",
      "Iteration 1073: Policy loss: 0.143288. Value loss: 0.041336. Entropy: 0.314845.\n",
      "Iteration 1074: Policy loss: 0.144790. Value loss: 0.032310. Entropy: 0.314412.\n",
      "episode: 442   score: 155.0  epsilon: 1.0    steps: 432  evaluation reward: 234.4\n",
      "episode: 443   score: 210.0  epsilon: 1.0    steps: 480  evaluation reward: 234.95\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1075: Policy loss: 0.053326. Value loss: 0.039443. Entropy: 0.313842.\n",
      "Iteration 1076: Policy loss: 0.050906. Value loss: 0.023286. Entropy: 0.314927.\n",
      "Iteration 1077: Policy loss: 0.047087. Value loss: 0.020317. Entropy: 0.314971.\n",
      "episode: 444   score: 80.0  epsilon: 1.0    steps: 528  evaluation reward: 233.15\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1078: Policy loss: -0.040622. Value loss: 0.076869. Entropy: 0.318630.\n",
      "Iteration 1079: Policy loss: -0.043035. Value loss: 0.033975. Entropy: 0.318721.\n",
      "Iteration 1080: Policy loss: -0.043073. Value loss: 0.020939. Entropy: 0.319014.\n",
      "episode: 445   score: 525.0  epsilon: 1.0    steps: 200  evaluation reward: 235.05\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1081: Policy loss: -0.084552. Value loss: 0.029318. Entropy: 0.314606.\n",
      "Iteration 1082: Policy loss: -0.086389. Value loss: 0.016555. Entropy: 0.313375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1083: Policy loss: -0.086207. Value loss: 0.014305. Entropy: 0.311300.\n",
      "episode: 446   score: 155.0  epsilon: 1.0    steps: 480  evaluation reward: 235.05\n",
      "episode: 447   score: 270.0  epsilon: 1.0    steps: 528  evaluation reward: 235.65\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1084: Policy loss: 0.030910. Value loss: 0.037592. Entropy: 0.312698.\n",
      "Iteration 1085: Policy loss: 0.035983. Value loss: 0.022686. Entropy: 0.313099.\n",
      "Iteration 1086: Policy loss: 0.028398. Value loss: 0.021619. Entropy: 0.313733.\n",
      "episode: 448   score: 180.0  epsilon: 1.0    steps: 400  evaluation reward: 235.55\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1087: Policy loss: -0.006672. Value loss: 0.033265. Entropy: 0.320068.\n",
      "Iteration 1088: Policy loss: -0.011792. Value loss: 0.024389. Entropy: 0.319143.\n",
      "Iteration 1089: Policy loss: -0.011044. Value loss: 0.022047. Entropy: 0.319373.\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1090: Policy loss: -0.363881. Value loss: 0.335361. Entropy: 0.315652.\n",
      "Iteration 1091: Policy loss: -0.403046. Value loss: 0.318963. Entropy: 0.315608.\n",
      "Iteration 1092: Policy loss: -0.417399. Value loss: 0.294803. Entropy: 0.313291.\n",
      "episode: 449   score: 380.0  epsilon: 1.0    steps: 600  evaluation reward: 230.15\n",
      "episode: 450   score: 180.0  epsilon: 1.0    steps: 752  evaluation reward: 228.35\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1093: Policy loss: 0.054574. Value loss: 0.044155. Entropy: 0.309951.\n",
      "Iteration 1094: Policy loss: 0.047243. Value loss: 0.024147. Entropy: 0.308982.\n",
      "Iteration 1095: Policy loss: 0.047250. Value loss: 0.016167. Entropy: 0.310310.\n",
      "now time :  2019-09-05 15:23:18.968961\n",
      "episode: 451   score: 210.0  epsilon: 1.0    steps: 856  evaluation reward: 228.65\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1096: Policy loss: 0.053825. Value loss: 0.061638. Entropy: 0.316210.\n",
      "Iteration 1097: Policy loss: 0.050974. Value loss: 0.033874. Entropy: 0.316045.\n",
      "Iteration 1098: Policy loss: 0.051536. Value loss: 0.024277. Entropy: 0.315701.\n",
      "episode: 452   score: 240.0  epsilon: 1.0    steps: 280  evaluation reward: 225.75\n",
      "episode: 453   score: 210.0  epsilon: 1.0    steps: 496  evaluation reward: 225.75\n",
      "Training network. lr: 0.000242. clip: 0.096784\n",
      "Iteration 1099: Policy loss: 0.133853. Value loss: 0.082331. Entropy: 0.316755.\n",
      "Iteration 1100: Policy loss: 0.142300. Value loss: 0.041177. Entropy: 0.315402.\n",
      "Iteration 1101: Policy loss: 0.134787. Value loss: 0.034232. Entropy: 0.314685.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1102: Policy loss: 0.042655. Value loss: 0.149364. Entropy: 0.317623.\n",
      "Iteration 1103: Policy loss: 0.034790. Value loss: 0.112461. Entropy: 0.317022.\n",
      "Iteration 1104: Policy loss: 0.038230. Value loss: 0.098365. Entropy: 0.316970.\n",
      "episode: 454   score: 210.0  epsilon: 1.0    steps: 280  evaluation reward: 226.4\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1105: Policy loss: 0.052143. Value loss: 0.041169. Entropy: 0.309922.\n",
      "Iteration 1106: Policy loss: 0.055633. Value loss: 0.020480. Entropy: 0.309907.\n",
      "Iteration 1107: Policy loss: 0.047884. Value loss: 0.017510. Entropy: 0.310207.\n",
      "episode: 455   score: 380.0  epsilon: 1.0    steps: 256  evaluation reward: 228.1\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1108: Policy loss: 0.081857. Value loss: 0.065021. Entropy: 0.309321.\n",
      "Iteration 1109: Policy loss: 0.086533. Value loss: 0.035657. Entropy: 0.308685.\n",
      "Iteration 1110: Policy loss: 0.075483. Value loss: 0.028054. Entropy: 0.308801.\n",
      "episode: 456   score: 330.0  epsilon: 1.0    steps: 392  evaluation reward: 229.3\n",
      "episode: 457   score: 185.0  epsilon: 1.0    steps: 896  evaluation reward: 228.55\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1111: Policy loss: -0.276190. Value loss: 0.346866. Entropy: 0.314426.\n",
      "Iteration 1112: Policy loss: -0.313208. Value loss: 0.258865. Entropy: 0.316634.\n",
      "Iteration 1113: Policy loss: -0.319353. Value loss: 0.177823. Entropy: 0.316292.\n",
      "episode: 458   score: 210.0  epsilon: 1.0    steps: 208  evaluation reward: 230.35\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1114: Policy loss: -0.147817. Value loss: 0.191253. Entropy: 0.317200.\n",
      "Iteration 1115: Policy loss: -0.163564. Value loss: 0.104581. Entropy: 0.317456.\n",
      "Iteration 1116: Policy loss: -0.134802. Value loss: 0.067920. Entropy: 0.315700.\n",
      "episode: 459   score: 410.0  epsilon: 1.0    steps: 448  evaluation reward: 232.65\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1117: Policy loss: -0.098771. Value loss: 0.072903. Entropy: 0.318230.\n",
      "Iteration 1118: Policy loss: -0.092685. Value loss: 0.041581. Entropy: 0.317833.\n",
      "Iteration 1119: Policy loss: -0.098332. Value loss: 0.035345. Entropy: 0.317661.\n",
      "episode: 460   score: 210.0  epsilon: 1.0    steps: 296  evaluation reward: 233.7\n",
      "episode: 461   score: 460.0  epsilon: 1.0    steps: 816  evaluation reward: 235.65\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1120: Policy loss: -0.174366. Value loss: 0.245158. Entropy: 0.313954.\n",
      "Iteration 1121: Policy loss: -0.204960. Value loss: 0.108915. Entropy: 0.311343.\n",
      "Iteration 1122: Policy loss: -0.201076. Value loss: 0.084832. Entropy: 0.312445.\n",
      "episode: 462   score: 380.0  epsilon: 1.0    steps: 488  evaluation reward: 238.4\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1123: Policy loss: 0.042963. Value loss: 0.054850. Entropy: 0.307343.\n",
      "Iteration 1124: Policy loss: 0.040533. Value loss: 0.029384. Entropy: 0.309969.\n",
      "Iteration 1125: Policy loss: 0.037184. Value loss: 0.020944. Entropy: 0.310757.\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1126: Policy loss: -0.170671. Value loss: 0.298718. Entropy: 0.311105.\n",
      "Iteration 1127: Policy loss: -0.149024. Value loss: 0.181355. Entropy: 0.311709.\n",
      "Iteration 1128: Policy loss: -0.150682. Value loss: 0.114020. Entropy: 0.312636.\n",
      "episode: 463   score: 180.0  epsilon: 1.0    steps: 560  evaluation reward: 239.1\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1129: Policy loss: 0.235861. Value loss: 0.066685. Entropy: 0.308186.\n",
      "Iteration 1130: Policy loss: 0.232377. Value loss: 0.032035. Entropy: 0.306361.\n",
      "Iteration 1131: Policy loss: 0.232252. Value loss: 0.028201. Entropy: 0.305104.\n",
      "episode: 464   score: 380.0  epsilon: 1.0    steps: 464  evaluation reward: 240.8\n",
      "episode: 465   score: 225.0  epsilon: 1.0    steps: 848  evaluation reward: 240.95\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1132: Policy loss: 0.043840. Value loss: 0.121500. Entropy: 0.309803.\n",
      "Iteration 1133: Policy loss: 0.034944. Value loss: 0.053748. Entropy: 0.309243.\n",
      "Iteration 1134: Policy loss: 0.008312. Value loss: 0.037255. Entropy: 0.310274.\n",
      "episode: 466   score: 210.0  epsilon: 1.0    steps: 32  evaluation reward: 240.45\n",
      "episode: 467   score: 180.0  epsilon: 1.0    steps: 632  evaluation reward: 240.15\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1135: Policy loss: 0.018554. Value loss: 0.077429. Entropy: 0.309826.\n",
      "Iteration 1136: Policy loss: 0.019658. Value loss: 0.042048. Entropy: 0.308940.\n",
      "Iteration 1137: Policy loss: 0.018783. Value loss: 0.031857. Entropy: 0.309560.\n",
      "episode: 468   score: 210.0  epsilon: 1.0    steps: 888  evaluation reward: 240.75\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1138: Policy loss: -0.062276. Value loss: 0.064347. Entropy: 0.307859.\n",
      "Iteration 1139: Policy loss: -0.060730. Value loss: 0.042650. Entropy: 0.307285.\n",
      "Iteration 1140: Policy loss: -0.061406. Value loss: 0.040086. Entropy: 0.308318.\n",
      "episode: 469   score: 210.0  epsilon: 1.0    steps: 448  evaluation reward: 240.75\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1141: Policy loss: 0.012170. Value loss: 0.036090. Entropy: 0.303642.\n",
      "Iteration 1142: Policy loss: 0.007280. Value loss: 0.017447. Entropy: 0.303606.\n",
      "Iteration 1143: Policy loss: 0.005782. Value loss: 0.013666. Entropy: 0.303857.\n",
      "episode: 470   score: 195.0  epsilon: 1.0    steps: 1008  evaluation reward: 241.6\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1144: Policy loss: 0.098949. Value loss: 0.060131. Entropy: 0.308165.\n",
      "Iteration 1145: Policy loss: 0.097285. Value loss: 0.030137. Entropy: 0.306796.\n",
      "Iteration 1146: Policy loss: 0.094766. Value loss: 0.024449. Entropy: 0.307522.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1147: Policy loss: -0.202875. Value loss: 0.289967. Entropy: 0.305634.\n",
      "Iteration 1148: Policy loss: -0.203725. Value loss: 0.207373. Entropy: 0.304608.\n",
      "Iteration 1149: Policy loss: -0.232034. Value loss: 0.159725. Entropy: 0.306849.\n",
      "episode: 471   score: 210.0  epsilon: 1.0    steps: 224  evaluation reward: 241.85\n",
      "episode: 472   score: 180.0  epsilon: 1.0    steps: 984  evaluation reward: 241.55\n",
      "Training network. lr: 0.000242. clip: 0.096627\n",
      "Iteration 1150: Policy loss: -0.215885. Value loss: 0.199545. Entropy: 0.306260.\n",
      "Iteration 1151: Policy loss: -0.222716. Value loss: 0.123808. Entropy: 0.308236.\n",
      "Iteration 1152: Policy loss: -0.233832. Value loss: 0.092535. Entropy: 0.308163.\n",
      "episode: 473   score: 180.0  epsilon: 1.0    steps: 8  evaluation reward: 241.55\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1153: Policy loss: 0.020605. Value loss: 0.069664. Entropy: 0.306789.\n",
      "Iteration 1154: Policy loss: 0.014537. Value loss: 0.044788. Entropy: 0.308430.\n",
      "Iteration 1155: Policy loss: 0.015951. Value loss: 0.032880. Entropy: 0.307718.\n",
      "episode: 474   score: 380.0  epsilon: 1.0    steps: 240  evaluation reward: 243.7\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1156: Policy loss: 0.034913. Value loss: 0.069214. Entropy: 0.308161.\n",
      "Iteration 1157: Policy loss: 0.035214. Value loss: 0.048793. Entropy: 0.309312.\n",
      "Iteration 1158: Policy loss: 0.035116. Value loss: 0.037751. Entropy: 0.308530.\n",
      "episode: 475   score: 180.0  epsilon: 1.0    steps: 72  evaluation reward: 244.45\n",
      "episode: 476   score: 480.0  epsilon: 1.0    steps: 88  evaluation reward: 246.65\n",
      "episode: 477   score: 180.0  epsilon: 1.0    steps: 848  evaluation reward: 245.25\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1159: Policy loss: 0.023392. Value loss: 0.059621. Entropy: 0.310791.\n",
      "Iteration 1160: Policy loss: 0.018877. Value loss: 0.035488. Entropy: 0.311476.\n",
      "Iteration 1161: Policy loss: 0.017134. Value loss: 0.030651. Entropy: 0.311777.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1162: Policy loss: -0.254453. Value loss: 0.299368. Entropy: 0.313297.\n",
      "Iteration 1163: Policy loss: -0.250304. Value loss: 0.095478. Entropy: 0.306357.\n",
      "Iteration 1164: Policy loss: -0.266641. Value loss: 0.084954. Entropy: 0.309102.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1165: Policy loss: -0.075309. Value loss: 0.331849. Entropy: 0.311538.\n",
      "Iteration 1166: Policy loss: -0.081681. Value loss: 0.138363. Entropy: 0.312153.\n",
      "Iteration 1167: Policy loss: -0.077772. Value loss: 0.088212. Entropy: 0.311249.\n",
      "episode: 478   score: 110.0  epsilon: 1.0    steps: 328  evaluation reward: 244.8\n",
      "episode: 479   score: 210.0  epsilon: 1.0    steps: 376  evaluation reward: 244.2\n",
      "episode: 480   score: 385.0  epsilon: 1.0    steps: 584  evaluation reward: 245.45\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1168: Policy loss: -0.088914. Value loss: 0.076969. Entropy: 0.307641.\n",
      "Iteration 1169: Policy loss: -0.098341. Value loss: 0.049027. Entropy: 0.307983.\n",
      "Iteration 1170: Policy loss: -0.106691. Value loss: 0.043426. Entropy: 0.308098.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1171: Policy loss: 0.536033. Value loss: 0.181850. Entropy: 0.311630.\n",
      "Iteration 1172: Policy loss: 0.495133. Value loss: 0.077824. Entropy: 0.310309.\n",
      "Iteration 1173: Policy loss: 0.480782. Value loss: 0.052088. Entropy: 0.311413.\n",
      "episode: 481   score: 260.0  epsilon: 1.0    steps: 808  evaluation reward: 246.1\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1174: Policy loss: 0.485061. Value loss: 0.222825. Entropy: 0.312325.\n",
      "Iteration 1175: Policy loss: 0.469443. Value loss: 0.096642. Entropy: 0.310804.\n",
      "Iteration 1176: Policy loss: 0.463082. Value loss: 0.063746. Entropy: 0.310040.\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1177: Policy loss: -0.076618. Value loss: 0.126239. Entropy: 0.314245.\n",
      "Iteration 1178: Policy loss: -0.088272. Value loss: 0.062573. Entropy: 0.312273.\n",
      "Iteration 1179: Policy loss: -0.084794. Value loss: 0.048977. Entropy: 0.312798.\n",
      "episode: 482   score: 460.0  epsilon: 1.0    steps: 216  evaluation reward: 248.9\n",
      "episode: 483   score: 275.0  epsilon: 1.0    steps: 400  evaluation reward: 250.9\n",
      "episode: 484   score: 210.0  epsilon: 1.0    steps: 488  evaluation reward: 250.9\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1180: Policy loss: 0.294853. Value loss: 0.142183. Entropy: 0.311868.\n",
      "Iteration 1181: Policy loss: 0.284826. Value loss: 0.060445. Entropy: 0.310274.\n",
      "Iteration 1182: Policy loss: 0.280040. Value loss: 0.048897. Entropy: 0.309697.\n",
      "episode: 485   score: 315.0  epsilon: 1.0    steps: 560  evaluation reward: 251.45\n",
      "episode: 486   score: 105.0  epsilon: 1.0    steps: 800  evaluation reward: 250.7\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1183: Policy loss: 0.369563. Value loss: 0.165780. Entropy: 0.312709.\n",
      "Iteration 1184: Policy loss: 0.352850. Value loss: 0.081661. Entropy: 0.310954.\n",
      "Iteration 1185: Policy loss: 0.356410. Value loss: 0.072068. Entropy: 0.311634.\n",
      "episode: 487   score: 135.0  epsilon: 1.0    steps: 552  evaluation reward: 248.25\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1186: Policy loss: 0.342480. Value loss: 0.094084. Entropy: 0.310831.\n",
      "Iteration 1187: Policy loss: 0.340844. Value loss: 0.052561. Entropy: 0.309794.\n",
      "Iteration 1188: Policy loss: 0.335441. Value loss: 0.038007. Entropy: 0.309707.\n",
      "episode: 488   score: 160.0  epsilon: 1.0    steps: 40  evaluation reward: 247.75\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1189: Policy loss: 0.235456. Value loss: 0.077276. Entropy: 0.315724.\n",
      "Iteration 1190: Policy loss: 0.230456. Value loss: 0.032650. Entropy: 0.314371.\n",
      "Iteration 1191: Policy loss: 0.225986. Value loss: 0.024665. Entropy: 0.312261.\n",
      "episode: 489   score: 120.0  epsilon: 1.0    steps: 392  evaluation reward: 244.85\n",
      "episode: 490   score: 15.0  epsilon: 1.0    steps: 872  evaluation reward: 242.9\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1192: Policy loss: 0.372542. Value loss: 0.093529. Entropy: 0.313181.\n",
      "Iteration 1193: Policy loss: 0.377285. Value loss: 0.048329. Entropy: 0.313611.\n",
      "Iteration 1194: Policy loss: 0.370256. Value loss: 0.040552. Entropy: 0.313329.\n",
      "episode: 491   score: 125.0  epsilon: 1.0    steps: 632  evaluation reward: 242.95\n",
      "episode: 492   score: 105.0  epsilon: 1.0    steps: 696  evaluation reward: 240.35\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1195: Policy loss: -0.100182. Value loss: 0.239139. Entropy: 0.309214.\n",
      "Iteration 1196: Policy loss: -0.109040. Value loss: 0.204300. Entropy: 0.308419.\n",
      "Iteration 1197: Policy loss: -0.104343. Value loss: 0.191198. Entropy: 0.307908.\n",
      "episode: 493   score: 380.0  epsilon: 1.0    steps: 440  evaluation reward: 242.35\n",
      "Training network. lr: 0.000241. clip: 0.096470\n",
      "Iteration 1198: Policy loss: -0.458742. Value loss: 0.397344. Entropy: 0.315745.\n",
      "Iteration 1199: Policy loss: -0.431340. Value loss: 0.301637. Entropy: 0.316891.\n",
      "Iteration 1200: Policy loss: -0.456253. Value loss: 0.239666. Entropy: 0.315233.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1201: Policy loss: 0.060737. Value loss: 0.101824. Entropy: 0.316521.\n",
      "Iteration 1202: Policy loss: 0.058962. Value loss: 0.036756. Entropy: 0.314571.\n",
      "Iteration 1203: Policy loss: 0.063370. Value loss: 0.029975. Entropy: 0.315561.\n",
      "episode: 494   score: 410.0  epsilon: 1.0    steps: 440  evaluation reward: 243.55\n",
      "episode: 495   score: 180.0  epsilon: 1.0    steps: 864  evaluation reward: 243.25\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1204: Policy loss: 0.090606. Value loss: 0.038164. Entropy: 0.311106.\n",
      "Iteration 1205: Policy loss: 0.090192. Value loss: 0.022657. Entropy: 0.308836.\n",
      "Iteration 1206: Policy loss: 0.086764. Value loss: 0.019746. Entropy: 0.311546.\n",
      "episode: 496   score: 135.0  epsilon: 1.0    steps: 280  evaluation reward: 242.8\n",
      "episode: 497   score: 105.0  epsilon: 1.0    steps: 480  evaluation reward: 242.75\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1207: Policy loss: -0.195535. Value loss: 0.261748. Entropy: 0.312061.\n",
      "Iteration 1208: Policy loss: -0.181790. Value loss: 0.158583. Entropy: 0.312945.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1209: Policy loss: -0.178575. Value loss: 0.063094. Entropy: 0.312586.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1210: Policy loss: 0.445260. Value loss: 0.227201. Entropy: 0.314056.\n",
      "Iteration 1211: Policy loss: 0.417243. Value loss: 0.077166. Entropy: 0.312360.\n",
      "Iteration 1212: Policy loss: 0.413206. Value loss: 0.057094. Entropy: 0.310822.\n",
      "episode: 498   score: 410.0  epsilon: 1.0    steps: 352  evaluation reward: 244.75\n",
      "episode: 499   score: 110.0  epsilon: 1.0    steps: 456  evaluation reward: 243.25\n",
      "episode: 500   score: 210.0  epsilon: 1.0    steps: 960  evaluation reward: 243.25\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1213: Policy loss: -0.004709. Value loss: 0.066209. Entropy: 0.318927.\n",
      "Iteration 1214: Policy loss: 0.000368. Value loss: 0.031836. Entropy: 0.316756.\n",
      "Iteration 1215: Policy loss: -0.003027. Value loss: 0.029191. Entropy: 0.318033.\n",
      "now time :  2019-09-05 15:30:44.973579\n",
      "episode: 501   score: 180.0  epsilon: 1.0    steps: 24  evaluation reward: 240.45\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1216: Policy loss: 0.092274. Value loss: 0.079516. Entropy: 0.315308.\n",
      "Iteration 1217: Policy loss: 0.089172. Value loss: 0.053619. Entropy: 0.317093.\n",
      "Iteration 1218: Policy loss: 0.085436. Value loss: 0.036322. Entropy: 0.317559.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1219: Policy loss: 0.021412. Value loss: 0.110157. Entropy: 0.307841.\n",
      "Iteration 1220: Policy loss: 0.017853. Value loss: 0.048790. Entropy: 0.308484.\n",
      "Iteration 1221: Policy loss: 0.011855. Value loss: 0.034851. Entropy: 0.309270.\n",
      "episode: 502   score: 210.0  epsilon: 1.0    steps: 864  evaluation reward: 240.3\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1222: Policy loss: 0.139948. Value loss: 0.059301. Entropy: 0.314315.\n",
      "Iteration 1223: Policy loss: 0.135077. Value loss: 0.022151. Entropy: 0.311967.\n",
      "Iteration 1224: Policy loss: 0.135525. Value loss: 0.016160. Entropy: 0.313520.\n",
      "episode: 503   score: 180.0  epsilon: 1.0    steps: 208  evaluation reward: 239.5\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1225: Policy loss: 0.009421. Value loss: 0.065187. Entropy: 0.312922.\n",
      "Iteration 1226: Policy loss: 0.011164. Value loss: 0.038294. Entropy: 0.315822.\n",
      "Iteration 1227: Policy loss: 0.008954. Value loss: 0.027754. Entropy: 0.314742.\n",
      "episode: 504   score: 210.0  epsilon: 1.0    steps: 48  evaluation reward: 239.5\n",
      "episode: 505   score: 225.0  epsilon: 1.0    steps: 952  evaluation reward: 239.95\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1228: Policy loss: -0.186611. Value loss: 0.118238. Entropy: 0.314482.\n",
      "Iteration 1229: Policy loss: -0.193824. Value loss: 0.066433. Entropy: 0.313428.\n",
      "Iteration 1230: Policy loss: -0.192888. Value loss: 0.051433. Entropy: 0.314141.\n",
      "episode: 506   score: 180.0  epsilon: 1.0    steps: 608  evaluation reward: 237.5\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1231: Policy loss: 0.037430. Value loss: 0.044251. Entropy: 0.315152.\n",
      "Iteration 1232: Policy loss: 0.038571. Value loss: 0.027077. Entropy: 0.315280.\n",
      "Iteration 1233: Policy loss: 0.040171. Value loss: 0.021874. Entropy: 0.315492.\n",
      "episode: 507   score: 120.0  epsilon: 1.0    steps: 40  evaluation reward: 236.6\n",
      "episode: 508   score: 210.0  epsilon: 1.0    steps: 440  evaluation reward: 233.4\n",
      "episode: 509   score: 285.0  epsilon: 1.0    steps: 856  evaluation reward: 234.15\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1234: Policy loss: -0.052320. Value loss: 0.054913. Entropy: 0.317917.\n",
      "Iteration 1235: Policy loss: -0.057968. Value loss: 0.031710. Entropy: 0.317603.\n",
      "Iteration 1236: Policy loss: -0.052757. Value loss: 0.023983. Entropy: 0.318920.\n",
      "episode: 510   score: 105.0  epsilon: 1.0    steps: 816  evaluation reward: 230.8\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1237: Policy loss: -0.325313. Value loss: 0.291043. Entropy: 0.309650.\n",
      "Iteration 1238: Policy loss: -0.311549. Value loss: 0.172622. Entropy: 0.310955.\n",
      "Iteration 1239: Policy loss: -0.316546. Value loss: 0.121356. Entropy: 0.310673.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1240: Policy loss: 0.060899. Value loss: 0.155255. Entropy: 0.315463.\n",
      "Iteration 1241: Policy loss: 0.050928. Value loss: 0.045921. Entropy: 0.314020.\n",
      "Iteration 1242: Policy loss: 0.052603. Value loss: 0.031998. Entropy: 0.313887.\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1243: Policy loss: -0.044902. Value loss: 0.044315. Entropy: 0.316889.\n",
      "Iteration 1244: Policy loss: -0.051150. Value loss: 0.025388. Entropy: 0.317181.\n",
      "Iteration 1245: Policy loss: -0.052104. Value loss: 0.018343. Entropy: 0.318180.\n",
      "episode: 511   score: 455.0  epsilon: 1.0    steps: 624  evaluation reward: 231.25\n",
      "episode: 512   score: 210.0  epsilon: 1.0    steps: 776  evaluation reward: 232.25\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1246: Policy loss: 0.286072. Value loss: 0.076519. Entropy: 0.313579.\n",
      "Iteration 1247: Policy loss: 0.277281. Value loss: 0.030734. Entropy: 0.311819.\n",
      "Iteration 1248: Policy loss: 0.269529. Value loss: 0.023348. Entropy: 0.311913.\n",
      "episode: 513   score: 180.0  epsilon: 1.0    steps: 352  evaluation reward: 232.4\n",
      "Training network. lr: 0.000241. clip: 0.096323\n",
      "Iteration 1249: Policy loss: 0.159366. Value loss: 0.088474. Entropy: 0.313758.\n",
      "Iteration 1250: Policy loss: 0.152612. Value loss: 0.045660. Entropy: 0.314741.\n",
      "Iteration 1251: Policy loss: 0.150269. Value loss: 0.033723. Entropy: 0.313457.\n",
      "episode: 514   score: 185.0  epsilon: 1.0    steps: 192  evaluation reward: 232.45\n",
      "episode: 515   score: 180.0  epsilon: 1.0    steps: 376  evaluation reward: 233.2\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1252: Policy loss: 0.186516. Value loss: 0.088022. Entropy: 0.318842.\n",
      "Iteration 1253: Policy loss: 0.179269. Value loss: 0.048364. Entropy: 0.316557.\n",
      "Iteration 1254: Policy loss: 0.171658. Value loss: 0.035798. Entropy: 0.316047.\n",
      "episode: 516   score: 210.0  epsilon: 1.0    steps: 24  evaluation reward: 233.5\n",
      "episode: 517   score: 155.0  epsilon: 1.0    steps: 480  evaluation reward: 234.0\n",
      "episode: 518   score: 75.0  epsilon: 1.0    steps: 896  evaluation reward: 233.65\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1255: Policy loss: 0.167732. Value loss: 0.051575. Entropy: 0.312381.\n",
      "Iteration 1256: Policy loss: 0.165125. Value loss: 0.029267. Entropy: 0.312388.\n",
      "Iteration 1257: Policy loss: 0.159295. Value loss: 0.025037. Entropy: 0.311958.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1258: Policy loss: -0.073232. Value loss: 0.071103. Entropy: 0.311456.\n",
      "Iteration 1259: Policy loss: -0.073685. Value loss: 0.044082. Entropy: 0.310743.\n",
      "Iteration 1260: Policy loss: -0.068382. Value loss: 0.030719. Entropy: 0.310316.\n",
      "episode: 519   score: 235.0  epsilon: 1.0    steps: 392  evaluation reward: 233.9\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1261: Policy loss: 0.022104. Value loss: 0.068733. Entropy: 0.308649.\n",
      "Iteration 1262: Policy loss: 0.021523. Value loss: 0.040104. Entropy: 0.308066.\n",
      "Iteration 1263: Policy loss: 0.020710. Value loss: 0.036006. Entropy: 0.307751.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1264: Policy loss: -0.168151. Value loss: 0.068798. Entropy: 0.308259.\n",
      "Iteration 1265: Policy loss: -0.168397. Value loss: 0.030307. Entropy: 0.307620.\n",
      "Iteration 1266: Policy loss: -0.171297. Value loss: 0.021805. Entropy: 0.308130.\n",
      "episode: 520   score: 105.0  epsilon: 1.0    steps: 376  evaluation reward: 233.15\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1267: Policy loss: 0.154697. Value loss: 0.075648. Entropy: 0.310269.\n",
      "Iteration 1268: Policy loss: 0.148509. Value loss: 0.035161. Entropy: 0.310026.\n",
      "Iteration 1269: Policy loss: 0.143218. Value loss: 0.027234. Entropy: 0.309607.\n",
      "episode: 521   score: 285.0  epsilon: 1.0    steps: 240  evaluation reward: 234.2\n",
      "episode: 522   score: 110.0  epsilon: 1.0    steps: 384  evaluation reward: 233.5\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1270: Policy loss: 0.121415. Value loss: 0.128538. Entropy: 0.307818.\n",
      "Iteration 1271: Policy loss: 0.119227. Value loss: 0.069143. Entropy: 0.308149.\n",
      "Iteration 1272: Policy loss: 0.109253. Value loss: 0.058959. Entropy: 0.308126.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 523   score: 135.0  epsilon: 1.0    steps: 576  evaluation reward: 232.0\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1273: Policy loss: 0.142035. Value loss: 0.102947. Entropy: 0.314545.\n",
      "Iteration 1274: Policy loss: 0.140948. Value loss: 0.053422. Entropy: 0.314247.\n",
      "Iteration 1275: Policy loss: 0.139409. Value loss: 0.038223. Entropy: 0.314830.\n",
      "episode: 524   score: 230.0  epsilon: 1.0    steps: 672  evaluation reward: 232.2\n",
      "episode: 525   score: 185.0  epsilon: 1.0    steps: 808  evaluation reward: 231.95\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1276: Policy loss: 0.189719. Value loss: 0.039574. Entropy: 0.308117.\n",
      "Iteration 1277: Policy loss: 0.178981. Value loss: 0.017683. Entropy: 0.307149.\n",
      "Iteration 1278: Policy loss: 0.180516. Value loss: 0.014216. Entropy: 0.307280.\n",
      "episode: 526   score: 180.0  epsilon: 1.0    steps: 784  evaluation reward: 228.1\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1279: Policy loss: 0.017583. Value loss: 0.040424. Entropy: 0.318011.\n",
      "Iteration 1280: Policy loss: 0.011064. Value loss: 0.027886. Entropy: 0.318523.\n",
      "Iteration 1281: Policy loss: 0.009917. Value loss: 0.024782. Entropy: 0.318416.\n",
      "episode: 527   score: 225.0  epsilon: 1.0    steps: 848  evaluation reward: 227.85\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1282: Policy loss: 0.078714. Value loss: 0.059860. Entropy: 0.311801.\n",
      "Iteration 1283: Policy loss: 0.079818. Value loss: 0.028101. Entropy: 0.311972.\n",
      "Iteration 1284: Policy loss: 0.081182. Value loss: 0.022038. Entropy: 0.311625.\n",
      "episode: 528   score: 180.0  epsilon: 1.0    steps: 928  evaluation reward: 228.55\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1285: Policy loss: 0.045227. Value loss: 0.049324. Entropy: 0.314050.\n",
      "Iteration 1286: Policy loss: 0.040001. Value loss: 0.025641. Entropy: 0.314153.\n",
      "Iteration 1287: Policy loss: 0.040343. Value loss: 0.019631. Entropy: 0.312772.\n",
      "episode: 529   score: 160.0  epsilon: 1.0    steps: 704  evaluation reward: 226.8\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1288: Policy loss: 0.040043. Value loss: 0.065402. Entropy: 0.318173.\n",
      "Iteration 1289: Policy loss: 0.039838. Value loss: 0.027751. Entropy: 0.317069.\n",
      "Iteration 1290: Policy loss: 0.038926. Value loss: 0.022873. Entropy: 0.316609.\n",
      "episode: 530   score: 260.0  epsilon: 1.0    steps: 520  evaluation reward: 225.6\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1291: Policy loss: -0.002648. Value loss: 0.086525. Entropy: 0.317175.\n",
      "Iteration 1292: Policy loss: -0.004052. Value loss: 0.048688. Entropy: 0.316982.\n",
      "Iteration 1293: Policy loss: -0.004394. Value loss: 0.031022. Entropy: 0.318204.\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1294: Policy loss: 0.024060. Value loss: 0.094734. Entropy: 0.320960.\n",
      "Iteration 1295: Policy loss: 0.019996. Value loss: 0.039124. Entropy: 0.321738.\n",
      "Iteration 1296: Policy loss: 0.021018. Value loss: 0.030235. Entropy: 0.319452.\n",
      "episode: 531   score: 270.0  epsilon: 1.0    steps: 88  evaluation reward: 226.75\n",
      "episode: 532   score: 110.0  epsilon: 1.0    steps: 728  evaluation reward: 225.25\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1297: Policy loss: 0.166269. Value loss: 0.066853. Entropy: 0.315800.\n",
      "Iteration 1298: Policy loss: 0.158363. Value loss: 0.038304. Entropy: 0.314180.\n",
      "Iteration 1299: Policy loss: 0.157620. Value loss: 0.027915. Entropy: 0.314540.\n",
      "episode: 533   score: 260.0  epsilon: 1.0    steps: 72  evaluation reward: 225.75\n",
      "Training network. lr: 0.000240. clip: 0.096166\n",
      "Iteration 1300: Policy loss: 0.030713. Value loss: 0.044002. Entropy: 0.312187.\n",
      "Iteration 1301: Policy loss: 0.027684. Value loss: 0.024271. Entropy: 0.311178.\n",
      "Iteration 1302: Policy loss: 0.025885. Value loss: 0.020182. Entropy: 0.312370.\n",
      "episode: 534   score: 180.0  epsilon: 1.0    steps: 232  evaluation reward: 225.75\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1303: Policy loss: -0.060722. Value loss: 0.085820. Entropy: 0.312111.\n",
      "Iteration 1304: Policy loss: -0.059648. Value loss: 0.037904. Entropy: 0.313212.\n",
      "Iteration 1305: Policy loss: -0.062318. Value loss: 0.028354. Entropy: 0.313557.\n",
      "episode: 535   score: 180.0  epsilon: 1.0    steps: 376  evaluation reward: 223.75\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1306: Policy loss: -0.003418. Value loss: 0.071925. Entropy: 0.312650.\n",
      "Iteration 1307: Policy loss: -0.015311. Value loss: 0.033760. Entropy: 0.313060.\n",
      "Iteration 1308: Policy loss: -0.010216. Value loss: 0.024555. Entropy: 0.312134.\n",
      "episode: 536   score: 345.0  epsilon: 1.0    steps: 280  evaluation reward: 224.8\n",
      "episode: 537   score: 410.0  epsilon: 1.0    steps: 424  evaluation reward: 227.1\n",
      "episode: 538   score: 210.0  epsilon: 1.0    steps: 920  evaluation reward: 227.1\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1309: Policy loss: 0.090181. Value loss: 0.062782. Entropy: 0.315809.\n",
      "Iteration 1310: Policy loss: 0.084162. Value loss: 0.032465. Entropy: 0.315083.\n",
      "Iteration 1311: Policy loss: 0.086037. Value loss: 0.026731. Entropy: 0.314731.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1312: Policy loss: 0.082068. Value loss: 0.066343. Entropy: 0.319909.\n",
      "Iteration 1313: Policy loss: 0.071276. Value loss: 0.041892. Entropy: 0.319721.\n",
      "Iteration 1314: Policy loss: 0.072553. Value loss: 0.034943. Entropy: 0.318664.\n",
      "episode: 539   score: 380.0  epsilon: 1.0    steps: 760  evaluation reward: 228.3\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1315: Policy loss: 0.128350. Value loss: 0.079971. Entropy: 0.315879.\n",
      "Iteration 1316: Policy loss: 0.134488. Value loss: 0.044302. Entropy: 0.316536.\n",
      "Iteration 1317: Policy loss: 0.129290. Value loss: 0.038231. Entropy: 0.315385.\n",
      "episode: 540   score: 210.0  epsilon: 1.0    steps: 232  evaluation reward: 227.8\n",
      "episode: 541   score: 155.0  epsilon: 1.0    steps: 416  evaluation reward: 226.35\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1318: Policy loss: 0.030071. Value loss: 0.029519. Entropy: 0.311130.\n",
      "Iteration 1319: Policy loss: 0.026901. Value loss: 0.014274. Entropy: 0.309982.\n",
      "Iteration 1320: Policy loss: 0.029226. Value loss: 0.012085. Entropy: 0.309641.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1321: Policy loss: -0.085409. Value loss: 0.040374. Entropy: 0.311179.\n",
      "Iteration 1322: Policy loss: -0.088980. Value loss: 0.023679. Entropy: 0.310247.\n",
      "Iteration 1323: Policy loss: -0.093277. Value loss: 0.020718. Entropy: 0.310032.\n",
      "episode: 542   score: 210.0  epsilon: 1.0    steps: 248  evaluation reward: 226.9\n",
      "episode: 543   score: 210.0  epsilon: 1.0    steps: 672  evaluation reward: 226.9\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1324: Policy loss: -0.314723. Value loss: 0.378962. Entropy: 0.303973.\n",
      "Iteration 1325: Policy loss: -0.340714. Value loss: 0.314203. Entropy: 0.305226.\n",
      "Iteration 1326: Policy loss: -0.322599. Value loss: 0.252264. Entropy: 0.305647.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1327: Policy loss: 0.135083. Value loss: 0.063253. Entropy: 0.311671.\n",
      "Iteration 1328: Policy loss: 0.122705. Value loss: 0.030690. Entropy: 0.309559.\n",
      "Iteration 1329: Policy loss: 0.129523. Value loss: 0.024040. Entropy: 0.309758.\n",
      "episode: 544   score: 180.0  epsilon: 1.0    steps: 248  evaluation reward: 227.9\n",
      "episode: 545   score: 410.0  epsilon: 1.0    steps: 696  evaluation reward: 226.75\n",
      "episode: 546   score: 180.0  epsilon: 1.0    steps: 816  evaluation reward: 227.0\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1330: Policy loss: -0.358849. Value loss: 0.485613. Entropy: 0.308266.\n",
      "Iteration 1331: Policy loss: -0.405154. Value loss: 0.213299. Entropy: 0.310523.\n",
      "Iteration 1332: Policy loss: -0.404782. Value loss: 0.163205. Entropy: 0.311864.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1333: Policy loss: 0.189765. Value loss: 0.116549. Entropy: 0.318666.\n",
      "Iteration 1334: Policy loss: 0.168565. Value loss: 0.047833. Entropy: 0.317227.\n",
      "Iteration 1335: Policy loss: 0.173652. Value loss: 0.039882. Entropy: 0.318819.\n",
      "episode: 547   score: 355.0  epsilon: 1.0    steps: 176  evaluation reward: 227.85\n",
      "episode: 548   score: 385.0  epsilon: 1.0    steps: 464  evaluation reward: 229.9\n",
      "Training network. lr: 0.000240. clip: 0.096009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1336: Policy loss: -0.188311. Value loss: 0.099911. Entropy: 0.312454.\n",
      "Iteration 1337: Policy loss: -0.195644. Value loss: 0.047685. Entropy: 0.313423.\n",
      "Iteration 1338: Policy loss: -0.197213. Value loss: 0.036661. Entropy: 0.313257.\n",
      "episode: 549   score: 105.0  epsilon: 1.0    steps: 432  evaluation reward: 227.15\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1339: Policy loss: -0.018762. Value loss: 0.051266. Entropy: 0.311965.\n",
      "Iteration 1340: Policy loss: -0.021670. Value loss: 0.024547. Entropy: 0.311910.\n",
      "Iteration 1341: Policy loss: -0.022929. Value loss: 0.018020. Entropy: 0.312344.\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1342: Policy loss: -0.040171. Value loss: 0.050105. Entropy: 0.313103.\n",
      "Iteration 1343: Policy loss: -0.041769. Value loss: 0.026972. Entropy: 0.312572.\n",
      "Iteration 1344: Policy loss: -0.040616. Value loss: 0.022075. Entropy: 0.313074.\n",
      "episode: 550   score: 180.0  epsilon: 1.0    steps: 352  evaluation reward: 227.15\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1345: Policy loss: -0.183430. Value loss: 0.262285. Entropy: 0.309248.\n",
      "Iteration 1346: Policy loss: -0.169342. Value loss: 0.172761. Entropy: 0.308892.\n",
      "Iteration 1347: Policy loss: -0.173830. Value loss: 0.150007. Entropy: 0.309039.\n",
      "now time :  2019-09-05 15:38:58.138690\n",
      "episode: 551   score: 135.0  epsilon: 1.0    steps: 424  evaluation reward: 226.4\n",
      "Training network. lr: 0.000240. clip: 0.096009\n",
      "Iteration 1348: Policy loss: 0.292055. Value loss: 0.099296. Entropy: 0.315616.\n",
      "Iteration 1349: Policy loss: 0.293174. Value loss: 0.049814. Entropy: 0.315921.\n",
      "Iteration 1350: Policy loss: 0.290885. Value loss: 0.036875. Entropy: 0.316422.\n",
      "episode: 552   score: 180.0  epsilon: 1.0    steps: 168  evaluation reward: 225.8\n",
      "episode: 553   score: 570.0  epsilon: 1.0    steps: 784  evaluation reward: 229.4\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1351: Policy loss: 0.224724. Value loss: 0.125855. Entropy: 0.317846.\n",
      "Iteration 1352: Policy loss: 0.211118. Value loss: 0.065069. Entropy: 0.317096.\n",
      "Iteration 1353: Policy loss: 0.213463. Value loss: 0.046145. Entropy: 0.317499.\n",
      "episode: 554   score: 185.0  epsilon: 1.0    steps: 424  evaluation reward: 229.15\n",
      "episode: 555   score: 135.0  epsilon: 1.0    steps: 720  evaluation reward: 226.7\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1354: Policy loss: -0.270067. Value loss: 0.381809. Entropy: 0.313634.\n",
      "Iteration 1355: Policy loss: -0.290407. Value loss: 0.217284. Entropy: 0.312309.\n",
      "Iteration 1356: Policy loss: -0.264578. Value loss: 0.147369. Entropy: 0.314074.\n",
      "episode: 556   score: 110.0  epsilon: 1.0    steps: 376  evaluation reward: 224.5\n",
      "episode: 557   score: 410.0  epsilon: 1.0    steps: 632  evaluation reward: 226.75\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1357: Policy loss: 0.010453. Value loss: 0.090257. Entropy: 0.311822.\n",
      "Iteration 1358: Policy loss: 0.011504. Value loss: 0.041415. Entropy: 0.312699.\n",
      "Iteration 1359: Policy loss: -0.000529. Value loss: 0.031580. Entropy: 0.311890.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1360: Policy loss: 0.051512. Value loss: 0.106944. Entropy: 0.315547.\n",
      "Iteration 1361: Policy loss: 0.045893. Value loss: 0.035342. Entropy: 0.314463.\n",
      "Iteration 1362: Policy loss: 0.042721. Value loss: 0.024045. Entropy: 0.314660.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1363: Policy loss: 0.187316. Value loss: 0.053225. Entropy: 0.314701.\n",
      "Iteration 1364: Policy loss: 0.183381. Value loss: 0.023252. Entropy: 0.313822.\n",
      "Iteration 1365: Policy loss: 0.186192. Value loss: 0.018790. Entropy: 0.313562.\n",
      "episode: 558   score: 120.0  epsilon: 1.0    steps: 344  evaluation reward: 225.85\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1366: Policy loss: 0.328670. Value loss: 0.069184. Entropy: 0.311532.\n",
      "Iteration 1367: Policy loss: 0.324673. Value loss: 0.030854. Entropy: 0.310241.\n",
      "Iteration 1368: Policy loss: 0.315015. Value loss: 0.021773. Entropy: 0.310186.\n",
      "episode: 559   score: 180.0  epsilon: 1.0    steps: 552  evaluation reward: 223.55\n",
      "episode: 560   score: 105.0  epsilon: 1.0    steps: 808  evaluation reward: 222.5\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1369: Policy loss: 0.114073. Value loss: 0.077054. Entropy: 0.309497.\n",
      "Iteration 1370: Policy loss: 0.110861. Value loss: 0.045271. Entropy: 0.307667.\n",
      "Iteration 1371: Policy loss: 0.106763. Value loss: 0.031896. Entropy: 0.308558.\n",
      "episode: 561   score: 210.0  epsilon: 1.0    steps: 120  evaluation reward: 220.0\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1372: Policy loss: 0.130286. Value loss: 0.084964. Entropy: 0.314235.\n",
      "Iteration 1373: Policy loss: 0.117481. Value loss: 0.038017. Entropy: 0.314916.\n",
      "Iteration 1374: Policy loss: 0.122551. Value loss: 0.028286. Entropy: 0.315409.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1375: Policy loss: -0.065398. Value loss: 0.129936. Entropy: 0.317701.\n",
      "Iteration 1376: Policy loss: -0.075450. Value loss: 0.075850. Entropy: 0.316615.\n",
      "Iteration 1377: Policy loss: -0.078264. Value loss: 0.053615. Entropy: 0.317206.\n",
      "episode: 562   score: 210.0  epsilon: 1.0    steps: 72  evaluation reward: 218.3\n",
      "episode: 563   score: 210.0  epsilon: 1.0    steps: 112  evaluation reward: 218.6\n",
      "episode: 564   score: 335.0  epsilon: 1.0    steps: 632  evaluation reward: 218.15\n",
      "episode: 565   score: 345.0  epsilon: 1.0    steps: 928  evaluation reward: 219.35\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1378: Policy loss: 0.040433. Value loss: 0.082607. Entropy: 0.305705.\n",
      "Iteration 1379: Policy loss: 0.030530. Value loss: 0.043369. Entropy: 0.304754.\n",
      "Iteration 1380: Policy loss: 0.034989. Value loss: 0.031382. Entropy: 0.305056.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1381: Policy loss: 0.081939. Value loss: 0.042378. Entropy: 0.311421.\n",
      "Iteration 1382: Policy loss: 0.079631. Value loss: 0.030245. Entropy: 0.310200.\n",
      "Iteration 1383: Policy loss: 0.083677. Value loss: 0.022289. Entropy: 0.310503.\n",
      "episode: 566   score: 120.0  epsilon: 1.0    steps: 424  evaluation reward: 218.45\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1384: Policy loss: 0.183093. Value loss: 0.054647. Entropy: 0.309023.\n",
      "Iteration 1385: Policy loss: 0.179990. Value loss: 0.027868. Entropy: 0.307214.\n",
      "Iteration 1386: Policy loss: 0.182067. Value loss: 0.023176. Entropy: 0.308116.\n",
      "episode: 567   score: 110.0  epsilon: 1.0    steps: 640  evaluation reward: 217.75\n",
      "episode: 568   score: 210.0  epsilon: 1.0    steps: 776  evaluation reward: 217.75\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1387: Policy loss: 0.080298. Value loss: 0.099709. Entropy: 0.311183.\n",
      "Iteration 1388: Policy loss: 0.086947. Value loss: 0.053774. Entropy: 0.310798.\n",
      "Iteration 1389: Policy loss: 0.082337. Value loss: 0.039154. Entropy: 0.310805.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1390: Policy loss: 0.079960. Value loss: 0.032987. Entropy: 0.307629.\n",
      "Iteration 1391: Policy loss: 0.077415. Value loss: 0.016918. Entropy: 0.308209.\n",
      "Iteration 1392: Policy loss: 0.077445. Value loss: 0.014604. Entropy: 0.307934.\n",
      "episode: 569   score: 215.0  epsilon: 1.0    steps: 472  evaluation reward: 217.8\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1393: Policy loss: -0.150441. Value loss: 0.373407. Entropy: 0.308022.\n",
      "Iteration 1394: Policy loss: -0.166429. Value loss: 0.260208. Entropy: 0.307820.\n",
      "Iteration 1395: Policy loss: -0.154493. Value loss: 0.166565. Entropy: 0.309630.\n",
      "episode: 570   score: 105.0  epsilon: 1.0    steps: 72  evaluation reward: 216.9\n",
      "episode: 571   score: 210.0  epsilon: 1.0    steps: 616  evaluation reward: 216.9\n",
      "episode: 572   score: 410.0  epsilon: 1.0    steps: 648  evaluation reward: 219.2\n",
      "episode: 573   score: 135.0  epsilon: 1.0    steps: 904  evaluation reward: 218.75\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1396: Policy loss: 0.201746. Value loss: 0.082435. Entropy: 0.311530.\n",
      "Iteration 1397: Policy loss: 0.203626. Value loss: 0.035596. Entropy: 0.310459.\n",
      "Iteration 1398: Policy loss: 0.199132. Value loss: 0.026925. Entropy: 0.310558.\n",
      "Training network. lr: 0.000240. clip: 0.095862\n",
      "Iteration 1399: Policy loss: 0.165593. Value loss: 0.050957. Entropy: 0.308923.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1400: Policy loss: 0.168765. Value loss: 0.029598. Entropy: 0.310243.\n",
      "Iteration 1401: Policy loss: 0.171736. Value loss: 0.021091. Entropy: 0.309909.\n",
      "episode: 574   score: 180.0  epsilon: 1.0    steps: 728  evaluation reward: 216.75\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1402: Policy loss: 0.198986. Value loss: 0.061248. Entropy: 0.307980.\n",
      "Iteration 1403: Policy loss: 0.193473. Value loss: 0.035644. Entropy: 0.307474.\n",
      "Iteration 1404: Policy loss: 0.196900. Value loss: 0.031844. Entropy: 0.307023.\n",
      "episode: 575   score: 135.0  epsilon: 1.0    steps: 928  evaluation reward: 216.3\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1405: Policy loss: 0.121817. Value loss: 0.041688. Entropy: 0.306735.\n",
      "Iteration 1406: Policy loss: 0.122748. Value loss: 0.024122. Entropy: 0.304812.\n",
      "Iteration 1407: Policy loss: 0.121967. Value loss: 0.018623. Entropy: 0.304565.\n",
      "episode: 576   score: 210.0  epsilon: 1.0    steps: 440  evaluation reward: 213.6\n",
      "episode: 577   score: 105.0  epsilon: 1.0    steps: 608  evaluation reward: 212.85\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1408: Policy loss: 0.112301. Value loss: 0.034533. Entropy: 0.306782.\n",
      "Iteration 1409: Policy loss: 0.116836. Value loss: 0.018432. Entropy: 0.305995.\n",
      "Iteration 1410: Policy loss: 0.113764. Value loss: 0.015147. Entropy: 0.306459.\n",
      "episode: 578   score: 105.0  epsilon: 1.0    steps: 664  evaluation reward: 212.8\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1411: Policy loss: 0.063318. Value loss: 0.044968. Entropy: 0.312799.\n",
      "Iteration 1412: Policy loss: 0.062391. Value loss: 0.032344. Entropy: 0.312794.\n",
      "Iteration 1413: Policy loss: 0.063946. Value loss: 0.027732. Entropy: 0.311899.\n",
      "episode: 579   score: 110.0  epsilon: 1.0    steps: 248  evaluation reward: 211.8\n",
      "episode: 580   score: 210.0  epsilon: 1.0    steps: 696  evaluation reward: 210.05\n",
      "episode: 581   score: 155.0  epsilon: 1.0    steps: 1016  evaluation reward: 209.0\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1414: Policy loss: 0.021780. Value loss: 0.054286. Entropy: 0.317238.\n",
      "Iteration 1415: Policy loss: 0.013792. Value loss: 0.029977. Entropy: 0.316756.\n",
      "Iteration 1416: Policy loss: 0.014284. Value loss: 0.025175. Entropy: 0.316527.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1417: Policy loss: -0.399430. Value loss: 0.313379. Entropy: 0.311253.\n",
      "Iteration 1418: Policy loss: -0.391751. Value loss: 0.152539. Entropy: 0.310835.\n",
      "Iteration 1419: Policy loss: -0.392244. Value loss: 0.105798. Entropy: 0.311307.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1420: Policy loss: -0.155896. Value loss: 0.087992. Entropy: 0.311455.\n",
      "Iteration 1421: Policy loss: -0.162359. Value loss: 0.041166. Entropy: 0.309649.\n",
      "Iteration 1422: Policy loss: -0.158429. Value loss: 0.025366. Entropy: 0.310059.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1423: Policy loss: 0.100241. Value loss: 0.074753. Entropy: 0.309965.\n",
      "Iteration 1424: Policy loss: 0.090701. Value loss: 0.024304. Entropy: 0.308709.\n",
      "Iteration 1425: Policy loss: 0.083530. Value loss: 0.017638. Entropy: 0.309473.\n",
      "episode: 582   score: 425.0  epsilon: 1.0    steps: 296  evaluation reward: 208.65\n",
      "episode: 583   score: 210.0  epsilon: 1.0    steps: 904  evaluation reward: 208.0\n",
      "episode: 584   score: 210.0  epsilon: 1.0    steps: 920  evaluation reward: 208.0\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1426: Policy loss: -0.261656. Value loss: 0.267161. Entropy: 0.310515.\n",
      "Iteration 1427: Policy loss: -0.234047. Value loss: 0.106070. Entropy: 0.308177.\n",
      "Iteration 1428: Policy loss: -0.250447. Value loss: 0.073796. Entropy: 0.310366.\n",
      "episode: 585   score: 215.0  epsilon: 1.0    steps: 312  evaluation reward: 207.0\n",
      "episode: 586   score: 380.0  epsilon: 1.0    steps: 872  evaluation reward: 209.75\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1429: Policy loss: -0.030638. Value loss: 0.244340. Entropy: 0.312880.\n",
      "Iteration 1430: Policy loss: -0.049519. Value loss: 0.117739. Entropy: 0.313687.\n",
      "Iteration 1431: Policy loss: -0.038562. Value loss: 0.080542. Entropy: 0.315440.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1432: Policy loss: 0.021379. Value loss: 0.135183. Entropy: 0.315820.\n",
      "Iteration 1433: Policy loss: 0.018852. Value loss: 0.060943. Entropy: 0.315892.\n",
      "Iteration 1434: Policy loss: 0.023835. Value loss: 0.043341. Entropy: 0.315728.\n",
      "episode: 587   score: 210.0  epsilon: 1.0    steps: 448  evaluation reward: 210.5\n",
      "episode: 588   score: 410.0  epsilon: 1.0    steps: 480  evaluation reward: 213.0\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1435: Policy loss: 0.066403. Value loss: 0.060025. Entropy: 0.307981.\n",
      "Iteration 1436: Policy loss: 0.064335. Value loss: 0.033662. Entropy: 0.308042.\n",
      "Iteration 1437: Policy loss: 0.064826. Value loss: 0.024816. Entropy: 0.307625.\n",
      "episode: 589   score: 300.0  epsilon: 1.0    steps: 240  evaluation reward: 214.8\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1438: Policy loss: 0.079193. Value loss: 0.045944. Entropy: 0.311113.\n",
      "Iteration 1439: Policy loss: 0.076344. Value loss: 0.022876. Entropy: 0.311349.\n",
      "Iteration 1440: Policy loss: 0.077701. Value loss: 0.021284. Entropy: 0.311320.\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1441: Policy loss: 0.066238. Value loss: 0.059042. Entropy: 0.309129.\n",
      "Iteration 1442: Policy loss: 0.063444. Value loss: 0.034756. Entropy: 0.308324.\n",
      "Iteration 1443: Policy loss: 0.060793. Value loss: 0.030487. Entropy: 0.308799.\n",
      "episode: 590   score: 155.0  epsilon: 1.0    steps: 608  evaluation reward: 216.2\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1444: Policy loss: 0.150096. Value loss: 0.081544. Entropy: 0.309967.\n",
      "Iteration 1445: Policy loss: 0.153424. Value loss: 0.030789. Entropy: 0.309296.\n",
      "Iteration 1446: Policy loss: 0.138810. Value loss: 0.023752. Entropy: 0.310057.\n",
      "episode: 591   score: 155.0  epsilon: 1.0    steps: 496  evaluation reward: 216.5\n",
      "episode: 592   score: 180.0  epsilon: 1.0    steps: 584  evaluation reward: 217.25\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1447: Policy loss: 0.152301. Value loss: 0.060106. Entropy: 0.308833.\n",
      "Iteration 1448: Policy loss: 0.146671. Value loss: 0.027652. Entropy: 0.308221.\n",
      "Iteration 1449: Policy loss: 0.142714. Value loss: 0.022994. Entropy: 0.308042.\n",
      "episode: 593   score: 285.0  epsilon: 1.0    steps: 480  evaluation reward: 216.3\n",
      "episode: 594   score: 105.0  epsilon: 1.0    steps: 488  evaluation reward: 213.25\n",
      "Training network. lr: 0.000239. clip: 0.095705\n",
      "Iteration 1450: Policy loss: 0.097583. Value loss: 0.053652. Entropy: 0.313102.\n",
      "Iteration 1451: Policy loss: 0.096927. Value loss: 0.039586. Entropy: 0.311920.\n",
      "Iteration 1452: Policy loss: 0.092176. Value loss: 0.030426. Entropy: 0.312277.\n",
      "episode: 595   score: 75.0  epsilon: 1.0    steps: 808  evaluation reward: 212.2\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1453: Policy loss: -0.117988. Value loss: 0.095607. Entropy: 0.307091.\n",
      "Iteration 1454: Policy loss: -0.113369. Value loss: 0.050878. Entropy: 0.308028.\n",
      "Iteration 1455: Policy loss: -0.116229. Value loss: 0.038765. Entropy: 0.308183.\n",
      "episode: 596   score: 160.0  epsilon: 1.0    steps: 464  evaluation reward: 212.45\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1456: Policy loss: -0.014424. Value loss: 0.039857. Entropy: 0.308811.\n",
      "Iteration 1457: Policy loss: -0.018116. Value loss: 0.013782. Entropy: 0.309421.\n",
      "Iteration 1458: Policy loss: -0.020381. Value loss: 0.010010. Entropy: 0.308654.\n",
      "episode: 597   score: 265.0  epsilon: 1.0    steps: 416  evaluation reward: 214.05\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1459: Policy loss: -0.067244. Value loss: 0.048817. Entropy: 0.304852.\n",
      "Iteration 1460: Policy loss: -0.074768. Value loss: 0.017919. Entropy: 0.304578.\n",
      "Iteration 1461: Policy loss: -0.073503. Value loss: 0.015767. Entropy: 0.303886.\n",
      "episode: 598   score: 285.0  epsilon: 1.0    steps: 536  evaluation reward: 212.8\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1462: Policy loss: -0.405250. Value loss: 0.265881. Entropy: 0.308179.\n",
      "Iteration 1463: Policy loss: -0.387691. Value loss: 0.084565. Entropy: 0.308534.\n",
      "Iteration 1464: Policy loss: -0.426539. Value loss: 0.054486. Entropy: 0.308296.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 599   score: 180.0  epsilon: 1.0    steps: 832  evaluation reward: 213.5\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1465: Policy loss: 0.154402. Value loss: 0.120820. Entropy: 0.307644.\n",
      "Iteration 1466: Policy loss: 0.145311. Value loss: 0.051990. Entropy: 0.307487.\n",
      "Iteration 1467: Policy loss: 0.149044. Value loss: 0.036868. Entropy: 0.307712.\n",
      "episode: 600   score: 180.0  epsilon: 1.0    steps: 224  evaluation reward: 213.2\n",
      "now time :  2019-09-05 15:46:27.088156\n",
      "episode: 601   score: 210.0  epsilon: 1.0    steps: 1016  evaluation reward: 213.5\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1468: Policy loss: -0.013089. Value loss: 0.063904. Entropy: 0.309140.\n",
      "Iteration 1469: Policy loss: -0.018970. Value loss: 0.030006. Entropy: 0.308689.\n",
      "Iteration 1470: Policy loss: -0.016967. Value loss: 0.025862. Entropy: 0.308779.\n",
      "episode: 602   score: 180.0  epsilon: 1.0    steps: 240  evaluation reward: 213.2\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1471: Policy loss: 0.079350. Value loss: 0.052911. Entropy: 0.308623.\n",
      "Iteration 1472: Policy loss: 0.082175. Value loss: 0.028490. Entropy: 0.307126.\n",
      "Iteration 1473: Policy loss: 0.080162. Value loss: 0.024158. Entropy: 0.307738.\n",
      "episode: 603   score: 180.0  epsilon: 1.0    steps: 1008  evaluation reward: 213.2\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1474: Policy loss: 0.057123. Value loss: 0.055414. Entropy: 0.311988.\n",
      "Iteration 1475: Policy loss: 0.054158. Value loss: 0.031571. Entropy: 0.311807.\n",
      "Iteration 1476: Policy loss: 0.053395. Value loss: 0.026314. Entropy: 0.312150.\n",
      "episode: 604   score: 470.0  epsilon: 1.0    steps: 600  evaluation reward: 215.8\n",
      "episode: 605   score: 135.0  epsilon: 1.0    steps: 648  evaluation reward: 214.9\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1477: Policy loss: 0.126144. Value loss: 0.041676. Entropy: 0.309277.\n",
      "Iteration 1478: Policy loss: 0.125235. Value loss: 0.018819. Entropy: 0.309735.\n",
      "Iteration 1479: Policy loss: 0.126573. Value loss: 0.012615. Entropy: 0.308874.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1480: Policy loss: 0.101425. Value loss: 0.036793. Entropy: 0.315612.\n",
      "Iteration 1481: Policy loss: 0.102683. Value loss: 0.022607. Entropy: 0.315997.\n",
      "Iteration 1482: Policy loss: 0.100119. Value loss: 0.017571. Entropy: 0.315971.\n",
      "episode: 606   score: 155.0  epsilon: 1.0    steps: 8  evaluation reward: 214.65\n",
      "episode: 607   score: 165.0  epsilon: 1.0    steps: 1024  evaluation reward: 215.1\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1483: Policy loss: 0.139017. Value loss: 0.045737. Entropy: 0.310988.\n",
      "Iteration 1484: Policy loss: 0.133715. Value loss: 0.021683. Entropy: 0.310854.\n",
      "Iteration 1485: Policy loss: 0.134794. Value loss: 0.015283. Entropy: 0.311272.\n",
      "episode: 608   score: 155.0  epsilon: 1.0    steps: 488  evaluation reward: 214.55\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1486: Policy loss: 0.006322. Value loss: 0.048546. Entropy: 0.313828.\n",
      "Iteration 1487: Policy loss: 0.006710. Value loss: 0.027960. Entropy: 0.313110.\n",
      "Iteration 1488: Policy loss: 0.002059. Value loss: 0.022437. Entropy: 0.313632.\n",
      "episode: 609   score: 210.0  epsilon: 1.0    steps: 352  evaluation reward: 213.8\n",
      "episode: 610   score: 210.0  epsilon: 1.0    steps: 784  evaluation reward: 214.85\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1489: Policy loss: -0.058335. Value loss: 0.040594. Entropy: 0.314037.\n",
      "Iteration 1490: Policy loss: -0.064644. Value loss: 0.025313. Entropy: 0.314016.\n",
      "Iteration 1491: Policy loss: -0.059538. Value loss: 0.019690. Entropy: 0.313715.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1492: Policy loss: 0.060419. Value loss: 0.043755. Entropy: 0.317296.\n",
      "Iteration 1493: Policy loss: 0.058666. Value loss: 0.021835. Entropy: 0.317289.\n",
      "Iteration 1494: Policy loss: 0.058333. Value loss: 0.017177. Entropy: 0.317006.\n",
      "episode: 611   score: 180.0  epsilon: 1.0    steps: 560  evaluation reward: 212.1\n",
      "episode: 612   score: 155.0  epsilon: 1.0    steps: 848  evaluation reward: 211.55\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1495: Policy loss: 0.021025. Value loss: 0.028237. Entropy: 0.313111.\n",
      "Iteration 1496: Policy loss: 0.024830. Value loss: 0.017307. Entropy: 0.313103.\n",
      "Iteration 1497: Policy loss: 0.022751. Value loss: 0.016144. Entropy: 0.314020.\n",
      "Training network. lr: 0.000239. clip: 0.095549\n",
      "Iteration 1498: Policy loss: 0.006995. Value loss: 0.032309. Entropy: 0.311739.\n",
      "Iteration 1499: Policy loss: 0.003876. Value loss: 0.018560. Entropy: 0.310794.\n",
      "Iteration 1500: Policy loss: 0.005708. Value loss: 0.017304. Entropy: 0.310893.\n",
      "episode: 613   score: 210.0  epsilon: 1.0    steps: 728  evaluation reward: 211.85\n",
      "episode: 614   score: 105.0  epsilon: 1.0    steps: 784  evaluation reward: 211.05\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1501: Policy loss: -0.407806. Value loss: 0.335262. Entropy: 0.319831.\n",
      "Iteration 1502: Policy loss: -0.415218. Value loss: 0.151097. Entropy: 0.319144.\n",
      "Iteration 1503: Policy loss: -0.400699. Value loss: 0.090769. Entropy: 0.320261.\n",
      "episode: 615   score: 225.0  epsilon: 1.0    steps: 136  evaluation reward: 211.5\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1504: Policy loss: -0.021754. Value loss: 0.078579. Entropy: 0.314363.\n",
      "Iteration 1505: Policy loss: -0.019428. Value loss: 0.029024. Entropy: 0.314772.\n",
      "Iteration 1506: Policy loss: -0.018921. Value loss: 0.021987. Entropy: 0.316208.\n",
      "episode: 616   score: 260.0  epsilon: 1.0    steps: 432  evaluation reward: 212.0\n",
      "episode: 617   score: 160.0  epsilon: 1.0    steps: 1016  evaluation reward: 212.05\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1507: Policy loss: 0.116173. Value loss: 0.083358. Entropy: 0.323777.\n",
      "Iteration 1508: Policy loss: 0.112952. Value loss: 0.033851. Entropy: 0.323555.\n",
      "Iteration 1509: Policy loss: 0.118445. Value loss: 0.027303. Entropy: 0.323434.\n",
      "episode: 618   score: 110.0  epsilon: 1.0    steps: 832  evaluation reward: 212.4\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1510: Policy loss: 0.025877. Value loss: 0.036756. Entropy: 0.317383.\n",
      "Iteration 1511: Policy loss: 0.023720. Value loss: 0.024291. Entropy: 0.316416.\n",
      "Iteration 1512: Policy loss: 0.020252. Value loss: 0.019410. Entropy: 0.316887.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1513: Policy loss: 0.010476. Value loss: 0.052740. Entropy: 0.315317.\n",
      "Iteration 1514: Policy loss: 0.004274. Value loss: 0.030012. Entropy: 0.315581.\n",
      "Iteration 1515: Policy loss: 0.006330. Value loss: 0.022234. Entropy: 0.314650.\n",
      "episode: 619   score: 110.0  epsilon: 1.0    steps: 968  evaluation reward: 211.15\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1516: Policy loss: -0.640196. Value loss: 0.385485. Entropy: 0.306684.\n",
      "Iteration 1517: Policy loss: -0.668710. Value loss: 0.187759. Entropy: 0.308630.\n",
      "Iteration 1518: Policy loss: -0.659507. Value loss: 0.131198. Entropy: 0.308327.\n",
      "episode: 620   score: 265.0  epsilon: 1.0    steps: 120  evaluation reward: 212.75\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1519: Policy loss: 0.010702. Value loss: 0.034952. Entropy: 0.309689.\n",
      "Iteration 1520: Policy loss: 0.010433. Value loss: 0.015841. Entropy: 0.310409.\n",
      "Iteration 1521: Policy loss: 0.008164. Value loss: 0.013906. Entropy: 0.309765.\n",
      "episode: 621   score: 210.0  epsilon: 1.0    steps: 608  evaluation reward: 212.0\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1522: Policy loss: -0.004761. Value loss: 0.097552. Entropy: 0.316900.\n",
      "Iteration 1523: Policy loss: -0.005974. Value loss: 0.047291. Entropy: 0.317112.\n",
      "Iteration 1524: Policy loss: -0.013013. Value loss: 0.035667. Entropy: 0.316208.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1525: Policy loss: 0.219614. Value loss: 0.130657. Entropy: 0.313026.\n",
      "Iteration 1526: Policy loss: 0.201760. Value loss: 0.045794. Entropy: 0.312273.\n",
      "Iteration 1527: Policy loss: 0.199016. Value loss: 0.033643. Entropy: 0.312245.\n",
      "episode: 622   score: 155.0  epsilon: 1.0    steps: 80  evaluation reward: 212.45\n",
      "episode: 623   score: 515.0  epsilon: 1.0    steps: 432  evaluation reward: 216.25\n",
      "Training network. lr: 0.000239. clip: 0.095401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1528: Policy loss: 0.048697. Value loss: 0.146396. Entropy: 0.311537.\n",
      "Iteration 1529: Policy loss: 0.041499. Value loss: 0.117827. Entropy: 0.311345.\n",
      "Iteration 1530: Policy loss: 0.053499. Value loss: 0.095325. Entropy: 0.311309.\n",
      "episode: 624   score: 210.0  epsilon: 1.0    steps: 304  evaluation reward: 216.05\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1531: Policy loss: -0.033787. Value loss: 0.077636. Entropy: 0.312716.\n",
      "Iteration 1532: Policy loss: -0.034780. Value loss: 0.037772. Entropy: 0.312031.\n",
      "Iteration 1533: Policy loss: -0.031790. Value loss: 0.029440. Entropy: 0.312737.\n",
      "episode: 625   score: 890.0  epsilon: 1.0    steps: 160  evaluation reward: 223.1\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1534: Policy loss: 0.071247. Value loss: 0.055659. Entropy: 0.309230.\n",
      "Iteration 1535: Policy loss: 0.068623. Value loss: 0.031527. Entropy: 0.308693.\n",
      "Iteration 1536: Policy loss: 0.068956. Value loss: 0.025179. Entropy: 0.309060.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1537: Policy loss: -0.066687. Value loss: 0.063234. Entropy: 0.311133.\n",
      "Iteration 1538: Policy loss: -0.071041. Value loss: 0.034556. Entropy: 0.310718.\n",
      "Iteration 1539: Policy loss: -0.079984. Value loss: 0.025450. Entropy: 0.311334.\n",
      "episode: 626   score: 245.0  epsilon: 1.0    steps: 448  evaluation reward: 223.75\n",
      "episode: 627   score: 250.0  epsilon: 1.0    steps: 480  evaluation reward: 224.0\n",
      "episode: 628   score: 345.0  epsilon: 1.0    steps: 616  evaluation reward: 225.65\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1540: Policy loss: 0.101454. Value loss: 0.064368. Entropy: 0.309559.\n",
      "Iteration 1541: Policy loss: 0.097177. Value loss: 0.032247. Entropy: 0.309516.\n",
      "Iteration 1542: Policy loss: 0.099091. Value loss: 0.025619. Entropy: 0.309167.\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1543: Policy loss: -0.137615. Value loss: 0.071318. Entropy: 0.313551.\n",
      "Iteration 1544: Policy loss: -0.141081. Value loss: 0.047542. Entropy: 0.314070.\n",
      "Iteration 1545: Policy loss: -0.143825. Value loss: 0.035779. Entropy: 0.314203.\n",
      "episode: 629   score: 280.0  epsilon: 1.0    steps: 704  evaluation reward: 226.85\n",
      "episode: 630   score: 210.0  epsilon: 1.0    steps: 760  evaluation reward: 226.35\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1546: Policy loss: 0.001996. Value loss: 0.078953. Entropy: 0.305225.\n",
      "Iteration 1547: Policy loss: -0.000397. Value loss: 0.049021. Entropy: 0.303479.\n",
      "Iteration 1548: Policy loss: 0.000098. Value loss: 0.036361. Entropy: 0.303796.\n",
      "episode: 631   score: 135.0  epsilon: 1.0    steps: 568  evaluation reward: 225.0\n",
      "episode: 632   score: 270.0  epsilon: 1.0    steps: 880  evaluation reward: 226.6\n",
      "Training network. lr: 0.000239. clip: 0.095401\n",
      "Iteration 1549: Policy loss: 0.046551. Value loss: 0.051371. Entropy: 0.307427.\n",
      "Iteration 1550: Policy loss: 0.047232. Value loss: 0.032790. Entropy: 0.307122.\n",
      "Iteration 1551: Policy loss: 0.042855. Value loss: 0.021162. Entropy: 0.307395.\n",
      "episode: 633   score: 180.0  epsilon: 1.0    steps: 456  evaluation reward: 225.8\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1552: Policy loss: -0.053282. Value loss: 0.050884. Entropy: 0.308507.\n",
      "Iteration 1553: Policy loss: -0.057938. Value loss: 0.021671. Entropy: 0.307990.\n",
      "Iteration 1554: Policy loss: -0.060317. Value loss: 0.014874. Entropy: 0.307853.\n",
      "episode: 634   score: 110.0  epsilon: 1.0    steps: 488  evaluation reward: 225.1\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1555: Policy loss: 0.031106. Value loss: 0.037170. Entropy: 0.303344.\n",
      "Iteration 1556: Policy loss: 0.029319. Value loss: 0.020343. Entropy: 0.302296.\n",
      "Iteration 1557: Policy loss: 0.024088. Value loss: 0.018536. Entropy: 0.302696.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1558: Policy loss: 0.123392. Value loss: 0.038938. Entropy: 0.308651.\n",
      "Iteration 1559: Policy loss: 0.127666. Value loss: 0.018443. Entropy: 0.308338.\n",
      "Iteration 1560: Policy loss: 0.124440. Value loss: 0.014370. Entropy: 0.307420.\n",
      "episode: 635   score: 180.0  epsilon: 1.0    steps: 584  evaluation reward: 225.1\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1561: Policy loss: -0.052517. Value loss: 0.067820. Entropy: 0.306383.\n",
      "Iteration 1562: Policy loss: -0.057566. Value loss: 0.038588. Entropy: 0.307051.\n",
      "Iteration 1563: Policy loss: -0.055333. Value loss: 0.025705. Entropy: 0.307426.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1564: Policy loss: -0.299274. Value loss: 0.325085. Entropy: 0.309433.\n",
      "Iteration 1565: Policy loss: -0.316696. Value loss: 0.189979. Entropy: 0.309298.\n",
      "Iteration 1566: Policy loss: -0.307051. Value loss: 0.107832. Entropy: 0.309828.\n",
      "episode: 636   score: 325.0  epsilon: 1.0    steps: 168  evaluation reward: 224.9\n",
      "episode: 637   score: 180.0  epsilon: 1.0    steps: 680  evaluation reward: 222.6\n",
      "episode: 638   score: 380.0  epsilon: 1.0    steps: 992  evaluation reward: 224.3\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1567: Policy loss: 0.299977. Value loss: 0.081106. Entropy: 0.310080.\n",
      "Iteration 1568: Policy loss: 0.293304. Value loss: 0.035481. Entropy: 0.309768.\n",
      "Iteration 1569: Policy loss: 0.299023. Value loss: 0.027284. Entropy: 0.310663.\n",
      "episode: 639   score: 180.0  epsilon: 1.0    steps: 200  evaluation reward: 222.3\n",
      "episode: 640   score: 155.0  epsilon: 1.0    steps: 792  evaluation reward: 221.75\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1570: Policy loss: 0.168370. Value loss: 0.098193. Entropy: 0.312200.\n",
      "Iteration 1571: Policy loss: 0.167966. Value loss: 0.046726. Entropy: 0.312162.\n",
      "Iteration 1572: Policy loss: 0.161402. Value loss: 0.034483. Entropy: 0.311564.\n",
      "episode: 641   score: 180.0  epsilon: 1.0    steps: 1000  evaluation reward: 222.0\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1573: Policy loss: 0.056750. Value loss: 0.084550. Entropy: 0.306052.\n",
      "Iteration 1574: Policy loss: 0.046122. Value loss: 0.040381. Entropy: 0.306100.\n",
      "Iteration 1575: Policy loss: 0.054286. Value loss: 0.031399. Entropy: 0.306424.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1576: Policy loss: -0.238052. Value loss: 0.301075. Entropy: 0.309868.\n",
      "Iteration 1577: Policy loss: -0.256994. Value loss: 0.216662. Entropy: 0.308839.\n",
      "Iteration 1578: Policy loss: -0.252542. Value loss: 0.187333. Entropy: 0.309183.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1579: Policy loss: -0.172392. Value loss: 0.080416. Entropy: 0.303718.\n",
      "Iteration 1580: Policy loss: -0.175091. Value loss: 0.039305. Entropy: 0.301738.\n",
      "Iteration 1581: Policy loss: -0.182659. Value loss: 0.026545. Entropy: 0.305746.\n",
      "episode: 642   score: 620.0  epsilon: 1.0    steps: 224  evaluation reward: 226.1\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1582: Policy loss: -0.295697. Value loss: 0.332597. Entropy: 0.311359.\n",
      "Iteration 1583: Policy loss: -0.330056. Value loss: 0.225663. Entropy: 0.310815.\n",
      "Iteration 1584: Policy loss: -0.326893. Value loss: 0.126795. Entropy: 0.310552.\n",
      "episode: 643   score: 290.0  epsilon: 1.0    steps: 160  evaluation reward: 226.9\n",
      "episode: 644   score: 155.0  epsilon: 1.0    steps: 200  evaluation reward: 226.65\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1585: Policy loss: 0.215067. Value loss: 0.125928. Entropy: 0.315331.\n",
      "Iteration 1586: Policy loss: 0.211832. Value loss: 0.059538. Entropy: 0.314409.\n",
      "Iteration 1587: Policy loss: 0.207612. Value loss: 0.046122. Entropy: 0.313862.\n",
      "episode: 645   score: 410.0  epsilon: 1.0    steps: 120  evaluation reward: 226.65\n",
      "episode: 646   score: 155.0  epsilon: 1.0    steps: 224  evaluation reward: 226.4\n",
      "episode: 647   score: 135.0  epsilon: 1.0    steps: 488  evaluation reward: 224.2\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1588: Policy loss: 0.153497. Value loss: 0.100918. Entropy: 0.315042.\n",
      "Iteration 1589: Policy loss: 0.148426. Value loss: 0.046001. Entropy: 0.314221.\n",
      "Iteration 1590: Policy loss: 0.143213. Value loss: 0.035863. Entropy: 0.314580.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1591: Policy loss: 0.046031. Value loss: 0.102623. Entropy: 0.314793.\n",
      "Iteration 1592: Policy loss: 0.038669. Value loss: 0.061427. Entropy: 0.314516.\n",
      "Iteration 1593: Policy loss: 0.039648. Value loss: 0.046489. Entropy: 0.313870.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 648   score: 180.0  epsilon: 1.0    steps: 696  evaluation reward: 222.15\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1594: Policy loss: 0.075551. Value loss: 0.112224. Entropy: 0.311709.\n",
      "Iteration 1595: Policy loss: 0.066678. Value loss: 0.089131. Entropy: 0.309093.\n",
      "Iteration 1596: Policy loss: 0.062639. Value loss: 0.081407. Entropy: 0.310534.\n",
      "episode: 649   score: 105.0  epsilon: 1.0    steps: 656  evaluation reward: 222.15\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1597: Policy loss: -0.086287. Value loss: 0.052516. Entropy: 0.310729.\n",
      "Iteration 1598: Policy loss: -0.089890. Value loss: 0.028958. Entropy: 0.310565.\n",
      "Iteration 1599: Policy loss: -0.093391. Value loss: 0.020048. Entropy: 0.310236.\n",
      "Training network. lr: 0.000238. clip: 0.095245\n",
      "Iteration 1600: Policy loss: -0.166700. Value loss: 0.066960. Entropy: 0.308742.\n",
      "Iteration 1601: Policy loss: -0.167217. Value loss: 0.039448. Entropy: 0.310391.\n",
      "Iteration 1602: Policy loss: -0.162142. Value loss: 0.027681. Entropy: 0.309931.\n",
      "episode: 650   score: 155.0  epsilon: 1.0    steps: 504  evaluation reward: 221.9\n",
      "now time :  2019-09-05 15:54:48.650474\n",
      "episode: 651   score: 270.0  epsilon: 1.0    steps: 664  evaluation reward: 223.25\n",
      "episode: 652   score: 420.0  epsilon: 1.0    steps: 776  evaluation reward: 225.65\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1603: Policy loss: -0.189411. Value loss: 0.253649. Entropy: 0.308566.\n",
      "Iteration 1604: Policy loss: -0.205236. Value loss: 0.176010. Entropy: 0.309541.\n",
      "Iteration 1605: Policy loss: -0.217282. Value loss: 0.110484. Entropy: 0.309435.\n",
      "episode: 653   score: 470.0  epsilon: 1.0    steps: 584  evaluation reward: 224.65\n",
      "episode: 654   score: 410.0  epsilon: 1.0    steps: 904  evaluation reward: 226.9\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1606: Policy loss: 0.128297. Value loss: 0.073854. Entropy: 0.305945.\n",
      "Iteration 1607: Policy loss: 0.126800. Value loss: 0.035837. Entropy: 0.305052.\n",
      "Iteration 1608: Policy loss: 0.128812. Value loss: 0.026246. Entropy: 0.306774.\n",
      "episode: 655   score: 270.0  epsilon: 1.0    steps: 744  evaluation reward: 228.25\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1609: Policy loss: -0.162907. Value loss: 0.311788. Entropy: 0.304202.\n",
      "Iteration 1610: Policy loss: -0.163451. Value loss: 0.109777. Entropy: 0.301862.\n",
      "Iteration 1611: Policy loss: -0.163510. Value loss: 0.089280. Entropy: 0.302218.\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1612: Policy loss: 0.094141. Value loss: 0.085351. Entropy: 0.309119.\n",
      "Iteration 1613: Policy loss: 0.087853. Value loss: 0.041312. Entropy: 0.309185.\n",
      "Iteration 1614: Policy loss: 0.090174. Value loss: 0.033832. Entropy: 0.308717.\n",
      "episode: 656   score: 410.0  epsilon: 1.0    steps: 256  evaluation reward: 231.25\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1615: Policy loss: 0.117854. Value loss: 0.034877. Entropy: 0.298360.\n",
      "Iteration 1616: Policy loss: 0.122035. Value loss: 0.020164. Entropy: 0.295579.\n",
      "Iteration 1617: Policy loss: 0.122978. Value loss: 0.016900. Entropy: 0.296211.\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1618: Policy loss: -0.044082. Value loss: 0.053393. Entropy: 0.303426.\n",
      "Iteration 1619: Policy loss: -0.040108. Value loss: 0.028255. Entropy: 0.302446.\n",
      "Iteration 1620: Policy loss: -0.045199. Value loss: 0.019429. Entropy: 0.297463.\n",
      "episode: 657   score: 230.0  epsilon: 1.0    steps: 160  evaluation reward: 229.45\n",
      "episode: 658   score: 125.0  epsilon: 1.0    steps: 736  evaluation reward: 229.5\n",
      "episode: 659   score: 135.0  epsilon: 1.0    steps: 768  evaluation reward: 229.05\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1621: Policy loss: 0.002703. Value loss: 0.104898. Entropy: 0.298440.\n",
      "Iteration 1622: Policy loss: -0.005544. Value loss: 0.055103. Entropy: 0.295803.\n",
      "Iteration 1623: Policy loss: -0.016712. Value loss: 0.043665. Entropy: 0.294715.\n",
      "episode: 660   score: 210.0  epsilon: 1.0    steps: 72  evaluation reward: 230.1\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1624: Policy loss: 0.156908. Value loss: 0.103376. Entropy: 0.300790.\n",
      "Iteration 1625: Policy loss: 0.148780. Value loss: 0.061064. Entropy: 0.301344.\n",
      "Iteration 1626: Policy loss: 0.150373. Value loss: 0.050419. Entropy: 0.299191.\n",
      "episode: 661   score: 240.0  epsilon: 1.0    steps: 408  evaluation reward: 230.4\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1627: Policy loss: 0.106978. Value loss: 0.081975. Entropy: 0.305234.\n",
      "Iteration 1628: Policy loss: 0.109043. Value loss: 0.044871. Entropy: 0.306370.\n",
      "Iteration 1629: Policy loss: 0.109671. Value loss: 0.036775. Entropy: 0.306222.\n",
      "episode: 662   score: 180.0  epsilon: 1.0    steps: 440  evaluation reward: 230.1\n",
      "episode: 663   score: 260.0  epsilon: 1.0    steps: 656  evaluation reward: 230.6\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1630: Policy loss: 0.201269. Value loss: 0.062965. Entropy: 0.307041.\n",
      "Iteration 1631: Policy loss: 0.201808. Value loss: 0.032046. Entropy: 0.307045.\n",
      "Iteration 1632: Policy loss: 0.205408. Value loss: 0.025505. Entropy: 0.306163.\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1633: Policy loss: 0.038708. Value loss: 0.069042. Entropy: 0.301537.\n",
      "Iteration 1634: Policy loss: 0.036423. Value loss: 0.046068. Entropy: 0.301073.\n",
      "Iteration 1635: Policy loss: 0.036348. Value loss: 0.031843. Entropy: 0.301248.\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1636: Policy loss: -0.008317. Value loss: 0.060720. Entropy: 0.303498.\n",
      "Iteration 1637: Policy loss: -0.012749. Value loss: 0.035361. Entropy: 0.303859.\n",
      "Iteration 1638: Policy loss: -0.012619. Value loss: 0.026294. Entropy: 0.304704.\n",
      "episode: 664   score: 105.0  epsilon: 1.0    steps: 32  evaluation reward: 228.3\n",
      "episode: 665   score: 180.0  epsilon: 1.0    steps: 688  evaluation reward: 226.65\n",
      "episode: 666   score: 265.0  epsilon: 1.0    steps: 760  evaluation reward: 228.1\n",
      "episode: 667   score: 180.0  epsilon: 1.0    steps: 928  evaluation reward: 228.8\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1639: Policy loss: -0.145960. Value loss: 0.097122. Entropy: 0.306566.\n",
      "Iteration 1640: Policy loss: -0.147146. Value loss: 0.060103. Entropy: 0.307938.\n",
      "Iteration 1641: Policy loss: -0.151906. Value loss: 0.046939. Entropy: 0.308630.\n",
      "episode: 668   score: 210.0  epsilon: 1.0    steps: 136  evaluation reward: 228.8\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1642: Policy loss: 0.070017. Value loss: 0.049968. Entropy: 0.303028.\n",
      "Iteration 1643: Policy loss: 0.069018. Value loss: 0.027712. Entropy: 0.300840.\n",
      "Iteration 1644: Policy loss: 0.062821. Value loss: 0.021512. Entropy: 0.301300.\n",
      "episode: 669   score: 155.0  epsilon: 1.0    steps: 784  evaluation reward: 228.2\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1645: Policy loss: -0.036525. Value loss: 0.112480. Entropy: 0.307356.\n",
      "Iteration 1646: Policy loss: -0.047151. Value loss: 0.062139. Entropy: 0.307731.\n",
      "Iteration 1647: Policy loss: -0.050983. Value loss: 0.047600. Entropy: 0.307268.\n",
      "Training network. lr: 0.000238. clip: 0.095088\n",
      "Iteration 1648: Policy loss: -0.762041. Value loss: 0.615175. Entropy: 0.300529.\n",
      "Iteration 1649: Policy loss: -0.710505. Value loss: 0.283118. Entropy: 0.298770.\n",
      "Iteration 1650: Policy loss: -0.764041. Value loss: 0.123668. Entropy: 0.301241.\n",
      "episode: 670   score: 210.0  epsilon: 1.0    steps: 152  evaluation reward: 229.25\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1651: Policy loss: 0.153904. Value loss: 0.102618. Entropy: 0.289025.\n",
      "Iteration 1652: Policy loss: 0.148144. Value loss: 0.047648. Entropy: 0.285497.\n",
      "Iteration 1653: Policy loss: 0.147835. Value loss: 0.038709. Entropy: 0.280304.\n",
      "episode: 671   score: 240.0  epsilon: 1.0    steps: 112  evaluation reward: 229.55\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1654: Policy loss: -0.297101. Value loss: 0.237309. Entropy: 0.309808.\n",
      "Iteration 1655: Policy loss: -0.306960. Value loss: 0.102366. Entropy: 0.308056.\n",
      "Iteration 1656: Policy loss: -0.315420. Value loss: 0.072268. Entropy: 0.307630.\n",
      "episode: 672   score: 210.0  epsilon: 1.0    steps: 288  evaluation reward: 227.55\n",
      "Training network. lr: 0.000237. clip: 0.094940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1657: Policy loss: 0.258656. Value loss: 0.142841. Entropy: 0.305459.\n",
      "Iteration 1658: Policy loss: 0.256712. Value loss: 0.070056. Entropy: 0.306692.\n",
      "Iteration 1659: Policy loss: 0.247652. Value loss: 0.046741. Entropy: 0.305143.\n",
      "episode: 673   score: 355.0  epsilon: 1.0    steps: 184  evaluation reward: 229.75\n",
      "episode: 674   score: 410.0  epsilon: 1.0    steps: 280  evaluation reward: 232.05\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1660: Policy loss: 0.337248. Value loss: 0.079010. Entropy: 0.300479.\n",
      "Iteration 1661: Policy loss: 0.337340. Value loss: 0.037760. Entropy: 0.299633.\n",
      "Iteration 1662: Policy loss: 0.322666. Value loss: 0.029281. Entropy: 0.299157.\n",
      "episode: 675   score: 440.0  epsilon: 1.0    steps: 504  evaluation reward: 235.1\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1663: Policy loss: -0.040488. Value loss: 0.143035. Entropy: 0.300727.\n",
      "Iteration 1664: Policy loss: -0.050415. Value loss: 0.070190. Entropy: 0.302345.\n",
      "Iteration 1665: Policy loss: -0.052470. Value loss: 0.049026. Entropy: 0.301363.\n",
      "episode: 676   score: 215.0  epsilon: 1.0    steps: 576  evaluation reward: 235.15\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1666: Policy loss: -0.260882. Value loss: 0.362077. Entropy: 0.283619.\n",
      "Iteration 1667: Policy loss: -0.265618. Value loss: 0.228542. Entropy: 0.283010.\n",
      "Iteration 1668: Policy loss: -0.279038. Value loss: 0.158300. Entropy: 0.281473.\n",
      "episode: 677   score: 135.0  epsilon: 1.0    steps: 88  evaluation reward: 235.45\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1669: Policy loss: -0.016287. Value loss: 0.216785. Entropy: 0.306868.\n",
      "Iteration 1670: Policy loss: -0.038150. Value loss: 0.141270. Entropy: 0.305943.\n",
      "Iteration 1671: Policy loss: -0.030877. Value loss: 0.101144. Entropy: 0.304675.\n",
      "episode: 678   score: 105.0  epsilon: 1.0    steps: 808  evaluation reward: 235.45\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1672: Policy loss: 0.208719. Value loss: 0.190375. Entropy: 0.299282.\n",
      "Iteration 1673: Policy loss: 0.196525. Value loss: 0.078756. Entropy: 0.300630.\n",
      "Iteration 1674: Policy loss: 0.206629. Value loss: 0.050990. Entropy: 0.298566.\n",
      "episode: 679   score: 395.0  epsilon: 1.0    steps: 392  evaluation reward: 238.3\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1675: Policy loss: -0.179486. Value loss: 0.147433. Entropy: 0.306013.\n",
      "Iteration 1676: Policy loss: -0.185357. Value loss: 0.064374. Entropy: 0.306781.\n",
      "Iteration 1677: Policy loss: -0.190553. Value loss: 0.046184. Entropy: 0.306635.\n",
      "episode: 680   score: 440.0  epsilon: 1.0    steps: 912  evaluation reward: 240.6\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1678: Policy loss: 0.311304. Value loss: 0.114815. Entropy: 0.307144.\n",
      "Iteration 1679: Policy loss: 0.301298. Value loss: 0.050633. Entropy: 0.306960.\n",
      "Iteration 1680: Policy loss: 0.296710. Value loss: 0.035107. Entropy: 0.306461.\n",
      "episode: 681   score: 265.0  epsilon: 1.0    steps: 1016  evaluation reward: 241.7\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1681: Policy loss: 0.198527. Value loss: 0.151763. Entropy: 0.295830.\n",
      "Iteration 1682: Policy loss: 0.198718. Value loss: 0.073790. Entropy: 0.297064.\n",
      "Iteration 1683: Policy loss: 0.199388. Value loss: 0.055615. Entropy: 0.297716.\n",
      "episode: 682   score: 240.0  epsilon: 1.0    steps: 64  evaluation reward: 239.85\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1684: Policy loss: -0.125895. Value loss: 0.218178. Entropy: 0.305533.\n",
      "Iteration 1685: Policy loss: -0.119343. Value loss: 0.111870. Entropy: 0.306253.\n",
      "Iteration 1686: Policy loss: -0.146577. Value loss: 0.082397. Entropy: 0.306194.\n",
      "episode: 683   score: 155.0  epsilon: 1.0    steps: 144  evaluation reward: 239.3\n",
      "episode: 684   score: 295.0  epsilon: 1.0    steps: 328  evaluation reward: 240.15\n",
      "episode: 685   score: 465.0  epsilon: 1.0    steps: 488  evaluation reward: 242.65\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1687: Policy loss: 0.316684. Value loss: 0.214110. Entropy: 0.308909.\n",
      "Iteration 1688: Policy loss: 0.299795. Value loss: 0.109513. Entropy: 0.309230.\n",
      "Iteration 1689: Policy loss: 0.295199. Value loss: 0.078737. Entropy: 0.308333.\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1690: Policy loss: 0.073405. Value loss: 0.154341. Entropy: 0.303672.\n",
      "Iteration 1691: Policy loss: 0.056331. Value loss: 0.080977. Entropy: 0.303299.\n",
      "Iteration 1692: Policy loss: 0.053009. Value loss: 0.054009. Entropy: 0.302418.\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1693: Policy loss: -0.031041. Value loss: 0.130805. Entropy: 0.304601.\n",
      "Iteration 1694: Policy loss: -0.032247. Value loss: 0.054817. Entropy: 0.307246.\n",
      "Iteration 1695: Policy loss: -0.032930. Value loss: 0.038875. Entropy: 0.305959.\n",
      "episode: 686   score: 75.0  epsilon: 1.0    steps: 160  evaluation reward: 239.6\n",
      "episode: 687   score: 225.0  epsilon: 1.0    steps: 552  evaluation reward: 239.75\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1696: Policy loss: 0.400754. Value loss: 0.092216. Entropy: 0.311003.\n",
      "Iteration 1697: Policy loss: 0.393488. Value loss: 0.042151. Entropy: 0.310467.\n",
      "Iteration 1698: Policy loss: 0.388305. Value loss: 0.026982. Entropy: 0.310097.\n",
      "episode: 688   score: 275.0  epsilon: 1.0    steps: 256  evaluation reward: 238.4\n",
      "Training network. lr: 0.000237. clip: 0.094940\n",
      "Iteration 1699: Policy loss: 0.140022. Value loss: 0.089097. Entropy: 0.308716.\n",
      "Iteration 1700: Policy loss: 0.137149. Value loss: 0.050410. Entropy: 0.308335.\n",
      "Iteration 1701: Policy loss: 0.133078. Value loss: 0.031145. Entropy: 0.307622.\n",
      "episode: 689   score: 215.0  epsilon: 1.0    steps: 16  evaluation reward: 237.55\n",
      "episode: 690   score: 220.0  epsilon: 1.0    steps: 744  evaluation reward: 238.2\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1702: Policy loss: 0.089007. Value loss: 0.055220. Entropy: 0.303932.\n",
      "Iteration 1703: Policy loss: 0.086576. Value loss: 0.026603. Entropy: 0.303612.\n",
      "Iteration 1704: Policy loss: 0.087722. Value loss: 0.019722. Entropy: 0.303947.\n",
      "episode: 691   score: 200.0  epsilon: 1.0    steps: 448  evaluation reward: 238.65\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1705: Policy loss: -0.027125. Value loss: 0.100504. Entropy: 0.305271.\n",
      "Iteration 1706: Policy loss: -0.035064. Value loss: 0.053899. Entropy: 0.302643.\n",
      "Iteration 1707: Policy loss: -0.041062. Value loss: 0.040431. Entropy: 0.303200.\n",
      "episode: 692   score: 95.0  epsilon: 1.0    steps: 16  evaluation reward: 237.8\n",
      "episode: 693   score: 215.0  epsilon: 1.0    steps: 344  evaluation reward: 237.1\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1708: Policy loss: 0.285953. Value loss: 0.094558. Entropy: 0.305746.\n",
      "Iteration 1709: Policy loss: 0.289384. Value loss: 0.041629. Entropy: 0.304393.\n",
      "Iteration 1710: Policy loss: 0.284732. Value loss: 0.031633. Entropy: 0.305581.\n",
      "episode: 694   score: 115.0  epsilon: 1.0    steps: 432  evaluation reward: 237.2\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1711: Policy loss: -0.021406. Value loss: 0.092388. Entropy: 0.305741.\n",
      "Iteration 1712: Policy loss: -0.033344. Value loss: 0.049626. Entropy: 0.306943.\n",
      "Iteration 1713: Policy loss: -0.027027. Value loss: 0.039496. Entropy: 0.306687.\n",
      "episode: 695   score: 125.0  epsilon: 1.0    steps: 160  evaluation reward: 237.7\n",
      "episode: 696   score: 110.0  epsilon: 1.0    steps: 1024  evaluation reward: 237.2\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1714: Policy loss: 0.153579. Value loss: 0.107557. Entropy: 0.303506.\n",
      "Iteration 1715: Policy loss: 0.151926. Value loss: 0.047824. Entropy: 0.303198.\n",
      "Iteration 1716: Policy loss: 0.146610. Value loss: 0.034880. Entropy: 0.303855.\n",
      "episode: 697   score: 55.0  epsilon: 1.0    steps: 160  evaluation reward: 235.1\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1717: Policy loss: 0.030054. Value loss: 0.122010. Entropy: 0.306417.\n",
      "Iteration 1718: Policy loss: 0.026406. Value loss: 0.055723. Entropy: 0.305095.\n",
      "Iteration 1719: Policy loss: 0.019326. Value loss: 0.039337. Entropy: 0.304239.\n",
      "episode: 698   score: 315.0  epsilon: 1.0    steps: 840  evaluation reward: 235.4\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1720: Policy loss: 0.037517. Value loss: 0.108502. Entropy: 0.302116.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1721: Policy loss: 0.027552. Value loss: 0.055792. Entropy: 0.302339.\n",
      "Iteration 1722: Policy loss: 0.025511. Value loss: 0.034373. Entropy: 0.303455.\n",
      "episode: 699   score: 95.0  epsilon: 1.0    steps: 840  evaluation reward: 234.55\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1723: Policy loss: -0.299452. Value loss: 0.150055. Entropy: 0.297371.\n",
      "Iteration 1724: Policy loss: -0.300907. Value loss: 0.082464. Entropy: 0.298506.\n",
      "Iteration 1725: Policy loss: -0.307422. Value loss: 0.065561. Entropy: 0.298976.\n",
      "episode: 700   score: 180.0  epsilon: 1.0    steps: 32  evaluation reward: 234.55\n",
      "now time :  2019-09-05 16:02:24.833556\n",
      "episode: 701   score: 215.0  epsilon: 1.0    steps: 168  evaluation reward: 234.6\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1726: Policy loss: -0.116625. Value loss: 0.088813. Entropy: 0.306035.\n",
      "Iteration 1727: Policy loss: -0.115225. Value loss: 0.045088. Entropy: 0.305847.\n",
      "Iteration 1728: Policy loss: -0.114435. Value loss: 0.031626. Entropy: 0.306666.\n",
      "episode: 702   score: 260.0  epsilon: 1.0    steps: 616  evaluation reward: 235.4\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1729: Policy loss: 0.110943. Value loss: 0.135704. Entropy: 0.306979.\n",
      "Iteration 1730: Policy loss: 0.110486. Value loss: 0.073709. Entropy: 0.307663.\n",
      "Iteration 1731: Policy loss: 0.102599. Value loss: 0.054700. Entropy: 0.306772.\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1732: Policy loss: -0.255529. Value loss: 0.349340. Entropy: 0.309466.\n",
      "Iteration 1733: Policy loss: -0.275949. Value loss: 0.162611. Entropy: 0.307652.\n",
      "Iteration 1734: Policy loss: -0.281393. Value loss: 0.102253. Entropy: 0.307847.\n",
      "episode: 703   score: 315.0  epsilon: 1.0    steps: 456  evaluation reward: 236.75\n",
      "episode: 704   score: 365.0  epsilon: 1.0    steps: 576  evaluation reward: 235.7\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1735: Policy loss: -0.065561. Value loss: 0.306941. Entropy: 0.304077.\n",
      "Iteration 1736: Policy loss: -0.091221. Value loss: 0.121823. Entropy: 0.304090.\n",
      "Iteration 1737: Policy loss: -0.098203. Value loss: 0.080201. Entropy: 0.304252.\n",
      "episode: 705   score: 50.0  epsilon: 1.0    steps: 488  evaluation reward: 234.85\n",
      "episode: 706   score: 190.0  epsilon: 1.0    steps: 736  evaluation reward: 235.2\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1738: Policy loss: -0.056268. Value loss: 0.404562. Entropy: 0.302699.\n",
      "Iteration 1739: Policy loss: -0.091342. Value loss: 0.245596. Entropy: 0.304239.\n",
      "Iteration 1740: Policy loss: -0.081716. Value loss: 0.181190. Entropy: 0.303903.\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1741: Policy loss: 0.032579. Value loss: 0.120915. Entropy: 0.308180.\n",
      "Iteration 1742: Policy loss: 0.029882. Value loss: 0.057454. Entropy: 0.308044.\n",
      "Iteration 1743: Policy loss: 0.019227. Value loss: 0.041680. Entropy: 0.307424.\n",
      "episode: 707   score: 485.0  epsilon: 1.0    steps: 312  evaluation reward: 238.4\n",
      "episode: 708   score: 55.0  epsilon: 1.0    steps: 552  evaluation reward: 237.4\n",
      "episode: 709   score: 400.0  epsilon: 1.0    steps: 608  evaluation reward: 239.3\n",
      "episode: 710   score: 210.0  epsilon: 1.0    steps: 656  evaluation reward: 239.3\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1744: Policy loss: 0.123603. Value loss: 0.051608. Entropy: 0.305553.\n",
      "Iteration 1745: Policy loss: 0.116504. Value loss: 0.028671. Entropy: 0.305073.\n",
      "Iteration 1746: Policy loss: 0.116743. Value loss: 0.021074. Entropy: 0.305558.\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1747: Policy loss: 0.031663. Value loss: 0.131692. Entropy: 0.308052.\n",
      "Iteration 1748: Policy loss: 0.025583. Value loss: 0.083271. Entropy: 0.307834.\n",
      "Iteration 1749: Policy loss: 0.018837. Value loss: 0.069567. Entropy: 0.307771.\n",
      "episode: 711   score: 135.0  epsilon: 1.0    steps: 912  evaluation reward: 238.85\n",
      "Training network. lr: 0.000237. clip: 0.094784\n",
      "Iteration 1750: Policy loss: 0.082607. Value loss: 0.099058. Entropy: 0.309415.\n",
      "Iteration 1751: Policy loss: 0.076418. Value loss: 0.047084. Entropy: 0.308212.\n",
      "Iteration 1752: Policy loss: 0.072986. Value loss: 0.038449. Entropy: 0.307826.\n",
      "episode: 712   score: 15.0  epsilon: 1.0    steps: 184  evaluation reward: 237.45\n",
      "episode: 713   score: 50.0  epsilon: 1.0    steps: 656  evaluation reward: 235.85\n",
      "episode: 714   score: 110.0  epsilon: 1.0    steps: 680  evaluation reward: 235.9\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1753: Policy loss: 0.089110. Value loss: 0.031393. Entropy: 0.306359.\n",
      "Iteration 1754: Policy loss: 0.086165. Value loss: 0.018111. Entropy: 0.306165.\n",
      "Iteration 1755: Policy loss: 0.082719. Value loss: 0.012948. Entropy: 0.305826.\n",
      "episode: 715   score: 145.0  epsilon: 1.0    steps: 584  evaluation reward: 235.1\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1756: Policy loss: -0.268434. Value loss: 0.132663. Entropy: 0.306893.\n",
      "Iteration 1757: Policy loss: -0.263122. Value loss: 0.067108. Entropy: 0.306606.\n",
      "Iteration 1758: Policy loss: -0.278647. Value loss: 0.051218. Entropy: 0.307190.\n",
      "episode: 716   score: 210.0  epsilon: 1.0    steps: 248  evaluation reward: 234.6\n",
      "episode: 717   score: 75.0  epsilon: 1.0    steps: 824  evaluation reward: 233.75\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1759: Policy loss: -0.049499. Value loss: 0.101335. Entropy: 0.307314.\n",
      "Iteration 1760: Policy loss: -0.053253. Value loss: 0.043498. Entropy: 0.306485.\n",
      "Iteration 1761: Policy loss: -0.045186. Value loss: 0.032554. Entropy: 0.306077.\n",
      "episode: 718   score: 25.0  epsilon: 1.0    steps: 304  evaluation reward: 232.9\n",
      "episode: 719   score: 365.0  epsilon: 1.0    steps: 496  evaluation reward: 235.45\n",
      "episode: 720   score: 235.0  epsilon: 1.0    steps: 960  evaluation reward: 235.15\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1762: Policy loss: -0.020233. Value loss: 0.094369. Entropy: 0.302581.\n",
      "Iteration 1763: Policy loss: -0.020334. Value loss: 0.043574. Entropy: 0.302522.\n",
      "Iteration 1764: Policy loss: -0.026981. Value loss: 0.031556. Entropy: 0.302387.\n",
      "episode: 721   score: 65.0  epsilon: 1.0    steps: 480  evaluation reward: 233.7\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1765: Policy loss: 0.090174. Value loss: 0.049977. Entropy: 0.305977.\n",
      "Iteration 1766: Policy loss: 0.087679. Value loss: 0.029017. Entropy: 0.305746.\n",
      "Iteration 1767: Policy loss: 0.085658. Value loss: 0.021713. Entropy: 0.305316.\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1768: Policy loss: 0.084429. Value loss: 0.110615. Entropy: 0.302016.\n",
      "Iteration 1769: Policy loss: 0.078513. Value loss: 0.041085. Entropy: 0.302901.\n",
      "Iteration 1770: Policy loss: 0.069099. Value loss: 0.030588. Entropy: 0.301007.\n",
      "episode: 722   score: 110.0  epsilon: 1.0    steps: 104  evaluation reward: 233.25\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1771: Policy loss: 0.183218. Value loss: 0.080986. Entropy: 0.302343.\n",
      "Iteration 1772: Policy loss: 0.175289. Value loss: 0.038316. Entropy: 0.301685.\n",
      "Iteration 1773: Policy loss: 0.179452. Value loss: 0.027831. Entropy: 0.300956.\n",
      "episode: 723   score: 120.0  epsilon: 1.0    steps: 288  evaluation reward: 229.3\n",
      "episode: 724   score: 55.0  epsilon: 1.0    steps: 496  evaluation reward: 227.75\n",
      "episode: 725   score: 115.0  epsilon: 1.0    steps: 992  evaluation reward: 220.0\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1774: Policy loss: 0.067352. Value loss: 0.083171. Entropy: 0.303259.\n",
      "Iteration 1775: Policy loss: 0.056724. Value loss: 0.041505. Entropy: 0.303700.\n",
      "Iteration 1776: Policy loss: 0.057882. Value loss: 0.031701. Entropy: 0.303244.\n",
      "episode: 726   score: 115.0  epsilon: 1.0    steps: 512  evaluation reward: 218.7\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1777: Policy loss: -0.138539. Value loss: 0.097708. Entropy: 0.303157.\n",
      "Iteration 1778: Policy loss: -0.151434. Value loss: 0.041429. Entropy: 0.303915.\n",
      "Iteration 1779: Policy loss: -0.143348. Value loss: 0.034911. Entropy: 0.303914.\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1780: Policy loss: -0.103771. Value loss: 0.126594. Entropy: 0.305254.\n",
      "Iteration 1781: Policy loss: -0.107550. Value loss: 0.056822. Entropy: 0.305378.\n",
      "Iteration 1782: Policy loss: -0.109343. Value loss: 0.042698. Entropy: 0.305236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 727   score: 345.0  epsilon: 1.0    steps: 304  evaluation reward: 219.65\n",
      "episode: 728   score: 50.0  epsilon: 1.0    steps: 416  evaluation reward: 216.7\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1783: Policy loss: -0.158962. Value loss: 0.110381. Entropy: 0.300410.\n",
      "Iteration 1784: Policy loss: -0.153231. Value loss: 0.056759. Entropy: 0.299886.\n",
      "Iteration 1785: Policy loss: -0.163111. Value loss: 0.043410. Entropy: 0.300051.\n",
      "episode: 729   score: 180.0  epsilon: 1.0    steps: 48  evaluation reward: 215.7\n",
      "episode: 730   score: 320.0  epsilon: 1.0    steps: 256  evaluation reward: 216.8\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1786: Policy loss: 0.033022. Value loss: 0.134080. Entropy: 0.306399.\n",
      "Iteration 1787: Policy loss: 0.028131. Value loss: 0.067449. Entropy: 0.305557.\n",
      "Iteration 1788: Policy loss: 0.027726. Value loss: 0.043136. Entropy: 0.306091.\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1789: Policy loss: 0.116047. Value loss: 0.120047. Entropy: 0.303042.\n",
      "Iteration 1790: Policy loss: 0.109267. Value loss: 0.053522. Entropy: 0.304210.\n",
      "Iteration 1791: Policy loss: 0.108606. Value loss: 0.039804. Entropy: 0.303069.\n",
      "episode: 731   score: 75.0  epsilon: 1.0    steps: 456  evaluation reward: 216.2\n",
      "episode: 732   score: 210.0  epsilon: 1.0    steps: 536  evaluation reward: 215.6\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1792: Policy loss: -0.061186. Value loss: 0.104188. Entropy: 0.305122.\n",
      "Iteration 1793: Policy loss: -0.062750. Value loss: 0.054001. Entropy: 0.306203.\n",
      "Iteration 1794: Policy loss: -0.060056. Value loss: 0.036059. Entropy: 0.304773.\n",
      "episode: 733   score: 125.0  epsilon: 1.0    steps: 56  evaluation reward: 215.05\n",
      "episode: 734   score: 295.0  epsilon: 1.0    steps: 904  evaluation reward: 216.9\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1795: Policy loss: 0.102621. Value loss: 0.097910. Entropy: 0.303464.\n",
      "Iteration 1796: Policy loss: 0.106394. Value loss: 0.045203. Entropy: 0.304167.\n",
      "Iteration 1797: Policy loss: 0.094519. Value loss: 0.034221. Entropy: 0.302500.\n",
      "Training network. lr: 0.000237. clip: 0.094627\n",
      "Iteration 1798: Policy loss: 0.111018. Value loss: 0.102974. Entropy: 0.303251.\n",
      "Iteration 1799: Policy loss: 0.103113. Value loss: 0.044788. Entropy: 0.301490.\n",
      "Iteration 1800: Policy loss: 0.101276. Value loss: 0.036838. Entropy: 0.301516.\n",
      "episode: 735   score: 135.0  epsilon: 1.0    steps: 320  evaluation reward: 216.45\n",
      "episode: 736   score: 240.0  epsilon: 1.0    steps: 776  evaluation reward: 215.6\n",
      "episode: 737   score: 350.0  epsilon: 1.0    steps: 1000  evaluation reward: 217.3\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1801: Policy loss: 0.054090. Value loss: 0.076799. Entropy: 0.307232.\n",
      "Iteration 1802: Policy loss: 0.058935. Value loss: 0.030758. Entropy: 0.307623.\n",
      "Iteration 1803: Policy loss: 0.049067. Value loss: 0.022892. Entropy: 0.306477.\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1804: Policy loss: 0.103644. Value loss: 0.061993. Entropy: 0.306522.\n",
      "Iteration 1805: Policy loss: 0.097438. Value loss: 0.021063. Entropy: 0.305354.\n",
      "Iteration 1806: Policy loss: 0.103405. Value loss: 0.015034. Entropy: 0.306035.\n",
      "episode: 738   score: 275.0  epsilon: 1.0    steps: 448  evaluation reward: 216.25\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1807: Policy loss: 0.166447. Value loss: 0.079099. Entropy: 0.306713.\n",
      "Iteration 1808: Policy loss: 0.165939. Value loss: 0.035993. Entropy: 0.306476.\n",
      "Iteration 1809: Policy loss: 0.160391. Value loss: 0.029495. Entropy: 0.307171.\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1810: Policy loss: -0.089992. Value loss: 0.086430. Entropy: 0.302532.\n",
      "Iteration 1811: Policy loss: -0.088659. Value loss: 0.040014. Entropy: 0.303844.\n",
      "Iteration 1812: Policy loss: -0.094267. Value loss: 0.027562. Entropy: 0.303572.\n",
      "episode: 739   score: 210.0  epsilon: 1.0    steps: 144  evaluation reward: 216.55\n",
      "episode: 740   score: 210.0  epsilon: 1.0    steps: 752  evaluation reward: 217.1\n",
      "episode: 741   score: 195.0  epsilon: 1.0    steps: 944  evaluation reward: 217.25\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1813: Policy loss: -0.117822. Value loss: 0.086077. Entropy: 0.306103.\n",
      "Iteration 1814: Policy loss: -0.121272. Value loss: 0.040299. Entropy: 0.304246.\n",
      "Iteration 1815: Policy loss: -0.120221. Value loss: 0.031809. Entropy: 0.305133.\n",
      "episode: 742   score: 210.0  epsilon: 1.0    steps: 152  evaluation reward: 213.15\n",
      "episode: 743   score: 110.0  epsilon: 1.0    steps: 424  evaluation reward: 211.35\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1816: Policy loss: -0.330098. Value loss: 0.326320. Entropy: 0.305156.\n",
      "Iteration 1817: Policy loss: -0.316118. Value loss: 0.251639. Entropy: 0.304767.\n",
      "Iteration 1818: Policy loss: -0.321744. Value loss: 0.205946. Entropy: 0.303949.\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1819: Policy loss: -0.096110. Value loss: 0.129879. Entropy: 0.304460.\n",
      "Iteration 1820: Policy loss: -0.096169. Value loss: 0.070951. Entropy: 0.302144.\n",
      "Iteration 1821: Policy loss: -0.100194. Value loss: 0.054607. Entropy: 0.302692.\n",
      "episode: 744   score: 165.0  epsilon: 1.0    steps: 272  evaluation reward: 211.45\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1822: Policy loss: 0.020118. Value loss: 0.058518. Entropy: 0.307615.\n",
      "Iteration 1823: Policy loss: 0.022203. Value loss: 0.031853. Entropy: 0.308197.\n",
      "Iteration 1824: Policy loss: 0.019782. Value loss: 0.019863. Entropy: 0.306622.\n",
      "episode: 745   score: 465.0  epsilon: 1.0    steps: 536  evaluation reward: 212.0\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1825: Policy loss: -0.019621. Value loss: 0.056942. Entropy: 0.310300.\n",
      "Iteration 1826: Policy loss: -0.029393. Value loss: 0.030941. Entropy: 0.309505.\n",
      "Iteration 1827: Policy loss: -0.030414. Value loss: 0.021337. Entropy: 0.309881.\n",
      "episode: 746   score: 275.0  epsilon: 1.0    steps: 80  evaluation reward: 213.2\n",
      "episode: 747   score: 75.0  epsilon: 1.0    steps: 256  evaluation reward: 212.6\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1828: Policy loss: -0.383101. Value loss: 0.333654. Entropy: 0.305455.\n",
      "Iteration 1829: Policy loss: -0.397375. Value loss: 0.170381. Entropy: 0.304321.\n",
      "Iteration 1830: Policy loss: -0.410860. Value loss: 0.072596. Entropy: 0.305146.\n",
      "episode: 748   score: 110.0  epsilon: 1.0    steps: 168  evaluation reward: 211.9\n",
      "episode: 749   score: 410.0  epsilon: 1.0    steps: 784  evaluation reward: 214.95\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1831: Policy loss: 0.181621. Value loss: 0.073916. Entropy: 0.306773.\n",
      "Iteration 1832: Policy loss: 0.179212. Value loss: 0.032246. Entropy: 0.306674.\n",
      "Iteration 1833: Policy loss: 0.179212. Value loss: 0.025797. Entropy: 0.305919.\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1834: Policy loss: 0.068144. Value loss: 0.116218. Entropy: 0.304000.\n",
      "Iteration 1835: Policy loss: 0.065438. Value loss: 0.047582. Entropy: 0.304008.\n",
      "Iteration 1836: Policy loss: 0.065337. Value loss: 0.036459. Entropy: 0.302977.\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1837: Policy loss: 0.034935. Value loss: 0.128151. Entropy: 0.307960.\n",
      "Iteration 1838: Policy loss: 0.023851. Value loss: 0.062222. Entropy: 0.308083.\n",
      "Iteration 1839: Policy loss: 0.025087. Value loss: 0.047328. Entropy: 0.307254.\n",
      "episode: 750   score: 240.0  epsilon: 1.0    steps: 96  evaluation reward: 215.8\n",
      "now time :  2019-09-05 16:09:29.928375\n",
      "episode: 751   score: 80.0  epsilon: 1.0    steps: 328  evaluation reward: 213.9\n",
      "episode: 752   score: 265.0  epsilon: 1.0    steps: 552  evaluation reward: 212.35\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1840: Policy loss: 0.144367. Value loss: 0.045368. Entropy: 0.307211.\n",
      "Iteration 1841: Policy loss: 0.143137. Value loss: 0.019870. Entropy: 0.306463.\n",
      "Iteration 1842: Policy loss: 0.141355. Value loss: 0.014879. Entropy: 0.306798.\n",
      "episode: 753   score: 135.0  epsilon: 1.0    steps: 112  evaluation reward: 209.0\n",
      "episode: 754   score: 225.0  epsilon: 1.0    steps: 688  evaluation reward: 207.15\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1843: Policy loss: 0.213027. Value loss: 0.078934. Entropy: 0.304646.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1844: Policy loss: 0.206900. Value loss: 0.033552. Entropy: 0.303949.\n",
      "Iteration 1845: Policy loss: 0.204404. Value loss: 0.025224. Entropy: 0.304616.\n",
      "episode: 755   score: 55.0  epsilon: 1.0    steps: 24  evaluation reward: 205.0\n",
      "episode: 756   score: 180.0  epsilon: 1.0    steps: 512  evaluation reward: 202.7\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1846: Policy loss: 0.012469. Value loss: 0.080442. Entropy: 0.303073.\n",
      "Iteration 1847: Policy loss: 0.009887. Value loss: 0.050038. Entropy: 0.303198.\n",
      "Iteration 1848: Policy loss: 0.003613. Value loss: 0.035607. Entropy: 0.302713.\n",
      "episode: 757   score: 105.0  epsilon: 1.0    steps: 336  evaluation reward: 201.45\n",
      "episode: 758   score: 180.0  epsilon: 1.0    steps: 576  evaluation reward: 202.0\n",
      "Training network. lr: 0.000236. clip: 0.094480\n",
      "Iteration 1849: Policy loss: -0.004164. Value loss: 0.030583. Entropy: 0.305554.\n",
      "Iteration 1850: Policy loss: -0.004695. Value loss: 0.018973. Entropy: 0.306029.\n",
      "Iteration 1851: Policy loss: -0.008987. Value loss: 0.015228. Entropy: 0.305609.\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1852: Policy loss: 0.010280. Value loss: 0.037558. Entropy: 0.309712.\n",
      "Iteration 1853: Policy loss: 0.006167. Value loss: 0.024305. Entropy: 0.309576.\n",
      "Iteration 1854: Policy loss: 0.006744. Value loss: 0.018960. Entropy: 0.309004.\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1855: Policy loss: -0.372309. Value loss: 0.342298. Entropy: 0.308327.\n",
      "Iteration 1856: Policy loss: -0.383415. Value loss: 0.276058. Entropy: 0.308693.\n",
      "Iteration 1857: Policy loss: -0.405455. Value loss: 0.250245. Entropy: 0.308866.\n",
      "episode: 759   score: 185.0  epsilon: 1.0    steps: 528  evaluation reward: 202.5\n",
      "episode: 760   score: 210.0  epsilon: 1.0    steps: 968  evaluation reward: 202.5\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1858: Policy loss: -0.047573. Value loss: 0.050843. Entropy: 0.310739.\n",
      "Iteration 1859: Policy loss: -0.046551. Value loss: 0.022538. Entropy: 0.310403.\n",
      "Iteration 1860: Policy loss: -0.049546. Value loss: 0.017506. Entropy: 0.310398.\n",
      "episode: 761   score: 135.0  epsilon: 1.0    steps: 256  evaluation reward: 201.45\n",
      "episode: 762   score: 180.0  epsilon: 1.0    steps: 416  evaluation reward: 201.45\n",
      "episode: 763   score: 105.0  epsilon: 1.0    steps: 672  evaluation reward: 199.9\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1861: Policy loss: -0.155028. Value loss: 0.091019. Entropy: 0.305506.\n",
      "Iteration 1862: Policy loss: -0.159122. Value loss: 0.049337. Entropy: 0.305060.\n",
      "Iteration 1863: Policy loss: -0.161814. Value loss: 0.038960. Entropy: 0.305477.\n",
      "episode: 764   score: 410.0  epsilon: 1.0    steps: 808  evaluation reward: 202.95\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1864: Policy loss: -0.497120. Value loss: 0.389447. Entropy: 0.306967.\n",
      "Iteration 1865: Policy loss: -0.517259. Value loss: 0.288012. Entropy: 0.305199.\n",
      "Iteration 1866: Policy loss: -0.516082. Value loss: 0.239538. Entropy: 0.306599.\n",
      "episode: 765   score: 410.0  epsilon: 1.0    steps: 736  evaluation reward: 205.25\n",
      "episode: 766   score: 140.0  epsilon: 1.0    steps: 808  evaluation reward: 204.0\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1867: Policy loss: 0.181011. Value loss: 0.075891. Entropy: 0.308452.\n",
      "Iteration 1868: Policy loss: 0.176541. Value loss: 0.027161. Entropy: 0.308039.\n",
      "Iteration 1869: Policy loss: 0.172733. Value loss: 0.021084. Entropy: 0.307134.\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1870: Policy loss: -0.175065. Value loss: 0.061602. Entropy: 0.308225.\n",
      "Iteration 1871: Policy loss: -0.178493. Value loss: 0.035864. Entropy: 0.307435.\n",
      "Iteration 1872: Policy loss: -0.179618. Value loss: 0.028432. Entropy: 0.306770.\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1873: Policy loss: -0.290947. Value loss: 0.091048. Entropy: 0.308488.\n",
      "Iteration 1874: Policy loss: -0.296453. Value loss: 0.046337. Entropy: 0.308147.\n",
      "Iteration 1875: Policy loss: -0.298138. Value loss: 0.034604. Entropy: 0.308625.\n",
      "episode: 767   score: 180.0  epsilon: 1.0    steps: 136  evaluation reward: 204.0\n",
      "episode: 768   score: 110.0  epsilon: 1.0    steps: 688  evaluation reward: 203.0\n",
      "episode: 769   score: 140.0  epsilon: 1.0    steps: 880  evaluation reward: 202.85\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1876: Policy loss: -0.149369. Value loss: 0.189356. Entropy: 0.306954.\n",
      "Iteration 1877: Policy loss: -0.119399. Value loss: 0.081727. Entropy: 0.306458.\n",
      "Iteration 1878: Policy loss: -0.162468. Value loss: 0.051704. Entropy: 0.306178.\n",
      "episode: 770   score: 155.0  epsilon: 1.0    steps: 560  evaluation reward: 202.3\n",
      "episode: 771   score: 380.0  epsilon: 1.0    steps: 1000  evaluation reward: 203.7\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1879: Policy loss: -0.299990. Value loss: 0.362177. Entropy: 0.305229.\n",
      "Iteration 1880: Policy loss: -0.323607. Value loss: 0.302233. Entropy: 0.305287.\n",
      "Iteration 1881: Policy loss: -0.312321. Value loss: 0.191457. Entropy: 0.305366.\n",
      "episode: 772   score: 135.0  epsilon: 1.0    steps: 1000  evaluation reward: 202.95\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1882: Policy loss: 0.194513. Value loss: 0.101323. Entropy: 0.305090.\n",
      "Iteration 1883: Policy loss: 0.197610. Value loss: 0.042765. Entropy: 0.304290.\n",
      "Iteration 1884: Policy loss: 0.192977. Value loss: 0.032202. Entropy: 0.304568.\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1885: Policy loss: 0.068038. Value loss: 0.066018. Entropy: 0.310410.\n",
      "Iteration 1886: Policy loss: 0.061675. Value loss: 0.031376. Entropy: 0.310270.\n",
      "Iteration 1887: Policy loss: 0.054103. Value loss: 0.024309. Entropy: 0.309298.\n",
      "episode: 773   score: 105.0  epsilon: 1.0    steps: 360  evaluation reward: 200.45\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1888: Policy loss: -0.062576. Value loss: 0.087506. Entropy: 0.307792.\n",
      "Iteration 1889: Policy loss: -0.059667. Value loss: 0.040816. Entropy: 0.306945.\n",
      "Iteration 1890: Policy loss: -0.061305. Value loss: 0.032908. Entropy: 0.307396.\n",
      "episode: 774   score: 460.0  epsilon: 1.0    steps: 248  evaluation reward: 200.95\n",
      "episode: 775   score: 260.0  epsilon: 1.0    steps: 632  evaluation reward: 199.15\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1891: Policy loss: -0.006123. Value loss: 0.026698. Entropy: 0.308856.\n",
      "Iteration 1892: Policy loss: -0.011387. Value loss: 0.012748. Entropy: 0.308197.\n",
      "Iteration 1893: Policy loss: -0.007868. Value loss: 0.010471. Entropy: 0.307872.\n",
      "episode: 776   score: 180.0  epsilon: 1.0    steps: 912  evaluation reward: 198.8\n",
      "episode: 777   score: 180.0  epsilon: 1.0    steps: 984  evaluation reward: 199.25\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1894: Policy loss: -0.275471. Value loss: 0.157743. Entropy: 0.306730.\n",
      "Iteration 1895: Policy loss: -0.267134. Value loss: 0.084673. Entropy: 0.307436.\n",
      "Iteration 1896: Policy loss: -0.269316. Value loss: 0.060675. Entropy: 0.307479.\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1897: Policy loss: 0.045398. Value loss: 0.045257. Entropy: 0.301114.\n",
      "Iteration 1898: Policy loss: 0.044797. Value loss: 0.024592. Entropy: 0.302094.\n",
      "Iteration 1899: Policy loss: 0.042137. Value loss: 0.018363. Entropy: 0.301772.\n",
      "episode: 778   score: 380.0  epsilon: 1.0    steps: 16  evaluation reward: 202.0\n",
      "episode: 779   score: 145.0  epsilon: 1.0    steps: 120  evaluation reward: 199.5\n",
      "Training network. lr: 0.000236. clip: 0.094323\n",
      "Iteration 1900: Policy loss: 0.017944. Value loss: 0.098818. Entropy: 0.308399.\n",
      "Iteration 1901: Policy loss: 0.010587. Value loss: 0.038636. Entropy: 0.307670.\n",
      "Iteration 1902: Policy loss: 0.009420. Value loss: 0.030727. Entropy: 0.308309.\n",
      "episode: 780   score: 135.0  epsilon: 1.0    steps: 232  evaluation reward: 196.45\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1903: Policy loss: 0.269853. Value loss: 0.100510. Entropy: 0.309672.\n",
      "Iteration 1904: Policy loss: 0.259923. Value loss: 0.044661. Entropy: 0.309329.\n",
      "Iteration 1905: Policy loss: 0.268964. Value loss: 0.037703. Entropy: 0.311261.\n",
      "episode: 781   score: 180.0  epsilon: 1.0    steps: 744  evaluation reward: 195.6\n",
      "Training network. lr: 0.000235. clip: 0.094166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1906: Policy loss: 0.114060. Value loss: 0.076014. Entropy: 0.309114.\n",
      "Iteration 1907: Policy loss: 0.105534. Value loss: 0.035580. Entropy: 0.309122.\n",
      "Iteration 1908: Policy loss: 0.100910. Value loss: 0.027011. Entropy: 0.307997.\n",
      "episode: 782   score: 180.0  epsilon: 1.0    steps: 648  evaluation reward: 195.0\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1909: Policy loss: -0.065965. Value loss: 0.108078. Entropy: 0.309508.\n",
      "Iteration 1910: Policy loss: -0.078105. Value loss: 0.045633. Entropy: 0.309063.\n",
      "Iteration 1911: Policy loss: -0.075902. Value loss: 0.033280. Entropy: 0.309396.\n",
      "episode: 783   score: 80.0  epsilon: 1.0    steps: 304  evaluation reward: 194.25\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1912: Policy loss: -0.560201. Value loss: 0.364594. Entropy: 0.306260.\n",
      "Iteration 1913: Policy loss: -0.571373. Value loss: 0.187950. Entropy: 0.308046.\n",
      "Iteration 1914: Policy loss: -0.573579. Value loss: 0.116739. Entropy: 0.308938.\n",
      "episode: 784   score: 260.0  epsilon: 1.0    steps: 40  evaluation reward: 193.9\n",
      "episode: 785   score: 380.0  epsilon: 1.0    steps: 176  evaluation reward: 193.05\n",
      "episode: 786   score: 355.0  epsilon: 1.0    steps: 440  evaluation reward: 195.85\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1915: Policy loss: 0.095569. Value loss: 0.067330. Entropy: 0.306899.\n",
      "Iteration 1916: Policy loss: 0.086547. Value loss: 0.029853. Entropy: 0.306886.\n",
      "Iteration 1917: Policy loss: 0.089665. Value loss: 0.022003. Entropy: 0.307052.\n",
      "episode: 787   score: 380.0  epsilon: 1.0    steps: 296  evaluation reward: 197.4\n",
      "episode: 788   score: 120.0  epsilon: 1.0    steps: 368  evaluation reward: 195.85\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1918: Policy loss: -0.044062. Value loss: 0.091092. Entropy: 0.312109.\n",
      "Iteration 1919: Policy loss: -0.051490. Value loss: 0.046889. Entropy: 0.311554.\n",
      "Iteration 1920: Policy loss: -0.048454. Value loss: 0.034802. Entropy: 0.311603.\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1921: Policy loss: 0.097580. Value loss: 0.070317. Entropy: 0.316256.\n",
      "Iteration 1922: Policy loss: 0.093352. Value loss: 0.037724. Entropy: 0.314812.\n",
      "Iteration 1923: Policy loss: 0.097903. Value loss: 0.028117. Entropy: 0.313825.\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1924: Policy loss: 0.105207. Value loss: 0.049756. Entropy: 0.314663.\n",
      "Iteration 1925: Policy loss: 0.105746. Value loss: 0.027247. Entropy: 0.314182.\n",
      "Iteration 1926: Policy loss: 0.100697. Value loss: 0.020055. Entropy: 0.314444.\n",
      "episode: 789   score: 50.0  epsilon: 1.0    steps: 208  evaluation reward: 194.2\n",
      "episode: 790   score: 180.0  epsilon: 1.0    steps: 560  evaluation reward: 193.8\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1927: Policy loss: -0.380618. Value loss: 0.386979. Entropy: 0.311925.\n",
      "Iteration 1928: Policy loss: -0.372449. Value loss: 0.263647. Entropy: 0.311981.\n",
      "Iteration 1929: Policy loss: -0.395868. Value loss: 0.220994. Entropy: 0.314088.\n",
      "episode: 791   score: 180.0  epsilon: 1.0    steps: 680  evaluation reward: 193.6\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1930: Policy loss: -0.314575. Value loss: 0.312964. Entropy: 0.318251.\n",
      "Iteration 1931: Policy loss: -0.333429. Value loss: 0.138992. Entropy: 0.317943.\n",
      "Iteration 1932: Policy loss: -0.326152. Value loss: 0.079628. Entropy: 0.320407.\n",
      "episode: 792   score: 210.0  epsilon: 1.0    steps: 744  evaluation reward: 194.75\n",
      "episode: 793   score: 210.0  epsilon: 1.0    steps: 1000  evaluation reward: 194.7\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1933: Policy loss: 0.054353. Value loss: 0.097444. Entropy: 0.318114.\n",
      "Iteration 1934: Policy loss: 0.053945. Value loss: 0.047475. Entropy: 0.318821.\n",
      "Iteration 1935: Policy loss: 0.051326. Value loss: 0.035005. Entropy: 0.318956.\n",
      "episode: 794   score: 410.0  epsilon: 1.0    steps: 120  evaluation reward: 197.65\n",
      "episode: 795   score: 500.0  epsilon: 1.0    steps: 144  evaluation reward: 201.4\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1936: Policy loss: -0.051064. Value loss: 0.059127. Entropy: 0.313992.\n",
      "Iteration 1937: Policy loss: -0.056237. Value loss: 0.036510. Entropy: 0.313230.\n",
      "Iteration 1938: Policy loss: -0.052476. Value loss: 0.026278. Entropy: 0.313670.\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1939: Policy loss: 0.272261. Value loss: 0.093442. Entropy: 0.318393.\n",
      "Iteration 1940: Policy loss: 0.266200. Value loss: 0.034113. Entropy: 0.317764.\n",
      "Iteration 1941: Policy loss: 0.268628. Value loss: 0.025358. Entropy: 0.317689.\n",
      "episode: 796   score: 240.0  epsilon: 1.0    steps: 128  evaluation reward: 202.7\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1942: Policy loss: 0.098082. Value loss: 0.061499. Entropy: 0.314557.\n",
      "Iteration 1943: Policy loss: 0.098447. Value loss: 0.028563. Entropy: 0.314402.\n",
      "Iteration 1944: Policy loss: 0.094775. Value loss: 0.023439. Entropy: 0.314792.\n",
      "episode: 797   score: 210.0  epsilon: 1.0    steps: 784  evaluation reward: 204.25\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1945: Policy loss: -0.101647. Value loss: 0.093517. Entropy: 0.312639.\n",
      "Iteration 1946: Policy loss: -0.105647. Value loss: 0.048231. Entropy: 0.311751.\n",
      "Iteration 1947: Policy loss: -0.110172. Value loss: 0.034380. Entropy: 0.312064.\n",
      "Training network. lr: 0.000235. clip: 0.094166\n",
      "Iteration 1948: Policy loss: -0.022989. Value loss: 0.061469. Entropy: 0.313559.\n",
      "Iteration 1949: Policy loss: -0.027006. Value loss: 0.034562. Entropy: 0.313568.\n",
      "Iteration 1950: Policy loss: -0.027050. Value loss: 0.034156. Entropy: 0.313387.\n",
      "episode: 798   score: 240.0  epsilon: 1.0    steps: 152  evaluation reward: 203.5\n",
      "episode: 799   score: 110.0  epsilon: 1.0    steps: 208  evaluation reward: 203.65\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1951: Policy loss: -0.001146. Value loss: 0.085168. Entropy: 0.313707.\n",
      "Iteration 1952: Policy loss: -0.004000. Value loss: 0.050115. Entropy: 0.313005.\n",
      "Iteration 1953: Policy loss: -0.002024. Value loss: 0.038500. Entropy: 0.312792.\n",
      "episode: 800   score: 180.0  epsilon: 1.0    steps: 504  evaluation reward: 203.65\n",
      "now time :  2019-09-05 16:16:36.068281\n",
      "episode: 801   score: 210.0  epsilon: 1.0    steps: 872  evaluation reward: 203.6\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1954: Policy loss: 0.180669. Value loss: 0.098154. Entropy: 0.318758.\n",
      "Iteration 1955: Policy loss: 0.175999. Value loss: 0.043774. Entropy: 0.319253.\n",
      "Iteration 1956: Policy loss: 0.169417. Value loss: 0.035033. Entropy: 0.319318.\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1957: Policy loss: -0.090699. Value loss: 0.095126. Entropy: 0.314369.\n",
      "Iteration 1958: Policy loss: -0.090053. Value loss: 0.051092. Entropy: 0.314147.\n",
      "Iteration 1959: Policy loss: -0.090400. Value loss: 0.034496. Entropy: 0.315140.\n",
      "episode: 802   score: 50.0  epsilon: 1.0    steps: 448  evaluation reward: 201.5\n",
      "episode: 803   score: 210.0  epsilon: 1.0    steps: 496  evaluation reward: 200.45\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1960: Policy loss: 0.117397. Value loss: 0.044962. Entropy: 0.318007.\n",
      "Iteration 1961: Policy loss: 0.117938. Value loss: 0.017350. Entropy: 0.318460.\n",
      "Iteration 1962: Policy loss: 0.111455. Value loss: 0.012538. Entropy: 0.317454.\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1963: Policy loss: -0.219401. Value loss: 0.236534. Entropy: 0.311884.\n",
      "Iteration 1964: Policy loss: -0.215449. Value loss: 0.185961. Entropy: 0.312709.\n",
      "Iteration 1965: Policy loss: -0.233367. Value loss: 0.190272. Entropy: 0.313638.\n",
      "episode: 804   score: 370.0  epsilon: 1.0    steps: 232  evaluation reward: 200.5\n",
      "episode: 805   score: 210.0  epsilon: 1.0    steps: 568  evaluation reward: 202.1\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1966: Policy loss: -0.104255. Value loss: 0.090028. Entropy: 0.311922.\n",
      "Iteration 1967: Policy loss: -0.110041. Value loss: 0.039603. Entropy: 0.311169.\n",
      "Iteration 1968: Policy loss: -0.115329. Value loss: 0.027378. Entropy: 0.312411.\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1969: Policy loss: -0.087875. Value loss: 0.092154. Entropy: 0.310304.\n",
      "Iteration 1970: Policy loss: -0.095017. Value loss: 0.029217. Entropy: 0.310485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1971: Policy loss: -0.104906. Value loss: 0.023856. Entropy: 0.310852.\n",
      "episode: 806   score: 670.0  epsilon: 1.0    steps: 104  evaluation reward: 206.9\n",
      "episode: 807   score: 135.0  epsilon: 1.0    steps: 584  evaluation reward: 203.4\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1972: Policy loss: 0.103580. Value loss: 0.088231. Entropy: 0.314413.\n",
      "Iteration 1973: Policy loss: 0.091848. Value loss: 0.040847. Entropy: 0.315570.\n",
      "Iteration 1974: Policy loss: 0.088372. Value loss: 0.026012. Entropy: 0.314710.\n",
      "episode: 808   score: 305.0  epsilon: 1.0    steps: 776  evaluation reward: 205.9\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1975: Policy loss: 0.044838. Value loss: 0.041550. Entropy: 0.317571.\n",
      "Iteration 1976: Policy loss: 0.044132. Value loss: 0.021350. Entropy: 0.317610.\n",
      "Iteration 1977: Policy loss: 0.044928. Value loss: 0.015028. Entropy: 0.317575.\n",
      "episode: 809   score: 290.0  epsilon: 1.0    steps: 496  evaluation reward: 204.8\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1978: Policy loss: 0.059223. Value loss: 0.048632. Entropy: 0.316595.\n",
      "Iteration 1979: Policy loss: 0.058529. Value loss: 0.023670. Entropy: 0.315587.\n",
      "Iteration 1980: Policy loss: 0.057993. Value loss: 0.018131. Entropy: 0.314634.\n",
      "episode: 810   score: 195.0  epsilon: 1.0    steps: 608  evaluation reward: 204.65\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1981: Policy loss: 0.083604. Value loss: 0.053100. Entropy: 0.312097.\n",
      "Iteration 1982: Policy loss: 0.082159. Value loss: 0.027822. Entropy: 0.311863.\n",
      "Iteration 1983: Policy loss: 0.077017. Value loss: 0.018673. Entropy: 0.313116.\n",
      "episode: 811   score: 260.0  epsilon: 1.0    steps: 224  evaluation reward: 205.9\n",
      "episode: 812   score: 210.0  epsilon: 1.0    steps: 968  evaluation reward: 207.85\n",
      "episode: 813   score: 180.0  epsilon: 1.0    steps: 976  evaluation reward: 209.15\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1984: Policy loss: 0.088727. Value loss: 0.062525. Entropy: 0.315917.\n",
      "Iteration 1985: Policy loss: 0.084517. Value loss: 0.036535. Entropy: 0.315584.\n",
      "Iteration 1986: Policy loss: 0.085713. Value loss: 0.029268. Entropy: 0.314678.\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1987: Policy loss: 0.044951. Value loss: 0.044571. Entropy: 0.315590.\n",
      "Iteration 1988: Policy loss: 0.037948. Value loss: 0.020537. Entropy: 0.314853.\n",
      "Iteration 1989: Policy loss: 0.038842. Value loss: 0.016352. Entropy: 0.314833.\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1990: Policy loss: -0.000806. Value loss: 0.043955. Entropy: 0.319470.\n",
      "Iteration 1991: Policy loss: -0.003006. Value loss: 0.022557. Entropy: 0.319056.\n",
      "Iteration 1992: Policy loss: -0.005094. Value loss: 0.016518. Entropy: 0.318297.\n",
      "episode: 814   score: 240.0  epsilon: 1.0    steps: 704  evaluation reward: 210.45\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1993: Policy loss: -0.182079. Value loss: 0.322894. Entropy: 0.316174.\n",
      "Iteration 1994: Policy loss: -0.185538. Value loss: 0.134458. Entropy: 0.315198.\n",
      "Iteration 1995: Policy loss: -0.151248. Value loss: 0.128796. Entropy: 0.314509.\n",
      "episode: 815   score: 275.0  epsilon: 1.0    steps: 544  evaluation reward: 211.75\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1996: Policy loss: 0.021806. Value loss: 0.052202. Entropy: 0.313065.\n",
      "Iteration 1997: Policy loss: 0.018982. Value loss: 0.025381. Entropy: 0.314822.\n",
      "Iteration 1998: Policy loss: 0.018037. Value loss: 0.020455. Entropy: 0.314440.\n",
      "episode: 816   score: 410.0  epsilon: 1.0    steps: 64  evaluation reward: 213.75\n",
      "episode: 817   score: 210.0  epsilon: 1.0    steps: 912  evaluation reward: 215.1\n",
      "Training network. lr: 0.000235. clip: 0.094019\n",
      "Iteration 1999: Policy loss: -0.024888. Value loss: 0.052764. Entropy: 0.315365.\n",
      "Iteration 2000: Policy loss: -0.026449. Value loss: 0.033447. Entropy: 0.316309.\n",
      "Iteration 2001: Policy loss: -0.029991. Value loss: 0.024341. Entropy: 0.315786.\n",
      "episode: 818   score: 180.0  epsilon: 1.0    steps: 496  evaluation reward: 216.65\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2002: Policy loss: 0.104528. Value loss: 0.041973. Entropy: 0.318649.\n",
      "Iteration 2003: Policy loss: 0.101767. Value loss: 0.021585. Entropy: 0.317764.\n",
      "Iteration 2004: Policy loss: 0.098699. Value loss: 0.018817. Entropy: 0.318992.\n",
      "episode: 819   score: 210.0  epsilon: 1.0    steps: 432  evaluation reward: 215.1\n",
      "episode: 820   score: 180.0  epsilon: 1.0    steps: 848  evaluation reward: 214.55\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2005: Policy loss: -0.204138. Value loss: 0.292030. Entropy: 0.314464.\n",
      "Iteration 2006: Policy loss: -0.235291. Value loss: 0.237823. Entropy: 0.316441.\n",
      "Iteration 2007: Policy loss: -0.226263. Value loss: 0.171991. Entropy: 0.316262.\n",
      "episode: 821   score: 490.0  epsilon: 1.0    steps: 992  evaluation reward: 218.8\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2008: Policy loss: 0.004030. Value loss: 0.080690. Entropy: 0.314618.\n",
      "Iteration 2009: Policy loss: 0.001086. Value loss: 0.045127. Entropy: 0.315422.\n",
      "Iteration 2010: Policy loss: 0.000521. Value loss: 0.034011. Entropy: 0.313020.\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2011: Policy loss: 0.220065. Value loss: 0.058419. Entropy: 0.315125.\n",
      "Iteration 2012: Policy loss: 0.214608. Value loss: 0.030640. Entropy: 0.314651.\n",
      "Iteration 2013: Policy loss: 0.209848. Value loss: 0.022311. Entropy: 0.314610.\n",
      "episode: 822   score: 110.0  epsilon: 1.0    steps: 56  evaluation reward: 218.8\n",
      "episode: 823   score: 180.0  epsilon: 1.0    steps: 472  evaluation reward: 219.4\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2014: Policy loss: 0.058062. Value loss: 0.043960. Entropy: 0.315080.\n",
      "Iteration 2015: Policy loss: 0.054785. Value loss: 0.018398. Entropy: 0.315957.\n",
      "Iteration 2016: Policy loss: 0.053733. Value loss: 0.013157. Entropy: 0.315459.\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2017: Policy loss: -0.003335. Value loss: 0.049664. Entropy: 0.318020.\n",
      "Iteration 2018: Policy loss: -0.012431. Value loss: 0.029044. Entropy: 0.316621.\n",
      "Iteration 2019: Policy loss: -0.005834. Value loss: 0.024059. Entropy: 0.317332.\n",
      "episode: 824   score: 260.0  epsilon: 1.0    steps: 280  evaluation reward: 221.45\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2020: Policy loss: 0.128510. Value loss: 0.046428. Entropy: 0.317157.\n",
      "Iteration 2021: Policy loss: 0.128457. Value loss: 0.023825. Entropy: 0.317086.\n",
      "Iteration 2022: Policy loss: 0.127517. Value loss: 0.019541. Entropy: 0.316175.\n",
      "episode: 825   score: 210.0  epsilon: 1.0    steps: 200  evaluation reward: 222.4\n",
      "episode: 826   score: 250.0  epsilon: 1.0    steps: 488  evaluation reward: 223.75\n",
      "episode: 827   score: 155.0  epsilon: 1.0    steps: 944  evaluation reward: 221.85\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2023: Policy loss: 0.203122. Value loss: 0.080783. Entropy: 0.318395.\n",
      "Iteration 2024: Policy loss: 0.191922. Value loss: 0.037539. Entropy: 0.318191.\n",
      "Iteration 2025: Policy loss: 0.194260. Value loss: 0.028993. Entropy: 0.318134.\n",
      "episode: 828   score: 210.0  epsilon: 1.0    steps: 136  evaluation reward: 223.45\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2026: Policy loss: -0.053569. Value loss: 0.073758. Entropy: 0.311228.\n",
      "Iteration 2027: Policy loss: -0.057077. Value loss: 0.036439. Entropy: 0.311474.\n",
      "Iteration 2028: Policy loss: -0.059533. Value loss: 0.026663. Entropy: 0.312278.\n",
      "episode: 829   score: 210.0  epsilon: 1.0    steps: 128  evaluation reward: 223.75\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2029: Policy loss: 0.120019. Value loss: 0.082782. Entropy: 0.313902.\n",
      "Iteration 2030: Policy loss: 0.120465. Value loss: 0.036157. Entropy: 0.312507.\n",
      "Iteration 2031: Policy loss: 0.113440. Value loss: 0.028573. Entropy: 0.311538.\n",
      "episode: 830   score: 180.0  epsilon: 1.0    steps: 312  evaluation reward: 222.35\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2032: Policy loss: 0.115717. Value loss: 0.050936. Entropy: 0.315716.\n",
      "Iteration 2033: Policy loss: 0.114713. Value loss: 0.026732. Entropy: 0.314180.\n",
      "Iteration 2034: Policy loss: 0.108390. Value loss: 0.021989. Entropy: 0.314410.\n",
      "episode: 831   score: 180.0  epsilon: 1.0    steps: 48  evaluation reward: 223.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2035: Policy loss: -0.025122. Value loss: 0.037172. Entropy: 0.314454.\n",
      "Iteration 2036: Policy loss: -0.026624. Value loss: 0.021219. Entropy: 0.313686.\n",
      "Iteration 2037: Policy loss: -0.022871. Value loss: 0.015988. Entropy: 0.313149.\n",
      "episode: 832   score: 105.0  epsilon: 1.0    steps: 272  evaluation reward: 222.35\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2038: Policy loss: 0.039074. Value loss: 0.022901. Entropy: 0.312180.\n",
      "Iteration 2039: Policy loss: 0.036625. Value loss: 0.013035. Entropy: 0.311907.\n",
      "Iteration 2040: Policy loss: 0.039957. Value loss: 0.009726. Entropy: 0.311517.\n",
      "episode: 833   score: 165.0  epsilon: 1.0    steps: 704  evaluation reward: 222.75\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2041: Policy loss: -0.562535. Value loss: 0.396664. Entropy: 0.315814.\n",
      "Iteration 2042: Policy loss: -0.612317. Value loss: 0.137991. Entropy: 0.316569.\n",
      "Iteration 2043: Policy loss: -0.659417. Value loss: 0.090666. Entropy: 0.317357.\n",
      "episode: 834   score: 160.0  epsilon: 1.0    steps: 128  evaluation reward: 221.4\n",
      "episode: 835   score: 410.0  epsilon: 1.0    steps: 552  evaluation reward: 224.15\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2044: Policy loss: -0.187604. Value loss: 0.084037. Entropy: 0.312642.\n",
      "Iteration 2045: Policy loss: -0.193199. Value loss: 0.043949. Entropy: 0.312202.\n",
      "Iteration 2046: Policy loss: -0.194840. Value loss: 0.034613. Entropy: 0.312393.\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2047: Policy loss: 0.105101. Value loss: 0.276530. Entropy: 0.317081.\n",
      "Iteration 2048: Policy loss: 0.098462. Value loss: 0.065828. Entropy: 0.316020.\n",
      "Iteration 2049: Policy loss: 0.086515. Value loss: 0.042306. Entropy: 0.315995.\n",
      "episode: 836   score: 460.0  epsilon: 1.0    steps: 936  evaluation reward: 226.35\n",
      "Training network. lr: 0.000235. clip: 0.093862\n",
      "Iteration 2050: Policy loss: 0.180670. Value loss: 0.114988. Entropy: 0.315315.\n",
      "Iteration 2051: Policy loss: 0.169311. Value loss: 0.056324. Entropy: 0.315929.\n",
      "Iteration 2052: Policy loss: 0.174117. Value loss: 0.042029. Entropy: 0.314686.\n",
      "episode: 837   score: 410.0  epsilon: 1.0    steps: 72  evaluation reward: 226.95\n",
      "episode: 838   score: 210.0  epsilon: 1.0    steps: 712  evaluation reward: 226.3\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2053: Policy loss: 0.007573. Value loss: 0.065583. Entropy: 0.311941.\n",
      "Iteration 2054: Policy loss: 0.004064. Value loss: 0.027325. Entropy: 0.311361.\n",
      "Iteration 2055: Policy loss: 0.003718. Value loss: 0.020009. Entropy: 0.311007.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2056: Policy loss: -0.093240. Value loss: 0.087985. Entropy: 0.316118.\n",
      "Iteration 2057: Policy loss: -0.094223. Value loss: 0.044149. Entropy: 0.313893.\n",
      "Iteration 2058: Policy loss: -0.095930. Value loss: 0.030920. Entropy: 0.316425.\n",
      "episode: 839   score: 435.0  epsilon: 1.0    steps: 896  evaluation reward: 228.55\n",
      "episode: 840   score: 180.0  epsilon: 1.0    steps: 1000  evaluation reward: 228.25\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2059: Policy loss: 0.342862. Value loss: 0.194813. Entropy: 0.315300.\n",
      "Iteration 2060: Policy loss: 0.324027. Value loss: 0.059742. Entropy: 0.314694.\n",
      "Iteration 2061: Policy loss: 0.314938. Value loss: 0.029764. Entropy: 0.314316.\n",
      "episode: 841   score: 270.0  epsilon: 1.0    steps: 128  evaluation reward: 229.0\n",
      "episode: 842   score: 135.0  epsilon: 1.0    steps: 264  evaluation reward: 228.25\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2062: Policy loss: -0.030955. Value loss: 0.061137. Entropy: 0.311335.\n",
      "Iteration 2063: Policy loss: -0.037066. Value loss: 0.022434. Entropy: 0.311611.\n",
      "Iteration 2064: Policy loss: -0.032148. Value loss: 0.014406. Entropy: 0.311785.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2065: Policy loss: -0.102739. Value loss: 0.229005. Entropy: 0.313368.\n",
      "Iteration 2066: Policy loss: -0.093944. Value loss: 0.090596. Entropy: 0.312448.\n",
      "Iteration 2067: Policy loss: -0.109578. Value loss: 0.059804. Entropy: 0.311994.\n",
      "episode: 843   score: 105.0  epsilon: 1.0    steps: 376  evaluation reward: 228.2\n",
      "episode: 844   score: 320.0  epsilon: 1.0    steps: 896  evaluation reward: 229.75\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2068: Policy loss: 0.155362. Value loss: 0.123059. Entropy: 0.309500.\n",
      "Iteration 2069: Policy loss: 0.158714. Value loss: 0.066950. Entropy: 0.308917.\n",
      "Iteration 2070: Policy loss: 0.145605. Value loss: 0.055229. Entropy: 0.308119.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2071: Policy loss: -0.038710. Value loss: 0.042836. Entropy: 0.313501.\n",
      "Iteration 2072: Policy loss: -0.039377. Value loss: 0.023562. Entropy: 0.312910.\n",
      "Iteration 2073: Policy loss: -0.042679. Value loss: 0.017143. Entropy: 0.313041.\n",
      "episode: 845   score: 210.0  epsilon: 1.0    steps: 296  evaluation reward: 227.2\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2074: Policy loss: 0.114412. Value loss: 0.041429. Entropy: 0.312773.\n",
      "Iteration 2075: Policy loss: 0.108604. Value loss: 0.021090. Entropy: 0.311405.\n",
      "Iteration 2076: Policy loss: 0.108285. Value loss: 0.016465. Entropy: 0.311051.\n",
      "episode: 846   score: 445.0  epsilon: 1.0    steps: 776  evaluation reward: 228.9\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2077: Policy loss: -0.051956. Value loss: 0.069607. Entropy: 0.311679.\n",
      "Iteration 2078: Policy loss: -0.049627. Value loss: 0.035723. Entropy: 0.312273.\n",
      "Iteration 2079: Policy loss: -0.050371. Value loss: 0.028158. Entropy: 0.312107.\n",
      "episode: 847   score: 210.0  epsilon: 1.0    steps: 560  evaluation reward: 230.25\n",
      "episode: 848   score: 210.0  epsilon: 1.0    steps: 568  evaluation reward: 231.25\n",
      "episode: 849   score: 210.0  epsilon: 1.0    steps: 600  evaluation reward: 229.25\n",
      "episode: 850   score: 210.0  epsilon: 1.0    steps: 720  evaluation reward: 228.95\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2080: Policy loss: 0.042412. Value loss: 0.062559. Entropy: 0.312803.\n",
      "Iteration 2081: Policy loss: 0.041107. Value loss: 0.034742. Entropy: 0.313471.\n",
      "Iteration 2082: Policy loss: 0.035592. Value loss: 0.028362. Entropy: 0.312519.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2083: Policy loss: -0.260016. Value loss: 0.150829. Entropy: 0.312079.\n",
      "Iteration 2084: Policy loss: -0.281640. Value loss: 0.067347. Entropy: 0.311806.\n",
      "Iteration 2085: Policy loss: -0.292905. Value loss: 0.044574. Entropy: 0.312273.\n",
      "now time :  2019-09-05 16:24:47.016839\n",
      "episode: 851   score: 120.0  epsilon: 1.0    steps: 936  evaluation reward: 229.35\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2086: Policy loss: 0.196779. Value loss: 0.086112. Entropy: 0.311083.\n",
      "Iteration 2087: Policy loss: 0.199092. Value loss: 0.044458. Entropy: 0.309895.\n",
      "Iteration 2088: Policy loss: 0.196404. Value loss: 0.035923. Entropy: 0.310319.\n",
      "episode: 852   score: 410.0  epsilon: 1.0    steps: 24  evaluation reward: 230.8\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2089: Policy loss: -0.070023. Value loss: 0.040981. Entropy: 0.310949.\n",
      "Iteration 2090: Policy loss: -0.070614. Value loss: 0.019169. Entropy: 0.312291.\n",
      "Iteration 2091: Policy loss: -0.068422. Value loss: 0.015541. Entropy: 0.311644.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2092: Policy loss: -0.082537. Value loss: 0.044137. Entropy: 0.317783.\n",
      "Iteration 2093: Policy loss: -0.087580. Value loss: 0.018926. Entropy: 0.317257.\n",
      "Iteration 2094: Policy loss: -0.083487. Value loss: 0.014124. Entropy: 0.317006.\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2095: Policy loss: 0.110132. Value loss: 0.065015. Entropy: 0.309786.\n",
      "Iteration 2096: Policy loss: 0.103493. Value loss: 0.026894. Entropy: 0.308336.\n",
      "Iteration 2097: Policy loss: 0.099607. Value loss: 0.019370. Entropy: 0.308815.\n",
      "episode: 853   score: 260.0  epsilon: 1.0    steps: 96  evaluation reward: 232.05\n",
      "episode: 854   score: 105.0  epsilon: 1.0    steps: 480  evaluation reward: 230.85\n",
      "episode: 855   score: 180.0  epsilon: 1.0    steps: 768  evaluation reward: 232.1\n",
      "Training network. lr: 0.000234. clip: 0.093705\n",
      "Iteration 2098: Policy loss: 0.197159. Value loss: 0.094998. Entropy: 0.314024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2099: Policy loss: 0.187381. Value loss: 0.051266. Entropy: 0.313835.\n",
      "Iteration 2100: Policy loss: 0.185924. Value loss: 0.037994. Entropy: 0.314115.\n",
      "episode: 856   score: 260.0  epsilon: 1.0    steps: 592  evaluation reward: 232.9\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2101: Policy loss: 0.115766. Value loss: 0.068279. Entropy: 0.314547.\n",
      "Iteration 2102: Policy loss: 0.111018. Value loss: 0.034848. Entropy: 0.313725.\n",
      "Iteration 2103: Policy loss: 0.111075. Value loss: 0.023849. Entropy: 0.314195.\n",
      "episode: 857   score: 260.0  epsilon: 1.0    steps: 208  evaluation reward: 234.45\n",
      "episode: 858   score: 260.0  epsilon: 1.0    steps: 384  evaluation reward: 235.25\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2104: Policy loss: 0.266719. Value loss: 0.093514. Entropy: 0.311927.\n",
      "Iteration 2105: Policy loss: 0.257548. Value loss: 0.038369. Entropy: 0.311845.\n",
      "Iteration 2106: Policy loss: 0.255513. Value loss: 0.028374. Entropy: 0.311472.\n",
      "episode: 859   score: 180.0  epsilon: 1.0    steps: 192  evaluation reward: 235.2\n",
      "episode: 860   score: 210.0  epsilon: 1.0    steps: 760  evaluation reward: 235.2\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2107: Policy loss: 0.098464. Value loss: 0.059852. Entropy: 0.312841.\n",
      "Iteration 2108: Policy loss: 0.099106. Value loss: 0.036272. Entropy: 0.312748.\n",
      "Iteration 2109: Policy loss: 0.091685. Value loss: 0.031346. Entropy: 0.312475.\n",
      "episode: 861   score: 50.0  epsilon: 1.0    steps: 568  evaluation reward: 234.35\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2110: Policy loss: -0.138587. Value loss: 0.207858. Entropy: 0.314193.\n",
      "Iteration 2111: Policy loss: -0.141477. Value loss: 0.065596. Entropy: 0.313613.\n",
      "Iteration 2112: Policy loss: -0.152978. Value loss: 0.038215. Entropy: 0.314744.\n",
      "episode: 862   score: 105.0  epsilon: 1.0    steps: 512  evaluation reward: 233.6\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2113: Policy loss: 0.017317. Value loss: 0.063489. Entropy: 0.317561.\n",
      "Iteration 2114: Policy loss: 0.017015. Value loss: 0.031089. Entropy: 0.317629.\n",
      "Iteration 2115: Policy loss: 0.011787. Value loss: 0.023122. Entropy: 0.317177.\n",
      "episode: 863   score: 75.0  epsilon: 1.0    steps: 624  evaluation reward: 233.3\n",
      "episode: 864   score: 380.0  epsilon: 1.0    steps: 792  evaluation reward: 233.0\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2116: Policy loss: 0.012559. Value loss: 0.052195. Entropy: 0.312808.\n",
      "Iteration 2117: Policy loss: 0.010306. Value loss: 0.025642. Entropy: 0.313527.\n",
      "Iteration 2118: Policy loss: 0.005887. Value loss: 0.018431. Entropy: 0.312121.\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2119: Policy loss: 0.016357. Value loss: 0.058855. Entropy: 0.318999.\n",
      "Iteration 2120: Policy loss: 0.010862. Value loss: 0.029867. Entropy: 0.318954.\n",
      "Iteration 2121: Policy loss: 0.013726. Value loss: 0.017763. Entropy: 0.318631.\n",
      "episode: 865   score: 260.0  epsilon: 1.0    steps: 216  evaluation reward: 231.5\n",
      "episode: 866   score: 210.0  epsilon: 1.0    steps: 864  evaluation reward: 232.2\n",
      "episode: 867   score: 155.0  epsilon: 1.0    steps: 968  evaluation reward: 231.95\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2122: Policy loss: -0.034020. Value loss: 0.122278. Entropy: 0.318182.\n",
      "Iteration 2123: Policy loss: -0.045002. Value loss: 0.040433. Entropy: 0.316072.\n",
      "Iteration 2124: Policy loss: -0.047995. Value loss: 0.035887. Entropy: 0.316201.\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2125: Policy loss: -0.039915. Value loss: 0.059316. Entropy: 0.317923.\n",
      "Iteration 2126: Policy loss: -0.046076. Value loss: 0.033179. Entropy: 0.317998.\n",
      "Iteration 2127: Policy loss: -0.043980. Value loss: 0.020996. Entropy: 0.318008.\n",
      "episode: 868   score: 410.0  epsilon: 1.0    steps: 264  evaluation reward: 234.95\n",
      "episode: 869   score: 180.0  epsilon: 1.0    steps: 944  evaluation reward: 235.35\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2128: Policy loss: -0.014176. Value loss: 0.073001. Entropy: 0.313228.\n",
      "Iteration 2129: Policy loss: -0.021358. Value loss: 0.042764. Entropy: 0.312226.\n",
      "Iteration 2130: Policy loss: -0.021562. Value loss: 0.032557. Entropy: 0.312980.\n",
      "episode: 870   score: 160.0  epsilon: 1.0    steps: 680  evaluation reward: 235.4\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2131: Policy loss: -0.173396. Value loss: 0.236832. Entropy: 0.315241.\n",
      "Iteration 2132: Policy loss: -0.183675. Value loss: 0.067376. Entropy: 0.315634.\n",
      "Iteration 2133: Policy loss: -0.179059. Value loss: 0.033828. Entropy: 0.316787.\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2134: Policy loss: -0.014238. Value loss: 0.054975. Entropy: 0.317094.\n",
      "Iteration 2135: Policy loss: -0.019458. Value loss: 0.023524. Entropy: 0.318000.\n",
      "Iteration 2136: Policy loss: -0.020115. Value loss: 0.019029. Entropy: 0.317201.\n",
      "episode: 871   score: 180.0  epsilon: 1.0    steps: 72  evaluation reward: 233.4\n",
      "episode: 872   score: 410.0  epsilon: 1.0    steps: 360  evaluation reward: 236.15\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2137: Policy loss: 0.003436. Value loss: 0.103641. Entropy: 0.313533.\n",
      "Iteration 2138: Policy loss: -0.000132. Value loss: 0.061172. Entropy: 0.313592.\n",
      "Iteration 2139: Policy loss: 0.006512. Value loss: 0.044701. Entropy: 0.312181.\n",
      "episode: 873   score: 210.0  epsilon: 1.0    steps: 976  evaluation reward: 237.2\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2140: Policy loss: 0.130851. Value loss: 0.102472. Entropy: 0.318416.\n",
      "Iteration 2141: Policy loss: 0.126781. Value loss: 0.047560. Entropy: 0.318368.\n",
      "Iteration 2142: Policy loss: 0.121445. Value loss: 0.034240. Entropy: 0.317647.\n",
      "episode: 874   score: 180.0  epsilon: 1.0    steps: 504  evaluation reward: 234.4\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2143: Policy loss: 0.083722. Value loss: 0.173057. Entropy: 0.315614.\n",
      "Iteration 2144: Policy loss: 0.075642. Value loss: 0.055857. Entropy: 0.314865.\n",
      "Iteration 2145: Policy loss: 0.063780. Value loss: 0.030842. Entropy: 0.314157.\n",
      "episode: 875   score: 155.0  epsilon: 1.0    steps: 552  evaluation reward: 233.35\n",
      "episode: 876   score: 240.0  epsilon: 1.0    steps: 800  evaluation reward: 233.95\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2146: Policy loss: 0.214465. Value loss: 0.077763. Entropy: 0.316438.\n",
      "Iteration 2147: Policy loss: 0.211508. Value loss: 0.033467. Entropy: 0.316842.\n",
      "Iteration 2148: Policy loss: 0.203152. Value loss: 0.025269. Entropy: 0.316679.\n",
      "episode: 877   score: 180.0  epsilon: 1.0    steps: 464  evaluation reward: 233.95\n",
      "Training network. lr: 0.000234. clip: 0.093558\n",
      "Iteration 2149: Policy loss: 0.033434. Value loss: 0.040648. Entropy: 0.317878.\n",
      "Iteration 2150: Policy loss: 0.030008. Value loss: 0.023136. Entropy: 0.317040.\n",
      "Iteration 2151: Policy loss: 0.032871. Value loss: 0.017487. Entropy: 0.317018.\n",
      "episode: 878   score: 180.0  epsilon: 1.0    steps: 128  evaluation reward: 231.95\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2152: Policy loss: -0.172022. Value loss: 0.235773. Entropy: 0.315685.\n",
      "Iteration 2153: Policy loss: -0.170073. Value loss: 0.095688. Entropy: 0.314492.\n",
      "Iteration 2154: Policy loss: -0.160529. Value loss: 0.058778. Entropy: 0.314734.\n",
      "episode: 879   score: 210.0  epsilon: 1.0    steps: 664  evaluation reward: 232.6\n",
      "episode: 880   score: 410.0  epsilon: 1.0    steps: 688  evaluation reward: 235.35\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2155: Policy loss: 0.056209. Value loss: 0.066094. Entropy: 0.315318.\n",
      "Iteration 2156: Policy loss: 0.053824. Value loss: 0.031718. Entropy: 0.314148.\n",
      "Iteration 2157: Policy loss: 0.058078. Value loss: 0.023514. Entropy: 0.314428.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2158: Policy loss: 0.035074. Value loss: 0.056150. Entropy: 0.313673.\n",
      "Iteration 2159: Policy loss: 0.025176. Value loss: 0.030357. Entropy: 0.312094.\n",
      "Iteration 2160: Policy loss: 0.026200. Value loss: 0.022883. Entropy: 0.311554.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2161: Policy loss: -0.058324. Value loss: 0.097386. Entropy: 0.313535.\n",
      "Iteration 2162: Policy loss: -0.064170. Value loss: 0.040618. Entropy: 0.313817.\n",
      "Iteration 2163: Policy loss: -0.061664. Value loss: 0.031191. Entropy: 0.312535.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 881   score: 410.0  epsilon: 1.0    steps: 1016  evaluation reward: 237.65\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2164: Policy loss: -0.081376. Value loss: 0.068085. Entropy: 0.317643.\n",
      "Iteration 2165: Policy loss: -0.084689. Value loss: 0.046011. Entropy: 0.317793.\n",
      "Iteration 2166: Policy loss: -0.089889. Value loss: 0.039383. Entropy: 0.318391.\n",
      "episode: 882   score: 300.0  epsilon: 1.0    steps: 72  evaluation reward: 238.85\n",
      "episode: 883   score: 285.0  epsilon: 1.0    steps: 112  evaluation reward: 240.9\n",
      "episode: 884   score: 210.0  epsilon: 1.0    steps: 856  evaluation reward: 240.4\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2167: Policy loss: 0.040483. Value loss: 0.055373. Entropy: 0.312864.\n",
      "Iteration 2168: Policy loss: 0.038685. Value loss: 0.028122. Entropy: 0.312237.\n",
      "Iteration 2169: Policy loss: 0.035219. Value loss: 0.020851. Entropy: 0.313389.\n",
      "episode: 885   score: 210.0  epsilon: 1.0    steps: 896  evaluation reward: 238.7\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2170: Policy loss: -0.017245. Value loss: 0.231633. Entropy: 0.312202.\n",
      "Iteration 2171: Policy loss: -0.016596. Value loss: 0.093311. Entropy: 0.310996.\n",
      "Iteration 2172: Policy loss: -0.037268. Value loss: 0.060803. Entropy: 0.311778.\n",
      "episode: 886   score: 305.0  epsilon: 1.0    steps: 216  evaluation reward: 238.2\n",
      "episode: 887   score: 410.0  epsilon: 1.0    steps: 976  evaluation reward: 238.5\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2173: Policy loss: 0.089921. Value loss: 0.063614. Entropy: 0.313579.\n",
      "Iteration 2174: Policy loss: 0.092390. Value loss: 0.030410. Entropy: 0.314918.\n",
      "Iteration 2175: Policy loss: 0.088602. Value loss: 0.022785. Entropy: 0.314005.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2176: Policy loss: -0.133972. Value loss: 0.054067. Entropy: 0.311806.\n",
      "Iteration 2177: Policy loss: -0.134074. Value loss: 0.036510. Entropy: 0.311578.\n",
      "Iteration 2178: Policy loss: -0.139193. Value loss: 0.029460. Entropy: 0.311670.\n",
      "episode: 888   score: 270.0  epsilon: 1.0    steps: 568  evaluation reward: 240.0\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2179: Policy loss: 0.184949. Value loss: 0.209816. Entropy: 0.315740.\n",
      "Iteration 2180: Policy loss: 0.185073. Value loss: 0.063434. Entropy: 0.315233.\n",
      "Iteration 2181: Policy loss: 0.154533. Value loss: 0.037949. Entropy: 0.314326.\n",
      "episode: 889   score: 110.0  epsilon: 1.0    steps: 136  evaluation reward: 240.6\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2182: Policy loss: 0.045529. Value loss: 0.069699. Entropy: 0.317062.\n",
      "Iteration 2183: Policy loss: 0.036226. Value loss: 0.032192. Entropy: 0.316925.\n",
      "Iteration 2184: Policy loss: 0.042631. Value loss: 0.023505. Entropy: 0.315825.\n",
      "episode: 890   score: 180.0  epsilon: 1.0    steps: 976  evaluation reward: 240.6\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2185: Policy loss: 0.051807. Value loss: 0.074192. Entropy: 0.311654.\n",
      "Iteration 2186: Policy loss: 0.048771. Value loss: 0.041617. Entropy: 0.313800.\n",
      "Iteration 2187: Policy loss: 0.041419. Value loss: 0.033507. Entropy: 0.311366.\n",
      "episode: 891   score: 260.0  epsilon: 1.0    steps: 576  evaluation reward: 241.4\n",
      "episode: 892   score: 260.0  epsilon: 1.0    steps: 832  evaluation reward: 241.9\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2188: Policy loss: 0.276308. Value loss: 0.069828. Entropy: 0.313878.\n",
      "Iteration 2189: Policy loss: 0.276828. Value loss: 0.035179. Entropy: 0.314433.\n",
      "Iteration 2190: Policy loss: 0.268461. Value loss: 0.027788. Entropy: 0.315088.\n",
      "episode: 893   score: 210.0  epsilon: 1.0    steps: 704  evaluation reward: 241.9\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2191: Policy loss: -0.033327. Value loss: 0.052402. Entropy: 0.311995.\n",
      "Iteration 2192: Policy loss: -0.034214. Value loss: 0.026371. Entropy: 0.312393.\n",
      "Iteration 2193: Policy loss: -0.033402. Value loss: 0.021153. Entropy: 0.310819.\n",
      "episode: 894   score: 260.0  epsilon: 1.0    steps: 448  evaluation reward: 240.4\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2194: Policy loss: 0.062983. Value loss: 0.052909. Entropy: 0.317311.\n",
      "Iteration 2195: Policy loss: 0.068178. Value loss: 0.028568. Entropy: 0.316699.\n",
      "Iteration 2196: Policy loss: 0.062897. Value loss: 0.021271. Entropy: 0.315191.\n",
      "episode: 895   score: 260.0  epsilon: 1.0    steps: 368  evaluation reward: 238.0\n",
      "episode: 896   score: 160.0  epsilon: 1.0    steps: 624  evaluation reward: 237.2\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2197: Policy loss: 0.027973. Value loss: 0.051240. Entropy: 0.314692.\n",
      "Iteration 2198: Policy loss: 0.026728. Value loss: 0.026729. Entropy: 0.313299.\n",
      "Iteration 2199: Policy loss: 0.027310. Value loss: 0.020642. Entropy: 0.312790.\n",
      "Training network. lr: 0.000234. clip: 0.093401\n",
      "Iteration 2200: Policy loss: 0.045900. Value loss: 0.043427. Entropy: 0.316711.\n",
      "Iteration 2201: Policy loss: 0.042282. Value loss: 0.031683. Entropy: 0.316524.\n",
      "Iteration 2202: Policy loss: 0.043126. Value loss: 0.026154. Entropy: 0.317070.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2203: Policy loss: 0.086818. Value loss: 0.072293. Entropy: 0.317240.\n",
      "Iteration 2204: Policy loss: 0.085914. Value loss: 0.032801. Entropy: 0.315990.\n",
      "Iteration 2205: Policy loss: 0.090869. Value loss: 0.023988. Entropy: 0.317447.\n",
      "episode: 897   score: 265.0  epsilon: 1.0    steps: 80  evaluation reward: 237.75\n",
      "episode: 898   score: 120.0  epsilon: 1.0    steps: 288  evaluation reward: 236.55\n",
      "episode: 899   score: 210.0  epsilon: 1.0    steps: 784  evaluation reward: 237.55\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2206: Policy loss: -0.184703. Value loss: 0.258925. Entropy: 0.315351.\n",
      "Iteration 2207: Policy loss: -0.189496. Value loss: 0.100544. Entropy: 0.314820.\n",
      "Iteration 2208: Policy loss: -0.193874. Value loss: 0.067732. Entropy: 0.315937.\n",
      "episode: 900   score: 210.0  epsilon: 1.0    steps: 408  evaluation reward: 237.85\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2209: Policy loss: -0.074469. Value loss: 0.070816. Entropy: 0.310792.\n",
      "Iteration 2210: Policy loss: -0.074943. Value loss: 0.039897. Entropy: 0.311884.\n",
      "Iteration 2211: Policy loss: -0.075758. Value loss: 0.027833. Entropy: 0.311568.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2212: Policy loss: 0.011202. Value loss: 0.085839. Entropy: 0.315514.\n",
      "Iteration 2213: Policy loss: 0.009156. Value loss: 0.034456. Entropy: 0.316288.\n",
      "Iteration 2214: Policy loss: 0.008102. Value loss: 0.022451. Entropy: 0.314691.\n",
      "now time :  2019-09-05 16:32:46.730887\n",
      "episode: 901   score: 310.0  epsilon: 1.0    steps: 576  evaluation reward: 238.85\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2215: Policy loss: 0.013756. Value loss: 0.292341. Entropy: 0.314370.\n",
      "Iteration 2216: Policy loss: 0.001823. Value loss: 0.172923. Entropy: 0.312642.\n",
      "Iteration 2217: Policy loss: -0.002024. Value loss: 0.138196. Entropy: 0.313178.\n",
      "episode: 902   score: 260.0  epsilon: 1.0    steps: 216  evaluation reward: 240.95\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2218: Policy loss: 0.099464. Value loss: 0.084538. Entropy: 0.315059.\n",
      "Iteration 2219: Policy loss: 0.093351. Value loss: 0.026365. Entropy: 0.313568.\n",
      "Iteration 2220: Policy loss: 0.090258. Value loss: 0.020409. Entropy: 0.314346.\n",
      "episode: 903   score: 240.0  epsilon: 1.0    steps: 584  evaluation reward: 241.25\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2221: Policy loss: -0.093669. Value loss: 0.269584. Entropy: 0.309799.\n",
      "Iteration 2222: Policy loss: -0.102786. Value loss: 0.113187. Entropy: 0.310022.\n",
      "Iteration 2223: Policy loss: -0.092330. Value loss: 0.053487. Entropy: 0.309379.\n",
      "episode: 904   score: 135.0  epsilon: 1.0    steps: 392  evaluation reward: 238.9\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2224: Policy loss: 0.101133. Value loss: 0.165377. Entropy: 0.313968.\n",
      "Iteration 2225: Policy loss: 0.098903. Value loss: 0.059667. Entropy: 0.311915.\n",
      "Iteration 2226: Policy loss: 0.081259. Value loss: 0.041804. Entropy: 0.312643.\n",
      "episode: 905   score: 515.0  epsilon: 1.0    steps: 184  evaluation reward: 241.95\n",
      "episode: 906   score: 410.0  epsilon: 1.0    steps: 512  evaluation reward: 239.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 907   score: 210.0  epsilon: 1.0    steps: 856  evaluation reward: 240.1\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2227: Policy loss: -0.055754. Value loss: 0.064109. Entropy: 0.309835.\n",
      "Iteration 2228: Policy loss: -0.057385. Value loss: 0.027767. Entropy: 0.308416.\n",
      "Iteration 2229: Policy loss: -0.058272. Value loss: 0.019794. Entropy: 0.307930.\n",
      "episode: 908   score: 110.0  epsilon: 1.0    steps: 1016  evaluation reward: 238.15\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2230: Policy loss: 0.082338. Value loss: 0.059480. Entropy: 0.311577.\n",
      "Iteration 2231: Policy loss: 0.072174. Value loss: 0.028261. Entropy: 0.311272.\n",
      "Iteration 2232: Policy loss: 0.077377. Value loss: 0.021993. Entropy: 0.313319.\n",
      "episode: 909   score: 110.0  epsilon: 1.0    steps: 464  evaluation reward: 236.35\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2233: Policy loss: 0.090755. Value loss: 0.090228. Entropy: 0.310182.\n",
      "Iteration 2234: Policy loss: 0.086314. Value loss: 0.044945. Entropy: 0.311391.\n",
      "Iteration 2235: Policy loss: 0.075704. Value loss: 0.033208. Entropy: 0.310062.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2236: Policy loss: 0.022409. Value loss: 0.076444. Entropy: 0.320251.\n",
      "Iteration 2237: Policy loss: 0.013035. Value loss: 0.032941. Entropy: 0.320179.\n",
      "Iteration 2238: Policy loss: 0.013243. Value loss: 0.026062. Entropy: 0.317904.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2239: Policy loss: 0.135887. Value loss: 0.046521. Entropy: 0.314971.\n",
      "Iteration 2240: Policy loss: 0.136845. Value loss: 0.023034. Entropy: 0.314464.\n",
      "Iteration 2241: Policy loss: 0.138003. Value loss: 0.016946. Entropy: 0.314339.\n",
      "episode: 910   score: 155.0  epsilon: 1.0    steps: 24  evaluation reward: 235.95\n",
      "episode: 911   score: 425.0  epsilon: 1.0    steps: 32  evaluation reward: 237.6\n",
      "episode: 912   score: 155.0  epsilon: 1.0    steps: 640  evaluation reward: 237.05\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2242: Policy loss: 0.080859. Value loss: 0.082549. Entropy: 0.315669.\n",
      "Iteration 2243: Policy loss: 0.084751. Value loss: 0.030291. Entropy: 0.314796.\n",
      "Iteration 2244: Policy loss: 0.081216. Value loss: 0.022285. Entropy: 0.314278.\n",
      "episode: 913   score: 180.0  epsilon: 1.0    steps: 536  evaluation reward: 237.05\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2245: Policy loss: 0.042928. Value loss: 0.073267. Entropy: 0.315365.\n",
      "Iteration 2246: Policy loss: 0.044234. Value loss: 0.039397. Entropy: 0.315328.\n",
      "Iteration 2247: Policy loss: 0.038459. Value loss: 0.031466. Entropy: 0.315267.\n",
      "Training network. lr: 0.000233. clip: 0.093245\n",
      "Iteration 2248: Policy loss: 0.164424. Value loss: 0.045729. Entropy: 0.313626.\n",
      "Iteration 2249: Policy loss: 0.162751. Value loss: 0.020271. Entropy: 0.316043.\n",
      "Iteration 2250: Policy loss: 0.165519. Value loss: 0.018648. Entropy: 0.314870.\n",
      "episode: 914   score: 285.0  epsilon: 1.0    steps: 416  evaluation reward: 237.5\n",
      "episode: 915   score: 155.0  epsilon: 1.0    steps: 744  evaluation reward: 236.3\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2251: Policy loss: 0.151000. Value loss: 0.052676. Entropy: 0.307684.\n",
      "Iteration 2252: Policy loss: 0.146361. Value loss: 0.024987. Entropy: 0.308326.\n",
      "Iteration 2253: Policy loss: 0.145511. Value loss: 0.018899. Entropy: 0.308389.\n",
      "episode: 916   score: 225.0  epsilon: 1.0    steps: 128  evaluation reward: 234.45\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2254: Policy loss: -0.035883. Value loss: 0.053158. Entropy: 0.310261.\n",
      "Iteration 2255: Policy loss: -0.039012. Value loss: 0.020890. Entropy: 0.311257.\n",
      "Iteration 2256: Policy loss: -0.044784. Value loss: 0.017237. Entropy: 0.312075.\n",
      "episode: 917   score: 105.0  epsilon: 1.0    steps: 208  evaluation reward: 233.4\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2257: Policy loss: -0.175407. Value loss: 0.061549. Entropy: 0.312782.\n",
      "Iteration 2258: Policy loss: -0.180979. Value loss: 0.029066. Entropy: 0.312936.\n",
      "Iteration 2259: Policy loss: -0.184850. Value loss: 0.020249. Entropy: 0.311912.\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2260: Policy loss: -0.346955. Value loss: 0.281626. Entropy: 0.309095.\n",
      "Iteration 2261: Policy loss: -0.363283. Value loss: 0.188443. Entropy: 0.313320.\n",
      "Iteration 2262: Policy loss: -0.346641. Value loss: 0.067511. Entropy: 0.311381.\n",
      "episode: 918   score: 180.0  epsilon: 1.0    steps: 24  evaluation reward: 233.4\n",
      "episode: 919   score: 240.0  epsilon: 1.0    steps: 744  evaluation reward: 233.7\n",
      "episode: 920   score: 460.0  epsilon: 1.0    steps: 840  evaluation reward: 236.5\n",
      "episode: 921   score: 210.0  epsilon: 1.0    steps: 968  evaluation reward: 233.7\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2263: Policy loss: -0.000660. Value loss: 0.056751. Entropy: 0.311088.\n",
      "Iteration 2264: Policy loss: -0.004656. Value loss: 0.027582. Entropy: 0.310686.\n",
      "Iteration 2265: Policy loss: -0.006048. Value loss: 0.020191. Entropy: 0.310607.\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2266: Policy loss: 0.235070. Value loss: 0.072665. Entropy: 0.310526.\n",
      "Iteration 2267: Policy loss: 0.233679. Value loss: 0.029982. Entropy: 0.310380.\n",
      "Iteration 2268: Policy loss: 0.219347. Value loss: 0.022791. Entropy: 0.308658.\n",
      "episode: 922   score: 210.0  epsilon: 1.0    steps: 976  evaluation reward: 234.7\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2269: Policy loss: -0.005811. Value loss: 0.070312. Entropy: 0.311498.\n",
      "Iteration 2270: Policy loss: -0.010561. Value loss: 0.037774. Entropy: 0.310826.\n",
      "Iteration 2271: Policy loss: -0.010610. Value loss: 0.030639. Entropy: 0.310468.\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2272: Policy loss: -0.035605. Value loss: 0.054450. Entropy: 0.312536.\n",
      "Iteration 2273: Policy loss: -0.040111. Value loss: 0.027020. Entropy: 0.312628.\n",
      "Iteration 2274: Policy loss: -0.040567. Value loss: 0.022452. Entropy: 0.312262.\n",
      "episode: 923   score: 275.0  epsilon: 1.0    steps: 1008  evaluation reward: 235.65\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2275: Policy loss: 0.151935. Value loss: 0.069045. Entropy: 0.309562.\n",
      "Iteration 2276: Policy loss: 0.146530. Value loss: 0.029159. Entropy: 0.308951.\n",
      "Iteration 2277: Policy loss: 0.148127. Value loss: 0.024270. Entropy: 0.309192.\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2278: Policy loss: 0.056934. Value loss: 0.054170. Entropy: 0.311489.\n",
      "Iteration 2279: Policy loss: 0.042294. Value loss: 0.027940. Entropy: 0.310601.\n",
      "Iteration 2280: Policy loss: 0.045074. Value loss: 0.021399. Entropy: 0.309960.\n",
      "episode: 924   score: 155.0  epsilon: 1.0    steps: 280  evaluation reward: 234.6\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2281: Policy loss: -0.064169. Value loss: 0.065584. Entropy: 0.309053.\n",
      "Iteration 2282: Policy loss: -0.058602. Value loss: 0.032296. Entropy: 0.307853.\n",
      "Iteration 2283: Policy loss: -0.065613. Value loss: 0.026250. Entropy: 0.307707.\n",
      "episode: 925   score: 210.0  epsilon: 1.0    steps: 248  evaluation reward: 234.6\n",
      "episode: 926   score: 210.0  epsilon: 1.0    steps: 656  evaluation reward: 234.2\n",
      "episode: 927   score: 260.0  epsilon: 1.0    steps: 824  evaluation reward: 235.25\n",
      "episode: 928   score: 110.0  epsilon: 1.0    steps: 1016  evaluation reward: 234.25\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2284: Policy loss: 0.034482. Value loss: 0.103016. Entropy: 0.303423.\n",
      "Iteration 2285: Policy loss: 0.040813. Value loss: 0.048141. Entropy: 0.305475.\n",
      "Iteration 2286: Policy loss: 0.026671. Value loss: 0.036514. Entropy: 0.305638.\n",
      "episode: 929   score: 370.0  epsilon: 1.0    steps: 272  evaluation reward: 235.85\n",
      "episode: 930   score: 230.0  epsilon: 1.0    steps: 592  evaluation reward: 236.35\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2287: Policy loss: 0.262521. Value loss: 0.038057. Entropy: 0.308713.\n",
      "Iteration 2288: Policy loss: 0.259227. Value loss: 0.014256. Entropy: 0.308212.\n",
      "Iteration 2289: Policy loss: 0.255596. Value loss: 0.010468. Entropy: 0.307468.\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2290: Policy loss: -0.053251. Value loss: 0.058709. Entropy: 0.303769.\n",
      "Iteration 2291: Policy loss: -0.058538. Value loss: 0.030847. Entropy: 0.302724.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2292: Policy loss: -0.063155. Value loss: 0.025565. Entropy: 0.303731.\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2293: Policy loss: -0.006727. Value loss: 0.083363. Entropy: 0.302939.\n",
      "Iteration 2294: Policy loss: -0.006923. Value loss: 0.035899. Entropy: 0.304224.\n",
      "Iteration 2295: Policy loss: -0.009924. Value loss: 0.026893. Entropy: 0.303997.\n",
      "episode: 931   score: 180.0  epsilon: 1.0    steps: 720  evaluation reward: 236.35\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2296: Policy loss: 0.102585. Value loss: 0.035325. Entropy: 0.312669.\n",
      "Iteration 2297: Policy loss: 0.100874. Value loss: 0.014777. Entropy: 0.312647.\n",
      "Iteration 2298: Policy loss: 0.098819. Value loss: 0.012489. Entropy: 0.312109.\n",
      "episode: 932   score: 240.0  epsilon: 1.0    steps: 624  evaluation reward: 237.7\n",
      "episode: 933   score: 135.0  epsilon: 1.0    steps: 904  evaluation reward: 237.4\n",
      "Training network. lr: 0.000233. clip: 0.093097\n",
      "Iteration 2299: Policy loss: 0.168693. Value loss: 0.046538. Entropy: 0.316236.\n",
      "Iteration 2300: Policy loss: 0.167963. Value loss: 0.022144. Entropy: 0.316055.\n",
      "Iteration 2301: Policy loss: 0.170797. Value loss: 0.017358. Entropy: 0.317230.\n",
      "episode: 934   score: 135.0  epsilon: 1.0    steps: 384  evaluation reward: 237.15\n",
      "episode: 935   score: 105.0  epsilon: 1.0    steps: 600  evaluation reward: 234.1\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2302: Policy loss: 0.137258. Value loss: 0.056085. Entropy: 0.305851.\n",
      "Iteration 2303: Policy loss: 0.134001. Value loss: 0.032588. Entropy: 0.304061.\n",
      "Iteration 2304: Policy loss: 0.135438. Value loss: 0.025498. Entropy: 0.303567.\n",
      "episode: 936   score: 180.0  epsilon: 1.0    steps: 336  evaluation reward: 231.3\n",
      "episode: 937   score: 155.0  epsilon: 1.0    steps: 584  evaluation reward: 228.75\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2305: Policy loss: -0.053870. Value loss: 0.073784. Entropy: 0.308432.\n",
      "Iteration 2306: Policy loss: -0.045264. Value loss: 0.034607. Entropy: 0.307608.\n",
      "Iteration 2307: Policy loss: -0.055545. Value loss: 0.024311. Entropy: 0.307357.\n",
      "episode: 938   score: 255.0  epsilon: 1.0    steps: 920  evaluation reward: 229.2\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2308: Policy loss: -0.086148. Value loss: 0.069063. Entropy: 0.313347.\n",
      "Iteration 2309: Policy loss: -0.093945. Value loss: 0.038046. Entropy: 0.312153.\n",
      "Iteration 2310: Policy loss: -0.090948. Value loss: 0.030876. Entropy: 0.311326.\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2311: Policy loss: -0.002556. Value loss: 0.035435. Entropy: 0.313175.\n",
      "Iteration 2312: Policy loss: -0.007318. Value loss: 0.016662. Entropy: 0.312362.\n",
      "Iteration 2313: Policy loss: -0.006833. Value loss: 0.013702. Entropy: 0.313107.\n",
      "episode: 939   score: 50.0  epsilon: 1.0    steps: 696  evaluation reward: 225.35\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2314: Policy loss: -0.260709. Value loss: 0.337321. Entropy: 0.311904.\n",
      "Iteration 2315: Policy loss: -0.258391. Value loss: 0.220435. Entropy: 0.313318.\n",
      "Iteration 2316: Policy loss: -0.248428. Value loss: 0.094140. Entropy: 0.316199.\n",
      "episode: 940   score: 180.0  epsilon: 1.0    steps: 120  evaluation reward: 225.35\n",
      "episode: 941   score: 125.0  epsilon: 1.0    steps: 672  evaluation reward: 223.9\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2317: Policy loss: -0.051329. Value loss: 0.083804. Entropy: 0.309869.\n",
      "Iteration 2318: Policy loss: -0.059347. Value loss: 0.036848. Entropy: 0.307287.\n",
      "Iteration 2319: Policy loss: -0.062085. Value loss: 0.026519. Entropy: 0.308681.\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2320: Policy loss: 0.162858. Value loss: 0.085368. Entropy: 0.312331.\n",
      "Iteration 2321: Policy loss: 0.156615. Value loss: 0.035301. Entropy: 0.312871.\n",
      "Iteration 2322: Policy loss: 0.153688. Value loss: 0.026585. Entropy: 0.314090.\n",
      "episode: 942   score: 180.0  epsilon: 1.0    steps: 16  evaluation reward: 224.35\n",
      "episode: 943   score: 465.0  epsilon: 1.0    steps: 80  evaluation reward: 227.95\n",
      "episode: 944   score: 180.0  epsilon: 1.0    steps: 752  evaluation reward: 226.55\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2323: Policy loss: 0.016359. Value loss: 0.066172. Entropy: 0.307756.\n",
      "Iteration 2324: Policy loss: 0.009853. Value loss: 0.043648. Entropy: 0.309552.\n",
      "Iteration 2325: Policy loss: 0.007998. Value loss: 0.031276. Entropy: 0.311926.\n",
      "episode: 945   score: 210.0  epsilon: 1.0    steps: 72  evaluation reward: 226.55\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2326: Policy loss: -0.028386. Value loss: 0.091410. Entropy: 0.311725.\n",
      "Iteration 2327: Policy loss: -0.025512. Value loss: 0.042755. Entropy: 0.311365.\n",
      "Iteration 2328: Policy loss: -0.036534. Value loss: 0.030560. Entropy: 0.310947.\n",
      "episode: 946   score: 210.0  epsilon: 1.0    steps: 256  evaluation reward: 224.2\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2329: Policy loss: 0.065785. Value loss: 0.053206. Entropy: 0.315377.\n",
      "Iteration 2330: Policy loss: 0.063215. Value loss: 0.032119. Entropy: 0.315266.\n",
      "Iteration 2331: Policy loss: 0.062823. Value loss: 0.021810. Entropy: 0.313218.\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2332: Policy loss: -0.145185. Value loss: 0.347362. Entropy: 0.310760.\n",
      "Iteration 2333: Policy loss: -0.133560. Value loss: 0.177757. Entropy: 0.309377.\n",
      "Iteration 2334: Policy loss: -0.142864. Value loss: 0.086968. Entropy: 0.307832.\n",
      "episode: 947   score: 155.0  epsilon: 1.0    steps: 312  evaluation reward: 223.65\n",
      "episode: 948   score: 155.0  epsilon: 1.0    steps: 544  evaluation reward: 223.1\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2335: Policy loss: 0.011828. Value loss: 0.065363. Entropy: 0.313336.\n",
      "Iteration 2336: Policy loss: 0.012769. Value loss: 0.030197. Entropy: 0.309135.\n",
      "Iteration 2337: Policy loss: 0.009260. Value loss: 0.020151. Entropy: 0.309600.\n",
      "episode: 949   score: 110.0  epsilon: 1.0    steps: 128  evaluation reward: 222.1\n",
      "episode: 950   score: 210.0  epsilon: 1.0    steps: 400  evaluation reward: 222.1\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2338: Policy loss: 0.211146. Value loss: 0.079783. Entropy: 0.306539.\n",
      "Iteration 2339: Policy loss: 0.202215. Value loss: 0.027129. Entropy: 0.306695.\n",
      "Iteration 2340: Policy loss: 0.199821. Value loss: 0.018950. Entropy: 0.306983.\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2341: Policy loss: 0.113432. Value loss: 0.091721. Entropy: 0.307011.\n",
      "Iteration 2342: Policy loss: 0.110183. Value loss: 0.047986. Entropy: 0.307130.\n",
      "Iteration 2343: Policy loss: 0.109384. Value loss: 0.036951. Entropy: 0.307260.\n",
      "now time :  2019-09-05 16:40:46.644762\n",
      "episode: 951   score: 210.0  epsilon: 1.0    steps: 400  evaluation reward: 223.0\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2344: Policy loss: 0.070435. Value loss: 0.059169. Entropy: 0.306415.\n",
      "Iteration 2345: Policy loss: 0.066138. Value loss: 0.024190. Entropy: 0.305627.\n",
      "Iteration 2346: Policy loss: 0.062285. Value loss: 0.017165. Entropy: 0.305392.\n",
      "episode: 952   score: 345.0  epsilon: 1.0    steps: 256  evaluation reward: 222.35\n",
      "episode: 953   score: 485.0  epsilon: 1.0    steps: 616  evaluation reward: 224.6\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2347: Policy loss: -0.055791. Value loss: 0.064853. Entropy: 0.308510.\n",
      "Iteration 2348: Policy loss: -0.063010. Value loss: 0.034930. Entropy: 0.308014.\n",
      "Iteration 2349: Policy loss: -0.065006. Value loss: 0.025177. Entropy: 0.307462.\n",
      "Training network. lr: 0.000232. clip: 0.092941\n",
      "Iteration 2350: Policy loss: -0.289625. Value loss: 0.275494. Entropy: 0.307618.\n",
      "Iteration 2351: Policy loss: -0.329323. Value loss: 0.140260. Entropy: 0.308064.\n",
      "Iteration 2352: Policy loss: -0.324310. Value loss: 0.072891. Entropy: 0.307811.\n",
      "episode: 954   score: 105.0  epsilon: 1.0    steps: 440  evaluation reward: 224.6\n",
      "episode: 955   score: 135.0  epsilon: 1.0    steps: 528  evaluation reward: 224.15\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2353: Policy loss: 0.117797. Value loss: 0.078690. Entropy: 0.307695.\n",
      "Iteration 2354: Policy loss: 0.112635. Value loss: 0.035751. Entropy: 0.310752.\n",
      "Iteration 2355: Policy loss: 0.110422. Value loss: 0.022875. Entropy: 0.309614.\n",
      "episode: 956   score: 410.0  epsilon: 1.0    steps: 48  evaluation reward: 225.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 957   score: 210.0  epsilon: 1.0    steps: 368  evaluation reward: 225.15\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2356: Policy loss: 0.035675. Value loss: 0.106984. Entropy: 0.309629.\n",
      "Iteration 2357: Policy loss: 0.032156. Value loss: 0.036718. Entropy: 0.307967.\n",
      "Iteration 2358: Policy loss: 0.025862. Value loss: 0.021291. Entropy: 0.307593.\n",
      "episode: 958   score: 285.0  epsilon: 1.0    steps: 136  evaluation reward: 225.4\n",
      "episode: 959   score: 210.0  epsilon: 1.0    steps: 944  evaluation reward: 225.7\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2359: Policy loss: 0.173690. Value loss: 0.101289. Entropy: 0.307883.\n",
      "Iteration 2360: Policy loss: 0.171572. Value loss: 0.051215. Entropy: 0.308737.\n",
      "Iteration 2361: Policy loss: 0.172703. Value loss: 0.037749. Entropy: 0.308606.\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2362: Policy loss: 0.128250. Value loss: 0.076492. Entropy: 0.306721.\n",
      "Iteration 2363: Policy loss: 0.122346. Value loss: 0.037707. Entropy: 0.306594.\n",
      "Iteration 2364: Policy loss: 0.119298. Value loss: 0.027107. Entropy: 0.306353.\n",
      "episode: 960   score: 160.0  epsilon: 1.0    steps: 632  evaluation reward: 225.2\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2365: Policy loss: 0.109811. Value loss: 0.054755. Entropy: 0.310158.\n",
      "Iteration 2366: Policy loss: 0.097503. Value loss: 0.029702. Entropy: 0.308442.\n",
      "Iteration 2367: Policy loss: 0.099969. Value loss: 0.023652. Entropy: 0.309090.\n",
      "episode: 961   score: 210.0  epsilon: 1.0    steps: 104  evaluation reward: 226.8\n",
      "episode: 962   score: 105.0  epsilon: 1.0    steps: 344  evaluation reward: 226.8\n",
      "episode: 963   score: 110.0  epsilon: 1.0    steps: 960  evaluation reward: 227.15\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2368: Policy loss: 0.063383. Value loss: 0.054593. Entropy: 0.305287.\n",
      "Iteration 2369: Policy loss: 0.056078. Value loss: 0.032127. Entropy: 0.304480.\n",
      "Iteration 2370: Policy loss: 0.058076. Value loss: 0.021905. Entropy: 0.304298.\n",
      "episode: 964   score: 105.0  epsilon: 1.0    steps: 88  evaluation reward: 224.4\n",
      "episode: 965   score: 210.0  epsilon: 1.0    steps: 832  evaluation reward: 223.9\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2371: Policy loss: 0.078592. Value loss: 0.081163. Entropy: 0.303081.\n",
      "Iteration 2372: Policy loss: 0.069022. Value loss: 0.036421. Entropy: 0.301531.\n",
      "Iteration 2373: Policy loss: 0.072607. Value loss: 0.025449. Entropy: 0.300447.\n",
      "episode: 966   score: 210.0  epsilon: 1.0    steps: 64  evaluation reward: 223.9\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2374: Policy loss: 0.203343. Value loss: 0.070828. Entropy: 0.308770.\n",
      "Iteration 2375: Policy loss: 0.195536. Value loss: 0.035836. Entropy: 0.308987.\n",
      "Iteration 2376: Policy loss: 0.193919. Value loss: 0.026771. Entropy: 0.308408.\n",
      "episode: 967   score: 120.0  epsilon: 1.0    steps: 608  evaluation reward: 223.55\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2377: Policy loss: 0.001874. Value loss: 0.089479. Entropy: 0.306641.\n",
      "Iteration 2378: Policy loss: -0.004345. Value loss: 0.046108. Entropy: 0.305506.\n",
      "Iteration 2379: Policy loss: -0.008452. Value loss: 0.032983. Entropy: 0.305724.\n",
      "episode: 968   score: 75.0  epsilon: 1.0    steps: 448  evaluation reward: 220.2\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2380: Policy loss: 0.125129. Value loss: 0.082384. Entropy: 0.304172.\n",
      "Iteration 2381: Policy loss: 0.118383. Value loss: 0.030501. Entropy: 0.304966.\n",
      "Iteration 2382: Policy loss: 0.113231. Value loss: 0.020773. Entropy: 0.306426.\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2383: Policy loss: -0.329641. Value loss: 0.278479. Entropy: 0.310340.\n",
      "Iteration 2384: Policy loss: -0.334164. Value loss: 0.171189. Entropy: 0.310537.\n",
      "Iteration 2385: Policy loss: -0.320849. Value loss: 0.103023. Entropy: 0.309581.\n",
      "episode: 969   score: 155.0  epsilon: 1.0    steps: 392  evaluation reward: 219.95\n",
      "episode: 970   score: 105.0  epsilon: 1.0    steps: 872  evaluation reward: 219.4\n",
      "episode: 971   score: 135.0  epsilon: 1.0    steps: 960  evaluation reward: 218.95\n",
      "episode: 972   score: 155.0  epsilon: 1.0    steps: 1016  evaluation reward: 216.4\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2386: Policy loss: 0.152468. Value loss: 0.053600. Entropy: 0.305120.\n",
      "Iteration 2387: Policy loss: 0.149972. Value loss: 0.029698. Entropy: 0.304637.\n",
      "Iteration 2388: Policy loss: 0.147511. Value loss: 0.022537. Entropy: 0.304741.\n",
      "episode: 973   score: 105.0  epsilon: 1.0    steps: 464  evaluation reward: 215.35\n",
      "episode: 974   score: 415.0  epsilon: 1.0    steps: 768  evaluation reward: 217.7\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2389: Policy loss: -0.042784. Value loss: 0.062104. Entropy: 0.304070.\n",
      "Iteration 2390: Policy loss: -0.047440. Value loss: 0.025585. Entropy: 0.305403.\n",
      "Iteration 2391: Policy loss: -0.046001. Value loss: 0.019206. Entropy: 0.305401.\n",
      "episode: 975   score: 295.0  epsilon: 1.0    steps: 704  evaluation reward: 219.1\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2392: Policy loss: -0.200503. Value loss: 0.100415. Entropy: 0.305606.\n",
      "Iteration 2393: Policy loss: -0.208024. Value loss: 0.049341. Entropy: 0.304207.\n",
      "Iteration 2394: Policy loss: -0.209889. Value loss: 0.035489. Entropy: 0.305120.\n",
      "episode: 976   score: 75.0  epsilon: 1.0    steps: 920  evaluation reward: 217.45\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2395: Policy loss: 0.006602. Value loss: 0.065746. Entropy: 0.309414.\n",
      "Iteration 2396: Policy loss: 0.001525. Value loss: 0.032319. Entropy: 0.308518.\n",
      "Iteration 2397: Policy loss: 0.004103. Value loss: 0.027018. Entropy: 0.308997.\n",
      "Training network. lr: 0.000232. clip: 0.092784\n",
      "Iteration 2398: Policy loss: 0.210916. Value loss: 0.079852. Entropy: 0.312990.\n",
      "Iteration 2399: Policy loss: 0.203292. Value loss: 0.023293. Entropy: 0.312973.\n",
      "Iteration 2400: Policy loss: 0.204937. Value loss: 0.016921. Entropy: 0.311778.\n",
      "episode: 977   score: 275.0  epsilon: 1.0    steps: 544  evaluation reward: 218.4\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2401: Policy loss: 0.116100. Value loss: 0.057082. Entropy: 0.308411.\n",
      "Iteration 2402: Policy loss: 0.108503. Value loss: 0.025612. Entropy: 0.308278.\n",
      "Iteration 2403: Policy loss: 0.110666. Value loss: 0.019452. Entropy: 0.308037.\n",
      "episode: 978   score: 120.0  epsilon: 1.0    steps: 1008  evaluation reward: 217.8\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2404: Policy loss: -0.036402. Value loss: 0.072519. Entropy: 0.307746.\n",
      "Iteration 2405: Policy loss: -0.039571. Value loss: 0.033176. Entropy: 0.306933.\n",
      "Iteration 2406: Policy loss: -0.043108. Value loss: 0.029265. Entropy: 0.306641.\n",
      "episode: 979   score: 135.0  epsilon: 1.0    steps: 600  evaluation reward: 217.05\n",
      "episode: 980   score: 270.0  epsilon: 1.0    steps: 1000  evaluation reward: 215.65\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2407: Policy loss: 0.016841. Value loss: 0.103208. Entropy: 0.305315.\n",
      "Iteration 2408: Policy loss: 0.007140. Value loss: 0.044481. Entropy: 0.305378.\n",
      "Iteration 2409: Policy loss: 0.002087. Value loss: 0.031102. Entropy: 0.306533.\n",
      "episode: 981   score: 140.0  epsilon: 1.0    steps: 184  evaluation reward: 212.95\n",
      "episode: 982   score: 255.0  epsilon: 1.0    steps: 576  evaluation reward: 212.5\n",
      "episode: 983   score: 290.0  epsilon: 1.0    steps: 768  evaluation reward: 212.55\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2410: Policy loss: 0.193982. Value loss: 0.051280. Entropy: 0.305556.\n",
      "Iteration 2411: Policy loss: 0.190780. Value loss: 0.023422. Entropy: 0.304791.\n",
      "Iteration 2412: Policy loss: 0.195845. Value loss: 0.018068. Entropy: 0.304920.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2413: Policy loss: -0.036624. Value loss: 0.085024. Entropy: 0.304095.\n",
      "Iteration 2414: Policy loss: -0.039666. Value loss: 0.043768. Entropy: 0.303879.\n",
      "Iteration 2415: Policy loss: -0.037111. Value loss: 0.032235. Entropy: 0.304689.\n",
      "episode: 984   score: 210.0  epsilon: 1.0    steps: 616  evaluation reward: 212.55\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2416: Policy loss: 0.053656. Value loss: 0.076837. Entropy: 0.305805.\n",
      "Iteration 2417: Policy loss: 0.051367. Value loss: 0.033720. Entropy: 0.306307.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2418: Policy loss: 0.046958. Value loss: 0.026255. Entropy: 0.306460.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2419: Policy loss: 0.067862. Value loss: 0.035420. Entropy: 0.308262.\n",
      "Iteration 2420: Policy loss: 0.064136. Value loss: 0.016129. Entropy: 0.308356.\n",
      "Iteration 2421: Policy loss: 0.065233. Value loss: 0.012950. Entropy: 0.307238.\n",
      "episode: 985   score: 105.0  epsilon: 1.0    steps: 184  evaluation reward: 211.5\n",
      "episode: 986   score: 85.0  epsilon: 1.0    steps: 200  evaluation reward: 209.3\n",
      "episode: 987   score: 225.0  epsilon: 1.0    steps: 456  evaluation reward: 207.45\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2422: Policy loss: 0.083568. Value loss: 0.080277. Entropy: 0.308596.\n",
      "Iteration 2423: Policy loss: 0.078489. Value loss: 0.039965. Entropy: 0.307918.\n",
      "Iteration 2424: Policy loss: 0.078573. Value loss: 0.031324. Entropy: 0.308338.\n",
      "episode: 988   score: 145.0  epsilon: 1.0    steps: 128  evaluation reward: 206.2\n",
      "episode: 989   score: 155.0  epsilon: 1.0    steps: 952  evaluation reward: 206.65\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2425: Policy loss: 0.029261. Value loss: 0.093994. Entropy: 0.305590.\n",
      "Iteration 2426: Policy loss: 0.021549. Value loss: 0.046730. Entropy: 0.304331.\n",
      "Iteration 2427: Policy loss: 0.021986. Value loss: 0.037887. Entropy: 0.303994.\n",
      "episode: 990   score: 215.0  epsilon: 1.0    steps: 456  evaluation reward: 207.0\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2428: Policy loss: 0.011037. Value loss: 0.067999. Entropy: 0.310528.\n",
      "Iteration 2429: Policy loss: 0.003196. Value loss: 0.031125. Entropy: 0.311119.\n",
      "Iteration 2430: Policy loss: 0.006245. Value loss: 0.025005. Entropy: 0.308920.\n",
      "episode: 991   score: 210.0  epsilon: 1.0    steps: 240  evaluation reward: 206.5\n",
      "episode: 992   score: 80.0  epsilon: 1.0    steps: 264  evaluation reward: 204.7\n",
      "episode: 993   score: 130.0  epsilon: 1.0    steps: 976  evaluation reward: 203.9\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2431: Policy loss: 0.038070. Value loss: 0.067078. Entropy: 0.309720.\n",
      "Iteration 2432: Policy loss: 0.034348. Value loss: 0.033695. Entropy: 0.307185.\n",
      "Iteration 2433: Policy loss: 0.030329. Value loss: 0.022426. Entropy: 0.308590.\n",
      "episode: 994   score: 110.0  epsilon: 1.0    steps: 472  evaluation reward: 202.4\n",
      "episode: 995   score: 210.0  epsilon: 1.0    steps: 880  evaluation reward: 201.9\n",
      "episode: 996   score: 120.0  epsilon: 1.0    steps: 912  evaluation reward: 201.5\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2434: Policy loss: -0.068812. Value loss: 0.104672. Entropy: 0.310209.\n",
      "Iteration 2435: Policy loss: -0.066516. Value loss: 0.047179. Entropy: 0.309169.\n",
      "Iteration 2436: Policy loss: -0.071755. Value loss: 0.037606. Entropy: 0.308782.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2437: Policy loss: 0.333646. Value loss: 0.080627. Entropy: 0.305880.\n",
      "Iteration 2438: Policy loss: 0.335681. Value loss: 0.039631. Entropy: 0.303987.\n",
      "Iteration 2439: Policy loss: 0.333709. Value loss: 0.028991. Entropy: 0.305552.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2440: Policy loss: -0.114834. Value loss: 0.123425. Entropy: 0.303219.\n",
      "Iteration 2441: Policy loss: -0.116270. Value loss: 0.060682. Entropy: 0.303635.\n",
      "Iteration 2442: Policy loss: -0.124783. Value loss: 0.045982. Entropy: 0.305551.\n",
      "episode: 997   score: 195.0  epsilon: 1.0    steps: 816  evaluation reward: 200.8\n",
      "episode: 998   score: 240.0  epsilon: 1.0    steps: 920  evaluation reward: 202.0\n",
      "episode: 999   score: 155.0  epsilon: 1.0    steps: 960  evaluation reward: 201.45\n",
      "episode: 1000   score: 50.0  epsilon: 1.0    steps: 1024  evaluation reward: 199.85\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2443: Policy loss: -0.060112. Value loss: 0.236583. Entropy: 0.305800.\n",
      "Iteration 2444: Policy loss: -0.057584. Value loss: 0.100835. Entropy: 0.306018.\n",
      "Iteration 2445: Policy loss: -0.072330. Value loss: 0.072425. Entropy: 0.305777.\n",
      "now time :  2019-09-05 16:47:05.818263\n",
      "episode: 1001   score: 180.0  epsilon: 1.0    steps: 112  evaluation reward: 198.55\n",
      "episode: 1002   score: 185.0  epsilon: 1.0    steps: 720  evaluation reward: 197.8\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2446: Policy loss: -0.191306. Value loss: 0.274481. Entropy: 0.305262.\n",
      "Iteration 2447: Policy loss: -0.219595. Value loss: 0.221070. Entropy: 0.304723.\n",
      "Iteration 2448: Policy loss: -0.211563. Value loss: 0.203937. Entropy: 0.306372.\n",
      "Training network. lr: 0.000232. clip: 0.092636\n",
      "Iteration 2449: Policy loss: 0.228772. Value loss: 0.104737. Entropy: 0.301160.\n",
      "Iteration 2450: Policy loss: 0.232079. Value loss: 0.057034. Entropy: 0.301332.\n",
      "Iteration 2451: Policy loss: 0.221341. Value loss: 0.039765. Entropy: 0.301358.\n",
      "episode: 1003   score: 135.0  epsilon: 1.0    steps: 568  evaluation reward: 196.75\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2452: Policy loss: -0.106245. Value loss: 0.160826. Entropy: 0.306468.\n",
      "Iteration 2453: Policy loss: -0.123765. Value loss: 0.070256. Entropy: 0.304967.\n",
      "Iteration 2454: Policy loss: -0.129913. Value loss: 0.052635. Entropy: 0.305809.\n",
      "episode: 1004   score: 105.0  epsilon: 1.0    steps: 296  evaluation reward: 196.45\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2455: Policy loss: 0.258349. Value loss: 0.099846. Entropy: 0.307862.\n",
      "Iteration 2456: Policy loss: 0.256465. Value loss: 0.033477. Entropy: 0.306381.\n",
      "Iteration 2457: Policy loss: 0.257050. Value loss: 0.023888. Entropy: 0.306272.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2458: Policy loss: -0.018166. Value loss: 0.094044. Entropy: 0.313283.\n",
      "Iteration 2459: Policy loss: -0.027267. Value loss: 0.049759. Entropy: 0.312912.\n",
      "Iteration 2460: Policy loss: -0.029563. Value loss: 0.034544. Entropy: 0.311992.\n",
      "episode: 1005   score: 535.0  epsilon: 1.0    steps: 40  evaluation reward: 196.65\n",
      "episode: 1006   score: 135.0  epsilon: 1.0    steps: 224  evaluation reward: 193.9\n",
      "episode: 1007   score: 120.0  epsilon: 1.0    steps: 392  evaluation reward: 193.0\n",
      "episode: 1008   score: 140.0  epsilon: 1.0    steps: 688  evaluation reward: 193.3\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2461: Policy loss: 0.029557. Value loss: 0.046255. Entropy: 0.304613.\n",
      "Iteration 2462: Policy loss: 0.031861. Value loss: 0.018115. Entropy: 0.304296.\n",
      "Iteration 2463: Policy loss: 0.025288. Value loss: 0.013552. Entropy: 0.303539.\n",
      "episode: 1009   score: 240.0  epsilon: 1.0    steps: 960  evaluation reward: 194.6\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2464: Policy loss: 0.093428. Value loss: 0.103176. Entropy: 0.308676.\n",
      "Iteration 2465: Policy loss: 0.093567. Value loss: 0.041259. Entropy: 0.307716.\n",
      "Iteration 2466: Policy loss: 0.086532. Value loss: 0.028637. Entropy: 0.307609.\n",
      "episode: 1010   score: 155.0  epsilon: 1.0    steps: 960  evaluation reward: 194.6\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2467: Policy loss: -0.079082. Value loss: 0.093670. Entropy: 0.308775.\n",
      "Iteration 2468: Policy loss: -0.087637. Value loss: 0.050339. Entropy: 0.309694.\n",
      "Iteration 2469: Policy loss: -0.088359. Value loss: 0.036888. Entropy: 0.309441.\n",
      "episode: 1011   score: 75.0  epsilon: 1.0    steps: 376  evaluation reward: 191.1\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2470: Policy loss: 0.021395. Value loss: 0.093925. Entropy: 0.307686.\n",
      "Iteration 2471: Policy loss: 0.018950. Value loss: 0.038731. Entropy: 0.306896.\n",
      "Iteration 2472: Policy loss: 0.012718. Value loss: 0.030783. Entropy: 0.306267.\n",
      "episode: 1012   score: 110.0  epsilon: 1.0    steps: 336  evaluation reward: 190.65\n",
      "episode: 1013   score: 210.0  epsilon: 1.0    steps: 744  evaluation reward: 190.95\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2473: Policy loss: 0.102589. Value loss: 0.083174. Entropy: 0.305454.\n",
      "Iteration 2474: Policy loss: 0.103573. Value loss: 0.045219. Entropy: 0.306133.\n",
      "Iteration 2475: Policy loss: 0.104999. Value loss: 0.034293. Entropy: 0.305663.\n",
      "episode: 1014   score: 135.0  epsilon: 1.0    steps: 904  evaluation reward: 189.45\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2476: Policy loss: 0.211532. Value loss: 0.054904. Entropy: 0.305771.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2477: Policy loss: 0.205135. Value loss: 0.021132. Entropy: 0.305368.\n",
      "Iteration 2478: Policy loss: 0.207363. Value loss: 0.015880. Entropy: 0.304925.\n",
      "episode: 1015   score: 215.0  epsilon: 1.0    steps: 552  evaluation reward: 190.05\n",
      "episode: 1016   score: 35.0  epsilon: 1.0    steps: 904  evaluation reward: 188.15\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2479: Policy loss: 0.154079. Value loss: 0.071900. Entropy: 0.303259.\n",
      "Iteration 2480: Policy loss: 0.151619. Value loss: 0.034289. Entropy: 0.303418.\n",
      "Iteration 2481: Policy loss: 0.141205. Value loss: 0.029060. Entropy: 0.302845.\n",
      "episode: 1017   score: 80.0  epsilon: 1.0    steps: 224  evaluation reward: 187.9\n",
      "episode: 1018   score: 50.0  epsilon: 1.0    steps: 248  evaluation reward: 186.6\n",
      "episode: 1019   score: 80.0  epsilon: 1.0    steps: 592  evaluation reward: 185.0\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2482: Policy loss: -0.155891. Value loss: 0.124803. Entropy: 0.306068.\n",
      "Iteration 2483: Policy loss: -0.165448. Value loss: 0.063470. Entropy: 0.306857.\n",
      "Iteration 2484: Policy loss: -0.172238. Value loss: 0.047894. Entropy: 0.307173.\n",
      "episode: 1020   score: 380.0  epsilon: 1.0    steps: 96  evaluation reward: 184.2\n",
      "episode: 1021   score: 210.0  epsilon: 1.0    steps: 136  evaluation reward: 184.2\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2485: Policy loss: -0.287413. Value loss: 0.087527. Entropy: 0.305890.\n",
      "Iteration 2486: Policy loss: -0.295053. Value loss: 0.040190. Entropy: 0.307257.\n",
      "Iteration 2487: Policy loss: -0.298084. Value loss: 0.031895. Entropy: 0.305745.\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2488: Policy loss: 0.022545. Value loss: 0.089820. Entropy: 0.312657.\n",
      "Iteration 2489: Policy loss: 0.020558. Value loss: 0.042396. Entropy: 0.310857.\n",
      "Iteration 2490: Policy loss: 0.021455. Value loss: 0.033787. Entropy: 0.310908.\n",
      "episode: 1022   score: 120.0  epsilon: 1.0    steps: 648  evaluation reward: 183.3\n",
      "episode: 1023   score: 60.0  epsilon: 1.0    steps: 896  evaluation reward: 181.15\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2491: Policy loss: 0.012817. Value loss: 0.072061. Entropy: 0.310396.\n",
      "Iteration 2492: Policy loss: 0.017265. Value loss: 0.039951. Entropy: 0.310385.\n",
      "Iteration 2493: Policy loss: 0.008609. Value loss: 0.030485. Entropy: 0.310723.\n",
      "episode: 1024   score: 120.0  epsilon: 1.0    steps: 128  evaluation reward: 180.8\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2494: Policy loss: -0.130783. Value loss: 0.101421. Entropy: 0.307715.\n",
      "Iteration 2495: Policy loss: -0.135652. Value loss: 0.053726. Entropy: 0.307398.\n",
      "Iteration 2496: Policy loss: -0.144675. Value loss: 0.046461. Entropy: 0.306782.\n",
      "episode: 1025   score: 140.0  epsilon: 1.0    steps: 24  evaluation reward: 180.1\n",
      "episode: 1026   score: 210.0  epsilon: 1.0    steps: 24  evaluation reward: 180.1\n",
      "episode: 1027   score: 200.0  epsilon: 1.0    steps: 472  evaluation reward: 179.5\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2497: Policy loss: -0.025950. Value loss: 0.098925. Entropy: 0.303940.\n",
      "Iteration 2498: Policy loss: -0.027681. Value loss: 0.051901. Entropy: 0.303477.\n",
      "Iteration 2499: Policy loss: -0.030186. Value loss: 0.040039. Entropy: 0.302706.\n",
      "episode: 1028   score: 180.0  epsilon: 1.0    steps: 152  evaluation reward: 180.2\n",
      "episode: 1029   score: 210.0  epsilon: 1.0    steps: 432  evaluation reward: 178.6\n",
      "Training network. lr: 0.000231. clip: 0.092480\n",
      "Iteration 2500: Policy loss: 0.242156. Value loss: 0.099966. Entropy: 0.306845.\n",
      "Iteration 2501: Policy loss: 0.242701. Value loss: 0.049042. Entropy: 0.306565.\n",
      "Iteration 2502: Policy loss: 0.236222. Value loss: 0.033150. Entropy: 0.306998.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2503: Policy loss: 0.181095. Value loss: 0.084618. Entropy: 0.305271.\n",
      "Iteration 2504: Policy loss: 0.178419. Value loss: 0.038994. Entropy: 0.305380.\n",
      "Iteration 2505: Policy loss: 0.171415. Value loss: 0.027270. Entropy: 0.305663.\n",
      "episode: 1030   score: 120.0  epsilon: 1.0    steps: 768  evaluation reward: 177.5\n",
      "episode: 1031   score: 120.0  epsilon: 1.0    steps: 1024  evaluation reward: 176.9\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2506: Policy loss: 0.042369. Value loss: 0.070297. Entropy: 0.305880.\n",
      "Iteration 2507: Policy loss: 0.027689. Value loss: 0.030651. Entropy: 0.307278.\n",
      "Iteration 2508: Policy loss: 0.035548. Value loss: 0.023238. Entropy: 0.306371.\n",
      "episode: 1032   score: 120.0  epsilon: 1.0    steps: 80  evaluation reward: 175.7\n",
      "episode: 1033   score: 130.0  epsilon: 1.0    steps: 288  evaluation reward: 175.65\n",
      "episode: 1034   score: 50.0  epsilon: 1.0    steps: 360  evaluation reward: 174.8\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2509: Policy loss: 0.080704. Value loss: 0.055606. Entropy: 0.311819.\n",
      "Iteration 2510: Policy loss: 0.080601. Value loss: 0.021924. Entropy: 0.312859.\n",
      "Iteration 2511: Policy loss: 0.076618. Value loss: 0.016101. Entropy: 0.313334.\n",
      "episode: 1035   score: 170.0  epsilon: 1.0    steps: 144  evaluation reward: 175.45\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2512: Policy loss: -0.233144. Value loss: 0.130581. Entropy: 0.304274.\n",
      "Iteration 2513: Policy loss: -0.233401. Value loss: 0.058762. Entropy: 0.304688.\n",
      "Iteration 2514: Policy loss: -0.237819. Value loss: 0.037705. Entropy: 0.304395.\n",
      "episode: 1036   score: 195.0  epsilon: 1.0    steps: 320  evaluation reward: 175.6\n",
      "episode: 1037   score: 75.0  epsilon: 1.0    steps: 760  evaluation reward: 174.8\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2515: Policy loss: -0.011084. Value loss: 0.110816. Entropy: 0.312030.\n",
      "Iteration 2516: Policy loss: -0.010175. Value loss: 0.060108. Entropy: 0.309635.\n",
      "Iteration 2517: Policy loss: -0.024278. Value loss: 0.044578. Entropy: 0.310016.\n",
      "episode: 1038   score: 210.0  epsilon: 1.0    steps: 512  evaluation reward: 174.35\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2518: Policy loss: -0.022308. Value loss: 0.110266. Entropy: 0.312349.\n",
      "Iteration 2519: Policy loss: -0.035127. Value loss: 0.044916. Entropy: 0.310386.\n",
      "Iteration 2520: Policy loss: -0.034768. Value loss: 0.031325. Entropy: 0.311062.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2521: Policy loss: -0.115582. Value loss: 0.103251. Entropy: 0.315630.\n",
      "Iteration 2522: Policy loss: -0.121581. Value loss: 0.053864. Entropy: 0.314727.\n",
      "Iteration 2523: Policy loss: -0.130853. Value loss: 0.040825. Entropy: 0.315718.\n",
      "episode: 1039   score: 160.0  epsilon: 1.0    steps: 312  evaluation reward: 175.45\n",
      "episode: 1040   score: 210.0  epsilon: 1.0    steps: 984  evaluation reward: 175.75\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2524: Policy loss: -0.026392. Value loss: 0.081573. Entropy: 0.306817.\n",
      "Iteration 2525: Policy loss: -0.028782. Value loss: 0.035290. Entropy: 0.306292.\n",
      "Iteration 2526: Policy loss: -0.034418. Value loss: 0.022764. Entropy: 0.307811.\n",
      "episode: 1041   score: 135.0  epsilon: 1.0    steps: 608  evaluation reward: 175.85\n",
      "episode: 1042   score: 80.0  epsilon: 1.0    steps: 744  evaluation reward: 174.85\n",
      "episode: 1043   score: 225.0  epsilon: 1.0    steps: 824  evaluation reward: 172.45\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2527: Policy loss: 0.214193. Value loss: 0.062151. Entropy: 0.312369.\n",
      "Iteration 2528: Policy loss: 0.210814. Value loss: 0.027229. Entropy: 0.311904.\n",
      "Iteration 2529: Policy loss: 0.203485. Value loss: 0.021675. Entropy: 0.311576.\n",
      "episode: 1044   score: 225.0  epsilon: 1.0    steps: 760  evaluation reward: 172.9\n",
      "episode: 1045   score: 120.0  epsilon: 1.0    steps: 840  evaluation reward: 172.0\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2530: Policy loss: -0.020517. Value loss: 0.100739. Entropy: 0.308416.\n",
      "Iteration 2531: Policy loss: -0.028094. Value loss: 0.038306. Entropy: 0.307550.\n",
      "Iteration 2532: Policy loss: -0.033196. Value loss: 0.025622. Entropy: 0.308190.\n",
      "episode: 1046   score: 45.0  epsilon: 1.0    steps: 320  evaluation reward: 170.35\n",
      "episode: 1047   score: 345.0  epsilon: 1.0    steps: 400  evaluation reward: 172.25\n",
      "episode: 1048   score: 80.0  epsilon: 1.0    steps: 896  evaluation reward: 171.5\n",
      "Training network. lr: 0.000231. clip: 0.092323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2533: Policy loss: -0.131828. Value loss: 0.103553. Entropy: 0.306850.\n",
      "Iteration 2534: Policy loss: -0.140427. Value loss: 0.051093. Entropy: 0.308858.\n",
      "Iteration 2535: Policy loss: -0.146217. Value loss: 0.038257. Entropy: 0.309375.\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2536: Policy loss: -0.148410. Value loss: 0.083132. Entropy: 0.314422.\n",
      "Iteration 2537: Policy loss: -0.151852. Value loss: 0.036000. Entropy: 0.314413.\n",
      "Iteration 2538: Policy loss: -0.158816. Value loss: 0.030305. Entropy: 0.314861.\n",
      "episode: 1049   score: 105.0  epsilon: 1.0    steps: 24  evaluation reward: 171.45\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2539: Policy loss: 0.036329. Value loss: 0.073877. Entropy: 0.312102.\n",
      "Iteration 2540: Policy loss: 0.026498. Value loss: 0.036181. Entropy: 0.312134.\n",
      "Iteration 2541: Policy loss: 0.022827. Value loss: 0.027163. Entropy: 0.312477.\n",
      "episode: 1050   score: 120.0  epsilon: 1.0    steps: 344  evaluation reward: 170.55\n",
      "now time :  2019-09-05 16:53:03.884130\n",
      "episode: 1051   score: 50.0  epsilon: 1.0    steps: 816  evaluation reward: 168.95\n",
      "episode: 1052   score: 135.0  epsilon: 1.0    steps: 856  evaluation reward: 166.85\n",
      "episode: 1053   score: 140.0  epsilon: 1.0    steps: 976  evaluation reward: 163.4\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2542: Policy loss: -0.080440. Value loss: 0.140627. Entropy: 0.312274.\n",
      "Iteration 2543: Policy loss: -0.083058. Value loss: 0.070629. Entropy: 0.312650.\n",
      "Iteration 2544: Policy loss: -0.097777. Value loss: 0.046391. Entropy: 0.311344.\n",
      "episode: 1054   score: 120.0  epsilon: 1.0    steps: 472  evaluation reward: 163.55\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2545: Policy loss: -0.241093. Value loss: 0.096317. Entropy: 0.306984.\n",
      "Iteration 2546: Policy loss: -0.245218. Value loss: 0.050112. Entropy: 0.305564.\n",
      "Iteration 2547: Policy loss: -0.248831. Value loss: 0.037525. Entropy: 0.305743.\n",
      "episode: 1055   score: 210.0  epsilon: 1.0    steps: 512  evaluation reward: 164.3\n",
      "Training network. lr: 0.000231. clip: 0.092323\n",
      "Iteration 2548: Policy loss: -0.073670. Value loss: 0.119380. Entropy: 0.312987.\n",
      "Iteration 2549: Policy loss: -0.075611. Value loss: 0.051528. Entropy: 0.313061.\n",
      "Iteration 2550: Policy loss: -0.071898. Value loss: 0.039227. Entropy: 0.313198.\n",
      "episode: 1056   score: 280.0  epsilon: 1.0    steps: 344  evaluation reward: 163.0\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2551: Policy loss: 0.157168. Value loss: 0.142486. Entropy: 0.306316.\n",
      "Iteration 2552: Policy loss: 0.141924. Value loss: 0.053478. Entropy: 0.306327.\n",
      "Iteration 2553: Policy loss: 0.139769. Value loss: 0.035468. Entropy: 0.306539.\n",
      "episode: 1057   score: 70.0  epsilon: 1.0    steps: 352  evaluation reward: 161.6\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2554: Policy loss: 0.036021. Value loss: 0.160887. Entropy: 0.310345.\n",
      "Iteration 2555: Policy loss: 0.031757. Value loss: 0.068554. Entropy: 0.312103.\n",
      "Iteration 2556: Policy loss: 0.032611. Value loss: 0.047589. Entropy: 0.311555.\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2557: Policy loss: -0.042737. Value loss: 0.096674. Entropy: 0.312635.\n",
      "Iteration 2558: Policy loss: -0.041271. Value loss: 0.040551. Entropy: 0.313755.\n",
      "Iteration 2559: Policy loss: -0.051157. Value loss: 0.029793. Entropy: 0.313023.\n",
      "episode: 1058   score: 440.0  epsilon: 1.0    steps: 608  evaluation reward: 163.15\n",
      "episode: 1059   score: 205.0  epsilon: 1.0    steps: 744  evaluation reward: 163.1\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2560: Policy loss: 0.102108. Value loss: 0.099372. Entropy: 0.315957.\n",
      "Iteration 2561: Policy loss: 0.096813. Value loss: 0.044330. Entropy: 0.314934.\n",
      "Iteration 2562: Policy loss: 0.089607. Value loss: 0.033955. Entropy: 0.314501.\n",
      "episode: 1060   score: 220.0  epsilon: 1.0    steps: 424  evaluation reward: 163.7\n",
      "episode: 1061   score: 110.0  epsilon: 1.0    steps: 736  evaluation reward: 162.7\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2563: Policy loss: 0.085554. Value loss: 0.090144. Entropy: 0.308986.\n",
      "Iteration 2564: Policy loss: 0.077566. Value loss: 0.040732. Entropy: 0.308586.\n",
      "Iteration 2565: Policy loss: 0.078076. Value loss: 0.032199. Entropy: 0.308687.\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2566: Policy loss: -0.099023. Value loss: 0.186401. Entropy: 0.310447.\n",
      "Iteration 2567: Policy loss: -0.104679. Value loss: 0.092773. Entropy: 0.311041.\n",
      "Iteration 2568: Policy loss: -0.110002. Value loss: 0.064880. Entropy: 0.311284.\n",
      "episode: 1062   score: 320.0  epsilon: 1.0    steps: 128  evaluation reward: 164.85\n",
      "episode: 1063   score: 305.0  epsilon: 1.0    steps: 688  evaluation reward: 166.8\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2569: Policy loss: -0.293899. Value loss: 0.408132. Entropy: 0.313058.\n",
      "Iteration 2570: Policy loss: -0.295356. Value loss: 0.268760. Entropy: 0.312708.\n",
      "Iteration 2571: Policy loss: -0.296197. Value loss: 0.197605. Entropy: 0.311881.\n",
      "episode: 1064   score: 125.0  epsilon: 1.0    steps: 704  evaluation reward: 167.0\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2572: Policy loss: 0.293748. Value loss: 0.067885. Entropy: 0.311539.\n",
      "Iteration 2573: Policy loss: 0.288108. Value loss: 0.021111. Entropy: 0.311088.\n",
      "Iteration 2574: Policy loss: 0.287401. Value loss: 0.017006. Entropy: 0.311159.\n",
      "episode: 1065   score: 350.0  epsilon: 1.0    steps: 704  evaluation reward: 168.4\n",
      "episode: 1066   score: 120.0  epsilon: 1.0    steps: 920  evaluation reward: 167.5\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2575: Policy loss: 0.400588. Value loss: 0.100537. Entropy: 0.309564.\n",
      "Iteration 2576: Policy loss: 0.395481. Value loss: 0.040975. Entropy: 0.309133.\n",
      "Iteration 2577: Policy loss: 0.396324. Value loss: 0.032114. Entropy: 0.309005.\n",
      "episode: 1067   score: 400.0  epsilon: 1.0    steps: 216  evaluation reward: 170.3\n",
      "episode: 1068   score: 75.0  epsilon: 1.0    steps: 352  evaluation reward: 170.3\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2578: Policy loss: 0.075298. Value loss: 0.066199. Entropy: 0.308473.\n",
      "Iteration 2579: Policy loss: 0.076262. Value loss: 0.037975. Entropy: 0.308256.\n",
      "Iteration 2580: Policy loss: 0.074446. Value loss: 0.031657. Entropy: 0.307948.\n",
      "episode: 1069   score: 155.0  epsilon: 1.0    steps: 88  evaluation reward: 170.3\n",
      "episode: 1070   score: 220.0  epsilon: 1.0    steps: 264  evaluation reward: 171.45\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2581: Policy loss: -0.007232. Value loss: 0.061123. Entropy: 0.311974.\n",
      "Iteration 2582: Policy loss: -0.012333. Value loss: 0.029364. Entropy: 0.311127.\n",
      "Iteration 2583: Policy loss: -0.012703. Value loss: 0.022309. Entropy: 0.311192.\n",
      "episode: 1071   score: 105.0  epsilon: 1.0    steps: 688  evaluation reward: 171.15\n",
      "episode: 1072   score: 155.0  epsilon: 1.0    steps: 880  evaluation reward: 171.15\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2584: Policy loss: 0.162144. Value loss: 0.091719. Entropy: 0.311351.\n",
      "Iteration 2585: Policy loss: 0.152489. Value loss: 0.042055. Entropy: 0.309798.\n",
      "Iteration 2586: Policy loss: 0.157433. Value loss: 0.031809. Entropy: 0.309842.\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2587: Policy loss: -0.175316. Value loss: 0.140198. Entropy: 0.307364.\n",
      "Iteration 2588: Policy loss: -0.185442. Value loss: 0.073323. Entropy: 0.308311.\n",
      "Iteration 2589: Policy loss: -0.195763. Value loss: 0.058835. Entropy: 0.307590.\n",
      "episode: 1073   score: 150.0  epsilon: 1.0    steps: 400  evaluation reward: 171.6\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2590: Policy loss: 0.089626. Value loss: 0.075663. Entropy: 0.308941.\n",
      "Iteration 2591: Policy loss: 0.083889. Value loss: 0.037508. Entropy: 0.307711.\n",
      "Iteration 2592: Policy loss: 0.081072. Value loss: 0.026637. Entropy: 0.307486.\n",
      "episode: 1074   score: 135.0  epsilon: 1.0    steps: 152  evaluation reward: 168.8\n",
      "episode: 1075   score: 150.0  epsilon: 1.0    steps: 912  evaluation reward: 167.35\n",
      "episode: 1076   score: 90.0  epsilon: 1.0    steps: 976  evaluation reward: 167.5\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2593: Policy loss: 0.088975. Value loss: 0.061273. Entropy: 0.306826.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2594: Policy loss: 0.088154. Value loss: 0.030526. Entropy: 0.306330.\n",
      "Iteration 2595: Policy loss: 0.078389. Value loss: 0.024217. Entropy: 0.306780.\n",
      "episode: 1077   score: 155.0  epsilon: 1.0    steps: 480  evaluation reward: 166.3\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2596: Policy loss: -0.133946. Value loss: 0.059201. Entropy: 0.303826.\n",
      "Iteration 2597: Policy loss: -0.137811. Value loss: 0.032445. Entropy: 0.302666.\n",
      "Iteration 2598: Policy loss: -0.138765. Value loss: 0.026333. Entropy: 0.303166.\n",
      "episode: 1078   score: 135.0  epsilon: 1.0    steps: 848  evaluation reward: 166.45\n",
      "Training network. lr: 0.000230. clip: 0.092176\n",
      "Iteration 2599: Policy loss: 0.209522. Value loss: 0.057969. Entropy: 0.311442.\n",
      "Iteration 2600: Policy loss: 0.203806. Value loss: 0.020787. Entropy: 0.311484.\n",
      "Iteration 2601: Policy loss: 0.200156. Value loss: 0.012073. Entropy: 0.310445.\n",
      "episode: 1079   score: 265.0  epsilon: 1.0    steps: 56  evaluation reward: 167.75\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2602: Policy loss: -0.261424. Value loss: 0.313683. Entropy: 0.305171.\n",
      "Iteration 2603: Policy loss: -0.286599. Value loss: 0.205484. Entropy: 0.304555.\n",
      "Iteration 2604: Policy loss: -0.287987. Value loss: 0.116243. Entropy: 0.304305.\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2605: Policy loss: -0.048643. Value loss: 0.092742. Entropy: 0.310114.\n",
      "Iteration 2606: Policy loss: -0.046114. Value loss: 0.043993. Entropy: 0.309342.\n",
      "Iteration 2607: Policy loss: -0.051255. Value loss: 0.032494. Entropy: 0.309879.\n",
      "episode: 1080   score: 210.0  epsilon: 1.0    steps: 688  evaluation reward: 167.15\n",
      "episode: 1081   score: 245.0  epsilon: 1.0    steps: 904  evaluation reward: 168.2\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2608: Policy loss: 0.108830. Value loss: 0.118832. Entropy: 0.306536.\n",
      "Iteration 2609: Policy loss: 0.098974. Value loss: 0.048174. Entropy: 0.304940.\n",
      "Iteration 2610: Policy loss: 0.100465. Value loss: 0.033410. Entropy: 0.305258.\n",
      "episode: 1082   score: 150.0  epsilon: 1.0    steps: 64  evaluation reward: 167.15\n",
      "episode: 1083   score: 95.0  epsilon: 1.0    steps: 104  evaluation reward: 165.2\n",
      "episode: 1084   score: 410.0  epsilon: 1.0    steps: 448  evaluation reward: 167.2\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2611: Policy loss: -0.018012. Value loss: 0.057505. Entropy: 0.304421.\n",
      "Iteration 2612: Policy loss: -0.022807. Value loss: 0.032616. Entropy: 0.304781.\n",
      "Iteration 2613: Policy loss: -0.025139. Value loss: 0.024824. Entropy: 0.304709.\n",
      "episode: 1085   score: 215.0  epsilon: 1.0    steps: 256  evaluation reward: 168.3\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2614: Policy loss: 0.189898. Value loss: 0.119414. Entropy: 0.309458.\n",
      "Iteration 2615: Policy loss: 0.189789. Value loss: 0.071140. Entropy: 0.308706.\n",
      "Iteration 2616: Policy loss: 0.182223. Value loss: 0.054285. Entropy: 0.308605.\n",
      "episode: 1086   score: 210.0  epsilon: 1.0    steps: 160  evaluation reward: 169.55\n",
      "episode: 1087   score: 70.0  epsilon: 1.0    steps: 960  evaluation reward: 168.0\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2617: Policy loss: 0.142010. Value loss: 0.101264. Entropy: 0.306075.\n",
      "Iteration 2618: Policy loss: 0.133480. Value loss: 0.047480. Entropy: 0.306928.\n",
      "Iteration 2619: Policy loss: 0.136041. Value loss: 0.036200. Entropy: 0.306953.\n",
      "episode: 1088   score: 240.0  epsilon: 1.0    steps: 480  evaluation reward: 168.95\n",
      "episode: 1089   score: 120.0  epsilon: 1.0    steps: 816  evaluation reward: 168.6\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2620: Policy loss: -0.030936. Value loss: 0.077193. Entropy: 0.303782.\n",
      "Iteration 2621: Policy loss: -0.036093. Value loss: 0.036778. Entropy: 0.302871.\n",
      "Iteration 2622: Policy loss: -0.037775. Value loss: 0.028110. Entropy: 0.303785.\n",
      "episode: 1090   score: 50.0  epsilon: 1.0    steps: 912  evaluation reward: 166.95\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2623: Policy loss: 0.017553. Value loss: 0.117528. Entropy: 0.306652.\n",
      "Iteration 2624: Policy loss: 0.004168. Value loss: 0.058323. Entropy: 0.306840.\n",
      "Iteration 2625: Policy loss: -0.001556. Value loss: 0.040797. Entropy: 0.306051.\n",
      "episode: 1091   score: 135.0  epsilon: 1.0    steps: 48  evaluation reward: 166.2\n",
      "episode: 1092   score: 140.0  epsilon: 1.0    steps: 288  evaluation reward: 166.8\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2626: Policy loss: 0.098513. Value loss: 0.061292. Entropy: 0.304417.\n",
      "Iteration 2627: Policy loss: 0.097607. Value loss: 0.025816. Entropy: 0.305550.\n",
      "Iteration 2628: Policy loss: 0.090617. Value loss: 0.018775. Entropy: 0.305261.\n",
      "episode: 1093   score: 260.0  epsilon: 1.0    steps: 896  evaluation reward: 168.1\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2629: Policy loss: -0.010152. Value loss: 0.089793. Entropy: 0.304693.\n",
      "Iteration 2630: Policy loss: -0.021021. Value loss: 0.041881. Entropy: 0.305500.\n",
      "Iteration 2631: Policy loss: -0.020755. Value loss: 0.032888. Entropy: 0.303416.\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2632: Policy loss: 0.001602. Value loss: 0.102257. Entropy: 0.306184.\n",
      "Iteration 2633: Policy loss: 0.005593. Value loss: 0.047284. Entropy: 0.304627.\n",
      "Iteration 2634: Policy loss: -0.006033. Value loss: 0.036392. Entropy: 0.305145.\n",
      "episode: 1094   score: 210.0  epsilon: 1.0    steps: 568  evaluation reward: 169.1\n",
      "episode: 1095   score: 155.0  epsilon: 1.0    steps: 1000  evaluation reward: 168.55\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2635: Policy loss: 0.185894. Value loss: 0.085832. Entropy: 0.304284.\n",
      "Iteration 2636: Policy loss: 0.174703. Value loss: 0.041209. Entropy: 0.303328.\n",
      "Iteration 2637: Policy loss: 0.179864. Value loss: 0.029678. Entropy: 0.304132.\n",
      "episode: 1096   score: 80.0  epsilon: 1.0    steps: 328  evaluation reward: 168.15\n",
      "episode: 1097   score: 130.0  epsilon: 1.0    steps: 336  evaluation reward: 167.5\n",
      "episode: 1098   score: 240.0  epsilon: 1.0    steps: 800  evaluation reward: 167.5\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2638: Policy loss: 0.034779. Value loss: 0.063215. Entropy: 0.306315.\n",
      "Iteration 2639: Policy loss: 0.033695. Value loss: 0.023801. Entropy: 0.306774.\n",
      "Iteration 2640: Policy loss: 0.036161. Value loss: 0.017420. Entropy: 0.306509.\n",
      "episode: 1099   score: 105.0  epsilon: 1.0    steps: 456  evaluation reward: 167.0\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2641: Policy loss: 0.125649. Value loss: 0.061820. Entropy: 0.305343.\n",
      "Iteration 2642: Policy loss: 0.123509. Value loss: 0.024155. Entropy: 0.305147.\n",
      "Iteration 2643: Policy loss: 0.120392. Value loss: 0.017316. Entropy: 0.304852.\n",
      "episode: 1100   score: 260.0  epsilon: 1.0    steps: 128  evaluation reward: 169.1\n",
      "now time :  2019-09-05 16:59:22.686920\n",
      "episode: 1101   score: 110.0  epsilon: 1.0    steps: 208  evaluation reward: 168.4\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2644: Policy loss: 0.110706. Value loss: 0.047101. Entropy: 0.308325.\n",
      "Iteration 2645: Policy loss: 0.104458. Value loss: 0.031171. Entropy: 0.307292.\n",
      "Iteration 2646: Policy loss: 0.105476. Value loss: 0.026424. Entropy: 0.306840.\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2647: Policy loss: 0.042354. Value loss: 0.098803. Entropy: 0.313222.\n",
      "Iteration 2648: Policy loss: 0.034672. Value loss: 0.046937. Entropy: 0.312757.\n",
      "Iteration 2649: Policy loss: 0.035939. Value loss: 0.031333. Entropy: 0.312955.\n",
      "episode: 1102   score: 135.0  epsilon: 1.0    steps: 600  evaluation reward: 167.9\n",
      "episode: 1103   score: 120.0  epsilon: 1.0    steps: 728  evaluation reward: 167.75\n",
      "Training network. lr: 0.000230. clip: 0.092019\n",
      "Iteration 2650: Policy loss: 0.155008. Value loss: 0.016809. Entropy: 0.304408.\n",
      "Iteration 2651: Policy loss: 0.152851. Value loss: 0.008878. Entropy: 0.302679.\n",
      "Iteration 2652: Policy loss: 0.151806. Value loss: 0.006879. Entropy: 0.302192.\n",
      "episode: 1104   score: 105.0  epsilon: 1.0    steps: 312  evaluation reward: 167.75\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2653: Policy loss: -0.156742. Value loss: 0.128959. Entropy: 0.308637.\n",
      "Iteration 2654: Policy loss: -0.173601. Value loss: 0.096064. Entropy: 0.308736.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2655: Policy loss: -0.167466. Value loss: 0.088160. Entropy: 0.308398.\n",
      "episode: 1105   score: 170.0  epsilon: 1.0    steps: 656  evaluation reward: 164.1\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2656: Policy loss: 0.131384. Value loss: 0.116947. Entropy: 0.309920.\n",
      "Iteration 2657: Policy loss: 0.123270. Value loss: 0.046998. Entropy: 0.309435.\n",
      "Iteration 2658: Policy loss: 0.124173. Value loss: 0.036353. Entropy: 0.309580.\n",
      "episode: 1106   score: 280.0  epsilon: 1.0    steps: 136  evaluation reward: 165.55\n",
      "episode: 1107   score: 155.0  epsilon: 1.0    steps: 392  evaluation reward: 165.9\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2659: Policy loss: -0.068289. Value loss: 0.070705. Entropy: 0.308897.\n",
      "Iteration 2660: Policy loss: -0.070755. Value loss: 0.032301. Entropy: 0.308335.\n",
      "Iteration 2661: Policy loss: -0.072582. Value loss: 0.022950. Entropy: 0.309506.\n",
      "episode: 1108   score: 210.0  epsilon: 1.0    steps: 40  evaluation reward: 166.6\n",
      "episode: 1109   score: 410.0  epsilon: 1.0    steps: 504  evaluation reward: 168.3\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2662: Policy loss: 0.027191. Value loss: 0.114969. Entropy: 0.312231.\n",
      "Iteration 2663: Policy loss: 0.014026. Value loss: 0.051989. Entropy: 0.312301.\n",
      "Iteration 2664: Policy loss: 0.015315. Value loss: 0.037624. Entropy: 0.312419.\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2665: Policy loss: 0.232942. Value loss: 0.059058. Entropy: 0.310871.\n",
      "Iteration 2666: Policy loss: 0.228437. Value loss: 0.027109. Entropy: 0.309561.\n",
      "Iteration 2667: Policy loss: 0.225702. Value loss: 0.021346. Entropy: 0.308311.\n",
      "episode: 1110   score: 160.0  epsilon: 1.0    steps: 80  evaluation reward: 168.35\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2668: Policy loss: 0.020268. Value loss: 0.066236. Entropy: 0.305433.\n",
      "Iteration 2669: Policy loss: 0.013493. Value loss: 0.031218. Entropy: 0.305710.\n",
      "Iteration 2670: Policy loss: 0.005088. Value loss: 0.024278. Entropy: 0.304893.\n",
      "episode: 1111   score: 135.0  epsilon: 1.0    steps: 112  evaluation reward: 168.95\n",
      "episode: 1112   score: 210.0  epsilon: 1.0    steps: 680  evaluation reward: 169.95\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2671: Policy loss: -0.154608. Value loss: 0.308572. Entropy: 0.311097.\n",
      "Iteration 2672: Policy loss: -0.195777. Value loss: 0.207290. Entropy: 0.311077.\n",
      "Iteration 2673: Policy loss: -0.196724. Value loss: 0.121986. Entropy: 0.310431.\n",
      "episode: 1113   score: 155.0  epsilon: 1.0    steps: 424  evaluation reward: 169.4\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2674: Policy loss: -0.052077. Value loss: 0.104514. Entropy: 0.307204.\n",
      "Iteration 2675: Policy loss: -0.059052. Value loss: 0.048755. Entropy: 0.306652.\n",
      "Iteration 2676: Policy loss: -0.051732. Value loss: 0.037402. Entropy: 0.306759.\n",
      "episode: 1114   score: 145.0  epsilon: 1.0    steps: 80  evaluation reward: 169.5\n",
      "episode: 1115   score: 380.0  epsilon: 1.0    steps: 200  evaluation reward: 171.15\n",
      "episode: 1116   score: 110.0  epsilon: 1.0    steps: 408  evaluation reward: 171.9\n",
      "episode: 1117   score: 75.0  epsilon: 1.0    steps: 544  evaluation reward: 171.85\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2677: Policy loss: 0.195652. Value loss: 0.043478. Entropy: 0.306265.\n",
      "Iteration 2678: Policy loss: 0.186150. Value loss: 0.019106. Entropy: 0.305235.\n",
      "Iteration 2679: Policy loss: 0.183481. Value loss: 0.014997. Entropy: 0.304659.\n",
      "episode: 1118   score: 340.0  epsilon: 1.0    steps: 768  evaluation reward: 174.75\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2680: Policy loss: 0.003765. Value loss: 0.129175. Entropy: 0.309046.\n",
      "Iteration 2681: Policy loss: 0.004021. Value loss: 0.056002. Entropy: 0.307783.\n",
      "Iteration 2682: Policy loss: -0.001075. Value loss: 0.037256. Entropy: 0.307836.\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2683: Policy loss: -0.004508. Value loss: 0.093280. Entropy: 0.304630.\n",
      "Iteration 2684: Policy loss: -0.014743. Value loss: 0.043522. Entropy: 0.303964.\n",
      "Iteration 2685: Policy loss: -0.021268. Value loss: 0.031078. Entropy: 0.305742.\n",
      "episode: 1119   score: 100.0  epsilon: 1.0    steps: 240  evaluation reward: 174.95\n",
      "episode: 1120   score: 180.0  epsilon: 1.0    steps: 392  evaluation reward: 172.95\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2686: Policy loss: 0.020995. Value loss: 0.067249. Entropy: 0.309292.\n",
      "Iteration 2687: Policy loss: 0.014982. Value loss: 0.030155. Entropy: 0.308675.\n",
      "Iteration 2688: Policy loss: 0.014070. Value loss: 0.023068. Entropy: 0.308475.\n",
      "episode: 1121   score: 110.0  epsilon: 1.0    steps: 472  evaluation reward: 171.95\n",
      "episode: 1122   score: 210.0  epsilon: 1.0    steps: 928  evaluation reward: 172.85\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2689: Policy loss: -0.069348. Value loss: 0.088168. Entropy: 0.309300.\n",
      "Iteration 2690: Policy loss: -0.081178. Value loss: 0.052937. Entropy: 0.309013.\n",
      "Iteration 2691: Policy loss: -0.082002. Value loss: 0.041276. Entropy: 0.308843.\n",
      "episode: 1123   score: 110.0  epsilon: 1.0    steps: 368  evaluation reward: 173.35\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2692: Policy loss: 0.078472. Value loss: 0.094902. Entropy: 0.309162.\n",
      "Iteration 2693: Policy loss: 0.071929. Value loss: 0.043178. Entropy: 0.308718.\n",
      "Iteration 2694: Policy loss: 0.071068. Value loss: 0.030684. Entropy: 0.308769.\n",
      "episode: 1124   score: 120.0  epsilon: 1.0    steps: 152  evaluation reward: 173.35\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2695: Policy loss: 0.174382. Value loss: 0.069934. Entropy: 0.306389.\n",
      "Iteration 2696: Policy loss: 0.158123. Value loss: 0.030855. Entropy: 0.305203.\n",
      "Iteration 2697: Policy loss: 0.154934. Value loss: 0.022382. Entropy: 0.305549.\n",
      "episode: 1125   score: 215.0  epsilon: 1.0    steps: 16  evaluation reward: 174.1\n",
      "episode: 1126   score: 140.0  epsilon: 1.0    steps: 560  evaluation reward: 173.4\n",
      "episode: 1127   score: 105.0  epsilon: 1.0    steps: 864  evaluation reward: 172.45\n",
      "Training network. lr: 0.000230. clip: 0.091862\n",
      "Iteration 2698: Policy loss: 0.050477. Value loss: 0.078358. Entropy: 0.309694.\n",
      "Iteration 2699: Policy loss: 0.049259. Value loss: 0.041099. Entropy: 0.309595.\n",
      "Iteration 2700: Policy loss: 0.051010. Value loss: 0.033829. Entropy: 0.309307.\n",
      "episode: 1128   score: 135.0  epsilon: 1.0    steps: 168  evaluation reward: 172.0\n",
      "episode: 1129   score: 240.0  epsilon: 1.0    steps: 200  evaluation reward: 172.3\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2701: Policy loss: -0.050370. Value loss: 0.104531. Entropy: 0.308424.\n",
      "Iteration 2702: Policy loss: -0.052017. Value loss: 0.053266. Entropy: 0.307441.\n",
      "Iteration 2703: Policy loss: -0.057081. Value loss: 0.042761. Entropy: 0.307907.\n",
      "episode: 1130   score: 120.0  epsilon: 1.0    steps: 168  evaluation reward: 172.3\n",
      "episode: 1131   score: 210.0  epsilon: 1.0    steps: 704  evaluation reward: 173.2\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2704: Policy loss: 0.289309. Value loss: 0.134368. Entropy: 0.307129.\n",
      "Iteration 2705: Policy loss: 0.280183. Value loss: 0.064745. Entropy: 0.305898.\n",
      "Iteration 2706: Policy loss: 0.274152. Value loss: 0.045340. Entropy: 0.306444.\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2707: Policy loss: 0.153389. Value loss: 0.117605. Entropy: 0.305490.\n",
      "Iteration 2708: Policy loss: 0.146766. Value loss: 0.060690. Entropy: 0.305501.\n",
      "Iteration 2709: Policy loss: 0.146098. Value loss: 0.042244. Entropy: 0.305335.\n",
      "episode: 1132   score: 135.0  epsilon: 1.0    steps: 600  evaluation reward: 173.35\n",
      "episode: 1133   score: 155.0  epsilon: 1.0    steps: 688  evaluation reward: 173.6\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2710: Policy loss: -0.083131. Value loss: 0.169099. Entropy: 0.305747.\n",
      "Iteration 2711: Policy loss: -0.082793. Value loss: 0.082280. Entropy: 0.304842.\n",
      "Iteration 2712: Policy loss: -0.091949. Value loss: 0.061994. Entropy: 0.305319.\n",
      "episode: 1134   score: 15.0  epsilon: 1.0    steps: 384  evaluation reward: 173.25\n",
      "episode: 1135   score: 220.0  epsilon: 1.0    steps: 408  evaluation reward: 173.75\n",
      "episode: 1136   score: 270.0  epsilon: 1.0    steps: 504  evaluation reward: 174.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1137   score: 180.0  epsilon: 1.0    steps: 584  evaluation reward: 175.55\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2713: Policy loss: 0.150807. Value loss: 0.092564. Entropy: 0.304272.\n",
      "Iteration 2714: Policy loss: 0.144419. Value loss: 0.040186. Entropy: 0.303797.\n",
      "Iteration 2715: Policy loss: 0.137442. Value loss: 0.026629. Entropy: 0.303686.\n",
      "episode: 1138   score: 215.0  epsilon: 1.0    steps: 408  evaluation reward: 175.6\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2716: Policy loss: -0.087580. Value loss: 0.125708. Entropy: 0.306560.\n",
      "Iteration 2717: Policy loss: -0.095231. Value loss: 0.063423. Entropy: 0.307571.\n",
      "Iteration 2718: Policy loss: -0.094035. Value loss: 0.048372. Entropy: 0.307309.\n",
      "episode: 1139   score: 180.0  epsilon: 1.0    steps: 584  evaluation reward: 175.8\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2719: Policy loss: -0.079490. Value loss: 0.116821. Entropy: 0.307413.\n",
      "Iteration 2720: Policy loss: -0.081809. Value loss: 0.057766. Entropy: 0.306872.\n",
      "Iteration 2721: Policy loss: -0.088438. Value loss: 0.043826. Entropy: 0.307211.\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2722: Policy loss: -0.106359. Value loss: 0.137639. Entropy: 0.303410.\n",
      "Iteration 2723: Policy loss: -0.109081. Value loss: 0.051526. Entropy: 0.303692.\n",
      "Iteration 2724: Policy loss: -0.104392. Value loss: 0.038284. Entropy: 0.303451.\n",
      "episode: 1140   score: 130.0  epsilon: 1.0    steps: 456  evaluation reward: 175.0\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2725: Policy loss: 0.107978. Value loss: 0.077040. Entropy: 0.306728.\n",
      "Iteration 2726: Policy loss: 0.112330. Value loss: 0.030311. Entropy: 0.305637.\n",
      "Iteration 2727: Policy loss: 0.105802. Value loss: 0.023013. Entropy: 0.305294.\n",
      "episode: 1141   score: 150.0  epsilon: 1.0    steps: 152  evaluation reward: 175.15\n",
      "episode: 1142   score: 180.0  epsilon: 1.0    steps: 688  evaluation reward: 176.15\n",
      "episode: 1143   score: 120.0  epsilon: 1.0    steps: 696  evaluation reward: 175.1\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2728: Policy loss: -0.280501. Value loss: 0.346876. Entropy: 0.303489.\n",
      "Iteration 2729: Policy loss: -0.280362. Value loss: 0.259225. Entropy: 0.301224.\n",
      "Iteration 2730: Policy loss: -0.275967. Value loss: 0.185538. Entropy: 0.303075.\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2731: Policy loss: 0.016786. Value loss: 0.124746. Entropy: 0.304077.\n",
      "Iteration 2732: Policy loss: 0.011188. Value loss: 0.045647. Entropy: 0.305274.\n",
      "Iteration 2733: Policy loss: 0.002655. Value loss: 0.032163. Entropy: 0.304705.\n",
      "episode: 1144   score: 160.0  epsilon: 1.0    steps: 48  evaluation reward: 174.45\n",
      "episode: 1145   score: 340.0  epsilon: 1.0    steps: 944  evaluation reward: 176.65\n",
      "episode: 1146   score: 135.0  epsilon: 1.0    steps: 984  evaluation reward: 177.55\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2734: Policy loss: -0.103077. Value loss: 0.137006. Entropy: 0.308670.\n",
      "Iteration 2735: Policy loss: -0.110766. Value loss: 0.056422. Entropy: 0.309141.\n",
      "Iteration 2736: Policy loss: -0.113589. Value loss: 0.040172. Entropy: 0.308327.\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2737: Policy loss: -0.049863. Value loss: 0.103177. Entropy: 0.305022.\n",
      "Iteration 2738: Policy loss: -0.055398. Value loss: 0.044674. Entropy: 0.303906.\n",
      "Iteration 2739: Policy loss: -0.054587. Value loss: 0.034073. Entropy: 0.303611.\n",
      "episode: 1147   score: 105.0  epsilon: 1.0    steps: 144  evaluation reward: 175.15\n",
      "episode: 1148   score: 460.0  epsilon: 1.0    steps: 688  evaluation reward: 178.95\n",
      "episode: 1149   score: 65.0  epsilon: 1.0    steps: 816  evaluation reward: 178.55\n",
      "episode: 1150   score: 180.0  epsilon: 1.0    steps: 992  evaluation reward: 179.15\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2740: Policy loss: 0.118501. Value loss: 0.116709. Entropy: 0.307558.\n",
      "Iteration 2741: Policy loss: 0.113595. Value loss: 0.048789. Entropy: 0.307016.\n",
      "Iteration 2742: Policy loss: 0.117243. Value loss: 0.034125. Entropy: 0.306801.\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2743: Policy loss: -0.073402. Value loss: 0.087067. Entropy: 0.305498.\n",
      "Iteration 2744: Policy loss: -0.084063. Value loss: 0.042582. Entropy: 0.306354.\n",
      "Iteration 2745: Policy loss: -0.090774. Value loss: 0.034013. Entropy: 0.305643.\n",
      "now time :  2019-09-05 17:05:43.536327\n",
      "episode: 1151   score: 210.0  epsilon: 1.0    steps: 472  evaluation reward: 180.75\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2746: Policy loss: -0.218054. Value loss: 0.154270. Entropy: 0.308777.\n",
      "Iteration 2747: Policy loss: -0.217410. Value loss: 0.051175. Entropy: 0.308500.\n",
      "Iteration 2748: Policy loss: -0.230906. Value loss: 0.036316. Entropy: 0.308097.\n",
      "episode: 1152   score: 235.0  epsilon: 1.0    steps: 504  evaluation reward: 181.75\n",
      "Training network. lr: 0.000229. clip: 0.091715\n",
      "Iteration 2749: Policy loss: 0.020871. Value loss: 0.155525. Entropy: 0.306685.\n",
      "Iteration 2750: Policy loss: 0.011904. Value loss: 0.073838. Entropy: 0.307498.\n",
      "Iteration 2751: Policy loss: 0.005412. Value loss: 0.050331. Entropy: 0.307663.\n",
      "episode: 1153   score: 50.0  epsilon: 1.0    steps: 72  evaluation reward: 180.85\n",
      "episode: 1154   score: 80.0  epsilon: 1.0    steps: 288  evaluation reward: 180.45\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2752: Policy loss: 0.054707. Value loss: 0.099838. Entropy: 0.306235.\n",
      "Iteration 2753: Policy loss: 0.049646. Value loss: 0.038381. Entropy: 0.305904.\n",
      "Iteration 2754: Policy loss: 0.042221. Value loss: 0.028255. Entropy: 0.305582.\n",
      "episode: 1155   score: 305.0  epsilon: 1.0    steps: 168  evaluation reward: 181.4\n",
      "episode: 1156   score: 215.0  epsilon: 1.0    steps: 576  evaluation reward: 180.75\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2755: Policy loss: -0.098272. Value loss: 0.083258. Entropy: 0.304311.\n",
      "Iteration 2756: Policy loss: -0.100064. Value loss: 0.040145. Entropy: 0.304519.\n",
      "Iteration 2757: Policy loss: -0.098777. Value loss: 0.028901. Entropy: 0.303646.\n",
      "episode: 1157   score: 55.0  epsilon: 1.0    steps: 480  evaluation reward: 180.6\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2758: Policy loss: 0.081374. Value loss: 0.082331. Entropy: 0.304739.\n",
      "Iteration 2759: Policy loss: 0.071249. Value loss: 0.037315. Entropy: 0.305522.\n",
      "Iteration 2760: Policy loss: 0.069307. Value loss: 0.026703. Entropy: 0.305501.\n",
      "episode: 1158   score: 295.0  epsilon: 1.0    steps: 336  evaluation reward: 179.15\n",
      "episode: 1159   score: 125.0  epsilon: 1.0    steps: 872  evaluation reward: 178.35\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2761: Policy loss: 0.016029. Value loss: 0.107060. Entropy: 0.302787.\n",
      "Iteration 2762: Policy loss: 0.012336. Value loss: 0.046411. Entropy: 0.302445.\n",
      "Iteration 2763: Policy loss: 0.008738. Value loss: 0.034112. Entropy: 0.303772.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2764: Policy loss: 0.129954. Value loss: 0.088544. Entropy: 0.307627.\n",
      "Iteration 2765: Policy loss: 0.122911. Value loss: 0.038077. Entropy: 0.306253.\n",
      "Iteration 2766: Policy loss: 0.114937. Value loss: 0.027356. Entropy: 0.306318.\n",
      "episode: 1160   score: 95.0  epsilon: 1.0    steps: 256  evaluation reward: 177.1\n",
      "episode: 1161   score: 180.0  epsilon: 1.0    steps: 560  evaluation reward: 177.8\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2767: Policy loss: 0.068219. Value loss: 0.086386. Entropy: 0.300990.\n",
      "Iteration 2768: Policy loss: 0.053757. Value loss: 0.044788. Entropy: 0.302496.\n",
      "Iteration 2769: Policy loss: 0.051200. Value loss: 0.034296. Entropy: 0.302164.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2770: Policy loss: -0.425865. Value loss: 0.343563. Entropy: 0.305517.\n",
      "Iteration 2771: Policy loss: -0.428698. Value loss: 0.197508. Entropy: 0.307653.\n",
      "Iteration 2772: Policy loss: -0.430645. Value loss: 0.148473. Entropy: 0.306712.\n",
      "episode: 1162   score: 210.0  epsilon: 1.0    steps: 840  evaluation reward: 176.7\n",
      "episode: 1163   score: 95.0  epsilon: 1.0    steps: 864  evaluation reward: 174.6\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2773: Policy loss: -0.064304. Value loss: 0.159024. Entropy: 0.309361.\n",
      "Iteration 2774: Policy loss: -0.075022. Value loss: 0.059704. Entropy: 0.310451.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2775: Policy loss: -0.082765. Value loss: 0.043096. Entropy: 0.309514.\n",
      "episode: 1164   score: 440.0  epsilon: 1.0    steps: 320  evaluation reward: 177.75\n",
      "episode: 1165   score: 335.0  epsilon: 1.0    steps: 968  evaluation reward: 177.6\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2776: Policy loss: -0.306636. Value loss: 0.405849. Entropy: 0.304897.\n",
      "Iteration 2777: Policy loss: -0.313148. Value loss: 0.216375. Entropy: 0.306354.\n",
      "Iteration 2778: Policy loss: -0.325853. Value loss: 0.117472. Entropy: 0.305703.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2779: Policy loss: 0.077285. Value loss: 0.100590. Entropy: 0.309373.\n",
      "Iteration 2780: Policy loss: 0.060093. Value loss: 0.048435. Entropy: 0.307723.\n",
      "Iteration 2781: Policy loss: 0.064781. Value loss: 0.033398. Entropy: 0.307334.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2782: Policy loss: 0.090910. Value loss: 0.142448. Entropy: 0.308551.\n",
      "Iteration 2783: Policy loss: 0.087172. Value loss: 0.068346. Entropy: 0.307499.\n",
      "Iteration 2784: Policy loss: 0.085137. Value loss: 0.050185. Entropy: 0.310089.\n",
      "episode: 1166   score: 455.0  epsilon: 1.0    steps: 232  evaluation reward: 180.95\n",
      "episode: 1167   score: 410.0  epsilon: 1.0    steps: 856  evaluation reward: 181.05\n",
      "episode: 1168   score: 120.0  epsilon: 1.0    steps: 904  evaluation reward: 181.5\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2785: Policy loss: 0.219976. Value loss: 0.085112. Entropy: 0.310960.\n",
      "Iteration 2786: Policy loss: 0.211406. Value loss: 0.033789. Entropy: 0.309905.\n",
      "Iteration 2787: Policy loss: 0.213348. Value loss: 0.024290. Entropy: 0.310142.\n",
      "episode: 1169   score: 675.0  epsilon: 1.0    steps: 216  evaluation reward: 186.7\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2788: Policy loss: 0.107621. Value loss: 0.070485. Entropy: 0.311198.\n",
      "Iteration 2789: Policy loss: 0.096558. Value loss: 0.030081. Entropy: 0.310660.\n",
      "Iteration 2790: Policy loss: 0.089228. Value loss: 0.023485. Entropy: 0.308617.\n",
      "episode: 1170   score: 150.0  epsilon: 1.0    steps: 576  evaluation reward: 186.0\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2791: Policy loss: 0.093064. Value loss: 0.066596. Entropy: 0.308158.\n",
      "Iteration 2792: Policy loss: 0.091277. Value loss: 0.036599. Entropy: 0.307556.\n",
      "Iteration 2793: Policy loss: 0.091057. Value loss: 0.027643. Entropy: 0.305598.\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2794: Policy loss: -0.032931. Value loss: 0.240731. Entropy: 0.305616.\n",
      "Iteration 2795: Policy loss: -0.037576. Value loss: 0.187873. Entropy: 0.303472.\n",
      "Iteration 2796: Policy loss: -0.043309. Value loss: 0.173594. Entropy: 0.304482.\n",
      "episode: 1171   score: 425.0  epsilon: 1.0    steps: 216  evaluation reward: 189.2\n",
      "episode: 1172   score: 260.0  epsilon: 1.0    steps: 896  evaluation reward: 190.25\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2797: Policy loss: -0.021231. Value loss: 0.077517. Entropy: 0.307471.\n",
      "Iteration 2798: Policy loss: -0.021877. Value loss: 0.036360. Entropy: 0.307352.\n",
      "Iteration 2799: Policy loss: -0.028534. Value loss: 0.025976. Entropy: 0.307712.\n",
      "episode: 1173   score: 155.0  epsilon: 1.0    steps: 360  evaluation reward: 190.3\n",
      "episode: 1174   score: 80.0  epsilon: 1.0    steps: 552  evaluation reward: 189.75\n",
      "episode: 1175   score: 320.0  epsilon: 1.0    steps: 816  evaluation reward: 191.45\n",
      "Training network. lr: 0.000229. clip: 0.091558\n",
      "Iteration 2800: Policy loss: 0.021334. Value loss: 0.092880. Entropy: 0.306897.\n",
      "Iteration 2801: Policy loss: 0.016576. Value loss: 0.038656. Entropy: 0.305646.\n",
      "Iteration 2802: Policy loss: 0.006778. Value loss: 0.030183. Entropy: 0.304038.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2803: Policy loss: -0.116289. Value loss: 0.136719. Entropy: 0.304817.\n",
      "Iteration 2804: Policy loss: -0.115453. Value loss: 0.059161. Entropy: 0.304506.\n",
      "Iteration 2805: Policy loss: -0.122191. Value loss: 0.043832. Entropy: 0.303719.\n",
      "episode: 1176   score: 210.0  epsilon: 1.0    steps: 136  evaluation reward: 192.65\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2806: Policy loss: -0.259300. Value loss: 0.314231. Entropy: 0.304971.\n",
      "Iteration 2807: Policy loss: -0.290511. Value loss: 0.220305. Entropy: 0.304271.\n",
      "Iteration 2808: Policy loss: -0.301520. Value loss: 0.155574. Entropy: 0.304539.\n",
      "episode: 1177   score: 240.0  epsilon: 1.0    steps: 544  evaluation reward: 193.5\n",
      "episode: 1178   score: 440.0  epsilon: 1.0    steps: 920  evaluation reward: 196.55\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2809: Policy loss: 0.055559. Value loss: 0.127434. Entropy: 0.303289.\n",
      "Iteration 2810: Policy loss: 0.054191. Value loss: 0.058456. Entropy: 0.303543.\n",
      "Iteration 2811: Policy loss: 0.044314. Value loss: 0.036341. Entropy: 0.303225.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2812: Policy loss: -0.056978. Value loss: 0.057201. Entropy: 0.304189.\n",
      "Iteration 2813: Policy loss: -0.054696. Value loss: 0.025767. Entropy: 0.306214.\n",
      "Iteration 2814: Policy loss: -0.069529. Value loss: 0.018448. Entropy: 0.304507.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2815: Policy loss: -0.009142. Value loss: 0.090258. Entropy: 0.306640.\n",
      "Iteration 2816: Policy loss: -0.015045. Value loss: 0.041074. Entropy: 0.306560.\n",
      "Iteration 2817: Policy loss: -0.021323. Value loss: 0.028829. Entropy: 0.306758.\n",
      "episode: 1179   score: 90.0  epsilon: 1.0    steps: 344  evaluation reward: 194.8\n",
      "episode: 1180   score: 255.0  epsilon: 1.0    steps: 920  evaluation reward: 195.25\n",
      "episode: 1181   score: 210.0  epsilon: 1.0    steps: 952  evaluation reward: 194.9\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2818: Policy loss: 0.023078. Value loss: 0.072676. Entropy: 0.306525.\n",
      "Iteration 2819: Policy loss: 0.024354. Value loss: 0.027130. Entropy: 0.304322.\n",
      "Iteration 2820: Policy loss: 0.019240. Value loss: 0.019514. Entropy: 0.303646.\n",
      "episode: 1182   score: 340.0  epsilon: 1.0    steps: 768  evaluation reward: 196.8\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2821: Policy loss: 0.039439. Value loss: 0.084230. Entropy: 0.308221.\n",
      "Iteration 2822: Policy loss: 0.027362. Value loss: 0.047012. Entropy: 0.306663.\n",
      "Iteration 2823: Policy loss: 0.028695. Value loss: 0.034728. Entropy: 0.306378.\n",
      "episode: 1183   score: 500.0  epsilon: 1.0    steps: 928  evaluation reward: 200.85\n",
      "episode: 1184   score: 500.0  epsilon: 1.0    steps: 1008  evaluation reward: 201.75\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2824: Policy loss: -0.191775. Value loss: 0.293049. Entropy: 0.303431.\n",
      "Iteration 2825: Policy loss: -0.184567. Value loss: 0.161623. Entropy: 0.301228.\n",
      "Iteration 2826: Policy loss: -0.200190. Value loss: 0.097720. Entropy: 0.301669.\n",
      "episode: 1185   score: 210.0  epsilon: 1.0    steps: 1016  evaluation reward: 201.7\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2827: Policy loss: -0.102526. Value loss: 0.084555. Entropy: 0.302994.\n",
      "Iteration 2828: Policy loss: -0.108124. Value loss: 0.037481. Entropy: 0.300943.\n",
      "Iteration 2829: Policy loss: -0.106989. Value loss: 0.028645. Entropy: 0.300473.\n",
      "episode: 1186   score: 50.0  epsilon: 1.0    steps: 72  evaluation reward: 200.1\n",
      "episode: 1187   score: 110.0  epsilon: 1.0    steps: 160  evaluation reward: 200.5\n",
      "episode: 1188   score: 210.0  epsilon: 1.0    steps: 208  evaluation reward: 200.2\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2830: Policy loss: 0.037586. Value loss: 0.052283. Entropy: 0.302690.\n",
      "Iteration 2831: Policy loss: 0.033389. Value loss: 0.026155. Entropy: 0.303353.\n",
      "Iteration 2832: Policy loss: 0.031240. Value loss: 0.021383. Entropy: 0.302967.\n",
      "episode: 1189   score: 80.0  epsilon: 1.0    steps: 392  evaluation reward: 199.8\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2833: Policy loss: 0.152295. Value loss: 0.135481. Entropy: 0.303965.\n",
      "Iteration 2834: Policy loss: 0.155039. Value loss: 0.077464. Entropy: 0.305085.\n",
      "Iteration 2835: Policy loss: 0.139768. Value loss: 0.057536. Entropy: 0.303602.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2836: Policy loss: -0.062688. Value loss: 0.124303. Entropy: 0.298683.\n",
      "Iteration 2837: Policy loss: -0.067547. Value loss: 0.072094. Entropy: 0.300263.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2838: Policy loss: -0.067564. Value loss: 0.051015. Entropy: 0.300259.\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2839: Policy loss: 0.116314. Value loss: 0.076566. Entropy: 0.306290.\n",
      "Iteration 2840: Policy loss: 0.118478. Value loss: 0.027060. Entropy: 0.307295.\n",
      "Iteration 2841: Policy loss: 0.117059. Value loss: 0.020523. Entropy: 0.307351.\n",
      "episode: 1190   score: 115.0  epsilon: 1.0    steps: 184  evaluation reward: 200.45\n",
      "episode: 1191   score: 180.0  epsilon: 1.0    steps: 272  evaluation reward: 200.9\n",
      "episode: 1192   score: 180.0  epsilon: 1.0    steps: 272  evaluation reward: 201.3\n",
      "episode: 1193   score: 125.0  epsilon: 1.0    steps: 328  evaluation reward: 199.95\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2842: Policy loss: 0.071886. Value loss: 0.103267. Entropy: 0.301996.\n",
      "Iteration 2843: Policy loss: 0.066992. Value loss: 0.047478. Entropy: 0.300885.\n",
      "Iteration 2844: Policy loss: 0.071509. Value loss: 0.033289. Entropy: 0.301769.\n",
      "episode: 1194   score: 305.0  epsilon: 1.0    steps: 368  evaluation reward: 200.9\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2845: Policy loss: 0.029433. Value loss: 0.123459. Entropy: 0.302391.\n",
      "Iteration 2846: Policy loss: 0.033407. Value loss: 0.065517. Entropy: 0.301004.\n",
      "Iteration 2847: Policy loss: 0.022090. Value loss: 0.045771. Entropy: 0.302341.\n",
      "episode: 1195   score: 210.0  epsilon: 1.0    steps: 288  evaluation reward: 201.45\n",
      "Training network. lr: 0.000229. clip: 0.091401\n",
      "Iteration 2848: Policy loss: -0.024528. Value loss: 0.089592. Entropy: 0.304926.\n",
      "Iteration 2849: Policy loss: -0.026478. Value loss: 0.043828. Entropy: 0.307228.\n",
      "Iteration 2850: Policy loss: -0.025463. Value loss: 0.036644. Entropy: 0.305658.\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2851: Policy loss: 0.106818. Value loss: 0.118148. Entropy: 0.297640.\n",
      "Iteration 2852: Policy loss: 0.096706. Value loss: 0.052099. Entropy: 0.298457.\n",
      "Iteration 2853: Policy loss: 0.098353. Value loss: 0.043804. Entropy: 0.298357.\n",
      "episode: 1196   score: 275.0  epsilon: 1.0    steps: 8  evaluation reward: 203.4\n",
      "episode: 1197   score: 155.0  epsilon: 1.0    steps: 496  evaluation reward: 203.65\n",
      "episode: 1198   score: 155.0  epsilon: 1.0    steps: 576  evaluation reward: 202.8\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2854: Policy loss: 0.161193. Value loss: 0.062627. Entropy: 0.303546.\n",
      "Iteration 2855: Policy loss: 0.157306. Value loss: 0.028639. Entropy: 0.302133.\n",
      "Iteration 2856: Policy loss: 0.156153. Value loss: 0.021898. Entropy: 0.301229.\n",
      "episode: 1199   score: 285.0  epsilon: 1.0    steps: 56  evaluation reward: 204.6\n",
      "episode: 1200   score: 120.0  epsilon: 1.0    steps: 840  evaluation reward: 203.2\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2857: Policy loss: 0.077553. Value loss: 0.092091. Entropy: 0.296696.\n",
      "Iteration 2858: Policy loss: 0.068609. Value loss: 0.041256. Entropy: 0.299941.\n",
      "Iteration 2859: Policy loss: 0.073073. Value loss: 0.028836. Entropy: 0.299569.\n",
      "now time :  2019-09-05 17:12:49.288248\n",
      "episode: 1201   score: 225.0  epsilon: 1.0    steps: 664  evaluation reward: 204.35\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2860: Policy loss: 0.038987. Value loss: 0.074687. Entropy: 0.302163.\n",
      "Iteration 2861: Policy loss: 0.034092. Value loss: 0.032979. Entropy: 0.301663.\n",
      "Iteration 2862: Policy loss: 0.031809. Value loss: 0.024919. Entropy: 0.300829.\n",
      "episode: 1202   score: 155.0  epsilon: 1.0    steps: 488  evaluation reward: 204.55\n",
      "episode: 1203   score: 105.0  epsilon: 1.0    steps: 872  evaluation reward: 204.4\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2863: Policy loss: 0.116163. Value loss: 0.065351. Entropy: 0.309300.\n",
      "Iteration 2864: Policy loss: 0.106308. Value loss: 0.021698. Entropy: 0.307823.\n",
      "Iteration 2865: Policy loss: 0.102700. Value loss: 0.016253. Entropy: 0.309634.\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2866: Policy loss: -0.025902. Value loss: 0.088760. Entropy: 0.307217.\n",
      "Iteration 2867: Policy loss: -0.025234. Value loss: 0.035262. Entropy: 0.305298.\n",
      "Iteration 2868: Policy loss: -0.026320. Value loss: 0.026766. Entropy: 0.306886.\n",
      "episode: 1204   score: 180.0  epsilon: 1.0    steps: 536  evaluation reward: 205.15\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2869: Policy loss: -0.080067. Value loss: 0.060217. Entropy: 0.301450.\n",
      "Iteration 2870: Policy loss: -0.080170. Value loss: 0.029135. Entropy: 0.304369.\n",
      "Iteration 2871: Policy loss: -0.078625. Value loss: 0.021376. Entropy: 0.305025.\n",
      "episode: 1205   score: 210.0  epsilon: 1.0    steps: 832  evaluation reward: 205.55\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2872: Policy loss: -0.038942. Value loss: 0.032559. Entropy: 0.307137.\n",
      "Iteration 2873: Policy loss: -0.044490. Value loss: 0.016744. Entropy: 0.308528.\n",
      "Iteration 2874: Policy loss: -0.042961. Value loss: 0.013182. Entropy: 0.305594.\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2875: Policy loss: -0.006627. Value loss: 0.083443. Entropy: 0.295563.\n",
      "Iteration 2876: Policy loss: -0.006481. Value loss: 0.042986. Entropy: 0.296507.\n",
      "Iteration 2877: Policy loss: -0.008771. Value loss: 0.027740. Entropy: 0.295192.\n",
      "episode: 1206   score: 260.0  epsilon: 1.0    steps: 160  evaluation reward: 205.35\n",
      "episode: 1207   score: 410.0  epsilon: 1.0    steps: 216  evaluation reward: 207.9\n",
      "episode: 1208   score: 390.0  epsilon: 1.0    steps: 896  evaluation reward: 209.7\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2878: Policy loss: 0.041118. Value loss: 0.099205. Entropy: 0.294114.\n",
      "Iteration 2879: Policy loss: 0.037528. Value loss: 0.039848. Entropy: 0.296786.\n",
      "Iteration 2880: Policy loss: 0.032149. Value loss: 0.030497. Entropy: 0.295773.\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2881: Policy loss: 0.028066. Value loss: 0.074930. Entropy: 0.301884.\n",
      "Iteration 2882: Policy loss: 0.025592. Value loss: 0.028480. Entropy: 0.297308.\n",
      "Iteration 2883: Policy loss: 0.025682. Value loss: 0.021128. Entropy: 0.298479.\n",
      "episode: 1209   score: 225.0  epsilon: 1.0    steps: 232  evaluation reward: 207.85\n",
      "episode: 1210   score: 210.0  epsilon: 1.0    steps: 288  evaluation reward: 208.35\n",
      "episode: 1211   score: 135.0  epsilon: 1.0    steps: 352  evaluation reward: 208.35\n",
      "episode: 1212   score: 210.0  epsilon: 1.0    steps: 552  evaluation reward: 208.35\n",
      "episode: 1213   score: 155.0  epsilon: 1.0    steps: 760  evaluation reward: 208.35\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2884: Policy loss: 0.071147. Value loss: 0.046470. Entropy: 0.310162.\n",
      "Iteration 2885: Policy loss: 0.066026. Value loss: 0.032715. Entropy: 0.309097.\n",
      "Iteration 2886: Policy loss: 0.067446. Value loss: 0.026114. Entropy: 0.308410.\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2887: Policy loss: 0.354765. Value loss: 0.084503. Entropy: 0.304325.\n",
      "Iteration 2888: Policy loss: 0.351680. Value loss: 0.034116. Entropy: 0.301948.\n",
      "Iteration 2889: Policy loss: 0.337537. Value loss: 0.025376. Entropy: 0.305820.\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2890: Policy loss: 0.009446. Value loss: 0.061100. Entropy: 0.307653.\n",
      "Iteration 2891: Policy loss: -0.000290. Value loss: 0.027602. Entropy: 0.307336.\n",
      "Iteration 2892: Policy loss: 0.003455. Value loss: 0.019542. Entropy: 0.306773.\n",
      "episode: 1214   score: 155.0  epsilon: 1.0    steps: 984  evaluation reward: 208.45\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2893: Policy loss: -0.175144. Value loss: 0.118718. Entropy: 0.290213.\n",
      "Iteration 2894: Policy loss: -0.174395. Value loss: 0.046301. Entropy: 0.289151.\n",
      "Iteration 2895: Policy loss: -0.180368. Value loss: 0.030697. Entropy: 0.288259.\n",
      "episode: 1215   score: 115.0  epsilon: 1.0    steps: 368  evaluation reward: 205.8\n",
      "episode: 1216   score: 90.0  epsilon: 1.0    steps: 408  evaluation reward: 205.6\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2896: Policy loss: -0.305444. Value loss: 0.327551. Entropy: 0.308439.\n",
      "Iteration 2897: Policy loss: -0.330416. Value loss: 0.221531. Entropy: 0.309642.\n",
      "Iteration 2898: Policy loss: -0.340229. Value loss: 0.176615. Entropy: 0.310134.\n",
      "episode: 1217   score: 285.0  epsilon: 1.0    steps: 912  evaluation reward: 207.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1218   score: 240.0  epsilon: 1.0    steps: 1008  evaluation reward: 206.7\n",
      "Training network. lr: 0.000228. clip: 0.091254\n",
      "Iteration 2899: Policy loss: -0.084231. Value loss: 0.101351. Entropy: 0.287313.\n",
      "Iteration 2900: Policy loss: -0.082507. Value loss: 0.051323. Entropy: 0.285151.\n",
      "Iteration 2901: Policy loss: -0.081416. Value loss: 0.037666. Entropy: 0.286726.\n",
      "episode: 1219   score: 545.0  epsilon: 1.0    steps: 728  evaluation reward: 211.15\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2902: Policy loss: 0.060892. Value loss: 0.116322. Entropy: 0.307733.\n",
      "Iteration 2903: Policy loss: 0.053944. Value loss: 0.052955. Entropy: 0.307649.\n",
      "Iteration 2904: Policy loss: 0.046909. Value loss: 0.037261. Entropy: 0.307409.\n",
      "episode: 1220   score: 210.0  epsilon: 1.0    steps: 80  evaluation reward: 211.45\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2905: Policy loss: 0.159037. Value loss: 0.100332. Entropy: 0.303588.\n",
      "Iteration 2906: Policy loss: 0.157230. Value loss: 0.047948. Entropy: 0.302381.\n",
      "Iteration 2907: Policy loss: 0.154142. Value loss: 0.035185. Entropy: 0.302116.\n",
      "episode: 1221   score: 285.0  epsilon: 1.0    steps: 96  evaluation reward: 213.2\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2908: Policy loss: 0.111723. Value loss: 0.093701. Entropy: 0.304524.\n",
      "Iteration 2909: Policy loss: 0.105157. Value loss: 0.040397. Entropy: 0.301983.\n",
      "Iteration 2910: Policy loss: 0.107418. Value loss: 0.029104. Entropy: 0.303205.\n",
      "episode: 1222   score: 180.0  epsilon: 1.0    steps: 440  evaluation reward: 212.9\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2911: Policy loss: -0.024261. Value loss: 0.071406. Entropy: 0.296212.\n",
      "Iteration 2912: Policy loss: -0.023557. Value loss: 0.029361. Entropy: 0.294980.\n",
      "Iteration 2913: Policy loss: -0.026805. Value loss: 0.020130. Entropy: 0.295247.\n",
      "episode: 1223   score: 75.0  epsilon: 1.0    steps: 256  evaluation reward: 212.55\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2914: Policy loss: -0.225972. Value loss: 0.334350. Entropy: 0.300751.\n",
      "Iteration 2915: Policy loss: -0.242978. Value loss: 0.221265. Entropy: 0.298891.\n",
      "Iteration 2916: Policy loss: -0.231257. Value loss: 0.139422. Entropy: 0.300353.\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2917: Policy loss: 0.038247. Value loss: 0.100444. Entropy: 0.295376.\n",
      "Iteration 2918: Policy loss: 0.039519. Value loss: 0.041280. Entropy: 0.296076.\n",
      "Iteration 2919: Policy loss: 0.038579. Value loss: 0.028200. Entropy: 0.295237.\n",
      "episode: 1224   score: 210.0  epsilon: 1.0    steps: 120  evaluation reward: 213.45\n",
      "episode: 1225   score: 180.0  epsilon: 1.0    steps: 280  evaluation reward: 213.1\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2920: Policy loss: 0.097382. Value loss: 0.104235. Entropy: 0.307891.\n",
      "Iteration 2921: Policy loss: 0.100374. Value loss: 0.040760. Entropy: 0.306997.\n",
      "Iteration 2922: Policy loss: 0.090386. Value loss: 0.033861. Entropy: 0.307450.\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2923: Policy loss: 0.048944. Value loss: 0.093118. Entropy: 0.302139.\n",
      "Iteration 2924: Policy loss: 0.043644. Value loss: 0.043859. Entropy: 0.299701.\n",
      "Iteration 2925: Policy loss: 0.037906. Value loss: 0.031735. Entropy: 0.299645.\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2926: Policy loss: 0.107178. Value loss: 0.116716. Entropy: 0.308174.\n",
      "Iteration 2927: Policy loss: 0.103193. Value loss: 0.043272. Entropy: 0.308103.\n",
      "Iteration 2928: Policy loss: 0.096978. Value loss: 0.029596. Entropy: 0.307395.\n",
      "episode: 1226   score: 210.0  epsilon: 1.0    steps: 600  evaluation reward: 213.8\n",
      "episode: 1227   score: 260.0  epsilon: 1.0    steps: 896  evaluation reward: 215.35\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2929: Policy loss: 0.325646. Value loss: 0.063315. Entropy: 0.306716.\n",
      "Iteration 2930: Policy loss: 0.316383. Value loss: 0.026387. Entropy: 0.306249.\n",
      "Iteration 2931: Policy loss: 0.311053. Value loss: 0.018391. Entropy: 0.305897.\n",
      "episode: 1228   score: 470.0  epsilon: 1.0    steps: 376  evaluation reward: 218.7\n",
      "episode: 1229   score: 180.0  epsilon: 1.0    steps: 616  evaluation reward: 218.1\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2932: Policy loss: 0.080255. Value loss: 0.058762. Entropy: 0.307176.\n",
      "Iteration 2933: Policy loss: 0.070667. Value loss: 0.030864. Entropy: 0.307426.\n",
      "Iteration 2934: Policy loss: 0.072872. Value loss: 0.024627. Entropy: 0.307894.\n",
      "episode: 1230   score: 390.0  epsilon: 1.0    steps: 216  evaluation reward: 220.8\n",
      "episode: 1231   score: 550.0  epsilon: 1.0    steps: 736  evaluation reward: 224.2\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2935: Policy loss: 0.130507. Value loss: 0.094289. Entropy: 0.304619.\n",
      "Iteration 2936: Policy loss: 0.118547. Value loss: 0.038764. Entropy: 0.302421.\n",
      "Iteration 2937: Policy loss: 0.115410. Value loss: 0.030421. Entropy: 0.301862.\n",
      "episode: 1232   score: 210.0  epsilon: 1.0    steps: 504  evaluation reward: 224.95\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2938: Policy loss: 0.221047. Value loss: 0.078553. Entropy: 0.307322.\n",
      "Iteration 2939: Policy loss: 0.219318. Value loss: 0.032630. Entropy: 0.305470.\n",
      "Iteration 2940: Policy loss: 0.222799. Value loss: 0.022901. Entropy: 0.306522.\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2941: Policy loss: 0.199930. Value loss: 0.087559. Entropy: 0.306730.\n",
      "Iteration 2942: Policy loss: 0.196585. Value loss: 0.038630. Entropy: 0.304525.\n",
      "Iteration 2943: Policy loss: 0.198532. Value loss: 0.030374. Entropy: 0.308365.\n",
      "episode: 1233   score: 240.0  epsilon: 1.0    steps: 16  evaluation reward: 225.8\n",
      "episode: 1234   score: 50.0  epsilon: 1.0    steps: 600  evaluation reward: 226.15\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2944: Policy loss: 0.273821. Value loss: 0.040176. Entropy: 0.309099.\n",
      "Iteration 2945: Policy loss: 0.268464. Value loss: 0.017703. Entropy: 0.309479.\n",
      "Iteration 2946: Policy loss: 0.273799. Value loss: 0.013214. Entropy: 0.309421.\n",
      "episode: 1235   score: 180.0  epsilon: 1.0    steps: 984  evaluation reward: 225.75\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2947: Policy loss: -0.003908. Value loss: 0.048102. Entropy: 0.308953.\n",
      "Iteration 2948: Policy loss: -0.007834. Value loss: 0.023496. Entropy: 0.311112.\n",
      "Iteration 2949: Policy loss: -0.009597. Value loss: 0.017691. Entropy: 0.310351.\n",
      "episode: 1236   score: 210.0  epsilon: 1.0    steps: 384  evaluation reward: 225.15\n",
      "episode: 1237   score: 210.0  epsilon: 1.0    steps: 584  evaluation reward: 225.45\n",
      "episode: 1238   score: 210.0  epsilon: 1.0    steps: 944  evaluation reward: 225.4\n",
      "Training network. lr: 0.000228. clip: 0.091097\n",
      "Iteration 2950: Policy loss: -0.054815. Value loss: 0.065055. Entropy: 0.308056.\n",
      "Iteration 2951: Policy loss: -0.059864. Value loss: 0.035782. Entropy: 0.309196.\n",
      "Iteration 2952: Policy loss: -0.058824. Value loss: 0.028652. Entropy: 0.309437.\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2953: Policy loss: -0.426986. Value loss: 0.315073. Entropy: 0.310430.\n",
      "Iteration 2954: Policy loss: -0.448323. Value loss: 0.211712. Entropy: 0.309500.\n",
      "Iteration 2955: Policy loss: -0.454941. Value loss: 0.127040. Entropy: 0.308359.\n",
      "episode: 1239   score: 285.0  epsilon: 1.0    steps: 976  evaluation reward: 226.45\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2956: Policy loss: 0.044991. Value loss: 0.113235. Entropy: 0.304422.\n",
      "Iteration 2957: Policy loss: 0.040082. Value loss: 0.061148. Entropy: 0.307775.\n",
      "Iteration 2958: Policy loss: 0.044352. Value loss: 0.044727. Entropy: 0.308111.\n",
      "episode: 1240   score: 210.0  epsilon: 1.0    steps: 56  evaluation reward: 227.25\n",
      "episode: 1241   score: 105.0  epsilon: 1.0    steps: 992  evaluation reward: 226.8\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2959: Policy loss: 0.058760. Value loss: 0.077548. Entropy: 0.306578.\n",
      "Iteration 2960: Policy loss: 0.060096. Value loss: 0.032587. Entropy: 0.307987.\n",
      "Iteration 2961: Policy loss: 0.060218. Value loss: 0.022780. Entropy: 0.307016.\n",
      "episode: 1242   score: 80.0  epsilon: 1.0    steps: 376  evaluation reward: 225.8\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2962: Policy loss: 0.133540. Value loss: 0.064988. Entropy: 0.306403.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2963: Policy loss: 0.131641. Value loss: 0.030800. Entropy: 0.305363.\n",
      "Iteration 2964: Policy loss: 0.129936. Value loss: 0.021786. Entropy: 0.304983.\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2965: Policy loss: -0.021960. Value loss: 0.113944. Entropy: 0.306808.\n",
      "Iteration 2966: Policy loss: -0.029928. Value loss: 0.058269. Entropy: 0.305670.\n",
      "Iteration 2967: Policy loss: -0.032994. Value loss: 0.045611. Entropy: 0.304640.\n",
      "episode: 1243   score: 210.0  epsilon: 1.0    steps: 576  evaluation reward: 226.7\n",
      "episode: 1244   score: 550.0  epsilon: 1.0    steps: 1000  evaluation reward: 230.6\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2968: Policy loss: -0.364220. Value loss: 0.327139. Entropy: 0.308246.\n",
      "Iteration 2969: Policy loss: -0.341147. Value loss: 0.234200. Entropy: 0.308481.\n",
      "Iteration 2970: Policy loss: -0.376053. Value loss: 0.189966. Entropy: 0.308919.\n",
      "episode: 1245   score: 285.0  epsilon: 1.0    steps: 896  evaluation reward: 230.05\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2971: Policy loss: -0.035801. Value loss: 0.102830. Entropy: 0.307563.\n",
      "Iteration 2972: Policy loss: -0.041045. Value loss: 0.046260. Entropy: 0.309251.\n",
      "Iteration 2973: Policy loss: -0.045317. Value loss: 0.033866. Entropy: 0.307851.\n",
      "episode: 1246   score: 490.0  epsilon: 1.0    steps: 576  evaluation reward: 233.6\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2974: Policy loss: -0.046269. Value loss: 0.159475. Entropy: 0.303466.\n",
      "Iteration 2975: Policy loss: -0.059244. Value loss: 0.082013. Entropy: 0.304471.\n",
      "Iteration 2976: Policy loss: -0.057215. Value loss: 0.046037. Entropy: 0.306216.\n",
      "episode: 1247   score: 135.0  epsilon: 1.0    steps: 176  evaluation reward: 233.9\n",
      "episode: 1248   score: 180.0  epsilon: 1.0    steps: 304  evaluation reward: 231.1\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2977: Policy loss: -0.172094. Value loss: 0.264184. Entropy: 0.305200.\n",
      "Iteration 2978: Policy loss: -0.174726. Value loss: 0.097396. Entropy: 0.306061.\n",
      "Iteration 2979: Policy loss: -0.211050. Value loss: 0.056617. Entropy: 0.305369.\n",
      "episode: 1249   score: 380.0  epsilon: 1.0    steps: 472  evaluation reward: 234.25\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2980: Policy loss: 0.067017. Value loss: 0.046909. Entropy: 0.315481.\n",
      "Iteration 2981: Policy loss: 0.065769. Value loss: 0.024112. Entropy: 0.313802.\n",
      "Iteration 2982: Policy loss: 0.066957. Value loss: 0.020306. Entropy: 0.313515.\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2983: Policy loss: 0.136867. Value loss: 0.089700. Entropy: 0.312190.\n",
      "Iteration 2984: Policy loss: 0.123352. Value loss: 0.033916. Entropy: 0.312818.\n",
      "Iteration 2985: Policy loss: 0.132044. Value loss: 0.023388. Entropy: 0.313343.\n",
      "episode: 1250   score: 470.0  epsilon: 1.0    steps: 360  evaluation reward: 237.15\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2986: Policy loss: 0.146440. Value loss: 0.106662. Entropy: 0.308866.\n",
      "Iteration 2987: Policy loss: 0.141373. Value loss: 0.033484. Entropy: 0.307133.\n",
      "Iteration 2988: Policy loss: 0.137119. Value loss: 0.020433. Entropy: 0.308257.\n",
      "now time :  2019-09-05 17:20:48.892169\n",
      "episode: 1251   score: 165.0  epsilon: 1.0    steps: 272  evaluation reward: 236.7\n",
      "episode: 1252   score: 80.0  epsilon: 1.0    steps: 560  evaluation reward: 235.15\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2989: Policy loss: -0.099789. Value loss: 0.077260. Entropy: 0.310365.\n",
      "Iteration 2990: Policy loss: -0.104074. Value loss: 0.029817. Entropy: 0.309414.\n",
      "Iteration 2991: Policy loss: -0.104562. Value loss: 0.021734. Entropy: 0.311914.\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2992: Policy loss: -0.246177. Value loss: 0.303892. Entropy: 0.313650.\n",
      "Iteration 2993: Policy loss: -0.256589. Value loss: 0.193545. Entropy: 0.310903.\n",
      "Iteration 2994: Policy loss: -0.251691. Value loss: 0.131843. Entropy: 0.311070.\n",
      "episode: 1253   score: 270.0  epsilon: 1.0    steps: 544  evaluation reward: 237.35\n",
      "episode: 1254   score: 210.0  epsilon: 1.0    steps: 912  evaluation reward: 238.65\n",
      "episode: 1255   score: 180.0  epsilon: 1.0    steps: 952  evaluation reward: 237.4\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2995: Policy loss: 0.371008. Value loss: 0.127867. Entropy: 0.312987.\n",
      "Iteration 2996: Policy loss: 0.360950. Value loss: 0.039338. Entropy: 0.313212.\n",
      "Iteration 2997: Policy loss: 0.352209. Value loss: 0.025654. Entropy: 0.311804.\n",
      "episode: 1256   score: 460.0  epsilon: 1.0    steps: 248  evaluation reward: 239.85\n",
      "episode: 1257   score: 325.0  epsilon: 1.0    steps: 904  evaluation reward: 242.55\n",
      "Training network. lr: 0.000227. clip: 0.090941\n",
      "Iteration 2998: Policy loss: 0.041642. Value loss: 0.040223. Entropy: 0.309566.\n",
      "Iteration 2999: Policy loss: 0.033519. Value loss: 0.019155. Entropy: 0.310227.\n",
      "Iteration 3000: Policy loss: 0.037582. Value loss: 0.020157. Entropy: 0.309828.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3001: Policy loss: -0.291065. Value loss: 0.276378. Entropy: 0.312836.\n",
      "Iteration 3002: Policy loss: -0.294541. Value loss: 0.129963. Entropy: 0.316219.\n",
      "Iteration 3003: Policy loss: -0.318905. Value loss: 0.071720. Entropy: 0.314416.\n",
      "episode: 1258   score: 410.0  epsilon: 1.0    steps: 808  evaluation reward: 243.7\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3004: Policy loss: 0.036476. Value loss: 0.043136. Entropy: 0.319427.\n",
      "Iteration 3005: Policy loss: 0.038025. Value loss: 0.018448. Entropy: 0.318152.\n",
      "Iteration 3006: Policy loss: 0.036026. Value loss: 0.014829. Entropy: 0.318482.\n",
      "episode: 1259   score: 120.0  epsilon: 1.0    steps: 304  evaluation reward: 243.65\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3007: Policy loss: 0.106626. Value loss: 0.035055. Entropy: 0.311356.\n",
      "Iteration 3008: Policy loss: 0.100822. Value loss: 0.012883. Entropy: 0.311522.\n",
      "Iteration 3009: Policy loss: 0.098346. Value loss: 0.009101. Entropy: 0.310542.\n",
      "episode: 1260   score: 180.0  epsilon: 1.0    steps: 120  evaluation reward: 244.5\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3010: Policy loss: -0.106172. Value loss: 0.359289. Entropy: 0.317220.\n",
      "Iteration 3011: Policy loss: -0.143426. Value loss: 0.269821. Entropy: 0.313292.\n",
      "Iteration 3012: Policy loss: -0.134960. Value loss: 0.154574. Entropy: 0.316942.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3013: Policy loss: 0.106577. Value loss: 0.106457. Entropy: 0.313664.\n",
      "Iteration 3014: Policy loss: 0.113441. Value loss: 0.050258. Entropy: 0.313631.\n",
      "Iteration 3015: Policy loss: 0.108639. Value loss: 0.040283. Entropy: 0.312437.\n",
      "episode: 1261   score: 180.0  epsilon: 1.0    steps: 136  evaluation reward: 244.5\n",
      "episode: 1262   score: 180.0  epsilon: 1.0    steps: 640  evaluation reward: 244.2\n",
      "episode: 1263   score: 380.0  epsilon: 1.0    steps: 864  evaluation reward: 247.05\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3016: Policy loss: 0.168344. Value loss: 0.076063. Entropy: 0.314651.\n",
      "Iteration 3017: Policy loss: 0.163320. Value loss: 0.029393. Entropy: 0.314705.\n",
      "Iteration 3018: Policy loss: 0.165356. Value loss: 0.021958. Entropy: 0.316505.\n",
      "episode: 1264   score: 210.0  epsilon: 1.0    steps: 408  evaluation reward: 244.75\n",
      "episode: 1265   score: 260.0  epsilon: 1.0    steps: 552  evaluation reward: 244.0\n",
      "episode: 1266   score: 110.0  epsilon: 1.0    steps: 864  evaluation reward: 240.55\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3019: Policy loss: 0.020621. Value loss: 0.032939. Entropy: 0.317687.\n",
      "Iteration 3020: Policy loss: 0.019776. Value loss: 0.019741. Entropy: 0.317507.\n",
      "Iteration 3021: Policy loss: 0.020602. Value loss: 0.016252. Entropy: 0.318613.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3022: Policy loss: 0.016693. Value loss: 0.039810. Entropy: 0.318726.\n",
      "Iteration 3023: Policy loss: 0.016543. Value loss: 0.019435. Entropy: 0.318746.\n",
      "Iteration 3024: Policy loss: 0.007531. Value loss: 0.013737. Entropy: 0.317850.\n",
      "episode: 1267   score: 180.0  epsilon: 1.0    steps: 664  evaluation reward: 238.25\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3025: Policy loss: 0.135750. Value loss: 0.061633. Entropy: 0.311842.\n",
      "Iteration 3026: Policy loss: 0.129875. Value loss: 0.031491. Entropy: 0.311768.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3027: Policy loss: 0.135243. Value loss: 0.022274. Entropy: 0.312505.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3028: Policy loss: -0.020235. Value loss: 0.027603. Entropy: 0.312508.\n",
      "Iteration 3029: Policy loss: -0.023239. Value loss: 0.011740. Entropy: 0.311300.\n",
      "Iteration 3030: Policy loss: -0.023573. Value loss: 0.008882. Entropy: 0.312077.\n",
      "episode: 1268   score: 270.0  epsilon: 1.0    steps: 680  evaluation reward: 239.75\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3031: Policy loss: -0.073749. Value loss: 0.056016. Entropy: 0.315688.\n",
      "Iteration 3032: Policy loss: -0.081229. Value loss: 0.032357. Entropy: 0.315376.\n",
      "Iteration 3033: Policy loss: -0.079559. Value loss: 0.024469. Entropy: 0.315669.\n",
      "episode: 1269   score: 110.0  epsilon: 1.0    steps: 752  evaluation reward: 234.1\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3034: Policy loss: 0.111311. Value loss: 0.068697. Entropy: 0.314130.\n",
      "Iteration 3035: Policy loss: 0.111206. Value loss: 0.024384. Entropy: 0.313535.\n",
      "Iteration 3036: Policy loss: 0.103076. Value loss: 0.017489. Entropy: 0.314121.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3037: Policy loss: 0.058761. Value loss: 0.065906. Entropy: 0.314570.\n",
      "Iteration 3038: Policy loss: 0.056502. Value loss: 0.036619. Entropy: 0.314629.\n",
      "Iteration 3039: Policy loss: 0.056426. Value loss: 0.029154. Entropy: 0.314591.\n",
      "episode: 1270   score: 210.0  epsilon: 1.0    steps: 136  evaluation reward: 234.7\n",
      "episode: 1271   score: 260.0  epsilon: 1.0    steps: 440  evaluation reward: 233.05\n",
      "episode: 1272   score: 210.0  epsilon: 1.0    steps: 688  evaluation reward: 232.55\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3040: Policy loss: -0.306023. Value loss: 0.288664. Entropy: 0.310015.\n",
      "Iteration 3041: Policy loss: -0.296040. Value loss: 0.121636. Entropy: 0.309856.\n",
      "Iteration 3042: Policy loss: -0.299336. Value loss: 0.053741. Entropy: 0.311396.\n",
      "episode: 1273   score: 295.0  epsilon: 1.0    steps: 864  evaluation reward: 233.95\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3043: Policy loss: -0.333882. Value loss: 0.287586. Entropy: 0.313366.\n",
      "Iteration 3044: Policy loss: -0.342549. Value loss: 0.189259. Entropy: 0.317262.\n",
      "Iteration 3045: Policy loss: -0.342928. Value loss: 0.115998. Entropy: 0.311682.\n",
      "episode: 1274   score: 105.0  epsilon: 1.0    steps: 768  evaluation reward: 234.2\n",
      "episode: 1275   score: 515.0  epsilon: 1.0    steps: 952  evaluation reward: 236.15\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3046: Policy loss: 0.200233. Value loss: 0.091057. Entropy: 0.313251.\n",
      "Iteration 3047: Policy loss: 0.184312. Value loss: 0.029844. Entropy: 0.311393.\n",
      "Iteration 3048: Policy loss: 0.192144. Value loss: 0.021866. Entropy: 0.312990.\n",
      "Training network. lr: 0.000227. clip: 0.090793\n",
      "Iteration 3049: Policy loss: 0.031948. Value loss: 0.030040. Entropy: 0.313240.\n",
      "Iteration 3050: Policy loss: 0.029324. Value loss: 0.014423. Entropy: 0.313975.\n",
      "Iteration 3051: Policy loss: 0.026039. Value loss: 0.009685. Entropy: 0.312765.\n",
      "episode: 1276   score: 500.0  epsilon: 1.0    steps: 152  evaluation reward: 239.05\n",
      "episode: 1277   score: 120.0  epsilon: 1.0    steps: 640  evaluation reward: 237.85\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3052: Policy loss: -0.116690. Value loss: 0.226851. Entropy: 0.315397.\n",
      "Iteration 3053: Policy loss: -0.137913. Value loss: 0.065402. Entropy: 0.316045.\n",
      "Iteration 3054: Policy loss: -0.126585. Value loss: 0.059721. Entropy: 0.314912.\n",
      "episode: 1278   score: 180.0  epsilon: 1.0    steps: 392  evaluation reward: 235.25\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3055: Policy loss: 0.007730. Value loss: 0.047183. Entropy: 0.315255.\n",
      "Iteration 3056: Policy loss: 0.003423. Value loss: 0.021126. Entropy: 0.315683.\n",
      "Iteration 3057: Policy loss: 0.002390. Value loss: 0.015057. Entropy: 0.313750.\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3058: Policy loss: 0.401130. Value loss: 0.217963. Entropy: 0.316199.\n",
      "Iteration 3059: Policy loss: 0.406928. Value loss: 0.073797. Entropy: 0.316178.\n",
      "Iteration 3060: Policy loss: 0.384312. Value loss: 0.043293. Entropy: 0.316046.\n",
      "episode: 1279   score: 180.0  epsilon: 1.0    steps: 80  evaluation reward: 236.15\n",
      "episode: 1280   score: 470.0  epsilon: 1.0    steps: 872  evaluation reward: 238.3\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3061: Policy loss: 0.380123. Value loss: 0.094971. Entropy: 0.318348.\n",
      "Iteration 3062: Policy loss: 0.383703. Value loss: 0.029448. Entropy: 0.316083.\n",
      "Iteration 3063: Policy loss: 0.368780. Value loss: 0.021133. Entropy: 0.317078.\n",
      "episode: 1281   score: 180.0  epsilon: 1.0    steps: 304  evaluation reward: 238.0\n",
      "episode: 1282   score: 75.0  epsilon: 1.0    steps: 528  evaluation reward: 235.35\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3064: Policy loss: -0.092212. Value loss: 0.105460. Entropy: 0.315811.\n",
      "Iteration 3065: Policy loss: -0.102429. Value loss: 0.051511. Entropy: 0.315090.\n",
      "Iteration 3066: Policy loss: -0.103566. Value loss: 0.040018. Entropy: 0.315935.\n",
      "episode: 1283   score: 210.0  epsilon: 1.0    steps: 368  evaluation reward: 232.45\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3067: Policy loss: 0.168064. Value loss: 0.106207. Entropy: 0.315143.\n",
      "Iteration 3068: Policy loss: 0.162946. Value loss: 0.052632. Entropy: 0.313121.\n",
      "Iteration 3069: Policy loss: 0.162616. Value loss: 0.035956. Entropy: 0.312316.\n",
      "episode: 1284   score: 185.0  epsilon: 1.0    steps: 576  evaluation reward: 229.3\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3070: Policy loss: -0.055930. Value loss: 0.043054. Entropy: 0.310345.\n",
      "Iteration 3071: Policy loss: -0.053330. Value loss: 0.021323. Entropy: 0.310450.\n",
      "Iteration 3072: Policy loss: -0.056300. Value loss: 0.018884. Entropy: 0.310008.\n",
      "episode: 1285   score: 285.0  epsilon: 1.0    steps: 88  evaluation reward: 230.05\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3073: Policy loss: 0.109053. Value loss: 0.030599. Entropy: 0.314475.\n",
      "Iteration 3074: Policy loss: 0.106726. Value loss: 0.014945. Entropy: 0.314447.\n",
      "Iteration 3075: Policy loss: 0.102567. Value loss: 0.011562. Entropy: 0.315044.\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3076: Policy loss: 0.071301. Value loss: 0.087622. Entropy: 0.310724.\n",
      "Iteration 3077: Policy loss: 0.064147. Value loss: 0.041630. Entropy: 0.310972.\n",
      "Iteration 3078: Policy loss: 0.059265. Value loss: 0.024353. Entropy: 0.310374.\n",
      "episode: 1286   score: 310.0  epsilon: 1.0    steps: 64  evaluation reward: 232.65\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3079: Policy loss: -0.106336. Value loss: 0.081212. Entropy: 0.314977.\n",
      "Iteration 3080: Policy loss: -0.115618. Value loss: 0.035830. Entropy: 0.314853.\n",
      "Iteration 3081: Policy loss: -0.113928. Value loss: 0.027015. Entropy: 0.314547.\n",
      "episode: 1287   score: 210.0  epsilon: 1.0    steps: 256  evaluation reward: 233.65\n",
      "episode: 1288   score: 210.0  epsilon: 1.0    steps: 920  evaluation reward: 233.65\n",
      "episode: 1289   score: 285.0  epsilon: 1.0    steps: 1016  evaluation reward: 235.7\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3082: Policy loss: -0.084756. Value loss: 0.121779. Entropy: 0.310285.\n",
      "Iteration 3083: Policy loss: -0.090278. Value loss: 0.063273. Entropy: 0.312882.\n",
      "Iteration 3084: Policy loss: -0.087120. Value loss: 0.054000. Entropy: 0.312342.\n",
      "episode: 1290   score: 210.0  epsilon: 1.0    steps: 688  evaluation reward: 236.65\n",
      "episode: 1291   score: 110.0  epsilon: 1.0    steps: 736  evaluation reward: 235.95\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3085: Policy loss: 0.130182. Value loss: 0.060701. Entropy: 0.310672.\n",
      "Iteration 3086: Policy loss: 0.126904. Value loss: 0.035190. Entropy: 0.309146.\n",
      "Iteration 3087: Policy loss: 0.122574. Value loss: 0.029179. Entropy: 0.308785.\n",
      "episode: 1292   score: 330.0  epsilon: 1.0    steps: 640  evaluation reward: 237.45\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3088: Policy loss: -0.031111. Value loss: 0.052925. Entropy: 0.311805.\n",
      "Iteration 3089: Policy loss: -0.034626. Value loss: 0.027679. Entropy: 0.315854.\n",
      "Iteration 3090: Policy loss: -0.035629. Value loss: 0.022183. Entropy: 0.314747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3091: Policy loss: -0.004307. Value loss: 0.029337. Entropy: 0.311855.\n",
      "Iteration 3092: Policy loss: -0.007361. Value loss: 0.016709. Entropy: 0.310357.\n",
      "Iteration 3093: Policy loss: -0.010134. Value loss: 0.010378. Entropy: 0.310828.\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3094: Policy loss: -0.001379. Value loss: 0.054047. Entropy: 0.314098.\n",
      "Iteration 3095: Policy loss: -0.007095. Value loss: 0.022039. Entropy: 0.311336.\n",
      "Iteration 3096: Policy loss: -0.009638. Value loss: 0.014217. Entropy: 0.312770.\n",
      "episode: 1293   score: 265.0  epsilon: 1.0    steps: 184  evaluation reward: 238.85\n",
      "episode: 1294   score: 110.0  epsilon: 1.0    steps: 352  evaluation reward: 236.9\n",
      "episode: 1295   score: 180.0  epsilon: 1.0    steps: 360  evaluation reward: 236.6\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3097: Policy loss: -0.193539. Value loss: 0.163650. Entropy: 0.308770.\n",
      "Iteration 3098: Policy loss: -0.226664. Value loss: 0.054473. Entropy: 0.311085.\n",
      "Iteration 3099: Policy loss: -0.240183. Value loss: 0.033129. Entropy: 0.309929.\n",
      "Training network. lr: 0.000227. clip: 0.090637\n",
      "Iteration 3100: Policy loss: 0.062204. Value loss: 0.048969. Entropy: 0.316640.\n",
      "Iteration 3101: Policy loss: 0.053547. Value loss: 0.026100. Entropy: 0.317388.\n",
      "Iteration 3102: Policy loss: 0.054818. Value loss: 0.020887. Entropy: 0.316288.\n",
      "episode: 1296   score: 210.0  epsilon: 1.0    steps: 208  evaluation reward: 235.95\n",
      "episode: 1297   score: 380.0  epsilon: 1.0    steps: 984  evaluation reward: 238.2\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3103: Policy loss: 0.132342. Value loss: 0.069455. Entropy: 0.307227.\n",
      "Iteration 3104: Policy loss: 0.128938. Value loss: 0.042956. Entropy: 0.308655.\n",
      "Iteration 3105: Policy loss: 0.129543. Value loss: 0.029581. Entropy: 0.308546.\n",
      "episode: 1298   score: 180.0  epsilon: 1.0    steps: 672  evaluation reward: 238.45\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3106: Policy loss: 0.165573. Value loss: 0.039874. Entropy: 0.311871.\n",
      "Iteration 3107: Policy loss: 0.164318. Value loss: 0.017073. Entropy: 0.310765.\n",
      "Iteration 3108: Policy loss: 0.163819. Value loss: 0.014903. Entropy: 0.310578.\n",
      "episode: 1299   score: 180.0  epsilon: 1.0    steps: 304  evaluation reward: 237.4\n",
      "episode: 1300   score: 265.0  epsilon: 1.0    steps: 400  evaluation reward: 238.85\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3109: Policy loss: -0.049584. Value loss: 0.063382. Entropy: 0.306823.\n",
      "Iteration 3110: Policy loss: -0.060333. Value loss: 0.024030. Entropy: 0.305272.\n",
      "Iteration 3111: Policy loss: -0.058065. Value loss: 0.020368. Entropy: 0.305970.\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3112: Policy loss: 0.011009. Value loss: 0.041195. Entropy: 0.305872.\n",
      "Iteration 3113: Policy loss: 0.006928. Value loss: 0.016757. Entropy: 0.305426.\n",
      "Iteration 3114: Policy loss: 0.007429. Value loss: 0.010211. Entropy: 0.305843.\n",
      "now time :  2019-09-05 17:28:38.891114\n",
      "episode: 1301   score: 155.0  epsilon: 1.0    steps: 336  evaluation reward: 238.15\n",
      "episode: 1302   score: 210.0  epsilon: 1.0    steps: 624  evaluation reward: 238.7\n",
      "episode: 1303   score: 180.0  epsilon: 1.0    steps: 664  evaluation reward: 239.45\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3115: Policy loss: 0.133565. Value loss: 0.078732. Entropy: 0.305862.\n",
      "Iteration 3116: Policy loss: 0.129761. Value loss: 0.027265. Entropy: 0.306260.\n",
      "Iteration 3117: Policy loss: 0.127054. Value loss: 0.023407. Entropy: 0.306011.\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3118: Policy loss: -0.010722. Value loss: 0.051547. Entropy: 0.304357.\n",
      "Iteration 3119: Policy loss: -0.010290. Value loss: 0.025412. Entropy: 0.305215.\n",
      "Iteration 3120: Policy loss: -0.010351. Value loss: 0.019750. Entropy: 0.304828.\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3121: Policy loss: -0.149687. Value loss: 0.066882. Entropy: 0.307418.\n",
      "Iteration 3122: Policy loss: -0.151531. Value loss: 0.033822. Entropy: 0.307974.\n",
      "Iteration 3123: Policy loss: -0.152497. Value loss: 0.027980. Entropy: 0.308905.\n",
      "episode: 1304   score: 120.0  epsilon: 1.0    steps: 480  evaluation reward: 238.85\n",
      "episode: 1305   score: 110.0  epsilon: 1.0    steps: 744  evaluation reward: 237.85\n",
      "episode: 1306   score: 110.0  epsilon: 1.0    steps: 752  evaluation reward: 236.35\n",
      "episode: 1307   score: 240.0  epsilon: 1.0    steps: 936  evaluation reward: 234.65\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3124: Policy loss: 0.155728. Value loss: 0.069359. Entropy: 0.308680.\n",
      "Iteration 3125: Policy loss: 0.147483. Value loss: 0.027477. Entropy: 0.307949.\n",
      "Iteration 3126: Policy loss: 0.155401. Value loss: 0.020179. Entropy: 0.309176.\n",
      "episode: 1308   score: 285.0  epsilon: 1.0    steps: 552  evaluation reward: 233.6\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3127: Policy loss: -0.039571. Value loss: 0.048932. Entropy: 0.312254.\n",
      "Iteration 3128: Policy loss: -0.042986. Value loss: 0.024209. Entropy: 0.308627.\n",
      "Iteration 3129: Policy loss: -0.046450. Value loss: 0.019627. Entropy: 0.309898.\n",
      "episode: 1309   score: 245.0  epsilon: 1.0    steps: 536  evaluation reward: 233.8\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3130: Policy loss: -0.041225. Value loss: 0.159800. Entropy: 0.310792.\n",
      "Iteration 3131: Policy loss: -0.028687. Value loss: 0.084852. Entropy: 0.310958.\n",
      "Iteration 3132: Policy loss: -0.041354. Value loss: 0.060112. Entropy: 0.311790.\n",
      "episode: 1310   score: 110.0  epsilon: 1.0    steps: 720  evaluation reward: 232.8\n",
      "episode: 1311   score: 210.0  epsilon: 1.0    steps: 960  evaluation reward: 233.55\n",
      "episode: 1312   score: 380.0  epsilon: 1.0    steps: 976  evaluation reward: 235.25\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3133: Policy loss: 0.022959. Value loss: 0.039848. Entropy: 0.312472.\n",
      "Iteration 3134: Policy loss: 0.016550. Value loss: 0.024461. Entropy: 0.311377.\n",
      "Iteration 3135: Policy loss: 0.012938. Value loss: 0.018479. Entropy: 0.312298.\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3136: Policy loss: -0.075398. Value loss: 0.047956. Entropy: 0.313443.\n",
      "Iteration 3137: Policy loss: -0.078886. Value loss: 0.030420. Entropy: 0.314192.\n",
      "Iteration 3138: Policy loss: -0.083252. Value loss: 0.025569. Entropy: 0.313925.\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3139: Policy loss: -0.305097. Value loss: 0.151252. Entropy: 0.313354.\n",
      "Iteration 3140: Policy loss: -0.324831. Value loss: 0.068438. Entropy: 0.311165.\n",
      "Iteration 3141: Policy loss: -0.337745. Value loss: 0.055064. Entropy: 0.313688.\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3142: Policy loss: -0.631028. Value loss: 0.241186. Entropy: 0.311468.\n",
      "Iteration 3143: Policy loss: -0.637182. Value loss: 0.090831. Entropy: 0.311443.\n",
      "Iteration 3144: Policy loss: -0.635748. Value loss: 0.036220. Entropy: 0.309910.\n",
      "episode: 1313   score: 105.0  epsilon: 1.0    steps: 376  evaluation reward: 234.75\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3145: Policy loss: 0.204639. Value loss: 0.104708. Entropy: 0.312278.\n",
      "Iteration 3146: Policy loss: 0.187700. Value loss: 0.029728. Entropy: 0.313085.\n",
      "Iteration 3147: Policy loss: 0.187562. Value loss: 0.023544. Entropy: 0.311521.\n",
      "episode: 1314   score: 210.0  epsilon: 1.0    steps: 72  evaluation reward: 235.3\n",
      "episode: 1315   score: 460.0  epsilon: 1.0    steps: 144  evaluation reward: 238.75\n",
      "episode: 1316   score: 285.0  epsilon: 1.0    steps: 568  evaluation reward: 240.7\n",
      "episode: 1317   score: 465.0  epsilon: 1.0    steps: 672  evaluation reward: 242.5\n",
      "episode: 1318   score: 155.0  epsilon: 1.0    steps: 912  evaluation reward: 241.65\n",
      "Training network. lr: 0.000226. clip: 0.090480\n",
      "Iteration 3148: Policy loss: 0.195706. Value loss: 0.117353. Entropy: 0.313395.\n",
      "Iteration 3149: Policy loss: 0.176037. Value loss: 0.045426. Entropy: 0.312785.\n",
      "Iteration 3150: Policy loss: 0.187056. Value loss: 0.041370. Entropy: 0.312709.\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3151: Policy loss: 0.030764. Value loss: 0.082034. Entropy: 0.308713.\n",
      "Iteration 3152: Policy loss: 0.026348. Value loss: 0.037193. Entropy: 0.308552.\n",
      "Iteration 3153: Policy loss: 0.025110. Value loss: 0.028479. Entropy: 0.309792.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1319   score: 180.0  epsilon: 1.0    steps: 264  evaluation reward: 238.0\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3154: Policy loss: 0.133795. Value loss: 0.046245. Entropy: 0.309546.\n",
      "Iteration 3155: Policy loss: 0.126200. Value loss: 0.022030. Entropy: 0.308473.\n",
      "Iteration 3156: Policy loss: 0.127240. Value loss: 0.018926. Entropy: 0.308964.\n",
      "episode: 1320   score: 265.0  epsilon: 1.0    steps: 320  evaluation reward: 238.55\n",
      "episode: 1321   score: 105.0  epsilon: 1.0    steps: 552  evaluation reward: 236.75\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3157: Policy loss: 0.111420. Value loss: 0.048483. Entropy: 0.312509.\n",
      "Iteration 3158: Policy loss: 0.108330. Value loss: 0.022803. Entropy: 0.312198.\n",
      "Iteration 3159: Policy loss: 0.101693. Value loss: 0.017869. Entropy: 0.312167.\n",
      "episode: 1322   score: 120.0  epsilon: 1.0    steps: 176  evaluation reward: 236.15\n",
      "episode: 1323   score: 155.0  epsilon: 1.0    steps: 688  evaluation reward: 236.95\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3160: Policy loss: -0.050038. Value loss: 0.059877. Entropy: 0.307407.\n",
      "Iteration 3161: Policy loss: -0.051788. Value loss: 0.031208. Entropy: 0.306164.\n",
      "Iteration 3162: Policy loss: -0.046660. Value loss: 0.023477. Entropy: 0.305805.\n",
      "episode: 1324   score: 50.0  epsilon: 1.0    steps: 472  evaluation reward: 235.35\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3163: Policy loss: -0.194269. Value loss: 0.209102. Entropy: 0.307861.\n",
      "Iteration 3164: Policy loss: -0.219097. Value loss: 0.088407. Entropy: 0.307895.\n",
      "Iteration 3165: Policy loss: -0.224166. Value loss: 0.038080. Entropy: 0.308577.\n",
      "episode: 1325   score: 110.0  epsilon: 1.0    steps: 120  evaluation reward: 234.65\n",
      "episode: 1326   score: 410.0  epsilon: 1.0    steps: 464  evaluation reward: 236.65\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3166: Policy loss: -0.069987. Value loss: 0.039819. Entropy: 0.310594.\n",
      "Iteration 3167: Policy loss: -0.074137. Value loss: 0.018959. Entropy: 0.311147.\n",
      "Iteration 3168: Policy loss: -0.075515. Value loss: 0.014292. Entropy: 0.310914.\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3169: Policy loss: -0.245106. Value loss: 0.237072. Entropy: 0.310091.\n",
      "Iteration 3170: Policy loss: -0.259351. Value loss: 0.075640. Entropy: 0.311189.\n",
      "Iteration 3171: Policy loss: -0.243109. Value loss: 0.048081. Entropy: 0.311403.\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3172: Policy loss: -0.073755. Value loss: 0.135715. Entropy: 0.304931.\n",
      "Iteration 3173: Policy loss: -0.092016. Value loss: 0.060458. Entropy: 0.307309.\n",
      "Iteration 3174: Policy loss: -0.084816. Value loss: 0.041931. Entropy: 0.307031.\n",
      "episode: 1327   score: 520.0  epsilon: 1.0    steps: 8  evaluation reward: 239.25\n",
      "episode: 1328   score: 105.0  epsilon: 1.0    steps: 536  evaluation reward: 235.6\n",
      "episode: 1329   score: 180.0  epsilon: 1.0    steps: 896  evaluation reward: 235.6\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3175: Policy loss: 0.116034. Value loss: 0.057314. Entropy: 0.300019.\n",
      "Iteration 3176: Policy loss: 0.116079. Value loss: 0.021444. Entropy: 0.301519.\n",
      "Iteration 3177: Policy loss: 0.113542. Value loss: 0.016241. Entropy: 0.304587.\n",
      "episode: 1330   score: 180.0  epsilon: 1.0    steps: 600  evaluation reward: 233.5\n",
      "episode: 1331   score: 260.0  epsilon: 1.0    steps: 960  evaluation reward: 230.6\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3178: Policy loss: 0.200177. Value loss: 0.098527. Entropy: 0.309238.\n",
      "Iteration 3179: Policy loss: 0.192743. Value loss: 0.045886. Entropy: 0.310429.\n",
      "Iteration 3180: Policy loss: 0.188980. Value loss: 0.037681. Entropy: 0.309519.\n",
      "episode: 1332   score: 210.0  epsilon: 1.0    steps: 992  evaluation reward: 230.6\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3181: Policy loss: 0.164800. Value loss: 0.105175. Entropy: 0.309707.\n",
      "Iteration 3182: Policy loss: 0.161995. Value loss: 0.049357. Entropy: 0.310403.\n",
      "Iteration 3183: Policy loss: 0.160803. Value loss: 0.040974. Entropy: 0.309801.\n",
      "episode: 1333   score: 460.0  epsilon: 1.0    steps: 152  evaluation reward: 232.8\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3184: Policy loss: 0.172686. Value loss: 0.094088. Entropy: 0.309695.\n",
      "Iteration 3185: Policy loss: 0.177112. Value loss: 0.041285. Entropy: 0.309780.\n",
      "Iteration 3186: Policy loss: 0.169433. Value loss: 0.032713. Entropy: 0.309512.\n",
      "episode: 1334   score: 210.0  epsilon: 1.0    steps: 80  evaluation reward: 234.4\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3187: Policy loss: 0.103091. Value loss: 0.075189. Entropy: 0.307962.\n",
      "Iteration 3188: Policy loss: 0.101064. Value loss: 0.036355. Entropy: 0.307886.\n",
      "Iteration 3189: Policy loss: 0.093312. Value loss: 0.024200. Entropy: 0.307953.\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3190: Policy loss: 0.061893. Value loss: 0.098888. Entropy: 0.308025.\n",
      "Iteration 3191: Policy loss: 0.058678. Value loss: 0.041858. Entropy: 0.307667.\n",
      "Iteration 3192: Policy loss: 0.052701. Value loss: 0.022656. Entropy: 0.307394.\n",
      "episode: 1335   score: 155.0  epsilon: 1.0    steps: 312  evaluation reward: 234.15\n",
      "episode: 1336   score: 120.0  epsilon: 1.0    steps: 456  evaluation reward: 233.25\n",
      "episode: 1337   score: 110.0  epsilon: 1.0    steps: 688  evaluation reward: 232.25\n",
      "episode: 1338   score: 155.0  epsilon: 1.0    steps: 832  evaluation reward: 231.7\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3193: Policy loss: 0.374664. Value loss: 0.208096. Entropy: 0.308197.\n",
      "Iteration 3194: Policy loss: 0.350864. Value loss: 0.064759. Entropy: 0.308009.\n",
      "Iteration 3195: Policy loss: 0.334163. Value loss: 0.036545. Entropy: 0.308140.\n",
      "episode: 1339   score: 210.0  epsilon: 1.0    steps: 8  evaluation reward: 230.95\n",
      "episode: 1340   score: 110.0  epsilon: 1.0    steps: 448  evaluation reward: 229.95\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3196: Policy loss: 0.111407. Value loss: 0.076336. Entropy: 0.305648.\n",
      "Iteration 3197: Policy loss: 0.111713. Value loss: 0.032366. Entropy: 0.306029.\n",
      "Iteration 3198: Policy loss: 0.106361. Value loss: 0.027874. Entropy: 0.305657.\n",
      "episode: 1341   score: 135.0  epsilon: 1.0    steps: 504  evaluation reward: 230.25\n",
      "episode: 1342   score: 80.0  epsilon: 1.0    steps: 768  evaluation reward: 230.25\n",
      "Training network. lr: 0.000226. clip: 0.090332\n",
      "Iteration 3199: Policy loss: 0.307998. Value loss: 0.060983. Entropy: 0.309313.\n",
      "Iteration 3200: Policy loss: 0.305421. Value loss: 0.031931. Entropy: 0.307803.\n",
      "Iteration 3201: Policy loss: 0.307495. Value loss: 0.019202. Entropy: 0.308263.\n",
      "episode: 1343   score: 45.0  epsilon: 1.0    steps: 32  evaluation reward: 228.6\n",
      "episode: 1344   score: 80.0  epsilon: 1.0    steps: 808  evaluation reward: 223.9\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3202: Policy loss: -0.027755. Value loss: 0.075578. Entropy: 0.307074.\n",
      "Iteration 3203: Policy loss: -0.034174. Value loss: 0.046312. Entropy: 0.306960.\n",
      "Iteration 3204: Policy loss: -0.034741. Value loss: 0.029510. Entropy: 0.306234.\n",
      "episode: 1345   score: 125.0  epsilon: 1.0    steps: 232  evaluation reward: 222.3\n",
      "episode: 1346   score: 50.0  epsilon: 1.0    steps: 720  evaluation reward: 217.9\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3205: Policy loss: 0.073282. Value loss: 0.044105. Entropy: 0.307040.\n",
      "Iteration 3206: Policy loss: 0.068952. Value loss: 0.022806. Entropy: 0.307009.\n",
      "Iteration 3207: Policy loss: 0.070300. Value loss: 0.015414. Entropy: 0.306712.\n",
      "episode: 1347   score: 100.0  epsilon: 1.0    steps: 8  evaluation reward: 217.55\n",
      "episode: 1348   score: 115.0  epsilon: 1.0    steps: 904  evaluation reward: 216.9\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3208: Policy loss: -0.031552. Value loss: 0.058736. Entropy: 0.308432.\n",
      "Iteration 3209: Policy loss: -0.040655. Value loss: 0.031276. Entropy: 0.309039.\n",
      "Iteration 3210: Policy loss: -0.038164. Value loss: 0.021238. Entropy: 0.308847.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3211: Policy loss: 0.010259. Value loss: 0.059715. Entropy: 0.309076.\n",
      "Iteration 3212: Policy loss: 0.008544. Value loss: 0.022129. Entropy: 0.309002.\n",
      "Iteration 3213: Policy loss: 0.003605. Value loss: 0.015712. Entropy: 0.308849.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1349   score: 380.0  epsilon: 1.0    steps: 712  evaluation reward: 216.9\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3214: Policy loss: -0.099421. Value loss: 0.272553. Entropy: 0.309435.\n",
      "Iteration 3215: Policy loss: -0.114367. Value loss: 0.210854. Entropy: 0.308441.\n",
      "Iteration 3216: Policy loss: -0.101748. Value loss: 0.147264. Entropy: 0.309089.\n",
      "episode: 1350   score: 110.0  epsilon: 1.0    steps: 184  evaluation reward: 213.3\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3217: Policy loss: -0.029500. Value loss: 0.061596. Entropy: 0.300521.\n",
      "Iteration 3218: Policy loss: -0.034818. Value loss: 0.023173. Entropy: 0.303238.\n",
      "Iteration 3219: Policy loss: -0.036024. Value loss: 0.017114. Entropy: 0.303364.\n",
      "now time :  2019-09-05 17:35:09.979882\n",
      "episode: 1351   score: 120.0  epsilon: 1.0    steps: 296  evaluation reward: 212.85\n",
      "episode: 1352   score: 120.0  epsilon: 1.0    steps: 528  evaluation reward: 213.25\n",
      "episode: 1353   score: 110.0  epsilon: 1.0    steps: 824  evaluation reward: 211.65\n",
      "episode: 1354   score: 105.0  epsilon: 1.0    steps: 848  evaluation reward: 210.6\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3220: Policy loss: 0.046388. Value loss: 0.053606. Entropy: 0.312350.\n",
      "Iteration 3221: Policy loss: 0.040381. Value loss: 0.030731. Entropy: 0.312195.\n",
      "Iteration 3222: Policy loss: 0.041069. Value loss: 0.022770. Entropy: 0.311829.\n",
      "episode: 1355   score: 255.0  epsilon: 1.0    steps: 296  evaluation reward: 211.35\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3223: Policy loss: -0.119607. Value loss: 0.059989. Entropy: 0.305627.\n",
      "Iteration 3224: Policy loss: -0.128226. Value loss: 0.029107. Entropy: 0.305410.\n",
      "Iteration 3225: Policy loss: -0.126131. Value loss: 0.023700. Entropy: 0.305984.\n",
      "episode: 1356   score: 135.0  epsilon: 1.0    steps: 48  evaluation reward: 208.1\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3226: Policy loss: -0.037279. Value loss: 0.043002. Entropy: 0.306926.\n",
      "Iteration 3227: Policy loss: -0.040263. Value loss: 0.014636. Entropy: 0.307203.\n",
      "Iteration 3228: Policy loss: -0.038134. Value loss: 0.009423. Entropy: 0.307171.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3229: Policy loss: -0.086049. Value loss: 0.056401. Entropy: 0.305716.\n",
      "Iteration 3230: Policy loss: -0.095706. Value loss: 0.022571. Entropy: 0.305421.\n",
      "Iteration 3231: Policy loss: -0.091102. Value loss: 0.017161. Entropy: 0.305686.\n",
      "episode: 1357   score: 165.0  epsilon: 1.0    steps: 992  evaluation reward: 206.5\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3232: Policy loss: -0.536771. Value loss: 0.288246. Entropy: 0.305246.\n",
      "Iteration 3233: Policy loss: -0.565316. Value loss: 0.115660. Entropy: 0.305424.\n",
      "Iteration 3234: Policy loss: -0.532988. Value loss: 0.066072. Entropy: 0.306512.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3235: Policy loss: -0.045425. Value loss: 0.093769. Entropy: 0.307797.\n",
      "Iteration 3236: Policy loss: -0.050415. Value loss: 0.034335. Entropy: 0.308104.\n",
      "Iteration 3237: Policy loss: -0.054021. Value loss: 0.024222. Entropy: 0.308397.\n",
      "episode: 1358   score: 240.0  epsilon: 1.0    steps: 712  evaluation reward: 204.8\n",
      "episode: 1359   score: 210.0  epsilon: 1.0    steps: 776  evaluation reward: 205.7\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3238: Policy loss: -0.031301. Value loss: 0.119236. Entropy: 0.301220.\n",
      "Iteration 3239: Policy loss: -0.042740. Value loss: 0.055248. Entropy: 0.302332.\n",
      "Iteration 3240: Policy loss: -0.045347. Value loss: 0.043208. Entropy: 0.301097.\n",
      "episode: 1360   score: 210.0  epsilon: 1.0    steps: 320  evaluation reward: 206.0\n",
      "episode: 1361   score: 215.0  epsilon: 1.0    steps: 776  evaluation reward: 206.35\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3241: Policy loss: -0.019660. Value loss: 0.075914. Entropy: 0.303661.\n",
      "Iteration 3242: Policy loss: -0.026096. Value loss: 0.033809. Entropy: 0.303614.\n",
      "Iteration 3243: Policy loss: -0.023909. Value loss: 0.023848. Entropy: 0.302505.\n",
      "episode: 1362   score: 210.0  epsilon: 1.0    steps: 448  evaluation reward: 206.65\n",
      "episode: 1363   score: 450.0  epsilon: 1.0    steps: 744  evaluation reward: 207.35\n",
      "episode: 1364   score: 260.0  epsilon: 1.0    steps: 992  evaluation reward: 207.85\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3244: Policy loss: 0.185166. Value loss: 0.077847. Entropy: 0.306162.\n",
      "Iteration 3245: Policy loss: 0.183511. Value loss: 0.035977. Entropy: 0.305427.\n",
      "Iteration 3246: Policy loss: 0.188088. Value loss: 0.026177. Entropy: 0.304483.\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3247: Policy loss: 0.105687. Value loss: 0.049710. Entropy: 0.303201.\n",
      "Iteration 3248: Policy loss: 0.097084. Value loss: 0.021861. Entropy: 0.303810.\n",
      "Iteration 3249: Policy loss: 0.097166. Value loss: 0.018391. Entropy: 0.302316.\n",
      "episode: 1365   score: 50.0  epsilon: 1.0    steps: 624  evaluation reward: 205.75\n",
      "Training network. lr: 0.000225. clip: 0.090176\n",
      "Iteration 3250: Policy loss: -0.087900. Value loss: 0.059872. Entropy: 0.304794.\n",
      "Iteration 3251: Policy loss: -0.087584. Value loss: 0.027066. Entropy: 0.304970.\n",
      "Iteration 3252: Policy loss: -0.097451. Value loss: 0.019329. Entropy: 0.304899.\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3253: Policy loss: -0.176713. Value loss: 0.190520. Entropy: 0.306901.\n",
      "Iteration 3254: Policy loss: -0.199851. Value loss: 0.059518. Entropy: 0.307274.\n",
      "Iteration 3255: Policy loss: -0.227418. Value loss: 0.029777. Entropy: 0.307591.\n",
      "episode: 1366   score: 260.0  epsilon: 1.0    steps: 448  evaluation reward: 207.25\n",
      "episode: 1367   score: 165.0  epsilon: 1.0    steps: 1016  evaluation reward: 207.1\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3256: Policy loss: 0.002405. Value loss: 0.048164. Entropy: 0.307964.\n",
      "Iteration 3257: Policy loss: 0.005886. Value loss: 0.024153. Entropy: 0.306860.\n",
      "Iteration 3258: Policy loss: 0.001821. Value loss: 0.018461. Entropy: 0.306244.\n",
      "episode: 1368   score: 410.0  epsilon: 1.0    steps: 592  evaluation reward: 208.5\n",
      "episode: 1369   score: 210.0  epsilon: 1.0    steps: 664  evaluation reward: 209.5\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3259: Policy loss: 0.087676. Value loss: 0.123801. Entropy: 0.308086.\n",
      "Iteration 3260: Policy loss: 0.066977. Value loss: 0.037970. Entropy: 0.306522.\n",
      "Iteration 3261: Policy loss: 0.059971. Value loss: 0.022619. Entropy: 0.307573.\n",
      "episode: 1370   score: 180.0  epsilon: 1.0    steps: 872  evaluation reward: 209.2\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3262: Policy loss: 0.024937. Value loss: 0.070769. Entropy: 0.308963.\n",
      "Iteration 3263: Policy loss: 0.024474. Value loss: 0.042491. Entropy: 0.309213.\n",
      "Iteration 3264: Policy loss: 0.026896. Value loss: 0.034066. Entropy: 0.307905.\n",
      "episode: 1371   score: 180.0  epsilon: 1.0    steps: 72  evaluation reward: 208.4\n",
      "episode: 1372   score: 110.0  epsilon: 1.0    steps: 760  evaluation reward: 207.4\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3265: Policy loss: 0.021941. Value loss: 0.107208. Entropy: 0.310583.\n",
      "Iteration 3266: Policy loss: 0.009613. Value loss: 0.043696. Entropy: 0.308939.\n",
      "Iteration 3267: Policy loss: 0.001811. Value loss: 0.032519. Entropy: 0.309877.\n",
      "episode: 1373   score: 105.0  epsilon: 1.0    steps: 768  evaluation reward: 205.5\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3268: Policy loss: -0.021568. Value loss: 0.068577. Entropy: 0.309785.\n",
      "Iteration 3269: Policy loss: -0.029560. Value loss: 0.037310. Entropy: 0.309923.\n",
      "Iteration 3270: Policy loss: -0.033072. Value loss: 0.026995. Entropy: 0.309367.\n",
      "episode: 1374   score: 210.0  epsilon: 1.0    steps: 136  evaluation reward: 206.55\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3271: Policy loss: -0.029932. Value loss: 0.081583. Entropy: 0.307468.\n",
      "Iteration 3272: Policy loss: -0.031397. Value loss: 0.040720. Entropy: 0.306978.\n",
      "Iteration 3273: Policy loss: -0.037449. Value loss: 0.022831. Entropy: 0.307377.\n",
      "episode: 1375   score: 110.0  epsilon: 1.0    steps: 632  evaluation reward: 202.5\n",
      "episode: 1376   score: 330.0  epsilon: 1.0    steps: 856  evaluation reward: 200.8\n",
      "episode: 1377   score: 105.0  epsilon: 1.0    steps: 872  evaluation reward: 200.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3274: Policy loss: 0.300113. Value loss: 0.070862. Entropy: 0.304554.\n",
      "Iteration 3275: Policy loss: 0.294186. Value loss: 0.026119. Entropy: 0.303051.\n",
      "Iteration 3276: Policy loss: 0.289028. Value loss: 0.020910. Entropy: 0.302737.\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3277: Policy loss: 0.078076. Value loss: 0.034719. Entropy: 0.312003.\n",
      "Iteration 3278: Policy loss: 0.080106. Value loss: 0.019772. Entropy: 0.311689.\n",
      "Iteration 3279: Policy loss: 0.075565. Value loss: 0.015146. Entropy: 0.311672.\n",
      "episode: 1378   score: 180.0  epsilon: 1.0    steps: 216  evaluation reward: 200.65\n",
      "episode: 1379   score: 75.0  epsilon: 1.0    steps: 248  evaluation reward: 199.6\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3280: Policy loss: -0.019949. Value loss: 0.061797. Entropy: 0.311217.\n",
      "Iteration 3281: Policy loss: -0.026405. Value loss: 0.033296. Entropy: 0.311893.\n",
      "Iteration 3282: Policy loss: -0.026488. Value loss: 0.024774. Entropy: 0.311062.\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3283: Policy loss: -0.059199. Value loss: 0.051868. Entropy: 0.315428.\n",
      "Iteration 3284: Policy loss: -0.058878. Value loss: 0.027577. Entropy: 0.314966.\n",
      "Iteration 3285: Policy loss: -0.062275. Value loss: 0.020566. Entropy: 0.315096.\n",
      "episode: 1380   score: 260.0  epsilon: 1.0    steps: 560  evaluation reward: 197.5\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3286: Policy loss: 0.026153. Value loss: 0.037821. Entropy: 0.305958.\n",
      "Iteration 3287: Policy loss: 0.030317. Value loss: 0.019583. Entropy: 0.306418.\n",
      "Iteration 3288: Policy loss: 0.023396. Value loss: 0.014382. Entropy: 0.304809.\n",
      "episode: 1381   score: 210.0  epsilon: 1.0    steps: 176  evaluation reward: 197.8\n",
      "episode: 1382   score: 110.0  epsilon: 1.0    steps: 888  evaluation reward: 198.15\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3289: Policy loss: -0.531006. Value loss: 0.333586. Entropy: 0.310657.\n",
      "Iteration 3290: Policy loss: -0.527307. Value loss: 0.136721. Entropy: 0.311164.\n",
      "Iteration 3291: Policy loss: -0.524528. Value loss: 0.098486. Entropy: 0.311060.\n",
      "episode: 1383   score: 185.0  epsilon: 1.0    steps: 64  evaluation reward: 197.9\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3292: Policy loss: -0.253017. Value loss: 0.241027. Entropy: 0.304604.\n",
      "Iteration 3293: Policy loss: -0.288480. Value loss: 0.094634. Entropy: 0.304802.\n",
      "Iteration 3294: Policy loss: -0.282115. Value loss: 0.065061. Entropy: 0.305417.\n",
      "episode: 1384   score: 140.0  epsilon: 1.0    steps: 536  evaluation reward: 197.45\n",
      "episode: 1385   score: 365.0  epsilon: 1.0    steps: 904  evaluation reward: 198.25\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3295: Policy loss: -0.065476. Value loss: 0.104049. Entropy: 0.310011.\n",
      "Iteration 3296: Policy loss: -0.076787. Value loss: 0.043613. Entropy: 0.308605.\n",
      "Iteration 3297: Policy loss: -0.086690. Value loss: 0.032853. Entropy: 0.308350.\n",
      "Training network. lr: 0.000225. clip: 0.090019\n",
      "Iteration 3298: Policy loss: 0.067553. Value loss: 0.144795. Entropy: 0.307307.\n",
      "Iteration 3299: Policy loss: 0.052344. Value loss: 0.041214. Entropy: 0.307762.\n",
      "Iteration 3300: Policy loss: 0.049059. Value loss: 0.027559. Entropy: 0.307732.\n",
      "episode: 1386   score: 495.0  epsilon: 1.0    steps: 248  evaluation reward: 200.1\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3301: Policy loss: 0.070236. Value loss: 0.066765. Entropy: 0.306773.\n",
      "Iteration 3302: Policy loss: 0.070048. Value loss: 0.029712. Entropy: 0.306592.\n",
      "Iteration 3303: Policy loss: 0.063166. Value loss: 0.023964. Entropy: 0.305953.\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3304: Policy loss: 0.037332. Value loss: 0.071184. Entropy: 0.308946.\n",
      "Iteration 3305: Policy loss: 0.033314. Value loss: 0.032572. Entropy: 0.308308.\n",
      "Iteration 3306: Policy loss: 0.029139. Value loss: 0.023332. Entropy: 0.308303.\n",
      "episode: 1387   score: 180.0  epsilon: 1.0    steps: 176  evaluation reward: 199.8\n",
      "episode: 1388   score: 120.0  epsilon: 1.0    steps: 272  evaluation reward: 198.9\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3307: Policy loss: -0.058034. Value loss: 0.080873. Entropy: 0.306634.\n",
      "Iteration 3308: Policy loss: -0.059866. Value loss: 0.034135. Entropy: 0.306465.\n",
      "Iteration 3309: Policy loss: -0.074129. Value loss: 0.027261. Entropy: 0.305577.\n",
      "episode: 1389   score: 240.0  epsilon: 1.0    steps: 632  evaluation reward: 198.45\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3310: Policy loss: 0.076368. Value loss: 0.101841. Entropy: 0.311642.\n",
      "Iteration 3311: Policy loss: 0.070648. Value loss: 0.038978. Entropy: 0.311241.\n",
      "Iteration 3312: Policy loss: 0.065244. Value loss: 0.028623. Entropy: 0.310115.\n",
      "episode: 1390   score: 545.0  epsilon: 1.0    steps: 264  evaluation reward: 201.8\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3313: Policy loss: -0.219835. Value loss: 0.316910. Entropy: 0.305514.\n",
      "Iteration 3314: Policy loss: -0.226836. Value loss: 0.151717. Entropy: 0.306663.\n",
      "Iteration 3315: Policy loss: -0.228662. Value loss: 0.087523. Entropy: 0.304981.\n",
      "episode: 1391   score: 75.0  epsilon: 1.0    steps: 288  evaluation reward: 201.45\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3316: Policy loss: 0.061176. Value loss: 0.061227. Entropy: 0.310717.\n",
      "Iteration 3317: Policy loss: 0.061107. Value loss: 0.033170. Entropy: 0.309107.\n",
      "Iteration 3318: Policy loss: 0.057347. Value loss: 0.026736. Entropy: 0.308958.\n",
      "episode: 1392   score: 440.0  epsilon: 1.0    steps: 520  evaluation reward: 202.55\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3319: Policy loss: 0.013444. Value loss: 0.084908. Entropy: 0.306088.\n",
      "Iteration 3320: Policy loss: 0.007563. Value loss: 0.044139. Entropy: 0.306265.\n",
      "Iteration 3321: Policy loss: 0.006308. Value loss: 0.028908. Entropy: 0.308196.\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3322: Policy loss: 0.143564. Value loss: 0.100174. Entropy: 0.308669.\n",
      "Iteration 3323: Policy loss: 0.139137. Value loss: 0.042175. Entropy: 0.309734.\n",
      "Iteration 3324: Policy loss: 0.134976. Value loss: 0.030057. Entropy: 0.308383.\n",
      "episode: 1393   score: 470.0  epsilon: 1.0    steps: 592  evaluation reward: 204.6\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3325: Policy loss: 0.099131. Value loss: 0.216488. Entropy: 0.306533.\n",
      "Iteration 3326: Policy loss: 0.060569. Value loss: 0.066436. Entropy: 0.303721.\n",
      "Iteration 3327: Policy loss: 0.052413. Value loss: 0.043501. Entropy: 0.305928.\n",
      "episode: 1394   score: 260.0  epsilon: 1.0    steps: 760  evaluation reward: 206.1\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3328: Policy loss: 0.027786. Value loss: 0.123064. Entropy: 0.309687.\n",
      "Iteration 3329: Policy loss: 0.028345. Value loss: 0.054046. Entropy: 0.310368.\n",
      "Iteration 3330: Policy loss: 0.026269. Value loss: 0.035034. Entropy: 0.310178.\n",
      "episode: 1395   score: 210.0  epsilon: 1.0    steps: 8  evaluation reward: 206.4\n",
      "episode: 1396   score: 380.0  epsilon: 1.0    steps: 424  evaluation reward: 208.1\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3331: Policy loss: 0.033202. Value loss: 0.137087. Entropy: 0.305179.\n",
      "Iteration 3332: Policy loss: 0.024430. Value loss: 0.062255. Entropy: 0.304545.\n",
      "Iteration 3333: Policy loss: 0.019839. Value loss: 0.048706. Entropy: 0.304913.\n",
      "episode: 1397   score: 285.0  epsilon: 1.0    steps: 792  evaluation reward: 207.15\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3334: Policy loss: -0.273068. Value loss: 0.145467. Entropy: 0.303652.\n",
      "Iteration 3335: Policy loss: -0.288176. Value loss: 0.081324. Entropy: 0.304570.\n",
      "Iteration 3336: Policy loss: -0.286378. Value loss: 0.063290. Entropy: 0.304205.\n",
      "episode: 1398   score: 480.0  epsilon: 1.0    steps: 136  evaluation reward: 210.15\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3337: Policy loss: 0.019803. Value loss: 0.043633. Entropy: 0.306786.\n",
      "Iteration 3338: Policy loss: 0.014515. Value loss: 0.021613. Entropy: 0.306002.\n",
      "Iteration 3339: Policy loss: 0.009702. Value loss: 0.016964. Entropy: 0.305861.\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3340: Policy loss: -0.168802. Value loss: 0.281981. Entropy: 0.302144.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3341: Policy loss: -0.154911. Value loss: 0.151528. Entropy: 0.304633.\n",
      "Iteration 3342: Policy loss: -0.189403. Value loss: 0.104835. Entropy: 0.305456.\n",
      "episode: 1399   score: 270.0  epsilon: 1.0    steps: 40  evaluation reward: 211.05\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3343: Policy loss: 0.157908. Value loss: 0.157212. Entropy: 0.301561.\n",
      "Iteration 3344: Policy loss: 0.156158. Value loss: 0.075145. Entropy: 0.300812.\n",
      "Iteration 3345: Policy loss: 0.148939. Value loss: 0.050316. Entropy: 0.301303.\n",
      "episode: 1400   score: 135.0  epsilon: 1.0    steps: 512  evaluation reward: 209.75\n",
      "now time :  2019-09-05 17:42:59.969142\n",
      "episode: 1401   score: 155.0  epsilon: 1.0    steps: 856  evaluation reward: 209.75\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3346: Policy loss: 0.128987. Value loss: 0.109393. Entropy: 0.305104.\n",
      "Iteration 3347: Policy loss: 0.116643. Value loss: 0.057488. Entropy: 0.304033.\n",
      "Iteration 3348: Policy loss: 0.116856. Value loss: 0.037642. Entropy: 0.303630.\n",
      "episode: 1402   score: 510.0  epsilon: 1.0    steps: 304  evaluation reward: 212.75\n",
      "episode: 1403   score: 320.0  epsilon: 1.0    steps: 816  evaluation reward: 214.15\n",
      "Training network. lr: 0.000225. clip: 0.089872\n",
      "Iteration 3349: Policy loss: 0.259223. Value loss: 0.145432. Entropy: 0.305490.\n",
      "Iteration 3350: Policy loss: 0.257735. Value loss: 0.095205. Entropy: 0.303633.\n",
      "Iteration 3351: Policy loss: 0.245253. Value loss: 0.067146. Entropy: 0.304106.\n",
      "episode: 1404   score: 105.0  epsilon: 1.0    steps: 272  evaluation reward: 214.0\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3352: Policy loss: 0.149339. Value loss: 0.100893. Entropy: 0.310280.\n",
      "Iteration 3353: Policy loss: 0.141923. Value loss: 0.056852. Entropy: 0.310439.\n",
      "Iteration 3354: Policy loss: 0.138673. Value loss: 0.042931. Entropy: 0.308690.\n",
      "episode: 1405   score: 155.0  epsilon: 1.0    steps: 208  evaluation reward: 214.45\n",
      "episode: 1406   score: 45.0  epsilon: 1.0    steps: 816  evaluation reward: 213.8\n",
      "episode: 1407   score: 545.0  epsilon: 1.0    steps: 1016  evaluation reward: 216.85\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3355: Policy loss: -0.002962. Value loss: 0.101403. Entropy: 0.305075.\n",
      "Iteration 3356: Policy loss: -0.011081. Value loss: 0.055468. Entropy: 0.307123.\n",
      "Iteration 3357: Policy loss: -0.013315. Value loss: 0.041756. Entropy: 0.305440.\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3358: Policy loss: 0.045746. Value loss: 0.234507. Entropy: 0.307979.\n",
      "Iteration 3359: Policy loss: 0.027494. Value loss: 0.106047. Entropy: 0.305962.\n",
      "Iteration 3360: Policy loss: 0.013069. Value loss: 0.071636. Entropy: 0.305841.\n",
      "episode: 1408   score: 210.0  epsilon: 1.0    steps: 496  evaluation reward: 216.1\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3361: Policy loss: 0.037609. Value loss: 0.098273. Entropy: 0.308506.\n",
      "Iteration 3362: Policy loss: 0.029390. Value loss: 0.050538. Entropy: 0.308274.\n",
      "Iteration 3363: Policy loss: 0.032447. Value loss: 0.035137. Entropy: 0.308439.\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3364: Policy loss: -0.279581. Value loss: 0.281981. Entropy: 0.309547.\n",
      "Iteration 3365: Policy loss: -0.287228. Value loss: 0.099547. Entropy: 0.307868.\n",
      "Iteration 3366: Policy loss: -0.289395. Value loss: 0.068572. Entropy: 0.307689.\n",
      "episode: 1409   score: 200.0  epsilon: 1.0    steps: 104  evaluation reward: 215.65\n",
      "episode: 1410   score: 225.0  epsilon: 1.0    steps: 472  evaluation reward: 216.8\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3367: Policy loss: 0.012781. Value loss: 0.123523. Entropy: 0.306222.\n",
      "Iteration 3368: Policy loss: -0.008367. Value loss: 0.055505. Entropy: 0.304913.\n",
      "Iteration 3369: Policy loss: -0.000998. Value loss: 0.042375. Entropy: 0.306151.\n",
      "episode: 1411   score: 225.0  epsilon: 1.0    steps: 304  evaluation reward: 216.95\n",
      "episode: 1412   score: 410.0  epsilon: 1.0    steps: 824  evaluation reward: 217.25\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3370: Policy loss: 0.077349. Value loss: 0.118103. Entropy: 0.307310.\n",
      "Iteration 3371: Policy loss: 0.070077. Value loss: 0.058810. Entropy: 0.306129.\n",
      "Iteration 3372: Policy loss: 0.064460. Value loss: 0.047102. Entropy: 0.305712.\n",
      "episode: 1413   score: 265.0  epsilon: 1.0    steps: 256  evaluation reward: 218.85\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3373: Policy loss: 0.390178. Value loss: 0.123042. Entropy: 0.311539.\n",
      "Iteration 3374: Policy loss: 0.380653. Value loss: 0.042505. Entropy: 0.312485.\n",
      "Iteration 3375: Policy loss: 0.375788. Value loss: 0.030720. Entropy: 0.311727.\n",
      "episode: 1414   score: 215.0  epsilon: 1.0    steps: 248  evaluation reward: 218.9\n",
      "episode: 1415   score: 105.0  epsilon: 1.0    steps: 944  evaluation reward: 215.35\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3376: Policy loss: -0.019969. Value loss: 0.136344. Entropy: 0.311462.\n",
      "Iteration 3377: Policy loss: -0.022771. Value loss: 0.077471. Entropy: 0.310884.\n",
      "Iteration 3378: Policy loss: -0.025038. Value loss: 0.059848. Entropy: 0.310127.\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3379: Policy loss: 0.040876. Value loss: 0.110013. Entropy: 0.309701.\n",
      "Iteration 3380: Policy loss: 0.036335. Value loss: 0.044243. Entropy: 0.310175.\n",
      "Iteration 3381: Policy loss: 0.025325. Value loss: 0.034278. Entropy: 0.309725.\n",
      "episode: 1416   score: 245.0  epsilon: 1.0    steps: 192  evaluation reward: 214.95\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3382: Policy loss: -0.027561. Value loss: 0.089893. Entropy: 0.304597.\n",
      "Iteration 3383: Policy loss: -0.036083. Value loss: 0.044693. Entropy: 0.304999.\n",
      "Iteration 3384: Policy loss: -0.038699. Value loss: 0.031116. Entropy: 0.305302.\n",
      "episode: 1417   score: 110.0  epsilon: 1.0    steps: 280  evaluation reward: 211.4\n",
      "episode: 1418   score: 290.0  epsilon: 1.0    steps: 352  evaluation reward: 212.75\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3385: Policy loss: 0.264176. Value loss: 0.126398. Entropy: 0.305354.\n",
      "Iteration 3386: Policy loss: 0.255829. Value loss: 0.052090. Entropy: 0.305663.\n",
      "Iteration 3387: Policy loss: 0.249748. Value loss: 0.034296. Entropy: 0.304609.\n",
      "episode: 1419   score: 120.0  epsilon: 1.0    steps: 440  evaluation reward: 212.15\n",
      "episode: 1420   score: 210.0  epsilon: 1.0    steps: 624  evaluation reward: 211.6\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3388: Policy loss: -0.019101. Value loss: 0.099466. Entropy: 0.307549.\n",
      "Iteration 3389: Policy loss: -0.028241. Value loss: 0.041383. Entropy: 0.307054.\n",
      "Iteration 3390: Policy loss: -0.039220. Value loss: 0.031772. Entropy: 0.306520.\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3391: Policy loss: 0.412858. Value loss: 0.113132. Entropy: 0.303039.\n",
      "Iteration 3392: Policy loss: 0.403663. Value loss: 0.045961. Entropy: 0.303275.\n",
      "Iteration 3393: Policy loss: 0.404358. Value loss: 0.035291. Entropy: 0.303839.\n",
      "episode: 1421   score: 85.0  epsilon: 1.0    steps: 160  evaluation reward: 211.4\n",
      "episode: 1422   score: 180.0  epsilon: 1.0    steps: 640  evaluation reward: 212.0\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3394: Policy loss: 0.192339. Value loss: 0.059373. Entropy: 0.310128.\n",
      "Iteration 3395: Policy loss: 0.192818. Value loss: 0.026279. Entropy: 0.309628.\n",
      "Iteration 3396: Policy loss: 0.181340. Value loss: 0.020251. Entropy: 0.309929.\n",
      "episode: 1423   score: 100.0  epsilon: 1.0    steps: 352  evaluation reward: 211.45\n",
      "episode: 1424   score: 210.0  epsilon: 1.0    steps: 720  evaluation reward: 213.05\n",
      "episode: 1425   score: 60.0  epsilon: 1.0    steps: 816  evaluation reward: 212.55\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3397: Policy loss: 0.044152. Value loss: 0.110248. Entropy: 0.312875.\n",
      "Iteration 3398: Policy loss: 0.040827. Value loss: 0.052753. Entropy: 0.313376.\n",
      "Iteration 3399: Policy loss: 0.040911. Value loss: 0.038500. Entropy: 0.312257.\n",
      "episode: 1426   score: 110.0  epsilon: 1.0    steps: 296  evaluation reward: 209.55\n",
      "episode: 1427   score: 390.0  epsilon: 1.0    steps: 584  evaluation reward: 208.25\n",
      "Training network. lr: 0.000224. clip: 0.089715\n",
      "Iteration 3400: Policy loss: 0.125460. Value loss: 0.075237. Entropy: 0.316449.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3401: Policy loss: 0.117273. Value loss: 0.036785. Entropy: 0.314503.\n",
      "Iteration 3402: Policy loss: 0.113025. Value loss: 0.028240. Entropy: 0.313311.\n",
      "episode: 1428   score: 165.0  epsilon: 1.0    steps: 496  evaluation reward: 208.85\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3403: Policy loss: 0.055046. Value loss: 0.085758. Entropy: 0.313917.\n",
      "Iteration 3404: Policy loss: 0.048050. Value loss: 0.041475. Entropy: 0.312528.\n",
      "Iteration 3405: Policy loss: 0.052669. Value loss: 0.026849. Entropy: 0.312715.\n",
      "episode: 1429   score: 70.0  epsilon: 1.0    steps: 376  evaluation reward: 207.75\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3406: Policy loss: -0.159601. Value loss: 0.103547. Entropy: 0.312059.\n",
      "Iteration 3407: Policy loss: -0.170926. Value loss: 0.049818. Entropy: 0.312987.\n",
      "Iteration 3408: Policy loss: -0.168208. Value loss: 0.034028. Entropy: 0.312437.\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3409: Policy loss: -0.110814. Value loss: 0.110266. Entropy: 0.311146.\n",
      "Iteration 3410: Policy loss: -0.116201. Value loss: 0.044657. Entropy: 0.309942.\n",
      "Iteration 3411: Policy loss: -0.117762. Value loss: 0.032955. Entropy: 0.310052.\n",
      "episode: 1430   score: 155.0  epsilon: 1.0    steps: 112  evaluation reward: 207.5\n",
      "episode: 1431   score: 155.0  epsilon: 1.0    steps: 728  evaluation reward: 206.45\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3412: Policy loss: -0.076072. Value loss: 0.057389. Entropy: 0.304388.\n",
      "Iteration 3413: Policy loss: -0.082661. Value loss: 0.023240. Entropy: 0.304628.\n",
      "Iteration 3414: Policy loss: -0.088960. Value loss: 0.016048. Entropy: 0.305851.\n",
      "episode: 1432   score: 135.0  epsilon: 1.0    steps: 912  evaluation reward: 205.7\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3415: Policy loss: -0.019590. Value loss: 0.071764. Entropy: 0.310131.\n",
      "Iteration 3416: Policy loss: -0.024473. Value loss: 0.027146. Entropy: 0.310911.\n",
      "Iteration 3417: Policy loss: -0.026139. Value loss: 0.019114. Entropy: 0.311723.\n",
      "episode: 1433   score: 155.0  epsilon: 1.0    steps: 400  evaluation reward: 202.65\n",
      "episode: 1434   score: 210.0  epsilon: 1.0    steps: 552  evaluation reward: 202.65\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3418: Policy loss: -0.032993. Value loss: 0.101477. Entropy: 0.316151.\n",
      "Iteration 3419: Policy loss: -0.041019. Value loss: 0.037850. Entropy: 0.316008.\n",
      "Iteration 3420: Policy loss: -0.043371. Value loss: 0.028077. Entropy: 0.315623.\n",
      "episode: 1435   score: 30.0  epsilon: 1.0    steps: 464  evaluation reward: 201.4\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3421: Policy loss: -0.354711. Value loss: 0.178317. Entropy: 0.311424.\n",
      "Iteration 3422: Policy loss: -0.401277. Value loss: 0.058201. Entropy: 0.311790.\n",
      "Iteration 3423: Policy loss: -0.372504. Value loss: 0.045380. Entropy: 0.309982.\n",
      "episode: 1436   score: 260.0  epsilon: 1.0    steps: 496  evaluation reward: 202.8\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3424: Policy loss: -0.300260. Value loss: 0.218399. Entropy: 0.311279.\n",
      "Iteration 3425: Policy loss: -0.295295. Value loss: 0.080202. Entropy: 0.312043.\n",
      "Iteration 3426: Policy loss: -0.317488. Value loss: 0.055174. Entropy: 0.311560.\n",
      "episode: 1437   score: 545.0  epsilon: 1.0    steps: 480  evaluation reward: 207.15\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3427: Policy loss: -0.178505. Value loss: 0.188150. Entropy: 0.309075.\n",
      "Iteration 3428: Policy loss: -0.201812. Value loss: 0.085893. Entropy: 0.308511.\n",
      "Iteration 3429: Policy loss: -0.191700. Value loss: 0.053098. Entropy: 0.308912.\n",
      "episode: 1438   score: 210.0  epsilon: 1.0    steps: 440  evaluation reward: 207.7\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3430: Policy loss: -0.031559. Value loss: 0.089541. Entropy: 0.310116.\n",
      "Iteration 3431: Policy loss: -0.035628. Value loss: 0.044034. Entropy: 0.309107.\n",
      "Iteration 3432: Policy loss: -0.039396. Value loss: 0.030183. Entropy: 0.309519.\n",
      "episode: 1439   score: 210.0  epsilon: 1.0    steps: 472  evaluation reward: 207.7\n",
      "episode: 1440   score: 160.0  epsilon: 1.0    steps: 576  evaluation reward: 208.2\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3433: Policy loss: -0.161149. Value loss: 0.116520. Entropy: 0.307929.\n",
      "Iteration 3434: Policy loss: -0.174104. Value loss: 0.062055. Entropy: 0.307389.\n",
      "Iteration 3435: Policy loss: -0.176649. Value loss: 0.046771. Entropy: 0.307215.\n",
      "episode: 1441   score: 180.0  epsilon: 1.0    steps: 296  evaluation reward: 208.65\n",
      "episode: 1442   score: 620.0  epsilon: 1.0    steps: 848  evaluation reward: 214.05\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3436: Policy loss: 0.099761. Value loss: 0.129096. Entropy: 0.308890.\n",
      "Iteration 3437: Policy loss: 0.099290. Value loss: 0.060426. Entropy: 0.309696.\n",
      "Iteration 3438: Policy loss: 0.091946. Value loss: 0.040692. Entropy: 0.309587.\n",
      "episode: 1443   score: 210.0  epsilon: 1.0    steps: 592  evaluation reward: 215.7\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3439: Policy loss: -0.138106. Value loss: 0.119573. Entropy: 0.310139.\n",
      "Iteration 3440: Policy loss: -0.130125. Value loss: 0.051658. Entropy: 0.310156.\n",
      "Iteration 3441: Policy loss: -0.143418. Value loss: 0.041931. Entropy: 0.309708.\n",
      "episode: 1444   score: 235.0  epsilon: 1.0    steps: 920  evaluation reward: 217.25\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3442: Policy loss: -0.217081. Value loss: 0.145508. Entropy: 0.309117.\n",
      "Iteration 3443: Policy loss: -0.210405. Value loss: 0.059150. Entropy: 0.309794.\n",
      "Iteration 3444: Policy loss: -0.214458. Value loss: 0.042863. Entropy: 0.309496.\n",
      "episode: 1445   score: 155.0  epsilon: 1.0    steps: 112  evaluation reward: 217.55\n",
      "episode: 1446   score: 210.0  epsilon: 1.0    steps: 784  evaluation reward: 219.15\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3445: Policy loss: 0.000228. Value loss: 0.081683. Entropy: 0.303975.\n",
      "Iteration 3446: Policy loss: -0.005611. Value loss: 0.036163. Entropy: 0.304412.\n",
      "Iteration 3447: Policy loss: -0.013812. Value loss: 0.029838. Entropy: 0.302882.\n",
      "Training network. lr: 0.000224. clip: 0.089558\n",
      "Iteration 3448: Policy loss: 0.085896. Value loss: 0.058149. Entropy: 0.307639.\n",
      "Iteration 3449: Policy loss: 0.081311. Value loss: 0.031733. Entropy: 0.307132.\n",
      "Iteration 3450: Policy loss: 0.082801. Value loss: 0.026011. Entropy: 0.306465.\n",
      "episode: 1447   score: 135.0  epsilon: 1.0    steps: 408  evaluation reward: 219.5\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3451: Policy loss: -0.017709. Value loss: 0.079037. Entropy: 0.304783.\n",
      "Iteration 3452: Policy loss: -0.026028. Value loss: 0.036772. Entropy: 0.303841.\n",
      "Iteration 3453: Policy loss: -0.022313. Value loss: 0.026034. Entropy: 0.302815.\n",
      "episode: 1448   score: 90.0  epsilon: 1.0    steps: 168  evaluation reward: 219.25\n",
      "episode: 1449   score: 165.0  epsilon: 1.0    steps: 672  evaluation reward: 217.1\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3454: Policy loss: 0.005720. Value loss: 0.055992. Entropy: 0.310450.\n",
      "Iteration 3455: Policy loss: -0.002933. Value loss: 0.024198. Entropy: 0.308672.\n",
      "Iteration 3456: Policy loss: -0.004910. Value loss: 0.020272. Entropy: 0.307765.\n",
      "episode: 1450   score: 260.0  epsilon: 1.0    steps: 48  evaluation reward: 218.6\n",
      "now time :  2019-09-05 17:49:51.640739\n",
      "episode: 1451   score: 290.0  epsilon: 1.0    steps: 184  evaluation reward: 220.3\n",
      "episode: 1452   score: 135.0  epsilon: 1.0    steps: 256  evaluation reward: 220.45\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3457: Policy loss: 0.042666. Value loss: 0.040740. Entropy: 0.309576.\n",
      "Iteration 3458: Policy loss: 0.040261. Value loss: 0.018365. Entropy: 0.308661.\n",
      "Iteration 3459: Policy loss: 0.039969. Value loss: 0.014800. Entropy: 0.308475.\n",
      "episode: 1453   score: 260.0  epsilon: 1.0    steps: 472  evaluation reward: 221.95\n",
      "episode: 1454   score: 110.0  epsilon: 1.0    steps: 912  evaluation reward: 222.0\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3460: Policy loss: 0.130277. Value loss: 0.066395. Entropy: 0.312789.\n",
      "Iteration 3461: Policy loss: 0.120373. Value loss: 0.026513. Entropy: 0.309800.\n",
      "Iteration 3462: Policy loss: 0.120040. Value loss: 0.022368. Entropy: 0.311246.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1455   score: 125.0  epsilon: 1.0    steps: 648  evaluation reward: 220.7\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3463: Policy loss: 0.053064. Value loss: 0.087018. Entropy: 0.309147.\n",
      "Iteration 3464: Policy loss: 0.041767. Value loss: 0.034698. Entropy: 0.309362.\n",
      "Iteration 3465: Policy loss: 0.042323. Value loss: 0.026883. Entropy: 0.309320.\n",
      "episode: 1456   score: 110.0  epsilon: 1.0    steps: 712  evaluation reward: 220.45\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3466: Policy loss: -0.188499. Value loss: 0.120551. Entropy: 0.307540.\n",
      "Iteration 3467: Policy loss: -0.192988. Value loss: 0.060793. Entropy: 0.307020.\n",
      "Iteration 3468: Policy loss: -0.189293. Value loss: 0.050564. Entropy: 0.306574.\n",
      "episode: 1457   score: 95.0  epsilon: 1.0    steps: 320  evaluation reward: 219.75\n",
      "episode: 1458   score: 240.0  epsilon: 1.0    steps: 536  evaluation reward: 219.75\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3469: Policy loss: 0.036346. Value loss: 0.072156. Entropy: 0.305850.\n",
      "Iteration 3470: Policy loss: 0.030654. Value loss: 0.043594. Entropy: 0.306701.\n",
      "Iteration 3471: Policy loss: 0.027867. Value loss: 0.032780. Entropy: 0.306996.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3472: Policy loss: -0.053213. Value loss: 0.111871. Entropy: 0.307424.\n",
      "Iteration 3473: Policy loss: -0.057006. Value loss: 0.042554. Entropy: 0.306138.\n",
      "Iteration 3474: Policy loss: -0.061451. Value loss: 0.034266. Entropy: 0.307487.\n",
      "episode: 1459   score: 180.0  epsilon: 1.0    steps: 24  evaluation reward: 219.45\n",
      "episode: 1460   score: 210.0  epsilon: 1.0    steps: 600  evaluation reward: 219.45\n",
      "episode: 1461   score: 245.0  epsilon: 1.0    steps: 928  evaluation reward: 219.75\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3475: Policy loss: 0.046712. Value loss: 0.084061. Entropy: 0.311427.\n",
      "Iteration 3476: Policy loss: 0.039776. Value loss: 0.038747. Entropy: 0.311379.\n",
      "Iteration 3477: Policy loss: 0.034224. Value loss: 0.030118. Entropy: 0.311686.\n",
      "episode: 1462   score: 160.0  epsilon: 1.0    steps: 760  evaluation reward: 219.25\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3478: Policy loss: -0.082830. Value loss: 0.072658. Entropy: 0.312212.\n",
      "Iteration 3479: Policy loss: -0.100861. Value loss: 0.030782. Entropy: 0.312831.\n",
      "Iteration 3480: Policy loss: -0.100251. Value loss: 0.023427. Entropy: 0.312319.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3481: Policy loss: -0.073117. Value loss: 0.078509. Entropy: 0.308574.\n",
      "Iteration 3482: Policy loss: -0.086242. Value loss: 0.036519. Entropy: 0.308870.\n",
      "Iteration 3483: Policy loss: -0.085816. Value loss: 0.026671. Entropy: 0.307903.\n",
      "episode: 1463   score: 210.0  epsilon: 1.0    steps: 96  evaluation reward: 216.85\n",
      "episode: 1464   score: 210.0  epsilon: 1.0    steps: 624  evaluation reward: 216.35\n",
      "episode: 1465   score: 135.0  epsilon: 1.0    steps: 800  evaluation reward: 217.2\n",
      "episode: 1466   score: 260.0  epsilon: 1.0    steps: 832  evaluation reward: 217.2\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3484: Policy loss: 0.172856. Value loss: 0.087648. Entropy: 0.306341.\n",
      "Iteration 3485: Policy loss: 0.157042. Value loss: 0.044060. Entropy: 0.305282.\n",
      "Iteration 3486: Policy loss: 0.164107. Value loss: 0.032021. Entropy: 0.303810.\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3487: Policy loss: 0.219250. Value loss: 0.102160. Entropy: 0.310834.\n",
      "Iteration 3488: Policy loss: 0.205876. Value loss: 0.037896. Entropy: 0.310108.\n",
      "Iteration 3489: Policy loss: 0.199844. Value loss: 0.025808. Entropy: 0.310032.\n",
      "episode: 1467   score: 155.0  epsilon: 1.0    steps: 104  evaluation reward: 217.1\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3490: Policy loss: 0.040212. Value loss: 0.098907. Entropy: 0.309854.\n",
      "Iteration 3491: Policy loss: 0.030961. Value loss: 0.044344. Entropy: 0.309804.\n",
      "Iteration 3492: Policy loss: 0.030819. Value loss: 0.035000. Entropy: 0.309611.\n",
      "episode: 1468   score: 210.0  epsilon: 1.0    steps: 808  evaluation reward: 215.1\n",
      "episode: 1469   score: 105.0  epsilon: 1.0    steps: 952  evaluation reward: 214.05\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3493: Policy loss: -0.112000. Value loss: 0.116023. Entropy: 0.306261.\n",
      "Iteration 3494: Policy loss: -0.116429. Value loss: 0.056856. Entropy: 0.306299.\n",
      "Iteration 3495: Policy loss: -0.117769. Value loss: 0.043721. Entropy: 0.305930.\n",
      "episode: 1470   score: 215.0  epsilon: 1.0    steps: 520  evaluation reward: 214.4\n",
      "episode: 1471   score: 145.0  epsilon: 1.0    steps: 880  evaluation reward: 214.05\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3496: Policy loss: 0.005798. Value loss: 0.108652. Entropy: 0.301483.\n",
      "Iteration 3497: Policy loss: -0.000047. Value loss: 0.055968. Entropy: 0.302027.\n",
      "Iteration 3498: Policy loss: 0.005942. Value loss: 0.038626. Entropy: 0.301782.\n",
      "episode: 1472   score: 245.0  epsilon: 1.0    steps: 440  evaluation reward: 215.4\n",
      "Training network. lr: 0.000224. clip: 0.089411\n",
      "Iteration 3499: Policy loss: 0.039197. Value loss: 0.085698. Entropy: 0.305418.\n",
      "Iteration 3500: Policy loss: 0.040501. Value loss: 0.040197. Entropy: 0.305049.\n",
      "Iteration 3501: Policy loss: 0.030262. Value loss: 0.025735. Entropy: 0.306976.\n",
      "episode: 1473   score: 180.0  epsilon: 1.0    steps: 928  evaluation reward: 216.15\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3502: Policy loss: -0.094428. Value loss: 0.119042. Entropy: 0.312082.\n",
      "Iteration 3503: Policy loss: -0.102982. Value loss: 0.052774. Entropy: 0.312174.\n",
      "Iteration 3504: Policy loss: -0.110665. Value loss: 0.036777. Entropy: 0.312185.\n",
      "episode: 1474   score: 375.0  epsilon: 1.0    steps: 312  evaluation reward: 217.8\n",
      "episode: 1475   score: 285.0  epsilon: 1.0    steps: 968  evaluation reward: 219.55\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3505: Policy loss: 0.136964. Value loss: 0.153531. Entropy: 0.308766.\n",
      "Iteration 3506: Policy loss: 0.126807. Value loss: 0.075380. Entropy: 0.306879.\n",
      "Iteration 3507: Policy loss: 0.119634. Value loss: 0.058019. Entropy: 0.307000.\n",
      "episode: 1476   score: 160.0  epsilon: 1.0    steps: 544  evaluation reward: 217.85\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3508: Policy loss: 0.261987. Value loss: 0.110805. Entropy: 0.309717.\n",
      "Iteration 3509: Policy loss: 0.261008. Value loss: 0.048936. Entropy: 0.307968.\n",
      "Iteration 3510: Policy loss: 0.255896. Value loss: 0.036597. Entropy: 0.308043.\n",
      "episode: 1477   score: 240.0  epsilon: 1.0    steps: 48  evaluation reward: 219.2\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3511: Policy loss: 0.083978. Value loss: 0.086393. Entropy: 0.314445.\n",
      "Iteration 3512: Policy loss: 0.089038. Value loss: 0.041749. Entropy: 0.313512.\n",
      "Iteration 3513: Policy loss: 0.082205. Value loss: 0.029820. Entropy: 0.313444.\n",
      "episode: 1478   score: 185.0  epsilon: 1.0    steps: 240  evaluation reward: 219.25\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3514: Policy loss: 0.138107. Value loss: 0.095808. Entropy: 0.315457.\n",
      "Iteration 3515: Policy loss: 0.137592. Value loss: 0.044975. Entropy: 0.314232.\n",
      "Iteration 3516: Policy loss: 0.132255. Value loss: 0.032284. Entropy: 0.313419.\n",
      "episode: 1479   score: 120.0  epsilon: 1.0    steps: 744  evaluation reward: 219.7\n",
      "episode: 1480   score: 105.0  epsilon: 1.0    steps: 880  evaluation reward: 218.15\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3517: Policy loss: -0.054879. Value loss: 0.326747. Entropy: 0.307042.\n",
      "Iteration 3518: Policy loss: -0.071672. Value loss: 0.155414. Entropy: 0.306758.\n",
      "Iteration 3519: Policy loss: -0.048347. Value loss: 0.064678. Entropy: 0.306309.\n",
      "episode: 1481   score: 210.0  epsilon: 1.0    steps: 792  evaluation reward: 218.15\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3520: Policy loss: 0.148532. Value loss: 0.191239. Entropy: 0.307609.\n",
      "Iteration 3521: Policy loss: 0.139962. Value loss: 0.087501. Entropy: 0.307730.\n",
      "Iteration 3522: Policy loss: 0.144135. Value loss: 0.065498. Entropy: 0.307044.\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3523: Policy loss: 0.083623. Value loss: 0.180368. Entropy: 0.313744.\n",
      "Iteration 3524: Policy loss: 0.078866. Value loss: 0.080916. Entropy: 0.313888.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3525: Policy loss: 0.075813. Value loss: 0.056780. Entropy: 0.312543.\n",
      "episode: 1482   score: 45.0  epsilon: 1.0    steps: 872  evaluation reward: 217.5\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3526: Policy loss: 0.251776. Value loss: 0.150613. Entropy: 0.310113.\n",
      "Iteration 3527: Policy loss: 0.235542. Value loss: 0.075736. Entropy: 0.308437.\n",
      "Iteration 3528: Policy loss: 0.238958. Value loss: 0.057362. Entropy: 0.307533.\n",
      "episode: 1483   score: 455.0  epsilon: 1.0    steps: 376  evaluation reward: 220.2\n",
      "episode: 1484   score: 235.0  epsilon: 1.0    steps: 408  evaluation reward: 221.15\n",
      "episode: 1485   score: 45.0  epsilon: 1.0    steps: 592  evaluation reward: 217.95\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3529: Policy loss: 0.345058. Value loss: 0.137155. Entropy: 0.304343.\n",
      "Iteration 3530: Policy loss: 0.325249. Value loss: 0.055530. Entropy: 0.300954.\n",
      "Iteration 3531: Policy loss: 0.315427. Value loss: 0.042589. Entropy: 0.301624.\n",
      "episode: 1486   score: 435.0  epsilon: 1.0    steps: 152  evaluation reward: 217.35\n",
      "episode: 1487   score: 375.0  epsilon: 1.0    steps: 520  evaluation reward: 219.3\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3532: Policy loss: 0.150372. Value loss: 0.194449. Entropy: 0.309868.\n",
      "Iteration 3533: Policy loss: 0.151242. Value loss: 0.091876. Entropy: 0.307817.\n",
      "Iteration 3534: Policy loss: 0.132213. Value loss: 0.069043. Entropy: 0.308635.\n",
      "episode: 1488   score: 180.0  epsilon: 1.0    steps: 352  evaluation reward: 219.9\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3535: Policy loss: -0.080355. Value loss: 0.135469. Entropy: 0.312415.\n",
      "Iteration 3536: Policy loss: -0.086508. Value loss: 0.071581. Entropy: 0.310883.\n",
      "Iteration 3537: Policy loss: -0.100341. Value loss: 0.050912. Entropy: 0.311904.\n",
      "episode: 1489   score: 225.0  epsilon: 1.0    steps: 160  evaluation reward: 219.75\n",
      "episode: 1490   score: 30.0  epsilon: 1.0    steps: 992  evaluation reward: 214.6\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3538: Policy loss: 0.051821. Value loss: 0.115662. Entropy: 0.311095.\n",
      "Iteration 3539: Policy loss: 0.036948. Value loss: 0.043784. Entropy: 0.309359.\n",
      "Iteration 3540: Policy loss: 0.024942. Value loss: 0.032292. Entropy: 0.309016.\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3541: Policy loss: 0.026632. Value loss: 0.094768. Entropy: 0.311465.\n",
      "Iteration 3542: Policy loss: 0.029784. Value loss: 0.049770. Entropy: 0.311858.\n",
      "Iteration 3543: Policy loss: 0.024647. Value loss: 0.037365. Entropy: 0.311551.\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3544: Policy loss: 0.179969. Value loss: 0.075072. Entropy: 0.311981.\n",
      "Iteration 3545: Policy loss: 0.171882. Value loss: 0.030206. Entropy: 0.311823.\n",
      "Iteration 3546: Policy loss: 0.166294. Value loss: 0.024447. Entropy: 0.311271.\n",
      "episode: 1491   score: 170.0  epsilon: 1.0    steps: 40  evaluation reward: 215.55\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3547: Policy loss: -0.100354. Value loss: 0.135155. Entropy: 0.308619.\n",
      "Iteration 3548: Policy loss: -0.096730. Value loss: 0.050939. Entropy: 0.307657.\n",
      "Iteration 3549: Policy loss: -0.103235. Value loss: 0.035416. Entropy: 0.309772.\n",
      "episode: 1492   score: 95.0  epsilon: 1.0    steps: 256  evaluation reward: 212.1\n",
      "episode: 1493   score: 305.0  epsilon: 1.0    steps: 952  evaluation reward: 210.45\n",
      "Training network. lr: 0.000223. clip: 0.089254\n",
      "Iteration 3550: Policy loss: -0.005157. Value loss: 0.230678. Entropy: 0.305090.\n",
      "Iteration 3551: Policy loss: -0.012758. Value loss: 0.109898. Entropy: 0.305979.\n",
      "Iteration 3552: Policy loss: -0.029612. Value loss: 0.084366. Entropy: 0.304951.\n",
      "episode: 1494   score: 90.0  epsilon: 1.0    steps: 336  evaluation reward: 208.75\n",
      "episode: 1495   score: 310.0  epsilon: 1.0    steps: 456  evaluation reward: 209.75\n",
      "episode: 1496   score: 215.0  epsilon: 1.0    steps: 960  evaluation reward: 208.1\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3553: Policy loss: 0.326761. Value loss: 0.154978. Entropy: 0.302401.\n",
      "Iteration 3554: Policy loss: 0.316699. Value loss: 0.047726. Entropy: 0.302502.\n",
      "Iteration 3555: Policy loss: 0.311896. Value loss: 0.032915. Entropy: 0.302400.\n",
      "episode: 1497   score: 295.0  epsilon: 1.0    steps: 392  evaluation reward: 208.2\n",
      "episode: 1498   score: 380.0  epsilon: 1.0    steps: 792  evaluation reward: 207.2\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3556: Policy loss: 0.362567. Value loss: 0.121518. Entropy: 0.308492.\n",
      "Iteration 3557: Policy loss: 0.353718. Value loss: 0.046100. Entropy: 0.307575.\n",
      "Iteration 3558: Policy loss: 0.356328. Value loss: 0.032087. Entropy: 0.307165.\n",
      "episode: 1499   score: 75.0  epsilon: 1.0    steps: 512  evaluation reward: 205.25\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3559: Policy loss: 0.019051. Value loss: 0.095499. Entropy: 0.307033.\n",
      "Iteration 3560: Policy loss: 0.007127. Value loss: 0.055390. Entropy: 0.309921.\n",
      "Iteration 3561: Policy loss: 0.003097. Value loss: 0.038819. Entropy: 0.308309.\n",
      "episode: 1500   score: 75.0  epsilon: 1.0    steps: 120  evaluation reward: 204.65\n",
      "now time :  2019-09-05 17:56:24.477481\n",
      "episode: 1501   score: 30.0  epsilon: 1.0    steps: 768  evaluation reward: 203.4\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3562: Policy loss: -0.265062. Value loss: 0.178761. Entropy: 0.311039.\n",
      "Iteration 3563: Policy loss: -0.260141. Value loss: 0.085185. Entropy: 0.310657.\n",
      "Iteration 3564: Policy loss: -0.276150. Value loss: 0.066971. Entropy: 0.311536.\n",
      "episode: 1502   score: 95.0  epsilon: 1.0    steps: 440  evaluation reward: 199.25\n",
      "episode: 1503   score: 120.0  epsilon: 1.0    steps: 512  evaluation reward: 197.25\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3565: Policy loss: 0.134470. Value loss: 0.077769. Entropy: 0.310344.\n",
      "Iteration 3566: Policy loss: 0.124526. Value loss: 0.037410. Entropy: 0.308992.\n",
      "Iteration 3567: Policy loss: 0.124832. Value loss: 0.030037. Entropy: 0.309886.\n",
      "episode: 1504   score: 185.0  epsilon: 1.0    steps: 248  evaluation reward: 198.05\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3568: Policy loss: -0.023769. Value loss: 0.110012. Entropy: 0.313063.\n",
      "Iteration 3569: Policy loss: -0.028699. Value loss: 0.057412. Entropy: 0.312582.\n",
      "Iteration 3570: Policy loss: -0.031792. Value loss: 0.040842. Entropy: 0.310727.\n",
      "episode: 1505   score: 230.0  epsilon: 1.0    steps: 688  evaluation reward: 198.8\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3571: Policy loss: 0.039502. Value loss: 0.076227. Entropy: 0.312156.\n",
      "Iteration 3572: Policy loss: 0.039981. Value loss: 0.029671. Entropy: 0.311058.\n",
      "Iteration 3573: Policy loss: 0.034151. Value loss: 0.020216. Entropy: 0.310999.\n",
      "episode: 1506   score: 210.0  epsilon: 1.0    steps: 184  evaluation reward: 200.45\n",
      "episode: 1507   score: 135.0  epsilon: 1.0    steps: 240  evaluation reward: 196.35\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3574: Policy loss: -0.263380. Value loss: 0.331532. Entropy: 0.300851.\n",
      "Iteration 3575: Policy loss: -0.292061. Value loss: 0.259427. Entropy: 0.300525.\n",
      "Iteration 3576: Policy loss: -0.295187. Value loss: 0.201787. Entropy: 0.301240.\n",
      "episode: 1508   score: 120.0  epsilon: 1.0    steps: 488  evaluation reward: 195.45\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3577: Policy loss: 0.096808. Value loss: 0.088611. Entropy: 0.314833.\n",
      "Iteration 3578: Policy loss: 0.092841. Value loss: 0.035262. Entropy: 0.312977.\n",
      "Iteration 3579: Policy loss: 0.089433. Value loss: 0.024519. Entropy: 0.313459.\n",
      "episode: 1509   score: 90.0  epsilon: 1.0    steps: 120  evaluation reward: 194.35\n",
      "episode: 1510   score: 410.0  epsilon: 1.0    steps: 464  evaluation reward: 196.2\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3580: Policy loss: -0.282583. Value loss: 0.134959. Entropy: 0.308529.\n",
      "Iteration 3581: Policy loss: -0.285763. Value loss: 0.073173. Entropy: 0.310209.\n",
      "Iteration 3582: Policy loss: -0.292310. Value loss: 0.050930. Entropy: 0.308775.\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3583: Policy loss: 0.008294. Value loss: 0.161741. Entropy: 0.313058.\n",
      "Iteration 3584: Policy loss: 0.004284. Value loss: 0.070723. Entropy: 0.312646.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3585: Policy loss: 0.007074. Value loss: 0.046435. Entropy: 0.313248.\n",
      "episode: 1511   score: 180.0  epsilon: 1.0    steps: 536  evaluation reward: 195.75\n",
      "episode: 1512   score: 215.0  epsilon: 1.0    steps: 624  evaluation reward: 193.8\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3586: Policy loss: 0.324710. Value loss: 0.089796. Entropy: 0.301732.\n",
      "Iteration 3587: Policy loss: 0.316629. Value loss: 0.032886. Entropy: 0.302263.\n",
      "Iteration 3588: Policy loss: 0.310241. Value loss: 0.023603. Entropy: 0.301710.\n",
      "episode: 1513   score: 105.0  epsilon: 1.0    steps: 96  evaluation reward: 192.2\n",
      "episode: 1514   score: 350.0  epsilon: 1.0    steps: 504  evaluation reward: 193.55\n",
      "episode: 1515   score: 195.0  epsilon: 1.0    steps: 1024  evaluation reward: 194.45\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3589: Policy loss: 0.141460. Value loss: 0.088751. Entropy: 0.307311.\n",
      "Iteration 3590: Policy loss: 0.138878. Value loss: 0.041391. Entropy: 0.305583.\n",
      "Iteration 3591: Policy loss: 0.134791. Value loss: 0.033650. Entropy: 0.306046.\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3592: Policy loss: 0.021162. Value loss: 0.111829. Entropy: 0.314284.\n",
      "Iteration 3593: Policy loss: 0.015532. Value loss: 0.049376. Entropy: 0.313667.\n",
      "Iteration 3594: Policy loss: 0.005558. Value loss: 0.036391. Entropy: 0.313886.\n",
      "episode: 1516   score: 270.0  epsilon: 1.0    steps: 568  evaluation reward: 194.7\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3595: Policy loss: -0.137516. Value loss: 0.119152. Entropy: 0.306073.\n",
      "Iteration 3596: Policy loss: -0.146027. Value loss: 0.058962. Entropy: 0.305332.\n",
      "Iteration 3597: Policy loss: -0.148805. Value loss: 0.041407. Entropy: 0.306134.\n",
      "episode: 1517   score: 490.0  epsilon: 1.0    steps: 968  evaluation reward: 198.5\n",
      "Training network. lr: 0.000223. clip: 0.089097\n",
      "Iteration 3598: Policy loss: -0.057004. Value loss: 0.183449. Entropy: 0.311783.\n",
      "Iteration 3599: Policy loss: -0.063526. Value loss: 0.062637. Entropy: 0.312001.\n",
      "Iteration 3600: Policy loss: -0.073537. Value loss: 0.049204. Entropy: 0.311908.\n",
      "episode: 1518   score: 345.0  epsilon: 1.0    steps: 392  evaluation reward: 199.05\n",
      "episode: 1519   score: 180.0  epsilon: 1.0    steps: 544  evaluation reward: 199.65\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3601: Policy loss: 0.169392. Value loss: 0.113121. Entropy: 0.303975.\n",
      "Iteration 3602: Policy loss: 0.155282. Value loss: 0.050885. Entropy: 0.303226.\n",
      "Iteration 3603: Policy loss: 0.147469. Value loss: 0.035150. Entropy: 0.303392.\n",
      "episode: 1520   score: 100.0  epsilon: 1.0    steps: 96  evaluation reward: 198.55\n",
      "episode: 1521   score: 210.0  epsilon: 1.0    steps: 584  evaluation reward: 199.8\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3604: Policy loss: 0.065945. Value loss: 0.151493. Entropy: 0.303101.\n",
      "Iteration 3605: Policy loss: 0.054453. Value loss: 0.086888. Entropy: 0.301275.\n",
      "Iteration 3606: Policy loss: 0.046907. Value loss: 0.052299. Entropy: 0.301307.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3607: Policy loss: 0.195650. Value loss: 0.127985. Entropy: 0.308378.\n",
      "Iteration 3608: Policy loss: 0.187363. Value loss: 0.061452. Entropy: 0.310508.\n",
      "Iteration 3609: Policy loss: 0.182574. Value loss: 0.048070. Entropy: 0.309723.\n",
      "episode: 1522   score: 180.0  epsilon: 1.0    steps: 184  evaluation reward: 199.8\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3610: Policy loss: 0.039569. Value loss: 0.133175. Entropy: 0.305080.\n",
      "Iteration 3611: Policy loss: 0.038828. Value loss: 0.072803. Entropy: 0.304307.\n",
      "Iteration 3612: Policy loss: 0.037024. Value loss: 0.048604. Entropy: 0.304352.\n",
      "episode: 1523   score: 290.0  epsilon: 1.0    steps: 192  evaluation reward: 201.7\n",
      "episode: 1524   score: 80.0  epsilon: 1.0    steps: 296  evaluation reward: 200.4\n",
      "episode: 1525   score: 180.0  epsilon: 1.0    steps: 864  evaluation reward: 201.6\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3613: Policy loss: -0.259255. Value loss: 0.358563. Entropy: 0.301172.\n",
      "Iteration 3614: Policy loss: -0.261600. Value loss: 0.263040. Entropy: 0.300916.\n",
      "Iteration 3615: Policy loss: -0.286377. Value loss: 0.220251. Entropy: 0.299871.\n",
      "episode: 1526   score: 410.0  epsilon: 1.0    steps: 264  evaluation reward: 204.6\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3616: Policy loss: 0.253451. Value loss: 0.121895. Entropy: 0.308298.\n",
      "Iteration 3617: Policy loss: 0.239829. Value loss: 0.045575. Entropy: 0.308733.\n",
      "Iteration 3618: Policy loss: 0.237526. Value loss: 0.031702. Entropy: 0.308130.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3619: Policy loss: -0.137733. Value loss: 0.147473. Entropy: 0.303735.\n",
      "Iteration 3620: Policy loss: -0.147450. Value loss: 0.061103. Entropy: 0.304449.\n",
      "Iteration 3621: Policy loss: -0.162776. Value loss: 0.045138. Entropy: 0.303996.\n",
      "episode: 1527   score: 260.0  epsilon: 1.0    steps: 440  evaluation reward: 203.3\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3622: Policy loss: -0.158329. Value loss: 0.138168. Entropy: 0.304729.\n",
      "Iteration 3623: Policy loss: -0.168569. Value loss: 0.062813. Entropy: 0.303031.\n",
      "Iteration 3624: Policy loss: -0.173566. Value loss: 0.047804. Entropy: 0.303223.\n",
      "episode: 1528   score: 180.0  epsilon: 1.0    steps: 232  evaluation reward: 203.45\n",
      "episode: 1529   score: 635.0  epsilon: 1.0    steps: 568  evaluation reward: 209.1\n",
      "episode: 1530   score: 125.0  epsilon: 1.0    steps: 944  evaluation reward: 208.8\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3625: Policy loss: 0.029690. Value loss: 0.148624. Entropy: 0.302761.\n",
      "Iteration 3626: Policy loss: 0.015207. Value loss: 0.062358. Entropy: 0.302622.\n",
      "Iteration 3627: Policy loss: 0.015198. Value loss: 0.046450. Entropy: 0.302629.\n",
      "episode: 1531   score: 155.0  epsilon: 1.0    steps: 144  evaluation reward: 208.8\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3628: Policy loss: -0.099081. Value loss: 0.141041. Entropy: 0.307806.\n",
      "Iteration 3629: Policy loss: -0.106330. Value loss: 0.050455. Entropy: 0.306373.\n",
      "Iteration 3630: Policy loss: -0.113513. Value loss: 0.038633. Entropy: 0.305687.\n",
      "episode: 1532   score: 375.0  epsilon: 1.0    steps: 584  evaluation reward: 211.2\n",
      "episode: 1533   score: 320.0  epsilon: 1.0    steps: 584  evaluation reward: 212.85\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3631: Policy loss: 0.391770. Value loss: 0.146566. Entropy: 0.294864.\n",
      "Iteration 3632: Policy loss: 0.382508. Value loss: 0.059586. Entropy: 0.294070.\n",
      "Iteration 3633: Policy loss: 0.378079. Value loss: 0.045102. Entropy: 0.294041.\n",
      "episode: 1534   score: 60.0  epsilon: 1.0    steps: 664  evaluation reward: 211.35\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3634: Policy loss: -0.026045. Value loss: 0.186042. Entropy: 0.304543.\n",
      "Iteration 3635: Policy loss: -0.040707. Value loss: 0.095873. Entropy: 0.304128.\n",
      "Iteration 3636: Policy loss: -0.038762. Value loss: 0.065824. Entropy: 0.304356.\n",
      "episode: 1535   score: 365.0  epsilon: 1.0    steps: 240  evaluation reward: 214.7\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3637: Policy loss: 0.304579. Value loss: 0.150044. Entropy: 0.308412.\n",
      "Iteration 3638: Policy loss: 0.292108. Value loss: 0.066438. Entropy: 0.306762.\n",
      "Iteration 3639: Policy loss: 0.294600. Value loss: 0.049286. Entropy: 0.306318.\n",
      "episode: 1536   score: 165.0  epsilon: 1.0    steps: 600  evaluation reward: 213.75\n",
      "episode: 1537   score: 180.0  epsilon: 1.0    steps: 752  evaluation reward: 210.1\n",
      "episode: 1538   score: 285.0  epsilon: 1.0    steps: 872  evaluation reward: 210.85\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3640: Policy loss: 0.218100. Value loss: 0.146186. Entropy: 0.290962.\n",
      "Iteration 3641: Policy loss: 0.201236. Value loss: 0.056474. Entropy: 0.288680.\n",
      "Iteration 3642: Policy loss: 0.198867. Value loss: 0.041234. Entropy: 0.288276.\n",
      "episode: 1539   score: 155.0  epsilon: 1.0    steps: 272  evaluation reward: 210.3\n",
      "episode: 1540   score: 75.0  epsilon: 1.0    steps: 920  evaluation reward: 209.45\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3643: Policy loss: 0.251876. Value loss: 0.074680. Entropy: 0.306364.\n",
      "Iteration 3644: Policy loss: 0.246600. Value loss: 0.032960. Entropy: 0.305247.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3645: Policy loss: 0.244014. Value loss: 0.023272. Entropy: 0.304962.\n",
      "episode: 1541   score: 170.0  epsilon: 1.0    steps: 168  evaluation reward: 209.35\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3646: Policy loss: 0.028212. Value loss: 0.115088. Entropy: 0.294956.\n",
      "Iteration 3647: Policy loss: 0.020647. Value loss: 0.050148. Entropy: 0.293680.\n",
      "Iteration 3648: Policy loss: 0.015985. Value loss: 0.038967. Entropy: 0.295525.\n",
      "Training network. lr: 0.000222. clip: 0.088950\n",
      "Iteration 3649: Policy loss: -0.193552. Value loss: 0.316432. Entropy: 0.305706.\n",
      "Iteration 3650: Policy loss: -0.192746. Value loss: 0.150736. Entropy: 0.305591.\n",
      "Iteration 3651: Policy loss: -0.211956. Value loss: 0.082111. Entropy: 0.306698.\n",
      "episode: 1542   score: 215.0  epsilon: 1.0    steps: 88  evaluation reward: 205.3\n",
      "episode: 1543   score: 50.0  epsilon: 1.0    steps: 864  evaluation reward: 203.7\n",
      "episode: 1544   score: 120.0  epsilon: 1.0    steps: 944  evaluation reward: 202.55\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3652: Policy loss: 0.385366. Value loss: 0.185042. Entropy: 0.296162.\n",
      "Iteration 3653: Policy loss: 0.397266. Value loss: 0.090032. Entropy: 0.292482.\n",
      "Iteration 3654: Policy loss: 0.374320. Value loss: 0.070818. Entropy: 0.293148.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3655: Policy loss: 0.182439. Value loss: 0.137029. Entropy: 0.309434.\n",
      "Iteration 3656: Policy loss: 0.170341. Value loss: 0.074008. Entropy: 0.308178.\n",
      "Iteration 3657: Policy loss: 0.162542. Value loss: 0.057537. Entropy: 0.308377.\n",
      "episode: 1545   score: 115.0  epsilon: 1.0    steps: 144  evaluation reward: 202.15\n",
      "episode: 1546   score: 160.0  epsilon: 1.0    steps: 944  evaluation reward: 201.65\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3658: Policy loss: -0.000930. Value loss: 0.121811. Entropy: 0.297269.\n",
      "Iteration 3659: Policy loss: -0.007506. Value loss: 0.051693. Entropy: 0.297473.\n",
      "Iteration 3660: Policy loss: -0.008944. Value loss: 0.035716. Entropy: 0.296910.\n",
      "episode: 1547   score: 415.0  epsilon: 1.0    steps: 112  evaluation reward: 204.45\n",
      "episode: 1548   score: 410.0  epsilon: 1.0    steps: 888  evaluation reward: 207.65\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3661: Policy loss: -0.057005. Value loss: 0.118279. Entropy: 0.304057.\n",
      "Iteration 3662: Policy loss: -0.068042. Value loss: 0.057720. Entropy: 0.304497.\n",
      "Iteration 3663: Policy loss: -0.063673. Value loss: 0.043338. Entropy: 0.303790.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3664: Policy loss: -0.118553. Value loss: 0.123635. Entropy: 0.304029.\n",
      "Iteration 3665: Policy loss: -0.121588. Value loss: 0.054397. Entropy: 0.305504.\n",
      "Iteration 3666: Policy loss: -0.116350. Value loss: 0.038892. Entropy: 0.304754.\n",
      "episode: 1549   score: 100.0  epsilon: 1.0    steps: 88  evaluation reward: 207.0\n",
      "episode: 1550   score: 190.0  epsilon: 1.0    steps: 136  evaluation reward: 206.3\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3667: Policy loss: -0.341653. Value loss: 0.378429. Entropy: 0.304182.\n",
      "Iteration 3668: Policy loss: -0.381847. Value loss: 0.234743. Entropy: 0.304590.\n",
      "Iteration 3669: Policy loss: -0.389447. Value loss: 0.192975. Entropy: 0.304513.\n",
      "now time :  2019-09-05 18:03:05.832726\n",
      "episode: 1551   score: 360.0  epsilon: 1.0    steps: 264  evaluation reward: 207.0\n",
      "episode: 1552   score: 145.0  epsilon: 1.0    steps: 968  evaluation reward: 207.1\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3670: Policy loss: 0.041292. Value loss: 0.145918. Entropy: 0.301984.\n",
      "Iteration 3671: Policy loss: 0.027245. Value loss: 0.063959. Entropy: 0.300912.\n",
      "Iteration 3672: Policy loss: 0.028913. Value loss: 0.047843. Entropy: 0.300808.\n",
      "episode: 1553   score: 135.0  epsilon: 1.0    steps: 120  evaluation reward: 205.85\n",
      "episode: 1554   score: 180.0  epsilon: 1.0    steps: 784  evaluation reward: 206.55\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3673: Policy loss: -0.197678. Value loss: 0.114463. Entropy: 0.303011.\n",
      "Iteration 3674: Policy loss: -0.202627. Value loss: 0.053532. Entropy: 0.302967.\n",
      "Iteration 3675: Policy loss: -0.206165. Value loss: 0.039335. Entropy: 0.303524.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3676: Policy loss: 0.020100. Value loss: 0.144125. Entropy: 0.305261.\n",
      "Iteration 3677: Policy loss: 0.013055. Value loss: 0.086841. Entropy: 0.306289.\n",
      "Iteration 3678: Policy loss: 0.010242. Value loss: 0.066482. Entropy: 0.304934.\n",
      "episode: 1555   score: 145.0  epsilon: 1.0    steps: 248  evaluation reward: 206.75\n",
      "episode: 1556   score: 655.0  epsilon: 1.0    steps: 624  evaluation reward: 212.2\n",
      "episode: 1557   score: 110.0  epsilon: 1.0    steps: 784  evaluation reward: 212.35\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3679: Policy loss: 0.121263. Value loss: 0.099903. Entropy: 0.293679.\n",
      "Iteration 3680: Policy loss: 0.112797. Value loss: 0.037214. Entropy: 0.295859.\n",
      "Iteration 3681: Policy loss: 0.110213. Value loss: 0.026395. Entropy: 0.293429.\n",
      "episode: 1558   score: 180.0  epsilon: 1.0    steps: 1016  evaluation reward: 211.75\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3682: Policy loss: -0.071112. Value loss: 0.113821. Entropy: 0.310595.\n",
      "Iteration 3683: Policy loss: -0.072867. Value loss: 0.041401. Entropy: 0.308986.\n",
      "Iteration 3684: Policy loss: -0.081807. Value loss: 0.027040. Entropy: 0.309135.\n",
      "episode: 1559   score: 350.0  epsilon: 1.0    steps: 200  evaluation reward: 213.45\n",
      "episode: 1560   score: 140.0  epsilon: 1.0    steps: 656  evaluation reward: 212.75\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3685: Policy loss: 0.022486. Value loss: 0.118024. Entropy: 0.303668.\n",
      "Iteration 3686: Policy loss: 0.017242. Value loss: 0.044677. Entropy: 0.302973.\n",
      "Iteration 3687: Policy loss: 0.005702. Value loss: 0.029465. Entropy: 0.302992.\n",
      "episode: 1561   score: 240.0  epsilon: 1.0    steps: 888  evaluation reward: 212.7\n",
      "episode: 1562   score: 150.0  epsilon: 1.0    steps: 904  evaluation reward: 212.6\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3688: Policy loss: 0.295689. Value loss: 0.109047. Entropy: 0.296325.\n",
      "Iteration 3689: Policy loss: 0.293706. Value loss: 0.048280. Entropy: 0.298350.\n",
      "Iteration 3690: Policy loss: 0.292793. Value loss: 0.035678. Entropy: 0.296735.\n",
      "episode: 1563   score: 75.0  epsilon: 1.0    steps: 1000  evaluation reward: 211.25\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3691: Policy loss: 0.340303. Value loss: 0.180760. Entropy: 0.304730.\n",
      "Iteration 3692: Policy loss: 0.334750. Value loss: 0.092375. Entropy: 0.303447.\n",
      "Iteration 3693: Policy loss: 0.330474. Value loss: 0.065509. Entropy: 0.303529.\n",
      "episode: 1564   score: 120.0  epsilon: 1.0    steps: 720  evaluation reward: 210.35\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3694: Policy loss: -0.023057. Value loss: 0.123155. Entropy: 0.301607.\n",
      "Iteration 3695: Policy loss: -0.026180. Value loss: 0.069106. Entropy: 0.302320.\n",
      "Iteration 3696: Policy loss: -0.031284. Value loss: 0.052098. Entropy: 0.302749.\n",
      "episode: 1565   score: 215.0  epsilon: 1.0    steps: 880  evaluation reward: 211.15\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3697: Policy loss: 0.258471. Value loss: 0.128412. Entropy: 0.303409.\n",
      "Iteration 3698: Policy loss: 0.249927. Value loss: 0.047765. Entropy: 0.302089.\n",
      "Iteration 3699: Policy loss: 0.244154. Value loss: 0.034541. Entropy: 0.302072.\n",
      "Training network. lr: 0.000222. clip: 0.088793\n",
      "Iteration 3700: Policy loss: -0.131067. Value loss: 0.111990. Entropy: 0.305042.\n",
      "Iteration 3701: Policy loss: -0.130572. Value loss: 0.058742. Entropy: 0.304224.\n",
      "Iteration 3702: Policy loss: -0.132253. Value loss: 0.047226. Entropy: 0.304256.\n",
      "episode: 1566   score: 285.0  epsilon: 1.0    steps: 184  evaluation reward: 211.4\n",
      "episode: 1567   score: 155.0  epsilon: 1.0    steps: 224  evaluation reward: 211.4\n",
      "episode: 1568   score: 175.0  epsilon: 1.0    steps: 544  evaluation reward: 211.05\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3703: Policy loss: 0.166725. Value loss: 0.045682. Entropy: 0.295321.\n",
      "Iteration 3704: Policy loss: 0.164983. Value loss: 0.022275. Entropy: 0.293230.\n",
      "Iteration 3705: Policy loss: 0.158513. Value loss: 0.017899. Entropy: 0.293883.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1569   score: 240.0  epsilon: 1.0    steps: 192  evaluation reward: 212.4\n",
      "episode: 1570   score: 105.0  epsilon: 1.0    steps: 672  evaluation reward: 211.3\n",
      "episode: 1571   score: 125.0  epsilon: 1.0    steps: 816  evaluation reward: 211.1\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3706: Policy loss: 0.092202. Value loss: 0.207010. Entropy: 0.294404.\n",
      "Iteration 3707: Policy loss: 0.059726. Value loss: 0.125320. Entropy: 0.290657.\n",
      "Iteration 3708: Policy loss: 0.051847. Value loss: 0.102046. Entropy: 0.289225.\n",
      "episode: 1572   score: 715.0  epsilon: 1.0    steps: 816  evaluation reward: 215.8\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3709: Policy loss: 0.145706. Value loss: 0.112369. Entropy: 0.303970.\n",
      "Iteration 3710: Policy loss: 0.142026. Value loss: 0.042241. Entropy: 0.300961.\n",
      "Iteration 3711: Policy loss: 0.123893. Value loss: 0.034314. Entropy: 0.301465.\n",
      "episode: 1573   score: 65.0  epsilon: 1.0    steps: 352  evaluation reward: 214.65\n",
      "episode: 1574   score: 50.0  epsilon: 1.0    steps: 392  evaluation reward: 211.4\n",
      "episode: 1575   score: 155.0  epsilon: 1.0    steps: 648  evaluation reward: 210.1\n",
      "episode: 1576   score: 125.0  epsilon: 1.0    steps: 1000  evaluation reward: 209.75\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3712: Policy loss: 0.045237. Value loss: 0.067999. Entropy: 0.290834.\n",
      "Iteration 3713: Policy loss: 0.037504. Value loss: 0.037447. Entropy: 0.291254.\n",
      "Iteration 3714: Policy loss: 0.036355. Value loss: 0.030670. Entropy: 0.289726.\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3715: Policy loss: 0.306604. Value loss: 0.098919. Entropy: 0.304472.\n",
      "Iteration 3716: Policy loss: 0.306424. Value loss: 0.035480. Entropy: 0.305647.\n",
      "Iteration 3717: Policy loss: 0.294837. Value loss: 0.024715. Entropy: 0.302833.\n",
      "episode: 1577   score: 125.0  epsilon: 1.0    steps: 464  evaluation reward: 208.6\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3718: Policy loss: -0.002318. Value loss: 0.110040. Entropy: 0.296853.\n",
      "Iteration 3719: Policy loss: -0.004330. Value loss: 0.053297. Entropy: 0.294918.\n",
      "Iteration 3720: Policy loss: -0.012029. Value loss: 0.040071. Entropy: 0.296721.\n",
      "episode: 1578   score: 195.0  epsilon: 1.0    steps: 328  evaluation reward: 208.7\n",
      "episode: 1579   score: 215.0  epsilon: 1.0    steps: 800  evaluation reward: 209.65\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3721: Policy loss: -0.090136. Value loss: 0.093551. Entropy: 0.296215.\n",
      "Iteration 3722: Policy loss: -0.099046. Value loss: 0.042900. Entropy: 0.295785.\n",
      "Iteration 3723: Policy loss: -0.098351. Value loss: 0.034122. Entropy: 0.297365.\n",
      "episode: 1580   score: 155.0  epsilon: 1.0    steps: 1000  evaluation reward: 210.15\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3724: Policy loss: -0.127349. Value loss: 0.088773. Entropy: 0.305323.\n",
      "Iteration 3725: Policy loss: -0.140277. Value loss: 0.041058. Entropy: 0.304936.\n",
      "Iteration 3726: Policy loss: -0.133083. Value loss: 0.030933. Entropy: 0.304662.\n",
      "episode: 1581   score: 135.0  epsilon: 1.0    steps: 384  evaluation reward: 209.4\n",
      "episode: 1582   score: 190.0  epsilon: 1.0    steps: 904  evaluation reward: 210.85\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3727: Policy loss: 0.000867. Value loss: 0.102033. Entropy: 0.289425.\n",
      "Iteration 3728: Policy loss: -0.009637. Value loss: 0.035788. Entropy: 0.288859.\n",
      "Iteration 3729: Policy loss: -0.009891. Value loss: 0.028725. Entropy: 0.291017.\n",
      "episode: 1583   score: 210.0  epsilon: 1.0    steps: 1000  evaluation reward: 208.4\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3730: Policy loss: 0.004885. Value loss: 0.109757. Entropy: 0.304871.\n",
      "Iteration 3731: Policy loss: -0.004844. Value loss: 0.056643. Entropy: 0.303859.\n",
      "Iteration 3732: Policy loss: -0.009233. Value loss: 0.037934. Entropy: 0.303556.\n",
      "episode: 1584   score: 350.0  epsilon: 1.0    steps: 56  evaluation reward: 209.55\n",
      "episode: 1585   score: 135.0  epsilon: 1.0    steps: 528  evaluation reward: 210.45\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3733: Policy loss: 0.291307. Value loss: 0.068875. Entropy: 0.284437.\n",
      "Iteration 3734: Policy loss: 0.284590. Value loss: 0.025276. Entropy: 0.283820.\n",
      "Iteration 3735: Policy loss: 0.279394. Value loss: 0.020261. Entropy: 0.279204.\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3736: Policy loss: 0.021058. Value loss: 0.108111. Entropy: 0.307638.\n",
      "Iteration 3737: Policy loss: 0.013933. Value loss: 0.052288. Entropy: 0.307190.\n",
      "Iteration 3738: Policy loss: 0.017409. Value loss: 0.034282. Entropy: 0.307657.\n",
      "episode: 1586   score: 155.0  epsilon: 1.0    steps: 904  evaluation reward: 207.65\n",
      "episode: 1587   score: 50.0  epsilon: 1.0    steps: 960  evaluation reward: 204.4\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3739: Policy loss: -0.421393. Value loss: 0.311426. Entropy: 0.296036.\n",
      "Iteration 3740: Policy loss: -0.460207. Value loss: 0.199061. Entropy: 0.296106.\n",
      "Iteration 3741: Policy loss: -0.472669. Value loss: 0.092912. Entropy: 0.296372.\n",
      "episode: 1588   score: 410.0  epsilon: 1.0    steps: 104  evaluation reward: 206.7\n",
      "episode: 1589   score: 135.0  epsilon: 1.0    steps: 240  evaluation reward: 205.8\n",
      "episode: 1590   score: 390.0  epsilon: 1.0    steps: 992  evaluation reward: 209.4\n",
      "episode: 1591   score: 65.0  epsilon: 1.0    steps: 1024  evaluation reward: 208.35\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3742: Policy loss: -0.151409. Value loss: 0.133099. Entropy: 0.267378.\n",
      "Iteration 3743: Policy loss: -0.157969. Value loss: 0.048121. Entropy: 0.274091.\n",
      "Iteration 3744: Policy loss: -0.155930. Value loss: 0.031213. Entropy: 0.274152.\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3745: Policy loss: -0.183166. Value loss: 0.129722. Entropy: 0.292865.\n",
      "Iteration 3746: Policy loss: -0.190971. Value loss: 0.054691. Entropy: 0.292687.\n",
      "Iteration 3747: Policy loss: -0.190446. Value loss: 0.045217. Entropy: 0.289816.\n",
      "Training network. lr: 0.000222. clip: 0.088637\n",
      "Iteration 3748: Policy loss: -0.008735. Value loss: 0.106405. Entropy: 0.305724.\n",
      "Iteration 3749: Policy loss: -0.021218. Value loss: 0.046928. Entropy: 0.306286.\n",
      "Iteration 3750: Policy loss: -0.016938. Value loss: 0.035869. Entropy: 0.305803.\n",
      "episode: 1592   score: 485.0  epsilon: 1.0    steps: 48  evaluation reward: 212.25\n",
      "episode: 1593   score: 110.0  epsilon: 1.0    steps: 280  evaluation reward: 210.3\n",
      "episode: 1594   score: 285.0  epsilon: 1.0    steps: 856  evaluation reward: 212.25\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3751: Policy loss: 0.223391. Value loss: 0.099512. Entropy: 0.282364.\n",
      "Iteration 3752: Policy loss: 0.217194. Value loss: 0.041336. Entropy: 0.281365.\n",
      "Iteration 3753: Policy loss: 0.218456. Value loss: 0.031575. Entropy: 0.284674.\n",
      "episode: 1595   score: 165.0  epsilon: 1.0    steps: 824  evaluation reward: 210.8\n",
      "episode: 1596   score: 180.0  epsilon: 1.0    steps: 1016  evaluation reward: 210.45\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3754: Policy loss: 0.103762. Value loss: 0.110211. Entropy: 0.297491.\n",
      "Iteration 3755: Policy loss: 0.100874. Value loss: 0.042306. Entropy: 0.297183.\n",
      "Iteration 3756: Policy loss: 0.099134. Value loss: 0.032155. Entropy: 0.297360.\n",
      "episode: 1597   score: 175.0  epsilon: 1.0    steps: 24  evaluation reward: 209.25\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3757: Policy loss: 0.071254. Value loss: 0.080234. Entropy: 0.287258.\n",
      "Iteration 3758: Policy loss: 0.065226. Value loss: 0.032666. Entropy: 0.283802.\n",
      "Iteration 3759: Policy loss: 0.067602. Value loss: 0.020464. Entropy: 0.287355.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3760: Policy loss: -0.076216. Value loss: 0.141542. Entropy: 0.298509.\n",
      "Iteration 3761: Policy loss: -0.070770. Value loss: 0.062573. Entropy: 0.298407.\n",
      "Iteration 3762: Policy loss: -0.080702. Value loss: 0.052030. Entropy: 0.297747.\n",
      "episode: 1598   score: 130.0  epsilon: 1.0    steps: 464  evaluation reward: 206.75\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3763: Policy loss: 0.034023. Value loss: 0.071455. Entropy: 0.292767.\n",
      "Iteration 3764: Policy loss: 0.032013. Value loss: 0.034339. Entropy: 0.292714.\n",
      "Iteration 3765: Policy loss: 0.024702. Value loss: 0.023905. Entropy: 0.290765.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1599   score: 270.0  epsilon: 1.0    steps: 304  evaluation reward: 208.7\n",
      "episode: 1600   score: 125.0  epsilon: 1.0    steps: 656  evaluation reward: 209.2\n",
      "now time :  2019-09-05 18:09:05.471321\n",
      "episode: 1601   score: 290.0  epsilon: 1.0    steps: 952  evaluation reward: 211.8\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3766: Policy loss: 0.012848. Value loss: 0.111114. Entropy: 0.278083.\n",
      "Iteration 3767: Policy loss: 0.008256. Value loss: 0.056457. Entropy: 0.277805.\n",
      "Iteration 3768: Policy loss: 0.001604. Value loss: 0.047326. Entropy: 0.276056.\n",
      "episode: 1602   score: 160.0  epsilon: 1.0    steps: 264  evaluation reward: 212.45\n",
      "episode: 1603   score: 210.0  epsilon: 1.0    steps: 888  evaluation reward: 213.35\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3769: Policy loss: -0.070339. Value loss: 0.067852. Entropy: 0.280204.\n",
      "Iteration 3770: Policy loss: -0.068157. Value loss: 0.031027. Entropy: 0.282329.\n",
      "Iteration 3771: Policy loss: -0.078059. Value loss: 0.027047. Entropy: 0.285283.\n",
      "episode: 1604   score: 210.0  epsilon: 1.0    steps: 408  evaluation reward: 213.6\n",
      "episode: 1605   score: 260.0  epsilon: 1.0    steps: 912  evaluation reward: 213.9\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3772: Policy loss: 0.155827. Value loss: 0.106264. Entropy: 0.288445.\n",
      "Iteration 3773: Policy loss: 0.154675. Value loss: 0.044775. Entropy: 0.290258.\n",
      "Iteration 3774: Policy loss: 0.147481. Value loss: 0.036953. Entropy: 0.288819.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3775: Policy loss: -0.038629. Value loss: 0.129843. Entropy: 0.303981.\n",
      "Iteration 3776: Policy loss: -0.042200. Value loss: 0.056572. Entropy: 0.302237.\n",
      "Iteration 3777: Policy loss: -0.048242. Value loss: 0.044983. Entropy: 0.302475.\n",
      "episode: 1606   score: 160.0  epsilon: 1.0    steps: 512  evaluation reward: 213.4\n",
      "episode: 1607   score: 155.0  epsilon: 1.0    steps: 744  evaluation reward: 213.6\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3778: Policy loss: -0.097235. Value loss: 0.159680. Entropy: 0.288946.\n",
      "Iteration 3779: Policy loss: -0.105706. Value loss: 0.068938. Entropy: 0.289467.\n",
      "Iteration 3780: Policy loss: -0.117193. Value loss: 0.046157. Entropy: 0.289287.\n",
      "episode: 1608   score: 245.0  epsilon: 1.0    steps: 504  evaluation reward: 214.85\n",
      "episode: 1609   score: 60.0  epsilon: 1.0    steps: 784  evaluation reward: 214.55\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3781: Policy loss: 0.373453. Value loss: 0.114986. Entropy: 0.286186.\n",
      "Iteration 3782: Policy loss: 0.369664. Value loss: 0.038802. Entropy: 0.284069.\n",
      "Iteration 3783: Policy loss: 0.363271. Value loss: 0.027783. Entropy: 0.282790.\n",
      "episode: 1610   score: 90.0  epsilon: 1.0    steps: 480  evaluation reward: 211.35\n",
      "episode: 1611   score: 205.0  epsilon: 1.0    steps: 720  evaluation reward: 211.6\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3784: Policy loss: 0.212698. Value loss: 0.076789. Entropy: 0.287574.\n",
      "Iteration 3785: Policy loss: 0.206654. Value loss: 0.039383. Entropy: 0.287626.\n",
      "Iteration 3786: Policy loss: 0.205517. Value loss: 0.031041. Entropy: 0.287980.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3787: Policy loss: 0.040817. Value loss: 0.116017. Entropy: 0.302241.\n",
      "Iteration 3788: Policy loss: 0.043017. Value loss: 0.047771. Entropy: 0.303725.\n",
      "Iteration 3789: Policy loss: 0.028553. Value loss: 0.038747. Entropy: 0.302149.\n",
      "episode: 1612   score: 60.0  epsilon: 1.0    steps: 744  evaluation reward: 210.05\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3790: Policy loss: -0.510686. Value loss: 0.360220. Entropy: 0.297880.\n",
      "Iteration 3791: Policy loss: -0.511789. Value loss: 0.250477. Entropy: 0.300495.\n",
      "Iteration 3792: Policy loss: -0.509274. Value loss: 0.155574. Entropy: 0.298391.\n",
      "episode: 1613   score: 455.0  epsilon: 1.0    steps: 384  evaluation reward: 213.55\n",
      "episode: 1614   score: 255.0  epsilon: 1.0    steps: 408  evaluation reward: 212.6\n",
      "episode: 1615   score: 155.0  epsilon: 1.0    steps: 432  evaluation reward: 212.2\n",
      "episode: 1616   score: 180.0  epsilon: 1.0    steps: 928  evaluation reward: 211.3\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3793: Policy loss: 0.124540. Value loss: 0.089666. Entropy: 0.275754.\n",
      "Iteration 3794: Policy loss: 0.124837. Value loss: 0.040822. Entropy: 0.284109.\n",
      "Iteration 3795: Policy loss: 0.112131. Value loss: 0.033753. Entropy: 0.275513.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3796: Policy loss: 0.101351. Value loss: 0.085750. Entropy: 0.298931.\n",
      "Iteration 3797: Policy loss: 0.101042. Value loss: 0.036677. Entropy: 0.299884.\n",
      "Iteration 3798: Policy loss: 0.098552. Value loss: 0.025709. Entropy: 0.301520.\n",
      "Training network. lr: 0.000221. clip: 0.088489\n",
      "Iteration 3799: Policy loss: -0.044826. Value loss: 0.118092. Entropy: 0.305569.\n",
      "Iteration 3800: Policy loss: -0.053656. Value loss: 0.052827. Entropy: 0.306496.\n",
      "Iteration 3801: Policy loss: -0.057433. Value loss: 0.036035. Entropy: 0.306952.\n",
      "episode: 1617   score: 270.0  epsilon: 1.0    steps: 88  evaluation reward: 209.1\n",
      "episode: 1618   score: 30.0  epsilon: 1.0    steps: 776  evaluation reward: 205.95\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3802: Policy loss: -0.094815. Value loss: 0.144734. Entropy: 0.284263.\n",
      "Iteration 3803: Policy loss: -0.098789. Value loss: 0.066680. Entropy: 0.283441.\n",
      "Iteration 3804: Policy loss: -0.099976. Value loss: 0.048136. Entropy: 0.286791.\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3805: Policy loss: -0.147161. Value loss: 0.103326. Entropy: 0.305236.\n",
      "Iteration 3806: Policy loss: -0.154624. Value loss: 0.040569. Entropy: 0.305386.\n",
      "Iteration 3807: Policy loss: -0.158301. Value loss: 0.027333. Entropy: 0.305341.\n",
      "episode: 1619   score: 260.0  epsilon: 1.0    steps: 392  evaluation reward: 206.75\n",
      "episode: 1620   score: 85.0  epsilon: 1.0    steps: 936  evaluation reward: 206.6\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3808: Policy loss: 0.074249. Value loss: 0.093832. Entropy: 0.287650.\n",
      "Iteration 3809: Policy loss: 0.066595. Value loss: 0.035293. Entropy: 0.285585.\n",
      "Iteration 3810: Policy loss: 0.063217. Value loss: 0.027967. Entropy: 0.285319.\n",
      "episode: 1621   score: 210.0  epsilon: 1.0    steps: 128  evaluation reward: 206.6\n",
      "episode: 1622   score: 225.0  epsilon: 1.0    steps: 696  evaluation reward: 207.05\n",
      "episode: 1623   score: 345.0  epsilon: 1.0    steps: 712  evaluation reward: 207.6\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3811: Policy loss: -0.157397. Value loss: 0.324050. Entropy: 0.254385.\n",
      "Iteration 3812: Policy loss: -0.167721. Value loss: 0.202639. Entropy: 0.256615.\n",
      "Iteration 3813: Policy loss: -0.145413. Value loss: 0.132425. Entropy: 0.256079.\n",
      "episode: 1624   score: 250.0  epsilon: 1.0    steps: 168  evaluation reward: 209.3\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3814: Policy loss: -0.026253. Value loss: 0.106149. Entropy: 0.284135.\n",
      "Iteration 3815: Policy loss: -0.028696. Value loss: 0.043844. Entropy: 0.278225.\n",
      "Iteration 3816: Policy loss: -0.032447. Value loss: 0.033321. Entropy: 0.279459.\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3817: Policy loss: -0.195610. Value loss: 0.120090. Entropy: 0.302538.\n",
      "Iteration 3818: Policy loss: -0.200733. Value loss: 0.057377. Entropy: 0.303699.\n",
      "Iteration 3819: Policy loss: -0.203467. Value loss: 0.040313. Entropy: 0.302693.\n",
      "episode: 1625   score: 385.0  epsilon: 1.0    steps: 200  evaluation reward: 211.35\n",
      "episode: 1626   score: 210.0  epsilon: 1.0    steps: 632  evaluation reward: 209.35\n",
      "episode: 1627   score: 55.0  epsilon: 1.0    steps: 736  evaluation reward: 207.3\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3820: Policy loss: 0.174917. Value loss: 0.085381. Entropy: 0.262885.\n",
      "Iteration 3821: Policy loss: 0.169238. Value loss: 0.033067. Entropy: 0.263548.\n",
      "Iteration 3822: Policy loss: 0.172583. Value loss: 0.023882. Entropy: 0.263912.\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3823: Policy loss: -0.051103. Value loss: 0.153733. Entropy: 0.300536.\n",
      "Iteration 3824: Policy loss: -0.056853. Value loss: 0.067343. Entropy: 0.299550.\n",
      "Iteration 3825: Policy loss: -0.070041. Value loss: 0.050241. Entropy: 0.300054.\n",
      "episode: 1628   score: 160.0  epsilon: 1.0    steps: 32  evaluation reward: 207.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3826: Policy loss: -0.007195. Value loss: 0.130759. Entropy: 0.292389.\n",
      "Iteration 3827: Policy loss: -0.013108. Value loss: 0.043033. Entropy: 0.292133.\n",
      "Iteration 3828: Policy loss: -0.015269. Value loss: 0.029916. Entropy: 0.290458.\n",
      "episode: 1629   score: 240.0  epsilon: 1.0    steps: 136  evaluation reward: 203.15\n",
      "episode: 1630   score: 210.0  epsilon: 1.0    steps: 240  evaluation reward: 204.0\n",
      "episode: 1631   score: 215.0  epsilon: 1.0    steps: 256  evaluation reward: 204.6\n",
      "episode: 1632   score: 75.0  epsilon: 1.0    steps: 784  evaluation reward: 201.6\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3829: Policy loss: 0.120638. Value loss: 0.066699. Entropy: 0.247401.\n",
      "Iteration 3830: Policy loss: 0.114227. Value loss: 0.037506. Entropy: 0.252817.\n",
      "Iteration 3831: Policy loss: 0.116463. Value loss: 0.030481. Entropy: 0.249189.\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3832: Policy loss: 0.155883. Value loss: 0.096618. Entropy: 0.297487.\n",
      "Iteration 3833: Policy loss: 0.158895. Value loss: 0.037288. Entropy: 0.293180.\n",
      "Iteration 3834: Policy loss: 0.149781. Value loss: 0.027843. Entropy: 0.295519.\n",
      "episode: 1633   score: 210.0  epsilon: 1.0    steps: 432  evaluation reward: 200.5\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3835: Policy loss: 0.057614. Value loss: 0.087507. Entropy: 0.286496.\n",
      "Iteration 3836: Policy loss: 0.057048. Value loss: 0.045814. Entropy: 0.286855.\n",
      "Iteration 3837: Policy loss: 0.053576. Value loss: 0.032877. Entropy: 0.288064.\n",
      "episode: 1634   score: 65.0  epsilon: 1.0    steps: 240  evaluation reward: 200.55\n",
      "episode: 1635   score: 110.0  epsilon: 1.0    steps: 936  evaluation reward: 198.0\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3838: Policy loss: 0.112419. Value loss: 0.088297. Entropy: 0.276447.\n",
      "Iteration 3839: Policy loss: 0.107333. Value loss: 0.038411. Entropy: 0.277299.\n",
      "Iteration 3840: Policy loss: 0.102526. Value loss: 0.026110. Entropy: 0.276677.\n",
      "episode: 1636   score: 115.0  epsilon: 1.0    steps: 32  evaluation reward: 197.5\n",
      "episode: 1637   score: 100.0  epsilon: 1.0    steps: 568  evaluation reward: 196.7\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3841: Policy loss: 0.096133. Value loss: 0.051125. Entropy: 0.267509.\n",
      "Iteration 3842: Policy loss: 0.085785. Value loss: 0.024534. Entropy: 0.266146.\n",
      "Iteration 3843: Policy loss: 0.090750. Value loss: 0.019648. Entropy: 0.264909.\n",
      "episode: 1638   score: 80.0  epsilon: 1.0    steps: 128  evaluation reward: 194.65\n",
      "episode: 1639   score: 30.0  epsilon: 1.0    steps: 240  evaluation reward: 193.4\n",
      "episode: 1640   score: 265.0  epsilon: 1.0    steps: 568  evaluation reward: 195.3\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3844: Policy loss: 0.055272. Value loss: 0.076846. Entropy: 0.263479.\n",
      "Iteration 3845: Policy loss: 0.048320. Value loss: 0.036056. Entropy: 0.263013.\n",
      "Iteration 3846: Policy loss: 0.041241. Value loss: 0.028634. Entropy: 0.263733.\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3847: Policy loss: -0.132071. Value loss: 0.126616. Entropy: 0.295241.\n",
      "Iteration 3848: Policy loss: -0.131202. Value loss: 0.053844. Entropy: 0.298666.\n",
      "Iteration 3849: Policy loss: -0.151063. Value loss: 0.037695. Entropy: 0.299143.\n",
      "episode: 1641   score: 75.0  epsilon: 1.0    steps: 192  evaluation reward: 194.35\n",
      "episode: 1642   score: 420.0  epsilon: 1.0    steps: 480  evaluation reward: 196.4\n",
      "Training network. lr: 0.000221. clip: 0.088333\n",
      "Iteration 3850: Policy loss: 0.051233. Value loss: 0.078428. Entropy: 0.280074.\n",
      "Iteration 3851: Policy loss: 0.052046. Value loss: 0.044075. Entropy: 0.278876.\n",
      "Iteration 3852: Policy loss: 0.047059. Value loss: 0.031738. Entropy: 0.278114.\n",
      "episode: 1643   score: 80.0  epsilon: 1.0    steps: 240  evaluation reward: 196.7\n",
      "episode: 1644   score: 110.0  epsilon: 1.0    steps: 432  evaluation reward: 196.6\n",
      "episode: 1645   score: 140.0  epsilon: 1.0    steps: 976  evaluation reward: 196.85\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3853: Policy loss: -0.090239. Value loss: 0.056350. Entropy: 0.281806.\n",
      "Iteration 3854: Policy loss: -0.094604. Value loss: 0.029469. Entropy: 0.280134.\n",
      "Iteration 3855: Policy loss: -0.095469. Value loss: 0.022766. Entropy: 0.279667.\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3856: Policy loss: -0.169437. Value loss: 0.077945. Entropy: 0.300945.\n",
      "Iteration 3857: Policy loss: -0.168504. Value loss: 0.033565. Entropy: 0.299224.\n",
      "Iteration 3858: Policy loss: -0.179155. Value loss: 0.024494. Entropy: 0.300354.\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3859: Policy loss: -0.191046. Value loss: 0.244624. Entropy: 0.310657.\n",
      "Iteration 3860: Policy loss: -0.178240. Value loss: 0.100487. Entropy: 0.311393.\n",
      "Iteration 3861: Policy loss: -0.179812. Value loss: 0.055499. Entropy: 0.310704.\n",
      "episode: 1646   score: 345.0  epsilon: 1.0    steps: 24  evaluation reward: 198.7\n",
      "episode: 1647   score: 210.0  epsilon: 1.0    steps: 272  evaluation reward: 196.65\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3862: Policy loss: 0.026639. Value loss: 0.048443. Entropy: 0.280463.\n",
      "Iteration 3863: Policy loss: 0.017301. Value loss: 0.020406. Entropy: 0.279927.\n",
      "Iteration 3864: Policy loss: 0.020526. Value loss: 0.015616. Entropy: 0.280703.\n",
      "episode: 1648   score: 285.0  epsilon: 1.0    steps: 688  evaluation reward: 195.4\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3865: Policy loss: 0.079654. Value loss: 0.075067. Entropy: 0.295767.\n",
      "Iteration 3866: Policy loss: 0.067541. Value loss: 0.026722. Entropy: 0.297044.\n",
      "Iteration 3867: Policy loss: 0.072169. Value loss: 0.017758. Entropy: 0.296274.\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3868: Policy loss: -0.203668. Value loss: 0.265394. Entropy: 0.306778.\n",
      "Iteration 3869: Policy loss: -0.208353. Value loss: 0.084172. Entropy: 0.306479.\n",
      "Iteration 3870: Policy loss: -0.220383. Value loss: 0.042686. Entropy: 0.307895.\n",
      "episode: 1649   score: 265.0  epsilon: 1.0    steps: 872  evaluation reward: 197.05\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3871: Policy loss: 0.146957. Value loss: 0.139568. Entropy: 0.302799.\n",
      "Iteration 3872: Policy loss: 0.147684. Value loss: 0.068144. Entropy: 0.302285.\n",
      "Iteration 3873: Policy loss: 0.142607. Value loss: 0.043355. Entropy: 0.301995.\n",
      "episode: 1650   score: 380.0  epsilon: 1.0    steps: 128  evaluation reward: 198.95\n",
      "now time :  2019-09-05 18:15:48.273135\n",
      "episode: 1651   score: 520.0  epsilon: 1.0    steps: 960  evaluation reward: 200.55\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3874: Policy loss: -0.026583. Value loss: 0.051431. Entropy: 0.293505.\n",
      "Iteration 3875: Policy loss: -0.031824. Value loss: 0.024550. Entropy: 0.293163.\n",
      "Iteration 3876: Policy loss: -0.031811. Value loss: 0.016154. Entropy: 0.291656.\n",
      "episode: 1652   score: 285.0  epsilon: 1.0    steps: 8  evaluation reward: 201.95\n",
      "episode: 1653   score: 105.0  epsilon: 1.0    steps: 352  evaluation reward: 201.65\n",
      "episode: 1654   score: 245.0  epsilon: 1.0    steps: 368  evaluation reward: 202.3\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3877: Policy loss: -0.004560. Value loss: 0.076468. Entropy: 0.264751.\n",
      "Iteration 3878: Policy loss: -0.009164. Value loss: 0.034001. Entropy: 0.261899.\n",
      "Iteration 3879: Policy loss: -0.007169. Value loss: 0.025665. Entropy: 0.263786.\n",
      "episode: 1655   score: 180.0  epsilon: 1.0    steps: 344  evaluation reward: 202.65\n",
      "episode: 1656   score: 100.0  epsilon: 1.0    steps: 936  evaluation reward: 197.1\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3880: Policy loss: 0.008700. Value loss: 0.078611. Entropy: 0.293973.\n",
      "Iteration 3881: Policy loss: 0.002750. Value loss: 0.040467. Entropy: 0.292927.\n",
      "Iteration 3882: Policy loss: -0.000607. Value loss: 0.032054. Entropy: 0.293311.\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3883: Policy loss: 0.151072. Value loss: 0.072116. Entropy: 0.292757.\n",
      "Iteration 3884: Policy loss: 0.147353. Value loss: 0.031054. Entropy: 0.289069.\n",
      "Iteration 3885: Policy loss: 0.144159. Value loss: 0.024227. Entropy: 0.290212.\n",
      "episode: 1657   score: 285.0  epsilon: 1.0    steps: 320  evaluation reward: 198.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3886: Policy loss: -0.149572. Value loss: 0.072509. Entropy: 0.292866.\n",
      "Iteration 3887: Policy loss: -0.154069. Value loss: 0.030878. Entropy: 0.292401.\n",
      "Iteration 3888: Policy loss: -0.158255. Value loss: 0.024375. Entropy: 0.293168.\n",
      "episode: 1658   score: 135.0  epsilon: 1.0    steps: 984  evaluation reward: 198.4\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3889: Policy loss: 0.126642. Value loss: 0.051692. Entropy: 0.306677.\n",
      "Iteration 3890: Policy loss: 0.125306. Value loss: 0.019154. Entropy: 0.306370.\n",
      "Iteration 3891: Policy loss: 0.123872. Value loss: 0.013942. Entropy: 0.306528.\n",
      "episode: 1659   score: 150.0  epsilon: 1.0    steps: 336  evaluation reward: 196.4\n",
      "episode: 1660   score: 180.0  epsilon: 1.0    steps: 448  evaluation reward: 196.8\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3892: Policy loss: 0.064001. Value loss: 0.058275. Entropy: 0.269641.\n",
      "Iteration 3893: Policy loss: 0.063893. Value loss: 0.028552. Entropy: 0.270195.\n",
      "Iteration 3894: Policy loss: 0.057668. Value loss: 0.020087. Entropy: 0.271304.\n",
      "episode: 1661   score: 210.0  epsilon: 1.0    steps: 312  evaluation reward: 196.5\n",
      "episode: 1662   score: 180.0  epsilon: 1.0    steps: 448  evaluation reward: 196.8\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3895: Policy loss: 0.073728. Value loss: 0.127306. Entropy: 0.281497.\n",
      "Iteration 3896: Policy loss: 0.058235. Value loss: 0.053476. Entropy: 0.280866.\n",
      "Iteration 3897: Policy loss: 0.061217. Value loss: 0.035692. Entropy: 0.282208.\n",
      "Training network. lr: 0.000220. clip: 0.088176\n",
      "Iteration 3898: Policy loss: -0.085035. Value loss: 0.105916. Entropy: 0.303917.\n",
      "Iteration 3899: Policy loss: -0.089975. Value loss: 0.050544. Entropy: 0.305719.\n",
      "Iteration 3900: Policy loss: -0.096666. Value loss: 0.042466. Entropy: 0.304253.\n",
      "episode: 1663   score: 210.0  epsilon: 1.0    steps: 56  evaluation reward: 198.15\n",
      "episode: 1664   score: 205.0  epsilon: 1.0    steps: 176  evaluation reward: 199.0\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3901: Policy loss: 0.035648. Value loss: 0.055014. Entropy: 0.278408.\n",
      "Iteration 3902: Policy loss: 0.033132. Value loss: 0.024005. Entropy: 0.278510.\n",
      "Iteration 3903: Policy loss: 0.026801. Value loss: 0.016721. Entropy: 0.278078.\n",
      "episode: 1665   score: 210.0  epsilon: 1.0    steps: 608  evaluation reward: 198.95\n",
      "episode: 1666   score: 210.0  epsilon: 1.0    steps: 840  evaluation reward: 198.2\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3904: Policy loss: -0.030238. Value loss: 0.071337. Entropy: 0.287260.\n",
      "Iteration 3905: Policy loss: -0.038511. Value loss: 0.030537. Entropy: 0.287749.\n",
      "Iteration 3906: Policy loss: -0.039348. Value loss: 0.025411. Entropy: 0.286567.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3907: Policy loss: 0.114419. Value loss: 0.064352. Entropy: 0.303031.\n",
      "Iteration 3908: Policy loss: 0.116938. Value loss: 0.024199. Entropy: 0.305438.\n",
      "Iteration 3909: Policy loss: 0.114509. Value loss: 0.017978. Entropy: 0.302841.\n",
      "episode: 1667   score: 120.0  epsilon: 1.0    steps: 160  evaluation reward: 197.85\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3910: Policy loss: -0.357772. Value loss: 0.338904. Entropy: 0.296424.\n",
      "Iteration 3911: Policy loss: -0.323384. Value loss: 0.184927. Entropy: 0.296399.\n",
      "Iteration 3912: Policy loss: -0.363906. Value loss: 0.096533. Entropy: 0.296110.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3913: Policy loss: 0.018484. Value loss: 0.074371. Entropy: 0.309437.\n",
      "Iteration 3914: Policy loss: 0.009832. Value loss: 0.035355. Entropy: 0.309595.\n",
      "Iteration 3915: Policy loss: 0.007907. Value loss: 0.025932. Entropy: 0.309631.\n",
      "episode: 1668   score: 190.0  epsilon: 1.0    steps: 16  evaluation reward: 198.0\n",
      "episode: 1669   score: 380.0  epsilon: 1.0    steps: 112  evaluation reward: 199.4\n",
      "episode: 1670   score: 265.0  epsilon: 1.0    steps: 136  evaluation reward: 201.0\n",
      "episode: 1671   score: 270.0  epsilon: 1.0    steps: 696  evaluation reward: 202.45\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3916: Policy loss: 0.064089. Value loss: 0.094714. Entropy: 0.257228.\n",
      "Iteration 3917: Policy loss: 0.059770. Value loss: 0.044144. Entropy: 0.255372.\n",
      "Iteration 3918: Policy loss: 0.057811. Value loss: 0.032431. Entropy: 0.254892.\n",
      "episode: 1672   score: 155.0  epsilon: 1.0    steps: 408  evaluation reward: 196.85\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3919: Policy loss: -0.023484. Value loss: 0.077382. Entropy: 0.298085.\n",
      "Iteration 3920: Policy loss: -0.024798. Value loss: 0.036438. Entropy: 0.298516.\n",
      "Iteration 3921: Policy loss: -0.023447. Value loss: 0.028736. Entropy: 0.297328.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3922: Policy loss: -0.126446. Value loss: 0.082707. Entropy: 0.307863.\n",
      "Iteration 3923: Policy loss: -0.130503. Value loss: 0.035137. Entropy: 0.307436.\n",
      "Iteration 3924: Policy loss: -0.134332. Value loss: 0.026307. Entropy: 0.307673.\n",
      "episode: 1673   score: 210.0  epsilon: 1.0    steps: 64  evaluation reward: 198.3\n",
      "episode: 1674   score: 110.0  epsilon: 1.0    steps: 440  evaluation reward: 198.9\n",
      "episode: 1675   score: 225.0  epsilon: 1.0    steps: 616  evaluation reward: 199.6\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3925: Policy loss: 0.100874. Value loss: 0.053574. Entropy: 0.267963.\n",
      "Iteration 3926: Policy loss: 0.092747. Value loss: 0.021939. Entropy: 0.268295.\n",
      "Iteration 3927: Policy loss: 0.094025. Value loss: 0.015811. Entropy: 0.268324.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3928: Policy loss: 0.141016. Value loss: 0.089086. Entropy: 0.309217.\n",
      "Iteration 3929: Policy loss: 0.134563. Value loss: 0.035608. Entropy: 0.309089.\n",
      "Iteration 3930: Policy loss: 0.125338. Value loss: 0.022782. Entropy: 0.308200.\n",
      "episode: 1676   score: 170.0  epsilon: 1.0    steps: 112  evaluation reward: 200.05\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3931: Policy loss: -0.056451. Value loss: 0.054986. Entropy: 0.293286.\n",
      "Iteration 3932: Policy loss: -0.058687. Value loss: 0.024032. Entropy: 0.291714.\n",
      "Iteration 3933: Policy loss: -0.068449. Value loss: 0.018221. Entropy: 0.292854.\n",
      "episode: 1677   score: 185.0  epsilon: 1.0    steps: 456  evaluation reward: 200.65\n",
      "episode: 1678   score: 300.0  epsilon: 1.0    steps: 712  evaluation reward: 201.7\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3934: Policy loss: -0.003032. Value loss: 0.059985. Entropy: 0.284534.\n",
      "Iteration 3935: Policy loss: -0.007905. Value loss: 0.030890. Entropy: 0.284652.\n",
      "Iteration 3936: Policy loss: -0.005120. Value loss: 0.023681. Entropy: 0.283725.\n",
      "episode: 1679   score: 210.0  epsilon: 1.0    steps: 200  evaluation reward: 201.65\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3937: Policy loss: -0.114551. Value loss: 0.045338. Entropy: 0.290962.\n",
      "Iteration 3938: Policy loss: -0.115271. Value loss: 0.021712. Entropy: 0.289490.\n",
      "Iteration 3939: Policy loss: -0.117791. Value loss: 0.016690. Entropy: 0.290977.\n",
      "episode: 1680   score: 110.0  epsilon: 1.0    steps: 416  evaluation reward: 201.2\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3940: Policy loss: 0.122684. Value loss: 0.055786. Entropy: 0.294094.\n",
      "Iteration 3941: Policy loss: 0.117288. Value loss: 0.023916. Entropy: 0.293275.\n",
      "Iteration 3942: Policy loss: 0.116689. Value loss: 0.018833. Entropy: 0.292258.\n",
      "episode: 1681   score: 240.0  epsilon: 1.0    steps: 248  evaluation reward: 202.25\n",
      "episode: 1682   score: 210.0  epsilon: 1.0    steps: 504  evaluation reward: 202.45\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3943: Policy loss: 0.012218. Value loss: 0.023390. Entropy: 0.281077.\n",
      "Iteration 3944: Policy loss: 0.011104. Value loss: 0.012138. Entropy: 0.280572.\n",
      "Iteration 3945: Policy loss: 0.008627. Value loss: 0.009422. Entropy: 0.280792.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3946: Policy loss: -0.037634. Value loss: 0.111342. Entropy: 0.305525.\n",
      "Iteration 3947: Policy loss: -0.046054. Value loss: 0.043275. Entropy: 0.306182.\n",
      "Iteration 3948: Policy loss: -0.048799. Value loss: 0.030501. Entropy: 0.305396.\n",
      "Training network. lr: 0.000220. clip: 0.088028\n",
      "Iteration 3949: Policy loss: 0.001993. Value loss: 0.090546. Entropy: 0.308407.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3950: Policy loss: -0.002286. Value loss: 0.046216. Entropy: 0.307704.\n",
      "Iteration 3951: Policy loss: -0.007093. Value loss: 0.031152. Entropy: 0.306002.\n",
      "episode: 1683   score: 300.0  epsilon: 1.0    steps: 240  evaluation reward: 203.35\n",
      "episode: 1684   score: 490.0  epsilon: 1.0    steps: 952  evaluation reward: 204.75\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3952: Policy loss: -0.126781. Value loss: 0.091149. Entropy: 0.291628.\n",
      "Iteration 3953: Policy loss: -0.128222. Value loss: 0.045720. Entropy: 0.292247.\n",
      "Iteration 3954: Policy loss: -0.137355. Value loss: 0.036825. Entropy: 0.292324.\n",
      "episode: 1685   score: 210.0  epsilon: 1.0    steps: 312  evaluation reward: 205.5\n",
      "episode: 1686   score: 265.0  epsilon: 1.0    steps: 416  evaluation reward: 206.6\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3955: Policy loss: 0.109270. Value loss: 0.068462. Entropy: 0.271005.\n",
      "Iteration 3956: Policy loss: 0.105819. Value loss: 0.034096. Entropy: 0.270388.\n",
      "Iteration 3957: Policy loss: 0.097156. Value loss: 0.025334. Entropy: 0.270892.\n",
      "episode: 1687   score: 260.0  epsilon: 1.0    steps: 584  evaluation reward: 208.7\n",
      "episode: 1688   score: 180.0  epsilon: 1.0    steps: 712  evaluation reward: 206.4\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3958: Policy loss: 0.014706. Value loss: 0.066108. Entropy: 0.284151.\n",
      "Iteration 3959: Policy loss: 0.013159. Value loss: 0.039116. Entropy: 0.282485.\n",
      "Iteration 3960: Policy loss: 0.005158. Value loss: 0.028945. Entropy: 0.282640.\n",
      "episode: 1689   score: 80.0  epsilon: 1.0    steps: 584  evaluation reward: 205.85\n",
      "episode: 1690   score: 255.0  epsilon: 1.0    steps: 968  evaluation reward: 204.5\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3961: Policy loss: 0.122859. Value loss: 0.067462. Entropy: 0.289191.\n",
      "Iteration 3962: Policy loss: 0.120509. Value loss: 0.031957. Entropy: 0.290525.\n",
      "Iteration 3963: Policy loss: 0.118579. Value loss: 0.023836. Entropy: 0.287784.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3964: Policy loss: 0.009239. Value loss: 0.081144. Entropy: 0.296682.\n",
      "Iteration 3965: Policy loss: 0.005548. Value loss: 0.039644. Entropy: 0.296097.\n",
      "Iteration 3966: Policy loss: 0.002116. Value loss: 0.029452. Entropy: 0.295551.\n",
      "episode: 1691   score: 240.0  epsilon: 1.0    steps: 8  evaluation reward: 206.25\n",
      "episode: 1692   score: 120.0  epsilon: 1.0    steps: 224  evaluation reward: 202.6\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3967: Policy loss: 0.206230. Value loss: 0.065729. Entropy: 0.283896.\n",
      "Iteration 3968: Policy loss: 0.201608. Value loss: 0.031876. Entropy: 0.282404.\n",
      "Iteration 3969: Policy loss: 0.197290. Value loss: 0.025107. Entropy: 0.282858.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3970: Policy loss: -0.147339. Value loss: 0.307869. Entropy: 0.304209.\n",
      "Iteration 3971: Policy loss: -0.156337. Value loss: 0.114618. Entropy: 0.304748.\n",
      "Iteration 3972: Policy loss: -0.171679. Value loss: 0.072116. Entropy: 0.305758.\n",
      "episode: 1693   score: 210.0  epsilon: 1.0    steps: 680  evaluation reward: 203.6\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3973: Policy loss: 0.228271. Value loss: 0.061116. Entropy: 0.293927.\n",
      "Iteration 3974: Policy loss: 0.221515. Value loss: 0.022022. Entropy: 0.292536.\n",
      "Iteration 3975: Policy loss: 0.214111. Value loss: 0.017932. Entropy: 0.293353.\n",
      "episode: 1694   score: 410.0  epsilon: 1.0    steps: 16  evaluation reward: 204.85\n",
      "episode: 1695   score: 50.0  epsilon: 1.0    steps: 64  evaluation reward: 203.7\n",
      "episode: 1696   score: 105.0  epsilon: 1.0    steps: 648  evaluation reward: 202.95\n",
      "episode: 1697   score: 105.0  epsilon: 1.0    steps: 840  evaluation reward: 202.25\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3976: Policy loss: 0.002283. Value loss: 0.056570. Entropy: 0.261884.\n",
      "Iteration 3977: Policy loss: -0.002032. Value loss: 0.027810. Entropy: 0.262931.\n",
      "Iteration 3978: Policy loss: -0.000055. Value loss: 0.018625. Entropy: 0.260645.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3979: Policy loss: -0.158586. Value loss: 0.086130. Entropy: 0.301651.\n",
      "Iteration 3980: Policy loss: -0.161984. Value loss: 0.036980. Entropy: 0.301515.\n",
      "Iteration 3981: Policy loss: -0.165124. Value loss: 0.028227. Entropy: 0.300599.\n",
      "episode: 1698   score: 270.0  epsilon: 1.0    steps: 208  evaluation reward: 203.65\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3982: Policy loss: -0.318373. Value loss: 0.299289. Entropy: 0.296640.\n",
      "Iteration 3983: Policy loss: -0.343076. Value loss: 0.157152. Entropy: 0.293111.\n",
      "Iteration 3984: Policy loss: -0.342747. Value loss: 0.081959. Entropy: 0.294842.\n",
      "episode: 1699   score: 110.0  epsilon: 1.0    steps: 448  evaluation reward: 202.05\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3985: Policy loss: -0.099211. Value loss: 0.056854. Entropy: 0.295295.\n",
      "Iteration 3986: Policy loss: -0.097976. Value loss: 0.026907. Entropy: 0.295646.\n",
      "Iteration 3987: Policy loss: -0.102555. Value loss: 0.020181. Entropy: 0.296094.\n",
      "episode: 1700   score: 440.0  epsilon: 1.0    steps: 792  evaluation reward: 205.2\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3988: Policy loss: 0.070927. Value loss: 0.080921. Entropy: 0.299035.\n",
      "Iteration 3989: Policy loss: 0.061264. Value loss: 0.025890. Entropy: 0.299528.\n",
      "Iteration 3990: Policy loss: 0.060326. Value loss: 0.020725. Entropy: 0.298812.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3991: Policy loss: -0.012952. Value loss: 0.143769. Entropy: 0.307142.\n",
      "Iteration 3992: Policy loss: -0.032521. Value loss: 0.065336. Entropy: 0.305906.\n",
      "Iteration 3993: Policy loss: -0.036044. Value loss: 0.038079. Entropy: 0.306948.\n",
      "now time :  2019-09-05 18:23:13.925319\n",
      "episode: 1701   score: 210.0  epsilon: 1.0    steps: 528  evaluation reward: 204.4\n",
      "episode: 1702   score: 210.0  epsilon: 1.0    steps: 736  evaluation reward: 204.9\n",
      "episode: 1703   score: 285.0  epsilon: 1.0    steps: 960  evaluation reward: 205.65\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3994: Policy loss: 0.058512. Value loss: 0.093189. Entropy: 0.286620.\n",
      "Iteration 3995: Policy loss: 0.059104. Value loss: 0.037727. Entropy: 0.286817.\n",
      "Iteration 3996: Policy loss: 0.044326. Value loss: 0.028366. Entropy: 0.288717.\n",
      "episode: 1704   score: 270.0  epsilon: 1.0    steps: 368  evaluation reward: 206.25\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 3997: Policy loss: -0.031140. Value loss: 0.076320. Entropy: 0.289568.\n",
      "Iteration 3998: Policy loss: -0.038144. Value loss: 0.033922. Entropy: 0.288770.\n",
      "Iteration 3999: Policy loss: -0.036498. Value loss: 0.023018. Entropy: 0.289984.\n",
      "Training network. lr: 0.000220. clip: 0.087872\n",
      "Iteration 4000: Policy loss: 0.225186. Value loss: 0.085650. Entropy: 0.313818.\n",
      "Iteration 4001: Policy loss: 0.220264. Value loss: 0.041709. Entropy: 0.312704.\n",
      "Iteration 4002: Policy loss: 0.221131. Value loss: 0.029795. Entropy: 0.311982.\n",
      "episode: 1705   score: 260.0  epsilon: 1.0    steps: 16  evaluation reward: 206.25\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4003: Policy loss: -0.319930. Value loss: 0.324489. Entropy: 0.300831.\n",
      "Iteration 4004: Policy loss: -0.315645. Value loss: 0.171615. Entropy: 0.302040.\n",
      "Iteration 4005: Policy loss: -0.323530. Value loss: 0.090743. Entropy: 0.301939.\n",
      "episode: 1706   score: 235.0  epsilon: 1.0    steps: 8  evaluation reward: 207.0\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4006: Policy loss: 0.054028. Value loss: 0.116940. Entropy: 0.301517.\n",
      "Iteration 4007: Policy loss: 0.054754. Value loss: 0.056811. Entropy: 0.299675.\n",
      "Iteration 4008: Policy loss: 0.050662. Value loss: 0.038302. Entropy: 0.299476.\n",
      "episode: 1707   score: 140.0  epsilon: 1.0    steps: 264  evaluation reward: 206.85\n",
      "episode: 1708   score: 180.0  epsilon: 1.0    steps: 496  evaluation reward: 206.2\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4009: Policy loss: -0.155148. Value loss: 0.237615. Entropy: 0.290397.\n",
      "Iteration 4010: Policy loss: -0.164592. Value loss: 0.117261. Entropy: 0.288605.\n",
      "Iteration 4011: Policy loss: -0.179031. Value loss: 0.067704. Entropy: 0.288856.\n",
      "episode: 1709   score: 160.0  epsilon: 1.0    steps: 296  evaluation reward: 207.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1710   score: 360.0  epsilon: 1.0    steps: 832  evaluation reward: 209.9\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4012: Policy loss: 0.065593. Value loss: 0.057587. Entropy: 0.294711.\n",
      "Iteration 4013: Policy loss: 0.061959. Value loss: 0.030797. Entropy: 0.292614.\n",
      "Iteration 4014: Policy loss: 0.058022. Value loss: 0.024642. Entropy: 0.292232.\n",
      "episode: 1711   score: 555.0  epsilon: 1.0    steps: 216  evaluation reward: 213.4\n",
      "episode: 1712   score: 135.0  epsilon: 1.0    steps: 416  evaluation reward: 214.15\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4015: Policy loss: 0.101973. Value loss: 0.085238. Entropy: 0.277656.\n",
      "Iteration 4016: Policy loss: 0.095878. Value loss: 0.036416. Entropy: 0.279318.\n",
      "Iteration 4017: Policy loss: 0.099838. Value loss: 0.027982. Entropy: 0.279098.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4018: Policy loss: 0.086066. Value loss: 0.104112. Entropy: 0.308995.\n",
      "Iteration 4019: Policy loss: 0.088124. Value loss: 0.039989. Entropy: 0.308624.\n",
      "Iteration 4020: Policy loss: 0.082747. Value loss: 0.024931. Entropy: 0.308449.\n",
      "episode: 1713   score: 230.0  epsilon: 1.0    steps: 56  evaluation reward: 211.9\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4021: Policy loss: -0.198531. Value loss: 0.263466. Entropy: 0.295053.\n",
      "Iteration 4022: Policy loss: -0.206598. Value loss: 0.100797. Entropy: 0.297310.\n",
      "Iteration 4023: Policy loss: -0.206322. Value loss: 0.049824. Entropy: 0.295828.\n",
      "episode: 1714   score: 135.0  epsilon: 1.0    steps: 232  evaluation reward: 210.7\n",
      "episode: 1715   score: 105.0  epsilon: 1.0    steps: 472  evaluation reward: 210.2\n",
      "episode: 1716   score: 260.0  epsilon: 1.0    steps: 816  evaluation reward: 211.0\n",
      "episode: 1717   score: 120.0  epsilon: 1.0    steps: 936  evaluation reward: 209.5\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4024: Policy loss: 0.380906. Value loss: 0.104637. Entropy: 0.286323.\n",
      "Iteration 4025: Policy loss: 0.367919. Value loss: 0.040401. Entropy: 0.291853.\n",
      "Iteration 4026: Policy loss: 0.369804. Value loss: 0.027792. Entropy: 0.288304.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4027: Policy loss: 0.128571. Value loss: 0.068789. Entropy: 0.307035.\n",
      "Iteration 4028: Policy loss: 0.127968. Value loss: 0.033485. Entropy: 0.308248.\n",
      "Iteration 4029: Policy loss: 0.121217. Value loss: 0.025463. Entropy: 0.306278.\n",
      "episode: 1718   score: 160.0  epsilon: 1.0    steps: 464  evaluation reward: 210.8\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4030: Policy loss: 0.089482. Value loss: 0.098332. Entropy: 0.304024.\n",
      "Iteration 4031: Policy loss: 0.082334. Value loss: 0.044996. Entropy: 0.308366.\n",
      "Iteration 4032: Policy loss: 0.077476. Value loss: 0.031331. Entropy: 0.305522.\n",
      "episode: 1719   score: 210.0  epsilon: 1.0    steps: 184  evaluation reward: 210.3\n",
      "episode: 1720   score: 105.0  epsilon: 1.0    steps: 1024  evaluation reward: 210.5\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4033: Policy loss: 0.110006. Value loss: 0.104212. Entropy: 0.304150.\n",
      "Iteration 4034: Policy loss: 0.099479. Value loss: 0.033424. Entropy: 0.303300.\n",
      "Iteration 4035: Policy loss: 0.099433. Value loss: 0.023462. Entropy: 0.306470.\n",
      "episode: 1721   score: 460.0  epsilon: 1.0    steps: 32  evaluation reward: 213.0\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4036: Policy loss: -0.127492. Value loss: 0.292481. Entropy: 0.305229.\n",
      "Iteration 4037: Policy loss: -0.115421. Value loss: 0.154561. Entropy: 0.308249.\n",
      "Iteration 4038: Policy loss: -0.134367. Value loss: 0.088004. Entropy: 0.307822.\n",
      "episode: 1722   score: 105.0  epsilon: 1.0    steps: 368  evaluation reward: 211.8\n",
      "episode: 1723   score: 210.0  epsilon: 1.0    steps: 784  evaluation reward: 210.45\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4039: Policy loss: 0.013264. Value loss: 0.099613. Entropy: 0.298366.\n",
      "Iteration 4040: Policy loss: 0.012709. Value loss: 0.045396. Entropy: 0.299420.\n",
      "Iteration 4041: Policy loss: 0.008128. Value loss: 0.030915. Entropy: 0.300436.\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4042: Policy loss: 0.074701. Value loss: 0.083733. Entropy: 0.306742.\n",
      "Iteration 4043: Policy loss: 0.069684. Value loss: 0.050279. Entropy: 0.307300.\n",
      "Iteration 4044: Policy loss: 0.066295. Value loss: 0.039623. Entropy: 0.307558.\n",
      "episode: 1724   score: 210.0  epsilon: 1.0    steps: 96  evaluation reward: 210.05\n",
      "episode: 1725   score: 210.0  epsilon: 1.0    steps: 376  evaluation reward: 208.3\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4045: Policy loss: -0.074696. Value loss: 0.096502. Entropy: 0.297291.\n",
      "Iteration 4046: Policy loss: -0.081648. Value loss: 0.038188. Entropy: 0.296462.\n",
      "Iteration 4047: Policy loss: -0.087314. Value loss: 0.026701. Entropy: 0.294475.\n",
      "episode: 1726   score: 120.0  epsilon: 1.0    steps: 832  evaluation reward: 207.4\n",
      "Training network. lr: 0.000219. clip: 0.087715\n",
      "Iteration 4048: Policy loss: -0.214752. Value loss: 0.311963. Entropy: 0.304555.\n",
      "Iteration 4049: Policy loss: -0.229887. Value loss: 0.078593. Entropy: 0.304783.\n",
      "Iteration 4050: Policy loss: -0.242228. Value loss: 0.039437. Entropy: 0.304145.\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4051: Policy loss: -0.370231. Value loss: 0.199662. Entropy: 0.307174.\n",
      "Iteration 4052: Policy loss: -0.376782. Value loss: 0.093437. Entropy: 0.309195.\n",
      "Iteration 4053: Policy loss: -0.385005. Value loss: 0.048149. Entropy: 0.307872.\n",
      "episode: 1727   score: 695.0  epsilon: 1.0    steps: 328  evaluation reward: 213.8\n",
      "episode: 1728   score: 110.0  epsilon: 1.0    steps: 704  evaluation reward: 213.3\n",
      "episode: 1729   score: 410.0  epsilon: 1.0    steps: 752  evaluation reward: 215.0\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4054: Policy loss: 0.297615. Value loss: 0.337201. Entropy: 0.277329.\n",
      "Iteration 4055: Policy loss: 0.254064. Value loss: 0.240481. Entropy: 0.273486.\n",
      "Iteration 4056: Policy loss: 0.274113. Value loss: 0.205201. Entropy: 0.277224.\n",
      "episode: 1730   score: 210.0  epsilon: 1.0    steps: 776  evaluation reward: 215.0\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4057: Policy loss: 0.203480. Value loss: 0.113657. Entropy: 0.302022.\n",
      "Iteration 4058: Policy loss: 0.188000. Value loss: 0.040144. Entropy: 0.301776.\n",
      "Iteration 4059: Policy loss: 0.182189. Value loss: 0.027201. Entropy: 0.300060.\n",
      "episode: 1731   score: 310.0  epsilon: 1.0    steps: 792  evaluation reward: 215.95\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4060: Policy loss: 0.144630. Value loss: 0.087165. Entropy: 0.299014.\n",
      "Iteration 4061: Policy loss: 0.142043. Value loss: 0.039892. Entropy: 0.296261.\n",
      "Iteration 4062: Policy loss: 0.141715. Value loss: 0.029071. Entropy: 0.296018.\n",
      "episode: 1732   score: 240.0  epsilon: 1.0    steps: 232  evaluation reward: 217.6\n",
      "episode: 1733   score: 80.0  epsilon: 1.0    steps: 776  evaluation reward: 216.3\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4063: Policy loss: -0.021687. Value loss: 0.112627. Entropy: 0.280410.\n",
      "Iteration 4064: Policy loss: -0.023968. Value loss: 0.054492. Entropy: 0.279067.\n",
      "Iteration 4065: Policy loss: -0.029370. Value loss: 0.039631. Entropy: 0.278998.\n",
      "episode: 1734   score: 345.0  epsilon: 1.0    steps: 112  evaluation reward: 219.1\n",
      "episode: 1735   score: 150.0  epsilon: 1.0    steps: 128  evaluation reward: 219.5\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4066: Policy loss: 0.011828. Value loss: 0.282597. Entropy: 0.276994.\n",
      "Iteration 4067: Policy loss: -0.025655. Value loss: 0.147857. Entropy: 0.278209.\n",
      "Iteration 4068: Policy loss: -0.032179. Value loss: 0.063783. Entropy: 0.278225.\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4069: Policy loss: -0.063994. Value loss: 0.103753. Entropy: 0.307249.\n",
      "Iteration 4070: Policy loss: -0.072869. Value loss: 0.041442. Entropy: 0.308040.\n",
      "Iteration 4071: Policy loss: -0.071969. Value loss: 0.032565. Entropy: 0.307203.\n",
      "episode: 1736   score: 185.0  epsilon: 1.0    steps: 728  evaluation reward: 220.2\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4072: Policy loss: -0.322425. Value loss: 0.141124. Entropy: 0.295878.\n",
      "Iteration 4073: Policy loss: -0.331334. Value loss: 0.070829. Entropy: 0.296560.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4074: Policy loss: -0.329597. Value loss: 0.049800. Entropy: 0.296713.\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4075: Policy loss: -0.218568. Value loss: 0.364597. Entropy: 0.312019.\n",
      "Iteration 4076: Policy loss: -0.240991. Value loss: 0.109619. Entropy: 0.312526.\n",
      "Iteration 4077: Policy loss: -0.265324. Value loss: 0.062711. Entropy: 0.313000.\n",
      "episode: 1737   score: 180.0  epsilon: 1.0    steps: 88  evaluation reward: 221.0\n",
      "episode: 1738   score: 265.0  epsilon: 1.0    steps: 312  evaluation reward: 222.85\n",
      "episode: 1739   score: 180.0  epsilon: 1.0    steps: 1000  evaluation reward: 224.35\n",
      "episode: 1740   score: 215.0  epsilon: 1.0    steps: 1016  evaluation reward: 223.85\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4078: Policy loss: 0.240650. Value loss: 0.077566. Entropy: 0.282143.\n",
      "Iteration 4079: Policy loss: 0.233297. Value loss: 0.038839. Entropy: 0.283101.\n",
      "Iteration 4080: Policy loss: 0.239294. Value loss: 0.026382. Entropy: 0.282159.\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4081: Policy loss: 0.028897. Value loss: 0.092605. Entropy: 0.285710.\n",
      "Iteration 4082: Policy loss: 0.029501. Value loss: 0.043440. Entropy: 0.285742.\n",
      "Iteration 4083: Policy loss: 0.021822. Value loss: 0.033137. Entropy: 0.285389.\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4084: Policy loss: 0.035330. Value loss: 0.070061. Entropy: 0.306363.\n",
      "Iteration 4085: Policy loss: 0.029480. Value loss: 0.034045. Entropy: 0.307014.\n",
      "Iteration 4086: Policy loss: 0.028719. Value loss: 0.024313. Entropy: 0.306570.\n",
      "episode: 1741   score: 650.0  epsilon: 1.0    steps: 576  evaluation reward: 229.6\n",
      "episode: 1742   score: 260.0  epsilon: 1.0    steps: 696  evaluation reward: 228.0\n",
      "episode: 1743   score: 535.0  epsilon: 1.0    steps: 760  evaluation reward: 232.55\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4087: Policy loss: 0.003122. Value loss: 0.101322. Entropy: 0.275005.\n",
      "Iteration 4088: Policy loss: -0.000489. Value loss: 0.055057. Entropy: 0.273507.\n",
      "Iteration 4089: Policy loss: 0.001946. Value loss: 0.038079. Entropy: 0.275105.\n",
      "episode: 1744   score: 135.0  epsilon: 1.0    steps: 808  evaluation reward: 232.8\n",
      "episode: 1745   score: 210.0  epsilon: 1.0    steps: 968  evaluation reward: 233.5\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4090: Policy loss: 0.035462. Value loss: 0.067284. Entropy: 0.297626.\n",
      "Iteration 4091: Policy loss: 0.030235. Value loss: 0.029857. Entropy: 0.296776.\n",
      "Iteration 4092: Policy loss: 0.030544. Value loss: 0.022895. Entropy: 0.296348.\n",
      "episode: 1746   score: 180.0  epsilon: 1.0    steps: 616  evaluation reward: 231.85\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4093: Policy loss: 0.189456. Value loss: 0.087538. Entropy: 0.278752.\n",
      "Iteration 4094: Policy loss: 0.186330. Value loss: 0.034428. Entropy: 0.279421.\n",
      "Iteration 4095: Policy loss: 0.184683. Value loss: 0.028915. Entropy: 0.278715.\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4096: Policy loss: -0.105673. Value loss: 0.104177. Entropy: 0.308510.\n",
      "Iteration 4097: Policy loss: -0.116554. Value loss: 0.048902. Entropy: 0.308180.\n",
      "Iteration 4098: Policy loss: -0.116791. Value loss: 0.035018. Entropy: 0.307070.\n",
      "Training network. lr: 0.000219. clip: 0.087568\n",
      "Iteration 4099: Policy loss: 0.192309. Value loss: 0.094606. Entropy: 0.310885.\n",
      "Iteration 4100: Policy loss: 0.196662. Value loss: 0.033542. Entropy: 0.309983.\n",
      "Iteration 4101: Policy loss: 0.171315. Value loss: 0.020843. Entropy: 0.310171.\n",
      "episode: 1747   score: 240.0  epsilon: 1.0    steps: 496  evaluation reward: 232.15\n",
      "episode: 1748   score: 135.0  epsilon: 1.0    steps: 544  evaluation reward: 230.65\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4102: Policy loss: 0.103684. Value loss: 0.209148. Entropy: 0.276547.\n",
      "Iteration 4103: Policy loss: 0.081965. Value loss: 0.137603. Entropy: 0.274607.\n",
      "Iteration 4104: Policy loss: 0.088909. Value loss: 0.084214. Entropy: 0.274297.\n",
      "episode: 1749   score: 80.0  epsilon: 1.0    steps: 128  evaluation reward: 228.8\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4105: Policy loss: 0.137578. Value loss: 0.094812. Entropy: 0.292833.\n",
      "Iteration 4106: Policy loss: 0.134238. Value loss: 0.037081. Entropy: 0.292845.\n",
      "Iteration 4107: Policy loss: 0.123179. Value loss: 0.027728. Entropy: 0.292561.\n",
      "episode: 1750   score: 345.0  epsilon: 1.0    steps: 176  evaluation reward: 228.45\n",
      "now time :  2019-09-05 18:30:16.746382\n",
      "episode: 1751   score: 210.0  epsilon: 1.0    steps: 280  evaluation reward: 225.35\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4108: Policy loss: 0.035612. Value loss: 0.090469. Entropy: 0.278197.\n",
      "Iteration 4109: Policy loss: 0.029368. Value loss: 0.041976. Entropy: 0.277614.\n",
      "Iteration 4110: Policy loss: 0.021145. Value loss: 0.033594. Entropy: 0.278060.\n",
      "episode: 1752   score: 425.0  epsilon: 1.0    steps: 72  evaluation reward: 226.75\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4111: Policy loss: 0.104771. Value loss: 0.131117. Entropy: 0.295180.\n",
      "Iteration 4112: Policy loss: 0.107627. Value loss: 0.065794. Entropy: 0.293690.\n",
      "Iteration 4113: Policy loss: 0.103214. Value loss: 0.041232. Entropy: 0.293696.\n",
      "episode: 1753   score: 280.0  epsilon: 1.0    steps: 512  evaluation reward: 228.5\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4114: Policy loss: -0.005759. Value loss: 0.053870. Entropy: 0.295844.\n",
      "Iteration 4115: Policy loss: -0.014723. Value loss: 0.025158. Entropy: 0.296230.\n",
      "Iteration 4116: Policy loss: -0.014689. Value loss: 0.019315. Entropy: 0.295446.\n",
      "episode: 1754   score: 260.0  epsilon: 1.0    steps: 376  evaluation reward: 228.65\n",
      "episode: 1755   score: 80.0  epsilon: 1.0    steps: 736  evaluation reward: 227.65\n",
      "episode: 1756   score: 260.0  epsilon: 1.0    steps: 872  evaluation reward: 229.25\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4117: Policy loss: -0.084737. Value loss: 0.116706. Entropy: 0.280402.\n",
      "Iteration 4118: Policy loss: -0.088216. Value loss: 0.045175. Entropy: 0.280976.\n",
      "Iteration 4119: Policy loss: -0.090225. Value loss: 0.033640. Entropy: 0.280606.\n",
      "episode: 1757   score: 210.0  epsilon: 1.0    steps: 432  evaluation reward: 228.5\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4120: Policy loss: 0.095113. Value loss: 0.080859. Entropy: 0.286035.\n",
      "Iteration 4121: Policy loss: 0.079892. Value loss: 0.036330. Entropy: 0.287316.\n",
      "Iteration 4122: Policy loss: 0.082682. Value loss: 0.023650. Entropy: 0.284896.\n",
      "episode: 1758   score: 125.0  epsilon: 1.0    steps: 688  evaluation reward: 228.4\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4123: Policy loss: -0.023666. Value loss: 0.116996. Entropy: 0.298487.\n",
      "Iteration 4124: Policy loss: -0.028384. Value loss: 0.046510. Entropy: 0.297419.\n",
      "Iteration 4125: Policy loss: -0.030666. Value loss: 0.032344. Entropy: 0.298220.\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4126: Policy loss: 0.103348. Value loss: 0.084064. Entropy: 0.307014.\n",
      "Iteration 4127: Policy loss: 0.098274. Value loss: 0.025074. Entropy: 0.306087.\n",
      "Iteration 4128: Policy loss: 0.091574. Value loss: 0.016959. Entropy: 0.305796.\n",
      "episode: 1759   score: 305.0  epsilon: 1.0    steps: 168  evaluation reward: 229.95\n",
      "episode: 1760   score: 225.0  epsilon: 1.0    steps: 520  evaluation reward: 230.4\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4129: Policy loss: 0.154589. Value loss: 0.053588. Entropy: 0.280586.\n",
      "Iteration 4130: Policy loss: 0.142548. Value loss: 0.027026. Entropy: 0.279868.\n",
      "Iteration 4131: Policy loss: 0.144883. Value loss: 0.025129. Entropy: 0.279824.\n",
      "episode: 1761   score: 120.0  epsilon: 1.0    steps: 176  evaluation reward: 229.5\n",
      "episode: 1762   score: 190.0  epsilon: 1.0    steps: 616  evaluation reward: 229.6\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4132: Policy loss: -0.162417. Value loss: 0.064513. Entropy: 0.280394.\n",
      "Iteration 4133: Policy loss: -0.156085. Value loss: 0.027697. Entropy: 0.279715.\n",
      "Iteration 4134: Policy loss: -0.159753. Value loss: 0.024836. Entropy: 0.280338.\n",
      "episode: 1763   score: 140.0  epsilon: 1.0    steps: 512  evaluation reward: 228.9\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4135: Policy loss: -0.054085. Value loss: 0.103717. Entropy: 0.298103.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4136: Policy loss: -0.060323. Value loss: 0.040896. Entropy: 0.296542.\n",
      "Iteration 4137: Policy loss: -0.066963. Value loss: 0.028867. Entropy: 0.297375.\n",
      "episode: 1764   score: 210.0  epsilon: 1.0    steps: 872  evaluation reward: 228.95\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4138: Policy loss: -0.670966. Value loss: 0.366866. Entropy: 0.303785.\n",
      "Iteration 4139: Policy loss: -0.703337. Value loss: 0.134494. Entropy: 0.304524.\n",
      "Iteration 4140: Policy loss: -0.693265. Value loss: 0.076898. Entropy: 0.302891.\n",
      "episode: 1765   score: 250.0  epsilon: 1.0    steps: 328  evaluation reward: 229.35\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4141: Policy loss: -0.076715. Value loss: 0.101261. Entropy: 0.290097.\n",
      "Iteration 4142: Policy loss: -0.080041. Value loss: 0.051772. Entropy: 0.290275.\n",
      "Iteration 4143: Policy loss: -0.088494. Value loss: 0.039109. Entropy: 0.289527.\n",
      "episode: 1766   score: 440.0  epsilon: 1.0    steps: 136  evaluation reward: 231.65\n",
      "episode: 1767   score: 230.0  epsilon: 1.0    steps: 408  evaluation reward: 232.75\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4144: Policy loss: -0.039496. Value loss: 0.099645. Entropy: 0.278306.\n",
      "Iteration 4145: Policy loss: -0.049733. Value loss: 0.039241. Entropy: 0.277214.\n",
      "Iteration 4146: Policy loss: -0.045418. Value loss: 0.031516. Entropy: 0.274591.\n",
      "episode: 1768   score: 290.0  epsilon: 1.0    steps: 696  evaluation reward: 233.75\n",
      "episode: 1769   score: 135.0  epsilon: 1.0    steps: 808  evaluation reward: 231.3\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4147: Policy loss: 0.144345. Value loss: 0.156423. Entropy: 0.289878.\n",
      "Iteration 4148: Policy loss: 0.142991. Value loss: 0.076003. Entropy: 0.287530.\n",
      "Iteration 4149: Policy loss: 0.135974. Value loss: 0.054400. Entropy: 0.287471.\n",
      "Training network. lr: 0.000219. clip: 0.087411\n",
      "Iteration 4150: Policy loss: 0.229938. Value loss: 0.059557. Entropy: 0.309948.\n",
      "Iteration 4151: Policy loss: 0.224204. Value loss: 0.027897. Entropy: 0.308998.\n",
      "Iteration 4152: Policy loss: 0.224244. Value loss: 0.021303. Entropy: 0.309121.\n",
      "episode: 1770   score: 210.0  epsilon: 1.0    steps: 920  evaluation reward: 230.75\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4153: Policy loss: 0.053034. Value loss: 0.131096. Entropy: 0.303099.\n",
      "Iteration 4154: Policy loss: 0.042281. Value loss: 0.054389. Entropy: 0.302347.\n",
      "Iteration 4155: Policy loss: 0.036566. Value loss: 0.035211. Entropy: 0.302857.\n",
      "episode: 1771   score: 155.0  epsilon: 1.0    steps: 600  evaluation reward: 229.6\n",
      "episode: 1772   score: 120.0  epsilon: 1.0    steps: 848  evaluation reward: 229.25\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4156: Policy loss: 0.070411. Value loss: 0.106273. Entropy: 0.280685.\n",
      "Iteration 4157: Policy loss: 0.066677. Value loss: 0.048701. Entropy: 0.278446.\n",
      "Iteration 4158: Policy loss: 0.063496. Value loss: 0.030293. Entropy: 0.279300.\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4159: Policy loss: -0.029260. Value loss: 0.091815. Entropy: 0.304199.\n",
      "Iteration 4160: Policy loss: -0.034878. Value loss: 0.044824. Entropy: 0.304475.\n",
      "Iteration 4161: Policy loss: -0.039091. Value loss: 0.028433. Entropy: 0.303932.\n",
      "episode: 1773   score: 375.0  epsilon: 1.0    steps: 16  evaluation reward: 230.9\n",
      "episode: 1774   score: 300.0  epsilon: 1.0    steps: 576  evaluation reward: 232.8\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4162: Policy loss: 0.115826. Value loss: 0.070976. Entropy: 0.279516.\n",
      "Iteration 4163: Policy loss: 0.111509. Value loss: 0.035483. Entropy: 0.280207.\n",
      "Iteration 4164: Policy loss: 0.109940. Value loss: 0.028784. Entropy: 0.279955.\n",
      "episode: 1775   score: 165.0  epsilon: 1.0    steps: 960  evaluation reward: 232.2\n",
      "episode: 1776   score: 180.0  epsilon: 1.0    steps: 1016  evaluation reward: 232.3\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4165: Policy loss: -0.028373. Value loss: 0.089585. Entropy: 0.307770.\n",
      "Iteration 4166: Policy loss: -0.030402. Value loss: 0.042500. Entropy: 0.306809.\n",
      "Iteration 4167: Policy loss: -0.027120. Value loss: 0.034316. Entropy: 0.307083.\n",
      "episode: 1777   score: 75.0  epsilon: 1.0    steps: 24  evaluation reward: 231.2\n",
      "episode: 1778   score: 490.0  epsilon: 1.0    steps: 368  evaluation reward: 233.1\n",
      "episode: 1779   score: 175.0  epsilon: 1.0    steps: 848  evaluation reward: 232.75\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4168: Policy loss: -0.237777. Value loss: 0.284327. Entropy: 0.249692.\n",
      "Iteration 4169: Policy loss: -0.257074. Value loss: 0.203720. Entropy: 0.251147.\n",
      "Iteration 4170: Policy loss: -0.249311. Value loss: 0.117187. Entropy: 0.250784.\n",
      "episode: 1780   score: 100.0  epsilon: 1.0    steps: 608  evaluation reward: 232.65\n",
      "episode: 1781   score: 195.0  epsilon: 1.0    steps: 864  evaluation reward: 232.2\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4171: Policy loss: -0.006709. Value loss: 0.077426. Entropy: 0.286391.\n",
      "Iteration 4172: Policy loss: -0.009613. Value loss: 0.038763. Entropy: 0.285570.\n",
      "Iteration 4173: Policy loss: -0.013798. Value loss: 0.030004. Entropy: 0.286529.\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4174: Policy loss: -0.025559. Value loss: 0.059516. Entropy: 0.305942.\n",
      "Iteration 4175: Policy loss: -0.032792. Value loss: 0.028015. Entropy: 0.305217.\n",
      "Iteration 4176: Policy loss: -0.022536. Value loss: 0.017934. Entropy: 0.305056.\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4177: Policy loss: -0.075958. Value loss: 0.056476. Entropy: 0.307854.\n",
      "Iteration 4178: Policy loss: -0.085328. Value loss: 0.028399. Entropy: 0.306091.\n",
      "Iteration 4179: Policy loss: -0.087129. Value loss: 0.020319. Entropy: 0.305663.\n",
      "episode: 1782   score: 95.0  epsilon: 1.0    steps: 232  evaluation reward: 231.05\n",
      "episode: 1783   score: 155.0  epsilon: 1.0    steps: 496  evaluation reward: 229.6\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4180: Policy loss: 0.069979. Value loss: 0.069743. Entropy: 0.280505.\n",
      "Iteration 4181: Policy loss: 0.063264. Value loss: 0.026668. Entropy: 0.280317.\n",
      "Iteration 4182: Policy loss: 0.060953. Value loss: 0.018248. Entropy: 0.279676.\n",
      "episode: 1784   score: 210.0  epsilon: 1.0    steps: 112  evaluation reward: 226.8\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4183: Policy loss: -0.092676. Value loss: 0.082854. Entropy: 0.294715.\n",
      "Iteration 4184: Policy loss: -0.096694. Value loss: 0.032643. Entropy: 0.296040.\n",
      "Iteration 4185: Policy loss: -0.101185. Value loss: 0.026879. Entropy: 0.294823.\n",
      "episode: 1785   score: 475.0  epsilon: 1.0    steps: 264  evaluation reward: 229.45\n",
      "episode: 1786   score: 210.0  epsilon: 1.0    steps: 368  evaluation reward: 228.9\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4186: Policy loss: 0.080877. Value loss: 0.073892. Entropy: 0.282538.\n",
      "Iteration 4187: Policy loss: 0.074410. Value loss: 0.033656. Entropy: 0.281696.\n",
      "Iteration 4188: Policy loss: 0.067453. Value loss: 0.024856. Entropy: 0.280812.\n",
      "episode: 1787   score: 210.0  epsilon: 1.0    steps: 88  evaluation reward: 228.4\n",
      "episode: 1788   score: 160.0  epsilon: 1.0    steps: 960  evaluation reward: 228.2\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4189: Policy loss: 0.004584. Value loss: 0.084064. Entropy: 0.291167.\n",
      "Iteration 4190: Policy loss: -0.002783. Value loss: 0.036289. Entropy: 0.291900.\n",
      "Iteration 4191: Policy loss: -0.006823. Value loss: 0.026530. Entropy: 0.291748.\n",
      "episode: 1789   score: 210.0  epsilon: 1.0    steps: 200  evaluation reward: 229.5\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4192: Policy loss: -0.175589. Value loss: 0.094175. Entropy: 0.285373.\n",
      "Iteration 4193: Policy loss: -0.181837. Value loss: 0.038763. Entropy: 0.285104.\n",
      "Iteration 4194: Policy loss: -0.187171. Value loss: 0.030526. Entropy: 0.284620.\n",
      "episode: 1790   score: 230.0  epsilon: 1.0    steps: 120  evaluation reward: 229.25\n",
      "episode: 1791   score: 155.0  epsilon: 1.0    steps: 344  evaluation reward: 228.4\n",
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4195: Policy loss: -0.221626. Value loss: 0.299464. Entropy: 0.279821.\n",
      "Iteration 4196: Policy loss: -0.229612. Value loss: 0.215201. Entropy: 0.279703.\n",
      "Iteration 4197: Policy loss: -0.230413. Value loss: 0.109591. Entropy: 0.280011.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000218. clip: 0.087254\n",
      "Iteration 4198: Policy loss: -0.031681. Value loss: 0.056731. Entropy: 0.309265.\n",
      "Iteration 4199: Policy loss: -0.034564. Value loss: 0.027014. Entropy: 0.309139.\n",
      "Iteration 4200: Policy loss: -0.040164. Value loss: 0.025588. Entropy: 0.308023.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4201: Policy loss: -0.102668. Value loss: 0.072650. Entropy: 0.304891.\n",
      "Iteration 4202: Policy loss: -0.106344. Value loss: 0.034785. Entropy: 0.304633.\n",
      "Iteration 4203: Policy loss: -0.106117. Value loss: 0.026902. Entropy: 0.305551.\n",
      "episode: 1792   score: 180.0  epsilon: 1.0    steps: 632  evaluation reward: 229.0\n",
      "episode: 1793   score: 105.0  epsilon: 1.0    steps: 784  evaluation reward: 227.95\n",
      "episode: 1794   score: 185.0  epsilon: 1.0    steps: 952  evaluation reward: 225.7\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4204: Policy loss: 0.027300. Value loss: 0.125204. Entropy: 0.282884.\n",
      "Iteration 4205: Policy loss: 0.018385. Value loss: 0.039727. Entropy: 0.281517.\n",
      "Iteration 4206: Policy loss: 0.011664. Value loss: 0.029836. Entropy: 0.283039.\n",
      "episode: 1795   score: 440.0  epsilon: 1.0    steps: 848  evaluation reward: 229.6\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4207: Policy loss: -0.064105. Value loss: 0.080881. Entropy: 0.290033.\n",
      "Iteration 4208: Policy loss: -0.061632. Value loss: 0.030974. Entropy: 0.290454.\n",
      "Iteration 4209: Policy loss: -0.073132. Value loss: 0.020786. Entropy: 0.289774.\n",
      "episode: 1796   score: 240.0  epsilon: 1.0    steps: 640  evaluation reward: 230.95\n",
      "episode: 1797   score: 370.0  epsilon: 1.0    steps: 792  evaluation reward: 233.6\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4210: Policy loss: 0.004396. Value loss: 0.104084. Entropy: 0.279469.\n",
      "Iteration 4211: Policy loss: -0.003535. Value loss: 0.040668. Entropy: 0.278868.\n",
      "Iteration 4212: Policy loss: -0.001534. Value loss: 0.029527. Entropy: 0.279556.\n",
      "episode: 1798   score: 345.0  epsilon: 1.0    steps: 632  evaluation reward: 234.35\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4213: Policy loss: 0.042383. Value loss: 0.097836. Entropy: 0.288753.\n",
      "Iteration 4214: Policy loss: 0.032653. Value loss: 0.042065. Entropy: 0.288835.\n",
      "Iteration 4215: Policy loss: 0.026559. Value loss: 0.028634. Entropy: 0.288921.\n",
      "episode: 1799   score: 110.0  epsilon: 1.0    steps: 688  evaluation reward: 234.35\n",
      "episode: 1800   score: 265.0  epsilon: 1.0    steps: 960  evaluation reward: 232.6\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4216: Policy loss: 0.150055. Value loss: 0.060544. Entropy: 0.289851.\n",
      "Iteration 4217: Policy loss: 0.144652. Value loss: 0.025466. Entropy: 0.290119.\n",
      "Iteration 4218: Policy loss: 0.139799. Value loss: 0.021925. Entropy: 0.288785.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4219: Policy loss: -0.110103. Value loss: 0.097757. Entropy: 0.296975.\n",
      "Iteration 4220: Policy loss: -0.115308. Value loss: 0.046788. Entropy: 0.297429.\n",
      "Iteration 4221: Policy loss: -0.116713. Value loss: 0.031813. Entropy: 0.298455.\n",
      "now time :  2019-09-05 18:37:22.321193\n",
      "episode: 1801   score: 135.0  epsilon: 1.0    steps: 768  evaluation reward: 231.85\n",
      "episode: 1802   score: 200.0  epsilon: 1.0    steps: 840  evaluation reward: 231.75\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4222: Policy loss: -0.112324. Value loss: 0.134437. Entropy: 0.288632.\n",
      "Iteration 4223: Policy loss: -0.113663. Value loss: 0.059574. Entropy: 0.287777.\n",
      "Iteration 4224: Policy loss: -0.124837. Value loss: 0.040439. Entropy: 0.288862.\n",
      "episode: 1803   score: 110.0  epsilon: 1.0    steps: 624  evaluation reward: 230.0\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4225: Policy loss: -0.107036. Value loss: 0.323226. Entropy: 0.288182.\n",
      "Iteration 4226: Policy loss: -0.107878. Value loss: 0.179574. Entropy: 0.286250.\n",
      "Iteration 4227: Policy loss: -0.132974. Value loss: 0.085330. Entropy: 0.285729.\n",
      "episode: 1804   score: 485.0  epsilon: 1.0    steps: 104  evaluation reward: 232.15\n",
      "episode: 1805   score: 265.0  epsilon: 1.0    steps: 192  evaluation reward: 232.2\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4228: Policy loss: -0.092242. Value loss: 0.138884. Entropy: 0.277633.\n",
      "Iteration 4229: Policy loss: -0.104524. Value loss: 0.050304. Entropy: 0.276949.\n",
      "Iteration 4230: Policy loss: -0.100887. Value loss: 0.034912. Entropy: 0.277441.\n",
      "episode: 1806   score: 120.0  epsilon: 1.0    steps: 600  evaluation reward: 231.05\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4231: Policy loss: 0.136848. Value loss: 0.095720. Entropy: 0.295617.\n",
      "Iteration 4232: Policy loss: 0.125692. Value loss: 0.043273. Entropy: 0.295089.\n",
      "Iteration 4233: Policy loss: 0.120861. Value loss: 0.032213. Entropy: 0.294983.\n",
      "episode: 1807   score: 160.0  epsilon: 1.0    steps: 8  evaluation reward: 231.25\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4234: Policy loss: -0.260254. Value loss: 0.269463. Entropy: 0.292564.\n",
      "Iteration 4235: Policy loss: -0.279331. Value loss: 0.159755. Entropy: 0.291895.\n",
      "Iteration 4236: Policy loss: -0.275126. Value loss: 0.102292. Entropy: 0.293758.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4237: Policy loss: 0.107408. Value loss: 0.156214. Entropy: 0.306177.\n",
      "Iteration 4238: Policy loss: 0.095833. Value loss: 0.078053. Entropy: 0.306547.\n",
      "Iteration 4239: Policy loss: 0.093343. Value loss: 0.058660. Entropy: 0.305985.\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4240: Policy loss: -0.013725. Value loss: 0.097615. Entropy: 0.306731.\n",
      "Iteration 4241: Policy loss: -0.013802. Value loss: 0.038490. Entropy: 0.307495.\n",
      "Iteration 4242: Policy loss: -0.014201. Value loss: 0.027879. Entropy: 0.306610.\n",
      "episode: 1808   score: 180.0  epsilon: 1.0    steps: 32  evaluation reward: 231.25\n",
      "episode: 1809   score: 210.0  epsilon: 1.0    steps: 160  evaluation reward: 231.75\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4243: Policy loss: 0.195726. Value loss: 0.073274. Entropy: 0.285643.\n",
      "Iteration 4244: Policy loss: 0.188870. Value loss: 0.023431. Entropy: 0.282820.\n",
      "Iteration 4245: Policy loss: 0.187920. Value loss: 0.016807. Entropy: 0.285521.\n",
      "episode: 1810   score: 455.0  epsilon: 1.0    steps: 120  evaluation reward: 232.7\n",
      "episode: 1811   score: 240.0  epsilon: 1.0    steps: 664  evaluation reward: 229.55\n",
      "episode: 1812   score: 210.0  epsilon: 1.0    steps: 680  evaluation reward: 230.3\n",
      "episode: 1813   score: 550.0  epsilon: 1.0    steps: 880  evaluation reward: 233.5\n",
      "episode: 1814   score: 210.0  epsilon: 1.0    steps: 976  evaluation reward: 234.25\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4246: Policy loss: -0.057659. Value loss: 0.135437. Entropy: 0.273808.\n",
      "Iteration 4247: Policy loss: -0.054418. Value loss: 0.061489. Entropy: 0.270738.\n",
      "Iteration 4248: Policy loss: -0.076251. Value loss: 0.045447. Entropy: 0.271458.\n",
      "episode: 1815   score: 225.0  epsilon: 1.0    steps: 560  evaluation reward: 235.45\n",
      "Training network. lr: 0.000218. clip: 0.087107\n",
      "Iteration 4249: Policy loss: 0.173026. Value loss: 0.102832. Entropy: 0.281260.\n",
      "Iteration 4250: Policy loss: 0.169223. Value loss: 0.057760. Entropy: 0.279874.\n",
      "Iteration 4251: Policy loss: 0.168256. Value loss: 0.035725. Entropy: 0.280696.\n",
      "episode: 1816   score: 90.0  epsilon: 1.0    steps: 256  evaluation reward: 233.75\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4252: Policy loss: 0.105047. Value loss: 0.127239. Entropy: 0.295143.\n",
      "Iteration 4253: Policy loss: 0.095328. Value loss: 0.057075. Entropy: 0.293371.\n",
      "Iteration 4254: Policy loss: 0.096504. Value loss: 0.035896. Entropy: 0.293267.\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4255: Policy loss: -0.141600. Value loss: 0.290098. Entropy: 0.304650.\n",
      "Iteration 4256: Policy loss: -0.152663. Value loss: 0.102761. Entropy: 0.306344.\n",
      "Iteration 4257: Policy loss: -0.184718. Value loss: 0.057783. Entropy: 0.305006.\n",
      "episode: 1817   score: 150.0  epsilon: 1.0    steps: 16  evaluation reward: 234.05\n",
      "episode: 1818   score: 125.0  epsilon: 1.0    steps: 272  evaluation reward: 233.7\n",
      "episode: 1819   score: 130.0  epsilon: 1.0    steps: 688  evaluation reward: 232.9\n",
      "Training network. lr: 0.000217. clip: 0.086950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4258: Policy loss: 0.020607. Value loss: 0.103367. Entropy: 0.273507.\n",
      "Iteration 4259: Policy loss: 0.008595. Value loss: 0.041198. Entropy: 0.270273.\n",
      "Iteration 4260: Policy loss: 0.004503. Value loss: 0.025111. Entropy: 0.269854.\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4261: Policy loss: 0.007979. Value loss: 0.151386. Entropy: 0.306496.\n",
      "Iteration 4262: Policy loss: -0.004730. Value loss: 0.051584. Entropy: 0.306367.\n",
      "Iteration 4263: Policy loss: -0.013339. Value loss: 0.037738. Entropy: 0.305368.\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4264: Policy loss: 0.112938. Value loss: 0.148129. Entropy: 0.306703.\n",
      "Iteration 4265: Policy loss: 0.101902. Value loss: 0.059880. Entropy: 0.305864.\n",
      "Iteration 4266: Policy loss: 0.099184. Value loss: 0.041849. Entropy: 0.305716.\n",
      "episode: 1820   score: 215.0  epsilon: 1.0    steps: 64  evaluation reward: 234.0\n",
      "episode: 1821   score: 285.0  epsilon: 1.0    steps: 392  evaluation reward: 232.25\n",
      "episode: 1822   score: 445.0  epsilon: 1.0    steps: 880  evaluation reward: 235.65\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4267: Policy loss: 0.097677. Value loss: 0.093069. Entropy: 0.275302.\n",
      "Iteration 4268: Policy loss: 0.088689. Value loss: 0.034097. Entropy: 0.274447.\n",
      "Iteration 4269: Policy loss: 0.088984. Value loss: 0.023856. Entropy: 0.274270.\n",
      "episode: 1823   score: 155.0  epsilon: 1.0    steps: 240  evaluation reward: 235.1\n",
      "episode: 1824   score: 150.0  epsilon: 1.0    steps: 272  evaluation reward: 234.5\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4270: Policy loss: -0.233839. Value loss: 0.191731. Entropy: 0.274329.\n",
      "Iteration 4271: Policy loss: -0.245513. Value loss: 0.064681. Entropy: 0.273650.\n",
      "Iteration 4272: Policy loss: -0.249989. Value loss: 0.046578. Entropy: 0.273597.\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4273: Policy loss: 0.014019. Value loss: 0.112200. Entropy: 0.308609.\n",
      "Iteration 4274: Policy loss: -0.006263. Value loss: 0.051215. Entropy: 0.307495.\n",
      "Iteration 4275: Policy loss: -0.004069. Value loss: 0.034346. Entropy: 0.307385.\n",
      "episode: 1825   score: 210.0  epsilon: 1.0    steps: 696  evaluation reward: 234.5\n",
      "episode: 1826   score: 530.0  epsilon: 1.0    steps: 984  evaluation reward: 238.6\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4276: Policy loss: -0.163814. Value loss: 0.239297. Entropy: 0.294079.\n",
      "Iteration 4277: Policy loss: -0.205808. Value loss: 0.072776. Entropy: 0.294773.\n",
      "Iteration 4278: Policy loss: -0.182694. Value loss: 0.044021. Entropy: 0.294597.\n",
      "episode: 1827   score: 150.0  epsilon: 1.0    steps: 1000  evaluation reward: 233.15\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4279: Policy loss: 0.060551. Value loss: 0.146920. Entropy: 0.297179.\n",
      "Iteration 4280: Policy loss: 0.058120. Value loss: 0.064777. Entropy: 0.296889.\n",
      "Iteration 4281: Policy loss: 0.050459. Value loss: 0.044992. Entropy: 0.296443.\n",
      "episode: 1828   score: 460.0  epsilon: 1.0    steps: 280  evaluation reward: 236.65\n",
      "episode: 1829   score: 205.0  epsilon: 1.0    steps: 744  evaluation reward: 234.6\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4282: Policy loss: 0.001615. Value loss: 0.085133. Entropy: 0.272552.\n",
      "Iteration 4283: Policy loss: -0.001471. Value loss: 0.041945. Entropy: 0.272077.\n",
      "Iteration 4284: Policy loss: -0.008719. Value loss: 0.031432. Entropy: 0.272184.\n",
      "episode: 1830   score: 180.0  epsilon: 1.0    steps: 400  evaluation reward: 234.3\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4285: Policy loss: 0.084961. Value loss: 0.083032. Entropy: 0.292344.\n",
      "Iteration 4286: Policy loss: 0.084381. Value loss: 0.039991. Entropy: 0.292304.\n",
      "Iteration 4287: Policy loss: 0.078217. Value loss: 0.033765. Entropy: 0.292883.\n",
      "episode: 1831   score: 180.0  epsilon: 1.0    steps: 304  evaluation reward: 233.0\n",
      "episode: 1832   score: 135.0  epsilon: 1.0    steps: 928  evaluation reward: 231.95\n",
      "episode: 1833   score: 80.0  epsilon: 1.0    steps: 984  evaluation reward: 231.95\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4288: Policy loss: 0.072481. Value loss: 0.049601. Entropy: 0.289791.\n",
      "Iteration 4289: Policy loss: 0.067983. Value loss: 0.028592. Entropy: 0.290160.\n",
      "Iteration 4290: Policy loss: 0.066457. Value loss: 0.025880. Entropy: 0.291231.\n",
      "episode: 1834   score: 100.0  epsilon: 1.0    steps: 712  evaluation reward: 229.5\n",
      "episode: 1835   score: 240.0  epsilon: 1.0    steps: 744  evaluation reward: 230.4\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4291: Policy loss: 0.190995. Value loss: 0.076444. Entropy: 0.282605.\n",
      "Iteration 4292: Policy loss: 0.187609. Value loss: 0.037903. Entropy: 0.285700.\n",
      "Iteration 4293: Policy loss: 0.183610. Value loss: 0.028937. Entropy: 0.284034.\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4294: Policy loss: -0.037121. Value loss: 0.128261. Entropy: 0.308409.\n",
      "Iteration 4295: Policy loss: -0.043792. Value loss: 0.046259. Entropy: 0.307162.\n",
      "Iteration 4296: Policy loss: -0.037104. Value loss: 0.033866. Entropy: 0.307953.\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4297: Policy loss: 0.110912. Value loss: 0.099901. Entropy: 0.308107.\n",
      "Iteration 4298: Policy loss: 0.107113. Value loss: 0.031376. Entropy: 0.307797.\n",
      "Iteration 4299: Policy loss: 0.096156. Value loss: 0.022896. Entropy: 0.307428.\n",
      "episode: 1836   score: 135.0  epsilon: 1.0    steps: 24  evaluation reward: 229.9\n",
      "episode: 1837   score: 120.0  epsilon: 1.0    steps: 152  evaluation reward: 229.3\n",
      "episode: 1838   score: 75.0  epsilon: 1.0    steps: 856  evaluation reward: 227.4\n",
      "episode: 1839   score: 265.0  epsilon: 1.0    steps: 984  evaluation reward: 228.25\n",
      "Training network. lr: 0.000217. clip: 0.086950\n",
      "Iteration 4300: Policy loss: 0.049842. Value loss: 0.059466. Entropy: 0.285059.\n",
      "Iteration 4301: Policy loss: 0.039592. Value loss: 0.026771. Entropy: 0.285986.\n",
      "Iteration 4302: Policy loss: 0.043125. Value loss: 0.019171. Entropy: 0.282116.\n",
      "episode: 1840   score: 155.0  epsilon: 1.0    steps: 304  evaluation reward: 227.65\n",
      "episode: 1841   score: 180.0  epsilon: 1.0    steps: 592  evaluation reward: 222.95\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4303: Policy loss: 0.234985. Value loss: 0.070312. Entropy: 0.289372.\n",
      "Iteration 4304: Policy loss: 0.224583. Value loss: 0.030514. Entropy: 0.288031.\n",
      "Iteration 4305: Policy loss: 0.226649. Value loss: 0.022263. Entropy: 0.284203.\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4306: Policy loss: -0.122333. Value loss: 0.138391. Entropy: 0.309069.\n",
      "Iteration 4307: Policy loss: -0.129774. Value loss: 0.060966. Entropy: 0.308235.\n",
      "Iteration 4308: Policy loss: -0.133947. Value loss: 0.040949. Entropy: 0.308837.\n",
      "episode: 1842   score: 85.0  epsilon: 1.0    steps: 536  evaluation reward: 221.2\n",
      "episode: 1843   score: 210.0  epsilon: 1.0    steps: 968  evaluation reward: 217.95\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4309: Policy loss: 0.042463. Value loss: 0.070545. Entropy: 0.301624.\n",
      "Iteration 4310: Policy loss: 0.038371. Value loss: 0.031666. Entropy: 0.299586.\n",
      "Iteration 4311: Policy loss: 0.029808. Value loss: 0.025492. Entropy: 0.300014.\n",
      "episode: 1844   score: 120.0  epsilon: 1.0    steps: 480  evaluation reward: 217.8\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4312: Policy loss: -0.227263. Value loss: 0.306068. Entropy: 0.287417.\n",
      "Iteration 4313: Policy loss: -0.267465. Value loss: 0.186244. Entropy: 0.286951.\n",
      "Iteration 4314: Policy loss: -0.249868. Value loss: 0.131703. Entropy: 0.285404.\n",
      "episode: 1845   score: 260.0  epsilon: 1.0    steps: 944  evaluation reward: 218.3\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4315: Policy loss: 0.021057. Value loss: 0.097099. Entropy: 0.305324.\n",
      "Iteration 4316: Policy loss: 0.011810. Value loss: 0.042762. Entropy: 0.304988.\n",
      "Iteration 4317: Policy loss: 0.005744. Value loss: 0.029391. Entropy: 0.304788.\n",
      "episode: 1846   score: 105.0  epsilon: 1.0    steps: 32  evaluation reward: 217.55\n",
      "episode: 1847   score: 75.0  epsilon: 1.0    steps: 784  evaluation reward: 215.9\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4318: Policy loss: -0.229538. Value loss: 0.222181. Entropy: 0.284258.\n",
      "Iteration 4319: Policy loss: -0.242487. Value loss: 0.100575. Entropy: 0.283790.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4320: Policy loss: -0.237503. Value loss: 0.082556. Entropy: 0.285914.\n",
      "episode: 1848   score: 495.0  epsilon: 1.0    steps: 736  evaluation reward: 219.5\n",
      "episode: 1849   score: 210.0  epsilon: 1.0    steps: 960  evaluation reward: 220.8\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4321: Policy loss: -0.066580. Value loss: 0.114805. Entropy: 0.297329.\n",
      "Iteration 4322: Policy loss: -0.067931. Value loss: 0.044798. Entropy: 0.297944.\n",
      "Iteration 4323: Policy loss: -0.074037. Value loss: 0.032048. Entropy: 0.297090.\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4324: Policy loss: -0.017955. Value loss: 0.117095. Entropy: 0.302032.\n",
      "Iteration 4325: Policy loss: -0.011645. Value loss: 0.035593. Entropy: 0.303086.\n",
      "Iteration 4326: Policy loss: -0.021710. Value loss: 0.022273. Entropy: 0.301661.\n",
      "episode: 1850   score: 505.0  epsilon: 1.0    steps: 168  evaluation reward: 222.4\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4327: Policy loss: -0.001494. Value loss: 0.132069. Entropy: 0.298451.\n",
      "Iteration 4328: Policy loss: -0.016445. Value loss: 0.053411. Entropy: 0.299912.\n",
      "Iteration 4329: Policy loss: -0.016416. Value loss: 0.032091. Entropy: 0.297653.\n",
      "now time :  2019-09-05 18:44:02.398192\n",
      "episode: 1851   score: 215.0  epsilon: 1.0    steps: 64  evaluation reward: 222.45\n",
      "episode: 1852   score: 120.0  epsilon: 1.0    steps: 160  evaluation reward: 219.4\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4330: Policy loss: -0.058394. Value loss: 0.088241. Entropy: 0.292774.\n",
      "Iteration 4331: Policy loss: -0.062263. Value loss: 0.044388. Entropy: 0.292856.\n",
      "Iteration 4332: Policy loss: -0.065682. Value loss: 0.031858. Entropy: 0.293634.\n",
      "episode: 1853   score: 210.0  epsilon: 1.0    steps: 336  evaluation reward: 218.7\n",
      "episode: 1854   score: 105.0  epsilon: 1.0    steps: 776  evaluation reward: 217.15\n",
      "episode: 1855   score: 145.0  epsilon: 1.0    steps: 920  evaluation reward: 217.8\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4333: Policy loss: 0.207164. Value loss: 0.132136. Entropy: 0.293946.\n",
      "Iteration 4334: Policy loss: 0.190033. Value loss: 0.061128. Entropy: 0.291950.\n",
      "Iteration 4335: Policy loss: 0.203765. Value loss: 0.039454. Entropy: 0.291975.\n",
      "episode: 1856   score: 225.0  epsilon: 1.0    steps: 672  evaluation reward: 217.45\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4336: Policy loss: -0.106224. Value loss: 0.130395. Entropy: 0.297958.\n",
      "Iteration 4337: Policy loss: -0.101670. Value loss: 0.061742. Entropy: 0.294729.\n",
      "Iteration 4338: Policy loss: -0.107074. Value loss: 0.043519. Entropy: 0.293989.\n",
      "episode: 1857   score: 310.0  epsilon: 1.0    steps: 480  evaluation reward: 218.45\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4339: Policy loss: 0.081453. Value loss: 0.095405. Entropy: 0.299047.\n",
      "Iteration 4340: Policy loss: 0.080243. Value loss: 0.042900. Entropy: 0.296547.\n",
      "Iteration 4341: Policy loss: 0.077574. Value loss: 0.032839. Entropy: 0.295009.\n",
      "episode: 1858   score: 160.0  epsilon: 1.0    steps: 16  evaluation reward: 218.8\n",
      "episode: 1859   score: 120.0  epsilon: 1.0    steps: 680  evaluation reward: 216.95\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4342: Policy loss: -0.163693. Value loss: 0.098804. Entropy: 0.290714.\n",
      "Iteration 4343: Policy loss: -0.169572. Value loss: 0.043875. Entropy: 0.289075.\n",
      "Iteration 4344: Policy loss: -0.167828. Value loss: 0.029852. Entropy: 0.285964.\n",
      "episode: 1860   score: 165.0  epsilon: 1.0    steps: 280  evaluation reward: 216.35\n",
      "episode: 1861   score: 320.0  epsilon: 1.0    steps: 352  evaluation reward: 218.35\n",
      "episode: 1862   score: 105.0  epsilon: 1.0    steps: 784  evaluation reward: 217.5\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4345: Policy loss: -0.176460. Value loss: 0.069009. Entropy: 0.278396.\n",
      "Iteration 4346: Policy loss: -0.182004. Value loss: 0.029782. Entropy: 0.272999.\n",
      "Iteration 4347: Policy loss: -0.181382. Value loss: 0.022629. Entropy: 0.274344.\n",
      "episode: 1863   score: 155.0  epsilon: 1.0    steps: 904  evaluation reward: 217.65\n",
      "Training network. lr: 0.000217. clip: 0.086793\n",
      "Iteration 4348: Policy loss: 0.010387. Value loss: 0.084642. Entropy: 0.300546.\n",
      "Iteration 4349: Policy loss: 0.008560. Value loss: 0.043001. Entropy: 0.301301.\n",
      "Iteration 4350: Policy loss: 0.006713. Value loss: 0.036245. Entropy: 0.299774.\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4351: Policy loss: -0.070427. Value loss: 0.095118. Entropy: 0.298514.\n",
      "Iteration 4352: Policy loss: -0.070898. Value loss: 0.035287. Entropy: 0.297960.\n",
      "Iteration 4353: Policy loss: -0.079803. Value loss: 0.025195. Entropy: 0.298433.\n",
      "episode: 1864   score: 150.0  epsilon: 1.0    steps: 144  evaluation reward: 217.05\n",
      "episode: 1865   score: 225.0  epsilon: 1.0    steps: 768  evaluation reward: 216.8\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4354: Policy loss: 0.169639. Value loss: 0.052044. Entropy: 0.283996.\n",
      "Iteration 4355: Policy loss: 0.167285. Value loss: 0.024981. Entropy: 0.284703.\n",
      "Iteration 4356: Policy loss: 0.167708. Value loss: 0.020229. Entropy: 0.281687.\n",
      "episode: 1866   score: 140.0  epsilon: 1.0    steps: 480  evaluation reward: 213.8\n",
      "episode: 1867   score: 110.0  epsilon: 1.0    steps: 1000  evaluation reward: 212.6\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4357: Policy loss: -0.384884. Value loss: 0.339362. Entropy: 0.287312.\n",
      "Iteration 4358: Policy loss: -0.392292. Value loss: 0.186474. Entropy: 0.288597.\n",
      "Iteration 4359: Policy loss: -0.420549. Value loss: 0.114070. Entropy: 0.286017.\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4360: Policy loss: 0.055467. Value loss: 0.120022. Entropy: 0.295056.\n",
      "Iteration 4361: Policy loss: 0.045294. Value loss: 0.055891. Entropy: 0.296442.\n",
      "Iteration 4362: Policy loss: 0.054891. Value loss: 0.038108. Entropy: 0.294654.\n",
      "episode: 1868   score: 315.0  epsilon: 1.0    steps: 872  evaluation reward: 212.85\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4363: Policy loss: 0.064184. Value loss: 0.172113. Entropy: 0.299094.\n",
      "Iteration 4364: Policy loss: 0.061413. Value loss: 0.065947. Entropy: 0.298172.\n",
      "Iteration 4365: Policy loss: 0.065581. Value loss: 0.049401. Entropy: 0.297475.\n",
      "episode: 1869   score: 295.0  epsilon: 1.0    steps: 64  evaluation reward: 214.45\n",
      "episode: 1870   score: 405.0  epsilon: 1.0    steps: 176  evaluation reward: 216.4\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4366: Policy loss: 0.029523. Value loss: 0.062473. Entropy: 0.268164.\n",
      "Iteration 4367: Policy loss: 0.021552. Value loss: 0.032246. Entropy: 0.268360.\n",
      "Iteration 4368: Policy loss: 0.022685. Value loss: 0.025410. Entropy: 0.267269.\n",
      "episode: 1871   score: 155.0  epsilon: 1.0    steps: 744  evaluation reward: 216.4\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4369: Policy loss: 0.081427. Value loss: 0.097374. Entropy: 0.300661.\n",
      "Iteration 4370: Policy loss: 0.077882. Value loss: 0.037380. Entropy: 0.300098.\n",
      "Iteration 4371: Policy loss: 0.080148. Value loss: 0.026497. Entropy: 0.300229.\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4372: Policy loss: -0.138196. Value loss: 0.131584. Entropy: 0.307801.\n",
      "Iteration 4373: Policy loss: -0.154019. Value loss: 0.048647. Entropy: 0.309302.\n",
      "Iteration 4374: Policy loss: -0.157947. Value loss: 0.035702. Entropy: 0.308059.\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4375: Policy loss: 0.164689. Value loss: 0.077016. Entropy: 0.305492.\n",
      "Iteration 4376: Policy loss: 0.152206. Value loss: 0.029463. Entropy: 0.303337.\n",
      "Iteration 4377: Policy loss: 0.145933. Value loss: 0.018683. Entropy: 0.302301.\n",
      "episode: 1872   score: 210.0  epsilon: 1.0    steps: 360  evaluation reward: 217.3\n",
      "episode: 1873   score: 495.0  epsilon: 1.0    steps: 368  evaluation reward: 218.5\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4378: Policy loss: -0.031001. Value loss: 0.067635. Entropy: 0.274561.\n",
      "Iteration 4379: Policy loss: -0.036806. Value loss: 0.028931. Entropy: 0.273968.\n",
      "Iteration 4380: Policy loss: -0.036655. Value loss: 0.022992. Entropy: 0.272411.\n",
      "episode: 1874   score: 185.0  epsilon: 1.0    steps: 840  evaluation reward: 217.35\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4381: Policy loss: 0.151504. Value loss: 0.080839. Entropy: 0.299422.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4382: Policy loss: 0.141673. Value loss: 0.034738. Entropy: 0.299157.\n",
      "Iteration 4383: Policy loss: 0.141770. Value loss: 0.024891. Entropy: 0.298033.\n",
      "episode: 1875   score: 300.0  epsilon: 1.0    steps: 688  evaluation reward: 218.7\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4384: Policy loss: -0.294478. Value loss: 0.255007. Entropy: 0.289714.\n",
      "Iteration 4385: Policy loss: -0.283580. Value loss: 0.080416. Entropy: 0.289425.\n",
      "Iteration 4386: Policy loss: -0.310762. Value loss: 0.059456. Entropy: 0.290659.\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4387: Policy loss: 0.191934. Value loss: 0.147653. Entropy: 0.302512.\n",
      "Iteration 4388: Policy loss: 0.179541. Value loss: 0.061269. Entropy: 0.298874.\n",
      "Iteration 4389: Policy loss: 0.174454. Value loss: 0.037109. Entropy: 0.297831.\n",
      "episode: 1876   score: 390.0  epsilon: 1.0    steps: 40  evaluation reward: 220.8\n",
      "episode: 1877   score: 305.0  epsilon: 1.0    steps: 376  evaluation reward: 223.1\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4390: Policy loss: 0.287765. Value loss: 0.052505. Entropy: 0.278457.\n",
      "Iteration 4391: Policy loss: 0.283604. Value loss: 0.019779. Entropy: 0.283464.\n",
      "Iteration 4392: Policy loss: 0.280598. Value loss: 0.015259. Entropy: 0.283755.\n",
      "episode: 1878   score: 300.0  epsilon: 1.0    steps: 48  evaluation reward: 221.2\n",
      "episode: 1879   score: 155.0  epsilon: 1.0    steps: 352  evaluation reward: 221.0\n",
      "episode: 1880   score: 80.0  epsilon: 1.0    steps: 840  evaluation reward: 220.8\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4393: Policy loss: -0.066169. Value loss: 0.102758. Entropy: 0.269149.\n",
      "Iteration 4394: Policy loss: -0.064358. Value loss: 0.049545. Entropy: 0.270973.\n",
      "Iteration 4395: Policy loss: -0.073740. Value loss: 0.038707. Entropy: 0.270407.\n",
      "episode: 1881   score: 525.0  epsilon: 1.0    steps: 224  evaluation reward: 224.1\n",
      "episode: 1882   score: 210.0  epsilon: 1.0    steps: 944  evaluation reward: 225.25\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4396: Policy loss: -0.001903. Value loss: 0.087546. Entropy: 0.282713.\n",
      "Iteration 4397: Policy loss: -0.006091. Value loss: 0.045412. Entropy: 0.282978.\n",
      "Iteration 4398: Policy loss: -0.011545. Value loss: 0.031332. Entropy: 0.282905.\n",
      "episode: 1883   score: 110.0  epsilon: 1.0    steps: 528  evaluation reward: 224.8\n",
      "Training network. lr: 0.000217. clip: 0.086646\n",
      "Iteration 4399: Policy loss: 0.026457. Value loss: 0.080103. Entropy: 0.285441.\n",
      "Iteration 4400: Policy loss: 0.031014. Value loss: 0.037623. Entropy: 0.285687.\n",
      "Iteration 4401: Policy loss: 0.020338. Value loss: 0.021672. Entropy: 0.285135.\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4402: Policy loss: 0.021216. Value loss: 0.056056. Entropy: 0.308630.\n",
      "Iteration 4403: Policy loss: 0.018380. Value loss: 0.028082. Entropy: 0.307430.\n",
      "Iteration 4404: Policy loss: 0.014018. Value loss: 0.019374. Entropy: 0.307684.\n",
      "episode: 1884   score: 260.0  epsilon: 1.0    steps: 456  evaluation reward: 225.3\n",
      "episode: 1885   score: 105.0  epsilon: 1.0    steps: 536  evaluation reward: 221.6\n",
      "episode: 1886   score: 210.0  epsilon: 1.0    steps: 1008  evaluation reward: 221.6\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4405: Policy loss: 0.097485. Value loss: 0.117346. Entropy: 0.278046.\n",
      "Iteration 4406: Policy loss: 0.097580. Value loss: 0.058271. Entropy: 0.275482.\n",
      "Iteration 4407: Policy loss: 0.091647. Value loss: 0.038932. Entropy: 0.276605.\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4408: Policy loss: 0.090497. Value loss: 0.115923. Entropy: 0.292132.\n",
      "Iteration 4409: Policy loss: 0.075593. Value loss: 0.048771. Entropy: 0.292138.\n",
      "Iteration 4410: Policy loss: 0.062636. Value loss: 0.034761. Entropy: 0.294062.\n",
      "episode: 1887   score: 295.0  epsilon: 1.0    steps: 80  evaluation reward: 222.45\n",
      "episode: 1888   score: 180.0  epsilon: 1.0    steps: 224  evaluation reward: 222.65\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4411: Policy loss: -0.226924. Value loss: 0.270685. Entropy: 0.278464.\n",
      "Iteration 4412: Policy loss: -0.200864. Value loss: 0.137032. Entropy: 0.278141.\n",
      "Iteration 4413: Policy loss: -0.240727. Value loss: 0.065493. Entropy: 0.278780.\n",
      "episode: 1889   score: 180.0  epsilon: 1.0    steps: 288  evaluation reward: 222.35\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4414: Policy loss: 0.256378. Value loss: 0.099365. Entropy: 0.292287.\n",
      "Iteration 4415: Policy loss: 0.252450. Value loss: 0.050175. Entropy: 0.290801.\n",
      "Iteration 4416: Policy loss: 0.246983. Value loss: 0.035478. Entropy: 0.290548.\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4417: Policy loss: 0.117881. Value loss: 0.069961. Entropy: 0.306004.\n",
      "Iteration 4418: Policy loss: 0.117847. Value loss: 0.037201. Entropy: 0.305037.\n",
      "Iteration 4419: Policy loss: 0.112320. Value loss: 0.026758. Entropy: 0.304705.\n",
      "episode: 1890   score: 260.0  epsilon: 1.0    steps: 456  evaluation reward: 222.65\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4420: Policy loss: 0.043973. Value loss: 0.046001. Entropy: 0.288177.\n",
      "Iteration 4421: Policy loss: 0.040209. Value loss: 0.017106. Entropy: 0.288001.\n",
      "Iteration 4422: Policy loss: 0.037771. Value loss: 0.014666. Entropy: 0.286229.\n",
      "episode: 1891   score: 460.0  epsilon: 1.0    steps: 48  evaluation reward: 225.7\n",
      "episode: 1892   score: 180.0  epsilon: 1.0    steps: 480  evaluation reward: 225.7\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4423: Policy loss: -0.068808. Value loss: 0.115737. Entropy: 0.278337.\n",
      "Iteration 4424: Policy loss: -0.073055. Value loss: 0.055507. Entropy: 0.277156.\n",
      "Iteration 4425: Policy loss: -0.082197. Value loss: 0.041226. Entropy: 0.279001.\n",
      "episode: 1893   score: 110.0  epsilon: 1.0    steps: 592  evaluation reward: 225.75\n",
      "episode: 1894   score: 285.0  epsilon: 1.0    steps: 808  evaluation reward: 226.75\n",
      "episode: 1895   score: 260.0  epsilon: 1.0    steps: 936  evaluation reward: 224.95\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4426: Policy loss: 0.150845. Value loss: 0.135874. Entropy: 0.285076.\n",
      "Iteration 4427: Policy loss: 0.150749. Value loss: 0.045907. Entropy: 0.283142.\n",
      "Iteration 4428: Policy loss: 0.148436. Value loss: 0.030859. Entropy: 0.281868.\n",
      "episode: 1896   score: 135.0  epsilon: 1.0    steps: 16  evaluation reward: 223.9\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4429: Policy loss: -0.244262. Value loss: 0.111738. Entropy: 0.272797.\n",
      "Iteration 4430: Policy loss: -0.251266. Value loss: 0.060172. Entropy: 0.272973.\n",
      "Iteration 4431: Policy loss: -0.256826. Value loss: 0.041126. Entropy: 0.271436.\n",
      "episode: 1897   score: 270.0  epsilon: 1.0    steps: 824  evaluation reward: 222.9\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4432: Policy loss: 0.058323. Value loss: 0.108985. Entropy: 0.298181.\n",
      "Iteration 4433: Policy loss: 0.054442. Value loss: 0.044500. Entropy: 0.298121.\n",
      "Iteration 4434: Policy loss: 0.044451. Value loss: 0.030817. Entropy: 0.298485.\n",
      "episode: 1898   score: 165.0  epsilon: 1.0    steps: 328  evaluation reward: 221.1\n",
      "episode: 1899   score: 130.0  epsilon: 1.0    steps: 704  evaluation reward: 221.3\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4435: Policy loss: 0.097042. Value loss: 0.087769. Entropy: 0.272875.\n",
      "Iteration 4436: Policy loss: 0.085878. Value loss: 0.039461. Entropy: 0.271732.\n",
      "Iteration 4437: Policy loss: 0.087536. Value loss: 0.024651. Entropy: 0.270113.\n",
      "episode: 1900   score: 150.0  epsilon: 1.0    steps: 720  evaluation reward: 220.15\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4438: Policy loss: 0.016919. Value loss: 0.059682. Entropy: 0.290836.\n",
      "Iteration 4439: Policy loss: 0.011547. Value loss: 0.027450. Entropy: 0.290754.\n",
      "Iteration 4440: Policy loss: 0.008928. Value loss: 0.019670. Entropy: 0.289597.\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4441: Policy loss: 0.101220. Value loss: 0.075644. Entropy: 0.305209.\n",
      "Iteration 4442: Policy loss: 0.096370. Value loss: 0.031074. Entropy: 0.304070.\n",
      "Iteration 4443: Policy loss: 0.092691. Value loss: 0.023282. Entropy: 0.303503.\n",
      "now time :  2019-09-05 18:51:08.418327\n",
      "episode: 1901   score: 250.0  epsilon: 1.0    steps: 816  evaluation reward: 221.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4444: Policy loss: 0.049681. Value loss: 0.049177. Entropy: 0.298077.\n",
      "Iteration 4445: Policy loss: 0.046389. Value loss: 0.024300. Entropy: 0.297891.\n",
      "Iteration 4446: Policy loss: 0.042677. Value loss: 0.017109. Entropy: 0.297217.\n",
      "episode: 1902   score: 210.0  epsilon: 1.0    steps: 176  evaluation reward: 221.4\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4447: Policy loss: -0.062261. Value loss: 0.069846. Entropy: 0.284719.\n",
      "Iteration 4448: Policy loss: -0.066497. Value loss: 0.028873. Entropy: 0.281748.\n",
      "Iteration 4449: Policy loss: -0.070685. Value loss: 0.019370. Entropy: 0.283638.\n",
      "episode: 1903   score: 270.0  epsilon: 1.0    steps: 24  evaluation reward: 223.0\n",
      "Training network. lr: 0.000216. clip: 0.086489\n",
      "Iteration 4450: Policy loss: 0.190029. Value loss: 0.064358. Entropy: 0.291942.\n",
      "Iteration 4451: Policy loss: 0.185603. Value loss: 0.029010. Entropy: 0.292261.\n",
      "Iteration 4452: Policy loss: 0.182315. Value loss: 0.021320. Entropy: 0.290214.\n",
      "episode: 1904   score: 285.0  epsilon: 1.0    steps: 80  evaluation reward: 221.0\n",
      "episode: 1905   score: 120.0  epsilon: 1.0    steps: 112  evaluation reward: 219.55\n",
      "episode: 1906   score: 210.0  epsilon: 1.0    steps: 336  evaluation reward: 220.45\n",
      "episode: 1907   score: 210.0  epsilon: 1.0    steps: 440  evaluation reward: 220.95\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4453: Policy loss: 0.059847. Value loss: 0.066157. Entropy: 0.247260.\n",
      "Iteration 4454: Policy loss: 0.054082. Value loss: 0.032277. Entropy: 0.246992.\n",
      "Iteration 4455: Policy loss: 0.049597. Value loss: 0.025187. Entropy: 0.247941.\n",
      "episode: 1908   score: 240.0  epsilon: 1.0    steps: 1000  evaluation reward: 221.55\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4456: Policy loss: 0.261569. Value loss: 0.084177. Entropy: 0.305864.\n",
      "Iteration 4457: Policy loss: 0.260674. Value loss: 0.038156. Entropy: 0.305083.\n",
      "Iteration 4458: Policy loss: 0.250779. Value loss: 0.029283. Entropy: 0.304263.\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4459: Policy loss: -0.111101. Value loss: 0.066040. Entropy: 0.294150.\n",
      "Iteration 4460: Policy loss: -0.119217. Value loss: 0.025280. Entropy: 0.295307.\n",
      "Iteration 4461: Policy loss: -0.124611. Value loss: 0.018872. Entropy: 0.295720.\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4462: Policy loss: -0.130727. Value loss: 0.107317. Entropy: 0.305694.\n",
      "Iteration 4463: Policy loss: -0.135332. Value loss: 0.043567. Entropy: 0.306341.\n",
      "Iteration 4464: Policy loss: -0.137736. Value loss: 0.033257. Entropy: 0.306595.\n",
      "episode: 1909   score: 210.0  epsilon: 1.0    steps: 448  evaluation reward: 221.55\n",
      "episode: 1910   score: 135.0  epsilon: 1.0    steps: 944  evaluation reward: 218.35\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4465: Policy loss: 0.237919. Value loss: 0.096480. Entropy: 0.289659.\n",
      "Iteration 4466: Policy loss: 0.231549. Value loss: 0.044850. Entropy: 0.290064.\n",
      "Iteration 4467: Policy loss: 0.229688. Value loss: 0.033301. Entropy: 0.288540.\n",
      "episode: 1911   score: 260.0  epsilon: 1.0    steps: 576  evaluation reward: 218.55\n",
      "episode: 1912   score: 210.0  epsilon: 1.0    steps: 840  evaluation reward: 218.55\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4468: Policy loss: 0.044327. Value loss: 0.052169. Entropy: 0.277774.\n",
      "Iteration 4469: Policy loss: 0.034645. Value loss: 0.030224. Entropy: 0.278190.\n",
      "Iteration 4470: Policy loss: 0.037824. Value loss: 0.022711. Entropy: 0.280163.\n",
      "episode: 1913   score: 240.0  epsilon: 1.0    steps: 352  evaluation reward: 215.45\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4471: Policy loss: -0.075161. Value loss: 0.079401. Entropy: 0.287207.\n",
      "Iteration 4472: Policy loss: -0.077695. Value loss: 0.035946. Entropy: 0.286989.\n",
      "Iteration 4473: Policy loss: -0.077315. Value loss: 0.024168. Entropy: 0.284508.\n",
      "episode: 1914   score: 315.0  epsilon: 1.0    steps: 520  evaluation reward: 216.5\n",
      "episode: 1915   score: 225.0  epsilon: 1.0    steps: 1008  evaluation reward: 216.5\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4474: Policy loss: -0.014066. Value loss: 0.109998. Entropy: 0.295289.\n",
      "Iteration 4475: Policy loss: -0.017743. Value loss: 0.051660. Entropy: 0.296030.\n",
      "Iteration 4476: Policy loss: -0.028025. Value loss: 0.039750. Entropy: 0.296548.\n",
      "episode: 1916   score: 230.0  epsilon: 1.0    steps: 296  evaluation reward: 217.9\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4477: Policy loss: -0.103429. Value loss: 0.062658. Entropy: 0.285990.\n",
      "Iteration 4478: Policy loss: -0.105685. Value loss: 0.027550. Entropy: 0.282382.\n",
      "Iteration 4479: Policy loss: -0.109094. Value loss: 0.019851. Entropy: 0.282841.\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4480: Policy loss: -0.053020. Value loss: 0.079011. Entropy: 0.305493.\n",
      "Iteration 4481: Policy loss: -0.066264. Value loss: 0.027863. Entropy: 0.305322.\n",
      "Iteration 4482: Policy loss: -0.062132. Value loss: 0.017559. Entropy: 0.305309.\n",
      "episode: 1917   score: 135.0  epsilon: 1.0    steps: 48  evaluation reward: 217.75\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4483: Policy loss: 0.014596. Value loss: 0.058239. Entropy: 0.289012.\n",
      "Iteration 4484: Policy loss: 0.004519. Value loss: 0.026280. Entropy: 0.289541.\n",
      "Iteration 4485: Policy loss: 0.007666. Value loss: 0.018742. Entropy: 0.288918.\n",
      "episode: 1918   score: 240.0  epsilon: 1.0    steps: 104  evaluation reward: 218.9\n",
      "episode: 1919   score: 210.0  epsilon: 1.0    steps: 952  evaluation reward: 219.7\n",
      "episode: 1920   score: 260.0  epsilon: 1.0    steps: 952  evaluation reward: 220.15\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4486: Policy loss: 0.035744. Value loss: 0.106576. Entropy: 0.286212.\n",
      "Iteration 4487: Policy loss: 0.021777. Value loss: 0.047848. Entropy: 0.284226.\n",
      "Iteration 4488: Policy loss: 0.018748. Value loss: 0.029956. Entropy: 0.283918.\n",
      "episode: 1921   score: 260.0  epsilon: 1.0    steps: 456  evaluation reward: 219.9\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4489: Policy loss: 0.147509. Value loss: 0.075392. Entropy: 0.274186.\n",
      "Iteration 4490: Policy loss: 0.147188. Value loss: 0.031537. Entropy: 0.271643.\n",
      "Iteration 4491: Policy loss: 0.144810. Value loss: 0.026320. Entropy: 0.271840.\n",
      "episode: 1922   score: 180.0  epsilon: 1.0    steps: 392  evaluation reward: 217.25\n",
      "episode: 1923   score: 50.0  epsilon: 1.0    steps: 512  evaluation reward: 216.2\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4492: Policy loss: 0.138407. Value loss: 0.137354. Entropy: 0.276000.\n",
      "Iteration 4493: Policy loss: 0.124154. Value loss: 0.054803. Entropy: 0.274572.\n",
      "Iteration 4494: Policy loss: 0.123764. Value loss: 0.040455. Entropy: 0.273957.\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4495: Policy loss: -0.066511. Value loss: 0.117557. Entropy: 0.304608.\n",
      "Iteration 4496: Policy loss: -0.071880. Value loss: 0.050169. Entropy: 0.305121.\n",
      "Iteration 4497: Policy loss: -0.085910. Value loss: 0.034733. Entropy: 0.304733.\n",
      "episode: 1924   score: 190.0  epsilon: 1.0    steps: 112  evaluation reward: 216.6\n",
      "episode: 1925   score: 285.0  epsilon: 1.0    steps: 136  evaluation reward: 217.35\n",
      "Training network. lr: 0.000216. clip: 0.086333\n",
      "Iteration 4498: Policy loss: 0.140402. Value loss: 0.080031. Entropy: 0.269448.\n",
      "Iteration 4499: Policy loss: 0.130181. Value loss: 0.039820. Entropy: 0.270563.\n",
      "Iteration 4500: Policy loss: 0.129022. Value loss: 0.035104. Entropy: 0.270650.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4501: Policy loss: 0.041902. Value loss: 0.115888. Entropy: 0.305186.\n",
      "Iteration 4502: Policy loss: 0.046981. Value loss: 0.045199. Entropy: 0.304312.\n",
      "Iteration 4503: Policy loss: 0.039109. Value loss: 0.026588. Entropy: 0.303808.\n",
      "episode: 1926   score: 135.0  epsilon: 1.0    steps: 400  evaluation reward: 213.4\n",
      "episode: 1927   score: 180.0  epsilon: 1.0    steps: 424  evaluation reward: 213.7\n",
      "episode: 1928   score: 210.0  epsilon: 1.0    steps: 1024  evaluation reward: 211.2\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4504: Policy loss: 0.126488. Value loss: 0.087982. Entropy: 0.278895.\n",
      "Iteration 4505: Policy loss: 0.128520. Value loss: 0.038997. Entropy: 0.276654.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4506: Policy loss: 0.126217. Value loss: 0.026996. Entropy: 0.278411.\n",
      "episode: 1929   score: 210.0  epsilon: 1.0    steps: 200  evaluation reward: 211.25\n",
      "episode: 1930   score: 210.0  epsilon: 1.0    steps: 456  evaluation reward: 211.55\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4507: Policy loss: -0.038504. Value loss: 0.124574. Entropy: 0.261726.\n",
      "Iteration 4508: Policy loss: -0.051995. Value loss: 0.060988. Entropy: 0.262168.\n",
      "Iteration 4509: Policy loss: -0.042911. Value loss: 0.042969. Entropy: 0.262558.\n",
      "episode: 1931   score: 180.0  epsilon: 1.0    steps: 616  evaluation reward: 211.55\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4510: Policy loss: -0.156713. Value loss: 0.131801. Entropy: 0.295201.\n",
      "Iteration 4511: Policy loss: -0.162554. Value loss: 0.055581. Entropy: 0.293970.\n",
      "Iteration 4512: Policy loss: -0.163275. Value loss: 0.040976. Entropy: 0.293491.\n",
      "episode: 1932   score: 435.0  epsilon: 1.0    steps: 136  evaluation reward: 214.55\n",
      "episode: 1933   score: 100.0  epsilon: 1.0    steps: 376  evaluation reward: 214.75\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4513: Policy loss: 0.022754. Value loss: 0.097306. Entropy: 0.279220.\n",
      "Iteration 4514: Policy loss: 0.013975. Value loss: 0.048975. Entropy: 0.277526.\n",
      "Iteration 4515: Policy loss: 0.009914. Value loss: 0.034494. Entropy: 0.277600.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4516: Policy loss: 0.009317. Value loss: 0.087572. Entropy: 0.305779.\n",
      "Iteration 4517: Policy loss: -0.000890. Value loss: 0.028846. Entropy: 0.304077.\n",
      "Iteration 4518: Policy loss: 0.005161. Value loss: 0.016541. Entropy: 0.304471.\n",
      "episode: 1934   score: 330.0  epsilon: 1.0    steps: 216  evaluation reward: 217.05\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4519: Policy loss: 0.292770. Value loss: 0.118429. Entropy: 0.292187.\n",
      "Iteration 4520: Policy loss: 0.277437. Value loss: 0.060357. Entropy: 0.290321.\n",
      "Iteration 4521: Policy loss: 0.267110. Value loss: 0.036184. Entropy: 0.290601.\n",
      "episode: 1935   score: 145.0  epsilon: 1.0    steps: 160  evaluation reward: 216.1\n",
      "episode: 1936   score: 210.0  epsilon: 1.0    steps: 944  evaluation reward: 216.85\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4522: Policy loss: -0.066247. Value loss: 0.095123. Entropy: 0.287309.\n",
      "Iteration 4523: Policy loss: -0.070204. Value loss: 0.037269. Entropy: 0.287085.\n",
      "Iteration 4524: Policy loss: -0.074509. Value loss: 0.024772. Entropy: 0.287573.\n",
      "episode: 1937   score: 120.0  epsilon: 1.0    steps: 256  evaluation reward: 216.85\n",
      "episode: 1938   score: 365.0  epsilon: 1.0    steps: 272  evaluation reward: 219.75\n",
      "episode: 1939   score: 185.0  epsilon: 1.0    steps: 696  evaluation reward: 218.95\n",
      "episode: 1940   score: 290.0  epsilon: 1.0    steps: 968  evaluation reward: 220.3\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4525: Policy loss: -0.001671. Value loss: 0.086777. Entropy: 0.252260.\n",
      "Iteration 4526: Policy loss: -0.008542. Value loss: 0.043398. Entropy: 0.251521.\n",
      "Iteration 4527: Policy loss: -0.008314. Value loss: 0.032959. Entropy: 0.251282.\n",
      "episode: 1941   score: 210.0  epsilon: 1.0    steps: 904  evaluation reward: 220.6\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4528: Policy loss: -0.160488. Value loss: 0.093789. Entropy: 0.290183.\n",
      "Iteration 4529: Policy loss: -0.160906. Value loss: 0.047085. Entropy: 0.290460.\n",
      "Iteration 4530: Policy loss: -0.168366. Value loss: 0.034465. Entropy: 0.289047.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4531: Policy loss: -0.160356. Value loss: 0.098260. Entropy: 0.297459.\n",
      "Iteration 4532: Policy loss: -0.159594. Value loss: 0.043409. Entropy: 0.299346.\n",
      "Iteration 4533: Policy loss: -0.174916. Value loss: 0.031139. Entropy: 0.297991.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4534: Policy loss: -0.009295. Value loss: 0.069356. Entropy: 0.310367.\n",
      "Iteration 4535: Policy loss: -0.017637. Value loss: 0.033813. Entropy: 0.309559.\n",
      "Iteration 4536: Policy loss: -0.017289. Value loss: 0.025021. Entropy: 0.309804.\n",
      "episode: 1942   score: 150.0  epsilon: 1.0    steps: 8  evaluation reward: 221.25\n",
      "episode: 1943   score: 135.0  epsilon: 1.0    steps: 496  evaluation reward: 220.5\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4537: Policy loss: 0.223249. Value loss: 0.075329. Entropy: 0.282538.\n",
      "Iteration 4538: Policy loss: 0.215788. Value loss: 0.027998. Entropy: 0.281439.\n",
      "Iteration 4539: Policy loss: 0.203594. Value loss: 0.019796. Entropy: 0.279980.\n",
      "episode: 1944   score: 240.0  epsilon: 1.0    steps: 208  evaluation reward: 221.7\n",
      "episode: 1945   score: 225.0  epsilon: 1.0    steps: 352  evaluation reward: 221.35\n",
      "episode: 1946   score: 155.0  epsilon: 1.0    steps: 624  evaluation reward: 221.85\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4540: Policy loss: 0.035786. Value loss: 0.052425. Entropy: 0.263337.\n",
      "Iteration 4541: Policy loss: 0.030467. Value loss: 0.024297. Entropy: 0.262933.\n",
      "Iteration 4542: Policy loss: 0.025916. Value loss: 0.018798. Entropy: 0.264452.\n",
      "episode: 1947   score: 185.0  epsilon: 1.0    steps: 504  evaluation reward: 222.95\n",
      "episode: 1948   score: 210.0  epsilon: 1.0    steps: 712  evaluation reward: 220.1\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4543: Policy loss: 0.067688. Value loss: 0.085408. Entropy: 0.284256.\n",
      "Iteration 4544: Policy loss: 0.067192. Value loss: 0.044934. Entropy: 0.282956.\n",
      "Iteration 4545: Policy loss: 0.062889. Value loss: 0.035550. Entropy: 0.284438.\n",
      "episode: 1949   score: 210.0  epsilon: 1.0    steps: 328  evaluation reward: 220.1\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4546: Policy loss: -0.028922. Value loss: 0.099566. Entropy: 0.294730.\n",
      "Iteration 4547: Policy loss: -0.026913. Value loss: 0.055684. Entropy: 0.292775.\n",
      "Iteration 4548: Policy loss: -0.028719. Value loss: 0.039448. Entropy: 0.293990.\n",
      "Training network. lr: 0.000215. clip: 0.086185\n",
      "Iteration 4549: Policy loss: -0.230089. Value loss: 0.331165. Entropy: 0.310071.\n",
      "Iteration 4550: Policy loss: -0.265316. Value loss: 0.238173. Entropy: 0.308240.\n",
      "Iteration 4551: Policy loss: -0.264304. Value loss: 0.173559. Entropy: 0.309061.\n",
      "episode: 1950   score: 150.0  epsilon: 1.0    steps: 152  evaluation reward: 216.55\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4552: Policy loss: 0.150832. Value loss: 0.077630. Entropy: 0.292950.\n",
      "Iteration 4553: Policy loss: 0.146405. Value loss: 0.030970. Entropy: 0.292827.\n",
      "Iteration 4554: Policy loss: 0.145105. Value loss: 0.025751. Entropy: 0.292691.\n",
      "now time :  2019-09-05 18:57:59.473722\n",
      "episode: 1951   score: 65.0  epsilon: 1.0    steps: 360  evaluation reward: 215.05\n",
      "episode: 1952   score: 210.0  epsilon: 1.0    steps: 880  evaluation reward: 215.95\n",
      "episode: 1953   score: 260.0  epsilon: 1.0    steps: 896  evaluation reward: 216.45\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4555: Policy loss: 0.007124. Value loss: 0.109024. Entropy: 0.284022.\n",
      "Iteration 4556: Policy loss: -0.000897. Value loss: 0.049295. Entropy: 0.283969.\n",
      "Iteration 4557: Policy loss: -0.005436. Value loss: 0.038919. Entropy: 0.284101.\n",
      "episode: 1954   score: 380.0  epsilon: 1.0    steps: 232  evaluation reward: 219.2\n",
      "episode: 1955   score: 105.0  epsilon: 1.0    steps: 736  evaluation reward: 218.8\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4558: Policy loss: 0.082059. Value loss: 0.093528. Entropy: 0.268066.\n",
      "Iteration 4559: Policy loss: 0.079077. Value loss: 0.039786. Entropy: 0.267064.\n",
      "Iteration 4560: Policy loss: 0.072024. Value loss: 0.026379. Entropy: 0.265386.\n",
      "episode: 1956   score: 320.0  epsilon: 1.0    steps: 656  evaluation reward: 219.75\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4561: Policy loss: 0.070411. Value loss: 0.132427. Entropy: 0.295604.\n",
      "Iteration 4562: Policy loss: 0.057824. Value loss: 0.058329. Entropy: 0.296347.\n",
      "Iteration 4563: Policy loss: 0.052337. Value loss: 0.047738. Entropy: 0.295639.\n",
      "episode: 1957   score: 260.0  epsilon: 1.0    steps: 136  evaluation reward: 219.25\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4564: Policy loss: 0.022489. Value loss: 0.097826. Entropy: 0.297529.\n",
      "Iteration 4565: Policy loss: 0.019067. Value loss: 0.044291. Entropy: 0.296922.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4566: Policy loss: 0.015791. Value loss: 0.032774. Entropy: 0.297691.\n",
      "episode: 1958   score: 185.0  epsilon: 1.0    steps: 656  evaluation reward: 219.5\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4567: Policy loss: -0.024466. Value loss: 0.068519. Entropy: 0.295802.\n",
      "Iteration 4568: Policy loss: -0.027434. Value loss: 0.030521. Entropy: 0.294232.\n",
      "Iteration 4569: Policy loss: -0.031347. Value loss: 0.024439. Entropy: 0.294900.\n",
      "episode: 1959   score: 155.0  epsilon: 1.0    steps: 192  evaluation reward: 219.85\n",
      "episode: 1960   score: 160.0  epsilon: 1.0    steps: 304  evaluation reward: 219.8\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4570: Policy loss: -0.022365. Value loss: 0.091257. Entropy: 0.281768.\n",
      "Iteration 4571: Policy loss: -0.025013. Value loss: 0.052845. Entropy: 0.282308.\n",
      "Iteration 4572: Policy loss: -0.030739. Value loss: 0.039667. Entropy: 0.282447.\n",
      "episode: 1961   score: 155.0  epsilon: 1.0    steps: 24  evaluation reward: 218.15\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4573: Policy loss: 0.084654. Value loss: 0.117300. Entropy: 0.298413.\n",
      "Iteration 4574: Policy loss: 0.082051. Value loss: 0.060257. Entropy: 0.298063.\n",
      "Iteration 4575: Policy loss: 0.082128. Value loss: 0.034634. Entropy: 0.296577.\n",
      "episode: 1962   score: 255.0  epsilon: 1.0    steps: 472  evaluation reward: 219.65\n",
      "episode: 1963   score: 210.0  epsilon: 1.0    steps: 936  evaluation reward: 220.2\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4576: Policy loss: 0.007688. Value loss: 0.077775. Entropy: 0.294226.\n",
      "Iteration 4577: Policy loss: -0.003140. Value loss: 0.035358. Entropy: 0.292790.\n",
      "Iteration 4578: Policy loss: 0.001274. Value loss: 0.026055. Entropy: 0.294141.\n",
      "episode: 1964   score: 210.0  epsilon: 1.0    steps: 592  evaluation reward: 220.8\n",
      "episode: 1965   score: 160.0  epsilon: 1.0    steps: 816  evaluation reward: 220.15\n",
      "episode: 1966   score: 55.0  epsilon: 1.0    steps: 832  evaluation reward: 219.3\n",
      "episode: 1967   score: 225.0  epsilon: 1.0    steps: 864  evaluation reward: 220.45\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4579: Policy loss: -0.130956. Value loss: 0.321636. Entropy: 0.269080.\n",
      "Iteration 4580: Policy loss: -0.127302. Value loss: 0.133499. Entropy: 0.268405.\n",
      "Iteration 4581: Policy loss: -0.148777. Value loss: 0.077331. Entropy: 0.269802.\n",
      "episode: 1968   score: 80.0  epsilon: 1.0    steps: 72  evaluation reward: 218.1\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4582: Policy loss: 0.007430. Value loss: 0.092557. Entropy: 0.281782.\n",
      "Iteration 4583: Policy loss: 0.000839. Value loss: 0.049204. Entropy: 0.281819.\n",
      "Iteration 4584: Policy loss: -0.007487. Value loss: 0.038917. Entropy: 0.281731.\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4585: Policy loss: -0.262247. Value loss: 0.219874. Entropy: 0.309913.\n",
      "Iteration 4586: Policy loss: -0.278389. Value loss: 0.078681. Entropy: 0.310022.\n",
      "Iteration 4587: Policy loss: -0.285850. Value loss: 0.074184. Entropy: 0.310113.\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4588: Policy loss: -0.032752. Value loss: 0.119149. Entropy: 0.303738.\n",
      "Iteration 4589: Policy loss: -0.038077. Value loss: 0.047815. Entropy: 0.304491.\n",
      "Iteration 4590: Policy loss: -0.044151. Value loss: 0.031589. Entropy: 0.304093.\n",
      "episode: 1969   score: 110.0  epsilon: 1.0    steps: 184  evaluation reward: 216.25\n",
      "episode: 1970   score: 155.0  epsilon: 1.0    steps: 1016  evaluation reward: 213.75\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4591: Policy loss: -0.015399. Value loss: 0.094076. Entropy: 0.293887.\n",
      "Iteration 4592: Policy loss: -0.016742. Value loss: 0.035624. Entropy: 0.295567.\n",
      "Iteration 4593: Policy loss: -0.023836. Value loss: 0.025741. Entropy: 0.293892.\n",
      "episode: 1971   score: 445.0  epsilon: 1.0    steps: 264  evaluation reward: 216.65\n",
      "episode: 1972   score: 210.0  epsilon: 1.0    steps: 720  evaluation reward: 216.65\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4594: Policy loss: 0.000071. Value loss: 0.106720. Entropy: 0.266558.\n",
      "Iteration 4595: Policy loss: -0.008735. Value loss: 0.051061. Entropy: 0.267667.\n",
      "Iteration 4596: Policy loss: -0.003772. Value loss: 0.037187. Entropy: 0.265724.\n",
      "episode: 1973   score: 180.0  epsilon: 1.0    steps: 400  evaluation reward: 213.5\n",
      "episode: 1974   score: 410.0  epsilon: 1.0    steps: 528  evaluation reward: 215.75\n",
      "episode: 1975   score: 240.0  epsilon: 1.0    steps: 848  evaluation reward: 215.15\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4597: Policy loss: 0.053167. Value loss: 0.060795. Entropy: 0.274975.\n",
      "Iteration 4598: Policy loss: 0.052364. Value loss: 0.029131. Entropy: 0.275309.\n",
      "Iteration 4599: Policy loss: 0.044394. Value loss: 0.021700. Entropy: 0.275659.\n",
      "episode: 1976   score: 225.0  epsilon: 1.0    steps: 320  evaluation reward: 213.5\n",
      "Training network. lr: 0.000215. clip: 0.086029\n",
      "Iteration 4600: Policy loss: -0.330702. Value loss: 0.229510. Entropy: 0.288434.\n",
      "Iteration 4601: Policy loss: -0.357175. Value loss: 0.108346. Entropy: 0.289472.\n",
      "Iteration 4602: Policy loss: -0.365298. Value loss: 0.079187. Entropy: 0.289322.\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4603: Policy loss: 0.030895. Value loss: 0.098399. Entropy: 0.310361.\n",
      "Iteration 4604: Policy loss: 0.024996. Value loss: 0.039963. Entropy: 0.309733.\n",
      "Iteration 4605: Policy loss: 0.023742. Value loss: 0.027253. Entropy: 0.309229.\n",
      "episode: 1977   score: 155.0  epsilon: 1.0    steps: 120  evaluation reward: 212.0\n",
      "episode: 1978   score: 60.0  epsilon: 1.0    steps: 392  evaluation reward: 209.6\n",
      "episode: 1979   score: 85.0  epsilon: 1.0    steps: 808  evaluation reward: 208.9\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4606: Policy loss: 0.132700. Value loss: 0.068293. Entropy: 0.267698.\n",
      "Iteration 4607: Policy loss: 0.129494. Value loss: 0.036304. Entropy: 0.268414.\n",
      "Iteration 4608: Policy loss: 0.123263. Value loss: 0.027506. Entropy: 0.267839.\n",
      "episode: 1980   score: 120.0  epsilon: 1.0    steps: 632  evaluation reward: 209.3\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4609: Policy loss: -0.128269. Value loss: 0.074196. Entropy: 0.290449.\n",
      "Iteration 4610: Policy loss: -0.132184. Value loss: 0.031784. Entropy: 0.289861.\n",
      "Iteration 4611: Policy loss: -0.143144. Value loss: 0.021514. Entropy: 0.288443.\n",
      "episode: 1981   score: 195.0  epsilon: 1.0    steps: 440  evaluation reward: 206.0\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4612: Policy loss: 0.022544. Value loss: 0.087656. Entropy: 0.296412.\n",
      "Iteration 4613: Policy loss: 0.022628. Value loss: 0.038308. Entropy: 0.296091.\n",
      "Iteration 4614: Policy loss: 0.014667. Value loss: 0.023761. Entropy: 0.295704.\n",
      "episode: 1982   score: 470.0  epsilon: 1.0    steps: 552  evaluation reward: 208.6\n",
      "episode: 1983   score: 265.0  epsilon: 1.0    steps: 880  evaluation reward: 210.15\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4615: Policy loss: -0.041453. Value loss: 0.097638. Entropy: 0.291684.\n",
      "Iteration 4616: Policy loss: -0.044302. Value loss: 0.041689. Entropy: 0.290766.\n",
      "Iteration 4617: Policy loss: -0.051307. Value loss: 0.028029. Entropy: 0.290161.\n",
      "episode: 1984   score: 60.0  epsilon: 1.0    steps: 800  evaluation reward: 208.15\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4618: Policy loss: -0.130796. Value loss: 0.136023. Entropy: 0.296152.\n",
      "Iteration 4619: Policy loss: -0.149823. Value loss: 0.053080. Entropy: 0.296972.\n",
      "Iteration 4620: Policy loss: -0.148776. Value loss: 0.032438. Entropy: 0.296810.\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4621: Policy loss: 0.320599. Value loss: 0.289441. Entropy: 0.306178.\n",
      "Iteration 4622: Policy loss: 0.281113. Value loss: 0.099841. Entropy: 0.308240.\n",
      "Iteration 4623: Policy loss: 0.262702. Value loss: 0.070586. Entropy: 0.307364.\n",
      "episode: 1985   score: 260.0  epsilon: 1.0    steps: 48  evaluation reward: 209.7\n",
      "episode: 1986   score: 210.0  epsilon: 1.0    steps: 224  evaluation reward: 209.7\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4624: Policy loss: 0.046335. Value loss: 0.092961. Entropy: 0.280980.\n",
      "Iteration 4625: Policy loss: 0.041518. Value loss: 0.049754. Entropy: 0.282550.\n",
      "Iteration 4626: Policy loss: 0.037905. Value loss: 0.036543. Entropy: 0.282753.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1987   score: 410.0  epsilon: 1.0    steps: 768  evaluation reward: 210.85\n",
      "episode: 1988   score: 270.0  epsilon: 1.0    steps: 960  evaluation reward: 211.75\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4627: Policy loss: 0.188276. Value loss: 0.109858. Entropy: 0.299495.\n",
      "Iteration 4628: Policy loss: 0.187414. Value loss: 0.058216. Entropy: 0.298803.\n",
      "Iteration 4629: Policy loss: 0.175221. Value loss: 0.042385. Entropy: 0.298045.\n",
      "episode: 1989   score: 180.0  epsilon: 1.0    steps: 792  evaluation reward: 211.75\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4630: Policy loss: 0.215674. Value loss: 0.156944. Entropy: 0.293744.\n",
      "Iteration 4631: Policy loss: 0.221254. Value loss: 0.057408. Entropy: 0.294680.\n",
      "Iteration 4632: Policy loss: 0.214929. Value loss: 0.039783. Entropy: 0.292416.\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4633: Policy loss: 0.126910. Value loss: 0.096629. Entropy: 0.310189.\n",
      "Iteration 4634: Policy loss: 0.119061. Value loss: 0.038549. Entropy: 0.307879.\n",
      "Iteration 4635: Policy loss: 0.116596. Value loss: 0.022154. Entropy: 0.308423.\n",
      "episode: 1990   score: 135.0  epsilon: 1.0    steps: 40  evaluation reward: 210.5\n",
      "episode: 1991   score: 270.0  epsilon: 1.0    steps: 96  evaluation reward: 208.6\n",
      "episode: 1992   score: 135.0  epsilon: 1.0    steps: 192  evaluation reward: 208.15\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4636: Policy loss: -0.145989. Value loss: 0.291586. Entropy: 0.274951.\n",
      "Iteration 4637: Policy loss: -0.166225. Value loss: 0.169370. Entropy: 0.276174.\n",
      "Iteration 4638: Policy loss: -0.172813. Value loss: 0.117931. Entropy: 0.285659.\n",
      "episode: 1993   score: 240.0  epsilon: 1.0    steps: 200  evaluation reward: 209.45\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4639: Policy loss: -0.252163. Value loss: 0.105662. Entropy: 0.300695.\n",
      "Iteration 4640: Policy loss: -0.260659. Value loss: 0.050818. Entropy: 0.301010.\n",
      "Iteration 4641: Policy loss: -0.264582. Value loss: 0.040929. Entropy: 0.303646.\n",
      "episode: 1994   score: 290.0  epsilon: 1.0    steps: 336  evaluation reward: 209.5\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4642: Policy loss: 0.163386. Value loss: 0.131181. Entropy: 0.301800.\n",
      "Iteration 4643: Policy loss: 0.147656. Value loss: 0.042366. Entropy: 0.301936.\n",
      "Iteration 4644: Policy loss: 0.155530. Value loss: 0.032899. Entropy: 0.303239.\n",
      "episode: 1995   score: 410.0  epsilon: 1.0    steps: 928  evaluation reward: 211.0\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4645: Policy loss: 0.294755. Value loss: 0.104560. Entropy: 0.309873.\n",
      "Iteration 4646: Policy loss: 0.281524. Value loss: 0.036671. Entropy: 0.309698.\n",
      "Iteration 4647: Policy loss: 0.277202. Value loss: 0.026710. Entropy: 0.308125.\n",
      "Training network. lr: 0.000215. clip: 0.085872\n",
      "Iteration 4648: Policy loss: 0.276464. Value loss: 0.087651. Entropy: 0.307439.\n",
      "Iteration 4649: Policy loss: 0.265090. Value loss: 0.032105. Entropy: 0.304595.\n",
      "Iteration 4650: Policy loss: 0.265406. Value loss: 0.026894. Entropy: 0.304742.\n",
      "episode: 1996   score: 35.0  epsilon: 1.0    steps: 48  evaluation reward: 210.0\n",
      "episode: 1997   score: 155.0  epsilon: 1.0    steps: 184  evaluation reward: 208.85\n",
      "episode: 1998   score: 110.0  epsilon: 1.0    steps: 192  evaluation reward: 208.3\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4651: Policy loss: -0.055510. Value loss: 0.085648. Entropy: 0.277527.\n",
      "Iteration 4652: Policy loss: -0.057311. Value loss: 0.039017. Entropy: 0.275743.\n",
      "Iteration 4653: Policy loss: -0.058926. Value loss: 0.032436. Entropy: 0.279923.\n",
      "episode: 1999   score: 210.0  epsilon: 1.0    steps: 424  evaluation reward: 209.1\n",
      "episode: 2000   score: 290.0  epsilon: 1.0    steps: 864  evaluation reward: 210.5\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4654: Policy loss: 0.082279. Value loss: 0.084155. Entropy: 0.292852.\n",
      "Iteration 4655: Policy loss: 0.075634. Value loss: 0.042162. Entropy: 0.291186.\n",
      "Iteration 4656: Policy loss: 0.079743. Value loss: 0.029538. Entropy: 0.290251.\n",
      "now time :  2019-09-05 19:04:18.385136\n",
      "episode: 2001   score: 300.0  epsilon: 1.0    steps: 256  evaluation reward: 211.0\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4657: Policy loss: -0.053477. Value loss: 0.094031. Entropy: 0.295753.\n",
      "Iteration 4658: Policy loss: -0.058142. Value loss: 0.036890. Entropy: 0.292073.\n",
      "Iteration 4659: Policy loss: -0.065707. Value loss: 0.027601. Entropy: 0.294388.\n",
      "episode: 2002   score: 105.0  epsilon: 1.0    steps: 24  evaluation reward: 209.95\n",
      "episode: 2003   score: 320.0  epsilon: 1.0    steps: 824  evaluation reward: 210.45\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4660: Policy loss: 0.260071. Value loss: 0.078667. Entropy: 0.296606.\n",
      "Iteration 4661: Policy loss: 0.252255. Value loss: 0.033528. Entropy: 0.295411.\n",
      "Iteration 4662: Policy loss: 0.251713. Value loss: 0.021327. Entropy: 0.294607.\n",
      "episode: 2004   score: 110.0  epsilon: 1.0    steps: 272  evaluation reward: 208.7\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4663: Policy loss: -0.011149. Value loss: 0.103523. Entropy: 0.289787.\n",
      "Iteration 4664: Policy loss: -0.022344. Value loss: 0.052606. Entropy: 0.287334.\n",
      "Iteration 4665: Policy loss: -0.015189. Value loss: 0.041148. Entropy: 0.287860.\n",
      "episode: 2005   score: 210.0  epsilon: 1.0    steps: 384  evaluation reward: 209.6\n",
      "episode: 2006   score: 135.0  epsilon: 1.0    steps: 928  evaluation reward: 208.85\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4666: Policy loss: -0.054286. Value loss: 0.095497. Entropy: 0.293415.\n",
      "Iteration 4667: Policy loss: -0.063237. Value loss: 0.044094. Entropy: 0.291943.\n",
      "Iteration 4668: Policy loss: -0.065958. Value loss: 0.030660. Entropy: 0.291378.\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4669: Policy loss: -0.028802. Value loss: 0.064222. Entropy: 0.298899.\n",
      "Iteration 4670: Policy loss: -0.028731. Value loss: 0.018238. Entropy: 0.299556.\n",
      "Iteration 4671: Policy loss: -0.037258. Value loss: 0.012414. Entropy: 0.297863.\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4672: Policy loss: 0.054789. Value loss: 0.093481. Entropy: 0.308992.\n",
      "Iteration 4673: Policy loss: 0.042446. Value loss: 0.040280. Entropy: 0.308419.\n",
      "Iteration 4674: Policy loss: 0.043596. Value loss: 0.030917. Entropy: 0.308008.\n",
      "episode: 2007   score: 280.0  epsilon: 1.0    steps: 56  evaluation reward: 209.55\n",
      "episode: 2008   score: 210.0  epsilon: 1.0    steps: 592  evaluation reward: 209.25\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4675: Policy loss: 0.111239. Value loss: 0.053924. Entropy: 0.281047.\n",
      "Iteration 4676: Policy loss: 0.105282. Value loss: 0.023778. Entropy: 0.279290.\n",
      "Iteration 4677: Policy loss: 0.108543. Value loss: 0.018409. Entropy: 0.279475.\n",
      "episode: 2009   score: 260.0  epsilon: 1.0    steps: 264  evaluation reward: 209.75\n",
      "episode: 2010   score: 110.0  epsilon: 1.0    steps: 384  evaluation reward: 209.5\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4678: Policy loss: -0.000993. Value loss: 0.070067. Entropy: 0.282091.\n",
      "Iteration 4679: Policy loss: -0.004887. Value loss: 0.030535. Entropy: 0.286006.\n",
      "Iteration 4680: Policy loss: -0.009990. Value loss: 0.024104. Entropy: 0.295899.\n",
      "episode: 2011   score: 285.0  epsilon: 1.0    steps: 472  evaluation reward: 209.75\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4681: Policy loss: 0.107392. Value loss: 0.066479. Entropy: 0.297920.\n",
      "Iteration 4682: Policy loss: 0.095579. Value loss: 0.026881. Entropy: 0.296622.\n",
      "Iteration 4683: Policy loss: 0.085786. Value loss: 0.019610. Entropy: 0.297621.\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4684: Policy loss: -0.066377. Value loss: 0.091020. Entropy: 0.304832.\n",
      "Iteration 4685: Policy loss: -0.064109. Value loss: 0.039109. Entropy: 0.304567.\n",
      "Iteration 4686: Policy loss: -0.070140. Value loss: 0.029355. Entropy: 0.304196.\n",
      "episode: 2012   score: 110.0  epsilon: 1.0    steps: 64  evaluation reward: 208.75\n",
      "episode: 2013   score: 225.0  epsilon: 1.0    steps: 152  evaluation reward: 208.6\n",
      "episode: 2014   score: 210.0  epsilon: 1.0    steps: 496  evaluation reward: 207.55\n",
      "episode: 2015   score: 330.0  epsilon: 1.0    steps: 896  evaluation reward: 208.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4687: Policy loss: 0.083504. Value loss: 0.049051. Entropy: 0.293637.\n",
      "Iteration 4688: Policy loss: 0.079692. Value loss: 0.023764. Entropy: 0.295007.\n",
      "Iteration 4689: Policy loss: 0.075649. Value loss: 0.018322. Entropy: 0.294048.\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4690: Policy loss: -0.054595. Value loss: 0.093748. Entropy: 0.311375.\n",
      "Iteration 4691: Policy loss: -0.059340. Value loss: 0.039094. Entropy: 0.311995.\n",
      "Iteration 4692: Policy loss: -0.067386. Value loss: 0.026891. Entropy: 0.310048.\n",
      "episode: 2016   score: 55.0  epsilon: 1.0    steps: 816  evaluation reward: 206.85\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4693: Policy loss: 0.059946. Value loss: 0.092618. Entropy: 0.305802.\n",
      "Iteration 4694: Policy loss: 0.059159. Value loss: 0.034664. Entropy: 0.304339.\n",
      "Iteration 4695: Policy loss: 0.056100. Value loss: 0.026218. Entropy: 0.305042.\n",
      "episode: 2017   score: 210.0  epsilon: 1.0    steps: 448  evaluation reward: 207.6\n",
      "episode: 2018   score: 290.0  epsilon: 1.0    steps: 824  evaluation reward: 208.1\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4696: Policy loss: -0.122153. Value loss: 0.080057. Entropy: 0.297715.\n",
      "Iteration 4697: Policy loss: -0.118829. Value loss: 0.038977. Entropy: 0.297007.\n",
      "Iteration 4698: Policy loss: -0.127794. Value loss: 0.030167. Entropy: 0.298545.\n",
      "episode: 2019   score: 310.0  epsilon: 1.0    steps: 784  evaluation reward: 209.1\n",
      "Training network. lr: 0.000214. clip: 0.085724\n",
      "Iteration 4699: Policy loss: 0.005349. Value loss: 0.093593. Entropy: 0.303907.\n",
      "Iteration 4700: Policy loss: -0.012448. Value loss: 0.046345. Entropy: 0.303747.\n",
      "Iteration 4701: Policy loss: -0.008046. Value loss: 0.033559. Entropy: 0.304572.\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4702: Policy loss: 0.037865. Value loss: 0.058604. Entropy: 0.306232.\n",
      "Iteration 4703: Policy loss: 0.039806. Value loss: 0.025083. Entropy: 0.305419.\n",
      "Iteration 4704: Policy loss: 0.028610. Value loss: 0.017539. Entropy: 0.304361.\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4705: Policy loss: -0.165863. Value loss: 0.270582. Entropy: 0.305765.\n",
      "Iteration 4706: Policy loss: -0.175101. Value loss: 0.198925. Entropy: 0.307197.\n",
      "Iteration 4707: Policy loss: -0.190684. Value loss: 0.176805. Entropy: 0.306335.\n",
      "episode: 2020   score: 285.0  epsilon: 1.0    steps: 608  evaluation reward: 209.35\n",
      "episode: 2021   score: 330.0  epsilon: 1.0    steps: 760  evaluation reward: 210.05\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4708: Policy loss: 0.179618. Value loss: 0.070234. Entropy: 0.297139.\n",
      "Iteration 4709: Policy loss: 0.175097. Value loss: 0.039060. Entropy: 0.297886.\n",
      "Iteration 4710: Policy loss: 0.174113. Value loss: 0.029889. Entropy: 0.295983.\n",
      "episode: 2022   score: 340.0  epsilon: 1.0    steps: 904  evaluation reward: 211.65\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4711: Policy loss: -0.272328. Value loss: 0.133513. Entropy: 0.301566.\n",
      "Iteration 4712: Policy loss: -0.288626. Value loss: 0.064500. Entropy: 0.301639.\n",
      "Iteration 4713: Policy loss: -0.292719. Value loss: 0.043953. Entropy: 0.301432.\n",
      "episode: 2023   score: 210.0  epsilon: 1.0    steps: 184  evaluation reward: 213.25\n",
      "episode: 2024   score: 210.0  epsilon: 1.0    steps: 232  evaluation reward: 213.45\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4714: Policy loss: -0.002366. Value loss: 0.072190. Entropy: 0.281309.\n",
      "Iteration 4715: Policy loss: -0.004012. Value loss: 0.032197. Entropy: 0.281190.\n",
      "Iteration 4716: Policy loss: -0.014555. Value loss: 0.024920. Entropy: 0.283323.\n",
      "episode: 2025   score: 210.0  epsilon: 1.0    steps: 736  evaluation reward: 212.7\n",
      "episode: 2026   score: 270.0  epsilon: 1.0    steps: 992  evaluation reward: 214.05\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4717: Policy loss: -0.234902. Value loss: 0.213521. Entropy: 0.296898.\n",
      "Iteration 4718: Policy loss: -0.227610. Value loss: 0.086512. Entropy: 0.297543.\n",
      "Iteration 4719: Policy loss: -0.270843. Value loss: 0.066304. Entropy: 0.298329.\n",
      "episode: 2027   score: 820.0  epsilon: 1.0    steps: 888  evaluation reward: 220.45\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4720: Policy loss: -0.172988. Value loss: 0.134794. Entropy: 0.292377.\n",
      "Iteration 4721: Policy loss: -0.180001. Value loss: 0.056936. Entropy: 0.292464.\n",
      "Iteration 4722: Policy loss: -0.185027. Value loss: 0.041640. Entropy: 0.292289.\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4723: Policy loss: -0.324574. Value loss: 0.221576. Entropy: 0.302744.\n",
      "Iteration 4724: Policy loss: -0.370731. Value loss: 0.112629. Entropy: 0.300860.\n",
      "Iteration 4725: Policy loss: -0.376308. Value loss: 0.083448. Entropy: 0.302128.\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4726: Policy loss: 0.098715. Value loss: 0.116127. Entropy: 0.307960.\n",
      "Iteration 4727: Policy loss: 0.089154. Value loss: 0.038795. Entropy: 0.307627.\n",
      "Iteration 4728: Policy loss: 0.089198. Value loss: 0.028628. Entropy: 0.307690.\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4729: Policy loss: 0.008419. Value loss: 0.093289. Entropy: 0.305916.\n",
      "Iteration 4730: Policy loss: -0.000325. Value loss: 0.040599. Entropy: 0.305356.\n",
      "Iteration 4731: Policy loss: -0.003140. Value loss: 0.031154. Entropy: 0.305425.\n",
      "episode: 2028   score: 470.0  epsilon: 1.0    steps: 88  evaluation reward: 223.05\n",
      "episode: 2029   score: 210.0  epsilon: 1.0    steps: 648  evaluation reward: 223.05\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4732: Policy loss: -0.010831. Value loss: 0.167100. Entropy: 0.280252.\n",
      "Iteration 4733: Policy loss: -0.014747. Value loss: 0.072729. Entropy: 0.279513.\n",
      "Iteration 4734: Policy loss: -0.030439. Value loss: 0.047406. Entropy: 0.278144.\n",
      "episode: 2030   score: 180.0  epsilon: 1.0    steps: 440  evaluation reward: 222.75\n",
      "episode: 2031   score: 450.0  epsilon: 1.0    steps: 800  evaluation reward: 225.45\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4735: Policy loss: -0.476285. Value loss: 0.535288. Entropy: 0.281711.\n",
      "Iteration 4736: Policy loss: -0.505159. Value loss: 0.181753. Entropy: 0.277178.\n",
      "Iteration 4737: Policy loss: -0.528751. Value loss: 0.091921. Entropy: 0.282126.\n",
      "episode: 2032   score: 330.0  epsilon: 1.0    steps: 560  evaluation reward: 224.4\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4738: Policy loss: 0.345788. Value loss: 0.273224. Entropy: 0.285948.\n",
      "Iteration 4739: Policy loss: 0.329338. Value loss: 0.105139. Entropy: 0.285290.\n",
      "Iteration 4740: Policy loss: 0.303333. Value loss: 0.070910. Entropy: 0.284821.\n",
      "episode: 2033   score: 365.0  epsilon: 1.0    steps: 904  evaluation reward: 227.05\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4741: Policy loss: 0.224951. Value loss: 0.144972. Entropy: 0.299707.\n",
      "Iteration 4742: Policy loss: 0.214832. Value loss: 0.057708. Entropy: 0.299071.\n",
      "Iteration 4743: Policy loss: 0.212062. Value loss: 0.042266. Entropy: 0.298057.\n",
      "episode: 2034   score: 440.0  epsilon: 1.0    steps: 200  evaluation reward: 228.15\n",
      "episode: 2035   score: 95.0  epsilon: 1.0    steps: 896  evaluation reward: 227.65\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4744: Policy loss: 0.211391. Value loss: 0.063710. Entropy: 0.277123.\n",
      "Iteration 4745: Policy loss: 0.210259. Value loss: 0.029164. Entropy: 0.277927.\n",
      "Iteration 4746: Policy loss: 0.204102. Value loss: 0.022028. Entropy: 0.276784.\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4747: Policy loss: -0.121594. Value loss: 0.143995. Entropy: 0.300748.\n",
      "Iteration 4748: Policy loss: -0.114897. Value loss: 0.049793. Entropy: 0.299745.\n",
      "Iteration 4749: Policy loss: -0.128922. Value loss: 0.036433. Entropy: 0.300500.\n",
      "episode: 2036   score: 180.0  epsilon: 1.0    steps: 64  evaluation reward: 227.35\n",
      "episode: 2037   score: 435.0  epsilon: 1.0    steps: 560  evaluation reward: 230.5\n",
      "Training network. lr: 0.000214. clip: 0.085568\n",
      "Iteration 4750: Policy loss: 0.022329. Value loss: 0.070741. Entropy: 0.276648.\n",
      "Iteration 4751: Policy loss: 0.011673. Value loss: 0.035841. Entropy: 0.276849.\n",
      "Iteration 4752: Policy loss: 0.007189. Value loss: 0.027344. Entropy: 0.277559.\n",
      "episode: 2038   score: 240.0  epsilon: 1.0    steps: 112  evaluation reward: 229.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4753: Policy loss: 0.185553. Value loss: 0.094211. Entropy: 0.293916.\n",
      "Iteration 4754: Policy loss: 0.171898. Value loss: 0.041087. Entropy: 0.292758.\n",
      "Iteration 4755: Policy loss: 0.166828. Value loss: 0.027624. Entropy: 0.293595.\n",
      "episode: 2039   score: 240.0  epsilon: 1.0    steps: 40  evaluation reward: 229.8\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4756: Policy loss: 0.009498. Value loss: 0.057772. Entropy: 0.290332.\n",
      "Iteration 4757: Policy loss: 0.006347. Value loss: 0.027603. Entropy: 0.291023.\n",
      "Iteration 4758: Policy loss: 0.003575. Value loss: 0.019544. Entropy: 0.290451.\n",
      "episode: 2040   score: 235.0  epsilon: 1.0    steps: 536  evaluation reward: 229.25\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4759: Policy loss: -0.093708. Value loss: 0.083473. Entropy: 0.293555.\n",
      "Iteration 4760: Policy loss: -0.098124. Value loss: 0.043872. Entropy: 0.294505.\n",
      "Iteration 4761: Policy loss: -0.104875. Value loss: 0.031473. Entropy: 0.294050.\n",
      "episode: 2041   score: 260.0  epsilon: 1.0    steps: 696  evaluation reward: 229.75\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4762: Policy loss: -0.024217. Value loss: 0.250501. Entropy: 0.289525.\n",
      "Iteration 4763: Policy loss: -0.036791. Value loss: 0.109534. Entropy: 0.291204.\n",
      "Iteration 4764: Policy loss: -0.044794. Value loss: 0.065692. Entropy: 0.289667.\n",
      "episode: 2042   score: 345.0  epsilon: 1.0    steps: 1024  evaluation reward: 231.7\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4765: Policy loss: -0.094804. Value loss: 0.189095. Entropy: 0.303758.\n",
      "Iteration 4766: Policy loss: -0.093257. Value loss: 0.089957. Entropy: 0.304381.\n",
      "Iteration 4767: Policy loss: -0.110855. Value loss: 0.073429. Entropy: 0.303346.\n",
      "episode: 2043   score: 210.0  epsilon: 1.0    steps: 400  evaluation reward: 232.45\n",
      "episode: 2044   score: 445.0  epsilon: 1.0    steps: 400  evaluation reward: 234.5\n",
      "episode: 2045   score: 65.0  epsilon: 1.0    steps: 424  evaluation reward: 232.9\n",
      "episode: 2046   score: 180.0  epsilon: 1.0    steps: 768  evaluation reward: 233.15\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4768: Policy loss: 0.107044. Value loss: 0.095542. Entropy: 0.248266.\n",
      "Iteration 4769: Policy loss: 0.097888. Value loss: 0.047899. Entropy: 0.248655.\n",
      "Iteration 4770: Policy loss: 0.095018. Value loss: 0.033945. Entropy: 0.248775.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4771: Policy loss: -0.078319. Value loss: 0.150199. Entropy: 0.301753.\n",
      "Iteration 4772: Policy loss: -0.082104. Value loss: 0.079697. Entropy: 0.301780.\n",
      "Iteration 4773: Policy loss: -0.092070. Value loss: 0.043288. Entropy: 0.300925.\n",
      "episode: 2047   score: 460.0  epsilon: 1.0    steps: 128  evaluation reward: 235.9\n",
      "episode: 2048   score: 225.0  epsilon: 1.0    steps: 512  evaluation reward: 236.05\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4774: Policy loss: -0.085147. Value loss: 0.099585. Entropy: 0.279562.\n",
      "Iteration 4775: Policy loss: -0.095262. Value loss: 0.056044. Entropy: 0.279020.\n",
      "Iteration 4776: Policy loss: -0.096150. Value loss: 0.045092. Entropy: 0.278834.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4777: Policy loss: 0.089792. Value loss: 0.075630. Entropy: 0.306143.\n",
      "Iteration 4778: Policy loss: 0.085022. Value loss: 0.023081. Entropy: 0.305782.\n",
      "Iteration 4779: Policy loss: 0.082730. Value loss: 0.017139. Entropy: 0.306527.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4780: Policy loss: 0.247202. Value loss: 0.107716. Entropy: 0.307677.\n",
      "Iteration 4781: Policy loss: 0.229448. Value loss: 0.036699. Entropy: 0.306657.\n",
      "Iteration 4782: Policy loss: 0.222036. Value loss: 0.026059. Entropy: 0.305910.\n",
      "episode: 2049   score: 165.0  epsilon: 1.0    steps: 464  evaluation reward: 235.6\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4783: Policy loss: -0.198828. Value loss: 0.238411. Entropy: 0.290506.\n",
      "Iteration 4784: Policy loss: -0.188863. Value loss: 0.096517. Entropy: 0.292069.\n",
      "Iteration 4785: Policy loss: -0.191694. Value loss: 0.055139. Entropy: 0.293693.\n",
      "episode: 2050   score: 210.0  epsilon: 1.0    steps: 472  evaluation reward: 236.2\n",
      "now time :  2019-09-05 19:12:18.755018\n",
      "episode: 2051   score: 285.0  epsilon: 1.0    steps: 1016  evaluation reward: 238.4\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4786: Policy loss: -0.080160. Value loss: 0.144047. Entropy: 0.291662.\n",
      "Iteration 4787: Policy loss: -0.080555. Value loss: 0.059462. Entropy: 0.292279.\n",
      "Iteration 4788: Policy loss: -0.092367. Value loss: 0.041648. Entropy: 0.292340.\n",
      "episode: 2052   score: 310.0  epsilon: 1.0    steps: 368  evaluation reward: 239.4\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4789: Policy loss: -0.061093. Value loss: 0.052347. Entropy: 0.287293.\n",
      "Iteration 4790: Policy loss: -0.063542. Value loss: 0.028100. Entropy: 0.290078.\n",
      "Iteration 4791: Policy loss: -0.068930. Value loss: 0.021214. Entropy: 0.290933.\n",
      "episode: 2053   score: 245.0  epsilon: 1.0    steps: 192  evaluation reward: 239.25\n",
      "episode: 2054   score: 330.0  epsilon: 1.0    steps: 272  evaluation reward: 238.75\n",
      "episode: 2055   score: 460.0  epsilon: 1.0    steps: 984  evaluation reward: 242.3\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4792: Policy loss: 0.053441. Value loss: 0.059901. Entropy: 0.279697.\n",
      "Iteration 4793: Policy loss: 0.045722. Value loss: 0.036347. Entropy: 0.278274.\n",
      "Iteration 4794: Policy loss: 0.047563. Value loss: 0.029378. Entropy: 0.280793.\n",
      "episode: 2056   score: 315.0  epsilon: 1.0    steps: 912  evaluation reward: 242.25\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4795: Policy loss: 0.067888. Value loss: 0.109002. Entropy: 0.297371.\n",
      "Iteration 4796: Policy loss: 0.059225. Value loss: 0.056007. Entropy: 0.295477.\n",
      "Iteration 4797: Policy loss: 0.048350. Value loss: 0.037415. Entropy: 0.297243.\n",
      "Training network. lr: 0.000214. clip: 0.085411\n",
      "Iteration 4798: Policy loss: 0.036491. Value loss: 0.082108. Entropy: 0.301326.\n",
      "Iteration 4799: Policy loss: 0.030219. Value loss: 0.033830. Entropy: 0.300206.\n",
      "Iteration 4800: Policy loss: 0.030995. Value loss: 0.032671. Entropy: 0.299630.\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4801: Policy loss: 0.177080. Value loss: 0.117976. Entropy: 0.308349.\n",
      "Iteration 4802: Policy loss: 0.169938. Value loss: 0.048849. Entropy: 0.306981.\n",
      "Iteration 4803: Policy loss: 0.167744. Value loss: 0.032491. Entropy: 0.307669.\n",
      "episode: 2057   score: 160.0  epsilon: 1.0    steps: 256  evaluation reward: 241.25\n",
      "episode: 2058   score: 290.0  epsilon: 1.0    steps: 432  evaluation reward: 242.3\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4804: Policy loss: 0.154852. Value loss: 0.054210. Entropy: 0.283155.\n",
      "Iteration 4805: Policy loss: 0.151149. Value loss: 0.028780. Entropy: 0.281881.\n",
      "Iteration 4806: Policy loss: 0.148690. Value loss: 0.020822. Entropy: 0.281129.\n",
      "episode: 2059   score: 155.0  epsilon: 1.0    steps: 376  evaluation reward: 242.3\n",
      "episode: 2060   score: 180.0  epsilon: 1.0    steps: 880  evaluation reward: 242.5\n",
      "episode: 2061   score: 225.0  epsilon: 1.0    steps: 896  evaluation reward: 243.2\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4807: Policy loss: 0.160390. Value loss: 0.106616. Entropy: 0.286690.\n",
      "Iteration 4808: Policy loss: 0.152083. Value loss: 0.043371. Entropy: 0.286555.\n",
      "Iteration 4809: Policy loss: 0.148994. Value loss: 0.032900. Entropy: 0.285089.\n",
      "episode: 2062   score: 260.0  epsilon: 1.0    steps: 104  evaluation reward: 243.25\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4810: Policy loss: -0.038510. Value loss: 0.061629. Entropy: 0.273044.\n",
      "Iteration 4811: Policy loss: -0.037777. Value loss: 0.032356. Entropy: 0.272517.\n",
      "Iteration 4812: Policy loss: -0.039979. Value loss: 0.026907. Entropy: 0.273867.\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4813: Policy loss: 0.154285. Value loss: 0.148006. Entropy: 0.312210.\n",
      "Iteration 4814: Policy loss: 0.140994. Value loss: 0.056727. Entropy: 0.312282.\n",
      "Iteration 4815: Policy loss: 0.137001. Value loss: 0.030255. Entropy: 0.311753.\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4816: Policy loss: 0.133567. Value loss: 0.060962. Entropy: 0.308216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4817: Policy loss: 0.129565. Value loss: 0.023255. Entropy: 0.307062.\n",
      "Iteration 4818: Policy loss: 0.123714. Value loss: 0.018018. Entropy: 0.307022.\n",
      "episode: 2063   score: 100.0  epsilon: 1.0    steps: 56  evaluation reward: 242.15\n",
      "episode: 2064   score: 325.0  epsilon: 1.0    steps: 112  evaluation reward: 243.3\n",
      "episode: 2065   score: 260.0  epsilon: 1.0    steps: 280  evaluation reward: 244.3\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4819: Policy loss: -0.340211. Value loss: 0.303493. Entropy: 0.263564.\n",
      "Iteration 4820: Policy loss: -0.326296. Value loss: 0.135881. Entropy: 0.263990.\n",
      "Iteration 4821: Policy loss: -0.340266. Value loss: 0.070542. Entropy: 0.264361.\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4822: Policy loss: -0.204722. Value loss: 0.158060. Entropy: 0.308343.\n",
      "Iteration 4823: Policy loss: -0.206469. Value loss: 0.070781. Entropy: 0.306102.\n",
      "Iteration 4824: Policy loss: -0.220239. Value loss: 0.042958. Entropy: 0.307431.\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4825: Policy loss: -0.221859. Value loss: 0.236059. Entropy: 0.312339.\n",
      "Iteration 4826: Policy loss: -0.226845. Value loss: 0.112587. Entropy: 0.310460.\n",
      "Iteration 4827: Policy loss: -0.221663. Value loss: 0.072419. Entropy: 0.311502.\n",
      "episode: 2066   score: 390.0  epsilon: 1.0    steps: 200  evaluation reward: 247.65\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4828: Policy loss: -0.136019. Value loss: 0.088281. Entropy: 0.289127.\n",
      "Iteration 4829: Policy loss: -0.141497. Value loss: 0.037286. Entropy: 0.290558.\n",
      "Iteration 4830: Policy loss: -0.150083. Value loss: 0.026514. Entropy: 0.290740.\n",
      "episode: 2067   score: 490.0  epsilon: 1.0    steps: 360  evaluation reward: 250.3\n",
      "episode: 2068   score: 335.0  epsilon: 1.0    steps: 824  evaluation reward: 252.85\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4831: Policy loss: -0.011548. Value loss: 0.098138. Entropy: 0.286372.\n",
      "Iteration 4832: Policy loss: -0.021495. Value loss: 0.051330. Entropy: 0.285578.\n",
      "Iteration 4833: Policy loss: -0.030725. Value loss: 0.034728. Entropy: 0.285925.\n",
      "episode: 2069   score: 535.0  epsilon: 1.0    steps: 8  evaluation reward: 257.1\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4834: Policy loss: -0.079324. Value loss: 0.144832. Entropy: 0.287125.\n",
      "Iteration 4835: Policy loss: -0.091767. Value loss: 0.062570. Entropy: 0.288906.\n",
      "Iteration 4836: Policy loss: -0.090411. Value loss: 0.046023. Entropy: 0.288394.\n",
      "episode: 2070   score: 245.0  epsilon: 1.0    steps: 496  evaluation reward: 258.0\n",
      "episode: 2071   score: 670.0  epsilon: 1.0    steps: 560  evaluation reward: 260.25\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4837: Policy loss: 0.045304. Value loss: 0.262001. Entropy: 0.278381.\n",
      "Iteration 4838: Policy loss: 0.024712. Value loss: 0.103243. Entropy: 0.277865.\n",
      "Iteration 4839: Policy loss: 0.011984. Value loss: 0.070543. Entropy: 0.277672.\n",
      "episode: 2072   score: 285.0  epsilon: 1.0    steps: 552  evaluation reward: 261.0\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4840: Policy loss: -0.144961. Value loss: 0.081557. Entropy: 0.293644.\n",
      "Iteration 4841: Policy loss: -0.149275. Value loss: 0.037131. Entropy: 0.293193.\n",
      "Iteration 4842: Policy loss: -0.162180. Value loss: 0.031000. Entropy: 0.294123.\n",
      "episode: 2073   score: 335.0  epsilon: 1.0    steps: 736  evaluation reward: 262.55\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4843: Policy loss: 0.207549. Value loss: 0.083527. Entropy: 0.296079.\n",
      "Iteration 4844: Policy loss: 0.196140. Value loss: 0.035148. Entropy: 0.298305.\n",
      "Iteration 4845: Policy loss: 0.193402. Value loss: 0.025011. Entropy: 0.298073.\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4846: Policy loss: 0.081785. Value loss: 0.089859. Entropy: 0.305032.\n",
      "Iteration 4847: Policy loss: 0.069892. Value loss: 0.040372. Entropy: 0.304323.\n",
      "Iteration 4848: Policy loss: 0.073173. Value loss: 0.029238. Entropy: 0.305255.\n",
      "episode: 2074   score: 465.0  epsilon: 1.0    steps: 144  evaluation reward: 263.1\n",
      "Training network. lr: 0.000213. clip: 0.085264\n",
      "Iteration 4849: Policy loss: 0.253210. Value loss: 0.150518. Entropy: 0.287925.\n",
      "Iteration 4850: Policy loss: 0.241965. Value loss: 0.051295. Entropy: 0.288477.\n",
      "Iteration 4851: Policy loss: 0.245579. Value loss: 0.033882. Entropy: 0.288614.\n",
      "episode: 2075   score: 240.0  epsilon: 1.0    steps: 344  evaluation reward: 263.1\n",
      "episode: 2076   score: 215.0  epsilon: 1.0    steps: 464  evaluation reward: 263.0\n",
      "episode: 2077   score: 260.0  epsilon: 1.0    steps: 1016  evaluation reward: 264.05\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4852: Policy loss: -0.041867. Value loss: 0.056413. Entropy: 0.275482.\n",
      "Iteration 4853: Policy loss: -0.041974. Value loss: 0.033397. Entropy: 0.276374.\n",
      "Iteration 4854: Policy loss: -0.043741. Value loss: 0.027842. Entropy: 0.276079.\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4855: Policy loss: 0.151782. Value loss: 0.098941. Entropy: 0.289315.\n",
      "Iteration 4856: Policy loss: 0.150614. Value loss: 0.033927. Entropy: 0.288379.\n",
      "Iteration 4857: Policy loss: 0.144450. Value loss: 0.022710. Entropy: 0.289225.\n",
      "episode: 2078   score: 345.0  epsilon: 1.0    steps: 944  evaluation reward: 266.9\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4858: Policy loss: -0.040568. Value loss: 0.068725. Entropy: 0.304937.\n",
      "Iteration 4859: Policy loss: -0.038663. Value loss: 0.032659. Entropy: 0.303962.\n",
      "Iteration 4860: Policy loss: -0.039080. Value loss: 0.025094. Entropy: 0.305706.\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4861: Policy loss: 0.172911. Value loss: 0.125864. Entropy: 0.291025.\n",
      "Iteration 4862: Policy loss: 0.158643. Value loss: 0.052203. Entropy: 0.291604.\n",
      "Iteration 4863: Policy loss: 0.156750. Value loss: 0.034892. Entropy: 0.289853.\n",
      "episode: 2079   score: 240.0  epsilon: 1.0    steps: 328  evaluation reward: 268.45\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4864: Policy loss: 0.205929. Value loss: 0.086656. Entropy: 0.290108.\n",
      "Iteration 4865: Policy loss: 0.202517. Value loss: 0.038617. Entropy: 0.289846.\n",
      "Iteration 4866: Policy loss: 0.194657. Value loss: 0.025274. Entropy: 0.290193.\n",
      "episode: 2080   score: 260.0  epsilon: 1.0    steps: 80  evaluation reward: 269.85\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4867: Policy loss: -0.038090. Value loss: 0.097725. Entropy: 0.290858.\n",
      "Iteration 4868: Policy loss: -0.046511. Value loss: 0.040876. Entropy: 0.290598.\n",
      "Iteration 4869: Policy loss: -0.041581. Value loss: 0.032843. Entropy: 0.289887.\n",
      "episode: 2081   score: 290.0  epsilon: 1.0    steps: 416  evaluation reward: 270.8\n",
      "episode: 2082   score: 240.0  epsilon: 1.0    steps: 992  evaluation reward: 268.5\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4870: Policy loss: -0.199132. Value loss: 0.135994. Entropy: 0.292680.\n",
      "Iteration 4871: Policy loss: -0.205417. Value loss: 0.053737. Entropy: 0.294015.\n",
      "Iteration 4872: Policy loss: -0.202472. Value loss: 0.037511. Entropy: 0.292886.\n",
      "episode: 2083   score: 210.0  epsilon: 1.0    steps: 272  evaluation reward: 267.95\n",
      "episode: 2084   score: 545.0  epsilon: 1.0    steps: 944  evaluation reward: 272.8\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4873: Policy loss: 0.060682. Value loss: 0.102863. Entropy: 0.275222.\n",
      "Iteration 4874: Policy loss: 0.056062. Value loss: 0.047790. Entropy: 0.273746.\n",
      "Iteration 4875: Policy loss: 0.044121. Value loss: 0.033076. Entropy: 0.274400.\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4876: Policy loss: -0.086273. Value loss: 0.049620. Entropy: 0.296423.\n",
      "Iteration 4877: Policy loss: -0.089189. Value loss: 0.028756. Entropy: 0.294733.\n",
      "Iteration 4878: Policy loss: -0.092875. Value loss: 0.021196. Entropy: 0.295605.\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4879: Policy loss: -0.054219. Value loss: 0.088418. Entropy: 0.309280.\n",
      "Iteration 4880: Policy loss: -0.051571. Value loss: 0.030740. Entropy: 0.306588.\n",
      "Iteration 4881: Policy loss: -0.055299. Value loss: 0.023321. Entropy: 0.307556.\n",
      "episode: 2085   score: 365.0  epsilon: 1.0    steps: 616  evaluation reward: 273.85\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4882: Policy loss: -0.180252. Value loss: 0.320992. Entropy: 0.291926.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4883: Policy loss: -0.215748. Value loss: 0.118512. Entropy: 0.292883.\n",
      "Iteration 4884: Policy loss: -0.213055. Value loss: 0.063418. Entropy: 0.290383.\n",
      "episode: 2086   score: 195.0  epsilon: 1.0    steps: 24  evaluation reward: 273.7\n",
      "episode: 2087   score: 445.0  epsilon: 1.0    steps: 424  evaluation reward: 274.05\n",
      "episode: 2088   score: 210.0  epsilon: 1.0    steps: 608  evaluation reward: 273.45\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4885: Policy loss: 0.100058. Value loss: 0.062946. Entropy: 0.263910.\n",
      "Iteration 4886: Policy loss: 0.094480. Value loss: 0.026507. Entropy: 0.264721.\n",
      "Iteration 4887: Policy loss: 0.086981. Value loss: 0.021334. Entropy: 0.265140.\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4888: Policy loss: -0.059209. Value loss: 0.067743. Entropy: 0.305193.\n",
      "Iteration 4889: Policy loss: -0.062223. Value loss: 0.035631. Entropy: 0.306241.\n",
      "Iteration 4890: Policy loss: -0.063426. Value loss: 0.023865. Entropy: 0.305260.\n",
      "episode: 2089   score: 270.0  epsilon: 1.0    steps: 928  evaluation reward: 274.35\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4891: Policy loss: -0.039133. Value loss: 0.091686. Entropy: 0.304809.\n",
      "Iteration 4892: Policy loss: -0.034564. Value loss: 0.039941. Entropy: 0.304491.\n",
      "Iteration 4893: Policy loss: -0.041485. Value loss: 0.030378. Entropy: 0.304926.\n",
      "episode: 2090   score: 240.0  epsilon: 1.0    steps: 656  evaluation reward: 275.4\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4894: Policy loss: 0.058603. Value loss: 0.078323. Entropy: 0.283289.\n",
      "Iteration 4895: Policy loss: 0.046025. Value loss: 0.033675. Entropy: 0.280965.\n",
      "Iteration 4896: Policy loss: 0.046632. Value loss: 0.023551. Entropy: 0.281551.\n",
      "episode: 2091   score: 300.0  epsilon: 1.0    steps: 872  evaluation reward: 275.7\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4897: Policy loss: 0.137983. Value loss: 0.124159. Entropy: 0.298831.\n",
      "Iteration 4898: Policy loss: 0.120976. Value loss: 0.058116. Entropy: 0.299815.\n",
      "Iteration 4899: Policy loss: 0.116585. Value loss: 0.038958. Entropy: 0.298037.\n",
      "episode: 2092   score: 210.0  epsilon: 1.0    steps: 376  evaluation reward: 276.45\n",
      "Training network. lr: 0.000213. clip: 0.085107\n",
      "Iteration 4900: Policy loss: -0.105996. Value loss: 0.072203. Entropy: 0.287304.\n",
      "Iteration 4901: Policy loss: -0.109279. Value loss: 0.037391. Entropy: 0.287492.\n",
      "Iteration 4902: Policy loss: -0.100675. Value loss: 0.025458. Entropy: 0.285422.\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4903: Policy loss: -0.073928. Value loss: 0.082819. Entropy: 0.302673.\n",
      "Iteration 4904: Policy loss: -0.072332. Value loss: 0.039078. Entropy: 0.302266.\n",
      "Iteration 4905: Policy loss: -0.083396. Value loss: 0.026858. Entropy: 0.301028.\n",
      "episode: 2093   score: 275.0  epsilon: 1.0    steps: 512  evaluation reward: 276.8\n",
      "episode: 2094   score: 335.0  epsilon: 1.0    steps: 536  evaluation reward: 277.25\n",
      "episode: 2095   score: 620.0  epsilon: 1.0    steps: 560  evaluation reward: 279.35\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4906: Policy loss: 0.048657. Value loss: 0.076688. Entropy: 0.270751.\n",
      "Iteration 4907: Policy loss: 0.046361. Value loss: 0.035230. Entropy: 0.269906.\n",
      "Iteration 4908: Policy loss: 0.039516. Value loss: 0.025945. Entropy: 0.267825.\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4909: Policy loss: -0.435737. Value loss: 0.227048. Entropy: 0.303387.\n",
      "Iteration 4910: Policy loss: -0.450536. Value loss: 0.077349. Entropy: 0.301536.\n",
      "Iteration 4911: Policy loss: -0.467332. Value loss: 0.054565. Entropy: 0.302707.\n",
      "episode: 2096   score: 240.0  epsilon: 1.0    steps: 600  evaluation reward: 281.4\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4912: Policy loss: 0.097853. Value loss: 0.089411. Entropy: 0.295398.\n",
      "Iteration 4913: Policy loss: 0.092038. Value loss: 0.039671. Entropy: 0.294713.\n",
      "Iteration 4914: Policy loss: 0.079195. Value loss: 0.024854. Entropy: 0.294930.\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4915: Policy loss: -0.603449. Value loss: 0.423551. Entropy: 0.302249.\n",
      "Iteration 4916: Policy loss: -0.622095. Value loss: 0.128169. Entropy: 0.304671.\n",
      "Iteration 4917: Policy loss: -0.626230. Value loss: 0.063873. Entropy: 0.302954.\n",
      "episode: 2097   score: 225.0  epsilon: 1.0    steps: 896  evaluation reward: 282.1\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4918: Policy loss: -0.041605. Value loss: 0.165448. Entropy: 0.301364.\n",
      "Iteration 4919: Policy loss: -0.049936. Value loss: 0.077729. Entropy: 0.300809.\n",
      "Iteration 4920: Policy loss: -0.052700. Value loss: 0.050535. Entropy: 0.301377.\n",
      "episode: 2098   score: 565.0  epsilon: 1.0    steps: 400  evaluation reward: 286.65\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4921: Policy loss: 0.332668. Value loss: 0.147534. Entropy: 0.285216.\n",
      "Iteration 4922: Policy loss: 0.322003. Value loss: 0.045713. Entropy: 0.282736.\n",
      "Iteration 4923: Policy loss: 0.319921. Value loss: 0.026190. Entropy: 0.283857.\n",
      "episode: 2099   score: 550.0  epsilon: 1.0    steps: 264  evaluation reward: 290.05\n",
      "episode: 2100   score: 310.0  epsilon: 1.0    steps: 792  evaluation reward: 290.25\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4924: Policy loss: 0.217984. Value loss: 0.093790. Entropy: 0.283476.\n",
      "Iteration 4925: Policy loss: 0.206311. Value loss: 0.038127. Entropy: 0.282543.\n",
      "Iteration 4926: Policy loss: 0.202179. Value loss: 0.029928. Entropy: 0.282015.\n",
      "now time :  2019-09-05 19:21:01.968398\n",
      "episode: 2101   score: 415.0  epsilon: 1.0    steps: 24  evaluation reward: 291.4\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4927: Policy loss: 0.094870. Value loss: 0.063997. Entropy: 0.288715.\n",
      "Iteration 4928: Policy loss: 0.093924. Value loss: 0.032865. Entropy: 0.289884.\n",
      "Iteration 4929: Policy loss: 0.092765. Value loss: 0.024858. Entropy: 0.288878.\n",
      "episode: 2102   score: 240.0  epsilon: 1.0    steps: 56  evaluation reward: 292.75\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4930: Policy loss: -0.206235. Value loss: 0.213375. Entropy: 0.297063.\n",
      "Iteration 4931: Policy loss: -0.228007. Value loss: 0.086570. Entropy: 0.296516.\n",
      "Iteration 4932: Policy loss: -0.243301. Value loss: 0.062450. Entropy: 0.295397.\n",
      "episode: 2103   score: 490.0  epsilon: 1.0    steps: 616  evaluation reward: 294.45\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4933: Policy loss: 0.042753. Value loss: 0.173723. Entropy: 0.291574.\n",
      "Iteration 4934: Policy loss: 0.024929. Value loss: 0.058048. Entropy: 0.288983.\n",
      "Iteration 4935: Policy loss: 0.016235. Value loss: 0.034994. Entropy: 0.288624.\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4936: Policy loss: 0.245096. Value loss: 0.187379. Entropy: 0.305380.\n",
      "Iteration 4937: Policy loss: 0.235495. Value loss: 0.061529. Entropy: 0.303982.\n",
      "Iteration 4938: Policy loss: 0.229622. Value loss: 0.034273. Entropy: 0.304582.\n",
      "episode: 2104   score: 335.0  epsilon: 1.0    steps: 88  evaluation reward: 296.7\n",
      "episode: 2105   score: 245.0  epsilon: 1.0    steps: 880  evaluation reward: 297.05\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4939: Policy loss: -0.314008. Value loss: 0.223800. Entropy: 0.284892.\n",
      "Iteration 4940: Policy loss: -0.310174. Value loss: 0.110579. Entropy: 0.283744.\n",
      "Iteration 4941: Policy loss: -0.332747. Value loss: 0.083091. Entropy: 0.286712.\n",
      "episode: 2106   score: 210.0  epsilon: 1.0    steps: 168  evaluation reward: 297.8\n",
      "episode: 2107   score: 515.0  epsilon: 1.0    steps: 632  evaluation reward: 300.15\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4942: Policy loss: 0.079280. Value loss: 0.128721. Entropy: 0.268860.\n",
      "Iteration 4943: Policy loss: 0.066850. Value loss: 0.044585. Entropy: 0.270176.\n",
      "Iteration 4944: Policy loss: 0.063996. Value loss: 0.031848. Entropy: 0.268415.\n",
      "episode: 2108   score: 435.0  epsilon: 1.0    steps: 24  evaluation reward: 302.4\n",
      "episode: 2109   score: 155.0  epsilon: 1.0    steps: 112  evaluation reward: 301.35\n",
      "episode: 2110   score: 490.0  epsilon: 1.0    steps: 904  evaluation reward: 305.15\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4945: Policy loss: 0.396227. Value loss: 0.215204. Entropy: 0.271951.\n",
      "Iteration 4946: Policy loss: 0.377701. Value loss: 0.056373. Entropy: 0.268533.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4947: Policy loss: 0.361089. Value loss: 0.041319. Entropy: 0.269798.\n",
      "Training network. lr: 0.000212. clip: 0.084950\n",
      "Iteration 4948: Policy loss: 0.050818. Value loss: 0.117728. Entropy: 0.298395.\n",
      "Iteration 4949: Policy loss: 0.044045. Value loss: 0.056505. Entropy: 0.298367.\n",
      "Iteration 4950: Policy loss: 0.043624. Value loss: 0.047598. Entropy: 0.299422.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4951: Policy loss: 0.018311. Value loss: 0.144309. Entropy: 0.307534.\n",
      "Iteration 4952: Policy loss: 0.009424. Value loss: 0.059913. Entropy: 0.307761.\n",
      "Iteration 4953: Policy loss: -0.002565. Value loss: 0.047691. Entropy: 0.307936.\n",
      "episode: 2111   score: 210.0  epsilon: 1.0    steps: 672  evaluation reward: 304.4\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4954: Policy loss: 0.243635. Value loss: 0.090055. Entropy: 0.294173.\n",
      "Iteration 4955: Policy loss: 0.232115. Value loss: 0.032055. Entropy: 0.293531.\n",
      "Iteration 4956: Policy loss: 0.230352. Value loss: 0.020291. Entropy: 0.293366.\n",
      "episode: 2112   score: 260.0  epsilon: 1.0    steps: 528  evaluation reward: 305.9\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4957: Policy loss: 0.142964. Value loss: 0.058728. Entropy: 0.292113.\n",
      "Iteration 4958: Policy loss: 0.136604. Value loss: 0.024407. Entropy: 0.291226.\n",
      "Iteration 4959: Policy loss: 0.130706. Value loss: 0.016987. Entropy: 0.290757.\n",
      "episode: 2113   score: 210.0  epsilon: 1.0    steps: 424  evaluation reward: 305.75\n",
      "episode: 2114   score: 240.0  epsilon: 1.0    steps: 1008  evaluation reward: 306.05\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4960: Policy loss: -0.032610. Value loss: 0.100779. Entropy: 0.291886.\n",
      "Iteration 4961: Policy loss: -0.037792. Value loss: 0.043758. Entropy: 0.292669.\n",
      "Iteration 4962: Policy loss: -0.038672. Value loss: 0.029279. Entropy: 0.292151.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4963: Policy loss: -0.347121. Value loss: 0.318924. Entropy: 0.293604.\n",
      "Iteration 4964: Policy loss: -0.351799. Value loss: 0.132976. Entropy: 0.291806.\n",
      "Iteration 4965: Policy loss: -0.386067. Value loss: 0.081707. Entropy: 0.293081.\n",
      "episode: 2115   score: 210.0  epsilon: 1.0    steps: 176  evaluation reward: 304.85\n",
      "episode: 2116   score: 265.0  epsilon: 1.0    steps: 824  evaluation reward: 306.95\n",
      "episode: 2117   score: 575.0  epsilon: 1.0    steps: 848  evaluation reward: 310.6\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4966: Policy loss: 0.156007. Value loss: 0.104281. Entropy: 0.280974.\n",
      "Iteration 4967: Policy loss: 0.149839. Value loss: 0.050059. Entropy: 0.279324.\n",
      "Iteration 4968: Policy loss: 0.145512. Value loss: 0.036458. Entropy: 0.279982.\n",
      "episode: 2118   score: 370.0  epsilon: 1.0    steps: 176  evaluation reward: 311.4\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4969: Policy loss: -0.052736. Value loss: 0.064336. Entropy: 0.280740.\n",
      "Iteration 4970: Policy loss: -0.052793. Value loss: 0.032744. Entropy: 0.280112.\n",
      "Iteration 4971: Policy loss: -0.055638. Value loss: 0.025069. Entropy: 0.277486.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4972: Policy loss: -0.136403. Value loss: 0.239027. Entropy: 0.308489.\n",
      "Iteration 4973: Policy loss: -0.150829. Value loss: 0.084416. Entropy: 0.307623.\n",
      "Iteration 4974: Policy loss: -0.162630. Value loss: 0.048256. Entropy: 0.308288.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4975: Policy loss: -0.093897. Value loss: 0.074062. Entropy: 0.308763.\n",
      "Iteration 4976: Policy loss: -0.099459. Value loss: 0.035972. Entropy: 0.308875.\n",
      "Iteration 4977: Policy loss: -0.098819. Value loss: 0.029308. Entropy: 0.309435.\n",
      "episode: 2119   score: 345.0  epsilon: 1.0    steps: 408  evaluation reward: 311.75\n",
      "episode: 2120   score: 290.0  epsilon: 1.0    steps: 800  evaluation reward: 311.8\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4978: Policy loss: 0.312879. Value loss: 0.157884. Entropy: 0.284793.\n",
      "Iteration 4979: Policy loss: 0.298102. Value loss: 0.052157. Entropy: 0.282356.\n",
      "Iteration 4980: Policy loss: 0.295970. Value loss: 0.034637. Entropy: 0.283104.\n",
      "episode: 2121   score: 290.0  epsilon: 1.0    steps: 1000  evaluation reward: 311.4\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4981: Policy loss: 0.033626. Value loss: 0.091612. Entropy: 0.300639.\n",
      "Iteration 4982: Policy loss: 0.034837. Value loss: 0.042262. Entropy: 0.300199.\n",
      "Iteration 4983: Policy loss: 0.031044. Value loss: 0.035014. Entropy: 0.299008.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4984: Policy loss: -0.302015. Value loss: 0.279893. Entropy: 0.290365.\n",
      "Iteration 4985: Policy loss: -0.321220. Value loss: 0.136058. Entropy: 0.291704.\n",
      "Iteration 4986: Policy loss: -0.338061. Value loss: 0.064097. Entropy: 0.290898.\n",
      "episode: 2122   score: 415.0  epsilon: 1.0    steps: 440  evaluation reward: 312.15\n",
      "episode: 2123   score: 565.0  epsilon: 1.0    steps: 480  evaluation reward: 315.7\n",
      "episode: 2124   score: 225.0  epsilon: 1.0    steps: 824  evaluation reward: 315.85\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4987: Policy loss: 0.002489. Value loss: 0.106625. Entropy: 0.273936.\n",
      "Iteration 4988: Policy loss: -0.009321. Value loss: 0.041129. Entropy: 0.273287.\n",
      "Iteration 4989: Policy loss: -0.018605. Value loss: 0.030046. Entropy: 0.272749.\n",
      "episode: 2125   score: 300.0  epsilon: 1.0    steps: 56  evaluation reward: 316.75\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4990: Policy loss: -0.170615. Value loss: 0.149210. Entropy: 0.281928.\n",
      "Iteration 4991: Policy loss: -0.165456. Value loss: 0.057560. Entropy: 0.279118.\n",
      "Iteration 4992: Policy loss: -0.169529. Value loss: 0.040368. Entropy: 0.284139.\n",
      "episode: 2126   score: 315.0  epsilon: 1.0    steps: 176  evaluation reward: 317.2\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4993: Policy loss: 0.006266. Value loss: 0.105860. Entropy: 0.296616.\n",
      "Iteration 4994: Policy loss: -0.007931. Value loss: 0.042950. Entropy: 0.294853.\n",
      "Iteration 4995: Policy loss: -0.006515. Value loss: 0.026185. Entropy: 0.295441.\n",
      "episode: 2127   score: 225.0  epsilon: 1.0    steps: 640  evaluation reward: 311.25\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4996: Policy loss: -0.310983. Value loss: 0.212513. Entropy: 0.294735.\n",
      "Iteration 4997: Policy loss: -0.311519. Value loss: 0.062410. Entropy: 0.294747.\n",
      "Iteration 4998: Policy loss: -0.320551. Value loss: 0.036423. Entropy: 0.294224.\n",
      "Training network. lr: 0.000212. clip: 0.084803\n",
      "Iteration 4999: Policy loss: 0.268392. Value loss: 0.164334. Entropy: 0.300866.\n",
      "Iteration 5000: Policy loss: 0.258559. Value loss: 0.061992. Entropy: 0.302788.\n",
      "Iteration 5001: Policy loss: 0.256691. Value loss: 0.040085. Entropy: 0.301066.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5002: Policy loss: -0.122073. Value loss: 0.106976. Entropy: 0.307380.\n",
      "Iteration 5003: Policy loss: -0.117394. Value loss: 0.058224. Entropy: 0.305881.\n",
      "Iteration 5004: Policy loss: -0.130163. Value loss: 0.039368. Entropy: 0.305624.\n",
      "episode: 2128   score: 345.0  epsilon: 1.0    steps: 952  evaluation reward: 310.0\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5005: Policy loss: -0.120479. Value loss: 0.099777. Entropy: 0.302079.\n",
      "Iteration 5006: Policy loss: -0.123294. Value loss: 0.040022. Entropy: 0.301449.\n",
      "Iteration 5007: Policy loss: -0.135611. Value loss: 0.029724. Entropy: 0.301247.\n",
      "episode: 2129   score: 465.0  epsilon: 1.0    steps: 584  evaluation reward: 312.55\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5008: Policy loss: 0.437013. Value loss: 0.276867. Entropy: 0.280872.\n",
      "Iteration 5009: Policy loss: 0.408571. Value loss: 0.080959. Entropy: 0.281637.\n",
      "Iteration 5010: Policy loss: 0.387757. Value loss: 0.045399. Entropy: 0.280155.\n",
      "episode: 2130   score: 620.0  epsilon: 1.0    steps: 768  evaluation reward: 316.95\n",
      "episode: 2131   score: 290.0  epsilon: 1.0    steps: 856  evaluation reward: 315.35\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5011: Policy loss: 0.277736. Value loss: 0.193305. Entropy: 0.281990.\n",
      "Iteration 5012: Policy loss: 0.272579. Value loss: 0.103020. Entropy: 0.277878.\n",
      "Iteration 5013: Policy loss: 0.246680. Value loss: 0.082931. Entropy: 0.282324.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5014: Policy loss: 0.002714. Value loss: 0.128883. Entropy: 0.292077.\n",
      "Iteration 5015: Policy loss: 0.000146. Value loss: 0.048709. Entropy: 0.293634.\n",
      "Iteration 5016: Policy loss: -0.009912. Value loss: 0.035822. Entropy: 0.290839.\n",
      "episode: 2132   score: 390.0  epsilon: 1.0    steps: 600  evaluation reward: 315.95\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5017: Policy loss: -0.049629. Value loss: 0.157047. Entropy: 0.292627.\n",
      "Iteration 5018: Policy loss: -0.052832. Value loss: 0.070007. Entropy: 0.291554.\n",
      "Iteration 5019: Policy loss: -0.058316. Value loss: 0.052322. Entropy: 0.290108.\n",
      "episode: 2133   score: 120.0  epsilon: 1.0    steps: 544  evaluation reward: 313.5\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5020: Policy loss: 0.077881. Value loss: 0.110108. Entropy: 0.294941.\n",
      "Iteration 5021: Policy loss: 0.065882. Value loss: 0.054650. Entropy: 0.294822.\n",
      "Iteration 5022: Policy loss: 0.065323. Value loss: 0.037218. Entropy: 0.293871.\n",
      "episode: 2134   score: 500.0  epsilon: 1.0    steps: 752  evaluation reward: 314.1\n",
      "episode: 2135   score: 700.0  epsilon: 1.0    steps: 872  evaluation reward: 320.15\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5023: Policy loss: 0.188901. Value loss: 0.134712. Entropy: 0.287461.\n",
      "Iteration 5024: Policy loss: 0.180886. Value loss: 0.055632. Entropy: 0.288585.\n",
      "Iteration 5025: Policy loss: 0.182059. Value loss: 0.040080. Entropy: 0.288208.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5026: Policy loss: 0.009162. Value loss: 0.121346. Entropy: 0.291132.\n",
      "Iteration 5027: Policy loss: 0.008167. Value loss: 0.057547. Entropy: 0.291124.\n",
      "Iteration 5028: Policy loss: -0.006631. Value loss: 0.037009. Entropy: 0.291383.\n",
      "episode: 2136   score: 335.0  epsilon: 1.0    steps: 760  evaluation reward: 321.7\n",
      "episode: 2137   score: 215.0  epsilon: 1.0    steps: 1000  evaluation reward: 319.5\n",
      "episode: 2138   score: 450.0  epsilon: 1.0    steps: 1024  evaluation reward: 321.6\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5029: Policy loss: 0.410792. Value loss: 0.160449. Entropy: 0.294174.\n",
      "Iteration 5030: Policy loss: 0.409250. Value loss: 0.071077. Entropy: 0.292559.\n",
      "Iteration 5031: Policy loss: 0.396691. Value loss: 0.057521. Entropy: 0.293117.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5032: Policy loss: 0.141494. Value loss: 0.074798. Entropy: 0.274887.\n",
      "Iteration 5033: Policy loss: 0.134053. Value loss: 0.043428. Entropy: 0.272417.\n",
      "Iteration 5034: Policy loss: 0.132366. Value loss: 0.033412. Entropy: 0.272577.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5035: Policy loss: -0.192130. Value loss: 0.087634. Entropy: 0.303555.\n",
      "Iteration 5036: Policy loss: -0.202290. Value loss: 0.041387. Entropy: 0.305154.\n",
      "Iteration 5037: Policy loss: -0.204107. Value loss: 0.032872. Entropy: 0.303768.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5038: Policy loss: -0.305712. Value loss: 0.316043. Entropy: 0.311238.\n",
      "Iteration 5039: Policy loss: -0.318739. Value loss: 0.129583. Entropy: 0.313285.\n",
      "Iteration 5040: Policy loss: -0.349112. Value loss: 0.063968. Entropy: 0.313926.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5041: Policy loss: 0.036124. Value loss: 0.404104. Entropy: 0.308161.\n",
      "Iteration 5042: Policy loss: 0.018866. Value loss: 0.206584. Entropy: 0.306526.\n",
      "Iteration 5043: Policy loss: 0.050884. Value loss: 0.122432. Entropy: 0.305564.\n",
      "episode: 2139   score: 350.0  epsilon: 1.0    steps: 320  evaluation reward: 322.7\n",
      "episode: 2140   score: 210.0  epsilon: 1.0    steps: 936  evaluation reward: 322.45\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5044: Policy loss: -0.028073. Value loss: 0.164274. Entropy: 0.289958.\n",
      "Iteration 5045: Policy loss: -0.043391. Value loss: 0.076253. Entropy: 0.287952.\n",
      "Iteration 5046: Policy loss: -0.034748. Value loss: 0.045626. Entropy: 0.288820.\n",
      "episode: 2141   score: 450.0  epsilon: 1.0    steps: 208  evaluation reward: 324.35\n",
      "episode: 2142   score: 335.0  epsilon: 1.0    steps: 760  evaluation reward: 324.25\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5047: Policy loss: -0.083220. Value loss: 0.458351. Entropy: 0.265312.\n",
      "Iteration 5048: Policy loss: -0.092377. Value loss: 0.207245. Entropy: 0.262105.\n",
      "Iteration 5049: Policy loss: -0.119540. Value loss: 0.122712. Entropy: 0.263092.\n",
      "Training network. lr: 0.000212. clip: 0.084646\n",
      "Iteration 5050: Policy loss: 0.073902. Value loss: 0.178337. Entropy: 0.305757.\n",
      "Iteration 5051: Policy loss: 0.061918. Value loss: 0.089505. Entropy: 0.303431.\n",
      "Iteration 5052: Policy loss: 0.047315. Value loss: 0.064882. Entropy: 0.303487.\n",
      "episode: 2143   score: 665.0  epsilon: 1.0    steps: 16  evaluation reward: 328.8\n",
      "episode: 2144   score: 350.0  epsilon: 1.0    steps: 488  evaluation reward: 327.85\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5053: Policy loss: 0.052769. Value loss: 0.093963. Entropy: 0.281025.\n",
      "Iteration 5054: Policy loss: 0.045885. Value loss: 0.040768. Entropy: 0.279674.\n",
      "Iteration 5055: Policy loss: 0.033956. Value loss: 0.032662. Entropy: 0.280650.\n",
      "episode: 2145   score: 560.0  epsilon: 1.0    steps: 136  evaluation reward: 332.8\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5056: Policy loss: -0.088207. Value loss: 0.226756. Entropy: 0.289801.\n",
      "Iteration 5057: Policy loss: -0.107040. Value loss: 0.094268. Entropy: 0.290910.\n",
      "Iteration 5058: Policy loss: -0.096596. Value loss: 0.064675. Entropy: 0.288217.\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5059: Policy loss: -0.097491. Value loss: 0.085134. Entropy: 0.305227.\n",
      "Iteration 5060: Policy loss: -0.109346. Value loss: 0.030115. Entropy: 0.303233.\n",
      "Iteration 5061: Policy loss: -0.109244. Value loss: 0.018910. Entropy: 0.304253.\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5062: Policy loss: -0.072560. Value loss: 0.213406. Entropy: 0.307094.\n",
      "Iteration 5063: Policy loss: -0.091186. Value loss: 0.057513. Entropy: 0.305746.\n",
      "Iteration 5064: Policy loss: -0.098098. Value loss: 0.033943. Entropy: 0.305206.\n",
      "episode: 2146   score: 270.0  epsilon: 1.0    steps: 632  evaluation reward: 333.7\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5065: Policy loss: 0.130079. Value loss: 0.129987. Entropy: 0.297797.\n",
      "Iteration 5066: Policy loss: 0.116526. Value loss: 0.068612. Entropy: 0.298292.\n",
      "Iteration 5067: Policy loss: 0.117822. Value loss: 0.046195. Entropy: 0.298015.\n",
      "episode: 2147   score: 910.0  epsilon: 1.0    steps: 600  evaluation reward: 338.2\n",
      "episode: 2148   score: 455.0  epsilon: 1.0    steps: 920  evaluation reward: 340.5\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5068: Policy loss: 0.295368. Value loss: 0.126594. Entropy: 0.284207.\n",
      "Iteration 5069: Policy loss: 0.293101. Value loss: 0.056222. Entropy: 0.280980.\n",
      "Iteration 5070: Policy loss: 0.282808. Value loss: 0.039672. Entropy: 0.282011.\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5071: Policy loss: 0.336833. Value loss: 0.112864. Entropy: 0.292526.\n",
      "Iteration 5072: Policy loss: 0.327503. Value loss: 0.049581. Entropy: 0.290133.\n",
      "Iteration 5073: Policy loss: 0.323659. Value loss: 0.038890. Entropy: 0.290018.\n",
      "episode: 2149   score: 245.0  epsilon: 1.0    steps: 392  evaluation reward: 341.3\n",
      "episode: 2150   score: 210.0  epsilon: 1.0    steps: 968  evaluation reward: 341.3\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5074: Policy loss: 0.060997. Value loss: 0.112709. Entropy: 0.290812.\n",
      "Iteration 5075: Policy loss: 0.050038. Value loss: 0.047830. Entropy: 0.290177.\n",
      "Iteration 5076: Policy loss: 0.047746. Value loss: 0.034851. Entropy: 0.288567.\n",
      "now time :  2019-09-05 19:30:21.319871\n",
      "episode: 2151   score: 515.0  epsilon: 1.0    steps: 392  evaluation reward: 343.6\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5077: Policy loss: -0.132770. Value loss: 0.304448. Entropy: 0.276382.\n",
      "Iteration 5078: Policy loss: -0.158858. Value loss: 0.107356. Entropy: 0.274736.\n",
      "Iteration 5079: Policy loss: -0.162792. Value loss: 0.061027. Entropy: 0.273100.\n",
      "episode: 2152   score: 435.0  epsilon: 1.0    steps: 344  evaluation reward: 344.85\n",
      "episode: 2153   score: 280.0  epsilon: 1.0    steps: 360  evaluation reward: 345.2\n",
      "Training network. lr: 0.000211. clip: 0.084489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5080: Policy loss: -0.037553. Value loss: 0.076185. Entropy: 0.271365.\n",
      "Iteration 5081: Policy loss: -0.039732. Value loss: 0.048443. Entropy: 0.272780.\n",
      "Iteration 5082: Policy loss: -0.043433. Value loss: 0.039313. Entropy: 0.271748.\n",
      "episode: 2154   score: 210.0  epsilon: 1.0    steps: 968  evaluation reward: 344.0\n",
      "episode: 2155   score: 180.0  epsilon: 1.0    steps: 976  evaluation reward: 341.2\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5083: Policy loss: 0.080371. Value loss: 0.082303. Entropy: 0.294591.\n",
      "Iteration 5084: Policy loss: 0.080186. Value loss: 0.035444. Entropy: 0.292907.\n",
      "Iteration 5085: Policy loss: 0.072922. Value loss: 0.029236. Entropy: 0.294206.\n",
      "episode: 2156   score: 120.0  epsilon: 1.0    steps: 216  evaluation reward: 339.25\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5086: Policy loss: -0.075394. Value loss: 0.062924. Entropy: 0.265818.\n",
      "Iteration 5087: Policy loss: -0.081651. Value loss: 0.038916. Entropy: 0.265807.\n",
      "Iteration 5088: Policy loss: -0.078530. Value loss: 0.032922. Entropy: 0.263918.\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5089: Policy loss: -0.416323. Value loss: 0.204900. Entropy: 0.306156.\n",
      "Iteration 5090: Policy loss: -0.415514. Value loss: 0.060933. Entropy: 0.305230.\n",
      "Iteration 5091: Policy loss: -0.432692. Value loss: 0.049162. Entropy: 0.305341.\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5092: Policy loss: -0.226516. Value loss: 0.121112. Entropy: 0.307300.\n",
      "Iteration 5093: Policy loss: -0.229923. Value loss: 0.052076. Entropy: 0.305622.\n",
      "Iteration 5094: Policy loss: -0.242962. Value loss: 0.037878. Entropy: 0.305875.\n",
      "episode: 2157   score: 535.0  epsilon: 1.0    steps: 136  evaluation reward: 343.0\n",
      "episode: 2158   score: 225.0  epsilon: 1.0    steps: 760  evaluation reward: 342.35\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5095: Policy loss: 0.665765. Value loss: 0.261055. Entropy: 0.282735.\n",
      "Iteration 5096: Policy loss: 0.654262. Value loss: 0.078423. Entropy: 0.280635.\n",
      "Iteration 5097: Policy loss: 0.637017. Value loss: 0.046276. Entropy: 0.280583.\n",
      "episode: 2159   score: 320.0  epsilon: 1.0    steps: 912  evaluation reward: 344.0\n",
      "Training network. lr: 0.000211. clip: 0.084489\n",
      "Iteration 5098: Policy loss: 0.088484. Value loss: 0.115343. Entropy: 0.297402.\n",
      "Iteration 5099: Policy loss: 0.082180. Value loss: 0.062221. Entropy: 0.297076.\n",
      "Iteration 5100: Policy loss: 0.084755. Value loss: 0.048592. Entropy: 0.297079.\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5101: Policy loss: -0.002201. Value loss: 0.085063. Entropy: 0.296460.\n",
      "Iteration 5102: Policy loss: -0.011016. Value loss: 0.033800. Entropy: 0.296294.\n",
      "Iteration 5103: Policy loss: -0.011564. Value loss: 0.022753. Entropy: 0.294554.\n",
      "episode: 2160   score: 455.0  epsilon: 1.0    steps: 288  evaluation reward: 346.75\n",
      "episode: 2161   score: 240.0  epsilon: 1.0    steps: 840  evaluation reward: 346.9\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5104: Policy loss: 0.092169. Value loss: 0.075227. Entropy: 0.282823.\n",
      "Iteration 5105: Policy loss: 0.080172. Value loss: 0.031163. Entropy: 0.284651.\n",
      "Iteration 5106: Policy loss: 0.074033. Value loss: 0.022338. Entropy: 0.284521.\n",
      "episode: 2162   score: 480.0  epsilon: 1.0    steps: 184  evaluation reward: 349.1\n",
      "episode: 2163   score: 315.0  epsilon: 1.0    steps: 856  evaluation reward: 351.25\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5107: Policy loss: -0.209431. Value loss: 0.113273. Entropy: 0.278016.\n",
      "Iteration 5108: Policy loss: -0.229556. Value loss: 0.061092. Entropy: 0.277094.\n",
      "Iteration 5109: Policy loss: -0.235871. Value loss: 0.043865. Entropy: 0.276417.\n",
      "episode: 2164   score: 390.0  epsilon: 1.0    steps: 776  evaluation reward: 351.9\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5110: Policy loss: -0.077146. Value loss: 0.131784. Entropy: 0.290734.\n",
      "Iteration 5111: Policy loss: -0.079855. Value loss: 0.042127. Entropy: 0.289440.\n",
      "Iteration 5112: Policy loss: -0.084576. Value loss: 0.029126. Entropy: 0.287557.\n",
      "episode: 2165   score: 260.0  epsilon: 1.0    steps: 816  evaluation reward: 351.9\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5113: Policy loss: 0.165400. Value loss: 0.083795. Entropy: 0.294344.\n",
      "Iteration 5114: Policy loss: 0.152922. Value loss: 0.033378. Entropy: 0.295322.\n",
      "Iteration 5115: Policy loss: 0.152050. Value loss: 0.024168. Entropy: 0.292989.\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5116: Policy loss: -0.096719. Value loss: 0.389573. Entropy: 0.299536.\n",
      "Iteration 5117: Policy loss: -0.114835. Value loss: 0.159388. Entropy: 0.300400.\n",
      "Iteration 5118: Policy loss: -0.125272. Value loss: 0.066920. Entropy: 0.300901.\n",
      "episode: 2166   score: 300.0  epsilon: 1.0    steps: 520  evaluation reward: 351.0\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5119: Policy loss: -0.029834. Value loss: 0.117681. Entropy: 0.289862.\n",
      "Iteration 5120: Policy loss: -0.038881. Value loss: 0.050049. Entropy: 0.289402.\n",
      "Iteration 5121: Policy loss: -0.045441. Value loss: 0.037094. Entropy: 0.289737.\n",
      "episode: 2167   score: 350.0  epsilon: 1.0    steps: 984  evaluation reward: 349.6\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5122: Policy loss: 0.086639. Value loss: 0.103301. Entropy: 0.300634.\n",
      "Iteration 5123: Policy loss: 0.070353. Value loss: 0.044877. Entropy: 0.300581.\n",
      "Iteration 5124: Policy loss: 0.071980. Value loss: 0.030680. Entropy: 0.300849.\n",
      "episode: 2168   score: 255.0  epsilon: 1.0    steps: 456  evaluation reward: 348.8\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5125: Policy loss: 0.055293. Value loss: 0.079343. Entropy: 0.278723.\n",
      "Iteration 5126: Policy loss: 0.053316. Value loss: 0.040705. Entropy: 0.277537.\n",
      "Iteration 5127: Policy loss: 0.049921. Value loss: 0.028871. Entropy: 0.276161.\n",
      "episode: 2169   score: 500.0  epsilon: 1.0    steps: 688  evaluation reward: 348.45\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5128: Policy loss: -0.304172. Value loss: 0.444862. Entropy: 0.291005.\n",
      "Iteration 5129: Policy loss: -0.314127. Value loss: 0.125978. Entropy: 0.290766.\n",
      "Iteration 5130: Policy loss: -0.318960. Value loss: 0.067132. Entropy: 0.289999.\n",
      "episode: 2170   score: 335.0  epsilon: 1.0    steps: 656  evaluation reward: 349.35\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5131: Policy loss: -0.041265. Value loss: 0.108736. Entropy: 0.284758.\n",
      "Iteration 5132: Policy loss: -0.050665. Value loss: 0.046546. Entropy: 0.286565.\n",
      "Iteration 5133: Policy loss: -0.058888. Value loss: 0.034573. Entropy: 0.287305.\n",
      "episode: 2171   score: 230.0  epsilon: 1.0    steps: 240  evaluation reward: 344.95\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5134: Policy loss: 0.343055. Value loss: 0.223472. Entropy: 0.287947.\n",
      "Iteration 5135: Policy loss: 0.333172. Value loss: 0.080017. Entropy: 0.287867.\n",
      "Iteration 5136: Policy loss: 0.329146. Value loss: 0.049296. Entropy: 0.287481.\n",
      "episode: 2172   score: 645.0  epsilon: 1.0    steps: 560  evaluation reward: 348.55\n",
      "episode: 2173   score: 545.0  epsilon: 1.0    steps: 736  evaluation reward: 350.65\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5137: Policy loss: 0.014954. Value loss: 0.089080. Entropy: 0.276044.\n",
      "Iteration 5138: Policy loss: 0.012351. Value loss: 0.041586. Entropy: 0.277030.\n",
      "Iteration 5139: Policy loss: 0.010213. Value loss: 0.034724. Entropy: 0.276115.\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5140: Policy loss: -0.257902. Value loss: 0.200051. Entropy: 0.298329.\n",
      "Iteration 5141: Policy loss: -0.274889. Value loss: 0.087771. Entropy: 0.300676.\n",
      "Iteration 5142: Policy loss: -0.283190. Value loss: 0.068105. Entropy: 0.299846.\n",
      "episode: 2174   score: 535.0  epsilon: 1.0    steps: 40  evaluation reward: 351.35\n",
      "episode: 2175   score: 165.0  epsilon: 1.0    steps: 632  evaluation reward: 350.6\n",
      "episode: 2176   score: 210.0  epsilon: 1.0    steps: 792  evaluation reward: 350.55\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5143: Policy loss: 0.312403. Value loss: 0.123553. Entropy: 0.264207.\n",
      "Iteration 5144: Policy loss: 0.312819. Value loss: 0.063688. Entropy: 0.265234.\n",
      "Iteration 5145: Policy loss: 0.305622. Value loss: 0.034370. Entropy: 0.264299.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5146: Policy loss: 0.087739. Value loss: 0.187625. Entropy: 0.299016.\n",
      "Iteration 5147: Policy loss: 0.063715. Value loss: 0.047148. Entropy: 0.297836.\n",
      "Iteration 5148: Policy loss: 0.063899. Value loss: 0.033474. Entropy: 0.298443.\n",
      "Training network. lr: 0.000211. clip: 0.084342\n",
      "Iteration 5149: Policy loss: 0.030314. Value loss: 0.186495. Entropy: 0.306356.\n",
      "Iteration 5150: Policy loss: 0.020497. Value loss: 0.064580. Entropy: 0.305996.\n",
      "Iteration 5151: Policy loss: 0.006065. Value loss: 0.047922. Entropy: 0.307507.\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5152: Policy loss: 0.002924. Value loss: 0.078207. Entropy: 0.304531.\n",
      "Iteration 5153: Policy loss: -0.006818. Value loss: 0.035828. Entropy: 0.304867.\n",
      "Iteration 5154: Policy loss: -0.006101. Value loss: 0.021733. Entropy: 0.304866.\n",
      "episode: 2177   score: 335.0  epsilon: 1.0    steps: 688  evaluation reward: 351.3\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5155: Policy loss: -0.285221. Value loss: 0.188034. Entropy: 0.298720.\n",
      "Iteration 5156: Policy loss: -0.291793. Value loss: 0.089515. Entropy: 0.297323.\n",
      "Iteration 5157: Policy loss: -0.294387. Value loss: 0.070950. Entropy: 0.297678.\n",
      "episode: 2178   score: 270.0  epsilon: 1.0    steps: 184  evaluation reward: 350.55\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5158: Policy loss: -0.006436. Value loss: 0.055929. Entropy: 0.291792.\n",
      "Iteration 5159: Policy loss: -0.010668. Value loss: 0.021903. Entropy: 0.291989.\n",
      "Iteration 5160: Policy loss: -0.016685. Value loss: 0.014258. Entropy: 0.291214.\n",
      "episode: 2179   score: 575.0  epsilon: 1.0    steps: 864  evaluation reward: 353.9\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5161: Policy loss: -0.120118. Value loss: 0.199182. Entropy: 0.299729.\n",
      "Iteration 5162: Policy loss: -0.137915. Value loss: 0.117805. Entropy: 0.298241.\n",
      "Iteration 5163: Policy loss: -0.138838. Value loss: 0.077234. Entropy: 0.299594.\n",
      "episode: 2180   score: 535.0  epsilon: 1.0    steps: 24  evaluation reward: 356.65\n",
      "episode: 2181   score: 335.0  epsilon: 1.0    steps: 920  evaluation reward: 357.1\n",
      "episode: 2182   score: 435.0  epsilon: 1.0    steps: 1024  evaluation reward: 359.05\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5164: Policy loss: 0.085722. Value loss: 0.106530. Entropy: 0.276963.\n",
      "Iteration 5165: Policy loss: 0.077098. Value loss: 0.048135. Entropy: 0.278270.\n",
      "Iteration 5166: Policy loss: 0.077224. Value loss: 0.034574. Entropy: 0.276452.\n",
      "episode: 2183   score: 560.0  epsilon: 1.0    steps: 192  evaluation reward: 362.55\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5167: Policy loss: 0.226286. Value loss: 0.191581. Entropy: 0.268088.\n",
      "Iteration 5168: Policy loss: 0.232453. Value loss: 0.064301. Entropy: 0.268136.\n",
      "Iteration 5169: Policy loss: 0.222101. Value loss: 0.042977. Entropy: 0.265667.\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5170: Policy loss: 0.008135. Value loss: 0.076600. Entropy: 0.306578.\n",
      "Iteration 5171: Policy loss: -0.001669. Value loss: 0.034860. Entropy: 0.306719.\n",
      "Iteration 5172: Policy loss: -0.006409. Value loss: 0.024262. Entropy: 0.306607.\n",
      "episode: 2184   score: 210.0  epsilon: 1.0    steps: 544  evaluation reward: 359.2\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5173: Policy loss: 0.087619. Value loss: 0.116262. Entropy: 0.297682.\n",
      "Iteration 5174: Policy loss: 0.072782. Value loss: 0.055781. Entropy: 0.298274.\n",
      "Iteration 5175: Policy loss: 0.083388. Value loss: 0.043146. Entropy: 0.297450.\n",
      "episode: 2185   score: 620.0  epsilon: 1.0    steps: 592  evaluation reward: 361.75\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5176: Policy loss: 0.159479. Value loss: 0.107392. Entropy: 0.294257.\n",
      "Iteration 5177: Policy loss: 0.148142. Value loss: 0.034177. Entropy: 0.292842.\n",
      "Iteration 5178: Policy loss: 0.139281. Value loss: 0.026503. Entropy: 0.292716.\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5179: Policy loss: -0.027172. Value loss: 0.118985. Entropy: 0.306761.\n",
      "Iteration 5180: Policy loss: -0.042253. Value loss: 0.037549. Entropy: 0.307297.\n",
      "Iteration 5181: Policy loss: -0.039706. Value loss: 0.027936. Entropy: 0.306030.\n",
      "episode: 2186   score: 210.0  epsilon: 1.0    steps: 136  evaluation reward: 361.9\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5182: Policy loss: -0.302142. Value loss: 0.279730. Entropy: 0.286594.\n",
      "Iteration 5183: Policy loss: -0.314575. Value loss: 0.085672. Entropy: 0.286273.\n",
      "Iteration 5184: Policy loss: -0.322935. Value loss: 0.052125. Entropy: 0.283596.\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5185: Policy loss: 0.437426. Value loss: 0.263011. Entropy: 0.308034.\n",
      "Iteration 5186: Policy loss: 0.414343. Value loss: 0.077251. Entropy: 0.307929.\n",
      "Iteration 5187: Policy loss: 0.411986. Value loss: 0.050814. Entropy: 0.306159.\n",
      "episode: 2187   score: 345.0  epsilon: 1.0    steps: 40  evaluation reward: 360.9\n",
      "episode: 2188   score: 365.0  epsilon: 1.0    steps: 288  evaluation reward: 362.45\n",
      "episode: 2189   score: 345.0  epsilon: 1.0    steps: 344  evaluation reward: 363.2\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5188: Policy loss: -0.078342. Value loss: 0.078985. Entropy: 0.257398.\n",
      "Iteration 5189: Policy loss: -0.086050. Value loss: 0.036931. Entropy: 0.257898.\n",
      "Iteration 5190: Policy loss: -0.084341. Value loss: 0.028825. Entropy: 0.256979.\n",
      "episode: 2190   score: 390.0  epsilon: 1.0    steps: 520  evaluation reward: 364.7\n",
      "episode: 2191   score: 315.0  epsilon: 1.0    steps: 968  evaluation reward: 364.85\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5191: Policy loss: 0.409513. Value loss: 0.158135. Entropy: 0.285020.\n",
      "Iteration 5192: Policy loss: 0.384321. Value loss: 0.063056. Entropy: 0.283545.\n",
      "Iteration 5193: Policy loss: 0.383329. Value loss: 0.044096. Entropy: 0.284820.\n",
      "episode: 2192   score: 475.0  epsilon: 1.0    steps: 576  evaluation reward: 367.5\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5194: Policy loss: 0.248072. Value loss: 0.112379. Entropy: 0.272130.\n",
      "Iteration 5195: Policy loss: 0.245015. Value loss: 0.046587. Entropy: 0.267635.\n",
      "Iteration 5196: Policy loss: 0.239312. Value loss: 0.034397. Entropy: 0.270736.\n",
      "episode: 2193   score: 165.0  epsilon: 1.0    steps: 48  evaluation reward: 366.4\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5197: Policy loss: -0.101252. Value loss: 0.190356. Entropy: 0.287981.\n",
      "Iteration 5198: Policy loss: -0.109704. Value loss: 0.092397. Entropy: 0.286504.\n",
      "Iteration 5199: Policy loss: -0.107195. Value loss: 0.057513. Entropy: 0.286375.\n",
      "Training network. lr: 0.000210. clip: 0.084185\n",
      "Iteration 5200: Policy loss: -0.059336. Value loss: 0.095523. Entropy: 0.306390.\n",
      "Iteration 5201: Policy loss: -0.070573. Value loss: 0.049057. Entropy: 0.307306.\n",
      "Iteration 5202: Policy loss: -0.070627. Value loss: 0.040788. Entropy: 0.306426.\n",
      "episode: 2194   score: 365.0  epsilon: 1.0    steps: 248  evaluation reward: 366.7\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5203: Policy loss: -0.054954. Value loss: 0.234034. Entropy: 0.291611.\n",
      "Iteration 5204: Policy loss: -0.068503. Value loss: 0.093503. Entropy: 0.291217.\n",
      "Iteration 5205: Policy loss: -0.079176. Value loss: 0.068716. Entropy: 0.291508.\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5206: Policy loss: 0.044252. Value loss: 0.115685. Entropy: 0.307814.\n",
      "Iteration 5207: Policy loss: 0.029497. Value loss: 0.058541. Entropy: 0.307576.\n",
      "Iteration 5208: Policy loss: 0.027807. Value loss: 0.038906. Entropy: 0.307224.\n",
      "episode: 2195   score: 320.0  epsilon: 1.0    steps: 40  evaluation reward: 363.7\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5209: Policy loss: 0.087569. Value loss: 0.150010. Entropy: 0.292910.\n",
      "Iteration 5210: Policy loss: 0.081608. Value loss: 0.073641. Entropy: 0.292060.\n",
      "Iteration 5211: Policy loss: 0.077310. Value loss: 0.056045. Entropy: 0.293053.\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5212: Policy loss: 0.184636. Value loss: 0.140825. Entropy: 0.315238.\n",
      "Iteration 5213: Policy loss: 0.162417. Value loss: 0.054232. Entropy: 0.314083.\n",
      "Iteration 5214: Policy loss: 0.161277. Value loss: 0.037853. Entropy: 0.312996.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2196   score: 80.0  epsilon: 1.0    steps: 1000  evaluation reward: 362.1\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5215: Policy loss: 0.058912. Value loss: 0.083321. Entropy: 0.296942.\n",
      "Iteration 5216: Policy loss: 0.048133. Value loss: 0.044821. Entropy: 0.298115.\n",
      "Iteration 5217: Policy loss: 0.046686. Value loss: 0.031913. Entropy: 0.298194.\n",
      "episode: 2197   score: 590.0  epsilon: 1.0    steps: 168  evaluation reward: 365.75\n",
      "episode: 2198   score: 285.0  epsilon: 1.0    steps: 792  evaluation reward: 362.95\n",
      "episode: 2199   score: 590.0  epsilon: 1.0    steps: 808  evaluation reward: 363.35\n",
      "episode: 2200   score: 395.0  epsilon: 1.0    steps: 896  evaluation reward: 364.2\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5218: Policy loss: 0.173790. Value loss: 0.104979. Entropy: 0.262954.\n",
      "Iteration 5219: Policy loss: 0.170747. Value loss: 0.046778. Entropy: 0.261451.\n",
      "Iteration 5220: Policy loss: 0.159786. Value loss: 0.035327. Entropy: 0.261501.\n",
      "now time :  2019-09-05 19:39:17.108480\n",
      "episode: 2201   score: 445.0  epsilon: 1.0    steps: 288  evaluation reward: 364.5\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5221: Policy loss: 0.072576. Value loss: 0.132832. Entropy: 0.276539.\n",
      "Iteration 5222: Policy loss: 0.056545. Value loss: 0.072141. Entropy: 0.278524.\n",
      "Iteration 5223: Policy loss: 0.058986. Value loss: 0.050210. Entropy: 0.277468.\n",
      "episode: 2202   score: 435.0  epsilon: 1.0    steps: 600  evaluation reward: 366.45\n",
      "episode: 2203   score: 285.0  epsilon: 1.0    steps: 808  evaluation reward: 364.4\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5224: Policy loss: 0.001791. Value loss: 0.097225. Entropy: 0.284416.\n",
      "Iteration 5225: Policy loss: -0.004592. Value loss: 0.043359. Entropy: 0.286660.\n",
      "Iteration 5226: Policy loss: -0.005764. Value loss: 0.030946. Entropy: 0.286010.\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5227: Policy loss: -0.215121. Value loss: 0.163344. Entropy: 0.306067.\n",
      "Iteration 5228: Policy loss: -0.225547. Value loss: 0.060325. Entropy: 0.305921.\n",
      "Iteration 5229: Policy loss: -0.232882. Value loss: 0.051040. Entropy: 0.306020.\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5230: Policy loss: 0.065002. Value loss: 0.089182. Entropy: 0.306154.\n",
      "Iteration 5231: Policy loss: 0.059616. Value loss: 0.037161. Entropy: 0.305259.\n",
      "Iteration 5232: Policy loss: 0.054168. Value loss: 0.025641. Entropy: 0.306720.\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5233: Policy loss: -0.040661. Value loss: 0.317527. Entropy: 0.306265.\n",
      "Iteration 5234: Policy loss: -0.044513. Value loss: 0.241898. Entropy: 0.305325.\n",
      "Iteration 5235: Policy loss: -0.062315. Value loss: 0.234724. Entropy: 0.305623.\n",
      "episode: 2204   score: 485.0  epsilon: 1.0    steps: 912  evaluation reward: 365.9\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5236: Policy loss: 0.022925. Value loss: 0.133402. Entropy: 0.302638.\n",
      "Iteration 5237: Policy loss: 0.023136. Value loss: 0.057442. Entropy: 0.303270.\n",
      "Iteration 5238: Policy loss: 0.015251. Value loss: 0.039118. Entropy: 0.302412.\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5239: Policy loss: -0.182945. Value loss: 0.118841. Entropy: 0.300671.\n",
      "Iteration 5240: Policy loss: -0.181667. Value loss: 0.056408. Entropy: 0.300127.\n",
      "Iteration 5241: Policy loss: -0.184697. Value loss: 0.033568. Entropy: 0.299345.\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5242: Policy loss: -0.182142. Value loss: 0.220936. Entropy: 0.304704.\n",
      "Iteration 5243: Policy loss: -0.194000. Value loss: 0.082598. Entropy: 0.302471.\n",
      "Iteration 5244: Policy loss: -0.197878. Value loss: 0.049054. Entropy: 0.304763.\n",
      "episode: 2205   score: 355.0  epsilon: 1.0    steps: 144  evaluation reward: 367.0\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5245: Policy loss: -0.360127. Value loss: 0.252549. Entropy: 0.285873.\n",
      "Iteration 5246: Policy loss: -0.392269. Value loss: 0.098008. Entropy: 0.281812.\n",
      "Iteration 5247: Policy loss: -0.411581. Value loss: 0.061748. Entropy: 0.282646.\n",
      "episode: 2206   score: 275.0  epsilon: 1.0    steps: 304  evaluation reward: 367.65\n",
      "episode: 2207   score: 620.0  epsilon: 1.0    steps: 936  evaluation reward: 368.7\n",
      "Training network. lr: 0.000210. clip: 0.084029\n",
      "Iteration 5248: Policy loss: 0.003940. Value loss: 0.198361. Entropy: 0.286415.\n",
      "Iteration 5249: Policy loss: -0.039077. Value loss: 0.091688. Entropy: 0.285575.\n",
      "Iteration 5250: Policy loss: -0.030482. Value loss: 0.071616. Entropy: 0.284485.\n",
      "episode: 2208   score: 560.0  epsilon: 1.0    steps: 320  evaluation reward: 369.95\n",
      "episode: 2209   score: 425.0  epsilon: 1.0    steps: 720  evaluation reward: 372.65\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5251: Policy loss: 0.246813. Value loss: 0.136575. Entropy: 0.267357.\n",
      "Iteration 5252: Policy loss: 0.230595. Value loss: 0.049453. Entropy: 0.267706.\n",
      "Iteration 5253: Policy loss: 0.222700. Value loss: 0.030858. Entropy: 0.266468.\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5254: Policy loss: 0.003744. Value loss: 0.121528. Entropy: 0.297529.\n",
      "Iteration 5255: Policy loss: 0.000958. Value loss: 0.045620. Entropy: 0.299025.\n",
      "Iteration 5256: Policy loss: -0.003461. Value loss: 0.031918. Entropy: 0.298700.\n",
      "episode: 2210   score: 525.0  epsilon: 1.0    steps: 200  evaluation reward: 373.0\n",
      "episode: 2211   score: 645.0  epsilon: 1.0    steps: 384  evaluation reward: 377.35\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5257: Policy loss: -0.562421. Value loss: 0.372619. Entropy: 0.271670.\n",
      "Iteration 5258: Policy loss: -0.577916. Value loss: 0.167731. Entropy: 0.273760.\n",
      "Iteration 5259: Policy loss: -0.601536. Value loss: 0.076397. Entropy: 0.272962.\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5260: Policy loss: 0.217569. Value loss: 0.191582. Entropy: 0.301785.\n",
      "Iteration 5261: Policy loss: 0.212799. Value loss: 0.068596. Entropy: 0.300724.\n",
      "Iteration 5262: Policy loss: 0.207083. Value loss: 0.046301. Entropy: 0.302099.\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5263: Policy loss: 0.214253. Value loss: 0.134947. Entropy: 0.297849.\n",
      "Iteration 5264: Policy loss: 0.203610. Value loss: 0.042987. Entropy: 0.297343.\n",
      "Iteration 5265: Policy loss: 0.199252. Value loss: 0.032538. Entropy: 0.296492.\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5266: Policy loss: 0.393923. Value loss: 0.179085. Entropy: 0.304706.\n",
      "Iteration 5267: Policy loss: 0.382685. Value loss: 0.066091. Entropy: 0.304925.\n",
      "Iteration 5268: Policy loss: 0.369384. Value loss: 0.041344. Entropy: 0.303695.\n",
      "episode: 2212   score: 345.0  epsilon: 1.0    steps: 528  evaluation reward: 378.2\n",
      "episode: 2213   score: 420.0  epsilon: 1.0    steps: 864  evaluation reward: 380.3\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5269: Policy loss: 0.057400. Value loss: 0.081465. Entropy: 0.291172.\n",
      "Iteration 5270: Policy loss: 0.057066. Value loss: 0.035387. Entropy: 0.290016.\n",
      "Iteration 5271: Policy loss: 0.050896. Value loss: 0.025625. Entropy: 0.289408.\n",
      "episode: 2214   score: 575.0  epsilon: 1.0    steps: 584  evaluation reward: 383.65\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5272: Policy loss: -0.009058. Value loss: 0.164840. Entropy: 0.282841.\n",
      "Iteration 5273: Policy loss: -0.006705. Value loss: 0.063221. Entropy: 0.284278.\n",
      "Iteration 5274: Policy loss: -0.023677. Value loss: 0.045744. Entropy: 0.283762.\n",
      "episode: 2215   score: 510.0  epsilon: 1.0    steps: 208  evaluation reward: 386.65\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5275: Policy loss: -0.065996. Value loss: 0.089390. Entropy: 0.285900.\n",
      "Iteration 5276: Policy loss: -0.077481. Value loss: 0.045451. Entropy: 0.285188.\n",
      "Iteration 5277: Policy loss: -0.072273. Value loss: 0.033488. Entropy: 0.285844.\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5278: Policy loss: -0.090040. Value loss: 0.268365. Entropy: 0.299830.\n",
      "Iteration 5279: Policy loss: -0.095040. Value loss: 0.142474. Entropy: 0.303688.\n",
      "Iteration 5280: Policy loss: -0.111551. Value loss: 0.080221. Entropy: 0.300066.\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5281: Policy loss: -0.391714. Value loss: 0.294616. Entropy: 0.303031.\n",
      "Iteration 5282: Policy loss: -0.397837. Value loss: 0.089397. Entropy: 0.300520.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5283: Policy loss: -0.426356. Value loss: 0.053165. Entropy: 0.301864.\n",
      "episode: 2216   score: 335.0  epsilon: 1.0    steps: 112  evaluation reward: 387.35\n",
      "episode: 2217   score: 620.0  epsilon: 1.0    steps: 840  evaluation reward: 387.8\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5284: Policy loss: 0.243341. Value loss: 0.130143. Entropy: 0.285561.\n",
      "Iteration 5285: Policy loss: 0.238396. Value loss: 0.057308. Entropy: 0.283189.\n",
      "Iteration 5286: Policy loss: 0.222689. Value loss: 0.047019. Entropy: 0.284991.\n",
      "episode: 2218   score: 680.0  epsilon: 1.0    steps: 312  evaluation reward: 390.9\n",
      "episode: 2219   score: 240.0  epsilon: 1.0    steps: 1008  evaluation reward: 389.85\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5287: Policy loss: 0.207227. Value loss: 0.084806. Entropy: 0.284060.\n",
      "Iteration 5288: Policy loss: 0.203890. Value loss: 0.040282. Entropy: 0.284717.\n",
      "Iteration 5289: Policy loss: 0.198876. Value loss: 0.031829. Entropy: 0.281959.\n",
      "episode: 2220   score: 480.0  epsilon: 1.0    steps: 736  evaluation reward: 391.75\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5290: Policy loss: 0.016389. Value loss: 0.087437. Entropy: 0.276879.\n",
      "Iteration 5291: Policy loss: -0.003339. Value loss: 0.039473. Entropy: 0.278222.\n",
      "Iteration 5292: Policy loss: -0.001336. Value loss: 0.032361. Entropy: 0.278503.\n",
      "episode: 2221   score: 225.0  epsilon: 1.0    steps: 144  evaluation reward: 391.1\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5293: Policy loss: 0.094709. Value loss: 0.141559. Entropy: 0.282981.\n",
      "Iteration 5294: Policy loss: 0.087303. Value loss: 0.045214. Entropy: 0.281602.\n",
      "Iteration 5295: Policy loss: 0.079739. Value loss: 0.031150. Entropy: 0.281582.\n",
      "episode: 2222   score: 265.0  epsilon: 1.0    steps: 472  evaluation reward: 389.6\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5296: Policy loss: 0.209728. Value loss: 0.097773. Entropy: 0.284259.\n",
      "Iteration 5297: Policy loss: 0.211922. Value loss: 0.047115. Entropy: 0.283009.\n",
      "Iteration 5298: Policy loss: 0.203553. Value loss: 0.035339. Entropy: 0.280907.\n",
      "Training network. lr: 0.000210. clip: 0.083881\n",
      "Iteration 5299: Policy loss: -0.167716. Value loss: 0.250401. Entropy: 0.303584.\n",
      "Iteration 5300: Policy loss: -0.194409. Value loss: 0.097026. Entropy: 0.300766.\n",
      "Iteration 5301: Policy loss: -0.214878. Value loss: 0.059731. Entropy: 0.302450.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5302: Policy loss: -0.090799. Value loss: 0.093811. Entropy: 0.304798.\n",
      "Iteration 5303: Policy loss: -0.101969. Value loss: 0.038913. Entropy: 0.304841.\n",
      "Iteration 5304: Policy loss: -0.100898. Value loss: 0.028357. Entropy: 0.303802.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5305: Policy loss: -0.313426. Value loss: 0.315132. Entropy: 0.303817.\n",
      "Iteration 5306: Policy loss: -0.331231. Value loss: 0.207493. Entropy: 0.304798.\n",
      "Iteration 5307: Policy loss: -0.328472. Value loss: 0.092573. Entropy: 0.304347.\n",
      "episode: 2223   score: 740.0  epsilon: 1.0    steps: 72  evaluation reward: 391.35\n",
      "episode: 2224   score: 310.0  epsilon: 1.0    steps: 680  evaluation reward: 392.2\n",
      "episode: 2225   score: 485.0  epsilon: 1.0    steps: 728  evaluation reward: 394.05\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5308: Policy loss: -0.619630. Value loss: 0.423633. Entropy: 0.262914.\n",
      "Iteration 5309: Policy loss: -0.641845. Value loss: 0.161453. Entropy: 0.264349.\n",
      "Iteration 5310: Policy loss: -0.623604. Value loss: 0.075915. Entropy: 0.263205.\n",
      "episode: 2226   score: 300.0  epsilon: 1.0    steps: 264  evaluation reward: 393.9\n",
      "episode: 2227   score: 395.0  epsilon: 1.0    steps: 632  evaluation reward: 395.6\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5311: Policy loss: 0.325490. Value loss: 0.158296. Entropy: 0.275935.\n",
      "Iteration 5312: Policy loss: 0.304825. Value loss: 0.050876. Entropy: 0.273720.\n",
      "Iteration 5313: Policy loss: 0.298535. Value loss: 0.036028. Entropy: 0.272473.\n",
      "episode: 2228   score: 290.0  epsilon: 1.0    steps: 80  evaluation reward: 395.05\n",
      "episode: 2229   score: 565.0  epsilon: 1.0    steps: 992  evaluation reward: 396.05\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5314: Policy loss: -0.151869. Value loss: 0.145386. Entropy: 0.289284.\n",
      "Iteration 5315: Policy loss: -0.158981. Value loss: 0.078720. Entropy: 0.288124.\n",
      "Iteration 5316: Policy loss: -0.166711. Value loss: 0.057416. Entropy: 0.286767.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5317: Policy loss: 0.119692. Value loss: 0.117408. Entropy: 0.296757.\n",
      "Iteration 5318: Policy loss: 0.117913. Value loss: 0.049525. Entropy: 0.297009.\n",
      "Iteration 5319: Policy loss: 0.117140. Value loss: 0.038465. Entropy: 0.297438.\n",
      "episode: 2230   score: 395.0  epsilon: 1.0    steps: 216  evaluation reward: 393.8\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5320: Policy loss: -0.174940. Value loss: 0.072751. Entropy: 0.291346.\n",
      "Iteration 5321: Policy loss: -0.179085. Value loss: 0.032595. Entropy: 0.290921.\n",
      "Iteration 5322: Policy loss: -0.183992. Value loss: 0.026606. Entropy: 0.291272.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5323: Policy loss: 0.067690. Value loss: 0.111605. Entropy: 0.311667.\n",
      "Iteration 5324: Policy loss: 0.050062. Value loss: 0.042522. Entropy: 0.312305.\n",
      "Iteration 5325: Policy loss: 0.055762. Value loss: 0.029017. Entropy: 0.311518.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5326: Policy loss: -0.340218. Value loss: 0.389276. Entropy: 0.305166.\n",
      "Iteration 5327: Policy loss: -0.378217. Value loss: 0.153194. Entropy: 0.304180.\n",
      "Iteration 5328: Policy loss: -0.373048. Value loss: 0.110414. Entropy: 0.303031.\n",
      "episode: 2231   score: 295.0  epsilon: 1.0    steps: 824  evaluation reward: 393.85\n",
      "episode: 2232   score: 315.0  epsilon: 1.0    steps: 912  evaluation reward: 393.1\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5329: Policy loss: 0.049534. Value loss: 0.218788. Entropy: 0.289318.\n",
      "Iteration 5330: Policy loss: 0.037514. Value loss: 0.118172. Entropy: 0.287902.\n",
      "Iteration 5331: Policy loss: 0.059178. Value loss: 0.073224. Entropy: 0.287651.\n",
      "episode: 2233   score: 415.0  epsilon: 1.0    steps: 736  evaluation reward: 396.05\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5332: Policy loss: -0.010467. Value loss: 0.148669. Entropy: 0.283935.\n",
      "Iteration 5333: Policy loss: -0.005275. Value loss: 0.073413. Entropy: 0.282293.\n",
      "Iteration 5334: Policy loss: -0.013770. Value loss: 0.057349. Entropy: 0.283652.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5335: Policy loss: -0.013577. Value loss: 0.125412. Entropy: 0.304247.\n",
      "Iteration 5336: Policy loss: -0.021157. Value loss: 0.060491. Entropy: 0.302313.\n",
      "Iteration 5337: Policy loss: -0.035579. Value loss: 0.042553. Entropy: 0.302910.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5338: Policy loss: 0.233779. Value loss: 0.362963. Entropy: 0.301078.\n",
      "Iteration 5339: Policy loss: 0.212565. Value loss: 0.112858. Entropy: 0.300925.\n",
      "Iteration 5340: Policy loss: 0.226172. Value loss: 0.078955. Entropy: 0.301927.\n",
      "episode: 2234   score: 700.0  epsilon: 1.0    steps: 480  evaluation reward: 398.05\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5341: Policy loss: -0.436288. Value loss: 0.452254. Entropy: 0.292678.\n",
      "Iteration 5342: Policy loss: -0.470621. Value loss: 0.212110. Entropy: 0.288669.\n",
      "Iteration 5343: Policy loss: -0.478119. Value loss: 0.146557. Entropy: 0.291244.\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5344: Policy loss: 0.683759. Value loss: 0.475586. Entropy: 0.299865.\n",
      "Iteration 5345: Policy loss: 0.645054. Value loss: 0.120600. Entropy: 0.302291.\n",
      "Iteration 5346: Policy loss: 0.641459. Value loss: 0.060822. Entropy: 0.298446.\n",
      "episode: 2235   score: 675.0  epsilon: 1.0    steps: 504  evaluation reward: 397.8\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5347: Policy loss: 0.553849. Value loss: 0.207481. Entropy: 0.290008.\n",
      "Iteration 5348: Policy loss: 0.544440. Value loss: 0.077182. Entropy: 0.289490.\n",
      "Iteration 5349: Policy loss: 0.525786. Value loss: 0.049275. Entropy: 0.287105.\n",
      "episode: 2236   score: 420.0  epsilon: 1.0    steps: 24  evaluation reward: 398.65\n",
      "episode: 2237   score: 635.0  epsilon: 1.0    steps: 88  evaluation reward: 402.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2238   score: 240.0  epsilon: 1.0    steps: 488  evaluation reward: 400.75\n",
      "Training network. lr: 0.000209. clip: 0.083725\n",
      "Iteration 5350: Policy loss: 0.249842. Value loss: 0.156122. Entropy: 0.264457.\n",
      "Iteration 5351: Policy loss: 0.237060. Value loss: 0.067922. Entropy: 0.262397.\n",
      "Iteration 5352: Policy loss: 0.224422. Value loss: 0.049465. Entropy: 0.263823.\n",
      "episode: 2239   score: 645.0  epsilon: 1.0    steps: 488  evaluation reward: 403.7\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5353: Policy loss: -0.006621. Value loss: 0.233397. Entropy: 0.281213.\n",
      "Iteration 5354: Policy loss: -0.016963. Value loss: 0.119735. Entropy: 0.280597.\n",
      "Iteration 5355: Policy loss: -0.027196. Value loss: 0.089123. Entropy: 0.279863.\n",
      "episode: 2240   score: 340.0  epsilon: 1.0    steps: 528  evaluation reward: 405.0\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5356: Policy loss: 0.262024. Value loss: 0.088404. Entropy: 0.286696.\n",
      "Iteration 5357: Policy loss: 0.254792. Value loss: 0.034463. Entropy: 0.287830.\n",
      "Iteration 5358: Policy loss: 0.253986. Value loss: 0.024389. Entropy: 0.286350.\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5359: Policy loss: -0.109537. Value loss: 0.136611. Entropy: 0.304203.\n",
      "Iteration 5360: Policy loss: -0.120099. Value loss: 0.058216. Entropy: 0.305126.\n",
      "Iteration 5361: Policy loss: -0.131609. Value loss: 0.041480. Entropy: 0.304522.\n",
      "episode: 2241   score: 460.0  epsilon: 1.0    steps: 960  evaluation reward: 405.1\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5362: Policy loss: 0.791979. Value loss: 0.268653. Entropy: 0.306976.\n",
      "Iteration 5363: Policy loss: 0.765525. Value loss: 0.098847. Entropy: 0.306466.\n",
      "Iteration 5364: Policy loss: 0.751052. Value loss: 0.058358. Entropy: 0.305209.\n",
      "episode: 2242   score: 565.0  epsilon: 1.0    steps: 48  evaluation reward: 407.4\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5365: Policy loss: -0.346883. Value loss: 0.306929. Entropy: 0.282782.\n",
      "Iteration 5366: Policy loss: -0.353372. Value loss: 0.130732. Entropy: 0.283259.\n",
      "Iteration 5367: Policy loss: -0.326285. Value loss: 0.087733. Entropy: 0.281181.\n",
      "episode: 2243   score: 255.0  epsilon: 1.0    steps: 488  evaluation reward: 403.3\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5368: Policy loss: -0.022014. Value loss: 0.200119. Entropy: 0.286615.\n",
      "Iteration 5369: Policy loss: -0.030310. Value loss: 0.105048. Entropy: 0.288067.\n",
      "Iteration 5370: Policy loss: -0.026287. Value loss: 0.080102. Entropy: 0.288408.\n",
      "episode: 2244   score: 365.0  epsilon: 1.0    steps: 800  evaluation reward: 403.45\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5371: Policy loss: 0.289160. Value loss: 0.203502. Entropy: 0.296487.\n",
      "Iteration 5372: Policy loss: 0.272595. Value loss: 0.095128. Entropy: 0.295401.\n",
      "Iteration 5373: Policy loss: 0.257215. Value loss: 0.067362. Entropy: 0.294241.\n",
      "episode: 2245   score: 240.0  epsilon: 1.0    steps: 64  evaluation reward: 400.25\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5374: Policy loss: 0.293604. Value loss: 0.094401. Entropy: 0.288267.\n",
      "Iteration 5375: Policy loss: 0.274976. Value loss: 0.027547. Entropy: 0.284583.\n",
      "Iteration 5376: Policy loss: 0.276241. Value loss: 0.019605. Entropy: 0.287121.\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5377: Policy loss: 0.194340. Value loss: 0.217842. Entropy: 0.305141.\n",
      "Iteration 5378: Policy loss: 0.195822. Value loss: 0.080622. Entropy: 0.305768.\n",
      "Iteration 5379: Policy loss: 0.185013. Value loss: 0.052914. Entropy: 0.304072.\n",
      "episode: 2246   score: 210.0  epsilon: 1.0    steps: 312  evaluation reward: 399.65\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5380: Policy loss: 0.217103. Value loss: 0.115972. Entropy: 0.289349.\n",
      "Iteration 5381: Policy loss: 0.208179. Value loss: 0.034022. Entropy: 0.290535.\n",
      "Iteration 5382: Policy loss: 0.199568. Value loss: 0.025070. Entropy: 0.289636.\n",
      "episode: 2247   score: 365.0  epsilon: 1.0    steps: 200  evaluation reward: 394.2\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5383: Policy loss: -0.040250. Value loss: 0.121792. Entropy: 0.293894.\n",
      "Iteration 5384: Policy loss: -0.047314. Value loss: 0.047678. Entropy: 0.292957.\n",
      "Iteration 5385: Policy loss: -0.053564. Value loss: 0.033973. Entropy: 0.292485.\n",
      "episode: 2248   score: 695.0  epsilon: 1.0    steps: 568  evaluation reward: 396.6\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5386: Policy loss: -0.037781. Value loss: 0.143276. Entropy: 0.289616.\n",
      "Iteration 5387: Policy loss: -0.047398. Value loss: 0.062312. Entropy: 0.290422.\n",
      "Iteration 5388: Policy loss: -0.057786. Value loss: 0.041494. Entropy: 0.290468.\n",
      "episode: 2249   score: 210.0  epsilon: 1.0    steps: 128  evaluation reward: 396.25\n",
      "episode: 2250   score: 370.0  epsilon: 1.0    steps: 384  evaluation reward: 397.85\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5389: Policy loss: -0.233258. Value loss: 0.249647. Entropy: 0.275202.\n",
      "Iteration 5390: Policy loss: -0.241265. Value loss: 0.142645. Entropy: 0.275410.\n",
      "Iteration 5391: Policy loss: -0.237251. Value loss: 0.075192. Entropy: 0.278117.\n",
      "now time :  2019-09-05 19:49:54.970681\n",
      "episode: 2251   score: 460.0  epsilon: 1.0    steps: 744  evaluation reward: 397.3\n",
      "episode: 2252   score: 665.0  epsilon: 1.0    steps: 752  evaluation reward: 399.6\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5392: Policy loss: 0.066733. Value loss: 0.264792. Entropy: 0.279820.\n",
      "Iteration 5393: Policy loss: 0.032407. Value loss: 0.098583. Entropy: 0.282284.\n",
      "Iteration 5394: Policy loss: 0.031887. Value loss: 0.056463. Entropy: 0.281183.\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5395: Policy loss: 0.105055. Value loss: 0.158264. Entropy: 0.299322.\n",
      "Iteration 5396: Policy loss: 0.103926. Value loss: 0.092036. Entropy: 0.297492.\n",
      "Iteration 5397: Policy loss: 0.103138. Value loss: 0.068647. Entropy: 0.299765.\n",
      "episode: 2253   score: 100.0  epsilon: 1.0    steps: 176  evaluation reward: 397.8\n",
      "episode: 2254   score: 395.0  epsilon: 1.0    steps: 520  evaluation reward: 399.65\n",
      "episode: 2255   score: 410.0  epsilon: 1.0    steps: 664  evaluation reward: 401.95\n",
      "Training network. lr: 0.000209. clip: 0.083568\n",
      "Iteration 5398: Policy loss: 0.525101. Value loss: 0.169478. Entropy: 0.261113.\n",
      "Iteration 5399: Policy loss: 0.511921. Value loss: 0.049954. Entropy: 0.261866.\n",
      "Iteration 5400: Policy loss: 0.514135. Value loss: 0.038619. Entropy: 0.262426.\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5401: Policy loss: 0.089811. Value loss: 0.093352. Entropy: 0.307745.\n",
      "Iteration 5402: Policy loss: 0.081412. Value loss: 0.033825. Entropy: 0.306665.\n",
      "Iteration 5403: Policy loss: 0.076871. Value loss: 0.023683. Entropy: 0.306276.\n",
      "episode: 2256   score: 330.0  epsilon: 1.0    steps: 712  evaluation reward: 404.05\n",
      "episode: 2257   score: 215.0  epsilon: 1.0    steps: 944  evaluation reward: 400.85\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5404: Policy loss: 0.041703. Value loss: 0.086486. Entropy: 0.293341.\n",
      "Iteration 5405: Policy loss: 0.029675. Value loss: 0.039777. Entropy: 0.294583.\n",
      "Iteration 5406: Policy loss: 0.034787. Value loss: 0.026998. Entropy: 0.294487.\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5407: Policy loss: 0.335131. Value loss: 0.188891. Entropy: 0.293644.\n",
      "Iteration 5408: Policy loss: 0.331225. Value loss: 0.065651. Entropy: 0.295014.\n",
      "Iteration 5409: Policy loss: 0.317883. Value loss: 0.046803. Entropy: 0.292883.\n",
      "episode: 2258   score: 210.0  epsilon: 1.0    steps: 880  evaluation reward: 400.7\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5410: Policy loss: 0.182290. Value loss: 0.090487. Entropy: 0.303150.\n",
      "Iteration 5411: Policy loss: 0.178643. Value loss: 0.037635. Entropy: 0.301618.\n",
      "Iteration 5412: Policy loss: 0.174579. Value loss: 0.027166. Entropy: 0.300902.\n",
      "episode: 2259   score: 180.0  epsilon: 1.0    steps: 968  evaluation reward: 399.3\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5413: Policy loss: 0.233735. Value loss: 0.074179. Entropy: 0.297090.\n",
      "Iteration 5414: Policy loss: 0.229952. Value loss: 0.041428. Entropy: 0.295403.\n",
      "Iteration 5415: Policy loss: 0.224942. Value loss: 0.032656. Entropy: 0.294389.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5416: Policy loss: -0.017249. Value loss: 0.066397. Entropy: 0.298901.\n",
      "Iteration 5417: Policy loss: -0.017665. Value loss: 0.033924. Entropy: 0.298383.\n",
      "Iteration 5418: Policy loss: -0.020233. Value loss: 0.027115. Entropy: 0.297154.\n",
      "episode: 2260   score: 310.0  epsilon: 1.0    steps: 56  evaluation reward: 397.85\n",
      "episode: 2261   score: 290.0  epsilon: 1.0    steps: 416  evaluation reward: 398.35\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5419: Policy loss: -0.240484. Value loss: 0.321218. Entropy: 0.279361.\n",
      "Iteration 5420: Policy loss: -0.262770. Value loss: 0.096741. Entropy: 0.279065.\n",
      "Iteration 5421: Policy loss: -0.270508. Value loss: 0.041294. Entropy: 0.277152.\n",
      "episode: 2262   score: 290.0  epsilon: 1.0    steps: 64  evaluation reward: 396.45\n",
      "episode: 2263   score: 650.0  epsilon: 1.0    steps: 256  evaluation reward: 399.8\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5422: Policy loss: 0.364391. Value loss: 0.137465. Entropy: 0.281026.\n",
      "Iteration 5423: Policy loss: 0.348968. Value loss: 0.047349. Entropy: 0.281616.\n",
      "Iteration 5424: Policy loss: 0.340349. Value loss: 0.032295. Entropy: 0.280313.\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5425: Policy loss: 0.064552. Value loss: 0.047387. Entropy: 0.309584.\n",
      "Iteration 5426: Policy loss: 0.060481. Value loss: 0.026913. Entropy: 0.307629.\n",
      "Iteration 5427: Policy loss: 0.057358. Value loss: 0.019523. Entropy: 0.306699.\n",
      "episode: 2264   score: 265.0  epsilon: 1.0    steps: 8  evaluation reward: 398.55\n",
      "episode: 2265   score: 255.0  epsilon: 1.0    steps: 248  evaluation reward: 398.5\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5428: Policy loss: -0.313557. Value loss: 0.174591. Entropy: 0.279749.\n",
      "Iteration 5429: Policy loss: -0.319544. Value loss: 0.050393. Entropy: 0.279560.\n",
      "Iteration 5430: Policy loss: -0.332952. Value loss: 0.030365. Entropy: 0.280547.\n",
      "episode: 2266   score: 285.0  epsilon: 1.0    steps: 952  evaluation reward: 398.35\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5431: Policy loss: 0.240914. Value loss: 0.116136. Entropy: 0.308033.\n",
      "Iteration 5432: Policy loss: 0.235594. Value loss: 0.037198. Entropy: 0.306501.\n",
      "Iteration 5433: Policy loss: 0.227216. Value loss: 0.026976. Entropy: 0.306309.\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5434: Policy loss: 0.096914. Value loss: 0.122405. Entropy: 0.302358.\n",
      "Iteration 5435: Policy loss: 0.080764. Value loss: 0.049401. Entropy: 0.301054.\n",
      "Iteration 5436: Policy loss: 0.071501. Value loss: 0.032576. Entropy: 0.302632.\n",
      "episode: 2267   score: 260.0  epsilon: 1.0    steps: 88  evaluation reward: 397.45\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5437: Policy loss: -0.134677. Value loss: 0.262442. Entropy: 0.294862.\n",
      "Iteration 5438: Policy loss: -0.146517. Value loss: 0.091454. Entropy: 0.295469.\n",
      "Iteration 5439: Policy loss: -0.130150. Value loss: 0.047044. Entropy: 0.296709.\n",
      "episode: 2268   score: 345.0  epsilon: 1.0    steps: 240  evaluation reward: 398.35\n",
      "episode: 2269   score: 500.0  epsilon: 1.0    steps: 560  evaluation reward: 398.35\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5440: Policy loss: 0.065525. Value loss: 0.110566. Entropy: 0.286966.\n",
      "Iteration 5441: Policy loss: 0.050544. Value loss: 0.063675. Entropy: 0.290165.\n",
      "Iteration 5442: Policy loss: 0.051286. Value loss: 0.045820. Entropy: 0.286954.\n",
      "episode: 2270   score: 285.0  epsilon: 1.0    steps: 336  evaluation reward: 397.85\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5443: Policy loss: 0.046054. Value loss: 0.082021. Entropy: 0.297262.\n",
      "Iteration 5444: Policy loss: 0.042182. Value loss: 0.038508. Entropy: 0.298874.\n",
      "Iteration 5445: Policy loss: 0.047015. Value loss: 0.030186. Entropy: 0.296491.\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5446: Policy loss: 0.253791. Value loss: 0.102747. Entropy: 0.306729.\n",
      "Iteration 5447: Policy loss: 0.241244. Value loss: 0.038230. Entropy: 0.304652.\n",
      "Iteration 5448: Policy loss: 0.234441. Value loss: 0.029300. Entropy: 0.304867.\n",
      "episode: 2271   score: 260.0  epsilon: 1.0    steps: 344  evaluation reward: 398.15\n",
      "episode: 2272   score: 520.0  epsilon: 1.0    steps: 400  evaluation reward: 396.9\n",
      "Training network. lr: 0.000209. clip: 0.083420\n",
      "Iteration 5449: Policy loss: 0.147477. Value loss: 0.075036. Entropy: 0.298939.\n",
      "Iteration 5450: Policy loss: 0.144972. Value loss: 0.040040. Entropy: 0.296081.\n",
      "Iteration 5451: Policy loss: 0.134716. Value loss: 0.029273. Entropy: 0.300785.\n",
      "episode: 2273   score: 155.0  epsilon: 1.0    steps: 432  evaluation reward: 393.0\n",
      "episode: 2274   score: 205.0  epsilon: 1.0    steps: 856  evaluation reward: 389.7\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5452: Policy loss: -0.047109. Value loss: 0.096754. Entropy: 0.294979.\n",
      "Iteration 5453: Policy loss: -0.057397. Value loss: 0.043264. Entropy: 0.290263.\n",
      "Iteration 5454: Policy loss: -0.060299. Value loss: 0.029129. Entropy: 0.294204.\n",
      "episode: 2275   score: 465.0  epsilon: 1.0    steps: 16  evaluation reward: 392.7\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5455: Policy loss: 0.107104. Value loss: 0.120800. Entropy: 0.305593.\n",
      "Iteration 5456: Policy loss: 0.103962. Value loss: 0.051315. Entropy: 0.302972.\n",
      "Iteration 5457: Policy loss: 0.106438. Value loss: 0.033042. Entropy: 0.303718.\n",
      "episode: 2276   score: 280.0  epsilon: 1.0    steps: 304  evaluation reward: 393.4\n",
      "episode: 2277   score: 225.0  epsilon: 1.0    steps: 992  evaluation reward: 392.3\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5458: Policy loss: 0.025318. Value loss: 0.067796. Entropy: 0.300589.\n",
      "Iteration 5459: Policy loss: 0.016441. Value loss: 0.033079. Entropy: 0.301315.\n",
      "Iteration 5460: Policy loss: 0.020757. Value loss: 0.026485. Entropy: 0.299503.\n",
      "episode: 2278   score: 210.0  epsilon: 1.0    steps: 504  evaluation reward: 391.7\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5461: Policy loss: -0.029109. Value loss: 0.067590. Entropy: 0.296966.\n",
      "Iteration 5462: Policy loss: -0.032218. Value loss: 0.030166. Entropy: 0.295839.\n",
      "Iteration 5463: Policy loss: -0.034113. Value loss: 0.019380. Entropy: 0.295746.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5464: Policy loss: 0.211431. Value loss: 0.091781. Entropy: 0.307300.\n",
      "Iteration 5465: Policy loss: 0.191549. Value loss: 0.035825. Entropy: 0.308229.\n",
      "Iteration 5466: Policy loss: 0.190059. Value loss: 0.025824. Entropy: 0.308585.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5467: Policy loss: -0.216811. Value loss: 0.141961. Entropy: 0.308503.\n",
      "Iteration 5468: Policy loss: -0.220923. Value loss: 0.051776. Entropy: 0.309330.\n",
      "Iteration 5469: Policy loss: -0.230567. Value loss: 0.035193. Entropy: 0.309071.\n",
      "episode: 2279   score: 210.0  epsilon: 1.0    steps: 88  evaluation reward: 388.05\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5470: Policy loss: -0.414425. Value loss: 0.261186. Entropy: 0.303228.\n",
      "Iteration 5471: Policy loss: -0.412473. Value loss: 0.084850. Entropy: 0.305656.\n",
      "Iteration 5472: Policy loss: -0.438713. Value loss: 0.045628. Entropy: 0.304824.\n",
      "episode: 2280   score: 210.0  epsilon: 1.0    steps: 480  evaluation reward: 384.8\n",
      "episode: 2281   score: 335.0  epsilon: 1.0    steps: 728  evaluation reward: 384.8\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5473: Policy loss: -0.007361. Value loss: 0.081106. Entropy: 0.293953.\n",
      "Iteration 5474: Policy loss: -0.014143. Value loss: 0.039073. Entropy: 0.296842.\n",
      "Iteration 5475: Policy loss: -0.019532. Value loss: 0.029807. Entropy: 0.296520.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5476: Policy loss: 0.086835. Value loss: 0.094019. Entropy: 0.308246.\n",
      "Iteration 5477: Policy loss: 0.068944. Value loss: 0.043620. Entropy: 0.305933.\n",
      "Iteration 5478: Policy loss: 0.077688. Value loss: 0.034740. Entropy: 0.307691.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5479: Policy loss: -0.130048. Value loss: 0.295762. Entropy: 0.309100.\n",
      "Iteration 5480: Policy loss: -0.155994. Value loss: 0.199714. Entropy: 0.309594.\n",
      "Iteration 5481: Policy loss: -0.142829. Value loss: 0.123221. Entropy: 0.308827.\n",
      "episode: 2282   score: 390.0  epsilon: 1.0    steps: 296  evaluation reward: 384.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2283   score: 315.0  epsilon: 1.0    steps: 784  evaluation reward: 381.9\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5482: Policy loss: 0.018581. Value loss: 0.137950. Entropy: 0.301295.\n",
      "Iteration 5483: Policy loss: 0.012309. Value loss: 0.061441. Entropy: 0.301423.\n",
      "Iteration 5484: Policy loss: 0.006865. Value loss: 0.047192. Entropy: 0.301568.\n",
      "episode: 2284   score: 690.0  epsilon: 1.0    steps: 128  evaluation reward: 386.7\n",
      "episode: 2285   score: 420.0  epsilon: 1.0    steps: 248  evaluation reward: 384.7\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5485: Policy loss: 0.196019. Value loss: 0.044372. Entropy: 0.292512.\n",
      "Iteration 5486: Policy loss: 0.189178. Value loss: 0.025082. Entropy: 0.293206.\n",
      "Iteration 5487: Policy loss: 0.183071. Value loss: 0.020443. Entropy: 0.291732.\n",
      "episode: 2286   score: 180.0  epsilon: 1.0    steps: 792  evaluation reward: 384.4\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5488: Policy loss: 0.037200. Value loss: 0.119733. Entropy: 0.305131.\n",
      "Iteration 5489: Policy loss: 0.037400. Value loss: 0.049561. Entropy: 0.302585.\n",
      "Iteration 5490: Policy loss: 0.039888. Value loss: 0.040834. Entropy: 0.303128.\n",
      "episode: 2287   score: 200.0  epsilon: 1.0    steps: 744  evaluation reward: 382.95\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5491: Policy loss: -0.009716. Value loss: 0.146540. Entropy: 0.299041.\n",
      "Iteration 5492: Policy loss: -0.022461. Value loss: 0.071285. Entropy: 0.298537.\n",
      "Iteration 5493: Policy loss: -0.032446. Value loss: 0.054447. Entropy: 0.296266.\n",
      "episode: 2288   score: 420.0  epsilon: 1.0    steps: 56  evaluation reward: 383.5\n",
      "episode: 2289   score: 820.0  epsilon: 1.0    steps: 336  evaluation reward: 388.25\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5494: Policy loss: 0.241033. Value loss: 0.115699. Entropy: 0.295555.\n",
      "Iteration 5495: Policy loss: 0.226163. Value loss: 0.045279. Entropy: 0.299971.\n",
      "Iteration 5496: Policy loss: 0.224724. Value loss: 0.032210. Entropy: 0.297722.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5497: Policy loss: 0.021981. Value loss: 0.105232. Entropy: 0.314327.\n",
      "Iteration 5498: Policy loss: 0.019434. Value loss: 0.034661. Entropy: 0.313741.\n",
      "Iteration 5499: Policy loss: 0.013889. Value loss: 0.024826. Entropy: 0.314288.\n",
      "Training network. lr: 0.000208. clip: 0.083264\n",
      "Iteration 5500: Policy loss: 0.017931. Value loss: 0.092378. Entropy: 0.312326.\n",
      "Iteration 5501: Policy loss: 0.009562. Value loss: 0.040149. Entropy: 0.312032.\n",
      "Iteration 5502: Policy loss: 0.002115. Value loss: 0.028943. Entropy: 0.311939.\n",
      "episode: 2290   score: 270.0  epsilon: 1.0    steps: 576  evaluation reward: 387.05\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5503: Policy loss: -0.127611. Value loss: 0.363828. Entropy: 0.302176.\n",
      "Iteration 5504: Policy loss: -0.151140. Value loss: 0.131943. Entropy: 0.303966.\n",
      "Iteration 5505: Policy loss: -0.140783. Value loss: 0.067950. Entropy: 0.302212.\n",
      "episode: 2291   score: 315.0  epsilon: 1.0    steps: 312  evaluation reward: 387.05\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5506: Policy loss: 0.188668. Value loss: 0.056993. Entropy: 0.300425.\n",
      "Iteration 5507: Policy loss: 0.178238. Value loss: 0.025207. Entropy: 0.301624.\n",
      "Iteration 5508: Policy loss: 0.179527. Value loss: 0.020538. Entropy: 0.300319.\n",
      "episode: 2292   score: 240.0  epsilon: 1.0    steps: 376  evaluation reward: 384.7\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5509: Policy loss: 0.184239. Value loss: 0.108989. Entropy: 0.299625.\n",
      "Iteration 5510: Policy loss: 0.170154. Value loss: 0.032968. Entropy: 0.298618.\n",
      "Iteration 5511: Policy loss: 0.162432. Value loss: 0.022420. Entropy: 0.298349.\n",
      "episode: 2293   score: 425.0  epsilon: 1.0    steps: 752  evaluation reward: 387.3\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5512: Policy loss: 0.151043. Value loss: 0.114355. Entropy: 0.301224.\n",
      "Iteration 5513: Policy loss: 0.152166. Value loss: 0.052124. Entropy: 0.301418.\n",
      "Iteration 5514: Policy loss: 0.141474. Value loss: 0.034462. Entropy: 0.300202.\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5515: Policy loss: 0.020382. Value loss: 0.106377. Entropy: 0.309772.\n",
      "Iteration 5516: Policy loss: 0.014743. Value loss: 0.039283. Entropy: 0.309384.\n",
      "Iteration 5517: Policy loss: 0.004582. Value loss: 0.026526. Entropy: 0.309894.\n",
      "episode: 2294   score: 480.0  epsilon: 1.0    steps: 136  evaluation reward: 388.45\n",
      "episode: 2295   score: 420.0  epsilon: 1.0    steps: 272  evaluation reward: 389.45\n",
      "episode: 2296   score: 255.0  epsilon: 1.0    steps: 336  evaluation reward: 391.2\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5518: Policy loss: 0.004264. Value loss: 0.058640. Entropy: 0.298911.\n",
      "Iteration 5519: Policy loss: -0.003266. Value loss: 0.023481. Entropy: 0.298891.\n",
      "Iteration 5520: Policy loss: -0.007581. Value loss: 0.020445. Entropy: 0.299959.\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5521: Policy loss: -0.718575. Value loss: 0.640246. Entropy: 0.307219.\n",
      "Iteration 5522: Policy loss: -0.741094. Value loss: 0.322854. Entropy: 0.307036.\n",
      "Iteration 5523: Policy loss: -0.759898. Value loss: 0.166068. Entropy: 0.307013.\n",
      "episode: 2297   score: 315.0  epsilon: 1.0    steps: 664  evaluation reward: 388.45\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5524: Policy loss: 0.020747. Value loss: 0.069958. Entropy: 0.309287.\n",
      "Iteration 5525: Policy loss: 0.008384. Value loss: 0.029895. Entropy: 0.310558.\n",
      "Iteration 5526: Policy loss: 0.003901. Value loss: 0.020443. Entropy: 0.310882.\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5527: Policy loss: -0.247992. Value loss: 0.405277. Entropy: 0.307947.\n",
      "Iteration 5528: Policy loss: -0.297324. Value loss: 0.107067. Entropy: 0.309267.\n",
      "Iteration 5529: Policy loss: -0.279465. Value loss: 0.059436. Entropy: 0.308228.\n",
      "episode: 2298   score: 335.0  epsilon: 1.0    steps: 120  evaluation reward: 388.95\n",
      "episode: 2299   score: 480.0  epsilon: 1.0    steps: 400  evaluation reward: 387.85\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5530: Policy loss: 0.081777. Value loss: 0.087440. Entropy: 0.300323.\n",
      "Iteration 5531: Policy loss: 0.077839. Value loss: 0.040071. Entropy: 0.297888.\n",
      "Iteration 5532: Policy loss: 0.069504. Value loss: 0.028823. Entropy: 0.298841.\n",
      "episode: 2300   score: 565.0  epsilon: 1.0    steps: 504  evaluation reward: 389.55\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5533: Policy loss: 0.143528. Value loss: 0.141238. Entropy: 0.308367.\n",
      "Iteration 5534: Policy loss: 0.141597. Value loss: 0.052175. Entropy: 0.306433.\n",
      "Iteration 5535: Policy loss: 0.131738. Value loss: 0.037525. Entropy: 0.308351.\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5536: Policy loss: 0.305381. Value loss: 0.169652. Entropy: 0.303750.\n",
      "Iteration 5537: Policy loss: 0.300656. Value loss: 0.047195. Entropy: 0.302441.\n",
      "Iteration 5538: Policy loss: 0.279516. Value loss: 0.027074. Entropy: 0.301807.\n",
      "now time :  2019-09-05 19:59:02.348061\n",
      "episode: 2301   score: 250.0  epsilon: 1.0    steps: 968  evaluation reward: 387.6\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5539: Policy loss: 0.185494. Value loss: 0.172456. Entropy: 0.305619.\n",
      "Iteration 5540: Policy loss: 0.169205. Value loss: 0.071966. Entropy: 0.305740.\n",
      "Iteration 5541: Policy loss: 0.168807. Value loss: 0.045311. Entropy: 0.305684.\n",
      "episode: 2302   score: 380.0  epsilon: 1.0    steps: 96  evaluation reward: 387.05\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5542: Policy loss: -0.199137. Value loss: 0.114880. Entropy: 0.283391.\n",
      "Iteration 5543: Policy loss: -0.204865. Value loss: 0.052666. Entropy: 0.284002.\n",
      "Iteration 5544: Policy loss: -0.212482. Value loss: 0.037786. Entropy: 0.282687.\n",
      "episode: 2303   score: 515.0  epsilon: 1.0    steps: 608  evaluation reward: 389.35\n",
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5545: Policy loss: -0.117943. Value loss: 0.108549. Entropy: 0.298492.\n",
      "Iteration 5546: Policy loss: -0.124796. Value loss: 0.041728. Entropy: 0.299568.\n",
      "Iteration 5547: Policy loss: -0.128947. Value loss: 0.029339. Entropy: 0.299487.\n",
      "episode: 2304   score: 500.0  epsilon: 1.0    steps: 120  evaluation reward: 389.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000208. clip: 0.083107\n",
      "Iteration 5548: Policy loss: 0.303836. Value loss: 0.086742. Entropy: 0.292794.\n",
      "Iteration 5549: Policy loss: 0.294408. Value loss: 0.041133. Entropy: 0.291588.\n",
      "Iteration 5550: Policy loss: 0.298013. Value loss: 0.030244. Entropy: 0.289658.\n",
      "episode: 2305   score: 260.0  epsilon: 1.0    steps: 424  evaluation reward: 388.55\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5551: Policy loss: 0.128444. Value loss: 0.068656. Entropy: 0.295540.\n",
      "Iteration 5552: Policy loss: 0.116741. Value loss: 0.033890. Entropy: 0.293253.\n",
      "Iteration 5553: Policy loss: 0.118955. Value loss: 0.025721. Entropy: 0.292668.\n",
      "episode: 2306   score: 295.0  epsilon: 1.0    steps: 768  evaluation reward: 388.75\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5554: Policy loss: 0.032538. Value loss: 0.080747. Entropy: 0.297356.\n",
      "Iteration 5555: Policy loss: 0.027998. Value loss: 0.034688. Entropy: 0.297445.\n",
      "Iteration 5556: Policy loss: 0.022512. Value loss: 0.023507. Entropy: 0.296979.\n",
      "episode: 2307   score: 460.0  epsilon: 1.0    steps: 976  evaluation reward: 387.15\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5557: Policy loss: -0.118900. Value loss: 0.255489. Entropy: 0.305075.\n",
      "Iteration 5558: Policy loss: -0.138404. Value loss: 0.110445. Entropy: 0.303380.\n",
      "Iteration 5559: Policy loss: -0.139640. Value loss: 0.052837. Entropy: 0.304540.\n",
      "episode: 2308   score: 240.0  epsilon: 1.0    steps: 424  evaluation reward: 383.95\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5560: Policy loss: 0.175097. Value loss: 0.162528. Entropy: 0.283628.\n",
      "Iteration 5561: Policy loss: 0.154175. Value loss: 0.060679. Entropy: 0.281782.\n",
      "Iteration 5562: Policy loss: 0.157614. Value loss: 0.038804. Entropy: 0.282977.\n",
      "episode: 2309   score: 420.0  epsilon: 1.0    steps: 24  evaluation reward: 383.9\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5563: Policy loss: 0.124776. Value loss: 0.077108. Entropy: 0.298317.\n",
      "Iteration 5564: Policy loss: 0.113223. Value loss: 0.037324. Entropy: 0.298136.\n",
      "Iteration 5565: Policy loss: 0.119781. Value loss: 0.027640. Entropy: 0.299402.\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5566: Policy loss: 0.066426. Value loss: 0.078750. Entropy: 0.310422.\n",
      "Iteration 5567: Policy loss: 0.062581. Value loss: 0.034168. Entropy: 0.309348.\n",
      "Iteration 5568: Policy loss: 0.063008. Value loss: 0.026142. Entropy: 0.309251.\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5569: Policy loss: 0.342216. Value loss: 0.124254. Entropy: 0.308300.\n",
      "Iteration 5570: Policy loss: 0.344923. Value loss: 0.040363. Entropy: 0.307934.\n",
      "Iteration 5571: Policy loss: 0.324855. Value loss: 0.028539. Entropy: 0.307908.\n",
      "episode: 2310   score: 345.0  epsilon: 1.0    steps: 336  evaluation reward: 382.1\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5572: Policy loss: 0.090717. Value loss: 0.131636. Entropy: 0.297515.\n",
      "Iteration 5573: Policy loss: 0.085122. Value loss: 0.048089. Entropy: 0.295935.\n",
      "Iteration 5574: Policy loss: 0.084260. Value loss: 0.032012. Entropy: 0.294690.\n",
      "episode: 2311   score: 420.0  epsilon: 1.0    steps: 424  evaluation reward: 379.85\n",
      "episode: 2312   score: 270.0  epsilon: 1.0    steps: 448  evaluation reward: 379.1\n",
      "episode: 2313   score: 290.0  epsilon: 1.0    steps: 936  evaluation reward: 377.8\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5575: Policy loss: 0.172351. Value loss: 0.109350. Entropy: 0.277006.\n",
      "Iteration 5576: Policy loss: 0.167307. Value loss: 0.053027. Entropy: 0.277796.\n",
      "Iteration 5577: Policy loss: 0.156777. Value loss: 0.040031. Entropy: 0.278691.\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5578: Policy loss: -0.104540. Value loss: 0.131024. Entropy: 0.297646.\n",
      "Iteration 5579: Policy loss: -0.107832. Value loss: 0.064573. Entropy: 0.298251.\n",
      "Iteration 5580: Policy loss: -0.119741. Value loss: 0.044302. Entropy: 0.299290.\n",
      "episode: 2314   score: 570.0  epsilon: 1.0    steps: 776  evaluation reward: 377.75\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5581: Policy loss: 0.111483. Value loss: 0.089717. Entropy: 0.300596.\n",
      "Iteration 5582: Policy loss: 0.108804. Value loss: 0.044849. Entropy: 0.300474.\n",
      "Iteration 5583: Policy loss: 0.103946. Value loss: 0.035426. Entropy: 0.300015.\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5584: Policy loss: 0.215344. Value loss: 0.117932. Entropy: 0.304705.\n",
      "Iteration 5585: Policy loss: 0.205608. Value loss: 0.046012. Entropy: 0.304235.\n",
      "Iteration 5586: Policy loss: 0.197403. Value loss: 0.029184. Entropy: 0.303458.\n",
      "episode: 2315   score: 285.0  epsilon: 1.0    steps: 192  evaluation reward: 375.5\n",
      "episode: 2316   score: 315.0  epsilon: 1.0    steps: 304  evaluation reward: 375.3\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5587: Policy loss: -0.026733. Value loss: 0.065669. Entropy: 0.290508.\n",
      "Iteration 5588: Policy loss: -0.031412. Value loss: 0.031337. Entropy: 0.289690.\n",
      "Iteration 5589: Policy loss: -0.038994. Value loss: 0.024968. Entropy: 0.290474.\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5590: Policy loss: -0.153715. Value loss: 0.344547. Entropy: 0.309778.\n",
      "Iteration 5591: Policy loss: -0.151924. Value loss: 0.168835. Entropy: 0.310189.\n",
      "Iteration 5592: Policy loss: -0.178866. Value loss: 0.102238. Entropy: 0.309713.\n",
      "episode: 2317   score: 265.0  epsilon: 1.0    steps: 888  evaluation reward: 371.75\n",
      "episode: 2318   score: 420.0  epsilon: 1.0    steps: 904  evaluation reward: 369.15\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5593: Policy loss: 0.112061. Value loss: 0.098057. Entropy: 0.297590.\n",
      "Iteration 5594: Policy loss: 0.099575. Value loss: 0.035944. Entropy: 0.297595.\n",
      "Iteration 5595: Policy loss: 0.096326. Value loss: 0.027931. Entropy: 0.297905.\n",
      "episode: 2319   score: 260.0  epsilon: 1.0    steps: 592  evaluation reward: 369.35\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5596: Policy loss: -0.060072. Value loss: 0.086133. Entropy: 0.282765.\n",
      "Iteration 5597: Policy loss: -0.067555. Value loss: 0.034256. Entropy: 0.281086.\n",
      "Iteration 5598: Policy loss: -0.072317. Value loss: 0.024359. Entropy: 0.279743.\n",
      "episode: 2320   score: 515.0  epsilon: 1.0    steps: 728  evaluation reward: 369.7\n",
      "Training network. lr: 0.000207. clip: 0.082960\n",
      "Iteration 5599: Policy loss: -0.025506. Value loss: 0.126589. Entropy: 0.299158.\n",
      "Iteration 5600: Policy loss: -0.029225. Value loss: 0.049837. Entropy: 0.299003.\n",
      "Iteration 5601: Policy loss: -0.039240. Value loss: 0.031399. Entropy: 0.298845.\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5602: Policy loss: -0.051030. Value loss: 0.066634. Entropy: 0.311534.\n",
      "Iteration 5603: Policy loss: -0.057233. Value loss: 0.028774. Entropy: 0.311696.\n",
      "Iteration 5604: Policy loss: -0.062173. Value loss: 0.020495. Entropy: 0.311342.\n",
      "episode: 2321   score: 240.0  epsilon: 1.0    steps: 720  evaluation reward: 369.85\n",
      "episode: 2322   score: 135.0  epsilon: 1.0    steps: 824  evaluation reward: 368.55\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5605: Policy loss: 0.109005. Value loss: 0.085621. Entropy: 0.287844.\n",
      "Iteration 5606: Policy loss: 0.095547. Value loss: 0.038179. Entropy: 0.287122.\n",
      "Iteration 5607: Policy loss: 0.089282. Value loss: 0.028403. Entropy: 0.287137.\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5608: Policy loss: -0.027060. Value loss: 0.117831. Entropy: 0.312070.\n",
      "Iteration 5609: Policy loss: -0.037106. Value loss: 0.038965. Entropy: 0.312027.\n",
      "Iteration 5610: Policy loss: -0.038777. Value loss: 0.026963. Entropy: 0.311854.\n",
      "episode: 2323   score: 330.0  epsilon: 1.0    steps: 56  evaluation reward: 364.45\n",
      "episode: 2324   score: 510.0  epsilon: 1.0    steps: 328  evaluation reward: 366.45\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5611: Policy loss: -0.068585. Value loss: 0.104233. Entropy: 0.281619.\n",
      "Iteration 5612: Policy loss: -0.068252. Value loss: 0.046703. Entropy: 0.281081.\n",
      "Iteration 5613: Policy loss: -0.082262. Value loss: 0.031895. Entropy: 0.280977.\n",
      "episode: 2325   score: 215.0  epsilon: 1.0    steps: 336  evaluation reward: 363.75\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5614: Policy loss: 0.140541. Value loss: 0.114138. Entropy: 0.296564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5615: Policy loss: 0.128969. Value loss: 0.049795. Entropy: 0.296527.\n",
      "Iteration 5616: Policy loss: 0.126740. Value loss: 0.036770. Entropy: 0.295059.\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5617: Policy loss: 0.144459. Value loss: 0.047718. Entropy: 0.308941.\n",
      "Iteration 5618: Policy loss: 0.134382. Value loss: 0.022503. Entropy: 0.308644.\n",
      "Iteration 5619: Policy loss: 0.133570. Value loss: 0.016917. Entropy: 0.309639.\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5620: Policy loss: 0.039234. Value loss: 0.068580. Entropy: 0.310589.\n",
      "Iteration 5621: Policy loss: 0.030674. Value loss: 0.021271. Entropy: 0.311189.\n",
      "Iteration 5622: Policy loss: 0.028070. Value loss: 0.017992. Entropy: 0.310759.\n",
      "episode: 2326   score: 500.0  epsilon: 1.0    steps: 728  evaluation reward: 365.75\n",
      "episode: 2327   score: 390.0  epsilon: 1.0    steps: 896  evaluation reward: 365.7\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5623: Policy loss: -0.025616. Value loss: 0.102021. Entropy: 0.294209.\n",
      "Iteration 5624: Policy loss: -0.033791. Value loss: 0.048141. Entropy: 0.292681.\n",
      "Iteration 5625: Policy loss: -0.040303. Value loss: 0.037215. Entropy: 0.292706.\n",
      "episode: 2328   score: 145.0  epsilon: 1.0    steps: 744  evaluation reward: 364.25\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5626: Policy loss: 0.016897. Value loss: 0.106886. Entropy: 0.292307.\n",
      "Iteration 5627: Policy loss: 0.006611. Value loss: 0.055651. Entropy: 0.292754.\n",
      "Iteration 5628: Policy loss: 0.004707. Value loss: 0.042245. Entropy: 0.292082.\n",
      "episode: 2329   score: 420.0  epsilon: 1.0    steps: 416  evaluation reward: 362.8\n",
      "episode: 2330   score: 285.0  epsilon: 1.0    steps: 656  evaluation reward: 361.7\n",
      "episode: 2331   score: 350.0  epsilon: 1.0    steps: 896  evaluation reward: 362.25\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5629: Policy loss: 0.100131. Value loss: 0.105430. Entropy: 0.274877.\n",
      "Iteration 5630: Policy loss: 0.099191. Value loss: 0.041018. Entropy: 0.275487.\n",
      "Iteration 5631: Policy loss: 0.094436. Value loss: 0.032085. Entropy: 0.275344.\n",
      "episode: 2332   score: 265.0  epsilon: 1.0    steps: 336  evaluation reward: 361.75\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5632: Policy loss: -0.367330. Value loss: 0.323282. Entropy: 0.286873.\n",
      "Iteration 5633: Policy loss: -0.417807. Value loss: 0.235545. Entropy: 0.284986.\n",
      "Iteration 5634: Policy loss: -0.386957. Value loss: 0.133901. Entropy: 0.285650.\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5635: Policy loss: -0.283132. Value loss: 0.335058. Entropy: 0.307601.\n",
      "Iteration 5636: Policy loss: -0.328655. Value loss: 0.121442. Entropy: 0.307302.\n",
      "Iteration 5637: Policy loss: -0.343076. Value loss: 0.048096. Entropy: 0.307855.\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5638: Policy loss: -0.257068. Value loss: 0.196754. Entropy: 0.307305.\n",
      "Iteration 5639: Policy loss: -0.256952. Value loss: 0.048081. Entropy: 0.304896.\n",
      "Iteration 5640: Policy loss: -0.273229. Value loss: 0.023974. Entropy: 0.305779.\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5641: Policy loss: 0.053440. Value loss: 0.139115. Entropy: 0.305833.\n",
      "Iteration 5642: Policy loss: 0.038593. Value loss: 0.060014. Entropy: 0.306796.\n",
      "Iteration 5643: Policy loss: 0.043470. Value loss: 0.047631. Entropy: 0.306027.\n",
      "episode: 2333   score: 665.0  epsilon: 1.0    steps: 456  evaluation reward: 364.25\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5644: Policy loss: 0.095042. Value loss: 0.370894. Entropy: 0.293944.\n",
      "Iteration 5645: Policy loss: 0.042465. Value loss: 0.159542. Entropy: 0.293961.\n",
      "Iteration 5646: Policy loss: 0.046679. Value loss: 0.091614. Entropy: 0.292878.\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5647: Policy loss: 0.082914. Value loss: 0.085294. Entropy: 0.309537.\n",
      "Iteration 5648: Policy loss: 0.070915. Value loss: 0.042086. Entropy: 0.310949.\n",
      "Iteration 5649: Policy loss: 0.064403. Value loss: 0.030162. Entropy: 0.309933.\n",
      "episode: 2334   score: 240.0  epsilon: 1.0    steps: 256  evaluation reward: 359.65\n",
      "Training network. lr: 0.000207. clip: 0.082803\n",
      "Iteration 5650: Policy loss: -0.006561. Value loss: 0.139430. Entropy: 0.290014.\n",
      "Iteration 5651: Policy loss: -0.016984. Value loss: 0.054649. Entropy: 0.290711.\n",
      "Iteration 5652: Policy loss: -0.027037. Value loss: 0.039334. Entropy: 0.290522.\n",
      "episode: 2335   score: 265.0  epsilon: 1.0    steps: 200  evaluation reward: 355.55\n",
      "episode: 2336   score: 285.0  epsilon: 1.0    steps: 272  evaluation reward: 354.2\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5653: Policy loss: 0.247829. Value loss: 0.128903. Entropy: 0.275682.\n",
      "Iteration 5654: Policy loss: 0.254359. Value loss: 0.035936. Entropy: 0.277239.\n",
      "Iteration 5655: Policy loss: 0.240638. Value loss: 0.022747. Entropy: 0.277805.\n",
      "episode: 2337   score: 765.0  epsilon: 1.0    steps: 128  evaluation reward: 355.5\n",
      "episode: 2338   score: 545.0  epsilon: 1.0    steps: 264  evaluation reward: 358.55\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5656: Policy loss: 0.024278. Value loss: 0.084635. Entropy: 0.281513.\n",
      "Iteration 5657: Policy loss: 0.022513. Value loss: 0.044462. Entropy: 0.281451.\n",
      "Iteration 5658: Policy loss: 0.014829. Value loss: 0.034286. Entropy: 0.279194.\n",
      "episode: 2339   score: 420.0  epsilon: 1.0    steps: 280  evaluation reward: 356.3\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5659: Policy loss: -0.066901. Value loss: 0.096022. Entropy: 0.294149.\n",
      "Iteration 5660: Policy loss: -0.073536. Value loss: 0.044421. Entropy: 0.294469.\n",
      "Iteration 5661: Policy loss: -0.077315. Value loss: 0.034243. Entropy: 0.293929.\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5662: Policy loss: 0.039979. Value loss: 0.098467. Entropy: 0.304877.\n",
      "Iteration 5663: Policy loss: 0.036005. Value loss: 0.039507. Entropy: 0.303948.\n",
      "Iteration 5664: Policy loss: 0.028033. Value loss: 0.029917. Entropy: 0.303522.\n",
      "episode: 2340   score: 625.0  epsilon: 1.0    steps: 16  evaluation reward: 359.15\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5665: Policy loss: -0.397143. Value loss: 0.187142. Entropy: 0.297377.\n",
      "Iteration 5666: Policy loss: -0.395488. Value loss: 0.047698. Entropy: 0.296355.\n",
      "Iteration 5667: Policy loss: -0.425342. Value loss: 0.034847. Entropy: 0.297861.\n",
      "episode: 2341   score: 360.0  epsilon: 1.0    steps: 400  evaluation reward: 358.15\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5668: Policy loss: -0.060715. Value loss: 0.270038. Entropy: 0.297589.\n",
      "Iteration 5669: Policy loss: -0.071748. Value loss: 0.125110. Entropy: 0.298769.\n",
      "Iteration 5670: Policy loss: -0.097868. Value loss: 0.058038. Entropy: 0.298821.\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5671: Policy loss: 0.198649. Value loss: 0.206613. Entropy: 0.305056.\n",
      "Iteration 5672: Policy loss: 0.188337. Value loss: 0.079430. Entropy: 0.304219.\n",
      "Iteration 5673: Policy loss: 0.177399. Value loss: 0.050358. Entropy: 0.304045.\n",
      "episode: 2342   score: 225.0  epsilon: 1.0    steps: 64  evaluation reward: 354.75\n",
      "episode: 2343   score: 290.0  epsilon: 1.0    steps: 272  evaluation reward: 355.1\n",
      "episode: 2344   score: 425.0  epsilon: 1.0    steps: 800  evaluation reward: 355.7\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5674: Policy loss: 0.116636. Value loss: 0.188492. Entropy: 0.271702.\n",
      "Iteration 5675: Policy loss: 0.112928. Value loss: 0.065949. Entropy: 0.272390.\n",
      "Iteration 5676: Policy loss: 0.103303. Value loss: 0.044342. Entropy: 0.271558.\n",
      "episode: 2345   score: 345.0  epsilon: 1.0    steps: 1008  evaluation reward: 356.75\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5677: Policy loss: 0.094591. Value loss: 0.125982. Entropy: 0.303916.\n",
      "Iteration 5678: Policy loss: 0.084483. Value loss: 0.054106. Entropy: 0.304181.\n",
      "Iteration 5679: Policy loss: 0.074637. Value loss: 0.034898. Entropy: 0.303425.\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5680: Policy loss: -0.327662. Value loss: 0.248016. Entropy: 0.292499.\n",
      "Iteration 5681: Policy loss: -0.343157. Value loss: 0.079686. Entropy: 0.293663.\n",
      "Iteration 5682: Policy loss: -0.359707. Value loss: 0.052412. Entropy: 0.293573.\n",
      "episode: 2346   score: 640.0  epsilon: 1.0    steps: 872  evaluation reward: 361.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5683: Policy loss: 0.023746. Value loss: 0.198452. Entropy: 0.302599.\n",
      "Iteration 5684: Policy loss: 0.039559. Value loss: 0.079856. Entropy: 0.300590.\n",
      "Iteration 5685: Policy loss: 0.029241. Value loss: 0.057344. Entropy: 0.300118.\n",
      "episode: 2347   score: 635.0  epsilon: 1.0    steps: 264  evaluation reward: 363.75\n",
      "episode: 2348   score: 145.0  epsilon: 1.0    steps: 368  evaluation reward: 358.25\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5686: Policy loss: 0.060547. Value loss: 0.051985. Entropy: 0.274781.\n",
      "Iteration 5687: Policy loss: 0.054979. Value loss: 0.025742. Entropy: 0.274112.\n",
      "Iteration 5688: Policy loss: 0.053362. Value loss: 0.019459. Entropy: 0.272364.\n",
      "episode: 2349   score: 290.0  epsilon: 1.0    steps: 960  evaluation reward: 359.05\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5689: Policy loss: 0.105015. Value loss: 0.169849. Entropy: 0.307340.\n",
      "Iteration 5690: Policy loss: 0.095014. Value loss: 0.056347. Entropy: 0.307636.\n",
      "Iteration 5691: Policy loss: 0.086557. Value loss: 0.033925. Entropy: 0.307835.\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5692: Policy loss: -0.163879. Value loss: 0.182307. Entropy: 0.298429.\n",
      "Iteration 5693: Policy loss: -0.182918. Value loss: 0.055768. Entropy: 0.297914.\n",
      "Iteration 5694: Policy loss: -0.184552. Value loss: 0.030975. Entropy: 0.297957.\n",
      "episode: 2350   score: 180.0  epsilon: 1.0    steps: 152  evaluation reward: 357.15\n",
      "now time :  2019-09-05 20:08:41.963000\n",
      "episode: 2351   score: 590.0  epsilon: 1.0    steps: 1000  evaluation reward: 358.45\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5695: Policy loss: -0.496508. Value loss: 0.256477. Entropy: 0.295853.\n",
      "Iteration 5696: Policy loss: -0.483016. Value loss: 0.085849. Entropy: 0.297016.\n",
      "Iteration 5697: Policy loss: -0.505421. Value loss: 0.056164. Entropy: 0.297965.\n",
      "episode: 2352   score: 590.0  epsilon: 1.0    steps: 80  evaluation reward: 357.7\n",
      "episode: 2353   score: 275.0  epsilon: 1.0    steps: 832  evaluation reward: 359.45\n",
      "Training network. lr: 0.000207. clip: 0.082646\n",
      "Iteration 5698: Policy loss: 0.082093. Value loss: 0.213254. Entropy: 0.273519.\n",
      "Iteration 5699: Policy loss: 0.068236. Value loss: 0.076998. Entropy: 0.273158.\n",
      "Iteration 5700: Policy loss: 0.061992. Value loss: 0.058252. Entropy: 0.276282.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5701: Policy loss: 0.042021. Value loss: 0.185356. Entropy: 0.306662.\n",
      "Iteration 5702: Policy loss: 0.043974. Value loss: 0.067477. Entropy: 0.305944.\n",
      "Iteration 5703: Policy loss: 0.027171. Value loss: 0.039618. Entropy: 0.305681.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5704: Policy loss: 0.300135. Value loss: 0.124075. Entropy: 0.309377.\n",
      "Iteration 5705: Policy loss: 0.286387. Value loss: 0.046519. Entropy: 0.307895.\n",
      "Iteration 5706: Policy loss: 0.280850. Value loss: 0.032838. Entropy: 0.307727.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5707: Policy loss: 0.018007. Value loss: 0.137718. Entropy: 0.302503.\n",
      "Iteration 5708: Policy loss: -0.003732. Value loss: 0.058316. Entropy: 0.301788.\n",
      "Iteration 5709: Policy loss: -0.001511. Value loss: 0.035444. Entropy: 0.304571.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5710: Policy loss: 0.067577. Value loss: 0.093030. Entropy: 0.306070.\n",
      "Iteration 5711: Policy loss: 0.057615. Value loss: 0.037413. Entropy: 0.307004.\n",
      "Iteration 5712: Policy loss: 0.061140. Value loss: 0.025005. Entropy: 0.305584.\n",
      "episode: 2354   score: 535.0  epsilon: 1.0    steps: 184  evaluation reward: 360.85\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5713: Policy loss: 0.057102. Value loss: 0.087108. Entropy: 0.303306.\n",
      "Iteration 5714: Policy loss: 0.062609. Value loss: 0.050701. Entropy: 0.301167.\n",
      "Iteration 5715: Policy loss: 0.050842. Value loss: 0.041964. Entropy: 0.295560.\n",
      "episode: 2355   score: 520.0  epsilon: 1.0    steps: 496  evaluation reward: 361.95\n",
      "episode: 2356   score: 470.0  epsilon: 1.0    steps: 712  evaluation reward: 363.35\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5716: Policy loss: -0.102333. Value loss: 0.227830. Entropy: 0.284642.\n",
      "Iteration 5717: Policy loss: -0.099140. Value loss: 0.090906. Entropy: 0.284557.\n",
      "Iteration 5718: Policy loss: -0.111949. Value loss: 0.059246. Entropy: 0.286418.\n",
      "episode: 2357   score: 240.0  epsilon: 1.0    steps: 320  evaluation reward: 363.6\n",
      "episode: 2358   score: 280.0  epsilon: 1.0    steps: 744  evaluation reward: 364.3\n",
      "episode: 2359   score: 290.0  epsilon: 1.0    steps: 816  evaluation reward: 365.4\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5719: Policy loss: 0.198317. Value loss: 0.108282. Entropy: 0.280657.\n",
      "Iteration 5720: Policy loss: 0.192100. Value loss: 0.049233. Entropy: 0.280843.\n",
      "Iteration 5721: Policy loss: 0.187671. Value loss: 0.035005. Entropy: 0.279897.\n",
      "episode: 2360   score: 620.0  epsilon: 1.0    steps: 264  evaluation reward: 368.5\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5722: Policy loss: 0.072313. Value loss: 0.152677. Entropy: 0.294969.\n",
      "Iteration 5723: Policy loss: 0.055777. Value loss: 0.055425. Entropy: 0.295050.\n",
      "Iteration 5724: Policy loss: 0.058438. Value loss: 0.039877. Entropy: 0.292162.\n",
      "episode: 2361   score: 620.0  epsilon: 1.0    steps: 248  evaluation reward: 371.8\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5725: Policy loss: -0.003788. Value loss: 0.089534. Entropy: 0.296308.\n",
      "Iteration 5726: Policy loss: -0.015179. Value loss: 0.039027. Entropy: 0.296654.\n",
      "Iteration 5727: Policy loss: -0.013707. Value loss: 0.025406. Entropy: 0.296179.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5728: Policy loss: 0.033812. Value loss: 0.043091. Entropy: 0.310659.\n",
      "Iteration 5729: Policy loss: 0.030848. Value loss: 0.018454. Entropy: 0.311504.\n",
      "Iteration 5730: Policy loss: 0.029264. Value loss: 0.015324. Entropy: 0.311357.\n",
      "episode: 2362   score: 225.0  epsilon: 1.0    steps: 688  evaluation reward: 371.15\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5731: Policy loss: 0.006725. Value loss: 0.249685. Entropy: 0.296166.\n",
      "Iteration 5732: Policy loss: -0.005826. Value loss: 0.071671. Entropy: 0.298062.\n",
      "Iteration 5733: Policy loss: 0.004724. Value loss: 0.042484. Entropy: 0.297680.\n",
      "episode: 2363   score: 185.0  epsilon: 1.0    steps: 56  evaluation reward: 366.5\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5734: Policy loss: 0.120450. Value loss: 0.129543. Entropy: 0.298811.\n",
      "Iteration 5735: Policy loss: 0.128394. Value loss: 0.065415. Entropy: 0.296928.\n",
      "Iteration 5736: Policy loss: 0.112027. Value loss: 0.050586. Entropy: 0.296791.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5737: Policy loss: 0.055554. Value loss: 0.188957. Entropy: 0.307972.\n",
      "Iteration 5738: Policy loss: 0.043074. Value loss: 0.096252. Entropy: 0.306316.\n",
      "Iteration 5739: Policy loss: 0.041682. Value loss: 0.070967. Entropy: 0.305991.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5740: Policy loss: -0.070219. Value loss: 0.137541. Entropy: 0.307365.\n",
      "Iteration 5741: Policy loss: -0.078397. Value loss: 0.072516. Entropy: 0.308060.\n",
      "Iteration 5742: Policy loss: -0.087629. Value loss: 0.053081. Entropy: 0.308166.\n",
      "episode: 2364   score: 360.0  epsilon: 1.0    steps: 320  evaluation reward: 367.45\n",
      "episode: 2365   score: 470.0  epsilon: 1.0    steps: 528  evaluation reward: 369.6\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5743: Policy loss: -0.245674. Value loss: 0.328938. Entropy: 0.289266.\n",
      "Iteration 5744: Policy loss: -0.254813. Value loss: 0.121640. Entropy: 0.288674.\n",
      "Iteration 5745: Policy loss: -0.270357. Value loss: 0.084299. Entropy: 0.292048.\n",
      "episode: 2366   score: 315.0  epsilon: 1.0    steps: 120  evaluation reward: 369.9\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5746: Policy loss: -0.366034. Value loss: 0.291415. Entropy: 0.300918.\n",
      "Iteration 5747: Policy loss: -0.381450. Value loss: 0.151976. Entropy: 0.302391.\n",
      "Iteration 5748: Policy loss: -0.385132. Value loss: 0.065352. Entropy: 0.302731.\n",
      "Training network. lr: 0.000206. clip: 0.082499\n",
      "Iteration 5749: Policy loss: -0.155842. Value loss: 0.185084. Entropy: 0.308826.\n",
      "Iteration 5750: Policy loss: -0.164963. Value loss: 0.086033. Entropy: 0.309162.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5751: Policy loss: -0.178905. Value loss: 0.050702. Entropy: 0.309861.\n",
      "episode: 2367   score: 435.0  epsilon: 1.0    steps: 264  evaluation reward: 371.65\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5752: Policy loss: 0.485267. Value loss: 0.214746. Entropy: 0.296796.\n",
      "Iteration 5753: Policy loss: 0.446147. Value loss: 0.067843. Entropy: 0.297099.\n",
      "Iteration 5754: Policy loss: 0.445767. Value loss: 0.041592. Entropy: 0.297478.\n",
      "episode: 2368   score: 525.0  epsilon: 1.0    steps: 192  evaluation reward: 373.45\n",
      "episode: 2369   score: 655.0  epsilon: 1.0    steps: 600  evaluation reward: 375.0\n",
      "episode: 2370   score: 345.0  epsilon: 1.0    steps: 864  evaluation reward: 375.6\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5755: Policy loss: 0.199180. Value loss: 0.111402. Entropy: 0.287040.\n",
      "Iteration 5756: Policy loss: 0.186574. Value loss: 0.049994. Entropy: 0.288049.\n",
      "Iteration 5757: Policy loss: 0.183143. Value loss: 0.032540. Entropy: 0.289567.\n",
      "episode: 2371   score: 620.0  epsilon: 1.0    steps: 720  evaluation reward: 379.2\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5758: Policy loss: -0.205212. Value loss: 0.322489. Entropy: 0.301317.\n",
      "Iteration 5759: Policy loss: -0.214537. Value loss: 0.189451. Entropy: 0.299596.\n",
      "Iteration 5760: Policy loss: -0.228847. Value loss: 0.139792. Entropy: 0.300417.\n",
      "episode: 2372   score: 135.0  epsilon: 1.0    steps: 976  evaluation reward: 375.35\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5761: Policy loss: -0.091372. Value loss: 0.340729. Entropy: 0.310352.\n",
      "Iteration 5762: Policy loss: -0.095841. Value loss: 0.206573. Entropy: 0.309700.\n",
      "Iteration 5763: Policy loss: -0.077225. Value loss: 0.128525. Entropy: 0.308420.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5764: Policy loss: -0.139585. Value loss: 0.156042. Entropy: 0.304306.\n",
      "Iteration 5765: Policy loss: -0.151958. Value loss: 0.050596. Entropy: 0.302977.\n",
      "Iteration 5766: Policy loss: -0.155497. Value loss: 0.033150. Entropy: 0.302412.\n",
      "episode: 2373   score: 360.0  epsilon: 1.0    steps: 864  evaluation reward: 377.4\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5767: Policy loss: 0.227415. Value loss: 0.347802. Entropy: 0.301077.\n",
      "Iteration 5768: Policy loss: 0.202362. Value loss: 0.117613. Entropy: 0.300679.\n",
      "Iteration 5769: Policy loss: 0.178262. Value loss: 0.076517. Entropy: 0.300700.\n",
      "episode: 2374   score: 545.0  epsilon: 1.0    steps: 272  evaluation reward: 380.8\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5770: Policy loss: 0.035429. Value loss: 0.107539. Entropy: 0.293045.\n",
      "Iteration 5771: Policy loss: 0.024638. Value loss: 0.035329. Entropy: 0.292174.\n",
      "Iteration 5772: Policy loss: 0.028316. Value loss: 0.025626. Entropy: 0.290254.\n",
      "episode: 2375   score: 260.0  epsilon: 1.0    steps: 768  evaluation reward: 378.75\n",
      "episode: 2376   score: 620.0  epsilon: 1.0    steps: 1016  evaluation reward: 382.15\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5773: Policy loss: 0.177056. Value loss: 0.129349. Entropy: 0.297362.\n",
      "Iteration 5774: Policy loss: 0.180771. Value loss: 0.042164. Entropy: 0.296560.\n",
      "Iteration 5775: Policy loss: 0.166077. Value loss: 0.028090. Entropy: 0.296545.\n",
      "episode: 2377   score: 260.0  epsilon: 1.0    steps: 920  evaluation reward: 382.5\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5776: Policy loss: 0.208815. Value loss: 0.152100. Entropy: 0.293515.\n",
      "Iteration 5777: Policy loss: 0.202580. Value loss: 0.056087. Entropy: 0.293795.\n",
      "Iteration 5778: Policy loss: 0.206511. Value loss: 0.039901. Entropy: 0.292899.\n",
      "episode: 2378   score: 345.0  epsilon: 1.0    steps: 304  evaluation reward: 383.85\n",
      "episode: 2379   score: 210.0  epsilon: 1.0    steps: 936  evaluation reward: 383.85\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5779: Policy loss: 0.234913. Value loss: 0.077992. Entropy: 0.287530.\n",
      "Iteration 5780: Policy loss: 0.222443. Value loss: 0.034186. Entropy: 0.285872.\n",
      "Iteration 5781: Policy loss: 0.220244. Value loss: 0.026841. Entropy: 0.283857.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5782: Policy loss: 0.072203. Value loss: 0.124475. Entropy: 0.300992.\n",
      "Iteration 5783: Policy loss: 0.060268. Value loss: 0.060449. Entropy: 0.301047.\n",
      "Iteration 5784: Policy loss: 0.060753. Value loss: 0.044425. Entropy: 0.300616.\n",
      "episode: 2380   score: 265.0  epsilon: 1.0    steps: 16  evaluation reward: 384.4\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5785: Policy loss: -0.085436. Value loss: 0.065810. Entropy: 0.294714.\n",
      "Iteration 5786: Policy loss: -0.088046. Value loss: 0.034874. Entropy: 0.294445.\n",
      "Iteration 5787: Policy loss: -0.093796. Value loss: 0.026540. Entropy: 0.295091.\n",
      "episode: 2381   score: 265.0  epsilon: 1.0    steps: 936  evaluation reward: 383.7\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5788: Policy loss: -0.265513. Value loss: 0.277117. Entropy: 0.300311.\n",
      "Iteration 5789: Policy loss: -0.266724. Value loss: 0.128678. Entropy: 0.299924.\n",
      "Iteration 5790: Policy loss: -0.273049. Value loss: 0.068264. Entropy: 0.300539.\n",
      "episode: 2382   score: 235.0  epsilon: 1.0    steps: 24  evaluation reward: 382.15\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5791: Policy loss: 0.073250. Value loss: 0.064186. Entropy: 0.284760.\n",
      "Iteration 5792: Policy loss: 0.071844. Value loss: 0.035679. Entropy: 0.282372.\n",
      "Iteration 5793: Policy loss: 0.071622. Value loss: 0.027682. Entropy: 0.283571.\n",
      "episode: 2383   score: 245.0  epsilon: 1.0    steps: 384  evaluation reward: 381.45\n",
      "episode: 2384   score: 210.0  epsilon: 1.0    steps: 616  evaluation reward: 376.65\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5794: Policy loss: -0.066046. Value loss: 0.357735. Entropy: 0.282714.\n",
      "Iteration 5795: Policy loss: -0.102658. Value loss: 0.117342. Entropy: 0.283613.\n",
      "Iteration 5796: Policy loss: -0.090333. Value loss: 0.067424. Entropy: 0.283025.\n",
      "episode: 2385   score: 210.0  epsilon: 1.0    steps: 288  evaluation reward: 374.55\n",
      "episode: 2386   score: 460.0  epsilon: 1.0    steps: 832  evaluation reward: 377.35\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5797: Policy loss: 0.030903. Value loss: 0.084799. Entropy: 0.287467.\n",
      "Iteration 5798: Policy loss: 0.029091. Value loss: 0.044454. Entropy: 0.287158.\n",
      "Iteration 5799: Policy loss: 0.023748. Value loss: 0.035378. Entropy: 0.285453.\n",
      "Training network. lr: 0.000206. clip: 0.082342\n",
      "Iteration 5800: Policy loss: 0.044145. Value loss: 0.097878. Entropy: 0.304292.\n",
      "Iteration 5801: Policy loss: 0.036421. Value loss: 0.042923. Entropy: 0.303318.\n",
      "Iteration 5802: Policy loss: 0.020830. Value loss: 0.030297. Entropy: 0.302954.\n",
      "episode: 2387   score: 580.0  epsilon: 1.0    steps: 480  evaluation reward: 381.15\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5803: Policy loss: -0.129708. Value loss: 0.117478. Entropy: 0.288602.\n",
      "Iteration 5804: Policy loss: -0.133573. Value loss: 0.035863. Entropy: 0.291769.\n",
      "Iteration 5805: Policy loss: -0.139583. Value loss: 0.024647. Entropy: 0.289968.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5806: Policy loss: -0.006474. Value loss: 0.317071. Entropy: 0.304748.\n",
      "Iteration 5807: Policy loss: -0.016128. Value loss: 0.114443. Entropy: 0.303167.\n",
      "Iteration 5808: Policy loss: -0.036341. Value loss: 0.078374. Entropy: 0.303910.\n",
      "episode: 2388   score: 390.0  epsilon: 1.0    steps: 352  evaluation reward: 380.85\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5809: Policy loss: -0.009119. Value loss: 0.089124. Entropy: 0.290344.\n",
      "Iteration 5810: Policy loss: -0.015232. Value loss: 0.043368. Entropy: 0.290431.\n",
      "Iteration 5811: Policy loss: -0.017029. Value loss: 0.032774. Entropy: 0.290629.\n",
      "episode: 2389   score: 210.0  epsilon: 1.0    steps: 672  evaluation reward: 374.75\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5812: Policy loss: -0.066115. Value loss: 0.325836. Entropy: 0.294287.\n",
      "Iteration 5813: Policy loss: -0.066996. Value loss: 0.151944. Entropy: 0.293402.\n",
      "Iteration 5814: Policy loss: -0.088601. Value loss: 0.102601. Entropy: 0.293371.\n",
      "episode: 2390   score: 300.0  epsilon: 1.0    steps: 208  evaluation reward: 375.05\n",
      "episode: 2391   score: 275.0  epsilon: 1.0    steps: 464  evaluation reward: 374.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5815: Policy loss: 0.169607. Value loss: 0.159754. Entropy: 0.280016.\n",
      "Iteration 5816: Policy loss: 0.160311. Value loss: 0.065905. Entropy: 0.278981.\n",
      "Iteration 5817: Policy loss: 0.151495. Value loss: 0.047828. Entropy: 0.278436.\n",
      "episode: 2392   score: 400.0  epsilon: 1.0    steps: 304  evaluation reward: 376.25\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5818: Policy loss: 0.070557. Value loss: 0.134830. Entropy: 0.293721.\n",
      "Iteration 5819: Policy loss: 0.064280. Value loss: 0.045064. Entropy: 0.293709.\n",
      "Iteration 5820: Policy loss: 0.054434. Value loss: 0.032699. Entropy: 0.294053.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5821: Policy loss: 0.040008. Value loss: 0.103036. Entropy: 0.304008.\n",
      "Iteration 5822: Policy loss: 0.031202. Value loss: 0.041247. Entropy: 0.303815.\n",
      "Iteration 5823: Policy loss: 0.027272. Value loss: 0.030454. Entropy: 0.303584.\n",
      "episode: 2393   score: 425.0  epsilon: 1.0    steps: 720  evaluation reward: 376.25\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5824: Policy loss: 0.140179. Value loss: 0.110053. Entropy: 0.295366.\n",
      "Iteration 5825: Policy loss: 0.135093. Value loss: 0.029500. Entropy: 0.292243.\n",
      "Iteration 5826: Policy loss: 0.127355. Value loss: 0.020287. Entropy: 0.292870.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5827: Policy loss: 0.024264. Value loss: 0.327749. Entropy: 0.304793.\n",
      "Iteration 5828: Policy loss: 0.034987. Value loss: 0.083090. Entropy: 0.302192.\n",
      "Iteration 5829: Policy loss: 0.020199. Value loss: 0.041746. Entropy: 0.303245.\n",
      "episode: 2394   score: 300.0  epsilon: 1.0    steps: 552  evaluation reward: 374.45\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5830: Policy loss: 0.135408. Value loss: 0.212698. Entropy: 0.290750.\n",
      "Iteration 5831: Policy loss: 0.123339. Value loss: 0.066399. Entropy: 0.290365.\n",
      "Iteration 5832: Policy loss: 0.126352. Value loss: 0.042170. Entropy: 0.290918.\n",
      "episode: 2395   score: 670.0  epsilon: 1.0    steps: 112  evaluation reward: 376.95\n",
      "episode: 2396   score: 835.0  epsilon: 1.0    steps: 288  evaluation reward: 382.75\n",
      "episode: 2397   score: 265.0  epsilon: 1.0    steps: 952  evaluation reward: 382.25\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5833: Policy loss: 0.188908. Value loss: 0.165581. Entropy: 0.274443.\n",
      "Iteration 5834: Policy loss: 0.172092. Value loss: 0.070577. Entropy: 0.272830.\n",
      "Iteration 5835: Policy loss: 0.174143. Value loss: 0.045514. Entropy: 0.273398.\n",
      "episode: 2398   score: 210.0  epsilon: 1.0    steps: 664  evaluation reward: 381.0\n",
      "episode: 2399   score: 125.0  epsilon: 1.0    steps: 704  evaluation reward: 377.45\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5836: Policy loss: 0.009309. Value loss: 0.145594. Entropy: 0.269719.\n",
      "Iteration 5837: Policy loss: -0.001739. Value loss: 0.063919. Entropy: 0.271277.\n",
      "Iteration 5838: Policy loss: -0.007793. Value loss: 0.041636. Entropy: 0.270578.\n",
      "episode: 2400   score: 290.0  epsilon: 1.0    steps: 336  evaluation reward: 374.7\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5839: Policy loss: -0.058040. Value loss: 0.143932. Entropy: 0.294479.\n",
      "Iteration 5840: Policy loss: -0.071187. Value loss: 0.056290. Entropy: 0.293087.\n",
      "Iteration 5841: Policy loss: -0.075763. Value loss: 0.040187. Entropy: 0.292403.\n",
      "now time :  2019-09-05 20:17:47.566303\n",
      "episode: 2401   score: 20.0  epsilon: 1.0    steps: 680  evaluation reward: 372.4\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5842: Policy loss: -0.168143. Value loss: 0.233559. Entropy: 0.291422.\n",
      "Iteration 5843: Policy loss: -0.170019. Value loss: 0.137269. Entropy: 0.290323.\n",
      "Iteration 5844: Policy loss: -0.200217. Value loss: 0.102888. Entropy: 0.291501.\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5845: Policy loss: -0.023671. Value loss: 0.106312. Entropy: 0.306805.\n",
      "Iteration 5846: Policy loss: -0.031762. Value loss: 0.039659. Entropy: 0.306702.\n",
      "Iteration 5847: Policy loss: -0.032438. Value loss: 0.026539. Entropy: 0.305177.\n",
      "episode: 2402   score: 390.0  epsilon: 1.0    steps: 912  evaluation reward: 372.5\n",
      "Training network. lr: 0.000205. clip: 0.082185\n",
      "Iteration 5848: Policy loss: 0.330812. Value loss: 0.212585. Entropy: 0.298347.\n",
      "Iteration 5849: Policy loss: 0.315758. Value loss: 0.071170. Entropy: 0.296733.\n",
      "Iteration 5850: Policy loss: 0.300425. Value loss: 0.049264. Entropy: 0.296834.\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5851: Policy loss: 0.228896. Value loss: 0.089679. Entropy: 0.298536.\n",
      "Iteration 5852: Policy loss: 0.219236. Value loss: 0.031676. Entropy: 0.297834.\n",
      "Iteration 5853: Policy loss: 0.211670. Value loss: 0.021741. Entropy: 0.297118.\n",
      "episode: 2403   score: 270.0  epsilon: 1.0    steps: 752  evaluation reward: 370.05\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5854: Policy loss: 0.142365. Value loss: 0.052068. Entropy: 0.294269.\n",
      "Iteration 5855: Policy loss: 0.139358. Value loss: 0.019597. Entropy: 0.295575.\n",
      "Iteration 5856: Policy loss: 0.136952. Value loss: 0.012401. Entropy: 0.294717.\n",
      "episode: 2404   score: 310.0  epsilon: 1.0    steps: 40  evaluation reward: 368.15\n",
      "episode: 2405   score: 430.0  epsilon: 1.0    steps: 704  evaluation reward: 369.85\n",
      "episode: 2406   score: 225.0  epsilon: 1.0    steps: 1000  evaluation reward: 369.15\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5857: Policy loss: -0.159008. Value loss: 0.131411. Entropy: 0.276484.\n",
      "Iteration 5858: Policy loss: -0.172468. Value loss: 0.051999. Entropy: 0.278555.\n",
      "Iteration 5859: Policy loss: -0.172092. Value loss: 0.037218. Entropy: 0.276254.\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5860: Policy loss: -0.123190. Value loss: 0.076924. Entropy: 0.292981.\n",
      "Iteration 5861: Policy loss: -0.124401. Value loss: 0.033712. Entropy: 0.293682.\n",
      "Iteration 5862: Policy loss: -0.129728. Value loss: 0.024229. Entropy: 0.293293.\n",
      "episode: 2407   score: 320.0  epsilon: 1.0    steps: 248  evaluation reward: 367.75\n",
      "episode: 2408   score: 390.0  epsilon: 1.0    steps: 784  evaluation reward: 369.25\n",
      "episode: 2409   score: 315.0  epsilon: 1.0    steps: 944  evaluation reward: 368.2\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5863: Policy loss: -0.140944. Value loss: 0.273151. Entropy: 0.280954.\n",
      "Iteration 5864: Policy loss: -0.140248. Value loss: 0.122324. Entropy: 0.280174.\n",
      "Iteration 5865: Policy loss: -0.160256. Value loss: 0.081656. Entropy: 0.279445.\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5866: Policy loss: 0.020778. Value loss: 0.132412. Entropy: 0.291332.\n",
      "Iteration 5867: Policy loss: 0.015466. Value loss: 0.039541. Entropy: 0.287995.\n",
      "Iteration 5868: Policy loss: 0.008414. Value loss: 0.022308. Entropy: 0.289340.\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5869: Policy loss: 0.292134. Value loss: 0.113644. Entropy: 0.305918.\n",
      "Iteration 5870: Policy loss: 0.288749. Value loss: 0.057453. Entropy: 0.303468.\n",
      "Iteration 5871: Policy loss: 0.288110. Value loss: 0.045383. Entropy: 0.303097.\n",
      "episode: 2410   score: 270.0  epsilon: 1.0    steps: 248  evaluation reward: 367.45\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5872: Policy loss: 0.213809. Value loss: 0.084535. Entropy: 0.290787.\n",
      "Iteration 5873: Policy loss: 0.210381. Value loss: 0.035279. Entropy: 0.289793.\n",
      "Iteration 5874: Policy loss: 0.204369. Value loss: 0.023746. Entropy: 0.290136.\n",
      "episode: 2411   score: 195.0  epsilon: 1.0    steps: 424  evaluation reward: 365.2\n",
      "episode: 2412   score: 410.0  epsilon: 1.0    steps: 904  evaluation reward: 366.6\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5875: Policy loss: 0.121829. Value loss: 0.079691. Entropy: 0.287733.\n",
      "Iteration 5876: Policy loss: 0.112204. Value loss: 0.033518. Entropy: 0.285937.\n",
      "Iteration 5877: Policy loss: 0.103754. Value loss: 0.023186. Entropy: 0.284869.\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5878: Policy loss: 0.059255. Value loss: 0.058943. Entropy: 0.298338.\n",
      "Iteration 5879: Policy loss: 0.056352. Value loss: 0.022743. Entropy: 0.298308.\n",
      "Iteration 5880: Policy loss: 0.052618. Value loss: 0.016731. Entropy: 0.297935.\n",
      "episode: 2413   score: 240.0  epsilon: 1.0    steps: 496  evaluation reward: 366.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5881: Policy loss: -0.021835. Value loss: 0.098555. Entropy: 0.294413.\n",
      "Iteration 5882: Policy loss: -0.035719. Value loss: 0.035983. Entropy: 0.295841.\n",
      "Iteration 5883: Policy loss: -0.035205. Value loss: 0.024865. Entropy: 0.295903.\n",
      "episode: 2414   score: 285.0  epsilon: 1.0    steps: 496  evaluation reward: 363.25\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5884: Policy loss: -0.054431. Value loss: 0.063986. Entropy: 0.296116.\n",
      "Iteration 5885: Policy loss: -0.063349. Value loss: 0.026243. Entropy: 0.298432.\n",
      "Iteration 5886: Policy loss: -0.060469. Value loss: 0.016264. Entropy: 0.300052.\n",
      "episode: 2415   score: 310.0  epsilon: 1.0    steps: 72  evaluation reward: 363.5\n",
      "episode: 2416   score: 240.0  epsilon: 1.0    steps: 872  evaluation reward: 362.75\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5887: Policy loss: -0.282935. Value loss: 0.323206. Entropy: 0.287376.\n",
      "Iteration 5888: Policy loss: -0.290546. Value loss: 0.122659. Entropy: 0.283385.\n",
      "Iteration 5889: Policy loss: -0.293386. Value loss: 0.071359. Entropy: 0.283685.\n",
      "episode: 2417   score: 565.0  epsilon: 1.0    steps: 880  evaluation reward: 365.75\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5890: Policy loss: -0.053931. Value loss: 0.142756. Entropy: 0.299466.\n",
      "Iteration 5891: Policy loss: -0.057445. Value loss: 0.062382. Entropy: 0.298046.\n",
      "Iteration 5892: Policy loss: -0.065818. Value loss: 0.043279. Entropy: 0.298179.\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5893: Policy loss: 0.182119. Value loss: 0.118310. Entropy: 0.303884.\n",
      "Iteration 5894: Policy loss: 0.169115. Value loss: 0.046283. Entropy: 0.302719.\n",
      "Iteration 5895: Policy loss: 0.157452. Value loss: 0.035156. Entropy: 0.301785.\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5896: Policy loss: 0.204449. Value loss: 0.105225. Entropy: 0.305265.\n",
      "Iteration 5897: Policy loss: 0.193434. Value loss: 0.046204. Entropy: 0.305146.\n",
      "Iteration 5898: Policy loss: 0.189938. Value loss: 0.035354. Entropy: 0.303927.\n",
      "episode: 2418   score: 320.0  epsilon: 1.0    steps: 176  evaluation reward: 364.75\n",
      "episode: 2419   score: 270.0  epsilon: 1.0    steps: 320  evaluation reward: 364.85\n",
      "episode: 2420   score: 145.0  epsilon: 1.0    steps: 872  evaluation reward: 361.15\n",
      "Training network. lr: 0.000205. clip: 0.082038\n",
      "Iteration 5899: Policy loss: 0.156762. Value loss: 0.135708. Entropy: 0.273120.\n",
      "Iteration 5900: Policy loss: 0.144014. Value loss: 0.055942. Entropy: 0.274733.\n",
      "Iteration 5901: Policy loss: 0.131346. Value loss: 0.035307. Entropy: 0.274551.\n",
      "episode: 2421   score: 395.0  epsilon: 1.0    steps: 224  evaluation reward: 362.7\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5902: Policy loss: -0.075671. Value loss: 0.120868. Entropy: 0.286918.\n",
      "Iteration 5903: Policy loss: -0.080838. Value loss: 0.053013. Entropy: 0.285985.\n",
      "Iteration 5904: Policy loss: -0.093750. Value loss: 0.039796. Entropy: 0.287021.\n",
      "episode: 2422   score: 195.0  epsilon: 1.0    steps: 120  evaluation reward: 363.3\n",
      "episode: 2423   score: 590.0  epsilon: 1.0    steps: 552  evaluation reward: 365.9\n",
      "episode: 2424   score: 155.0  epsilon: 1.0    steps: 584  evaluation reward: 362.35\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5905: Policy loss: 0.192177. Value loss: 0.105599. Entropy: 0.263681.\n",
      "Iteration 5906: Policy loss: 0.185431. Value loss: 0.047236. Entropy: 0.259876.\n",
      "Iteration 5907: Policy loss: 0.177952. Value loss: 0.033565. Entropy: 0.261740.\n",
      "episode: 2425   score: 90.0  epsilon: 1.0    steps: 888  evaluation reward: 361.1\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5908: Policy loss: 0.187176. Value loss: 0.163749. Entropy: 0.299542.\n",
      "Iteration 5909: Policy loss: 0.183404. Value loss: 0.064830. Entropy: 0.299105.\n",
      "Iteration 5910: Policy loss: 0.179264. Value loss: 0.042424. Entropy: 0.298169.\n",
      "episode: 2426   score: 325.0  epsilon: 1.0    steps: 168  evaluation reward: 359.35\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5911: Policy loss: -0.050985. Value loss: 0.126870. Entropy: 0.287494.\n",
      "Iteration 5912: Policy loss: -0.057795. Value loss: 0.062131. Entropy: 0.287232.\n",
      "Iteration 5913: Policy loss: -0.059311. Value loss: 0.039930. Entropy: 0.287373.\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5914: Policy loss: -0.068457. Value loss: 0.078872. Entropy: 0.301783.\n",
      "Iteration 5915: Policy loss: -0.077371. Value loss: 0.040520. Entropy: 0.302589.\n",
      "Iteration 5916: Policy loss: -0.085667. Value loss: 0.030143. Entropy: 0.301349.\n",
      "episode: 2427   score: 170.0  epsilon: 1.0    steps: 448  evaluation reward: 357.15\n",
      "episode: 2428   score: 90.0  epsilon: 1.0    steps: 648  evaluation reward: 356.6\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5917: Policy loss: -0.262408. Value loss: 0.308604. Entropy: 0.268409.\n",
      "Iteration 5918: Policy loss: -0.279426. Value loss: 0.104801. Entropy: 0.268679.\n",
      "Iteration 5919: Policy loss: -0.279271. Value loss: 0.040923. Entropy: 0.268699.\n",
      "episode: 2429   score: 240.0  epsilon: 1.0    steps: 344  evaluation reward: 354.8\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5920: Policy loss: -0.189792. Value loss: 0.322400. Entropy: 0.290525.\n",
      "Iteration 5921: Policy loss: -0.214425. Value loss: 0.169335. Entropy: 0.291570.\n",
      "Iteration 5922: Policy loss: -0.210768. Value loss: 0.102939. Entropy: 0.289389.\n",
      "episode: 2430   score: 360.0  epsilon: 1.0    steps: 608  evaluation reward: 355.55\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5923: Policy loss: 0.022343. Value loss: 0.109908. Entropy: 0.294591.\n",
      "Iteration 5924: Policy loss: 0.017081. Value loss: 0.040467. Entropy: 0.294591.\n",
      "Iteration 5925: Policy loss: 0.017592. Value loss: 0.029882. Entropy: 0.294999.\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5926: Policy loss: -0.073023. Value loss: 0.256222. Entropy: 0.304760.\n",
      "Iteration 5927: Policy loss: -0.079630. Value loss: 0.062222. Entropy: 0.305411.\n",
      "Iteration 5928: Policy loss: -0.102807. Value loss: 0.040432. Entropy: 0.305296.\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5929: Policy loss: 0.135353. Value loss: 0.079399. Entropy: 0.310746.\n",
      "Iteration 5930: Policy loss: 0.133601. Value loss: 0.037459. Entropy: 0.310032.\n",
      "Iteration 5931: Policy loss: 0.126318. Value loss: 0.027051. Entropy: 0.309170.\n",
      "episode: 2431   score: 390.0  epsilon: 1.0    steps: 464  evaluation reward: 355.95\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5932: Policy loss: -0.033107. Value loss: 0.261800. Entropy: 0.291648.\n",
      "Iteration 5933: Policy loss: -0.052412. Value loss: 0.078218. Entropy: 0.290762.\n",
      "Iteration 5934: Policy loss: -0.028316. Value loss: 0.057733. Entropy: 0.291405.\n",
      "episode: 2432   score: 595.0  epsilon: 1.0    steps: 608  evaluation reward: 359.25\n",
      "episode: 2433   score: 340.0  epsilon: 1.0    steps: 736  evaluation reward: 356.0\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5935: Policy loss: 0.282882. Value loss: 0.172923. Entropy: 0.274104.\n",
      "Iteration 5936: Policy loss: 0.281775. Value loss: 0.066712. Entropy: 0.273893.\n",
      "Iteration 5937: Policy loss: 0.269830. Value loss: 0.043345. Entropy: 0.274198.\n",
      "episode: 2434   score: 225.0  epsilon: 1.0    steps: 240  evaluation reward: 355.85\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5938: Policy loss: -0.133117. Value loss: 0.205975. Entropy: 0.296520.\n",
      "Iteration 5939: Policy loss: -0.149122. Value loss: 0.081499. Entropy: 0.295320.\n",
      "Iteration 5940: Policy loss: -0.148926. Value loss: 0.052421. Entropy: 0.295831.\n",
      "episode: 2435   score: 385.0  epsilon: 1.0    steps: 232  evaluation reward: 357.05\n",
      "episode: 2436   score: 625.0  epsilon: 1.0    steps: 904  evaluation reward: 360.45\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5941: Policy loss: 0.304166. Value loss: 0.103009. Entropy: 0.287811.\n",
      "Iteration 5942: Policy loss: 0.295155. Value loss: 0.038915. Entropy: 0.286642.\n",
      "Iteration 5943: Policy loss: 0.304407. Value loss: 0.029464. Entropy: 0.285891.\n",
      "episode: 2437   score: 345.0  epsilon: 1.0    steps: 16  evaluation reward: 356.25\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5944: Policy loss: -0.008447. Value loss: 0.098693. Entropy: 0.285825.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5945: Policy loss: -0.013752. Value loss: 0.041982. Entropy: 0.285748.\n",
      "Iteration 5946: Policy loss: -0.019191. Value loss: 0.029462. Entropy: 0.286674.\n",
      "episode: 2438   score: 335.0  epsilon: 1.0    steps: 96  evaluation reward: 354.15\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5947: Policy loss: -0.088685. Value loss: 0.091718. Entropy: 0.295082.\n",
      "Iteration 5948: Policy loss: -0.090417. Value loss: 0.047778. Entropy: 0.294771.\n",
      "Iteration 5949: Policy loss: -0.092056. Value loss: 0.033503. Entropy: 0.295230.\n",
      "Training network. lr: 0.000205. clip: 0.081881\n",
      "Iteration 5950: Policy loss: -0.119157. Value loss: 0.089439. Entropy: 0.313012.\n",
      "Iteration 5951: Policy loss: -0.124311. Value loss: 0.035884. Entropy: 0.312698.\n",
      "Iteration 5952: Policy loss: -0.133357. Value loss: 0.027394. Entropy: 0.313256.\n",
      "episode: 2439   score: 155.0  epsilon: 1.0    steps: 544  evaluation reward: 351.5\n",
      "episode: 2440   score: 210.0  epsilon: 1.0    steps: 728  evaluation reward: 347.35\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5953: Policy loss: 0.059163. Value loss: 0.118733. Entropy: 0.281956.\n",
      "Iteration 5954: Policy loss: 0.054802. Value loss: 0.051940. Entropy: 0.280682.\n",
      "Iteration 5955: Policy loss: 0.046758. Value loss: 0.037266. Entropy: 0.280219.\n",
      "episode: 2441   score: 270.0  epsilon: 1.0    steps: 496  evaluation reward: 346.45\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5956: Policy loss: 0.219213. Value loss: 0.107650. Entropy: 0.295611.\n",
      "Iteration 5957: Policy loss: 0.207728. Value loss: 0.041568. Entropy: 0.294188.\n",
      "Iteration 5958: Policy loss: 0.208881. Value loss: 0.029448. Entropy: 0.294701.\n",
      "episode: 2442   score: 285.0  epsilon: 1.0    steps: 512  evaluation reward: 347.05\n",
      "episode: 2443   score: 335.0  epsilon: 1.0    steps: 520  evaluation reward: 347.5\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5959: Policy loss: 0.245085. Value loss: 0.134875. Entropy: 0.280279.\n",
      "Iteration 5960: Policy loss: 0.241798. Value loss: 0.053237. Entropy: 0.281718.\n",
      "Iteration 5961: Policy loss: 0.227630. Value loss: 0.033004. Entropy: 0.281216.\n",
      "episode: 2444   score: 225.0  epsilon: 1.0    steps: 672  evaluation reward: 345.5\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5962: Policy loss: 0.168552. Value loss: 0.168061. Entropy: 0.291215.\n",
      "Iteration 5963: Policy loss: 0.162357. Value loss: 0.059802. Entropy: 0.290580.\n",
      "Iteration 5964: Policy loss: 0.160448. Value loss: 0.041416. Entropy: 0.289841.\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5965: Policy loss: 0.028128. Value loss: 0.095080. Entropy: 0.307061.\n",
      "Iteration 5966: Policy loss: 0.024745. Value loss: 0.046633. Entropy: 0.306753.\n",
      "Iteration 5967: Policy loss: 0.018294. Value loss: 0.034573. Entropy: 0.306968.\n",
      "episode: 2445   score: 330.0  epsilon: 1.0    steps: 568  evaluation reward: 345.35\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5968: Policy loss: 0.033892. Value loss: 0.062724. Entropy: 0.293633.\n",
      "Iteration 5969: Policy loss: 0.031128. Value loss: 0.036790. Entropy: 0.290238.\n",
      "Iteration 5970: Policy loss: 0.028522. Value loss: 0.031338. Entropy: 0.290865.\n",
      "episode: 2446   score: 185.0  epsilon: 1.0    steps: 896  evaluation reward: 340.8\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5971: Policy loss: -0.104227. Value loss: 0.067382. Entropy: 0.304309.\n",
      "Iteration 5972: Policy loss: -0.112143. Value loss: 0.026982. Entropy: 0.303473.\n",
      "Iteration 5973: Policy loss: -0.106821. Value loss: 0.020098. Entropy: 0.302100.\n",
      "episode: 2447   score: 285.0  epsilon: 1.0    steps: 520  evaluation reward: 337.3\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5974: Policy loss: -0.014752. Value loss: 0.076686. Entropy: 0.282780.\n",
      "Iteration 5975: Policy loss: -0.028729. Value loss: 0.038172. Entropy: 0.282018.\n",
      "Iteration 5976: Policy loss: -0.027146. Value loss: 0.027835. Entropy: 0.281276.\n",
      "episode: 2448   score: 390.0  epsilon: 1.0    steps: 712  evaluation reward: 339.75\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5977: Policy loss: -0.060834. Value loss: 0.105506. Entropy: 0.294835.\n",
      "Iteration 5978: Policy loss: -0.070411. Value loss: 0.045966. Entropy: 0.295663.\n",
      "Iteration 5979: Policy loss: -0.074172. Value loss: 0.032012. Entropy: 0.295711.\n",
      "episode: 2449   score: 285.0  epsilon: 1.0    steps: 392  evaluation reward: 339.7\n",
      "episode: 2450   score: 210.0  epsilon: 1.0    steps: 1000  evaluation reward: 340.0\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5980: Policy loss: 0.081513. Value loss: 0.058173. Entropy: 0.293214.\n",
      "Iteration 5981: Policy loss: 0.075844. Value loss: 0.030242. Entropy: 0.292135.\n",
      "Iteration 5982: Policy loss: 0.073119. Value loss: 0.022137. Entropy: 0.292601.\n",
      "now time :  2019-09-05 20:26:31.432049\n",
      "episode: 2451   score: 335.0  epsilon: 1.0    steps: 952  evaluation reward: 337.45\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5983: Policy loss: -0.206266. Value loss: 0.090087. Entropy: 0.296027.\n",
      "Iteration 5984: Policy loss: -0.209611. Value loss: 0.038523. Entropy: 0.296452.\n",
      "Iteration 5985: Policy loss: -0.217204. Value loss: 0.024752. Entropy: 0.296265.\n",
      "episode: 2452   score: 390.0  epsilon: 1.0    steps: 360  evaluation reward: 335.45\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5986: Policy loss: 0.124046. Value loss: 0.070248. Entropy: 0.285322.\n",
      "Iteration 5987: Policy loss: 0.118378. Value loss: 0.025454. Entropy: 0.285046.\n",
      "Iteration 5988: Policy loss: 0.113582. Value loss: 0.020805. Entropy: 0.284776.\n",
      "episode: 2453   score: 210.0  epsilon: 1.0    steps: 128  evaluation reward: 334.8\n",
      "episode: 2454   score: 210.0  epsilon: 1.0    steps: 352  evaluation reward: 331.55\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5989: Policy loss: -0.005341. Value loss: 0.044036. Entropy: 0.277074.\n",
      "Iteration 5990: Policy loss: -0.011425. Value loss: 0.024017. Entropy: 0.278099.\n",
      "Iteration 5991: Policy loss: -0.008293. Value loss: 0.019597. Entropy: 0.277833.\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5992: Policy loss: -0.111715. Value loss: 0.305746. Entropy: 0.307666.\n",
      "Iteration 5993: Policy loss: -0.124634. Value loss: 0.186007. Entropy: 0.305186.\n",
      "Iteration 5994: Policy loss: -0.144549. Value loss: 0.136846. Entropy: 0.305016.\n",
      "episode: 2455   score: 470.0  epsilon: 1.0    steps: 440  evaluation reward: 331.05\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5995: Policy loss: -0.125000. Value loss: 0.201590. Entropy: 0.292028.\n",
      "Iteration 5996: Policy loss: -0.122935. Value loss: 0.051983. Entropy: 0.292319.\n",
      "Iteration 5997: Policy loss: -0.130617. Value loss: 0.050816. Entropy: 0.291488.\n",
      "episode: 2456   score: 260.0  epsilon: 1.0    steps: 592  evaluation reward: 328.95\n",
      "episode: 2457   score: 210.0  epsilon: 1.0    steps: 624  evaluation reward: 328.65\n",
      "Training network. lr: 0.000204. clip: 0.081725\n",
      "Iteration 5998: Policy loss: 0.065769. Value loss: 0.077179. Entropy: 0.276494.\n",
      "Iteration 5999: Policy loss: 0.063505. Value loss: 0.031798. Entropy: 0.275344.\n",
      "Iteration 6000: Policy loss: 0.058792. Value loss: 0.025776. Entropy: 0.275369.\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6001: Policy loss: -0.098206. Value loss: 0.120935. Entropy: 0.304663.\n",
      "Iteration 6002: Policy loss: -0.110881. Value loss: 0.049691. Entropy: 0.305687.\n",
      "Iteration 6003: Policy loss: -0.099349. Value loss: 0.035313. Entropy: 0.304630.\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6004: Policy loss: 0.103821. Value loss: 0.156286. Entropy: 0.305194.\n",
      "Iteration 6005: Policy loss: 0.096827. Value loss: 0.060542. Entropy: 0.303789.\n",
      "Iteration 6006: Policy loss: 0.085017. Value loss: 0.046118. Entropy: 0.303071.\n",
      "episode: 2458   score: 490.0  epsilon: 1.0    steps: 280  evaluation reward: 330.75\n",
      "episode: 2459   score: 295.0  epsilon: 1.0    steps: 656  evaluation reward: 330.8\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6007: Policy loss: 0.122176. Value loss: 0.122880. Entropy: 0.279270.\n",
      "Iteration 6008: Policy loss: 0.108666. Value loss: 0.044521. Entropy: 0.278167.\n",
      "Iteration 6009: Policy loss: 0.110972. Value loss: 0.025487. Entropy: 0.278434.\n",
      "episode: 2460   score: 540.0  epsilon: 1.0    steps: 80  evaluation reward: 330.0\n",
      "episode: 2461   score: 260.0  epsilon: 1.0    steps: 128  evaluation reward: 326.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2462   score: 290.0  epsilon: 1.0    steps: 608  evaluation reward: 327.05\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6010: Policy loss: -0.111294. Value loss: 0.077230. Entropy: 0.267391.\n",
      "Iteration 6011: Policy loss: -0.117004. Value loss: 0.042610. Entropy: 0.265889.\n",
      "Iteration 6012: Policy loss: -0.122326. Value loss: 0.035189. Entropy: 0.266639.\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6013: Policy loss: -0.036735. Value loss: 0.080328. Entropy: 0.309951.\n",
      "Iteration 6014: Policy loss: -0.045045. Value loss: 0.029166. Entropy: 0.309682.\n",
      "Iteration 6015: Policy loss: -0.049088. Value loss: 0.019533. Entropy: 0.309970.\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6016: Policy loss: -0.412817. Value loss: 0.354609. Entropy: 0.305939.\n",
      "Iteration 6017: Policy loss: -0.392253. Value loss: 0.159296. Entropy: 0.304058.\n",
      "Iteration 6018: Policy loss: -0.429867. Value loss: 0.106015. Entropy: 0.304741.\n",
      "episode: 2463   score: 365.0  epsilon: 1.0    steps: 528  evaluation reward: 328.85\n",
      "episode: 2464   score: 300.0  epsilon: 1.0    steps: 888  evaluation reward: 328.25\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6019: Policy loss: 0.377541. Value loss: 0.142336. Entropy: 0.286778.\n",
      "Iteration 6020: Policy loss: 0.365176. Value loss: 0.052754. Entropy: 0.286650.\n",
      "Iteration 6021: Policy loss: 0.363887. Value loss: 0.037083. Entropy: 0.284633.\n",
      "episode: 2465   score: 180.0  epsilon: 1.0    steps: 784  evaluation reward: 325.35\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6022: Policy loss: 0.126850. Value loss: 0.089559. Entropy: 0.297546.\n",
      "Iteration 6023: Policy loss: 0.116743. Value loss: 0.037742. Entropy: 0.296504.\n",
      "Iteration 6024: Policy loss: 0.114209. Value loss: 0.025588. Entropy: 0.296251.\n",
      "episode: 2466   score: 335.0  epsilon: 1.0    steps: 24  evaluation reward: 325.55\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6025: Policy loss: 0.233026. Value loss: 0.104261. Entropy: 0.291232.\n",
      "Iteration 6026: Policy loss: 0.224486. Value loss: 0.039586. Entropy: 0.289833.\n",
      "Iteration 6027: Policy loss: 0.226166. Value loss: 0.028216. Entropy: 0.289079.\n",
      "episode: 2467   score: 265.0  epsilon: 1.0    steps: 1024  evaluation reward: 323.85\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6028: Policy loss: 0.114781. Value loss: 0.077268. Entropy: 0.307989.\n",
      "Iteration 6029: Policy loss: 0.107525. Value loss: 0.033328. Entropy: 0.307572.\n",
      "Iteration 6030: Policy loss: 0.105883. Value loss: 0.024756. Entropy: 0.307805.\n",
      "episode: 2468   score: 240.0  epsilon: 1.0    steps: 144  evaluation reward: 321.0\n",
      "episode: 2469   score: 425.0  epsilon: 1.0    steps: 160  evaluation reward: 318.7\n",
      "episode: 2470   score: 365.0  epsilon: 1.0    steps: 968  evaluation reward: 318.9\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6031: Policy loss: -0.183410. Value loss: 0.099169. Entropy: 0.271213.\n",
      "Iteration 6032: Policy loss: -0.194988. Value loss: 0.045456. Entropy: 0.274911.\n",
      "Iteration 6033: Policy loss: -0.195185. Value loss: 0.033758. Entropy: 0.281714.\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6034: Policy loss: 0.040208. Value loss: 0.121968. Entropy: 0.300094.\n",
      "Iteration 6035: Policy loss: 0.029103. Value loss: 0.043615. Entropy: 0.299943.\n",
      "Iteration 6036: Policy loss: 0.028612. Value loss: 0.030023. Entropy: 0.301415.\n",
      "episode: 2471   score: 185.0  epsilon: 1.0    steps: 984  evaluation reward: 314.55\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6037: Policy loss: 0.077270. Value loss: 0.100411. Entropy: 0.307558.\n",
      "Iteration 6038: Policy loss: 0.065107. Value loss: 0.047998. Entropy: 0.307679.\n",
      "Iteration 6039: Policy loss: 0.061363. Value loss: 0.035913. Entropy: 0.306546.\n",
      "episode: 2472   score: 240.0  epsilon: 1.0    steps: 928  evaluation reward: 315.6\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6040: Policy loss: 0.058306. Value loss: 0.080276. Entropy: 0.295167.\n",
      "Iteration 6041: Policy loss: 0.051708. Value loss: 0.031331. Entropy: 0.293910.\n",
      "Iteration 6042: Policy loss: 0.048992. Value loss: 0.023398. Entropy: 0.292667.\n",
      "episode: 2473   score: 215.0  epsilon: 1.0    steps: 904  evaluation reward: 314.15\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6043: Policy loss: 0.046539. Value loss: 0.076369. Entropy: 0.293081.\n",
      "Iteration 6044: Policy loss: 0.046453. Value loss: 0.034852. Entropy: 0.293593.\n",
      "Iteration 6045: Policy loss: 0.043225. Value loss: 0.026506. Entropy: 0.292120.\n",
      "episode: 2474   score: 180.0  epsilon: 1.0    steps: 96  evaluation reward: 310.5\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6046: Policy loss: 0.073989. Value loss: 0.077236. Entropy: 0.293136.\n",
      "Iteration 6047: Policy loss: 0.068751. Value loss: 0.031405. Entropy: 0.291344.\n",
      "Iteration 6048: Policy loss: 0.062514. Value loss: 0.021689. Entropy: 0.292686.\n",
      "episode: 2475   score: 285.0  epsilon: 1.0    steps: 1008  evaluation reward: 310.75\n",
      "Training network. lr: 0.000204. clip: 0.081577\n",
      "Iteration 6049: Policy loss: -0.269663. Value loss: 0.127791. Entropy: 0.305924.\n",
      "Iteration 6050: Policy loss: -0.270141. Value loss: 0.060723. Entropy: 0.306999.\n",
      "Iteration 6051: Policy loss: -0.280770. Value loss: 0.041334. Entropy: 0.306240.\n",
      "episode: 2476   score: 300.0  epsilon: 1.0    steps: 208  evaluation reward: 307.55\n",
      "episode: 2477   score: 215.0  epsilon: 1.0    steps: 624  evaluation reward: 307.1\n",
      "episode: 2478   score: 475.0  epsilon: 1.0    steps: 792  evaluation reward: 308.4\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6052: Policy loss: -0.019154. Value loss: 0.082804. Entropy: 0.279200.\n",
      "Iteration 6053: Policy loss: -0.026029. Value loss: 0.025458. Entropy: 0.278889.\n",
      "Iteration 6054: Policy loss: -0.030658. Value loss: 0.016696. Entropy: 0.283478.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6055: Policy loss: 0.014752. Value loss: 0.076737. Entropy: 0.307050.\n",
      "Iteration 6056: Policy loss: 0.009856. Value loss: 0.037584. Entropy: 0.307911.\n",
      "Iteration 6057: Policy loss: 0.002899. Value loss: 0.024597. Entropy: 0.306484.\n",
      "episode: 2479   score: 240.0  epsilon: 1.0    steps: 240  evaluation reward: 308.7\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6058: Policy loss: 0.012093. Value loss: 0.092217. Entropy: 0.298573.\n",
      "Iteration 6059: Policy loss: 0.001655. Value loss: 0.047301. Entropy: 0.299652.\n",
      "Iteration 6060: Policy loss: 0.002350. Value loss: 0.034243. Entropy: 0.297536.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6061: Policy loss: -0.178512. Value loss: 0.311560. Entropy: 0.304749.\n",
      "Iteration 6062: Policy loss: -0.239016. Value loss: 0.260971. Entropy: 0.305269.\n",
      "Iteration 6063: Policy loss: -0.228203. Value loss: 0.212749. Entropy: 0.304217.\n",
      "episode: 2480   score: 255.0  epsilon: 1.0    steps: 896  evaluation reward: 308.6\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6064: Policy loss: -0.372353. Value loss: 0.263919. Entropy: 0.299135.\n",
      "Iteration 6065: Policy loss: -0.375483. Value loss: 0.158389. Entropy: 0.297240.\n",
      "Iteration 6066: Policy loss: -0.369500. Value loss: 0.100220. Entropy: 0.299171.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6067: Policy loss: -0.019138. Value loss: 0.066426. Entropy: 0.301296.\n",
      "Iteration 6068: Policy loss: -0.025461. Value loss: 0.024988. Entropy: 0.299927.\n",
      "Iteration 6069: Policy loss: -0.026118. Value loss: 0.017289. Entropy: 0.299990.\n",
      "episode: 2481   score: 310.0  epsilon: 1.0    steps: 32  evaluation reward: 309.05\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6070: Policy loss: 0.079301. Value loss: 0.089076. Entropy: 0.294768.\n",
      "Iteration 6071: Policy loss: 0.074677. Value loss: 0.034569. Entropy: 0.293772.\n",
      "Iteration 6072: Policy loss: 0.074121. Value loss: 0.025158. Entropy: 0.294591.\n",
      "episode: 2482   score: 420.0  epsilon: 1.0    steps: 848  evaluation reward: 310.9\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6073: Policy loss: 0.143391. Value loss: 0.142978. Entropy: 0.303279.\n",
      "Iteration 6074: Policy loss: 0.132595. Value loss: 0.050560. Entropy: 0.304783.\n",
      "Iteration 6075: Policy loss: 0.126537. Value loss: 0.034310. Entropy: 0.304612.\n",
      "episode: 2483   score: 585.0  epsilon: 1.0    steps: 48  evaluation reward: 314.3\n",
      "episode: 2484   score: 265.0  epsilon: 1.0    steps: 248  evaluation reward: 314.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2485   score: 300.0  epsilon: 1.0    steps: 464  evaluation reward: 315.75\n",
      "episode: 2486   score: 490.0  epsilon: 1.0    steps: 464  evaluation reward: 316.05\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6076: Policy loss: 0.075214. Value loss: 0.123376. Entropy: 0.246979.\n",
      "Iteration 6077: Policy loss: 0.069343. Value loss: 0.050259. Entropy: 0.245272.\n",
      "Iteration 6078: Policy loss: 0.061521. Value loss: 0.034687. Entropy: 0.244773.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6079: Policy loss: 0.252333. Value loss: 0.104957. Entropy: 0.306697.\n",
      "Iteration 6080: Policy loss: 0.248595. Value loss: 0.043224. Entropy: 0.303325.\n",
      "Iteration 6081: Policy loss: 0.242365. Value loss: 0.027884. Entropy: 0.303824.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6082: Policy loss: 0.129894. Value loss: 0.082512. Entropy: 0.301923.\n",
      "Iteration 6083: Policy loss: 0.130492. Value loss: 0.042654. Entropy: 0.300149.\n",
      "Iteration 6084: Policy loss: 0.126671. Value loss: 0.029372. Entropy: 0.300090.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6085: Policy loss: -0.199575. Value loss: 0.155034. Entropy: 0.302159.\n",
      "Iteration 6086: Policy loss: -0.217906. Value loss: 0.036262. Entropy: 0.300753.\n",
      "Iteration 6087: Policy loss: -0.192996. Value loss: 0.020171. Entropy: 0.301414.\n",
      "episode: 2487   score: 260.0  epsilon: 1.0    steps: 184  evaluation reward: 312.85\n",
      "episode: 2488   score: 565.0  epsilon: 1.0    steps: 616  evaluation reward: 314.6\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6088: Policy loss: 0.067912. Value loss: 0.075389. Entropy: 0.280470.\n",
      "Iteration 6089: Policy loss: 0.064394. Value loss: 0.039935. Entropy: 0.279142.\n",
      "Iteration 6090: Policy loss: 0.061331. Value loss: 0.031111. Entropy: 0.278743.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6091: Policy loss: -0.020638. Value loss: 0.105280. Entropy: 0.310029.\n",
      "Iteration 6092: Policy loss: -0.023952. Value loss: 0.030475. Entropy: 0.310380.\n",
      "Iteration 6093: Policy loss: -0.031555. Value loss: 0.019029. Entropy: 0.309170.\n",
      "episode: 2489   score: 210.0  epsilon: 1.0    steps: 312  evaluation reward: 314.6\n",
      "episode: 2490   score: 235.0  epsilon: 1.0    steps: 728  evaluation reward: 313.95\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6094: Policy loss: -0.136399. Value loss: 0.147143. Entropy: 0.282743.\n",
      "Iteration 6095: Policy loss: -0.144929. Value loss: 0.075240. Entropy: 0.281911.\n",
      "Iteration 6096: Policy loss: -0.140205. Value loss: 0.056783. Entropy: 0.281654.\n",
      "episode: 2491   score: 240.0  epsilon: 1.0    steps: 184  evaluation reward: 313.6\n",
      "episode: 2492   score: 290.0  epsilon: 1.0    steps: 536  evaluation reward: 312.5\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6097: Policy loss: -0.018573. Value loss: 0.061569. Entropy: 0.277068.\n",
      "Iteration 6098: Policy loss: -0.023576. Value loss: 0.028317. Entropy: 0.275808.\n",
      "Iteration 6099: Policy loss: -0.025301. Value loss: 0.021686. Entropy: 0.277101.\n",
      "Training network. lr: 0.000204. clip: 0.081421\n",
      "Iteration 6100: Policy loss: -0.088374. Value loss: 0.056121. Entropy: 0.305372.\n",
      "Iteration 6101: Policy loss: -0.094733. Value loss: 0.031762. Entropy: 0.304563.\n",
      "Iteration 6102: Policy loss: -0.095794. Value loss: 0.022161. Entropy: 0.304695.\n",
      "episode: 2493   score: 180.0  epsilon: 1.0    steps: 112  evaluation reward: 310.05\n",
      "episode: 2494   score: 420.0  epsilon: 1.0    steps: 696  evaluation reward: 311.25\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6103: Policy loss: -0.339085. Value loss: 0.339320. Entropy: 0.274217.\n",
      "Iteration 6104: Policy loss: -0.348767. Value loss: 0.207806. Entropy: 0.276470.\n",
      "Iteration 6105: Policy loss: -0.343262. Value loss: 0.096134. Entropy: 0.278061.\n",
      "episode: 2495   score: 700.0  epsilon: 1.0    steps: 1000  evaluation reward: 311.55\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6106: Policy loss: -0.222626. Value loss: 0.145966. Entropy: 0.308766.\n",
      "Iteration 6107: Policy loss: -0.231371. Value loss: 0.054536. Entropy: 0.309331.\n",
      "Iteration 6108: Policy loss: -0.236340. Value loss: 0.038172. Entropy: 0.309873.\n",
      "episode: 2496   score: 305.0  epsilon: 1.0    steps: 904  evaluation reward: 306.25\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6109: Policy loss: 0.166681. Value loss: 0.047347. Entropy: 0.289959.\n",
      "Iteration 6110: Policy loss: 0.160167. Value loss: 0.020023. Entropy: 0.288890.\n",
      "Iteration 6111: Policy loss: 0.160493. Value loss: 0.016110. Entropy: 0.288459.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6112: Policy loss: -0.136089. Value loss: 0.117390. Entropy: 0.300586.\n",
      "Iteration 6113: Policy loss: -0.155936. Value loss: 0.041045. Entropy: 0.302943.\n",
      "Iteration 6114: Policy loss: -0.168368. Value loss: 0.030427. Entropy: 0.302126.\n",
      "episode: 2497   score: 285.0  epsilon: 1.0    steps: 208  evaluation reward: 306.45\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6115: Policy loss: 0.215591. Value loss: 0.096538. Entropy: 0.292521.\n",
      "Iteration 6116: Policy loss: 0.211247. Value loss: 0.041330. Entropy: 0.294006.\n",
      "Iteration 6117: Policy loss: 0.212219. Value loss: 0.032383. Entropy: 0.296225.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6118: Policy loss: 0.015753. Value loss: 0.104511. Entropy: 0.311934.\n",
      "Iteration 6119: Policy loss: 0.012103. Value loss: 0.052626. Entropy: 0.309659.\n",
      "Iteration 6120: Policy loss: 0.000643. Value loss: 0.039464. Entropy: 0.310131.\n",
      "episode: 2498   score: 390.0  epsilon: 1.0    steps: 8  evaluation reward: 308.25\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6121: Policy loss: 0.132365. Value loss: 0.044111. Entropy: 0.301940.\n",
      "Iteration 6122: Policy loss: 0.124213. Value loss: 0.017271. Entropy: 0.304338.\n",
      "Iteration 6123: Policy loss: 0.123038. Value loss: 0.012784. Entropy: 0.303268.\n",
      "episode: 2499   score: 260.0  epsilon: 1.0    steps: 984  evaluation reward: 309.6\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6124: Policy loss: 0.134101. Value loss: 0.117873. Entropy: 0.309415.\n",
      "Iteration 6125: Policy loss: 0.132668. Value loss: 0.042816. Entropy: 0.308875.\n",
      "Iteration 6126: Policy loss: 0.121601. Value loss: 0.029257. Entropy: 0.309369.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6127: Policy loss: 0.059723. Value loss: 0.123967. Entropy: 0.304136.\n",
      "Iteration 6128: Policy loss: 0.057135. Value loss: 0.047880. Entropy: 0.304851.\n",
      "Iteration 6129: Policy loss: 0.049787. Value loss: 0.033666. Entropy: 0.304114.\n",
      "episode: 2500   score: 400.0  epsilon: 1.0    steps: 152  evaluation reward: 310.7\n",
      "now time :  2019-09-05 20:35:34.933461\n",
      "episode: 2501   score: 345.0  epsilon: 1.0    steps: 176  evaluation reward: 313.95\n",
      "episode: 2502   score: 425.0  epsilon: 1.0    steps: 568  evaluation reward: 314.3\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6130: Policy loss: -0.026155. Value loss: 0.108160. Entropy: 0.278045.\n",
      "Iteration 6131: Policy loss: -0.029415. Value loss: 0.053762. Entropy: 0.281390.\n",
      "Iteration 6132: Policy loss: -0.035255. Value loss: 0.039916. Entropy: 0.279279.\n",
      "episode: 2503   score: 365.0  epsilon: 1.0    steps: 632  evaluation reward: 315.25\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6133: Policy loss: 0.201574. Value loss: 0.092297. Entropy: 0.300968.\n",
      "Iteration 6134: Policy loss: 0.197333. Value loss: 0.044824. Entropy: 0.299696.\n",
      "Iteration 6135: Policy loss: 0.189952. Value loss: 0.031603. Entropy: 0.300674.\n",
      "episode: 2504   score: 390.0  epsilon: 1.0    steps: 40  evaluation reward: 316.05\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6136: Policy loss: -0.327623. Value loss: 0.339225. Entropy: 0.291866.\n",
      "Iteration 6137: Policy loss: -0.328784. Value loss: 0.194123. Entropy: 0.290495.\n",
      "Iteration 6138: Policy loss: -0.311793. Value loss: 0.109312. Entropy: 0.291105.\n",
      "episode: 2505   score: 315.0  epsilon: 1.0    steps: 376  evaluation reward: 314.9\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6139: Policy loss: 0.205845. Value loss: 0.224209. Entropy: 0.298245.\n",
      "Iteration 6140: Policy loss: 0.191078. Value loss: 0.056232. Entropy: 0.297758.\n",
      "Iteration 6141: Policy loss: 0.168847. Value loss: 0.041149. Entropy: 0.295619.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6142: Policy loss: -0.134431. Value loss: 0.085958. Entropy: 0.302198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6143: Policy loss: -0.147282. Value loss: 0.036595. Entropy: 0.302767.\n",
      "Iteration 6144: Policy loss: -0.152323. Value loss: 0.028946. Entropy: 0.303927.\n",
      "episode: 2506   score: 415.0  epsilon: 1.0    steps: 24  evaluation reward: 316.8\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6145: Policy loss: 0.040132. Value loss: 0.092025. Entropy: 0.297222.\n",
      "Iteration 6146: Policy loss: 0.028972. Value loss: 0.038422. Entropy: 0.296049.\n",
      "Iteration 6147: Policy loss: 0.029878. Value loss: 0.025795. Entropy: 0.294902.\n",
      "Training network. lr: 0.000203. clip: 0.081264\n",
      "Iteration 6148: Policy loss: 0.095112. Value loss: 0.112623. Entropy: 0.311712.\n",
      "Iteration 6149: Policy loss: 0.093715. Value loss: 0.050936. Entropy: 0.311810.\n",
      "Iteration 6150: Policy loss: 0.090855. Value loss: 0.039658. Entropy: 0.310864.\n",
      "episode: 2507   score: 225.0  epsilon: 1.0    steps: 344  evaluation reward: 315.85\n",
      "episode: 2508   score: 250.0  epsilon: 1.0    steps: 1008  evaluation reward: 314.45\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6151: Policy loss: 0.141367. Value loss: 0.070192. Entropy: 0.291298.\n",
      "Iteration 6152: Policy loss: 0.140937. Value loss: 0.030460. Entropy: 0.289916.\n",
      "Iteration 6153: Policy loss: 0.140149. Value loss: 0.021538. Entropy: 0.290424.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6154: Policy loss: 0.025291. Value loss: 0.063138. Entropy: 0.293581.\n",
      "Iteration 6155: Policy loss: 0.025530. Value loss: 0.027852. Entropy: 0.293118.\n",
      "Iteration 6156: Policy loss: 0.018014. Value loss: 0.021143. Entropy: 0.292954.\n",
      "episode: 2509   score: 290.0  epsilon: 1.0    steps: 256  evaluation reward: 314.2\n",
      "episode: 2510   score: 275.0  epsilon: 1.0    steps: 744  evaluation reward: 314.25\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6157: Policy loss: -0.137884. Value loss: 0.108820. Entropy: 0.288699.\n",
      "Iteration 6158: Policy loss: -0.149169. Value loss: 0.052408. Entropy: 0.288364.\n",
      "Iteration 6159: Policy loss: -0.153038. Value loss: 0.040062. Entropy: 0.288884.\n",
      "episode: 2511   score: 275.0  epsilon: 1.0    steps: 192  evaluation reward: 315.05\n",
      "episode: 2512   score: 705.0  epsilon: 1.0    steps: 472  evaluation reward: 318.0\n",
      "episode: 2513   score: 620.0  epsilon: 1.0    steps: 1000  evaluation reward: 321.8\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6160: Policy loss: -0.047643. Value loss: 0.297164. Entropy: 0.281403.\n",
      "Iteration 6161: Policy loss: -0.053381. Value loss: 0.086289. Entropy: 0.280108.\n",
      "Iteration 6162: Policy loss: -0.071752. Value loss: 0.063067. Entropy: 0.278907.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6163: Policy loss: -0.126024. Value loss: 0.066320. Entropy: 0.298973.\n",
      "Iteration 6164: Policy loss: -0.133568. Value loss: 0.034509. Entropy: 0.298855.\n",
      "Iteration 6165: Policy loss: -0.131607. Value loss: 0.025591. Entropy: 0.299026.\n",
      "episode: 2514   score: 260.0  epsilon: 1.0    steps: 704  evaluation reward: 321.55\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6166: Policy loss: 0.106460. Value loss: 0.076233. Entropy: 0.291073.\n",
      "Iteration 6167: Policy loss: 0.100387. Value loss: 0.035998. Entropy: 0.291115.\n",
      "Iteration 6168: Policy loss: 0.099023. Value loss: 0.028661. Entropy: 0.292535.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6169: Policy loss: -0.271691. Value loss: 0.274750. Entropy: 0.308654.\n",
      "Iteration 6170: Policy loss: -0.288225. Value loss: 0.081345. Entropy: 0.307271.\n",
      "Iteration 6171: Policy loss: -0.296564. Value loss: 0.035820. Entropy: 0.308359.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6172: Policy loss: 0.028052. Value loss: 0.065444. Entropy: 0.303271.\n",
      "Iteration 6173: Policy loss: 0.028560. Value loss: 0.034371. Entropy: 0.304016.\n",
      "Iteration 6174: Policy loss: 0.032083. Value loss: 0.025291. Entropy: 0.302281.\n",
      "episode: 2515   score: 315.0  epsilon: 1.0    steps: 480  evaluation reward: 321.6\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6175: Policy loss: -0.142510. Value loss: 0.075213. Entropy: 0.290676.\n",
      "Iteration 6176: Policy loss: -0.146407. Value loss: 0.032048. Entropy: 0.288717.\n",
      "Iteration 6177: Policy loss: -0.149958. Value loss: 0.021928. Entropy: 0.288296.\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6178: Policy loss: -0.033739. Value loss: 0.164653. Entropy: 0.307457.\n",
      "Iteration 6179: Policy loss: -0.054124. Value loss: 0.074068. Entropy: 0.308114.\n",
      "Iteration 6180: Policy loss: -0.048851. Value loss: 0.049457. Entropy: 0.306960.\n",
      "episode: 2516   score: 260.0  epsilon: 1.0    steps: 408  evaluation reward: 321.8\n",
      "episode: 2517   score: 390.0  epsilon: 1.0    steps: 712  evaluation reward: 320.05\n",
      "episode: 2518   score: 345.0  epsilon: 1.0    steps: 880  evaluation reward: 320.3\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6181: Policy loss: 0.116883. Value loss: 0.173502. Entropy: 0.275164.\n",
      "Iteration 6182: Policy loss: 0.101602. Value loss: 0.076051. Entropy: 0.274374.\n",
      "Iteration 6183: Policy loss: 0.100413. Value loss: 0.054741. Entropy: 0.274531.\n",
      "episode: 2519   score: 325.0  epsilon: 1.0    steps: 240  evaluation reward: 320.85\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6184: Policy loss: -0.054182. Value loss: 0.110738. Entropy: 0.288616.\n",
      "Iteration 6185: Policy loss: -0.061658. Value loss: 0.059143. Entropy: 0.284801.\n",
      "Iteration 6186: Policy loss: -0.061286. Value loss: 0.043093. Entropy: 0.286797.\n",
      "episode: 2520   score: 450.0  epsilon: 1.0    steps: 64  evaluation reward: 323.9\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6187: Policy loss: 0.156745. Value loss: 0.135534. Entropy: 0.290109.\n",
      "Iteration 6188: Policy loss: 0.148918. Value loss: 0.061320. Entropy: 0.290979.\n",
      "Iteration 6189: Policy loss: 0.141712. Value loss: 0.043981. Entropy: 0.290396.\n",
      "episode: 2521   score: 360.0  epsilon: 1.0    steps: 856  evaluation reward: 323.55\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6190: Policy loss: -0.088821. Value loss: 0.098919. Entropy: 0.300802.\n",
      "Iteration 6191: Policy loss: -0.104825. Value loss: 0.040988. Entropy: 0.300463.\n",
      "Iteration 6192: Policy loss: -0.103983. Value loss: 0.029193. Entropy: 0.301047.\n",
      "episode: 2522   score: 155.0  epsilon: 1.0    steps: 608  evaluation reward: 323.15\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6193: Policy loss: 0.318228. Value loss: 0.084765. Entropy: 0.282094.\n",
      "Iteration 6194: Policy loss: 0.295610. Value loss: 0.032652. Entropy: 0.279912.\n",
      "Iteration 6195: Policy loss: 0.302126. Value loss: 0.025614. Entropy: 0.279906.\n",
      "episode: 2523   score: 710.0  epsilon: 1.0    steps: 704  evaluation reward: 324.35\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6196: Policy loss: -0.084665. Value loss: 0.375941. Entropy: 0.293199.\n",
      "Iteration 6197: Policy loss: -0.071145. Value loss: 0.250895. Entropy: 0.287880.\n",
      "Iteration 6198: Policy loss: -0.088275. Value loss: 0.210851. Entropy: 0.283207.\n",
      "episode: 2524   score: 210.0  epsilon: 1.0    steps: 848  evaluation reward: 324.9\n",
      "episode: 2525   score: 395.0  epsilon: 1.0    steps: 984  evaluation reward: 327.95\n",
      "Training network. lr: 0.000203. clip: 0.081116\n",
      "Iteration 6199: Policy loss: -0.238975. Value loss: 0.386913. Entropy: 0.293037.\n",
      "Iteration 6200: Policy loss: -0.256546. Value loss: 0.242958. Entropy: 0.292346.\n",
      "Iteration 6201: Policy loss: -0.247757. Value loss: 0.198956. Entropy: 0.294650.\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6202: Policy loss: 0.253724. Value loss: 0.102656. Entropy: 0.294062.\n",
      "Iteration 6203: Policy loss: 0.246070. Value loss: 0.048914. Entropy: 0.292504.\n",
      "Iteration 6204: Policy loss: 0.241116. Value loss: 0.036787. Entropy: 0.293519.\n",
      "episode: 2526   score: 460.0  epsilon: 1.0    steps: 816  evaluation reward: 329.3\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6205: Policy loss: 0.126779. Value loss: 0.077146. Entropy: 0.301071.\n",
      "Iteration 6206: Policy loss: 0.123164. Value loss: 0.034295. Entropy: 0.301115.\n",
      "Iteration 6207: Policy loss: 0.117768. Value loss: 0.026077. Entropy: 0.300834.\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6208: Policy loss: 0.117769. Value loss: 0.071547. Entropy: 0.301794.\n",
      "Iteration 6209: Policy loss: 0.112177. Value loss: 0.020930. Entropy: 0.303267.\n",
      "Iteration 6210: Policy loss: 0.111499. Value loss: 0.014182. Entropy: 0.300576.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6211: Policy loss: -0.207902. Value loss: 0.236473. Entropy: 0.303570.\n",
      "Iteration 6212: Policy loss: -0.204577. Value loss: 0.080382. Entropy: 0.302425.\n",
      "Iteration 6213: Policy loss: -0.219622. Value loss: 0.052803. Entropy: 0.302631.\n",
      "episode: 2527   score: 225.0  epsilon: 1.0    steps: 8  evaluation reward: 329.85\n",
      "episode: 2528   score: 820.0  epsilon: 1.0    steps: 664  evaluation reward: 337.15\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6214: Policy loss: 0.117073. Value loss: 0.150979. Entropy: 0.279203.\n",
      "Iteration 6215: Policy loss: 0.102818. Value loss: 0.050993. Entropy: 0.278330.\n",
      "Iteration 6216: Policy loss: 0.097633. Value loss: 0.036854. Entropy: 0.278305.\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6217: Policy loss: -0.017585. Value loss: 0.337266. Entropy: 0.305554.\n",
      "Iteration 6218: Policy loss: -0.029201. Value loss: 0.152652. Entropy: 0.304237.\n",
      "Iteration 6219: Policy loss: -0.055836. Value loss: 0.088365. Entropy: 0.304514.\n",
      "episode: 2529   score: 495.0  epsilon: 1.0    steps: 24  evaluation reward: 339.7\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6220: Policy loss: 0.109658. Value loss: 0.094387. Entropy: 0.295584.\n",
      "Iteration 6221: Policy loss: 0.101874. Value loss: 0.039891. Entropy: 0.296838.\n",
      "Iteration 6222: Policy loss: 0.100465. Value loss: 0.032188. Entropy: 0.295340.\n",
      "episode: 2530   score: 265.0  epsilon: 1.0    steps: 584  evaluation reward: 338.75\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6223: Policy loss: -0.147509. Value loss: 0.178350. Entropy: 0.289785.\n",
      "Iteration 6224: Policy loss: -0.167437. Value loss: 0.088817. Entropy: 0.288681.\n",
      "Iteration 6225: Policy loss: -0.176232. Value loss: 0.062173. Entropy: 0.290490.\n",
      "episode: 2531   score: 635.0  epsilon: 1.0    steps: 568  evaluation reward: 341.2\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6226: Policy loss: 0.100538. Value loss: 0.070770. Entropy: 0.293025.\n",
      "Iteration 6227: Policy loss: 0.096509. Value loss: 0.026081. Entropy: 0.291955.\n",
      "Iteration 6228: Policy loss: 0.084829. Value loss: 0.020074. Entropy: 0.291524.\n",
      "episode: 2532   score: 510.0  epsilon: 1.0    steps: 32  evaluation reward: 340.35\n",
      "episode: 2533   score: 275.0  epsilon: 1.0    steps: 488  evaluation reward: 339.7\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6229: Policy loss: -0.166131. Value loss: 0.271014. Entropy: 0.282598.\n",
      "Iteration 6230: Policy loss: -0.177199. Value loss: 0.111625. Entropy: 0.282450.\n",
      "Iteration 6231: Policy loss: -0.198364. Value loss: 0.064219. Entropy: 0.282959.\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6232: Policy loss: -0.029249. Value loss: 0.133448. Entropy: 0.308898.\n",
      "Iteration 6233: Policy loss: -0.028059. Value loss: 0.052866. Entropy: 0.308789.\n",
      "Iteration 6234: Policy loss: -0.033795. Value loss: 0.037913. Entropy: 0.308275.\n",
      "episode: 2534   score: 420.0  epsilon: 1.0    steps: 296  evaluation reward: 341.65\n",
      "episode: 2535   score: 270.0  epsilon: 1.0    steps: 360  evaluation reward: 340.5\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6235: Policy loss: 0.005842. Value loss: 0.149520. Entropy: 0.277871.\n",
      "Iteration 6236: Policy loss: 0.001660. Value loss: 0.070905. Entropy: 0.277535.\n",
      "Iteration 6237: Policy loss: -0.000644. Value loss: 0.061094. Entropy: 0.277981.\n",
      "episode: 2536   score: 345.0  epsilon: 1.0    steps: 320  evaluation reward: 337.7\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6238: Policy loss: 0.342312. Value loss: 0.085864. Entropy: 0.293651.\n",
      "Iteration 6239: Policy loss: 0.337772. Value loss: 0.034464. Entropy: 0.293259.\n",
      "Iteration 6240: Policy loss: 0.331961. Value loss: 0.025557. Entropy: 0.292737.\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6241: Policy loss: 0.225446. Value loss: 0.072110. Entropy: 0.308052.\n",
      "Iteration 6242: Policy loss: 0.222691. Value loss: 0.034798. Entropy: 0.307833.\n",
      "Iteration 6243: Policy loss: 0.214794. Value loss: 0.024769. Entropy: 0.307514.\n",
      "episode: 2537   score: 445.0  epsilon: 1.0    steps: 8  evaluation reward: 338.7\n",
      "episode: 2538   score: 150.0  epsilon: 1.0    steps: 352  evaluation reward: 336.85\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6244: Policy loss: 0.023133. Value loss: 0.117339. Entropy: 0.277722.\n",
      "Iteration 6245: Policy loss: 0.020400. Value loss: 0.047076. Entropy: 0.277403.\n",
      "Iteration 6246: Policy loss: 0.015417. Value loss: 0.031034. Entropy: 0.276813.\n",
      "episode: 2539   score: 230.0  epsilon: 1.0    steps: 992  evaluation reward: 337.6\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6247: Policy loss: -0.056963. Value loss: 0.173848. Entropy: 0.303437.\n",
      "Iteration 6248: Policy loss: -0.074091. Value loss: 0.067579. Entropy: 0.303640.\n",
      "Iteration 6249: Policy loss: -0.075821. Value loss: 0.045661. Entropy: 0.302153.\n",
      "episode: 2540   score: 590.0  epsilon: 1.0    steps: 760  evaluation reward: 341.4\n",
      "Training network. lr: 0.000202. clip: 0.080960\n",
      "Iteration 6250: Policy loss: 0.149328. Value loss: 0.106581. Entropy: 0.284704.\n",
      "Iteration 6251: Policy loss: 0.138025. Value loss: 0.055239. Entropy: 0.285285.\n",
      "Iteration 6252: Policy loss: 0.136421. Value loss: 0.037588. Entropy: 0.283313.\n",
      "episode: 2541   score: 335.0  epsilon: 1.0    steps: 624  evaluation reward: 342.05\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6253: Policy loss: 0.256806. Value loss: 0.078264. Entropy: 0.290890.\n",
      "Iteration 6254: Policy loss: 0.252294. Value loss: 0.034947. Entropy: 0.289605.\n",
      "Iteration 6255: Policy loss: 0.249275. Value loss: 0.025976. Entropy: 0.289955.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6256: Policy loss: -0.058700. Value loss: 0.120732. Entropy: 0.307240.\n",
      "Iteration 6257: Policy loss: -0.065658. Value loss: 0.050264. Entropy: 0.306207.\n",
      "Iteration 6258: Policy loss: -0.067373. Value loss: 0.033794. Entropy: 0.305305.\n",
      "episode: 2542   score: 365.0  epsilon: 1.0    steps: 728  evaluation reward: 342.85\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6259: Policy loss: -0.309351. Value loss: 0.306267. Entropy: 0.293735.\n",
      "Iteration 6260: Policy loss: -0.312141. Value loss: 0.173357. Entropy: 0.292468.\n",
      "Iteration 6261: Policy loss: -0.324843. Value loss: 0.069573. Entropy: 0.293474.\n",
      "episode: 2543   score: 330.0  epsilon: 1.0    steps: 328  evaluation reward: 342.8\n",
      "episode: 2544   score: 460.0  epsilon: 1.0    steps: 872  evaluation reward: 345.15\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6262: Policy loss: 0.173369. Value loss: 0.092654. Entropy: 0.285148.\n",
      "Iteration 6263: Policy loss: 0.161733. Value loss: 0.027111. Entropy: 0.283465.\n",
      "Iteration 6264: Policy loss: 0.155121. Value loss: 0.018028. Entropy: 0.283545.\n",
      "episode: 2545   score: 260.0  epsilon: 1.0    steps: 112  evaluation reward: 344.45\n",
      "episode: 2546   score: 245.0  epsilon: 1.0    steps: 792  evaluation reward: 345.05\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6265: Policy loss: -0.067279. Value loss: 0.333630. Entropy: 0.276416.\n",
      "Iteration 6266: Policy loss: -0.073054. Value loss: 0.199007. Entropy: 0.273986.\n",
      "Iteration 6267: Policy loss: -0.079254. Value loss: 0.166671. Entropy: 0.275860.\n",
      "episode: 2547   score: 485.0  epsilon: 1.0    steps: 952  evaluation reward: 347.05\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6268: Policy loss: 0.013593. Value loss: 0.087098. Entropy: 0.301190.\n",
      "Iteration 6269: Policy loss: 0.001565. Value loss: 0.042132. Entropy: 0.303528.\n",
      "Iteration 6270: Policy loss: -0.005246. Value loss: 0.031581. Entropy: 0.303344.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6271: Policy loss: 0.135714. Value loss: 0.071690. Entropy: 0.293560.\n",
      "Iteration 6272: Policy loss: 0.132496. Value loss: 0.030556. Entropy: 0.292595.\n",
      "Iteration 6273: Policy loss: 0.129215. Value loss: 0.025332. Entropy: 0.292031.\n",
      "episode: 2548   score: 535.0  epsilon: 1.0    steps: 896  evaluation reward: 348.5\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6274: Policy loss: -0.216663. Value loss: 0.259026. Entropy: 0.296772.\n",
      "Iteration 6275: Policy loss: -0.240793. Value loss: 0.155001. Entropy: 0.298384.\n",
      "Iteration 6276: Policy loss: -0.241155. Value loss: 0.105097. Entropy: 0.298422.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6277: Policy loss: 0.042919. Value loss: 0.143844. Entropy: 0.294177.\n",
      "Iteration 6278: Policy loss: 0.032479. Value loss: 0.044018. Entropy: 0.295026.\n",
      "Iteration 6279: Policy loss: 0.027348. Value loss: 0.029159. Entropy: 0.293680.\n",
      "episode: 2549   score: 285.0  epsilon: 1.0    steps: 1016  evaluation reward: 348.5\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6280: Policy loss: -0.140379. Value loss: 0.076851. Entropy: 0.305172.\n",
      "Iteration 6281: Policy loss: -0.144033. Value loss: 0.029983. Entropy: 0.303111.\n",
      "Iteration 6282: Policy loss: -0.149300. Value loss: 0.019209. Entropy: 0.303728.\n",
      "episode: 2550   score: 260.0  epsilon: 1.0    steps: 472  evaluation reward: 349.0\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6283: Policy loss: -0.056839. Value loss: 0.145357. Entropy: 0.266692.\n",
      "Iteration 6284: Policy loss: -0.071705. Value loss: 0.047115. Entropy: 0.264929.\n",
      "Iteration 6285: Policy loss: -0.079259. Value loss: 0.032165. Entropy: 0.263902.\n",
      "now time :  2019-09-05 20:45:15.843021\n",
      "episode: 2551   score: 275.0  epsilon: 1.0    steps: 232  evaluation reward: 348.4\n",
      "episode: 2552   score: 420.0  epsilon: 1.0    steps: 576  evaluation reward: 348.7\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6286: Policy loss: 0.378449. Value loss: 0.153607. Entropy: 0.276020.\n",
      "Iteration 6287: Policy loss: 0.361267. Value loss: 0.049239. Entropy: 0.274342.\n",
      "Iteration 6288: Policy loss: 0.358158. Value loss: 0.030567. Entropy: 0.277307.\n",
      "episode: 2553   score: 210.0  epsilon: 1.0    steps: 456  evaluation reward: 348.7\n",
      "episode: 2554   score: 365.0  epsilon: 1.0    steps: 544  evaluation reward: 350.25\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6289: Policy loss: -0.116471. Value loss: 0.087339. Entropy: 0.279550.\n",
      "Iteration 6290: Policy loss: -0.127591. Value loss: 0.051004. Entropy: 0.277793.\n",
      "Iteration 6291: Policy loss: -0.126495. Value loss: 0.041593. Entropy: 0.279399.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6292: Policy loss: 0.122762. Value loss: 0.121484. Entropy: 0.305124.\n",
      "Iteration 6293: Policy loss: 0.112929. Value loss: 0.058901. Entropy: 0.305295.\n",
      "Iteration 6294: Policy loss: 0.110384. Value loss: 0.045848. Entropy: 0.304748.\n",
      "episode: 2555   score: 485.0  epsilon: 1.0    steps: 616  evaluation reward: 350.4\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6295: Policy loss: 0.040398. Value loss: 0.134073. Entropy: 0.289081.\n",
      "Iteration 6296: Policy loss: 0.028926. Value loss: 0.091048. Entropy: 0.289310.\n",
      "Iteration 6297: Policy loss: 0.027319. Value loss: 0.081146. Entropy: 0.288640.\n",
      "Training network. lr: 0.000202. clip: 0.080803\n",
      "Iteration 6298: Policy loss: 0.097913. Value loss: 0.074708. Entropy: 0.300234.\n",
      "Iteration 6299: Policy loss: 0.092992. Value loss: 0.033260. Entropy: 0.299611.\n",
      "Iteration 6300: Policy loss: 0.088312. Value loss: 0.023543. Entropy: 0.298258.\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6301: Policy loss: 0.091528. Value loss: 0.120357. Entropy: 0.299500.\n",
      "Iteration 6302: Policy loss: 0.086651. Value loss: 0.050958. Entropy: 0.301070.\n",
      "Iteration 6303: Policy loss: 0.078163. Value loss: 0.037904. Entropy: 0.300618.\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6304: Policy loss: 0.072977. Value loss: 0.102475. Entropy: 0.301161.\n",
      "Iteration 6305: Policy loss: 0.072731. Value loss: 0.030037. Entropy: 0.299098.\n",
      "Iteration 6306: Policy loss: 0.062661. Value loss: 0.018833. Entropy: 0.301006.\n",
      "episode: 2556   score: 225.0  epsilon: 1.0    steps: 856  evaluation reward: 350.05\n",
      "episode: 2557   score: 265.0  epsilon: 1.0    steps: 952  evaluation reward: 350.6\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6307: Policy loss: 0.121059. Value loss: 0.119194. Entropy: 0.295880.\n",
      "Iteration 6308: Policy loss: 0.116407. Value loss: 0.059647. Entropy: 0.294276.\n",
      "Iteration 6309: Policy loss: 0.106921. Value loss: 0.040222. Entropy: 0.294366.\n",
      "episode: 2558   score: 540.0  epsilon: 1.0    steps: 784  evaluation reward: 351.1\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6310: Policy loss: -0.375595. Value loss: 0.272444. Entropy: 0.281163.\n",
      "Iteration 6311: Policy loss: -0.388798. Value loss: 0.113957. Entropy: 0.280132.\n",
      "Iteration 6312: Policy loss: -0.394057. Value loss: 0.055840. Entropy: 0.281770.\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6313: Policy loss: 0.038330. Value loss: 0.127907. Entropy: 0.306277.\n",
      "Iteration 6314: Policy loss: 0.024334. Value loss: 0.054400. Entropy: 0.305874.\n",
      "Iteration 6315: Policy loss: 0.020703. Value loss: 0.037692. Entropy: 0.304966.\n",
      "episode: 2559   score: 390.0  epsilon: 1.0    steps: 32  evaluation reward: 352.05\n",
      "episode: 2560   score: 255.0  epsilon: 1.0    steps: 456  evaluation reward: 349.2\n",
      "episode: 2561   score: 830.0  epsilon: 1.0    steps: 696  evaluation reward: 354.9\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6316: Policy loss: 0.213978. Value loss: 0.108168. Entropy: 0.266448.\n",
      "Iteration 6317: Policy loss: 0.201346. Value loss: 0.042569. Entropy: 0.265021.\n",
      "Iteration 6318: Policy loss: 0.196313. Value loss: 0.030345. Entropy: 0.265398.\n",
      "episode: 2562   score: 420.0  epsilon: 1.0    steps: 216  evaluation reward: 356.2\n",
      "episode: 2563   score: 335.0  epsilon: 1.0    steps: 968  evaluation reward: 355.9\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6319: Policy loss: 0.432563. Value loss: 0.112378. Entropy: 0.295351.\n",
      "Iteration 6320: Policy loss: 0.426053. Value loss: 0.043082. Entropy: 0.293977.\n",
      "Iteration 6321: Policy loss: 0.420223. Value loss: 0.033873. Entropy: 0.294061.\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6322: Policy loss: 0.135736. Value loss: 0.077492. Entropy: 0.298646.\n",
      "Iteration 6323: Policy loss: 0.126560. Value loss: 0.039760. Entropy: 0.296613.\n",
      "Iteration 6324: Policy loss: 0.126914. Value loss: 0.030649. Entropy: 0.296928.\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6325: Policy loss: 0.193963. Value loss: 0.086927. Entropy: 0.304479.\n",
      "Iteration 6326: Policy loss: 0.185245. Value loss: 0.039749. Entropy: 0.302590.\n",
      "Iteration 6327: Policy loss: 0.171156. Value loss: 0.030192. Entropy: 0.302735.\n",
      "episode: 2564   score: 210.0  epsilon: 1.0    steps: 120  evaluation reward: 355.0\n",
      "episode: 2565   score: 80.0  epsilon: 1.0    steps: 288  evaluation reward: 354.0\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6328: Policy loss: -0.135025. Value loss: 0.080386. Entropy: 0.273834.\n",
      "Iteration 6329: Policy loss: -0.143390. Value loss: 0.041032. Entropy: 0.272873.\n",
      "Iteration 6330: Policy loss: -0.141297. Value loss: 0.029436. Entropy: 0.273652.\n",
      "episode: 2566   score: 345.0  epsilon: 1.0    steps: 168  evaluation reward: 354.1\n",
      "episode: 2567   score: 365.0  epsilon: 1.0    steps: 848  evaluation reward: 355.1\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6331: Policy loss: 0.098062. Value loss: 0.076523. Entropy: 0.286918.\n",
      "Iteration 6332: Policy loss: 0.093906. Value loss: 0.037935. Entropy: 0.285862.\n",
      "Iteration 6333: Policy loss: 0.091978. Value loss: 0.028282. Entropy: 0.284595.\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6334: Policy loss: 0.125872. Value loss: 0.095438. Entropy: 0.305827.\n",
      "Iteration 6335: Policy loss: 0.117897. Value loss: 0.048181. Entropy: 0.305461.\n",
      "Iteration 6336: Policy loss: 0.111931. Value loss: 0.036526. Entropy: 0.304735.\n",
      "episode: 2568   score: 265.0  epsilon: 1.0    steps: 88  evaluation reward: 355.35\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6337: Policy loss: -0.065130. Value loss: 0.101063. Entropy: 0.288974.\n",
      "Iteration 6338: Policy loss: -0.068721. Value loss: 0.041403. Entropy: 0.288107.\n",
      "Iteration 6339: Policy loss: -0.075539. Value loss: 0.028167. Entropy: 0.288364.\n",
      "episode: 2569   score: 310.0  epsilon: 1.0    steps: 104  evaluation reward: 354.2\n",
      "episode: 2570   score: 345.0  epsilon: 1.0    steps: 752  evaluation reward: 354.0\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6340: Policy loss: 0.031583. Value loss: 0.049092. Entropy: 0.281089.\n",
      "Iteration 6341: Policy loss: 0.023906. Value loss: 0.024475. Entropy: 0.280843.\n",
      "Iteration 6342: Policy loss: 0.021441. Value loss: 0.019646. Entropy: 0.281433.\n",
      "Training network. lr: 0.000202. clip: 0.080656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6343: Policy loss: -0.000322. Value loss: 0.054578. Entropy: 0.304427.\n",
      "Iteration 6344: Policy loss: -0.002708. Value loss: 0.022541. Entropy: 0.303299.\n",
      "Iteration 6345: Policy loss: -0.002472. Value loss: 0.016293. Entropy: 0.303430.\n",
      "episode: 2571   score: 240.0  epsilon: 1.0    steps: 696  evaluation reward: 354.55\n",
      "episode: 2572   score: 225.0  epsilon: 1.0    steps: 776  evaluation reward: 354.4\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6346: Policy loss: -0.524441. Value loss: 0.531512. Entropy: 0.283170.\n",
      "Iteration 6347: Policy loss: -0.562957. Value loss: 0.270356. Entropy: 0.278518.\n",
      "Iteration 6348: Policy loss: -0.521485. Value loss: 0.131636. Entropy: 0.281039.\n",
      "episode: 2573   score: 240.0  epsilon: 1.0    steps: 872  evaluation reward: 354.65\n",
      "Training network. lr: 0.000202. clip: 0.080656\n",
      "Iteration 6349: Policy loss: 0.184998. Value loss: 0.091239. Entropy: 0.299776.\n",
      "Iteration 6350: Policy loss: 0.179950. Value loss: 0.042453. Entropy: 0.299029.\n",
      "Iteration 6351: Policy loss: 0.176244. Value loss: 0.025830. Entropy: 0.297987.\n",
      "episode: 2574   score: 620.0  epsilon: 1.0    steps: 736  evaluation reward: 359.05\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6352: Policy loss: 0.197072. Value loss: 0.138756. Entropy: 0.287063.\n",
      "Iteration 6353: Policy loss: 0.190877. Value loss: 0.044824. Entropy: 0.287518.\n",
      "Iteration 6354: Policy loss: 0.190078. Value loss: 0.030667. Entropy: 0.286565.\n",
      "episode: 2575   score: 210.0  epsilon: 1.0    steps: 64  evaluation reward: 358.3\n",
      "episode: 2576   score: 475.0  epsilon: 1.0    steps: 464  evaluation reward: 360.05\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6355: Policy loss: 0.031995. Value loss: 0.080512. Entropy: 0.279048.\n",
      "Iteration 6356: Policy loss: 0.026970. Value loss: 0.036975. Entropy: 0.279860.\n",
      "Iteration 6357: Policy loss: 0.025631. Value loss: 0.028320. Entropy: 0.277786.\n",
      "episode: 2577   score: 210.0  epsilon: 1.0    steps: 952  evaluation reward: 360.0\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6358: Policy loss: 0.122772. Value loss: 0.073590. Entropy: 0.302768.\n",
      "Iteration 6359: Policy loss: 0.120589. Value loss: 0.037307. Entropy: 0.304006.\n",
      "Iteration 6360: Policy loss: 0.120081. Value loss: 0.029121. Entropy: 0.302758.\n",
      "episode: 2578   score: 260.0  epsilon: 1.0    steps: 496  evaluation reward: 357.85\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6361: Policy loss: -0.354407. Value loss: 0.253157. Entropy: 0.278503.\n",
      "Iteration 6362: Policy loss: -0.337059. Value loss: 0.086056. Entropy: 0.277971.\n",
      "Iteration 6363: Policy loss: -0.349393. Value loss: 0.044207. Entropy: 0.283083.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6364: Policy loss: 0.030409. Value loss: 0.105093. Entropy: 0.301634.\n",
      "Iteration 6365: Policy loss: 0.023015. Value loss: 0.040953. Entropy: 0.300822.\n",
      "Iteration 6366: Policy loss: 0.013629. Value loss: 0.030060. Entropy: 0.299389.\n",
      "episode: 2579   score: 425.0  epsilon: 1.0    steps: 296  evaluation reward: 359.7\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6367: Policy loss: -0.132898. Value loss: 0.352476. Entropy: 0.289842.\n",
      "Iteration 6368: Policy loss: -0.143750. Value loss: 0.172034. Entropy: 0.289832.\n",
      "Iteration 6369: Policy loss: -0.160773. Value loss: 0.075039. Entropy: 0.288920.\n",
      "episode: 2580   score: 485.0  epsilon: 1.0    steps: 912  evaluation reward: 362.0\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6370: Policy loss: -0.128983. Value loss: 0.217745. Entropy: 0.300681.\n",
      "Iteration 6371: Policy loss: -0.145091. Value loss: 0.077386. Entropy: 0.302041.\n",
      "Iteration 6372: Policy loss: -0.156692. Value loss: 0.048324. Entropy: 0.301341.\n",
      "episode: 2581   score: 340.0  epsilon: 1.0    steps: 304  evaluation reward: 362.3\n",
      "episode: 2582   score: 215.0  epsilon: 1.0    steps: 456  evaluation reward: 360.25\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6373: Policy loss: 0.093836. Value loss: 0.080119. Entropy: 0.273941.\n",
      "Iteration 6374: Policy loss: 0.082835. Value loss: 0.039754. Entropy: 0.274446.\n",
      "Iteration 6375: Policy loss: 0.078651. Value loss: 0.031124. Entropy: 0.273154.\n",
      "episode: 2583   score: 240.0  epsilon: 1.0    steps: 664  evaluation reward: 356.8\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6376: Policy loss: 0.133082. Value loss: 0.075503. Entropy: 0.294103.\n",
      "Iteration 6377: Policy loss: 0.130033. Value loss: 0.030124. Entropy: 0.294163.\n",
      "Iteration 6378: Policy loss: 0.123154. Value loss: 0.023355. Entropy: 0.293939.\n",
      "episode: 2584   score: 210.0  epsilon: 1.0    steps: 368  evaluation reward: 356.25\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6379: Policy loss: -0.232730. Value loss: 0.229440. Entropy: 0.289729.\n",
      "Iteration 6380: Policy loss: -0.238440. Value loss: 0.101883. Entropy: 0.289961.\n",
      "Iteration 6381: Policy loss: -0.257305. Value loss: 0.065831. Entropy: 0.287748.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6382: Policy loss: 0.343333. Value loss: 0.197033. Entropy: 0.305635.\n",
      "Iteration 6383: Policy loss: 0.314933. Value loss: 0.061411. Entropy: 0.303621.\n",
      "Iteration 6384: Policy loss: 0.314813. Value loss: 0.042212. Entropy: 0.303311.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6385: Policy loss: 0.130323. Value loss: 0.308984. Entropy: 0.306667.\n",
      "Iteration 6386: Policy loss: 0.121490. Value loss: 0.104494. Entropy: 0.304964.\n",
      "Iteration 6387: Policy loss: 0.111376. Value loss: 0.047289. Entropy: 0.305155.\n",
      "episode: 2585   score: 720.0  epsilon: 1.0    steps: 680  evaluation reward: 360.45\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6388: Policy loss: -0.325798. Value loss: 0.249038. Entropy: 0.286781.\n",
      "Iteration 6389: Policy loss: -0.336096. Value loss: 0.098499. Entropy: 0.288743.\n",
      "Iteration 6390: Policy loss: -0.337121. Value loss: 0.065210. Entropy: 0.289355.\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6391: Policy loss: -0.225172. Value loss: 0.186114. Entropy: 0.301616.\n",
      "Iteration 6392: Policy loss: -0.225841. Value loss: 0.073328. Entropy: 0.301391.\n",
      "Iteration 6393: Policy loss: -0.239826. Value loss: 0.050895. Entropy: 0.301475.\n",
      "episode: 2586   score: 440.0  epsilon: 1.0    steps: 264  evaluation reward: 359.95\n",
      "episode: 2587   score: 815.0  epsilon: 1.0    steps: 304  evaluation reward: 365.5\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6394: Policy loss: 0.097441. Value loss: 0.083887. Entropy: 0.280520.\n",
      "Iteration 6395: Policy loss: 0.093934. Value loss: 0.042327. Entropy: 0.280232.\n",
      "Iteration 6396: Policy loss: 0.094258. Value loss: 0.028403. Entropy: 0.279379.\n",
      "episode: 2588   score: 260.0  epsilon: 1.0    steps: 48  evaluation reward: 362.45\n",
      "episode: 2589   score: 225.0  epsilon: 1.0    steps: 968  evaluation reward: 362.6\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6397: Policy loss: 0.094354. Value loss: 0.152337. Entropy: 0.293598.\n",
      "Iteration 6398: Policy loss: 0.077850. Value loss: 0.063784. Entropy: 0.291832.\n",
      "Iteration 6399: Policy loss: 0.072779. Value loss: 0.044129. Entropy: 0.292551.\n",
      "episode: 2590   score: 460.0  epsilon: 1.0    steps: 88  evaluation reward: 364.85\n",
      "episode: 2591   score: 465.0  epsilon: 1.0    steps: 1000  evaluation reward: 367.1\n",
      "Training network. lr: 0.000201. clip: 0.080499\n",
      "Iteration 6400: Policy loss: -0.163499. Value loss: 0.205261. Entropy: 0.285862.\n",
      "Iteration 6401: Policy loss: -0.173934. Value loss: 0.075829. Entropy: 0.285026.\n",
      "Iteration 6402: Policy loss: -0.188836. Value loss: 0.048429. Entropy: 0.284184.\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6403: Policy loss: 0.307717. Value loss: 0.273623. Entropy: 0.294252.\n",
      "Iteration 6404: Policy loss: 0.273684. Value loss: 0.089488. Entropy: 0.293702.\n",
      "Iteration 6405: Policy loss: 0.265129. Value loss: 0.040044. Entropy: 0.293348.\n",
      "episode: 2592   score: 395.0  epsilon: 1.0    steps: 864  evaluation reward: 368.15\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6406: Policy loss: 0.102595. Value loss: 0.096736. Entropy: 0.299534.\n",
      "Iteration 6407: Policy loss: 0.098719. Value loss: 0.043628. Entropy: 0.298519.\n",
      "Iteration 6408: Policy loss: 0.098051. Value loss: 0.033211. Entropy: 0.298382.\n",
      "episode: 2593   score: 310.0  epsilon: 1.0    steps: 840  evaluation reward: 369.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6409: Policy loss: 0.240498. Value loss: 0.120398. Entropy: 0.293316.\n",
      "Iteration 6410: Policy loss: 0.236827. Value loss: 0.034815. Entropy: 0.292958.\n",
      "Iteration 6411: Policy loss: 0.237935. Value loss: 0.022509. Entropy: 0.294284.\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6412: Policy loss: 0.127302. Value loss: 0.113949. Entropy: 0.304335.\n",
      "Iteration 6413: Policy loss: 0.121603. Value loss: 0.049121. Entropy: 0.304756.\n",
      "Iteration 6414: Policy loss: 0.118266. Value loss: 0.032596. Entropy: 0.303022.\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6415: Policy loss: -0.257278. Value loss: 0.352868. Entropy: 0.310772.\n",
      "Iteration 6416: Policy loss: -0.275748. Value loss: 0.215377. Entropy: 0.310331.\n",
      "Iteration 6417: Policy loss: -0.278353. Value loss: 0.162691. Entropy: 0.310467.\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6418: Policy loss: 0.048037. Value loss: 0.077238. Entropy: 0.296768.\n",
      "Iteration 6419: Policy loss: 0.048169. Value loss: 0.033992. Entropy: 0.298597.\n",
      "Iteration 6420: Policy loss: 0.041042. Value loss: 0.022516. Entropy: 0.297047.\n",
      "episode: 2594   score: 285.0  epsilon: 1.0    steps: 64  evaluation reward: 368.1\n",
      "episode: 2595   score: 270.0  epsilon: 1.0    steps: 880  evaluation reward: 363.8\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6421: Policy loss: 0.115175. Value loss: 0.080942. Entropy: 0.283490.\n",
      "Iteration 6422: Policy loss: 0.109132. Value loss: 0.033662. Entropy: 0.284124.\n",
      "Iteration 6423: Policy loss: 0.110832. Value loss: 0.019917. Entropy: 0.283104.\n",
      "episode: 2596   score: 535.0  epsilon: 1.0    steps: 408  evaluation reward: 366.1\n",
      "episode: 2597   score: 275.0  epsilon: 1.0    steps: 1008  evaluation reward: 366.0\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6424: Policy loss: 0.304954. Value loss: 0.201463. Entropy: 0.282878.\n",
      "Iteration 6425: Policy loss: 0.292017. Value loss: 0.059962. Entropy: 0.282057.\n",
      "Iteration 6426: Policy loss: 0.259523. Value loss: 0.037159. Entropy: 0.280210.\n",
      "episode: 2598   score: 590.0  epsilon: 1.0    steps: 680  evaluation reward: 368.0\n",
      "episode: 2599   score: 395.0  epsilon: 1.0    steps: 944  evaluation reward: 369.35\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6427: Policy loss: 0.225300. Value loss: 0.100650. Entropy: 0.281162.\n",
      "Iteration 6428: Policy loss: 0.214139. Value loss: 0.042484. Entropy: 0.279138.\n",
      "Iteration 6429: Policy loss: 0.215074. Value loss: 0.033264. Entropy: 0.279819.\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6430: Policy loss: -0.107674. Value loss: 0.119876. Entropy: 0.297723.\n",
      "Iteration 6431: Policy loss: -0.115772. Value loss: 0.056865. Entropy: 0.298435.\n",
      "Iteration 6432: Policy loss: -0.117950. Value loss: 0.040479. Entropy: 0.296994.\n",
      "episode: 2600   score: 315.0  epsilon: 1.0    steps: 176  evaluation reward: 368.5\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6433: Policy loss: -0.342770. Value loss: 0.294390. Entropy: 0.293937.\n",
      "Iteration 6434: Policy loss: -0.354258. Value loss: 0.175018. Entropy: 0.290644.\n",
      "Iteration 6435: Policy loss: -0.344065. Value loss: 0.059032. Entropy: 0.294087.\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6436: Policy loss: -0.263360. Value loss: 0.175799. Entropy: 0.303840.\n",
      "Iteration 6437: Policy loss: -0.264781. Value loss: 0.085808. Entropy: 0.300867.\n",
      "Iteration 6438: Policy loss: -0.258337. Value loss: 0.064536. Entropy: 0.302521.\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6439: Policy loss: -0.181959. Value loss: 0.334256. Entropy: 0.298287.\n",
      "Iteration 6440: Policy loss: -0.203120. Value loss: 0.196237. Entropy: 0.301654.\n",
      "Iteration 6441: Policy loss: -0.195887. Value loss: 0.151961. Entropy: 0.300110.\n",
      "now time :  2019-09-05 20:54:58.009545\n",
      "episode: 2601   score: 565.0  epsilon: 1.0    steps: 1000  evaluation reward: 370.7\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6442: Policy loss: 0.170888. Value loss: 0.186689. Entropy: 0.302886.\n",
      "Iteration 6443: Policy loss: 0.163922. Value loss: 0.064597. Entropy: 0.302253.\n",
      "Iteration 6444: Policy loss: 0.161769. Value loss: 0.039443. Entropy: 0.302439.\n",
      "episode: 2602   score: 420.0  epsilon: 1.0    steps: 280  evaluation reward: 370.65\n",
      "episode: 2603   score: 240.0  epsilon: 1.0    steps: 408  evaluation reward: 369.4\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6445: Policy loss: 0.229367. Value loss: 0.161638. Entropy: 0.270817.\n",
      "Iteration 6446: Policy loss: 0.216887. Value loss: 0.059343. Entropy: 0.273955.\n",
      "Iteration 6447: Policy loss: 0.212046. Value loss: 0.038021. Entropy: 0.275492.\n",
      "episode: 2604   score: 230.0  epsilon: 1.0    steps: 248  evaluation reward: 367.8\n",
      "episode: 2605   score: 285.0  epsilon: 1.0    steps: 984  evaluation reward: 367.5\n",
      "Training network. lr: 0.000201. clip: 0.080342\n",
      "Iteration 6448: Policy loss: -0.137207. Value loss: 0.189297. Entropy: 0.293617.\n",
      "Iteration 6449: Policy loss: -0.145315. Value loss: 0.075625. Entropy: 0.293915.\n",
      "Iteration 6450: Policy loss: -0.149899. Value loss: 0.055843. Entropy: 0.293759.\n",
      "episode: 2606   score: 590.0  epsilon: 1.0    steps: 8  evaluation reward: 369.25\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6451: Policy loss: -0.251910. Value loss: 0.100918. Entropy: 0.282838.\n",
      "Iteration 6452: Policy loss: -0.256171. Value loss: 0.056337. Entropy: 0.286552.\n",
      "Iteration 6453: Policy loss: -0.260033. Value loss: 0.043082. Entropy: 0.286402.\n",
      "episode: 2607   score: 290.0  epsilon: 1.0    steps: 1008  evaluation reward: 369.9\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6454: Policy loss: 0.216898. Value loss: 0.331747. Entropy: 0.308080.\n",
      "Iteration 6455: Policy loss: 0.206453. Value loss: 0.214074. Entropy: 0.303468.\n",
      "Iteration 6456: Policy loss: 0.199595. Value loss: 0.161129. Entropy: 0.305618.\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6457: Policy loss: 0.204737. Value loss: 0.099454. Entropy: 0.288797.\n",
      "Iteration 6458: Policy loss: 0.199352. Value loss: 0.043350. Entropy: 0.290303.\n",
      "Iteration 6459: Policy loss: 0.194499. Value loss: 0.028407. Entropy: 0.288988.\n",
      "episode: 2608   score: 565.0  epsilon: 1.0    steps: 192  evaluation reward: 373.05\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6460: Policy loss: -0.009467. Value loss: 0.114380. Entropy: 0.289710.\n",
      "Iteration 6461: Policy loss: -0.007127. Value loss: 0.051064. Entropy: 0.287651.\n",
      "Iteration 6462: Policy loss: -0.009565. Value loss: 0.037152. Entropy: 0.288390.\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6463: Policy loss: -0.035366. Value loss: 0.108861. Entropy: 0.306263.\n",
      "Iteration 6464: Policy loss: -0.038962. Value loss: 0.057506. Entropy: 0.306955.\n",
      "Iteration 6465: Policy loss: -0.040485. Value loss: 0.038863. Entropy: 0.306748.\n",
      "episode: 2609   score: 260.0  epsilon: 1.0    steps: 616  evaluation reward: 372.75\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6466: Policy loss: -0.029913. Value loss: 0.117116. Entropy: 0.290805.\n",
      "Iteration 6467: Policy loss: -0.037098. Value loss: 0.049558. Entropy: 0.289382.\n",
      "Iteration 6468: Policy loss: -0.037553. Value loss: 0.035798. Entropy: 0.290019.\n",
      "episode: 2610   score: 505.0  epsilon: 1.0    steps: 440  evaluation reward: 375.05\n",
      "episode: 2611   score: 225.0  epsilon: 1.0    steps: 568  evaluation reward: 374.55\n",
      "episode: 2612   score: 260.0  epsilon: 1.0    steps: 864  evaluation reward: 370.1\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6469: Policy loss: 0.157172. Value loss: 0.092735. Entropy: 0.273230.\n",
      "Iteration 6470: Policy loss: 0.148202. Value loss: 0.036074. Entropy: 0.272231.\n",
      "Iteration 6471: Policy loss: 0.149342. Value loss: 0.025938. Entropy: 0.270428.\n",
      "episode: 2613   score: 360.0  epsilon: 1.0    steps: 520  evaluation reward: 367.5\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6472: Policy loss: 0.100125. Value loss: 0.076974. Entropy: 0.288822.\n",
      "Iteration 6473: Policy loss: 0.091481. Value loss: 0.039172. Entropy: 0.288157.\n",
      "Iteration 6474: Policy loss: 0.092827. Value loss: 0.029427. Entropy: 0.289440.\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6475: Policy loss: -0.093512. Value loss: 0.099508. Entropy: 0.305652.\n",
      "Iteration 6476: Policy loss: -0.101652. Value loss: 0.043181. Entropy: 0.306492.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6477: Policy loss: -0.107349. Value loss: 0.034867. Entropy: 0.306577.\n",
      "episode: 2614   score: 245.0  epsilon: 1.0    steps: 384  evaluation reward: 367.35\n",
      "episode: 2615   score: 415.0  epsilon: 1.0    steps: 632  evaluation reward: 368.35\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6478: Policy loss: 0.019741. Value loss: 0.062430. Entropy: 0.279561.\n",
      "Iteration 6479: Policy loss: 0.009359. Value loss: 0.030134. Entropy: 0.276795.\n",
      "Iteration 6480: Policy loss: 0.011340. Value loss: 0.023260. Entropy: 0.277113.\n",
      "episode: 2616   score: 345.0  epsilon: 1.0    steps: 944  evaluation reward: 369.2\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6481: Policy loss: -0.110335. Value loss: 0.301756. Entropy: 0.302342.\n",
      "Iteration 6482: Policy loss: -0.140563. Value loss: 0.142143. Entropy: 0.300122.\n",
      "Iteration 6483: Policy loss: -0.137906. Value loss: 0.072794. Entropy: 0.302749.\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6484: Policy loss: 0.027126. Value loss: 0.078312. Entropy: 0.294054.\n",
      "Iteration 6485: Policy loss: 0.019939. Value loss: 0.035784. Entropy: 0.294247.\n",
      "Iteration 6486: Policy loss: 0.014372. Value loss: 0.027400. Entropy: 0.295326.\n",
      "episode: 2617   score: 215.0  epsilon: 1.0    steps: 368  evaluation reward: 367.45\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6487: Policy loss: 0.239338. Value loss: 0.151305. Entropy: 0.292332.\n",
      "Iteration 6488: Policy loss: 0.232525. Value loss: 0.058904. Entropy: 0.292840.\n",
      "Iteration 6489: Policy loss: 0.216922. Value loss: 0.040120. Entropy: 0.293189.\n",
      "episode: 2618   score: 280.0  epsilon: 1.0    steps: 880  evaluation reward: 366.8\n",
      "episode: 2619   score: 285.0  epsilon: 1.0    steps: 920  evaluation reward: 366.4\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6490: Policy loss: 0.182054. Value loss: 0.107365. Entropy: 0.298951.\n",
      "Iteration 6491: Policy loss: 0.162533. Value loss: 0.042247. Entropy: 0.300194.\n",
      "Iteration 6492: Policy loss: 0.166743. Value loss: 0.021928. Entropy: 0.299575.\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6493: Policy loss: 0.108429. Value loss: 0.097618. Entropy: 0.293333.\n",
      "Iteration 6494: Policy loss: 0.100977. Value loss: 0.040035. Entropy: 0.293207.\n",
      "Iteration 6495: Policy loss: 0.100655. Value loss: 0.026653. Entropy: 0.293365.\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6496: Policy loss: 0.139252. Value loss: 0.101515. Entropy: 0.308118.\n",
      "Iteration 6497: Policy loss: 0.132505. Value loss: 0.049400. Entropy: 0.306597.\n",
      "Iteration 6498: Policy loss: 0.131715. Value loss: 0.038484. Entropy: 0.306084.\n",
      "episode: 2620   score: 285.0  epsilon: 1.0    steps: 520  evaluation reward: 364.75\n",
      "Training network. lr: 0.000200. clip: 0.080195\n",
      "Iteration 6499: Policy loss: 0.126618. Value loss: 0.064882. Entropy: 0.291736.\n",
      "Iteration 6500: Policy loss: 0.125236. Value loss: 0.033302. Entropy: 0.291773.\n",
      "Iteration 6501: Policy loss: 0.122728. Value loss: 0.021688. Entropy: 0.289636.\n",
      "episode: 2621   score: 240.0  epsilon: 1.0    steps: 136  evaluation reward: 363.55\n",
      "episode: 2622   score: 390.0  epsilon: 1.0    steps: 552  evaluation reward: 365.9\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6502: Policy loss: 0.128762. Value loss: 0.070705. Entropy: 0.286287.\n",
      "Iteration 6503: Policy loss: 0.120189. Value loss: 0.029095. Entropy: 0.286907.\n",
      "Iteration 6504: Policy loss: 0.118500. Value loss: 0.022905. Entropy: 0.288719.\n",
      "episode: 2623   score: 210.0  epsilon: 1.0    steps: 488  evaluation reward: 360.9\n",
      "episode: 2624   score: 275.0  epsilon: 1.0    steps: 496  evaluation reward: 361.55\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6505: Policy loss: -0.055180. Value loss: 0.108247. Entropy: 0.284489.\n",
      "Iteration 6506: Policy loss: -0.059227. Value loss: 0.060534. Entropy: 0.283414.\n",
      "Iteration 6507: Policy loss: -0.056915. Value loss: 0.041398. Entropy: 0.282686.\n",
      "episode: 2625   score: 655.0  epsilon: 1.0    steps: 392  evaluation reward: 364.15\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6508: Policy loss: -0.086629. Value loss: 0.126057. Entropy: 0.294038.\n",
      "Iteration 6509: Policy loss: -0.097158. Value loss: 0.063974. Entropy: 0.293341.\n",
      "Iteration 6510: Policy loss: -0.094823. Value loss: 0.045180. Entropy: 0.292356.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6511: Policy loss: 0.022713. Value loss: 0.102936. Entropy: 0.310410.\n",
      "Iteration 6512: Policy loss: 0.005909. Value loss: 0.028376. Entropy: 0.309510.\n",
      "Iteration 6513: Policy loss: -0.002765. Value loss: 0.018036. Entropy: 0.308481.\n",
      "episode: 2626   score: 270.0  epsilon: 1.0    steps: 328  evaluation reward: 362.25\n",
      "episode: 2627   score: 280.0  epsilon: 1.0    steps: 608  evaluation reward: 362.8\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6514: Policy loss: -0.698326. Value loss: 0.601423. Entropy: 0.280654.\n",
      "Iteration 6515: Policy loss: -0.692748. Value loss: 0.384894. Entropy: 0.278775.\n",
      "Iteration 6516: Policy loss: -0.668003. Value loss: 0.172963. Entropy: 0.277591.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6517: Policy loss: -0.067180. Value loss: 0.136569. Entropy: 0.307698.\n",
      "Iteration 6518: Policy loss: -0.070456. Value loss: 0.064942. Entropy: 0.307959.\n",
      "Iteration 6519: Policy loss: -0.084656. Value loss: 0.049579. Entropy: 0.308200.\n",
      "episode: 2628   score: 185.0  epsilon: 1.0    steps: 296  evaluation reward: 356.45\n",
      "episode: 2629   score: 315.0  epsilon: 1.0    steps: 904  evaluation reward: 354.65\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6520: Policy loss: -0.111166. Value loss: 0.078212. Entropy: 0.286641.\n",
      "Iteration 6521: Policy loss: -0.115912. Value loss: 0.041627. Entropy: 0.287081.\n",
      "Iteration 6522: Policy loss: -0.112763. Value loss: 0.034805. Entropy: 0.286491.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6523: Policy loss: 0.000332. Value loss: 0.109437. Entropy: 0.292651.\n",
      "Iteration 6524: Policy loss: 0.006732. Value loss: 0.044005. Entropy: 0.294867.\n",
      "Iteration 6525: Policy loss: -0.007034. Value loss: 0.028534. Entropy: 0.292258.\n",
      "episode: 2630   score: 355.0  epsilon: 1.0    steps: 312  evaluation reward: 355.55\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6526: Policy loss: 0.225106. Value loss: 0.164353. Entropy: 0.296281.\n",
      "Iteration 6527: Policy loss: 0.229898. Value loss: 0.044092. Entropy: 0.294021.\n",
      "Iteration 6528: Policy loss: 0.221146. Value loss: 0.031751. Entropy: 0.294492.\n",
      "episode: 2631   score: 465.0  epsilon: 1.0    steps: 80  evaluation reward: 353.85\n",
      "episode: 2632   score: 465.0  epsilon: 1.0    steps: 424  evaluation reward: 353.4\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6529: Policy loss: 0.196242. Value loss: 0.112840. Entropy: 0.279256.\n",
      "Iteration 6530: Policy loss: 0.178236. Value loss: 0.034968. Entropy: 0.277584.\n",
      "Iteration 6531: Policy loss: 0.179031. Value loss: 0.025763. Entropy: 0.277277.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6532: Policy loss: 0.083521. Value loss: 0.114361. Entropy: 0.307250.\n",
      "Iteration 6533: Policy loss: 0.072879. Value loss: 0.049210. Entropy: 0.307272.\n",
      "Iteration 6534: Policy loss: 0.069164. Value loss: 0.034710. Entropy: 0.307346.\n",
      "episode: 2633   score: 575.0  epsilon: 1.0    steps: 800  evaluation reward: 356.4\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6535: Policy loss: 0.071272. Value loss: 0.127298. Entropy: 0.304737.\n",
      "Iteration 6536: Policy loss: 0.061324. Value loss: 0.041028. Entropy: 0.303616.\n",
      "Iteration 6537: Policy loss: 0.053919. Value loss: 0.028437. Entropy: 0.303863.\n",
      "episode: 2634   score: 260.0  epsilon: 1.0    steps: 208  evaluation reward: 354.8\n",
      "episode: 2635   score: 360.0  epsilon: 1.0    steps: 968  evaluation reward: 355.7\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6538: Policy loss: -0.110610. Value loss: 0.098930. Entropy: 0.294231.\n",
      "Iteration 6539: Policy loss: -0.125615. Value loss: 0.046453. Entropy: 0.296712.\n",
      "Iteration 6540: Policy loss: -0.127400. Value loss: 0.034784. Entropy: 0.294638.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6541: Policy loss: 0.130412. Value loss: 0.093233. Entropy: 0.299580.\n",
      "Iteration 6542: Policy loss: 0.123507. Value loss: 0.033231. Entropy: 0.299304.\n",
      "Iteration 6543: Policy loss: 0.121644. Value loss: 0.022004. Entropy: 0.301596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6544: Policy loss: 0.167579. Value loss: 0.121722. Entropy: 0.306084.\n",
      "Iteration 6545: Policy loss: 0.148066. Value loss: 0.047609. Entropy: 0.305834.\n",
      "Iteration 6546: Policy loss: 0.154034. Value loss: 0.033923. Entropy: 0.306749.\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6547: Policy loss: 0.011901. Value loss: 0.076616. Entropy: 0.304742.\n",
      "Iteration 6548: Policy loss: 0.005138. Value loss: 0.034872. Entropy: 0.304351.\n",
      "Iteration 6549: Policy loss: -0.000071. Value loss: 0.025528. Entropy: 0.302809.\n",
      "episode: 2636   score: 265.0  epsilon: 1.0    steps: 816  evaluation reward: 354.9\n",
      "Training network. lr: 0.000200. clip: 0.080038\n",
      "Iteration 6550: Policy loss: 0.029730. Value loss: 0.088080. Entropy: 0.299629.\n",
      "Iteration 6551: Policy loss: 0.016468. Value loss: 0.042820. Entropy: 0.300015.\n",
      "Iteration 6552: Policy loss: 0.007149. Value loss: 0.030932. Entropy: 0.298652.\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6553: Policy loss: 0.120622. Value loss: 0.108290. Entropy: 0.306175.\n",
      "Iteration 6554: Policy loss: 0.116954. Value loss: 0.042301. Entropy: 0.306200.\n",
      "Iteration 6555: Policy loss: 0.109867. Value loss: 0.029732. Entropy: 0.305125.\n",
      "episode: 2637   score: 405.0  epsilon: 1.0    steps: 144  evaluation reward: 354.5\n",
      "episode: 2638   score: 500.0  epsilon: 1.0    steps: 328  evaluation reward: 358.0\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6556: Policy loss: 0.256508. Value loss: 0.067910. Entropy: 0.292874.\n",
      "Iteration 6557: Policy loss: 0.248921. Value loss: 0.030253. Entropy: 0.293822.\n",
      "Iteration 6558: Policy loss: 0.247567. Value loss: 0.021555. Entropy: 0.294548.\n",
      "episode: 2639   score: 380.0  epsilon: 1.0    steps: 208  evaluation reward: 359.5\n",
      "episode: 2640   score: 270.0  epsilon: 1.0    steps: 928  evaluation reward: 356.3\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6559: Policy loss: -0.114963. Value loss: 0.145708. Entropy: 0.300019.\n",
      "Iteration 6560: Policy loss: -0.120759. Value loss: 0.059146. Entropy: 0.295732.\n",
      "Iteration 6561: Policy loss: -0.125813. Value loss: 0.041790. Entropy: 0.298089.\n",
      "episode: 2641   score: 315.0  epsilon: 1.0    steps: 56  evaluation reward: 356.1\n",
      "episode: 2642   score: 290.0  epsilon: 1.0    steps: 312  evaluation reward: 355.35\n",
      "episode: 2643   score: 495.0  epsilon: 1.0    steps: 408  evaluation reward: 357.0\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6562: Policy loss: -0.001422. Value loss: 0.073286. Entropy: 0.289302.\n",
      "Iteration 6563: Policy loss: -0.003724. Value loss: 0.039958. Entropy: 0.289157.\n",
      "Iteration 6564: Policy loss: -0.011982. Value loss: 0.028947. Entropy: 0.290520.\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6565: Policy loss: -0.006389. Value loss: 0.099600. Entropy: 0.309792.\n",
      "Iteration 6566: Policy loss: -0.017676. Value loss: 0.041776. Entropy: 0.307910.\n",
      "Iteration 6567: Policy loss: -0.019260. Value loss: 0.036279. Entropy: 0.308886.\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6568: Policy loss: 0.052894. Value loss: 0.056122. Entropy: 0.307929.\n",
      "Iteration 6569: Policy loss: 0.048026. Value loss: 0.027828. Entropy: 0.307122.\n",
      "Iteration 6570: Policy loss: 0.046482. Value loss: 0.021051. Entropy: 0.307415.\n",
      "episode: 2644   score: 155.0  epsilon: 1.0    steps: 80  evaluation reward: 353.95\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6571: Policy loss: -0.391588. Value loss: 0.336225. Entropy: 0.303150.\n",
      "Iteration 6572: Policy loss: -0.369545. Value loss: 0.204777. Entropy: 0.303182.\n",
      "Iteration 6573: Policy loss: -0.408388. Value loss: 0.147933. Entropy: 0.298259.\n",
      "episode: 2645   score: 345.0  epsilon: 1.0    steps: 256  evaluation reward: 354.8\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6574: Policy loss: 0.014491. Value loss: 0.076248. Entropy: 0.303405.\n",
      "Iteration 6575: Policy loss: 0.005415. Value loss: 0.021525. Entropy: 0.302073.\n",
      "Iteration 6576: Policy loss: -0.003906. Value loss: 0.016019. Entropy: 0.301871.\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6577: Policy loss: 0.239259. Value loss: 0.108527. Entropy: 0.310153.\n",
      "Iteration 6578: Policy loss: 0.233996. Value loss: 0.041323. Entropy: 0.309940.\n",
      "Iteration 6579: Policy loss: 0.233835. Value loss: 0.028461. Entropy: 0.309690.\n",
      "episode: 2646   score: 290.0  epsilon: 1.0    steps: 80  evaluation reward: 355.25\n",
      "episode: 2647   score: 265.0  epsilon: 1.0    steps: 960  evaluation reward: 353.05\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6580: Policy loss: -0.136972. Value loss: 0.101754. Entropy: 0.306437.\n",
      "Iteration 6581: Policy loss: -0.142611. Value loss: 0.041560. Entropy: 0.306627.\n",
      "Iteration 6582: Policy loss: -0.151466. Value loss: 0.029748. Entropy: 0.306459.\n",
      "episode: 2648   score: 250.0  epsilon: 1.0    steps: 840  evaluation reward: 350.2\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6583: Policy loss: -0.028417. Value loss: 0.079111. Entropy: 0.297253.\n",
      "Iteration 6584: Policy loss: -0.028711. Value loss: 0.044760. Entropy: 0.295757.\n",
      "Iteration 6585: Policy loss: -0.035334. Value loss: 0.034148. Entropy: 0.296118.\n",
      "episode: 2649   score: 390.0  epsilon: 1.0    steps: 672  evaluation reward: 351.25\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6586: Policy loss: 0.060033. Value loss: 0.049212. Entropy: 0.296285.\n",
      "Iteration 6587: Policy loss: 0.052908. Value loss: 0.023026. Entropy: 0.294690.\n",
      "Iteration 6588: Policy loss: 0.048364. Value loss: 0.018865. Entropy: 0.294261.\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6589: Policy loss: 0.092317. Value loss: 0.068828. Entropy: 0.307334.\n",
      "Iteration 6590: Policy loss: 0.085796. Value loss: 0.029666. Entropy: 0.307734.\n",
      "Iteration 6591: Policy loss: 0.083004. Value loss: 0.020571. Entropy: 0.306984.\n",
      "episode: 2650   score: 210.0  epsilon: 1.0    steps: 616  evaluation reward: 350.75\n",
      "now time :  2019-09-05 21:04:15.932341\n",
      "episode: 2651   score: 240.0  epsilon: 1.0    steps: 888  evaluation reward: 350.4\n",
      "episode: 2652   score: 620.0  epsilon: 1.0    steps: 936  evaluation reward: 352.4\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6592: Policy loss: 0.030710. Value loss: 0.073390. Entropy: 0.289187.\n",
      "Iteration 6593: Policy loss: 0.025210. Value loss: 0.033011. Entropy: 0.289436.\n",
      "Iteration 6594: Policy loss: 0.012787. Value loss: 0.023108. Entropy: 0.288878.\n",
      "episode: 2653   score: 495.0  epsilon: 1.0    steps: 1016  evaluation reward: 355.25\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6595: Policy loss: -0.064997. Value loss: 0.087676. Entropy: 0.299881.\n",
      "Iteration 6596: Policy loss: -0.071288. Value loss: 0.034109. Entropy: 0.300069.\n",
      "Iteration 6597: Policy loss: -0.084867. Value loss: 0.022122. Entropy: 0.300535.\n",
      "Training network. lr: 0.000200. clip: 0.079881\n",
      "Iteration 6598: Policy loss: 0.144996. Value loss: 0.133506. Entropy: 0.297027.\n",
      "Iteration 6599: Policy loss: 0.135303. Value loss: 0.037080. Entropy: 0.295255.\n",
      "Iteration 6600: Policy loss: 0.120669. Value loss: 0.025569. Entropy: 0.296599.\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6601: Policy loss: 0.047143. Value loss: 0.125477. Entropy: 0.308451.\n",
      "Iteration 6602: Policy loss: 0.033238. Value loss: 0.069125. Entropy: 0.309178.\n",
      "Iteration 6603: Policy loss: 0.041368. Value loss: 0.050585. Entropy: 0.309336.\n",
      "episode: 2654   score: 330.0  epsilon: 1.0    steps: 424  evaluation reward: 354.9\n",
      "episode: 2655   score: 285.0  epsilon: 1.0    steps: 848  evaluation reward: 352.9\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6604: Policy loss: -0.043754. Value loss: 0.044694. Entropy: 0.289298.\n",
      "Iteration 6605: Policy loss: -0.048742. Value loss: 0.024637. Entropy: 0.288582.\n",
      "Iteration 6606: Policy loss: -0.054616. Value loss: 0.020533. Entropy: 0.288961.\n",
      "episode: 2656   score: 180.0  epsilon: 1.0    steps: 248  evaluation reward: 352.45\n",
      "episode: 2657   score: 180.0  epsilon: 1.0    steps: 840  evaluation reward: 351.6\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6607: Policy loss: 0.023979. Value loss: 0.090917. Entropy: 0.283974.\n",
      "Iteration 6608: Policy loss: 0.023641. Value loss: 0.034891. Entropy: 0.283170.\n",
      "Iteration 6609: Policy loss: 0.014925. Value loss: 0.026695. Entropy: 0.283066.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6610: Policy loss: 0.079680. Value loss: 0.070470. Entropy: 0.303763.\n",
      "Iteration 6611: Policy loss: 0.075484. Value loss: 0.025035. Entropy: 0.302680.\n",
      "Iteration 6612: Policy loss: 0.069796. Value loss: 0.018356. Entropy: 0.302352.\n",
      "episode: 2658   score: 420.0  epsilon: 1.0    steps: 480  evaluation reward: 350.4\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6613: Policy loss: -0.320691. Value loss: 0.251304. Entropy: 0.294616.\n",
      "Iteration 6614: Policy loss: -0.353436. Value loss: 0.118017. Entropy: 0.293885.\n",
      "Iteration 6615: Policy loss: -0.352501. Value loss: 0.059894. Entropy: 0.296080.\n",
      "episode: 2659   score: 245.0  epsilon: 1.0    steps: 656  evaluation reward: 348.95\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6616: Policy loss: 0.253541. Value loss: 0.178311. Entropy: 0.295766.\n",
      "Iteration 6617: Policy loss: 0.233153. Value loss: 0.054268. Entropy: 0.294667.\n",
      "Iteration 6618: Policy loss: 0.230799. Value loss: 0.035884. Entropy: 0.294592.\n",
      "episode: 2660   score: 335.0  epsilon: 1.0    steps: 920  evaluation reward: 349.75\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6619: Policy loss: 0.025654. Value loss: 0.285353. Entropy: 0.302930.\n",
      "Iteration 6620: Policy loss: 0.034680. Value loss: 0.154592. Entropy: 0.302006.\n",
      "Iteration 6621: Policy loss: 0.028321. Value loss: 0.115683. Entropy: 0.301143.\n",
      "episode: 2661   score: 415.0  epsilon: 1.0    steps: 296  evaluation reward: 345.6\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6622: Policy loss: 0.086898. Value loss: 0.120312. Entropy: 0.287990.\n",
      "Iteration 6623: Policy loss: 0.078650. Value loss: 0.060024. Entropy: 0.287122.\n",
      "Iteration 6624: Policy loss: 0.064549. Value loss: 0.045152. Entropy: 0.286799.\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6625: Policy loss: -0.049123. Value loss: 0.076033. Entropy: 0.303125.\n",
      "Iteration 6626: Policy loss: -0.062260. Value loss: 0.038377. Entropy: 0.303680.\n",
      "Iteration 6627: Policy loss: -0.063531. Value loss: 0.030716. Entropy: 0.304142.\n",
      "episode: 2662   score: 250.0  epsilon: 1.0    steps: 776  evaluation reward: 343.9\n",
      "episode: 2663   score: 565.0  epsilon: 1.0    steps: 848  evaluation reward: 346.2\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6628: Policy loss: 0.137230. Value loss: 0.114138. Entropy: 0.294821.\n",
      "Iteration 6629: Policy loss: 0.126313. Value loss: 0.057592. Entropy: 0.293579.\n",
      "Iteration 6630: Policy loss: 0.121113. Value loss: 0.037986. Entropy: 0.294502.\n",
      "episode: 2664   score: 515.0  epsilon: 1.0    steps: 296  evaluation reward: 349.25\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6631: Policy loss: -0.014052. Value loss: 0.091008. Entropy: 0.291156.\n",
      "Iteration 6632: Policy loss: -0.014121. Value loss: 0.034689. Entropy: 0.289787.\n",
      "Iteration 6633: Policy loss: -0.020988. Value loss: 0.025722. Entropy: 0.289226.\n",
      "episode: 2665   score: 290.0  epsilon: 1.0    steps: 312  evaluation reward: 351.35\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6634: Policy loss: -0.038695. Value loss: 0.137176. Entropy: 0.292047.\n",
      "Iteration 6635: Policy loss: -0.053111. Value loss: 0.087819. Entropy: 0.289312.\n",
      "Iteration 6636: Policy loss: -0.062487. Value loss: 0.075501. Entropy: 0.291489.\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6637: Policy loss: 0.046415. Value loss: 0.064213. Entropy: 0.309207.\n",
      "Iteration 6638: Policy loss: 0.042010. Value loss: 0.030246. Entropy: 0.312369.\n",
      "Iteration 6639: Policy loss: 0.040372. Value loss: 0.022454. Entropy: 0.309927.\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6640: Policy loss: -0.193938. Value loss: 0.218444. Entropy: 0.307521.\n",
      "Iteration 6641: Policy loss: -0.199797. Value loss: 0.116639. Entropy: 0.306073.\n",
      "Iteration 6642: Policy loss: -0.208476. Value loss: 0.075238. Entropy: 0.306617.\n",
      "episode: 2666   score: 515.0  epsilon: 1.0    steps: 448  evaluation reward: 353.05\n",
      "episode: 2667   score: 290.0  epsilon: 1.0    steps: 776  evaluation reward: 352.3\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6643: Policy loss: -0.019312. Value loss: 0.062545. Entropy: 0.284839.\n",
      "Iteration 6644: Policy loss: -0.022847. Value loss: 0.030330. Entropy: 0.285166.\n",
      "Iteration 6645: Policy loss: -0.031269. Value loss: 0.022231. Entropy: 0.284890.\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6646: Policy loss: -0.065840. Value loss: 0.272105. Entropy: 0.305823.\n",
      "Iteration 6647: Policy loss: -0.087455. Value loss: 0.111856. Entropy: 0.306187.\n",
      "Iteration 6648: Policy loss: -0.088103. Value loss: 0.080132. Entropy: 0.303858.\n",
      "Training network. lr: 0.000199. clip: 0.079734\n",
      "Iteration 6649: Policy loss: 0.093128. Value loss: 0.116673. Entropy: 0.310486.\n",
      "Iteration 6650: Policy loss: 0.088847. Value loss: 0.047549. Entropy: 0.309880.\n",
      "Iteration 6651: Policy loss: 0.081481. Value loss: 0.030539. Entropy: 0.308990.\n",
      "episode: 2668   score: 620.0  epsilon: 1.0    steps: 88  evaluation reward: 355.85\n",
      "episode: 2669   score: 215.0  epsilon: 1.0    steps: 768  evaluation reward: 354.9\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6652: Policy loss: 0.126106. Value loss: 0.109045. Entropy: 0.287960.\n",
      "Iteration 6653: Policy loss: 0.122475. Value loss: 0.033736. Entropy: 0.288336.\n",
      "Iteration 6654: Policy loss: 0.115939. Value loss: 0.023556. Entropy: 0.292278.\n",
      "episode: 2670   score: 535.0  epsilon: 1.0    steps: 208  evaluation reward: 356.8\n",
      "episode: 2671   score: 365.0  epsilon: 1.0    steps: 640  evaluation reward: 358.05\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6655: Policy loss: 0.037550. Value loss: 0.072224. Entropy: 0.293305.\n",
      "Iteration 6656: Policy loss: 0.033114. Value loss: 0.034537. Entropy: 0.295264.\n",
      "Iteration 6657: Policy loss: 0.031205. Value loss: 0.027206. Entropy: 0.283019.\n",
      "episode: 2672   score: 285.0  epsilon: 1.0    steps: 248  evaluation reward: 358.65\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6658: Policy loss: -0.584144. Value loss: 0.544097. Entropy: 0.301494.\n",
      "Iteration 6659: Policy loss: -0.619732. Value loss: 0.336930. Entropy: 0.299506.\n",
      "Iteration 6660: Policy loss: -0.617799. Value loss: 0.138231. Entropy: 0.300562.\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6661: Policy loss: -0.004330. Value loss: 0.097635. Entropy: 0.312478.\n",
      "Iteration 6662: Policy loss: -0.014921. Value loss: 0.047515. Entropy: 0.312279.\n",
      "Iteration 6663: Policy loss: -0.019557. Value loss: 0.034954. Entropy: 0.311402.\n",
      "episode: 2673   score: 665.0  epsilon: 1.0    steps: 88  evaluation reward: 362.9\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6664: Policy loss: 0.439820. Value loss: 0.147578. Entropy: 0.300051.\n",
      "Iteration 6665: Policy loss: 0.419186. Value loss: 0.060308. Entropy: 0.300237.\n",
      "Iteration 6666: Policy loss: 0.420492. Value loss: 0.042413. Entropy: 0.300775.\n",
      "episode: 2674   score: 535.0  epsilon: 1.0    steps: 864  evaluation reward: 362.05\n",
      "episode: 2675   score: 180.0  epsilon: 1.0    steps: 1024  evaluation reward: 361.75\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6667: Policy loss: 0.218060. Value loss: 0.124489. Entropy: 0.300295.\n",
      "Iteration 6668: Policy loss: 0.212316. Value loss: 0.054363. Entropy: 0.300154.\n",
      "Iteration 6669: Policy loss: 0.210116. Value loss: 0.039786. Entropy: 0.299600.\n",
      "episode: 2676   score: 475.0  epsilon: 1.0    steps: 56  evaluation reward: 361.75\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6670: Policy loss: 0.070698. Value loss: 0.078155. Entropy: 0.301279.\n",
      "Iteration 6671: Policy loss: 0.064351. Value loss: 0.034296. Entropy: 0.298876.\n",
      "Iteration 6672: Policy loss: 0.058996. Value loss: 0.022308. Entropy: 0.300843.\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6673: Policy loss: -0.034303. Value loss: 0.363880. Entropy: 0.311294.\n",
      "Iteration 6674: Policy loss: -0.023410. Value loss: 0.202017. Entropy: 0.309876.\n",
      "Iteration 6675: Policy loss: -0.033474. Value loss: 0.153065. Entropy: 0.310319.\n",
      "episode: 2677   score: 315.0  epsilon: 1.0    steps: 840  evaluation reward: 362.8\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6676: Policy loss: -0.272473. Value loss: 0.258136. Entropy: 0.300927.\n",
      "Iteration 6677: Policy loss: -0.270170. Value loss: 0.090037. Entropy: 0.301885.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6678: Policy loss: -0.286547. Value loss: 0.056445. Entropy: 0.301041.\n",
      "episode: 2678   score: 335.0  epsilon: 1.0    steps: 720  evaluation reward: 363.55\n",
      "episode: 2679   score: 460.0  epsilon: 1.0    steps: 1016  evaluation reward: 363.9\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6679: Policy loss: -0.065833. Value loss: 0.272634. Entropy: 0.294037.\n",
      "Iteration 6680: Policy loss: -0.068609. Value loss: 0.146793. Entropy: 0.293303.\n",
      "Iteration 6681: Policy loss: -0.074777. Value loss: 0.059450. Entropy: 0.292984.\n",
      "episode: 2680   score: 750.0  epsilon: 1.0    steps: 632  evaluation reward: 366.55\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6682: Policy loss: -0.153017. Value loss: 0.392036. Entropy: 0.286447.\n",
      "Iteration 6683: Policy loss: -0.159847. Value loss: 0.218660. Entropy: 0.283448.\n",
      "Iteration 6684: Policy loss: -0.184885. Value loss: 0.101375. Entropy: 0.285282.\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6685: Policy loss: 0.238576. Value loss: 0.303481. Entropy: 0.306384.\n",
      "Iteration 6686: Policy loss: 0.207104. Value loss: 0.096204. Entropy: 0.305571.\n",
      "Iteration 6687: Policy loss: 0.204899. Value loss: 0.060835. Entropy: 0.303644.\n",
      "episode: 2681   score: 310.0  epsilon: 1.0    steps: 400  evaluation reward: 366.25\n",
      "episode: 2682   score: 210.0  epsilon: 1.0    steps: 848  evaluation reward: 366.2\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6688: Policy loss: 0.295735. Value loss: 0.212655. Entropy: 0.286442.\n",
      "Iteration 6689: Policy loss: 0.282031. Value loss: 0.089431. Entropy: 0.282700.\n",
      "Iteration 6690: Policy loss: 0.270559. Value loss: 0.056499. Entropy: 0.283277.\n",
      "episode: 2683   score: 460.0  epsilon: 1.0    steps: 144  evaluation reward: 368.4\n",
      "episode: 2684   score: 535.0  epsilon: 1.0    steps: 744  evaluation reward: 371.65\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6691: Policy loss: 0.193679. Value loss: 0.114897. Entropy: 0.283822.\n",
      "Iteration 6692: Policy loss: 0.191503. Value loss: 0.062710. Entropy: 0.282290.\n",
      "Iteration 6693: Policy loss: 0.181568. Value loss: 0.050078. Entropy: 0.280389.\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6694: Policy loss: 0.129076. Value loss: 0.099487. Entropy: 0.307343.\n",
      "Iteration 6695: Policy loss: 0.129304. Value loss: 0.048655. Entropy: 0.306984.\n",
      "Iteration 6696: Policy loss: 0.123819. Value loss: 0.039136. Entropy: 0.306555.\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6697: Policy loss: 0.097076. Value loss: 0.063363. Entropy: 0.305623.\n",
      "Iteration 6698: Policy loss: 0.096049. Value loss: 0.030366. Entropy: 0.304003.\n",
      "Iteration 6699: Policy loss: 0.089803. Value loss: 0.022534. Entropy: 0.304950.\n",
      "episode: 2685   score: 260.0  epsilon: 1.0    steps: 56  evaluation reward: 367.05\n",
      "episode: 2686   score: 210.0  epsilon: 1.0    steps: 648  evaluation reward: 364.75\n",
      "Training network. lr: 0.000199. clip: 0.079577\n",
      "Iteration 6700: Policy loss: -0.504991. Value loss: 0.474320. Entropy: 0.276471.\n",
      "Iteration 6701: Policy loss: -0.513036. Value loss: 0.257578. Entropy: 0.275632.\n",
      "Iteration 6702: Policy loss: -0.541742. Value loss: 0.112099. Entropy: 0.275181.\n",
      "episode: 2687   score: 290.0  epsilon: 1.0    steps: 264  evaluation reward: 359.5\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6703: Policy loss: -0.714264. Value loss: 0.355003. Entropy: 0.294640.\n",
      "Iteration 6704: Policy loss: -0.727382. Value loss: 0.103644. Entropy: 0.296348.\n",
      "Iteration 6705: Policy loss: -0.727500. Value loss: 0.077599. Entropy: 0.295099.\n",
      "episode: 2688   score: 500.0  epsilon: 1.0    steps: 448  evaluation reward: 361.9\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6706: Policy loss: -0.109870. Value loss: 0.162496. Entropy: 0.292639.\n",
      "Iteration 6707: Policy loss: -0.124393. Value loss: 0.063364. Entropy: 0.292835.\n",
      "Iteration 6708: Policy loss: -0.124521. Value loss: 0.038817. Entropy: 0.291185.\n",
      "episode: 2689   score: 285.0  epsilon: 1.0    steps: 840  evaluation reward: 362.5\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6709: Policy loss: -0.114956. Value loss: 0.244538. Entropy: 0.302658.\n",
      "Iteration 6710: Policy loss: -0.113342. Value loss: 0.114835. Entropy: 0.303639.\n",
      "Iteration 6711: Policy loss: -0.133892. Value loss: 0.085003. Entropy: 0.303193.\n",
      "episode: 2690   score: 440.0  epsilon: 1.0    steps: 208  evaluation reward: 362.3\n",
      "episode: 2691   score: 460.0  epsilon: 1.0    steps: 240  evaluation reward: 362.25\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6712: Policy loss: 0.132455. Value loss: 0.132575. Entropy: 0.273451.\n",
      "Iteration 6713: Policy loss: 0.126126. Value loss: 0.065223. Entropy: 0.273839.\n",
      "Iteration 6714: Policy loss: 0.121129. Value loss: 0.043609. Entropy: 0.272749.\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6715: Policy loss: 0.257604. Value loss: 0.110844. Entropy: 0.311041.\n",
      "Iteration 6716: Policy loss: 0.245009. Value loss: 0.052924. Entropy: 0.310640.\n",
      "Iteration 6717: Policy loss: 0.234725. Value loss: 0.035918. Entropy: 0.310170.\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6718: Policy loss: 0.282171. Value loss: 0.115984. Entropy: 0.308538.\n",
      "Iteration 6719: Policy loss: 0.277689. Value loss: 0.050013. Entropy: 0.306535.\n",
      "Iteration 6720: Policy loss: 0.273670. Value loss: 0.033606. Entropy: 0.306194.\n",
      "episode: 2692   score: 230.0  epsilon: 1.0    steps: 288  evaluation reward: 360.6\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6721: Policy loss: 0.178507. Value loss: 0.091795. Entropy: 0.292421.\n",
      "Iteration 6722: Policy loss: 0.168451. Value loss: 0.037958. Entropy: 0.292655.\n",
      "Iteration 6723: Policy loss: 0.170312. Value loss: 0.027748. Entropy: 0.293675.\n",
      "episode: 2693   score: 475.0  epsilon: 1.0    steps: 576  evaluation reward: 362.25\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6724: Policy loss: 0.035485. Value loss: 0.193474. Entropy: 0.292526.\n",
      "Iteration 6725: Policy loss: 0.030604. Value loss: 0.068001. Entropy: 0.291146.\n",
      "Iteration 6726: Policy loss: 0.011341. Value loss: 0.041078. Entropy: 0.292056.\n",
      "episode: 2694   score: 245.0  epsilon: 1.0    steps: 424  evaluation reward: 361.85\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6727: Policy loss: 0.232566. Value loss: 0.075118. Entropy: 0.299864.\n",
      "Iteration 6728: Policy loss: 0.229888. Value loss: 0.019868. Entropy: 0.298252.\n",
      "Iteration 6729: Policy loss: 0.219322. Value loss: 0.015969. Entropy: 0.296629.\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6730: Policy loss: 0.216770. Value loss: 0.057086. Entropy: 0.308304.\n",
      "Iteration 6731: Policy loss: 0.208846. Value loss: 0.030111. Entropy: 0.308297.\n",
      "Iteration 6732: Policy loss: 0.202084. Value loss: 0.024199. Entropy: 0.307533.\n",
      "episode: 2695   score: 285.0  epsilon: 1.0    steps: 312  evaluation reward: 362.0\n",
      "episode: 2696   score: 210.0  epsilon: 1.0    steps: 376  evaluation reward: 358.75\n",
      "episode: 2697   score: 225.0  epsilon: 1.0    steps: 512  evaluation reward: 358.25\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6733: Policy loss: -0.071655. Value loss: 0.251682. Entropy: 0.278366.\n",
      "Iteration 6734: Policy loss: -0.064437. Value loss: 0.124319. Entropy: 0.275811.\n",
      "Iteration 6735: Policy loss: -0.077341. Value loss: 0.088555. Entropy: 0.275784.\n",
      "episode: 2698   score: 620.0  epsilon: 1.0    steps: 176  evaluation reward: 358.55\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6736: Policy loss: 0.150830. Value loss: 0.119735. Entropy: 0.291382.\n",
      "Iteration 6737: Policy loss: 0.135930. Value loss: 0.054747. Entropy: 0.291592.\n",
      "Iteration 6738: Policy loss: 0.134777. Value loss: 0.039871. Entropy: 0.290219.\n",
      "episode: 2699   score: 680.0  epsilon: 1.0    steps: 56  evaluation reward: 361.4\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6739: Policy loss: 0.049284. Value loss: 0.058255. Entropy: 0.297456.\n",
      "Iteration 6740: Policy loss: 0.048554. Value loss: 0.027829. Entropy: 0.295283.\n",
      "Iteration 6741: Policy loss: 0.046619. Value loss: 0.020045. Entropy: 0.295612.\n",
      "episode: 2700   score: 185.0  epsilon: 1.0    steps: 88  evaluation reward: 360.1\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6742: Policy loss: 0.192580. Value loss: 0.054844. Entropy: 0.296900.\n",
      "Iteration 6743: Policy loss: 0.189576. Value loss: 0.027531. Entropy: 0.297067.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6744: Policy loss: 0.190255. Value loss: 0.022557. Entropy: 0.296897.\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6745: Policy loss: 0.029967. Value loss: 0.166002. Entropy: 0.308066.\n",
      "Iteration 6746: Policy loss: 0.019627. Value loss: 0.054791. Entropy: 0.308836.\n",
      "Iteration 6747: Policy loss: 0.011677. Value loss: 0.040468. Entropy: 0.307630.\n",
      "now time :  2019-09-05 21:13:55.411383\n",
      "episode: 2701   score: 455.0  epsilon: 1.0    steps: 512  evaluation reward: 359.0\n",
      "Training network. lr: 0.000199. clip: 0.079421\n",
      "Iteration 6748: Policy loss: 0.123612. Value loss: 0.060912. Entropy: 0.292124.\n",
      "Iteration 6749: Policy loss: 0.119567. Value loss: 0.027254. Entropy: 0.289713.\n",
      "Iteration 6750: Policy loss: 0.119072. Value loss: 0.020455. Entropy: 0.290365.\n",
      "episode: 2702   score: 215.0  epsilon: 1.0    steps: 864  evaluation reward: 356.95\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6751: Policy loss: 0.048739. Value loss: 0.095473. Entropy: 0.302572.\n",
      "Iteration 6752: Policy loss: 0.034217. Value loss: 0.039633. Entropy: 0.301276.\n",
      "Iteration 6753: Policy loss: 0.037057. Value loss: 0.028991. Entropy: 0.301473.\n",
      "episode: 2703   score: 290.0  epsilon: 1.0    steps: 616  evaluation reward: 357.45\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6754: Policy loss: 0.068158. Value loss: 0.123075. Entropy: 0.286925.\n",
      "Iteration 6755: Policy loss: 0.058035. Value loss: 0.045764. Entropy: 0.285940.\n",
      "Iteration 6756: Policy loss: 0.056009. Value loss: 0.031001. Entropy: 0.285384.\n",
      "episode: 2704   score: 210.0  epsilon: 1.0    steps: 952  evaluation reward: 357.25\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6757: Policy loss: 0.319703. Value loss: 0.105109. Entropy: 0.307575.\n",
      "Iteration 6758: Policy loss: 0.312024. Value loss: 0.042180. Entropy: 0.306854.\n",
      "Iteration 6759: Policy loss: 0.303837. Value loss: 0.033810. Entropy: 0.306694.\n",
      "episode: 2705   score: 315.0  epsilon: 1.0    steps: 48  evaluation reward: 357.55\n",
      "episode: 2706   score: 365.0  epsilon: 1.0    steps: 80  evaluation reward: 355.3\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6760: Policy loss: 0.340740. Value loss: 0.127221. Entropy: 0.270271.\n",
      "Iteration 6761: Policy loss: 0.326276. Value loss: 0.060583. Entropy: 0.266175.\n",
      "Iteration 6762: Policy loss: 0.315262. Value loss: 0.043672. Entropy: 0.266335.\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6763: Policy loss: -0.052967. Value loss: 0.077044. Entropy: 0.310498.\n",
      "Iteration 6764: Policy loss: -0.056050. Value loss: 0.032575. Entropy: 0.311559.\n",
      "Iteration 6765: Policy loss: -0.063141. Value loss: 0.026367. Entropy: 0.311538.\n",
      "episode: 2707   score: 420.0  epsilon: 1.0    steps: 480  evaluation reward: 356.6\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6766: Policy loss: 0.051248. Value loss: 0.105183. Entropy: 0.291958.\n",
      "Iteration 6767: Policy loss: 0.040052. Value loss: 0.050154. Entropy: 0.292694.\n",
      "Iteration 6768: Policy loss: 0.034119. Value loss: 0.038736. Entropy: 0.292298.\n",
      "episode: 2708   score: 285.0  epsilon: 1.0    steps: 24  evaluation reward: 353.8\n",
      "episode: 2709   score: 155.0  epsilon: 1.0    steps: 232  evaluation reward: 352.75\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6769: Policy loss: -0.254166. Value loss: 0.460028. Entropy: 0.284156.\n",
      "Iteration 6770: Policy loss: -0.270637. Value loss: 0.227994. Entropy: 0.280243.\n",
      "Iteration 6771: Policy loss: -0.299640. Value loss: 0.121850. Entropy: 0.286011.\n",
      "episode: 2710   score: 210.0  epsilon: 1.0    steps: 216  evaluation reward: 349.8\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6772: Policy loss: 0.139188. Value loss: 0.126677. Entropy: 0.303194.\n",
      "Iteration 6773: Policy loss: 0.126070. Value loss: 0.059535. Entropy: 0.303356.\n",
      "Iteration 6774: Policy loss: 0.118065. Value loss: 0.043500. Entropy: 0.302630.\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6775: Policy loss: -0.013812. Value loss: 0.189576. Entropy: 0.313057.\n",
      "Iteration 6776: Policy loss: -0.021788. Value loss: 0.083918. Entropy: 0.312592.\n",
      "Iteration 6777: Policy loss: -0.022764. Value loss: 0.052684. Entropy: 0.312999.\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6778: Policy loss: 0.065963. Value loss: 0.094281. Entropy: 0.308504.\n",
      "Iteration 6779: Policy loss: 0.064767. Value loss: 0.041153. Entropy: 0.310262.\n",
      "Iteration 6780: Policy loss: 0.061150. Value loss: 0.033889. Entropy: 0.309677.\n",
      "episode: 2711   score: 420.0  epsilon: 1.0    steps: 328  evaluation reward: 351.75\n",
      "episode: 2712   score: 330.0  epsilon: 1.0    steps: 440  evaluation reward: 352.45\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6781: Policy loss: 0.213780. Value loss: 0.040880. Entropy: 0.301869.\n",
      "Iteration 6782: Policy loss: 0.211162. Value loss: 0.018744. Entropy: 0.301802.\n",
      "Iteration 6783: Policy loss: 0.206526. Value loss: 0.015942. Entropy: 0.300223.\n",
      "episode: 2713   score: 230.0  epsilon: 1.0    steps: 800  evaluation reward: 351.15\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6784: Policy loss: -0.112781. Value loss: 0.103581. Entropy: 0.297569.\n",
      "Iteration 6785: Policy loss: -0.122701. Value loss: 0.044066. Entropy: 0.295462.\n",
      "Iteration 6786: Policy loss: -0.130472. Value loss: 0.032655. Entropy: 0.296119.\n",
      "episode: 2714   score: 240.0  epsilon: 1.0    steps: 592  evaluation reward: 351.1\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6787: Policy loss: -0.050257. Value loss: 0.101808. Entropy: 0.292358.\n",
      "Iteration 6788: Policy loss: -0.060587. Value loss: 0.042359. Entropy: 0.290455.\n",
      "Iteration 6789: Policy loss: -0.061167. Value loss: 0.030899. Entropy: 0.288986.\n",
      "episode: 2715   score: 250.0  epsilon: 1.0    steps: 120  evaluation reward: 349.45\n",
      "episode: 2716   score: 565.0  epsilon: 1.0    steps: 680  evaluation reward: 351.65\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6790: Policy loss: -0.138762. Value loss: 0.084088. Entropy: 0.283164.\n",
      "Iteration 6791: Policy loss: -0.148520. Value loss: 0.036668. Entropy: 0.283037.\n",
      "Iteration 6792: Policy loss: -0.146308. Value loss: 0.024067. Entropy: 0.282919.\n",
      "episode: 2717   score: 285.0  epsilon: 1.0    steps: 360  evaluation reward: 352.35\n",
      "episode: 2718   score: 665.0  epsilon: 1.0    steps: 912  evaluation reward: 356.2\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6793: Policy loss: 0.006056. Value loss: 0.136981. Entropy: 0.292444.\n",
      "Iteration 6794: Policy loss: -0.012884. Value loss: 0.052159. Entropy: 0.290879.\n",
      "Iteration 6795: Policy loss: -0.011443. Value loss: 0.029245. Entropy: 0.290511.\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6796: Policy loss: -0.110066. Value loss: 0.179174. Entropy: 0.303449.\n",
      "Iteration 6797: Policy loss: -0.111465. Value loss: 0.114349. Entropy: 0.303022.\n",
      "Iteration 6798: Policy loss: -0.123719. Value loss: 0.093684. Entropy: 0.303483.\n",
      "episode: 2719   score: 380.0  epsilon: 1.0    steps: 424  evaluation reward: 357.15\n",
      "Training network. lr: 0.000198. clip: 0.079273\n",
      "Iteration 6799: Policy loss: -0.202347. Value loss: 0.314813. Entropy: 0.294067.\n",
      "Iteration 6800: Policy loss: -0.227222. Value loss: 0.208618. Entropy: 0.292034.\n",
      "Iteration 6801: Policy loss: -0.240527. Value loss: 0.140222. Entropy: 0.293831.\n",
      "episode: 2720   score: 285.0  epsilon: 1.0    steps: 896  evaluation reward: 357.15\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6802: Policy loss: 0.154033. Value loss: 0.152395. Entropy: 0.300531.\n",
      "Iteration 6803: Policy loss: 0.143851. Value loss: 0.049569. Entropy: 0.300303.\n",
      "Iteration 6804: Policy loss: 0.133310. Value loss: 0.031865. Entropy: 0.299189.\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6805: Policy loss: 0.172761. Value loss: 0.068124. Entropy: 0.298960.\n",
      "Iteration 6806: Policy loss: 0.164814. Value loss: 0.038077. Entropy: 0.298334.\n",
      "Iteration 6807: Policy loss: 0.167375. Value loss: 0.028978. Entropy: 0.298049.\n",
      "episode: 2721   score: 240.0  epsilon: 1.0    steps: 152  evaluation reward: 357.15\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6808: Policy loss: 0.129021. Value loss: 0.093199. Entropy: 0.296882.\n",
      "Iteration 6809: Policy loss: 0.119100. Value loss: 0.034875. Entropy: 0.296155.\n",
      "Iteration 6810: Policy loss: 0.114926. Value loss: 0.026916. Entropy: 0.295276.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2722   score: 480.0  epsilon: 1.0    steps: 360  evaluation reward: 358.05\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6811: Policy loss: -0.138974. Value loss: 0.170069. Entropy: 0.294463.\n",
      "Iteration 6812: Policy loss: -0.148642. Value loss: 0.068764. Entropy: 0.293519.\n",
      "Iteration 6813: Policy loss: -0.144223. Value loss: 0.049739. Entropy: 0.292938.\n",
      "episode: 2723   score: 270.0  epsilon: 1.0    steps: 200  evaluation reward: 358.65\n",
      "episode: 2724   score: 285.0  epsilon: 1.0    steps: 848  evaluation reward: 358.75\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6814: Policy loss: 0.102817. Value loss: 0.079451. Entropy: 0.288372.\n",
      "Iteration 6815: Policy loss: 0.106880. Value loss: 0.030938. Entropy: 0.287111.\n",
      "Iteration 6816: Policy loss: 0.096691. Value loss: 0.022782. Entropy: 0.286680.\n",
      "episode: 2725   score: 520.0  epsilon: 1.0    steps: 744  evaluation reward: 357.4\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6817: Policy loss: -0.284679. Value loss: 0.199569. Entropy: 0.293793.\n",
      "Iteration 6818: Policy loss: -0.276899. Value loss: 0.078751. Entropy: 0.292594.\n",
      "Iteration 6819: Policy loss: -0.299984. Value loss: 0.052830. Entropy: 0.293432.\n",
      "episode: 2726   score: 795.0  epsilon: 1.0    steps: 496  evaluation reward: 362.65\n",
      "episode: 2727   score: 460.0  epsilon: 1.0    steps: 984  evaluation reward: 364.45\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6820: Policy loss: -0.039423. Value loss: 0.090887. Entropy: 0.291863.\n",
      "Iteration 6821: Policy loss: -0.050254. Value loss: 0.032896. Entropy: 0.291367.\n",
      "Iteration 6822: Policy loss: -0.053315. Value loss: 0.024936. Entropy: 0.292274.\n",
      "episode: 2728   score: 210.0  epsilon: 1.0    steps: 136  evaluation reward: 364.7\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6823: Policy loss: 0.726008. Value loss: 0.305383. Entropy: 0.283485.\n",
      "Iteration 6824: Policy loss: 0.723688. Value loss: 0.085950. Entropy: 0.283383.\n",
      "Iteration 6825: Policy loss: 0.693980. Value loss: 0.051860. Entropy: 0.283275.\n",
      "episode: 2729   score: 275.0  epsilon: 1.0    steps: 368  evaluation reward: 364.3\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6826: Policy loss: 0.183672. Value loss: 0.060913. Entropy: 0.296373.\n",
      "Iteration 6827: Policy loss: 0.184748. Value loss: 0.028805. Entropy: 0.295800.\n",
      "Iteration 6828: Policy loss: 0.176802. Value loss: 0.025526. Entropy: 0.295641.\n",
      "episode: 2730   score: 240.0  epsilon: 1.0    steps: 736  evaluation reward: 363.15\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6829: Policy loss: 0.015567. Value loss: 0.059905. Entropy: 0.295210.\n",
      "Iteration 6830: Policy loss: 0.014512. Value loss: 0.030949. Entropy: 0.294774.\n",
      "Iteration 6831: Policy loss: 0.015771. Value loss: 0.027249. Entropy: 0.294709.\n",
      "episode: 2731   score: 150.0  epsilon: 1.0    steps: 384  evaluation reward: 360.0\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6832: Policy loss: 0.035184. Value loss: 0.097883. Entropy: 0.294500.\n",
      "Iteration 6833: Policy loss: 0.014582. Value loss: 0.033834. Entropy: 0.292780.\n",
      "Iteration 6834: Policy loss: 0.015507. Value loss: 0.023356. Entropy: 0.293295.\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6835: Policy loss: -0.149533. Value loss: 0.316268. Entropy: 0.311649.\n",
      "Iteration 6836: Policy loss: -0.182995. Value loss: 0.087354. Entropy: 0.311709.\n",
      "Iteration 6837: Policy loss: -0.171830. Value loss: 0.057060. Entropy: 0.311435.\n",
      "episode: 2732   score: 425.0  epsilon: 1.0    steps: 312  evaluation reward: 359.6\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6838: Policy loss: 0.449533. Value loss: 0.178126. Entropy: 0.293302.\n",
      "Iteration 6839: Policy loss: 0.444991. Value loss: 0.049688. Entropy: 0.292812.\n",
      "Iteration 6840: Policy loss: 0.422466. Value loss: 0.031683. Entropy: 0.293692.\n",
      "episode: 2733   score: 260.0  epsilon: 1.0    steps: 256  evaluation reward: 356.45\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6841: Policy loss: -0.127585. Value loss: 0.078741. Entropy: 0.298106.\n",
      "Iteration 6842: Policy loss: -0.129849. Value loss: 0.034737. Entropy: 0.299208.\n",
      "Iteration 6843: Policy loss: -0.135580. Value loss: 0.025246. Entropy: 0.298564.\n",
      "episode: 2734   score: 240.0  epsilon: 1.0    steps: 120  evaluation reward: 356.25\n",
      "episode: 2735   score: 285.0  epsilon: 1.0    steps: 496  evaluation reward: 355.5\n",
      "episode: 2736   score: 285.0  epsilon: 1.0    steps: 648  evaluation reward: 355.7\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6844: Policy loss: -0.067184. Value loss: 0.065351. Entropy: 0.269501.\n",
      "Iteration 6845: Policy loss: -0.070769. Value loss: 0.044618. Entropy: 0.268531.\n",
      "Iteration 6846: Policy loss: -0.076127. Value loss: 0.035982. Entropy: 0.269547.\n",
      "episode: 2737   score: 210.0  epsilon: 1.0    steps: 960  evaluation reward: 353.75\n",
      "episode: 2738   score: 385.0  epsilon: 1.0    steps: 968  evaluation reward: 352.6\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6847: Policy loss: -0.184797. Value loss: 0.385169. Entropy: 0.308870.\n",
      "Iteration 6848: Policy loss: -0.217052. Value loss: 0.187582. Entropy: 0.306546.\n",
      "Iteration 6849: Policy loss: -0.209666. Value loss: 0.120203. Entropy: 0.306498.\n",
      "Training network. lr: 0.000198. clip: 0.079117\n",
      "Iteration 6850: Policy loss: 0.176332. Value loss: 0.067452. Entropy: 0.292814.\n",
      "Iteration 6851: Policy loss: 0.177852. Value loss: 0.037457. Entropy: 0.293122.\n",
      "Iteration 6852: Policy loss: 0.170089. Value loss: 0.031232. Entropy: 0.293751.\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6853: Policy loss: -0.090877. Value loss: 0.233560. Entropy: 0.309949.\n",
      "Iteration 6854: Policy loss: -0.099620. Value loss: 0.079787. Entropy: 0.308156.\n",
      "Iteration 6855: Policy loss: -0.120371. Value loss: 0.047727. Entropy: 0.308723.\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6856: Policy loss: -0.057373. Value loss: 0.258491. Entropy: 0.308121.\n",
      "Iteration 6857: Policy loss: -0.075282. Value loss: 0.151548. Entropy: 0.307832.\n",
      "Iteration 6858: Policy loss: -0.077942. Value loss: 0.095269. Entropy: 0.307793.\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6859: Policy loss: 0.189991. Value loss: 0.228727. Entropy: 0.309528.\n",
      "Iteration 6860: Policy loss: 0.171737. Value loss: 0.061435. Entropy: 0.307903.\n",
      "Iteration 6861: Policy loss: 0.156316. Value loss: 0.030646. Entropy: 0.307758.\n",
      "episode: 2739   score: 815.0  epsilon: 1.0    steps: 56  evaluation reward: 356.95\n",
      "episode: 2740   score: 270.0  epsilon: 1.0    steps: 1016  evaluation reward: 356.95\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6862: Policy loss: 0.010275. Value loss: 0.215031. Entropy: 0.294819.\n",
      "Iteration 6863: Policy loss: -0.012668. Value loss: 0.071972. Entropy: 0.296423.\n",
      "Iteration 6864: Policy loss: -0.013152. Value loss: 0.043149. Entropy: 0.294833.\n",
      "episode: 2741   score: 410.0  epsilon: 1.0    steps: 96  evaluation reward: 357.9\n",
      "episode: 2742   score: 490.0  epsilon: 1.0    steps: 592  evaluation reward: 359.9\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6865: Policy loss: -0.248830. Value loss: 0.087651. Entropy: 0.272624.\n",
      "Iteration 6866: Policy loss: -0.254248. Value loss: 0.040426. Entropy: 0.273454.\n",
      "Iteration 6867: Policy loss: -0.258188. Value loss: 0.028016. Entropy: 0.273254.\n",
      "episode: 2743   score: 210.0  epsilon: 1.0    steps: 328  evaluation reward: 357.05\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6868: Policy loss: 0.138028. Value loss: 0.237119. Entropy: 0.294697.\n",
      "Iteration 6869: Policy loss: 0.139324. Value loss: 0.070326. Entropy: 0.294481.\n",
      "Iteration 6870: Policy loss: 0.119860. Value loss: 0.045733. Entropy: 0.294558.\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6871: Policy loss: 0.273179. Value loss: 0.109372. Entropy: 0.310621.\n",
      "Iteration 6872: Policy loss: 0.268632. Value loss: 0.049307. Entropy: 0.310682.\n",
      "Iteration 6873: Policy loss: 0.260602. Value loss: 0.034937. Entropy: 0.310531.\n",
      "episode: 2744   score: 425.0  epsilon: 1.0    steps: 64  evaluation reward: 359.75\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6874: Policy loss: -0.072299. Value loss: 0.118163. Entropy: 0.293471.\n",
      "Iteration 6875: Policy loss: -0.080613. Value loss: 0.044422. Entropy: 0.293775.\n",
      "Iteration 6876: Policy loss: -0.079562. Value loss: 0.033782. Entropy: 0.293765.\n",
      "episode: 2745   score: 420.0  epsilon: 1.0    steps: 64  evaluation reward: 360.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6877: Policy loss: -0.155723. Value loss: 0.153435. Entropy: 0.294665.\n",
      "Iteration 6878: Policy loss: -0.154904. Value loss: 0.083133. Entropy: 0.294031.\n",
      "Iteration 6879: Policy loss: -0.161788. Value loss: 0.058360. Entropy: 0.294632.\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6880: Policy loss: 0.584936. Value loss: 0.286450. Entropy: 0.308066.\n",
      "Iteration 6881: Policy loss: 0.571497. Value loss: 0.079748. Entropy: 0.306601.\n",
      "Iteration 6882: Policy loss: 0.550194. Value loss: 0.047436. Entropy: 0.306790.\n",
      "episode: 2746   score: 210.0  epsilon: 1.0    steps: 264  evaluation reward: 359.7\n",
      "episode: 2747   score: 530.0  epsilon: 1.0    steps: 560  evaluation reward: 362.35\n",
      "episode: 2748   score: 230.0  epsilon: 1.0    steps: 960  evaluation reward: 362.15\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6883: Policy loss: 0.157600. Value loss: 0.124481. Entropy: 0.277915.\n",
      "Iteration 6884: Policy loss: 0.148046. Value loss: 0.043333. Entropy: 0.276576.\n",
      "Iteration 6885: Policy loss: 0.154192. Value loss: 0.032819. Entropy: 0.276754.\n",
      "episode: 2749   score: 345.0  epsilon: 1.0    steps: 696  evaluation reward: 361.7\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6886: Policy loss: -0.242558. Value loss: 0.252528. Entropy: 0.288294.\n",
      "Iteration 6887: Policy loss: -0.254146. Value loss: 0.082621. Entropy: 0.289645.\n",
      "Iteration 6888: Policy loss: -0.260854. Value loss: 0.061771. Entropy: 0.288614.\n",
      "episode: 2750   score: 280.0  epsilon: 1.0    steps: 536  evaluation reward: 362.4\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6889: Policy loss: -0.052876. Value loss: 0.122120. Entropy: 0.299548.\n",
      "Iteration 6890: Policy loss: -0.063515. Value loss: 0.065622. Entropy: 0.300173.\n",
      "Iteration 6891: Policy loss: -0.066511. Value loss: 0.051071. Entropy: 0.302299.\n",
      "now time :  2019-09-05 21:22:49.578923\n",
      "episode: 2751   score: 370.0  epsilon: 1.0    steps: 472  evaluation reward: 363.7\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6892: Policy loss: 0.071160. Value loss: 0.145354. Entropy: 0.294281.\n",
      "Iteration 6893: Policy loss: 0.057310. Value loss: 0.050300. Entropy: 0.294195.\n",
      "Iteration 6894: Policy loss: 0.055069. Value loss: 0.033037. Entropy: 0.295404.\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6895: Policy loss: -0.232548. Value loss: 0.148593. Entropy: 0.307178.\n",
      "Iteration 6896: Policy loss: -0.253432. Value loss: 0.047561. Entropy: 0.307651.\n",
      "Iteration 6897: Policy loss: -0.238894. Value loss: 0.029126. Entropy: 0.306623.\n",
      "episode: 2752   score: 415.0  epsilon: 1.0    steps: 72  evaluation reward: 361.65\n",
      "Training network. lr: 0.000197. clip: 0.078960\n",
      "Iteration 6898: Policy loss: 0.178340. Value loss: 0.352747. Entropy: 0.296574.\n",
      "Iteration 6899: Policy loss: 0.181049. Value loss: 0.101531. Entropy: 0.295770.\n",
      "Iteration 6900: Policy loss: 0.174547. Value loss: 0.055621. Entropy: 0.295814.\n",
      "episode: 2753   score: 360.0  epsilon: 1.0    steps: 896  evaluation reward: 360.3\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6901: Policy loss: 0.212510. Value loss: 0.081260. Entropy: 0.306521.\n",
      "Iteration 6902: Policy loss: 0.203231. Value loss: 0.037739. Entropy: 0.306688.\n",
      "Iteration 6903: Policy loss: 0.200475. Value loss: 0.029778. Entropy: 0.304604.\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6904: Policy loss: 0.169764. Value loss: 0.152272. Entropy: 0.300434.\n",
      "Iteration 6905: Policy loss: 0.164045. Value loss: 0.042752. Entropy: 0.300102.\n",
      "Iteration 6906: Policy loss: 0.154596. Value loss: 0.027502. Entropy: 0.301580.\n",
      "episode: 2754   score: 265.0  epsilon: 1.0    steps: 360  evaluation reward: 359.65\n",
      "episode: 2755   score: 165.0  epsilon: 1.0    steps: 432  evaluation reward: 358.45\n",
      "episode: 2756   score: 225.0  epsilon: 1.0    steps: 808  evaluation reward: 358.9\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6907: Policy loss: 0.266688. Value loss: 0.082912. Entropy: 0.281129.\n",
      "Iteration 6908: Policy loss: 0.247756. Value loss: 0.030905. Entropy: 0.283126.\n",
      "Iteration 6909: Policy loss: 0.247157. Value loss: 0.024657. Entropy: 0.281566.\n",
      "episode: 2757   score: 590.0  epsilon: 1.0    steps: 16  evaluation reward: 363.0\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6910: Policy loss: -0.006339. Value loss: 0.144364. Entropy: 0.300273.\n",
      "Iteration 6911: Policy loss: -0.020199. Value loss: 0.045444. Entropy: 0.300341.\n",
      "Iteration 6912: Policy loss: -0.025388. Value loss: 0.030868. Entropy: 0.300168.\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6913: Policy loss: 0.023490. Value loss: 0.100805. Entropy: 0.309867.\n",
      "Iteration 6914: Policy loss: 0.017550. Value loss: 0.049371. Entropy: 0.308846.\n",
      "Iteration 6915: Policy loss: 0.011178. Value loss: 0.040835. Entropy: 0.308883.\n",
      "episode: 2758   score: 595.0  epsilon: 1.0    steps: 696  evaluation reward: 364.75\n",
      "episode: 2759   score: 180.0  epsilon: 1.0    steps: 968  evaluation reward: 364.1\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6916: Policy loss: 0.148619. Value loss: 0.113434. Entropy: 0.292115.\n",
      "Iteration 6917: Policy loss: 0.144493. Value loss: 0.069596. Entropy: 0.290148.\n",
      "Iteration 6918: Policy loss: 0.127951. Value loss: 0.060889. Entropy: 0.291044.\n",
      "episode: 2760   score: 320.0  epsilon: 1.0    steps: 168  evaluation reward: 363.95\n",
      "episode: 2761   score: 255.0  epsilon: 1.0    steps: 1024  evaluation reward: 362.35\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6919: Policy loss: 0.060733. Value loss: 0.067017. Entropy: 0.292953.\n",
      "Iteration 6920: Policy loss: 0.052127. Value loss: 0.031932. Entropy: 0.290500.\n",
      "Iteration 6921: Policy loss: 0.051666. Value loss: 0.024473. Entropy: 0.290443.\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6922: Policy loss: -0.339633. Value loss: 0.200125. Entropy: 0.303394.\n",
      "Iteration 6923: Policy loss: -0.353648. Value loss: 0.075520. Entropy: 0.302507.\n",
      "Iteration 6924: Policy loss: -0.368919. Value loss: 0.048683. Entropy: 0.302771.\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6925: Policy loss: -0.576279. Value loss: 0.437929. Entropy: 0.309360.\n",
      "Iteration 6926: Policy loss: -0.576513. Value loss: 0.250038. Entropy: 0.306612.\n",
      "Iteration 6927: Policy loss: -0.609922. Value loss: 0.173820. Entropy: 0.306681.\n",
      "episode: 2762   score: 460.0  epsilon: 1.0    steps: 504  evaluation reward: 364.45\n",
      "episode: 2763   score: 460.0  epsilon: 1.0    steps: 1024  evaluation reward: 363.4\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6928: Policy loss: 0.144358. Value loss: 0.277940. Entropy: 0.298926.\n",
      "Iteration 6929: Policy loss: 0.134492. Value loss: 0.075213. Entropy: 0.296475.\n",
      "Iteration 6930: Policy loss: 0.134526. Value loss: 0.041700. Entropy: 0.296518.\n",
      "episode: 2764   score: 470.0  epsilon: 1.0    steps: 680  evaluation reward: 362.95\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6931: Policy loss: 0.282654. Value loss: 0.114487. Entropy: 0.297245.\n",
      "Iteration 6932: Policy loss: 0.278510. Value loss: 0.036915. Entropy: 0.297177.\n",
      "Iteration 6933: Policy loss: 0.272365. Value loss: 0.024628. Entropy: 0.298460.\n",
      "episode: 2765   score: 275.0  epsilon: 1.0    steps: 120  evaluation reward: 362.8\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6934: Policy loss: 0.015387. Value loss: 0.094598. Entropy: 0.308794.\n",
      "Iteration 6935: Policy loss: 0.004265. Value loss: 0.038584. Entropy: 0.309111.\n",
      "Iteration 6936: Policy loss: -0.001075. Value loss: 0.028623. Entropy: 0.309649.\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6937: Policy loss: 0.219955. Value loss: 0.131047. Entropy: 0.307232.\n",
      "Iteration 6938: Policy loss: 0.210715. Value loss: 0.037581. Entropy: 0.307922.\n",
      "Iteration 6939: Policy loss: 0.215310. Value loss: 0.033953. Entropy: 0.307733.\n",
      "episode: 2766   score: 275.0  epsilon: 1.0    steps: 752  evaluation reward: 360.4\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6940: Policy loss: -0.010984. Value loss: 0.173910. Entropy: 0.293673.\n",
      "Iteration 6941: Policy loss: -0.028261. Value loss: 0.072735. Entropy: 0.292127.\n",
      "Iteration 6942: Policy loss: -0.022465. Value loss: 0.053824. Entropy: 0.293928.\n",
      "episode: 2767   score: 260.0  epsilon: 1.0    steps: 656  evaluation reward: 360.1\n",
      "episode: 2768   score: 335.0  epsilon: 1.0    steps: 688  evaluation reward: 357.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6943: Policy loss: 0.191665. Value loss: 0.091963. Entropy: 0.296752.\n",
      "Iteration 6944: Policy loss: 0.177624. Value loss: 0.031838. Entropy: 0.295516.\n",
      "Iteration 6945: Policy loss: 0.178449. Value loss: 0.022606. Entropy: 0.293527.\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6946: Policy loss: 0.024471. Value loss: 0.082629. Entropy: 0.312686.\n",
      "Iteration 6947: Policy loss: 0.024501. Value loss: 0.045262. Entropy: 0.312080.\n",
      "Iteration 6948: Policy loss: 0.019736. Value loss: 0.036130. Entropy: 0.312287.\n",
      "episode: 2769   score: 565.0  epsilon: 1.0    steps: 872  evaluation reward: 360.75\n",
      "Training network. lr: 0.000197. clip: 0.078812\n",
      "Iteration 6949: Policy loss: 0.164586. Value loss: 0.155465. Entropy: 0.302590.\n",
      "Iteration 6950: Policy loss: 0.142604. Value loss: 0.059512. Entropy: 0.301399.\n",
      "Iteration 6951: Policy loss: 0.139820. Value loss: 0.039181. Entropy: 0.300479.\n",
      "episode: 2770   score: 240.0  epsilon: 1.0    steps: 480  evaluation reward: 357.8\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6952: Policy loss: 0.001358. Value loss: 0.071442. Entropy: 0.292888.\n",
      "Iteration 6953: Policy loss: 0.002288. Value loss: 0.033442. Entropy: 0.291762.\n",
      "Iteration 6954: Policy loss: -0.009995. Value loss: 0.026805. Entropy: 0.292007.\n",
      "episode: 2771   score: 260.0  epsilon: 1.0    steps: 728  evaluation reward: 356.75\n",
      "episode: 2772   score: 365.0  epsilon: 1.0    steps: 968  evaluation reward: 357.55\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6955: Policy loss: -0.098879. Value loss: 0.298125. Entropy: 0.299959.\n",
      "Iteration 6956: Policy loss: -0.116374. Value loss: 0.183543. Entropy: 0.298412.\n",
      "Iteration 6957: Policy loss: -0.124994. Value loss: 0.122683. Entropy: 0.299248.\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6958: Policy loss: -0.054772. Value loss: 0.067532. Entropy: 0.297029.\n",
      "Iteration 6959: Policy loss: -0.062455. Value loss: 0.027914. Entropy: 0.296167.\n",
      "Iteration 6960: Policy loss: -0.064754. Value loss: 0.021674. Entropy: 0.296089.\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6961: Policy loss: 0.015180. Value loss: 0.100834. Entropy: 0.308799.\n",
      "Iteration 6962: Policy loss: 0.002226. Value loss: 0.039855. Entropy: 0.308358.\n",
      "Iteration 6963: Policy loss: -0.004315. Value loss: 0.024872. Entropy: 0.308101.\n",
      "episode: 2773   score: 450.0  epsilon: 1.0    steps: 216  evaluation reward: 355.4\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6964: Policy loss: -0.380369. Value loss: 0.276101. Entropy: 0.292069.\n",
      "Iteration 6965: Policy loss: -0.394310. Value loss: 0.146751. Entropy: 0.291333.\n",
      "Iteration 6966: Policy loss: -0.390501. Value loss: 0.067545. Entropy: 0.291405.\n",
      "episode: 2774   score: 335.0  epsilon: 1.0    steps: 56  evaluation reward: 353.4\n",
      "episode: 2775   score: 410.0  epsilon: 1.0    steps: 808  evaluation reward: 355.7\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6967: Policy loss: 0.400427. Value loss: 0.148081. Entropy: 0.289270.\n",
      "Iteration 6968: Policy loss: 0.387334. Value loss: 0.045157. Entropy: 0.287862.\n",
      "Iteration 6969: Policy loss: 0.379280. Value loss: 0.028845. Entropy: 0.287546.\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6970: Policy loss: -0.125058. Value loss: 0.110595. Entropy: 0.306290.\n",
      "Iteration 6971: Policy loss: -0.136373. Value loss: 0.079432. Entropy: 0.304692.\n",
      "Iteration 6972: Policy loss: -0.138041. Value loss: 0.046876. Entropy: 0.305548.\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6973: Policy loss: -0.165089. Value loss: 0.287899. Entropy: 0.317382.\n",
      "Iteration 6974: Policy loss: -0.164380. Value loss: 0.105077. Entropy: 0.316174.\n",
      "Iteration 6975: Policy loss: -0.166640. Value loss: 0.064698. Entropy: 0.314333.\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6976: Policy loss: -0.124962. Value loss: 0.213945. Entropy: 0.305886.\n",
      "Iteration 6977: Policy loss: -0.149460. Value loss: 0.073391. Entropy: 0.307820.\n",
      "Iteration 6978: Policy loss: -0.147419. Value loss: 0.036340. Entropy: 0.307515.\n",
      "episode: 2776   score: 620.0  epsilon: 1.0    steps: 152  evaluation reward: 357.15\n",
      "episode: 2777   score: 450.0  epsilon: 1.0    steps: 512  evaluation reward: 358.5\n",
      "episode: 2778   score: 290.0  epsilon: 1.0    steps: 672  evaluation reward: 358.05\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6979: Policy loss: 0.243563. Value loss: 0.200791. Entropy: 0.274470.\n",
      "Iteration 6980: Policy loss: 0.231294. Value loss: 0.053365. Entropy: 0.273470.\n",
      "Iteration 6981: Policy loss: 0.216715. Value loss: 0.037571. Entropy: 0.273171.\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6982: Policy loss: 0.343726. Value loss: 0.082228. Entropy: 0.317969.\n",
      "Iteration 6983: Policy loss: 0.336975. Value loss: 0.031711. Entropy: 0.317552.\n",
      "Iteration 6984: Policy loss: 0.318530. Value loss: 0.025245. Entropy: 0.317117.\n",
      "episode: 2779   score: 595.0  epsilon: 1.0    steps: 808  evaluation reward: 359.4\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6985: Policy loss: 0.049711. Value loss: 0.151638. Entropy: 0.301162.\n",
      "Iteration 6986: Policy loss: 0.041978. Value loss: 0.075274. Entropy: 0.302292.\n",
      "Iteration 6987: Policy loss: 0.037043. Value loss: 0.058650. Entropy: 0.301550.\n",
      "episode: 2780   score: 425.0  epsilon: 1.0    steps: 440  evaluation reward: 356.15\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6988: Policy loss: 0.345529. Value loss: 0.155738. Entropy: 0.295399.\n",
      "Iteration 6989: Policy loss: 0.338805. Value loss: 0.054686. Entropy: 0.296141.\n",
      "Iteration 6990: Policy loss: 0.331735. Value loss: 0.037567. Entropy: 0.295209.\n",
      "episode: 2781   score: 390.0  epsilon: 1.0    steps: 152  evaluation reward: 356.95\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6991: Policy loss: -0.238622. Value loss: 0.216923. Entropy: 0.299617.\n",
      "Iteration 6992: Policy loss: -0.252913. Value loss: 0.101134. Entropy: 0.299168.\n",
      "Iteration 6993: Policy loss: -0.240867. Value loss: 0.077232. Entropy: 0.299157.\n",
      "episode: 2782   score: 375.0  epsilon: 1.0    steps: 88  evaluation reward: 358.6\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6994: Policy loss: -0.166879. Value loss: 0.183811. Entropy: 0.294713.\n",
      "Iteration 6995: Policy loss: -0.196652. Value loss: 0.082147. Entropy: 0.294991.\n",
      "Iteration 6996: Policy loss: -0.206073. Value loss: 0.034050. Entropy: 0.295791.\n",
      "episode: 2783   score: 590.0  epsilon: 1.0    steps: 624  evaluation reward: 359.9\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 6997: Policy loss: -0.070244. Value loss: 0.110743. Entropy: 0.295570.\n",
      "Iteration 6998: Policy loss: -0.070186. Value loss: 0.036582. Entropy: 0.295155.\n",
      "Iteration 6999: Policy loss: -0.083108. Value loss: 0.024422. Entropy: 0.295776.\n",
      "Training network. lr: 0.000197. clip: 0.078656\n",
      "Iteration 7000: Policy loss: 0.440587. Value loss: 0.216162. Entropy: 0.312179.\n",
      "Iteration 7001: Policy loss: 0.428097. Value loss: 0.064620. Entropy: 0.312440.\n",
      "Iteration 7002: Policy loss: 0.430765. Value loss: 0.041480. Entropy: 0.311253.\n",
      "episode: 2784   score: 460.0  epsilon: 1.0    steps: 576  evaluation reward: 359.15\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7003: Policy loss: 0.204090. Value loss: 0.113347. Entropy: 0.299370.\n",
      "Iteration 7004: Policy loss: 0.201816. Value loss: 0.038971. Entropy: 0.299903.\n",
      "Iteration 7005: Policy loss: 0.193367. Value loss: 0.024888. Entropy: 0.298596.\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7006: Policy loss: -0.052713. Value loss: 0.255325. Entropy: 0.313795.\n",
      "Iteration 7007: Policy loss: -0.073039. Value loss: 0.160427. Entropy: 0.314780.\n",
      "Iteration 7008: Policy loss: -0.081298. Value loss: 0.134702. Entropy: 0.313321.\n",
      "episode: 2785   score: 245.0  epsilon: 1.0    steps: 624  evaluation reward: 359.0\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7009: Policy loss: 0.122394. Value loss: 0.197308. Entropy: 0.296773.\n",
      "Iteration 7010: Policy loss: 0.107709. Value loss: 0.069767. Entropy: 0.295076.\n",
      "Iteration 7011: Policy loss: 0.109946. Value loss: 0.040914. Entropy: 0.295183.\n",
      "episode: 2786   score: 360.0  epsilon: 1.0    steps: 224  evaluation reward: 360.5\n",
      "episode: 2787   score: 345.0  epsilon: 1.0    steps: 392  evaluation reward: 361.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2788   score: 590.0  epsilon: 1.0    steps: 920  evaluation reward: 361.95\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7012: Policy loss: -0.209362. Value loss: 0.288209. Entropy: 0.276993.\n",
      "Iteration 7013: Policy loss: -0.218640. Value loss: 0.134835. Entropy: 0.276444.\n",
      "Iteration 7014: Policy loss: -0.222215. Value loss: 0.076305. Entropy: 0.275395.\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7015: Policy loss: -0.100707. Value loss: 0.072090. Entropy: 0.306639.\n",
      "Iteration 7016: Policy loss: -0.103475. Value loss: 0.036552. Entropy: 0.308015.\n",
      "Iteration 7017: Policy loss: -0.109795. Value loss: 0.030492. Entropy: 0.306321.\n",
      "episode: 2789   score: 445.0  epsilon: 1.0    steps: 904  evaluation reward: 363.55\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7018: Policy loss: -0.006016. Value loss: 0.107458. Entropy: 0.305319.\n",
      "Iteration 7019: Policy loss: -0.011599. Value loss: 0.053414. Entropy: 0.306396.\n",
      "Iteration 7020: Policy loss: -0.009338. Value loss: 0.033024. Entropy: 0.305967.\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7021: Policy loss: 0.539022. Value loss: 0.213168. Entropy: 0.306576.\n",
      "Iteration 7022: Policy loss: 0.511620. Value loss: 0.053022. Entropy: 0.307194.\n",
      "Iteration 7023: Policy loss: 0.506791. Value loss: 0.028431. Entropy: 0.305511.\n",
      "episode: 2790   score: 365.0  epsilon: 1.0    steps: 416  evaluation reward: 362.8\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7024: Policy loss: 0.231682. Value loss: 0.130006. Entropy: 0.297459.\n",
      "Iteration 7025: Policy loss: 0.229014. Value loss: 0.061279. Entropy: 0.295206.\n",
      "Iteration 7026: Policy loss: 0.232322. Value loss: 0.041842. Entropy: 0.296127.\n",
      "episode: 2791   score: 475.0  epsilon: 1.0    steps: 552  evaluation reward: 362.95\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7027: Policy loss: -0.026345. Value loss: 0.110079. Entropy: 0.302964.\n",
      "Iteration 7028: Policy loss: -0.033114. Value loss: 0.052790. Entropy: 0.302081.\n",
      "Iteration 7029: Policy loss: -0.038001. Value loss: 0.039617. Entropy: 0.300384.\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7030: Policy loss: 0.179049. Value loss: 0.130466. Entropy: 0.310086.\n",
      "Iteration 7031: Policy loss: 0.171088. Value loss: 0.062995. Entropy: 0.310389.\n",
      "Iteration 7032: Policy loss: 0.171454. Value loss: 0.042461. Entropy: 0.309193.\n",
      "episode: 2792   score: 635.0  epsilon: 1.0    steps: 280  evaluation reward: 367.0\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7033: Policy loss: 0.268046. Value loss: 0.102672. Entropy: 0.294660.\n",
      "Iteration 7034: Policy loss: 0.268645. Value loss: 0.035607. Entropy: 0.294288.\n",
      "Iteration 7035: Policy loss: 0.260853. Value loss: 0.024930. Entropy: 0.294657.\n",
      "episode: 2793   score: 310.0  epsilon: 1.0    steps: 48  evaluation reward: 365.35\n",
      "episode: 2794   score: 210.0  epsilon: 1.0    steps: 664  evaluation reward: 365.0\n",
      "episode: 2795   score: 260.0  epsilon: 1.0    steps: 864  evaluation reward: 364.75\n",
      "episode: 2796   score: 320.0  epsilon: 1.0    steps: 976  evaluation reward: 365.85\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7036: Policy loss: 0.112547. Value loss: 0.077159. Entropy: 0.280863.\n",
      "Iteration 7037: Policy loss: 0.110936. Value loss: 0.035274. Entropy: 0.282949.\n",
      "Iteration 7038: Policy loss: 0.106361. Value loss: 0.026265. Entropy: 0.282897.\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7039: Policy loss: -0.110874. Value loss: 0.110540. Entropy: 0.299263.\n",
      "Iteration 7040: Policy loss: -0.120479. Value loss: 0.052711. Entropy: 0.303656.\n",
      "Iteration 7041: Policy loss: -0.120993. Value loss: 0.041826. Entropy: 0.303694.\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7042: Policy loss: 0.066600. Value loss: 0.100137. Entropy: 0.318895.\n",
      "Iteration 7043: Policy loss: 0.060520. Value loss: 0.038112. Entropy: 0.318670.\n",
      "Iteration 7044: Policy loss: 0.061999. Value loss: 0.026912. Entropy: 0.318881.\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7045: Policy loss: 0.136646. Value loss: 0.050962. Entropy: 0.309252.\n",
      "Iteration 7046: Policy loss: 0.132493. Value loss: 0.022726. Entropy: 0.307986.\n",
      "Iteration 7047: Policy loss: 0.131318. Value loss: 0.015401. Entropy: 0.307878.\n",
      "episode: 2797   score: 440.0  epsilon: 1.0    steps: 296  evaluation reward: 368.0\n",
      "episode: 2798   score: 210.0  epsilon: 1.0    steps: 800  evaluation reward: 363.9\n",
      "Training network. lr: 0.000196. clip: 0.078499\n",
      "Iteration 7048: Policy loss: -0.214853. Value loss: 0.335227. Entropy: 0.295997.\n",
      "Iteration 7049: Policy loss: -0.251308. Value loss: 0.276650. Entropy: 0.295148.\n",
      "Iteration 7050: Policy loss: -0.242155. Value loss: 0.216104. Entropy: 0.296131.\n",
      "episode: 2799   score: 240.0  epsilon: 1.0    steps: 168  evaluation reward: 359.5\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7051: Policy loss: -0.188013. Value loss: 0.293431. Entropy: 0.301213.\n",
      "Iteration 7052: Policy loss: -0.188267. Value loss: 0.164892. Entropy: 0.300892.\n",
      "Iteration 7053: Policy loss: -0.215334. Value loss: 0.102458. Entropy: 0.301174.\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7054: Policy loss: -0.313600. Value loss: 0.236671. Entropy: 0.308248.\n",
      "Iteration 7055: Policy loss: -0.311149. Value loss: 0.108745. Entropy: 0.307554.\n",
      "Iteration 7056: Policy loss: -0.330496. Value loss: 0.085825. Entropy: 0.307393.\n",
      "episode: 2800   score: 300.0  epsilon: 1.0    steps: 744  evaluation reward: 360.65\n",
      "now time :  2019-09-05 21:33:02.932015\n",
      "episode: 2801   score: 580.0  epsilon: 1.0    steps: 872  evaluation reward: 361.9\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7057: Policy loss: 0.168241. Value loss: 0.092204. Entropy: 0.302533.\n",
      "Iteration 7058: Policy loss: 0.162081. Value loss: 0.040575. Entropy: 0.301769.\n",
      "Iteration 7059: Policy loss: 0.156915. Value loss: 0.029806. Entropy: 0.302160.\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7060: Policy loss: 0.374021. Value loss: 0.164675. Entropy: 0.314182.\n",
      "Iteration 7061: Policy loss: 0.347812. Value loss: 0.047054. Entropy: 0.314718.\n",
      "Iteration 7062: Policy loss: 0.362353. Value loss: 0.030270. Entropy: 0.314226.\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7063: Policy loss: 0.289119. Value loss: 0.081575. Entropy: 0.308410.\n",
      "Iteration 7064: Policy loss: 0.278568. Value loss: 0.037796. Entropy: 0.306504.\n",
      "Iteration 7065: Policy loss: 0.282872. Value loss: 0.028781. Entropy: 0.307039.\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7066: Policy loss: 0.037773. Value loss: 0.057286. Entropy: 0.311785.\n",
      "Iteration 7067: Policy loss: 0.032725. Value loss: 0.027868. Entropy: 0.312700.\n",
      "Iteration 7068: Policy loss: 0.027639. Value loss: 0.018491. Entropy: 0.312506.\n",
      "episode: 2802   score: 560.0  epsilon: 1.0    steps: 784  evaluation reward: 365.35\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7069: Policy loss: -0.171989. Value loss: 0.395297. Entropy: 0.304262.\n",
      "Iteration 7070: Policy loss: -0.167316. Value loss: 0.252725. Entropy: 0.304763.\n",
      "Iteration 7071: Policy loss: -0.183444. Value loss: 0.178888. Entropy: 0.304310.\n",
      "episode: 2803   score: 595.0  epsilon: 1.0    steps: 240  evaluation reward: 368.4\n",
      "episode: 2804   score: 250.0  epsilon: 1.0    steps: 304  evaluation reward: 368.8\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7072: Policy loss: 0.055829. Value loss: 0.120699. Entropy: 0.279049.\n",
      "Iteration 7073: Policy loss: 0.040538. Value loss: 0.052826. Entropy: 0.280485.\n",
      "Iteration 7074: Policy loss: 0.041484. Value loss: 0.039021. Entropy: 0.280150.\n",
      "episode: 2805   score: 315.0  epsilon: 1.0    steps: 464  evaluation reward: 368.8\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7075: Policy loss: 0.153867. Value loss: 0.077802. Entropy: 0.295187.\n",
      "Iteration 7076: Policy loss: 0.149077. Value loss: 0.031972. Entropy: 0.293461.\n",
      "Iteration 7077: Policy loss: 0.145063. Value loss: 0.024228. Entropy: 0.294814.\n",
      "episode: 2806   score: 295.0  epsilon: 1.0    steps: 136  evaluation reward: 368.1\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7078: Policy loss: -0.114839. Value loss: 0.119734. Entropy: 0.291925.\n",
      "Iteration 7079: Policy loss: -0.125771. Value loss: 0.052826. Entropy: 0.290515.\n",
      "Iteration 7080: Policy loss: -0.122171. Value loss: 0.033983. Entropy: 0.290695.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2807   score: 555.0  epsilon: 1.0    steps: 400  evaluation reward: 369.45\n",
      "episode: 2808   score: 260.0  epsilon: 1.0    steps: 456  evaluation reward: 369.2\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7081: Policy loss: 0.032172. Value loss: 0.093709. Entropy: 0.282434.\n",
      "Iteration 7082: Policy loss: 0.022628. Value loss: 0.056114. Entropy: 0.282188.\n",
      "Iteration 7083: Policy loss: 0.015903. Value loss: 0.048120. Entropy: 0.281364.\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7084: Policy loss: -0.539555. Value loss: 0.357075. Entropy: 0.311869.\n",
      "Iteration 7085: Policy loss: -0.538464. Value loss: 0.100751. Entropy: 0.312410.\n",
      "Iteration 7086: Policy loss: -0.550501. Value loss: 0.049517. Entropy: 0.310798.\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7087: Policy loss: -0.340045. Value loss: 0.119589. Entropy: 0.310336.\n",
      "Iteration 7088: Policy loss: -0.344601. Value loss: 0.046609. Entropy: 0.310619.\n",
      "Iteration 7089: Policy loss: -0.356533. Value loss: 0.031190. Entropy: 0.310506.\n",
      "episode: 2809   score: 415.0  epsilon: 1.0    steps: 88  evaluation reward: 371.8\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7090: Policy loss: 0.565416. Value loss: 0.415501. Entropy: 0.296103.\n",
      "Iteration 7091: Policy loss: 0.523480. Value loss: 0.185296. Entropy: 0.295026.\n",
      "Iteration 7092: Policy loss: 0.506968. Value loss: 0.092259. Entropy: 0.294467.\n",
      "episode: 2810   score: 620.0  epsilon: 1.0    steps: 24  evaluation reward: 375.9\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7093: Policy loss: -0.013290. Value loss: 0.105516. Entropy: 0.303669.\n",
      "Iteration 7094: Policy loss: -0.019599. Value loss: 0.036819. Entropy: 0.302042.\n",
      "Iteration 7095: Policy loss: -0.022302. Value loss: 0.025649. Entropy: 0.302559.\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7096: Policy loss: 0.435452. Value loss: 0.132537. Entropy: 0.306955.\n",
      "Iteration 7097: Policy loss: 0.429007. Value loss: 0.050916. Entropy: 0.306314.\n",
      "Iteration 7098: Policy loss: 0.424426. Value loss: 0.039406. Entropy: 0.305963.\n",
      "episode: 2811   score: 540.0  epsilon: 1.0    steps: 88  evaluation reward: 377.1\n",
      "Training network. lr: 0.000196. clip: 0.078352\n",
      "Iteration 7099: Policy loss: 0.100601. Value loss: 0.164087. Entropy: 0.295070.\n",
      "Iteration 7100: Policy loss: 0.095779. Value loss: 0.074386. Entropy: 0.293251.\n",
      "Iteration 7101: Policy loss: 0.080786. Value loss: 0.046147. Entropy: 0.294993.\n",
      "episode: 2812   score: 180.0  epsilon: 1.0    steps: 72  evaluation reward: 375.6\n",
      "episode: 2813   score: 505.0  epsilon: 1.0    steps: 96  evaluation reward: 378.35\n",
      "episode: 2814   score: 410.0  epsilon: 1.0    steps: 240  evaluation reward: 380.05\n",
      "episode: 2815   score: 590.0  epsilon: 1.0    steps: 736  evaluation reward: 383.45\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7102: Policy loss: 0.098252. Value loss: 0.135266. Entropy: 0.256315.\n",
      "Iteration 7103: Policy loss: 0.081558. Value loss: 0.056993. Entropy: 0.255486.\n",
      "Iteration 7104: Policy loss: 0.080964. Value loss: 0.042499. Entropy: 0.255905.\n",
      "episode: 2816   score: 270.0  epsilon: 1.0    steps: 152  evaluation reward: 380.5\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7105: Policy loss: -0.189889. Value loss: 0.115301. Entropy: 0.301079.\n",
      "Iteration 7106: Policy loss: -0.200895. Value loss: 0.044899. Entropy: 0.300395.\n",
      "Iteration 7107: Policy loss: -0.199787. Value loss: 0.027797. Entropy: 0.300951.\n",
      "episode: 2817   score: 525.0  epsilon: 1.0    steps: 784  evaluation reward: 382.9\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7108: Policy loss: -0.033787. Value loss: 0.235226. Entropy: 0.299319.\n",
      "Iteration 7109: Policy loss: -0.042252. Value loss: 0.099584. Entropy: 0.301870.\n",
      "Iteration 7110: Policy loss: -0.038367. Value loss: 0.066666. Entropy: 0.300172.\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7111: Policy loss: -0.289800. Value loss: 0.344196. Entropy: 0.308613.\n",
      "Iteration 7112: Policy loss: -0.300778. Value loss: 0.110230. Entropy: 0.308999.\n",
      "Iteration 7113: Policy loss: -0.309065. Value loss: 0.063586. Entropy: 0.308390.\n",
      "episode: 2818   score: 260.0  epsilon: 1.0    steps: 400  evaluation reward: 378.85\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7114: Policy loss: 0.151811. Value loss: 0.068710. Entropy: 0.291228.\n",
      "Iteration 7115: Policy loss: 0.149353. Value loss: 0.029221. Entropy: 0.291679.\n",
      "Iteration 7116: Policy loss: 0.151911. Value loss: 0.020031. Entropy: 0.290664.\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7117: Policy loss: -0.098957. Value loss: 0.135819. Entropy: 0.307033.\n",
      "Iteration 7118: Policy loss: -0.098024. Value loss: 0.051325. Entropy: 0.307799.\n",
      "Iteration 7119: Policy loss: -0.104847. Value loss: 0.039950. Entropy: 0.307948.\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7120: Policy loss: 0.122584. Value loss: 0.213032. Entropy: 0.308774.\n",
      "Iteration 7121: Policy loss: 0.097031. Value loss: 0.070817. Entropy: 0.309548.\n",
      "Iteration 7122: Policy loss: 0.100850. Value loss: 0.043303. Entropy: 0.308892.\n",
      "episode: 2819   score: 475.0  epsilon: 1.0    steps: 168  evaluation reward: 379.8\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7123: Policy loss: 0.017119. Value loss: 0.106592. Entropy: 0.291112.\n",
      "Iteration 7124: Policy loss: 0.010432. Value loss: 0.046342. Entropy: 0.290198.\n",
      "Iteration 7125: Policy loss: 0.005787. Value loss: 0.034098. Entropy: 0.289639.\n",
      "episode: 2820   score: 265.0  epsilon: 1.0    steps: 344  evaluation reward: 379.6\n",
      "episode: 2821   score: 640.0  epsilon: 1.0    steps: 368  evaluation reward: 383.6\n",
      "episode: 2822   score: 560.0  epsilon: 1.0    steps: 416  evaluation reward: 384.4\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7126: Policy loss: 0.077684. Value loss: 0.075848. Entropy: 0.268618.\n",
      "Iteration 7127: Policy loss: 0.073438. Value loss: 0.036100. Entropy: 0.269058.\n",
      "Iteration 7128: Policy loss: 0.070057. Value loss: 0.026774. Entropy: 0.267906.\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7129: Policy loss: 0.026946. Value loss: 0.110918. Entropy: 0.314656.\n",
      "Iteration 7130: Policy loss: 0.015909. Value loss: 0.050866. Entropy: 0.314130.\n",
      "Iteration 7131: Policy loss: 0.016819. Value loss: 0.039518. Entropy: 0.314895.\n",
      "episode: 2823   score: 320.0  epsilon: 1.0    steps: 704  evaluation reward: 384.9\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7132: Policy loss: 0.385969. Value loss: 0.143911. Entropy: 0.301195.\n",
      "Iteration 7133: Policy loss: 0.382127. Value loss: 0.048013. Entropy: 0.301675.\n",
      "Iteration 7134: Policy loss: 0.364079. Value loss: 0.036207. Entropy: 0.299579.\n",
      "episode: 2824   score: 390.0  epsilon: 1.0    steps: 640  evaluation reward: 385.95\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7135: Policy loss: 0.210103. Value loss: 0.149915. Entropy: 0.296107.\n",
      "Iteration 7136: Policy loss: 0.202153. Value loss: 0.055074. Entropy: 0.294738.\n",
      "Iteration 7137: Policy loss: 0.192798. Value loss: 0.044489. Entropy: 0.294970.\n",
      "episode: 2825   score: 180.0  epsilon: 1.0    steps: 384  evaluation reward: 382.55\n",
      "episode: 2826   score: 210.0  epsilon: 1.0    steps: 400  evaluation reward: 376.7\n",
      "episode: 2827   score: 480.0  epsilon: 1.0    steps: 544  evaluation reward: 376.9\n",
      "episode: 2828   score: 360.0  epsilon: 1.0    steps: 992  evaluation reward: 378.4\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7138: Policy loss: 0.019435. Value loss: 0.084646. Entropy: 0.268379.\n",
      "Iteration 7139: Policy loss: 0.015035. Value loss: 0.049819. Entropy: 0.267209.\n",
      "Iteration 7140: Policy loss: 0.009932. Value loss: 0.038647. Entropy: 0.267416.\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7141: Policy loss: 0.114027. Value loss: 0.099126. Entropy: 0.298992.\n",
      "Iteration 7142: Policy loss: 0.108968. Value loss: 0.041986. Entropy: 0.298299.\n",
      "Iteration 7143: Policy loss: 0.095534. Value loss: 0.028209. Entropy: 0.297869.\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7144: Policy loss: -0.023580. Value loss: 0.099654. Entropy: 0.310321.\n",
      "Iteration 7145: Policy loss: -0.025451. Value loss: 0.046746. Entropy: 0.309798.\n",
      "Iteration 7146: Policy loss: -0.024105. Value loss: 0.031720. Entropy: 0.308839.\n",
      "episode: 2829   score: 110.0  epsilon: 1.0    steps: 952  evaluation reward: 376.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7147: Policy loss: 0.227599. Value loss: 0.099031. Entropy: 0.304402.\n",
      "Iteration 7148: Policy loss: 0.222808. Value loss: 0.026906. Entropy: 0.303328.\n",
      "Iteration 7149: Policy loss: 0.221247. Value loss: 0.018071. Entropy: 0.303343.\n",
      "Training network. lr: 0.000195. clip: 0.078195\n",
      "Iteration 7150: Policy loss: -0.252650. Value loss: 0.272573. Entropy: 0.295057.\n",
      "Iteration 7151: Policy loss: -0.318451. Value loss: 0.150231. Entropy: 0.295572.\n",
      "Iteration 7152: Policy loss: -0.320124. Value loss: 0.080366. Entropy: 0.297591.\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7153: Policy loss: -0.085727. Value loss: 0.084171. Entropy: 0.306932.\n",
      "Iteration 7154: Policy loss: -0.088692. Value loss: 0.043137. Entropy: 0.307247.\n",
      "Iteration 7155: Policy loss: -0.092867. Value loss: 0.033072. Entropy: 0.307521.\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7156: Policy loss: -0.223099. Value loss: 0.261468. Entropy: 0.305389.\n",
      "Iteration 7157: Policy loss: -0.232583. Value loss: 0.096687. Entropy: 0.305629.\n",
      "Iteration 7158: Policy loss: -0.242810. Value loss: 0.069440. Entropy: 0.305468.\n",
      "episode: 2830   score: 640.0  epsilon: 1.0    steps: 240  evaluation reward: 380.75\n",
      "episode: 2831   score: 395.0  epsilon: 1.0    steps: 816  evaluation reward: 383.2\n",
      "episode: 2832   score: 220.0  epsilon: 1.0    steps: 872  evaluation reward: 381.15\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7159: Policy loss: 0.167298. Value loss: 0.202854. Entropy: 0.279690.\n",
      "Iteration 7160: Policy loss: 0.152822. Value loss: 0.083250. Entropy: 0.278750.\n",
      "Iteration 7161: Policy loss: 0.143671. Value loss: 0.050724. Entropy: 0.278838.\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7162: Policy loss: -0.329681. Value loss: 0.201527. Entropy: 0.297066.\n",
      "Iteration 7163: Policy loss: -0.329994. Value loss: 0.059498. Entropy: 0.297964.\n",
      "Iteration 7164: Policy loss: -0.340983. Value loss: 0.045840. Entropy: 0.299424.\n",
      "episode: 2833   score: 365.0  epsilon: 1.0    steps: 32  evaluation reward: 382.2\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7165: Policy loss: -0.142409. Value loss: 0.116506. Entropy: 0.300217.\n",
      "Iteration 7166: Policy loss: -0.149792. Value loss: 0.054881. Entropy: 0.298977.\n",
      "Iteration 7167: Policy loss: -0.157487. Value loss: 0.039883. Entropy: 0.299951.\n",
      "episode: 2834   score: 650.0  epsilon: 1.0    steps: 992  evaluation reward: 386.3\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7168: Policy loss: -0.110918. Value loss: 0.170097. Entropy: 0.309970.\n",
      "Iteration 7169: Policy loss: -0.117120. Value loss: 0.075296. Entropy: 0.309537.\n",
      "Iteration 7170: Policy loss: -0.127178. Value loss: 0.052202. Entropy: 0.308563.\n",
      "episode: 2835   score: 435.0  epsilon: 1.0    steps: 584  evaluation reward: 387.8\n",
      "episode: 2836   score: 620.0  epsilon: 1.0    steps: 848  evaluation reward: 391.15\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7171: Policy loss: 0.361840. Value loss: 0.162997. Entropy: 0.275038.\n",
      "Iteration 7172: Policy loss: 0.330896. Value loss: 0.056129. Entropy: 0.275064.\n",
      "Iteration 7173: Policy loss: 0.338911. Value loss: 0.031141. Entropy: 0.273544.\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7174: Policy loss: 0.008746. Value loss: 0.138121. Entropy: 0.311003.\n",
      "Iteration 7175: Policy loss: 0.011557. Value loss: 0.041515. Entropy: 0.311706.\n",
      "Iteration 7176: Policy loss: -0.008793. Value loss: 0.027302. Entropy: 0.310666.\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7177: Policy loss: 0.201879. Value loss: 0.092634. Entropy: 0.314424.\n",
      "Iteration 7178: Policy loss: 0.194702. Value loss: 0.041858. Entropy: 0.314413.\n",
      "Iteration 7179: Policy loss: 0.187864. Value loss: 0.031693. Entropy: 0.313054.\n",
      "episode: 2837   score: 620.0  epsilon: 1.0    steps: 592  evaluation reward: 395.25\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7180: Policy loss: 0.070251. Value loss: 0.394742. Entropy: 0.290507.\n",
      "Iteration 7181: Policy loss: 0.073359. Value loss: 0.285005. Entropy: 0.290843.\n",
      "Iteration 7182: Policy loss: 0.048976. Value loss: 0.265330. Entropy: 0.290885.\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7183: Policy loss: 0.213897. Value loss: 0.093376. Entropy: 0.310904.\n",
      "Iteration 7184: Policy loss: 0.199443. Value loss: 0.046338. Entropy: 0.309367.\n",
      "Iteration 7185: Policy loss: 0.190298. Value loss: 0.035373. Entropy: 0.309309.\n",
      "episode: 2838   score: 275.0  epsilon: 1.0    steps: 424  evaluation reward: 394.15\n",
      "episode: 2839   score: 345.0  epsilon: 1.0    steps: 792  evaluation reward: 389.45\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7186: Policy loss: 0.033421. Value loss: 0.076620. Entropy: 0.287831.\n",
      "Iteration 7187: Policy loss: 0.034602. Value loss: 0.031896. Entropy: 0.286183.\n",
      "Iteration 7188: Policy loss: 0.024100. Value loss: 0.023069. Entropy: 0.286078.\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7189: Policy loss: -0.237078. Value loss: 0.555174. Entropy: 0.308424.\n",
      "Iteration 7190: Policy loss: -0.261228. Value loss: 0.331973. Entropy: 0.307252.\n",
      "Iteration 7191: Policy loss: -0.275052. Value loss: 0.238198. Entropy: 0.307269.\n",
      "episode: 2840   score: 760.0  epsilon: 1.0    steps: 384  evaluation reward: 394.35\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7192: Policy loss: 0.261453. Value loss: 0.122126. Entropy: 0.294990.\n",
      "Iteration 7193: Policy loss: 0.248688. Value loss: 0.050929. Entropy: 0.293798.\n",
      "Iteration 7194: Policy loss: 0.241526. Value loss: 0.037210. Entropy: 0.292819.\n",
      "episode: 2841   score: 380.0  epsilon: 1.0    steps: 384  evaluation reward: 394.05\n",
      "episode: 2842   score: 260.0  epsilon: 1.0    steps: 488  evaluation reward: 391.75\n",
      "episode: 2843   score: 365.0  epsilon: 1.0    steps: 976  evaluation reward: 393.3\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7195: Policy loss: -0.033427. Value loss: 0.260312. Entropy: 0.278101.\n",
      "Iteration 7196: Policy loss: -0.042082. Value loss: 0.150655. Entropy: 0.277126.\n",
      "Iteration 7197: Policy loss: -0.039233. Value loss: 0.091077. Entropy: 0.277613.\n",
      "Training network. lr: 0.000195. clip: 0.078038\n",
      "Iteration 7198: Policy loss: -0.088758. Value loss: 0.112123. Entropy: 0.298729.\n",
      "Iteration 7199: Policy loss: -0.094531. Value loss: 0.054812. Entropy: 0.298999.\n",
      "Iteration 7200: Policy loss: -0.101520. Value loss: 0.035266. Entropy: 0.298870.\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7201: Policy loss: -0.164155. Value loss: 0.341092. Entropy: 0.309123.\n",
      "Iteration 7202: Policy loss: -0.199531. Value loss: 0.094223. Entropy: 0.310335.\n",
      "Iteration 7203: Policy loss: -0.190866. Value loss: 0.074596. Entropy: 0.311532.\n",
      "episode: 2844   score: 855.0  epsilon: 1.0    steps: 648  evaluation reward: 397.6\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7204: Policy loss: 0.290369. Value loss: 0.211590. Entropy: 0.297984.\n",
      "Iteration 7205: Policy loss: 0.285308. Value loss: 0.071069. Entropy: 0.297818.\n",
      "Iteration 7206: Policy loss: 0.273219. Value loss: 0.041694. Entropy: 0.297903.\n",
      "episode: 2845   score: 285.0  epsilon: 1.0    steps: 776  evaluation reward: 396.25\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7207: Policy loss: 0.154278. Value loss: 0.121398. Entropy: 0.299378.\n",
      "Iteration 7208: Policy loss: 0.140134. Value loss: 0.044149. Entropy: 0.298899.\n",
      "Iteration 7209: Policy loss: 0.136896. Value loss: 0.033875. Entropy: 0.298000.\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7210: Policy loss: -0.178348. Value loss: 0.291517. Entropy: 0.307660.\n",
      "Iteration 7211: Policy loss: -0.186594. Value loss: 0.134638. Entropy: 0.307316.\n",
      "Iteration 7212: Policy loss: -0.183981. Value loss: 0.089347. Entropy: 0.307050.\n",
      "episode: 2846   score: 405.0  epsilon: 1.0    steps: 744  evaluation reward: 398.2\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7213: Policy loss: -0.062130. Value loss: 0.162339. Entropy: 0.297021.\n",
      "Iteration 7214: Policy loss: -0.065993. Value loss: 0.077949. Entropy: 0.298529.\n",
      "Iteration 7215: Policy loss: -0.073825. Value loss: 0.053980. Entropy: 0.298280.\n",
      "episode: 2847   score: 340.0  epsilon: 1.0    steps: 504  evaluation reward: 396.3\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7216: Policy loss: 0.346823. Value loss: 0.160199. Entropy: 0.294208.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7217: Policy loss: 0.346490. Value loss: 0.057362. Entropy: 0.291368.\n",
      "Iteration 7218: Policy loss: 0.328435. Value loss: 0.043714. Entropy: 0.293130.\n",
      "episode: 2848   score: 530.0  epsilon: 1.0    steps: 352  evaluation reward: 399.3\n",
      "episode: 2849   score: 765.0  epsilon: 1.0    steps: 888  evaluation reward: 403.5\n",
      "episode: 2850   score: 335.0  epsilon: 1.0    steps: 1016  evaluation reward: 404.05\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7219: Policy loss: 0.240766. Value loss: 0.115573. Entropy: 0.287469.\n",
      "Iteration 7220: Policy loss: 0.229652. Value loss: 0.046706. Entropy: 0.286925.\n",
      "Iteration 7221: Policy loss: 0.228321. Value loss: 0.032634. Entropy: 0.286179.\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7222: Policy loss: 0.252362. Value loss: 0.157117. Entropy: 0.293494.\n",
      "Iteration 7223: Policy loss: 0.242450. Value loss: 0.051821. Entropy: 0.292337.\n",
      "Iteration 7224: Policy loss: 0.233983. Value loss: 0.032264. Entropy: 0.291885.\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7225: Policy loss: 0.281511. Value loss: 0.115931. Entropy: 0.309471.\n",
      "Iteration 7226: Policy loss: 0.276242. Value loss: 0.057540. Entropy: 0.309377.\n",
      "Iteration 7227: Policy loss: 0.273241. Value loss: 0.040841. Entropy: 0.308920.\n",
      "now time :  2019-09-05 21:43:33.235307\n",
      "episode: 2851   score: 210.0  epsilon: 1.0    steps: 56  evaluation reward: 402.45\n",
      "episode: 2852   score: 310.0  epsilon: 1.0    steps: 328  evaluation reward: 401.4\n",
      "episode: 2853   score: 435.0  epsilon: 1.0    steps: 928  evaluation reward: 402.15\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7228: Policy loss: 0.224552. Value loss: 0.107060. Entropy: 0.275977.\n",
      "Iteration 7229: Policy loss: 0.214412. Value loss: 0.053290. Entropy: 0.276548.\n",
      "Iteration 7230: Policy loss: 0.211742. Value loss: 0.043245. Entropy: 0.274982.\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7231: Policy loss: 0.032573. Value loss: 0.043671. Entropy: 0.302294.\n",
      "Iteration 7232: Policy loss: 0.026904. Value loss: 0.019859. Entropy: 0.300587.\n",
      "Iteration 7233: Policy loss: 0.028796. Value loss: 0.015474. Entropy: 0.300917.\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7234: Policy loss: -0.072684. Value loss: 0.136438. Entropy: 0.306924.\n",
      "Iteration 7235: Policy loss: -0.079574. Value loss: 0.061729. Entropy: 0.306649.\n",
      "Iteration 7236: Policy loss: -0.082341. Value loss: 0.048834. Entropy: 0.306771.\n",
      "episode: 2854   score: 320.0  epsilon: 1.0    steps: 864  evaluation reward: 402.7\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7237: Policy loss: 0.024448. Value loss: 0.107268. Entropy: 0.300659.\n",
      "Iteration 7238: Policy loss: 0.025511. Value loss: 0.038121. Entropy: 0.300823.\n",
      "Iteration 7239: Policy loss: 0.011414. Value loss: 0.028392. Entropy: 0.303259.\n",
      "episode: 2855   score: 370.0  epsilon: 1.0    steps: 128  evaluation reward: 404.75\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7240: Policy loss: 0.064554. Value loss: 0.091511. Entropy: 0.291249.\n",
      "Iteration 7241: Policy loss: 0.052022. Value loss: 0.048519. Entropy: 0.289593.\n",
      "Iteration 7242: Policy loss: 0.044555. Value loss: 0.040278. Entropy: 0.289817.\n",
      "episode: 2856   score: 285.0  epsilon: 1.0    steps: 64  evaluation reward: 405.35\n",
      "episode: 2857   score: 330.0  epsilon: 1.0    steps: 360  evaluation reward: 402.75\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7243: Policy loss: 0.297756. Value loss: 0.127278. Entropy: 0.277906.\n",
      "Iteration 7244: Policy loss: 0.282101. Value loss: 0.059657. Entropy: 0.276095.\n",
      "Iteration 7245: Policy loss: 0.273707. Value loss: 0.043480. Entropy: 0.277421.\n",
      "episode: 2858   score: 315.0  epsilon: 1.0    steps: 120  evaluation reward: 399.95\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7246: Policy loss: -0.434880. Value loss: 0.315430. Entropy: 0.292690.\n",
      "Iteration 7247: Policy loss: -0.448240. Value loss: 0.194863. Entropy: 0.295464.\n",
      "Iteration 7248: Policy loss: -0.448052. Value loss: 0.127004. Entropy: 0.293942.\n",
      "Training network. lr: 0.000195. clip: 0.077891\n",
      "Iteration 7249: Policy loss: -0.045557. Value loss: 0.114889. Entropy: 0.308483.\n",
      "Iteration 7250: Policy loss: -0.053340. Value loss: 0.055547. Entropy: 0.306371.\n",
      "Iteration 7251: Policy loss: -0.050615. Value loss: 0.037592. Entropy: 0.307867.\n",
      "episode: 2859   score: 360.0  epsilon: 1.0    steps: 360  evaluation reward: 401.75\n",
      "episode: 2860   score: 500.0  epsilon: 1.0    steps: 472  evaluation reward: 403.55\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7252: Policy loss: 0.048290. Value loss: 0.054628. Entropy: 0.278519.\n",
      "Iteration 7253: Policy loss: 0.044297. Value loss: 0.029814. Entropy: 0.277908.\n",
      "Iteration 7254: Policy loss: 0.043163. Value loss: 0.023382. Entropy: 0.279525.\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7255: Policy loss: 0.110231. Value loss: 0.152860. Entropy: 0.306014.\n",
      "Iteration 7256: Policy loss: 0.105739. Value loss: 0.045890. Entropy: 0.306005.\n",
      "Iteration 7257: Policy loss: 0.097075. Value loss: 0.029685. Entropy: 0.305935.\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7258: Policy loss: 0.136724. Value loss: 0.090674. Entropy: 0.304819.\n",
      "Iteration 7259: Policy loss: 0.137625. Value loss: 0.042908. Entropy: 0.305202.\n",
      "Iteration 7260: Policy loss: 0.132896. Value loss: 0.033254. Entropy: 0.305344.\n",
      "episode: 2861   score: 475.0  epsilon: 1.0    steps: 584  evaluation reward: 405.75\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7261: Policy loss: 0.099130. Value loss: 0.124107. Entropy: 0.295991.\n",
      "Iteration 7262: Policy loss: 0.092136. Value loss: 0.052087. Entropy: 0.295187.\n",
      "Iteration 7263: Policy loss: 0.093555. Value loss: 0.031933. Entropy: 0.294843.\n",
      "episode: 2862   score: 285.0  epsilon: 1.0    steps: 40  evaluation reward: 404.0\n",
      "episode: 2863   score: 335.0  epsilon: 1.0    steps: 568  evaluation reward: 402.75\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7264: Policy loss: 0.054877. Value loss: 0.078473. Entropy: 0.280964.\n",
      "Iteration 7265: Policy loss: 0.050969. Value loss: 0.030073. Entropy: 0.280295.\n",
      "Iteration 7266: Policy loss: 0.043436. Value loss: 0.022778. Entropy: 0.279701.\n",
      "episode: 2864   score: 390.0  epsilon: 1.0    steps: 624  evaluation reward: 401.95\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7267: Policy loss: 0.003570. Value loss: 0.060943. Entropy: 0.293788.\n",
      "Iteration 7268: Policy loss: -0.003865. Value loss: 0.028344. Entropy: 0.292841.\n",
      "Iteration 7269: Policy loss: -0.003132. Value loss: 0.025677. Entropy: 0.293235.\n",
      "episode: 2865   score: 390.0  epsilon: 1.0    steps: 784  evaluation reward: 403.1\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7270: Policy loss: -0.048443. Value loss: 0.089319. Entropy: 0.296434.\n",
      "Iteration 7271: Policy loss: -0.049195. Value loss: 0.046683. Entropy: 0.295718.\n",
      "Iteration 7272: Policy loss: -0.059412. Value loss: 0.034908. Entropy: 0.296830.\n",
      "episode: 2866   score: 420.0  epsilon: 1.0    steps: 8  evaluation reward: 404.55\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7273: Policy loss: -0.167042. Value loss: 0.311191. Entropy: 0.297642.\n",
      "Iteration 7274: Policy loss: -0.171795. Value loss: 0.213785. Entropy: 0.296138.\n",
      "Iteration 7275: Policy loss: -0.202152. Value loss: 0.157598. Entropy: 0.297427.\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7276: Policy loss: -0.015467. Value loss: 0.056472. Entropy: 0.306238.\n",
      "Iteration 7277: Policy loss: -0.022458. Value loss: 0.024940. Entropy: 0.307985.\n",
      "Iteration 7278: Policy loss: -0.023388. Value loss: 0.017646. Entropy: 0.306825.\n",
      "episode: 2867   score: 550.0  epsilon: 1.0    steps: 184  evaluation reward: 407.45\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7279: Policy loss: 0.211044. Value loss: 0.074274. Entropy: 0.294419.\n",
      "Iteration 7280: Policy loss: 0.208265. Value loss: 0.026526. Entropy: 0.291268.\n",
      "Iteration 7281: Policy loss: 0.203874. Value loss: 0.017872. Entropy: 0.292151.\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7282: Policy loss: 0.130109. Value loss: 0.137010. Entropy: 0.306416.\n",
      "Iteration 7283: Policy loss: 0.129974. Value loss: 0.043292. Entropy: 0.306721.\n",
      "Iteration 7284: Policy loss: 0.123088. Value loss: 0.026521. Entropy: 0.306449.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2868   score: 590.0  epsilon: 1.0    steps: 456  evaluation reward: 410.0\n",
      "episode: 2869   score: 300.0  epsilon: 1.0    steps: 976  evaluation reward: 407.35\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7285: Policy loss: 0.011867. Value loss: 0.125596. Entropy: 0.295697.\n",
      "Iteration 7286: Policy loss: 0.001233. Value loss: 0.060855. Entropy: 0.295758.\n",
      "Iteration 7287: Policy loss: -0.008978. Value loss: 0.042850. Entropy: 0.294842.\n",
      "episode: 2870   score: 390.0  epsilon: 1.0    steps: 112  evaluation reward: 408.85\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7288: Policy loss: -0.017072. Value loss: 0.070850. Entropy: 0.282552.\n",
      "Iteration 7289: Policy loss: -0.027041. Value loss: 0.032025. Entropy: 0.281405.\n",
      "Iteration 7290: Policy loss: -0.029482. Value loss: 0.024401. Entropy: 0.281779.\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7291: Policy loss: -0.065732. Value loss: 0.097807. Entropy: 0.308167.\n",
      "Iteration 7292: Policy loss: -0.070978. Value loss: 0.041546. Entropy: 0.306379.\n",
      "Iteration 7293: Policy loss: -0.075228. Value loss: 0.030537. Entropy: 0.306631.\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7294: Policy loss: -0.538371. Value loss: 0.422142. Entropy: 0.306462.\n",
      "Iteration 7295: Policy loss: -0.583744. Value loss: 0.188449. Entropy: 0.307330.\n",
      "Iteration 7296: Policy loss: -0.584066. Value loss: 0.080777. Entropy: 0.307149.\n",
      "episode: 2871   score: 620.0  epsilon: 1.0    steps: 1008  evaluation reward: 412.45\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7297: Policy loss: 0.129207. Value loss: 0.050758. Entropy: 0.305159.\n",
      "Iteration 7298: Policy loss: 0.116381. Value loss: 0.018250. Entropy: 0.304931.\n",
      "Iteration 7299: Policy loss: 0.118045. Value loss: 0.011464. Entropy: 0.305282.\n",
      "episode: 2872   score: 365.0  epsilon: 1.0    steps: 640  evaluation reward: 412.45\n",
      "Training network. lr: 0.000194. clip: 0.077734\n",
      "Iteration 7300: Policy loss: 0.156274. Value loss: 0.052799. Entropy: 0.282575.\n",
      "Iteration 7301: Policy loss: 0.143613. Value loss: 0.022175. Entropy: 0.281023.\n",
      "Iteration 7302: Policy loss: 0.138485. Value loss: 0.015343. Entropy: 0.281653.\n",
      "episode: 2873   score: 335.0  epsilon: 1.0    steps: 632  evaluation reward: 411.3\n",
      "episode: 2874   score: 390.0  epsilon: 1.0    steps: 632  evaluation reward: 411.85\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7303: Policy loss: 0.058994. Value loss: 0.136005. Entropy: 0.280936.\n",
      "Iteration 7304: Policy loss: 0.051647. Value loss: 0.056490. Entropy: 0.281384.\n",
      "Iteration 7305: Policy loss: 0.048246. Value loss: 0.040172. Entropy: 0.281133.\n",
      "episode: 2875   score: 210.0  epsilon: 1.0    steps: 408  evaluation reward: 409.85\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7306: Policy loss: 0.161933. Value loss: 0.085849. Entropy: 0.298419.\n",
      "Iteration 7307: Policy loss: 0.147708. Value loss: 0.041075. Entropy: 0.296862.\n",
      "Iteration 7308: Policy loss: 0.151756. Value loss: 0.033891. Entropy: 0.296375.\n",
      "episode: 2876   score: 465.0  epsilon: 1.0    steps: 64  evaluation reward: 408.3\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7309: Policy loss: 0.076900. Value loss: 0.088237. Entropy: 0.295668.\n",
      "Iteration 7310: Policy loss: 0.061404. Value loss: 0.034929. Entropy: 0.295350.\n",
      "Iteration 7311: Policy loss: 0.060876. Value loss: 0.025032. Entropy: 0.294810.\n",
      "episode: 2877   score: 540.0  epsilon: 1.0    steps: 32  evaluation reward: 409.2\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7312: Policy loss: 0.084551. Value loss: 0.060072. Entropy: 0.297132.\n",
      "Iteration 7313: Policy loss: 0.080811. Value loss: 0.023286. Entropy: 0.296755.\n",
      "Iteration 7314: Policy loss: 0.077351. Value loss: 0.017735. Entropy: 0.296715.\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7315: Policy loss: -0.300708. Value loss: 0.324055. Entropy: 0.303001.\n",
      "Iteration 7316: Policy loss: -0.318080. Value loss: 0.217821. Entropy: 0.301278.\n",
      "Iteration 7317: Policy loss: -0.330663. Value loss: 0.146679. Entropy: 0.302228.\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7318: Policy loss: -0.224901. Value loss: 0.245004. Entropy: 0.302670.\n",
      "Iteration 7319: Policy loss: -0.244002. Value loss: 0.072522. Entropy: 0.304401.\n",
      "Iteration 7320: Policy loss: -0.243729. Value loss: 0.039518. Entropy: 0.303654.\n",
      "episode: 2878   score: 260.0  epsilon: 1.0    steps: 504  evaluation reward: 408.9\n",
      "episode: 2879   score: 675.0  epsilon: 1.0    steps: 816  evaluation reward: 409.7\n",
      "episode: 2880   score: 215.0  epsilon: 1.0    steps: 960  evaluation reward: 407.6\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7321: Policy loss: 0.200998. Value loss: 0.174516. Entropy: 0.284192.\n",
      "Iteration 7322: Policy loss: 0.193380. Value loss: 0.057952. Entropy: 0.283932.\n",
      "Iteration 7323: Policy loss: 0.194846. Value loss: 0.040522. Entropy: 0.285421.\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7324: Policy loss: 0.037025. Value loss: 0.109644. Entropy: 0.296713.\n",
      "Iteration 7325: Policy loss: 0.029172. Value loss: 0.049255. Entropy: 0.297177.\n",
      "Iteration 7326: Policy loss: 0.024894. Value loss: 0.035717. Entropy: 0.299210.\n",
      "episode: 2881   score: 390.0  epsilon: 1.0    steps: 88  evaluation reward: 407.6\n",
      "episode: 2882   score: 285.0  epsilon: 1.0    steps: 784  evaluation reward: 406.7\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7327: Policy loss: 0.185807. Value loss: 0.108833. Entropy: 0.286118.\n",
      "Iteration 7328: Policy loss: 0.174837. Value loss: 0.048574. Entropy: 0.286974.\n",
      "Iteration 7329: Policy loss: 0.161041. Value loss: 0.038241. Entropy: 0.287627.\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7330: Policy loss: -0.181932. Value loss: 0.248066. Entropy: 0.307043.\n",
      "Iteration 7331: Policy loss: -0.209471. Value loss: 0.089538. Entropy: 0.306582.\n",
      "Iteration 7332: Policy loss: -0.232436. Value loss: 0.060068. Entropy: 0.306963.\n",
      "episode: 2883   score: 535.0  epsilon: 1.0    steps: 512  evaluation reward: 406.15\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7333: Policy loss: -0.067199. Value loss: 0.250198. Entropy: 0.290416.\n",
      "Iteration 7334: Policy loss: -0.060512. Value loss: 0.090786. Entropy: 0.289607.\n",
      "Iteration 7335: Policy loss: -0.067965. Value loss: 0.067699. Entropy: 0.290269.\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7336: Policy loss: 0.151134. Value loss: 0.204492. Entropy: 0.308888.\n",
      "Iteration 7337: Policy loss: 0.130521. Value loss: 0.053249. Entropy: 0.308151.\n",
      "Iteration 7338: Policy loss: 0.119175. Value loss: 0.032384. Entropy: 0.307356.\n",
      "episode: 2884   score: 670.0  epsilon: 1.0    steps: 440  evaluation reward: 408.25\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7339: Policy loss: 0.103641. Value loss: 0.110113. Entropy: 0.288151.\n",
      "Iteration 7340: Policy loss: 0.101964. Value loss: 0.050502. Entropy: 0.286840.\n",
      "Iteration 7341: Policy loss: 0.093787. Value loss: 0.036873. Entropy: 0.287186.\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7342: Policy loss: 0.195193. Value loss: 0.089342. Entropy: 0.306549.\n",
      "Iteration 7343: Policy loss: 0.187718. Value loss: 0.038746. Entropy: 0.305272.\n",
      "Iteration 7344: Policy loss: 0.183978. Value loss: 0.024094. Entropy: 0.304059.\n",
      "episode: 2885   score: 395.0  epsilon: 1.0    steps: 48  evaluation reward: 409.75\n",
      "episode: 2886   score: 485.0  epsilon: 1.0    steps: 488  evaluation reward: 411.0\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7345: Policy loss: -0.151467. Value loss: 0.108381. Entropy: 0.280408.\n",
      "Iteration 7346: Policy loss: -0.159430. Value loss: 0.050437. Entropy: 0.279852.\n",
      "Iteration 7347: Policy loss: -0.168692. Value loss: 0.036322. Entropy: 0.279981.\n",
      "episode: 2887   score: 285.0  epsilon: 1.0    steps: 264  evaluation reward: 410.4\n",
      "Training network. lr: 0.000194. clip: 0.077577\n",
      "Iteration 7348: Policy loss: 0.421096. Value loss: 0.149651. Entropy: 0.297499.\n",
      "Iteration 7349: Policy loss: 0.415796. Value loss: 0.044122. Entropy: 0.297230.\n",
      "Iteration 7350: Policy loss: 0.405029. Value loss: 0.030845. Entropy: 0.297339.\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7351: Policy loss: 0.012880. Value loss: 0.146641. Entropy: 0.304594.\n",
      "Iteration 7352: Policy loss: 0.008828. Value loss: 0.059415. Entropy: 0.305666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7353: Policy loss: -0.004665. Value loss: 0.045966. Entropy: 0.305508.\n",
      "episode: 2888   score: 100.0  epsilon: 1.0    steps: 296  evaluation reward: 405.5\n",
      "episode: 2889   score: 360.0  epsilon: 1.0    steps: 592  evaluation reward: 404.65\n",
      "episode: 2890   score: 625.0  epsilon: 1.0    steps: 1024  evaluation reward: 407.25\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7354: Policy loss: -0.004696. Value loss: 0.080390. Entropy: 0.279558.\n",
      "Iteration 7355: Policy loss: -0.013649. Value loss: 0.040072. Entropy: 0.278886.\n",
      "Iteration 7356: Policy loss: -0.021146. Value loss: 0.028573. Entropy: 0.278869.\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7357: Policy loss: 0.195523. Value loss: 0.121080. Entropy: 0.294406.\n",
      "Iteration 7358: Policy loss: 0.187997. Value loss: 0.045216. Entropy: 0.294913.\n",
      "Iteration 7359: Policy loss: 0.183804. Value loss: 0.026885. Entropy: 0.293697.\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7360: Policy loss: 0.125820. Value loss: 0.146359. Entropy: 0.304883.\n",
      "Iteration 7361: Policy loss: 0.116695. Value loss: 0.068170. Entropy: 0.304682.\n",
      "Iteration 7362: Policy loss: 0.106433. Value loss: 0.046338. Entropy: 0.304149.\n",
      "episode: 2891   score: 215.0  epsilon: 1.0    steps: 384  evaluation reward: 404.65\n",
      "episode: 2892   score: 385.0  epsilon: 1.0    steps: 752  evaluation reward: 402.15\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7363: Policy loss: -0.124147. Value loss: 0.315675. Entropy: 0.282846.\n",
      "Iteration 7364: Policy loss: -0.137374. Value loss: 0.181331. Entropy: 0.283255.\n",
      "Iteration 7365: Policy loss: -0.155992. Value loss: 0.124429. Entropy: 0.284696.\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7366: Policy loss: -0.006243. Value loss: 0.134750. Entropy: 0.304063.\n",
      "Iteration 7367: Policy loss: -0.018633. Value loss: 0.063792. Entropy: 0.303738.\n",
      "Iteration 7368: Policy loss: -0.015994. Value loss: 0.048249. Entropy: 0.303646.\n",
      "episode: 2893   score: 490.0  epsilon: 1.0    steps: 384  evaluation reward: 403.95\n",
      "episode: 2894   score: 360.0  epsilon: 1.0    steps: 896  evaluation reward: 405.45\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7369: Policy loss: -0.107157. Value loss: 0.091087. Entropy: 0.287144.\n",
      "Iteration 7370: Policy loss: -0.118282. Value loss: 0.041145. Entropy: 0.284269.\n",
      "Iteration 7371: Policy loss: -0.124136. Value loss: 0.032649. Entropy: 0.285270.\n",
      "episode: 2895   score: 210.0  epsilon: 1.0    steps: 456  evaluation reward: 404.95\n",
      "episode: 2896   score: 555.0  epsilon: 1.0    steps: 568  evaluation reward: 407.3\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7372: Policy loss: 0.133471. Value loss: 0.086303. Entropy: 0.274420.\n",
      "Iteration 7373: Policy loss: 0.126138. Value loss: 0.040813. Entropy: 0.274878.\n",
      "Iteration 7374: Policy loss: 0.124757. Value loss: 0.029302. Entropy: 0.272469.\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7375: Policy loss: 0.138628. Value loss: 0.143518. Entropy: 0.305436.\n",
      "Iteration 7376: Policy loss: 0.127865. Value loss: 0.054063. Entropy: 0.305235.\n",
      "Iteration 7377: Policy loss: 0.127761. Value loss: 0.034124. Entropy: 0.304825.\n",
      "episode: 2897   score: 335.0  epsilon: 1.0    steps: 176  evaluation reward: 406.25\n",
      "episode: 2898   score: 135.0  epsilon: 1.0    steps: 616  evaluation reward: 405.5\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7378: Policy loss: 0.220459. Value loss: 0.124877. Entropy: 0.281571.\n",
      "Iteration 7379: Policy loss: 0.206144. Value loss: 0.048086. Entropy: 0.280695.\n",
      "Iteration 7380: Policy loss: 0.209857. Value loss: 0.037827. Entropy: 0.281197.\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7381: Policy loss: -0.106661. Value loss: 0.314138. Entropy: 0.306652.\n",
      "Iteration 7382: Policy loss: -0.123292. Value loss: 0.194735. Entropy: 0.305051.\n",
      "Iteration 7383: Policy loss: -0.137562. Value loss: 0.072482. Entropy: 0.305571.\n",
      "episode: 2899   score: 155.0  epsilon: 1.0    steps: 400  evaluation reward: 404.65\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7384: Policy loss: 0.175594. Value loss: 0.067454. Entropy: 0.294320.\n",
      "Iteration 7385: Policy loss: 0.170275. Value loss: 0.032350. Entropy: 0.293959.\n",
      "Iteration 7386: Policy loss: 0.164949. Value loss: 0.025657. Entropy: 0.294166.\n",
      "episode: 2900   score: 285.0  epsilon: 1.0    steps: 168  evaluation reward: 404.5\n",
      "now time :  2019-09-05 21:53:25.169406\n",
      "episode: 2901   score: 390.0  epsilon: 1.0    steps: 520  evaluation reward: 402.6\n",
      "episode: 2902   score: 290.0  epsilon: 1.0    steps: 888  evaluation reward: 399.9\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7387: Policy loss: -0.010352. Value loss: 0.144831. Entropy: 0.274892.\n",
      "Iteration 7388: Policy loss: -0.015875. Value loss: 0.051518. Entropy: 0.274620.\n",
      "Iteration 7389: Policy loss: -0.023352. Value loss: 0.036666. Entropy: 0.270465.\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7390: Policy loss: 0.015459. Value loss: 0.111420. Entropy: 0.301800.\n",
      "Iteration 7391: Policy loss: 0.009079. Value loss: 0.052246. Entropy: 0.302640.\n",
      "Iteration 7392: Policy loss: 0.002983. Value loss: 0.034607. Entropy: 0.301805.\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7393: Policy loss: 0.111561. Value loss: 0.077455. Entropy: 0.304747.\n",
      "Iteration 7394: Policy loss: 0.105527. Value loss: 0.036874. Entropy: 0.304343.\n",
      "Iteration 7395: Policy loss: 0.105262. Value loss: 0.026980. Entropy: 0.304476.\n",
      "episode: 2903   score: 565.0  epsilon: 1.0    steps: 200  evaluation reward: 399.6\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7396: Policy loss: 0.054619. Value loss: 0.055471. Entropy: 0.293228.\n",
      "Iteration 7397: Policy loss: 0.048409. Value loss: 0.024175. Entropy: 0.293483.\n",
      "Iteration 7398: Policy loss: 0.045789. Value loss: 0.019534. Entropy: 0.294077.\n",
      "episode: 2904   score: 285.0  epsilon: 1.0    steps: 776  evaluation reward: 399.95\n",
      "episode: 2905   score: 260.0  epsilon: 1.0    steps: 976  evaluation reward: 399.4\n",
      "Training network. lr: 0.000194. clip: 0.077430\n",
      "Iteration 7399: Policy loss: -0.307349. Value loss: 0.192272. Entropy: 0.291681.\n",
      "Iteration 7400: Policy loss: -0.308535. Value loss: 0.078942. Entropy: 0.293982.\n",
      "Iteration 7401: Policy loss: -0.322524. Value loss: 0.053596. Entropy: 0.292801.\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7402: Policy loss: 0.024331. Value loss: 0.239712. Entropy: 0.294460.\n",
      "Iteration 7403: Policy loss: 0.007994. Value loss: 0.063711. Entropy: 0.293450.\n",
      "Iteration 7404: Policy loss: 0.001405. Value loss: 0.040553. Entropy: 0.294354.\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7405: Policy loss: -0.321996. Value loss: 0.300089. Entropy: 0.302202.\n",
      "Iteration 7406: Policy loss: -0.340009. Value loss: 0.113825. Entropy: 0.303706.\n",
      "Iteration 7407: Policy loss: -0.339669. Value loss: 0.083978. Entropy: 0.303393.\n",
      "episode: 2906   score: 695.0  epsilon: 1.0    steps: 168  evaluation reward: 403.4\n",
      "episode: 2907   score: 275.0  epsilon: 1.0    steps: 320  evaluation reward: 400.6\n",
      "episode: 2908   score: 350.0  epsilon: 1.0    steps: 440  evaluation reward: 401.5\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7408: Policy loss: 0.033382. Value loss: 0.116404. Entropy: 0.263955.\n",
      "Iteration 7409: Policy loss: 0.025160. Value loss: 0.040915. Entropy: 0.261993.\n",
      "Iteration 7410: Policy loss: 0.022401. Value loss: 0.027277. Entropy: 0.263624.\n",
      "episode: 2909   score: 275.0  epsilon: 1.0    steps: 16  evaluation reward: 400.1\n",
      "episode: 2910   score: 395.0  epsilon: 1.0    steps: 520  evaluation reward: 397.85\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7411: Policy loss: -0.087969. Value loss: 0.376619. Entropy: 0.277771.\n",
      "Iteration 7412: Policy loss: -0.082096. Value loss: 0.191153. Entropy: 0.278285.\n",
      "Iteration 7413: Policy loss: -0.106337. Value loss: 0.107818. Entropy: 0.278465.\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7414: Policy loss: 0.050774. Value loss: 0.129366. Entropy: 0.307332.\n",
      "Iteration 7415: Policy loss: 0.046184. Value loss: 0.086078. Entropy: 0.307824.\n",
      "Iteration 7416: Policy loss: 0.044539. Value loss: 0.067720. Entropy: 0.307716.\n",
      "episode: 2911   score: 550.0  epsilon: 1.0    steps: 744  evaluation reward: 397.95\n",
      "episode: 2912   score: 125.0  epsilon: 1.0    steps: 928  evaluation reward: 397.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7417: Policy loss: 0.137419. Value loss: 0.082484. Entropy: 0.288943.\n",
      "Iteration 7418: Policy loss: 0.126593. Value loss: 0.045307. Entropy: 0.288427.\n",
      "Iteration 7419: Policy loss: 0.123945. Value loss: 0.034955. Entropy: 0.287167.\n",
      "episode: 2913   score: 260.0  epsilon: 1.0    steps: 984  evaluation reward: 394.95\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7420: Policy loss: -0.081359. Value loss: 0.149358. Entropy: 0.295733.\n",
      "Iteration 7421: Policy loss: -0.089301. Value loss: 0.050834. Entropy: 0.295597.\n",
      "Iteration 7422: Policy loss: -0.082059. Value loss: 0.035196. Entropy: 0.293961.\n",
      "episode: 2914   score: 620.0  epsilon: 1.0    steps: 752  evaluation reward: 397.05\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7423: Policy loss: 0.117637. Value loss: 0.096331. Entropy: 0.284582.\n",
      "Iteration 7424: Policy loss: 0.104034. Value loss: 0.044323. Entropy: 0.284290.\n",
      "Iteration 7425: Policy loss: 0.111658. Value loss: 0.031138. Entropy: 0.284319.\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7426: Policy loss: -0.034355. Value loss: 0.116595. Entropy: 0.303789.\n",
      "Iteration 7427: Policy loss: -0.043401. Value loss: 0.066190. Entropy: 0.303941.\n",
      "Iteration 7428: Policy loss: -0.047322. Value loss: 0.050877. Entropy: 0.303537.\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7429: Policy loss: 0.029009. Value loss: 0.186010. Entropy: 0.306578.\n",
      "Iteration 7430: Policy loss: 0.009733. Value loss: 0.071644. Entropy: 0.306990.\n",
      "Iteration 7431: Policy loss: 0.014733. Value loss: 0.045238. Entropy: 0.306913.\n",
      "episode: 2915   score: 390.0  epsilon: 1.0    steps: 136  evaluation reward: 395.05\n",
      "episode: 2916   score: 295.0  epsilon: 1.0    steps: 992  evaluation reward: 395.3\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7432: Policy loss: 0.048708. Value loss: 0.058720. Entropy: 0.291544.\n",
      "Iteration 7433: Policy loss: 0.042037. Value loss: 0.032183. Entropy: 0.292194.\n",
      "Iteration 7434: Policy loss: 0.042812. Value loss: 0.026764. Entropy: 0.292047.\n",
      "episode: 2917   score: 150.0  epsilon: 1.0    steps: 72  evaluation reward: 391.55\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7435: Policy loss: 0.081319. Value loss: 0.100453. Entropy: 0.280766.\n",
      "Iteration 7436: Policy loss: 0.075935. Value loss: 0.027695. Entropy: 0.280136.\n",
      "Iteration 7437: Policy loss: 0.064821. Value loss: 0.018831. Entropy: 0.279276.\n",
      "episode: 2918   score: 230.0  epsilon: 1.0    steps: 536  evaluation reward: 391.25\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7438: Policy loss: -0.081064. Value loss: 0.132756. Entropy: 0.293007.\n",
      "Iteration 7439: Policy loss: -0.095515. Value loss: 0.050721. Entropy: 0.292259.\n",
      "Iteration 7440: Policy loss: -0.102976. Value loss: 0.035439. Entropy: 0.291613.\n",
      "episode: 2919   score: 470.0  epsilon: 1.0    steps: 432  evaluation reward: 391.2\n",
      "episode: 2920   score: 390.0  epsilon: 1.0    steps: 656  evaluation reward: 392.45\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7441: Policy loss: 0.088148. Value loss: 0.144918. Entropy: 0.283301.\n",
      "Iteration 7442: Policy loss: 0.068086. Value loss: 0.062581. Entropy: 0.283612.\n",
      "Iteration 7443: Policy loss: 0.069675. Value loss: 0.046211. Entropy: 0.283360.\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7444: Policy loss: 0.081845. Value loss: 0.080799. Entropy: 0.309842.\n",
      "Iteration 7445: Policy loss: 0.072440. Value loss: 0.037571. Entropy: 0.310034.\n",
      "Iteration 7446: Policy loss: 0.068308. Value loss: 0.029403. Entropy: 0.309438.\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7447: Policy loss: -0.132622. Value loss: 0.101570. Entropy: 0.308233.\n",
      "Iteration 7448: Policy loss: -0.144258. Value loss: 0.041188. Entropy: 0.309228.\n",
      "Iteration 7449: Policy loss: -0.138495. Value loss: 0.027653. Entropy: 0.308915.\n",
      "episode: 2921   score: 335.0  epsilon: 1.0    steps: 488  evaluation reward: 389.4\n",
      "Training network. lr: 0.000193. clip: 0.077273\n",
      "Iteration 7450: Policy loss: 0.014187. Value loss: 0.079525. Entropy: 0.292123.\n",
      "Iteration 7451: Policy loss: 0.007520. Value loss: 0.031237. Entropy: 0.292663.\n",
      "Iteration 7452: Policy loss: 0.007410. Value loss: 0.017829. Entropy: 0.291547.\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7453: Policy loss: -0.159987. Value loss: 0.233808. Entropy: 0.305298.\n",
      "Iteration 7454: Policy loss: -0.173448. Value loss: 0.089487. Entropy: 0.304690.\n",
      "Iteration 7455: Policy loss: -0.188806. Value loss: 0.054266. Entropy: 0.305864.\n",
      "episode: 2922   score: 390.0  epsilon: 1.0    steps: 472  evaluation reward: 387.7\n",
      "episode: 2923   score: 225.0  epsilon: 1.0    steps: 984  evaluation reward: 386.75\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7456: Policy loss: 0.055952. Value loss: 0.062995. Entropy: 0.293445.\n",
      "Iteration 7457: Policy loss: 0.051210. Value loss: 0.025573. Entropy: 0.292762.\n",
      "Iteration 7458: Policy loss: 0.050276. Value loss: 0.018842. Entropy: 0.292638.\n",
      "episode: 2924   score: 695.0  epsilon: 1.0    steps: 544  evaluation reward: 389.8\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7459: Policy loss: 0.122813. Value loss: 0.069023. Entropy: 0.283031.\n",
      "Iteration 7460: Policy loss: 0.119659. Value loss: 0.040529. Entropy: 0.284124.\n",
      "Iteration 7461: Policy loss: 0.118439. Value loss: 0.035320. Entropy: 0.283943.\n",
      "episode: 2925   score: 295.0  epsilon: 1.0    steps: 640  evaluation reward: 390.95\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7462: Policy loss: 0.127492. Value loss: 0.122187. Entropy: 0.292410.\n",
      "Iteration 7463: Policy loss: 0.122250. Value loss: 0.053976. Entropy: 0.291805.\n",
      "Iteration 7464: Policy loss: 0.114940. Value loss: 0.038980. Entropy: 0.291898.\n",
      "episode: 2926   score: 240.0  epsilon: 1.0    steps: 352  evaluation reward: 391.25\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7465: Policy loss: 0.000057. Value loss: 0.124044. Entropy: 0.294842.\n",
      "Iteration 7466: Policy loss: -0.011576. Value loss: 0.049612. Entropy: 0.293685.\n",
      "Iteration 7467: Policy loss: -0.025478. Value loss: 0.034650. Entropy: 0.295685.\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7468: Policy loss: 0.023780. Value loss: 0.104108. Entropy: 0.308429.\n",
      "Iteration 7469: Policy loss: 0.014789. Value loss: 0.039065. Entropy: 0.307445.\n",
      "Iteration 7470: Policy loss: 0.017780. Value loss: 0.029492. Entropy: 0.308690.\n",
      "episode: 2927   score: 420.0  epsilon: 1.0    steps: 304  evaluation reward: 390.65\n",
      "episode: 2928   score: 260.0  epsilon: 1.0    steps: 368  evaluation reward: 389.65\n",
      "episode: 2929   score: 575.0  epsilon: 1.0    steps: 912  evaluation reward: 394.3\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7471: Policy loss: -0.282885. Value loss: 0.257899. Entropy: 0.275169.\n",
      "Iteration 7472: Policy loss: -0.282194. Value loss: 0.090457. Entropy: 0.274792.\n",
      "Iteration 7473: Policy loss: -0.295998. Value loss: 0.048341. Entropy: 0.276757.\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7474: Policy loss: -0.285902. Value loss: 0.322745. Entropy: 0.301101.\n",
      "Iteration 7475: Policy loss: -0.313158. Value loss: 0.136840. Entropy: 0.304062.\n",
      "Iteration 7476: Policy loss: -0.334823. Value loss: 0.092682. Entropy: 0.303966.\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7477: Policy loss: -0.116292. Value loss: 0.111408. Entropy: 0.307719.\n",
      "Iteration 7478: Policy loss: -0.127495. Value loss: 0.055102. Entropy: 0.306886.\n",
      "Iteration 7479: Policy loss: -0.125438. Value loss: 0.036839. Entropy: 0.306420.\n",
      "episode: 2930   score: 320.0  epsilon: 1.0    steps: 536  evaluation reward: 391.1\n",
      "episode: 2931   score: 320.0  epsilon: 1.0    steps: 744  evaluation reward: 390.35\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7480: Policy loss: 0.446520. Value loss: 0.191972. Entropy: 0.286888.\n",
      "Iteration 7481: Policy loss: 0.431107. Value loss: 0.041123. Entropy: 0.287764.\n",
      "Iteration 7482: Policy loss: 0.419080. Value loss: 0.026301. Entropy: 0.287133.\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7483: Policy loss: 0.100891. Value loss: 0.091402. Entropy: 0.305942.\n",
      "Iteration 7484: Policy loss: 0.090064. Value loss: 0.036810. Entropy: 0.306178.\n",
      "Iteration 7485: Policy loss: 0.082898. Value loss: 0.026118. Entropy: 0.305050.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2932   score: 530.0  epsilon: 1.0    steps: 384  evaluation reward: 393.45\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7486: Policy loss: 0.008866. Value loss: 0.121076. Entropy: 0.297973.\n",
      "Iteration 7487: Policy loss: -0.000047. Value loss: 0.065670. Entropy: 0.297206.\n",
      "Iteration 7488: Policy loss: -0.003492. Value loss: 0.043874. Entropy: 0.295767.\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7489: Policy loss: -0.288476. Value loss: 0.454407. Entropy: 0.304355.\n",
      "Iteration 7490: Policy loss: -0.322792. Value loss: 0.322313. Entropy: 0.303883.\n",
      "Iteration 7491: Policy loss: -0.344718. Value loss: 0.262592. Entropy: 0.304179.\n",
      "episode: 2933   score: 665.0  epsilon: 1.0    steps: 320  evaluation reward: 396.45\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7492: Policy loss: -0.005303. Value loss: 0.091988. Entropy: 0.301673.\n",
      "Iteration 7493: Policy loss: 0.006960. Value loss: 0.042286. Entropy: 0.301884.\n",
      "Iteration 7494: Policy loss: -0.014385. Value loss: 0.031625. Entropy: 0.300615.\n",
      "episode: 2934   score: 455.0  epsilon: 1.0    steps: 112  evaluation reward: 394.5\n",
      "episode: 2935   score: 365.0  epsilon: 1.0    steps: 448  evaluation reward: 393.8\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7495: Policy loss: 0.088353. Value loss: 0.096031. Entropy: 0.289905.\n",
      "Iteration 7496: Policy loss: 0.089773. Value loss: 0.036079. Entropy: 0.291138.\n",
      "Iteration 7497: Policy loss: 0.086784. Value loss: 0.025002. Entropy: 0.291339.\n",
      "episode: 2936   score: 420.0  epsilon: 1.0    steps: 816  evaluation reward: 391.8\n",
      "Training network. lr: 0.000193. clip: 0.077117\n",
      "Iteration 7498: Policy loss: 0.269943. Value loss: 0.106720. Entropy: 0.299694.\n",
      "Iteration 7499: Policy loss: 0.258840. Value loss: 0.044460. Entropy: 0.298164.\n",
      "Iteration 7500: Policy loss: 0.266669. Value loss: 0.037484. Entropy: 0.297564.\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7501: Policy loss: 0.119436. Value loss: 0.112352. Entropy: 0.307240.\n",
      "Iteration 7502: Policy loss: 0.108434. Value loss: 0.047749. Entropy: 0.306422.\n",
      "Iteration 7503: Policy loss: 0.115040. Value loss: 0.032277. Entropy: 0.306234.\n",
      "episode: 2937   score: 420.0  epsilon: 1.0    steps: 32  evaluation reward: 389.8\n",
      "episode: 2938   score: 460.0  epsilon: 1.0    steps: 112  evaluation reward: 391.65\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7504: Policy loss: 0.164901. Value loss: 0.091156. Entropy: 0.293666.\n",
      "Iteration 7505: Policy loss: 0.157529. Value loss: 0.039164. Entropy: 0.291827.\n",
      "Iteration 7506: Policy loss: 0.156201. Value loss: 0.027017. Entropy: 0.293779.\n",
      "episode: 2939   score: 335.0  epsilon: 1.0    steps: 176  evaluation reward: 391.55\n",
      "episode: 2940   score: 260.0  epsilon: 1.0    steps: 616  evaluation reward: 386.55\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7507: Policy loss: 0.014466. Value loss: 0.076810. Entropy: 0.285673.\n",
      "Iteration 7508: Policy loss: 0.016870. Value loss: 0.042946. Entropy: 0.280893.\n",
      "Iteration 7509: Policy loss: 0.009062. Value loss: 0.035334. Entropy: 0.280881.\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7510: Policy loss: -0.232837. Value loss: 0.371633. Entropy: 0.309369.\n",
      "Iteration 7511: Policy loss: -0.225476. Value loss: 0.221022. Entropy: 0.309396.\n",
      "Iteration 7512: Policy loss: -0.258632. Value loss: 0.143142. Entropy: 0.309051.\n",
      "episode: 2941   score: 240.0  epsilon: 1.0    steps: 896  evaluation reward: 385.15\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7513: Policy loss: -0.114494. Value loss: 0.134516. Entropy: 0.301491.\n",
      "Iteration 7514: Policy loss: -0.122015. Value loss: 0.058249. Entropy: 0.302020.\n",
      "Iteration 7515: Policy loss: -0.121777. Value loss: 0.037908. Entropy: 0.301729.\n",
      "episode: 2942   score: 210.0  epsilon: 1.0    steps: 360  evaluation reward: 384.65\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7516: Policy loss: -0.071974. Value loss: 0.298844. Entropy: 0.289395.\n",
      "Iteration 7517: Policy loss: -0.086642. Value loss: 0.077848. Entropy: 0.292373.\n",
      "Iteration 7518: Policy loss: -0.093761. Value loss: 0.052652. Entropy: 0.290642.\n",
      "episode: 2943   score: 210.0  epsilon: 1.0    steps: 80  evaluation reward: 383.1\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7519: Policy loss: -0.040885. Value loss: 0.215530. Entropy: 0.298293.\n",
      "Iteration 7520: Policy loss: -0.034901. Value loss: 0.087460. Entropy: 0.296768.\n",
      "Iteration 7521: Policy loss: -0.045213. Value loss: 0.063211. Entropy: 0.296898.\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7522: Policy loss: -0.007543. Value loss: 0.110548. Entropy: 0.299069.\n",
      "Iteration 7523: Policy loss: -0.021091. Value loss: 0.047366. Entropy: 0.298397.\n",
      "Iteration 7524: Policy loss: -0.022591. Value loss: 0.031214. Entropy: 0.299062.\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7525: Policy loss: -0.143383. Value loss: 0.091710. Entropy: 0.305518.\n",
      "Iteration 7526: Policy loss: -0.152268. Value loss: 0.039952. Entropy: 0.305736.\n",
      "Iteration 7527: Policy loss: -0.153421. Value loss: 0.023506. Entropy: 0.305690.\n",
      "episode: 2944   score: 310.0  epsilon: 1.0    steps: 400  evaluation reward: 377.65\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7528: Policy loss: 0.175194. Value loss: 0.313822. Entropy: 0.302796.\n",
      "Iteration 7529: Policy loss: 0.152514. Value loss: 0.100553. Entropy: 0.302096.\n",
      "Iteration 7530: Policy loss: 0.127598. Value loss: 0.060965. Entropy: 0.302322.\n",
      "episode: 2945   score: 530.0  epsilon: 1.0    steps: 8  evaluation reward: 380.1\n",
      "episode: 2946   score: 850.0  epsilon: 1.0    steps: 560  evaluation reward: 384.55\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7531: Policy loss: -0.022827. Value loss: 0.097782. Entropy: 0.294360.\n",
      "Iteration 7532: Policy loss: -0.029085. Value loss: 0.043732. Entropy: 0.295129.\n",
      "Iteration 7533: Policy loss: -0.033021. Value loss: 0.033581. Entropy: 0.294242.\n",
      "episode: 2947   score: 260.0  epsilon: 1.0    steps: 792  evaluation reward: 383.75\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7534: Policy loss: 0.119399. Value loss: 0.120347. Entropy: 0.300547.\n",
      "Iteration 7535: Policy loss: 0.110210. Value loss: 0.053314. Entropy: 0.299774.\n",
      "Iteration 7536: Policy loss: 0.107916. Value loss: 0.037505. Entropy: 0.300022.\n",
      "episode: 2948   score: 635.0  epsilon: 1.0    steps: 1008  evaluation reward: 384.8\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7537: Policy loss: 0.013486. Value loss: 0.095143. Entropy: 0.310047.\n",
      "Iteration 7538: Policy loss: 0.008216. Value loss: 0.039536. Entropy: 0.310017.\n",
      "Iteration 7539: Policy loss: 0.001293. Value loss: 0.030304. Entropy: 0.310041.\n",
      "episode: 2949   score: 435.0  epsilon: 1.0    steps: 560  evaluation reward: 381.5\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7540: Policy loss: -0.329909. Value loss: 0.169585. Entropy: 0.303691.\n",
      "Iteration 7541: Policy loss: -0.344402. Value loss: 0.060507. Entropy: 0.304374.\n",
      "Iteration 7542: Policy loss: -0.347748. Value loss: 0.037467. Entropy: 0.304789.\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7543: Policy loss: -0.226562. Value loss: 0.137477. Entropy: 0.308390.\n",
      "Iteration 7544: Policy loss: -0.235124. Value loss: 0.033080. Entropy: 0.307658.\n",
      "Iteration 7545: Policy loss: -0.246348. Value loss: 0.016356. Entropy: 0.308376.\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7546: Policy loss: 0.714125. Value loss: 0.267524. Entropy: 0.310162.\n",
      "Iteration 7547: Policy loss: 0.702790. Value loss: 0.059177. Entropy: 0.309222.\n",
      "Iteration 7548: Policy loss: 0.712327. Value loss: 0.035325. Entropy: 0.309194.\n",
      "episode: 2950   score: 575.0  epsilon: 1.0    steps: 800  evaluation reward: 383.9\n",
      "now time :  2019-09-05 22:03:27.252283\n",
      "episode: 2951   score: 260.0  epsilon: 1.0    steps: 816  evaluation reward: 384.4\n",
      "Training network. lr: 0.000192. clip: 0.076969\n",
      "Iteration 7549: Policy loss: -0.124741. Value loss: 0.173231. Entropy: 0.299824.\n",
      "Iteration 7550: Policy loss: -0.134458. Value loss: 0.098724. Entropy: 0.299341.\n",
      "Iteration 7551: Policy loss: -0.117185. Value loss: 0.059630. Entropy: 0.300440.\n",
      "episode: 2952   score: 620.0  epsilon: 1.0    steps: 192  evaluation reward: 387.5\n",
      "episode: 2953   score: 310.0  epsilon: 1.0    steps: 472  evaluation reward: 386.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7552: Policy loss: 0.122342. Value loss: 0.068044. Entropy: 0.295827.\n",
      "Iteration 7553: Policy loss: 0.115628. Value loss: 0.033655. Entropy: 0.295757.\n",
      "Iteration 7554: Policy loss: 0.111707. Value loss: 0.027141. Entropy: 0.297592.\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7555: Policy loss: 0.121708. Value loss: 0.130916. Entropy: 0.309687.\n",
      "Iteration 7556: Policy loss: 0.106241. Value loss: 0.042065. Entropy: 0.308974.\n",
      "Iteration 7557: Policy loss: 0.105521. Value loss: 0.030844. Entropy: 0.308654.\n",
      "episode: 2954   score: 265.0  epsilon: 1.0    steps: 216  evaluation reward: 385.7\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7558: Policy loss: -0.038054. Value loss: 0.050555. Entropy: 0.303327.\n",
      "Iteration 7559: Policy loss: -0.046385. Value loss: 0.023374. Entropy: 0.304675.\n",
      "Iteration 7560: Policy loss: -0.047949. Value loss: 0.019308. Entropy: 0.304196.\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7561: Policy loss: 0.060949. Value loss: 0.470162. Entropy: 0.307886.\n",
      "Iteration 7562: Policy loss: 0.035088. Value loss: 0.239733. Entropy: 0.308157.\n",
      "Iteration 7563: Policy loss: 0.028492. Value loss: 0.147138. Entropy: 0.306785.\n",
      "episode: 2955   score: 365.0  epsilon: 1.0    steps: 232  evaluation reward: 385.65\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7564: Policy loss: 0.052167. Value loss: 0.086270. Entropy: 0.301502.\n",
      "Iteration 7565: Policy loss: 0.050347. Value loss: 0.032016. Entropy: 0.301026.\n",
      "Iteration 7566: Policy loss: 0.043849. Value loss: 0.019979. Entropy: 0.300394.\n",
      "episode: 2956   score: 620.0  epsilon: 1.0    steps: 560  evaluation reward: 389.0\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7567: Policy loss: 0.335676. Value loss: 0.144406. Entropy: 0.298292.\n",
      "Iteration 7568: Policy loss: 0.334277. Value loss: 0.053597. Entropy: 0.297020.\n",
      "Iteration 7569: Policy loss: 0.328191. Value loss: 0.032206. Entropy: 0.297749.\n",
      "episode: 2957   score: 415.0  epsilon: 1.0    steps: 504  evaluation reward: 389.85\n",
      "episode: 2958   score: 590.0  epsilon: 1.0    steps: 864  evaluation reward: 392.6\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7570: Policy loss: -0.062215. Value loss: 0.118645. Entropy: 0.293253.\n",
      "Iteration 7571: Policy loss: -0.069986. Value loss: 0.059989. Entropy: 0.292578.\n",
      "Iteration 7572: Policy loss: -0.071083. Value loss: 0.040357. Entropy: 0.292625.\n",
      "episode: 2959   score: 265.0  epsilon: 1.0    steps: 72  evaluation reward: 391.65\n",
      "episode: 2960   score: 290.0  epsilon: 1.0    steps: 472  evaluation reward: 389.55\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7573: Policy loss: 0.146982. Value loss: 0.073050. Entropy: 0.290560.\n",
      "Iteration 7574: Policy loss: 0.145918. Value loss: 0.036873. Entropy: 0.293050.\n",
      "Iteration 7575: Policy loss: 0.139117. Value loss: 0.031376. Entropy: 0.289627.\n",
      "episode: 2961   score: 210.0  epsilon: 1.0    steps: 856  evaluation reward: 386.9\n",
      "episode: 2962   score: 350.0  epsilon: 1.0    steps: 896  evaluation reward: 387.55\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7576: Policy loss: 0.101824. Value loss: 0.097535. Entropy: 0.305017.\n",
      "Iteration 7577: Policy loss: 0.102515. Value loss: 0.050237. Entropy: 0.303156.\n",
      "Iteration 7578: Policy loss: 0.095721. Value loss: 0.048129. Entropy: 0.303353.\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7579: Policy loss: -0.310530. Value loss: 0.217633. Entropy: 0.306633.\n",
      "Iteration 7580: Policy loss: -0.307993. Value loss: 0.098436. Entropy: 0.306378.\n",
      "Iteration 7581: Policy loss: -0.335593. Value loss: 0.054894. Entropy: 0.308165.\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7582: Policy loss: -0.191557. Value loss: 0.175152. Entropy: 0.306958.\n",
      "Iteration 7583: Policy loss: -0.211186. Value loss: 0.065431. Entropy: 0.307412.\n",
      "Iteration 7584: Policy loss: -0.207528. Value loss: 0.042144. Entropy: 0.306331.\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7585: Policy loss: -0.247157. Value loss: 0.258912. Entropy: 0.306060.\n",
      "Iteration 7586: Policy loss: -0.271471. Value loss: 0.100829. Entropy: 0.305745.\n",
      "Iteration 7587: Policy loss: -0.275229. Value loss: 0.050627. Entropy: 0.305666.\n",
      "episode: 2963   score: 210.0  epsilon: 1.0    steps: 480  evaluation reward: 386.3\n",
      "episode: 2964   score: 535.0  epsilon: 1.0    steps: 496  evaluation reward: 387.75\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7588: Policy loss: 0.222772. Value loss: 0.193675. Entropy: 0.295006.\n",
      "Iteration 7589: Policy loss: 0.190657. Value loss: 0.061072. Entropy: 0.295089.\n",
      "Iteration 7590: Policy loss: 0.206260. Value loss: 0.041947. Entropy: 0.295510.\n",
      "episode: 2965   score: 590.0  epsilon: 1.0    steps: 400  evaluation reward: 389.75\n",
      "episode: 2966   score: 315.0  epsilon: 1.0    steps: 496  evaluation reward: 388.7\n",
      "episode: 2967   score: 460.0  epsilon: 1.0    steps: 976  evaluation reward: 387.8\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7591: Policy loss: -0.114619. Value loss: 0.106094. Entropy: 0.294848.\n",
      "Iteration 7592: Policy loss: -0.115919. Value loss: 0.060045. Entropy: 0.287582.\n",
      "Iteration 7593: Policy loss: -0.120330. Value loss: 0.044153. Entropy: 0.288247.\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7594: Policy loss: -0.102447. Value loss: 0.192045. Entropy: 0.306368.\n",
      "Iteration 7595: Policy loss: -0.122365. Value loss: 0.068376. Entropy: 0.308184.\n",
      "Iteration 7596: Policy loss: -0.130374. Value loss: 0.043022. Entropy: 0.308559.\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7597: Policy loss: 0.269268. Value loss: 0.158771. Entropy: 0.306955.\n",
      "Iteration 7598: Policy loss: 0.251130. Value loss: 0.057057. Entropy: 0.306144.\n",
      "Iteration 7599: Policy loss: 0.254598. Value loss: 0.043883. Entropy: 0.306286.\n",
      "episode: 2968   score: 580.0  epsilon: 1.0    steps: 592  evaluation reward: 387.7\n",
      "Training network. lr: 0.000192. clip: 0.076813\n",
      "Iteration 7600: Policy loss: 0.007585. Value loss: 0.204989. Entropy: 0.299925.\n",
      "Iteration 7601: Policy loss: -0.003810. Value loss: 0.121673. Entropy: 0.299251.\n",
      "Iteration 7602: Policy loss: -0.017579. Value loss: 0.077745. Entropy: 0.298528.\n",
      "episode: 2969   score: 290.0  epsilon: 1.0    steps: 80  evaluation reward: 387.6\n",
      "episode: 2970   score: 290.0  epsilon: 1.0    steps: 232  evaluation reward: 386.6\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7603: Policy loss: 0.058563. Value loss: 0.055103. Entropy: 0.289347.\n",
      "Iteration 7604: Policy loss: 0.056402. Value loss: 0.032959. Entropy: 0.286576.\n",
      "Iteration 7605: Policy loss: 0.052989. Value loss: 0.027024. Entropy: 0.288083.\n",
      "episode: 2971   score: 240.0  epsilon: 1.0    steps: 784  evaluation reward: 382.8\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7606: Policy loss: -0.161559. Value loss: 0.222732. Entropy: 0.302699.\n",
      "Iteration 7607: Policy loss: -0.167357. Value loss: 0.073516. Entropy: 0.303633.\n",
      "Iteration 7608: Policy loss: -0.181909. Value loss: 0.049908. Entropy: 0.302893.\n",
      "episode: 2972   score: 270.0  epsilon: 1.0    steps: 648  evaluation reward: 381.85\n",
      "episode: 2973   score: 415.0  epsilon: 1.0    steps: 688  evaluation reward: 382.65\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7609: Policy loss: 0.157471. Value loss: 0.279142. Entropy: 0.297396.\n",
      "Iteration 7610: Policy loss: 0.151918. Value loss: 0.082191. Entropy: 0.298274.\n",
      "Iteration 7611: Policy loss: 0.128761. Value loss: 0.048482. Entropy: 0.295881.\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7612: Policy loss: 0.144536. Value loss: 0.223852. Entropy: 0.308262.\n",
      "Iteration 7613: Policy loss: 0.151940. Value loss: 0.084209. Entropy: 0.311077.\n",
      "Iteration 7614: Policy loss: 0.135435. Value loss: 0.052662. Entropy: 0.310168.\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7615: Policy loss: 0.253290. Value loss: 0.125523. Entropy: 0.306412.\n",
      "Iteration 7616: Policy loss: 0.253783. Value loss: 0.046093. Entropy: 0.305960.\n",
      "Iteration 7617: Policy loss: 0.237733. Value loss: 0.032288. Entropy: 0.306594.\n",
      "episode: 2974   score: 285.0  epsilon: 1.0    steps: 64  evaluation reward: 381.6\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7618: Policy loss: 0.139823. Value loss: 0.071980. Entropy: 0.299814.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7619: Policy loss: 0.135495. Value loss: 0.024321. Entropy: 0.300101.\n",
      "Iteration 7620: Policy loss: 0.133367. Value loss: 0.016388. Entropy: 0.300533.\n",
      "episode: 2975   score: 240.0  epsilon: 1.0    steps: 600  evaluation reward: 381.9\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7621: Policy loss: -0.062099. Value loss: 0.141761. Entropy: 0.300899.\n",
      "Iteration 7622: Policy loss: -0.056628. Value loss: 0.049939. Entropy: 0.301119.\n",
      "Iteration 7623: Policy loss: -0.067701. Value loss: 0.033119. Entropy: 0.300559.\n",
      "episode: 2976   score: 640.0  epsilon: 1.0    steps: 608  evaluation reward: 383.65\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7624: Policy loss: -0.073463. Value loss: 0.146394. Entropy: 0.302992.\n",
      "Iteration 7625: Policy loss: -0.077870. Value loss: 0.055867. Entropy: 0.303716.\n",
      "Iteration 7626: Policy loss: -0.081284. Value loss: 0.035725. Entropy: 0.303804.\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7627: Policy loss: -0.257237. Value loss: 0.333672. Entropy: 0.309909.\n",
      "Iteration 7628: Policy loss: -0.283588. Value loss: 0.163378. Entropy: 0.309210.\n",
      "Iteration 7629: Policy loss: -0.290763. Value loss: 0.077265. Entropy: 0.309805.\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7630: Policy loss: 0.132783. Value loss: 0.063118. Entropy: 0.304345.\n",
      "Iteration 7631: Policy loss: 0.127470. Value loss: 0.025973. Entropy: 0.304132.\n",
      "Iteration 7632: Policy loss: 0.125491. Value loss: 0.018303. Entropy: 0.303591.\n",
      "episode: 2977   score: 335.0  epsilon: 1.0    steps: 536  evaluation reward: 381.6\n",
      "episode: 2978   score: 290.0  epsilon: 1.0    steps: 816  evaluation reward: 381.9\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7633: Policy loss: -0.052690. Value loss: 0.094670. Entropy: 0.300040.\n",
      "Iteration 7634: Policy loss: -0.058074. Value loss: 0.041916. Entropy: 0.301273.\n",
      "Iteration 7635: Policy loss: -0.066303. Value loss: 0.028752. Entropy: 0.299074.\n",
      "episode: 2979   score: 435.0  epsilon: 1.0    steps: 336  evaluation reward: 379.5\n",
      "episode: 2980   score: 725.0  epsilon: 1.0    steps: 960  evaluation reward: 384.6\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7636: Policy loss: -0.313760. Value loss: 0.300240. Entropy: 0.297872.\n",
      "Iteration 7637: Policy loss: -0.324288. Value loss: 0.112760. Entropy: 0.299208.\n",
      "Iteration 7638: Policy loss: -0.314086. Value loss: 0.063912. Entropy: 0.297864.\n",
      "episode: 2981   score: 250.0  epsilon: 1.0    steps: 32  evaluation reward: 383.2\n",
      "episode: 2982   score: 420.0  epsilon: 1.0    steps: 448  evaluation reward: 384.55\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7639: Policy loss: -0.055605. Value loss: 0.084160. Entropy: 0.300527.\n",
      "Iteration 7640: Policy loss: -0.060616. Value loss: 0.035897. Entropy: 0.299577.\n",
      "Iteration 7641: Policy loss: -0.062284. Value loss: 0.029163. Entropy: 0.300082.\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7642: Policy loss: 0.184095. Value loss: 0.123506. Entropy: 0.306984.\n",
      "Iteration 7643: Policy loss: 0.185864. Value loss: 0.061973. Entropy: 0.306806.\n",
      "Iteration 7644: Policy loss: 0.182530. Value loss: 0.044878. Entropy: 0.305674.\n",
      "episode: 2983   score: 460.0  epsilon: 1.0    steps: 80  evaluation reward: 383.8\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7645: Policy loss: -0.338901. Value loss: 0.146112. Entropy: 0.305922.\n",
      "Iteration 7646: Policy loss: -0.345276. Value loss: 0.075725. Entropy: 0.306233.\n",
      "Iteration 7647: Policy loss: -0.348574. Value loss: 0.056857. Entropy: 0.305325.\n",
      "episode: 2984   score: 365.0  epsilon: 1.0    steps: 1024  evaluation reward: 380.75\n",
      "Training network. lr: 0.000192. clip: 0.076656\n",
      "Iteration 7648: Policy loss: 0.097262. Value loss: 0.075083. Entropy: 0.304337.\n",
      "Iteration 7649: Policy loss: 0.087810. Value loss: 0.019909. Entropy: 0.303931.\n",
      "Iteration 7650: Policy loss: 0.082969. Value loss: 0.015305. Entropy: 0.304134.\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7651: Policy loss: 0.030435. Value loss: 0.180345. Entropy: 0.304066.\n",
      "Iteration 7652: Policy loss: 0.012975. Value loss: 0.061572. Entropy: 0.302980.\n",
      "Iteration 7653: Policy loss: 0.017737. Value loss: 0.035412. Entropy: 0.303476.\n",
      "episode: 2985   score: 535.0  epsilon: 1.0    steps: 832  evaluation reward: 382.15\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7654: Policy loss: -0.164612. Value loss: 0.467488. Entropy: 0.303494.\n",
      "Iteration 7655: Policy loss: -0.203853. Value loss: 0.309387. Entropy: 0.303524.\n",
      "Iteration 7656: Policy loss: -0.219712. Value loss: 0.226292. Entropy: 0.303358.\n",
      "episode: 2986   score: 415.0  epsilon: 1.0    steps: 512  evaluation reward: 381.45\n",
      "episode: 2987   score: 285.0  epsilon: 1.0    steps: 616  evaluation reward: 381.45\n",
      "episode: 2988   score: 595.0  epsilon: 1.0    steps: 984  evaluation reward: 386.4\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7657: Policy loss: 0.145727. Value loss: 0.172086. Entropy: 0.296307.\n",
      "Iteration 7658: Policy loss: 0.139416. Value loss: 0.079488. Entropy: 0.296589.\n",
      "Iteration 7659: Policy loss: 0.122982. Value loss: 0.052821. Entropy: 0.297179.\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7660: Policy loss: -0.002091. Value loss: 0.137547. Entropy: 0.304900.\n",
      "Iteration 7661: Policy loss: -0.003788. Value loss: 0.060049. Entropy: 0.305392.\n",
      "Iteration 7662: Policy loss: -0.006643. Value loss: 0.039107. Entropy: 0.304985.\n",
      "episode: 2989   score: 315.0  epsilon: 1.0    steps: 400  evaluation reward: 385.95\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7663: Policy loss: -0.035068. Value loss: 0.107052. Entropy: 0.301182.\n",
      "Iteration 7664: Policy loss: -0.048552. Value loss: 0.046486. Entropy: 0.300943.\n",
      "Iteration 7665: Policy loss: -0.042327. Value loss: 0.035410. Entropy: 0.299690.\n",
      "episode: 2990   score: 365.0  epsilon: 1.0    steps: 784  evaluation reward: 383.35\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7666: Policy loss: 0.219803. Value loss: 0.123462. Entropy: 0.301130.\n",
      "Iteration 7667: Policy loss: 0.209470. Value loss: 0.047840. Entropy: 0.300295.\n",
      "Iteration 7668: Policy loss: 0.205962. Value loss: 0.032642. Entropy: 0.298944.\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7669: Policy loss: 0.182120. Value loss: 0.060988. Entropy: 0.305875.\n",
      "Iteration 7670: Policy loss: 0.174454. Value loss: 0.025111. Entropy: 0.304569.\n",
      "Iteration 7671: Policy loss: 0.172616. Value loss: 0.019820. Entropy: 0.304390.\n",
      "episode: 2991   score: 450.0  epsilon: 1.0    steps: 728  evaluation reward: 385.7\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7672: Policy loss: -0.224205. Value loss: 0.252332. Entropy: 0.300165.\n",
      "Iteration 7673: Policy loss: -0.213543. Value loss: 0.106355. Entropy: 0.299995.\n",
      "Iteration 7674: Policy loss: -0.223211. Value loss: 0.057895. Entropy: 0.300202.\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7675: Policy loss: 0.198137. Value loss: 0.144431. Entropy: 0.305419.\n",
      "Iteration 7676: Policy loss: 0.195054. Value loss: 0.058834. Entropy: 0.305452.\n",
      "Iteration 7677: Policy loss: 0.178326. Value loss: 0.022835. Entropy: 0.304770.\n",
      "episode: 2992   score: 425.0  epsilon: 1.0    steps: 224  evaluation reward: 386.1\n",
      "episode: 2993   score: 240.0  epsilon: 1.0    steps: 312  evaluation reward: 383.6\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7678: Policy loss: 0.484426. Value loss: 0.200554. Entropy: 0.300466.\n",
      "Iteration 7679: Policy loss: 0.477688. Value loss: 0.051233. Entropy: 0.301219.\n",
      "Iteration 7680: Policy loss: 0.457428. Value loss: 0.034388. Entropy: 0.298780.\n",
      "episode: 2994   score: 260.0  epsilon: 1.0    steps: 288  evaluation reward: 382.6\n",
      "episode: 2995   score: 410.0  epsilon: 1.0    steps: 824  evaluation reward: 384.6\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7681: Policy loss: -0.004366. Value loss: 0.117781. Entropy: 0.295914.\n",
      "Iteration 7682: Policy loss: -0.013850. Value loss: 0.058566. Entropy: 0.296838.\n",
      "Iteration 7683: Policy loss: -0.019194. Value loss: 0.041516. Entropy: 0.296327.\n",
      "episode: 2996   score: 395.0  epsilon: 1.0    steps: 832  evaluation reward: 383.0\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7684: Policy loss: -0.057059. Value loss: 0.148422. Entropy: 0.306535.\n",
      "Iteration 7685: Policy loss: -0.067488. Value loss: 0.053643. Entropy: 0.306114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7686: Policy loss: -0.069341. Value loss: 0.037451. Entropy: 0.305934.\n",
      "episode: 2997   score: 215.0  epsilon: 1.0    steps: 960  evaluation reward: 381.8\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7687: Policy loss: 0.348637. Value loss: 0.109411. Entropy: 0.303037.\n",
      "Iteration 7688: Policy loss: 0.346765. Value loss: 0.050039. Entropy: 0.302599.\n",
      "Iteration 7689: Policy loss: 0.339914. Value loss: 0.038271. Entropy: 0.302107.\n",
      "episode: 2998   score: 590.0  epsilon: 1.0    steps: 312  evaluation reward: 386.35\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7690: Policy loss: 0.210377. Value loss: 0.055297. Entropy: 0.300057.\n",
      "Iteration 7691: Policy loss: 0.209120. Value loss: 0.025428. Entropy: 0.300641.\n",
      "Iteration 7692: Policy loss: 0.202212. Value loss: 0.019516. Entropy: 0.299686.\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7693: Policy loss: -0.072336. Value loss: 0.137180. Entropy: 0.307828.\n",
      "Iteration 7694: Policy loss: -0.066628. Value loss: 0.035732. Entropy: 0.307779.\n",
      "Iteration 7695: Policy loss: -0.075986. Value loss: 0.020296. Entropy: 0.307360.\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7696: Policy loss: -0.257471. Value loss: 0.333103. Entropy: 0.305550.\n",
      "Iteration 7697: Policy loss: -0.248816. Value loss: 0.222725. Entropy: 0.304654.\n",
      "Iteration 7698: Policy loss: -0.256118. Value loss: 0.168731. Entropy: 0.304337.\n",
      "episode: 2999   score: 365.0  epsilon: 1.0    steps: 608  evaluation reward: 388.45\n",
      "episode: 3000   score: 240.0  epsilon: 1.0    steps: 704  evaluation reward: 388.0\n",
      "Training network. lr: 0.000191. clip: 0.076508\n",
      "Iteration 7699: Policy loss: 0.142228. Value loss: 0.166335. Entropy: 0.293311.\n",
      "Iteration 7700: Policy loss: 0.141842. Value loss: 0.063110. Entropy: 0.291243.\n",
      "Iteration 7701: Policy loss: 0.131269. Value loss: 0.039937. Entropy: 0.290766.\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7702: Policy loss: -0.305660. Value loss: 0.310586. Entropy: 0.307750.\n",
      "Iteration 7703: Policy loss: -0.330204. Value loss: 0.151599. Entropy: 0.306566.\n",
      "Iteration 7704: Policy loss: -0.326103. Value loss: 0.064806. Entropy: 0.306267.\n",
      "now time :  2019-09-05 22:13:05.461708\n",
      "episode: 3001   score: 560.0  epsilon: 1.0    steps: 544  evaluation reward: 389.7\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7705: Policy loss: -0.132988. Value loss: 0.099641. Entropy: 0.302103.\n",
      "Iteration 7706: Policy loss: -0.143476. Value loss: 0.043392. Entropy: 0.302522.\n",
      "Iteration 7707: Policy loss: -0.144978. Value loss: 0.031916. Entropy: 0.302230.\n",
      "episode: 3002   score: 590.0  epsilon: 1.0    steps: 616  evaluation reward: 392.7\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7708: Policy loss: -0.165501. Value loss: 0.199726. Entropy: 0.303393.\n",
      "Iteration 7709: Policy loss: -0.179112. Value loss: 0.076040. Entropy: 0.304206.\n",
      "Iteration 7710: Policy loss: -0.194226. Value loss: 0.060164. Entropy: 0.303994.\n",
      "episode: 3003   score: 590.0  epsilon: 1.0    steps: 272  evaluation reward: 392.95\n",
      "episode: 3004   score: 185.0  epsilon: 1.0    steps: 632  evaluation reward: 391.95\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7711: Policy loss: 0.103933. Value loss: 0.070764. Entropy: 0.294920.\n",
      "Iteration 7712: Policy loss: 0.100441. Value loss: 0.031369. Entropy: 0.294656.\n",
      "Iteration 7713: Policy loss: 0.096681. Value loss: 0.024503. Entropy: 0.294639.\n",
      "episode: 3005   score: 420.0  epsilon: 1.0    steps: 824  evaluation reward: 393.55\n",
      "episode: 3006   score: 440.0  epsilon: 1.0    steps: 952  evaluation reward: 391.0\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7714: Policy loss: -0.089192. Value loss: 0.148679. Entropy: 0.300877.\n",
      "Iteration 7715: Policy loss: -0.093709. Value loss: 0.048797. Entropy: 0.300470.\n",
      "Iteration 7716: Policy loss: -0.104777. Value loss: 0.041108. Entropy: 0.301349.\n",
      "episode: 3007   score: 125.0  epsilon: 1.0    steps: 800  evaluation reward: 389.5\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7717: Policy loss: 0.355182. Value loss: 0.188191. Entropy: 0.297055.\n",
      "Iteration 7718: Policy loss: 0.359883. Value loss: 0.079436. Entropy: 0.294469.\n",
      "Iteration 7719: Policy loss: 0.339991. Value loss: 0.048999. Entropy: 0.295248.\n",
      "episode: 3008   score: 370.0  epsilon: 1.0    steps: 72  evaluation reward: 389.7\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7720: Policy loss: 0.242801. Value loss: 0.161201. Entropy: 0.302443.\n",
      "Iteration 7721: Policy loss: 0.230142. Value loss: 0.055099. Entropy: 0.302541.\n",
      "Iteration 7722: Policy loss: 0.219796. Value loss: 0.038061. Entropy: 0.301221.\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7723: Policy loss: -0.082658. Value loss: 0.090887. Entropy: 0.306001.\n",
      "Iteration 7724: Policy loss: -0.094819. Value loss: 0.039146. Entropy: 0.305777.\n",
      "Iteration 7725: Policy loss: -0.095367. Value loss: 0.029984. Entropy: 0.305466.\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7726: Policy loss: 0.231327. Value loss: 0.351419. Entropy: 0.308653.\n",
      "Iteration 7727: Policy loss: 0.228787. Value loss: 0.109786. Entropy: 0.309743.\n",
      "Iteration 7728: Policy loss: 0.196082. Value loss: 0.089465. Entropy: 0.309243.\n",
      "episode: 3009   score: 545.0  epsilon: 1.0    steps: 992  evaluation reward: 392.4\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7729: Policy loss: -0.023944. Value loss: 0.256827. Entropy: 0.306811.\n",
      "Iteration 7730: Policy loss: -0.026257. Value loss: 0.133894. Entropy: 0.305789.\n",
      "Iteration 7731: Policy loss: -0.044652. Value loss: 0.056242. Entropy: 0.306200.\n",
      "episode: 3010   score: 390.0  epsilon: 1.0    steps: 496  evaluation reward: 392.35\n",
      "episode: 3011   score: 225.0  epsilon: 1.0    steps: 824  evaluation reward: 389.1\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7732: Policy loss: 0.118045. Value loss: 0.131963. Entropy: 0.296347.\n",
      "Iteration 7733: Policy loss: 0.105717. Value loss: 0.059824. Entropy: 0.295573.\n",
      "Iteration 7734: Policy loss: 0.097836. Value loss: 0.040110. Entropy: 0.295716.\n",
      "episode: 3012   score: 530.0  epsilon: 1.0    steps: 448  evaluation reward: 393.15\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7735: Policy loss: -0.184848. Value loss: 0.181938. Entropy: 0.299680.\n",
      "Iteration 7736: Policy loss: -0.189267. Value loss: 0.068059. Entropy: 0.297747.\n",
      "Iteration 7737: Policy loss: -0.197583. Value loss: 0.049210. Entropy: 0.297060.\n",
      "episode: 3013   score: 375.0  epsilon: 1.0    steps: 504  evaluation reward: 394.3\n",
      "episode: 3014   score: 310.0  epsilon: 1.0    steps: 960  evaluation reward: 391.2\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7738: Policy loss: 0.247853. Value loss: 0.172601. Entropy: 0.297227.\n",
      "Iteration 7739: Policy loss: 0.233885. Value loss: 0.075858. Entropy: 0.295175.\n",
      "Iteration 7740: Policy loss: 0.224259. Value loss: 0.048507. Entropy: 0.295399.\n",
      "episode: 3015   score: 370.0  epsilon: 1.0    steps: 1008  evaluation reward: 391.0\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7741: Policy loss: 0.113125. Value loss: 0.059391. Entropy: 0.305807.\n",
      "Iteration 7742: Policy loss: 0.109251. Value loss: 0.030117. Entropy: 0.304557.\n",
      "Iteration 7743: Policy loss: 0.104092. Value loss: 0.021642. Entropy: 0.304792.\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7744: Policy loss: -0.017490. Value loss: 0.319622. Entropy: 0.304007.\n",
      "Iteration 7745: Policy loss: -0.036405. Value loss: 0.221259. Entropy: 0.304777.\n",
      "Iteration 7746: Policy loss: -0.046763. Value loss: 0.170221. Entropy: 0.303877.\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7747: Policy loss: 0.420501. Value loss: 0.159493. Entropy: 0.302331.\n",
      "Iteration 7748: Policy loss: 0.407387. Value loss: 0.051390. Entropy: 0.301457.\n",
      "Iteration 7749: Policy loss: 0.408509. Value loss: 0.033114. Entropy: 0.301938.\n",
      "Training network. lr: 0.000191. clip: 0.076352\n",
      "Iteration 7750: Policy loss: 0.197920. Value loss: 0.112205. Entropy: 0.303767.\n",
      "Iteration 7751: Policy loss: 0.194092. Value loss: 0.044602. Entropy: 0.302971.\n",
      "Iteration 7752: Policy loss: 0.183056. Value loss: 0.029088. Entropy: 0.302100.\n",
      "episode: 3016   score: 210.0  epsilon: 1.0    steps: 184  evaluation reward: 390.15\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7753: Policy loss: 0.195403. Value loss: 0.129852. Entropy: 0.300266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7754: Policy loss: 0.185893. Value loss: 0.044644. Entropy: 0.299027.\n",
      "Iteration 7755: Policy loss: 0.189429. Value loss: 0.030587. Entropy: 0.299996.\n",
      "episode: 3017   score: 320.0  epsilon: 1.0    steps: 912  evaluation reward: 391.85\n",
      "episode: 3018   score: 460.0  epsilon: 1.0    steps: 1024  evaluation reward: 394.15\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7756: Policy loss: -0.023968. Value loss: 0.345293. Entropy: 0.301096.\n",
      "Iteration 7757: Policy loss: -0.059381. Value loss: 0.120123. Entropy: 0.300290.\n",
      "Iteration 7758: Policy loss: -0.057011. Value loss: 0.060263. Entropy: 0.299949.\n",
      "episode: 3019   score: 285.0  epsilon: 1.0    steps: 800  evaluation reward: 392.3\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7759: Policy loss: -0.066690. Value loss: 0.175662. Entropy: 0.299123.\n",
      "Iteration 7760: Policy loss: -0.084498. Value loss: 0.066982. Entropy: 0.298786.\n",
      "Iteration 7761: Policy loss: -0.095640. Value loss: 0.046931. Entropy: 0.298091.\n",
      "episode: 3020   score: 410.0  epsilon: 1.0    steps: 56  evaluation reward: 392.5\n",
      "episode: 3021   score: 165.0  epsilon: 1.0    steps: 368  evaluation reward: 390.8\n",
      "episode: 3022   score: 800.0  epsilon: 1.0    steps: 416  evaluation reward: 394.9\n",
      "episode: 3023   score: 590.0  epsilon: 1.0    steps: 664  evaluation reward: 398.55\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7762: Policy loss: 0.104090. Value loss: 0.053660. Entropy: 0.281455.\n",
      "Iteration 7763: Policy loss: 0.098743. Value loss: 0.031172. Entropy: 0.285748.\n",
      "Iteration 7764: Policy loss: 0.097270. Value loss: 0.026116. Entropy: 0.283610.\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7765: Policy loss: 0.042080. Value loss: 0.085101. Entropy: 0.310371.\n",
      "Iteration 7766: Policy loss: 0.033601. Value loss: 0.055352. Entropy: 0.309594.\n",
      "Iteration 7767: Policy loss: 0.031661. Value loss: 0.046417. Entropy: 0.309946.\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7768: Policy loss: 0.035766. Value loss: 0.166552. Entropy: 0.309064.\n",
      "Iteration 7769: Policy loss: 0.021557. Value loss: 0.079171. Entropy: 0.309035.\n",
      "Iteration 7770: Policy loss: 0.033098. Value loss: 0.062117. Entropy: 0.308331.\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7771: Policy loss: -0.023336. Value loss: 0.362989. Entropy: 0.309338.\n",
      "Iteration 7772: Policy loss: -0.035666. Value loss: 0.239445. Entropy: 0.308829.\n",
      "Iteration 7773: Policy loss: -0.052554. Value loss: 0.185031. Entropy: 0.309052.\n",
      "episode: 3024   score: 525.0  epsilon: 1.0    steps: 608  evaluation reward: 396.85\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7774: Policy loss: 0.387620. Value loss: 0.174021. Entropy: 0.305822.\n",
      "Iteration 7775: Policy loss: 0.363917. Value loss: 0.049105. Entropy: 0.305374.\n",
      "Iteration 7776: Policy loss: 0.364505. Value loss: 0.028936. Entropy: 0.305789.\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7777: Policy loss: 0.022533. Value loss: 0.159534. Entropy: 0.308267.\n",
      "Iteration 7778: Policy loss: 0.023794. Value loss: 0.064792. Entropy: 0.307956.\n",
      "Iteration 7779: Policy loss: 0.008230. Value loss: 0.043175. Entropy: 0.308943.\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7780: Policy loss: 0.245841. Value loss: 0.185256. Entropy: 0.307516.\n",
      "Iteration 7781: Policy loss: 0.240227. Value loss: 0.094142. Entropy: 0.305091.\n",
      "Iteration 7782: Policy loss: 0.241970. Value loss: 0.071878. Entropy: 0.305940.\n",
      "episode: 3025   score: 240.0  epsilon: 1.0    steps: 192  evaluation reward: 396.3\n",
      "episode: 3026   score: 365.0  epsilon: 1.0    steps: 432  evaluation reward: 397.55\n",
      "episode: 3027   score: 275.0  epsilon: 1.0    steps: 512  evaluation reward: 396.1\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7783: Policy loss: 0.370292. Value loss: 0.111248. Entropy: 0.284331.\n",
      "Iteration 7784: Policy loss: 0.365546. Value loss: 0.043967. Entropy: 0.281212.\n",
      "Iteration 7785: Policy loss: 0.367202. Value loss: 0.032375. Entropy: 0.280668.\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7786: Policy loss: -0.337737. Value loss: 0.310332. Entropy: 0.309505.\n",
      "Iteration 7787: Policy loss: -0.339131. Value loss: 0.172300. Entropy: 0.308892.\n",
      "Iteration 7788: Policy loss: -0.331423. Value loss: 0.093468. Entropy: 0.308474.\n",
      "episode: 3028   score: 335.0  epsilon: 1.0    steps: 88  evaluation reward: 396.85\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7789: Policy loss: 0.225172. Value loss: 0.088061. Entropy: 0.306949.\n",
      "Iteration 7790: Policy loss: 0.212269. Value loss: 0.037204. Entropy: 0.303416.\n",
      "Iteration 7791: Policy loss: 0.209820. Value loss: 0.028388. Entropy: 0.303425.\n",
      "episode: 3029   score: 590.0  epsilon: 1.0    steps: 80  evaluation reward: 397.0\n",
      "episode: 3030   score: 125.0  epsilon: 1.0    steps: 504  evaluation reward: 395.05\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7792: Policy loss: 0.441991. Value loss: 0.155749. Entropy: 0.290886.\n",
      "Iteration 7793: Policy loss: 0.439386. Value loss: 0.045203. Entropy: 0.291987.\n",
      "Iteration 7794: Policy loss: 0.429394. Value loss: 0.035891. Entropy: 0.290959.\n",
      "episode: 3031   score: 365.0  epsilon: 1.0    steps: 136  evaluation reward: 395.5\n",
      "episode: 3032   score: 80.0  epsilon: 1.0    steps: 920  evaluation reward: 391.0\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7795: Policy loss: -0.222882. Value loss: 0.295826. Entropy: 0.298439.\n",
      "Iteration 7796: Policy loss: -0.241097. Value loss: 0.177386. Entropy: 0.297646.\n",
      "Iteration 7797: Policy loss: -0.242823. Value loss: 0.091823. Entropy: 0.298389.\n",
      "Training network. lr: 0.000190. clip: 0.076195\n",
      "Iteration 7798: Policy loss: 0.069697. Value loss: 0.062237. Entropy: 0.304480.\n",
      "Iteration 7799: Policy loss: 0.061821. Value loss: 0.027284. Entropy: 0.305756.\n",
      "Iteration 7800: Policy loss: 0.061430. Value loss: 0.020744. Entropy: 0.304185.\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7801: Policy loss: 0.104451. Value loss: 0.138905. Entropy: 0.305188.\n",
      "Iteration 7802: Policy loss: 0.106170. Value loss: 0.045887. Entropy: 0.305329.\n",
      "Iteration 7803: Policy loss: 0.088384. Value loss: 0.031884. Entropy: 0.303828.\n",
      "episode: 3033   score: 760.0  epsilon: 1.0    steps: 128  evaluation reward: 391.95\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7804: Policy loss: -0.054644. Value loss: 0.285923. Entropy: 0.300374.\n",
      "Iteration 7805: Policy loss: -0.051950. Value loss: 0.096673. Entropy: 0.300821.\n",
      "Iteration 7806: Policy loss: -0.081582. Value loss: 0.061694. Entropy: 0.300442.\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7807: Policy loss: -0.078181. Value loss: 0.096263. Entropy: 0.309773.\n",
      "Iteration 7808: Policy loss: -0.085548. Value loss: 0.040601. Entropy: 0.310159.\n",
      "Iteration 7809: Policy loss: -0.094148. Value loss: 0.028222. Entropy: 0.309352.\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7810: Policy loss: -0.233548. Value loss: 0.348785. Entropy: 0.309224.\n",
      "Iteration 7811: Policy loss: -0.255025. Value loss: 0.132833. Entropy: 0.308711.\n",
      "Iteration 7812: Policy loss: -0.279554. Value loss: 0.089838. Entropy: 0.308976.\n",
      "episode: 3034   score: 390.0  epsilon: 1.0    steps: 136  evaluation reward: 391.3\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7813: Policy loss: 0.034549. Value loss: 0.185383. Entropy: 0.299269.\n",
      "Iteration 7814: Policy loss: 0.015418. Value loss: 0.069876. Entropy: 0.298510.\n",
      "Iteration 7815: Policy loss: 0.026174. Value loss: 0.039035. Entropy: 0.298267.\n",
      "episode: 3035   score: 440.0  epsilon: 1.0    steps: 568  evaluation reward: 392.05\n",
      "episode: 3036   score: 365.0  epsilon: 1.0    steps: 720  evaluation reward: 391.5\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7816: Policy loss: 0.179127. Value loss: 0.147948. Entropy: 0.289711.\n",
      "Iteration 7817: Policy loss: 0.171664. Value loss: 0.060960. Entropy: 0.289599.\n",
      "Iteration 7818: Policy loss: 0.153622. Value loss: 0.041693. Entropy: 0.289863.\n",
      "episode: 3037   score: 590.0  epsilon: 1.0    steps: 944  evaluation reward: 393.2\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7819: Policy loss: -0.374029. Value loss: 0.201517. Entropy: 0.309442.\n",
      "Iteration 7820: Policy loss: -0.382616. Value loss: 0.061953. Entropy: 0.309912.\n",
      "Iteration 7821: Policy loss: -0.407009. Value loss: 0.040669. Entropy: 0.310142.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3038   score: 905.0  epsilon: 1.0    steps: 104  evaluation reward: 397.65\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7822: Policy loss: 0.129552. Value loss: 0.165139. Entropy: 0.299401.\n",
      "Iteration 7823: Policy loss: 0.112944. Value loss: 0.065121. Entropy: 0.299734.\n",
      "Iteration 7824: Policy loss: 0.106087. Value loss: 0.045572. Entropy: 0.296274.\n",
      "episode: 3039   score: 670.0  epsilon: 1.0    steps: 648  evaluation reward: 401.0\n",
      "episode: 3040   score: 420.0  epsilon: 1.0    steps: 968  evaluation reward: 402.6\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7825: Policy loss: 0.093583. Value loss: 0.545675. Entropy: 0.302620.\n",
      "Iteration 7826: Policy loss: 0.075987. Value loss: 0.296728. Entropy: 0.301728.\n",
      "Iteration 7827: Policy loss: 0.058051. Value loss: 0.231439. Entropy: 0.301109.\n",
      "episode: 3041   score: 745.0  epsilon: 1.0    steps: 648  evaluation reward: 407.65\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7828: Policy loss: 0.054219. Value loss: 0.207976. Entropy: 0.290779.\n",
      "Iteration 7829: Policy loss: 0.026825. Value loss: 0.089597. Entropy: 0.289521.\n",
      "Iteration 7830: Policy loss: 0.020602. Value loss: 0.050223. Entropy: 0.289347.\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7831: Policy loss: -0.128195. Value loss: 0.150912. Entropy: 0.313100.\n",
      "Iteration 7832: Policy loss: -0.138317. Value loss: 0.061982. Entropy: 0.312635.\n",
      "Iteration 7833: Policy loss: -0.140457. Value loss: 0.041717. Entropy: 0.311292.\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7834: Policy loss: 0.032631. Value loss: 0.092209. Entropy: 0.310596.\n",
      "Iteration 7835: Policy loss: 0.026716. Value loss: 0.038267. Entropy: 0.309794.\n",
      "Iteration 7836: Policy loss: 0.019007. Value loss: 0.028986. Entropy: 0.310270.\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7837: Policy loss: 0.382523. Value loss: 0.223540. Entropy: 0.303544.\n",
      "Iteration 7838: Policy loss: 0.376323. Value loss: 0.074977. Entropy: 0.303182.\n",
      "Iteration 7839: Policy loss: 0.360250. Value loss: 0.045385. Entropy: 0.302644.\n",
      "episode: 3042   score: 240.0  epsilon: 1.0    steps: 304  evaluation reward: 407.95\n",
      "episode: 3043   score: 210.0  epsilon: 1.0    steps: 584  evaluation reward: 407.95\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7840: Policy loss: 0.055873. Value loss: 0.047735. Entropy: 0.287547.\n",
      "Iteration 7841: Policy loss: 0.045871. Value loss: 0.025672. Entropy: 0.284622.\n",
      "Iteration 7842: Policy loss: 0.043099. Value loss: 0.019314. Entropy: 0.286511.\n",
      "episode: 3044   score: 475.0  epsilon: 1.0    steps: 856  evaluation reward: 409.6\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7843: Policy loss: -0.149688. Value loss: 0.315858. Entropy: 0.302158.\n",
      "Iteration 7844: Policy loss: -0.153274. Value loss: 0.204019. Entropy: 0.299981.\n",
      "Iteration 7845: Policy loss: -0.148916. Value loss: 0.147422. Entropy: 0.299255.\n",
      "episode: 3045   score: 635.0  epsilon: 1.0    steps: 920  evaluation reward: 410.65\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7846: Policy loss: 0.009449. Value loss: 0.061781. Entropy: 0.300802.\n",
      "Iteration 7847: Policy loss: 0.009049. Value loss: 0.037637. Entropy: 0.302770.\n",
      "Iteration 7848: Policy loss: 0.003620. Value loss: 0.028321. Entropy: 0.302641.\n",
      "episode: 3046   score: 260.0  epsilon: 1.0    steps: 824  evaluation reward: 404.75\n",
      "Training network. lr: 0.000190. clip: 0.076048\n",
      "Iteration 7849: Policy loss: 0.084039. Value loss: 0.085875. Entropy: 0.296452.\n",
      "Iteration 7850: Policy loss: 0.081682. Value loss: 0.031459. Entropy: 0.297711.\n",
      "Iteration 7851: Policy loss: 0.072113. Value loss: 0.022976. Entropy: 0.296406.\n",
      "episode: 3047   score: 315.0  epsilon: 1.0    steps: 328  evaluation reward: 405.3\n",
      "episode: 3048   score: 450.0  epsilon: 1.0    steps: 392  evaluation reward: 403.45\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7852: Policy loss: -0.078279. Value loss: 0.043133. Entropy: 0.279608.\n",
      "Iteration 7853: Policy loss: -0.079456. Value loss: 0.018815. Entropy: 0.278071.\n",
      "Iteration 7854: Policy loss: -0.077105. Value loss: 0.015542. Entropy: 0.276982.\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7855: Policy loss: -0.235133. Value loss: 0.369264. Entropy: 0.309933.\n",
      "Iteration 7856: Policy loss: -0.256921. Value loss: 0.184909. Entropy: 0.308944.\n",
      "Iteration 7857: Policy loss: -0.263166. Value loss: 0.100760. Entropy: 0.309887.\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7858: Policy loss: 0.050053. Value loss: 0.260602. Entropy: 0.309869.\n",
      "Iteration 7859: Policy loss: 0.032028. Value loss: 0.144402. Entropy: 0.310054.\n",
      "Iteration 7860: Policy loss: 0.032562. Value loss: 0.089612. Entropy: 0.309307.\n",
      "episode: 3049   score: 260.0  epsilon: 1.0    steps: 992  evaluation reward: 401.7\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7861: Policy loss: -0.011990. Value loss: 0.106979. Entropy: 0.305543.\n",
      "Iteration 7862: Policy loss: -0.020646. Value loss: 0.033252. Entropy: 0.305733.\n",
      "Iteration 7863: Policy loss: -0.024330. Value loss: 0.023858. Entropy: 0.305137.\n",
      "episode: 3050   score: 265.0  epsilon: 1.0    steps: 104  evaluation reward: 398.6\n",
      "now time :  2019-09-05 22:22:55.706138\n",
      "episode: 3051   score: 495.0  epsilon: 1.0    steps: 592  evaluation reward: 400.95\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7864: Policy loss: 0.118252. Value loss: 0.096205. Entropy: 0.277933.\n",
      "Iteration 7865: Policy loss: 0.105529. Value loss: 0.033316. Entropy: 0.277394.\n",
      "Iteration 7866: Policy loss: 0.096340. Value loss: 0.022866. Entropy: 0.277863.\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7867: Policy loss: -0.049605. Value loss: 0.058613. Entropy: 0.314952.\n",
      "Iteration 7868: Policy loss: -0.056534. Value loss: 0.025834. Entropy: 0.315003.\n",
      "Iteration 7869: Policy loss: -0.058490. Value loss: 0.019074. Entropy: 0.314343.\n",
      "episode: 3052   score: 480.0  epsilon: 1.0    steps: 456  evaluation reward: 399.55\n",
      "episode: 3053   score: 240.0  epsilon: 1.0    steps: 544  evaluation reward: 398.85\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7870: Policy loss: 0.306898. Value loss: 0.128826. Entropy: 0.291388.\n",
      "Iteration 7871: Policy loss: 0.310914. Value loss: 0.051822. Entropy: 0.293709.\n",
      "Iteration 7872: Policy loss: 0.297770. Value loss: 0.039763. Entropy: 0.293725.\n",
      "episode: 3054   score: 460.0  epsilon: 1.0    steps: 640  evaluation reward: 400.8\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7873: Policy loss: -0.429754. Value loss: 0.184384. Entropy: 0.297480.\n",
      "Iteration 7874: Policy loss: -0.445373. Value loss: 0.087970. Entropy: 0.298170.\n",
      "Iteration 7875: Policy loss: -0.444024. Value loss: 0.063257. Entropy: 0.298815.\n",
      "episode: 3055   score: 340.0  epsilon: 1.0    steps: 456  evaluation reward: 400.55\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7876: Policy loss: 0.345137. Value loss: 0.156956. Entropy: 0.299019.\n",
      "Iteration 7877: Policy loss: 0.349884. Value loss: 0.065979. Entropy: 0.296526.\n",
      "Iteration 7878: Policy loss: 0.342571. Value loss: 0.039765. Entropy: 0.296642.\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7879: Policy loss: 0.114210. Value loss: 0.074519. Entropy: 0.309789.\n",
      "Iteration 7880: Policy loss: 0.102819. Value loss: 0.031540. Entropy: 0.309352.\n",
      "Iteration 7881: Policy loss: 0.104031. Value loss: 0.025700. Entropy: 0.309065.\n",
      "episode: 3056   score: 210.0  epsilon: 1.0    steps: 504  evaluation reward: 396.45\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7882: Policy loss: 0.152240. Value loss: 0.122380. Entropy: 0.296297.\n",
      "Iteration 7883: Policy loss: 0.146522. Value loss: 0.043243. Entropy: 0.294041.\n",
      "Iteration 7884: Policy loss: 0.137129. Value loss: 0.034933. Entropy: 0.293755.\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7885: Policy loss: 0.172310. Value loss: 0.089399. Entropy: 0.310328.\n",
      "Iteration 7886: Policy loss: 0.166287. Value loss: 0.032601. Entropy: 0.309985.\n",
      "Iteration 7887: Policy loss: 0.162499. Value loss: 0.024039. Entropy: 0.309658.\n",
      "episode: 3057   score: 435.0  epsilon: 1.0    steps: 240  evaluation reward: 396.65\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7888: Policy loss: -0.204288. Value loss: 0.252292. Entropy: 0.297157.\n",
      "Iteration 7889: Policy loss: -0.234033. Value loss: 0.141928. Entropy: 0.296304.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7890: Policy loss: -0.238149. Value loss: 0.078266. Entropy: 0.295286.\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7891: Policy loss: -0.004486. Value loss: 0.126283. Entropy: 0.311144.\n",
      "Iteration 7892: Policy loss: -0.013687. Value loss: 0.053998. Entropy: 0.309958.\n",
      "Iteration 7893: Policy loss: -0.021384. Value loss: 0.037984. Entropy: 0.309569.\n",
      "episode: 3058   score: 260.0  epsilon: 1.0    steps: 24  evaluation reward: 393.35\n",
      "episode: 3059   score: 310.0  epsilon: 1.0    steps: 752  evaluation reward: 393.8\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7894: Policy loss: 0.177059. Value loss: 0.108875. Entropy: 0.297962.\n",
      "Iteration 7895: Policy loss: 0.160508. Value loss: 0.029580. Entropy: 0.297415.\n",
      "Iteration 7896: Policy loss: 0.154393. Value loss: 0.024139. Entropy: 0.297565.\n",
      "episode: 3060   score: 470.0  epsilon: 1.0    steps: 376  evaluation reward: 395.6\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7897: Policy loss: -0.166478. Value loss: 0.300246. Entropy: 0.298615.\n",
      "Iteration 7898: Policy loss: -0.180186. Value loss: 0.180374. Entropy: 0.296793.\n",
      "Iteration 7899: Policy loss: -0.188880. Value loss: 0.065250. Entropy: 0.297329.\n",
      "episode: 3061   score: 820.0  epsilon: 1.0    steps: 232  evaluation reward: 401.7\n",
      "episode: 3062   score: 450.0  epsilon: 1.0    steps: 400  evaluation reward: 402.7\n",
      "episode: 3063   score: 210.0  epsilon: 1.0    steps: 856  evaluation reward: 402.7\n",
      "episode: 3064   score: 290.0  epsilon: 1.0    steps: 888  evaluation reward: 400.25\n",
      "Training network. lr: 0.000190. clip: 0.075891\n",
      "Iteration 7900: Policy loss: 0.138091. Value loss: 0.065931. Entropy: 0.281528.\n",
      "Iteration 7901: Policy loss: 0.128238. Value loss: 0.033854. Entropy: 0.280172.\n",
      "Iteration 7902: Policy loss: 0.128111. Value loss: 0.028403. Entropy: 0.281460.\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7903: Policy loss: 0.063073. Value loss: 0.133510. Entropy: 0.308582.\n",
      "Iteration 7904: Policy loss: 0.049833. Value loss: 0.068245. Entropy: 0.306249.\n",
      "Iteration 7905: Policy loss: 0.053874. Value loss: 0.049954. Entropy: 0.306819.\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7906: Policy loss: -0.398878. Value loss: 0.188692. Entropy: 0.308062.\n",
      "Iteration 7907: Policy loss: -0.408892. Value loss: 0.073598. Entropy: 0.308783.\n",
      "Iteration 7908: Policy loss: -0.411603. Value loss: 0.045699. Entropy: 0.308377.\n",
      "episode: 3065   score: 285.0  epsilon: 1.0    steps: 968  evaluation reward: 397.2\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7909: Policy loss: 0.362667. Value loss: 0.258830. Entropy: 0.307863.\n",
      "Iteration 7910: Policy loss: 0.335684. Value loss: 0.091995. Entropy: 0.307502.\n",
      "Iteration 7911: Policy loss: 0.321885. Value loss: 0.060142. Entropy: 0.306928.\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7912: Policy loss: 0.139817. Value loss: 0.072253. Entropy: 0.301842.\n",
      "Iteration 7913: Policy loss: 0.139960. Value loss: 0.023005. Entropy: 0.299975.\n",
      "Iteration 7914: Policy loss: 0.134084. Value loss: 0.015378. Entropy: 0.300423.\n",
      "episode: 3066   score: 240.0  epsilon: 1.0    steps: 792  evaluation reward: 396.45\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7915: Policy loss: 0.264562. Value loss: 0.104098. Entropy: 0.296939.\n",
      "Iteration 7916: Policy loss: 0.258792. Value loss: 0.043117. Entropy: 0.295510.\n",
      "Iteration 7917: Policy loss: 0.255060. Value loss: 0.032064. Entropy: 0.296114.\n",
      "episode: 3067   score: 285.0  epsilon: 1.0    steps: 400  evaluation reward: 394.7\n",
      "episode: 3068   score: 210.0  epsilon: 1.0    steps: 656  evaluation reward: 391.0\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7918: Policy loss: -0.158683. Value loss: 0.094635. Entropy: 0.286843.\n",
      "Iteration 7919: Policy loss: -0.163810. Value loss: 0.047223. Entropy: 0.286029.\n",
      "Iteration 7920: Policy loss: -0.162080. Value loss: 0.037955. Entropy: 0.291518.\n",
      "episode: 3069   score: 480.0  epsilon: 1.0    steps: 312  evaluation reward: 392.9\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7921: Policy loss: -0.327315. Value loss: 0.345049. Entropy: 0.298649.\n",
      "Iteration 7922: Policy loss: -0.319998. Value loss: 0.126481. Entropy: 0.299043.\n",
      "Iteration 7923: Policy loss: -0.322436. Value loss: 0.049186. Entropy: 0.299888.\n",
      "episode: 3070   score: 285.0  epsilon: 1.0    steps: 40  evaluation reward: 392.85\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7924: Policy loss: -0.100821. Value loss: 0.055236. Entropy: 0.304774.\n",
      "Iteration 7925: Policy loss: -0.106243. Value loss: 0.031185. Entropy: 0.305786.\n",
      "Iteration 7926: Policy loss: -0.101084. Value loss: 0.017386. Entropy: 0.304186.\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7927: Policy loss: 0.161816. Value loss: 0.063519. Entropy: 0.307063.\n",
      "Iteration 7928: Policy loss: 0.156974. Value loss: 0.017662. Entropy: 0.305764.\n",
      "Iteration 7929: Policy loss: 0.151484. Value loss: 0.013512. Entropy: 0.305232.\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7930: Policy loss: -0.530884. Value loss: 0.466069. Entropy: 0.310924.\n",
      "Iteration 7931: Policy loss: -0.523843. Value loss: 0.179503. Entropy: 0.310542.\n",
      "Iteration 7932: Policy loss: -0.555351. Value loss: 0.090760. Entropy: 0.310853.\n",
      "episode: 3071   score: 265.0  epsilon: 1.0    steps: 904  evaluation reward: 393.1\n",
      "episode: 3072   score: 545.0  epsilon: 1.0    steps: 976  evaluation reward: 395.85\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7933: Policy loss: 0.094199. Value loss: 0.193502. Entropy: 0.302712.\n",
      "Iteration 7934: Policy loss: 0.100657. Value loss: 0.057917. Entropy: 0.302209.\n",
      "Iteration 7935: Policy loss: 0.084735. Value loss: 0.047780. Entropy: 0.303235.\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7936: Policy loss: -0.113968. Value loss: 0.077993. Entropy: 0.298146.\n",
      "Iteration 7937: Policy loss: -0.116235. Value loss: 0.031650. Entropy: 0.298362.\n",
      "Iteration 7938: Policy loss: -0.121992. Value loss: 0.020924. Entropy: 0.299480.\n",
      "episode: 3073   score: 850.0  epsilon: 1.0    steps: 96  evaluation reward: 400.2\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7939: Policy loss: 0.285382. Value loss: 0.089765. Entropy: 0.298084.\n",
      "Iteration 7940: Policy loss: 0.276590. Value loss: 0.037207. Entropy: 0.297407.\n",
      "Iteration 7941: Policy loss: 0.271346. Value loss: 0.030748. Entropy: 0.295009.\n",
      "episode: 3074   score: 300.0  epsilon: 1.0    steps: 464  evaluation reward: 400.35\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7942: Policy loss: -0.148535. Value loss: 0.205946. Entropy: 0.295298.\n",
      "Iteration 7943: Policy loss: -0.147050. Value loss: 0.098303. Entropy: 0.297579.\n",
      "Iteration 7944: Policy loss: -0.151786. Value loss: 0.081950. Entropy: 0.295722.\n",
      "episode: 3075   score: 150.0  epsilon: 1.0    steps: 96  evaluation reward: 399.45\n",
      "episode: 3076   score: 460.0  epsilon: 1.0    steps: 744  evaluation reward: 397.65\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7945: Policy loss: -0.077230. Value loss: 0.085507. Entropy: 0.283902.\n",
      "Iteration 7946: Policy loss: -0.084830. Value loss: 0.051718. Entropy: 0.283853.\n",
      "Iteration 7947: Policy loss: -0.085322. Value loss: 0.040581. Entropy: 0.283010.\n",
      "Training network. lr: 0.000189. clip: 0.075734\n",
      "Iteration 7948: Policy loss: 0.369868. Value loss: 0.269841. Entropy: 0.310707.\n",
      "Iteration 7949: Policy loss: 0.345500. Value loss: 0.066650. Entropy: 0.309995.\n",
      "Iteration 7950: Policy loss: 0.335339. Value loss: 0.043248. Entropy: 0.309260.\n",
      "episode: 3077   score: 390.0  epsilon: 1.0    steps: 144  evaluation reward: 398.2\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7951: Policy loss: -0.088976. Value loss: 0.424022. Entropy: 0.296957.\n",
      "Iteration 7952: Policy loss: -0.110893. Value loss: 0.183337. Entropy: 0.295999.\n",
      "Iteration 7953: Policy loss: -0.127014. Value loss: 0.100213. Entropy: 0.297374.\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7954: Policy loss: 0.036322. Value loss: 0.123351. Entropy: 0.305555.\n",
      "Iteration 7955: Policy loss: 0.026924. Value loss: 0.071938. Entropy: 0.305277.\n",
      "Iteration 7956: Policy loss: 0.028186. Value loss: 0.051221. Entropy: 0.304816.\n",
      "episode: 3078   score: 380.0  epsilon: 1.0    steps: 392  evaluation reward: 399.1\n",
      "episode: 3079   score: 440.0  epsilon: 1.0    steps: 640  evaluation reward: 399.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7957: Policy loss: -0.446010. Value loss: 0.281024. Entropy: 0.282022.\n",
      "Iteration 7958: Policy loss: -0.478966. Value loss: 0.097537. Entropy: 0.280673.\n",
      "Iteration 7959: Policy loss: -0.484525. Value loss: 0.056052. Entropy: 0.281512.\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7960: Policy loss: -0.148859. Value loss: 0.236901. Entropy: 0.312493.\n",
      "Iteration 7961: Policy loss: -0.159272. Value loss: 0.119983. Entropy: 0.310648.\n",
      "Iteration 7962: Policy loss: -0.167485. Value loss: 0.074548. Entropy: 0.311238.\n",
      "episode: 3080   score: 565.0  epsilon: 1.0    steps: 528  evaluation reward: 397.55\n",
      "episode: 3081   score: 545.0  epsilon: 1.0    steps: 1016  evaluation reward: 400.5\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7963: Policy loss: 0.190439. Value loss: 0.201872. Entropy: 0.301907.\n",
      "Iteration 7964: Policy loss: 0.191716. Value loss: 0.081875. Entropy: 0.303825.\n",
      "Iteration 7965: Policy loss: 0.180167. Value loss: 0.054724. Entropy: 0.302046.\n",
      "episode: 3082   score: 460.0  epsilon: 1.0    steps: 560  evaluation reward: 400.9\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7966: Policy loss: 0.167405. Value loss: 0.131195. Entropy: 0.293102.\n",
      "Iteration 7967: Policy loss: 0.165323. Value loss: 0.055850. Entropy: 0.291147.\n",
      "Iteration 7968: Policy loss: 0.157103. Value loss: 0.035681. Entropy: 0.292144.\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7969: Policy loss: -0.081463. Value loss: 0.144690. Entropy: 0.312890.\n",
      "Iteration 7970: Policy loss: -0.093443. Value loss: 0.059575. Entropy: 0.311109.\n",
      "Iteration 7971: Policy loss: -0.103342. Value loss: 0.042403. Entropy: 0.310715.\n",
      "episode: 3083   score: 125.0  epsilon: 1.0    steps: 520  evaluation reward: 397.55\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7972: Policy loss: 0.110000. Value loss: 0.124335. Entropy: 0.305581.\n",
      "Iteration 7973: Policy loss: 0.105050. Value loss: 0.070010. Entropy: 0.302980.\n",
      "Iteration 7974: Policy loss: 0.104473. Value loss: 0.046489. Entropy: 0.303742.\n",
      "episode: 3084   score: 640.0  epsilon: 1.0    steps: 1024  evaluation reward: 400.3\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7975: Policy loss: 0.252273. Value loss: 0.208842. Entropy: 0.302341.\n",
      "Iteration 7976: Policy loss: 0.230753. Value loss: 0.066997. Entropy: 0.300912.\n",
      "Iteration 7977: Policy loss: 0.226332. Value loss: 0.040432. Entropy: 0.302058.\n",
      "episode: 3085   score: 345.0  epsilon: 1.0    steps: 720  evaluation reward: 398.4\n",
      "episode: 3086   score: 620.0  epsilon: 1.0    steps: 920  evaluation reward: 400.45\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7978: Policy loss: 0.168979. Value loss: 0.122928. Entropy: 0.294899.\n",
      "Iteration 7979: Policy loss: 0.173048. Value loss: 0.051257. Entropy: 0.294202.\n",
      "Iteration 7980: Policy loss: 0.161059. Value loss: 0.036726. Entropy: 0.295484.\n",
      "episode: 3087   score: 210.0  epsilon: 1.0    steps: 104  evaluation reward: 399.7\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7981: Policy loss: -0.273993. Value loss: 0.246909. Entropy: 0.301016.\n",
      "Iteration 7982: Policy loss: -0.275872. Value loss: 0.098904. Entropy: 0.298412.\n",
      "Iteration 7983: Policy loss: -0.277811. Value loss: 0.069259. Entropy: 0.300950.\n",
      "episode: 3088   score: 345.0  epsilon: 1.0    steps: 544  evaluation reward: 397.2\n",
      "episode: 3089   score: 415.0  epsilon: 1.0    steps: 968  evaluation reward: 398.2\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7984: Policy loss: 0.066563. Value loss: 0.137955. Entropy: 0.302023.\n",
      "Iteration 7985: Policy loss: 0.052806. Value loss: 0.069115. Entropy: 0.300525.\n",
      "Iteration 7986: Policy loss: 0.058325. Value loss: 0.045716. Entropy: 0.299092.\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7987: Policy loss: -0.082138. Value loss: 0.057894. Entropy: 0.302484.\n",
      "Iteration 7988: Policy loss: -0.084470. Value loss: 0.030486. Entropy: 0.301835.\n",
      "Iteration 7989: Policy loss: -0.087017. Value loss: 0.021002. Entropy: 0.302875.\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7990: Policy loss: -0.288185. Value loss: 0.196528. Entropy: 0.306804.\n",
      "Iteration 7991: Policy loss: -0.301087. Value loss: 0.083184. Entropy: 0.307751.\n",
      "Iteration 7992: Policy loss: -0.308579. Value loss: 0.049357. Entropy: 0.307771.\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7993: Policy loss: -0.148360. Value loss: 0.253083. Entropy: 0.307585.\n",
      "Iteration 7994: Policy loss: -0.161185. Value loss: 0.065305. Entropy: 0.308797.\n",
      "Iteration 7995: Policy loss: -0.174997. Value loss: 0.040975. Entropy: 0.309337.\n",
      "episode: 3090   score: 460.0  epsilon: 1.0    steps: 216  evaluation reward: 399.15\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7996: Policy loss: 0.001255. Value loss: 0.301873. Entropy: 0.295465.\n",
      "Iteration 7997: Policy loss: -0.000707. Value loss: 0.080562. Entropy: 0.296837.\n",
      "Iteration 7998: Policy loss: -0.028420. Value loss: 0.046653. Entropy: 0.295721.\n",
      "episode: 3091   score: 465.0  epsilon: 1.0    steps: 992  evaluation reward: 399.3\n",
      "Training network. lr: 0.000189. clip: 0.075587\n",
      "Iteration 7999: Policy loss: 0.233776. Value loss: 0.155335. Entropy: 0.305542.\n",
      "Iteration 8000: Policy loss: 0.226048. Value loss: 0.050799. Entropy: 0.304657.\n",
      "Iteration 8001: Policy loss: 0.217125. Value loss: 0.033177. Entropy: 0.306186.\n",
      "episode: 3092   score: 395.0  epsilon: 1.0    steps: 96  evaluation reward: 399.0\n",
      "episode: 3093   score: 240.0  epsilon: 1.0    steps: 200  evaluation reward: 399.0\n",
      "episode: 3094   score: 490.0  epsilon: 1.0    steps: 768  evaluation reward: 401.3\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8002: Policy loss: 0.066622. Value loss: 0.095522. Entropy: 0.284407.\n",
      "Iteration 8003: Policy loss: 0.062714. Value loss: 0.041865. Entropy: 0.285271.\n",
      "Iteration 8004: Policy loss: 0.056022. Value loss: 0.031321. Entropy: 0.281755.\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8005: Policy loss: -0.167870. Value loss: 0.088067. Entropy: 0.309516.\n",
      "Iteration 8006: Policy loss: -0.179833. Value loss: 0.040286. Entropy: 0.309211.\n",
      "Iteration 8007: Policy loss: -0.180934. Value loss: 0.029179. Entropy: 0.310199.\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8008: Policy loss: -0.213270. Value loss: 0.301290. Entropy: 0.304695.\n",
      "Iteration 8009: Policy loss: -0.236927. Value loss: 0.142451. Entropy: 0.304860.\n",
      "Iteration 8010: Policy loss: -0.240612. Value loss: 0.095968. Entropy: 0.305428.\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8011: Policy loss: 0.074132. Value loss: 0.106003. Entropy: 0.304034.\n",
      "Iteration 8012: Policy loss: 0.056566. Value loss: 0.026643. Entropy: 0.304249.\n",
      "Iteration 8013: Policy loss: 0.056327. Value loss: 0.017711. Entropy: 0.304907.\n",
      "episode: 3095   score: 635.0  epsilon: 1.0    steps: 704  evaluation reward: 403.55\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8014: Policy loss: 0.180200. Value loss: 0.156638. Entropy: 0.296055.\n",
      "Iteration 8015: Policy loss: 0.177935. Value loss: 0.055608. Entropy: 0.296027.\n",
      "Iteration 8016: Policy loss: 0.154507. Value loss: 0.031201. Entropy: 0.295302.\n",
      "episode: 3096   score: 300.0  epsilon: 1.0    steps: 240  evaluation reward: 402.6\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8017: Policy loss: 0.265612. Value loss: 0.136258. Entropy: 0.297728.\n",
      "Iteration 8018: Policy loss: 0.250454. Value loss: 0.062719. Entropy: 0.297290.\n",
      "Iteration 8019: Policy loss: 0.243179. Value loss: 0.043472. Entropy: 0.298552.\n",
      "episode: 3097   score: 450.0  epsilon: 1.0    steps: 160  evaluation reward: 404.95\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8020: Policy loss: -0.073481. Value loss: 0.122113. Entropy: 0.302460.\n",
      "Iteration 8021: Policy loss: -0.093074. Value loss: 0.060191. Entropy: 0.301900.\n",
      "Iteration 8022: Policy loss: -0.093045. Value loss: 0.043565. Entropy: 0.302303.\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8023: Policy loss: 0.100705. Value loss: 0.121707. Entropy: 0.307676.\n",
      "Iteration 8024: Policy loss: 0.104603. Value loss: 0.057877. Entropy: 0.306818.\n",
      "Iteration 8025: Policy loss: 0.089416. Value loss: 0.039631. Entropy: 0.307386.\n",
      "episode: 3098   score: 575.0  epsilon: 1.0    steps: 160  evaluation reward: 404.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3099   score: 260.0  epsilon: 1.0    steps: 432  evaluation reward: 403.75\n",
      "episode: 3100   score: 535.0  epsilon: 1.0    steps: 808  evaluation reward: 406.7\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8026: Policy loss: -0.177271. Value loss: 0.240016. Entropy: 0.276031.\n",
      "Iteration 8027: Policy loss: -0.188505. Value loss: 0.146078. Entropy: 0.274464.\n",
      "Iteration 8028: Policy loss: -0.188411. Value loss: 0.070414. Entropy: 0.275327.\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8029: Policy loss: -0.022451. Value loss: 0.071845. Entropy: 0.311658.\n",
      "Iteration 8030: Policy loss: -0.023845. Value loss: 0.035172. Entropy: 0.310418.\n",
      "Iteration 8031: Policy loss: -0.025356. Value loss: 0.027159. Entropy: 0.310259.\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8032: Policy loss: -0.022985. Value loss: 0.194713. Entropy: 0.305457.\n",
      "Iteration 8033: Policy loss: -0.027927. Value loss: 0.063671. Entropy: 0.306063.\n",
      "Iteration 8034: Policy loss: -0.034198. Value loss: 0.038529. Entropy: 0.306557.\n",
      "now time :  2019-09-05 22:33:30.691618\n",
      "episode: 3101   score: 390.0  epsilon: 1.0    steps: 232  evaluation reward: 405.0\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8035: Policy loss: 0.213944. Value loss: 0.201808. Entropy: 0.297134.\n",
      "Iteration 8036: Policy loss: 0.191373. Value loss: 0.060777. Entropy: 0.297183.\n",
      "Iteration 8037: Policy loss: 0.181458. Value loss: 0.026922. Entropy: 0.296544.\n",
      "episode: 3102   score: 745.0  epsilon: 1.0    steps: 80  evaluation reward: 406.55\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8038: Policy loss: 0.045514. Value loss: 0.072187. Entropy: 0.304045.\n",
      "Iteration 8039: Policy loss: 0.043800. Value loss: 0.038296. Entropy: 0.303881.\n",
      "Iteration 8040: Policy loss: 0.044004. Value loss: 0.029976. Entropy: 0.304248.\n",
      "episode: 3103   score: 515.0  epsilon: 1.0    steps: 696  evaluation reward: 405.8\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8041: Policy loss: -0.232751. Value loss: 0.306034. Entropy: 0.297654.\n",
      "Iteration 8042: Policy loss: -0.267283. Value loss: 0.079773. Entropy: 0.298728.\n",
      "Iteration 8043: Policy loss: -0.290049. Value loss: 0.046466. Entropy: 0.298145.\n",
      "episode: 3104   score: 665.0  epsilon: 1.0    steps: 736  evaluation reward: 410.6\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8044: Policy loss: 0.253719. Value loss: 0.202068. Entropy: 0.304565.\n",
      "Iteration 8045: Policy loss: 0.253292. Value loss: 0.065143. Entropy: 0.306685.\n",
      "Iteration 8046: Policy loss: 0.237427. Value loss: 0.038998. Entropy: 0.304923.\n",
      "episode: 3105   score: 790.0  epsilon: 1.0    steps: 856  evaluation reward: 414.3\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8047: Policy loss: 0.161813. Value loss: 0.136702. Entropy: 0.305763.\n",
      "Iteration 8048: Policy loss: 0.152864. Value loss: 0.049688. Entropy: 0.304130.\n",
      "Iteration 8049: Policy loss: 0.153356. Value loss: 0.029349. Entropy: 0.304348.\n",
      "episode: 3106   score: 505.0  epsilon: 1.0    steps: 432  evaluation reward: 414.95\n",
      "Training network. lr: 0.000189. clip: 0.075430\n",
      "Iteration 8050: Policy loss: 0.004603. Value loss: 0.113520. Entropy: 0.298667.\n",
      "Iteration 8051: Policy loss: 0.002786. Value loss: 0.049919. Entropy: 0.298674.\n",
      "Iteration 8052: Policy loss: -0.003974. Value loss: 0.032793. Entropy: 0.299777.\n",
      "episode: 3107   score: 310.0  epsilon: 1.0    steps: 352  evaluation reward: 416.8\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8053: Policy loss: -0.042899. Value loss: 0.086949. Entropy: 0.301824.\n",
      "Iteration 8054: Policy loss: -0.053197. Value loss: 0.043307. Entropy: 0.302919.\n",
      "Iteration 8055: Policy loss: -0.048399. Value loss: 0.029099. Entropy: 0.301330.\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8056: Policy loss: -0.016195. Value loss: 0.304559. Entropy: 0.306931.\n",
      "Iteration 8057: Policy loss: -0.022370. Value loss: 0.112950. Entropy: 0.306351.\n",
      "Iteration 8058: Policy loss: -0.034584. Value loss: 0.055029. Entropy: 0.304489.\n",
      "episode: 3108   score: 420.0  epsilon: 1.0    steps: 440  evaluation reward: 417.3\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8059: Policy loss: 0.320745. Value loss: 0.152737. Entropy: 0.294984.\n",
      "Iteration 8060: Policy loss: 0.313370. Value loss: 0.044221. Entropy: 0.292800.\n",
      "Iteration 8061: Policy loss: 0.307648. Value loss: 0.023411. Entropy: 0.290873.\n",
      "episode: 3109   score: 210.0  epsilon: 1.0    steps: 312  evaluation reward: 413.95\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8062: Policy loss: 0.016551. Value loss: 0.091532. Entropy: 0.294468.\n",
      "Iteration 8063: Policy loss: 0.005321. Value loss: 0.028201. Entropy: 0.294453.\n",
      "Iteration 8064: Policy loss: -0.000879. Value loss: 0.015747. Entropy: 0.293884.\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8065: Policy loss: -0.202662. Value loss: 0.243178. Entropy: 0.308884.\n",
      "Iteration 8066: Policy loss: -0.187806. Value loss: 0.100439. Entropy: 0.308633.\n",
      "Iteration 8067: Policy loss: -0.205321. Value loss: 0.075081. Entropy: 0.309022.\n",
      "episode: 3110   score: 620.0  epsilon: 1.0    steps: 952  evaluation reward: 416.25\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8068: Policy loss: 0.025476. Value loss: 0.153405. Entropy: 0.300268.\n",
      "Iteration 8069: Policy loss: 0.004170. Value loss: 0.064972. Entropy: 0.300873.\n",
      "Iteration 8070: Policy loss: 0.004801. Value loss: 0.044921. Entropy: 0.300434.\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8071: Policy loss: -0.133290. Value loss: 0.082416. Entropy: 0.306118.\n",
      "Iteration 8072: Policy loss: -0.137920. Value loss: 0.029549. Entropy: 0.304546.\n",
      "Iteration 8073: Policy loss: -0.137712. Value loss: 0.021149. Entropy: 0.304957.\n",
      "episode: 3111   score: 355.0  epsilon: 1.0    steps: 848  evaluation reward: 417.55\n",
      "episode: 3112   score: 460.0  epsilon: 1.0    steps: 872  evaluation reward: 416.85\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8074: Policy loss: 0.289081. Value loss: 0.125993. Entropy: 0.296452.\n",
      "Iteration 8075: Policy loss: 0.273786. Value loss: 0.042083. Entropy: 0.293595.\n",
      "Iteration 8076: Policy loss: 0.276299. Value loss: 0.030393. Entropy: 0.294014.\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8077: Policy loss: -0.056811. Value loss: 0.079107. Entropy: 0.310442.\n",
      "Iteration 8078: Policy loss: -0.065991. Value loss: 0.034992. Entropy: 0.310595.\n",
      "Iteration 8079: Policy loss: -0.072515. Value loss: 0.026204. Entropy: 0.311348.\n",
      "episode: 3113   score: 650.0  epsilon: 1.0    steps: 40  evaluation reward: 419.6\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8080: Policy loss: 0.003666. Value loss: 0.082999. Entropy: 0.306889.\n",
      "Iteration 8081: Policy loss: 0.000303. Value loss: 0.041940. Entropy: 0.306015.\n",
      "Iteration 8082: Policy loss: 0.002873. Value loss: 0.032067. Entropy: 0.306546.\n",
      "episode: 3114   score: 485.0  epsilon: 1.0    steps: 936  evaluation reward: 421.35\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8083: Policy loss: 0.044881. Value loss: 0.092558. Entropy: 0.302096.\n",
      "Iteration 8084: Policy loss: 0.043304. Value loss: 0.036466. Entropy: 0.301618.\n",
      "Iteration 8085: Policy loss: 0.035058. Value loss: 0.028448. Entropy: 0.302040.\n",
      "episode: 3115   score: 395.0  epsilon: 1.0    steps: 240  evaluation reward: 421.6\n",
      "episode: 3116   score: 440.0  epsilon: 1.0    steps: 360  evaluation reward: 423.9\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8086: Policy loss: 0.100583. Value loss: 0.064546. Entropy: 0.298117.\n",
      "Iteration 8087: Policy loss: 0.097529. Value loss: 0.027073. Entropy: 0.296724.\n",
      "Iteration 8088: Policy loss: 0.095016. Value loss: 0.019328. Entropy: 0.298175.\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8089: Policy loss: 0.025135. Value loss: 0.060048. Entropy: 0.306545.\n",
      "Iteration 8090: Policy loss: 0.015378. Value loss: 0.029908. Entropy: 0.307451.\n",
      "Iteration 8091: Policy loss: 0.019580. Value loss: 0.020723. Entropy: 0.306461.\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8092: Policy loss: 0.081045. Value loss: 0.074088. Entropy: 0.309498.\n",
      "Iteration 8093: Policy loss: 0.071310. Value loss: 0.036505. Entropy: 0.309141.\n",
      "Iteration 8094: Policy loss: 0.074700. Value loss: 0.027211. Entropy: 0.309348.\n",
      "episode: 3117   score: 390.0  epsilon: 1.0    steps: 200  evaluation reward: 424.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8095: Policy loss: 0.074608. Value loss: 0.057357. Entropy: 0.298878.\n",
      "Iteration 8096: Policy loss: 0.068759. Value loss: 0.027321. Entropy: 0.297200.\n",
      "Iteration 8097: Policy loss: 0.069071. Value loss: 0.020226. Entropy: 0.297020.\n",
      "Training network. lr: 0.000188. clip: 0.075273\n",
      "Iteration 8098: Policy loss: 0.084022. Value loss: 0.063624. Entropy: 0.303656.\n",
      "Iteration 8099: Policy loss: 0.076726. Value loss: 0.029633. Entropy: 0.304064.\n",
      "Iteration 8100: Policy loss: 0.074319. Value loss: 0.022220. Entropy: 0.304478.\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8101: Policy loss: -0.011917. Value loss: 0.068604. Entropy: 0.307349.\n",
      "Iteration 8102: Policy loss: -0.014368. Value loss: 0.031730. Entropy: 0.305449.\n",
      "Iteration 8103: Policy loss: -0.020373. Value loss: 0.021859. Entropy: 0.306005.\n",
      "episode: 3118   score: 420.0  epsilon: 1.0    steps: 464  evaluation reward: 424.2\n",
      "episode: 3119   score: 345.0  epsilon: 1.0    steps: 608  evaluation reward: 424.8\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8104: Policy loss: -0.177381. Value loss: 0.125477. Entropy: 0.303973.\n",
      "Iteration 8105: Policy loss: -0.204831. Value loss: 0.092218. Entropy: 0.302774.\n",
      "Iteration 8106: Policy loss: -0.211075. Value loss: 0.082356. Entropy: 0.301276.\n",
      "episode: 3120   score: 270.0  epsilon: 1.0    steps: 576  evaluation reward: 423.4\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8107: Policy loss: -0.132315. Value loss: 0.105116. Entropy: 0.306989.\n",
      "Iteration 8108: Policy loss: -0.128229. Value loss: 0.035647. Entropy: 0.304221.\n",
      "Iteration 8109: Policy loss: -0.130276. Value loss: 0.030220. Entropy: 0.303972.\n",
      "episode: 3121   score: 470.0  epsilon: 1.0    steps: 664  evaluation reward: 426.45\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8110: Policy loss: 0.301592. Value loss: 0.111900. Entropy: 0.298397.\n",
      "Iteration 8111: Policy loss: 0.291798. Value loss: 0.047293. Entropy: 0.297035.\n",
      "Iteration 8112: Policy loss: 0.290877. Value loss: 0.034398. Entropy: 0.298132.\n",
      "episode: 3122   score: 510.0  epsilon: 1.0    steps: 232  evaluation reward: 423.55\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8113: Policy loss: 0.175238. Value loss: 0.073901. Entropy: 0.307037.\n",
      "Iteration 8114: Policy loss: 0.170067. Value loss: 0.026909. Entropy: 0.305911.\n",
      "Iteration 8115: Policy loss: 0.165784. Value loss: 0.017738. Entropy: 0.305686.\n",
      "episode: 3123   score: 80.0  epsilon: 1.0    steps: 432  evaluation reward: 418.45\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8116: Policy loss: -0.071520. Value loss: 0.084978. Entropy: 0.300457.\n",
      "Iteration 8117: Policy loss: -0.074148. Value loss: 0.040547. Entropy: 0.298921.\n",
      "Iteration 8118: Policy loss: -0.075439. Value loss: 0.031704. Entropy: 0.299207.\n",
      "episode: 3124   score: 335.0  epsilon: 1.0    steps: 544  evaluation reward: 416.55\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8119: Policy loss: 0.080478. Value loss: 0.092921. Entropy: 0.299344.\n",
      "Iteration 8120: Policy loss: 0.076706. Value loss: 0.033521. Entropy: 0.298797.\n",
      "Iteration 8121: Policy loss: 0.063457. Value loss: 0.023191. Entropy: 0.299527.\n",
      "episode: 3125   score: 395.0  epsilon: 1.0    steps: 320  evaluation reward: 418.1\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8122: Policy loss: 0.041545. Value loss: 0.084678. Entropy: 0.295441.\n",
      "Iteration 8123: Policy loss: 0.026773. Value loss: 0.040537. Entropy: 0.294434.\n",
      "Iteration 8124: Policy loss: 0.025768. Value loss: 0.028689. Entropy: 0.294133.\n",
      "episode: 3126   score: 260.0  epsilon: 1.0    steps: 184  evaluation reward: 417.05\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8125: Policy loss: 0.056335. Value loss: 0.053534. Entropy: 0.296056.\n",
      "Iteration 8126: Policy loss: 0.054401. Value loss: 0.033582. Entropy: 0.296061.\n",
      "Iteration 8127: Policy loss: 0.045417. Value loss: 0.026652. Entropy: 0.296399.\n",
      "episode: 3127   score: 345.0  epsilon: 1.0    steps: 696  evaluation reward: 417.75\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8128: Policy loss: -0.018686. Value loss: 0.091037. Entropy: 0.299484.\n",
      "Iteration 8129: Policy loss: -0.016417. Value loss: 0.037148. Entropy: 0.298557.\n",
      "Iteration 8130: Policy loss: -0.028641. Value loss: 0.027965. Entropy: 0.299195.\n",
      "episode: 3128   score: 695.0  epsilon: 1.0    steps: 536  evaluation reward: 421.35\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8131: Policy loss: 0.108533. Value loss: 0.079868. Entropy: 0.301399.\n",
      "Iteration 8132: Policy loss: 0.100353. Value loss: 0.044649. Entropy: 0.301383.\n",
      "Iteration 8133: Policy loss: 0.097639. Value loss: 0.034533. Entropy: 0.301862.\n",
      "episode: 3129   score: 240.0  epsilon: 1.0    steps: 512  evaluation reward: 417.85\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8134: Policy loss: 0.104936. Value loss: 0.094694. Entropy: 0.297446.\n",
      "Iteration 8135: Policy loss: 0.094332. Value loss: 0.031943. Entropy: 0.298791.\n",
      "Iteration 8136: Policy loss: 0.093263. Value loss: 0.015708. Entropy: 0.297967.\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8137: Policy loss: 0.081329. Value loss: 0.057696. Entropy: 0.311353.\n",
      "Iteration 8138: Policy loss: 0.073205. Value loss: 0.025525. Entropy: 0.310730.\n",
      "Iteration 8139: Policy loss: 0.073163. Value loss: 0.020160. Entropy: 0.310547.\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8140: Policy loss: 0.052146. Value loss: 0.069331. Entropy: 0.311766.\n",
      "Iteration 8141: Policy loss: 0.041640. Value loss: 0.033255. Entropy: 0.311406.\n",
      "Iteration 8142: Policy loss: 0.039374. Value loss: 0.019832. Entropy: 0.310991.\n",
      "episode: 3130   score: 315.0  epsilon: 1.0    steps: 112  evaluation reward: 419.75\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8143: Policy loss: 0.040858. Value loss: 0.050224. Entropy: 0.302415.\n",
      "Iteration 8144: Policy loss: 0.034091. Value loss: 0.019433. Entropy: 0.300395.\n",
      "Iteration 8145: Policy loss: 0.035028. Value loss: 0.014672. Entropy: 0.300766.\n",
      "episode: 3131   score: 285.0  epsilon: 1.0    steps: 248  evaluation reward: 418.95\n",
      "episode: 3132   score: 420.0  epsilon: 1.0    steps: 768  evaluation reward: 422.35\n",
      "episode: 3133   score: 265.0  epsilon: 1.0    steps: 896  evaluation reward: 417.4\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8146: Policy loss: -0.063399. Value loss: 0.057516. Entropy: 0.291260.\n",
      "Iteration 8147: Policy loss: -0.072397. Value loss: 0.029385. Entropy: 0.290067.\n",
      "Iteration 8148: Policy loss: -0.070091. Value loss: 0.021378. Entropy: 0.289328.\n",
      "Training network. lr: 0.000188. clip: 0.075126\n",
      "Iteration 8149: Policy loss: -0.182623. Value loss: 0.102530. Entropy: 0.303530.\n",
      "Iteration 8150: Policy loss: -0.188643. Value loss: 0.060338. Entropy: 0.303657.\n",
      "Iteration 8151: Policy loss: -0.191520. Value loss: 0.041008. Entropy: 0.303689.\n",
      "episode: 3134   score: 395.0  epsilon: 1.0    steps: 712  evaluation reward: 417.45\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8152: Policy loss: 0.149501. Value loss: 0.058478. Entropy: 0.298303.\n",
      "Iteration 8153: Policy loss: 0.150398. Value loss: 0.025877. Entropy: 0.297325.\n",
      "Iteration 8154: Policy loss: 0.149126. Value loss: 0.020627. Entropy: 0.298177.\n",
      "episode: 3135   score: 300.0  epsilon: 1.0    steps: 48  evaluation reward: 416.05\n",
      "episode: 3136   score: 330.0  epsilon: 1.0    steps: 192  evaluation reward: 415.7\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8155: Policy loss: -0.158587. Value loss: 0.269712. Entropy: 0.298011.\n",
      "Iteration 8156: Policy loss: -0.165917. Value loss: 0.229719. Entropy: 0.299793.\n",
      "Iteration 8157: Policy loss: -0.200448. Value loss: 0.244321. Entropy: 0.298283.\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8158: Policy loss: -0.051723. Value loss: 0.055797. Entropy: 0.305142.\n",
      "Iteration 8159: Policy loss: -0.059714. Value loss: 0.032075. Entropy: 0.304911.\n",
      "Iteration 8160: Policy loss: -0.065969. Value loss: 0.021877. Entropy: 0.305097.\n",
      "episode: 3137   score: 300.0  epsilon: 1.0    steps: 408  evaluation reward: 412.8\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8161: Policy loss: 0.091511. Value loss: 0.060537. Entropy: 0.302376.\n",
      "Iteration 8162: Policy loss: 0.092135. Value loss: 0.031612. Entropy: 0.302132.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8163: Policy loss: 0.088906. Value loss: 0.022173. Entropy: 0.303879.\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8164: Policy loss: -0.139226. Value loss: 0.138944. Entropy: 0.304960.\n",
      "Iteration 8165: Policy loss: -0.143792. Value loss: 0.052421. Entropy: 0.305028.\n",
      "Iteration 8166: Policy loss: -0.159089. Value loss: 0.035698. Entropy: 0.304667.\n",
      "episode: 3138   score: 335.0  epsilon: 1.0    steps: 168  evaluation reward: 407.1\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8167: Policy loss: -0.037112. Value loss: 0.096240. Entropy: 0.300381.\n",
      "Iteration 8168: Policy loss: -0.049204. Value loss: 0.037342. Entropy: 0.301485.\n",
      "Iteration 8169: Policy loss: -0.053664. Value loss: 0.027718. Entropy: 0.299977.\n",
      "episode: 3139   score: 515.0  epsilon: 1.0    steps: 144  evaluation reward: 405.55\n",
      "episode: 3140   score: 325.0  epsilon: 1.0    steps: 680  evaluation reward: 404.6\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8170: Policy loss: -0.301175. Value loss: 0.367436. Entropy: 0.298416.\n",
      "Iteration 8171: Policy loss: -0.272729. Value loss: 0.217740. Entropy: 0.293895.\n",
      "Iteration 8172: Policy loss: -0.319599. Value loss: 0.134372. Entropy: 0.298263.\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8173: Policy loss: -0.182373. Value loss: 0.303996. Entropy: 0.310585.\n",
      "Iteration 8174: Policy loss: -0.188449. Value loss: 0.072861. Entropy: 0.309000.\n",
      "Iteration 8175: Policy loss: -0.209316. Value loss: 0.036181. Entropy: 0.309985.\n",
      "episode: 3141   score: 485.0  epsilon: 1.0    steps: 216  evaluation reward: 402.0\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8176: Policy loss: -0.041498. Value loss: 0.061203. Entropy: 0.302162.\n",
      "Iteration 8177: Policy loss: -0.046384. Value loss: 0.029634. Entropy: 0.301468.\n",
      "Iteration 8178: Policy loss: -0.048748. Value loss: 0.021813. Entropy: 0.300765.\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8179: Policy loss: 0.176755. Value loss: 0.093730. Entropy: 0.308133.\n",
      "Iteration 8180: Policy loss: 0.171887. Value loss: 0.037994. Entropy: 0.307574.\n",
      "Iteration 8181: Policy loss: 0.165250. Value loss: 0.030216. Entropy: 0.307508.\n",
      "episode: 3142   score: 665.0  epsilon: 1.0    steps: 336  evaluation reward: 406.25\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8182: Policy loss: -0.236460. Value loss: 0.287741. Entropy: 0.302945.\n",
      "Iteration 8183: Policy loss: -0.245461. Value loss: 0.086951. Entropy: 0.304582.\n",
      "Iteration 8184: Policy loss: -0.272168. Value loss: 0.041149. Entropy: 0.302925.\n",
      "episode: 3143   score: 125.0  epsilon: 1.0    steps: 224  evaluation reward: 405.4\n",
      "episode: 3144   score: 620.0  epsilon: 1.0    steps: 552  evaluation reward: 406.85\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8185: Policy loss: -0.151577. Value loss: 0.345546. Entropy: 0.293109.\n",
      "Iteration 8186: Policy loss: -0.197870. Value loss: 0.118371. Entropy: 0.292966.\n",
      "Iteration 8187: Policy loss: -0.225763. Value loss: 0.055615. Entropy: 0.291155.\n",
      "episode: 3145   score: 495.0  epsilon: 1.0    steps: 808  evaluation reward: 405.45\n",
      "episode: 3146   score: 425.0  epsilon: 1.0    steps: 912  evaluation reward: 407.1\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8188: Policy loss: 0.026150. Value loss: 0.168696. Entropy: 0.306120.\n",
      "Iteration 8189: Policy loss: 0.007477. Value loss: 0.058295. Entropy: 0.305091.\n",
      "Iteration 8190: Policy loss: 0.008330. Value loss: 0.041904. Entropy: 0.305477.\n",
      "episode: 3147   score: 330.0  epsilon: 1.0    steps: 688  evaluation reward: 407.25\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8191: Policy loss: 0.292535. Value loss: 0.097970. Entropy: 0.306340.\n",
      "Iteration 8192: Policy loss: 0.289599. Value loss: 0.043225. Entropy: 0.308188.\n",
      "Iteration 8193: Policy loss: 0.282791. Value loss: 0.033852. Entropy: 0.306967.\n",
      "episode: 3148   score: 345.0  epsilon: 1.0    steps: 24  evaluation reward: 406.2\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8194: Policy loss: -0.087197. Value loss: 0.064606. Entropy: 0.303649.\n",
      "Iteration 8195: Policy loss: -0.089867. Value loss: 0.039647. Entropy: 0.304511.\n",
      "Iteration 8196: Policy loss: -0.086871. Value loss: 0.027653. Entropy: 0.305261.\n",
      "episode: 3149   score: 705.0  epsilon: 1.0    steps: 424  evaluation reward: 410.65\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8197: Policy loss: -0.214315. Value loss: 0.240702. Entropy: 0.301449.\n",
      "Iteration 8198: Policy loss: -0.224635. Value loss: 0.100116. Entropy: 0.301956.\n",
      "Iteration 8199: Policy loss: -0.228821. Value loss: 0.068281. Entropy: 0.300452.\n",
      "Training network. lr: 0.000187. clip: 0.074969\n",
      "Iteration 8200: Policy loss: -0.355346. Value loss: 0.183480. Entropy: 0.304251.\n",
      "Iteration 8201: Policy loss: -0.354956. Value loss: 0.085275. Entropy: 0.305420.\n",
      "Iteration 8202: Policy loss: -0.373051. Value loss: 0.066282. Entropy: 0.305022.\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8203: Policy loss: 0.004641. Value loss: 0.564968. Entropy: 0.303598.\n",
      "Iteration 8204: Policy loss: 0.000421. Value loss: 0.185309. Entropy: 0.303047.\n",
      "Iteration 8205: Policy loss: -0.030284. Value loss: 0.093135. Entropy: 0.302876.\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8206: Policy loss: 0.277334. Value loss: 0.181588. Entropy: 0.304018.\n",
      "Iteration 8207: Policy loss: 0.277575. Value loss: 0.077088. Entropy: 0.304821.\n",
      "Iteration 8208: Policy loss: 0.263768. Value loss: 0.049530. Entropy: 0.302742.\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8209: Policy loss: 0.320366. Value loss: 0.155422. Entropy: 0.306262.\n",
      "Iteration 8210: Policy loss: 0.307629. Value loss: 0.050317. Entropy: 0.304405.\n",
      "Iteration 8211: Policy loss: 0.297779. Value loss: 0.034208. Entropy: 0.304312.\n",
      "episode: 3150   score: 565.0  epsilon: 1.0    steps: 40  evaluation reward: 413.65\n",
      "now time :  2019-09-05 22:44:29.025052\n",
      "episode: 3151   score: 240.0  epsilon: 1.0    steps: 296  evaluation reward: 411.1\n",
      "episode: 3152   score: 365.0  epsilon: 1.0    steps: 728  evaluation reward: 409.95\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8212: Policy loss: 0.488209. Value loss: 0.137049. Entropy: 0.281890.\n",
      "Iteration 8213: Policy loss: 0.481984. Value loss: 0.061399. Entropy: 0.281228.\n",
      "Iteration 8214: Policy loss: 0.476766. Value loss: 0.050890. Entropy: 0.281577.\n",
      "episode: 3153   score: 405.0  epsilon: 1.0    steps: 648  evaluation reward: 411.6\n",
      "episode: 3154   score: 460.0  epsilon: 1.0    steps: 688  evaluation reward: 411.6\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8215: Policy loss: 0.098066. Value loss: 0.110797. Entropy: 0.289105.\n",
      "Iteration 8216: Policy loss: 0.091458. Value loss: 0.054307. Entropy: 0.289759.\n",
      "Iteration 8217: Policy loss: 0.088084. Value loss: 0.039959. Entropy: 0.290304.\n",
      "episode: 3155   score: 315.0  epsilon: 1.0    steps: 552  evaluation reward: 411.35\n",
      "episode: 3156   score: 630.0  epsilon: 1.0    steps: 696  evaluation reward: 415.55\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8218: Policy loss: -0.083133. Value loss: 0.086995. Entropy: 0.290796.\n",
      "Iteration 8219: Policy loss: -0.091871. Value loss: 0.050100. Entropy: 0.289124.\n",
      "Iteration 8220: Policy loss: -0.093179. Value loss: 0.039958. Entropy: 0.289719.\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8221: Policy loss: 0.029350. Value loss: 0.095512. Entropy: 0.309251.\n",
      "Iteration 8222: Policy loss: 0.020840. Value loss: 0.029011. Entropy: 0.309022.\n",
      "Iteration 8223: Policy loss: 0.011877. Value loss: 0.022804. Entropy: 0.308693.\n",
      "episode: 3157   score: 650.0  epsilon: 1.0    steps: 400  evaluation reward: 417.7\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8224: Policy loss: -0.194152. Value loss: 0.315901. Entropy: 0.297736.\n",
      "Iteration 8225: Policy loss: -0.213616. Value loss: 0.185456. Entropy: 0.297543.\n",
      "Iteration 8226: Policy loss: -0.208383. Value loss: 0.129704. Entropy: 0.297170.\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8227: Policy loss: 0.032603. Value loss: 0.385802. Entropy: 0.308998.\n",
      "Iteration 8228: Policy loss: 0.026814. Value loss: 0.172653. Entropy: 0.307687.\n",
      "Iteration 8229: Policy loss: 0.014494. Value loss: 0.077752. Entropy: 0.309467.\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8230: Policy loss: 0.132499. Value loss: 0.171272. Entropy: 0.306172.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8231: Policy loss: 0.110439. Value loss: 0.065386. Entropy: 0.305387.\n",
      "Iteration 8232: Policy loss: 0.118291. Value loss: 0.045232. Entropy: 0.306815.\n",
      "episode: 3158   score: 330.0  epsilon: 1.0    steps: 952  evaluation reward: 418.4\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8233: Policy loss: 0.092191. Value loss: 0.137542. Entropy: 0.305606.\n",
      "Iteration 8234: Policy loss: 0.095418. Value loss: 0.058891. Entropy: 0.304420.\n",
      "Iteration 8235: Policy loss: 0.076364. Value loss: 0.040224. Entropy: 0.304532.\n",
      "episode: 3159   score: 300.0  epsilon: 1.0    steps: 152  evaluation reward: 418.3\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8236: Policy loss: 0.170337. Value loss: 0.132299. Entropy: 0.292757.\n",
      "Iteration 8237: Policy loss: 0.159038. Value loss: 0.051056. Entropy: 0.289232.\n",
      "Iteration 8238: Policy loss: 0.163808. Value loss: 0.038552. Entropy: 0.291771.\n",
      "episode: 3160   score: 360.0  epsilon: 1.0    steps: 896  evaluation reward: 417.2\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8239: Policy loss: 0.081105. Value loss: 0.064510. Entropy: 0.308569.\n",
      "Iteration 8240: Policy loss: 0.078727. Value loss: 0.033117. Entropy: 0.308050.\n",
      "Iteration 8241: Policy loss: 0.070876. Value loss: 0.025650. Entropy: 0.308450.\n",
      "episode: 3161   score: 260.0  epsilon: 1.0    steps: 216  evaluation reward: 411.6\n",
      "episode: 3162   score: 225.0  epsilon: 1.0    steps: 576  evaluation reward: 409.35\n",
      "episode: 3163   score: 495.0  epsilon: 1.0    steps: 624  evaluation reward: 412.2\n",
      "episode: 3164   score: 530.0  epsilon: 1.0    steps: 672  evaluation reward: 414.6\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8242: Policy loss: 0.135987. Value loss: 0.073228. Entropy: 0.248685.\n",
      "Iteration 8243: Policy loss: 0.126636. Value loss: 0.035184. Entropy: 0.248033.\n",
      "Iteration 8244: Policy loss: 0.119707. Value loss: 0.030015. Entropy: 0.245655.\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8245: Policy loss: 0.217195. Value loss: 0.077817. Entropy: 0.311328.\n",
      "Iteration 8246: Policy loss: 0.218288. Value loss: 0.041407. Entropy: 0.309922.\n",
      "Iteration 8247: Policy loss: 0.214362. Value loss: 0.029346. Entropy: 0.309721.\n",
      "episode: 3165   score: 595.0  epsilon: 1.0    steps: 840  evaluation reward: 417.7\n",
      "episode: 3166   score: 150.0  epsilon: 1.0    steps: 856  evaluation reward: 416.8\n",
      "Training network. lr: 0.000187. clip: 0.074813\n",
      "Iteration 8248: Policy loss: 0.096463. Value loss: 0.144111. Entropy: 0.295096.\n",
      "Iteration 8249: Policy loss: 0.092535. Value loss: 0.064947. Entropy: 0.295106.\n",
      "Iteration 8250: Policy loss: 0.086778. Value loss: 0.051267. Entropy: 0.295434.\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8251: Policy loss: 0.231858. Value loss: 0.092652. Entropy: 0.300673.\n",
      "Iteration 8252: Policy loss: 0.228531. Value loss: 0.031075. Entropy: 0.303089.\n",
      "Iteration 8253: Policy loss: 0.229442. Value loss: 0.024844. Entropy: 0.301574.\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8254: Policy loss: -0.016954. Value loss: 0.071800. Entropy: 0.313646.\n",
      "Iteration 8255: Policy loss: -0.021003. Value loss: 0.028282. Entropy: 0.313532.\n",
      "Iteration 8256: Policy loss: -0.022925. Value loss: 0.020666. Entropy: 0.313601.\n",
      "episode: 3167   score: 255.0  epsilon: 1.0    steps: 368  evaluation reward: 416.5\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8257: Policy loss: 0.117187. Value loss: 0.075683. Entropy: 0.295646.\n",
      "Iteration 8258: Policy loss: 0.112082. Value loss: 0.041794. Entropy: 0.297967.\n",
      "Iteration 8259: Policy loss: 0.113473. Value loss: 0.025738. Entropy: 0.295894.\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8260: Policy loss: -0.048491. Value loss: 0.078477. Entropy: 0.309608.\n",
      "Iteration 8261: Policy loss: -0.044826. Value loss: 0.035352. Entropy: 0.309347.\n",
      "Iteration 8262: Policy loss: -0.051835. Value loss: 0.025068. Entropy: 0.310484.\n",
      "episode: 3168   score: 285.0  epsilon: 1.0    steps: 560  evaluation reward: 417.25\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8263: Policy loss: -0.124381. Value loss: 0.118455. Entropy: 0.299150.\n",
      "Iteration 8264: Policy loss: -0.137237. Value loss: 0.046988. Entropy: 0.299158.\n",
      "Iteration 8265: Policy loss: -0.142629. Value loss: 0.031562. Entropy: 0.299127.\n",
      "episode: 3169   score: 345.0  epsilon: 1.0    steps: 120  evaluation reward: 415.9\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8266: Policy loss: 0.053243. Value loss: 0.066854. Entropy: 0.304971.\n",
      "Iteration 8267: Policy loss: 0.045918. Value loss: 0.023193. Entropy: 0.303777.\n",
      "Iteration 8268: Policy loss: 0.051629. Value loss: 0.016535. Entropy: 0.304857.\n",
      "episode: 3170   score: 300.0  epsilon: 1.0    steps: 224  evaluation reward: 416.05\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8269: Policy loss: 0.268251. Value loss: 0.081280. Entropy: 0.302284.\n",
      "Iteration 8270: Policy loss: 0.259677. Value loss: 0.037022. Entropy: 0.302997.\n",
      "Iteration 8271: Policy loss: 0.256474. Value loss: 0.028599. Entropy: 0.302044.\n",
      "episode: 3171   score: 330.0  epsilon: 1.0    steps: 280  evaluation reward: 416.7\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8272: Policy loss: -0.025721. Value loss: 0.079909. Entropy: 0.303768.\n",
      "Iteration 8273: Policy loss: -0.038230. Value loss: 0.033680. Entropy: 0.303371.\n",
      "Iteration 8274: Policy loss: -0.038454. Value loss: 0.024864. Entropy: 0.302778.\n",
      "episode: 3172   score: 325.0  epsilon: 1.0    steps: 336  evaluation reward: 414.5\n",
      "episode: 3173   score: 390.0  epsilon: 1.0    steps: 464  evaluation reward: 409.9\n",
      "episode: 3174   score: 420.0  epsilon: 1.0    steps: 648  evaluation reward: 411.1\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8275: Policy loss: 0.063225. Value loss: 0.058448. Entropy: 0.281605.\n",
      "Iteration 8276: Policy loss: 0.057492. Value loss: 0.034395. Entropy: 0.282352.\n",
      "Iteration 8277: Policy loss: 0.056027. Value loss: 0.030003. Entropy: 0.281493.\n",
      "episode: 3175   score: 285.0  epsilon: 1.0    steps: 592  evaluation reward: 412.45\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8278: Policy loss: -0.066377. Value loss: 0.069776. Entropy: 0.299915.\n",
      "Iteration 8279: Policy loss: -0.072659. Value loss: 0.029718. Entropy: 0.300052.\n",
      "Iteration 8280: Policy loss: -0.075493. Value loss: 0.022372. Entropy: 0.299057.\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8281: Policy loss: -0.123268. Value loss: 0.104048. Entropy: 0.310089.\n",
      "Iteration 8282: Policy loss: -0.140381. Value loss: 0.042165. Entropy: 0.309325.\n",
      "Iteration 8283: Policy loss: -0.138033. Value loss: 0.034254. Entropy: 0.309585.\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8284: Policy loss: -0.375842. Value loss: 0.554295. Entropy: 0.306939.\n",
      "Iteration 8285: Policy loss: -0.407597. Value loss: 0.366969. Entropy: 0.306825.\n",
      "Iteration 8286: Policy loss: -0.368249. Value loss: 0.145068. Entropy: 0.304708.\n",
      "episode: 3176   score: 345.0  epsilon: 1.0    steps: 128  evaluation reward: 411.3\n",
      "episode: 3177   score: 265.0  epsilon: 1.0    steps: 312  evaluation reward: 410.05\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8287: Policy loss: -0.072469. Value loss: 0.069293. Entropy: 0.282146.\n",
      "Iteration 8288: Policy loss: -0.073063. Value loss: 0.032962. Entropy: 0.280201.\n",
      "Iteration 8289: Policy loss: -0.076513. Value loss: 0.023522. Entropy: 0.280336.\n",
      "episode: 3178   score: 600.0  epsilon: 1.0    steps: 816  evaluation reward: 412.25\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8290: Policy loss: 0.134804. Value loss: 0.232776. Entropy: 0.302606.\n",
      "Iteration 8291: Policy loss: 0.102489. Value loss: 0.060805. Entropy: 0.300868.\n",
      "Iteration 8292: Policy loss: 0.113311. Value loss: 0.036529. Entropy: 0.301183.\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8293: Policy loss: -0.077137. Value loss: 0.073424. Entropy: 0.305345.\n",
      "Iteration 8294: Policy loss: -0.082481. Value loss: 0.041262. Entropy: 0.305825.\n",
      "Iteration 8295: Policy loss: -0.085998. Value loss: 0.030422. Entropy: 0.306002.\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8296: Policy loss: -0.114376. Value loss: 0.093760. Entropy: 0.303531.\n",
      "Iteration 8297: Policy loss: -0.129202. Value loss: 0.044643. Entropy: 0.303085.\n",
      "Iteration 8298: Policy loss: -0.130612. Value loss: 0.035123. Entropy: 0.302729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3179   score: 345.0  epsilon: 1.0    steps: 136  evaluation reward: 411.3\n",
      "episode: 3180   score: 590.0  epsilon: 1.0    steps: 256  evaluation reward: 411.55\n",
      "Training network. lr: 0.000187. clip: 0.074665\n",
      "Iteration 8299: Policy loss: -0.081377. Value loss: 0.118748. Entropy: 0.279404.\n",
      "Iteration 8300: Policy loss: -0.092090. Value loss: 0.045486. Entropy: 0.277698.\n",
      "Iteration 8301: Policy loss: -0.097316. Value loss: 0.031992. Entropy: 0.278611.\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8302: Policy loss: -0.210529. Value loss: 0.292269. Entropy: 0.307892.\n",
      "Iteration 8303: Policy loss: -0.217324. Value loss: 0.095973. Entropy: 0.306022.\n",
      "Iteration 8304: Policy loss: -0.229815. Value loss: 0.055110. Entropy: 0.307936.\n",
      "episode: 3181   score: 420.0  epsilon: 1.0    steps: 112  evaluation reward: 410.3\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8305: Policy loss: 0.174759. Value loss: 0.065657. Entropy: 0.293881.\n",
      "Iteration 8306: Policy loss: 0.168824. Value loss: 0.030247. Entropy: 0.293672.\n",
      "Iteration 8307: Policy loss: 0.171974. Value loss: 0.024222. Entropy: 0.292792.\n",
      "episode: 3182   score: 265.0  epsilon: 1.0    steps: 728  evaluation reward: 408.35\n",
      "episode: 3183   score: 420.0  epsilon: 1.0    steps: 968  evaluation reward: 411.3\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8308: Policy loss: 0.226317. Value loss: 0.115645. Entropy: 0.293204.\n",
      "Iteration 8309: Policy loss: 0.213371. Value loss: 0.042327. Entropy: 0.291914.\n",
      "Iteration 8310: Policy loss: 0.201499. Value loss: 0.028775. Entropy: 0.292337.\n",
      "episode: 3184   score: 395.0  epsilon: 1.0    steps: 152  evaluation reward: 408.85\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8311: Policy loss: -0.110332. Value loss: 0.092609. Entropy: 0.287512.\n",
      "Iteration 8312: Policy loss: -0.118149. Value loss: 0.044761. Entropy: 0.284561.\n",
      "Iteration 8313: Policy loss: -0.113317. Value loss: 0.032321. Entropy: 0.284563.\n",
      "episode: 3185   score: 545.0  epsilon: 1.0    steps: 608  evaluation reward: 410.85\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8314: Policy loss: 0.295086. Value loss: 0.162204. Entropy: 0.294699.\n",
      "Iteration 8315: Policy loss: 0.279234. Value loss: 0.041917. Entropy: 0.290548.\n",
      "Iteration 8316: Policy loss: 0.261826. Value loss: 0.027442. Entropy: 0.291621.\n",
      "episode: 3186   score: 545.0  epsilon: 1.0    steps: 56  evaluation reward: 410.1\n",
      "episode: 3187   score: 215.0  epsilon: 1.0    steps: 816  evaluation reward: 410.15\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8317: Policy loss: 0.038543. Value loss: 0.350704. Entropy: 0.290142.\n",
      "Iteration 8318: Policy loss: 0.021852. Value loss: 0.176288. Entropy: 0.288662.\n",
      "Iteration 8319: Policy loss: 0.007445. Value loss: 0.091908. Entropy: 0.289834.\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8320: Policy loss: -0.096599. Value loss: 0.082113. Entropy: 0.311068.\n",
      "Iteration 8321: Policy loss: -0.104000. Value loss: 0.038753. Entropy: 0.312617.\n",
      "Iteration 8322: Policy loss: -0.104978. Value loss: 0.028973. Entropy: 0.311837.\n",
      "episode: 3188   score: 345.0  epsilon: 1.0    steps: 72  evaluation reward: 410.15\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8323: Policy loss: 0.148832. Value loss: 0.141054. Entropy: 0.301103.\n",
      "Iteration 8324: Policy loss: 0.131489. Value loss: 0.046070. Entropy: 0.300313.\n",
      "Iteration 8325: Policy loss: 0.132818. Value loss: 0.027967. Entropy: 0.300792.\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8326: Policy loss: 0.390757. Value loss: 0.127729. Entropy: 0.303624.\n",
      "Iteration 8327: Policy loss: 0.377343. Value loss: 0.036919. Entropy: 0.303952.\n",
      "Iteration 8328: Policy loss: 0.365435. Value loss: 0.027471. Entropy: 0.304401.\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8329: Policy loss: -0.174226. Value loss: 0.206505. Entropy: 0.305902.\n",
      "Iteration 8330: Policy loss: -0.156872. Value loss: 0.072818. Entropy: 0.305933.\n",
      "Iteration 8331: Policy loss: -0.158263. Value loss: 0.046545. Entropy: 0.306170.\n",
      "episode: 3189   score: 280.0  epsilon: 1.0    steps: 480  evaluation reward: 408.8\n",
      "episode: 3190   score: 285.0  epsilon: 1.0    steps: 784  evaluation reward: 407.05\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8332: Policy loss: 0.148224. Value loss: 0.073564. Entropy: 0.286125.\n",
      "Iteration 8333: Policy loss: 0.140553. Value loss: 0.040595. Entropy: 0.285939.\n",
      "Iteration 8334: Policy loss: 0.137998. Value loss: 0.032263. Entropy: 0.284766.\n",
      "episode: 3191   score: 365.0  epsilon: 1.0    steps: 992  evaluation reward: 406.05\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8335: Policy loss: 0.099919. Value loss: 0.083582. Entropy: 0.311019.\n",
      "Iteration 8336: Policy loss: 0.089483. Value loss: 0.034625. Entropy: 0.310808.\n",
      "Iteration 8337: Policy loss: 0.082073. Value loss: 0.027555. Entropy: 0.310481.\n",
      "episode: 3192   score: 270.0  epsilon: 1.0    steps: 16  evaluation reward: 404.8\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8338: Policy loss: -0.167501. Value loss: 0.273770. Entropy: 0.287426.\n",
      "Iteration 8339: Policy loss: -0.223820. Value loss: 0.141608. Entropy: 0.287657.\n",
      "Iteration 8340: Policy loss: -0.227022. Value loss: 0.072424. Entropy: 0.286481.\n",
      "episode: 3193   score: 365.0  epsilon: 1.0    steps: 816  evaluation reward: 406.05\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8341: Policy loss: 0.038222. Value loss: 0.075588. Entropy: 0.298498.\n",
      "Iteration 8342: Policy loss: 0.036354. Value loss: 0.034367. Entropy: 0.298794.\n",
      "Iteration 8343: Policy loss: 0.037373. Value loss: 0.026998. Entropy: 0.298798.\n",
      "episode: 3194   score: 820.0  epsilon: 1.0    steps: 512  evaluation reward: 409.35\n",
      "episode: 3195   score: 535.0  epsilon: 1.0    steps: 560  evaluation reward: 408.35\n",
      "episode: 3196   score: 260.0  epsilon: 1.0    steps: 736  evaluation reward: 407.95\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8344: Policy loss: 0.156418. Value loss: 0.063167. Entropy: 0.270287.\n",
      "Iteration 8345: Policy loss: 0.151563. Value loss: 0.025655. Entropy: 0.272511.\n",
      "Iteration 8346: Policy loss: 0.148287. Value loss: 0.021202. Entropy: 0.270304.\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8347: Policy loss: -0.069942. Value loss: 0.281799. Entropy: 0.305722.\n",
      "Iteration 8348: Policy loss: -0.079366. Value loss: 0.075882. Entropy: 0.304765.\n",
      "Iteration 8349: Policy loss: -0.096565. Value loss: 0.037893. Entropy: 0.304753.\n",
      "Training network. lr: 0.000186. clip: 0.074509\n",
      "Iteration 8350: Policy loss: -0.149620. Value loss: 0.228702. Entropy: 0.303801.\n",
      "Iteration 8351: Policy loss: -0.179647. Value loss: 0.111400. Entropy: 0.304550.\n",
      "Iteration 8352: Policy loss: -0.183731. Value loss: 0.085831. Entropy: 0.303699.\n",
      "episode: 3197   score: 410.0  epsilon: 1.0    steps: 264  evaluation reward: 407.55\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8353: Policy loss: -0.104748. Value loss: 0.094470. Entropy: 0.292516.\n",
      "Iteration 8354: Policy loss: -0.110150. Value loss: 0.039896. Entropy: 0.291966.\n",
      "Iteration 8355: Policy loss: -0.113107. Value loss: 0.025359. Entropy: 0.291902.\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8356: Policy loss: -0.227099. Value loss: 0.213593. Entropy: 0.305663.\n",
      "Iteration 8357: Policy loss: -0.242997. Value loss: 0.073274. Entropy: 0.306018.\n",
      "Iteration 8358: Policy loss: -0.241187. Value loss: 0.056947. Entropy: 0.306638.\n",
      "episode: 3198   score: 500.0  epsilon: 1.0    steps: 840  evaluation reward: 406.8\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8359: Policy loss: 0.084915. Value loss: 0.121230. Entropy: 0.308277.\n",
      "Iteration 8360: Policy loss: 0.082784. Value loss: 0.069455. Entropy: 0.307444.\n",
      "Iteration 8361: Policy loss: 0.075807. Value loss: 0.044258. Entropy: 0.306426.\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8362: Policy loss: -0.002612. Value loss: 0.244238. Entropy: 0.306860.\n",
      "Iteration 8363: Policy loss: -0.007428. Value loss: 0.083251. Entropy: 0.306083.\n",
      "Iteration 8364: Policy loss: -0.021280. Value loss: 0.057632. Entropy: 0.306587.\n",
      "episode: 3199   score: 465.0  epsilon: 1.0    steps: 296  evaluation reward: 408.85\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8365: Policy loss: -0.101570. Value loss: 0.189632. Entropy: 0.292962.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8366: Policy loss: -0.109836. Value loss: 0.064854. Entropy: 0.294619.\n",
      "Iteration 8367: Policy loss: -0.129317. Value loss: 0.041329. Entropy: 0.294784.\n",
      "episode: 3200   score: 695.0  epsilon: 1.0    steps: 816  evaluation reward: 410.45\n",
      "now time :  2019-09-05 22:54:10.166291\n",
      "episode: 3201   score: 420.0  epsilon: 1.0    steps: 928  evaluation reward: 410.75\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8368: Policy loss: 0.079482. Value loss: 0.222376. Entropy: 0.297406.\n",
      "Iteration 8369: Policy loss: 0.075450. Value loss: 0.085234. Entropy: 0.297638.\n",
      "Iteration 8370: Policy loss: 0.073203. Value loss: 0.060349. Entropy: 0.298001.\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8371: Policy loss: 0.104413. Value loss: 0.083514. Entropy: 0.300699.\n",
      "Iteration 8372: Policy loss: 0.095598. Value loss: 0.039334. Entropy: 0.300520.\n",
      "Iteration 8373: Policy loss: 0.093804. Value loss: 0.029117. Entropy: 0.301293.\n",
      "episode: 3202   score: 285.0  epsilon: 1.0    steps: 688  evaluation reward: 406.15\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8374: Policy loss: -0.043739. Value loss: 0.454208. Entropy: 0.299286.\n",
      "Iteration 8375: Policy loss: -0.037024. Value loss: 0.261718. Entropy: 0.299150.\n",
      "Iteration 8376: Policy loss: -0.068655. Value loss: 0.220840. Entropy: 0.300010.\n",
      "episode: 3203   score: 420.0  epsilon: 1.0    steps: 504  evaluation reward: 405.2\n",
      "episode: 3204   score: 620.0  epsilon: 1.0    steps: 584  evaluation reward: 404.75\n",
      "episode: 3205   score: 670.0  epsilon: 1.0    steps: 976  evaluation reward: 403.55\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8377: Policy loss: -0.098740. Value loss: 0.121982. Entropy: 0.282202.\n",
      "Iteration 8378: Policy loss: -0.104962. Value loss: 0.055328. Entropy: 0.284810.\n",
      "Iteration 8379: Policy loss: -0.107403. Value loss: 0.041711. Entropy: 0.283599.\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8380: Policy loss: 0.168961. Value loss: 0.140297. Entropy: 0.300709.\n",
      "Iteration 8381: Policy loss: 0.152409. Value loss: 0.049505. Entropy: 0.299836.\n",
      "Iteration 8382: Policy loss: 0.144928. Value loss: 0.035014. Entropy: 0.299451.\n",
      "episode: 3206   score: 215.0  epsilon: 1.0    steps: 864  evaluation reward: 400.65\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8383: Policy loss: 0.087214. Value loss: 0.079805. Entropy: 0.305920.\n",
      "Iteration 8384: Policy loss: 0.077799. Value loss: 0.037260. Entropy: 0.306793.\n",
      "Iteration 8385: Policy loss: 0.075122. Value loss: 0.027166. Entropy: 0.305602.\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8386: Policy loss: 0.374598. Value loss: 0.182039. Entropy: 0.305994.\n",
      "Iteration 8387: Policy loss: 0.357754. Value loss: 0.048942. Entropy: 0.306503.\n",
      "Iteration 8388: Policy loss: 0.349256. Value loss: 0.032904. Entropy: 0.304768.\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8389: Policy loss: 0.388639. Value loss: 0.165216. Entropy: 0.311905.\n",
      "Iteration 8390: Policy loss: 0.380024. Value loss: 0.063971. Entropy: 0.310751.\n",
      "Iteration 8391: Policy loss: 0.377876. Value loss: 0.043357. Entropy: 0.310709.\n",
      "episode: 3207   score: 260.0  epsilon: 1.0    steps: 864  evaluation reward: 400.15\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8392: Policy loss: -0.283573. Value loss: 0.331336. Entropy: 0.304465.\n",
      "Iteration 8393: Policy loss: -0.284106. Value loss: 0.121851. Entropy: 0.304525.\n",
      "Iteration 8394: Policy loss: -0.299838. Value loss: 0.078532. Entropy: 0.305479.\n",
      "episode: 3208   score: 790.0  epsilon: 1.0    steps: 536  evaluation reward: 403.85\n",
      "episode: 3209   score: 335.0  epsilon: 1.0    steps: 632  evaluation reward: 405.1\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8395: Policy loss: -0.044910. Value loss: 0.081942. Entropy: 0.286048.\n",
      "Iteration 8396: Policy loss: -0.048085. Value loss: 0.042960. Entropy: 0.285298.\n",
      "Iteration 8397: Policy loss: -0.050291. Value loss: 0.032847. Entropy: 0.286344.\n",
      "Training network. lr: 0.000186. clip: 0.074352\n",
      "Iteration 8398: Policy loss: 0.054622. Value loss: 0.109620. Entropy: 0.309867.\n",
      "Iteration 8399: Policy loss: 0.042289. Value loss: 0.046803. Entropy: 0.308391.\n",
      "Iteration 8400: Policy loss: 0.049344. Value loss: 0.032963. Entropy: 0.309085.\n",
      "episode: 3210   score: 265.0  epsilon: 1.0    steps: 208  evaluation reward: 401.55\n",
      "episode: 3211   score: 270.0  epsilon: 1.0    steps: 600  evaluation reward: 400.7\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8401: Policy loss: -0.022364. Value loss: 0.037273. Entropy: 0.288603.\n",
      "Iteration 8402: Policy loss: -0.022389. Value loss: 0.021405. Entropy: 0.288685.\n",
      "Iteration 8403: Policy loss: -0.024952. Value loss: 0.018303. Entropy: 0.287972.\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8404: Policy loss: -0.200975. Value loss: 0.133076. Entropy: 0.306675.\n",
      "Iteration 8405: Policy loss: -0.201426. Value loss: 0.043226. Entropy: 0.306171.\n",
      "Iteration 8406: Policy loss: -0.217007. Value loss: 0.022001. Entropy: 0.306656.\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8407: Policy loss: 0.177814. Value loss: 0.299216. Entropy: 0.311736.\n",
      "Iteration 8408: Policy loss: 0.123092. Value loss: 0.172293. Entropy: 0.310244.\n",
      "Iteration 8409: Policy loss: 0.116875. Value loss: 0.112800. Entropy: 0.310203.\n",
      "episode: 3212   score: 375.0  epsilon: 1.0    steps: 120  evaluation reward: 399.85\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8410: Policy loss: 0.021156. Value loss: 0.136164. Entropy: 0.294269.\n",
      "Iteration 8411: Policy loss: 0.018788. Value loss: 0.045563. Entropy: 0.293752.\n",
      "Iteration 8412: Policy loss: 0.018647. Value loss: 0.031198. Entropy: 0.294999.\n",
      "episode: 3213   score: 495.0  epsilon: 1.0    steps: 608  evaluation reward: 398.3\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8413: Policy loss: 0.178859. Value loss: 0.093693. Entropy: 0.303205.\n",
      "Iteration 8414: Policy loss: 0.168820. Value loss: 0.048248. Entropy: 0.303428.\n",
      "Iteration 8415: Policy loss: 0.169496. Value loss: 0.033704. Entropy: 0.302638.\n",
      "episode: 3214   score: 750.0  epsilon: 1.0    steps: 1008  evaluation reward: 400.95\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8416: Policy loss: -0.234089. Value loss: 0.371400. Entropy: 0.306858.\n",
      "Iteration 8417: Policy loss: -0.248228. Value loss: 0.093343. Entropy: 0.306638.\n",
      "Iteration 8418: Policy loss: -0.243452. Value loss: 0.061496. Entropy: 0.308152.\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8419: Policy loss: 0.349122. Value loss: 0.083556. Entropy: 0.306288.\n",
      "Iteration 8420: Policy loss: 0.334512. Value loss: 0.020353. Entropy: 0.304815.\n",
      "Iteration 8421: Policy loss: 0.333316. Value loss: 0.014786. Entropy: 0.304598.\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8422: Policy loss: 0.202512. Value loss: 0.176688. Entropy: 0.303249.\n",
      "Iteration 8423: Policy loss: 0.184785. Value loss: 0.060351. Entropy: 0.302509.\n",
      "Iteration 8424: Policy loss: 0.180976. Value loss: 0.042360. Entropy: 0.301934.\n",
      "episode: 3215   score: 340.0  epsilon: 1.0    steps: 920  evaluation reward: 400.4\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8425: Policy loss: 0.210818. Value loss: 0.138732. Entropy: 0.301784.\n",
      "Iteration 8426: Policy loss: 0.196100. Value loss: 0.054922. Entropy: 0.301657.\n",
      "Iteration 8427: Policy loss: 0.195008. Value loss: 0.038615. Entropy: 0.301158.\n",
      "episode: 3216   score: 370.0  epsilon: 1.0    steps: 128  evaluation reward: 399.7\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8428: Policy loss: -0.190774. Value loss: 0.323218. Entropy: 0.298401.\n",
      "Iteration 8429: Policy loss: -0.225308. Value loss: 0.198404. Entropy: 0.300210.\n",
      "Iteration 8430: Policy loss: -0.232862. Value loss: 0.117301. Entropy: 0.298866.\n",
      "episode: 3217   score: 365.0  epsilon: 1.0    steps: 16  evaluation reward: 399.45\n",
      "episode: 3218   score: 740.0  epsilon: 1.0    steps: 224  evaluation reward: 402.65\n",
      "episode: 3219   score: 730.0  epsilon: 1.0    steps: 432  evaluation reward: 406.5\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8431: Policy loss: 0.055321. Value loss: 0.082044. Entropy: 0.286660.\n",
      "Iteration 8432: Policy loss: 0.049517. Value loss: 0.049929. Entropy: 0.288213.\n",
      "Iteration 8433: Policy loss: 0.044940. Value loss: 0.037813. Entropy: 0.288071.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8434: Policy loss: -0.035808. Value loss: 0.089350. Entropy: 0.311253.\n",
      "Iteration 8435: Policy loss: -0.043248. Value loss: 0.041898. Entropy: 0.310090.\n",
      "Iteration 8436: Policy loss: -0.047401. Value loss: 0.032384. Entropy: 0.309621.\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8437: Policy loss: 0.114468. Value loss: 0.103378. Entropy: 0.308389.\n",
      "Iteration 8438: Policy loss: 0.101475. Value loss: 0.050508. Entropy: 0.306530.\n",
      "Iteration 8439: Policy loss: 0.097541. Value loss: 0.033853. Entropy: 0.306678.\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8440: Policy loss: 0.120889. Value loss: 0.318322. Entropy: 0.313149.\n",
      "Iteration 8441: Policy loss: 0.100445. Value loss: 0.171184. Entropy: 0.312218.\n",
      "Iteration 8442: Policy loss: 0.085868. Value loss: 0.121646. Entropy: 0.312472.\n",
      "episode: 3220   score: 615.0  epsilon: 1.0    steps: 328  evaluation reward: 409.95\n",
      "episode: 3221   score: 335.0  epsilon: 1.0    steps: 368  evaluation reward: 408.6\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8443: Policy loss: -0.189983. Value loss: 0.229587. Entropy: 0.284176.\n",
      "Iteration 8444: Policy loss: -0.213999. Value loss: 0.123590. Entropy: 0.284362.\n",
      "Iteration 8445: Policy loss: -0.224533. Value loss: 0.067925. Entropy: 0.284001.\n",
      "episode: 3222   score: 575.0  epsilon: 1.0    steps: 968  evaluation reward: 409.25\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8446: Policy loss: -0.337772. Value loss: 0.293811. Entropy: 0.310316.\n",
      "Iteration 8447: Policy loss: -0.334383. Value loss: 0.172118. Entropy: 0.311038.\n",
      "Iteration 8448: Policy loss: -0.337526. Value loss: 0.085277. Entropy: 0.310798.\n",
      "episode: 3223   score: 255.0  epsilon: 1.0    steps: 240  evaluation reward: 411.0\n",
      "Training network. lr: 0.000186. clip: 0.074204\n",
      "Iteration 8449: Policy loss: -0.166395. Value loss: 0.137743. Entropy: 0.285078.\n",
      "Iteration 8450: Policy loss: -0.166357. Value loss: 0.058843. Entropy: 0.283217.\n",
      "Iteration 8451: Policy loss: -0.175215. Value loss: 0.036553. Entropy: 0.283011.\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8452: Policy loss: 0.206143. Value loss: 0.112369. Entropy: 0.313419.\n",
      "Iteration 8453: Policy loss: 0.211091. Value loss: 0.032288. Entropy: 0.312785.\n",
      "Iteration 8454: Policy loss: 0.199312. Value loss: 0.025916. Entropy: 0.312005.\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8455: Policy loss: -0.370493. Value loss: 0.419112. Entropy: 0.305578.\n",
      "Iteration 8456: Policy loss: -0.387213. Value loss: 0.158060. Entropy: 0.305695.\n",
      "Iteration 8457: Policy loss: -0.396733. Value loss: 0.083503. Entropy: 0.305626.\n",
      "episode: 3224   score: 570.0  epsilon: 1.0    steps: 632  evaluation reward: 413.35\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8458: Policy loss: 0.372763. Value loss: 0.166537. Entropy: 0.293611.\n",
      "Iteration 8459: Policy loss: 0.352271. Value loss: 0.044480. Entropy: 0.293519.\n",
      "Iteration 8460: Policy loss: 0.349058. Value loss: 0.025882. Entropy: 0.293375.\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8461: Policy loss: 0.019161. Value loss: 0.164447. Entropy: 0.307180.\n",
      "Iteration 8462: Policy loss: 0.007395. Value loss: 0.050982. Entropy: 0.307232.\n",
      "Iteration 8463: Policy loss: 0.016130. Value loss: 0.034157. Entropy: 0.307421.\n",
      "episode: 3225   score: 735.0  epsilon: 1.0    steps: 616  evaluation reward: 416.75\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8464: Policy loss: 0.254181. Value loss: 0.132415. Entropy: 0.295855.\n",
      "Iteration 8465: Policy loss: 0.241902. Value loss: 0.057116. Entropy: 0.295398.\n",
      "Iteration 8466: Policy loss: 0.249308. Value loss: 0.044894. Entropy: 0.295491.\n",
      "episode: 3226   score: 495.0  epsilon: 1.0    steps: 296  evaluation reward: 419.1\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8467: Policy loss: 0.124951. Value loss: 0.110980. Entropy: 0.292967.\n",
      "Iteration 8468: Policy loss: 0.116514. Value loss: 0.043094. Entropy: 0.294622.\n",
      "Iteration 8469: Policy loss: 0.122186. Value loss: 0.032497. Entropy: 0.294264.\n",
      "episode: 3227   score: 470.0  epsilon: 1.0    steps: 656  evaluation reward: 420.35\n",
      "episode: 3228   score: 290.0  epsilon: 1.0    steps: 720  evaluation reward: 416.3\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8470: Policy loss: 0.121098. Value loss: 0.100798. Entropy: 0.284375.\n",
      "Iteration 8471: Policy loss: 0.108916. Value loss: 0.049799. Entropy: 0.285602.\n",
      "Iteration 8472: Policy loss: 0.104894. Value loss: 0.038550. Entropy: 0.284757.\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8473: Policy loss: -0.298605. Value loss: 0.201728. Entropy: 0.308888.\n",
      "Iteration 8474: Policy loss: -0.308101. Value loss: 0.134616. Entropy: 0.308695.\n",
      "Iteration 8475: Policy loss: -0.309984. Value loss: 0.077226. Entropy: 0.308635.\n",
      "episode: 3229   score: 620.0  epsilon: 1.0    steps: 384  evaluation reward: 420.1\n",
      "episode: 3230   score: 470.0  epsilon: 1.0    steps: 896  evaluation reward: 421.65\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8476: Policy loss: -0.101687. Value loss: 0.214830. Entropy: 0.290989.\n",
      "Iteration 8477: Policy loss: -0.105364. Value loss: 0.083828. Entropy: 0.291108.\n",
      "Iteration 8478: Policy loss: -0.090073. Value loss: 0.040529. Entropy: 0.290428.\n",
      "episode: 3231   score: 620.0  epsilon: 1.0    steps: 808  evaluation reward: 425.0\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8479: Policy loss: 0.138468. Value loss: 0.204842. Entropy: 0.292491.\n",
      "Iteration 8480: Policy loss: 0.122229. Value loss: 0.065380. Entropy: 0.292467.\n",
      "Iteration 8481: Policy loss: 0.103932. Value loss: 0.038083. Entropy: 0.292585.\n",
      "episode: 3232   score: 210.0  epsilon: 1.0    steps: 904  evaluation reward: 422.9\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8482: Policy loss: 0.224687. Value loss: 0.106476. Entropy: 0.298819.\n",
      "Iteration 8483: Policy loss: 0.217714. Value loss: 0.046174. Entropy: 0.298919.\n",
      "Iteration 8484: Policy loss: 0.213881. Value loss: 0.032550. Entropy: 0.298882.\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8485: Policy loss: 0.153646. Value loss: 0.093325. Entropy: 0.299749.\n",
      "Iteration 8486: Policy loss: 0.143928. Value loss: 0.027080. Entropy: 0.300050.\n",
      "Iteration 8487: Policy loss: 0.141832. Value loss: 0.018026. Entropy: 0.299690.\n",
      "episode: 3233   score: 530.0  epsilon: 1.0    steps: 408  evaluation reward: 425.55\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8488: Policy loss: 0.021970. Value loss: 0.261036. Entropy: 0.299618.\n",
      "Iteration 8489: Policy loss: 0.015079. Value loss: 0.141420. Entropy: 0.298477.\n",
      "Iteration 8490: Policy loss: 0.005837. Value loss: 0.092943. Entropy: 0.299307.\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8491: Policy loss: 0.063413. Value loss: 0.100534. Entropy: 0.307521.\n",
      "Iteration 8492: Policy loss: 0.056988. Value loss: 0.051918. Entropy: 0.307773.\n",
      "Iteration 8493: Policy loss: 0.052704. Value loss: 0.038818. Entropy: 0.308044.\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8494: Policy loss: 0.131561. Value loss: 0.091673. Entropy: 0.304210.\n",
      "Iteration 8495: Policy loss: 0.123209. Value loss: 0.039823. Entropy: 0.303232.\n",
      "Iteration 8496: Policy loss: 0.117269. Value loss: 0.028938. Entropy: 0.303334.\n",
      "episode: 3234   score: 450.0  epsilon: 1.0    steps: 672  evaluation reward: 426.1\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8497: Policy loss: 0.214509. Value loss: 0.096484. Entropy: 0.297165.\n",
      "Iteration 8498: Policy loss: 0.200525. Value loss: 0.049047. Entropy: 0.296801.\n",
      "Iteration 8499: Policy loss: 0.204177. Value loss: 0.032474. Entropy: 0.295351.\n",
      "Training network. lr: 0.000185. clip: 0.074048\n",
      "Iteration 8500: Policy loss: 0.145554. Value loss: 0.116892. Entropy: 0.309032.\n",
      "Iteration 8501: Policy loss: 0.133058. Value loss: 0.040136. Entropy: 0.308008.\n",
      "Iteration 8502: Policy loss: 0.126644. Value loss: 0.027320. Entropy: 0.307658.\n",
      "episode: 3235   score: 225.0  epsilon: 1.0    steps: 504  evaluation reward: 425.35\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8503: Policy loss: 0.335039. Value loss: 0.102570. Entropy: 0.296123.\n",
      "Iteration 8504: Policy loss: 0.329143. Value loss: 0.022995. Entropy: 0.296513.\n",
      "Iteration 8505: Policy loss: 0.329829. Value loss: 0.015244. Entropy: 0.295767.\n",
      "episode: 3236   score: 395.0  epsilon: 1.0    steps: 16  evaluation reward: 426.0\n",
      "episode: 3237   score: 395.0  epsilon: 1.0    steps: 48  evaluation reward: 426.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8506: Policy loss: -0.054027. Value loss: 0.054115. Entropy: 0.285917.\n",
      "Iteration 8507: Policy loss: -0.058750. Value loss: 0.021986. Entropy: 0.287247.\n",
      "Iteration 8508: Policy loss: -0.060072. Value loss: 0.015355. Entropy: 0.287358.\n",
      "episode: 3238   score: 240.0  epsilon: 1.0    steps: 24  evaluation reward: 426.0\n",
      "episode: 3239   score: 345.0  epsilon: 1.0    steps: 432  evaluation reward: 424.3\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8509: Policy loss: 0.148966. Value loss: 0.121342. Entropy: 0.288843.\n",
      "Iteration 8510: Policy loss: 0.139363. Value loss: 0.041992. Entropy: 0.285611.\n",
      "Iteration 8511: Policy loss: 0.136742. Value loss: 0.030447. Entropy: 0.284698.\n",
      "episode: 3240   score: 480.0  epsilon: 1.0    steps: 464  evaluation reward: 425.85\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8512: Policy loss: 0.101146. Value loss: 0.095475. Entropy: 0.295623.\n",
      "Iteration 8513: Policy loss: 0.084815. Value loss: 0.048446. Entropy: 0.296415.\n",
      "Iteration 8514: Policy loss: 0.089009. Value loss: 0.033715. Entropy: 0.294260.\n",
      "episode: 3241   score: 620.0  epsilon: 1.0    steps: 296  evaluation reward: 427.2\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8515: Policy loss: -0.001769. Value loss: 0.077290. Entropy: 0.294169.\n",
      "Iteration 8516: Policy loss: -0.010215. Value loss: 0.035814. Entropy: 0.294996.\n",
      "Iteration 8517: Policy loss: -0.007629. Value loss: 0.026224. Entropy: 0.294676.\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8518: Policy loss: 0.036157. Value loss: 0.086733. Entropy: 0.309595.\n",
      "Iteration 8519: Policy loss: 0.032509. Value loss: 0.034872. Entropy: 0.308692.\n",
      "Iteration 8520: Policy loss: 0.032245. Value loss: 0.026822. Entropy: 0.308625.\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8521: Policy loss: -0.029371. Value loss: 0.069287. Entropy: 0.308077.\n",
      "Iteration 8522: Policy loss: -0.035980. Value loss: 0.035621. Entropy: 0.308098.\n",
      "Iteration 8523: Policy loss: -0.036748. Value loss: 0.028123. Entropy: 0.307565.\n",
      "episode: 3242   score: 210.0  epsilon: 1.0    steps: 976  evaluation reward: 422.65\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8524: Policy loss: 0.178090. Value loss: 0.074484. Entropy: 0.303593.\n",
      "Iteration 8525: Policy loss: 0.176086. Value loss: 0.029831. Entropy: 0.301444.\n",
      "Iteration 8526: Policy loss: 0.172711. Value loss: 0.024222. Entropy: 0.303014.\n",
      "episode: 3243   score: 315.0  epsilon: 1.0    steps: 264  evaluation reward: 424.55\n",
      "episode: 3244   score: 285.0  epsilon: 1.0    steps: 488  evaluation reward: 421.2\n",
      "episode: 3245   score: 345.0  epsilon: 1.0    steps: 520  evaluation reward: 419.7\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8527: Policy loss: -0.006975. Value loss: 0.070530. Entropy: 0.259559.\n",
      "Iteration 8528: Policy loss: -0.017851. Value loss: 0.030556. Entropy: 0.260549.\n",
      "Iteration 8529: Policy loss: -0.018328. Value loss: 0.026943. Entropy: 0.260949.\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8530: Policy loss: -0.029735. Value loss: 0.108311. Entropy: 0.310643.\n",
      "Iteration 8531: Policy loss: -0.034841. Value loss: 0.043264. Entropy: 0.310630.\n",
      "Iteration 8532: Policy loss: -0.037583. Value loss: 0.031665. Entropy: 0.310570.\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8533: Policy loss: 0.093256. Value loss: 0.090850. Entropy: 0.308548.\n",
      "Iteration 8534: Policy loss: 0.092362. Value loss: 0.041634. Entropy: 0.309037.\n",
      "Iteration 8535: Policy loss: 0.083392. Value loss: 0.030292. Entropy: 0.308156.\n",
      "episode: 3246   score: 270.0  epsilon: 1.0    steps: 152  evaluation reward: 418.15\n",
      "episode: 3247   score: 575.0  epsilon: 1.0    steps: 648  evaluation reward: 420.6\n",
      "episode: 3248   score: 260.0  epsilon: 1.0    steps: 1000  evaluation reward: 419.75\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8536: Policy loss: 0.146926. Value loss: 0.052704. Entropy: 0.277295.\n",
      "Iteration 8537: Policy loss: 0.138512. Value loss: 0.025978. Entropy: 0.276513.\n",
      "Iteration 8538: Policy loss: 0.134774. Value loss: 0.020927. Entropy: 0.277109.\n",
      "episode: 3249   score: 365.0  epsilon: 1.0    steps: 776  evaluation reward: 416.35\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8539: Policy loss: 0.037501. Value loss: 0.114593. Entropy: 0.284848.\n",
      "Iteration 8540: Policy loss: 0.028874. Value loss: 0.047227. Entropy: 0.284462.\n",
      "Iteration 8541: Policy loss: 0.027836. Value loss: 0.032457. Entropy: 0.284371.\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8542: Policy loss: 0.094233. Value loss: 0.082889. Entropy: 0.311369.\n",
      "Iteration 8543: Policy loss: 0.081062. Value loss: 0.032282. Entropy: 0.310528.\n",
      "Iteration 8544: Policy loss: 0.084768. Value loss: 0.024327. Entropy: 0.311311.\n",
      "episode: 3250   score: 100.0  epsilon: 1.0    steps: 56  evaluation reward: 411.7\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8545: Policy loss: -0.038450. Value loss: 0.066523. Entropy: 0.298509.\n",
      "Iteration 8546: Policy loss: -0.036358. Value loss: 0.029697. Entropy: 0.300049.\n",
      "Iteration 8547: Policy loss: -0.034922. Value loss: 0.022343. Entropy: 0.299999.\n",
      "now time :  2019-09-05 23:05:18.269562\n",
      "episode: 3251   score: 290.0  epsilon: 1.0    steps: 488  evaluation reward: 412.2\n",
      "episode: 3252   score: 285.0  epsilon: 1.0    steps: 568  evaluation reward: 411.4\n",
      "Training network. lr: 0.000185. clip: 0.073891\n",
      "Iteration 8548: Policy loss: 0.066810. Value loss: 0.043934. Entropy: 0.284890.\n",
      "Iteration 8549: Policy loss: 0.061722. Value loss: 0.029217. Entropy: 0.286835.\n",
      "Iteration 8550: Policy loss: 0.058556. Value loss: 0.023968. Entropy: 0.285603.\n",
      "episode: 3253   score: 275.0  epsilon: 1.0    steps: 56  evaluation reward: 410.1\n",
      "episode: 3254   score: 150.0  epsilon: 1.0    steps: 552  evaluation reward: 407.0\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8551: Policy loss: -0.013577. Value loss: 0.049669. Entropy: 0.282892.\n",
      "Iteration 8552: Policy loss: -0.021726. Value loss: 0.028854. Entropy: 0.282306.\n",
      "Iteration 8553: Policy loss: -0.018470. Value loss: 0.024033. Entropy: 0.283502.\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8554: Policy loss: -0.326138. Value loss: 0.296979. Entropy: 0.306308.\n",
      "Iteration 8555: Policy loss: -0.329317. Value loss: 0.133148. Entropy: 0.306209.\n",
      "Iteration 8556: Policy loss: -0.332604. Value loss: 0.082187. Entropy: 0.306870.\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8557: Policy loss: 0.127407. Value loss: 0.131407. Entropy: 0.304808.\n",
      "Iteration 8558: Policy loss: 0.115673. Value loss: 0.053167. Entropy: 0.303773.\n",
      "Iteration 8559: Policy loss: 0.102576. Value loss: 0.039012. Entropy: 0.304379.\n",
      "episode: 3255   score: 310.0  epsilon: 1.0    steps: 352  evaluation reward: 406.95\n",
      "episode: 3256   score: 155.0  epsilon: 1.0    steps: 496  evaluation reward: 402.2\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8560: Policy loss: 0.043097. Value loss: 0.042436. Entropy: 0.281522.\n",
      "Iteration 8561: Policy loss: 0.039451. Value loss: 0.022767. Entropy: 0.280612.\n",
      "Iteration 8562: Policy loss: 0.037145. Value loss: 0.019023. Entropy: 0.281093.\n",
      "episode: 3257   score: 875.0  epsilon: 1.0    steps: 424  evaluation reward: 404.45\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8563: Policy loss: 0.010300. Value loss: 0.269492. Entropy: 0.293726.\n",
      "Iteration 8564: Policy loss: 0.003538. Value loss: 0.193945. Entropy: 0.294033.\n",
      "Iteration 8565: Policy loss: -0.021325. Value loss: 0.146546. Entropy: 0.294381.\n",
      "episode: 3258   score: 285.0  epsilon: 1.0    steps: 192  evaluation reward: 404.0\n",
      "episode: 3259   score: 410.0  epsilon: 1.0    steps: 952  evaluation reward: 405.1\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8566: Policy loss: -0.417328. Value loss: 0.298823. Entropy: 0.291154.\n",
      "Iteration 8567: Policy loss: -0.444699. Value loss: 0.103832. Entropy: 0.292005.\n",
      "Iteration 8568: Policy loss: -0.440648. Value loss: 0.049896. Entropy: 0.292039.\n",
      "episode: 3260   score: 365.0  epsilon: 1.0    steps: 416  evaluation reward: 405.15\n",
      "episode: 3261   score: 255.0  epsilon: 1.0    steps: 712  evaluation reward: 405.1\n",
      "episode: 3262   score: 345.0  epsilon: 1.0    steps: 992  evaluation reward: 406.3\n",
      "Training network. lr: 0.000184. clip: 0.073744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8569: Policy loss: 0.158421. Value loss: 0.092699. Entropy: 0.275151.\n",
      "Iteration 8570: Policy loss: 0.144178. Value loss: 0.041441. Entropy: 0.270869.\n",
      "Iteration 8571: Policy loss: 0.144413. Value loss: 0.034784. Entropy: 0.273083.\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8572: Policy loss: 0.133269. Value loss: 0.130821. Entropy: 0.296673.\n",
      "Iteration 8573: Policy loss: 0.133166. Value loss: 0.057582. Entropy: 0.294739.\n",
      "Iteration 8574: Policy loss: 0.124649. Value loss: 0.044061. Entropy: 0.296452.\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8575: Policy loss: 0.021411. Value loss: 0.148048. Entropy: 0.304727.\n",
      "Iteration 8576: Policy loss: 0.009868. Value loss: 0.040683. Entropy: 0.305428.\n",
      "Iteration 8577: Policy loss: 0.009827. Value loss: 0.029352. Entropy: 0.303988.\n",
      "episode: 3263   score: 180.0  epsilon: 1.0    steps: 312  evaluation reward: 403.15\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8578: Policy loss: -0.346982. Value loss: 0.199192. Entropy: 0.291776.\n",
      "Iteration 8579: Policy loss: -0.345518. Value loss: 0.083515. Entropy: 0.292510.\n",
      "Iteration 8580: Policy loss: -0.352978. Value loss: 0.047110. Entropy: 0.292854.\n",
      "episode: 3264   score: 290.0  epsilon: 1.0    steps: 456  evaluation reward: 400.75\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8581: Policy loss: -0.182640. Value loss: 0.080930. Entropy: 0.290076.\n",
      "Iteration 8582: Policy loss: -0.191156. Value loss: 0.033717. Entropy: 0.291642.\n",
      "Iteration 8583: Policy loss: -0.186177. Value loss: 0.026627. Entropy: 0.290741.\n",
      "episode: 3265   score: 315.0  epsilon: 1.0    steps: 776  evaluation reward: 397.95\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8584: Policy loss: -0.203261. Value loss: 0.294255. Entropy: 0.298373.\n",
      "Iteration 8585: Policy loss: -0.207790. Value loss: 0.107371. Entropy: 0.297526.\n",
      "Iteration 8586: Policy loss: -0.226105. Value loss: 0.041554. Entropy: 0.297516.\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8587: Policy loss: 0.264698. Value loss: 0.276736. Entropy: 0.306500.\n",
      "Iteration 8588: Policy loss: 0.250330. Value loss: 0.081682. Entropy: 0.306344.\n",
      "Iteration 8589: Policy loss: 0.232804. Value loss: 0.057491. Entropy: 0.306557.\n",
      "episode: 3266   score: 225.0  epsilon: 1.0    steps: 88  evaluation reward: 398.7\n",
      "episode: 3267   score: 260.0  epsilon: 1.0    steps: 456  evaluation reward: 398.75\n",
      "episode: 3268   score: 520.0  epsilon: 1.0    steps: 904  evaluation reward: 401.1\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8590: Policy loss: 0.246217. Value loss: 0.225394. Entropy: 0.275836.\n",
      "Iteration 8591: Policy loss: 0.232882. Value loss: 0.070681. Entropy: 0.275307.\n",
      "Iteration 8592: Policy loss: 0.225402. Value loss: 0.046914. Entropy: 0.275436.\n",
      "episode: 3269   score: 420.0  epsilon: 1.0    steps: 424  evaluation reward: 401.85\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8593: Policy loss: 0.243487. Value loss: 0.132777. Entropy: 0.286770.\n",
      "Iteration 8594: Policy loss: 0.247754. Value loss: 0.047874. Entropy: 0.286272.\n",
      "Iteration 8595: Policy loss: 0.243964. Value loss: 0.033292. Entropy: 0.286818.\n",
      "episode: 3270   score: 535.0  epsilon: 1.0    steps: 360  evaluation reward: 404.2\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8596: Policy loss: 0.105146. Value loss: 0.112998. Entropy: 0.296608.\n",
      "Iteration 8597: Policy loss: 0.100450. Value loss: 0.058173. Entropy: 0.294947.\n",
      "Iteration 8598: Policy loss: 0.095177. Value loss: 0.042030. Entropy: 0.296830.\n",
      "Training network. lr: 0.000184. clip: 0.073744\n",
      "Iteration 8599: Policy loss: 0.057043. Value loss: 0.075486. Entropy: 0.307944.\n",
      "Iteration 8600: Policy loss: 0.051601. Value loss: 0.043916. Entropy: 0.306719.\n",
      "Iteration 8601: Policy loss: 0.048278. Value loss: 0.034906. Entropy: 0.306356.\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8602: Policy loss: -0.019696. Value loss: 0.106855. Entropy: 0.306255.\n",
      "Iteration 8603: Policy loss: -0.025109. Value loss: 0.057215. Entropy: 0.305678.\n",
      "Iteration 8604: Policy loss: -0.033945. Value loss: 0.045287. Entropy: 0.305325.\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8605: Policy loss: 0.221307. Value loss: 0.102668. Entropy: 0.307891.\n",
      "Iteration 8606: Policy loss: 0.209472. Value loss: 0.029625. Entropy: 0.307649.\n",
      "Iteration 8607: Policy loss: 0.197847. Value loss: 0.015837. Entropy: 0.307407.\n",
      "episode: 3271   score: 210.0  epsilon: 1.0    steps: 40  evaluation reward: 403.0\n",
      "episode: 3272   score: 210.0  epsilon: 1.0    steps: 360  evaluation reward: 401.85\n",
      "episode: 3273   score: 285.0  epsilon: 1.0    steps: 376  evaluation reward: 400.8\n",
      "episode: 3274   score: 340.0  epsilon: 1.0    steps: 1000  evaluation reward: 400.0\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8608: Policy loss: 0.170403. Value loss: 0.073119. Entropy: 0.263970.\n",
      "Iteration 8609: Policy loss: 0.170436. Value loss: 0.023203. Entropy: 0.262772.\n",
      "Iteration 8610: Policy loss: 0.173679. Value loss: 0.017216. Entropy: 0.262211.\n",
      "episode: 3275   score: 375.0  epsilon: 1.0    steps: 904  evaluation reward: 400.9\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8611: Policy loss: 0.125949. Value loss: 0.142721. Entropy: 0.286896.\n",
      "Iteration 8612: Policy loss: 0.120513. Value loss: 0.063261. Entropy: 0.286407.\n",
      "Iteration 8613: Policy loss: 0.115066. Value loss: 0.044979. Entropy: 0.287114.\n",
      "episode: 3276   score: 315.0  epsilon: 1.0    steps: 416  evaluation reward: 400.6\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8614: Policy loss: -0.042954. Value loss: 0.123061. Entropy: 0.287626.\n",
      "Iteration 8615: Policy loss: -0.045196. Value loss: 0.073167. Entropy: 0.287638.\n",
      "Iteration 8616: Policy loss: -0.047930. Value loss: 0.057621. Entropy: 0.288747.\n",
      "episode: 3277   score: 260.0  epsilon: 1.0    steps: 56  evaluation reward: 400.55\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8617: Policy loss: 0.190177. Value loss: 0.123084. Entropy: 0.290788.\n",
      "Iteration 8618: Policy loss: 0.184086. Value loss: 0.052645. Entropy: 0.290982.\n",
      "Iteration 8619: Policy loss: 0.178816. Value loss: 0.033617. Entropy: 0.290992.\n",
      "episode: 3278   score: 180.0  epsilon: 1.0    steps: 128  evaluation reward: 396.35\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8620: Policy loss: -0.049013. Value loss: 0.047705. Entropy: 0.294611.\n",
      "Iteration 8621: Policy loss: -0.057509. Value loss: 0.023828. Entropy: 0.295965.\n",
      "Iteration 8622: Policy loss: -0.055100. Value loss: 0.021338. Entropy: 0.297047.\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8623: Policy loss: 0.131575. Value loss: 0.066558. Entropy: 0.304593.\n",
      "Iteration 8624: Policy loss: 0.126263. Value loss: 0.024631. Entropy: 0.305122.\n",
      "Iteration 8625: Policy loss: 0.120018. Value loss: 0.016521. Entropy: 0.304091.\n",
      "episode: 3279   score: 420.0  epsilon: 1.0    steps: 1008  evaluation reward: 397.1\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8626: Policy loss: -0.127579. Value loss: 0.132098. Entropy: 0.303686.\n",
      "Iteration 8627: Policy loss: -0.137550. Value loss: 0.051803. Entropy: 0.304343.\n",
      "Iteration 8628: Policy loss: -0.140115. Value loss: 0.036629. Entropy: 0.304372.\n",
      "episode: 3280   score: 260.0  epsilon: 1.0    steps: 56  evaluation reward: 393.8\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8629: Policy loss: 0.142227. Value loss: 0.085478. Entropy: 0.283171.\n",
      "Iteration 8630: Policy loss: 0.135316. Value loss: 0.029683. Entropy: 0.285077.\n",
      "Iteration 8631: Policy loss: 0.128715. Value loss: 0.022363. Entropy: 0.285489.\n",
      "episode: 3281   score: 330.0  epsilon: 1.0    steps: 136  evaluation reward: 392.9\n",
      "episode: 3282   score: 330.0  epsilon: 1.0    steps: 248  evaluation reward: 393.55\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8632: Policy loss: -0.195914. Value loss: 0.125298. Entropy: 0.291521.\n",
      "Iteration 8633: Policy loss: -0.207185. Value loss: 0.047336. Entropy: 0.291757.\n",
      "Iteration 8634: Policy loss: -0.207965. Value loss: 0.033880. Entropy: 0.291134.\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8635: Policy loss: -0.334332. Value loss: 0.337294. Entropy: 0.308221.\n",
      "Iteration 8636: Policy loss: -0.341308. Value loss: 0.198139. Entropy: 0.306674.\n",
      "Iteration 8637: Policy loss: -0.351534. Value loss: 0.098234. Entropy: 0.306479.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3283   score: 315.0  epsilon: 1.0    steps: 384  evaluation reward: 392.5\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8638: Policy loss: 0.204191. Value loss: 0.079368. Entropy: 0.299334.\n",
      "Iteration 8639: Policy loss: 0.194668. Value loss: 0.025025. Entropy: 0.296375.\n",
      "Iteration 8640: Policy loss: 0.187630. Value loss: 0.019584. Entropy: 0.296034.\n",
      "episode: 3284   score: 285.0  epsilon: 1.0    steps: 752  evaluation reward: 391.4\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8641: Policy loss: 0.149945. Value loss: 0.096638. Entropy: 0.294100.\n",
      "Iteration 8642: Policy loss: 0.149362. Value loss: 0.039739. Entropy: 0.294728.\n",
      "Iteration 8643: Policy loss: 0.128432. Value loss: 0.029361. Entropy: 0.292730.\n",
      "episode: 3285   score: 420.0  epsilon: 1.0    steps: 544  evaluation reward: 390.15\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8644: Policy loss: 0.011258. Value loss: 0.143167. Entropy: 0.292820.\n",
      "Iteration 8645: Policy loss: -0.002648. Value loss: 0.050397. Entropy: 0.292153.\n",
      "Iteration 8646: Policy loss: -0.003099. Value loss: 0.038284. Entropy: 0.293714.\n",
      "episode: 3286   score: 395.0  epsilon: 1.0    steps: 432  evaluation reward: 388.65\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8647: Policy loss: 0.271284. Value loss: 0.092932. Entropy: 0.294628.\n",
      "Iteration 8648: Policy loss: 0.265906. Value loss: 0.027397. Entropy: 0.294852.\n",
      "Iteration 8649: Policy loss: 0.261720. Value loss: 0.019257. Entropy: 0.295221.\n",
      "episode: 3287   score: 260.0  epsilon: 1.0    steps: 616  evaluation reward: 389.1\n",
      "Training network. lr: 0.000184. clip: 0.073587\n",
      "Iteration 8650: Policy loss: 0.034436. Value loss: 0.089422. Entropy: 0.299935.\n",
      "Iteration 8651: Policy loss: 0.022577. Value loss: 0.039077. Entropy: 0.301195.\n",
      "Iteration 8652: Policy loss: 0.021709. Value loss: 0.029105. Entropy: 0.300227.\n",
      "episode: 3288   score: 365.0  epsilon: 1.0    steps: 640  evaluation reward: 389.3\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8653: Policy loss: -0.092414. Value loss: 0.092214. Entropy: 0.299624.\n",
      "Iteration 8654: Policy loss: -0.097992. Value loss: 0.055353. Entropy: 0.298456.\n",
      "Iteration 8655: Policy loss: -0.110902. Value loss: 0.040096. Entropy: 0.298368.\n",
      "episode: 3289   score: 365.0  epsilon: 1.0    steps: 488  evaluation reward: 390.15\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8656: Policy loss: 0.107208. Value loss: 0.068588. Entropy: 0.297180.\n",
      "Iteration 8657: Policy loss: 0.095408. Value loss: 0.030138. Entropy: 0.297134.\n",
      "Iteration 8658: Policy loss: 0.101207. Value loss: 0.022667. Entropy: 0.296929.\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8659: Policy loss: -0.397946. Value loss: 0.286116. Entropy: 0.305669.\n",
      "Iteration 8660: Policy loss: -0.397987. Value loss: 0.150243. Entropy: 0.305956.\n",
      "Iteration 8661: Policy loss: -0.411023. Value loss: 0.098905. Entropy: 0.307774.\n",
      "episode: 3290   score: 420.0  epsilon: 1.0    steps: 976  evaluation reward: 391.5\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8662: Policy loss: -0.008147. Value loss: 0.100173. Entropy: 0.307247.\n",
      "Iteration 8663: Policy loss: -0.013098. Value loss: 0.047310. Entropy: 0.305636.\n",
      "Iteration 8664: Policy loss: -0.025314. Value loss: 0.031075. Entropy: 0.305800.\n",
      "episode: 3291   score: 385.0  epsilon: 1.0    steps: 832  evaluation reward: 391.7\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8665: Policy loss: -0.068282. Value loss: 0.163576. Entropy: 0.292179.\n",
      "Iteration 8666: Policy loss: -0.077820. Value loss: 0.072935. Entropy: 0.290946.\n",
      "Iteration 8667: Policy loss: -0.089805. Value loss: 0.047257. Entropy: 0.290582.\n",
      "episode: 3292   score: 465.0  epsilon: 1.0    steps: 928  evaluation reward: 393.65\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8668: Policy loss: 0.052706. Value loss: 0.115958. Entropy: 0.306401.\n",
      "Iteration 8669: Policy loss: 0.037979. Value loss: 0.051663. Entropy: 0.305855.\n",
      "Iteration 8670: Policy loss: 0.039446. Value loss: 0.039478. Entropy: 0.305419.\n",
      "episode: 3293   score: 800.0  epsilon: 1.0    steps: 224  evaluation reward: 398.0\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8671: Policy loss: 0.187473. Value loss: 0.111365. Entropy: 0.292987.\n",
      "Iteration 8672: Policy loss: 0.175773. Value loss: 0.058311. Entropy: 0.292312.\n",
      "Iteration 8673: Policy loss: 0.176794. Value loss: 0.040703. Entropy: 0.293150.\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8674: Policy loss: -0.041324. Value loss: 0.067107. Entropy: 0.312761.\n",
      "Iteration 8675: Policy loss: -0.048028. Value loss: 0.031322. Entropy: 0.311724.\n",
      "Iteration 8676: Policy loss: -0.051820. Value loss: 0.026664. Entropy: 0.311557.\n",
      "episode: 3294   score: 620.0  epsilon: 1.0    steps: 32  evaluation reward: 396.0\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8677: Policy loss: -0.424006. Value loss: 0.339906. Entropy: 0.296890.\n",
      "Iteration 8678: Policy loss: -0.416702. Value loss: 0.111998. Entropy: 0.295822.\n",
      "Iteration 8679: Policy loss: -0.463316. Value loss: 0.075383. Entropy: 0.297721.\n",
      "episode: 3295   score: 435.0  epsilon: 1.0    steps: 496  evaluation reward: 395.0\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8680: Policy loss: 0.017651. Value loss: 0.096473. Entropy: 0.297096.\n",
      "Iteration 8681: Policy loss: -0.001859. Value loss: 0.039725. Entropy: 0.296907.\n",
      "Iteration 8682: Policy loss: -0.003903. Value loss: 0.024128. Entropy: 0.296096.\n",
      "episode: 3296   score: 410.0  epsilon: 1.0    steps: 192  evaluation reward: 396.5\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8683: Policy loss: -0.055939. Value loss: 0.284885. Entropy: 0.295291.\n",
      "Iteration 8684: Policy loss: -0.073313. Value loss: 0.084721. Entropy: 0.293662.\n",
      "Iteration 8685: Policy loss: -0.102633. Value loss: 0.049350. Entropy: 0.295829.\n",
      "episode: 3297   score: 620.0  epsilon: 1.0    steps: 200  evaluation reward: 398.6\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8686: Policy loss: 0.150345. Value loss: 0.121501. Entropy: 0.299328.\n",
      "Iteration 8687: Policy loss: 0.136020. Value loss: 0.055156. Entropy: 0.297740.\n",
      "Iteration 8688: Policy loss: 0.145513. Value loss: 0.039408. Entropy: 0.297857.\n",
      "episode: 3298   score: 495.0  epsilon: 1.0    steps: 752  evaluation reward: 398.55\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8689: Policy loss: 0.071189. Value loss: 0.173208. Entropy: 0.299158.\n",
      "Iteration 8690: Policy loss: 0.064913. Value loss: 0.070500. Entropy: 0.297382.\n",
      "Iteration 8691: Policy loss: 0.051369. Value loss: 0.048849. Entropy: 0.298225.\n",
      "episode: 3299   score: 300.0  epsilon: 1.0    steps: 104  evaluation reward: 396.9\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8692: Policy loss: -0.048123. Value loss: 0.109238. Entropy: 0.300677.\n",
      "Iteration 8693: Policy loss: -0.052231. Value loss: 0.051913. Entropy: 0.301830.\n",
      "Iteration 8694: Policy loss: -0.060559. Value loss: 0.034652. Entropy: 0.300885.\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8695: Policy loss: -0.047099. Value loss: 0.159982. Entropy: 0.309794.\n",
      "Iteration 8696: Policy loss: -0.055370. Value loss: 0.064800. Entropy: 0.309134.\n",
      "Iteration 8697: Policy loss: -0.056619. Value loss: 0.048938. Entropy: 0.308613.\n",
      "episode: 3300   score: 620.0  epsilon: 1.0    steps: 696  evaluation reward: 396.15\n",
      "Training network. lr: 0.000184. clip: 0.073430\n",
      "Iteration 8698: Policy loss: 0.009628. Value loss: 0.131724. Entropy: 0.302987.\n",
      "Iteration 8699: Policy loss: 0.007252. Value loss: 0.062412. Entropy: 0.303928.\n",
      "Iteration 8700: Policy loss: -0.000647. Value loss: 0.046992. Entropy: 0.303942.\n",
      "now time :  2019-09-05 23:14:47.900576\n",
      "episode: 3301   score: 265.0  epsilon: 1.0    steps: 56  evaluation reward: 394.6\n",
      "episode: 3302   score: 555.0  epsilon: 1.0    steps: 808  evaluation reward: 397.3\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8701: Policy loss: -0.539265. Value loss: 0.548651. Entropy: 0.294302.\n",
      "Iteration 8702: Policy loss: -0.530864. Value loss: 0.214075. Entropy: 0.296837.\n",
      "Iteration 8703: Policy loss: -0.544217. Value loss: 0.129151. Entropy: 0.297381.\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8704: Policy loss: -0.135517. Value loss: 0.159397. Entropy: 0.306719.\n",
      "Iteration 8705: Policy loss: -0.144078. Value loss: 0.063812. Entropy: 0.307234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8706: Policy loss: -0.155368. Value loss: 0.038916. Entropy: 0.306182.\n",
      "episode: 3303   score: 575.0  epsilon: 1.0    steps: 560  evaluation reward: 398.85\n",
      "episode: 3304   score: 590.0  epsilon: 1.0    steps: 848  evaluation reward: 398.55\n",
      "episode: 3305   score: 640.0  epsilon: 1.0    steps: 848  evaluation reward: 398.25\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8707: Policy loss: 0.319379. Value loss: 0.476497. Entropy: 0.283799.\n",
      "Iteration 8708: Policy loss: 0.307521. Value loss: 0.177159. Entropy: 0.283548.\n",
      "Iteration 8709: Policy loss: 0.287845. Value loss: 0.104175. Entropy: 0.283009.\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8710: Policy loss: 0.171026. Value loss: 0.121373. Entropy: 0.308229.\n",
      "Iteration 8711: Policy loss: 0.160287. Value loss: 0.055526. Entropy: 0.307027.\n",
      "Iteration 8712: Policy loss: 0.159322. Value loss: 0.039077. Entropy: 0.306176.\n",
      "episode: 3306   score: 330.0  epsilon: 1.0    steps: 224  evaluation reward: 399.4\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8713: Policy loss: -0.047903. Value loss: 0.122875. Entropy: 0.304818.\n",
      "Iteration 8714: Policy loss: -0.050294. Value loss: 0.059338. Entropy: 0.304167.\n",
      "Iteration 8715: Policy loss: -0.057975. Value loss: 0.038178. Entropy: 0.303913.\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8716: Policy loss: 0.247292. Value loss: 0.430918. Entropy: 0.307126.\n",
      "Iteration 8717: Policy loss: 0.244754. Value loss: 0.209646. Entropy: 0.305249.\n",
      "Iteration 8718: Policy loss: 0.228309. Value loss: 0.110820. Entropy: 0.306094.\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8719: Policy loss: 0.006094. Value loss: 0.361077. Entropy: 0.306017.\n",
      "Iteration 8720: Policy loss: -0.035470. Value loss: 0.091054. Entropy: 0.304542.\n",
      "Iteration 8721: Policy loss: -0.014461. Value loss: 0.059567. Entropy: 0.307242.\n",
      "episode: 3307   score: 285.0  epsilon: 1.0    steps: 632  evaluation reward: 399.65\n",
      "episode: 3308   score: 515.0  epsilon: 1.0    steps: 792  evaluation reward: 396.9\n",
      "episode: 3309   score: 155.0  epsilon: 1.0    steps: 1000  evaluation reward: 395.1\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8722: Policy loss: 0.051177. Value loss: 0.097378. Entropy: 0.290803.\n",
      "Iteration 8723: Policy loss: 0.048291. Value loss: 0.044718. Entropy: 0.290977.\n",
      "Iteration 8724: Policy loss: 0.050156. Value loss: 0.030081. Entropy: 0.290353.\n",
      "episode: 3310   score: 320.0  epsilon: 1.0    steps: 40  evaluation reward: 395.65\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8725: Policy loss: 0.156687. Value loss: 0.169402. Entropy: 0.295377.\n",
      "Iteration 8726: Policy loss: 0.141600. Value loss: 0.071522. Entropy: 0.295563.\n",
      "Iteration 8727: Policy loss: 0.134966. Value loss: 0.047210. Entropy: 0.296045.\n",
      "episode: 3311   score: 680.0  epsilon: 1.0    steps: 88  evaluation reward: 399.75\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8728: Policy loss: -0.470850. Value loss: 0.173914. Entropy: 0.300629.\n",
      "Iteration 8729: Policy loss: -0.484362. Value loss: 0.064300. Entropy: 0.301286.\n",
      "Iteration 8730: Policy loss: -0.496087. Value loss: 0.043030. Entropy: 0.301624.\n",
      "episode: 3312   score: 610.0  epsilon: 1.0    steps: 912  evaluation reward: 402.1\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8731: Policy loss: -0.242228. Value loss: 0.276677. Entropy: 0.303659.\n",
      "Iteration 8732: Policy loss: -0.241679. Value loss: 0.165265. Entropy: 0.303239.\n",
      "Iteration 8733: Policy loss: -0.254461. Value loss: 0.131771. Entropy: 0.303819.\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8734: Policy loss: 0.088210. Value loss: 0.218302. Entropy: 0.302444.\n",
      "Iteration 8735: Policy loss: 0.071227. Value loss: 0.055485. Entropy: 0.302648.\n",
      "Iteration 8736: Policy loss: 0.055302. Value loss: 0.030450. Entropy: 0.302346.\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8737: Policy loss: 0.299086. Value loss: 0.123769. Entropy: 0.309238.\n",
      "Iteration 8738: Policy loss: 0.264634. Value loss: 0.031894. Entropy: 0.307892.\n",
      "Iteration 8739: Policy loss: 0.279190. Value loss: 0.021912. Entropy: 0.307330.\n",
      "episode: 3313   score: 470.0  epsilon: 1.0    steps: 920  evaluation reward: 401.85\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8740: Policy loss: 0.358201. Value loss: 0.164345. Entropy: 0.300131.\n",
      "Iteration 8741: Policy loss: 0.338643. Value loss: 0.063793. Entropy: 0.300363.\n",
      "Iteration 8742: Policy loss: 0.334242. Value loss: 0.041555. Entropy: 0.299658.\n",
      "episode: 3314   score: 665.0  epsilon: 1.0    steps: 560  evaluation reward: 401.0\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8743: Policy loss: -0.045926. Value loss: 0.128852. Entropy: 0.285488.\n",
      "Iteration 8744: Policy loss: -0.051134. Value loss: 0.050536. Entropy: 0.283052.\n",
      "Iteration 8745: Policy loss: -0.067931. Value loss: 0.035164. Entropy: 0.283161.\n",
      "episode: 3315   score: 325.0  epsilon: 1.0    steps: 56  evaluation reward: 400.85\n",
      "episode: 3316   score: 285.0  epsilon: 1.0    steps: 176  evaluation reward: 400.0\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8746: Policy loss: -0.008177. Value loss: 0.080788. Entropy: 0.282245.\n",
      "Iteration 8747: Policy loss: -0.015668. Value loss: 0.042665. Entropy: 0.282083.\n",
      "Iteration 8748: Policy loss: -0.021699. Value loss: 0.034776. Entropy: 0.281698.\n",
      "episode: 3317   score: 320.0  epsilon: 1.0    steps: 96  evaluation reward: 399.55\n",
      "Training network. lr: 0.000183. clip: 0.073283\n",
      "Iteration 8749: Policy loss: 0.021605. Value loss: 0.242795. Entropy: 0.296966.\n",
      "Iteration 8750: Policy loss: 0.015713. Value loss: 0.085771. Entropy: 0.296398.\n",
      "Iteration 8751: Policy loss: -0.008108. Value loss: 0.054291. Entropy: 0.296161.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8752: Policy loss: 0.180005. Value loss: 0.193441. Entropy: 0.312538.\n",
      "Iteration 8753: Policy loss: 0.159448. Value loss: 0.085846. Entropy: 0.311995.\n",
      "Iteration 8754: Policy loss: 0.154077. Value loss: 0.050646. Entropy: 0.312377.\n",
      "episode: 3318   score: 520.0  epsilon: 1.0    steps: 336  evaluation reward: 397.35\n",
      "episode: 3319   score: 590.0  epsilon: 1.0    steps: 536  evaluation reward: 395.95\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8755: Policy loss: 0.051503. Value loss: 0.246779. Entropy: 0.282338.\n",
      "Iteration 8756: Policy loss: 0.065443. Value loss: 0.098360. Entropy: 0.281885.\n",
      "Iteration 8757: Policy loss: 0.039114. Value loss: 0.068890. Entropy: 0.280917.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8758: Policy loss: 0.042893. Value loss: 0.216789. Entropy: 0.309005.\n",
      "Iteration 8759: Policy loss: 0.030504. Value loss: 0.076418. Entropy: 0.310026.\n",
      "Iteration 8760: Policy loss: 0.033642. Value loss: 0.041365. Entropy: 0.309650.\n",
      "episode: 3320   score: 635.0  epsilon: 1.0    steps: 432  evaluation reward: 396.15\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8761: Policy loss: 0.256057. Value loss: 0.115460. Entropy: 0.297048.\n",
      "Iteration 8762: Policy loss: 0.250313. Value loss: 0.047404. Entropy: 0.296214.\n",
      "Iteration 8763: Policy loss: 0.245426. Value loss: 0.036043. Entropy: 0.297059.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8764: Policy loss: 0.143107. Value loss: 0.239071. Entropy: 0.310426.\n",
      "Iteration 8765: Policy loss: 0.115352. Value loss: 0.065306. Entropy: 0.308660.\n",
      "Iteration 8766: Policy loss: 0.119270. Value loss: 0.041729. Entropy: 0.308741.\n",
      "episode: 3321   score: 435.0  epsilon: 1.0    steps: 104  evaluation reward: 397.15\n",
      "episode: 3322   score: 485.0  epsilon: 1.0    steps: 160  evaluation reward: 396.25\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8767: Policy loss: 0.256277. Value loss: 0.080954. Entropy: 0.279299.\n",
      "Iteration 8768: Policy loss: 0.240867. Value loss: 0.034437. Entropy: 0.278983.\n",
      "Iteration 8769: Policy loss: 0.244476. Value loss: 0.028215. Entropy: 0.279172.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8770: Policy loss: 0.031346. Value loss: 0.097056. Entropy: 0.309591.\n",
      "Iteration 8771: Policy loss: 0.017847. Value loss: 0.042714. Entropy: 0.308408.\n",
      "Iteration 8772: Policy loss: 0.022437. Value loss: 0.031293. Entropy: 0.308482.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8773: Policy loss: 0.159134. Value loss: 0.204514. Entropy: 0.308522.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8774: Policy loss: 0.155994. Value loss: 0.139115. Entropy: 0.308550.\n",
      "Iteration 8775: Policy loss: 0.153892. Value loss: 0.107733. Entropy: 0.307948.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8776: Policy loss: 0.194582. Value loss: 0.158297. Entropy: 0.309771.\n",
      "Iteration 8777: Policy loss: 0.187848. Value loss: 0.053700. Entropy: 0.308903.\n",
      "Iteration 8778: Policy loss: 0.173247. Value loss: 0.033671. Entropy: 0.309861.\n",
      "episode: 3323   score: 680.0  epsilon: 1.0    steps: 488  evaluation reward: 400.5\n",
      "episode: 3324   score: 650.0  epsilon: 1.0    steps: 576  evaluation reward: 401.3\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8779: Policy loss: 0.196234. Value loss: 0.090946. Entropy: 0.283164.\n",
      "Iteration 8780: Policy loss: 0.183493. Value loss: 0.037459. Entropy: 0.282880.\n",
      "Iteration 8781: Policy loss: 0.182057. Value loss: 0.026068. Entropy: 0.281773.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8782: Policy loss: 0.097375. Value loss: 0.136781. Entropy: 0.307064.\n",
      "Iteration 8783: Policy loss: 0.100504. Value loss: 0.049004. Entropy: 0.305220.\n",
      "Iteration 8784: Policy loss: 0.093744. Value loss: 0.033165. Entropy: 0.305963.\n",
      "episode: 3325   score: 450.0  epsilon: 1.0    steps: 64  evaluation reward: 398.45\n",
      "episode: 3326   score: 270.0  epsilon: 1.0    steps: 648  evaluation reward: 396.2\n",
      "episode: 3327   score: 405.0  epsilon: 1.0    steps: 800  evaluation reward: 395.55\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8785: Policy loss: 0.125448. Value loss: 0.144056. Entropy: 0.273520.\n",
      "Iteration 8786: Policy loss: 0.109755. Value loss: 0.054230. Entropy: 0.273233.\n",
      "Iteration 8787: Policy loss: 0.094869. Value loss: 0.043562. Entropy: 0.273542.\n",
      "episode: 3328   score: 420.0  epsilon: 1.0    steps: 280  evaluation reward: 396.85\n",
      "episode: 3329   score: 415.0  epsilon: 1.0    steps: 792  evaluation reward: 394.8\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8788: Policy loss: 0.135655. Value loss: 0.110197. Entropy: 0.280109.\n",
      "Iteration 8789: Policy loss: 0.136652. Value loss: 0.041195. Entropy: 0.279611.\n",
      "Iteration 8790: Policy loss: 0.127119. Value loss: 0.030815. Entropy: 0.279147.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8791: Policy loss: -0.192365. Value loss: 0.184486. Entropy: 0.309192.\n",
      "Iteration 8792: Policy loss: -0.195080. Value loss: 0.142710. Entropy: 0.309947.\n",
      "Iteration 8793: Policy loss: -0.203075. Value loss: 0.122839. Entropy: 0.309447.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8794: Policy loss: 0.121215. Value loss: 0.318114. Entropy: 0.310559.\n",
      "Iteration 8795: Policy loss: 0.092879. Value loss: 0.102108. Entropy: 0.310194.\n",
      "Iteration 8796: Policy loss: 0.091421. Value loss: 0.051757. Entropy: 0.308235.\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8797: Policy loss: 0.199168. Value loss: 0.138822. Entropy: 0.306276.\n",
      "Iteration 8798: Policy loss: 0.190725. Value loss: 0.043991. Entropy: 0.305390.\n",
      "Iteration 8799: Policy loss: 0.176583. Value loss: 0.031464. Entropy: 0.305208.\n",
      "episode: 3330   score: 445.0  epsilon: 1.0    steps: 40  evaluation reward: 394.55\n",
      "episode: 3331   score: 465.0  epsilon: 1.0    steps: 1000  evaluation reward: 393.0\n",
      "Training network. lr: 0.000183. clip: 0.073126\n",
      "Iteration 8800: Policy loss: 0.019248. Value loss: 0.087465. Entropy: 0.297907.\n",
      "Iteration 8801: Policy loss: 0.008513. Value loss: 0.035955. Entropy: 0.298106.\n",
      "Iteration 8802: Policy loss: 0.011773. Value loss: 0.026823. Entropy: 0.296352.\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8803: Policy loss: -0.343828. Value loss: 0.273370. Entropy: 0.297856.\n",
      "Iteration 8804: Policy loss: -0.328424. Value loss: 0.101485. Entropy: 0.299126.\n",
      "Iteration 8805: Policy loss: -0.356197. Value loss: 0.050847. Entropy: 0.300574.\n",
      "episode: 3332   score: 210.0  epsilon: 1.0    steps: 160  evaluation reward: 393.0\n",
      "episode: 3333   score: 440.0  epsilon: 1.0    steps: 872  evaluation reward: 392.1\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8806: Policy loss: -0.019315. Value loss: 0.090209. Entropy: 0.289827.\n",
      "Iteration 8807: Policy loss: -0.032929. Value loss: 0.030430. Entropy: 0.289520.\n",
      "Iteration 8808: Policy loss: -0.038610. Value loss: 0.022287. Entropy: 0.289376.\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8809: Policy loss: -0.103293. Value loss: 0.228742. Entropy: 0.301662.\n",
      "Iteration 8810: Policy loss: -0.138198. Value loss: 0.086342. Entropy: 0.302955.\n",
      "Iteration 8811: Policy loss: -0.160502. Value loss: 0.059814. Entropy: 0.302265.\n",
      "episode: 3334   score: 390.0  epsilon: 1.0    steps: 288  evaluation reward: 391.5\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8812: Policy loss: -0.116142. Value loss: 0.082449. Entropy: 0.294474.\n",
      "Iteration 8813: Policy loss: -0.127309. Value loss: 0.039823. Entropy: 0.294359.\n",
      "Iteration 8814: Policy loss: -0.134881. Value loss: 0.024465. Entropy: 0.294198.\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8815: Policy loss: -0.278367. Value loss: 0.324766. Entropy: 0.307021.\n",
      "Iteration 8816: Policy loss: -0.295015. Value loss: 0.164944. Entropy: 0.308039.\n",
      "Iteration 8817: Policy loss: -0.307179. Value loss: 0.103008. Entropy: 0.308011.\n",
      "episode: 3335   score: 605.0  epsilon: 1.0    steps: 720  evaluation reward: 395.3\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8818: Policy loss: -0.139483. Value loss: 0.405112. Entropy: 0.294606.\n",
      "Iteration 8819: Policy loss: -0.171583. Value loss: 0.274906. Entropy: 0.294167.\n",
      "Iteration 8820: Policy loss: -0.176989. Value loss: 0.158544. Entropy: 0.293544.\n",
      "episode: 3336   score: 360.0  epsilon: 1.0    steps: 784  evaluation reward: 394.95\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8821: Policy loss: 0.144788. Value loss: 0.153817. Entropy: 0.303439.\n",
      "Iteration 8822: Policy loss: 0.128855. Value loss: 0.072772. Entropy: 0.302648.\n",
      "Iteration 8823: Policy loss: 0.125772. Value loss: 0.051589. Entropy: 0.302677.\n",
      "episode: 3337   score: 580.0  epsilon: 1.0    steps: 992  evaluation reward: 396.8\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8824: Policy loss: 0.014569. Value loss: 0.097013. Entropy: 0.302073.\n",
      "Iteration 8825: Policy loss: 0.012237. Value loss: 0.050562. Entropy: 0.302469.\n",
      "Iteration 8826: Policy loss: 0.008087. Value loss: 0.039838. Entropy: 0.302620.\n",
      "episode: 3338   score: 365.0  epsilon: 1.0    steps: 80  evaluation reward: 398.05\n",
      "episode: 3339   score: 320.0  epsilon: 1.0    steps: 672  evaluation reward: 397.8\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8827: Policy loss: 0.022918. Value loss: 0.093985. Entropy: 0.273848.\n",
      "Iteration 8828: Policy loss: 0.025397. Value loss: 0.040372. Entropy: 0.272759.\n",
      "Iteration 8829: Policy loss: 0.021925. Value loss: 0.029366. Entropy: 0.273608.\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8830: Policy loss: -0.210698. Value loss: 0.245108. Entropy: 0.309875.\n",
      "Iteration 8831: Policy loss: -0.218337. Value loss: 0.078841. Entropy: 0.307708.\n",
      "Iteration 8832: Policy loss: -0.223018. Value loss: 0.050288. Entropy: 0.311021.\n",
      "episode: 3340   score: 980.0  epsilon: 1.0    steps: 384  evaluation reward: 402.8\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8833: Policy loss: -0.033140. Value loss: 0.241444. Entropy: 0.294505.\n",
      "Iteration 8834: Policy loss: -0.037263. Value loss: 0.083532. Entropy: 0.293561.\n",
      "Iteration 8835: Policy loss: -0.055050. Value loss: 0.052792. Entropy: 0.293912.\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8836: Policy loss: 0.250513. Value loss: 0.166228. Entropy: 0.310290.\n",
      "Iteration 8837: Policy loss: 0.237468. Value loss: 0.039731. Entropy: 0.308824.\n",
      "Iteration 8838: Policy loss: 0.222151. Value loss: 0.024728. Entropy: 0.308994.\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8839: Policy loss: -0.101857. Value loss: 0.156905. Entropy: 0.308516.\n",
      "Iteration 8840: Policy loss: -0.091918. Value loss: 0.059119. Entropy: 0.308285.\n",
      "Iteration 8841: Policy loss: -0.111018. Value loss: 0.039914. Entropy: 0.307453.\n",
      "episode: 3341   score: 650.0  epsilon: 1.0    steps: 248  evaluation reward: 403.1\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8842: Policy loss: 0.732330. Value loss: 0.225614. Entropy: 0.293452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8843: Policy loss: 0.708038. Value loss: 0.062340. Entropy: 0.292227.\n",
      "Iteration 8844: Policy loss: 0.702539. Value loss: 0.040703. Entropy: 0.291329.\n",
      "episode: 3342   score: 605.0  epsilon: 1.0    steps: 928  evaluation reward: 407.05\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8845: Policy loss: -0.039755. Value loss: 0.178662. Entropy: 0.306557.\n",
      "Iteration 8846: Policy loss: -0.042503. Value loss: 0.077675. Entropy: 0.306480.\n",
      "Iteration 8847: Policy loss: -0.054366. Value loss: 0.054618. Entropy: 0.306741.\n",
      "episode: 3343   score: 240.0  epsilon: 1.0    steps: 152  evaluation reward: 406.3\n",
      "episode: 3344   score: 525.0  epsilon: 1.0    steps: 392  evaluation reward: 408.7\n",
      "Training network. lr: 0.000182. clip: 0.072969\n",
      "Iteration 8848: Policy loss: 0.082882. Value loss: 0.115004. Entropy: 0.274775.\n",
      "Iteration 8849: Policy loss: 0.082660. Value loss: 0.034898. Entropy: 0.274215.\n",
      "Iteration 8850: Policy loss: 0.076410. Value loss: 0.027609. Entropy: 0.274256.\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8851: Policy loss: 0.059391. Value loss: 0.075764. Entropy: 0.312768.\n",
      "Iteration 8852: Policy loss: 0.052153. Value loss: 0.034733. Entropy: 0.310889.\n",
      "Iteration 8853: Policy loss: 0.053340. Value loss: 0.027362. Entropy: 0.311417.\n",
      "episode: 3345   score: 780.0  epsilon: 1.0    steps: 872  evaluation reward: 413.05\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8854: Policy loss: -0.044467. Value loss: 0.066917. Entropy: 0.304193.\n",
      "Iteration 8855: Policy loss: -0.051746. Value loss: 0.037076. Entropy: 0.303555.\n",
      "Iteration 8856: Policy loss: -0.059487. Value loss: 0.027557. Entropy: 0.305014.\n",
      "episode: 3346   score: 420.0  epsilon: 1.0    steps: 520  evaluation reward: 414.55\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8857: Policy loss: -0.095704. Value loss: 0.386632. Entropy: 0.292069.\n",
      "Iteration 8858: Policy loss: -0.114807. Value loss: 0.182391. Entropy: 0.291643.\n",
      "Iteration 8859: Policy loss: -0.115771. Value loss: 0.120119. Entropy: 0.290677.\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8860: Policy loss: 0.255443. Value loss: 0.108721. Entropy: 0.304564.\n",
      "Iteration 8861: Policy loss: 0.255339. Value loss: 0.033341. Entropy: 0.304213.\n",
      "Iteration 8862: Policy loss: 0.237270. Value loss: 0.023607. Entropy: 0.304649.\n",
      "episode: 3347   score: 460.0  epsilon: 1.0    steps: 960  evaluation reward: 413.4\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8863: Policy loss: 0.179335. Value loss: 0.105189. Entropy: 0.304257.\n",
      "Iteration 8864: Policy loss: 0.171755. Value loss: 0.046531. Entropy: 0.303671.\n",
      "Iteration 8865: Policy loss: 0.162999. Value loss: 0.029806. Entropy: 0.304150.\n",
      "episode: 3348   score: 435.0  epsilon: 1.0    steps: 456  evaluation reward: 415.15\n",
      "episode: 3349   score: 240.0  epsilon: 1.0    steps: 720  evaluation reward: 413.9\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8866: Policy loss: 0.220491. Value loss: 0.121171. Entropy: 0.267723.\n",
      "Iteration 8867: Policy loss: 0.213860. Value loss: 0.064693. Entropy: 0.267042.\n",
      "Iteration 8868: Policy loss: 0.216456. Value loss: 0.047039. Entropy: 0.267059.\n",
      "episode: 3350   score: 500.0  epsilon: 1.0    steps: 496  evaluation reward: 417.9\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8869: Policy loss: -0.156444. Value loss: 0.225431. Entropy: 0.293262.\n",
      "Iteration 8870: Policy loss: -0.156739. Value loss: 0.098672. Entropy: 0.293371.\n",
      "Iteration 8871: Policy loss: -0.166717. Value loss: 0.060506. Entropy: 0.294129.\n",
      "now time :  2019-09-05 23:25:27.306308\n",
      "episode: 3351   score: 810.0  epsilon: 1.0    steps: 904  evaluation reward: 423.1\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8872: Policy loss: -0.193333. Value loss: 0.248303. Entropy: 0.308793.\n",
      "Iteration 8873: Policy loss: -0.191156. Value loss: 0.075501. Entropy: 0.309077.\n",
      "Iteration 8874: Policy loss: -0.206110. Value loss: 0.046623. Entropy: 0.309432.\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8875: Policy loss: -0.100693. Value loss: 0.120942. Entropy: 0.304878.\n",
      "Iteration 8876: Policy loss: -0.107827. Value loss: 0.034181. Entropy: 0.305030.\n",
      "Iteration 8877: Policy loss: -0.113259. Value loss: 0.021168. Entropy: 0.305574.\n",
      "episode: 3352   score: 395.0  epsilon: 1.0    steps: 728  evaluation reward: 424.2\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8878: Policy loss: 0.220155. Value loss: 0.424352. Entropy: 0.297248.\n",
      "Iteration 8879: Policy loss: 0.210421. Value loss: 0.121604. Entropy: 0.296792.\n",
      "Iteration 8880: Policy loss: 0.166150. Value loss: 0.088670. Entropy: 0.297937.\n",
      "episode: 3353   score: 550.0  epsilon: 1.0    steps: 712  evaluation reward: 426.95\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8881: Policy loss: 0.022957. Value loss: 0.072579. Entropy: 0.300917.\n",
      "Iteration 8882: Policy loss: 0.016454. Value loss: 0.044286. Entropy: 0.301947.\n",
      "Iteration 8883: Policy loss: 0.021913. Value loss: 0.033933. Entropy: 0.301556.\n",
      "episode: 3354   score: 285.0  epsilon: 1.0    steps: 872  evaluation reward: 428.3\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8884: Policy loss: 0.075832. Value loss: 0.065502. Entropy: 0.306450.\n",
      "Iteration 8885: Policy loss: 0.071842. Value loss: 0.033964. Entropy: 0.306437.\n",
      "Iteration 8886: Policy loss: 0.071402. Value loss: 0.026301. Entropy: 0.306714.\n",
      "episode: 3355   score: 315.0  epsilon: 1.0    steps: 976  evaluation reward: 428.35\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8887: Policy loss: -0.007696. Value loss: 0.135422. Entropy: 0.309508.\n",
      "Iteration 8888: Policy loss: -0.017301. Value loss: 0.058158. Entropy: 0.307782.\n",
      "Iteration 8889: Policy loss: -0.034794. Value loss: 0.044282. Entropy: 0.307857.\n",
      "episode: 3356   score: 840.0  epsilon: 1.0    steps: 32  evaluation reward: 435.2\n",
      "episode: 3357   score: 240.0  epsilon: 1.0    steps: 80  evaluation reward: 428.85\n",
      "episode: 3358   score: 210.0  epsilon: 1.0    steps: 224  evaluation reward: 428.1\n",
      "episode: 3359   score: 210.0  epsilon: 1.0    steps: 656  evaluation reward: 426.1\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8890: Policy loss: -0.009913. Value loss: 0.323533. Entropy: 0.248447.\n",
      "Iteration 8891: Policy loss: -0.034114. Value loss: 0.127970. Entropy: 0.248897.\n",
      "Iteration 8892: Policy loss: -0.040003. Value loss: 0.091227. Entropy: 0.247967.\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8893: Policy loss: -0.092390. Value loss: 0.127750. Entropy: 0.310274.\n",
      "Iteration 8894: Policy loss: -0.104480. Value loss: 0.079071. Entropy: 0.309900.\n",
      "Iteration 8895: Policy loss: -0.107273. Value loss: 0.067146. Entropy: 0.309751.\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8896: Policy loss: -0.040446. Value loss: 0.182161. Entropy: 0.306355.\n",
      "Iteration 8897: Policy loss: -0.056272. Value loss: 0.086458. Entropy: 0.307321.\n",
      "Iteration 8898: Policy loss: -0.057244. Value loss: 0.059714. Entropy: 0.305404.\n",
      "episode: 3360   score: 420.0  epsilon: 1.0    steps: 280  evaluation reward: 426.65\n",
      "episode: 3361   score: 210.0  epsilon: 1.0    steps: 832  evaluation reward: 426.2\n",
      "Training network. lr: 0.000182. clip: 0.072822\n",
      "Iteration 8899: Policy loss: 0.244002. Value loss: 0.111520. Entropy: 0.290254.\n",
      "Iteration 8900: Policy loss: 0.245771. Value loss: 0.049431. Entropy: 0.289643.\n",
      "Iteration 8901: Policy loss: 0.229723. Value loss: 0.028598. Entropy: 0.290162.\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8902: Policy loss: -0.152132. Value loss: 0.109739. Entropy: 0.306474.\n",
      "Iteration 8903: Policy loss: -0.150353. Value loss: 0.036275. Entropy: 0.307243.\n",
      "Iteration 8904: Policy loss: -0.160182. Value loss: 0.027236. Entropy: 0.306946.\n",
      "episode: 3362   score: 565.0  epsilon: 1.0    steps: 392  evaluation reward: 428.4\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8905: Policy loss: 0.008254. Value loss: 0.120996. Entropy: 0.301626.\n",
      "Iteration 8906: Policy loss: 0.007403. Value loss: 0.047809. Entropy: 0.302755.\n",
      "Iteration 8907: Policy loss: -0.009147. Value loss: 0.031619. Entropy: 0.302678.\n",
      "episode: 3363   score: 215.0  epsilon: 1.0    steps: 128  evaluation reward: 428.75\n",
      "episode: 3364   score: 215.0  epsilon: 1.0    steps: 200  evaluation reward: 428.0\n",
      "episode: 3365   score: 435.0  epsilon: 1.0    steps: 520  evaluation reward: 429.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3366   score: 255.0  epsilon: 1.0    steps: 656  evaluation reward: 429.5\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8908: Policy loss: 0.128562. Value loss: 0.121771. Entropy: 0.295378.\n",
      "Iteration 8909: Policy loss: 0.133038. Value loss: 0.057314. Entropy: 0.294620.\n",
      "Iteration 8910: Policy loss: 0.118852. Value loss: 0.045557. Entropy: 0.297706.\n",
      "episode: 3367   score: 150.0  epsilon: 1.0    steps: 104  evaluation reward: 428.4\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8911: Policy loss: 0.134449. Value loss: 0.113982. Entropy: 0.308757.\n",
      "Iteration 8912: Policy loss: 0.125505. Value loss: 0.057828. Entropy: 0.309134.\n",
      "Iteration 8913: Policy loss: 0.124272. Value loss: 0.042007. Entropy: 0.308354.\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8914: Policy loss: 0.031610. Value loss: 0.056253. Entropy: 0.311280.\n",
      "Iteration 8915: Policy loss: 0.026415. Value loss: 0.027817. Entropy: 0.311796.\n",
      "Iteration 8916: Policy loss: 0.025104. Value loss: 0.022184. Entropy: 0.311544.\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8917: Policy loss: -0.323710. Value loss: 0.211788. Entropy: 0.308140.\n",
      "Iteration 8918: Policy loss: -0.340431. Value loss: 0.066601. Entropy: 0.308657.\n",
      "Iteration 8919: Policy loss: -0.360733. Value loss: 0.047801. Entropy: 0.308866.\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8920: Policy loss: 0.237914. Value loss: 0.103639. Entropy: 0.309970.\n",
      "Iteration 8921: Policy loss: 0.221375. Value loss: 0.040568. Entropy: 0.309607.\n",
      "Iteration 8922: Policy loss: 0.217658. Value loss: 0.027829. Entropy: 0.309679.\n",
      "episode: 3368   score: 435.0  epsilon: 1.0    steps: 288  evaluation reward: 427.55\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8923: Policy loss: 0.150234. Value loss: 0.113948. Entropy: 0.303886.\n",
      "Iteration 8924: Policy loss: 0.142647. Value loss: 0.052583. Entropy: 0.300514.\n",
      "Iteration 8925: Policy loss: 0.139591. Value loss: 0.037357. Entropy: 0.302634.\n",
      "episode: 3369   score: 225.0  epsilon: 1.0    steps: 40  evaluation reward: 425.6\n",
      "episode: 3370   score: 275.0  epsilon: 1.0    steps: 344  evaluation reward: 423.0\n",
      "episode: 3371   score: 255.0  epsilon: 1.0    steps: 728  evaluation reward: 423.45\n",
      "episode: 3372   score: 215.0  epsilon: 1.0    steps: 768  evaluation reward: 423.5\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8926: Policy loss: 0.147827. Value loss: 0.116711. Entropy: 0.270320.\n",
      "Iteration 8927: Policy loss: 0.144428. Value loss: 0.059224. Entropy: 0.269416.\n",
      "Iteration 8928: Policy loss: 0.136354. Value loss: 0.041115. Entropy: 0.265556.\n",
      "episode: 3373   score: 440.0  epsilon: 1.0    steps: 856  evaluation reward: 425.05\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8929: Policy loss: 0.067990. Value loss: 0.126239. Entropy: 0.305305.\n",
      "Iteration 8930: Policy loss: 0.058364. Value loss: 0.053158. Entropy: 0.306030.\n",
      "Iteration 8931: Policy loss: 0.052879. Value loss: 0.039132. Entropy: 0.305183.\n",
      "episode: 3374   score: 450.0  epsilon: 1.0    steps: 576  evaluation reward: 426.15\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8932: Policy loss: 0.168437. Value loss: 0.133529. Entropy: 0.289247.\n",
      "Iteration 8933: Policy loss: 0.163286. Value loss: 0.047068. Entropy: 0.288747.\n",
      "Iteration 8934: Policy loss: 0.163801. Value loss: 0.027344. Entropy: 0.287457.\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8935: Policy loss: 0.104017. Value loss: 0.091917. Entropy: 0.313464.\n",
      "Iteration 8936: Policy loss: 0.103664. Value loss: 0.037139. Entropy: 0.313268.\n",
      "Iteration 8937: Policy loss: 0.105059. Value loss: 0.024723. Entropy: 0.312912.\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8938: Policy loss: -0.191647. Value loss: 0.381421. Entropy: 0.311269.\n",
      "Iteration 8939: Policy loss: -0.181855. Value loss: 0.238767. Entropy: 0.311847.\n",
      "Iteration 8940: Policy loss: -0.227066. Value loss: 0.191459. Entropy: 0.311071.\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8941: Policy loss: 0.073351. Value loss: 0.100213. Entropy: 0.309392.\n",
      "Iteration 8942: Policy loss: 0.069535. Value loss: 0.037925. Entropy: 0.309871.\n",
      "Iteration 8943: Policy loss: 0.063868. Value loss: 0.024839. Entropy: 0.309499.\n",
      "episode: 3375   score: 215.0  epsilon: 1.0    steps: 56  evaluation reward: 424.55\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8944: Policy loss: 0.033127. Value loss: 0.120876. Entropy: 0.301770.\n",
      "Iteration 8945: Policy loss: 0.023439. Value loss: 0.055647. Entropy: 0.300752.\n",
      "Iteration 8946: Policy loss: 0.016107. Value loss: 0.041026. Entropy: 0.300945.\n",
      "episode: 3376   score: 330.0  epsilon: 1.0    steps: 192  evaluation reward: 424.7\n",
      "episode: 3377   score: 335.0  epsilon: 1.0    steps: 440  evaluation reward: 425.45\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8947: Policy loss: -0.089444. Value loss: 0.104853. Entropy: 0.288489.\n",
      "Iteration 8948: Policy loss: -0.091029. Value loss: 0.048528. Entropy: 0.286822.\n",
      "Iteration 8949: Policy loss: -0.093358. Value loss: 0.036482. Entropy: 0.287860.\n",
      "episode: 3378   score: 400.0  epsilon: 1.0    steps: 184  evaluation reward: 427.65\n",
      "episode: 3379   score: 540.0  epsilon: 1.0    steps: 592  evaluation reward: 428.85\n",
      "Training network. lr: 0.000182. clip: 0.072665\n",
      "Iteration 8950: Policy loss: 0.166567. Value loss: 0.087737. Entropy: 0.284399.\n",
      "Iteration 8951: Policy loss: 0.155026. Value loss: 0.037642. Entropy: 0.283193.\n",
      "Iteration 8952: Policy loss: 0.153215. Value loss: 0.031284. Entropy: 0.284663.\n",
      "episode: 3380   score: 490.0  epsilon: 1.0    steps: 224  evaluation reward: 431.15\n",
      "episode: 3381   score: 365.0  epsilon: 1.0    steps: 544  evaluation reward: 431.5\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8953: Policy loss: 0.162505. Value loss: 0.102073. Entropy: 0.282134.\n",
      "Iteration 8954: Policy loss: 0.150259. Value loss: 0.041921. Entropy: 0.282632.\n",
      "Iteration 8955: Policy loss: 0.152574. Value loss: 0.032098. Entropy: 0.279465.\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8956: Policy loss: -0.298594. Value loss: 0.299467. Entropy: 0.309671.\n",
      "Iteration 8957: Policy loss: -0.307241. Value loss: 0.210012. Entropy: 0.308491.\n",
      "Iteration 8958: Policy loss: -0.313826. Value loss: 0.148895. Entropy: 0.307636.\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8959: Policy loss: -0.057214. Value loss: 0.116750. Entropy: 0.311201.\n",
      "Iteration 8960: Policy loss: -0.053783. Value loss: 0.049333. Entropy: 0.312410.\n",
      "Iteration 8961: Policy loss: -0.060918. Value loss: 0.034478. Entropy: 0.310212.\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8962: Policy loss: -0.000954. Value loss: 0.156392. Entropy: 0.307895.\n",
      "Iteration 8963: Policy loss: -0.006728. Value loss: 0.049561. Entropy: 0.309101.\n",
      "Iteration 8964: Policy loss: -0.002995. Value loss: 0.033820. Entropy: 0.308737.\n",
      "episode: 3382   score: 145.0  epsilon: 1.0    steps: 40  evaluation reward: 429.65\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8965: Policy loss: -0.353288. Value loss: 0.177905. Entropy: 0.302897.\n",
      "Iteration 8966: Policy loss: -0.369817. Value loss: 0.065266. Entropy: 0.302238.\n",
      "Iteration 8967: Policy loss: -0.370227. Value loss: 0.049421. Entropy: 0.302443.\n",
      "episode: 3383   score: 430.0  epsilon: 1.0    steps: 64  evaluation reward: 430.8\n",
      "episode: 3384   score: 525.0  epsilon: 1.0    steps: 200  evaluation reward: 433.2\n",
      "episode: 3385   score: 265.0  epsilon: 1.0    steps: 520  evaluation reward: 431.65\n",
      "episode: 3386   score: 560.0  epsilon: 1.0    steps: 856  evaluation reward: 433.3\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8968: Policy loss: -0.244543. Value loss: 0.346073. Entropy: 0.266268.\n",
      "Iteration 8969: Policy loss: -0.269907. Value loss: 0.284545. Entropy: 0.266139.\n",
      "Iteration 8970: Policy loss: -0.275166. Value loss: 0.244503. Entropy: 0.264393.\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8971: Policy loss: -0.034526. Value loss: 0.096626. Entropy: 0.305386.\n",
      "Iteration 8972: Policy loss: -0.041503. Value loss: 0.048609. Entropy: 0.305316.\n",
      "Iteration 8973: Policy loss: -0.046412. Value loss: 0.035245. Entropy: 0.304961.\n",
      "episode: 3387   score: 540.0  epsilon: 1.0    steps: 664  evaluation reward: 436.1\n",
      "Training network. lr: 0.000181. clip: 0.072509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8974: Policy loss: -0.026612. Value loss: 0.367270. Entropy: 0.295769.\n",
      "Iteration 8975: Policy loss: -0.027890. Value loss: 0.184733. Entropy: 0.298006.\n",
      "Iteration 8976: Policy loss: -0.034087. Value loss: 0.053523. Entropy: 0.294430.\n",
      "episode: 3388   score: 120.0  epsilon: 1.0    steps: 944  evaluation reward: 433.65\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8977: Policy loss: 0.236921. Value loss: 0.088097. Entropy: 0.313224.\n",
      "Iteration 8978: Policy loss: 0.233802. Value loss: 0.032490. Entropy: 0.311851.\n",
      "Iteration 8979: Policy loss: 0.233211. Value loss: 0.024596. Entropy: 0.311761.\n",
      "episode: 3389   score: 210.0  epsilon: 1.0    steps: 104  evaluation reward: 432.1\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8980: Policy loss: -0.211281. Value loss: 0.106133. Entropy: 0.287294.\n",
      "Iteration 8981: Policy loss: -0.224252. Value loss: 0.057001. Entropy: 0.287787.\n",
      "Iteration 8982: Policy loss: -0.219289. Value loss: 0.040671. Entropy: 0.288460.\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8983: Policy loss: 0.002020. Value loss: 0.109389. Entropy: 0.312037.\n",
      "Iteration 8984: Policy loss: -0.009828. Value loss: 0.044151. Entropy: 0.311327.\n",
      "Iteration 8985: Policy loss: -0.012734. Value loss: 0.034187. Entropy: 0.310261.\n",
      "episode: 3390   score: 320.0  epsilon: 1.0    steps: 144  evaluation reward: 431.1\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8986: Policy loss: 0.141337. Value loss: 0.168960. Entropy: 0.297222.\n",
      "Iteration 8987: Policy loss: 0.142419. Value loss: 0.073078. Entropy: 0.296090.\n",
      "Iteration 8988: Policy loss: 0.127673. Value loss: 0.050827. Entropy: 0.295773.\n",
      "episode: 3391   score: 570.0  epsilon: 1.0    steps: 80  evaluation reward: 432.95\n",
      "episode: 3392   score: 685.0  epsilon: 1.0    steps: 560  evaluation reward: 435.15\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8989: Policy loss: 0.043275. Value loss: 0.063068. Entropy: 0.285469.\n",
      "Iteration 8990: Policy loss: 0.030289. Value loss: 0.035230. Entropy: 0.284022.\n",
      "Iteration 8991: Policy loss: 0.029283. Value loss: 0.027008. Entropy: 0.284163.\n",
      "episode: 3393   score: 450.0  epsilon: 1.0    steps: 360  evaluation reward: 431.65\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8992: Policy loss: 0.085222. Value loss: 0.058157. Entropy: 0.303227.\n",
      "Iteration 8993: Policy loss: 0.075302. Value loss: 0.032752. Entropy: 0.303115.\n",
      "Iteration 8994: Policy loss: 0.077247. Value loss: 0.025664. Entropy: 0.304047.\n",
      "episode: 3394   score: 290.0  epsilon: 1.0    steps: 496  evaluation reward: 428.35\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8995: Policy loss: -0.032794. Value loss: 0.091573. Entropy: 0.302180.\n",
      "Iteration 8996: Policy loss: -0.043312. Value loss: 0.036877. Entropy: 0.303103.\n",
      "Iteration 8997: Policy loss: -0.047807. Value loss: 0.024830. Entropy: 0.302690.\n",
      "episode: 3395   score: 265.0  epsilon: 1.0    steps: 360  evaluation reward: 426.65\n",
      "Training network. lr: 0.000181. clip: 0.072509\n",
      "Iteration 8998: Policy loss: -0.214647. Value loss: 0.208502. Entropy: 0.300625.\n",
      "Iteration 8999: Policy loss: -0.205510. Value loss: 0.082573. Entropy: 0.298408.\n",
      "Iteration 9000: Policy loss: -0.225026. Value loss: 0.050537. Entropy: 0.300666.\n",
      "episode: 3396   score: 370.0  epsilon: 1.0    steps: 648  evaluation reward: 426.25\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9001: Policy loss: -0.052000. Value loss: 0.079788. Entropy: 0.297016.\n",
      "Iteration 9002: Policy loss: -0.064592. Value loss: 0.034246. Entropy: 0.298475.\n",
      "Iteration 9003: Policy loss: -0.062673. Value loss: 0.025882. Entropy: 0.296533.\n",
      "episode: 3397   score: 620.0  epsilon: 1.0    steps: 512  evaluation reward: 426.25\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9004: Policy loss: -0.302730. Value loss: 0.256844. Entropy: 0.296682.\n",
      "Iteration 9005: Policy loss: -0.311769. Value loss: 0.078698. Entropy: 0.296078.\n",
      "Iteration 9006: Policy loss: -0.312630. Value loss: 0.041954. Entropy: 0.296592.\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9007: Policy loss: -0.089223. Value loss: 0.140619. Entropy: 0.308119.\n",
      "Iteration 9008: Policy loss: -0.096670. Value loss: 0.063932. Entropy: 0.308920.\n",
      "Iteration 9009: Policy loss: -0.100930. Value loss: 0.042720. Entropy: 0.308141.\n",
      "episode: 3398   score: 390.0  epsilon: 1.0    steps: 64  evaluation reward: 425.2\n",
      "episode: 3399   score: 305.0  epsilon: 1.0    steps: 256  evaluation reward: 425.25\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9010: Policy loss: 0.050663. Value loss: 0.094828. Entropy: 0.290369.\n",
      "Iteration 9011: Policy loss: 0.042332. Value loss: 0.043794. Entropy: 0.286065.\n",
      "Iteration 9012: Policy loss: 0.038008. Value loss: 0.034074. Entropy: 0.285636.\n",
      "episode: 3400   score: 315.0  epsilon: 1.0    steps: 184  evaluation reward: 422.2\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9013: Policy loss: 0.008125. Value loss: 0.172680. Entropy: 0.295764.\n",
      "Iteration 9014: Policy loss: -0.005212. Value loss: 0.049734. Entropy: 0.296629.\n",
      "Iteration 9015: Policy loss: -0.011372. Value loss: 0.030189. Entropy: 0.295242.\n",
      "now time :  2019-09-05 23:34:21.973175\n",
      "episode: 3401   score: 345.0  epsilon: 1.0    steps: 208  evaluation reward: 423.0\n",
      "episode: 3402   score: 410.0  epsilon: 1.0    steps: 936  evaluation reward: 421.55\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9016: Policy loss: 0.160455. Value loss: 0.071894. Entropy: 0.294802.\n",
      "Iteration 9017: Policy loss: 0.148962. Value loss: 0.034839. Entropy: 0.294361.\n",
      "Iteration 9018: Policy loss: 0.150389. Value loss: 0.025421. Entropy: 0.293158.\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9019: Policy loss: 0.092176. Value loss: 0.173555. Entropy: 0.304514.\n",
      "Iteration 9020: Policy loss: 0.082565. Value loss: 0.055744. Entropy: 0.303392.\n",
      "Iteration 9021: Policy loss: 0.071663. Value loss: 0.029745. Entropy: 0.303282.\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9022: Policy loss: 0.288441. Value loss: 0.127402. Entropy: 0.309560.\n",
      "Iteration 9023: Policy loss: 0.274877. Value loss: 0.046907. Entropy: 0.307475.\n",
      "Iteration 9024: Policy loss: 0.269638. Value loss: 0.027095. Entropy: 0.306959.\n",
      "episode: 3403   score: 270.0  epsilon: 1.0    steps: 8  evaluation reward: 418.5\n",
      "episode: 3404   score: 590.0  epsilon: 1.0    steps: 640  evaluation reward: 418.5\n",
      "episode: 3405   score: 365.0  epsilon: 1.0    steps: 976  evaluation reward: 415.75\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9025: Policy loss: 0.175571. Value loss: 0.160765. Entropy: 0.292637.\n",
      "Iteration 9026: Policy loss: 0.172312. Value loss: 0.048889. Entropy: 0.293452.\n",
      "Iteration 9027: Policy loss: 0.170737. Value loss: 0.033855. Entropy: 0.294828.\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9028: Policy loss: 0.084442. Value loss: 0.113921. Entropy: 0.303361.\n",
      "Iteration 9029: Policy loss: 0.075447. Value loss: 0.057331. Entropy: 0.303330.\n",
      "Iteration 9030: Policy loss: 0.072651. Value loss: 0.045086. Entropy: 0.303677.\n",
      "episode: 3406   score: 260.0  epsilon: 1.0    steps: 560  evaluation reward: 415.05\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9031: Policy loss: 0.175041. Value loss: 0.153180. Entropy: 0.298899.\n",
      "Iteration 9032: Policy loss: 0.158544. Value loss: 0.064285. Entropy: 0.299184.\n",
      "Iteration 9033: Policy loss: 0.161088. Value loss: 0.050677. Entropy: 0.297206.\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9034: Policy loss: -0.123969. Value loss: 0.133925. Entropy: 0.312499.\n",
      "Iteration 9035: Policy loss: -0.128095. Value loss: 0.059572. Entropy: 0.314359.\n",
      "Iteration 9036: Policy loss: -0.131458. Value loss: 0.041475. Entropy: 0.313380.\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9037: Policy loss: -0.025367. Value loss: 0.248406. Entropy: 0.312789.\n",
      "Iteration 9038: Policy loss: -0.038985. Value loss: 0.112186. Entropy: 0.310953.\n",
      "Iteration 9039: Policy loss: -0.044598. Value loss: 0.064412. Entropy: 0.311591.\n",
      "episode: 3407   score: 310.0  epsilon: 1.0    steps: 648  evaluation reward: 415.3\n",
      "episode: 3408   score: 735.0  epsilon: 1.0    steps: 736  evaluation reward: 417.5\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9040: Policy loss: 0.350543. Value loss: 0.177660. Entropy: 0.290043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9041: Policy loss: 0.315213. Value loss: 0.049653. Entropy: 0.290747.\n",
      "Iteration 9042: Policy loss: 0.305201. Value loss: 0.034418. Entropy: 0.290148.\n",
      "episode: 3409   score: 395.0  epsilon: 1.0    steps: 408  evaluation reward: 419.9\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9043: Policy loss: 0.097858. Value loss: 0.081917. Entropy: 0.302261.\n",
      "Iteration 9044: Policy loss: 0.093058. Value loss: 0.031290. Entropy: 0.302096.\n",
      "Iteration 9045: Policy loss: 0.090953. Value loss: 0.024286. Entropy: 0.300918.\n",
      "episode: 3410   score: 320.0  epsilon: 1.0    steps: 104  evaluation reward: 419.9\n",
      "episode: 3411   score: 240.0  epsilon: 1.0    steps: 384  evaluation reward: 415.5\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9046: Policy loss: 0.022839. Value loss: 0.096332. Entropy: 0.288990.\n",
      "Iteration 9047: Policy loss: 0.016562. Value loss: 0.041964. Entropy: 0.289617.\n",
      "Iteration 9048: Policy loss: 0.010615. Value loss: 0.033623. Entropy: 0.288650.\n",
      "episode: 3412   score: 470.0  epsilon: 1.0    steps: 408  evaluation reward: 414.1\n",
      "episode: 3413   score: 235.0  epsilon: 1.0    steps: 920  evaluation reward: 411.75\n",
      "Training network. lr: 0.000181. clip: 0.072361\n",
      "Iteration 9049: Policy loss: 0.108475. Value loss: 0.085096. Entropy: 0.296760.\n",
      "Iteration 9050: Policy loss: 0.110939. Value loss: 0.043541. Entropy: 0.297035.\n",
      "Iteration 9051: Policy loss: 0.103712. Value loss: 0.031365. Entropy: 0.298039.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9052: Policy loss: -0.270513. Value loss: 0.261484. Entropy: 0.307772.\n",
      "Iteration 9053: Policy loss: -0.281966. Value loss: 0.172301. Entropy: 0.307649.\n",
      "Iteration 9054: Policy loss: -0.279070. Value loss: 0.113878. Entropy: 0.308320.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9055: Policy loss: 0.041430. Value loss: 0.099032. Entropy: 0.309888.\n",
      "Iteration 9056: Policy loss: 0.027777. Value loss: 0.029073. Entropy: 0.310094.\n",
      "Iteration 9057: Policy loss: 0.017355. Value loss: 0.017972. Entropy: 0.310141.\n",
      "episode: 3414   score: 445.0  epsilon: 1.0    steps: 640  evaluation reward: 409.55\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9058: Policy loss: 0.052669. Value loss: 0.097650. Entropy: 0.299199.\n",
      "Iteration 9059: Policy loss: 0.048207. Value loss: 0.043691. Entropy: 0.298617.\n",
      "Iteration 9060: Policy loss: 0.033520. Value loss: 0.033616. Entropy: 0.297845.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9061: Policy loss: -0.128826. Value loss: 0.060685. Entropy: 0.319295.\n",
      "Iteration 9062: Policy loss: -0.137144. Value loss: 0.025526. Entropy: 0.319519.\n",
      "Iteration 9063: Policy loss: -0.137868. Value loss: 0.018856. Entropy: 0.320364.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9064: Policy loss: 0.036346. Value loss: 0.118487. Entropy: 0.312773.\n",
      "Iteration 9065: Policy loss: 0.021399. Value loss: 0.036168. Entropy: 0.312608.\n",
      "Iteration 9066: Policy loss: 0.015464. Value loss: 0.025119. Entropy: 0.311916.\n",
      "episode: 3415   score: 285.0  epsilon: 1.0    steps: 208  evaluation reward: 409.15\n",
      "episode: 3416   score: 295.0  epsilon: 1.0    steps: 408  evaluation reward: 409.25\n",
      "episode: 3417   score: 410.0  epsilon: 1.0    steps: 744  evaluation reward: 410.15\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9067: Policy loss: -0.022243. Value loss: 0.074747. Entropy: 0.280009.\n",
      "Iteration 9068: Policy loss: -0.034874. Value loss: 0.035231. Entropy: 0.279846.\n",
      "Iteration 9069: Policy loss: -0.034911. Value loss: 0.027344. Entropy: 0.279887.\n",
      "episode: 3418   score: 360.0  epsilon: 1.0    steps: 856  evaluation reward: 408.55\n",
      "episode: 3419   score: 385.0  epsilon: 1.0    steps: 920  evaluation reward: 406.5\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9070: Policy loss: 0.173982. Value loss: 0.106429. Entropy: 0.302706.\n",
      "Iteration 9071: Policy loss: 0.169214. Value loss: 0.046767. Entropy: 0.302856.\n",
      "Iteration 9072: Policy loss: 0.163247. Value loss: 0.035308. Entropy: 0.302373.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9073: Policy loss: -0.218359. Value loss: 0.090123. Entropy: 0.304718.\n",
      "Iteration 9074: Policy loss: -0.218347. Value loss: 0.046548. Entropy: 0.303126.\n",
      "Iteration 9075: Policy loss: -0.223555. Value loss: 0.034914. Entropy: 0.305525.\n",
      "episode: 3420   score: 695.0  epsilon: 1.0    steps: 880  evaluation reward: 407.1\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9076: Policy loss: 0.239994. Value loss: 0.125376. Entropy: 0.303631.\n",
      "Iteration 9077: Policy loss: 0.231176. Value loss: 0.044826. Entropy: 0.302693.\n",
      "Iteration 9078: Policy loss: 0.213792. Value loss: 0.031146. Entropy: 0.302029.\n",
      "episode: 3421   score: 315.0  epsilon: 1.0    steps: 848  evaluation reward: 405.9\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9079: Policy loss: 0.130879. Value loss: 0.065466. Entropy: 0.305327.\n",
      "Iteration 9080: Policy loss: 0.121133. Value loss: 0.028223. Entropy: 0.306263.\n",
      "Iteration 9081: Policy loss: 0.121170. Value loss: 0.021933. Entropy: 0.305176.\n",
      "episode: 3422   score: 445.0  epsilon: 1.0    steps: 816  evaluation reward: 405.5\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9082: Policy loss: -0.098095. Value loss: 0.095187. Entropy: 0.306318.\n",
      "Iteration 9083: Policy loss: -0.095521. Value loss: 0.047586. Entropy: 0.307215.\n",
      "Iteration 9084: Policy loss: -0.105668. Value loss: 0.037824. Entropy: 0.308469.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9085: Policy loss: -0.225848. Value loss: 0.367877. Entropy: 0.308948.\n",
      "Iteration 9086: Policy loss: -0.259552. Value loss: 0.253445. Entropy: 0.311290.\n",
      "Iteration 9087: Policy loss: -0.258369. Value loss: 0.195757. Entropy: 0.312012.\n",
      "episode: 3423   score: 345.0  epsilon: 1.0    steps: 784  evaluation reward: 402.15\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9088: Policy loss: 0.058330. Value loss: 0.231204. Entropy: 0.306302.\n",
      "Iteration 9089: Policy loss: 0.050328. Value loss: 0.057597. Entropy: 0.307188.\n",
      "Iteration 9090: Policy loss: 0.035665. Value loss: 0.035923. Entropy: 0.306425.\n",
      "episode: 3424   score: 405.0  epsilon: 1.0    steps: 336  evaluation reward: 399.7\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9091: Policy loss: 0.004634. Value loss: 0.068835. Entropy: 0.296870.\n",
      "Iteration 9092: Policy loss: 0.004578. Value loss: 0.029236. Entropy: 0.296917.\n",
      "Iteration 9093: Policy loss: 0.004235. Value loss: 0.021880. Entropy: 0.297679.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9094: Policy loss: 0.030237. Value loss: 0.123947. Entropy: 0.308455.\n",
      "Iteration 9095: Policy loss: 0.014419. Value loss: 0.038933. Entropy: 0.307873.\n",
      "Iteration 9096: Policy loss: 0.005059. Value loss: 0.026867. Entropy: 0.307268.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9097: Policy loss: 0.084825. Value loss: 0.072951. Entropy: 0.317247.\n",
      "Iteration 9098: Policy loss: 0.080679. Value loss: 0.034194. Entropy: 0.316619.\n",
      "Iteration 9099: Policy loss: 0.076814. Value loss: 0.024678. Entropy: 0.316622.\n",
      "Training network. lr: 0.000181. clip: 0.072205\n",
      "Iteration 9100: Policy loss: -0.087294. Value loss: 0.297821. Entropy: 0.314713.\n",
      "Iteration 9101: Policy loss: -0.103695. Value loss: 0.138350. Entropy: 0.313860.\n",
      "Iteration 9102: Policy loss: -0.104364. Value loss: 0.071948. Entropy: 0.314613.\n",
      "episode: 3425   score: 335.0  epsilon: 1.0    steps: 560  evaluation reward: 398.55\n",
      "episode: 3426   score: 385.0  epsilon: 1.0    steps: 808  evaluation reward: 399.7\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9103: Policy loss: 0.150432. Value loss: 0.144374. Entropy: 0.298742.\n",
      "Iteration 9104: Policy loss: 0.138853. Value loss: 0.063003. Entropy: 0.298962.\n",
      "Iteration 9105: Policy loss: 0.135396. Value loss: 0.042740. Entropy: 0.298025.\n",
      "episode: 3427   score: 500.0  epsilon: 1.0    steps: 16  evaluation reward: 400.65\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9106: Policy loss: -0.067420. Value loss: 0.131515. Entropy: 0.308530.\n",
      "Iteration 9107: Policy loss: -0.076262. Value loss: 0.064950. Entropy: 0.309396.\n",
      "Iteration 9108: Policy loss: -0.079177. Value loss: 0.042257. Entropy: 0.308297.\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9109: Policy loss: 0.135902. Value loss: 0.135362. Entropy: 0.310662.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9110: Policy loss: 0.140080. Value loss: 0.064155. Entropy: 0.308713.\n",
      "Iteration 9111: Policy loss: 0.133173. Value loss: 0.045943. Entropy: 0.309421.\n",
      "episode: 3428   score: 465.0  epsilon: 1.0    steps: 280  evaluation reward: 401.1\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9112: Policy loss: -0.187614. Value loss: 0.248562. Entropy: 0.300704.\n",
      "Iteration 9113: Policy loss: -0.218142. Value loss: 0.121307. Entropy: 0.301329.\n",
      "Iteration 9114: Policy loss: -0.198360. Value loss: 0.065854. Entropy: 0.301215.\n",
      "episode: 3429   score: 425.0  epsilon: 1.0    steps: 376  evaluation reward: 401.2\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9115: Policy loss: 0.081358. Value loss: 0.047885. Entropy: 0.297163.\n",
      "Iteration 9116: Policy loss: 0.075991. Value loss: 0.027994. Entropy: 0.297536.\n",
      "Iteration 9117: Policy loss: 0.074409. Value loss: 0.022991. Entropy: 0.297480.\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9118: Policy loss: -0.155658. Value loss: 0.136566. Entropy: 0.312212.\n",
      "Iteration 9119: Policy loss: -0.159245. Value loss: 0.061190. Entropy: 0.313543.\n",
      "Iteration 9120: Policy loss: -0.166445. Value loss: 0.045880. Entropy: 0.313067.\n",
      "episode: 3430   score: 520.0  epsilon: 1.0    steps: 128  evaluation reward: 401.95\n",
      "episode: 3431   score: 1030.0  epsilon: 1.0    steps: 896  evaluation reward: 407.6\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9121: Policy loss: 0.363545. Value loss: 0.210543. Entropy: 0.293295.\n",
      "Iteration 9122: Policy loss: 0.349247. Value loss: 0.073857. Entropy: 0.293147.\n",
      "Iteration 9123: Policy loss: 0.346261. Value loss: 0.051762. Entropy: 0.292633.\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9124: Policy loss: 0.103615. Value loss: 0.179954. Entropy: 0.309740.\n",
      "Iteration 9125: Policy loss: 0.093663. Value loss: 0.083415. Entropy: 0.310081.\n",
      "Iteration 9126: Policy loss: 0.083223. Value loss: 0.056324. Entropy: 0.310052.\n",
      "episode: 3432   score: 605.0  epsilon: 1.0    steps: 944  evaluation reward: 411.55\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9127: Policy loss: 0.179441. Value loss: 0.101727. Entropy: 0.312345.\n",
      "Iteration 9128: Policy loss: 0.184337. Value loss: 0.036841. Entropy: 0.311267.\n",
      "Iteration 9129: Policy loss: 0.179266. Value loss: 0.026680. Entropy: 0.311152.\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9130: Policy loss: 0.071318. Value loss: 0.105645. Entropy: 0.309132.\n",
      "Iteration 9131: Policy loss: 0.063500. Value loss: 0.040536. Entropy: 0.308708.\n",
      "Iteration 9132: Policy loss: 0.057724. Value loss: 0.022651. Entropy: 0.309444.\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9133: Policy loss: -0.034635. Value loss: 0.095324. Entropy: 0.306301.\n",
      "Iteration 9134: Policy loss: -0.036622. Value loss: 0.036844. Entropy: 0.307311.\n",
      "Iteration 9135: Policy loss: -0.043829. Value loss: 0.024414. Entropy: 0.306673.\n",
      "episode: 3433   score: 450.0  epsilon: 1.0    steps: 600  evaluation reward: 411.65\n",
      "episode: 3434   score: 330.0  epsilon: 1.0    steps: 736  evaluation reward: 411.05\n",
      "episode: 3435   score: 420.0  epsilon: 1.0    steps: 976  evaluation reward: 409.2\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9136: Policy loss: 0.115297. Value loss: 0.086957. Entropy: 0.291896.\n",
      "Iteration 9137: Policy loss: 0.111047. Value loss: 0.042894. Entropy: 0.293335.\n",
      "Iteration 9138: Policy loss: 0.108026. Value loss: 0.028907. Entropy: 0.292954.\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9139: Policy loss: 0.070600. Value loss: 0.152194. Entropy: 0.309176.\n",
      "Iteration 9140: Policy loss: 0.055392. Value loss: 0.088435. Entropy: 0.307809.\n",
      "Iteration 9141: Policy loss: 0.062285. Value loss: 0.071040. Entropy: 0.307795.\n",
      "episode: 3436   score: 735.0  epsilon: 1.0    steps: 808  evaluation reward: 412.95\n",
      "episode: 3437   score: 575.0  epsilon: 1.0    steps: 1016  evaluation reward: 412.9\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9142: Policy loss: 0.206632. Value loss: 0.167984. Entropy: 0.305047.\n",
      "Iteration 9143: Policy loss: 0.197320. Value loss: 0.080220. Entropy: 0.302289.\n",
      "Iteration 9144: Policy loss: 0.205951. Value loss: 0.055322. Entropy: 0.301962.\n",
      "episode: 3438   score: 335.0  epsilon: 1.0    steps: 88  evaluation reward: 412.6\n",
      "episode: 3439   score: 240.0  epsilon: 1.0    steps: 664  evaluation reward: 411.8\n",
      "episode: 3440   score: 155.0  epsilon: 1.0    steps: 880  evaluation reward: 403.55\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9145: Policy loss: 0.118479. Value loss: 0.053673. Entropy: 0.278891.\n",
      "Iteration 9146: Policy loss: 0.118856. Value loss: 0.034233. Entropy: 0.279497.\n",
      "Iteration 9147: Policy loss: 0.111084. Value loss: 0.028754. Entropy: 0.279104.\n",
      "Training network. lr: 0.000180. clip: 0.072048\n",
      "Iteration 9148: Policy loss: 0.018321. Value loss: 0.099343. Entropy: 0.307419.\n",
      "Iteration 9149: Policy loss: 0.008589. Value loss: 0.039597. Entropy: 0.306371.\n",
      "Iteration 9150: Policy loss: 0.006402. Value loss: 0.034597. Entropy: 0.307550.\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9151: Policy loss: 0.029519. Value loss: 0.119811. Entropy: 0.309820.\n",
      "Iteration 9152: Policy loss: 0.018207. Value loss: 0.039947. Entropy: 0.309978.\n",
      "Iteration 9153: Policy loss: 0.018763. Value loss: 0.029184. Entropy: 0.310688.\n",
      "episode: 3441   score: 135.0  epsilon: 1.0    steps: 40  evaluation reward: 398.4\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9154: Policy loss: 0.084651. Value loss: 0.119526. Entropy: 0.296144.\n",
      "Iteration 9155: Policy loss: 0.087748. Value loss: 0.038500. Entropy: 0.296656.\n",
      "Iteration 9156: Policy loss: 0.080528. Value loss: 0.023412. Entropy: 0.295245.\n",
      "episode: 3442   score: 215.0  epsilon: 1.0    steps: 208  evaluation reward: 394.5\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9157: Policy loss: -0.332529. Value loss: 0.360611. Entropy: 0.297587.\n",
      "Iteration 9158: Policy loss: -0.350708. Value loss: 0.249900. Entropy: 0.296145.\n",
      "Iteration 9159: Policy loss: -0.354541. Value loss: 0.203669. Entropy: 0.297935.\n",
      "episode: 3443   score: 495.0  epsilon: 1.0    steps: 832  evaluation reward: 397.05\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9160: Policy loss: -0.158725. Value loss: 0.111259. Entropy: 0.301412.\n",
      "Iteration 9161: Policy loss: -0.164587. Value loss: 0.037767. Entropy: 0.302028.\n",
      "Iteration 9162: Policy loss: -0.170485. Value loss: 0.026438. Entropy: 0.301475.\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9163: Policy loss: 0.156409. Value loss: 0.066688. Entropy: 0.313583.\n",
      "Iteration 9164: Policy loss: 0.142863. Value loss: 0.036028. Entropy: 0.311651.\n",
      "Iteration 9165: Policy loss: 0.141329. Value loss: 0.024950. Entropy: 0.310973.\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9166: Policy loss: -0.281117. Value loss: 0.327616. Entropy: 0.308111.\n",
      "Iteration 9167: Policy loss: -0.290723. Value loss: 0.098919. Entropy: 0.308181.\n",
      "Iteration 9168: Policy loss: -0.280010. Value loss: 0.046980. Entropy: 0.308440.\n",
      "episode: 3444   score: 470.0  epsilon: 1.0    steps: 8  evaluation reward: 396.5\n",
      "episode: 3445   score: 595.0  epsilon: 1.0    steps: 184  evaluation reward: 394.65\n",
      "episode: 3446   score: 290.0  epsilon: 1.0    steps: 472  evaluation reward: 393.35\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9169: Policy loss: 0.030534. Value loss: 0.141083. Entropy: 0.274358.\n",
      "Iteration 9170: Policy loss: 0.021239. Value loss: 0.049119. Entropy: 0.271529.\n",
      "Iteration 9171: Policy loss: 0.022048. Value loss: 0.037296. Entropy: 0.272756.\n",
      "episode: 3447   score: 665.0  epsilon: 1.0    steps: 176  evaluation reward: 395.4\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9172: Policy loss: -0.005570. Value loss: 0.111238. Entropy: 0.301814.\n",
      "Iteration 9173: Policy loss: -0.013471. Value loss: 0.053124. Entropy: 0.302203.\n",
      "Iteration 9174: Policy loss: -0.020419. Value loss: 0.034784. Entropy: 0.302178.\n",
      "episode: 3448   score: 270.0  epsilon: 1.0    steps: 544  evaluation reward: 393.75\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9175: Policy loss: -0.126026. Value loss: 0.174985. Entropy: 0.296040.\n",
      "Iteration 9176: Policy loss: -0.145813. Value loss: 0.059354. Entropy: 0.295638.\n",
      "Iteration 9177: Policy loss: -0.143366. Value loss: 0.037730. Entropy: 0.297209.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9178: Policy loss: 0.137938. Value loss: 0.075997. Entropy: 0.310260.\n",
      "Iteration 9179: Policy loss: 0.138755. Value loss: 0.030090. Entropy: 0.308499.\n",
      "Iteration 9180: Policy loss: 0.129032. Value loss: 0.024709. Entropy: 0.308154.\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9181: Policy loss: 0.054530. Value loss: 0.078729. Entropy: 0.311402.\n",
      "Iteration 9182: Policy loss: 0.047211. Value loss: 0.038836. Entropy: 0.312044.\n",
      "Iteration 9183: Policy loss: 0.047922. Value loss: 0.026404. Entropy: 0.312528.\n",
      "episode: 3449   score: 695.0  epsilon: 1.0    steps: 248  evaluation reward: 398.3\n",
      "episode: 3450   score: 365.0  epsilon: 1.0    steps: 736  evaluation reward: 396.95\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9184: Policy loss: -0.367985. Value loss: 0.317180. Entropy: 0.284576.\n",
      "Iteration 9185: Policy loss: -0.386005. Value loss: 0.251440. Entropy: 0.287075.\n",
      "Iteration 9186: Policy loss: -0.384357. Value loss: 0.130858. Entropy: 0.286304.\n",
      "now time :  2019-09-05 23:44:59.071708\n",
      "episode: 3451   score: 415.0  epsilon: 1.0    steps: 512  evaluation reward: 393.0\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9187: Policy loss: -0.075206. Value loss: 0.173854. Entropy: 0.300153.\n",
      "Iteration 9188: Policy loss: -0.080883. Value loss: 0.069477. Entropy: 0.302626.\n",
      "Iteration 9189: Policy loss: -0.094190. Value loss: 0.041929. Entropy: 0.301722.\n",
      "episode: 3452   score: 470.0  epsilon: 1.0    steps: 456  evaluation reward: 393.75\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9190: Policy loss: 0.213453. Value loss: 0.116407. Entropy: 0.299916.\n",
      "Iteration 9191: Policy loss: 0.200470. Value loss: 0.052138. Entropy: 0.300676.\n",
      "Iteration 9192: Policy loss: 0.198364. Value loss: 0.036286. Entropy: 0.300437.\n",
      "episode: 3453   score: 335.0  epsilon: 1.0    steps: 240  evaluation reward: 391.6\n",
      "episode: 3454   score: 435.0  epsilon: 1.0    steps: 640  evaluation reward: 393.1\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9193: Policy loss: 0.288108. Value loss: 0.067616. Entropy: 0.289310.\n",
      "Iteration 9194: Policy loss: 0.280680. Value loss: 0.028897. Entropy: 0.287790.\n",
      "Iteration 9195: Policy loss: 0.278597. Value loss: 0.024399. Entropy: 0.286707.\n",
      "episode: 3455   score: 525.0  epsilon: 1.0    steps: 360  evaluation reward: 395.2\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9196: Policy loss: 0.082849. Value loss: 0.075485. Entropy: 0.292489.\n",
      "Iteration 9197: Policy loss: 0.088389. Value loss: 0.037293. Entropy: 0.293891.\n",
      "Iteration 9198: Policy loss: 0.080386. Value loss: 0.028184. Entropy: 0.293137.\n",
      "episode: 3456   score: 260.0  epsilon: 1.0    steps: 80  evaluation reward: 389.4\n",
      "Training network. lr: 0.000180. clip: 0.071900\n",
      "Iteration 9199: Policy loss: 0.028873. Value loss: 0.092824. Entropy: 0.297231.\n",
      "Iteration 9200: Policy loss: 0.018922. Value loss: 0.040378. Entropy: 0.297615.\n",
      "Iteration 9201: Policy loss: 0.017937. Value loss: 0.031736. Entropy: 0.298388.\n",
      "episode: 3457   score: 270.0  epsilon: 1.0    steps: 888  evaluation reward: 389.7\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9202: Policy loss: -0.147186. Value loss: 0.090725. Entropy: 0.306248.\n",
      "Iteration 9203: Policy loss: -0.153869. Value loss: 0.032281. Entropy: 0.307312.\n",
      "Iteration 9204: Policy loss: -0.146687. Value loss: 0.019501. Entropy: 0.308046.\n",
      "episode: 3458   score: 315.0  epsilon: 1.0    steps: 456  evaluation reward: 390.75\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9205: Policy loss: 0.009881. Value loss: 0.054984. Entropy: 0.297669.\n",
      "Iteration 9206: Policy loss: 0.006422. Value loss: 0.022165. Entropy: 0.299499.\n",
      "Iteration 9207: Policy loss: 0.002720. Value loss: 0.016762. Entropy: 0.299618.\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9208: Policy loss: -0.048709. Value loss: 0.082115. Entropy: 0.309616.\n",
      "Iteration 9209: Policy loss: -0.057896. Value loss: 0.032033. Entropy: 0.310624.\n",
      "Iteration 9210: Policy loss: -0.065801. Value loss: 0.026351. Entropy: 0.309129.\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9211: Policy loss: 0.010665. Value loss: 0.112176. Entropy: 0.312274.\n",
      "Iteration 9212: Policy loss: -0.005140. Value loss: 0.037702. Entropy: 0.313813.\n",
      "Iteration 9213: Policy loss: -0.001455. Value loss: 0.026894. Entropy: 0.312868.\n",
      "episode: 3459   score: 290.0  epsilon: 1.0    steps: 216  evaluation reward: 391.55\n",
      "episode: 3460   score: 365.0  epsilon: 1.0    steps: 224  evaluation reward: 391.0\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9214: Policy loss: -0.102464. Value loss: 0.100524. Entropy: 0.292318.\n",
      "Iteration 9215: Policy loss: -0.113859. Value loss: 0.045050. Entropy: 0.295511.\n",
      "Iteration 9216: Policy loss: -0.116001. Value loss: 0.030267. Entropy: 0.293352.\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9217: Policy loss: -0.228543. Value loss: 0.229206. Entropy: 0.311457.\n",
      "Iteration 9218: Policy loss: -0.243850. Value loss: 0.194571. Entropy: 0.311225.\n",
      "Iteration 9219: Policy loss: -0.254851. Value loss: 0.184956. Entropy: 0.309818.\n",
      "episode: 3461   score: 435.0  epsilon: 1.0    steps: 248  evaluation reward: 393.25\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9220: Policy loss: -0.075941. Value loss: 0.075319. Entropy: 0.308396.\n",
      "Iteration 9221: Policy loss: -0.081260. Value loss: 0.033291. Entropy: 0.309303.\n",
      "Iteration 9222: Policy loss: -0.087266. Value loss: 0.026306. Entropy: 0.309506.\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9223: Policy loss: 0.049610. Value loss: 0.060352. Entropy: 0.311897.\n",
      "Iteration 9224: Policy loss: 0.048717. Value loss: 0.022647. Entropy: 0.311510.\n",
      "Iteration 9225: Policy loss: 0.046225. Value loss: 0.016356. Entropy: 0.311711.\n",
      "episode: 3462   score: 275.0  epsilon: 1.0    steps: 800  evaluation reward: 390.35\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9226: Policy loss: 0.003286. Value loss: 0.191539. Entropy: 0.305264.\n",
      "Iteration 9227: Policy loss: -0.033364. Value loss: 0.071864. Entropy: 0.304589.\n",
      "Iteration 9228: Policy loss: -0.035649. Value loss: 0.052278. Entropy: 0.304883.\n",
      "episode: 3463   score: 425.0  epsilon: 1.0    steps: 224  evaluation reward: 392.45\n",
      "episode: 3464   score: 465.0  epsilon: 1.0    steps: 232  evaluation reward: 394.95\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9229: Policy loss: 0.131805. Value loss: 0.056613. Entropy: 0.302211.\n",
      "Iteration 9230: Policy loss: 0.129464. Value loss: 0.031755. Entropy: 0.300871.\n",
      "Iteration 9231: Policy loss: 0.124967. Value loss: 0.025276. Entropy: 0.301143.\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9232: Policy loss: -0.001824. Value loss: 0.097195. Entropy: 0.318243.\n",
      "Iteration 9233: Policy loss: -0.004030. Value loss: 0.049438. Entropy: 0.317528.\n",
      "Iteration 9234: Policy loss: -0.008770. Value loss: 0.038013. Entropy: 0.318257.\n",
      "episode: 3465   score: 510.0  epsilon: 1.0    steps: 32  evaluation reward: 395.7\n",
      "episode: 3466   score: 345.0  epsilon: 1.0    steps: 224  evaluation reward: 396.6\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9235: Policy loss: 0.057123. Value loss: 0.072357. Entropy: 0.302960.\n",
      "Iteration 9236: Policy loss: 0.044312. Value loss: 0.039480. Entropy: 0.302851.\n",
      "Iteration 9237: Policy loss: 0.043752. Value loss: 0.031892. Entropy: 0.302449.\n",
      "episode: 3467   score: 880.0  epsilon: 1.0    steps: 928  evaluation reward: 403.9\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9238: Policy loss: 0.097924. Value loss: 0.060411. Entropy: 0.309103.\n",
      "Iteration 9239: Policy loss: 0.099815. Value loss: 0.035304. Entropy: 0.308090.\n",
      "Iteration 9240: Policy loss: 0.095601. Value loss: 0.026472. Entropy: 0.308408.\n",
      "episode: 3468   score: 295.0  epsilon: 1.0    steps: 584  evaluation reward: 402.5\n",
      "episode: 3469   score: 210.0  epsilon: 1.0    steps: 744  evaluation reward: 402.35\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9241: Policy loss: -0.169406. Value loss: 0.307436. Entropy: 0.302308.\n",
      "Iteration 9242: Policy loss: -0.190808. Value loss: 0.231135. Entropy: 0.301435.\n",
      "Iteration 9243: Policy loss: -0.195431. Value loss: 0.172356. Entropy: 0.302474.\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9244: Policy loss: 0.098822. Value loss: 0.112559. Entropy: 0.314632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9245: Policy loss: 0.088624. Value loss: 0.037074. Entropy: 0.314765.\n",
      "Iteration 9246: Policy loss: 0.082806. Value loss: 0.024842. Entropy: 0.314070.\n",
      "episode: 3470   score: 245.0  epsilon: 1.0    steps: 696  evaluation reward: 402.05\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9247: Policy loss: 0.256859. Value loss: 0.108182. Entropy: 0.310412.\n",
      "Iteration 9248: Policy loss: 0.248103. Value loss: 0.049890. Entropy: 0.308705.\n",
      "Iteration 9249: Policy loss: 0.252883. Value loss: 0.034833. Entropy: 0.308874.\n",
      "episode: 3471   score: 260.0  epsilon: 1.0    steps: 792  evaluation reward: 402.1\n",
      "Training network. lr: 0.000179. clip: 0.071744\n",
      "Iteration 9250: Policy loss: 0.120581. Value loss: 0.070613. Entropy: 0.310181.\n",
      "Iteration 9251: Policy loss: 0.113713. Value loss: 0.025204. Entropy: 0.310736.\n",
      "Iteration 9252: Policy loss: 0.110421. Value loss: 0.020670. Entropy: 0.310502.\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9253: Policy loss: -0.214408. Value loss: 0.099048. Entropy: 0.311247.\n",
      "Iteration 9254: Policy loss: -0.213859. Value loss: 0.041106. Entropy: 0.311117.\n",
      "Iteration 9255: Policy loss: -0.224445. Value loss: 0.027274. Entropy: 0.310729.\n",
      "episode: 3472   score: 570.0  epsilon: 1.0    steps: 152  evaluation reward: 405.65\n",
      "episode: 3473   score: 285.0  epsilon: 1.0    steps: 168  evaluation reward: 404.1\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9256: Policy loss: 0.108628. Value loss: 0.083550. Entropy: 0.307886.\n",
      "Iteration 9257: Policy loss: 0.108783. Value loss: 0.033326. Entropy: 0.310056.\n",
      "Iteration 9258: Policy loss: 0.101695. Value loss: 0.024921. Entropy: 0.310125.\n",
      "episode: 3474   score: 640.0  epsilon: 1.0    steps: 664  evaluation reward: 406.0\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9259: Policy loss: -0.420289. Value loss: 0.312505. Entropy: 0.304985.\n",
      "Iteration 9260: Policy loss: -0.438567. Value loss: 0.220658. Entropy: 0.305708.\n",
      "Iteration 9261: Policy loss: -0.444583. Value loss: 0.146733. Entropy: 0.304188.\n",
      "episode: 3475   score: 360.0  epsilon: 1.0    steps: 752  evaluation reward: 407.45\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9262: Policy loss: -0.341464. Value loss: 0.281443. Entropy: 0.310367.\n",
      "Iteration 9263: Policy loss: -0.356401. Value loss: 0.183011. Entropy: 0.310284.\n",
      "Iteration 9264: Policy loss: -0.380525. Value loss: 0.129548. Entropy: 0.310699.\n",
      "episode: 3476   score: 405.0  epsilon: 1.0    steps: 992  evaluation reward: 408.2\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9265: Policy loss: 0.035525. Value loss: 0.117119. Entropy: 0.314475.\n",
      "Iteration 9266: Policy loss: 0.029877. Value loss: 0.039333. Entropy: 0.313451.\n",
      "Iteration 9267: Policy loss: 0.024750. Value loss: 0.028974. Entropy: 0.312454.\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9268: Policy loss: 0.183982. Value loss: 0.093541. Entropy: 0.314796.\n",
      "Iteration 9269: Policy loss: 0.180322. Value loss: 0.044995. Entropy: 0.313462.\n",
      "Iteration 9270: Policy loss: 0.173914. Value loss: 0.029756. Entropy: 0.313369.\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9271: Policy loss: -0.102200. Value loss: 0.109287. Entropy: 0.312772.\n",
      "Iteration 9272: Policy loss: -0.112131. Value loss: 0.045098. Entropy: 0.312254.\n",
      "Iteration 9273: Policy loss: -0.118143. Value loss: 0.033597. Entropy: 0.312370.\n",
      "episode: 3477   score: 455.0  epsilon: 1.0    steps: 448  evaluation reward: 409.4\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9274: Policy loss: 0.082677. Value loss: 0.103074. Entropy: 0.312245.\n",
      "Iteration 9275: Policy loss: 0.071595. Value loss: 0.036661. Entropy: 0.312066.\n",
      "Iteration 9276: Policy loss: 0.063983. Value loss: 0.026350. Entropy: 0.311926.\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9277: Policy loss: -0.211500. Value loss: 0.385893. Entropy: 0.314777.\n",
      "Iteration 9278: Policy loss: -0.235398. Value loss: 0.162233. Entropy: 0.314766.\n",
      "Iteration 9279: Policy loss: -0.214622. Value loss: 0.051051. Entropy: 0.315227.\n",
      "episode: 3478   score: 445.0  epsilon: 1.0    steps: 416  evaluation reward: 409.85\n",
      "episode: 3479   score: 695.0  epsilon: 1.0    steps: 984  evaluation reward: 411.4\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9280: Policy loss: 0.039576. Value loss: 0.073576. Entropy: 0.310811.\n",
      "Iteration 9281: Policy loss: 0.028756. Value loss: 0.032671. Entropy: 0.310950.\n",
      "Iteration 9282: Policy loss: 0.027065. Value loss: 0.025681. Entropy: 0.310092.\n",
      "episode: 3480   score: 325.0  epsilon: 1.0    steps: 120  evaluation reward: 409.75\n",
      "episode: 3481   score: 620.0  epsilon: 1.0    steps: 968  evaluation reward: 412.3\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9283: Policy loss: 0.250795. Value loss: 0.155596. Entropy: 0.315325.\n",
      "Iteration 9284: Policy loss: 0.245464. Value loss: 0.054789. Entropy: 0.317284.\n",
      "Iteration 9285: Policy loss: 0.237813. Value loss: 0.041406. Entropy: 0.315768.\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9286: Policy loss: 0.071518. Value loss: 0.137531. Entropy: 0.307801.\n",
      "Iteration 9287: Policy loss: 0.064462. Value loss: 0.059653. Entropy: 0.309185.\n",
      "Iteration 9288: Policy loss: 0.048347. Value loss: 0.041863. Entropy: 0.307510.\n",
      "episode: 3482   score: 680.0  epsilon: 1.0    steps: 616  evaluation reward: 417.65\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9289: Policy loss: 0.346109. Value loss: 0.116027. Entropy: 0.307621.\n",
      "Iteration 9290: Policy loss: 0.336201. Value loss: 0.047908. Entropy: 0.304614.\n",
      "Iteration 9291: Policy loss: 0.323308. Value loss: 0.033837. Entropy: 0.303969.\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9292: Policy loss: 0.079596. Value loss: 0.078792. Entropy: 0.313793.\n",
      "Iteration 9293: Policy loss: 0.067303. Value loss: 0.029095. Entropy: 0.313410.\n",
      "Iteration 9294: Policy loss: 0.073790. Value loss: 0.019618. Entropy: 0.312765.\n",
      "episode: 3483   score: 435.0  epsilon: 1.0    steps: 752  evaluation reward: 417.7\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9295: Policy loss: -0.014485. Value loss: 0.079813. Entropy: 0.308860.\n",
      "Iteration 9296: Policy loss: -0.018077. Value loss: 0.045526. Entropy: 0.308305.\n",
      "Iteration 9297: Policy loss: -0.019399. Value loss: 0.035621. Entropy: 0.308494.\n",
      "episode: 3484   score: 210.0  epsilon: 1.0    steps: 368  evaluation reward: 414.55\n",
      "episode: 3485   score: 435.0  epsilon: 1.0    steps: 944  evaluation reward: 416.25\n",
      "Training network. lr: 0.000179. clip: 0.071587\n",
      "Iteration 9298: Policy loss: -0.432658. Value loss: 0.293751. Entropy: 0.306628.\n",
      "Iteration 9299: Policy loss: -0.446514. Value loss: 0.141168. Entropy: 0.307769.\n",
      "Iteration 9300: Policy loss: -0.447223. Value loss: 0.082004. Entropy: 0.305255.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9301: Policy loss: -0.205954. Value loss: 0.207435. Entropy: 0.311760.\n",
      "Iteration 9302: Policy loss: -0.229127. Value loss: 0.083833. Entropy: 0.311590.\n",
      "Iteration 9303: Policy loss: -0.241342. Value loss: 0.054986. Entropy: 0.312032.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9304: Policy loss: -0.012737. Value loss: 0.109911. Entropy: 0.306090.\n",
      "Iteration 9305: Policy loss: -0.011635. Value loss: 0.051061. Entropy: 0.306853.\n",
      "Iteration 9306: Policy loss: -0.024309. Value loss: 0.037716. Entropy: 0.308303.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9307: Policy loss: 0.305282. Value loss: 0.313930. Entropy: 0.308265.\n",
      "Iteration 9308: Policy loss: 0.285112. Value loss: 0.084366. Entropy: 0.307675.\n",
      "Iteration 9309: Policy loss: 0.271745. Value loss: 0.047091. Entropy: 0.306775.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9310: Policy loss: -0.461854. Value loss: 0.505998. Entropy: 0.311879.\n",
      "Iteration 9311: Policy loss: -0.442649. Value loss: 0.270014. Entropy: 0.310364.\n",
      "Iteration 9312: Policy loss: -0.485280. Value loss: 0.175430. Entropy: 0.310306.\n",
      "episode: 3486   score: 390.0  epsilon: 1.0    steps: 56  evaluation reward: 414.55\n",
      "episode: 3487   score: 620.0  epsilon: 1.0    steps: 184  evaluation reward: 415.35\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9313: Policy loss: -0.122057. Value loss: 0.127380. Entropy: 0.305846.\n",
      "Iteration 9314: Policy loss: -0.123944. Value loss: 0.069343. Entropy: 0.303112.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9315: Policy loss: -0.131731. Value loss: 0.046653. Entropy: 0.302986.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9316: Policy loss: 0.267907. Value loss: 0.153962. Entropy: 0.312309.\n",
      "Iteration 9317: Policy loss: 0.253072. Value loss: 0.067228. Entropy: 0.312488.\n",
      "Iteration 9318: Policy loss: 0.247714. Value loss: 0.049729. Entropy: 0.311502.\n",
      "episode: 3488   score: 905.0  epsilon: 1.0    steps: 136  evaluation reward: 423.2\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9319: Policy loss: -0.318576. Value loss: 0.448247. Entropy: 0.309767.\n",
      "Iteration 9320: Policy loss: -0.365008. Value loss: 0.141469. Entropy: 0.309440.\n",
      "Iteration 9321: Policy loss: -0.358128. Value loss: 0.089308. Entropy: 0.309775.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9322: Policy loss: -0.075710. Value loss: 0.164859. Entropy: 0.310521.\n",
      "Iteration 9323: Policy loss: -0.096776. Value loss: 0.064089. Entropy: 0.311725.\n",
      "Iteration 9324: Policy loss: -0.101248. Value loss: 0.055990. Entropy: 0.311920.\n",
      "episode: 3489   score: 820.0  epsilon: 1.0    steps: 48  evaluation reward: 429.3\n",
      "episode: 3490   score: 390.0  epsilon: 1.0    steps: 416  evaluation reward: 430.0\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9325: Policy loss: -0.206998. Value loss: 0.182337. Entropy: 0.305437.\n",
      "Iteration 9326: Policy loss: -0.209024. Value loss: 0.071710. Entropy: 0.305564.\n",
      "Iteration 9327: Policy loss: -0.221192. Value loss: 0.054100. Entropy: 0.306346.\n",
      "episode: 3491   score: 825.0  epsilon: 1.0    steps: 544  evaluation reward: 432.55\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9328: Policy loss: 0.403147. Value loss: 0.249419. Entropy: 0.316384.\n",
      "Iteration 9329: Policy loss: 0.391327. Value loss: 0.093950. Entropy: 0.314635.\n",
      "Iteration 9330: Policy loss: 0.372320. Value loss: 0.063447. Entropy: 0.314786.\n",
      "episode: 3492   score: 575.0  epsilon: 1.0    steps: 24  evaluation reward: 431.45\n",
      "episode: 3493   score: 420.0  epsilon: 1.0    steps: 440  evaluation reward: 431.15\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9331: Policy loss: 0.405748. Value loss: 0.124917. Entropy: 0.310493.\n",
      "Iteration 9332: Policy loss: 0.399325. Value loss: 0.043962. Entropy: 0.308038.\n",
      "Iteration 9333: Policy loss: 0.399145. Value loss: 0.032810. Entropy: 0.308477.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9334: Policy loss: 0.032629. Value loss: 0.081259. Entropy: 0.314478.\n",
      "Iteration 9335: Policy loss: 0.025972. Value loss: 0.043486. Entropy: 0.313266.\n",
      "Iteration 9336: Policy loss: 0.025469. Value loss: 0.033314. Entropy: 0.313323.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9337: Policy loss: -0.106908. Value loss: 0.458688. Entropy: 0.310058.\n",
      "Iteration 9338: Policy loss: -0.126503. Value loss: 0.256097. Entropy: 0.308535.\n",
      "Iteration 9339: Policy loss: -0.135492. Value loss: 0.182817. Entropy: 0.308924.\n",
      "episode: 3494   score: 335.0  epsilon: 1.0    steps: 992  evaluation reward: 431.6\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9340: Policy loss: -0.295614. Value loss: 0.369671. Entropy: 0.307793.\n",
      "Iteration 9341: Policy loss: -0.312811. Value loss: 0.100423. Entropy: 0.307460.\n",
      "Iteration 9342: Policy loss: -0.322301. Value loss: 0.062989. Entropy: 0.307492.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9343: Policy loss: 0.242427. Value loss: 0.111084. Entropy: 0.310770.\n",
      "Iteration 9344: Policy loss: 0.240935. Value loss: 0.049969. Entropy: 0.310313.\n",
      "Iteration 9345: Policy loss: 0.237566. Value loss: 0.037202. Entropy: 0.309659.\n",
      "episode: 3495   score: 210.0  epsilon: 1.0    steps: 320  evaluation reward: 431.05\n",
      "episode: 3496   score: 885.0  epsilon: 1.0    steps: 880  evaluation reward: 436.2\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9346: Policy loss: -0.269387. Value loss: 0.330621. Entropy: 0.305614.\n",
      "Iteration 9347: Policy loss: -0.269927. Value loss: 0.134564. Entropy: 0.303999.\n",
      "Iteration 9348: Policy loss: -0.287032. Value loss: 0.088184. Entropy: 0.305392.\n",
      "Training network. lr: 0.000179. clip: 0.071440\n",
      "Iteration 9349: Policy loss: 0.111678. Value loss: 0.109125. Entropy: 0.315375.\n",
      "Iteration 9350: Policy loss: 0.109120. Value loss: 0.044628. Entropy: 0.315219.\n",
      "Iteration 9351: Policy loss: 0.104529. Value loss: 0.032598. Entropy: 0.314001.\n",
      "episode: 3497   score: 995.0  epsilon: 1.0    steps: 808  evaluation reward: 439.95\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9352: Policy loss: 0.146062. Value loss: 0.108785. Entropy: 0.302358.\n",
      "Iteration 9353: Policy loss: 0.140580. Value loss: 0.043833. Entropy: 0.302980.\n",
      "Iteration 9354: Policy loss: 0.135278. Value loss: 0.032358. Entropy: 0.303482.\n",
      "episode: 3498   score: 665.0  epsilon: 1.0    steps: 480  evaluation reward: 442.7\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9355: Policy loss: -0.181260. Value loss: 0.324858. Entropy: 0.307264.\n",
      "Iteration 9356: Policy loss: -0.202092. Value loss: 0.096981. Entropy: 0.306334.\n",
      "Iteration 9357: Policy loss: -0.208972. Value loss: 0.059193. Entropy: 0.306873.\n",
      "episode: 3499   score: 620.0  epsilon: 1.0    steps: 448  evaluation reward: 445.85\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9358: Policy loss: 0.367627. Value loss: 0.183899. Entropy: 0.303573.\n",
      "Iteration 9359: Policy loss: 0.347061. Value loss: 0.087854. Entropy: 0.302613.\n",
      "Iteration 9360: Policy loss: 0.336732. Value loss: 0.071270. Entropy: 0.302300.\n",
      "episode: 3500   score: 420.0  epsilon: 1.0    steps: 800  evaluation reward: 446.9\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9361: Policy loss: 0.315812. Value loss: 0.224913. Entropy: 0.308832.\n",
      "Iteration 9362: Policy loss: 0.293869. Value loss: 0.074949. Entropy: 0.307309.\n",
      "Iteration 9363: Policy loss: 0.307032. Value loss: 0.049919. Entropy: 0.307151.\n",
      "now time :  2019-09-05 23:55:56.572177\n",
      "episode: 3501   score: 820.0  epsilon: 1.0    steps: 296  evaluation reward: 451.65\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9364: Policy loss: 0.068196. Value loss: 0.123220. Entropy: 0.311034.\n",
      "Iteration 9365: Policy loss: 0.068250. Value loss: 0.052335. Entropy: 0.311170.\n",
      "Iteration 9366: Policy loss: 0.055863. Value loss: 0.038896. Entropy: 0.310891.\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9367: Policy loss: 0.386150. Value loss: 0.221548. Entropy: 0.307804.\n",
      "Iteration 9368: Policy loss: 0.388180. Value loss: 0.074680. Entropy: 0.307217.\n",
      "Iteration 9369: Policy loss: 0.374974. Value loss: 0.046523. Entropy: 0.307554.\n",
      "episode: 3502   score: 360.0  epsilon: 1.0    steps: 88  evaluation reward: 451.15\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9370: Policy loss: 0.005138. Value loss: 0.112807. Entropy: 0.303006.\n",
      "Iteration 9371: Policy loss: -0.009011. Value loss: 0.047461. Entropy: 0.303420.\n",
      "Iteration 9372: Policy loss: -0.016933. Value loss: 0.032599. Entropy: 0.301762.\n",
      "episode: 3503   score: 260.0  epsilon: 1.0    steps: 312  evaluation reward: 451.05\n",
      "episode: 3504   score: 500.0  epsilon: 1.0    steps: 560  evaluation reward: 450.15\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9373: Policy loss: 0.432494. Value loss: 0.148150. Entropy: 0.305840.\n",
      "Iteration 9374: Policy loss: 0.427463. Value loss: 0.065048. Entropy: 0.304903.\n",
      "Iteration 9375: Policy loss: 0.424422. Value loss: 0.048818. Entropy: 0.304615.\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9376: Policy loss: 0.102528. Value loss: 0.126135. Entropy: 0.309044.\n",
      "Iteration 9377: Policy loss: 0.092353. Value loss: 0.058469. Entropy: 0.308801.\n",
      "Iteration 9378: Policy loss: 0.086002. Value loss: 0.039495. Entropy: 0.308374.\n",
      "episode: 3505   score: 365.0  epsilon: 1.0    steps: 192  evaluation reward: 450.15\n",
      "episode: 3506   score: 450.0  epsilon: 1.0    steps: 528  evaluation reward: 452.05\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9379: Policy loss: 0.231760. Value loss: 0.070745. Entropy: 0.304863.\n",
      "Iteration 9380: Policy loss: 0.220825. Value loss: 0.030026. Entropy: 0.305670.\n",
      "Iteration 9381: Policy loss: 0.219630. Value loss: 0.023637. Entropy: 0.305611.\n",
      "episode: 3507   score: 270.0  epsilon: 1.0    steps: 56  evaluation reward: 451.65\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9382: Policy loss: -0.100107. Value loss: 0.109008. Entropy: 0.308670.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9383: Policy loss: -0.109646. Value loss: 0.054569. Entropy: 0.309734.\n",
      "Iteration 9384: Policy loss: -0.113834. Value loss: 0.040265. Entropy: 0.309479.\n",
      "episode: 3508   score: 345.0  epsilon: 1.0    steps: 416  evaluation reward: 447.75\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9385: Policy loss: -0.033499. Value loss: 0.299844. Entropy: 0.302516.\n",
      "Iteration 9386: Policy loss: -0.036479. Value loss: 0.073143. Entropy: 0.302286.\n",
      "Iteration 9387: Policy loss: -0.050200. Value loss: 0.048189. Entropy: 0.304495.\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9388: Policy loss: 0.034461. Value loss: 0.107575. Entropy: 0.310168.\n",
      "Iteration 9389: Policy loss: 0.022723. Value loss: 0.037980. Entropy: 0.310206.\n",
      "Iteration 9390: Policy loss: 0.020967. Value loss: 0.027984. Entropy: 0.311427.\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9391: Policy loss: 0.070536. Value loss: 0.123691. Entropy: 0.307458.\n",
      "Iteration 9392: Policy loss: 0.062797. Value loss: 0.039966. Entropy: 0.306068.\n",
      "Iteration 9393: Policy loss: 0.055976. Value loss: 0.027382. Entropy: 0.306638.\n",
      "episode: 3509   score: 275.0  epsilon: 1.0    steps: 808  evaluation reward: 446.55\n",
      "episode: 3510   score: 470.0  epsilon: 1.0    steps: 904  evaluation reward: 448.05\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9394: Policy loss: -0.134099. Value loss: 0.339036. Entropy: 0.305131.\n",
      "Iteration 9395: Policy loss: -0.127650. Value loss: 0.098341. Entropy: 0.304622.\n",
      "Iteration 9396: Policy loss: -0.132456. Value loss: 0.058790. Entropy: 0.303189.\n",
      "episode: 3511   score: 260.0  epsilon: 1.0    steps: 720  evaluation reward: 448.25\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9397: Policy loss: 0.164876. Value loss: 0.057159. Entropy: 0.308283.\n",
      "Iteration 9398: Policy loss: 0.159169. Value loss: 0.028420. Entropy: 0.308743.\n",
      "Iteration 9399: Policy loss: 0.153466. Value loss: 0.023709. Entropy: 0.308538.\n",
      "episode: 3512   score: 410.0  epsilon: 1.0    steps: 800  evaluation reward: 447.65\n",
      "Training network. lr: 0.000178. clip: 0.071283\n",
      "Iteration 9400: Policy loss: 0.031464. Value loss: 0.114263. Entropy: 0.303415.\n",
      "Iteration 9401: Policy loss: 0.026750. Value loss: 0.057303. Entropy: 0.304325.\n",
      "Iteration 9402: Policy loss: 0.023862. Value loss: 0.046059. Entropy: 0.302358.\n",
      "episode: 3513   score: 650.0  epsilon: 1.0    steps: 96  evaluation reward: 451.8\n",
      "episode: 3514   score: 275.0  epsilon: 1.0    steps: 528  evaluation reward: 450.1\n",
      "episode: 3515   score: 240.0  epsilon: 1.0    steps: 1024  evaluation reward: 449.65\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9403: Policy loss: 0.104092. Value loss: 0.063654. Entropy: 0.300245.\n",
      "Iteration 9404: Policy loss: 0.099331. Value loss: 0.033066. Entropy: 0.299759.\n",
      "Iteration 9405: Policy loss: 0.092012. Value loss: 0.028223. Entropy: 0.299257.\n",
      "episode: 3516   score: 635.0  epsilon: 1.0    steps: 960  evaluation reward: 453.05\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9406: Policy loss: 0.326297. Value loss: 0.186722. Entropy: 0.305577.\n",
      "Iteration 9407: Policy loss: 0.312995. Value loss: 0.072154. Entropy: 0.306930.\n",
      "Iteration 9408: Policy loss: 0.314489. Value loss: 0.046650. Entropy: 0.306175.\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9409: Policy loss: 0.184329. Value loss: 0.098190. Entropy: 0.308781.\n",
      "Iteration 9410: Policy loss: 0.183734. Value loss: 0.043397. Entropy: 0.309894.\n",
      "Iteration 9411: Policy loss: 0.178500. Value loss: 0.027135. Entropy: 0.309296.\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9412: Policy loss: -0.224398. Value loss: 0.287181. Entropy: 0.308675.\n",
      "Iteration 9413: Policy loss: -0.261480. Value loss: 0.217749. Entropy: 0.309538.\n",
      "Iteration 9414: Policy loss: -0.263578. Value loss: 0.180087. Entropy: 0.309407.\n",
      "episode: 3517   score: 315.0  epsilon: 1.0    steps: 944  evaluation reward: 452.1\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9415: Policy loss: -0.188073. Value loss: 0.312707. Entropy: 0.307776.\n",
      "Iteration 9416: Policy loss: -0.186758. Value loss: 0.169852. Entropy: 0.309358.\n",
      "Iteration 9417: Policy loss: -0.194020. Value loss: 0.066903. Entropy: 0.308647.\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9418: Policy loss: -0.019151. Value loss: 0.176033. Entropy: 0.312911.\n",
      "Iteration 9419: Policy loss: -0.019596. Value loss: 0.048648. Entropy: 0.313916.\n",
      "Iteration 9420: Policy loss: -0.031307. Value loss: 0.026598. Entropy: 0.312968.\n",
      "episode: 3518   score: 260.0  epsilon: 1.0    steps: 488  evaluation reward: 451.1\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9421: Policy loss: 0.071443. Value loss: 0.122226. Entropy: 0.308586.\n",
      "Iteration 9422: Policy loss: 0.056566. Value loss: 0.040697. Entropy: 0.308504.\n",
      "Iteration 9423: Policy loss: 0.052342. Value loss: 0.025630. Entropy: 0.307135.\n",
      "episode: 3519   score: 530.0  epsilon: 1.0    steps: 704  evaluation reward: 452.55\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9424: Policy loss: 0.163569. Value loss: 0.082486. Entropy: 0.301745.\n",
      "Iteration 9425: Policy loss: 0.155557. Value loss: 0.031823. Entropy: 0.302593.\n",
      "Iteration 9426: Policy loss: 0.150964. Value loss: 0.024933. Entropy: 0.301293.\n",
      "episode: 3520   score: 315.0  epsilon: 1.0    steps: 208  evaluation reward: 448.75\n",
      "episode: 3521   score: 260.0  epsilon: 1.0    steps: 448  evaluation reward: 448.2\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9427: Policy loss: -0.099640. Value loss: 0.226123. Entropy: 0.304073.\n",
      "Iteration 9428: Policy loss: -0.114591. Value loss: 0.093293. Entropy: 0.301941.\n",
      "Iteration 9429: Policy loss: -0.119946. Value loss: 0.048529. Entropy: 0.302893.\n",
      "episode: 3522   score: 450.0  epsilon: 1.0    steps: 328  evaluation reward: 448.25\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9430: Policy loss: 0.053535. Value loss: 0.076479. Entropy: 0.309115.\n",
      "Iteration 9431: Policy loss: 0.043810. Value loss: 0.035992. Entropy: 0.309096.\n",
      "Iteration 9432: Policy loss: 0.046283. Value loss: 0.030355. Entropy: 0.309674.\n",
      "episode: 3523   score: 595.0  epsilon: 1.0    steps: 544  evaluation reward: 450.75\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9433: Policy loss: -0.006569. Value loss: 0.128153. Entropy: 0.301507.\n",
      "Iteration 9434: Policy loss: -0.009865. Value loss: 0.059912. Entropy: 0.300971.\n",
      "Iteration 9435: Policy loss: -0.008785. Value loss: 0.042572. Entropy: 0.300972.\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9436: Policy loss: 0.592626. Value loss: 0.225676. Entropy: 0.306770.\n",
      "Iteration 9437: Policy loss: 0.571080. Value loss: 0.053466. Entropy: 0.307376.\n",
      "Iteration 9438: Policy loss: 0.581889. Value loss: 0.028794. Entropy: 0.306974.\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9439: Policy loss: 0.368707. Value loss: 0.116535. Entropy: 0.309497.\n",
      "Iteration 9440: Policy loss: 0.353064. Value loss: 0.026789. Entropy: 0.309356.\n",
      "Iteration 9441: Policy loss: 0.351197. Value loss: 0.017154. Entropy: 0.309469.\n",
      "episode: 3524   score: 420.0  epsilon: 1.0    steps: 592  evaluation reward: 450.9\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9442: Policy loss: -0.309876. Value loss: 0.161484. Entropy: 0.305996.\n",
      "Iteration 9443: Policy loss: -0.318673. Value loss: 0.068479. Entropy: 0.306006.\n",
      "Iteration 9444: Policy loss: -0.330839. Value loss: 0.049252. Entropy: 0.306517.\n",
      "episode: 3525   score: 275.0  epsilon: 1.0    steps: 408  evaluation reward: 450.3\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9445: Policy loss: 0.197972. Value loss: 0.138406. Entropy: 0.305916.\n",
      "Iteration 9446: Policy loss: 0.193314. Value loss: 0.051023. Entropy: 0.304546.\n",
      "Iteration 9447: Policy loss: 0.183231. Value loss: 0.035013. Entropy: 0.304328.\n",
      "episode: 3526   score: 635.0  epsilon: 1.0    steps: 976  evaluation reward: 452.8\n",
      "Training network. lr: 0.000178. clip: 0.071126\n",
      "Iteration 9448: Policy loss: 0.030066. Value loss: 0.127283. Entropy: 0.308210.\n",
      "Iteration 9449: Policy loss: 0.021791. Value loss: 0.051494. Entropy: 0.308889.\n",
      "Iteration 9450: Policy loss: 0.022347. Value loss: 0.041565. Entropy: 0.307200.\n",
      "episode: 3527   score: 320.0  epsilon: 1.0    steps: 184  evaluation reward: 451.0\n",
      "episode: 3528   score: 360.0  epsilon: 1.0    steps: 968  evaluation reward: 449.95\n",
      "Training network. lr: 0.000177. clip: 0.070979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9451: Policy loss: 0.104270. Value loss: 0.098574. Entropy: 0.299051.\n",
      "Iteration 9452: Policy loss: 0.094912. Value loss: 0.033927. Entropy: 0.298160.\n",
      "Iteration 9453: Policy loss: 0.090340. Value loss: 0.025729. Entropy: 0.297196.\n",
      "episode: 3529   score: 265.0  epsilon: 1.0    steps: 32  evaluation reward: 448.35\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9454: Policy loss: 0.289625. Value loss: 0.102852. Entropy: 0.301639.\n",
      "Iteration 9455: Policy loss: 0.275778. Value loss: 0.044812. Entropy: 0.298462.\n",
      "Iteration 9456: Policy loss: 0.274806. Value loss: 0.034626. Entropy: 0.298571.\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9457: Policy loss: 0.044759. Value loss: 0.047673. Entropy: 0.309677.\n",
      "Iteration 9458: Policy loss: 0.044914. Value loss: 0.021047. Entropy: 0.311171.\n",
      "Iteration 9459: Policy loss: 0.040636. Value loss: 0.017388. Entropy: 0.311080.\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9460: Policy loss: 0.118996. Value loss: 0.110798. Entropy: 0.306954.\n",
      "Iteration 9461: Policy loss: 0.107013. Value loss: 0.043635. Entropy: 0.304506.\n",
      "Iteration 9462: Policy loss: 0.099642. Value loss: 0.025826. Entropy: 0.305787.\n",
      "episode: 3530   score: 465.0  epsilon: 1.0    steps: 8  evaluation reward: 447.8\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9463: Policy loss: 0.209985. Value loss: 0.062788. Entropy: 0.297898.\n",
      "Iteration 9464: Policy loss: 0.200543. Value loss: 0.021765. Entropy: 0.298207.\n",
      "Iteration 9465: Policy loss: 0.201070. Value loss: 0.015253. Entropy: 0.297788.\n",
      "episode: 3531   score: 240.0  epsilon: 1.0    steps: 456  evaluation reward: 439.9\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9466: Policy loss: 0.060453. Value loss: 0.066798. Entropy: 0.299085.\n",
      "Iteration 9467: Policy loss: 0.061297. Value loss: 0.027633. Entropy: 0.300832.\n",
      "Iteration 9468: Policy loss: 0.052219. Value loss: 0.018783. Entropy: 0.299938.\n",
      "episode: 3532   score: 510.0  epsilon: 1.0    steps: 952  evaluation reward: 438.95\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9469: Policy loss: 0.012374. Value loss: 0.088113. Entropy: 0.307240.\n",
      "Iteration 9470: Policy loss: 0.005704. Value loss: 0.034087. Entropy: 0.307911.\n",
      "Iteration 9471: Policy loss: 0.004154. Value loss: 0.026350. Entropy: 0.307641.\n",
      "episode: 3533   score: 260.0  epsilon: 1.0    steps: 824  evaluation reward: 437.05\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9472: Policy loss: -0.006213. Value loss: 0.111518. Entropy: 0.297382.\n",
      "Iteration 9473: Policy loss: -0.016954. Value loss: 0.041608. Entropy: 0.298703.\n",
      "Iteration 9474: Policy loss: -0.021374. Value loss: 0.029743. Entropy: 0.298280.\n",
      "episode: 3534   score: 390.0  epsilon: 1.0    steps: 448  evaluation reward: 437.65\n",
      "episode: 3535   score: 325.0  epsilon: 1.0    steps: 464  evaluation reward: 436.7\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9475: Policy loss: -0.138871. Value loss: 0.331347. Entropy: 0.289783.\n",
      "Iteration 9476: Policy loss: -0.154035. Value loss: 0.200470. Entropy: 0.289702.\n",
      "Iteration 9477: Policy loss: -0.169851. Value loss: 0.135132. Entropy: 0.288642.\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9478: Policy loss: -0.460028. Value loss: 0.314574. Entropy: 0.310562.\n",
      "Iteration 9479: Policy loss: -0.480354. Value loss: 0.156067. Entropy: 0.312491.\n",
      "Iteration 9480: Policy loss: -0.487008. Value loss: 0.103457. Entropy: 0.311691.\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9481: Policy loss: -0.127896. Value loss: 0.099151. Entropy: 0.308094.\n",
      "Iteration 9482: Policy loss: -0.131997. Value loss: 0.038561. Entropy: 0.309802.\n",
      "Iteration 9483: Policy loss: -0.139183. Value loss: 0.025228. Entropy: 0.309465.\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9484: Policy loss: -0.109337. Value loss: 0.267336. Entropy: 0.308307.\n",
      "Iteration 9485: Policy loss: -0.098900. Value loss: 0.130862. Entropy: 0.309527.\n",
      "Iteration 9486: Policy loss: -0.103913. Value loss: 0.082211. Entropy: 0.308296.\n",
      "episode: 3536   score: 450.0  epsilon: 1.0    steps: 184  evaluation reward: 433.85\n",
      "episode: 3537   score: 690.0  epsilon: 1.0    steps: 504  evaluation reward: 435.0\n",
      "episode: 3538   score: 265.0  epsilon: 1.0    steps: 880  evaluation reward: 434.3\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9487: Policy loss: 0.150089. Value loss: 0.129723. Entropy: 0.283747.\n",
      "Iteration 9488: Policy loss: 0.143159. Value loss: 0.045319. Entropy: 0.279400.\n",
      "Iteration 9489: Policy loss: 0.142396. Value loss: 0.030286. Entropy: 0.277499.\n",
      "episode: 3539   score: 210.0  epsilon: 1.0    steps: 160  evaluation reward: 434.0\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9490: Policy loss: -0.112931. Value loss: 0.251486. Entropy: 0.295888.\n",
      "Iteration 9491: Policy loss: -0.132835. Value loss: 0.081079. Entropy: 0.295518.\n",
      "Iteration 9492: Policy loss: -0.134226. Value loss: 0.047033. Entropy: 0.295980.\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9493: Policy loss: 0.226999. Value loss: 0.139527. Entropy: 0.315529.\n",
      "Iteration 9494: Policy loss: 0.223913. Value loss: 0.046444. Entropy: 0.313844.\n",
      "Iteration 9495: Policy loss: 0.215535. Value loss: 0.037776. Entropy: 0.313346.\n",
      "episode: 3540   score: 820.0  epsilon: 1.0    steps: 384  evaluation reward: 440.65\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9496: Policy loss: 0.457864. Value loss: 0.150069. Entropy: 0.298792.\n",
      "Iteration 9497: Policy loss: 0.461105. Value loss: 0.052287. Entropy: 0.297801.\n",
      "Iteration 9498: Policy loss: 0.453661. Value loss: 0.037076. Entropy: 0.297382.\n",
      "episode: 3541   score: 260.0  epsilon: 1.0    steps: 248  evaluation reward: 441.9\n",
      "Training network. lr: 0.000177. clip: 0.070979\n",
      "Iteration 9499: Policy loss: 0.115504. Value loss: 0.064700. Entropy: 0.290762.\n",
      "Iteration 9500: Policy loss: 0.110242. Value loss: 0.025899. Entropy: 0.291439.\n",
      "Iteration 9501: Policy loss: 0.104655. Value loss: 0.019814. Entropy: 0.293037.\n",
      "episode: 3542   score: 510.0  epsilon: 1.0    steps: 48  evaluation reward: 444.85\n",
      "episode: 3543   score: 180.0  epsilon: 1.0    steps: 904  evaluation reward: 441.7\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9502: Policy loss: -0.145966. Value loss: 0.265123. Entropy: 0.292723.\n",
      "Iteration 9503: Policy loss: -0.146917. Value loss: 0.086631. Entropy: 0.291018.\n",
      "Iteration 9504: Policy loss: -0.161468. Value loss: 0.057650. Entropy: 0.290997.\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9505: Policy loss: 0.473456. Value loss: 0.108519. Entropy: 0.304898.\n",
      "Iteration 9506: Policy loss: 0.462821. Value loss: 0.045732. Entropy: 0.303534.\n",
      "Iteration 9507: Policy loss: 0.460990. Value loss: 0.034980. Entropy: 0.302863.\n",
      "episode: 3544   score: 210.0  epsilon: 1.0    steps: 536  evaluation reward: 439.1\n",
      "episode: 3545   score: 635.0  epsilon: 1.0    steps: 744  evaluation reward: 439.5\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9508: Policy loss: -0.061012. Value loss: 0.101869. Entropy: 0.289363.\n",
      "Iteration 9509: Policy loss: -0.065571. Value loss: 0.050992. Entropy: 0.288330.\n",
      "Iteration 9510: Policy loss: -0.072337. Value loss: 0.039199. Entropy: 0.289066.\n",
      "episode: 3546   score: 315.0  epsilon: 1.0    steps: 848  evaluation reward: 439.75\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9511: Policy loss: 0.065726. Value loss: 0.078722. Entropy: 0.297825.\n",
      "Iteration 9512: Policy loss: 0.059385. Value loss: 0.043787. Entropy: 0.295745.\n",
      "Iteration 9513: Policy loss: 0.056251. Value loss: 0.034235. Entropy: 0.296201.\n",
      "episode: 3547   score: 260.0  epsilon: 1.0    steps: 64  evaluation reward: 435.7\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9514: Policy loss: -0.216040. Value loss: 0.302505. Entropy: 0.299060.\n",
      "Iteration 9515: Policy loss: -0.229803. Value loss: 0.113537. Entropy: 0.299754.\n",
      "Iteration 9516: Policy loss: -0.248209. Value loss: 0.063795. Entropy: 0.299336.\n",
      "episode: 3548   score: 155.0  epsilon: 1.0    steps: 248  evaluation reward: 434.55\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9517: Policy loss: 0.226981. Value loss: 0.201926. Entropy: 0.298999.\n",
      "Iteration 9518: Policy loss: 0.193878. Value loss: 0.087478. Entropy: 0.300451.\n",
      "Iteration 9519: Policy loss: 0.199264. Value loss: 0.066471. Entropy: 0.297516.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9520: Policy loss: 0.028780. Value loss: 0.063239. Entropy: 0.307616.\n",
      "Iteration 9521: Policy loss: 0.024760. Value loss: 0.029967. Entropy: 0.306473.\n",
      "Iteration 9522: Policy loss: 0.022655. Value loss: 0.021949. Entropy: 0.306425.\n",
      "episode: 3549   score: 470.0  epsilon: 1.0    steps: 392  evaluation reward: 432.3\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9523: Policy loss: -0.018428. Value loss: 0.060581. Entropy: 0.297859.\n",
      "Iteration 9524: Policy loss: -0.021563. Value loss: 0.024210. Entropy: 0.297489.\n",
      "Iteration 9525: Policy loss: -0.026163. Value loss: 0.016913. Entropy: 0.296941.\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9526: Policy loss: -0.158901. Value loss: 0.345596. Entropy: 0.307302.\n",
      "Iteration 9527: Policy loss: -0.164494. Value loss: 0.090527. Entropy: 0.306610.\n",
      "Iteration 9528: Policy loss: -0.164566. Value loss: 0.047730. Entropy: 0.305716.\n",
      "episode: 3550   score: 390.0  epsilon: 1.0    steps: 176  evaluation reward: 432.55\n",
      "now time :  2019-09-06 00:06:11.465523\n",
      "episode: 3551   score: 365.0  epsilon: 1.0    steps: 392  evaluation reward: 432.05\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9529: Policy loss: -0.114799. Value loss: 0.126196. Entropy: 0.287387.\n",
      "Iteration 9530: Policy loss: -0.117254. Value loss: 0.056839. Entropy: 0.289985.\n",
      "Iteration 9531: Policy loss: -0.127150. Value loss: 0.039416. Entropy: 0.289769.\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9532: Policy loss: 0.065573. Value loss: 0.255156. Entropy: 0.310488.\n",
      "Iteration 9533: Policy loss: 0.036866. Value loss: 0.072554. Entropy: 0.309735.\n",
      "Iteration 9534: Policy loss: 0.043517. Value loss: 0.037838. Entropy: 0.309739.\n",
      "episode: 3552   score: 310.0  epsilon: 1.0    steps: 216  evaluation reward: 430.45\n",
      "episode: 3553   score: 210.0  epsilon: 1.0    steps: 768  evaluation reward: 429.2\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9535: Policy loss: -0.128444. Value loss: 0.230838. Entropy: 0.291628.\n",
      "Iteration 9536: Policy loss: -0.134719. Value loss: 0.081603. Entropy: 0.292647.\n",
      "Iteration 9537: Policy loss: -0.154998. Value loss: 0.056777. Entropy: 0.293171.\n",
      "episode: 3554   score: 485.0  epsilon: 1.0    steps: 40  evaluation reward: 429.7\n",
      "episode: 3555   score: 490.0  epsilon: 1.0    steps: 96  evaluation reward: 429.35\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9538: Policy loss: 0.516712. Value loss: 0.185365. Entropy: 0.300740.\n",
      "Iteration 9539: Policy loss: 0.495431. Value loss: 0.070819. Entropy: 0.303222.\n",
      "Iteration 9540: Policy loss: 0.498758. Value loss: 0.047196. Entropy: 0.302380.\n",
      "episode: 3556   score: 210.0  epsilon: 1.0    steps: 872  evaluation reward: 428.85\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9541: Policy loss: 0.015843. Value loss: 0.083001. Entropy: 0.305354.\n",
      "Iteration 9542: Policy loss: 0.007290. Value loss: 0.037971. Entropy: 0.305806.\n",
      "Iteration 9543: Policy loss: -0.004336. Value loss: 0.029289. Entropy: 0.305477.\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9544: Policy loss: 0.065156. Value loss: 0.131372. Entropy: 0.308464.\n",
      "Iteration 9545: Policy loss: 0.062097. Value loss: 0.056491. Entropy: 0.308426.\n",
      "Iteration 9546: Policy loss: 0.054593. Value loss: 0.042268. Entropy: 0.308258.\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9547: Policy loss: 0.165718. Value loss: 0.155671. Entropy: 0.308051.\n",
      "Iteration 9548: Policy loss: 0.157762. Value loss: 0.056645. Entropy: 0.307626.\n",
      "Iteration 9549: Policy loss: 0.160608. Value loss: 0.037778. Entropy: 0.307227.\n",
      "episode: 3557   score: 260.0  epsilon: 1.0    steps: 520  evaluation reward: 428.75\n",
      "episode: 3558   score: 180.0  epsilon: 1.0    steps: 712  evaluation reward: 427.4\n",
      "Training network. lr: 0.000177. clip: 0.070822\n",
      "Iteration 9550: Policy loss: 0.247084. Value loss: 0.056589. Entropy: 0.294945.\n",
      "Iteration 9551: Policy loss: 0.232712. Value loss: 0.024412. Entropy: 0.295539.\n",
      "Iteration 9552: Policy loss: 0.236225. Value loss: 0.018820. Entropy: 0.292562.\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9553: Policy loss: 0.183904. Value loss: 0.067908. Entropy: 0.310462.\n",
      "Iteration 9554: Policy loss: 0.172259. Value loss: 0.022574. Entropy: 0.309674.\n",
      "Iteration 9555: Policy loss: 0.166210. Value loss: 0.016129. Entropy: 0.308496.\n",
      "episode: 3559   score: 210.0  epsilon: 1.0    steps: 344  evaluation reward: 426.6\n",
      "episode: 3560   score: 565.0  epsilon: 1.0    steps: 600  evaluation reward: 428.6\n",
      "episode: 3561   score: 225.0  epsilon: 1.0    steps: 832  evaluation reward: 426.5\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9556: Policy loss: 0.112177. Value loss: 0.090524. Entropy: 0.295984.\n",
      "Iteration 9557: Policy loss: 0.107484. Value loss: 0.043793. Entropy: 0.297356.\n",
      "Iteration 9558: Policy loss: 0.104923. Value loss: 0.036116. Entropy: 0.296467.\n",
      "episode: 3562   score: 285.0  epsilon: 1.0    steps: 48  evaluation reward: 426.6\n",
      "episode: 3563   score: 210.0  epsilon: 1.0    steps: 1024  evaluation reward: 424.45\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9559: Policy loss: 0.115510. Value loss: 0.082433. Entropy: 0.308299.\n",
      "Iteration 9560: Policy loss: 0.111342. Value loss: 0.041486. Entropy: 0.306653.\n",
      "Iteration 9561: Policy loss: 0.106558. Value loss: 0.029513. Entropy: 0.305759.\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9562: Policy loss: 0.009059. Value loss: 0.083292. Entropy: 0.304676.\n",
      "Iteration 9563: Policy loss: 0.008623. Value loss: 0.039550. Entropy: 0.304527.\n",
      "Iteration 9564: Policy loss: 0.000146. Value loss: 0.032644. Entropy: 0.304458.\n",
      "episode: 3564   score: 135.0  epsilon: 1.0    steps: 520  evaluation reward: 421.15\n",
      "episode: 3565   score: 395.0  epsilon: 1.0    steps: 928  evaluation reward: 420.0\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9565: Policy loss: 0.151841. Value loss: 0.052346. Entropy: 0.284002.\n",
      "Iteration 9566: Policy loss: 0.153290. Value loss: 0.024419. Entropy: 0.284822.\n",
      "Iteration 9567: Policy loss: 0.149576. Value loss: 0.017780. Entropy: 0.285310.\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9568: Policy loss: -0.311159. Value loss: 0.279088. Entropy: 0.306678.\n",
      "Iteration 9569: Policy loss: -0.313206. Value loss: 0.094947. Entropy: 0.305208.\n",
      "Iteration 9570: Policy loss: -0.324458. Value loss: 0.062429. Entropy: 0.306416.\n",
      "episode: 3566   score: 190.0  epsilon: 1.0    steps: 200  evaluation reward: 418.45\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9571: Policy loss: 0.115462. Value loss: 0.073074. Entropy: 0.298156.\n",
      "Iteration 9572: Policy loss: 0.102342. Value loss: 0.034717. Entropy: 0.297266.\n",
      "Iteration 9573: Policy loss: 0.100026. Value loss: 0.028225. Entropy: 0.296905.\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9574: Policy loss: -0.027893. Value loss: 0.082748. Entropy: 0.309110.\n",
      "Iteration 9575: Policy loss: -0.031515. Value loss: 0.035759. Entropy: 0.308150.\n",
      "Iteration 9576: Policy loss: -0.036244. Value loss: 0.029069. Entropy: 0.308573.\n",
      "episode: 3567   score: 225.0  epsilon: 1.0    steps: 192  evaluation reward: 411.9\n",
      "episode: 3568   score: 195.0  epsilon: 1.0    steps: 344  evaluation reward: 410.9\n",
      "episode: 3569   score: 225.0  epsilon: 1.0    steps: 992  evaluation reward: 411.05\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9577: Policy loss: -0.055934. Value loss: 0.069342. Entropy: 0.285665.\n",
      "Iteration 9578: Policy loss: -0.061608. Value loss: 0.031484. Entropy: 0.286964.\n",
      "Iteration 9579: Policy loss: -0.061059. Value loss: 0.023264. Entropy: 0.285903.\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9580: Policy loss: -0.392594. Value loss: 0.139577. Entropy: 0.305511.\n",
      "Iteration 9581: Policy loss: -0.390868. Value loss: 0.033953. Entropy: 0.305703.\n",
      "Iteration 9582: Policy loss: -0.401719. Value loss: 0.025322. Entropy: 0.302597.\n",
      "episode: 3570   score: 345.0  epsilon: 1.0    steps: 64  evaluation reward: 412.05\n",
      "episode: 3571   score: 215.0  epsilon: 1.0    steps: 632  evaluation reward: 411.6\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9583: Policy loss: -0.394717. Value loss: 0.242575. Entropy: 0.292578.\n",
      "Iteration 9584: Policy loss: -0.393208. Value loss: 0.079187. Entropy: 0.292293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9585: Policy loss: -0.418398. Value loss: 0.038000. Entropy: 0.291531.\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9586: Policy loss: 0.085737. Value loss: 0.143900. Entropy: 0.311439.\n",
      "Iteration 9587: Policy loss: 0.068286. Value loss: 0.043766. Entropy: 0.309926.\n",
      "Iteration 9588: Policy loss: 0.080637. Value loss: 0.028459. Entropy: 0.310100.\n",
      "episode: 3572   score: 225.0  epsilon: 1.0    steps: 368  evaluation reward: 408.15\n",
      "episode: 3573   score: 180.0  epsilon: 1.0    steps: 496  evaluation reward: 407.1\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9589: Policy loss: -0.032100. Value loss: 0.177907. Entropy: 0.299032.\n",
      "Iteration 9590: Policy loss: -0.053029. Value loss: 0.058799. Entropy: 0.298223.\n",
      "Iteration 9591: Policy loss: -0.062661. Value loss: 0.033012. Entropy: 0.299332.\n",
      "episode: 3574   score: 180.0  epsilon: 1.0    steps: 960  evaluation reward: 402.5\n",
      "episode: 3575   score: 800.0  epsilon: 1.0    steps: 1000  evaluation reward: 406.9\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9592: Policy loss: 0.034853. Value loss: 0.115637. Entropy: 0.306176.\n",
      "Iteration 9593: Policy loss: 0.023939. Value loss: 0.047849. Entropy: 0.305183.\n",
      "Iteration 9594: Policy loss: 0.020590. Value loss: 0.035685. Entropy: 0.305795.\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9595: Policy loss: -0.031755. Value loss: 0.062540. Entropy: 0.303781.\n",
      "Iteration 9596: Policy loss: -0.040631. Value loss: 0.032728. Entropy: 0.302138.\n",
      "Iteration 9597: Policy loss: -0.039998. Value loss: 0.024016. Entropy: 0.300918.\n",
      "Training network. lr: 0.000177. clip: 0.070665\n",
      "Iteration 9598: Policy loss: 0.116353. Value loss: 0.090685. Entropy: 0.311204.\n",
      "Iteration 9599: Policy loss: 0.107402. Value loss: 0.040252. Entropy: 0.310700.\n",
      "Iteration 9600: Policy loss: 0.101806. Value loss: 0.027505. Entropy: 0.309786.\n",
      "episode: 3576   score: 285.0  epsilon: 1.0    steps: 624  evaluation reward: 405.7\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9601: Policy loss: -0.024812. Value loss: 0.091939. Entropy: 0.302940.\n",
      "Iteration 9602: Policy loss: -0.026637. Value loss: 0.043335. Entropy: 0.301774.\n",
      "Iteration 9603: Policy loss: -0.032815. Value loss: 0.029601. Entropy: 0.300544.\n",
      "episode: 3577   score: 665.0  epsilon: 1.0    steps: 640  evaluation reward: 407.8\n",
      "episode: 3578   score: 210.0  epsilon: 1.0    steps: 864  evaluation reward: 405.45\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9604: Policy loss: 0.073009. Value loss: 0.053933. Entropy: 0.295777.\n",
      "Iteration 9605: Policy loss: 0.070775. Value loss: 0.025255. Entropy: 0.295048.\n",
      "Iteration 9606: Policy loss: 0.062341. Value loss: 0.018467. Entropy: 0.295478.\n",
      "episode: 3579   score: 215.0  epsilon: 1.0    steps: 464  evaluation reward: 400.65\n",
      "episode: 3580   score: 210.0  epsilon: 1.0    steps: 864  evaluation reward: 399.5\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9607: Policy loss: 0.170645. Value loss: 0.122491. Entropy: 0.296843.\n",
      "Iteration 9608: Policy loss: 0.159578. Value loss: 0.040942. Entropy: 0.296264.\n",
      "Iteration 9609: Policy loss: 0.159377. Value loss: 0.029625. Entropy: 0.296318.\n",
      "episode: 3581   score: 335.0  epsilon: 1.0    steps: 128  evaluation reward: 396.65\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9610: Policy loss: -0.005293. Value loss: 0.136260. Entropy: 0.299360.\n",
      "Iteration 9611: Policy loss: -0.024155. Value loss: 0.070328. Entropy: 0.298757.\n",
      "Iteration 9612: Policy loss: -0.021238. Value loss: 0.049215. Entropy: 0.298036.\n",
      "episode: 3582   score: 695.0  epsilon: 1.0    steps: 272  evaluation reward: 396.8\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9613: Policy loss: 0.003125. Value loss: 0.119733. Entropy: 0.294880.\n",
      "Iteration 9614: Policy loss: -0.006616. Value loss: 0.047716. Entropy: 0.295232.\n",
      "Iteration 9615: Policy loss: -0.009557. Value loss: 0.035646. Entropy: 0.296564.\n",
      "episode: 3583   score: 325.0  epsilon: 1.0    steps: 152  evaluation reward: 395.7\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9616: Policy loss: -0.298488. Value loss: 0.245385. Entropy: 0.298295.\n",
      "Iteration 9617: Policy loss: -0.306791. Value loss: 0.077155. Entropy: 0.298919.\n",
      "Iteration 9618: Policy loss: -0.311687. Value loss: 0.048068. Entropy: 0.299640.\n",
      "episode: 3584   score: 410.0  epsilon: 1.0    steps: 944  evaluation reward: 397.7\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9619: Policy loss: -0.115649. Value loss: 0.111395. Entropy: 0.302845.\n",
      "Iteration 9620: Policy loss: -0.114358. Value loss: 0.038113. Entropy: 0.300046.\n",
      "Iteration 9621: Policy loss: -0.123838. Value loss: 0.025586. Entropy: 0.300679.\n",
      "episode: 3585   score: 190.0  epsilon: 1.0    steps: 88  evaluation reward: 395.25\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9622: Policy loss: -0.244865. Value loss: 0.188633. Entropy: 0.293735.\n",
      "Iteration 9623: Policy loss: -0.273983. Value loss: 0.075214. Entropy: 0.294668.\n",
      "Iteration 9624: Policy loss: -0.284372. Value loss: 0.050353. Entropy: 0.294599.\n",
      "episode: 3586   score: 155.0  epsilon: 1.0    steps: 96  evaluation reward: 392.9\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9625: Policy loss: 0.504747. Value loss: 0.136040. Entropy: 0.298556.\n",
      "Iteration 9626: Policy loss: 0.490362. Value loss: 0.045432. Entropy: 0.296444.\n",
      "Iteration 9627: Policy loss: 0.483780. Value loss: 0.034427. Entropy: 0.296645.\n",
      "episode: 3587   score: 460.0  epsilon: 1.0    steps: 136  evaluation reward: 391.3\n",
      "episode: 3588   score: 240.0  epsilon: 1.0    steps: 832  evaluation reward: 384.65\n",
      "episode: 3589   score: 420.0  epsilon: 1.0    steps: 1024  evaluation reward: 380.65\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9628: Policy loss: -0.066859. Value loss: 0.129710. Entropy: 0.290854.\n",
      "Iteration 9629: Policy loss: -0.075103. Value loss: 0.048640. Entropy: 0.291168.\n",
      "Iteration 9630: Policy loss: -0.080944. Value loss: 0.038478. Entropy: 0.291645.\n",
      "episode: 3590   score: 240.0  epsilon: 1.0    steps: 768  evaluation reward: 379.15\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9631: Policy loss: 0.107288. Value loss: 0.148730. Entropy: 0.285075.\n",
      "Iteration 9632: Policy loss: 0.095242. Value loss: 0.059040. Entropy: 0.283519.\n",
      "Iteration 9633: Policy loss: 0.092802. Value loss: 0.039321. Entropy: 0.284493.\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9634: Policy loss: 0.415503. Value loss: 0.118579. Entropy: 0.307916.\n",
      "Iteration 9635: Policy loss: 0.392711. Value loss: 0.042924. Entropy: 0.309539.\n",
      "Iteration 9636: Policy loss: 0.386655. Value loss: 0.032070. Entropy: 0.307450.\n",
      "episode: 3591   score: 195.0  epsilon: 1.0    steps: 352  evaluation reward: 372.85\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9637: Policy loss: 0.258569. Value loss: 0.102048. Entropy: 0.299599.\n",
      "Iteration 9638: Policy loss: 0.245555. Value loss: 0.038964. Entropy: 0.299748.\n",
      "Iteration 9639: Policy loss: 0.245575. Value loss: 0.027673. Entropy: 0.298933.\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9640: Policy loss: 0.054138. Value loss: 0.107219. Entropy: 0.307549.\n",
      "Iteration 9641: Policy loss: 0.054811. Value loss: 0.041032. Entropy: 0.307344.\n",
      "Iteration 9642: Policy loss: 0.046404. Value loss: 0.028592. Entropy: 0.307077.\n",
      "episode: 3592   score: 245.0  epsilon: 1.0    steps: 320  evaluation reward: 369.55\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9643: Policy loss: -0.827197. Value loss: 0.505102. Entropy: 0.293959.\n",
      "Iteration 9644: Policy loss: -0.837041. Value loss: 0.272685. Entropy: 0.297090.\n",
      "Iteration 9645: Policy loss: -0.828586. Value loss: 0.148335. Entropy: 0.293818.\n",
      "episode: 3593   score: 840.0  epsilon: 1.0    steps: 672  evaluation reward: 373.75\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9646: Policy loss: 0.180933. Value loss: 0.302424. Entropy: 0.302576.\n",
      "Iteration 9647: Policy loss: 0.192210. Value loss: 0.097493. Entropy: 0.302070.\n",
      "Iteration 9648: Policy loss: 0.163635. Value loss: 0.047776. Entropy: 0.301183.\n",
      "episode: 3594   score: 355.0  epsilon: 1.0    steps: 808  evaluation reward: 373.95\n",
      "Training network. lr: 0.000176. clip: 0.070518\n",
      "Iteration 9649: Policy loss: 0.186728. Value loss: 0.082165. Entropy: 0.305295.\n",
      "Iteration 9650: Policy loss: 0.187885. Value loss: 0.040889. Entropy: 0.305217.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9651: Policy loss: 0.177717. Value loss: 0.028066. Entropy: 0.304425.\n",
      "episode: 3595   score: 120.0  epsilon: 1.0    steps: 376  evaluation reward: 373.05\n",
      "episode: 3596   score: 455.0  epsilon: 1.0    steps: 512  evaluation reward: 368.75\n",
      "episode: 3597   score: 610.0  epsilon: 1.0    steps: 848  evaluation reward: 364.9\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9652: Policy loss: 0.251390. Value loss: 0.113790. Entropy: 0.280469.\n",
      "Iteration 9653: Policy loss: 0.241876. Value loss: 0.047066. Entropy: 0.279815.\n",
      "Iteration 9654: Policy loss: 0.239541. Value loss: 0.036431. Entropy: 0.279162.\n",
      "episode: 3598   score: 270.0  epsilon: 1.0    steps: 400  evaluation reward: 360.95\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9655: Policy loss: 0.038172. Value loss: 0.112231. Entropy: 0.297351.\n",
      "Iteration 9656: Policy loss: 0.027721. Value loss: 0.056840. Entropy: 0.300725.\n",
      "Iteration 9657: Policy loss: 0.019212. Value loss: 0.041554. Entropy: 0.299219.\n",
      "episode: 3599   score: 265.0  epsilon: 1.0    steps: 784  evaluation reward: 357.4\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9658: Policy loss: 0.080922. Value loss: 0.128218. Entropy: 0.296742.\n",
      "Iteration 9659: Policy loss: 0.076996. Value loss: 0.053192. Entropy: 0.295892.\n",
      "Iteration 9660: Policy loss: 0.060526. Value loss: 0.036088. Entropy: 0.294981.\n",
      "episode: 3600   score: 420.0  epsilon: 1.0    steps: 56  evaluation reward: 357.4\n",
      "now time :  2019-09-06 00:14:23.820788\n",
      "episode: 3601   score: 100.0  epsilon: 1.0    steps: 400  evaluation reward: 350.2\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9661: Policy loss: -0.219824. Value loss: 0.266223. Entropy: 0.288478.\n",
      "Iteration 9662: Policy loss: -0.237595. Value loss: 0.166145. Entropy: 0.287738.\n",
      "Iteration 9663: Policy loss: -0.229207. Value loss: 0.119981. Entropy: 0.290169.\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9664: Policy loss: 0.004394. Value loss: 0.108222. Entropy: 0.299936.\n",
      "Iteration 9665: Policy loss: 0.005414. Value loss: 0.041351. Entropy: 0.301129.\n",
      "Iteration 9666: Policy loss: -0.000208. Value loss: 0.030724. Entropy: 0.299833.\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9667: Policy loss: -0.150445. Value loss: 0.182802. Entropy: 0.306875.\n",
      "Iteration 9668: Policy loss: -0.168564. Value loss: 0.074164. Entropy: 0.307302.\n",
      "Iteration 9669: Policy loss: -0.179317. Value loss: 0.046856. Entropy: 0.304745.\n",
      "episode: 3602   score: 180.0  epsilon: 1.0    steps: 48  evaluation reward: 348.4\n",
      "episode: 3603   score: 260.0  epsilon: 1.0    steps: 96  evaluation reward: 348.4\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9670: Policy loss: -0.073978. Value loss: 0.103812. Entropy: 0.288319.\n",
      "Iteration 9671: Policy loss: -0.083074. Value loss: 0.042402. Entropy: 0.287568.\n",
      "Iteration 9672: Policy loss: -0.094608. Value loss: 0.028383. Entropy: 0.290236.\n",
      "episode: 3604   score: 570.0  epsilon: 1.0    steps: 680  evaluation reward: 349.1\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9673: Policy loss: -0.155701. Value loss: 0.100672. Entropy: 0.295243.\n",
      "Iteration 9674: Policy loss: -0.164801. Value loss: 0.048760. Entropy: 0.296658.\n",
      "Iteration 9675: Policy loss: -0.169754. Value loss: 0.032119. Entropy: 0.296247.\n",
      "episode: 3605   score: 210.0  epsilon: 1.0    steps: 408  evaluation reward: 347.55\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9676: Policy loss: -0.148129. Value loss: 0.162144. Entropy: 0.292161.\n",
      "Iteration 9677: Policy loss: -0.144906. Value loss: 0.048510. Entropy: 0.290760.\n",
      "Iteration 9678: Policy loss: -0.167324. Value loss: 0.033553. Entropy: 0.291181.\n",
      "episode: 3606   score: 260.0  epsilon: 1.0    steps: 296  evaluation reward: 345.65\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9679: Policy loss: 0.033282. Value loss: 0.308857. Entropy: 0.298811.\n",
      "Iteration 9680: Policy loss: 0.002583. Value loss: 0.116226. Entropy: 0.298462.\n",
      "Iteration 9681: Policy loss: 0.003898. Value loss: 0.075446. Entropy: 0.297851.\n",
      "episode: 3607   score: 530.0  epsilon: 1.0    steps: 488  evaluation reward: 348.25\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9682: Policy loss: 0.357240. Value loss: 0.106528. Entropy: 0.295773.\n",
      "Iteration 9683: Policy loss: 0.343712. Value loss: 0.050500. Entropy: 0.294747.\n",
      "Iteration 9684: Policy loss: 0.342148. Value loss: 0.035767. Entropy: 0.294409.\n",
      "episode: 3608   score: 485.0  epsilon: 1.0    steps: 696  evaluation reward: 349.65\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9685: Policy loss: 0.099722. Value loss: 0.115572. Entropy: 0.296311.\n",
      "Iteration 9686: Policy loss: 0.091333. Value loss: 0.054034. Entropy: 0.297552.\n",
      "Iteration 9687: Policy loss: 0.093139. Value loss: 0.034988. Entropy: 0.295534.\n",
      "episode: 3609   score: 260.0  epsilon: 1.0    steps: 488  evaluation reward: 349.5\n",
      "episode: 3610   score: 210.0  epsilon: 1.0    steps: 816  evaluation reward: 346.9\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9688: Policy loss: 0.104200. Value loss: 0.160237. Entropy: 0.294813.\n",
      "Iteration 9689: Policy loss: 0.091115. Value loss: 0.057248. Entropy: 0.296415.\n",
      "Iteration 9690: Policy loss: 0.087722. Value loss: 0.034123. Entropy: 0.294767.\n",
      "episode: 3611   score: 305.0  epsilon: 1.0    steps: 712  evaluation reward: 347.35\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9691: Policy loss: 0.316084. Value loss: 0.121017. Entropy: 0.301535.\n",
      "Iteration 9692: Policy loss: 0.312893. Value loss: 0.037497. Entropy: 0.301156.\n",
      "Iteration 9693: Policy loss: 0.307220. Value loss: 0.030938. Entropy: 0.301108.\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9694: Policy loss: 0.043102. Value loss: 0.111809. Entropy: 0.311875.\n",
      "Iteration 9695: Policy loss: 0.030000. Value loss: 0.048019. Entropy: 0.310182.\n",
      "Iteration 9696: Policy loss: 0.033599. Value loss: 0.031150. Entropy: 0.310860.\n",
      "episode: 3612   score: 785.0  epsilon: 1.0    steps: 568  evaluation reward: 351.1\n",
      "episode: 3613   score: 285.0  epsilon: 1.0    steps: 608  evaluation reward: 347.45\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9697: Policy loss: -0.162937. Value loss: 0.203604. Entropy: 0.294105.\n",
      "Iteration 9698: Policy loss: -0.166437. Value loss: 0.082567. Entropy: 0.297036.\n",
      "Iteration 9699: Policy loss: -0.193944. Value loss: 0.047789. Entropy: 0.296264.\n",
      "episode: 3614   score: 425.0  epsilon: 1.0    steps: 712  evaluation reward: 348.95\n",
      "Training network. lr: 0.000176. clip: 0.070361\n",
      "Iteration 9700: Policy loss: 0.069346. Value loss: 0.065600. Entropy: 0.303989.\n",
      "Iteration 9701: Policy loss: 0.061533. Value loss: 0.027291. Entropy: 0.305534.\n",
      "Iteration 9702: Policy loss: 0.060842. Value loss: 0.021409. Entropy: 0.304895.\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9703: Policy loss: -0.266060. Value loss: 0.194808. Entropy: 0.307893.\n",
      "Iteration 9704: Policy loss: -0.270895. Value loss: 0.050133. Entropy: 0.306026.\n",
      "Iteration 9705: Policy loss: -0.278182. Value loss: 0.031883. Entropy: 0.306389.\n",
      "episode: 3615   score: 260.0  epsilon: 1.0    steps: 280  evaluation reward: 349.15\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9706: Policy loss: -0.084846. Value loss: 0.139337. Entropy: 0.308705.\n",
      "Iteration 9707: Policy loss: -0.084043. Value loss: 0.039807. Entropy: 0.309413.\n",
      "Iteration 9708: Policy loss: -0.096575. Value loss: 0.031759. Entropy: 0.309550.\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9709: Policy loss: -0.009721. Value loss: 0.256843. Entropy: 0.300173.\n",
      "Iteration 9710: Policy loss: -0.024013. Value loss: 0.071042. Entropy: 0.299067.\n",
      "Iteration 9711: Policy loss: -0.029421. Value loss: 0.051176. Entropy: 0.300218.\n",
      "episode: 3616   score: 260.0  epsilon: 1.0    steps: 328  evaluation reward: 345.4\n",
      "episode: 3617   score: 460.0  epsilon: 1.0    steps: 496  evaluation reward: 346.85\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9712: Policy loss: 0.119562. Value loss: 0.061033. Entropy: 0.305085.\n",
      "Iteration 9713: Policy loss: 0.114531. Value loss: 0.028576. Entropy: 0.305462.\n",
      "Iteration 9714: Policy loss: 0.114822. Value loss: 0.023119. Entropy: 0.304652.\n",
      "episode: 3618   score: 415.0  epsilon: 1.0    steps: 896  evaluation reward: 348.4\n",
      "Training network. lr: 0.000176. clip: 0.070205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9715: Policy loss: 0.054275. Value loss: 0.096761. Entropy: 0.305897.\n",
      "Iteration 9716: Policy loss: 0.046756. Value loss: 0.039254. Entropy: 0.306012.\n",
      "Iteration 9717: Policy loss: 0.039965. Value loss: 0.028999. Entropy: 0.305465.\n",
      "episode: 3619   score: 485.0  epsilon: 1.0    steps: 736  evaluation reward: 347.95\n",
      "episode: 3620   score: 275.0  epsilon: 1.0    steps: 736  evaluation reward: 347.55\n",
      "episode: 3621   score: 670.0  epsilon: 1.0    steps: 968  evaluation reward: 351.65\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9718: Policy loss: 0.076058. Value loss: 0.114413. Entropy: 0.304160.\n",
      "Iteration 9719: Policy loss: 0.066324. Value loss: 0.058006. Entropy: 0.303760.\n",
      "Iteration 9720: Policy loss: 0.066401. Value loss: 0.040121. Entropy: 0.303566.\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9721: Policy loss: -0.010281. Value loss: 0.206184. Entropy: 0.312300.\n",
      "Iteration 9722: Policy loss: -0.025743. Value loss: 0.035659. Entropy: 0.313985.\n",
      "Iteration 9723: Policy loss: -0.030230. Value loss: 0.022190. Entropy: 0.313285.\n",
      "episode: 3622   score: 240.0  epsilon: 1.0    steps: 1024  evaluation reward: 349.55\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9724: Policy loss: 0.110274. Value loss: 0.108051. Entropy: 0.313190.\n",
      "Iteration 9725: Policy loss: 0.112710. Value loss: 0.052647. Entropy: 0.312282.\n",
      "Iteration 9726: Policy loss: 0.104271. Value loss: 0.038329. Entropy: 0.312482.\n",
      "episode: 3623   score: 335.0  epsilon: 1.0    steps: 552  evaluation reward: 346.95\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9727: Policy loss: 0.221244. Value loss: 0.160210. Entropy: 0.313708.\n",
      "Iteration 9728: Policy loss: 0.208268. Value loss: 0.034291. Entropy: 0.313219.\n",
      "Iteration 9729: Policy loss: 0.194687. Value loss: 0.023199. Entropy: 0.313202.\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9730: Policy loss: -0.253166. Value loss: 0.207565. Entropy: 0.312296.\n",
      "Iteration 9731: Policy loss: -0.286819. Value loss: 0.102141. Entropy: 0.312906.\n",
      "Iteration 9732: Policy loss: -0.286923. Value loss: 0.056724. Entropy: 0.312783.\n",
      "episode: 3624   score: 240.0  epsilon: 1.0    steps: 904  evaluation reward: 345.15\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9733: Policy loss: -0.205527. Value loss: 0.147092. Entropy: 0.308223.\n",
      "Iteration 9734: Policy loss: -0.213250. Value loss: 0.057106. Entropy: 0.308393.\n",
      "Iteration 9735: Policy loss: -0.217443. Value loss: 0.037376. Entropy: 0.308549.\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9736: Policy loss: -0.093904. Value loss: 0.317505. Entropy: 0.308895.\n",
      "Iteration 9737: Policy loss: -0.103313. Value loss: 0.107600. Entropy: 0.310116.\n",
      "Iteration 9738: Policy loss: -0.128620. Value loss: 0.063385. Entropy: 0.309480.\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9739: Policy loss: 0.019349. Value loss: 0.193526. Entropy: 0.314587.\n",
      "Iteration 9740: Policy loss: 0.010311. Value loss: 0.087825. Entropy: 0.314150.\n",
      "Iteration 9741: Policy loss: -0.004834. Value loss: 0.059236. Entropy: 0.314055.\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9742: Policy loss: 0.110119. Value loss: 0.147240. Entropy: 0.308087.\n",
      "Iteration 9743: Policy loss: 0.090703. Value loss: 0.047561. Entropy: 0.306933.\n",
      "Iteration 9744: Policy loss: 0.080963. Value loss: 0.028456. Entropy: 0.307244.\n",
      "episode: 3625   score: 460.0  epsilon: 1.0    steps: 800  evaluation reward: 347.0\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9745: Policy loss: -0.110189. Value loss: 0.127415. Entropy: 0.308414.\n",
      "Iteration 9746: Policy loss: -0.113749. Value loss: 0.049996. Entropy: 0.308388.\n",
      "Iteration 9747: Policy loss: -0.129634. Value loss: 0.032704. Entropy: 0.308892.\n",
      "Training network. lr: 0.000176. clip: 0.070205\n",
      "Iteration 9748: Policy loss: 0.211794. Value loss: 0.165181. Entropy: 0.315672.\n",
      "Iteration 9749: Policy loss: 0.204944. Value loss: 0.058292. Entropy: 0.315742.\n",
      "Iteration 9750: Policy loss: 0.196481. Value loss: 0.038913. Entropy: 0.315225.\n",
      "episode: 3626   score: 510.0  epsilon: 1.0    steps: 368  evaluation reward: 345.75\n",
      "episode: 3627   score: 550.0  epsilon: 1.0    steps: 576  evaluation reward: 348.05\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9751: Policy loss: 0.392508. Value loss: 0.187774. Entropy: 0.304576.\n",
      "Iteration 9752: Policy loss: 0.390148. Value loss: 0.062044. Entropy: 0.303828.\n",
      "Iteration 9753: Policy loss: 0.383312. Value loss: 0.043281. Entropy: 0.303198.\n",
      "episode: 3628   score: 610.0  epsilon: 1.0    steps: 120  evaluation reward: 350.55\n",
      "episode: 3629   score: 395.0  epsilon: 1.0    steps: 352  evaluation reward: 351.85\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9754: Policy loss: -0.169160. Value loss: 0.153313. Entropy: 0.310169.\n",
      "Iteration 9755: Policy loss: -0.171226. Value loss: 0.067000. Entropy: 0.310377.\n",
      "Iteration 9756: Policy loss: -0.184240. Value loss: 0.047548. Entropy: 0.311730.\n",
      "episode: 3630   score: 695.0  epsilon: 1.0    steps: 80  evaluation reward: 354.15\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9757: Policy loss: 0.265383. Value loss: 0.172420. Entropy: 0.311772.\n",
      "Iteration 9758: Policy loss: 0.256829. Value loss: 0.078133. Entropy: 0.312466.\n",
      "Iteration 9759: Policy loss: 0.253812. Value loss: 0.050553. Entropy: 0.311208.\n",
      "episode: 3631   score: 470.0  epsilon: 1.0    steps: 112  evaluation reward: 356.45\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9760: Policy loss: 0.011402. Value loss: 0.101497. Entropy: 0.311826.\n",
      "Iteration 9761: Policy loss: 0.005357. Value loss: 0.041582. Entropy: 0.310881.\n",
      "Iteration 9762: Policy loss: -0.003226. Value loss: 0.033287. Entropy: 0.310537.\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9763: Policy loss: -0.102793. Value loss: 0.293220. Entropy: 0.304005.\n",
      "Iteration 9764: Policy loss: -0.124906. Value loss: 0.207603. Entropy: 0.303412.\n",
      "Iteration 9765: Policy loss: -0.126753. Value loss: 0.175961. Entropy: 0.302271.\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9766: Policy loss: 0.034581. Value loss: 0.086847. Entropy: 0.308990.\n",
      "Iteration 9767: Policy loss: 0.031369. Value loss: 0.031462. Entropy: 0.308033.\n",
      "Iteration 9768: Policy loss: 0.030013. Value loss: 0.022219. Entropy: 0.308967.\n",
      "episode: 3632   score: 420.0  epsilon: 1.0    steps: 112  evaluation reward: 355.55\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9769: Policy loss: 0.100909. Value loss: 0.128474. Entropy: 0.308831.\n",
      "Iteration 9770: Policy loss: 0.093516. Value loss: 0.037043. Entropy: 0.310424.\n",
      "Iteration 9771: Policy loss: 0.088946. Value loss: 0.026727. Entropy: 0.310038.\n",
      "episode: 3633   score: 280.0  epsilon: 1.0    steps: 512  evaluation reward: 355.75\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9772: Policy loss: 0.254070. Value loss: 0.129600. Entropy: 0.308079.\n",
      "Iteration 9773: Policy loss: 0.249145. Value loss: 0.052024. Entropy: 0.306249.\n",
      "Iteration 9774: Policy loss: 0.238779. Value loss: 0.039675. Entropy: 0.306381.\n",
      "episode: 3634   score: 390.0  epsilon: 1.0    steps: 944  evaluation reward: 355.75\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9775: Policy loss: 0.110130. Value loss: 0.150177. Entropy: 0.309738.\n",
      "Iteration 9776: Policy loss: 0.104029. Value loss: 0.060144. Entropy: 0.309070.\n",
      "Iteration 9777: Policy loss: 0.095017. Value loss: 0.039647. Entropy: 0.309411.\n",
      "episode: 3635   score: 335.0  epsilon: 1.0    steps: 208  evaluation reward: 355.85\n",
      "episode: 3636   score: 270.0  epsilon: 1.0    steps: 752  evaluation reward: 354.05\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9778: Policy loss: 0.244525. Value loss: 0.117523. Entropy: 0.310740.\n",
      "Iteration 9779: Policy loss: 0.241154. Value loss: 0.042058. Entropy: 0.310419.\n",
      "Iteration 9780: Policy loss: 0.229081. Value loss: 0.027119. Entropy: 0.309283.\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9781: Policy loss: 0.148114. Value loss: 0.064895. Entropy: 0.308440.\n",
      "Iteration 9782: Policy loss: 0.146940. Value loss: 0.028803. Entropy: 0.307654.\n",
      "Iteration 9783: Policy loss: 0.143385. Value loss: 0.020453. Entropy: 0.307918.\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9784: Policy loss: 0.054591. Value loss: 0.108279. Entropy: 0.313074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9785: Policy loss: 0.046116. Value loss: 0.038341. Entropy: 0.311626.\n",
      "Iteration 9786: Policy loss: 0.045701. Value loss: 0.028605. Entropy: 0.312195.\n",
      "episode: 3637   score: 500.0  epsilon: 1.0    steps: 312  evaluation reward: 352.15\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9787: Policy loss: -0.113680. Value loss: 0.131828. Entropy: 0.306410.\n",
      "Iteration 9788: Policy loss: -0.126647. Value loss: 0.066892. Entropy: 0.306880.\n",
      "Iteration 9789: Policy loss: -0.122296. Value loss: 0.049507. Entropy: 0.305855.\n",
      "episode: 3638   score: 700.0  epsilon: 1.0    steps: 248  evaluation reward: 356.5\n",
      "episode: 3639   score: 365.0  epsilon: 1.0    steps: 1016  evaluation reward: 358.05\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9790: Policy loss: -0.550761. Value loss: 0.643254. Entropy: 0.305490.\n",
      "Iteration 9791: Policy loss: -0.569718. Value loss: 0.435715. Entropy: 0.307076.\n",
      "Iteration 9792: Policy loss: -0.514030. Value loss: 0.303698. Entropy: 0.305993.\n",
      "episode: 3640   score: 260.0  epsilon: 1.0    steps: 984  evaluation reward: 352.45\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9793: Policy loss: -0.061724. Value loss: 0.080087. Entropy: 0.310965.\n",
      "Iteration 9794: Policy loss: -0.062102. Value loss: 0.035745. Entropy: 0.310773.\n",
      "Iteration 9795: Policy loss: -0.075962. Value loss: 0.022452. Entropy: 0.310010.\n",
      "episode: 3641   score: 315.0  epsilon: 1.0    steps: 776  evaluation reward: 353.0\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9796: Policy loss: 0.027588. Value loss: 0.116392. Entropy: 0.311167.\n",
      "Iteration 9797: Policy loss: 0.018353. Value loss: 0.066648. Entropy: 0.310787.\n",
      "Iteration 9798: Policy loss: 0.012038. Value loss: 0.044363. Entropy: 0.310721.\n",
      "episode: 3642   score: 515.0  epsilon: 1.0    steps: 216  evaluation reward: 353.05\n",
      "episode: 3643   score: 555.0  epsilon: 1.0    steps: 528  evaluation reward: 356.8\n",
      "Training network. lr: 0.000175. clip: 0.070057\n",
      "Iteration 9799: Policy loss: 0.068514. Value loss: 0.083337. Entropy: 0.307074.\n",
      "Iteration 9800: Policy loss: 0.061373. Value loss: 0.040052. Entropy: 0.306480.\n",
      "Iteration 9801: Policy loss: 0.051919. Value loss: 0.027700. Entropy: 0.306468.\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9802: Policy loss: -0.240008. Value loss: 0.319284. Entropy: 0.310243.\n",
      "Iteration 9803: Policy loss: -0.258594. Value loss: 0.230201. Entropy: 0.309734.\n",
      "Iteration 9804: Policy loss: -0.246144. Value loss: 0.180106. Entropy: 0.309475.\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9805: Policy loss: -0.097841. Value loss: 0.092083. Entropy: 0.311191.\n",
      "Iteration 9806: Policy loss: -0.096889. Value loss: 0.040662. Entropy: 0.310110.\n",
      "Iteration 9807: Policy loss: -0.104473. Value loss: 0.027766. Entropy: 0.310732.\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9808: Policy loss: -0.347958. Value loss: 0.383830. Entropy: 0.309222.\n",
      "Iteration 9809: Policy loss: -0.368641. Value loss: 0.220573. Entropy: 0.309051.\n",
      "Iteration 9810: Policy loss: -0.360129. Value loss: 0.149493. Entropy: 0.309697.\n",
      "episode: 3644   score: 820.0  epsilon: 1.0    steps: 480  evaluation reward: 362.9\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9811: Policy loss: -0.034004. Value loss: 0.289658. Entropy: 0.306952.\n",
      "Iteration 9812: Policy loss: -0.042196. Value loss: 0.167763. Entropy: 0.305304.\n",
      "Iteration 9813: Policy loss: -0.037738. Value loss: 0.112167. Entropy: 0.304987.\n",
      "episode: 3645   score: 300.0  epsilon: 1.0    steps: 280  evaluation reward: 359.55\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9814: Policy loss: 0.035460. Value loss: 0.165853. Entropy: 0.306665.\n",
      "Iteration 9815: Policy loss: 0.029841. Value loss: 0.055171. Entropy: 0.306533.\n",
      "Iteration 9816: Policy loss: 0.017709. Value loss: 0.035970. Entropy: 0.307812.\n",
      "episode: 3646   score: 230.0  epsilon: 1.0    steps: 824  evaluation reward: 358.7\n",
      "episode: 3647   score: 325.0  epsilon: 1.0    steps: 840  evaluation reward: 359.35\n",
      "episode: 3648   score: 430.0  epsilon: 1.0    steps: 856  evaluation reward: 362.1\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9817: Policy loss: -0.038493. Value loss: 0.120271. Entropy: 0.300607.\n",
      "Iteration 9818: Policy loss: -0.038306. Value loss: 0.048706. Entropy: 0.301481.\n",
      "Iteration 9819: Policy loss: -0.047218. Value loss: 0.038624. Entropy: 0.300452.\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9820: Policy loss: 0.108384. Value loss: 0.071745. Entropy: 0.313568.\n",
      "Iteration 9821: Policy loss: 0.108759. Value loss: 0.033387. Entropy: 0.311792.\n",
      "Iteration 9822: Policy loss: 0.104537. Value loss: 0.025409. Entropy: 0.311666.\n",
      "episode: 3649   score: 700.0  epsilon: 1.0    steps: 960  evaluation reward: 364.4\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9823: Policy loss: 0.042307. Value loss: 0.083424. Entropy: 0.311633.\n",
      "Iteration 9824: Policy loss: 0.043671. Value loss: 0.033718. Entropy: 0.313156.\n",
      "Iteration 9825: Policy loss: 0.041050. Value loss: 0.024552. Entropy: 0.312450.\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9826: Policy loss: -0.785313. Value loss: 0.494060. Entropy: 0.315789.\n",
      "Iteration 9827: Policy loss: -0.834133. Value loss: 0.185591. Entropy: 0.314365.\n",
      "Iteration 9828: Policy loss: -0.837018. Value loss: 0.103005. Entropy: 0.315141.\n",
      "episode: 3650   score: 870.0  epsilon: 1.0    steps: 776  evaluation reward: 369.2\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9829: Policy loss: 0.206819. Value loss: 0.331559. Entropy: 0.309681.\n",
      "Iteration 9830: Policy loss: 0.192281. Value loss: 0.073918. Entropy: 0.307729.\n",
      "Iteration 9831: Policy loss: 0.179532. Value loss: 0.036687. Entropy: 0.308649.\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9832: Policy loss: 0.129203. Value loss: 0.101626. Entropy: 0.308029.\n",
      "Iteration 9833: Policy loss: 0.110353. Value loss: 0.033169. Entropy: 0.308692.\n",
      "Iteration 9834: Policy loss: 0.114050. Value loss: 0.021583. Entropy: 0.308023.\n",
      "now time :  2019-09-06 00:25:09.507064\n",
      "episode: 3651   score: 440.0  epsilon: 1.0    steps: 32  evaluation reward: 369.95\n",
      "episode: 3652   score: 210.0  epsilon: 1.0    steps: 680  evaluation reward: 368.95\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9835: Policy loss: -0.032263. Value loss: 0.167351. Entropy: 0.305914.\n",
      "Iteration 9836: Policy loss: -0.039724. Value loss: 0.051656. Entropy: 0.304960.\n",
      "Iteration 9837: Policy loss: -0.048117. Value loss: 0.040510. Entropy: 0.304591.\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9838: Policy loss: 0.208068. Value loss: 0.101486. Entropy: 0.312601.\n",
      "Iteration 9839: Policy loss: 0.196642. Value loss: 0.045489. Entropy: 0.309682.\n",
      "Iteration 9840: Policy loss: 0.195113. Value loss: 0.029826. Entropy: 0.309733.\n",
      "episode: 3653   score: 575.0  epsilon: 1.0    steps: 888  evaluation reward: 372.6\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9841: Policy loss: -0.130299. Value loss: 0.138986. Entropy: 0.308681.\n",
      "Iteration 9842: Policy loss: -0.129634. Value loss: 0.063528. Entropy: 0.307423.\n",
      "Iteration 9843: Policy loss: -0.144939. Value loss: 0.042690. Entropy: 0.307823.\n",
      "episode: 3654   score: 560.0  epsilon: 1.0    steps: 40  evaluation reward: 373.35\n",
      "episode: 3655   score: 545.0  epsilon: 1.0    steps: 608  evaluation reward: 373.9\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9844: Policy loss: 0.151172. Value loss: 0.229619. Entropy: 0.303691.\n",
      "Iteration 9845: Policy loss: 0.139498. Value loss: 0.058707. Entropy: 0.301262.\n",
      "Iteration 9846: Policy loss: 0.132540. Value loss: 0.035364. Entropy: 0.303130.\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9847: Policy loss: 0.188328. Value loss: 0.141722. Entropy: 0.312944.\n",
      "Iteration 9848: Policy loss: 0.176492. Value loss: 0.043788. Entropy: 0.313982.\n",
      "Iteration 9849: Policy loss: 0.179873. Value loss: 0.032256. Entropy: 0.313480.\n",
      "episode: 3656   score: 390.0  epsilon: 1.0    steps: 648  evaluation reward: 375.7\n",
      "episode: 3657   score: 135.0  epsilon: 1.0    steps: 664  evaluation reward: 374.45\n",
      "Training network. lr: 0.000175. clip: 0.069901\n",
      "Iteration 9850: Policy loss: 0.304522. Value loss: 0.192298. Entropy: 0.306885.\n",
      "Iteration 9851: Policy loss: 0.288933. Value loss: 0.082481. Entropy: 0.305707.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9852: Policy loss: 0.290766. Value loss: 0.057390. Entropy: 0.305560.\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9853: Policy loss: 0.076630. Value loss: 0.092515. Entropy: 0.307496.\n",
      "Iteration 9854: Policy loss: 0.066538. Value loss: 0.041750. Entropy: 0.307500.\n",
      "Iteration 9855: Policy loss: 0.063941. Value loss: 0.029234. Entropy: 0.306309.\n",
      "episode: 3658   score: 590.0  epsilon: 1.0    steps: 96  evaluation reward: 378.55\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9856: Policy loss: 0.036871. Value loss: 0.054985. Entropy: 0.304965.\n",
      "Iteration 9857: Policy loss: 0.036580. Value loss: 0.023249. Entropy: 0.304023.\n",
      "Iteration 9858: Policy loss: 0.027809. Value loss: 0.016075. Entropy: 0.304854.\n",
      "episode: 3659   score: 210.0  epsilon: 1.0    steps: 656  evaluation reward: 378.55\n",
      "episode: 3660   score: 285.0  epsilon: 1.0    steps: 928  evaluation reward: 375.75\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9859: Policy loss: 0.154271. Value loss: 0.085849. Entropy: 0.308011.\n",
      "Iteration 9860: Policy loss: 0.140733. Value loss: 0.033316. Entropy: 0.306190.\n",
      "Iteration 9861: Policy loss: 0.143430. Value loss: 0.027481. Entropy: 0.306790.\n",
      "episode: 3661   score: 225.0  epsilon: 1.0    steps: 248  evaluation reward: 375.75\n",
      "episode: 3662   score: 420.0  epsilon: 1.0    steps: 640  evaluation reward: 377.1\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9862: Policy loss: 0.409837. Value loss: 0.111457. Entropy: 0.306561.\n",
      "Iteration 9863: Policy loss: 0.400478. Value loss: 0.036342. Entropy: 0.304794.\n",
      "Iteration 9864: Policy loss: 0.391000. Value loss: 0.025395. Entropy: 0.304030.\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9865: Policy loss: 0.041585. Value loss: 0.105525. Entropy: 0.316518.\n",
      "Iteration 9866: Policy loss: 0.030615. Value loss: 0.039412. Entropy: 0.317126.\n",
      "Iteration 9867: Policy loss: 0.027845. Value loss: 0.025293. Entropy: 0.315346.\n",
      "episode: 3663   score: 285.0  epsilon: 1.0    steps: 72  evaluation reward: 377.85\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9868: Policy loss: 0.228810. Value loss: 0.096507. Entropy: 0.307809.\n",
      "Iteration 9869: Policy loss: 0.229315. Value loss: 0.044260. Entropy: 0.307549.\n",
      "Iteration 9870: Policy loss: 0.223297. Value loss: 0.032769. Entropy: 0.306783.\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9871: Policy loss: 0.289385. Value loss: 0.083902. Entropy: 0.306081.\n",
      "Iteration 9872: Policy loss: 0.279390. Value loss: 0.031628. Entropy: 0.304822.\n",
      "Iteration 9873: Policy loss: 0.274799. Value loss: 0.021576. Entropy: 0.305542.\n",
      "episode: 3664   score: 225.0  epsilon: 1.0    steps: 24  evaluation reward: 378.75\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9874: Policy loss: -0.126880. Value loss: 0.082852. Entropy: 0.308996.\n",
      "Iteration 9875: Policy loss: -0.130514. Value loss: 0.036507. Entropy: 0.309123.\n",
      "Iteration 9876: Policy loss: -0.131108. Value loss: 0.027015. Entropy: 0.309031.\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9877: Policy loss: -0.023105. Value loss: 0.087748. Entropy: 0.310960.\n",
      "Iteration 9878: Policy loss: -0.027352. Value loss: 0.025262. Entropy: 0.310545.\n",
      "Iteration 9879: Policy loss: -0.037548. Value loss: 0.015337. Entropy: 0.311680.\n",
      "episode: 3665   score: 210.0  epsilon: 1.0    steps: 768  evaluation reward: 376.9\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9880: Policy loss: 0.198073. Value loss: 0.070760. Entropy: 0.310887.\n",
      "Iteration 9881: Policy loss: 0.187635. Value loss: 0.035009. Entropy: 0.310610.\n",
      "Iteration 9882: Policy loss: 0.184884. Value loss: 0.025562. Entropy: 0.309948.\n",
      "episode: 3666   score: 425.0  epsilon: 1.0    steps: 624  evaluation reward: 379.25\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9883: Policy loss: -0.020693. Value loss: 0.114188. Entropy: 0.310115.\n",
      "Iteration 9884: Policy loss: -0.033125. Value loss: 0.037881. Entropy: 0.310067.\n",
      "Iteration 9885: Policy loss: -0.042379. Value loss: 0.024817. Entropy: 0.308605.\n",
      "episode: 3667   score: 210.0  epsilon: 1.0    steps: 248  evaluation reward: 379.1\n",
      "episode: 3668   score: 365.0  epsilon: 1.0    steps: 440  evaluation reward: 380.8\n",
      "episode: 3669   score: 375.0  epsilon: 1.0    steps: 536  evaluation reward: 382.3\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9886: Policy loss: 0.096594. Value loss: 0.070317. Entropy: 0.301653.\n",
      "Iteration 9887: Policy loss: 0.096749. Value loss: 0.025027. Entropy: 0.301551.\n",
      "Iteration 9888: Policy loss: 0.087272. Value loss: 0.018819. Entropy: 0.299640.\n",
      "episode: 3670   score: 360.0  epsilon: 1.0    steps: 32  evaluation reward: 382.45\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9889: Policy loss: -0.407627. Value loss: 0.331381. Entropy: 0.308651.\n",
      "Iteration 9890: Policy loss: -0.410777. Value loss: 0.199728. Entropy: 0.308454.\n",
      "Iteration 9891: Policy loss: -0.438045. Value loss: 0.154682. Entropy: 0.309265.\n",
      "episode: 3671   score: 495.0  epsilon: 1.0    steps: 472  evaluation reward: 385.25\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9892: Policy loss: 0.115340. Value loss: 0.095689. Entropy: 0.307415.\n",
      "Iteration 9893: Policy loss: 0.106405. Value loss: 0.048213. Entropy: 0.306402.\n",
      "Iteration 9894: Policy loss: 0.104315. Value loss: 0.040444. Entropy: 0.306213.\n",
      "episode: 3672   score: 260.0  epsilon: 1.0    steps: 912  evaluation reward: 385.6\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9895: Policy loss: -0.178641. Value loss: 0.478680. Entropy: 0.309449.\n",
      "Iteration 9896: Policy loss: -0.191935. Value loss: 0.358426. Entropy: 0.307861.\n",
      "Iteration 9897: Policy loss: -0.183800. Value loss: 0.297246. Entropy: 0.305302.\n",
      "Training network. lr: 0.000174. clip: 0.069744\n",
      "Iteration 9898: Policy loss: 0.034432. Value loss: 0.138459. Entropy: 0.305727.\n",
      "Iteration 9899: Policy loss: 0.021233. Value loss: 0.080776. Entropy: 0.305779.\n",
      "Iteration 9900: Policy loss: 0.026820. Value loss: 0.071024. Entropy: 0.305981.\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9901: Policy loss: -0.375893. Value loss: 0.502876. Entropy: 0.312619.\n",
      "Iteration 9902: Policy loss: -0.407401. Value loss: 0.339506. Entropy: 0.312401.\n",
      "Iteration 9903: Policy loss: -0.408640. Value loss: 0.223244. Entropy: 0.312233.\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9904: Policy loss: -0.211485. Value loss: 0.156183. Entropy: 0.307665.\n",
      "Iteration 9905: Policy loss: -0.215923. Value loss: 0.066780. Entropy: 0.308134.\n",
      "Iteration 9906: Policy loss: -0.220742. Value loss: 0.046822. Entropy: 0.307858.\n",
      "episode: 3673   score: 720.0  epsilon: 1.0    steps: 136  evaluation reward: 391.0\n",
      "episode: 3674   score: 115.0  epsilon: 1.0    steps: 648  evaluation reward: 390.35\n",
      "episode: 3675   score: 555.0  epsilon: 1.0    steps: 696  evaluation reward: 387.9\n",
      "episode: 3676   score: 265.0  epsilon: 1.0    steps: 928  evaluation reward: 387.7\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9907: Policy loss: 0.124201. Value loss: 0.375868. Entropy: 0.303943.\n",
      "Iteration 9908: Policy loss: 0.078139. Value loss: 0.101560. Entropy: 0.303778.\n",
      "Iteration 9909: Policy loss: 0.100989. Value loss: 0.067811. Entropy: 0.303547.\n",
      "episode: 3677   score: 615.0  epsilon: 1.0    steps: 640  evaluation reward: 387.2\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9910: Policy loss: 0.095707. Value loss: 0.124408. Entropy: 0.312869.\n",
      "Iteration 9911: Policy loss: 0.089609. Value loss: 0.053402. Entropy: 0.312863.\n",
      "Iteration 9912: Policy loss: 0.089819. Value loss: 0.039214. Entropy: 0.313016.\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9913: Policy loss: -0.041938. Value loss: 0.136839. Entropy: 0.311537.\n",
      "Iteration 9914: Policy loss: -0.050845. Value loss: 0.063709. Entropy: 0.311848.\n",
      "Iteration 9915: Policy loss: -0.049558. Value loss: 0.044394. Entropy: 0.311248.\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9916: Policy loss: -0.047260. Value loss: 0.182524. Entropy: 0.310382.\n",
      "Iteration 9917: Policy loss: -0.044574. Value loss: 0.073810. Entropy: 0.310954.\n",
      "Iteration 9918: Policy loss: -0.037812. Value loss: 0.045511. Entropy: 0.309078.\n",
      "episode: 3678   score: 565.0  epsilon: 1.0    steps: 496  evaluation reward: 390.75\n",
      "episode: 3679   score: 365.0  epsilon: 1.0    steps: 896  evaluation reward: 392.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9919: Policy loss: 0.177018. Value loss: 0.111635. Entropy: 0.302461.\n",
      "Iteration 9920: Policy loss: 0.174383. Value loss: 0.034369. Entropy: 0.302142.\n",
      "Iteration 9921: Policy loss: 0.172399. Value loss: 0.022944. Entropy: 0.300998.\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9922: Policy loss: -0.107175. Value loss: 0.209956. Entropy: 0.310845.\n",
      "Iteration 9923: Policy loss: -0.111131. Value loss: 0.052001. Entropy: 0.311165.\n",
      "Iteration 9924: Policy loss: -0.133107. Value loss: 0.034565. Entropy: 0.310897.\n",
      "episode: 3680   score: 820.0  epsilon: 1.0    steps: 880  evaluation reward: 398.35\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9925: Policy loss: 0.108236. Value loss: 0.166619. Entropy: 0.308123.\n",
      "Iteration 9926: Policy loss: 0.096767. Value loss: 0.062794. Entropy: 0.308342.\n",
      "Iteration 9927: Policy loss: 0.093394. Value loss: 0.044296. Entropy: 0.307436.\n",
      "episode: 3681   score: 240.0  epsilon: 1.0    steps: 352  evaluation reward: 397.4\n",
      "episode: 3682   score: 440.0  epsilon: 1.0    steps: 496  evaluation reward: 394.85\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9928: Policy loss: 0.262791. Value loss: 0.112058. Entropy: 0.307761.\n",
      "Iteration 9929: Policy loss: 0.242200. Value loss: 0.046498. Entropy: 0.305332.\n",
      "Iteration 9930: Policy loss: 0.246109. Value loss: 0.033460. Entropy: 0.305478.\n",
      "episode: 3683   score: 315.0  epsilon: 1.0    steps: 704  evaluation reward: 394.75\n",
      "episode: 3684   score: 545.0  epsilon: 1.0    steps: 992  evaluation reward: 396.1\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9931: Policy loss: 0.091352. Value loss: 0.098986. Entropy: 0.307999.\n",
      "Iteration 9932: Policy loss: 0.089401. Value loss: 0.044513. Entropy: 0.308325.\n",
      "Iteration 9933: Policy loss: 0.084232. Value loss: 0.034304. Entropy: 0.307609.\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9934: Policy loss: 0.062539. Value loss: 0.154010. Entropy: 0.312903.\n",
      "Iteration 9935: Policy loss: 0.056144. Value loss: 0.072764. Entropy: 0.311919.\n",
      "Iteration 9936: Policy loss: 0.056553. Value loss: 0.046586. Entropy: 0.311755.\n",
      "episode: 3685   score: 210.0  epsilon: 1.0    steps: 352  evaluation reward: 396.3\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9937: Policy loss: 0.034916. Value loss: 0.168183. Entropy: 0.311287.\n",
      "Iteration 9938: Policy loss: 0.034439. Value loss: 0.052083. Entropy: 0.310963.\n",
      "Iteration 9939: Policy loss: 0.025772. Value loss: 0.031986. Entropy: 0.310540.\n",
      "episode: 3686   score: 210.0  epsilon: 1.0    steps: 536  evaluation reward: 396.85\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9940: Policy loss: 0.132874. Value loss: 0.136339. Entropy: 0.307625.\n",
      "Iteration 9941: Policy loss: 0.125207. Value loss: 0.032907. Entropy: 0.306756.\n",
      "Iteration 9942: Policy loss: 0.105697. Value loss: 0.020994. Entropy: 0.306936.\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9943: Policy loss: 0.080853. Value loss: 0.103624. Entropy: 0.309782.\n",
      "Iteration 9944: Policy loss: 0.069924. Value loss: 0.039735. Entropy: 0.307881.\n",
      "Iteration 9945: Policy loss: 0.069854. Value loss: 0.029509. Entropy: 0.308404.\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9946: Policy loss: -0.248840. Value loss: 0.199942. Entropy: 0.305554.\n",
      "Iteration 9947: Policy loss: -0.266736. Value loss: 0.074592. Entropy: 0.304873.\n",
      "Iteration 9948: Policy loss: -0.267493. Value loss: 0.047202. Entropy: 0.305001.\n",
      "episode: 3687   score: 525.0  epsilon: 1.0    steps: 432  evaluation reward: 397.5\n",
      "Training network. lr: 0.000174. clip: 0.069596\n",
      "Iteration 9949: Policy loss: 0.393422. Value loss: 0.197354. Entropy: 0.309957.\n",
      "Iteration 9950: Policy loss: 0.385630. Value loss: 0.061267. Entropy: 0.308817.\n",
      "Iteration 9951: Policy loss: 0.352112. Value loss: 0.033146. Entropy: 0.311034.\n",
      "episode: 3688   score: 545.0  epsilon: 1.0    steps: 704  evaluation reward: 400.55\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9952: Policy loss: 0.173397. Value loss: 0.123827. Entropy: 0.311530.\n",
      "Iteration 9953: Policy loss: 0.169545. Value loss: 0.038442. Entropy: 0.311038.\n",
      "Iteration 9954: Policy loss: 0.161796. Value loss: 0.027120. Entropy: 0.310577.\n",
      "episode: 3689   score: 285.0  epsilon: 1.0    steps: 176  evaluation reward: 399.2\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9955: Policy loss: -0.155663. Value loss: 0.137204. Entropy: 0.308245.\n",
      "Iteration 9956: Policy loss: -0.170185. Value loss: 0.053931. Entropy: 0.306722.\n",
      "Iteration 9957: Policy loss: -0.170869. Value loss: 0.037019. Entropy: 0.308089.\n",
      "episode: 3690   score: 510.0  epsilon: 1.0    steps: 184  evaluation reward: 401.9\n",
      "episode: 3691   score: 365.0  epsilon: 1.0    steps: 416  evaluation reward: 403.6\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9958: Policy loss: 0.113003. Value loss: 0.114545. Entropy: 0.295761.\n",
      "Iteration 9959: Policy loss: 0.107719. Value loss: 0.056696. Entropy: 0.295916.\n",
      "Iteration 9960: Policy loss: 0.104057. Value loss: 0.040650. Entropy: 0.294317.\n",
      "episode: 3692   score: 375.0  epsilon: 1.0    steps: 456  evaluation reward: 404.9\n",
      "episode: 3693   score: 545.0  epsilon: 1.0    steps: 984  evaluation reward: 401.95\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9961: Policy loss: 0.041343. Value loss: 0.111950. Entropy: 0.302685.\n",
      "Iteration 9962: Policy loss: 0.043046. Value loss: 0.051006. Entropy: 0.302465.\n",
      "Iteration 9963: Policy loss: 0.035172. Value loss: 0.038063. Entropy: 0.302546.\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9964: Policy loss: 0.035982. Value loss: 0.106228. Entropy: 0.306210.\n",
      "Iteration 9965: Policy loss: 0.026779. Value loss: 0.053033. Entropy: 0.306530.\n",
      "Iteration 9966: Policy loss: 0.019962. Value loss: 0.031607. Entropy: 0.306939.\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9967: Policy loss: 0.157053. Value loss: 0.219333. Entropy: 0.312192.\n",
      "Iteration 9968: Policy loss: 0.129882. Value loss: 0.115003. Entropy: 0.311987.\n",
      "Iteration 9969: Policy loss: 0.120302. Value loss: 0.048055. Entropy: 0.310560.\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9970: Policy loss: 0.127071. Value loss: 0.115356. Entropy: 0.308953.\n",
      "Iteration 9971: Policy loss: 0.118672. Value loss: 0.040793. Entropy: 0.308974.\n",
      "Iteration 9972: Policy loss: 0.105663. Value loss: 0.029785. Entropy: 0.308185.\n",
      "episode: 3694   score: 435.0  epsilon: 1.0    steps: 232  evaluation reward: 402.75\n",
      "episode: 3695   score: 365.0  epsilon: 1.0    steps: 488  evaluation reward: 405.2\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9973: Policy loss: -0.264400. Value loss: 0.249751. Entropy: 0.295263.\n",
      "Iteration 9974: Policy loss: -0.234734. Value loss: 0.119366. Entropy: 0.293862.\n",
      "Iteration 9975: Policy loss: -0.238071. Value loss: 0.061821. Entropy: 0.295910.\n",
      "episode: 3696   score: 390.0  epsilon: 1.0    steps: 800  evaluation reward: 404.55\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9976: Policy loss: -0.250466. Value loss: 0.163829. Entropy: 0.302255.\n",
      "Iteration 9977: Policy loss: -0.265618. Value loss: 0.066431. Entropy: 0.301902.\n",
      "Iteration 9978: Policy loss: -0.272480. Value loss: 0.044684. Entropy: 0.303516.\n",
      "episode: 3697   score: 275.0  epsilon: 1.0    steps: 456  evaluation reward: 401.2\n",
      "episode: 3698   score: 325.0  epsilon: 1.0    steps: 600  evaluation reward: 401.75\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9979: Policy loss: 0.132320. Value loss: 0.219774. Entropy: 0.297137.\n",
      "Iteration 9980: Policy loss: 0.111796. Value loss: 0.089638. Entropy: 0.293308.\n",
      "Iteration 9981: Policy loss: 0.108867. Value loss: 0.062115. Entropy: 0.293399.\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9982: Policy loss: 0.156556. Value loss: 0.114921. Entropy: 0.316244.\n",
      "Iteration 9983: Policy loss: 0.152697. Value loss: 0.053357. Entropy: 0.316743.\n",
      "Iteration 9984: Policy loss: 0.144300. Value loss: 0.038193. Entropy: 0.316632.\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9985: Policy loss: -0.272809. Value loss: 0.193032. Entropy: 0.308196.\n",
      "Iteration 9986: Policy loss: -0.295354. Value loss: 0.093389. Entropy: 0.307503.\n",
      "Iteration 9987: Policy loss: -0.325913. Value loss: 0.063071. Entropy: 0.307866.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9988: Policy loss: 0.116272. Value loss: 0.399381. Entropy: 0.307599.\n",
      "Iteration 9989: Policy loss: 0.102691. Value loss: 0.097333. Entropy: 0.306687.\n",
      "Iteration 9990: Policy loss: 0.086384. Value loss: 0.058653. Entropy: 0.306675.\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9991: Policy loss: -0.190750. Value loss: 0.416521. Entropy: 0.309118.\n",
      "Iteration 9992: Policy loss: -0.220763. Value loss: 0.237532. Entropy: 0.308138.\n",
      "Iteration 9993: Policy loss: -0.233064. Value loss: 0.156933. Entropy: 0.307738.\n",
      "episode: 3699   score: 180.0  epsilon: 1.0    steps: 80  evaluation reward: 400.9\n",
      "episode: 3700   score: 870.0  epsilon: 1.0    steps: 608  evaluation reward: 405.4\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9994: Policy loss: -0.015835. Value loss: 0.201874. Entropy: 0.289550.\n",
      "Iteration 9995: Policy loss: -0.017647. Value loss: 0.081187. Entropy: 0.285005.\n",
      "Iteration 9996: Policy loss: -0.024077. Value loss: 0.058796. Entropy: 0.286378.\n",
      "now time :  2019-09-06 00:35:13.280619\n",
      "episode: 3701   score: 440.0  epsilon: 1.0    steps: 192  evaluation reward: 408.8\n",
      "episode: 3702   score: 415.0  epsilon: 1.0    steps: 208  evaluation reward: 411.15\n",
      "episode: 3703   score: 1010.0  epsilon: 1.0    steps: 528  evaluation reward: 418.65\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 9997: Policy loss: 0.041873. Value loss: 0.081274. Entropy: 0.276594.\n",
      "Iteration 9998: Policy loss: 0.038945. Value loss: 0.037419. Entropy: 0.276903.\n",
      "Iteration 9999: Policy loss: 0.034708. Value loss: 0.028460. Entropy: 0.278864.\n",
      "Training network. lr: 0.000174. clip: 0.069440\n",
      "Iteration 10000: Policy loss: -0.422565. Value loss: 0.280661. Entropy: 0.309283.\n",
      "Iteration 10001: Policy loss: -0.434638. Value loss: 0.129079. Entropy: 0.310586.\n",
      "Iteration 10002: Policy loss: -0.441498. Value loss: 0.096864. Entropy: 0.310391.\n",
      "episode: 3704   score: 590.0  epsilon: 1.0    steps: 344  evaluation reward: 418.85\n",
      "episode: 3705   score: 765.0  epsilon: 1.0    steps: 760  evaluation reward: 424.4\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10003: Policy loss: 0.380466. Value loss: 0.216482. Entropy: 0.280612.\n",
      "Iteration 10004: Policy loss: 0.381285. Value loss: 0.080834. Entropy: 0.280127.\n",
      "Iteration 10005: Policy loss: 0.373039. Value loss: 0.061349. Entropy: 0.279436.\n",
      "episode: 3706   score: 120.0  epsilon: 1.0    steps: 416  evaluation reward: 423.0\n",
      "episode: 3707   score: 515.0  epsilon: 1.0    steps: 1024  evaluation reward: 422.85\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10006: Policy loss: 0.256093. Value loss: 0.105524. Entropy: 0.300195.\n",
      "Iteration 10007: Policy loss: 0.250415. Value loss: 0.048707. Entropy: 0.297837.\n",
      "Iteration 10008: Policy loss: 0.249134. Value loss: 0.032056. Entropy: 0.299488.\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10009: Policy loss: 0.078026. Value loss: 0.088088. Entropy: 0.302148.\n",
      "Iteration 10010: Policy loss: 0.064325. Value loss: 0.031715. Entropy: 0.303170.\n",
      "Iteration 10011: Policy loss: 0.067982. Value loss: 0.024609. Entropy: 0.303348.\n",
      "episode: 3708   score: 210.0  epsilon: 1.0    steps: 432  evaluation reward: 420.1\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10012: Policy loss: -0.154822. Value loss: 0.121374. Entropy: 0.300084.\n",
      "Iteration 10013: Policy loss: -0.160177. Value loss: 0.047312. Entropy: 0.301261.\n",
      "Iteration 10014: Policy loss: -0.173514. Value loss: 0.033767. Entropy: 0.302228.\n",
      "episode: 3709   score: 210.0  epsilon: 1.0    steps: 744  evaluation reward: 419.6\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10015: Policy loss: 0.014685. Value loss: 0.238728. Entropy: 0.306526.\n",
      "Iteration 10016: Policy loss: 0.016530. Value loss: 0.101988. Entropy: 0.306463.\n",
      "Iteration 10017: Policy loss: -0.003029. Value loss: 0.091855. Entropy: 0.306024.\n",
      "episode: 3710   score: 455.0  epsilon: 1.0    steps: 688  evaluation reward: 422.05\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10018: Policy loss: 0.266943. Value loss: 0.256522. Entropy: 0.302642.\n",
      "Iteration 10019: Policy loss: 0.253663. Value loss: 0.058771. Entropy: 0.303415.\n",
      "Iteration 10020: Policy loss: 0.255054. Value loss: 0.029603. Entropy: 0.302397.\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10021: Policy loss: 0.398771. Value loss: 0.150778. Entropy: 0.304869.\n",
      "Iteration 10022: Policy loss: 0.389679. Value loss: 0.042007. Entropy: 0.305246.\n",
      "Iteration 10023: Policy loss: 0.386100. Value loss: 0.025859. Entropy: 0.304032.\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10024: Policy loss: 0.137634. Value loss: 0.128355. Entropy: 0.307834.\n",
      "Iteration 10025: Policy loss: 0.131197. Value loss: 0.058588. Entropy: 0.307971.\n",
      "Iteration 10026: Policy loss: 0.129667. Value loss: 0.040941. Entropy: 0.307723.\n",
      "episode: 3711   score: 390.0  epsilon: 1.0    steps: 368  evaluation reward: 422.9\n",
      "episode: 3712   score: 215.0  epsilon: 1.0    steps: 472  evaluation reward: 417.2\n",
      "episode: 3713   score: 210.0  epsilon: 1.0    steps: 552  evaluation reward: 416.45\n",
      "episode: 3714   score: 345.0  epsilon: 1.0    steps: 936  evaluation reward: 415.65\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10027: Policy loss: 0.184011. Value loss: 0.119435. Entropy: 0.298903.\n",
      "Iteration 10028: Policy loss: 0.181068. Value loss: 0.063186. Entropy: 0.301336.\n",
      "Iteration 10029: Policy loss: 0.178268. Value loss: 0.047152. Entropy: 0.300923.\n",
      "episode: 3715   score: 365.0  epsilon: 1.0    steps: 640  evaluation reward: 416.7\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10030: Policy loss: 0.127780. Value loss: 0.080000. Entropy: 0.308513.\n",
      "Iteration 10031: Policy loss: 0.123008. Value loss: 0.047062. Entropy: 0.306583.\n",
      "Iteration 10032: Policy loss: 0.121604. Value loss: 0.033563. Entropy: 0.307667.\n",
      "episode: 3716   score: 290.0  epsilon: 1.0    steps: 296  evaluation reward: 417.0\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10033: Policy loss: 0.083968. Value loss: 0.088380. Entropy: 0.309735.\n",
      "Iteration 10034: Policy loss: 0.076238. Value loss: 0.044379. Entropy: 0.310499.\n",
      "Iteration 10035: Policy loss: 0.077056. Value loss: 0.036176. Entropy: 0.308354.\n",
      "episode: 3717   score: 530.0  epsilon: 1.0    steps: 616  evaluation reward: 417.7\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10036: Policy loss: 0.492208. Value loss: 0.126672. Entropy: 0.309485.\n",
      "Iteration 10037: Policy loss: 0.490934. Value loss: 0.045175. Entropy: 0.307820.\n",
      "Iteration 10038: Policy loss: 0.483660. Value loss: 0.033807. Entropy: 0.308527.\n",
      "episode: 3718   score: 210.0  epsilon: 1.0    steps: 368  evaluation reward: 415.65\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10039: Policy loss: -0.060046. Value loss: 0.083796. Entropy: 0.307437.\n",
      "Iteration 10040: Policy loss: -0.067119. Value loss: 0.040492. Entropy: 0.304684.\n",
      "Iteration 10041: Policy loss: -0.071585. Value loss: 0.025916. Entropy: 0.306062.\n",
      "episode: 3719   score: 265.0  epsilon: 1.0    steps: 328  evaluation reward: 413.45\n",
      "episode: 3720   score: 210.0  epsilon: 1.0    steps: 704  evaluation reward: 412.8\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10042: Policy loss: 0.224644. Value loss: 0.126777. Entropy: 0.305263.\n",
      "Iteration 10043: Policy loss: 0.215664. Value loss: 0.051842. Entropy: 0.303483.\n",
      "Iteration 10044: Policy loss: 0.223187. Value loss: 0.033858. Entropy: 0.304958.\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10045: Policy loss: -0.066251. Value loss: 0.153283. Entropy: 0.307634.\n",
      "Iteration 10046: Policy loss: -0.073057. Value loss: 0.057691. Entropy: 0.307798.\n",
      "Iteration 10047: Policy loss: -0.076372. Value loss: 0.041908. Entropy: 0.307935.\n",
      "episode: 3721   score: 270.0  epsilon: 1.0    steps: 704  evaluation reward: 408.8\n",
      "episode: 3722   score: 375.0  epsilon: 1.0    steps: 904  evaluation reward: 410.15\n",
      "Training network. lr: 0.000173. clip: 0.069283\n",
      "Iteration 10048: Policy loss: -0.090147. Value loss: 0.351180. Entropy: 0.301103.\n",
      "Iteration 10049: Policy loss: -0.096309. Value loss: 0.197034. Entropy: 0.300522.\n",
      "Iteration 10050: Policy loss: -0.108226. Value loss: 0.137810. Entropy: 0.300983.\n",
      "episode: 3723   score: 330.0  epsilon: 1.0    steps: 200  evaluation reward: 410.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10051: Policy loss: 0.034174. Value loss: 0.053370. Entropy: 0.310823.\n",
      "Iteration 10052: Policy loss: 0.030410. Value loss: 0.029600. Entropy: 0.311139.\n",
      "Iteration 10053: Policy loss: 0.029758. Value loss: 0.023448. Entropy: 0.310131.\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10054: Policy loss: 0.091443. Value loss: 0.166064. Entropy: 0.306769.\n",
      "Iteration 10055: Policy loss: 0.083860. Value loss: 0.073983. Entropy: 0.306727.\n",
      "Iteration 10056: Policy loss: 0.078636. Value loss: 0.056154. Entropy: 0.306041.\n",
      "episode: 3724   score: 240.0  epsilon: 1.0    steps: 80  evaluation reward: 410.1\n",
      "episode: 3725   score: 315.0  epsilon: 1.0    steps: 752  evaluation reward: 408.65\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10057: Policy loss: 0.102531. Value loss: 0.079356. Entropy: 0.296921.\n",
      "Iteration 10058: Policy loss: 0.096990. Value loss: 0.033257. Entropy: 0.295212.\n",
      "Iteration 10059: Policy loss: 0.095323. Value loss: 0.028269. Entropy: 0.293011.\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10060: Policy loss: -0.114602. Value loss: 0.160480. Entropy: 0.304951.\n",
      "Iteration 10061: Policy loss: -0.122392. Value loss: 0.053790. Entropy: 0.305599.\n",
      "Iteration 10062: Policy loss: -0.115250. Value loss: 0.035811. Entropy: 0.305890.\n",
      "episode: 3726   score: 600.0  epsilon: 1.0    steps: 544  evaluation reward: 409.55\n",
      "episode: 3727   score: 345.0  epsilon: 1.0    steps: 864  evaluation reward: 407.5\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10063: Policy loss: -0.142273. Value loss: 0.322973. Entropy: 0.296242.\n",
      "Iteration 10064: Policy loss: -0.143222. Value loss: 0.184027. Entropy: 0.294503.\n",
      "Iteration 10065: Policy loss: -0.174928. Value loss: 0.156255. Entropy: 0.294216.\n",
      "episode: 3728   score: 150.0  epsilon: 1.0    steps: 840  evaluation reward: 402.9\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10066: Policy loss: 0.052257. Value loss: 0.060042. Entropy: 0.307295.\n",
      "Iteration 10067: Policy loss: 0.045895. Value loss: 0.026732. Entropy: 0.305484.\n",
      "Iteration 10068: Policy loss: 0.046552. Value loss: 0.021100. Entropy: 0.304592.\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10069: Policy loss: 0.093526. Value loss: 0.093811. Entropy: 0.306501.\n",
      "Iteration 10070: Policy loss: 0.088906. Value loss: 0.037750. Entropy: 0.307302.\n",
      "Iteration 10071: Policy loss: 0.084375. Value loss: 0.027255. Entropy: 0.306726.\n",
      "episode: 3729   score: 320.0  epsilon: 1.0    steps: 256  evaluation reward: 402.15\n",
      "episode: 3730   score: 350.0  epsilon: 1.0    steps: 280  evaluation reward: 398.7\n",
      "episode: 3731   score: 335.0  epsilon: 1.0    steps: 832  evaluation reward: 397.35\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10072: Policy loss: 0.213415. Value loss: 0.104030. Entropy: 0.291086.\n",
      "Iteration 10073: Policy loss: 0.206179. Value loss: 0.038437. Entropy: 0.291101.\n",
      "Iteration 10074: Policy loss: 0.208282. Value loss: 0.031457. Entropy: 0.290700.\n",
      "episode: 3732   score: 670.0  epsilon: 1.0    steps: 1024  evaluation reward: 399.85\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10075: Policy loss: 0.032421. Value loss: 0.093894. Entropy: 0.304251.\n",
      "Iteration 10076: Policy loss: 0.019516. Value loss: 0.042831. Entropy: 0.304140.\n",
      "Iteration 10077: Policy loss: 0.019500. Value loss: 0.028560. Entropy: 0.303994.\n",
      "episode: 3733   score: 240.0  epsilon: 1.0    steps: 368  evaluation reward: 399.45\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10078: Policy loss: 0.031402. Value loss: 0.097992. Entropy: 0.299028.\n",
      "Iteration 10079: Policy loss: 0.017382. Value loss: 0.039814. Entropy: 0.298997.\n",
      "Iteration 10080: Policy loss: 0.016980. Value loss: 0.024214. Entropy: 0.298479.\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10081: Policy loss: -0.017233. Value loss: 0.109390. Entropy: 0.307370.\n",
      "Iteration 10082: Policy loss: -0.023384. Value loss: 0.042174. Entropy: 0.307538.\n",
      "Iteration 10083: Policy loss: -0.028558. Value loss: 0.027414. Entropy: 0.306838.\n",
      "episode: 3734   score: 260.0  epsilon: 1.0    steps: 216  evaluation reward: 398.15\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10084: Policy loss: 0.110675. Value loss: 0.067585. Entropy: 0.300457.\n",
      "Iteration 10085: Policy loss: 0.102007. Value loss: 0.025594. Entropy: 0.298729.\n",
      "Iteration 10086: Policy loss: 0.100912. Value loss: 0.018368. Entropy: 0.298908.\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10087: Policy loss: 0.334486. Value loss: 0.131640. Entropy: 0.304241.\n",
      "Iteration 10088: Policy loss: 0.334741. Value loss: 0.049384. Entropy: 0.303049.\n",
      "Iteration 10089: Policy loss: 0.334539. Value loss: 0.032135. Entropy: 0.301688.\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10090: Policy loss: 0.154330. Value loss: 0.100888. Entropy: 0.302651.\n",
      "Iteration 10091: Policy loss: 0.150751. Value loss: 0.046298. Entropy: 0.302054.\n",
      "Iteration 10092: Policy loss: 0.150951. Value loss: 0.035791. Entropy: 0.301230.\n",
      "episode: 3735   score: 345.0  epsilon: 1.0    steps: 552  evaluation reward: 398.25\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10093: Policy loss: 0.166624. Value loss: 0.115918. Entropy: 0.296573.\n",
      "Iteration 10094: Policy loss: 0.159497. Value loss: 0.047477. Entropy: 0.294445.\n",
      "Iteration 10095: Policy loss: 0.156447. Value loss: 0.030639. Entropy: 0.296001.\n",
      "episode: 3736   score: 305.0  epsilon: 1.0    steps: 16  evaluation reward: 398.6\n",
      "episode: 3737   score: 335.0  epsilon: 1.0    steps: 168  evaluation reward: 396.95\n",
      "episode: 3738   score: 210.0  epsilon: 1.0    steps: 768  evaluation reward: 392.05\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10096: Policy loss: 0.251341. Value loss: 0.107552. Entropy: 0.292402.\n",
      "Iteration 10097: Policy loss: 0.238451. Value loss: 0.041833. Entropy: 0.290139.\n",
      "Iteration 10098: Policy loss: 0.233283. Value loss: 0.031609. Entropy: 0.291334.\n",
      "episode: 3739   score: 285.0  epsilon: 1.0    steps: 64  evaluation reward: 391.25\n",
      "episode: 3740   score: 500.0  epsilon: 1.0    steps: 384  evaluation reward: 393.65\n",
      "episode: 3741   score: 330.0  epsilon: 1.0    steps: 888  evaluation reward: 393.8\n",
      "Training network. lr: 0.000173. clip: 0.069136\n",
      "Iteration 10099: Policy loss: 0.305853. Value loss: 0.112770. Entropy: 0.295964.\n",
      "Iteration 10100: Policy loss: 0.301471. Value loss: 0.062762. Entropy: 0.295466.\n",
      "Iteration 10101: Policy loss: 0.286856. Value loss: 0.045250. Entropy: 0.293575.\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10102: Policy loss: 0.409632. Value loss: 0.103806. Entropy: 0.309350.\n",
      "Iteration 10103: Policy loss: 0.408290. Value loss: 0.046044. Entropy: 0.308560.\n",
      "Iteration 10104: Policy loss: 0.395348. Value loss: 0.033525. Entropy: 0.308147.\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10105: Policy loss: 0.099775. Value loss: 0.118549. Entropy: 0.303665.\n",
      "Iteration 10106: Policy loss: 0.095086. Value loss: 0.049243. Entropy: 0.303328.\n",
      "Iteration 10107: Policy loss: 0.091278. Value loss: 0.036685. Entropy: 0.302944.\n",
      "episode: 3742   score: 215.0  epsilon: 1.0    steps: 456  evaluation reward: 390.8\n",
      "episode: 3743   score: 45.0  epsilon: 1.0    steps: 552  evaluation reward: 385.7\n",
      "episode: 3744   score: 320.0  epsilon: 1.0    steps: 584  evaluation reward: 380.7\n",
      "episode: 3745   score: 120.0  epsilon: 1.0    steps: 600  evaluation reward: 378.9\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10108: Policy loss: -0.005751. Value loss: 0.054178. Entropy: 0.288961.\n",
      "Iteration 10109: Policy loss: -0.006307. Value loss: 0.026531. Entropy: 0.289317.\n",
      "Iteration 10110: Policy loss: -0.006774. Value loss: 0.023405. Entropy: 0.287971.\n",
      "episode: 3746   score: 160.0  epsilon: 1.0    steps: 392  evaluation reward: 378.2\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10111: Policy loss: 0.155164. Value loss: 0.104534. Entropy: 0.303387.\n",
      "Iteration 10112: Policy loss: 0.144259. Value loss: 0.052361. Entropy: 0.302206.\n",
      "Iteration 10113: Policy loss: 0.142396. Value loss: 0.035731. Entropy: 0.302690.\n",
      "episode: 3747   score: 205.0  epsilon: 1.0    steps: 200  evaluation reward: 377.0\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10114: Policy loss: 0.132102. Value loss: 0.114862. Entropy: 0.300342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10115: Policy loss: 0.128178. Value loss: 0.056150. Entropy: 0.300231.\n",
      "Iteration 10116: Policy loss: 0.126853. Value loss: 0.043459. Entropy: 0.300682.\n",
      "episode: 3748   score: 125.0  epsilon: 1.0    steps: 664  evaluation reward: 373.95\n",
      "episode: 3749   score: 260.0  epsilon: 1.0    steps: 720  evaluation reward: 369.55\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10117: Policy loss: 0.024572. Value loss: 0.077451. Entropy: 0.297141.\n",
      "Iteration 10118: Policy loss: 0.025352. Value loss: 0.037855. Entropy: 0.298461.\n",
      "Iteration 10119: Policy loss: 0.017933. Value loss: 0.028719. Entropy: 0.297371.\n",
      "episode: 3750   score: 390.0  epsilon: 1.0    steps: 880  evaluation reward: 364.75\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10120: Policy loss: -0.005067. Value loss: 0.119553. Entropy: 0.301082.\n",
      "Iteration 10121: Policy loss: -0.006983. Value loss: 0.046181. Entropy: 0.300912.\n",
      "Iteration 10122: Policy loss: -0.016669. Value loss: 0.032895. Entropy: 0.299860.\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10123: Policy loss: -0.447373. Value loss: 0.393506. Entropy: 0.304796.\n",
      "Iteration 10124: Policy loss: -0.421692. Value loss: 0.225705. Entropy: 0.305142.\n",
      "Iteration 10125: Policy loss: -0.435679. Value loss: 0.205257. Entropy: 0.304602.\n",
      "now time :  2019-09-06 00:43:14.556757\n",
      "episode: 3751   score: 255.0  epsilon: 1.0    steps: 440  evaluation reward: 362.9\n",
      "episode: 3752   score: 195.0  epsilon: 1.0    steps: 608  evaluation reward: 362.75\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10126: Policy loss: 0.152350. Value loss: 0.058632. Entropy: 0.298864.\n",
      "Iteration 10127: Policy loss: 0.144892. Value loss: 0.027325. Entropy: 0.299564.\n",
      "Iteration 10128: Policy loss: 0.143347. Value loss: 0.020839. Entropy: 0.299979.\n",
      "episode: 3753   score: 180.0  epsilon: 1.0    steps: 336  evaluation reward: 358.8\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10129: Policy loss: -0.276545. Value loss: 0.331372. Entropy: 0.301248.\n",
      "Iteration 10130: Policy loss: -0.287788. Value loss: 0.213451. Entropy: 0.302645.\n",
      "Iteration 10131: Policy loss: -0.305120. Value loss: 0.112903. Entropy: 0.301575.\n",
      "episode: 3754   score: 210.0  epsilon: 1.0    steps: 248  evaluation reward: 355.3\n",
      "episode: 3755   score: 180.0  epsilon: 1.0    steps: 920  evaluation reward: 351.65\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10132: Policy loss: 0.023786. Value loss: 0.131527. Entropy: 0.296146.\n",
      "Iteration 10133: Policy loss: 0.017195. Value loss: 0.046695. Entropy: 0.296407.\n",
      "Iteration 10134: Policy loss: 0.008727. Value loss: 0.035412. Entropy: 0.294595.\n",
      "episode: 3756   score: 445.0  epsilon: 1.0    steps: 240  evaluation reward: 352.2\n",
      "episode: 3757   score: 210.0  epsilon: 1.0    steps: 280  evaluation reward: 352.95\n",
      "episode: 3758   score: 125.0  epsilon: 1.0    steps: 848  evaluation reward: 348.3\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10135: Policy loss: 0.254950. Value loss: 0.086848. Entropy: 0.302457.\n",
      "Iteration 10136: Policy loss: 0.238773. Value loss: 0.042893. Entropy: 0.302184.\n",
      "Iteration 10137: Policy loss: 0.236457. Value loss: 0.031498. Entropy: 0.301660.\n",
      "episode: 3759   score: 380.0  epsilon: 1.0    steps: 480  evaluation reward: 350.0\n",
      "episode: 3760   score: 155.0  epsilon: 1.0    steps: 856  evaluation reward: 348.7\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10138: Policy loss: 0.013794. Value loss: 0.104914. Entropy: 0.297555.\n",
      "Iteration 10139: Policy loss: 0.008209. Value loss: 0.057489. Entropy: 0.297637.\n",
      "Iteration 10140: Policy loss: 0.008453. Value loss: 0.044354. Entropy: 0.298373.\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10141: Policy loss: -0.016596. Value loss: 0.080516. Entropy: 0.303104.\n",
      "Iteration 10142: Policy loss: -0.028034. Value loss: 0.036538. Entropy: 0.304391.\n",
      "Iteration 10143: Policy loss: -0.031860. Value loss: 0.029282. Entropy: 0.304088.\n",
      "episode: 3761   score: 180.0  epsilon: 1.0    steps: 608  evaluation reward: 348.25\n",
      "episode: 3762   score: 190.0  epsilon: 1.0    steps: 760  evaluation reward: 345.95\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10144: Policy loss: -0.023014. Value loss: 0.072745. Entropy: 0.301925.\n",
      "Iteration 10145: Policy loss: -0.030783. Value loss: 0.048526. Entropy: 0.301894.\n",
      "Iteration 10146: Policy loss: -0.034498. Value loss: 0.036849. Entropy: 0.300998.\n",
      "episode: 3763   score: 260.0  epsilon: 1.0    steps: 640  evaluation reward: 345.7\n",
      "episode: 3764   score: 155.0  epsilon: 1.0    steps: 776  evaluation reward: 345.0\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10147: Policy loss: -0.021776. Value loss: 0.123189. Entropy: 0.300807.\n",
      "Iteration 10148: Policy loss: -0.039131. Value loss: 0.043610. Entropy: 0.298566.\n",
      "Iteration 10149: Policy loss: -0.038731. Value loss: 0.032974. Entropy: 0.299270.\n",
      "Training network. lr: 0.000172. clip: 0.068979\n",
      "Iteration 10150: Policy loss: -0.003799. Value loss: 0.086278. Entropy: 0.307328.\n",
      "Iteration 10151: Policy loss: -0.008494. Value loss: 0.047796. Entropy: 0.306239.\n",
      "Iteration 10152: Policy loss: -0.009521. Value loss: 0.031327. Entropy: 0.306828.\n",
      "episode: 3765   score: 220.0  epsilon: 1.0    steps: 384  evaluation reward: 345.1\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10153: Policy loss: -0.443051. Value loss: 0.303759. Entropy: 0.303611.\n",
      "Iteration 10154: Policy loss: -0.452495. Value loss: 0.105540. Entropy: 0.303348.\n",
      "Iteration 10155: Policy loss: -0.473705. Value loss: 0.075466. Entropy: 0.304267.\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10156: Policy loss: -0.040395. Value loss: 0.143558. Entropy: 0.305621.\n",
      "Iteration 10157: Policy loss: -0.051243. Value loss: 0.053112. Entropy: 0.305269.\n",
      "Iteration 10158: Policy loss: -0.058950. Value loss: 0.038473. Entropy: 0.305818.\n",
      "episode: 3766   score: 415.0  epsilon: 1.0    steps: 584  evaluation reward: 345.0\n",
      "episode: 3767   score: 240.0  epsilon: 1.0    steps: 632  evaluation reward: 345.3\n",
      "episode: 3768   score: 220.0  epsilon: 1.0    steps: 664  evaluation reward: 343.85\n",
      "episode: 3769   score: 155.0  epsilon: 1.0    steps: 936  evaluation reward: 341.65\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10159: Policy loss: 0.132544. Value loss: 0.118797. Entropy: 0.298767.\n",
      "Iteration 10160: Policy loss: 0.118891. Value loss: 0.048069. Entropy: 0.295224.\n",
      "Iteration 10161: Policy loss: 0.109374. Value loss: 0.037857. Entropy: 0.296561.\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10162: Policy loss: 0.038572. Value loss: 0.112790. Entropy: 0.310840.\n",
      "Iteration 10163: Policy loss: 0.033146. Value loss: 0.063176. Entropy: 0.310424.\n",
      "Iteration 10164: Policy loss: 0.036326. Value loss: 0.045999. Entropy: 0.309944.\n",
      "episode: 3770   score: 445.0  epsilon: 1.0    steps: 256  evaluation reward: 342.5\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10165: Policy loss: -0.028799. Value loss: 0.090207. Entropy: 0.303986.\n",
      "Iteration 10166: Policy loss: -0.036129. Value loss: 0.042866. Entropy: 0.304493.\n",
      "Iteration 10167: Policy loss: -0.037046. Value loss: 0.028797. Entropy: 0.303440.\n",
      "episode: 3771   score: 255.0  epsilon: 1.0    steps: 392  evaluation reward: 340.1\n",
      "episode: 3772   score: 375.0  epsilon: 1.0    steps: 736  evaluation reward: 341.25\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10168: Policy loss: 0.055704. Value loss: 0.080529. Entropy: 0.287901.\n",
      "Iteration 10169: Policy loss: 0.046374. Value loss: 0.033031. Entropy: 0.289816.\n",
      "Iteration 10170: Policy loss: 0.045212. Value loss: 0.028020. Entropy: 0.288231.\n",
      "episode: 3773   score: 210.0  epsilon: 1.0    steps: 520  evaluation reward: 336.15\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10171: Policy loss: -0.065288. Value loss: 0.105409. Entropy: 0.301222.\n",
      "Iteration 10172: Policy loss: -0.074263. Value loss: 0.049340. Entropy: 0.303301.\n",
      "Iteration 10173: Policy loss: -0.076249. Value loss: 0.035038. Entropy: 0.303099.\n",
      "episode: 3774   score: 180.0  epsilon: 1.0    steps: 8  evaluation reward: 336.8\n",
      "episode: 3775   score: 290.0  epsilon: 1.0    steps: 448  evaluation reward: 334.15\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10174: Policy loss: 0.006388. Value loss: 0.058609. Entropy: 0.304829.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10175: Policy loss: 0.003524. Value loss: 0.023721. Entropy: 0.304315.\n",
      "Iteration 10176: Policy loss: 0.003952. Value loss: 0.016150. Entropy: 0.302063.\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10177: Policy loss: 0.322706. Value loss: 0.163446. Entropy: 0.304910.\n",
      "Iteration 10178: Policy loss: 0.319198. Value loss: 0.072038. Entropy: 0.303974.\n",
      "Iteration 10179: Policy loss: 0.313516. Value loss: 0.056713. Entropy: 0.303482.\n",
      "episode: 3776   score: 220.0  epsilon: 1.0    steps: 488  evaluation reward: 333.7\n",
      "episode: 3777   score: 330.0  epsilon: 1.0    steps: 600  evaluation reward: 330.85\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10180: Policy loss: 0.131390. Value loss: 0.063235. Entropy: 0.296225.\n",
      "Iteration 10181: Policy loss: 0.125291. Value loss: 0.026523. Entropy: 0.298073.\n",
      "Iteration 10182: Policy loss: 0.120299. Value loss: 0.018768. Entropy: 0.295448.\n",
      "episode: 3778   score: 395.0  epsilon: 1.0    steps: 640  evaluation reward: 329.15\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10183: Policy loss: -0.046633. Value loss: 0.165302. Entropy: 0.301573.\n",
      "Iteration 10184: Policy loss: -0.055521. Value loss: 0.070822. Entropy: 0.300894.\n",
      "Iteration 10185: Policy loss: -0.066151. Value loss: 0.050568. Entropy: 0.301282.\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10186: Policy loss: -0.210752. Value loss: 0.479373. Entropy: 0.310769.\n",
      "Iteration 10187: Policy loss: -0.231761. Value loss: 0.303416. Entropy: 0.310622.\n",
      "Iteration 10188: Policy loss: -0.212466. Value loss: 0.231148. Entropy: 0.309659.\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10189: Policy loss: -0.274875. Value loss: 0.198755. Entropy: 0.313111.\n",
      "Iteration 10190: Policy loss: -0.277852. Value loss: 0.077081. Entropy: 0.312409.\n",
      "Iteration 10191: Policy loss: -0.283441. Value loss: 0.056347. Entropy: 0.312812.\n",
      "episode: 3779   score: 515.0  epsilon: 1.0    steps: 352  evaluation reward: 330.65\n",
      "episode: 3780   score: 285.0  epsilon: 1.0    steps: 376  evaluation reward: 325.3\n",
      "episode: 3781   score: 440.0  epsilon: 1.0    steps: 768  evaluation reward: 327.3\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10192: Policy loss: 0.304911. Value loss: 0.143596. Entropy: 0.286254.\n",
      "Iteration 10193: Policy loss: 0.291612. Value loss: 0.057862. Entropy: 0.285611.\n",
      "Iteration 10194: Policy loss: 0.283138. Value loss: 0.036392. Entropy: 0.284797.\n",
      "episode: 3782   score: 210.0  epsilon: 1.0    steps: 400  evaluation reward: 325.0\n",
      "episode: 3783   score: 210.0  epsilon: 1.0    steps: 728  evaluation reward: 323.95\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10195: Policy loss: 0.210527. Value loss: 0.085874. Entropy: 0.295927.\n",
      "Iteration 10196: Policy loss: 0.197033. Value loss: 0.032451. Entropy: 0.295977.\n",
      "Iteration 10197: Policy loss: 0.198874. Value loss: 0.025716. Entropy: 0.294318.\n",
      "episode: 3784   score: 440.0  epsilon: 1.0    steps: 656  evaluation reward: 322.9\n",
      "episode: 3785   score: 180.0  epsilon: 1.0    steps: 792  evaluation reward: 322.6\n",
      "Training network. lr: 0.000172. clip: 0.068822\n",
      "Iteration 10198: Policy loss: 0.139517. Value loss: 0.151367. Entropy: 0.288605.\n",
      "Iteration 10199: Policy loss: 0.129451. Value loss: 0.060638. Entropy: 0.289250.\n",
      "Iteration 10200: Policy loss: 0.138162. Value loss: 0.045609. Entropy: 0.286900.\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10201: Policy loss: 0.080605. Value loss: 0.150333. Entropy: 0.305384.\n",
      "Iteration 10202: Policy loss: 0.077674. Value loss: 0.070124. Entropy: 0.305732.\n",
      "Iteration 10203: Policy loss: 0.077318. Value loss: 0.050424. Entropy: 0.305995.\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10204: Policy loss: 0.181525. Value loss: 0.122596. Entropy: 0.307253.\n",
      "Iteration 10205: Policy loss: 0.169356. Value loss: 0.048197. Entropy: 0.306087.\n",
      "Iteration 10206: Policy loss: 0.152012. Value loss: 0.035319. Entropy: 0.306019.\n",
      "episode: 3786   score: 585.0  epsilon: 1.0    steps: 384  evaluation reward: 326.35\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10207: Policy loss: -0.068338. Value loss: 0.094673. Entropy: 0.301108.\n",
      "Iteration 10208: Policy loss: -0.071157. Value loss: 0.040880. Entropy: 0.301405.\n",
      "Iteration 10209: Policy loss: -0.073808. Value loss: 0.032076. Entropy: 0.301766.\n",
      "episode: 3787   score: 225.0  epsilon: 1.0    steps: 448  evaluation reward: 323.35\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10210: Policy loss: -0.347693. Value loss: 0.579821. Entropy: 0.303110.\n",
      "Iteration 10211: Policy loss: -0.414268. Value loss: 0.435618. Entropy: 0.302659.\n",
      "Iteration 10212: Policy loss: -0.406764. Value loss: 0.334016. Entropy: 0.302399.\n",
      "episode: 3788   score: 360.0  epsilon: 1.0    steps: 616  evaluation reward: 321.5\n",
      "episode: 3789   score: 215.0  epsilon: 1.0    steps: 656  evaluation reward: 320.8\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10213: Policy loss: 0.148303. Value loss: 0.172377. Entropy: 0.293233.\n",
      "Iteration 10214: Policy loss: 0.144766. Value loss: 0.063846. Entropy: 0.291794.\n",
      "Iteration 10215: Policy loss: 0.138925. Value loss: 0.042886. Entropy: 0.291143.\n",
      "episode: 3790   score: 390.0  epsilon: 1.0    steps: 536  evaluation reward: 319.6\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10216: Policy loss: -0.094230. Value loss: 0.150444. Entropy: 0.304460.\n",
      "Iteration 10217: Policy loss: -0.102261. Value loss: 0.076730. Entropy: 0.306678.\n",
      "Iteration 10218: Policy loss: -0.103594. Value loss: 0.055889. Entropy: 0.306447.\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10219: Policy loss: 0.131642. Value loss: 0.139240. Entropy: 0.306913.\n",
      "Iteration 10220: Policy loss: 0.126392. Value loss: 0.046798. Entropy: 0.306497.\n",
      "Iteration 10221: Policy loss: 0.119697. Value loss: 0.034023. Entropy: 0.305611.\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10222: Policy loss: 0.148752. Value loss: 0.096094. Entropy: 0.304269.\n",
      "Iteration 10223: Policy loss: 0.144554. Value loss: 0.037339. Entropy: 0.304182.\n",
      "Iteration 10224: Policy loss: 0.137062. Value loss: 0.026812. Entropy: 0.303912.\n",
      "episode: 3791   score: 405.0  epsilon: 1.0    steps: 8  evaluation reward: 320.0\n",
      "episode: 3792   score: 560.0  epsilon: 1.0    steps: 552  evaluation reward: 321.85\n",
      "episode: 3793   score: 215.0  epsilon: 1.0    steps: 624  evaluation reward: 318.55\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10225: Policy loss: 0.417727. Value loss: 0.076997. Entropy: 0.292492.\n",
      "Iteration 10226: Policy loss: 0.419086. Value loss: 0.032690. Entropy: 0.292349.\n",
      "Iteration 10227: Policy loss: 0.414250. Value loss: 0.026946. Entropy: 0.293389.\n",
      "episode: 3794   score: 185.0  epsilon: 1.0    steps: 104  evaluation reward: 316.05\n",
      "episode: 3795   score: 215.0  epsilon: 1.0    steps: 600  evaluation reward: 314.55\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10228: Policy loss: 0.000186. Value loss: 0.072813. Entropy: 0.305233.\n",
      "Iteration 10229: Policy loss: -0.003479. Value loss: 0.043328. Entropy: 0.306183.\n",
      "Iteration 10230: Policy loss: -0.005838. Value loss: 0.032966. Entropy: 0.306218.\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10231: Policy loss: -0.430637. Value loss: 0.355097. Entropy: 0.308942.\n",
      "Iteration 10232: Policy loss: -0.440848. Value loss: 0.219904. Entropy: 0.308906.\n",
      "Iteration 10233: Policy loss: -0.462769. Value loss: 0.125505. Entropy: 0.309033.\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10234: Policy loss: 0.135947. Value loss: 0.138763. Entropy: 0.307042.\n",
      "Iteration 10235: Policy loss: 0.124980. Value loss: 0.043811. Entropy: 0.306584.\n",
      "Iteration 10236: Policy loss: 0.112900. Value loss: 0.028623. Entropy: 0.306537.\n",
      "episode: 3796   score: 315.0  epsilon: 1.0    steps: 272  evaluation reward: 313.8\n",
      "episode: 3797   score: 240.0  epsilon: 1.0    steps: 960  evaluation reward: 313.45\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10237: Policy loss: 0.205580. Value loss: 0.083692. Entropy: 0.304902.\n",
      "Iteration 10238: Policy loss: 0.199620. Value loss: 0.033669. Entropy: 0.304308.\n",
      "Iteration 10239: Policy loss: 0.200008. Value loss: 0.025582. Entropy: 0.303855.\n",
      "episode: 3798   score: 150.0  epsilon: 1.0    steps: 904  evaluation reward: 311.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10240: Policy loss: -0.073914. Value loss: 0.145959. Entropy: 0.310926.\n",
      "Iteration 10241: Policy loss: -0.077488. Value loss: 0.052766. Entropy: 0.309034.\n",
      "Iteration 10242: Policy loss: -0.082363. Value loss: 0.036810. Entropy: 0.309241.\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10243: Policy loss: -0.020219. Value loss: 0.176193. Entropy: 0.309004.\n",
      "Iteration 10244: Policy loss: -0.035598. Value loss: 0.079773. Entropy: 0.307208.\n",
      "Iteration 10245: Policy loss: -0.033312. Value loss: 0.052380. Entropy: 0.307894.\n",
      "episode: 3799   score: 1005.0  epsilon: 1.0    steps: 616  evaluation reward: 319.95\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10246: Policy loss: 0.141963. Value loss: 0.159139. Entropy: 0.299453.\n",
      "Iteration 10247: Policy loss: 0.132312. Value loss: 0.066790. Entropy: 0.298243.\n",
      "Iteration 10248: Policy loss: 0.140485. Value loss: 0.046258. Entropy: 0.297220.\n",
      "episode: 3800   score: 355.0  epsilon: 1.0    steps: 288  evaluation reward: 314.8\n",
      "now time :  2019-09-06 00:50:53.514535\n",
      "episode: 3801   score: 515.0  epsilon: 1.0    steps: 336  evaluation reward: 315.55\n",
      "Training network. lr: 0.000172. clip: 0.068675\n",
      "Iteration 10249: Policy loss: 0.311350. Value loss: 0.101002. Entropy: 0.292490.\n",
      "Iteration 10250: Policy loss: 0.308626. Value loss: 0.034858. Entropy: 0.295240.\n",
      "Iteration 10251: Policy loss: 0.299578. Value loss: 0.026599. Entropy: 0.295515.\n",
      "episode: 3802   score: 365.0  epsilon: 1.0    steps: 88  evaluation reward: 315.05\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10252: Policy loss: 0.014277. Value loss: 0.085015. Entropy: 0.300437.\n",
      "Iteration 10253: Policy loss: 0.012211. Value loss: 0.037134. Entropy: 0.301193.\n",
      "Iteration 10254: Policy loss: 0.000040. Value loss: 0.028546. Entropy: 0.300669.\n",
      "episode: 3803   score: 180.0  epsilon: 1.0    steps: 344  evaluation reward: 306.75\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10255: Policy loss: 0.141181. Value loss: 0.092721. Entropy: 0.298785.\n",
      "Iteration 10256: Policy loss: 0.137137. Value loss: 0.035230. Entropy: 0.297727.\n",
      "Iteration 10257: Policy loss: 0.132161. Value loss: 0.025991. Entropy: 0.297547.\n",
      "episode: 3804   score: 310.0  epsilon: 1.0    steps: 952  evaluation reward: 303.95\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10258: Policy loss: -0.255294. Value loss: 0.366765. Entropy: 0.305167.\n",
      "Iteration 10259: Policy loss: -0.259071. Value loss: 0.236000. Entropy: 0.305900.\n",
      "Iteration 10260: Policy loss: -0.286322. Value loss: 0.175861. Entropy: 0.304933.\n",
      "episode: 3805   score: 210.0  epsilon: 1.0    steps: 552  evaluation reward: 298.4\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10261: Policy loss: 0.026087. Value loss: 0.071536. Entropy: 0.303666.\n",
      "Iteration 10262: Policy loss: 0.028463. Value loss: 0.031091. Entropy: 0.302505.\n",
      "Iteration 10263: Policy loss: 0.022312. Value loss: 0.023616. Entropy: 0.303765.\n",
      "episode: 3806   score: 210.0  epsilon: 1.0    steps: 344  evaluation reward: 299.3\n",
      "episode: 3807   score: 170.0  epsilon: 1.0    steps: 464  evaluation reward: 295.85\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10264: Policy loss: 0.018764. Value loss: 0.068210. Entropy: 0.299357.\n",
      "Iteration 10265: Policy loss: 0.011898. Value loss: 0.034548. Entropy: 0.299472.\n",
      "Iteration 10266: Policy loss: 0.006635. Value loss: 0.027958. Entropy: 0.297027.\n",
      "episode: 3808   score: 185.0  epsilon: 1.0    steps: 664  evaluation reward: 295.6\n",
      "episode: 3809   score: 555.0  epsilon: 1.0    steps: 728  evaluation reward: 299.05\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10267: Policy loss: 0.070121. Value loss: 0.108750. Entropy: 0.297947.\n",
      "Iteration 10268: Policy loss: 0.062906. Value loss: 0.042280. Entropy: 0.296071.\n",
      "Iteration 10269: Policy loss: 0.060991. Value loss: 0.033311. Entropy: 0.298333.\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10270: Policy loss: -0.170995. Value loss: 0.117486. Entropy: 0.307504.\n",
      "Iteration 10271: Policy loss: -0.184497. Value loss: 0.047387. Entropy: 0.307032.\n",
      "Iteration 10272: Policy loss: -0.187086. Value loss: 0.032582. Entropy: 0.307185.\n",
      "episode: 3810   score: 465.0  epsilon: 1.0    steps: 600  evaluation reward: 299.15\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10273: Policy loss: 0.141665. Value loss: 0.141384. Entropy: 0.298290.\n",
      "Iteration 10274: Policy loss: 0.127285. Value loss: 0.062715. Entropy: 0.295258.\n",
      "Iteration 10275: Policy loss: 0.128109. Value loss: 0.042982. Entropy: 0.294160.\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10276: Policy loss: -0.015883. Value loss: 0.087785. Entropy: 0.306372.\n",
      "Iteration 10277: Policy loss: -0.023669. Value loss: 0.036698. Entropy: 0.305134.\n",
      "Iteration 10278: Policy loss: -0.033484. Value loss: 0.029699. Entropy: 0.305771.\n",
      "episode: 3811   score: 440.0  epsilon: 1.0    steps: 592  evaluation reward: 299.65\n",
      "episode: 3812   score: 180.0  epsilon: 1.0    steps: 640  evaluation reward: 299.3\n",
      "episode: 3813   score: 360.0  epsilon: 1.0    steps: 680  evaluation reward: 300.8\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10279: Policy loss: -0.164345. Value loss: 0.358366. Entropy: 0.285878.\n",
      "Iteration 10280: Policy loss: -0.192892. Value loss: 0.234558. Entropy: 0.285071.\n",
      "Iteration 10281: Policy loss: -0.188564. Value loss: 0.184303. Entropy: 0.283350.\n",
      "episode: 3814   score: 325.0  epsilon: 1.0    steps: 968  evaluation reward: 300.6\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10282: Policy loss: 0.238843. Value loss: 0.141797. Entropy: 0.307341.\n",
      "Iteration 10283: Policy loss: 0.228077. Value loss: 0.060078. Entropy: 0.307114.\n",
      "Iteration 10284: Policy loss: 0.224602. Value loss: 0.044002. Entropy: 0.305980.\n",
      "episode: 3815   score: 180.0  epsilon: 1.0    steps: 360  evaluation reward: 298.75\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10285: Policy loss: 0.089366. Value loss: 0.106842. Entropy: 0.299521.\n",
      "Iteration 10286: Policy loss: 0.092822. Value loss: 0.051011. Entropy: 0.298011.\n",
      "Iteration 10287: Policy loss: 0.076375. Value loss: 0.041714. Entropy: 0.297665.\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10288: Policy loss: -0.252019. Value loss: 0.111251. Entropy: 0.306241.\n",
      "Iteration 10289: Policy loss: -0.253443. Value loss: 0.050008. Entropy: 0.307162.\n",
      "Iteration 10290: Policy loss: -0.263715. Value loss: 0.034480. Entropy: 0.307044.\n",
      "episode: 3816   score: 390.0  epsilon: 1.0    steps: 184  evaluation reward: 299.75\n",
      "episode: 3817   score: 565.0  epsilon: 1.0    steps: 280  evaluation reward: 300.1\n",
      "episode: 3818   score: 420.0  epsilon: 1.0    steps: 1024  evaluation reward: 302.2\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10291: Policy loss: 0.073751. Value loss: 0.067401. Entropy: 0.287151.\n",
      "Iteration 10292: Policy loss: 0.074242. Value loss: 0.031588. Entropy: 0.287050.\n",
      "Iteration 10293: Policy loss: 0.072829. Value loss: 0.021997. Entropy: 0.287629.\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10294: Policy loss: -0.401773. Value loss: 0.372827. Entropy: 0.297191.\n",
      "Iteration 10295: Policy loss: -0.424317. Value loss: 0.249291. Entropy: 0.295605.\n",
      "Iteration 10296: Policy loss: -0.407404. Value loss: 0.192482. Entropy: 0.297296.\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10297: Policy loss: 0.075688. Value loss: 0.106365. Entropy: 0.306261.\n",
      "Iteration 10298: Policy loss: 0.074771. Value loss: 0.046349. Entropy: 0.307401.\n",
      "Iteration 10299: Policy loss: 0.058890. Value loss: 0.033787. Entropy: 0.306888.\n",
      "Training network. lr: 0.000171. clip: 0.068518\n",
      "Iteration 10300: Policy loss: 0.112777. Value loss: 0.161856. Entropy: 0.304880.\n",
      "Iteration 10301: Policy loss: 0.104029. Value loss: 0.070248. Entropy: 0.305358.\n",
      "Iteration 10302: Policy loss: 0.098501. Value loss: 0.050345. Entropy: 0.304525.\n",
      "episode: 3819   score: 335.0  epsilon: 1.0    steps: 472  evaluation reward: 302.9\n",
      "episode: 3820   score: 490.0  epsilon: 1.0    steps: 952  evaluation reward: 305.7\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10303: Policy loss: 0.179800. Value loss: 0.126222. Entropy: 0.293604.\n",
      "Iteration 10304: Policy loss: 0.166435. Value loss: 0.045473. Entropy: 0.292991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10305: Policy loss: 0.163178. Value loss: 0.026684. Entropy: 0.292359.\n",
      "episode: 3821   score: 480.0  epsilon: 1.0    steps: 208  evaluation reward: 307.8\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10306: Policy loss: 0.188497. Value loss: 0.065720. Entropy: 0.280644.\n",
      "Iteration 10307: Policy loss: 0.180294. Value loss: 0.021471. Entropy: 0.280748.\n",
      "Iteration 10308: Policy loss: 0.177785. Value loss: 0.014887. Entropy: 0.279838.\n",
      "episode: 3822   score: 180.0  epsilon: 1.0    steps: 88  evaluation reward: 305.85\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10309: Policy loss: -0.399014. Value loss: 0.404181. Entropy: 0.291281.\n",
      "Iteration 10310: Policy loss: -0.418114. Value loss: 0.126032. Entropy: 0.289181.\n",
      "Iteration 10311: Policy loss: -0.412260. Value loss: 0.075666. Entropy: 0.290013.\n",
      "episode: 3823   score: 450.0  epsilon: 1.0    steps: 224  evaluation reward: 307.05\n",
      "episode: 3824   score: 180.0  epsilon: 1.0    steps: 872  evaluation reward: 306.45\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10312: Policy loss: 0.095143. Value loss: 0.131920. Entropy: 0.285047.\n",
      "Iteration 10313: Policy loss: 0.087658. Value loss: 0.058909. Entropy: 0.284191.\n",
      "Iteration 10314: Policy loss: 0.087903. Value loss: 0.044892. Entropy: 0.282576.\n",
      "episode: 3825   score: 425.0  epsilon: 1.0    steps: 264  evaluation reward: 307.55\n",
      "episode: 3826   score: 590.0  epsilon: 1.0    steps: 368  evaluation reward: 307.45\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10315: Policy loss: 0.003500. Value loss: 0.068252. Entropy: 0.277184.\n",
      "Iteration 10316: Policy loss: 0.005053. Value loss: 0.028959. Entropy: 0.274614.\n",
      "Iteration 10317: Policy loss: -0.006834. Value loss: 0.022988. Entropy: 0.275105.\n",
      "episode: 3827   score: 180.0  epsilon: 1.0    steps: 48  evaluation reward: 305.8\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10318: Policy loss: -0.037423. Value loss: 0.078493. Entropy: 0.295385.\n",
      "Iteration 10319: Policy loss: -0.045060. Value loss: 0.037858. Entropy: 0.295236.\n",
      "Iteration 10320: Policy loss: -0.046626. Value loss: 0.030227. Entropy: 0.294801.\n",
      "episode: 3828   score: 230.0  epsilon: 1.0    steps: 632  evaluation reward: 306.6\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10321: Policy loss: 0.168013. Value loss: 0.120882. Entropy: 0.292398.\n",
      "Iteration 10322: Policy loss: 0.160516. Value loss: 0.050493. Entropy: 0.291873.\n",
      "Iteration 10323: Policy loss: 0.159863. Value loss: 0.034374. Entropy: 0.291106.\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10324: Policy loss: 0.099937. Value loss: 0.077093. Entropy: 0.304373.\n",
      "Iteration 10325: Policy loss: 0.092507. Value loss: 0.032249. Entropy: 0.305767.\n",
      "Iteration 10326: Policy loss: 0.089970. Value loss: 0.024658. Entropy: 0.304896.\n",
      "episode: 3829   score: 155.0  epsilon: 1.0    steps: 504  evaluation reward: 304.95\n",
      "episode: 3830   score: 260.0  epsilon: 1.0    steps: 560  evaluation reward: 304.05\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10327: Policy loss: -0.227326. Value loss: 0.356118. Entropy: 0.281798.\n",
      "Iteration 10328: Policy loss: -0.215758. Value loss: 0.241021. Entropy: 0.279571.\n",
      "Iteration 10329: Policy loss: -0.251416. Value loss: 0.214562. Entropy: 0.282194.\n",
      "episode: 3831   score: 215.0  epsilon: 1.0    steps: 328  evaluation reward: 302.85\n",
      "episode: 3832   score: 285.0  epsilon: 1.0    steps: 432  evaluation reward: 299.0\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10330: Policy loss: -0.131341. Value loss: 0.374176. Entropy: 0.279946.\n",
      "Iteration 10331: Policy loss: -0.153022. Value loss: 0.252469. Entropy: 0.281146.\n",
      "Iteration 10332: Policy loss: -0.155892. Value loss: 0.206426. Entropy: 0.279239.\n",
      "episode: 3833   score: 230.0  epsilon: 1.0    steps: 304  evaluation reward: 298.9\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10333: Policy loss: 0.141448. Value loss: 0.132136. Entropy: 0.296661.\n",
      "Iteration 10334: Policy loss: 0.130030. Value loss: 0.064867. Entropy: 0.297536.\n",
      "Iteration 10335: Policy loss: 0.133317. Value loss: 0.046364. Entropy: 0.294016.\n",
      "episode: 3834   score: 125.0  epsilon: 1.0    steps: 544  evaluation reward: 297.55\n",
      "episode: 3835   score: 210.0  epsilon: 1.0    steps: 960  evaluation reward: 296.2\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10336: Policy loss: -0.149953. Value loss: 0.127260. Entropy: 0.292317.\n",
      "Iteration 10337: Policy loss: -0.156494. Value loss: 0.059088. Entropy: 0.292076.\n",
      "Iteration 10338: Policy loss: -0.150023. Value loss: 0.039600. Entropy: 0.291082.\n",
      "episode: 3836   score: 560.0  epsilon: 1.0    steps: 840  evaluation reward: 298.75\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10339: Policy loss: -0.025441. Value loss: 0.096957. Entropy: 0.292938.\n",
      "Iteration 10340: Policy loss: -0.026842. Value loss: 0.044830. Entropy: 0.294086.\n",
      "Iteration 10341: Policy loss: -0.034495. Value loss: 0.033624. Entropy: 0.294478.\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10342: Policy loss: -0.329678. Value loss: 0.331866. Entropy: 0.304426.\n",
      "Iteration 10343: Policy loss: -0.335192. Value loss: 0.169899. Entropy: 0.304737.\n",
      "Iteration 10344: Policy loss: -0.335672. Value loss: 0.066796. Entropy: 0.303208.\n",
      "episode: 3837   score: 185.0  epsilon: 1.0    steps: 32  evaluation reward: 297.25\n",
      "episode: 3838   score: 845.0  epsilon: 1.0    steps: 96  evaluation reward: 303.6\n",
      "episode: 3839   score: 265.0  epsilon: 1.0    steps: 288  evaluation reward: 303.4\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10345: Policy loss: -0.026289. Value loss: 0.077582. Entropy: 0.279275.\n",
      "Iteration 10346: Policy loss: -0.028709. Value loss: 0.046463. Entropy: 0.280624.\n",
      "Iteration 10347: Policy loss: -0.034684. Value loss: 0.034809. Entropy: 0.277394.\n",
      "episode: 3840   score: 210.0  epsilon: 1.0    steps: 240  evaluation reward: 300.5\n",
      "episode: 3841   score: 160.0  epsilon: 1.0    steps: 712  evaluation reward: 298.8\n",
      "episode: 3842   score: 155.0  epsilon: 1.0    steps: 856  evaluation reward: 298.2\n",
      "Training network. lr: 0.000171. clip: 0.068361\n",
      "Iteration 10348: Policy loss: 0.070468. Value loss: 0.110454. Entropy: 0.277600.\n",
      "Iteration 10349: Policy loss: 0.070552. Value loss: 0.062490. Entropy: 0.278188.\n",
      "Iteration 10350: Policy loss: 0.069155. Value loss: 0.048104. Entropy: 0.277564.\n",
      "episode: 3843   score: 345.0  epsilon: 1.0    steps: 992  evaluation reward: 301.2\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10351: Policy loss: 0.300602. Value loss: 0.117581. Entropy: 0.306671.\n",
      "Iteration 10352: Policy loss: 0.290242. Value loss: 0.057166. Entropy: 0.305808.\n",
      "Iteration 10353: Policy loss: 0.289444. Value loss: 0.048379. Entropy: 0.305130.\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10354: Policy loss: -0.006004. Value loss: 0.105481. Entropy: 0.301071.\n",
      "Iteration 10355: Policy loss: -0.010207. Value loss: 0.043160. Entropy: 0.300019.\n",
      "Iteration 10356: Policy loss: -0.012245. Value loss: 0.033739. Entropy: 0.299454.\n",
      "episode: 3844   score: 110.0  epsilon: 1.0    steps: 368  evaluation reward: 299.1\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10357: Policy loss: 0.002517. Value loss: 0.057748. Entropy: 0.289840.\n",
      "Iteration 10358: Policy loss: -0.001728. Value loss: 0.024473. Entropy: 0.289206.\n",
      "Iteration 10359: Policy loss: -0.003269. Value loss: 0.018168. Entropy: 0.289758.\n",
      "episode: 3845   score: 260.0  epsilon: 1.0    steps: 256  evaluation reward: 300.5\n",
      "episode: 3846   score: 210.0  epsilon: 1.0    steps: 352  evaluation reward: 301.0\n",
      "episode: 3847   score: 230.0  epsilon: 1.0    steps: 376  evaluation reward: 301.25\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10360: Policy loss: 0.070183. Value loss: 0.077966. Entropy: 0.265280.\n",
      "Iteration 10361: Policy loss: 0.068116. Value loss: 0.028989. Entropy: 0.262378.\n",
      "Iteration 10362: Policy loss: 0.057004. Value loss: 0.021633. Entropy: 0.262162.\n",
      "episode: 3848   score: 250.0  epsilon: 1.0    steps: 400  evaluation reward: 302.5\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10363: Policy loss: 0.219861. Value loss: 0.079192. Entropy: 0.294987.\n",
      "Iteration 10364: Policy loss: 0.212240. Value loss: 0.036874. Entropy: 0.293763.\n",
      "Iteration 10365: Policy loss: 0.210970. Value loss: 0.030912. Entropy: 0.292893.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3849   score: 210.0  epsilon: 1.0    steps: 208  evaluation reward: 302.0\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10366: Policy loss: 0.251670. Value loss: 0.079540. Entropy: 0.296986.\n",
      "Iteration 10367: Policy loss: 0.249114. Value loss: 0.038849. Entropy: 0.296894.\n",
      "Iteration 10368: Policy loss: 0.244283. Value loss: 0.031583. Entropy: 0.295254.\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10369: Policy loss: 0.033246. Value loss: 0.079789. Entropy: 0.304586.\n",
      "Iteration 10370: Policy loss: 0.028666. Value loss: 0.032232. Entropy: 0.304322.\n",
      "Iteration 10371: Policy loss: 0.017095. Value loss: 0.026123. Entropy: 0.303544.\n",
      "episode: 3850   score: 160.0  epsilon: 1.0    steps: 384  evaluation reward: 299.7\n",
      "now time :  2019-09-06 00:58:31.695139\n",
      "episode: 3851   score: 155.0  epsilon: 1.0    steps: 416  evaluation reward: 298.7\n",
      "episode: 3852   score: 285.0  epsilon: 1.0    steps: 440  evaluation reward: 299.6\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10372: Policy loss: 0.026713. Value loss: 0.113379. Entropy: 0.265416.\n",
      "Iteration 10373: Policy loss: 0.017259. Value loss: 0.053688. Entropy: 0.264566.\n",
      "Iteration 10374: Policy loss: 0.012074. Value loss: 0.041417. Entropy: 0.265420.\n",
      "episode: 3853   score: 210.0  epsilon: 1.0    steps: 56  evaluation reward: 299.9\n",
      "episode: 3854   score: 340.0  epsilon: 1.0    steps: 112  evaluation reward: 301.2\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10375: Policy loss: -0.002025. Value loss: 0.075289. Entropy: 0.280395.\n",
      "Iteration 10376: Policy loss: -0.008972. Value loss: 0.032531. Entropy: 0.279021.\n",
      "Iteration 10377: Policy loss: -0.015699. Value loss: 0.024988. Entropy: 0.279031.\n",
      "episode: 3855   score: 210.0  epsilon: 1.0    steps: 328  evaluation reward: 301.5\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10378: Policy loss: 0.063977. Value loss: 0.123945. Entropy: 0.290568.\n",
      "Iteration 10379: Policy loss: 0.062362. Value loss: 0.069137. Entropy: 0.288250.\n",
      "Iteration 10380: Policy loss: 0.057046. Value loss: 0.049842. Entropy: 0.289840.\n",
      "episode: 3856   score: 125.0  epsilon: 1.0    steps: 952  evaluation reward: 298.3\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10381: Policy loss: -0.025792. Value loss: 0.066079. Entropy: 0.301272.\n",
      "Iteration 10382: Policy loss: -0.034155. Value loss: 0.028753. Entropy: 0.301533.\n",
      "Iteration 10383: Policy loss: -0.030952. Value loss: 0.021478. Entropy: 0.301723.\n",
      "episode: 3857   score: 210.0  epsilon: 1.0    steps: 256  evaluation reward: 298.3\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10384: Policy loss: 0.167439. Value loss: 0.040625. Entropy: 0.291290.\n",
      "Iteration 10385: Policy loss: 0.163321. Value loss: 0.019541. Entropy: 0.290014.\n",
      "Iteration 10386: Policy loss: 0.164175. Value loss: 0.015408. Entropy: 0.291319.\n",
      "episode: 3858   score: 210.0  epsilon: 1.0    steps: 800  evaluation reward: 299.15\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10387: Policy loss: -0.225402. Value loss: 0.236354. Entropy: 0.300746.\n",
      "Iteration 10388: Policy loss: -0.263768. Value loss: 0.166657. Entropy: 0.300547.\n",
      "Iteration 10389: Policy loss: -0.281863. Value loss: 0.103961. Entropy: 0.298061.\n",
      "episode: 3859   score: 525.0  epsilon: 1.0    steps: 288  evaluation reward: 300.6\n",
      "episode: 3860   score: 210.0  epsilon: 1.0    steps: 312  evaluation reward: 301.15\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10390: Policy loss: -0.077430. Value loss: 0.080775. Entropy: 0.280226.\n",
      "Iteration 10391: Policy loss: -0.084880. Value loss: 0.044143. Entropy: 0.279558.\n",
      "Iteration 10392: Policy loss: -0.084703. Value loss: 0.034518. Entropy: 0.279762.\n",
      "episode: 3861   score: 290.0  epsilon: 1.0    steps: 632  evaluation reward: 302.25\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10393: Policy loss: 0.014115. Value loss: 0.142361. Entropy: 0.291521.\n",
      "Iteration 10394: Policy loss: 0.005956. Value loss: 0.048326. Entropy: 0.291546.\n",
      "Iteration 10395: Policy loss: -0.003822. Value loss: 0.029274. Entropy: 0.291339.\n",
      "episode: 3862   score: 315.0  epsilon: 1.0    steps: 200  evaluation reward: 303.5\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10396: Policy loss: 0.309775. Value loss: 0.137111. Entropy: 0.294560.\n",
      "Iteration 10397: Policy loss: 0.306582. Value loss: 0.050263. Entropy: 0.293389.\n",
      "Iteration 10398: Policy loss: 0.299278. Value loss: 0.037365. Entropy: 0.294162.\n",
      "episode: 3863   score: 105.0  epsilon: 1.0    steps: 240  evaluation reward: 301.95\n",
      "episode: 3864   score: 225.0  epsilon: 1.0    steps: 440  evaluation reward: 302.65\n",
      "Training network. lr: 0.000171. clip: 0.068214\n",
      "Iteration 10399: Policy loss: -0.008043. Value loss: 0.066187. Entropy: 0.278116.\n",
      "Iteration 10400: Policy loss: -0.008140. Value loss: 0.036274. Entropy: 0.277189.\n",
      "Iteration 10401: Policy loss: -0.015026. Value loss: 0.026390. Entropy: 0.276399.\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10402: Policy loss: 0.179573. Value loss: 0.076901. Entropy: 0.304334.\n",
      "Iteration 10403: Policy loss: 0.177814. Value loss: 0.033780. Entropy: 0.302493.\n",
      "Iteration 10404: Policy loss: 0.170733. Value loss: 0.024972. Entropy: 0.302317.\n",
      "episode: 3865   score: 535.0  epsilon: 1.0    steps: 240  evaluation reward: 305.8\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10405: Policy loss: 0.074872. Value loss: 0.123712. Entropy: 0.285460.\n",
      "Iteration 10406: Policy loss: 0.062885. Value loss: 0.053105. Entropy: 0.283982.\n",
      "Iteration 10407: Policy loss: 0.061515. Value loss: 0.040447. Entropy: 0.285507.\n",
      "episode: 3866   score: 335.0  epsilon: 1.0    steps: 792  evaluation reward: 305.0\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10408: Policy loss: 0.021583. Value loss: 0.153723. Entropy: 0.304684.\n",
      "Iteration 10409: Policy loss: 0.018098. Value loss: 0.051368. Entropy: 0.304392.\n",
      "Iteration 10410: Policy loss: 0.014343. Value loss: 0.033487. Entropy: 0.305477.\n",
      "episode: 3867   score: 440.0  epsilon: 1.0    steps: 696  evaluation reward: 307.0\n",
      "episode: 3868   score: 275.0  epsilon: 1.0    steps: 816  evaluation reward: 307.55\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10411: Policy loss: 0.095339. Value loss: 0.098162. Entropy: 0.288234.\n",
      "Iteration 10412: Policy loss: 0.092806. Value loss: 0.037910. Entropy: 0.286852.\n",
      "Iteration 10413: Policy loss: 0.082166. Value loss: 0.027993. Entropy: 0.286394.\n",
      "episode: 3869   score: 435.0  epsilon: 1.0    steps: 584  evaluation reward: 310.35\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10414: Policy loss: -0.114931. Value loss: 0.118856. Entropy: 0.292420.\n",
      "Iteration 10415: Policy loss: -0.130637. Value loss: 0.047239. Entropy: 0.293067.\n",
      "Iteration 10416: Policy loss: -0.134722. Value loss: 0.035486. Entropy: 0.292945.\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10417: Policy loss: -0.089437. Value loss: 0.118146. Entropy: 0.310308.\n",
      "Iteration 10418: Policy loss: -0.103025. Value loss: 0.057757. Entropy: 0.312683.\n",
      "Iteration 10419: Policy loss: -0.101840. Value loss: 0.040712. Entropy: 0.310819.\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10420: Policy loss: -0.000749. Value loss: 0.089647. Entropy: 0.306085.\n",
      "Iteration 10421: Policy loss: -0.005420. Value loss: 0.040848. Entropy: 0.305119.\n",
      "Iteration 10422: Policy loss: -0.008469. Value loss: 0.028128. Entropy: 0.305025.\n",
      "episode: 3870   score: 315.0  epsilon: 1.0    steps: 40  evaluation reward: 309.05\n",
      "episode: 3871   score: 400.0  epsilon: 1.0    steps: 712  evaluation reward: 310.5\n",
      "episode: 3872   score: 225.0  epsilon: 1.0    steps: 744  evaluation reward: 309.0\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10423: Policy loss: 0.318753. Value loss: 0.074099. Entropy: 0.273839.\n",
      "Iteration 10424: Policy loss: 0.314382. Value loss: 0.023627. Entropy: 0.274010.\n",
      "Iteration 10425: Policy loss: 0.309988. Value loss: 0.016895. Entropy: 0.274199.\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10426: Policy loss: -0.047862. Value loss: 0.104826. Entropy: 0.310826.\n",
      "Iteration 10427: Policy loss: -0.053334. Value loss: 0.045184. Entropy: 0.311568.\n",
      "Iteration 10428: Policy loss: -0.054165. Value loss: 0.029100. Entropy: 0.311917.\n",
      "episode: 3873   score: 345.0  epsilon: 1.0    steps: 40  evaluation reward: 310.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3874   score: 285.0  epsilon: 1.0    steps: 656  evaluation reward: 311.4\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10429: Policy loss: 0.196878. Value loss: 0.132578. Entropy: 0.288028.\n",
      "Iteration 10430: Policy loss: 0.191167. Value loss: 0.057749. Entropy: 0.285699.\n",
      "Iteration 10431: Policy loss: 0.181266. Value loss: 0.043015. Entropy: 0.285449.\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10432: Policy loss: -0.348180. Value loss: 0.158205. Entropy: 0.310552.\n",
      "Iteration 10433: Policy loss: -0.354501. Value loss: 0.065641. Entropy: 0.311370.\n",
      "Iteration 10434: Policy loss: -0.364253. Value loss: 0.040066. Entropy: 0.311879.\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10435: Policy loss: 0.174126. Value loss: 0.090686. Entropy: 0.308769.\n",
      "Iteration 10436: Policy loss: 0.166868. Value loss: 0.038997. Entropy: 0.309294.\n",
      "Iteration 10437: Policy loss: 0.171429. Value loss: 0.026068. Entropy: 0.308455.\n",
      "episode: 3875   score: 410.0  epsilon: 1.0    steps: 352  evaluation reward: 312.6\n",
      "episode: 3876   score: 390.0  epsilon: 1.0    steps: 648  evaluation reward: 314.3\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10438: Policy loss: -0.022624. Value loss: 0.139155. Entropy: 0.278330.\n",
      "Iteration 10439: Policy loss: -0.035699. Value loss: 0.052132. Entropy: 0.277547.\n",
      "Iteration 10440: Policy loss: -0.042423. Value loss: 0.035423. Entropy: 0.278337.\n",
      "episode: 3877   score: 210.0  epsilon: 1.0    steps: 416  evaluation reward: 313.1\n",
      "episode: 3878   score: 275.0  epsilon: 1.0    steps: 504  evaluation reward: 311.9\n",
      "episode: 3879   score: 405.0  epsilon: 1.0    steps: 816  evaluation reward: 310.8\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10441: Policy loss: 0.075464. Value loss: 0.067475. Entropy: 0.270740.\n",
      "Iteration 10442: Policy loss: 0.066580. Value loss: 0.027013. Entropy: 0.271777.\n",
      "Iteration 10443: Policy loss: 0.071095. Value loss: 0.020322. Entropy: 0.271003.\n",
      "episode: 3880   score: 345.0  epsilon: 1.0    steps: 440  evaluation reward: 311.4\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10444: Policy loss: 0.186764. Value loss: 0.097860. Entropy: 0.297018.\n",
      "Iteration 10445: Policy loss: 0.177864. Value loss: 0.046678. Entropy: 0.297442.\n",
      "Iteration 10446: Policy loss: 0.168038. Value loss: 0.035395. Entropy: 0.296459.\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10447: Policy loss: 0.004805. Value loss: 0.120177. Entropy: 0.310824.\n",
      "Iteration 10448: Policy loss: -0.006749. Value loss: 0.053633. Entropy: 0.310692.\n",
      "Iteration 10449: Policy loss: -0.012054. Value loss: 0.036452. Entropy: 0.311919.\n",
      "Training network. lr: 0.000170. clip: 0.068057\n",
      "Iteration 10450: Policy loss: 0.141890. Value loss: 0.064103. Entropy: 0.309559.\n",
      "Iteration 10451: Policy loss: 0.140239. Value loss: 0.031040. Entropy: 0.307879.\n",
      "Iteration 10452: Policy loss: 0.135185. Value loss: 0.020537. Entropy: 0.306787.\n",
      "episode: 3881   score: 345.0  epsilon: 1.0    steps: 232  evaluation reward: 310.45\n",
      "episode: 3882   score: 210.0  epsilon: 1.0    steps: 280  evaluation reward: 310.45\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10453: Policy loss: 0.029364. Value loss: 0.028199. Entropy: 0.278750.\n",
      "Iteration 10454: Policy loss: 0.027744. Value loss: 0.013483. Entropy: 0.278915.\n",
      "Iteration 10455: Policy loss: 0.023387. Value loss: 0.010532. Entropy: 0.279432.\n",
      "episode: 3883   score: 210.0  epsilon: 1.0    steps: 384  evaluation reward: 310.45\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10456: Policy loss: -0.361705. Value loss: 0.276801. Entropy: 0.296443.\n",
      "Iteration 10457: Policy loss: -0.393181. Value loss: 0.137811. Entropy: 0.296418.\n",
      "Iteration 10458: Policy loss: -0.391671. Value loss: 0.070442. Entropy: 0.295326.\n",
      "episode: 3884   score: 285.0  epsilon: 1.0    steps: 384  evaluation reward: 308.9\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10459: Policy loss: 0.196833. Value loss: 0.096914. Entropy: 0.301746.\n",
      "Iteration 10460: Policy loss: 0.190936. Value loss: 0.035755. Entropy: 0.300300.\n",
      "Iteration 10461: Policy loss: 0.190369. Value loss: 0.021972. Entropy: 0.300762.\n",
      "episode: 3885   score: 420.0  epsilon: 1.0    steps: 232  evaluation reward: 311.3\n",
      "episode: 3886   score: 335.0  epsilon: 1.0    steps: 960  evaluation reward: 308.8\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10462: Policy loss: -0.000334. Value loss: 0.138641. Entropy: 0.292493.\n",
      "Iteration 10463: Policy loss: -0.008267. Value loss: 0.049251. Entropy: 0.291389.\n",
      "Iteration 10464: Policy loss: -0.015442. Value loss: 0.035763. Entropy: 0.292243.\n",
      "episode: 3887   score: 455.0  epsilon: 1.0    steps: 360  evaluation reward: 311.1\n",
      "episode: 3888   score: 350.0  epsilon: 1.0    steps: 736  evaluation reward: 311.0\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10465: Policy loss: 0.084284. Value loss: 0.088285. Entropy: 0.275938.\n",
      "Iteration 10466: Policy loss: 0.069936. Value loss: 0.048972. Entropy: 0.276389.\n",
      "Iteration 10467: Policy loss: 0.070610. Value loss: 0.036282. Entropy: 0.277347.\n",
      "episode: 3889   score: 210.0  epsilon: 1.0    steps: 656  evaluation reward: 310.95\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10468: Policy loss: -0.019420. Value loss: 0.123768. Entropy: 0.299811.\n",
      "Iteration 10469: Policy loss: -0.028261. Value loss: 0.057069. Entropy: 0.299439.\n",
      "Iteration 10470: Policy loss: -0.030728. Value loss: 0.043019. Entropy: 0.299724.\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10471: Policy loss: 0.024788. Value loss: 0.096939. Entropy: 0.308719.\n",
      "Iteration 10472: Policy loss: 0.011115. Value loss: 0.040772. Entropy: 0.308779.\n",
      "Iteration 10473: Policy loss: 0.014819. Value loss: 0.025166. Entropy: 0.308362.\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10474: Policy loss: 0.262010. Value loss: 0.069831. Entropy: 0.306539.\n",
      "Iteration 10475: Policy loss: 0.257398. Value loss: 0.033594. Entropy: 0.306698.\n",
      "Iteration 10476: Policy loss: 0.253674. Value loss: 0.026851. Entropy: 0.306154.\n",
      "episode: 3890   score: 185.0  epsilon: 1.0    steps: 120  evaluation reward: 308.9\n",
      "episode: 3891   score: 290.0  epsilon: 1.0    steps: 504  evaluation reward: 307.75\n",
      "episode: 3892   score: 210.0  epsilon: 1.0    steps: 632  evaluation reward: 304.25\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10477: Policy loss: 0.043244. Value loss: 0.080174. Entropy: 0.270236.\n",
      "Iteration 10478: Policy loss: 0.037869. Value loss: 0.034444. Entropy: 0.270691.\n",
      "Iteration 10479: Policy loss: 0.036643. Value loss: 0.025569. Entropy: 0.270205.\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10480: Policy loss: -0.001229. Value loss: 0.137616. Entropy: 0.308309.\n",
      "Iteration 10481: Policy loss: -0.008719. Value loss: 0.053794. Entropy: 0.308385.\n",
      "Iteration 10482: Policy loss: 0.001454. Value loss: 0.036966. Entropy: 0.308959.\n",
      "episode: 3893   score: 395.0  epsilon: 1.0    steps: 424  evaluation reward: 306.05\n",
      "episode: 3894   score: 155.0  epsilon: 1.0    steps: 896  evaluation reward: 305.75\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10483: Policy loss: -0.028592. Value loss: 0.126752. Entropy: 0.291927.\n",
      "Iteration 10484: Policy loss: -0.029838. Value loss: 0.047425. Entropy: 0.292475.\n",
      "Iteration 10485: Policy loss: -0.038056. Value loss: 0.037397. Entropy: 0.293005.\n",
      "episode: 3895   score: 540.0  epsilon: 1.0    steps: 544  evaluation reward: 309.0\n",
      "episode: 3896   score: 305.0  epsilon: 1.0    steps: 920  evaluation reward: 308.9\n",
      "episode: 3897   score: 285.0  epsilon: 1.0    steps: 968  evaluation reward: 309.35\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10486: Policy loss: 0.153824. Value loss: 0.144125. Entropy: 0.288040.\n",
      "Iteration 10487: Policy loss: 0.143074. Value loss: 0.069039. Entropy: 0.288146.\n",
      "Iteration 10488: Policy loss: 0.135191. Value loss: 0.054200. Entropy: 0.288109.\n",
      "episode: 3898   score: 180.0  epsilon: 1.0    steps: 752  evaluation reward: 309.65\n",
      "episode: 3899   score: 210.0  epsilon: 1.0    steps: 928  evaluation reward: 301.7\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10489: Policy loss: -0.164875. Value loss: 0.156265. Entropy: 0.275441.\n",
      "Iteration 10490: Policy loss: -0.177969. Value loss: 0.066836. Entropy: 0.276404.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10491: Policy loss: -0.177243. Value loss: 0.053581. Entropy: 0.274513.\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10492: Policy loss: 0.006356. Value loss: 0.296970. Entropy: 0.305423.\n",
      "Iteration 10493: Policy loss: -0.002517. Value loss: 0.099397. Entropy: 0.305647.\n",
      "Iteration 10494: Policy loss: -0.018945. Value loss: 0.037799. Entropy: 0.303608.\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10495: Policy loss: 0.151166. Value loss: 0.125143. Entropy: 0.308841.\n",
      "Iteration 10496: Policy loss: 0.138488. Value loss: 0.040255. Entropy: 0.306331.\n",
      "Iteration 10497: Policy loss: 0.142724. Value loss: 0.027736. Entropy: 0.307337.\n",
      "episode: 3900   score: 215.0  epsilon: 1.0    steps: 872  evaluation reward: 300.3\n",
      "Training network. lr: 0.000170. clip: 0.067901\n",
      "Iteration 10498: Policy loss: 0.231062. Value loss: 0.069766. Entropy: 0.297097.\n",
      "Iteration 10499: Policy loss: 0.228697. Value loss: 0.034569. Entropy: 0.295802.\n",
      "Iteration 10500: Policy loss: 0.225317. Value loss: 0.027642. Entropy: 0.294705.\n",
      "now time :  2019-09-06 01:06:32.115200\n",
      "episode: 3901   score: 135.0  epsilon: 1.0    steps: 792  evaluation reward: 296.5\n",
      "episode: 3902   score: 250.0  epsilon: 1.0    steps: 872  evaluation reward: 295.35\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10501: Policy loss: -0.032908. Value loss: 0.110640. Entropy: 0.284209.\n",
      "Iteration 10502: Policy loss: -0.037616. Value loss: 0.045444. Entropy: 0.286236.\n",
      "Iteration 10503: Policy loss: -0.039445. Value loss: 0.034297. Entropy: 0.284578.\n",
      "episode: 3903   score: 210.0  epsilon: 1.0    steps: 432  evaluation reward: 295.65\n",
      "episode: 3904   score: 210.0  epsilon: 1.0    steps: 592  evaluation reward: 294.65\n",
      "episode: 3905   score: 500.0  epsilon: 1.0    steps: 952  evaluation reward: 297.55\n",
      "episode: 3906   score: 210.0  epsilon: 1.0    steps: 1000  evaluation reward: 297.55\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10504: Policy loss: -0.004291. Value loss: 0.116514. Entropy: 0.273516.\n",
      "Iteration 10505: Policy loss: -0.014746. Value loss: 0.056801. Entropy: 0.273277.\n",
      "Iteration 10506: Policy loss: -0.010416. Value loss: 0.041989. Entropy: 0.275681.\n",
      "episode: 3907   score: 520.0  epsilon: 1.0    steps: 728  evaluation reward: 301.05\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10507: Policy loss: -0.063778. Value loss: 0.164015. Entropy: 0.285133.\n",
      "Iteration 10508: Policy loss: -0.074159. Value loss: 0.068420. Entropy: 0.285142.\n",
      "Iteration 10509: Policy loss: -0.091892. Value loss: 0.043997. Entropy: 0.285385.\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10510: Policy loss: 0.138545. Value loss: 0.105945. Entropy: 0.306240.\n",
      "Iteration 10511: Policy loss: 0.138967. Value loss: 0.043606. Entropy: 0.306941.\n",
      "Iteration 10512: Policy loss: 0.136943. Value loss: 0.031738. Entropy: 0.305956.\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10513: Policy loss: 0.219666. Value loss: 0.050742. Entropy: 0.314559.\n",
      "Iteration 10514: Policy loss: 0.213443. Value loss: 0.021961. Entropy: 0.315152.\n",
      "Iteration 10515: Policy loss: 0.209959. Value loss: 0.021021. Entropy: 0.314168.\n",
      "episode: 3908   score: 210.0  epsilon: 1.0    steps: 760  evaluation reward: 301.3\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10516: Policy loss: -0.141579. Value loss: 0.069212. Entropy: 0.294132.\n",
      "Iteration 10517: Policy loss: -0.145758. Value loss: 0.035882. Entropy: 0.295530.\n",
      "Iteration 10518: Policy loss: -0.146060. Value loss: 0.026096. Entropy: 0.294740.\n",
      "episode: 3909   score: 215.0  epsilon: 1.0    steps: 16  evaluation reward: 297.9\n",
      "episode: 3910   score: 220.0  epsilon: 1.0    steps: 472  evaluation reward: 295.45\n",
      "episode: 3911   score: 110.0  epsilon: 1.0    steps: 712  evaluation reward: 292.15\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10519: Policy loss: -0.056964. Value loss: 0.060301. Entropy: 0.271179.\n",
      "Iteration 10520: Policy loss: -0.059013. Value loss: 0.033830. Entropy: 0.270609.\n",
      "Iteration 10521: Policy loss: -0.057499. Value loss: 0.025519. Entropy: 0.272012.\n",
      "episode: 3912   score: 230.0  epsilon: 1.0    steps: 248  evaluation reward: 292.65\n",
      "episode: 3913   score: 275.0  epsilon: 1.0    steps: 768  evaluation reward: 291.8\n",
      "episode: 3914   score: 370.0  epsilon: 1.0    steps: 840  evaluation reward: 292.25\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10522: Policy loss: -0.091999. Value loss: 0.102029. Entropy: 0.284344.\n",
      "Iteration 10523: Policy loss: -0.098589. Value loss: 0.041046. Entropy: 0.284529.\n",
      "Iteration 10524: Policy loss: -0.095929. Value loss: 0.028744. Entropy: 0.283101.\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10525: Policy loss: -0.098135. Value loss: 0.094863. Entropy: 0.308994.\n",
      "Iteration 10526: Policy loss: -0.103113. Value loss: 0.035166. Entropy: 0.308138.\n",
      "Iteration 10527: Policy loss: -0.107070. Value loss: 0.027191. Entropy: 0.308993.\n",
      "episode: 3915   score: 330.0  epsilon: 1.0    steps: 224  evaluation reward: 293.75\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10528: Policy loss: 0.030630. Value loss: 0.060857. Entropy: 0.298625.\n",
      "Iteration 10529: Policy loss: 0.027720. Value loss: 0.026183. Entropy: 0.298583.\n",
      "Iteration 10530: Policy loss: 0.024574. Value loss: 0.021773. Entropy: 0.296631.\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10531: Policy loss: 0.093595. Value loss: 0.043969. Entropy: 0.303225.\n",
      "Iteration 10532: Policy loss: 0.087802. Value loss: 0.014234. Entropy: 0.305816.\n",
      "Iteration 10533: Policy loss: 0.086759. Value loss: 0.011410. Entropy: 0.305271.\n",
      "episode: 3916   score: 220.0  epsilon: 1.0    steps: 392  evaluation reward: 292.05\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10534: Policy loss: -0.055436. Value loss: 0.331330. Entropy: 0.294945.\n",
      "Iteration 10535: Policy loss: -0.069914. Value loss: 0.113520. Entropy: 0.293282.\n",
      "Iteration 10536: Policy loss: -0.092668. Value loss: 0.062188. Entropy: 0.290927.\n",
      "episode: 3917   score: 315.0  epsilon: 1.0    steps: 928  evaluation reward: 289.55\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10537: Policy loss: 0.091037. Value loss: 0.102154. Entropy: 0.306878.\n",
      "Iteration 10538: Policy loss: 0.085505. Value loss: 0.049280. Entropy: 0.306526.\n",
      "Iteration 10539: Policy loss: 0.078626. Value loss: 0.042210. Entropy: 0.307190.\n",
      "episode: 3918   score: 180.0  epsilon: 1.0    steps: 600  evaluation reward: 287.15\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10540: Policy loss: 0.183237. Value loss: 0.183140. Entropy: 0.289329.\n",
      "Iteration 10541: Policy loss: 0.178815. Value loss: 0.106254. Entropy: 0.287928.\n",
      "Iteration 10542: Policy loss: 0.165544. Value loss: 0.059123. Entropy: 0.287153.\n",
      "episode: 3919   score: 390.0  epsilon: 1.0    steps: 1000  evaluation reward: 287.7\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10543: Policy loss: -0.128232. Value loss: 0.136946. Entropy: 0.309790.\n",
      "Iteration 10544: Policy loss: -0.138488. Value loss: 0.065419. Entropy: 0.309381.\n",
      "Iteration 10545: Policy loss: -0.133908. Value loss: 0.041553. Entropy: 0.309520.\n",
      "episode: 3920   score: 515.0  epsilon: 1.0    steps: 24  evaluation reward: 287.95\n",
      "episode: 3921   score: 180.0  epsilon: 1.0    steps: 104  evaluation reward: 284.95\n",
      "episode: 3922   score: 345.0  epsilon: 1.0    steps: 480  evaluation reward: 286.6\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10546: Policy loss: -0.238154. Value loss: 0.228047. Entropy: 0.260407.\n",
      "Iteration 10547: Policy loss: -0.244088. Value loss: 0.073932. Entropy: 0.261275.\n",
      "Iteration 10548: Policy loss: -0.263254. Value loss: 0.053459. Entropy: 0.260746.\n",
      "episode: 3923   score: 345.0  epsilon: 1.0    steps: 632  evaluation reward: 285.55\n",
      "Training network. lr: 0.000169. clip: 0.067753\n",
      "Iteration 10549: Policy loss: 0.051036. Value loss: 0.109909. Entropy: 0.296216.\n",
      "Iteration 10550: Policy loss: 0.045484. Value loss: 0.045431. Entropy: 0.296728.\n",
      "Iteration 10551: Policy loss: 0.033935. Value loss: 0.031035. Entropy: 0.296703.\n",
      "episode: 3924   score: 650.0  epsilon: 1.0    steps: 576  evaluation reward: 290.25\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10552: Policy loss: 0.127777. Value loss: 0.113070. Entropy: 0.291542.\n",
      "Iteration 10553: Policy loss: 0.110556. Value loss: 0.049833. Entropy: 0.291840.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10554: Policy loss: 0.114908. Value loss: 0.035256. Entropy: 0.291796.\n",
      "episode: 3925   score: 220.0  epsilon: 1.0    steps: 288  evaluation reward: 288.2\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10555: Policy loss: 0.174823. Value loss: 0.088296. Entropy: 0.297796.\n",
      "Iteration 10556: Policy loss: 0.164220. Value loss: 0.036987. Entropy: 0.296592.\n",
      "Iteration 10557: Policy loss: 0.161558. Value loss: 0.026487. Entropy: 0.296276.\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10558: Policy loss: 0.237331. Value loss: 0.085637. Entropy: 0.303619.\n",
      "Iteration 10559: Policy loss: 0.228466. Value loss: 0.033623. Entropy: 0.305204.\n",
      "Iteration 10560: Policy loss: 0.230329. Value loss: 0.022213. Entropy: 0.303896.\n",
      "episode: 3926   score: 210.0  epsilon: 1.0    steps: 232  evaluation reward: 284.4\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10561: Policy loss: 0.018165. Value loss: 0.111531. Entropy: 0.292679.\n",
      "Iteration 10562: Policy loss: 0.019233. Value loss: 0.044354. Entropy: 0.294256.\n",
      "Iteration 10563: Policy loss: 0.003527. Value loss: 0.031311. Entropy: 0.292690.\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10564: Policy loss: -0.068256. Value loss: 0.125344. Entropy: 0.309785.\n",
      "Iteration 10565: Policy loss: -0.081265. Value loss: 0.048269. Entropy: 0.309474.\n",
      "Iteration 10566: Policy loss: -0.079312. Value loss: 0.038263. Entropy: 0.309525.\n",
      "episode: 3927   score: 435.0  epsilon: 1.0    steps: 384  evaluation reward: 286.95\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10567: Policy loss: -0.050469. Value loss: 0.067633. Entropy: 0.295763.\n",
      "Iteration 10568: Policy loss: -0.052762. Value loss: 0.031279. Entropy: 0.295779.\n",
      "Iteration 10569: Policy loss: -0.053460. Value loss: 0.022390. Entropy: 0.295797.\n",
      "episode: 3928   score: 320.0  epsilon: 1.0    steps: 144  evaluation reward: 287.85\n",
      "episode: 3929   score: 285.0  epsilon: 1.0    steps: 144  evaluation reward: 289.15\n",
      "episode: 3930   score: 395.0  epsilon: 1.0    steps: 264  evaluation reward: 290.5\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10570: Policy loss: 0.125629. Value loss: 0.059384. Entropy: 0.271477.\n",
      "Iteration 10571: Policy loss: 0.121816. Value loss: 0.031505. Entropy: 0.273092.\n",
      "Iteration 10572: Policy loss: 0.116601. Value loss: 0.023593. Entropy: 0.272222.\n",
      "episode: 3931   score: 170.0  epsilon: 1.0    steps: 192  evaluation reward: 290.05\n",
      "episode: 3932   score: 345.0  epsilon: 1.0    steps: 896  evaluation reward: 290.65\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10573: Policy loss: 0.363413. Value loss: 0.105586. Entropy: 0.292884.\n",
      "Iteration 10574: Policy loss: 0.357207. Value loss: 0.048590. Entropy: 0.292149.\n",
      "Iteration 10575: Policy loss: 0.356329. Value loss: 0.033375. Entropy: 0.291028.\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10576: Policy loss: -0.000538. Value loss: 0.072094. Entropy: 0.304974.\n",
      "Iteration 10577: Policy loss: -0.003853. Value loss: 0.026397. Entropy: 0.306131.\n",
      "Iteration 10578: Policy loss: -0.011914. Value loss: 0.022326. Entropy: 0.305376.\n",
      "episode: 3933   score: 210.0  epsilon: 1.0    steps: 936  evaluation reward: 290.45\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10579: Policy loss: 0.081417. Value loss: 0.095466. Entropy: 0.306845.\n",
      "Iteration 10580: Policy loss: 0.076064. Value loss: 0.061139. Entropy: 0.306901.\n",
      "Iteration 10581: Policy loss: 0.079649. Value loss: 0.049350. Entropy: 0.307097.\n",
      "episode: 3934   score: 315.0  epsilon: 1.0    steps: 40  evaluation reward: 292.35\n",
      "episode: 3935   score: 160.0  epsilon: 1.0    steps: 112  evaluation reward: 291.85\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10582: Policy loss: -0.347648. Value loss: 0.313454. Entropy: 0.280078.\n",
      "Iteration 10583: Policy loss: -0.355096. Value loss: 0.155380. Entropy: 0.276709.\n",
      "Iteration 10584: Policy loss: -0.355564. Value loss: 0.081473. Entropy: 0.277537.\n",
      "episode: 3936   score: 185.0  epsilon: 1.0    steps: 368  evaluation reward: 288.1\n",
      "episode: 3937   score: 725.0  epsilon: 1.0    steps: 704  evaluation reward: 293.5\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10585: Policy loss: 0.101509. Value loss: 0.156846. Entropy: 0.283434.\n",
      "Iteration 10586: Policy loss: 0.088230. Value loss: 0.072253. Entropy: 0.281638.\n",
      "Iteration 10587: Policy loss: 0.101171. Value loss: 0.053333. Entropy: 0.281917.\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10588: Policy loss: -0.130655. Value loss: 0.207890. Entropy: 0.306267.\n",
      "Iteration 10589: Policy loss: -0.138227. Value loss: 0.065721. Entropy: 0.307567.\n",
      "Iteration 10590: Policy loss: -0.146702. Value loss: 0.047370. Entropy: 0.307056.\n",
      "episode: 3938   score: 585.0  epsilon: 1.0    steps: 960  evaluation reward: 290.9\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10591: Policy loss: 0.055608. Value loss: 0.133090. Entropy: 0.307310.\n",
      "Iteration 10592: Policy loss: 0.039305. Value loss: 0.073411. Entropy: 0.306636.\n",
      "Iteration 10593: Policy loss: 0.033034. Value loss: 0.056161. Entropy: 0.306873.\n",
      "episode: 3939   score: 180.0  epsilon: 1.0    steps: 568  evaluation reward: 290.05\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10594: Policy loss: 0.240039. Value loss: 0.072694. Entropy: 0.285232.\n",
      "Iteration 10595: Policy loss: 0.235659. Value loss: 0.026415. Entropy: 0.284510.\n",
      "Iteration 10596: Policy loss: 0.236040. Value loss: 0.020723. Entropy: 0.282754.\n",
      "episode: 3940   score: 335.0  epsilon: 1.0    steps: 256  evaluation reward: 291.3\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10597: Policy loss: 0.159190. Value loss: 0.109918. Entropy: 0.295049.\n",
      "Iteration 10598: Policy loss: 0.158150. Value loss: 0.042083. Entropy: 0.294399.\n",
      "Iteration 10599: Policy loss: 0.149687. Value loss: 0.023533. Entropy: 0.294522.\n",
      "episode: 3941   score: 240.0  epsilon: 1.0    steps: 256  evaluation reward: 292.1\n",
      "episode: 3942   score: 210.0  epsilon: 1.0    steps: 656  evaluation reward: 292.65\n",
      "episode: 3943   score: 210.0  epsilon: 1.0    steps: 896  evaluation reward: 291.3\n",
      "Training network. lr: 0.000169. clip: 0.067597\n",
      "Iteration 10600: Policy loss: -0.054188. Value loss: 0.115510. Entropy: 0.279558.\n",
      "Iteration 10601: Policy loss: -0.058891. Value loss: 0.050064. Entropy: 0.279908.\n",
      "Iteration 10602: Policy loss: -0.065259. Value loss: 0.035109. Entropy: 0.281713.\n",
      "episode: 3944   score: 590.0  epsilon: 1.0    steps: 824  evaluation reward: 296.1\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10603: Policy loss: -0.060331. Value loss: 0.084215. Entropy: 0.301420.\n",
      "Iteration 10604: Policy loss: -0.065537. Value loss: 0.050332. Entropy: 0.300833.\n",
      "Iteration 10605: Policy loss: -0.070395. Value loss: 0.039164. Entropy: 0.301024.\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10606: Policy loss: -0.218767. Value loss: 0.270814. Entropy: 0.308401.\n",
      "Iteration 10607: Policy loss: -0.246115. Value loss: 0.174565. Entropy: 0.308025.\n",
      "Iteration 10608: Policy loss: -0.253574. Value loss: 0.076898. Entropy: 0.308918.\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10609: Policy loss: 0.096552. Value loss: 0.267068. Entropy: 0.308646.\n",
      "Iteration 10610: Policy loss: 0.079242. Value loss: 0.076691. Entropy: 0.308717.\n",
      "Iteration 10611: Policy loss: 0.055871. Value loss: 0.044896. Entropy: 0.308889.\n",
      "episode: 3945   score: 280.0  epsilon: 1.0    steps: 432  evaluation reward: 296.3\n",
      "episode: 3946   score: 380.0  epsilon: 1.0    steps: 704  evaluation reward: 298.0\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10612: Policy loss: 0.247548. Value loss: 0.137283. Entropy: 0.284370.\n",
      "Iteration 10613: Policy loss: 0.233707. Value loss: 0.062830. Entropy: 0.283312.\n",
      "Iteration 10614: Policy loss: 0.235520. Value loss: 0.052478. Entropy: 0.282820.\n",
      "episode: 3947   score: 210.0  epsilon: 1.0    steps: 704  evaluation reward: 297.8\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10615: Policy loss: 0.154339. Value loss: 0.123957. Entropy: 0.301755.\n",
      "Iteration 10616: Policy loss: 0.144040. Value loss: 0.049842. Entropy: 0.301206.\n",
      "Iteration 10617: Policy loss: 0.133287. Value loss: 0.032156. Entropy: 0.302073.\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10618: Policy loss: -0.112267. Value loss: 0.133484. Entropy: 0.304894.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10619: Policy loss: -0.126245. Value loss: 0.066583. Entropy: 0.306367.\n",
      "Iteration 10620: Policy loss: -0.127109. Value loss: 0.046887. Entropy: 0.306056.\n",
      "episode: 3948   score: 325.0  epsilon: 1.0    steps: 120  evaluation reward: 298.55\n",
      "episode: 3949   score: 565.0  epsilon: 1.0    steps: 552  evaluation reward: 302.1\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10621: Policy loss: -0.245249. Value loss: 0.178127. Entropy: 0.274089.\n",
      "Iteration 10622: Policy loss: -0.245315. Value loss: 0.064325. Entropy: 0.273784.\n",
      "Iteration 10623: Policy loss: -0.256045. Value loss: 0.049123. Entropy: 0.272961.\n",
      "episode: 3950   score: 450.0  epsilon: 1.0    steps: 168  evaluation reward: 305.0\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10624: Policy loss: 0.291898. Value loss: 0.163746. Entropy: 0.298158.\n",
      "Iteration 10625: Policy loss: 0.281214. Value loss: 0.052728. Entropy: 0.296080.\n",
      "Iteration 10626: Policy loss: 0.269162. Value loss: 0.035232. Entropy: 0.295575.\n",
      "now time :  2019-09-06 01:14:20.397727\n",
      "episode: 3951   score: 210.0  epsilon: 1.0    steps: 520  evaluation reward: 305.55\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10627: Policy loss: 0.108438. Value loss: 0.099372. Entropy: 0.291820.\n",
      "Iteration 10628: Policy loss: 0.103168. Value loss: 0.044783. Entropy: 0.292101.\n",
      "Iteration 10629: Policy loss: 0.104461. Value loss: 0.031704. Entropy: 0.291827.\n",
      "episode: 3952   score: 365.0  epsilon: 1.0    steps: 880  evaluation reward: 306.35\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10630: Policy loss: 0.146690. Value loss: 0.104839. Entropy: 0.305916.\n",
      "Iteration 10631: Policy loss: 0.141032. Value loss: 0.055015. Entropy: 0.304237.\n",
      "Iteration 10632: Policy loss: 0.140628. Value loss: 0.045987. Entropy: 0.304389.\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10633: Policy loss: 0.106282. Value loss: 0.174460. Entropy: 0.305493.\n",
      "Iteration 10634: Policy loss: 0.086105. Value loss: 0.057497. Entropy: 0.303393.\n",
      "Iteration 10635: Policy loss: 0.101566. Value loss: 0.040892. Entropy: 0.304427.\n",
      "episode: 3953   score: 565.0  epsilon: 1.0    steps: 416  evaluation reward: 309.9\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10636: Policy loss: -0.089618. Value loss: 0.150403. Entropy: 0.294387.\n",
      "Iteration 10637: Policy loss: -0.086514. Value loss: 0.055122. Entropy: 0.293372.\n",
      "Iteration 10638: Policy loss: -0.092900. Value loss: 0.036133. Entropy: 0.293875.\n",
      "episode: 3954   score: 335.0  epsilon: 1.0    steps: 72  evaluation reward: 309.85\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10639: Policy loss: -0.011392. Value loss: 0.133178. Entropy: 0.300017.\n",
      "Iteration 10640: Policy loss: -0.011801. Value loss: 0.062489. Entropy: 0.300749.\n",
      "Iteration 10641: Policy loss: -0.022924. Value loss: 0.045202. Entropy: 0.300125.\n",
      "episode: 3955   score: 360.0  epsilon: 1.0    steps: 320  evaluation reward: 311.35\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10642: Policy loss: -0.341846. Value loss: 0.323663. Entropy: 0.298444.\n",
      "Iteration 10643: Policy loss: -0.357334. Value loss: 0.151617. Entropy: 0.299838.\n",
      "Iteration 10644: Policy loss: -0.378547. Value loss: 0.098188. Entropy: 0.299678.\n",
      "episode: 3956   score: 430.0  epsilon: 1.0    steps: 56  evaluation reward: 314.4\n",
      "episode: 3957   score: 440.0  epsilon: 1.0    steps: 912  evaluation reward: 316.7\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10645: Policy loss: -0.594304. Value loss: 0.397813. Entropy: 0.296199.\n",
      "Iteration 10646: Policy loss: -0.614786. Value loss: 0.094404. Entropy: 0.296658.\n",
      "Iteration 10647: Policy loss: -0.624448. Value loss: 0.056248. Entropy: 0.296905.\n",
      "episode: 3958   score: 445.0  epsilon: 1.0    steps: 1008  evaluation reward: 319.05\n",
      "Training network. lr: 0.000169. clip: 0.067440\n",
      "Iteration 10648: Policy loss: 0.125257. Value loss: 0.186979. Entropy: 0.309726.\n",
      "Iteration 10649: Policy loss: 0.104572. Value loss: 0.066824. Entropy: 0.309196.\n",
      "Iteration 10650: Policy loss: 0.099735. Value loss: 0.050844. Entropy: 0.309142.\n",
      "episode: 3959   score: 620.0  epsilon: 1.0    steps: 920  evaluation reward: 320.0\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10651: Policy loss: 0.254923. Value loss: 0.176696. Entropy: 0.298868.\n",
      "Iteration 10652: Policy loss: 0.259790. Value loss: 0.041984. Entropy: 0.299312.\n",
      "Iteration 10653: Policy loss: 0.254369. Value loss: 0.026741. Entropy: 0.298471.\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10654: Policy loss: 0.110697. Value loss: 0.140881. Entropy: 0.306290.\n",
      "Iteration 10655: Policy loss: 0.100784. Value loss: 0.055771. Entropy: 0.305979.\n",
      "Iteration 10656: Policy loss: 0.085367. Value loss: 0.039432. Entropy: 0.305719.\n",
      "episode: 3960   score: 465.0  epsilon: 1.0    steps: 944  evaluation reward: 322.55\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10657: Policy loss: -0.096327. Value loss: 0.329206. Entropy: 0.308337.\n",
      "Iteration 10658: Policy loss: -0.100512. Value loss: 0.143561. Entropy: 0.308777.\n",
      "Iteration 10659: Policy loss: -0.110623. Value loss: 0.093632. Entropy: 0.308838.\n",
      "episode: 3961   score: 355.0  epsilon: 1.0    steps: 696  evaluation reward: 323.2\n",
      "episode: 3962   score: 555.0  epsilon: 1.0    steps: 1008  evaluation reward: 325.6\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10660: Policy loss: 0.220554. Value loss: 0.136424. Entropy: 0.289255.\n",
      "Iteration 10661: Policy loss: 0.212085. Value loss: 0.058079. Entropy: 0.288786.\n",
      "Iteration 10662: Policy loss: 0.205433. Value loss: 0.043440. Entropy: 0.288457.\n",
      "episode: 3963   score: 125.0  epsilon: 1.0    steps: 528  evaluation reward: 325.8\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10663: Policy loss: 0.135899. Value loss: 0.129681. Entropy: 0.288570.\n",
      "Iteration 10664: Policy loss: 0.134983. Value loss: 0.041902. Entropy: 0.287520.\n",
      "Iteration 10665: Policy loss: 0.119623. Value loss: 0.026204. Entropy: 0.287995.\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10666: Policy loss: -0.167541. Value loss: 0.103254. Entropy: 0.311677.\n",
      "Iteration 10667: Policy loss: -0.177072. Value loss: 0.051618. Entropy: 0.311965.\n",
      "Iteration 10668: Policy loss: -0.183135. Value loss: 0.036545. Entropy: 0.311370.\n",
      "episode: 3964   score: 445.0  epsilon: 1.0    steps: 520  evaluation reward: 328.0\n",
      "episode: 3965   score: 315.0  epsilon: 1.0    steps: 920  evaluation reward: 325.8\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10669: Policy loss: -0.107746. Value loss: 0.166323. Entropy: 0.294270.\n",
      "Iteration 10670: Policy loss: -0.107108. Value loss: 0.063200. Entropy: 0.293911.\n",
      "Iteration 10671: Policy loss: -0.122959. Value loss: 0.047188. Entropy: 0.293965.\n",
      "episode: 3966   score: 595.0  epsilon: 1.0    steps: 608  evaluation reward: 328.4\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10672: Policy loss: 0.580948. Value loss: 0.227115. Entropy: 0.291329.\n",
      "Iteration 10673: Policy loss: 0.556671. Value loss: 0.084858. Entropy: 0.289513.\n",
      "Iteration 10674: Policy loss: 0.551232. Value loss: 0.045507. Entropy: 0.289921.\n",
      "episode: 3967   score: 775.0  epsilon: 1.0    steps: 768  evaluation reward: 331.75\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10675: Policy loss: 0.024963. Value loss: 0.193280. Entropy: 0.303660.\n",
      "Iteration 10676: Policy loss: 0.008351. Value loss: 0.075170. Entropy: 0.303351.\n",
      "Iteration 10677: Policy loss: 0.000686. Value loss: 0.055888. Entropy: 0.302773.\n",
      "episode: 3968   score: 305.0  epsilon: 1.0    steps: 608  evaluation reward: 332.05\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10678: Policy loss: -0.168614. Value loss: 0.307525. Entropy: 0.296672.\n",
      "Iteration 10679: Policy loss: -0.189691. Value loss: 0.130335. Entropy: 0.296504.\n",
      "Iteration 10680: Policy loss: -0.196314. Value loss: 0.070014. Entropy: 0.296302.\n",
      "episode: 3969   score: 590.0  epsilon: 1.0    steps: 568  evaluation reward: 333.6\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10681: Policy loss: 0.173094. Value loss: 0.114056. Entropy: 0.293969.\n",
      "Iteration 10682: Policy loss: 0.166652. Value loss: 0.055133. Entropy: 0.293201.\n",
      "Iteration 10683: Policy loss: 0.170582. Value loss: 0.043902. Entropy: 0.294375.\n",
      "episode: 3970   score: 265.0  epsilon: 1.0    steps: 112  evaluation reward: 333.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10684: Policy loss: 0.097935. Value loss: 0.116807. Entropy: 0.298187.\n",
      "Iteration 10685: Policy loss: 0.095753. Value loss: 0.053893. Entropy: 0.297852.\n",
      "Iteration 10686: Policy loss: 0.094967. Value loss: 0.040421. Entropy: 0.297741.\n",
      "episode: 3971   score: 410.0  epsilon: 1.0    steps: 856  evaluation reward: 333.2\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10687: Policy loss: -0.063565. Value loss: 0.111106. Entropy: 0.307166.\n",
      "Iteration 10688: Policy loss: -0.072216. Value loss: 0.046509. Entropy: 0.307078.\n",
      "Iteration 10689: Policy loss: -0.078259. Value loss: 0.031381. Entropy: 0.307463.\n",
      "episode: 3972   score: 330.0  epsilon: 1.0    steps: 720  evaluation reward: 334.25\n",
      "episode: 3973   score: 210.0  epsilon: 1.0    steps: 776  evaluation reward: 332.9\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10690: Policy loss: 0.033048. Value loss: 0.124996. Entropy: 0.296684.\n",
      "Iteration 10691: Policy loss: 0.032487. Value loss: 0.052927. Entropy: 0.298030.\n",
      "Iteration 10692: Policy loss: 0.031706. Value loss: 0.036378. Entropy: 0.297928.\n",
      "episode: 3974   score: 570.0  epsilon: 1.0    steps: 304  evaluation reward: 335.75\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10693: Policy loss: -0.222618. Value loss: 0.235382. Entropy: 0.303998.\n",
      "Iteration 10694: Policy loss: -0.233398. Value loss: 0.099096. Entropy: 0.305453.\n",
      "Iteration 10695: Policy loss: -0.216223. Value loss: 0.055683. Entropy: 0.306633.\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10696: Policy loss: 0.272472. Value loss: 0.200241. Entropy: 0.312447.\n",
      "Iteration 10697: Policy loss: 0.258637. Value loss: 0.064583. Entropy: 0.312226.\n",
      "Iteration 10698: Policy loss: 0.240997. Value loss: 0.042832. Entropy: 0.312477.\n",
      "episode: 3975   score: 270.0  epsilon: 1.0    steps: 232  evaluation reward: 334.35\n",
      "Training network. lr: 0.000168. clip: 0.067292\n",
      "Iteration 10699: Policy loss: 0.196184. Value loss: 0.161645. Entropy: 0.299552.\n",
      "Iteration 10700: Policy loss: 0.191756. Value loss: 0.075084. Entropy: 0.300810.\n",
      "Iteration 10701: Policy loss: 0.185144. Value loss: 0.049464. Entropy: 0.299241.\n",
      "episode: 3976   score: 355.0  epsilon: 1.0    steps: 912  evaluation reward: 334.0\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10702: Policy loss: 0.159821. Value loss: 0.127900. Entropy: 0.309520.\n",
      "Iteration 10703: Policy loss: 0.163589. Value loss: 0.047494. Entropy: 0.309471.\n",
      "Iteration 10704: Policy loss: 0.154265. Value loss: 0.031250. Entropy: 0.309212.\n",
      "episode: 3977   score: 215.0  epsilon: 1.0    steps: 688  evaluation reward: 334.05\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10705: Policy loss: 0.016678. Value loss: 0.121926. Entropy: 0.300706.\n",
      "Iteration 10706: Policy loss: 0.011612. Value loss: 0.056331. Entropy: 0.302795.\n",
      "Iteration 10707: Policy loss: -0.000158. Value loss: 0.041101. Entropy: 0.301110.\n",
      "episode: 3978   score: 210.0  epsilon: 1.0    steps: 272  evaluation reward: 333.4\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10708: Policy loss: 0.040120. Value loss: 0.193383. Entropy: 0.301235.\n",
      "Iteration 10709: Policy loss: 0.023464. Value loss: 0.088303. Entropy: 0.300888.\n",
      "Iteration 10710: Policy loss: 0.022129. Value loss: 0.059729. Entropy: 0.301295.\n",
      "episode: 3979   score: 345.0  epsilon: 1.0    steps: 440  evaluation reward: 332.8\n",
      "episode: 3980   score: 220.0  epsilon: 1.0    steps: 536  evaluation reward: 331.55\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10711: Policy loss: 0.349989. Value loss: 0.095730. Entropy: 0.294376.\n",
      "Iteration 10712: Policy loss: 0.338109. Value loss: 0.040483. Entropy: 0.292571.\n",
      "Iteration 10713: Policy loss: 0.325643. Value loss: 0.030022. Entropy: 0.292849.\n",
      "episode: 3981   score: 660.0  epsilon: 1.0    steps: 728  evaluation reward: 334.7\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10714: Policy loss: 0.111989. Value loss: 0.097147. Entropy: 0.303803.\n",
      "Iteration 10715: Policy loss: 0.117209. Value loss: 0.043915. Entropy: 0.301888.\n",
      "Iteration 10716: Policy loss: 0.104259. Value loss: 0.030506. Entropy: 0.301602.\n",
      "episode: 3982   score: 595.0  epsilon: 1.0    steps: 88  evaluation reward: 338.55\n",
      "episode: 3983   score: 210.0  epsilon: 1.0    steps: 264  evaluation reward: 338.55\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10717: Policy loss: -0.225416. Value loss: 0.194790. Entropy: 0.289524.\n",
      "Iteration 10718: Policy loss: -0.239202. Value loss: 0.063691. Entropy: 0.290672.\n",
      "Iteration 10719: Policy loss: -0.246065. Value loss: 0.035621. Entropy: 0.289371.\n",
      "episode: 3984   score: 185.0  epsilon: 1.0    steps: 320  evaluation reward: 337.55\n",
      "episode: 3985   score: 285.0  epsilon: 1.0    steps: 880  evaluation reward: 336.2\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10720: Policy loss: -0.339691. Value loss: 0.196012. Entropy: 0.297548.\n",
      "Iteration 10721: Policy loss: -0.336877. Value loss: 0.082065. Entropy: 0.297599.\n",
      "Iteration 10722: Policy loss: -0.356800. Value loss: 0.067193. Entropy: 0.296895.\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10723: Policy loss: -0.221611. Value loss: 0.271544. Entropy: 0.307134.\n",
      "Iteration 10724: Policy loss: -0.242237. Value loss: 0.097263. Entropy: 0.305861.\n",
      "Iteration 10725: Policy loss: -0.262328. Value loss: 0.070336. Entropy: 0.306952.\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10726: Policy loss: 0.194301. Value loss: 0.080981. Entropy: 0.309808.\n",
      "Iteration 10727: Policy loss: 0.195972. Value loss: 0.034482. Entropy: 0.309827.\n",
      "Iteration 10728: Policy loss: 0.189938. Value loss: 0.022929. Entropy: 0.309370.\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10729: Policy loss: 0.287475. Value loss: 0.279897. Entropy: 0.304359.\n",
      "Iteration 10730: Policy loss: 0.256536. Value loss: 0.074817. Entropy: 0.303451.\n",
      "Iteration 10731: Policy loss: 0.270284. Value loss: 0.034931. Entropy: 0.304958.\n",
      "episode: 3986   score: 515.0  epsilon: 1.0    steps: 928  evaluation reward: 338.0\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10732: Policy loss: 0.280940. Value loss: 0.141021. Entropy: 0.303054.\n",
      "Iteration 10733: Policy loss: 0.274649. Value loss: 0.041838. Entropy: 0.303082.\n",
      "Iteration 10734: Policy loss: 0.264473. Value loss: 0.028449. Entropy: 0.303340.\n",
      "episode: 3987   score: 440.0  epsilon: 1.0    steps: 328  evaluation reward: 337.85\n",
      "episode: 3988   score: 360.0  epsilon: 1.0    steps: 976  evaluation reward: 337.95\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10735: Policy loss: -0.187602. Value loss: 0.144681. Entropy: 0.289112.\n",
      "Iteration 10736: Policy loss: -0.199350. Value loss: 0.058795. Entropy: 0.288123.\n",
      "Iteration 10737: Policy loss: -0.207309. Value loss: 0.042901. Entropy: 0.288184.\n",
      "episode: 3989   score: 345.0  epsilon: 1.0    steps: 120  evaluation reward: 339.3\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10738: Policy loss: 0.109008. Value loss: 0.149009. Entropy: 0.296808.\n",
      "Iteration 10739: Policy loss: 0.106248. Value loss: 0.055410. Entropy: 0.293702.\n",
      "Iteration 10740: Policy loss: 0.096995. Value loss: 0.033826. Entropy: 0.294266.\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10741: Policy loss: 0.246632. Value loss: 0.150415. Entropy: 0.312047.\n",
      "Iteration 10742: Policy loss: 0.243154. Value loss: 0.061474. Entropy: 0.311054.\n",
      "Iteration 10743: Policy loss: 0.236163. Value loss: 0.045507. Entropy: 0.310853.\n",
      "episode: 3990   score: 245.0  epsilon: 1.0    steps: 560  evaluation reward: 339.9\n",
      "episode: 3991   score: 435.0  epsilon: 1.0    steps: 736  evaluation reward: 341.35\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10744: Policy loss: 0.063695. Value loss: 0.243780. Entropy: 0.296046.\n",
      "Iteration 10745: Policy loss: 0.056434. Value loss: 0.090244. Entropy: 0.295877.\n",
      "Iteration 10746: Policy loss: 0.038385. Value loss: 0.056953. Entropy: 0.295112.\n",
      "episode: 3992   score: 705.0  epsilon: 1.0    steps: 152  evaluation reward: 346.3\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10747: Policy loss: 0.099100. Value loss: 0.076303. Entropy: 0.307617.\n",
      "Iteration 10748: Policy loss: 0.104148. Value loss: 0.033731. Entropy: 0.306570.\n",
      "Iteration 10749: Policy loss: 0.091809. Value loss: 0.025705. Entropy: 0.307612.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3993   score: 210.0  epsilon: 1.0    steps: 472  evaluation reward: 344.45\n",
      "Training network. lr: 0.000168. clip: 0.067136\n",
      "Iteration 10750: Policy loss: -0.141580. Value loss: 0.068386. Entropy: 0.297937.\n",
      "Iteration 10751: Policy loss: -0.141960. Value loss: 0.032266. Entropy: 0.299345.\n",
      "Iteration 10752: Policy loss: -0.146926. Value loss: 0.024821. Entropy: 0.299596.\n",
      "episode: 3994   score: 330.0  epsilon: 1.0    steps: 1016  evaluation reward: 346.2\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10753: Policy loss: 0.138957. Value loss: 0.140088. Entropy: 0.309966.\n",
      "Iteration 10754: Policy loss: 0.126470. Value loss: 0.045554. Entropy: 0.309924.\n",
      "Iteration 10755: Policy loss: 0.119652. Value loss: 0.029498. Entropy: 0.309067.\n",
      "episode: 3995   score: 240.0  epsilon: 1.0    steps: 600  evaluation reward: 343.2\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10756: Policy loss: 0.022146. Value loss: 0.092795. Entropy: 0.291384.\n",
      "Iteration 10757: Policy loss: 0.014442. Value loss: 0.048899. Entropy: 0.288713.\n",
      "Iteration 10758: Policy loss: 0.013971. Value loss: 0.032496. Entropy: 0.288646.\n",
      "episode: 3996   score: 555.0  epsilon: 1.0    steps: 576  evaluation reward: 345.7\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10759: Policy loss: -0.136316. Value loss: 0.260243. Entropy: 0.294556.\n",
      "Iteration 10760: Policy loss: -0.150042. Value loss: 0.131397. Entropy: 0.294825.\n",
      "Iteration 10761: Policy loss: -0.151478. Value loss: 0.047313. Entropy: 0.295957.\n",
      "episode: 3997   score: 525.0  epsilon: 1.0    steps: 800  evaluation reward: 348.1\n",
      "episode: 3998   score: 220.0  epsilon: 1.0    steps: 816  evaluation reward: 348.5\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10762: Policy loss: -0.004789. Value loss: 0.119321. Entropy: 0.294064.\n",
      "Iteration 10763: Policy loss: -0.011970. Value loss: 0.045538. Entropy: 0.294020.\n",
      "Iteration 10764: Policy loss: -0.012983. Value loss: 0.031445. Entropy: 0.293863.\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10765: Policy loss: 0.204263. Value loss: 0.142722. Entropy: 0.305782.\n",
      "Iteration 10766: Policy loss: 0.196006. Value loss: 0.046602. Entropy: 0.305518.\n",
      "Iteration 10767: Policy loss: 0.194224. Value loss: 0.034531. Entropy: 0.305588.\n",
      "episode: 3999   score: 365.0  epsilon: 1.0    steps: 720  evaluation reward: 350.05\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10768: Policy loss: 0.184386. Value loss: 0.111145. Entropy: 0.297924.\n",
      "Iteration 10769: Policy loss: 0.185294. Value loss: 0.041288. Entropy: 0.296196.\n",
      "Iteration 10770: Policy loss: 0.171209. Value loss: 0.034395. Entropy: 0.296578.\n",
      "episode: 4000   score: 365.0  epsilon: 1.0    steps: 448  evaluation reward: 351.55\n",
      "now time :  2019-09-06 01:23:18.444519\n",
      "episode: 4001   score: 545.0  epsilon: 1.0    steps: 1000  evaluation reward: 355.65\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10771: Policy loss: 0.282942. Value loss: 0.067814. Entropy: 0.297125.\n",
      "Iteration 10772: Policy loss: 0.283501. Value loss: 0.025601. Entropy: 0.298094.\n",
      "Iteration 10773: Policy loss: 0.275812. Value loss: 0.021944. Entropy: 0.297350.\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10774: Policy loss: 0.024377. Value loss: 0.075957. Entropy: 0.303399.\n",
      "Iteration 10775: Policy loss: 0.009533. Value loss: 0.030476. Entropy: 0.302666.\n",
      "Iteration 10776: Policy loss: 0.007253. Value loss: 0.020284. Entropy: 0.303142.\n",
      "episode: 4002   score: 335.0  epsilon: 1.0    steps: 744  evaluation reward: 356.5\n",
      "episode: 4003   score: 230.0  epsilon: 1.0    steps: 912  evaluation reward: 356.7\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10777: Policy loss: -0.084586. Value loss: 0.106818. Entropy: 0.293899.\n",
      "Iteration 10778: Policy loss: -0.088424. Value loss: 0.056571. Entropy: 0.292400.\n",
      "Iteration 10779: Policy loss: -0.091304. Value loss: 0.044013. Entropy: 0.292856.\n",
      "episode: 4004   score: 390.0  epsilon: 1.0    steps: 504  evaluation reward: 358.5\n",
      "episode: 4005   score: 330.0  epsilon: 1.0    steps: 1024  evaluation reward: 356.8\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10780: Policy loss: 0.164741. Value loss: 0.108975. Entropy: 0.297163.\n",
      "Iteration 10781: Policy loss: 0.151844. Value loss: 0.056015. Entropy: 0.296276.\n",
      "Iteration 10782: Policy loss: 0.151311. Value loss: 0.042178. Entropy: 0.295821.\n",
      "episode: 4006   score: 210.0  epsilon: 1.0    steps: 816  evaluation reward: 356.8\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10783: Policy loss: 0.167884. Value loss: 0.098852. Entropy: 0.292975.\n",
      "Iteration 10784: Policy loss: 0.161288. Value loss: 0.044048. Entropy: 0.295341.\n",
      "Iteration 10785: Policy loss: 0.162916. Value loss: 0.025919. Entropy: 0.294378.\n",
      "episode: 4007   score: 420.0  epsilon: 1.0    steps: 608  evaluation reward: 355.8\n",
      "episode: 4008   score: 180.0  epsilon: 1.0    steps: 848  evaluation reward: 355.5\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10786: Policy loss: 0.184370. Value loss: 0.097864. Entropy: 0.294779.\n",
      "Iteration 10787: Policy loss: 0.172251. Value loss: 0.040682. Entropy: 0.293449.\n",
      "Iteration 10788: Policy loss: 0.170006. Value loss: 0.029586. Entropy: 0.293548.\n",
      "episode: 4009   score: 215.0  epsilon: 1.0    steps: 808  evaluation reward: 355.5\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10789: Policy loss: -0.302975. Value loss: 0.358875. Entropy: 0.306655.\n",
      "Iteration 10790: Policy loss: -0.311133. Value loss: 0.184688. Entropy: 0.306958.\n",
      "Iteration 10791: Policy loss: -0.329111. Value loss: 0.118668. Entropy: 0.307249.\n",
      "episode: 4010   score: 175.0  epsilon: 1.0    steps: 456  evaluation reward: 355.05\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10792: Policy loss: 0.101567. Value loss: 0.047397. Entropy: 0.302941.\n",
      "Iteration 10793: Policy loss: 0.102402. Value loss: 0.022286. Entropy: 0.304020.\n",
      "Iteration 10794: Policy loss: 0.097653. Value loss: 0.015872. Entropy: 0.302252.\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10795: Policy loss: -0.051035. Value loss: 0.117813. Entropy: 0.312678.\n",
      "Iteration 10796: Policy loss: -0.059317. Value loss: 0.052545. Entropy: 0.313127.\n",
      "Iteration 10797: Policy loss: -0.064383. Value loss: 0.039967. Entropy: 0.312617.\n",
      "episode: 4011   score: 210.0  epsilon: 1.0    steps: 216  evaluation reward: 356.05\n",
      "Training network. lr: 0.000167. clip: 0.066979\n",
      "Iteration 10798: Policy loss: 0.021039. Value loss: 0.073314. Entropy: 0.298706.\n",
      "Iteration 10799: Policy loss: 0.022937. Value loss: 0.021954. Entropy: 0.298934.\n",
      "Iteration 10800: Policy loss: 0.018301. Value loss: 0.020096. Entropy: 0.298747.\n",
      "episode: 4012   score: 215.0  epsilon: 1.0    steps: 728  evaluation reward: 355.9\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10801: Policy loss: -0.479344. Value loss: 0.336384. Entropy: 0.296339.\n",
      "Iteration 10802: Policy loss: -0.490398. Value loss: 0.241526. Entropy: 0.298606.\n",
      "Iteration 10803: Policy loss: -0.487841. Value loss: 0.190558. Entropy: 0.297221.\n",
      "episode: 4013   score: 605.0  epsilon: 1.0    steps: 976  evaluation reward: 359.2\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10804: Policy loss: -0.058733. Value loss: 0.174456. Entropy: 0.311781.\n",
      "Iteration 10805: Policy loss: -0.069592. Value loss: 0.072158. Entropy: 0.310829.\n",
      "Iteration 10806: Policy loss: -0.088483. Value loss: 0.044029. Entropy: 0.312200.\n",
      "episode: 4014   score: 425.0  epsilon: 1.0    steps: 440  evaluation reward: 359.75\n",
      "episode: 4015   score: 495.0  epsilon: 1.0    steps: 896  evaluation reward: 361.4\n",
      "episode: 4016   score: 225.0  epsilon: 1.0    steps: 1000  evaluation reward: 361.45\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10807: Policy loss: -0.456068. Value loss: 0.552831. Entropy: 0.284928.\n",
      "Iteration 10808: Policy loss: -0.481307. Value loss: 0.417200. Entropy: 0.283868.\n",
      "Iteration 10809: Policy loss: -0.496785. Value loss: 0.374939. Entropy: 0.284962.\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10810: Policy loss: 0.025871. Value loss: 0.128289. Entropy: 0.291755.\n",
      "Iteration 10811: Policy loss: 0.015035. Value loss: 0.055668. Entropy: 0.291228.\n",
      "Iteration 10812: Policy loss: 0.016154. Value loss: 0.038396. Entropy: 0.291119.\n",
      "episode: 4017   score: 570.0  epsilon: 1.0    steps: 192  evaluation reward: 364.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10813: Policy loss: 0.081218. Value loss: 0.063215. Entropy: 0.293834.\n",
      "Iteration 10814: Policy loss: 0.074684. Value loss: 0.029776. Entropy: 0.293107.\n",
      "Iteration 10815: Policy loss: 0.075062. Value loss: 0.022876. Entropy: 0.293396.\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10816: Policy loss: -0.376173. Value loss: 0.319973. Entropy: 0.308601.\n",
      "Iteration 10817: Policy loss: -0.380982. Value loss: 0.091997. Entropy: 0.308269.\n",
      "Iteration 10818: Policy loss: -0.414899. Value loss: 0.051762. Entropy: 0.309259.\n",
      "episode: 4018   score: 625.0  epsilon: 1.0    steps: 176  evaluation reward: 368.45\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10819: Policy loss: -0.086639. Value loss: 0.173252. Entropy: 0.297121.\n",
      "Iteration 10820: Policy loss: -0.093630. Value loss: 0.060043. Entropy: 0.298440.\n",
      "Iteration 10821: Policy loss: -0.094375. Value loss: 0.038077. Entropy: 0.299726.\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10822: Policy loss: 0.195394. Value loss: 0.197823. Entropy: 0.307907.\n",
      "Iteration 10823: Policy loss: 0.180287. Value loss: 0.052081. Entropy: 0.306229.\n",
      "Iteration 10824: Policy loss: 0.162557. Value loss: 0.033197. Entropy: 0.305414.\n",
      "episode: 4019   score: 340.0  epsilon: 1.0    steps: 824  evaluation reward: 367.95\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10825: Policy loss: 0.249143. Value loss: 0.211119. Entropy: 0.301807.\n",
      "Iteration 10826: Policy loss: 0.239265. Value loss: 0.097798. Entropy: 0.302216.\n",
      "Iteration 10827: Policy loss: 0.237232. Value loss: 0.061277. Entropy: 0.300898.\n",
      "episode: 4020   score: 450.0  epsilon: 1.0    steps: 72  evaluation reward: 367.3\n",
      "episode: 4021   score: 465.0  epsilon: 1.0    steps: 264  evaluation reward: 370.15\n",
      "episode: 4022   score: 515.0  epsilon: 1.0    steps: 768  evaluation reward: 371.85\n",
      "episode: 4023   score: 335.0  epsilon: 1.0    steps: 912  evaluation reward: 371.75\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10828: Policy loss: 0.063359. Value loss: 0.126819. Entropy: 0.268528.\n",
      "Iteration 10829: Policy loss: 0.056639. Value loss: 0.064068. Entropy: 0.266995.\n",
      "Iteration 10830: Policy loss: 0.055874. Value loss: 0.045203. Entropy: 0.267190.\n",
      "episode: 4024   score: 215.0  epsilon: 1.0    steps: 312  evaluation reward: 367.4\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10831: Policy loss: 0.046916. Value loss: 0.109625. Entropy: 0.298425.\n",
      "Iteration 10832: Policy loss: 0.038479. Value loss: 0.046749. Entropy: 0.298459.\n",
      "Iteration 10833: Policy loss: 0.040647. Value loss: 0.035245. Entropy: 0.298357.\n",
      "episode: 4025   score: 295.0  epsilon: 1.0    steps: 80  evaluation reward: 368.15\n",
      "episode: 4026   score: 240.0  epsilon: 1.0    steps: 856  evaluation reward: 368.45\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10834: Policy loss: 0.065973. Value loss: 0.114032. Entropy: 0.292074.\n",
      "Iteration 10835: Policy loss: 0.069294. Value loss: 0.071331. Entropy: 0.292933.\n",
      "Iteration 10836: Policy loss: 0.073159. Value loss: 0.050629. Entropy: 0.293819.\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10837: Policy loss: 0.290233. Value loss: 0.173745. Entropy: 0.308162.\n",
      "Iteration 10838: Policy loss: 0.281950. Value loss: 0.055299. Entropy: 0.307691.\n",
      "Iteration 10839: Policy loss: 0.278287. Value loss: 0.035202. Entropy: 0.307812.\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10840: Policy loss: 0.062355. Value loss: 0.053816. Entropy: 0.307233.\n",
      "Iteration 10841: Policy loss: 0.061165. Value loss: 0.028392. Entropy: 0.306820.\n",
      "Iteration 10842: Policy loss: 0.055804. Value loss: 0.022447. Entropy: 0.307458.\n",
      "episode: 4027   score: 240.0  epsilon: 1.0    steps: 352  evaluation reward: 366.5\n",
      "episode: 4028   score: 260.0  epsilon: 1.0    steps: 432  evaluation reward: 365.9\n",
      "episode: 4029   score: 240.0  epsilon: 1.0    steps: 872  evaluation reward: 365.45\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10843: Policy loss: 0.046115. Value loss: 0.107066. Entropy: 0.277877.\n",
      "Iteration 10844: Policy loss: 0.044002. Value loss: 0.051913. Entropy: 0.276386.\n",
      "Iteration 10845: Policy loss: 0.044533. Value loss: 0.037673. Entropy: 0.276403.\n",
      "episode: 4030   score: 250.0  epsilon: 1.0    steps: 440  evaluation reward: 364.0\n",
      "episode: 4031   score: 240.0  epsilon: 1.0    steps: 488  evaluation reward: 364.7\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10846: Policy loss: 0.319949. Value loss: 0.098805. Entropy: 0.281630.\n",
      "Iteration 10847: Policy loss: 0.319250. Value loss: 0.044165. Entropy: 0.280617.\n",
      "Iteration 10848: Policy loss: 0.310235. Value loss: 0.035743. Entropy: 0.281861.\n",
      "Training network. lr: 0.000167. clip: 0.066832\n",
      "Iteration 10849: Policy loss: -0.024545. Value loss: 0.148969. Entropy: 0.310604.\n",
      "Iteration 10850: Policy loss: -0.023344. Value loss: 0.060599. Entropy: 0.310465.\n",
      "Iteration 10851: Policy loss: -0.028739. Value loss: 0.038670. Entropy: 0.309879.\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10852: Policy loss: -0.071128. Value loss: 0.096323. Entropy: 0.312187.\n",
      "Iteration 10853: Policy loss: -0.080188. Value loss: 0.042903. Entropy: 0.312416.\n",
      "Iteration 10854: Policy loss: -0.090292. Value loss: 0.029493. Entropy: 0.312284.\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10855: Policy loss: -0.146832. Value loss: 0.389657. Entropy: 0.308729.\n",
      "Iteration 10856: Policy loss: -0.186647. Value loss: 0.279442. Entropy: 0.308202.\n",
      "Iteration 10857: Policy loss: -0.168022. Value loss: 0.209676. Entropy: 0.307524.\n",
      "episode: 4032   score: 575.0  epsilon: 1.0    steps: 288  evaluation reward: 367.0\n",
      "episode: 4033   score: 365.0  epsilon: 1.0    steps: 312  evaluation reward: 368.55\n",
      "episode: 4034   score: 245.0  epsilon: 1.0    steps: 912  evaluation reward: 367.85\n",
      "episode: 4035   score: 355.0  epsilon: 1.0    steps: 960  evaluation reward: 369.8\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10858: Policy loss: -0.267820. Value loss: 0.325847. Entropy: 0.277383.\n",
      "Iteration 10859: Policy loss: -0.291936. Value loss: 0.237231. Entropy: 0.274121.\n",
      "Iteration 10860: Policy loss: -0.280689. Value loss: 0.190881. Entropy: 0.273583.\n",
      "episode: 4036   score: 425.0  epsilon: 1.0    steps: 456  evaluation reward: 372.2\n",
      "episode: 4037   score: 225.0  epsilon: 1.0    steps: 480  evaluation reward: 367.2\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10861: Policy loss: -0.031645. Value loss: 0.068886. Entropy: 0.268528.\n",
      "Iteration 10862: Policy loss: -0.031212. Value loss: 0.035820. Entropy: 0.268095.\n",
      "Iteration 10863: Policy loss: -0.035453. Value loss: 0.026053. Entropy: 0.267731.\n",
      "episode: 4038   score: 215.0  epsilon: 1.0    steps: 272  evaluation reward: 363.5\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10864: Policy loss: 0.003396. Value loss: 0.100682. Entropy: 0.295300.\n",
      "Iteration 10865: Policy loss: -0.004293. Value loss: 0.052575. Entropy: 0.295233.\n",
      "Iteration 10866: Policy loss: -0.004699. Value loss: 0.033042. Entropy: 0.295447.\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10867: Policy loss: 0.134389. Value loss: 0.087207. Entropy: 0.310478.\n",
      "Iteration 10868: Policy loss: 0.127231. Value loss: 0.034779. Entropy: 0.310056.\n",
      "Iteration 10869: Policy loss: 0.122193. Value loss: 0.026643. Entropy: 0.310541.\n",
      "episode: 4039   score: 315.0  epsilon: 1.0    steps: 368  evaluation reward: 364.85\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10870: Policy loss: -0.199611. Value loss: 0.233510. Entropy: 0.294947.\n",
      "Iteration 10871: Policy loss: -0.205772. Value loss: 0.112698. Entropy: 0.295091.\n",
      "Iteration 10872: Policy loss: -0.209293. Value loss: 0.049673. Entropy: 0.294996.\n",
      "episode: 4040   score: 210.0  epsilon: 1.0    steps: 552  evaluation reward: 363.6\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10873: Policy loss: -0.097786. Value loss: 0.148721. Entropy: 0.299238.\n",
      "Iteration 10874: Policy loss: -0.124441. Value loss: 0.129684. Entropy: 0.300360.\n",
      "Iteration 10875: Policy loss: -0.124519. Value loss: 0.105094. Entropy: 0.301639.\n",
      "episode: 4041   score: 210.0  epsilon: 1.0    steps: 496  evaluation reward: 363.3\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10876: Policy loss: 0.086461. Value loss: 0.082251. Entropy: 0.295360.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10877: Policy loss: 0.076102. Value loss: 0.040596. Entropy: 0.295907.\n",
      "Iteration 10878: Policy loss: 0.075029. Value loss: 0.031579. Entropy: 0.296098.\n",
      "episode: 4042   score: 320.0  epsilon: 1.0    steps: 1000  evaluation reward: 364.4\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10879: Policy loss: -0.140406. Value loss: 0.278586. Entropy: 0.314052.\n",
      "Iteration 10880: Policy loss: -0.147312. Value loss: 0.099209. Entropy: 0.313556.\n",
      "Iteration 10881: Policy loss: -0.144948. Value loss: 0.063809. Entropy: 0.313688.\n",
      "episode: 4043   score: 350.0  epsilon: 1.0    steps: 56  evaluation reward: 365.8\n",
      "episode: 4044   score: 560.0  epsilon: 1.0    steps: 352  evaluation reward: 365.5\n",
      "episode: 4045   score: 275.0  epsilon: 1.0    steps: 520  evaluation reward: 365.45\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10882: Policy loss: 0.186221. Value loss: 0.222371. Entropy: 0.261448.\n",
      "Iteration 10883: Policy loss: 0.183092. Value loss: 0.050339. Entropy: 0.263052.\n",
      "Iteration 10884: Policy loss: 0.173089. Value loss: 0.032476. Entropy: 0.260183.\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10885: Policy loss: 0.142414. Value loss: 0.080471. Entropy: 0.308087.\n",
      "Iteration 10886: Policy loss: 0.126179. Value loss: 0.028328. Entropy: 0.308350.\n",
      "Iteration 10887: Policy loss: 0.122632. Value loss: 0.018893. Entropy: 0.308974.\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10888: Policy loss: 0.177538. Value loss: 0.190391. Entropy: 0.309888.\n",
      "Iteration 10889: Policy loss: 0.161461. Value loss: 0.071740. Entropy: 0.308563.\n",
      "Iteration 10890: Policy loss: 0.181393. Value loss: 0.041108. Entropy: 0.309161.\n",
      "episode: 4046   score: 560.0  epsilon: 1.0    steps: 992  evaluation reward: 367.25\n",
      "episode: 4047   score: 215.0  epsilon: 1.0    steps: 1024  evaluation reward: 367.3\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10891: Policy loss: -0.034323. Value loss: 0.163184. Entropy: 0.308037.\n",
      "Iteration 10892: Policy loss: -0.053813. Value loss: 0.044100. Entropy: 0.308565.\n",
      "Iteration 10893: Policy loss: -0.055186. Value loss: 0.031911. Entropy: 0.308542.\n",
      "episode: 4048   score: 595.0  epsilon: 1.0    steps: 40  evaluation reward: 370.0\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10894: Policy loss: 0.180066. Value loss: 0.111391. Entropy: 0.283099.\n",
      "Iteration 10895: Policy loss: 0.181253. Value loss: 0.044759. Entropy: 0.279974.\n",
      "Iteration 10896: Policy loss: 0.175313. Value loss: 0.029586. Entropy: 0.279478.\n",
      "episode: 4049   score: 225.0  epsilon: 1.0    steps: 424  evaluation reward: 366.6\n",
      "episode: 4050   score: 440.0  epsilon: 1.0    steps: 984  evaluation reward: 366.5\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10897: Policy loss: -0.075371. Value loss: 0.091109. Entropy: 0.300081.\n",
      "Iteration 10898: Policy loss: -0.081616. Value loss: 0.045074. Entropy: 0.299658.\n",
      "Iteration 10899: Policy loss: -0.084870. Value loss: 0.033478. Entropy: 0.299540.\n",
      "Training network. lr: 0.000167. clip: 0.066675\n",
      "Iteration 10900: Policy loss: 0.055535. Value loss: 0.129339. Entropy: 0.302057.\n",
      "Iteration 10901: Policy loss: 0.034622. Value loss: 0.047285. Entropy: 0.301924.\n",
      "Iteration 10902: Policy loss: 0.038346. Value loss: 0.029879. Entropy: 0.301669.\n",
      "now time :  2019-09-06 01:31:28.006193\n",
      "episode: 4051   score: 285.0  epsilon: 1.0    steps: 104  evaluation reward: 367.25\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10903: Policy loss: -0.291141. Value loss: 0.260184. Entropy: 0.299028.\n",
      "Iteration 10904: Policy loss: -0.311624. Value loss: 0.078660. Entropy: 0.299494.\n",
      "Iteration 10905: Policy loss: -0.310101. Value loss: 0.043600. Entropy: 0.300412.\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10906: Policy loss: 0.248877. Value loss: 0.143586. Entropy: 0.304612.\n",
      "Iteration 10907: Policy loss: 0.245807. Value loss: 0.046865. Entropy: 0.304050.\n",
      "Iteration 10908: Policy loss: 0.237905. Value loss: 0.030364. Entropy: 0.304214.\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10909: Policy loss: 0.063943. Value loss: 0.135192. Entropy: 0.311333.\n",
      "Iteration 10910: Policy loss: 0.058725. Value loss: 0.053606. Entropy: 0.311119.\n",
      "Iteration 10911: Policy loss: 0.052459. Value loss: 0.037913. Entropy: 0.311157.\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10912: Policy loss: 0.111434. Value loss: 0.072205. Entropy: 0.309396.\n",
      "Iteration 10913: Policy loss: 0.103694. Value loss: 0.026358. Entropy: 0.309027.\n",
      "Iteration 10914: Policy loss: 0.103708. Value loss: 0.019586. Entropy: 0.308364.\n",
      "episode: 4052   score: 390.0  epsilon: 1.0    steps: 280  evaluation reward: 367.5\n",
      "episode: 4053   score: 275.0  epsilon: 1.0    steps: 288  evaluation reward: 364.6\n",
      "episode: 4054   score: 325.0  epsilon: 1.0    steps: 352  evaluation reward: 364.5\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10915: Policy loss: 0.007916. Value loss: 0.073733. Entropy: 0.283714.\n",
      "Iteration 10916: Policy loss: 0.002277. Value loss: 0.037599. Entropy: 0.285976.\n",
      "Iteration 10917: Policy loss: -0.002660. Value loss: 0.027485. Entropy: 0.285811.\n",
      "episode: 4055   score: 280.0  epsilon: 1.0    steps: 976  evaluation reward: 363.7\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10918: Policy loss: 0.130411. Value loss: 0.084912. Entropy: 0.307442.\n",
      "Iteration 10919: Policy loss: 0.130426. Value loss: 0.030827. Entropy: 0.307349.\n",
      "Iteration 10920: Policy loss: 0.127609. Value loss: 0.025103. Entropy: 0.306348.\n",
      "episode: 4056   score: 495.0  epsilon: 1.0    steps: 56  evaluation reward: 364.35\n",
      "episode: 4057   score: 350.0  epsilon: 1.0    steps: 208  evaluation reward: 363.45\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10921: Policy loss: 0.032193. Value loss: 0.069084. Entropy: 0.292944.\n",
      "Iteration 10922: Policy loss: 0.028837. Value loss: 0.033219. Entropy: 0.292880.\n",
      "Iteration 10923: Policy loss: 0.023431. Value loss: 0.024183. Entropy: 0.291433.\n",
      "episode: 4058   score: 240.0  epsilon: 1.0    steps: 576  evaluation reward: 361.4\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10924: Policy loss: 0.304206. Value loss: 0.131224. Entropy: 0.298072.\n",
      "Iteration 10925: Policy loss: 0.297716. Value loss: 0.045968. Entropy: 0.298028.\n",
      "Iteration 10926: Policy loss: 0.302609. Value loss: 0.033651. Entropy: 0.297957.\n",
      "episode: 4059   score: 670.0  epsilon: 1.0    steps: 232  evaluation reward: 361.9\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10927: Policy loss: -0.003343. Value loss: 0.078056. Entropy: 0.302137.\n",
      "Iteration 10928: Policy loss: -0.011487. Value loss: 0.032571. Entropy: 0.302365.\n",
      "Iteration 10929: Policy loss: -0.015344. Value loss: 0.025258. Entropy: 0.302294.\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10930: Policy loss: 0.112034. Value loss: 0.137326. Entropy: 0.310907.\n",
      "Iteration 10931: Policy loss: 0.107004. Value loss: 0.056433. Entropy: 0.310931.\n",
      "Iteration 10932: Policy loss: 0.106303. Value loss: 0.035770. Entropy: 0.309287.\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10933: Policy loss: 0.014374. Value loss: 0.098175. Entropy: 0.307632.\n",
      "Iteration 10934: Policy loss: 0.005855. Value loss: 0.041542. Entropy: 0.308422.\n",
      "Iteration 10935: Policy loss: 0.006796. Value loss: 0.032043. Entropy: 0.307805.\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10936: Policy loss: -0.195360. Value loss: 0.386322. Entropy: 0.308226.\n",
      "Iteration 10937: Policy loss: -0.189539. Value loss: 0.252573. Entropy: 0.307985.\n",
      "Iteration 10938: Policy loss: -0.198043. Value loss: 0.189715. Entropy: 0.308228.\n",
      "episode: 4060   score: 320.0  epsilon: 1.0    steps: 208  evaluation reward: 360.45\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10939: Policy loss: -0.007219. Value loss: 0.097692. Entropy: 0.297844.\n",
      "Iteration 10940: Policy loss: -0.014428. Value loss: 0.039258. Entropy: 0.297991.\n",
      "Iteration 10941: Policy loss: -0.019336. Value loss: 0.025471. Entropy: 0.297779.\n",
      "episode: 4061   score: 290.0  epsilon: 1.0    steps: 976  evaluation reward: 359.8\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10942: Policy loss: 0.161445. Value loss: 0.144360. Entropy: 0.303727.\n",
      "Iteration 10943: Policy loss: 0.148349. Value loss: 0.058907. Entropy: 0.303777.\n",
      "Iteration 10944: Policy loss: 0.145889. Value loss: 0.038748. Entropy: 0.303229.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4062   score: 335.0  epsilon: 1.0    steps: 168  evaluation reward: 357.6\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10945: Policy loss: -0.367295. Value loss: 0.322734. Entropy: 0.289506.\n",
      "Iteration 10946: Policy loss: -0.382779. Value loss: 0.222956. Entropy: 0.289260.\n",
      "Iteration 10947: Policy loss: -0.372050. Value loss: 0.190376. Entropy: 0.287811.\n",
      "episode: 4063   score: 630.0  epsilon: 1.0    steps: 592  evaluation reward: 362.65\n",
      "episode: 4064   score: 365.0  epsilon: 1.0    steps: 736  evaluation reward: 361.85\n",
      "episode: 4065   score: 670.0  epsilon: 1.0    steps: 792  evaluation reward: 365.4\n",
      "episode: 4066   score: 420.0  epsilon: 1.0    steps: 968  evaluation reward: 363.65\n",
      "Training network. lr: 0.000166. clip: 0.066518\n",
      "Iteration 10948: Policy loss: 0.076097. Value loss: 0.138904. Entropy: 0.274862.\n",
      "Iteration 10949: Policy loss: 0.074869. Value loss: 0.054118. Entropy: 0.274379.\n",
      "Iteration 10950: Policy loss: 0.078018. Value loss: 0.039984. Entropy: 0.272668.\n",
      "episode: 4067   score: 390.0  epsilon: 1.0    steps: 456  evaluation reward: 359.8\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10951: Policy loss: -0.040312. Value loss: 0.113882. Entropy: 0.283032.\n",
      "Iteration 10952: Policy loss: -0.050329. Value loss: 0.062344. Entropy: 0.283185.\n",
      "Iteration 10953: Policy loss: -0.049372. Value loss: 0.047058. Entropy: 0.282579.\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10954: Policy loss: -0.036886. Value loss: 0.070406. Entropy: 0.311854.\n",
      "Iteration 10955: Policy loss: -0.043845. Value loss: 0.030461. Entropy: 0.311371.\n",
      "Iteration 10956: Policy loss: -0.043184. Value loss: 0.024865. Entropy: 0.310574.\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10957: Policy loss: -0.073994. Value loss: 0.046383. Entropy: 0.307447.\n",
      "Iteration 10958: Policy loss: -0.080262. Value loss: 0.021108. Entropy: 0.306775.\n",
      "Iteration 10959: Policy loss: -0.084315. Value loss: 0.018027. Entropy: 0.305800.\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10960: Policy loss: 0.186776. Value loss: 0.078383. Entropy: 0.304487.\n",
      "Iteration 10961: Policy loss: 0.185195. Value loss: 0.021968. Entropy: 0.303306.\n",
      "Iteration 10962: Policy loss: 0.179605. Value loss: 0.013435. Entropy: 0.303746.\n",
      "episode: 4068   score: 235.0  epsilon: 1.0    steps: 192  evaluation reward: 359.1\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10963: Policy loss: 0.042448. Value loss: 0.076987. Entropy: 0.299406.\n",
      "Iteration 10964: Policy loss: 0.038570. Value loss: 0.036613. Entropy: 0.298056.\n",
      "Iteration 10965: Policy loss: 0.036785. Value loss: 0.027028. Entropy: 0.297339.\n",
      "episode: 4069   score: 210.0  epsilon: 1.0    steps: 608  evaluation reward: 355.3\n",
      "episode: 4070   score: 540.0  epsilon: 1.0    steps: 968  evaluation reward: 358.05\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10966: Policy loss: 0.134854. Value loss: 0.150470. Entropy: 0.292605.\n",
      "Iteration 10967: Policy loss: 0.130198. Value loss: 0.053237. Entropy: 0.291172.\n",
      "Iteration 10968: Policy loss: 0.116855. Value loss: 0.035045. Entropy: 0.292605.\n",
      "episode: 4071   score: 270.0  epsilon: 1.0    steps: 600  evaluation reward: 356.65\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10969: Policy loss: 0.139866. Value loss: 0.104585. Entropy: 0.288364.\n",
      "Iteration 10970: Policy loss: 0.129196. Value loss: 0.040443. Entropy: 0.286720.\n",
      "Iteration 10971: Policy loss: 0.129205. Value loss: 0.028550. Entropy: 0.284771.\n",
      "episode: 4072   score: 390.0  epsilon: 1.0    steps: 224  evaluation reward: 357.25\n",
      "episode: 4073   score: 260.0  epsilon: 1.0    steps: 496  evaluation reward: 357.75\n",
      "episode: 4074   score: 345.0  epsilon: 1.0    steps: 792  evaluation reward: 355.5\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10972: Policy loss: 0.054158. Value loss: 0.082762. Entropy: 0.274544.\n",
      "Iteration 10973: Policy loss: 0.053416. Value loss: 0.043296. Entropy: 0.275204.\n",
      "Iteration 10974: Policy loss: 0.049417. Value loss: 0.034865. Entropy: 0.277574.\n",
      "episode: 4075   score: 405.0  epsilon: 1.0    steps: 216  evaluation reward: 356.85\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10975: Policy loss: -0.070047. Value loss: 0.054958. Entropy: 0.301470.\n",
      "Iteration 10976: Policy loss: -0.077804. Value loss: 0.032676. Entropy: 0.301861.\n",
      "Iteration 10977: Policy loss: -0.080459. Value loss: 0.028314. Entropy: 0.302705.\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10978: Policy loss: -0.118365. Value loss: 0.062954. Entropy: 0.311056.\n",
      "Iteration 10979: Policy loss: -0.118409. Value loss: 0.023086. Entropy: 0.310966.\n",
      "Iteration 10980: Policy loss: -0.122987. Value loss: 0.015894. Entropy: 0.310963.\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10981: Policy loss: -0.027403. Value loss: 0.034540. Entropy: 0.308215.\n",
      "Iteration 10982: Policy loss: -0.036752. Value loss: 0.014037. Entropy: 0.308118.\n",
      "Iteration 10983: Policy loss: -0.036922. Value loss: 0.009828. Entropy: 0.309040.\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10984: Policy loss: 0.017681. Value loss: 0.091800. Entropy: 0.309955.\n",
      "Iteration 10985: Policy loss: 0.010877. Value loss: 0.038492. Entropy: 0.310062.\n",
      "Iteration 10986: Policy loss: 0.005008. Value loss: 0.025219. Entropy: 0.309781.\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10987: Policy loss: -0.262385. Value loss: 0.364286. Entropy: 0.310138.\n",
      "Iteration 10988: Policy loss: -0.257514. Value loss: 0.196803. Entropy: 0.308797.\n",
      "Iteration 10989: Policy loss: -0.271881. Value loss: 0.146776. Entropy: 0.309022.\n",
      "episode: 4076   score: 460.0  epsilon: 1.0    steps: 504  evaluation reward: 357.9\n",
      "episode: 4077   score: 315.0  epsilon: 1.0    steps: 920  evaluation reward: 358.9\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10990: Policy loss: 0.112954. Value loss: 0.101981. Entropy: 0.292625.\n",
      "Iteration 10991: Policy loss: 0.115920. Value loss: 0.046069. Entropy: 0.290737.\n",
      "Iteration 10992: Policy loss: 0.110742. Value loss: 0.031854. Entropy: 0.291230.\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10993: Policy loss: 0.204662. Value loss: 0.113363. Entropy: 0.306128.\n",
      "Iteration 10994: Policy loss: 0.202909. Value loss: 0.044119. Entropy: 0.305999.\n",
      "Iteration 10995: Policy loss: 0.200852. Value loss: 0.031007. Entropy: 0.305328.\n",
      "episode: 4078   score: 395.0  epsilon: 1.0    steps: 176  evaluation reward: 360.75\n",
      "episode: 4079   score: 295.0  epsilon: 1.0    steps: 624  evaluation reward: 360.25\n",
      "episode: 4080   score: 345.0  epsilon: 1.0    steps: 912  evaluation reward: 361.5\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10996: Policy loss: 0.094551. Value loss: 0.098942. Entropy: 0.279059.\n",
      "Iteration 10997: Policy loss: 0.090438. Value loss: 0.049334. Entropy: 0.279019.\n",
      "Iteration 10998: Policy loss: 0.085109. Value loss: 0.037022. Entropy: 0.278244.\n",
      "episode: 4081   score: 495.0  epsilon: 1.0    steps: 88  evaluation reward: 359.85\n",
      "Training network. lr: 0.000166. clip: 0.066371\n",
      "Iteration 10999: Policy loss: 0.076036. Value loss: 0.079176. Entropy: 0.289748.\n",
      "Iteration 11000: Policy loss: 0.073869. Value loss: 0.043700. Entropy: 0.289271.\n",
      "Iteration 11001: Policy loss: 0.068995. Value loss: 0.031908. Entropy: 0.289104.\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11002: Policy loss: -0.447346. Value loss: 0.290477. Entropy: 0.306625.\n",
      "Iteration 11003: Policy loss: -0.401874. Value loss: 0.114172. Entropy: 0.308164.\n",
      "Iteration 11004: Policy loss: -0.448923. Value loss: 0.074868. Entropy: 0.306638.\n",
      "episode: 4082   score: 470.0  epsilon: 1.0    steps: 288  evaluation reward: 358.6\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11005: Policy loss: -0.125971. Value loss: 0.073273. Entropy: 0.295518.\n",
      "Iteration 11006: Policy loss: -0.124523. Value loss: 0.032212. Entropy: 0.294704.\n",
      "Iteration 11007: Policy loss: -0.131176. Value loss: 0.022572. Entropy: 0.295218.\n",
      "episode: 4083   score: 635.0  epsilon: 1.0    steps: 760  evaluation reward: 362.85\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11008: Policy loss: -0.173129. Value loss: 0.362377. Entropy: 0.295290.\n",
      "Iteration 11009: Policy loss: -0.208656. Value loss: 0.285510. Entropy: 0.297308.\n",
      "Iteration 11010: Policy loss: -0.216888. Value loss: 0.266984. Entropy: 0.296822.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11011: Policy loss: 0.204866. Value loss: 0.099628. Entropy: 0.312724.\n",
      "Iteration 11012: Policy loss: 0.196365. Value loss: 0.035575. Entropy: 0.312059.\n",
      "Iteration 11013: Policy loss: 0.187064. Value loss: 0.023294. Entropy: 0.311207.\n",
      "episode: 4084   score: 375.0  epsilon: 1.0    steps: 48  evaluation reward: 364.75\n",
      "episode: 4085   score: 210.0  epsilon: 1.0    steps: 560  evaluation reward: 364.0\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11014: Policy loss: 0.154654. Value loss: 0.175939. Entropy: 0.284144.\n",
      "Iteration 11015: Policy loss: 0.136435. Value loss: 0.082650. Entropy: 0.285053.\n",
      "Iteration 11016: Policy loss: 0.138444. Value loss: 0.063599. Entropy: 0.284460.\n",
      "episode: 4086   score: 470.0  epsilon: 1.0    steps: 248  evaluation reward: 363.55\n",
      "episode: 4087   score: 315.0  epsilon: 1.0    steps: 880  evaluation reward: 362.3\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11017: Policy loss: 0.327475. Value loss: 0.122530. Entropy: 0.289493.\n",
      "Iteration 11018: Policy loss: 0.324471. Value loss: 0.054745. Entropy: 0.288064.\n",
      "Iteration 11019: Policy loss: 0.311739. Value loss: 0.045152. Entropy: 0.288563.\n",
      "episode: 4088   score: 535.0  epsilon: 1.0    steps: 328  evaluation reward: 364.05\n",
      "episode: 4089   score: 285.0  epsilon: 1.0    steps: 600  evaluation reward: 363.45\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11020: Policy loss: 0.002083. Value loss: 0.122567. Entropy: 0.278727.\n",
      "Iteration 11021: Policy loss: -0.007122. Value loss: 0.053367. Entropy: 0.279351.\n",
      "Iteration 11022: Policy loss: -0.008788. Value loss: 0.037731. Entropy: 0.278180.\n",
      "episode: 4090   score: 60.0  epsilon: 1.0    steps: 464  evaluation reward: 361.6\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11023: Policy loss: -0.209538. Value loss: 0.126516. Entropy: 0.297917.\n",
      "Iteration 11024: Policy loss: -0.208530. Value loss: 0.061956. Entropy: 0.298614.\n",
      "Iteration 11025: Policy loss: -0.217387. Value loss: 0.045998. Entropy: 0.299035.\n",
      "episode: 4091   score: 135.0  epsilon: 1.0    steps: 264  evaluation reward: 358.6\n",
      "episode: 4092   score: 210.0  epsilon: 1.0    steps: 296  evaluation reward: 353.65\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11026: Policy loss: 0.089576. Value loss: 0.059787. Entropy: 0.282532.\n",
      "Iteration 11027: Policy loss: 0.087746. Value loss: 0.034470. Entropy: 0.283524.\n",
      "Iteration 11028: Policy loss: 0.089473. Value loss: 0.029938. Entropy: 0.284204.\n",
      "episode: 4093   score: 495.0  epsilon: 1.0    steps: 624  evaluation reward: 356.5\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11029: Policy loss: -0.134838. Value loss: 0.069069. Entropy: 0.300591.\n",
      "Iteration 11030: Policy loss: -0.141839. Value loss: 0.027395. Entropy: 0.299667.\n",
      "Iteration 11031: Policy loss: -0.139152. Value loss: 0.017563. Entropy: 0.301174.\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11032: Policy loss: 0.033907. Value loss: 0.079184. Entropy: 0.308394.\n",
      "Iteration 11033: Policy loss: 0.025317. Value loss: 0.031999. Entropy: 0.306843.\n",
      "Iteration 11034: Policy loss: 0.021699. Value loss: 0.021027. Entropy: 0.306527.\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11035: Policy loss: 0.007972. Value loss: 0.114085. Entropy: 0.312901.\n",
      "Iteration 11036: Policy loss: 0.010708. Value loss: 0.047822. Entropy: 0.312537.\n",
      "Iteration 11037: Policy loss: 0.004873. Value loss: 0.034215. Entropy: 0.312293.\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11038: Policy loss: -0.008578. Value loss: 0.128612. Entropy: 0.306238.\n",
      "Iteration 11039: Policy loss: -0.017418. Value loss: 0.066913. Entropy: 0.305708.\n",
      "Iteration 11040: Policy loss: -0.019177. Value loss: 0.048974. Entropy: 0.304731.\n",
      "episode: 4094   score: 400.0  epsilon: 1.0    steps: 256  evaluation reward: 357.2\n",
      "episode: 4095   score: 340.0  epsilon: 1.0    steps: 256  evaluation reward: 358.2\n",
      "episode: 4096   score: 345.0  epsilon: 1.0    steps: 952  evaluation reward: 356.1\n",
      "episode: 4097   score: 260.0  epsilon: 1.0    steps: 1000  evaluation reward: 353.45\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11041: Policy loss: 0.163375. Value loss: 0.094007. Entropy: 0.287952.\n",
      "Iteration 11042: Policy loss: 0.162082. Value loss: 0.040780. Entropy: 0.288834.\n",
      "Iteration 11043: Policy loss: 0.157716. Value loss: 0.033585. Entropy: 0.287161.\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11044: Policy loss: -0.192026. Value loss: 0.109192. Entropy: 0.297274.\n",
      "Iteration 11045: Policy loss: -0.191326. Value loss: 0.035165. Entropy: 0.298619.\n",
      "Iteration 11046: Policy loss: -0.200960. Value loss: 0.024497. Entropy: 0.298668.\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11047: Policy loss: 0.119789. Value loss: 0.104639. Entropy: 0.312116.\n",
      "Iteration 11048: Policy loss: 0.117043. Value loss: 0.055111. Entropy: 0.311077.\n",
      "Iteration 11049: Policy loss: 0.112116. Value loss: 0.039563. Entropy: 0.310718.\n",
      "episode: 4098   score: 390.0  epsilon: 1.0    steps: 664  evaluation reward: 355.15\n",
      "Training network. lr: 0.000166. clip: 0.066214\n",
      "Iteration 11050: Policy loss: -0.531370. Value loss: 0.466424. Entropy: 0.297883.\n",
      "Iteration 11051: Policy loss: -0.590296. Value loss: 0.335400. Entropy: 0.296705.\n",
      "Iteration 11052: Policy loss: -0.598384. Value loss: 0.298511. Entropy: 0.297392.\n",
      "episode: 4099   score: 125.0  epsilon: 1.0    steps: 80  evaluation reward: 352.75\n",
      "episode: 4100   score: 620.0  epsilon: 1.0    steps: 128  evaluation reward: 355.3\n",
      "now time :  2019-09-06 01:40:46.349960\n",
      "episode: 4101   score: 260.0  epsilon: 1.0    steps: 424  evaluation reward: 352.45\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11053: Policy loss: 0.141034. Value loss: 0.150114. Entropy: 0.262934.\n",
      "Iteration 11054: Policy loss: 0.132349. Value loss: 0.045817. Entropy: 0.261685.\n",
      "Iteration 11055: Policy loss: 0.129912. Value loss: 0.031246. Entropy: 0.262141.\n",
      "episode: 4102   score: 210.0  epsilon: 1.0    steps: 1008  evaluation reward: 351.2\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11056: Policy loss: -0.021604. Value loss: 0.114143. Entropy: 0.311849.\n",
      "Iteration 11057: Policy loss: -0.024041. Value loss: 0.057067. Entropy: 0.312061.\n",
      "Iteration 11058: Policy loss: -0.031539. Value loss: 0.043842. Entropy: 0.311754.\n",
      "episode: 4103   score: 460.0  epsilon: 1.0    steps: 528  evaluation reward: 353.5\n",
      "episode: 4104   score: 125.0  epsilon: 1.0    steps: 960  evaluation reward: 350.85\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11059: Policy loss: -0.017723. Value loss: 0.141382. Entropy: 0.284983.\n",
      "Iteration 11060: Policy loss: -0.029864. Value loss: 0.060994. Entropy: 0.284923.\n",
      "Iteration 11061: Policy loss: -0.037913. Value loss: 0.047414. Entropy: 0.284487.\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11062: Policy loss: -0.139384. Value loss: 0.084300. Entropy: 0.301696.\n",
      "Iteration 11063: Policy loss: -0.137736. Value loss: 0.048535. Entropy: 0.303522.\n",
      "Iteration 11064: Policy loss: -0.142941. Value loss: 0.035731. Entropy: 0.302992.\n",
      "episode: 4105   score: 210.0  epsilon: 1.0    steps: 648  evaluation reward: 349.65\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11065: Policy loss: -0.075755. Value loss: 0.037885. Entropy: 0.290941.\n",
      "Iteration 11066: Policy loss: -0.077736. Value loss: 0.017030. Entropy: 0.291384.\n",
      "Iteration 11067: Policy loss: -0.078756. Value loss: 0.013992. Entropy: 0.291233.\n",
      "episode: 4106   score: 210.0  epsilon: 1.0    steps: 384  evaluation reward: 349.65\n",
      "episode: 4107   score: 210.0  epsilon: 1.0    steps: 456  evaluation reward: 347.55\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11068: Policy loss: -0.222751. Value loss: 0.394314. Entropy: 0.278333.\n",
      "Iteration 11069: Policy loss: -0.235174. Value loss: 0.211280. Entropy: 0.278126.\n",
      "Iteration 11070: Policy loss: -0.242465. Value loss: 0.140358. Entropy: 0.279832.\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11071: Policy loss: 0.109234. Value loss: 0.084651. Entropy: 0.305431.\n",
      "Iteration 11072: Policy loss: 0.109569. Value loss: 0.043959. Entropy: 0.304545.\n",
      "Iteration 11073: Policy loss: 0.105685. Value loss: 0.035087. Entropy: 0.304516.\n",
      "episode: 4108   score: 420.0  epsilon: 1.0    steps: 344  evaluation reward: 349.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11074: Policy loss: -0.697412. Value loss: 0.411854. Entropy: 0.294807.\n",
      "Iteration 11075: Policy loss: -0.751175. Value loss: 0.131727. Entropy: 0.295292.\n",
      "Iteration 11076: Policy loss: -0.759243. Value loss: 0.057113. Entropy: 0.295617.\n",
      "episode: 4109   score: 915.0  epsilon: 1.0    steps: 408  evaluation reward: 356.95\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11077: Policy loss: 0.323383. Value loss: 0.244537. Entropy: 0.293850.\n",
      "Iteration 11078: Policy loss: 0.304386. Value loss: 0.087668. Entropy: 0.294487.\n",
      "Iteration 11079: Policy loss: 0.295364. Value loss: 0.045642. Entropy: 0.293483.\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11080: Policy loss: 0.233911. Value loss: 0.146791. Entropy: 0.316358.\n",
      "Iteration 11081: Policy loss: 0.226883. Value loss: 0.062888. Entropy: 0.315936.\n",
      "Iteration 11082: Policy loss: 0.225781. Value loss: 0.040822. Entropy: 0.314715.\n",
      "episode: 4110   score: 465.0  epsilon: 1.0    steps: 104  evaluation reward: 359.85\n",
      "episode: 4111   score: 270.0  epsilon: 1.0    steps: 352  evaluation reward: 360.45\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11083: Policy loss: 0.046770. Value loss: 0.101212. Entropy: 0.286984.\n",
      "Iteration 11084: Policy loss: 0.037053. Value loss: 0.041917. Entropy: 0.284165.\n",
      "Iteration 11085: Policy loss: 0.036120. Value loss: 0.026125. Entropy: 0.284807.\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11086: Policy loss: 0.025123. Value loss: 0.206985. Entropy: 0.313147.\n",
      "Iteration 11087: Policy loss: 0.013723. Value loss: 0.079655. Entropy: 0.312505.\n",
      "Iteration 11088: Policy loss: 0.006332. Value loss: 0.053017. Entropy: 0.312568.\n",
      "episode: 4112   score: 355.0  epsilon: 1.0    steps: 816  evaluation reward: 361.85\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11089: Policy loss: 0.131412. Value loss: 0.266458. Entropy: 0.300305.\n",
      "Iteration 11090: Policy loss: 0.144368. Value loss: 0.140712. Entropy: 0.300359.\n",
      "Iteration 11091: Policy loss: 0.125913. Value loss: 0.103555. Entropy: 0.300468.\n",
      "episode: 4113   score: 375.0  epsilon: 1.0    steps: 736  evaluation reward: 359.55\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11092: Policy loss: 0.095363. Value loss: 0.083732. Entropy: 0.290964.\n",
      "Iteration 11093: Policy loss: 0.082340. Value loss: 0.034008. Entropy: 0.290195.\n",
      "Iteration 11094: Policy loss: 0.084545. Value loss: 0.025395. Entropy: 0.290101.\n",
      "episode: 4114   score: 695.0  epsilon: 1.0    steps: 72  evaluation reward: 362.25\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11095: Policy loss: -0.060352. Value loss: 0.097622. Entropy: 0.293716.\n",
      "Iteration 11096: Policy loss: -0.067650. Value loss: 0.037397. Entropy: 0.295373.\n",
      "Iteration 11097: Policy loss: -0.065079. Value loss: 0.026002. Entropy: 0.293975.\n",
      "episode: 4115   score: 515.0  epsilon: 1.0    steps: 512  evaluation reward: 362.45\n",
      "episode: 4116   score: 590.0  epsilon: 1.0    steps: 664  evaluation reward: 366.1\n",
      "Training network. lr: 0.000165. clip: 0.066057\n",
      "Iteration 11098: Policy loss: 0.018640. Value loss: 0.376283. Entropy: 0.285311.\n",
      "Iteration 11099: Policy loss: -0.016436. Value loss: 0.282100. Entropy: 0.283213.\n",
      "Iteration 11100: Policy loss: -0.006244. Value loss: 0.218348. Entropy: 0.284752.\n",
      "episode: 4117   score: 295.0  epsilon: 1.0    steps: 112  evaluation reward: 363.35\n",
      "episode: 4118   score: 210.0  epsilon: 1.0    steps: 208  evaluation reward: 359.2\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11101: Policy loss: -0.023107. Value loss: 0.082668. Entropy: 0.283506.\n",
      "Iteration 11102: Policy loss: -0.029677. Value loss: 0.048258. Entropy: 0.282814.\n",
      "Iteration 11103: Policy loss: -0.030887. Value loss: 0.041381. Entropy: 0.282657.\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11104: Policy loss: 0.025308. Value loss: 0.064971. Entropy: 0.307660.\n",
      "Iteration 11105: Policy loss: 0.023861. Value loss: 0.024994. Entropy: 0.308062.\n",
      "Iteration 11106: Policy loss: 0.016002. Value loss: 0.016607. Entropy: 0.308541.\n",
      "episode: 4119   score: 595.0  epsilon: 1.0    steps: 56  evaluation reward: 361.75\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11107: Policy loss: -0.326354. Value loss: 0.367655. Entropy: 0.296629.\n",
      "Iteration 11108: Policy loss: -0.353859. Value loss: 0.200174. Entropy: 0.295079.\n",
      "Iteration 11109: Policy loss: -0.352948. Value loss: 0.134045. Entropy: 0.295479.\n",
      "episode: 4120   score: 210.0  epsilon: 1.0    steps: 352  evaluation reward: 359.35\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11110: Policy loss: 0.038061. Value loss: 0.124902. Entropy: 0.294242.\n",
      "Iteration 11111: Policy loss: 0.027668. Value loss: 0.044706. Entropy: 0.295237.\n",
      "Iteration 11112: Policy loss: 0.022480. Value loss: 0.030035. Entropy: 0.293250.\n",
      "episode: 4121   score: 365.0  epsilon: 1.0    steps: 24  evaluation reward: 358.35\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11113: Policy loss: 0.054154. Value loss: 0.065525. Entropy: 0.298936.\n",
      "Iteration 11114: Policy loss: 0.047583. Value loss: 0.025889. Entropy: 0.298092.\n",
      "Iteration 11115: Policy loss: 0.044521. Value loss: 0.020599. Entropy: 0.296605.\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11116: Policy loss: 0.092287. Value loss: 0.100318. Entropy: 0.311784.\n",
      "Iteration 11117: Policy loss: 0.087465. Value loss: 0.048719. Entropy: 0.311726.\n",
      "Iteration 11118: Policy loss: 0.079285. Value loss: 0.040311. Entropy: 0.311461.\n",
      "episode: 4122   score: 285.0  epsilon: 1.0    steps: 296  evaluation reward: 356.05\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11119: Policy loss: -0.058488. Value loss: 0.128000. Entropy: 0.295074.\n",
      "Iteration 11120: Policy loss: -0.070117. Value loss: 0.039031. Entropy: 0.295683.\n",
      "Iteration 11121: Policy loss: -0.073786. Value loss: 0.026712. Entropy: 0.295651.\n",
      "episode: 4123   score: 560.0  epsilon: 1.0    steps: 856  evaluation reward: 358.3\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11122: Policy loss: 0.083992. Value loss: 0.117957. Entropy: 0.302610.\n",
      "Iteration 11123: Policy loss: 0.073386. Value loss: 0.044108. Entropy: 0.302293.\n",
      "Iteration 11124: Policy loss: 0.071695. Value loss: 0.033878. Entropy: 0.301490.\n",
      "episode: 4124   score: 400.0  epsilon: 1.0    steps: 280  evaluation reward: 360.15\n",
      "episode: 4125   score: 435.0  epsilon: 1.0    steps: 664  evaluation reward: 361.55\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11125: Policy loss: 0.163515. Value loss: 0.071271. Entropy: 0.278190.\n",
      "Iteration 11126: Policy loss: 0.156751. Value loss: 0.031668. Entropy: 0.279162.\n",
      "Iteration 11127: Policy loss: 0.155662. Value loss: 0.026020. Entropy: 0.277742.\n",
      "episode: 4126   score: 440.0  epsilon: 1.0    steps: 400  evaluation reward: 363.55\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11128: Policy loss: -0.263857. Value loss: 0.304386. Entropy: 0.298131.\n",
      "Iteration 11129: Policy loss: -0.248136. Value loss: 0.117581. Entropy: 0.297027.\n",
      "Iteration 11130: Policy loss: -0.280351. Value loss: 0.074464. Entropy: 0.297484.\n",
      "episode: 4127   score: 495.0  epsilon: 1.0    steps: 184  evaluation reward: 366.1\n",
      "episode: 4128   score: 290.0  epsilon: 1.0    steps: 712  evaluation reward: 366.4\n",
      "episode: 4129   score: 285.0  epsilon: 1.0    steps: 880  evaluation reward: 366.85\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11131: Policy loss: 0.161094. Value loss: 0.150597. Entropy: 0.280151.\n",
      "Iteration 11132: Policy loss: 0.151253. Value loss: 0.052080. Entropy: 0.278663.\n",
      "Iteration 11133: Policy loss: 0.143351. Value loss: 0.041947. Entropy: 0.278370.\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11134: Policy loss: 0.021214. Value loss: 0.180941. Entropy: 0.302799.\n",
      "Iteration 11135: Policy loss: 0.007408. Value loss: 0.056798. Entropy: 0.303116.\n",
      "Iteration 11136: Policy loss: 0.014185. Value loss: 0.043421. Entropy: 0.302566.\n",
      "episode: 4130   score: 155.0  epsilon: 1.0    steps: 160  evaluation reward: 365.9\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11137: Policy loss: -0.169956. Value loss: 0.115492. Entropy: 0.295812.\n",
      "Iteration 11138: Policy loss: -0.174030. Value loss: 0.046283. Entropy: 0.295835.\n",
      "Iteration 11139: Policy loss: -0.179966. Value loss: 0.030376. Entropy: 0.295374.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11140: Policy loss: 0.404287. Value loss: 0.198962. Entropy: 0.306237.\n",
      "Iteration 11141: Policy loss: 0.400780. Value loss: 0.040318. Entropy: 0.304048.\n",
      "Iteration 11142: Policy loss: 0.381163. Value loss: 0.020514. Entropy: 0.305061.\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11143: Policy loss: 0.108899. Value loss: 0.118271. Entropy: 0.313442.\n",
      "Iteration 11144: Policy loss: 0.109793. Value loss: 0.054848. Entropy: 0.313049.\n",
      "Iteration 11145: Policy loss: 0.099543. Value loss: 0.045686. Entropy: 0.313469.\n",
      "episode: 4131   score: 315.0  epsilon: 1.0    steps: 928  evaluation reward: 366.65\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11146: Policy loss: 0.189363. Value loss: 0.130472. Entropy: 0.301365.\n",
      "Iteration 11147: Policy loss: 0.186200. Value loss: 0.057595. Entropy: 0.300605.\n",
      "Iteration 11148: Policy loss: 0.178899. Value loss: 0.041440. Entropy: 0.299499.\n",
      "episode: 4132   score: 340.0  epsilon: 1.0    steps: 568  evaluation reward: 364.3\n",
      "Training network. lr: 0.000165. clip: 0.065910\n",
      "Iteration 11149: Policy loss: 0.159373. Value loss: 0.149286. Entropy: 0.286409.\n",
      "Iteration 11150: Policy loss: 0.159434. Value loss: 0.056434. Entropy: 0.284668.\n",
      "Iteration 11151: Policy loss: 0.140895. Value loss: 0.037834. Entropy: 0.283450.\n",
      "episode: 4133   score: 210.0  epsilon: 1.0    steps: 80  evaluation reward: 362.75\n",
      "episode: 4134   score: 270.0  epsilon: 1.0    steps: 624  evaluation reward: 363.0\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11152: Policy loss: -0.135573. Value loss: 0.149405. Entropy: 0.280379.\n",
      "Iteration 11153: Policy loss: -0.145858. Value loss: 0.063954. Entropy: 0.280645.\n",
      "Iteration 11154: Policy loss: -0.139682. Value loss: 0.044008. Entropy: 0.278203.\n",
      "episode: 4135   score: 710.0  epsilon: 1.0    steps: 136  evaluation reward: 366.55\n",
      "episode: 4136   score: 465.0  epsilon: 1.0    steps: 608  evaluation reward: 366.95\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11155: Policy loss: 0.228541. Value loss: 0.090182. Entropy: 0.283466.\n",
      "Iteration 11156: Policy loss: 0.221912. Value loss: 0.040393. Entropy: 0.282326.\n",
      "Iteration 11157: Policy loss: 0.205890. Value loss: 0.029518. Entropy: 0.282784.\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11158: Policy loss: -0.509144. Value loss: 0.514122. Entropy: 0.310579.\n",
      "Iteration 11159: Policy loss: -0.526977. Value loss: 0.340010. Entropy: 0.311029.\n",
      "Iteration 11160: Policy loss: -0.536337. Value loss: 0.251505. Entropy: 0.309632.\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11161: Policy loss: 0.060358. Value loss: 0.129369. Entropy: 0.303614.\n",
      "Iteration 11162: Policy loss: 0.053465. Value loss: 0.037763. Entropy: 0.303968.\n",
      "Iteration 11163: Policy loss: 0.050318. Value loss: 0.026995. Entropy: 0.303498.\n",
      "episode: 4137   score: 700.0  epsilon: 1.0    steps: 400  evaluation reward: 371.7\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11164: Policy loss: -0.386699. Value loss: 0.362334. Entropy: 0.293197.\n",
      "Iteration 11165: Policy loss: -0.375748. Value loss: 0.221118. Entropy: 0.291511.\n",
      "Iteration 11166: Policy loss: -0.400731. Value loss: 0.189391. Entropy: 0.292217.\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11167: Policy loss: 0.044892. Value loss: 0.109418. Entropy: 0.306729.\n",
      "Iteration 11168: Policy loss: 0.037287. Value loss: 0.037581. Entropy: 0.306369.\n",
      "Iteration 11169: Policy loss: 0.033615. Value loss: 0.028097. Entropy: 0.306562.\n",
      "episode: 4138   score: 625.0  epsilon: 1.0    steps: 936  evaluation reward: 375.8\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11170: Policy loss: -0.160637. Value loss: 0.139468. Entropy: 0.309301.\n",
      "Iteration 11171: Policy loss: -0.163797. Value loss: 0.068260. Entropy: 0.308710.\n",
      "Iteration 11172: Policy loss: -0.175007. Value loss: 0.050021. Entropy: 0.308554.\n",
      "episode: 4139   score: 310.0  epsilon: 1.0    steps: 688  evaluation reward: 375.75\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11173: Policy loss: 0.156566. Value loss: 0.063265. Entropy: 0.286240.\n",
      "Iteration 11174: Policy loss: 0.151439. Value loss: 0.026751. Entropy: 0.287457.\n",
      "Iteration 11175: Policy loss: 0.149889. Value loss: 0.020877. Entropy: 0.288092.\n",
      "episode: 4140   score: 320.0  epsilon: 1.0    steps: 800  evaluation reward: 376.85\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11176: Policy loss: -0.035519. Value loss: 0.121905. Entropy: 0.298828.\n",
      "Iteration 11177: Policy loss: -0.036966. Value loss: 0.044990. Entropy: 0.299043.\n",
      "Iteration 11178: Policy loss: -0.052879. Value loss: 0.032093. Entropy: 0.297679.\n",
      "episode: 4141   score: 605.0  epsilon: 1.0    steps: 176  evaluation reward: 380.8\n",
      "episode: 4142   score: 475.0  epsilon: 1.0    steps: 896  evaluation reward: 382.35\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11179: Policy loss: -0.072984. Value loss: 0.210513. Entropy: 0.284241.\n",
      "Iteration 11180: Policy loss: -0.091682. Value loss: 0.085179. Entropy: 0.284228.\n",
      "Iteration 11181: Policy loss: -0.076025. Value loss: 0.061195. Entropy: 0.282816.\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11182: Policy loss: 0.126743. Value loss: 0.129348. Entropy: 0.303865.\n",
      "Iteration 11183: Policy loss: 0.117522. Value loss: 0.046215. Entropy: 0.303630.\n",
      "Iteration 11184: Policy loss: 0.113153. Value loss: 0.032711. Entropy: 0.302915.\n",
      "episode: 4143   score: 640.0  epsilon: 1.0    steps: 16  evaluation reward: 385.25\n",
      "episode: 4144   score: 305.0  epsilon: 1.0    steps: 744  evaluation reward: 382.7\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11185: Policy loss: -0.163975. Value loss: 0.127335. Entropy: 0.287617.\n",
      "Iteration 11186: Policy loss: -0.165977. Value loss: 0.043814. Entropy: 0.285695.\n",
      "Iteration 11187: Policy loss: -0.171375. Value loss: 0.031275. Entropy: 0.286525.\n",
      "episode: 4145   score: 540.0  epsilon: 1.0    steps: 88  evaluation reward: 385.35\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11188: Policy loss: 0.175045. Value loss: 0.221396. Entropy: 0.298846.\n",
      "Iteration 11189: Policy loss: 0.170317. Value loss: 0.063205. Entropy: 0.299030.\n",
      "Iteration 11190: Policy loss: 0.157644. Value loss: 0.042617. Entropy: 0.299421.\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11191: Policy loss: 0.053790. Value loss: 0.095546. Entropy: 0.310102.\n",
      "Iteration 11192: Policy loss: 0.055612. Value loss: 0.034546. Entropy: 0.309046.\n",
      "Iteration 11193: Policy loss: 0.047649. Value loss: 0.022583. Entropy: 0.309067.\n",
      "episode: 4146   score: 565.0  epsilon: 1.0    steps: 944  evaluation reward: 385.4\n",
      "episode: 4147   score: 315.0  epsilon: 1.0    steps: 952  evaluation reward: 386.4\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11194: Policy loss: 0.102718. Value loss: 0.154713. Entropy: 0.301065.\n",
      "Iteration 11195: Policy loss: 0.093335. Value loss: 0.057315. Entropy: 0.301326.\n",
      "Iteration 11196: Policy loss: 0.077963. Value loss: 0.041150. Entropy: 0.301479.\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11197: Policy loss: -0.380064. Value loss: 0.355478. Entropy: 0.301450.\n",
      "Iteration 11198: Policy loss: -0.378670. Value loss: 0.253072. Entropy: 0.303768.\n",
      "Iteration 11199: Policy loss: -0.369975. Value loss: 0.193820. Entropy: 0.303459.\n",
      "episode: 4148   score: 330.0  epsilon: 1.0    steps: 848  evaluation reward: 383.75\n",
      "Training network. lr: 0.000164. clip: 0.065753\n",
      "Iteration 11200: Policy loss: -0.071565. Value loss: 0.114401. Entropy: 0.304081.\n",
      "Iteration 11201: Policy loss: -0.073520. Value loss: 0.053530. Entropy: 0.304194.\n",
      "Iteration 11202: Policy loss: -0.084283. Value loss: 0.038161. Entropy: 0.304486.\n",
      "episode: 4149   score: 480.0  epsilon: 1.0    steps: 640  evaluation reward: 386.3\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11203: Policy loss: 0.220434. Value loss: 0.163396. Entropy: 0.293379.\n",
      "Iteration 11204: Policy loss: 0.221254. Value loss: 0.055575. Entropy: 0.289077.\n",
      "Iteration 11205: Policy loss: 0.211897. Value loss: 0.035231. Entropy: 0.291958.\n",
      "episode: 4150   score: 300.0  epsilon: 1.0    steps: 160  evaluation reward: 384.9\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11206: Policy loss: 0.021150. Value loss: 0.089064. Entropy: 0.296886.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11207: Policy loss: 0.012123. Value loss: 0.036106. Entropy: 0.297220.\n",
      "Iteration 11208: Policy loss: 0.012103. Value loss: 0.024477. Entropy: 0.296504.\n",
      "now time :  2019-09-06 01:50:27.047729\n",
      "episode: 4151   score: 600.0  epsilon: 1.0    steps: 640  evaluation reward: 388.05\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11209: Policy loss: 0.250180. Value loss: 0.097026. Entropy: 0.296510.\n",
      "Iteration 11210: Policy loss: 0.250658. Value loss: 0.032550. Entropy: 0.295823.\n",
      "Iteration 11211: Policy loss: 0.235939. Value loss: 0.024259. Entropy: 0.295834.\n",
      "episode: 4152   score: 390.0  epsilon: 1.0    steps: 312  evaluation reward: 388.05\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11212: Policy loss: -0.064719. Value loss: 0.311763. Entropy: 0.299223.\n",
      "Iteration 11213: Policy loss: -0.094234. Value loss: 0.221090. Entropy: 0.300475.\n",
      "Iteration 11214: Policy loss: -0.084714. Value loss: 0.109815. Entropy: 0.298012.\n",
      "episode: 4153   score: 210.0  epsilon: 1.0    steps: 160  evaluation reward: 387.4\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11215: Policy loss: -0.035800. Value loss: 0.125079. Entropy: 0.297816.\n",
      "Iteration 11216: Policy loss: -0.046115. Value loss: 0.058229. Entropy: 0.298636.\n",
      "Iteration 11217: Policy loss: -0.052761. Value loss: 0.039616. Entropy: 0.297863.\n",
      "episode: 4154   score: 495.0  epsilon: 1.0    steps: 608  evaluation reward: 389.1\n",
      "episode: 4155   score: 405.0  epsilon: 1.0    steps: 624  evaluation reward: 390.35\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11218: Policy loss: 0.108722. Value loss: 0.139927. Entropy: 0.281274.\n",
      "Iteration 11219: Policy loss: 0.102806. Value loss: 0.058051. Entropy: 0.280653.\n",
      "Iteration 11220: Policy loss: 0.084803. Value loss: 0.042529. Entropy: 0.281713.\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11221: Policy loss: -0.235433. Value loss: 0.314790. Entropy: 0.309251.\n",
      "Iteration 11222: Policy loss: -0.243445. Value loss: 0.118493. Entropy: 0.310852.\n",
      "Iteration 11223: Policy loss: -0.245872. Value loss: 0.061365. Entropy: 0.309619.\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11224: Policy loss: 0.053986. Value loss: 0.093320. Entropy: 0.313860.\n",
      "Iteration 11225: Policy loss: 0.054301. Value loss: 0.036523. Entropy: 0.312878.\n",
      "Iteration 11226: Policy loss: 0.049675. Value loss: 0.024841. Entropy: 0.313464.\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11227: Policy loss: -0.371772. Value loss: 0.352562. Entropy: 0.311958.\n",
      "Iteration 11228: Policy loss: -0.376931. Value loss: 0.239716. Entropy: 0.312476.\n",
      "Iteration 11229: Policy loss: -0.371719. Value loss: 0.185505. Entropy: 0.311535.\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11230: Policy loss: 0.033319. Value loss: 0.102028. Entropy: 0.311414.\n",
      "Iteration 11231: Policy loss: 0.033095. Value loss: 0.041516. Entropy: 0.311547.\n",
      "Iteration 11232: Policy loss: 0.028266. Value loss: 0.027354. Entropy: 0.312366.\n",
      "episode: 4156   score: 285.0  epsilon: 1.0    steps: 296  evaluation reward: 388.25\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11233: Policy loss: 0.012748. Value loss: 0.258909. Entropy: 0.299177.\n",
      "Iteration 11234: Policy loss: -0.011439. Value loss: 0.178142. Entropy: 0.297162.\n",
      "Iteration 11235: Policy loss: -0.015178. Value loss: 0.155221. Entropy: 0.297167.\n",
      "episode: 4157   score: 570.0  epsilon: 1.0    steps: 488  evaluation reward: 390.45\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11236: Policy loss: 0.039222. Value loss: 0.195186. Entropy: 0.295602.\n",
      "Iteration 11237: Policy loss: 0.028887. Value loss: 0.074254. Entropy: 0.295029.\n",
      "Iteration 11238: Policy loss: 0.031860. Value loss: 0.049048. Entropy: 0.295124.\n",
      "episode: 4158   score: 870.0  epsilon: 1.0    steps: 72  evaluation reward: 396.75\n",
      "episode: 4159   score: 375.0  epsilon: 1.0    steps: 160  evaluation reward: 393.8\n",
      "episode: 4160   score: 730.0  epsilon: 1.0    steps: 776  evaluation reward: 397.9\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11239: Policy loss: 0.198139. Value loss: 0.092327. Entropy: 0.278022.\n",
      "Iteration 11240: Policy loss: 0.193831. Value loss: 0.049650. Entropy: 0.280119.\n",
      "Iteration 11241: Policy loss: 0.193594. Value loss: 0.036370. Entropy: 0.279011.\n",
      "episode: 4161   score: 345.0  epsilon: 1.0    steps: 112  evaluation reward: 398.45\n",
      "episode: 4162   score: 260.0  epsilon: 1.0    steps: 344  evaluation reward: 397.7\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11242: Policy loss: -0.303737. Value loss: 0.206655. Entropy: 0.281332.\n",
      "Iteration 11243: Policy loss: -0.312546. Value loss: 0.094886. Entropy: 0.280348.\n",
      "Iteration 11244: Policy loss: -0.321698. Value loss: 0.070527. Entropy: 0.282553.\n",
      "episode: 4163   score: 585.0  epsilon: 1.0    steps: 232  evaluation reward: 397.25\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11245: Policy loss: -0.055124. Value loss: 0.180714. Entropy: 0.290720.\n",
      "Iteration 11246: Policy loss: -0.067336. Value loss: 0.094758. Entropy: 0.291127.\n",
      "Iteration 11247: Policy loss: -0.075714. Value loss: 0.072123. Entropy: 0.291525.\n",
      "Training network. lr: 0.000164. clip: 0.065597\n",
      "Iteration 11248: Policy loss: -0.035808. Value loss: 0.268143. Entropy: 0.310169.\n",
      "Iteration 11249: Policy loss: -0.035198. Value loss: 0.104551. Entropy: 0.309194.\n",
      "Iteration 11250: Policy loss: -0.036344. Value loss: 0.079785. Entropy: 0.308494.\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11251: Policy loss: 0.395860. Value loss: 0.306895. Entropy: 0.306290.\n",
      "Iteration 11252: Policy loss: 0.387310. Value loss: 0.098537. Entropy: 0.306429.\n",
      "Iteration 11253: Policy loss: 0.364884. Value loss: 0.061758. Entropy: 0.306549.\n",
      "episode: 4164   score: 545.0  epsilon: 1.0    steps: 584  evaluation reward: 399.05\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11254: Policy loss: 0.198849. Value loss: 0.114969. Entropy: 0.301778.\n",
      "Iteration 11255: Policy loss: 0.197637. Value loss: 0.059513. Entropy: 0.300475.\n",
      "Iteration 11256: Policy loss: 0.184146. Value loss: 0.042086. Entropy: 0.300092.\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11257: Policy loss: -0.130310. Value loss: 0.073874. Entropy: 0.314010.\n",
      "Iteration 11258: Policy loss: -0.141057. Value loss: 0.035569. Entropy: 0.313509.\n",
      "Iteration 11259: Policy loss: -0.141831. Value loss: 0.025852. Entropy: 0.311536.\n",
      "episode: 4165   score: 260.0  epsilon: 1.0    steps: 64  evaluation reward: 394.95\n",
      "episode: 4166   score: 550.0  epsilon: 1.0    steps: 720  evaluation reward: 396.25\n",
      "episode: 4167   score: 345.0  epsilon: 1.0    steps: 872  evaluation reward: 395.8\n",
      "episode: 4168   score: 290.0  epsilon: 1.0    steps: 968  evaluation reward: 396.35\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11260: Policy loss: 0.137465. Value loss: 0.095972. Entropy: 0.283716.\n",
      "Iteration 11261: Policy loss: 0.124715. Value loss: 0.048683. Entropy: 0.283564.\n",
      "Iteration 11262: Policy loss: 0.125718. Value loss: 0.041442. Entropy: 0.283689.\n",
      "episode: 4169   score: 360.0  epsilon: 1.0    steps: 840  evaluation reward: 397.85\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11263: Policy loss: -0.369543. Value loss: 0.362950. Entropy: 0.290178.\n",
      "Iteration 11264: Policy loss: -0.386370. Value loss: 0.238729. Entropy: 0.291030.\n",
      "Iteration 11265: Policy loss: -0.411754. Value loss: 0.177870. Entropy: 0.291141.\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11266: Policy loss: 0.128834. Value loss: 0.099457. Entropy: 0.306005.\n",
      "Iteration 11267: Policy loss: 0.118878. Value loss: 0.029748. Entropy: 0.307200.\n",
      "Iteration 11268: Policy loss: 0.120413. Value loss: 0.020446. Entropy: 0.306471.\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11269: Policy loss: -0.243692. Value loss: 0.159742. Entropy: 0.309146.\n",
      "Iteration 11270: Policy loss: -0.244026. Value loss: 0.055962. Entropy: 0.310054.\n",
      "Iteration 11271: Policy loss: -0.266128. Value loss: 0.037715. Entropy: 0.309951.\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11272: Policy loss: 0.220176. Value loss: 0.194925. Entropy: 0.309781.\n",
      "Iteration 11273: Policy loss: 0.204867. Value loss: 0.054195. Entropy: 0.308301.\n",
      "Iteration 11274: Policy loss: 0.193109. Value loss: 0.029108. Entropy: 0.309478.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11275: Policy loss: -0.176935. Value loss: 0.281923. Entropy: 0.312813.\n",
      "Iteration 11276: Policy loss: -0.191464. Value loss: 0.135701. Entropy: 0.312990.\n",
      "Iteration 11277: Policy loss: -0.176820. Value loss: 0.054378. Entropy: 0.312691.\n",
      "episode: 4170   score: 210.0  epsilon: 1.0    steps: 520  evaluation reward: 394.55\n",
      "episode: 4171   score: 660.0  epsilon: 1.0    steps: 680  evaluation reward: 398.45\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11278: Policy loss: 0.505432. Value loss: 0.267027. Entropy: 0.289550.\n",
      "Iteration 11279: Policy loss: 0.487006. Value loss: 0.081925. Entropy: 0.286994.\n",
      "Iteration 11280: Policy loss: 0.478626. Value loss: 0.044330. Entropy: 0.288992.\n",
      "episode: 4172   score: 470.0  epsilon: 1.0    steps: 88  evaluation reward: 399.25\n",
      "episode: 4173   score: 700.0  epsilon: 1.0    steps: 256  evaluation reward: 403.65\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11281: Policy loss: 0.158186. Value loss: 0.108250. Entropy: 0.286399.\n",
      "Iteration 11282: Policy loss: 0.157157. Value loss: 0.048415. Entropy: 0.287152.\n",
      "Iteration 11283: Policy loss: 0.155375. Value loss: 0.035541. Entropy: 0.288296.\n",
      "episode: 4174   score: 290.0  epsilon: 1.0    steps: 560  evaluation reward: 403.1\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11284: Policy loss: -0.078132. Value loss: 0.085041. Entropy: 0.297884.\n",
      "Iteration 11285: Policy loss: -0.083874. Value loss: 0.038317. Entropy: 0.295984.\n",
      "Iteration 11286: Policy loss: -0.090313. Value loss: 0.029577. Entropy: 0.296191.\n",
      "episode: 4175   score: 330.0  epsilon: 1.0    steps: 248  evaluation reward: 402.35\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11287: Policy loss: -0.522696. Value loss: 0.247095. Entropy: 0.295666.\n",
      "Iteration 11288: Policy loss: -0.527741. Value loss: 0.097001. Entropy: 0.296964.\n",
      "Iteration 11289: Policy loss: -0.532538. Value loss: 0.052655. Entropy: 0.297811.\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11290: Policy loss: 0.144000. Value loss: 0.143618. Entropy: 0.308241.\n",
      "Iteration 11291: Policy loss: 0.137479. Value loss: 0.053998. Entropy: 0.307938.\n",
      "Iteration 11292: Policy loss: 0.132722. Value loss: 0.034183. Entropy: 0.308037.\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11293: Policy loss: -0.156432. Value loss: 0.279593. Entropy: 0.308852.\n",
      "Iteration 11294: Policy loss: -0.157080. Value loss: 0.126929. Entropy: 0.307711.\n",
      "Iteration 11295: Policy loss: -0.155151. Value loss: 0.074717. Entropy: 0.307932.\n",
      "episode: 4176   score: 650.0  epsilon: 1.0    steps: 184  evaluation reward: 404.25\n",
      "episode: 4177   score: 465.0  epsilon: 1.0    steps: 712  evaluation reward: 405.75\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11296: Policy loss: 0.244670. Value loss: 0.175242. Entropy: 0.296637.\n",
      "Iteration 11297: Policy loss: 0.243097. Value loss: 0.061336. Entropy: 0.297809.\n",
      "Iteration 11298: Policy loss: 0.237430. Value loss: 0.042438. Entropy: 0.298431.\n",
      "Training network. lr: 0.000164. clip: 0.065449\n",
      "Iteration 11299: Policy loss: 0.022652. Value loss: 0.125177. Entropy: 0.303327.\n",
      "Iteration 11300: Policy loss: 0.024996. Value loss: 0.064104. Entropy: 0.302908.\n",
      "Iteration 11301: Policy loss: 0.017155. Value loss: 0.042542. Entropy: 0.302638.\n",
      "episode: 4178   score: 590.0  epsilon: 1.0    steps: 832  evaluation reward: 407.7\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11302: Policy loss: 0.112630. Value loss: 0.114884. Entropy: 0.305155.\n",
      "Iteration 11303: Policy loss: 0.107130. Value loss: 0.048001. Entropy: 0.306853.\n",
      "Iteration 11304: Policy loss: 0.105459. Value loss: 0.035596. Entropy: 0.305177.\n",
      "episode: 4179   score: 565.0  epsilon: 1.0    steps: 8  evaluation reward: 410.4\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11305: Policy loss: 0.044275. Value loss: 0.135375. Entropy: 0.310225.\n",
      "Iteration 11306: Policy loss: 0.041419. Value loss: 0.048767. Entropy: 0.310555.\n",
      "Iteration 11307: Policy loss: 0.031348. Value loss: 0.029806. Entropy: 0.311219.\n",
      "episode: 4180   score: 540.0  epsilon: 1.0    steps: 792  evaluation reward: 412.35\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11308: Policy loss: 0.303978. Value loss: 0.080152. Entropy: 0.305733.\n",
      "Iteration 11309: Policy loss: 0.296742. Value loss: 0.035817. Entropy: 0.304650.\n",
      "Iteration 11310: Policy loss: 0.294527. Value loss: 0.028861. Entropy: 0.304452.\n",
      "episode: 4181   score: 275.0  epsilon: 1.0    steps: 192  evaluation reward: 410.15\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11311: Policy loss: 0.100223. Value loss: 0.048159. Entropy: 0.307885.\n",
      "Iteration 11312: Policy loss: 0.097777. Value loss: 0.021401. Entropy: 0.308841.\n",
      "Iteration 11313: Policy loss: 0.095152. Value loss: 0.015223. Entropy: 0.308574.\n",
      "episode: 4182   score: 210.0  epsilon: 1.0    steps: 480  evaluation reward: 407.55\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11314: Policy loss: -0.208148. Value loss: 0.119671. Entropy: 0.305166.\n",
      "Iteration 11315: Policy loss: -0.220102. Value loss: 0.041984. Entropy: 0.305062.\n",
      "Iteration 11316: Policy loss: -0.227610. Value loss: 0.027885. Entropy: 0.305444.\n",
      "episode: 4183   score: 470.0  epsilon: 1.0    steps: 928  evaluation reward: 405.9\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11317: Policy loss: 0.021813. Value loss: 0.150136. Entropy: 0.299880.\n",
      "Iteration 11318: Policy loss: 0.012213. Value loss: 0.077613. Entropy: 0.298655.\n",
      "Iteration 11319: Policy loss: 0.000424. Value loss: 0.052574. Entropy: 0.298439.\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11320: Policy loss: -0.101075. Value loss: 0.217932. Entropy: 0.313083.\n",
      "Iteration 11321: Policy loss: -0.091667. Value loss: 0.070205. Entropy: 0.311290.\n",
      "Iteration 11322: Policy loss: -0.117240. Value loss: 0.050147. Entropy: 0.311014.\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11323: Policy loss: -0.117032. Value loss: 0.086344. Entropy: 0.306102.\n",
      "Iteration 11324: Policy loss: -0.119346. Value loss: 0.027949. Entropy: 0.306027.\n",
      "Iteration 11325: Policy loss: -0.121947. Value loss: 0.021732. Entropy: 0.307471.\n",
      "episode: 4184   score: 800.0  epsilon: 1.0    steps: 536  evaluation reward: 410.15\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11326: Policy loss: 0.210915. Value loss: 0.169694. Entropy: 0.304764.\n",
      "Iteration 11327: Policy loss: 0.201826. Value loss: 0.048675. Entropy: 0.304173.\n",
      "Iteration 11328: Policy loss: 0.204095. Value loss: 0.031952. Entropy: 0.303742.\n",
      "episode: 4185   score: 325.0  epsilon: 1.0    steps: 496  evaluation reward: 411.3\n",
      "episode: 4186   score: 560.0  epsilon: 1.0    steps: 512  evaluation reward: 412.2\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11329: Policy loss: 0.019070. Value loss: 0.108308. Entropy: 0.303297.\n",
      "Iteration 11330: Policy loss: 0.006348. Value loss: 0.043579. Entropy: 0.304270.\n",
      "Iteration 11331: Policy loss: 0.014753. Value loss: 0.029791. Entropy: 0.303839.\n",
      "episode: 4187   score: 420.0  epsilon: 1.0    steps: 152  evaluation reward: 413.25\n",
      "episode: 4188   score: 270.0  epsilon: 1.0    steps: 272  evaluation reward: 410.6\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11332: Policy loss: 0.266129. Value loss: 0.073566. Entropy: 0.307667.\n",
      "Iteration 11333: Policy loss: 0.258807. Value loss: 0.034792. Entropy: 0.306738.\n",
      "Iteration 11334: Policy loss: 0.254852. Value loss: 0.028524. Entropy: 0.307248.\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11335: Policy loss: 0.095460. Value loss: 0.094705. Entropy: 0.304782.\n",
      "Iteration 11336: Policy loss: 0.091848. Value loss: 0.051251. Entropy: 0.304138.\n",
      "Iteration 11337: Policy loss: 0.086439. Value loss: 0.035990. Entropy: 0.301939.\n",
      "episode: 4189   score: 275.0  epsilon: 1.0    steps: 64  evaluation reward: 410.5\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11338: Policy loss: -0.103176. Value loss: 0.275117. Entropy: 0.305079.\n",
      "Iteration 11339: Policy loss: -0.124174. Value loss: 0.211518. Entropy: 0.305485.\n",
      "Iteration 11340: Policy loss: -0.126251. Value loss: 0.153139. Entropy: 0.304468.\n",
      "episode: 4190   score: 255.0  epsilon: 1.0    steps: 112  evaluation reward: 412.45\n",
      "Training network. lr: 0.000163. clip: 0.065293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11341: Policy loss: 0.204840. Value loss: 0.086489. Entropy: 0.305086.\n",
      "Iteration 11342: Policy loss: 0.190927. Value loss: 0.032647. Entropy: 0.304648.\n",
      "Iteration 11343: Policy loss: 0.190665. Value loss: 0.024254. Entropy: 0.303964.\n",
      "episode: 4191   score: 485.0  epsilon: 1.0    steps: 792  evaluation reward: 415.95\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11344: Policy loss: 0.148839. Value loss: 0.096484. Entropy: 0.303420.\n",
      "Iteration 11345: Policy loss: 0.145691. Value loss: 0.040848. Entropy: 0.300586.\n",
      "Iteration 11346: Policy loss: 0.134597. Value loss: 0.030917. Entropy: 0.299467.\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11347: Policy loss: 0.171529. Value loss: 0.103612. Entropy: 0.313008.\n",
      "Iteration 11348: Policy loss: 0.165680. Value loss: 0.033730. Entropy: 0.311911.\n",
      "Iteration 11349: Policy loss: 0.155323. Value loss: 0.028370. Entropy: 0.312533.\n",
      "Training network. lr: 0.000163. clip: 0.065293\n",
      "Iteration 11350: Policy loss: 0.044756. Value loss: 0.118785. Entropy: 0.311229.\n",
      "Iteration 11351: Policy loss: 0.043686. Value loss: 0.049764. Entropy: 0.310273.\n",
      "Iteration 11352: Policy loss: 0.036095. Value loss: 0.033581. Entropy: 0.311676.\n",
      "episode: 4192   score: 280.0  epsilon: 1.0    steps: 320  evaluation reward: 416.65\n",
      "episode: 4193   score: 295.0  epsilon: 1.0    steps: 688  evaluation reward: 414.65\n",
      "episode: 4194   score: 390.0  epsilon: 1.0    steps: 912  evaluation reward: 414.55\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11353: Policy loss: 0.044040. Value loss: 0.080281. Entropy: 0.300283.\n",
      "Iteration 11354: Policy loss: 0.037102. Value loss: 0.040579. Entropy: 0.301440.\n",
      "Iteration 11355: Policy loss: 0.034473. Value loss: 0.033357. Entropy: 0.298560.\n",
      "episode: 4195   score: 330.0  epsilon: 1.0    steps: 720  evaluation reward: 414.45\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11356: Policy loss: -0.199356. Value loss: 0.217352. Entropy: 0.302608.\n",
      "Iteration 11357: Policy loss: -0.204361. Value loss: 0.109345. Entropy: 0.301256.\n",
      "Iteration 11358: Policy loss: -0.215333. Value loss: 0.062668. Entropy: 0.300306.\n",
      "episode: 4196   score: 620.0  epsilon: 1.0    steps: 552  evaluation reward: 417.2\n",
      "episode: 4197   score: 410.0  epsilon: 1.0    steps: 648  evaluation reward: 418.7\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11359: Policy loss: -0.156718. Value loss: 0.128917. Entropy: 0.299347.\n",
      "Iteration 11360: Policy loss: -0.160398. Value loss: 0.048343. Entropy: 0.298321.\n",
      "Iteration 11361: Policy loss: -0.165634. Value loss: 0.034484. Entropy: 0.297723.\n",
      "episode: 4198   score: 280.0  epsilon: 1.0    steps: 8  evaluation reward: 417.6\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11362: Policy loss: -0.078765. Value loss: 0.079999. Entropy: 0.302230.\n",
      "Iteration 11363: Policy loss: -0.084734. Value loss: 0.038441. Entropy: 0.300987.\n",
      "Iteration 11364: Policy loss: -0.086401. Value loss: 0.025292. Entropy: 0.300422.\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11365: Policy loss: 0.096124. Value loss: 0.068108. Entropy: 0.309486.\n",
      "Iteration 11366: Policy loss: 0.091139. Value loss: 0.031822. Entropy: 0.308914.\n",
      "Iteration 11367: Policy loss: 0.086787. Value loss: 0.023426. Entropy: 0.310097.\n",
      "episode: 4199   score: 330.0  epsilon: 1.0    steps: 352  evaluation reward: 419.65\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11368: Policy loss: -0.282861. Value loss: 0.335579. Entropy: 0.304220.\n",
      "Iteration 11369: Policy loss: -0.291386. Value loss: 0.219348. Entropy: 0.305770.\n",
      "Iteration 11370: Policy loss: -0.314346. Value loss: 0.127555. Entropy: 0.304836.\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11371: Policy loss: 0.026607. Value loss: 0.125421. Entropy: 0.307443.\n",
      "Iteration 11372: Policy loss: 0.025757. Value loss: 0.061358. Entropy: 0.304445.\n",
      "Iteration 11373: Policy loss: 0.018830. Value loss: 0.046890. Entropy: 0.306152.\n",
      "episode: 4200   score: 215.0  epsilon: 1.0    steps: 248  evaluation reward: 415.6\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11374: Policy loss: 0.020026. Value loss: 0.117725. Entropy: 0.307523.\n",
      "Iteration 11375: Policy loss: 0.014288. Value loss: 0.042503. Entropy: 0.306650.\n",
      "Iteration 11376: Policy loss: -0.002385. Value loss: 0.026278. Entropy: 0.307294.\n",
      "now time :  2019-09-06 02:00:50.682462\n",
      "episode: 4201   score: 240.0  epsilon: 1.0    steps: 296  evaluation reward: 415.4\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11377: Policy loss: 0.070850. Value loss: 0.153455. Entropy: 0.309396.\n",
      "Iteration 11378: Policy loss: 0.063500. Value loss: 0.051530. Entropy: 0.309144.\n",
      "Iteration 11379: Policy loss: 0.050066. Value loss: 0.035239. Entropy: 0.308764.\n",
      "episode: 4202   score: 420.0  epsilon: 1.0    steps: 792  evaluation reward: 417.5\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11380: Policy loss: -0.055027. Value loss: 0.074283. Entropy: 0.308036.\n",
      "Iteration 11381: Policy loss: -0.061278. Value loss: 0.036684. Entropy: 0.308113.\n",
      "Iteration 11382: Policy loss: -0.060822. Value loss: 0.028050. Entropy: 0.307944.\n",
      "episode: 4203   score: 365.0  epsilon: 1.0    steps: 312  evaluation reward: 416.55\n",
      "episode: 4204   score: 335.0  epsilon: 1.0    steps: 400  evaluation reward: 418.65\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11383: Policy loss: -0.075243. Value loss: 0.092523. Entropy: 0.301176.\n",
      "Iteration 11384: Policy loss: -0.085434. Value loss: 0.037917. Entropy: 0.301277.\n",
      "Iteration 11385: Policy loss: -0.079445. Value loss: 0.026897. Entropy: 0.300585.\n",
      "episode: 4205   score: 505.0  epsilon: 1.0    steps: 568  evaluation reward: 421.6\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11386: Policy loss: -0.025727. Value loss: 0.082057. Entropy: 0.295603.\n",
      "Iteration 11387: Policy loss: -0.026980. Value loss: 0.037610. Entropy: 0.294081.\n",
      "Iteration 11388: Policy loss: -0.028063. Value loss: 0.027351. Entropy: 0.292980.\n",
      "episode: 4206   score: 510.0  epsilon: 1.0    steps: 256  evaluation reward: 424.6\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11389: Policy loss: -0.651428. Value loss: 0.464627. Entropy: 0.301495.\n",
      "Iteration 11390: Policy loss: -0.692409. Value loss: 0.272440. Entropy: 0.302160.\n",
      "Iteration 11391: Policy loss: -0.694515. Value loss: 0.142979. Entropy: 0.301218.\n",
      "episode: 4207   score: 325.0  epsilon: 1.0    steps: 192  evaluation reward: 425.75\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11392: Policy loss: 0.299826. Value loss: 0.164083. Entropy: 0.301675.\n",
      "Iteration 11393: Policy loss: 0.295956. Value loss: 0.047278. Entropy: 0.302824.\n",
      "Iteration 11394: Policy loss: 0.290053. Value loss: 0.033597. Entropy: 0.304157.\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11395: Policy loss: 0.204370. Value loss: 0.093777. Entropy: 0.299651.\n",
      "Iteration 11396: Policy loss: 0.198311. Value loss: 0.038988. Entropy: 0.302869.\n",
      "Iteration 11397: Policy loss: 0.190322. Value loss: 0.027753. Entropy: 0.302092.\n",
      "episode: 4208   score: 475.0  epsilon: 1.0    steps: 24  evaluation reward: 426.3\n",
      "episode: 4209   score: 590.0  epsilon: 1.0    steps: 800  evaluation reward: 423.05\n",
      "Training network. lr: 0.000163. clip: 0.065136\n",
      "Iteration 11398: Policy loss: -0.172074. Value loss: 0.087048. Entropy: 0.294765.\n",
      "Iteration 11399: Policy loss: -0.185560. Value loss: 0.037874. Entropy: 0.295204.\n",
      "Iteration 11400: Policy loss: -0.186966. Value loss: 0.032432. Entropy: 0.293787.\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11401: Policy loss: 0.186766. Value loss: 0.128313. Entropy: 0.303794.\n",
      "Iteration 11402: Policy loss: 0.181299. Value loss: 0.045316. Entropy: 0.305062.\n",
      "Iteration 11403: Policy loss: 0.172494. Value loss: 0.030983. Entropy: 0.304279.\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11404: Policy loss: 0.035838. Value loss: 0.242424. Entropy: 0.309629.\n",
      "Iteration 11405: Policy loss: 0.043265. Value loss: 0.061886. Entropy: 0.305623.\n",
      "Iteration 11406: Policy loss: 0.027781. Value loss: 0.035218. Entropy: 0.306303.\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11407: Policy loss: -0.277082. Value loss: 0.281482. Entropy: 0.316006.\n",
      "Iteration 11408: Policy loss: -0.288946. Value loss: 0.186002. Entropy: 0.316320.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11409: Policy loss: -0.280377. Value loss: 0.128698. Entropy: 0.316435.\n",
      "episode: 4210   score: 290.0  epsilon: 1.0    steps: 112  evaluation reward: 421.3\n",
      "episode: 4211   score: 545.0  epsilon: 1.0    steps: 168  evaluation reward: 424.05\n",
      "episode: 4212   score: 270.0  epsilon: 1.0    steps: 968  evaluation reward: 423.2\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11410: Policy loss: 0.184285. Value loss: 0.113294. Entropy: 0.273095.\n",
      "Iteration 11411: Policy loss: 0.171261. Value loss: 0.035506. Entropy: 0.275021.\n",
      "Iteration 11412: Policy loss: 0.172048. Value loss: 0.026265. Entropy: 0.274690.\n",
      "episode: 4213   score: 470.0  epsilon: 1.0    steps: 808  evaluation reward: 424.15\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11413: Policy loss: 0.165678. Value loss: 0.169119. Entropy: 0.291706.\n",
      "Iteration 11414: Policy loss: 0.156097. Value loss: 0.055265. Entropy: 0.291846.\n",
      "Iteration 11415: Policy loss: 0.150913. Value loss: 0.033216. Entropy: 0.291557.\n",
      "episode: 4214   score: 260.0  epsilon: 1.0    steps: 120  evaluation reward: 419.8\n",
      "episode: 4215   score: 555.0  epsilon: 1.0    steps: 408  evaluation reward: 420.2\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11416: Policy loss: 0.092588. Value loss: 0.104764. Entropy: 0.278826.\n",
      "Iteration 11417: Policy loss: 0.089138. Value loss: 0.045733. Entropy: 0.276768.\n",
      "Iteration 11418: Policy loss: 0.084734. Value loss: 0.033270. Entropy: 0.278600.\n",
      "episode: 4216   score: 260.0  epsilon: 1.0    steps: 896  evaluation reward: 416.9\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11419: Policy loss: 0.284623. Value loss: 0.148976. Entropy: 0.299502.\n",
      "Iteration 11420: Policy loss: 0.284403. Value loss: 0.068978. Entropy: 0.298727.\n",
      "Iteration 11421: Policy loss: 0.275915. Value loss: 0.053235. Entropy: 0.299555.\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11422: Policy loss: 0.074295. Value loss: 0.072625. Entropy: 0.308288.\n",
      "Iteration 11423: Policy loss: 0.063427. Value loss: 0.039575. Entropy: 0.309414.\n",
      "Iteration 11424: Policy loss: 0.062072. Value loss: 0.029549. Entropy: 0.308209.\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11425: Policy loss: -0.118641. Value loss: 0.272794. Entropy: 0.314637.\n",
      "Iteration 11426: Policy loss: -0.140139. Value loss: 0.216185. Entropy: 0.315487.\n",
      "Iteration 11427: Policy loss: -0.153295. Value loss: 0.160799. Entropy: 0.314234.\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11428: Policy loss: 0.105147. Value loss: 0.079837. Entropy: 0.309635.\n",
      "Iteration 11429: Policy loss: 0.101087. Value loss: 0.033826. Entropy: 0.309026.\n",
      "Iteration 11430: Policy loss: 0.098943. Value loss: 0.022444. Entropy: 0.309015.\n",
      "episode: 4217   score: 240.0  epsilon: 1.0    steps: 832  evaluation reward: 416.35\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11431: Policy loss: 0.114694. Value loss: 0.073434. Entropy: 0.303338.\n",
      "Iteration 11432: Policy loss: 0.119094. Value loss: 0.030165. Entropy: 0.302119.\n",
      "Iteration 11433: Policy loss: 0.112003. Value loss: 0.021414. Entropy: 0.302341.\n",
      "episode: 4218   score: 290.0  epsilon: 1.0    steps: 240  evaluation reward: 417.15\n",
      "episode: 4219   score: 420.0  epsilon: 1.0    steps: 280  evaluation reward: 415.4\n",
      "episode: 4220   score: 315.0  epsilon: 1.0    steps: 856  evaluation reward: 416.45\n",
      "episode: 4221   score: 285.0  epsilon: 1.0    steps: 1024  evaluation reward: 415.65\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11434: Policy loss: -0.070602. Value loss: 0.105309. Entropy: 0.276530.\n",
      "Iteration 11435: Policy loss: -0.073087. Value loss: 0.048721. Entropy: 0.278053.\n",
      "Iteration 11436: Policy loss: -0.074397. Value loss: 0.039773. Entropy: 0.276194.\n",
      "episode: 4222   score: 315.0  epsilon: 1.0    steps: 264  evaluation reward: 415.95\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11437: Policy loss: -0.132784. Value loss: 0.107922. Entropy: 0.282237.\n",
      "Iteration 11438: Policy loss: -0.139776. Value loss: 0.047931. Entropy: 0.282527.\n",
      "Iteration 11439: Policy loss: -0.148294. Value loss: 0.033238. Entropy: 0.285129.\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11440: Policy loss: 0.006035. Value loss: 0.156727. Entropy: 0.307480.\n",
      "Iteration 11441: Policy loss: -0.002669. Value loss: 0.075279. Entropy: 0.305735.\n",
      "Iteration 11442: Policy loss: -0.006585. Value loss: 0.048566. Entropy: 0.307381.\n",
      "episode: 4223   score: 495.0  epsilon: 1.0    steps: 72  evaluation reward: 415.3\n",
      "episode: 4224   score: 530.0  epsilon: 1.0    steps: 240  evaluation reward: 416.6\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11443: Policy loss: 0.071773. Value loss: 0.148064. Entropy: 0.289999.\n",
      "Iteration 11444: Policy loss: 0.055396. Value loss: 0.117584. Entropy: 0.290774.\n",
      "Iteration 11445: Policy loss: 0.057619. Value loss: 0.108838. Entropy: 0.293003.\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11446: Policy loss: 0.045703. Value loss: 0.086031. Entropy: 0.311387.\n",
      "Iteration 11447: Policy loss: 0.045139. Value loss: 0.036000. Entropy: 0.309751.\n",
      "Iteration 11448: Policy loss: 0.037411. Value loss: 0.024915. Entropy: 0.309315.\n",
      "Training network. lr: 0.000162. clip: 0.064988\n",
      "Iteration 11449: Policy loss: -0.643482. Value loss: 0.643565. Entropy: 0.308859.\n",
      "Iteration 11450: Policy loss: -0.653193. Value loss: 0.488516. Entropy: 0.310037.\n",
      "Iteration 11451: Policy loss: -0.662262. Value loss: 0.425746. Entropy: 0.308458.\n",
      "episode: 4225   score: 270.0  epsilon: 1.0    steps: 216  evaluation reward: 414.95\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11452: Policy loss: 0.024898. Value loss: 0.080069. Entropy: 0.304378.\n",
      "Iteration 11453: Policy loss: 0.024188. Value loss: 0.035933. Entropy: 0.305591.\n",
      "Iteration 11454: Policy loss: 0.022683. Value loss: 0.027132. Entropy: 0.305066.\n",
      "episode: 4226   score: 500.0  epsilon: 1.0    steps: 560  evaluation reward: 415.55\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11455: Policy loss: 0.004216. Value loss: 0.114622. Entropy: 0.297914.\n",
      "Iteration 11456: Policy loss: -0.005564. Value loss: 0.045053. Entropy: 0.297750.\n",
      "Iteration 11457: Policy loss: -0.013538. Value loss: 0.032324. Entropy: 0.298586.\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11458: Policy loss: -0.102847. Value loss: 0.103962. Entropy: 0.310800.\n",
      "Iteration 11459: Policy loss: -0.109368. Value loss: 0.043133. Entropy: 0.313700.\n",
      "Iteration 11460: Policy loss: -0.111862. Value loss: 0.030875. Entropy: 0.311260.\n",
      "episode: 4227   score: 420.0  epsilon: 1.0    steps: 800  evaluation reward: 414.8\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11461: Policy loss: -0.130145. Value loss: 0.289042. Entropy: 0.298785.\n",
      "Iteration 11462: Policy loss: -0.158415. Value loss: 0.224167. Entropy: 0.298553.\n",
      "Iteration 11463: Policy loss: -0.161130. Value loss: 0.189186. Entropy: 0.299196.\n",
      "episode: 4228   score: 360.0  epsilon: 1.0    steps: 448  evaluation reward: 415.5\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11464: Policy loss: 0.069861. Value loss: 0.076205. Entropy: 0.292680.\n",
      "Iteration 11465: Policy loss: 0.067739. Value loss: 0.036293. Entropy: 0.292142.\n",
      "Iteration 11466: Policy loss: 0.061659. Value loss: 0.028987. Entropy: 0.291708.\n",
      "episode: 4229   score: 675.0  epsilon: 1.0    steps: 320  evaluation reward: 419.4\n",
      "episode: 4230   score: 575.0  epsilon: 1.0    steps: 576  evaluation reward: 423.6\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11467: Policy loss: 0.035227. Value loss: 0.087037. Entropy: 0.280730.\n",
      "Iteration 11468: Policy loss: 0.025358. Value loss: 0.043035. Entropy: 0.280245.\n",
      "Iteration 11469: Policy loss: 0.024225. Value loss: 0.034276. Entropy: 0.281622.\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11470: Policy loss: 0.103035. Value loss: 0.108634. Entropy: 0.309368.\n",
      "Iteration 11471: Policy loss: 0.094973. Value loss: 0.042652. Entropy: 0.309044.\n",
      "Iteration 11472: Policy loss: 0.091260. Value loss: 0.032137. Entropy: 0.308010.\n",
      "episode: 4231   score: 595.0  epsilon: 1.0    steps: 224  evaluation reward: 426.4\n",
      "episode: 4232   score: 345.0  epsilon: 1.0    steps: 320  evaluation reward: 426.45\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11473: Policy loss: -0.032037. Value loss: 0.061511. Entropy: 0.287308.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11474: Policy loss: -0.039943. Value loss: 0.031146. Entropy: 0.285991.\n",
      "Iteration 11475: Policy loss: -0.044441. Value loss: 0.022713. Entropy: 0.285770.\n",
      "episode: 4233   score: 325.0  epsilon: 1.0    steps: 472  evaluation reward: 427.6\n",
      "episode: 4234   score: 365.0  epsilon: 1.0    steps: 608  evaluation reward: 428.55\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11476: Policy loss: -0.113839. Value loss: 0.322808. Entropy: 0.284714.\n",
      "Iteration 11477: Policy loss: -0.121468. Value loss: 0.249457. Entropy: 0.284128.\n",
      "Iteration 11478: Policy loss: -0.126334. Value loss: 0.217552. Entropy: 0.284975.\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11479: Policy loss: -0.008866. Value loss: 0.092846. Entropy: 0.312629.\n",
      "Iteration 11480: Policy loss: -0.017518. Value loss: 0.045110. Entropy: 0.308985.\n",
      "Iteration 11481: Policy loss: -0.014361. Value loss: 0.036834. Entropy: 0.308921.\n",
      "episode: 4235   score: 510.0  epsilon: 1.0    steps: 928  evaluation reward: 426.55\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11482: Policy loss: 0.019330. Value loss: 0.092659. Entropy: 0.311891.\n",
      "Iteration 11483: Policy loss: 0.011166. Value loss: 0.039013. Entropy: 0.311253.\n",
      "Iteration 11484: Policy loss: 0.005791. Value loss: 0.028854. Entropy: 0.311017.\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11485: Policy loss: -0.054601. Value loss: 0.067029. Entropy: 0.304280.\n",
      "Iteration 11486: Policy loss: -0.053469. Value loss: 0.037550. Entropy: 0.303824.\n",
      "Iteration 11487: Policy loss: -0.060973. Value loss: 0.029934. Entropy: 0.304065.\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11488: Policy loss: 0.039006. Value loss: 0.078286. Entropy: 0.313404.\n",
      "Iteration 11489: Policy loss: 0.027231. Value loss: 0.026611. Entropy: 0.312449.\n",
      "Iteration 11490: Policy loss: 0.028939. Value loss: 0.019995. Entropy: 0.313335.\n",
      "episode: 4236   score: 300.0  epsilon: 1.0    steps: 304  evaluation reward: 424.9\n",
      "episode: 4237   score: 410.0  epsilon: 1.0    steps: 912  evaluation reward: 422.0\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11491: Policy loss: -0.159947. Value loss: 0.281519. Entropy: 0.300563.\n",
      "Iteration 11492: Policy loss: -0.157248. Value loss: 0.163825. Entropy: 0.299107.\n",
      "Iteration 11493: Policy loss: -0.173730. Value loss: 0.122955. Entropy: 0.300975.\n",
      "episode: 4238   score: 285.0  epsilon: 1.0    steps: 504  evaluation reward: 418.6\n",
      "episode: 4239   score: 440.0  epsilon: 1.0    steps: 928  evaluation reward: 419.9\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11494: Policy loss: 0.012617. Value loss: 0.138656. Entropy: 0.287072.\n",
      "Iteration 11495: Policy loss: 0.007666. Value loss: 0.055705. Entropy: 0.284338.\n",
      "Iteration 11496: Policy loss: 0.001314. Value loss: 0.043765. Entropy: 0.284076.\n",
      "episode: 4240   score: 390.0  epsilon: 1.0    steps: 480  evaluation reward: 420.6\n",
      "episode: 4241   score: 350.0  epsilon: 1.0    steps: 896  evaluation reward: 418.05\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11497: Policy loss: 0.042177. Value loss: 0.098528. Entropy: 0.285297.\n",
      "Iteration 11498: Policy loss: 0.034798. Value loss: 0.048462. Entropy: 0.285618.\n",
      "Iteration 11499: Policy loss: 0.028889. Value loss: 0.037537. Entropy: 0.284606.\n",
      "Training network. lr: 0.000162. clip: 0.064832\n",
      "Iteration 11500: Policy loss: -0.076142. Value loss: 0.050460. Entropy: 0.308097.\n",
      "Iteration 11501: Policy loss: -0.079694. Value loss: 0.024629. Entropy: 0.309690.\n",
      "Iteration 11502: Policy loss: -0.081212. Value loss: 0.020599. Entropy: 0.309133.\n",
      "episode: 4242   score: 435.0  epsilon: 1.0    steps: 792  evaluation reward: 417.65\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11503: Policy loss: 0.317358. Value loss: 0.126445. Entropy: 0.297927.\n",
      "Iteration 11504: Policy loss: 0.314078. Value loss: 0.057464. Entropy: 0.296115.\n",
      "Iteration 11505: Policy loss: 0.303500. Value loss: 0.044424. Entropy: 0.294419.\n",
      "episode: 4243   score: 390.0  epsilon: 1.0    steps: 456  evaluation reward: 415.15\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11506: Policy loss: 0.167991. Value loss: 0.062591. Entropy: 0.299632.\n",
      "Iteration 11507: Policy loss: 0.156890. Value loss: 0.026926. Entropy: 0.299860.\n",
      "Iteration 11508: Policy loss: 0.153721. Value loss: 0.018673. Entropy: 0.298380.\n",
      "episode: 4244   score: 210.0  epsilon: 1.0    steps: 376  evaluation reward: 414.2\n",
      "episode: 4245   score: 230.0  epsilon: 1.0    steps: 416  evaluation reward: 411.1\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11509: Policy loss: -0.042593. Value loss: 0.097417. Entropy: 0.290954.\n",
      "Iteration 11510: Policy loss: -0.051384. Value loss: 0.040225. Entropy: 0.289187.\n",
      "Iteration 11511: Policy loss: -0.049950. Value loss: 0.029450. Entropy: 0.287699.\n",
      "episode: 4246   score: 225.0  epsilon: 1.0    steps: 1016  evaluation reward: 407.7\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11512: Policy loss: 0.145092. Value loss: 0.091188. Entropy: 0.310701.\n",
      "Iteration 11513: Policy loss: 0.131505. Value loss: 0.041737. Entropy: 0.311713.\n",
      "Iteration 11514: Policy loss: 0.131043. Value loss: 0.027393. Entropy: 0.311027.\n",
      "episode: 4247   score: 335.0  epsilon: 1.0    steps: 40  evaluation reward: 407.9\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11515: Policy loss: 0.076010. Value loss: 0.115619. Entropy: 0.288897.\n",
      "Iteration 11516: Policy loss: 0.067389. Value loss: 0.054217. Entropy: 0.289152.\n",
      "Iteration 11517: Policy loss: 0.061544. Value loss: 0.040793. Entropy: 0.288478.\n",
      "episode: 4248   score: 300.0  epsilon: 1.0    steps: 968  evaluation reward: 407.6\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11518: Policy loss: -0.023134. Value loss: 0.065476. Entropy: 0.313891.\n",
      "Iteration 11519: Policy loss: -0.026049. Value loss: 0.024505. Entropy: 0.313673.\n",
      "Iteration 11520: Policy loss: -0.033787. Value loss: 0.018668. Entropy: 0.314108.\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11521: Policy loss: 0.035218. Value loss: 0.078129. Entropy: 0.305803.\n",
      "Iteration 11522: Policy loss: 0.024304. Value loss: 0.034568. Entropy: 0.306951.\n",
      "Iteration 11523: Policy loss: 0.023201. Value loss: 0.021278. Entropy: 0.308111.\n",
      "episode: 4249   score: 210.0  epsilon: 1.0    steps: 72  evaluation reward: 404.9\n",
      "episode: 4250   score: 290.0  epsilon: 1.0    steps: 624  evaluation reward: 404.8\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11524: Policy loss: 0.218701. Value loss: 0.040440. Entropy: 0.286181.\n",
      "Iteration 11525: Policy loss: 0.214683. Value loss: 0.019494. Entropy: 0.285722.\n",
      "Iteration 11526: Policy loss: 0.211216. Value loss: 0.016225. Entropy: 0.286752.\n",
      "now time :  2019-09-06 02:10:11.202778\n",
      "episode: 4251   score: 450.0  epsilon: 1.0    steps: 992  evaluation reward: 403.3\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11527: Policy loss: -0.114093. Value loss: 0.097922. Entropy: 0.313314.\n",
      "Iteration 11528: Policy loss: -0.115405. Value loss: 0.038671. Entropy: 0.313107.\n",
      "Iteration 11529: Policy loss: -0.117848. Value loss: 0.025794. Entropy: 0.313605.\n",
      "episode: 4252   score: 275.0  epsilon: 1.0    steps: 464  evaluation reward: 402.15\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11530: Policy loss: -0.365358. Value loss: 0.359022. Entropy: 0.293913.\n",
      "Iteration 11531: Policy loss: -0.381346. Value loss: 0.190134. Entropy: 0.294958.\n",
      "Iteration 11532: Policy loss: -0.367920. Value loss: 0.082118. Entropy: 0.293143.\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11533: Policy loss: 0.177276. Value loss: 0.131197. Entropy: 0.311066.\n",
      "Iteration 11534: Policy loss: 0.166505. Value loss: 0.047609. Entropy: 0.310768.\n",
      "Iteration 11535: Policy loss: 0.164652. Value loss: 0.038711. Entropy: 0.312247.\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11536: Policy loss: -0.412635. Value loss: 0.185611. Entropy: 0.308001.\n",
      "Iteration 11537: Policy loss: -0.433474. Value loss: 0.055055. Entropy: 0.309704.\n",
      "Iteration 11538: Policy loss: -0.437531. Value loss: 0.024383. Entropy: 0.308693.\n",
      "episode: 4253   score: 450.0  epsilon: 1.0    steps: 304  evaluation reward: 404.55\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11539: Policy loss: -0.125869. Value loss: 0.207167. Entropy: 0.300139.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11540: Policy loss: -0.128501. Value loss: 0.077924. Entropy: 0.300782.\n",
      "Iteration 11541: Policy loss: -0.132168. Value loss: 0.038803. Entropy: 0.300574.\n",
      "episode: 4254   score: 620.0  epsilon: 1.0    steps: 736  evaluation reward: 405.8\n",
      "episode: 4255   score: 525.0  epsilon: 1.0    steps: 736  evaluation reward: 407.0\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11542: Policy loss: 0.189789. Value loss: 0.125496. Entropy: 0.288240.\n",
      "Iteration 11543: Policy loss: 0.181204. Value loss: 0.046931. Entropy: 0.287039.\n",
      "Iteration 11544: Policy loss: 0.175928. Value loss: 0.033563. Entropy: 0.287041.\n",
      "episode: 4256   score: 425.0  epsilon: 1.0    steps: 392  evaluation reward: 408.4\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11545: Policy loss: -0.028822. Value loss: 0.143886. Entropy: 0.298241.\n",
      "Iteration 11546: Policy loss: -0.031788. Value loss: 0.053026. Entropy: 0.298820.\n",
      "Iteration 11547: Policy loss: -0.039958. Value loss: 0.037546. Entropy: 0.297500.\n",
      "episode: 4257   score: 515.0  epsilon: 1.0    steps: 224  evaluation reward: 407.85\n",
      "episode: 4258   score: 470.0  epsilon: 1.0    steps: 424  evaluation reward: 403.85\n",
      "episode: 4259   score: 225.0  epsilon: 1.0    steps: 448  evaluation reward: 402.35\n",
      "Training network. lr: 0.000162. clip: 0.064675\n",
      "Iteration 11548: Policy loss: -0.080818. Value loss: 0.143468. Entropy: 0.281767.\n",
      "Iteration 11549: Policy loss: -0.090879. Value loss: 0.063509. Entropy: 0.281978.\n",
      "Iteration 11550: Policy loss: -0.093733. Value loss: 0.046369. Entropy: 0.283003.\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11551: Policy loss: 0.279850. Value loss: 0.086569. Entropy: 0.310281.\n",
      "Iteration 11552: Policy loss: 0.270163. Value loss: 0.035736. Entropy: 0.308999.\n",
      "Iteration 11553: Policy loss: 0.264448. Value loss: 0.029222. Entropy: 0.307643.\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11554: Policy loss: 0.161536. Value loss: 0.078490. Entropy: 0.311717.\n",
      "Iteration 11555: Policy loss: 0.154466. Value loss: 0.039709. Entropy: 0.312936.\n",
      "Iteration 11556: Policy loss: 0.150579. Value loss: 0.028815. Entropy: 0.311071.\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11557: Policy loss: -0.141564. Value loss: 0.326308. Entropy: 0.313341.\n",
      "Iteration 11558: Policy loss: -0.145658. Value loss: 0.176382. Entropy: 0.312300.\n",
      "Iteration 11559: Policy loss: -0.162093. Value loss: 0.135133. Entropy: 0.311342.\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11560: Policy loss: 0.054367. Value loss: 0.225930. Entropy: 0.310049.\n",
      "Iteration 11561: Policy loss: 0.050575. Value loss: 0.078288. Entropy: 0.310841.\n",
      "Iteration 11562: Policy loss: 0.036179. Value loss: 0.051021. Entropy: 0.310588.\n",
      "episode: 4260   score: 590.0  epsilon: 1.0    steps: 312  evaluation reward: 400.95\n",
      "episode: 4261   score: 930.0  epsilon: 1.0    steps: 512  evaluation reward: 406.8\n",
      "episode: 4262   score: 285.0  epsilon: 1.0    steps: 872  evaluation reward: 407.05\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11563: Policy loss: 0.238832. Value loss: 0.127855. Entropy: 0.282688.\n",
      "Iteration 11564: Policy loss: 0.233054. Value loss: 0.049690. Entropy: 0.282045.\n",
      "Iteration 11565: Policy loss: 0.224975. Value loss: 0.033821. Entropy: 0.281176.\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11566: Policy loss: 0.000113. Value loss: 0.091812. Entropy: 0.311422.\n",
      "Iteration 11567: Policy loss: -0.008090. Value loss: 0.047440. Entropy: 0.311386.\n",
      "Iteration 11568: Policy loss: -0.010891. Value loss: 0.031843. Entropy: 0.310768.\n",
      "episode: 4263   score: 300.0  epsilon: 1.0    steps: 136  evaluation reward: 404.2\n",
      "episode: 4264   score: 265.0  epsilon: 1.0    steps: 840  evaluation reward: 401.4\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11569: Policy loss: 0.097832. Value loss: 0.079931. Entropy: 0.295055.\n",
      "Iteration 11570: Policy loss: 0.088314. Value loss: 0.038700. Entropy: 0.294266.\n",
      "Iteration 11571: Policy loss: 0.090242. Value loss: 0.031481. Entropy: 0.293618.\n",
      "episode: 4265   score: 360.0  epsilon: 1.0    steps: 896  evaluation reward: 402.4\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11572: Policy loss: 0.129364. Value loss: 0.059431. Entropy: 0.301722.\n",
      "Iteration 11573: Policy loss: 0.125193. Value loss: 0.027049. Entropy: 0.300648.\n",
      "Iteration 11574: Policy loss: 0.120223. Value loss: 0.019553. Entropy: 0.302469.\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11575: Policy loss: -0.124155. Value loss: 0.249057. Entropy: 0.302943.\n",
      "Iteration 11576: Policy loss: -0.110796. Value loss: 0.074468. Entropy: 0.303123.\n",
      "Iteration 11577: Policy loss: -0.138354. Value loss: 0.035904. Entropy: 0.301009.\n",
      "episode: 4266   score: 590.0  epsilon: 1.0    steps: 96  evaluation reward: 402.8\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11578: Policy loss: -0.149978. Value loss: 0.122565. Entropy: 0.299761.\n",
      "Iteration 11579: Policy loss: -0.156488. Value loss: 0.057230. Entropy: 0.296929.\n",
      "Iteration 11580: Policy loss: -0.158297. Value loss: 0.042152. Entropy: 0.297851.\n",
      "episode: 4267   score: 495.0  epsilon: 1.0    steps: 264  evaluation reward: 404.3\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11581: Policy loss: -0.524702. Value loss: 0.300738. Entropy: 0.298960.\n",
      "Iteration 11582: Policy loss: -0.537760. Value loss: 0.102798. Entropy: 0.299354.\n",
      "Iteration 11583: Policy loss: -0.550036. Value loss: 0.068137. Entropy: 0.297721.\n",
      "episode: 4268   score: 330.0  epsilon: 1.0    steps: 1024  evaluation reward: 404.7\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11584: Policy loss: 0.050370. Value loss: 0.101905. Entropy: 0.311745.\n",
      "Iteration 11585: Policy loss: 0.040494. Value loss: 0.040256. Entropy: 0.310986.\n",
      "Iteration 11586: Policy loss: 0.039579. Value loss: 0.026976. Entropy: 0.310856.\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11587: Policy loss: 0.029852. Value loss: 0.229282. Entropy: 0.302789.\n",
      "Iteration 11588: Policy loss: 0.010341. Value loss: 0.058746. Entropy: 0.301127.\n",
      "Iteration 11589: Policy loss: 0.012232. Value loss: 0.034505. Entropy: 0.300786.\n",
      "episode: 4269   score: 285.0  epsilon: 1.0    steps: 200  evaluation reward: 403.95\n",
      "episode: 4270   score: 320.0  epsilon: 1.0    steps: 880  evaluation reward: 405.05\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11590: Policy loss: 0.553128. Value loss: 0.165532. Entropy: 0.289841.\n",
      "Iteration 11591: Policy loss: 0.535162. Value loss: 0.052440. Entropy: 0.287522.\n",
      "Iteration 11592: Policy loss: 0.534497. Value loss: 0.038941. Entropy: 0.287764.\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11593: Policy loss: -0.201369. Value loss: 0.224838. Entropy: 0.305386.\n",
      "Iteration 11594: Policy loss: -0.193226. Value loss: 0.069408. Entropy: 0.304389.\n",
      "Iteration 11595: Policy loss: -0.198187. Value loss: 0.041344. Entropy: 0.304362.\n",
      "episode: 4271   score: 460.0  epsilon: 1.0    steps: 896  evaluation reward: 403.05\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11596: Policy loss: 0.075197. Value loss: 0.121130. Entropy: 0.307149.\n",
      "Iteration 11597: Policy loss: 0.066721. Value loss: 0.043924. Entropy: 0.305676.\n",
      "Iteration 11598: Policy loss: 0.060730. Value loss: 0.032064. Entropy: 0.306645.\n",
      "Training network. lr: 0.000161. clip: 0.064528\n",
      "Iteration 11599: Policy loss: 0.148596. Value loss: 0.230005. Entropy: 0.303737.\n",
      "Iteration 11600: Policy loss: 0.134602. Value loss: 0.048964. Entropy: 0.304207.\n",
      "Iteration 11601: Policy loss: 0.124236. Value loss: 0.028746. Entropy: 0.305623.\n",
      "episode: 4272   score: 450.0  epsilon: 1.0    steps: 208  evaluation reward: 402.85\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11602: Policy loss: 0.412261. Value loss: 0.137819. Entropy: 0.301866.\n",
      "Iteration 11603: Policy loss: 0.401323. Value loss: 0.029937. Entropy: 0.300601.\n",
      "Iteration 11604: Policy loss: 0.395473. Value loss: 0.020911. Entropy: 0.299880.\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11605: Policy loss: 0.175329. Value loss: 0.087299. Entropy: 0.306053.\n",
      "Iteration 11606: Policy loss: 0.171555. Value loss: 0.033509. Entropy: 0.305363.\n",
      "Iteration 11607: Policy loss: 0.166731. Value loss: 0.024073. Entropy: 0.304884.\n",
      "episode: 4273   score: 755.0  epsilon: 1.0    steps: 304  evaluation reward: 403.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11608: Policy loss: 0.164654. Value loss: 0.065460. Entropy: 0.301987.\n",
      "Iteration 11609: Policy loss: 0.160265. Value loss: 0.032241. Entropy: 0.301950.\n",
      "Iteration 11610: Policy loss: 0.157027. Value loss: 0.028357. Entropy: 0.302252.\n",
      "episode: 4274   score: 335.0  epsilon: 1.0    steps: 256  evaluation reward: 403.85\n",
      "episode: 4275   score: 260.0  epsilon: 1.0    steps: 664  evaluation reward: 403.15\n",
      "episode: 4276   score: 425.0  epsilon: 1.0    steps: 744  evaluation reward: 400.9\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11611: Policy loss: 0.066132. Value loss: 0.075687. Entropy: 0.276817.\n",
      "Iteration 11612: Policy loss: 0.059503. Value loss: 0.038406. Entropy: 0.276753.\n",
      "Iteration 11613: Policy loss: 0.060366. Value loss: 0.030368. Entropy: 0.277854.\n",
      "episode: 4277   score: 655.0  epsilon: 1.0    steps: 768  evaluation reward: 402.8\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11614: Policy loss: 0.192952. Value loss: 0.067819. Entropy: 0.296635.\n",
      "Iteration 11615: Policy loss: 0.187687. Value loss: 0.027050. Entropy: 0.294839.\n",
      "Iteration 11616: Policy loss: 0.183734. Value loss: 0.020669. Entropy: 0.295367.\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11617: Policy loss: -0.159316. Value loss: 0.321507. Entropy: 0.306459.\n",
      "Iteration 11618: Policy loss: -0.165925. Value loss: 0.161359. Entropy: 0.305821.\n",
      "Iteration 11619: Policy loss: -0.170205. Value loss: 0.111662. Entropy: 0.307326.\n",
      "episode: 4278   score: 475.0  epsilon: 1.0    steps: 80  evaluation reward: 401.65\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11620: Policy loss: 0.033564. Value loss: 0.186756. Entropy: 0.299882.\n",
      "Iteration 11621: Policy loss: 0.030719. Value loss: 0.066668. Entropy: 0.300759.\n",
      "Iteration 11622: Policy loss: 0.017130. Value loss: 0.048976. Entropy: 0.301781.\n",
      "episode: 4279   score: 240.0  epsilon: 1.0    steps: 904  evaluation reward: 398.4\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11623: Policy loss: -0.051679. Value loss: 0.313145. Entropy: 0.298095.\n",
      "Iteration 11624: Policy loss: -0.076064. Value loss: 0.136888. Entropy: 0.298267.\n",
      "Iteration 11625: Policy loss: -0.069327. Value loss: 0.083208. Entropy: 0.297164.\n",
      "episode: 4280   score: 495.0  epsilon: 1.0    steps: 480  evaluation reward: 397.95\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11626: Policy loss: 0.238726. Value loss: 0.098359. Entropy: 0.303023.\n",
      "Iteration 11627: Policy loss: 0.225489. Value loss: 0.031991. Entropy: 0.304191.\n",
      "Iteration 11628: Policy loss: 0.228998. Value loss: 0.021842. Entropy: 0.302610.\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11629: Policy loss: -0.067807. Value loss: 0.159164. Entropy: 0.306967.\n",
      "Iteration 11630: Policy loss: -0.078219. Value loss: 0.107843. Entropy: 0.307708.\n",
      "Iteration 11631: Policy loss: -0.083662. Value loss: 0.093725. Entropy: 0.306819.\n",
      "episode: 4281   score: 240.0  epsilon: 1.0    steps: 104  evaluation reward: 397.6\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11632: Policy loss: 0.019924. Value loss: 0.157910. Entropy: 0.304136.\n",
      "Iteration 11633: Policy loss: 0.015937. Value loss: 0.061387. Entropy: 0.303426.\n",
      "Iteration 11634: Policy loss: 0.013324. Value loss: 0.036967. Entropy: 0.304042.\n",
      "episode: 4282   score: 465.0  epsilon: 1.0    steps: 96  evaluation reward: 400.15\n",
      "episode: 4283   score: 300.0  epsilon: 1.0    steps: 248  evaluation reward: 398.45\n",
      "episode: 4284   score: 180.0  epsilon: 1.0    steps: 1016  evaluation reward: 392.25\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11635: Policy loss: -0.452556. Value loss: 0.353753. Entropy: 0.291790.\n",
      "Iteration 11636: Policy loss: -0.460958. Value loss: 0.183960. Entropy: 0.289990.\n",
      "Iteration 11637: Policy loss: -0.474644. Value loss: 0.092201. Entropy: 0.292124.\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11638: Policy loss: 0.108627. Value loss: 0.132875. Entropy: 0.300610.\n",
      "Iteration 11639: Policy loss: 0.095958. Value loss: 0.050909. Entropy: 0.298823.\n",
      "Iteration 11640: Policy loss: 0.098863. Value loss: 0.032912. Entropy: 0.298412.\n",
      "episode: 4285   score: 590.0  epsilon: 1.0    steps: 704  evaluation reward: 394.9\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11641: Policy loss: 0.205427. Value loss: 0.103582. Entropy: 0.298091.\n",
      "Iteration 11642: Policy loss: 0.202794. Value loss: 0.048446. Entropy: 0.297107.\n",
      "Iteration 11643: Policy loss: 0.206125. Value loss: 0.036542. Entropy: 0.297491.\n",
      "episode: 4286   score: 125.0  epsilon: 1.0    steps: 240  evaluation reward: 390.55\n",
      "episode: 4287   score: 395.0  epsilon: 1.0    steps: 432  evaluation reward: 390.3\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11644: Policy loss: -0.014103. Value loss: 0.062798. Entropy: 0.285297.\n",
      "Iteration 11645: Policy loss: -0.013889. Value loss: 0.042607. Entropy: 0.284501.\n",
      "Iteration 11646: Policy loss: -0.016492. Value loss: 0.033129. Entropy: 0.282529.\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11647: Policy loss: -0.163886. Value loss: 0.285585. Entropy: 0.305788.\n",
      "Iteration 11648: Policy loss: -0.190727. Value loss: 0.103012. Entropy: 0.305129.\n",
      "Iteration 11649: Policy loss: -0.198462. Value loss: 0.055947. Entropy: 0.304976.\n",
      "Training network. lr: 0.000161. clip: 0.064371\n",
      "Iteration 11650: Policy loss: 0.104862. Value loss: 0.078217. Entropy: 0.308856.\n",
      "Iteration 11651: Policy loss: 0.093376. Value loss: 0.044611. Entropy: 0.308921.\n",
      "Iteration 11652: Policy loss: 0.093576. Value loss: 0.029416. Entropy: 0.308029.\n",
      "episode: 4288   score: 955.0  epsilon: 1.0    steps: 80  evaluation reward: 397.15\n",
      "episode: 4289   score: 260.0  epsilon: 1.0    steps: 584  evaluation reward: 397.0\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11653: Policy loss: 0.200072. Value loss: 0.106763. Entropy: 0.295883.\n",
      "Iteration 11654: Policy loss: 0.186492. Value loss: 0.037290. Entropy: 0.296806.\n",
      "Iteration 11655: Policy loss: 0.182059. Value loss: 0.021262. Entropy: 0.295876.\n",
      "episode: 4290   score: 210.0  epsilon: 1.0    steps: 224  evaluation reward: 396.55\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11656: Policy loss: 0.045138. Value loss: 0.065918. Entropy: 0.300712.\n",
      "Iteration 11657: Policy loss: 0.045775. Value loss: 0.027710. Entropy: 0.301841.\n",
      "Iteration 11658: Policy loss: 0.039433. Value loss: 0.018714. Entropy: 0.300974.\n",
      "episode: 4291   score: 495.0  epsilon: 1.0    steps: 1008  evaluation reward: 396.65\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11659: Policy loss: 0.096292. Value loss: 0.070929. Entropy: 0.303237.\n",
      "Iteration 11660: Policy loss: 0.089602. Value loss: 0.032823. Entropy: 0.303396.\n",
      "Iteration 11661: Policy loss: 0.087754. Value loss: 0.026920. Entropy: 0.304223.\n",
      "episode: 4292   score: 210.0  epsilon: 1.0    steps: 296  evaluation reward: 395.95\n",
      "episode: 4293   score: 225.0  epsilon: 1.0    steps: 800  evaluation reward: 395.25\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11662: Policy loss: 0.121039. Value loss: 0.114395. Entropy: 0.293989.\n",
      "Iteration 11663: Policy loss: 0.112828. Value loss: 0.043593. Entropy: 0.296714.\n",
      "Iteration 11664: Policy loss: 0.113682. Value loss: 0.030975. Entropy: 0.295412.\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11665: Policy loss: 0.318905. Value loss: 0.106508. Entropy: 0.302585.\n",
      "Iteration 11666: Policy loss: 0.307681. Value loss: 0.044321. Entropy: 0.303112.\n",
      "Iteration 11667: Policy loss: 0.307483. Value loss: 0.029192. Entropy: 0.303415.\n",
      "episode: 4294   score: 440.0  epsilon: 1.0    steps: 792  evaluation reward: 395.75\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11668: Policy loss: -0.329559. Value loss: 0.349778. Entropy: 0.303094.\n",
      "Iteration 11669: Policy loss: -0.334727. Value loss: 0.242332. Entropy: 0.303910.\n",
      "Iteration 11670: Policy loss: -0.358588. Value loss: 0.212774. Entropy: 0.303894.\n",
      "episode: 4295   score: 315.0  epsilon: 1.0    steps: 80  evaluation reward: 395.6\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11671: Policy loss: -0.184737. Value loss: 0.090332. Entropy: 0.309268.\n",
      "Iteration 11672: Policy loss: -0.185143. Value loss: 0.034557. Entropy: 0.309518.\n",
      "Iteration 11673: Policy loss: -0.188611. Value loss: 0.021644. Entropy: 0.309402.\n",
      "episode: 4296   score: 180.0  epsilon: 1.0    steps: 72  evaluation reward: 391.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4297   score: 285.0  epsilon: 1.0    steps: 392  evaluation reward: 389.95\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11674: Policy loss: -0.126729. Value loss: 0.117256. Entropy: 0.303164.\n",
      "Iteration 11675: Policy loss: -0.123663. Value loss: 0.053949. Entropy: 0.303212.\n",
      "Iteration 11676: Policy loss: -0.132778. Value loss: 0.039133. Entropy: 0.302639.\n",
      "episode: 4298   score: 575.0  epsilon: 1.0    steps: 88  evaluation reward: 392.9\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11677: Policy loss: -0.100782. Value loss: 0.112642. Entropy: 0.299693.\n",
      "Iteration 11678: Policy loss: -0.108414. Value loss: 0.037459. Entropy: 0.300264.\n",
      "Iteration 11679: Policy loss: -0.108943. Value loss: 0.023605. Entropy: 0.301218.\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11680: Policy loss: 0.142553. Value loss: 0.111515. Entropy: 0.308845.\n",
      "Iteration 11681: Policy loss: 0.138692. Value loss: 0.033073. Entropy: 0.308540.\n",
      "Iteration 11682: Policy loss: 0.130347. Value loss: 0.024523. Entropy: 0.307445.\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11683: Policy loss: -0.027551. Value loss: 0.104275. Entropy: 0.309230.\n",
      "Iteration 11684: Policy loss: -0.032625. Value loss: 0.027709. Entropy: 0.308591.\n",
      "Iteration 11685: Policy loss: -0.042161. Value loss: 0.018716. Entropy: 0.308078.\n",
      "episode: 4299   score: 235.0  epsilon: 1.0    steps: 24  evaluation reward: 391.95\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11686: Policy loss: -0.013916. Value loss: 0.104555. Entropy: 0.304324.\n",
      "Iteration 11687: Policy loss: -0.006443. Value loss: 0.030697. Entropy: 0.303957.\n",
      "Iteration 11688: Policy loss: -0.008179. Value loss: 0.021837. Entropy: 0.303860.\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11689: Policy loss: 0.364871. Value loss: 0.161549. Entropy: 0.310580.\n",
      "Iteration 11690: Policy loss: 0.358415. Value loss: 0.071634. Entropy: 0.310014.\n",
      "Iteration 11691: Policy loss: 0.350142. Value loss: 0.052566. Entropy: 0.309830.\n",
      "episode: 4300   score: 505.0  epsilon: 1.0    steps: 136  evaluation reward: 394.85\n",
      "now time :  2019-09-06 02:20:26.727480\n",
      "episode: 4301   score: 260.0  epsilon: 1.0    steps: 816  evaluation reward: 395.05\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11692: Policy loss: 0.175788. Value loss: 0.086734. Entropy: 0.300364.\n",
      "Iteration 11693: Policy loss: 0.161373. Value loss: 0.042618. Entropy: 0.299596.\n",
      "Iteration 11694: Policy loss: 0.160044. Value loss: 0.030557. Entropy: 0.299212.\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11695: Policy loss: 0.007255. Value loss: 0.093228. Entropy: 0.303592.\n",
      "Iteration 11696: Policy loss: 0.006768. Value loss: 0.047185. Entropy: 0.301655.\n",
      "Iteration 11697: Policy loss: 0.004155. Value loss: 0.033592. Entropy: 0.302904.\n",
      "episode: 4302   score: 525.0  epsilon: 1.0    steps: 208  evaluation reward: 396.1\n",
      "episode: 4303   score: 390.0  epsilon: 1.0    steps: 616  evaluation reward: 396.35\n",
      "episode: 4304   score: 335.0  epsilon: 1.0    steps: 784  evaluation reward: 396.35\n",
      "Training network. lr: 0.000161. clip: 0.064214\n",
      "Iteration 11698: Policy loss: -0.361340. Value loss: 0.268603. Entropy: 0.295693.\n",
      "Iteration 11699: Policy loss: -0.364265. Value loss: 0.087251. Entropy: 0.293607.\n",
      "Iteration 11700: Policy loss: -0.387628. Value loss: 0.047878. Entropy: 0.295972.\n",
      "episode: 4305   score: 390.0  epsilon: 1.0    steps: 472  evaluation reward: 395.2\n",
      "episode: 4306   score: 275.0  epsilon: 1.0    steps: 904  evaluation reward: 392.85\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11701: Policy loss: -0.014525. Value loss: 0.095297. Entropy: 0.295019.\n",
      "Iteration 11702: Policy loss: -0.010735. Value loss: 0.049964. Entropy: 0.297505.\n",
      "Iteration 11703: Policy loss: -0.017435. Value loss: 0.038899. Entropy: 0.296856.\n",
      "episode: 4307   score: 710.0  epsilon: 1.0    steps: 1024  evaluation reward: 396.7\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11704: Policy loss: 0.260767. Value loss: 0.116454. Entropy: 0.305503.\n",
      "Iteration 11705: Policy loss: 0.250803. Value loss: 0.049349. Entropy: 0.305524.\n",
      "Iteration 11706: Policy loss: 0.257305. Value loss: 0.038668. Entropy: 0.306480.\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11707: Policy loss: -0.175137. Value loss: 0.095315. Entropy: 0.305240.\n",
      "Iteration 11708: Policy loss: -0.178289. Value loss: 0.040087. Entropy: 0.306314.\n",
      "Iteration 11709: Policy loss: -0.177033. Value loss: 0.028552. Entropy: 0.305421.\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11710: Policy loss: 0.213808. Value loss: 0.088324. Entropy: 0.305031.\n",
      "Iteration 11711: Policy loss: 0.206316. Value loss: 0.043651. Entropy: 0.304162.\n",
      "Iteration 11712: Policy loss: 0.198214. Value loss: 0.032006. Entropy: 0.303600.\n",
      "episode: 4308   score: 335.0  epsilon: 1.0    steps: 1016  evaluation reward: 395.3\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11713: Policy loss: 0.090821. Value loss: 0.104653. Entropy: 0.304962.\n",
      "Iteration 11714: Policy loss: 0.084850. Value loss: 0.044570. Entropy: 0.305463.\n",
      "Iteration 11715: Policy loss: 0.084003. Value loss: 0.032148. Entropy: 0.304645.\n",
      "episode: 4309   score: 210.0  epsilon: 1.0    steps: 80  evaluation reward: 391.5\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11716: Policy loss: -0.009594. Value loss: 0.060367. Entropy: 0.307297.\n",
      "Iteration 11717: Policy loss: -0.017704. Value loss: 0.030295. Entropy: 0.307456.\n",
      "Iteration 11718: Policy loss: -0.017783. Value loss: 0.021500. Entropy: 0.307894.\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11719: Policy loss: 0.270524. Value loss: 0.108101. Entropy: 0.298274.\n",
      "Iteration 11720: Policy loss: 0.268090. Value loss: 0.044148. Entropy: 0.300206.\n",
      "Iteration 11721: Policy loss: 0.262341. Value loss: 0.028790. Entropy: 0.299855.\n",
      "episode: 4310   score: 240.0  epsilon: 1.0    steps: 56  evaluation reward: 391.0\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11722: Policy loss: 0.128785. Value loss: 0.099554. Entropy: 0.305422.\n",
      "Iteration 11723: Policy loss: 0.122816. Value loss: 0.034846. Entropy: 0.305999.\n",
      "Iteration 11724: Policy loss: 0.116273. Value loss: 0.023233. Entropy: 0.305145.\n",
      "episode: 4311   score: 395.0  epsilon: 1.0    steps: 320  evaluation reward: 389.5\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11725: Policy loss: -0.015988. Value loss: 0.073147. Entropy: 0.306864.\n",
      "Iteration 11726: Policy loss: -0.022435. Value loss: 0.039159. Entropy: 0.303837.\n",
      "Iteration 11727: Policy loss: -0.019732. Value loss: 0.030248. Entropy: 0.304984.\n",
      "episode: 4312   score: 495.0  epsilon: 1.0    steps: 456  evaluation reward: 391.75\n",
      "episode: 4313   score: 440.0  epsilon: 1.0    steps: 456  evaluation reward: 391.45\n",
      "episode: 4314   score: 380.0  epsilon: 1.0    steps: 672  evaluation reward: 392.65\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11728: Policy loss: 0.092784. Value loss: 0.076516. Entropy: 0.301063.\n",
      "Iteration 11729: Policy loss: 0.089249. Value loss: 0.034003. Entropy: 0.299658.\n",
      "Iteration 11730: Policy loss: 0.082955. Value loss: 0.028670. Entropy: 0.298362.\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11731: Policy loss: -0.204673. Value loss: 0.328672. Entropy: 0.298236.\n",
      "Iteration 11732: Policy loss: -0.213625. Value loss: 0.199815. Entropy: 0.300899.\n",
      "Iteration 11733: Policy loss: -0.217476. Value loss: 0.123617. Entropy: 0.300129.\n",
      "episode: 4315   score: 435.0  epsilon: 1.0    steps: 144  evaluation reward: 391.45\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11734: Policy loss: -0.211494. Value loss: 0.280893. Entropy: 0.301798.\n",
      "Iteration 11735: Policy loss: -0.226174. Value loss: 0.112191. Entropy: 0.302394.\n",
      "Iteration 11736: Policy loss: -0.222712. Value loss: 0.048802. Entropy: 0.302179.\n",
      "episode: 4316   score: 260.0  epsilon: 1.0    steps: 520  evaluation reward: 391.45\n",
      "episode: 4317   score: 440.0  epsilon: 1.0    steps: 752  evaluation reward: 393.45\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11737: Policy loss: -0.096633. Value loss: 0.125642. Entropy: 0.298406.\n",
      "Iteration 11738: Policy loss: -0.099783. Value loss: 0.039538. Entropy: 0.296225.\n",
      "Iteration 11739: Policy loss: -0.112767. Value loss: 0.026858. Entropy: 0.293789.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11740: Policy loss: 0.334937. Value loss: 0.138419. Entropy: 0.312473.\n",
      "Iteration 11741: Policy loss: 0.333741. Value loss: 0.047013. Entropy: 0.312925.\n",
      "Iteration 11742: Policy loss: 0.317563. Value loss: 0.030922. Entropy: 0.312297.\n",
      "episode: 4318   score: 360.0  epsilon: 1.0    steps: 632  evaluation reward: 394.15\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11743: Policy loss: 0.098329. Value loss: 0.069012. Entropy: 0.305269.\n",
      "Iteration 11744: Policy loss: 0.097549. Value loss: 0.033493. Entropy: 0.304336.\n",
      "Iteration 11745: Policy loss: 0.088906. Value loss: 0.026384. Entropy: 0.304285.\n",
      "episode: 4319   score: 240.0  epsilon: 1.0    steps: 608  evaluation reward: 392.35\n",
      "episode: 4320   score: 210.0  epsilon: 1.0    steps: 800  evaluation reward: 391.3\n",
      "episode: 4321   score: 470.0  epsilon: 1.0    steps: 920  evaluation reward: 393.15\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11746: Policy loss: 0.077421. Value loss: 0.115384. Entropy: 0.292018.\n",
      "Iteration 11747: Policy loss: 0.079053. Value loss: 0.045370. Entropy: 0.289558.\n",
      "Iteration 11748: Policy loss: 0.067954. Value loss: 0.032266. Entropy: 0.289329.\n",
      "episode: 4322   score: 270.0  epsilon: 1.0    steps: 976  evaluation reward: 392.7\n",
      "Training network. lr: 0.000160. clip: 0.064067\n",
      "Iteration 11749: Policy loss: -0.109741. Value loss: 0.261076. Entropy: 0.305683.\n",
      "Iteration 11750: Policy loss: -0.104421. Value loss: 0.105697. Entropy: 0.306910.\n",
      "Iteration 11751: Policy loss: -0.120452. Value loss: 0.078617. Entropy: 0.306136.\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11752: Policy loss: -0.036386. Value loss: 0.100881. Entropy: 0.305648.\n",
      "Iteration 11753: Policy loss: -0.046415. Value loss: 0.047200. Entropy: 0.305403.\n",
      "Iteration 11754: Policy loss: -0.044305. Value loss: 0.035156. Entropy: 0.304684.\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11755: Policy loss: 0.107554. Value loss: 0.105330. Entropy: 0.306653.\n",
      "Iteration 11756: Policy loss: 0.104106. Value loss: 0.036610. Entropy: 0.306916.\n",
      "Iteration 11757: Policy loss: 0.097270. Value loss: 0.025895. Entropy: 0.307392.\n",
      "episode: 4323   score: 330.0  epsilon: 1.0    steps: 992  evaluation reward: 391.05\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11758: Policy loss: 0.148152. Value loss: 0.432259. Entropy: 0.303182.\n",
      "Iteration 11759: Policy loss: 0.163458. Value loss: 0.208678. Entropy: 0.303273.\n",
      "Iteration 11760: Policy loss: 0.118053. Value loss: 0.185148. Entropy: 0.302597.\n",
      "episode: 4324   score: 180.0  epsilon: 1.0    steps: 200  evaluation reward: 387.55\n",
      "episode: 4325   score: 180.0  epsilon: 1.0    steps: 568  evaluation reward: 386.65\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11761: Policy loss: 0.156600. Value loss: 0.118423. Entropy: 0.298697.\n",
      "Iteration 11762: Policy loss: 0.152072. Value loss: 0.047126. Entropy: 0.295710.\n",
      "Iteration 11763: Policy loss: 0.143207. Value loss: 0.031056. Entropy: 0.296208.\n",
      "episode: 4326   score: 420.0  epsilon: 1.0    steps: 104  evaluation reward: 385.85\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11764: Policy loss: -0.148633. Value loss: 0.068067. Entropy: 0.296541.\n",
      "Iteration 11765: Policy loss: -0.156676. Value loss: 0.030887. Entropy: 0.298317.\n",
      "Iteration 11766: Policy loss: -0.158338. Value loss: 0.023916. Entropy: 0.296568.\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11767: Policy loss: 0.014647. Value loss: 0.046405. Entropy: 0.308378.\n",
      "Iteration 11768: Policy loss: 0.004097. Value loss: 0.021646. Entropy: 0.308766.\n",
      "Iteration 11769: Policy loss: -0.001823. Value loss: 0.016419. Entropy: 0.308441.\n",
      "episode: 4327   score: 345.0  epsilon: 1.0    steps: 160  evaluation reward: 385.1\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11770: Policy loss: 0.260392. Value loss: 0.066303. Entropy: 0.301869.\n",
      "Iteration 11771: Policy loss: 0.251010. Value loss: 0.022605. Entropy: 0.301335.\n",
      "Iteration 11772: Policy loss: 0.253852. Value loss: 0.017779. Entropy: 0.301702.\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11773: Policy loss: -0.009504. Value loss: 0.100294. Entropy: 0.308290.\n",
      "Iteration 11774: Policy loss: -0.017934. Value loss: 0.044048. Entropy: 0.308185.\n",
      "Iteration 11775: Policy loss: -0.013868. Value loss: 0.031866. Entropy: 0.308243.\n",
      "episode: 4328   score: 895.0  epsilon: 1.0    steps: 464  evaluation reward: 390.45\n",
      "episode: 4329   score: 365.0  epsilon: 1.0    steps: 576  evaluation reward: 387.35\n",
      "episode: 4330   score: 435.0  epsilon: 1.0    steps: 632  evaluation reward: 385.95\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11776: Policy loss: 0.152645. Value loss: 0.070773. Entropy: 0.289633.\n",
      "Iteration 11777: Policy loss: 0.143163. Value loss: 0.026415. Entropy: 0.290789.\n",
      "Iteration 11778: Policy loss: 0.140688. Value loss: 0.021320. Entropy: 0.290109.\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11779: Policy loss: 0.121884. Value loss: 0.119284. Entropy: 0.304106.\n",
      "Iteration 11780: Policy loss: 0.120040. Value loss: 0.050667. Entropy: 0.303002.\n",
      "Iteration 11781: Policy loss: 0.125607. Value loss: 0.037125. Entropy: 0.303313.\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11782: Policy loss: 0.022137. Value loss: 0.120211. Entropy: 0.308883.\n",
      "Iteration 11783: Policy loss: 0.019362. Value loss: 0.054276. Entropy: 0.307884.\n",
      "Iteration 11784: Policy loss: 0.015948. Value loss: 0.039380. Entropy: 0.308601.\n",
      "episode: 4331   score: 265.0  epsilon: 1.0    steps: 72  evaluation reward: 382.65\n",
      "episode: 4332   score: 125.0  epsilon: 1.0    steps: 704  evaluation reward: 380.45\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11785: Policy loss: 0.185669. Value loss: 0.059263. Entropy: 0.294444.\n",
      "Iteration 11786: Policy loss: 0.181360. Value loss: 0.029263. Entropy: 0.293769.\n",
      "Iteration 11787: Policy loss: 0.176463. Value loss: 0.021947. Entropy: 0.292974.\n",
      "episode: 4333   score: 315.0  epsilon: 1.0    steps: 24  evaluation reward: 380.35\n",
      "episode: 4334   score: 350.0  epsilon: 1.0    steps: 952  evaluation reward: 380.2\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11788: Policy loss: 0.171455. Value loss: 0.055577. Entropy: 0.298031.\n",
      "Iteration 11789: Policy loss: 0.166673. Value loss: 0.023671. Entropy: 0.297235.\n",
      "Iteration 11790: Policy loss: 0.161712. Value loss: 0.016732. Entropy: 0.296706.\n",
      "episode: 4335   score: 260.0  epsilon: 1.0    steps: 528  evaluation reward: 377.7\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11791: Policy loss: -0.057097. Value loss: 0.088517. Entropy: 0.297004.\n",
      "Iteration 11792: Policy loss: -0.060110. Value loss: 0.037801. Entropy: 0.296462.\n",
      "Iteration 11793: Policy loss: -0.059678. Value loss: 0.027799. Entropy: 0.294864.\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11794: Policy loss: 0.063975. Value loss: 0.050940. Entropy: 0.307910.\n",
      "Iteration 11795: Policy loss: 0.058990. Value loss: 0.021466. Entropy: 0.308502.\n",
      "Iteration 11796: Policy loss: 0.054168. Value loss: 0.016172. Entropy: 0.307631.\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11797: Policy loss: 0.155877. Value loss: 0.070328. Entropy: 0.309991.\n",
      "Iteration 11798: Policy loss: 0.151754. Value loss: 0.027560. Entropy: 0.310058.\n",
      "Iteration 11799: Policy loss: 0.148260. Value loss: 0.018471. Entropy: 0.309330.\n",
      "episode: 4336   score: 260.0  epsilon: 1.0    steps: 216  evaluation reward: 377.3\n",
      "episode: 4337   score: 240.0  epsilon: 1.0    steps: 224  evaluation reward: 375.6\n",
      "Training network. lr: 0.000160. clip: 0.063910\n",
      "Iteration 11800: Policy loss: 0.056801. Value loss: 0.041275. Entropy: 0.295262.\n",
      "Iteration 11801: Policy loss: 0.055955. Value loss: 0.022107. Entropy: 0.295833.\n",
      "Iteration 11802: Policy loss: 0.055947. Value loss: 0.019387. Entropy: 0.294240.\n",
      "episode: 4338   score: 575.0  epsilon: 1.0    steps: 56  evaluation reward: 378.5\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11803: Policy loss: -0.024043. Value loss: 0.095893. Entropy: 0.305500.\n",
      "Iteration 11804: Policy loss: -0.029709. Value loss: 0.044490. Entropy: 0.305304.\n",
      "Iteration 11805: Policy loss: -0.041619. Value loss: 0.030124. Entropy: 0.305275.\n",
      "Training network. lr: 0.000159. clip: 0.063753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11806: Policy loss: 0.045796. Value loss: 0.316726. Entropy: 0.305198.\n",
      "Iteration 11807: Policy loss: 0.052873. Value loss: 0.233694. Entropy: 0.304673.\n",
      "Iteration 11808: Policy loss: 0.028084. Value loss: 0.222220. Entropy: 0.304139.\n",
      "episode: 4339   score: 330.0  epsilon: 1.0    steps: 744  evaluation reward: 377.4\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11809: Policy loss: -0.092116. Value loss: 0.088416. Entropy: 0.305762.\n",
      "Iteration 11810: Policy loss: -0.097258. Value loss: 0.033205. Entropy: 0.305608.\n",
      "Iteration 11811: Policy loss: -0.101218. Value loss: 0.018892. Entropy: 0.306195.\n",
      "episode: 4340   score: 285.0  epsilon: 1.0    steps: 816  evaluation reward: 376.35\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11812: Policy loss: 0.036173. Value loss: 0.092647. Entropy: 0.300621.\n",
      "Iteration 11813: Policy loss: 0.034666. Value loss: 0.039750. Entropy: 0.300794.\n",
      "Iteration 11814: Policy loss: 0.032638. Value loss: 0.027893. Entropy: 0.300372.\n",
      "episode: 4341   score: 515.0  epsilon: 1.0    steps: 928  evaluation reward: 378.0\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11815: Policy loss: 0.101585. Value loss: 0.068722. Entropy: 0.305728.\n",
      "Iteration 11816: Policy loss: 0.095007. Value loss: 0.029741. Entropy: 0.304961.\n",
      "Iteration 11817: Policy loss: 0.092763. Value loss: 0.022593. Entropy: 0.305495.\n",
      "episode: 4342   score: 395.0  epsilon: 1.0    steps: 680  evaluation reward: 377.6\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11818: Policy loss: 0.102826. Value loss: 0.049526. Entropy: 0.303120.\n",
      "Iteration 11819: Policy loss: 0.098972. Value loss: 0.022921. Entropy: 0.303574.\n",
      "Iteration 11820: Policy loss: 0.089751. Value loss: 0.018936. Entropy: 0.303096.\n",
      "episode: 4343   score: 220.0  epsilon: 1.0    steps: 664  evaluation reward: 375.9\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11821: Policy loss: 0.073090. Value loss: 0.080849. Entropy: 0.299808.\n",
      "Iteration 11822: Policy loss: 0.075323. Value loss: 0.038054. Entropy: 0.299725.\n",
      "Iteration 11823: Policy loss: 0.066142. Value loss: 0.026766. Entropy: 0.299460.\n",
      "episode: 4344   score: 420.0  epsilon: 1.0    steps: 320  evaluation reward: 378.0\n",
      "episode: 4345   score: 260.0  epsilon: 1.0    steps: 896  evaluation reward: 378.3\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11824: Policy loss: -0.098318. Value loss: 0.040125. Entropy: 0.297356.\n",
      "Iteration 11825: Policy loss: -0.098325. Value loss: 0.026133. Entropy: 0.298864.\n",
      "Iteration 11826: Policy loss: -0.103248. Value loss: 0.022887. Entropy: 0.298950.\n",
      "episode: 4346   score: 335.0  epsilon: 1.0    steps: 296  evaluation reward: 379.4\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11827: Policy loss: 0.043598. Value loss: 0.042828. Entropy: 0.302205.\n",
      "Iteration 11828: Policy loss: 0.031108. Value loss: 0.016185. Entropy: 0.301384.\n",
      "Iteration 11829: Policy loss: 0.031605. Value loss: 0.012406. Entropy: 0.300635.\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11830: Policy loss: 0.087925. Value loss: 0.084105. Entropy: 0.306651.\n",
      "Iteration 11831: Policy loss: 0.088582. Value loss: 0.043330. Entropy: 0.307121.\n",
      "Iteration 11832: Policy loss: 0.084241. Value loss: 0.031047. Entropy: 0.306543.\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11833: Policy loss: -0.304309. Value loss: 0.290576. Entropy: 0.306538.\n",
      "Iteration 11834: Policy loss: -0.326425. Value loss: 0.199028. Entropy: 0.308026.\n",
      "Iteration 11835: Policy loss: -0.313672. Value loss: 0.141514. Entropy: 0.307707.\n",
      "episode: 4347   score: 260.0  epsilon: 1.0    steps: 232  evaluation reward: 378.65\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11836: Policy loss: -0.044777. Value loss: 0.062158. Entropy: 0.303030.\n",
      "Iteration 11837: Policy loss: -0.045786. Value loss: 0.016224. Entropy: 0.303514.\n",
      "Iteration 11838: Policy loss: -0.046831. Value loss: 0.011498. Entropy: 0.302450.\n",
      "episode: 4348   score: 265.0  epsilon: 1.0    steps: 776  evaluation reward: 378.3\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11839: Policy loss: -0.089035. Value loss: 0.097381. Entropy: 0.301299.\n",
      "Iteration 11840: Policy loss: -0.092560. Value loss: 0.037510. Entropy: 0.301864.\n",
      "Iteration 11841: Policy loss: -0.096354. Value loss: 0.024069. Entropy: 0.301226.\n",
      "episode: 4349   score: 345.0  epsilon: 1.0    steps: 864  evaluation reward: 379.65\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11842: Policy loss: -0.551080. Value loss: 0.545258. Entropy: 0.301263.\n",
      "Iteration 11843: Policy loss: -0.549870. Value loss: 0.240973. Entropy: 0.301963.\n",
      "Iteration 11844: Policy loss: -0.570322. Value loss: 0.097940. Entropy: 0.303270.\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11845: Policy loss: 0.187436. Value loss: 0.083799. Entropy: 0.307793.\n",
      "Iteration 11846: Policy loss: 0.185610. Value loss: 0.032504. Entropy: 0.307595.\n",
      "Iteration 11847: Policy loss: 0.176184. Value loss: 0.023847. Entropy: 0.308123.\n",
      "episode: 4350   score: 440.0  epsilon: 1.0    steps: 744  evaluation reward: 381.15\n",
      "Training network. lr: 0.000159. clip: 0.063753\n",
      "Iteration 11848: Policy loss: 0.080986. Value loss: 0.100908. Entropy: 0.303863.\n",
      "Iteration 11849: Policy loss: 0.068057. Value loss: 0.037372. Entropy: 0.302442.\n",
      "Iteration 11850: Policy loss: 0.066100. Value loss: 0.029522. Entropy: 0.302717.\n",
      "now time :  2019-09-06 02:30:18.963062\n",
      "episode: 4351   score: 365.0  epsilon: 1.0    steps: 104  evaluation reward: 380.3\n",
      "episode: 4352   score: 375.0  epsilon: 1.0    steps: 520  evaluation reward: 381.3\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11851: Policy loss: -0.128580. Value loss: 0.138457. Entropy: 0.296425.\n",
      "Iteration 11852: Policy loss: -0.139197. Value loss: 0.061497. Entropy: 0.294308.\n",
      "Iteration 11853: Policy loss: -0.140027. Value loss: 0.043792. Entropy: 0.294555.\n",
      "episode: 4353   score: 530.0  epsilon: 1.0    steps: 584  evaluation reward: 382.1\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11854: Policy loss: -0.141066. Value loss: 0.328152. Entropy: 0.301209.\n",
      "Iteration 11855: Policy loss: -0.152880. Value loss: 0.222769. Entropy: 0.301568.\n",
      "Iteration 11856: Policy loss: -0.172392. Value loss: 0.187522. Entropy: 0.301145.\n",
      "episode: 4354   score: 850.0  epsilon: 1.0    steps: 88  evaluation reward: 384.4\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11857: Policy loss: -0.104472. Value loss: 0.294498. Entropy: 0.307828.\n",
      "Iteration 11858: Policy loss: -0.092903. Value loss: 0.146059. Entropy: 0.303993.\n",
      "Iteration 11859: Policy loss: -0.112998. Value loss: 0.074307. Entropy: 0.304573.\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11860: Policy loss: 0.212004. Value loss: 0.081888. Entropy: 0.307558.\n",
      "Iteration 11861: Policy loss: 0.203522. Value loss: 0.029069. Entropy: 0.306131.\n",
      "Iteration 11862: Policy loss: 0.196132. Value loss: 0.016687. Entropy: 0.306801.\n",
      "episode: 4355   score: 460.0  epsilon: 1.0    steps: 328  evaluation reward: 383.75\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11863: Policy loss: -0.344776. Value loss: 0.217722. Entropy: 0.302898.\n",
      "Iteration 11864: Policy loss: -0.368154. Value loss: 0.086392. Entropy: 0.302320.\n",
      "Iteration 11865: Policy loss: -0.370416. Value loss: 0.038011. Entropy: 0.301334.\n",
      "episode: 4356   score: 465.0  epsilon: 1.0    steps: 704  evaluation reward: 384.15\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11866: Policy loss: 0.432473. Value loss: 0.323862. Entropy: 0.304295.\n",
      "Iteration 11867: Policy loss: 0.418540. Value loss: 0.071533. Entropy: 0.302485.\n",
      "Iteration 11868: Policy loss: 0.406979. Value loss: 0.036452. Entropy: 0.302671.\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11869: Policy loss: 0.005654. Value loss: 0.139970. Entropy: 0.305217.\n",
      "Iteration 11870: Policy loss: 0.002962. Value loss: 0.049180. Entropy: 0.305295.\n",
      "Iteration 11871: Policy loss: -0.002178. Value loss: 0.031622. Entropy: 0.305271.\n",
      "episode: 4357   score: 510.0  epsilon: 1.0    steps: 176  evaluation reward: 384.1\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11872: Policy loss: -0.062810. Value loss: 0.105505. Entropy: 0.303931.\n",
      "Iteration 11873: Policy loss: -0.066417. Value loss: 0.050325. Entropy: 0.302623.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11874: Policy loss: -0.069419. Value loss: 0.037050. Entropy: 0.303388.\n",
      "episode: 4358   score: 540.0  epsilon: 1.0    steps: 416  evaluation reward: 384.8\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11875: Policy loss: 0.062210. Value loss: 0.116848. Entropy: 0.305514.\n",
      "Iteration 11876: Policy loss: 0.049876. Value loss: 0.039194. Entropy: 0.304843.\n",
      "Iteration 11877: Policy loss: 0.046016. Value loss: 0.031345. Entropy: 0.302733.\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11878: Policy loss: -0.687195. Value loss: 0.397792. Entropy: 0.308886.\n",
      "Iteration 11879: Policy loss: -0.697319. Value loss: 0.109834. Entropy: 0.307905.\n",
      "Iteration 11880: Policy loss: -0.696631. Value loss: 0.058818. Entropy: 0.308850.\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11881: Policy loss: 0.044789. Value loss: 0.181170. Entropy: 0.306634.\n",
      "Iteration 11882: Policy loss: 0.021321. Value loss: 0.052397. Entropy: 0.305899.\n",
      "Iteration 11883: Policy loss: 0.033378. Value loss: 0.033513. Entropy: 0.306737.\n",
      "episode: 4359   score: 680.0  epsilon: 1.0    steps: 560  evaluation reward: 389.35\n",
      "episode: 4360   score: 430.0  epsilon: 1.0    steps: 968  evaluation reward: 387.75\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11884: Policy loss: -0.440912. Value loss: 0.384604. Entropy: 0.299084.\n",
      "Iteration 11885: Policy loss: -0.439052. Value loss: 0.158061. Entropy: 0.299012.\n",
      "Iteration 11886: Policy loss: -0.465392. Value loss: 0.130438. Entropy: 0.299950.\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11887: Policy loss: 0.195057. Value loss: 0.150822. Entropy: 0.300034.\n",
      "Iteration 11888: Policy loss: 0.193149. Value loss: 0.042875. Entropy: 0.300013.\n",
      "Iteration 11889: Policy loss: 0.185373. Value loss: 0.029189. Entropy: 0.298676.\n",
      "episode: 4361   score: 785.0  epsilon: 1.0    steps: 16  evaluation reward: 386.3\n",
      "episode: 4362   score: 710.0  epsilon: 1.0    steps: 568  evaluation reward: 390.55\n",
      "episode: 4363   score: 410.0  epsilon: 1.0    steps: 832  evaluation reward: 391.65\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11890: Policy loss: 0.213675. Value loss: 0.089505. Entropy: 0.288223.\n",
      "Iteration 11891: Policy loss: 0.204485. Value loss: 0.040587. Entropy: 0.285015.\n",
      "Iteration 11892: Policy loss: 0.196608. Value loss: 0.030707. Entropy: 0.288104.\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11893: Policy loss: 0.075022. Value loss: 0.143536. Entropy: 0.305409.\n",
      "Iteration 11894: Policy loss: 0.056894. Value loss: 0.046493. Entropy: 0.305601.\n",
      "Iteration 11895: Policy loss: 0.055824. Value loss: 0.030478. Entropy: 0.304865.\n",
      "episode: 4364   score: 440.0  epsilon: 1.0    steps: 776  evaluation reward: 393.4\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11896: Policy loss: 0.318058. Value loss: 0.116438. Entropy: 0.308535.\n",
      "Iteration 11897: Policy loss: 0.322430. Value loss: 0.036678. Entropy: 0.307085.\n",
      "Iteration 11898: Policy loss: 0.316235. Value loss: 0.025519. Entropy: 0.306545.\n",
      "episode: 4365   score: 275.0  epsilon: 1.0    steps: 184  evaluation reward: 392.55\n",
      "Training network. lr: 0.000159. clip: 0.063606\n",
      "Iteration 11899: Policy loss: -0.058460. Value loss: 0.077050. Entropy: 0.310399.\n",
      "Iteration 11900: Policy loss: -0.063085. Value loss: 0.030051. Entropy: 0.309820.\n",
      "Iteration 11901: Policy loss: -0.064893. Value loss: 0.019179. Entropy: 0.311296.\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11902: Policy loss: -0.059790. Value loss: 0.379768. Entropy: 0.304918.\n",
      "Iteration 11903: Policy loss: -0.069366. Value loss: 0.236340. Entropy: 0.304209.\n",
      "Iteration 11904: Policy loss: -0.077452. Value loss: 0.180331. Entropy: 0.302461.\n",
      "episode: 4366   score: 495.0  epsilon: 1.0    steps: 584  evaluation reward: 391.6\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11905: Policy loss: 0.208737. Value loss: 0.169992. Entropy: 0.304027.\n",
      "Iteration 11906: Policy loss: 0.205672. Value loss: 0.080398. Entropy: 0.304065.\n",
      "Iteration 11907: Policy loss: 0.192297. Value loss: 0.060896. Entropy: 0.303177.\n",
      "episode: 4367   score: 320.0  epsilon: 1.0    steps: 184  evaluation reward: 389.85\n",
      "episode: 4368   score: 330.0  epsilon: 1.0    steps: 440  evaluation reward: 389.85\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11908: Policy loss: 0.289505. Value loss: 0.086339. Entropy: 0.302602.\n",
      "Iteration 11909: Policy loss: 0.281376. Value loss: 0.037117. Entropy: 0.303757.\n",
      "Iteration 11910: Policy loss: 0.279476. Value loss: 0.029205. Entropy: 0.302688.\n",
      "episode: 4369   score: 450.0  epsilon: 1.0    steps: 320  evaluation reward: 391.5\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11911: Policy loss: 0.137453. Value loss: 0.124599. Entropy: 0.307755.\n",
      "Iteration 11912: Policy loss: 0.131925. Value loss: 0.050916. Entropy: 0.307674.\n",
      "Iteration 11913: Policy loss: 0.127195. Value loss: 0.035116. Entropy: 0.305439.\n",
      "episode: 4370   score: 260.0  epsilon: 1.0    steps: 320  evaluation reward: 390.9\n",
      "episode: 4371   score: 215.0  epsilon: 1.0    steps: 528  evaluation reward: 388.45\n",
      "episode: 4372   score: 325.0  epsilon: 1.0    steps: 600  evaluation reward: 387.2\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11914: Policy loss: 0.186938. Value loss: 0.112243. Entropy: 0.301525.\n",
      "Iteration 11915: Policy loss: 0.172656. Value loss: 0.055761. Entropy: 0.301571.\n",
      "Iteration 11916: Policy loss: 0.172799. Value loss: 0.043238. Entropy: 0.302811.\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11917: Policy loss: -0.123889. Value loss: 0.100735. Entropy: 0.310556.\n",
      "Iteration 11918: Policy loss: -0.128352. Value loss: 0.041498. Entropy: 0.312465.\n",
      "Iteration 11919: Policy loss: -0.135384. Value loss: 0.032095. Entropy: 0.311585.\n",
      "episode: 4373   score: 425.0  epsilon: 1.0    steps: 496  evaluation reward: 383.9\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11920: Policy loss: 0.012503. Value loss: 0.076044. Entropy: 0.304568.\n",
      "Iteration 11921: Policy loss: 0.003145. Value loss: 0.035748. Entropy: 0.304250.\n",
      "Iteration 11922: Policy loss: 0.005627. Value loss: 0.028054. Entropy: 0.303212.\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11923: Policy loss: -0.606034. Value loss: 0.492584. Entropy: 0.310320.\n",
      "Iteration 11924: Policy loss: -0.642583. Value loss: 0.297953. Entropy: 0.308031.\n",
      "Iteration 11925: Policy loss: -0.631604. Value loss: 0.202024. Entropy: 0.307472.\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11926: Policy loss: 0.118855. Value loss: 0.191674. Entropy: 0.303375.\n",
      "Iteration 11927: Policy loss: 0.130520. Value loss: 0.055848. Entropy: 0.301653.\n",
      "Iteration 11928: Policy loss: 0.102183. Value loss: 0.052069. Entropy: 0.300750.\n",
      "episode: 4374   score: 410.0  epsilon: 1.0    steps: 680  evaluation reward: 384.65\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11929: Policy loss: -0.084545. Value loss: 0.266645. Entropy: 0.300727.\n",
      "Iteration 11930: Policy loss: -0.083701. Value loss: 0.080100. Entropy: 0.299220.\n",
      "Iteration 11931: Policy loss: -0.102173. Value loss: 0.045232. Entropy: 0.300789.\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11932: Policy loss: -0.171941. Value loss: 0.174922. Entropy: 0.303827.\n",
      "Iteration 11933: Policy loss: -0.187160. Value loss: 0.068442. Entropy: 0.303697.\n",
      "Iteration 11934: Policy loss: -0.184863. Value loss: 0.038638. Entropy: 0.302569.\n",
      "episode: 4375   score: 570.0  epsilon: 1.0    steps: 88  evaluation reward: 387.75\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11935: Policy loss: 0.186694. Value loss: 0.137483. Entropy: 0.306957.\n",
      "Iteration 11936: Policy loss: 0.186583. Value loss: 0.058140. Entropy: 0.305365.\n",
      "Iteration 11937: Policy loss: 0.185312. Value loss: 0.040711. Entropy: 0.305285.\n",
      "episode: 4376   score: 620.0  epsilon: 1.0    steps: 544  evaluation reward: 389.7\n",
      "episode: 4377   score: 365.0  epsilon: 1.0    steps: 632  evaluation reward: 386.8\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11938: Policy loss: -0.063119. Value loss: 0.206669. Entropy: 0.302868.\n",
      "Iteration 11939: Policy loss: -0.080947. Value loss: 0.065052. Entropy: 0.300822.\n",
      "Iteration 11940: Policy loss: -0.084382. Value loss: 0.034164. Entropy: 0.300630.\n",
      "Training network. lr: 0.000159. clip: 0.063449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11941: Policy loss: 0.111401. Value loss: 0.076879. Entropy: 0.306848.\n",
      "Iteration 11942: Policy loss: 0.101975. Value loss: 0.030290. Entropy: 0.307015.\n",
      "Iteration 11943: Policy loss: 0.104076. Value loss: 0.023324. Entropy: 0.308374.\n",
      "episode: 4378   score: 550.0  epsilon: 1.0    steps: 200  evaluation reward: 387.55\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11944: Policy loss: -0.092968. Value loss: 0.245506. Entropy: 0.304692.\n",
      "Iteration 11945: Policy loss: -0.107441. Value loss: 0.116537. Entropy: 0.305934.\n",
      "Iteration 11946: Policy loss: -0.106198. Value loss: 0.062414. Entropy: 0.303235.\n",
      "episode: 4379   score: 635.0  epsilon: 1.0    steps: 624  evaluation reward: 391.5\n",
      "episode: 4380   score: 790.0  epsilon: 1.0    steps: 784  evaluation reward: 394.45\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11947: Policy loss: -0.215059. Value loss: 0.422743. Entropy: 0.291559.\n",
      "Iteration 11948: Policy loss: -0.229435. Value loss: 0.231702. Entropy: 0.293556.\n",
      "Iteration 11949: Policy loss: -0.262369. Value loss: 0.188604. Entropy: 0.291882.\n",
      "episode: 4381   score: 700.0  epsilon: 1.0    steps: 16  evaluation reward: 399.05\n",
      "Training network. lr: 0.000159. clip: 0.063449\n",
      "Iteration 11950: Policy loss: 0.015347. Value loss: 0.074294. Entropy: 0.304229.\n",
      "Iteration 11951: Policy loss: 0.020256. Value loss: 0.034829. Entropy: 0.302825.\n",
      "Iteration 11952: Policy loss: 0.014091. Value loss: 0.024517. Entropy: 0.302087.\n",
      "episode: 4382   score: 510.0  epsilon: 1.0    steps: 248  evaluation reward: 399.5\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11953: Policy loss: -0.134311. Value loss: 0.339850. Entropy: 0.304109.\n",
      "Iteration 11954: Policy loss: -0.152578. Value loss: 0.211187. Entropy: 0.302592.\n",
      "Iteration 11955: Policy loss: -0.142428. Value loss: 0.159412. Entropy: 0.299660.\n",
      "episode: 4383   score: 395.0  epsilon: 1.0    steps: 912  evaluation reward: 400.45\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11956: Policy loss: 0.084225. Value loss: 0.115222. Entropy: 0.305301.\n",
      "Iteration 11957: Policy loss: 0.079499. Value loss: 0.041176. Entropy: 0.304184.\n",
      "Iteration 11958: Policy loss: 0.077627. Value loss: 0.029817. Entropy: 0.304985.\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11959: Policy loss: -0.040919. Value loss: 0.159951. Entropy: 0.311064.\n",
      "Iteration 11960: Policy loss: -0.057984. Value loss: 0.066103. Entropy: 0.310648.\n",
      "Iteration 11961: Policy loss: -0.058361. Value loss: 0.047060. Entropy: 0.310677.\n",
      "episode: 4384   score: 270.0  epsilon: 1.0    steps: 136  evaluation reward: 401.35\n",
      "episode: 4385   score: 515.0  epsilon: 1.0    steps: 440  evaluation reward: 400.6\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11962: Policy loss: 0.229965. Value loss: 0.202768. Entropy: 0.299980.\n",
      "Iteration 11963: Policy loss: 0.213500. Value loss: 0.063321. Entropy: 0.300722.\n",
      "Iteration 11964: Policy loss: 0.213293. Value loss: 0.042324. Entropy: 0.299638.\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11965: Policy loss: 0.073761. Value loss: 0.164076. Entropy: 0.308201.\n",
      "Iteration 11966: Policy loss: 0.063774. Value loss: 0.051406. Entropy: 0.307358.\n",
      "Iteration 11967: Policy loss: 0.067917. Value loss: 0.029345. Entropy: 0.307290.\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11968: Policy loss: 0.181949. Value loss: 0.192200. Entropy: 0.305779.\n",
      "Iteration 11969: Policy loss: 0.178272. Value loss: 0.082280. Entropy: 0.305631.\n",
      "Iteration 11970: Policy loss: 0.169374. Value loss: 0.051836. Entropy: 0.306359.\n",
      "episode: 4386   score: 285.0  epsilon: 1.0    steps: 80  evaluation reward: 402.2\n",
      "episode: 4387   score: 350.0  epsilon: 1.0    steps: 720  evaluation reward: 401.75\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11971: Policy loss: 0.036423. Value loss: 0.142468. Entropy: 0.299455.\n",
      "Iteration 11972: Policy loss: 0.030834. Value loss: 0.050644. Entropy: 0.297363.\n",
      "Iteration 11973: Policy loss: 0.024562. Value loss: 0.033614. Entropy: 0.298283.\n",
      "episode: 4388   score: 540.0  epsilon: 1.0    steps: 624  evaluation reward: 397.6\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11974: Policy loss: 0.103138. Value loss: 0.100280. Entropy: 0.300550.\n",
      "Iteration 11975: Policy loss: 0.094964. Value loss: 0.041977. Entropy: 0.299036.\n",
      "Iteration 11976: Policy loss: 0.088918. Value loss: 0.029495. Entropy: 0.300148.\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11977: Policy loss: -0.062524. Value loss: 0.334768. Entropy: 0.309577.\n",
      "Iteration 11978: Policy loss: -0.069468. Value loss: 0.193691. Entropy: 0.308879.\n",
      "Iteration 11979: Policy loss: -0.072699. Value loss: 0.133616. Entropy: 0.309572.\n",
      "episode: 4389   score: 390.0  epsilon: 1.0    steps: 1008  evaluation reward: 398.9\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11980: Policy loss: -0.087908. Value loss: 0.216706. Entropy: 0.308650.\n",
      "Iteration 11981: Policy loss: -0.102755. Value loss: 0.077296. Entropy: 0.307885.\n",
      "Iteration 11982: Policy loss: -0.118748. Value loss: 0.053359. Entropy: 0.308131.\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11983: Policy loss: 0.098265. Value loss: 0.124896. Entropy: 0.308650.\n",
      "Iteration 11984: Policy loss: 0.093336. Value loss: 0.038559. Entropy: 0.307967.\n",
      "Iteration 11985: Policy loss: 0.077190. Value loss: 0.023928. Entropy: 0.307868.\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11986: Policy loss: 0.314063. Value loss: 0.123158. Entropy: 0.315150.\n",
      "Iteration 11987: Policy loss: 0.308932. Value loss: 0.041291. Entropy: 0.313474.\n",
      "Iteration 11988: Policy loss: 0.303019. Value loss: 0.029054. Entropy: 0.313526.\n",
      "episode: 4390   score: 525.0  epsilon: 1.0    steps: 1016  evaluation reward: 402.05\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11989: Policy loss: -0.196571. Value loss: 0.231020. Entropy: 0.311159.\n",
      "Iteration 11990: Policy loss: -0.205401. Value loss: 0.086926. Entropy: 0.311996.\n",
      "Iteration 11991: Policy loss: -0.219136. Value loss: 0.056862. Entropy: 0.310336.\n",
      "episode: 4391   score: 495.0  epsilon: 1.0    steps: 328  evaluation reward: 402.05\n",
      "episode: 4392   score: 405.0  epsilon: 1.0    steps: 728  evaluation reward: 404.0\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11992: Policy loss: 0.218249. Value loss: 0.119845. Entropy: 0.300367.\n",
      "Iteration 11993: Policy loss: 0.209102. Value loss: 0.055698. Entropy: 0.296560.\n",
      "Iteration 11994: Policy loss: 0.205589. Value loss: 0.041544. Entropy: 0.298744.\n",
      "episode: 4393   score: 420.0  epsilon: 1.0    steps: 120  evaluation reward: 405.95\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11995: Policy loss: 0.145727. Value loss: 0.121302. Entropy: 0.301642.\n",
      "Iteration 11996: Policy loss: 0.142736. Value loss: 0.059641. Entropy: 0.300660.\n",
      "Iteration 11997: Policy loss: 0.144032. Value loss: 0.048565. Entropy: 0.300864.\n",
      "episode: 4394   score: 385.0  epsilon: 1.0    steps: 368  evaluation reward: 405.4\n",
      "Training network. lr: 0.000158. clip: 0.063293\n",
      "Iteration 11998: Policy loss: 0.204579. Value loss: 0.111152. Entropy: 0.304970.\n",
      "Iteration 11999: Policy loss: 0.193861. Value loss: 0.048833. Entropy: 0.303134.\n",
      "Iteration 12000: Policy loss: 0.199300. Value loss: 0.034072. Entropy: 0.301762.\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12001: Policy loss: 0.243730. Value loss: 0.194378. Entropy: 0.311478.\n",
      "Iteration 12002: Policy loss: 0.233061. Value loss: 0.073492. Entropy: 0.310596.\n",
      "Iteration 12003: Policy loss: 0.221317. Value loss: 0.049794. Entropy: 0.309723.\n",
      "episode: 4395   score: 635.0  epsilon: 1.0    steps: 256  evaluation reward: 408.6\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12004: Policy loss: -0.175704. Value loss: 0.194779. Entropy: 0.299174.\n",
      "Iteration 12005: Policy loss: -0.188142. Value loss: 0.089325. Entropy: 0.299094.\n",
      "Iteration 12006: Policy loss: -0.198423. Value loss: 0.066680. Entropy: 0.298705.\n",
      "episode: 4396   score: 290.0  epsilon: 1.0    steps: 168  evaluation reward: 409.7\n",
      "episode: 4397   score: 900.0  epsilon: 1.0    steps: 672  evaluation reward: 415.85\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12007: Policy loss: 0.244617. Value loss: 0.115224. Entropy: 0.288545.\n",
      "Iteration 12008: Policy loss: 0.225451. Value loss: 0.031867. Entropy: 0.284543.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12009: Policy loss: 0.230033. Value loss: 0.023968. Entropy: 0.286131.\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12010: Policy loss: 0.202384. Value loss: 0.100672. Entropy: 0.305522.\n",
      "Iteration 12011: Policy loss: 0.195271. Value loss: 0.034216. Entropy: 0.303463.\n",
      "Iteration 12012: Policy loss: 0.192043. Value loss: 0.023862. Entropy: 0.303963.\n",
      "episode: 4398   score: 260.0  epsilon: 1.0    steps: 224  evaluation reward: 412.7\n",
      "episode: 4399   score: 215.0  epsilon: 1.0    steps: 360  evaluation reward: 412.5\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12013: Policy loss: -0.119756. Value loss: 0.188873. Entropy: 0.284203.\n",
      "Iteration 12014: Policy loss: -0.137521. Value loss: 0.123396. Entropy: 0.281615.\n",
      "Iteration 12015: Policy loss: -0.131920. Value loss: 0.083905. Entropy: 0.281734.\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12016: Policy loss: -0.199014. Value loss: 0.408263. Entropy: 0.308532.\n",
      "Iteration 12017: Policy loss: -0.213206. Value loss: 0.200156. Entropy: 0.307784.\n",
      "Iteration 12018: Policy loss: -0.226068. Value loss: 0.128982. Entropy: 0.306550.\n",
      "episode: 4400   score: 565.0  epsilon: 1.0    steps: 8  evaluation reward: 413.1\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12019: Policy loss: 0.132943. Value loss: 0.112793. Entropy: 0.301825.\n",
      "Iteration 12020: Policy loss: 0.126772. Value loss: 0.044952. Entropy: 0.302877.\n",
      "Iteration 12021: Policy loss: 0.122235. Value loss: 0.031223. Entropy: 0.303463.\n",
      "now time :  2019-09-06 02:40:58.111060\n",
      "episode: 4401   score: 440.0  epsilon: 1.0    steps: 504  evaluation reward: 414.9\n",
      "episode: 4402   score: 345.0  epsilon: 1.0    steps: 536  evaluation reward: 413.1\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12022: Policy loss: -0.598914. Value loss: 0.428914. Entropy: 0.296774.\n",
      "Iteration 12023: Policy loss: -0.620131. Value loss: 0.136858. Entropy: 0.297570.\n",
      "Iteration 12024: Policy loss: -0.620178. Value loss: 0.072137. Entropy: 0.298029.\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12025: Policy loss: -0.043633. Value loss: 0.126443. Entropy: 0.309006.\n",
      "Iteration 12026: Policy loss: -0.050259. Value loss: 0.046014. Entropy: 0.307613.\n",
      "Iteration 12027: Policy loss: -0.053913. Value loss: 0.033338. Entropy: 0.308069.\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12028: Policy loss: 0.311106. Value loss: 0.166364. Entropy: 0.308467.\n",
      "Iteration 12029: Policy loss: 0.299748. Value loss: 0.047124. Entropy: 0.308087.\n",
      "Iteration 12030: Policy loss: 0.289645. Value loss: 0.028634. Entropy: 0.307582.\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12031: Policy loss: -0.197002. Value loss: 0.107452. Entropy: 0.315511.\n",
      "Iteration 12032: Policy loss: -0.198489. Value loss: 0.044649. Entropy: 0.315998.\n",
      "Iteration 12033: Policy loss: -0.199844. Value loss: 0.023520. Entropy: 0.316242.\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12034: Policy loss: -0.132695. Value loss: 0.279692. Entropy: 0.311299.\n",
      "Iteration 12035: Policy loss: -0.142211. Value loss: 0.130040. Entropy: 0.311908.\n",
      "Iteration 12036: Policy loss: -0.150751. Value loss: 0.071271. Entropy: 0.311655.\n",
      "episode: 4403   score: 595.0  epsilon: 1.0    steps: 480  evaluation reward: 415.15\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12037: Policy loss: 0.005953. Value loss: 0.146982. Entropy: 0.308632.\n",
      "Iteration 12038: Policy loss: -0.002820. Value loss: 0.065569. Entropy: 0.308677.\n",
      "Iteration 12039: Policy loss: -0.001459. Value loss: 0.043643. Entropy: 0.308337.\n",
      "episode: 4404   score: 605.0  epsilon: 1.0    steps: 496  evaluation reward: 417.85\n",
      "episode: 4405   score: 390.0  epsilon: 1.0    steps: 952  evaluation reward: 417.85\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12040: Policy loss: 0.114127. Value loss: 0.131855. Entropy: 0.297420.\n",
      "Iteration 12041: Policy loss: 0.112879. Value loss: 0.065314. Entropy: 0.296298.\n",
      "Iteration 12042: Policy loss: 0.108680. Value loss: 0.045291. Entropy: 0.296532.\n",
      "episode: 4406   score: 440.0  epsilon: 1.0    steps: 264  evaluation reward: 419.5\n",
      "episode: 4407   score: 810.0  epsilon: 1.0    steps: 744  evaluation reward: 420.5\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12043: Policy loss: 0.027214. Value loss: 0.126286. Entropy: 0.285340.\n",
      "Iteration 12044: Policy loss: 0.022805. Value loss: 0.070682. Entropy: 0.283402.\n",
      "Iteration 12045: Policy loss: 0.018982. Value loss: 0.049578. Entropy: 0.283265.\n",
      "episode: 4408   score: 575.0  epsilon: 1.0    steps: 256  evaluation reward: 422.9\n",
      "episode: 4409   score: 650.0  epsilon: 1.0    steps: 376  evaluation reward: 427.3\n",
      "episode: 4410   score: 415.0  epsilon: 1.0    steps: 800  evaluation reward: 429.05\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12046: Policy loss: 0.020091. Value loss: 0.119351. Entropy: 0.276825.\n",
      "Iteration 12047: Policy loss: 0.016444. Value loss: 0.052640. Entropy: 0.278801.\n",
      "Iteration 12048: Policy loss: 0.005534. Value loss: 0.036824. Entropy: 0.278334.\n",
      "Training network. lr: 0.000158. clip: 0.063145\n",
      "Iteration 12049: Policy loss: 0.428608. Value loss: 0.155261. Entropy: 0.311047.\n",
      "Iteration 12050: Policy loss: 0.396591. Value loss: 0.044997. Entropy: 0.310477.\n",
      "Iteration 12051: Policy loss: 0.397445. Value loss: 0.028142. Entropy: 0.311330.\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12052: Policy loss: 0.003473. Value loss: 0.098023. Entropy: 0.311894.\n",
      "Iteration 12053: Policy loss: 0.002999. Value loss: 0.041571. Entropy: 0.310557.\n",
      "Iteration 12054: Policy loss: -0.006993. Value loss: 0.029285. Entropy: 0.311871.\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12055: Policy loss: -0.139124. Value loss: 0.527820. Entropy: 0.312617.\n",
      "Iteration 12056: Policy loss: -0.161342. Value loss: 0.339742. Entropy: 0.311439.\n",
      "Iteration 12057: Policy loss: -0.169327. Value loss: 0.231694. Entropy: 0.311735.\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12058: Policy loss: 0.318845. Value loss: 0.110123. Entropy: 0.310202.\n",
      "Iteration 12059: Policy loss: 0.313937. Value loss: 0.044197. Entropy: 0.309965.\n",
      "Iteration 12060: Policy loss: 0.313246. Value loss: 0.033019. Entropy: 0.309121.\n",
      "episode: 4411   score: 330.0  epsilon: 1.0    steps: 8  evaluation reward: 428.4\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12061: Policy loss: 0.241360. Value loss: 0.109877. Entropy: 0.303382.\n",
      "Iteration 12062: Policy loss: 0.236727. Value loss: 0.046441. Entropy: 0.303046.\n",
      "Iteration 12063: Policy loss: 0.232489. Value loss: 0.031653. Entropy: 0.302766.\n",
      "episode: 4412   score: 500.0  epsilon: 1.0    steps: 408  evaluation reward: 428.45\n",
      "episode: 4413   score: 275.0  epsilon: 1.0    steps: 464  evaluation reward: 426.8\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12064: Policy loss: 0.210861. Value loss: 0.131645. Entropy: 0.300634.\n",
      "Iteration 12065: Policy loss: 0.209418. Value loss: 0.060353. Entropy: 0.297449.\n",
      "Iteration 12066: Policy loss: 0.205706. Value loss: 0.041015. Entropy: 0.296121.\n",
      "episode: 4414   score: 330.0  epsilon: 1.0    steps: 600  evaluation reward: 426.3\n",
      "episode: 4415   score: 625.0  epsilon: 1.0    steps: 680  evaluation reward: 428.2\n",
      "episode: 4416   score: 240.0  epsilon: 1.0    steps: 888  evaluation reward: 428.0\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12067: Policy loss: 0.071408. Value loss: 0.115692. Entropy: 0.288458.\n",
      "Iteration 12068: Policy loss: 0.066597. Value loss: 0.059730. Entropy: 0.289461.\n",
      "Iteration 12069: Policy loss: 0.060608. Value loss: 0.046669. Entropy: 0.287827.\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12070: Policy loss: -0.329776. Value loss: 0.302267. Entropy: 0.309166.\n",
      "Iteration 12071: Policy loss: -0.353096. Value loss: 0.183646. Entropy: 0.310638.\n",
      "Iteration 12072: Policy loss: -0.354452. Value loss: 0.133456. Entropy: 0.310595.\n",
      "episode: 4417   score: 635.0  epsilon: 1.0    steps: 672  evaluation reward: 429.95\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12073: Policy loss: 0.072153. Value loss: 0.124257. Entropy: 0.305509.\n",
      "Iteration 12074: Policy loss: 0.067718. Value loss: 0.053979. Entropy: 0.304662.\n",
      "Iteration 12075: Policy loss: 0.064628. Value loss: 0.040088. Entropy: 0.305094.\n",
      "Training network. lr: 0.000157. clip: 0.062989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12076: Policy loss: -0.519823. Value loss: 0.528891. Entropy: 0.312112.\n",
      "Iteration 12077: Policy loss: -0.536226. Value loss: 0.232842. Entropy: 0.311506.\n",
      "Iteration 12078: Policy loss: -0.553217. Value loss: 0.069650. Entropy: 0.310967.\n",
      "episode: 4418   score: 635.0  epsilon: 1.0    steps: 192  evaluation reward: 432.7\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12079: Policy loss: 0.120815. Value loss: 0.069944. Entropy: 0.307010.\n",
      "Iteration 12080: Policy loss: 0.108205. Value loss: 0.028859. Entropy: 0.305770.\n",
      "Iteration 12081: Policy loss: 0.102291. Value loss: 0.021520. Entropy: 0.307509.\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12082: Policy loss: -0.445908. Value loss: 0.297843. Entropy: 0.307728.\n",
      "Iteration 12083: Policy loss: -0.451398. Value loss: 0.088973. Entropy: 0.306550.\n",
      "Iteration 12084: Policy loss: -0.449222. Value loss: 0.057381. Entropy: 0.306877.\n",
      "episode: 4419   score: 215.0  epsilon: 1.0    steps: 816  evaluation reward: 432.45\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12085: Policy loss: 0.010993. Value loss: 0.339807. Entropy: 0.306604.\n",
      "Iteration 12086: Policy loss: -0.003201. Value loss: 0.114438. Entropy: 0.306409.\n",
      "Iteration 12087: Policy loss: -0.023331. Value loss: 0.070942. Entropy: 0.305253.\n",
      "episode: 4420   score: 260.0  epsilon: 1.0    steps: 72  evaluation reward: 432.95\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12088: Policy loss: 0.228428. Value loss: 0.202054. Entropy: 0.306786.\n",
      "Iteration 12089: Policy loss: 0.211702. Value loss: 0.069148. Entropy: 0.308126.\n",
      "Iteration 12090: Policy loss: 0.200597. Value loss: 0.043035. Entropy: 0.309325.\n",
      "episode: 4421   score: 625.0  epsilon: 1.0    steps: 968  evaluation reward: 434.5\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12091: Policy loss: 0.187472. Value loss: 0.125833. Entropy: 0.301215.\n",
      "Iteration 12092: Policy loss: 0.186019. Value loss: 0.053618. Entropy: 0.299337.\n",
      "Iteration 12093: Policy loss: 0.183147. Value loss: 0.035479. Entropy: 0.298181.\n",
      "episode: 4422   score: 435.0  epsilon: 1.0    steps: 64  evaluation reward: 436.15\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12094: Policy loss: 0.060385. Value loss: 0.189960. Entropy: 0.308217.\n",
      "Iteration 12095: Policy loss: 0.059649. Value loss: 0.107625. Entropy: 0.308203.\n",
      "Iteration 12096: Policy loss: 0.054966. Value loss: 0.080953. Entropy: 0.309016.\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12097: Policy loss: -0.226735. Value loss: 0.265164. Entropy: 0.305582.\n",
      "Iteration 12098: Policy loss: -0.212040. Value loss: 0.066457. Entropy: 0.305756.\n",
      "Iteration 12099: Policy loss: -0.230574. Value loss: 0.038769. Entropy: 0.305905.\n",
      "episode: 4423   score: 450.0  epsilon: 1.0    steps: 72  evaluation reward: 437.35\n",
      "Training network. lr: 0.000157. clip: 0.062989\n",
      "Iteration 12100: Policy loss: 0.414324. Value loss: 0.284985. Entropy: 0.306847.\n",
      "Iteration 12101: Policy loss: 0.408476. Value loss: 0.084243. Entropy: 0.304854.\n",
      "Iteration 12102: Policy loss: 0.398929. Value loss: 0.054643. Entropy: 0.306534.\n",
      "episode: 4424   score: 620.0  epsilon: 1.0    steps: 120  evaluation reward: 441.75\n",
      "episode: 4425   score: 590.0  epsilon: 1.0    steps: 456  evaluation reward: 445.85\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12103: Policy loss: 0.050715. Value loss: 0.182820. Entropy: 0.292550.\n",
      "Iteration 12104: Policy loss: 0.045566. Value loss: 0.057940. Entropy: 0.292862.\n",
      "Iteration 12105: Policy loss: 0.036102. Value loss: 0.042120. Entropy: 0.292183.\n",
      "episode: 4426   score: 565.0  epsilon: 1.0    steps: 464  evaluation reward: 447.3\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12106: Policy loss: 0.430616. Value loss: 0.183139. Entropy: 0.300128.\n",
      "Iteration 12107: Policy loss: 0.424001. Value loss: 0.048406. Entropy: 0.298610.\n",
      "Iteration 12108: Policy loss: 0.409470. Value loss: 0.033508. Entropy: 0.299037.\n",
      "episode: 4427   score: 530.0  epsilon: 1.0    steps: 336  evaluation reward: 449.15\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12109: Policy loss: -0.097299. Value loss: 0.172951. Entropy: 0.296270.\n",
      "Iteration 12110: Policy loss: -0.104904. Value loss: 0.071104. Entropy: 0.295427.\n",
      "Iteration 12111: Policy loss: -0.115201. Value loss: 0.053435. Entropy: 0.295427.\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12112: Policy loss: 0.167861. Value loss: 0.141178. Entropy: 0.310529.\n",
      "Iteration 12113: Policy loss: 0.162502. Value loss: 0.039300. Entropy: 0.311062.\n",
      "Iteration 12114: Policy loss: 0.156973. Value loss: 0.023662. Entropy: 0.309835.\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12115: Policy loss: 0.027796. Value loss: 0.107750. Entropy: 0.312684.\n",
      "Iteration 12116: Policy loss: 0.024236. Value loss: 0.044618. Entropy: 0.313842.\n",
      "Iteration 12117: Policy loss: 0.023508. Value loss: 0.026533. Entropy: 0.312480.\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12118: Policy loss: -0.400464. Value loss: 0.848929. Entropy: 0.308101.\n",
      "Iteration 12119: Policy loss: -0.420528. Value loss: 0.553011. Entropy: 0.309112.\n",
      "Iteration 12120: Policy loss: -0.444440. Value loss: 0.280844. Entropy: 0.308279.\n",
      "episode: 4428   score: 415.0  epsilon: 1.0    steps: 704  evaluation reward: 444.35\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12121: Policy loss: 0.144217. Value loss: 0.190147. Entropy: 0.302401.\n",
      "Iteration 12122: Policy loss: 0.142626. Value loss: 0.081495. Entropy: 0.302642.\n",
      "Iteration 12123: Policy loss: 0.135329. Value loss: 0.052247. Entropy: 0.301804.\n",
      "episode: 4429   score: 420.0  epsilon: 1.0    steps: 240  evaluation reward: 444.9\n",
      "episode: 4430   score: 420.0  epsilon: 1.0    steps: 800  evaluation reward: 444.75\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12124: Policy loss: -0.021864. Value loss: 0.152749. Entropy: 0.297890.\n",
      "Iteration 12125: Policy loss: -0.032628. Value loss: 0.062921. Entropy: 0.298617.\n",
      "Iteration 12126: Policy loss: -0.034932. Value loss: 0.044138. Entropy: 0.297838.\n",
      "episode: 4431   score: 520.0  epsilon: 1.0    steps: 96  evaluation reward: 447.3\n",
      "episode: 4432   score: 470.0  epsilon: 1.0    steps: 456  evaluation reward: 450.75\n",
      "episode: 4433   score: 775.0  epsilon: 1.0    steps: 688  evaluation reward: 455.35\n",
      "episode: 4434   score: 260.0  epsilon: 1.0    steps: 928  evaluation reward: 454.45\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12127: Policy loss: 0.242998. Value loss: 0.100496. Entropy: 0.292920.\n",
      "Iteration 12128: Policy loss: 0.235217. Value loss: 0.053902. Entropy: 0.293992.\n",
      "Iteration 12129: Policy loss: 0.237032. Value loss: 0.042476. Entropy: 0.293053.\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12130: Policy loss: 0.094032. Value loss: 0.108253. Entropy: 0.307714.\n",
      "Iteration 12131: Policy loss: 0.092104. Value loss: 0.044557. Entropy: 0.306341.\n",
      "Iteration 12132: Policy loss: 0.078567. Value loss: 0.033529. Entropy: 0.309143.\n",
      "episode: 4435   score: 440.0  epsilon: 1.0    steps: 384  evaluation reward: 456.25\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12133: Policy loss: 0.204304. Value loss: 0.123049. Entropy: 0.307066.\n",
      "Iteration 12134: Policy loss: 0.196704. Value loss: 0.048467. Entropy: 0.304964.\n",
      "Iteration 12135: Policy loss: 0.199529. Value loss: 0.031383. Entropy: 0.303771.\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12136: Policy loss: -0.081657. Value loss: 0.137751. Entropy: 0.308521.\n",
      "Iteration 12137: Policy loss: -0.083610. Value loss: 0.050714. Entropy: 0.309578.\n",
      "Iteration 12138: Policy loss: -0.092715. Value loss: 0.029381. Entropy: 0.309320.\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12139: Policy loss: 0.243637. Value loss: 0.313321. Entropy: 0.303137.\n",
      "Iteration 12140: Policy loss: 0.230695. Value loss: 0.090775. Entropy: 0.302566.\n",
      "Iteration 12141: Policy loss: 0.221517. Value loss: 0.057739. Entropy: 0.301829.\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12142: Policy loss: 0.031043. Value loss: 0.173373. Entropy: 0.304601.\n",
      "Iteration 12143: Policy loss: 0.024232. Value loss: 0.096302. Entropy: 0.304593.\n",
      "Iteration 12144: Policy loss: 0.026957. Value loss: 0.063127. Entropy: 0.304221.\n",
      "episode: 4436   score: 485.0  epsilon: 1.0    steps: 312  evaluation reward: 458.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4437   score: 240.0  epsilon: 1.0    steps: 760  evaluation reward: 458.5\n",
      "episode: 4438   score: 240.0  epsilon: 1.0    steps: 1008  evaluation reward: 455.15\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12145: Policy loss: 0.155835. Value loss: 0.127158. Entropy: 0.298208.\n",
      "Iteration 12146: Policy loss: 0.137748. Value loss: 0.062395. Entropy: 0.298895.\n",
      "Iteration 12147: Policy loss: 0.138591. Value loss: 0.049457. Entropy: 0.298076.\n",
      "episode: 4439   score: 335.0  epsilon: 1.0    steps: 488  evaluation reward: 455.2\n",
      "Training network. lr: 0.000157. clip: 0.062832\n",
      "Iteration 12148: Policy loss: 0.220740. Value loss: 0.119760. Entropy: 0.302933.\n",
      "Iteration 12149: Policy loss: 0.210138. Value loss: 0.056757. Entropy: 0.303385.\n",
      "Iteration 12150: Policy loss: 0.208565. Value loss: 0.045748. Entropy: 0.302484.\n",
      "episode: 4440   score: 335.0  epsilon: 1.0    steps: 448  evaluation reward: 455.7\n",
      "episode: 4441   score: 315.0  epsilon: 1.0    steps: 496  evaluation reward: 453.7\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12151: Policy loss: 0.055124. Value loss: 0.073193. Entropy: 0.298709.\n",
      "Iteration 12152: Policy loss: 0.055001. Value loss: 0.038160. Entropy: 0.296863.\n",
      "Iteration 12153: Policy loss: 0.050205. Value loss: 0.032278. Entropy: 0.297947.\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12154: Policy loss: -0.100605. Value loss: 0.289064. Entropy: 0.301721.\n",
      "Iteration 12155: Policy loss: -0.111763. Value loss: 0.107072. Entropy: 0.301909.\n",
      "Iteration 12156: Policy loss: -0.120033. Value loss: 0.053171. Entropy: 0.300673.\n",
      "episode: 4442   score: 535.0  epsilon: 1.0    steps: 688  evaluation reward: 455.1\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12157: Policy loss: -0.083071. Value loss: 0.162289. Entropy: 0.309607.\n",
      "Iteration 12158: Policy loss: -0.074448. Value loss: 0.053788. Entropy: 0.307891.\n",
      "Iteration 12159: Policy loss: -0.090099. Value loss: 0.039311. Entropy: 0.308242.\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12160: Policy loss: 0.066929. Value loss: 0.142805. Entropy: 0.308479.\n",
      "Iteration 12161: Policy loss: 0.052595. Value loss: 0.054818. Entropy: 0.308106.\n",
      "Iteration 12162: Policy loss: 0.056471. Value loss: 0.033862. Entropy: 0.307347.\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12163: Policy loss: 0.214498. Value loss: 0.095738. Entropy: 0.307957.\n",
      "Iteration 12164: Policy loss: 0.209300. Value loss: 0.037841. Entropy: 0.306742.\n",
      "Iteration 12165: Policy loss: 0.202365. Value loss: 0.028122. Entropy: 0.307134.\n",
      "episode: 4443   score: 260.0  epsilon: 1.0    steps: 400  evaluation reward: 455.5\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12166: Policy loss: -0.029632. Value loss: 0.149155. Entropy: 0.307962.\n",
      "Iteration 12167: Policy loss: -0.036389. Value loss: 0.064965. Entropy: 0.307829.\n",
      "Iteration 12168: Policy loss: -0.035031. Value loss: 0.044306. Entropy: 0.307626.\n",
      "episode: 4444   score: 465.0  epsilon: 1.0    steps: 40  evaluation reward: 455.95\n",
      "episode: 4445   score: 225.0  epsilon: 1.0    steps: 480  evaluation reward: 455.6\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12169: Policy loss: 0.082541. Value loss: 0.169765. Entropy: 0.306796.\n",
      "Iteration 12170: Policy loss: 0.067736. Value loss: 0.069966. Entropy: 0.303800.\n",
      "Iteration 12171: Policy loss: 0.067449. Value loss: 0.052720. Entropy: 0.304553.\n",
      "episode: 4446   score: 505.0  epsilon: 1.0    steps: 88  evaluation reward: 457.3\n",
      "episode: 4447   score: 645.0  epsilon: 1.0    steps: 936  evaluation reward: 461.15\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12172: Policy loss: 0.076628. Value loss: 0.118658. Entropy: 0.293778.\n",
      "Iteration 12173: Policy loss: 0.074569. Value loss: 0.061782. Entropy: 0.293374.\n",
      "Iteration 12174: Policy loss: 0.060361. Value loss: 0.049626. Entropy: 0.293988.\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12175: Policy loss: 0.024257. Value loss: 0.057973. Entropy: 0.312568.\n",
      "Iteration 12176: Policy loss: 0.025331. Value loss: 0.023532. Entropy: 0.313479.\n",
      "Iteration 12177: Policy loss: 0.019064. Value loss: 0.019462. Entropy: 0.311413.\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12178: Policy loss: 0.099827. Value loss: 0.073352. Entropy: 0.309428.\n",
      "Iteration 12179: Policy loss: 0.097427. Value loss: 0.032581. Entropy: 0.309073.\n",
      "Iteration 12180: Policy loss: 0.089120. Value loss: 0.021362. Entropy: 0.309099.\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12181: Policy loss: -0.068838. Value loss: 0.341849. Entropy: 0.307345.\n",
      "Iteration 12182: Policy loss: -0.066924. Value loss: 0.136244. Entropy: 0.305473.\n",
      "Iteration 12183: Policy loss: -0.073045. Value loss: 0.051916. Entropy: 0.304532.\n",
      "episode: 4448   score: 435.0  epsilon: 1.0    steps: 96  evaluation reward: 462.85\n",
      "episode: 4449   score: 620.0  epsilon: 1.0    steps: 168  evaluation reward: 465.6\n",
      "episode: 4450   score: 365.0  epsilon: 1.0    steps: 200  evaluation reward: 464.85\n",
      "now time :  2019-09-06 02:51:03.926928\n",
      "episode: 4451   score: 245.0  epsilon: 1.0    steps: 744  evaluation reward: 463.65\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12184: Policy loss: 0.024339. Value loss: 0.071266. Entropy: 0.300446.\n",
      "Iteration 12185: Policy loss: 0.014998. Value loss: 0.036714. Entropy: 0.301511.\n",
      "Iteration 12186: Policy loss: 0.014709. Value loss: 0.026141. Entropy: 0.299864.\n",
      "episode: 4452   score: 210.0  epsilon: 1.0    steps: 104  evaluation reward: 462.0\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12187: Policy loss: -0.298801. Value loss: 0.260467. Entropy: 0.301148.\n",
      "Iteration 12188: Policy loss: -0.302811. Value loss: 0.087696. Entropy: 0.299640.\n",
      "Iteration 12189: Policy loss: -0.313341. Value loss: 0.044362. Entropy: 0.298965.\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12190: Policy loss: -0.123745. Value loss: 0.300490. Entropy: 0.310049.\n",
      "Iteration 12191: Policy loss: -0.162233. Value loss: 0.127698. Entropy: 0.308475.\n",
      "Iteration 12192: Policy loss: -0.148176. Value loss: 0.085049. Entropy: 0.310025.\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12193: Policy loss: 0.261234. Value loss: 0.240128. Entropy: 0.315903.\n",
      "Iteration 12194: Policy loss: 0.267222. Value loss: 0.059363. Entropy: 0.315298.\n",
      "Iteration 12195: Policy loss: 0.246238. Value loss: 0.034010. Entropy: 0.313895.\n",
      "episode: 4453   score: 360.0  epsilon: 1.0    steps: 88  evaluation reward: 460.3\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12196: Policy loss: -0.201428. Value loss: 0.326957. Entropy: 0.307677.\n",
      "Iteration 12197: Policy loss: -0.203883. Value loss: 0.172294. Entropy: 0.306427.\n",
      "Iteration 12198: Policy loss: -0.208818. Value loss: 0.075245. Entropy: 0.307305.\n",
      "Training network. lr: 0.000157. clip: 0.062684\n",
      "Iteration 12199: Policy loss: -0.020366. Value loss: 0.117377. Entropy: 0.307589.\n",
      "Iteration 12200: Policy loss: -0.023004. Value loss: 0.054841. Entropy: 0.307365.\n",
      "Iteration 12201: Policy loss: -0.027999. Value loss: 0.036616. Entropy: 0.309309.\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12202: Policy loss: -0.083922. Value loss: 0.143383. Entropy: 0.308931.\n",
      "Iteration 12203: Policy loss: -0.103891. Value loss: 0.074253. Entropy: 0.308285.\n",
      "Iteration 12204: Policy loss: -0.106527. Value loss: 0.049618. Entropy: 0.308921.\n",
      "episode: 4454   score: 620.0  epsilon: 1.0    steps: 8  evaluation reward: 458.0\n",
      "episode: 4455   score: 310.0  epsilon: 1.0    steps: 472  evaluation reward: 456.5\n",
      "episode: 4456   score: 365.0  epsilon: 1.0    steps: 888  evaluation reward: 455.5\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12205: Policy loss: 0.122048. Value loss: 0.182929. Entropy: 0.300801.\n",
      "Iteration 12206: Policy loss: 0.114402. Value loss: 0.093017. Entropy: 0.296073.\n",
      "Iteration 12207: Policy loss: 0.110987. Value loss: 0.061153. Entropy: 0.296398.\n",
      "episode: 4457   score: 330.0  epsilon: 1.0    steps: 24  evaluation reward: 453.7\n",
      "episode: 4458   score: 695.0  epsilon: 1.0    steps: 696  evaluation reward: 455.25\n",
      "episode: 4459   score: 420.0  epsilon: 1.0    steps: 760  evaluation reward: 452.65\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12208: Policy loss: 0.199859. Value loss: 0.114022. Entropy: 0.286709.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12209: Policy loss: 0.185397. Value loss: 0.058663. Entropy: 0.283864.\n",
      "Iteration 12210: Policy loss: 0.173097. Value loss: 0.045742. Entropy: 0.285340.\n",
      "episode: 4460   score: 395.0  epsilon: 1.0    steps: 232  evaluation reward: 452.3\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12211: Policy loss: 0.143817. Value loss: 0.158685. Entropy: 0.300826.\n",
      "Iteration 12212: Policy loss: 0.131084. Value loss: 0.061049. Entropy: 0.299756.\n",
      "Iteration 12213: Policy loss: 0.127898. Value loss: 0.041966. Entropy: 0.299676.\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12214: Policy loss: 0.024285. Value loss: 0.146686. Entropy: 0.310116.\n",
      "Iteration 12215: Policy loss: 0.008629. Value loss: 0.053044. Entropy: 0.309037.\n",
      "Iteration 12216: Policy loss: 0.001364. Value loss: 0.036866. Entropy: 0.308259.\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12217: Policy loss: 0.033002. Value loss: 0.063192. Entropy: 0.309507.\n",
      "Iteration 12218: Policy loss: 0.026375. Value loss: 0.021020. Entropy: 0.309368.\n",
      "Iteration 12219: Policy loss: 0.022963. Value loss: 0.014060. Entropy: 0.309645.\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12220: Policy loss: -0.001687. Value loss: 0.055888. Entropy: 0.309570.\n",
      "Iteration 12221: Policy loss: -0.006669. Value loss: 0.019806. Entropy: 0.309413.\n",
      "Iteration 12222: Policy loss: -0.006288. Value loss: 0.014642. Entropy: 0.310026.\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12223: Policy loss: -0.504327. Value loss: 0.357991. Entropy: 0.310129.\n",
      "Iteration 12224: Policy loss: -0.525347. Value loss: 0.260774. Entropy: 0.310932.\n",
      "Iteration 12225: Policy loss: -0.528238. Value loss: 0.213778. Entropy: 0.311009.\n",
      "episode: 4461   score: 620.0  epsilon: 1.0    steps: 480  evaluation reward: 450.65\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12226: Policy loss: 0.093458. Value loss: 0.134981. Entropy: 0.302096.\n",
      "Iteration 12227: Policy loss: 0.088368. Value loss: 0.056106. Entropy: 0.298536.\n",
      "Iteration 12228: Policy loss: 0.081481. Value loss: 0.038828. Entropy: 0.300068.\n",
      "episode: 4462   score: 390.0  epsilon: 1.0    steps: 824  evaluation reward: 447.45\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12229: Policy loss: 0.000994. Value loss: 0.122414. Entropy: 0.302241.\n",
      "Iteration 12230: Policy loss: -0.005308. Value loss: 0.059463. Entropy: 0.302850.\n",
      "Iteration 12231: Policy loss: -0.012654. Value loss: 0.046242. Entropy: 0.301899.\n",
      "episode: 4463   score: 395.0  epsilon: 1.0    steps: 568  evaluation reward: 447.3\n",
      "episode: 4464   score: 590.0  epsilon: 1.0    steps: 656  evaluation reward: 448.8\n",
      "episode: 4465   score: 345.0  epsilon: 1.0    steps: 736  evaluation reward: 449.5\n",
      "episode: 4466   score: 365.0  epsilon: 1.0    steps: 776  evaluation reward: 448.2\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12232: Policy loss: 0.088384. Value loss: 0.188326. Entropy: 0.286752.\n",
      "Iteration 12233: Policy loss: 0.099117. Value loss: 0.112812. Entropy: 0.285243.\n",
      "Iteration 12234: Policy loss: 0.082016. Value loss: 0.077365. Entropy: 0.284864.\n",
      "episode: 4467   score: 445.0  epsilon: 1.0    steps: 1016  evaluation reward: 449.45\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12235: Policy loss: 0.195955. Value loss: 0.124489. Entropy: 0.305095.\n",
      "Iteration 12236: Policy loss: 0.188363. Value loss: 0.051786. Entropy: 0.302421.\n",
      "Iteration 12237: Policy loss: 0.188440. Value loss: 0.042304. Entropy: 0.304093.\n",
      "episode: 4468   score: 590.0  epsilon: 1.0    steps: 112  evaluation reward: 452.05\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12238: Policy loss: 0.058980. Value loss: 0.061904. Entropy: 0.297666.\n",
      "Iteration 12239: Policy loss: 0.053850. Value loss: 0.030371. Entropy: 0.300677.\n",
      "Iteration 12240: Policy loss: 0.051977. Value loss: 0.023855. Entropy: 0.300887.\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12241: Policy loss: -0.569655. Value loss: 0.504166. Entropy: 0.310674.\n",
      "Iteration 12242: Policy loss: -0.570136. Value loss: 0.224451. Entropy: 0.311947.\n",
      "Iteration 12243: Policy loss: -0.571741. Value loss: 0.078567. Entropy: 0.313639.\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12244: Policy loss: 0.352834. Value loss: 0.164026. Entropy: 0.311535.\n",
      "Iteration 12245: Policy loss: 0.343788. Value loss: 0.037918. Entropy: 0.310670.\n",
      "Iteration 12246: Policy loss: 0.317145. Value loss: 0.021413. Entropy: 0.310496.\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12247: Policy loss: 0.163011. Value loss: 0.142275. Entropy: 0.307835.\n",
      "Iteration 12248: Policy loss: 0.157108. Value loss: 0.058969. Entropy: 0.306933.\n",
      "Iteration 12249: Policy loss: 0.145992. Value loss: 0.037693. Entropy: 0.307502.\n",
      "episode: 4469   score: 285.0  epsilon: 1.0    steps: 832  evaluation reward: 450.4\n",
      "Training network. lr: 0.000156. clip: 0.062528\n",
      "Iteration 12250: Policy loss: -0.156999. Value loss: 0.284220. Entropy: 0.300466.\n",
      "Iteration 12251: Policy loss: -0.148815. Value loss: 0.120036. Entropy: 0.299932.\n",
      "Iteration 12252: Policy loss: -0.170491. Value loss: 0.083861. Entropy: 0.299820.\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12253: Policy loss: 0.168646. Value loss: 0.196137. Entropy: 0.303794.\n",
      "Iteration 12254: Policy loss: 0.149586. Value loss: 0.074562. Entropy: 0.304408.\n",
      "Iteration 12255: Policy loss: 0.144376. Value loss: 0.052634. Entropy: 0.304292.\n",
      "episode: 4470   score: 440.0  epsilon: 1.0    steps: 200  evaluation reward: 452.2\n",
      "episode: 4471   score: 635.0  epsilon: 1.0    steps: 456  evaluation reward: 456.4\n",
      "episode: 4472   score: 365.0  epsilon: 1.0    steps: 1008  evaluation reward: 456.8\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12256: Policy loss: -0.248043. Value loss: 0.245274. Entropy: 0.292972.\n",
      "Iteration 12257: Policy loss: -0.262231. Value loss: 0.095988. Entropy: 0.291411.\n",
      "Iteration 12258: Policy loss: -0.259944. Value loss: 0.056977. Entropy: 0.292194.\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12259: Policy loss: 0.414930. Value loss: 0.168644. Entropy: 0.309435.\n",
      "Iteration 12260: Policy loss: 0.408297. Value loss: 0.057445. Entropy: 0.308791.\n",
      "Iteration 12261: Policy loss: 0.404088. Value loss: 0.037354. Entropy: 0.309516.\n",
      "episode: 4473   score: 605.0  epsilon: 1.0    steps: 224  evaluation reward: 458.6\n",
      "episode: 4474   score: 525.0  epsilon: 1.0    steps: 648  evaluation reward: 459.75\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12262: Policy loss: 0.130389. Value loss: 0.129204. Entropy: 0.290958.\n",
      "Iteration 12263: Policy loss: 0.128769. Value loss: 0.054192. Entropy: 0.290221.\n",
      "Iteration 12264: Policy loss: 0.121850. Value loss: 0.036631. Entropy: 0.288612.\n",
      "episode: 4475   score: 435.0  epsilon: 1.0    steps: 568  evaluation reward: 458.4\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12265: Policy loss: 0.140351. Value loss: 0.067148. Entropy: 0.305892.\n",
      "Iteration 12266: Policy loss: 0.129793. Value loss: 0.031336. Entropy: 0.305125.\n",
      "Iteration 12267: Policy loss: 0.131641. Value loss: 0.019790. Entropy: 0.304831.\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12268: Policy loss: 0.044903. Value loss: 0.112268. Entropy: 0.312093.\n",
      "Iteration 12269: Policy loss: 0.035837. Value loss: 0.044118. Entropy: 0.310487.\n",
      "Iteration 12270: Policy loss: 0.034199. Value loss: 0.031845. Entropy: 0.310056.\n",
      "episode: 4476   score: 460.0  epsilon: 1.0    steps: 48  evaluation reward: 456.8\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12271: Policy loss: -0.216575. Value loss: 0.285950. Entropy: 0.302982.\n",
      "Iteration 12272: Policy loss: -0.224345. Value loss: 0.188997. Entropy: 0.303069.\n",
      "Iteration 12273: Policy loss: -0.231697. Value loss: 0.138929. Entropy: 0.304604.\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12274: Policy loss: 0.095499. Value loss: 0.094349. Entropy: 0.304480.\n",
      "Iteration 12275: Policy loss: 0.084585. Value loss: 0.038156. Entropy: 0.304692.\n",
      "Iteration 12276: Policy loss: 0.077947. Value loss: 0.026579. Entropy: 0.304309.\n",
      "episode: 4477   score: 315.0  epsilon: 1.0    steps: 184  evaluation reward: 456.3\n",
      "episode: 4478   score: 560.0  epsilon: 1.0    steps: 792  evaluation reward: 456.4\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12277: Policy loss: 0.175960. Value loss: 0.151303. Entropy: 0.293606.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12278: Policy loss: 0.165184. Value loss: 0.048847. Entropy: 0.294830.\n",
      "Iteration 12279: Policy loss: 0.153252. Value loss: 0.033939. Entropy: 0.295383.\n",
      "episode: 4479   score: 360.0  epsilon: 1.0    steps: 408  evaluation reward: 453.65\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12280: Policy loss: 0.094285. Value loss: 0.141529. Entropy: 0.304615.\n",
      "Iteration 12281: Policy loss: 0.082684. Value loss: 0.044544. Entropy: 0.303149.\n",
      "Iteration 12282: Policy loss: 0.081037. Value loss: 0.030560. Entropy: 0.304546.\n",
      "episode: 4480   score: 405.0  epsilon: 1.0    steps: 1008  evaluation reward: 449.8\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12283: Policy loss: -0.077655. Value loss: 0.353091. Entropy: 0.303119.\n",
      "Iteration 12284: Policy loss: -0.106353. Value loss: 0.179022. Entropy: 0.304093.\n",
      "Iteration 12285: Policy loss: -0.095481. Value loss: 0.117681. Entropy: 0.302151.\n",
      "episode: 4481   score: 480.0  epsilon: 1.0    steps: 968  evaluation reward: 447.6\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12286: Policy loss: 0.048837. Value loss: 0.132667. Entropy: 0.302213.\n",
      "Iteration 12287: Policy loss: 0.028815. Value loss: 0.047727. Entropy: 0.300100.\n",
      "Iteration 12288: Policy loss: 0.025498. Value loss: 0.033533. Entropy: 0.298916.\n",
      "episode: 4482   score: 315.0  epsilon: 1.0    steps: 848  evaluation reward: 445.65\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12289: Policy loss: -0.348308. Value loss: 0.224780. Entropy: 0.304331.\n",
      "Iteration 12290: Policy loss: -0.360136. Value loss: 0.078089. Entropy: 0.304673.\n",
      "Iteration 12291: Policy loss: -0.370276. Value loss: 0.058946. Entropy: 0.304613.\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12292: Policy loss: -0.638808. Value loss: 0.422358. Entropy: 0.309490.\n",
      "Iteration 12293: Policy loss: -0.647840. Value loss: 0.174046. Entropy: 0.310084.\n",
      "Iteration 12294: Policy loss: -0.660954. Value loss: 0.078308. Entropy: 0.309741.\n",
      "episode: 4483   score: 650.0  epsilon: 1.0    steps: 104  evaluation reward: 448.2\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12295: Policy loss: 0.240810. Value loss: 0.121681. Entropy: 0.305310.\n",
      "Iteration 12296: Policy loss: 0.237104. Value loss: 0.049436. Entropy: 0.305720.\n",
      "Iteration 12297: Policy loss: 0.237361. Value loss: 0.031348. Entropy: 0.304686.\n",
      "episode: 4484   score: 740.0  epsilon: 1.0    steps: 600  evaluation reward: 452.9\n",
      "episode: 4485   score: 535.0  epsilon: 1.0    steps: 632  evaluation reward: 453.1\n",
      "Training network. lr: 0.000156. clip: 0.062371\n",
      "Iteration 12298: Policy loss: -0.247695. Value loss: 0.321281. Entropy: 0.294137.\n",
      "Iteration 12299: Policy loss: -0.237153. Value loss: 0.131025. Entropy: 0.297276.\n",
      "Iteration 12300: Policy loss: -0.268039. Value loss: 0.061007. Entropy: 0.295107.\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12301: Policy loss: 0.422638. Value loss: 0.226710. Entropy: 0.308646.\n",
      "Iteration 12302: Policy loss: 0.398269. Value loss: 0.076700. Entropy: 0.307918.\n",
      "Iteration 12303: Policy loss: 0.402418. Value loss: 0.051378. Entropy: 0.307249.\n",
      "episode: 4486   score: 575.0  epsilon: 1.0    steps: 904  evaluation reward: 456.0\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12304: Policy loss: 0.308653. Value loss: 0.127084. Entropy: 0.303046.\n",
      "Iteration 12305: Policy loss: 0.295077. Value loss: 0.045731. Entropy: 0.303304.\n",
      "Iteration 12306: Policy loss: 0.299006. Value loss: 0.028115. Entropy: 0.302948.\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12307: Policy loss: -0.090171. Value loss: 0.155893. Entropy: 0.308545.\n",
      "Iteration 12308: Policy loss: -0.092888. Value loss: 0.059348. Entropy: 0.308154.\n",
      "Iteration 12309: Policy loss: -0.096394. Value loss: 0.037816. Entropy: 0.309238.\n",
      "episode: 4487   score: 365.0  epsilon: 1.0    steps: 376  evaluation reward: 456.15\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12310: Policy loss: 0.231411. Value loss: 0.181905. Entropy: 0.302916.\n",
      "Iteration 12311: Policy loss: 0.217698. Value loss: 0.070933. Entropy: 0.302621.\n",
      "Iteration 12312: Policy loss: 0.216726. Value loss: 0.049997. Entropy: 0.303095.\n",
      "episode: 4488   score: 605.0  epsilon: 1.0    steps: 80  evaluation reward: 456.8\n",
      "episode: 4489   score: 530.0  epsilon: 1.0    steps: 480  evaluation reward: 458.2\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12313: Policy loss: 0.211434. Value loss: 0.157612. Entropy: 0.288523.\n",
      "Iteration 12314: Policy loss: 0.200183. Value loss: 0.055882. Entropy: 0.287805.\n",
      "Iteration 12315: Policy loss: 0.201649. Value loss: 0.039213. Entropy: 0.286855.\n",
      "episode: 4490   score: 370.0  epsilon: 1.0    steps: 416  evaluation reward: 456.65\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12316: Policy loss: 0.194560. Value loss: 0.114535. Entropy: 0.297424.\n",
      "Iteration 12317: Policy loss: 0.192297. Value loss: 0.037402. Entropy: 0.296738.\n",
      "Iteration 12318: Policy loss: 0.182915. Value loss: 0.026434. Entropy: 0.297488.\n",
      "episode: 4491   score: 330.0  epsilon: 1.0    steps: 840  evaluation reward: 455.0\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12319: Policy loss: -0.153900. Value loss: 0.331296. Entropy: 0.306619.\n",
      "Iteration 12320: Policy loss: -0.167342. Value loss: 0.156625. Entropy: 0.306151.\n",
      "Iteration 12321: Policy loss: -0.144235. Value loss: 0.080571. Entropy: 0.306777.\n",
      "episode: 4492   score: 385.0  epsilon: 1.0    steps: 360  evaluation reward: 454.8\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12322: Policy loss: -0.052522. Value loss: 0.079376. Entropy: 0.308064.\n",
      "Iteration 12323: Policy loss: -0.058615. Value loss: 0.044300. Entropy: 0.307519.\n",
      "Iteration 12324: Policy loss: -0.060292. Value loss: 0.034086. Entropy: 0.306495.\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12325: Policy loss: 0.102177. Value loss: 0.210437. Entropy: 0.308762.\n",
      "Iteration 12326: Policy loss: 0.082102. Value loss: 0.068311. Entropy: 0.308905.\n",
      "Iteration 12327: Policy loss: 0.064462. Value loss: 0.042770. Entropy: 0.308951.\n",
      "episode: 4493   score: 415.0  epsilon: 1.0    steps: 592  evaluation reward: 454.75\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12328: Policy loss: 0.342194. Value loss: 0.155706. Entropy: 0.303612.\n",
      "Iteration 12329: Policy loss: 0.344107. Value loss: 0.048126. Entropy: 0.300520.\n",
      "Iteration 12330: Policy loss: 0.325896. Value loss: 0.029333. Entropy: 0.300231.\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12331: Policy loss: -0.209581. Value loss: 0.197514. Entropy: 0.313584.\n",
      "Iteration 12332: Policy loss: -0.214826. Value loss: 0.075619. Entropy: 0.314031.\n",
      "Iteration 12333: Policy loss: -0.226027. Value loss: 0.051185. Entropy: 0.312915.\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12334: Policy loss: -0.477510. Value loss: 0.352353. Entropy: 0.306207.\n",
      "Iteration 12335: Policy loss: -0.494242. Value loss: 0.190408. Entropy: 0.308441.\n",
      "Iteration 12336: Policy loss: -0.496840. Value loss: 0.125234. Entropy: 0.308075.\n",
      "episode: 4494   score: 375.0  epsilon: 1.0    steps: 984  evaluation reward: 454.65\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12337: Policy loss: 0.350665. Value loss: 0.208526. Entropy: 0.312198.\n",
      "Iteration 12338: Policy loss: 0.350903. Value loss: 0.067550. Entropy: 0.313675.\n",
      "Iteration 12339: Policy loss: 0.334756. Value loss: 0.041741. Entropy: 0.312890.\n",
      "episode: 4495   score: 500.0  epsilon: 1.0    steps: 568  evaluation reward: 453.3\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12340: Policy loss: -0.077314. Value loss: 0.109462. Entropy: 0.290121.\n",
      "Iteration 12341: Policy loss: -0.089517. Value loss: 0.048330. Entropy: 0.289960.\n",
      "Iteration 12342: Policy loss: -0.090374. Value loss: 0.029886. Entropy: 0.287120.\n",
      "episode: 4496   score: 390.0  epsilon: 1.0    steps: 648  evaluation reward: 454.3\n",
      "episode: 4497   score: 575.0  epsilon: 1.0    steps: 1008  evaluation reward: 451.05\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12343: Policy loss: 0.373967. Value loss: 0.198656. Entropy: 0.294554.\n",
      "Iteration 12344: Policy loss: 0.380542. Value loss: 0.070454. Entropy: 0.294614.\n",
      "Iteration 12345: Policy loss: 0.363375. Value loss: 0.052414. Entropy: 0.294036.\n",
      "episode: 4498   score: 285.0  epsilon: 1.0    steps: 240  evaluation reward: 451.3\n",
      "episode: 4499   score: 755.0  epsilon: 1.0    steps: 264  evaluation reward: 456.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12346: Policy loss: 0.040691. Value loss: 0.085056. Entropy: 0.271007.\n",
      "Iteration 12347: Policy loss: 0.040461. Value loss: 0.037206. Entropy: 0.267648.\n",
      "Iteration 12348: Policy loss: 0.041275. Value loss: 0.029004. Entropy: 0.268819.\n",
      "episode: 4500   score: 650.0  epsilon: 1.0    steps: 472  evaluation reward: 457.55\n",
      "Training network. lr: 0.000156. clip: 0.062224\n",
      "Iteration 12349: Policy loss: 0.097470. Value loss: 0.057478. Entropy: 0.291893.\n",
      "Iteration 12350: Policy loss: 0.100162. Value loss: 0.029802. Entropy: 0.293164.\n",
      "Iteration 12351: Policy loss: 0.100383. Value loss: 0.020285. Entropy: 0.292506.\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12352: Policy loss: -0.178563. Value loss: 0.321462. Entropy: 0.309092.\n",
      "Iteration 12353: Policy loss: -0.187944. Value loss: 0.099161. Entropy: 0.309138.\n",
      "Iteration 12354: Policy loss: -0.201035. Value loss: 0.053099. Entropy: 0.308041.\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12355: Policy loss: -0.024821. Value loss: 0.231764. Entropy: 0.308128.\n",
      "Iteration 12356: Policy loss: -0.013085. Value loss: 0.082982. Entropy: 0.305476.\n",
      "Iteration 12357: Policy loss: -0.013880. Value loss: 0.053500. Entropy: 0.304945.\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12358: Policy loss: -0.002451. Value loss: 0.134908. Entropy: 0.311557.\n",
      "Iteration 12359: Policy loss: -0.010394. Value loss: 0.049466. Entropy: 0.309988.\n",
      "Iteration 12360: Policy loss: -0.017040. Value loss: 0.028800. Entropy: 0.309080.\n",
      "now time :  2019-09-06 03:02:04.688677\n",
      "episode: 4501   score: 390.0  epsilon: 1.0    steps: 608  evaluation reward: 457.05\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12361: Policy loss: -0.076617. Value loss: 0.165739. Entropy: 0.294168.\n",
      "Iteration 12362: Policy loss: -0.081450. Value loss: 0.073940. Entropy: 0.294482.\n",
      "Iteration 12363: Policy loss: -0.088771. Value loss: 0.048657. Entropy: 0.294583.\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12364: Policy loss: 0.179202. Value loss: 0.078120. Entropy: 0.308335.\n",
      "Iteration 12365: Policy loss: 0.177748. Value loss: 0.030586. Entropy: 0.307962.\n",
      "Iteration 12366: Policy loss: 0.173672. Value loss: 0.020129. Entropy: 0.306930.\n",
      "episode: 4502   score: 515.0  epsilon: 1.0    steps: 736  evaluation reward: 458.75\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12367: Policy loss: 0.137036. Value loss: 0.071566. Entropy: 0.292090.\n",
      "Iteration 12368: Policy loss: 0.124855. Value loss: 0.026976. Entropy: 0.293761.\n",
      "Iteration 12369: Policy loss: 0.126515. Value loss: 0.018764. Entropy: 0.294378.\n",
      "episode: 4503   score: 330.0  epsilon: 1.0    steps: 64  evaluation reward: 456.1\n",
      "episode: 4504   score: 470.0  epsilon: 1.0    steps: 872  evaluation reward: 454.75\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12370: Policy loss: -0.075983. Value loss: 0.148155. Entropy: 0.294114.\n",
      "Iteration 12371: Policy loss: -0.075673. Value loss: 0.050392. Entropy: 0.293989.\n",
      "Iteration 12372: Policy loss: -0.090121. Value loss: 0.037078. Entropy: 0.294690.\n",
      "episode: 4505   score: 390.0  epsilon: 1.0    steps: 264  evaluation reward: 454.75\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12373: Policy loss: 0.136330. Value loss: 0.079816. Entropy: 0.289521.\n",
      "Iteration 12374: Policy loss: 0.132098. Value loss: 0.039652. Entropy: 0.289005.\n",
      "Iteration 12375: Policy loss: 0.130384. Value loss: 0.032142. Entropy: 0.288347.\n",
      "episode: 4506   score: 495.0  epsilon: 1.0    steps: 744  evaluation reward: 455.3\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12376: Policy loss: 0.000926. Value loss: 0.085820. Entropy: 0.295103.\n",
      "Iteration 12377: Policy loss: -0.000525. Value loss: 0.033037. Entropy: 0.297263.\n",
      "Iteration 12378: Policy loss: -0.006657. Value loss: 0.024221. Entropy: 0.296693.\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12379: Policy loss: 0.039458. Value loss: 0.137128. Entropy: 0.307713.\n",
      "Iteration 12380: Policy loss: 0.026086. Value loss: 0.042337. Entropy: 0.306462.\n",
      "Iteration 12381: Policy loss: 0.022522. Value loss: 0.027523. Entropy: 0.306542.\n",
      "episode: 4507   score: 760.0  epsilon: 1.0    steps: 496  evaluation reward: 454.8\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12382: Policy loss: 0.142393. Value loss: 0.088847. Entropy: 0.292352.\n",
      "Iteration 12383: Policy loss: 0.136339. Value loss: 0.028604. Entropy: 0.295487.\n",
      "Iteration 12384: Policy loss: 0.138984. Value loss: 0.022353. Entropy: 0.293860.\n",
      "episode: 4508   score: 360.0  epsilon: 1.0    steps: 272  evaluation reward: 452.65\n",
      "episode: 4509   score: 665.0  epsilon: 1.0    steps: 440  evaluation reward: 452.8\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12385: Policy loss: -0.115199. Value loss: 0.333984. Entropy: 0.280114.\n",
      "Iteration 12386: Policy loss: -0.106841. Value loss: 0.126890. Entropy: 0.280533.\n",
      "Iteration 12387: Policy loss: -0.134515. Value loss: 0.074943. Entropy: 0.281250.\n",
      "episode: 4510   score: 415.0  epsilon: 1.0    steps: 128  evaluation reward: 452.8\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12388: Policy loss: 0.007865. Value loss: 0.080791. Entropy: 0.288572.\n",
      "Iteration 12389: Policy loss: 0.004543. Value loss: 0.037025. Entropy: 0.288981.\n",
      "Iteration 12390: Policy loss: 0.000420. Value loss: 0.025030. Entropy: 0.287398.\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12391: Policy loss: 0.174046. Value loss: 0.187240. Entropy: 0.305090.\n",
      "Iteration 12392: Policy loss: 0.167726. Value loss: 0.046529. Entropy: 0.303888.\n",
      "Iteration 12393: Policy loss: 0.151487. Value loss: 0.037824. Entropy: 0.304057.\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12394: Policy loss: 0.105347. Value loss: 0.057150. Entropy: 0.303295.\n",
      "Iteration 12395: Policy loss: 0.100601. Value loss: 0.029246. Entropy: 0.303209.\n",
      "Iteration 12396: Policy loss: 0.094257. Value loss: 0.022728. Entropy: 0.303534.\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12397: Policy loss: 0.014379. Value loss: 0.057431. Entropy: 0.313789.\n",
      "Iteration 12398: Policy loss: 0.006547. Value loss: 0.021389. Entropy: 0.313426.\n",
      "Iteration 12399: Policy loss: 0.005740. Value loss: 0.015595. Entropy: 0.313151.\n",
      "episode: 4511   score: 420.0  epsilon: 1.0    steps: 552  evaluation reward: 453.7\n",
      "Training network. lr: 0.000155. clip: 0.062067\n",
      "Iteration 12400: Policy loss: -0.125613. Value loss: 0.077854. Entropy: 0.302645.\n",
      "Iteration 12401: Policy loss: -0.128367. Value loss: 0.038999. Entropy: 0.304484.\n",
      "Iteration 12402: Policy loss: -0.133143. Value loss: 0.028722. Entropy: 0.304477.\n",
      "episode: 4512   score: 645.0  epsilon: 1.0    steps: 104  evaluation reward: 455.15\n",
      "episode: 4513   score: 425.0  epsilon: 1.0    steps: 488  evaluation reward: 456.65\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12403: Policy loss: 0.129605. Value loss: 0.155085. Entropy: 0.294036.\n",
      "Iteration 12404: Policy loss: 0.120823. Value loss: 0.057738. Entropy: 0.296326.\n",
      "Iteration 12405: Policy loss: 0.117427. Value loss: 0.042452. Entropy: 0.293185.\n",
      "episode: 4514   score: 320.0  epsilon: 1.0    steps: 552  evaluation reward: 456.55\n",
      "episode: 4515   score: 365.0  epsilon: 1.0    steps: 616  evaluation reward: 453.95\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12406: Policy loss: 0.135407. Value loss: 0.119780. Entropy: 0.292690.\n",
      "Iteration 12407: Policy loss: 0.115159. Value loss: 0.044146. Entropy: 0.292907.\n",
      "Iteration 12408: Policy loss: 0.114508. Value loss: 0.036757. Entropy: 0.292915.\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12409: Policy loss: 0.041545. Value loss: 0.075203. Entropy: 0.312500.\n",
      "Iteration 12410: Policy loss: 0.034185. Value loss: 0.033217. Entropy: 0.311151.\n",
      "Iteration 12411: Policy loss: 0.032963. Value loss: 0.026472. Entropy: 0.310848.\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12412: Policy loss: -0.521150. Value loss: 0.417937. Entropy: 0.306405.\n",
      "Iteration 12413: Policy loss: -0.507902. Value loss: 0.182088. Entropy: 0.297309.\n",
      "Iteration 12414: Policy loss: -0.534682. Value loss: 0.078363. Entropy: 0.304489.\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12415: Policy loss: -0.073323. Value loss: 0.269396. Entropy: 0.312768.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12416: Policy loss: -0.093547. Value loss: 0.100291. Entropy: 0.313509.\n",
      "Iteration 12417: Policy loss: -0.103700. Value loss: 0.068779. Entropy: 0.313520.\n",
      "episode: 4516   score: 725.0  epsilon: 1.0    steps: 736  evaluation reward: 458.8\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12418: Policy loss: 0.167665. Value loss: 0.163623. Entropy: 0.305465.\n",
      "Iteration 12419: Policy loss: 0.155014. Value loss: 0.055395. Entropy: 0.304825.\n",
      "Iteration 12420: Policy loss: 0.157084. Value loss: 0.039206. Entropy: 0.305937.\n",
      "episode: 4517   score: 415.0  epsilon: 1.0    steps: 160  evaluation reward: 456.6\n",
      "episode: 4518   score: 525.0  epsilon: 1.0    steps: 216  evaluation reward: 455.5\n",
      "episode: 4519   score: 525.0  epsilon: 1.0    steps: 992  evaluation reward: 458.6\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12421: Policy loss: -0.017607. Value loss: 0.128283. Entropy: 0.295858.\n",
      "Iteration 12422: Policy loss: -0.024024. Value loss: 0.053556. Entropy: 0.293669.\n",
      "Iteration 12423: Policy loss: -0.027124. Value loss: 0.045721. Entropy: 0.294500.\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12424: Policy loss: 0.243155. Value loss: 0.152657. Entropy: 0.297114.\n",
      "Iteration 12425: Policy loss: 0.230377. Value loss: 0.078124. Entropy: 0.294396.\n",
      "Iteration 12426: Policy loss: 0.224136. Value loss: 0.055987. Entropy: 0.293880.\n",
      "episode: 4520   score: 650.0  epsilon: 1.0    steps: 704  evaluation reward: 462.5\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12427: Policy loss: 0.179441. Value loss: 0.068899. Entropy: 0.288804.\n",
      "Iteration 12428: Policy loss: 0.175203. Value loss: 0.029364. Entropy: 0.291788.\n",
      "Iteration 12429: Policy loss: 0.166752. Value loss: 0.021045. Entropy: 0.291159.\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12430: Policy loss: 0.161923. Value loss: 0.099459. Entropy: 0.310190.\n",
      "Iteration 12431: Policy loss: 0.153641. Value loss: 0.036833. Entropy: 0.308688.\n",
      "Iteration 12432: Policy loss: 0.150838. Value loss: 0.023643. Entropy: 0.309900.\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12433: Policy loss: 0.114819. Value loss: 0.076300. Entropy: 0.309250.\n",
      "Iteration 12434: Policy loss: 0.110831. Value loss: 0.023517. Entropy: 0.308277.\n",
      "Iteration 12435: Policy loss: 0.107277. Value loss: 0.016104. Entropy: 0.309021.\n",
      "episode: 4521   score: 285.0  epsilon: 1.0    steps: 576  evaluation reward: 459.1\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12436: Policy loss: -0.108457. Value loss: 0.320535. Entropy: 0.300446.\n",
      "Iteration 12437: Policy loss: -0.114777. Value loss: 0.164944. Entropy: 0.299754.\n",
      "Iteration 12438: Policy loss: -0.127998. Value loss: 0.120220. Entropy: 0.299643.\n",
      "episode: 4522   score: 620.0  epsilon: 1.0    steps: 560  evaluation reward: 460.95\n",
      "episode: 4523   score: 755.0  epsilon: 1.0    steps: 920  evaluation reward: 464.0\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12439: Policy loss: 0.068006. Value loss: 0.176979. Entropy: 0.291521.\n",
      "Iteration 12440: Policy loss: 0.054260. Value loss: 0.068822. Entropy: 0.291905.\n",
      "Iteration 12441: Policy loss: 0.052288. Value loss: 0.045856. Entropy: 0.290631.\n",
      "episode: 4524   score: 320.0  epsilon: 1.0    steps: 360  evaluation reward: 461.0\n",
      "episode: 4525   score: 345.0  epsilon: 1.0    steps: 872  evaluation reward: 458.55\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12442: Policy loss: 0.082069. Value loss: 0.115545. Entropy: 0.284559.\n",
      "Iteration 12443: Policy loss: 0.082557. Value loss: 0.053115. Entropy: 0.283727.\n",
      "Iteration 12444: Policy loss: 0.076755. Value loss: 0.040621. Entropy: 0.284079.\n",
      "episode: 4526   score: 420.0  epsilon: 1.0    steps: 304  evaluation reward: 457.1\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12445: Policy loss: -0.117619. Value loss: 0.289545. Entropy: 0.286715.\n",
      "Iteration 12446: Policy loss: -0.116684. Value loss: 0.095654. Entropy: 0.290342.\n",
      "Iteration 12447: Policy loss: -0.133103. Value loss: 0.051594. Entropy: 0.288545.\n",
      "Training network. lr: 0.000155. clip: 0.061910\n",
      "Iteration 12448: Policy loss: -0.110830. Value loss: 0.094246. Entropy: 0.308680.\n",
      "Iteration 12449: Policy loss: -0.115419. Value loss: 0.033146. Entropy: 0.307579.\n",
      "Iteration 12450: Policy loss: -0.113978. Value loss: 0.024719. Entropy: 0.307358.\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12451: Policy loss: -0.242862. Value loss: 0.328868. Entropy: 0.309872.\n",
      "Iteration 12452: Policy loss: -0.289065. Value loss: 0.215326. Entropy: 0.310663.\n",
      "Iteration 12453: Policy loss: -0.290994. Value loss: 0.157278. Entropy: 0.310065.\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12454: Policy loss: 0.208646. Value loss: 0.119253. Entropy: 0.318375.\n",
      "Iteration 12455: Policy loss: 0.205910. Value loss: 0.042643. Entropy: 0.319291.\n",
      "Iteration 12456: Policy loss: 0.207139. Value loss: 0.029111. Entropy: 0.319538.\n",
      "episode: 4527   score: 405.0  epsilon: 1.0    steps: 576  evaluation reward: 455.85\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12457: Policy loss: 0.191149. Value loss: 0.150974. Entropy: 0.297302.\n",
      "Iteration 12458: Policy loss: 0.183926. Value loss: 0.061824. Entropy: 0.296084.\n",
      "Iteration 12459: Policy loss: 0.179935. Value loss: 0.038718. Entropy: 0.296492.\n",
      "episode: 4528   score: 620.0  epsilon: 1.0    steps: 416  evaluation reward: 457.9\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12460: Policy loss: 0.223145. Value loss: 0.080820. Entropy: 0.297478.\n",
      "Iteration 12461: Policy loss: 0.216729. Value loss: 0.041620. Entropy: 0.297910.\n",
      "Iteration 12462: Policy loss: 0.210684. Value loss: 0.031331. Entropy: 0.296395.\n",
      "episode: 4529   score: 260.0  epsilon: 1.0    steps: 176  evaluation reward: 456.3\n",
      "episode: 4530   score: 470.0  epsilon: 1.0    steps: 952  evaluation reward: 456.8\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12463: Policy loss: 0.222440. Value loss: 0.143350. Entropy: 0.289187.\n",
      "Iteration 12464: Policy loss: 0.219758. Value loss: 0.058895. Entropy: 0.291307.\n",
      "Iteration 12465: Policy loss: 0.212105. Value loss: 0.040016. Entropy: 0.290583.\n",
      "episode: 4531   score: 405.0  epsilon: 1.0    steps: 72  evaluation reward: 455.65\n",
      "episode: 4532   score: 360.0  epsilon: 1.0    steps: 536  evaluation reward: 454.55\n",
      "episode: 4533   score: 740.0  epsilon: 1.0    steps: 864  evaluation reward: 454.2\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12466: Policy loss: 0.176695. Value loss: 0.095769. Entropy: 0.261538.\n",
      "Iteration 12467: Policy loss: 0.174161. Value loss: 0.055556. Entropy: 0.262406.\n",
      "Iteration 12468: Policy loss: 0.170406. Value loss: 0.047335. Entropy: 0.261854.\n",
      "episode: 4534   score: 395.0  epsilon: 1.0    steps: 576  evaluation reward: 455.55\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12469: Policy loss: -0.041372. Value loss: 0.172269. Entropy: 0.291325.\n",
      "Iteration 12470: Policy loss: -0.046168. Value loss: 0.066035. Entropy: 0.293720.\n",
      "Iteration 12471: Policy loss: -0.042268. Value loss: 0.034736. Entropy: 0.290973.\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12472: Policy loss: -0.123728. Value loss: 0.213429. Entropy: 0.304304.\n",
      "Iteration 12473: Policy loss: -0.140409. Value loss: 0.056179. Entropy: 0.302518.\n",
      "Iteration 12474: Policy loss: -0.133161. Value loss: 0.039059. Entropy: 0.303942.\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12475: Policy loss: 0.159587. Value loss: 0.084467. Entropy: 0.313702.\n",
      "Iteration 12476: Policy loss: 0.161405. Value loss: 0.031352. Entropy: 0.312209.\n",
      "Iteration 12477: Policy loss: 0.153608. Value loss: 0.019819. Entropy: 0.312482.\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12478: Policy loss: 0.244185. Value loss: 0.105815. Entropy: 0.311138.\n",
      "Iteration 12479: Policy loss: 0.238761. Value loss: 0.036751. Entropy: 0.309834.\n",
      "Iteration 12480: Policy loss: 0.237549. Value loss: 0.028985. Entropy: 0.309874.\n",
      "episode: 4535   score: 390.0  epsilon: 1.0    steps: 896  evaluation reward: 455.05\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12481: Policy loss: -0.019511. Value loss: 0.110554. Entropy: 0.305880.\n",
      "Iteration 12482: Policy loss: -0.028027. Value loss: 0.052994. Entropy: 0.307426.\n",
      "Iteration 12483: Policy loss: -0.031889. Value loss: 0.039649. Entropy: 0.306830.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4536   score: 480.0  epsilon: 1.0    steps: 392  evaluation reward: 455.0\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12484: Policy loss: 0.132981. Value loss: 0.149738. Entropy: 0.289158.\n",
      "Iteration 12485: Policy loss: 0.137997. Value loss: 0.061084. Entropy: 0.290737.\n",
      "Iteration 12486: Policy loss: 0.130241. Value loss: 0.040066. Entropy: 0.288868.\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12487: Policy loss: -0.098711. Value loss: 0.155011. Entropy: 0.311641.\n",
      "Iteration 12488: Policy loss: -0.100493. Value loss: 0.068104. Entropy: 0.309457.\n",
      "Iteration 12489: Policy loss: -0.100802. Value loss: 0.050997. Entropy: 0.310059.\n",
      "episode: 4537   score: 390.0  epsilon: 1.0    steps: 584  evaluation reward: 456.5\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12490: Policy loss: -0.114755. Value loss: 0.099237. Entropy: 0.300270.\n",
      "Iteration 12491: Policy loss: -0.124677. Value loss: 0.048712. Entropy: 0.300868.\n",
      "Iteration 12492: Policy loss: -0.127354. Value loss: 0.033415. Entropy: 0.302123.\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12493: Policy loss: 0.007492. Value loss: 0.174453. Entropy: 0.313555.\n",
      "Iteration 12494: Policy loss: -0.003327. Value loss: 0.048623. Entropy: 0.312280.\n",
      "Iteration 12495: Policy loss: 0.007898. Value loss: 0.026164. Entropy: 0.310477.\n",
      "episode: 4538   score: 365.0  epsilon: 1.0    steps: 40  evaluation reward: 457.75\n",
      "episode: 4539   score: 460.0  epsilon: 1.0    steps: 384  evaluation reward: 459.0\n",
      "episode: 4540   score: 210.0  epsilon: 1.0    steps: 792  evaluation reward: 457.75\n",
      "episode: 4541   score: 590.0  epsilon: 1.0    steps: 896  evaluation reward: 460.5\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12496: Policy loss: -0.080963. Value loss: 0.181224. Entropy: 0.279628.\n",
      "Iteration 12497: Policy loss: -0.097979. Value loss: 0.075954. Entropy: 0.280033.\n",
      "Iteration 12498: Policy loss: -0.095467. Value loss: 0.054910. Entropy: 0.279951.\n",
      "episode: 4542   score: 565.0  epsilon: 1.0    steps: 248  evaluation reward: 460.8\n",
      "episode: 4543   score: 680.0  epsilon: 1.0    steps: 696  evaluation reward: 465.0\n",
      "Training network. lr: 0.000154. clip: 0.061763\n",
      "Iteration 12499: Policy loss: 0.262859. Value loss: 0.203822. Entropy: 0.297855.\n",
      "Iteration 12500: Policy loss: 0.242064. Value loss: 0.073527. Entropy: 0.298016.\n",
      "Iteration 12501: Policy loss: 0.223484. Value loss: 0.047704. Entropy: 0.298457.\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12502: Policy loss: 0.020691. Value loss: 0.127054. Entropy: 0.300606.\n",
      "Iteration 12503: Policy loss: 0.010567. Value loss: 0.063258. Entropy: 0.301779.\n",
      "Iteration 12504: Policy loss: 0.021571. Value loss: 0.038361. Entropy: 0.301005.\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12505: Policy loss: 0.171838. Value loss: 0.289106. Entropy: 0.306720.\n",
      "Iteration 12506: Policy loss: 0.178967. Value loss: 0.112702. Entropy: 0.306201.\n",
      "Iteration 12507: Policy loss: 0.167106. Value loss: 0.069909. Entropy: 0.307443.\n",
      "episode: 4544   score: 415.0  epsilon: 1.0    steps: 448  evaluation reward: 464.5\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12508: Policy loss: 0.258668. Value loss: 0.107913. Entropy: 0.310942.\n",
      "Iteration 12509: Policy loss: 0.249728. Value loss: 0.037466. Entropy: 0.309223.\n",
      "Iteration 12510: Policy loss: 0.241323. Value loss: 0.023782. Entropy: 0.309204.\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12511: Policy loss: 0.038249. Value loss: 0.093378. Entropy: 0.309394.\n",
      "Iteration 12512: Policy loss: 0.026548. Value loss: 0.043823. Entropy: 0.310968.\n",
      "Iteration 12513: Policy loss: 0.028270. Value loss: 0.033037. Entropy: 0.310310.\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12514: Policy loss: -0.168228. Value loss: 0.185773. Entropy: 0.306662.\n",
      "Iteration 12515: Policy loss: -0.180409. Value loss: 0.053912. Entropy: 0.308088.\n",
      "Iteration 12516: Policy loss: -0.214071. Value loss: 0.038360. Entropy: 0.307390.\n",
      "episode: 4545   score: 240.0  epsilon: 1.0    steps: 40  evaluation reward: 464.65\n",
      "episode: 4546   score: 280.0  epsilon: 1.0    steps: 800  evaluation reward: 462.4\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12517: Policy loss: 0.064374. Value loss: 0.118052. Entropy: 0.297584.\n",
      "Iteration 12518: Policy loss: 0.058295. Value loss: 0.056598. Entropy: 0.297680.\n",
      "Iteration 12519: Policy loss: 0.060184. Value loss: 0.039856. Entropy: 0.297228.\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12520: Policy loss: 0.026706. Value loss: 0.103315. Entropy: 0.312387.\n",
      "Iteration 12521: Policy loss: 0.022622. Value loss: 0.038666. Entropy: 0.310826.\n",
      "Iteration 12522: Policy loss: 0.018816. Value loss: 0.028329. Entropy: 0.312083.\n",
      "episode: 4547   score: 550.0  epsilon: 1.0    steps: 256  evaluation reward: 461.45\n",
      "episode: 4548   score: 345.0  epsilon: 1.0    steps: 320  evaluation reward: 460.55\n",
      "episode: 4549   score: 665.0  epsilon: 1.0    steps: 536  evaluation reward: 461.0\n",
      "episode: 4550   score: 350.0  epsilon: 1.0    steps: 680  evaluation reward: 460.85\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12523: Policy loss: -0.005920. Value loss: 0.116007. Entropy: 0.297906.\n",
      "Iteration 12524: Policy loss: 0.000151. Value loss: 0.045907. Entropy: 0.292945.\n",
      "Iteration 12525: Policy loss: -0.017061. Value loss: 0.035865. Entropy: 0.294755.\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12526: Policy loss: 0.237763. Value loss: 0.207495. Entropy: 0.306437.\n",
      "Iteration 12527: Policy loss: 0.225951. Value loss: 0.073491. Entropy: 0.308158.\n",
      "Iteration 12528: Policy loss: 0.205707. Value loss: 0.051302. Entropy: 0.307028.\n",
      "now time :  2019-09-06 03:12:29.807778\n",
      "episode: 4551   score: 275.0  epsilon: 1.0    steps: 776  evaluation reward: 461.15\n",
      "episode: 4552   score: 670.0  epsilon: 1.0    steps: 1016  evaluation reward: 465.75\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12529: Policy loss: 0.124018. Value loss: 0.181750. Entropy: 0.294981.\n",
      "Iteration 12530: Policy loss: 0.124560. Value loss: 0.086241. Entropy: 0.293497.\n",
      "Iteration 12531: Policy loss: 0.117825. Value loss: 0.062115. Entropy: 0.292720.\n",
      "episode: 4553   score: 210.0  epsilon: 1.0    steps: 648  evaluation reward: 464.25\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12532: Policy loss: 0.282414. Value loss: 0.136639. Entropy: 0.312213.\n",
      "Iteration 12533: Policy loss: 0.262334. Value loss: 0.042212. Entropy: 0.312428.\n",
      "Iteration 12534: Policy loss: 0.259533. Value loss: 0.026735. Entropy: 0.312438.\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12535: Policy loss: -0.086383. Value loss: 0.066286. Entropy: 0.307217.\n",
      "Iteration 12536: Policy loss: -0.088188. Value loss: 0.028020. Entropy: 0.305942.\n",
      "Iteration 12537: Policy loss: -0.093812. Value loss: 0.020942. Entropy: 0.305032.\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12538: Policy loss: -0.050971. Value loss: 0.343648. Entropy: 0.307378.\n",
      "Iteration 12539: Policy loss: -0.058736. Value loss: 0.136445. Entropy: 0.307625.\n",
      "Iteration 12540: Policy loss: -0.073975. Value loss: 0.083805. Entropy: 0.307641.\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12541: Policy loss: -0.347250. Value loss: 0.333676. Entropy: 0.312549.\n",
      "Iteration 12542: Policy loss: -0.343870. Value loss: 0.127145. Entropy: 0.312915.\n",
      "Iteration 12543: Policy loss: -0.355726. Value loss: 0.067050. Entropy: 0.312336.\n",
      "episode: 4554   score: 285.0  epsilon: 1.0    steps: 304  evaluation reward: 460.9\n",
      "episode: 4555   score: 260.0  epsilon: 1.0    steps: 856  evaluation reward: 460.4\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12544: Policy loss: -0.025975. Value loss: 0.098045. Entropy: 0.303585.\n",
      "Iteration 12545: Policy loss: -0.028221. Value loss: 0.038632. Entropy: 0.303586.\n",
      "Iteration 12546: Policy loss: -0.033516. Value loss: 0.026933. Entropy: 0.301494.\n",
      "episode: 4556   score: 580.0  epsilon: 1.0    steps: 32  evaluation reward: 462.55\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12547: Policy loss: -0.041504. Value loss: 0.125586. Entropy: 0.311812.\n",
      "Iteration 12548: Policy loss: -0.044918. Value loss: 0.053757. Entropy: 0.310372.\n",
      "Iteration 12549: Policy loss: -0.043644. Value loss: 0.037355. Entropy: 0.309388.\n",
      "episode: 4557   score: 365.0  epsilon: 1.0    steps: 32  evaluation reward: 462.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4558   score: 365.0  epsilon: 1.0    steps: 992  evaluation reward: 459.6\n",
      "Training network. lr: 0.000154. clip: 0.061606\n",
      "Iteration 12550: Policy loss: 0.216658. Value loss: 0.205165. Entropy: 0.297353.\n",
      "Iteration 12551: Policy loss: 0.199712. Value loss: 0.070422. Entropy: 0.296186.\n",
      "Iteration 12552: Policy loss: 0.202780. Value loss: 0.049320. Entropy: 0.297413.\n",
      "episode: 4559   score: 330.0  epsilon: 1.0    steps: 200  evaluation reward: 458.7\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12553: Policy loss: 0.277264. Value loss: 0.139604. Entropy: 0.294231.\n",
      "Iteration 12554: Policy loss: 0.272608. Value loss: 0.055881. Entropy: 0.292220.\n",
      "Iteration 12555: Policy loss: 0.265727. Value loss: 0.038827. Entropy: 0.291804.\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12556: Policy loss: -0.164473. Value loss: 0.198470. Entropy: 0.301208.\n",
      "Iteration 12557: Policy loss: -0.185269. Value loss: 0.065725. Entropy: 0.302616.\n",
      "Iteration 12558: Policy loss: -0.190652. Value loss: 0.040384. Entropy: 0.302788.\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12559: Policy loss: -0.065749. Value loss: 0.091778. Entropy: 0.309722.\n",
      "Iteration 12560: Policy loss: -0.066851. Value loss: 0.031169. Entropy: 0.308603.\n",
      "Iteration 12561: Policy loss: -0.068479. Value loss: 0.020535. Entropy: 0.309784.\n",
      "episode: 4560   score: 390.0  epsilon: 1.0    steps: 736  evaluation reward: 458.65\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12562: Policy loss: 0.058735. Value loss: 0.435989. Entropy: 0.304543.\n",
      "Iteration 12563: Policy loss: 0.041142. Value loss: 0.130783. Entropy: 0.306820.\n",
      "Iteration 12564: Policy loss: 0.049196. Value loss: 0.070071. Entropy: 0.305357.\n",
      "episode: 4561   score: 235.0  epsilon: 1.0    steps: 256  evaluation reward: 454.8\n",
      "episode: 4562   score: 650.0  epsilon: 1.0    steps: 928  evaluation reward: 457.4\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12565: Policy loss: 0.035782. Value loss: 0.168365. Entropy: 0.302134.\n",
      "Iteration 12566: Policy loss: 0.020164. Value loss: 0.070257. Entropy: 0.302373.\n",
      "Iteration 12567: Policy loss: 0.022792. Value loss: 0.047125. Entropy: 0.301348.\n",
      "episode: 4563   score: 695.0  epsilon: 1.0    steps: 600  evaluation reward: 460.4\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12568: Policy loss: -0.040750. Value loss: 0.166142. Entropy: 0.301000.\n",
      "Iteration 12569: Policy loss: -0.041177. Value loss: 0.059788. Entropy: 0.300525.\n",
      "Iteration 12570: Policy loss: -0.054561. Value loss: 0.038026. Entropy: 0.299258.\n",
      "episode: 4564   score: 135.0  epsilon: 1.0    steps: 688  evaluation reward: 455.85\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12571: Policy loss: 0.141164. Value loss: 0.143956. Entropy: 0.300317.\n",
      "Iteration 12572: Policy loss: 0.144103. Value loss: 0.063614. Entropy: 0.296877.\n",
      "Iteration 12573: Policy loss: 0.144122. Value loss: 0.044360. Entropy: 0.296652.\n",
      "episode: 4565   score: 390.0  epsilon: 1.0    steps: 368  evaluation reward: 456.3\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12574: Policy loss: 0.202553. Value loss: 0.176100. Entropy: 0.299686.\n",
      "Iteration 12575: Policy loss: 0.195926. Value loss: 0.053710. Entropy: 0.298511.\n",
      "Iteration 12576: Policy loss: 0.198699. Value loss: 0.037144. Entropy: 0.297577.\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12577: Policy loss: -0.133810. Value loss: 0.141528. Entropy: 0.308179.\n",
      "Iteration 12578: Policy loss: -0.141543. Value loss: 0.070994. Entropy: 0.309485.\n",
      "Iteration 12579: Policy loss: -0.145732. Value loss: 0.047474. Entropy: 0.309473.\n",
      "episode: 4566   score: 500.0  epsilon: 1.0    steps: 648  evaluation reward: 457.65\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12580: Policy loss: 0.268548. Value loss: 0.231937. Entropy: 0.305518.\n",
      "Iteration 12581: Policy loss: 0.277957. Value loss: 0.078547. Entropy: 0.303584.\n",
      "Iteration 12582: Policy loss: 0.259116. Value loss: 0.040823. Entropy: 0.304099.\n",
      "episode: 4567   score: 620.0  epsilon: 1.0    steps: 744  evaluation reward: 459.4\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12583: Policy loss: 0.012457. Value loss: 0.260618. Entropy: 0.304375.\n",
      "Iteration 12584: Policy loss: 0.019219. Value loss: 0.068713. Entropy: 0.304978.\n",
      "Iteration 12585: Policy loss: -0.000557. Value loss: 0.049771. Entropy: 0.303645.\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12586: Policy loss: 0.017778. Value loss: 0.149972. Entropy: 0.305738.\n",
      "Iteration 12587: Policy loss: 0.001632. Value loss: 0.064888. Entropy: 0.304774.\n",
      "Iteration 12588: Policy loss: 0.000178. Value loss: 0.048942. Entropy: 0.305944.\n",
      "episode: 4568   score: 405.0  epsilon: 1.0    steps: 24  evaluation reward: 457.55\n",
      "episode: 4569   score: 725.0  epsilon: 1.0    steps: 168  evaluation reward: 461.95\n",
      "episode: 4570   score: 390.0  epsilon: 1.0    steps: 712  evaluation reward: 461.45\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12589: Policy loss: 0.223426. Value loss: 0.165083. Entropy: 0.298450.\n",
      "Iteration 12590: Policy loss: 0.201426. Value loss: 0.075046. Entropy: 0.298737.\n",
      "Iteration 12591: Policy loss: 0.203230. Value loss: 0.050061. Entropy: 0.298911.\n",
      "episode: 4571   score: 365.0  epsilon: 1.0    steps: 336  evaluation reward: 458.75\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12592: Policy loss: 0.217697. Value loss: 0.086862. Entropy: 0.303568.\n",
      "Iteration 12593: Policy loss: 0.207635. Value loss: 0.037315. Entropy: 0.305494.\n",
      "Iteration 12594: Policy loss: 0.211910. Value loss: 0.025839. Entropy: 0.304784.\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12595: Policy loss: 0.086770. Value loss: 0.177579. Entropy: 0.306782.\n",
      "Iteration 12596: Policy loss: 0.070863. Value loss: 0.080500. Entropy: 0.307081.\n",
      "Iteration 12597: Policy loss: 0.057141. Value loss: 0.050713. Entropy: 0.306072.\n",
      "Training network. lr: 0.000154. clip: 0.061449\n",
      "Iteration 12598: Policy loss: -0.420885. Value loss: 0.431933. Entropy: 0.309663.\n",
      "Iteration 12599: Policy loss: -0.422418. Value loss: 0.185970. Entropy: 0.309168.\n",
      "Iteration 12600: Policy loss: -0.392777. Value loss: 0.097503. Entropy: 0.307936.\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12601: Policy loss: -0.321962. Value loss: 0.381480. Entropy: 0.312839.\n",
      "Iteration 12602: Policy loss: -0.334866. Value loss: 0.187628. Entropy: 0.313432.\n",
      "Iteration 12603: Policy loss: -0.350009. Value loss: 0.090212. Entropy: 0.314231.\n",
      "episode: 4572   score: 375.0  epsilon: 1.0    steps: 312  evaluation reward: 458.85\n",
      "episode: 4573   score: 415.0  epsilon: 1.0    steps: 400  evaluation reward: 456.95\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12604: Policy loss: 0.315092. Value loss: 0.272979. Entropy: 0.300777.\n",
      "Iteration 12605: Policy loss: 0.314588. Value loss: 0.105165. Entropy: 0.303171.\n",
      "Iteration 12606: Policy loss: 0.297719. Value loss: 0.062707. Entropy: 0.303181.\n",
      "episode: 4574   score: 495.0  epsilon: 1.0    steps: 232  evaluation reward: 456.65\n",
      "episode: 4575   score: 775.0  epsilon: 1.0    steps: 280  evaluation reward: 460.05\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12607: Policy loss: -0.099541. Value loss: 0.151666. Entropy: 0.302849.\n",
      "Iteration 12608: Policy loss: -0.102584. Value loss: 0.086657. Entropy: 0.302647.\n",
      "Iteration 12609: Policy loss: -0.107867. Value loss: 0.062616. Entropy: 0.302060.\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12610: Policy loss: -0.096721. Value loss: 0.129921. Entropy: 0.305779.\n",
      "Iteration 12611: Policy loss: -0.100088. Value loss: 0.062091. Entropy: 0.305491.\n",
      "Iteration 12612: Policy loss: -0.100189. Value loss: 0.044290. Entropy: 0.304196.\n",
      "episode: 4576   score: 465.0  epsilon: 1.0    steps: 152  evaluation reward: 460.1\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12613: Policy loss: 0.383064. Value loss: 0.167567. Entropy: 0.308217.\n",
      "Iteration 12614: Policy loss: 0.365632. Value loss: 0.045512. Entropy: 0.307648.\n",
      "Iteration 12615: Policy loss: 0.364269. Value loss: 0.030564. Entropy: 0.307093.\n",
      "episode: 4577   score: 655.0  epsilon: 1.0    steps: 712  evaluation reward: 463.5\n",
      "episode: 4578   score: 515.0  epsilon: 1.0    steps: 880  evaluation reward: 463.05\n",
      "Training network. lr: 0.000153. clip: 0.061302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12616: Policy loss: -0.146566. Value loss: 0.230604. Entropy: 0.304040.\n",
      "Iteration 12617: Policy loss: -0.149138. Value loss: 0.067151. Entropy: 0.303757.\n",
      "Iteration 12618: Policy loss: -0.155873. Value loss: 0.043117. Entropy: 0.304586.\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12619: Policy loss: 0.033274. Value loss: 0.245471. Entropy: 0.316503.\n",
      "Iteration 12620: Policy loss: 0.014375. Value loss: 0.127881. Entropy: 0.317201.\n",
      "Iteration 12621: Policy loss: 0.014029. Value loss: 0.106909. Entropy: 0.316241.\n",
      "episode: 4579   score: 695.0  epsilon: 1.0    steps: 288  evaluation reward: 466.4\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12622: Policy loss: 0.135709. Value loss: 0.176853. Entropy: 0.299706.\n",
      "Iteration 12623: Policy loss: 0.134510. Value loss: 0.081250. Entropy: 0.296570.\n",
      "Iteration 12624: Policy loss: 0.122406. Value loss: 0.051548. Entropy: 0.298354.\n",
      "episode: 4580   score: 285.0  epsilon: 1.0    steps: 176  evaluation reward: 465.2\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12625: Policy loss: -0.252436. Value loss: 0.308364. Entropy: 0.298281.\n",
      "Iteration 12626: Policy loss: -0.274521. Value loss: 0.219695. Entropy: 0.298838.\n",
      "Iteration 12627: Policy loss: -0.268962. Value loss: 0.190089. Entropy: 0.298758.\n",
      "episode: 4581   score: 310.0  epsilon: 1.0    steps: 304  evaluation reward: 463.5\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12628: Policy loss: 0.247852. Value loss: 0.229494. Entropy: 0.296679.\n",
      "Iteration 12629: Policy loss: 0.243405. Value loss: 0.078569. Entropy: 0.296665.\n",
      "Iteration 12630: Policy loss: 0.223077. Value loss: 0.049392. Entropy: 0.297465.\n",
      "episode: 4582   score: 635.0  epsilon: 1.0    steps: 264  evaluation reward: 466.7\n",
      "episode: 4583   score: 590.0  epsilon: 1.0    steps: 672  evaluation reward: 466.1\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12631: Policy loss: 0.038260. Value loss: 0.244212. Entropy: 0.297210.\n",
      "Iteration 12632: Policy loss: 0.010576. Value loss: 0.088194. Entropy: 0.296782.\n",
      "Iteration 12633: Policy loss: 0.008010. Value loss: 0.045328. Entropy: 0.297264.\n",
      "episode: 4584   score: 270.0  epsilon: 1.0    steps: 208  evaluation reward: 461.4\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12634: Policy loss: 0.092168. Value loss: 0.125302. Entropy: 0.305687.\n",
      "Iteration 12635: Policy loss: 0.076900. Value loss: 0.050065. Entropy: 0.306160.\n",
      "Iteration 12636: Policy loss: 0.076685. Value loss: 0.036288. Entropy: 0.305776.\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12637: Policy loss: 0.148849. Value loss: 0.131480. Entropy: 0.307319.\n",
      "Iteration 12638: Policy loss: 0.137521. Value loss: 0.052302. Entropy: 0.306546.\n",
      "Iteration 12639: Policy loss: 0.132767. Value loss: 0.035003. Entropy: 0.306411.\n",
      "episode: 4585   score: 315.0  epsilon: 1.0    steps: 128  evaluation reward: 459.2\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12640: Policy loss: 0.216106. Value loss: 0.135386. Entropy: 0.306535.\n",
      "Iteration 12641: Policy loss: 0.207690. Value loss: 0.050236. Entropy: 0.306278.\n",
      "Iteration 12642: Policy loss: 0.199793. Value loss: 0.034203. Entropy: 0.306273.\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12643: Policy loss: 0.510024. Value loss: 0.287969. Entropy: 0.312103.\n",
      "Iteration 12644: Policy loss: 0.478771. Value loss: 0.072379. Entropy: 0.311857.\n",
      "Iteration 12645: Policy loss: 0.463199. Value loss: 0.040112. Entropy: 0.311647.\n",
      "episode: 4586   score: 515.0  epsilon: 1.0    steps: 232  evaluation reward: 458.6\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12646: Policy loss: -0.420143. Value loss: 0.293665. Entropy: 0.305110.\n",
      "Iteration 12647: Policy loss: -0.431952. Value loss: 0.164282. Entropy: 0.305648.\n",
      "Iteration 12648: Policy loss: -0.433351. Value loss: 0.122821. Entropy: 0.305678.\n",
      "episode: 4587   score: 355.0  epsilon: 1.0    steps: 480  evaluation reward: 458.5\n",
      "episode: 4588   score: 350.0  epsilon: 1.0    steps: 528  evaluation reward: 455.95\n",
      "Training network. lr: 0.000153. clip: 0.061302\n",
      "Iteration 12649: Policy loss: 0.264291. Value loss: 0.145343. Entropy: 0.297036.\n",
      "Iteration 12650: Policy loss: 0.252766. Value loss: 0.064074. Entropy: 0.296762.\n",
      "Iteration 12651: Policy loss: 0.253623. Value loss: 0.042110. Entropy: 0.296395.\n",
      "episode: 4589   score: 450.0  epsilon: 1.0    steps: 16  evaluation reward: 455.15\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12652: Policy loss: 0.217934. Value loss: 0.121296. Entropy: 0.308288.\n",
      "Iteration 12653: Policy loss: 0.201759. Value loss: 0.052827. Entropy: 0.306199.\n",
      "Iteration 12654: Policy loss: 0.208674. Value loss: 0.040855. Entropy: 0.307808.\n",
      "episode: 4590   score: 290.0  epsilon: 1.0    steps: 1008  evaluation reward: 454.35\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12655: Policy loss: 0.203574. Value loss: 0.164142. Entropy: 0.309404.\n",
      "Iteration 12656: Policy loss: 0.205930. Value loss: 0.067613. Entropy: 0.308822.\n",
      "Iteration 12657: Policy loss: 0.194375. Value loss: 0.050461. Entropy: 0.308318.\n",
      "episode: 4591   score: 365.0  epsilon: 1.0    steps: 48  evaluation reward: 454.7\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12658: Policy loss: 0.096554. Value loss: 0.115996. Entropy: 0.295142.\n",
      "Iteration 12659: Policy loss: 0.089503. Value loss: 0.042682. Entropy: 0.291417.\n",
      "Iteration 12660: Policy loss: 0.084872. Value loss: 0.032255. Entropy: 0.292255.\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12661: Policy loss: 0.222590. Value loss: 0.166012. Entropy: 0.307675.\n",
      "Iteration 12662: Policy loss: 0.199682. Value loss: 0.089846. Entropy: 0.306972.\n",
      "Iteration 12663: Policy loss: 0.204384. Value loss: 0.079746. Entropy: 0.305844.\n",
      "episode: 4592   score: 640.0  epsilon: 1.0    steps: 168  evaluation reward: 457.25\n",
      "episode: 4593   score: 420.0  epsilon: 1.0    steps: 752  evaluation reward: 457.3\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12664: Policy loss: 0.051220. Value loss: 0.068049. Entropy: 0.296529.\n",
      "Iteration 12665: Policy loss: 0.048277. Value loss: 0.035375. Entropy: 0.296432.\n",
      "Iteration 12666: Policy loss: 0.040876. Value loss: 0.028055. Entropy: 0.297294.\n",
      "episode: 4594   score: 270.0  epsilon: 1.0    steps: 936  evaluation reward: 456.25\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12667: Policy loss: -0.107563. Value loss: 0.343682. Entropy: 0.308197.\n",
      "Iteration 12668: Policy loss: -0.130965. Value loss: 0.145213. Entropy: 0.307736.\n",
      "Iteration 12669: Policy loss: -0.143342. Value loss: 0.095663. Entropy: 0.307098.\n",
      "episode: 4595   score: 395.0  epsilon: 1.0    steps: 240  evaluation reward: 455.2\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12670: Policy loss: -0.091451. Value loss: 0.058763. Entropy: 0.305126.\n",
      "Iteration 12671: Policy loss: -0.086357. Value loss: 0.030083. Entropy: 0.304768.\n",
      "Iteration 12672: Policy loss: -0.093973. Value loss: 0.023569. Entropy: 0.305398.\n",
      "episode: 4596   score: 360.0  epsilon: 1.0    steps: 512  evaluation reward: 454.9\n",
      "episode: 4597   score: 625.0  epsilon: 1.0    steps: 800  evaluation reward: 455.4\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12673: Policy loss: -0.021062. Value loss: 0.070222. Entropy: 0.299463.\n",
      "Iteration 12674: Policy loss: -0.029418. Value loss: 0.030075. Entropy: 0.300384.\n",
      "Iteration 12675: Policy loss: -0.037838. Value loss: 0.023281. Entropy: 0.298010.\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12676: Policy loss: 0.094109. Value loss: 0.168298. Entropy: 0.312917.\n",
      "Iteration 12677: Policy loss: 0.091781. Value loss: 0.068860. Entropy: 0.312456.\n",
      "Iteration 12678: Policy loss: 0.079864. Value loss: 0.046918. Entropy: 0.312017.\n",
      "episode: 4598   score: 315.0  epsilon: 1.0    steps: 368  evaluation reward: 455.7\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12679: Policy loss: 0.002351. Value loss: 0.051686. Entropy: 0.300275.\n",
      "Iteration 12680: Policy loss: -0.000738. Value loss: 0.024136. Entropy: 0.300423.\n",
      "Iteration 12681: Policy loss: -0.000229. Value loss: 0.016780. Entropy: 0.299573.\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12682: Policy loss: 0.147831. Value loss: 0.117388. Entropy: 0.311937.\n",
      "Iteration 12683: Policy loss: 0.136425. Value loss: 0.054743. Entropy: 0.311398.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12684: Policy loss: 0.131183. Value loss: 0.035379. Entropy: 0.311153.\n",
      "episode: 4599   score: 285.0  epsilon: 1.0    steps: 800  evaluation reward: 451.0\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12685: Policy loss: -0.142222. Value loss: 0.118017. Entropy: 0.305231.\n",
      "Iteration 12686: Policy loss: -0.157329. Value loss: 0.049813. Entropy: 0.306566.\n",
      "Iteration 12687: Policy loss: -0.159702. Value loss: 0.033987. Entropy: 0.305994.\n",
      "episode: 4600   score: 420.0  epsilon: 1.0    steps: 800  evaluation reward: 448.7\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12688: Policy loss: -0.193936. Value loss: 0.328763. Entropy: 0.310787.\n",
      "Iteration 12689: Policy loss: -0.195941. Value loss: 0.140331. Entropy: 0.311630.\n",
      "Iteration 12690: Policy loss: -0.234032. Value loss: 0.075527. Entropy: 0.310235.\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12691: Policy loss: -0.077899. Value loss: 0.092053. Entropy: 0.309003.\n",
      "Iteration 12692: Policy loss: -0.087127. Value loss: 0.040756. Entropy: 0.308070.\n",
      "Iteration 12693: Policy loss: -0.081236. Value loss: 0.028440. Entropy: 0.308758.\n",
      "now time :  2019-09-06 03:22:46.502197\n",
      "episode: 4601   score: 620.0  epsilon: 1.0    steps: 848  evaluation reward: 451.0\n",
      "episode: 4602   score: 365.0  epsilon: 1.0    steps: 1008  evaluation reward: 449.5\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12694: Policy loss: 0.150732. Value loss: 0.112191. Entropy: 0.309372.\n",
      "Iteration 12695: Policy loss: 0.153057. Value loss: 0.044284. Entropy: 0.308980.\n",
      "Iteration 12696: Policy loss: 0.137990. Value loss: 0.032227. Entropy: 0.308078.\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12697: Policy loss: -0.003037. Value loss: 0.098916. Entropy: 0.307224.\n",
      "Iteration 12698: Policy loss: -0.010264. Value loss: 0.036934. Entropy: 0.306871.\n",
      "Iteration 12699: Policy loss: -0.012004. Value loss: 0.022004. Entropy: 0.306763.\n",
      "episode: 4603   score: 425.0  epsilon: 1.0    steps: 880  evaluation reward: 450.45\n",
      "Training network. lr: 0.000153. clip: 0.061145\n",
      "Iteration 12700: Policy loss: 0.139646. Value loss: 0.118849. Entropy: 0.308297.\n",
      "Iteration 12701: Policy loss: 0.135406. Value loss: 0.051422. Entropy: 0.307795.\n",
      "Iteration 12702: Policy loss: 0.121225. Value loss: 0.033918. Entropy: 0.307096.\n",
      "episode: 4604   score: 370.0  epsilon: 1.0    steps: 472  evaluation reward: 449.45\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12703: Policy loss: 0.387341. Value loss: 0.089034. Entropy: 0.301655.\n",
      "Iteration 12704: Policy loss: 0.381801. Value loss: 0.029518. Entropy: 0.300210.\n",
      "Iteration 12705: Policy loss: 0.379817. Value loss: 0.023051. Entropy: 0.299612.\n",
      "episode: 4605   score: 225.0  epsilon: 1.0    steps: 536  evaluation reward: 447.8\n",
      "episode: 4606   score: 375.0  epsilon: 1.0    steps: 864  evaluation reward: 446.6\n",
      "episode: 4607   score: 575.0  epsilon: 1.0    steps: 944  evaluation reward: 444.75\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12706: Policy loss: 0.077654. Value loss: 0.083638. Entropy: 0.299439.\n",
      "Iteration 12707: Policy loss: 0.075363. Value loss: 0.040762. Entropy: 0.299442.\n",
      "Iteration 12708: Policy loss: 0.071043. Value loss: 0.031811. Entropy: 0.299943.\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12709: Policy loss: -0.093916. Value loss: 0.083018. Entropy: 0.307156.\n",
      "Iteration 12710: Policy loss: -0.102544. Value loss: 0.040001. Entropy: 0.307961.\n",
      "Iteration 12711: Policy loss: -0.099849. Value loss: 0.031199. Entropy: 0.307855.\n",
      "episode: 4608   score: 315.0  epsilon: 1.0    steps: 824  evaluation reward: 444.3\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12712: Policy loss: 0.030998. Value loss: 0.108740. Entropy: 0.305198.\n",
      "Iteration 12713: Policy loss: 0.028334. Value loss: 0.053460. Entropy: 0.303450.\n",
      "Iteration 12714: Policy loss: 0.023279. Value loss: 0.036597. Entropy: 0.303675.\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12715: Policy loss: -0.259699. Value loss: 0.363327. Entropy: 0.310321.\n",
      "Iteration 12716: Policy loss: -0.270851. Value loss: 0.258300. Entropy: 0.309511.\n",
      "Iteration 12717: Policy loss: -0.253277. Value loss: 0.180351. Entropy: 0.309260.\n",
      "episode: 4609   score: 320.0  epsilon: 1.0    steps: 392  evaluation reward: 440.85\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12718: Policy loss: -0.070112. Value loss: 0.115153. Entropy: 0.304010.\n",
      "Iteration 12719: Policy loss: -0.074625. Value loss: 0.048154. Entropy: 0.303620.\n",
      "Iteration 12720: Policy loss: -0.074630. Value loss: 0.035316. Entropy: 0.303726.\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12721: Policy loss: -0.262414. Value loss: 0.274837. Entropy: 0.312470.\n",
      "Iteration 12722: Policy loss: -0.268505. Value loss: 0.105190. Entropy: 0.312095.\n",
      "Iteration 12723: Policy loss: -0.274709. Value loss: 0.049536. Entropy: 0.313065.\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12724: Policy loss: -0.152264. Value loss: 0.318730. Entropy: 0.305878.\n",
      "Iteration 12725: Policy loss: -0.156451. Value loss: 0.148868. Entropy: 0.306244.\n",
      "Iteration 12726: Policy loss: -0.176751. Value loss: 0.076173. Entropy: 0.305223.\n",
      "episode: 4610   score: 590.0  epsilon: 1.0    steps: 576  evaluation reward: 442.6\n",
      "episode: 4611   score: 560.0  epsilon: 1.0    steps: 688  evaluation reward: 444.0\n",
      "episode: 4612   score: 545.0  epsilon: 1.0    steps: 792  evaluation reward: 443.0\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12727: Policy loss: 0.137094. Value loss: 0.093493. Entropy: 0.281246.\n",
      "Iteration 12728: Policy loss: 0.134807. Value loss: 0.036979. Entropy: 0.282037.\n",
      "Iteration 12729: Policy loss: 0.129468. Value loss: 0.025803. Entropy: 0.281977.\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12730: Policy loss: 0.017884. Value loss: 0.056762. Entropy: 0.310809.\n",
      "Iteration 12731: Policy loss: 0.010877. Value loss: 0.024722. Entropy: 0.310449.\n",
      "Iteration 12732: Policy loss: 0.005842. Value loss: 0.018175. Entropy: 0.310550.\n",
      "episode: 4613   score: 495.0  epsilon: 1.0    steps: 304  evaluation reward: 443.7\n",
      "episode: 4614   score: 285.0  epsilon: 1.0    steps: 640  evaluation reward: 443.35\n",
      "episode: 4615   score: 455.0  epsilon: 1.0    steps: 944  evaluation reward: 444.25\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12733: Policy loss: 0.067709. Value loss: 0.152407. Entropy: 0.287206.\n",
      "Iteration 12734: Policy loss: 0.056190. Value loss: 0.053029. Entropy: 0.286777.\n",
      "Iteration 12735: Policy loss: 0.052620. Value loss: 0.034667. Entropy: 0.286195.\n",
      "episode: 4616   score: 420.0  epsilon: 1.0    steps: 896  evaluation reward: 441.2\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12736: Policy loss: -0.405136. Value loss: 0.425632. Entropy: 0.302677.\n",
      "Iteration 12737: Policy loss: -0.404438. Value loss: 0.138634. Entropy: 0.302125.\n",
      "Iteration 12738: Policy loss: -0.388805. Value loss: 0.106723. Entropy: 0.301532.\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12739: Policy loss: -0.007807. Value loss: 0.200571. Entropy: 0.300699.\n",
      "Iteration 12740: Policy loss: -0.022157. Value loss: 0.089499. Entropy: 0.300106.\n",
      "Iteration 12741: Policy loss: -0.028880. Value loss: 0.062254. Entropy: 0.300872.\n",
      "episode: 4617   score: 900.0  epsilon: 1.0    steps: 96  evaluation reward: 446.05\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12742: Policy loss: 0.102417. Value loss: 0.132090. Entropy: 0.300982.\n",
      "Iteration 12743: Policy loss: 0.095477. Value loss: 0.044379. Entropy: 0.301001.\n",
      "Iteration 12744: Policy loss: 0.075043. Value loss: 0.030403. Entropy: 0.300824.\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12745: Policy loss: 0.049561. Value loss: 0.127056. Entropy: 0.309484.\n",
      "Iteration 12746: Policy loss: 0.031306. Value loss: 0.061519. Entropy: 0.309691.\n",
      "Iteration 12747: Policy loss: 0.029222. Value loss: 0.044237. Entropy: 0.309661.\n",
      "episode: 4618   score: 315.0  epsilon: 1.0    steps: 504  evaluation reward: 443.95\n",
      "Training network. lr: 0.000152. clip: 0.060989\n",
      "Iteration 12748: Policy loss: -0.388067. Value loss: 0.322386. Entropy: 0.302551.\n",
      "Iteration 12749: Policy loss: -0.396194. Value loss: 0.079439. Entropy: 0.300994.\n",
      "Iteration 12750: Policy loss: -0.397024. Value loss: 0.049851. Entropy: 0.301244.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12751: Policy loss: -0.080564. Value loss: 0.155979. Entropy: 0.307060.\n",
      "Iteration 12752: Policy loss: -0.075217. Value loss: 0.055383. Entropy: 0.307182.\n",
      "Iteration 12753: Policy loss: -0.090258. Value loss: 0.035091. Entropy: 0.305528.\n",
      "episode: 4619   score: 345.0  epsilon: 1.0    steps: 736  evaluation reward: 442.15\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12754: Policy loss: 0.333788. Value loss: 0.143880. Entropy: 0.305607.\n",
      "Iteration 12755: Policy loss: 0.324676. Value loss: 0.053926. Entropy: 0.305499.\n",
      "Iteration 12756: Policy loss: 0.309556. Value loss: 0.033181. Entropy: 0.304786.\n",
      "episode: 4620   score: 620.0  epsilon: 1.0    steps: 672  evaluation reward: 441.85\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12757: Policy loss: -0.239754. Value loss: 0.113437. Entropy: 0.307584.\n",
      "Iteration 12758: Policy loss: -0.237169. Value loss: 0.032929. Entropy: 0.307182.\n",
      "Iteration 12759: Policy loss: -0.245152. Value loss: 0.025363. Entropy: 0.307626.\n",
      "episode: 4621   score: 655.0  epsilon: 1.0    steps: 824  evaluation reward: 445.55\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12760: Policy loss: -0.009619. Value loss: 0.219599. Entropy: 0.309123.\n",
      "Iteration 12761: Policy loss: -0.029377. Value loss: 0.065965. Entropy: 0.308864.\n",
      "Iteration 12762: Policy loss: -0.042900. Value loss: 0.039432. Entropy: 0.308854.\n",
      "episode: 4622   score: 925.0  epsilon: 1.0    steps: 552  evaluation reward: 448.6\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12763: Policy loss: -0.092193. Value loss: 0.304730. Entropy: 0.297894.\n",
      "Iteration 12764: Policy loss: -0.154559. Value loss: 0.233226. Entropy: 0.299712.\n",
      "Iteration 12765: Policy loss: -0.143946. Value loss: 0.178935. Entropy: 0.299195.\n",
      "episode: 4623   score: 570.0  epsilon: 1.0    steps: 336  evaluation reward: 446.75\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12766: Policy loss: 0.250700. Value loss: 0.153898. Entropy: 0.301160.\n",
      "Iteration 12767: Policy loss: 0.233607. Value loss: 0.051347. Entropy: 0.300426.\n",
      "Iteration 12768: Policy loss: 0.229906. Value loss: 0.033293. Entropy: 0.301126.\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12769: Policy loss: -0.065045. Value loss: 0.201038. Entropy: 0.309953.\n",
      "Iteration 12770: Policy loss: -0.072115. Value loss: 0.064349. Entropy: 0.309241.\n",
      "Iteration 12771: Policy loss: -0.069477. Value loss: 0.050108. Entropy: 0.310121.\n",
      "episode: 4624   score: 390.0  epsilon: 1.0    steps: 360  evaluation reward: 447.45\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12772: Policy loss: 0.095399. Value loss: 0.083898. Entropy: 0.305778.\n",
      "Iteration 12773: Policy loss: 0.084069. Value loss: 0.035232. Entropy: 0.303002.\n",
      "Iteration 12774: Policy loss: 0.088080. Value loss: 0.028602. Entropy: 0.303749.\n",
      "episode: 4625   score: 420.0  epsilon: 1.0    steps: 80  evaluation reward: 448.2\n",
      "episode: 4626   score: 500.0  epsilon: 1.0    steps: 784  evaluation reward: 449.0\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12775: Policy loss: -0.097063. Value loss: 0.252957. Entropy: 0.298487.\n",
      "Iteration 12776: Policy loss: -0.119885. Value loss: 0.093323. Entropy: 0.299680.\n",
      "Iteration 12777: Policy loss: -0.126363. Value loss: 0.058047. Entropy: 0.300455.\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12778: Policy loss: -0.097136. Value loss: 0.238001. Entropy: 0.306612.\n",
      "Iteration 12779: Policy loss: -0.109499. Value loss: 0.052328. Entropy: 0.305294.\n",
      "Iteration 12780: Policy loss: -0.115704. Value loss: 0.048903. Entropy: 0.306459.\n",
      "episode: 4627   score: 440.0  epsilon: 1.0    steps: 376  evaluation reward: 449.35\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12781: Policy loss: 0.012765. Value loss: 0.122454. Entropy: 0.305123.\n",
      "Iteration 12782: Policy loss: 0.016601. Value loss: 0.037661. Entropy: 0.304283.\n",
      "Iteration 12783: Policy loss: 0.012255. Value loss: 0.022774. Entropy: 0.304489.\n",
      "episode: 4628   score: 335.0  epsilon: 1.0    steps: 856  evaluation reward: 446.5\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12784: Policy loss: -0.255253. Value loss: 0.422526. Entropy: 0.307737.\n",
      "Iteration 12785: Policy loss: -0.267313. Value loss: 0.195039. Entropy: 0.307816.\n",
      "Iteration 12786: Policy loss: -0.284060. Value loss: 0.116880. Entropy: 0.308085.\n",
      "episode: 4629   score: 635.0  epsilon: 1.0    steps: 760  evaluation reward: 450.25\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12787: Policy loss: 0.038201. Value loss: 0.104821. Entropy: 0.303322.\n",
      "Iteration 12788: Policy loss: 0.033915. Value loss: 0.044536. Entropy: 0.302898.\n",
      "Iteration 12789: Policy loss: 0.027988. Value loss: 0.034017. Entropy: 0.303806.\n",
      "episode: 4630   score: 620.0  epsilon: 1.0    steps: 232  evaluation reward: 451.75\n",
      "episode: 4631   score: 695.0  epsilon: 1.0    steps: 672  evaluation reward: 454.65\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12790: Policy loss: -0.023902. Value loss: 0.145257. Entropy: 0.295124.\n",
      "Iteration 12791: Policy loss: -0.028827. Value loss: 0.062886. Entropy: 0.295105.\n",
      "Iteration 12792: Policy loss: -0.036050. Value loss: 0.049468. Entropy: 0.294398.\n",
      "episode: 4632   score: 350.0  epsilon: 1.0    steps: 936  evaluation reward: 454.55\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12793: Policy loss: 0.406839. Value loss: 0.270856. Entropy: 0.305227.\n",
      "Iteration 12794: Policy loss: 0.382836. Value loss: 0.084739. Entropy: 0.305730.\n",
      "Iteration 12795: Policy loss: 0.365831. Value loss: 0.054208. Entropy: 0.303635.\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12796: Policy loss: 0.225427. Value loss: 0.118008. Entropy: 0.302303.\n",
      "Iteration 12797: Policy loss: 0.218987. Value loss: 0.059462. Entropy: 0.300074.\n",
      "Iteration 12798: Policy loss: 0.220941. Value loss: 0.043288. Entropy: 0.301009.\n",
      "episode: 4633   score: 495.0  epsilon: 1.0    steps: 728  evaluation reward: 452.1\n",
      "episode: 4634   score: 585.0  epsilon: 1.0    steps: 872  evaluation reward: 454.0\n",
      "Training network. lr: 0.000152. clip: 0.060841\n",
      "Iteration 12799: Policy loss: 0.567802. Value loss: 0.195643. Entropy: 0.298209.\n",
      "Iteration 12800: Policy loss: 0.553003. Value loss: 0.058459. Entropy: 0.296639.\n",
      "Iteration 12801: Policy loss: 0.546587. Value loss: 0.039253. Entropy: 0.297259.\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12802: Policy loss: 0.168156. Value loss: 0.108412. Entropy: 0.305302.\n",
      "Iteration 12803: Policy loss: 0.156196. Value loss: 0.044367. Entropy: 0.305085.\n",
      "Iteration 12804: Policy loss: 0.149757. Value loss: 0.033088. Entropy: 0.305166.\n",
      "episode: 4635   score: 385.0  epsilon: 1.0    steps: 288  evaluation reward: 453.95\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12805: Policy loss: -0.358983. Value loss: 0.206465. Entropy: 0.297601.\n",
      "Iteration 12806: Policy loss: -0.372088. Value loss: 0.064243. Entropy: 0.297568.\n",
      "Iteration 12807: Policy loss: -0.371257. Value loss: 0.042973. Entropy: 0.299163.\n",
      "episode: 4636   score: 290.0  epsilon: 1.0    steps: 192  evaluation reward: 452.05\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12808: Policy loss: -0.380806. Value loss: 0.236109. Entropy: 0.306116.\n",
      "Iteration 12809: Policy loss: -0.392335. Value loss: 0.095598. Entropy: 0.305677.\n",
      "Iteration 12810: Policy loss: -0.396736. Value loss: 0.066534. Entropy: 0.305725.\n",
      "episode: 4637   score: 360.0  epsilon: 1.0    steps: 512  evaluation reward: 451.75\n",
      "episode: 4638   score: 390.0  epsilon: 1.0    steps: 664  evaluation reward: 452.0\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12811: Policy loss: 0.448032. Value loss: 0.202025. Entropy: 0.299534.\n",
      "Iteration 12812: Policy loss: 0.450207. Value loss: 0.051322. Entropy: 0.299296.\n",
      "Iteration 12813: Policy loss: 0.443828. Value loss: 0.032142. Entropy: 0.298380.\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12814: Policy loss: 0.158948. Value loss: 0.125801. Entropy: 0.308876.\n",
      "Iteration 12815: Policy loss: 0.159786. Value loss: 0.041489. Entropy: 0.308916.\n",
      "Iteration 12816: Policy loss: 0.152510. Value loss: 0.028533. Entropy: 0.308180.\n",
      "episode: 4639   score: 510.0  epsilon: 1.0    steps: 16  evaluation reward: 452.5\n",
      "Training network. lr: 0.000152. clip: 0.060685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12817: Policy loss: -0.177775. Value loss: 0.332181. Entropy: 0.305527.\n",
      "Iteration 12818: Policy loss: -0.201570. Value loss: 0.224995. Entropy: 0.306289.\n",
      "Iteration 12819: Policy loss: -0.206595. Value loss: 0.167293. Entropy: 0.305842.\n",
      "episode: 4640   score: 390.0  epsilon: 1.0    steps: 408  evaluation reward: 454.3\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12820: Policy loss: 0.175916. Value loss: 0.130211. Entropy: 0.299511.\n",
      "Iteration 12821: Policy loss: 0.154646. Value loss: 0.054325. Entropy: 0.298782.\n",
      "Iteration 12822: Policy loss: 0.163828. Value loss: 0.040936. Entropy: 0.299551.\n",
      "episode: 4641   score: 525.0  epsilon: 1.0    steps: 232  evaluation reward: 453.65\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12823: Policy loss: -0.049483. Value loss: 0.174068. Entropy: 0.303182.\n",
      "Iteration 12824: Policy loss: -0.055462. Value loss: 0.065240. Entropy: 0.303348.\n",
      "Iteration 12825: Policy loss: -0.059399. Value loss: 0.042521. Entropy: 0.303287.\n",
      "episode: 4642   score: 405.0  epsilon: 1.0    steps: 440  evaluation reward: 452.05\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12826: Policy loss: 0.026230. Value loss: 0.100533. Entropy: 0.304737.\n",
      "Iteration 12827: Policy loss: 0.023100. Value loss: 0.042673. Entropy: 0.304141.\n",
      "Iteration 12828: Policy loss: 0.018095. Value loss: 0.026640. Entropy: 0.304868.\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12829: Policy loss: -0.143146. Value loss: 0.369315. Entropy: 0.309608.\n",
      "Iteration 12830: Policy loss: -0.153607. Value loss: 0.092272. Entropy: 0.310789.\n",
      "Iteration 12831: Policy loss: -0.165248. Value loss: 0.056063. Entropy: 0.310665.\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12832: Policy loss: 0.201107. Value loss: 0.228345. Entropy: 0.310067.\n",
      "Iteration 12833: Policy loss: 0.193386. Value loss: 0.062205. Entropy: 0.309156.\n",
      "Iteration 12834: Policy loss: 0.190545. Value loss: 0.033853. Entropy: 0.308362.\n",
      "episode: 4643   score: 475.0  epsilon: 1.0    steps: 240  evaluation reward: 450.0\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12835: Policy loss: 0.122572. Value loss: 0.093284. Entropy: 0.303743.\n",
      "Iteration 12836: Policy loss: 0.110151. Value loss: 0.038735. Entropy: 0.302709.\n",
      "Iteration 12837: Policy loss: 0.109617. Value loss: 0.029531. Entropy: 0.302086.\n",
      "episode: 4644   score: 285.0  epsilon: 1.0    steps: 8  evaluation reward: 448.7\n",
      "episode: 4645   score: 645.0  epsilon: 1.0    steps: 216  evaluation reward: 452.75\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12838: Policy loss: 0.146264. Value loss: 0.128333. Entropy: 0.300155.\n",
      "Iteration 12839: Policy loss: 0.133081. Value loss: 0.048815. Entropy: 0.300544.\n",
      "Iteration 12840: Policy loss: 0.133238. Value loss: 0.035097. Entropy: 0.300833.\n",
      "episode: 4646   score: 495.0  epsilon: 1.0    steps: 104  evaluation reward: 454.9\n",
      "episode: 4647   score: 720.0  epsilon: 1.0    steps: 760  evaluation reward: 456.6\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12841: Policy loss: -0.048990. Value loss: 0.150195. Entropy: 0.303837.\n",
      "Iteration 12842: Policy loss: -0.055019. Value loss: 0.080820. Entropy: 0.302620.\n",
      "Iteration 12843: Policy loss: -0.057916. Value loss: 0.055652. Entropy: 0.302748.\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12844: Policy loss: 0.319461. Value loss: 0.119238. Entropy: 0.308984.\n",
      "Iteration 12845: Policy loss: 0.315204. Value loss: 0.058826. Entropy: 0.308494.\n",
      "Iteration 12846: Policy loss: 0.301472. Value loss: 0.043699. Entropy: 0.308011.\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12847: Policy loss: -0.245110. Value loss: 0.347145. Entropy: 0.309913.\n",
      "Iteration 12848: Policy loss: -0.253889. Value loss: 0.201167. Entropy: 0.310679.\n",
      "Iteration 12849: Policy loss: -0.283273. Value loss: 0.151020. Entropy: 0.311038.\n",
      "episode: 4648   score: 345.0  epsilon: 1.0    steps: 488  evaluation reward: 456.6\n",
      "episode: 4649   score: 210.0  epsilon: 1.0    steps: 760  evaluation reward: 452.05\n",
      "Training network. lr: 0.000152. clip: 0.060685\n",
      "Iteration 12850: Policy loss: 0.278410. Value loss: 0.101804. Entropy: 0.298894.\n",
      "Iteration 12851: Policy loss: 0.270921. Value loss: 0.037193. Entropy: 0.298317.\n",
      "Iteration 12852: Policy loss: 0.278557. Value loss: 0.025179. Entropy: 0.297133.\n",
      "episode: 4650   score: 330.0  epsilon: 1.0    steps: 296  evaluation reward: 451.85\n",
      "now time :  2019-09-06 03:32:39.731391\n",
      "episode: 4651   score: 620.0  epsilon: 1.0    steps: 800  evaluation reward: 455.3\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12853: Policy loss: 0.084801. Value loss: 0.105573. Entropy: 0.305558.\n",
      "Iteration 12854: Policy loss: 0.080228. Value loss: 0.040032. Entropy: 0.304866.\n",
      "Iteration 12855: Policy loss: 0.082456. Value loss: 0.029368. Entropy: 0.304438.\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12856: Policy loss: 0.165510. Value loss: 0.109287. Entropy: 0.307505.\n",
      "Iteration 12857: Policy loss: 0.157067. Value loss: 0.041699. Entropy: 0.305749.\n",
      "Iteration 12858: Policy loss: 0.150791. Value loss: 0.027878. Entropy: 0.306370.\n",
      "episode: 4652   score: 270.0  epsilon: 1.0    steps: 64  evaluation reward: 451.3\n",
      "episode: 4653   score: 315.0  epsilon: 1.0    steps: 888  evaluation reward: 452.35\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12859: Policy loss: 0.168872. Value loss: 0.112917. Entropy: 0.300962.\n",
      "Iteration 12860: Policy loss: 0.169848. Value loss: 0.054761. Entropy: 0.300801.\n",
      "Iteration 12861: Policy loss: 0.162165. Value loss: 0.042338. Entropy: 0.300513.\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12862: Policy loss: 0.126504. Value loss: 0.052415. Entropy: 0.305815.\n",
      "Iteration 12863: Policy loss: 0.125584. Value loss: 0.022218. Entropy: 0.304582.\n",
      "Iteration 12864: Policy loss: 0.114037. Value loss: 0.015829. Entropy: 0.304705.\n",
      "episode: 4654   score: 290.0  epsilon: 1.0    steps: 688  evaluation reward: 452.4\n",
      "episode: 4655   score: 335.0  epsilon: 1.0    steps: 880  evaluation reward: 453.15\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12865: Policy loss: -0.178776. Value loss: 0.288679. Entropy: 0.299286.\n",
      "Iteration 12866: Policy loss: -0.184308. Value loss: 0.213074. Entropy: 0.299459.\n",
      "Iteration 12867: Policy loss: -0.179534. Value loss: 0.120609. Entropy: 0.297815.\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12868: Policy loss: 0.049562. Value loss: 0.088508. Entropy: 0.308094.\n",
      "Iteration 12869: Policy loss: 0.033835. Value loss: 0.039467. Entropy: 0.307337.\n",
      "Iteration 12870: Policy loss: 0.034330. Value loss: 0.028840. Entropy: 0.306903.\n",
      "episode: 4656   score: 315.0  epsilon: 1.0    steps: 872  evaluation reward: 450.5\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12871: Policy loss: -0.354511. Value loss: 0.129425. Entropy: 0.306338.\n",
      "Iteration 12872: Policy loss: -0.359620. Value loss: 0.058748. Entropy: 0.306146.\n",
      "Iteration 12873: Policy loss: -0.367066. Value loss: 0.041136. Entropy: 0.305131.\n",
      "episode: 4657   score: 360.0  epsilon: 1.0    steps: 544  evaluation reward: 450.45\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12874: Policy loss: -0.127724. Value loss: 0.138852. Entropy: 0.298566.\n",
      "Iteration 12875: Policy loss: -0.135577. Value loss: 0.047723. Entropy: 0.297195.\n",
      "Iteration 12876: Policy loss: -0.138377. Value loss: 0.034637. Entropy: 0.296869.\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12877: Policy loss: 0.424496. Value loss: 0.121324. Entropy: 0.308341.\n",
      "Iteration 12878: Policy loss: 0.411630. Value loss: 0.042725. Entropy: 0.306513.\n",
      "Iteration 12879: Policy loss: 0.408758. Value loss: 0.027879. Entropy: 0.305251.\n",
      "episode: 4658   score: 315.0  epsilon: 1.0    steps: 48  evaluation reward: 449.95\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12880: Policy loss: -0.127908. Value loss: 0.094100. Entropy: 0.303723.\n",
      "Iteration 12881: Policy loss: -0.129840. Value loss: 0.040311. Entropy: 0.303143.\n",
      "Iteration 12882: Policy loss: -0.133276. Value loss: 0.026627. Entropy: 0.303923.\n",
      "episode: 4659   score: 525.0  epsilon: 1.0    steps: 984  evaluation reward: 451.9\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12883: Policy loss: 0.300259. Value loss: 0.124038. Entropy: 0.305329.\n",
      "Iteration 12884: Policy loss: 0.286223. Value loss: 0.052949. Entropy: 0.304101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12885: Policy loss: 0.279939. Value loss: 0.036166. Entropy: 0.304389.\n",
      "episode: 4660   score: 470.0  epsilon: 1.0    steps: 24  evaluation reward: 452.7\n",
      "episode: 4661   score: 695.0  epsilon: 1.0    steps: 424  evaluation reward: 457.3\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12886: Policy loss: 0.220254. Value loss: 0.092425. Entropy: 0.297155.\n",
      "Iteration 12887: Policy loss: 0.210911. Value loss: 0.046941. Entropy: 0.296090.\n",
      "Iteration 12888: Policy loss: 0.201828. Value loss: 0.035416. Entropy: 0.295289.\n",
      "episode: 4662   score: 395.0  epsilon: 1.0    steps: 576  evaluation reward: 454.75\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12889: Policy loss: -0.215781. Value loss: 0.305529. Entropy: 0.301101.\n",
      "Iteration 12890: Policy loss: -0.236913. Value loss: 0.209992. Entropy: 0.301893.\n",
      "Iteration 12891: Policy loss: -0.249246. Value loss: 0.164611. Entropy: 0.300417.\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12892: Policy loss: 0.037967. Value loss: 0.102262. Entropy: 0.311326.\n",
      "Iteration 12893: Policy loss: 0.038491. Value loss: 0.049490. Entropy: 0.310134.\n",
      "Iteration 12894: Policy loss: 0.030855. Value loss: 0.031433. Entropy: 0.311759.\n",
      "episode: 4663   score: 240.0  epsilon: 1.0    steps: 112  evaluation reward: 450.2\n",
      "episode: 4664   score: 315.0  epsilon: 1.0    steps: 536  evaluation reward: 452.0\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12895: Policy loss: -0.081576. Value loss: 0.238246. Entropy: 0.298353.\n",
      "Iteration 12896: Policy loss: -0.062655. Value loss: 0.085920. Entropy: 0.297211.\n",
      "Iteration 12897: Policy loss: -0.101751. Value loss: 0.053219. Entropy: 0.296710.\n",
      "episode: 4665   score: 315.0  epsilon: 1.0    steps: 904  evaluation reward: 451.25\n",
      "Training network. lr: 0.000151. clip: 0.060528\n",
      "Iteration 12898: Policy loss: 0.186618. Value loss: 0.114460. Entropy: 0.305875.\n",
      "Iteration 12899: Policy loss: 0.178805. Value loss: 0.051202. Entropy: 0.304652.\n",
      "Iteration 12900: Policy loss: 0.171658. Value loss: 0.037544. Entropy: 0.304619.\n",
      "episode: 4666   score: 510.0  epsilon: 1.0    steps: 48  evaluation reward: 451.35\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12901: Policy loss: 0.044791. Value loss: 0.079788. Entropy: 0.301495.\n",
      "Iteration 12902: Policy loss: 0.040018. Value loss: 0.030280. Entropy: 0.301936.\n",
      "Iteration 12903: Policy loss: 0.034558. Value loss: 0.021836. Entropy: 0.300541.\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12904: Policy loss: 0.200654. Value loss: 0.076751. Entropy: 0.303685.\n",
      "Iteration 12905: Policy loss: 0.198545. Value loss: 0.031649. Entropy: 0.301856.\n",
      "Iteration 12906: Policy loss: 0.196805. Value loss: 0.022547. Entropy: 0.302457.\n",
      "episode: 4667   score: 175.0  epsilon: 1.0    steps: 80  evaluation reward: 446.9\n",
      "episode: 4668   score: 280.0  epsilon: 1.0    steps: 888  evaluation reward: 445.65\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12907: Policy loss: 0.022654. Value loss: 0.090191. Entropy: 0.295871.\n",
      "Iteration 12908: Policy loss: 0.011936. Value loss: 0.032440. Entropy: 0.295437.\n",
      "Iteration 12909: Policy loss: 0.007082. Value loss: 0.022639. Entropy: 0.295454.\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12910: Policy loss: 0.170775. Value loss: 0.151089. Entropy: 0.307444.\n",
      "Iteration 12911: Policy loss: 0.160000. Value loss: 0.048023. Entropy: 0.307906.\n",
      "Iteration 12912: Policy loss: 0.153383. Value loss: 0.032526. Entropy: 0.309915.\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12913: Policy loss: 0.046363. Value loss: 0.066709. Entropy: 0.307535.\n",
      "Iteration 12914: Policy loss: 0.037476. Value loss: 0.023753. Entropy: 0.306611.\n",
      "Iteration 12915: Policy loss: 0.038757. Value loss: 0.018171. Entropy: 0.306423.\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12916: Policy loss: 0.306567. Value loss: 0.196300. Entropy: 0.310092.\n",
      "Iteration 12917: Policy loss: 0.311238. Value loss: 0.062956. Entropy: 0.307715.\n",
      "Iteration 12918: Policy loss: 0.303379. Value loss: 0.040683. Entropy: 0.307643.\n",
      "episode: 4669   score: 420.0  epsilon: 1.0    steps: 216  evaluation reward: 442.6\n",
      "episode: 4670   score: 275.0  epsilon: 1.0    steps: 808  evaluation reward: 441.45\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12919: Policy loss: 0.017664. Value loss: 0.155187. Entropy: 0.293344.\n",
      "Iteration 12920: Policy loss: 0.015657. Value loss: 0.056233. Entropy: 0.292770.\n",
      "Iteration 12921: Policy loss: 0.000684. Value loss: 0.035453. Entropy: 0.293062.\n",
      "episode: 4671   score: 315.0  epsilon: 1.0    steps: 256  evaluation reward: 440.95\n",
      "episode: 4672   score: 370.0  epsilon: 1.0    steps: 608  evaluation reward: 440.9\n",
      "episode: 4673   score: 725.0  epsilon: 1.0    steps: 624  evaluation reward: 444.0\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12922: Policy loss: 0.007073. Value loss: 0.083370. Entropy: 0.283475.\n",
      "Iteration 12923: Policy loss: 0.000765. Value loss: 0.043646. Entropy: 0.282007.\n",
      "Iteration 12924: Policy loss: 0.003303. Value loss: 0.033269. Entropy: 0.284211.\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12925: Policy loss: -0.044125. Value loss: 0.114687. Entropy: 0.308079.\n",
      "Iteration 12926: Policy loss: -0.052991. Value loss: 0.055352. Entropy: 0.308243.\n",
      "Iteration 12927: Policy loss: -0.060262. Value loss: 0.038728. Entropy: 0.307764.\n",
      "episode: 4674   score: 415.0  epsilon: 1.0    steps: 520  evaluation reward: 443.2\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12928: Policy loss: -0.189600. Value loss: 0.414350. Entropy: 0.298658.\n",
      "Iteration 12929: Policy loss: -0.192130. Value loss: 0.275537. Entropy: 0.296899.\n",
      "Iteration 12930: Policy loss: -0.159651. Value loss: 0.185291. Entropy: 0.297775.\n",
      "episode: 4675   score: 365.0  epsilon: 1.0    steps: 168  evaluation reward: 439.1\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12931: Policy loss: -0.233422. Value loss: 0.266032. Entropy: 0.306871.\n",
      "Iteration 12932: Policy loss: -0.239274. Value loss: 0.173708. Entropy: 0.305045.\n",
      "Iteration 12933: Policy loss: -0.229839. Value loss: 0.123486. Entropy: 0.305322.\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12934: Policy loss: 0.129573. Value loss: 0.129011. Entropy: 0.310440.\n",
      "Iteration 12935: Policy loss: 0.118202. Value loss: 0.056115. Entropy: 0.311925.\n",
      "Iteration 12936: Policy loss: 0.109876. Value loss: 0.036076. Entropy: 0.310458.\n",
      "episode: 4676   score: 485.0  epsilon: 1.0    steps: 432  evaluation reward: 439.3\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12937: Policy loss: 0.103201. Value loss: 0.120764. Entropy: 0.301881.\n",
      "Iteration 12938: Policy loss: 0.100690. Value loss: 0.038283. Entropy: 0.301298.\n",
      "Iteration 12939: Policy loss: 0.090661. Value loss: 0.026814. Entropy: 0.299916.\n",
      "episode: 4677   score: 215.0  epsilon: 1.0    steps: 960  evaluation reward: 434.9\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12940: Policy loss: 0.455265. Value loss: 0.125630. Entropy: 0.308363.\n",
      "Iteration 12941: Policy loss: 0.452180. Value loss: 0.048045. Entropy: 0.307973.\n",
      "Iteration 12942: Policy loss: 0.442358. Value loss: 0.029231. Entropy: 0.306961.\n",
      "episode: 4678   score: 490.0  epsilon: 1.0    steps: 608  evaluation reward: 434.65\n",
      "episode: 4679   score: 310.0  epsilon: 1.0    steps: 968  evaluation reward: 430.8\n",
      "episode: 4680   score: 465.0  epsilon: 1.0    steps: 1008  evaluation reward: 432.6\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12943: Policy loss: 0.252026. Value loss: 0.163451. Entropy: 0.298597.\n",
      "Iteration 12944: Policy loss: 0.249779. Value loss: 0.068710. Entropy: 0.297109.\n",
      "Iteration 12945: Policy loss: 0.238915. Value loss: 0.046116. Entropy: 0.295920.\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12946: Policy loss: 0.200985. Value loss: 0.147608. Entropy: 0.305332.\n",
      "Iteration 12947: Policy loss: 0.184574. Value loss: 0.043175. Entropy: 0.307506.\n",
      "Iteration 12948: Policy loss: 0.188265. Value loss: 0.028768. Entropy: 0.306225.\n",
      "episode: 4681   score: 215.0  epsilon: 1.0    steps: 256  evaluation reward: 431.65\n",
      "Training network. lr: 0.000151. clip: 0.060380\n",
      "Iteration 12949: Policy loss: 0.106004. Value loss: 0.121193. Entropy: 0.303505.\n",
      "Iteration 12950: Policy loss: 0.100049. Value loss: 0.055986. Entropy: 0.302132.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12951: Policy loss: 0.089486. Value loss: 0.035042. Entropy: 0.301864.\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12952: Policy loss: -0.088193. Value loss: 0.103093. Entropy: 0.308161.\n",
      "Iteration 12953: Policy loss: -0.093219. Value loss: 0.045027. Entropy: 0.309684.\n",
      "Iteration 12954: Policy loss: -0.091804. Value loss: 0.029955. Entropy: 0.308408.\n",
      "episode: 4682   score: 545.0  epsilon: 1.0    steps: 72  evaluation reward: 430.75\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12955: Policy loss: -0.060240. Value loss: 0.057997. Entropy: 0.305954.\n",
      "Iteration 12956: Policy loss: -0.063013. Value loss: 0.027423. Entropy: 0.306637.\n",
      "Iteration 12957: Policy loss: -0.060893. Value loss: 0.020338. Entropy: 0.306661.\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12958: Policy loss: -0.089340. Value loss: 0.273387. Entropy: 0.304799.\n",
      "Iteration 12959: Policy loss: -0.090256. Value loss: 0.060808. Entropy: 0.305574.\n",
      "Iteration 12960: Policy loss: -0.083546. Value loss: 0.032267. Entropy: 0.305601.\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12961: Policy loss: 0.231975. Value loss: 0.086163. Entropy: 0.309502.\n",
      "Iteration 12962: Policy loss: 0.224911. Value loss: 0.034088. Entropy: 0.308292.\n",
      "Iteration 12963: Policy loss: 0.220139. Value loss: 0.023385. Entropy: 0.307314.\n",
      "episode: 4683   score: 450.0  epsilon: 1.0    steps: 848  evaluation reward: 429.35\n",
      "episode: 4684   score: 240.0  epsilon: 1.0    steps: 1000  evaluation reward: 429.05\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12964: Policy loss: 0.318752. Value loss: 0.150172. Entropy: 0.297396.\n",
      "Iteration 12965: Policy loss: 0.321346. Value loss: 0.049349. Entropy: 0.296350.\n",
      "Iteration 12966: Policy loss: 0.322297. Value loss: 0.036227. Entropy: 0.295632.\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12967: Policy loss: -0.110217. Value loss: 0.126676. Entropy: 0.298237.\n",
      "Iteration 12968: Policy loss: -0.123498. Value loss: 0.049299. Entropy: 0.297980.\n",
      "Iteration 12969: Policy loss: -0.124465. Value loss: 0.029608. Entropy: 0.297115.\n",
      "episode: 4685   score: 380.0  epsilon: 1.0    steps: 24  evaluation reward: 429.7\n",
      "episode: 4686   score: 330.0  epsilon: 1.0    steps: 416  evaluation reward: 427.85\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12970: Policy loss: -0.180249. Value loss: 0.072462. Entropy: 0.287686.\n",
      "Iteration 12971: Policy loss: -0.182733. Value loss: 0.034278. Entropy: 0.286728.\n",
      "Iteration 12972: Policy loss: -0.187501. Value loss: 0.025951. Entropy: 0.286736.\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12973: Policy loss: 0.107979. Value loss: 0.100828. Entropy: 0.312029.\n",
      "Iteration 12974: Policy loss: 0.096481. Value loss: 0.034301. Entropy: 0.311928.\n",
      "Iteration 12975: Policy loss: 0.102480. Value loss: 0.023598. Entropy: 0.311361.\n",
      "episode: 4687   score: 440.0  epsilon: 1.0    steps: 144  evaluation reward: 428.7\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12976: Policy loss: -0.396948. Value loss: 0.430352. Entropy: 0.297121.\n",
      "Iteration 12977: Policy loss: -0.399854. Value loss: 0.296355. Entropy: 0.295454.\n",
      "Iteration 12978: Policy loss: -0.424727. Value loss: 0.253841. Entropy: 0.295832.\n",
      "episode: 4688   score: 400.0  epsilon: 1.0    steps: 136  evaluation reward: 429.2\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12979: Policy loss: -0.318040. Value loss: 0.331856. Entropy: 0.297909.\n",
      "Iteration 12980: Policy loss: -0.335527. Value loss: 0.139666. Entropy: 0.298381.\n",
      "Iteration 12981: Policy loss: -0.343744. Value loss: 0.055881. Entropy: 0.297427.\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12982: Policy loss: 0.025296. Value loss: 0.121261. Entropy: 0.309321.\n",
      "Iteration 12983: Policy loss: 0.018341. Value loss: 0.053608. Entropy: 0.308884.\n",
      "Iteration 12984: Policy loss: 0.014830. Value loss: 0.036103. Entropy: 0.309217.\n",
      "episode: 4689   score: 620.0  epsilon: 1.0    steps: 976  evaluation reward: 430.9\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12985: Policy loss: 0.160924. Value loss: 0.140284. Entropy: 0.308474.\n",
      "Iteration 12986: Policy loss: 0.146709. Value loss: 0.053347. Entropy: 0.308360.\n",
      "Iteration 12987: Policy loss: 0.148512. Value loss: 0.033328. Entropy: 0.308186.\n",
      "episode: 4690   score: 615.0  epsilon: 1.0    steps: 1024  evaluation reward: 434.15\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12988: Policy loss: -0.139459. Value loss: 0.378842. Entropy: 0.304785.\n",
      "Iteration 12989: Policy loss: -0.146713. Value loss: 0.182642. Entropy: 0.307081.\n",
      "Iteration 12990: Policy loss: -0.168778. Value loss: 0.109698. Entropy: 0.305558.\n",
      "episode: 4691   score: 470.0  epsilon: 1.0    steps: 488  evaluation reward: 435.2\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12991: Policy loss: 0.288117. Value loss: 0.083288. Entropy: 0.297767.\n",
      "Iteration 12992: Policy loss: 0.290325. Value loss: 0.032328. Entropy: 0.297274.\n",
      "Iteration 12993: Policy loss: 0.284535. Value loss: 0.022007. Entropy: 0.297505.\n",
      "episode: 4692   score: 1080.0  epsilon: 1.0    steps: 24  evaluation reward: 439.6\n",
      "episode: 4693   score: 325.0  epsilon: 1.0    steps: 224  evaluation reward: 438.65\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12994: Policy loss: 0.164178. Value loss: 0.093097. Entropy: 0.292889.\n",
      "Iteration 12995: Policy loss: 0.164225. Value loss: 0.041277. Entropy: 0.293823.\n",
      "Iteration 12996: Policy loss: 0.163116. Value loss: 0.031967. Entropy: 0.294892.\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 12997: Policy loss: 0.151485. Value loss: 0.112485. Entropy: 0.311353.\n",
      "Iteration 12998: Policy loss: 0.145977. Value loss: 0.043461. Entropy: 0.311384.\n",
      "Iteration 12999: Policy loss: 0.143230. Value loss: 0.031127. Entropy: 0.310614.\n",
      "episode: 4694   score: 335.0  epsilon: 1.0    steps: 936  evaluation reward: 439.3\n",
      "Training network. lr: 0.000151. clip: 0.060224\n",
      "Iteration 13000: Policy loss: 0.035474. Value loss: 0.079828. Entropy: 0.298997.\n",
      "Iteration 13001: Policy loss: 0.033781. Value loss: 0.038169. Entropy: 0.300556.\n",
      "Iteration 13002: Policy loss: 0.037862. Value loss: 0.028228. Entropy: 0.300206.\n",
      "episode: 4695   score: 525.0  epsilon: 1.0    steps: 904  evaluation reward: 440.6\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13003: Policy loss: 0.307984. Value loss: 0.074491. Entropy: 0.302423.\n",
      "Iteration 13004: Policy loss: 0.309285. Value loss: 0.024561. Entropy: 0.301635.\n",
      "Iteration 13005: Policy loss: 0.299992. Value loss: 0.018767. Entropy: 0.301682.\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13006: Policy loss: 0.252208. Value loss: 0.143465. Entropy: 0.303499.\n",
      "Iteration 13007: Policy loss: 0.247021. Value loss: 0.054492. Entropy: 0.302379.\n",
      "Iteration 13008: Policy loss: 0.242950. Value loss: 0.035952. Entropy: 0.301941.\n",
      "episode: 4696   score: 270.0  epsilon: 1.0    steps: 784  evaluation reward: 439.7\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13009: Policy loss: 0.150348. Value loss: 0.165228. Entropy: 0.302010.\n",
      "Iteration 13010: Policy loss: 0.142369. Value loss: 0.074126. Entropy: 0.301565.\n",
      "Iteration 13011: Policy loss: 0.134011. Value loss: 0.054102. Entropy: 0.301313.\n",
      "episode: 4697   score: 225.0  epsilon: 1.0    steps: 768  evaluation reward: 435.7\n",
      "episode: 4698   score: 820.0  epsilon: 1.0    steps: 856  evaluation reward: 440.75\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13012: Policy loss: -0.193010. Value loss: 0.201952. Entropy: 0.298401.\n",
      "Iteration 13013: Policy loss: -0.206728. Value loss: 0.061973. Entropy: 0.297037.\n",
      "Iteration 13014: Policy loss: -0.211547. Value loss: 0.035788. Entropy: 0.297220.\n",
      "episode: 4699   score: 285.0  epsilon: 1.0    steps: 208  evaluation reward: 440.75\n",
      "episode: 4700   score: 265.0  epsilon: 1.0    steps: 496  evaluation reward: 439.2\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13015: Policy loss: 0.107634. Value loss: 0.061137. Entropy: 0.298273.\n",
      "Iteration 13016: Policy loss: 0.103156. Value loss: 0.033201. Entropy: 0.296351.\n",
      "Iteration 13017: Policy loss: 0.101795. Value loss: 0.027154. Entropy: 0.298465.\n",
      "now time :  2019-09-06 03:42:55.697032\n",
      "episode: 4701   score: 420.0  epsilon: 1.0    steps: 848  evaluation reward: 437.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13018: Policy loss: -0.115650. Value loss: 0.081771. Entropy: 0.302297.\n",
      "Iteration 13019: Policy loss: -0.120664. Value loss: 0.048369. Entropy: 0.302748.\n",
      "Iteration 13020: Policy loss: -0.120077. Value loss: 0.037881. Entropy: 0.302395.\n",
      "episode: 4702   score: 210.0  epsilon: 1.0    steps: 864  evaluation reward: 435.65\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13021: Policy loss: -0.169760. Value loss: 0.334976. Entropy: 0.298725.\n",
      "Iteration 13022: Policy loss: -0.194571. Value loss: 0.243017. Entropy: 0.298567.\n",
      "Iteration 13023: Policy loss: -0.191747. Value loss: 0.195061. Entropy: 0.298459.\n",
      "episode: 4703   score: 315.0  epsilon: 1.0    steps: 80  evaluation reward: 434.55\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13024: Policy loss: 0.283928. Value loss: 0.061490. Entropy: 0.300003.\n",
      "Iteration 13025: Policy loss: 0.275963. Value loss: 0.021085. Entropy: 0.299917.\n",
      "Iteration 13026: Policy loss: 0.269857. Value loss: 0.015123. Entropy: 0.299824.\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13027: Policy loss: -0.112362. Value loss: 0.118530. Entropy: 0.309444.\n",
      "Iteration 13028: Policy loss: -0.113957. Value loss: 0.051471. Entropy: 0.308480.\n",
      "Iteration 13029: Policy loss: -0.121044. Value loss: 0.040128. Entropy: 0.309355.\n",
      "episode: 4704   score: 260.0  epsilon: 1.0    steps: 952  evaluation reward: 433.45\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13030: Policy loss: -0.042349. Value loss: 0.101621. Entropy: 0.305342.\n",
      "Iteration 13031: Policy loss: -0.042898. Value loss: 0.051037. Entropy: 0.306153.\n",
      "Iteration 13032: Policy loss: -0.051380. Value loss: 0.038786. Entropy: 0.305423.\n",
      "episode: 4705   score: 345.0  epsilon: 1.0    steps: 408  evaluation reward: 434.65\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13033: Policy loss: 0.251011. Value loss: 0.159076. Entropy: 0.301054.\n",
      "Iteration 13034: Policy loss: 0.232520. Value loss: 0.055207. Entropy: 0.299353.\n",
      "Iteration 13035: Policy loss: 0.228943. Value loss: 0.031365. Entropy: 0.300265.\n",
      "episode: 4706   score: 360.0  epsilon: 1.0    steps: 936  evaluation reward: 434.5\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13036: Policy loss: -0.372676. Value loss: 0.365666. Entropy: 0.305026.\n",
      "Iteration 13037: Policy loss: -0.386256. Value loss: 0.232615. Entropy: 0.304523.\n",
      "Iteration 13038: Policy loss: -0.389460. Value loss: 0.161607. Entropy: 0.304519.\n",
      "episode: 4707   score: 240.0  epsilon: 1.0    steps: 56  evaluation reward: 431.15\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13039: Policy loss: 0.012580. Value loss: 0.077366. Entropy: 0.294059.\n",
      "Iteration 13040: Policy loss: 0.005852. Value loss: 0.037139. Entropy: 0.294154.\n",
      "Iteration 13041: Policy loss: 0.008199. Value loss: 0.029618. Entropy: 0.293958.\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13042: Policy loss: -0.308385. Value loss: 0.222419. Entropy: 0.310376.\n",
      "Iteration 13043: Policy loss: -0.332446. Value loss: 0.071693. Entropy: 0.311491.\n",
      "Iteration 13044: Policy loss: -0.352546. Value loss: 0.046324. Entropy: 0.311673.\n",
      "episode: 4708   score: 545.0  epsilon: 1.0    steps: 48  evaluation reward: 433.45\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13045: Policy loss: 0.212428. Value loss: 0.138954. Entropy: 0.294583.\n",
      "Iteration 13046: Policy loss: 0.195971. Value loss: 0.044119. Entropy: 0.295772.\n",
      "Iteration 13047: Policy loss: 0.196844. Value loss: 0.024214. Entropy: 0.294562.\n",
      "Training network. lr: 0.000150. clip: 0.060067\n",
      "Iteration 13048: Policy loss: -0.443679. Value loss: 0.380389. Entropy: 0.305907.\n",
      "Iteration 13049: Policy loss: -0.453874. Value loss: 0.116617. Entropy: 0.305289.\n",
      "Iteration 13050: Policy loss: -0.464572. Value loss: 0.065847. Entropy: 0.304552.\n",
      "episode: 4709   score: 525.0  epsilon: 1.0    steps: 464  evaluation reward: 435.5\n",
      "episode: 4710   score: 550.0  epsilon: 1.0    steps: 1024  evaluation reward: 435.1\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13051: Policy loss: 0.187861. Value loss: 0.196303. Entropy: 0.296852.\n",
      "Iteration 13052: Policy loss: 0.183343. Value loss: 0.098541. Entropy: 0.296371.\n",
      "Iteration 13053: Policy loss: 0.182440. Value loss: 0.064009. Entropy: 0.294598.\n",
      "episode: 4711   score: 290.0  epsilon: 1.0    steps: 288  evaluation reward: 432.4\n",
      "episode: 4712   score: 295.0  epsilon: 1.0    steps: 848  evaluation reward: 429.9\n",
      "episode: 4713   score: 360.0  epsilon: 1.0    steps: 856  evaluation reward: 428.55\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13054: Policy loss: 0.144692. Value loss: 0.094060. Entropy: 0.280970.\n",
      "Iteration 13055: Policy loss: 0.139054. Value loss: 0.050527. Entropy: 0.280421.\n",
      "Iteration 13056: Policy loss: 0.137510. Value loss: 0.031901. Entropy: 0.282886.\n",
      "episode: 4714   score: 410.0  epsilon: 1.0    steps: 264  evaluation reward: 429.8\n",
      "episode: 4715   score: 1000.0  epsilon: 1.0    steps: 872  evaluation reward: 435.25\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13057: Policy loss: -0.001802. Value loss: 0.083428. Entropy: 0.287069.\n",
      "Iteration 13058: Policy loss: -0.008004. Value loss: 0.044960. Entropy: 0.288807.\n",
      "Iteration 13059: Policy loss: -0.005792. Value loss: 0.034523. Entropy: 0.287864.\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13060: Policy loss: 0.061652. Value loss: 0.229765. Entropy: 0.305078.\n",
      "Iteration 13061: Policy loss: 0.049677. Value loss: 0.072993. Entropy: 0.304837.\n",
      "Iteration 13062: Policy loss: 0.047315. Value loss: 0.038030. Entropy: 0.305079.\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13063: Policy loss: -0.082820. Value loss: 0.244606. Entropy: 0.303821.\n",
      "Iteration 13064: Policy loss: -0.074251. Value loss: 0.075035. Entropy: 0.301399.\n",
      "Iteration 13065: Policy loss: -0.098352. Value loss: 0.053031. Entropy: 0.302175.\n",
      "episode: 4716   score: 315.0  epsilon: 1.0    steps: 104  evaluation reward: 434.2\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13066: Policy loss: 0.160488. Value loss: 0.087358. Entropy: 0.299076.\n",
      "Iteration 13067: Policy loss: 0.151440. Value loss: 0.041078. Entropy: 0.298977.\n",
      "Iteration 13068: Policy loss: 0.148986. Value loss: 0.031979. Entropy: 0.299128.\n",
      "episode: 4717   score: 210.0  epsilon: 1.0    steps: 240  evaluation reward: 427.3\n",
      "episode: 4718   score: 135.0  epsilon: 1.0    steps: 688  evaluation reward: 425.5\n",
      "episode: 4719   score: 460.0  epsilon: 1.0    steps: 864  evaluation reward: 426.65\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13069: Policy loss: 0.558045. Value loss: 0.251902. Entropy: 0.286807.\n",
      "Iteration 13070: Policy loss: 0.544498. Value loss: 0.080298. Entropy: 0.285494.\n",
      "Iteration 13071: Policy loss: 0.533894. Value loss: 0.052173. Entropy: 0.284025.\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13072: Policy loss: 0.043063. Value loss: 0.120228. Entropy: 0.302403.\n",
      "Iteration 13073: Policy loss: 0.035002. Value loss: 0.051111. Entropy: 0.301376.\n",
      "Iteration 13074: Policy loss: 0.020728. Value loss: 0.034641. Entropy: 0.301754.\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13075: Policy loss: 0.180980. Value loss: 0.131851. Entropy: 0.302702.\n",
      "Iteration 13076: Policy loss: 0.177784. Value loss: 0.064033. Entropy: 0.301402.\n",
      "Iteration 13077: Policy loss: 0.164212. Value loss: 0.047734. Entropy: 0.301043.\n",
      "episode: 4720   score: 225.0  epsilon: 1.0    steps: 296  evaluation reward: 422.7\n",
      "episode: 4721   score: 510.0  epsilon: 1.0    steps: 448  evaluation reward: 421.25\n",
      "episode: 4722   score: 345.0  epsilon: 1.0    steps: 680  evaluation reward: 415.45\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13078: Policy loss: 0.240761. Value loss: 0.107435. Entropy: 0.279947.\n",
      "Iteration 13079: Policy loss: 0.238446. Value loss: 0.042455. Entropy: 0.278883.\n",
      "Iteration 13080: Policy loss: 0.238331. Value loss: 0.027651. Entropy: 0.278933.\n",
      "episode: 4723   score: 155.0  epsilon: 1.0    steps: 752  evaluation reward: 411.3\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13081: Policy loss: 0.121602. Value loss: 0.102188. Entropy: 0.302706.\n",
      "Iteration 13082: Policy loss: 0.109518. Value loss: 0.046288. Entropy: 0.301841.\n",
      "Iteration 13083: Policy loss: 0.115239. Value loss: 0.034761. Entropy: 0.301986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13084: Policy loss: -0.264583. Value loss: 0.239852. Entropy: 0.311697.\n",
      "Iteration 13085: Policy loss: -0.275203. Value loss: 0.077849. Entropy: 0.311009.\n",
      "Iteration 13086: Policy loss: -0.285596. Value loss: 0.049208. Entropy: 0.311869.\n",
      "episode: 4724   score: 280.0  epsilon: 1.0    steps: 728  evaluation reward: 410.2\n",
      "episode: 4725   score: 255.0  epsilon: 1.0    steps: 872  evaluation reward: 408.55\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13087: Policy loss: 0.150277. Value loss: 0.151109. Entropy: 0.295523.\n",
      "Iteration 13088: Policy loss: 0.143358. Value loss: 0.065786. Entropy: 0.295962.\n",
      "Iteration 13089: Policy loss: 0.123630. Value loss: 0.046628. Entropy: 0.295520.\n",
      "episode: 4726   score: 265.0  epsilon: 1.0    steps: 72  evaluation reward: 406.2\n",
      "episode: 4727   score: 180.0  epsilon: 1.0    steps: 88  evaluation reward: 403.6\n",
      "episode: 4728   score: 665.0  epsilon: 1.0    steps: 552  evaluation reward: 406.9\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13090: Policy loss: 0.018676. Value loss: 0.042806. Entropy: 0.283447.\n",
      "Iteration 13091: Policy loss: 0.015685. Value loss: 0.026398. Entropy: 0.287322.\n",
      "Iteration 13092: Policy loss: 0.015135. Value loss: 0.023135. Entropy: 0.287086.\n",
      "episode: 4729   score: 210.0  epsilon: 1.0    steps: 1016  evaluation reward: 402.65\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13093: Policy loss: 0.025129. Value loss: 0.082119. Entropy: 0.308910.\n",
      "Iteration 13094: Policy loss: 0.024982. Value loss: 0.037396. Entropy: 0.308709.\n",
      "Iteration 13095: Policy loss: 0.022006. Value loss: 0.027351. Entropy: 0.308729.\n",
      "episode: 4730   score: 180.0  epsilon: 1.0    steps: 392  evaluation reward: 398.25\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13096: Policy loss: -0.387334. Value loss: 0.282022. Entropy: 0.299311.\n",
      "Iteration 13097: Policy loss: -0.383066. Value loss: 0.104173. Entropy: 0.300403.\n",
      "Iteration 13098: Policy loss: -0.386249. Value loss: 0.061970. Entropy: 0.299914.\n",
      "episode: 4731   score: 345.0  epsilon: 1.0    steps: 616  evaluation reward: 394.75\n",
      "Training network. lr: 0.000150. clip: 0.059920\n",
      "Iteration 13099: Policy loss: -0.054915. Value loss: 0.178105. Entropy: 0.303718.\n",
      "Iteration 13100: Policy loss: -0.057387. Value loss: 0.097080. Entropy: 0.303913.\n",
      "Iteration 13101: Policy loss: -0.059437. Value loss: 0.060845. Entropy: 0.304235.\n",
      "episode: 4732   score: 180.0  epsilon: 1.0    steps: 584  evaluation reward: 393.05\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13102: Policy loss: 0.014558. Value loss: 0.154739. Entropy: 0.299646.\n",
      "Iteration 13103: Policy loss: 0.007872. Value loss: 0.078755. Entropy: 0.300587.\n",
      "Iteration 13104: Policy loss: 0.000731. Value loss: 0.048906. Entropy: 0.299762.\n",
      "episode: 4733   score: 210.0  epsilon: 1.0    steps: 1008  evaluation reward: 390.2\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13105: Policy loss: 0.115450. Value loss: 0.093993. Entropy: 0.307074.\n",
      "Iteration 13106: Policy loss: 0.102509. Value loss: 0.036611. Entropy: 0.307429.\n",
      "Iteration 13107: Policy loss: 0.098773. Value loss: 0.023510. Entropy: 0.307268.\n",
      "episode: 4734   score: 315.0  epsilon: 1.0    steps: 768  evaluation reward: 387.5\n",
      "episode: 4735   score: 430.0  epsilon: 1.0    steps: 784  evaluation reward: 387.95\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13108: Policy loss: 0.020986. Value loss: 0.130112. Entropy: 0.297089.\n",
      "Iteration 13109: Policy loss: 0.016859. Value loss: 0.064858. Entropy: 0.298071.\n",
      "Iteration 13110: Policy loss: 0.012720. Value loss: 0.042757. Entropy: 0.297172.\n",
      "episode: 4736   score: 210.0  epsilon: 1.0    steps: 680  evaluation reward: 387.15\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13111: Policy loss: -0.102091. Value loss: 0.228054. Entropy: 0.301761.\n",
      "Iteration 13112: Policy loss: -0.118883. Value loss: 0.065177. Entropy: 0.302441.\n",
      "Iteration 13113: Policy loss: -0.118959. Value loss: 0.043897. Entropy: 0.303696.\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13114: Policy loss: 0.053391. Value loss: 0.092912. Entropy: 0.302877.\n",
      "Iteration 13115: Policy loss: 0.043412. Value loss: 0.047390. Entropy: 0.304092.\n",
      "Iteration 13116: Policy loss: 0.038967. Value loss: 0.034389. Entropy: 0.304205.\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13117: Policy loss: 0.250863. Value loss: 0.149793. Entropy: 0.306185.\n",
      "Iteration 13118: Policy loss: 0.240143. Value loss: 0.064036. Entropy: 0.305794.\n",
      "Iteration 13119: Policy loss: 0.223992. Value loss: 0.042512. Entropy: 0.305293.\n",
      "episode: 4737   score: 240.0  epsilon: 1.0    steps: 712  evaluation reward: 385.95\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13120: Policy loss: 0.264781. Value loss: 0.079691. Entropy: 0.306233.\n",
      "Iteration 13121: Policy loss: 0.257390. Value loss: 0.030414. Entropy: 0.305053.\n",
      "Iteration 13122: Policy loss: 0.254271. Value loss: 0.026685. Entropy: 0.304305.\n",
      "episode: 4738   score: 240.0  epsilon: 1.0    steps: 288  evaluation reward: 384.45\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13123: Policy loss: 0.107323. Value loss: 0.077833. Entropy: 0.306495.\n",
      "Iteration 13124: Policy loss: 0.103149. Value loss: 0.027172. Entropy: 0.305392.\n",
      "Iteration 13125: Policy loss: 0.100930. Value loss: 0.018131. Entropy: 0.305087.\n",
      "episode: 4739   score: 405.0  epsilon: 1.0    steps: 656  evaluation reward: 383.4\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13126: Policy loss: -0.279355. Value loss: 0.380851. Entropy: 0.301091.\n",
      "Iteration 13127: Policy loss: -0.304871. Value loss: 0.208075. Entropy: 0.301705.\n",
      "Iteration 13128: Policy loss: -0.309310. Value loss: 0.114421. Entropy: 0.300393.\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13129: Policy loss: -0.054122. Value loss: 0.127821. Entropy: 0.311033.\n",
      "Iteration 13130: Policy loss: -0.063946. Value loss: 0.052077. Entropy: 0.310977.\n",
      "Iteration 13131: Policy loss: -0.069881. Value loss: 0.041314. Entropy: 0.311980.\n",
      "episode: 4740   score: 320.0  epsilon: 1.0    steps: 8  evaluation reward: 382.7\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13132: Policy loss: 0.108631. Value loss: 0.140758. Entropy: 0.302911.\n",
      "Iteration 13133: Policy loss: 0.104382. Value loss: 0.050058. Entropy: 0.302145.\n",
      "Iteration 13134: Policy loss: 0.091296. Value loss: 0.034702. Entropy: 0.303193.\n",
      "episode: 4741   score: 440.0  epsilon: 1.0    steps: 64  evaluation reward: 381.85\n",
      "episode: 4742   score: 860.0  epsilon: 1.0    steps: 880  evaluation reward: 386.4\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13135: Policy loss: 0.098498. Value loss: 0.116359. Entropy: 0.302614.\n",
      "Iteration 13136: Policy loss: 0.081213. Value loss: 0.043044. Entropy: 0.301794.\n",
      "Iteration 13137: Policy loss: 0.090228. Value loss: 0.027119. Entropy: 0.300160.\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13138: Policy loss: 0.001590. Value loss: 0.058560. Entropy: 0.300592.\n",
      "Iteration 13139: Policy loss: 0.002279. Value loss: 0.023386. Entropy: 0.299996.\n",
      "Iteration 13140: Policy loss: -0.003766. Value loss: 0.019010. Entropy: 0.299867.\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13141: Policy loss: -0.229333. Value loss: 0.221149. Entropy: 0.302295.\n",
      "Iteration 13142: Policy loss: -0.232320. Value loss: 0.078149. Entropy: 0.302430.\n",
      "Iteration 13143: Policy loss: -0.241471. Value loss: 0.051952. Entropy: 0.302261.\n",
      "episode: 4743   score: 260.0  epsilon: 1.0    steps: 320  evaluation reward: 384.25\n",
      "episode: 4744   score: 470.0  epsilon: 1.0    steps: 560  evaluation reward: 386.1\n",
      "episode: 4745   score: 795.0  epsilon: 1.0    steps: 872  evaluation reward: 387.6\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13144: Policy loss: -0.090055. Value loss: 0.270463. Entropy: 0.292951.\n",
      "Iteration 13145: Policy loss: -0.103485. Value loss: 0.200662. Entropy: 0.291310.\n",
      "Iteration 13146: Policy loss: -0.122230. Value loss: 0.176518. Entropy: 0.291085.\n",
      "episode: 4746   score: 360.0  epsilon: 1.0    steps: 104  evaluation reward: 386.25\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13147: Policy loss: -0.030531. Value loss: 0.101770. Entropy: 0.301556.\n",
      "Iteration 13148: Policy loss: -0.029836. Value loss: 0.047104. Entropy: 0.300349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13149: Policy loss: -0.029995. Value loss: 0.034396. Entropy: 0.300074.\n",
      "episode: 4747   score: 390.0  epsilon: 1.0    steps: 168  evaluation reward: 382.95\n",
      "Training network. lr: 0.000149. clip: 0.059763\n",
      "Iteration 13150: Policy loss: 0.212543. Value loss: 0.102724. Entropy: 0.294686.\n",
      "Iteration 13151: Policy loss: 0.209083. Value loss: 0.049306. Entropy: 0.295433.\n",
      "Iteration 13152: Policy loss: 0.203115. Value loss: 0.033731. Entropy: 0.294744.\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13153: Policy loss: 0.310591. Value loss: 0.143064. Entropy: 0.305237.\n",
      "Iteration 13154: Policy loss: 0.314604. Value loss: 0.045054. Entropy: 0.303341.\n",
      "Iteration 13155: Policy loss: 0.291419. Value loss: 0.031795. Entropy: 0.302823.\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13156: Policy loss: -0.078737. Value loss: 0.261728. Entropy: 0.308277.\n",
      "Iteration 13157: Policy loss: -0.072828. Value loss: 0.063710. Entropy: 0.306779.\n",
      "Iteration 13158: Policy loss: -0.105549. Value loss: 0.044221. Entropy: 0.308517.\n",
      "episode: 4748   score: 80.0  epsilon: 1.0    steps: 16  evaluation reward: 380.3\n",
      "episode: 4749   score: 455.0  epsilon: 1.0    steps: 616  evaluation reward: 382.75\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13159: Policy loss: -0.029293. Value loss: 0.067689. Entropy: 0.292384.\n",
      "Iteration 13160: Policy loss: -0.034825. Value loss: 0.026592. Entropy: 0.291277.\n",
      "Iteration 13161: Policy loss: -0.042275. Value loss: 0.017246. Entropy: 0.290516.\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13162: Policy loss: 0.115582. Value loss: 0.138215. Entropy: 0.302071.\n",
      "Iteration 13163: Policy loss: 0.102836. Value loss: 0.052430. Entropy: 0.301607.\n",
      "Iteration 13164: Policy loss: 0.105369. Value loss: 0.035034. Entropy: 0.301075.\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13165: Policy loss: 0.214146. Value loss: 0.179707. Entropy: 0.309707.\n",
      "Iteration 13166: Policy loss: 0.196447. Value loss: 0.059653. Entropy: 0.310154.\n",
      "Iteration 13167: Policy loss: 0.196713. Value loss: 0.042136. Entropy: 0.310136.\n",
      "episode: 4750   score: 545.0  epsilon: 1.0    steps: 336  evaluation reward: 384.9\n",
      "now time :  2019-09-06 03:52:14.514576\n",
      "episode: 4751   score: 315.0  epsilon: 1.0    steps: 432  evaluation reward: 381.85\n",
      "episode: 4752   score: 275.0  epsilon: 1.0    steps: 640  evaluation reward: 381.9\n",
      "episode: 4753   score: 405.0  epsilon: 1.0    steps: 800  evaluation reward: 382.8\n",
      "episode: 4754   score: 435.0  epsilon: 1.0    steps: 800  evaluation reward: 384.25\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13168: Policy loss: 0.226750. Value loss: 0.146504. Entropy: 0.265691.\n",
      "Iteration 13169: Policy loss: 0.221104. Value loss: 0.065415. Entropy: 0.260616.\n",
      "Iteration 13170: Policy loss: 0.213609. Value loss: 0.047955. Entropy: 0.260283.\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13171: Policy loss: 0.054174. Value loss: 0.071115. Entropy: 0.307963.\n",
      "Iteration 13172: Policy loss: 0.049782. Value loss: 0.040507. Entropy: 0.310524.\n",
      "Iteration 13173: Policy loss: 0.039598. Value loss: 0.031887. Entropy: 0.310308.\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13174: Policy loss: -0.003676. Value loss: 0.144825. Entropy: 0.307717.\n",
      "Iteration 13175: Policy loss: -0.021018. Value loss: 0.055663. Entropy: 0.307078.\n",
      "Iteration 13176: Policy loss: -0.017029. Value loss: 0.041369. Entropy: 0.307825.\n",
      "episode: 4755   score: 820.0  epsilon: 1.0    steps: 632  evaluation reward: 389.1\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13177: Policy loss: -0.548004. Value loss: 0.519275. Entropy: 0.297865.\n",
      "Iteration 13178: Policy loss: -0.525888. Value loss: 0.360963. Entropy: 0.298747.\n",
      "Iteration 13179: Policy loss: -0.585042. Value loss: 0.306527. Entropy: 0.300589.\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13180: Policy loss: 0.238718. Value loss: 0.154704. Entropy: 0.312018.\n",
      "Iteration 13181: Policy loss: 0.239359. Value loss: 0.061511. Entropy: 0.311184.\n",
      "Iteration 13182: Policy loss: 0.234156. Value loss: 0.039546. Entropy: 0.311830.\n",
      "episode: 4756   score: 460.0  epsilon: 1.0    steps: 16  evaluation reward: 390.55\n",
      "episode: 4757   score: 450.0  epsilon: 1.0    steps: 176  evaluation reward: 391.45\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13183: Policy loss: 0.202456. Value loss: 0.107367. Entropy: 0.290306.\n",
      "Iteration 13184: Policy loss: 0.193128. Value loss: 0.038716. Entropy: 0.286992.\n",
      "Iteration 13185: Policy loss: 0.200534. Value loss: 0.026767. Entropy: 0.286463.\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13186: Policy loss: 0.048257. Value loss: 0.142252. Entropy: 0.307352.\n",
      "Iteration 13187: Policy loss: 0.043448. Value loss: 0.058894. Entropy: 0.307156.\n",
      "Iteration 13188: Policy loss: 0.037047. Value loss: 0.042544. Entropy: 0.307147.\n",
      "episode: 4758   score: 280.0  epsilon: 1.0    steps: 928  evaluation reward: 391.1\n",
      "episode: 4759   score: 445.0  epsilon: 1.0    steps: 976  evaluation reward: 390.3\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13189: Policy loss: 0.283966. Value loss: 0.424684. Entropy: 0.304012.\n",
      "Iteration 13190: Policy loss: 0.263924. Value loss: 0.341082. Entropy: 0.303840.\n",
      "Iteration 13191: Policy loss: 0.266788. Value loss: 0.292787. Entropy: 0.302759.\n",
      "episode: 4760   score: 240.0  epsilon: 1.0    steps: 504  evaluation reward: 388.0\n",
      "episode: 4761   score: 270.0  epsilon: 1.0    steps: 536  evaluation reward: 383.75\n",
      "episode: 4762   score: 320.0  epsilon: 1.0    steps: 680  evaluation reward: 383.0\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13192: Policy loss: 0.115050. Value loss: 0.098014. Entropy: 0.253353.\n",
      "Iteration 13193: Policy loss: 0.102464. Value loss: 0.035762. Entropy: 0.254218.\n",
      "Iteration 13194: Policy loss: 0.108670. Value loss: 0.024044. Entropy: 0.253264.\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13195: Policy loss: 0.096861. Value loss: 0.141609. Entropy: 0.311924.\n",
      "Iteration 13196: Policy loss: 0.091987. Value loss: 0.064401. Entropy: 0.311707.\n",
      "Iteration 13197: Policy loss: 0.091247. Value loss: 0.040069. Entropy: 0.310732.\n",
      "Training network. lr: 0.000149. clip: 0.059606\n",
      "Iteration 13198: Policy loss: -0.099092. Value loss: 0.371073. Entropy: 0.303684.\n",
      "Iteration 13199: Policy loss: -0.103034. Value loss: 0.179110. Entropy: 0.303699.\n",
      "Iteration 13200: Policy loss: -0.095432. Value loss: 0.119616. Entropy: 0.304214.\n",
      "episode: 4763   score: 30.0  epsilon: 1.0    steps: 376  evaluation reward: 380.9\n",
      "episode: 4764   score: 400.0  epsilon: 1.0    steps: 952  evaluation reward: 381.75\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13201: Policy loss: -0.092467. Value loss: 0.193177. Entropy: 0.293444.\n",
      "Iteration 13202: Policy loss: -0.102161. Value loss: 0.079465. Entropy: 0.293513.\n",
      "Iteration 13203: Policy loss: -0.098483. Value loss: 0.045132. Entropy: 0.292449.\n",
      "episode: 4765   score: 310.0  epsilon: 1.0    steps: 800  evaluation reward: 381.7\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13204: Policy loss: 0.160000. Value loss: 0.112273. Entropy: 0.300557.\n",
      "Iteration 13205: Policy loss: 0.151020. Value loss: 0.047942. Entropy: 0.298414.\n",
      "Iteration 13206: Policy loss: 0.157888. Value loss: 0.032548. Entropy: 0.298491.\n",
      "episode: 4766   score: 165.0  epsilon: 1.0    steps: 928  evaluation reward: 378.25\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13207: Policy loss: 0.056442. Value loss: 0.097744. Entropy: 0.303172.\n",
      "Iteration 13208: Policy loss: 0.048292. Value loss: 0.038318. Entropy: 0.304328.\n",
      "Iteration 13209: Policy loss: 0.047802. Value loss: 0.028186. Entropy: 0.303862.\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13210: Policy loss: -0.008531. Value loss: 0.245549. Entropy: 0.304706.\n",
      "Iteration 13211: Policy loss: -0.021172. Value loss: 0.067089. Entropy: 0.304171.\n",
      "Iteration 13212: Policy loss: -0.024269. Value loss: 0.038488. Entropy: 0.305026.\n",
      "episode: 4767   score: 285.0  epsilon: 1.0    steps: 272  evaluation reward: 379.35\n",
      "episode: 4768   score: 565.0  epsilon: 1.0    steps: 976  evaluation reward: 382.2\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13213: Policy loss: 0.152698. Value loss: 0.137884. Entropy: 0.302099.\n",
      "Iteration 13214: Policy loss: 0.143099. Value loss: 0.056425. Entropy: 0.302582.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13215: Policy loss: 0.139761. Value loss: 0.039879. Entropy: 0.300475.\n",
      "episode: 4769   score: 670.0  epsilon: 1.0    steps: 336  evaluation reward: 384.7\n",
      "episode: 4770   score: 210.0  epsilon: 1.0    steps: 584  evaluation reward: 384.05\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13216: Policy loss: -0.399954. Value loss: 0.281286. Entropy: 0.272338.\n",
      "Iteration 13217: Policy loss: -0.415489. Value loss: 0.166535. Entropy: 0.271434.\n",
      "Iteration 13218: Policy loss: -0.400053. Value loss: 0.090002. Entropy: 0.271412.\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13219: Policy loss: 0.348743. Value loss: 0.249888. Entropy: 0.306530.\n",
      "Iteration 13220: Policy loss: 0.325824. Value loss: 0.087974. Entropy: 0.305139.\n",
      "Iteration 13221: Policy loss: 0.327955. Value loss: 0.059071. Entropy: 0.305765.\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13222: Policy loss: 0.381014. Value loss: 0.223434. Entropy: 0.310151.\n",
      "Iteration 13223: Policy loss: 0.364051. Value loss: 0.066857. Entropy: 0.309360.\n",
      "Iteration 13224: Policy loss: 0.354385. Value loss: 0.047410. Entropy: 0.309717.\n",
      "episode: 4771   score: 640.0  epsilon: 1.0    steps: 344  evaluation reward: 387.3\n",
      "episode: 4772   score: 155.0  epsilon: 1.0    steps: 976  evaluation reward: 385.15\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13225: Policy loss: 0.204445. Value loss: 0.125258. Entropy: 0.297451.\n",
      "Iteration 13226: Policy loss: 0.203614. Value loss: 0.053171. Entropy: 0.296749.\n",
      "Iteration 13227: Policy loss: 0.191551. Value loss: 0.038658. Entropy: 0.296302.\n",
      "episode: 4773   score: 315.0  epsilon: 1.0    steps: 304  evaluation reward: 381.05\n",
      "episode: 4774   score: 565.0  epsilon: 1.0    steps: 648  evaluation reward: 382.55\n",
      "episode: 4775   score: 210.0  epsilon: 1.0    steps: 1008  evaluation reward: 381.0\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13228: Policy loss: -0.020732. Value loss: 0.125934. Entropy: 0.291518.\n",
      "Iteration 13229: Policy loss: -0.035081. Value loss: 0.065054. Entropy: 0.290222.\n",
      "Iteration 13230: Policy loss: -0.035752. Value loss: 0.049806. Entropy: 0.290165.\n",
      "episode: 4776   score: 210.0  epsilon: 1.0    steps: 760  evaluation reward: 378.25\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13231: Policy loss: -0.172730. Value loss: 0.159099. Entropy: 0.293364.\n",
      "Iteration 13232: Policy loss: -0.171558. Value loss: 0.059280. Entropy: 0.292949.\n",
      "Iteration 13233: Policy loss: -0.176904. Value loss: 0.041850. Entropy: 0.294511.\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13234: Policy loss: -0.068746. Value loss: 0.163615. Entropy: 0.302877.\n",
      "Iteration 13235: Policy loss: -0.082904. Value loss: 0.053529. Entropy: 0.301792.\n",
      "Iteration 13236: Policy loss: -0.092390. Value loss: 0.039065. Entropy: 0.301644.\n",
      "episode: 4777   score: 320.0  epsilon: 1.0    steps: 312  evaluation reward: 379.3\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13237: Policy loss: -0.005176. Value loss: 0.347996. Entropy: 0.304448.\n",
      "Iteration 13238: Policy loss: 0.011535. Value loss: 0.218940. Entropy: 0.303297.\n",
      "Iteration 13239: Policy loss: -0.003057. Value loss: 0.140358. Entropy: 0.303618.\n",
      "episode: 4778   score: 365.0  epsilon: 1.0    steps: 920  evaluation reward: 378.05\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13240: Policy loss: 0.124145. Value loss: 0.090135. Entropy: 0.309609.\n",
      "Iteration 13241: Policy loss: 0.127597. Value loss: 0.046916. Entropy: 0.308539.\n",
      "Iteration 13242: Policy loss: 0.122750. Value loss: 0.035449. Entropy: 0.308412.\n",
      "episode: 4779   score: 185.0  epsilon: 1.0    steps: 416  evaluation reward: 376.8\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13243: Policy loss: 0.146486. Value loss: 0.124009. Entropy: 0.299135.\n",
      "Iteration 13244: Policy loss: 0.140630. Value loss: 0.065358. Entropy: 0.298670.\n",
      "Iteration 13245: Policy loss: 0.141554. Value loss: 0.048396. Entropy: 0.298372.\n",
      "episode: 4780   score: 180.0  epsilon: 1.0    steps: 56  evaluation reward: 373.95\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13246: Policy loss: 0.076067. Value loss: 0.117798. Entropy: 0.299173.\n",
      "Iteration 13247: Policy loss: 0.076069. Value loss: 0.045927. Entropy: 0.298468.\n",
      "Iteration 13248: Policy loss: 0.065370. Value loss: 0.032158. Entropy: 0.298567.\n",
      "Training network. lr: 0.000149. clip: 0.059459\n",
      "Iteration 13249: Policy loss: -0.010006. Value loss: 0.160494. Entropy: 0.305338.\n",
      "Iteration 13250: Policy loss: 0.004963. Value loss: 0.092912. Entropy: 0.304687.\n",
      "Iteration 13251: Policy loss: -0.025806. Value loss: 0.091032. Entropy: 0.304080.\n",
      "episode: 4781   score: 480.0  epsilon: 1.0    steps: 64  evaluation reward: 376.6\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13252: Policy loss: -0.193556. Value loss: 0.300339. Entropy: 0.303054.\n",
      "Iteration 13253: Policy loss: -0.218764. Value loss: 0.183186. Entropy: 0.303095.\n",
      "Iteration 13254: Policy loss: -0.204061. Value loss: 0.101397. Entropy: 0.302159.\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13255: Policy loss: 0.143188. Value loss: 0.078856. Entropy: 0.302386.\n",
      "Iteration 13256: Policy loss: 0.140763. Value loss: 0.041759. Entropy: 0.302304.\n",
      "Iteration 13257: Policy loss: 0.133983. Value loss: 0.032060. Entropy: 0.302803.\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13258: Policy loss: -0.252016. Value loss: 0.130190. Entropy: 0.310333.\n",
      "Iteration 13259: Policy loss: -0.259248. Value loss: 0.049824. Entropy: 0.311758.\n",
      "Iteration 13260: Policy loss: -0.271074. Value loss: 0.033826. Entropy: 0.311947.\n",
      "episode: 4782   score: 650.0  epsilon: 1.0    steps: 480  evaluation reward: 377.65\n",
      "episode: 4783   score: 745.0  epsilon: 1.0    steps: 704  evaluation reward: 380.6\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13261: Policy loss: 0.092138. Value loss: 0.147369. Entropy: 0.296996.\n",
      "Iteration 13262: Policy loss: 0.084786. Value loss: 0.043737. Entropy: 0.293436.\n",
      "Iteration 13263: Policy loss: 0.083385. Value loss: 0.030720. Entropy: 0.294682.\n",
      "episode: 4784   score: 390.0  epsilon: 1.0    steps: 336  evaluation reward: 382.1\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13264: Policy loss: 0.115547. Value loss: 0.179451. Entropy: 0.305255.\n",
      "Iteration 13265: Policy loss: 0.103780. Value loss: 0.055980. Entropy: 0.303392.\n",
      "Iteration 13266: Policy loss: 0.106288. Value loss: 0.036299. Entropy: 0.303322.\n",
      "episode: 4785   score: 335.0  epsilon: 1.0    steps: 704  evaluation reward: 381.65\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13267: Policy loss: 0.064439. Value loss: 0.098852. Entropy: 0.296200.\n",
      "Iteration 13268: Policy loss: 0.054337. Value loss: 0.042430. Entropy: 0.295997.\n",
      "Iteration 13269: Policy loss: 0.049465. Value loss: 0.030396. Entropy: 0.296268.\n",
      "episode: 4786   score: 420.0  epsilon: 1.0    steps: 48  evaluation reward: 382.55\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13270: Policy loss: -0.075404. Value loss: 0.092060. Entropy: 0.300369.\n",
      "Iteration 13271: Policy loss: -0.087694. Value loss: 0.043588. Entropy: 0.300704.\n",
      "Iteration 13272: Policy loss: -0.090113. Value loss: 0.030597. Entropy: 0.300727.\n",
      "episode: 4787   score: 330.0  epsilon: 1.0    steps: 56  evaluation reward: 381.45\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13273: Policy loss: -0.022704. Value loss: 0.068879. Entropy: 0.289406.\n",
      "Iteration 13274: Policy loss: -0.029028. Value loss: 0.027078. Entropy: 0.288777.\n",
      "Iteration 13275: Policy loss: -0.028737. Value loss: 0.018789. Entropy: 0.289059.\n",
      "episode: 4788   score: 460.0  epsilon: 1.0    steps: 376  evaluation reward: 382.05\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13276: Policy loss: -0.216402. Value loss: 0.265516. Entropy: 0.294304.\n",
      "Iteration 13277: Policy loss: -0.219586. Value loss: 0.132717. Entropy: 0.295769.\n",
      "Iteration 13278: Policy loss: -0.216888. Value loss: 0.088901. Entropy: 0.295623.\n",
      "episode: 4789   score: 620.0  epsilon: 1.0    steps: 80  evaluation reward: 382.05\n",
      "episode: 4790   score: 210.0  epsilon: 1.0    steps: 800  evaluation reward: 378.0\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13279: Policy loss: 0.366786. Value loss: 0.126693. Entropy: 0.298128.\n",
      "Iteration 13280: Policy loss: 0.361145. Value loss: 0.053023. Entropy: 0.297825.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13281: Policy loss: 0.350238. Value loss: 0.040040. Entropy: 0.298029.\n",
      "episode: 4791   score: 360.0  epsilon: 1.0    steps: 688  evaluation reward: 376.9\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13282: Policy loss: 0.276260. Value loss: 0.132523. Entropy: 0.293139.\n",
      "Iteration 13283: Policy loss: 0.268364. Value loss: 0.059977. Entropy: 0.291900.\n",
      "Iteration 13284: Policy loss: 0.261610. Value loss: 0.042240. Entropy: 0.292014.\n",
      "episode: 4792   score: 390.0  epsilon: 1.0    steps: 600  evaluation reward: 370.0\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13285: Policy loss: 0.026552. Value loss: 0.117925. Entropy: 0.296951.\n",
      "Iteration 13286: Policy loss: 0.025773. Value loss: 0.053997. Entropy: 0.297771.\n",
      "Iteration 13287: Policy loss: 0.023884. Value loss: 0.041052. Entropy: 0.296848.\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13288: Policy loss: -0.154119. Value loss: 0.291868. Entropy: 0.308219.\n",
      "Iteration 13289: Policy loss: -0.161703. Value loss: 0.180241. Entropy: 0.306940.\n",
      "Iteration 13290: Policy loss: -0.166052. Value loss: 0.110497. Entropy: 0.306513.\n",
      "episode: 4793   score: 300.0  epsilon: 1.0    steps: 400  evaluation reward: 369.75\n",
      "episode: 4794   score: 150.0  epsilon: 1.0    steps: 560  evaluation reward: 367.9\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13291: Policy loss: 0.051372. Value loss: 0.100691. Entropy: 0.284808.\n",
      "Iteration 13292: Policy loss: 0.051027. Value loss: 0.042520. Entropy: 0.283829.\n",
      "Iteration 13293: Policy loss: 0.046639. Value loss: 0.029695. Entropy: 0.283502.\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13294: Policy loss: 0.174224. Value loss: 0.100966. Entropy: 0.307647.\n",
      "Iteration 13295: Policy loss: 0.165567. Value loss: 0.036595. Entropy: 0.306969.\n",
      "Iteration 13296: Policy loss: 0.161477. Value loss: 0.024224. Entropy: 0.306588.\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13297: Policy loss: 0.159163. Value loss: 0.116383. Entropy: 0.306015.\n",
      "Iteration 13298: Policy loss: 0.159381. Value loss: 0.036135. Entropy: 0.306244.\n",
      "Iteration 13299: Policy loss: 0.153445. Value loss: 0.026278. Entropy: 0.304853.\n",
      "episode: 4795   score: 285.0  epsilon: 1.0    steps: 600  evaluation reward: 365.5\n",
      "Training network. lr: 0.000148. clip: 0.059302\n",
      "Iteration 13300: Policy loss: -0.114436. Value loss: 0.131249. Entropy: 0.300492.\n",
      "Iteration 13301: Policy loss: -0.110865. Value loss: 0.053980. Entropy: 0.299593.\n",
      "Iteration 13302: Policy loss: -0.123359. Value loss: 0.038434. Entropy: 0.299610.\n",
      "episode: 4796   score: 225.0  epsilon: 1.0    steps: 408  evaluation reward: 365.05\n",
      "episode: 4797   score: 680.0  epsilon: 1.0    steps: 816  evaluation reward: 369.6\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13303: Policy loss: -0.195028. Value loss: 0.129615. Entropy: 0.293878.\n",
      "Iteration 13304: Policy loss: -0.205853. Value loss: 0.063160. Entropy: 0.294376.\n",
      "Iteration 13305: Policy loss: -0.205840. Value loss: 0.046951. Entropy: 0.295665.\n",
      "episode: 4798   score: 400.0  epsilon: 1.0    steps: 720  evaluation reward: 365.4\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13306: Policy loss: 0.051387. Value loss: 0.179747. Entropy: 0.295399.\n",
      "Iteration 13307: Policy loss: 0.055782. Value loss: 0.065342. Entropy: 0.294162.\n",
      "Iteration 13308: Policy loss: 0.040379. Value loss: 0.050535. Entropy: 0.294132.\n",
      "episode: 4799   score: 465.0  epsilon: 1.0    steps: 368  evaluation reward: 367.2\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13309: Policy loss: 0.346103. Value loss: 0.159637. Entropy: 0.292922.\n",
      "Iteration 13310: Policy loss: 0.339831. Value loss: 0.046560. Entropy: 0.292850.\n",
      "Iteration 13311: Policy loss: 0.343682. Value loss: 0.030684. Entropy: 0.291809.\n",
      "episode: 4800   score: 285.0  epsilon: 1.0    steps: 416  evaluation reward: 367.4\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13312: Policy loss: -0.295009. Value loss: 0.317826. Entropy: 0.288142.\n",
      "Iteration 13313: Policy loss: -0.309440. Value loss: 0.165344. Entropy: 0.286611.\n",
      "Iteration 13314: Policy loss: -0.299061. Value loss: 0.084997. Entropy: 0.287344.\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13315: Policy loss: -0.215038. Value loss: 0.096571. Entropy: 0.295186.\n",
      "Iteration 13316: Policy loss: -0.217876. Value loss: 0.037883. Entropy: 0.294766.\n",
      "Iteration 13317: Policy loss: -0.223119. Value loss: 0.026305. Entropy: 0.294629.\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13318: Policy loss: 0.209771. Value loss: 0.164398. Entropy: 0.305058.\n",
      "Iteration 13319: Policy loss: 0.198230. Value loss: 0.056274. Entropy: 0.302684.\n",
      "Iteration 13320: Policy loss: 0.187954. Value loss: 0.035569. Entropy: 0.303510.\n",
      "now time :  2019-09-06 04:01:46.507516\n",
      "episode: 4801   score: 390.0  epsilon: 1.0    steps: 888  evaluation reward: 367.1\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13321: Policy loss: 0.310607. Value loss: 0.125636. Entropy: 0.299141.\n",
      "Iteration 13322: Policy loss: 0.307530. Value loss: 0.055620. Entropy: 0.298215.\n",
      "Iteration 13323: Policy loss: 0.298385. Value loss: 0.039329. Entropy: 0.297981.\n",
      "episode: 4802   score: 440.0  epsilon: 1.0    steps: 672  evaluation reward: 369.4\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13324: Policy loss: 0.085158. Value loss: 0.107878. Entropy: 0.300293.\n",
      "Iteration 13325: Policy loss: 0.083732. Value loss: 0.044616. Entropy: 0.302629.\n",
      "Iteration 13326: Policy loss: 0.078785. Value loss: 0.030026. Entropy: 0.302181.\n",
      "episode: 4803   score: 320.0  epsilon: 1.0    steps: 32  evaluation reward: 369.45\n",
      "episode: 4804   score: 260.0  epsilon: 1.0    steps: 752  evaluation reward: 369.45\n",
      "episode: 4805   score: 600.0  epsilon: 1.0    steps: 952  evaluation reward: 372.0\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13327: Policy loss: 0.073508. Value loss: 0.063061. Entropy: 0.291304.\n",
      "Iteration 13328: Policy loss: 0.064226. Value loss: 0.028295. Entropy: 0.291663.\n",
      "Iteration 13329: Policy loss: 0.067021. Value loss: 0.021100. Entropy: 0.291950.\n",
      "episode: 4806   score: 270.0  epsilon: 1.0    steps: 928  evaluation reward: 371.1\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13330: Policy loss: 0.388442. Value loss: 0.157996. Entropy: 0.295411.\n",
      "Iteration 13331: Policy loss: 0.394876. Value loss: 0.069367. Entropy: 0.296183.\n",
      "Iteration 13332: Policy loss: 0.384907. Value loss: 0.045813. Entropy: 0.295957.\n",
      "episode: 4807   score: 350.0  epsilon: 1.0    steps: 208  evaluation reward: 372.2\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13333: Policy loss: 0.084978. Value loss: 0.072060. Entropy: 0.280199.\n",
      "Iteration 13334: Policy loss: 0.084272. Value loss: 0.028977. Entropy: 0.281986.\n",
      "Iteration 13335: Policy loss: 0.075190. Value loss: 0.022395. Entropy: 0.280798.\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13336: Policy loss: 0.143256. Value loss: 0.095582. Entropy: 0.296212.\n",
      "Iteration 13337: Policy loss: 0.139635. Value loss: 0.029748. Entropy: 0.296025.\n",
      "Iteration 13338: Policy loss: 0.133584. Value loss: 0.022783. Entropy: 0.296501.\n",
      "episode: 4808   score: 180.0  epsilon: 1.0    steps: 128  evaluation reward: 368.55\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13339: Policy loss: -0.205371. Value loss: 0.265128. Entropy: 0.299464.\n",
      "Iteration 13340: Policy loss: -0.212601. Value loss: 0.140367. Entropy: 0.300142.\n",
      "Iteration 13341: Policy loss: -0.215494. Value loss: 0.068737. Entropy: 0.299715.\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13342: Policy loss: -0.174646. Value loss: 0.320095. Entropy: 0.300887.\n",
      "Iteration 13343: Policy loss: -0.200839. Value loss: 0.174940. Entropy: 0.300989.\n",
      "Iteration 13344: Policy loss: -0.190832. Value loss: 0.096940. Entropy: 0.302244.\n",
      "episode: 4809   score: 640.0  epsilon: 1.0    steps: 144  evaluation reward: 369.7\n",
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13345: Policy loss: -0.198421. Value loss: 0.129290. Entropy: 0.302024.\n",
      "Iteration 13346: Policy loss: -0.203959. Value loss: 0.048789. Entropy: 0.302847.\n",
      "Iteration 13347: Policy loss: -0.218046. Value loss: 0.032107. Entropy: 0.303492.\n",
      "episode: 4810   score: 505.0  epsilon: 1.0    steps: 88  evaluation reward: 369.25\n",
      "episode: 4811   score: 490.0  epsilon: 1.0    steps: 520  evaluation reward: 371.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000148. clip: 0.059145\n",
      "Iteration 13348: Policy loss: -0.105645. Value loss: 0.269599. Entropy: 0.286817.\n",
      "Iteration 13349: Policy loss: -0.113336. Value loss: 0.168114. Entropy: 0.287007.\n",
      "Iteration 13350: Policy loss: -0.119505. Value loss: 0.124045. Entropy: 0.286264.\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13351: Policy loss: 0.031157. Value loss: 0.060158. Entropy: 0.304908.\n",
      "Iteration 13352: Policy loss: 0.028012. Value loss: 0.028098. Entropy: 0.306497.\n",
      "Iteration 13353: Policy loss: 0.027154. Value loss: 0.019564. Entropy: 0.304714.\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13354: Policy loss: 0.072026. Value loss: 0.116775. Entropy: 0.301036.\n",
      "Iteration 13355: Policy loss: 0.070229. Value loss: 0.035532. Entropy: 0.299151.\n",
      "Iteration 13356: Policy loss: 0.060303. Value loss: 0.026119. Entropy: 0.299078.\n",
      "episode: 4812   score: 465.0  epsilon: 1.0    steps: 856  evaluation reward: 372.95\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13357: Policy loss: -0.449780. Value loss: 0.513161. Entropy: 0.307954.\n",
      "Iteration 13358: Policy loss: -0.454970. Value loss: 0.251732. Entropy: 0.306981.\n",
      "Iteration 13359: Policy loss: -0.481003. Value loss: 0.148862. Entropy: 0.307507.\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13360: Policy loss: 0.043213. Value loss: 0.172230. Entropy: 0.301257.\n",
      "Iteration 13361: Policy loss: 0.037799. Value loss: 0.069857. Entropy: 0.300945.\n",
      "Iteration 13362: Policy loss: 0.028941. Value loss: 0.048377. Entropy: 0.300696.\n",
      "episode: 4813   score: 495.0  epsilon: 1.0    steps: 280  evaluation reward: 374.3\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13363: Policy loss: 0.067805. Value loss: 0.149185. Entropy: 0.297126.\n",
      "Iteration 13364: Policy loss: 0.056832. Value loss: 0.060703. Entropy: 0.296321.\n",
      "Iteration 13365: Policy loss: 0.058726. Value loss: 0.044810. Entropy: 0.297593.\n",
      "episode: 4814   score: 620.0  epsilon: 1.0    steps: 280  evaluation reward: 376.4\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13366: Policy loss: 0.007080. Value loss: 0.136012. Entropy: 0.295993.\n",
      "Iteration 13367: Policy loss: 0.012145. Value loss: 0.043809. Entropy: 0.295620.\n",
      "Iteration 13368: Policy loss: 0.002307. Value loss: 0.026272. Entropy: 0.297197.\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13369: Policy loss: -0.017262. Value loss: 0.091318. Entropy: 0.307363.\n",
      "Iteration 13370: Policy loss: -0.024504. Value loss: 0.029496. Entropy: 0.307463.\n",
      "Iteration 13371: Policy loss: -0.036639. Value loss: 0.019788. Entropy: 0.307231.\n",
      "episode: 4815   score: 590.0  epsilon: 1.0    steps: 624  evaluation reward: 372.3\n",
      "episode: 4816   score: 450.0  epsilon: 1.0    steps: 632  evaluation reward: 373.65\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13372: Policy loss: 0.058851. Value loss: 0.269369. Entropy: 0.287406.\n",
      "Iteration 13373: Policy loss: 0.044133. Value loss: 0.102880. Entropy: 0.287696.\n",
      "Iteration 13374: Policy loss: 0.005691. Value loss: 0.079739. Entropy: 0.287254.\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13375: Policy loss: 0.230045. Value loss: 0.135693. Entropy: 0.311708.\n",
      "Iteration 13376: Policy loss: 0.218833. Value loss: 0.041694. Entropy: 0.310627.\n",
      "Iteration 13377: Policy loss: 0.215023. Value loss: 0.026816. Entropy: 0.310949.\n",
      "episode: 4817   score: 500.0  epsilon: 1.0    steps: 488  evaluation reward: 376.55\n",
      "episode: 4818   score: 590.0  epsilon: 1.0    steps: 656  evaluation reward: 381.1\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13378: Policy loss: -0.041198. Value loss: 0.145335. Entropy: 0.270923.\n",
      "Iteration 13379: Policy loss: -0.048376. Value loss: 0.071278. Entropy: 0.268492.\n",
      "Iteration 13380: Policy loss: -0.051183. Value loss: 0.048675. Entropy: 0.268124.\n",
      "episode: 4819   score: 670.0  epsilon: 1.0    steps: 448  evaluation reward: 383.2\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13381: Policy loss: -0.002369. Value loss: 0.170699. Entropy: 0.297984.\n",
      "Iteration 13382: Policy loss: -0.005948. Value loss: 0.054937. Entropy: 0.297820.\n",
      "Iteration 13383: Policy loss: 0.000683. Value loss: 0.036785. Entropy: 0.298437.\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13384: Policy loss: 0.085190. Value loss: 0.068395. Entropy: 0.297731.\n",
      "Iteration 13385: Policy loss: 0.072514. Value loss: 0.027787. Entropy: 0.293561.\n",
      "Iteration 13386: Policy loss: 0.072407. Value loss: 0.022354. Entropy: 0.293406.\n",
      "episode: 4820   score: 415.0  epsilon: 1.0    steps: 456  evaluation reward: 385.1\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13387: Policy loss: -0.135644. Value loss: 0.279611. Entropy: 0.291439.\n",
      "Iteration 13388: Policy loss: -0.144831. Value loss: 0.212861. Entropy: 0.290561.\n",
      "Iteration 13389: Policy loss: -0.145889. Value loss: 0.166756. Entropy: 0.288560.\n",
      "episode: 4821   score: 330.0  epsilon: 1.0    steps: 344  evaluation reward: 383.3\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13390: Policy loss: 0.051324. Value loss: 0.055396. Entropy: 0.296591.\n",
      "Iteration 13391: Policy loss: 0.051169. Value loss: 0.027197. Entropy: 0.297319.\n",
      "Iteration 13392: Policy loss: 0.047603. Value loss: 0.021210. Entropy: 0.297148.\n",
      "episode: 4822   score: 260.0  epsilon: 1.0    steps: 704  evaluation reward: 382.45\n",
      "episode: 4823   score: 350.0  epsilon: 1.0    steps: 808  evaluation reward: 384.4\n",
      "episode: 4824   score: 490.0  epsilon: 1.0    steps: 856  evaluation reward: 386.5\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13393: Policy loss: 0.068800. Value loss: 0.145531. Entropy: 0.274292.\n",
      "Iteration 13394: Policy loss: 0.066312. Value loss: 0.076441. Entropy: 0.275199.\n",
      "Iteration 13395: Policy loss: 0.053966. Value loss: 0.048403. Entropy: 0.275954.\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13396: Policy loss: 0.037255. Value loss: 0.069612. Entropy: 0.301682.\n",
      "Iteration 13397: Policy loss: 0.032293. Value loss: 0.029935. Entropy: 0.303560.\n",
      "Iteration 13398: Policy loss: 0.029884. Value loss: 0.022786. Entropy: 0.302898.\n",
      "Training network. lr: 0.000147. clip: 0.058998\n",
      "Iteration 13399: Policy loss: -0.111249. Value loss: 0.361853. Entropy: 0.284710.\n",
      "Iteration 13400: Policy loss: -0.128399. Value loss: 0.265255. Entropy: 0.286763.\n",
      "Iteration 13401: Policy loss: -0.131180. Value loss: 0.209944. Entropy: 0.287653.\n",
      "episode: 4825   score: 285.0  epsilon: 1.0    steps: 424  evaluation reward: 386.8\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13402: Policy loss: 0.044009. Value loss: 0.113283. Entropy: 0.296366.\n",
      "Iteration 13403: Policy loss: 0.045632. Value loss: 0.034465. Entropy: 0.295585.\n",
      "Iteration 13404: Policy loss: 0.040183. Value loss: 0.022156. Entropy: 0.293819.\n",
      "episode: 4826   score: 595.0  epsilon: 1.0    steps: 184  evaluation reward: 390.1\n",
      "episode: 4827   score: 320.0  epsilon: 1.0    steps: 512  evaluation reward: 391.5\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13405: Policy loss: -0.230966. Value loss: 0.306041. Entropy: 0.284912.\n",
      "Iteration 13406: Policy loss: -0.237370. Value loss: 0.167273. Entropy: 0.285806.\n",
      "Iteration 13407: Policy loss: -0.237854. Value loss: 0.100739. Entropy: 0.285704.\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13408: Policy loss: 0.016880. Value loss: 0.180066. Entropy: 0.299548.\n",
      "Iteration 13409: Policy loss: 0.003408. Value loss: 0.084188. Entropy: 0.300347.\n",
      "Iteration 13410: Policy loss: -0.002934. Value loss: 0.049957. Entropy: 0.298956.\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13411: Policy loss: 0.043208. Value loss: 0.093186. Entropy: 0.286744.\n",
      "Iteration 13412: Policy loss: 0.039718. Value loss: 0.037092. Entropy: 0.286801.\n",
      "Iteration 13413: Policy loss: 0.033508. Value loss: 0.024640. Entropy: 0.286287.\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13414: Policy loss: -0.146064. Value loss: 0.093430. Entropy: 0.316944.\n",
      "Iteration 13415: Policy loss: -0.151021. Value loss: 0.036308. Entropy: 0.316399.\n",
      "Iteration 13416: Policy loss: -0.152462. Value loss: 0.025580. Entropy: 0.315947.\n",
      "episode: 4828   score: 595.0  epsilon: 1.0    steps: 960  evaluation reward: 390.8\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13417: Policy loss: -0.014477. Value loss: 0.081188. Entropy: 0.312442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13418: Policy loss: -0.022419. Value loss: 0.031019. Entropy: 0.311855.\n",
      "Iteration 13419: Policy loss: -0.020889. Value loss: 0.022356. Entropy: 0.312287.\n",
      "episode: 4829   score: 650.0  epsilon: 1.0    steps: 960  evaluation reward: 395.2\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13420: Policy loss: -0.151879. Value loss: 0.336565. Entropy: 0.307998.\n",
      "Iteration 13421: Policy loss: -0.152494. Value loss: 0.176768. Entropy: 0.308525.\n",
      "Iteration 13422: Policy loss: -0.154722. Value loss: 0.112291. Entropy: 0.307680.\n",
      "episode: 4830   score: 290.0  epsilon: 1.0    steps: 520  evaluation reward: 396.3\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13423: Policy loss: 0.314799. Value loss: 0.107065. Entropy: 0.290565.\n",
      "Iteration 13424: Policy loss: 0.298116. Value loss: 0.042326. Entropy: 0.290332.\n",
      "Iteration 13425: Policy loss: 0.295015. Value loss: 0.028957. Entropy: 0.289739.\n",
      "episode: 4831   score: 395.0  epsilon: 1.0    steps: 448  evaluation reward: 396.8\n",
      "episode: 4832   score: 945.0  epsilon: 1.0    steps: 768  evaluation reward: 404.45\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13426: Policy loss: -0.268637. Value loss: 0.311760. Entropy: 0.285889.\n",
      "Iteration 13427: Policy loss: -0.271782. Value loss: 0.146455. Entropy: 0.286255.\n",
      "Iteration 13428: Policy loss: -0.280408. Value loss: 0.083875. Entropy: 0.286297.\n",
      "episode: 4833   score: 525.0  epsilon: 1.0    steps: 128  evaluation reward: 407.6\n",
      "episode: 4834   score: 365.0  epsilon: 1.0    steps: 464  evaluation reward: 408.1\n",
      "episode: 4835   score: 365.0  epsilon: 1.0    steps: 856  evaluation reward: 407.45\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13429: Policy loss: -0.028487. Value loss: 0.060480. Entropy: 0.273131.\n",
      "Iteration 13430: Policy loss: -0.034104. Value loss: 0.041662. Entropy: 0.273373.\n",
      "Iteration 13431: Policy loss: -0.031984. Value loss: 0.035536. Entropy: 0.273751.\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13432: Policy loss: 0.123558. Value loss: 0.089703. Entropy: 0.295360.\n",
      "Iteration 13433: Policy loss: 0.122461. Value loss: 0.051951. Entropy: 0.291363.\n",
      "Iteration 13434: Policy loss: 0.117215. Value loss: 0.035346. Entropy: 0.293265.\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13435: Policy loss: -0.140019. Value loss: 0.251286. Entropy: 0.284191.\n",
      "Iteration 13436: Policy loss: -0.149709. Value loss: 0.091979. Entropy: 0.283878.\n",
      "Iteration 13437: Policy loss: -0.142451. Value loss: 0.057223. Entropy: 0.284097.\n",
      "episode: 4836   score: 105.0  epsilon: 1.0    steps: 48  evaluation reward: 406.4\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13438: Policy loss: 0.162327. Value loss: 0.209413. Entropy: 0.303003.\n",
      "Iteration 13439: Policy loss: 0.154484. Value loss: 0.062363. Entropy: 0.302142.\n",
      "Iteration 13440: Policy loss: 0.142103. Value loss: 0.039134. Entropy: 0.302145.\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13441: Policy loss: -0.301126. Value loss: 0.261068. Entropy: 0.299277.\n",
      "Iteration 13442: Policy loss: -0.301615. Value loss: 0.122274. Entropy: 0.300454.\n",
      "Iteration 13443: Policy loss: -0.308735. Value loss: 0.082987. Entropy: 0.301477.\n",
      "episode: 4837   score: 360.0  epsilon: 1.0    steps: 400  evaluation reward: 407.6\n",
      "episode: 4838   score: 620.0  epsilon: 1.0    steps: 512  evaluation reward: 411.4\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13444: Policy loss: 0.011552. Value loss: 0.166283. Entropy: 0.286121.\n",
      "Iteration 13445: Policy loss: 0.017633. Value loss: 0.078553. Entropy: 0.284297.\n",
      "Iteration 13446: Policy loss: 0.002249. Value loss: 0.052811. Entropy: 0.285822.\n",
      "episode: 4839   score: 440.0  epsilon: 1.0    steps: 96  evaluation reward: 411.75\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13447: Policy loss: -0.033591. Value loss: 0.141978. Entropy: 0.298059.\n",
      "Iteration 13448: Policy loss: -0.041806. Value loss: 0.060930. Entropy: 0.298292.\n",
      "Iteration 13449: Policy loss: -0.036963. Value loss: 0.043005. Entropy: 0.299474.\n",
      "episode: 4840   score: 535.0  epsilon: 1.0    steps: 792  evaluation reward: 413.9\n",
      "Training network. lr: 0.000147. clip: 0.058841\n",
      "Iteration 13450: Policy loss: 0.122743. Value loss: 0.178199. Entropy: 0.287863.\n",
      "Iteration 13451: Policy loss: 0.116156. Value loss: 0.076807. Entropy: 0.290264.\n",
      "Iteration 13452: Policy loss: 0.114728. Value loss: 0.049966. Entropy: 0.289150.\n",
      "episode: 4841   score: 405.0  epsilon: 1.0    steps: 904  evaluation reward: 413.55\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13453: Policy loss: 0.043565. Value loss: 0.138142. Entropy: 0.308057.\n",
      "Iteration 13454: Policy loss: 0.033858. Value loss: 0.055673. Entropy: 0.308215.\n",
      "Iteration 13455: Policy loss: 0.028537. Value loss: 0.035951. Entropy: 0.308814.\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13456: Policy loss: 0.198002. Value loss: 0.131856. Entropy: 0.303930.\n",
      "Iteration 13457: Policy loss: 0.207757. Value loss: 0.047947. Entropy: 0.302242.\n",
      "Iteration 13458: Policy loss: 0.198509. Value loss: 0.033412. Entropy: 0.304071.\n",
      "episode: 4842   score: 495.0  epsilon: 1.0    steps: 160  evaluation reward: 409.9\n",
      "episode: 4843   score: 365.0  epsilon: 1.0    steps: 984  evaluation reward: 410.95\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13459: Policy loss: 0.415395. Value loss: 0.126904. Entropy: 0.291320.\n",
      "Iteration 13460: Policy loss: 0.409801. Value loss: 0.059628. Entropy: 0.289112.\n",
      "Iteration 13461: Policy loss: 0.408138. Value loss: 0.044950. Entropy: 0.289869.\n",
      "episode: 4844   score: 510.0  epsilon: 1.0    steps: 768  evaluation reward: 411.35\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13462: Policy loss: 0.138692. Value loss: 0.096124. Entropy: 0.293890.\n",
      "Iteration 13463: Policy loss: 0.129778. Value loss: 0.040842. Entropy: 0.293612.\n",
      "Iteration 13464: Policy loss: 0.115075. Value loss: 0.033345. Entropy: 0.292012.\n",
      "episode: 4845   score: 210.0  epsilon: 1.0    steps: 680  evaluation reward: 405.5\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13465: Policy loss: -0.258725. Value loss: 0.290104. Entropy: 0.290858.\n",
      "Iteration 13466: Policy loss: -0.267181. Value loss: 0.157859. Entropy: 0.289293.\n",
      "Iteration 13467: Policy loss: -0.277398. Value loss: 0.063358. Entropy: 0.289077.\n",
      "episode: 4846   score: 315.0  epsilon: 1.0    steps: 1008  evaluation reward: 405.05\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13468: Policy loss: 0.095269. Value loss: 0.124110. Entropy: 0.303155.\n",
      "Iteration 13469: Policy loss: 0.082478. Value loss: 0.046586. Entropy: 0.302017.\n",
      "Iteration 13470: Policy loss: 0.085468. Value loss: 0.038249. Entropy: 0.302025.\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13471: Policy loss: -0.002584. Value loss: 0.075818. Entropy: 0.294733.\n",
      "Iteration 13472: Policy loss: -0.005091. Value loss: 0.020048. Entropy: 0.293743.\n",
      "Iteration 13473: Policy loss: -0.009266. Value loss: 0.014309. Entropy: 0.294074.\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13474: Policy loss: -0.043858. Value loss: 0.110956. Entropy: 0.304272.\n",
      "Iteration 13475: Policy loss: -0.052184. Value loss: 0.039115. Entropy: 0.304125.\n",
      "Iteration 13476: Policy loss: -0.051767. Value loss: 0.025996. Entropy: 0.304619.\n",
      "episode: 4847   score: 545.0  epsilon: 1.0    steps: 360  evaluation reward: 406.6\n",
      "episode: 4848   score: 420.0  epsilon: 1.0    steps: 448  evaluation reward: 410.0\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13477: Policy loss: -0.129830. Value loss: 0.423296. Entropy: 0.287620.\n",
      "Iteration 13478: Policy loss: -0.159939. Value loss: 0.168976. Entropy: 0.287519.\n",
      "Iteration 13479: Policy loss: -0.164845. Value loss: 0.071142. Entropy: 0.287369.\n",
      "episode: 4849   score: 635.0  epsilon: 1.0    steps: 232  evaluation reward: 411.8\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13480: Policy loss: 0.116648. Value loss: 0.057534. Entropy: 0.301089.\n",
      "Iteration 13481: Policy loss: 0.112237. Value loss: 0.031499. Entropy: 0.301812.\n",
      "Iteration 13482: Policy loss: 0.110356. Value loss: 0.024548. Entropy: 0.302283.\n",
      "episode: 4850   score: 335.0  epsilon: 1.0    steps: 136  evaluation reward: 409.7\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13483: Policy loss: 0.389621. Value loss: 0.150238. Entropy: 0.277374.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13484: Policy loss: 0.374229. Value loss: 0.062773. Entropy: 0.279806.\n",
      "Iteration 13485: Policy loss: 0.379655. Value loss: 0.049094. Entropy: 0.278771.\n",
      "now time :  2019-09-06 04:11:59.881842\n",
      "episode: 4851   score: 360.0  epsilon: 1.0    steps: 160  evaluation reward: 410.15\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13486: Policy loss: 0.415706. Value loss: 0.148570. Entropy: 0.290778.\n",
      "Iteration 13487: Policy loss: 0.420972. Value loss: 0.068815. Entropy: 0.292939.\n",
      "Iteration 13488: Policy loss: 0.409977. Value loss: 0.050840. Entropy: 0.294066.\n",
      "episode: 4852   score: 565.0  epsilon: 1.0    steps: 720  evaluation reward: 413.05\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13489: Policy loss: -0.200697. Value loss: 0.137990. Entropy: 0.297876.\n",
      "Iteration 13490: Policy loss: -0.209752. Value loss: 0.053489. Entropy: 0.297385.\n",
      "Iteration 13491: Policy loss: -0.207429. Value loss: 0.037061. Entropy: 0.297911.\n",
      "episode: 4853   score: 670.0  epsilon: 1.0    steps: 152  evaluation reward: 415.7\n",
      "episode: 4854   score: 210.0  epsilon: 1.0    steps: 880  evaluation reward: 413.45\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13492: Policy loss: -0.334583. Value loss: 0.231594. Entropy: 0.292535.\n",
      "Iteration 13493: Policy loss: -0.342600. Value loss: 0.079992. Entropy: 0.293097.\n",
      "Iteration 13494: Policy loss: -0.361042. Value loss: 0.063555. Entropy: 0.293561.\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13495: Policy loss: 0.123277. Value loss: 0.195458. Entropy: 0.294689.\n",
      "Iteration 13496: Policy loss: 0.122250. Value loss: 0.061259. Entropy: 0.296238.\n",
      "Iteration 13497: Policy loss: 0.113751. Value loss: 0.039555. Entropy: 0.295278.\n",
      "episode: 4855   score: 330.0  epsilon: 1.0    steps: 408  evaluation reward: 408.55\n",
      "Training network. lr: 0.000147. clip: 0.058685\n",
      "Iteration 13498: Policy loss: 0.197649. Value loss: 0.218980. Entropy: 0.293640.\n",
      "Iteration 13499: Policy loss: 0.192979. Value loss: 0.077833. Entropy: 0.295561.\n",
      "Iteration 13500: Policy loss: 0.171614. Value loss: 0.055237. Entropy: 0.294125.\n",
      "episode: 4856   score: 770.0  epsilon: 1.0    steps: 272  evaluation reward: 411.65\n",
      "episode: 4857   score: 285.0  epsilon: 1.0    steps: 680  evaluation reward: 410.0\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13501: Policy loss: 0.147501. Value loss: 0.120274. Entropy: 0.280352.\n",
      "Iteration 13502: Policy loss: 0.134580. Value loss: 0.046200. Entropy: 0.278536.\n",
      "Iteration 13503: Policy loss: 0.126831. Value loss: 0.029973. Entropy: 0.278327.\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13504: Policy loss: 0.077350. Value loss: 0.072850. Entropy: 0.308356.\n",
      "Iteration 13505: Policy loss: 0.069266. Value loss: 0.033051. Entropy: 0.307585.\n",
      "Iteration 13506: Policy loss: 0.066582. Value loss: 0.023712. Entropy: 0.306246.\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13507: Policy loss: -0.079652. Value loss: 0.263644. Entropy: 0.301908.\n",
      "Iteration 13508: Policy loss: -0.102904. Value loss: 0.111181. Entropy: 0.302403.\n",
      "Iteration 13509: Policy loss: -0.115837. Value loss: 0.064126. Entropy: 0.302529.\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13510: Policy loss: -0.198607. Value loss: 0.253714. Entropy: 0.312875.\n",
      "Iteration 13511: Policy loss: -0.221047. Value loss: 0.072394. Entropy: 0.312490.\n",
      "Iteration 13512: Policy loss: -0.218575. Value loss: 0.036683. Entropy: 0.312473.\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13513: Policy loss: 0.096543. Value loss: 0.178325. Entropy: 0.312076.\n",
      "Iteration 13514: Policy loss: 0.090982. Value loss: 0.056237. Entropy: 0.312408.\n",
      "Iteration 13515: Policy loss: 0.090229. Value loss: 0.035150. Entropy: 0.311754.\n",
      "episode: 4858   score: 500.0  epsilon: 1.0    steps: 360  evaluation reward: 412.2\n",
      "episode: 4859   score: 665.0  epsilon: 1.0    steps: 600  evaluation reward: 414.4\n",
      "episode: 4860   score: 425.0  epsilon: 1.0    steps: 608  evaluation reward: 416.25\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13516: Policy loss: -0.302676. Value loss: 0.412100. Entropy: 0.267155.\n",
      "Iteration 13517: Policy loss: -0.306924. Value loss: 0.148457. Entropy: 0.267750.\n",
      "Iteration 13518: Policy loss: -0.327390. Value loss: 0.068756. Entropy: 0.267426.\n",
      "episode: 4861   score: 320.0  epsilon: 1.0    steps: 296  evaluation reward: 416.75\n",
      "episode: 4862   score: 695.0  epsilon: 1.0    steps: 760  evaluation reward: 420.5\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13519: Policy loss: 0.135864. Value loss: 0.168092. Entropy: 0.288317.\n",
      "Iteration 13520: Policy loss: 0.122011. Value loss: 0.057348. Entropy: 0.288661.\n",
      "Iteration 13521: Policy loss: 0.119783. Value loss: 0.036999. Entropy: 0.285435.\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13522: Policy loss: 0.121857. Value loss: 0.140279. Entropy: 0.288439.\n",
      "Iteration 13523: Policy loss: 0.117129. Value loss: 0.056187. Entropy: 0.286433.\n",
      "Iteration 13524: Policy loss: 0.116289. Value loss: 0.040665. Entropy: 0.284958.\n",
      "episode: 4863   score: 525.0  epsilon: 1.0    steps: 960  evaluation reward: 425.45\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13525: Policy loss: 0.006619. Value loss: 0.192876. Entropy: 0.295783.\n",
      "Iteration 13526: Policy loss: 0.009685. Value loss: 0.063989. Entropy: 0.295888.\n",
      "Iteration 13527: Policy loss: -0.007249. Value loss: 0.050562. Entropy: 0.295185.\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13528: Policy loss: 0.099523. Value loss: 0.348701. Entropy: 0.304267.\n",
      "Iteration 13529: Policy loss: 0.073896. Value loss: 0.181500. Entropy: 0.304001.\n",
      "Iteration 13530: Policy loss: 0.053887. Value loss: 0.111983. Entropy: 0.302806.\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13531: Policy loss: 0.077228. Value loss: 0.161835. Entropy: 0.301609.\n",
      "Iteration 13532: Policy loss: 0.070638. Value loss: 0.075030. Entropy: 0.301153.\n",
      "Iteration 13533: Policy loss: 0.067713. Value loss: 0.057522. Entropy: 0.300984.\n",
      "episode: 4864   score: 415.0  epsilon: 1.0    steps: 624  evaluation reward: 425.6\n",
      "episode: 4865   score: 695.0  epsilon: 1.0    steps: 872  evaluation reward: 429.45\n",
      "episode: 4866   score: 645.0  epsilon: 1.0    steps: 1016  evaluation reward: 434.25\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13534: Policy loss: 0.364002. Value loss: 0.228285. Entropy: 0.297610.\n",
      "Iteration 13535: Policy loss: 0.352580. Value loss: 0.102601. Entropy: 0.296405.\n",
      "Iteration 13536: Policy loss: 0.344352. Value loss: 0.070016. Entropy: 0.296740.\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13537: Policy loss: 0.415819. Value loss: 0.184252. Entropy: 0.299168.\n",
      "Iteration 13538: Policy loss: 0.412671. Value loss: 0.067622. Entropy: 0.299508.\n",
      "Iteration 13539: Policy loss: 0.398635. Value loss: 0.047560. Entropy: 0.298304.\n",
      "episode: 4867   score: 285.0  epsilon: 1.0    steps: 296  evaluation reward: 434.25\n",
      "episode: 4868   score: 425.0  epsilon: 1.0    steps: 1008  evaluation reward: 432.85\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13540: Policy loss: 0.123042. Value loss: 0.088941. Entropy: 0.287699.\n",
      "Iteration 13541: Policy loss: 0.117576. Value loss: 0.048588. Entropy: 0.286400.\n",
      "Iteration 13542: Policy loss: 0.111697. Value loss: 0.039784. Entropy: 0.283315.\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13543: Policy loss: -0.015875. Value loss: 0.292672. Entropy: 0.305933.\n",
      "Iteration 13544: Policy loss: -0.062251. Value loss: 0.091293. Entropy: 0.304441.\n",
      "Iteration 13545: Policy loss: -0.056486. Value loss: 0.060504. Entropy: 0.305472.\n",
      "episode: 4869   score: 665.0  epsilon: 1.0    steps: 376  evaluation reward: 432.8\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13546: Policy loss: 0.137438. Value loss: 0.167664. Entropy: 0.289442.\n",
      "Iteration 13547: Policy loss: 0.129029. Value loss: 0.084918. Entropy: 0.289007.\n",
      "Iteration 13548: Policy loss: 0.124201. Value loss: 0.064968. Entropy: 0.287470.\n",
      "episode: 4870   score: 125.0  epsilon: 1.0    steps: 288  evaluation reward: 431.95\n",
      "Training network. lr: 0.000146. clip: 0.058537\n",
      "Iteration 13549: Policy loss: -0.063894. Value loss: 0.235243. Entropy: 0.299547.\n",
      "Iteration 13550: Policy loss: -0.083995. Value loss: 0.090534. Entropy: 0.298274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13551: Policy loss: -0.114384. Value loss: 0.055397. Entropy: 0.298861.\n",
      "episode: 4871   score: 450.0  epsilon: 1.0    steps: 384  evaluation reward: 430.05\n",
      "episode: 4872   score: 520.0  epsilon: 1.0    steps: 600  evaluation reward: 433.7\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13552: Policy loss: 0.161387. Value loss: 0.106854. Entropy: 0.293767.\n",
      "Iteration 13553: Policy loss: 0.148537. Value loss: 0.042511. Entropy: 0.293314.\n",
      "Iteration 13554: Policy loss: 0.144773. Value loss: 0.031101. Entropy: 0.292746.\n",
      "episode: 4873   score: 290.0  epsilon: 1.0    steps: 424  evaluation reward: 433.45\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13555: Policy loss: -0.033609. Value loss: 0.310251. Entropy: 0.293293.\n",
      "Iteration 13556: Policy loss: -0.056863. Value loss: 0.124477. Entropy: 0.295718.\n",
      "Iteration 13557: Policy loss: -0.064127. Value loss: 0.063491. Entropy: 0.292859.\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13558: Policy loss: 0.125772. Value loss: 0.188653. Entropy: 0.300497.\n",
      "Iteration 13559: Policy loss: 0.126277. Value loss: 0.100645. Entropy: 0.299701.\n",
      "Iteration 13560: Policy loss: 0.120715. Value loss: 0.074519. Entropy: 0.301305.\n",
      "episode: 4874   score: 565.0  epsilon: 1.0    steps: 472  evaluation reward: 433.45\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13561: Policy loss: 0.059844. Value loss: 0.101207. Entropy: 0.300598.\n",
      "Iteration 13562: Policy loss: 0.057068. Value loss: 0.046999. Entropy: 0.301784.\n",
      "Iteration 13563: Policy loss: 0.059045. Value loss: 0.028885. Entropy: 0.299873.\n",
      "episode: 4875   score: 595.0  epsilon: 1.0    steps: 1024  evaluation reward: 437.3\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13564: Policy loss: 0.083460. Value loss: 0.113013. Entropy: 0.311595.\n",
      "Iteration 13565: Policy loss: 0.076716. Value loss: 0.042650. Entropy: 0.310483.\n",
      "Iteration 13566: Policy loss: 0.077090. Value loss: 0.027752. Entropy: 0.310955.\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13567: Policy loss: 0.199287. Value loss: 0.085974. Entropy: 0.293615.\n",
      "Iteration 13568: Policy loss: 0.190572. Value loss: 0.034810. Entropy: 0.292659.\n",
      "Iteration 13569: Policy loss: 0.187690. Value loss: 0.024124. Entropy: 0.292829.\n",
      "episode: 4876   score: 295.0  epsilon: 1.0    steps: 488  evaluation reward: 438.15\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13570: Policy loss: -0.764275. Value loss: 0.610049. Entropy: 0.296156.\n",
      "Iteration 13571: Policy loss: -0.740243. Value loss: 0.247542. Entropy: 0.298467.\n",
      "Iteration 13572: Policy loss: -0.788632. Value loss: 0.168143. Entropy: 0.299829.\n",
      "episode: 4877   score: 475.0  epsilon: 1.0    steps: 776  evaluation reward: 439.7\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13573: Policy loss: 0.077407. Value loss: 0.254187. Entropy: 0.303607.\n",
      "Iteration 13574: Policy loss: 0.077320. Value loss: 0.083576. Entropy: 0.303517.\n",
      "Iteration 13575: Policy loss: 0.070680. Value loss: 0.050047. Entropy: 0.303016.\n",
      "episode: 4878   score: 500.0  epsilon: 1.0    steps: 592  evaluation reward: 441.05\n",
      "episode: 4879   score: 740.0  epsilon: 1.0    steps: 696  evaluation reward: 446.6\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13576: Policy loss: -0.069317. Value loss: 0.139243. Entropy: 0.290142.\n",
      "Iteration 13577: Policy loss: -0.073631. Value loss: 0.053416. Entropy: 0.291457.\n",
      "Iteration 13578: Policy loss: -0.081476. Value loss: 0.033907. Entropy: 0.292768.\n",
      "episode: 4880   score: 560.0  epsilon: 1.0    steps: 56  evaluation reward: 450.4\n",
      "episode: 4881   score: 420.0  epsilon: 1.0    steps: 608  evaluation reward: 449.8\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13579: Policy loss: 0.053040. Value loss: 0.079106. Entropy: 0.294327.\n",
      "Iteration 13580: Policy loss: 0.046665. Value loss: 0.035532. Entropy: 0.293577.\n",
      "Iteration 13581: Policy loss: 0.046540. Value loss: 0.027539. Entropy: 0.293475.\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13582: Policy loss: -0.013608. Value loss: 0.107873. Entropy: 0.309996.\n",
      "Iteration 13583: Policy loss: -0.022326. Value loss: 0.054357. Entropy: 0.309317.\n",
      "Iteration 13584: Policy loss: -0.018065. Value loss: 0.040287. Entropy: 0.307988.\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13585: Policy loss: 0.269810. Value loss: 0.276206. Entropy: 0.313774.\n",
      "Iteration 13586: Policy loss: 0.268293. Value loss: 0.095672. Entropy: 0.312514.\n",
      "Iteration 13587: Policy loss: 0.256786. Value loss: 0.071811. Entropy: 0.312539.\n",
      "episode: 4882   score: 590.0  epsilon: 1.0    steps: 104  evaluation reward: 449.2\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13588: Policy loss: -0.044605. Value loss: 0.107817. Entropy: 0.300477.\n",
      "Iteration 13589: Policy loss: -0.043668. Value loss: 0.047094. Entropy: 0.300293.\n",
      "Iteration 13590: Policy loss: -0.046929. Value loss: 0.031983. Entropy: 0.300283.\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13591: Policy loss: 0.390566. Value loss: 0.207600. Entropy: 0.307317.\n",
      "Iteration 13592: Policy loss: 0.387061. Value loss: 0.052687. Entropy: 0.306559.\n",
      "Iteration 13593: Policy loss: 0.377610. Value loss: 0.031105. Entropy: 0.307537.\n",
      "episode: 4883   score: 305.0  epsilon: 1.0    steps: 400  evaluation reward: 444.8\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13594: Policy loss: 0.215353. Value loss: 0.166589. Entropy: 0.302109.\n",
      "Iteration 13595: Policy loss: 0.215129. Value loss: 0.057567. Entropy: 0.301448.\n",
      "Iteration 13596: Policy loss: 0.195674. Value loss: 0.036351. Entropy: 0.301337.\n",
      "episode: 4884   score: 225.0  epsilon: 1.0    steps: 336  evaluation reward: 443.15\n",
      "episode: 4885   score: 545.0  epsilon: 1.0    steps: 992  evaluation reward: 445.25\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13597: Policy loss: 0.306715. Value loss: 0.105184. Entropy: 0.298192.\n",
      "Iteration 13598: Policy loss: 0.303343. Value loss: 0.045936. Entropy: 0.298569.\n",
      "Iteration 13599: Policy loss: 0.307238. Value loss: 0.030368. Entropy: 0.298547.\n",
      "episode: 4886   score: 295.0  epsilon: 1.0    steps: 432  evaluation reward: 444.0\n",
      "episode: 4887   score: 680.0  epsilon: 1.0    steps: 952  evaluation reward: 447.5\n",
      "Training network. lr: 0.000146. clip: 0.058381\n",
      "Iteration 13600: Policy loss: 0.141082. Value loss: 0.141487. Entropy: 0.293128.\n",
      "Iteration 13601: Policy loss: 0.128696. Value loss: 0.071609. Entropy: 0.292049.\n",
      "Iteration 13602: Policy loss: 0.122782. Value loss: 0.049790. Entropy: 0.291618.\n",
      "episode: 4888   score: 365.0  epsilon: 1.0    steps: 264  evaluation reward: 446.55\n",
      "episode: 4889   score: 290.0  epsilon: 1.0    steps: 392  evaluation reward: 443.25\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13603: Policy loss: -0.241814. Value loss: 0.202305. Entropy: 0.280317.\n",
      "Iteration 13604: Policy loss: -0.270693. Value loss: 0.061522. Entropy: 0.283080.\n",
      "Iteration 13605: Policy loss: -0.270690. Value loss: 0.044203. Entropy: 0.281425.\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13606: Policy loss: -0.149435. Value loss: 0.090982. Entropy: 0.310600.\n",
      "Iteration 13607: Policy loss: -0.151774. Value loss: 0.043619. Entropy: 0.309221.\n",
      "Iteration 13608: Policy loss: -0.150829. Value loss: 0.030368. Entropy: 0.309390.\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13609: Policy loss: -0.015250. Value loss: 0.295512. Entropy: 0.301626.\n",
      "Iteration 13610: Policy loss: -0.044974. Value loss: 0.120890. Entropy: 0.303704.\n",
      "Iteration 13611: Policy loss: -0.029756. Value loss: 0.076652. Entropy: 0.302525.\n",
      "episode: 4890   score: 560.0  epsilon: 1.0    steps: 728  evaluation reward: 446.75\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13612: Policy loss: 0.193251. Value loss: 0.149620. Entropy: 0.300648.\n",
      "Iteration 13613: Policy loss: 0.194908. Value loss: 0.048739. Entropy: 0.300332.\n",
      "Iteration 13614: Policy loss: 0.176257. Value loss: 0.032450. Entropy: 0.300096.\n",
      "episode: 4891   score: 340.0  epsilon: 1.0    steps: 520  evaluation reward: 446.55\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13615: Policy loss: 0.331937. Value loss: 0.150071. Entropy: 0.300437.\n",
      "Iteration 13616: Policy loss: 0.332159. Value loss: 0.070837. Entropy: 0.298934.\n",
      "Iteration 13617: Policy loss: 0.326124. Value loss: 0.048682. Entropy: 0.299704.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4892   score: 265.0  epsilon: 1.0    steps: 448  evaluation reward: 445.3\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13618: Policy loss: -0.235453. Value loss: 0.347320. Entropy: 0.299752.\n",
      "Iteration 13619: Policy loss: -0.230404. Value loss: 0.127027. Entropy: 0.301023.\n",
      "Iteration 13620: Policy loss: -0.246439. Value loss: 0.076233. Entropy: 0.298853.\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13621: Policy loss: 0.200218. Value loss: 0.110886. Entropy: 0.310666.\n",
      "Iteration 13622: Policy loss: 0.192617. Value loss: 0.051543. Entropy: 0.309865.\n",
      "Iteration 13623: Policy loss: 0.189925. Value loss: 0.037936. Entropy: 0.310097.\n",
      "episode: 4893   score: 570.0  epsilon: 1.0    steps: 392  evaluation reward: 448.0\n",
      "episode: 4894   score: 230.0  epsilon: 1.0    steps: 664  evaluation reward: 448.8\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13624: Policy loss: 0.038048. Value loss: 0.280083. Entropy: 0.285305.\n",
      "Iteration 13625: Policy loss: 0.037535. Value loss: 0.123477. Entropy: 0.284456.\n",
      "Iteration 13626: Policy loss: 0.017369. Value loss: 0.089196. Entropy: 0.283378.\n",
      "episode: 4895   score: 605.0  epsilon: 1.0    steps: 328  evaluation reward: 452.0\n",
      "episode: 4896   score: 330.0  epsilon: 1.0    steps: 368  evaluation reward: 453.05\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13627: Policy loss: 0.354111. Value loss: 0.157868. Entropy: 0.290375.\n",
      "Iteration 13628: Policy loss: 0.336093. Value loss: 0.047507. Entropy: 0.290332.\n",
      "Iteration 13629: Policy loss: 0.339392. Value loss: 0.030813. Entropy: 0.290458.\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13630: Policy loss: 0.341229. Value loss: 0.113566. Entropy: 0.305160.\n",
      "Iteration 13631: Policy loss: 0.336683. Value loss: 0.047933. Entropy: 0.304570.\n",
      "Iteration 13632: Policy loss: 0.335653. Value loss: 0.035948. Entropy: 0.304720.\n",
      "episode: 4897   score: 390.0  epsilon: 1.0    steps: 176  evaluation reward: 450.15\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13633: Policy loss: -0.217413. Value loss: 0.271101. Entropy: 0.294703.\n",
      "Iteration 13634: Policy loss: -0.200611. Value loss: 0.070640. Entropy: 0.294095.\n",
      "Iteration 13635: Policy loss: -0.218030. Value loss: 0.043252. Entropy: 0.293442.\n",
      "episode: 4898   score: 315.0  epsilon: 1.0    steps: 600  evaluation reward: 449.3\n",
      "episode: 4899   score: 460.0  epsilon: 1.0    steps: 936  evaluation reward: 449.25\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13636: Policy loss: 0.201806. Value loss: 0.093829. Entropy: 0.300492.\n",
      "Iteration 13637: Policy loss: 0.197858. Value loss: 0.030727. Entropy: 0.300003.\n",
      "Iteration 13638: Policy loss: 0.199315. Value loss: 0.021691. Entropy: 0.299613.\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13639: Policy loss: 0.183758. Value loss: 0.254640. Entropy: 0.301098.\n",
      "Iteration 13640: Policy loss: 0.176269. Value loss: 0.076407. Entropy: 0.301619.\n",
      "Iteration 13641: Policy loss: 0.165614. Value loss: 0.048270. Entropy: 0.301590.\n",
      "episode: 4900   score: 265.0  epsilon: 1.0    steps: 592  evaluation reward: 449.05\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13642: Policy loss: 0.186909. Value loss: 0.140269. Entropy: 0.298353.\n",
      "Iteration 13643: Policy loss: 0.180867. Value loss: 0.044837. Entropy: 0.296664.\n",
      "Iteration 13644: Policy loss: 0.176126. Value loss: 0.030637. Entropy: 0.297580.\n",
      "now time :  2019-09-06 04:21:53.456610\n",
      "episode: 4901   score: 240.0  epsilon: 1.0    steps: 744  evaluation reward: 447.55\n",
      "episode: 4902   score: 180.0  epsilon: 1.0    steps: 1008  evaluation reward: 444.95\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13645: Policy loss: 0.043174. Value loss: 0.123553. Entropy: 0.298925.\n",
      "Iteration 13646: Policy loss: 0.037313. Value loss: 0.050276. Entropy: 0.299830.\n",
      "Iteration 13647: Policy loss: 0.031009. Value loss: 0.036755. Entropy: 0.300421.\n",
      "episode: 4903   score: 395.0  epsilon: 1.0    steps: 352  evaluation reward: 445.7\n",
      "episode: 4904   score: 155.0  epsilon: 1.0    steps: 744  evaluation reward: 444.65\n",
      "episode: 4905   score: 240.0  epsilon: 1.0    steps: 824  evaluation reward: 441.05\n",
      "Training network. lr: 0.000146. clip: 0.058224\n",
      "Iteration 13648: Policy loss: 0.047976. Value loss: 0.066940. Entropy: 0.275116.\n",
      "Iteration 13649: Policy loss: 0.049291. Value loss: 0.038874. Entropy: 0.274659.\n",
      "Iteration 13650: Policy loss: 0.050904. Value loss: 0.028489. Entropy: 0.274988.\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13651: Policy loss: 0.049893. Value loss: 0.085076. Entropy: 0.299549.\n",
      "Iteration 13652: Policy loss: 0.049723. Value loss: 0.037121. Entropy: 0.298824.\n",
      "Iteration 13653: Policy loss: 0.043488. Value loss: 0.024893. Entropy: 0.299245.\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13654: Policy loss: -0.208427. Value loss: 0.407570. Entropy: 0.305105.\n",
      "Iteration 13655: Policy loss: -0.218573. Value loss: 0.268317. Entropy: 0.305232.\n",
      "Iteration 13656: Policy loss: -0.203841. Value loss: 0.145643. Entropy: 0.306026.\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13657: Policy loss: 0.044225. Value loss: 0.157518. Entropy: 0.306864.\n",
      "Iteration 13658: Policy loss: 0.040966. Value loss: 0.059574. Entropy: 0.306098.\n",
      "Iteration 13659: Policy loss: 0.039009. Value loss: 0.042520. Entropy: 0.306470.\n",
      "episode: 4906   score: 440.0  epsilon: 1.0    steps: 144  evaluation reward: 442.75\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13660: Policy loss: 0.072257. Value loss: 0.075631. Entropy: 0.302154.\n",
      "Iteration 13661: Policy loss: 0.071868. Value loss: 0.028917. Entropy: 0.302431.\n",
      "Iteration 13662: Policy loss: 0.065243. Value loss: 0.021667. Entropy: 0.301741.\n",
      "episode: 4907   score: 165.0  epsilon: 1.0    steps: 712  evaluation reward: 440.9\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13663: Policy loss: 0.192010. Value loss: 0.169890. Entropy: 0.299465.\n",
      "Iteration 13664: Policy loss: 0.179130. Value loss: 0.055365. Entropy: 0.299184.\n",
      "Iteration 13665: Policy loss: 0.172431. Value loss: 0.035635. Entropy: 0.299629.\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13666: Policy loss: 0.122255. Value loss: 0.106657. Entropy: 0.302347.\n",
      "Iteration 13667: Policy loss: 0.120909. Value loss: 0.045345. Entropy: 0.302556.\n",
      "Iteration 13668: Policy loss: 0.113906. Value loss: 0.029124. Entropy: 0.303043.\n",
      "episode: 4908   score: 540.0  epsilon: 1.0    steps: 112  evaluation reward: 444.5\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13669: Policy loss: 0.016819. Value loss: 0.088803. Entropy: 0.297498.\n",
      "Iteration 13670: Policy loss: 0.023220. Value loss: 0.039589. Entropy: 0.297233.\n",
      "Iteration 13671: Policy loss: 0.017118. Value loss: 0.030422. Entropy: 0.297742.\n",
      "episode: 4909   score: 285.0  epsilon: 1.0    steps: 784  evaluation reward: 440.95\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13672: Policy loss: 0.078607. Value loss: 0.086852. Entropy: 0.297776.\n",
      "Iteration 13673: Policy loss: 0.064090. Value loss: 0.041201. Entropy: 0.297301.\n",
      "Iteration 13674: Policy loss: 0.065380. Value loss: 0.031131. Entropy: 0.297503.\n",
      "episode: 4910   score: 365.0  epsilon: 1.0    steps: 128  evaluation reward: 439.55\n",
      "episode: 4911   score: 470.0  epsilon: 1.0    steps: 808  evaluation reward: 439.35\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13675: Policy loss: -0.319130. Value loss: 0.222793. Entropy: 0.292588.\n",
      "Iteration 13676: Policy loss: -0.338287. Value loss: 0.069880. Entropy: 0.291779.\n",
      "Iteration 13677: Policy loss: -0.338966. Value loss: 0.046594. Entropy: 0.290861.\n",
      "episode: 4912   score: 640.0  epsilon: 1.0    steps: 584  evaluation reward: 441.1\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13678: Policy loss: -0.413189. Value loss: 0.333986. Entropy: 0.294067.\n",
      "Iteration 13679: Policy loss: -0.413221. Value loss: 0.100186. Entropy: 0.292931.\n",
      "Iteration 13680: Policy loss: -0.450285. Value loss: 0.054363. Entropy: 0.293021.\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13681: Policy loss: 0.343828. Value loss: 0.343701. Entropy: 0.307744.\n",
      "Iteration 13682: Policy loss: 0.336992. Value loss: 0.113949. Entropy: 0.306575.\n",
      "Iteration 13683: Policy loss: 0.338726. Value loss: 0.081129. Entropy: 0.306968.\n",
      "episode: 4913   score: 100.0  epsilon: 1.0    steps: 584  evaluation reward: 437.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13684: Policy loss: 0.263702. Value loss: 0.094346. Entropy: 0.290135.\n",
      "Iteration 13685: Policy loss: 0.257541. Value loss: 0.041321. Entropy: 0.290659.\n",
      "Iteration 13686: Policy loss: 0.256620. Value loss: 0.027986. Entropy: 0.289648.\n",
      "episode: 4914   score: 660.0  epsilon: 1.0    steps: 456  evaluation reward: 437.55\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13687: Policy loss: -0.020835. Value loss: 0.105588. Entropy: 0.301305.\n",
      "Iteration 13688: Policy loss: -0.029640. Value loss: 0.043553. Entropy: 0.301355.\n",
      "Iteration 13689: Policy loss: -0.028576. Value loss: 0.030553. Entropy: 0.300768.\n",
      "episode: 4915   score: 525.0  epsilon: 1.0    steps: 72  evaluation reward: 436.9\n",
      "episode: 4916   score: 425.0  epsilon: 1.0    steps: 1000  evaluation reward: 436.65\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13690: Policy loss: -0.099152. Value loss: 0.109663. Entropy: 0.291407.\n",
      "Iteration 13691: Policy loss: -0.099222. Value loss: 0.046471. Entropy: 0.291488.\n",
      "Iteration 13692: Policy loss: -0.101768. Value loss: 0.033577. Entropy: 0.290763.\n",
      "episode: 4917   score: 620.0  epsilon: 1.0    steps: 344  evaluation reward: 437.85\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13693: Policy loss: -0.283756. Value loss: 0.219513. Entropy: 0.282162.\n",
      "Iteration 13694: Policy loss: -0.296887. Value loss: 0.060890. Entropy: 0.281452.\n",
      "Iteration 13695: Policy loss: -0.294636. Value loss: 0.040649. Entropy: 0.279517.\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13696: Policy loss: 0.281898. Value loss: 0.169693. Entropy: 0.301569.\n",
      "Iteration 13697: Policy loss: 0.269036. Value loss: 0.047083. Entropy: 0.301939.\n",
      "Iteration 13698: Policy loss: 0.249817. Value loss: 0.030367. Entropy: 0.300973.\n",
      "episode: 4918   score: 390.0  epsilon: 1.0    steps: 288  evaluation reward: 435.85\n",
      "Training network. lr: 0.000145. clip: 0.058076\n",
      "Iteration 13699: Policy loss: 0.237144. Value loss: 0.149072. Entropy: 0.293801.\n",
      "Iteration 13700: Policy loss: 0.230663. Value loss: 0.058765. Entropy: 0.294174.\n",
      "Iteration 13701: Policy loss: 0.222890. Value loss: 0.034023. Entropy: 0.292808.\n",
      "episode: 4919   score: 315.0  epsilon: 1.0    steps: 488  evaluation reward: 432.3\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13702: Policy loss: 0.118080. Value loss: 0.075009. Entropy: 0.294946.\n",
      "Iteration 13703: Policy loss: 0.108532. Value loss: 0.028700. Entropy: 0.294883.\n",
      "Iteration 13704: Policy loss: 0.105476. Value loss: 0.014675. Entropy: 0.295141.\n",
      "episode: 4920   score: 605.0  epsilon: 1.0    steps: 784  evaluation reward: 434.2\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13705: Policy loss: -0.125355. Value loss: 0.349893. Entropy: 0.295341.\n",
      "Iteration 13706: Policy loss: -0.123602. Value loss: 0.251726. Entropy: 0.296816.\n",
      "Iteration 13707: Policy loss: -0.122494. Value loss: 0.191418. Entropy: 0.294980.\n",
      "episode: 4921   score: 295.0  epsilon: 1.0    steps: 712  evaluation reward: 433.85\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13708: Policy loss: 0.238222. Value loss: 0.077998. Entropy: 0.295050.\n",
      "Iteration 13709: Policy loss: 0.238676. Value loss: 0.036189. Entropy: 0.294308.\n",
      "Iteration 13710: Policy loss: 0.231140. Value loss: 0.028440. Entropy: 0.294662.\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13711: Policy loss: 0.073255. Value loss: 0.126288. Entropy: 0.304243.\n",
      "Iteration 13712: Policy loss: 0.065372. Value loss: 0.054851. Entropy: 0.304114.\n",
      "Iteration 13713: Policy loss: 0.061693. Value loss: 0.040064. Entropy: 0.302756.\n",
      "episode: 4922   score: 125.0  epsilon: 1.0    steps: 416  evaluation reward: 432.5\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13714: Policy loss: 0.124994. Value loss: 0.083920. Entropy: 0.294395.\n",
      "Iteration 13715: Policy loss: 0.121951. Value loss: 0.034962. Entropy: 0.293357.\n",
      "Iteration 13716: Policy loss: 0.115264. Value loss: 0.025805. Entropy: 0.294044.\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13717: Policy loss: 0.252502. Value loss: 0.109055. Entropy: 0.309890.\n",
      "Iteration 13718: Policy loss: 0.242703. Value loss: 0.033464. Entropy: 0.309574.\n",
      "Iteration 13719: Policy loss: 0.243401. Value loss: 0.025867. Entropy: 0.309581.\n",
      "episode: 4923   score: 620.0  epsilon: 1.0    steps: 168  evaluation reward: 435.2\n",
      "episode: 4924   score: 365.0  epsilon: 1.0    steps: 440  evaluation reward: 433.95\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13720: Policy loss: 0.018774. Value loss: 0.076722. Entropy: 0.277753.\n",
      "Iteration 13721: Policy loss: 0.019047. Value loss: 0.031062. Entropy: 0.279920.\n",
      "Iteration 13722: Policy loss: 0.014525. Value loss: 0.021524. Entropy: 0.278625.\n",
      "episode: 4925   score: 365.0  epsilon: 1.0    steps: 48  evaluation reward: 434.75\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13723: Policy loss: -0.188199. Value loss: 0.145917. Entropy: 0.294805.\n",
      "Iteration 13724: Policy loss: -0.190053. Value loss: 0.039169. Entropy: 0.296553.\n",
      "Iteration 13725: Policy loss: -0.201419. Value loss: 0.026816. Entropy: 0.296230.\n",
      "episode: 4926   score: 440.0  epsilon: 1.0    steps: 376  evaluation reward: 433.2\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13726: Policy loss: -0.087513. Value loss: 0.150178. Entropy: 0.293128.\n",
      "Iteration 13727: Policy loss: -0.095536. Value loss: 0.074698. Entropy: 0.290375.\n",
      "Iteration 13728: Policy loss: -0.105177. Value loss: 0.051031. Entropy: 0.290105.\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13729: Policy loss: 0.124760. Value loss: 0.071857. Entropy: 0.309058.\n",
      "Iteration 13730: Policy loss: 0.126498. Value loss: 0.037754. Entropy: 0.308219.\n",
      "Iteration 13731: Policy loss: 0.122128. Value loss: 0.030657. Entropy: 0.307936.\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13732: Policy loss: 0.254017. Value loss: 0.110270. Entropy: 0.305763.\n",
      "Iteration 13733: Policy loss: 0.248968. Value loss: 0.045298. Entropy: 0.304943.\n",
      "Iteration 13734: Policy loss: 0.245716. Value loss: 0.031733. Entropy: 0.305207.\n",
      "episode: 4927   score: 340.0  epsilon: 1.0    steps: 264  evaluation reward: 433.4\n",
      "episode: 4928   score: 680.0  epsilon: 1.0    steps: 528  evaluation reward: 434.25\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13735: Policy loss: 0.089440. Value loss: 0.113364. Entropy: 0.288195.\n",
      "Iteration 13736: Policy loss: 0.084337. Value loss: 0.059391. Entropy: 0.287635.\n",
      "Iteration 13737: Policy loss: 0.091112. Value loss: 0.037566. Entropy: 0.288734.\n",
      "episode: 4929   score: 285.0  epsilon: 1.0    steps: 128  evaluation reward: 430.6\n",
      "episode: 4930   score: 280.0  epsilon: 1.0    steps: 992  evaluation reward: 430.5\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13738: Policy loss: 0.158340. Value loss: 0.113788. Entropy: 0.297440.\n",
      "Iteration 13739: Policy loss: 0.156478. Value loss: 0.054869. Entropy: 0.298503.\n",
      "Iteration 13740: Policy loss: 0.154128. Value loss: 0.040123. Entropy: 0.297166.\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13741: Policy loss: 0.047858. Value loss: 0.079419. Entropy: 0.303072.\n",
      "Iteration 13742: Policy loss: 0.043315. Value loss: 0.038573. Entropy: 0.301892.\n",
      "Iteration 13743: Policy loss: 0.041728. Value loss: 0.025311. Entropy: 0.302301.\n",
      "episode: 4931   score: 420.0  epsilon: 1.0    steps: 344  evaluation reward: 430.75\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13744: Policy loss: 0.233818. Value loss: 0.083003. Entropy: 0.306389.\n",
      "Iteration 13745: Policy loss: 0.229115. Value loss: 0.035598. Entropy: 0.305775.\n",
      "Iteration 13746: Policy loss: 0.222381. Value loss: 0.026373. Entropy: 0.305866.\n",
      "episode: 4932   score: 315.0  epsilon: 1.0    steps: 144  evaluation reward: 424.45\n",
      "episode: 4933   score: 505.0  epsilon: 1.0    steps: 864  evaluation reward: 424.25\n",
      "Training network. lr: 0.000145. clip: 0.057920\n",
      "Iteration 13747: Policy loss: -0.326376. Value loss: 0.335023. Entropy: 0.298233.\n",
      "Iteration 13748: Policy loss: -0.336356. Value loss: 0.247812. Entropy: 0.297177.\n",
      "Iteration 13749: Policy loss: -0.357961. Value loss: 0.210691. Entropy: 0.298963.\n",
      "episode: 4934   score: 395.0  epsilon: 1.0    steps: 800  evaluation reward: 424.55\n",
      "Training network. lr: 0.000145. clip: 0.057920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13750: Policy loss: -0.066928. Value loss: 0.074673. Entropy: 0.304340.\n",
      "Iteration 13751: Policy loss: -0.070034. Value loss: 0.035704. Entropy: 0.304537.\n",
      "Iteration 13752: Policy loss: -0.072209. Value loss: 0.027386. Entropy: 0.304149.\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13753: Policy loss: 0.159513. Value loss: 0.089757. Entropy: 0.309339.\n",
      "Iteration 13754: Policy loss: 0.160290. Value loss: 0.030449. Entropy: 0.309333.\n",
      "Iteration 13755: Policy loss: 0.152283. Value loss: 0.020818. Entropy: 0.308811.\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13756: Policy loss: -0.387531. Value loss: 0.337736. Entropy: 0.300533.\n",
      "Iteration 13757: Policy loss: -0.408868. Value loss: 0.241963. Entropy: 0.303179.\n",
      "Iteration 13758: Policy loss: -0.406500. Value loss: 0.200855. Entropy: 0.302783.\n",
      "episode: 4935   score: 535.0  epsilon: 1.0    steps: 408  evaluation reward: 426.25\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13759: Policy loss: -0.233321. Value loss: 0.120782. Entropy: 0.305251.\n",
      "Iteration 13760: Policy loss: -0.241779. Value loss: 0.050115. Entropy: 0.304200.\n",
      "Iteration 13761: Policy loss: -0.242083. Value loss: 0.034835. Entropy: 0.304288.\n",
      "episode: 4936   score: 360.0  epsilon: 1.0    steps: 688  evaluation reward: 428.8\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13762: Policy loss: -0.082035. Value loss: 0.086968. Entropy: 0.305701.\n",
      "Iteration 13763: Policy loss: -0.091207. Value loss: 0.032939. Entropy: 0.304983.\n",
      "Iteration 13764: Policy loss: -0.091871. Value loss: 0.022251. Entropy: 0.305139.\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13765: Policy loss: -0.041237. Value loss: 0.103871. Entropy: 0.304334.\n",
      "Iteration 13766: Policy loss: -0.050503. Value loss: 0.044358. Entropy: 0.306020.\n",
      "Iteration 13767: Policy loss: -0.048792. Value loss: 0.028761. Entropy: 0.305223.\n",
      "episode: 4937   score: 300.0  epsilon: 1.0    steps: 192  evaluation reward: 428.2\n",
      "episode: 4938   score: 420.0  epsilon: 1.0    steps: 216  evaluation reward: 426.2\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13768: Policy loss: 0.062606. Value loss: 0.089913. Entropy: 0.297120.\n",
      "Iteration 13769: Policy loss: 0.061643. Value loss: 0.037252. Entropy: 0.294504.\n",
      "Iteration 13770: Policy loss: 0.053515. Value loss: 0.025761. Entropy: 0.294943.\n",
      "episode: 4939   score: 235.0  epsilon: 1.0    steps: 1008  evaluation reward: 424.15\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13771: Policy loss: 0.319384. Value loss: 0.133014. Entropy: 0.308022.\n",
      "Iteration 13772: Policy loss: 0.301824. Value loss: 0.058586. Entropy: 0.307295.\n",
      "Iteration 13773: Policy loss: 0.302105. Value loss: 0.040313. Entropy: 0.308101.\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13774: Policy loss: 0.018922. Value loss: 0.070605. Entropy: 0.300620.\n",
      "Iteration 13775: Policy loss: 0.015777. Value loss: 0.025065. Entropy: 0.299507.\n",
      "Iteration 13776: Policy loss: 0.014552. Value loss: 0.014859. Entropy: 0.299396.\n",
      "episode: 4940   score: 550.0  epsilon: 1.0    steps: 224  evaluation reward: 424.3\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13777: Policy loss: -0.252653. Value loss: 0.334202. Entropy: 0.296777.\n",
      "Iteration 13778: Policy loss: -0.253256. Value loss: 0.203853. Entropy: 0.294393.\n",
      "Iteration 13779: Policy loss: -0.269834. Value loss: 0.146516. Entropy: 0.294446.\n",
      "episode: 4941   score: 635.0  epsilon: 1.0    steps: 216  evaluation reward: 426.6\n",
      "episode: 4942   score: 345.0  epsilon: 1.0    steps: 1008  evaluation reward: 425.1\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13780: Policy loss: -0.209663. Value loss: 0.336534. Entropy: 0.296196.\n",
      "Iteration 13781: Policy loss: -0.223773. Value loss: 0.184567. Entropy: 0.295469.\n",
      "Iteration 13782: Policy loss: -0.201483. Value loss: 0.078877. Entropy: 0.297378.\n",
      "episode: 4943   score: 665.0  epsilon: 1.0    steps: 648  evaluation reward: 428.1\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13783: Policy loss: -0.072529. Value loss: 0.081094. Entropy: 0.285503.\n",
      "Iteration 13784: Policy loss: -0.074875. Value loss: 0.033781. Entropy: 0.284804.\n",
      "Iteration 13785: Policy loss: -0.076460. Value loss: 0.022064. Entropy: 0.284587.\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13786: Policy loss: -0.005433. Value loss: 0.254670. Entropy: 0.308309.\n",
      "Iteration 13787: Policy loss: -0.006013. Value loss: 0.139244. Entropy: 0.309387.\n",
      "Iteration 13788: Policy loss: -0.018254. Value loss: 0.080023. Entropy: 0.307378.\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13789: Policy loss: 0.355529. Value loss: 0.124455. Entropy: 0.310670.\n",
      "Iteration 13790: Policy loss: 0.344478. Value loss: 0.054592. Entropy: 0.309265.\n",
      "Iteration 13791: Policy loss: 0.347871. Value loss: 0.042442. Entropy: 0.307866.\n",
      "episode: 4944   score: 210.0  epsilon: 1.0    steps: 112  evaluation reward: 425.1\n",
      "episode: 4945   score: 545.0  epsilon: 1.0    steps: 216  evaluation reward: 428.45\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13792: Policy loss: 0.001409. Value loss: 0.075478. Entropy: 0.284484.\n",
      "Iteration 13793: Policy loss: -0.007569. Value loss: 0.035048. Entropy: 0.285562.\n",
      "Iteration 13794: Policy loss: -0.007006. Value loss: 0.027178. Entropy: 0.285586.\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13795: Policy loss: 0.197988. Value loss: 0.181510. Entropy: 0.311712.\n",
      "Iteration 13796: Policy loss: 0.185766. Value loss: 0.056920. Entropy: 0.310438.\n",
      "Iteration 13797: Policy loss: 0.187085. Value loss: 0.035682. Entropy: 0.311075.\n",
      "episode: 4946   score: 210.0  epsilon: 1.0    steps: 760  evaluation reward: 427.4\n",
      "Training network. lr: 0.000144. clip: 0.057763\n",
      "Iteration 13798: Policy loss: 0.200425. Value loss: 0.128805. Entropy: 0.302401.\n",
      "Iteration 13799: Policy loss: 0.192756. Value loss: 0.052468. Entropy: 0.299820.\n",
      "Iteration 13800: Policy loss: 0.193254. Value loss: 0.037950. Entropy: 0.300359.\n",
      "episode: 4947   score: 420.0  epsilon: 1.0    steps: 640  evaluation reward: 426.15\n",
      "episode: 4948   score: 695.0  epsilon: 1.0    steps: 736  evaluation reward: 428.9\n",
      "episode: 4949   score: 625.0  epsilon: 1.0    steps: 792  evaluation reward: 428.8\n",
      "episode: 4950   score: 210.0  epsilon: 1.0    steps: 968  evaluation reward: 427.55\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13801: Policy loss: 0.177151. Value loss: 0.127804. Entropy: 0.275313.\n",
      "Iteration 13802: Policy loss: 0.172921. Value loss: 0.059047. Entropy: 0.272287.\n",
      "Iteration 13803: Policy loss: 0.164881. Value loss: 0.042429. Entropy: 0.271660.\n",
      "now time :  2019-09-06 04:31:45.831149\n",
      "episode: 4951   score: 600.0  epsilon: 1.0    steps: 744  evaluation reward: 429.95\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13804: Policy loss: -0.181138. Value loss: 0.081989. Entropy: 0.295301.\n",
      "Iteration 13805: Policy loss: -0.183755. Value loss: 0.043016. Entropy: 0.292849.\n",
      "Iteration 13806: Policy loss: -0.187804. Value loss: 0.035059. Entropy: 0.293520.\n",
      "episode: 4952   score: 210.0  epsilon: 1.0    steps: 944  evaluation reward: 426.4\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13807: Policy loss: -0.044347. Value loss: 0.077194. Entropy: 0.307423.\n",
      "Iteration 13808: Policy loss: -0.046005. Value loss: 0.037259. Entropy: 0.307207.\n",
      "Iteration 13809: Policy loss: -0.051152. Value loss: 0.028470. Entropy: 0.307213.\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13810: Policy loss: -0.212092. Value loss: 0.326748. Entropy: 0.301613.\n",
      "Iteration 13811: Policy loss: -0.214067. Value loss: 0.105231. Entropy: 0.300743.\n",
      "Iteration 13812: Policy loss: -0.212847. Value loss: 0.063576. Entropy: 0.299958.\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13813: Policy loss: -0.422751. Value loss: 0.413059. Entropy: 0.303904.\n",
      "Iteration 13814: Policy loss: -0.428808. Value loss: 0.269116. Entropy: 0.303532.\n",
      "Iteration 13815: Policy loss: -0.463966. Value loss: 0.196541. Entropy: 0.304156.\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13816: Policy loss: -0.127794. Value loss: 0.162621. Entropy: 0.303220.\n",
      "Iteration 13817: Policy loss: -0.138605. Value loss: 0.068518. Entropy: 0.303215.\n",
      "Iteration 13818: Policy loss: -0.141854. Value loss: 0.038091. Entropy: 0.303797.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13819: Policy loss: 0.273240. Value loss: 0.195377. Entropy: 0.310326.\n",
      "Iteration 13820: Policy loss: 0.260658. Value loss: 0.060758. Entropy: 0.309195.\n",
      "Iteration 13821: Policy loss: 0.251218. Value loss: 0.033326. Entropy: 0.308990.\n",
      "episode: 4953   score: 565.0  epsilon: 1.0    steps: 1008  evaluation reward: 425.35\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13822: Policy loss: 0.311992. Value loss: 0.164922. Entropy: 0.298685.\n",
      "Iteration 13823: Policy loss: 0.292794. Value loss: 0.068820. Entropy: 0.296902.\n",
      "Iteration 13824: Policy loss: 0.289781. Value loss: 0.044344. Entropy: 0.296111.\n",
      "episode: 4954   score: 460.0  epsilon: 1.0    steps: 280  evaluation reward: 427.85\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13825: Policy loss: -0.281105. Value loss: 0.337679. Entropy: 0.274414.\n",
      "Iteration 13826: Policy loss: -0.280813. Value loss: 0.180460. Entropy: 0.273885.\n",
      "Iteration 13827: Policy loss: -0.304388. Value loss: 0.132254. Entropy: 0.276250.\n",
      "episode: 4955   score: 315.0  epsilon: 1.0    steps: 720  evaluation reward: 427.7\n",
      "episode: 4956   score: 545.0  epsilon: 1.0    steps: 744  evaluation reward: 425.45\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13828: Policy loss: -0.079366. Value loss: 0.080162. Entropy: 0.284538.\n",
      "Iteration 13829: Policy loss: -0.083521. Value loss: 0.045369. Entropy: 0.284447.\n",
      "Iteration 13830: Policy loss: -0.090587. Value loss: 0.032879. Entropy: 0.286558.\n",
      "episode: 4957   score: 800.0  epsilon: 1.0    steps: 432  evaluation reward: 430.6\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13831: Policy loss: -0.125705. Value loss: 0.234391. Entropy: 0.293478.\n",
      "Iteration 13832: Policy loss: -0.138865. Value loss: 0.107014. Entropy: 0.292815.\n",
      "Iteration 13833: Policy loss: -0.119074. Value loss: 0.060783. Entropy: 0.291595.\n",
      "episode: 4958   score: 420.0  epsilon: 1.0    steps: 824  evaluation reward: 429.8\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13834: Policy loss: -0.031974. Value loss: 0.249695. Entropy: 0.304054.\n",
      "Iteration 13835: Policy loss: -0.052549. Value loss: 0.099592. Entropy: 0.304338.\n",
      "Iteration 13836: Policy loss: -0.049095. Value loss: 0.055354. Entropy: 0.303729.\n",
      "episode: 4959   score: 790.0  epsilon: 1.0    steps: 520  evaluation reward: 431.05\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13837: Policy loss: 0.161495. Value loss: 0.374391. Entropy: 0.298775.\n",
      "Iteration 13838: Policy loss: 0.148706. Value loss: 0.141780. Entropy: 0.296270.\n",
      "Iteration 13839: Policy loss: 0.121574. Value loss: 0.098586. Entropy: 0.297841.\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13840: Policy loss: 0.241892. Value loss: 0.198846. Entropy: 0.307335.\n",
      "Iteration 13841: Policy loss: 0.228002. Value loss: 0.061507. Entropy: 0.306547.\n",
      "Iteration 13842: Policy loss: 0.221909. Value loss: 0.037970. Entropy: 0.305677.\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13843: Policy loss: -0.264314. Value loss: 0.189045. Entropy: 0.310700.\n",
      "Iteration 13844: Policy loss: -0.263348. Value loss: 0.080911. Entropy: 0.310597.\n",
      "Iteration 13845: Policy loss: -0.266636. Value loss: 0.052863. Entropy: 0.309920.\n",
      "episode: 4960   score: 775.0  epsilon: 1.0    steps: 112  evaluation reward: 434.55\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13846: Policy loss: 0.231140. Value loss: 0.218084. Entropy: 0.295586.\n",
      "Iteration 13847: Policy loss: 0.222126. Value loss: 0.073781. Entropy: 0.296878.\n",
      "Iteration 13848: Policy loss: 0.214776. Value loss: 0.046038. Entropy: 0.296095.\n",
      "episode: 4961   score: 370.0  epsilon: 1.0    steps: 480  evaluation reward: 435.05\n",
      "Training network. lr: 0.000144. clip: 0.057616\n",
      "Iteration 13849: Policy loss: -0.258908. Value loss: 0.385811. Entropy: 0.299364.\n",
      "Iteration 13850: Policy loss: -0.289921. Value loss: 0.245273. Entropy: 0.298986.\n",
      "Iteration 13851: Policy loss: -0.283351. Value loss: 0.185851. Entropy: 0.299138.\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13852: Policy loss: 0.200700. Value loss: 0.152019. Entropy: 0.301842.\n",
      "Iteration 13853: Policy loss: 0.185248. Value loss: 0.055658. Entropy: 0.302269.\n",
      "Iteration 13854: Policy loss: 0.176524. Value loss: 0.037954. Entropy: 0.302897.\n",
      "episode: 4962   score: 340.0  epsilon: 1.0    steps: 304  evaluation reward: 431.5\n",
      "episode: 4963   score: 345.0  epsilon: 1.0    steps: 456  evaluation reward: 429.7\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13855: Policy loss: 0.161888. Value loss: 0.102891. Entropy: 0.279809.\n",
      "Iteration 13856: Policy loss: 0.150477. Value loss: 0.040723. Entropy: 0.278601.\n",
      "Iteration 13857: Policy loss: 0.143413. Value loss: 0.027130. Entropy: 0.277901.\n",
      "episode: 4964   score: 675.0  epsilon: 1.0    steps: 416  evaluation reward: 432.3\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13858: Policy loss: 0.020811. Value loss: 0.062573. Entropy: 0.297499.\n",
      "Iteration 13859: Policy loss: 0.017229. Value loss: 0.028808. Entropy: 0.297994.\n",
      "Iteration 13860: Policy loss: 0.015513. Value loss: 0.022131. Entropy: 0.297099.\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13861: Policy loss: -0.109469. Value loss: 0.159389. Entropy: 0.311526.\n",
      "Iteration 13862: Policy loss: -0.123168. Value loss: 0.073529. Entropy: 0.308445.\n",
      "Iteration 13863: Policy loss: -0.130314. Value loss: 0.047240. Entropy: 0.310611.\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13864: Policy loss: -0.256137. Value loss: 0.594540. Entropy: 0.315223.\n",
      "Iteration 13865: Policy loss: -0.262688. Value loss: 0.258411. Entropy: 0.312424.\n",
      "Iteration 13866: Policy loss: -0.295724. Value loss: 0.162403. Entropy: 0.313896.\n",
      "episode: 4965   score: 940.0  epsilon: 1.0    steps: 24  evaluation reward: 434.75\n",
      "episode: 4966   score: 285.0  epsilon: 1.0    steps: 144  evaluation reward: 431.15\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13867: Policy loss: 0.577706. Value loss: 0.203917. Entropy: 0.281931.\n",
      "Iteration 13868: Policy loss: 0.576073. Value loss: 0.068524. Entropy: 0.283075.\n",
      "Iteration 13869: Policy loss: 0.571649. Value loss: 0.048174. Entropy: 0.280817.\n",
      "episode: 4967   score: 455.0  epsilon: 1.0    steps: 536  evaluation reward: 432.85\n",
      "episode: 4968   score: 320.0  epsilon: 1.0    steps: 840  evaluation reward: 431.8\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13870: Policy loss: 0.172123. Value loss: 0.149719. Entropy: 0.288372.\n",
      "Iteration 13871: Policy loss: 0.171038. Value loss: 0.061855. Entropy: 0.287645.\n",
      "Iteration 13872: Policy loss: 0.162255. Value loss: 0.044164. Entropy: 0.288874.\n",
      "episode: 4969   score: 895.0  epsilon: 1.0    steps: 472  evaluation reward: 434.1\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13873: Policy loss: 0.281234. Value loss: 0.140355. Entropy: 0.291643.\n",
      "Iteration 13874: Policy loss: 0.278497. Value loss: 0.045247. Entropy: 0.289919.\n",
      "Iteration 13875: Policy loss: 0.279266. Value loss: 0.027679. Entropy: 0.291038.\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13876: Policy loss: 0.310319. Value loss: 0.140672. Entropy: 0.307932.\n",
      "Iteration 13877: Policy loss: 0.307801. Value loss: 0.050482. Entropy: 0.307360.\n",
      "Iteration 13878: Policy loss: 0.287949. Value loss: 0.037188. Entropy: 0.307127.\n",
      "episode: 4970   score: 500.0  epsilon: 1.0    steps: 416  evaluation reward: 437.85\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13879: Policy loss: -0.291970. Value loss: 0.309701. Entropy: 0.288870.\n",
      "Iteration 13880: Policy loss: -0.300524. Value loss: 0.215786. Entropy: 0.288823.\n",
      "Iteration 13881: Policy loss: -0.301095. Value loss: 0.170969. Entropy: 0.287663.\n",
      "episode: 4971   score: 365.0  epsilon: 1.0    steps: 552  evaluation reward: 437.0\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13882: Policy loss: 0.144420. Value loss: 0.113173. Entropy: 0.289545.\n",
      "Iteration 13883: Policy loss: 0.131027. Value loss: 0.046469. Entropy: 0.290051.\n",
      "Iteration 13884: Policy loss: 0.127858. Value loss: 0.030838. Entropy: 0.290144.\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13885: Policy loss: 0.358066. Value loss: 0.136364. Entropy: 0.305452.\n",
      "Iteration 13886: Policy loss: 0.354651. Value loss: 0.047368. Entropy: 0.305743.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13887: Policy loss: 0.343204. Value loss: 0.029123. Entropy: 0.306091.\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13888: Policy loss: -0.019030. Value loss: 0.168571. Entropy: 0.308689.\n",
      "Iteration 13889: Policy loss: -0.021450. Value loss: 0.064498. Entropy: 0.309123.\n",
      "Iteration 13890: Policy loss: -0.025324. Value loss: 0.041114. Entropy: 0.309183.\n",
      "episode: 4972   score: 340.0  epsilon: 1.0    steps: 936  evaluation reward: 435.2\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13891: Policy loss: 0.131167. Value loss: 0.115040. Entropy: 0.308491.\n",
      "Iteration 13892: Policy loss: 0.130816. Value loss: 0.036986. Entropy: 0.309590.\n",
      "Iteration 13893: Policy loss: 0.123065. Value loss: 0.025929. Entropy: 0.310033.\n",
      "episode: 4973   score: 590.0  epsilon: 1.0    steps: 184  evaluation reward: 438.2\n",
      "episode: 4974   score: 230.0  epsilon: 1.0    steps: 224  evaluation reward: 434.85\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13894: Policy loss: -0.032954. Value loss: 0.064439. Entropy: 0.283618.\n",
      "Iteration 13895: Policy loss: -0.039411. Value loss: 0.025699. Entropy: 0.283911.\n",
      "Iteration 13896: Policy loss: -0.042804. Value loss: 0.020064. Entropy: 0.284563.\n",
      "episode: 4975   score: 245.0  epsilon: 1.0    steps: 296  evaluation reward: 431.35\n",
      "episode: 4976   score: 425.0  epsilon: 1.0    steps: 848  evaluation reward: 432.65\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13897: Policy loss: -0.042635. Value loss: 0.340814. Entropy: 0.288399.\n",
      "Iteration 13898: Policy loss: -0.078565. Value loss: 0.241027. Entropy: 0.288604.\n",
      "Iteration 13899: Policy loss: -0.077691. Value loss: 0.183076. Entropy: 0.286771.\n",
      "Training network. lr: 0.000144. clip: 0.057459\n",
      "Iteration 13900: Policy loss: 0.176577. Value loss: 0.102315. Entropy: 0.303788.\n",
      "Iteration 13901: Policy loss: 0.169320. Value loss: 0.043477. Entropy: 0.302498.\n",
      "Iteration 13902: Policy loss: 0.160853. Value loss: 0.034266. Entropy: 0.301999.\n",
      "episode: 4977   score: 510.0  epsilon: 1.0    steps: 96  evaluation reward: 433.0\n",
      "episode: 4978   score: 180.0  epsilon: 1.0    steps: 552  evaluation reward: 429.8\n",
      "episode: 4979   score: 620.0  epsilon: 1.0    steps: 1008  evaluation reward: 428.6\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13903: Policy loss: 0.153087. Value loss: 0.085871. Entropy: 0.285375.\n",
      "Iteration 13904: Policy loss: 0.150909. Value loss: 0.043157. Entropy: 0.283930.\n",
      "Iteration 13905: Policy loss: 0.147615. Value loss: 0.032838. Entropy: 0.285114.\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13906: Policy loss: -0.018338. Value loss: 0.072269. Entropy: 0.301162.\n",
      "Iteration 13907: Policy loss: -0.013269. Value loss: 0.026519. Entropy: 0.301559.\n",
      "Iteration 13908: Policy loss: -0.014469. Value loss: 0.016386. Entropy: 0.300807.\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13909: Policy loss: 0.168577. Value loss: 0.096683. Entropy: 0.307241.\n",
      "Iteration 13910: Policy loss: 0.159282. Value loss: 0.031283. Entropy: 0.306690.\n",
      "Iteration 13911: Policy loss: 0.164376. Value loss: 0.023712. Entropy: 0.306771.\n",
      "episode: 4980   score: 135.0  epsilon: 1.0    steps: 752  evaluation reward: 424.35\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13912: Policy loss: 0.128377. Value loss: 0.176490. Entropy: 0.300700.\n",
      "Iteration 13913: Policy loss: 0.119411. Value loss: 0.051106. Entropy: 0.301716.\n",
      "Iteration 13914: Policy loss: 0.113038. Value loss: 0.034269. Entropy: 0.300648.\n",
      "episode: 4981   score: 240.0  epsilon: 1.0    steps: 880  evaluation reward: 422.55\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13915: Policy loss: -0.094410. Value loss: 0.149229. Entropy: 0.305338.\n",
      "Iteration 13916: Policy loss: -0.097525. Value loss: 0.059669. Entropy: 0.304722.\n",
      "Iteration 13917: Policy loss: -0.112493. Value loss: 0.037175. Entropy: 0.303926.\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13918: Policy loss: -0.284152. Value loss: 0.292331. Entropy: 0.305805.\n",
      "Iteration 13919: Policy loss: -0.286917. Value loss: 0.076033. Entropy: 0.307169.\n",
      "Iteration 13920: Policy loss: -0.300442. Value loss: 0.040411. Entropy: 0.306191.\n",
      "episode: 4982   score: 660.0  epsilon: 1.0    steps: 104  evaluation reward: 423.25\n",
      "episode: 4983   score: 400.0  epsilon: 1.0    steps: 704  evaluation reward: 424.2\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13921: Policy loss: -0.059697. Value loss: 0.122544. Entropy: 0.301433.\n",
      "Iteration 13922: Policy loss: -0.060994. Value loss: 0.049236. Entropy: 0.301649.\n",
      "Iteration 13923: Policy loss: -0.072930. Value loss: 0.036235. Entropy: 0.299987.\n",
      "episode: 4984   score: 270.0  epsilon: 1.0    steps: 320  evaluation reward: 424.65\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13924: Policy loss: 0.281401. Value loss: 0.111343. Entropy: 0.307726.\n",
      "Iteration 13925: Policy loss: 0.273147. Value loss: 0.047322. Entropy: 0.308824.\n",
      "Iteration 13926: Policy loss: 0.274059. Value loss: 0.038302. Entropy: 0.308229.\n",
      "episode: 4985   score: 260.0  epsilon: 1.0    steps: 120  evaluation reward: 421.8\n",
      "episode: 4986   score: 380.0  epsilon: 1.0    steps: 296  evaluation reward: 422.65\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13927: Policy loss: -0.160496. Value loss: 0.150066. Entropy: 0.299452.\n",
      "Iteration 13928: Policy loss: -0.143080. Value loss: 0.070727. Entropy: 0.299399.\n",
      "Iteration 13929: Policy loss: -0.157661. Value loss: 0.051489. Entropy: 0.300975.\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13930: Policy loss: 0.074714. Value loss: 0.106359. Entropy: 0.306711.\n",
      "Iteration 13931: Policy loss: 0.036442. Value loss: 0.072062. Entropy: 0.306786.\n",
      "Iteration 13932: Policy loss: 0.059810. Value loss: 0.043602. Entropy: 0.307102.\n",
      "episode: 4987   score: 605.0  epsilon: 1.0    steps: 368  evaluation reward: 421.9\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13933: Policy loss: 0.146066. Value loss: 0.057839. Entropy: 0.305848.\n",
      "Iteration 13934: Policy loss: 0.137058. Value loss: 0.029094. Entropy: 0.305525.\n",
      "Iteration 13935: Policy loss: 0.140569. Value loss: 0.021799. Entropy: 0.305030.\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13936: Policy loss: -0.483230. Value loss: 0.601092. Entropy: 0.305152.\n",
      "Iteration 13937: Policy loss: -0.493499. Value loss: 0.444272. Entropy: 0.303949.\n",
      "Iteration 13938: Policy loss: -0.533049. Value loss: 0.351236. Entropy: 0.305582.\n",
      "episode: 4988   score: 285.0  epsilon: 1.0    steps: 976  evaluation reward: 421.1\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13939: Policy loss: 0.066463. Value loss: 0.151869. Entropy: 0.304984.\n",
      "Iteration 13940: Policy loss: 0.063885. Value loss: 0.064569. Entropy: 0.304643.\n",
      "Iteration 13941: Policy loss: 0.057660. Value loss: 0.044958. Entropy: 0.305039.\n",
      "episode: 4989   score: 335.0  epsilon: 1.0    steps: 192  evaluation reward: 421.55\n",
      "episode: 4990   score: 410.0  epsilon: 1.0    steps: 264  evaluation reward: 420.05\n",
      "episode: 4991   score: 360.0  epsilon: 1.0    steps: 960  evaluation reward: 420.25\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13942: Policy loss: 0.036892. Value loss: 0.082978. Entropy: 0.278408.\n",
      "Iteration 13943: Policy loss: 0.031466. Value loss: 0.037560. Entropy: 0.278576.\n",
      "Iteration 13944: Policy loss: 0.026485. Value loss: 0.029926. Entropy: 0.277486.\n",
      "episode: 4992   score: 655.0  epsilon: 1.0    steps: 544  evaluation reward: 424.15\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13945: Policy loss: -0.254784. Value loss: 0.260332. Entropy: 0.298766.\n",
      "Iteration 13946: Policy loss: -0.244876. Value loss: 0.078887. Entropy: 0.299544.\n",
      "Iteration 13947: Policy loss: -0.271184. Value loss: 0.051822. Entropy: 0.298294.\n",
      "episode: 4993   score: 570.0  epsilon: 1.0    steps: 648  evaluation reward: 424.15\n",
      "episode: 4994   score: 460.0  epsilon: 1.0    steps: 832  evaluation reward: 426.45\n",
      "Training network. lr: 0.000143. clip: 0.057302\n",
      "Iteration 13948: Policy loss: 0.015243. Value loss: 0.099694. Entropy: 0.296005.\n",
      "Iteration 13949: Policy loss: 0.013523. Value loss: 0.044314. Entropy: 0.296516.\n",
      "Iteration 13950: Policy loss: 0.013440. Value loss: 0.037617. Entropy: 0.296382.\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13951: Policy loss: 0.010159. Value loss: 0.098166. Entropy: 0.315122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13952: Policy loss: 0.009194. Value loss: 0.045191. Entropy: 0.317072.\n",
      "Iteration 13953: Policy loss: 0.001637. Value loss: 0.033677. Entropy: 0.315841.\n",
      "episode: 4995   score: 285.0  epsilon: 1.0    steps: 192  evaluation reward: 423.25\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13954: Policy loss: -0.471267. Value loss: 0.339699. Entropy: 0.302895.\n",
      "Iteration 13955: Policy loss: -0.499427. Value loss: 0.141714. Entropy: 0.303934.\n",
      "Iteration 13956: Policy loss: -0.516259. Value loss: 0.089423. Entropy: 0.304393.\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13957: Policy loss: 0.053315. Value loss: 0.259432. Entropy: 0.305157.\n",
      "Iteration 13958: Policy loss: 0.042784. Value loss: 0.093029. Entropy: 0.304171.\n",
      "Iteration 13959: Policy loss: 0.031710. Value loss: 0.048776. Entropy: 0.303602.\n",
      "episode: 4996   score: 260.0  epsilon: 1.0    steps: 304  evaluation reward: 422.55\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13960: Policy loss: -0.231778. Value loss: 0.163245. Entropy: 0.293274.\n",
      "Iteration 13961: Policy loss: -0.234722. Value loss: 0.069313. Entropy: 0.292926.\n",
      "Iteration 13962: Policy loss: -0.244049. Value loss: 0.046047. Entropy: 0.292972.\n",
      "episode: 4997   score: 590.0  epsilon: 1.0    steps: 656  evaluation reward: 424.55\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13963: Policy loss: 0.149492. Value loss: 0.195025. Entropy: 0.297024.\n",
      "Iteration 13964: Policy loss: 0.148842. Value loss: 0.061784. Entropy: 0.297028.\n",
      "Iteration 13965: Policy loss: 0.128634. Value loss: 0.045242. Entropy: 0.296273.\n",
      "episode: 4998   score: 390.0  epsilon: 1.0    steps: 112  evaluation reward: 425.3\n",
      "episode: 4999   score: 370.0  epsilon: 1.0    steps: 1000  evaluation reward: 424.4\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13966: Policy loss: 0.076189. Value loss: 0.141015. Entropy: 0.297836.\n",
      "Iteration 13967: Policy loss: 0.070590. Value loss: 0.064232. Entropy: 0.296340.\n",
      "Iteration 13968: Policy loss: 0.067528. Value loss: 0.046215. Entropy: 0.298979.\n",
      "episode: 5000   score: 330.0  epsilon: 1.0    steps: 776  evaluation reward: 425.05\n",
      "now time :  2019-09-06 04:42:02.160529\n",
      "episode: 5001   score: 670.0  epsilon: 1.0    steps: 1008  evaluation reward: 429.35\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13969: Policy loss: -0.124363. Value loss: 0.139701. Entropy: 0.287174.\n",
      "Iteration 13970: Policy loss: -0.127226. Value loss: 0.075764. Entropy: 0.288005.\n",
      "Iteration 13971: Policy loss: -0.128868. Value loss: 0.052213. Entropy: 0.287891.\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13972: Policy loss: 0.114786. Value loss: 0.178295. Entropy: 0.297592.\n",
      "Iteration 13973: Policy loss: 0.113576. Value loss: 0.061237. Entropy: 0.297314.\n",
      "Iteration 13974: Policy loss: 0.106462. Value loss: 0.037625. Entropy: 0.297868.\n",
      "episode: 5002   score: 465.0  epsilon: 1.0    steps: 296  evaluation reward: 432.2\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13975: Policy loss: 0.064394. Value loss: 0.166040. Entropy: 0.301263.\n",
      "Iteration 13976: Policy loss: 0.051371. Value loss: 0.054133. Entropy: 0.300278.\n",
      "Iteration 13977: Policy loss: 0.049873. Value loss: 0.037795. Entropy: 0.299047.\n",
      "episode: 5003   score: 395.0  epsilon: 1.0    steps: 224  evaluation reward: 432.2\n",
      "episode: 5004   score: 215.0  epsilon: 1.0    steps: 808  evaluation reward: 432.8\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13978: Policy loss: 0.118403. Value loss: 0.103896. Entropy: 0.289127.\n",
      "Iteration 13979: Policy loss: 0.113798. Value loss: 0.042245. Entropy: 0.290547.\n",
      "Iteration 13980: Policy loss: 0.101100. Value loss: 0.027889. Entropy: 0.290547.\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13981: Policy loss: -0.011254. Value loss: 0.083227. Entropy: 0.301979.\n",
      "Iteration 13982: Policy loss: -0.016584. Value loss: 0.035225. Entropy: 0.302035.\n",
      "Iteration 13983: Policy loss: -0.017651. Value loss: 0.024657. Entropy: 0.302127.\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13984: Policy loss: 0.070251. Value loss: 0.107826. Entropy: 0.309081.\n",
      "Iteration 13985: Policy loss: 0.063785. Value loss: 0.040505. Entropy: 0.307521.\n",
      "Iteration 13986: Policy loss: 0.062546. Value loss: 0.026885. Entropy: 0.307235.\n",
      "episode: 5005   score: 285.0  epsilon: 1.0    steps: 112  evaluation reward: 433.25\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13987: Policy loss: -0.732674. Value loss: 0.507285. Entropy: 0.295546.\n",
      "Iteration 13988: Policy loss: -0.736667. Value loss: 0.250908. Entropy: 0.294268.\n",
      "Iteration 13989: Policy loss: -0.755256. Value loss: 0.110490. Entropy: 0.297929.\n",
      "episode: 5006   score: 450.0  epsilon: 1.0    steps: 176  evaluation reward: 433.35\n",
      "episode: 5007   score: 315.0  epsilon: 1.0    steps: 1016  evaluation reward: 434.85\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13990: Policy loss: 0.206926. Value loss: 0.145688. Entropy: 0.295164.\n",
      "Iteration 13991: Policy loss: 0.196789. Value loss: 0.061976. Entropy: 0.297428.\n",
      "Iteration 13992: Policy loss: 0.188939. Value loss: 0.040878. Entropy: 0.297198.\n",
      "episode: 5008   score: 365.0  epsilon: 1.0    steps: 440  evaluation reward: 433.1\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13993: Policy loss: 0.189900. Value loss: 0.085758. Entropy: 0.295207.\n",
      "Iteration 13994: Policy loss: 0.188201. Value loss: 0.035348. Entropy: 0.294269.\n",
      "Iteration 13995: Policy loss: 0.181352. Value loss: 0.024848. Entropy: 0.296522.\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13996: Policy loss: -0.109034. Value loss: 0.126485. Entropy: 0.307192.\n",
      "Iteration 13997: Policy loss: -0.115495. Value loss: 0.055378. Entropy: 0.306985.\n",
      "Iteration 13998: Policy loss: -0.110713. Value loss: 0.037991. Entropy: 0.307568.\n",
      "Training network. lr: 0.000143. clip: 0.057155\n",
      "Iteration 13999: Policy loss: 0.080174. Value loss: 0.350020. Entropy: 0.312493.\n",
      "Iteration 14000: Policy loss: 0.053598. Value loss: 0.145296. Entropy: 0.311882.\n",
      "Iteration 14001: Policy loss: 0.055405. Value loss: 0.101723. Entropy: 0.312093.\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14002: Policy loss: 0.201466. Value loss: 0.145541. Entropy: 0.310784.\n",
      "Iteration 14003: Policy loss: 0.202543. Value loss: 0.058169. Entropy: 0.309571.\n",
      "Iteration 14004: Policy loss: 0.201448. Value loss: 0.038156. Entropy: 0.308935.\n",
      "episode: 5009   score: 650.0  epsilon: 1.0    steps: 224  evaluation reward: 436.75\n",
      "episode: 5010   score: 210.0  epsilon: 1.0    steps: 264  evaluation reward: 435.2\n",
      "episode: 5011   score: 500.0  epsilon: 1.0    steps: 720  evaluation reward: 435.5\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14005: Policy loss: -0.158596. Value loss: 0.350653. Entropy: 0.283891.\n",
      "Iteration 14006: Policy loss: -0.177288. Value loss: 0.112978. Entropy: 0.284810.\n",
      "Iteration 14007: Policy loss: -0.193466. Value loss: 0.070407. Entropy: 0.283897.\n",
      "episode: 5012   score: 675.0  epsilon: 1.0    steps: 536  evaluation reward: 435.85\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14008: Policy loss: 0.348905. Value loss: 0.175374. Entropy: 0.301081.\n",
      "Iteration 14009: Policy loss: 0.339503. Value loss: 0.074298. Entropy: 0.299628.\n",
      "Iteration 14010: Policy loss: 0.336431. Value loss: 0.049772. Entropy: 0.298931.\n",
      "episode: 5013   score: 240.0  epsilon: 1.0    steps: 752  evaluation reward: 437.25\n",
      "episode: 5014   score: 700.0  epsilon: 1.0    steps: 960  evaluation reward: 437.65\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14011: Policy loss: 0.163781. Value loss: 0.113661. Entropy: 0.299325.\n",
      "Iteration 14012: Policy loss: 0.161402. Value loss: 0.045755. Entropy: 0.300160.\n",
      "Iteration 14013: Policy loss: 0.157615. Value loss: 0.033282. Entropy: 0.299175.\n",
      "episode: 5015   score: 490.0  epsilon: 1.0    steps: 544  evaluation reward: 437.3\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14014: Policy loss: 0.081595. Value loss: 0.142466. Entropy: 0.296803.\n",
      "Iteration 14015: Policy loss: 0.091603. Value loss: 0.057495. Entropy: 0.296531.\n",
      "Iteration 14016: Policy loss: 0.081308. Value loss: 0.038163. Entropy: 0.294946.\n",
      "episode: 5016   score: 695.0  epsilon: 1.0    steps: 760  evaluation reward: 440.0\n",
      "Training network. lr: 0.000142. clip: 0.056998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14017: Policy loss: -0.149772. Value loss: 0.245146. Entropy: 0.303678.\n",
      "Iteration 14018: Policy loss: -0.147961. Value loss: 0.090253. Entropy: 0.303674.\n",
      "Iteration 14019: Policy loss: -0.155505. Value loss: 0.056157. Entropy: 0.303617.\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14020: Policy loss: 0.098790. Value loss: 0.170559. Entropy: 0.302051.\n",
      "Iteration 14021: Policy loss: 0.089167. Value loss: 0.068179. Entropy: 0.301805.\n",
      "Iteration 14022: Policy loss: 0.082335. Value loss: 0.047614. Entropy: 0.300354.\n",
      "episode: 5017   score: 210.0  epsilon: 1.0    steps: 544  evaluation reward: 435.9\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14023: Policy loss: -0.023430. Value loss: 0.115354. Entropy: 0.297356.\n",
      "Iteration 14024: Policy loss: -0.017682. Value loss: 0.039104. Entropy: 0.297426.\n",
      "Iteration 14025: Policy loss: -0.024908. Value loss: 0.027006. Entropy: 0.297044.\n",
      "episode: 5018   score: 250.0  epsilon: 1.0    steps: 608  evaluation reward: 434.5\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14026: Policy loss: 0.419974. Value loss: 0.162455. Entropy: 0.303199.\n",
      "Iteration 14027: Policy loss: 0.414830. Value loss: 0.054690. Entropy: 0.301953.\n",
      "Iteration 14028: Policy loss: 0.413236. Value loss: 0.038385. Entropy: 0.301534.\n",
      "episode: 5019   score: 210.0  epsilon: 1.0    steps: 944  evaluation reward: 433.45\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14029: Policy loss: -0.020012. Value loss: 0.132333. Entropy: 0.304009.\n",
      "Iteration 14030: Policy loss: -0.024924. Value loss: 0.052174. Entropy: 0.304214.\n",
      "Iteration 14031: Policy loss: -0.035885. Value loss: 0.037958. Entropy: 0.304148.\n",
      "episode: 5020   score: 440.0  epsilon: 1.0    steps: 264  evaluation reward: 431.8\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14032: Policy loss: 0.052837. Value loss: 0.086692. Entropy: 0.293210.\n",
      "Iteration 14033: Policy loss: 0.043713. Value loss: 0.035548. Entropy: 0.291905.\n",
      "Iteration 14034: Policy loss: 0.050348. Value loss: 0.025008. Entropy: 0.291981.\n",
      "episode: 5021   score: 320.0  epsilon: 1.0    steps: 608  evaluation reward: 432.05\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14035: Policy loss: -0.109052. Value loss: 0.098661. Entropy: 0.291734.\n",
      "Iteration 14036: Policy loss: -0.120777. Value loss: 0.042610. Entropy: 0.292659.\n",
      "Iteration 14037: Policy loss: -0.120699. Value loss: 0.030171. Entropy: 0.292000.\n",
      "episode: 5022   score: 210.0  epsilon: 1.0    steps: 664  evaluation reward: 432.9\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14038: Policy loss: 0.192443. Value loss: 0.332921. Entropy: 0.296032.\n",
      "Iteration 14039: Policy loss: 0.202059. Value loss: 0.128981. Entropy: 0.295602.\n",
      "Iteration 14040: Policy loss: 0.174859. Value loss: 0.074412. Entropy: 0.293631.\n",
      "episode: 5023   score: 625.0  epsilon: 1.0    steps: 200  evaluation reward: 432.95\n",
      "episode: 5024   score: 370.0  epsilon: 1.0    steps: 216  evaluation reward: 433.0\n",
      "episode: 5025   score: 375.0  epsilon: 1.0    steps: 624  evaluation reward: 433.1\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14041: Policy loss: 0.234410. Value loss: 0.086752. Entropy: 0.273045.\n",
      "Iteration 14042: Policy loss: 0.224047. Value loss: 0.046025. Entropy: 0.274616.\n",
      "Iteration 14043: Policy loss: 0.221970. Value loss: 0.037551. Entropy: 0.270639.\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14044: Policy loss: 0.077967. Value loss: 0.086858. Entropy: 0.311062.\n",
      "Iteration 14045: Policy loss: 0.085531. Value loss: 0.048578. Entropy: 0.312555.\n",
      "Iteration 14046: Policy loss: 0.084641. Value loss: 0.033834. Entropy: 0.311594.\n",
      "episode: 5026   score: 190.0  epsilon: 1.0    steps: 8  evaluation reward: 430.6\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14047: Policy loss: 0.008070. Value loss: 0.117727. Entropy: 0.302041.\n",
      "Iteration 14048: Policy loss: 0.011305. Value loss: 0.047607. Entropy: 0.302664.\n",
      "Iteration 14049: Policy loss: 0.004409. Value loss: 0.033227. Entropy: 0.302941.\n",
      "Training network. lr: 0.000142. clip: 0.056998\n",
      "Iteration 14050: Policy loss: 0.359863. Value loss: 0.172797. Entropy: 0.306075.\n",
      "Iteration 14051: Policy loss: 0.346199. Value loss: 0.059402. Entropy: 0.305490.\n",
      "Iteration 14052: Policy loss: 0.335445. Value loss: 0.036895. Entropy: 0.305197.\n",
      "episode: 5027   score: 455.0  epsilon: 1.0    steps: 80  evaluation reward: 431.75\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14053: Policy loss: -0.109837. Value loss: 0.277426. Entropy: 0.292526.\n",
      "Iteration 14054: Policy loss: -0.122478. Value loss: 0.125721. Entropy: 0.294156.\n",
      "Iteration 14055: Policy loss: -0.100659. Value loss: 0.050090. Entropy: 0.295684.\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14056: Policy loss: 0.019777. Value loss: 0.119705. Entropy: 0.299976.\n",
      "Iteration 14057: Policy loss: 0.005755. Value loss: 0.059468. Entropy: 0.299644.\n",
      "Iteration 14058: Policy loss: 0.013546. Value loss: 0.037138. Entropy: 0.298893.\n",
      "episode: 5028   score: 260.0  epsilon: 1.0    steps: 288  evaluation reward: 427.55\n",
      "episode: 5029   score: 330.0  epsilon: 1.0    steps: 360  evaluation reward: 428.0\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14059: Policy loss: -0.108408. Value loss: 0.188327. Entropy: 0.296400.\n",
      "Iteration 14060: Policy loss: -0.109899. Value loss: 0.082517. Entropy: 0.295993.\n",
      "Iteration 14061: Policy loss: -0.111120. Value loss: 0.054204. Entropy: 0.296266.\n",
      "episode: 5030   score: 350.0  epsilon: 1.0    steps: 448  evaluation reward: 428.7\n",
      "episode: 5031   score: 285.0  epsilon: 1.0    steps: 592  evaluation reward: 427.35\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14062: Policy loss: 0.099020. Value loss: 0.073360. Entropy: 0.294091.\n",
      "Iteration 14063: Policy loss: 0.106872. Value loss: 0.036920. Entropy: 0.295400.\n",
      "Iteration 14064: Policy loss: 0.099285. Value loss: 0.025577. Entropy: 0.294069.\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14065: Policy loss: -0.219872. Value loss: 0.440629. Entropy: 0.306245.\n",
      "Iteration 14066: Policy loss: -0.266044. Value loss: 0.311175. Entropy: 0.307172.\n",
      "Iteration 14067: Policy loss: -0.260542. Value loss: 0.245201. Entropy: 0.307648.\n",
      "episode: 5032   score: 555.0  epsilon: 1.0    steps: 616  evaluation reward: 429.75\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14068: Policy loss: 0.351458. Value loss: 0.103641. Entropy: 0.304902.\n",
      "Iteration 14069: Policy loss: 0.349058. Value loss: 0.038999. Entropy: 0.304084.\n",
      "Iteration 14070: Policy loss: 0.341304. Value loss: 0.028379. Entropy: 0.304432.\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14071: Policy loss: -0.033164. Value loss: 0.118680. Entropy: 0.307945.\n",
      "Iteration 14072: Policy loss: -0.036151. Value loss: 0.047581. Entropy: 0.307540.\n",
      "Iteration 14073: Policy loss: -0.039358. Value loss: 0.031416. Entropy: 0.307575.\n",
      "episode: 5033   score: 490.0  epsilon: 1.0    steps: 552  evaluation reward: 429.6\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14074: Policy loss: -0.244784. Value loss: 0.373027. Entropy: 0.285737.\n",
      "Iteration 14075: Policy loss: -0.235472. Value loss: 0.253575. Entropy: 0.284161.\n",
      "Iteration 14076: Policy loss: -0.271871. Value loss: 0.225391. Entropy: 0.283161.\n",
      "episode: 5034   score: 320.0  epsilon: 1.0    steps: 712  evaluation reward: 428.85\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14077: Policy loss: -0.076529. Value loss: 0.106219. Entropy: 0.305120.\n",
      "Iteration 14078: Policy loss: -0.073788. Value loss: 0.039131. Entropy: 0.305651.\n",
      "Iteration 14079: Policy loss: -0.084825. Value loss: 0.026182. Entropy: 0.305567.\n",
      "episode: 5035   score: 215.0  epsilon: 1.0    steps: 504  evaluation reward: 425.65\n",
      "episode: 5036   score: 335.0  epsilon: 1.0    steps: 776  evaluation reward: 425.4\n",
      "episode: 5037   score: 260.0  epsilon: 1.0    steps: 784  evaluation reward: 425.0\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14080: Policy loss: 0.096567. Value loss: 0.130180. Entropy: 0.284217.\n",
      "Iteration 14081: Policy loss: 0.093347. Value loss: 0.054374. Entropy: 0.281538.\n",
      "Iteration 14082: Policy loss: 0.089376. Value loss: 0.037303. Entropy: 0.282811.\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14083: Policy loss: 0.189602. Value loss: 0.109477. Entropy: 0.310121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14084: Policy loss: 0.189425. Value loss: 0.051701. Entropy: 0.310718.\n",
      "Iteration 14085: Policy loss: 0.185040. Value loss: 0.039148. Entropy: 0.310189.\n",
      "episode: 5038   score: 775.0  epsilon: 1.0    steps: 480  evaluation reward: 428.55\n",
      "episode: 5039   score: 315.0  epsilon: 1.0    steps: 1016  evaluation reward: 429.35\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14086: Policy loss: 0.081852. Value loss: 0.109350. Entropy: 0.302227.\n",
      "Iteration 14087: Policy loss: 0.074060. Value loss: 0.053786. Entropy: 0.302421.\n",
      "Iteration 14088: Policy loss: 0.075116. Value loss: 0.038493. Entropy: 0.303679.\n",
      "episode: 5040   score: 260.0  epsilon: 1.0    steps: 48  evaluation reward: 426.45\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14089: Policy loss: -0.072136. Value loss: 0.124776. Entropy: 0.299273.\n",
      "Iteration 14090: Policy loss: -0.078425. Value loss: 0.043907. Entropy: 0.297912.\n",
      "Iteration 14091: Policy loss: -0.080907. Value loss: 0.033711. Entropy: 0.297893.\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14092: Policy loss: 0.143180. Value loss: 0.131508. Entropy: 0.311148.\n",
      "Iteration 14093: Policy loss: 0.134152. Value loss: 0.057845. Entropy: 0.312053.\n",
      "Iteration 14094: Policy loss: 0.127439. Value loss: 0.040153. Entropy: 0.311115.\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14095: Policy loss: 0.148791. Value loss: 0.102479. Entropy: 0.312718.\n",
      "Iteration 14096: Policy loss: 0.142818. Value loss: 0.034296. Entropy: 0.311394.\n",
      "Iteration 14097: Policy loss: 0.140405. Value loss: 0.021324. Entropy: 0.311562.\n",
      "episode: 5041   score: 275.0  epsilon: 1.0    steps: 520  evaluation reward: 422.85\n",
      "Training network. lr: 0.000142. clip: 0.056841\n",
      "Iteration 14098: Policy loss: 0.249210. Value loss: 0.090991. Entropy: 0.297151.\n",
      "Iteration 14099: Policy loss: 0.237127. Value loss: 0.033731. Entropy: 0.296443.\n",
      "Iteration 14100: Policy loss: 0.243234. Value loss: 0.024249. Entropy: 0.295403.\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14101: Policy loss: 0.295209. Value loss: 0.100805. Entropy: 0.312216.\n",
      "Iteration 14102: Policy loss: 0.297549. Value loss: 0.040158. Entropy: 0.311874.\n",
      "Iteration 14103: Policy loss: 0.284576. Value loss: 0.027201. Entropy: 0.312003.\n",
      "episode: 5042   score: 225.0  epsilon: 1.0    steps: 56  evaluation reward: 421.65\n",
      "episode: 5043   score: 210.0  epsilon: 1.0    steps: 96  evaluation reward: 417.1\n",
      "episode: 5044   score: 345.0  epsilon: 1.0    steps: 928  evaluation reward: 418.45\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14104: Policy loss: 0.019887. Value loss: 0.053789. Entropy: 0.297407.\n",
      "Iteration 14105: Policy loss: 0.014340. Value loss: 0.026105. Entropy: 0.296816.\n",
      "Iteration 14106: Policy loss: 0.010376. Value loss: 0.020232. Entropy: 0.295424.\n",
      "episode: 5045   score: 285.0  epsilon: 1.0    steps: 616  evaluation reward: 415.85\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14107: Policy loss: -0.640450. Value loss: 0.612729. Entropy: 0.302346.\n",
      "Iteration 14108: Policy loss: -0.624172. Value loss: 0.383418. Entropy: 0.302201.\n",
      "Iteration 14109: Policy loss: -0.685067. Value loss: 0.325688. Entropy: 0.301439.\n",
      "episode: 5046   score: 215.0  epsilon: 1.0    steps: 368  evaluation reward: 415.9\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14110: Policy loss: -0.227304. Value loss: 0.323093. Entropy: 0.309360.\n",
      "Iteration 14111: Policy loss: -0.229093. Value loss: 0.197464. Entropy: 0.308008.\n",
      "Iteration 14112: Policy loss: -0.234355. Value loss: 0.147129. Entropy: 0.308160.\n",
      "episode: 5047   score: 575.0  epsilon: 1.0    steps: 56  evaluation reward: 417.45\n",
      "episode: 5048   score: 620.0  epsilon: 1.0    steps: 560  evaluation reward: 416.7\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14113: Policy loss: 0.144085. Value loss: 0.076181. Entropy: 0.296981.\n",
      "Iteration 14114: Policy loss: 0.137275. Value loss: 0.032594. Entropy: 0.297831.\n",
      "Iteration 14115: Policy loss: 0.130176. Value loss: 0.022937. Entropy: 0.296573.\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14116: Policy loss: -0.031894. Value loss: 0.072717. Entropy: 0.314499.\n",
      "Iteration 14117: Policy loss: -0.039619. Value loss: 0.027699. Entropy: 0.314063.\n",
      "Iteration 14118: Policy loss: -0.041513. Value loss: 0.018742. Entropy: 0.314269.\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14119: Policy loss: 0.101435. Value loss: 0.082616. Entropy: 0.309348.\n",
      "Iteration 14120: Policy loss: 0.092673. Value loss: 0.037188. Entropy: 0.308742.\n",
      "Iteration 14121: Policy loss: 0.095733. Value loss: 0.027010. Entropy: 0.308856.\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14122: Policy loss: -0.114846. Value loss: 0.364545. Entropy: 0.307603.\n",
      "Iteration 14123: Policy loss: -0.091829. Value loss: 0.206819. Entropy: 0.305610.\n",
      "Iteration 14124: Policy loss: -0.114772. Value loss: 0.095645. Entropy: 0.304911.\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14125: Policy loss: -0.395635. Value loss: 0.336291. Entropy: 0.310458.\n",
      "Iteration 14126: Policy loss: -0.408577. Value loss: 0.094219. Entropy: 0.309150.\n",
      "Iteration 14127: Policy loss: -0.428713. Value loss: 0.070091. Entropy: 0.311793.\n",
      "episode: 5049   score: 535.0  epsilon: 1.0    steps: 664  evaluation reward: 415.8\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14128: Policy loss: 0.000978. Value loss: 0.116227. Entropy: 0.305144.\n",
      "Iteration 14129: Policy loss: -0.002159. Value loss: 0.043856. Entropy: 0.304557.\n",
      "Iteration 14130: Policy loss: -0.011470. Value loss: 0.030969. Entropy: 0.304453.\n",
      "episode: 5050   score: 345.0  epsilon: 1.0    steps: 584  evaluation reward: 417.15\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14131: Policy loss: 0.314785. Value loss: 0.147700. Entropy: 0.301810.\n",
      "Iteration 14132: Policy loss: 0.300208. Value loss: 0.045709. Entropy: 0.301079.\n",
      "Iteration 14133: Policy loss: 0.308520. Value loss: 0.031119. Entropy: 0.300413.\n",
      "now time :  2019-09-06 04:52:16.582701\n",
      "episode: 5051   score: 260.0  epsilon: 1.0    steps: 1016  evaluation reward: 413.75\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14134: Policy loss: 0.103512. Value loss: 0.107433. Entropy: 0.311772.\n",
      "Iteration 14135: Policy loss: 0.097729. Value loss: 0.040627. Entropy: 0.311234.\n",
      "Iteration 14136: Policy loss: 0.096545. Value loss: 0.031626. Entropy: 0.311136.\n",
      "episode: 5052   score: 405.0  epsilon: 1.0    steps: 368  evaluation reward: 415.7\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14137: Policy loss: 0.203848. Value loss: 0.068272. Entropy: 0.297752.\n",
      "Iteration 14138: Policy loss: 0.196474. Value loss: 0.029329. Entropy: 0.298986.\n",
      "Iteration 14139: Policy loss: 0.197192. Value loss: 0.020610. Entropy: 0.299467.\n",
      "episode: 5053   score: 580.0  epsilon: 1.0    steps: 680  evaluation reward: 415.85\n",
      "episode: 5054   score: 745.0  epsilon: 1.0    steps: 992  evaluation reward: 418.7\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14140: Policy loss: 0.075607. Value loss: 0.190276. Entropy: 0.306620.\n",
      "Iteration 14141: Policy loss: 0.063144. Value loss: 0.086120. Entropy: 0.308200.\n",
      "Iteration 14142: Policy loss: 0.062773. Value loss: 0.055226. Entropy: 0.307501.\n",
      "episode: 5055   score: 435.0  epsilon: 1.0    steps: 872  evaluation reward: 419.9\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14143: Policy loss: -0.116276. Value loss: 0.160663. Entropy: 0.302950.\n",
      "Iteration 14144: Policy loss: -0.118135. Value loss: 0.054256. Entropy: 0.303820.\n",
      "Iteration 14145: Policy loss: -0.117662. Value loss: 0.035002. Entropy: 0.304569.\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14146: Policy loss: -0.635750. Value loss: 0.477208. Entropy: 0.313491.\n",
      "Iteration 14147: Policy loss: -0.637114. Value loss: 0.217217. Entropy: 0.311927.\n",
      "Iteration 14148: Policy loss: -0.621932. Value loss: 0.104616. Entropy: 0.312328.\n",
      "Training network. lr: 0.000142. clip: 0.056694\n",
      "Iteration 14149: Policy loss: 0.116108. Value loss: 0.206157. Entropy: 0.306650.\n",
      "Iteration 14150: Policy loss: 0.108586. Value loss: 0.084845. Entropy: 0.307202.\n",
      "Iteration 14151: Policy loss: 0.106653. Value loss: 0.049474. Entropy: 0.305676.\n",
      "episode: 5056   score: 1110.0  epsilon: 1.0    steps: 528  evaluation reward: 425.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14152: Policy loss: 0.459755. Value loss: 0.228411. Entropy: 0.302139.\n",
      "Iteration 14153: Policy loss: 0.449273. Value loss: 0.075400. Entropy: 0.303998.\n",
      "Iteration 14154: Policy loss: 0.440298. Value loss: 0.050822. Entropy: 0.302944.\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14155: Policy loss: 0.207239. Value loss: 0.133116. Entropy: 0.305333.\n",
      "Iteration 14156: Policy loss: 0.197494. Value loss: 0.047521. Entropy: 0.305219.\n",
      "Iteration 14157: Policy loss: 0.188617. Value loss: 0.033947. Entropy: 0.304602.\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14158: Policy loss: 0.146266. Value loss: 0.079311. Entropy: 0.312337.\n",
      "Iteration 14159: Policy loss: 0.141162. Value loss: 0.036957. Entropy: 0.312185.\n",
      "Iteration 14160: Policy loss: 0.133945. Value loss: 0.026114. Entropy: 0.311612.\n",
      "episode: 5057   score: 290.0  epsilon: 1.0    steps: 696  evaluation reward: 420.45\n",
      "episode: 5058   score: 550.0  epsilon: 1.0    steps: 912  evaluation reward: 421.75\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14161: Policy loss: 0.196127. Value loss: 0.152126. Entropy: 0.309973.\n",
      "Iteration 14162: Policy loss: 0.198081. Value loss: 0.055076. Entropy: 0.309366.\n",
      "Iteration 14163: Policy loss: 0.187249. Value loss: 0.036504. Entropy: 0.309352.\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14164: Policy loss: 0.036769. Value loss: 0.136004. Entropy: 0.311333.\n",
      "Iteration 14165: Policy loss: 0.032364. Value loss: 0.075546. Entropy: 0.311525.\n",
      "Iteration 14166: Policy loss: 0.030791. Value loss: 0.056458. Entropy: 0.312369.\n",
      "episode: 5059   score: 390.0  epsilon: 1.0    steps: 80  evaluation reward: 417.75\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14167: Policy loss: 0.156093. Value loss: 0.103218. Entropy: 0.312864.\n",
      "Iteration 14168: Policy loss: 0.145820. Value loss: 0.039667. Entropy: 0.313700.\n",
      "Iteration 14169: Policy loss: 0.141726. Value loss: 0.025879. Entropy: 0.313821.\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14170: Policy loss: 0.185430. Value loss: 0.119405. Entropy: 0.308518.\n",
      "Iteration 14171: Policy loss: 0.187763. Value loss: 0.052425. Entropy: 0.309268.\n",
      "Iteration 14172: Policy loss: 0.173369. Value loss: 0.037514. Entropy: 0.308304.\n",
      "episode: 5060   score: 625.0  epsilon: 1.0    steps: 136  evaluation reward: 416.25\n",
      "episode: 5061   score: 330.0  epsilon: 1.0    steps: 288  evaluation reward: 415.85\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14173: Policy loss: 0.395663. Value loss: 0.081687. Entropy: 0.302265.\n",
      "Iteration 14174: Policy loss: 0.392544. Value loss: 0.031626. Entropy: 0.302005.\n",
      "Iteration 14175: Policy loss: 0.389155. Value loss: 0.021239. Entropy: 0.301529.\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14176: Policy loss: -0.024328. Value loss: 0.111206. Entropy: 0.313350.\n",
      "Iteration 14177: Policy loss: -0.032383. Value loss: 0.048621. Entropy: 0.313826.\n",
      "Iteration 14178: Policy loss: -0.032750. Value loss: 0.035859. Entropy: 0.313978.\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14179: Policy loss: 0.059936. Value loss: 0.060686. Entropy: 0.317959.\n",
      "Iteration 14180: Policy loss: 0.060004. Value loss: 0.027313. Entropy: 0.317469.\n",
      "Iteration 14181: Policy loss: 0.054588. Value loss: 0.020235. Entropy: 0.317760.\n",
      "episode: 5062   score: 390.0  epsilon: 1.0    steps: 376  evaluation reward: 416.35\n",
      "episode: 5063   score: 550.0  epsilon: 1.0    steps: 896  evaluation reward: 418.4\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14182: Policy loss: 0.063238. Value loss: 0.115611. Entropy: 0.303685.\n",
      "Iteration 14183: Policy loss: 0.054006. Value loss: 0.036991. Entropy: 0.300579.\n",
      "Iteration 14184: Policy loss: 0.056595. Value loss: 0.023222. Entropy: 0.299659.\n",
      "episode: 5064   score: 295.0  epsilon: 1.0    steps: 48  evaluation reward: 414.6\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14185: Policy loss: -0.083272. Value loss: 0.120913. Entropy: 0.308488.\n",
      "Iteration 14186: Policy loss: -0.092465. Value loss: 0.052800. Entropy: 0.307879.\n",
      "Iteration 14187: Policy loss: -0.094748. Value loss: 0.036072. Entropy: 0.308219.\n",
      "episode: 5065   score: 660.0  epsilon: 1.0    steps: 144  evaluation reward: 411.8\n",
      "episode: 5066   score: 330.0  epsilon: 1.0    steps: 368  evaluation reward: 412.25\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14188: Policy loss: 0.108975. Value loss: 0.115704. Entropy: 0.301408.\n",
      "Iteration 14189: Policy loss: 0.103463. Value loss: 0.053819. Entropy: 0.300449.\n",
      "Iteration 14190: Policy loss: 0.102487. Value loss: 0.043166. Entropy: 0.299790.\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14191: Policy loss: 0.065655. Value loss: 0.093567. Entropy: 0.312577.\n",
      "Iteration 14192: Policy loss: 0.060569. Value loss: 0.041122. Entropy: 0.312890.\n",
      "Iteration 14193: Policy loss: 0.053779. Value loss: 0.027840. Entropy: 0.312666.\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14194: Policy loss: 0.076501. Value loss: 0.064397. Entropy: 0.308919.\n",
      "Iteration 14195: Policy loss: 0.074313. Value loss: 0.032098. Entropy: 0.309615.\n",
      "Iteration 14196: Policy loss: 0.072715. Value loss: 0.023955. Entropy: 0.308603.\n",
      "episode: 5067   score: 345.0  epsilon: 1.0    steps: 104  evaluation reward: 411.15\n",
      "episode: 5068   score: 300.0  epsilon: 1.0    steps: 416  evaluation reward: 410.95\n",
      "episode: 5069   score: 325.0  epsilon: 1.0    steps: 664  evaluation reward: 405.25\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14197: Policy loss: -0.033453. Value loss: 0.064049. Entropy: 0.288501.\n",
      "Iteration 14198: Policy loss: -0.031831. Value loss: 0.025802. Entropy: 0.286816.\n",
      "Iteration 14199: Policy loss: -0.040739. Value loss: 0.021120. Entropy: 0.288107.\n",
      "Training network. lr: 0.000141. clip: 0.056537\n",
      "Iteration 14200: Policy loss: 0.260832. Value loss: 0.096310. Entropy: 0.312506.\n",
      "Iteration 14201: Policy loss: 0.247701. Value loss: 0.041633. Entropy: 0.312397.\n",
      "Iteration 14202: Policy loss: 0.241071. Value loss: 0.030545. Entropy: 0.311074.\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14203: Policy loss: -0.082699. Value loss: 0.099831. Entropy: 0.313705.\n",
      "Iteration 14204: Policy loss: -0.087564. Value loss: 0.035176. Entropy: 0.313844.\n",
      "Iteration 14205: Policy loss: -0.091426. Value loss: 0.023730. Entropy: 0.313333.\n",
      "episode: 5070   score: 395.0  epsilon: 1.0    steps: 360  evaluation reward: 404.2\n",
      "episode: 5071   score: 260.0  epsilon: 1.0    steps: 664  evaluation reward: 403.15\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14206: Policy loss: -0.091107. Value loss: 0.111594. Entropy: 0.291875.\n",
      "Iteration 14207: Policy loss: -0.101205. Value loss: 0.043578. Entropy: 0.289422.\n",
      "Iteration 14208: Policy loss: -0.098952. Value loss: 0.028141. Entropy: 0.289263.\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14209: Policy loss: -0.052909. Value loss: 0.116967. Entropy: 0.311180.\n",
      "Iteration 14210: Policy loss: -0.067381. Value loss: 0.037890. Entropy: 0.312078.\n",
      "Iteration 14211: Policy loss: -0.062479. Value loss: 0.029279. Entropy: 0.312011.\n",
      "episode: 5072   score: 285.0  epsilon: 1.0    steps: 240  evaluation reward: 402.6\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14212: Policy loss: -0.078807. Value loss: 0.097488. Entropy: 0.307102.\n",
      "Iteration 14213: Policy loss: -0.082408. Value loss: 0.038490. Entropy: 0.306491.\n",
      "Iteration 14214: Policy loss: -0.084730. Value loss: 0.028617. Entropy: 0.306120.\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14215: Policy loss: -0.243789. Value loss: 0.354302. Entropy: 0.314527.\n",
      "Iteration 14216: Policy loss: -0.247285. Value loss: 0.261963. Entropy: 0.315193.\n",
      "Iteration 14217: Policy loss: -0.260742. Value loss: 0.226460. Entropy: 0.315137.\n",
      "episode: 5073   score: 560.0  epsilon: 1.0    steps: 888  evaluation reward: 402.3\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14218: Policy loss: -0.133395. Value loss: 0.127763. Entropy: 0.305012.\n",
      "Iteration 14219: Policy loss: -0.143841. Value loss: 0.065392. Entropy: 0.306080.\n",
      "Iteration 14220: Policy loss: -0.140290. Value loss: 0.044937. Entropy: 0.306154.\n",
      "episode: 5074   score: 420.0  epsilon: 1.0    steps: 552  evaluation reward: 404.2\n",
      "episode: 5075   score: 495.0  epsilon: 1.0    steps: 592  evaluation reward: 406.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5076   score: 375.0  epsilon: 1.0    steps: 960  evaluation reward: 406.2\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14221: Policy loss: -0.123049. Value loss: 0.085358. Entropy: 0.298465.\n",
      "Iteration 14222: Policy loss: -0.127811. Value loss: 0.038465. Entropy: 0.302352.\n",
      "Iteration 14223: Policy loss: -0.131641. Value loss: 0.027366. Entropy: 0.301454.\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14224: Policy loss: -0.004266. Value loss: 0.110854. Entropy: 0.308584.\n",
      "Iteration 14225: Policy loss: -0.007084. Value loss: 0.035324. Entropy: 0.308568.\n",
      "Iteration 14226: Policy loss: -0.013107. Value loss: 0.023541. Entropy: 0.308551.\n",
      "episode: 5077   score: 410.0  epsilon: 1.0    steps: 416  evaluation reward: 405.2\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14227: Policy loss: 0.159574. Value loss: 0.104824. Entropy: 0.308142.\n",
      "Iteration 14228: Policy loss: 0.158594. Value loss: 0.051662. Entropy: 0.306152.\n",
      "Iteration 14229: Policy loss: 0.145923. Value loss: 0.038215. Entropy: 0.308183.\n",
      "episode: 5078   score: 320.0  epsilon: 1.0    steps: 144  evaluation reward: 406.6\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14230: Policy loss: 0.127886. Value loss: 0.085646. Entropy: 0.305438.\n",
      "Iteration 14231: Policy loss: 0.113875. Value loss: 0.031398. Entropy: 0.304618.\n",
      "Iteration 14232: Policy loss: 0.110968. Value loss: 0.024128. Entropy: 0.304414.\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14233: Policy loss: 0.044971. Value loss: 0.058432. Entropy: 0.311149.\n",
      "Iteration 14234: Policy loss: 0.040344. Value loss: 0.024488. Entropy: 0.311386.\n",
      "Iteration 14235: Policy loss: 0.037804. Value loss: 0.018753. Entropy: 0.310917.\n",
      "episode: 5079   score: 400.0  epsilon: 1.0    steps: 848  evaluation reward: 404.4\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14236: Policy loss: 0.078626. Value loss: 0.083826. Entropy: 0.305144.\n",
      "Iteration 14237: Policy loss: 0.073047. Value loss: 0.035742. Entropy: 0.305239.\n",
      "Iteration 14238: Policy loss: 0.068931. Value loss: 0.024960. Entropy: 0.304527.\n",
      "episode: 5080   score: 455.0  epsilon: 1.0    steps: 928  evaluation reward: 407.6\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14239: Policy loss: -0.246700. Value loss: 0.363606. Entropy: 0.304818.\n",
      "Iteration 14240: Policy loss: -0.261180. Value loss: 0.222350. Entropy: 0.305587.\n",
      "Iteration 14241: Policy loss: -0.268208. Value loss: 0.164369. Entropy: 0.305847.\n",
      "episode: 5081   score: 345.0  epsilon: 1.0    steps: 672  evaluation reward: 408.65\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14242: Policy loss: 0.188611. Value loss: 0.100052. Entropy: 0.308930.\n",
      "Iteration 14243: Policy loss: 0.180915. Value loss: 0.044629. Entropy: 0.309727.\n",
      "Iteration 14244: Policy loss: 0.178822. Value loss: 0.031990. Entropy: 0.308292.\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14245: Policy loss: 0.111566. Value loss: 0.119598. Entropy: 0.314737.\n",
      "Iteration 14246: Policy loss: 0.115081. Value loss: 0.046826. Entropy: 0.313707.\n",
      "Iteration 14247: Policy loss: 0.111514. Value loss: 0.029518. Entropy: 0.313446.\n",
      "episode: 5082   score: 340.0  epsilon: 1.0    steps: 184  evaluation reward: 405.45\n",
      "Training network. lr: 0.000141. clip: 0.056381\n",
      "Iteration 14248: Policy loss: 0.055467. Value loss: 0.107912. Entropy: 0.312178.\n",
      "Iteration 14249: Policy loss: 0.057393. Value loss: 0.042453. Entropy: 0.310631.\n",
      "Iteration 14250: Policy loss: 0.057241. Value loss: 0.033078. Entropy: 0.310976.\n",
      "episode: 5083   score: 395.0  epsilon: 1.0    steps: 24  evaluation reward: 405.4\n",
      "episode: 5084   score: 360.0  epsilon: 1.0    steps: 896  evaluation reward: 406.3\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14251: Policy loss: 0.057744. Value loss: 0.086921. Entropy: 0.309891.\n",
      "Iteration 14252: Policy loss: 0.050390. Value loss: 0.037755. Entropy: 0.308880.\n",
      "Iteration 14253: Policy loss: 0.048081. Value loss: 0.029575. Entropy: 0.308266.\n",
      "episode: 5085   score: 455.0  epsilon: 1.0    steps: 848  evaluation reward: 408.25\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14254: Policy loss: 0.108204. Value loss: 0.073527. Entropy: 0.310706.\n",
      "Iteration 14255: Policy loss: 0.097857. Value loss: 0.033702. Entropy: 0.310474.\n",
      "Iteration 14256: Policy loss: 0.097766. Value loss: 0.029456. Entropy: 0.309126.\n",
      "episode: 5086   score: 150.0  epsilon: 1.0    steps: 40  evaluation reward: 405.95\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14257: Policy loss: 0.037215. Value loss: 0.080241. Entropy: 0.307940.\n",
      "Iteration 14258: Policy loss: 0.025255. Value loss: 0.033962. Entropy: 0.307329.\n",
      "Iteration 14259: Policy loss: 0.031300. Value loss: 0.024775. Entropy: 0.306573.\n",
      "episode: 5087   score: 215.0  epsilon: 1.0    steps: 496  evaluation reward: 402.05\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14260: Policy loss: -0.039661. Value loss: 0.136394. Entropy: 0.307240.\n",
      "Iteration 14261: Policy loss: -0.048054. Value loss: 0.046576. Entropy: 0.308417.\n",
      "Iteration 14262: Policy loss: -0.040705. Value loss: 0.030330. Entropy: 0.306695.\n",
      "episode: 5088   score: 525.0  epsilon: 1.0    steps: 168  evaluation reward: 404.45\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14263: Policy loss: -0.218045. Value loss: 0.085948. Entropy: 0.308876.\n",
      "Iteration 14264: Policy loss: -0.216604. Value loss: 0.036819. Entropy: 0.309167.\n",
      "Iteration 14265: Policy loss: -0.221738. Value loss: 0.027152. Entropy: 0.309102.\n",
      "episode: 5089   score: 685.0  epsilon: 1.0    steps: 712  evaluation reward: 407.95\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14266: Policy loss: 0.056158. Value loss: 0.105498. Entropy: 0.310113.\n",
      "Iteration 14267: Policy loss: 0.049911. Value loss: 0.035801. Entropy: 0.308681.\n",
      "Iteration 14268: Policy loss: 0.048906. Value loss: 0.025506. Entropy: 0.308806.\n",
      "episode: 5090   score: 360.0  epsilon: 1.0    steps: 952  evaluation reward: 407.45\n",
      "episode: 5091   score: 285.0  epsilon: 1.0    steps: 1016  evaluation reward: 406.7\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14269: Policy loss: 0.156784. Value loss: 0.118065. Entropy: 0.305979.\n",
      "Iteration 14270: Policy loss: 0.145562. Value loss: 0.044452. Entropy: 0.305324.\n",
      "Iteration 14271: Policy loss: 0.139057. Value loss: 0.031197. Entropy: 0.304486.\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14272: Policy loss: 0.100523. Value loss: 0.108173. Entropy: 0.311217.\n",
      "Iteration 14273: Policy loss: 0.099634. Value loss: 0.040117. Entropy: 0.310819.\n",
      "Iteration 14274: Policy loss: 0.100939. Value loss: 0.028472. Entropy: 0.311351.\n",
      "episode: 5092   score: 285.0  epsilon: 1.0    steps: 248  evaluation reward: 403.0\n",
      "episode: 5093   score: 315.0  epsilon: 1.0    steps: 784  evaluation reward: 400.45\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14275: Policy loss: 0.066861. Value loss: 0.110931. Entropy: 0.304129.\n",
      "Iteration 14276: Policy loss: 0.066882. Value loss: 0.047170. Entropy: 0.304720.\n",
      "Iteration 14277: Policy loss: 0.057991. Value loss: 0.034372. Entropy: 0.304166.\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14278: Policy loss: 0.089426. Value loss: 0.120071. Entropy: 0.310502.\n",
      "Iteration 14279: Policy loss: 0.090951. Value loss: 0.061900. Entropy: 0.309756.\n",
      "Iteration 14280: Policy loss: 0.087138. Value loss: 0.041804. Entropy: 0.310373.\n",
      "episode: 5094   score: 210.0  epsilon: 1.0    steps: 992  evaluation reward: 397.95\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14281: Policy loss: 0.010430. Value loss: 0.077797. Entropy: 0.305890.\n",
      "Iteration 14282: Policy loss: -0.001751. Value loss: 0.041907. Entropy: 0.305269.\n",
      "Iteration 14283: Policy loss: -0.002050. Value loss: 0.033906. Entropy: 0.305365.\n",
      "episode: 5095   score: 330.0  epsilon: 1.0    steps: 488  evaluation reward: 398.4\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14284: Policy loss: -0.294374. Value loss: 0.315187. Entropy: 0.304288.\n",
      "Iteration 14285: Policy loss: -0.271026. Value loss: 0.128091. Entropy: 0.302800.\n",
      "Iteration 14286: Policy loss: -0.310306. Value loss: 0.098799. Entropy: 0.304718.\n",
      "episode: 5096   score: 210.0  epsilon: 1.0    steps: 160  evaluation reward: 397.9\n",
      "Training network. lr: 0.000141. clip: 0.056233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14287: Policy loss: 0.076737. Value loss: 0.119756. Entropy: 0.307273.\n",
      "Iteration 14288: Policy loss: 0.067054. Value loss: 0.042658. Entropy: 0.307758.\n",
      "Iteration 14289: Policy loss: 0.055291. Value loss: 0.026186. Entropy: 0.307439.\n",
      "episode: 5097   score: 440.0  epsilon: 1.0    steps: 208  evaluation reward: 396.4\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14290: Policy loss: -0.019694. Value loss: 0.147178. Entropy: 0.303520.\n",
      "Iteration 14291: Policy loss: -0.023719. Value loss: 0.064956. Entropy: 0.303768.\n",
      "Iteration 14292: Policy loss: -0.034171. Value loss: 0.044830. Entropy: 0.304486.\n",
      "episode: 5098   score: 420.0  epsilon: 1.0    steps: 520  evaluation reward: 396.7\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14293: Policy loss: 0.015646. Value loss: 0.090866. Entropy: 0.308295.\n",
      "Iteration 14294: Policy loss: 0.015644. Value loss: 0.040067. Entropy: 0.308031.\n",
      "Iteration 14295: Policy loss: 0.009522. Value loss: 0.028191. Entropy: 0.308548.\n",
      "episode: 5099   score: 180.0  epsilon: 1.0    steps: 448  evaluation reward: 394.8\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14296: Policy loss: -0.295029. Value loss: 0.250780. Entropy: 0.304642.\n",
      "Iteration 14297: Policy loss: -0.300763. Value loss: 0.070154. Entropy: 0.304364.\n",
      "Iteration 14298: Policy loss: -0.321722. Value loss: 0.046306. Entropy: 0.304750.\n",
      "episode: 5100   score: 565.0  epsilon: 1.0    steps: 840  evaluation reward: 397.15\n",
      "Training network. lr: 0.000141. clip: 0.056233\n",
      "Iteration 14299: Policy loss: 0.200757. Value loss: 0.098810. Entropy: 0.309513.\n",
      "Iteration 14300: Policy loss: 0.189535. Value loss: 0.037430. Entropy: 0.308811.\n",
      "Iteration 14301: Policy loss: 0.191287. Value loss: 0.022650. Entropy: 0.308271.\n",
      "now time :  2019-09-06 05:02:41.189430\n",
      "episode: 5101   score: 620.0  epsilon: 1.0    steps: 656  evaluation reward: 396.65\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14302: Policy loss: 0.071085. Value loss: 0.073498. Entropy: 0.307269.\n",
      "Iteration 14303: Policy loss: 0.071138. Value loss: 0.035444. Entropy: 0.307978.\n",
      "Iteration 14304: Policy loss: 0.067185. Value loss: 0.024649. Entropy: 0.307413.\n",
      "episode: 5102   score: 390.0  epsilon: 1.0    steps: 440  evaluation reward: 395.9\n",
      "episode: 5103   score: 320.0  epsilon: 1.0    steps: 816  evaluation reward: 395.15\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14305: Policy loss: -0.064967. Value loss: 0.100198. Entropy: 0.305174.\n",
      "Iteration 14306: Policy loss: -0.068258. Value loss: 0.038401. Entropy: 0.305454.\n",
      "Iteration 14307: Policy loss: -0.073903. Value loss: 0.024847. Entropy: 0.304762.\n",
      "episode: 5104   score: 420.0  epsilon: 1.0    steps: 456  evaluation reward: 397.2\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14308: Policy loss: 0.199312. Value loss: 0.148584. Entropy: 0.305495.\n",
      "Iteration 14309: Policy loss: 0.191633. Value loss: 0.061470. Entropy: 0.303729.\n",
      "Iteration 14310: Policy loss: 0.199044. Value loss: 0.043691. Entropy: 0.304047.\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14311: Policy loss: -0.040092. Value loss: 0.129466. Entropy: 0.305686.\n",
      "Iteration 14312: Policy loss: -0.036629. Value loss: 0.062914. Entropy: 0.306145.\n",
      "Iteration 14313: Policy loss: -0.044609. Value loss: 0.041803. Entropy: 0.306282.\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14314: Policy loss: 0.014110. Value loss: 0.081202. Entropy: 0.307863.\n",
      "Iteration 14315: Policy loss: 0.008308. Value loss: 0.041022. Entropy: 0.306698.\n",
      "Iteration 14316: Policy loss: 0.000143. Value loss: 0.032720. Entropy: 0.307604.\n",
      "episode: 5105   score: 270.0  epsilon: 1.0    steps: 240  evaluation reward: 397.05\n",
      "episode: 5106   score: 330.0  epsilon: 1.0    steps: 576  evaluation reward: 395.85\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14317: Policy loss: -0.150908. Value loss: 0.326299. Entropy: 0.307015.\n",
      "Iteration 14318: Policy loss: -0.184838. Value loss: 0.224416. Entropy: 0.303676.\n",
      "Iteration 14319: Policy loss: -0.184174. Value loss: 0.157287. Entropy: 0.306055.\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14320: Policy loss: 0.166208. Value loss: 0.139599. Entropy: 0.307787.\n",
      "Iteration 14321: Policy loss: 0.161933. Value loss: 0.060816. Entropy: 0.308563.\n",
      "Iteration 14322: Policy loss: 0.155289. Value loss: 0.040645. Entropy: 0.307014.\n",
      "episode: 5107   score: 270.0  epsilon: 1.0    steps: 72  evaluation reward: 395.4\n",
      "episode: 5108   score: 210.0  epsilon: 1.0    steps: 184  evaluation reward: 393.85\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14323: Policy loss: -0.152484. Value loss: 0.126158. Entropy: 0.302783.\n",
      "Iteration 14324: Policy loss: -0.158374. Value loss: 0.067084. Entropy: 0.301195.\n",
      "Iteration 14325: Policy loss: -0.157318. Value loss: 0.047202. Entropy: 0.300902.\n",
      "episode: 5109   score: 285.0  epsilon: 1.0    steps: 600  evaluation reward: 390.2\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14326: Policy loss: 0.271120. Value loss: 0.100961. Entropy: 0.312628.\n",
      "Iteration 14327: Policy loss: 0.270566. Value loss: 0.038504. Entropy: 0.311463.\n",
      "Iteration 14328: Policy loss: 0.263924. Value loss: 0.027807. Entropy: 0.309896.\n",
      "episode: 5110   score: 600.0  epsilon: 1.0    steps: 184  evaluation reward: 394.1\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14329: Policy loss: -0.095325. Value loss: 0.072455. Entropy: 0.310414.\n",
      "Iteration 14330: Policy loss: -0.099793. Value loss: 0.032182. Entropy: 0.311217.\n",
      "Iteration 14331: Policy loss: -0.105626. Value loss: 0.024128. Entropy: 0.310688.\n",
      "episode: 5111   score: 365.0  epsilon: 1.0    steps: 272  evaluation reward: 392.75\n",
      "episode: 5112   score: 555.0  epsilon: 1.0    steps: 440  evaluation reward: 391.55\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14332: Policy loss: -0.042475. Value loss: 0.101419. Entropy: 0.302168.\n",
      "Iteration 14333: Policy loss: -0.046847. Value loss: 0.046338. Entropy: 0.301568.\n",
      "Iteration 14334: Policy loss: -0.058520. Value loss: 0.035202. Entropy: 0.301921.\n",
      "episode: 5113   score: 215.0  epsilon: 1.0    steps: 320  evaluation reward: 391.3\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14335: Policy loss: 0.018173. Value loss: 0.086472. Entropy: 0.304813.\n",
      "Iteration 14336: Policy loss: 0.012088. Value loss: 0.039915. Entropy: 0.303942.\n",
      "Iteration 14337: Policy loss: 0.004695. Value loss: 0.030933. Entropy: 0.303899.\n",
      "episode: 5114   score: 210.0  epsilon: 1.0    steps: 312  evaluation reward: 386.4\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14338: Policy loss: 0.039635. Value loss: 0.063361. Entropy: 0.307546.\n",
      "Iteration 14339: Policy loss: 0.038823. Value loss: 0.029374. Entropy: 0.306436.\n",
      "Iteration 14340: Policy loss: 0.035819. Value loss: 0.022522. Entropy: 0.306386.\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14341: Policy loss: -0.096260. Value loss: 0.088961. Entropy: 0.307820.\n",
      "Iteration 14342: Policy loss: -0.100855. Value loss: 0.045211. Entropy: 0.307491.\n",
      "Iteration 14343: Policy loss: -0.100479. Value loss: 0.033054. Entropy: 0.307607.\n",
      "episode: 5115   score: 300.0  epsilon: 1.0    steps: 240  evaluation reward: 384.5\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14344: Policy loss: 0.119917. Value loss: 0.123382. Entropy: 0.295625.\n",
      "Iteration 14345: Policy loss: 0.129952. Value loss: 0.043690. Entropy: 0.296315.\n",
      "Iteration 14346: Policy loss: 0.117194. Value loss: 0.029920. Entropy: 0.292905.\n",
      "episode: 5116   score: 190.0  epsilon: 1.0    steps: 248  evaluation reward: 379.45\n",
      "episode: 5117   score: 285.0  epsilon: 1.0    steps: 472  evaluation reward: 380.2\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14347: Policy loss: 0.223277. Value loss: 0.078550. Entropy: 0.290277.\n",
      "Iteration 14348: Policy loss: 0.219839. Value loss: 0.028260. Entropy: 0.289540.\n",
      "Iteration 14349: Policy loss: 0.212342. Value loss: 0.021845. Entropy: 0.289864.\n",
      "episode: 5118   score: 510.0  epsilon: 1.0    steps: 912  evaluation reward: 382.8\n",
      "episode: 5119   score: 275.0  epsilon: 1.0    steps: 1008  evaluation reward: 383.45\n",
      "Training network. lr: 0.000140. clip: 0.056077\n",
      "Iteration 14350: Policy loss: 0.046612. Value loss: 0.113126. Entropy: 0.304733.\n",
      "Iteration 14351: Policy loss: 0.042934. Value loss: 0.058376. Entropy: 0.304554.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14352: Policy loss: 0.033554. Value loss: 0.045472. Entropy: 0.304013.\n",
      "episode: 5120   score: 330.0  epsilon: 1.0    steps: 528  evaluation reward: 382.35\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14353: Policy loss: -0.051892. Value loss: 0.078087. Entropy: 0.296208.\n",
      "Iteration 14354: Policy loss: -0.059292. Value loss: 0.039194. Entropy: 0.295878.\n",
      "Iteration 14355: Policy loss: -0.056710. Value loss: 0.030465. Entropy: 0.295647.\n",
      "episode: 5121   score: 315.0  epsilon: 1.0    steps: 216  evaluation reward: 382.3\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14356: Policy loss: -0.539072. Value loss: 0.468196. Entropy: 0.287567.\n",
      "Iteration 14357: Policy loss: -0.549631. Value loss: 0.237155. Entropy: 0.286948.\n",
      "Iteration 14358: Policy loss: -0.567907. Value loss: 0.180925. Entropy: 0.288497.\n",
      "episode: 5122   score: 215.0  epsilon: 1.0    steps: 704  evaluation reward: 382.35\n",
      "episode: 5123   score: 365.0  epsilon: 1.0    steps: 896  evaluation reward: 379.75\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14359: Policy loss: 0.008709. Value loss: 0.082657. Entropy: 0.299297.\n",
      "Iteration 14360: Policy loss: 0.011091. Value loss: 0.051445. Entropy: 0.298131.\n",
      "Iteration 14361: Policy loss: 0.007217. Value loss: 0.043475. Entropy: 0.299102.\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14362: Policy loss: -0.041683. Value loss: 0.249685. Entropy: 0.305975.\n",
      "Iteration 14363: Policy loss: -0.058576. Value loss: 0.088491. Entropy: 0.306582.\n",
      "Iteration 14364: Policy loss: -0.054088. Value loss: 0.051300. Entropy: 0.307432.\n",
      "episode: 5124   score: 380.0  epsilon: 1.0    steps: 288  evaluation reward: 379.85\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14365: Policy loss: 0.085195. Value loss: 0.059553. Entropy: 0.298759.\n",
      "Iteration 14366: Policy loss: 0.084902. Value loss: 0.027666. Entropy: 0.299619.\n",
      "Iteration 14367: Policy loss: 0.083828. Value loss: 0.018317. Entropy: 0.300617.\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14368: Policy loss: 0.013969. Value loss: 0.101616. Entropy: 0.311457.\n",
      "Iteration 14369: Policy loss: 0.004809. Value loss: 0.043745. Entropy: 0.309626.\n",
      "Iteration 14370: Policy loss: 0.005494. Value loss: 0.029627. Entropy: 0.309029.\n",
      "episode: 5125   score: 235.0  epsilon: 1.0    steps: 496  evaluation reward: 378.45\n",
      "episode: 5126   score: 500.0  epsilon: 1.0    steps: 832  evaluation reward: 381.55\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14371: Policy loss: 0.320137. Value loss: 0.146712. Entropy: 0.296154.\n",
      "Iteration 14372: Policy loss: 0.312376. Value loss: 0.045726. Entropy: 0.295687.\n",
      "Iteration 14373: Policy loss: 0.295052. Value loss: 0.032323. Entropy: 0.296437.\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14374: Policy loss: -0.234152. Value loss: 0.266074. Entropy: 0.309534.\n",
      "Iteration 14375: Policy loss: -0.237421. Value loss: 0.072222. Entropy: 0.309551.\n",
      "Iteration 14376: Policy loss: -0.247934. Value loss: 0.039884. Entropy: 0.310240.\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14377: Policy loss: -0.108455. Value loss: 0.141102. Entropy: 0.305348.\n",
      "Iteration 14378: Policy loss: -0.111983. Value loss: 0.064120. Entropy: 0.304694.\n",
      "Iteration 14379: Policy loss: -0.115717. Value loss: 0.044254. Entropy: 0.305346.\n",
      "episode: 5127   score: 365.0  epsilon: 1.0    steps: 272  evaluation reward: 380.65\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14380: Policy loss: 0.064674. Value loss: 0.104749. Entropy: 0.302459.\n",
      "Iteration 14381: Policy loss: 0.053370. Value loss: 0.049868. Entropy: 0.303127.\n",
      "Iteration 14382: Policy loss: 0.058995. Value loss: 0.035776. Entropy: 0.302096.\n",
      "episode: 5128   score: 300.0  epsilon: 1.0    steps: 80  evaluation reward: 381.05\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14383: Policy loss: 0.092932. Value loss: 0.061057. Entropy: 0.306021.\n",
      "Iteration 14384: Policy loss: 0.090808. Value loss: 0.029009. Entropy: 0.307006.\n",
      "Iteration 14385: Policy loss: 0.087532. Value loss: 0.024322. Entropy: 0.306687.\n",
      "episode: 5129   score: 285.0  epsilon: 1.0    steps: 280  evaluation reward: 380.6\n",
      "episode: 5130   score: 200.0  epsilon: 1.0    steps: 544  evaluation reward: 379.1\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14386: Policy loss: 0.027458. Value loss: 0.099046. Entropy: 0.300331.\n",
      "Iteration 14387: Policy loss: 0.022136. Value loss: 0.035732. Entropy: 0.298358.\n",
      "Iteration 14388: Policy loss: 0.019472. Value loss: 0.021791. Entropy: 0.297810.\n",
      "episode: 5131   score: 975.0  epsilon: 1.0    steps: 40  evaluation reward: 386.0\n",
      "episode: 5132   score: 525.0  epsilon: 1.0    steps: 520  evaluation reward: 385.7\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14389: Policy loss: -0.379449. Value loss: 0.279433. Entropy: 0.298077.\n",
      "Iteration 14390: Policy loss: -0.396368. Value loss: 0.081281. Entropy: 0.299408.\n",
      "Iteration 14391: Policy loss: -0.406825. Value loss: 0.042525. Entropy: 0.300110.\n",
      "episode: 5133   score: 395.0  epsilon: 1.0    steps: 496  evaluation reward: 384.75\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14392: Policy loss: 0.077290. Value loss: 0.086264. Entropy: 0.307537.\n",
      "Iteration 14393: Policy loss: 0.073748. Value loss: 0.041061. Entropy: 0.307765.\n",
      "Iteration 14394: Policy loss: 0.070957. Value loss: 0.029784. Entropy: 0.308837.\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14395: Policy loss: -0.047579. Value loss: 0.068491. Entropy: 0.313102.\n",
      "Iteration 14396: Policy loss: -0.049962. Value loss: 0.022832. Entropy: 0.311336.\n",
      "Iteration 14397: Policy loss: -0.053165. Value loss: 0.017614. Entropy: 0.312452.\n",
      "Training network. lr: 0.000140. clip: 0.055920\n",
      "Iteration 14398: Policy loss: -0.086337. Value loss: 0.366185. Entropy: 0.308896.\n",
      "Iteration 14399: Policy loss: -0.108723. Value loss: 0.188051. Entropy: 0.310362.\n",
      "Iteration 14400: Policy loss: -0.129378. Value loss: 0.138419. Entropy: 0.310164.\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14401: Policy loss: -0.089231. Value loss: 0.130448. Entropy: 0.306641.\n",
      "Iteration 14402: Policy loss: -0.096423. Value loss: 0.053380. Entropy: 0.307002.\n",
      "Iteration 14403: Policy loss: -0.103201. Value loss: 0.036037. Entropy: 0.307424.\n",
      "episode: 5134   score: 335.0  epsilon: 1.0    steps: 760  evaluation reward: 384.9\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14404: Policy loss: -0.097199. Value loss: 0.104883. Entropy: 0.303670.\n",
      "Iteration 14405: Policy loss: -0.104199. Value loss: 0.040180. Entropy: 0.302877.\n",
      "Iteration 14406: Policy loss: -0.109552. Value loss: 0.028360. Entropy: 0.303926.\n",
      "episode: 5135   score: 330.0  epsilon: 1.0    steps: 568  evaluation reward: 386.05\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14407: Policy loss: 0.169776. Value loss: 0.118120. Entropy: 0.303206.\n",
      "Iteration 14408: Policy loss: 0.165472. Value loss: 0.033110. Entropy: 0.303393.\n",
      "Iteration 14409: Policy loss: 0.167683. Value loss: 0.022234. Entropy: 0.303156.\n",
      "episode: 5136   score: 650.0  epsilon: 1.0    steps: 584  evaluation reward: 389.2\n",
      "episode: 5137   score: 420.0  epsilon: 1.0    steps: 776  evaluation reward: 390.8\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14410: Policy loss: 0.088616. Value loss: 0.122505. Entropy: 0.302128.\n",
      "Iteration 14411: Policy loss: 0.079907. Value loss: 0.047889. Entropy: 0.300785.\n",
      "Iteration 14412: Policy loss: 0.079969. Value loss: 0.031165. Entropy: 0.300260.\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14413: Policy loss: 0.247592. Value loss: 0.163852. Entropy: 0.305716.\n",
      "Iteration 14414: Policy loss: 0.231913. Value loss: 0.065989. Entropy: 0.305658.\n",
      "Iteration 14415: Policy loss: 0.228716. Value loss: 0.045093. Entropy: 0.306663.\n",
      "episode: 5138   score: 285.0  epsilon: 1.0    steps: 64  evaluation reward: 385.9\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14416: Policy loss: -0.037517. Value loss: 0.092972. Entropy: 0.304463.\n",
      "Iteration 14417: Policy loss: -0.040190. Value loss: 0.048304. Entropy: 0.302635.\n",
      "Iteration 14418: Policy loss: -0.045242. Value loss: 0.038606. Entropy: 0.303150.\n",
      "episode: 5139   score: 655.0  epsilon: 1.0    steps: 440  evaluation reward: 389.3\n",
      "episode: 5140   score: 495.0  epsilon: 1.0    steps: 544  evaluation reward: 391.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14419: Policy loss: 0.121521. Value loss: 0.082669. Entropy: 0.293068.\n",
      "Iteration 14420: Policy loss: 0.112011. Value loss: 0.035575. Entropy: 0.292698.\n",
      "Iteration 14421: Policy loss: 0.113728. Value loss: 0.027699. Entropy: 0.293116.\n",
      "episode: 5141   score: 475.0  epsilon: 1.0    steps: 232  evaluation reward: 393.65\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14422: Policy loss: -0.527589. Value loss: 0.512845. Entropy: 0.303408.\n",
      "Iteration 14423: Policy loss: -0.513948. Value loss: 0.331176. Entropy: 0.302588.\n",
      "Iteration 14424: Policy loss: -0.533282. Value loss: 0.181932. Entropy: 0.302648.\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14425: Policy loss: 0.013581. Value loss: 0.063798. Entropy: 0.306241.\n",
      "Iteration 14426: Policy loss: 0.005858. Value loss: 0.034777. Entropy: 0.306289.\n",
      "Iteration 14427: Policy loss: 0.003384. Value loss: 0.026939. Entropy: 0.306247.\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14428: Policy loss: 0.102289. Value loss: 0.080197. Entropy: 0.304466.\n",
      "Iteration 14429: Policy loss: 0.093650. Value loss: 0.034156. Entropy: 0.305065.\n",
      "Iteration 14430: Policy loss: 0.084378. Value loss: 0.023676. Entropy: 0.304899.\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14431: Policy loss: -0.106095. Value loss: 0.284895. Entropy: 0.310925.\n",
      "Iteration 14432: Policy loss: -0.126318. Value loss: 0.172696. Entropy: 0.311376.\n",
      "Iteration 14433: Policy loss: -0.130799. Value loss: 0.145579. Entropy: 0.311186.\n",
      "episode: 5142   score: 395.0  epsilon: 1.0    steps: 112  evaluation reward: 395.35\n",
      "episode: 5143   score: 550.0  epsilon: 1.0    steps: 912  evaluation reward: 398.75\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14434: Policy loss: -0.588364. Value loss: 0.307605. Entropy: 0.293971.\n",
      "Iteration 14435: Policy loss: -0.603986. Value loss: 0.103024. Entropy: 0.295520.\n",
      "Iteration 14436: Policy loss: -0.620466. Value loss: 0.058601. Entropy: 0.295079.\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14437: Policy loss: 0.030446. Value loss: 0.361634. Entropy: 0.296554.\n",
      "Iteration 14438: Policy loss: 0.024291. Value loss: 0.166757. Entropy: 0.297937.\n",
      "Iteration 14439: Policy loss: 0.013697. Value loss: 0.087953. Entropy: 0.297119.\n",
      "episode: 5144   score: 720.0  epsilon: 1.0    steps: 320  evaluation reward: 402.5\n",
      "episode: 5145   score: 640.0  epsilon: 1.0    steps: 448  evaluation reward: 406.05\n",
      "episode: 5146   score: 335.0  epsilon: 1.0    steps: 864  evaluation reward: 407.25\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14440: Policy loss: 0.305875. Value loss: 0.077301. Entropy: 0.288729.\n",
      "Iteration 14441: Policy loss: 0.295246. Value loss: 0.034345. Entropy: 0.288443.\n",
      "Iteration 14442: Policy loss: 0.294985. Value loss: 0.026079. Entropy: 0.287211.\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14443: Policy loss: 0.076028. Value loss: 0.075344. Entropy: 0.312973.\n",
      "Iteration 14444: Policy loss: 0.069227. Value loss: 0.030142. Entropy: 0.311775.\n",
      "Iteration 14445: Policy loss: 0.071386. Value loss: 0.020755. Entropy: 0.311840.\n",
      "episode: 5147   score: 395.0  epsilon: 1.0    steps: 640  evaluation reward: 405.45\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14446: Policy loss: 0.021888. Value loss: 0.175657. Entropy: 0.300033.\n",
      "Iteration 14447: Policy loss: 0.019131. Value loss: 0.083110. Entropy: 0.299303.\n",
      "Iteration 14448: Policy loss: 0.007157. Value loss: 0.066438. Entropy: 0.298386.\n",
      "episode: 5148   score: 590.0  epsilon: 1.0    steps: 912  evaluation reward: 405.15\n",
      "Training network. lr: 0.000139. clip: 0.055772\n",
      "Iteration 14449: Policy loss: 0.202832. Value loss: 0.093600. Entropy: 0.305379.\n",
      "Iteration 14450: Policy loss: 0.201820. Value loss: 0.046007. Entropy: 0.304777.\n",
      "Iteration 14451: Policy loss: 0.195325. Value loss: 0.032733. Entropy: 0.304115.\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14452: Policy loss: -0.199591. Value loss: 0.083213. Entropy: 0.306415.\n",
      "Iteration 14453: Policy loss: -0.211790. Value loss: 0.036179. Entropy: 0.305961.\n",
      "Iteration 14454: Policy loss: -0.213314. Value loss: 0.028885. Entropy: 0.307162.\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14455: Policy loss: 0.095523. Value loss: 0.151750. Entropy: 0.312016.\n",
      "Iteration 14456: Policy loss: 0.095080. Value loss: 0.052339. Entropy: 0.310855.\n",
      "Iteration 14457: Policy loss: 0.079044. Value loss: 0.029441. Entropy: 0.310326.\n",
      "episode: 5149   score: 725.0  epsilon: 1.0    steps: 256  evaluation reward: 407.05\n",
      "episode: 5150   score: 435.0  epsilon: 1.0    steps: 656  evaluation reward: 407.95\n",
      "now time :  2019-09-06 05:12:22.708320\n",
      "episode: 5151   score: 395.0  epsilon: 1.0    steps: 840  evaluation reward: 409.3\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14458: Policy loss: -0.112357. Value loss: 0.117464. Entropy: 0.270465.\n",
      "Iteration 14459: Policy loss: -0.104233. Value loss: 0.058847. Entropy: 0.270726.\n",
      "Iteration 14460: Policy loss: -0.116275. Value loss: 0.035245. Entropy: 0.271045.\n",
      "episode: 5152   score: 270.0  epsilon: 1.0    steps: 832  evaluation reward: 407.95\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14461: Policy loss: 0.404700. Value loss: 0.142167. Entropy: 0.294698.\n",
      "Iteration 14462: Policy loss: 0.379142. Value loss: 0.058572. Entropy: 0.293214.\n",
      "Iteration 14463: Policy loss: 0.382030. Value loss: 0.042478. Entropy: 0.294519.\n",
      "episode: 5153   score: 365.0  epsilon: 1.0    steps: 512  evaluation reward: 405.8\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14464: Policy loss: 0.073605. Value loss: 0.058341. Entropy: 0.294745.\n",
      "Iteration 14465: Policy loss: 0.073277. Value loss: 0.023498. Entropy: 0.295545.\n",
      "Iteration 14466: Policy loss: 0.070253. Value loss: 0.017520. Entropy: 0.297816.\n",
      "episode: 5154   score: 385.0  epsilon: 1.0    steps: 96  evaluation reward: 402.2\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14467: Policy loss: -0.143732. Value loss: 0.329882. Entropy: 0.296834.\n",
      "Iteration 14468: Policy loss: -0.144115. Value loss: 0.207458. Entropy: 0.297493.\n",
      "Iteration 14469: Policy loss: -0.157134. Value loss: 0.161769. Entropy: 0.298364.\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14470: Policy loss: -0.203164. Value loss: 0.203302. Entropy: 0.313528.\n",
      "Iteration 14471: Policy loss: -0.192130. Value loss: 0.121325. Entropy: 0.312347.\n",
      "Iteration 14472: Policy loss: -0.196231. Value loss: 0.100589. Entropy: 0.313093.\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14473: Policy loss: -0.113139. Value loss: 0.114303. Entropy: 0.314893.\n",
      "Iteration 14474: Policy loss: -0.122577. Value loss: 0.054336. Entropy: 0.314767.\n",
      "Iteration 14475: Policy loss: -0.130036. Value loss: 0.034849. Entropy: 0.313844.\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14476: Policy loss: -0.438097. Value loss: 0.392988. Entropy: 0.306785.\n",
      "Iteration 14477: Policy loss: -0.446787. Value loss: 0.142962. Entropy: 0.305307.\n",
      "Iteration 14478: Policy loss: -0.455942. Value loss: 0.080188. Entropy: 0.306378.\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14479: Policy loss: 0.227642. Value loss: 0.165599. Entropy: 0.302650.\n",
      "Iteration 14480: Policy loss: 0.223712. Value loss: 0.061703. Entropy: 0.303061.\n",
      "Iteration 14481: Policy loss: 0.205981. Value loss: 0.036552. Entropy: 0.302900.\n",
      "episode: 5155   score: 455.0  epsilon: 1.0    steps: 120  evaluation reward: 402.4\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14482: Policy loss: -0.028014. Value loss: 0.135585. Entropy: 0.290290.\n",
      "Iteration 14483: Policy loss: -0.034883. Value loss: 0.061426. Entropy: 0.289230.\n",
      "Iteration 14484: Policy loss: -0.035911. Value loss: 0.039354. Entropy: 0.288695.\n",
      "episode: 5156   score: 620.0  epsilon: 1.0    steps: 336  evaluation reward: 397.5\n",
      "episode: 5157   score: 470.0  epsilon: 1.0    steps: 680  evaluation reward: 399.3\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14485: Policy loss: -0.005106. Value loss: 0.242126. Entropy: 0.290600.\n",
      "Iteration 14486: Policy loss: -0.028628. Value loss: 0.124885. Entropy: 0.291821.\n",
      "Iteration 14487: Policy loss: -0.031398. Value loss: 0.076462. Entropy: 0.289762.\n",
      "Training network. lr: 0.000139. clip: 0.055616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14488: Policy loss: 0.256546. Value loss: 0.086816. Entropy: 0.311939.\n",
      "Iteration 14489: Policy loss: 0.252829. Value loss: 0.031478. Entropy: 0.310845.\n",
      "Iteration 14490: Policy loss: 0.255796. Value loss: 0.022843. Entropy: 0.312225.\n",
      "episode: 5158   score: 420.0  epsilon: 1.0    steps: 752  evaluation reward: 398.0\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14491: Policy loss: -0.125088. Value loss: 0.339781. Entropy: 0.303041.\n",
      "Iteration 14492: Policy loss: -0.140594. Value loss: 0.152893. Entropy: 0.303349.\n",
      "Iteration 14493: Policy loss: -0.121747. Value loss: 0.105609. Entropy: 0.304477.\n",
      "episode: 5159   score: 880.0  epsilon: 1.0    steps: 192  evaluation reward: 402.9\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14494: Policy loss: 0.267877. Value loss: 0.137623. Entropy: 0.293865.\n",
      "Iteration 14495: Policy loss: 0.254914. Value loss: 0.053802. Entropy: 0.291224.\n",
      "Iteration 14496: Policy loss: 0.261589. Value loss: 0.042071. Entropy: 0.291240.\n",
      "episode: 5160   score: 210.0  epsilon: 1.0    steps: 232  evaluation reward: 398.75\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14497: Policy loss: 0.025652. Value loss: 0.094246. Entropy: 0.295949.\n",
      "Iteration 14498: Policy loss: 0.026392. Value loss: 0.045295. Entropy: 0.295829.\n",
      "Iteration 14499: Policy loss: 0.021931. Value loss: 0.032127. Entropy: 0.294833.\n",
      "episode: 5161   score: 475.0  epsilon: 1.0    steps: 176  evaluation reward: 400.2\n",
      "Training network. lr: 0.000139. clip: 0.055616\n",
      "Iteration 14500: Policy loss: 0.109291. Value loss: 0.053704. Entropy: 0.299306.\n",
      "Iteration 14501: Policy loss: 0.096543. Value loss: 0.031920. Entropy: 0.298918.\n",
      "Iteration 14502: Policy loss: 0.091401. Value loss: 0.025880. Entropy: 0.299657.\n",
      "episode: 5162   score: 810.0  epsilon: 1.0    steps: 24  evaluation reward: 404.4\n",
      "episode: 5163   score: 600.0  epsilon: 1.0    steps: 320  evaluation reward: 404.9\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14503: Policy loss: -0.398271. Value loss: 0.338212. Entropy: 0.284902.\n",
      "Iteration 14504: Policy loss: -0.384954. Value loss: 0.105966. Entropy: 0.283397.\n",
      "Iteration 14505: Policy loss: -0.427890. Value loss: 0.083856. Entropy: 0.284570.\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14506: Policy loss: 0.367311. Value loss: 0.272906. Entropy: 0.310559.\n",
      "Iteration 14507: Policy loss: 0.361174. Value loss: 0.066996. Entropy: 0.308557.\n",
      "Iteration 14508: Policy loss: 0.339507. Value loss: 0.034657. Entropy: 0.308337.\n",
      "episode: 5164   score: 210.0  epsilon: 1.0    steps: 384  evaluation reward: 404.05\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14509: Policy loss: -0.014327. Value loss: 0.197228. Entropy: 0.298779.\n",
      "Iteration 14510: Policy loss: -0.010354. Value loss: 0.070590. Entropy: 0.299424.\n",
      "Iteration 14511: Policy loss: -0.024661. Value loss: 0.047277. Entropy: 0.299212.\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14512: Policy loss: 0.163234. Value loss: 0.128365. Entropy: 0.303144.\n",
      "Iteration 14513: Policy loss: 0.154150. Value loss: 0.056284. Entropy: 0.303150.\n",
      "Iteration 14514: Policy loss: 0.152575. Value loss: 0.044035. Entropy: 0.302298.\n",
      "episode: 5165   score: 345.0  epsilon: 1.0    steps: 112  evaluation reward: 400.9\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14515: Policy loss: -0.142068. Value loss: 0.134486. Entropy: 0.294837.\n",
      "Iteration 14516: Policy loss: -0.146877. Value loss: 0.067468. Entropy: 0.295887.\n",
      "Iteration 14517: Policy loss: -0.157717. Value loss: 0.047696. Entropy: 0.295688.\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14518: Policy loss: 0.287393. Value loss: 0.154111. Entropy: 0.306457.\n",
      "Iteration 14519: Policy loss: 0.286976. Value loss: 0.052768. Entropy: 0.306935.\n",
      "Iteration 14520: Policy loss: 0.271617. Value loss: 0.030634. Entropy: 0.307044.\n",
      "episode: 5166   score: 625.0  epsilon: 1.0    steps: 432  evaluation reward: 403.85\n",
      "episode: 5167   score: 305.0  epsilon: 1.0    steps: 832  evaluation reward: 403.45\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14521: Policy loss: -0.304894. Value loss: 0.323631. Entropy: 0.285593.\n",
      "Iteration 14522: Policy loss: -0.292197. Value loss: 0.143783. Entropy: 0.284096.\n",
      "Iteration 14523: Policy loss: -0.288061. Value loss: 0.105139. Entropy: 0.286554.\n",
      "episode: 5168   score: 370.0  epsilon: 1.0    steps: 1016  evaluation reward: 404.15\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14524: Policy loss: -0.068137. Value loss: 0.347825. Entropy: 0.305402.\n",
      "Iteration 14525: Policy loss: -0.080428. Value loss: 0.090018. Entropy: 0.305554.\n",
      "Iteration 14526: Policy loss: -0.070722. Value loss: 0.043996. Entropy: 0.304509.\n",
      "episode: 5169   score: 290.0  epsilon: 1.0    steps: 1008  evaluation reward: 403.8\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14527: Policy loss: -0.178968. Value loss: 0.156463. Entropy: 0.297355.\n",
      "Iteration 14528: Policy loss: -0.188832. Value loss: 0.065829. Entropy: 0.297378.\n",
      "Iteration 14529: Policy loss: -0.191151. Value loss: 0.047687. Entropy: 0.297690.\n",
      "episode: 5170   score: 635.0  epsilon: 1.0    steps: 32  evaluation reward: 406.2\n",
      "episode: 5171   score: 545.0  epsilon: 1.0    steps: 816  evaluation reward: 409.05\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14530: Policy loss: 0.141483. Value loss: 0.236678. Entropy: 0.274576.\n",
      "Iteration 14531: Policy loss: 0.135663. Value loss: 0.091026. Entropy: 0.273585.\n",
      "Iteration 14532: Policy loss: 0.129552. Value loss: 0.060375. Entropy: 0.272603.\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14533: Policy loss: -0.138733. Value loss: 0.236587. Entropy: 0.304843.\n",
      "Iteration 14534: Policy loss: -0.135422. Value loss: 0.093253. Entropy: 0.305842.\n",
      "Iteration 14535: Policy loss: -0.166369. Value loss: 0.070314. Entropy: 0.305538.\n",
      "episode: 5172   score: 370.0  epsilon: 1.0    steps: 584  evaluation reward: 409.9\n",
      "episode: 5173   score: 1155.0  epsilon: 1.0    steps: 960  evaluation reward: 415.85\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14536: Policy loss: 0.047787. Value loss: 0.086055. Entropy: 0.293967.\n",
      "Iteration 14537: Policy loss: 0.038321. Value loss: 0.047032. Entropy: 0.296130.\n",
      "Iteration 14538: Policy loss: 0.040153. Value loss: 0.035749. Entropy: 0.296318.\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14539: Policy loss: -0.027280. Value loss: 0.411043. Entropy: 0.304730.\n",
      "Iteration 14540: Policy loss: -0.009516. Value loss: 0.119669. Entropy: 0.303862.\n",
      "Iteration 14541: Policy loss: -0.035485. Value loss: 0.071529. Entropy: 0.304106.\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14542: Policy loss: -0.037680. Value loss: 0.240597. Entropy: 0.307703.\n",
      "Iteration 14543: Policy loss: -0.029636. Value loss: 0.077898. Entropy: 0.307235.\n",
      "Iteration 14544: Policy loss: -0.046746. Value loss: 0.053556. Entropy: 0.307945.\n",
      "episode: 5174   score: 390.0  epsilon: 1.0    steps: 120  evaluation reward: 415.55\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14545: Policy loss: -0.029563. Value loss: 0.437602. Entropy: 0.295177.\n",
      "Iteration 14546: Policy loss: -0.064632. Value loss: 0.153221. Entropy: 0.295553.\n",
      "Iteration 14547: Policy loss: -0.065627. Value loss: 0.095063. Entropy: 0.295633.\n",
      "episode: 5175   score: 495.0  epsilon: 1.0    steps: 808  evaluation reward: 415.55\n",
      "Training network. lr: 0.000139. clip: 0.055459\n",
      "Iteration 14548: Policy loss: 0.020425. Value loss: 0.184528. Entropy: 0.304998.\n",
      "Iteration 14549: Policy loss: 0.016695. Value loss: 0.075884. Entropy: 0.305841.\n",
      "Iteration 14550: Policy loss: 0.016284. Value loss: 0.049853. Entropy: 0.305346.\n",
      "episode: 5176   score: 265.0  epsilon: 1.0    steps: 752  evaluation reward: 414.45\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14551: Policy loss: 0.496917. Value loss: 0.223433. Entropy: 0.299644.\n",
      "Iteration 14552: Policy loss: 0.498773. Value loss: 0.073994. Entropy: 0.298013.\n",
      "Iteration 14553: Policy loss: 0.487423. Value loss: 0.051638. Entropy: 0.298066.\n",
      "episode: 5177   score: 345.0  epsilon: 1.0    steps: 112  evaluation reward: 413.8\n",
      "episode: 5178   score: 400.0  epsilon: 1.0    steps: 640  evaluation reward: 414.6\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14554: Policy loss: -0.134578. Value loss: 0.358647. Entropy: 0.290747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14555: Policy loss: -0.150839. Value loss: 0.214371. Entropy: 0.290939.\n",
      "Iteration 14556: Policy loss: -0.142720. Value loss: 0.129122. Entropy: 0.289934.\n",
      "episode: 5179   score: 670.0  epsilon: 1.0    steps: 128  evaluation reward: 417.3\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14557: Policy loss: -0.007999. Value loss: 0.119262. Entropy: 0.299540.\n",
      "Iteration 14558: Policy loss: -0.014632. Value loss: 0.042008. Entropy: 0.297787.\n",
      "Iteration 14559: Policy loss: -0.015283. Value loss: 0.027376. Entropy: 0.299670.\n",
      "episode: 5180   score: 190.0  epsilon: 1.0    steps: 824  evaluation reward: 414.65\n",
      "episode: 5181   score: 805.0  epsilon: 1.0    steps: 936  evaluation reward: 419.25\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14560: Policy loss: -0.036369. Value loss: 0.488955. Entropy: 0.293930.\n",
      "Iteration 14561: Policy loss: -0.046800. Value loss: 0.283490. Entropy: 0.294065.\n",
      "Iteration 14562: Policy loss: -0.056193. Value loss: 0.189510. Entropy: 0.292887.\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14563: Policy loss: 0.158682. Value loss: 0.092566. Entropy: 0.300198.\n",
      "Iteration 14564: Policy loss: 0.154204. Value loss: 0.047643. Entropy: 0.299488.\n",
      "Iteration 14565: Policy loss: 0.157814. Value loss: 0.034497. Entropy: 0.298732.\n",
      "episode: 5182   score: 360.0  epsilon: 1.0    steps: 56  evaluation reward: 419.45\n",
      "episode: 5183   score: 260.0  epsilon: 1.0    steps: 488  evaluation reward: 418.1\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14566: Policy loss: 0.172199. Value loss: 0.149432. Entropy: 0.283343.\n",
      "Iteration 14567: Policy loss: 0.167594. Value loss: 0.076506. Entropy: 0.283562.\n",
      "Iteration 14568: Policy loss: 0.162751. Value loss: 0.056155. Entropy: 0.282272.\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14569: Policy loss: 0.099417. Value loss: 0.199078. Entropy: 0.310786.\n",
      "Iteration 14570: Policy loss: 0.094396. Value loss: 0.107148. Entropy: 0.309853.\n",
      "Iteration 14571: Policy loss: 0.102496. Value loss: 0.069563. Entropy: 0.309885.\n",
      "episode: 5184   score: 515.0  epsilon: 1.0    steps: 648  evaluation reward: 419.65\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14572: Policy loss: -0.086044. Value loss: 0.264444. Entropy: 0.295070.\n",
      "Iteration 14573: Policy loss: -0.087536. Value loss: 0.106934. Entropy: 0.295154.\n",
      "Iteration 14574: Policy loss: -0.097724. Value loss: 0.076268. Entropy: 0.294521.\n",
      "episode: 5185   score: 135.0  epsilon: 1.0    steps: 256  evaluation reward: 416.45\n",
      "episode: 5186   score: 610.0  epsilon: 1.0    steps: 880  evaluation reward: 421.05\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14575: Policy loss: 0.148617. Value loss: 0.143917. Entropy: 0.291104.\n",
      "Iteration 14576: Policy loss: 0.168724. Value loss: 0.038208. Entropy: 0.290339.\n",
      "Iteration 14577: Policy loss: 0.144824. Value loss: 0.026104. Entropy: 0.289185.\n",
      "episode: 5187   score: 330.0  epsilon: 1.0    steps: 528  evaluation reward: 422.2\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14578: Policy loss: 0.182358. Value loss: 0.071892. Entropy: 0.285540.\n",
      "Iteration 14579: Policy loss: 0.176467. Value loss: 0.032412. Entropy: 0.286490.\n",
      "Iteration 14580: Policy loss: 0.174611. Value loss: 0.026826. Entropy: 0.285639.\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14581: Policy loss: 0.000753. Value loss: 0.100421. Entropy: 0.306957.\n",
      "Iteration 14582: Policy loss: -0.010473. Value loss: 0.040554. Entropy: 0.306721.\n",
      "Iteration 14583: Policy loss: -0.006957. Value loss: 0.030673. Entropy: 0.308638.\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14584: Policy loss: 0.132680. Value loss: 0.073254. Entropy: 0.309178.\n",
      "Iteration 14585: Policy loss: 0.125777. Value loss: 0.037266. Entropy: 0.307754.\n",
      "Iteration 14586: Policy loss: 0.124708. Value loss: 0.026132. Entropy: 0.307791.\n",
      "episode: 5188   score: 470.0  epsilon: 1.0    steps: 80  evaluation reward: 421.65\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14587: Policy loss: 0.146499. Value loss: 0.122892. Entropy: 0.291336.\n",
      "Iteration 14588: Policy loss: 0.131520. Value loss: 0.047471. Entropy: 0.291184.\n",
      "Iteration 14589: Policy loss: 0.131415. Value loss: 0.026776. Entropy: 0.292146.\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14590: Policy loss: -0.180822. Value loss: 0.396101. Entropy: 0.307392.\n",
      "Iteration 14591: Policy loss: -0.178461. Value loss: 0.250465. Entropy: 0.308633.\n",
      "Iteration 14592: Policy loss: -0.210007. Value loss: 0.204733. Entropy: 0.308942.\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14593: Policy loss: -0.017480. Value loss: 0.120601. Entropy: 0.309090.\n",
      "Iteration 14594: Policy loss: -0.026671. Value loss: 0.043493. Entropy: 0.309453.\n",
      "Iteration 14595: Policy loss: -0.029808. Value loss: 0.026464. Entropy: 0.308286.\n",
      "episode: 5189   score: 270.0  epsilon: 1.0    steps: 192  evaluation reward: 417.5\n",
      "episode: 5190   score: 210.0  epsilon: 1.0    steps: 232  evaluation reward: 416.0\n",
      "episode: 5191   score: 345.0  epsilon: 1.0    steps: 480  evaluation reward: 416.6\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14596: Policy loss: 0.002207. Value loss: 0.095037. Entropy: 0.267467.\n",
      "Iteration 14597: Policy loss: 0.003342. Value loss: 0.037747. Entropy: 0.268062.\n",
      "Iteration 14598: Policy loss: -0.004216. Value loss: 0.028697. Entropy: 0.267641.\n",
      "episode: 5192   score: 430.0  epsilon: 1.0    steps: 216  evaluation reward: 418.05\n",
      "episode: 5193   score: 390.0  epsilon: 1.0    steps: 344  evaluation reward: 418.8\n",
      "Training network. lr: 0.000138. clip: 0.055312\n",
      "Iteration 14599: Policy loss: 0.072879. Value loss: 0.094485. Entropy: 0.289507.\n",
      "Iteration 14600: Policy loss: 0.066850. Value loss: 0.049622. Entropy: 0.289214.\n",
      "Iteration 14601: Policy loss: 0.064609. Value loss: 0.032958. Entropy: 0.288957.\n",
      "episode: 5194   score: 425.0  epsilon: 1.0    steps: 160  evaluation reward: 420.95\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14602: Policy loss: -0.014322. Value loss: 0.085740. Entropy: 0.297722.\n",
      "Iteration 14603: Policy loss: -0.012589. Value loss: 0.043273. Entropy: 0.297355.\n",
      "Iteration 14604: Policy loss: -0.022807. Value loss: 0.036133. Entropy: 0.297093.\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14605: Policy loss: 0.081007. Value loss: 0.139422. Entropy: 0.313000.\n",
      "Iteration 14606: Policy loss: 0.071020. Value loss: 0.051413. Entropy: 0.311407.\n",
      "Iteration 14607: Policy loss: 0.079539. Value loss: 0.031450. Entropy: 0.311404.\n",
      "episode: 5195   score: 490.0  epsilon: 1.0    steps: 384  evaluation reward: 422.55\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14608: Policy loss: 0.196606. Value loss: 0.118591. Entropy: 0.294876.\n",
      "Iteration 14609: Policy loss: 0.196657. Value loss: 0.037990. Entropy: 0.295384.\n",
      "Iteration 14610: Policy loss: 0.193149. Value loss: 0.024707. Entropy: 0.295395.\n",
      "episode: 5196   score: 295.0  epsilon: 1.0    steps: 160  evaluation reward: 423.4\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14611: Policy loss: -0.427129. Value loss: 0.386929. Entropy: 0.294647.\n",
      "Iteration 14612: Policy loss: -0.453251. Value loss: 0.205651. Entropy: 0.295090.\n",
      "Iteration 14613: Policy loss: -0.449024. Value loss: 0.157953. Entropy: 0.295787.\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14614: Policy loss: -0.149255. Value loss: 0.113618. Entropy: 0.309934.\n",
      "Iteration 14615: Policy loss: -0.155652. Value loss: 0.045720. Entropy: 0.310278.\n",
      "Iteration 14616: Policy loss: -0.156985. Value loss: 0.032548. Entropy: 0.309871.\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14617: Policy loss: 0.304678. Value loss: 0.092270. Entropy: 0.310455.\n",
      "Iteration 14618: Policy loss: 0.301879. Value loss: 0.032549. Entropy: 0.309504.\n",
      "Iteration 14619: Policy loss: 0.293693. Value loss: 0.021849. Entropy: 0.309907.\n",
      "episode: 5197   score: 520.0  epsilon: 1.0    steps: 656  evaluation reward: 424.2\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14620: Policy loss: -0.174896. Value loss: 0.367379. Entropy: 0.310682.\n",
      "Iteration 14621: Policy loss: -0.187169. Value loss: 0.163355. Entropy: 0.311199.\n",
      "Iteration 14622: Policy loss: -0.207872. Value loss: 0.077987. Entropy: 0.310230.\n",
      "episode: 5198   score: 395.0  epsilon: 1.0    steps: 152  evaluation reward: 423.95\n",
      "episode: 5199   score: 270.0  epsilon: 1.0    steps: 176  evaluation reward: 424.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5200   score: 390.0  epsilon: 1.0    steps: 296  evaluation reward: 423.1\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14623: Policy loss: 0.126251. Value loss: 0.106447. Entropy: 0.296615.\n",
      "Iteration 14624: Policy loss: 0.129388. Value loss: 0.045461. Entropy: 0.295226.\n",
      "Iteration 14625: Policy loss: 0.126613. Value loss: 0.033737. Entropy: 0.295720.\n",
      "now time :  2019-09-06 05:22:48.201755\n",
      "episode: 5201   score: 415.0  epsilon: 1.0    steps: 480  evaluation reward: 421.05\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14626: Policy loss: -0.016539. Value loss: 0.092572. Entropy: 0.305789.\n",
      "Iteration 14627: Policy loss: -0.018491. Value loss: 0.049541. Entropy: 0.305958.\n",
      "Iteration 14628: Policy loss: -0.021685. Value loss: 0.036101. Entropy: 0.305992.\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14629: Policy loss: -0.298813. Value loss: 0.380087. Entropy: 0.312884.\n",
      "Iteration 14630: Policy loss: -0.313164. Value loss: 0.286805. Entropy: 0.310790.\n",
      "Iteration 14631: Policy loss: -0.328914. Value loss: 0.223585. Entropy: 0.310819.\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14632: Policy loss: 0.013853. Value loss: 0.174886. Entropy: 0.313259.\n",
      "Iteration 14633: Policy loss: 0.024431. Value loss: 0.059120. Entropy: 0.314706.\n",
      "Iteration 14634: Policy loss: 0.011102. Value loss: 0.042234. Entropy: 0.313545.\n",
      "episode: 5202   score: 325.0  epsilon: 1.0    steps: 96  evaluation reward: 420.4\n",
      "episode: 5203   score: 285.0  epsilon: 1.0    steps: 136  evaluation reward: 420.05\n",
      "episode: 5204   score: 545.0  epsilon: 1.0    steps: 936  evaluation reward: 421.3\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14635: Policy loss: 0.207601. Value loss: 0.129675. Entropy: 0.282441.\n",
      "Iteration 14636: Policy loss: 0.205531. Value loss: 0.040514. Entropy: 0.283079.\n",
      "Iteration 14637: Policy loss: 0.193587. Value loss: 0.029220. Entropy: 0.280843.\n",
      "episode: 5205   score: 210.0  epsilon: 1.0    steps: 896  evaluation reward: 420.7\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14638: Policy loss: -0.112566. Value loss: 0.331019. Entropy: 0.297439.\n",
      "Iteration 14639: Policy loss: -0.126389. Value loss: 0.244429. Entropy: 0.296953.\n",
      "Iteration 14640: Policy loss: -0.139002. Value loss: 0.207669. Entropy: 0.296062.\n",
      "episode: 5206   score: 215.0  epsilon: 1.0    steps: 192  evaluation reward: 419.55\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14641: Policy loss: 0.149905. Value loss: 0.077839. Entropy: 0.290257.\n",
      "Iteration 14642: Policy loss: 0.148842. Value loss: 0.036719. Entropy: 0.291094.\n",
      "Iteration 14643: Policy loss: 0.138510. Value loss: 0.027847. Entropy: 0.290910.\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14644: Policy loss: -0.017652. Value loss: 0.071050. Entropy: 0.315407.\n",
      "Iteration 14645: Policy loss: -0.020756. Value loss: 0.027631. Entropy: 0.315735.\n",
      "Iteration 14646: Policy loss: -0.024710. Value loss: 0.019220. Entropy: 0.315452.\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14647: Policy loss: 0.327826. Value loss: 0.090218. Entropy: 0.306704.\n",
      "Iteration 14648: Policy loss: 0.324103. Value loss: 0.040264. Entropy: 0.306310.\n",
      "Iteration 14649: Policy loss: 0.325814. Value loss: 0.029231. Entropy: 0.305992.\n",
      "Training network. lr: 0.000138. clip: 0.055155\n",
      "Iteration 14650: Policy loss: -0.001232. Value loss: 0.096460. Entropy: 0.309427.\n",
      "Iteration 14651: Policy loss: -0.004641. Value loss: 0.043200. Entropy: 0.308660.\n",
      "Iteration 14652: Policy loss: -0.014315. Value loss: 0.029439. Entropy: 0.308681.\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14653: Policy loss: 0.155722. Value loss: 0.124867. Entropy: 0.311882.\n",
      "Iteration 14654: Policy loss: 0.143935. Value loss: 0.037410. Entropy: 0.311028.\n",
      "Iteration 14655: Policy loss: 0.139292. Value loss: 0.022290. Entropy: 0.311649.\n",
      "episode: 5207   score: 210.0  epsilon: 1.0    steps: 568  evaluation reward: 418.95\n",
      "episode: 5208   score: 365.0  epsilon: 1.0    steps: 816  evaluation reward: 420.5\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14656: Policy loss: 0.021844. Value loss: 0.169925. Entropy: 0.304775.\n",
      "Iteration 14657: Policy loss: 0.015481. Value loss: 0.084719. Entropy: 0.304598.\n",
      "Iteration 14658: Policy loss: 0.013243. Value loss: 0.063782. Entropy: 0.305477.\n",
      "episode: 5209   score: 345.0  epsilon: 1.0    steps: 232  evaluation reward: 421.1\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14659: Policy loss: -0.056144. Value loss: 0.081867. Entropy: 0.307029.\n",
      "Iteration 14660: Policy loss: -0.065279. Value loss: 0.041073. Entropy: 0.309740.\n",
      "Iteration 14661: Policy loss: -0.060930. Value loss: 0.029723. Entropy: 0.308783.\n",
      "episode: 5210   score: 330.0  epsilon: 1.0    steps: 120  evaluation reward: 418.4\n",
      "episode: 5211   score: 355.0  epsilon: 1.0    steps: 536  evaluation reward: 418.3\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14662: Policy loss: -0.010694. Value loss: 0.108949. Entropy: 0.302975.\n",
      "Iteration 14663: Policy loss: -0.021915. Value loss: 0.043986. Entropy: 0.302916.\n",
      "Iteration 14664: Policy loss: -0.026057. Value loss: 0.030304. Entropy: 0.303640.\n",
      "episode: 5212   score: 445.0  epsilon: 1.0    steps: 440  evaluation reward: 417.2\n",
      "episode: 5213   score: 420.0  epsilon: 1.0    steps: 856  evaluation reward: 419.25\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14665: Policy loss: 0.023631. Value loss: 0.131902. Entropy: 0.303420.\n",
      "Iteration 14666: Policy loss: 0.023745. Value loss: 0.056114. Entropy: 0.303056.\n",
      "Iteration 14667: Policy loss: 0.019731. Value loss: 0.039670. Entropy: 0.303786.\n",
      "episode: 5214   score: 210.0  epsilon: 1.0    steps: 416  evaluation reward: 419.25\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14668: Policy loss: -0.076138. Value loss: 0.067202. Entropy: 0.307558.\n",
      "Iteration 14669: Policy loss: -0.080018. Value loss: 0.036519. Entropy: 0.308038.\n",
      "Iteration 14670: Policy loss: -0.081733. Value loss: 0.027960. Entropy: 0.307311.\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14671: Policy loss: -0.050096. Value loss: 0.086163. Entropy: 0.309835.\n",
      "Iteration 14672: Policy loss: -0.054516. Value loss: 0.031877. Entropy: 0.309969.\n",
      "Iteration 14673: Policy loss: -0.052895. Value loss: 0.023599. Entropy: 0.308810.\n",
      "episode: 5215   score: 615.0  epsilon: 1.0    steps: 640  evaluation reward: 422.4\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14674: Policy loss: -0.654563. Value loss: 0.428924. Entropy: 0.307026.\n",
      "Iteration 14675: Policy loss: -0.691714. Value loss: 0.251675. Entropy: 0.306230.\n",
      "Iteration 14676: Policy loss: -0.716373. Value loss: 0.147947. Entropy: 0.307146.\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14677: Policy loss: -0.013779. Value loss: 0.124042. Entropy: 0.307838.\n",
      "Iteration 14678: Policy loss: -0.013309. Value loss: 0.053136. Entropy: 0.308676.\n",
      "Iteration 14679: Policy loss: -0.019259. Value loss: 0.036718. Entropy: 0.307188.\n",
      "episode: 5216   score: 245.0  epsilon: 1.0    steps: 376  evaluation reward: 422.95\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14680: Policy loss: 0.116756. Value loss: 0.152594. Entropy: 0.302895.\n",
      "Iteration 14681: Policy loss: 0.114317. Value loss: 0.056842. Entropy: 0.302657.\n",
      "Iteration 14682: Policy loss: 0.107153. Value loss: 0.038189. Entropy: 0.302000.\n",
      "episode: 5217   score: 485.0  epsilon: 1.0    steps: 320  evaluation reward: 424.95\n",
      "episode: 5218   score: 205.0  epsilon: 1.0    steps: 456  evaluation reward: 421.9\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14683: Policy loss: -0.074446. Value loss: 0.285410. Entropy: 0.297715.\n",
      "Iteration 14684: Policy loss: -0.083136. Value loss: 0.161812. Entropy: 0.297494.\n",
      "Iteration 14685: Policy loss: -0.097375. Value loss: 0.122494. Entropy: 0.297360.\n",
      "episode: 5219   score: 795.0  epsilon: 1.0    steps: 720  evaluation reward: 427.1\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14686: Policy loss: 0.228876. Value loss: 0.154001. Entropy: 0.304562.\n",
      "Iteration 14687: Policy loss: 0.227927. Value loss: 0.068437. Entropy: 0.303180.\n",
      "Iteration 14688: Policy loss: 0.223439. Value loss: 0.042452. Entropy: 0.302806.\n",
      "episode: 5220   score: 305.0  epsilon: 1.0    steps: 144  evaluation reward: 426.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5221   score: 285.0  epsilon: 1.0    steps: 472  evaluation reward: 426.55\n",
      "episode: 5222   score: 530.0  epsilon: 1.0    steps: 656  evaluation reward: 429.7\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14689: Policy loss: 0.212393. Value loss: 0.136196. Entropy: 0.292539.\n",
      "Iteration 14690: Policy loss: 0.194307. Value loss: 0.046728. Entropy: 0.290298.\n",
      "Iteration 14691: Policy loss: 0.189601. Value loss: 0.035216. Entropy: 0.290386.\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14692: Policy loss: 0.040353. Value loss: 0.144197. Entropy: 0.307658.\n",
      "Iteration 14693: Policy loss: 0.034081. Value loss: 0.069106. Entropy: 0.306191.\n",
      "Iteration 14694: Policy loss: 0.021967. Value loss: 0.051059. Entropy: 0.306683.\n",
      "episode: 5223   score: 330.0  epsilon: 1.0    steps: 800  evaluation reward: 429.35\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14695: Policy loss: 0.125728. Value loss: 0.128840. Entropy: 0.304972.\n",
      "Iteration 14696: Policy loss: 0.128676. Value loss: 0.054655. Entropy: 0.302883.\n",
      "Iteration 14697: Policy loss: 0.119227. Value loss: 0.042179. Entropy: 0.303476.\n",
      "Training network. lr: 0.000137. clip: 0.054998\n",
      "Iteration 14698: Policy loss: -0.171549. Value loss: 0.277803. Entropy: 0.303560.\n",
      "Iteration 14699: Policy loss: -0.194220. Value loss: 0.161312. Entropy: 0.303804.\n",
      "Iteration 14700: Policy loss: -0.187250. Value loss: 0.115009. Entropy: 0.303126.\n",
      "episode: 5224   score: 495.0  epsilon: 1.0    steps: 928  evaluation reward: 430.5\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14701: Policy loss: 0.005197. Value loss: 0.099819. Entropy: 0.303879.\n",
      "Iteration 14702: Policy loss: -0.004876. Value loss: 0.038825. Entropy: 0.303099.\n",
      "Iteration 14703: Policy loss: -0.000219. Value loss: 0.026831. Entropy: 0.303505.\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14704: Policy loss: 0.029559. Value loss: 0.313223. Entropy: 0.311307.\n",
      "Iteration 14705: Policy loss: 0.024366. Value loss: 0.115286. Entropy: 0.310978.\n",
      "Iteration 14706: Policy loss: 0.012530. Value loss: 0.056498. Entropy: 0.310582.\n",
      "episode: 5225   score: 315.0  epsilon: 1.0    steps: 576  evaluation reward: 431.3\n",
      "episode: 5226   score: 440.0  epsilon: 1.0    steps: 776  evaluation reward: 430.7\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14707: Policy loss: 0.302134. Value loss: 0.126143. Entropy: 0.301080.\n",
      "Iteration 14708: Policy loss: 0.299975. Value loss: 0.045225. Entropy: 0.300893.\n",
      "Iteration 14709: Policy loss: 0.290901. Value loss: 0.033720. Entropy: 0.300655.\n",
      "episode: 5227   score: 440.0  epsilon: 1.0    steps: 104  evaluation reward: 431.45\n",
      "episode: 5228   score: 245.0  epsilon: 1.0    steps: 496  evaluation reward: 430.9\n",
      "episode: 5229   score: 255.0  epsilon: 1.0    steps: 616  evaluation reward: 430.6\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14710: Policy loss: -0.268971. Value loss: 0.283471. Entropy: 0.300786.\n",
      "Iteration 14711: Policy loss: -0.269190. Value loss: 0.163247. Entropy: 0.300446.\n",
      "Iteration 14712: Policy loss: -0.277089. Value loss: 0.103568. Entropy: 0.300460.\n",
      "episode: 5230   score: 460.0  epsilon: 1.0    steps: 104  evaluation reward: 433.2\n",
      "episode: 5231   score: 210.0  epsilon: 1.0    steps: 424  evaluation reward: 425.55\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14713: Policy loss: 0.076499. Value loss: 0.161534. Entropy: 0.308793.\n",
      "Iteration 14714: Policy loss: 0.083839. Value loss: 0.053640. Entropy: 0.307979.\n",
      "Iteration 14715: Policy loss: 0.074059. Value loss: 0.039823. Entropy: 0.307526.\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14716: Policy loss: 0.013183. Value loss: 0.123796. Entropy: 0.313886.\n",
      "Iteration 14717: Policy loss: 0.004661. Value loss: 0.069602. Entropy: 0.315042.\n",
      "Iteration 14718: Policy loss: -0.001425. Value loss: 0.054740. Entropy: 0.314420.\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14719: Policy loss: 0.058314. Value loss: 0.105947. Entropy: 0.305905.\n",
      "Iteration 14720: Policy loss: 0.056355. Value loss: 0.041979. Entropy: 0.306192.\n",
      "Iteration 14721: Policy loss: 0.054259. Value loss: 0.029107. Entropy: 0.305679.\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14722: Policy loss: -0.264093. Value loss: 0.320827. Entropy: 0.304578.\n",
      "Iteration 14723: Policy loss: -0.261052. Value loss: 0.166406. Entropy: 0.305063.\n",
      "Iteration 14724: Policy loss: -0.257378. Value loss: 0.097605. Entropy: 0.304762.\n",
      "episode: 5232   score: 180.0  epsilon: 1.0    steps: 120  evaluation reward: 422.1\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14725: Policy loss: 0.173828. Value loss: 0.287199. Entropy: 0.306515.\n",
      "Iteration 14726: Policy loss: 0.163356. Value loss: 0.099790. Entropy: 0.305855.\n",
      "Iteration 14727: Policy loss: 0.156006. Value loss: 0.061457. Entropy: 0.306640.\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14728: Policy loss: 0.078801. Value loss: 0.103927. Entropy: 0.313650.\n",
      "Iteration 14729: Policy loss: 0.069658. Value loss: 0.042431. Entropy: 0.312344.\n",
      "Iteration 14730: Policy loss: 0.065625. Value loss: 0.030052. Entropy: 0.311958.\n",
      "episode: 5233   score: 575.0  epsilon: 1.0    steps: 160  evaluation reward: 423.9\n",
      "episode: 5234   score: 390.0  epsilon: 1.0    steps: 728  evaluation reward: 424.45\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14731: Policy loss: -0.130652. Value loss: 0.131332. Entropy: 0.302000.\n",
      "Iteration 14732: Policy loss: -0.133067. Value loss: 0.060551. Entropy: 0.302949.\n",
      "Iteration 14733: Policy loss: -0.134885. Value loss: 0.041713. Entropy: 0.302179.\n",
      "episode: 5235   score: 335.0  epsilon: 1.0    steps: 344  evaluation reward: 424.5\n",
      "episode: 5236   score: 315.0  epsilon: 1.0    steps: 792  evaluation reward: 421.15\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14734: Policy loss: 0.071159. Value loss: 0.074373. Entropy: 0.307132.\n",
      "Iteration 14735: Policy loss: 0.061265. Value loss: 0.030651. Entropy: 0.306729.\n",
      "Iteration 14736: Policy loss: 0.060955. Value loss: 0.023046. Entropy: 0.306195.\n",
      "episode: 5237   score: 500.0  epsilon: 1.0    steps: 656  evaluation reward: 421.95\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14737: Policy loss: -0.064002. Value loss: 0.077830. Entropy: 0.307832.\n",
      "Iteration 14738: Policy loss: -0.063043. Value loss: 0.044538. Entropy: 0.308684.\n",
      "Iteration 14739: Policy loss: -0.061607. Value loss: 0.034230. Entropy: 0.307322.\n",
      "episode: 5238   score: 420.0  epsilon: 1.0    steps: 696  evaluation reward: 423.3\n",
      "episode: 5239   score: 345.0  epsilon: 1.0    steps: 880  evaluation reward: 420.2\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14740: Policy loss: -0.184017. Value loss: 0.314406. Entropy: 0.304866.\n",
      "Iteration 14741: Policy loss: -0.196235. Value loss: 0.176438. Entropy: 0.304393.\n",
      "Iteration 14742: Policy loss: -0.167268. Value loss: 0.114236. Entropy: 0.303276.\n",
      "episode: 5240   score: 215.0  epsilon: 1.0    steps: 496  evaluation reward: 417.4\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14743: Policy loss: -0.171403. Value loss: 0.352670. Entropy: 0.307246.\n",
      "Iteration 14744: Policy loss: -0.186061. Value loss: 0.102487. Entropy: 0.306738.\n",
      "Iteration 14745: Policy loss: -0.196953. Value loss: 0.064169. Entropy: 0.305985.\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14746: Policy loss: -0.083165. Value loss: 0.106505. Entropy: 0.310858.\n",
      "Iteration 14747: Policy loss: -0.084689. Value loss: 0.042349. Entropy: 0.311268.\n",
      "Iteration 14748: Policy loss: -0.087237. Value loss: 0.027018. Entropy: 0.310780.\n",
      "Training network. lr: 0.000137. clip: 0.054851\n",
      "Iteration 14749: Policy loss: -0.354419. Value loss: 0.278136. Entropy: 0.311021.\n",
      "Iteration 14750: Policy loss: -0.344814. Value loss: 0.117576. Entropy: 0.311588.\n",
      "Iteration 14751: Policy loss: -0.350125. Value loss: 0.073203. Entropy: 0.312154.\n",
      "episode: 5241   score: 340.0  epsilon: 1.0    steps: 616  evaluation reward: 416.05\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14752: Policy loss: -0.086252. Value loss: 0.164538. Entropy: 0.304573.\n",
      "Iteration 14753: Policy loss: -0.089262. Value loss: 0.045210. Entropy: 0.304007.\n",
      "Iteration 14754: Policy loss: -0.089077. Value loss: 0.030987. Entropy: 0.303890.\n",
      "episode: 5242   score: 570.0  epsilon: 1.0    steps: 144  evaluation reward: 417.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14755: Policy loss: -0.086558. Value loss: 0.203679. Entropy: 0.304779.\n",
      "Iteration 14756: Policy loss: -0.105745. Value loss: 0.085736. Entropy: 0.304042.\n",
      "Iteration 14757: Policy loss: -0.108599. Value loss: 0.063025. Entropy: 0.304196.\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14758: Policy loss: 0.121828. Value loss: 0.124117. Entropy: 0.315047.\n",
      "Iteration 14759: Policy loss: 0.111008. Value loss: 0.048384. Entropy: 0.314234.\n",
      "Iteration 14760: Policy loss: 0.110784. Value loss: 0.032211. Entropy: 0.314491.\n",
      "episode: 5243   score: 330.0  epsilon: 1.0    steps: 744  evaluation reward: 415.6\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14761: Policy loss: 0.092039. Value loss: 0.145707. Entropy: 0.308730.\n",
      "Iteration 14762: Policy loss: 0.095331. Value loss: 0.045498. Entropy: 0.307513.\n",
      "Iteration 14763: Policy loss: 0.079706. Value loss: 0.027851. Entropy: 0.307350.\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14764: Policy loss: -0.064148. Value loss: 0.189683. Entropy: 0.314350.\n",
      "Iteration 14765: Policy loss: -0.095773. Value loss: 0.115821. Entropy: 0.313665.\n",
      "Iteration 14766: Policy loss: -0.089434. Value loss: 0.083166. Entropy: 0.314848.\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14767: Policy loss: 0.037491. Value loss: 0.076118. Entropy: 0.313950.\n",
      "Iteration 14768: Policy loss: 0.040243. Value loss: 0.035234. Entropy: 0.313654.\n",
      "Iteration 14769: Policy loss: 0.037801. Value loss: 0.025487. Entropy: 0.313286.\n",
      "episode: 5244   score: 705.0  epsilon: 1.0    steps: 912  evaluation reward: 415.45\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14770: Policy loss: -0.083040. Value loss: 0.191509. Entropy: 0.308082.\n",
      "Iteration 14771: Policy loss: -0.078119. Value loss: 0.060239. Entropy: 0.307114.\n",
      "Iteration 14772: Policy loss: -0.089990. Value loss: 0.038886. Entropy: 0.307135.\n",
      "episode: 5245   score: 620.0  epsilon: 1.0    steps: 152  evaluation reward: 415.25\n",
      "episode: 5246   score: 440.0  epsilon: 1.0    steps: 176  evaluation reward: 416.3\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14773: Policy loss: -0.117652. Value loss: 0.183806. Entropy: 0.290920.\n",
      "Iteration 14774: Policy loss: -0.121982. Value loss: 0.073735. Entropy: 0.291583.\n",
      "Iteration 14775: Policy loss: -0.124267. Value loss: 0.049590. Entropy: 0.290450.\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14776: Policy loss: -0.082674. Value loss: 0.395947. Entropy: 0.310514.\n",
      "Iteration 14777: Policy loss: -0.080373. Value loss: 0.250963. Entropy: 0.309509.\n",
      "Iteration 14778: Policy loss: -0.092227. Value loss: 0.184173. Entropy: 0.308979.\n",
      "episode: 5247   score: 540.0  epsilon: 1.0    steps: 656  evaluation reward: 417.75\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14779: Policy loss: -0.046830. Value loss: 0.126960. Entropy: 0.301289.\n",
      "Iteration 14780: Policy loss: -0.049247. Value loss: 0.057875. Entropy: 0.299967.\n",
      "Iteration 14781: Policy loss: -0.053102. Value loss: 0.040524. Entropy: 0.299624.\n",
      "episode: 5248   score: 285.0  epsilon: 1.0    steps: 344  evaluation reward: 414.7\n",
      "episode: 5249   score: 950.0  epsilon: 1.0    steps: 416  evaluation reward: 416.95\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14782: Policy loss: -0.019604. Value loss: 0.188585. Entropy: 0.284854.\n",
      "Iteration 14783: Policy loss: -0.023942. Value loss: 0.123460. Entropy: 0.282977.\n",
      "Iteration 14784: Policy loss: -0.033382. Value loss: 0.102367. Entropy: 0.282674.\n",
      "episode: 5250   score: 645.0  epsilon: 1.0    steps: 272  evaluation reward: 419.05\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14785: Policy loss: 0.035591. Value loss: 0.124639. Entropy: 0.298958.\n",
      "Iteration 14786: Policy loss: 0.028391. Value loss: 0.051433. Entropy: 0.299666.\n",
      "Iteration 14787: Policy loss: 0.018321. Value loss: 0.035938. Entropy: 0.299747.\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14788: Policy loss: 0.105865. Value loss: 0.170126. Entropy: 0.309683.\n",
      "Iteration 14789: Policy loss: 0.097388. Value loss: 0.056981. Entropy: 0.308770.\n",
      "Iteration 14790: Policy loss: 0.087828. Value loss: 0.040082. Entropy: 0.307428.\n",
      "now time :  2019-09-06 05:33:01.393269\n",
      "episode: 5251   score: 455.0  epsilon: 1.0    steps: 216  evaluation reward: 419.65\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14791: Policy loss: 0.226212. Value loss: 0.158637. Entropy: 0.300399.\n",
      "Iteration 14792: Policy loss: 0.220184. Value loss: 0.046113. Entropy: 0.299740.\n",
      "Iteration 14793: Policy loss: 0.211011. Value loss: 0.031841. Entropy: 0.299164.\n",
      "episode: 5252   score: 370.0  epsilon: 1.0    steps: 864  evaluation reward: 420.65\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14794: Policy loss: 0.231667. Value loss: 0.095083. Entropy: 0.307665.\n",
      "Iteration 14795: Policy loss: 0.225018. Value loss: 0.034105. Entropy: 0.307373.\n",
      "Iteration 14796: Policy loss: 0.227529. Value loss: 0.023903. Entropy: 0.307614.\n",
      "episode: 5253   score: 585.0  epsilon: 1.0    steps: 360  evaluation reward: 422.85\n",
      "episode: 5254   score: 560.0  epsilon: 1.0    steps: 456  evaluation reward: 424.6\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14797: Policy loss: 0.229963. Value loss: 0.096246. Entropy: 0.299408.\n",
      "Iteration 14798: Policy loss: 0.220024. Value loss: 0.052819. Entropy: 0.299895.\n",
      "Iteration 14799: Policy loss: 0.220638. Value loss: 0.039536. Entropy: 0.298956.\n",
      "episode: 5255   score: 245.0  epsilon: 1.0    steps: 912  evaluation reward: 422.5\n",
      "Training network. lr: 0.000137. clip: 0.054694\n",
      "Iteration 14800: Policy loss: 0.076587. Value loss: 0.119666. Entropy: 0.306523.\n",
      "Iteration 14801: Policy loss: 0.073587. Value loss: 0.062351. Entropy: 0.306782.\n",
      "Iteration 14802: Policy loss: 0.063664. Value loss: 0.043995. Entropy: 0.306875.\n",
      "episode: 5256   score: 245.0  epsilon: 1.0    steps: 400  evaluation reward: 418.75\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14803: Policy loss: -0.099098. Value loss: 0.262087. Entropy: 0.306804.\n",
      "Iteration 14804: Policy loss: -0.094781. Value loss: 0.171482. Entropy: 0.306466.\n",
      "Iteration 14805: Policy loss: -0.086514. Value loss: 0.110957. Entropy: 0.305631.\n",
      "episode: 5257   score: 485.0  epsilon: 1.0    steps: 1008  evaluation reward: 418.9\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14806: Policy loss: 0.053534. Value loss: 0.071977. Entropy: 0.307411.\n",
      "Iteration 14807: Policy loss: 0.048892. Value loss: 0.031624. Entropy: 0.306549.\n",
      "Iteration 14808: Policy loss: 0.045979. Value loss: 0.024650. Entropy: 0.306283.\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14809: Policy loss: -0.127032. Value loss: 0.329970. Entropy: 0.308092.\n",
      "Iteration 14810: Policy loss: -0.150087. Value loss: 0.164542. Entropy: 0.305555.\n",
      "Iteration 14811: Policy loss: -0.132425. Value loss: 0.074820. Entropy: 0.306920.\n",
      "episode: 5258   score: 420.0  epsilon: 1.0    steps: 264  evaluation reward: 418.9\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14812: Policy loss: 0.079991. Value loss: 0.102329. Entropy: 0.303697.\n",
      "Iteration 14813: Policy loss: 0.074835. Value loss: 0.047113. Entropy: 0.302624.\n",
      "Iteration 14814: Policy loss: 0.074247. Value loss: 0.034316. Entropy: 0.302295.\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14815: Policy loss: 0.093258. Value loss: 0.075364. Entropy: 0.308228.\n",
      "Iteration 14816: Policy loss: 0.096713. Value loss: 0.035352. Entropy: 0.308425.\n",
      "Iteration 14817: Policy loss: 0.090074. Value loss: 0.029224. Entropy: 0.308256.\n",
      "episode: 5259   score: 390.0  epsilon: 1.0    steps: 880  evaluation reward: 414.0\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14818: Policy loss: 0.042837. Value loss: 0.107967. Entropy: 0.305441.\n",
      "Iteration 14819: Policy loss: 0.033243. Value loss: 0.041515. Entropy: 0.304773.\n",
      "Iteration 14820: Policy loss: 0.027766. Value loss: 0.029774. Entropy: 0.304525.\n",
      "episode: 5260   score: 425.0  epsilon: 1.0    steps: 376  evaluation reward: 416.15\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14821: Policy loss: 0.156968. Value loss: 0.090975. Entropy: 0.302881.\n",
      "Iteration 14822: Policy loss: 0.156614. Value loss: 0.034498. Entropy: 0.302032.\n",
      "Iteration 14823: Policy loss: 0.152195. Value loss: 0.025376. Entropy: 0.300904.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5261   score: 495.0  epsilon: 1.0    steps: 696  evaluation reward: 416.35\n",
      "episode: 5262   score: 260.0  epsilon: 1.0    steps: 896  evaluation reward: 410.85\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14824: Policy loss: -0.116322. Value loss: 0.228719. Entropy: 0.302048.\n",
      "Iteration 14825: Policy loss: -0.126480. Value loss: 0.072303. Entropy: 0.304386.\n",
      "Iteration 14826: Policy loss: -0.122901. Value loss: 0.048354. Entropy: 0.302262.\n",
      "episode: 5263   score: 315.0  epsilon: 1.0    steps: 192  evaluation reward: 408.0\n",
      "episode: 5264   score: 620.0  epsilon: 1.0    steps: 480  evaluation reward: 412.1\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14827: Policy loss: 0.053939. Value loss: 0.126396. Entropy: 0.296480.\n",
      "Iteration 14828: Policy loss: 0.049175. Value loss: 0.060063. Entropy: 0.295929.\n",
      "Iteration 14829: Policy loss: 0.050565. Value loss: 0.042477. Entropy: 0.296083.\n",
      "episode: 5265   score: 505.0  epsilon: 1.0    steps: 80  evaluation reward: 413.7\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14830: Policy loss: -0.502265. Value loss: 0.243536. Entropy: 0.298356.\n",
      "Iteration 14831: Policy loss: -0.527282. Value loss: 0.094335. Entropy: 0.300358.\n",
      "Iteration 14832: Policy loss: -0.538358. Value loss: 0.074381. Entropy: 0.298214.\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14833: Policy loss: 0.016832. Value loss: 0.073231. Entropy: 0.306831.\n",
      "Iteration 14834: Policy loss: 0.008733. Value loss: 0.032348. Entropy: 0.306960.\n",
      "Iteration 14835: Policy loss: 0.007225. Value loss: 0.026232. Entropy: 0.306245.\n",
      "episode: 5266   score: 505.0  epsilon: 1.0    steps: 200  evaluation reward: 412.5\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14836: Policy loss: 0.159758. Value loss: 0.302738. Entropy: 0.300754.\n",
      "Iteration 14837: Policy loss: 0.165157. Value loss: 0.074016. Entropy: 0.302077.\n",
      "Iteration 14838: Policy loss: 0.160943. Value loss: 0.029097. Entropy: 0.300366.\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14839: Policy loss: 0.194158. Value loss: 0.166666. Entropy: 0.310716.\n",
      "Iteration 14840: Policy loss: 0.180644. Value loss: 0.045371. Entropy: 0.310284.\n",
      "Iteration 14841: Policy loss: 0.162959. Value loss: 0.030253. Entropy: 0.308883.\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14842: Policy loss: -0.017211. Value loss: 0.392025. Entropy: 0.309208.\n",
      "Iteration 14843: Policy loss: -0.030217. Value loss: 0.181990. Entropy: 0.306280.\n",
      "Iteration 14844: Policy loss: 0.004931. Value loss: 0.078059. Entropy: 0.307253.\n",
      "episode: 5267   score: 315.0  epsilon: 1.0    steps: 416  evaluation reward: 412.6\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14845: Policy loss: -0.122265. Value loss: 0.301975. Entropy: 0.298417.\n",
      "Iteration 14846: Policy loss: -0.127992. Value loss: 0.151933. Entropy: 0.296669.\n",
      "Iteration 14847: Policy loss: -0.140774. Value loss: 0.075805. Entropy: 0.296566.\n",
      "episode: 5268   score: 530.0  epsilon: 1.0    steps: 24  evaluation reward: 414.2\n",
      "episode: 5269   score: 790.0  epsilon: 1.0    steps: 688  evaluation reward: 419.2\n",
      "episode: 5270   score: 260.0  epsilon: 1.0    steps: 816  evaluation reward: 415.45\n",
      "Training network. lr: 0.000136. clip: 0.054537\n",
      "Iteration 14848: Policy loss: 0.148424. Value loss: 0.154863. Entropy: 0.282987.\n",
      "Iteration 14849: Policy loss: 0.137856. Value loss: 0.063265. Entropy: 0.282975.\n",
      "Iteration 14850: Policy loss: 0.142805. Value loss: 0.047598. Entropy: 0.282074.\n",
      "episode: 5271   score: 270.0  epsilon: 1.0    steps: 80  evaluation reward: 412.7\n",
      "episode: 5272   score: 390.0  epsilon: 1.0    steps: 456  evaluation reward: 412.9\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14851: Policy loss: -0.098759. Value loss: 0.111934. Entropy: 0.295580.\n",
      "Iteration 14852: Policy loss: -0.101624. Value loss: 0.053230. Entropy: 0.295665.\n",
      "Iteration 14853: Policy loss: -0.104805. Value loss: 0.045711. Entropy: 0.294128.\n",
      "episode: 5273   score: 290.0  epsilon: 1.0    steps: 376  evaluation reward: 404.25\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14854: Policy loss: -0.147728. Value loss: 0.094286. Entropy: 0.301709.\n",
      "Iteration 14855: Policy loss: -0.158550. Value loss: 0.045667. Entropy: 0.302583.\n",
      "Iteration 14856: Policy loss: -0.159860. Value loss: 0.034642. Entropy: 0.301677.\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14857: Policy loss: -0.563558. Value loss: 0.318784. Entropy: 0.307724.\n",
      "Iteration 14858: Policy loss: -0.563035. Value loss: 0.151288. Entropy: 0.308751.\n",
      "Iteration 14859: Policy loss: -0.566337. Value loss: 0.112382. Entropy: 0.308911.\n",
      "episode: 5274   score: 700.0  epsilon: 1.0    steps: 400  evaluation reward: 407.35\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14860: Policy loss: -0.082148. Value loss: 0.095879. Entropy: 0.302403.\n",
      "Iteration 14861: Policy loss: -0.083864. Value loss: 0.046433. Entropy: 0.302714.\n",
      "Iteration 14862: Policy loss: -0.074783. Value loss: 0.028990. Entropy: 0.302599.\n",
      "episode: 5275   score: 210.0  epsilon: 1.0    steps: 16  evaluation reward: 404.5\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14863: Policy loss: 0.179153. Value loss: 0.387574. Entropy: 0.309954.\n",
      "Iteration 14864: Policy loss: 0.205983. Value loss: 0.166602. Entropy: 0.308509.\n",
      "Iteration 14865: Policy loss: 0.167969. Value loss: 0.082634. Entropy: 0.309410.\n",
      "episode: 5276   score: 180.0  epsilon: 1.0    steps: 472  evaluation reward: 403.65\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14866: Policy loss: -0.029708. Value loss: 0.157392. Entropy: 0.299048.\n",
      "Iteration 14867: Policy loss: -0.043790. Value loss: 0.071175. Entropy: 0.298390.\n",
      "Iteration 14868: Policy loss: -0.047334. Value loss: 0.052157. Entropy: 0.298439.\n",
      "episode: 5277   score: 225.0  epsilon: 1.0    steps: 416  evaluation reward: 402.45\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14869: Policy loss: 0.116275. Value loss: 0.367310. Entropy: 0.299549.\n",
      "Iteration 14870: Policy loss: 0.088180. Value loss: 0.144531. Entropy: 0.297489.\n",
      "Iteration 14871: Policy loss: 0.078549. Value loss: 0.082852. Entropy: 0.297980.\n",
      "episode: 5278   score: 545.0  epsilon: 1.0    steps: 448  evaluation reward: 403.9\n",
      "episode: 5279   score: 520.0  epsilon: 1.0    steps: 944  evaluation reward: 402.4\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14872: Policy loss: -0.342195. Value loss: 0.299401. Entropy: 0.296561.\n",
      "Iteration 14873: Policy loss: -0.367074. Value loss: 0.115211. Entropy: 0.297369.\n",
      "Iteration 14874: Policy loss: -0.382634. Value loss: 0.051488. Entropy: 0.295700.\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14875: Policy loss: 0.438677. Value loss: 0.119033. Entropy: 0.305304.\n",
      "Iteration 14876: Policy loss: 0.438968. Value loss: 0.033335. Entropy: 0.304138.\n",
      "Iteration 14877: Policy loss: 0.425319. Value loss: 0.025196. Entropy: 0.303863.\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14878: Policy loss: 0.020931. Value loss: 0.115765. Entropy: 0.310405.\n",
      "Iteration 14879: Policy loss: 0.025577. Value loss: 0.060361. Entropy: 0.307798.\n",
      "Iteration 14880: Policy loss: 0.018732. Value loss: 0.038908. Entropy: 0.309684.\n",
      "episode: 5280   score: 420.0  epsilon: 1.0    steps: 952  evaluation reward: 404.7\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14881: Policy loss: 0.443158. Value loss: 0.183987. Entropy: 0.306690.\n",
      "Iteration 14882: Policy loss: 0.419623. Value loss: 0.053704. Entropy: 0.305243.\n",
      "Iteration 14883: Policy loss: 0.412452. Value loss: 0.033907. Entropy: 0.305509.\n",
      "episode: 5281   score: 325.0  epsilon: 1.0    steps: 16  evaluation reward: 399.9\n",
      "episode: 5282   score: 955.0  epsilon: 1.0    steps: 896  evaluation reward: 405.85\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14884: Policy loss: 0.252136. Value loss: 0.109947. Entropy: 0.288111.\n",
      "Iteration 14885: Policy loss: 0.246382. Value loss: 0.049320. Entropy: 0.284337.\n",
      "Iteration 14886: Policy loss: 0.238770. Value loss: 0.037102. Entropy: 0.285406.\n",
      "episode: 5283   score: 290.0  epsilon: 1.0    steps: 608  evaluation reward: 406.15\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14887: Policy loss: -0.031817. Value loss: 0.092239. Entropy: 0.291667.\n",
      "Iteration 14888: Policy loss: -0.036345. Value loss: 0.041989. Entropy: 0.290201.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14889: Policy loss: -0.038010. Value loss: 0.033124. Entropy: 0.288830.\n",
      "episode: 5284   score: 300.0  epsilon: 1.0    steps: 760  evaluation reward: 404.0\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14890: Policy loss: 0.100438. Value loss: 0.167269. Entropy: 0.297565.\n",
      "Iteration 14891: Policy loss: 0.093718. Value loss: 0.071638. Entropy: 0.295981.\n",
      "Iteration 14892: Policy loss: 0.090030. Value loss: 0.049246. Entropy: 0.296033.\n",
      "episode: 5285   score: 385.0  epsilon: 1.0    steps: 856  evaluation reward: 406.5\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14893: Policy loss: -0.076468. Value loss: 0.269418. Entropy: 0.302739.\n",
      "Iteration 14894: Policy loss: -0.117292. Value loss: 0.108064. Entropy: 0.300783.\n",
      "Iteration 14895: Policy loss: -0.112097. Value loss: 0.059586. Entropy: 0.301673.\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14896: Policy loss: -0.004285. Value loss: 0.072377. Entropy: 0.305981.\n",
      "Iteration 14897: Policy loss: -0.005043. Value loss: 0.032139. Entropy: 0.305766.\n",
      "Iteration 14898: Policy loss: -0.008491. Value loss: 0.023264. Entropy: 0.306602.\n",
      "episode: 5286   score: 290.0  epsilon: 1.0    steps: 960  evaluation reward: 403.3\n",
      "Training network. lr: 0.000136. clip: 0.054390\n",
      "Iteration 14899: Policy loss: 0.092231. Value loss: 0.123526. Entropy: 0.304666.\n",
      "Iteration 14900: Policy loss: 0.086991. Value loss: 0.061824. Entropy: 0.304476.\n",
      "Iteration 14901: Policy loss: 0.088075. Value loss: 0.044072. Entropy: 0.303204.\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14902: Policy loss: 0.003878. Value loss: 0.138184. Entropy: 0.298373.\n",
      "Iteration 14903: Policy loss: -0.013043. Value loss: 0.049195. Entropy: 0.297498.\n",
      "Iteration 14904: Policy loss: -0.008952. Value loss: 0.029567. Entropy: 0.298040.\n",
      "episode: 5287   score: 525.0  epsilon: 1.0    steps: 856  evaluation reward: 405.25\n",
      "episode: 5288   score: 295.0  epsilon: 1.0    steps: 984  evaluation reward: 403.5\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14905: Policy loss: -0.108695. Value loss: 0.145501. Entropy: 0.298255.\n",
      "Iteration 14906: Policy loss: -0.116220. Value loss: 0.068825. Entropy: 0.296824.\n",
      "Iteration 14907: Policy loss: -0.122944. Value loss: 0.050694. Entropy: 0.297601.\n",
      "episode: 5289   score: 530.0  epsilon: 1.0    steps: 672  evaluation reward: 406.1\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14908: Policy loss: 0.149387. Value loss: 0.122726. Entropy: 0.290503.\n",
      "Iteration 14909: Policy loss: 0.142504. Value loss: 0.046930. Entropy: 0.292253.\n",
      "Iteration 14910: Policy loss: 0.133388. Value loss: 0.030821. Entropy: 0.292116.\n",
      "episode: 5290   score: 450.0  epsilon: 1.0    steps: 832  evaluation reward: 408.5\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14911: Policy loss: -0.020099. Value loss: 0.119824. Entropy: 0.302100.\n",
      "Iteration 14912: Policy loss: -0.020394. Value loss: 0.058089. Entropy: 0.300627.\n",
      "Iteration 14913: Policy loss: -0.024950. Value loss: 0.041193. Entropy: 0.300785.\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14914: Policy loss: 0.232726. Value loss: 0.076912. Entropy: 0.304979.\n",
      "Iteration 14915: Policy loss: 0.227147. Value loss: 0.028707. Entropy: 0.303453.\n",
      "Iteration 14916: Policy loss: 0.216690. Value loss: 0.019873. Entropy: 0.304187.\n",
      "episode: 5291   score: 575.0  epsilon: 1.0    steps: 840  evaluation reward: 410.8\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14917: Policy loss: 0.009255. Value loss: 0.068815. Entropy: 0.300415.\n",
      "Iteration 14918: Policy loss: 0.010132. Value loss: 0.033787. Entropy: 0.299905.\n",
      "Iteration 14919: Policy loss: 0.006860. Value loss: 0.023947. Entropy: 0.299932.\n",
      "episode: 5292   score: 230.0  epsilon: 1.0    steps: 512  evaluation reward: 408.8\n",
      "episode: 5293   score: 390.0  epsilon: 1.0    steps: 584  evaluation reward: 408.8\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14920: Policy loss: -0.014022. Value loss: 0.059784. Entropy: 0.279931.\n",
      "Iteration 14921: Policy loss: -0.012884. Value loss: 0.031579. Entropy: 0.280278.\n",
      "Iteration 14922: Policy loss: -0.019544. Value loss: 0.025204. Entropy: 0.281707.\n",
      "episode: 5294   score: 390.0  epsilon: 1.0    steps: 640  evaluation reward: 408.45\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14923: Policy loss: -0.268072. Value loss: 0.545580. Entropy: 0.293144.\n",
      "Iteration 14924: Policy loss: -0.284267. Value loss: 0.317687. Entropy: 0.292867.\n",
      "Iteration 14925: Policy loss: -0.279881. Value loss: 0.210598. Entropy: 0.291247.\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14926: Policy loss: 0.078719. Value loss: 0.123928. Entropy: 0.306485.\n",
      "Iteration 14927: Policy loss: 0.078798. Value loss: 0.057543. Entropy: 0.305543.\n",
      "Iteration 14928: Policy loss: 0.074672. Value loss: 0.041605. Entropy: 0.305510.\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14929: Policy loss: -0.032255. Value loss: 0.252027. Entropy: 0.306221.\n",
      "Iteration 14930: Policy loss: -0.029386. Value loss: 0.119718. Entropy: 0.304743.\n",
      "Iteration 14931: Policy loss: -0.047440. Value loss: 0.073535. Entropy: 0.304873.\n",
      "episode: 5295   score: 520.0  epsilon: 1.0    steps: 8  evaluation reward: 408.75\n",
      "episode: 5296   score: 500.0  epsilon: 1.0    steps: 16  evaluation reward: 410.8\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14932: Policy loss: 0.037782. Value loss: 0.081173. Entropy: 0.289407.\n",
      "Iteration 14933: Policy loss: 0.033073. Value loss: 0.036533. Entropy: 0.288582.\n",
      "Iteration 14934: Policy loss: 0.035002. Value loss: 0.022518. Entropy: 0.288124.\n",
      "episode: 5297   score: 345.0  epsilon: 1.0    steps: 432  evaluation reward: 409.05\n",
      "episode: 5298   score: 310.0  epsilon: 1.0    steps: 760  evaluation reward: 408.2\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14935: Policy loss: -0.157502. Value loss: 0.321753. Entropy: 0.286923.\n",
      "Iteration 14936: Policy loss: -0.147729. Value loss: 0.114965. Entropy: 0.286829.\n",
      "Iteration 14937: Policy loss: -0.165351. Value loss: 0.049121. Entropy: 0.286415.\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14938: Policy loss: 0.101299. Value loss: 0.078241. Entropy: 0.306117.\n",
      "Iteration 14939: Policy loss: 0.093380. Value loss: 0.037047. Entropy: 0.305241.\n",
      "Iteration 14940: Policy loss: 0.095190. Value loss: 0.028569. Entropy: 0.305213.\n",
      "episode: 5299   score: 460.0  epsilon: 1.0    steps: 640  evaluation reward: 410.1\n",
      "episode: 5300   score: 490.0  epsilon: 1.0    steps: 888  evaluation reward: 411.1\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14941: Policy loss: 0.256325. Value loss: 0.150378. Entropy: 0.290035.\n",
      "Iteration 14942: Policy loss: 0.240064. Value loss: 0.050601. Entropy: 0.289934.\n",
      "Iteration 14943: Policy loss: 0.240607. Value loss: 0.035357. Entropy: 0.290496.\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14944: Policy loss: 0.011918. Value loss: 0.102874. Entropy: 0.305700.\n",
      "Iteration 14945: Policy loss: 0.006976. Value loss: 0.041195. Entropy: 0.304178.\n",
      "Iteration 14946: Policy loss: -0.000333. Value loss: 0.028012. Entropy: 0.304446.\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14947: Policy loss: -0.250683. Value loss: 0.163862. Entropy: 0.304200.\n",
      "Iteration 14948: Policy loss: -0.238315. Value loss: 0.064217. Entropy: 0.306520.\n",
      "Iteration 14949: Policy loss: -0.262616. Value loss: 0.051048. Entropy: 0.305459.\n",
      "now time :  2019-09-06 05:42:52.994720\n",
      "episode: 5301   score: 385.0  epsilon: 1.0    steps: 72  evaluation reward: 410.8\n",
      "Training network. lr: 0.000136. clip: 0.054233\n",
      "Iteration 14950: Policy loss: -0.468376. Value loss: 0.343359. Entropy: 0.297296.\n",
      "Iteration 14951: Policy loss: -0.486463. Value loss: 0.255050. Entropy: 0.296826.\n",
      "Iteration 14952: Policy loss: -0.493662. Value loss: 0.213908. Entropy: 0.296539.\n",
      "episode: 5302   score: 530.0  epsilon: 1.0    steps: 232  evaluation reward: 412.85\n",
      "episode: 5303   score: 400.0  epsilon: 1.0    steps: 992  evaluation reward: 414.0\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14953: Policy loss: -0.050086. Value loss: 0.217808. Entropy: 0.292023.\n",
      "Iteration 14954: Policy loss: -0.045350. Value loss: 0.075904. Entropy: 0.291696.\n",
      "Iteration 14955: Policy loss: -0.053354. Value loss: 0.053592. Entropy: 0.291643.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5304   score: 365.0  epsilon: 1.0    steps: 936  evaluation reward: 412.2\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14956: Policy loss: 0.297953. Value loss: 0.121796. Entropy: 0.298643.\n",
      "Iteration 14957: Policy loss: 0.295516. Value loss: 0.046722. Entropy: 0.297057.\n",
      "Iteration 14958: Policy loss: 0.291842. Value loss: 0.037501. Entropy: 0.297129.\n",
      "episode: 5305   score: 585.0  epsilon: 1.0    steps: 408  evaluation reward: 415.95\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14959: Policy loss: -0.286050. Value loss: 0.309210. Entropy: 0.287892.\n",
      "Iteration 14960: Policy loss: -0.290609. Value loss: 0.201863. Entropy: 0.286340.\n",
      "Iteration 14961: Policy loss: -0.311435. Value loss: 0.140086. Entropy: 0.286601.\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14962: Policy loss: -0.430441. Value loss: 0.194173. Entropy: 0.312183.\n",
      "Iteration 14963: Policy loss: -0.433845. Value loss: 0.075662. Entropy: 0.312727.\n",
      "Iteration 14964: Policy loss: -0.441808. Value loss: 0.051975. Entropy: 0.312409.\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14965: Policy loss: 0.156810. Value loss: 0.140165. Entropy: 0.307515.\n",
      "Iteration 14966: Policy loss: 0.152564. Value loss: 0.052809. Entropy: 0.307858.\n",
      "Iteration 14967: Policy loss: 0.143749. Value loss: 0.042873. Entropy: 0.306919.\n",
      "episode: 5306   score: 210.0  epsilon: 1.0    steps: 216  evaluation reward: 415.9\n",
      "episode: 5307   score: 670.0  epsilon: 1.0    steps: 736  evaluation reward: 420.5\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14968: Policy loss: 0.348228. Value loss: 0.204879. Entropy: 0.282955.\n",
      "Iteration 14969: Policy loss: 0.348963. Value loss: 0.071033. Entropy: 0.280989.\n",
      "Iteration 14970: Policy loss: 0.331763. Value loss: 0.047132. Entropy: 0.282176.\n",
      "episode: 5308   score: 750.0  epsilon: 1.0    steps: 336  evaluation reward: 424.35\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14971: Policy loss: 0.137336. Value loss: 0.078139. Entropy: 0.298496.\n",
      "Iteration 14972: Policy loss: 0.131572. Value loss: 0.029509. Entropy: 0.298098.\n",
      "Iteration 14973: Policy loss: 0.132386. Value loss: 0.019352. Entropy: 0.299196.\n",
      "episode: 5309   score: 700.0  epsilon: 1.0    steps: 856  evaluation reward: 427.9\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14974: Policy loss: -0.191785. Value loss: 0.163458. Entropy: 0.302255.\n",
      "Iteration 14975: Policy loss: -0.199524. Value loss: 0.064292. Entropy: 0.303287.\n",
      "Iteration 14976: Policy loss: -0.202331. Value loss: 0.045669. Entropy: 0.303697.\n",
      "episode: 5310   score: 620.0  epsilon: 1.0    steps: 656  evaluation reward: 430.8\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14977: Policy loss: 0.387460. Value loss: 0.231047. Entropy: 0.289630.\n",
      "Iteration 14978: Policy loss: 0.399073. Value loss: 0.094146. Entropy: 0.291632.\n",
      "Iteration 14979: Policy loss: 0.377731. Value loss: 0.064286. Entropy: 0.291182.\n",
      "episode: 5311   score: 265.0  epsilon: 1.0    steps: 120  evaluation reward: 429.9\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14980: Policy loss: 0.220315. Value loss: 0.106441. Entropy: 0.298677.\n",
      "Iteration 14981: Policy loss: 0.205583. Value loss: 0.040862. Entropy: 0.297942.\n",
      "Iteration 14982: Policy loss: 0.207346. Value loss: 0.028084. Entropy: 0.297731.\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14983: Policy loss: -0.111461. Value loss: 0.100509. Entropy: 0.307348.\n",
      "Iteration 14984: Policy loss: -0.109395. Value loss: 0.047237. Entropy: 0.305953.\n",
      "Iteration 14985: Policy loss: -0.117581. Value loss: 0.033826. Entropy: 0.306636.\n",
      "episode: 5312   score: 650.0  epsilon: 1.0    steps: 568  evaluation reward: 431.95\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14986: Policy loss: 0.340828. Value loss: 0.169561. Entropy: 0.299730.\n",
      "Iteration 14987: Policy loss: 0.338245. Value loss: 0.053118. Entropy: 0.300360.\n",
      "Iteration 14988: Policy loss: 0.323156. Value loss: 0.032685. Entropy: 0.299345.\n",
      "episode: 5313   score: 420.0  epsilon: 1.0    steps: 416  evaluation reward: 431.95\n",
      "episode: 5314   score: 240.0  epsilon: 1.0    steps: 712  evaluation reward: 432.25\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14989: Policy loss: 0.490949. Value loss: 0.226048. Entropy: 0.298908.\n",
      "Iteration 14990: Policy loss: 0.478851. Value loss: 0.081659. Entropy: 0.297338.\n",
      "Iteration 14991: Policy loss: 0.466484. Value loss: 0.057563. Entropy: 0.296701.\n",
      "episode: 5315   score: 345.0  epsilon: 1.0    steps: 520  evaluation reward: 429.55\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14992: Policy loss: -0.255257. Value loss: 0.266871. Entropy: 0.300481.\n",
      "Iteration 14993: Policy loss: -0.258330. Value loss: 0.158330. Entropy: 0.302041.\n",
      "Iteration 14994: Policy loss: -0.243411. Value loss: 0.095857. Entropy: 0.301689.\n",
      "episode: 5316   score: 300.0  epsilon: 1.0    steps: 56  evaluation reward: 430.1\n",
      "episode: 5317   score: 290.0  epsilon: 1.0    steps: 728  evaluation reward: 428.15\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14995: Policy loss: 0.065014. Value loss: 0.113485. Entropy: 0.290386.\n",
      "Iteration 14996: Policy loss: 0.070648. Value loss: 0.061280. Entropy: 0.289778.\n",
      "Iteration 14997: Policy loss: 0.067168. Value loss: 0.039309. Entropy: 0.290353.\n",
      "episode: 5318   score: 380.0  epsilon: 1.0    steps: 864  evaluation reward: 429.9\n",
      "Training network. lr: 0.000135. clip: 0.054077\n",
      "Iteration 14998: Policy loss: 0.154074. Value loss: 0.113946. Entropy: 0.302849.\n",
      "Iteration 14999: Policy loss: 0.152881. Value loss: 0.043816. Entropy: 0.302902.\n",
      "Iteration 15000: Policy loss: 0.155350. Value loss: 0.036009. Entropy: 0.301924.\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15001: Policy loss: 0.199104. Value loss: 0.086555. Entropy: 0.306571.\n",
      "Iteration 15002: Policy loss: 0.192449. Value loss: 0.027635. Entropy: 0.305716.\n",
      "Iteration 15003: Policy loss: 0.185553. Value loss: 0.019235. Entropy: 0.305599.\n",
      "episode: 5319   score: 490.0  epsilon: 1.0    steps: 584  evaluation reward: 426.85\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15004: Policy loss: 0.215809. Value loss: 0.105427. Entropy: 0.304808.\n",
      "Iteration 15005: Policy loss: 0.208925. Value loss: 0.043575. Entropy: 0.303478.\n",
      "Iteration 15006: Policy loss: 0.206411. Value loss: 0.031416. Entropy: 0.302689.\n",
      "episode: 5320   score: 225.0  epsilon: 1.0    steps: 1016  evaluation reward: 426.05\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15007: Policy loss: 0.044741. Value loss: 0.097677. Entropy: 0.307354.\n",
      "Iteration 15008: Policy loss: 0.044249. Value loss: 0.037187. Entropy: 0.307648.\n",
      "Iteration 15009: Policy loss: 0.039399. Value loss: 0.028219. Entropy: 0.307099.\n",
      "episode: 5321   score: 240.0  epsilon: 1.0    steps: 376  evaluation reward: 425.6\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15010: Policy loss: 0.157077. Value loss: 0.119771. Entropy: 0.305363.\n",
      "Iteration 15011: Policy loss: 0.142409. Value loss: 0.059216. Entropy: 0.303939.\n",
      "Iteration 15012: Policy loss: 0.135765. Value loss: 0.040490. Entropy: 0.304071.\n",
      "episode: 5322   score: 350.0  epsilon: 1.0    steps: 232  evaluation reward: 423.8\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15013: Policy loss: 0.008243. Value loss: 0.122890. Entropy: 0.305807.\n",
      "Iteration 15014: Policy loss: 0.005834. Value loss: 0.055747. Entropy: 0.306165.\n",
      "Iteration 15015: Policy loss: 0.004687. Value loss: 0.034389. Entropy: 0.306188.\n",
      "episode: 5323   score: 270.0  epsilon: 1.0    steps: 336  evaluation reward: 423.2\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15016: Policy loss: 0.018789. Value loss: 0.090447. Entropy: 0.305878.\n",
      "Iteration 15017: Policy loss: 0.020045. Value loss: 0.041619. Entropy: 0.305151.\n",
      "Iteration 15018: Policy loss: 0.022627. Value loss: 0.030921. Entropy: 0.305228.\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15019: Policy loss: 0.191245. Value loss: 0.070061. Entropy: 0.312391.\n",
      "Iteration 15020: Policy loss: 0.185092. Value loss: 0.023156. Entropy: 0.311245.\n",
      "Iteration 15021: Policy loss: 0.178726. Value loss: 0.015379. Entropy: 0.310657.\n",
      "episode: 5324   score: 465.0  epsilon: 1.0    steps: 584  evaluation reward: 422.9\n",
      "Training network. lr: 0.000135. clip: 0.053929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15022: Policy loss: 0.010150. Value loss: 0.072098. Entropy: 0.307051.\n",
      "Iteration 15023: Policy loss: 0.011901. Value loss: 0.028731. Entropy: 0.306263.\n",
      "Iteration 15024: Policy loss: 0.004979. Value loss: 0.021380. Entropy: 0.306840.\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15025: Policy loss: -0.466075. Value loss: 0.329894. Entropy: 0.310907.\n",
      "Iteration 15026: Policy loss: -0.471653. Value loss: 0.141467. Entropy: 0.310815.\n",
      "Iteration 15027: Policy loss: -0.464419. Value loss: 0.053332. Entropy: 0.311231.\n",
      "episode: 5325   score: 535.0  epsilon: 1.0    steps: 184  evaluation reward: 425.1\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15028: Policy loss: 0.233258. Value loss: 0.204467. Entropy: 0.305013.\n",
      "Iteration 15029: Policy loss: 0.234915. Value loss: 0.071027. Entropy: 0.305593.\n",
      "Iteration 15030: Policy loss: 0.223000. Value loss: 0.041947. Entropy: 0.305344.\n",
      "episode: 5326   score: 315.0  epsilon: 1.0    steps: 80  evaluation reward: 423.85\n",
      "episode: 5327   score: 555.0  epsilon: 1.0    steps: 504  evaluation reward: 425.0\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15031: Policy loss: -0.202035. Value loss: 0.294886. Entropy: 0.303822.\n",
      "Iteration 15032: Policy loss: -0.209334. Value loss: 0.170453. Entropy: 0.302815.\n",
      "Iteration 15033: Policy loss: -0.221857. Value loss: 0.109087. Entropy: 0.302168.\n",
      "episode: 5328   score: 725.0  epsilon: 1.0    steps: 608  evaluation reward: 429.8\n",
      "episode: 5329   score: 385.0  epsilon: 1.0    steps: 976  evaluation reward: 431.1\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15034: Policy loss: -0.251122. Value loss: 0.327347. Entropy: 0.305871.\n",
      "Iteration 15035: Policy loss: -0.253546. Value loss: 0.177550. Entropy: 0.305920.\n",
      "Iteration 15036: Policy loss: -0.264762. Value loss: 0.073115. Entropy: 0.305875.\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15037: Policy loss: 0.033534. Value loss: 0.084517. Entropy: 0.308595.\n",
      "Iteration 15038: Policy loss: 0.028003. Value loss: 0.043092. Entropy: 0.309088.\n",
      "Iteration 15039: Policy loss: 0.030552. Value loss: 0.030139. Entropy: 0.309140.\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15040: Policy loss: -0.031921. Value loss: 0.098807. Entropy: 0.310881.\n",
      "Iteration 15041: Policy loss: -0.043879. Value loss: 0.042032. Entropy: 0.311015.\n",
      "Iteration 15042: Policy loss: -0.042857. Value loss: 0.031124. Entropy: 0.311056.\n",
      "episode: 5330   score: 450.0  epsilon: 1.0    steps: 528  evaluation reward: 431.0\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15043: Policy loss: 0.475342. Value loss: 0.167594. Entropy: 0.304599.\n",
      "Iteration 15044: Policy loss: 0.462879. Value loss: 0.054142. Entropy: 0.304072.\n",
      "Iteration 15045: Policy loss: 0.466100. Value loss: 0.039548. Entropy: 0.303538.\n",
      "episode: 5331   score: 365.0  epsilon: 1.0    steps: 32  evaluation reward: 432.55\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15046: Policy loss: -0.044570. Value loss: 0.076850. Entropy: 0.307253.\n",
      "Iteration 15047: Policy loss: -0.049145. Value loss: 0.039555. Entropy: 0.307267.\n",
      "Iteration 15048: Policy loss: -0.051413. Value loss: 0.031079. Entropy: 0.307145.\n",
      "Training network. lr: 0.000135. clip: 0.053929\n",
      "Iteration 15049: Policy loss: -0.033978. Value loss: 0.329931. Entropy: 0.310087.\n",
      "Iteration 15050: Policy loss: -0.001052. Value loss: 0.207494. Entropy: 0.309465.\n",
      "Iteration 15051: Policy loss: -0.064225. Value loss: 0.205069. Entropy: 0.310058.\n",
      "episode: 5332   score: 285.0  epsilon: 1.0    steps: 256  evaluation reward: 433.6\n",
      "episode: 5333   score: 450.0  epsilon: 1.0    steps: 664  evaluation reward: 432.35\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15052: Policy loss: -0.098162. Value loss: 0.169865. Entropy: 0.300382.\n",
      "Iteration 15053: Policy loss: -0.119583. Value loss: 0.049012. Entropy: 0.300778.\n",
      "Iteration 15054: Policy loss: -0.129882. Value loss: 0.036568. Entropy: 0.301054.\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15055: Policy loss: 0.029533. Value loss: 0.108676. Entropy: 0.309717.\n",
      "Iteration 15056: Policy loss: 0.028023. Value loss: 0.045046. Entropy: 0.309065.\n",
      "Iteration 15057: Policy loss: 0.022331. Value loss: 0.032377. Entropy: 0.308956.\n",
      "episode: 5334   score: 330.0  epsilon: 1.0    steps: 104  evaluation reward: 431.75\n",
      "episode: 5335   score: 350.0  epsilon: 1.0    steps: 456  evaluation reward: 431.9\n",
      "episode: 5336   score: 700.0  epsilon: 1.0    steps: 1008  evaluation reward: 435.75\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15058: Policy loss: 0.054102. Value loss: 0.157682. Entropy: 0.295035.\n",
      "Iteration 15059: Policy loss: 0.052228. Value loss: 0.079235. Entropy: 0.295331.\n",
      "Iteration 15060: Policy loss: 0.043326. Value loss: 0.058803. Entropy: 0.293377.\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15061: Policy loss: 0.149652. Value loss: 0.350971. Entropy: 0.310701.\n",
      "Iteration 15062: Policy loss: 0.133811. Value loss: 0.172847. Entropy: 0.309291.\n",
      "Iteration 15063: Policy loss: 0.148121. Value loss: 0.112216. Entropy: 0.308683.\n",
      "episode: 5337   score: 425.0  epsilon: 1.0    steps: 48  evaluation reward: 435.0\n",
      "episode: 5338   score: 765.0  epsilon: 1.0    steps: 80  evaluation reward: 438.45\n",
      "episode: 5339   score: 380.0  epsilon: 1.0    steps: 688  evaluation reward: 438.8\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15064: Policy loss: 0.097887. Value loss: 0.068470. Entropy: 0.301022.\n",
      "Iteration 15065: Policy loss: 0.096227. Value loss: 0.040195. Entropy: 0.302342.\n",
      "Iteration 15066: Policy loss: 0.095163. Value loss: 0.035409. Entropy: 0.303004.\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15067: Policy loss: 0.149388. Value loss: 0.095538. Entropy: 0.310772.\n",
      "Iteration 15068: Policy loss: 0.148706. Value loss: 0.043047. Entropy: 0.308864.\n",
      "Iteration 15069: Policy loss: 0.139263. Value loss: 0.028274. Entropy: 0.308866.\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15070: Policy loss: 0.236875. Value loss: 0.174923. Entropy: 0.306046.\n",
      "Iteration 15071: Policy loss: 0.227550. Value loss: 0.067883. Entropy: 0.304990.\n",
      "Iteration 15072: Policy loss: 0.219908. Value loss: 0.046497. Entropy: 0.304667.\n",
      "episode: 5340   score: 345.0  epsilon: 1.0    steps: 544  evaluation reward: 440.1\n",
      "episode: 5341   score: 180.0  epsilon: 1.0    steps: 600  evaluation reward: 438.5\n",
      "episode: 5342   score: 300.0  epsilon: 1.0    steps: 832  evaluation reward: 435.8\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15073: Policy loss: 0.101006. Value loss: 0.181834. Entropy: 0.297928.\n",
      "Iteration 15074: Policy loss: 0.084796. Value loss: 0.135388. Entropy: 0.298015.\n",
      "Iteration 15075: Policy loss: 0.082577. Value loss: 0.123806. Entropy: 0.297791.\n",
      "episode: 5343   score: 210.0  epsilon: 1.0    steps: 80  evaluation reward: 434.6\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15076: Policy loss: -0.091151. Value loss: 0.085823. Entropy: 0.306082.\n",
      "Iteration 15077: Policy loss: -0.089657. Value loss: 0.045833. Entropy: 0.306534.\n",
      "Iteration 15078: Policy loss: -0.091306. Value loss: 0.036124. Entropy: 0.306356.\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15079: Policy loss: -0.050055. Value loss: 0.117224. Entropy: 0.304107.\n",
      "Iteration 15080: Policy loss: -0.051658. Value loss: 0.037725. Entropy: 0.301205.\n",
      "Iteration 15081: Policy loss: -0.055930. Value loss: 0.024509. Entropy: 0.302260.\n",
      "episode: 5344   score: 255.0  epsilon: 1.0    steps: 528  evaluation reward: 430.1\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15082: Policy loss: 0.265970. Value loss: 0.386964. Entropy: 0.305063.\n",
      "Iteration 15083: Policy loss: 0.260368. Value loss: 0.117628. Entropy: 0.306347.\n",
      "Iteration 15084: Policy loss: 0.289220. Value loss: 0.067985. Entropy: 0.303935.\n",
      "episode: 5345   score: 315.0  epsilon: 1.0    steps: 752  evaluation reward: 427.05\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15085: Policy loss: -0.224329. Value loss: 0.335417. Entropy: 0.306800.\n",
      "Iteration 15086: Policy loss: -0.240943. Value loss: 0.240934. Entropy: 0.306856.\n",
      "Iteration 15087: Policy loss: -0.246180. Value loss: 0.184687. Entropy: 0.307292.\n",
      "episode: 5346   score: 590.0  epsilon: 1.0    steps: 944  evaluation reward: 428.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15088: Policy loss: 0.061466. Value loss: 0.120890. Entropy: 0.304520.\n",
      "Iteration 15089: Policy loss: 0.057180. Value loss: 0.053523. Entropy: 0.303487.\n",
      "Iteration 15090: Policy loss: 0.056079. Value loss: 0.038664. Entropy: 0.302478.\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15091: Policy loss: 0.130603. Value loss: 0.138082. Entropy: 0.307122.\n",
      "Iteration 15092: Policy loss: 0.129780. Value loss: 0.054979. Entropy: 0.307288.\n",
      "Iteration 15093: Policy loss: 0.124445. Value loss: 0.032756. Entropy: 0.306573.\n",
      "episode: 5347   score: 320.0  epsilon: 1.0    steps: 672  evaluation reward: 426.35\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15094: Policy loss: 0.314640. Value loss: 0.166851. Entropy: 0.306381.\n",
      "Iteration 15095: Policy loss: 0.298679. Value loss: 0.043077. Entropy: 0.305594.\n",
      "Iteration 15096: Policy loss: 0.294545. Value loss: 0.031328. Entropy: 0.305750.\n",
      "episode: 5348   score: 320.0  epsilon: 1.0    steps: 520  evaluation reward: 426.7\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15097: Policy loss: -0.512725. Value loss: 0.459131. Entropy: 0.305501.\n",
      "Iteration 15098: Policy loss: -0.534595. Value loss: 0.184244. Entropy: 0.306607.\n",
      "Iteration 15099: Policy loss: -0.567605. Value loss: 0.087640. Entropy: 0.306801.\n",
      "episode: 5349   score: 795.0  epsilon: 1.0    steps: 56  evaluation reward: 425.15\n",
      "episode: 5350   score: 500.0  epsilon: 1.0    steps: 768  evaluation reward: 423.7\n",
      "Training network. lr: 0.000134. clip: 0.053773\n",
      "Iteration 15100: Policy loss: 0.256414. Value loss: 0.133158. Entropy: 0.302247.\n",
      "Iteration 15101: Policy loss: 0.252131. Value loss: 0.065834. Entropy: 0.301858.\n",
      "Iteration 15102: Policy loss: 0.249344. Value loss: 0.045555. Entropy: 0.301424.\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15103: Policy loss: 0.191545. Value loss: 0.165933. Entropy: 0.307562.\n",
      "Iteration 15104: Policy loss: 0.182441. Value loss: 0.058935. Entropy: 0.306407.\n",
      "Iteration 15105: Policy loss: 0.191632. Value loss: 0.040176. Entropy: 0.306230.\n",
      "now time :  2019-09-06 05:52:36.119812\n",
      "episode: 5351   score: 375.0  epsilon: 1.0    steps: 800  evaluation reward: 422.9\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15106: Policy loss: 0.254813. Value loss: 0.116904. Entropy: 0.301161.\n",
      "Iteration 15107: Policy loss: 0.245095. Value loss: 0.045451. Entropy: 0.301263.\n",
      "Iteration 15108: Policy loss: 0.239902. Value loss: 0.031938. Entropy: 0.300444.\n",
      "episode: 5352   score: 405.0  epsilon: 1.0    steps: 216  evaluation reward: 423.25\n",
      "episode: 5353   score: 135.0  epsilon: 1.0    steps: 504  evaluation reward: 418.75\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15109: Policy loss: -0.323371. Value loss: 0.271444. Entropy: 0.306884.\n",
      "Iteration 15110: Policy loss: -0.314868. Value loss: 0.065505. Entropy: 0.308786.\n",
      "Iteration 15111: Policy loss: -0.329631. Value loss: 0.036996. Entropy: 0.308038.\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15112: Policy loss: 0.236561. Value loss: 0.351021. Entropy: 0.311936.\n",
      "Iteration 15113: Policy loss: 0.213839. Value loss: 0.096294. Entropy: 0.310185.\n",
      "Iteration 15114: Policy loss: 0.194670. Value loss: 0.056129. Entropy: 0.309581.\n",
      "episode: 5354   score: 225.0  epsilon: 1.0    steps: 288  evaluation reward: 415.4\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15115: Policy loss: 0.136280. Value loss: 0.128226. Entropy: 0.304528.\n",
      "Iteration 15116: Policy loss: 0.141485. Value loss: 0.043491. Entropy: 0.304308.\n",
      "Iteration 15117: Policy loss: 0.133235. Value loss: 0.030587. Entropy: 0.304917.\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15118: Policy loss: 0.228496. Value loss: 0.196502. Entropy: 0.309900.\n",
      "Iteration 15119: Policy loss: 0.212842. Value loss: 0.052687. Entropy: 0.308828.\n",
      "Iteration 15120: Policy loss: 0.219096. Value loss: 0.033653. Entropy: 0.308119.\n",
      "episode: 5355   score: 545.0  epsilon: 1.0    steps: 128  evaluation reward: 418.4\n",
      "episode: 5356   score: 470.0  epsilon: 1.0    steps: 352  evaluation reward: 420.65\n",
      "episode: 5357   score: 295.0  epsilon: 1.0    steps: 848  evaluation reward: 418.75\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15121: Policy loss: 0.171422. Value loss: 0.095137. Entropy: 0.298461.\n",
      "Iteration 15122: Policy loss: 0.169141. Value loss: 0.051882. Entropy: 0.298668.\n",
      "Iteration 15123: Policy loss: 0.162758. Value loss: 0.041123. Entropy: 0.298207.\n",
      "episode: 5358   score: 715.0  epsilon: 1.0    steps: 104  evaluation reward: 421.7\n",
      "episode: 5359   score: 210.0  epsilon: 1.0    steps: 320  evaluation reward: 419.9\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15124: Policy loss: -0.270257. Value loss: 0.254357. Entropy: 0.306284.\n",
      "Iteration 15125: Policy loss: -0.279941. Value loss: 0.111619. Entropy: 0.306399.\n",
      "Iteration 15126: Policy loss: -0.275448. Value loss: 0.082676. Entropy: 0.306330.\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15127: Policy loss: -0.026516. Value loss: 0.151558. Entropy: 0.301777.\n",
      "Iteration 15128: Policy loss: -0.029545. Value loss: 0.083962. Entropy: 0.302589.\n",
      "Iteration 15129: Policy loss: -0.025714. Value loss: 0.062643. Entropy: 0.302199.\n",
      "episode: 5360   score: 380.0  epsilon: 1.0    steps: 568  evaluation reward: 419.45\n",
      "episode: 5361   score: 375.0  epsilon: 1.0    steps: 776  evaluation reward: 418.25\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15130: Policy loss: 0.137611. Value loss: 0.090839. Entropy: 0.305455.\n",
      "Iteration 15131: Policy loss: 0.133711. Value loss: 0.041945. Entropy: 0.305984.\n",
      "Iteration 15132: Policy loss: 0.130300. Value loss: 0.034778. Entropy: 0.304822.\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15133: Policy loss: -0.070903. Value loss: 0.256500. Entropy: 0.312201.\n",
      "Iteration 15134: Policy loss: -0.085966. Value loss: 0.137420. Entropy: 0.311809.\n",
      "Iteration 15135: Policy loss: -0.105944. Value loss: 0.100984. Entropy: 0.311069.\n",
      "episode: 5362   score: 495.0  epsilon: 1.0    steps: 208  evaluation reward: 420.6\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15136: Policy loss: 0.208748. Value loss: 0.124873. Entropy: 0.307355.\n",
      "Iteration 15137: Policy loss: 0.208579. Value loss: 0.059005. Entropy: 0.305380.\n",
      "Iteration 15138: Policy loss: 0.203736. Value loss: 0.034866. Entropy: 0.306918.\n",
      "episode: 5363   score: 430.0  epsilon: 1.0    steps: 976  evaluation reward: 421.75\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15139: Policy loss: 0.128410. Value loss: 0.152264. Entropy: 0.307683.\n",
      "Iteration 15140: Policy loss: 0.129887. Value loss: 0.052403. Entropy: 0.307926.\n",
      "Iteration 15141: Policy loss: 0.119826. Value loss: 0.033799. Entropy: 0.307816.\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15142: Policy loss: 0.173993. Value loss: 0.190518. Entropy: 0.311271.\n",
      "Iteration 15143: Policy loss: 0.165197. Value loss: 0.062206. Entropy: 0.310415.\n",
      "Iteration 15144: Policy loss: 0.165866. Value loss: 0.037418. Entropy: 0.310476.\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15145: Policy loss: 0.248870. Value loss: 0.186027. Entropy: 0.313807.\n",
      "Iteration 15146: Policy loss: 0.235890. Value loss: 0.061507. Entropy: 0.313125.\n",
      "Iteration 15147: Policy loss: 0.227363. Value loss: 0.040579. Entropy: 0.313440.\n",
      "episode: 5364   score: 370.0  epsilon: 1.0    steps: 336  evaluation reward: 419.25\n",
      "episode: 5365   score: 445.0  epsilon: 1.0    steps: 680  evaluation reward: 418.65\n",
      "Training network. lr: 0.000134. clip: 0.053616\n",
      "Iteration 15148: Policy loss: -0.152683. Value loss: 0.296712. Entropy: 0.304894.\n",
      "Iteration 15149: Policy loss: -0.168812. Value loss: 0.181227. Entropy: 0.305890.\n",
      "Iteration 15150: Policy loss: -0.189024. Value loss: 0.133043. Entropy: 0.304900.\n",
      "episode: 5366   score: 365.0  epsilon: 1.0    steps: 536  evaluation reward: 417.25\n",
      "episode: 5367   score: 330.0  epsilon: 1.0    steps: 608  evaluation reward: 417.4\n",
      "episode: 5368   score: 240.0  epsilon: 1.0    steps: 776  evaluation reward: 414.5\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15151: Policy loss: 0.340420. Value loss: 0.175607. Entropy: 0.296074.\n",
      "Iteration 15152: Policy loss: 0.336918. Value loss: 0.075906. Entropy: 0.294331.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15153: Policy loss: 0.322375. Value loss: 0.053536. Entropy: 0.295094.\n",
      "episode: 5369   score: 425.0  epsilon: 1.0    steps: 728  evaluation reward: 410.85\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15154: Policy loss: 0.349210. Value loss: 0.138793. Entropy: 0.308883.\n",
      "Iteration 15155: Policy loss: 0.341309. Value loss: 0.044051. Entropy: 0.307432.\n",
      "Iteration 15156: Policy loss: 0.329837. Value loss: 0.030137. Entropy: 0.305923.\n",
      "episode: 5370   score: 105.0  epsilon: 1.0    steps: 240  evaluation reward: 409.3\n",
      "episode: 5371   score: 260.0  epsilon: 1.0    steps: 976  evaluation reward: 409.2\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15157: Policy loss: -0.130823. Value loss: 0.260189. Entropy: 0.302568.\n",
      "Iteration 15158: Policy loss: -0.115715. Value loss: 0.087569. Entropy: 0.303491.\n",
      "Iteration 15159: Policy loss: -0.150613. Value loss: 0.052899. Entropy: 0.303535.\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15160: Policy loss: -0.188477. Value loss: 0.173370. Entropy: 0.307196.\n",
      "Iteration 15161: Policy loss: -0.187974. Value loss: 0.073384. Entropy: 0.306996.\n",
      "Iteration 15162: Policy loss: -0.192586. Value loss: 0.045315. Entropy: 0.307215.\n",
      "episode: 5372   score: 515.0  epsilon: 1.0    steps: 288  evaluation reward: 410.45\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15163: Policy loss: -0.098399. Value loss: 0.168570. Entropy: 0.305915.\n",
      "Iteration 15164: Policy loss: -0.098895. Value loss: 0.093634. Entropy: 0.305706.\n",
      "Iteration 15165: Policy loss: -0.107206. Value loss: 0.054819. Entropy: 0.305221.\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15166: Policy loss: 0.102711. Value loss: 0.168323. Entropy: 0.307002.\n",
      "Iteration 15167: Policy loss: 0.098595. Value loss: 0.070192. Entropy: 0.306924.\n",
      "Iteration 15168: Policy loss: 0.098359. Value loss: 0.053619. Entropy: 0.305923.\n",
      "episode: 5373   score: 180.0  epsilon: 1.0    steps: 56  evaluation reward: 409.35\n",
      "episode: 5374   score: 205.0  epsilon: 1.0    steps: 856  evaluation reward: 404.4\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15169: Policy loss: 0.198861. Value loss: 0.164728. Entropy: 0.302832.\n",
      "Iteration 15170: Policy loss: 0.190476. Value loss: 0.052505. Entropy: 0.301913.\n",
      "Iteration 15171: Policy loss: 0.183820. Value loss: 0.035079. Entropy: 0.301861.\n",
      "episode: 5375   score: 605.0  epsilon: 1.0    steps: 312  evaluation reward: 408.35\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15172: Policy loss: -0.343115. Value loss: 0.281627. Entropy: 0.308484.\n",
      "Iteration 15173: Policy loss: -0.345396. Value loss: 0.089585. Entropy: 0.309296.\n",
      "Iteration 15174: Policy loss: -0.349838. Value loss: 0.052403. Entropy: 0.308323.\n",
      "episode: 5376   score: 520.0  epsilon: 1.0    steps: 680  evaluation reward: 411.75\n",
      "episode: 5377   score: 485.0  epsilon: 1.0    steps: 936  evaluation reward: 414.35\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15175: Policy loss: -0.049224. Value loss: 0.182853. Entropy: 0.304179.\n",
      "Iteration 15176: Policy loss: -0.047640. Value loss: 0.072504. Entropy: 0.305903.\n",
      "Iteration 15177: Policy loss: -0.055346. Value loss: 0.043773. Entropy: 0.303963.\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15178: Policy loss: 0.205061. Value loss: 0.187225. Entropy: 0.310619.\n",
      "Iteration 15179: Policy loss: 0.195291. Value loss: 0.061666. Entropy: 0.310025.\n",
      "Iteration 15180: Policy loss: 0.185847. Value loss: 0.036955. Entropy: 0.309285.\n",
      "episode: 5378   score: 300.0  epsilon: 1.0    steps: 8  evaluation reward: 411.9\n",
      "episode: 5379   score: 560.0  epsilon: 1.0    steps: 32  evaluation reward: 412.3\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15181: Policy loss: 0.163039. Value loss: 0.118990. Entropy: 0.305690.\n",
      "Iteration 15182: Policy loss: 0.164186. Value loss: 0.050251. Entropy: 0.305701.\n",
      "Iteration 15183: Policy loss: 0.153434. Value loss: 0.032944. Entropy: 0.304780.\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15184: Policy loss: -0.376294. Value loss: 0.222633. Entropy: 0.312569.\n",
      "Iteration 15185: Policy loss: -0.383769. Value loss: 0.089180. Entropy: 0.312482.\n",
      "Iteration 15186: Policy loss: -0.389892. Value loss: 0.056619. Entropy: 0.311627.\n",
      "episode: 5380   score: 295.0  epsilon: 1.0    steps: 376  evaluation reward: 411.05\n",
      "episode: 5381   score: 375.0  epsilon: 1.0    steps: 552  evaluation reward: 411.55\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15187: Policy loss: -0.075306. Value loss: 0.225253. Entropy: 0.306614.\n",
      "Iteration 15188: Policy loss: -0.093845. Value loss: 0.074453. Entropy: 0.306492.\n",
      "Iteration 15189: Policy loss: -0.098012. Value loss: 0.041453. Entropy: 0.305630.\n",
      "episode: 5382   score: 255.0  epsilon: 1.0    steps: 1008  evaluation reward: 404.55\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15190: Policy loss: 0.030654. Value loss: 0.174990. Entropy: 0.310213.\n",
      "Iteration 15191: Policy loss: 0.010444. Value loss: 0.098790. Entropy: 0.308928.\n",
      "Iteration 15192: Policy loss: 0.008057. Value loss: 0.069533. Entropy: 0.309605.\n",
      "episode: 5383   score: 180.0  epsilon: 1.0    steps: 424  evaluation reward: 403.45\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15193: Policy loss: -0.097446. Value loss: 0.117113. Entropy: 0.307265.\n",
      "Iteration 15194: Policy loss: -0.097827. Value loss: 0.045188. Entropy: 0.307070.\n",
      "Iteration 15195: Policy loss: -0.105715. Value loss: 0.033308. Entropy: 0.308451.\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15196: Policy loss: 0.242890. Value loss: 0.267151. Entropy: 0.311331.\n",
      "Iteration 15197: Policy loss: 0.236333. Value loss: 0.085227. Entropy: 0.311180.\n",
      "Iteration 15198: Policy loss: 0.236167. Value loss: 0.049807. Entropy: 0.311584.\n",
      "Training network. lr: 0.000134. clip: 0.053468\n",
      "Iteration 15199: Policy loss: -0.133183. Value loss: 0.117272. Entropy: 0.311535.\n",
      "Iteration 15200: Policy loss: -0.132715. Value loss: 0.045549. Entropy: 0.311138.\n",
      "Iteration 15201: Policy loss: -0.134445. Value loss: 0.028364. Entropy: 0.311058.\n",
      "episode: 5384   score: 420.0  epsilon: 1.0    steps: 896  evaluation reward: 404.65\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15202: Policy loss: 0.093736. Value loss: 0.520727. Entropy: 0.309719.\n",
      "Iteration 15203: Policy loss: 0.068815. Value loss: 0.231417. Entropy: 0.309135.\n",
      "Iteration 15204: Policy loss: 0.071704. Value loss: 0.146169. Entropy: 0.308781.\n",
      "episode: 5385   score: 615.0  epsilon: 1.0    steps: 256  evaluation reward: 406.95\n",
      "episode: 5386   score: 415.0  epsilon: 1.0    steps: 392  evaluation reward: 408.2\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15205: Policy loss: 0.403005. Value loss: 0.142036. Entropy: 0.310379.\n",
      "Iteration 15206: Policy loss: 0.396109. Value loss: 0.039273. Entropy: 0.309615.\n",
      "Iteration 15207: Policy loss: 0.387365. Value loss: 0.026902. Entropy: 0.310335.\n",
      "episode: 5387   score: 330.0  epsilon: 1.0    steps: 768  evaluation reward: 406.25\n",
      "episode: 5388   score: 590.0  epsilon: 1.0    steps: 896  evaluation reward: 409.2\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15208: Policy loss: 0.154604. Value loss: 0.184390. Entropy: 0.307553.\n",
      "Iteration 15209: Policy loss: 0.150313. Value loss: 0.090040. Entropy: 0.304220.\n",
      "Iteration 15210: Policy loss: 0.147232. Value loss: 0.064923. Entropy: 0.305222.\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15211: Policy loss: 0.183257. Value loss: 0.110099. Entropy: 0.307579.\n",
      "Iteration 15212: Policy loss: 0.180130. Value loss: 0.049338. Entropy: 0.309419.\n",
      "Iteration 15213: Policy loss: 0.174744. Value loss: 0.032721. Entropy: 0.309371.\n",
      "episode: 5389   score: 655.0  epsilon: 1.0    steps: 688  evaluation reward: 410.45\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15214: Policy loss: -0.234351. Value loss: 0.310289. Entropy: 0.311639.\n",
      "Iteration 15215: Policy loss: -0.246530. Value loss: 0.121677. Entropy: 0.311441.\n",
      "Iteration 15216: Policy loss: -0.241871. Value loss: 0.064870. Entropy: 0.310902.\n",
      "episode: 5390   score: 340.0  epsilon: 1.0    steps: 752  evaluation reward: 409.35\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15217: Policy loss: -0.128143. Value loss: 0.179478. Entropy: 0.313697.\n",
      "Iteration 15218: Policy loss: -0.125086. Value loss: 0.094388. Entropy: 0.314335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15219: Policy loss: -0.148866. Value loss: 0.068918. Entropy: 0.313655.\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15220: Policy loss: -0.072854. Value loss: 0.277407. Entropy: 0.311807.\n",
      "Iteration 15221: Policy loss: -0.084190. Value loss: 0.078424. Entropy: 0.312498.\n",
      "Iteration 15222: Policy loss: -0.068427. Value loss: 0.054864. Entropy: 0.311365.\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15223: Policy loss: 0.288722. Value loss: 0.150888. Entropy: 0.308394.\n",
      "Iteration 15224: Policy loss: 0.290682. Value loss: 0.049447. Entropy: 0.306189.\n",
      "Iteration 15225: Policy loss: 0.282215. Value loss: 0.030970. Entropy: 0.306628.\n",
      "episode: 5391   score: 500.0  epsilon: 1.0    steps: 168  evaluation reward: 408.6\n",
      "episode: 5392   score: 210.0  epsilon: 1.0    steps: 176  evaluation reward: 408.4\n",
      "episode: 5393   score: 460.0  epsilon: 1.0    steps: 600  evaluation reward: 409.1\n",
      "episode: 5394   score: 535.0  epsilon: 1.0    steps: 968  evaluation reward: 410.55\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15226: Policy loss: 0.127627. Value loss: 0.119542. Entropy: 0.294262.\n",
      "Iteration 15227: Policy loss: 0.130964. Value loss: 0.049474. Entropy: 0.292424.\n",
      "Iteration 15228: Policy loss: 0.125323. Value loss: 0.032812. Entropy: 0.293282.\n",
      "episode: 5395   score: 515.0  epsilon: 1.0    steps: 656  evaluation reward: 410.5\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15229: Policy loss: 0.247827. Value loss: 0.188290. Entropy: 0.302972.\n",
      "Iteration 15230: Policy loss: 0.222968. Value loss: 0.060078. Entropy: 0.303099.\n",
      "Iteration 15231: Policy loss: 0.224711. Value loss: 0.037195. Entropy: 0.303233.\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15232: Policy loss: -0.371329. Value loss: 0.344776. Entropy: 0.312892.\n",
      "Iteration 15233: Policy loss: -0.384229. Value loss: 0.235677. Entropy: 0.314373.\n",
      "Iteration 15234: Policy loss: -0.380298. Value loss: 0.152318. Entropy: 0.313260.\n",
      "episode: 5396   score: 590.0  epsilon: 1.0    steps: 608  evaluation reward: 411.4\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15235: Policy loss: -0.158543. Value loss: 0.351442. Entropy: 0.309050.\n",
      "Iteration 15236: Policy loss: -0.163882. Value loss: 0.120253. Entropy: 0.308988.\n",
      "Iteration 15237: Policy loss: -0.188887. Value loss: 0.083187. Entropy: 0.309118.\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15238: Policy loss: 0.105843. Value loss: 0.137955. Entropy: 0.308599.\n",
      "Iteration 15239: Policy loss: 0.097337. Value loss: 0.062918. Entropy: 0.308167.\n",
      "Iteration 15240: Policy loss: 0.095933. Value loss: 0.041585. Entropy: 0.308282.\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15241: Policy loss: -0.589964. Value loss: 0.398954. Entropy: 0.309411.\n",
      "Iteration 15242: Policy loss: -0.593933. Value loss: 0.235344. Entropy: 0.308985.\n",
      "Iteration 15243: Policy loss: -0.601821. Value loss: 0.185116. Entropy: 0.308044.\n",
      "episode: 5397   score: 410.0  epsilon: 1.0    steps: 408  evaluation reward: 412.05\n",
      "episode: 5398   score: 210.0  epsilon: 1.0    steps: 808  evaluation reward: 411.05\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15244: Policy loss: 0.411051. Value loss: 0.265483. Entropy: 0.300365.\n",
      "Iteration 15245: Policy loss: 0.381253. Value loss: 0.073042. Entropy: 0.300996.\n",
      "Iteration 15246: Policy loss: 0.378874. Value loss: 0.043057. Entropy: 0.298432.\n",
      "episode: 5399   score: 260.0  epsilon: 1.0    steps: 720  evaluation reward: 409.05\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15247: Policy loss: 0.233446. Value loss: 0.410305. Entropy: 0.305515.\n",
      "Iteration 15248: Policy loss: 0.226784. Value loss: 0.176787. Entropy: 0.305656.\n",
      "Iteration 15249: Policy loss: 0.201682. Value loss: 0.110596. Entropy: 0.303449.\n",
      "episode: 5400   score: 560.0  epsilon: 1.0    steps: 144  evaluation reward: 409.75\n",
      "now time :  2019-09-06 06:01:32.924085\n",
      "episode: 5401   score: 420.0  epsilon: 1.0    steps: 824  evaluation reward: 410.1\n",
      "episode: 5402   score: 770.0  epsilon: 1.0    steps: 1024  evaluation reward: 412.5\n",
      "Training network. lr: 0.000133. clip: 0.053312\n",
      "Iteration 15250: Policy loss: 0.163029. Value loss: 0.086714. Entropy: 0.306569.\n",
      "Iteration 15251: Policy loss: 0.154398. Value loss: 0.037739. Entropy: 0.305973.\n",
      "Iteration 15252: Policy loss: 0.145522. Value loss: 0.028968. Entropy: 0.303775.\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15253: Policy loss: 0.389071. Value loss: 0.223383. Entropy: 0.300895.\n",
      "Iteration 15254: Policy loss: 0.393506. Value loss: 0.078215. Entropy: 0.299103.\n",
      "Iteration 15255: Policy loss: 0.384787. Value loss: 0.045879. Entropy: 0.298437.\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15256: Policy loss: -0.084820. Value loss: 0.321576. Entropy: 0.300349.\n",
      "Iteration 15257: Policy loss: -0.070785. Value loss: 0.149724. Entropy: 0.300685.\n",
      "Iteration 15258: Policy loss: -0.079776. Value loss: 0.122004. Entropy: 0.299222.\n",
      "episode: 5403   score: 180.0  epsilon: 1.0    steps: 616  evaluation reward: 410.3\n",
      "episode: 5404   score: 315.0  epsilon: 1.0    steps: 840  evaluation reward: 409.8\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15259: Policy loss: -0.271433. Value loss: 0.274411. Entropy: 0.300343.\n",
      "Iteration 15260: Policy loss: -0.273486. Value loss: 0.162653. Entropy: 0.301201.\n",
      "Iteration 15261: Policy loss: -0.274620. Value loss: 0.119548. Entropy: 0.299695.\n",
      "episode: 5405   score: 340.0  epsilon: 1.0    steps: 8  evaluation reward: 407.35\n",
      "episode: 5406   score: 910.0  epsilon: 1.0    steps: 568  evaluation reward: 414.35\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15262: Policy loss: -0.171025. Value loss: 0.513882. Entropy: 0.302994.\n",
      "Iteration 15263: Policy loss: -0.178905. Value loss: 0.270696. Entropy: 0.303521.\n",
      "Iteration 15264: Policy loss: -0.198797. Value loss: 0.203887. Entropy: 0.304288.\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15265: Policy loss: 0.143314. Value loss: 0.158476. Entropy: 0.298896.\n",
      "Iteration 15266: Policy loss: 0.146664. Value loss: 0.063393. Entropy: 0.296663.\n",
      "Iteration 15267: Policy loss: 0.142259. Value loss: 0.043097. Entropy: 0.298908.\n",
      "episode: 5407   score: 410.0  epsilon: 1.0    steps: 320  evaluation reward: 411.75\n",
      "episode: 5408   score: 285.0  epsilon: 1.0    steps: 1008  evaluation reward: 407.1\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15268: Policy loss: 0.363096. Value loss: 0.295009. Entropy: 0.297552.\n",
      "Iteration 15269: Policy loss: 0.340675. Value loss: 0.113465. Entropy: 0.294609.\n",
      "Iteration 15270: Policy loss: 0.334062. Value loss: 0.067757. Entropy: 0.293558.\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15271: Policy loss: -0.162161. Value loss: 0.324272. Entropy: 0.311705.\n",
      "Iteration 15272: Policy loss: -0.187430. Value loss: 0.249223. Entropy: 0.311253.\n",
      "Iteration 15273: Policy loss: -0.161032. Value loss: 0.174508. Entropy: 0.310695.\n",
      "episode: 5409   score: 515.0  epsilon: 1.0    steps: 144  evaluation reward: 405.25\n",
      "episode: 5410   score: 585.0  epsilon: 1.0    steps: 616  evaluation reward: 404.9\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15274: Policy loss: 0.121832. Value loss: 0.111056. Entropy: 0.285192.\n",
      "Iteration 15275: Policy loss: 0.118739. Value loss: 0.044086. Entropy: 0.282874.\n",
      "Iteration 15276: Policy loss: 0.117649. Value loss: 0.031902. Entropy: 0.282664.\n",
      "episode: 5411   score: 155.0  epsilon: 1.0    steps: 616  evaluation reward: 403.8\n",
      "episode: 5412   score: 510.0  epsilon: 1.0    steps: 632  evaluation reward: 402.4\n",
      "episode: 5413   score: 185.0  epsilon: 1.0    steps: 664  evaluation reward: 400.05\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15277: Policy loss: 0.157339. Value loss: 0.178996. Entropy: 0.284442.\n",
      "Iteration 15278: Policy loss: 0.147566. Value loss: 0.055070. Entropy: 0.283740.\n",
      "Iteration 15279: Policy loss: 0.138291. Value loss: 0.039283. Entropy: 0.283676.\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15280: Policy loss: 0.114908. Value loss: 0.153819. Entropy: 0.307726.\n",
      "Iteration 15281: Policy loss: 0.106319. Value loss: 0.073086. Entropy: 0.308053.\n",
      "Iteration 15282: Policy loss: 0.109984. Value loss: 0.048463. Entropy: 0.307571.\n",
      "episode: 5414   score: 625.0  epsilon: 1.0    steps: 672  evaluation reward: 403.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15283: Policy loss: -0.213277. Value loss: 0.239710. Entropy: 0.291455.\n",
      "Iteration 15284: Policy loss: -0.197392. Value loss: 0.109487. Entropy: 0.292136.\n",
      "Iteration 15285: Policy loss: -0.217446. Value loss: 0.068437. Entropy: 0.292155.\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15286: Policy loss: -0.614220. Value loss: 0.263377. Entropy: 0.306910.\n",
      "Iteration 15287: Policy loss: -0.619158. Value loss: 0.097061. Entropy: 0.309191.\n",
      "Iteration 15288: Policy loss: -0.626119. Value loss: 0.082146. Entropy: 0.308558.\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15289: Policy loss: 0.069080. Value loss: 0.121199. Entropy: 0.300867.\n",
      "Iteration 15290: Policy loss: 0.066692. Value loss: 0.048213. Entropy: 0.301551.\n",
      "Iteration 15291: Policy loss: 0.060643. Value loss: 0.033596. Entropy: 0.300511.\n",
      "episode: 5415   score: 375.0  epsilon: 1.0    steps: 72  evaluation reward: 404.2\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15292: Policy loss: 0.221258. Value loss: 0.172298. Entropy: 0.300657.\n",
      "Iteration 15293: Policy loss: 0.233041. Value loss: 0.082919. Entropy: 0.299829.\n",
      "Iteration 15294: Policy loss: 0.202617. Value loss: 0.085276. Entropy: 0.299655.\n",
      "episode: 5416   score: 320.0  epsilon: 1.0    steps: 248  evaluation reward: 404.4\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15295: Policy loss: 0.431017. Value loss: 0.146187. Entropy: 0.298810.\n",
      "Iteration 15296: Policy loss: 0.426586. Value loss: 0.057515. Entropy: 0.299954.\n",
      "Iteration 15297: Policy loss: 0.420643. Value loss: 0.038272. Entropy: 0.298357.\n",
      "Training network. lr: 0.000133. clip: 0.053155\n",
      "Iteration 15298: Policy loss: -0.537615. Value loss: 0.309809. Entropy: 0.304937.\n",
      "Iteration 15299: Policy loss: -0.537248. Value loss: 0.109030. Entropy: 0.304951.\n",
      "Iteration 15300: Policy loss: -0.547700. Value loss: 0.089900. Entropy: 0.304724.\n",
      "episode: 5417   score: 640.0  epsilon: 1.0    steps: 864  evaluation reward: 407.9\n",
      "episode: 5418   score: 665.0  epsilon: 1.0    steps: 864  evaluation reward: 410.75\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15301: Policy loss: 0.165632. Value loss: 0.148608. Entropy: 0.301292.\n",
      "Iteration 15302: Policy loss: 0.166983. Value loss: 0.062549. Entropy: 0.301646.\n",
      "Iteration 15303: Policy loss: 0.154819. Value loss: 0.046325. Entropy: 0.301521.\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15304: Policy loss: 0.076146. Value loss: 0.114102. Entropy: 0.306993.\n",
      "Iteration 15305: Policy loss: 0.078507. Value loss: 0.046532. Entropy: 0.306895.\n",
      "Iteration 15306: Policy loss: 0.083598. Value loss: 0.031975. Entropy: 0.308587.\n",
      "episode: 5419   score: 210.0  epsilon: 1.0    steps: 712  evaluation reward: 407.95\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15307: Policy loss: -0.388608. Value loss: 0.383759. Entropy: 0.293446.\n",
      "Iteration 15308: Policy loss: -0.403022. Value loss: 0.221418. Entropy: 0.292957.\n",
      "Iteration 15309: Policy loss: -0.414744. Value loss: 0.162045. Entropy: 0.289580.\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15310: Policy loss: 0.089656. Value loss: 0.386890. Entropy: 0.311310.\n",
      "Iteration 15311: Policy loss: 0.092735. Value loss: 0.090913. Entropy: 0.309556.\n",
      "Iteration 15312: Policy loss: 0.092846. Value loss: 0.045411. Entropy: 0.310649.\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15313: Policy loss: 0.302378. Value loss: 0.145379. Entropy: 0.304725.\n",
      "Iteration 15314: Policy loss: 0.300550. Value loss: 0.060161. Entropy: 0.303878.\n",
      "Iteration 15315: Policy loss: 0.294649. Value loss: 0.042038. Entropy: 0.304098.\n",
      "episode: 5420   score: 760.0  epsilon: 1.0    steps: 528  evaluation reward: 413.3\n",
      "episode: 5421   score: 420.0  epsilon: 1.0    steps: 544  evaluation reward: 415.1\n",
      "episode: 5422   score: 670.0  epsilon: 1.0    steps: 848  evaluation reward: 418.3\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15316: Policy loss: 0.245209. Value loss: 0.183047. Entropy: 0.296218.\n",
      "Iteration 15317: Policy loss: 0.242933. Value loss: 0.059549. Entropy: 0.296853.\n",
      "Iteration 15318: Policy loss: 0.233684. Value loss: 0.042277. Entropy: 0.295893.\n",
      "episode: 5423   score: 210.0  epsilon: 1.0    steps: 640  evaluation reward: 417.7\n",
      "episode: 5424   score: 405.0  epsilon: 1.0    steps: 736  evaluation reward: 417.1\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15319: Policy loss: -0.075555. Value loss: 0.082598. Entropy: 0.303059.\n",
      "Iteration 15320: Policy loss: -0.081545. Value loss: 0.041740. Entropy: 0.302316.\n",
      "Iteration 15321: Policy loss: -0.087584. Value loss: 0.031312. Entropy: 0.302525.\n",
      "episode: 5425   score: 975.0  epsilon: 1.0    steps: 64  evaluation reward: 421.5\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15322: Policy loss: -0.074726. Value loss: 0.079548. Entropy: 0.284345.\n",
      "Iteration 15323: Policy loss: -0.069921. Value loss: 0.041654. Entropy: 0.283102.\n",
      "Iteration 15324: Policy loss: -0.074256. Value loss: 0.033138. Entropy: 0.284213.\n",
      "episode: 5426   score: 315.0  epsilon: 1.0    steps: 240  evaluation reward: 421.5\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15325: Policy loss: -0.041446. Value loss: 0.398988. Entropy: 0.291372.\n",
      "Iteration 15326: Policy loss: -0.079193. Value loss: 0.271567. Entropy: 0.288229.\n",
      "Iteration 15327: Policy loss: -0.084814. Value loss: 0.222890. Entropy: 0.287698.\n",
      "episode: 5427   score: 330.0  epsilon: 1.0    steps: 784  evaluation reward: 419.25\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15328: Policy loss: 0.301058. Value loss: 0.106372. Entropy: 0.292126.\n",
      "Iteration 15329: Policy loss: 0.301302. Value loss: 0.047669. Entropy: 0.293669.\n",
      "Iteration 15330: Policy loss: 0.294065. Value loss: 0.037491. Entropy: 0.291759.\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15331: Policy loss: -0.194153. Value loss: 0.318144. Entropy: 0.309122.\n",
      "Iteration 15332: Policy loss: -0.200598. Value loss: 0.228856. Entropy: 0.307678.\n",
      "Iteration 15333: Policy loss: -0.190282. Value loss: 0.179649. Entropy: 0.308582.\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15334: Policy loss: -0.060383. Value loss: 0.095866. Entropy: 0.310429.\n",
      "Iteration 15335: Policy loss: -0.066557. Value loss: 0.044531. Entropy: 0.310436.\n",
      "Iteration 15336: Policy loss: -0.066081. Value loss: 0.033084. Entropy: 0.310739.\n",
      "episode: 5428   score: 185.0  epsilon: 1.0    steps: 112  evaluation reward: 413.85\n",
      "episode: 5429   score: 300.0  epsilon: 1.0    steps: 496  evaluation reward: 413.0\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15337: Policy loss: 0.138149. Value loss: 0.098286. Entropy: 0.300600.\n",
      "Iteration 15338: Policy loss: 0.140069. Value loss: 0.047831. Entropy: 0.301203.\n",
      "Iteration 15339: Policy loss: 0.138093. Value loss: 0.032172. Entropy: 0.301184.\n",
      "episode: 5430   score: 365.0  epsilon: 1.0    steps: 392  evaluation reward: 412.15\n",
      "episode: 5431   score: 530.0  epsilon: 1.0    steps: 952  evaluation reward: 413.8\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15340: Policy loss: -0.112975. Value loss: 0.101228. Entropy: 0.310628.\n",
      "Iteration 15341: Policy loss: -0.114132. Value loss: 0.042932. Entropy: 0.309140.\n",
      "Iteration 15342: Policy loss: -0.121407. Value loss: 0.031882. Entropy: 0.310335.\n",
      "episode: 5432   score: 345.0  epsilon: 1.0    steps: 664  evaluation reward: 414.4\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15343: Policy loss: 0.062655. Value loss: 0.102267. Entropy: 0.302307.\n",
      "Iteration 15344: Policy loss: 0.061735. Value loss: 0.037721. Entropy: 0.303165.\n",
      "Iteration 15345: Policy loss: 0.052781. Value loss: 0.021589. Entropy: 0.304977.\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15346: Policy loss: -0.170940. Value loss: 0.329531. Entropy: 0.305877.\n",
      "Iteration 15347: Policy loss: -0.205617. Value loss: 0.192879. Entropy: 0.304411.\n",
      "Iteration 15348: Policy loss: -0.199176. Value loss: 0.121775. Entropy: 0.304670.\n",
      "episode: 5433   score: 820.0  epsilon: 1.0    steps: 184  evaluation reward: 418.1\n",
      "Training network. lr: 0.000133. clip: 0.053008\n",
      "Iteration 15349: Policy loss: -0.280255. Value loss: 0.235593. Entropy: 0.296711.\n",
      "Iteration 15350: Policy loss: -0.299773. Value loss: 0.126075. Entropy: 0.298678.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15351: Policy loss: -0.293008. Value loss: 0.078268. Entropy: 0.297207.\n",
      "Training network. lr: 0.000132. clip: 0.052851\n",
      "Iteration 15352: Policy loss: 0.057693. Value loss: 0.129597. Entropy: 0.305200.\n",
      "Iteration 15353: Policy loss: 0.053119. Value loss: 0.058373. Entropy: 0.304441.\n",
      "Iteration 15354: Policy loss: 0.046549. Value loss: 0.038958. Entropy: 0.304664.\n",
      "Training network. lr: 0.000132. clip: 0.052851\n",
      "Iteration 15355: Policy loss: -0.744059. Value loss: 0.304182. Entropy: 0.311291.\n",
      "Iteration 15356: Policy loss: -0.749257. Value loss: 0.098901. Entropy: 0.312874.\n",
      "Iteration 15357: Policy loss: -0.754736. Value loss: 0.045535. Entropy: 0.312687.\n",
      "episode: 5434   score: 745.0  epsilon: 1.0    steps: 328  evaluation reward: 422.25\n",
      "episode: 5435   score: 340.0  epsilon: 1.0    steps: 808  evaluation reward: 422.15\n",
      "Training network. lr: 0.000132. clip: 0.052851\n",
      "Iteration 15358: Policy loss: 0.243949. Value loss: 0.191967. Entropy: 0.294596.\n",
      "Iteration 15359: Policy loss: 0.223765. Value loss: 0.065754. Entropy: 0.294463.\n",
      "Iteration 15360: Policy loss: 0.213468. Value loss: 0.037984. Entropy: 0.294864.\n",
      "episode: 5436   score: 590.0  epsilon: 1.0    steps: 336  evaluation reward: 421.05\n",
      "episode: 5437   score: 345.0  epsilon: 1.0    steps: 424  evaluation reward: 420.25\n",
      "Training network. lr: 0.000132. clip: 0.052851\n",
      "Iteration 15361: Policy loss: 0.237047. Value loss: 0.147052. Entropy: 0.292736.\n",
      "Iteration 15362: Policy loss: 0.229492. Value loss: 0.061309. Entropy: 0.293903.\n",
      "Iteration 15363: Policy loss: 0.223816. Value loss: 0.042181. Entropy: 0.291919.\n",
      "episode: 5438   score: 500.0  epsilon: 1.0    steps: 552  evaluation reward: 417.6\n",
      "episode: 5439   score: 290.0  epsilon: 1.0    steps: 760  evaluation reward: 416.7\n",
      "Training network. lr: 0.000132. clip: 0.052851\n",
      "Iteration 15364: Policy loss: 0.250024. Value loss: 0.190947. Entropy: 0.286092.\n",
      "Iteration 15365: Policy loss: 0.236083. Value loss: 0.084098. Entropy: 0.283878.\n",
      "Iteration 15366: Policy loss: 0.242316. Value loss: 0.060891. Entropy: 0.284727.\n",
      "episode: 5440   score: 440.0  epsilon: 1.0    steps: 720  evaluation reward: 417.65\n",
      "Training network. lr: 0.000132. clip: 0.052851\n",
      "Iteration 15367: Policy loss: 0.017470. Value loss: 0.168795. Entropy: 0.290668.\n",
      "Iteration 15368: Policy loss: -0.004488. Value loss: 0.073179. Entropy: 0.289811.\n",
      "Iteration 15369: Policy loss: -0.004372. Value loss: 0.044478. Entropy: 0.290369.\n",
      "episode: 5441   score: 285.0  epsilon: 1.0    steps: 536  evaluation reward: 418.7\n",
      "Training network. lr: 0.000132. clip: 0.052851\n",
      "Iteration 15370: Policy loss: 0.078072. Value loss: 0.085369. Entropy: 0.289516.\n",
      "Iteration 15371: Policy loss: 0.071682. Value loss: 0.042989. Entropy: 0.289665.\n",
      "Iteration 15372: Policy loss: 0.067390. Value loss: 0.033968. Entropy: 0.289452.\n",
      "Training network. lr: 0.000132. clip: 0.052851\n",
      "Iteration 15373: Policy loss: -0.157852. Value loss: 0.094727. Entropy: 0.308989.\n",
      "Iteration 15374: Policy loss: -0.175064. Value loss: 0.035065. Entropy: 0.308336.\n",
      "Iteration 15375: Policy loss: -0.170825. Value loss: 0.024228. Entropy: 0.309466.\n",
      "Training network. lr: 0.000132. clip: 0.052851\n",
      "Iteration 15376: Policy loss: 0.257862. Value loss: 0.141807. Entropy: 0.303696.\n",
      "Iteration 15377: Policy loss: 0.256071. Value loss: 0.045092. Entropy: 0.303962.\n",
      "Iteration 15378: Policy loss: 0.244648. Value loss: 0.032615. Entropy: 0.304792.\n",
      "episode: 5442   score: 220.0  epsilon: 1.0    steps: 464  evaluation reward: 417.9\n",
      "Training network. lr: 0.000132. clip: 0.052851\n",
      "Iteration 15379: Policy loss: -0.282890. Value loss: 0.459386. Entropy: 0.293648.\n",
      "Iteration 15380: Policy loss: -0.306295. Value loss: 0.138694. Entropy: 0.292656.\n",
      "Iteration 15381: Policy loss: -0.326182. Value loss: 0.100121. Entropy: 0.291716.\n",
      "Training network. lr: 0.000132. clip: 0.052851\n",
      "Iteration 15382: Policy loss: -0.092853. Value loss: 0.124630. Entropy: 0.310860.\n",
      "Iteration 15383: Policy loss: -0.096675. Value loss: 0.052496. Entropy: 0.311091.\n",
      "Iteration 15384: Policy loss: -0.094754. Value loss: 0.037058. Entropy: 0.311577.\n",
      "Training network. lr: 0.000132. clip: 0.052851\n",
      "Iteration 15385: Policy loss: 0.008746. Value loss: 0.129648. Entropy: 0.307121.\n",
      "Iteration 15386: Policy loss: 0.002388. Value loss: 0.059856. Entropy: 0.306883.\n",
      "Iteration 15387: Policy loss: -0.003824. Value loss: 0.041185. Entropy: 0.306384.\n",
      "Training network. lr: 0.000132. clip: 0.052851\n",
      "Iteration 15388: Policy loss: 0.466707. Value loss: 0.328194. Entropy: 0.309635.\n",
      "Iteration 15389: Policy loss: 0.453172. Value loss: 0.112603. Entropy: 0.309861.\n",
      "Iteration 15390: Policy loss: 0.455106. Value loss: 0.063816. Entropy: 0.308965.\n",
      "episode: 5443   score: 405.0  epsilon: 1.0    steps: 528  evaluation reward: 419.85\n",
      "episode: 5444   score: 700.0  epsilon: 1.0    steps: 560  evaluation reward: 424.3\n",
      "Training network. lr: 0.000132. clip: 0.052851\n",
      "Iteration 15391: Policy loss: 0.230766. Value loss: 0.156851. Entropy: 0.282474.\n",
      "Iteration 15392: Policy loss: 0.228985. Value loss: 0.062673. Entropy: 0.281269.\n",
      "Iteration 15393: Policy loss: 0.223571. Value loss: 0.051912. Entropy: 0.280689.\n",
      "episode: 5445   score: 470.0  epsilon: 1.0    steps: 240  evaluation reward: 425.85\n",
      "episode: 5446   score: 450.0  epsilon: 1.0    steps: 344  evaluation reward: 424.45\n",
      "Training network. lr: 0.000132. clip: 0.052851\n",
      "Iteration 15394: Policy loss: 0.272661. Value loss: 0.117219. Entropy: 0.284465.\n",
      "Iteration 15395: Policy loss: 0.269255. Value loss: 0.050219. Entropy: 0.285306.\n",
      "Iteration 15396: Policy loss: 0.265315. Value loss: 0.035485. Entropy: 0.285977.\n",
      "episode: 5447   score: 390.0  epsilon: 1.0    steps: 320  evaluation reward: 425.15\n",
      "episode: 5448   score: 420.0  epsilon: 1.0    steps: 976  evaluation reward: 426.15\n",
      "Training network. lr: 0.000132. clip: 0.052851\n",
      "Iteration 15397: Policy loss: -0.078626. Value loss: 0.116893. Entropy: 0.290849.\n",
      "Iteration 15398: Policy loss: -0.083515. Value loss: 0.058861. Entropy: 0.291935.\n",
      "Iteration 15399: Policy loss: -0.080401. Value loss: 0.040304. Entropy: 0.289665.\n",
      "episode: 5449   score: 260.0  epsilon: 1.0    steps: 120  evaluation reward: 420.8\n",
      "episode: 5450   score: 105.0  epsilon: 1.0    steps: 440  evaluation reward: 416.85\n",
      "Training network. lr: 0.000132. clip: 0.052851\n",
      "Iteration 15400: Policy loss: 0.069358. Value loss: 0.172829. Entropy: 0.285024.\n",
      "Iteration 15401: Policy loss: 0.052812. Value loss: 0.080617. Entropy: 0.285766.\n",
      "Iteration 15402: Policy loss: 0.054013. Value loss: 0.062488. Entropy: 0.284208.\n",
      "now time :  2019-09-06 06:11:01.866490\n",
      "episode: 5451   score: 805.0  epsilon: 1.0    steps: 704  evaluation reward: 421.15\n",
      "Training network. lr: 0.000132. clip: 0.052694\n",
      "Iteration 15403: Policy loss: -0.078440. Value loss: 0.183437. Entropy: 0.291425.\n",
      "Iteration 15404: Policy loss: -0.093501. Value loss: 0.070944. Entropy: 0.291075.\n",
      "Iteration 15405: Policy loss: -0.103025. Value loss: 0.039591. Entropy: 0.290440.\n",
      "Training network. lr: 0.000132. clip: 0.052694\n",
      "Iteration 15406: Policy loss: -0.799700. Value loss: 0.463573. Entropy: 0.307136.\n",
      "Iteration 15407: Policy loss: -0.797174. Value loss: 0.141525. Entropy: 0.307580.\n",
      "Iteration 15408: Policy loss: -0.823399. Value loss: 0.067326. Entropy: 0.308705.\n",
      "Training network. lr: 0.000132. clip: 0.052694\n",
      "Iteration 15409: Policy loss: 0.287580. Value loss: 0.258222. Entropy: 0.306477.\n",
      "Iteration 15410: Policy loss: 0.285155. Value loss: 0.065093. Entropy: 0.305338.\n",
      "Iteration 15411: Policy loss: 0.278596. Value loss: 0.039499. Entropy: 0.304582.\n",
      "Training network. lr: 0.000132. clip: 0.052694\n",
      "Iteration 15412: Policy loss: 0.106874. Value loss: 0.149864. Entropy: 0.306066.\n",
      "Iteration 15413: Policy loss: 0.100408. Value loss: 0.070092. Entropy: 0.305875.\n",
      "Iteration 15414: Policy loss: 0.094532. Value loss: 0.051845. Entropy: 0.306010.\n",
      "episode: 5452   score: 515.0  epsilon: 1.0    steps: 312  evaluation reward: 422.25\n",
      "Training network. lr: 0.000132. clip: 0.052694\n",
      "Iteration 15415: Policy loss: -0.320883. Value loss: 0.260162. Entropy: 0.300610.\n",
      "Iteration 15416: Policy loss: -0.322847. Value loss: 0.078125. Entropy: 0.301248.\n",
      "Iteration 15417: Policy loss: -0.337870. Value loss: 0.049961. Entropy: 0.301937.\n",
      "Training network. lr: 0.000132. clip: 0.052694\n",
      "Iteration 15418: Policy loss: -0.183011. Value loss: 0.195614. Entropy: 0.306549.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15419: Policy loss: -0.198061. Value loss: 0.103124. Entropy: 0.306578.\n",
      "Iteration 15420: Policy loss: -0.206274. Value loss: 0.061199. Entropy: 0.306655.\n",
      "Training network. lr: 0.000132. clip: 0.052694\n",
      "Iteration 15421: Policy loss: 0.344864. Value loss: 0.290772. Entropy: 0.312201.\n",
      "Iteration 15422: Policy loss: 0.317901. Value loss: 0.073074. Entropy: 0.311359.\n",
      "Iteration 15423: Policy loss: 0.314260. Value loss: 0.037144. Entropy: 0.311534.\n",
      "episode: 5453   score: 595.0  epsilon: 1.0    steps: 216  evaluation reward: 426.85\n",
      "episode: 5454   score: 640.0  epsilon: 1.0    steps: 744  evaluation reward: 431.0\n",
      "Training network. lr: 0.000132. clip: 0.052694\n",
      "Iteration 15424: Policy loss: 0.185133. Value loss: 0.183689. Entropy: 0.291613.\n",
      "Iteration 15425: Policy loss: 0.192555. Value loss: 0.081595. Entropy: 0.292776.\n",
      "Iteration 15426: Policy loss: 0.187808. Value loss: 0.044845. Entropy: 0.291788.\n",
      "episode: 5455   score: 345.0  epsilon: 1.0    steps: 200  evaluation reward: 429.0\n",
      "episode: 5456   score: 390.0  epsilon: 1.0    steps: 920  evaluation reward: 428.2\n",
      "Training network. lr: 0.000132. clip: 0.052694\n",
      "Iteration 15427: Policy loss: 0.517143. Value loss: 0.242259. Entropy: 0.300001.\n",
      "Iteration 15428: Policy loss: 0.519218. Value loss: 0.092595. Entropy: 0.299774.\n",
      "Iteration 15429: Policy loss: 0.503598. Value loss: 0.055193. Entropy: 0.298332.\n",
      "episode: 5457   score: 420.0  epsilon: 1.0    steps: 64  evaluation reward: 429.45\n",
      "episode: 5458   score: 925.0  epsilon: 1.0    steps: 760  evaluation reward: 431.55\n",
      "Training network. lr: 0.000132. clip: 0.052694\n",
      "Iteration 15430: Policy loss: -0.004809. Value loss: 0.080595. Entropy: 0.290063.\n",
      "Iteration 15431: Policy loss: -0.002230. Value loss: 0.046756. Entropy: 0.288810.\n",
      "Iteration 15432: Policy loss: -0.008802. Value loss: 0.040556. Entropy: 0.288567.\n",
      "episode: 5459   score: 470.0  epsilon: 1.0    steps: 64  evaluation reward: 434.15\n",
      "Training network. lr: 0.000132. clip: 0.052694\n",
      "Iteration 15433: Policy loss: -0.126284. Value loss: 0.155077. Entropy: 0.300152.\n",
      "Iteration 15434: Policy loss: -0.136178. Value loss: 0.066964. Entropy: 0.301698.\n",
      "Iteration 15435: Policy loss: -0.137450. Value loss: 0.040823. Entropy: 0.300910.\n",
      "Training network. lr: 0.000132. clip: 0.052694\n",
      "Iteration 15436: Policy loss: 0.085285. Value loss: 0.134720. Entropy: 0.301782.\n",
      "Iteration 15437: Policy loss: 0.082891. Value loss: 0.056303. Entropy: 0.301559.\n",
      "Iteration 15438: Policy loss: 0.079309. Value loss: 0.039848. Entropy: 0.301684.\n",
      "Training network. lr: 0.000132. clip: 0.052694\n",
      "Iteration 15439: Policy loss: 0.144494. Value loss: 0.095422. Entropy: 0.310488.\n",
      "Iteration 15440: Policy loss: 0.135330. Value loss: 0.050098. Entropy: 0.309719.\n",
      "Iteration 15441: Policy loss: 0.135533. Value loss: 0.039739. Entropy: 0.310762.\n",
      "Training network. lr: 0.000132. clip: 0.052694\n",
      "Iteration 15442: Policy loss: -0.339188. Value loss: 0.337376. Entropy: 0.309996.\n",
      "Iteration 15443: Policy loss: -0.359501. Value loss: 0.233122. Entropy: 0.310085.\n",
      "Iteration 15444: Policy loss: -0.343968. Value loss: 0.154530. Entropy: 0.309762.\n",
      "episode: 5460   score: 315.0  epsilon: 1.0    steps: 488  evaluation reward: 433.5\n",
      "Training network. lr: 0.000132. clip: 0.052694\n",
      "Iteration 15445: Policy loss: -0.345665. Value loss: 0.336968. Entropy: 0.301914.\n",
      "Iteration 15446: Policy loss: -0.380391. Value loss: 0.144823. Entropy: 0.303048.\n",
      "Iteration 15447: Policy loss: -0.375729. Value loss: 0.087249. Entropy: 0.303090.\n",
      "episode: 5461   score: 495.0  epsilon: 1.0    steps: 1016  evaluation reward: 434.7\n",
      "Training network. lr: 0.000132. clip: 0.052694\n",
      "Iteration 15448: Policy loss: 0.015029. Value loss: 0.200346. Entropy: 0.305747.\n",
      "Iteration 15449: Policy loss: 0.014334. Value loss: 0.095275. Entropy: 0.305798.\n",
      "Iteration 15450: Policy loss: 0.012868. Value loss: 0.069761. Entropy: 0.306052.\n",
      "episode: 5462   score: 395.0  epsilon: 1.0    steps: 392  evaluation reward: 433.7\n",
      "episode: 5463   score: 560.0  epsilon: 1.0    steps: 544  evaluation reward: 435.0\n",
      "Training network. lr: 0.000131. clip: 0.052547\n",
      "Iteration 15451: Policy loss: 0.070653. Value loss: 0.225814. Entropy: 0.298835.\n",
      "Iteration 15452: Policy loss: 0.057185. Value loss: 0.092972. Entropy: 0.299582.\n",
      "Iteration 15453: Policy loss: 0.051114. Value loss: 0.057431. Entropy: 0.297750.\n",
      "episode: 5464   score: 360.0  epsilon: 1.0    steps: 184  evaluation reward: 434.9\n",
      "Training network. lr: 0.000131. clip: 0.052547\n",
      "Iteration 15454: Policy loss: 0.465110. Value loss: 0.202925. Entropy: 0.307689.\n",
      "Iteration 15455: Policy loss: 0.451818. Value loss: 0.069523. Entropy: 0.303730.\n",
      "Iteration 15456: Policy loss: 0.435199. Value loss: 0.044153. Entropy: 0.303582.\n",
      "episode: 5465   score: 640.0  epsilon: 1.0    steps: 360  evaluation reward: 436.85\n",
      "Training network. lr: 0.000131. clip: 0.052547\n",
      "Iteration 15457: Policy loss: -0.087437. Value loss: 0.149786. Entropy: 0.294136.\n",
      "Iteration 15458: Policy loss: -0.089322. Value loss: 0.065700. Entropy: 0.295550.\n",
      "Iteration 15459: Policy loss: -0.090490. Value loss: 0.045178. Entropy: 0.294060.\n",
      "Training network. lr: 0.000131. clip: 0.052547\n",
      "Iteration 15460: Policy loss: 0.369659. Value loss: 0.332450. Entropy: 0.310562.\n",
      "Iteration 15461: Policy loss: 0.341787. Value loss: 0.159132. Entropy: 0.308736.\n",
      "Iteration 15462: Policy loss: 0.336778. Value loss: 0.070242. Entropy: 0.308697.\n",
      "episode: 5466   score: 435.0  epsilon: 1.0    steps: 368  evaluation reward: 437.55\n",
      "episode: 5467   score: 555.0  epsilon: 1.0    steps: 864  evaluation reward: 439.8\n",
      "Training network. lr: 0.000131. clip: 0.052547\n",
      "Iteration 15463: Policy loss: 0.213458. Value loss: 0.147266. Entropy: 0.290163.\n",
      "Iteration 15464: Policy loss: 0.209798. Value loss: 0.067742. Entropy: 0.289466.\n",
      "Iteration 15465: Policy loss: 0.222761. Value loss: 0.048541. Entropy: 0.289648.\n",
      "episode: 5468   score: 215.0  epsilon: 1.0    steps: 368  evaluation reward: 439.55\n",
      "Training network. lr: 0.000131. clip: 0.052547\n",
      "Iteration 15466: Policy loss: 0.043506. Value loss: 0.100760. Entropy: 0.295320.\n",
      "Iteration 15467: Policy loss: 0.031699. Value loss: 0.043814. Entropy: 0.297088.\n",
      "Iteration 15468: Policy loss: 0.034791. Value loss: 0.031276. Entropy: 0.297100.\n",
      "episode: 5469   score: 210.0  epsilon: 1.0    steps: 272  evaluation reward: 437.4\n",
      "episode: 5470   score: 455.0  epsilon: 1.0    steps: 280  evaluation reward: 440.9\n",
      "episode: 5471   score: 300.0  epsilon: 1.0    steps: 736  evaluation reward: 441.3\n",
      "Training network. lr: 0.000131. clip: 0.052547\n",
      "Iteration 15469: Policy loss: -0.012369. Value loss: 0.072444. Entropy: 0.282273.\n",
      "Iteration 15470: Policy loss: -0.016745. Value loss: 0.041847. Entropy: 0.279904.\n",
      "Iteration 15471: Policy loss: -0.014831. Value loss: 0.034822. Entropy: 0.281013.\n",
      "episode: 5472   score: 210.0  epsilon: 1.0    steps: 944  evaluation reward: 438.25\n",
      "Training network. lr: 0.000131. clip: 0.052547\n",
      "Iteration 15472: Policy loss: 0.045144. Value loss: 0.094806. Entropy: 0.303294.\n",
      "Iteration 15473: Policy loss: 0.043608. Value loss: 0.042598. Entropy: 0.304122.\n",
      "Iteration 15474: Policy loss: 0.042129. Value loss: 0.028752. Entropy: 0.304447.\n",
      "Training network. lr: 0.000131. clip: 0.052547\n",
      "Iteration 15475: Policy loss: 0.085610. Value loss: 0.081226. Entropy: 0.301520.\n",
      "Iteration 15476: Policy loss: 0.086497. Value loss: 0.033498. Entropy: 0.301542.\n",
      "Iteration 15477: Policy loss: 0.076691. Value loss: 0.025920. Entropy: 0.300926.\n",
      "Training network. lr: 0.000131. clip: 0.052547\n",
      "Iteration 15478: Policy loss: 0.190583. Value loss: 0.163067. Entropy: 0.308118.\n",
      "Iteration 15479: Policy loss: 0.182956. Value loss: 0.071426. Entropy: 0.307503.\n",
      "Iteration 15480: Policy loss: 0.178527. Value loss: 0.049516. Entropy: 0.308341.\n",
      "Training network. lr: 0.000131. clip: 0.052547\n",
      "Iteration 15481: Policy loss: 0.063705. Value loss: 0.095739. Entropy: 0.307873.\n",
      "Iteration 15482: Policy loss: 0.059810. Value loss: 0.035320. Entropy: 0.307193.\n",
      "Iteration 15483: Policy loss: 0.064560. Value loss: 0.021741. Entropy: 0.306995.\n",
      "episode: 5473   score: 285.0  epsilon: 1.0    steps: 288  evaluation reward: 439.3\n",
      "episode: 5474   score: 435.0  epsilon: 1.0    steps: 680  evaluation reward: 441.6\n",
      "Training network. lr: 0.000131. clip: 0.052547\n",
      "Iteration 15484: Policy loss: -0.236323. Value loss: 0.244020. Entropy: 0.289960.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15485: Policy loss: -0.241852. Value loss: 0.081792. Entropy: 0.290229.\n",
      "Iteration 15486: Policy loss: -0.265183. Value loss: 0.052970. Entropy: 0.292873.\n",
      "Training network. lr: 0.000131. clip: 0.052547\n",
      "Iteration 15487: Policy loss: -0.042457. Value loss: 0.088114. Entropy: 0.310595.\n",
      "Iteration 15488: Policy loss: -0.053214. Value loss: 0.044935. Entropy: 0.311107.\n",
      "Iteration 15489: Policy loss: -0.054937. Value loss: 0.028999. Entropy: 0.310477.\n",
      "Training network. lr: 0.000131. clip: 0.052547\n",
      "Iteration 15490: Policy loss: -0.077509. Value loss: 0.211459. Entropy: 0.304412.\n",
      "Iteration 15491: Policy loss: -0.102084. Value loss: 0.059149. Entropy: 0.304617.\n",
      "Iteration 15492: Policy loss: -0.096733. Value loss: 0.033252. Entropy: 0.304951.\n",
      "episode: 5475   score: 545.0  epsilon: 1.0    steps: 8  evaluation reward: 441.0\n",
      "episode: 5476   score: 330.0  epsilon: 1.0    steps: 480  evaluation reward: 439.1\n",
      "episode: 5477   score: 355.0  epsilon: 1.0    steps: 736  evaluation reward: 437.8\n",
      "episode: 5478   score: 365.0  epsilon: 1.0    steps: 984  evaluation reward: 438.45\n",
      "Training network. lr: 0.000131. clip: 0.052547\n",
      "Iteration 15493: Policy loss: 0.534820. Value loss: 0.179273. Entropy: 0.278181.\n",
      "Iteration 15494: Policy loss: 0.521033. Value loss: 0.043409. Entropy: 0.276138.\n",
      "Iteration 15495: Policy loss: 0.515029. Value loss: 0.030534. Entropy: 0.276358.\n",
      "episode: 5479   score: 365.0  epsilon: 1.0    steps: 200  evaluation reward: 436.5\n",
      "Training network. lr: 0.000131. clip: 0.052547\n",
      "Iteration 15496: Policy loss: -0.312138. Value loss: 0.288994. Entropy: 0.294142.\n",
      "Iteration 15497: Policy loss: -0.307827. Value loss: 0.162775. Entropy: 0.296745.\n",
      "Iteration 15498: Policy loss: -0.338627. Value loss: 0.134243. Entropy: 0.296678.\n",
      "Training network. lr: 0.000131. clip: 0.052547\n",
      "Iteration 15499: Policy loss: 0.161760. Value loss: 0.061663. Entropy: 0.301488.\n",
      "Iteration 15500: Policy loss: 0.157673. Value loss: 0.028642. Entropy: 0.301018.\n",
      "Iteration 15501: Policy loss: 0.156028. Value loss: 0.021200. Entropy: 0.302490.\n",
      "episode: 5480   score: 390.0  epsilon: 1.0    steps: 144  evaluation reward: 437.45\n",
      "Training network. lr: 0.000131. clip: 0.052390\n",
      "Iteration 15502: Policy loss: -0.108059. Value loss: 0.107856. Entropy: 0.290236.\n",
      "Iteration 15503: Policy loss: -0.114316. Value loss: 0.036518. Entropy: 0.288052.\n",
      "Iteration 15504: Policy loss: -0.118680. Value loss: 0.023877. Entropy: 0.289408.\n",
      "episode: 5481   score: 285.0  epsilon: 1.0    steps: 368  evaluation reward: 436.55\n",
      "Training network. lr: 0.000131. clip: 0.052390\n",
      "Iteration 15505: Policy loss: 0.486767. Value loss: 0.165760. Entropy: 0.299901.\n",
      "Iteration 15506: Policy loss: 0.476209. Value loss: 0.049593. Entropy: 0.299485.\n",
      "Iteration 15507: Policy loss: 0.461349. Value loss: 0.029979. Entropy: 0.299137.\n",
      "Training network. lr: 0.000131. clip: 0.052390\n",
      "Iteration 15508: Policy loss: 0.124973. Value loss: 0.080676. Entropy: 0.308458.\n",
      "Iteration 15509: Policy loss: 0.119651. Value loss: 0.034399. Entropy: 0.308081.\n",
      "Iteration 15510: Policy loss: 0.115099. Value loss: 0.024727. Entropy: 0.308941.\n",
      "Training network. lr: 0.000131. clip: 0.052390\n",
      "Iteration 15511: Policy loss: -0.133188. Value loss: 0.376484. Entropy: 0.308533.\n",
      "Iteration 15512: Policy loss: -0.137024. Value loss: 0.243055. Entropy: 0.308795.\n",
      "Iteration 15513: Policy loss: -0.152124. Value loss: 0.180578. Entropy: 0.308438.\n",
      "episode: 5482   score: 210.0  epsilon: 1.0    steps: 128  evaluation reward: 436.1\n",
      "Training network. lr: 0.000131. clip: 0.052390\n",
      "Iteration 15514: Policy loss: 0.035238. Value loss: 0.229230. Entropy: 0.301329.\n",
      "Iteration 15515: Policy loss: 0.021270. Value loss: 0.102738. Entropy: 0.300908.\n",
      "Iteration 15516: Policy loss: 0.026619. Value loss: 0.060406. Entropy: 0.300865.\n",
      "episode: 5483   score: 650.0  epsilon: 1.0    steps: 328  evaluation reward: 440.8\n",
      "episode: 5484   score: 330.0  epsilon: 1.0    steps: 416  evaluation reward: 439.9\n",
      "Training network. lr: 0.000131. clip: 0.052390\n",
      "Iteration 15517: Policy loss: 0.094475. Value loss: 0.068890. Entropy: 0.283188.\n",
      "Iteration 15518: Policy loss: 0.088285. Value loss: 0.023992. Entropy: 0.281351.\n",
      "Iteration 15519: Policy loss: 0.089019. Value loss: 0.017112. Entropy: 0.280875.\n",
      "Training network. lr: 0.000131. clip: 0.052390\n",
      "Iteration 15520: Policy loss: 0.025748. Value loss: 0.173425. Entropy: 0.311716.\n",
      "Iteration 15521: Policy loss: 0.029948. Value loss: 0.048170. Entropy: 0.310583.\n",
      "Iteration 15522: Policy loss: 0.014124. Value loss: 0.028084. Entropy: 0.310435.\n",
      "episode: 5485   score: 390.0  epsilon: 1.0    steps: 856  evaluation reward: 437.65\n",
      "episode: 5486   score: 270.0  epsilon: 1.0    steps: 920  evaluation reward: 436.2\n",
      "Training network. lr: 0.000131. clip: 0.052390\n",
      "Iteration 15523: Policy loss: 0.331544. Value loss: 0.111697. Entropy: 0.298053.\n",
      "Iteration 15524: Policy loss: 0.323060. Value loss: 0.045644. Entropy: 0.298054.\n",
      "Iteration 15525: Policy loss: 0.315958. Value loss: 0.034413. Entropy: 0.295936.\n",
      "Training network. lr: 0.000131. clip: 0.052390\n",
      "Iteration 15526: Policy loss: 0.153554. Value loss: 0.146567. Entropy: 0.300437.\n",
      "Iteration 15527: Policy loss: 0.143207. Value loss: 0.054165. Entropy: 0.299791.\n",
      "Iteration 15528: Policy loss: 0.145871. Value loss: 0.030541. Entropy: 0.300784.\n",
      "episode: 5487   score: 450.0  epsilon: 1.0    steps: 136  evaluation reward: 437.4\n",
      "Training network. lr: 0.000131. clip: 0.052390\n",
      "Iteration 15529: Policy loss: -0.156646. Value loss: 0.143225. Entropy: 0.296150.\n",
      "Iteration 15530: Policy loss: -0.159331. Value loss: 0.067911. Entropy: 0.295045.\n",
      "Iteration 15531: Policy loss: -0.157675. Value loss: 0.049847. Entropy: 0.296411.\n",
      "Training network. lr: 0.000131. clip: 0.052390\n",
      "Iteration 15532: Policy loss: 0.145179. Value loss: 0.329288. Entropy: 0.306539.\n",
      "Iteration 15533: Policy loss: 0.140210. Value loss: 0.132372. Entropy: 0.306764.\n",
      "Iteration 15534: Policy loss: 0.127824. Value loss: 0.095379. Entropy: 0.306361.\n",
      "episode: 5488   score: 285.0  epsilon: 1.0    steps: 304  evaluation reward: 434.35\n",
      "episode: 5489   score: 590.0  epsilon: 1.0    steps: 992  evaluation reward: 433.7\n",
      "Training network. lr: 0.000131. clip: 0.052390\n",
      "Iteration 15535: Policy loss: 0.206672. Value loss: 0.201867. Entropy: 0.299785.\n",
      "Iteration 15536: Policy loss: 0.191283. Value loss: 0.077053. Entropy: 0.301114.\n",
      "Iteration 15537: Policy loss: 0.191638. Value loss: 0.046355. Entropy: 0.300090.\n",
      "episode: 5490   score: 520.0  epsilon: 1.0    steps: 128  evaluation reward: 435.5\n",
      "episode: 5491   score: 285.0  epsilon: 1.0    steps: 784  evaluation reward: 433.35\n",
      "Training network. lr: 0.000131. clip: 0.052390\n",
      "Iteration 15538: Policy loss: 0.071820. Value loss: 0.147517. Entropy: 0.283214.\n",
      "Iteration 15539: Policy loss: 0.057965. Value loss: 0.048211. Entropy: 0.283941.\n",
      "Iteration 15540: Policy loss: 0.059001. Value loss: 0.037477. Entropy: 0.283535.\n",
      "Training network. lr: 0.000131. clip: 0.052390\n",
      "Iteration 15541: Policy loss: -0.024172. Value loss: 0.091341. Entropy: 0.310507.\n",
      "Iteration 15542: Policy loss: -0.021750. Value loss: 0.036239. Entropy: 0.310031.\n",
      "Iteration 15543: Policy loss: -0.024182. Value loss: 0.022844. Entropy: 0.310564.\n",
      "Training network. lr: 0.000131. clip: 0.052390\n",
      "Iteration 15544: Policy loss: 0.439031. Value loss: 0.140761. Entropy: 0.315915.\n",
      "Iteration 15545: Policy loss: 0.432703. Value loss: 0.052225. Entropy: 0.315705.\n",
      "Iteration 15546: Policy loss: 0.424538. Value loss: 0.036488. Entropy: 0.314302.\n",
      "episode: 5492   score: 335.0  epsilon: 1.0    steps: 848  evaluation reward: 434.6\n",
      "Training network. lr: 0.000131. clip: 0.052390\n",
      "Iteration 15547: Policy loss: -0.142578. Value loss: 0.351278. Entropy: 0.305971.\n",
      "Iteration 15548: Policy loss: -0.138892. Value loss: 0.199034. Entropy: 0.304635.\n",
      "Iteration 15549: Policy loss: -0.129888. Value loss: 0.116909. Entropy: 0.305181.\n",
      "episode: 5493   score: 695.0  epsilon: 1.0    steps: 480  evaluation reward: 436.95\n",
      "Training network. lr: 0.000131. clip: 0.052390\n",
      "Iteration 15550: Policy loss: -0.011298. Value loss: 0.100321. Entropy: 0.302730.\n",
      "Iteration 15551: Policy loss: -0.021990. Value loss: 0.050076. Entropy: 0.302246.\n",
      "Iteration 15552: Policy loss: -0.029143. Value loss: 0.041747. Entropy: 0.301937.\n",
      "episode: 5494   score: 395.0  epsilon: 1.0    steps: 160  evaluation reward: 435.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000131. clip: 0.052233\n",
      "Iteration 15553: Policy loss: 0.108291. Value loss: 0.148678. Entropy: 0.304753.\n",
      "Iteration 15554: Policy loss: 0.115419. Value loss: 0.059515. Entropy: 0.305611.\n",
      "Iteration 15555: Policy loss: 0.110543. Value loss: 0.039061. Entropy: 0.305036.\n",
      "episode: 5495   score: 695.0  epsilon: 1.0    steps: 520  evaluation reward: 437.35\n",
      "Training network. lr: 0.000131. clip: 0.052233\n",
      "Iteration 15556: Policy loss: 0.068017. Value loss: 0.098190. Entropy: 0.302956.\n",
      "Iteration 15557: Policy loss: 0.064385. Value loss: 0.044004. Entropy: 0.303525.\n",
      "Iteration 15558: Policy loss: 0.064126. Value loss: 0.034116. Entropy: 0.303973.\n",
      "episode: 5496   score: 240.0  epsilon: 1.0    steps: 232  evaluation reward: 433.85\n",
      "episode: 5497   score: 345.0  epsilon: 1.0    steps: 520  evaluation reward: 433.2\n",
      "Training network. lr: 0.000131. clip: 0.052233\n",
      "Iteration 15559: Policy loss: 0.152436. Value loss: 0.244467. Entropy: 0.293024.\n",
      "Iteration 15560: Policy loss: 0.153590. Value loss: 0.152405. Entropy: 0.292189.\n",
      "Iteration 15561: Policy loss: 0.147408. Value loss: 0.134613. Entropy: 0.291172.\n",
      "Training network. lr: 0.000131. clip: 0.052233\n",
      "Iteration 15562: Policy loss: 0.038586. Value loss: 0.067514. Entropy: 0.310512.\n",
      "Iteration 15563: Policy loss: 0.039626. Value loss: 0.033796. Entropy: 0.310648.\n",
      "Iteration 15564: Policy loss: 0.036089. Value loss: 0.027700. Entropy: 0.310176.\n",
      "episode: 5498   score: 180.0  epsilon: 1.0    steps: 448  evaluation reward: 432.9\n",
      "Training network. lr: 0.000131. clip: 0.052233\n",
      "Iteration 15565: Policy loss: 0.238455. Value loss: 0.121147. Entropy: 0.296708.\n",
      "Iteration 15566: Policy loss: 0.228324. Value loss: 0.054943. Entropy: 0.297667.\n",
      "Iteration 15567: Policy loss: 0.230289. Value loss: 0.037461. Entropy: 0.295781.\n",
      "Training network. lr: 0.000131. clip: 0.052233\n",
      "Iteration 15568: Policy loss: -0.460232. Value loss: 0.331944. Entropy: 0.312167.\n",
      "Iteration 15569: Policy loss: -0.465565. Value loss: 0.202904. Entropy: 0.312540.\n",
      "Iteration 15570: Policy loss: -0.478104. Value loss: 0.150302. Entropy: 0.312703.\n",
      "Training network. lr: 0.000131. clip: 0.052233\n",
      "Iteration 15571: Policy loss: 0.211908. Value loss: 0.131676. Entropy: 0.307176.\n",
      "Iteration 15572: Policy loss: 0.203578. Value loss: 0.045089. Entropy: 0.307001.\n",
      "Iteration 15573: Policy loss: 0.198375. Value loss: 0.028738. Entropy: 0.307776.\n",
      "episode: 5499   score: 465.0  epsilon: 1.0    steps: 80  evaluation reward: 434.95\n",
      "episode: 5500   score: 285.0  epsilon: 1.0    steps: 208  evaluation reward: 432.2\n",
      "Training network. lr: 0.000131. clip: 0.052233\n",
      "Iteration 15574: Policy loss: -0.089591. Value loss: 0.102707. Entropy: 0.303323.\n",
      "Iteration 15575: Policy loss: -0.098424. Value loss: 0.052502. Entropy: 0.303233.\n",
      "Iteration 15576: Policy loss: -0.096583. Value loss: 0.037661. Entropy: 0.304231.\n",
      "now time :  2019-09-06 06:21:47.557375\n",
      "episode: 5501   score: 725.0  epsilon: 1.0    steps: 392  evaluation reward: 435.25\n",
      "Training network. lr: 0.000131. clip: 0.052233\n",
      "Iteration 15577: Policy loss: -0.000730. Value loss: 0.127727. Entropy: 0.303861.\n",
      "Iteration 15578: Policy loss: -0.012407. Value loss: 0.060017. Entropy: 0.304590.\n",
      "Iteration 15579: Policy loss: -0.015075. Value loss: 0.044259. Entropy: 0.304662.\n",
      "episode: 5502   score: 390.0  epsilon: 1.0    steps: 256  evaluation reward: 431.45\n",
      "Training network. lr: 0.000131. clip: 0.052233\n",
      "Iteration 15580: Policy loss: 0.086073. Value loss: 0.082976. Entropy: 0.303986.\n",
      "Iteration 15581: Policy loss: 0.080061. Value loss: 0.036367. Entropy: 0.305299.\n",
      "Iteration 15582: Policy loss: 0.077313. Value loss: 0.025813. Entropy: 0.305271.\n",
      "episode: 5503   score: 390.0  epsilon: 1.0    steps: 392  evaluation reward: 433.55\n",
      "Training network. lr: 0.000131. clip: 0.052233\n",
      "Iteration 15583: Policy loss: 0.099531. Value loss: 0.078019. Entropy: 0.300314.\n",
      "Iteration 15584: Policy loss: 0.094886. Value loss: 0.033042. Entropy: 0.298598.\n",
      "Iteration 15585: Policy loss: 0.096511. Value loss: 0.023754. Entropy: 0.298081.\n",
      "Training network. lr: 0.000131. clip: 0.052233\n",
      "Iteration 15586: Policy loss: 0.136212. Value loss: 0.066903. Entropy: 0.308107.\n",
      "Iteration 15587: Policy loss: 0.129084. Value loss: 0.027585. Entropy: 0.308654.\n",
      "Iteration 15588: Policy loss: 0.130621. Value loss: 0.018034. Entropy: 0.308294.\n",
      "episode: 5504   score: 365.0  epsilon: 1.0    steps: 576  evaluation reward: 434.05\n",
      "episode: 5505   score: 765.0  epsilon: 1.0    steps: 752  evaluation reward: 438.3\n",
      "Training network. lr: 0.000131. clip: 0.052233\n",
      "Iteration 15589: Policy loss: -0.206446. Value loss: 0.239915. Entropy: 0.296744.\n",
      "Iteration 15590: Policy loss: -0.203151. Value loss: 0.121545. Entropy: 0.292286.\n",
      "Iteration 15591: Policy loss: -0.242817. Value loss: 0.084672. Entropy: 0.294341.\n",
      "episode: 5506   score: 570.0  epsilon: 1.0    steps: 496  evaluation reward: 434.9\n",
      "Training network. lr: 0.000131. clip: 0.052233\n",
      "Iteration 15592: Policy loss: 0.014439. Value loss: 0.085069. Entropy: 0.300698.\n",
      "Iteration 15593: Policy loss: 0.010627. Value loss: 0.033961. Entropy: 0.298109.\n",
      "Iteration 15594: Policy loss: 0.008866. Value loss: 0.024035. Entropy: 0.298228.\n",
      "Training network. lr: 0.000131. clip: 0.052233\n",
      "Iteration 15595: Policy loss: -0.432167. Value loss: 0.414422. Entropy: 0.308191.\n",
      "Iteration 15596: Policy loss: -0.471190. Value loss: 0.243226. Entropy: 0.308580.\n",
      "Iteration 15597: Policy loss: -0.476622. Value loss: 0.185094. Entropy: 0.307591.\n",
      "episode: 5507   score: 460.0  epsilon: 1.0    steps: 760  evaluation reward: 435.4\n",
      "Training network. lr: 0.000131. clip: 0.052233\n",
      "Iteration 15598: Policy loss: 0.356805. Value loss: 0.268094. Entropy: 0.298951.\n",
      "Iteration 15599: Policy loss: 0.339588. Value loss: 0.091681. Entropy: 0.298813.\n",
      "Iteration 15600: Policy loss: 0.336309. Value loss: 0.056843. Entropy: 0.299438.\n",
      "Training network. lr: 0.000130. clip: 0.052086\n",
      "Iteration 15601: Policy loss: 0.400905. Value loss: 0.105227. Entropy: 0.309469.\n",
      "Iteration 15602: Policy loss: 0.390397. Value loss: 0.031761. Entropy: 0.309727.\n",
      "Iteration 15603: Policy loss: 0.389968. Value loss: 0.020459. Entropy: 0.310044.\n",
      "episode: 5508   score: 460.0  epsilon: 1.0    steps: 272  evaluation reward: 437.15\n",
      "Training network. lr: 0.000130. clip: 0.052086\n",
      "Iteration 15604: Policy loss: 0.093244. Value loss: 0.160615. Entropy: 0.297682.\n",
      "Iteration 15605: Policy loss: 0.086750. Value loss: 0.061608. Entropy: 0.299598.\n",
      "Iteration 15606: Policy loss: 0.085567. Value loss: 0.037271. Entropy: 0.298689.\n",
      "episode: 5509   score: 345.0  epsilon: 1.0    steps: 320  evaluation reward: 435.45\n",
      "episode: 5510   score: 520.0  epsilon: 1.0    steps: 952  evaluation reward: 434.8\n",
      "Training network. lr: 0.000130. clip: 0.052086\n",
      "Iteration 15607: Policy loss: 0.020102. Value loss: 0.070276. Entropy: 0.289952.\n",
      "Iteration 15608: Policy loss: 0.017477. Value loss: 0.036523. Entropy: 0.289205.\n",
      "Iteration 15609: Policy loss: 0.010640. Value loss: 0.031720. Entropy: 0.290657.\n",
      "episode: 5511   score: 575.0  epsilon: 1.0    steps: 744  evaluation reward: 439.0\n",
      "Training network. lr: 0.000130. clip: 0.052086\n",
      "Iteration 15610: Policy loss: 0.202798. Value loss: 0.124264. Entropy: 0.297357.\n",
      "Iteration 15611: Policy loss: 0.196279. Value loss: 0.062644. Entropy: 0.298132.\n",
      "Iteration 15612: Policy loss: 0.187634. Value loss: 0.040379. Entropy: 0.297959.\n",
      "episode: 5512   score: 465.0  epsilon: 1.0    steps: 656  evaluation reward: 438.55\n",
      "episode: 5513   score: 230.0  epsilon: 1.0    steps: 896  evaluation reward: 439.0\n",
      "Training network. lr: 0.000130. clip: 0.052086\n",
      "Iteration 15613: Policy loss: 0.054888. Value loss: 0.083363. Entropy: 0.297219.\n",
      "Iteration 15614: Policy loss: 0.048749. Value loss: 0.045404. Entropy: 0.296094.\n",
      "Iteration 15615: Policy loss: 0.047090. Value loss: 0.033153. Entropy: 0.295670.\n",
      "episode: 5514   score: 290.0  epsilon: 1.0    steps: 256  evaluation reward: 435.65\n",
      "Training network. lr: 0.000130. clip: 0.052086\n",
      "Iteration 15616: Policy loss: -0.106153. Value loss: 0.369037. Entropy: 0.297540.\n",
      "Iteration 15617: Policy loss: -0.098447. Value loss: 0.251351. Entropy: 0.296115.\n",
      "Iteration 15618: Policy loss: -0.102827. Value loss: 0.218375. Entropy: 0.296164.\n",
      "Training network. lr: 0.000130. clip: 0.052086\n",
      "Iteration 15619: Policy loss: -0.050834. Value loss: 0.097721. Entropy: 0.307676.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15620: Policy loss: -0.049102. Value loss: 0.042995. Entropy: 0.306068.\n",
      "Iteration 15621: Policy loss: -0.057743. Value loss: 0.030907. Entropy: 0.307009.\n",
      "Training network. lr: 0.000130. clip: 0.052086\n",
      "Iteration 15622: Policy loss: -0.006266. Value loss: 0.098679. Entropy: 0.308823.\n",
      "Iteration 15623: Policy loss: -0.019003. Value loss: 0.048519. Entropy: 0.308508.\n",
      "Iteration 15624: Policy loss: -0.011752. Value loss: 0.036316. Entropy: 0.309180.\n",
      "episode: 5515   score: 410.0  epsilon: 1.0    steps: 16  evaluation reward: 436.0\n",
      "Training network. lr: 0.000130. clip: 0.052086\n",
      "Iteration 15625: Policy loss: 0.233843. Value loss: 0.118048. Entropy: 0.295816.\n",
      "Iteration 15626: Policy loss: 0.222014. Value loss: 0.046530. Entropy: 0.294417.\n",
      "Iteration 15627: Policy loss: 0.227889. Value loss: 0.034543. Entropy: 0.295058.\n",
      "Training network. lr: 0.000130. clip: 0.052086\n",
      "Iteration 15628: Policy loss: 0.156005. Value loss: 0.092098. Entropy: 0.308766.\n",
      "Iteration 15629: Policy loss: 0.141902. Value loss: 0.039342. Entropy: 0.308762.\n",
      "Iteration 15630: Policy loss: 0.143252. Value loss: 0.028342. Entropy: 0.308304.\n",
      "Training network. lr: 0.000130. clip: 0.052086\n",
      "Iteration 15631: Policy loss: -0.237190. Value loss: 0.286724. Entropy: 0.312894.\n",
      "Iteration 15632: Policy loss: -0.258783. Value loss: 0.130277. Entropy: 0.313913.\n",
      "Iteration 15633: Policy loss: -0.272122. Value loss: 0.068947. Entropy: 0.311750.\n",
      "episode: 5516   score: 345.0  epsilon: 1.0    steps: 928  evaluation reward: 436.25\n",
      "Training network. lr: 0.000130. clip: 0.052086\n",
      "Iteration 15634: Policy loss: 0.153540. Value loss: 0.262811. Entropy: 0.307077.\n",
      "Iteration 15635: Policy loss: 0.128547. Value loss: 0.132948. Entropy: 0.306287.\n",
      "Iteration 15636: Policy loss: 0.123755. Value loss: 0.083489. Entropy: 0.305785.\n",
      "episode: 5517   score: 500.0  epsilon: 1.0    steps: 152  evaluation reward: 434.85\n",
      "episode: 5518   score: 460.0  epsilon: 1.0    steps: 256  evaluation reward: 432.8\n",
      "episode: 5519   score: 315.0  epsilon: 1.0    steps: 336  evaluation reward: 433.85\n",
      "episode: 5520   score: 405.0  epsilon: 1.0    steps: 608  evaluation reward: 430.3\n",
      "Training network. lr: 0.000130. clip: 0.052086\n",
      "Iteration 15637: Policy loss: 0.105965. Value loss: 0.078711. Entropy: 0.250589.\n",
      "Iteration 15638: Policy loss: 0.110643. Value loss: 0.040412. Entropy: 0.248735.\n",
      "Iteration 15639: Policy loss: 0.107454. Value loss: 0.028943. Entropy: 0.249012.\n",
      "episode: 5521   score: 870.0  epsilon: 1.0    steps: 928  evaluation reward: 434.8\n",
      "Training network. lr: 0.000130. clip: 0.052086\n",
      "Iteration 15640: Policy loss: 0.028851. Value loss: 0.283874. Entropy: 0.303846.\n",
      "Iteration 15641: Policy loss: 0.012148. Value loss: 0.089765. Entropy: 0.303321.\n",
      "Iteration 15642: Policy loss: 0.003000. Value loss: 0.045307. Entropy: 0.304572.\n",
      "episode: 5522   score: 365.0  epsilon: 1.0    steps: 744  evaluation reward: 431.75\n",
      "Training network. lr: 0.000130. clip: 0.052086\n",
      "Iteration 15643: Policy loss: 0.135718. Value loss: 0.083029. Entropy: 0.286865.\n",
      "Iteration 15644: Policy loss: 0.138966. Value loss: 0.040638. Entropy: 0.289493.\n",
      "Iteration 15645: Policy loss: 0.129562. Value loss: 0.030840. Entropy: 0.288179.\n",
      "Training network. lr: 0.000130. clip: 0.052086\n",
      "Iteration 15646: Policy loss: 0.116149. Value loss: 0.108627. Entropy: 0.308987.\n",
      "Iteration 15647: Policy loss: 0.124428. Value loss: 0.032643. Entropy: 0.308052.\n",
      "Iteration 15648: Policy loss: 0.112692. Value loss: 0.021266. Entropy: 0.307175.\n",
      "Training network. lr: 0.000130. clip: 0.052086\n",
      "Iteration 15649: Policy loss: -0.051005. Value loss: 0.074011. Entropy: 0.307394.\n",
      "Iteration 15650: Policy loss: -0.046154. Value loss: 0.041837. Entropy: 0.307102.\n",
      "Iteration 15651: Policy loss: -0.056918. Value loss: 0.029128. Entropy: 0.307111.\n",
      "Training network. lr: 0.000130. clip: 0.051929\n",
      "Iteration 15652: Policy loss: -0.057538. Value loss: 0.324937. Entropy: 0.312874.\n",
      "Iteration 15653: Policy loss: -0.067408. Value loss: 0.132802. Entropy: 0.310790.\n",
      "Iteration 15654: Policy loss: -0.077913. Value loss: 0.060389. Entropy: 0.311525.\n",
      "Training network. lr: 0.000130. clip: 0.051929\n",
      "Iteration 15655: Policy loss: 0.128373. Value loss: 0.150079. Entropy: 0.312259.\n",
      "Iteration 15656: Policy loss: 0.118099. Value loss: 0.058902. Entropy: 0.312236.\n",
      "Iteration 15657: Policy loss: 0.115533. Value loss: 0.033936. Entropy: 0.310992.\n",
      "episode: 5523   score: 285.0  epsilon: 1.0    steps: 8  evaluation reward: 432.5\n",
      "episode: 5524   score: 330.0  epsilon: 1.0    steps: 648  evaluation reward: 431.75\n",
      "episode: 5525   score: 375.0  epsilon: 1.0    steps: 800  evaluation reward: 425.75\n",
      "Training network. lr: 0.000130. clip: 0.051929\n",
      "Iteration 15658: Policy loss: -0.231454. Value loss: 0.323278. Entropy: 0.279809.\n",
      "Iteration 15659: Policy loss: -0.239478. Value loss: 0.238808. Entropy: 0.276893.\n",
      "Iteration 15660: Policy loss: -0.239379. Value loss: 0.206812. Entropy: 0.277001.\n",
      "episode: 5526   score: 430.0  epsilon: 1.0    steps: 480  evaluation reward: 426.9\n",
      "Training network. lr: 0.000130. clip: 0.051929\n",
      "Iteration 15661: Policy loss: 0.104686. Value loss: 0.115080. Entropy: 0.297564.\n",
      "Iteration 15662: Policy loss: 0.091808. Value loss: 0.046623. Entropy: 0.297388.\n",
      "Iteration 15663: Policy loss: 0.095122. Value loss: 0.030857. Entropy: 0.296071.\n",
      "Training network. lr: 0.000130. clip: 0.051929\n",
      "Iteration 15664: Policy loss: -0.146886. Value loss: 0.330072. Entropy: 0.306711.\n",
      "Iteration 15665: Policy loss: -0.171163. Value loss: 0.223157. Entropy: 0.305852.\n",
      "Iteration 15666: Policy loss: -0.181263. Value loss: 0.155544. Entropy: 0.306204.\n",
      "episode: 5527   score: 260.0  epsilon: 1.0    steps: 432  evaluation reward: 426.2\n",
      "episode: 5528   score: 565.0  epsilon: 1.0    steps: 936  evaluation reward: 430.0\n",
      "Training network. lr: 0.000130. clip: 0.051929\n",
      "Iteration 15667: Policy loss: -0.183096. Value loss: 0.187329. Entropy: 0.296042.\n",
      "Iteration 15668: Policy loss: -0.185392. Value loss: 0.082405. Entropy: 0.297052.\n",
      "Iteration 15669: Policy loss: -0.188339. Value loss: 0.052228. Entropy: 0.296255.\n",
      "episode: 5529   score: 820.0  epsilon: 1.0    steps: 632  evaluation reward: 435.2\n",
      "Training network. lr: 0.000130. clip: 0.051929\n",
      "Iteration 15670: Policy loss: 0.236910. Value loss: 0.092228. Entropy: 0.288717.\n",
      "Iteration 15671: Policy loss: 0.224892. Value loss: 0.029683. Entropy: 0.288809.\n",
      "Iteration 15672: Policy loss: 0.228703. Value loss: 0.019119. Entropy: 0.289555.\n",
      "Training network. lr: 0.000130. clip: 0.051929\n",
      "Iteration 15673: Policy loss: -0.278559. Value loss: 0.347256. Entropy: 0.312060.\n",
      "Iteration 15674: Policy loss: -0.290080. Value loss: 0.124462. Entropy: 0.312492.\n",
      "Iteration 15675: Policy loss: -0.311482. Value loss: 0.072545. Entropy: 0.313090.\n",
      "episode: 5530   score: 240.0  epsilon: 1.0    steps: 144  evaluation reward: 433.95\n",
      "Training network. lr: 0.000130. clip: 0.051929\n",
      "Iteration 15676: Policy loss: 0.233497. Value loss: 0.102808. Entropy: 0.294892.\n",
      "Iteration 15677: Policy loss: 0.236464. Value loss: 0.032348. Entropy: 0.294087.\n",
      "Iteration 15678: Policy loss: 0.220203. Value loss: 0.023587. Entropy: 0.295123.\n",
      "Training network. lr: 0.000130. clip: 0.051929\n",
      "Iteration 15679: Policy loss: 0.087808. Value loss: 0.161119. Entropy: 0.309841.\n",
      "Iteration 15680: Policy loss: 0.090536. Value loss: 0.074231. Entropy: 0.309981.\n",
      "Iteration 15681: Policy loss: 0.089775. Value loss: 0.049512. Entropy: 0.310459.\n",
      "episode: 5531   score: 285.0  epsilon: 1.0    steps: 56  evaluation reward: 431.5\n",
      "episode: 5532   score: 755.0  epsilon: 1.0    steps: 200  evaluation reward: 435.6\n",
      "Training network. lr: 0.000130. clip: 0.051929\n",
      "Iteration 15682: Policy loss: -0.007500. Value loss: 0.115313. Entropy: 0.287556.\n",
      "Iteration 15683: Policy loss: -0.014385. Value loss: 0.049607. Entropy: 0.291287.\n",
      "Iteration 15684: Policy loss: -0.013275. Value loss: 0.035604. Entropy: 0.290548.\n",
      "Training network. lr: 0.000130. clip: 0.051929\n",
      "Iteration 15685: Policy loss: -0.047509. Value loss: 0.081716. Entropy: 0.307671.\n",
      "Iteration 15686: Policy loss: -0.054531. Value loss: 0.035118. Entropy: 0.306901.\n",
      "Iteration 15687: Policy loss: -0.057506. Value loss: 0.026476. Entropy: 0.306382.\n",
      "Training network. lr: 0.000130. clip: 0.051929\n",
      "Iteration 15688: Policy loss: -0.302654. Value loss: 0.376130. Entropy: 0.308834.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15689: Policy loss: -0.306114. Value loss: 0.108616. Entropy: 0.308997.\n",
      "Iteration 15690: Policy loss: -0.327061. Value loss: 0.066293. Entropy: 0.309892.\n",
      "episode: 5533   score: 590.0  epsilon: 1.0    steps: 536  evaluation reward: 433.3\n",
      "episode: 5534   score: 620.0  epsilon: 1.0    steps: 960  evaluation reward: 432.05\n",
      "episode: 5535   score: 330.0  epsilon: 1.0    steps: 1016  evaluation reward: 431.95\n",
      "Training network. lr: 0.000130. clip: 0.051929\n",
      "Iteration 15691: Policy loss: 0.251071. Value loss: 0.103187. Entropy: 0.296618.\n",
      "Iteration 15692: Policy loss: 0.237618. Value loss: 0.037476. Entropy: 0.295409.\n",
      "Iteration 15693: Policy loss: 0.233109. Value loss: 0.026376. Entropy: 0.294563.\n",
      "episode: 5536   score: 330.0  epsilon: 1.0    steps: 152  evaluation reward: 429.35\n",
      "Training network. lr: 0.000130. clip: 0.051929\n",
      "Iteration 15694: Policy loss: -0.014591. Value loss: 0.109814. Entropy: 0.288101.\n",
      "Iteration 15695: Policy loss: -0.028033. Value loss: 0.059136. Entropy: 0.286493.\n",
      "Iteration 15696: Policy loss: -0.025774. Value loss: 0.039765. Entropy: 0.288469.\n",
      "episode: 5537   score: 255.0  epsilon: 1.0    steps: 96  evaluation reward: 428.45\n",
      "episode: 5538   score: 315.0  epsilon: 1.0    steps: 264  evaluation reward: 426.6\n",
      "Training network. lr: 0.000130. clip: 0.051929\n",
      "Iteration 15697: Policy loss: 0.130485. Value loss: 0.138868. Entropy: 0.291114.\n",
      "Iteration 15698: Policy loss: 0.130195. Value loss: 0.074705. Entropy: 0.290658.\n",
      "Iteration 15699: Policy loss: 0.130582. Value loss: 0.061226. Entropy: 0.290633.\n",
      "Training network. lr: 0.000130. clip: 0.051929\n",
      "Iteration 15700: Policy loss: -0.112619. Value loss: 0.062604. Entropy: 0.305478.\n",
      "Iteration 15701: Policy loss: -0.119537. Value loss: 0.030283. Entropy: 0.306258.\n",
      "Iteration 15702: Policy loss: -0.123619. Value loss: 0.023473. Entropy: 0.305889.\n",
      "Training network. lr: 0.000129. clip: 0.051773\n",
      "Iteration 15703: Policy loss: 0.211440. Value loss: 0.074165. Entropy: 0.306195.\n",
      "Iteration 15704: Policy loss: 0.201185. Value loss: 0.028805. Entropy: 0.304547.\n",
      "Iteration 15705: Policy loss: 0.198322. Value loss: 0.017538. Entropy: 0.305297.\n",
      "episode: 5539   score: 410.0  epsilon: 1.0    steps: 112  evaluation reward: 427.8\n",
      "Training network. lr: 0.000129. clip: 0.051773\n",
      "Iteration 15706: Policy loss: -0.412575. Value loss: 0.504356. Entropy: 0.298582.\n",
      "Iteration 15707: Policy loss: -0.411218. Value loss: 0.206955. Entropy: 0.297527.\n",
      "Iteration 15708: Policy loss: -0.403579. Value loss: 0.090997. Entropy: 0.296346.\n",
      "Training network. lr: 0.000129. clip: 0.051773\n",
      "Iteration 15709: Policy loss: 0.152231. Value loss: 0.247362. Entropy: 0.306771.\n",
      "Iteration 15710: Policy loss: 0.133342. Value loss: 0.083922. Entropy: 0.307329.\n",
      "Iteration 15711: Policy loss: 0.139256. Value loss: 0.044596. Entropy: 0.306469.\n",
      "episode: 5540   score: 410.0  epsilon: 1.0    steps: 416  evaluation reward: 427.5\n",
      "Training network. lr: 0.000129. clip: 0.051773\n",
      "Iteration 15712: Policy loss: 0.261836. Value loss: 0.191591. Entropy: 0.292749.\n",
      "Iteration 15713: Policy loss: 0.238357. Value loss: 0.073791. Entropy: 0.290962.\n",
      "Iteration 15714: Policy loss: 0.229381. Value loss: 0.049606. Entropy: 0.290807.\n",
      "episode: 5541   score: 365.0  epsilon: 1.0    steps: 128  evaluation reward: 428.3\n",
      "episode: 5542   score: 390.0  epsilon: 1.0    steps: 192  evaluation reward: 430.0\n",
      "episode: 5543   score: 440.0  epsilon: 1.0    steps: 328  evaluation reward: 430.35\n",
      "episode: 5544   score: 265.0  epsilon: 1.0    steps: 872  evaluation reward: 426.0\n",
      "Training network. lr: 0.000129. clip: 0.051773\n",
      "Iteration 15715: Policy loss: -0.091926. Value loss: 0.130313. Entropy: 0.264653.\n",
      "Iteration 15716: Policy loss: -0.105882. Value loss: 0.068860. Entropy: 0.264094.\n",
      "Iteration 15717: Policy loss: -0.097953. Value loss: 0.053242. Entropy: 0.262939.\n",
      "Training network. lr: 0.000129. clip: 0.051773\n",
      "Iteration 15718: Policy loss: 0.144370. Value loss: 0.106735. Entropy: 0.304751.\n",
      "Iteration 15719: Policy loss: 0.142500. Value loss: 0.050200. Entropy: 0.304758.\n",
      "Iteration 15720: Policy loss: 0.143987. Value loss: 0.035740. Entropy: 0.303526.\n",
      "episode: 5545   score: 300.0  epsilon: 1.0    steps: 192  evaluation reward: 424.3\n",
      "Training network. lr: 0.000129. clip: 0.051773\n",
      "Iteration 15721: Policy loss: -0.208707. Value loss: 0.260902. Entropy: 0.296539.\n",
      "Iteration 15722: Policy loss: -0.210319. Value loss: 0.090645. Entropy: 0.294496.\n",
      "Iteration 15723: Policy loss: -0.220744. Value loss: 0.053569. Entropy: 0.294150.\n",
      "episode: 5546   score: 210.0  epsilon: 1.0    steps: 8  evaluation reward: 421.9\n",
      "Training network. lr: 0.000129. clip: 0.051773\n",
      "Iteration 15724: Policy loss: -0.042341. Value loss: 0.341221. Entropy: 0.295946.\n",
      "Iteration 15725: Policy loss: -0.089253. Value loss: 0.133978. Entropy: 0.296419.\n",
      "Iteration 15726: Policy loss: -0.099664. Value loss: 0.084006. Entropy: 0.295261.\n",
      "Training network. lr: 0.000129. clip: 0.051773\n",
      "Iteration 15727: Policy loss: -0.007467. Value loss: 0.528472. Entropy: 0.304794.\n",
      "Iteration 15728: Policy loss: -0.011038. Value loss: 0.129274. Entropy: 0.303874.\n",
      "Iteration 15729: Policy loss: -0.040628. Value loss: 0.076931. Entropy: 0.305610.\n",
      "episode: 5547   score: 480.0  epsilon: 1.0    steps: 176  evaluation reward: 422.8\n",
      "Training network. lr: 0.000129. clip: 0.051773\n",
      "Iteration 15730: Policy loss: 0.150831. Value loss: 0.223358. Entropy: 0.307047.\n",
      "Iteration 15731: Policy loss: 0.148767. Value loss: 0.080897. Entropy: 0.306873.\n",
      "Iteration 15732: Policy loss: 0.142746. Value loss: 0.056920. Entropy: 0.307396.\n",
      "Training network. lr: 0.000129. clip: 0.051773\n",
      "Iteration 15733: Policy loss: 0.025428. Value loss: 0.104649. Entropy: 0.307529.\n",
      "Iteration 15734: Policy loss: 0.022601. Value loss: 0.042238. Entropy: 0.307899.\n",
      "Iteration 15735: Policy loss: 0.019185. Value loss: 0.028134. Entropy: 0.307643.\n",
      "episode: 5548   score: 455.0  epsilon: 1.0    steps: 920  evaluation reward: 423.15\n",
      "Training network. lr: 0.000129. clip: 0.051773\n",
      "Iteration 15736: Policy loss: 0.378289. Value loss: 0.161655. Entropy: 0.306539.\n",
      "Iteration 15737: Policy loss: 0.368274. Value loss: 0.062399. Entropy: 0.305983.\n",
      "Iteration 15738: Policy loss: 0.358737. Value loss: 0.046036. Entropy: 0.306414.\n",
      "episode: 5549   score: 330.0  epsilon: 1.0    steps: 272  evaluation reward: 423.85\n",
      "episode: 5550   score: 565.0  epsilon: 1.0    steps: 312  evaluation reward: 428.45\n",
      "Training network. lr: 0.000129. clip: 0.051773\n",
      "Iteration 15739: Policy loss: 0.027822. Value loss: 0.102730. Entropy: 0.293230.\n",
      "Iteration 15740: Policy loss: 0.026024. Value loss: 0.050686. Entropy: 0.292846.\n",
      "Iteration 15741: Policy loss: 0.018016. Value loss: 0.035633. Entropy: 0.294569.\n",
      "now time :  2019-09-06 06:31:59.886782\n",
      "episode: 5551   score: 185.0  epsilon: 1.0    steps: 184  evaluation reward: 422.25\n",
      "Training network. lr: 0.000129. clip: 0.051773\n",
      "Iteration 15742: Policy loss: 0.044023. Value loss: 0.170107. Entropy: 0.308314.\n",
      "Iteration 15743: Policy loss: 0.041931. Value loss: 0.074964. Entropy: 0.308449.\n",
      "Iteration 15744: Policy loss: 0.034946. Value loss: 0.050572. Entropy: 0.308710.\n",
      "episode: 5552   score: 405.0  epsilon: 1.0    steps: 464  evaluation reward: 421.15\n",
      "episode: 5553   score: 665.0  epsilon: 1.0    steps: 688  evaluation reward: 421.85\n",
      "episode: 5554   score: 420.0  epsilon: 1.0    steps: 816  evaluation reward: 419.65\n",
      "Training network. lr: 0.000129. clip: 0.051773\n",
      "Iteration 15745: Policy loss: 0.273840. Value loss: 0.187431. Entropy: 0.285659.\n",
      "Iteration 15746: Policy loss: 0.276675. Value loss: 0.081729. Entropy: 0.287229.\n",
      "Iteration 15747: Policy loss: 0.270894. Value loss: 0.063246. Entropy: 0.288792.\n",
      "Training network. lr: 0.000129. clip: 0.051773\n",
      "Iteration 15748: Policy loss: -0.205124. Value loss: 0.264462. Entropy: 0.306787.\n",
      "Iteration 15749: Policy loss: -0.234028. Value loss: 0.129677. Entropy: 0.306961.\n",
      "Iteration 15750: Policy loss: -0.247352. Value loss: 0.077604. Entropy: 0.306567.\n",
      "episode: 5555   score: 265.0  epsilon: 1.0    steps: 584  evaluation reward: 418.85\n",
      "Training network. lr: 0.000129. clip: 0.051625\n",
      "Iteration 15751: Policy loss: 0.057725. Value loss: 0.246283. Entropy: 0.299349.\n",
      "Iteration 15752: Policy loss: 0.066185. Value loss: 0.100227. Entropy: 0.299627.\n",
      "Iteration 15753: Policy loss: 0.052120. Value loss: 0.061949. Entropy: 0.298929.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000129. clip: 0.051625\n",
      "Iteration 15754: Policy loss: 0.200658. Value loss: 0.224468. Entropy: 0.308578.\n",
      "Iteration 15755: Policy loss: 0.204687. Value loss: 0.074206. Entropy: 0.307577.\n",
      "Iteration 15756: Policy loss: 0.179882. Value loss: 0.046240. Entropy: 0.307453.\n",
      "Training network. lr: 0.000129. clip: 0.051625\n",
      "Iteration 15757: Policy loss: 0.082397. Value loss: 0.167519. Entropy: 0.304939.\n",
      "Iteration 15758: Policy loss: 0.058959. Value loss: 0.050431. Entropy: 0.304637.\n",
      "Iteration 15759: Policy loss: 0.073494. Value loss: 0.030245. Entropy: 0.305479.\n",
      "episode: 5556   score: 495.0  epsilon: 1.0    steps: 592  evaluation reward: 419.9\n",
      "episode: 5557   score: 300.0  epsilon: 1.0    steps: 936  evaluation reward: 418.7\n",
      "Training network. lr: 0.000129. clip: 0.051625\n",
      "Iteration 15760: Policy loss: 0.234649. Value loss: 0.119771. Entropy: 0.299145.\n",
      "Iteration 15761: Policy loss: 0.224028. Value loss: 0.050330. Entropy: 0.298701.\n",
      "Iteration 15762: Policy loss: 0.222168. Value loss: 0.037187. Entropy: 0.298303.\n",
      "episode: 5558   score: 590.0  epsilon: 1.0    steps: 448  evaluation reward: 415.35\n",
      "Training network. lr: 0.000129. clip: 0.051625\n",
      "Iteration 15763: Policy loss: 0.138092. Value loss: 0.121783. Entropy: 0.301731.\n",
      "Iteration 15764: Policy loss: 0.125136. Value loss: 0.043888. Entropy: 0.301905.\n",
      "Iteration 15765: Policy loss: 0.126156. Value loss: 0.030766. Entropy: 0.301340.\n",
      "episode: 5559   score: 290.0  epsilon: 1.0    steps: 1000  evaluation reward: 413.55\n",
      "Training network. lr: 0.000129. clip: 0.051625\n",
      "Iteration 15766: Policy loss: -0.264063. Value loss: 0.257509. Entropy: 0.311118.\n",
      "Iteration 15767: Policy loss: -0.286516. Value loss: 0.081243. Entropy: 0.310792.\n",
      "Iteration 15768: Policy loss: -0.297140. Value loss: 0.052759. Entropy: 0.309699.\n",
      "episode: 5560   score: 310.0  epsilon: 1.0    steps: 560  evaluation reward: 413.5\n",
      "episode: 5561   score: 625.0  epsilon: 1.0    steps: 736  evaluation reward: 414.8\n",
      "Training network. lr: 0.000129. clip: 0.051625\n",
      "Iteration 15769: Policy loss: -0.093664. Value loss: 0.271689. Entropy: 0.302962.\n",
      "Iteration 15770: Policy loss: -0.107580. Value loss: 0.118830. Entropy: 0.302522.\n",
      "Iteration 15771: Policy loss: -0.093913. Value loss: 0.064428. Entropy: 0.303459.\n",
      "Training network. lr: 0.000129. clip: 0.051625\n",
      "Iteration 15772: Policy loss: -0.383155. Value loss: 0.177493. Entropy: 0.305373.\n",
      "Iteration 15773: Policy loss: -0.389816. Value loss: 0.092434. Entropy: 0.306193.\n",
      "Iteration 15774: Policy loss: -0.391202. Value loss: 0.059966. Entropy: 0.305995.\n",
      "episode: 5562   score: 270.0  epsilon: 1.0    steps: 600  evaluation reward: 413.55\n",
      "Training network. lr: 0.000129. clip: 0.051625\n",
      "Iteration 15775: Policy loss: 0.362553. Value loss: 0.413111. Entropy: 0.305186.\n",
      "Iteration 15776: Policy loss: 0.353387. Value loss: 0.123843. Entropy: 0.304807.\n",
      "Iteration 15777: Policy loss: 0.336156. Value loss: 0.060893. Entropy: 0.305687.\n",
      "episode: 5563   score: 420.0  epsilon: 1.0    steps: 136  evaluation reward: 412.15\n",
      "Training network. lr: 0.000129. clip: 0.051625\n",
      "Iteration 15778: Policy loss: 0.230115. Value loss: 0.176878. Entropy: 0.305070.\n",
      "Iteration 15779: Policy loss: 0.221612. Value loss: 0.068653. Entropy: 0.305440.\n",
      "Iteration 15780: Policy loss: 0.208081. Value loss: 0.045948. Entropy: 0.305065.\n",
      "episode: 5564   score: 215.0  epsilon: 1.0    steps: 880  evaluation reward: 410.7\n",
      "Training network. lr: 0.000129. clip: 0.051625\n",
      "Iteration 15781: Policy loss: -0.032385. Value loss: 0.119436. Entropy: 0.306852.\n",
      "Iteration 15782: Policy loss: -0.038750. Value loss: 0.052451. Entropy: 0.306698.\n",
      "Iteration 15783: Policy loss: -0.043168. Value loss: 0.035160. Entropy: 0.308056.\n",
      "episode: 5565   score: 565.0  epsilon: 1.0    steps: 576  evaluation reward: 409.95\n",
      "Training network. lr: 0.000129. clip: 0.051625\n",
      "Iteration 15784: Policy loss: 0.086198. Value loss: 0.087548. Entropy: 0.301818.\n",
      "Iteration 15785: Policy loss: 0.084043. Value loss: 0.048290. Entropy: 0.302597.\n",
      "Iteration 15786: Policy loss: 0.079944. Value loss: 0.035788. Entropy: 0.302590.\n",
      "episode: 5566   score: 590.0  epsilon: 1.0    steps: 560  evaluation reward: 411.5\n",
      "Training network. lr: 0.000129. clip: 0.051625\n",
      "Iteration 15787: Policy loss: -0.213755. Value loss: 0.216182. Entropy: 0.310230.\n",
      "Iteration 15788: Policy loss: -0.204467. Value loss: 0.092956. Entropy: 0.310500.\n",
      "Iteration 15789: Policy loss: -0.230395. Value loss: 0.050721. Entropy: 0.310071.\n",
      "Training network. lr: 0.000129. clip: 0.051625\n",
      "Iteration 15790: Policy loss: -0.123461. Value loss: 0.135943. Entropy: 0.299051.\n",
      "Iteration 15791: Policy loss: -0.125101. Value loss: 0.050272. Entropy: 0.298695.\n",
      "Iteration 15792: Policy loss: -0.134569. Value loss: 0.033160. Entropy: 0.299558.\n",
      "episode: 5567   score: 300.0  epsilon: 1.0    steps: 64  evaluation reward: 408.95\n",
      "episode: 5568   score: 340.0  epsilon: 1.0    steps: 104  evaluation reward: 410.2\n",
      "Training network. lr: 0.000129. clip: 0.051625\n",
      "Iteration 15793: Policy loss: 0.211951. Value loss: 0.132741. Entropy: 0.286631.\n",
      "Iteration 15794: Policy loss: 0.210988. Value loss: 0.035476. Entropy: 0.286039.\n",
      "Iteration 15795: Policy loss: 0.207400. Value loss: 0.025256. Entropy: 0.286998.\n",
      "episode: 5569   score: 285.0  epsilon: 1.0    steps: 504  evaluation reward: 410.95\n",
      "Training network. lr: 0.000129. clip: 0.051625\n",
      "Iteration 15796: Policy loss: 0.366656. Value loss: 0.149878. Entropy: 0.294922.\n",
      "Iteration 15797: Policy loss: 0.350935. Value loss: 0.053200. Entropy: 0.294036.\n",
      "Iteration 15798: Policy loss: 0.352819. Value loss: 0.037199. Entropy: 0.294307.\n",
      "Training network. lr: 0.000129. clip: 0.051625\n",
      "Iteration 15799: Policy loss: 0.219220. Value loss: 0.107295. Entropy: 0.309098.\n",
      "Iteration 15800: Policy loss: 0.212233. Value loss: 0.043827. Entropy: 0.310183.\n",
      "Iteration 15801: Policy loss: 0.210907. Value loss: 0.030866. Entropy: 0.309922.\n",
      "episode: 5570   score: 535.0  epsilon: 1.0    steps: 88  evaluation reward: 411.75\n",
      "Training network. lr: 0.000129. clip: 0.051469\n",
      "Iteration 15802: Policy loss: -0.229358. Value loss: 0.101613. Entropy: 0.304532.\n",
      "Iteration 15803: Policy loss: -0.238245. Value loss: 0.042390. Entropy: 0.303817.\n",
      "Iteration 15804: Policy loss: -0.235496. Value loss: 0.029304. Entropy: 0.303856.\n",
      "episode: 5571   score: 275.0  epsilon: 1.0    steps: 816  evaluation reward: 411.5\n",
      "Training network. lr: 0.000129. clip: 0.051469\n",
      "Iteration 15805: Policy loss: 0.182866. Value loss: 0.111475. Entropy: 0.298519.\n",
      "Iteration 15806: Policy loss: 0.178670. Value loss: 0.041794. Entropy: 0.297360.\n",
      "Iteration 15807: Policy loss: 0.177178. Value loss: 0.030168. Entropy: 0.298116.\n",
      "Training network. lr: 0.000129. clip: 0.051469\n",
      "Iteration 15808: Policy loss: 0.116659. Value loss: 0.147404. Entropy: 0.293662.\n",
      "Iteration 15809: Policy loss: 0.097483. Value loss: 0.063649. Entropy: 0.293745.\n",
      "Iteration 15810: Policy loss: 0.097006. Value loss: 0.048709. Entropy: 0.292439.\n",
      "episode: 5572   score: 285.0  epsilon: 1.0    steps: 904  evaluation reward: 412.25\n",
      "Training network. lr: 0.000129. clip: 0.051469\n",
      "Iteration 15811: Policy loss: 0.045077. Value loss: 0.087237. Entropy: 0.302689.\n",
      "Iteration 15812: Policy loss: 0.043278. Value loss: 0.041647. Entropy: 0.303582.\n",
      "Iteration 15813: Policy loss: 0.042394. Value loss: 0.034058. Entropy: 0.301839.\n",
      "episode: 5573   score: 285.0  epsilon: 1.0    steps: 176  evaluation reward: 412.25\n",
      "episode: 5574   score: 680.0  epsilon: 1.0    steps: 296  evaluation reward: 414.7\n",
      "Training network. lr: 0.000129. clip: 0.051469\n",
      "Iteration 15814: Policy loss: -0.416697. Value loss: 0.222118. Entropy: 0.281432.\n",
      "Iteration 15815: Policy loss: -0.427854. Value loss: 0.078661. Entropy: 0.278900.\n",
      "Iteration 15816: Policy loss: -0.432823. Value loss: 0.039004. Entropy: 0.279642.\n",
      "episode: 5575   score: 620.0  epsilon: 1.0    steps: 936  evaluation reward: 415.45\n",
      "Training network. lr: 0.000129. clip: 0.051469\n",
      "Iteration 15817: Policy loss: 0.293844. Value loss: 0.099291. Entropy: 0.306712.\n",
      "Iteration 15818: Policy loss: 0.289416. Value loss: 0.045703. Entropy: 0.306608.\n",
      "Iteration 15819: Policy loss: 0.285015. Value loss: 0.031296. Entropy: 0.306513.\n",
      "episode: 5576   score: 370.0  epsilon: 1.0    steps: 632  evaluation reward: 415.85\n",
      "Training network. lr: 0.000129. clip: 0.051469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15820: Policy loss: -0.013716. Value loss: 0.132625. Entropy: 0.288023.\n",
      "Iteration 15821: Policy loss: -0.005284. Value loss: 0.055842. Entropy: 0.287645.\n",
      "Iteration 15822: Policy loss: -0.018852. Value loss: 0.042241. Entropy: 0.288307.\n",
      "Training network. lr: 0.000129. clip: 0.051469\n",
      "Iteration 15823: Policy loss: 0.123310. Value loss: 0.083717. Entropy: 0.315427.\n",
      "Iteration 15824: Policy loss: 0.111017. Value loss: 0.036802. Entropy: 0.315900.\n",
      "Iteration 15825: Policy loss: 0.111621. Value loss: 0.026338. Entropy: 0.315678.\n",
      "episode: 5577   score: 605.0  epsilon: 1.0    steps: 16  evaluation reward: 418.35\n",
      "episode: 5578   score: 265.0  epsilon: 1.0    steps: 896  evaluation reward: 417.35\n",
      "Training network. lr: 0.000129. clip: 0.051469\n",
      "Iteration 15826: Policy loss: 0.097220. Value loss: 0.075588. Entropy: 0.295655.\n",
      "Iteration 15827: Policy loss: 0.087624. Value loss: 0.028125. Entropy: 0.294134.\n",
      "Iteration 15828: Policy loss: 0.084647. Value loss: 0.021555. Entropy: 0.293152.\n",
      "Training network. lr: 0.000129. clip: 0.051469\n",
      "Iteration 15829: Policy loss: -0.110472. Value loss: 0.349400. Entropy: 0.301559.\n",
      "Iteration 15830: Policy loss: -0.112260. Value loss: 0.212348. Entropy: 0.302193.\n",
      "Iteration 15831: Policy loss: -0.126725. Value loss: 0.161910. Entropy: 0.300760.\n",
      "episode: 5579   score: 435.0  epsilon: 1.0    steps: 576  evaluation reward: 418.05\n",
      "Training network. lr: 0.000129. clip: 0.051469\n",
      "Iteration 15832: Policy loss: -0.299026. Value loss: 0.286422. Entropy: 0.293677.\n",
      "Iteration 15833: Policy loss: -0.298743. Value loss: 0.144820. Entropy: 0.295171.\n",
      "Iteration 15834: Policy loss: -0.297448. Value loss: 0.066345. Entropy: 0.295064.\n",
      "episode: 5580   score: 270.0  epsilon: 1.0    steps: 616  evaluation reward: 416.85\n",
      "episode: 5581   score: 320.0  epsilon: 1.0    steps: 672  evaluation reward: 417.2\n",
      "Training network. lr: 0.000129. clip: 0.051469\n",
      "Iteration 15835: Policy loss: -0.313083. Value loss: 0.275510. Entropy: 0.282613.\n",
      "Iteration 15836: Policy loss: -0.312459. Value loss: 0.121396. Entropy: 0.285068.\n",
      "Iteration 15837: Policy loss: -0.329412. Value loss: 0.091270. Entropy: 0.282725.\n",
      "Training network. lr: 0.000129. clip: 0.051469\n",
      "Iteration 15838: Policy loss: -0.077561. Value loss: 0.208925. Entropy: 0.317807.\n",
      "Iteration 15839: Policy loss: -0.079912. Value loss: 0.067466. Entropy: 0.317838.\n",
      "Iteration 15840: Policy loss: -0.086363. Value loss: 0.047173. Entropy: 0.317740.\n",
      "episode: 5582   score: 590.0  epsilon: 1.0    steps: 768  evaluation reward: 421.0\n",
      "Training network. lr: 0.000129. clip: 0.051469\n",
      "Iteration 15841: Policy loss: 0.098721. Value loss: 0.063209. Entropy: 0.301917.\n",
      "Iteration 15842: Policy loss: 0.094577. Value loss: 0.029802. Entropy: 0.301976.\n",
      "Iteration 15843: Policy loss: 0.097256. Value loss: 0.022620. Entropy: 0.303024.\n",
      "Training network. lr: 0.000129. clip: 0.051469\n",
      "Iteration 15844: Policy loss: 0.026757. Value loss: 0.177642. Entropy: 0.310486.\n",
      "Iteration 15845: Policy loss: 0.017691. Value loss: 0.069161. Entropy: 0.309777.\n",
      "Iteration 15846: Policy loss: 0.014154. Value loss: 0.046848. Entropy: 0.310229.\n",
      "episode: 5583   score: 315.0  epsilon: 1.0    steps: 16  evaluation reward: 417.65\n",
      "episode: 5584   score: 450.0  epsilon: 1.0    steps: 152  evaluation reward: 418.85\n",
      "Training network. lr: 0.000129. clip: 0.051469\n",
      "Iteration 15847: Policy loss: 0.228703. Value loss: 0.094934. Entropy: 0.288119.\n",
      "Iteration 15848: Policy loss: 0.218148. Value loss: 0.040962. Entropy: 0.287866.\n",
      "Iteration 15849: Policy loss: 0.213154. Value loss: 0.027427. Entropy: 0.287698.\n",
      "Training network. lr: 0.000129. clip: 0.051469\n",
      "Iteration 15850: Policy loss: 0.155386. Value loss: 0.093526. Entropy: 0.314912.\n",
      "Iteration 15851: Policy loss: 0.147359. Value loss: 0.042452. Entropy: 0.314853.\n",
      "Iteration 15852: Policy loss: 0.145969. Value loss: 0.029113. Entropy: 0.314591.\n",
      "episode: 5585   score: 685.0  epsilon: 1.0    steps: 584  evaluation reward: 421.8\n",
      "Training network. lr: 0.000128. clip: 0.051312\n",
      "Iteration 15853: Policy loss: -0.181779. Value loss: 0.193239. Entropy: 0.287602.\n",
      "Iteration 15854: Policy loss: -0.205489. Value loss: 0.071551. Entropy: 0.286607.\n",
      "Iteration 15855: Policy loss: -0.203149. Value loss: 0.051724. Entropy: 0.287057.\n",
      "episode: 5586   score: 375.0  epsilon: 1.0    steps: 232  evaluation reward: 422.85\n",
      "Training network. lr: 0.000128. clip: 0.051312\n",
      "Iteration 15856: Policy loss: 0.320755. Value loss: 0.105034. Entropy: 0.299190.\n",
      "Iteration 15857: Policy loss: 0.311573. Value loss: 0.041597. Entropy: 0.297720.\n",
      "Iteration 15858: Policy loss: 0.307491. Value loss: 0.032204. Entropy: 0.297938.\n",
      "episode: 5587   score: 210.0  epsilon: 1.0    steps: 608  evaluation reward: 420.45\n",
      "episode: 5588   score: 395.0  epsilon: 1.0    steps: 776  evaluation reward: 421.55\n",
      "Training network. lr: 0.000128. clip: 0.051312\n",
      "Iteration 15859: Policy loss: 0.185643. Value loss: 0.088738. Entropy: 0.284171.\n",
      "Iteration 15860: Policy loss: 0.184609. Value loss: 0.045539. Entropy: 0.283772.\n",
      "Iteration 15861: Policy loss: 0.185023. Value loss: 0.035365. Entropy: 0.283659.\n",
      "episode: 5589   score: 670.0  epsilon: 1.0    steps: 208  evaluation reward: 422.35\n",
      "episode: 5590   score: 260.0  epsilon: 1.0    steps: 256  evaluation reward: 419.75\n",
      "Training network. lr: 0.000128. clip: 0.051312\n",
      "Iteration 15862: Policy loss: 0.112815. Value loss: 0.090783. Entropy: 0.283625.\n",
      "Iteration 15863: Policy loss: 0.110056. Value loss: 0.049749. Entropy: 0.284181.\n",
      "Iteration 15864: Policy loss: 0.102427. Value loss: 0.040430. Entropy: 0.281623.\n",
      "episode: 5591   score: 395.0  epsilon: 1.0    steps: 512  evaluation reward: 420.85\n",
      "Training network. lr: 0.000128. clip: 0.051312\n",
      "Iteration 15865: Policy loss: 0.152938. Value loss: 0.058912. Entropy: 0.298001.\n",
      "Iteration 15866: Policy loss: 0.154355. Value loss: 0.029682. Entropy: 0.297525.\n",
      "Iteration 15867: Policy loss: 0.150820. Value loss: 0.024002. Entropy: 0.298442.\n",
      "episode: 5592   score: 395.0  epsilon: 1.0    steps: 560  evaluation reward: 421.45\n",
      "Training network. lr: 0.000128. clip: 0.051312\n",
      "Iteration 15868: Policy loss: -0.021471. Value loss: 0.096334. Entropy: 0.298723.\n",
      "Iteration 15869: Policy loss: -0.031363. Value loss: 0.047915. Entropy: 0.298257.\n",
      "Iteration 15870: Policy loss: -0.033517. Value loss: 0.036665. Entropy: 0.298582.\n",
      "Training network. lr: 0.000128. clip: 0.051312\n",
      "Iteration 15871: Policy loss: 0.092311. Value loss: 0.079801. Entropy: 0.317599.\n",
      "Iteration 15872: Policy loss: 0.092619. Value loss: 0.033741. Entropy: 0.315923.\n",
      "Iteration 15873: Policy loss: 0.087765. Value loss: 0.023898. Entropy: 0.315477.\n",
      "episode: 5593   score: 345.0  epsilon: 1.0    steps: 928  evaluation reward: 417.95\n",
      "episode: 5594   score: 180.0  epsilon: 1.0    steps: 992  evaluation reward: 415.8\n",
      "Training network. lr: 0.000128. clip: 0.051312\n",
      "Iteration 15874: Policy loss: 0.174162. Value loss: 0.106164. Entropy: 0.307269.\n",
      "Iteration 15875: Policy loss: 0.165584. Value loss: 0.049885. Entropy: 0.308261.\n",
      "Iteration 15876: Policy loss: 0.158982. Value loss: 0.033595. Entropy: 0.306191.\n",
      "Training network. lr: 0.000128. clip: 0.051312\n",
      "Iteration 15877: Policy loss: -0.159496. Value loss: 0.123273. Entropy: 0.292522.\n",
      "Iteration 15878: Policy loss: -0.164231. Value loss: 0.054116. Entropy: 0.288853.\n",
      "Iteration 15879: Policy loss: -0.173111. Value loss: 0.037368. Entropy: 0.290029.\n",
      "Training network. lr: 0.000128. clip: 0.051312\n",
      "Iteration 15880: Policy loss: -0.105476. Value loss: 0.126318. Entropy: 0.308672.\n",
      "Iteration 15881: Policy loss: -0.104289. Value loss: 0.051140. Entropy: 0.309281.\n",
      "Iteration 15882: Policy loss: -0.113601. Value loss: 0.034192. Entropy: 0.309523.\n",
      "episode: 5595   score: 450.0  epsilon: 1.0    steps: 344  evaluation reward: 413.35\n",
      "Training network. lr: 0.000128. clip: 0.051312\n",
      "Iteration 15883: Policy loss: 0.270923. Value loss: 0.125573. Entropy: 0.301527.\n",
      "Iteration 15884: Policy loss: 0.263516. Value loss: 0.060236. Entropy: 0.302164.\n",
      "Iteration 15885: Policy loss: 0.261605. Value loss: 0.043482. Entropy: 0.301857.\n",
      "episode: 5596   score: 350.0  epsilon: 1.0    steps: 48  evaluation reward: 414.45\n",
      "episode: 5597   score: 360.0  epsilon: 1.0    steps: 120  evaluation reward: 414.6\n",
      "episode: 5598   score: 435.0  epsilon: 1.0    steps: 872  evaluation reward: 417.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000128. clip: 0.051312\n",
      "Iteration 15886: Policy loss: 0.060926. Value loss: 0.102860. Entropy: 0.284635.\n",
      "Iteration 15887: Policy loss: 0.059000. Value loss: 0.052414. Entropy: 0.287013.\n",
      "Iteration 15888: Policy loss: 0.050217. Value loss: 0.037830. Entropy: 0.286911.\n",
      "episode: 5599   score: 285.0  epsilon: 1.0    steps: 832  evaluation reward: 415.35\n",
      "Training network. lr: 0.000128. clip: 0.051312\n",
      "Iteration 15889: Policy loss: 0.237828. Value loss: 0.158838. Entropy: 0.300103.\n",
      "Iteration 15890: Policy loss: 0.236257. Value loss: 0.079081. Entropy: 0.300267.\n",
      "Iteration 15891: Policy loss: 0.229920. Value loss: 0.052191. Entropy: 0.298844.\n",
      "Training network. lr: 0.000128. clip: 0.051312\n",
      "Iteration 15892: Policy loss: -0.017594. Value loss: 0.083533. Entropy: 0.309422.\n",
      "Iteration 15893: Policy loss: -0.019836. Value loss: 0.040872. Entropy: 0.309362.\n",
      "Iteration 15894: Policy loss: -0.027619. Value loss: 0.030446. Entropy: 0.308987.\n",
      "episode: 5600   score: 260.0  epsilon: 1.0    steps: 256  evaluation reward: 415.1\n",
      "Training network. lr: 0.000128. clip: 0.051312\n",
      "Iteration 15895: Policy loss: -0.110376. Value loss: 0.229875. Entropy: 0.293173.\n",
      "Iteration 15896: Policy loss: -0.135659. Value loss: 0.113140. Entropy: 0.291047.\n",
      "Iteration 15897: Policy loss: -0.121226. Value loss: 0.066366. Entropy: 0.292151.\n",
      "now time :  2019-09-06 06:41:40.609143\n",
      "episode: 5601   score: 270.0  epsilon: 1.0    steps: 40  evaluation reward: 410.55\n",
      "episode: 5602   score: 210.0  epsilon: 1.0    steps: 40  evaluation reward: 408.75\n",
      "Training network. lr: 0.000128. clip: 0.051312\n",
      "Iteration 15898: Policy loss: 0.102112. Value loss: 0.068923. Entropy: 0.286709.\n",
      "Iteration 15899: Policy loss: 0.100028. Value loss: 0.038429. Entropy: 0.286964.\n",
      "Iteration 15900: Policy loss: 0.097414. Value loss: 0.032456. Entropy: 0.287108.\n",
      "Training network. lr: 0.000128. clip: 0.051164\n",
      "Iteration 15901: Policy loss: -0.050067. Value loss: 0.089030. Entropy: 0.310822.\n",
      "Iteration 15902: Policy loss: -0.051092. Value loss: 0.042703. Entropy: 0.312579.\n",
      "Iteration 15903: Policy loss: -0.052557. Value loss: 0.030317. Entropy: 0.311475.\n",
      "Training network. lr: 0.000128. clip: 0.051164\n",
      "Iteration 15904: Policy loss: -0.645052. Value loss: 0.429330. Entropy: 0.310066.\n",
      "Iteration 15905: Policy loss: -0.673708. Value loss: 0.196610. Entropy: 0.310166.\n",
      "Iteration 15906: Policy loss: -0.688568. Value loss: 0.148755. Entropy: 0.311540.\n",
      "Training network. lr: 0.000128. clip: 0.051164\n",
      "Iteration 15907: Policy loss: -0.093998. Value loss: 0.132729. Entropy: 0.310761.\n",
      "Iteration 15908: Policy loss: -0.089663. Value loss: 0.056292. Entropy: 0.310951.\n",
      "Iteration 15909: Policy loss: -0.101815. Value loss: 0.040982. Entropy: 0.311553.\n",
      "episode: 5603   score: 920.0  epsilon: 1.0    steps: 240  evaluation reward: 414.05\n",
      "Training network. lr: 0.000128. clip: 0.051164\n",
      "Iteration 15910: Policy loss: -0.058275. Value loss: 0.140353. Entropy: 0.296021.\n",
      "Iteration 15911: Policy loss: -0.056634. Value loss: 0.065577. Entropy: 0.297079.\n",
      "Iteration 15912: Policy loss: -0.065890. Value loss: 0.046867. Entropy: 0.294736.\n",
      "episode: 5604   score: 430.0  epsilon: 1.0    steps: 200  evaluation reward: 414.7\n",
      "episode: 5605   score: 255.0  epsilon: 1.0    steps: 208  evaluation reward: 409.6\n",
      "episode: 5606   score: 255.0  epsilon: 1.0    steps: 544  evaluation reward: 406.45\n",
      "episode: 5607   score: 620.0  epsilon: 1.0    steps: 592  evaluation reward: 408.05\n",
      "episode: 5608   score: 450.0  epsilon: 1.0    steps: 776  evaluation reward: 407.95\n",
      "Training network. lr: 0.000128. clip: 0.051164\n",
      "Iteration 15913: Policy loss: 0.352215. Value loss: 0.144263. Entropy: 0.249383.\n",
      "Iteration 15914: Policy loss: 0.345469. Value loss: 0.061348. Entropy: 0.250950.\n",
      "Iteration 15915: Policy loss: 0.341495. Value loss: 0.047920. Entropy: 0.251764.\n",
      "Training network. lr: 0.000128. clip: 0.051164\n",
      "Iteration 15916: Policy loss: -0.093477. Value loss: 0.146431. Entropy: 0.315668.\n",
      "Iteration 15917: Policy loss: -0.092089. Value loss: 0.073311. Entropy: 0.315751.\n",
      "Iteration 15918: Policy loss: -0.092717. Value loss: 0.055307. Entropy: 0.316079.\n",
      "Training network. lr: 0.000128. clip: 0.051164\n",
      "Iteration 15919: Policy loss: 0.091075. Value loss: 0.072672. Entropy: 0.313801.\n",
      "Iteration 15920: Policy loss: 0.087870. Value loss: 0.035469. Entropy: 0.314337.\n",
      "Iteration 15921: Policy loss: 0.086432. Value loss: 0.029007. Entropy: 0.313734.\n",
      "episode: 5609   score: 335.0  epsilon: 1.0    steps: 96  evaluation reward: 407.85\n",
      "episode: 5610   score: 415.0  epsilon: 1.0    steps: 832  evaluation reward: 406.8\n",
      "Training network. lr: 0.000128. clip: 0.051164\n",
      "Iteration 15922: Policy loss: -0.039429. Value loss: 0.088499. Entropy: 0.291964.\n",
      "Iteration 15923: Policy loss: -0.050433. Value loss: 0.043357. Entropy: 0.292795.\n",
      "Iteration 15924: Policy loss: -0.052572. Value loss: 0.030134. Entropy: 0.293593.\n",
      "episode: 5611   score: 210.0  epsilon: 1.0    steps: 32  evaluation reward: 403.15\n",
      "episode: 5612   score: 210.0  epsilon: 1.0    steps: 632  evaluation reward: 400.6\n",
      "Training network. lr: 0.000128. clip: 0.051164\n",
      "Iteration 15925: Policy loss: 0.006805. Value loss: 0.103879. Entropy: 0.285226.\n",
      "Iteration 15926: Policy loss: 0.005625. Value loss: 0.053010. Entropy: 0.289260.\n",
      "Iteration 15927: Policy loss: 0.002722. Value loss: 0.043025. Entropy: 0.289185.\n",
      "episode: 5613   score: 185.0  epsilon: 1.0    steps: 160  evaluation reward: 400.15\n",
      "Training network. lr: 0.000128. clip: 0.051164\n",
      "Iteration 15928: Policy loss: -0.107414. Value loss: 0.109246. Entropy: 0.303916.\n",
      "Iteration 15929: Policy loss: -0.110487. Value loss: 0.041507. Entropy: 0.303215.\n",
      "Iteration 15930: Policy loss: -0.113153. Value loss: 0.031538. Entropy: 0.303573.\n",
      "Training network. lr: 0.000128. clip: 0.051164\n",
      "Iteration 15931: Policy loss: -0.140783. Value loss: 0.341084. Entropy: 0.313990.\n",
      "Iteration 15932: Policy loss: -0.163758. Value loss: 0.131472. Entropy: 0.312612.\n",
      "Iteration 15933: Policy loss: -0.159456. Value loss: 0.060385. Entropy: 0.312470.\n",
      "episode: 5614   score: 300.0  epsilon: 1.0    steps: 320  evaluation reward: 400.25\n",
      "episode: 5615   score: 385.0  epsilon: 1.0    steps: 736  evaluation reward: 400.0\n",
      "Training network. lr: 0.000128. clip: 0.051164\n",
      "Iteration 15934: Policy loss: -0.114252. Value loss: 0.117728. Entropy: 0.289633.\n",
      "Iteration 15935: Policy loss: -0.120258. Value loss: 0.060236. Entropy: 0.290775.\n",
      "Iteration 15936: Policy loss: -0.119652. Value loss: 0.044155. Entropy: 0.292435.\n",
      "episode: 5616   score: 365.0  epsilon: 1.0    steps: 136  evaluation reward: 400.2\n",
      "Training network. lr: 0.000128. clip: 0.051164\n",
      "Iteration 15937: Policy loss: 0.104196. Value loss: 0.123128. Entropy: 0.302015.\n",
      "Iteration 15938: Policy loss: 0.108962. Value loss: 0.054109. Entropy: 0.301843.\n",
      "Iteration 15939: Policy loss: 0.097632. Value loss: 0.036412. Entropy: 0.302477.\n",
      "episode: 5617   score: 245.0  epsilon: 1.0    steps: 616  evaluation reward: 397.65\n",
      "Training network. lr: 0.000128. clip: 0.051164\n",
      "Iteration 15940: Policy loss: 0.327742. Value loss: 0.111700. Entropy: 0.307224.\n",
      "Iteration 15941: Policy loss: 0.322085. Value loss: 0.041906. Entropy: 0.306207.\n",
      "Iteration 15942: Policy loss: 0.312630. Value loss: 0.026918. Entropy: 0.306877.\n",
      "episode: 5618   score: 265.0  epsilon: 1.0    steps: 248  evaluation reward: 395.7\n",
      "Training network. lr: 0.000128. clip: 0.051164\n",
      "Iteration 15943: Policy loss: 0.175091. Value loss: 0.070551. Entropy: 0.302200.\n",
      "Iteration 15944: Policy loss: 0.171276. Value loss: 0.037586. Entropy: 0.302858.\n",
      "Iteration 15945: Policy loss: 0.170040. Value loss: 0.029196. Entropy: 0.302038.\n",
      "Training network. lr: 0.000128. clip: 0.051164\n",
      "Iteration 15946: Policy loss: 0.042513. Value loss: 0.075509. Entropy: 0.312284.\n",
      "Iteration 15947: Policy loss: 0.033142. Value loss: 0.032890. Entropy: 0.312804.\n",
      "Iteration 15948: Policy loss: 0.031896. Value loss: 0.027709. Entropy: 0.312503.\n",
      "episode: 5619   score: 590.0  epsilon: 1.0    steps: 56  evaluation reward: 398.45\n",
      "episode: 5620   score: 210.0  epsilon: 1.0    steps: 424  evaluation reward: 396.5\n",
      "Training network. lr: 0.000128. clip: 0.051164\n",
      "Iteration 15949: Policy loss: -0.151436. Value loss: 0.153043. Entropy: 0.288399.\n",
      "Iteration 15950: Policy loss: -0.156178. Value loss: 0.072282. Entropy: 0.289022.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15951: Policy loss: -0.156460. Value loss: 0.051530. Entropy: 0.287650.\n",
      "episode: 5621   score: 225.0  epsilon: 1.0    steps: 952  evaluation reward: 390.05\n",
      "Training network. lr: 0.000128. clip: 0.051008\n",
      "Iteration 15952: Policy loss: 0.639480. Value loss: 0.316008. Entropy: 0.306581.\n",
      "Iteration 15953: Policy loss: 0.612150. Value loss: 0.086134. Entropy: 0.305851.\n",
      "Iteration 15954: Policy loss: 0.617651. Value loss: 0.059098. Entropy: 0.305929.\n",
      "episode: 5622   score: 295.0  epsilon: 1.0    steps: 392  evaluation reward: 389.35\n",
      "episode: 5623   score: 335.0  epsilon: 1.0    steps: 496  evaluation reward: 389.85\n",
      "Training network. lr: 0.000128. clip: 0.051008\n",
      "Iteration 15955: Policy loss: 0.130658. Value loss: 0.102187. Entropy: 0.288329.\n",
      "Iteration 15956: Policy loss: 0.122237. Value loss: 0.044457. Entropy: 0.288819.\n",
      "Iteration 15957: Policy loss: 0.117245. Value loss: 0.032625. Entropy: 0.289283.\n",
      "episode: 5624   score: 210.0  epsilon: 1.0    steps: 520  evaluation reward: 388.65\n",
      "episode: 5625   score: 285.0  epsilon: 1.0    steps: 992  evaluation reward: 387.75\n",
      "Training network. lr: 0.000128. clip: 0.051008\n",
      "Iteration 15958: Policy loss: 0.092351. Value loss: 0.135113. Entropy: 0.303220.\n",
      "Iteration 15959: Policy loss: 0.091722. Value loss: 0.071414. Entropy: 0.303619.\n",
      "Iteration 15960: Policy loss: 0.082275. Value loss: 0.053218. Entropy: 0.304176.\n",
      "Training network. lr: 0.000128. clip: 0.051008\n",
      "Iteration 15961: Policy loss: 0.186889. Value loss: 0.089303. Entropy: 0.310734.\n",
      "Iteration 15962: Policy loss: 0.177880. Value loss: 0.042137. Entropy: 0.310804.\n",
      "Iteration 15963: Policy loss: 0.185550. Value loss: 0.029946. Entropy: 0.310167.\n",
      "episode: 5626   score: 210.0  epsilon: 1.0    steps: 568  evaluation reward: 385.55\n",
      "episode: 5627   score: 215.0  epsilon: 1.0    steps: 816  evaluation reward: 385.1\n",
      "Training network. lr: 0.000128. clip: 0.051008\n",
      "Iteration 15964: Policy loss: 0.021369. Value loss: 0.101987. Entropy: 0.296094.\n",
      "Iteration 15965: Policy loss: 0.015178. Value loss: 0.045528. Entropy: 0.297027.\n",
      "Iteration 15966: Policy loss: 0.015370. Value loss: 0.030795. Entropy: 0.297972.\n",
      "episode: 5628   score: 470.0  epsilon: 1.0    steps: 1000  evaluation reward: 384.15\n",
      "Training network. lr: 0.000128. clip: 0.051008\n",
      "Iteration 15967: Policy loss: -0.095490. Value loss: 0.231594. Entropy: 0.308470.\n",
      "Iteration 15968: Policy loss: -0.123822. Value loss: 0.127768. Entropy: 0.309354.\n",
      "Iteration 15969: Policy loss: -0.131160. Value loss: 0.075675. Entropy: 0.308536.\n",
      "Training network. lr: 0.000128. clip: 0.051008\n",
      "Iteration 15970: Policy loss: 0.258888. Value loss: 0.131978. Entropy: 0.306767.\n",
      "Iteration 15971: Policy loss: 0.244909. Value loss: 0.067661. Entropy: 0.307618.\n",
      "Iteration 15972: Policy loss: 0.246632. Value loss: 0.051861. Entropy: 0.306971.\n",
      "episode: 5629   score: 310.0  epsilon: 1.0    steps: 368  evaluation reward: 379.05\n",
      "episode: 5630   score: 125.0  epsilon: 1.0    steps: 376  evaluation reward: 377.9\n",
      "Training network. lr: 0.000128. clip: 0.051008\n",
      "Iteration 15973: Policy loss: 0.124487. Value loss: 0.119555. Entropy: 0.304912.\n",
      "Iteration 15974: Policy loss: 0.121550. Value loss: 0.061883. Entropy: 0.301558.\n",
      "Iteration 15975: Policy loss: 0.119999. Value loss: 0.043157. Entropy: 0.303098.\n",
      "episode: 5631   score: 335.0  epsilon: 1.0    steps: 1008  evaluation reward: 378.4\n",
      "Training network. lr: 0.000128. clip: 0.051008\n",
      "Iteration 15976: Policy loss: -0.249328. Value loss: 0.128092. Entropy: 0.312867.\n",
      "Iteration 15977: Policy loss: -0.256926. Value loss: 0.073669. Entropy: 0.311790.\n",
      "Iteration 15978: Policy loss: -0.266250. Value loss: 0.055878. Entropy: 0.311393.\n",
      "episode: 5632   score: 420.0  epsilon: 1.0    steps: 480  evaluation reward: 375.05\n",
      "episode: 5633   score: 210.0  epsilon: 1.0    steps: 872  evaluation reward: 371.25\n",
      "Training network. lr: 0.000128. clip: 0.051008\n",
      "Iteration 15979: Policy loss: 0.082964. Value loss: 0.074589. Entropy: 0.305280.\n",
      "Iteration 15980: Policy loss: 0.074483. Value loss: 0.034511. Entropy: 0.305778.\n",
      "Iteration 15981: Policy loss: 0.066414. Value loss: 0.027253. Entropy: 0.304812.\n",
      "Training network. lr: 0.000128. clip: 0.051008\n",
      "Iteration 15982: Policy loss: 0.091722. Value loss: 0.097924. Entropy: 0.314188.\n",
      "Iteration 15983: Policy loss: 0.096897. Value loss: 0.035213. Entropy: 0.314192.\n",
      "Iteration 15984: Policy loss: 0.085803. Value loss: 0.023254. Entropy: 0.313726.\n",
      "episode: 5634   score: 300.0  epsilon: 1.0    steps: 304  evaluation reward: 368.05\n",
      "episode: 5635   score: 450.0  epsilon: 1.0    steps: 832  evaluation reward: 369.25\n",
      "episode: 5636   score: 160.0  epsilon: 1.0    steps: 912  evaluation reward: 367.55\n",
      "Training network. lr: 0.000128. clip: 0.051008\n",
      "Iteration 15985: Policy loss: 0.089589. Value loss: 0.078967. Entropy: 0.302418.\n",
      "Iteration 15986: Policy loss: 0.083111. Value loss: 0.043590. Entropy: 0.303641.\n",
      "Iteration 15987: Policy loss: 0.081151. Value loss: 0.037605. Entropy: 0.302748.\n",
      "Training network. lr: 0.000128. clip: 0.051008\n",
      "Iteration 15988: Policy loss: -0.089722. Value loss: 0.091690. Entropy: 0.315386.\n",
      "Iteration 15989: Policy loss: -0.094607. Value loss: 0.044455. Entropy: 0.315279.\n",
      "Iteration 15990: Policy loss: -0.098784. Value loss: 0.033452. Entropy: 0.315174.\n",
      "Training network. lr: 0.000128. clip: 0.051008\n",
      "Iteration 15991: Policy loss: -0.020849. Value loss: 0.100510. Entropy: 0.310951.\n",
      "Iteration 15992: Policy loss: -0.022259. Value loss: 0.049084. Entropy: 0.311444.\n",
      "Iteration 15993: Policy loss: -0.023342. Value loss: 0.032257. Entropy: 0.311247.\n",
      "Training network. lr: 0.000128. clip: 0.051008\n",
      "Iteration 15994: Policy loss: -0.030162. Value loss: 0.095136. Entropy: 0.317426.\n",
      "Iteration 15995: Policy loss: -0.034949. Value loss: 0.043693. Entropy: 0.316106.\n",
      "Iteration 15996: Policy loss: -0.043809. Value loss: 0.030521. Entropy: 0.315602.\n",
      "episode: 5637   score: 390.0  epsilon: 1.0    steps: 272  evaluation reward: 368.9\n",
      "episode: 5638   score: 340.0  epsilon: 1.0    steps: 768  evaluation reward: 369.15\n",
      "Training network. lr: 0.000128. clip: 0.051008\n",
      "Iteration 15997: Policy loss: 0.092028. Value loss: 0.095189. Entropy: 0.305108.\n",
      "Iteration 15998: Policy loss: 0.087218. Value loss: 0.046469. Entropy: 0.305805.\n",
      "Iteration 15999: Policy loss: 0.089793. Value loss: 0.034241. Entropy: 0.306020.\n",
      "Training network. lr: 0.000128. clip: 0.051008\n",
      "Iteration 16000: Policy loss: -0.137342. Value loss: 0.343668. Entropy: 0.310265.\n",
      "Iteration 16001: Policy loss: -0.125433. Value loss: 0.240202. Entropy: 0.310386.\n",
      "Iteration 16002: Policy loss: -0.134118. Value loss: 0.189205. Entropy: 0.310461.\n",
      "episode: 5639   score: 210.0  epsilon: 1.0    steps: 96  evaluation reward: 367.15\n",
      "episode: 5640   score: 215.0  epsilon: 1.0    steps: 456  evaluation reward: 365.2\n",
      "Training network. lr: 0.000127. clip: 0.050851\n",
      "Iteration 16003: Policy loss: -0.071475. Value loss: 0.146731. Entropy: 0.301128.\n",
      "Iteration 16004: Policy loss: -0.065081. Value loss: 0.070360. Entropy: 0.299689.\n",
      "Iteration 16005: Policy loss: -0.066925. Value loss: 0.045806. Entropy: 0.300212.\n",
      "Training network. lr: 0.000127. clip: 0.050851\n",
      "Iteration 16006: Policy loss: -0.059832. Value loss: 0.294372. Entropy: 0.309248.\n",
      "Iteration 16007: Policy loss: -0.066893. Value loss: 0.061959. Entropy: 0.308488.\n",
      "Iteration 16008: Policy loss: -0.071117. Value loss: 0.036484. Entropy: 0.308467.\n",
      "episode: 5641   score: 575.0  epsilon: 1.0    steps: 760  evaluation reward: 367.3\n",
      "episode: 5642   score: 530.0  epsilon: 1.0    steps: 872  evaluation reward: 368.7\n",
      "Training network. lr: 0.000127. clip: 0.050851\n",
      "Iteration 16009: Policy loss: 0.275403. Value loss: 0.145274. Entropy: 0.304030.\n",
      "Iteration 16010: Policy loss: 0.271423. Value loss: 0.066960. Entropy: 0.304100.\n",
      "Iteration 16011: Policy loss: 0.266783. Value loss: 0.046983. Entropy: 0.303792.\n",
      "episode: 5643   score: 365.0  epsilon: 1.0    steps: 288  evaluation reward: 367.95\n",
      "episode: 5644   score: 345.0  epsilon: 1.0    steps: 504  evaluation reward: 368.75\n",
      "Training network. lr: 0.000127. clip: 0.050851\n",
      "Iteration 16012: Policy loss: -0.066082. Value loss: 0.100116. Entropy: 0.306058.\n",
      "Iteration 16013: Policy loss: -0.069486. Value loss: 0.051589. Entropy: 0.306105.\n",
      "Iteration 16014: Policy loss: -0.065888. Value loss: 0.037809. Entropy: 0.307160.\n",
      "episode: 5645   score: 210.0  epsilon: 1.0    steps: 72  evaluation reward: 367.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000127. clip: 0.050851\n",
      "Iteration 16015: Policy loss: 0.055269. Value loss: 0.121383. Entropy: 0.312797.\n",
      "Iteration 16016: Policy loss: 0.054438. Value loss: 0.068795. Entropy: 0.312172.\n",
      "Iteration 16017: Policy loss: 0.052129. Value loss: 0.049959. Entropy: 0.313446.\n",
      "Training network. lr: 0.000127. clip: 0.050851\n",
      "Iteration 16018: Policy loss: -0.164606. Value loss: 0.101972. Entropy: 0.305716.\n",
      "Iteration 16019: Policy loss: -0.163570. Value loss: 0.049059. Entropy: 0.306415.\n",
      "Iteration 16020: Policy loss: -0.169346. Value loss: 0.035092. Entropy: 0.306122.\n",
      "episode: 5646   score: 640.0  epsilon: 1.0    steps: 376  evaluation reward: 372.15\n",
      "Training network. lr: 0.000127. clip: 0.050851\n",
      "Iteration 16021: Policy loss: -0.069867. Value loss: 0.099704. Entropy: 0.307660.\n",
      "Iteration 16022: Policy loss: -0.073940. Value loss: 0.049492. Entropy: 0.307154.\n",
      "Iteration 16023: Policy loss: -0.077550. Value loss: 0.034004. Entropy: 0.307155.\n",
      "episode: 5647   score: 210.0  epsilon: 1.0    steps: 544  evaluation reward: 369.45\n",
      "episode: 5648   score: 430.0  epsilon: 1.0    steps: 768  evaluation reward: 369.2\n",
      "Training network. lr: 0.000127. clip: 0.050851\n",
      "Iteration 16024: Policy loss: 0.062104. Value loss: 0.128110. Entropy: 0.305268.\n",
      "Iteration 16025: Policy loss: 0.055695. Value loss: 0.051572. Entropy: 0.305478.\n",
      "Iteration 16026: Policy loss: 0.040756. Value loss: 0.036939. Entropy: 0.306357.\n",
      "episode: 5649   score: 215.0  epsilon: 1.0    steps: 824  evaluation reward: 368.05\n",
      "Training network. lr: 0.000127. clip: 0.050851\n",
      "Iteration 16027: Policy loss: 0.096959. Value loss: 0.151372. Entropy: 0.310133.\n",
      "Iteration 16028: Policy loss: 0.104909. Value loss: 0.066107. Entropy: 0.308959.\n",
      "Iteration 16029: Policy loss: 0.093278. Value loss: 0.039226. Entropy: 0.309527.\n",
      "episode: 5650   score: 210.0  epsilon: 1.0    steps: 320  evaluation reward: 364.5\n",
      "now time :  2019-09-06 06:49:54.811631\n",
      "episode: 5651   score: 290.0  epsilon: 1.0    steps: 576  evaluation reward: 365.55\n",
      "Training network. lr: 0.000127. clip: 0.050851\n",
      "Iteration 16030: Policy loss: 0.125722. Value loss: 0.127743. Entropy: 0.308108.\n",
      "Iteration 16031: Policy loss: 0.132495. Value loss: 0.034002. Entropy: 0.307064.\n",
      "Iteration 16032: Policy loss: 0.127982. Value loss: 0.025844. Entropy: 0.306228.\n",
      "Training network. lr: 0.000127. clip: 0.050851\n",
      "Iteration 16033: Policy loss: -0.583913. Value loss: 0.326799. Entropy: 0.311052.\n",
      "Iteration 16034: Policy loss: -0.594798. Value loss: 0.104050. Entropy: 0.312021.\n",
      "Iteration 16035: Policy loss: -0.580711. Value loss: 0.061255. Entropy: 0.312582.\n",
      "episode: 5652   score: 395.0  epsilon: 1.0    steps: 960  evaluation reward: 365.45\n",
      "Training network. lr: 0.000127. clip: 0.050851\n",
      "Iteration 16036: Policy loss: 0.173362. Value loss: 0.176872. Entropy: 0.306360.\n",
      "Iteration 16037: Policy loss: 0.153306. Value loss: 0.039656. Entropy: 0.306361.\n",
      "Iteration 16038: Policy loss: 0.155445. Value loss: 0.025684. Entropy: 0.306513.\n",
      "Training network. lr: 0.000127. clip: 0.050851\n",
      "Iteration 16039: Policy loss: 0.340537. Value loss: 0.151058. Entropy: 0.311181.\n",
      "Iteration 16040: Policy loss: 0.337284. Value loss: 0.048191. Entropy: 0.310381.\n",
      "Iteration 16041: Policy loss: 0.337666. Value loss: 0.034242. Entropy: 0.310514.\n",
      "Training network. lr: 0.000127. clip: 0.050851\n",
      "Iteration 16042: Policy loss: -0.079865. Value loss: 0.366088. Entropy: 0.308289.\n",
      "Iteration 16043: Policy loss: -0.110629. Value loss: 0.199040. Entropy: 0.308038.\n",
      "Iteration 16044: Policy loss: -0.118785. Value loss: 0.153984. Entropy: 0.308145.\n",
      "episode: 5653   score: 210.0  epsilon: 1.0    steps: 16  evaluation reward: 360.9\n",
      "episode: 5654   score: 465.0  epsilon: 1.0    steps: 608  evaluation reward: 361.35\n",
      "Training network. lr: 0.000127. clip: 0.050851\n",
      "Iteration 16045: Policy loss: 0.117349. Value loss: 0.130734. Entropy: 0.309240.\n",
      "Iteration 16046: Policy loss: 0.103944. Value loss: 0.053252. Entropy: 0.309670.\n",
      "Iteration 16047: Policy loss: 0.100687. Value loss: 0.036561. Entropy: 0.309233.\n",
      "episode: 5655   score: 465.0  epsilon: 1.0    steps: 72  evaluation reward: 363.35\n",
      "episode: 5656   score: 510.0  epsilon: 1.0    steps: 80  evaluation reward: 363.5\n",
      "Training network. lr: 0.000127. clip: 0.050851\n",
      "Iteration 16048: Policy loss: -0.165473. Value loss: 0.107654. Entropy: 0.314234.\n",
      "Iteration 16049: Policy loss: -0.170664. Value loss: 0.039855. Entropy: 0.313911.\n",
      "Iteration 16050: Policy loss: -0.172656. Value loss: 0.032462. Entropy: 0.313798.\n",
      "Training network. lr: 0.000127. clip: 0.050704\n",
      "Iteration 16051: Policy loss: 0.187806. Value loss: 0.110957. Entropy: 0.313940.\n",
      "Iteration 16052: Policy loss: 0.193388. Value loss: 0.052160. Entropy: 0.312615.\n",
      "Iteration 16053: Policy loss: 0.190638. Value loss: 0.037641. Entropy: 0.313138.\n",
      "episode: 5657   score: 380.0  epsilon: 1.0    steps: 104  evaluation reward: 364.3\n",
      "episode: 5658   score: 215.0  epsilon: 1.0    steps: 928  evaluation reward: 360.55\n",
      "Training network. lr: 0.000127. clip: 0.050704\n",
      "Iteration 16054: Policy loss: 0.204591. Value loss: 0.078089. Entropy: 0.305857.\n",
      "Iteration 16055: Policy loss: 0.202314. Value loss: 0.028438. Entropy: 0.304956.\n",
      "Iteration 16056: Policy loss: 0.194277. Value loss: 0.020123. Entropy: 0.305557.\n",
      "episode: 5659   score: 355.0  epsilon: 1.0    steps: 448  evaluation reward: 361.2\n",
      "Training network. lr: 0.000127. clip: 0.050704\n",
      "Iteration 16057: Policy loss: -0.050664. Value loss: 0.058309. Entropy: 0.310326.\n",
      "Iteration 16058: Policy loss: -0.055984. Value loss: 0.030833. Entropy: 0.309932.\n",
      "Iteration 16059: Policy loss: -0.054940. Value loss: 0.023572. Entropy: 0.310212.\n",
      "episode: 5660   score: 240.0  epsilon: 1.0    steps: 1008  evaluation reward: 360.5\n",
      "Training network. lr: 0.000127. clip: 0.050704\n",
      "Iteration 16060: Policy loss: -0.014797. Value loss: 0.069895. Entropy: 0.311104.\n",
      "Iteration 16061: Policy loss: -0.019901. Value loss: 0.032134. Entropy: 0.310331.\n",
      "Iteration 16062: Policy loss: -0.021711. Value loss: 0.023562. Entropy: 0.310719.\n",
      "episode: 5661   score: 210.0  epsilon: 1.0    steps: 256  evaluation reward: 356.35\n",
      "episode: 5662   score: 425.0  epsilon: 1.0    steps: 824  evaluation reward: 357.9\n",
      "Training network. lr: 0.000127. clip: 0.050704\n",
      "Iteration 16063: Policy loss: -0.166771. Value loss: 0.080091. Entropy: 0.305499.\n",
      "Iteration 16064: Policy loss: -0.164546. Value loss: 0.035690. Entropy: 0.302907.\n",
      "Iteration 16065: Policy loss: -0.165925. Value loss: 0.024675. Entropy: 0.304141.\n",
      "Training network. lr: 0.000127. clip: 0.050704\n",
      "Iteration 16066: Policy loss: -0.016763. Value loss: 0.080863. Entropy: 0.312078.\n",
      "Iteration 16067: Policy loss: -0.013032. Value loss: 0.027631. Entropy: 0.312356.\n",
      "Iteration 16068: Policy loss: -0.023558. Value loss: 0.021063. Entropy: 0.312146.\n",
      "Training network. lr: 0.000127. clip: 0.050704\n",
      "Iteration 16069: Policy loss: -0.031570. Value loss: 0.077169. Entropy: 0.305765.\n",
      "Iteration 16070: Policy loss: -0.033824. Value loss: 0.030240. Entropy: 0.306477.\n",
      "Iteration 16071: Policy loss: -0.032605. Value loss: 0.023652. Entropy: 0.305361.\n",
      "episode: 5663   score: 390.0  epsilon: 1.0    steps: 712  evaluation reward: 357.6\n",
      "Training network. lr: 0.000127. clip: 0.050704\n",
      "Iteration 16072: Policy loss: 0.074041. Value loss: 0.100517. Entropy: 0.303739.\n",
      "Iteration 16073: Policy loss: 0.062763. Value loss: 0.040153. Entropy: 0.303501.\n",
      "Iteration 16074: Policy loss: 0.071364. Value loss: 0.024411. Entropy: 0.303005.\n",
      "episode: 5664   score: 335.0  epsilon: 1.0    steps: 240  evaluation reward: 358.8\n",
      "Training network. lr: 0.000127. clip: 0.050704\n",
      "Iteration 16075: Policy loss: -0.169424. Value loss: 0.244676. Entropy: 0.301132.\n",
      "Iteration 16076: Policy loss: -0.182679. Value loss: 0.084974. Entropy: 0.303929.\n",
      "Iteration 16077: Policy loss: -0.189338. Value loss: 0.053828. Entropy: 0.304318.\n",
      "Training network. lr: 0.000127. clip: 0.050704\n",
      "Iteration 16078: Policy loss: -0.076842. Value loss: 0.122603. Entropy: 0.306864.\n",
      "Iteration 16079: Policy loss: -0.081890. Value loss: 0.060154. Entropy: 0.306581.\n",
      "Iteration 16080: Policy loss: -0.087858. Value loss: 0.038619. Entropy: 0.307172.\n",
      "episode: 5665   score: 665.0  epsilon: 1.0    steps: 496  evaluation reward: 359.8\n",
      "episode: 5666   score: 185.0  epsilon: 1.0    steps: 648  evaluation reward: 355.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5667   score: 345.0  epsilon: 1.0    steps: 1008  evaluation reward: 356.2\n",
      "Training network. lr: 0.000127. clip: 0.050704\n",
      "Iteration 16081: Policy loss: 0.051944. Value loss: 0.100628. Entropy: 0.296741.\n",
      "Iteration 16082: Policy loss: 0.040341. Value loss: 0.035451. Entropy: 0.297092.\n",
      "Iteration 16083: Policy loss: 0.035627. Value loss: 0.026751. Entropy: 0.296647.\n",
      "Training network. lr: 0.000127. clip: 0.050704\n",
      "Iteration 16084: Policy loss: 0.097855. Value loss: 0.070424. Entropy: 0.302957.\n",
      "Iteration 16085: Policy loss: 0.096092. Value loss: 0.038093. Entropy: 0.301951.\n",
      "Iteration 16086: Policy loss: 0.086745. Value loss: 0.030439. Entropy: 0.302257.\n",
      "Training network. lr: 0.000127. clip: 0.050704\n",
      "Iteration 16087: Policy loss: 0.076121. Value loss: 0.131994. Entropy: 0.309955.\n",
      "Iteration 16088: Policy loss: 0.070212. Value loss: 0.044831. Entropy: 0.309624.\n",
      "Iteration 16089: Policy loss: 0.071255. Value loss: 0.026329. Entropy: 0.308511.\n",
      "episode: 5668   score: 420.0  epsilon: 1.0    steps: 168  evaluation reward: 357.0\n",
      "episode: 5669   score: 480.0  epsilon: 1.0    steps: 248  evaluation reward: 358.95\n",
      "Training network. lr: 0.000127. clip: 0.050704\n",
      "Iteration 16090: Policy loss: -0.006009. Value loss: 0.071224. Entropy: 0.292011.\n",
      "Iteration 16091: Policy loss: -0.011353. Value loss: 0.037007. Entropy: 0.290676.\n",
      "Iteration 16092: Policy loss: -0.010754. Value loss: 0.029852. Entropy: 0.290883.\n",
      "Training network. lr: 0.000127. clip: 0.050704\n",
      "Iteration 16093: Policy loss: -0.016850. Value loss: 0.139143. Entropy: 0.310627.\n",
      "Iteration 16094: Policy loss: -0.021353. Value loss: 0.063514. Entropy: 0.310270.\n",
      "Iteration 16095: Policy loss: -0.030056. Value loss: 0.044799. Entropy: 0.311457.\n",
      "episode: 5670   score: 375.0  epsilon: 1.0    steps: 384  evaluation reward: 357.35\n",
      "episode: 5671   score: 335.0  epsilon: 1.0    steps: 608  evaluation reward: 357.95\n",
      "episode: 5672   score: 180.0  epsilon: 1.0    steps: 760  evaluation reward: 356.9\n",
      "episode: 5673   score: 495.0  epsilon: 1.0    steps: 792  evaluation reward: 359.0\n",
      "Training network. lr: 0.000127. clip: 0.050704\n",
      "Iteration 16096: Policy loss: 0.020735. Value loss: 0.108102. Entropy: 0.275592.\n",
      "Iteration 16097: Policy loss: 0.021030. Value loss: 0.041100. Entropy: 0.275170.\n",
      "Iteration 16098: Policy loss: 0.006035. Value loss: 0.030427. Entropy: 0.275683.\n",
      "Training network. lr: 0.000127. clip: 0.050704\n",
      "Iteration 16099: Policy loss: 0.238203. Value loss: 0.078604. Entropy: 0.318856.\n",
      "Iteration 16100: Policy loss: 0.231962. Value loss: 0.028820. Entropy: 0.320192.\n",
      "Iteration 16101: Policy loss: 0.235052. Value loss: 0.018157. Entropy: 0.319569.\n",
      "Training network. lr: 0.000126. clip: 0.050547\n",
      "Iteration 16102: Policy loss: 0.324267. Value loss: 0.140994. Entropy: 0.308535.\n",
      "Iteration 16103: Policy loss: 0.318738. Value loss: 0.076386. Entropy: 0.308083.\n",
      "Iteration 16104: Policy loss: 0.312098. Value loss: 0.058741. Entropy: 0.308139.\n",
      "episode: 5674   score: 180.0  epsilon: 1.0    steps: 440  evaluation reward: 354.0\n",
      "episode: 5675   score: 335.0  epsilon: 1.0    steps: 904  evaluation reward: 351.15\n",
      "Training network. lr: 0.000126. clip: 0.050547\n",
      "Iteration 16105: Policy loss: -0.210862. Value loss: 0.335173. Entropy: 0.290231.\n",
      "Iteration 16106: Policy loss: -0.238326. Value loss: 0.206267. Entropy: 0.289583.\n",
      "Iteration 16107: Policy loss: -0.250618. Value loss: 0.163151. Entropy: 0.288729.\n",
      "episode: 5676   score: 420.0  epsilon: 1.0    steps: 408  evaluation reward: 351.65\n",
      "episode: 5677   score: 155.0  epsilon: 1.0    steps: 608  evaluation reward: 347.15\n",
      "Training network. lr: 0.000126. clip: 0.050547\n",
      "Iteration 16108: Policy loss: -0.068369. Value loss: 0.118829. Entropy: 0.284725.\n",
      "Iteration 16109: Policy loss: -0.073959. Value loss: 0.051112. Entropy: 0.283806.\n",
      "Iteration 16110: Policy loss: -0.075779. Value loss: 0.034956. Entropy: 0.281580.\n",
      "episode: 5678   score: 210.0  epsilon: 1.0    steps: 920  evaluation reward: 346.6\n",
      "Training network. lr: 0.000126. clip: 0.050547\n",
      "Iteration 16111: Policy loss: -0.027365. Value loss: 0.160269. Entropy: 0.305581.\n",
      "Iteration 16112: Policy loss: -0.036943. Value loss: 0.058308. Entropy: 0.306149.\n",
      "Iteration 16113: Policy loss: -0.033600. Value loss: 0.036270. Entropy: 0.306675.\n",
      "Training network. lr: 0.000126. clip: 0.050547\n",
      "Iteration 16114: Policy loss: -0.271182. Value loss: 0.316765. Entropy: 0.304146.\n",
      "Iteration 16115: Policy loss: -0.291985. Value loss: 0.226476. Entropy: 0.305167.\n",
      "Iteration 16116: Policy loss: -0.287944. Value loss: 0.190093. Entropy: 0.304584.\n",
      "Training network. lr: 0.000126. clip: 0.050547\n",
      "Iteration 16117: Policy loss: 0.150933. Value loss: 0.086792. Entropy: 0.305707.\n",
      "Iteration 16118: Policy loss: 0.144130. Value loss: 0.027012. Entropy: 0.304547.\n",
      "Iteration 16119: Policy loss: 0.145430. Value loss: 0.017310. Entropy: 0.304723.\n",
      "episode: 5679   score: 340.0  epsilon: 1.0    steps: 352  evaluation reward: 345.65\n",
      "episode: 5680   score: 580.0  epsilon: 1.0    steps: 416  evaluation reward: 348.75\n",
      "Training network. lr: 0.000126. clip: 0.050547\n",
      "Iteration 16120: Policy loss: -0.139968. Value loss: 0.126658. Entropy: 0.285145.\n",
      "Iteration 16121: Policy loss: -0.147709. Value loss: 0.045563. Entropy: 0.286552.\n",
      "Iteration 16122: Policy loss: -0.156527. Value loss: 0.030291. Entropy: 0.286162.\n",
      "episode: 5681   score: 440.0  epsilon: 1.0    steps: 560  evaluation reward: 349.95\n",
      "Training network. lr: 0.000126. clip: 0.050547\n",
      "Iteration 16123: Policy loss: -0.152494. Value loss: 0.375376. Entropy: 0.298674.\n",
      "Iteration 16124: Policy loss: -0.185037. Value loss: 0.228076. Entropy: 0.297613.\n",
      "Iteration 16125: Policy loss: -0.174326. Value loss: 0.104569. Entropy: 0.295811.\n",
      "episode: 5682   score: 565.0  epsilon: 1.0    steps: 752  evaluation reward: 349.7\n",
      "Training network. lr: 0.000126. clip: 0.050547\n",
      "Iteration 16126: Policy loss: -0.050505. Value loss: 0.164925. Entropy: 0.300563.\n",
      "Iteration 16127: Policy loss: -0.062151. Value loss: 0.080545. Entropy: 0.299737.\n",
      "Iteration 16128: Policy loss: -0.055949. Value loss: 0.051249. Entropy: 0.299595.\n",
      "episode: 5683   score: 330.0  epsilon: 1.0    steps: 840  evaluation reward: 349.85\n",
      "Training network. lr: 0.000126. clip: 0.050547\n",
      "Iteration 16129: Policy loss: 0.056737. Value loss: 0.094236. Entropy: 0.304128.\n",
      "Iteration 16130: Policy loss: 0.046034. Value loss: 0.035091. Entropy: 0.302962.\n",
      "Iteration 16131: Policy loss: 0.048105. Value loss: 0.025115. Entropy: 0.303642.\n",
      "Training network. lr: 0.000126. clip: 0.050547\n",
      "Iteration 16132: Policy loss: 0.015456. Value loss: 0.081206. Entropy: 0.305691.\n",
      "Iteration 16133: Policy loss: 0.020630. Value loss: 0.031463. Entropy: 0.305310.\n",
      "Iteration 16134: Policy loss: 0.010901. Value loss: 0.020234. Entropy: 0.305129.\n",
      "Training network. lr: 0.000126. clip: 0.050547\n",
      "Iteration 16135: Policy loss: -0.073047. Value loss: 0.159641. Entropy: 0.310356.\n",
      "Iteration 16136: Policy loss: -0.076364. Value loss: 0.047959. Entropy: 0.308546.\n",
      "Iteration 16137: Policy loss: -0.077791. Value loss: 0.027483. Entropy: 0.310085.\n",
      "Training network. lr: 0.000126. clip: 0.050547\n",
      "Iteration 16138: Policy loss: -0.244840. Value loss: 0.452123. Entropy: 0.305117.\n",
      "Iteration 16139: Policy loss: -0.251404. Value loss: 0.212743. Entropy: 0.304033.\n",
      "Iteration 16140: Policy loss: -0.270856. Value loss: 0.150065. Entropy: 0.302393.\n",
      "episode: 5684   score: 285.0  epsilon: 1.0    steps: 416  evaluation reward: 348.2\n",
      "episode: 5685   score: 620.0  epsilon: 1.0    steps: 440  evaluation reward: 347.55\n",
      "episode: 5686   score: 340.0  epsilon: 1.0    steps: 800  evaluation reward: 347.2\n",
      "Training network. lr: 0.000126. clip: 0.050547\n",
      "Iteration 16141: Policy loss: -0.015888. Value loss: 0.150673. Entropy: 0.281300.\n",
      "Iteration 16142: Policy loss: -0.013342. Value loss: 0.068376. Entropy: 0.279421.\n",
      "Iteration 16143: Policy loss: -0.025014. Value loss: 0.049109. Entropy: 0.279450.\n",
      "Training network. lr: 0.000126. clip: 0.050547\n",
      "Iteration 16144: Policy loss: -0.249991. Value loss: 0.144391. Entropy: 0.308805.\n",
      "Iteration 16145: Policy loss: -0.260933. Value loss: 0.067692. Entropy: 0.309166.\n",
      "Iteration 16146: Policy loss: -0.261703. Value loss: 0.047933. Entropy: 0.308939.\n",
      "Training network. lr: 0.000126. clip: 0.050547\n",
      "Iteration 16147: Policy loss: -0.012705. Value loss: 0.152823. Entropy: 0.311342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16148: Policy loss: -0.009396. Value loss: 0.080731. Entropy: 0.311259.\n",
      "Iteration 16149: Policy loss: -0.010296. Value loss: 0.054405. Entropy: 0.311670.\n",
      "episode: 5687   score: 575.0  epsilon: 1.0    steps: 208  evaluation reward: 350.85\n",
      "Training network. lr: 0.000126. clip: 0.050547\n",
      "Iteration 16150: Policy loss: 0.420277. Value loss: 0.138194. Entropy: 0.299420.\n",
      "Iteration 16151: Policy loss: 0.405724. Value loss: 0.061064. Entropy: 0.299008.\n",
      "Iteration 16152: Policy loss: 0.396814. Value loss: 0.038403. Entropy: 0.299053.\n",
      "episode: 5688   score: 490.0  epsilon: 1.0    steps: 96  evaluation reward: 351.8\n",
      "episode: 5689   score: 880.0  epsilon: 1.0    steps: 736  evaluation reward: 353.9\n",
      "episode: 5690   score: 420.0  epsilon: 1.0    steps: 976  evaluation reward: 355.5\n",
      "Training network. lr: 0.000126. clip: 0.050390\n",
      "Iteration 16153: Policy loss: 0.344950. Value loss: 0.125000. Entropy: 0.293344.\n",
      "Iteration 16154: Policy loss: 0.347522. Value loss: 0.047491. Entropy: 0.294848.\n",
      "Iteration 16155: Policy loss: 0.342263. Value loss: 0.032838. Entropy: 0.294856.\n",
      "Training network. lr: 0.000126. clip: 0.050390\n",
      "Iteration 16156: Policy loss: 0.128000. Value loss: 0.103699. Entropy: 0.304436.\n",
      "Iteration 16157: Policy loss: 0.125124. Value loss: 0.041453. Entropy: 0.304525.\n",
      "Iteration 16158: Policy loss: 0.124495. Value loss: 0.027896. Entropy: 0.305549.\n",
      "Training network. lr: 0.000126. clip: 0.050390\n",
      "Iteration 16159: Policy loss: -0.098265. Value loss: 0.180027. Entropy: 0.315107.\n",
      "Iteration 16160: Policy loss: -0.111570. Value loss: 0.088172. Entropy: 0.315450.\n",
      "Iteration 16161: Policy loss: -0.115283. Value loss: 0.064615. Entropy: 0.314502.\n",
      "Training network. lr: 0.000126. clip: 0.050390\n",
      "Iteration 16162: Policy loss: 0.271637. Value loss: 0.189117. Entropy: 0.308342.\n",
      "Iteration 16163: Policy loss: 0.260123. Value loss: 0.053628. Entropy: 0.308675.\n",
      "Iteration 16164: Policy loss: 0.254841. Value loss: 0.031168. Entropy: 0.307707.\n",
      "episode: 5691   score: 335.0  epsilon: 1.0    steps: 304  evaluation reward: 354.9\n",
      "episode: 5692   score: 405.0  epsilon: 1.0    steps: 808  evaluation reward: 355.0\n",
      "Training network. lr: 0.000126. clip: 0.050390\n",
      "Iteration 16165: Policy loss: 0.126230. Value loss: 0.089488. Entropy: 0.299515.\n",
      "Iteration 16166: Policy loss: 0.120458. Value loss: 0.049490. Entropy: 0.299041.\n",
      "Iteration 16167: Policy loss: 0.116601. Value loss: 0.036214. Entropy: 0.298851.\n",
      "episode: 5693   score: 570.0  epsilon: 1.0    steps: 104  evaluation reward: 357.25\n",
      "Training network. lr: 0.000126. clip: 0.050390\n",
      "Iteration 16168: Policy loss: 0.062100. Value loss: 0.095418. Entropy: 0.302140.\n",
      "Iteration 16169: Policy loss: 0.059867. Value loss: 0.044307. Entropy: 0.301827.\n",
      "Iteration 16170: Policy loss: 0.055390. Value loss: 0.031335. Entropy: 0.302264.\n",
      "Training network. lr: 0.000126. clip: 0.050390\n",
      "Iteration 16171: Policy loss: -0.624926. Value loss: 0.397863. Entropy: 0.307589.\n",
      "Iteration 16172: Policy loss: -0.649256. Value loss: 0.242069. Entropy: 0.310447.\n",
      "Iteration 16173: Policy loss: -0.638537. Value loss: 0.173492. Entropy: 0.309842.\n",
      "episode: 5694   score: 365.0  epsilon: 1.0    steps: 168  evaluation reward: 359.1\n",
      "episode: 5695   score: 625.0  epsilon: 1.0    steps: 496  evaluation reward: 360.85\n",
      "episode: 5696   score: 290.0  epsilon: 1.0    steps: 936  evaluation reward: 360.25\n",
      "Training network. lr: 0.000126. clip: 0.050390\n",
      "Iteration 16174: Policy loss: -0.229823. Value loss: 0.356385. Entropy: 0.287123.\n",
      "Iteration 16175: Policy loss: -0.225416. Value loss: 0.213252. Entropy: 0.286637.\n",
      "Iteration 16176: Policy loss: -0.221625. Value loss: 0.154174. Entropy: 0.285129.\n",
      "episode: 5697   score: 495.0  epsilon: 1.0    steps: 688  evaluation reward: 361.6\n",
      "episode: 5698   score: 445.0  epsilon: 1.0    steps: 856  evaluation reward: 361.7\n",
      "Training network. lr: 0.000126. clip: 0.050390\n",
      "Iteration 16177: Policy loss: -0.253607. Value loss: 0.223448. Entropy: 0.292913.\n",
      "Iteration 16178: Policy loss: -0.265959. Value loss: 0.086192. Entropy: 0.291039.\n",
      "Iteration 16179: Policy loss: -0.279687. Value loss: 0.058393. Entropy: 0.289740.\n",
      "Training network. lr: 0.000126. clip: 0.050390\n",
      "Iteration 16180: Policy loss: -0.020963. Value loss: 0.150436. Entropy: 0.312206.\n",
      "Iteration 16181: Policy loss: -0.018796. Value loss: 0.073837. Entropy: 0.313103.\n",
      "Iteration 16182: Policy loss: -0.029152. Value loss: 0.049171. Entropy: 0.312667.\n",
      "Training network. lr: 0.000126. clip: 0.050390\n",
      "Iteration 16183: Policy loss: 0.193626. Value loss: 0.141320. Entropy: 0.302989.\n",
      "Iteration 16184: Policy loss: 0.186499. Value loss: 0.054058. Entropy: 0.303353.\n",
      "Iteration 16185: Policy loss: 0.179659. Value loss: 0.040228. Entropy: 0.303130.\n",
      "Training network. lr: 0.000126. clip: 0.050390\n",
      "Iteration 16186: Policy loss: 0.221178. Value loss: 0.148036. Entropy: 0.314180.\n",
      "Iteration 16187: Policy loss: 0.216175. Value loss: 0.085972. Entropy: 0.313389.\n",
      "Iteration 16188: Policy loss: 0.209546. Value loss: 0.059608. Entropy: 0.313253.\n",
      "episode: 5699   score: 350.0  epsilon: 1.0    steps: 136  evaluation reward: 362.35\n",
      "episode: 5700   score: 545.0  epsilon: 1.0    steps: 288  evaluation reward: 365.2\n",
      "now time :  2019-09-06 06:59:46.736815\n",
      "episode: 5701   score: 615.0  epsilon: 1.0    steps: 376  evaluation reward: 368.65\n",
      "Training network. lr: 0.000126. clip: 0.050390\n",
      "Iteration 16189: Policy loss: 0.115659. Value loss: 0.135912. Entropy: 0.280345.\n",
      "Iteration 16190: Policy loss: 0.118771. Value loss: 0.045550. Entropy: 0.280602.\n",
      "Iteration 16191: Policy loss: 0.117898. Value loss: 0.033940. Entropy: 0.279715.\n",
      "Training network. lr: 0.000126. clip: 0.050390\n",
      "Iteration 16192: Policy loss: -0.098254. Value loss: 0.111310. Entropy: 0.307258.\n",
      "Iteration 16193: Policy loss: -0.099764. Value loss: 0.053417. Entropy: 0.306462.\n",
      "Iteration 16194: Policy loss: -0.099836. Value loss: 0.039940. Entropy: 0.307383.\n",
      "Training network. lr: 0.000126. clip: 0.050390\n",
      "Iteration 16195: Policy loss: 0.291670. Value loss: 0.135075. Entropy: 0.308985.\n",
      "Iteration 16196: Policy loss: 0.287642. Value loss: 0.048765. Entropy: 0.309243.\n",
      "Iteration 16197: Policy loss: 0.283334. Value loss: 0.031577. Entropy: 0.308815.\n",
      "episode: 5702   score: 580.0  epsilon: 1.0    steps: 64  evaluation reward: 372.35\n",
      "Training network. lr: 0.000126. clip: 0.050390\n",
      "Iteration 16198: Policy loss: 0.042322. Value loss: 0.165360. Entropy: 0.296227.\n",
      "Iteration 16199: Policy loss: 0.041349. Value loss: 0.049441. Entropy: 0.293770.\n",
      "Iteration 16200: Policy loss: 0.027343. Value loss: 0.037374. Entropy: 0.293767.\n",
      "Training network. lr: 0.000126. clip: 0.050243\n",
      "Iteration 16201: Policy loss: 0.244889. Value loss: 0.173544. Entropy: 0.297185.\n",
      "Iteration 16202: Policy loss: 0.244022. Value loss: 0.077462. Entropy: 0.296940.\n",
      "Iteration 16203: Policy loss: 0.235295. Value loss: 0.052451. Entropy: 0.296462.\n",
      "episode: 5703   score: 360.0  epsilon: 1.0    steps: 56  evaluation reward: 366.75\n",
      "episode: 5704   score: 560.0  epsilon: 1.0    steps: 720  evaluation reward: 368.05\n",
      "episode: 5705   score: 330.0  epsilon: 1.0    steps: 928  evaluation reward: 368.8\n",
      "Training network. lr: 0.000126. clip: 0.050243\n",
      "Iteration 16204: Policy loss: -0.085156. Value loss: 0.124506. Entropy: 0.286326.\n",
      "Iteration 16205: Policy loss: -0.088959. Value loss: 0.054975. Entropy: 0.287855.\n",
      "Iteration 16206: Policy loss: -0.100068. Value loss: 0.036330. Entropy: 0.288365.\n",
      "episode: 5706   score: 390.0  epsilon: 1.0    steps: 136  evaluation reward: 370.15\n",
      "Training network. lr: 0.000126. clip: 0.050243\n",
      "Iteration 16207: Policy loss: 0.068972. Value loss: 0.108232. Entropy: 0.298218.\n",
      "Iteration 16208: Policy loss: 0.068923. Value loss: 0.061927. Entropy: 0.299147.\n",
      "Iteration 16209: Policy loss: 0.064457. Value loss: 0.044270. Entropy: 0.298687.\n",
      "Training network. lr: 0.000126. clip: 0.050243\n",
      "Iteration 16210: Policy loss: -0.269577. Value loss: 0.108310. Entropy: 0.307418.\n",
      "Iteration 16211: Policy loss: -0.275472. Value loss: 0.046664. Entropy: 0.306928.\n",
      "Iteration 16212: Policy loss: -0.280672. Value loss: 0.032645. Entropy: 0.307615.\n",
      "episode: 5707   score: 705.0  epsilon: 1.0    steps: 752  evaluation reward: 371.0\n",
      "episode: 5708   score: 445.0  epsilon: 1.0    steps: 760  evaluation reward: 370.95\n",
      "Training network. lr: 0.000126. clip: 0.050243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16213: Policy loss: -0.367144. Value loss: 0.597153. Entropy: 0.293204.\n",
      "Iteration 16214: Policy loss: -0.391051. Value loss: 0.417451. Entropy: 0.290782.\n",
      "Iteration 16215: Policy loss: -0.370189. Value loss: 0.295391. Entropy: 0.292359.\n",
      "Training network. lr: 0.000126. clip: 0.050243\n",
      "Iteration 16216: Policy loss: -0.255288. Value loss: 0.249852. Entropy: 0.314541.\n",
      "Iteration 16217: Policy loss: -0.267907. Value loss: 0.171195. Entropy: 0.314247.\n",
      "Iteration 16218: Policy loss: -0.268394. Value loss: 0.132166. Entropy: 0.313965.\n",
      "episode: 5709   score: 460.0  epsilon: 1.0    steps: 648  evaluation reward: 372.2\n",
      "Training network. lr: 0.000126. clip: 0.050243\n",
      "Iteration 16219: Policy loss: 0.008981. Value loss: 0.225490. Entropy: 0.287829.\n",
      "Iteration 16220: Policy loss: -0.003909. Value loss: 0.114391. Entropy: 0.288346.\n",
      "Iteration 16221: Policy loss: -0.018530. Value loss: 0.066608. Entropy: 0.288786.\n",
      "episode: 5710   score: 700.0  epsilon: 1.0    steps: 840  evaluation reward: 375.05\n",
      "Training network. lr: 0.000126. clip: 0.050243\n",
      "Iteration 16222: Policy loss: 0.325471. Value loss: 0.374565. Entropy: 0.305825.\n",
      "Iteration 16223: Policy loss: 0.330345. Value loss: 0.172684. Entropy: 0.305916.\n",
      "Iteration 16224: Policy loss: 0.322826. Value loss: 0.096676. Entropy: 0.305395.\n",
      "Training network. lr: 0.000126. clip: 0.050243\n",
      "Iteration 16225: Policy loss: 0.057649. Value loss: 0.139991. Entropy: 0.310568.\n",
      "Iteration 16226: Policy loss: 0.044616. Value loss: 0.069729. Entropy: 0.311089.\n",
      "Iteration 16227: Policy loss: 0.042794. Value loss: 0.052645. Entropy: 0.310784.\n",
      "Training network. lr: 0.000126. clip: 0.050243\n",
      "Iteration 16228: Policy loss: 0.226471. Value loss: 0.109122. Entropy: 0.310939.\n",
      "Iteration 16229: Policy loss: 0.219483. Value loss: 0.050153. Entropy: 0.311760.\n",
      "Iteration 16230: Policy loss: 0.217242. Value loss: 0.037353. Entropy: 0.311034.\n",
      "Training network. lr: 0.000126. clip: 0.050243\n",
      "Iteration 16231: Policy loss: -0.104296. Value loss: 0.187293. Entropy: 0.306156.\n",
      "Iteration 16232: Policy loss: -0.120942. Value loss: 0.068379. Entropy: 0.305945.\n",
      "Iteration 16233: Policy loss: -0.117166. Value loss: 0.044972. Entropy: 0.306874.\n",
      "episode: 5711   score: 180.0  epsilon: 1.0    steps: 816  evaluation reward: 374.75\n",
      "Training network. lr: 0.000126. clip: 0.050243\n",
      "Iteration 16234: Policy loss: 0.040637. Value loss: 0.368215. Entropy: 0.286869.\n",
      "Iteration 16235: Policy loss: 0.024942. Value loss: 0.096792. Entropy: 0.284295.\n",
      "Iteration 16236: Policy loss: 0.015513. Value loss: 0.061171. Entropy: 0.284453.\n",
      "episode: 5712   score: 820.0  epsilon: 1.0    steps: 624  evaluation reward: 380.85\n",
      "episode: 5713   score: 620.0  epsilon: 1.0    steps: 656  evaluation reward: 385.2\n",
      "episode: 5714   score: 450.0  epsilon: 1.0    steps: 912  evaluation reward: 386.7\n",
      "Training network. lr: 0.000126. clip: 0.050243\n",
      "Iteration 16237: Policy loss: 1.022650. Value loss: 0.372719. Entropy: 0.276231.\n",
      "Iteration 16238: Policy loss: 0.987694. Value loss: 0.102288. Entropy: 0.275709.\n",
      "Iteration 16239: Policy loss: 0.991862. Value loss: 0.062756. Entropy: 0.275881.\n",
      "episode: 5715   score: 725.0  epsilon: 1.0    steps: 800  evaluation reward: 390.1\n",
      "Training network. lr: 0.000126. clip: 0.050243\n",
      "Iteration 16240: Policy loss: -0.030233. Value loss: 0.156376. Entropy: 0.294774.\n",
      "Iteration 16241: Policy loss: -0.040568. Value loss: 0.090663. Entropy: 0.296412.\n",
      "Iteration 16242: Policy loss: -0.037583. Value loss: 0.063178. Entropy: 0.296344.\n",
      "Training network. lr: 0.000126. clip: 0.050243\n",
      "Iteration 16243: Policy loss: 0.335361. Value loss: 0.288796. Entropy: 0.303475.\n",
      "Iteration 16244: Policy loss: 0.293970. Value loss: 0.075172. Entropy: 0.301084.\n",
      "Iteration 16245: Policy loss: 0.316238. Value loss: 0.045229. Entropy: 0.301272.\n",
      "episode: 5716   score: 240.0  epsilon: 1.0    steps: 512  evaluation reward: 388.85\n",
      "Training network. lr: 0.000126. clip: 0.050243\n",
      "Iteration 16246: Policy loss: 0.272748. Value loss: 0.201964. Entropy: 0.291952.\n",
      "Iteration 16247: Policy loss: 0.262783. Value loss: 0.084085. Entropy: 0.292576.\n",
      "Iteration 16248: Policy loss: 0.262573. Value loss: 0.057989. Entropy: 0.292031.\n",
      "Training network. lr: 0.000126. clip: 0.050243\n",
      "Iteration 16249: Policy loss: 0.044072. Value loss: 0.116463. Entropy: 0.304849.\n",
      "Iteration 16250: Policy loss: 0.044402. Value loss: 0.065537. Entropy: 0.304578.\n",
      "Iteration 16251: Policy loss: 0.040002. Value loss: 0.044887. Entropy: 0.304287.\n",
      "episode: 5717   score: 710.0  epsilon: 1.0    steps: 456  evaluation reward: 393.5\n",
      "Training network. lr: 0.000125. clip: 0.050086\n",
      "Iteration 16252: Policy loss: -0.152565. Value loss: 0.336411. Entropy: 0.289706.\n",
      "Iteration 16253: Policy loss: -0.172672. Value loss: 0.208873. Entropy: 0.287325.\n",
      "Iteration 16254: Policy loss: -0.169125. Value loss: 0.158252. Entropy: 0.288065.\n",
      "Training network. lr: 0.000125. clip: 0.050086\n",
      "Iteration 16255: Policy loss: -0.203765. Value loss: 0.216689. Entropy: 0.313234.\n",
      "Iteration 16256: Policy loss: -0.209691. Value loss: 0.086374. Entropy: 0.312204.\n",
      "Iteration 16257: Policy loss: -0.214359. Value loss: 0.055879. Entropy: 0.312097.\n",
      "episode: 5718   score: 355.0  epsilon: 1.0    steps: 776  evaluation reward: 394.4\n",
      "episode: 5719   score: 320.0  epsilon: 1.0    steps: 800  evaluation reward: 391.7\n",
      "Training network. lr: 0.000125. clip: 0.050086\n",
      "Iteration 16258: Policy loss: 0.181340. Value loss: 0.167284. Entropy: 0.285699.\n",
      "Iteration 16259: Policy loss: 0.179459. Value loss: 0.084804. Entropy: 0.284176.\n",
      "Iteration 16260: Policy loss: 0.170840. Value loss: 0.064752. Entropy: 0.284923.\n",
      "episode: 5720   score: 360.0  epsilon: 1.0    steps: 80  evaluation reward: 393.2\n",
      "episode: 5721   score: 225.0  epsilon: 1.0    steps: 176  evaluation reward: 393.2\n",
      "Training network. lr: 0.000125. clip: 0.050086\n",
      "Iteration 16261: Policy loss: 0.158714. Value loss: 0.090189. Entropy: 0.284672.\n",
      "Iteration 16262: Policy loss: 0.157726. Value loss: 0.038075. Entropy: 0.286000.\n",
      "Iteration 16263: Policy loss: 0.155345. Value loss: 0.026699. Entropy: 0.284304.\n",
      "episode: 5722   score: 650.0  epsilon: 1.0    steps: 872  evaluation reward: 396.75\n",
      "Training network. lr: 0.000125. clip: 0.050086\n",
      "Iteration 16264: Policy loss: 0.276007. Value loss: 0.090447. Entropy: 0.307866.\n",
      "Iteration 16265: Policy loss: 0.263122. Value loss: 0.039300. Entropy: 0.307141.\n",
      "Iteration 16266: Policy loss: 0.264162. Value loss: 0.029475. Entropy: 0.306550.\n",
      "Training network. lr: 0.000125. clip: 0.050086\n",
      "Iteration 16267: Policy loss: -0.437035. Value loss: 0.381014. Entropy: 0.297212.\n",
      "Iteration 16268: Policy loss: -0.451700. Value loss: 0.167993. Entropy: 0.298511.\n",
      "Iteration 16269: Policy loss: -0.468624. Value loss: 0.095726. Entropy: 0.298368.\n",
      "episode: 5723   score: 415.0  epsilon: 1.0    steps: 752  evaluation reward: 397.55\n",
      "episode: 5724   score: 260.0  epsilon: 1.0    steps: 768  evaluation reward: 398.05\n",
      "Training network. lr: 0.000125. clip: 0.050086\n",
      "Iteration 16270: Policy loss: 0.181731. Value loss: 0.226218. Entropy: 0.287849.\n",
      "Iteration 16271: Policy loss: 0.172357. Value loss: 0.074373. Entropy: 0.288170.\n",
      "Iteration 16272: Policy loss: 0.176123. Value loss: 0.049821. Entropy: 0.286909.\n",
      "Training network. lr: 0.000125. clip: 0.050086\n",
      "Iteration 16273: Policy loss: 0.231662. Value loss: 0.207938. Entropy: 0.310250.\n",
      "Iteration 16274: Policy loss: 0.219636. Value loss: 0.074225. Entropy: 0.310774.\n",
      "Iteration 16275: Policy loss: 0.220966. Value loss: 0.055733. Entropy: 0.310673.\n",
      "episode: 5725   score: 1005.0  epsilon: 1.0    steps: 440  evaluation reward: 405.25\n",
      "Training network. lr: 0.000125. clip: 0.050086\n",
      "Iteration 16276: Policy loss: -0.022398. Value loss: 0.095111. Entropy: 0.293383.\n",
      "Iteration 16277: Policy loss: -0.025067. Value loss: 0.048264. Entropy: 0.294666.\n",
      "Iteration 16278: Policy loss: -0.026218. Value loss: 0.033904. Entropy: 0.294529.\n",
      "episode: 5726   score: 240.0  epsilon: 1.0    steps: 48  evaluation reward: 405.55\n",
      "episode: 5727   score: 560.0  epsilon: 1.0    steps: 1016  evaluation reward: 409.0\n",
      "Training network. lr: 0.000125. clip: 0.050086\n",
      "Iteration 16279: Policy loss: -0.043626. Value loss: 0.178947. Entropy: 0.292678.\n",
      "Iteration 16280: Policy loss: -0.044530. Value loss: 0.063268. Entropy: 0.294102.\n",
      "Iteration 16281: Policy loss: -0.049218. Value loss: 0.044131. Entropy: 0.293813.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5728   score: 330.0  epsilon: 1.0    steps: 120  evaluation reward: 407.6\n",
      "episode: 5729   score: 365.0  epsilon: 1.0    steps: 1000  evaluation reward: 408.15\n",
      "Training network. lr: 0.000125. clip: 0.050086\n",
      "Iteration 16282: Policy loss: -0.071266. Value loss: 0.097726. Entropy: 0.289376.\n",
      "Iteration 16283: Policy loss: -0.068080. Value loss: 0.038018. Entropy: 0.289805.\n",
      "Iteration 16284: Policy loss: -0.070540. Value loss: 0.032590. Entropy: 0.290432.\n",
      "episode: 5730   score: 260.0  epsilon: 1.0    steps: 936  evaluation reward: 409.5\n",
      "Training network. lr: 0.000125. clip: 0.050086\n",
      "Iteration 16285: Policy loss: 0.313711. Value loss: 0.113081. Entropy: 0.293139.\n",
      "Iteration 16286: Policy loss: 0.300417. Value loss: 0.044555. Entropy: 0.293274.\n",
      "Iteration 16287: Policy loss: 0.304018. Value loss: 0.029706. Entropy: 0.294655.\n",
      "Training network. lr: 0.000125. clip: 0.050086\n",
      "Iteration 16288: Policy loss: 0.097408. Value loss: 0.112642. Entropy: 0.299644.\n",
      "Iteration 16289: Policy loss: 0.100927. Value loss: 0.040423. Entropy: 0.300604.\n",
      "Iteration 16290: Policy loss: 0.084973. Value loss: 0.027657. Entropy: 0.300419.\n",
      "episode: 5731   score: 335.0  epsilon: 1.0    steps: 792  evaluation reward: 409.5\n",
      "episode: 5732   score: 305.0  epsilon: 1.0    steps: 960  evaluation reward: 408.35\n",
      "Training network. lr: 0.000125. clip: 0.050086\n",
      "Iteration 16291: Policy loss: -0.047665. Value loss: 0.076439. Entropy: 0.297754.\n",
      "Iteration 16292: Policy loss: -0.054471. Value loss: 0.034828. Entropy: 0.299050.\n",
      "Iteration 16293: Policy loss: -0.053016. Value loss: 0.024999. Entropy: 0.299208.\n",
      "Training network. lr: 0.000125. clip: 0.050086\n",
      "Iteration 16294: Policy loss: -0.080680. Value loss: 0.118916. Entropy: 0.309369.\n",
      "Iteration 16295: Policy loss: -0.084237. Value loss: 0.042408. Entropy: 0.310237.\n",
      "Iteration 16296: Policy loss: -0.089942. Value loss: 0.027561. Entropy: 0.309702.\n",
      "Training network. lr: 0.000125. clip: 0.050086\n",
      "Iteration 16297: Policy loss: -0.470539. Value loss: 0.575464. Entropy: 0.311678.\n",
      "Iteration 16298: Policy loss: -0.482585. Value loss: 0.334988. Entropy: 0.312000.\n",
      "Iteration 16299: Policy loss: -0.505311. Value loss: 0.268654. Entropy: 0.312207.\n",
      "episode: 5733   score: 410.0  epsilon: 1.0    steps: 864  evaluation reward: 410.35\n",
      "Training network. lr: 0.000125. clip: 0.050086\n",
      "Iteration 16300: Policy loss: -0.031671. Value loss: 0.368650. Entropy: 0.308326.\n",
      "Iteration 16301: Policy loss: -0.041823. Value loss: 0.230781. Entropy: 0.307688.\n",
      "Iteration 16302: Policy loss: -0.050710. Value loss: 0.172933. Entropy: 0.308457.\n",
      "episode: 5734   score: 495.0  epsilon: 1.0    steps: 136  evaluation reward: 412.3\n",
      "episode: 5735   score: 655.0  epsilon: 1.0    steps: 888  evaluation reward: 414.35\n",
      "episode: 5736   score: 345.0  epsilon: 1.0    steps: 928  evaluation reward: 416.2\n",
      "Training network. lr: 0.000125. clip: 0.049929\n",
      "Iteration 16303: Policy loss: 0.196509. Value loss: 0.181805. Entropy: 0.300389.\n",
      "Iteration 16304: Policy loss: 0.201473. Value loss: 0.080728. Entropy: 0.298266.\n",
      "Iteration 16305: Policy loss: 0.193410. Value loss: 0.057224. Entropy: 0.297828.\n",
      "episode: 5737   score: 415.0  epsilon: 1.0    steps: 280  evaluation reward: 416.45\n",
      "Training network. lr: 0.000125. clip: 0.049929\n",
      "Iteration 16306: Policy loss: 0.132880. Value loss: 0.165168. Entropy: 0.300931.\n",
      "Iteration 16307: Policy loss: 0.119330. Value loss: 0.061335. Entropy: 0.301306.\n",
      "Iteration 16308: Policy loss: 0.115758. Value loss: 0.043712. Entropy: 0.301169.\n",
      "episode: 5738   score: 380.0  epsilon: 1.0    steps: 176  evaluation reward: 416.85\n",
      "Training network. lr: 0.000125. clip: 0.049929\n",
      "Iteration 16309: Policy loss: 0.052604. Value loss: 0.125480. Entropy: 0.297295.\n",
      "Iteration 16310: Policy loss: 0.041998. Value loss: 0.051538. Entropy: 0.294749.\n",
      "Iteration 16311: Policy loss: 0.033950. Value loss: 0.039124. Entropy: 0.296742.\n",
      "Training network. lr: 0.000125. clip: 0.049929\n",
      "Iteration 16312: Policy loss: 0.239287. Value loss: 0.124165. Entropy: 0.306520.\n",
      "Iteration 16313: Policy loss: 0.234294. Value loss: 0.048437. Entropy: 0.306116.\n",
      "Iteration 16314: Policy loss: 0.226061. Value loss: 0.032313. Entropy: 0.305452.\n",
      "episode: 5739   score: 320.0  epsilon: 1.0    steps: 136  evaluation reward: 417.95\n",
      "Training network. lr: 0.000125. clip: 0.049929\n",
      "Iteration 16315: Policy loss: -0.225154. Value loss: 0.279303. Entropy: 0.297234.\n",
      "Iteration 16316: Policy loss: -0.232053. Value loss: 0.153326. Entropy: 0.296669.\n",
      "Iteration 16317: Policy loss: -0.231374. Value loss: 0.063814. Entropy: 0.298382.\n",
      "Training network. lr: 0.000125. clip: 0.049929\n",
      "Iteration 16318: Policy loss: 0.138528. Value loss: 0.187262. Entropy: 0.308683.\n",
      "Iteration 16319: Policy loss: 0.139304. Value loss: 0.072727. Entropy: 0.306823.\n",
      "Iteration 16320: Policy loss: 0.140973. Value loss: 0.051718. Entropy: 0.306659.\n",
      "Training network. lr: 0.000125. clip: 0.049929\n",
      "Iteration 16321: Policy loss: -0.252986. Value loss: 0.235415. Entropy: 0.301338.\n",
      "Iteration 16322: Policy loss: -0.251641. Value loss: 0.089354. Entropy: 0.301361.\n",
      "Iteration 16323: Policy loss: -0.269375. Value loss: 0.056591. Entropy: 0.300122.\n",
      "episode: 5740   score: 180.0  epsilon: 1.0    steps: 112  evaluation reward: 417.6\n",
      "episode: 5741   score: 345.0  epsilon: 1.0    steps: 392  evaluation reward: 415.3\n",
      "Training network. lr: 0.000125. clip: 0.049929\n",
      "Iteration 16324: Policy loss: 0.020497. Value loss: 0.186811. Entropy: 0.273537.\n",
      "Iteration 16325: Policy loss: 0.006593. Value loss: 0.081653. Entropy: 0.273958.\n",
      "Iteration 16326: Policy loss: 0.002060. Value loss: 0.052834. Entropy: 0.272514.\n",
      "episode: 5742   score: 925.0  epsilon: 1.0    steps: 216  evaluation reward: 419.25\n",
      "Training network. lr: 0.000125. clip: 0.049929\n",
      "Iteration 16327: Policy loss: 0.344969. Value loss: 0.171669. Entropy: 0.290093.\n",
      "Iteration 16328: Policy loss: 0.334025. Value loss: 0.043086. Entropy: 0.290128.\n",
      "Iteration 16329: Policy loss: 0.328089. Value loss: 0.033277. Entropy: 0.290331.\n",
      "Training network. lr: 0.000125. clip: 0.049929\n",
      "Iteration 16330: Policy loss: 0.048886. Value loss: 0.070432. Entropy: 0.301475.\n",
      "Iteration 16331: Policy loss: 0.049049. Value loss: 0.034367. Entropy: 0.302048.\n",
      "Iteration 16332: Policy loss: 0.050864. Value loss: 0.026430. Entropy: 0.301647.\n",
      "episode: 5743   score: 560.0  epsilon: 1.0    steps: 1016  evaluation reward: 421.2\n",
      "Training network. lr: 0.000125. clip: 0.049929\n",
      "Iteration 16333: Policy loss: 0.340274. Value loss: 0.182724. Entropy: 0.304421.\n",
      "Iteration 16334: Policy loss: 0.340052. Value loss: 0.059400. Entropy: 0.303386.\n",
      "Iteration 16335: Policy loss: 0.327082. Value loss: 0.035765. Entropy: 0.304353.\n",
      "episode: 5744   score: 360.0  epsilon: 1.0    steps: 616  evaluation reward: 421.35\n",
      "episode: 5745   score: 450.0  epsilon: 1.0    steps: 800  evaluation reward: 423.75\n",
      "episode: 5746   score: 500.0  epsilon: 1.0    steps: 896  evaluation reward: 422.35\n",
      "Training network. lr: 0.000125. clip: 0.049929\n",
      "Iteration 16336: Policy loss: 0.123027. Value loss: 0.323745. Entropy: 0.272122.\n",
      "Iteration 16337: Policy loss: 0.098674. Value loss: 0.156205. Entropy: 0.270717.\n",
      "Iteration 16338: Policy loss: 0.123301. Value loss: 0.079708. Entropy: 0.270489.\n",
      "Training network. lr: 0.000125. clip: 0.049929\n",
      "Iteration 16339: Policy loss: 0.087041. Value loss: 0.083645. Entropy: 0.296879.\n",
      "Iteration 16340: Policy loss: 0.087122. Value loss: 0.047594. Entropy: 0.295422.\n",
      "Iteration 16341: Policy loss: 0.081726. Value loss: 0.037086. Entropy: 0.294702.\n",
      "episode: 5747   score: 475.0  epsilon: 1.0    steps: 728  evaluation reward: 425.0\n",
      "Training network. lr: 0.000125. clip: 0.049929\n",
      "Iteration 16342: Policy loss: -0.460689. Value loss: 0.404234. Entropy: 0.291942.\n",
      "Iteration 16343: Policy loss: -0.460389. Value loss: 0.228624. Entropy: 0.292075.\n",
      "Iteration 16344: Policy loss: -0.474593. Value loss: 0.175027. Entropy: 0.291686.\n",
      "Training network. lr: 0.000125. clip: 0.049929\n",
      "Iteration 16345: Policy loss: -0.139674. Value loss: 0.367267. Entropy: 0.304842.\n",
      "Iteration 16346: Policy loss: -0.137103. Value loss: 0.228468. Entropy: 0.303762.\n",
      "Iteration 16347: Policy loss: -0.156775. Value loss: 0.160770. Entropy: 0.304691.\n",
      "episode: 5748   score: 290.0  epsilon: 1.0    steps: 200  evaluation reward: 423.6\n",
      "Training network. lr: 0.000125. clip: 0.049929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16348: Policy loss: 0.014464. Value loss: 0.388646. Entropy: 0.295687.\n",
      "Iteration 16349: Policy loss: 0.039921. Value loss: 0.219292. Entropy: 0.295657.\n",
      "Iteration 16350: Policy loss: 0.015173. Value loss: 0.135471. Entropy: 0.295404.\n",
      "episode: 5749   score: 830.0  epsilon: 1.0    steps: 568  evaluation reward: 429.75\n",
      "Training network. lr: 0.000124. clip: 0.049782\n",
      "Iteration 16351: Policy loss: 0.103248. Value loss: 0.148480. Entropy: 0.286097.\n",
      "Iteration 16352: Policy loss: 0.093095. Value loss: 0.074509. Entropy: 0.284977.\n",
      "Iteration 16353: Policy loss: 0.094819. Value loss: 0.052687. Entropy: 0.287171.\n",
      "Training network. lr: 0.000124. clip: 0.049782\n",
      "Iteration 16354: Policy loss: 0.094343. Value loss: 0.212691. Entropy: 0.312811.\n",
      "Iteration 16355: Policy loss: 0.072169. Value loss: 0.075429. Entropy: 0.312657.\n",
      "Iteration 16356: Policy loss: 0.056856. Value loss: 0.049341. Entropy: 0.312309.\n",
      "episode: 5750   score: 490.0  epsilon: 1.0    steps: 192  evaluation reward: 432.55\n",
      "now time :  2019-09-06 07:10:12.987805\n",
      "episode: 5751   score: 590.0  epsilon: 1.0    steps: 944  evaluation reward: 435.55\n",
      "Training network. lr: 0.000124. clip: 0.049782\n",
      "Iteration 16357: Policy loss: -0.037407. Value loss: 0.127203. Entropy: 0.297897.\n",
      "Iteration 16358: Policy loss: -0.041259. Value loss: 0.046645. Entropy: 0.298570.\n",
      "Iteration 16359: Policy loss: -0.051989. Value loss: 0.030937. Entropy: 0.297425.\n",
      "Training network. lr: 0.000124. clip: 0.049782\n",
      "Iteration 16360: Policy loss: 0.250594. Value loss: 0.119522. Entropy: 0.298904.\n",
      "Iteration 16361: Policy loss: 0.237660. Value loss: 0.031307. Entropy: 0.297764.\n",
      "Iteration 16362: Policy loss: 0.236014. Value loss: 0.019543. Entropy: 0.298156.\n",
      "episode: 5752   score: 425.0  epsilon: 1.0    steps: 48  evaluation reward: 435.85\n",
      "episode: 5753   score: 620.0  epsilon: 1.0    steps: 424  evaluation reward: 439.95\n",
      "Training network. lr: 0.000124. clip: 0.049782\n",
      "Iteration 16363: Policy loss: 0.140913. Value loss: 0.094969. Entropy: 0.286515.\n",
      "Iteration 16364: Policy loss: 0.138611. Value loss: 0.039601. Entropy: 0.286956.\n",
      "Iteration 16365: Policy loss: 0.133778. Value loss: 0.029075. Entropy: 0.285533.\n",
      "episode: 5754   score: 390.0  epsilon: 1.0    steps: 752  evaluation reward: 439.2\n",
      "Training network. lr: 0.000124. clip: 0.049782\n",
      "Iteration 16366: Policy loss: 0.289408. Value loss: 0.118164. Entropy: 0.293895.\n",
      "Iteration 16367: Policy loss: 0.287386. Value loss: 0.047334. Entropy: 0.293917.\n",
      "Iteration 16368: Policy loss: 0.283058. Value loss: 0.034124. Entropy: 0.293087.\n",
      "episode: 5755   score: 540.0  epsilon: 1.0    steps: 608  evaluation reward: 439.95\n",
      "episode: 5756   score: 390.0  epsilon: 1.0    steps: 816  evaluation reward: 438.75\n",
      "Training network. lr: 0.000124. clip: 0.049782\n",
      "Iteration 16369: Policy loss: 0.017799. Value loss: 0.381547. Entropy: 0.294551.\n",
      "Iteration 16370: Policy loss: 0.003471. Value loss: 0.145858. Entropy: 0.294436.\n",
      "Iteration 16371: Policy loss: -0.024881. Value loss: 0.051306. Entropy: 0.293300.\n",
      "episode: 5757   score: 320.0  epsilon: 1.0    steps: 600  evaluation reward: 438.15\n",
      "Training network. lr: 0.000124. clip: 0.049782\n",
      "Iteration 16372: Policy loss: -0.059076. Value loss: 0.331863. Entropy: 0.298374.\n",
      "Iteration 16373: Policy loss: -0.068395. Value loss: 0.145267. Entropy: 0.298770.\n",
      "Iteration 16374: Policy loss: -0.069189. Value loss: 0.075809. Entropy: 0.297820.\n",
      "Training network. lr: 0.000124. clip: 0.049782\n",
      "Iteration 16375: Policy loss: 0.151706. Value loss: 0.164727. Entropy: 0.300056.\n",
      "Iteration 16376: Policy loss: 0.152484. Value loss: 0.077556. Entropy: 0.300238.\n",
      "Iteration 16377: Policy loss: 0.145663. Value loss: 0.053020. Entropy: 0.298962.\n",
      "episode: 5758   score: 180.0  epsilon: 1.0    steps: 168  evaluation reward: 437.8\n",
      "Training network. lr: 0.000124. clip: 0.049782\n",
      "Iteration 16378: Policy loss: -0.272151. Value loss: 0.519763. Entropy: 0.294133.\n",
      "Iteration 16379: Policy loss: -0.274434. Value loss: 0.152938. Entropy: 0.295012.\n",
      "Iteration 16380: Policy loss: -0.288385. Value loss: 0.090656. Entropy: 0.293642.\n",
      "episode: 5759   score: 185.0  epsilon: 1.0    steps: 504  evaluation reward: 436.1\n",
      "episode: 5760   score: 180.0  epsilon: 1.0    steps: 632  evaluation reward: 435.5\n",
      "Training network. lr: 0.000124. clip: 0.049782\n",
      "Iteration 16381: Policy loss: 0.119020. Value loss: 0.157935. Entropy: 0.291363.\n",
      "Iteration 16382: Policy loss: 0.106622. Value loss: 0.061114. Entropy: 0.292477.\n",
      "Iteration 16383: Policy loss: 0.100495. Value loss: 0.036855. Entropy: 0.292962.\n",
      "episode: 5761   score: 320.0  epsilon: 1.0    steps: 688  evaluation reward: 436.6\n",
      "Training network. lr: 0.000124. clip: 0.049782\n",
      "Iteration 16384: Policy loss: 0.285935. Value loss: 0.183481. Entropy: 0.297901.\n",
      "Iteration 16385: Policy loss: 0.275435. Value loss: 0.076458. Entropy: 0.297864.\n",
      "Iteration 16386: Policy loss: 0.286157. Value loss: 0.046064. Entropy: 0.299290.\n",
      "episode: 5762   score: 845.0  epsilon: 1.0    steps: 800  evaluation reward: 440.8\n",
      "Training network. lr: 0.000124. clip: 0.049782\n",
      "Iteration 16387: Policy loss: 0.274068. Value loss: 0.157832. Entropy: 0.296794.\n",
      "Iteration 16388: Policy loss: 0.260528. Value loss: 0.047354. Entropy: 0.295589.\n",
      "Iteration 16389: Policy loss: 0.255005. Value loss: 0.031983. Entropy: 0.296265.\n",
      "Training network. lr: 0.000124. clip: 0.049782\n",
      "Iteration 16390: Policy loss: -0.452606. Value loss: 0.383513. Entropy: 0.309368.\n",
      "Iteration 16391: Policy loss: -0.443364. Value loss: 0.204436. Entropy: 0.309416.\n",
      "Iteration 16392: Policy loss: -0.435344. Value loss: 0.138233. Entropy: 0.309934.\n",
      "episode: 5763   score: 180.0  epsilon: 1.0    steps: 576  evaluation reward: 438.7\n",
      "Training network. lr: 0.000124. clip: 0.049782\n",
      "Iteration 16393: Policy loss: 0.073989. Value loss: 0.072384. Entropy: 0.291601.\n",
      "Iteration 16394: Policy loss: 0.076455. Value loss: 0.034433. Entropy: 0.291104.\n",
      "Iteration 16395: Policy loss: 0.072132. Value loss: 0.024457. Entropy: 0.290593.\n",
      "episode: 5764   score: 665.0  epsilon: 1.0    steps: 48  evaluation reward: 442.0\n",
      "episode: 5765   score: 745.0  epsilon: 1.0    steps: 768  evaluation reward: 442.8\n",
      "Training network. lr: 0.000124. clip: 0.049782\n",
      "Iteration 16396: Policy loss: -0.098195. Value loss: 0.150258. Entropy: 0.287942.\n",
      "Iteration 16397: Policy loss: -0.100395. Value loss: 0.060666. Entropy: 0.288150.\n",
      "Iteration 16398: Policy loss: -0.107261. Value loss: 0.037926. Entropy: 0.288184.\n",
      "Training network. lr: 0.000124. clip: 0.049782\n",
      "Iteration 16399: Policy loss: 0.252070. Value loss: 0.237487. Entropy: 0.309484.\n",
      "Iteration 16400: Policy loss: 0.231324. Value loss: 0.091822. Entropy: 0.309796.\n",
      "Iteration 16401: Policy loss: 0.223860. Value loss: 0.062975. Entropy: 0.309304.\n",
      "Training network. lr: 0.000124. clip: 0.049625\n",
      "Iteration 16402: Policy loss: 0.192614. Value loss: 0.106462. Entropy: 0.304999.\n",
      "Iteration 16403: Policy loss: 0.189392. Value loss: 0.047747. Entropy: 0.306442.\n",
      "Iteration 16404: Policy loss: 0.185722. Value loss: 0.034585. Entropy: 0.305659.\n",
      "episode: 5766   score: 560.0  epsilon: 1.0    steps: 248  evaluation reward: 446.55\n",
      "episode: 5767   score: 155.0  epsilon: 1.0    steps: 848  evaluation reward: 444.65\n",
      "Training network. lr: 0.000124. clip: 0.049625\n",
      "Iteration 16405: Policy loss: 0.162633. Value loss: 0.118399. Entropy: 0.298165.\n",
      "Iteration 16406: Policy loss: 0.161021. Value loss: 0.044841. Entropy: 0.297471.\n",
      "Iteration 16407: Policy loss: 0.158735. Value loss: 0.032536. Entropy: 0.297652.\n",
      "Training network. lr: 0.000124. clip: 0.049625\n",
      "Iteration 16408: Policy loss: 0.086919. Value loss: 0.132174. Entropy: 0.311596.\n",
      "Iteration 16409: Policy loss: 0.079195. Value loss: 0.053442. Entropy: 0.311300.\n",
      "Iteration 16410: Policy loss: 0.076008. Value loss: 0.036337. Entropy: 0.310889.\n",
      "episode: 5768   score: 270.0  epsilon: 1.0    steps: 240  evaluation reward: 443.15\n",
      "episode: 5769   score: 850.0  epsilon: 1.0    steps: 960  evaluation reward: 446.85\n",
      "Training network. lr: 0.000124. clip: 0.049625\n",
      "Iteration 16411: Policy loss: -0.101278. Value loss: 0.156602. Entropy: 0.287865.\n",
      "Iteration 16412: Policy loss: -0.112563. Value loss: 0.095771. Entropy: 0.286719.\n",
      "Iteration 16413: Policy loss: -0.115126. Value loss: 0.068273. Entropy: 0.287398.\n",
      "episode: 5770   score: 215.0  epsilon: 1.0    steps: 360  evaluation reward: 445.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5771   score: 290.0  epsilon: 1.0    steps: 896  evaluation reward: 444.8\n",
      "Training network. lr: 0.000124. clip: 0.049625\n",
      "Iteration 16414: Policy loss: 0.452104. Value loss: 0.214438. Entropy: 0.300465.\n",
      "Iteration 16415: Policy loss: 0.452180. Value loss: 0.053311. Entropy: 0.297916.\n",
      "Iteration 16416: Policy loss: 0.451859. Value loss: 0.029737. Entropy: 0.299117.\n",
      "episode: 5772   score: 620.0  epsilon: 1.0    steps: 168  evaluation reward: 449.2\n",
      "Training network. lr: 0.000124. clip: 0.049625\n",
      "Iteration 16417: Policy loss: -0.644393. Value loss: 0.539465. Entropy: 0.297575.\n",
      "Iteration 16418: Policy loss: -0.629888. Value loss: 0.376635. Entropy: 0.297262.\n",
      "Iteration 16419: Policy loss: -0.663558. Value loss: 0.309292. Entropy: 0.295965.\n",
      "Training network. lr: 0.000124. clip: 0.049625\n",
      "Iteration 16420: Policy loss: -0.046937. Value loss: 0.186465. Entropy: 0.296969.\n",
      "Iteration 16421: Policy loss: -0.048818. Value loss: 0.083603. Entropy: 0.294872.\n",
      "Iteration 16422: Policy loss: -0.062667. Value loss: 0.060900. Entropy: 0.297595.\n",
      "Training network. lr: 0.000124. clip: 0.049625\n",
      "Iteration 16423: Policy loss: 0.451703. Value loss: 0.209192. Entropy: 0.298465.\n",
      "Iteration 16424: Policy loss: 0.453020. Value loss: 0.068999. Entropy: 0.297254.\n",
      "Iteration 16425: Policy loss: 0.449598. Value loss: 0.043687. Entropy: 0.297991.\n",
      "Training network. lr: 0.000124. clip: 0.049625\n",
      "Iteration 16426: Policy loss: -0.624526. Value loss: 0.500241. Entropy: 0.285081.\n",
      "Iteration 16427: Policy loss: -0.632691. Value loss: 0.327033. Entropy: 0.280800.\n",
      "Iteration 16428: Policy loss: -0.637078. Value loss: 0.189066. Entropy: 0.283280.\n",
      "episode: 5773   score: 210.0  epsilon: 1.0    steps: 560  evaluation reward: 446.35\n",
      "episode: 5774   score: 540.0  epsilon: 1.0    steps: 888  evaluation reward: 449.95\n",
      "Training network. lr: 0.000124. clip: 0.049625\n",
      "Iteration 16429: Policy loss: 0.070770. Value loss: 0.205439. Entropy: 0.297649.\n",
      "Iteration 16430: Policy loss: 0.058275. Value loss: 0.120932. Entropy: 0.298249.\n",
      "Iteration 16431: Policy loss: 0.061429. Value loss: 0.092926. Entropy: 0.297973.\n",
      "episode: 5775   score: 580.0  epsilon: 1.0    steps: 136  evaluation reward: 452.4\n",
      "episode: 5776   score: 255.0  epsilon: 1.0    steps: 256  evaluation reward: 450.75\n",
      "Training network. lr: 0.000124. clip: 0.049625\n",
      "Iteration 16432: Policy loss: 0.017387. Value loss: 0.099609. Entropy: 0.304507.\n",
      "Iteration 16433: Policy loss: 0.017711. Value loss: 0.049509. Entropy: 0.305634.\n",
      "Iteration 16434: Policy loss: 0.020649. Value loss: 0.038818. Entropy: 0.304369.\n",
      "Training network. lr: 0.000124. clip: 0.049625\n",
      "Iteration 16435: Policy loss: -0.264910. Value loss: 0.304565. Entropy: 0.306659.\n",
      "Iteration 16436: Policy loss: -0.268002. Value loss: 0.124086. Entropy: 0.305985.\n",
      "Iteration 16437: Policy loss: -0.288673. Value loss: 0.089798. Entropy: 0.305317.\n",
      "episode: 5777   score: 745.0  epsilon: 1.0    steps: 808  evaluation reward: 456.65\n",
      "Training network. lr: 0.000124. clip: 0.049625\n",
      "Iteration 16438: Policy loss: 0.235626. Value loss: 0.208438. Entropy: 0.288187.\n",
      "Iteration 16439: Policy loss: 0.232249. Value loss: 0.091774. Entropy: 0.288781.\n",
      "Iteration 16440: Policy loss: 0.219300. Value loss: 0.065055. Entropy: 0.289150.\n",
      "episode: 5778   score: 465.0  epsilon: 1.0    steps: 368  evaluation reward: 459.2\n",
      "Training network. lr: 0.000124. clip: 0.049625\n",
      "Iteration 16441: Policy loss: 0.221374. Value loss: 0.254467. Entropy: 0.281464.\n",
      "Iteration 16442: Policy loss: 0.216796. Value loss: 0.072292. Entropy: 0.282762.\n",
      "Iteration 16443: Policy loss: 0.201846. Value loss: 0.040839. Entropy: 0.281313.\n",
      "Training network. lr: 0.000124. clip: 0.049625\n",
      "Iteration 16444: Policy loss: 0.225687. Value loss: 0.121234. Entropy: 0.301965.\n",
      "Iteration 16445: Policy loss: 0.226736. Value loss: 0.049862. Entropy: 0.302218.\n",
      "Iteration 16446: Policy loss: 0.217158. Value loss: 0.034038. Entropy: 0.300246.\n",
      "episode: 5779   score: 630.0  epsilon: 1.0    steps: 728  evaluation reward: 462.1\n",
      "Training network. lr: 0.000124. clip: 0.049625\n",
      "Iteration 16447: Policy loss: -0.339974. Value loss: 0.287062. Entropy: 0.301798.\n",
      "Iteration 16448: Policy loss: -0.327492. Value loss: 0.114744. Entropy: 0.301753.\n",
      "Iteration 16449: Policy loss: -0.335044. Value loss: 0.086583. Entropy: 0.301796.\n",
      "episode: 5780   score: 285.0  epsilon: 1.0    steps: 840  evaluation reward: 459.15\n",
      "Training network. lr: 0.000124. clip: 0.049625\n",
      "Iteration 16450: Policy loss: 0.372942. Value loss: 0.187575. Entropy: 0.295942.\n",
      "Iteration 16451: Policy loss: 0.358682. Value loss: 0.072770. Entropy: 0.293429.\n",
      "Iteration 16452: Policy loss: 0.353691. Value loss: 0.051685. Entropy: 0.292846.\n",
      "episode: 5781   score: 330.0  epsilon: 1.0    steps: 424  evaluation reward: 458.05\n",
      "Training network. lr: 0.000124. clip: 0.049469\n",
      "Iteration 16453: Policy loss: 0.009372. Value loss: 0.109415. Entropy: 0.304737.\n",
      "Iteration 16454: Policy loss: 0.008166. Value loss: 0.053051. Entropy: 0.302788.\n",
      "Iteration 16455: Policy loss: -0.001628. Value loss: 0.036438. Entropy: 0.303846.\n",
      "episode: 5782   score: 450.0  epsilon: 1.0    steps: 376  evaluation reward: 456.9\n",
      "episode: 5783   score: 295.0  epsilon: 1.0    steps: 600  evaluation reward: 456.55\n",
      "Training network. lr: 0.000124. clip: 0.049469\n",
      "Iteration 16456: Policy loss: 0.235370. Value loss: 0.152005. Entropy: 0.302515.\n",
      "Iteration 16457: Policy loss: 0.227291. Value loss: 0.066362. Entropy: 0.300653.\n",
      "Iteration 16458: Policy loss: 0.226317. Value loss: 0.042566. Entropy: 0.301565.\n",
      "episode: 5784   score: 875.0  epsilon: 1.0    steps: 136  evaluation reward: 462.45\n",
      "Training network. lr: 0.000124. clip: 0.049469\n",
      "Iteration 16459: Policy loss: 0.162044. Value loss: 0.185908. Entropy: 0.307466.\n",
      "Iteration 16460: Policy loss: 0.161892. Value loss: 0.080581. Entropy: 0.307236.\n",
      "Iteration 16461: Policy loss: 0.148379. Value loss: 0.055222. Entropy: 0.305558.\n",
      "Training network. lr: 0.000124. clip: 0.049469\n",
      "Iteration 16462: Policy loss: 0.050583. Value loss: 0.088068. Entropy: 0.295647.\n",
      "Iteration 16463: Policy loss: 0.049501. Value loss: 0.045793. Entropy: 0.294837.\n",
      "Iteration 16464: Policy loss: 0.044826. Value loss: 0.032278. Entropy: 0.293702.\n",
      "Training network. lr: 0.000124. clip: 0.049469\n",
      "Iteration 16465: Policy loss: -0.041328. Value loss: 0.108804. Entropy: 0.285424.\n",
      "Iteration 16466: Policy loss: -0.041276. Value loss: 0.043128. Entropy: 0.284336.\n",
      "Iteration 16467: Policy loss: -0.059598. Value loss: 0.025841. Entropy: 0.283448.\n",
      "episode: 5785   score: 315.0  epsilon: 1.0    steps: 264  evaluation reward: 459.4\n",
      "Training network. lr: 0.000124. clip: 0.049469\n",
      "Iteration 16468: Policy loss: -0.112964. Value loss: 0.249697. Entropy: 0.296276.\n",
      "Iteration 16469: Policy loss: -0.119901. Value loss: 0.172815. Entropy: 0.295698.\n",
      "Iteration 16470: Policy loss: -0.124704. Value loss: 0.144499. Entropy: 0.294848.\n",
      "episode: 5786   score: 530.0  epsilon: 1.0    steps: 648  evaluation reward: 461.3\n",
      "episode: 5787   score: 650.0  epsilon: 1.0    steps: 1000  evaluation reward: 462.05\n",
      "episode: 5788   score: 245.0  epsilon: 1.0    steps: 1024  evaluation reward: 459.6\n",
      "Training network. lr: 0.000124. clip: 0.049469\n",
      "Iteration 16471: Policy loss: 0.220425. Value loss: 0.158196. Entropy: 0.300099.\n",
      "Iteration 16472: Policy loss: 0.207888. Value loss: 0.069855. Entropy: 0.299691.\n",
      "Iteration 16473: Policy loss: 0.209512. Value loss: 0.042428. Entropy: 0.301019.\n",
      "Training network. lr: 0.000124. clip: 0.049469\n",
      "Iteration 16474: Policy loss: -0.013561. Value loss: 0.083211. Entropy: 0.314990.\n",
      "Iteration 16475: Policy loss: -0.018086. Value loss: 0.037600. Entropy: 0.314285.\n",
      "Iteration 16476: Policy loss: -0.017832. Value loss: 0.026866. Entropy: 0.314666.\n",
      "episode: 5789   score: 395.0  epsilon: 1.0    steps: 8  evaluation reward: 454.75\n",
      "episode: 5790   score: 315.0  epsilon: 1.0    steps: 320  evaluation reward: 453.7\n",
      "Training network. lr: 0.000124. clip: 0.049469\n",
      "Iteration 16477: Policy loss: 0.046032. Value loss: 0.070670. Entropy: 0.308592.\n",
      "Iteration 16478: Policy loss: 0.049417. Value loss: 0.034316. Entropy: 0.309452.\n",
      "Iteration 16479: Policy loss: 0.041646. Value loss: 0.024220. Entropy: 0.308160.\n",
      "Training network. lr: 0.000124. clip: 0.049469\n",
      "Iteration 16480: Policy loss: 0.107743. Value loss: 0.068713. Entropy: 0.276909.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16481: Policy loss: 0.098507. Value loss: 0.035040. Entropy: 0.274232.\n",
      "Iteration 16482: Policy loss: 0.098798. Value loss: 0.029518. Entropy: 0.273477.\n",
      "Training network. lr: 0.000124. clip: 0.049469\n",
      "Iteration 16483: Policy loss: 0.127530. Value loss: 0.372279. Entropy: 0.295507.\n",
      "Iteration 16484: Policy loss: 0.127687. Value loss: 0.237803. Entropy: 0.293859.\n",
      "Iteration 16485: Policy loss: 0.131487. Value loss: 0.210136. Entropy: 0.293395.\n",
      "Training network. lr: 0.000124. clip: 0.049469\n",
      "Iteration 16486: Policy loss: 0.152058. Value loss: 0.144723. Entropy: 0.301217.\n",
      "Iteration 16487: Policy loss: 0.159872. Value loss: 0.052428. Entropy: 0.302154.\n",
      "Iteration 16488: Policy loss: 0.149867. Value loss: 0.035580. Entropy: 0.301278.\n",
      "episode: 5791   score: 210.0  epsilon: 1.0    steps: 48  evaluation reward: 452.45\n",
      "episode: 5792   score: 670.0  epsilon: 1.0    steps: 848  evaluation reward: 455.1\n",
      "episode: 5793   score: 535.0  epsilon: 1.0    steps: 888  evaluation reward: 454.75\n",
      "Training network. lr: 0.000124. clip: 0.049469\n",
      "Iteration 16489: Policy loss: 0.170667. Value loss: 0.110151. Entropy: 0.307627.\n",
      "Iteration 16490: Policy loss: 0.158593. Value loss: 0.044400. Entropy: 0.306375.\n",
      "Iteration 16491: Policy loss: 0.157856. Value loss: 0.032765. Entropy: 0.306519.\n",
      "episode: 5794   score: 210.0  epsilon: 1.0    steps: 504  evaluation reward: 453.2\n",
      "Training network. lr: 0.000124. clip: 0.049469\n",
      "Iteration 16492: Policy loss: 0.058305. Value loss: 0.091089. Entropy: 0.309060.\n",
      "Iteration 16493: Policy loss: 0.045898. Value loss: 0.038390. Entropy: 0.308479.\n",
      "Iteration 16494: Policy loss: 0.052542. Value loss: 0.027812. Entropy: 0.308598.\n",
      "Training network. lr: 0.000124. clip: 0.049469\n",
      "Iteration 16495: Policy loss: 0.077549. Value loss: 0.072606. Entropy: 0.297779.\n",
      "Iteration 16496: Policy loss: 0.077345. Value loss: 0.039982. Entropy: 0.298080.\n",
      "Iteration 16497: Policy loss: 0.075948. Value loss: 0.028446. Entropy: 0.298974.\n",
      "Training network. lr: 0.000124. clip: 0.049469\n",
      "Iteration 16498: Policy loss: -0.131910. Value loss: 0.324192. Entropy: 0.300810.\n",
      "Iteration 16499: Policy loss: -0.117248. Value loss: 0.195809. Entropy: 0.299794.\n",
      "Iteration 16500: Policy loss: -0.125516. Value loss: 0.144472. Entropy: 0.299131.\n",
      "Training network. lr: 0.000123. clip: 0.049321\n",
      "Iteration 16501: Policy loss: -0.181953. Value loss: 0.289488. Entropy: 0.307549.\n",
      "Iteration 16502: Policy loss: -0.201066. Value loss: 0.180067. Entropy: 0.307744.\n",
      "Iteration 16503: Policy loss: -0.221136. Value loss: 0.119207. Entropy: 0.307430.\n",
      "episode: 5795   score: 440.0  epsilon: 1.0    steps: 328  evaluation reward: 451.35\n",
      "episode: 5796   score: 345.0  epsilon: 1.0    steps: 1016  evaluation reward: 451.9\n",
      "Training network. lr: 0.000123. clip: 0.049321\n",
      "Iteration 16504: Policy loss: 0.171150. Value loss: 0.114330. Entropy: 0.306322.\n",
      "Iteration 16505: Policy loss: 0.162323. Value loss: 0.066568. Entropy: 0.306535.\n",
      "Iteration 16506: Policy loss: 0.153609. Value loss: 0.044685. Entropy: 0.306152.\n",
      "episode: 5797   score: 210.0  epsilon: 1.0    steps: 304  evaluation reward: 449.05\n",
      "episode: 5798   score: 210.0  epsilon: 1.0    steps: 336  evaluation reward: 446.7\n",
      "Training network. lr: 0.000123. clip: 0.049321\n",
      "Iteration 16507: Policy loss: -0.407631. Value loss: 0.313580. Entropy: 0.309545.\n",
      "Iteration 16508: Policy loss: -0.404803. Value loss: 0.209889. Entropy: 0.308927.\n",
      "Iteration 16509: Policy loss: -0.425401. Value loss: 0.181243. Entropy: 0.308044.\n",
      "episode: 5799   score: 385.0  epsilon: 1.0    steps: 976  evaluation reward: 447.05\n",
      "Training network. lr: 0.000123. clip: 0.049321\n",
      "Iteration 16510: Policy loss: -0.006759. Value loss: 0.137457. Entropy: 0.304966.\n",
      "Iteration 16511: Policy loss: -0.010719. Value loss: 0.061743. Entropy: 0.304385.\n",
      "Iteration 16512: Policy loss: -0.020588. Value loss: 0.045313. Entropy: 0.303914.\n",
      "episode: 5800   score: 640.0  epsilon: 1.0    steps: 416  evaluation reward: 448.0\n",
      "Training network. lr: 0.000123. clip: 0.049321\n",
      "Iteration 16513: Policy loss: 0.006956. Value loss: 0.103200. Entropy: 0.308370.\n",
      "Iteration 16514: Policy loss: 0.001816. Value loss: 0.050985. Entropy: 0.306876.\n",
      "Iteration 16515: Policy loss: 0.004855. Value loss: 0.032042. Entropy: 0.307315.\n",
      "Training network. lr: 0.000123. clip: 0.049321\n",
      "Iteration 16516: Policy loss: -0.336536. Value loss: 0.194245. Entropy: 0.309111.\n",
      "Iteration 16517: Policy loss: -0.357042. Value loss: 0.088303. Entropy: 0.309559.\n",
      "Iteration 16518: Policy loss: -0.354631. Value loss: 0.054819. Entropy: 0.309906.\n",
      "now time :  2019-09-06 07:20:13.053326\n",
      "episode: 5801   score: 370.0  epsilon: 1.0    steps: 888  evaluation reward: 445.55\n",
      "Training network. lr: 0.000123. clip: 0.049321\n",
      "Iteration 16519: Policy loss: 0.105990. Value loss: 0.105377. Entropy: 0.297932.\n",
      "Iteration 16520: Policy loss: 0.097469. Value loss: 0.044342. Entropy: 0.298422.\n",
      "Iteration 16521: Policy loss: 0.091528. Value loss: 0.029748. Entropy: 0.297384.\n",
      "episode: 5802   score: 600.0  epsilon: 1.0    steps: 576  evaluation reward: 445.75\n",
      "Training network. lr: 0.000123. clip: 0.049321\n",
      "Iteration 16522: Policy loss: 0.192588. Value loss: 0.139991. Entropy: 0.295498.\n",
      "Iteration 16523: Policy loss: 0.185709. Value loss: 0.052667. Entropy: 0.294430.\n",
      "Iteration 16524: Policy loss: 0.178779. Value loss: 0.033115. Entropy: 0.295484.\n",
      "episode: 5803   score: 180.0  epsilon: 1.0    steps: 432  evaluation reward: 443.95\n",
      "episode: 5804   score: 240.0  epsilon: 1.0    steps: 488  evaluation reward: 440.75\n",
      "Training network. lr: 0.000123. clip: 0.049321\n",
      "Iteration 16525: Policy loss: 0.089705. Value loss: 0.135603. Entropy: 0.293154.\n",
      "Iteration 16526: Policy loss: 0.085257. Value loss: 0.069058. Entropy: 0.288981.\n",
      "Iteration 16527: Policy loss: 0.082453. Value loss: 0.048906. Entropy: 0.288850.\n",
      "episode: 5805   score: 240.0  epsilon: 1.0    steps: 136  evaluation reward: 439.85\n",
      "Training network. lr: 0.000123. clip: 0.049321\n",
      "Iteration 16528: Policy loss: 0.098226. Value loss: 0.116084. Entropy: 0.302634.\n",
      "Iteration 16529: Policy loss: 0.093690. Value loss: 0.042965. Entropy: 0.303178.\n",
      "Iteration 16530: Policy loss: 0.089424. Value loss: 0.030788. Entropy: 0.302923.\n",
      "episode: 5806   score: 575.0  epsilon: 1.0    steps: 296  evaluation reward: 441.7\n",
      "Training network. lr: 0.000123. clip: 0.049321\n",
      "Iteration 16531: Policy loss: 0.040902. Value loss: 0.076993. Entropy: 0.299454.\n",
      "Iteration 16532: Policy loss: 0.034273. Value loss: 0.039890. Entropy: 0.298297.\n",
      "Iteration 16533: Policy loss: 0.031939. Value loss: 0.033337. Entropy: 0.299448.\n",
      "episode: 5807   score: 345.0  epsilon: 1.0    steps: 72  evaluation reward: 438.1\n",
      "Training network. lr: 0.000123. clip: 0.049321\n",
      "Iteration 16534: Policy loss: -0.089374. Value loss: 0.100082. Entropy: 0.289839.\n",
      "Iteration 16535: Policy loss: -0.103446. Value loss: 0.047436. Entropy: 0.290135.\n",
      "Iteration 16536: Policy loss: -0.101813. Value loss: 0.035383. Entropy: 0.289711.\n",
      "episode: 5808   score: 430.0  epsilon: 1.0    steps: 608  evaluation reward: 437.95\n",
      "Training network. lr: 0.000123. clip: 0.049321\n",
      "Iteration 16537: Policy loss: -0.068177. Value loss: 0.277224. Entropy: 0.294061.\n",
      "Iteration 16538: Policy loss: -0.095744. Value loss: 0.220431. Entropy: 0.292424.\n",
      "Iteration 16539: Policy loss: -0.103033. Value loss: 0.160149. Entropy: 0.290873.\n",
      "episode: 5809   score: 225.0  epsilon: 1.0    steps: 168  evaluation reward: 435.6\n",
      "Training network. lr: 0.000123. clip: 0.049321\n",
      "Iteration 16540: Policy loss: -0.547355. Value loss: 0.477290. Entropy: 0.297962.\n",
      "Iteration 16541: Policy loss: -0.545893. Value loss: 0.199759. Entropy: 0.296140.\n",
      "Iteration 16542: Policy loss: -0.565034. Value loss: 0.133002. Entropy: 0.298329.\n",
      "Training network. lr: 0.000123. clip: 0.049321\n",
      "Iteration 16543: Policy loss: 0.154009. Value loss: 0.164654. Entropy: 0.306944.\n",
      "Iteration 16544: Policy loss: 0.148851. Value loss: 0.071765. Entropy: 0.306757.\n",
      "Iteration 16545: Policy loss: 0.141532. Value loss: 0.051038. Entropy: 0.306028.\n",
      "episode: 5810   score: 345.0  epsilon: 1.0    steps: 584  evaluation reward: 432.05\n",
      "episode: 5811   score: 330.0  epsilon: 1.0    steps: 976  evaluation reward: 433.55\n",
      "Training network. lr: 0.000123. clip: 0.049321\n",
      "Iteration 16546: Policy loss: 0.172504. Value loss: 0.108663. Entropy: 0.283058.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16547: Policy loss: 0.171056. Value loss: 0.052003. Entropy: 0.283797.\n",
      "Iteration 16548: Policy loss: 0.174368. Value loss: 0.039796. Entropy: 0.284028.\n",
      "Training network. lr: 0.000123. clip: 0.049321\n",
      "Iteration 16549: Policy loss: -0.159585. Value loss: 0.158843. Entropy: 0.298652.\n",
      "Iteration 16550: Policy loss: -0.154780. Value loss: 0.061553. Entropy: 0.297975.\n",
      "Iteration 16551: Policy loss: -0.169998. Value loss: 0.038915. Entropy: 0.299050.\n",
      "Training network. lr: 0.000123. clip: 0.049165\n",
      "Iteration 16552: Policy loss: 0.073421. Value loss: 0.158672. Entropy: 0.300524.\n",
      "Iteration 16553: Policy loss: 0.065548. Value loss: 0.065520. Entropy: 0.300190.\n",
      "Iteration 16554: Policy loss: 0.062824. Value loss: 0.041532. Entropy: 0.300124.\n",
      "episode: 5812   score: 520.0  epsilon: 1.0    steps: 488  evaluation reward: 430.55\n",
      "episode: 5813   score: 555.0  epsilon: 1.0    steps: 584  evaluation reward: 429.9\n",
      "Training network. lr: 0.000123. clip: 0.049165\n",
      "Iteration 16555: Policy loss: 0.368932. Value loss: 0.131816. Entropy: 0.284195.\n",
      "Iteration 16556: Policy loss: 0.367832. Value loss: 0.052198. Entropy: 0.285282.\n",
      "Iteration 16557: Policy loss: 0.368922. Value loss: 0.038601. Entropy: 0.285986.\n",
      "episode: 5814   score: 625.0  epsilon: 1.0    steps: 376  evaluation reward: 431.65\n",
      "episode: 5815   score: 340.0  epsilon: 1.0    steps: 944  evaluation reward: 427.8\n",
      "Training network. lr: 0.000123. clip: 0.049165\n",
      "Iteration 16558: Policy loss: 0.159353. Value loss: 0.090306. Entropy: 0.297407.\n",
      "Iteration 16559: Policy loss: 0.148925. Value loss: 0.041000. Entropy: 0.297920.\n",
      "Iteration 16560: Policy loss: 0.149133. Value loss: 0.028427. Entropy: 0.295519.\n",
      "episode: 5816   score: 125.0  epsilon: 1.0    steps: 64  evaluation reward: 426.65\n",
      "episode: 5817   score: 520.0  epsilon: 1.0    steps: 168  evaluation reward: 424.75\n",
      "Training network. lr: 0.000123. clip: 0.049165\n",
      "Iteration 16561: Policy loss: -0.018636. Value loss: 0.106982. Entropy: 0.291090.\n",
      "Iteration 16562: Policy loss: -0.030548. Value loss: 0.048360. Entropy: 0.290382.\n",
      "Iteration 16563: Policy loss: -0.024832. Value loss: 0.035118. Entropy: 0.289335.\n",
      "episode: 5818   score: 545.0  epsilon: 1.0    steps: 472  evaluation reward: 426.65\n",
      "Training network. lr: 0.000123. clip: 0.049165\n",
      "Iteration 16564: Policy loss: 0.258777. Value loss: 0.164887. Entropy: 0.289742.\n",
      "Iteration 16565: Policy loss: 0.260110. Value loss: 0.069507. Entropy: 0.291603.\n",
      "Iteration 16566: Policy loss: 0.243993. Value loss: 0.059075. Entropy: 0.292810.\n",
      "Training network. lr: 0.000123. clip: 0.049165\n",
      "Iteration 16567: Policy loss: -0.101825. Value loss: 0.236321. Entropy: 0.292454.\n",
      "Iteration 16568: Policy loss: -0.115518. Value loss: 0.151711. Entropy: 0.291036.\n",
      "Iteration 16569: Policy loss: -0.128599. Value loss: 0.118922. Entropy: 0.290989.\n",
      "episode: 5819   score: 155.0  epsilon: 1.0    steps: 432  evaluation reward: 425.0\n",
      "Training network. lr: 0.000123. clip: 0.049165\n",
      "Iteration 16570: Policy loss: -0.026263. Value loss: 0.139365. Entropy: 0.299989.\n",
      "Iteration 16571: Policy loss: -0.030260. Value loss: 0.044972. Entropy: 0.299976.\n",
      "Iteration 16572: Policy loss: -0.036777. Value loss: 0.031953. Entropy: 0.299088.\n",
      "Training network. lr: 0.000123. clip: 0.049165\n",
      "Iteration 16573: Policy loss: 0.185967. Value loss: 0.128883. Entropy: 0.302526.\n",
      "Iteration 16574: Policy loss: 0.172826. Value loss: 0.053732. Entropy: 0.301422.\n",
      "Iteration 16575: Policy loss: 0.170928. Value loss: 0.037814. Entropy: 0.302552.\n",
      "Training network. lr: 0.000123. clip: 0.049165\n",
      "Iteration 16576: Policy loss: 0.083307. Value loss: 0.234022. Entropy: 0.311777.\n",
      "Iteration 16577: Policy loss: 0.085043. Value loss: 0.149484. Entropy: 0.311219.\n",
      "Iteration 16578: Policy loss: 0.071219. Value loss: 0.085875. Entropy: 0.311101.\n",
      "episode: 5820   score: 305.0  epsilon: 1.0    steps: 832  evaluation reward: 424.45\n",
      "Training network. lr: 0.000123. clip: 0.049165\n",
      "Iteration 16579: Policy loss: 0.288219. Value loss: 0.161101. Entropy: 0.292451.\n",
      "Iteration 16580: Policy loss: 0.281040. Value loss: 0.053395. Entropy: 0.291782.\n",
      "Iteration 16581: Policy loss: 0.284126. Value loss: 0.034226. Entropy: 0.293124.\n",
      "episode: 5821   score: 400.0  epsilon: 1.0    steps: 16  evaluation reward: 426.2\n",
      "episode: 5822   score: 355.0  epsilon: 1.0    steps: 296  evaluation reward: 423.25\n",
      "Training network. lr: 0.000123. clip: 0.049165\n",
      "Iteration 16582: Policy loss: 0.206591. Value loss: 0.062793. Entropy: 0.285870.\n",
      "Iteration 16583: Policy loss: 0.201506. Value loss: 0.034175. Entropy: 0.285947.\n",
      "Iteration 16584: Policy loss: 0.205352. Value loss: 0.026798. Entropy: 0.286459.\n",
      "episode: 5823   score: 390.0  epsilon: 1.0    steps: 48  evaluation reward: 423.0\n",
      "Training network. lr: 0.000123. clip: 0.049165\n",
      "Iteration 16585: Policy loss: 0.305803. Value loss: 0.087574. Entropy: 0.303017.\n",
      "Iteration 16586: Policy loss: 0.300760. Value loss: 0.045840. Entropy: 0.303373.\n",
      "Iteration 16587: Policy loss: 0.304220. Value loss: 0.034225. Entropy: 0.304490.\n",
      "episode: 5824   score: 180.0  epsilon: 1.0    steps: 168  evaluation reward: 422.2\n",
      "episode: 5825   score: 650.0  epsilon: 1.0    steps: 480  evaluation reward: 418.65\n",
      "Training network. lr: 0.000123. clip: 0.049165\n",
      "Iteration 16588: Policy loss: 0.076221. Value loss: 0.064206. Entropy: 0.301171.\n",
      "Iteration 16589: Policy loss: 0.075494. Value loss: 0.037462. Entropy: 0.302700.\n",
      "Iteration 16590: Policy loss: 0.071986. Value loss: 0.031826. Entropy: 0.301000.\n",
      "Training network. lr: 0.000123. clip: 0.049165\n",
      "Iteration 16591: Policy loss: 0.152438. Value loss: 0.095157. Entropy: 0.300920.\n",
      "Iteration 16592: Policy loss: 0.141882. Value loss: 0.041493. Entropy: 0.300621.\n",
      "Iteration 16593: Policy loss: 0.143911. Value loss: 0.027688. Entropy: 0.301614.\n",
      "episode: 5826   score: 440.0  epsilon: 1.0    steps: 584  evaluation reward: 420.65\n",
      "Training network. lr: 0.000123. clip: 0.049165\n",
      "Iteration 16594: Policy loss: -0.049241. Value loss: 0.125023. Entropy: 0.302425.\n",
      "Iteration 16595: Policy loss: -0.058136. Value loss: 0.083576. Entropy: 0.301748.\n",
      "Iteration 16596: Policy loss: -0.059118. Value loss: 0.071064. Entropy: 0.301733.\n",
      "episode: 5827   score: 100.0  epsilon: 1.0    steps: 648  evaluation reward: 416.05\n",
      "episode: 5828   score: 260.0  epsilon: 1.0    steps: 888  evaluation reward: 415.35\n",
      "Training network. lr: 0.000123. clip: 0.049165\n",
      "Iteration 16597: Policy loss: 0.065013. Value loss: 0.098105. Entropy: 0.300350.\n",
      "Iteration 16598: Policy loss: 0.059350. Value loss: 0.041389. Entropy: 0.301442.\n",
      "Iteration 16599: Policy loss: 0.058314. Value loss: 0.029743. Entropy: 0.300175.\n",
      "episode: 5829   score: 330.0  epsilon: 1.0    steps: 960  evaluation reward: 415.0\n",
      "Training network. lr: 0.000123. clip: 0.049165\n",
      "Iteration 16600: Policy loss: -0.123937. Value loss: 0.171856. Entropy: 0.307757.\n",
      "Iteration 16601: Policy loss: -0.118175. Value loss: 0.067648. Entropy: 0.308354.\n",
      "Iteration 16602: Policy loss: -0.139630. Value loss: 0.043581. Entropy: 0.308279.\n",
      "episode: 5830   score: 490.0  epsilon: 1.0    steps: 256  evaluation reward: 417.3\n",
      "Training network. lr: 0.000123. clip: 0.049008\n",
      "Iteration 16603: Policy loss: 0.030014. Value loss: 0.166302. Entropy: 0.292316.\n",
      "Iteration 16604: Policy loss: 0.012989. Value loss: 0.065233. Entropy: 0.291475.\n",
      "Iteration 16605: Policy loss: 0.011639. Value loss: 0.044355. Entropy: 0.290241.\n",
      "episode: 5831   score: 255.0  epsilon: 1.0    steps: 368  evaluation reward: 416.5\n",
      "Training network. lr: 0.000123. clip: 0.049008\n",
      "Iteration 16606: Policy loss: 0.098852. Value loss: 0.177544. Entropy: 0.297981.\n",
      "Iteration 16607: Policy loss: 0.087425. Value loss: 0.077507. Entropy: 0.298048.\n",
      "Iteration 16608: Policy loss: 0.079683. Value loss: 0.049692. Entropy: 0.298016.\n",
      "episode: 5832   score: 320.0  epsilon: 1.0    steps: 320  evaluation reward: 416.65\n",
      "Training network. lr: 0.000123. clip: 0.049008\n",
      "Iteration 16609: Policy loss: 0.077150. Value loss: 0.134079. Entropy: 0.302691.\n",
      "Iteration 16610: Policy loss: 0.063238. Value loss: 0.052586. Entropy: 0.303290.\n",
      "Iteration 16611: Policy loss: 0.064296. Value loss: 0.038079. Entropy: 0.302319.\n",
      "episode: 5833   score: 600.0  epsilon: 1.0    steps: 504  evaluation reward: 418.55\n",
      "Training network. lr: 0.000123. clip: 0.049008\n",
      "Iteration 16612: Policy loss: 0.029775. Value loss: 0.153763. Entropy: 0.298902.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16613: Policy loss: 0.018241. Value loss: 0.062434. Entropy: 0.298271.\n",
      "Iteration 16614: Policy loss: 0.017862. Value loss: 0.038428. Entropy: 0.298822.\n",
      "episode: 5834   score: 155.0  epsilon: 1.0    steps: 424  evaluation reward: 415.15\n",
      "episode: 5835   score: 320.0  epsilon: 1.0    steps: 456  evaluation reward: 411.8\n",
      "Training network. lr: 0.000123. clip: 0.049008\n",
      "Iteration 16615: Policy loss: 0.038361. Value loss: 0.171111. Entropy: 0.293205.\n",
      "Iteration 16616: Policy loss: 0.032948. Value loss: 0.049203. Entropy: 0.292998.\n",
      "Iteration 16617: Policy loss: 0.021773. Value loss: 0.034552. Entropy: 0.292691.\n",
      "Training network. lr: 0.000123. clip: 0.049008\n",
      "Iteration 16618: Policy loss: 0.239104. Value loss: 0.155424. Entropy: 0.301037.\n",
      "Iteration 16619: Policy loss: 0.230035. Value loss: 0.057848. Entropy: 0.300206.\n",
      "Iteration 16620: Policy loss: 0.226524. Value loss: 0.033395. Entropy: 0.299063.\n",
      "episode: 5836   score: 215.0  epsilon: 1.0    steps: 136  evaluation reward: 410.5\n",
      "episode: 5837   score: 210.0  epsilon: 1.0    steps: 592  evaluation reward: 408.45\n",
      "Training network. lr: 0.000123. clip: 0.049008\n",
      "Iteration 16621: Policy loss: -0.142701. Value loss: 0.271117. Entropy: 0.295227.\n",
      "Iteration 16622: Policy loss: -0.149154. Value loss: 0.108636. Entropy: 0.293475.\n",
      "Iteration 16623: Policy loss: -0.154977. Value loss: 0.067559. Entropy: 0.293205.\n",
      "episode: 5838   score: 360.0  epsilon: 1.0    steps: 200  evaluation reward: 408.25\n",
      "Training network. lr: 0.000123. clip: 0.049008\n",
      "Iteration 16624: Policy loss: 0.195765. Value loss: 0.149151. Entropy: 0.302321.\n",
      "Iteration 16625: Policy loss: 0.189224. Value loss: 0.051884. Entropy: 0.300331.\n",
      "Iteration 16626: Policy loss: 0.184486. Value loss: 0.035614. Entropy: 0.300271.\n",
      "episode: 5839   score: 210.0  epsilon: 1.0    steps: 800  evaluation reward: 407.15\n",
      "Training network. lr: 0.000123. clip: 0.049008\n",
      "Iteration 16627: Policy loss: -0.103069. Value loss: 0.118404. Entropy: 0.301759.\n",
      "Iteration 16628: Policy loss: -0.103203. Value loss: 0.052194. Entropy: 0.302165.\n",
      "Iteration 16629: Policy loss: -0.114228. Value loss: 0.038618. Entropy: 0.302183.\n",
      "episode: 5840   score: 210.0  epsilon: 1.0    steps: 296  evaluation reward: 407.45\n",
      "episode: 5841   score: 370.0  epsilon: 1.0    steps: 360  evaluation reward: 407.7\n",
      "Training network. lr: 0.000123. clip: 0.049008\n",
      "Iteration 16630: Policy loss: -0.174078. Value loss: 0.243191. Entropy: 0.289256.\n",
      "Iteration 16631: Policy loss: -0.187396. Value loss: 0.083378. Entropy: 0.288601.\n",
      "Iteration 16632: Policy loss: -0.190312. Value loss: 0.039774. Entropy: 0.288012.\n",
      "episode: 5842   score: 500.0  epsilon: 1.0    steps: 184  evaluation reward: 403.45\n",
      "episode: 5843   score: 110.0  epsilon: 1.0    steps: 200  evaluation reward: 398.95\n",
      "Training network. lr: 0.000123. clip: 0.049008\n",
      "Iteration 16633: Policy loss: -0.235357. Value loss: 0.219228. Entropy: 0.298671.\n",
      "Iteration 16634: Policy loss: -0.252597. Value loss: 0.134663. Entropy: 0.297727.\n",
      "Iteration 16635: Policy loss: -0.243056. Value loss: 0.101568. Entropy: 0.298369.\n",
      "Training network. lr: 0.000123. clip: 0.049008\n",
      "Iteration 16636: Policy loss: 0.042460. Value loss: 0.187586. Entropy: 0.306951.\n",
      "Iteration 16637: Policy loss: 0.031948. Value loss: 0.078606. Entropy: 0.306955.\n",
      "Iteration 16638: Policy loss: 0.026664. Value loss: 0.052742. Entropy: 0.308238.\n",
      "Training network. lr: 0.000123. clip: 0.049008\n",
      "Iteration 16639: Policy loss: 0.297055. Value loss: 0.203150. Entropy: 0.304332.\n",
      "Iteration 16640: Policy loss: 0.289393. Value loss: 0.072216. Entropy: 0.304669.\n",
      "Iteration 16641: Policy loss: 0.275224. Value loss: 0.045533. Entropy: 0.304478.\n",
      "episode: 5844   score: 330.0  epsilon: 1.0    steps: 120  evaluation reward: 398.65\n",
      "episode: 5845   score: 105.0  epsilon: 1.0    steps: 352  evaluation reward: 395.2\n",
      "Training network. lr: 0.000123. clip: 0.049008\n",
      "Iteration 16642: Policy loss: 0.463898. Value loss: 0.159894. Entropy: 0.288499.\n",
      "Iteration 16643: Policy loss: 0.456797. Value loss: 0.066310. Entropy: 0.289036.\n",
      "Iteration 16644: Policy loss: 0.445365. Value loss: 0.043501. Entropy: 0.290144.\n",
      "episode: 5846   score: 495.0  epsilon: 1.0    steps: 96  evaluation reward: 395.15\n",
      "episode: 5847   score: 165.0  epsilon: 1.0    steps: 640  evaluation reward: 392.05\n",
      "episode: 5848   score: 590.0  epsilon: 1.0    steps: 968  evaluation reward: 395.05\n",
      "Training network. lr: 0.000123. clip: 0.049008\n",
      "Iteration 16645: Policy loss: 0.098868. Value loss: 0.084067. Entropy: 0.288754.\n",
      "Iteration 16646: Policy loss: 0.094624. Value loss: 0.045074. Entropy: 0.291449.\n",
      "Iteration 16647: Policy loss: 0.091719. Value loss: 0.036712. Entropy: 0.291882.\n",
      "episode: 5849   score: 125.0  epsilon: 1.0    steps: 16  evaluation reward: 388.0\n",
      "Training network. lr: 0.000123. clip: 0.049008\n",
      "Iteration 16648: Policy loss: -0.067178. Value loss: 0.129604. Entropy: 0.302172.\n",
      "Iteration 16649: Policy loss: -0.082611. Value loss: 0.072423. Entropy: 0.301393.\n",
      "Iteration 16650: Policy loss: -0.080556. Value loss: 0.060237. Entropy: 0.303023.\n",
      "Training network. lr: 0.000122. clip: 0.048860\n",
      "Iteration 16651: Policy loss: -0.299864. Value loss: 0.409751. Entropy: 0.293906.\n",
      "Iteration 16652: Policy loss: -0.309497. Value loss: 0.200854. Entropy: 0.292713.\n",
      "Iteration 16653: Policy loss: -0.322563. Value loss: 0.113131. Entropy: 0.292340.\n",
      "Training network. lr: 0.000122. clip: 0.048860\n",
      "Iteration 16654: Policy loss: -0.254576. Value loss: 0.310257. Entropy: 0.298637.\n",
      "Iteration 16655: Policy loss: -0.255952. Value loss: 0.166349. Entropy: 0.294878.\n",
      "Iteration 16656: Policy loss: -0.278718. Value loss: 0.093049. Entropy: 0.295333.\n",
      "episode: 5850   score: 540.0  epsilon: 1.0    steps: 528  evaluation reward: 388.5\n",
      "Training network. lr: 0.000122. clip: 0.048860\n",
      "Iteration 16657: Policy loss: -0.288430. Value loss: 0.465319. Entropy: 0.296369.\n",
      "Iteration 16658: Policy loss: -0.349376. Value loss: 0.255345. Entropy: 0.298712.\n",
      "Iteration 16659: Policy loss: -0.352013. Value loss: 0.183911. Entropy: 0.297164.\n",
      "now time :  2019-09-06 07:28:58.114144\n",
      "episode: 5851   score: 225.0  epsilon: 1.0    steps: 984  evaluation reward: 384.85\n",
      "Training network. lr: 0.000122. clip: 0.048860\n",
      "Iteration 16660: Policy loss: -0.118797. Value loss: 0.212405. Entropy: 0.302314.\n",
      "Iteration 16661: Policy loss: -0.122891. Value loss: 0.101225. Entropy: 0.301866.\n",
      "Iteration 16662: Policy loss: -0.132175. Value loss: 0.074856. Entropy: 0.301834.\n",
      "Training network. lr: 0.000122. clip: 0.048860\n",
      "Iteration 16663: Policy loss: 0.301713. Value loss: 0.200570. Entropy: 0.312234.\n",
      "Iteration 16664: Policy loss: 0.290432. Value loss: 0.073987. Entropy: 0.312363.\n",
      "Iteration 16665: Policy loss: 0.293700. Value loss: 0.047571. Entropy: 0.311763.\n",
      "episode: 5852   score: 530.0  epsilon: 1.0    steps: 536  evaluation reward: 385.9\n",
      "Training network. lr: 0.000122. clip: 0.048860\n",
      "Iteration 16666: Policy loss: -0.038725. Value loss: 0.131363. Entropy: 0.301547.\n",
      "Iteration 16667: Policy loss: -0.042293. Value loss: 0.066135. Entropy: 0.302208.\n",
      "Iteration 16668: Policy loss: -0.048021. Value loss: 0.044676. Entropy: 0.301122.\n",
      "episode: 5853   score: 815.0  epsilon: 1.0    steps: 944  evaluation reward: 387.85\n",
      "Training network. lr: 0.000122. clip: 0.048860\n",
      "Iteration 16669: Policy loss: -0.316417. Value loss: 0.432927. Entropy: 0.294877.\n",
      "Iteration 16670: Policy loss: -0.332627. Value loss: 0.242485. Entropy: 0.295006.\n",
      "Iteration 16671: Policy loss: -0.371719. Value loss: 0.227274. Entropy: 0.295099.\n",
      "episode: 5854   score: 360.0  epsilon: 1.0    steps: 160  evaluation reward: 387.55\n",
      "Training network. lr: 0.000122. clip: 0.048860\n",
      "Iteration 16672: Policy loss: 0.365437. Value loss: 0.179154. Entropy: 0.284582.\n",
      "Iteration 16673: Policy loss: 0.353797. Value loss: 0.065127. Entropy: 0.285630.\n",
      "Iteration 16674: Policy loss: 0.361853. Value loss: 0.046284. Entropy: 0.283800.\n",
      "Training network. lr: 0.000122. clip: 0.048860\n",
      "Iteration 16675: Policy loss: 0.058251. Value loss: 0.203207. Entropy: 0.304689.\n",
      "Iteration 16676: Policy loss: 0.063071. Value loss: 0.099607. Entropy: 0.303525.\n",
      "Iteration 16677: Policy loss: 0.050508. Value loss: 0.068309. Entropy: 0.304045.\n",
      "episode: 5855   score: 260.0  epsilon: 1.0    steps: 8  evaluation reward: 384.75\n",
      "episode: 5856   score: 425.0  epsilon: 1.0    steps: 144  evaluation reward: 385.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000122. clip: 0.048860\n",
      "Iteration 16678: Policy loss: -0.091337. Value loss: 0.252169. Entropy: 0.290626.\n",
      "Iteration 16679: Policy loss: -0.098126. Value loss: 0.187891. Entropy: 0.287871.\n",
      "Iteration 16680: Policy loss: -0.108066. Value loss: 0.126815. Entropy: 0.288596.\n",
      "episode: 5857   score: 830.0  epsilon: 1.0    steps: 104  evaluation reward: 390.2\n",
      "episode: 5858   score: 290.0  epsilon: 1.0    steps: 1024  evaluation reward: 391.3\n",
      "Training network. lr: 0.000122. clip: 0.048860\n",
      "Iteration 16681: Policy loss: -0.046656. Value loss: 0.125581. Entropy: 0.303172.\n",
      "Iteration 16682: Policy loss: -0.058042. Value loss: 0.057367. Entropy: 0.302176.\n",
      "Iteration 16683: Policy loss: -0.050940. Value loss: 0.037736. Entropy: 0.302783.\n",
      "Training network. lr: 0.000122. clip: 0.048860\n",
      "Iteration 16684: Policy loss: -0.373376. Value loss: 0.325934. Entropy: 0.292003.\n",
      "Iteration 16685: Policy loss: -0.391003. Value loss: 0.113107. Entropy: 0.291318.\n",
      "Iteration 16686: Policy loss: -0.403379. Value loss: 0.075831. Entropy: 0.289557.\n",
      "episode: 5859   score: 800.0  epsilon: 1.0    steps: 640  evaluation reward: 397.45\n",
      "Training network. lr: 0.000122. clip: 0.048860\n",
      "Iteration 16687: Policy loss: 0.352120. Value loss: 0.298261. Entropy: 0.292855.\n",
      "Iteration 16688: Policy loss: 0.338181. Value loss: 0.161284. Entropy: 0.292580.\n",
      "Iteration 16689: Policy loss: 0.331071. Value loss: 0.113678. Entropy: 0.292138.\n",
      "episode: 5860   score: 265.0  epsilon: 1.0    steps: 1008  evaluation reward: 398.3\n",
      "Training network. lr: 0.000122. clip: 0.048860\n",
      "Iteration 16690: Policy loss: 0.258360. Value loss: 0.164015. Entropy: 0.308150.\n",
      "Iteration 16691: Policy loss: 0.261215. Value loss: 0.064962. Entropy: 0.307767.\n",
      "Iteration 16692: Policy loss: 0.262078. Value loss: 0.047395. Entropy: 0.307025.\n",
      "Training network. lr: 0.000122. clip: 0.048860\n",
      "Iteration 16693: Policy loss: 0.327708. Value loss: 0.173920. Entropy: 0.295155.\n",
      "Iteration 16694: Policy loss: 0.321847. Value loss: 0.068928. Entropy: 0.294566.\n",
      "Iteration 16695: Policy loss: 0.318007. Value loss: 0.048299. Entropy: 0.293740.\n",
      "episode: 5861   score: 620.0  epsilon: 1.0    steps: 688  evaluation reward: 401.3\n",
      "Training network. lr: 0.000122. clip: 0.048860\n",
      "Iteration 16696: Policy loss: -0.176970. Value loss: 0.421169. Entropy: 0.290545.\n",
      "Iteration 16697: Policy loss: -0.178908. Value loss: 0.215517. Entropy: 0.291572.\n",
      "Iteration 16698: Policy loss: -0.204496. Value loss: 0.133257. Entropy: 0.290057.\n",
      "episode: 5862   score: 290.0  epsilon: 1.0    steps: 128  evaluation reward: 395.75\n",
      "Training network. lr: 0.000122. clip: 0.048860\n",
      "Iteration 16699: Policy loss: 0.547078. Value loss: 0.268116. Entropy: 0.296918.\n",
      "Iteration 16700: Policy loss: 0.541057. Value loss: 0.091738. Entropy: 0.296292.\n",
      "Iteration 16701: Policy loss: 0.536272. Value loss: 0.055195. Entropy: 0.295281.\n",
      "episode: 5863   score: 180.0  epsilon: 1.0    steps: 48  evaluation reward: 395.75\n",
      "episode: 5864   score: 590.0  epsilon: 1.0    steps: 224  evaluation reward: 395.0\n",
      "episode: 5865   score: 620.0  epsilon: 1.0    steps: 520  evaluation reward: 393.75\n",
      "episode: 5866   score: 410.0  epsilon: 1.0    steps: 680  evaluation reward: 392.25\n",
      "Training network. lr: 0.000122. clip: 0.048704\n",
      "Iteration 16702: Policy loss: 0.279624. Value loss: 0.136613. Entropy: 0.283325.\n",
      "Iteration 16703: Policy loss: 0.267196. Value loss: 0.060302. Entropy: 0.285436.\n",
      "Iteration 16704: Policy loss: 0.263382. Value loss: 0.043741. Entropy: 0.283633.\n",
      "episode: 5867   score: 210.0  epsilon: 1.0    steps: 864  evaluation reward: 392.8\n",
      "Training network. lr: 0.000122. clip: 0.048704\n",
      "Iteration 16705: Policy loss: 0.072412. Value loss: 0.141700. Entropy: 0.308911.\n",
      "Iteration 16706: Policy loss: 0.056335. Value loss: 0.046756. Entropy: 0.307484.\n",
      "Iteration 16707: Policy loss: 0.064228. Value loss: 0.033318. Entropy: 0.308394.\n",
      "Training network. lr: 0.000122. clip: 0.048704\n",
      "Iteration 16708: Policy loss: 0.050482. Value loss: 0.116241. Entropy: 0.299633.\n",
      "Iteration 16709: Policy loss: 0.048937. Value loss: 0.058681. Entropy: 0.300780.\n",
      "Iteration 16710: Policy loss: 0.044078. Value loss: 0.044006. Entropy: 0.300937.\n",
      "episode: 5868   score: 285.0  epsilon: 1.0    steps: 728  evaluation reward: 392.95\n",
      "Training network. lr: 0.000122. clip: 0.048704\n",
      "Iteration 16711: Policy loss: 0.225773. Value loss: 0.209597. Entropy: 0.297266.\n",
      "Iteration 16712: Policy loss: 0.221857. Value loss: 0.065748. Entropy: 0.296494.\n",
      "Iteration 16713: Policy loss: 0.216753. Value loss: 0.041512. Entropy: 0.298555.\n",
      "episode: 5869   score: 155.0  epsilon: 1.0    steps: 680  evaluation reward: 386.0\n",
      "Training network. lr: 0.000122. clip: 0.048704\n",
      "Iteration 16714: Policy loss: 0.029220. Value loss: 0.216258. Entropy: 0.301042.\n",
      "Iteration 16715: Policy loss: 0.029048. Value loss: 0.079118. Entropy: 0.300311.\n",
      "Iteration 16716: Policy loss: 0.023285. Value loss: 0.059450. Entropy: 0.300973.\n",
      "Training network. lr: 0.000122. clip: 0.048704\n",
      "Iteration 16717: Policy loss: -0.154492. Value loss: 0.176291. Entropy: 0.309934.\n",
      "Iteration 16718: Policy loss: -0.155027. Value loss: 0.070391. Entropy: 0.310187.\n",
      "Iteration 16719: Policy loss: -0.166971. Value loss: 0.047178. Entropy: 0.310108.\n",
      "episode: 5870   score: 400.0  epsilon: 1.0    steps: 896  evaluation reward: 387.85\n",
      "Training network. lr: 0.000122. clip: 0.048704\n",
      "Iteration 16720: Policy loss: 0.186194. Value loss: 0.137615. Entropy: 0.302442.\n",
      "Iteration 16721: Policy loss: 0.172874. Value loss: 0.056664. Entropy: 0.300425.\n",
      "Iteration 16722: Policy loss: 0.180329. Value loss: 0.042654. Entropy: 0.300534.\n",
      "episode: 5871   score: 135.0  epsilon: 1.0    steps: 392  evaluation reward: 386.3\n",
      "episode: 5872   score: 325.0  epsilon: 1.0    steps: 648  evaluation reward: 383.35\n",
      "episode: 5873   score: 545.0  epsilon: 1.0    steps: 824  evaluation reward: 386.7\n",
      "Training network. lr: 0.000122. clip: 0.048704\n",
      "Iteration 16723: Policy loss: 0.052304. Value loss: 0.101060. Entropy: 0.289416.\n",
      "Iteration 16724: Policy loss: 0.052284. Value loss: 0.037331. Entropy: 0.286307.\n",
      "Iteration 16725: Policy loss: 0.044899. Value loss: 0.025896. Entropy: 0.288663.\n",
      "episode: 5874   score: 360.0  epsilon: 1.0    steps: 24  evaluation reward: 384.9\n",
      "episode: 5875   score: 375.0  epsilon: 1.0    steps: 680  evaluation reward: 382.85\n",
      "Training network. lr: 0.000122. clip: 0.048704\n",
      "Iteration 16726: Policy loss: 0.342226. Value loss: 0.160871. Entropy: 0.305352.\n",
      "Iteration 16727: Policy loss: 0.341237. Value loss: 0.066455. Entropy: 0.303941.\n",
      "Iteration 16728: Policy loss: 0.336458. Value loss: 0.045170. Entropy: 0.303422.\n",
      "episode: 5876   score: 325.0  epsilon: 1.0    steps: 296  evaluation reward: 383.55\n",
      "Training network. lr: 0.000122. clip: 0.048704\n",
      "Iteration 16729: Policy loss: -0.089716. Value loss: 0.133412. Entropy: 0.303412.\n",
      "Iteration 16730: Policy loss: -0.093270. Value loss: 0.062577. Entropy: 0.303312.\n",
      "Iteration 16731: Policy loss: -0.088441. Value loss: 0.044436. Entropy: 0.302592.\n",
      "Training network. lr: 0.000122. clip: 0.048704\n",
      "Iteration 16732: Policy loss: -0.150621. Value loss: 0.285975. Entropy: 0.303934.\n",
      "Iteration 16733: Policy loss: -0.141036. Value loss: 0.155708. Entropy: 0.301983.\n",
      "Iteration 16734: Policy loss: -0.152743. Value loss: 0.123539. Entropy: 0.304061.\n",
      "episode: 5877   score: 135.0  epsilon: 1.0    steps: 552  evaluation reward: 377.45\n",
      "Training network. lr: 0.000122. clip: 0.048704\n",
      "Iteration 16735: Policy loss: 0.162250. Value loss: 0.075169. Entropy: 0.295189.\n",
      "Iteration 16736: Policy loss: 0.165097. Value loss: 0.025642. Entropy: 0.295328.\n",
      "Iteration 16737: Policy loss: 0.152338. Value loss: 0.020534. Entropy: 0.294760.\n",
      "episode: 5878   score: 80.0  epsilon: 1.0    steps: 360  evaluation reward: 373.6\n",
      "episode: 5879   score: 315.0  epsilon: 1.0    steps: 616  evaluation reward: 370.45\n",
      "Training network. lr: 0.000122. clip: 0.048704\n",
      "Iteration 16738: Policy loss: -0.029236. Value loss: 0.113589. Entropy: 0.291306.\n",
      "Iteration 16739: Policy loss: -0.027035. Value loss: 0.042387. Entropy: 0.291556.\n",
      "Iteration 16740: Policy loss: -0.035384. Value loss: 0.033650. Entropy: 0.291857.\n",
      "Training network. lr: 0.000122. clip: 0.048704\n",
      "Iteration 16741: Policy loss: -0.196229. Value loss: 0.323558. Entropy: 0.311118.\n",
      "Iteration 16742: Policy loss: -0.200979. Value loss: 0.183368. Entropy: 0.313179.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16743: Policy loss: -0.201277. Value loss: 0.121057. Entropy: 0.312142.\n",
      "episode: 5880   score: 305.0  epsilon: 1.0    steps: 184  evaluation reward: 370.65\n",
      "episode: 5881   score: 515.0  epsilon: 1.0    steps: 544  evaluation reward: 372.5\n",
      "Training network. lr: 0.000122. clip: 0.048704\n",
      "Iteration 16744: Policy loss: 0.064560. Value loss: 0.131422. Entropy: 0.297973.\n",
      "Iteration 16745: Policy loss: 0.067771. Value loss: 0.062044. Entropy: 0.298518.\n",
      "Iteration 16746: Policy loss: 0.053540. Value loss: 0.046558. Entropy: 0.298025.\n",
      "episode: 5882   score: 485.0  epsilon: 1.0    steps: 648  evaluation reward: 372.85\n",
      "Training network. lr: 0.000122. clip: 0.048704\n",
      "Iteration 16747: Policy loss: -0.315049. Value loss: 0.375038. Entropy: 0.296583.\n",
      "Iteration 16748: Policy loss: -0.318110. Value loss: 0.160369. Entropy: 0.296799.\n",
      "Iteration 16749: Policy loss: -0.331933. Value loss: 0.089219. Entropy: 0.296861.\n",
      "episode: 5883   score: 590.0  epsilon: 1.0    steps: 512  evaluation reward: 375.8\n",
      "episode: 5884   score: 290.0  epsilon: 1.0    steps: 536  evaluation reward: 369.95\n",
      "Training network. lr: 0.000122. clip: 0.048704\n",
      "Iteration 16750: Policy loss: 0.170537. Value loss: 0.123081. Entropy: 0.297461.\n",
      "Iteration 16751: Policy loss: 0.155827. Value loss: 0.051988. Entropy: 0.295720.\n",
      "Iteration 16752: Policy loss: 0.164917. Value loss: 0.037388. Entropy: 0.294042.\n",
      "Training network. lr: 0.000121. clip: 0.048547\n",
      "Iteration 16753: Policy loss: -0.023597. Value loss: 0.098983. Entropy: 0.308303.\n",
      "Iteration 16754: Policy loss: -0.032801. Value loss: 0.045976. Entropy: 0.310303.\n",
      "Iteration 16755: Policy loss: -0.034667. Value loss: 0.034620. Entropy: 0.309308.\n",
      "episode: 5885   score: 320.0  epsilon: 1.0    steps: 496  evaluation reward: 370.0\n",
      "Training network. lr: 0.000121. clip: 0.048547\n",
      "Iteration 16756: Policy loss: -0.119210. Value loss: 0.093670. Entropy: 0.284784.\n",
      "Iteration 16757: Policy loss: -0.123755. Value loss: 0.041160. Entropy: 0.285774.\n",
      "Iteration 16758: Policy loss: -0.123881. Value loss: 0.028015. Entropy: 0.286530.\n",
      "episode: 5886   score: 285.0  epsilon: 1.0    steps: 584  evaluation reward: 367.55\n",
      "Training network. lr: 0.000121. clip: 0.048547\n",
      "Iteration 16759: Policy loss: 0.107315. Value loss: 0.101010. Entropy: 0.288311.\n",
      "Iteration 16760: Policy loss: 0.109496. Value loss: 0.044105. Entropy: 0.287434.\n",
      "Iteration 16761: Policy loss: 0.103822. Value loss: 0.030696. Entropy: 0.288463.\n",
      "episode: 5887   score: 625.0  epsilon: 1.0    steps: 824  evaluation reward: 367.3\n",
      "Training network. lr: 0.000121. clip: 0.048547\n",
      "Iteration 16762: Policy loss: 0.387273. Value loss: 0.162640. Entropy: 0.294928.\n",
      "Iteration 16763: Policy loss: 0.377112. Value loss: 0.045035. Entropy: 0.294059.\n",
      "Iteration 16764: Policy loss: 0.371561. Value loss: 0.027047. Entropy: 0.292845.\n",
      "episode: 5888   score: 325.0  epsilon: 1.0    steps: 368  evaluation reward: 368.1\n",
      "Training network. lr: 0.000121. clip: 0.048547\n",
      "Iteration 16765: Policy loss: 0.065746. Value loss: 0.215017. Entropy: 0.305224.\n",
      "Iteration 16766: Policy loss: 0.041606. Value loss: 0.141548. Entropy: 0.304839.\n",
      "Iteration 16767: Policy loss: 0.037871. Value loss: 0.121031. Entropy: 0.304833.\n",
      "episode: 5889   score: 215.0  epsilon: 1.0    steps: 784  evaluation reward: 366.3\n",
      "Training network. lr: 0.000121. clip: 0.048547\n",
      "Iteration 16768: Policy loss: -0.042415. Value loss: 0.376555. Entropy: 0.290965.\n",
      "Iteration 16769: Policy loss: -0.037336. Value loss: 0.121544. Entropy: 0.287887.\n",
      "Iteration 16770: Policy loss: -0.044301. Value loss: 0.056029. Entropy: 0.286930.\n",
      "episode: 5890   score: 210.0  epsilon: 1.0    steps: 200  evaluation reward: 365.25\n",
      "episode: 5891   score: 105.0  epsilon: 1.0    steps: 656  evaluation reward: 364.2\n",
      "Training network. lr: 0.000121. clip: 0.048547\n",
      "Iteration 16771: Policy loss: 0.006135. Value loss: 0.102585. Entropy: 0.288285.\n",
      "Iteration 16772: Policy loss: -0.000619. Value loss: 0.039095. Entropy: 0.288083.\n",
      "Iteration 16773: Policy loss: 0.005031. Value loss: 0.025936. Entropy: 0.287087.\n",
      "episode: 5892   score: 215.0  epsilon: 1.0    steps: 784  evaluation reward: 359.65\n",
      "Training network. lr: 0.000121. clip: 0.048547\n",
      "Iteration 16774: Policy loss: -0.172162. Value loss: 0.107103. Entropy: 0.299715.\n",
      "Iteration 16775: Policy loss: -0.179586. Value loss: 0.040310. Entropy: 0.299368.\n",
      "Iteration 16776: Policy loss: -0.180271. Value loss: 0.029584. Entropy: 0.298699.\n",
      "episode: 5893   score: 595.0  epsilon: 1.0    steps: 136  evaluation reward: 360.25\n",
      "episode: 5894   score: 575.0  epsilon: 1.0    steps: 872  evaluation reward: 363.9\n",
      "Training network. lr: 0.000121. clip: 0.048547\n",
      "Iteration 16777: Policy loss: 0.298246. Value loss: 0.265309. Entropy: 0.292699.\n",
      "Iteration 16778: Policy loss: 0.298377. Value loss: 0.085650. Entropy: 0.290706.\n",
      "Iteration 16779: Policy loss: 0.287446. Value loss: 0.053163. Entropy: 0.290670.\n",
      "Training network. lr: 0.000121. clip: 0.048547\n",
      "Iteration 16780: Policy loss: -0.081512. Value loss: 0.198604. Entropy: 0.296157.\n",
      "Iteration 16781: Policy loss: -0.085022. Value loss: 0.119578. Entropy: 0.297646.\n",
      "Iteration 16782: Policy loss: -0.098873. Value loss: 0.103146. Entropy: 0.294731.\n",
      "Training network. lr: 0.000121. clip: 0.048547\n",
      "Iteration 16783: Policy loss: 0.011199. Value loss: 0.146901. Entropy: 0.306051.\n",
      "Iteration 16784: Policy loss: 0.000342. Value loss: 0.080367. Entropy: 0.305963.\n",
      "Iteration 16785: Policy loss: 0.001501. Value loss: 0.050248. Entropy: 0.306345.\n",
      "episode: 5895   score: 260.0  epsilon: 1.0    steps: 456  evaluation reward: 362.1\n",
      "episode: 5896   score: 195.0  epsilon: 1.0    steps: 552  evaluation reward: 360.6\n",
      "Training network. lr: 0.000121. clip: 0.048547\n",
      "Iteration 16786: Policy loss: 0.098893. Value loss: 0.099414. Entropy: 0.290186.\n",
      "Iteration 16787: Policy loss: 0.100239. Value loss: 0.043930. Entropy: 0.292689.\n",
      "Iteration 16788: Policy loss: 0.091451. Value loss: 0.032097. Entropy: 0.290834.\n",
      "episode: 5897   score: 185.0  epsilon: 1.0    steps: 88  evaluation reward: 360.35\n",
      "Training network. lr: 0.000121. clip: 0.048547\n",
      "Iteration 16789: Policy loss: 0.047380. Value loss: 0.121670. Entropy: 0.309367.\n",
      "Iteration 16790: Policy loss: 0.034650. Value loss: 0.050368. Entropy: 0.310126.\n",
      "Iteration 16791: Policy loss: 0.035494. Value loss: 0.030475. Entropy: 0.310026.\n",
      "episode: 5898   score: 515.0  epsilon: 1.0    steps: 224  evaluation reward: 363.4\n",
      "episode: 5899   score: 490.0  epsilon: 1.0    steps: 792  evaluation reward: 364.45\n",
      "Training network. lr: 0.000121. clip: 0.048547\n",
      "Iteration 16792: Policy loss: -0.243784. Value loss: 0.312949. Entropy: 0.288988.\n",
      "Iteration 16793: Policy loss: -0.247149. Value loss: 0.187312. Entropy: 0.286982.\n",
      "Iteration 16794: Policy loss: -0.251022. Value loss: 0.133129. Entropy: 0.289703.\n",
      "episode: 5900   score: 260.0  epsilon: 1.0    steps: 200  evaluation reward: 360.65\n",
      "Training network. lr: 0.000121. clip: 0.048547\n",
      "Iteration 16795: Policy loss: -0.050321. Value loss: 0.123844. Entropy: 0.303520.\n",
      "Iteration 16796: Policy loss: -0.050448. Value loss: 0.049853. Entropy: 0.305347.\n",
      "Iteration 16797: Policy loss: -0.047792. Value loss: 0.034715. Entropy: 0.303517.\n",
      "Training network. lr: 0.000121. clip: 0.048547\n",
      "Iteration 16798: Policy loss: -0.146967. Value loss: 0.131254. Entropy: 0.291957.\n",
      "Iteration 16799: Policy loss: -0.160206. Value loss: 0.055765. Entropy: 0.289462.\n",
      "Iteration 16800: Policy loss: -0.165019. Value loss: 0.041423. Entropy: 0.290179.\n",
      "now time :  2019-09-06 07:37:39.731979\n",
      "episode: 5901   score: 440.0  epsilon: 1.0    steps: 160  evaluation reward: 361.35\n",
      "episode: 5902   score: 210.0  epsilon: 1.0    steps: 752  evaluation reward: 357.45\n",
      "Training network. lr: 0.000121. clip: 0.048400\n",
      "Iteration 16801: Policy loss: -0.211617. Value loss: 0.247362. Entropy: 0.263556.\n",
      "Iteration 16802: Policy loss: -0.218575. Value loss: 0.100618. Entropy: 0.267204.\n",
      "Iteration 16803: Policy loss: -0.228204. Value loss: 0.060015. Entropy: 0.264371.\n",
      "Training network. lr: 0.000121. clip: 0.048400\n",
      "Iteration 16804: Policy loss: -0.101997. Value loss: 0.318152. Entropy: 0.310783.\n",
      "Iteration 16805: Policy loss: -0.119630. Value loss: 0.164887. Entropy: 0.310246.\n",
      "Iteration 16806: Policy loss: -0.143374. Value loss: 0.112649. Entropy: 0.310784.\n",
      "episode: 5903   score: 465.0  epsilon: 1.0    steps: 976  evaluation reward: 360.3\n",
      "Training network. lr: 0.000121. clip: 0.048400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16807: Policy loss: 0.229956. Value loss: 0.130985. Entropy: 0.289038.\n",
      "Iteration 16808: Policy loss: 0.235185. Value loss: 0.058801. Entropy: 0.288350.\n",
      "Iteration 16809: Policy loss: 0.218724. Value loss: 0.047871. Entropy: 0.288165.\n",
      "Training network. lr: 0.000121. clip: 0.048400\n",
      "Iteration 16810: Policy loss: -0.312030. Value loss: 0.320109. Entropy: 0.295558.\n",
      "Iteration 16811: Policy loss: -0.344929. Value loss: 0.131819. Entropy: 0.295540.\n",
      "Iteration 16812: Policy loss: -0.346809. Value loss: 0.080724. Entropy: 0.295227.\n",
      "episode: 5904   score: 330.0  epsilon: 1.0    steps: 376  evaluation reward: 361.2\n",
      "episode: 5905   score: 700.0  epsilon: 1.0    steps: 680  evaluation reward: 365.8\n",
      "Training network. lr: 0.000121. clip: 0.048400\n",
      "Iteration 16813: Policy loss: -0.058550. Value loss: 0.154274. Entropy: 0.298080.\n",
      "Iteration 16814: Policy loss: -0.069529. Value loss: 0.077603. Entropy: 0.295569.\n",
      "Iteration 16815: Policy loss: -0.071092. Value loss: 0.054176. Entropy: 0.298063.\n",
      "episode: 5906   score: 530.0  epsilon: 1.0    steps: 288  evaluation reward: 365.35\n",
      "Training network. lr: 0.000121. clip: 0.048400\n",
      "Iteration 16816: Policy loss: -0.276097. Value loss: 0.168876. Entropy: 0.298049.\n",
      "Iteration 16817: Policy loss: -0.291838. Value loss: 0.110208. Entropy: 0.297023.\n",
      "Iteration 16818: Policy loss: -0.290853. Value loss: 0.096994. Entropy: 0.298020.\n",
      "episode: 5907   score: 350.0  epsilon: 1.0    steps: 32  evaluation reward: 365.4\n",
      "episode: 5908   score: 425.0  epsilon: 1.0    steps: 584  evaluation reward: 365.35\n",
      "Training network. lr: 0.000121. clip: 0.048400\n",
      "Iteration 16819: Policy loss: -0.002430. Value loss: 0.279424. Entropy: 0.280905.\n",
      "Iteration 16820: Policy loss: -0.009380. Value loss: 0.084616. Entropy: 0.280260.\n",
      "Iteration 16821: Policy loss: -0.033168. Value loss: 0.052974. Entropy: 0.280138.\n",
      "episode: 5909   score: 425.0  epsilon: 1.0    steps: 752  evaluation reward: 367.35\n",
      "Training network. lr: 0.000121. clip: 0.048400\n",
      "Iteration 16822: Policy loss: 0.463559. Value loss: 0.214496. Entropy: 0.298280.\n",
      "Iteration 16823: Policy loss: 0.462016. Value loss: 0.075274. Entropy: 0.299629.\n",
      "Iteration 16824: Policy loss: 0.450213. Value loss: 0.059495. Entropy: 0.298901.\n",
      "Training network. lr: 0.000121. clip: 0.048400\n",
      "Iteration 16825: Policy loss: 0.117413. Value loss: 0.145126. Entropy: 0.302490.\n",
      "Iteration 16826: Policy loss: 0.117655. Value loss: 0.059974. Entropy: 0.301932.\n",
      "Iteration 16827: Policy loss: 0.115686. Value loss: 0.043262. Entropy: 0.302402.\n",
      "episode: 5910   score: 545.0  epsilon: 1.0    steps: 1024  evaluation reward: 369.35\n",
      "Training network. lr: 0.000121. clip: 0.048400\n",
      "Iteration 16828: Policy loss: 0.067527. Value loss: 0.087599. Entropy: 0.293898.\n",
      "Iteration 16829: Policy loss: 0.061030. Value loss: 0.034081. Entropy: 0.294262.\n",
      "Iteration 16830: Policy loss: 0.060179. Value loss: 0.025444. Entropy: 0.293629.\n",
      "episode: 5911   score: 210.0  epsilon: 1.0    steps: 520  evaluation reward: 368.15\n",
      "episode: 5912   score: 315.0  epsilon: 1.0    steps: 960  evaluation reward: 366.1\n",
      "Training network. lr: 0.000121. clip: 0.048400\n",
      "Iteration 16831: Policy loss: 0.116525. Value loss: 0.280035. Entropy: 0.279654.\n",
      "Iteration 16832: Policy loss: 0.100654. Value loss: 0.097854. Entropy: 0.277539.\n",
      "Iteration 16833: Policy loss: 0.083267. Value loss: 0.059612. Entropy: 0.276710.\n",
      "episode: 5913   score: 390.0  epsilon: 1.0    steps: 232  evaluation reward: 364.45\n",
      "Training network. lr: 0.000121. clip: 0.048400\n",
      "Iteration 16834: Policy loss: 0.459990. Value loss: 0.151376. Entropy: 0.292339.\n",
      "Iteration 16835: Policy loss: 0.453798. Value loss: 0.056806. Entropy: 0.289424.\n",
      "Iteration 16836: Policy loss: 0.451319. Value loss: 0.038516. Entropy: 0.290893.\n",
      "Training network. lr: 0.000121. clip: 0.048400\n",
      "Iteration 16837: Policy loss: 0.061520. Value loss: 0.363964. Entropy: 0.307470.\n",
      "Iteration 16838: Policy loss: 0.067921. Value loss: 0.102685. Entropy: 0.307300.\n",
      "Iteration 16839: Policy loss: 0.066598. Value loss: 0.055726. Entropy: 0.306963.\n",
      "episode: 5914   score: 335.0  epsilon: 1.0    steps: 352  evaluation reward: 361.55\n",
      "episode: 5915   score: 255.0  epsilon: 1.0    steps: 640  evaluation reward: 360.7\n",
      "Training network. lr: 0.000121. clip: 0.048400\n",
      "Iteration 16840: Policy loss: 0.198431. Value loss: 0.134217. Entropy: 0.279198.\n",
      "Iteration 16841: Policy loss: 0.197197. Value loss: 0.059148. Entropy: 0.278269.\n",
      "Iteration 16842: Policy loss: 0.189614. Value loss: 0.040960. Entropy: 0.278720.\n",
      "episode: 5916   score: 440.0  epsilon: 1.0    steps: 480  evaluation reward: 363.85\n",
      "Training network. lr: 0.000121. clip: 0.048400\n",
      "Iteration 16843: Policy loss: 0.066521. Value loss: 0.086440. Entropy: 0.292285.\n",
      "Iteration 16844: Policy loss: 0.064986. Value loss: 0.041210. Entropy: 0.292592.\n",
      "Iteration 16845: Policy loss: 0.061110. Value loss: 0.030211. Entropy: 0.293474.\n",
      "episode: 5917   score: 390.0  epsilon: 1.0    steps: 760  evaluation reward: 362.55\n",
      "episode: 5918   score: 90.0  epsilon: 1.0    steps: 768  evaluation reward: 358.0\n",
      "Training network. lr: 0.000121. clip: 0.048400\n",
      "Iteration 16846: Policy loss: -0.062413. Value loss: 0.181600. Entropy: 0.272282.\n",
      "Iteration 16847: Policy loss: -0.070220. Value loss: 0.070884. Entropy: 0.270925.\n",
      "Iteration 16848: Policy loss: -0.085659. Value loss: 0.048024. Entropy: 0.270750.\n",
      "Training network. lr: 0.000121. clip: 0.048400\n",
      "Iteration 16849: Policy loss: -0.103379. Value loss: 0.256426. Entropy: 0.312906.\n",
      "Iteration 16850: Policy loss: -0.098350. Value loss: 0.092563. Entropy: 0.314170.\n",
      "Iteration 16851: Policy loss: -0.123161. Value loss: 0.066920. Entropy: 0.313150.\n",
      "episode: 5919   score: 315.0  epsilon: 1.0    steps: 176  evaluation reward: 359.6\n",
      "episode: 5920   score: 490.0  epsilon: 1.0    steps: 568  evaluation reward: 361.45\n",
      "episode: 5921   score: 320.0  epsilon: 1.0    steps: 648  evaluation reward: 360.65\n",
      "Training network. lr: 0.000121. clip: 0.048243\n",
      "Iteration 16852: Policy loss: 0.206429. Value loss: 0.161897. Entropy: 0.275002.\n",
      "Iteration 16853: Policy loss: 0.199042. Value loss: 0.067919. Entropy: 0.275940.\n",
      "Iteration 16854: Policy loss: 0.191633. Value loss: 0.049108. Entropy: 0.275667.\n",
      "Training network. lr: 0.000121. clip: 0.048243\n",
      "Iteration 16855: Policy loss: 0.358610. Value loss: 0.155891. Entropy: 0.311009.\n",
      "Iteration 16856: Policy loss: 0.341318. Value loss: 0.051965. Entropy: 0.313196.\n",
      "Iteration 16857: Policy loss: 0.348889. Value loss: 0.034756. Entropy: 0.312391.\n",
      "episode: 5922   score: 185.0  epsilon: 1.0    steps: 824  evaluation reward: 358.95\n",
      "Training network. lr: 0.000121. clip: 0.048243\n",
      "Iteration 16858: Policy loss: -0.065597. Value loss: 0.137574. Entropy: 0.282317.\n",
      "Iteration 16859: Policy loss: -0.072506. Value loss: 0.069525. Entropy: 0.282290.\n",
      "Iteration 16860: Policy loss: -0.071256. Value loss: 0.050814. Entropy: 0.281971.\n",
      "Training network. lr: 0.000121. clip: 0.048243\n",
      "Iteration 16861: Policy loss: -0.035154. Value loss: 0.390941. Entropy: 0.309531.\n",
      "Iteration 16862: Policy loss: -0.058095. Value loss: 0.215242. Entropy: 0.308147.\n",
      "Iteration 16863: Policy loss: -0.052452. Value loss: 0.151593. Entropy: 0.307718.\n",
      "episode: 5923   score: 345.0  epsilon: 1.0    steps: 408  evaluation reward: 358.5\n",
      "episode: 5924   score: 335.0  epsilon: 1.0    steps: 720  evaluation reward: 360.05\n",
      "episode: 5925   score: 390.0  epsilon: 1.0    steps: 1000  evaluation reward: 357.45\n",
      "Training network. lr: 0.000121. clip: 0.048243\n",
      "Iteration 16864: Policy loss: 0.078855. Value loss: 0.088662. Entropy: 0.284179.\n",
      "Iteration 16865: Policy loss: 0.079362. Value loss: 0.038854. Entropy: 0.284000.\n",
      "Iteration 16866: Policy loss: 0.079096. Value loss: 0.031860. Entropy: 0.283195.\n",
      "Training network. lr: 0.000121. clip: 0.048243\n",
      "Iteration 16867: Policy loss: 0.007247. Value loss: 0.129898. Entropy: 0.301509.\n",
      "Iteration 16868: Policy loss: 0.008137. Value loss: 0.055838. Entropy: 0.302409.\n",
      "Iteration 16869: Policy loss: 0.003159. Value loss: 0.037398. Entropy: 0.302107.\n",
      "episode: 5926   score: 440.0  epsilon: 1.0    steps: 720  evaluation reward: 357.45\n",
      "Training network. lr: 0.000121. clip: 0.048243\n",
      "Iteration 16870: Policy loss: 0.270132. Value loss: 0.195961. Entropy: 0.292070.\n",
      "Iteration 16871: Policy loss: 0.261769. Value loss: 0.070370. Entropy: 0.292063.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16872: Policy loss: 0.264121. Value loss: 0.046249. Entropy: 0.292004.\n",
      "episode: 5927   score: 275.0  epsilon: 1.0    steps: 464  evaluation reward: 359.2\n",
      "episode: 5928   score: 485.0  epsilon: 1.0    steps: 528  evaluation reward: 361.45\n",
      "Training network. lr: 0.000121. clip: 0.048243\n",
      "Iteration 16873: Policy loss: 0.296516. Value loss: 0.158609. Entropy: 0.282631.\n",
      "Iteration 16874: Policy loss: 0.288976. Value loss: 0.073206. Entropy: 0.281024.\n",
      "Iteration 16875: Policy loss: 0.293190. Value loss: 0.048140. Entropy: 0.281411.\n",
      "episode: 5929   score: 360.0  epsilon: 1.0    steps: 376  evaluation reward: 361.75\n",
      "Training network. lr: 0.000121. clip: 0.048243\n",
      "Iteration 16876: Policy loss: 0.223060. Value loss: 0.084780. Entropy: 0.296941.\n",
      "Iteration 16877: Policy loss: 0.221501. Value loss: 0.035533. Entropy: 0.297063.\n",
      "Iteration 16878: Policy loss: 0.213498. Value loss: 0.024575. Entropy: 0.296715.\n",
      "episode: 5930   score: 125.0  epsilon: 1.0    steps: 256  evaluation reward: 358.1\n",
      "Training network. lr: 0.000121. clip: 0.048243\n",
      "Iteration 16879: Policy loss: 0.040891. Value loss: 0.155615. Entropy: 0.298434.\n",
      "Iteration 16880: Policy loss: 0.037658. Value loss: 0.070995. Entropy: 0.296069.\n",
      "Iteration 16881: Policy loss: 0.026791. Value loss: 0.049772. Entropy: 0.298433.\n",
      "Training network. lr: 0.000121. clip: 0.048243\n",
      "Iteration 16882: Policy loss: -0.294848. Value loss: 0.305606. Entropy: 0.307151.\n",
      "Iteration 16883: Policy loss: -0.277952. Value loss: 0.109344. Entropy: 0.308831.\n",
      "Iteration 16884: Policy loss: -0.303426. Value loss: 0.075593. Entropy: 0.309490.\n",
      "episode: 5931   score: 290.0  epsilon: 1.0    steps: 112  evaluation reward: 358.45\n",
      "Training network. lr: 0.000121. clip: 0.048243\n",
      "Iteration 16885: Policy loss: -0.276541. Value loss: 0.165860. Entropy: 0.280462.\n",
      "Iteration 16886: Policy loss: -0.291740. Value loss: 0.069520. Entropy: 0.278268.\n",
      "Iteration 16887: Policy loss: -0.288980. Value loss: 0.042989. Entropy: 0.278179.\n",
      "episode: 5932   score: 395.0  epsilon: 1.0    steps: 160  evaluation reward: 359.2\n",
      "Training network. lr: 0.000121. clip: 0.048243\n",
      "Iteration 16888: Policy loss: -0.179419. Value loss: 0.220983. Entropy: 0.268228.\n",
      "Iteration 16889: Policy loss: -0.190001. Value loss: 0.074836. Entropy: 0.266742.\n",
      "Iteration 16890: Policy loss: -0.189070. Value loss: 0.047845. Entropy: 0.265595.\n",
      "episode: 5933   score: 270.0  epsilon: 1.0    steps: 312  evaluation reward: 355.9\n",
      "Training network. lr: 0.000121. clip: 0.048243\n",
      "Iteration 16891: Policy loss: -0.012845. Value loss: 0.151072. Entropy: 0.279923.\n",
      "Iteration 16892: Policy loss: -0.013920. Value loss: 0.083616. Entropy: 0.279787.\n",
      "Iteration 16893: Policy loss: -0.012896. Value loss: 0.058908. Entropy: 0.279285.\n",
      "episode: 5934   score: 560.0  epsilon: 1.0    steps: 416  evaluation reward: 359.95\n",
      "episode: 5935   score: 435.0  epsilon: 1.0    steps: 512  evaluation reward: 361.1\n",
      "episode: 5936   score: 570.0  epsilon: 1.0    steps: 968  evaluation reward: 364.65\n",
      "Training network. lr: 0.000121. clip: 0.048243\n",
      "Iteration 16894: Policy loss: 0.108389. Value loss: 0.178887. Entropy: 0.268346.\n",
      "Iteration 16895: Policy loss: 0.107638. Value loss: 0.065419. Entropy: 0.267262.\n",
      "Iteration 16896: Policy loss: 0.105364. Value loss: 0.047365. Entropy: 0.267789.\n",
      "Training network. lr: 0.000121. clip: 0.048243\n",
      "Iteration 16897: Policy loss: 0.178933. Value loss: 0.102473. Entropy: 0.298000.\n",
      "Iteration 16898: Policy loss: 0.174453. Value loss: 0.040722. Entropy: 0.297268.\n",
      "Iteration 16899: Policy loss: 0.168355. Value loss: 0.028707. Entropy: 0.297055.\n",
      "Training network. lr: 0.000121. clip: 0.048243\n",
      "Iteration 16900: Policy loss: -0.026992. Value loss: 0.108015. Entropy: 0.306086.\n",
      "Iteration 16901: Policy loss: -0.029329. Value loss: 0.052890. Entropy: 0.305679.\n",
      "Iteration 16902: Policy loss: -0.028536. Value loss: 0.038132. Entropy: 0.306861.\n",
      "episode: 5937   score: 485.0  epsilon: 1.0    steps: 648  evaluation reward: 367.4\n",
      "Training network. lr: 0.000120. clip: 0.048086\n",
      "Iteration 16903: Policy loss: 0.219168. Value loss: 0.204295. Entropy: 0.292069.\n",
      "Iteration 16904: Policy loss: 0.212109. Value loss: 0.059919. Entropy: 0.292481.\n",
      "Iteration 16905: Policy loss: 0.198275. Value loss: 0.035712. Entropy: 0.293225.\n",
      "episode: 5938   score: 420.0  epsilon: 1.0    steps: 80  evaluation reward: 368.0\n",
      "episode: 5939   score: 260.0  epsilon: 1.0    steps: 864  evaluation reward: 368.5\n",
      "Training network. lr: 0.000120. clip: 0.048086\n",
      "Iteration 16906: Policy loss: 0.139262. Value loss: 0.101099. Entropy: 0.283934.\n",
      "Iteration 16907: Policy loss: 0.145476. Value loss: 0.059786. Entropy: 0.281874.\n",
      "Iteration 16908: Policy loss: 0.142656. Value loss: 0.048396. Entropy: 0.283288.\n",
      "episode: 5940   score: 395.0  epsilon: 1.0    steps: 792  evaluation reward: 370.35\n",
      "Training network. lr: 0.000120. clip: 0.048086\n",
      "Iteration 16909: Policy loss: 0.054323. Value loss: 0.094611. Entropy: 0.300564.\n",
      "Iteration 16910: Policy loss: 0.048834. Value loss: 0.046289. Entropy: 0.300324.\n",
      "Iteration 16911: Policy loss: 0.047641. Value loss: 0.033134. Entropy: 0.301934.\n",
      "Training network. lr: 0.000120. clip: 0.048086\n",
      "Iteration 16912: Policy loss: -0.113887. Value loss: 0.146352. Entropy: 0.288992.\n",
      "Iteration 16913: Policy loss: -0.120342. Value loss: 0.065382. Entropy: 0.289982.\n",
      "Iteration 16914: Policy loss: -0.127883. Value loss: 0.045073. Entropy: 0.291198.\n",
      "episode: 5941   score: 320.0  epsilon: 1.0    steps: 136  evaluation reward: 369.85\n",
      "Training network. lr: 0.000120. clip: 0.048086\n",
      "Iteration 16915: Policy loss: -0.225740. Value loss: 0.280511. Entropy: 0.282815.\n",
      "Iteration 16916: Policy loss: -0.239930. Value loss: 0.116138. Entropy: 0.283702.\n",
      "Iteration 16917: Policy loss: -0.236692. Value loss: 0.061144. Entropy: 0.285050.\n",
      "episode: 5942   score: 360.0  epsilon: 1.0    steps: 288  evaluation reward: 368.45\n",
      "episode: 5943   score: 395.0  epsilon: 1.0    steps: 680  evaluation reward: 371.3\n",
      "Training network. lr: 0.000120. clip: 0.048086\n",
      "Iteration 16918: Policy loss: 0.281052. Value loss: 0.141587. Entropy: 0.238428.\n",
      "Iteration 16919: Policy loss: 0.274773. Value loss: 0.055947. Entropy: 0.239565.\n",
      "Iteration 16920: Policy loss: 0.267614. Value loss: 0.035594. Entropy: 0.240672.\n",
      "Training network. lr: 0.000120. clip: 0.048086\n",
      "Iteration 16921: Policy loss: 0.121145. Value loss: 0.094839. Entropy: 0.306617.\n",
      "Iteration 16922: Policy loss: 0.109433. Value loss: 0.035573. Entropy: 0.307926.\n",
      "Iteration 16923: Policy loss: 0.109058. Value loss: 0.025775. Entropy: 0.307184.\n",
      "episode: 5944   score: 470.0  epsilon: 1.0    steps: 200  evaluation reward: 372.7\n",
      "episode: 5945   score: 210.0  epsilon: 1.0    steps: 376  evaluation reward: 373.75\n",
      "episode: 5946   score: 315.0  epsilon: 1.0    steps: 824  evaluation reward: 371.95\n",
      "Training network. lr: 0.000120. clip: 0.048086\n",
      "Iteration 16924: Policy loss: -0.081753. Value loss: 0.118187. Entropy: 0.280501.\n",
      "Iteration 16925: Policy loss: -0.081854. Value loss: 0.045722. Entropy: 0.278885.\n",
      "Iteration 16926: Policy loss: -0.086425. Value loss: 0.030727. Entropy: 0.278165.\n",
      "episode: 5947   score: 490.0  epsilon: 1.0    steps: 24  evaluation reward: 375.2\n",
      "Training network. lr: 0.000120. clip: 0.048086\n",
      "Iteration 16927: Policy loss: 0.274416. Value loss: 0.133108. Entropy: 0.291850.\n",
      "Iteration 16928: Policy loss: 0.266087. Value loss: 0.047400. Entropy: 0.291426.\n",
      "Iteration 16929: Policy loss: 0.262672. Value loss: 0.030941. Entropy: 0.290932.\n",
      "episode: 5948   score: 210.0  epsilon: 1.0    steps: 64  evaluation reward: 371.4\n",
      "Training network. lr: 0.000120. clip: 0.048086\n",
      "Iteration 16930: Policy loss: 0.067653. Value loss: 0.091776. Entropy: 0.274160.\n",
      "Iteration 16931: Policy loss: 0.070130. Value loss: 0.048661. Entropy: 0.275743.\n",
      "Iteration 16932: Policy loss: 0.059896. Value loss: 0.035538. Entropy: 0.274646.\n",
      "Training network. lr: 0.000120. clip: 0.048086\n",
      "Iteration 16933: Policy loss: 0.122061. Value loss: 0.130298. Entropy: 0.303213.\n",
      "Iteration 16934: Policy loss: 0.108935. Value loss: 0.044976. Entropy: 0.305276.\n",
      "Iteration 16935: Policy loss: 0.108355. Value loss: 0.032758. Entropy: 0.304557.\n",
      "Training network. lr: 0.000120. clip: 0.048086\n",
      "Iteration 16936: Policy loss: -0.015887. Value loss: 0.070912. Entropy: 0.301277.\n",
      "Iteration 16937: Policy loss: -0.019333. Value loss: 0.035173. Entropy: 0.301228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16938: Policy loss: -0.022127. Value loss: 0.027048. Entropy: 0.301720.\n",
      "Training network. lr: 0.000120. clip: 0.048086\n",
      "Iteration 16939: Policy loss: -0.024191. Value loss: 0.110199. Entropy: 0.304394.\n",
      "Iteration 16940: Policy loss: -0.031073. Value loss: 0.047926. Entropy: 0.300125.\n",
      "Iteration 16941: Policy loss: -0.034301. Value loss: 0.035780. Entropy: 0.300962.\n",
      "episode: 5949   score: 245.0  epsilon: 1.0    steps: 896  evaluation reward: 372.6\n",
      "Training network. lr: 0.000120. clip: 0.048086\n",
      "Iteration 16942: Policy loss: -0.155284. Value loss: 0.372540. Entropy: 0.288213.\n",
      "Iteration 16943: Policy loss: -0.146650. Value loss: 0.201333. Entropy: 0.291778.\n",
      "Iteration 16944: Policy loss: -0.157871. Value loss: 0.091910. Entropy: 0.289815.\n",
      "episode: 5950   score: 440.0  epsilon: 1.0    steps: 256  evaluation reward: 371.6\n",
      "Training network. lr: 0.000120. clip: 0.048086\n",
      "Iteration 16945: Policy loss: -0.007023. Value loss: 0.093344. Entropy: 0.292019.\n",
      "Iteration 16946: Policy loss: -0.012542. Value loss: 0.031820. Entropy: 0.291308.\n",
      "Iteration 16947: Policy loss: -0.010551. Value loss: 0.021946. Entropy: 0.292217.\n",
      "now time :  2019-09-06 07:46:46.571116\n",
      "episode: 5951   score: 425.0  epsilon: 1.0    steps: 104  evaluation reward: 373.6\n",
      "episode: 5952   score: 595.0  epsilon: 1.0    steps: 416  evaluation reward: 374.25\n",
      "episode: 5953   score: 295.0  epsilon: 1.0    steps: 784  evaluation reward: 369.05\n",
      "Training network. lr: 0.000120. clip: 0.048086\n",
      "Iteration 16948: Policy loss: -0.051996. Value loss: 0.298226. Entropy: 0.267524.\n",
      "Iteration 16949: Policy loss: -0.061741. Value loss: 0.184933. Entropy: 0.265921.\n",
      "Iteration 16950: Policy loss: -0.039743. Value loss: 0.141838. Entropy: 0.268250.\n",
      "episode: 5954   score: 535.0  epsilon: 1.0    steps: 480  evaluation reward: 370.8\n",
      "episode: 5955   score: 700.0  epsilon: 1.0    steps: 920  evaluation reward: 375.2\n",
      "Training network. lr: 0.000120. clip: 0.047939\n",
      "Iteration 16951: Policy loss: -0.407862. Value loss: 0.351472. Entropy: 0.294903.\n",
      "Iteration 16952: Policy loss: -0.450140. Value loss: 0.235700. Entropy: 0.293883.\n",
      "Iteration 16953: Policy loss: -0.442972. Value loss: 0.173066. Entropy: 0.293049.\n",
      "Training network. lr: 0.000120. clip: 0.047939\n",
      "Iteration 16954: Policy loss: 0.011936. Value loss: 0.135169. Entropy: 0.307741.\n",
      "Iteration 16955: Policy loss: 0.015491. Value loss: 0.061258. Entropy: 0.308402.\n",
      "Iteration 16956: Policy loss: 0.009661. Value loss: 0.042732. Entropy: 0.309673.\n",
      "episode: 5956   score: 495.0  epsilon: 1.0    steps: 24  evaluation reward: 375.9\n",
      "Training network. lr: 0.000120. clip: 0.047939\n",
      "Iteration 16957: Policy loss: 0.002941. Value loss: 0.113194. Entropy: 0.294051.\n",
      "Iteration 16958: Policy loss: -0.009220. Value loss: 0.052830. Entropy: 0.291163.\n",
      "Iteration 16959: Policy loss: -0.008549. Value loss: 0.034613. Entropy: 0.292013.\n",
      "episode: 5957   score: 225.0  epsilon: 1.0    steps: 632  evaluation reward: 369.85\n",
      "Training network. lr: 0.000120. clip: 0.047939\n",
      "Iteration 16960: Policy loss: 0.024148. Value loss: 0.132164. Entropy: 0.293238.\n",
      "Iteration 16961: Policy loss: 0.011688. Value loss: 0.058937. Entropy: 0.294751.\n",
      "Iteration 16962: Policy loss: 0.015349. Value loss: 0.044921. Entropy: 0.292858.\n",
      "episode: 5958   score: 125.0  epsilon: 1.0    steps: 24  evaluation reward: 368.2\n",
      "Training network. lr: 0.000120. clip: 0.047939\n",
      "Iteration 16963: Policy loss: 0.207663. Value loss: 0.120495. Entropy: 0.300619.\n",
      "Iteration 16964: Policy loss: 0.204208. Value loss: 0.031910. Entropy: 0.299593.\n",
      "Iteration 16965: Policy loss: 0.209709. Value loss: 0.020605. Entropy: 0.299691.\n",
      "episode: 5959   score: 295.0  epsilon: 1.0    steps: 152  evaluation reward: 363.15\n",
      "Training network. lr: 0.000120. clip: 0.047939\n",
      "Iteration 16966: Policy loss: -0.163076. Value loss: 0.281274. Entropy: 0.301690.\n",
      "Iteration 16967: Policy loss: -0.157503. Value loss: 0.069463. Entropy: 0.301941.\n",
      "Iteration 16968: Policy loss: -0.180019. Value loss: 0.047261. Entropy: 0.300600.\n",
      "episode: 5960   score: 525.0  epsilon: 1.0    steps: 256  evaluation reward: 365.75\n",
      "Training network. lr: 0.000120. clip: 0.047939\n",
      "Iteration 16969: Policy loss: 0.111776. Value loss: 0.207169. Entropy: 0.299947.\n",
      "Iteration 16970: Policy loss: 0.113224. Value loss: 0.067083. Entropy: 0.298313.\n",
      "Iteration 16971: Policy loss: 0.100671. Value loss: 0.045542. Entropy: 0.298331.\n",
      "episode: 5961   score: 305.0  epsilon: 1.0    steps: 312  evaluation reward: 362.6\n",
      "episode: 5962   score: 150.0  epsilon: 1.0    steps: 848  evaluation reward: 361.2\n",
      "episode: 5963   score: 340.0  epsilon: 1.0    steps: 864  evaluation reward: 362.8\n",
      "Training network. lr: 0.000120. clip: 0.047939\n",
      "Iteration 16972: Policy loss: 0.426296. Value loss: 0.184641. Entropy: 0.289956.\n",
      "Iteration 16973: Policy loss: 0.422685. Value loss: 0.077735. Entropy: 0.290661.\n",
      "Iteration 16974: Policy loss: 0.418115. Value loss: 0.049107. Entropy: 0.289671.\n",
      "Training network. lr: 0.000120. clip: 0.047939\n",
      "Iteration 16975: Policy loss: 0.085840. Value loss: 0.112064. Entropy: 0.309012.\n",
      "Iteration 16976: Policy loss: 0.080461. Value loss: 0.041971. Entropy: 0.307968.\n",
      "Iteration 16977: Policy loss: 0.081929. Value loss: 0.030518. Entropy: 0.307550.\n",
      "Training network. lr: 0.000120. clip: 0.047939\n",
      "Iteration 16978: Policy loss: 0.123151. Value loss: 0.126539. Entropy: 0.308681.\n",
      "Iteration 16979: Policy loss: 0.121794. Value loss: 0.049056. Entropy: 0.309252.\n",
      "Iteration 16980: Policy loss: 0.114673. Value loss: 0.034765. Entropy: 0.308913.\n",
      "episode: 5964   score: 765.0  epsilon: 1.0    steps: 192  evaluation reward: 364.55\n",
      "Training network. lr: 0.000120. clip: 0.047939\n",
      "Iteration 16981: Policy loss: 0.219154. Value loss: 0.152749. Entropy: 0.277151.\n",
      "Iteration 16982: Policy loss: 0.212236. Value loss: 0.054403. Entropy: 0.277740.\n",
      "Iteration 16983: Policy loss: 0.208701. Value loss: 0.033824. Entropy: 0.277889.\n",
      "episode: 5965   score: 285.0  epsilon: 1.0    steps: 416  evaluation reward: 361.2\n",
      "episode: 5966   score: 315.0  epsilon: 1.0    steps: 728  evaluation reward: 360.25\n",
      "episode: 5967   score: 345.0  epsilon: 1.0    steps: 1016  evaluation reward: 361.6\n",
      "Training network. lr: 0.000120. clip: 0.047939\n",
      "Iteration 16984: Policy loss: 0.127559. Value loss: 0.161750. Entropy: 0.284110.\n",
      "Iteration 16985: Policy loss: 0.130649. Value loss: 0.067420. Entropy: 0.284256.\n",
      "Iteration 16986: Policy loss: 0.127262. Value loss: 0.045644. Entropy: 0.283058.\n",
      "Training network. lr: 0.000120. clip: 0.047939\n",
      "Iteration 16987: Policy loss: -0.100884. Value loss: 0.100757. Entropy: 0.304003.\n",
      "Iteration 16988: Policy loss: -0.100167. Value loss: 0.046223. Entropy: 0.304907.\n",
      "Iteration 16989: Policy loss: -0.106787. Value loss: 0.032130. Entropy: 0.303257.\n",
      "Training network. lr: 0.000120. clip: 0.047939\n",
      "Iteration 16990: Policy loss: -0.184200. Value loss: 0.293494. Entropy: 0.305821.\n",
      "Iteration 16991: Policy loss: -0.192218. Value loss: 0.167790. Entropy: 0.306089.\n",
      "Iteration 16992: Policy loss: -0.200540. Value loss: 0.110224. Entropy: 0.307794.\n",
      "Training network. lr: 0.000120. clip: 0.047939\n",
      "Iteration 16993: Policy loss: -0.105378. Value loss: 0.117653. Entropy: 0.308513.\n",
      "Iteration 16994: Policy loss: -0.122620. Value loss: 0.060170. Entropy: 0.308595.\n",
      "Iteration 16995: Policy loss: -0.126255. Value loss: 0.043741. Entropy: 0.308211.\n",
      "episode: 5968   score: 590.0  epsilon: 1.0    steps: 552  evaluation reward: 364.65\n",
      "Training network. lr: 0.000120. clip: 0.047939\n",
      "Iteration 16996: Policy loss: -0.020458. Value loss: 0.139292. Entropy: 0.295486.\n",
      "Iteration 16997: Policy loss: -0.031144. Value loss: 0.060916. Entropy: 0.295309.\n",
      "Iteration 16998: Policy loss: -0.037750. Value loss: 0.042929. Entropy: 0.294650.\n",
      "Training network. lr: 0.000120. clip: 0.047939\n",
      "Iteration 16999: Policy loss: -0.110667. Value loss: 0.160815. Entropy: 0.301654.\n",
      "Iteration 17000: Policy loss: -0.127808. Value loss: 0.108573. Entropy: 0.302775.\n",
      "Iteration 17001: Policy loss: -0.133765. Value loss: 0.079726. Entropy: 0.302535.\n",
      "episode: 5969   score: 405.0  epsilon: 1.0    steps: 600  evaluation reward: 367.15\n",
      "Training network. lr: 0.000119. clip: 0.047782\n",
      "Iteration 17002: Policy loss: 0.146735. Value loss: 0.125634. Entropy: 0.297573.\n",
      "Iteration 17003: Policy loss: 0.143801. Value loss: 0.040873. Entropy: 0.297331.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17004: Policy loss: 0.141167. Value loss: 0.028271. Entropy: 0.297380.\n",
      "episode: 5970   score: 640.0  epsilon: 1.0    steps: 424  evaluation reward: 369.55\n",
      "episode: 5971   score: 495.0  epsilon: 1.0    steps: 1024  evaluation reward: 373.15\n",
      "Training network. lr: 0.000119. clip: 0.047782\n",
      "Iteration 17005: Policy loss: -0.150835. Value loss: 0.338960. Entropy: 0.305289.\n",
      "Iteration 17006: Policy loss: -0.175656. Value loss: 0.150884. Entropy: 0.303129.\n",
      "Iteration 17007: Policy loss: -0.165888. Value loss: 0.089604. Entropy: 0.304123.\n",
      "episode: 5972   score: 300.0  epsilon: 1.0    steps: 112  evaluation reward: 372.9\n",
      "episode: 5973   score: 550.0  epsilon: 1.0    steps: 704  evaluation reward: 372.95\n",
      "Training network. lr: 0.000119. clip: 0.047782\n",
      "Iteration 17008: Policy loss: 0.155256. Value loss: 0.102395. Entropy: 0.284830.\n",
      "Iteration 17009: Policy loss: 0.147022. Value loss: 0.040660. Entropy: 0.283573.\n",
      "Iteration 17010: Policy loss: 0.142055. Value loss: 0.030255. Entropy: 0.284400.\n",
      "episode: 5974   score: 615.0  epsilon: 1.0    steps: 968  evaluation reward: 375.5\n",
      "Training network. lr: 0.000119. clip: 0.047782\n",
      "Iteration 17011: Policy loss: 0.103870. Value loss: 0.525149. Entropy: 0.298094.\n",
      "Iteration 17012: Policy loss: 0.042694. Value loss: 0.341289. Entropy: 0.297241.\n",
      "Iteration 17013: Policy loss: 0.033772. Value loss: 0.251350. Entropy: 0.298088.\n",
      "Training network. lr: 0.000119. clip: 0.047782\n",
      "Iteration 17014: Policy loss: -0.057872. Value loss: 0.098559. Entropy: 0.270603.\n",
      "Iteration 17015: Policy loss: -0.066942. Value loss: 0.049015. Entropy: 0.270318.\n",
      "Iteration 17016: Policy loss: -0.059566. Value loss: 0.032192. Entropy: 0.269909.\n",
      "episode: 5975   score: 640.0  epsilon: 1.0    steps: 528  evaluation reward: 378.15\n",
      "episode: 5976   score: 165.0  epsilon: 1.0    steps: 1024  evaluation reward: 376.55\n",
      "episode: 5977   score: 210.0  epsilon: 1.0    steps: 1024  evaluation reward: 377.3\n",
      "Training network. lr: 0.000119. clip: 0.047782\n",
      "Iteration 17017: Policy loss: 0.123696. Value loss: 0.142872. Entropy: 0.291557.\n",
      "Iteration 17018: Policy loss: 0.111538. Value loss: 0.072748. Entropy: 0.291812.\n",
      "Iteration 17019: Policy loss: 0.106345. Value loss: 0.056962. Entropy: 0.292372.\n",
      "episode: 5978   score: 340.0  epsilon: 1.0    steps: 840  evaluation reward: 379.9\n",
      "Training network. lr: 0.000119. clip: 0.047782\n",
      "Iteration 17020: Policy loss: -0.281472. Value loss: 0.108792. Entropy: 0.263003.\n",
      "Iteration 17021: Policy loss: -0.279927. Value loss: 0.048416. Entropy: 0.263717.\n",
      "Iteration 17022: Policy loss: -0.294759. Value loss: 0.040182. Entropy: 0.264803.\n",
      "episode: 5979   score: 210.0  epsilon: 1.0    steps: 384  evaluation reward: 378.85\n",
      "Training network. lr: 0.000119. clip: 0.047782\n",
      "Iteration 17023: Policy loss: -0.147395. Value loss: 0.165496. Entropy: 0.299821.\n",
      "Iteration 17024: Policy loss: -0.157656. Value loss: 0.112434. Entropy: 0.301086.\n",
      "Iteration 17025: Policy loss: -0.152781. Value loss: 0.076641. Entropy: 0.301078.\n",
      "Training network. lr: 0.000119. clip: 0.047782\n",
      "Iteration 17026: Policy loss: -0.152850. Value loss: 0.134144. Entropy: 0.264026.\n",
      "Iteration 17027: Policy loss: -0.155326. Value loss: 0.049265. Entropy: 0.264976.\n",
      "Iteration 17028: Policy loss: -0.159037. Value loss: 0.030027. Entropy: 0.262323.\n",
      "episode: 5980   score: 345.0  epsilon: 1.0    steps: 208  evaluation reward: 379.25\n",
      "Training network. lr: 0.000119. clip: 0.047782\n",
      "Iteration 17029: Policy loss: 0.233562. Value loss: 0.164848. Entropy: 0.295226.\n",
      "Iteration 17030: Policy loss: 0.218459. Value loss: 0.060831. Entropy: 0.293316.\n",
      "Iteration 17031: Policy loss: 0.209641. Value loss: 0.036677. Entropy: 0.293110.\n",
      "Training network. lr: 0.000119. clip: 0.047782\n",
      "Iteration 17032: Policy loss: -0.107240. Value loss: 0.074482. Entropy: 0.298003.\n",
      "Iteration 17033: Policy loss: -0.117248. Value loss: 0.032988. Entropy: 0.298333.\n",
      "Iteration 17034: Policy loss: -0.111223. Value loss: 0.026432. Entropy: 0.298050.\n",
      "Training network. lr: 0.000119. clip: 0.047782\n",
      "Iteration 17035: Policy loss: -0.002429. Value loss: 0.103400. Entropy: 0.285599.\n",
      "Iteration 17036: Policy loss: -0.007603. Value loss: 0.049442. Entropy: 0.285377.\n",
      "Iteration 17037: Policy loss: -0.009671. Value loss: 0.033617. Entropy: 0.285961.\n",
      "episode: 5981   score: 635.0  epsilon: 1.0    steps: 936  evaluation reward: 380.45\n",
      "episode: 5982   score: 155.0  epsilon: 1.0    steps: 952  evaluation reward: 377.15\n",
      "Training network. lr: 0.000119. clip: 0.047782\n",
      "Iteration 17038: Policy loss: 0.295808. Value loss: 0.266873. Entropy: 0.270148.\n",
      "Iteration 17039: Policy loss: 0.308229. Value loss: 0.071235. Entropy: 0.268670.\n",
      "Iteration 17040: Policy loss: 0.289291. Value loss: 0.039683. Entropy: 0.265964.\n",
      "episode: 5983   score: 385.0  epsilon: 1.0    steps: 128  evaluation reward: 375.1\n",
      "episode: 5984   score: 590.0  epsilon: 1.0    steps: 864  evaluation reward: 378.1\n",
      "Training network. lr: 0.000119. clip: 0.047782\n",
      "Iteration 17041: Policy loss: -0.039739. Value loss: 0.112940. Entropy: 0.282554.\n",
      "Iteration 17042: Policy loss: -0.040327. Value loss: 0.057495. Entropy: 0.279681.\n",
      "Iteration 17043: Policy loss: -0.043840. Value loss: 0.041124. Entropy: 0.279892.\n",
      "episode: 5985   score: 390.0  epsilon: 1.0    steps: 152  evaluation reward: 378.8\n",
      "episode: 5986   score: 285.0  epsilon: 1.0    steps: 488  evaluation reward: 378.8\n",
      "Training network. lr: 0.000119. clip: 0.047782\n",
      "Iteration 17044: Policy loss: 0.161006. Value loss: 0.114751. Entropy: 0.288381.\n",
      "Iteration 17045: Policy loss: 0.154784. Value loss: 0.060972. Entropy: 0.286970.\n",
      "Iteration 17046: Policy loss: 0.158116. Value loss: 0.041549. Entropy: 0.287735.\n",
      "Training network. lr: 0.000119. clip: 0.047782\n",
      "Iteration 17047: Policy loss: -0.478584. Value loss: 0.276486. Entropy: 0.312147.\n",
      "Iteration 17048: Policy loss: -0.468989. Value loss: 0.104243. Entropy: 0.313621.\n",
      "Iteration 17049: Policy loss: -0.481408. Value loss: 0.063644. Entropy: 0.313041.\n",
      "episode: 5987   score: 620.0  epsilon: 1.0    steps: 312  evaluation reward: 378.75\n",
      "Training network. lr: 0.000119. clip: 0.047782\n",
      "Iteration 17050: Policy loss: -0.152803. Value loss: 0.072322. Entropy: 0.275914.\n",
      "Iteration 17051: Policy loss: -0.162376. Value loss: 0.044143. Entropy: 0.274588.\n",
      "Iteration 17052: Policy loss: -0.168902. Value loss: 0.032452. Entropy: 0.274376.\n",
      "episode: 5988   score: 335.0  epsilon: 1.0    steps: 784  evaluation reward: 378.85\n",
      "Training network. lr: 0.000119. clip: 0.047625\n",
      "Iteration 17053: Policy loss: 0.399097. Value loss: 0.167353. Entropy: 0.271504.\n",
      "Iteration 17054: Policy loss: 0.393009. Value loss: 0.062092. Entropy: 0.272268.\n",
      "Iteration 17055: Policy loss: 0.397134. Value loss: 0.041760. Entropy: 0.271521.\n",
      "Training network. lr: 0.000119. clip: 0.047625\n",
      "Iteration 17056: Policy loss: -0.222980. Value loss: 0.116935. Entropy: 0.295930.\n",
      "Iteration 17057: Policy loss: -0.225129. Value loss: 0.050247. Entropy: 0.294436.\n",
      "Iteration 17058: Policy loss: -0.230750. Value loss: 0.034163. Entropy: 0.295502.\n",
      "episode: 5989   score: 315.0  epsilon: 1.0    steps: 544  evaluation reward: 379.85\n",
      "Training network. lr: 0.000119. clip: 0.047625\n",
      "Iteration 17059: Policy loss: 0.262428. Value loss: 0.195911. Entropy: 0.286694.\n",
      "Iteration 17060: Policy loss: 0.246755. Value loss: 0.094009. Entropy: 0.286966.\n",
      "Iteration 17061: Policy loss: 0.239324. Value loss: 0.067414. Entropy: 0.287403.\n",
      "episode: 5990   score: 240.0  epsilon: 1.0    steps: 776  evaluation reward: 380.15\n",
      "Training network. lr: 0.000119. clip: 0.047625\n",
      "Iteration 17062: Policy loss: 0.077907. Value loss: 0.113774. Entropy: 0.274092.\n",
      "Iteration 17063: Policy loss: 0.077289. Value loss: 0.042388. Entropy: 0.273778.\n",
      "Iteration 17064: Policy loss: 0.072417. Value loss: 0.030329. Entropy: 0.271637.\n",
      "episode: 5991   score: 390.0  epsilon: 1.0    steps: 96  evaluation reward: 383.0\n",
      "episode: 5992   score: 410.0  epsilon: 1.0    steps: 760  evaluation reward: 384.95\n",
      "Training network. lr: 0.000119. clip: 0.047625\n",
      "Iteration 17065: Policy loss: 0.116038. Value loss: 0.070050. Entropy: 0.267038.\n",
      "Iteration 17066: Policy loss: 0.110235. Value loss: 0.026888. Entropy: 0.266884.\n",
      "Iteration 17067: Policy loss: 0.115473. Value loss: 0.020244. Entropy: 0.266666.\n",
      "episode: 5993   score: 210.0  epsilon: 1.0    steps: 392  evaluation reward: 381.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000119. clip: 0.047625\n",
      "Iteration 17068: Policy loss: 0.075251. Value loss: 0.113989. Entropy: 0.296599.\n",
      "Iteration 17069: Policy loss: 0.055852. Value loss: 0.052216. Entropy: 0.295973.\n",
      "Iteration 17070: Policy loss: 0.060898. Value loss: 0.035427. Entropy: 0.295104.\n",
      "episode: 5994   score: 420.0  epsilon: 1.0    steps: 392  evaluation reward: 379.55\n",
      "Training network. lr: 0.000119. clip: 0.047625\n",
      "Iteration 17071: Policy loss: -0.284306. Value loss: 0.319931. Entropy: 0.296928.\n",
      "Iteration 17072: Policy loss: -0.275206. Value loss: 0.162073. Entropy: 0.296623.\n",
      "Iteration 17073: Policy loss: -0.276906. Value loss: 0.071645. Entropy: 0.296230.\n",
      "episode: 5995   score: 420.0  epsilon: 1.0    steps: 456  evaluation reward: 381.15\n",
      "episode: 5996   score: 350.0  epsilon: 1.0    steps: 736  evaluation reward: 382.7\n",
      "Training network. lr: 0.000119. clip: 0.047625\n",
      "Iteration 17074: Policy loss: -0.146457. Value loss: 0.101979. Entropy: 0.258163.\n",
      "Iteration 17075: Policy loss: -0.152705. Value loss: 0.039887. Entropy: 0.256245.\n",
      "Iteration 17076: Policy loss: -0.157476. Value loss: 0.027801. Entropy: 0.258500.\n",
      "Training network. lr: 0.000119. clip: 0.047625\n",
      "Iteration 17077: Policy loss: -0.163132. Value loss: 0.350025. Entropy: 0.293778.\n",
      "Iteration 17078: Policy loss: -0.177951. Value loss: 0.175589. Entropy: 0.290344.\n",
      "Iteration 17079: Policy loss: -0.183453. Value loss: 0.102322. Entropy: 0.292937.\n",
      "episode: 5997   score: 330.0  epsilon: 1.0    steps: 456  evaluation reward: 384.15\n",
      "Training network. lr: 0.000119. clip: 0.047625\n",
      "Iteration 17080: Policy loss: 0.238520. Value loss: 0.161029. Entropy: 0.280293.\n",
      "Iteration 17081: Policy loss: 0.240003. Value loss: 0.081138. Entropy: 0.280381.\n",
      "Iteration 17082: Policy loss: 0.229132. Value loss: 0.060446. Entropy: 0.279684.\n",
      "Training network. lr: 0.000119. clip: 0.047625\n",
      "Iteration 17083: Policy loss: 0.169531. Value loss: 0.125468. Entropy: 0.293947.\n",
      "Iteration 17084: Policy loss: 0.170154. Value loss: 0.052293. Entropy: 0.293810.\n",
      "Iteration 17085: Policy loss: 0.160502. Value loss: 0.037432. Entropy: 0.293008.\n",
      "episode: 5998   score: 480.0  epsilon: 1.0    steps: 32  evaluation reward: 383.8\n",
      "episode: 5999   score: 405.0  epsilon: 1.0    steps: 440  evaluation reward: 382.95\n",
      "Training network. lr: 0.000119. clip: 0.047625\n",
      "Iteration 17086: Policy loss: -0.118891. Value loss: 0.113644. Entropy: 0.279183.\n",
      "Iteration 17087: Policy loss: -0.124869. Value loss: 0.041476. Entropy: 0.279577.\n",
      "Iteration 17088: Policy loss: -0.124317. Value loss: 0.026858. Entropy: 0.279422.\n",
      "episode: 6000   score: 385.0  epsilon: 1.0    steps: 624  evaluation reward: 384.2\n",
      "Training network. lr: 0.000119. clip: 0.047625\n",
      "Iteration 17089: Policy loss: 0.352043. Value loss: 0.131315. Entropy: 0.293302.\n",
      "Iteration 17090: Policy loss: 0.354733. Value loss: 0.053392. Entropy: 0.293189.\n",
      "Iteration 17091: Policy loss: 0.344892. Value loss: 0.036300. Entropy: 0.291957.\n",
      "now time :  2019-09-06 07:55:42.720572\n",
      "episode: 6001   score: 550.0  epsilon: 1.0    steps: 168  evaluation reward: 385.3\n",
      "episode: 6002   score: 360.0  epsilon: 1.0    steps: 600  evaluation reward: 386.8\n",
      "Training network. lr: 0.000119. clip: 0.047625\n",
      "Iteration 17092: Policy loss: 0.060181. Value loss: 0.180472. Entropy: 0.291526.\n",
      "Iteration 17093: Policy loss: 0.060892. Value loss: 0.071980. Entropy: 0.293567.\n",
      "Iteration 17094: Policy loss: 0.056717. Value loss: 0.047190. Entropy: 0.292126.\n",
      "episode: 6003   score: 365.0  epsilon: 1.0    steps: 584  evaluation reward: 385.8\n",
      "episode: 6004   score: 285.0  epsilon: 1.0    steps: 704  evaluation reward: 385.35\n",
      "Training network. lr: 0.000119. clip: 0.047625\n",
      "Iteration 17095: Policy loss: 0.461999. Value loss: 0.199680. Entropy: 0.285827.\n",
      "Iteration 17096: Policy loss: 0.465692. Value loss: 0.076614. Entropy: 0.284313.\n",
      "Iteration 17097: Policy loss: 0.449893. Value loss: 0.051062. Entropy: 0.284758.\n",
      "Training network. lr: 0.000119. clip: 0.047625\n",
      "Iteration 17098: Policy loss: -0.184009. Value loss: 0.162537. Entropy: 0.264480.\n",
      "Iteration 17099: Policy loss: -0.197711. Value loss: 0.081754. Entropy: 0.259081.\n",
      "Iteration 17100: Policy loss: -0.189661. Value loss: 0.057831. Entropy: 0.261748.\n",
      "Training network. lr: 0.000119. clip: 0.047478\n",
      "Iteration 17101: Policy loss: -0.220287. Value loss: 0.305412. Entropy: 0.287891.\n",
      "Iteration 17102: Policy loss: -0.197649. Value loss: 0.137898. Entropy: 0.287737.\n",
      "Iteration 17103: Policy loss: -0.247292. Value loss: 0.087897. Entropy: 0.285558.\n",
      "episode: 6005   score: 210.0  epsilon: 1.0    steps: 632  evaluation reward: 380.45\n",
      "episode: 6006   score: 315.0  epsilon: 1.0    steps: 704  evaluation reward: 378.3\n",
      "Training network. lr: 0.000119. clip: 0.047478\n",
      "Iteration 17104: Policy loss: -0.064530. Value loss: 0.219674. Entropy: 0.269194.\n",
      "Iteration 17105: Policy loss: -0.079451. Value loss: 0.076463. Entropy: 0.270885.\n",
      "Iteration 17106: Policy loss: -0.087320. Value loss: 0.053149. Entropy: 0.270300.\n",
      "episode: 6007   score: 155.0  epsilon: 1.0    steps: 528  evaluation reward: 376.35\n",
      "Training network. lr: 0.000119. clip: 0.047478\n",
      "Iteration 17107: Policy loss: 0.296983. Value loss: 0.138996. Entropy: 0.292435.\n",
      "Iteration 17108: Policy loss: 0.282448. Value loss: 0.058177. Entropy: 0.290931.\n",
      "Iteration 17109: Policy loss: 0.284835. Value loss: 0.042174. Entropy: 0.290169.\n",
      "episode: 6008   score: 455.0  epsilon: 1.0    steps: 808  evaluation reward: 376.65\n",
      "Training network. lr: 0.000119. clip: 0.047478\n",
      "Iteration 17110: Policy loss: 0.143710. Value loss: 0.167705. Entropy: 0.291248.\n",
      "Iteration 17111: Policy loss: 0.150174. Value loss: 0.055978. Entropy: 0.292268.\n",
      "Iteration 17112: Policy loss: 0.132490. Value loss: 0.041178. Entropy: 0.290152.\n",
      "Training network. lr: 0.000119. clip: 0.047478\n",
      "Iteration 17113: Policy loss: -0.108428. Value loss: 0.071526. Entropy: 0.275551.\n",
      "Iteration 17114: Policy loss: -0.115240. Value loss: 0.037503. Entropy: 0.275314.\n",
      "Iteration 17115: Policy loss: -0.112470. Value loss: 0.030001. Entropy: 0.275395.\n",
      "episode: 6009   score: 420.0  epsilon: 1.0    steps: 360  evaluation reward: 376.6\n",
      "Training network. lr: 0.000119. clip: 0.047478\n",
      "Iteration 17116: Policy loss: 0.030889. Value loss: 0.075153. Entropy: 0.252242.\n",
      "Iteration 17117: Policy loss: 0.030675. Value loss: 0.028885. Entropy: 0.255975.\n",
      "Iteration 17118: Policy loss: 0.022296. Value loss: 0.018637. Entropy: 0.255921.\n",
      "episode: 6010   score: 425.0  epsilon: 1.0    steps: 448  evaluation reward: 375.4\n",
      "Training network. lr: 0.000119. clip: 0.047478\n",
      "Iteration 17119: Policy loss: 0.215869. Value loss: 0.161316. Entropy: 0.256993.\n",
      "Iteration 17120: Policy loss: 0.214201. Value loss: 0.061689. Entropy: 0.257593.\n",
      "Iteration 17121: Policy loss: 0.207695. Value loss: 0.037224. Entropy: 0.256734.\n",
      "episode: 6011   score: 420.0  epsilon: 1.0    steps: 792  evaluation reward: 377.5\n",
      "Training network. lr: 0.000119. clip: 0.047478\n",
      "Iteration 17122: Policy loss: 0.129484. Value loss: 0.128432. Entropy: 0.302342.\n",
      "Iteration 17123: Policy loss: 0.123122. Value loss: 0.054085. Entropy: 0.299844.\n",
      "Iteration 17124: Policy loss: 0.125170. Value loss: 0.034876. Entropy: 0.299858.\n",
      "episode: 6012   score: 290.0  epsilon: 1.0    steps: 296  evaluation reward: 377.25\n",
      "Training network. lr: 0.000119. clip: 0.047478\n",
      "Iteration 17125: Policy loss: 0.036979. Value loss: 0.102202. Entropy: 0.287604.\n",
      "Iteration 17126: Policy loss: 0.029367. Value loss: 0.045713. Entropy: 0.286798.\n",
      "Iteration 17127: Policy loss: 0.032660. Value loss: 0.033232. Entropy: 0.286778.\n",
      "episode: 6013   score: 700.0  epsilon: 1.0    steps: 136  evaluation reward: 380.35\n",
      "episode: 6014   score: 390.0  epsilon: 1.0    steps: 824  evaluation reward: 380.9\n",
      "Training network. lr: 0.000119. clip: 0.047478\n",
      "Iteration 17128: Policy loss: 0.118310. Value loss: 0.127980. Entropy: 0.296514.\n",
      "Iteration 17129: Policy loss: 0.101710. Value loss: 0.052706. Entropy: 0.295110.\n",
      "Iteration 17130: Policy loss: 0.109462. Value loss: 0.035271. Entropy: 0.295470.\n",
      "episode: 6015   score: 180.0  epsilon: 1.0    steps: 984  evaluation reward: 380.15\n",
      "Training network. lr: 0.000119. clip: 0.047478\n",
      "Iteration 17131: Policy loss: 0.047108. Value loss: 0.122120. Entropy: 0.279355.\n",
      "Iteration 17132: Policy loss: 0.040738. Value loss: 0.058835. Entropy: 0.279239.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17133: Policy loss: 0.041718. Value loss: 0.040474. Entropy: 0.279931.\n",
      "episode: 6016   score: 355.0  epsilon: 1.0    steps: 960  evaluation reward: 379.3\n",
      "Training network. lr: 0.000119. clip: 0.047478\n",
      "Iteration 17134: Policy loss: -0.106789. Value loss: 0.117014. Entropy: 0.306142.\n",
      "Iteration 17135: Policy loss: -0.111454. Value loss: 0.048059. Entropy: 0.305196.\n",
      "Iteration 17136: Policy loss: -0.107889. Value loss: 0.032812. Entropy: 0.305106.\n",
      "episode: 6017   score: 355.0  epsilon: 1.0    steps: 488  evaluation reward: 378.95\n",
      "Training network. lr: 0.000119. clip: 0.047478\n",
      "Iteration 17137: Policy loss: -0.215688. Value loss: 0.139625. Entropy: 0.273363.\n",
      "Iteration 17138: Policy loss: -0.221238. Value loss: 0.056113. Entropy: 0.273055.\n",
      "Iteration 17139: Policy loss: -0.219813. Value loss: 0.036992. Entropy: 0.272798.\n",
      "Training network. lr: 0.000119. clip: 0.047478\n",
      "Iteration 17140: Policy loss: 0.043245. Value loss: 0.137125. Entropy: 0.284343.\n",
      "Iteration 17141: Policy loss: 0.032781. Value loss: 0.050498. Entropy: 0.285191.\n",
      "Iteration 17142: Policy loss: 0.019632. Value loss: 0.031268. Entropy: 0.283795.\n",
      "episode: 6018   score: 240.0  epsilon: 1.0    steps: 360  evaluation reward: 380.45\n",
      "episode: 6019   score: 210.0  epsilon: 1.0    steps: 600  evaluation reward: 379.4\n",
      "Training network. lr: 0.000119. clip: 0.047478\n",
      "Iteration 17143: Policy loss: -0.230250. Value loss: 0.350271. Entropy: 0.273925.\n",
      "Iteration 17144: Policy loss: -0.251540. Value loss: 0.187349. Entropy: 0.274523.\n",
      "Iteration 17145: Policy loss: -0.244730. Value loss: 0.076661. Entropy: 0.274865.\n",
      "episode: 6020   score: 360.0  epsilon: 1.0    steps: 256  evaluation reward: 378.1\n",
      "episode: 6021   score: 580.0  epsilon: 1.0    steps: 864  evaluation reward: 380.7\n",
      "Training network. lr: 0.000119. clip: 0.047478\n",
      "Iteration 17146: Policy loss: 0.057549. Value loss: 0.173877. Entropy: 0.295607.\n",
      "Iteration 17147: Policy loss: 0.044516. Value loss: 0.086816. Entropy: 0.296194.\n",
      "Iteration 17148: Policy loss: 0.059472. Value loss: 0.063737. Entropy: 0.294721.\n",
      "episode: 6022   score: 150.0  epsilon: 1.0    steps: 672  evaluation reward: 380.35\n",
      "episode: 6023   score: 365.0  epsilon: 1.0    steps: 800  evaluation reward: 380.55\n",
      "Training network. lr: 0.000119. clip: 0.047478\n",
      "Iteration 17149: Policy loss: 0.103160. Value loss: 0.123043. Entropy: 0.285756.\n",
      "Iteration 17150: Policy loss: 0.104478. Value loss: 0.061361. Entropy: 0.285471.\n",
      "Iteration 17151: Policy loss: 0.097929. Value loss: 0.043097. Entropy: 0.283736.\n",
      "episode: 6024   score: 240.0  epsilon: 1.0    steps: 336  evaluation reward: 379.6\n",
      "Training network. lr: 0.000118. clip: 0.047321\n",
      "Iteration 17152: Policy loss: 0.115846. Value loss: 0.118709. Entropy: 0.292944.\n",
      "Iteration 17153: Policy loss: 0.099962. Value loss: 0.045712. Entropy: 0.294382.\n",
      "Iteration 17154: Policy loss: 0.097866. Value loss: 0.035734. Entropy: 0.293223.\n",
      "Training network. lr: 0.000118. clip: 0.047321\n",
      "Iteration 17155: Policy loss: -0.374156. Value loss: 0.363604. Entropy: 0.308856.\n",
      "Iteration 17156: Policy loss: -0.371313. Value loss: 0.144630. Entropy: 0.308058.\n",
      "Iteration 17157: Policy loss: -0.380754. Value loss: 0.064220. Entropy: 0.309576.\n",
      "Training network. lr: 0.000118. clip: 0.047321\n",
      "Iteration 17158: Policy loss: -0.004084. Value loss: 0.184206. Entropy: 0.294714.\n",
      "Iteration 17159: Policy loss: 0.007710. Value loss: 0.051705. Entropy: 0.289191.\n",
      "Iteration 17160: Policy loss: -0.005287. Value loss: 0.035462. Entropy: 0.292738.\n",
      "episode: 6025   score: 235.0  epsilon: 1.0    steps: 592  evaluation reward: 378.05\n",
      "episode: 6026   score: 185.0  epsilon: 1.0    steps: 752  evaluation reward: 375.5\n",
      "Training network. lr: 0.000118. clip: 0.047321\n",
      "Iteration 17161: Policy loss: 0.238094. Value loss: 0.104723. Entropy: 0.278310.\n",
      "Iteration 17162: Policy loss: 0.239319. Value loss: 0.040743. Entropy: 0.280509.\n",
      "Iteration 17163: Policy loss: 0.228524. Value loss: 0.029861. Entropy: 0.280287.\n",
      "Training network. lr: 0.000118. clip: 0.047321\n",
      "Iteration 17164: Policy loss: -0.497379. Value loss: 0.355755. Entropy: 0.298888.\n",
      "Iteration 17165: Policy loss: -0.504308. Value loss: 0.169843. Entropy: 0.296822.\n",
      "Iteration 17166: Policy loss: -0.497784. Value loss: 0.095313. Entropy: 0.298342.\n",
      "episode: 6027   score: 285.0  epsilon: 1.0    steps: 496  evaluation reward: 375.6\n",
      "episode: 6028   score: 185.0  epsilon: 1.0    steps: 512  evaluation reward: 372.6\n",
      "episode: 6029   score: 870.0  epsilon: 1.0    steps: 672  evaluation reward: 377.7\n",
      "episode: 6030   score: 550.0  epsilon: 1.0    steps: 744  evaluation reward: 381.95\n",
      "Training network. lr: 0.000118. clip: 0.047321\n",
      "Iteration 17167: Policy loss: -0.134255. Value loss: 0.284763. Entropy: 0.278167.\n",
      "Iteration 17168: Policy loss: -0.141843. Value loss: 0.139430. Entropy: 0.283681.\n",
      "Iteration 17169: Policy loss: -0.136479. Value loss: 0.075504. Entropy: 0.280577.\n",
      "episode: 6031   score: 645.0  epsilon: 1.0    steps: 960  evaluation reward: 385.5\n",
      "Training network. lr: 0.000118. clip: 0.047321\n",
      "Iteration 17170: Policy loss: 0.037540. Value loss: 0.194196. Entropy: 0.294537.\n",
      "Iteration 17171: Policy loss: 0.034342. Value loss: 0.074791. Entropy: 0.295893.\n",
      "Iteration 17172: Policy loss: 0.032993. Value loss: 0.052702. Entropy: 0.294865.\n",
      "episode: 6032   score: 285.0  epsilon: 1.0    steps: 640  evaluation reward: 384.4\n",
      "Training network. lr: 0.000118. clip: 0.047321\n",
      "Iteration 17173: Policy loss: 0.020099. Value loss: 0.079002. Entropy: 0.290201.\n",
      "Iteration 17174: Policy loss: 0.017382. Value loss: 0.040122. Entropy: 0.290418.\n",
      "Iteration 17175: Policy loss: 0.018824. Value loss: 0.032905. Entropy: 0.290431.\n",
      "Training network. lr: 0.000118. clip: 0.047321\n",
      "Iteration 17176: Policy loss: 0.228090. Value loss: 0.202434. Entropy: 0.289037.\n",
      "Iteration 17177: Policy loss: 0.228655. Value loss: 0.099268. Entropy: 0.290010.\n",
      "Iteration 17178: Policy loss: 0.224160. Value loss: 0.068032. Entropy: 0.287571.\n",
      "episode: 6033   score: 125.0  epsilon: 1.0    steps: 912  evaluation reward: 382.95\n",
      "Training network. lr: 0.000118. clip: 0.047321\n",
      "Iteration 17179: Policy loss: 0.031164. Value loss: 0.081645. Entropy: 0.293699.\n",
      "Iteration 17180: Policy loss: 0.031920. Value loss: 0.042528. Entropy: 0.295096.\n",
      "Iteration 17181: Policy loss: 0.026458. Value loss: 0.034057. Entropy: 0.294588.\n",
      "Training network. lr: 0.000118. clip: 0.047321\n",
      "Iteration 17182: Policy loss: 0.118732. Value loss: 0.191129. Entropy: 0.292678.\n",
      "Iteration 17183: Policy loss: 0.107180. Value loss: 0.110618. Entropy: 0.293165.\n",
      "Iteration 17184: Policy loss: 0.105328. Value loss: 0.080786. Entropy: 0.293908.\n",
      "episode: 6034   score: 210.0  epsilon: 1.0    steps: 240  evaluation reward: 379.45\n",
      "episode: 6035   score: 420.0  epsilon: 1.0    steps: 880  evaluation reward: 379.3\n",
      "Training network. lr: 0.000118. clip: 0.047321\n",
      "Iteration 17185: Policy loss: -0.015762. Value loss: 0.161478. Entropy: 0.276058.\n",
      "Iteration 17186: Policy loss: -0.016049. Value loss: 0.063130. Entropy: 0.276615.\n",
      "Iteration 17187: Policy loss: -0.023284. Value loss: 0.045392. Entropy: 0.276379.\n",
      "episode: 6036   score: 315.0  epsilon: 1.0    steps: 544  evaluation reward: 376.75\n",
      "episode: 6037   score: 320.0  epsilon: 1.0    steps: 576  evaluation reward: 375.1\n",
      "Training network. lr: 0.000118. clip: 0.047321\n",
      "Iteration 17188: Policy loss: 0.138430. Value loss: 0.125408. Entropy: 0.264338.\n",
      "Iteration 17189: Policy loss: 0.139070. Value loss: 0.056861. Entropy: 0.266913.\n",
      "Iteration 17190: Policy loss: 0.129932. Value loss: 0.039858. Entropy: 0.267455.\n",
      "episode: 6038   score: 230.0  epsilon: 1.0    steps: 760  evaluation reward: 373.2\n",
      "Training network. lr: 0.000118. clip: 0.047321\n",
      "Iteration 17191: Policy loss: -0.732586. Value loss: 0.469827. Entropy: 0.299567.\n",
      "Iteration 17192: Policy loss: -0.731324. Value loss: 0.272049. Entropy: 0.300577.\n",
      "Iteration 17193: Policy loss: -0.740776. Value loss: 0.206469. Entropy: 0.300373.\n",
      "episode: 6039   score: 615.0  epsilon: 1.0    steps: 16  evaluation reward: 376.75\n",
      "Training network. lr: 0.000118. clip: 0.047321\n",
      "Iteration 17194: Policy loss: -0.209742. Value loss: 0.199212. Entropy: 0.279307.\n",
      "Iteration 17195: Policy loss: -0.222243. Value loss: 0.079216. Entropy: 0.280413.\n",
      "Iteration 17196: Policy loss: -0.223339. Value loss: 0.057484. Entropy: 0.277879.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000118. clip: 0.047321\n",
      "Iteration 17197: Policy loss: -0.257769. Value loss: 0.285911. Entropy: 0.291044.\n",
      "Iteration 17198: Policy loss: -0.263711. Value loss: 0.123575. Entropy: 0.290510.\n",
      "Iteration 17199: Policy loss: -0.272068. Value loss: 0.075916. Entropy: 0.290929.\n",
      "episode: 6040   score: 165.0  epsilon: 1.0    steps: 216  evaluation reward: 374.45\n",
      "Training network. lr: 0.000118. clip: 0.047321\n",
      "Iteration 17200: Policy loss: 0.046707. Value loss: 0.466569. Entropy: 0.292164.\n",
      "Iteration 17201: Policy loss: 0.015866. Value loss: 0.307577. Entropy: 0.287236.\n",
      "Iteration 17202: Policy loss: 0.018170. Value loss: 0.228137. Entropy: 0.290205.\n",
      "episode: 6041   score: 500.0  epsilon: 1.0    steps: 192  evaluation reward: 376.25\n",
      "Training network. lr: 0.000118. clip: 0.047165\n",
      "Iteration 17203: Policy loss: -0.164834. Value loss: 0.161824. Entropy: 0.277829.\n",
      "Iteration 17204: Policy loss: -0.167687. Value loss: 0.077377. Entropy: 0.277553.\n",
      "Iteration 17205: Policy loss: -0.167945. Value loss: 0.054296. Entropy: 0.278989.\n",
      "Training network. lr: 0.000118. clip: 0.047165\n",
      "Iteration 17206: Policy loss: 0.239189. Value loss: 0.196282. Entropy: 0.301085.\n",
      "Iteration 17207: Policy loss: 0.232262. Value loss: 0.073588. Entropy: 0.300434.\n",
      "Iteration 17208: Policy loss: 0.226633. Value loss: 0.047514. Entropy: 0.299938.\n",
      "Training network. lr: 0.000118. clip: 0.047165\n",
      "Iteration 17209: Policy loss: -0.192347. Value loss: 0.177634. Entropy: 0.270957.\n",
      "Iteration 17210: Policy loss: -0.197360. Value loss: 0.085278. Entropy: 0.273127.\n",
      "Iteration 17211: Policy loss: -0.202296. Value loss: 0.057986. Entropy: 0.273515.\n",
      "episode: 6042   score: 315.0  epsilon: 1.0    steps: 408  evaluation reward: 375.8\n",
      "Training network. lr: 0.000118. clip: 0.047165\n",
      "Iteration 17212: Policy loss: 0.043684. Value loss: 0.189314. Entropy: 0.273978.\n",
      "Iteration 17213: Policy loss: 0.048041. Value loss: 0.095911. Entropy: 0.272243.\n",
      "Iteration 17214: Policy loss: 0.050120. Value loss: 0.064557. Entropy: 0.272099.\n",
      "episode: 6043   score: 760.0  epsilon: 1.0    steps: 320  evaluation reward: 379.45\n",
      "episode: 6044   score: 620.0  epsilon: 1.0    steps: 448  evaluation reward: 380.95\n",
      "episode: 6045   score: 485.0  epsilon: 1.0    steps: 456  evaluation reward: 383.7\n",
      "Training network. lr: 0.000118. clip: 0.047165\n",
      "Iteration 17215: Policy loss: -0.177961. Value loss: 0.331463. Entropy: 0.277825.\n",
      "Iteration 17216: Policy loss: -0.175403. Value loss: 0.101738. Entropy: 0.275753.\n",
      "Iteration 17217: Policy loss: -0.187907. Value loss: 0.063661. Entropy: 0.276221.\n",
      "episode: 6046   score: 300.0  epsilon: 1.0    steps: 816  evaluation reward: 383.55\n",
      "Training network. lr: 0.000118. clip: 0.047165\n",
      "Iteration 17218: Policy loss: 0.095786. Value loss: 0.248876. Entropy: 0.303175.\n",
      "Iteration 17219: Policy loss: 0.086108. Value loss: 0.122235. Entropy: 0.302643.\n",
      "Iteration 17220: Policy loss: 0.082154. Value loss: 0.083178. Entropy: 0.302080.\n",
      "episode: 6047   score: 695.0  epsilon: 1.0    steps: 576  evaluation reward: 385.6\n",
      "episode: 6048   score: 1000.0  epsilon: 1.0    steps: 688  evaluation reward: 393.5\n",
      "Training network. lr: 0.000118. clip: 0.047165\n",
      "Iteration 17221: Policy loss: 0.069746. Value loss: 0.107935. Entropy: 0.288903.\n",
      "Iteration 17222: Policy loss: 0.075144. Value loss: 0.053713. Entropy: 0.289969.\n",
      "Iteration 17223: Policy loss: 0.070732. Value loss: 0.040051. Entropy: 0.288396.\n",
      "Training network. lr: 0.000118. clip: 0.047165\n",
      "Iteration 17224: Policy loss: 0.186045. Value loss: 0.204213. Entropy: 0.306349.\n",
      "Iteration 17225: Policy loss: 0.162430. Value loss: 0.072600. Entropy: 0.306754.\n",
      "Iteration 17226: Policy loss: 0.155541. Value loss: 0.051253. Entropy: 0.306214.\n",
      "episode: 6049   score: 395.0  epsilon: 1.0    steps: 448  evaluation reward: 395.0\n",
      "Training network. lr: 0.000118. clip: 0.047165\n",
      "Iteration 17227: Policy loss: 0.315616. Value loss: 0.131277. Entropy: 0.286189.\n",
      "Iteration 17228: Policy loss: 0.314418. Value loss: 0.055078. Entropy: 0.287039.\n",
      "Iteration 17229: Policy loss: 0.310434. Value loss: 0.042447. Entropy: 0.288147.\n",
      "Training network. lr: 0.000118. clip: 0.047165\n",
      "Iteration 17230: Policy loss: -0.290118. Value loss: 0.247238. Entropy: 0.294361.\n",
      "Iteration 17231: Policy loss: -0.304877. Value loss: 0.095538. Entropy: 0.290916.\n",
      "Iteration 17232: Policy loss: -0.305829. Value loss: 0.056779. Entropy: 0.291942.\n",
      "episode: 6050   score: 315.0  epsilon: 1.0    steps: 496  evaluation reward: 393.75\n",
      "Training network. lr: 0.000118. clip: 0.047165\n",
      "Iteration 17233: Policy loss: 0.332218. Value loss: 0.249906. Entropy: 0.293807.\n",
      "Iteration 17234: Policy loss: 0.323139. Value loss: 0.096637. Entropy: 0.294446.\n",
      "Iteration 17235: Policy loss: 0.304817. Value loss: 0.056709. Entropy: 0.294991.\n",
      "now time :  2019-09-06 08:04:37.525651\n",
      "episode: 6051   score: 305.0  epsilon: 1.0    steps: 56  evaluation reward: 392.55\n",
      "episode: 6052   score: 330.0  epsilon: 1.0    steps: 184  evaluation reward: 389.9\n",
      "Training network. lr: 0.000118. clip: 0.047165\n",
      "Iteration 17236: Policy loss: 0.004231. Value loss: 0.145581. Entropy: 0.280575.\n",
      "Iteration 17237: Policy loss: -0.001851. Value loss: 0.056248. Entropy: 0.278539.\n",
      "Iteration 17238: Policy loss: -0.002311. Value loss: 0.037114. Entropy: 0.277964.\n",
      "Training network. lr: 0.000118. clip: 0.047165\n",
      "Iteration 17239: Policy loss: -0.014546. Value loss: 0.138464. Entropy: 0.300206.\n",
      "Iteration 17240: Policy loss: -0.020917. Value loss: 0.060294. Entropy: 0.300905.\n",
      "Iteration 17241: Policy loss: -0.020918. Value loss: 0.043574. Entropy: 0.300673.\n",
      "episode: 6053   score: 400.0  epsilon: 1.0    steps: 656  evaluation reward: 390.95\n",
      "Training network. lr: 0.000118. clip: 0.047165\n",
      "Iteration 17242: Policy loss: 0.103111. Value loss: 0.112774. Entropy: 0.267827.\n",
      "Iteration 17243: Policy loss: 0.098100. Value loss: 0.046059. Entropy: 0.265087.\n",
      "Iteration 17244: Policy loss: 0.091798. Value loss: 0.031544. Entropy: 0.266219.\n",
      "Training network. lr: 0.000118. clip: 0.047165\n",
      "Iteration 17245: Policy loss: -0.092383. Value loss: 0.330921. Entropy: 0.274325.\n",
      "Iteration 17246: Policy loss: -0.098852. Value loss: 0.214342. Entropy: 0.273645.\n",
      "Iteration 17247: Policy loss: -0.113587. Value loss: 0.178220. Entropy: 0.273277.\n",
      "episode: 6054   score: 420.0  epsilon: 1.0    steps: 32  evaluation reward: 389.8\n",
      "Training network. lr: 0.000118. clip: 0.047165\n",
      "Iteration 17248: Policy loss: 0.404257. Value loss: 0.134515. Entropy: 0.287259.\n",
      "Iteration 17249: Policy loss: 0.398067. Value loss: 0.056835. Entropy: 0.285887.\n",
      "Iteration 17250: Policy loss: 0.391715. Value loss: 0.040161. Entropy: 0.286226.\n",
      "episode: 6055   score: 435.0  epsilon: 1.0    steps: 1008  evaluation reward: 387.15\n",
      "Training network. lr: 0.000118. clip: 0.047017\n",
      "Iteration 17251: Policy loss: 0.049217. Value loss: 0.096507. Entropy: 0.295450.\n",
      "Iteration 17252: Policy loss: 0.046391. Value loss: 0.052378. Entropy: 0.296617.\n",
      "Iteration 17253: Policy loss: 0.047344. Value loss: 0.038935. Entropy: 0.296606.\n",
      "episode: 6056   score: 635.0  epsilon: 1.0    steps: 344  evaluation reward: 388.55\n",
      "episode: 6057   score: 345.0  epsilon: 1.0    steps: 864  evaluation reward: 389.75\n",
      "Training network. lr: 0.000118. clip: 0.047017\n",
      "Iteration 17254: Policy loss: 0.121667. Value loss: 0.122240. Entropy: 0.281843.\n",
      "Iteration 17255: Policy loss: 0.123796. Value loss: 0.062287. Entropy: 0.282437.\n",
      "Iteration 17256: Policy loss: 0.119442. Value loss: 0.042961. Entropy: 0.280627.\n",
      "episode: 6058   score: 515.0  epsilon: 1.0    steps: 120  evaluation reward: 393.65\n",
      "episode: 6059   score: 415.0  epsilon: 1.0    steps: 832  evaluation reward: 394.85\n",
      "Training network. lr: 0.000118. clip: 0.047017\n",
      "Iteration 17257: Policy loss: 0.182477. Value loss: 0.111761. Entropy: 0.289804.\n",
      "Iteration 17258: Policy loss: 0.173165. Value loss: 0.040955. Entropy: 0.290059.\n",
      "Iteration 17259: Policy loss: 0.164370. Value loss: 0.029200. Entropy: 0.290457.\n",
      "Training network. lr: 0.000118. clip: 0.047017\n",
      "Iteration 17260: Policy loss: -0.000970. Value loss: 0.105409. Entropy: 0.306026.\n",
      "Iteration 17261: Policy loss: -0.005673. Value loss: 0.039574. Entropy: 0.304612.\n",
      "Iteration 17262: Policy loss: -0.003831. Value loss: 0.027073. Entropy: 0.305320.\n",
      "episode: 6060   score: 300.0  epsilon: 1.0    steps: 72  evaluation reward: 392.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6061   score: 375.0  epsilon: 1.0    steps: 792  evaluation reward: 393.3\n",
      "Training network. lr: 0.000118. clip: 0.047017\n",
      "Iteration 17263: Policy loss: -0.272012. Value loss: 0.293800. Entropy: 0.284555.\n",
      "Iteration 17264: Policy loss: -0.270245. Value loss: 0.112442. Entropy: 0.281512.\n",
      "Iteration 17265: Policy loss: -0.285505. Value loss: 0.075496. Entropy: 0.281904.\n",
      "Training network. lr: 0.000118. clip: 0.047017\n",
      "Iteration 17266: Policy loss: -0.213420. Value loss: 0.314065. Entropy: 0.307002.\n",
      "Iteration 17267: Policy loss: -0.218894. Value loss: 0.105345. Entropy: 0.306171.\n",
      "Iteration 17268: Policy loss: -0.236527. Value loss: 0.045317. Entropy: 0.306950.\n",
      "Training network. lr: 0.000118. clip: 0.047017\n",
      "Iteration 17269: Policy loss: 0.008075. Value loss: 0.114632. Entropy: 0.296395.\n",
      "Iteration 17270: Policy loss: -0.002894. Value loss: 0.045550. Entropy: 0.293268.\n",
      "Iteration 17271: Policy loss: -0.003653. Value loss: 0.036575. Entropy: 0.294493.\n",
      "episode: 6062   score: 440.0  epsilon: 1.0    steps: 688  evaluation reward: 396.2\n",
      "Training network. lr: 0.000118. clip: 0.047017\n",
      "Iteration 17272: Policy loss: 0.183816. Value loss: 0.133943. Entropy: 0.302724.\n",
      "Iteration 17273: Policy loss: 0.176751. Value loss: 0.050804. Entropy: 0.302381.\n",
      "Iteration 17274: Policy loss: 0.166675. Value loss: 0.035908. Entropy: 0.301564.\n",
      "episode: 6063   score: 565.0  epsilon: 1.0    steps: 616  evaluation reward: 398.45\n",
      "Training network. lr: 0.000118. clip: 0.047017\n",
      "Iteration 17275: Policy loss: 0.121800. Value loss: 0.132511. Entropy: 0.300654.\n",
      "Iteration 17276: Policy loss: 0.131699. Value loss: 0.050707. Entropy: 0.301126.\n",
      "Iteration 17277: Policy loss: 0.126204. Value loss: 0.033585. Entropy: 0.300393.\n",
      "episode: 6064   score: 390.0  epsilon: 1.0    steps: 952  evaluation reward: 394.7\n",
      "Training network. lr: 0.000118. clip: 0.047017\n",
      "Iteration 17278: Policy loss: 0.167076. Value loss: 0.115362. Entropy: 0.298634.\n",
      "Iteration 17279: Policy loss: 0.163482. Value loss: 0.059358. Entropy: 0.297008.\n",
      "Iteration 17280: Policy loss: 0.163519. Value loss: 0.046783. Entropy: 0.297317.\n",
      "episode: 6065   score: 655.0  epsilon: 1.0    steps: 344  evaluation reward: 398.4\n",
      "Training network. lr: 0.000118. clip: 0.047017\n",
      "Iteration 17281: Policy loss: 0.073553. Value loss: 0.082966. Entropy: 0.295624.\n",
      "Iteration 17282: Policy loss: 0.069986. Value loss: 0.032057. Entropy: 0.295621.\n",
      "Iteration 17283: Policy loss: 0.071266. Value loss: 0.024328. Entropy: 0.298041.\n",
      "Training network. lr: 0.000118. clip: 0.047017\n",
      "Iteration 17284: Policy loss: -0.009878. Value loss: 0.105078. Entropy: 0.294244.\n",
      "Iteration 17285: Policy loss: -0.013396. Value loss: 0.045513. Entropy: 0.293152.\n",
      "Iteration 17286: Policy loss: -0.019363. Value loss: 0.030181. Entropy: 0.293561.\n",
      "episode: 6066   score: 495.0  epsilon: 1.0    steps: 608  evaluation reward: 400.2\n",
      "episode: 6067   score: 450.0  epsilon: 1.0    steps: 608  evaluation reward: 401.25\n",
      "Training network. lr: 0.000118. clip: 0.047017\n",
      "Iteration 17287: Policy loss: -0.378687. Value loss: 0.485413. Entropy: 0.288859.\n",
      "Iteration 17288: Policy loss: -0.379267. Value loss: 0.247724. Entropy: 0.289315.\n",
      "Iteration 17289: Policy loss: -0.376886. Value loss: 0.199477. Entropy: 0.290742.\n",
      "Training network. lr: 0.000118. clip: 0.047017\n",
      "Iteration 17290: Policy loss: -0.433771. Value loss: 0.170106. Entropy: 0.269193.\n",
      "Iteration 17291: Policy loss: -0.428330. Value loss: 0.049412. Entropy: 0.267007.\n",
      "Iteration 17292: Policy loss: -0.437039. Value loss: 0.035800. Entropy: 0.268257.\n",
      "Training network. lr: 0.000118. clip: 0.047017\n",
      "Iteration 17293: Policy loss: -0.107476. Value loss: 0.302073. Entropy: 0.302816.\n",
      "Iteration 17294: Policy loss: -0.131711. Value loss: 0.165657. Entropy: 0.303232.\n",
      "Iteration 17295: Policy loss: -0.125006. Value loss: 0.092767. Entropy: 0.302688.\n",
      "episode: 6068   score: 620.0  epsilon: 1.0    steps: 144  evaluation reward: 401.55\n",
      "episode: 6069   score: 450.0  epsilon: 1.0    steps: 720  evaluation reward: 402.0\n",
      "Training network. lr: 0.000118. clip: 0.047017\n",
      "Iteration 17296: Policy loss: 0.117581. Value loss: 0.132545. Entropy: 0.273721.\n",
      "Iteration 17297: Policy loss: 0.115731. Value loss: 0.062095. Entropy: 0.270470.\n",
      "Iteration 17298: Policy loss: 0.107819. Value loss: 0.039528. Entropy: 0.271387.\n",
      "episode: 6070   score: 350.0  epsilon: 1.0    steps: 896  evaluation reward: 399.1\n",
      "Training network. lr: 0.000118. clip: 0.047017\n",
      "Iteration 17299: Policy loss: 0.171898. Value loss: 0.117789. Entropy: 0.303202.\n",
      "Iteration 17300: Policy loss: 0.164041. Value loss: 0.049128. Entropy: 0.301787.\n",
      "Iteration 17301: Policy loss: 0.153786. Value loss: 0.034077. Entropy: 0.301046.\n",
      "Training network. lr: 0.000117. clip: 0.046861\n",
      "Iteration 17302: Policy loss: 0.333277. Value loss: 0.135849. Entropy: 0.293289.\n",
      "Iteration 17303: Policy loss: 0.335858. Value loss: 0.047500. Entropy: 0.290293.\n",
      "Iteration 17304: Policy loss: 0.330226. Value loss: 0.031056. Entropy: 0.291440.\n",
      "episode: 6071   score: 565.0  epsilon: 1.0    steps: 96  evaluation reward: 399.8\n",
      "episode: 6072   score: 670.0  epsilon: 1.0    steps: 592  evaluation reward: 403.5\n",
      "Training network. lr: 0.000117. clip: 0.046861\n",
      "Iteration 17305: Policy loss: 0.147025. Value loss: 0.125266. Entropy: 0.294628.\n",
      "Iteration 17306: Policy loss: 0.142235. Value loss: 0.052639. Entropy: 0.293602.\n",
      "Iteration 17307: Policy loss: 0.137746. Value loss: 0.036554. Entropy: 0.292991.\n",
      "Training network. lr: 0.000117. clip: 0.046861\n",
      "Iteration 17308: Policy loss: 0.108410. Value loss: 0.140738. Entropy: 0.300610.\n",
      "Iteration 17309: Policy loss: 0.100780. Value loss: 0.054643. Entropy: 0.298660.\n",
      "Iteration 17310: Policy loss: 0.095873. Value loss: 0.036647. Entropy: 0.298258.\n",
      "episode: 6073   score: 390.0  epsilon: 1.0    steps: 336  evaluation reward: 401.9\n",
      "episode: 6074   score: 375.0  epsilon: 1.0    steps: 840  evaluation reward: 399.5\n",
      "Training network. lr: 0.000117. clip: 0.046861\n",
      "Iteration 17311: Policy loss: 0.016450. Value loss: 0.268074. Entropy: 0.290451.\n",
      "Iteration 17312: Policy loss: 0.007578. Value loss: 0.139698. Entropy: 0.289359.\n",
      "Iteration 17313: Policy loss: -0.008593. Value loss: 0.083491. Entropy: 0.290319.\n",
      "episode: 6075   score: 260.0  epsilon: 1.0    steps: 520  evaluation reward: 395.7\n",
      "episode: 6076   score: 775.0  epsilon: 1.0    steps: 656  evaluation reward: 401.8\n",
      "Training network. lr: 0.000117. clip: 0.046861\n",
      "Iteration 17314: Policy loss: 0.147735. Value loss: 0.145141. Entropy: 0.297142.\n",
      "Iteration 17315: Policy loss: 0.148501. Value loss: 0.067402. Entropy: 0.297782.\n",
      "Iteration 17316: Policy loss: 0.139063. Value loss: 0.047999. Entropy: 0.298126.\n",
      "episode: 6077   score: 305.0  epsilon: 1.0    steps: 288  evaluation reward: 402.75\n",
      "Training network. lr: 0.000117. clip: 0.046861\n",
      "Iteration 17317: Policy loss: 0.097876. Value loss: 0.072863. Entropy: 0.297133.\n",
      "Iteration 17318: Policy loss: 0.087717. Value loss: 0.044731. Entropy: 0.297813.\n",
      "Iteration 17319: Policy loss: 0.086433. Value loss: 0.036723. Entropy: 0.297961.\n",
      "episode: 6078   score: 545.0  epsilon: 1.0    steps: 904  evaluation reward: 404.8\n",
      "Training network. lr: 0.000117. clip: 0.046861\n",
      "Iteration 17320: Policy loss: 0.096061. Value loss: 0.084439. Entropy: 0.306197.\n",
      "Iteration 17321: Policy loss: 0.091486. Value loss: 0.039999. Entropy: 0.302279.\n",
      "Iteration 17322: Policy loss: 0.095211. Value loss: 0.029021. Entropy: 0.303826.\n",
      "Training network. lr: 0.000117. clip: 0.046861\n",
      "Iteration 17323: Policy loss: -0.920976. Value loss: 0.510389. Entropy: 0.297266.\n",
      "Iteration 17324: Policy loss: -0.948080. Value loss: 0.254652. Entropy: 0.298885.\n",
      "Iteration 17325: Policy loss: -0.978272. Value loss: 0.201888. Entropy: 0.299441.\n",
      "episode: 6079   score: 435.0  epsilon: 1.0    steps: 288  evaluation reward: 407.05\n",
      "episode: 6080   score: 385.0  epsilon: 1.0    steps: 896  evaluation reward: 407.45\n",
      "Training network. lr: 0.000117. clip: 0.046861\n",
      "Iteration 17326: Policy loss: 0.201915. Value loss: 0.322473. Entropy: 0.293868.\n",
      "Iteration 17327: Policy loss: 0.196983. Value loss: 0.098809. Entropy: 0.294490.\n",
      "Iteration 17328: Policy loss: 0.171910. Value loss: 0.063249. Entropy: 0.294070.\n",
      "Training network. lr: 0.000117. clip: 0.046861\n",
      "Iteration 17329: Policy loss: -0.069453. Value loss: 0.091451. Entropy: 0.307637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17330: Policy loss: -0.079939. Value loss: 0.042450. Entropy: 0.305596.\n",
      "Iteration 17331: Policy loss: -0.081977. Value loss: 0.029226. Entropy: 0.306909.\n",
      "episode: 6081   score: 555.0  epsilon: 1.0    steps: 976  evaluation reward: 406.65\n",
      "Training network. lr: 0.000117. clip: 0.046861\n",
      "Iteration 17332: Policy loss: 0.125903. Value loss: 0.085601. Entropy: 0.302902.\n",
      "Iteration 17333: Policy loss: 0.123830. Value loss: 0.043715. Entropy: 0.302481.\n",
      "Iteration 17334: Policy loss: 0.119264. Value loss: 0.033395. Entropy: 0.302605.\n",
      "episode: 6082   score: 530.0  epsilon: 1.0    steps: 872  evaluation reward: 410.4\n",
      "Training network. lr: 0.000117. clip: 0.046861\n",
      "Iteration 17335: Policy loss: 0.153997. Value loss: 0.088072. Entropy: 0.296162.\n",
      "Iteration 17336: Policy loss: 0.151241. Value loss: 0.037451. Entropy: 0.295886.\n",
      "Iteration 17337: Policy loss: 0.139815. Value loss: 0.022815. Entropy: 0.296857.\n",
      "episode: 6083   score: 570.0  epsilon: 1.0    steps: 8  evaluation reward: 412.25\n",
      "Training network. lr: 0.000117. clip: 0.046861\n",
      "Iteration 17338: Policy loss: 0.186700. Value loss: 0.151217. Entropy: 0.297801.\n",
      "Iteration 17339: Policy loss: 0.169761. Value loss: 0.053593. Entropy: 0.299720.\n",
      "Iteration 17340: Policy loss: 0.169108. Value loss: 0.033820. Entropy: 0.299198.\n",
      "episode: 6084   score: 420.0  epsilon: 1.0    steps: 280  evaluation reward: 410.55\n",
      "Training network. lr: 0.000117. clip: 0.046861\n",
      "Iteration 17341: Policy loss: 0.143942. Value loss: 0.159744. Entropy: 0.298846.\n",
      "Iteration 17342: Policy loss: 0.131189. Value loss: 0.058904. Entropy: 0.298874.\n",
      "Iteration 17343: Policy loss: 0.130259. Value loss: 0.038005. Entropy: 0.297938.\n",
      "Training network. lr: 0.000117. clip: 0.046861\n",
      "Iteration 17344: Policy loss: -0.297122. Value loss: 0.370314. Entropy: 0.294555.\n",
      "Iteration 17345: Policy loss: -0.300071. Value loss: 0.209325. Entropy: 0.291414.\n",
      "Iteration 17346: Policy loss: -0.278046. Value loss: 0.117770. Entropy: 0.292061.\n",
      "episode: 6085   score: 435.0  epsilon: 1.0    steps: 112  evaluation reward: 411.0\n",
      "episode: 6086   score: 315.0  epsilon: 1.0    steps: 792  evaluation reward: 411.3\n",
      "Training network. lr: 0.000117. clip: 0.046861\n",
      "Iteration 17347: Policy loss: 0.113777. Value loss: 0.391815. Entropy: 0.269448.\n",
      "Iteration 17348: Policy loss: 0.100433. Value loss: 0.146397. Entropy: 0.271126.\n",
      "Iteration 17349: Policy loss: 0.074007. Value loss: 0.091202. Entropy: 0.270475.\n",
      "episode: 6087   score: 580.0  epsilon: 1.0    steps: 104  evaluation reward: 410.9\n",
      "episode: 6088   score: 675.0  epsilon: 1.0    steps: 520  evaluation reward: 414.3\n",
      "Training network. lr: 0.000117. clip: 0.046861\n",
      "Iteration 17350: Policy loss: -0.152719. Value loss: 0.289703. Entropy: 0.278982.\n",
      "Iteration 17351: Policy loss: -0.161774. Value loss: 0.118076. Entropy: 0.277435.\n",
      "Iteration 17352: Policy loss: -0.157548. Value loss: 0.072459. Entropy: 0.276653.\n",
      "episode: 6089   score: 315.0  epsilon: 1.0    steps: 936  evaluation reward: 414.3\n",
      "Training network. lr: 0.000117. clip: 0.046704\n",
      "Iteration 17353: Policy loss: -0.041560. Value loss: 0.225655. Entropy: 0.298341.\n",
      "Iteration 17354: Policy loss: -0.053381. Value loss: 0.092793. Entropy: 0.297791.\n",
      "Iteration 17355: Policy loss: -0.054019. Value loss: 0.063152. Entropy: 0.298884.\n",
      "Training network. lr: 0.000117. clip: 0.046704\n",
      "Iteration 17356: Policy loss: 0.226100. Value loss: 0.206417. Entropy: 0.276685.\n",
      "Iteration 17357: Policy loss: 0.217202. Value loss: 0.074034. Entropy: 0.275719.\n",
      "Iteration 17358: Policy loss: 0.219784. Value loss: 0.049579. Entropy: 0.274152.\n",
      "Training network. lr: 0.000117. clip: 0.046704\n",
      "Iteration 17359: Policy loss: 0.228650. Value loss: 0.254533. Entropy: 0.308536.\n",
      "Iteration 17360: Policy loss: 0.218207. Value loss: 0.083297. Entropy: 0.305781.\n",
      "Iteration 17361: Policy loss: 0.219582. Value loss: 0.054432. Entropy: 0.305269.\n",
      "episode: 6090   score: 155.0  epsilon: 1.0    steps: 808  evaluation reward: 413.45\n",
      "episode: 6091   score: 315.0  epsilon: 1.0    steps: 960  evaluation reward: 412.7\n",
      "Training network. lr: 0.000117. clip: 0.046704\n",
      "Iteration 17362: Policy loss: 0.110233. Value loss: 0.132940. Entropy: 0.269552.\n",
      "Iteration 17363: Policy loss: 0.113106. Value loss: 0.069597. Entropy: 0.271250.\n",
      "Iteration 17364: Policy loss: 0.097509. Value loss: 0.049291. Entropy: 0.271746.\n",
      "Training network. lr: 0.000117. clip: 0.046704\n",
      "Iteration 17365: Policy loss: 0.117302. Value loss: 0.141888. Entropy: 0.296734.\n",
      "Iteration 17366: Policy loss: 0.117540. Value loss: 0.057544. Entropy: 0.297785.\n",
      "Iteration 17367: Policy loss: 0.108901. Value loss: 0.038683. Entropy: 0.297073.\n",
      "episode: 6092   score: 450.0  epsilon: 1.0    steps: 632  evaluation reward: 413.1\n",
      "Training network. lr: 0.000117. clip: 0.046704\n",
      "Iteration 17368: Policy loss: 0.410508. Value loss: 0.143388. Entropy: 0.294141.\n",
      "Iteration 17369: Policy loss: 0.405268. Value loss: 0.049718. Entropy: 0.294458.\n",
      "Iteration 17370: Policy loss: 0.381590. Value loss: 0.030999. Entropy: 0.294434.\n",
      "episode: 6093   score: 335.0  epsilon: 1.0    steps: 32  evaluation reward: 414.35\n",
      "episode: 6094   score: 410.0  epsilon: 1.0    steps: 88  evaluation reward: 414.25\n",
      "Training network. lr: 0.000117. clip: 0.046704\n",
      "Iteration 17371: Policy loss: 0.253821. Value loss: 0.135326. Entropy: 0.297702.\n",
      "Iteration 17372: Policy loss: 0.258804. Value loss: 0.048460. Entropy: 0.299397.\n",
      "Iteration 17373: Policy loss: 0.246367. Value loss: 0.038935. Entropy: 0.297240.\n",
      "episode: 6095   score: 360.0  epsilon: 1.0    steps: 1024  evaluation reward: 413.65\n",
      "Training network. lr: 0.000117. clip: 0.046704\n",
      "Iteration 17374: Policy loss: 0.030476. Value loss: 0.106137. Entropy: 0.303786.\n",
      "Iteration 17375: Policy loss: 0.022690. Value loss: 0.047450. Entropy: 0.303856.\n",
      "Iteration 17376: Policy loss: 0.015071. Value loss: 0.030915. Entropy: 0.303614.\n",
      "Training network. lr: 0.000117. clip: 0.046704\n",
      "Iteration 17377: Policy loss: -0.258587. Value loss: 0.340920. Entropy: 0.282222.\n",
      "Iteration 17378: Policy loss: -0.255992. Value loss: 0.214896. Entropy: 0.282512.\n",
      "Iteration 17379: Policy loss: -0.272138. Value loss: 0.145496. Entropy: 0.283464.\n",
      "Training network. lr: 0.000117. clip: 0.046704\n",
      "Iteration 17380: Policy loss: -0.538968. Value loss: 0.274267. Entropy: 0.309311.\n",
      "Iteration 17381: Policy loss: -0.540821. Value loss: 0.098283. Entropy: 0.307551.\n",
      "Iteration 17382: Policy loss: -0.535238. Value loss: 0.064955. Entropy: 0.307633.\n",
      "episode: 6096   score: 775.0  epsilon: 1.0    steps: 352  evaluation reward: 417.9\n",
      "episode: 6097   score: 695.0  epsilon: 1.0    steps: 384  evaluation reward: 421.55\n",
      "Training network. lr: 0.000117. clip: 0.046704\n",
      "Iteration 17383: Policy loss: -0.036912. Value loss: 0.166420. Entropy: 0.293472.\n",
      "Iteration 17384: Policy loss: -0.047332. Value loss: 0.062919. Entropy: 0.293860.\n",
      "Iteration 17385: Policy loss: -0.062706. Value loss: 0.034806. Entropy: 0.292215.\n",
      "Training network. lr: 0.000117. clip: 0.046704\n",
      "Iteration 17386: Policy loss: 0.419436. Value loss: 0.235171. Entropy: 0.308162.\n",
      "Iteration 17387: Policy loss: 0.404053. Value loss: 0.073758. Entropy: 0.307190.\n",
      "Iteration 17388: Policy loss: 0.387887. Value loss: 0.050234. Entropy: 0.308243.\n",
      "episode: 6098   score: 445.0  epsilon: 1.0    steps: 8  evaluation reward: 421.2\n",
      "episode: 6099   score: 400.0  epsilon: 1.0    steps: 296  evaluation reward: 421.15\n",
      "episode: 6100   score: 515.0  epsilon: 1.0    steps: 768  evaluation reward: 422.45\n",
      "Training network. lr: 0.000117. clip: 0.046704\n",
      "Iteration 17389: Policy loss: 0.306118. Value loss: 0.209123. Entropy: 0.277718.\n",
      "Iteration 17390: Policy loss: 0.304502. Value loss: 0.066884. Entropy: 0.277436.\n",
      "Iteration 17391: Policy loss: 0.303166. Value loss: 0.047855. Entropy: 0.275773.\n",
      "Training network. lr: 0.000117. clip: 0.046704\n",
      "Iteration 17392: Policy loss: -0.095462. Value loss: 0.275318. Entropy: 0.304543.\n",
      "Iteration 17393: Policy loss: -0.104709. Value loss: 0.108385. Entropy: 0.303948.\n",
      "Iteration 17394: Policy loss: -0.101078. Value loss: 0.088887. Entropy: 0.303813.\n",
      "now time :  2019-09-06 08:14:28.717023\n",
      "episode: 6101   score: 335.0  epsilon: 1.0    steps: 176  evaluation reward: 420.3\n",
      "Training network. lr: 0.000117. clip: 0.046704\n",
      "Iteration 17395: Policy loss: -0.155400. Value loss: 0.171338. Entropy: 0.293591.\n",
      "Iteration 17396: Policy loss: -0.161945. Value loss: 0.068579. Entropy: 0.289880.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17397: Policy loss: -0.165217. Value loss: 0.043831. Entropy: 0.288363.\n",
      "episode: 6102   score: 565.0  epsilon: 1.0    steps: 568  evaluation reward: 422.35\n",
      "Training network. lr: 0.000117. clip: 0.046704\n",
      "Iteration 17398: Policy loss: 0.156025. Value loss: 0.325453. Entropy: 0.293410.\n",
      "Iteration 17399: Policy loss: 0.130224. Value loss: 0.181497. Entropy: 0.291356.\n",
      "Iteration 17400: Policy loss: 0.148509. Value loss: 0.137772. Entropy: 0.292143.\n",
      "episode: 6103   score: 855.0  epsilon: 1.0    steps: 952  evaluation reward: 427.25\n",
      "episode: 6104   score: 260.0  epsilon: 1.0    steps: 1008  evaluation reward: 427.0\n",
      "Training network. lr: 0.000116. clip: 0.046556\n",
      "Iteration 17401: Policy loss: 0.355501. Value loss: 0.212218. Entropy: 0.305622.\n",
      "Iteration 17402: Policy loss: 0.356947. Value loss: 0.061702. Entropy: 0.303072.\n",
      "Iteration 17403: Policy loss: 0.349207. Value loss: 0.036040. Entropy: 0.304221.\n",
      "Training network. lr: 0.000116. clip: 0.046556\n",
      "Iteration 17404: Policy loss: 0.074509. Value loss: 0.142697. Entropy: 0.280192.\n",
      "Iteration 17405: Policy loss: 0.067786. Value loss: 0.054883. Entropy: 0.280908.\n",
      "Iteration 17406: Policy loss: 0.063351. Value loss: 0.035760. Entropy: 0.280921.\n",
      "episode: 6105   score: 305.0  epsilon: 1.0    steps: 480  evaluation reward: 427.95\n",
      "Training network. lr: 0.000116. clip: 0.046556\n",
      "Iteration 17407: Policy loss: -0.058779. Value loss: 0.120957. Entropy: 0.302264.\n",
      "Iteration 17408: Policy loss: -0.060997. Value loss: 0.061029. Entropy: 0.303878.\n",
      "Iteration 17409: Policy loss: -0.066154. Value loss: 0.046974. Entropy: 0.303196.\n",
      "Training network. lr: 0.000116. clip: 0.046556\n",
      "Iteration 17410: Policy loss: -0.281795. Value loss: 0.396091. Entropy: 0.308905.\n",
      "Iteration 17411: Policy loss: -0.283027. Value loss: 0.249954. Entropy: 0.309644.\n",
      "Iteration 17412: Policy loss: -0.295993. Value loss: 0.197820. Entropy: 0.310567.\n",
      "episode: 6106   score: 680.0  epsilon: 1.0    steps: 640  evaluation reward: 431.6\n",
      "Training network. lr: 0.000116. clip: 0.046556\n",
      "Iteration 17413: Policy loss: -0.082531. Value loss: 0.313433. Entropy: 0.299029.\n",
      "Iteration 17414: Policy loss: -0.082883. Value loss: 0.100910. Entropy: 0.300448.\n",
      "Iteration 17415: Policy loss: -0.109721. Value loss: 0.048666. Entropy: 0.302221.\n",
      "episode: 6107   score: 480.0  epsilon: 1.0    steps: 104  evaluation reward: 434.85\n",
      "episode: 6108   score: 240.0  epsilon: 1.0    steps: 952  evaluation reward: 432.7\n",
      "Training network. lr: 0.000116. clip: 0.046556\n",
      "Iteration 17416: Policy loss: 0.413864. Value loss: 0.176714. Entropy: 0.298278.\n",
      "Iteration 17417: Policy loss: 0.406278. Value loss: 0.086372. Entropy: 0.298193.\n",
      "Iteration 17418: Policy loss: 0.407635. Value loss: 0.065178. Entropy: 0.298462.\n",
      "episode: 6109   score: 595.0  epsilon: 1.0    steps: 200  evaluation reward: 434.45\n",
      "Training network. lr: 0.000116. clip: 0.046556\n",
      "Iteration 17419: Policy loss: -0.123187. Value loss: 0.109017. Entropy: 0.298894.\n",
      "Iteration 17420: Policy loss: -0.130249. Value loss: 0.062273. Entropy: 0.297568.\n",
      "Iteration 17421: Policy loss: -0.133434. Value loss: 0.051592. Entropy: 0.299236.\n",
      "Training network. lr: 0.000116. clip: 0.046556\n",
      "Iteration 17422: Policy loss: 0.167346. Value loss: 0.224497. Entropy: 0.295469.\n",
      "Iteration 17423: Policy loss: 0.171678. Value loss: 0.102080. Entropy: 0.292919.\n",
      "Iteration 17424: Policy loss: 0.160404. Value loss: 0.061359. Entropy: 0.292612.\n",
      "episode: 6110   score: 590.0  epsilon: 1.0    steps: 72  evaluation reward: 436.1\n",
      "Training network. lr: 0.000116. clip: 0.046556\n",
      "Iteration 17425: Policy loss: 0.001516. Value loss: 0.238901. Entropy: 0.299005.\n",
      "Iteration 17426: Policy loss: 0.000572. Value loss: 0.063214. Entropy: 0.298179.\n",
      "Iteration 17427: Policy loss: -0.001537. Value loss: 0.043663. Entropy: 0.299345.\n",
      "episode: 6111   score: 570.0  epsilon: 1.0    steps: 152  evaluation reward: 437.6\n",
      "episode: 6112   score: 450.0  epsilon: 1.0    steps: 496  evaluation reward: 439.2\n",
      "Training network. lr: 0.000116. clip: 0.046556\n",
      "Iteration 17428: Policy loss: -0.183975. Value loss: 0.343027. Entropy: 0.288182.\n",
      "Iteration 17429: Policy loss: -0.184893. Value loss: 0.231320. Entropy: 0.286117.\n",
      "Iteration 17430: Policy loss: -0.198041. Value loss: 0.193543. Entropy: 0.286727.\n",
      "episode: 6113   score: 380.0  epsilon: 1.0    steps: 424  evaluation reward: 436.0\n",
      "Training network. lr: 0.000116. clip: 0.046556\n",
      "Iteration 17431: Policy loss: -0.155525. Value loss: 0.168130. Entropy: 0.298821.\n",
      "Iteration 17432: Policy loss: -0.171079. Value loss: 0.073754. Entropy: 0.299338.\n",
      "Iteration 17433: Policy loss: -0.172573. Value loss: 0.050688. Entropy: 0.298082.\n",
      "episode: 6114   score: 470.0  epsilon: 1.0    steps: 616  evaluation reward: 436.8\n",
      "Training network. lr: 0.000116. clip: 0.046556\n",
      "Iteration 17434: Policy loss: 0.052694. Value loss: 0.259128. Entropy: 0.294913.\n",
      "Iteration 17435: Policy loss: 0.040561. Value loss: 0.085351. Entropy: 0.295608.\n",
      "Iteration 17436: Policy loss: 0.039861. Value loss: 0.051028. Entropy: 0.295081.\n",
      "Training network. lr: 0.000116. clip: 0.046556\n",
      "Iteration 17437: Policy loss: -0.137357. Value loss: 0.240104. Entropy: 0.312472.\n",
      "Iteration 17438: Policy loss: -0.143936. Value loss: 0.092132. Entropy: 0.311572.\n",
      "Iteration 17439: Policy loss: -0.145373. Value loss: 0.056875. Entropy: 0.311785.\n",
      "episode: 6115   score: 300.0  epsilon: 1.0    steps: 136  evaluation reward: 438.0\n",
      "episode: 6116   score: 695.0  epsilon: 1.0    steps: 584  evaluation reward: 441.4\n",
      "Training network. lr: 0.000116. clip: 0.046556\n",
      "Iteration 17440: Policy loss: 0.026594. Value loss: 0.113006. Entropy: 0.299198.\n",
      "Iteration 17441: Policy loss: 0.024244. Value loss: 0.056778. Entropy: 0.298109.\n",
      "Iteration 17442: Policy loss: 0.018642. Value loss: 0.045117. Entropy: 0.298801.\n",
      "Training network. lr: 0.000116. clip: 0.046556\n",
      "Iteration 17443: Policy loss: -0.337786. Value loss: 0.491508. Entropy: 0.306451.\n",
      "Iteration 17444: Policy loss: -0.359566. Value loss: 0.325568. Entropy: 0.306988.\n",
      "Iteration 17445: Policy loss: -0.358727. Value loss: 0.253814. Entropy: 0.306482.\n",
      "episode: 6117   score: 285.0  epsilon: 1.0    steps: 736  evaluation reward: 440.7\n",
      "Training network. lr: 0.000116. clip: 0.046556\n",
      "Iteration 17446: Policy loss: 0.160484. Value loss: 0.142775. Entropy: 0.300664.\n",
      "Iteration 17447: Policy loss: 0.153432. Value loss: 0.067986. Entropy: 0.301956.\n",
      "Iteration 17448: Policy loss: 0.146393. Value loss: 0.047919. Entropy: 0.301580.\n",
      "episode: 6118   score: 225.0  epsilon: 1.0    steps: 104  evaluation reward: 440.55\n",
      "episode: 6119   score: 405.0  epsilon: 1.0    steps: 112  evaluation reward: 442.5\n",
      "Training network. lr: 0.000116. clip: 0.046556\n",
      "Iteration 17449: Policy loss: -0.232083. Value loss: 0.239502. Entropy: 0.278065.\n",
      "Iteration 17450: Policy loss: -0.251234. Value loss: 0.147717. Entropy: 0.275914.\n",
      "Iteration 17451: Policy loss: -0.237452. Value loss: 0.093717. Entropy: 0.273582.\n",
      "episode: 6120   score: 615.0  epsilon: 1.0    steps: 104  evaluation reward: 445.05\n",
      "episode: 6121   score: 685.0  epsilon: 1.0    steps: 392  evaluation reward: 446.1\n",
      "Training network. lr: 0.000116. clip: 0.046400\n",
      "Iteration 17452: Policy loss: 0.243206. Value loss: 0.139002. Entropy: 0.294430.\n",
      "Iteration 17453: Policy loss: 0.233578. Value loss: 0.063716. Entropy: 0.291767.\n",
      "Iteration 17454: Policy loss: 0.231554. Value loss: 0.048206. Entropy: 0.291805.\n",
      "episode: 6122   score: 215.0  epsilon: 1.0    steps: 776  evaluation reward: 446.75\n",
      "Training network. lr: 0.000116. clip: 0.046400\n",
      "Iteration 17455: Policy loss: 0.207360. Value loss: 0.214083. Entropy: 0.298384.\n",
      "Iteration 17456: Policy loss: 0.199572. Value loss: 0.101023. Entropy: 0.298112.\n",
      "Iteration 17457: Policy loss: 0.185701. Value loss: 0.065738. Entropy: 0.299743.\n",
      "episode: 6123   score: 535.0  epsilon: 1.0    steps: 136  evaluation reward: 448.45\n",
      "Training network. lr: 0.000116. clip: 0.046400\n",
      "Iteration 17458: Policy loss: 0.016873. Value loss: 0.079060. Entropy: 0.296173.\n",
      "Iteration 17459: Policy loss: 0.008952. Value loss: 0.044177. Entropy: 0.294734.\n",
      "Iteration 17460: Policy loss: 0.005036. Value loss: 0.032982. Entropy: 0.294104.\n",
      "Training network. lr: 0.000116. clip: 0.046400\n",
      "Iteration 17461: Policy loss: -0.115826. Value loss: 0.088176. Entropy: 0.303474.\n",
      "Iteration 17462: Policy loss: -0.122277. Value loss: 0.034059. Entropy: 0.304800.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17463: Policy loss: -0.127255. Value loss: 0.025339. Entropy: 0.304360.\n",
      "Training network. lr: 0.000116. clip: 0.046400\n",
      "Iteration 17464: Policy loss: 0.209736. Value loss: 0.171508. Entropy: 0.309493.\n",
      "Iteration 17465: Policy loss: 0.190955. Value loss: 0.059394. Entropy: 0.307554.\n",
      "Iteration 17466: Policy loss: 0.199543. Value loss: 0.041142. Entropy: 0.308796.\n",
      "episode: 6124   score: 350.0  epsilon: 1.0    steps: 1024  evaluation reward: 449.55\n",
      "Training network. lr: 0.000116. clip: 0.046400\n",
      "Iteration 17467: Policy loss: 0.313630. Value loss: 0.179803. Entropy: 0.308460.\n",
      "Iteration 17468: Policy loss: 0.302237. Value loss: 0.068948. Entropy: 0.307528.\n",
      "Iteration 17469: Policy loss: 0.301773. Value loss: 0.046817. Entropy: 0.307657.\n",
      "episode: 6125   score: 350.0  epsilon: 1.0    steps: 424  evaluation reward: 450.7\n",
      "episode: 6126   score: 420.0  epsilon: 1.0    steps: 856  evaluation reward: 453.05\n",
      "Training network. lr: 0.000116. clip: 0.046400\n",
      "Iteration 17470: Policy loss: -0.007790. Value loss: 0.125217. Entropy: 0.284183.\n",
      "Iteration 17471: Policy loss: -0.011464. Value loss: 0.053536. Entropy: 0.283515.\n",
      "Iteration 17472: Policy loss: -0.014734. Value loss: 0.033317. Entropy: 0.282902.\n",
      "episode: 6127   score: 395.0  epsilon: 1.0    steps: 680  evaluation reward: 454.15\n",
      "episode: 6128   score: 350.0  epsilon: 1.0    steps: 952  evaluation reward: 455.8\n",
      "Training network. lr: 0.000116. clip: 0.046400\n",
      "Iteration 17473: Policy loss: 0.047046. Value loss: 0.377725. Entropy: 0.291774.\n",
      "Iteration 17474: Policy loss: 0.022479. Value loss: 0.165356. Entropy: 0.290752.\n",
      "Iteration 17475: Policy loss: 0.030192. Value loss: 0.065039. Entropy: 0.291904.\n",
      "episode: 6129   score: 350.0  epsilon: 1.0    steps: 1016  evaluation reward: 450.6\n",
      "episode: 6130   score: 305.0  epsilon: 1.0    steps: 1016  evaluation reward: 448.15\n",
      "Training network. lr: 0.000116. clip: 0.046400\n",
      "Iteration 17476: Policy loss: 0.126758. Value loss: 0.115210. Entropy: 0.299472.\n",
      "Iteration 17477: Policy loss: 0.123384. Value loss: 0.059073. Entropy: 0.300804.\n",
      "Iteration 17478: Policy loss: 0.126360. Value loss: 0.045652. Entropy: 0.299505.\n",
      "Training network. lr: 0.000116. clip: 0.046400\n",
      "Iteration 17479: Policy loss: -0.011127. Value loss: 0.099018. Entropy: 0.293057.\n",
      "Iteration 17480: Policy loss: -0.010426. Value loss: 0.038291. Entropy: 0.292214.\n",
      "Iteration 17481: Policy loss: -0.015711. Value loss: 0.027324. Entropy: 0.294552.\n",
      "episode: 6131   score: 725.0  epsilon: 1.0    steps: 840  evaluation reward: 448.95\n",
      "Training network. lr: 0.000116. clip: 0.046400\n",
      "Iteration 17482: Policy loss: 0.163694. Value loss: 0.151713. Entropy: 0.302088.\n",
      "Iteration 17483: Policy loss: 0.159960. Value loss: 0.082064. Entropy: 0.301297.\n",
      "Iteration 17484: Policy loss: 0.158245. Value loss: 0.062049. Entropy: 0.301547.\n",
      "Training network. lr: 0.000116. clip: 0.046400\n",
      "Iteration 17485: Policy loss: 0.044489. Value loss: 0.079557. Entropy: 0.300449.\n",
      "Iteration 17486: Policy loss: 0.040389. Value loss: 0.029586. Entropy: 0.300394.\n",
      "Iteration 17487: Policy loss: 0.041512. Value loss: 0.022906. Entropy: 0.300139.\n",
      "Training network. lr: 0.000116. clip: 0.046400\n",
      "Iteration 17488: Policy loss: -0.125935. Value loss: 0.144628. Entropy: 0.302122.\n",
      "Iteration 17489: Policy loss: -0.137685. Value loss: 0.053025. Entropy: 0.302313.\n",
      "Iteration 17490: Policy loss: -0.133542. Value loss: 0.032360. Entropy: 0.301797.\n",
      "episode: 6132   score: 345.0  epsilon: 1.0    steps: 880  evaluation reward: 449.55\n",
      "Training network. lr: 0.000116. clip: 0.046400\n",
      "Iteration 17491: Policy loss: -0.970217. Value loss: 0.707735. Entropy: 0.293973.\n",
      "Iteration 17492: Policy loss: -0.987870. Value loss: 0.314564. Entropy: 0.295220.\n",
      "Iteration 17493: Policy loss: -1.030171. Value loss: 0.173730. Entropy: 0.295761.\n",
      "episode: 6133   score: 360.0  epsilon: 1.0    steps: 656  evaluation reward: 451.9\n",
      "Training network. lr: 0.000116. clip: 0.046400\n",
      "Iteration 17494: Policy loss: 0.201796. Value loss: 0.120506. Entropy: 0.292525.\n",
      "Iteration 17495: Policy loss: 0.192039. Value loss: 0.049714. Entropy: 0.291865.\n",
      "Iteration 17496: Policy loss: 0.190167. Value loss: 0.036071. Entropy: 0.293217.\n",
      "Training network. lr: 0.000116. clip: 0.046400\n",
      "Iteration 17497: Policy loss: -0.042656. Value loss: 0.149546. Entropy: 0.312008.\n",
      "Iteration 17498: Policy loss: -0.052658. Value loss: 0.066314. Entropy: 0.311074.\n",
      "Iteration 17499: Policy loss: -0.052278. Value loss: 0.043621. Entropy: 0.311936.\n",
      "episode: 6134   score: 535.0  epsilon: 1.0    steps: 144  evaluation reward: 455.15\n",
      "episode: 6135   score: 825.0  epsilon: 1.0    steps: 616  evaluation reward: 459.2\n",
      "episode: 6136   score: 395.0  epsilon: 1.0    steps: 944  evaluation reward: 460.0\n",
      "episode: 6137   score: 425.0  epsilon: 1.0    steps: 992  evaluation reward: 461.05\n",
      "Training network. lr: 0.000116. clip: 0.046400\n",
      "Iteration 17500: Policy loss: 0.105563. Value loss: 0.182788. Entropy: 0.269299.\n",
      "Iteration 17501: Policy loss: 0.109250. Value loss: 0.081969. Entropy: 0.269146.\n",
      "Iteration 17502: Policy loss: 0.103414. Value loss: 0.057699. Entropy: 0.269415.\n",
      "episode: 6138   score: 335.0  epsilon: 1.0    steps: 920  evaluation reward: 462.1\n",
      "Training network. lr: 0.000116. clip: 0.046243\n",
      "Iteration 17503: Policy loss: 0.392214. Value loss: 0.221595. Entropy: 0.289553.\n",
      "Iteration 17504: Policy loss: 0.363357. Value loss: 0.065451. Entropy: 0.290560.\n",
      "Iteration 17505: Policy loss: 0.368680. Value loss: 0.046560. Entropy: 0.290911.\n",
      "episode: 6139   score: 615.0  epsilon: 1.0    steps: 528  evaluation reward: 462.1\n",
      "Training network. lr: 0.000116. clip: 0.046243\n",
      "Iteration 17506: Policy loss: -0.079019. Value loss: 0.063660. Entropy: 0.290517.\n",
      "Iteration 17507: Policy loss: -0.084625. Value loss: 0.031624. Entropy: 0.292207.\n",
      "Iteration 17508: Policy loss: -0.084626. Value loss: 0.023283. Entropy: 0.292324.\n",
      "episode: 6140   score: 210.0  epsilon: 1.0    steps: 784  evaluation reward: 462.55\n",
      "Training network. lr: 0.000116. clip: 0.046243\n",
      "Iteration 17509: Policy loss: -0.175604. Value loss: 0.273842. Entropy: 0.301682.\n",
      "Iteration 17510: Policy loss: -0.192721. Value loss: 0.101759. Entropy: 0.301267.\n",
      "Iteration 17511: Policy loss: -0.198413. Value loss: 0.059627. Entropy: 0.302368.\n",
      "episode: 6141   score: 215.0  epsilon: 1.0    steps: 528  evaluation reward: 459.7\n",
      "Training network. lr: 0.000116. clip: 0.046243\n",
      "Iteration 17512: Policy loss: 0.368751. Value loss: 0.158063. Entropy: 0.302869.\n",
      "Iteration 17513: Policy loss: 0.363004. Value loss: 0.074707. Entropy: 0.302150.\n",
      "Iteration 17514: Policy loss: 0.349699. Value loss: 0.054241. Entropy: 0.301667.\n",
      "episode: 6142   score: 165.0  epsilon: 1.0    steps: 408  evaluation reward: 458.2\n",
      "Training network. lr: 0.000116. clip: 0.046243\n",
      "Iteration 17515: Policy loss: 0.024939. Value loss: 0.104769. Entropy: 0.296723.\n",
      "Iteration 17516: Policy loss: 0.021004. Value loss: 0.042711. Entropy: 0.296479.\n",
      "Iteration 17517: Policy loss: 0.025351. Value loss: 0.032705. Entropy: 0.296698.\n",
      "Training network. lr: 0.000116. clip: 0.046243\n",
      "Iteration 17518: Policy loss: 0.057647. Value loss: 0.098318. Entropy: 0.310802.\n",
      "Iteration 17519: Policy loss: 0.048117. Value loss: 0.044128. Entropy: 0.310911.\n",
      "Iteration 17520: Policy loss: 0.049848. Value loss: 0.032196. Entropy: 0.310079.\n",
      "episode: 6143   score: 330.0  epsilon: 1.0    steps: 200  evaluation reward: 453.9\n",
      "Training network. lr: 0.000116. clip: 0.046243\n",
      "Iteration 17521: Policy loss: -0.192317. Value loss: 0.113684. Entropy: 0.288374.\n",
      "Iteration 17522: Policy loss: -0.193385. Value loss: 0.056504. Entropy: 0.287498.\n",
      "Iteration 17523: Policy loss: -0.202456. Value loss: 0.044493. Entropy: 0.287255.\n",
      "episode: 6144   score: 355.0  epsilon: 1.0    steps: 552  evaluation reward: 451.25\n",
      "Training network. lr: 0.000116. clip: 0.046243\n",
      "Iteration 17524: Policy loss: -0.285125. Value loss: 0.233840. Entropy: 0.290972.\n",
      "Iteration 17525: Policy loss: -0.286240. Value loss: 0.079991. Entropy: 0.291990.\n",
      "Iteration 17526: Policy loss: -0.286302. Value loss: 0.051061. Entropy: 0.290415.\n",
      "Training network. lr: 0.000116. clip: 0.046243\n",
      "Iteration 17527: Policy loss: 0.163097. Value loss: 0.395297. Entropy: 0.306288.\n",
      "Iteration 17528: Policy loss: 0.140213. Value loss: 0.095459. Entropy: 0.306755.\n",
      "Iteration 17529: Policy loss: 0.112167. Value loss: 0.059098. Entropy: 0.307431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6145   score: 480.0  epsilon: 1.0    steps: 200  evaluation reward: 451.2\n",
      "episode: 6146   score: 535.0  epsilon: 1.0    steps: 880  evaluation reward: 453.55\n",
      "Training network. lr: 0.000116. clip: 0.046243\n",
      "Iteration 17530: Policy loss: 0.045751. Value loss: 0.160343. Entropy: 0.284722.\n",
      "Iteration 17531: Policy loss: 0.040813. Value loss: 0.077905. Entropy: 0.288236.\n",
      "Iteration 17532: Policy loss: 0.036595. Value loss: 0.053927. Entropy: 0.286780.\n",
      "episode: 6147   score: 310.0  epsilon: 1.0    steps: 784  evaluation reward: 449.7\n",
      "Training network. lr: 0.000116. clip: 0.046243\n",
      "Iteration 17533: Policy loss: 0.122014. Value loss: 0.172600. Entropy: 0.306164.\n",
      "Iteration 17534: Policy loss: 0.119514. Value loss: 0.060622. Entropy: 0.306905.\n",
      "Iteration 17535: Policy loss: 0.115497. Value loss: 0.044740. Entropy: 0.305721.\n",
      "episode: 6148   score: 290.0  epsilon: 1.0    steps: 424  evaluation reward: 442.6\n",
      "Training network. lr: 0.000116. clip: 0.046243\n",
      "Iteration 17536: Policy loss: 0.039964. Value loss: 0.079030. Entropy: 0.304417.\n",
      "Iteration 17537: Policy loss: 0.035772. Value loss: 0.049893. Entropy: 0.304901.\n",
      "Iteration 17538: Policy loss: 0.032201. Value loss: 0.038640. Entropy: 0.304197.\n",
      "Training network. lr: 0.000116. clip: 0.046243\n",
      "Iteration 17539: Policy loss: -0.143809. Value loss: 0.166439. Entropy: 0.301251.\n",
      "Iteration 17540: Policy loss: -0.155204. Value loss: 0.060955. Entropy: 0.299656.\n",
      "Iteration 17541: Policy loss: -0.155167. Value loss: 0.041360. Entropy: 0.298497.\n",
      "episode: 6149   score: 935.0  epsilon: 1.0    steps: 832  evaluation reward: 448.0\n",
      "Training network. lr: 0.000116. clip: 0.046243\n",
      "Iteration 17542: Policy loss: 0.009605. Value loss: 0.153670. Entropy: 0.286295.\n",
      "Iteration 17543: Policy loss: 0.010987. Value loss: 0.081290. Entropy: 0.286436.\n",
      "Iteration 17544: Policy loss: 0.007663. Value loss: 0.055449. Entropy: 0.286170.\n",
      "Training network. lr: 0.000116. clip: 0.046243\n",
      "Iteration 17545: Policy loss: -0.095058. Value loss: 0.291843. Entropy: 0.308019.\n",
      "Iteration 17546: Policy loss: -0.101180. Value loss: 0.103046. Entropy: 0.307610.\n",
      "Iteration 17547: Policy loss: -0.124555. Value loss: 0.064442. Entropy: 0.306909.\n",
      "episode: 6150   score: 300.0  epsilon: 1.0    steps: 96  evaluation reward: 447.85\n",
      "Training network. lr: 0.000116. clip: 0.046243\n",
      "Iteration 17548: Policy loss: -0.099217. Value loss: 0.070465. Entropy: 0.299764.\n",
      "Iteration 17549: Policy loss: -0.096686. Value loss: 0.034153. Entropy: 0.299025.\n",
      "Iteration 17550: Policy loss: -0.103950. Value loss: 0.027155. Entropy: 0.297733.\n",
      "now time :  2019-09-06 08:24:08.843014\n",
      "episode: 6151   score: 780.0  epsilon: 1.0    steps: 448  evaluation reward: 452.6\n",
      "episode: 6152   score: 445.0  epsilon: 1.0    steps: 840  evaluation reward: 453.75\n",
      "Training network. lr: 0.000115. clip: 0.046096\n",
      "Iteration 17551: Policy loss: 0.010269. Value loss: 0.312317. Entropy: 0.290788.\n",
      "Iteration 17552: Policy loss: -0.009870. Value loss: 0.121818. Entropy: 0.290802.\n",
      "Iteration 17553: Policy loss: -0.019992. Value loss: 0.073010. Entropy: 0.289770.\n",
      "Training network. lr: 0.000115. clip: 0.046096\n",
      "Iteration 17554: Policy loss: 0.285745. Value loss: 0.131300. Entropy: 0.308583.\n",
      "Iteration 17555: Policy loss: 0.282440. Value loss: 0.060911. Entropy: 0.310682.\n",
      "Iteration 17556: Policy loss: 0.280687. Value loss: 0.044035. Entropy: 0.309855.\n",
      "episode: 6153   score: 400.0  epsilon: 1.0    steps: 192  evaluation reward: 453.75\n",
      "Training network. lr: 0.000115. clip: 0.046096\n",
      "Iteration 17557: Policy loss: -0.040417. Value loss: 0.103270. Entropy: 0.299544.\n",
      "Iteration 17558: Policy loss: -0.044240. Value loss: 0.056414. Entropy: 0.300702.\n",
      "Iteration 17559: Policy loss: -0.046331. Value loss: 0.041764. Entropy: 0.299534.\n",
      "Training network. lr: 0.000115. clip: 0.046096\n",
      "Iteration 17560: Policy loss: -0.117759. Value loss: 0.123265. Entropy: 0.305298.\n",
      "Iteration 17561: Policy loss: -0.114419. Value loss: 0.045960. Entropy: 0.304001.\n",
      "Iteration 17562: Policy loss: -0.128190. Value loss: 0.029783. Entropy: 0.303950.\n",
      "episode: 6154   score: 540.0  epsilon: 1.0    steps: 416  evaluation reward: 454.95\n",
      "episode: 6155   score: 210.0  epsilon: 1.0    steps: 920  evaluation reward: 452.7\n",
      "Training network. lr: 0.000115. clip: 0.046096\n",
      "Iteration 17563: Policy loss: 0.273931. Value loss: 0.186946. Entropy: 0.288032.\n",
      "Iteration 17564: Policy loss: 0.265038. Value loss: 0.091464. Entropy: 0.286398.\n",
      "Iteration 17565: Policy loss: 0.264091. Value loss: 0.061727. Entropy: 0.287367.\n",
      "Training network. lr: 0.000115. clip: 0.046096\n",
      "Iteration 17566: Policy loss: 0.064765. Value loss: 0.100544. Entropy: 0.310915.\n",
      "Iteration 17567: Policy loss: 0.058584. Value loss: 0.041627. Entropy: 0.310509.\n",
      "Iteration 17568: Policy loss: 0.059842. Value loss: 0.028382. Entropy: 0.311521.\n",
      "episode: 6156   score: 210.0  epsilon: 1.0    steps: 96  evaluation reward: 448.45\n",
      "episode: 6157   score: 680.0  epsilon: 1.0    steps: 400  evaluation reward: 451.8\n",
      "Training network. lr: 0.000115. clip: 0.046096\n",
      "Iteration 17569: Policy loss: 0.014371. Value loss: 0.091586. Entropy: 0.299713.\n",
      "Iteration 17570: Policy loss: 0.017681. Value loss: 0.040967. Entropy: 0.299616.\n",
      "Iteration 17571: Policy loss: 0.012382. Value loss: 0.032051. Entropy: 0.301061.\n",
      "Training network. lr: 0.000115. clip: 0.046096\n",
      "Iteration 17572: Policy loss: -0.179037. Value loss: 0.318843. Entropy: 0.290731.\n",
      "Iteration 17573: Policy loss: -0.208482. Value loss: 0.196201. Entropy: 0.289069.\n",
      "Iteration 17574: Policy loss: -0.227846. Value loss: 0.164122. Entropy: 0.289316.\n",
      "episode: 6158   score: 430.0  epsilon: 1.0    steps: 320  evaluation reward: 450.95\n",
      "Training network. lr: 0.000115. clip: 0.046096\n",
      "Iteration 17575: Policy loss: 0.004411. Value loss: 0.116422. Entropy: 0.298705.\n",
      "Iteration 17576: Policy loss: -0.000872. Value loss: 0.047050. Entropy: 0.294939.\n",
      "Iteration 17577: Policy loss: -0.011180. Value loss: 0.036333. Entropy: 0.296637.\n",
      "episode: 6159   score: 940.0  epsilon: 1.0    steps: 504  evaluation reward: 456.2\n",
      "Training network. lr: 0.000115. clip: 0.046096\n",
      "Iteration 17578: Policy loss: 0.100209. Value loss: 0.111641. Entropy: 0.269236.\n",
      "Iteration 17579: Policy loss: 0.090312. Value loss: 0.040501. Entropy: 0.268352.\n",
      "Iteration 17580: Policy loss: 0.082153. Value loss: 0.026972. Entropy: 0.269524.\n",
      "episode: 6160   score: 525.0  epsilon: 1.0    steps: 608  evaluation reward: 458.45\n",
      "Training network. lr: 0.000115. clip: 0.046096\n",
      "Iteration 17581: Policy loss: -0.050259. Value loss: 0.092317. Entropy: 0.300761.\n",
      "Iteration 17582: Policy loss: -0.056357. Value loss: 0.038555. Entropy: 0.301051.\n",
      "Iteration 17583: Policy loss: -0.057805. Value loss: 0.031927. Entropy: 0.300121.\n",
      "episode: 6161   score: 300.0  epsilon: 1.0    steps: 624  evaluation reward: 457.7\n",
      "Training network. lr: 0.000115. clip: 0.046096\n",
      "Iteration 17584: Policy loss: -0.144286. Value loss: 0.346930. Entropy: 0.298354.\n",
      "Iteration 17585: Policy loss: -0.149450. Value loss: 0.202987. Entropy: 0.296926.\n",
      "Iteration 17586: Policy loss: -0.170806. Value loss: 0.131039. Entropy: 0.296525.\n",
      "Training network. lr: 0.000115. clip: 0.046096\n",
      "Iteration 17587: Policy loss: 0.162557. Value loss: 0.144004. Entropy: 0.310981.\n",
      "Iteration 17588: Policy loss: 0.153340. Value loss: 0.079539. Entropy: 0.309730.\n",
      "Iteration 17589: Policy loss: 0.147423. Value loss: 0.048080. Entropy: 0.310127.\n",
      "episode: 6162   score: 515.0  epsilon: 1.0    steps: 848  evaluation reward: 458.45\n",
      "Training network. lr: 0.000115. clip: 0.046096\n",
      "Iteration 17590: Policy loss: 0.013316. Value loss: 0.098388. Entropy: 0.299880.\n",
      "Iteration 17591: Policy loss: 0.013835. Value loss: 0.042081. Entropy: 0.297419.\n",
      "Iteration 17592: Policy loss: 0.006278. Value loss: 0.028975. Entropy: 0.298123.\n",
      "episode: 6163   score: 410.0  epsilon: 1.0    steps: 392  evaluation reward: 456.9\n",
      "episode: 6164   score: 440.0  epsilon: 1.0    steps: 568  evaluation reward: 457.4\n",
      "Training network. lr: 0.000115. clip: 0.046096\n",
      "Iteration 17593: Policy loss: 0.046548. Value loss: 0.199331. Entropy: 0.280681.\n",
      "Iteration 17594: Policy loss: 0.047776. Value loss: 0.109533. Entropy: 0.279667.\n",
      "Iteration 17595: Policy loss: 0.034828. Value loss: 0.080656. Entropy: 0.279987.\n",
      "episode: 6165   score: 330.0  epsilon: 1.0    steps: 208  evaluation reward: 454.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6166   score: 835.0  epsilon: 1.0    steps: 680  evaluation reward: 457.55\n",
      "Training network. lr: 0.000115. clip: 0.046096\n",
      "Iteration 17596: Policy loss: -0.068911. Value loss: 0.114418. Entropy: 0.260304.\n",
      "Iteration 17597: Policy loss: -0.073704. Value loss: 0.046250. Entropy: 0.259563.\n",
      "Iteration 17598: Policy loss: -0.079123. Value loss: 0.037756. Entropy: 0.258844.\n",
      "Training network. lr: 0.000115. clip: 0.046096\n",
      "Iteration 17599: Policy loss: 0.093746. Value loss: 0.189331. Entropy: 0.279744.\n",
      "Iteration 17600: Policy loss: 0.092286. Value loss: 0.070793. Entropy: 0.281204.\n",
      "Iteration 17601: Policy loss: 0.087931. Value loss: 0.042611. Entropy: 0.278265.\n",
      "episode: 6167   score: 340.0  epsilon: 1.0    steps: 968  evaluation reward: 456.45\n",
      "Training network. lr: 0.000115. clip: 0.045939\n",
      "Iteration 17602: Policy loss: -0.282832. Value loss: 0.566038. Entropy: 0.307065.\n",
      "Iteration 17603: Policy loss: -0.315155. Value loss: 0.425799. Entropy: 0.305777.\n",
      "Iteration 17604: Policy loss: -0.338045. Value loss: 0.380970. Entropy: 0.306224.\n",
      "Training network. lr: 0.000115. clip: 0.045939\n",
      "Iteration 17605: Policy loss: -0.025444. Value loss: 0.079438. Entropy: 0.281678.\n",
      "Iteration 17606: Policy loss: -0.035274. Value loss: 0.032456. Entropy: 0.278962.\n",
      "Iteration 17607: Policy loss: -0.040587. Value loss: 0.022850. Entropy: 0.279884.\n",
      "episode: 6168   score: 435.0  epsilon: 1.0    steps: 608  evaluation reward: 454.6\n",
      "Training network. lr: 0.000115. clip: 0.045939\n",
      "Iteration 17608: Policy loss: 0.156714. Value loss: 0.207982. Entropy: 0.283121.\n",
      "Iteration 17609: Policy loss: 0.149448. Value loss: 0.070279. Entropy: 0.283433.\n",
      "Iteration 17610: Policy loss: 0.140330. Value loss: 0.044388. Entropy: 0.283285.\n",
      "Training network. lr: 0.000115. clip: 0.045939\n",
      "Iteration 17611: Policy loss: 0.526531. Value loss: 0.214843. Entropy: 0.304059.\n",
      "Iteration 17612: Policy loss: 0.507449. Value loss: 0.073803. Entropy: 0.303800.\n",
      "Iteration 17613: Policy loss: 0.511111. Value loss: 0.049709. Entropy: 0.304018.\n",
      "episode: 6169   score: 325.0  epsilon: 1.0    steps: 640  evaluation reward: 453.35\n",
      "episode: 6170   score: 320.0  epsilon: 1.0    steps: 872  evaluation reward: 453.05\n",
      "episode: 6171   score: 590.0  epsilon: 1.0    steps: 912  evaluation reward: 453.3\n",
      "Training network. lr: 0.000115. clip: 0.045939\n",
      "Iteration 17614: Policy loss: 0.078955. Value loss: 0.129246. Entropy: 0.292104.\n",
      "Iteration 17615: Policy loss: 0.081036. Value loss: 0.066625. Entropy: 0.292820.\n",
      "Iteration 17616: Policy loss: 0.075520. Value loss: 0.051249. Entropy: 0.292873.\n",
      "episode: 6172   score: 630.0  epsilon: 1.0    steps: 288  evaluation reward: 452.9\n",
      "Training network. lr: 0.000115. clip: 0.045939\n",
      "Iteration 17617: Policy loss: 0.105598. Value loss: 0.083103. Entropy: 0.290079.\n",
      "Iteration 17618: Policy loss: 0.098711. Value loss: 0.039134. Entropy: 0.293303.\n",
      "Iteration 17619: Policy loss: 0.095651. Value loss: 0.026283. Entropy: 0.290543.\n",
      "Training network. lr: 0.000115. clip: 0.045939\n",
      "Iteration 17620: Policy loss: 0.102931. Value loss: 0.137457. Entropy: 0.308799.\n",
      "Iteration 17621: Policy loss: 0.100193. Value loss: 0.058998. Entropy: 0.309245.\n",
      "Iteration 17622: Policy loss: 0.098639. Value loss: 0.043359. Entropy: 0.307396.\n",
      "Training network. lr: 0.000115. clip: 0.045939\n",
      "Iteration 17623: Policy loss: -0.172388. Value loss: 0.316184. Entropy: 0.282780.\n",
      "Iteration 17624: Policy loss: -0.183386. Value loss: 0.170651. Entropy: 0.282643.\n",
      "Iteration 17625: Policy loss: -0.178727. Value loss: 0.122750. Entropy: 0.282221.\n",
      "episode: 6173   score: 555.0  epsilon: 1.0    steps: 360  evaluation reward: 454.55\n",
      "episode: 6174   score: 225.0  epsilon: 1.0    steps: 464  evaluation reward: 453.05\n",
      "Training network. lr: 0.000115. clip: 0.045939\n",
      "Iteration 17626: Policy loss: 0.126673. Value loss: 0.112556. Entropy: 0.264644.\n",
      "Iteration 17627: Policy loss: 0.124170. Value loss: 0.040692. Entropy: 0.265053.\n",
      "Iteration 17628: Policy loss: 0.124238. Value loss: 0.028322. Entropy: 0.263973.\n",
      "Training network. lr: 0.000115. clip: 0.045939\n",
      "Iteration 17629: Policy loss: 0.116530. Value loss: 0.120916. Entropy: 0.301172.\n",
      "Iteration 17630: Policy loss: 0.105345. Value loss: 0.056455. Entropy: 0.302125.\n",
      "Iteration 17631: Policy loss: 0.105308. Value loss: 0.038746. Entropy: 0.302178.\n",
      "Training network. lr: 0.000115. clip: 0.045939\n",
      "Iteration 17632: Policy loss: 0.176498. Value loss: 0.163340. Entropy: 0.305089.\n",
      "Iteration 17633: Policy loss: 0.167734. Value loss: 0.058041. Entropy: 0.304680.\n",
      "Iteration 17634: Policy loss: 0.170654. Value loss: 0.039750. Entropy: 0.304980.\n",
      "episode: 6175   score: 450.0  epsilon: 1.0    steps: 168  evaluation reward: 454.95\n",
      "episode: 6176   score: 310.0  epsilon: 1.0    steps: 824  evaluation reward: 450.3\n",
      "Training network. lr: 0.000115. clip: 0.045939\n",
      "Iteration 17635: Policy loss: -0.034863. Value loss: 0.094932. Entropy: 0.292587.\n",
      "Iteration 17636: Policy loss: -0.031351. Value loss: 0.044963. Entropy: 0.294193.\n",
      "Iteration 17637: Policy loss: -0.038685. Value loss: 0.035673. Entropy: 0.292507.\n",
      "Training network. lr: 0.000115. clip: 0.045939\n",
      "Iteration 17638: Policy loss: -0.062783. Value loss: 0.104706. Entropy: 0.310634.\n",
      "Iteration 17639: Policy loss: -0.071142. Value loss: 0.057622. Entropy: 0.307664.\n",
      "Iteration 17640: Policy loss: -0.073485. Value loss: 0.047367. Entropy: 0.309555.\n",
      "episode: 6177   score: 575.0  epsilon: 1.0    steps: 256  evaluation reward: 453.0\n",
      "episode: 6178   score: 225.0  epsilon: 1.0    steps: 1016  evaluation reward: 449.8\n",
      "Training network. lr: 0.000115. clip: 0.045939\n",
      "Iteration 17641: Policy loss: 0.198934. Value loss: 0.070478. Entropy: 0.285165.\n",
      "Iteration 17642: Policy loss: 0.194875. Value loss: 0.033047. Entropy: 0.287275.\n",
      "Iteration 17643: Policy loss: 0.189443. Value loss: 0.023495. Entropy: 0.286374.\n",
      "episode: 6179   score: 260.0  epsilon: 1.0    steps: 456  evaluation reward: 448.05\n",
      "Training network. lr: 0.000115. clip: 0.045939\n",
      "Iteration 17644: Policy loss: -0.195810. Value loss: 0.298074. Entropy: 0.267120.\n",
      "Iteration 17645: Policy loss: -0.200528. Value loss: 0.193500. Entropy: 0.268602.\n",
      "Iteration 17646: Policy loss: -0.212244. Value loss: 0.154160. Entropy: 0.267929.\n",
      "episode: 6180   score: 420.0  epsilon: 1.0    steps: 600  evaluation reward: 448.4\n",
      "Training network. lr: 0.000115. clip: 0.045939\n",
      "Iteration 17647: Policy loss: -0.020747. Value loss: 0.105956. Entropy: 0.288378.\n",
      "Iteration 17648: Policy loss: -0.021760. Value loss: 0.046278. Entropy: 0.289406.\n",
      "Iteration 17649: Policy loss: -0.030189. Value loss: 0.036828. Entropy: 0.289753.\n",
      "Training network. lr: 0.000115. clip: 0.045939\n",
      "Iteration 17650: Policy loss: -0.155042. Value loss: 0.100373. Entropy: 0.288625.\n",
      "Iteration 17651: Policy loss: -0.158232. Value loss: 0.048099. Entropy: 0.289629.\n",
      "Iteration 17652: Policy loss: -0.160858. Value loss: 0.032057. Entropy: 0.288605.\n",
      "Training network. lr: 0.000114. clip: 0.045782\n",
      "Iteration 17653: Policy loss: 0.312168. Value loss: 0.154062. Entropy: 0.308593.\n",
      "Iteration 17654: Policy loss: 0.298105. Value loss: 0.049505. Entropy: 0.307740.\n",
      "Iteration 17655: Policy loss: 0.297361. Value loss: 0.031463. Entropy: 0.307730.\n",
      "episode: 6181   score: 600.0  epsilon: 1.0    steps: 752  evaluation reward: 448.85\n",
      "Training network. lr: 0.000114. clip: 0.045782\n",
      "Iteration 17656: Policy loss: -0.153657. Value loss: 0.137691. Entropy: 0.278159.\n",
      "Iteration 17657: Policy loss: -0.160498. Value loss: 0.067924. Entropy: 0.283069.\n",
      "Iteration 17658: Policy loss: -0.165381. Value loss: 0.050990. Entropy: 0.281078.\n",
      "episode: 6182   score: 565.0  epsilon: 1.0    steps: 432  evaluation reward: 449.2\n",
      "episode: 6183   score: 450.0  epsilon: 1.0    steps: 584  evaluation reward: 448.0\n",
      "episode: 6184   score: 800.0  epsilon: 1.0    steps: 696  evaluation reward: 451.8\n",
      "Training network. lr: 0.000114. clip: 0.045782\n",
      "Iteration 17659: Policy loss: -0.381626. Value loss: 0.269943. Entropy: 0.284461.\n",
      "Iteration 17660: Policy loss: -0.401494. Value loss: 0.206648. Entropy: 0.284048.\n",
      "Iteration 17661: Policy loss: -0.399417. Value loss: 0.178594. Entropy: 0.285622.\n",
      "Training network. lr: 0.000114. clip: 0.045782\n",
      "Iteration 17662: Policy loss: -0.150905. Value loss: 0.108369. Entropy: 0.306737.\n",
      "Iteration 17663: Policy loss: -0.151319. Value loss: 0.057101. Entropy: 0.305348.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17664: Policy loss: -0.152125. Value loss: 0.045762. Entropy: 0.306932.\n",
      "episode: 6185   score: 465.0  epsilon: 1.0    steps: 464  evaluation reward: 452.1\n",
      "episode: 6186   score: 335.0  epsilon: 1.0    steps: 472  evaluation reward: 452.3\n",
      "Training network. lr: 0.000114. clip: 0.045782\n",
      "Iteration 17665: Policy loss: 0.377558. Value loss: 0.160812. Entropy: 0.281952.\n",
      "Iteration 17666: Policy loss: 0.376021. Value loss: 0.092982. Entropy: 0.278491.\n",
      "Iteration 17667: Policy loss: 0.358671. Value loss: 0.068740. Entropy: 0.278024.\n",
      "Training network. lr: 0.000114. clip: 0.045782\n",
      "Iteration 17668: Policy loss: 0.003766. Value loss: 0.129221. Entropy: 0.287503.\n",
      "Iteration 17669: Policy loss: -0.004482. Value loss: 0.040150. Entropy: 0.286723.\n",
      "Iteration 17670: Policy loss: -0.011591. Value loss: 0.027223. Entropy: 0.285181.\n",
      "episode: 6187   score: 615.0  epsilon: 1.0    steps: 680  evaluation reward: 452.65\n",
      "Training network. lr: 0.000114. clip: 0.045782\n",
      "Iteration 17671: Policy loss: 0.065887. Value loss: 0.125007. Entropy: 0.291490.\n",
      "Iteration 17672: Policy loss: 0.069273. Value loss: 0.058247. Entropy: 0.289546.\n",
      "Iteration 17673: Policy loss: 0.071584. Value loss: 0.046391. Entropy: 0.290899.\n",
      "episode: 6188   score: 525.0  epsilon: 1.0    steps: 504  evaluation reward: 451.15\n",
      "Training network. lr: 0.000114. clip: 0.045782\n",
      "Iteration 17674: Policy loss: -0.372166. Value loss: 0.275046. Entropy: 0.286272.\n",
      "Iteration 17675: Policy loss: -0.359689. Value loss: 0.081776. Entropy: 0.286249.\n",
      "Iteration 17676: Policy loss: -0.355514. Value loss: 0.040252. Entropy: 0.286495.\n",
      "episode: 6189   score: 315.0  epsilon: 1.0    steps: 744  evaluation reward: 451.15\n",
      "Training network. lr: 0.000114. clip: 0.045782\n",
      "Iteration 17677: Policy loss: 0.022055. Value loss: 0.086022. Entropy: 0.300184.\n",
      "Iteration 17678: Policy loss: 0.017310. Value loss: 0.044133. Entropy: 0.301571.\n",
      "Iteration 17679: Policy loss: 0.017032. Value loss: 0.032506. Entropy: 0.300627.\n",
      "episode: 6190   score: 285.0  epsilon: 1.0    steps: 632  evaluation reward: 452.45\n",
      "Training network. lr: 0.000114. clip: 0.045782\n",
      "Iteration 17680: Policy loss: -0.204716. Value loss: 0.140547. Entropy: 0.284427.\n",
      "Iteration 17681: Policy loss: -0.206497. Value loss: 0.056256. Entropy: 0.283634.\n",
      "Iteration 17682: Policy loss: -0.208402. Value loss: 0.043671. Entropy: 0.285590.\n",
      "episode: 6191   score: 380.0  epsilon: 1.0    steps: 664  evaluation reward: 453.1\n",
      "Training network. lr: 0.000114. clip: 0.045782\n",
      "Iteration 17683: Policy loss: -0.038673. Value loss: 0.272146. Entropy: 0.291049.\n",
      "Iteration 17684: Policy loss: -0.036942. Value loss: 0.085045. Entropy: 0.289843.\n",
      "Iteration 17685: Policy loss: -0.053824. Value loss: 0.038423. Entropy: 0.291714.\n",
      "Training network. lr: 0.000114. clip: 0.045782\n",
      "Iteration 17686: Policy loss: -0.117449. Value loss: 0.305481. Entropy: 0.308877.\n",
      "Iteration 17687: Policy loss: -0.136440. Value loss: 0.212416. Entropy: 0.308041.\n",
      "Iteration 17688: Policy loss: -0.164194. Value loss: 0.186311. Entropy: 0.307344.\n",
      "Training network. lr: 0.000114. clip: 0.045782\n",
      "Iteration 17689: Policy loss: 0.222296. Value loss: 0.182322. Entropy: 0.299724.\n",
      "Iteration 17690: Policy loss: 0.213329. Value loss: 0.064431. Entropy: 0.299265.\n",
      "Iteration 17691: Policy loss: 0.208039. Value loss: 0.039633. Entropy: 0.300300.\n",
      "Training network. lr: 0.000114. clip: 0.045782\n",
      "Iteration 17692: Policy loss: -0.341729. Value loss: 0.318310. Entropy: 0.294141.\n",
      "Iteration 17693: Policy loss: -0.332797. Value loss: 0.155411. Entropy: 0.294849.\n",
      "Iteration 17694: Policy loss: -0.360792. Value loss: 0.107691. Entropy: 0.295522.\n",
      "episode: 6192   score: 180.0  epsilon: 1.0    steps: 856  evaluation reward: 450.4\n",
      "Training network. lr: 0.000114. clip: 0.045782\n",
      "Iteration 17695: Policy loss: 0.059269. Value loss: 0.129523. Entropy: 0.286116.\n",
      "Iteration 17696: Policy loss: 0.066309. Value loss: 0.055423. Entropy: 0.285427.\n",
      "Iteration 17697: Policy loss: 0.051236. Value loss: 0.034877. Entropy: 0.286129.\n",
      "episode: 6193   score: 770.0  epsilon: 1.0    steps: 880  evaluation reward: 454.75\n",
      "Training network. lr: 0.000114. clip: 0.045782\n",
      "Iteration 17698: Policy loss: -0.129162. Value loss: 0.163808. Entropy: 0.304135.\n",
      "Iteration 17699: Policy loss: -0.136268. Value loss: 0.070462. Entropy: 0.304045.\n",
      "Iteration 17700: Policy loss: -0.137484. Value loss: 0.049913. Entropy: 0.303808.\n",
      "episode: 6194   score: 450.0  epsilon: 1.0    steps: 560  evaluation reward: 455.15\n",
      "episode: 6195   score: 925.0  epsilon: 1.0    steps: 680  evaluation reward: 460.8\n",
      "Training network. lr: 0.000114. clip: 0.045635\n",
      "Iteration 17701: Policy loss: 0.034870. Value loss: 0.101548. Entropy: 0.280823.\n",
      "Iteration 17702: Policy loss: 0.037024. Value loss: 0.053798. Entropy: 0.279198.\n",
      "Iteration 17703: Policy loss: 0.033169. Value loss: 0.042304. Entropy: 0.278154.\n",
      "episode: 6196   score: 420.0  epsilon: 1.0    steps: 440  evaluation reward: 457.25\n",
      "episode: 6197   score: 410.0  epsilon: 1.0    steps: 720  evaluation reward: 454.4\n",
      "Training network. lr: 0.000114. clip: 0.045635\n",
      "Iteration 17704: Policy loss: 0.278716. Value loss: 0.184843. Entropy: 0.289307.\n",
      "Iteration 17705: Policy loss: 0.262098. Value loss: 0.070578. Entropy: 0.285882.\n",
      "Iteration 17706: Policy loss: 0.250180. Value loss: 0.055262. Entropy: 0.286358.\n",
      "episode: 6198   score: 630.0  epsilon: 1.0    steps: 192  evaluation reward: 456.25\n",
      "Training network. lr: 0.000114. clip: 0.045635\n",
      "Iteration 17707: Policy loss: 0.041246. Value loss: 0.174505. Entropy: 0.290087.\n",
      "Iteration 17708: Policy loss: 0.038382. Value loss: 0.084679. Entropy: 0.292747.\n",
      "Iteration 17709: Policy loss: 0.030497. Value loss: 0.061810. Entropy: 0.292639.\n",
      "Training network. lr: 0.000114. clip: 0.045635\n",
      "Iteration 17710: Policy loss: -0.320165. Value loss: 0.270875. Entropy: 0.291539.\n",
      "Iteration 17711: Policy loss: -0.327576. Value loss: 0.180690. Entropy: 0.291166.\n",
      "Iteration 17712: Policy loss: -0.309048. Value loss: 0.100064. Entropy: 0.291762.\n",
      "Training network. lr: 0.000114. clip: 0.045635\n",
      "Iteration 17713: Policy loss: 0.053932. Value loss: 0.237784. Entropy: 0.294009.\n",
      "Iteration 17714: Policy loss: 0.037992. Value loss: 0.089260. Entropy: 0.297495.\n",
      "Iteration 17715: Policy loss: 0.029970. Value loss: 0.054619. Entropy: 0.295553.\n",
      "Training network. lr: 0.000114. clip: 0.045635\n",
      "Iteration 17716: Policy loss: -0.621615. Value loss: 0.536924. Entropy: 0.299705.\n",
      "Iteration 17717: Policy loss: -0.637950. Value loss: 0.282798. Entropy: 0.299311.\n",
      "Iteration 17718: Policy loss: -0.642260. Value loss: 0.133985. Entropy: 0.298634.\n",
      "episode: 6199   score: 425.0  epsilon: 1.0    steps: 296  evaluation reward: 456.5\n",
      "episode: 6200   score: 1060.0  epsilon: 1.0    steps: 872  evaluation reward: 461.95\n",
      "Training network. lr: 0.000114. clip: 0.045635\n",
      "Iteration 17719: Policy loss: 0.336402. Value loss: 0.167382. Entropy: 0.298068.\n",
      "Iteration 17720: Policy loss: 0.321973. Value loss: 0.060712. Entropy: 0.297438.\n",
      "Iteration 17721: Policy loss: 0.320782. Value loss: 0.042231. Entropy: 0.297662.\n",
      "Training network. lr: 0.000114. clip: 0.045635\n",
      "Iteration 17722: Policy loss: -0.030739. Value loss: 0.107317. Entropy: 0.309755.\n",
      "Iteration 17723: Policy loss: -0.036492. Value loss: 0.039436. Entropy: 0.308805.\n",
      "Iteration 17724: Policy loss: -0.032503. Value loss: 0.026657. Entropy: 0.310965.\n",
      "Training network. lr: 0.000114. clip: 0.045635\n",
      "Iteration 17725: Policy loss: 0.088838. Value loss: 0.134450. Entropy: 0.309338.\n",
      "Iteration 17726: Policy loss: 0.099041. Value loss: 0.056946. Entropy: 0.309411.\n",
      "Iteration 17727: Policy loss: 0.091801. Value loss: 0.042419. Entropy: 0.309463.\n",
      "now time :  2019-09-06 08:35:05.572099\n",
      "episode: 6201   score: 480.0  epsilon: 1.0    steps: 248  evaluation reward: 463.4\n",
      "episode: 6202   score: 590.0  epsilon: 1.0    steps: 992  evaluation reward: 463.65\n",
      "Training network. lr: 0.000114. clip: 0.045635\n",
      "Iteration 17728: Policy loss: 0.351867. Value loss: 0.100541. Entropy: 0.298363.\n",
      "Iteration 17729: Policy loss: 0.345557. Value loss: 0.038201. Entropy: 0.299261.\n",
      "Iteration 17730: Policy loss: 0.346500. Value loss: 0.026506. Entropy: 0.297883.\n",
      "episode: 6203   score: 575.0  epsilon: 1.0    steps: 136  evaluation reward: 460.85\n",
      "Training network. lr: 0.000114. clip: 0.045635\n",
      "Iteration 17731: Policy loss: -0.313416. Value loss: 0.255339. Entropy: 0.293681.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17732: Policy loss: -0.323117. Value loss: 0.121173. Entropy: 0.297075.\n",
      "Iteration 17733: Policy loss: -0.336415. Value loss: 0.071733. Entropy: 0.297008.\n",
      "Training network. lr: 0.000114. clip: 0.045635\n",
      "Iteration 17734: Policy loss: -0.120443. Value loss: 0.204766. Entropy: 0.295619.\n",
      "Iteration 17735: Policy loss: -0.146317. Value loss: 0.065705. Entropy: 0.293687.\n",
      "Iteration 17736: Policy loss: -0.158701. Value loss: 0.044203. Entropy: 0.292822.\n",
      "episode: 6204   score: 680.0  epsilon: 1.0    steps: 288  evaluation reward: 465.05\n",
      "episode: 6205   score: 695.0  epsilon: 1.0    steps: 408  evaluation reward: 468.95\n",
      "Training network. lr: 0.000114. clip: 0.045635\n",
      "Iteration 17737: Policy loss: 0.214990. Value loss: 0.226112. Entropy: 0.288844.\n",
      "Iteration 17738: Policy loss: 0.218589. Value loss: 0.067453. Entropy: 0.287563.\n",
      "Iteration 17739: Policy loss: 0.213760. Value loss: 0.047274. Entropy: 0.285689.\n",
      "Training network. lr: 0.000114. clip: 0.045635\n",
      "Iteration 17740: Policy loss: 0.250633. Value loss: 0.173214. Entropy: 0.308087.\n",
      "Iteration 17741: Policy loss: 0.248015. Value loss: 0.063466. Entropy: 0.307078.\n",
      "Iteration 17742: Policy loss: 0.240438. Value loss: 0.043015. Entropy: 0.305564.\n",
      "episode: 6206   score: 380.0  epsilon: 1.0    steps: 200  evaluation reward: 465.95\n",
      "Training network. lr: 0.000114. clip: 0.045635\n",
      "Iteration 17743: Policy loss: -0.065181. Value loss: 0.156211. Entropy: 0.303701.\n",
      "Iteration 17744: Policy loss: -0.072137. Value loss: 0.073705. Entropy: 0.301742.\n",
      "Iteration 17745: Policy loss: -0.077665. Value loss: 0.050435. Entropy: 0.303826.\n",
      "episode: 6207   score: 425.0  epsilon: 1.0    steps: 104  evaluation reward: 465.4\n",
      "episode: 6208   score: 470.0  epsilon: 1.0    steps: 184  evaluation reward: 467.7\n",
      "Training network. lr: 0.000114. clip: 0.045635\n",
      "Iteration 17746: Policy loss: 0.060645. Value loss: 0.115464. Entropy: 0.299918.\n",
      "Iteration 17747: Policy loss: 0.047389. Value loss: 0.038622. Entropy: 0.298277.\n",
      "Iteration 17748: Policy loss: 0.043465. Value loss: 0.026635. Entropy: 0.299570.\n",
      "Training network. lr: 0.000114. clip: 0.045635\n",
      "Iteration 17749: Policy loss: 0.306129. Value loss: 0.134583. Entropy: 0.303942.\n",
      "Iteration 17750: Policy loss: 0.300941. Value loss: 0.057866. Entropy: 0.302065.\n",
      "Iteration 17751: Policy loss: 0.296584. Value loss: 0.039790. Entropy: 0.302996.\n",
      "episode: 6209   score: 350.0  epsilon: 1.0    steps: 144  evaluation reward: 465.25\n",
      "episode: 6210   score: 345.0  epsilon: 1.0    steps: 208  evaluation reward: 462.8\n",
      "Training network. lr: 0.000114. clip: 0.045478\n",
      "Iteration 17752: Policy loss: -0.214507. Value loss: 0.338863. Entropy: 0.281373.\n",
      "Iteration 17753: Policy loss: -0.205447. Value loss: 0.159685. Entropy: 0.279551.\n",
      "Iteration 17754: Policy loss: -0.236891. Value loss: 0.123267. Entropy: 0.277728.\n",
      "Training network. lr: 0.000114. clip: 0.045478\n",
      "Iteration 17755: Policy loss: -0.161281. Value loss: 0.325424. Entropy: 0.304813.\n",
      "Iteration 17756: Policy loss: -0.163639. Value loss: 0.192955. Entropy: 0.304808.\n",
      "Iteration 17757: Policy loss: -0.159622. Value loss: 0.114051. Entropy: 0.304811.\n",
      "episode: 6211   score: 555.0  epsilon: 1.0    steps: 208  evaluation reward: 462.65\n",
      "episode: 6212   score: 370.0  epsilon: 1.0    steps: 336  evaluation reward: 461.85\n",
      "Training network. lr: 0.000114. clip: 0.045478\n",
      "Iteration 17758: Policy loss: -0.184257. Value loss: 0.136999. Entropy: 0.291442.\n",
      "Iteration 17759: Policy loss: -0.184979. Value loss: 0.060305. Entropy: 0.290451.\n",
      "Iteration 17760: Policy loss: -0.195352. Value loss: 0.039844. Entropy: 0.289709.\n",
      "episode: 6213   score: 430.0  epsilon: 1.0    steps: 376  evaluation reward: 462.35\n",
      "Training network. lr: 0.000114. clip: 0.045478\n",
      "Iteration 17761: Policy loss: 0.417738. Value loss: 0.245764. Entropy: 0.297785.\n",
      "Iteration 17762: Policy loss: 0.412672. Value loss: 0.089073. Entropy: 0.294617.\n",
      "Iteration 17763: Policy loss: 0.404114. Value loss: 0.058363. Entropy: 0.293683.\n",
      "Training network. lr: 0.000114. clip: 0.045478\n",
      "Iteration 17764: Policy loss: -0.138615. Value loss: 0.148017. Entropy: 0.310174.\n",
      "Iteration 17765: Policy loss: -0.143024. Value loss: 0.068819. Entropy: 0.309957.\n",
      "Iteration 17766: Policy loss: -0.150278. Value loss: 0.043650. Entropy: 0.311001.\n",
      "Training network. lr: 0.000114. clip: 0.045478\n",
      "Iteration 17767: Policy loss: 0.248644. Value loss: 0.202784. Entropy: 0.309082.\n",
      "Iteration 17768: Policy loss: 0.244892. Value loss: 0.077389. Entropy: 0.307125.\n",
      "Iteration 17769: Policy loss: 0.249080. Value loss: 0.053475. Entropy: 0.306151.\n",
      "episode: 6214   score: 635.0  epsilon: 1.0    steps: 344  evaluation reward: 464.0\n",
      "Training network. lr: 0.000114. clip: 0.045478\n",
      "Iteration 17770: Policy loss: 0.146512. Value loss: 0.153925. Entropy: 0.296797.\n",
      "Iteration 17771: Policy loss: 0.139090. Value loss: 0.048627. Entropy: 0.297448.\n",
      "Iteration 17772: Policy loss: 0.129514. Value loss: 0.033759. Entropy: 0.295973.\n",
      "episode: 6215   score: 425.0  epsilon: 1.0    steps: 568  evaluation reward: 465.25\n",
      "Training network. lr: 0.000114. clip: 0.045478\n",
      "Iteration 17773: Policy loss: 0.185769. Value loss: 0.116511. Entropy: 0.288803.\n",
      "Iteration 17774: Policy loss: 0.175625. Value loss: 0.047916. Entropy: 0.286575.\n",
      "Iteration 17775: Policy loss: 0.172506. Value loss: 0.031969. Entropy: 0.286349.\n",
      "Training network. lr: 0.000114. clip: 0.045478\n",
      "Iteration 17776: Policy loss: 0.305758. Value loss: 0.199403. Entropy: 0.304772.\n",
      "Iteration 17777: Policy loss: 0.297911. Value loss: 0.092920. Entropy: 0.304774.\n",
      "Iteration 17778: Policy loss: 0.296376. Value loss: 0.056666. Entropy: 0.303610.\n",
      "episode: 6216   score: 650.0  epsilon: 1.0    steps: 432  evaluation reward: 464.8\n",
      "episode: 6217   score: 290.0  epsilon: 1.0    steps: 680  evaluation reward: 464.85\n",
      "episode: 6218   score: 360.0  epsilon: 1.0    steps: 840  evaluation reward: 466.2\n",
      "Training network. lr: 0.000114. clip: 0.045478\n",
      "Iteration 17779: Policy loss: 0.252061. Value loss: 0.136954. Entropy: 0.283589.\n",
      "Iteration 17780: Policy loss: 0.250176. Value loss: 0.075130. Entropy: 0.283127.\n",
      "Iteration 17781: Policy loss: 0.238858. Value loss: 0.057731. Entropy: 0.283004.\n",
      "episode: 6219   score: 450.0  epsilon: 1.0    steps: 568  evaluation reward: 466.65\n",
      "episode: 6220   score: 395.0  epsilon: 1.0    steps: 1008  evaluation reward: 464.45\n",
      "Training network. lr: 0.000114. clip: 0.045478\n",
      "Iteration 17782: Policy loss: -0.242100. Value loss: 0.249555. Entropy: 0.304500.\n",
      "Iteration 17783: Policy loss: -0.239846. Value loss: 0.140463. Entropy: 0.307140.\n",
      "Iteration 17784: Policy loss: -0.256425. Value loss: 0.114184. Entropy: 0.306300.\n",
      "Training network. lr: 0.000114. clip: 0.045478\n",
      "Iteration 17785: Policy loss: 0.152246. Value loss: 0.145369. Entropy: 0.306535.\n",
      "Iteration 17786: Policy loss: 0.140432. Value loss: 0.075469. Entropy: 0.305439.\n",
      "Iteration 17787: Policy loss: 0.144870. Value loss: 0.053190. Entropy: 0.305270.\n",
      "episode: 6221   score: 540.0  epsilon: 1.0    steps: 120  evaluation reward: 463.0\n",
      "episode: 6222   score: 290.0  epsilon: 1.0    steps: 976  evaluation reward: 463.75\n",
      "Training network. lr: 0.000114. clip: 0.045478\n",
      "Iteration 17788: Policy loss: -0.293423. Value loss: 0.233309. Entropy: 0.294974.\n",
      "Iteration 17789: Policy loss: -0.288906. Value loss: 0.112409. Entropy: 0.296877.\n",
      "Iteration 17790: Policy loss: -0.304502. Value loss: 0.068322. Entropy: 0.296757.\n",
      "Training network. lr: 0.000114. clip: 0.045478\n",
      "Iteration 17791: Policy loss: 0.132630. Value loss: 0.125778. Entropy: 0.303925.\n",
      "Iteration 17792: Policy loss: 0.119573. Value loss: 0.050394. Entropy: 0.302419.\n",
      "Iteration 17793: Policy loss: 0.124456. Value loss: 0.034438. Entropy: 0.302275.\n",
      "Training network. lr: 0.000114. clip: 0.045478\n",
      "Iteration 17794: Policy loss: 0.450284. Value loss: 0.217066. Entropy: 0.311026.\n",
      "Iteration 17795: Policy loss: 0.437781. Value loss: 0.076184. Entropy: 0.309896.\n",
      "Iteration 17796: Policy loss: 0.417221. Value loss: 0.050925. Entropy: 0.309951.\n",
      "episode: 6223   score: 455.0  epsilon: 1.0    steps: 888  evaluation reward: 462.95\n",
      "episode: 6224   score: 260.0  epsilon: 1.0    steps: 960  evaluation reward: 462.05\n",
      "Training network. lr: 0.000114. clip: 0.045478\n",
      "Iteration 17797: Policy loss: 0.119553. Value loss: 0.142970. Entropy: 0.299985.\n",
      "Iteration 17798: Policy loss: 0.116374. Value loss: 0.070574. Entropy: 0.299366.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17799: Policy loss: 0.115199. Value loss: 0.049166. Entropy: 0.298897.\n",
      "Training network. lr: 0.000114. clip: 0.045478\n",
      "Iteration 17800: Policy loss: -0.043237. Value loss: 0.091522. Entropy: 0.305393.\n",
      "Iteration 17801: Policy loss: -0.054121. Value loss: 0.045151. Entropy: 0.306935.\n",
      "Iteration 17802: Policy loss: -0.053799. Value loss: 0.034712. Entropy: 0.305038.\n",
      "episode: 6225   score: 620.0  epsilon: 1.0    steps: 632  evaluation reward: 464.75\n",
      "Training network. lr: 0.000113. clip: 0.045321\n",
      "Iteration 17803: Policy loss: 0.084031. Value loss: 0.120997. Entropy: 0.298625.\n",
      "Iteration 17804: Policy loss: 0.073638. Value loss: 0.050717. Entropy: 0.299502.\n",
      "Iteration 17805: Policy loss: 0.073783. Value loss: 0.037213. Entropy: 0.299407.\n",
      "Training network. lr: 0.000113. clip: 0.045321\n",
      "Iteration 17806: Policy loss: -0.120035. Value loss: 0.374255. Entropy: 0.307562.\n",
      "Iteration 17807: Policy loss: -0.121923. Value loss: 0.202939. Entropy: 0.305860.\n",
      "Iteration 17808: Policy loss: -0.145611. Value loss: 0.146981. Entropy: 0.307802.\n",
      "episode: 6226   score: 485.0  epsilon: 1.0    steps: 712  evaluation reward: 465.4\n",
      "episode: 6227   score: 425.0  epsilon: 1.0    steps: 800  evaluation reward: 465.7\n",
      "episode: 6228   score: 435.0  epsilon: 1.0    steps: 936  evaluation reward: 466.55\n",
      "Training network. lr: 0.000113. clip: 0.045321\n",
      "Iteration 17809: Policy loss: 0.092170. Value loss: 0.087327. Entropy: 0.288682.\n",
      "Iteration 17810: Policy loss: 0.091989. Value loss: 0.045122. Entropy: 0.289356.\n",
      "Iteration 17811: Policy loss: 0.093128. Value loss: 0.034825. Entropy: 0.289235.\n",
      "episode: 6229   score: 370.0  epsilon: 1.0    steps: 40  evaluation reward: 466.75\n",
      "episode: 6230   score: 420.0  epsilon: 1.0    steps: 480  evaluation reward: 467.9\n",
      "Training network. lr: 0.000113. clip: 0.045321\n",
      "Iteration 17812: Policy loss: -0.057381. Value loss: 0.087968. Entropy: 0.272426.\n",
      "Iteration 17813: Policy loss: -0.058413. Value loss: 0.051973. Entropy: 0.270642.\n",
      "Iteration 17814: Policy loss: -0.056147. Value loss: 0.042638. Entropy: 0.272817.\n",
      "Training network. lr: 0.000113. clip: 0.045321\n",
      "Iteration 17815: Policy loss: -0.025179. Value loss: 0.195757. Entropy: 0.306057.\n",
      "Iteration 17816: Policy loss: -0.029501. Value loss: 0.077711. Entropy: 0.305301.\n",
      "Iteration 17817: Policy loss: -0.037154. Value loss: 0.047940. Entropy: 0.305746.\n",
      "Training network. lr: 0.000113. clip: 0.045321\n",
      "Iteration 17818: Policy loss: 0.007938. Value loss: 0.277920. Entropy: 0.300791.\n",
      "Iteration 17819: Policy loss: -0.000139. Value loss: 0.109231. Entropy: 0.297696.\n",
      "Iteration 17820: Policy loss: -0.007827. Value loss: 0.066908. Entropy: 0.298911.\n",
      "episode: 6231   score: 305.0  epsilon: 1.0    steps: 120  evaluation reward: 463.7\n",
      "episode: 6232   score: 390.0  epsilon: 1.0    steps: 328  evaluation reward: 464.15\n",
      "Training network. lr: 0.000113. clip: 0.045321\n",
      "Iteration 17821: Policy loss: 0.216987. Value loss: 0.381633. Entropy: 0.278865.\n",
      "Iteration 17822: Policy loss: 0.217115. Value loss: 0.183463. Entropy: 0.279496.\n",
      "Iteration 17823: Policy loss: 0.199012. Value loss: 0.082036. Entropy: 0.279965.\n",
      "Training network. lr: 0.000113. clip: 0.045321\n",
      "Iteration 17824: Policy loss: -0.007215. Value loss: 0.165101. Entropy: 0.307950.\n",
      "Iteration 17825: Policy loss: -0.004541. Value loss: 0.099383. Entropy: 0.308187.\n",
      "Iteration 17826: Policy loss: -0.008839. Value loss: 0.068075. Entropy: 0.306299.\n",
      "Training network. lr: 0.000113. clip: 0.045321\n",
      "Iteration 17827: Policy loss: -0.058638. Value loss: 0.157312. Entropy: 0.310096.\n",
      "Iteration 17828: Policy loss: -0.063199. Value loss: 0.062459. Entropy: 0.310186.\n",
      "Iteration 17829: Policy loss: -0.056158. Value loss: 0.041355. Entropy: 0.310241.\n",
      "Training network. lr: 0.000113. clip: 0.045321\n",
      "Iteration 17830: Policy loss: -0.097367. Value loss: 0.323525. Entropy: 0.311949.\n",
      "Iteration 17831: Policy loss: -0.086283. Value loss: 0.151523. Entropy: 0.311475.\n",
      "Iteration 17832: Policy loss: -0.096298. Value loss: 0.045697. Entropy: 0.310973.\n",
      "episode: 6233   score: 295.0  epsilon: 1.0    steps: 200  evaluation reward: 463.5\n",
      "episode: 6234   score: 410.0  epsilon: 1.0    steps: 928  evaluation reward: 462.25\n",
      "episode: 6235   score: 670.0  epsilon: 1.0    steps: 968  evaluation reward: 460.7\n",
      "Training network. lr: 0.000113. clip: 0.045321\n",
      "Iteration 17833: Policy loss: -0.028840. Value loss: 0.271163. Entropy: 0.294051.\n",
      "Iteration 17834: Policy loss: -0.047991. Value loss: 0.129428. Entropy: 0.295571.\n",
      "Iteration 17835: Policy loss: -0.061161. Value loss: 0.092134. Entropy: 0.295529.\n",
      "episode: 6236   score: 835.0  epsilon: 1.0    steps: 104  evaluation reward: 465.1\n",
      "episode: 6237   score: 380.0  epsilon: 1.0    steps: 304  evaluation reward: 464.65\n",
      "Training network. lr: 0.000113. clip: 0.045321\n",
      "Iteration 17836: Policy loss: 0.022915. Value loss: 0.085619. Entropy: 0.287898.\n",
      "Iteration 17837: Policy loss: 0.022043. Value loss: 0.045460. Entropy: 0.289472.\n",
      "Iteration 17838: Policy loss: 0.025301. Value loss: 0.037013. Entropy: 0.282778.\n",
      "Training network. lr: 0.000113. clip: 0.045321\n",
      "Iteration 17839: Policy loss: 0.326093. Value loss: 0.253406. Entropy: 0.304716.\n",
      "Iteration 17840: Policy loss: 0.325481. Value loss: 0.109578. Entropy: 0.305195.\n",
      "Iteration 17841: Policy loss: 0.304618. Value loss: 0.073821. Entropy: 0.304823.\n",
      "episode: 6238   score: 495.0  epsilon: 1.0    steps: 872  evaluation reward: 466.25\n",
      "Training network. lr: 0.000113. clip: 0.045321\n",
      "Iteration 17842: Policy loss: -0.093083. Value loss: 0.224064. Entropy: 0.310863.\n",
      "Iteration 17843: Policy loss: -0.098390. Value loss: 0.084052. Entropy: 0.311204.\n",
      "Iteration 17844: Policy loss: -0.091689. Value loss: 0.064344. Entropy: 0.311860.\n",
      "episode: 6239   score: 365.0  epsilon: 1.0    steps: 160  evaluation reward: 463.75\n",
      "episode: 6240   score: 610.0  epsilon: 1.0    steps: 216  evaluation reward: 467.75\n",
      "Training network. lr: 0.000113. clip: 0.045321\n",
      "Iteration 17845: Policy loss: 0.091651. Value loss: 0.085030. Entropy: 0.298809.\n",
      "Iteration 17846: Policy loss: 0.080450. Value loss: 0.037733. Entropy: 0.296121.\n",
      "Iteration 17847: Policy loss: 0.086911. Value loss: 0.029592. Entropy: 0.295309.\n",
      "Training network. lr: 0.000113. clip: 0.045321\n",
      "Iteration 17848: Policy loss: -0.249075. Value loss: 0.414315. Entropy: 0.305307.\n",
      "Iteration 17849: Policy loss: -0.265711. Value loss: 0.155219. Entropy: 0.305632.\n",
      "Iteration 17850: Policy loss: -0.273203. Value loss: 0.088817. Entropy: 0.304109.\n",
      "Training network. lr: 0.000113. clip: 0.045174\n",
      "Iteration 17851: Policy loss: -0.279400. Value loss: 0.367077. Entropy: 0.302102.\n",
      "Iteration 17852: Policy loss: -0.289305. Value loss: 0.163395. Entropy: 0.303539.\n",
      "Iteration 17853: Policy loss: -0.296520. Value loss: 0.109648. Entropy: 0.301620.\n",
      "episode: 6241   score: 515.0  epsilon: 1.0    steps: 672  evaluation reward: 470.75\n",
      "episode: 6242   score: 600.0  epsilon: 1.0    steps: 800  evaluation reward: 475.1\n",
      "Training network. lr: 0.000113. clip: 0.045174\n",
      "Iteration 17854: Policy loss: 0.716789. Value loss: 0.407662. Entropy: 0.289900.\n",
      "Iteration 17855: Policy loss: 0.715593. Value loss: 0.129474. Entropy: 0.287848.\n",
      "Iteration 17856: Policy loss: 0.692038. Value loss: 0.063467. Entropy: 0.287477.\n",
      "episode: 6243   score: 335.0  epsilon: 1.0    steps: 656  evaluation reward: 475.15\n",
      "episode: 6244   score: 340.0  epsilon: 1.0    steps: 888  evaluation reward: 475.0\n",
      "Training network. lr: 0.000113. clip: 0.045174\n",
      "Iteration 17857: Policy loss: 0.204527. Value loss: 0.184238. Entropy: 0.292227.\n",
      "Iteration 17858: Policy loss: 0.206949. Value loss: 0.092288. Entropy: 0.293887.\n",
      "Iteration 17859: Policy loss: 0.202766. Value loss: 0.065488. Entropy: 0.290589.\n",
      "episode: 6245   score: 595.0  epsilon: 1.0    steps: 224  evaluation reward: 476.15\n",
      "Training network. lr: 0.000113. clip: 0.045174\n",
      "Iteration 17860: Policy loss: 0.248250. Value loss: 0.198593. Entropy: 0.303380.\n",
      "Iteration 17861: Policy loss: 0.244818. Value loss: 0.079292. Entropy: 0.301365.\n",
      "Iteration 17862: Policy loss: 0.241728. Value loss: 0.056658. Entropy: 0.301728.\n",
      "Training network. lr: 0.000113. clip: 0.045174\n",
      "Iteration 17863: Policy loss: -0.156018. Value loss: 0.211882. Entropy: 0.297705.\n",
      "Iteration 17864: Policy loss: -0.175891. Value loss: 0.102241. Entropy: 0.297355.\n",
      "Iteration 17865: Policy loss: -0.188170. Value loss: 0.080671. Entropy: 0.296812.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000113. clip: 0.045174\n",
      "Iteration 17866: Policy loss: -0.046558. Value loss: 0.581169. Entropy: 0.311081.\n",
      "Iteration 17867: Policy loss: -0.064794. Value loss: 0.299698. Entropy: 0.311439.\n",
      "Iteration 17868: Policy loss: -0.090153. Value loss: 0.184205. Entropy: 0.311930.\n",
      "episode: 6246   score: 390.0  epsilon: 1.0    steps: 512  evaluation reward: 474.7\n",
      "Training network. lr: 0.000113. clip: 0.045174\n",
      "Iteration 17869: Policy loss: 0.160534. Value loss: 0.115746. Entropy: 0.305499.\n",
      "Iteration 17870: Policy loss: 0.154537. Value loss: 0.059859. Entropy: 0.306475.\n",
      "Iteration 17871: Policy loss: 0.155917. Value loss: 0.040115. Entropy: 0.305656.\n",
      "Training network. lr: 0.000113. clip: 0.045174\n",
      "Iteration 17872: Policy loss: 0.162472. Value loss: 0.188764. Entropy: 0.302470.\n",
      "Iteration 17873: Policy loss: 0.151696. Value loss: 0.060123. Entropy: 0.303583.\n",
      "Iteration 17874: Policy loss: 0.149252. Value loss: 0.032976. Entropy: 0.303578.\n",
      "episode: 6247   score: 480.0  epsilon: 1.0    steps: 424  evaluation reward: 476.4\n",
      "Training network. lr: 0.000113. clip: 0.045174\n",
      "Iteration 17875: Policy loss: 0.355599. Value loss: 0.360451. Entropy: 0.308403.\n",
      "Iteration 17876: Policy loss: 0.328738. Value loss: 0.128353. Entropy: 0.307069.\n",
      "Iteration 17877: Policy loss: 0.337672. Value loss: 0.090275. Entropy: 0.306826.\n",
      "episode: 6248   score: 450.0  epsilon: 1.0    steps: 232  evaluation reward: 478.0\n",
      "episode: 6249   score: 595.0  epsilon: 1.0    steps: 856  evaluation reward: 474.6\n",
      "Training network. lr: 0.000113. clip: 0.045174\n",
      "Iteration 17878: Policy loss: -0.073042. Value loss: 0.177062. Entropy: 0.302204.\n",
      "Iteration 17879: Policy loss: -0.078472. Value loss: 0.081680. Entropy: 0.302583.\n",
      "Iteration 17880: Policy loss: -0.080169. Value loss: 0.065747. Entropy: 0.303673.\n",
      "episode: 6250   score: 335.0  epsilon: 1.0    steps: 328  evaluation reward: 474.95\n",
      "now time :  2019-09-06 08:44:35.967421\n",
      "episode: 6251   score: 155.0  epsilon: 1.0    steps: 896  evaluation reward: 468.7\n",
      "episode: 6252   score: 640.0  epsilon: 1.0    steps: 936  evaluation reward: 470.65\n",
      "Training network. lr: 0.000113. clip: 0.045174\n",
      "Iteration 17881: Policy loss: 0.040565. Value loss: 0.094564. Entropy: 0.300710.\n",
      "Iteration 17882: Policy loss: 0.032309. Value loss: 0.048182. Entropy: 0.301163.\n",
      "Iteration 17883: Policy loss: 0.029698. Value loss: 0.038764. Entropy: 0.301290.\n",
      "episode: 6253   score: 470.0  epsilon: 1.0    steps: 104  evaluation reward: 471.35\n",
      "Training network. lr: 0.000113. clip: 0.045174\n",
      "Iteration 17884: Policy loss: 0.035321. Value loss: 0.137682. Entropy: 0.302077.\n",
      "Iteration 17885: Policy loss: 0.027595. Value loss: 0.062591. Entropy: 0.299253.\n",
      "Iteration 17886: Policy loss: 0.028541. Value loss: 0.046391. Entropy: 0.299275.\n",
      "episode: 6254   score: 640.0  epsilon: 1.0    steps: 496  evaluation reward: 472.35\n",
      "Training network. lr: 0.000113. clip: 0.045174\n",
      "Iteration 17887: Policy loss: 0.208189. Value loss: 0.151492. Entropy: 0.302007.\n",
      "Iteration 17888: Policy loss: 0.207226. Value loss: 0.083996. Entropy: 0.301388.\n",
      "Iteration 17889: Policy loss: 0.200512. Value loss: 0.070743. Entropy: 0.300767.\n",
      "Training network. lr: 0.000113. clip: 0.045174\n",
      "Iteration 17890: Policy loss: 0.233849. Value loss: 0.141543. Entropy: 0.298881.\n",
      "Iteration 17891: Policy loss: 0.231545. Value loss: 0.076709. Entropy: 0.297849.\n",
      "Iteration 17892: Policy loss: 0.226683. Value loss: 0.060465. Entropy: 0.299745.\n",
      "Training network. lr: 0.000113. clip: 0.045174\n",
      "Iteration 17893: Policy loss: 0.023938. Value loss: 0.317247. Entropy: 0.309485.\n",
      "Iteration 17894: Policy loss: 0.013418. Value loss: 0.131114. Entropy: 0.310054.\n",
      "Iteration 17895: Policy loss: 0.006461. Value loss: 0.067650. Entropy: 0.310327.\n",
      "Training network. lr: 0.000113. clip: 0.045174\n",
      "Iteration 17896: Policy loss: 0.125526. Value loss: 0.158644. Entropy: 0.302277.\n",
      "Iteration 17897: Policy loss: 0.115102. Value loss: 0.066437. Entropy: 0.304673.\n",
      "Iteration 17898: Policy loss: 0.115358. Value loss: 0.041093. Entropy: 0.302151.\n",
      "episode: 6255   score: 210.0  epsilon: 1.0    steps: 528  evaluation reward: 472.35\n",
      "episode: 6256   score: 600.0  epsilon: 1.0    steps: 584  evaluation reward: 476.25\n",
      "episode: 6257   score: 285.0  epsilon: 1.0    steps: 944  evaluation reward: 472.3\n",
      "Training network. lr: 0.000113. clip: 0.045174\n",
      "Iteration 17899: Policy loss: 0.226581. Value loss: 0.180333. Entropy: 0.308702.\n",
      "Iteration 17900: Policy loss: 0.226030. Value loss: 0.088545. Entropy: 0.308363.\n",
      "Iteration 17901: Policy loss: 0.223779. Value loss: 0.056275. Entropy: 0.308267.\n",
      "episode: 6258   score: 415.0  epsilon: 1.0    steps: 264  evaluation reward: 472.15\n",
      "Training network. lr: 0.000113. clip: 0.045017\n",
      "Iteration 17902: Policy loss: 0.070237. Value loss: 0.118234. Entropy: 0.312212.\n",
      "Iteration 17903: Policy loss: 0.067780. Value loss: 0.063035. Entropy: 0.311025.\n",
      "Iteration 17904: Policy loss: 0.068425. Value loss: 0.043336. Entropy: 0.311243.\n",
      "episode: 6259   score: 265.0  epsilon: 1.0    steps: 208  evaluation reward: 465.4\n",
      "episode: 6260   score: 260.0  epsilon: 1.0    steps: 288  evaluation reward: 462.75\n",
      "episode: 6261   score: 640.0  epsilon: 1.0    steps: 960  evaluation reward: 466.15\n",
      "Training network. lr: 0.000113. clip: 0.045017\n",
      "Iteration 17905: Policy loss: -0.153997. Value loss: 0.120310. Entropy: 0.298954.\n",
      "Iteration 17906: Policy loss: -0.156404. Value loss: 0.073695. Entropy: 0.298036.\n",
      "Iteration 17907: Policy loss: -0.162365. Value loss: 0.060397. Entropy: 0.298604.\n",
      "Training network. lr: 0.000113. clip: 0.045017\n",
      "Iteration 17908: Policy loss: -0.340720. Value loss: 0.249856. Entropy: 0.293015.\n",
      "Iteration 17909: Policy loss: -0.345272. Value loss: 0.105819. Entropy: 0.291162.\n",
      "Iteration 17910: Policy loss: -0.336916. Value loss: 0.063615. Entropy: 0.290998.\n",
      "Training network. lr: 0.000113. clip: 0.045017\n",
      "Iteration 17911: Policy loss: 0.129679. Value loss: 0.175327. Entropy: 0.305619.\n",
      "Iteration 17912: Policy loss: 0.114911. Value loss: 0.089218. Entropy: 0.303969.\n",
      "Iteration 17913: Policy loss: 0.099054. Value loss: 0.050113. Entropy: 0.304512.\n",
      "Training network. lr: 0.000113. clip: 0.045017\n",
      "Iteration 17914: Policy loss: -0.422955. Value loss: 0.403007. Entropy: 0.296330.\n",
      "Iteration 17915: Policy loss: -0.467552. Value loss: 0.199097. Entropy: 0.296579.\n",
      "Iteration 17916: Policy loss: -0.475397. Value loss: 0.136218. Entropy: 0.295113.\n",
      "Training network. lr: 0.000113. clip: 0.045017\n",
      "Iteration 17917: Policy loss: 0.243494. Value loss: 0.206359. Entropy: 0.309835.\n",
      "Iteration 17918: Policy loss: 0.255172. Value loss: 0.066225. Entropy: 0.308908.\n",
      "Iteration 17919: Policy loss: 0.240663. Value loss: 0.038546. Entropy: 0.308353.\n",
      "episode: 6262   score: 625.0  epsilon: 1.0    steps: 120  evaluation reward: 467.25\n",
      "Training network. lr: 0.000113. clip: 0.045017\n",
      "Iteration 17920: Policy loss: -0.454054. Value loss: 0.308471. Entropy: 0.312294.\n",
      "Iteration 17921: Policy loss: -0.464146. Value loss: 0.120313. Entropy: 0.312295.\n",
      "Iteration 17922: Policy loss: -0.468301. Value loss: 0.069014. Entropy: 0.312744.\n",
      "Training network. lr: 0.000113. clip: 0.045017\n",
      "Iteration 17923: Policy loss: -0.348831. Value loss: 0.359579. Entropy: 0.306380.\n",
      "Iteration 17924: Policy loss: -0.351075. Value loss: 0.127677. Entropy: 0.305852.\n",
      "Iteration 17925: Policy loss: -0.340995. Value loss: 0.071610. Entropy: 0.306255.\n",
      "episode: 6263   score: 625.0  epsilon: 1.0    steps: 32  evaluation reward: 469.4\n",
      "episode: 6264   score: 390.0  epsilon: 1.0    steps: 176  evaluation reward: 468.9\n",
      "episode: 6265   score: 515.0  epsilon: 1.0    steps: 336  evaluation reward: 470.75\n",
      "episode: 6266   score: 530.0  epsilon: 1.0    steps: 704  evaluation reward: 467.7\n",
      "Training network. lr: 0.000113. clip: 0.045017\n",
      "Iteration 17926: Policy loss: 0.028886. Value loss: 0.236410. Entropy: 0.295762.\n",
      "Iteration 17927: Policy loss: 0.026140. Value loss: 0.090609. Entropy: 0.298468.\n",
      "Iteration 17928: Policy loss: 0.018994. Value loss: 0.052151. Entropy: 0.297936.\n",
      "Training network. lr: 0.000113. clip: 0.045017\n",
      "Iteration 17929: Policy loss: 0.140377. Value loss: 0.170469. Entropy: 0.298502.\n",
      "Iteration 17930: Policy loss: 0.138110. Value loss: 0.061395. Entropy: 0.297098.\n",
      "Iteration 17931: Policy loss: 0.135121. Value loss: 0.043044. Entropy: 0.297272.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000113. clip: 0.045017\n",
      "Iteration 17932: Policy loss: -0.120568. Value loss: 0.400924. Entropy: 0.308029.\n",
      "Iteration 17933: Policy loss: -0.139337. Value loss: 0.162740. Entropy: 0.306812.\n",
      "Iteration 17934: Policy loss: -0.140550. Value loss: 0.089777. Entropy: 0.307224.\n",
      "episode: 6267   score: 670.0  epsilon: 1.0    steps: 184  evaluation reward: 471.0\n",
      "episode: 6268   score: 800.0  epsilon: 1.0    steps: 248  evaluation reward: 474.65\n",
      "Training network. lr: 0.000113. clip: 0.045017\n",
      "Iteration 17935: Policy loss: 0.335507. Value loss: 0.203803. Entropy: 0.301828.\n",
      "Iteration 17936: Policy loss: 0.341358. Value loss: 0.095394. Entropy: 0.298594.\n",
      "Iteration 17937: Policy loss: 0.345071. Value loss: 0.064595. Entropy: 0.298505.\n",
      "Training network. lr: 0.000113. clip: 0.045017\n",
      "Iteration 17938: Policy loss: 0.124627. Value loss: 0.165636. Entropy: 0.307223.\n",
      "Iteration 17939: Policy loss: 0.120344. Value loss: 0.066787. Entropy: 0.307529.\n",
      "Iteration 17940: Policy loss: 0.112756. Value loss: 0.044558. Entropy: 0.306824.\n",
      "episode: 6269   score: 320.0  epsilon: 1.0    steps: 520  evaluation reward: 474.6\n",
      "Training network. lr: 0.000113. clip: 0.045017\n",
      "Iteration 17941: Policy loss: -0.012532. Value loss: 0.178216. Entropy: 0.304520.\n",
      "Iteration 17942: Policy loss: -0.017745. Value loss: 0.060737. Entropy: 0.305102.\n",
      "Iteration 17943: Policy loss: -0.029664. Value loss: 0.035689. Entropy: 0.304359.\n",
      "episode: 6270   score: 575.0  epsilon: 1.0    steps: 752  evaluation reward: 477.15\n",
      "Training network. lr: 0.000113. clip: 0.045017\n",
      "Iteration 17944: Policy loss: 0.076053. Value loss: 0.432645. Entropy: 0.310113.\n",
      "Iteration 17945: Policy loss: 0.066544. Value loss: 0.161127. Entropy: 0.308386.\n",
      "Iteration 17946: Policy loss: 0.065876. Value loss: 0.087535. Entropy: 0.307791.\n",
      "episode: 6271   score: 240.0  epsilon: 1.0    steps: 1008  evaluation reward: 473.65\n",
      "Training network. lr: 0.000113. clip: 0.045017\n",
      "Iteration 17947: Policy loss: 0.166955. Value loss: 0.201345. Entropy: 0.308509.\n",
      "Iteration 17948: Policy loss: 0.162778. Value loss: 0.080403. Entropy: 0.308167.\n",
      "Iteration 17949: Policy loss: 0.156962. Value loss: 0.049733. Entropy: 0.307773.\n",
      "episode: 6272   score: 645.0  epsilon: 1.0    steps: 520  evaluation reward: 473.8\n",
      "episode: 6273   score: 395.0  epsilon: 1.0    steps: 608  evaluation reward: 472.2\n",
      "Training network. lr: 0.000113. clip: 0.045017\n",
      "Iteration 17950: Policy loss: 0.439337. Value loss: 0.229725. Entropy: 0.301511.\n",
      "Iteration 17951: Policy loss: 0.428496. Value loss: 0.075429. Entropy: 0.301611.\n",
      "Iteration 17952: Policy loss: 0.401913. Value loss: 0.049381. Entropy: 0.300260.\n",
      "Training network. lr: 0.000112. clip: 0.044861\n",
      "Iteration 17953: Policy loss: -0.182911. Value loss: 0.238949. Entropy: 0.313084.\n",
      "Iteration 17954: Policy loss: -0.186478. Value loss: 0.106684. Entropy: 0.312875.\n",
      "Iteration 17955: Policy loss: -0.195259. Value loss: 0.069950. Entropy: 0.312579.\n",
      "episode: 6274   score: 285.0  epsilon: 1.0    steps: 112  evaluation reward: 472.8\n",
      "episode: 6275   score: 400.0  epsilon: 1.0    steps: 920  evaluation reward: 472.3\n",
      "Training network. lr: 0.000112. clip: 0.044861\n",
      "Iteration 17956: Policy loss: -0.055250. Value loss: 0.138714. Entropy: 0.301579.\n",
      "Iteration 17957: Policy loss: -0.052763. Value loss: 0.065553. Entropy: 0.300858.\n",
      "Iteration 17958: Policy loss: -0.059874. Value loss: 0.048797. Entropy: 0.301537.\n",
      "Training network. lr: 0.000112. clip: 0.044861\n",
      "Iteration 17959: Policy loss: -0.254323. Value loss: 0.249293. Entropy: 0.301507.\n",
      "Iteration 17960: Policy loss: -0.265862. Value loss: 0.103070. Entropy: 0.301647.\n",
      "Iteration 17961: Policy loss: -0.265431. Value loss: 0.045238. Entropy: 0.301694.\n",
      "episode: 6276   score: 345.0  epsilon: 1.0    steps: 608  evaluation reward: 472.65\n",
      "Training network. lr: 0.000112. clip: 0.044861\n",
      "Iteration 17962: Policy loss: 0.265898. Value loss: 0.125383. Entropy: 0.302250.\n",
      "Iteration 17963: Policy loss: 0.263133. Value loss: 0.044561. Entropy: 0.302356.\n",
      "Iteration 17964: Policy loss: 0.248217. Value loss: 0.028970. Entropy: 0.301289.\n",
      "Training network. lr: 0.000112. clip: 0.044861\n",
      "Iteration 17965: Policy loss: -0.380797. Value loss: 0.289477. Entropy: 0.294572.\n",
      "Iteration 17966: Policy loss: -0.402036. Value loss: 0.117955. Entropy: 0.296530.\n",
      "Iteration 17967: Policy loss: -0.401768. Value loss: 0.070645. Entropy: 0.296181.\n",
      "episode: 6277   score: 265.0  epsilon: 1.0    steps: 120  evaluation reward: 469.55\n",
      "episode: 6278   score: 320.0  epsilon: 1.0    steps: 952  evaluation reward: 470.5\n",
      "Training network. lr: 0.000112. clip: 0.044861\n",
      "Iteration 17968: Policy loss: -0.080429. Value loss: 0.304550. Entropy: 0.311456.\n",
      "Iteration 17969: Policy loss: -0.072206. Value loss: 0.123518. Entropy: 0.309737.\n",
      "Iteration 17970: Policy loss: -0.099585. Value loss: 0.109685. Entropy: 0.310275.\n",
      "Training network. lr: 0.000112. clip: 0.044861\n",
      "Iteration 17971: Policy loss: -0.235198. Value loss: 0.182505. Entropy: 0.308543.\n",
      "Iteration 17972: Policy loss: -0.244224. Value loss: 0.072623. Entropy: 0.308786.\n",
      "Iteration 17973: Policy loss: -0.250539. Value loss: 0.052244. Entropy: 0.308861.\n",
      "episode: 6279   score: 1165.0  epsilon: 1.0    steps: 712  evaluation reward: 479.55\n",
      "episode: 6280   score: 645.0  epsilon: 1.0    steps: 1024  evaluation reward: 481.8\n",
      "Training network. lr: 0.000112. clip: 0.044861\n",
      "Iteration 17974: Policy loss: 0.008983. Value loss: 0.092910. Entropy: 0.300273.\n",
      "Iteration 17975: Policy loss: -0.001942. Value loss: 0.045576. Entropy: 0.300881.\n",
      "Iteration 17976: Policy loss: -0.000564. Value loss: 0.035866. Entropy: 0.301007.\n",
      "Training network. lr: 0.000112. clip: 0.044861\n",
      "Iteration 17977: Policy loss: 0.286875. Value loss: 0.152265. Entropy: 0.301129.\n",
      "Iteration 17978: Policy loss: 0.280220. Value loss: 0.049898. Entropy: 0.301118.\n",
      "Iteration 17979: Policy loss: 0.273411. Value loss: 0.032098. Entropy: 0.299774.\n",
      "episode: 6281   score: 660.0  epsilon: 1.0    steps: 584  evaluation reward: 482.4\n",
      "Training network. lr: 0.000112. clip: 0.044861\n",
      "Iteration 17980: Policy loss: 0.568593. Value loss: 0.288817. Entropy: 0.303435.\n",
      "Iteration 17981: Policy loss: 0.558806. Value loss: 0.075606. Entropy: 0.301452.\n",
      "Iteration 17982: Policy loss: 0.540041. Value loss: 0.044590. Entropy: 0.301971.\n",
      "Training network. lr: 0.000112. clip: 0.044861\n",
      "Iteration 17983: Policy loss: -0.139791. Value loss: 0.183181. Entropy: 0.296206.\n",
      "Iteration 17984: Policy loss: -0.160405. Value loss: 0.089916. Entropy: 0.294441.\n",
      "Iteration 17985: Policy loss: -0.167469. Value loss: 0.052534. Entropy: 0.294256.\n",
      "episode: 6282   score: 590.0  epsilon: 1.0    steps: 496  evaluation reward: 482.65\n",
      "Training network. lr: 0.000112. clip: 0.044861\n",
      "Iteration 17986: Policy loss: 0.447641. Value loss: 0.154170. Entropy: 0.294024.\n",
      "Iteration 17987: Policy loss: 0.440598. Value loss: 0.050183. Entropy: 0.295530.\n",
      "Iteration 17988: Policy loss: 0.433230. Value loss: 0.029268. Entropy: 0.294500.\n",
      "episode: 6283   score: 915.0  epsilon: 1.0    steps: 856  evaluation reward: 487.3\n",
      "Training network. lr: 0.000112. clip: 0.044861\n",
      "Iteration 17989: Policy loss: 0.290086. Value loss: 0.200535. Entropy: 0.306760.\n",
      "Iteration 17990: Policy loss: 0.277672. Value loss: 0.083720. Entropy: 0.306317.\n",
      "Iteration 17991: Policy loss: 0.274696. Value loss: 0.056723. Entropy: 0.304993.\n",
      "episode: 6284   score: 985.0  epsilon: 1.0    steps: 88  evaluation reward: 489.15\n",
      "episode: 6285   score: 255.0  epsilon: 1.0    steps: 920  evaluation reward: 487.05\n",
      "Training network. lr: 0.000112. clip: 0.044861\n",
      "Iteration 17992: Policy loss: -0.029089. Value loss: 0.107453. Entropy: 0.301285.\n",
      "Iteration 17993: Policy loss: -0.034317. Value loss: 0.053187. Entropy: 0.299276.\n",
      "Iteration 17994: Policy loss: -0.036100. Value loss: 0.039351. Entropy: 0.300761.\n",
      "Training network. lr: 0.000112. clip: 0.044861\n",
      "Iteration 17995: Policy loss: -0.137299. Value loss: 0.160269. Entropy: 0.305205.\n",
      "Iteration 17996: Policy loss: -0.141913. Value loss: 0.042517. Entropy: 0.304979.\n",
      "Iteration 17997: Policy loss: -0.146343. Value loss: 0.028608. Entropy: 0.305928.\n",
      "episode: 6286   score: 320.0  epsilon: 1.0    steps: 128  evaluation reward: 486.9\n",
      "episode: 6287   score: 420.0  epsilon: 1.0    steps: 736  evaluation reward: 484.95\n",
      "Training network. lr: 0.000112. clip: 0.044861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17998: Policy loss: -0.133107. Value loss: 0.224163. Entropy: 0.307419.\n",
      "Iteration 17999: Policy loss: -0.153831. Value loss: 0.096958. Entropy: 0.306268.\n",
      "Iteration 18000: Policy loss: -0.153215. Value loss: 0.064603. Entropy: 0.307300.\n",
      "episode: 6288   score: 460.0  epsilon: 1.0    steps: 840  evaluation reward: 484.3\n",
      "Training network. lr: 0.000112. clip: 0.044713\n",
      "Iteration 18001: Policy loss: 0.155994. Value loss: 0.300211. Entropy: 0.299260.\n",
      "Iteration 18002: Policy loss: 0.157626. Value loss: 0.091526. Entropy: 0.299280.\n",
      "Iteration 18003: Policy loss: 0.150845. Value loss: 0.050316. Entropy: 0.298622.\n",
      "Training network. lr: 0.000112. clip: 0.044713\n",
      "Iteration 18004: Policy loss: 0.029899. Value loss: 0.092407. Entropy: 0.309880.\n",
      "Iteration 18005: Policy loss: 0.024396. Value loss: 0.036708. Entropy: 0.309326.\n",
      "Iteration 18006: Policy loss: 0.021662. Value loss: 0.024606. Entropy: 0.309513.\n",
      "episode: 6289   score: 405.0  epsilon: 1.0    steps: 320  evaluation reward: 485.2\n",
      "Training network. lr: 0.000112. clip: 0.044713\n",
      "Iteration 18007: Policy loss: -0.208446. Value loss: 0.352969. Entropy: 0.306408.\n",
      "Iteration 18008: Policy loss: -0.209297. Value loss: 0.119918. Entropy: 0.307378.\n",
      "Iteration 18009: Policy loss: -0.218535. Value loss: 0.060804. Entropy: 0.307466.\n",
      "episode: 6290   score: 615.0  epsilon: 1.0    steps: 792  evaluation reward: 488.5\n",
      "Training network. lr: 0.000112. clip: 0.044713\n",
      "Iteration 18010: Policy loss: 0.378220. Value loss: 0.276401. Entropy: 0.308169.\n",
      "Iteration 18011: Policy loss: 0.369034. Value loss: 0.085994. Entropy: 0.304869.\n",
      "Iteration 18012: Policy loss: 0.360806. Value loss: 0.051457. Entropy: 0.304605.\n",
      "episode: 6291   score: 195.0  epsilon: 1.0    steps: 544  evaluation reward: 486.65\n",
      "episode: 6292   score: 405.0  epsilon: 1.0    steps: 952  evaluation reward: 488.9\n",
      "Training network. lr: 0.000112. clip: 0.044713\n",
      "Iteration 18013: Policy loss: 0.233616. Value loss: 0.209772. Entropy: 0.301511.\n",
      "Iteration 18014: Policy loss: 0.229747. Value loss: 0.091629. Entropy: 0.300876.\n",
      "Iteration 18015: Policy loss: 0.230749. Value loss: 0.066669. Entropy: 0.299849.\n",
      "Training network. lr: 0.000112. clip: 0.044713\n",
      "Iteration 18016: Policy loss: -0.394395. Value loss: 0.309607. Entropy: 0.302527.\n",
      "Iteration 18017: Policy loss: -0.413771. Value loss: 0.197458. Entropy: 0.302090.\n",
      "Iteration 18018: Policy loss: -0.418084. Value loss: 0.144002. Entropy: 0.301745.\n",
      "episode: 6293   score: 315.0  epsilon: 1.0    steps: 416  evaluation reward: 484.35\n",
      "episode: 6294   score: 895.0  epsilon: 1.0    steps: 856  evaluation reward: 488.8\n",
      "Training network. lr: 0.000112. clip: 0.044713\n",
      "Iteration 18019: Policy loss: 0.323034. Value loss: 0.155543. Entropy: 0.296364.\n",
      "Iteration 18020: Policy loss: 0.333342. Value loss: 0.047932. Entropy: 0.292683.\n",
      "Iteration 18021: Policy loss: 0.314729. Value loss: 0.035548. Entropy: 0.295628.\n",
      "episode: 6295   score: 285.0  epsilon: 1.0    steps: 680  evaluation reward: 482.4\n",
      "Training network. lr: 0.000112. clip: 0.044713\n",
      "Iteration 18022: Policy loss: 0.125552. Value loss: 0.112038. Entropy: 0.297131.\n",
      "Iteration 18023: Policy loss: 0.116120. Value loss: 0.045411. Entropy: 0.298477.\n",
      "Iteration 18024: Policy loss: 0.117113. Value loss: 0.031363. Entropy: 0.295139.\n",
      "Training network. lr: 0.000112. clip: 0.044713\n",
      "Iteration 18025: Policy loss: -0.326358. Value loss: 0.265566. Entropy: 0.306972.\n",
      "Iteration 18026: Policy loss: -0.328543. Value loss: 0.179110. Entropy: 0.307127.\n",
      "Iteration 18027: Policy loss: -0.323690. Value loss: 0.146126. Entropy: 0.309647.\n",
      "Training network. lr: 0.000112. clip: 0.044713\n",
      "Iteration 18028: Policy loss: -0.182170. Value loss: 0.142139. Entropy: 0.305334.\n",
      "Iteration 18029: Policy loss: -0.179595. Value loss: 0.048942. Entropy: 0.304587.\n",
      "Iteration 18030: Policy loss: -0.186465. Value loss: 0.031896. Entropy: 0.304925.\n",
      "Training network. lr: 0.000112. clip: 0.044713\n",
      "Iteration 18031: Policy loss: -0.005995. Value loss: 0.260523. Entropy: 0.300459.\n",
      "Iteration 18032: Policy loss: -0.016813. Value loss: 0.085804. Entropy: 0.296973.\n",
      "Iteration 18033: Policy loss: -0.026377. Value loss: 0.053127. Entropy: 0.297232.\n",
      "Training network. lr: 0.000112. clip: 0.044713\n",
      "Iteration 18034: Policy loss: -0.006794. Value loss: 0.237395. Entropy: 0.311415.\n",
      "Iteration 18035: Policy loss: -0.009662. Value loss: 0.148824. Entropy: 0.310490.\n",
      "Iteration 18036: Policy loss: -0.024586. Value loss: 0.123974. Entropy: 0.310102.\n",
      "Training network. lr: 0.000112. clip: 0.044713\n",
      "Iteration 18037: Policy loss: -0.157724. Value loss: 0.162197. Entropy: 0.304857.\n",
      "Iteration 18038: Policy loss: -0.162621. Value loss: 0.074190. Entropy: 0.305132.\n",
      "Iteration 18039: Policy loss: -0.165535. Value loss: 0.051604. Entropy: 0.304881.\n",
      "episode: 6296   score: 975.0  epsilon: 1.0    steps: 928  evaluation reward: 487.95\n",
      "Training network. lr: 0.000112. clip: 0.044713\n",
      "Iteration 18040: Policy loss: 0.363665. Value loss: 0.164520. Entropy: 0.304711.\n",
      "Iteration 18041: Policy loss: 0.350602. Value loss: 0.067967. Entropy: 0.306265.\n",
      "Iteration 18042: Policy loss: 0.334204. Value loss: 0.047693. Entropy: 0.305663.\n",
      "episode: 6297   score: 590.0  epsilon: 1.0    steps: 32  evaluation reward: 489.75\n",
      "episode: 6298   score: 695.0  epsilon: 1.0    steps: 120  evaluation reward: 490.4\n",
      "episode: 6299   score: 540.0  epsilon: 1.0    steps: 480  evaluation reward: 491.55\n",
      "episode: 6300   score: 395.0  epsilon: 1.0    steps: 544  evaluation reward: 484.9\n",
      "Training network. lr: 0.000112. clip: 0.044713\n",
      "Iteration 18043: Policy loss: 0.072026. Value loss: 0.116134. Entropy: 0.298006.\n",
      "Iteration 18044: Policy loss: 0.062052. Value loss: 0.059833. Entropy: 0.298708.\n",
      "Iteration 18045: Policy loss: 0.059660. Value loss: 0.043263. Entropy: 0.298624.\n",
      "now time :  2019-09-06 08:54:47.920370\n",
      "episode: 6301   score: 465.0  epsilon: 1.0    steps: 464  evaluation reward: 484.75\n",
      "Training network. lr: 0.000112. clip: 0.044713\n",
      "Iteration 18046: Policy loss: 0.141070. Value loss: 0.143742. Entropy: 0.307923.\n",
      "Iteration 18047: Policy loss: 0.138626. Value loss: 0.066961. Entropy: 0.308884.\n",
      "Iteration 18048: Policy loss: 0.136886. Value loss: 0.050396. Entropy: 0.307322.\n",
      "Training network. lr: 0.000112. clip: 0.044713\n",
      "Iteration 18049: Policy loss: 0.022422. Value loss: 0.092429. Entropy: 0.310289.\n",
      "Iteration 18050: Policy loss: 0.024268. Value loss: 0.039652. Entropy: 0.309052.\n",
      "Iteration 18051: Policy loss: 0.019564. Value loss: 0.024582. Entropy: 0.310017.\n",
      "Training network. lr: 0.000111. clip: 0.044557\n",
      "Iteration 18052: Policy loss: -0.020942. Value loss: 0.256621. Entropy: 0.297506.\n",
      "Iteration 18053: Policy loss: -0.023545. Value loss: 0.123517. Entropy: 0.297559.\n",
      "Iteration 18054: Policy loss: -0.020791. Value loss: 0.077830. Entropy: 0.295815.\n",
      "episode: 6302   score: 755.0  epsilon: 1.0    steps: 328  evaluation reward: 486.4\n",
      "episode: 6303   score: 600.0  epsilon: 1.0    steps: 344  evaluation reward: 486.65\n",
      "episode: 6304   score: 210.0  epsilon: 1.0    steps: 600  evaluation reward: 481.95\n",
      "Training network. lr: 0.000111. clip: 0.044557\n",
      "Iteration 18055: Policy loss: -0.006643. Value loss: 0.124561. Entropy: 0.284654.\n",
      "Iteration 18056: Policy loss: -0.010811. Value loss: 0.050148. Entropy: 0.282011.\n",
      "Iteration 18057: Policy loss: -0.015620. Value loss: 0.038506. Entropy: 0.284374.\n",
      "Training network. lr: 0.000111. clip: 0.044557\n",
      "Iteration 18058: Policy loss: 0.062254. Value loss: 0.110848. Entropy: 0.306308.\n",
      "Iteration 18059: Policy loss: 0.065597. Value loss: 0.049768. Entropy: 0.307708.\n",
      "Iteration 18060: Policy loss: 0.063283. Value loss: 0.036697. Entropy: 0.307569.\n",
      "Training network. lr: 0.000111. clip: 0.044557\n",
      "Iteration 18061: Policy loss: -0.056701. Value loss: 0.157265. Entropy: 0.313669.\n",
      "Iteration 18062: Policy loss: -0.060443. Value loss: 0.057193. Entropy: 0.314722.\n",
      "Iteration 18063: Policy loss: -0.065813. Value loss: 0.039385. Entropy: 0.313733.\n",
      "Training network. lr: 0.000111. clip: 0.044557\n",
      "Iteration 18064: Policy loss: -0.140018. Value loss: 0.305370. Entropy: 0.278273.\n",
      "Iteration 18065: Policy loss: -0.153703. Value loss: 0.128198. Entropy: 0.275838.\n",
      "Iteration 18066: Policy loss: -0.153574. Value loss: 0.076038. Entropy: 0.278359.\n",
      "Training network. lr: 0.000111. clip: 0.044557\n",
      "Iteration 18067: Policy loss: 0.349815. Value loss: 0.164714. Entropy: 0.313714.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18068: Policy loss: 0.334818. Value loss: 0.064163. Entropy: 0.312586.\n",
      "Iteration 18069: Policy loss: 0.328615. Value loss: 0.038414. Entropy: 0.312616.\n",
      "Training network. lr: 0.000111. clip: 0.044557\n",
      "Iteration 18070: Policy loss: 0.118440. Value loss: 0.134931. Entropy: 0.307247.\n",
      "Iteration 18071: Policy loss: 0.117642. Value loss: 0.055487. Entropy: 0.307857.\n",
      "Iteration 18072: Policy loss: 0.116029. Value loss: 0.037918. Entropy: 0.307562.\n",
      "episode: 6305   score: 535.0  epsilon: 1.0    steps: 368  evaluation reward: 480.35\n",
      "episode: 6306   score: 435.0  epsilon: 1.0    steps: 984  evaluation reward: 480.9\n",
      "Training network. lr: 0.000111. clip: 0.044557\n",
      "Iteration 18073: Policy loss: -0.498572. Value loss: 0.397192. Entropy: 0.294107.\n",
      "Iteration 18074: Policy loss: -0.506012. Value loss: 0.128527. Entropy: 0.293255.\n",
      "Iteration 18075: Policy loss: -0.499476. Value loss: 0.078942. Entropy: 0.294257.\n",
      "episode: 6307   score: 270.0  epsilon: 1.0    steps: 432  evaluation reward: 479.35\n",
      "episode: 6308   score: 900.0  epsilon: 1.0    steps: 544  evaluation reward: 483.65\n",
      "Training network. lr: 0.000111. clip: 0.044557\n",
      "Iteration 18076: Policy loss: -0.148636. Value loss: 0.175255. Entropy: 0.292646.\n",
      "Iteration 18077: Policy loss: -0.144413. Value loss: 0.067671. Entropy: 0.295035.\n",
      "Iteration 18078: Policy loss: -0.151178. Value loss: 0.051121. Entropy: 0.295444.\n",
      "episode: 6309   score: 670.0  epsilon: 1.0    steps: 40  evaluation reward: 486.85\n",
      "episode: 6310   score: 620.0  epsilon: 1.0    steps: 344  evaluation reward: 489.6\n",
      "episode: 6311   score: 595.0  epsilon: 1.0    steps: 1000  evaluation reward: 490.0\n",
      "Training network. lr: 0.000111. clip: 0.044557\n",
      "Iteration 18079: Policy loss: -0.015623. Value loss: 0.093670. Entropy: 0.303514.\n",
      "Iteration 18080: Policy loss: -0.021303. Value loss: 0.058519. Entropy: 0.304192.\n",
      "Iteration 18081: Policy loss: -0.022442. Value loss: 0.044499. Entropy: 0.302625.\n",
      "Training network. lr: 0.000111. clip: 0.044557\n",
      "Iteration 18082: Policy loss: 0.231551. Value loss: 0.123306. Entropy: 0.297894.\n",
      "Iteration 18083: Policy loss: 0.226027. Value loss: 0.062405. Entropy: 0.296062.\n",
      "Iteration 18084: Policy loss: 0.222126. Value loss: 0.042542. Entropy: 0.296064.\n",
      "episode: 6312   score: 610.0  epsilon: 1.0    steps: 648  evaluation reward: 492.4\n",
      "episode: 6313   score: 150.0  epsilon: 1.0    steps: 840  evaluation reward: 489.6\n",
      "Training network. lr: 0.000111. clip: 0.044557\n",
      "Iteration 18085: Policy loss: -0.393148. Value loss: 0.500813. Entropy: 0.285743.\n",
      "Iteration 18086: Policy loss: -0.377273. Value loss: 0.333119. Entropy: 0.287087.\n",
      "Iteration 18087: Policy loss: -0.404640. Value loss: 0.300539. Entropy: 0.287756.\n",
      "Training network. lr: 0.000111. clip: 0.044557\n",
      "Iteration 18088: Policy loss: 0.194893. Value loss: 0.111904. Entropy: 0.315121.\n",
      "Iteration 18089: Policy loss: 0.189687. Value loss: 0.047288. Entropy: 0.315986.\n",
      "Iteration 18090: Policy loss: 0.183767. Value loss: 0.030688. Entropy: 0.315840.\n",
      "episode: 6314   score: 190.0  epsilon: 1.0    steps: 664  evaluation reward: 485.15\n",
      "Training network. lr: 0.000111. clip: 0.044557\n",
      "Iteration 18091: Policy loss: 0.328401. Value loss: 0.155883. Entropy: 0.298172.\n",
      "Iteration 18092: Policy loss: 0.335482. Value loss: 0.077345. Entropy: 0.298017.\n",
      "Iteration 18093: Policy loss: 0.331946. Value loss: 0.052203. Entropy: 0.297944.\n",
      "episode: 6315   score: 180.0  epsilon: 1.0    steps: 152  evaluation reward: 482.7\n",
      "Training network. lr: 0.000111. clip: 0.044557\n",
      "Iteration 18094: Policy loss: 0.168456. Value loss: 0.113210. Entropy: 0.308433.\n",
      "Iteration 18095: Policy loss: 0.167393. Value loss: 0.051817. Entropy: 0.308275.\n",
      "Iteration 18096: Policy loss: 0.162069. Value loss: 0.034642. Entropy: 0.309072.\n",
      "episode: 6316   score: 500.0  epsilon: 1.0    steps: 56  evaluation reward: 481.2\n",
      "Training network. lr: 0.000111. clip: 0.044557\n",
      "Iteration 18097: Policy loss: -0.039551. Value loss: 0.141093. Entropy: 0.306119.\n",
      "Iteration 18098: Policy loss: -0.045570. Value loss: 0.065884. Entropy: 0.305978.\n",
      "Iteration 18099: Policy loss: -0.050418. Value loss: 0.043211. Entropy: 0.306055.\n",
      "episode: 6317   score: 180.0  epsilon: 1.0    steps: 16  evaluation reward: 480.1\n",
      "episode: 6318   score: 315.0  epsilon: 1.0    steps: 184  evaluation reward: 479.65\n",
      "episode: 6319   score: 365.0  epsilon: 1.0    steps: 496  evaluation reward: 478.8\n",
      "episode: 6320   score: 460.0  epsilon: 1.0    steps: 704  evaluation reward: 479.45\n",
      "Training network. lr: 0.000111. clip: 0.044557\n",
      "Iteration 18100: Policy loss: 0.031221. Value loss: 0.105350. Entropy: 0.293612.\n",
      "Iteration 18101: Policy loss: 0.033661. Value loss: 0.050431. Entropy: 0.293994.\n",
      "Iteration 18102: Policy loss: 0.030524. Value loss: 0.036684. Entropy: 0.294876.\n",
      "Training network. lr: 0.000111. clip: 0.044400\n",
      "Iteration 18103: Policy loss: -0.002875. Value loss: 0.119968. Entropy: 0.314258.\n",
      "Iteration 18104: Policy loss: -0.006736. Value loss: 0.055864. Entropy: 0.313622.\n",
      "Iteration 18105: Policy loss: -0.003024. Value loss: 0.038448. Entropy: 0.314381.\n",
      "Training network. lr: 0.000111. clip: 0.044400\n",
      "Iteration 18106: Policy loss: -0.109279. Value loss: 0.089138. Entropy: 0.311586.\n",
      "Iteration 18107: Policy loss: -0.113608. Value loss: 0.036293. Entropy: 0.311088.\n",
      "Iteration 18108: Policy loss: -0.115454. Value loss: 0.027131. Entropy: 0.311510.\n",
      "episode: 6321   score: 345.0  epsilon: 1.0    steps: 1016  evaluation reward: 477.5\n",
      "Training network. lr: 0.000111. clip: 0.044400\n",
      "Iteration 18109: Policy loss: -0.586027. Value loss: 0.360361. Entropy: 0.292107.\n",
      "Iteration 18110: Policy loss: -0.609116. Value loss: 0.109838. Entropy: 0.291303.\n",
      "Iteration 18111: Policy loss: -0.609570. Value loss: 0.076958. Entropy: 0.290324.\n",
      "Training network. lr: 0.000111. clip: 0.044400\n",
      "Iteration 18112: Policy loss: -0.094041. Value loss: 0.149826. Entropy: 0.315328.\n",
      "Iteration 18113: Policy loss: -0.094607. Value loss: 0.061209. Entropy: 0.314418.\n",
      "Iteration 18114: Policy loss: -0.106631. Value loss: 0.042106. Entropy: 0.314399.\n",
      "episode: 6322   score: 380.0  epsilon: 1.0    steps: 928  evaluation reward: 478.4\n",
      "episode: 6323   score: 365.0  epsilon: 1.0    steps: 928  evaluation reward: 477.5\n",
      "Training network. lr: 0.000111. clip: 0.044400\n",
      "Iteration 18115: Policy loss: 0.101808. Value loss: 0.151966. Entropy: 0.307067.\n",
      "Iteration 18116: Policy loss: 0.087785. Value loss: 0.057901. Entropy: 0.306936.\n",
      "Iteration 18117: Policy loss: 0.086648. Value loss: 0.040551. Entropy: 0.306868.\n",
      "episode: 6324   score: 315.0  epsilon: 1.0    steps: 904  evaluation reward: 478.05\n",
      "Training network. lr: 0.000111. clip: 0.044400\n",
      "Iteration 18118: Policy loss: 0.127310. Value loss: 0.131949. Entropy: 0.303548.\n",
      "Iteration 18119: Policy loss: 0.119658. Value loss: 0.052444. Entropy: 0.303167.\n",
      "Iteration 18120: Policy loss: 0.121834. Value loss: 0.035791. Entropy: 0.304123.\n",
      "Training network. lr: 0.000111. clip: 0.044400\n",
      "Iteration 18121: Policy loss: -0.118979. Value loss: 0.184611. Entropy: 0.303900.\n",
      "Iteration 18122: Policy loss: -0.118906. Value loss: 0.090345. Entropy: 0.305348.\n",
      "Iteration 18123: Policy loss: -0.124145. Value loss: 0.061919. Entropy: 0.304848.\n",
      "episode: 6325   score: 565.0  epsilon: 1.0    steps: 256  evaluation reward: 477.5\n",
      "Training network. lr: 0.000111. clip: 0.044400\n",
      "Iteration 18124: Policy loss: -0.147585. Value loss: 0.120423. Entropy: 0.305375.\n",
      "Iteration 18125: Policy loss: -0.144283. Value loss: 0.046308. Entropy: 0.306258.\n",
      "Iteration 18126: Policy loss: -0.147127. Value loss: 0.032224. Entropy: 0.306333.\n",
      "episode: 6326   score: 525.0  epsilon: 1.0    steps: 416  evaluation reward: 477.9\n",
      "Training network. lr: 0.000111. clip: 0.044400\n",
      "Iteration 18127: Policy loss: 0.340616. Value loss: 0.213943. Entropy: 0.292635.\n",
      "Iteration 18128: Policy loss: 0.335077. Value loss: 0.076360. Entropy: 0.291852.\n",
      "Iteration 18129: Policy loss: 0.330421. Value loss: 0.049022. Entropy: 0.292124.\n",
      "episode: 6327   score: 420.0  epsilon: 1.0    steps: 424  evaluation reward: 477.85\n",
      "episode: 6328   score: 315.0  epsilon: 1.0    steps: 512  evaluation reward: 476.65\n",
      "episode: 6329   score: 700.0  epsilon: 1.0    steps: 704  evaluation reward: 479.95\n",
      "Training network. lr: 0.000111. clip: 0.044400\n",
      "Iteration 18130: Policy loss: -0.190246. Value loss: 0.188625. Entropy: 0.296686.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18131: Policy loss: -0.195222. Value loss: 0.080999. Entropy: 0.297689.\n",
      "Iteration 18132: Policy loss: -0.206884. Value loss: 0.055591. Entropy: 0.298311.\n",
      "Training network. lr: 0.000111. clip: 0.044400\n",
      "Iteration 18133: Policy loss: 0.008203. Value loss: 0.105115. Entropy: 0.312219.\n",
      "Iteration 18134: Policy loss: 0.003199. Value loss: 0.053476. Entropy: 0.313305.\n",
      "Iteration 18135: Policy loss: 0.001068. Value loss: 0.041022. Entropy: 0.312674.\n",
      "episode: 6330   score: 300.0  epsilon: 1.0    steps: 616  evaluation reward: 478.75\n",
      "Training network. lr: 0.000111. clip: 0.044400\n",
      "Iteration 18136: Policy loss: 0.224614. Value loss: 0.138083. Entropy: 0.305801.\n",
      "Iteration 18137: Policy loss: 0.227217. Value loss: 0.049814. Entropy: 0.303929.\n",
      "Iteration 18138: Policy loss: 0.205636. Value loss: 0.035216. Entropy: 0.305114.\n",
      "episode: 6331   score: 435.0  epsilon: 1.0    steps: 176  evaluation reward: 480.05\n",
      "Training network. lr: 0.000111. clip: 0.044400\n",
      "Iteration 18139: Policy loss: -0.323132. Value loss: 0.331488. Entropy: 0.292004.\n",
      "Iteration 18140: Policy loss: -0.333684. Value loss: 0.202189. Entropy: 0.292352.\n",
      "Iteration 18141: Policy loss: -0.332705. Value loss: 0.164545. Entropy: 0.291898.\n",
      "Training network. lr: 0.000111. clip: 0.044400\n",
      "Iteration 18142: Policy loss: 0.183163. Value loss: 0.182586. Entropy: 0.310341.\n",
      "Iteration 18143: Policy loss: 0.175822. Value loss: 0.072966. Entropy: 0.308668.\n",
      "Iteration 18144: Policy loss: 0.161941. Value loss: 0.047402. Entropy: 0.309397.\n",
      "Training network. lr: 0.000111. clip: 0.044400\n",
      "Iteration 18145: Policy loss: -0.260449. Value loss: 0.318204. Entropy: 0.309956.\n",
      "Iteration 18146: Policy loss: -0.273402. Value loss: 0.200668. Entropy: 0.310673.\n",
      "Iteration 18147: Policy loss: -0.289889. Value loss: 0.130798. Entropy: 0.310289.\n",
      "episode: 6332   score: 605.0  epsilon: 1.0    steps: 352  evaluation reward: 482.2\n",
      "Training network. lr: 0.000111. clip: 0.044400\n",
      "Iteration 18148: Policy loss: -0.007991. Value loss: 0.159528. Entropy: 0.308428.\n",
      "Iteration 18149: Policy loss: -0.015564. Value loss: 0.062415. Entropy: 0.307483.\n",
      "Iteration 18150: Policy loss: -0.012142. Value loss: 0.048201. Entropy: 0.307367.\n",
      "episode: 6333   score: 530.0  epsilon: 1.0    steps: 120  evaluation reward: 484.55\n",
      "episode: 6334   score: 265.0  epsilon: 1.0    steps: 632  evaluation reward: 483.1\n",
      "episode: 6335   score: 410.0  epsilon: 1.0    steps: 920  evaluation reward: 480.5\n",
      "Training network. lr: 0.000111. clip: 0.044252\n",
      "Iteration 18151: Policy loss: 0.154361. Value loss: 0.185080. Entropy: 0.282671.\n",
      "Iteration 18152: Policy loss: 0.151958. Value loss: 0.073669. Entropy: 0.282338.\n",
      "Iteration 18153: Policy loss: 0.146477. Value loss: 0.047713. Entropy: 0.279847.\n",
      "episode: 6336   score: 560.0  epsilon: 1.0    steps: 272  evaluation reward: 477.75\n",
      "episode: 6337   score: 210.0  epsilon: 1.0    steps: 696  evaluation reward: 476.05\n",
      "Training network. lr: 0.000111. clip: 0.044252\n",
      "Iteration 18154: Policy loss: 0.035326. Value loss: 0.221965. Entropy: 0.280374.\n",
      "Iteration 18155: Policy loss: 0.024271. Value loss: 0.137925. Entropy: 0.279876.\n",
      "Iteration 18156: Policy loss: 0.042806. Value loss: 0.102368. Entropy: 0.281551.\n",
      "Training network. lr: 0.000111. clip: 0.044252\n",
      "Iteration 18157: Policy loss: -0.038772. Value loss: 0.126396. Entropy: 0.309778.\n",
      "Iteration 18158: Policy loss: -0.047601. Value loss: 0.060719. Entropy: 0.309771.\n",
      "Iteration 18159: Policy loss: -0.060449. Value loss: 0.048842. Entropy: 0.309812.\n",
      "episode: 6338   score: 535.0  epsilon: 1.0    steps: 160  evaluation reward: 476.45\n",
      "Training network. lr: 0.000111. clip: 0.044252\n",
      "Iteration 18160: Policy loss: -0.021205. Value loss: 0.263173. Entropy: 0.301433.\n",
      "Iteration 18161: Policy loss: -0.044338. Value loss: 0.134730. Entropy: 0.302833.\n",
      "Iteration 18162: Policy loss: -0.041911. Value loss: 0.118193. Entropy: 0.302976.\n",
      "Training network. lr: 0.000111. clip: 0.044252\n",
      "Iteration 18163: Policy loss: -0.263739. Value loss: 0.322004. Entropy: 0.309987.\n",
      "Iteration 18164: Policy loss: -0.257762. Value loss: 0.094146. Entropy: 0.309101.\n",
      "Iteration 18165: Policy loss: -0.275357. Value loss: 0.045060. Entropy: 0.308637.\n",
      "episode: 6339   score: 760.0  epsilon: 1.0    steps: 264  evaluation reward: 480.4\n",
      "Training network. lr: 0.000111. clip: 0.044252\n",
      "Iteration 18166: Policy loss: -0.425326. Value loss: 0.351203. Entropy: 0.294314.\n",
      "Iteration 18167: Policy loss: -0.426556. Value loss: 0.193685. Entropy: 0.293275.\n",
      "Iteration 18168: Policy loss: -0.450838. Value loss: 0.117834. Entropy: 0.294105.\n",
      "Training network. lr: 0.000111. clip: 0.044252\n",
      "Iteration 18169: Policy loss: 0.092852. Value loss: 0.104263. Entropy: 0.308203.\n",
      "Iteration 18170: Policy loss: 0.079163. Value loss: 0.048833. Entropy: 0.307628.\n",
      "Iteration 18171: Policy loss: 0.087149. Value loss: 0.035800. Entropy: 0.306608.\n",
      "episode: 6340   score: 345.0  epsilon: 1.0    steps: 608  evaluation reward: 477.75\n",
      "episode: 6341   score: 330.0  epsilon: 1.0    steps: 848  evaluation reward: 475.9\n",
      "Training network. lr: 0.000111. clip: 0.044252\n",
      "Iteration 18172: Policy loss: 0.281522. Value loss: 0.096308. Entropy: 0.294648.\n",
      "Iteration 18173: Policy loss: 0.276387. Value loss: 0.045714. Entropy: 0.296121.\n",
      "Iteration 18174: Policy loss: 0.271839. Value loss: 0.032580. Entropy: 0.296239.\n",
      "Training network. lr: 0.000111. clip: 0.044252\n",
      "Iteration 18175: Policy loss: -0.254105. Value loss: 0.302645. Entropy: 0.307349.\n",
      "Iteration 18176: Policy loss: -0.262262. Value loss: 0.195611. Entropy: 0.309651.\n",
      "Iteration 18177: Policy loss: -0.268608. Value loss: 0.163256. Entropy: 0.309496.\n",
      "Training network. lr: 0.000111. clip: 0.044252\n",
      "Iteration 18178: Policy loss: 0.339167. Value loss: 0.166316. Entropy: 0.315816.\n",
      "Iteration 18179: Policy loss: 0.334553. Value loss: 0.060882. Entropy: 0.315411.\n",
      "Iteration 18180: Policy loss: 0.330644. Value loss: 0.038316. Entropy: 0.314680.\n",
      "episode: 6342   score: 420.0  epsilon: 1.0    steps: 160  evaluation reward: 474.1\n",
      "episode: 6343   score: 305.0  epsilon: 1.0    steps: 208  evaluation reward: 473.8\n",
      "episode: 6344   score: 700.0  epsilon: 1.0    steps: 368  evaluation reward: 477.4\n",
      "Training network. lr: 0.000111. clip: 0.044252\n",
      "Iteration 18181: Policy loss: 0.454413. Value loss: 0.133697. Entropy: 0.296509.\n",
      "Iteration 18182: Policy loss: 0.442356. Value loss: 0.042755. Entropy: 0.295365.\n",
      "Iteration 18183: Policy loss: 0.444674. Value loss: 0.031456. Entropy: 0.294727.\n",
      "episode: 6345   score: 355.0  epsilon: 1.0    steps: 56  evaluation reward: 475.0\n",
      "episode: 6346   score: 240.0  epsilon: 1.0    steps: 120  evaluation reward: 473.5\n",
      "Training network. lr: 0.000111. clip: 0.044252\n",
      "Iteration 18184: Policy loss: -0.260966. Value loss: 0.292353. Entropy: 0.300823.\n",
      "Iteration 18185: Policy loss: -0.277955. Value loss: 0.206516. Entropy: 0.302189.\n",
      "Iteration 18186: Policy loss: -0.282109. Value loss: 0.171260. Entropy: 0.301705.\n",
      "Training network. lr: 0.000111. clip: 0.044252\n",
      "Iteration 18187: Policy loss: 0.431548. Value loss: 0.136000. Entropy: 0.303889.\n",
      "Iteration 18188: Policy loss: 0.420938. Value loss: 0.052678. Entropy: 0.304388.\n",
      "Iteration 18189: Policy loss: 0.411671. Value loss: 0.041210. Entropy: 0.305426.\n",
      "Training network. lr: 0.000111. clip: 0.044252\n",
      "Iteration 18190: Policy loss: 0.341963. Value loss: 0.200376. Entropy: 0.314827.\n",
      "Iteration 18191: Policy loss: 0.339698. Value loss: 0.088828. Entropy: 0.314668.\n",
      "Iteration 18192: Policy loss: 0.336141. Value loss: 0.063604. Entropy: 0.315121.\n",
      "episode: 6347   score: 260.0  epsilon: 1.0    steps: 152  evaluation reward: 471.3\n",
      "episode: 6348   score: 100.0  epsilon: 1.0    steps: 512  evaluation reward: 467.8\n",
      "episode: 6349   score: 290.0  epsilon: 1.0    steps: 552  evaluation reward: 464.75\n",
      "Training network. lr: 0.000111. clip: 0.044252\n",
      "Iteration 18193: Policy loss: -0.009026. Value loss: 0.132981. Entropy: 0.289676.\n",
      "Iteration 18194: Policy loss: -0.017457. Value loss: 0.051217. Entropy: 0.288659.\n",
      "Iteration 18195: Policy loss: -0.017758. Value loss: 0.035546. Entropy: 0.288361.\n",
      "episode: 6350   score: 890.0  epsilon: 1.0    steps: 520  evaluation reward: 470.3\n",
      "Training network. lr: 0.000111. clip: 0.044252\n",
      "Iteration 18196: Policy loss: -0.057104. Value loss: 0.188168. Entropy: 0.303008.\n",
      "Iteration 18197: Policy loss: -0.060109. Value loss: 0.081603. Entropy: 0.303044.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18198: Policy loss: -0.065207. Value loss: 0.056286. Entropy: 0.303252.\n",
      "now time :  2019-09-06 09:04:06.840218\n",
      "episode: 6351   score: 225.0  epsilon: 1.0    steps: 552  evaluation reward: 471.0\n",
      "Training network. lr: 0.000111. clip: 0.044252\n",
      "Iteration 18199: Policy loss: 0.161728. Value loss: 0.145959. Entropy: 0.309250.\n",
      "Iteration 18200: Policy loss: 0.156092. Value loss: 0.070710. Entropy: 0.307618.\n",
      "Iteration 18201: Policy loss: 0.160566. Value loss: 0.043967. Entropy: 0.308209.\n",
      "episode: 6352   score: 315.0  epsilon: 1.0    steps: 792  evaluation reward: 467.75\n",
      "Training network. lr: 0.000110. clip: 0.044096\n",
      "Iteration 18202: Policy loss: 0.170465. Value loss: 0.177205. Entropy: 0.304612.\n",
      "Iteration 18203: Policy loss: 0.168485. Value loss: 0.081755. Entropy: 0.302873.\n",
      "Iteration 18204: Policy loss: 0.159629. Value loss: 0.054273. Entropy: 0.303490.\n",
      "Training network. lr: 0.000110. clip: 0.044096\n",
      "Iteration 18205: Policy loss: -0.118907. Value loss: 0.093903. Entropy: 0.308314.\n",
      "Iteration 18206: Policy loss: -0.122854. Value loss: 0.043400. Entropy: 0.309997.\n",
      "Iteration 18207: Policy loss: -0.125488. Value loss: 0.030092. Entropy: 0.309536.\n",
      "episode: 6353   score: 210.0  epsilon: 1.0    steps: 160  evaluation reward: 465.15\n",
      "episode: 6354   score: 390.0  epsilon: 1.0    steps: 488  evaluation reward: 462.65\n",
      "episode: 6355   score: 210.0  epsilon: 1.0    steps: 880  evaluation reward: 462.65\n",
      "Training network. lr: 0.000110. clip: 0.044096\n",
      "Iteration 18208: Policy loss: -0.036755. Value loss: 0.091459. Entropy: 0.293236.\n",
      "Iteration 18209: Policy loss: -0.041535. Value loss: 0.045454. Entropy: 0.292232.\n",
      "Iteration 18210: Policy loss: -0.041316. Value loss: 0.031963. Entropy: 0.291880.\n",
      "episode: 6356   score: 365.0  epsilon: 1.0    steps: 448  evaluation reward: 460.3\n",
      "Training network. lr: 0.000110. clip: 0.044096\n",
      "Iteration 18211: Policy loss: -0.178309. Value loss: 0.116222. Entropy: 0.297410.\n",
      "Iteration 18212: Policy loss: -0.192355. Value loss: 0.052523. Entropy: 0.297650.\n",
      "Iteration 18213: Policy loss: -0.196948. Value loss: 0.036647. Entropy: 0.297074.\n",
      "Training network. lr: 0.000110. clip: 0.044096\n",
      "Iteration 18214: Policy loss: -0.054503. Value loss: 0.148495. Entropy: 0.311343.\n",
      "Iteration 18215: Policy loss: -0.058935. Value loss: 0.057254. Entropy: 0.311969.\n",
      "Iteration 18216: Policy loss: -0.079878. Value loss: 0.042313. Entropy: 0.311878.\n",
      "episode: 6357   score: 360.0  epsilon: 1.0    steps: 480  evaluation reward: 461.05\n",
      "Training network. lr: 0.000110. clip: 0.044096\n",
      "Iteration 18217: Policy loss: -0.439661. Value loss: 0.278736. Entropy: 0.300670.\n",
      "Iteration 18218: Policy loss: -0.416619. Value loss: 0.121413. Entropy: 0.298077.\n",
      "Iteration 18219: Policy loss: -0.444222. Value loss: 0.086280. Entropy: 0.300634.\n",
      "episode: 6358   score: 595.0  epsilon: 1.0    steps: 400  evaluation reward: 462.85\n",
      "Training network. lr: 0.000110. clip: 0.044096\n",
      "Iteration 18220: Policy loss: 0.084291. Value loss: 0.104698. Entropy: 0.303343.\n",
      "Iteration 18221: Policy loss: 0.090474. Value loss: 0.034864. Entropy: 0.302848.\n",
      "Iteration 18222: Policy loss: 0.080931. Value loss: 0.027692. Entropy: 0.302512.\n",
      "episode: 6359   score: 385.0  epsilon: 1.0    steps: 376  evaluation reward: 464.05\n",
      "Training network. lr: 0.000110. clip: 0.044096\n",
      "Iteration 18223: Policy loss: 0.215218. Value loss: 0.140105. Entropy: 0.299074.\n",
      "Iteration 18224: Policy loss: 0.204256. Value loss: 0.058490. Entropy: 0.298477.\n",
      "Iteration 18225: Policy loss: 0.196410. Value loss: 0.035755. Entropy: 0.299234.\n",
      "episode: 6360   score: 360.0  epsilon: 1.0    steps: 184  evaluation reward: 465.05\n",
      "Training network. lr: 0.000110. clip: 0.044096\n",
      "Iteration 18226: Policy loss: 0.234172. Value loss: 0.117457. Entropy: 0.304661.\n",
      "Iteration 18227: Policy loss: 0.231061. Value loss: 0.043042. Entropy: 0.303312.\n",
      "Iteration 18228: Policy loss: 0.223891. Value loss: 0.029542. Entropy: 0.302961.\n",
      "episode: 6361   score: 300.0  epsilon: 1.0    steps: 928  evaluation reward: 461.65\n",
      "Training network. lr: 0.000110. clip: 0.044096\n",
      "Iteration 18229: Policy loss: 0.237782. Value loss: 0.158180. Entropy: 0.308240.\n",
      "Iteration 18230: Policy loss: 0.233229. Value loss: 0.065828. Entropy: 0.308206.\n",
      "Iteration 18231: Policy loss: 0.242361. Value loss: 0.043702. Entropy: 0.308594.\n",
      "episode: 6362   score: 180.0  epsilon: 1.0    steps: 472  evaluation reward: 457.2\n",
      "episode: 6363   score: 365.0  epsilon: 1.0    steps: 648  evaluation reward: 454.6\n",
      "episode: 6364   score: 335.0  epsilon: 1.0    steps: 864  evaluation reward: 454.05\n",
      "Training network. lr: 0.000110. clip: 0.044096\n",
      "Iteration 18232: Policy loss: 0.121914. Value loss: 0.062831. Entropy: 0.284570.\n",
      "Iteration 18233: Policy loss: 0.120787. Value loss: 0.033255. Entropy: 0.286238.\n",
      "Iteration 18234: Policy loss: 0.120739. Value loss: 0.026433. Entropy: 0.286206.\n",
      "Training network. lr: 0.000110. clip: 0.044096\n",
      "Iteration 18235: Policy loss: -0.136253. Value loss: 0.080246. Entropy: 0.307751.\n",
      "Iteration 18236: Policy loss: -0.134590. Value loss: 0.048261. Entropy: 0.307245.\n",
      "Iteration 18237: Policy loss: -0.141396. Value loss: 0.038172. Entropy: 0.307506.\n",
      "episode: 6365   score: 420.0  epsilon: 1.0    steps: 712  evaluation reward: 453.1\n",
      "Training network. lr: 0.000110. clip: 0.044096\n",
      "Iteration 18238: Policy loss: -0.128447. Value loss: 0.128231. Entropy: 0.303851.\n",
      "Iteration 18239: Policy loss: -0.132742. Value loss: 0.057769. Entropy: 0.303596.\n",
      "Iteration 18240: Policy loss: -0.133958. Value loss: 0.040731. Entropy: 0.303447.\n",
      "Training network. lr: 0.000110. clip: 0.044096\n",
      "Iteration 18241: Policy loss: -0.103459. Value loss: 0.129517. Entropy: 0.301069.\n",
      "Iteration 18242: Policy loss: -0.105606. Value loss: 0.049331. Entropy: 0.301157.\n",
      "Iteration 18243: Policy loss: -0.106998. Value loss: 0.034356. Entropy: 0.301052.\n",
      "episode: 6366   score: 215.0  epsilon: 1.0    steps: 512  evaluation reward: 449.95\n",
      "Training network. lr: 0.000110. clip: 0.044096\n",
      "Iteration 18244: Policy loss: -0.176476. Value loss: 0.183160. Entropy: 0.300686.\n",
      "Iteration 18245: Policy loss: -0.204385. Value loss: 0.134725. Entropy: 0.300753.\n",
      "Iteration 18246: Policy loss: -0.206261. Value loss: 0.119070. Entropy: 0.299848.\n",
      "episode: 6367   score: 465.0  epsilon: 1.0    steps: 592  evaluation reward: 447.9\n",
      "Training network. lr: 0.000110. clip: 0.044096\n",
      "Iteration 18247: Policy loss: -0.198810. Value loss: 0.310317. Entropy: 0.297834.\n",
      "Iteration 18248: Policy loss: -0.211859. Value loss: 0.183448. Entropy: 0.298589.\n",
      "Iteration 18249: Policy loss: -0.237250. Value loss: 0.115525. Entropy: 0.297367.\n",
      "Training network. lr: 0.000110. clip: 0.044096\n",
      "Iteration 18250: Policy loss: -0.128550. Value loss: 0.207107. Entropy: 0.312263.\n",
      "Iteration 18251: Policy loss: -0.129444. Value loss: 0.085969. Entropy: 0.310661.\n",
      "Iteration 18252: Policy loss: -0.140979. Value loss: 0.052053. Entropy: 0.311262.\n",
      "episode: 6368   score: 375.0  epsilon: 1.0    steps: 216  evaluation reward: 443.65\n",
      "episode: 6369   score: 510.0  epsilon: 1.0    steps: 960  evaluation reward: 445.55\n",
      "episode: 6370   score: 375.0  epsilon: 1.0    steps: 1016  evaluation reward: 443.55\n",
      "Training network. lr: 0.000110. clip: 0.043939\n",
      "Iteration 18253: Policy loss: -0.100970. Value loss: 0.416343. Entropy: 0.294536.\n",
      "Iteration 18254: Policy loss: -0.138972. Value loss: 0.302037. Entropy: 0.293569.\n",
      "Iteration 18255: Policy loss: -0.153599. Value loss: 0.255474. Entropy: 0.293082.\n",
      "episode: 6371   score: 215.0  epsilon: 1.0    steps: 272  evaluation reward: 443.3\n",
      "Training network. lr: 0.000110. clip: 0.043939\n",
      "Iteration 18256: Policy loss: -0.149175. Value loss: 0.281473. Entropy: 0.283558.\n",
      "Iteration 18257: Policy loss: -0.179946. Value loss: 0.227131. Entropy: 0.283359.\n",
      "Iteration 18258: Policy loss: -0.205485. Value loss: 0.214532. Entropy: 0.284536.\n",
      "Training network. lr: 0.000110. clip: 0.043939\n",
      "Iteration 18259: Policy loss: -0.007588. Value loss: 0.122043. Entropy: 0.309643.\n",
      "Iteration 18260: Policy loss: -0.005761. Value loss: 0.055904. Entropy: 0.309696.\n",
      "Iteration 18261: Policy loss: -0.008169. Value loss: 0.037881. Entropy: 0.309315.\n",
      "Training network. lr: 0.000110. clip: 0.043939\n",
      "Iteration 18262: Policy loss: 0.108695. Value loss: 0.113707. Entropy: 0.310783.\n",
      "Iteration 18263: Policy loss: 0.098229. Value loss: 0.052835. Entropy: 0.311545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18264: Policy loss: 0.096104. Value loss: 0.034754. Entropy: 0.311518.\n",
      "episode: 6372   score: 725.0  epsilon: 1.0    steps: 512  evaluation reward: 444.1\n",
      "episode: 6373   score: 655.0  epsilon: 1.0    steps: 600  evaluation reward: 446.7\n",
      "Training network. lr: 0.000110. clip: 0.043939\n",
      "Iteration 18265: Policy loss: 0.120130. Value loss: 0.088899. Entropy: 0.288004.\n",
      "Iteration 18266: Policy loss: 0.108188. Value loss: 0.032817. Entropy: 0.287725.\n",
      "Iteration 18267: Policy loss: 0.104763. Value loss: 0.022122. Entropy: 0.288558.\n",
      "episode: 6374   score: 180.0  epsilon: 1.0    steps: 696  evaluation reward: 445.65\n",
      "Training network. lr: 0.000110. clip: 0.043939\n",
      "Iteration 18268: Policy loss: 0.087132. Value loss: 0.135917. Entropy: 0.296937.\n",
      "Iteration 18269: Policy loss: 0.084254. Value loss: 0.060566. Entropy: 0.297663.\n",
      "Iteration 18270: Policy loss: 0.084394. Value loss: 0.042580. Entropy: 0.297901.\n",
      "episode: 6375   score: 305.0  epsilon: 1.0    steps: 224  evaluation reward: 444.7\n",
      "Training network. lr: 0.000110. clip: 0.043939\n",
      "Iteration 18271: Policy loss: -0.119195. Value loss: 0.137620. Entropy: 0.301995.\n",
      "Iteration 18272: Policy loss: -0.120461. Value loss: 0.058151. Entropy: 0.302728.\n",
      "Iteration 18273: Policy loss: -0.122678. Value loss: 0.042347. Entropy: 0.301965.\n",
      "Training network. lr: 0.000110. clip: 0.043939\n",
      "Iteration 18274: Policy loss: -0.239191. Value loss: 0.391000. Entropy: 0.307010.\n",
      "Iteration 18275: Policy loss: -0.266637. Value loss: 0.112799. Entropy: 0.306418.\n",
      "Iteration 18276: Policy loss: -0.266104. Value loss: 0.073042. Entropy: 0.307535.\n",
      "episode: 6376   score: 390.0  epsilon: 1.0    steps: 696  evaluation reward: 445.15\n",
      "episode: 6377   score: 620.0  epsilon: 1.0    steps: 968  evaluation reward: 448.7\n",
      "Training network. lr: 0.000110. clip: 0.043939\n",
      "Iteration 18277: Policy loss: 0.021551. Value loss: 0.094715. Entropy: 0.303850.\n",
      "Iteration 18278: Policy loss: 0.021903. Value loss: 0.051442. Entropy: 0.304280.\n",
      "Iteration 18279: Policy loss: 0.017400. Value loss: 0.037772. Entropy: 0.304459.\n",
      "Training network. lr: 0.000110. clip: 0.043939\n",
      "Iteration 18280: Policy loss: 0.031032. Value loss: 0.074818. Entropy: 0.311431.\n",
      "Iteration 18281: Policy loss: 0.028055. Value loss: 0.030639. Entropy: 0.311003.\n",
      "Iteration 18282: Policy loss: 0.029012. Value loss: 0.019166. Entropy: 0.310340.\n",
      "episode: 6378   score: 440.0  epsilon: 1.0    steps: 696  evaluation reward: 449.9\n",
      "Training network. lr: 0.000110. clip: 0.043939\n",
      "Iteration 18283: Policy loss: 0.002121. Value loss: 0.132163. Entropy: 0.302585.\n",
      "Iteration 18284: Policy loss: 0.001208. Value loss: 0.062607. Entropy: 0.303112.\n",
      "Iteration 18285: Policy loss: -0.007915. Value loss: 0.044324. Entropy: 0.304260.\n",
      "Training network. lr: 0.000110. clip: 0.043939\n",
      "Iteration 18286: Policy loss: -0.825788. Value loss: 0.518004. Entropy: 0.308073.\n",
      "Iteration 18287: Policy loss: -0.839259. Value loss: 0.318110. Entropy: 0.310057.\n",
      "Iteration 18288: Policy loss: -0.865978. Value loss: 0.175221. Entropy: 0.308365.\n",
      "Training network. lr: 0.000110. clip: 0.043939\n",
      "Iteration 18289: Policy loss: -0.402894. Value loss: 0.419733. Entropy: 0.309515.\n",
      "Iteration 18290: Policy loss: -0.407466. Value loss: 0.150436. Entropy: 0.308299.\n",
      "Iteration 18291: Policy loss: -0.429021. Value loss: 0.082468. Entropy: 0.311072.\n",
      "episode: 6379   score: 525.0  epsilon: 1.0    steps: 48  evaluation reward: 443.5\n",
      "Training network. lr: 0.000110. clip: 0.043939\n",
      "Iteration 18292: Policy loss: 0.305788. Value loss: 0.155084. Entropy: 0.298963.\n",
      "Iteration 18293: Policy loss: 0.293128. Value loss: 0.044670. Entropy: 0.300385.\n",
      "Iteration 18294: Policy loss: 0.292420. Value loss: 0.026173. Entropy: 0.299411.\n",
      "episode: 6380   score: 605.0  epsilon: 1.0    steps: 256  evaluation reward: 443.1\n",
      "episode: 6381   score: 600.0  epsilon: 1.0    steps: 816  evaluation reward: 442.5\n",
      "Training network. lr: 0.000110. clip: 0.043939\n",
      "Iteration 18295: Policy loss: -0.049940. Value loss: 0.183043. Entropy: 0.298364.\n",
      "Iteration 18296: Policy loss: -0.057043. Value loss: 0.073582. Entropy: 0.298255.\n",
      "Iteration 18297: Policy loss: -0.057398. Value loss: 0.053028. Entropy: 0.297782.\n",
      "episode: 6382   score: 470.0  epsilon: 1.0    steps: 368  evaluation reward: 441.3\n",
      "Training network. lr: 0.000110. clip: 0.043939\n",
      "Iteration 18298: Policy loss: 0.144966. Value loss: 0.123605. Entropy: 0.295997.\n",
      "Iteration 18299: Policy loss: 0.147871. Value loss: 0.068672. Entropy: 0.297483.\n",
      "Iteration 18300: Policy loss: 0.145507. Value loss: 0.057983. Entropy: 0.297492.\n",
      "episode: 6383   score: 365.0  epsilon: 1.0    steps: 680  evaluation reward: 435.8\n",
      "Training network. lr: 0.000109. clip: 0.043792\n",
      "Iteration 18301: Policy loss: -0.303998. Value loss: 0.335981. Entropy: 0.298229.\n",
      "Iteration 18302: Policy loss: -0.283051. Value loss: 0.206256. Entropy: 0.298166.\n",
      "Iteration 18303: Policy loss: -0.325046. Value loss: 0.180946. Entropy: 0.298515.\n",
      "Training network. lr: 0.000109. clip: 0.043792\n",
      "Iteration 18304: Policy loss: 0.040003. Value loss: 0.258718. Entropy: 0.300125.\n",
      "Iteration 18305: Policy loss: 0.030831. Value loss: 0.094196. Entropy: 0.301126.\n",
      "Iteration 18306: Policy loss: 0.016870. Value loss: 0.055232. Entropy: 0.299829.\n",
      "Training network. lr: 0.000109. clip: 0.043792\n",
      "Iteration 18307: Policy loss: 0.101563. Value loss: 0.243324. Entropy: 0.305445.\n",
      "Iteration 18308: Policy loss: 0.096633. Value loss: 0.091143. Entropy: 0.306163.\n",
      "Iteration 18309: Policy loss: 0.121048. Value loss: 0.051717. Entropy: 0.306046.\n",
      "episode: 6384   score: 880.0  epsilon: 1.0    steps: 32  evaluation reward: 434.75\n",
      "episode: 6385   score: 365.0  epsilon: 1.0    steps: 192  evaluation reward: 435.85\n",
      "Training network. lr: 0.000109. clip: 0.043792\n",
      "Iteration 18310: Policy loss: -0.152914. Value loss: 0.143989. Entropy: 0.297616.\n",
      "Iteration 18311: Policy loss: -0.156644. Value loss: 0.063038. Entropy: 0.297795.\n",
      "Iteration 18312: Policy loss: -0.169664. Value loss: 0.047364. Entropy: 0.297178.\n",
      "episode: 6386   score: 825.0  epsilon: 1.0    steps: 248  evaluation reward: 440.9\n",
      "Training network. lr: 0.000109. clip: 0.043792\n",
      "Iteration 18313: Policy loss: 0.442435. Value loss: 0.306382. Entropy: 0.303526.\n",
      "Iteration 18314: Policy loss: 0.429245. Value loss: 0.108852. Entropy: 0.301705.\n",
      "Iteration 18315: Policy loss: 0.422766. Value loss: 0.066747. Entropy: 0.302107.\n",
      "episode: 6387   score: 225.0  epsilon: 1.0    steps: 208  evaluation reward: 438.95\n",
      "Training network. lr: 0.000109. clip: 0.043792\n",
      "Iteration 18316: Policy loss: 0.123102. Value loss: 0.147081. Entropy: 0.304811.\n",
      "Iteration 18317: Policy loss: 0.128533. Value loss: 0.056007. Entropy: 0.304510.\n",
      "Iteration 18318: Policy loss: 0.123260. Value loss: 0.036459. Entropy: 0.303748.\n",
      "episode: 6388   score: 315.0  epsilon: 1.0    steps: 904  evaluation reward: 437.5\n",
      "episode: 6389   score: 620.0  epsilon: 1.0    steps: 1024  evaluation reward: 439.65\n",
      "Training network. lr: 0.000109. clip: 0.043792\n",
      "Iteration 18319: Policy loss: 0.234922. Value loss: 0.122467. Entropy: 0.301894.\n",
      "Iteration 18320: Policy loss: 0.223099. Value loss: 0.040961. Entropy: 0.300796.\n",
      "Iteration 18321: Policy loss: 0.222760. Value loss: 0.033290. Entropy: 0.300391.\n",
      "Training network. lr: 0.000109. clip: 0.043792\n",
      "Iteration 18322: Policy loss: 0.132284. Value loss: 0.087283. Entropy: 0.299474.\n",
      "Iteration 18323: Policy loss: 0.126495. Value loss: 0.028465. Entropy: 0.299575.\n",
      "Iteration 18324: Policy loss: 0.117115. Value loss: 0.018107. Entropy: 0.300134.\n",
      "episode: 6390   score: 725.0  epsilon: 1.0    steps: 928  evaluation reward: 440.75\n",
      "Training network. lr: 0.000109. clip: 0.043792\n",
      "Iteration 18325: Policy loss: 0.136973. Value loss: 0.099394. Entropy: 0.306066.\n",
      "Iteration 18326: Policy loss: 0.132722. Value loss: 0.045777. Entropy: 0.306442.\n",
      "Iteration 18327: Policy loss: 0.132188. Value loss: 0.034684. Entropy: 0.306823.\n",
      "episode: 6391   score: 500.0  epsilon: 1.0    steps: 592  evaluation reward: 443.8\n",
      "Training network. lr: 0.000109. clip: 0.043792\n",
      "Iteration 18328: Policy loss: 0.323628. Value loss: 0.156190. Entropy: 0.297215.\n",
      "Iteration 18329: Policy loss: 0.309765. Value loss: 0.059391. Entropy: 0.295097.\n",
      "Iteration 18330: Policy loss: 0.316411. Value loss: 0.040935. Entropy: 0.296521.\n",
      "episode: 6392   score: 320.0  epsilon: 1.0    steps: 208  evaluation reward: 442.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000109. clip: 0.043792\n",
      "Iteration 18331: Policy loss: -0.092648. Value loss: 0.132708. Entropy: 0.305874.\n",
      "Iteration 18332: Policy loss: -0.098801. Value loss: 0.057204. Entropy: 0.307273.\n",
      "Iteration 18333: Policy loss: -0.101402. Value loss: 0.044438. Entropy: 0.306140.\n",
      "episode: 6393   score: 390.0  epsilon: 1.0    steps: 808  evaluation reward: 443.7\n",
      "Training network. lr: 0.000109. clip: 0.043792\n",
      "Iteration 18334: Policy loss: 0.045138. Value loss: 0.124024. Entropy: 0.299834.\n",
      "Iteration 18335: Policy loss: 0.038611. Value loss: 0.062579. Entropy: 0.297209.\n",
      "Iteration 18336: Policy loss: 0.035645. Value loss: 0.048631. Entropy: 0.298305.\n",
      "Training network. lr: 0.000109. clip: 0.043792\n",
      "Iteration 18337: Policy loss: 0.201547. Value loss: 0.131617. Entropy: 0.306202.\n",
      "Iteration 18338: Policy loss: 0.195231. Value loss: 0.048072. Entropy: 0.305222.\n",
      "Iteration 18339: Policy loss: 0.191659. Value loss: 0.033975. Entropy: 0.306072.\n",
      "episode: 6394   score: 400.0  epsilon: 1.0    steps: 704  evaluation reward: 438.75\n",
      "episode: 6395   score: 340.0  epsilon: 1.0    steps: 880  evaluation reward: 439.3\n",
      "Training network. lr: 0.000109. clip: 0.043792\n",
      "Iteration 18340: Policy loss: 0.168684. Value loss: 0.134865. Entropy: 0.292430.\n",
      "Iteration 18341: Policy loss: 0.156027. Value loss: 0.058734. Entropy: 0.291850.\n",
      "Iteration 18342: Policy loss: 0.156601. Value loss: 0.040193. Entropy: 0.291883.\n",
      "episode: 6396   score: 180.0  epsilon: 1.0    steps: 440  evaluation reward: 431.35\n",
      "Training network. lr: 0.000109. clip: 0.043792\n",
      "Iteration 18343: Policy loss: 0.176347. Value loss: 0.110751. Entropy: 0.300650.\n",
      "Iteration 18344: Policy loss: 0.179027. Value loss: 0.058639. Entropy: 0.302407.\n",
      "Iteration 18345: Policy loss: 0.174560. Value loss: 0.045527. Entropy: 0.302411.\n",
      "episode: 6397   score: 495.0  epsilon: 1.0    steps: 208  evaluation reward: 430.4\n",
      "Training network. lr: 0.000109. clip: 0.043792\n",
      "Iteration 18346: Policy loss: -0.018569. Value loss: 0.106569. Entropy: 0.310065.\n",
      "Iteration 18347: Policy loss: -0.022624. Value loss: 0.047597. Entropy: 0.309647.\n",
      "Iteration 18348: Policy loss: -0.031605. Value loss: 0.034871. Entropy: 0.309137.\n",
      "Training network. lr: 0.000109. clip: 0.043792\n",
      "Iteration 18349: Policy loss: 0.094699. Value loss: 0.114635. Entropy: 0.310639.\n",
      "Iteration 18350: Policy loss: 0.086488. Value loss: 0.052996. Entropy: 0.309449.\n",
      "Iteration 18351: Policy loss: 0.084016. Value loss: 0.034032. Entropy: 0.309091.\n",
      "episode: 6398   score: 375.0  epsilon: 1.0    steps: 456  evaluation reward: 427.2\n",
      "Training network. lr: 0.000109. clip: 0.043635\n",
      "Iteration 18352: Policy loss: -0.003801. Value loss: 0.069763. Entropy: 0.304452.\n",
      "Iteration 18353: Policy loss: -0.008984. Value loss: 0.033882. Entropy: 0.304942.\n",
      "Iteration 18354: Policy loss: -0.012219. Value loss: 0.025368. Entropy: 0.304602.\n",
      "episode: 6399   score: 370.0  epsilon: 1.0    steps: 680  evaluation reward: 425.5\n",
      "episode: 6400   score: 570.0  epsilon: 1.0    steps: 920  evaluation reward: 427.25\n",
      "Training network. lr: 0.000109. clip: 0.043635\n",
      "Iteration 18355: Policy loss: -0.088860. Value loss: 0.088236. Entropy: 0.300671.\n",
      "Iteration 18356: Policy loss: -0.089043. Value loss: 0.043481. Entropy: 0.301918.\n",
      "Iteration 18357: Policy loss: -0.092029. Value loss: 0.033754. Entropy: 0.301741.\n",
      "Training network. lr: 0.000109. clip: 0.043635\n",
      "Iteration 18358: Policy loss: -0.409272. Value loss: 0.257356. Entropy: 0.311298.\n",
      "Iteration 18359: Policy loss: -0.431612. Value loss: 0.103953. Entropy: 0.312265.\n",
      "Iteration 18360: Policy loss: -0.439316. Value loss: 0.051344. Entropy: 0.313145.\n",
      "now time :  2019-09-06 09:13:55.783783\n",
      "episode: 6401   score: 650.0  epsilon: 1.0    steps: 416  evaluation reward: 429.1\n",
      "Training network. lr: 0.000109. clip: 0.043635\n",
      "Iteration 18361: Policy loss: -0.084001. Value loss: 0.327614. Entropy: 0.312617.\n",
      "Iteration 18362: Policy loss: -0.091224. Value loss: 0.185330. Entropy: 0.311877.\n",
      "Iteration 18363: Policy loss: -0.092928. Value loss: 0.124911. Entropy: 0.311960.\n",
      "episode: 6402   score: 405.0  epsilon: 1.0    steps: 192  evaluation reward: 425.6\n",
      "episode: 6403   score: 485.0  epsilon: 1.0    steps: 928  evaluation reward: 424.45\n",
      "Training network. lr: 0.000109. clip: 0.043635\n",
      "Iteration 18364: Policy loss: 0.226475. Value loss: 0.087178. Entropy: 0.304630.\n",
      "Iteration 18365: Policy loss: 0.222535. Value loss: 0.034758. Entropy: 0.303964.\n",
      "Iteration 18366: Policy loss: 0.219722. Value loss: 0.025182. Entropy: 0.304660.\n",
      "episode: 6404   score: 315.0  epsilon: 1.0    steps: 208  evaluation reward: 425.5\n",
      "Training network. lr: 0.000109. clip: 0.043635\n",
      "Iteration 18367: Policy loss: 0.130875. Value loss: 0.082311. Entropy: 0.301208.\n",
      "Iteration 18368: Policy loss: 0.134511. Value loss: 0.034694. Entropy: 0.301218.\n",
      "Iteration 18369: Policy loss: 0.127031. Value loss: 0.024109. Entropy: 0.300409.\n",
      "Training network. lr: 0.000109. clip: 0.043635\n",
      "Iteration 18370: Policy loss: 0.021994. Value loss: 0.052881. Entropy: 0.305838.\n",
      "Iteration 18371: Policy loss: 0.016571. Value loss: 0.023601. Entropy: 0.304990.\n",
      "Iteration 18372: Policy loss: 0.018469. Value loss: 0.018250. Entropy: 0.305811.\n",
      "episode: 6405   score: 450.0  epsilon: 1.0    steps: 880  evaluation reward: 424.65\n",
      "Training network. lr: 0.000109. clip: 0.043635\n",
      "Iteration 18373: Policy loss: 0.092902. Value loss: 0.127873. Entropy: 0.296150.\n",
      "Iteration 18374: Policy loss: 0.089579. Value loss: 0.055062. Entropy: 0.294547.\n",
      "Iteration 18375: Policy loss: 0.081244. Value loss: 0.036780. Entropy: 0.294816.\n",
      "episode: 6406   score: 260.0  epsilon: 1.0    steps: 120  evaluation reward: 422.9\n",
      "episode: 6407   score: 330.0  epsilon: 1.0    steps: 792  evaluation reward: 423.5\n",
      "episode: 6408   score: 355.0  epsilon: 1.0    steps: 992  evaluation reward: 418.05\n",
      "Training network. lr: 0.000109. clip: 0.043635\n",
      "Iteration 18376: Policy loss: 0.011881. Value loss: 0.258636. Entropy: 0.301612.\n",
      "Iteration 18377: Policy loss: -0.009926. Value loss: 0.182270. Entropy: 0.300816.\n",
      "Iteration 18378: Policy loss: 0.009506. Value loss: 0.135329. Entropy: 0.300465.\n",
      "Training network. lr: 0.000109. clip: 0.043635\n",
      "Iteration 18379: Policy loss: 0.016760. Value loss: 0.121551. Entropy: 0.290813.\n",
      "Iteration 18380: Policy loss: 0.009531. Value loss: 0.054062. Entropy: 0.290795.\n",
      "Iteration 18381: Policy loss: 0.005544. Value loss: 0.039870. Entropy: 0.291278.\n",
      "Training network. lr: 0.000109. clip: 0.043635\n",
      "Iteration 18382: Policy loss: -0.298815. Value loss: 0.133995. Entropy: 0.313063.\n",
      "Iteration 18383: Policy loss: -0.304409. Value loss: 0.061385. Entropy: 0.313173.\n",
      "Iteration 18384: Policy loss: -0.299255. Value loss: 0.043770. Entropy: 0.312904.\n",
      "episode: 6409   score: 260.0  epsilon: 1.0    steps: 48  evaluation reward: 413.95\n",
      "episode: 6410   score: 360.0  epsilon: 1.0    steps: 296  evaluation reward: 411.35\n",
      "Training network. lr: 0.000109. clip: 0.043635\n",
      "Iteration 18385: Policy loss: -0.185373. Value loss: 0.217486. Entropy: 0.294062.\n",
      "Iteration 18386: Policy loss: -0.183997. Value loss: 0.119092. Entropy: 0.293707.\n",
      "Iteration 18387: Policy loss: -0.194233. Value loss: 0.093784. Entropy: 0.294752.\n",
      "episode: 6411   score: 350.0  epsilon: 1.0    steps: 56  evaluation reward: 408.9\n",
      "Training network. lr: 0.000109. clip: 0.043635\n",
      "Iteration 18388: Policy loss: -0.199517. Value loss: 0.115934. Entropy: 0.299510.\n",
      "Iteration 18389: Policy loss: -0.199806. Value loss: 0.036353. Entropy: 0.299247.\n",
      "Iteration 18390: Policy loss: -0.202953. Value loss: 0.027363. Entropy: 0.300892.\n",
      "episode: 6412   score: 635.0  epsilon: 1.0    steps: 280  evaluation reward: 409.15\n",
      "Training network. lr: 0.000109. clip: 0.043635\n",
      "Iteration 18391: Policy loss: 0.020018. Value loss: 0.121021. Entropy: 0.297241.\n",
      "Iteration 18392: Policy loss: 0.021559. Value loss: 0.043680. Entropy: 0.296468.\n",
      "Iteration 18393: Policy loss: 0.017163. Value loss: 0.031477. Entropy: 0.297149.\n",
      "episode: 6413   score: 260.0  epsilon: 1.0    steps: 672  evaluation reward: 410.25\n",
      "episode: 6414   score: 330.0  epsilon: 1.0    steps: 984  evaluation reward: 411.65\n",
      "Training network. lr: 0.000109. clip: 0.043635\n",
      "Iteration 18394: Policy loss: -0.151017. Value loss: 0.414787. Entropy: 0.300938.\n",
      "Iteration 18395: Policy loss: -0.168030. Value loss: 0.136550. Entropy: 0.300718.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18396: Policy loss: -0.176641. Value loss: 0.076554. Entropy: 0.300818.\n",
      "episode: 6415   score: 345.0  epsilon: 1.0    steps: 824  evaluation reward: 413.3\n",
      "Training network. lr: 0.000109. clip: 0.043635\n",
      "Iteration 18397: Policy loss: -0.043035. Value loss: 0.095315. Entropy: 0.294959.\n",
      "Iteration 18398: Policy loss: -0.059744. Value loss: 0.031981. Entropy: 0.296285.\n",
      "Iteration 18399: Policy loss: -0.054106. Value loss: 0.021904. Entropy: 0.295709.\n",
      "Training network. lr: 0.000109. clip: 0.043635\n",
      "Iteration 18400: Policy loss: 0.063128. Value loss: 0.307032. Entropy: 0.311576.\n",
      "Iteration 18401: Policy loss: 0.066431. Value loss: 0.122975. Entropy: 0.310605.\n",
      "Iteration 18402: Policy loss: 0.046931. Value loss: 0.089515. Entropy: 0.310798.\n",
      "Training network. lr: 0.000109. clip: 0.043478\n",
      "Iteration 18403: Policy loss: -0.207759. Value loss: 0.270103. Entropy: 0.307355.\n",
      "Iteration 18404: Policy loss: -0.219592. Value loss: 0.100268. Entropy: 0.307607.\n",
      "Iteration 18405: Policy loss: -0.224796. Value loss: 0.060424. Entropy: 0.308608.\n",
      "episode: 6416   score: 210.0  epsilon: 1.0    steps: 168  evaluation reward: 410.4\n",
      "Training network. lr: 0.000109. clip: 0.043478\n",
      "Iteration 18406: Policy loss: -0.074852. Value loss: 0.205540. Entropy: 0.302129.\n",
      "Iteration 18407: Policy loss: -0.089827. Value loss: 0.093192. Entropy: 0.303834.\n",
      "Iteration 18408: Policy loss: -0.088422. Value loss: 0.060328. Entropy: 0.303620.\n",
      "episode: 6417   score: 535.0  epsilon: 1.0    steps: 376  evaluation reward: 413.95\n",
      "episode: 6418   score: 315.0  epsilon: 1.0    steps: 552  evaluation reward: 413.95\n",
      "episode: 6419   score: 640.0  epsilon: 1.0    steps: 784  evaluation reward: 416.7\n",
      "Training network. lr: 0.000109. clip: 0.043478\n",
      "Iteration 18409: Policy loss: -0.108406. Value loss: 0.202038. Entropy: 0.292146.\n",
      "Iteration 18410: Policy loss: -0.123348. Value loss: 0.104658. Entropy: 0.290847.\n",
      "Iteration 18411: Policy loss: -0.124880. Value loss: 0.077556. Entropy: 0.292280.\n",
      "Training network. lr: 0.000109. clip: 0.043478\n",
      "Iteration 18412: Policy loss: -0.018729. Value loss: 0.162432. Entropy: 0.301634.\n",
      "Iteration 18413: Policy loss: -0.017011. Value loss: 0.081016. Entropy: 0.301623.\n",
      "Iteration 18414: Policy loss: -0.017108. Value loss: 0.059105. Entropy: 0.301062.\n",
      "Training network. lr: 0.000109. clip: 0.043478\n",
      "Iteration 18415: Policy loss: 0.400061. Value loss: 0.233650. Entropy: 0.317085.\n",
      "Iteration 18416: Policy loss: 0.399551. Value loss: 0.100346. Entropy: 0.316312.\n",
      "Iteration 18417: Policy loss: 0.392871. Value loss: 0.063621. Entropy: 0.316568.\n",
      "episode: 6420   score: 260.0  epsilon: 1.0    steps: 448  evaluation reward: 414.7\n",
      "Training network. lr: 0.000109. clip: 0.043478\n",
      "Iteration 18418: Policy loss: 0.015864. Value loss: 0.341491. Entropy: 0.301260.\n",
      "Iteration 18419: Policy loss: 0.005156. Value loss: 0.150748. Entropy: 0.300853.\n",
      "Iteration 18420: Policy loss: 0.003503. Value loss: 0.101904. Entropy: 0.300659.\n",
      "episode: 6421   score: 675.0  epsilon: 1.0    steps: 984  evaluation reward: 418.0\n",
      "Training network. lr: 0.000109. clip: 0.043478\n",
      "Iteration 18421: Policy loss: 0.186334. Value loss: 0.103531. Entropy: 0.309022.\n",
      "Iteration 18422: Policy loss: 0.176600. Value loss: 0.046847. Entropy: 0.308261.\n",
      "Iteration 18423: Policy loss: 0.169243. Value loss: 0.034994. Entropy: 0.308581.\n",
      "episode: 6422   score: 1105.0  epsilon: 1.0    steps: 408  evaluation reward: 425.25\n",
      "episode: 6423   score: 315.0  epsilon: 1.0    steps: 968  evaluation reward: 424.75\n",
      "Training network. lr: 0.000109. clip: 0.043478\n",
      "Iteration 18424: Policy loss: 0.328490. Value loss: 0.172517. Entropy: 0.292172.\n",
      "Iteration 18425: Policy loss: 0.314547. Value loss: 0.080024. Entropy: 0.290773.\n",
      "Iteration 18426: Policy loss: 0.309461. Value loss: 0.053079. Entropy: 0.291288.\n",
      "Training network. lr: 0.000109. clip: 0.043478\n",
      "Iteration 18427: Policy loss: 0.227332. Value loss: 0.168677. Entropy: 0.311009.\n",
      "Iteration 18428: Policy loss: 0.225113. Value loss: 0.082138. Entropy: 0.310647.\n",
      "Iteration 18429: Policy loss: 0.212726. Value loss: 0.057540. Entropy: 0.309406.\n",
      "episode: 6424   score: 460.0  epsilon: 1.0    steps: 848  evaluation reward: 426.2\n",
      "Training network. lr: 0.000109. clip: 0.043478\n",
      "Iteration 18430: Policy loss: 0.152347. Value loss: 0.186950. Entropy: 0.307567.\n",
      "Iteration 18431: Policy loss: 0.159872. Value loss: 0.070582. Entropy: 0.308851.\n",
      "Iteration 18432: Policy loss: 0.149278. Value loss: 0.052059. Entropy: 0.307503.\n",
      "episode: 6425   score: 755.0  epsilon: 1.0    steps: 232  evaluation reward: 428.1\n",
      "episode: 6426   score: 155.0  epsilon: 1.0    steps: 904  evaluation reward: 424.4\n",
      "Training network. lr: 0.000109. clip: 0.043478\n",
      "Iteration 18433: Policy loss: 0.174659. Value loss: 0.207220. Entropy: 0.297259.\n",
      "Iteration 18434: Policy loss: 0.188920. Value loss: 0.076908. Entropy: 0.296639.\n",
      "Iteration 18435: Policy loss: 0.176990. Value loss: 0.053183. Entropy: 0.297445.\n",
      "episode: 6427   score: 355.0  epsilon: 1.0    steps: 704  evaluation reward: 423.75\n",
      "Training network. lr: 0.000109. clip: 0.043478\n",
      "Iteration 18436: Policy loss: -0.018868. Value loss: 0.109946. Entropy: 0.295351.\n",
      "Iteration 18437: Policy loss: -0.020578. Value loss: 0.050268. Entropy: 0.297101.\n",
      "Iteration 18438: Policy loss: -0.021938. Value loss: 0.033971. Entropy: 0.296944.\n",
      "Training network. lr: 0.000109. clip: 0.043478\n",
      "Iteration 18439: Policy loss: -0.593597. Value loss: 0.487971. Entropy: 0.312474.\n",
      "Iteration 18440: Policy loss: -0.602835. Value loss: 0.253472. Entropy: 0.313465.\n",
      "Iteration 18441: Policy loss: -0.613513. Value loss: 0.167365. Entropy: 0.313080.\n",
      "episode: 6428   score: 640.0  epsilon: 1.0    steps: 536  evaluation reward: 427.0\n",
      "episode: 6429   score: 465.0  epsilon: 1.0    steps: 896  evaluation reward: 424.65\n",
      "Training network. lr: 0.000109. clip: 0.043478\n",
      "Iteration 18442: Policy loss: 0.068877. Value loss: 0.153776. Entropy: 0.302691.\n",
      "Iteration 18443: Policy loss: 0.065968. Value loss: 0.073662. Entropy: 0.302278.\n",
      "Iteration 18444: Policy loss: 0.060112. Value loss: 0.051393. Entropy: 0.303081.\n",
      "Training network. lr: 0.000109. clip: 0.043478\n",
      "Iteration 18445: Policy loss: 0.193726. Value loss: 0.137925. Entropy: 0.311060.\n",
      "Iteration 18446: Policy loss: 0.178839. Value loss: 0.046672. Entropy: 0.310664.\n",
      "Iteration 18447: Policy loss: 0.182313. Value loss: 0.027576. Entropy: 0.311242.\n",
      "episode: 6430   score: 560.0  epsilon: 1.0    steps: 208  evaluation reward: 427.25\n",
      "Training network. lr: 0.000109. clip: 0.043478\n",
      "Iteration 18448: Policy loss: -0.116036. Value loss: 0.345862. Entropy: 0.303577.\n",
      "Iteration 18449: Policy loss: -0.145180. Value loss: 0.195792. Entropy: 0.303648.\n",
      "Iteration 18450: Policy loss: -0.127955. Value loss: 0.122925. Entropy: 0.302124.\n",
      "Training network. lr: 0.000108. clip: 0.043331\n",
      "Iteration 18451: Policy loss: 0.087378. Value loss: 0.225981. Entropy: 0.311559.\n",
      "Iteration 18452: Policy loss: 0.083611. Value loss: 0.090906. Entropy: 0.309019.\n",
      "Iteration 18453: Policy loss: 0.075571. Value loss: 0.059900. Entropy: 0.309023.\n",
      "episode: 6431   score: 390.0  epsilon: 1.0    steps: 400  evaluation reward: 426.8\n",
      "Training network. lr: 0.000108. clip: 0.043331\n",
      "Iteration 18454: Policy loss: 0.113481. Value loss: 0.167825. Entropy: 0.302524.\n",
      "Iteration 18455: Policy loss: 0.108154. Value loss: 0.062053. Entropy: 0.302176.\n",
      "Iteration 18456: Policy loss: 0.111999. Value loss: 0.038626. Entropy: 0.301520.\n",
      "episode: 6432   score: 295.0  epsilon: 1.0    steps: 328  evaluation reward: 423.7\n",
      "episode: 6433   score: 455.0  epsilon: 1.0    steps: 664  evaluation reward: 422.95\n",
      "episode: 6434   score: 450.0  epsilon: 1.0    steps: 928  evaluation reward: 424.8\n",
      "Training network. lr: 0.000108. clip: 0.043331\n",
      "Iteration 18457: Policy loss: 0.030446. Value loss: 0.149363. Entropy: 0.283358.\n",
      "Iteration 18458: Policy loss: 0.029302. Value loss: 0.065325. Entropy: 0.283760.\n",
      "Iteration 18459: Policy loss: 0.021578. Value loss: 0.047526. Entropy: 0.281933.\n",
      "Training network. lr: 0.000108. clip: 0.043331\n",
      "Iteration 18460: Policy loss: -0.292430. Value loss: 0.240716. Entropy: 0.301810.\n",
      "Iteration 18461: Policy loss: -0.288038. Value loss: 0.089793. Entropy: 0.302806.\n",
      "Iteration 18462: Policy loss: -0.285848. Value loss: 0.051388. Entropy: 0.301974.\n",
      "Training network. lr: 0.000108. clip: 0.043331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18463: Policy loss: -0.114496. Value loss: 0.187288. Entropy: 0.306402.\n",
      "Iteration 18464: Policy loss: -0.126788. Value loss: 0.082784. Entropy: 0.305936.\n",
      "Iteration 18465: Policy loss: -0.134576. Value loss: 0.054001. Entropy: 0.306257.\n",
      "episode: 6435   score: 735.0  epsilon: 1.0    steps: 760  evaluation reward: 428.05\n",
      "Training network. lr: 0.000108. clip: 0.043331\n",
      "Iteration 18466: Policy loss: -0.376373. Value loss: 0.371293. Entropy: 0.297737.\n",
      "Iteration 18467: Policy loss: -0.357603. Value loss: 0.149980. Entropy: 0.299656.\n",
      "Iteration 18468: Policy loss: -0.391964. Value loss: 0.101727. Entropy: 0.297804.\n",
      "episode: 6436   score: 515.0  epsilon: 1.0    steps: 152  evaluation reward: 427.6\n",
      "episode: 6437   score: 585.0  epsilon: 1.0    steps: 928  evaluation reward: 431.35\n",
      "Training network. lr: 0.000108. clip: 0.043331\n",
      "Iteration 18469: Policy loss: 0.295420. Value loss: 0.277489. Entropy: 0.295518.\n",
      "Iteration 18470: Policy loss: 0.280492. Value loss: 0.100686. Entropy: 0.292022.\n",
      "Iteration 18471: Policy loss: 0.285340. Value loss: 0.059998. Entropy: 0.292401.\n",
      "episode: 6438   score: 770.0  epsilon: 1.0    steps: 520  evaluation reward: 433.7\n",
      "Training network. lr: 0.000108. clip: 0.043331\n",
      "Iteration 18472: Policy loss: 0.076420. Value loss: 0.136876. Entropy: 0.296360.\n",
      "Iteration 18473: Policy loss: 0.074440. Value loss: 0.059365. Entropy: 0.293782.\n",
      "Iteration 18474: Policy loss: 0.074266. Value loss: 0.043691. Entropy: 0.293486.\n",
      "episode: 6439   score: 95.0  epsilon: 1.0    steps: 656  evaluation reward: 427.05\n",
      "Training network. lr: 0.000108. clip: 0.043331\n",
      "Iteration 18475: Policy loss: 0.293906. Value loss: 0.253930. Entropy: 0.306760.\n",
      "Iteration 18476: Policy loss: 0.279797. Value loss: 0.124247. Entropy: 0.306367.\n",
      "Iteration 18477: Policy loss: 0.267729. Value loss: 0.079668. Entropy: 0.305958.\n",
      "episode: 6440   score: 420.0  epsilon: 1.0    steps: 608  evaluation reward: 427.8\n",
      "Training network. lr: 0.000108. clip: 0.043331\n",
      "Iteration 18478: Policy loss: 0.103360. Value loss: 0.337816. Entropy: 0.303507.\n",
      "Iteration 18479: Policy loss: 0.101623. Value loss: 0.140748. Entropy: 0.302990.\n",
      "Iteration 18480: Policy loss: 0.088144. Value loss: 0.090814. Entropy: 0.303259.\n",
      "episode: 6441   score: 590.0  epsilon: 1.0    steps: 784  evaluation reward: 430.4\n",
      "Training network. lr: 0.000108. clip: 0.043331\n",
      "Iteration 18481: Policy loss: 0.119500. Value loss: 0.109330. Entropy: 0.304286.\n",
      "Iteration 18482: Policy loss: 0.116130. Value loss: 0.051146. Entropy: 0.305655.\n",
      "Iteration 18483: Policy loss: 0.103234. Value loss: 0.039801. Entropy: 0.305862.\n",
      "episode: 6442   score: 210.0  epsilon: 1.0    steps: 312  evaluation reward: 428.3\n",
      "episode: 6443   score: 105.0  epsilon: 1.0    steps: 360  evaluation reward: 426.3\n",
      "Training network. lr: 0.000108. clip: 0.043331\n",
      "Iteration 18484: Policy loss: 0.007032. Value loss: 0.095364. Entropy: 0.281894.\n",
      "Iteration 18485: Policy loss: 0.008586. Value loss: 0.044875. Entropy: 0.283822.\n",
      "Iteration 18486: Policy loss: 0.004500. Value loss: 0.032389. Entropy: 0.283155.\n",
      "Training network. lr: 0.000108. clip: 0.043331\n",
      "Iteration 18487: Policy loss: 0.153340. Value loss: 0.274199. Entropy: 0.311346.\n",
      "Iteration 18488: Policy loss: 0.154004. Value loss: 0.086873. Entropy: 0.311483.\n",
      "Iteration 18489: Policy loss: 0.135610. Value loss: 0.049188. Entropy: 0.311747.\n",
      "episode: 6444   score: 665.0  epsilon: 1.0    steps: 424  evaluation reward: 425.95\n",
      "episode: 6445   score: 320.0  epsilon: 1.0    steps: 960  evaluation reward: 425.6\n",
      "Training network. lr: 0.000108. clip: 0.043331\n",
      "Iteration 18490: Policy loss: 0.047207. Value loss: 0.155678. Entropy: 0.299735.\n",
      "Iteration 18491: Policy loss: 0.045430. Value loss: 0.074256. Entropy: 0.299944.\n",
      "Iteration 18492: Policy loss: 0.038111. Value loss: 0.060434. Entropy: 0.299309.\n",
      "episode: 6446   score: 695.0  epsilon: 1.0    steps: 344  evaluation reward: 430.15\n",
      "episode: 6447   score: 210.0  epsilon: 1.0    steps: 520  evaluation reward: 429.65\n",
      "Training network. lr: 0.000108. clip: 0.043331\n",
      "Iteration 18493: Policy loss: 0.086592. Value loss: 0.083624. Entropy: 0.293358.\n",
      "Iteration 18494: Policy loss: 0.083578. Value loss: 0.042928. Entropy: 0.295028.\n",
      "Iteration 18495: Policy loss: 0.086766. Value loss: 0.033023. Entropy: 0.295218.\n",
      "Training network. lr: 0.000108. clip: 0.043331\n",
      "Iteration 18496: Policy loss: -0.096556. Value loss: 0.148352. Entropy: 0.314362.\n",
      "Iteration 18497: Policy loss: -0.095758. Value loss: 0.073128. Entropy: 0.314645.\n",
      "Iteration 18498: Policy loss: -0.103995. Value loss: 0.051003. Entropy: 0.314348.\n",
      "Training network. lr: 0.000108. clip: 0.043331\n",
      "Iteration 18499: Policy loss: -0.067979. Value loss: 0.130023. Entropy: 0.290695.\n",
      "Iteration 18500: Policy loss: -0.071288. Value loss: 0.054471. Entropy: 0.290474.\n",
      "Iteration 18501: Policy loss: -0.071828. Value loss: 0.033676. Entropy: 0.289863.\n",
      "episode: 6448   score: 180.0  epsilon: 1.0    steps: 352  evaluation reward: 430.45\n",
      "Training network. lr: 0.000108. clip: 0.043174\n",
      "Iteration 18502: Policy loss: 0.152450. Value loss: 0.351650. Entropy: 0.288704.\n",
      "Iteration 18503: Policy loss: 0.170160. Value loss: 0.117216. Entropy: 0.282944.\n",
      "Iteration 18504: Policy loss: 0.137204. Value loss: 0.082470. Entropy: 0.285144.\n",
      "episode: 6449   score: 595.0  epsilon: 1.0    steps: 688  evaluation reward: 433.5\n",
      "Training network. lr: 0.000108. clip: 0.043174\n",
      "Iteration 18505: Policy loss: 0.188302. Value loss: 0.190702. Entropy: 0.307606.\n",
      "Iteration 18506: Policy loss: 0.178743. Value loss: 0.064997. Entropy: 0.305689.\n",
      "Iteration 18507: Policy loss: 0.173662. Value loss: 0.046504. Entropy: 0.305926.\n",
      "Training network. lr: 0.000108. clip: 0.043174\n",
      "Iteration 18508: Policy loss: 0.119030. Value loss: 0.100362. Entropy: 0.297613.\n",
      "Iteration 18509: Policy loss: 0.113217. Value loss: 0.038654. Entropy: 0.298685.\n",
      "Iteration 18510: Policy loss: 0.112018. Value loss: 0.025624. Entropy: 0.298871.\n",
      "episode: 6450   score: 405.0  epsilon: 1.0    steps: 280  evaluation reward: 428.65\n",
      "Training network. lr: 0.000108. clip: 0.043174\n",
      "Iteration 18511: Policy loss: -0.284064. Value loss: 0.158327. Entropy: 0.302227.\n",
      "Iteration 18512: Policy loss: -0.285948. Value loss: 0.070708. Entropy: 0.303458.\n",
      "Iteration 18513: Policy loss: -0.293559. Value loss: 0.052025. Entropy: 0.303535.\n",
      "now time :  2019-09-06 09:23:12.543216\n",
      "episode: 6451   score: 335.0  epsilon: 1.0    steps: 544  evaluation reward: 429.75\n",
      "Training network. lr: 0.000108. clip: 0.043174\n",
      "Iteration 18514: Policy loss: -0.047578. Value loss: 0.168036. Entropy: 0.300486.\n",
      "Iteration 18515: Policy loss: -0.065781. Value loss: 0.050159. Entropy: 0.301339.\n",
      "Iteration 18516: Policy loss: -0.055774. Value loss: 0.032410. Entropy: 0.301419.\n",
      "episode: 6452   score: 500.0  epsilon: 1.0    steps: 480  evaluation reward: 431.6\n",
      "Training network. lr: 0.000108. clip: 0.043174\n",
      "Iteration 18517: Policy loss: -0.310885. Value loss: 0.514059. Entropy: 0.301339.\n",
      "Iteration 18518: Policy loss: -0.288820. Value loss: 0.343026. Entropy: 0.300084.\n",
      "Iteration 18519: Policy loss: -0.319070. Value loss: 0.306201. Entropy: 0.298486.\n",
      "Training network. lr: 0.000108. clip: 0.043174\n",
      "Iteration 18520: Policy loss: 0.174310. Value loss: 0.196236. Entropy: 0.295674.\n",
      "Iteration 18521: Policy loss: 0.170903. Value loss: 0.081844. Entropy: 0.295571.\n",
      "Iteration 18522: Policy loss: 0.162266. Value loss: 0.053262. Entropy: 0.296556.\n",
      "episode: 6453   score: 440.0  epsilon: 1.0    steps: 616  evaluation reward: 433.9\n",
      "Training network. lr: 0.000108. clip: 0.043174\n",
      "Iteration 18523: Policy loss: -0.137509. Value loss: 0.216676. Entropy: 0.301285.\n",
      "Iteration 18524: Policy loss: -0.140145. Value loss: 0.086993. Entropy: 0.301754.\n",
      "Iteration 18525: Policy loss: -0.154143. Value loss: 0.050770. Entropy: 0.302135.\n",
      "episode: 6454   score: 650.0  epsilon: 1.0    steps: 72  evaluation reward: 436.5\n",
      "episode: 6455   score: 295.0  epsilon: 1.0    steps: 400  evaluation reward: 437.35\n",
      "episode: 6456   score: 640.0  epsilon: 1.0    steps: 856  evaluation reward: 440.1\n",
      "Training network. lr: 0.000108. clip: 0.043174\n",
      "Iteration 18526: Policy loss: -0.344378. Value loss: 0.401252. Entropy: 0.278310.\n",
      "Iteration 18527: Policy loss: -0.381676. Value loss: 0.255770. Entropy: 0.279137.\n",
      "Iteration 18528: Policy loss: -0.387280. Value loss: 0.189617. Entropy: 0.277287.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6457   score: 230.0  epsilon: 1.0    steps: 168  evaluation reward: 438.8\n",
      "Training network. lr: 0.000108. clip: 0.043174\n",
      "Iteration 18529: Policy loss: 0.051962. Value loss: 0.090220. Entropy: 0.294115.\n",
      "Iteration 18530: Policy loss: 0.053523. Value loss: 0.056231. Entropy: 0.294263.\n",
      "Iteration 18531: Policy loss: 0.051031. Value loss: 0.040435. Entropy: 0.293508.\n",
      "Training network. lr: 0.000108. clip: 0.043174\n",
      "Iteration 18532: Policy loss: -0.152087. Value loss: 0.147805. Entropy: 0.312636.\n",
      "Iteration 18533: Policy loss: -0.159088. Value loss: 0.087740. Entropy: 0.314205.\n",
      "Iteration 18534: Policy loss: -0.163305. Value loss: 0.064561. Entropy: 0.313685.\n",
      "episode: 6458   score: 925.0  epsilon: 1.0    steps: 256  evaluation reward: 442.1\n",
      "Training network. lr: 0.000108. clip: 0.043174\n",
      "Iteration 18535: Policy loss: -0.179537. Value loss: 0.282149. Entropy: 0.282192.\n",
      "Iteration 18536: Policy loss: -0.191951. Value loss: 0.156981. Entropy: 0.282962.\n",
      "Iteration 18537: Policy loss: -0.211355. Value loss: 0.099303. Entropy: 0.281713.\n",
      "Training network. lr: 0.000108. clip: 0.043174\n",
      "Iteration 18538: Policy loss: 0.469128. Value loss: 0.267727. Entropy: 0.299974.\n",
      "Iteration 18539: Policy loss: 0.459707. Value loss: 0.092490. Entropy: 0.299599.\n",
      "Iteration 18540: Policy loss: 0.469317. Value loss: 0.058417. Entropy: 0.299706.\n",
      "episode: 6459   score: 645.0  epsilon: 1.0    steps: 984  evaluation reward: 444.7\n",
      "Training network. lr: 0.000108. clip: 0.043174\n",
      "Iteration 18541: Policy loss: 0.427537. Value loss: 0.177477. Entropy: 0.293434.\n",
      "Iteration 18542: Policy loss: 0.424113. Value loss: 0.087557. Entropy: 0.292842.\n",
      "Iteration 18543: Policy loss: 0.415054. Value loss: 0.062372. Entropy: 0.290194.\n",
      "Training network. lr: 0.000108. clip: 0.043174\n",
      "Iteration 18544: Policy loss: 0.185367. Value loss: 0.193287. Entropy: 0.293081.\n",
      "Iteration 18545: Policy loss: 0.172332. Value loss: 0.064490. Entropy: 0.291490.\n",
      "Iteration 18546: Policy loss: 0.175779. Value loss: 0.043481. Entropy: 0.290384.\n",
      "episode: 6460   score: 340.0  epsilon: 1.0    steps: 32  evaluation reward: 444.5\n",
      "episode: 6461   score: 535.0  epsilon: 1.0    steps: 496  evaluation reward: 446.85\n",
      "episode: 6462   score: 370.0  epsilon: 1.0    steps: 672  evaluation reward: 448.75\n",
      "Training network. lr: 0.000108. clip: 0.043174\n",
      "Iteration 18547: Policy loss: -0.027921. Value loss: 0.123194. Entropy: 0.283502.\n",
      "Iteration 18548: Policy loss: -0.033288. Value loss: 0.051806. Entropy: 0.282343.\n",
      "Iteration 18549: Policy loss: -0.040895. Value loss: 0.039233. Entropy: 0.283648.\n",
      "episode: 6463   score: 365.0  epsilon: 1.0    steps: 600  evaluation reward: 448.75\n",
      "Training network. lr: 0.000108. clip: 0.043174\n",
      "Iteration 18550: Policy loss: 0.044518. Value loss: 0.142513. Entropy: 0.302225.\n",
      "Iteration 18551: Policy loss: 0.040738. Value loss: 0.080712. Entropy: 0.301320.\n",
      "Iteration 18552: Policy loss: 0.036528. Value loss: 0.060895. Entropy: 0.300844.\n",
      "Training network. lr: 0.000108. clip: 0.043017\n",
      "Iteration 18553: Policy loss: 0.160169. Value loss: 0.139627. Entropy: 0.307152.\n",
      "Iteration 18554: Policy loss: 0.156496. Value loss: 0.063046. Entropy: 0.306610.\n",
      "Iteration 18555: Policy loss: 0.150533. Value loss: 0.045811. Entropy: 0.307826.\n",
      "Training network. lr: 0.000108. clip: 0.043017\n",
      "Iteration 18556: Policy loss: -0.215288. Value loss: 0.235033. Entropy: 0.302938.\n",
      "Iteration 18557: Policy loss: -0.209190. Value loss: 0.118546. Entropy: 0.302463.\n",
      "Iteration 18558: Policy loss: -0.234702. Value loss: 0.083721. Entropy: 0.302853.\n",
      "episode: 6464   score: 775.0  epsilon: 1.0    steps: 64  evaluation reward: 453.15\n",
      "Training network. lr: 0.000108. clip: 0.043017\n",
      "Iteration 18559: Policy loss: 0.425679. Value loss: 0.174011. Entropy: 0.303347.\n",
      "Iteration 18560: Policy loss: 0.416739. Value loss: 0.077973. Entropy: 0.303138.\n",
      "Iteration 18561: Policy loss: 0.418419. Value loss: 0.055228. Entropy: 0.302123.\n",
      "episode: 6465   score: 645.0  epsilon: 1.0    steps: 880  evaluation reward: 455.4\n",
      "Training network. lr: 0.000108. clip: 0.043017\n",
      "Iteration 18562: Policy loss: -0.138932. Value loss: 0.267954. Entropy: 0.285193.\n",
      "Iteration 18563: Policy loss: -0.156735. Value loss: 0.173430. Entropy: 0.287897.\n",
      "Iteration 18564: Policy loss: -0.162466. Value loss: 0.130710. Entropy: 0.288637.\n",
      "episode: 6466   score: 315.0  epsilon: 1.0    steps: 520  evaluation reward: 456.4\n",
      "episode: 6467   score: 295.0  epsilon: 1.0    steps: 624  evaluation reward: 454.7\n",
      "Training network. lr: 0.000108. clip: 0.043017\n",
      "Iteration 18565: Policy loss: 0.032968. Value loss: 0.096713. Entropy: 0.293272.\n",
      "Iteration 18566: Policy loss: 0.025247. Value loss: 0.052462. Entropy: 0.294588.\n",
      "Iteration 18567: Policy loss: 0.025171. Value loss: 0.040886. Entropy: 0.295654.\n",
      "Training network. lr: 0.000108. clip: 0.043017\n",
      "Iteration 18568: Policy loss: -0.255297. Value loss: 0.129588. Entropy: 0.310958.\n",
      "Iteration 18569: Policy loss: -0.256212. Value loss: 0.067524. Entropy: 0.311526.\n",
      "Iteration 18570: Policy loss: -0.264624. Value loss: 0.049728. Entropy: 0.310747.\n",
      "Training network. lr: 0.000108. clip: 0.043017\n",
      "Iteration 18571: Policy loss: -0.235175. Value loss: 0.192377. Entropy: 0.292255.\n",
      "Iteration 18572: Policy loss: -0.240655. Value loss: 0.069313. Entropy: 0.292462.\n",
      "Iteration 18573: Policy loss: -0.242963. Value loss: 0.050176. Entropy: 0.291083.\n",
      "episode: 6468   score: 655.0  epsilon: 1.0    steps: 560  evaluation reward: 457.5\n",
      "episode: 6469   score: 345.0  epsilon: 1.0    steps: 1016  evaluation reward: 455.85\n",
      "Training network. lr: 0.000108. clip: 0.043017\n",
      "Iteration 18574: Policy loss: -0.172695. Value loss: 0.681142. Entropy: 0.303521.\n",
      "Iteration 18575: Policy loss: -0.194993. Value loss: 0.295478. Entropy: 0.303811.\n",
      "Iteration 18576: Policy loss: -0.181582. Value loss: 0.221361. Entropy: 0.303300.\n",
      "episode: 6470   score: 625.0  epsilon: 1.0    steps: 704  evaluation reward: 458.35\n",
      "Training network. lr: 0.000108. clip: 0.043017\n",
      "Iteration 18577: Policy loss: 0.392012. Value loss: 0.129062. Entropy: 0.294187.\n",
      "Iteration 18578: Policy loss: 0.387208. Value loss: 0.056230. Entropy: 0.291884.\n",
      "Iteration 18579: Policy loss: 0.378502. Value loss: 0.043333. Entropy: 0.292195.\n",
      "episode: 6471   score: 785.0  epsilon: 1.0    steps: 128  evaluation reward: 464.05\n",
      "Training network. lr: 0.000108. clip: 0.043017\n",
      "Iteration 18580: Policy loss: -0.050276. Value loss: 0.083012. Entropy: 0.306372.\n",
      "Iteration 18581: Policy loss: -0.050402. Value loss: 0.043051. Entropy: 0.304767.\n",
      "Iteration 18582: Policy loss: -0.054436. Value loss: 0.031194. Entropy: 0.303783.\n",
      "episode: 6472   score: 270.0  epsilon: 1.0    steps: 848  evaluation reward: 459.5\n",
      "Training network. lr: 0.000108. clip: 0.043017\n",
      "Iteration 18583: Policy loss: -0.076409. Value loss: 0.228408. Entropy: 0.295680.\n",
      "Iteration 18584: Policy loss: -0.074595. Value loss: 0.107269. Entropy: 0.294524.\n",
      "Iteration 18585: Policy loss: -0.085261. Value loss: 0.071363. Entropy: 0.295088.\n",
      "Training network. lr: 0.000108. clip: 0.043017\n",
      "Iteration 18586: Policy loss: 0.365251. Value loss: 0.326648. Entropy: 0.292968.\n",
      "Iteration 18587: Policy loss: 0.338280. Value loss: 0.082271. Entropy: 0.291542.\n",
      "Iteration 18588: Policy loss: 0.339894. Value loss: 0.045037. Entropy: 0.294617.\n",
      "episode: 6473   score: 605.0  epsilon: 1.0    steps: 664  evaluation reward: 459.0\n",
      "Training network. lr: 0.000108. clip: 0.043017\n",
      "Iteration 18589: Policy loss: 0.310952. Value loss: 0.215511. Entropy: 0.288577.\n",
      "Iteration 18590: Policy loss: 0.327202. Value loss: 0.069354. Entropy: 0.286330.\n",
      "Iteration 18591: Policy loss: 0.313736. Value loss: 0.040338. Entropy: 0.286269.\n",
      "Training network. lr: 0.000108. clip: 0.043017\n",
      "Iteration 18592: Policy loss: -0.017541. Value loss: 0.498908. Entropy: 0.309570.\n",
      "Iteration 18593: Policy loss: -0.023530. Value loss: 0.311396. Entropy: 0.309765.\n",
      "Iteration 18594: Policy loss: -0.044068. Value loss: 0.235126. Entropy: 0.309740.\n",
      "episode: 6474   score: 565.0  epsilon: 1.0    steps: 248  evaluation reward: 462.85\n",
      "Training network. lr: 0.000108. clip: 0.043017\n",
      "Iteration 18595: Policy loss: 0.203575. Value loss: 0.129279. Entropy: 0.298479.\n",
      "Iteration 18596: Policy loss: 0.205864. Value loss: 0.054198. Entropy: 0.298197.\n",
      "Iteration 18597: Policy loss: 0.208867. Value loss: 0.034190. Entropy: 0.298538.\n",
      "episode: 6475   score: 650.0  epsilon: 1.0    steps: 64  evaluation reward: 466.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6476   score: 360.0  epsilon: 1.0    steps: 592  evaluation reward: 466.0\n",
      "Training network. lr: 0.000108. clip: 0.043017\n",
      "Iteration 18598: Policy loss: 0.146693. Value loss: 0.134405. Entropy: 0.274843.\n",
      "Iteration 18599: Policy loss: 0.132667. Value loss: 0.063884. Entropy: 0.273684.\n",
      "Iteration 18600: Policy loss: 0.142056. Value loss: 0.045185. Entropy: 0.275406.\n",
      "episode: 6477   score: 380.0  epsilon: 1.0    steps: 224  evaluation reward: 463.6\n",
      "episode: 6478   score: 395.0  epsilon: 1.0    steps: 912  evaluation reward: 463.15\n",
      "Training network. lr: 0.000107. clip: 0.042870\n",
      "Iteration 18601: Policy loss: 0.001502. Value loss: 0.258712. Entropy: 0.298057.\n",
      "Iteration 18602: Policy loss: 0.015026. Value loss: 0.126214. Entropy: 0.297777.\n",
      "Iteration 18603: Policy loss: -0.008644. Value loss: 0.068492. Entropy: 0.298901.\n",
      "Training network. lr: 0.000107. clip: 0.042870\n",
      "Iteration 18604: Policy loss: 0.037969. Value loss: 0.148464. Entropy: 0.288506.\n",
      "Iteration 18605: Policy loss: 0.031956. Value loss: 0.045784. Entropy: 0.286180.\n",
      "Iteration 18606: Policy loss: 0.022501. Value loss: 0.029697. Entropy: 0.286631.\n",
      "episode: 6479   score: 360.0  epsilon: 1.0    steps: 672  evaluation reward: 461.5\n",
      "Training network. lr: 0.000107. clip: 0.042870\n",
      "Iteration 18607: Policy loss: -0.010552. Value loss: 0.232926. Entropy: 0.294564.\n",
      "Iteration 18608: Policy loss: -0.033016. Value loss: 0.103192. Entropy: 0.292196.\n",
      "Iteration 18609: Policy loss: -0.031516. Value loss: 0.063176. Entropy: 0.291831.\n",
      "Training network. lr: 0.000107. clip: 0.042870\n",
      "Iteration 18610: Policy loss: 0.345476. Value loss: 0.241386. Entropy: 0.313159.\n",
      "Iteration 18611: Policy loss: 0.339577. Value loss: 0.101265. Entropy: 0.311807.\n",
      "Iteration 18612: Policy loss: 0.326196. Value loss: 0.059963. Entropy: 0.310729.\n",
      "episode: 6480   score: 640.0  epsilon: 1.0    steps: 624  evaluation reward: 461.85\n",
      "episode: 6481   score: 895.0  epsilon: 1.0    steps: 1024  evaluation reward: 464.8\n",
      "Training network. lr: 0.000107. clip: 0.042870\n",
      "Iteration 18613: Policy loss: 0.106057. Value loss: 0.268828. Entropy: 0.305778.\n",
      "Iteration 18614: Policy loss: 0.099479. Value loss: 0.111118. Entropy: 0.303785.\n",
      "Iteration 18615: Policy loss: 0.101014. Value loss: 0.081512. Entropy: 0.305185.\n",
      "episode: 6482   score: 210.0  epsilon: 1.0    steps: 144  evaluation reward: 462.2\n",
      "Training network. lr: 0.000107. clip: 0.042870\n",
      "Iteration 18616: Policy loss: 0.272473. Value loss: 0.166191. Entropy: 0.299912.\n",
      "Iteration 18617: Policy loss: 0.265219. Value loss: 0.088629. Entropy: 0.299595.\n",
      "Iteration 18618: Policy loss: 0.258034. Value loss: 0.063527. Entropy: 0.300087.\n",
      "Training network. lr: 0.000107. clip: 0.042870\n",
      "Iteration 18619: Policy loss: -0.130477. Value loss: 0.159797. Entropy: 0.310733.\n",
      "Iteration 18620: Policy loss: -0.131520. Value loss: 0.079965. Entropy: 0.311852.\n",
      "Iteration 18621: Policy loss: -0.133854. Value loss: 0.056866. Entropy: 0.311023.\n",
      "Training network. lr: 0.000107. clip: 0.042870\n",
      "Iteration 18622: Policy loss: 0.136015. Value loss: 0.120764. Entropy: 0.306410.\n",
      "Iteration 18623: Policy loss: 0.120069. Value loss: 0.052998. Entropy: 0.306271.\n",
      "Iteration 18624: Policy loss: 0.120206. Value loss: 0.035070. Entropy: 0.306465.\n",
      "episode: 6483   score: 705.0  epsilon: 1.0    steps: 344  evaluation reward: 465.6\n",
      "Training network. lr: 0.000107. clip: 0.042870\n",
      "Iteration 18625: Policy loss: -0.222057. Value loss: 0.168184. Entropy: 0.297918.\n",
      "Iteration 18626: Policy loss: -0.229832. Value loss: 0.062186. Entropy: 0.298917.\n",
      "Iteration 18627: Policy loss: -0.225067. Value loss: 0.039020. Entropy: 0.298656.\n",
      "episode: 6484   score: 540.0  epsilon: 1.0    steps: 632  evaluation reward: 462.2\n",
      "Training network. lr: 0.000107. clip: 0.042870\n",
      "Iteration 18628: Policy loss: 0.205924. Value loss: 0.123245. Entropy: 0.299587.\n",
      "Iteration 18629: Policy loss: 0.203238. Value loss: 0.055447. Entropy: 0.298425.\n",
      "Iteration 18630: Policy loss: 0.192457. Value loss: 0.038762. Entropy: 0.298308.\n",
      "episode: 6485   score: 635.0  epsilon: 1.0    steps: 664  evaluation reward: 464.9\n",
      "Training network. lr: 0.000107. clip: 0.042870\n",
      "Iteration 18631: Policy loss: 0.224478. Value loss: 0.092764. Entropy: 0.301086.\n",
      "Iteration 18632: Policy loss: 0.216766. Value loss: 0.036118. Entropy: 0.300571.\n",
      "Iteration 18633: Policy loss: 0.215138. Value loss: 0.025480. Entropy: 0.301031.\n",
      "episode: 6486   score: 350.0  epsilon: 1.0    steps: 712  evaluation reward: 460.15\n",
      "episode: 6487   score: 215.0  epsilon: 1.0    steps: 824  evaluation reward: 460.05\n",
      "Training network. lr: 0.000107. clip: 0.042870\n",
      "Iteration 18634: Policy loss: -0.023492. Value loss: 0.195840. Entropy: 0.292449.\n",
      "Iteration 18635: Policy loss: -0.028471. Value loss: 0.073069. Entropy: 0.291207.\n",
      "Iteration 18636: Policy loss: -0.040520. Value loss: 0.049980. Entropy: 0.293086.\n",
      "episode: 6488   score: 390.0  epsilon: 1.0    steps: 40  evaluation reward: 460.8\n",
      "episode: 6489   score: 600.0  epsilon: 1.0    steps: 392  evaluation reward: 460.6\n",
      "Training network. lr: 0.000107. clip: 0.042870\n",
      "Iteration 18637: Policy loss: 0.180905. Value loss: 0.093520. Entropy: 0.289336.\n",
      "Iteration 18638: Policy loss: 0.182565. Value loss: 0.049657. Entropy: 0.290411.\n",
      "Iteration 18639: Policy loss: 0.175656. Value loss: 0.037095. Entropy: 0.289088.\n",
      "Training network. lr: 0.000107. clip: 0.042870\n",
      "Iteration 18640: Policy loss: 0.080369. Value loss: 0.134959. Entropy: 0.315624.\n",
      "Iteration 18641: Policy loss: 0.073497. Value loss: 0.056074. Entropy: 0.314670.\n",
      "Iteration 18642: Policy loss: 0.069232. Value loss: 0.042327. Entropy: 0.314823.\n",
      "Training network. lr: 0.000107. clip: 0.042870\n",
      "Iteration 18643: Policy loss: -0.225322. Value loss: 0.355528. Entropy: 0.295975.\n",
      "Iteration 18644: Policy loss: -0.245868. Value loss: 0.254740. Entropy: 0.296919.\n",
      "Iteration 18645: Policy loss: -0.257980. Value loss: 0.171575. Entropy: 0.298486.\n",
      "episode: 6490   score: 405.0  epsilon: 1.0    steps: 416  evaluation reward: 457.4\n",
      "Training network. lr: 0.000107. clip: 0.042870\n",
      "Iteration 18646: Policy loss: 0.180292. Value loss: 0.256262. Entropy: 0.300476.\n",
      "Iteration 18647: Policy loss: 0.164623. Value loss: 0.129842. Entropy: 0.299597.\n",
      "Iteration 18648: Policy loss: 0.180336. Value loss: 0.086359. Entropy: 0.299076.\n",
      "Training network. lr: 0.000107. clip: 0.042870\n",
      "Iteration 18649: Policy loss: 0.272277. Value loss: 0.126013. Entropy: 0.308355.\n",
      "Iteration 18650: Policy loss: 0.267611. Value loss: 0.059643. Entropy: 0.309174.\n",
      "Iteration 18651: Policy loss: 0.253599. Value loss: 0.043385. Entropy: 0.309935.\n",
      "episode: 6491   score: 560.0  epsilon: 1.0    steps: 8  evaluation reward: 458.0\n",
      "episode: 6492   score: 240.0  epsilon: 1.0    steps: 480  evaluation reward: 457.2\n",
      "episode: 6493   score: 355.0  epsilon: 1.0    steps: 744  evaluation reward: 456.85\n",
      "Training network. lr: 0.000107. clip: 0.042713\n",
      "Iteration 18652: Policy loss: 0.300168. Value loss: 0.133362. Entropy: 0.287634.\n",
      "Iteration 18653: Policy loss: 0.288376. Value loss: 0.066491. Entropy: 0.285464.\n",
      "Iteration 18654: Policy loss: 0.281406. Value loss: 0.044838. Entropy: 0.286115.\n",
      "episode: 6494   score: 460.0  epsilon: 1.0    steps: 88  evaluation reward: 457.45\n",
      "episode: 6495   score: 650.0  epsilon: 1.0    steps: 680  evaluation reward: 460.55\n",
      "Training network. lr: 0.000107. clip: 0.042713\n",
      "Iteration 18655: Policy loss: 0.302847. Value loss: 0.146059. Entropy: 0.299953.\n",
      "Iteration 18656: Policy loss: 0.299113. Value loss: 0.054681. Entropy: 0.300166.\n",
      "Iteration 18657: Policy loss: 0.289919. Value loss: 0.040575. Entropy: 0.300355.\n",
      "episode: 6496   score: 315.0  epsilon: 1.0    steps: 96  evaluation reward: 461.9\n",
      "Training network. lr: 0.000107. clip: 0.042713\n",
      "Iteration 18658: Policy loss: 0.320377. Value loss: 0.100793. Entropy: 0.303974.\n",
      "Iteration 18659: Policy loss: 0.316192. Value loss: 0.051681. Entropy: 0.304377.\n",
      "Iteration 18660: Policy loss: 0.317332. Value loss: 0.041113. Entropy: 0.303402.\n",
      "Training network. lr: 0.000107. clip: 0.042713\n",
      "Iteration 18661: Policy loss: 0.025631. Value loss: 0.154661. Entropy: 0.313162.\n",
      "Iteration 18662: Policy loss: 0.019986. Value loss: 0.073672. Entropy: 0.313814.\n",
      "Iteration 18663: Policy loss: 0.018208. Value loss: 0.052628. Entropy: 0.313273.\n",
      "episode: 6497   score: 210.0  epsilon: 1.0    steps: 696  evaluation reward: 459.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000107. clip: 0.042713\n",
      "Iteration 18664: Policy loss: 0.177821. Value loss: 0.126782. Entropy: 0.304207.\n",
      "Iteration 18665: Policy loss: 0.178261. Value loss: 0.057589. Entropy: 0.304370.\n",
      "Iteration 18666: Policy loss: 0.175676. Value loss: 0.041383. Entropy: 0.303239.\n",
      "episode: 6498   score: 335.0  epsilon: 1.0    steps: 120  evaluation reward: 458.65\n",
      "episode: 6499   score: 210.0  epsilon: 1.0    steps: 408  evaluation reward: 457.05\n",
      "Training network. lr: 0.000107. clip: 0.042713\n",
      "Iteration 18667: Policy loss: -0.068039. Value loss: 0.084112. Entropy: 0.293004.\n",
      "Iteration 18668: Policy loss: -0.071856. Value loss: 0.038510. Entropy: 0.291627.\n",
      "Iteration 18669: Policy loss: -0.070307. Value loss: 0.029858. Entropy: 0.290888.\n",
      "episode: 6500   score: 215.0  epsilon: 1.0    steps: 72  evaluation reward: 453.5\n",
      "now time :  2019-09-06 09:32:41.706618\n",
      "episode: 6501   score: 590.0  epsilon: 1.0    steps: 896  evaluation reward: 452.9\n",
      "Training network. lr: 0.000107. clip: 0.042713\n",
      "Iteration 18670: Policy loss: -0.181722. Value loss: 0.101949. Entropy: 0.290947.\n",
      "Iteration 18671: Policy loss: -0.185401. Value loss: 0.050445. Entropy: 0.288897.\n",
      "Iteration 18672: Policy loss: -0.190243. Value loss: 0.038419. Entropy: 0.288314.\n",
      "Training network. lr: 0.000107. clip: 0.042713\n",
      "Iteration 18673: Policy loss: -0.059852. Value loss: 0.121394. Entropy: 0.298912.\n",
      "Iteration 18674: Policy loss: -0.059972. Value loss: 0.044256. Entropy: 0.299288.\n",
      "Iteration 18675: Policy loss: -0.058505. Value loss: 0.029184. Entropy: 0.299701.\n",
      "episode: 6502   score: 315.0  epsilon: 1.0    steps: 280  evaluation reward: 452.0\n",
      "episode: 6503   score: 360.0  epsilon: 1.0    steps: 472  evaluation reward: 450.75\n",
      "Training network. lr: 0.000107. clip: 0.042713\n",
      "Iteration 18676: Policy loss: 0.254859. Value loss: 0.169687. Entropy: 0.278450.\n",
      "Iteration 18677: Policy loss: 0.248318. Value loss: 0.055897. Entropy: 0.278887.\n",
      "Iteration 18678: Policy loss: 0.251420. Value loss: 0.037892. Entropy: 0.278118.\n",
      "Training network. lr: 0.000107. clip: 0.042713\n",
      "Iteration 18679: Policy loss: 0.102996. Value loss: 0.145668. Entropy: 0.310815.\n",
      "Iteration 18680: Policy loss: 0.095167. Value loss: 0.064276. Entropy: 0.309929.\n",
      "Iteration 18681: Policy loss: 0.087085. Value loss: 0.046718. Entropy: 0.310141.\n",
      "episode: 6504   score: 210.0  epsilon: 1.0    steps: 424  evaluation reward: 449.7\n",
      "Training network. lr: 0.000107. clip: 0.042713\n",
      "Iteration 18682: Policy loss: 0.024086. Value loss: 0.087125. Entropy: 0.300462.\n",
      "Iteration 18683: Policy loss: 0.019349. Value loss: 0.039954. Entropy: 0.298873.\n",
      "Iteration 18684: Policy loss: 0.015912. Value loss: 0.032359. Entropy: 0.300223.\n",
      "Training network. lr: 0.000107. clip: 0.042713\n",
      "Iteration 18685: Policy loss: -0.008621. Value loss: 0.154812. Entropy: 0.310583.\n",
      "Iteration 18686: Policy loss: -0.006457. Value loss: 0.062434. Entropy: 0.309462.\n",
      "Iteration 18687: Policy loss: -0.012242. Value loss: 0.038467. Entropy: 0.309890.\n",
      "episode: 6505   score: 435.0  epsilon: 1.0    steps: 200  evaluation reward: 449.55\n",
      "episode: 6506   score: 430.0  epsilon: 1.0    steps: 296  evaluation reward: 451.25\n",
      "episode: 6507   score: 520.0  epsilon: 1.0    steps: 616  evaluation reward: 453.15\n",
      "Training network. lr: 0.000107. clip: 0.042713\n",
      "Iteration 18688: Policy loss: 0.187340. Value loss: 0.096136. Entropy: 0.280740.\n",
      "Iteration 18689: Policy loss: 0.172905. Value loss: 0.048739. Entropy: 0.279061.\n",
      "Iteration 18690: Policy loss: 0.176713. Value loss: 0.037189. Entropy: 0.279783.\n",
      "episode: 6508   score: 195.0  epsilon: 1.0    steps: 496  evaluation reward: 451.55\n",
      "Training network. lr: 0.000107. clip: 0.042713\n",
      "Iteration 18691: Policy loss: 0.162784. Value loss: 0.105181. Entropy: 0.303517.\n",
      "Iteration 18692: Policy loss: 0.158010. Value loss: 0.052419. Entropy: 0.303867.\n",
      "Iteration 18693: Policy loss: 0.154815. Value loss: 0.039954. Entropy: 0.305043.\n",
      "Training network. lr: 0.000107. clip: 0.042713\n",
      "Iteration 18694: Policy loss: 0.130181. Value loss: 0.083809. Entropy: 0.307971.\n",
      "Iteration 18695: Policy loss: 0.127238. Value loss: 0.047188. Entropy: 0.308661.\n",
      "Iteration 18696: Policy loss: 0.119425. Value loss: 0.038180. Entropy: 0.308292.\n",
      "Training network. lr: 0.000107. clip: 0.042713\n",
      "Iteration 18697: Policy loss: 0.089965. Value loss: 0.073008. Entropy: 0.308807.\n",
      "Iteration 18698: Policy loss: 0.089939. Value loss: 0.032406. Entropy: 0.309960.\n",
      "Iteration 18699: Policy loss: 0.091434. Value loss: 0.025793. Entropy: 0.309502.\n",
      "episode: 6509   score: 240.0  epsilon: 1.0    steps: 496  evaluation reward: 451.35\n",
      "episode: 6510   score: 405.0  epsilon: 1.0    steps: 776  evaluation reward: 451.8\n",
      "Training network. lr: 0.000107. clip: 0.042713\n",
      "Iteration 18700: Policy loss: 0.093516. Value loss: 0.088483. Entropy: 0.293217.\n",
      "Iteration 18701: Policy loss: 0.085731. Value loss: 0.053019. Entropy: 0.292936.\n",
      "Iteration 18702: Policy loss: 0.086600. Value loss: 0.042722. Entropy: 0.293190.\n",
      "episode: 6511   score: 295.0  epsilon: 1.0    steps: 72  evaluation reward: 451.25\n",
      "episode: 6512   score: 485.0  epsilon: 1.0    steps: 728  evaluation reward: 449.75\n",
      "Training network. lr: 0.000106. clip: 0.042557\n",
      "Iteration 18703: Policy loss: -0.070399. Value loss: 0.268230. Entropy: 0.292473.\n",
      "Iteration 18704: Policy loss: -0.075649. Value loss: 0.209922. Entropy: 0.292336.\n",
      "Iteration 18705: Policy loss: -0.084498. Value loss: 0.189648. Entropy: 0.290813.\n",
      "episode: 6513   score: 210.0  epsilon: 1.0    steps: 176  evaluation reward: 449.25\n",
      "episode: 6514   score: 190.0  epsilon: 1.0    steps: 280  evaluation reward: 447.85\n",
      "Training network. lr: 0.000106. clip: 0.042557\n",
      "Iteration 18706: Policy loss: 0.183032. Value loss: 0.060301. Entropy: 0.297222.\n",
      "Iteration 18707: Policy loss: 0.181078. Value loss: 0.036576. Entropy: 0.296766.\n",
      "Iteration 18708: Policy loss: 0.177316. Value loss: 0.031738. Entropy: 0.296867.\n",
      "Training network. lr: 0.000106. clip: 0.042557\n",
      "Iteration 18709: Policy loss: 0.071977. Value loss: 0.092341. Entropy: 0.315666.\n",
      "Iteration 18710: Policy loss: 0.068927. Value loss: 0.038782. Entropy: 0.315268.\n",
      "Iteration 18711: Policy loss: 0.065064. Value loss: 0.029112. Entropy: 0.315262.\n",
      "episode: 6515   score: 445.0  epsilon: 1.0    steps: 80  evaluation reward: 448.85\n",
      "Training network. lr: 0.000106. clip: 0.042557\n",
      "Iteration 18712: Policy loss: 0.038462. Value loss: 0.110890. Entropy: 0.303970.\n",
      "Iteration 18713: Policy loss: 0.038610. Value loss: 0.056858. Entropy: 0.304217.\n",
      "Iteration 18714: Policy loss: 0.031177. Value loss: 0.044521. Entropy: 0.304005.\n",
      "episode: 6516   score: 330.0  epsilon: 1.0    steps: 408  evaluation reward: 450.05\n",
      "episode: 6517   score: 210.0  epsilon: 1.0    steps: 952  evaluation reward: 446.8\n",
      "Training network. lr: 0.000106. clip: 0.042557\n",
      "Iteration 18715: Policy loss: 0.022756. Value loss: 0.118982. Entropy: 0.301446.\n",
      "Iteration 18716: Policy loss: 0.019376. Value loss: 0.053945. Entropy: 0.303806.\n",
      "Iteration 18717: Policy loss: 0.013591. Value loss: 0.038543. Entropy: 0.301883.\n",
      "episode: 6518   score: 215.0  epsilon: 1.0    steps: 744  evaluation reward: 445.8\n",
      "Training network. lr: 0.000106. clip: 0.042557\n",
      "Iteration 18718: Policy loss: 0.017306. Value loss: 0.073586. Entropy: 0.292417.\n",
      "Iteration 18719: Policy loss: 0.010878. Value loss: 0.040387. Entropy: 0.293778.\n",
      "Iteration 18720: Policy loss: 0.006514. Value loss: 0.030099. Entropy: 0.294075.\n",
      "episode: 6519   score: 210.0  epsilon: 1.0    steps: 136  evaluation reward: 441.5\n",
      "episode: 6520   score: 325.0  epsilon: 1.0    steps: 872  evaluation reward: 442.15\n",
      "Training network. lr: 0.000106. clip: 0.042557\n",
      "Iteration 18721: Policy loss: 0.028773. Value loss: 0.102311. Entropy: 0.292313.\n",
      "Iteration 18722: Policy loss: 0.022840. Value loss: 0.036250. Entropy: 0.292370.\n",
      "Iteration 18723: Policy loss: 0.014859. Value loss: 0.025923. Entropy: 0.291641.\n",
      "episode: 6521   score: 240.0  epsilon: 1.0    steps: 240  evaluation reward: 437.8\n",
      "Training network. lr: 0.000106. clip: 0.042557\n",
      "Iteration 18724: Policy loss: -0.102382. Value loss: 0.100365. Entropy: 0.295282.\n",
      "Iteration 18725: Policy loss: -0.104620. Value loss: 0.048238. Entropy: 0.294980.\n",
      "Iteration 18726: Policy loss: -0.113622. Value loss: 0.034011. Entropy: 0.294703.\n",
      "Training network. lr: 0.000106. clip: 0.042557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18727: Policy loss: 0.020943. Value loss: 0.106732. Entropy: 0.316112.\n",
      "Iteration 18728: Policy loss: 0.016208. Value loss: 0.044532. Entropy: 0.316502.\n",
      "Iteration 18729: Policy loss: 0.011168. Value loss: 0.031825. Entropy: 0.315446.\n",
      "episode: 6522   score: 385.0  epsilon: 1.0    steps: 256  evaluation reward: 430.6\n",
      "Training network. lr: 0.000106. clip: 0.042557\n",
      "Iteration 18730: Policy loss: -0.031302. Value loss: 0.095165. Entropy: 0.295838.\n",
      "Iteration 18731: Policy loss: -0.032066. Value loss: 0.042407. Entropy: 0.295011.\n",
      "Iteration 18732: Policy loss: -0.037245. Value loss: 0.031924. Entropy: 0.295295.\n",
      "episode: 6523   score: 260.0  epsilon: 1.0    steps: 808  evaluation reward: 430.05\n",
      "Training network. lr: 0.000106. clip: 0.042557\n",
      "Iteration 18733: Policy loss: 0.061813. Value loss: 0.113301. Entropy: 0.309177.\n",
      "Iteration 18734: Policy loss: 0.050745. Value loss: 0.061156. Entropy: 0.307426.\n",
      "Iteration 18735: Policy loss: 0.043644. Value loss: 0.050510. Entropy: 0.307692.\n",
      "episode: 6524   score: 315.0  epsilon: 1.0    steps: 56  evaluation reward: 428.6\n",
      "episode: 6525   score: 420.0  epsilon: 1.0    steps: 480  evaluation reward: 425.25\n",
      "Training network. lr: 0.000106. clip: 0.042557\n",
      "Iteration 18736: Policy loss: -0.053028. Value loss: 0.087560. Entropy: 0.287377.\n",
      "Iteration 18737: Policy loss: -0.054973. Value loss: 0.051243. Entropy: 0.287398.\n",
      "Iteration 18738: Policy loss: -0.056762. Value loss: 0.038690. Entropy: 0.289416.\n",
      "episode: 6526   score: 330.0  epsilon: 1.0    steps: 240  evaluation reward: 427.0\n",
      "Training network. lr: 0.000106. clip: 0.042557\n",
      "Iteration 18739: Policy loss: 0.084229. Value loss: 0.090204. Entropy: 0.305489.\n",
      "Iteration 18740: Policy loss: 0.087873. Value loss: 0.036794. Entropy: 0.304386.\n",
      "Iteration 18741: Policy loss: 0.088050. Value loss: 0.026713. Entropy: 0.304216.\n",
      "episode: 6527   score: 285.0  epsilon: 1.0    steps: 976  evaluation reward: 426.3\n",
      "Training network. lr: 0.000106. clip: 0.042557\n",
      "Iteration 18742: Policy loss: -0.038720. Value loss: 0.113245. Entropy: 0.309344.\n",
      "Iteration 18743: Policy loss: -0.048997. Value loss: 0.053491. Entropy: 0.308988.\n",
      "Iteration 18744: Policy loss: -0.054377. Value loss: 0.040098. Entropy: 0.309427.\n",
      "episode: 6528   score: 315.0  epsilon: 1.0    steps: 32  evaluation reward: 423.05\n",
      "Training network. lr: 0.000106. clip: 0.042557\n",
      "Iteration 18745: Policy loss: -0.034503. Value loss: 0.082113. Entropy: 0.301823.\n",
      "Iteration 18746: Policy loss: -0.038278. Value loss: 0.046848. Entropy: 0.300116.\n",
      "Iteration 18747: Policy loss: -0.036662. Value loss: 0.034034. Entropy: 0.300924.\n",
      "episode: 6529   score: 330.0  epsilon: 1.0    steps: 640  evaluation reward: 421.7\n",
      "episode: 6530   score: 210.0  epsilon: 1.0    steps: 744  evaluation reward: 418.2\n",
      "Training network. lr: 0.000106. clip: 0.042557\n",
      "Iteration 18748: Policy loss: 0.075224. Value loss: 0.084057. Entropy: 0.302813.\n",
      "Iteration 18749: Policy loss: 0.069014. Value loss: 0.040991. Entropy: 0.300764.\n",
      "Iteration 18750: Policy loss: 0.069775. Value loss: 0.031168. Entropy: 0.300807.\n",
      "episode: 6531   score: 210.0  epsilon: 1.0    steps: 584  evaluation reward: 416.4\n",
      "Training network. lr: 0.000106. clip: 0.042409\n",
      "Iteration 18751: Policy loss: -0.150309. Value loss: 0.366017. Entropy: 0.303831.\n",
      "Iteration 18752: Policy loss: -0.159752. Value loss: 0.248947. Entropy: 0.302074.\n",
      "Iteration 18753: Policy loss: -0.141425. Value loss: 0.184737. Entropy: 0.301755.\n",
      "Training network. lr: 0.000106. clip: 0.042409\n",
      "Iteration 18754: Policy loss: 0.199984. Value loss: 0.165799. Entropy: 0.320455.\n",
      "Iteration 18755: Policy loss: 0.195996. Value loss: 0.069759. Entropy: 0.320898.\n",
      "Iteration 18756: Policy loss: 0.190950. Value loss: 0.045722. Entropy: 0.320626.\n",
      "episode: 6532   score: 390.0  epsilon: 1.0    steps: 344  evaluation reward: 417.35\n",
      "episode: 6533   score: 85.0  epsilon: 1.0    steps: 664  evaluation reward: 413.65\n",
      "Training network. lr: 0.000106. clip: 0.042409\n",
      "Iteration 18757: Policy loss: 0.079415. Value loss: 0.152963. Entropy: 0.296078.\n",
      "Iteration 18758: Policy loss: 0.077055. Value loss: 0.074277. Entropy: 0.297709.\n",
      "Iteration 18759: Policy loss: 0.080262. Value loss: 0.052520. Entropy: 0.298040.\n",
      "episode: 6534   score: 600.0  epsilon: 1.0    steps: 264  evaluation reward: 415.15\n",
      "episode: 6535   score: 260.0  epsilon: 1.0    steps: 688  evaluation reward: 410.4\n",
      "episode: 6536   score: 210.0  epsilon: 1.0    steps: 800  evaluation reward: 407.35\n",
      "Training network. lr: 0.000106. clip: 0.042409\n",
      "Iteration 18760: Policy loss: 0.198206. Value loss: 0.177981. Entropy: 0.288840.\n",
      "Iteration 18761: Policy loss: 0.199325. Value loss: 0.078382. Entropy: 0.290327.\n",
      "Iteration 18762: Policy loss: 0.195052. Value loss: 0.055984. Entropy: 0.288855.\n",
      "Training network. lr: 0.000106. clip: 0.042409\n",
      "Iteration 18763: Policy loss: 0.040010. Value loss: 0.093386. Entropy: 0.317064.\n",
      "Iteration 18764: Policy loss: 0.033382. Value loss: 0.051115. Entropy: 0.316983.\n",
      "Iteration 18765: Policy loss: 0.032045. Value loss: 0.038113. Entropy: 0.316759.\n",
      "episode: 6537   score: 490.0  epsilon: 1.0    steps: 624  evaluation reward: 406.4\n",
      "Training network. lr: 0.000106. clip: 0.042409\n",
      "Iteration 18766: Policy loss: 0.081319. Value loss: 0.123238. Entropy: 0.307880.\n",
      "Iteration 18767: Policy loss: 0.084376. Value loss: 0.053584. Entropy: 0.306844.\n",
      "Iteration 18768: Policy loss: 0.083026. Value loss: 0.037860. Entropy: 0.307110.\n",
      "episode: 6538   score: 315.0  epsilon: 1.0    steps: 368  evaluation reward: 401.85\n",
      "episode: 6539   score: 260.0  epsilon: 1.0    steps: 664  evaluation reward: 403.5\n",
      "Training network. lr: 0.000106. clip: 0.042409\n",
      "Iteration 18769: Policy loss: 0.052436. Value loss: 0.086244. Entropy: 0.279544.\n",
      "Iteration 18770: Policy loss: 0.045581. Value loss: 0.051315. Entropy: 0.279792.\n",
      "Iteration 18771: Policy loss: 0.044357. Value loss: 0.039992. Entropy: 0.281498.\n",
      "episode: 6540   score: 240.0  epsilon: 1.0    steps: 688  evaluation reward: 401.7\n",
      "Training network. lr: 0.000106. clip: 0.042409\n",
      "Iteration 18772: Policy loss: -0.193697. Value loss: 0.089037. Entropy: 0.305416.\n",
      "Iteration 18773: Policy loss: -0.198580. Value loss: 0.041021. Entropy: 0.304496.\n",
      "Iteration 18774: Policy loss: -0.200508. Value loss: 0.031721. Entropy: 0.305598.\n",
      "Training network. lr: 0.000106. clip: 0.042409\n",
      "Iteration 18775: Policy loss: -0.060921. Value loss: 0.124089. Entropy: 0.309028.\n",
      "Iteration 18776: Policy loss: -0.058582. Value loss: 0.057921. Entropy: 0.308955.\n",
      "Iteration 18777: Policy loss: -0.062481. Value loss: 0.041863. Entropy: 0.308475.\n",
      "Training network. lr: 0.000106. clip: 0.042409\n",
      "Iteration 18778: Policy loss: -0.035845. Value loss: 0.093986. Entropy: 0.309405.\n",
      "Iteration 18779: Policy loss: -0.043165. Value loss: 0.042883. Entropy: 0.309649.\n",
      "Iteration 18780: Policy loss: -0.041075. Value loss: 0.032587. Entropy: 0.309430.\n",
      "episode: 6541   score: 315.0  epsilon: 1.0    steps: 192  evaluation reward: 398.95\n",
      "Training network. lr: 0.000106. clip: 0.042409\n",
      "Iteration 18781: Policy loss: 0.083668. Value loss: 0.068907. Entropy: 0.300855.\n",
      "Iteration 18782: Policy loss: 0.075315. Value loss: 0.035703. Entropy: 0.301347.\n",
      "Iteration 18783: Policy loss: 0.068641. Value loss: 0.025911. Entropy: 0.301498.\n",
      "episode: 6542   score: 620.0  epsilon: 1.0    steps: 800  evaluation reward: 403.05\n",
      "Training network. lr: 0.000106. clip: 0.042409\n",
      "Iteration 18784: Policy loss: -0.131356. Value loss: 0.240950. Entropy: 0.307798.\n",
      "Iteration 18785: Policy loss: -0.139296. Value loss: 0.172822. Entropy: 0.307483.\n",
      "Iteration 18786: Policy loss: -0.146156. Value loss: 0.157504. Entropy: 0.308221.\n",
      "Training network. lr: 0.000106. clip: 0.042409\n",
      "Iteration 18787: Policy loss: 0.156766. Value loss: 0.137660. Entropy: 0.310758.\n",
      "Iteration 18788: Policy loss: 0.145911. Value loss: 0.058230. Entropy: 0.310276.\n",
      "Iteration 18789: Policy loss: 0.146645. Value loss: 0.039619. Entropy: 0.310803.\n",
      "episode: 6543   score: 315.0  epsilon: 1.0    steps: 192  evaluation reward: 405.15\n",
      "episode: 6544   score: 420.0  epsilon: 1.0    steps: 288  evaluation reward: 402.7\n",
      "episode: 6545   score: 420.0  epsilon: 1.0    steps: 392  evaluation reward: 403.7\n",
      "episode: 6546   score: 320.0  epsilon: 1.0    steps: 552  evaluation reward: 399.95\n",
      "episode: 6547   score: 460.0  epsilon: 1.0    steps: 768  evaluation reward: 402.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000106. clip: 0.042409\n",
      "Iteration 18790: Policy loss: -0.159212. Value loss: 0.340302. Entropy: 0.276981.\n",
      "Iteration 18791: Policy loss: -0.157724. Value loss: 0.246429. Entropy: 0.275008.\n",
      "Iteration 18792: Policy loss: -0.172857. Value loss: 0.191630. Entropy: 0.275487.\n",
      "Training network. lr: 0.000106. clip: 0.042409\n",
      "Iteration 18793: Policy loss: -0.024465. Value loss: 0.119541. Entropy: 0.309160.\n",
      "Iteration 18794: Policy loss: -0.031554. Value loss: 0.053574. Entropy: 0.308107.\n",
      "Iteration 18795: Policy loss: -0.029857. Value loss: 0.039447. Entropy: 0.308163.\n",
      "episode: 6548   score: 565.0  epsilon: 1.0    steps: 544  evaluation reward: 406.3\n",
      "Training network. lr: 0.000106. clip: 0.042409\n",
      "Iteration 18796: Policy loss: 0.201153. Value loss: 0.136642. Entropy: 0.308924.\n",
      "Iteration 18797: Policy loss: 0.194008. Value loss: 0.078293. Entropy: 0.306138.\n",
      "Iteration 18798: Policy loss: 0.197624. Value loss: 0.050360. Entropy: 0.306804.\n",
      "Training network. lr: 0.000106. clip: 0.042409\n",
      "Iteration 18799: Policy loss: -0.138872. Value loss: 0.285495. Entropy: 0.315492.\n",
      "Iteration 18800: Policy loss: -0.150331. Value loss: 0.222968. Entropy: 0.314824.\n",
      "Iteration 18801: Policy loss: -0.152721. Value loss: 0.184100. Entropy: 0.314941.\n",
      "episode: 6549   score: 540.0  epsilon: 1.0    steps: 448  evaluation reward: 405.75\n",
      "Training network. lr: 0.000106. clip: 0.042253\n",
      "Iteration 18802: Policy loss: -0.064695. Value loss: 0.067852. Entropy: 0.300413.\n",
      "Iteration 18803: Policy loss: -0.066139. Value loss: 0.036379. Entropy: 0.299336.\n",
      "Iteration 18804: Policy loss: -0.071104. Value loss: 0.028622. Entropy: 0.300126.\n",
      "Training network. lr: 0.000106. clip: 0.042253\n",
      "Iteration 18805: Policy loss: 0.069219. Value loss: 0.145429. Entropy: 0.310835.\n",
      "Iteration 18806: Policy loss: 0.055225. Value loss: 0.067170. Entropy: 0.311281.\n",
      "Iteration 18807: Policy loss: 0.057404. Value loss: 0.047014. Entropy: 0.310876.\n",
      "episode: 6550   score: 420.0  epsilon: 1.0    steps: 480  evaluation reward: 405.9\n",
      "Training network. lr: 0.000106. clip: 0.042253\n",
      "Iteration 18808: Policy loss: -0.008970. Value loss: 0.138766. Entropy: 0.305344.\n",
      "Iteration 18809: Policy loss: -0.014418. Value loss: 0.058916. Entropy: 0.305574.\n",
      "Iteration 18810: Policy loss: -0.020574. Value loss: 0.040804. Entropy: 0.305524.\n",
      "now time :  2019-09-06 09:41:13.305315\n",
      "episode: 6551   score: 315.0  epsilon: 1.0    steps: 168  evaluation reward: 405.7\n",
      "episode: 6552   score: 350.0  epsilon: 1.0    steps: 240  evaluation reward: 404.2\n",
      "episode: 6553   score: 330.0  epsilon: 1.0    steps: 936  evaluation reward: 403.1\n",
      "Training network. lr: 0.000106. clip: 0.042253\n",
      "Iteration 18811: Policy loss: 0.169585. Value loss: 0.108371. Entropy: 0.289220.\n",
      "Iteration 18812: Policy loss: 0.162149. Value loss: 0.050222. Entropy: 0.290273.\n",
      "Iteration 18813: Policy loss: 0.160932. Value loss: 0.036741. Entropy: 0.288661.\n",
      "Training network. lr: 0.000106. clip: 0.042253\n",
      "Iteration 18814: Policy loss: 0.033492. Value loss: 0.137945. Entropy: 0.308199.\n",
      "Iteration 18815: Policy loss: 0.028610. Value loss: 0.066156. Entropy: 0.308954.\n",
      "Iteration 18816: Policy loss: 0.023626. Value loss: 0.045282. Entropy: 0.307970.\n",
      "episode: 6554   score: 465.0  epsilon: 1.0    steps: 32  evaluation reward: 401.25\n",
      "Training network. lr: 0.000106. clip: 0.042253\n",
      "Iteration 18817: Policy loss: -0.027186. Value loss: 0.118267. Entropy: 0.306171.\n",
      "Iteration 18818: Policy loss: -0.028184. Value loss: 0.044818. Entropy: 0.305367.\n",
      "Iteration 18819: Policy loss: -0.032499. Value loss: 0.028482. Entropy: 0.305825.\n",
      "episode: 6555   score: 380.0  epsilon: 1.0    steps: 72  evaluation reward: 402.1\n",
      "episode: 6556   score: 240.0  epsilon: 1.0    steps: 256  evaluation reward: 398.1\n",
      "episode: 6557   score: 585.0  epsilon: 1.0    steps: 280  evaluation reward: 401.65\n",
      "Training network. lr: 0.000106. clip: 0.042253\n",
      "Iteration 18820: Policy loss: 0.214572. Value loss: 0.097064. Entropy: 0.279327.\n",
      "Iteration 18821: Policy loss: 0.214395. Value loss: 0.049614. Entropy: 0.278995.\n",
      "Iteration 18822: Policy loss: 0.206396. Value loss: 0.036471. Entropy: 0.277640.\n",
      "Training network. lr: 0.000106. clip: 0.042253\n",
      "Iteration 18823: Policy loss: 0.069921. Value loss: 0.107485. Entropy: 0.314392.\n",
      "Iteration 18824: Policy loss: 0.067561. Value loss: 0.057242. Entropy: 0.314844.\n",
      "Iteration 18825: Policy loss: 0.063679. Value loss: 0.042155. Entropy: 0.314902.\n",
      "Training network. lr: 0.000106. clip: 0.042253\n",
      "Iteration 18826: Policy loss: 0.063657. Value loss: 0.066670. Entropy: 0.311795.\n",
      "Iteration 18827: Policy loss: 0.061491. Value loss: 0.030947. Entropy: 0.311747.\n",
      "Iteration 18828: Policy loss: 0.058501. Value loss: 0.023594. Entropy: 0.311879.\n",
      "episode: 6558   score: 315.0  epsilon: 1.0    steps: 648  evaluation reward: 395.55\n",
      "Training network. lr: 0.000106. clip: 0.042253\n",
      "Iteration 18829: Policy loss: -0.562416. Value loss: 0.639024. Entropy: 0.302324.\n",
      "Iteration 18830: Policy loss: -0.596192. Value loss: 0.433143. Entropy: 0.304868.\n",
      "Iteration 18831: Policy loss: -0.601008. Value loss: 0.313191. Entropy: 0.305116.\n",
      "episode: 6559   score: 595.0  epsilon: 1.0    steps: 552  evaluation reward: 395.05\n",
      "episode: 6560   score: 390.0  epsilon: 1.0    steps: 840  evaluation reward: 395.55\n",
      "Training network. lr: 0.000106. clip: 0.042253\n",
      "Iteration 18832: Policy loss: 0.159564. Value loss: 0.171198. Entropy: 0.297972.\n",
      "Iteration 18833: Policy loss: 0.160295. Value loss: 0.070211. Entropy: 0.295394.\n",
      "Iteration 18834: Policy loss: 0.153230. Value loss: 0.045679. Entropy: 0.295715.\n",
      "episode: 6561   score: 465.0  epsilon: 1.0    steps: 544  evaluation reward: 394.85\n",
      "Training network. lr: 0.000106. clip: 0.042253\n",
      "Iteration 18835: Policy loss: -0.031957. Value loss: 0.268421. Entropy: 0.301532.\n",
      "Iteration 18836: Policy loss: -0.036264. Value loss: 0.136329. Entropy: 0.298992.\n",
      "Iteration 18837: Policy loss: -0.023553. Value loss: 0.068414. Entropy: 0.298858.\n",
      "episode: 6562   score: 260.0  epsilon: 1.0    steps: 632  evaluation reward: 393.75\n",
      "episode: 6563   score: 515.0  epsilon: 1.0    steps: 824  evaluation reward: 395.25\n",
      "Training network. lr: 0.000106. clip: 0.042253\n",
      "Iteration 18838: Policy loss: 0.245190. Value loss: 0.107744. Entropy: 0.294110.\n",
      "Iteration 18839: Policy loss: 0.243335. Value loss: 0.049657. Entropy: 0.293334.\n",
      "Iteration 18840: Policy loss: 0.226176. Value loss: 0.035399. Entropy: 0.292665.\n",
      "episode: 6564   score: 515.0  epsilon: 1.0    steps: 56  evaluation reward: 392.65\n",
      "Training network. lr: 0.000106. clip: 0.042253\n",
      "Iteration 18841: Policy loss: -0.142741. Value loss: 0.175975. Entropy: 0.301804.\n",
      "Iteration 18842: Policy loss: -0.156082. Value loss: 0.068276. Entropy: 0.301790.\n",
      "Iteration 18843: Policy loss: -0.153340. Value loss: 0.049784. Entropy: 0.302177.\n",
      "Training network. lr: 0.000106. clip: 0.042253\n",
      "Iteration 18844: Policy loss: 0.162583. Value loss: 0.101241. Entropy: 0.315715.\n",
      "Iteration 18845: Policy loss: 0.156357. Value loss: 0.055171. Entropy: 0.314546.\n",
      "Iteration 18846: Policy loss: 0.160041. Value loss: 0.040864. Entropy: 0.314947.\n",
      "Training network. lr: 0.000106. clip: 0.042253\n",
      "Iteration 18847: Policy loss: 0.020942. Value loss: 0.145106. Entropy: 0.310031.\n",
      "Iteration 18848: Policy loss: 0.013684. Value loss: 0.059042. Entropy: 0.309577.\n",
      "Iteration 18849: Policy loss: 0.013216. Value loss: 0.038256. Entropy: 0.310188.\n",
      "episode: 6565   score: 520.0  epsilon: 1.0    steps: 232  evaluation reward: 391.4\n",
      "Training network. lr: 0.000106. clip: 0.042253\n",
      "Iteration 18850: Policy loss: -0.047988. Value loss: 0.245022. Entropy: 0.302652.\n",
      "Iteration 18851: Policy loss: -0.045134. Value loss: 0.176194. Entropy: 0.304260.\n",
      "Iteration 18852: Policy loss: -0.061068. Value loss: 0.154229. Entropy: 0.304092.\n",
      "episode: 6566   score: 290.0  epsilon: 1.0    steps: 56  evaluation reward: 391.15\n",
      "Training network. lr: 0.000105. clip: 0.042096\n",
      "Iteration 18853: Policy loss: 0.025290. Value loss: 0.130266. Entropy: 0.297191.\n",
      "Iteration 18854: Policy loss: 0.022668. Value loss: 0.054267. Entropy: 0.298395.\n",
      "Iteration 18855: Policy loss: 0.015021. Value loss: 0.037396. Entropy: 0.297061.\n",
      "episode: 6567   score: 400.0  epsilon: 1.0    steps: 24  evaluation reward: 392.2\n",
      "Training network. lr: 0.000105. clip: 0.042096\n",
      "Iteration 18856: Policy loss: 0.062739. Value loss: 0.118204. Entropy: 0.306204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18857: Policy loss: 0.061204. Value loss: 0.057952. Entropy: 0.307490.\n",
      "Iteration 18858: Policy loss: 0.056946. Value loss: 0.041309. Entropy: 0.306058.\n",
      "episode: 6568   score: 470.0  epsilon: 1.0    steps: 376  evaluation reward: 390.35\n",
      "episode: 6569   score: 315.0  epsilon: 1.0    steps: 376  evaluation reward: 390.05\n",
      "Training network. lr: 0.000105. clip: 0.042096\n",
      "Iteration 18859: Policy loss: 0.275150. Value loss: 0.110751. Entropy: 0.291005.\n",
      "Iteration 18860: Policy loss: 0.277391. Value loss: 0.052934. Entropy: 0.289989.\n",
      "Iteration 18861: Policy loss: 0.268948. Value loss: 0.035777. Entropy: 0.290936.\n",
      "episode: 6570   score: 320.0  epsilon: 1.0    steps: 560  evaluation reward: 387.0\n",
      "episode: 6571   score: 500.0  epsilon: 1.0    steps: 904  evaluation reward: 384.15\n",
      "episode: 6572   score: 390.0  epsilon: 1.0    steps: 904  evaluation reward: 385.35\n",
      "Training network. lr: 0.000105. clip: 0.042096\n",
      "Iteration 18862: Policy loss: 0.133004. Value loss: 0.107652. Entropy: 0.295675.\n",
      "Iteration 18863: Policy loss: 0.125167. Value loss: 0.048065. Entropy: 0.296575.\n",
      "Iteration 18864: Policy loss: 0.120003. Value loss: 0.037616. Entropy: 0.295235.\n",
      "Training network. lr: 0.000105. clip: 0.042096\n",
      "Iteration 18865: Policy loss: 0.117945. Value loss: 0.132752. Entropy: 0.309174.\n",
      "Iteration 18866: Policy loss: 0.106438. Value loss: 0.061349. Entropy: 0.309096.\n",
      "Iteration 18867: Policy loss: 0.112625. Value loss: 0.045536. Entropy: 0.308106.\n",
      "Training network. lr: 0.000105. clip: 0.042096\n",
      "Iteration 18868: Policy loss: -0.236074. Value loss: 0.220634. Entropy: 0.315686.\n",
      "Iteration 18869: Policy loss: -0.243097. Value loss: 0.105323. Entropy: 0.314661.\n",
      "Iteration 18870: Policy loss: -0.222115. Value loss: 0.059216. Entropy: 0.314724.\n",
      "episode: 6573   score: 285.0  epsilon: 1.0    steps: 272  evaluation reward: 382.15\n",
      "Training network. lr: 0.000105. clip: 0.042096\n",
      "Iteration 18871: Policy loss: -0.296292. Value loss: 0.239832. Entropy: 0.303985.\n",
      "Iteration 18872: Policy loss: -0.311328. Value loss: 0.077097. Entropy: 0.305132.\n",
      "Iteration 18873: Policy loss: -0.314325. Value loss: 0.047828. Entropy: 0.304420.\n",
      "episode: 6574   score: 230.0  epsilon: 1.0    steps: 192  evaluation reward: 378.8\n",
      "episode: 6575   score: 365.0  epsilon: 1.0    steps: 640  evaluation reward: 375.95\n",
      "Training network. lr: 0.000105. clip: 0.042096\n",
      "Iteration 18874: Policy loss: -0.055067. Value loss: 0.283752. Entropy: 0.295214.\n",
      "Iteration 18875: Policy loss: -0.068440. Value loss: 0.186391. Entropy: 0.295261.\n",
      "Iteration 18876: Policy loss: -0.047614. Value loss: 0.141296. Entropy: 0.295630.\n",
      "Training network. lr: 0.000105. clip: 0.042096\n",
      "Iteration 18877: Policy loss: -0.001037. Value loss: 0.164572. Entropy: 0.318219.\n",
      "Iteration 18878: Policy loss: -0.008967. Value loss: 0.070719. Entropy: 0.318011.\n",
      "Iteration 18879: Policy loss: -0.012018. Value loss: 0.045926. Entropy: 0.319260.\n",
      "episode: 6576   score: 585.0  epsilon: 1.0    steps: 960  evaluation reward: 378.2\n",
      "Training network. lr: 0.000105. clip: 0.042096\n",
      "Iteration 18880: Policy loss: -0.215320. Value loss: 0.192171. Entropy: 0.313045.\n",
      "Iteration 18881: Policy loss: -0.214392. Value loss: 0.085222. Entropy: 0.312482.\n",
      "Iteration 18882: Policy loss: -0.217896. Value loss: 0.063289. Entropy: 0.313218.\n",
      "Training network. lr: 0.000105. clip: 0.042096\n",
      "Iteration 18883: Policy loss: 0.143201. Value loss: 0.126015. Entropy: 0.309240.\n",
      "Iteration 18884: Policy loss: 0.136587. Value loss: 0.052009. Entropy: 0.308101.\n",
      "Iteration 18885: Policy loss: 0.130261. Value loss: 0.036453. Entropy: 0.308008.\n",
      "episode: 6577   score: 530.0  epsilon: 1.0    steps: 344  evaluation reward: 379.7\n",
      "episode: 6578   score: 590.0  epsilon: 1.0    steps: 496  evaluation reward: 381.65\n",
      "episode: 6579   score: 615.0  epsilon: 1.0    steps: 528  evaluation reward: 384.2\n",
      "episode: 6580   score: 450.0  epsilon: 1.0    steps: 544  evaluation reward: 382.3\n",
      "Training network. lr: 0.000105. clip: 0.042096\n",
      "Iteration 18886: Policy loss: 0.111034. Value loss: 0.130111. Entropy: 0.274603.\n",
      "Iteration 18887: Policy loss: 0.106056. Value loss: 0.071046. Entropy: 0.273167.\n",
      "Iteration 18888: Policy loss: 0.106818. Value loss: 0.048584. Entropy: 0.274613.\n",
      "Training network. lr: 0.000105. clip: 0.042096\n",
      "Iteration 18889: Policy loss: -0.230344. Value loss: 0.333931. Entropy: 0.313148.\n",
      "Iteration 18890: Policy loss: -0.232081. Value loss: 0.227315. Entropy: 0.315110.\n",
      "Iteration 18891: Policy loss: -0.253751. Value loss: 0.130619. Entropy: 0.312092.\n",
      "episode: 6581   score: 315.0  epsilon: 1.0    steps: 912  evaluation reward: 376.5\n",
      "episode: 6582   score: 290.0  epsilon: 1.0    steps: 984  evaluation reward: 377.3\n",
      "Training network. lr: 0.000105. clip: 0.042096\n",
      "Iteration 18892: Policy loss: 0.219661. Value loss: 0.156270. Entropy: 0.307902.\n",
      "Iteration 18893: Policy loss: 0.213083. Value loss: 0.073885. Entropy: 0.307624.\n",
      "Iteration 18894: Policy loss: 0.216584. Value loss: 0.055163. Entropy: 0.307435.\n",
      "Training network. lr: 0.000105. clip: 0.042096\n",
      "Iteration 18895: Policy loss: 0.176947. Value loss: 0.093784. Entropy: 0.305001.\n",
      "Iteration 18896: Policy loss: 0.179372. Value loss: 0.031301. Entropy: 0.304812.\n",
      "Iteration 18897: Policy loss: 0.174378. Value loss: 0.023657. Entropy: 0.304143.\n",
      "Training network. lr: 0.000105. clip: 0.042096\n",
      "Iteration 18898: Policy loss: 0.117336. Value loss: 0.081690. Entropy: 0.318549.\n",
      "Iteration 18899: Policy loss: 0.113230. Value loss: 0.043641. Entropy: 0.317859.\n",
      "Iteration 18900: Policy loss: 0.117304. Value loss: 0.036280. Entropy: 0.317616.\n",
      "episode: 6583   score: 655.0  epsilon: 1.0    steps: 120  evaluation reward: 376.8\n",
      "Training network. lr: 0.000105. clip: 0.041948\n",
      "Iteration 18901: Policy loss: -0.032126. Value loss: 0.125709. Entropy: 0.305897.\n",
      "Iteration 18902: Policy loss: -0.038955. Value loss: 0.055737. Entropy: 0.304959.\n",
      "Iteration 18903: Policy loss: -0.039851. Value loss: 0.043527. Entropy: 0.306540.\n",
      "episode: 6584   score: 230.0  epsilon: 1.0    steps: 384  evaluation reward: 373.7\n",
      "episode: 6585   score: 315.0  epsilon: 1.0    steps: 928  evaluation reward: 370.5\n",
      "episode: 6586   score: 270.0  epsilon: 1.0    steps: 1024  evaluation reward: 369.7\n",
      "Training network. lr: 0.000105. clip: 0.041948\n",
      "Iteration 18904: Policy loss: -0.010062. Value loss: 0.120682. Entropy: 0.303758.\n",
      "Iteration 18905: Policy loss: -0.012789. Value loss: 0.057833. Entropy: 0.302228.\n",
      "Iteration 18906: Policy loss: -0.010634. Value loss: 0.039200. Entropy: 0.302621.\n",
      "episode: 6587   score: 330.0  epsilon: 1.0    steps: 312  evaluation reward: 370.85\n",
      "Training network. lr: 0.000105. clip: 0.041948\n",
      "Iteration 18907: Policy loss: 0.388868. Value loss: 0.089734. Entropy: 0.305437.\n",
      "Iteration 18908: Policy loss: 0.382555. Value loss: 0.032551. Entropy: 0.303937.\n",
      "Iteration 18909: Policy loss: 0.382300. Value loss: 0.023730. Entropy: 0.303639.\n",
      "Training network. lr: 0.000105. clip: 0.041948\n",
      "Iteration 18910: Policy loss: 0.023666. Value loss: 0.093324. Entropy: 0.314181.\n",
      "Iteration 18911: Policy loss: 0.025491. Value loss: 0.048676. Entropy: 0.315054.\n",
      "Iteration 18912: Policy loss: 0.022385. Value loss: 0.035815. Entropy: 0.315754.\n",
      "episode: 6588   score: 315.0  epsilon: 1.0    steps: 216  evaluation reward: 370.1\n",
      "Training network. lr: 0.000105. clip: 0.041948\n",
      "Iteration 18913: Policy loss: 0.128709. Value loss: 0.100681. Entropy: 0.312159.\n",
      "Iteration 18914: Policy loss: 0.118047. Value loss: 0.041685. Entropy: 0.310720.\n",
      "Iteration 18915: Policy loss: 0.116692. Value loss: 0.030055. Entropy: 0.310809.\n",
      "episode: 6589   score: 260.0  epsilon: 1.0    steps: 488  evaluation reward: 366.7\n",
      "episode: 6590   score: 495.0  epsilon: 1.0    steps: 848  evaluation reward: 367.6\n",
      "Training network. lr: 0.000105. clip: 0.041948\n",
      "Iteration 18916: Policy loss: 0.203543. Value loss: 0.259833. Entropy: 0.298605.\n",
      "Iteration 18917: Policy loss: 0.195594. Value loss: 0.105231. Entropy: 0.297628.\n",
      "Iteration 18918: Policy loss: 0.190822. Value loss: 0.086708. Entropy: 0.296571.\n",
      "Training network. lr: 0.000105. clip: 0.041948\n",
      "Iteration 18919: Policy loss: -0.226927. Value loss: 0.134941. Entropy: 0.310241.\n",
      "Iteration 18920: Policy loss: -0.234853. Value loss: 0.068458. Entropy: 0.310409.\n",
      "Iteration 18921: Policy loss: -0.229215. Value loss: 0.046273. Entropy: 0.310410.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6591   score: 260.0  epsilon: 1.0    steps: 976  evaluation reward: 364.6\n",
      "Training network. lr: 0.000105. clip: 0.041948\n",
      "Iteration 18922: Policy loss: 0.021304. Value loss: 0.227369. Entropy: 0.312182.\n",
      "Iteration 18923: Policy loss: 0.003377. Value loss: 0.100222. Entropy: 0.312572.\n",
      "Iteration 18924: Policy loss: -0.001392. Value loss: 0.073384. Entropy: 0.312809.\n",
      "episode: 6592   score: 300.0  epsilon: 1.0    steps: 680  evaluation reward: 365.2\n",
      "episode: 6593   score: 365.0  epsilon: 1.0    steps: 816  evaluation reward: 365.3\n",
      "Training network. lr: 0.000105. clip: 0.041948\n",
      "Iteration 18925: Policy loss: -0.010451. Value loss: 0.340564. Entropy: 0.292340.\n",
      "Iteration 18926: Policy loss: -0.013590. Value loss: 0.189287. Entropy: 0.290511.\n",
      "Iteration 18927: Policy loss: -0.020433. Value loss: 0.131788. Entropy: 0.291575.\n",
      "episode: 6594   score: 440.0  epsilon: 1.0    steps: 216  evaluation reward: 365.1\n",
      "episode: 6595   score: 365.0  epsilon: 1.0    steps: 392  evaluation reward: 362.25\n",
      "Training network. lr: 0.000105. clip: 0.041948\n",
      "Iteration 18928: Policy loss: 0.109306. Value loss: 0.100291. Entropy: 0.289369.\n",
      "Iteration 18929: Policy loss: 0.101183. Value loss: 0.057034. Entropy: 0.288395.\n",
      "Iteration 18930: Policy loss: 0.101328. Value loss: 0.045540. Entropy: 0.287680.\n",
      "Training network. lr: 0.000105. clip: 0.041948\n",
      "Iteration 18931: Policy loss: 0.045711. Value loss: 0.159582. Entropy: 0.316239.\n",
      "Iteration 18932: Policy loss: 0.049078. Value loss: 0.057723. Entropy: 0.316722.\n",
      "Iteration 18933: Policy loss: 0.026702. Value loss: 0.040282. Entropy: 0.317270.\n",
      "episode: 6596   score: 305.0  epsilon: 1.0    steps: 320  evaluation reward: 362.15\n",
      "Training network. lr: 0.000105. clip: 0.041948\n",
      "Iteration 18934: Policy loss: -0.137809. Value loss: 0.381022. Entropy: 0.304613.\n",
      "Iteration 18935: Policy loss: -0.147701. Value loss: 0.289822. Entropy: 0.304303.\n",
      "Iteration 18936: Policy loss: -0.153380. Value loss: 0.256250. Entropy: 0.305291.\n",
      "episode: 6597   score: 180.0  epsilon: 1.0    steps: 584  evaluation reward: 361.85\n",
      "Training network. lr: 0.000105. clip: 0.041948\n",
      "Iteration 18937: Policy loss: 0.023961. Value loss: 0.095612. Entropy: 0.306769.\n",
      "Iteration 18938: Policy loss: 0.015662. Value loss: 0.041861. Entropy: 0.307279.\n",
      "Iteration 18939: Policy loss: 0.019174. Value loss: 0.032565. Entropy: 0.307436.\n",
      "episode: 6598   score: 180.0  epsilon: 1.0    steps: 80  evaluation reward: 360.3\n",
      "episode: 6599   score: 495.0  epsilon: 1.0    steps: 640  evaluation reward: 363.15\n",
      "Training network. lr: 0.000105. clip: 0.041948\n",
      "Iteration 18940: Policy loss: -0.008050. Value loss: 0.124341. Entropy: 0.294889.\n",
      "Iteration 18941: Policy loss: -0.001913. Value loss: 0.058794. Entropy: 0.297406.\n",
      "Iteration 18942: Policy loss: -0.012662. Value loss: 0.043278. Entropy: 0.296534.\n",
      "episode: 6600   score: 210.0  epsilon: 1.0    steps: 592  evaluation reward: 363.1\n",
      "now time :  2019-09-06 09:49:17.121973\n",
      "episode: 6601   score: 630.0  epsilon: 1.0    steps: 952  evaluation reward: 363.5\n",
      "Training network. lr: 0.000105. clip: 0.041948\n",
      "Iteration 18943: Policy loss: 0.177547. Value loss: 0.108169. Entropy: 0.305895.\n",
      "Iteration 18944: Policy loss: 0.174277. Value loss: 0.057889. Entropy: 0.305340.\n",
      "Iteration 18945: Policy loss: 0.170600. Value loss: 0.045658. Entropy: 0.305407.\n",
      "Training network. lr: 0.000105. clip: 0.041948\n",
      "Iteration 18946: Policy loss: -0.075674. Value loss: 0.101118. Entropy: 0.312434.\n",
      "Iteration 18947: Policy loss: -0.081011. Value loss: 0.040633. Entropy: 0.313063.\n",
      "Iteration 18948: Policy loss: -0.084179. Value loss: 0.029784. Entropy: 0.311668.\n",
      "episode: 6602   score: 455.0  epsilon: 1.0    steps: 432  evaluation reward: 364.9\n",
      "episode: 6603   score: 190.0  epsilon: 1.0    steps: 736  evaluation reward: 363.2\n",
      "episode: 6604   score: 215.0  epsilon: 1.0    steps: 896  evaluation reward: 363.25\n",
      "Training network. lr: 0.000105. clip: 0.041948\n",
      "Iteration 18949: Policy loss: 0.112337. Value loss: 0.219647. Entropy: 0.298584.\n",
      "Iteration 18950: Policy loss: 0.103814. Value loss: 0.078993. Entropy: 0.299870.\n",
      "Iteration 18951: Policy loss: 0.097509. Value loss: 0.049663. Entropy: 0.297928.\n",
      "episode: 6605   score: 375.0  epsilon: 1.0    steps: 304  evaluation reward: 362.65\n",
      "Training network. lr: 0.000104. clip: 0.041792\n",
      "Iteration 18952: Policy loss: 0.468656. Value loss: 0.152999. Entropy: 0.303843.\n",
      "Iteration 18953: Policy loss: 0.463885. Value loss: 0.063929. Entropy: 0.303597.\n",
      "Iteration 18954: Policy loss: 0.453698. Value loss: 0.042312. Entropy: 0.303095.\n",
      "episode: 6606   score: 135.0  epsilon: 1.0    steps: 760  evaluation reward: 359.7\n",
      "Training network. lr: 0.000104. clip: 0.041792\n",
      "Iteration 18955: Policy loss: 0.028434. Value loss: 0.135640. Entropy: 0.313882.\n",
      "Iteration 18956: Policy loss: 0.022253. Value loss: 0.060042. Entropy: 0.315004.\n",
      "Iteration 18957: Policy loss: 0.015537. Value loss: 0.042781. Entropy: 0.315823.\n",
      "Training network. lr: 0.000104. clip: 0.041792\n",
      "Iteration 18958: Policy loss: 0.164459. Value loss: 0.094404. Entropy: 0.305736.\n",
      "Iteration 18959: Policy loss: 0.164751. Value loss: 0.037473. Entropy: 0.305705.\n",
      "Iteration 18960: Policy loss: 0.159285. Value loss: 0.027747. Entropy: 0.306821.\n",
      "episode: 6607   score: 430.0  epsilon: 1.0    steps: 208  evaluation reward: 358.8\n",
      "episode: 6608   score: 225.0  epsilon: 1.0    steps: 664  evaluation reward: 359.1\n",
      "episode: 6609   score: 250.0  epsilon: 1.0    steps: 1008  evaluation reward: 359.2\n",
      "Training network. lr: 0.000104. clip: 0.041792\n",
      "Iteration 18961: Policy loss: 0.117041. Value loss: 0.123262. Entropy: 0.298846.\n",
      "Iteration 18962: Policy loss: 0.124283. Value loss: 0.055757. Entropy: 0.298817.\n",
      "Iteration 18963: Policy loss: 0.113070. Value loss: 0.037584. Entropy: 0.299368.\n",
      "Training network. lr: 0.000104. clip: 0.041792\n",
      "Iteration 18964: Policy loss: 0.242781. Value loss: 0.070628. Entropy: 0.311329.\n",
      "Iteration 18965: Policy loss: 0.241986. Value loss: 0.032842. Entropy: 0.311251.\n",
      "Iteration 18966: Policy loss: 0.244328. Value loss: 0.024578. Entropy: 0.310269.\n",
      "episode: 6610   score: 210.0  epsilon: 1.0    steps: 240  evaluation reward: 357.25\n",
      "episode: 6611   score: 220.0  epsilon: 1.0    steps: 600  evaluation reward: 356.5\n",
      "Training network. lr: 0.000104. clip: 0.041792\n",
      "Iteration 18967: Policy loss: 0.144214. Value loss: 0.086758. Entropy: 0.300831.\n",
      "Iteration 18968: Policy loss: 0.142529. Value loss: 0.041007. Entropy: 0.301324.\n",
      "Iteration 18969: Policy loss: 0.135463. Value loss: 0.033749. Entropy: 0.302906.\n",
      "episode: 6612   score: 205.0  epsilon: 1.0    steps: 648  evaluation reward: 353.7\n",
      "Training network. lr: 0.000104. clip: 0.041792\n",
      "Iteration 18970: Policy loss: 0.003313. Value loss: 0.093382. Entropy: 0.311687.\n",
      "Iteration 18971: Policy loss: 0.001086. Value loss: 0.045275. Entropy: 0.311604.\n",
      "Iteration 18972: Policy loss: 0.003111. Value loss: 0.032474. Entropy: 0.312030.\n",
      "Training network. lr: 0.000104. clip: 0.041792\n",
      "Iteration 18973: Policy loss: -0.071869. Value loss: 0.082992. Entropy: 0.316165.\n",
      "Iteration 18974: Policy loss: -0.065698. Value loss: 0.032345. Entropy: 0.315713.\n",
      "Iteration 18975: Policy loss: -0.074896. Value loss: 0.023638. Entropy: 0.315721.\n",
      "episode: 6613   score: 205.0  epsilon: 1.0    steps: 64  evaluation reward: 353.65\n",
      "Training network. lr: 0.000104. clip: 0.041792\n",
      "Iteration 18976: Policy loss: 0.140766. Value loss: 0.069124. Entropy: 0.303616.\n",
      "Iteration 18977: Policy loss: 0.135986. Value loss: 0.032777. Entropy: 0.301993.\n",
      "Iteration 18978: Policy loss: 0.133679. Value loss: 0.024725. Entropy: 0.303084.\n",
      "episode: 6614   score: 225.0  epsilon: 1.0    steps: 256  evaluation reward: 354.0\n",
      "Training network. lr: 0.000104. clip: 0.041792\n",
      "Iteration 18979: Policy loss: -0.176457. Value loss: 0.084617. Entropy: 0.309125.\n",
      "Iteration 18980: Policy loss: -0.177191. Value loss: 0.038381. Entropy: 0.309365.\n",
      "Iteration 18981: Policy loss: -0.177491. Value loss: 0.025462. Entropy: 0.310053.\n",
      "episode: 6615   score: 405.0  epsilon: 1.0    steps: 512  evaluation reward: 353.6\n",
      "episode: 6616   score: 205.0  epsilon: 1.0    steps: 544  evaluation reward: 352.35\n",
      "Training network. lr: 0.000104. clip: 0.041792\n",
      "Iteration 18982: Policy loss: -0.122569. Value loss: 0.041106. Entropy: 0.302590.\n",
      "Iteration 18983: Policy loss: -0.127015. Value loss: 0.027469. Entropy: 0.302742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18984: Policy loss: -0.131739. Value loss: 0.022181. Entropy: 0.302494.\n",
      "episode: 6617   score: 305.0  epsilon: 1.0    steps: 944  evaluation reward: 353.3\n",
      "Training network. lr: 0.000104. clip: 0.041792\n",
      "Iteration 18985: Policy loss: -0.286082. Value loss: 0.103961. Entropy: 0.316970.\n",
      "Iteration 18986: Policy loss: -0.289834. Value loss: 0.047799. Entropy: 0.317336.\n",
      "Iteration 18987: Policy loss: -0.294075. Value loss: 0.034775. Entropy: 0.317007.\n",
      "episode: 6618   score: 285.0  epsilon: 1.0    steps: 400  evaluation reward: 354.0\n",
      "Training network. lr: 0.000104. clip: 0.041792\n",
      "Iteration 18988: Policy loss: -0.305024. Value loss: 0.207532. Entropy: 0.303313.\n",
      "Iteration 18989: Policy loss: -0.301057. Value loss: 0.093906. Entropy: 0.304783.\n",
      "Iteration 18990: Policy loss: -0.318810. Value loss: 0.058545. Entropy: 0.304585.\n",
      "Training network. lr: 0.000104. clip: 0.041792\n",
      "Iteration 18991: Policy loss: 0.217752. Value loss: 0.168173. Entropy: 0.312741.\n",
      "Iteration 18992: Policy loss: 0.205669. Value loss: 0.051424. Entropy: 0.313733.\n",
      "Iteration 18993: Policy loss: 0.191641. Value loss: 0.037165. Entropy: 0.313787.\n",
      "episode: 6619   score: 410.0  epsilon: 1.0    steps: 424  evaluation reward: 356.0\n",
      "Training network. lr: 0.000104. clip: 0.041792\n",
      "Iteration 18994: Policy loss: -0.103766. Value loss: 0.074039. Entropy: 0.306071.\n",
      "Iteration 18995: Policy loss: -0.110154. Value loss: 0.032303. Entropy: 0.306295.\n",
      "Iteration 18996: Policy loss: -0.109838. Value loss: 0.022774. Entropy: 0.306777.\n",
      "episode: 6620   score: 285.0  epsilon: 1.0    steps: 64  evaluation reward: 355.6\n",
      "Training network. lr: 0.000104. clip: 0.041792\n",
      "Iteration 18997: Policy loss: -0.171576. Value loss: 0.104377. Entropy: 0.304348.\n",
      "Iteration 18998: Policy loss: -0.176792. Value loss: 0.048227. Entropy: 0.304509.\n",
      "Iteration 18999: Policy loss: -0.177988. Value loss: 0.034529. Entropy: 0.304420.\n",
      "episode: 6621   score: 260.0  epsilon: 1.0    steps: 384  evaluation reward: 355.8\n",
      "Training network. lr: 0.000104. clip: 0.041792\n",
      "Iteration 19000: Policy loss: -0.387701. Value loss: 0.343652. Entropy: 0.308158.\n",
      "Iteration 19001: Policy loss: -0.401988. Value loss: 0.244790. Entropy: 0.307961.\n",
      "Iteration 19002: Policy loss: -0.379656. Value loss: 0.169165. Entropy: 0.306982.\n",
      "episode: 6622   score: 315.0  epsilon: 1.0    steps: 632  evaluation reward: 355.1\n",
      "Training network. lr: 0.000104. clip: 0.041635\n",
      "Iteration 19003: Policy loss: -0.484301. Value loss: 0.385230. Entropy: 0.310018.\n",
      "Iteration 19004: Policy loss: -0.489530. Value loss: 0.197582. Entropy: 0.310573.\n",
      "Iteration 19005: Policy loss: -0.477145. Value loss: 0.098924. Entropy: 0.310058.\n",
      "Training network. lr: 0.000104. clip: 0.041635\n",
      "Iteration 19006: Policy loss: -0.149654. Value loss: 0.398643. Entropy: 0.313613.\n",
      "Iteration 19007: Policy loss: -0.154887. Value loss: 0.139649. Entropy: 0.313442.\n",
      "Iteration 19008: Policy loss: -0.150007. Value loss: 0.066037. Entropy: 0.313619.\n",
      "episode: 6623   score: 305.0  epsilon: 1.0    steps: 432  evaluation reward: 355.55\n",
      "episode: 6624   score: 825.0  epsilon: 1.0    steps: 656  evaluation reward: 360.65\n",
      "Training network. lr: 0.000104. clip: 0.041635\n",
      "Iteration 19009: Policy loss: 0.007227. Value loss: 0.135996. Entropy: 0.296355.\n",
      "Iteration 19010: Policy loss: 0.002327. Value loss: 0.070734. Entropy: 0.295148.\n",
      "Iteration 19011: Policy loss: 0.002700. Value loss: 0.050246. Entropy: 0.294154.\n",
      "episode: 6625   score: 735.0  epsilon: 1.0    steps: 56  evaluation reward: 363.8\n",
      "Training network. lr: 0.000104. clip: 0.041635\n",
      "Iteration 19012: Policy loss: 0.041469. Value loss: 0.063556. Entropy: 0.308604.\n",
      "Iteration 19013: Policy loss: 0.036656. Value loss: 0.030226. Entropy: 0.308146.\n",
      "Iteration 19014: Policy loss: 0.039521. Value loss: 0.022293. Entropy: 0.307964.\n",
      "episode: 6626   score: 150.0  epsilon: 1.0    steps: 432  evaluation reward: 362.0\n",
      "Training network. lr: 0.000104. clip: 0.041635\n",
      "Iteration 19015: Policy loss: 0.078932. Value loss: 0.155296. Entropy: 0.304822.\n",
      "Iteration 19016: Policy loss: 0.063156. Value loss: 0.083456. Entropy: 0.305494.\n",
      "Iteration 19017: Policy loss: 0.063663. Value loss: 0.062859. Entropy: 0.303947.\n",
      "episode: 6627   score: 355.0  epsilon: 1.0    steps: 64  evaluation reward: 362.7\n",
      "Training network. lr: 0.000104. clip: 0.041635\n",
      "Iteration 19018: Policy loss: 0.165958. Value loss: 0.258519. Entropy: 0.299130.\n",
      "Iteration 19019: Policy loss: 0.181487. Value loss: 0.076062. Entropy: 0.299505.\n",
      "Iteration 19020: Policy loss: 0.166784. Value loss: 0.045403. Entropy: 0.298587.\n",
      "episode: 6628   score: 725.0  epsilon: 1.0    steps: 424  evaluation reward: 366.8\n",
      "Training network. lr: 0.000104. clip: 0.041635\n",
      "Iteration 19021: Policy loss: -0.144617. Value loss: 0.263429. Entropy: 0.304757.\n",
      "Iteration 19022: Policy loss: -0.144514. Value loss: 0.177504. Entropy: 0.304333.\n",
      "Iteration 19023: Policy loss: -0.165964. Value loss: 0.172328. Entropy: 0.303574.\n",
      "episode: 6629   score: 370.0  epsilon: 1.0    steps: 48  evaluation reward: 367.2\n",
      "Training network. lr: 0.000104. clip: 0.041635\n",
      "Iteration 19024: Policy loss: 0.262234. Value loss: 0.142504. Entropy: 0.302855.\n",
      "Iteration 19025: Policy loss: 0.265093. Value loss: 0.058817. Entropy: 0.301317.\n",
      "Iteration 19026: Policy loss: 0.257419. Value loss: 0.042271. Entropy: 0.301838.\n",
      "Training network. lr: 0.000104. clip: 0.041635\n",
      "Iteration 19027: Policy loss: -0.052150. Value loss: 0.180079. Entropy: 0.315946.\n",
      "Iteration 19028: Policy loss: -0.055574. Value loss: 0.083515. Entropy: 0.313877.\n",
      "Iteration 19029: Policy loss: -0.061639. Value loss: 0.057970. Entropy: 0.314564.\n",
      "episode: 6630   score: 470.0  epsilon: 1.0    steps: 16  evaluation reward: 369.8\n",
      "episode: 6631   score: 230.0  epsilon: 1.0    steps: 336  evaluation reward: 370.0\n",
      "Training network. lr: 0.000104. clip: 0.041635\n",
      "Iteration 19030: Policy loss: 0.386616. Value loss: 0.186079. Entropy: 0.284002.\n",
      "Iteration 19031: Policy loss: 0.384014. Value loss: 0.058068. Entropy: 0.285161.\n",
      "Iteration 19032: Policy loss: 0.365469. Value loss: 0.040476. Entropy: 0.284204.\n",
      "episode: 6632   score: 625.0  epsilon: 1.0    steps: 744  evaluation reward: 372.35\n",
      "Training network. lr: 0.000104. clip: 0.041635\n",
      "Iteration 19033: Policy loss: 0.049030. Value loss: 0.126591. Entropy: 0.311868.\n",
      "Iteration 19034: Policy loss: 0.039989. Value loss: 0.064920. Entropy: 0.311806.\n",
      "Iteration 19035: Policy loss: 0.043811. Value loss: 0.046364. Entropy: 0.311279.\n",
      "episode: 6633   score: 595.0  epsilon: 1.0    steps: 944  evaluation reward: 377.45\n",
      "Training network. lr: 0.000104. clip: 0.041635\n",
      "Iteration 19036: Policy loss: -0.094416. Value loss: 0.153608. Entropy: 0.309770.\n",
      "Iteration 19037: Policy loss: -0.099837. Value loss: 0.068351. Entropy: 0.309003.\n",
      "Iteration 19038: Policy loss: -0.097118. Value loss: 0.047386. Entropy: 0.309892.\n",
      "episode: 6634   score: 295.0  epsilon: 1.0    steps: 824  evaluation reward: 374.4\n",
      "episode: 6635   score: 180.0  epsilon: 1.0    steps: 952  evaluation reward: 373.6\n",
      "Training network. lr: 0.000104. clip: 0.041635\n",
      "Iteration 19039: Policy loss: 0.155862. Value loss: 0.098242. Entropy: 0.301922.\n",
      "Iteration 19040: Policy loss: 0.151317. Value loss: 0.036278. Entropy: 0.301495.\n",
      "Iteration 19041: Policy loss: 0.150021. Value loss: 0.024492. Entropy: 0.301523.\n",
      "Training network. lr: 0.000104. clip: 0.041635\n",
      "Iteration 19042: Policy loss: 0.094224. Value loss: 0.141288. Entropy: 0.308593.\n",
      "Iteration 19043: Policy loss: 0.092338. Value loss: 0.068661. Entropy: 0.310206.\n",
      "Iteration 19044: Policy loss: 0.087080. Value loss: 0.046954. Entropy: 0.309813.\n",
      "episode: 6636   score: 425.0  epsilon: 1.0    steps: 200  evaluation reward: 375.75\n",
      "episode: 6637   score: 285.0  epsilon: 1.0    steps: 400  evaluation reward: 373.7\n",
      "episode: 6638   score: 370.0  epsilon: 1.0    steps: 472  evaluation reward: 374.25\n",
      "Training network. lr: 0.000104. clip: 0.041635\n",
      "Iteration 19045: Policy loss: 0.308231. Value loss: 0.100002. Entropy: 0.294076.\n",
      "Iteration 19046: Policy loss: 0.301362. Value loss: 0.048656. Entropy: 0.295222.\n",
      "Iteration 19047: Policy loss: 0.301543. Value loss: 0.038138. Entropy: 0.296113.\n",
      "Training network. lr: 0.000104. clip: 0.041635\n",
      "Iteration 19048: Policy loss: 0.052597. Value loss: 0.127247. Entropy: 0.316744.\n",
      "Iteration 19049: Policy loss: 0.055645. Value loss: 0.065108. Entropy: 0.316012.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19050: Policy loss: 0.044549. Value loss: 0.051585. Entropy: 0.315648.\n",
      "Training network. lr: 0.000104. clip: 0.041488\n",
      "Iteration 19051: Policy loss: 0.037380. Value loss: 0.101153. Entropy: 0.310332.\n",
      "Iteration 19052: Policy loss: 0.030645. Value loss: 0.042266. Entropy: 0.309167.\n",
      "Iteration 19053: Policy loss: 0.030520. Value loss: 0.028495. Entropy: 0.308351.\n",
      "episode: 6639   score: 290.0  epsilon: 1.0    steps: 624  evaluation reward: 374.55\n",
      "episode: 6640   score: 225.0  epsilon: 1.0    steps: 688  evaluation reward: 374.4\n",
      "Training network. lr: 0.000104. clip: 0.041488\n",
      "Iteration 19054: Policy loss: -0.052658. Value loss: 0.092875. Entropy: 0.292144.\n",
      "Iteration 19055: Policy loss: -0.056286. Value loss: 0.045601. Entropy: 0.290669.\n",
      "Iteration 19056: Policy loss: -0.054383. Value loss: 0.035024. Entropy: 0.289298.\n",
      "episode: 6641   score: 360.0  epsilon: 1.0    steps: 560  evaluation reward: 374.85\n",
      "episode: 6642   score: 315.0  epsilon: 1.0    steps: 688  evaluation reward: 371.8\n",
      "Training network. lr: 0.000104. clip: 0.041488\n",
      "Iteration 19057: Policy loss: 0.263550. Value loss: 0.110593. Entropy: 0.296029.\n",
      "Iteration 19058: Policy loss: 0.258625. Value loss: 0.056919. Entropy: 0.293505.\n",
      "Iteration 19059: Policy loss: 0.255935. Value loss: 0.040730. Entropy: 0.293434.\n",
      "Training network. lr: 0.000104. clip: 0.041488\n",
      "Iteration 19060: Policy loss: -0.045007. Value loss: 0.094663. Entropy: 0.316082.\n",
      "Iteration 19061: Policy loss: -0.045899. Value loss: 0.046854. Entropy: 0.316466.\n",
      "Iteration 19062: Policy loss: -0.049067. Value loss: 0.036512. Entropy: 0.315222.\n",
      "episode: 6643   score: 375.0  epsilon: 1.0    steps: 320  evaluation reward: 372.4\n",
      "episode: 6644   score: 185.0  epsilon: 1.0    steps: 368  evaluation reward: 370.05\n",
      "Training network. lr: 0.000104. clip: 0.041488\n",
      "Iteration 19063: Policy loss: 0.085156. Value loss: 0.137886. Entropy: 0.289056.\n",
      "Iteration 19064: Policy loss: 0.075316. Value loss: 0.066297. Entropy: 0.290883.\n",
      "Iteration 19065: Policy loss: 0.075443. Value loss: 0.048000. Entropy: 0.290589.\n",
      "episode: 6645   score: 240.0  epsilon: 1.0    steps: 224  evaluation reward: 368.25\n",
      "episode: 6646   score: 310.0  epsilon: 1.0    steps: 464  evaluation reward: 368.15\n",
      "episode: 6647   score: 90.0  epsilon: 1.0    steps: 496  evaluation reward: 364.45\n",
      "Training network. lr: 0.000104. clip: 0.041488\n",
      "Iteration 19066: Policy loss: 0.019927. Value loss: 0.091549. Entropy: 0.281546.\n",
      "Iteration 19067: Policy loss: 0.019158. Value loss: 0.047438. Entropy: 0.280963.\n",
      "Iteration 19068: Policy loss: 0.017020. Value loss: 0.033843. Entropy: 0.281691.\n",
      "Training network. lr: 0.000104. clip: 0.041488\n",
      "Iteration 19069: Policy loss: -0.188953. Value loss: 0.099606. Entropy: 0.319329.\n",
      "Iteration 19070: Policy loss: -0.189176. Value loss: 0.045319. Entropy: 0.319653.\n",
      "Iteration 19071: Policy loss: -0.188015. Value loss: 0.029874. Entropy: 0.319713.\n",
      "Training network. lr: 0.000104. clip: 0.041488\n",
      "Iteration 19072: Policy loss: 0.006355. Value loss: 0.273113. Entropy: 0.307353.\n",
      "Iteration 19073: Policy loss: 0.009005. Value loss: 0.217353. Entropy: 0.306405.\n",
      "Iteration 19074: Policy loss: -0.025082. Value loss: 0.202739. Entropy: 0.306409.\n",
      "episode: 6648   score: 265.0  epsilon: 1.0    steps: 184  evaluation reward: 361.45\n",
      "Training network. lr: 0.000104. clip: 0.041488\n",
      "Iteration 19075: Policy loss: -0.211030. Value loss: 0.124885. Entropy: 0.305421.\n",
      "Iteration 19076: Policy loss: -0.216751. Value loss: 0.056415. Entropy: 0.304158.\n",
      "Iteration 19077: Policy loss: -0.211189. Value loss: 0.041284. Entropy: 0.304761.\n",
      "Training network. lr: 0.000104. clip: 0.041488\n",
      "Iteration 19078: Policy loss: -0.014821. Value loss: 0.115886. Entropy: 0.319685.\n",
      "Iteration 19079: Policy loss: -0.012778. Value loss: 0.052194. Entropy: 0.319205.\n",
      "Iteration 19080: Policy loss: -0.023407. Value loss: 0.037702. Entropy: 0.319636.\n",
      "episode: 6649   score: 335.0  epsilon: 1.0    steps: 160  evaluation reward: 359.4\n",
      "episode: 6650   score: 315.0  epsilon: 1.0    steps: 664  evaluation reward: 358.35\n",
      "now time :  2019-09-06 09:57:40.826035\n",
      "episode: 6651   score: 670.0  epsilon: 1.0    steps: 904  evaluation reward: 361.9\n",
      "episode: 6652   score: 200.0  epsilon: 1.0    steps: 992  evaluation reward: 360.4\n",
      "Training network. lr: 0.000104. clip: 0.041488\n",
      "Iteration 19081: Policy loss: -0.069312. Value loss: 0.133623. Entropy: 0.284484.\n",
      "Iteration 19082: Policy loss: -0.064451. Value loss: 0.059912. Entropy: 0.284842.\n",
      "Iteration 19083: Policy loss: -0.071277. Value loss: 0.040032. Entropy: 0.285458.\n",
      "Training network. lr: 0.000104. clip: 0.041488\n",
      "Iteration 19084: Policy loss: -0.075083. Value loss: 0.085870. Entropy: 0.307840.\n",
      "Iteration 19085: Policy loss: -0.073163. Value loss: 0.038730. Entropy: 0.307055.\n",
      "Iteration 19086: Policy loss: -0.074938. Value loss: 0.028472. Entropy: 0.307305.\n",
      "episode: 6653   score: 360.0  epsilon: 1.0    steps: 664  evaluation reward: 360.7\n",
      "Training network. lr: 0.000104. clip: 0.041488\n",
      "Iteration 19087: Policy loss: -0.047964. Value loss: 0.095486. Entropy: 0.306488.\n",
      "Iteration 19088: Policy loss: -0.048901. Value loss: 0.041956. Entropy: 0.306219.\n",
      "Iteration 19089: Policy loss: -0.055154. Value loss: 0.029716. Entropy: 0.307070.\n",
      "Training network. lr: 0.000104. clip: 0.041488\n",
      "Iteration 19090: Policy loss: 0.067838. Value loss: 0.064678. Entropy: 0.310277.\n",
      "Iteration 19091: Policy loss: 0.067239. Value loss: 0.027041. Entropy: 0.311229.\n",
      "Iteration 19092: Policy loss: 0.059541. Value loss: 0.018133. Entropy: 0.310759.\n",
      "episode: 6654   score: 420.0  epsilon: 1.0    steps: 464  evaluation reward: 360.25\n",
      "Training network. lr: 0.000104. clip: 0.041488\n",
      "Iteration 19093: Policy loss: -0.060271. Value loss: 0.077675. Entropy: 0.300757.\n",
      "Iteration 19094: Policy loss: -0.065216. Value loss: 0.038183. Entropy: 0.299510.\n",
      "Iteration 19095: Policy loss: -0.071661. Value loss: 0.024827. Entropy: 0.298565.\n",
      "episode: 6655   score: 420.0  epsilon: 1.0    steps: 280  evaluation reward: 360.65\n",
      "episode: 6656   score: 390.0  epsilon: 1.0    steps: 744  evaluation reward: 362.15\n",
      "Training network. lr: 0.000104. clip: 0.041488\n",
      "Iteration 19096: Policy loss: -0.458172. Value loss: 0.397027. Entropy: 0.289282.\n",
      "Iteration 19097: Policy loss: -0.485873. Value loss: 0.269855. Entropy: 0.288872.\n",
      "Iteration 19098: Policy loss: -0.460107. Value loss: 0.197949. Entropy: 0.288941.\n",
      "episode: 6657   score: 315.0  epsilon: 1.0    steps: 504  evaluation reward: 359.45\n",
      "Training network. lr: 0.000104. clip: 0.041488\n",
      "Iteration 19099: Policy loss: -0.163567. Value loss: 0.138380. Entropy: 0.303644.\n",
      "Iteration 19100: Policy loss: -0.169508. Value loss: 0.076166. Entropy: 0.305139.\n",
      "Iteration 19101: Policy loss: -0.170109. Value loss: 0.054481. Entropy: 0.304939.\n",
      "episode: 6658   score: 315.0  epsilon: 1.0    steps: 784  evaluation reward: 359.45\n",
      "Training network. lr: 0.000103. clip: 0.041331\n",
      "Iteration 19102: Policy loss: 0.064907. Value loss: 0.132157. Entropy: 0.304519.\n",
      "Iteration 19103: Policy loss: 0.052400. Value loss: 0.065173. Entropy: 0.304085.\n",
      "Iteration 19104: Policy loss: 0.053083. Value loss: 0.048223. Entropy: 0.303427.\n",
      "episode: 6659   score: 565.0  epsilon: 1.0    steps: 32  evaluation reward: 359.15\n",
      "episode: 6660   score: 405.0  epsilon: 1.0    steps: 992  evaluation reward: 359.3\n",
      "Training network. lr: 0.000103. clip: 0.041331\n",
      "Iteration 19105: Policy loss: 0.095271. Value loss: 0.091776. Entropy: 0.299703.\n",
      "Iteration 19106: Policy loss: 0.095970. Value loss: 0.039873. Entropy: 0.300334.\n",
      "Iteration 19107: Policy loss: 0.095538. Value loss: 0.029734. Entropy: 0.299169.\n",
      "episode: 6661   score: 180.0  epsilon: 1.0    steps: 568  evaluation reward: 356.45\n",
      "Training network. lr: 0.000103. clip: 0.041331\n",
      "Iteration 19108: Policy loss: -0.063052. Value loss: 0.085987. Entropy: 0.294374.\n",
      "Iteration 19109: Policy loss: -0.065190. Value loss: 0.039046. Entropy: 0.295012.\n",
      "Iteration 19110: Policy loss: -0.066262. Value loss: 0.028855. Entropy: 0.294671.\n",
      "Training network. lr: 0.000103. clip: 0.041331\n",
      "Iteration 19111: Policy loss: -0.225602. Value loss: 0.152058. Entropy: 0.317808.\n",
      "Iteration 19112: Policy loss: -0.230170. Value loss: 0.079085. Entropy: 0.318422.\n",
      "Iteration 19113: Policy loss: -0.238406. Value loss: 0.055492. Entropy: 0.318107.\n",
      "Training network. lr: 0.000103. clip: 0.041331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19114: Policy loss: 0.217035. Value loss: 0.104461. Entropy: 0.313129.\n",
      "Iteration 19115: Policy loss: 0.216332. Value loss: 0.038593. Entropy: 0.312714.\n",
      "Iteration 19116: Policy loss: 0.220547. Value loss: 0.029480. Entropy: 0.312129.\n",
      "Training network. lr: 0.000103. clip: 0.041331\n",
      "Iteration 19117: Policy loss: -0.022535. Value loss: 0.120137. Entropy: 0.313020.\n",
      "Iteration 19118: Policy loss: -0.031164. Value loss: 0.055330. Entropy: 0.312608.\n",
      "Iteration 19119: Policy loss: -0.028031. Value loss: 0.038533. Entropy: 0.312303.\n",
      "episode: 6662   score: 600.0  epsilon: 1.0    steps: 392  evaluation reward: 359.85\n",
      "episode: 6663   score: 355.0  epsilon: 1.0    steps: 960  evaluation reward: 358.25\n",
      "Training network. lr: 0.000103. clip: 0.041331\n",
      "Iteration 19120: Policy loss: 0.081527. Value loss: 0.092394. Entropy: 0.298981.\n",
      "Iteration 19121: Policy loss: 0.076362. Value loss: 0.037024. Entropy: 0.298822.\n",
      "Iteration 19122: Policy loss: 0.072917. Value loss: 0.025995. Entropy: 0.298048.\n",
      "episode: 6664   score: 350.0  epsilon: 1.0    steps: 856  evaluation reward: 356.6\n",
      "Training network. lr: 0.000103. clip: 0.041331\n",
      "Iteration 19123: Policy loss: 0.048057. Value loss: 0.094554. Entropy: 0.299285.\n",
      "Iteration 19124: Policy loss: 0.047772. Value loss: 0.042367. Entropy: 0.300535.\n",
      "Iteration 19125: Policy loss: 0.045662. Value loss: 0.034068. Entropy: 0.300445.\n",
      "episode: 6665   score: 375.0  epsilon: 1.0    steps: 312  evaluation reward: 355.15\n",
      "episode: 6666   score: 495.0  epsilon: 1.0    steps: 920  evaluation reward: 357.2\n",
      "Training network. lr: 0.000103. clip: 0.041331\n",
      "Iteration 19126: Policy loss: 0.099188. Value loss: 0.138343. Entropy: 0.301700.\n",
      "Iteration 19127: Policy loss: 0.097087. Value loss: 0.055551. Entropy: 0.301445.\n",
      "Iteration 19128: Policy loss: 0.090088. Value loss: 0.039894. Entropy: 0.301337.\n",
      "episode: 6667   score: 525.0  epsilon: 1.0    steps: 408  evaluation reward: 358.45\n",
      "episode: 6668   score: 330.0  epsilon: 1.0    steps: 568  evaluation reward: 357.05\n",
      "Training network. lr: 0.000103. clip: 0.041331\n",
      "Iteration 19129: Policy loss: -0.235419. Value loss: 0.256711. Entropy: 0.293405.\n",
      "Iteration 19130: Policy loss: -0.229282. Value loss: 0.158539. Entropy: 0.292325.\n",
      "Iteration 19131: Policy loss: -0.251787. Value loss: 0.143103. Entropy: 0.293823.\n",
      "Training network. lr: 0.000103. clip: 0.041331\n",
      "Iteration 19132: Policy loss: -0.239756. Value loss: 0.278732. Entropy: 0.318679.\n",
      "Iteration 19133: Policy loss: -0.276513. Value loss: 0.180821. Entropy: 0.320205.\n",
      "Iteration 19134: Policy loss: -0.281502. Value loss: 0.137931. Entropy: 0.320077.\n",
      "episode: 6669   score: 365.0  epsilon: 1.0    steps: 592  evaluation reward: 357.55\n",
      "Training network. lr: 0.000103. clip: 0.041331\n",
      "Iteration 19135: Policy loss: 0.591329. Value loss: 0.229292. Entropy: 0.298267.\n",
      "Iteration 19136: Policy loss: 0.592295. Value loss: 0.092625. Entropy: 0.298096.\n",
      "Iteration 19137: Policy loss: 0.586929. Value loss: 0.077497. Entropy: 0.297921.\n",
      "episode: 6670   score: 400.0  epsilon: 1.0    steps: 720  evaluation reward: 358.35\n",
      "Training network. lr: 0.000103. clip: 0.041331\n",
      "Iteration 19138: Policy loss: 0.392740. Value loss: 0.238914. Entropy: 0.306546.\n",
      "Iteration 19139: Policy loss: 0.385085. Value loss: 0.083439. Entropy: 0.307456.\n",
      "Iteration 19140: Policy loss: 0.373093. Value loss: 0.053792. Entropy: 0.305575.\n",
      "Training network. lr: 0.000103. clip: 0.041331\n",
      "Iteration 19141: Policy loss: 0.164952. Value loss: 0.081987. Entropy: 0.307422.\n",
      "Iteration 19142: Policy loss: 0.153049. Value loss: 0.041473. Entropy: 0.306733.\n",
      "Iteration 19143: Policy loss: 0.151922. Value loss: 0.030136. Entropy: 0.305757.\n",
      "episode: 6671   score: 370.0  epsilon: 1.0    steps: 160  evaluation reward: 357.05\n",
      "Training network. lr: 0.000103. clip: 0.041331\n",
      "Iteration 19144: Policy loss: -0.298936. Value loss: 0.310579. Entropy: 0.303769.\n",
      "Iteration 19145: Policy loss: -0.309811. Value loss: 0.131349. Entropy: 0.302522.\n",
      "Iteration 19146: Policy loss: -0.312614. Value loss: 0.090095. Entropy: 0.302308.\n",
      "episode: 6672   score: 330.0  epsilon: 1.0    steps: 48  evaluation reward: 356.45\n",
      "episode: 6673   score: 600.0  epsilon: 1.0    steps: 544  evaluation reward: 359.6\n",
      "Training network. lr: 0.000103. clip: 0.041331\n",
      "Iteration 19147: Policy loss: -0.014745. Value loss: 0.117979. Entropy: 0.294872.\n",
      "Iteration 19148: Policy loss: -0.013718. Value loss: 0.056345. Entropy: 0.296628.\n",
      "Iteration 19149: Policy loss: -0.016911. Value loss: 0.037214. Entropy: 0.298101.\n",
      "episode: 6674   score: 330.0  epsilon: 1.0    steps: 384  evaluation reward: 360.6\n",
      "episode: 6675   score: 360.0  epsilon: 1.0    steps: 520  evaluation reward: 360.55\n",
      "Training network. lr: 0.000103. clip: 0.041331\n",
      "Iteration 19150: Policy loss: 0.125095. Value loss: 0.116206. Entropy: 0.299601.\n",
      "Iteration 19151: Policy loss: 0.127593. Value loss: 0.053795. Entropy: 0.300883.\n",
      "Iteration 19152: Policy loss: 0.119322. Value loss: 0.041711. Entropy: 0.299774.\n",
      "Training network. lr: 0.000103. clip: 0.041174\n",
      "Iteration 19153: Policy loss: 0.271792. Value loss: 0.118306. Entropy: 0.315638.\n",
      "Iteration 19154: Policy loss: 0.265820. Value loss: 0.046956. Entropy: 0.316505.\n",
      "Iteration 19155: Policy loss: 0.263318. Value loss: 0.036000. Entropy: 0.315893.\n",
      "Training network. lr: 0.000103. clip: 0.041174\n",
      "Iteration 19156: Policy loss: -0.091438. Value loss: 0.165960. Entropy: 0.313910.\n",
      "Iteration 19157: Policy loss: -0.084782. Value loss: 0.063963. Entropy: 0.314212.\n",
      "Iteration 19158: Policy loss: -0.099350. Value loss: 0.043353. Entropy: 0.314834.\n",
      "Training network. lr: 0.000103. clip: 0.041174\n",
      "Iteration 19159: Policy loss: 0.306161. Value loss: 0.181861. Entropy: 0.312846.\n",
      "Iteration 19160: Policy loss: 0.294189. Value loss: 0.064585. Entropy: 0.312891.\n",
      "Iteration 19161: Policy loss: 0.295818. Value loss: 0.042805. Entropy: 0.312941.\n",
      "episode: 6676   score: 195.0  epsilon: 1.0    steps: 184  evaluation reward: 356.65\n",
      "Training network. lr: 0.000103. clip: 0.041174\n",
      "Iteration 19162: Policy loss: -0.031205. Value loss: 0.079147. Entropy: 0.302413.\n",
      "Iteration 19163: Policy loss: -0.034723. Value loss: 0.043840. Entropy: 0.302218.\n",
      "Iteration 19164: Policy loss: -0.031634. Value loss: 0.032922. Entropy: 0.302385.\n",
      "episode: 6677   score: 495.0  epsilon: 1.0    steps: 216  evaluation reward: 356.3\n",
      "Training network. lr: 0.000103. clip: 0.041174\n",
      "Iteration 19165: Policy loss: -0.028645. Value loss: 0.072322. Entropy: 0.303007.\n",
      "Iteration 19166: Policy loss: -0.032426. Value loss: 0.032093. Entropy: 0.301973.\n",
      "Iteration 19167: Policy loss: -0.037607. Value loss: 0.020516. Entropy: 0.303543.\n",
      "episode: 6678   score: 295.0  epsilon: 1.0    steps: 112  evaluation reward: 353.35\n",
      "episode: 6679   score: 270.0  epsilon: 1.0    steps: 840  evaluation reward: 349.9\n",
      "episode: 6680   score: 820.0  epsilon: 1.0    steps: 896  evaluation reward: 353.6\n",
      "Training network. lr: 0.000103. clip: 0.041174\n",
      "Iteration 19168: Policy loss: -0.322783. Value loss: 0.365687. Entropy: 0.291790.\n",
      "Iteration 19169: Policy loss: -0.326035. Value loss: 0.229458. Entropy: 0.291195.\n",
      "Iteration 19170: Policy loss: -0.334508. Value loss: 0.189030. Entropy: 0.290969.\n",
      "Training network. lr: 0.000103. clip: 0.041174\n",
      "Iteration 19171: Policy loss: -0.274121. Value loss: 0.398204. Entropy: 0.307222.\n",
      "Iteration 19172: Policy loss: -0.299881. Value loss: 0.253105. Entropy: 0.307848.\n",
      "Iteration 19173: Policy loss: -0.309107. Value loss: 0.203374. Entropy: 0.306819.\n",
      "episode: 6681   score: 530.0  epsilon: 1.0    steps: 160  evaluation reward: 355.75\n",
      "episode: 6682   score: 410.0  epsilon: 1.0    steps: 664  evaluation reward: 356.95\n",
      "episode: 6683   score: 365.0  epsilon: 1.0    steps: 712  evaluation reward: 354.05\n",
      "Training network. lr: 0.000103. clip: 0.041174\n",
      "Iteration 19174: Policy loss: 0.057579. Value loss: 0.140858. Entropy: 0.276923.\n",
      "Iteration 19175: Policy loss: 0.060509. Value loss: 0.049373. Entropy: 0.276128.\n",
      "Iteration 19176: Policy loss: 0.054015. Value loss: 0.037335. Entropy: 0.277363.\n",
      "Training network. lr: 0.000103. clip: 0.041174\n",
      "Iteration 19177: Policy loss: 0.109783. Value loss: 0.104203. Entropy: 0.309960.\n",
      "Iteration 19178: Policy loss: 0.109481. Value loss: 0.052839. Entropy: 0.309009.\n",
      "Iteration 19179: Policy loss: 0.104076. Value loss: 0.039557. Entropy: 0.310408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000103. clip: 0.041174\n",
      "Iteration 19180: Policy loss: -0.035786. Value loss: 0.085313. Entropy: 0.318469.\n",
      "Iteration 19181: Policy loss: -0.033004. Value loss: 0.037115. Entropy: 0.318836.\n",
      "Iteration 19182: Policy loss: -0.039465. Value loss: 0.025209. Entropy: 0.318007.\n",
      "Training network. lr: 0.000103. clip: 0.041174\n",
      "Iteration 19183: Policy loss: -0.261581. Value loss: 0.317859. Entropy: 0.312034.\n",
      "Iteration 19184: Policy loss: -0.241246. Value loss: 0.178016. Entropy: 0.311466.\n",
      "Iteration 19185: Policy loss: -0.254462. Value loss: 0.135217. Entropy: 0.311120.\n",
      "episode: 6684   score: 225.0  epsilon: 1.0    steps: 168  evaluation reward: 354.0\n",
      "episode: 6685   score: 215.0  epsilon: 1.0    steps: 520  evaluation reward: 353.0\n",
      "episode: 6686   score: 355.0  epsilon: 1.0    steps: 800  evaluation reward: 353.85\n",
      "Training network. lr: 0.000103. clip: 0.041174\n",
      "Iteration 19186: Policy loss: 0.405749. Value loss: 0.199825. Entropy: 0.272841.\n",
      "Iteration 19187: Policy loss: 0.402990. Value loss: 0.085202. Entropy: 0.272488.\n",
      "Iteration 19188: Policy loss: 0.403957. Value loss: 0.056739. Entropy: 0.271738.\n",
      "episode: 6687   score: 535.0  epsilon: 1.0    steps: 48  evaluation reward: 355.9\n",
      "Training network. lr: 0.000103. clip: 0.041174\n",
      "Iteration 19189: Policy loss: -0.028229. Value loss: 0.119603. Entropy: 0.309892.\n",
      "Iteration 19190: Policy loss: -0.028580. Value loss: 0.060837. Entropy: 0.308876.\n",
      "Iteration 19191: Policy loss: -0.031986. Value loss: 0.039817. Entropy: 0.308430.\n",
      "episode: 6688   score: 360.0  epsilon: 1.0    steps: 352  evaluation reward: 356.35\n",
      "episode: 6689   score: 285.0  epsilon: 1.0    steps: 584  evaluation reward: 356.6\n",
      "Training network. lr: 0.000103. clip: 0.041174\n",
      "Iteration 19192: Policy loss: -0.146604. Value loss: 0.120063. Entropy: 0.290338.\n",
      "Iteration 19193: Policy loss: -0.144426. Value loss: 0.055449. Entropy: 0.289945.\n",
      "Iteration 19194: Policy loss: -0.153424. Value loss: 0.043411. Entropy: 0.290704.\n",
      "episode: 6690   score: 505.0  epsilon: 1.0    steps: 104  evaluation reward: 356.7\n",
      "Training network. lr: 0.000103. clip: 0.041174\n",
      "Iteration 19195: Policy loss: -0.156458. Value loss: 0.089908. Entropy: 0.303678.\n",
      "Iteration 19196: Policy loss: -0.164196. Value loss: 0.041337. Entropy: 0.302784.\n",
      "Iteration 19197: Policy loss: -0.157425. Value loss: 0.028221. Entropy: 0.302683.\n",
      "Training network. lr: 0.000103. clip: 0.041174\n",
      "Iteration 19198: Policy loss: 0.167471. Value loss: 0.076068. Entropy: 0.314803.\n",
      "Iteration 19199: Policy loss: 0.161315. Value loss: 0.046433. Entropy: 0.313344.\n",
      "Iteration 19200: Policy loss: 0.162673. Value loss: 0.038821. Entropy: 0.312491.\n",
      "Training network. lr: 0.000103. clip: 0.041027\n",
      "Iteration 19201: Policy loss: -0.057030. Value loss: 0.102679. Entropy: 0.314005.\n",
      "Iteration 19202: Policy loss: -0.062108. Value loss: 0.046898. Entropy: 0.314163.\n",
      "Iteration 19203: Policy loss: -0.057831. Value loss: 0.036694. Entropy: 0.314189.\n",
      "episode: 6691   score: 210.0  epsilon: 1.0    steps: 16  evaluation reward: 356.2\n",
      "episode: 6692   score: 300.0  epsilon: 1.0    steps: 920  evaluation reward: 356.2\n",
      "Training network. lr: 0.000103. clip: 0.041027\n",
      "Iteration 19204: Policy loss: -0.022603. Value loss: 0.111231. Entropy: 0.303320.\n",
      "Iteration 19205: Policy loss: -0.025799. Value loss: 0.045119. Entropy: 0.301965.\n",
      "Iteration 19206: Policy loss: -0.032221. Value loss: 0.032153. Entropy: 0.302816.\n",
      "episode: 6693   score: 295.0  epsilon: 1.0    steps: 352  evaluation reward: 355.5\n",
      "Training network. lr: 0.000103. clip: 0.041027\n",
      "Iteration 19207: Policy loss: -0.083017. Value loss: 0.084383. Entropy: 0.298598.\n",
      "Iteration 19208: Policy loss: -0.092635. Value loss: 0.031333. Entropy: 0.298671.\n",
      "Iteration 19209: Policy loss: -0.087025. Value loss: 0.022781. Entropy: 0.299052.\n",
      "episode: 6694   score: 345.0  epsilon: 1.0    steps: 952  evaluation reward: 354.55\n",
      "Training network. lr: 0.000103. clip: 0.041027\n",
      "Iteration 19210: Policy loss: -0.008267. Value loss: 0.131772. Entropy: 0.317864.\n",
      "Iteration 19211: Policy loss: -0.010331. Value loss: 0.055482. Entropy: 0.318043.\n",
      "Iteration 19212: Policy loss: -0.013754. Value loss: 0.041588. Entropy: 0.317503.\n",
      "episode: 6695   score: 555.0  epsilon: 1.0    steps: 432  evaluation reward: 356.45\n",
      "Training network. lr: 0.000103. clip: 0.041027\n",
      "Iteration 19213: Policy loss: -0.129978. Value loss: 0.107741. Entropy: 0.296150.\n",
      "Iteration 19214: Policy loss: -0.140067. Value loss: 0.042865. Entropy: 0.297378.\n",
      "Iteration 19215: Policy loss: -0.141072. Value loss: 0.029468. Entropy: 0.297535.\n",
      "episode: 6696   score: 395.0  epsilon: 1.0    steps: 88  evaluation reward: 357.35\n",
      "episode: 6697   score: 360.0  epsilon: 1.0    steps: 360  evaluation reward: 359.15\n",
      "episode: 6698   score: 445.0  epsilon: 1.0    steps: 376  evaluation reward: 361.8\n",
      "Training network. lr: 0.000103. clip: 0.041027\n",
      "Iteration 19216: Policy loss: -0.076109. Value loss: 0.086116. Entropy: 0.271634.\n",
      "Iteration 19217: Policy loss: -0.080279. Value loss: 0.045464. Entropy: 0.271415.\n",
      "Iteration 19218: Policy loss: -0.078646. Value loss: 0.038295. Entropy: 0.272228.\n",
      "Training network. lr: 0.000103. clip: 0.041027\n",
      "Iteration 19219: Policy loss: 0.039949. Value loss: 0.083102. Entropy: 0.314829.\n",
      "Iteration 19220: Policy loss: 0.039227. Value loss: 0.043066. Entropy: 0.313682.\n",
      "Iteration 19221: Policy loss: 0.037587. Value loss: 0.034202. Entropy: 0.314555.\n",
      "Training network. lr: 0.000103. clip: 0.041027\n",
      "Iteration 19222: Policy loss: 0.066111. Value loss: 0.080570. Entropy: 0.311190.\n",
      "Iteration 19223: Policy loss: 0.063592. Value loss: 0.031489. Entropy: 0.310313.\n",
      "Iteration 19224: Policy loss: 0.058503. Value loss: 0.024815. Entropy: 0.309785.\n",
      "episode: 6699   score: 230.0  epsilon: 1.0    steps: 336  evaluation reward: 359.15\n",
      "episode: 6700   score: 380.0  epsilon: 1.0    steps: 688  evaluation reward: 360.85\n",
      "Training network. lr: 0.000103. clip: 0.041027\n",
      "Iteration 19225: Policy loss: -0.225453. Value loss: 0.343927. Entropy: 0.283822.\n",
      "Iteration 19226: Policy loss: -0.212008. Value loss: 0.216841. Entropy: 0.281762.\n",
      "Iteration 19227: Policy loss: -0.205986. Value loss: 0.164462. Entropy: 0.282909.\n",
      "Training network. lr: 0.000103. clip: 0.041027\n",
      "Iteration 19228: Policy loss: 0.042198. Value loss: 0.118667. Entropy: 0.311779.\n",
      "Iteration 19229: Policy loss: 0.047261. Value loss: 0.048595. Entropy: 0.311261.\n",
      "Iteration 19230: Policy loss: 0.039301. Value loss: 0.035231. Entropy: 0.311610.\n",
      "now time :  2019-09-06 10:06:46.378921\n",
      "episode: 6701   score: 370.0  epsilon: 1.0    steps: 56  evaluation reward: 358.25\n",
      "Training network. lr: 0.000103. clip: 0.041027\n",
      "Iteration 19231: Policy loss: 0.057965. Value loss: 0.096703. Entropy: 0.298059.\n",
      "Iteration 19232: Policy loss: 0.054630. Value loss: 0.037595. Entropy: 0.297735.\n",
      "Iteration 19233: Policy loss: 0.050865. Value loss: 0.026958. Entropy: 0.297837.\n",
      "episode: 6702   score: 560.0  epsilon: 1.0    steps: 48  evaluation reward: 359.3\n",
      "episode: 6703   score: 310.0  epsilon: 1.0    steps: 368  evaluation reward: 360.5\n",
      "Training network. lr: 0.000103. clip: 0.041027\n",
      "Iteration 19234: Policy loss: 0.137047. Value loss: 0.158781. Entropy: 0.289389.\n",
      "Iteration 19235: Policy loss: 0.138026. Value loss: 0.066395. Entropy: 0.289103.\n",
      "Iteration 19236: Policy loss: 0.136087. Value loss: 0.048884. Entropy: 0.289872.\n",
      "episode: 6704   score: 360.0  epsilon: 1.0    steps: 504  evaluation reward: 361.95\n",
      "Training network. lr: 0.000103. clip: 0.041027\n",
      "Iteration 19237: Policy loss: 0.179422. Value loss: 0.128969. Entropy: 0.304552.\n",
      "Iteration 19238: Policy loss: 0.170459. Value loss: 0.066420. Entropy: 0.304942.\n",
      "Iteration 19239: Policy loss: 0.170765. Value loss: 0.046398. Entropy: 0.304794.\n",
      "Training network. lr: 0.000103. clip: 0.041027\n",
      "Iteration 19240: Policy loss: 0.182795. Value loss: 0.132784. Entropy: 0.316011.\n",
      "Iteration 19241: Policy loss: 0.180603. Value loss: 0.048252. Entropy: 0.314357.\n",
      "Iteration 19242: Policy loss: 0.172976. Value loss: 0.034741. Entropy: 0.314697.\n",
      "Training network. lr: 0.000103. clip: 0.041027\n",
      "Iteration 19243: Policy loss: -0.255672. Value loss: 0.373083. Entropy: 0.316991.\n",
      "Iteration 19244: Policy loss: -0.238802. Value loss: 0.227178. Entropy: 0.315671.\n",
      "Iteration 19245: Policy loss: -0.263433. Value loss: 0.194272. Entropy: 0.315330.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6705   score: 340.0  epsilon: 1.0    steps: 152  evaluation reward: 361.6\n",
      "episode: 6706   score: 365.0  epsilon: 1.0    steps: 304  evaluation reward: 363.9\n",
      "episode: 6707   score: 335.0  epsilon: 1.0    steps: 1024  evaluation reward: 362.95\n",
      "Training network. lr: 0.000103. clip: 0.041027\n",
      "Iteration 19246: Policy loss: 0.269790. Value loss: 0.141777. Entropy: 0.282250.\n",
      "Iteration 19247: Policy loss: 0.258287. Value loss: 0.051608. Entropy: 0.281557.\n",
      "Iteration 19248: Policy loss: 0.262354. Value loss: 0.031417. Entropy: 0.280510.\n",
      "episode: 6708   score: 475.0  epsilon: 1.0    steps: 80  evaluation reward: 365.45\n",
      "Training network. lr: 0.000103. clip: 0.041027\n",
      "Iteration 19249: Policy loss: -0.081374. Value loss: 0.110832. Entropy: 0.289892.\n",
      "Iteration 19250: Policy loss: -0.077419. Value loss: 0.052463. Entropy: 0.288392.\n",
      "Iteration 19251: Policy loss: -0.089098. Value loss: 0.041025. Entropy: 0.288237.\n",
      "episode: 6709   score: 305.0  epsilon: 1.0    steps: 736  evaluation reward: 366.0\n",
      "Training network. lr: 0.000102. clip: 0.040870\n",
      "Iteration 19252: Policy loss: 0.159945. Value loss: 0.093133. Entropy: 0.302283.\n",
      "Iteration 19253: Policy loss: 0.161433. Value loss: 0.042830. Entropy: 0.302801.\n",
      "Iteration 19254: Policy loss: 0.155051. Value loss: 0.033433. Entropy: 0.302655.\n",
      "Training network. lr: 0.000102. clip: 0.040870\n",
      "Iteration 19255: Policy loss: 0.019152. Value loss: 0.145348. Entropy: 0.316937.\n",
      "Iteration 19256: Policy loss: 0.013060. Value loss: 0.057061. Entropy: 0.317550.\n",
      "Iteration 19257: Policy loss: 0.012576. Value loss: 0.039309. Entropy: 0.317526.\n",
      "Training network. lr: 0.000102. clip: 0.040870\n",
      "Iteration 19258: Policy loss: -0.182008. Value loss: 0.118928. Entropy: 0.308157.\n",
      "Iteration 19259: Policy loss: -0.184714. Value loss: 0.049639. Entropy: 0.307650.\n",
      "Iteration 19260: Policy loss: -0.179211. Value loss: 0.032661. Entropy: 0.307823.\n",
      "episode: 6710   score: 400.0  epsilon: 1.0    steps: 160  evaluation reward: 367.9\n",
      "episode: 6711   score: 420.0  epsilon: 1.0    steps: 216  evaluation reward: 369.9\n",
      "Training network. lr: 0.000102. clip: 0.040870\n",
      "Iteration 19261: Policy loss: 0.030898. Value loss: 0.087571. Entropy: 0.285710.\n",
      "Iteration 19262: Policy loss: 0.022014. Value loss: 0.035156. Entropy: 0.283798.\n",
      "Iteration 19263: Policy loss: 0.022832. Value loss: 0.023998. Entropy: 0.284199.\n",
      "episode: 6712   score: 655.0  epsilon: 1.0    steps: 760  evaluation reward: 374.4\n",
      "episode: 6713   score: 240.0  epsilon: 1.0    steps: 912  evaluation reward: 374.75\n",
      "Training network. lr: 0.000102. clip: 0.040870\n",
      "Iteration 19264: Policy loss: -0.129114. Value loss: 0.339290. Entropy: 0.300463.\n",
      "Iteration 19265: Policy loss: -0.118587. Value loss: 0.239687. Entropy: 0.299587.\n",
      "Iteration 19266: Policy loss: -0.139878. Value loss: 0.221856. Entropy: 0.299217.\n",
      "Training network. lr: 0.000102. clip: 0.040870\n",
      "Iteration 19267: Policy loss: -0.063622. Value loss: 0.056960. Entropy: 0.300505.\n",
      "Iteration 19268: Policy loss: -0.061450. Value loss: 0.028571. Entropy: 0.301820.\n",
      "Iteration 19269: Policy loss: -0.066914. Value loss: 0.023559. Entropy: 0.301079.\n",
      "Training network. lr: 0.000102. clip: 0.040870\n",
      "Iteration 19270: Policy loss: 0.248724. Value loss: 0.087041. Entropy: 0.318606.\n",
      "Iteration 19271: Policy loss: 0.253696. Value loss: 0.031945. Entropy: 0.318369.\n",
      "Iteration 19272: Policy loss: 0.250546. Value loss: 0.024409. Entropy: 0.319393.\n",
      "Training network. lr: 0.000102. clip: 0.040870\n",
      "Iteration 19273: Policy loss: -0.083967. Value loss: 0.086303. Entropy: 0.303896.\n",
      "Iteration 19274: Policy loss: -0.082844. Value loss: 0.047550. Entropy: 0.305972.\n",
      "Iteration 19275: Policy loss: -0.076077. Value loss: 0.035228. Entropy: 0.305357.\n",
      "Training network. lr: 0.000102. clip: 0.040870\n",
      "Iteration 19276: Policy loss: -0.066692. Value loss: 0.345362. Entropy: 0.311277.\n",
      "Iteration 19277: Policy loss: -0.055814. Value loss: 0.183794. Entropy: 0.312269.\n",
      "Iteration 19278: Policy loss: -0.046130. Value loss: 0.112148. Entropy: 0.311228.\n",
      "episode: 6714   score: 625.0  epsilon: 1.0    steps: 48  evaluation reward: 378.75\n",
      "episode: 6715   score: 730.0  epsilon: 1.0    steps: 904  evaluation reward: 382.0\n",
      "episode: 6716   score: 500.0  epsilon: 1.0    steps: 992  evaluation reward: 384.95\n",
      "Training network. lr: 0.000102. clip: 0.040870\n",
      "Iteration 19279: Policy loss: -0.260657. Value loss: 0.147636. Entropy: 0.296798.\n",
      "Iteration 19280: Policy loss: -0.271510. Value loss: 0.063671. Entropy: 0.296893.\n",
      "Iteration 19281: Policy loss: -0.269202. Value loss: 0.044633. Entropy: 0.296924.\n",
      "episode: 6717   score: 540.0  epsilon: 1.0    steps: 8  evaluation reward: 387.3\n",
      "Training network. lr: 0.000102. clip: 0.040870\n",
      "Iteration 19282: Policy loss: 0.378877. Value loss: 0.116584. Entropy: 0.283384.\n",
      "Iteration 19283: Policy loss: 0.368440. Value loss: 0.040605. Entropy: 0.280491.\n",
      "Iteration 19284: Policy loss: 0.358664. Value loss: 0.030332. Entropy: 0.282499.\n",
      "Training network. lr: 0.000102. clip: 0.040870\n",
      "Iteration 19285: Policy loss: 0.026905. Value loss: 0.111361. Entropy: 0.318948.\n",
      "Iteration 19286: Policy loss: 0.021048. Value loss: 0.052651. Entropy: 0.317981.\n",
      "Iteration 19287: Policy loss: 0.025974. Value loss: 0.037163. Entropy: 0.318920.\n",
      "episode: 6718   score: 395.0  epsilon: 1.0    steps: 656  evaluation reward: 388.4\n",
      "episode: 6719   score: 395.0  epsilon: 1.0    steps: 728  evaluation reward: 388.25\n",
      "Training network. lr: 0.000102. clip: 0.040870\n",
      "Iteration 19288: Policy loss: 0.195733. Value loss: 0.088029. Entropy: 0.288109.\n",
      "Iteration 19289: Policy loss: 0.188165. Value loss: 0.044012. Entropy: 0.287398.\n",
      "Iteration 19290: Policy loss: 0.187206. Value loss: 0.032970. Entropy: 0.287963.\n",
      "Training network. lr: 0.000102. clip: 0.040870\n",
      "Iteration 19291: Policy loss: -0.027717. Value loss: 0.110533. Entropy: 0.311616.\n",
      "Iteration 19292: Policy loss: -0.036877. Value loss: 0.060265. Entropy: 0.311893.\n",
      "Iteration 19293: Policy loss: -0.035431. Value loss: 0.046711. Entropy: 0.311655.\n",
      "episode: 6720   score: 475.0  epsilon: 1.0    steps: 80  evaluation reward: 390.15\n",
      "Training network. lr: 0.000102. clip: 0.040870\n",
      "Iteration 19294: Policy loss: -0.408490. Value loss: 0.294541. Entropy: 0.303942.\n",
      "Iteration 19295: Policy loss: -0.447570. Value loss: 0.207799. Entropy: 0.303699.\n",
      "Iteration 19296: Policy loss: -0.447342. Value loss: 0.111416. Entropy: 0.303718.\n",
      "Training network. lr: 0.000102. clip: 0.040870\n",
      "Iteration 19297: Policy loss: -0.085315. Value loss: 0.329198. Entropy: 0.312954.\n",
      "Iteration 19298: Policy loss: -0.083356. Value loss: 0.151392. Entropy: 0.313748.\n",
      "Iteration 19299: Policy loss: -0.093664. Value loss: 0.076959. Entropy: 0.313384.\n",
      "episode: 6721   score: 665.0  epsilon: 1.0    steps: 72  evaluation reward: 394.2\n",
      "Training network. lr: 0.000102. clip: 0.040870\n",
      "Iteration 19300: Policy loss: -0.113558. Value loss: 0.196410. Entropy: 0.304138.\n",
      "Iteration 19301: Policy loss: -0.114568. Value loss: 0.110048. Entropy: 0.304501.\n",
      "Iteration 19302: Policy loss: -0.125487. Value loss: 0.066469. Entropy: 0.303629.\n",
      "episode: 6722   score: 315.0  epsilon: 1.0    steps: 80  evaluation reward: 394.2\n",
      "Training network. lr: 0.000102. clip: 0.040713\n",
      "Iteration 19303: Policy loss: 0.027540. Value loss: 0.238763. Entropy: 0.304279.\n",
      "Iteration 19304: Policy loss: 0.009742. Value loss: 0.148285. Entropy: 0.307260.\n",
      "Iteration 19305: Policy loss: 0.003345. Value loss: 0.064108. Entropy: 0.305781.\n",
      "episode: 6723   score: 395.0  epsilon: 1.0    steps: 216  evaluation reward: 395.1\n",
      "Training network. lr: 0.000102. clip: 0.040713\n",
      "Iteration 19306: Policy loss: -0.028227. Value loss: 0.230173. Entropy: 0.305075.\n",
      "Iteration 19307: Policy loss: -0.032622. Value loss: 0.098613. Entropy: 0.304752.\n",
      "Iteration 19308: Policy loss: -0.034370. Value loss: 0.060260. Entropy: 0.304825.\n",
      "Training network. lr: 0.000102. clip: 0.040713\n",
      "Iteration 19309: Policy loss: 0.095870. Value loss: 0.349711. Entropy: 0.314058.\n",
      "Iteration 19310: Policy loss: 0.084447. Value loss: 0.092944. Entropy: 0.312444.\n",
      "Iteration 19311: Policy loss: 0.066521. Value loss: 0.045398. Entropy: 0.313909.\n",
      "episode: 6724   score: 590.0  epsilon: 1.0    steps: 608  evaluation reward: 392.75\n",
      "Training network. lr: 0.000102. clip: 0.040713\n",
      "Iteration 19312: Policy loss: 0.189404. Value loss: 0.142384. Entropy: 0.302798.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19313: Policy loss: 0.185184. Value loss: 0.054640. Entropy: 0.300869.\n",
      "Iteration 19314: Policy loss: 0.185448. Value loss: 0.032420. Entropy: 0.301492.\n",
      "Training network. lr: 0.000102. clip: 0.040713\n",
      "Iteration 19315: Policy loss: -0.288434. Value loss: 0.403931. Entropy: 0.319674.\n",
      "Iteration 19316: Policy loss: -0.299240. Value loss: 0.199586. Entropy: 0.320099.\n",
      "Iteration 19317: Policy loss: -0.304310. Value loss: 0.142732. Entropy: 0.319772.\n",
      "episode: 6725   score: 715.0  epsilon: 1.0    steps: 352  evaluation reward: 392.55\n",
      "episode: 6726   score: 660.0  epsilon: 1.0    steps: 824  evaluation reward: 397.65\n",
      "Training network. lr: 0.000102. clip: 0.040713\n",
      "Iteration 19318: Policy loss: 0.151515. Value loss: 0.118626. Entropy: 0.282643.\n",
      "Iteration 19319: Policy loss: 0.143891. Value loss: 0.051173. Entropy: 0.283012.\n",
      "Iteration 19320: Policy loss: 0.143817. Value loss: 0.033220. Entropy: 0.282280.\n",
      "episode: 6727   score: 300.0  epsilon: 1.0    steps: 8  evaluation reward: 397.1\n",
      "episode: 6728   score: 670.0  epsilon: 1.0    steps: 512  evaluation reward: 396.55\n",
      "episode: 6729   score: 315.0  epsilon: 1.0    steps: 816  evaluation reward: 396.0\n",
      "episode: 6730   score: 935.0  epsilon: 1.0    steps: 960  evaluation reward: 400.65\n",
      "Training network. lr: 0.000102. clip: 0.040713\n",
      "Iteration 19321: Policy loss: 0.068730. Value loss: 0.163365. Entropy: 0.284456.\n",
      "Iteration 19322: Policy loss: 0.070618. Value loss: 0.080243. Entropy: 0.283319.\n",
      "Iteration 19323: Policy loss: 0.061711. Value loss: 0.058868. Entropy: 0.284088.\n",
      "Training network. lr: 0.000102. clip: 0.040713\n",
      "Iteration 19324: Policy loss: -0.081487. Value loss: 0.146088. Entropy: 0.305445.\n",
      "Iteration 19325: Policy loss: -0.084974. Value loss: 0.064261. Entropy: 0.306062.\n",
      "Iteration 19326: Policy loss: -0.086562. Value loss: 0.043585. Entropy: 0.306697.\n",
      "Training network. lr: 0.000102. clip: 0.040713\n",
      "Iteration 19327: Policy loss: -0.234107. Value loss: 0.246185. Entropy: 0.310365.\n",
      "Iteration 19328: Policy loss: -0.253794. Value loss: 0.073990. Entropy: 0.309117.\n",
      "Iteration 19329: Policy loss: -0.238146. Value loss: 0.043174. Entropy: 0.308590.\n",
      "episode: 6731   score: 430.0  epsilon: 1.0    steps: 224  evaluation reward: 402.65\n",
      "Training network. lr: 0.000102. clip: 0.040713\n",
      "Iteration 19330: Policy loss: -0.089538. Value loss: 0.262266. Entropy: 0.306778.\n",
      "Iteration 19331: Policy loss: -0.100871. Value loss: 0.113467. Entropy: 0.307425.\n",
      "Iteration 19332: Policy loss: -0.088989. Value loss: 0.067884. Entropy: 0.307990.\n",
      "Training network. lr: 0.000102. clip: 0.040713\n",
      "Iteration 19333: Policy loss: -0.199220. Value loss: 0.344501. Entropy: 0.307045.\n",
      "Iteration 19334: Policy loss: -0.205428. Value loss: 0.264610. Entropy: 0.308825.\n",
      "Iteration 19335: Policy loss: -0.201904. Value loss: 0.230428. Entropy: 0.309105.\n",
      "episode: 6732   score: 430.0  epsilon: 1.0    steps: 864  evaluation reward: 400.7\n",
      "Training network. lr: 0.000102. clip: 0.040713\n",
      "Iteration 19336: Policy loss: 0.123548. Value loss: 0.162942. Entropy: 0.304921.\n",
      "Iteration 19337: Policy loss: 0.117458. Value loss: 0.072669. Entropy: 0.305684.\n",
      "Iteration 19338: Policy loss: 0.122021. Value loss: 0.055517. Entropy: 0.305024.\n",
      "episode: 6733   score: 395.0  epsilon: 1.0    steps: 368  evaluation reward: 398.7\n",
      "Training network. lr: 0.000102. clip: 0.040713\n",
      "Iteration 19339: Policy loss: 0.250356. Value loss: 0.186743. Entropy: 0.301174.\n",
      "Iteration 19340: Policy loss: 0.243839. Value loss: 0.069820. Entropy: 0.299057.\n",
      "Iteration 19341: Policy loss: 0.232079. Value loss: 0.046353. Entropy: 0.299544.\n",
      "episode: 6734   score: 545.0  epsilon: 1.0    steps: 880  evaluation reward: 401.2\n",
      "Training network. lr: 0.000102. clip: 0.040713\n",
      "Iteration 19342: Policy loss: 0.049903. Value loss: 0.066587. Entropy: 0.306878.\n",
      "Iteration 19343: Policy loss: 0.047072. Value loss: 0.036470. Entropy: 0.307414.\n",
      "Iteration 19344: Policy loss: 0.044789. Value loss: 0.028932. Entropy: 0.307001.\n",
      "Training network. lr: 0.000102. clip: 0.040713\n",
      "Iteration 19345: Policy loss: 0.095034. Value loss: 0.095908. Entropy: 0.306315.\n",
      "Iteration 19346: Policy loss: 0.088915. Value loss: 0.031325. Entropy: 0.304594.\n",
      "Iteration 19347: Policy loss: 0.089324. Value loss: 0.020151. Entropy: 0.305634.\n",
      "Training network. lr: 0.000102. clip: 0.040713\n",
      "Iteration 19348: Policy loss: -0.439262. Value loss: 0.577556. Entropy: 0.315346.\n",
      "Iteration 19349: Policy loss: -0.442579. Value loss: 0.269020. Entropy: 0.315749.\n",
      "Iteration 19350: Policy loss: -0.470890. Value loss: 0.177257. Entropy: 0.315611.\n",
      "episode: 6735   score: 490.0  epsilon: 1.0    steps: 272  evaluation reward: 404.3\n",
      "episode: 6736   score: 390.0  epsilon: 1.0    steps: 880  evaluation reward: 403.95\n",
      "Training network. lr: 0.000101. clip: 0.040566\n",
      "Iteration 19351: Policy loss: -0.106895. Value loss: 0.417696. Entropy: 0.292488.\n",
      "Iteration 19352: Policy loss: -0.114604. Value loss: 0.176486. Entropy: 0.293867.\n",
      "Iteration 19353: Policy loss: -0.112323. Value loss: 0.119300. Entropy: 0.292957.\n",
      "episode: 6737   score: 795.0  epsilon: 1.0    steps: 280  evaluation reward: 409.05\n",
      "episode: 6738   score: 485.0  epsilon: 1.0    steps: 584  evaluation reward: 410.2\n",
      "episode: 6739   score: 685.0  epsilon: 1.0    steps: 920  evaluation reward: 414.15\n",
      "Training network. lr: 0.000101. clip: 0.040566\n",
      "Iteration 19354: Policy loss: 0.038763. Value loss: 0.096687. Entropy: 0.287524.\n",
      "Iteration 19355: Policy loss: 0.033864. Value loss: 0.045927. Entropy: 0.286758.\n",
      "Iteration 19356: Policy loss: 0.028904. Value loss: 0.032867. Entropy: 0.287441.\n",
      "Training network. lr: 0.000101. clip: 0.040566\n",
      "Iteration 19357: Policy loss: -0.037101. Value loss: 0.132314. Entropy: 0.314327.\n",
      "Iteration 19358: Policy loss: -0.030191. Value loss: 0.067742. Entropy: 0.314718.\n",
      "Iteration 19359: Policy loss: -0.033589. Value loss: 0.049324. Entropy: 0.314625.\n",
      "episode: 6740   score: 360.0  epsilon: 1.0    steps: 776  evaluation reward: 415.5\n",
      "Training network. lr: 0.000101. clip: 0.040566\n",
      "Iteration 19360: Policy loss: 0.345884. Value loss: 0.187238. Entropy: 0.302067.\n",
      "Iteration 19361: Policy loss: 0.331078. Value loss: 0.072149. Entropy: 0.302722.\n",
      "Iteration 19362: Policy loss: 0.336516. Value loss: 0.046882. Entropy: 0.302611.\n",
      "Training network. lr: 0.000101. clip: 0.040566\n",
      "Iteration 19363: Policy loss: 0.243385. Value loss: 0.080218. Entropy: 0.304788.\n",
      "Iteration 19364: Policy loss: 0.240674. Value loss: 0.029288. Entropy: 0.306068.\n",
      "Iteration 19365: Policy loss: 0.233967. Value loss: 0.019460. Entropy: 0.306675.\n",
      "Training network. lr: 0.000101. clip: 0.040566\n",
      "Iteration 19366: Policy loss: -0.033773. Value loss: 0.083917. Entropy: 0.317499.\n",
      "Iteration 19367: Policy loss: -0.029744. Value loss: 0.047912. Entropy: 0.317449.\n",
      "Iteration 19368: Policy loss: -0.035684. Value loss: 0.037458. Entropy: 0.318108.\n",
      "Training network. lr: 0.000101. clip: 0.040566\n",
      "Iteration 19369: Policy loss: -0.413640. Value loss: 0.448085. Entropy: 0.306849.\n",
      "Iteration 19370: Policy loss: -0.447590. Value loss: 0.211353. Entropy: 0.307524.\n",
      "Iteration 19371: Policy loss: -0.440469. Value loss: 0.124593. Entropy: 0.308257.\n",
      "Training network. lr: 0.000101. clip: 0.040566\n",
      "Iteration 19372: Policy loss: -0.098251. Value loss: 0.318510. Entropy: 0.313979.\n",
      "Iteration 19373: Policy loss: -0.112097. Value loss: 0.168805. Entropy: 0.314118.\n",
      "Iteration 19374: Policy loss: -0.110130. Value loss: 0.118049. Entropy: 0.314448.\n",
      "episode: 6741   score: 590.0  epsilon: 1.0    steps: 248  evaluation reward: 417.8\n",
      "Training network. lr: 0.000101. clip: 0.040566\n",
      "Iteration 19375: Policy loss: 0.094380. Value loss: 0.126947. Entropy: 0.302768.\n",
      "Iteration 19376: Policy loss: 0.086209. Value loss: 0.064468. Entropy: 0.302892.\n",
      "Iteration 19377: Policy loss: 0.088247. Value loss: 0.044271. Entropy: 0.302132.\n",
      "episode: 6742   score: 785.0  epsilon: 1.0    steps: 96  evaluation reward: 422.5\n",
      "episode: 6743   score: 495.0  epsilon: 1.0    steps: 344  evaluation reward: 423.7\n",
      "Training network. lr: 0.000101. clip: 0.040566\n",
      "Iteration 19378: Policy loss: -0.214354. Value loss: 0.265597. Entropy: 0.291140.\n",
      "Iteration 19379: Policy loss: -0.221450. Value loss: 0.112192. Entropy: 0.288391.\n",
      "Iteration 19380: Policy loss: -0.234201. Value loss: 0.063650. Entropy: 0.289051.\n",
      "Training network. lr: 0.000101. clip: 0.040566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19381: Policy loss: -0.123216. Value loss: 0.256472. Entropy: 0.312680.\n",
      "Iteration 19382: Policy loss: -0.125880. Value loss: 0.086406. Entropy: 0.311278.\n",
      "Iteration 19383: Policy loss: -0.134501. Value loss: 0.057033. Entropy: 0.312266.\n",
      "episode: 6744   score: 535.0  epsilon: 1.0    steps: 496  evaluation reward: 427.2\n",
      "Training network. lr: 0.000101. clip: 0.040566\n",
      "Iteration 19384: Policy loss: 0.548653. Value loss: 0.458021. Entropy: 0.302401.\n",
      "Iteration 19385: Policy loss: 0.545196. Value loss: 0.133484. Entropy: 0.304089.\n",
      "Iteration 19386: Policy loss: 0.543712. Value loss: 0.083732. Entropy: 0.303793.\n",
      "episode: 6745   score: 680.0  epsilon: 1.0    steps: 80  evaluation reward: 431.6\n",
      "episode: 6746   score: 550.0  epsilon: 1.0    steps: 320  evaluation reward: 434.0\n",
      "episode: 6747   score: 895.0  epsilon: 1.0    steps: 536  evaluation reward: 442.05\n",
      "episode: 6748   score: 650.0  epsilon: 1.0    steps: 880  evaluation reward: 445.9\n",
      "Training network. lr: 0.000101. clip: 0.040566\n",
      "Iteration 19387: Policy loss: 0.815423. Value loss: 0.349252. Entropy: 0.273031.\n",
      "Iteration 19388: Policy loss: 0.807338. Value loss: 0.089307. Entropy: 0.273477.\n",
      "Iteration 19389: Policy loss: 0.800088. Value loss: 0.054609. Entropy: 0.270904.\n",
      "Training network. lr: 0.000101. clip: 0.040566\n",
      "Iteration 19390: Policy loss: 0.273827. Value loss: 0.123810. Entropy: 0.317634.\n",
      "Iteration 19391: Policy loss: 0.261506. Value loss: 0.059673. Entropy: 0.316352.\n",
      "Iteration 19392: Policy loss: 0.264708. Value loss: 0.046120. Entropy: 0.316416.\n",
      "Training network. lr: 0.000101. clip: 0.040566\n",
      "Iteration 19393: Policy loss: 0.357724. Value loss: 0.149387. Entropy: 0.317622.\n",
      "Iteration 19394: Policy loss: 0.355545. Value loss: 0.072494. Entropy: 0.317087.\n",
      "Iteration 19395: Policy loss: 0.352564. Value loss: 0.053383. Entropy: 0.317029.\n",
      "episode: 6749   score: 115.0  epsilon: 1.0    steps: 848  evaluation reward: 443.7\n",
      "Training network. lr: 0.000101. clip: 0.040566\n",
      "Iteration 19396: Policy loss: 0.531367. Value loss: 0.207166. Entropy: 0.303038.\n",
      "Iteration 19397: Policy loss: 0.517685. Value loss: 0.100972. Entropy: 0.302141.\n",
      "Iteration 19398: Policy loss: 0.526178. Value loss: 0.070133. Entropy: 0.301426.\n",
      "episode: 6750   score: 385.0  epsilon: 1.0    steps: 720  evaluation reward: 444.4\n",
      "Training network. lr: 0.000101. clip: 0.040566\n",
      "Iteration 19399: Policy loss: 0.035338. Value loss: 0.129095. Entropy: 0.305652.\n",
      "Iteration 19400: Policy loss: 0.031113. Value loss: 0.054761. Entropy: 0.306596.\n",
      "Iteration 19401: Policy loss: 0.034698. Value loss: 0.035304. Entropy: 0.304731.\n",
      "now time :  2019-09-06 10:17:08.929582\n",
      "episode: 6751   score: 425.0  epsilon: 1.0    steps: 48  evaluation reward: 441.95\n",
      "episode: 6752   score: 485.0  epsilon: 1.0    steps: 736  evaluation reward: 444.8\n",
      "Training network. lr: 0.000101. clip: 0.040409\n",
      "Iteration 19402: Policy loss: 0.022218. Value loss: 0.119396. Entropy: 0.291154.\n",
      "Iteration 19403: Policy loss: 0.021739. Value loss: 0.052149. Entropy: 0.291208.\n",
      "Iteration 19404: Policy loss: 0.011744. Value loss: 0.037998. Entropy: 0.290530.\n",
      "Training network. lr: 0.000101. clip: 0.040409\n",
      "Iteration 19405: Policy loss: -0.107060. Value loss: 0.146719. Entropy: 0.317088.\n",
      "Iteration 19406: Policy loss: -0.118525. Value loss: 0.075413. Entropy: 0.318228.\n",
      "Iteration 19407: Policy loss: -0.127402. Value loss: 0.050043. Entropy: 0.317650.\n",
      "Training network. lr: 0.000101. clip: 0.040409\n",
      "Iteration 19408: Policy loss: 0.208531. Value loss: 0.220007. Entropy: 0.313780.\n",
      "Iteration 19409: Policy loss: 0.193687. Value loss: 0.071456. Entropy: 0.313060.\n",
      "Iteration 19410: Policy loss: 0.187211. Value loss: 0.048489. Entropy: 0.313089.\n",
      "episode: 6753   score: 365.0  epsilon: 1.0    steps: 200  evaluation reward: 444.85\n",
      "Training network. lr: 0.000101. clip: 0.040409\n",
      "Iteration 19411: Policy loss: 0.214520. Value loss: 0.108967. Entropy: 0.307737.\n",
      "Iteration 19412: Policy loss: 0.222709. Value loss: 0.048837. Entropy: 0.307871.\n",
      "Iteration 19413: Policy loss: 0.217526. Value loss: 0.035415. Entropy: 0.308167.\n",
      "episode: 6754   score: 420.0  epsilon: 1.0    steps: 112  evaluation reward: 444.85\n",
      "Training network. lr: 0.000101. clip: 0.040409\n",
      "Iteration 19414: Policy loss: -0.245202. Value loss: 0.262867. Entropy: 0.300057.\n",
      "Iteration 19415: Policy loss: -0.248809. Value loss: 0.136126. Entropy: 0.302917.\n",
      "Iteration 19416: Policy loss: -0.256627. Value loss: 0.099742. Entropy: 0.300248.\n",
      "episode: 6755   score: 535.0  epsilon: 1.0    steps: 656  evaluation reward: 446.0\n",
      "Training network. lr: 0.000101. clip: 0.040409\n",
      "Iteration 19417: Policy loss: 0.080528. Value loss: 0.169372. Entropy: 0.304108.\n",
      "Iteration 19418: Policy loss: 0.086805. Value loss: 0.076946. Entropy: 0.303485.\n",
      "Iteration 19419: Policy loss: 0.066973. Value loss: 0.053495. Entropy: 0.305288.\n",
      "episode: 6756   score: 470.0  epsilon: 1.0    steps: 344  evaluation reward: 446.8\n",
      "episode: 6757   score: 275.0  epsilon: 1.0    steps: 424  evaluation reward: 446.4\n",
      "episode: 6758   score: 425.0  epsilon: 1.0    steps: 872  evaluation reward: 447.5\n",
      "Training network. lr: 0.000101. clip: 0.040409\n",
      "Iteration 19420: Policy loss: 0.372606. Value loss: 0.143156. Entropy: 0.281280.\n",
      "Iteration 19421: Policy loss: 0.369120. Value loss: 0.052583. Entropy: 0.282860.\n",
      "Iteration 19422: Policy loss: 0.369179. Value loss: 0.035222. Entropy: 0.281974.\n",
      "episode: 6759   score: 260.0  epsilon: 1.0    steps: 352  evaluation reward: 444.45\n",
      "Training network. lr: 0.000101. clip: 0.040409\n",
      "Iteration 19423: Policy loss: -0.266542. Value loss: 0.277066. Entropy: 0.308912.\n",
      "Iteration 19424: Policy loss: -0.278822. Value loss: 0.161574. Entropy: 0.307684.\n",
      "Iteration 19425: Policy loss: -0.264809. Value loss: 0.103098. Entropy: 0.308647.\n",
      "Training network. lr: 0.000101. clip: 0.040409\n",
      "Iteration 19426: Policy loss: -0.198413. Value loss: 0.120523. Entropy: 0.312963.\n",
      "Iteration 19427: Policy loss: -0.200668. Value loss: 0.067108. Entropy: 0.313037.\n",
      "Iteration 19428: Policy loss: -0.206890. Value loss: 0.050729. Entropy: 0.313387.\n",
      "Training network. lr: 0.000101. clip: 0.040409\n",
      "Iteration 19429: Policy loss: 0.007504. Value loss: 0.301188. Entropy: 0.315304.\n",
      "Iteration 19430: Policy loss: -0.003533. Value loss: 0.118007. Entropy: 0.314673.\n",
      "Iteration 19431: Policy loss: -0.000423. Value loss: 0.071241. Entropy: 0.313899.\n",
      "episode: 6760   score: 365.0  epsilon: 1.0    steps: 536  evaluation reward: 444.05\n",
      "Training network. lr: 0.000101. clip: 0.040409\n",
      "Iteration 19432: Policy loss: 0.157611. Value loss: 0.120954. Entropy: 0.300302.\n",
      "Iteration 19433: Policy loss: 0.150158. Value loss: 0.057989. Entropy: 0.298843.\n",
      "Iteration 19434: Policy loss: 0.144431. Value loss: 0.043671. Entropy: 0.299413.\n",
      "Training network. lr: 0.000101. clip: 0.040409\n",
      "Iteration 19435: Policy loss: -0.036062. Value loss: 0.086082. Entropy: 0.312653.\n",
      "Iteration 19436: Policy loss: -0.043052. Value loss: 0.041267. Entropy: 0.312767.\n",
      "Iteration 19437: Policy loss: -0.043750. Value loss: 0.031152. Entropy: 0.312780.\n",
      "Training network. lr: 0.000101. clip: 0.040409\n",
      "Iteration 19438: Policy loss: 0.043384. Value loss: 0.152546. Entropy: 0.314038.\n",
      "Iteration 19439: Policy loss: 0.032704. Value loss: 0.072482. Entropy: 0.313699.\n",
      "Iteration 19440: Policy loss: 0.031964. Value loss: 0.053672. Entropy: 0.313527.\n",
      "episode: 6761   score: 610.0  epsilon: 1.0    steps: 584  evaluation reward: 448.35\n",
      "episode: 6762   score: 315.0  epsilon: 1.0    steps: 808  evaluation reward: 445.5\n",
      "episode: 6763   score: 315.0  epsilon: 1.0    steps: 904  evaluation reward: 445.1\n",
      "Training network. lr: 0.000101. clip: 0.040409\n",
      "Iteration 19441: Policy loss: 0.130926. Value loss: 0.127498. Entropy: 0.288717.\n",
      "Iteration 19442: Policy loss: 0.124275. Value loss: 0.071410. Entropy: 0.289686.\n",
      "Iteration 19443: Policy loss: 0.117863. Value loss: 0.048504. Entropy: 0.290001.\n",
      "episode: 6764   score: 445.0  epsilon: 1.0    steps: 1016  evaluation reward: 446.05\n",
      "Training network. lr: 0.000101. clip: 0.040409\n",
      "Iteration 19444: Policy loss: -0.063677. Value loss: 0.136606. Entropy: 0.307886.\n",
      "Iteration 19445: Policy loss: -0.068195. Value loss: 0.063438. Entropy: 0.307693.\n",
      "Iteration 19446: Policy loss: -0.060095. Value loss: 0.042583. Entropy: 0.307380.\n",
      "episode: 6765   score: 365.0  epsilon: 1.0    steps: 216  evaluation reward: 445.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6766   score: 655.0  epsilon: 1.0    steps: 320  evaluation reward: 447.55\n",
      "Training network. lr: 0.000101. clip: 0.040409\n",
      "Iteration 19447: Policy loss: 0.054103. Value loss: 0.075703. Entropy: 0.283772.\n",
      "Iteration 19448: Policy loss: 0.059927. Value loss: 0.045163. Entropy: 0.284828.\n",
      "Iteration 19449: Policy loss: 0.051557. Value loss: 0.036240. Entropy: 0.283625.\n",
      "episode: 6767   score: 695.0  epsilon: 1.0    steps: 664  evaluation reward: 449.25\n",
      "Training network. lr: 0.000101. clip: 0.040409\n",
      "Iteration 19450: Policy loss: -0.029665. Value loss: 0.266729. Entropy: 0.305520.\n",
      "Iteration 19451: Policy loss: -0.039930. Value loss: 0.135824. Entropy: 0.304891.\n",
      "Iteration 19452: Policy loss: -0.027109. Value loss: 0.091200. Entropy: 0.305345.\n",
      "Training network. lr: 0.000101. clip: 0.040253\n",
      "Iteration 19453: Policy loss: 0.093040. Value loss: 0.207928. Entropy: 0.315517.\n",
      "Iteration 19454: Policy loss: 0.084580. Value loss: 0.122015. Entropy: 0.314692.\n",
      "Iteration 19455: Policy loss: 0.092603. Value loss: 0.086827. Entropy: 0.314592.\n",
      "episode: 6768   score: 435.0  epsilon: 1.0    steps: 480  evaluation reward: 450.3\n",
      "Training network. lr: 0.000101. clip: 0.040253\n",
      "Iteration 19456: Policy loss: 0.021039. Value loss: 0.510249. Entropy: 0.296405.\n",
      "Iteration 19457: Policy loss: -0.005975. Value loss: 0.243330. Entropy: 0.296254.\n",
      "Iteration 19458: Policy loss: 0.000993. Value loss: 0.143605. Entropy: 0.296526.\n",
      "episode: 6769   score: 180.0  epsilon: 1.0    steps: 856  evaluation reward: 448.45\n",
      "Training network. lr: 0.000101. clip: 0.040253\n",
      "Iteration 19459: Policy loss: 0.033710. Value loss: 0.266124. Entropy: 0.309116.\n",
      "Iteration 19460: Policy loss: 0.038389. Value loss: 0.147125. Entropy: 0.308606.\n",
      "Iteration 19461: Policy loss: 0.042908. Value loss: 0.098230. Entropy: 0.308734.\n",
      "Training network. lr: 0.000101. clip: 0.040253\n",
      "Iteration 19462: Policy loss: -0.100711. Value loss: 0.355927. Entropy: 0.303527.\n",
      "Iteration 19463: Policy loss: -0.104053. Value loss: 0.231232. Entropy: 0.302103.\n",
      "Iteration 19464: Policy loss: -0.104072. Value loss: 0.154595. Entropy: 0.302476.\n",
      "episode: 6770   score: 515.0  epsilon: 1.0    steps: 856  evaluation reward: 449.6\n",
      "Training network. lr: 0.000101. clip: 0.040253\n",
      "Iteration 19465: Policy loss: -0.177634. Value loss: 0.312947. Entropy: 0.310675.\n",
      "Iteration 19466: Policy loss: -0.177687. Value loss: 0.145687. Entropy: 0.310514.\n",
      "Iteration 19467: Policy loss: -0.186908. Value loss: 0.086949. Entropy: 0.310913.\n",
      "episode: 6771   score: 260.0  epsilon: 1.0    steps: 680  evaluation reward: 448.5\n",
      "Training network. lr: 0.000101. clip: 0.040253\n",
      "Iteration 19468: Policy loss: 0.394578. Value loss: 0.320510. Entropy: 0.294140.\n",
      "Iteration 19469: Policy loss: 0.381448. Value loss: 0.107699. Entropy: 0.293277.\n",
      "Iteration 19470: Policy loss: 0.377249. Value loss: 0.069836. Entropy: 0.293929.\n",
      "episode: 6772   score: 400.0  epsilon: 1.0    steps: 24  evaluation reward: 449.2\n",
      "episode: 6773   score: 595.0  epsilon: 1.0    steps: 920  evaluation reward: 449.15\n",
      "Training network. lr: 0.000101. clip: 0.040253\n",
      "Iteration 19471: Policy loss: -0.410105. Value loss: 0.375475. Entropy: 0.295609.\n",
      "Iteration 19472: Policy loss: -0.420823. Value loss: 0.282847. Entropy: 0.295834.\n",
      "Iteration 19473: Policy loss: -0.410137. Value loss: 0.221620. Entropy: 0.295972.\n",
      "Training network. lr: 0.000101. clip: 0.040253\n",
      "Iteration 19474: Policy loss: 0.016987. Value loss: 0.306876. Entropy: 0.307062.\n",
      "Iteration 19475: Policy loss: 0.001564. Value loss: 0.157025. Entropy: 0.306099.\n",
      "Iteration 19476: Policy loss: 0.004543. Value loss: 0.105561. Entropy: 0.306410.\n",
      "episode: 6774   score: 755.0  epsilon: 1.0    steps: 224  evaluation reward: 453.4\n",
      "episode: 6775   score: 360.0  epsilon: 1.0    steps: 680  evaluation reward: 453.4\n",
      "Training network. lr: 0.000101. clip: 0.040253\n",
      "Iteration 19477: Policy loss: 0.068413. Value loss: 0.167823. Entropy: 0.284291.\n",
      "Iteration 19478: Policy loss: 0.064963. Value loss: 0.080927. Entropy: 0.284637.\n",
      "Iteration 19479: Policy loss: 0.073267. Value loss: 0.057074. Entropy: 0.284235.\n",
      "Training network. lr: 0.000101. clip: 0.040253\n",
      "Iteration 19480: Policy loss: -0.102872. Value loss: 0.286296. Entropy: 0.317658.\n",
      "Iteration 19481: Policy loss: -0.103679. Value loss: 0.152390. Entropy: 0.316977.\n",
      "Iteration 19482: Policy loss: -0.112145. Value loss: 0.110592. Entropy: 0.317707.\n",
      "episode: 6776   score: 185.0  epsilon: 1.0    steps: 944  evaluation reward: 453.3\n",
      "Training network. lr: 0.000101. clip: 0.040253\n",
      "Iteration 19483: Policy loss: 0.449312. Value loss: 0.169947. Entropy: 0.311552.\n",
      "Iteration 19484: Policy loss: 0.452899. Value loss: 0.067407. Entropy: 0.311128.\n",
      "Iteration 19485: Policy loss: 0.449390. Value loss: 0.045568. Entropy: 0.310906.\n",
      "episode: 6777   score: 485.0  epsilon: 1.0    steps: 1008  evaluation reward: 453.2\n",
      "Training network. lr: 0.000101. clip: 0.040253\n",
      "Iteration 19486: Policy loss: -0.110656. Value loss: 0.209472. Entropy: 0.305703.\n",
      "Iteration 19487: Policy loss: -0.124334. Value loss: 0.110145. Entropy: 0.304772.\n",
      "Iteration 19488: Policy loss: -0.119924. Value loss: 0.081417. Entropy: 0.306044.\n",
      "Training network. lr: 0.000101. clip: 0.040253\n",
      "Iteration 19489: Policy loss: 0.033011. Value loss: 0.162235. Entropy: 0.297901.\n",
      "Iteration 19490: Policy loss: 0.025047. Value loss: 0.069847. Entropy: 0.297418.\n",
      "Iteration 19491: Policy loss: 0.023650. Value loss: 0.050738. Entropy: 0.297633.\n",
      "episode: 6778   score: 595.0  epsilon: 1.0    steps: 352  evaluation reward: 456.2\n",
      "episode: 6779   score: 1105.0  epsilon: 1.0    steps: 512  evaluation reward: 464.55\n",
      "episode: 6780   score: 670.0  epsilon: 1.0    steps: 704  evaluation reward: 463.05\n",
      "Training network. lr: 0.000101. clip: 0.040253\n",
      "Iteration 19492: Policy loss: 0.117692. Value loss: 0.132439. Entropy: 0.288221.\n",
      "Iteration 19493: Policy loss: 0.118355. Value loss: 0.066845. Entropy: 0.289124.\n",
      "Iteration 19494: Policy loss: 0.107231. Value loss: 0.055292. Entropy: 0.288786.\n",
      "episode: 6781   score: 565.0  epsilon: 1.0    steps: 912  evaluation reward: 463.4\n",
      "Training network. lr: 0.000101. clip: 0.040253\n",
      "Iteration 19495: Policy loss: 0.044715. Value loss: 0.181521. Entropy: 0.309399.\n",
      "Iteration 19496: Policy loss: 0.038384. Value loss: 0.066557. Entropy: 0.309443.\n",
      "Iteration 19497: Policy loss: 0.039099. Value loss: 0.042737. Entropy: 0.308872.\n",
      "episode: 6782   score: 285.0  epsilon: 1.0    steps: 448  evaluation reward: 462.15\n",
      "Training network. lr: 0.000101. clip: 0.040253\n",
      "Iteration 19498: Policy loss: 0.258742. Value loss: 0.217511. Entropy: 0.292163.\n",
      "Iteration 19499: Policy loss: 0.243454. Value loss: 0.104176. Entropy: 0.293050.\n",
      "Iteration 19500: Policy loss: 0.249240. Value loss: 0.078718. Entropy: 0.293214.\n",
      "Training network. lr: 0.000100. clip: 0.040105\n",
      "Iteration 19501: Policy loss: 0.301098. Value loss: 0.153493. Entropy: 0.320819.\n",
      "Iteration 19502: Policy loss: 0.293360. Value loss: 0.053630. Entropy: 0.321280.\n",
      "Iteration 19503: Policy loss: 0.293970. Value loss: 0.040884. Entropy: 0.320605.\n",
      "Training network. lr: 0.000100. clip: 0.040105\n",
      "Iteration 19504: Policy loss: -0.034849. Value loss: 0.142667. Entropy: 0.315097.\n",
      "Iteration 19505: Policy loss: -0.028142. Value loss: 0.060551. Entropy: 0.315261.\n",
      "Iteration 19506: Policy loss: -0.041144. Value loss: 0.039983. Entropy: 0.315762.\n",
      "Training network. lr: 0.000100. clip: 0.040105\n",
      "Iteration 19507: Policy loss: 0.205111. Value loss: 0.133920. Entropy: 0.316847.\n",
      "Iteration 19508: Policy loss: 0.202845. Value loss: 0.055934. Entropy: 0.315764.\n",
      "Iteration 19509: Policy loss: 0.196309. Value loss: 0.040698. Entropy: 0.316243.\n",
      "episode: 6783   score: 340.0  epsilon: 1.0    steps: 256  evaluation reward: 461.9\n",
      "episode: 6784   score: 450.0  epsilon: 1.0    steps: 696  evaluation reward: 464.15\n",
      "Training network. lr: 0.000100. clip: 0.040105\n",
      "Iteration 19510: Policy loss: 0.201855. Value loss: 0.087988. Entropy: 0.288348.\n",
      "Iteration 19511: Policy loss: 0.196131. Value loss: 0.042139. Entropy: 0.285738.\n",
      "Iteration 19512: Policy loss: 0.189126. Value loss: 0.032658. Entropy: 0.287263.\n",
      "episode: 6785   score: 340.0  epsilon: 1.0    steps: 896  evaluation reward: 465.4\n",
      "Training network. lr: 0.000100. clip: 0.040105\n",
      "Iteration 19513: Policy loss: 0.110553. Value loss: 0.050965. Entropy: 0.310707.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19514: Policy loss: 0.102138. Value loss: 0.025833. Entropy: 0.310653.\n",
      "Iteration 19515: Policy loss: 0.103468. Value loss: 0.020947. Entropy: 0.310368.\n",
      "episode: 6786   score: 365.0  epsilon: 1.0    steps: 16  evaluation reward: 465.5\n",
      "episode: 6787   score: 400.0  epsilon: 1.0    steps: 344  evaluation reward: 464.15\n",
      "Training network. lr: 0.000100. clip: 0.040105\n",
      "Iteration 19516: Policy loss: -0.052224. Value loss: 0.114022. Entropy: 0.278223.\n",
      "Iteration 19517: Policy loss: -0.053373. Value loss: 0.050684. Entropy: 0.278477.\n",
      "Iteration 19518: Policy loss: -0.057718. Value loss: 0.038591. Entropy: 0.278902.\n",
      "episode: 6788   score: 315.0  epsilon: 1.0    steps: 208  evaluation reward: 463.7\n",
      "episode: 6789   score: 495.0  epsilon: 1.0    steps: 872  evaluation reward: 465.8\n",
      "Training network. lr: 0.000100. clip: 0.040105\n",
      "Iteration 19519: Policy loss: -0.103325. Value loss: 0.114043. Entropy: 0.298626.\n",
      "Iteration 19520: Policy loss: -0.115653. Value loss: 0.055302. Entropy: 0.300219.\n",
      "Iteration 19521: Policy loss: -0.115694. Value loss: 0.038716. Entropy: 0.299712.\n",
      "episode: 6790   score: 545.0  epsilon: 1.0    steps: 56  evaluation reward: 466.2\n",
      "Training network. lr: 0.000100. clip: 0.040105\n",
      "Iteration 19522: Policy loss: 0.144163. Value loss: 0.118597. Entropy: 0.294932.\n",
      "Iteration 19523: Policy loss: 0.144089. Value loss: 0.058203. Entropy: 0.294334.\n",
      "Iteration 19524: Policy loss: 0.138189. Value loss: 0.041244. Entropy: 0.294692.\n",
      "Training network. lr: 0.000100. clip: 0.040105\n",
      "Iteration 19525: Policy loss: -0.112881. Value loss: 0.345460. Entropy: 0.315481.\n",
      "Iteration 19526: Policy loss: -0.131710. Value loss: 0.208293. Entropy: 0.314801.\n",
      "Iteration 19527: Policy loss: -0.132823. Value loss: 0.148274. Entropy: 0.314315.\n",
      "Training network. lr: 0.000100. clip: 0.040105\n",
      "Iteration 19528: Policy loss: -0.212529. Value loss: 0.329366. Entropy: 0.309027.\n",
      "Iteration 19529: Policy loss: -0.204293. Value loss: 0.116384. Entropy: 0.308644.\n",
      "Iteration 19530: Policy loss: -0.214327. Value loss: 0.075120. Entropy: 0.308334.\n",
      "episode: 6791   score: 320.0  epsilon: 1.0    steps: 88  evaluation reward: 467.3\n",
      "Training network. lr: 0.000100. clip: 0.040105\n",
      "Iteration 19531: Policy loss: 0.062719. Value loss: 0.135771. Entropy: 0.310832.\n",
      "Iteration 19532: Policy loss: 0.054306. Value loss: 0.074609. Entropy: 0.310707.\n",
      "Iteration 19533: Policy loss: 0.059360. Value loss: 0.054448. Entropy: 0.311704.\n",
      "Training network. lr: 0.000100. clip: 0.040105\n",
      "Iteration 19534: Policy loss: 0.125910. Value loss: 0.128727. Entropy: 0.310713.\n",
      "Iteration 19535: Policy loss: 0.119167. Value loss: 0.051916. Entropy: 0.310281.\n",
      "Iteration 19536: Policy loss: 0.115851. Value loss: 0.034431. Entropy: 0.310863.\n",
      "episode: 6792   score: 320.0  epsilon: 1.0    steps: 264  evaluation reward: 467.5\n",
      "episode: 6793   score: 315.0  epsilon: 1.0    steps: 688  evaluation reward: 467.7\n",
      "Training network. lr: 0.000100. clip: 0.040105\n",
      "Iteration 19537: Policy loss: -0.254020. Value loss: 0.224193. Entropy: 0.306917.\n",
      "Iteration 19538: Policy loss: -0.254107. Value loss: 0.085414. Entropy: 0.309070.\n",
      "Iteration 19539: Policy loss: -0.257036. Value loss: 0.057860. Entropy: 0.308200.\n",
      "Training network. lr: 0.000100. clip: 0.040105\n",
      "Iteration 19540: Policy loss: 0.401250. Value loss: 0.168411. Entropy: 0.304441.\n",
      "Iteration 19541: Policy loss: 0.404582. Value loss: 0.053267. Entropy: 0.304129.\n",
      "Iteration 19542: Policy loss: 0.397516. Value loss: 0.034327. Entropy: 0.304126.\n",
      "episode: 6794   score: 330.0  epsilon: 1.0    steps: 208  evaluation reward: 467.55\n",
      "Training network. lr: 0.000100. clip: 0.040105\n",
      "Iteration 19543: Policy loss: -0.016224. Value loss: 0.073600. Entropy: 0.311721.\n",
      "Iteration 19544: Policy loss: -0.014806. Value loss: 0.032921. Entropy: 0.312653.\n",
      "Iteration 19545: Policy loss: -0.014232. Value loss: 0.022368. Entropy: 0.311705.\n",
      "Training network. lr: 0.000100. clip: 0.040105\n",
      "Iteration 19546: Policy loss: 0.241291. Value loss: 0.114419. Entropy: 0.311220.\n",
      "Iteration 19547: Policy loss: 0.241472. Value loss: 0.048955. Entropy: 0.311155.\n",
      "Iteration 19548: Policy loss: 0.237975. Value loss: 0.035174. Entropy: 0.310977.\n",
      "episode: 6795   score: 690.0  epsilon: 1.0    steps: 248  evaluation reward: 468.9\n",
      "episode: 6796   score: 575.0  epsilon: 1.0    steps: 488  evaluation reward: 470.7\n",
      "Training network. lr: 0.000100. clip: 0.040105\n",
      "Iteration 19549: Policy loss: 0.021969. Value loss: 0.089875. Entropy: 0.307350.\n",
      "Iteration 19550: Policy loss: 0.016107. Value loss: 0.047817. Entropy: 0.306998.\n",
      "Iteration 19551: Policy loss: 0.010133. Value loss: 0.034035. Entropy: 0.306897.\n",
      "episode: 6797   score: 675.0  epsilon: 1.0    steps: 88  evaluation reward: 473.85\n",
      "Training network. lr: 0.000100. clip: 0.039949\n",
      "Iteration 19552: Policy loss: -0.242029. Value loss: 0.324965. Entropy: 0.312160.\n",
      "Iteration 19553: Policy loss: -0.239404. Value loss: 0.180227. Entropy: 0.311342.\n",
      "Iteration 19554: Policy loss: -0.236717. Value loss: 0.123907. Entropy: 0.311381.\n",
      "episode: 6798   score: 495.0  epsilon: 1.0    steps: 552  evaluation reward: 474.35\n",
      "Training network. lr: 0.000100. clip: 0.039949\n",
      "Iteration 19555: Policy loss: 0.222160. Value loss: 0.095603. Entropy: 0.317437.\n",
      "Iteration 19556: Policy loss: 0.224983. Value loss: 0.038187. Entropy: 0.317516.\n",
      "Iteration 19557: Policy loss: 0.214592. Value loss: 0.028220. Entropy: 0.316811.\n",
      "episode: 6799   score: 330.0  epsilon: 1.0    steps: 544  evaluation reward: 475.35\n",
      "Training network. lr: 0.000100. clip: 0.039949\n",
      "Iteration 19558: Policy loss: 0.202793. Value loss: 0.126528. Entropy: 0.311985.\n",
      "Iteration 19559: Policy loss: 0.196085. Value loss: 0.055432. Entropy: 0.312530.\n",
      "Iteration 19560: Policy loss: 0.200757. Value loss: 0.042060. Entropy: 0.313761.\n",
      "Training network. lr: 0.000100. clip: 0.039949\n",
      "Iteration 19561: Policy loss: 0.240749. Value loss: 0.109551. Entropy: 0.311880.\n",
      "Iteration 19562: Policy loss: 0.235502. Value loss: 0.044773. Entropy: 0.311394.\n",
      "Iteration 19563: Policy loss: 0.235021. Value loss: 0.030518. Entropy: 0.312402.\n",
      "episode: 6800   score: 300.0  epsilon: 1.0    steps: 96  evaluation reward: 474.55\n",
      "now time :  2019-09-06 10:26:59.497227\n",
      "episode: 6801   score: 135.0  epsilon: 1.0    steps: 464  evaluation reward: 472.2\n",
      "episode: 6802   score: 475.0  epsilon: 1.0    steps: 968  evaluation reward: 471.35\n",
      "Training network. lr: 0.000100. clip: 0.039949\n",
      "Iteration 19564: Policy loss: -0.033290. Value loss: 0.156734. Entropy: 0.305263.\n",
      "Iteration 19565: Policy loss: -0.031858. Value loss: 0.055350. Entropy: 0.304946.\n",
      "Iteration 19566: Policy loss: -0.037909. Value loss: 0.041920. Entropy: 0.305188.\n",
      "Training network. lr: 0.000100. clip: 0.039949\n",
      "Iteration 19567: Policy loss: 0.024488. Value loss: 0.128890. Entropy: 0.315353.\n",
      "Iteration 19568: Policy loss: 0.021178. Value loss: 0.048825. Entropy: 0.314476.\n",
      "Iteration 19569: Policy loss: 0.012845. Value loss: 0.032131. Entropy: 0.315357.\n",
      "episode: 6803   score: 655.0  epsilon: 1.0    steps: 360  evaluation reward: 474.8\n",
      "Training network. lr: 0.000100. clip: 0.039949\n",
      "Iteration 19570: Policy loss: -0.075379. Value loss: 0.097081. Entropy: 0.307880.\n",
      "Iteration 19571: Policy loss: -0.080798. Value loss: 0.041622. Entropy: 0.309063.\n",
      "Iteration 19572: Policy loss: -0.086222. Value loss: 0.032466. Entropy: 0.308348.\n",
      "episode: 6804   score: 390.0  epsilon: 1.0    steps: 568  evaluation reward: 475.1\n",
      "episode: 6805   score: 235.0  epsilon: 1.0    steps: 944  evaluation reward: 474.05\n",
      "Training network. lr: 0.000100. clip: 0.039949\n",
      "Iteration 19573: Policy loss: -0.531500. Value loss: 0.405831. Entropy: 0.300117.\n",
      "Iteration 19574: Policy loss: -0.551627. Value loss: 0.225663. Entropy: 0.298527.\n",
      "Iteration 19575: Policy loss: -0.565332. Value loss: 0.096045. Entropy: 0.300855.\n",
      "Training network. lr: 0.000100. clip: 0.039949\n",
      "Iteration 19576: Policy loss: 0.013523. Value loss: 0.104653. Entropy: 0.310289.\n",
      "Iteration 19577: Policy loss: 0.013669. Value loss: 0.045923. Entropy: 0.309919.\n",
      "Iteration 19578: Policy loss: 0.005763. Value loss: 0.032072. Entropy: 0.309206.\n",
      "episode: 6806   score: 455.0  epsilon: 1.0    steps: 928  evaluation reward: 474.95\n",
      "Training network. lr: 0.000100. clip: 0.039949\n",
      "Iteration 19579: Policy loss: -0.195945. Value loss: 0.354719. Entropy: 0.308045.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19580: Policy loss: -0.225067. Value loss: 0.174684. Entropy: 0.307716.\n",
      "Iteration 19581: Policy loss: -0.219634. Value loss: 0.073012. Entropy: 0.307353.\n",
      "episode: 6807   score: 390.0  epsilon: 1.0    steps: 376  evaluation reward: 475.5\n",
      "Training network. lr: 0.000100. clip: 0.039949\n",
      "Iteration 19582: Policy loss: 0.175025. Value loss: 0.126517. Entropy: 0.297868.\n",
      "Iteration 19583: Policy loss: 0.171253. Value loss: 0.062695. Entropy: 0.297079.\n",
      "Iteration 19584: Policy loss: 0.171534. Value loss: 0.040666. Entropy: 0.297206.\n",
      "episode: 6808   score: 325.0  epsilon: 1.0    steps: 656  evaluation reward: 474.0\n",
      "Training network. lr: 0.000100. clip: 0.039949\n",
      "Iteration 19585: Policy loss: 0.324713. Value loss: 0.190419. Entropy: 0.310115.\n",
      "Iteration 19586: Policy loss: 0.312556. Value loss: 0.101798. Entropy: 0.310911.\n",
      "Iteration 19587: Policy loss: 0.308192. Value loss: 0.067676. Entropy: 0.310508.\n",
      "episode: 6809   score: 410.0  epsilon: 1.0    steps: 80  evaluation reward: 475.05\n",
      "Training network. lr: 0.000100. clip: 0.039949\n",
      "Iteration 19588: Policy loss: 0.096607. Value loss: 0.106209. Entropy: 0.308759.\n",
      "Iteration 19589: Policy loss: 0.088801. Value loss: 0.045227. Entropy: 0.308871.\n",
      "Iteration 19590: Policy loss: 0.083971. Value loss: 0.035142. Entropy: 0.309652.\n",
      "episode: 6810   score: 240.0  epsilon: 1.0    steps: 448  evaluation reward: 473.45\n",
      "Training network. lr: 0.000100. clip: 0.039949\n",
      "Iteration 19591: Policy loss: 0.100229. Value loss: 0.147387. Entropy: 0.313711.\n",
      "Iteration 19592: Policy loss: 0.098319. Value loss: 0.064387. Entropy: 0.313530.\n",
      "Iteration 19593: Policy loss: 0.087478. Value loss: 0.044801. Entropy: 0.313161.\n",
      "episode: 6811   score: 150.0  epsilon: 1.0    steps: 88  evaluation reward: 470.75\n",
      "Training network. lr: 0.000100. clip: 0.039949\n",
      "Iteration 19594: Policy loss: 0.139633. Value loss: 0.309472. Entropy: 0.312725.\n",
      "Iteration 19595: Policy loss: 0.121145. Value loss: 0.076211. Entropy: 0.313596.\n",
      "Iteration 19596: Policy loss: 0.114983. Value loss: 0.045470. Entropy: 0.312199.\n",
      "episode: 6812   score: 360.0  epsilon: 1.0    steps: 48  evaluation reward: 467.8\n",
      "episode: 6813   score: 70.0  epsilon: 1.0    steps: 464  evaluation reward: 466.1\n",
      "episode: 6814   score: 650.0  epsilon: 1.0    steps: 696  evaluation reward: 466.35\n",
      "Training network. lr: 0.000100. clip: 0.039949\n",
      "Iteration 19597: Policy loss: 0.136098. Value loss: 0.155560. Entropy: 0.302754.\n",
      "Iteration 19598: Policy loss: 0.133420. Value loss: 0.066783. Entropy: 0.300276.\n",
      "Iteration 19599: Policy loss: 0.132530. Value loss: 0.047092. Entropy: 0.300171.\n",
      "Training network. lr: 0.000100. clip: 0.039949\n",
      "Iteration 19600: Policy loss: -0.102757. Value loss: 0.140314. Entropy: 0.313039.\n",
      "Iteration 19601: Policy loss: -0.108822. Value loss: 0.070831. Entropy: 0.313710.\n",
      "Iteration 19602: Policy loss: -0.111749. Value loss: 0.043945. Entropy: 0.314112.\n",
      "episode: 6815   score: 315.0  epsilon: 1.0    steps: 40  evaluation reward: 462.2\n",
      "Training network. lr: 0.000099. clip: 0.039792\n",
      "Iteration 19603: Policy loss: 0.122784. Value loss: 0.119269. Entropy: 0.313362.\n",
      "Iteration 19604: Policy loss: 0.111333. Value loss: 0.038411. Entropy: 0.313820.\n",
      "Iteration 19605: Policy loss: 0.112049. Value loss: 0.025021. Entropy: 0.313308.\n",
      "episode: 6816   score: 315.0  epsilon: 1.0    steps: 504  evaluation reward: 460.35\n",
      "episode: 6817   score: 1000.0  epsilon: 1.0    steps: 712  evaluation reward: 464.95\n",
      "Training network. lr: 0.000099. clip: 0.039792\n",
      "Iteration 19606: Policy loss: -0.060270. Value loss: 0.236661. Entropy: 0.309472.\n",
      "Iteration 19607: Policy loss: -0.068889. Value loss: 0.128689. Entropy: 0.309621.\n",
      "Iteration 19608: Policy loss: -0.064960. Value loss: 0.090284. Entropy: 0.310094.\n",
      "Training network. lr: 0.000099. clip: 0.039792\n",
      "Iteration 19609: Policy loss: -0.079903. Value loss: 0.257388. Entropy: 0.314272.\n",
      "Iteration 19610: Policy loss: -0.092613. Value loss: 0.076973. Entropy: 0.313590.\n",
      "Iteration 19611: Policy loss: -0.090705. Value loss: 0.055110. Entropy: 0.313642.\n",
      "episode: 6818   score: 385.0  epsilon: 1.0    steps: 288  evaluation reward: 464.85\n",
      "episode: 6819   score: 310.0  epsilon: 1.0    steps: 976  evaluation reward: 464.0\n",
      "Training network. lr: 0.000099. clip: 0.039792\n",
      "Iteration 19612: Policy loss: 0.059886. Value loss: 0.177118. Entropy: 0.313469.\n",
      "Iteration 19613: Policy loss: 0.042930. Value loss: 0.070200. Entropy: 0.313464.\n",
      "Iteration 19614: Policy loss: 0.048889. Value loss: 0.043758. Entropy: 0.313216.\n",
      "episode: 6820   score: 350.0  epsilon: 1.0    steps: 1024  evaluation reward: 462.75\n",
      "Training network. lr: 0.000099. clip: 0.039792\n",
      "Iteration 19615: Policy loss: 0.343365. Value loss: 0.258179. Entropy: 0.310929.\n",
      "Iteration 19616: Policy loss: 0.338351. Value loss: 0.081989. Entropy: 0.310025.\n",
      "Iteration 19617: Policy loss: 0.333798. Value loss: 0.043646. Entropy: 0.310001.\n",
      "Training network. lr: 0.000099. clip: 0.039792\n",
      "Iteration 19618: Policy loss: 0.026645. Value loss: 0.164376. Entropy: 0.314920.\n",
      "Iteration 19619: Policy loss: 0.021227. Value loss: 0.105210. Entropy: 0.314735.\n",
      "Iteration 19620: Policy loss: 0.011156. Value loss: 0.081972. Entropy: 0.313884.\n",
      "episode: 6821   score: 420.0  epsilon: 1.0    steps: 704  evaluation reward: 460.3\n",
      "episode: 6822   score: 620.0  epsilon: 1.0    steps: 920  evaluation reward: 463.35\n",
      "Training network. lr: 0.000099. clip: 0.039792\n",
      "Iteration 19621: Policy loss: -0.126423. Value loss: 0.205622. Entropy: 0.307164.\n",
      "Iteration 19622: Policy loss: -0.134813. Value loss: 0.103737. Entropy: 0.307387.\n",
      "Iteration 19623: Policy loss: -0.135862. Value loss: 0.072471. Entropy: 0.307414.\n",
      "episode: 6823   score: 210.0  epsilon: 1.0    steps: 352  evaluation reward: 461.5\n",
      "Training network. lr: 0.000099. clip: 0.039792\n",
      "Iteration 19624: Policy loss: 0.040656. Value loss: 0.161460. Entropy: 0.309242.\n",
      "Iteration 19625: Policy loss: 0.034746. Value loss: 0.065301. Entropy: 0.309577.\n",
      "Iteration 19626: Policy loss: 0.040245. Value loss: 0.046833. Entropy: 0.308872.\n",
      "Training network. lr: 0.000099. clip: 0.039792\n",
      "Iteration 19627: Policy loss: -0.145049. Value loss: 0.105773. Entropy: 0.313186.\n",
      "Iteration 19628: Policy loss: -0.150846. Value loss: 0.037595. Entropy: 0.314289.\n",
      "Iteration 19629: Policy loss: -0.150975. Value loss: 0.027740. Entropy: 0.314631.\n",
      "episode: 6824   score: 395.0  epsilon: 1.0    steps: 528  evaluation reward: 459.55\n",
      "episode: 6825   score: 230.0  epsilon: 1.0    steps: 1024  evaluation reward: 454.7\n",
      "Training network. lr: 0.000099. clip: 0.039792\n",
      "Iteration 19630: Policy loss: 0.321280. Value loss: 0.211096. Entropy: 0.305171.\n",
      "Iteration 19631: Policy loss: 0.315371. Value loss: 0.089795. Entropy: 0.303780.\n",
      "Iteration 19632: Policy loss: 0.327121. Value loss: 0.058732. Entropy: 0.303522.\n",
      "Training network. lr: 0.000099. clip: 0.039792\n",
      "Iteration 19633: Policy loss: 0.176257. Value loss: 0.133917. Entropy: 0.308135.\n",
      "Iteration 19634: Policy loss: 0.175675. Value loss: 0.048238. Entropy: 0.308081.\n",
      "Iteration 19635: Policy loss: 0.165462. Value loss: 0.033303. Entropy: 0.306673.\n",
      "episode: 6826   score: 290.0  epsilon: 1.0    steps: 208  evaluation reward: 451.0\n",
      "Training network. lr: 0.000099. clip: 0.039792\n",
      "Iteration 19636: Policy loss: 0.098807. Value loss: 0.112353. Entropy: 0.302405.\n",
      "Iteration 19637: Policy loss: 0.105182. Value loss: 0.072783. Entropy: 0.302411.\n",
      "Iteration 19638: Policy loss: 0.098483. Value loss: 0.057902. Entropy: 0.302281.\n",
      "Training network. lr: 0.000099. clip: 0.039792\n",
      "Iteration 19639: Policy loss: -0.071265. Value loss: 0.203672. Entropy: 0.312899.\n",
      "Iteration 19640: Policy loss: -0.077872. Value loss: 0.075279. Entropy: 0.313363.\n",
      "Iteration 19641: Policy loss: -0.074351. Value loss: 0.043801. Entropy: 0.312354.\n",
      "episode: 6827   score: 400.0  epsilon: 1.0    steps: 80  evaluation reward: 452.0\n",
      "episode: 6828   score: 290.0  epsilon: 1.0    steps: 648  evaluation reward: 448.2\n",
      "episode: 6829   score: 720.0  epsilon: 1.0    steps: 680  evaluation reward: 452.25\n",
      "episode: 6830   score: 325.0  epsilon: 1.0    steps: 888  evaluation reward: 446.15\n",
      "Training network. lr: 0.000099. clip: 0.039792\n",
      "Iteration 19642: Policy loss: 0.097711. Value loss: 0.102670. Entropy: 0.289901.\n",
      "Iteration 19643: Policy loss: 0.097838. Value loss: 0.041299. Entropy: 0.290659.\n",
      "Iteration 19644: Policy loss: 0.086042. Value loss: 0.031497. Entropy: 0.290331.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000099. clip: 0.039792\n",
      "Iteration 19645: Policy loss: -0.379851. Value loss: 0.328217. Entropy: 0.309941.\n",
      "Iteration 19646: Policy loss: -0.390311. Value loss: 0.209397. Entropy: 0.310050.\n",
      "Iteration 19647: Policy loss: -0.374377. Value loss: 0.139164. Entropy: 0.309360.\n",
      "episode: 6831   score: 390.0  epsilon: 1.0    steps: 440  evaluation reward: 445.75\n",
      "Training network. lr: 0.000099. clip: 0.039792\n",
      "Iteration 19648: Policy loss: -0.310352. Value loss: 0.280199. Entropy: 0.304267.\n",
      "Iteration 19649: Policy loss: -0.301247. Value loss: 0.125271. Entropy: 0.302954.\n",
      "Iteration 19650: Policy loss: -0.309858. Value loss: 0.079011. Entropy: 0.301937.\n",
      "episode: 6832   score: 460.0  epsilon: 1.0    steps: 488  evaluation reward: 446.05\n",
      "Training network. lr: 0.000099. clip: 0.039644\n",
      "Iteration 19651: Policy loss: 0.153863. Value loss: 0.096692. Entropy: 0.299081.\n",
      "Iteration 19652: Policy loss: 0.144048. Value loss: 0.034335. Entropy: 0.298795.\n",
      "Iteration 19653: Policy loss: 0.137558. Value loss: 0.021348. Entropy: 0.296580.\n",
      "Training network. lr: 0.000099. clip: 0.039644\n",
      "Iteration 19654: Policy loss: -0.455316. Value loss: 0.523759. Entropy: 0.322834.\n",
      "Iteration 19655: Policy loss: -0.484936. Value loss: 0.278192. Entropy: 0.323641.\n",
      "Iteration 19656: Policy loss: -0.481425. Value loss: 0.173017. Entropy: 0.322885.\n",
      "Training network. lr: 0.000099. clip: 0.039644\n",
      "Iteration 19657: Policy loss: -0.118038. Value loss: 0.196654. Entropy: 0.314467.\n",
      "Iteration 19658: Policy loss: -0.116486. Value loss: 0.082812. Entropy: 0.313583.\n",
      "Iteration 19659: Policy loss: -0.126456. Value loss: 0.057469. Entropy: 0.313588.\n",
      "Training network. lr: 0.000099. clip: 0.039644\n",
      "Iteration 19660: Policy loss: 0.183157. Value loss: 0.191818. Entropy: 0.315203.\n",
      "Iteration 19661: Policy loss: 0.172980. Value loss: 0.066196. Entropy: 0.314884.\n",
      "Iteration 19662: Policy loss: 0.157762. Value loss: 0.044022. Entropy: 0.315659.\n",
      "episode: 6833   score: 535.0  epsilon: 1.0    steps: 320  evaluation reward: 447.45\n",
      "episode: 6834   score: 535.0  epsilon: 1.0    steps: 1000  evaluation reward: 447.35\n",
      "Training network. lr: 0.000099. clip: 0.039644\n",
      "Iteration 19663: Policy loss: -0.035648. Value loss: 0.144321. Entropy: 0.294250.\n",
      "Iteration 19664: Policy loss: -0.034073. Value loss: 0.053467. Entropy: 0.295027.\n",
      "Iteration 19665: Policy loss: -0.041492. Value loss: 0.036891. Entropy: 0.293769.\n",
      "episode: 6835   score: 345.0  epsilon: 1.0    steps: 328  evaluation reward: 445.9\n",
      "episode: 6836   score: 425.0  epsilon: 1.0    steps: 496  evaluation reward: 446.25\n",
      "Training network. lr: 0.000099. clip: 0.039644\n",
      "Iteration 19666: Policy loss: 0.509418. Value loss: 0.152307. Entropy: 0.279032.\n",
      "Iteration 19667: Policy loss: 0.496134. Value loss: 0.070003. Entropy: 0.276282.\n",
      "Iteration 19668: Policy loss: 0.495665. Value loss: 0.057162. Entropy: 0.275596.\n",
      "episode: 6837   score: 335.0  epsilon: 1.0    steps: 848  evaluation reward: 441.65\n",
      "episode: 6838   score: 30.0  epsilon: 1.0    steps: 864  evaluation reward: 437.1\n",
      "Training network. lr: 0.000099. clip: 0.039644\n",
      "Iteration 19669: Policy loss: 0.075556. Value loss: 0.159287. Entropy: 0.306755.\n",
      "Iteration 19670: Policy loss: 0.069474. Value loss: 0.078327. Entropy: 0.304668.\n",
      "Iteration 19671: Policy loss: 0.065172. Value loss: 0.049999. Entropy: 0.304857.\n",
      "episode: 6839   score: 295.0  epsilon: 1.0    steps: 464  evaluation reward: 433.2\n",
      "Training network. lr: 0.000099. clip: 0.039644\n",
      "Iteration 19672: Policy loss: 0.035093. Value loss: 0.146707. Entropy: 0.305931.\n",
      "Iteration 19673: Policy loss: 0.032048. Value loss: 0.061311. Entropy: 0.306243.\n",
      "Iteration 19674: Policy loss: 0.025470. Value loss: 0.040887. Entropy: 0.306644.\n",
      "episode: 6840   score: 760.0  epsilon: 1.0    steps: 496  evaluation reward: 437.2\n",
      "Training network. lr: 0.000099. clip: 0.039644\n",
      "Iteration 19675: Policy loss: 0.101501. Value loss: 0.095603. Entropy: 0.313216.\n",
      "Iteration 19676: Policy loss: 0.105512. Value loss: 0.059229. Entropy: 0.312437.\n",
      "Iteration 19677: Policy loss: 0.106514. Value loss: 0.044826. Entropy: 0.312595.\n",
      "Training network. lr: 0.000099. clip: 0.039644\n",
      "Iteration 19678: Policy loss: 0.094157. Value loss: 0.176888. Entropy: 0.313493.\n",
      "Iteration 19679: Policy loss: 0.089697. Value loss: 0.079997. Entropy: 0.313898.\n",
      "Iteration 19680: Policy loss: 0.083270. Value loss: 0.057439. Entropy: 0.313099.\n",
      "episode: 6841   score: 210.0  epsilon: 1.0    steps: 536  evaluation reward: 433.4\n",
      "Training network. lr: 0.000099. clip: 0.039644\n",
      "Iteration 19681: Policy loss: 0.086663. Value loss: 0.165166. Entropy: 0.308492.\n",
      "Iteration 19682: Policy loss: 0.075583. Value loss: 0.053897. Entropy: 0.308263.\n",
      "Iteration 19683: Policy loss: 0.070149. Value loss: 0.037904. Entropy: 0.308249.\n",
      "episode: 6842   score: 775.0  epsilon: 1.0    steps: 240  evaluation reward: 433.3\n",
      "Training network. lr: 0.000099. clip: 0.039644\n",
      "Iteration 19684: Policy loss: 0.087005. Value loss: 0.178396. Entropy: 0.307112.\n",
      "Iteration 19685: Policy loss: 0.090009. Value loss: 0.066994. Entropy: 0.306256.\n",
      "Iteration 19686: Policy loss: 0.080495. Value loss: 0.043075. Entropy: 0.305375.\n",
      "episode: 6843   score: 325.0  epsilon: 1.0    steps: 600  evaluation reward: 431.6\n",
      "episode: 6844   score: 240.0  epsilon: 1.0    steps: 808  evaluation reward: 428.65\n",
      "episode: 6845   score: 245.0  epsilon: 1.0    steps: 976  evaluation reward: 424.3\n",
      "Training network. lr: 0.000099. clip: 0.039644\n",
      "Iteration 19687: Policy loss: 0.287678. Value loss: 0.192879. Entropy: 0.304064.\n",
      "Iteration 19688: Policy loss: 0.280260. Value loss: 0.088905. Entropy: 0.303501.\n",
      "Iteration 19689: Policy loss: 0.276406. Value loss: 0.054521. Entropy: 0.302820.\n",
      "Training network. lr: 0.000099. clip: 0.039644\n",
      "Iteration 19690: Policy loss: -0.121129. Value loss: 0.129848. Entropy: 0.313720.\n",
      "Iteration 19691: Policy loss: -0.124449. Value loss: 0.065541. Entropy: 0.313393.\n",
      "Iteration 19692: Policy loss: -0.118480. Value loss: 0.045815. Entropy: 0.313314.\n",
      "Training network. lr: 0.000099. clip: 0.039644\n",
      "Iteration 19693: Policy loss: 0.168504. Value loss: 0.103397. Entropy: 0.315060.\n",
      "Iteration 19694: Policy loss: 0.170887. Value loss: 0.052587. Entropy: 0.314965.\n",
      "Iteration 19695: Policy loss: 0.171832. Value loss: 0.041229. Entropy: 0.314588.\n",
      "episode: 6846   score: 210.0  epsilon: 1.0    steps: 1024  evaluation reward: 420.9\n",
      "episode: 6847   score: 565.0  epsilon: 1.0    steps: 1024  evaluation reward: 417.6\n",
      "Training network. lr: 0.000099. clip: 0.039644\n",
      "Iteration 19696: Policy loss: 0.179592. Value loss: 0.081005. Entropy: 0.311235.\n",
      "Iteration 19697: Policy loss: 0.178896. Value loss: 0.040335. Entropy: 0.311175.\n",
      "Iteration 19698: Policy loss: 0.173276. Value loss: 0.025602. Entropy: 0.311157.\n",
      "episode: 6848   score: 420.0  epsilon: 1.0    steps: 960  evaluation reward: 415.3\n",
      "Training network. lr: 0.000099. clip: 0.039644\n",
      "Iteration 19699: Policy loss: 0.029201. Value loss: 0.137276. Entropy: 0.310390.\n",
      "Iteration 19700: Policy loss: 0.030203. Value loss: 0.050493. Entropy: 0.310649.\n",
      "Iteration 19701: Policy loss: 0.025080. Value loss: 0.031246. Entropy: 0.311042.\n",
      "episode: 6849   score: 440.0  epsilon: 1.0    steps: 680  evaluation reward: 418.55\n",
      "Training network. lr: 0.000099. clip: 0.039488\n",
      "Iteration 19702: Policy loss: 0.004670. Value loss: 0.148089. Entropy: 0.309682.\n",
      "Iteration 19703: Policy loss: 0.001246. Value loss: 0.073751. Entropy: 0.309572.\n",
      "Iteration 19704: Policy loss: -0.002279. Value loss: 0.059539. Entropy: 0.308594.\n",
      "Training network. lr: 0.000099. clip: 0.039488\n",
      "Iteration 19705: Policy loss: -0.321969. Value loss: 0.323103. Entropy: 0.314159.\n",
      "Iteration 19706: Policy loss: -0.338364. Value loss: 0.172298. Entropy: 0.315154.\n",
      "Iteration 19707: Policy loss: -0.359228. Value loss: 0.122522. Entropy: 0.314651.\n",
      "Training network. lr: 0.000099. clip: 0.039488\n",
      "Iteration 19708: Policy loss: -0.232298. Value loss: 0.112632. Entropy: 0.311563.\n",
      "Iteration 19709: Policy loss: -0.242347. Value loss: 0.056164. Entropy: 0.311821.\n",
      "Iteration 19710: Policy loss: -0.241239. Value loss: 0.039739. Entropy: 0.312615.\n",
      "episode: 6850   score: 515.0  epsilon: 1.0    steps: 384  evaluation reward: 419.85\n",
      "now time :  2019-09-06 10:35:55.703894\n",
      "episode: 6851   score: 420.0  epsilon: 1.0    steps: 768  evaluation reward: 419.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6852   score: 285.0  epsilon: 1.0    steps: 976  evaluation reward: 417.8\n",
      "Training network. lr: 0.000099. clip: 0.039488\n",
      "Iteration 19711: Policy loss: 0.301259. Value loss: 0.172884. Entropy: 0.305662.\n",
      "Iteration 19712: Policy loss: 0.302015. Value loss: 0.058349. Entropy: 0.305050.\n",
      "Iteration 19713: Policy loss: 0.307211. Value loss: 0.033308. Entropy: 0.303799.\n",
      "episode: 6853   score: 210.0  epsilon: 1.0    steps: 784  evaluation reward: 416.25\n",
      "Training network. lr: 0.000099. clip: 0.039488\n",
      "Iteration 19714: Policy loss: 0.021793. Value loss: 0.165220. Entropy: 0.306251.\n",
      "Iteration 19715: Policy loss: 0.013938. Value loss: 0.078693. Entropy: 0.305315.\n",
      "Iteration 19716: Policy loss: 0.006822. Value loss: 0.056309. Entropy: 0.305144.\n",
      "Training network. lr: 0.000099. clip: 0.039488\n",
      "Iteration 19717: Policy loss: 0.164736. Value loss: 0.125468. Entropy: 0.315553.\n",
      "Iteration 19718: Policy loss: 0.158836. Value loss: 0.052174. Entropy: 0.316151.\n",
      "Iteration 19719: Policy loss: 0.146632. Value loss: 0.038222. Entropy: 0.315698.\n",
      "episode: 6854   score: 450.0  epsilon: 1.0    steps: 256  evaluation reward: 416.55\n",
      "episode: 6855   score: 645.0  epsilon: 1.0    steps: 976  evaluation reward: 417.65\n",
      "Training network. lr: 0.000099. clip: 0.039488\n",
      "Iteration 19720: Policy loss: 0.034611. Value loss: 0.130253. Entropy: 0.305013.\n",
      "Iteration 19721: Policy loss: 0.027216. Value loss: 0.064766. Entropy: 0.305183.\n",
      "Iteration 19722: Policy loss: 0.030204. Value loss: 0.050110. Entropy: 0.304884.\n",
      "episode: 6856   score: 445.0  epsilon: 1.0    steps: 888  evaluation reward: 417.4\n",
      "Training network. lr: 0.000099. clip: 0.039488\n",
      "Iteration 19723: Policy loss: -0.590040. Value loss: 0.448880. Entropy: 0.310733.\n",
      "Iteration 19724: Policy loss: -0.610341. Value loss: 0.216899. Entropy: 0.310633.\n",
      "Iteration 19725: Policy loss: -0.607474. Value loss: 0.158695. Entropy: 0.310692.\n",
      "episode: 6857   score: 430.0  epsilon: 1.0    steps: 784  evaluation reward: 418.95\n",
      "Training network. lr: 0.000099. clip: 0.039488\n",
      "Iteration 19726: Policy loss: -0.017191. Value loss: 0.067645. Entropy: 0.312522.\n",
      "Iteration 19727: Policy loss: -0.019449. Value loss: 0.030720. Entropy: 0.312136.\n",
      "Iteration 19728: Policy loss: -0.020824. Value loss: 0.022158. Entropy: 0.313508.\n",
      "Training network. lr: 0.000099. clip: 0.039488\n",
      "Iteration 19729: Policy loss: -0.062435. Value loss: 0.095867. Entropy: 0.312171.\n",
      "Iteration 19730: Policy loss: -0.065469. Value loss: 0.044532. Entropy: 0.313304.\n",
      "Iteration 19731: Policy loss: -0.069276. Value loss: 0.029883. Entropy: 0.312775.\n",
      "Training network. lr: 0.000099. clip: 0.039488\n",
      "Iteration 19732: Policy loss: 0.014547. Value loss: 0.462349. Entropy: 0.313380.\n",
      "Iteration 19733: Policy loss: 0.025352. Value loss: 0.215934. Entropy: 0.312481.\n",
      "Iteration 19734: Policy loss: 0.021209. Value loss: 0.128095. Entropy: 0.312873.\n",
      "Training network. lr: 0.000099. clip: 0.039488\n",
      "Iteration 19735: Policy loss: 0.115464. Value loss: 0.083318. Entropy: 0.311376.\n",
      "Iteration 19736: Policy loss: 0.119340. Value loss: 0.032876. Entropy: 0.312473.\n",
      "Iteration 19737: Policy loss: 0.111738. Value loss: 0.024890. Entropy: 0.311971.\n",
      "episode: 6858   score: 435.0  epsilon: 1.0    steps: 32  evaluation reward: 419.05\n",
      "episode: 6859   score: 295.0  epsilon: 1.0    steps: 728  evaluation reward: 419.4\n",
      "Training network. lr: 0.000099. clip: 0.039488\n",
      "Iteration 19738: Policy loss: 0.207280. Value loss: 0.103363. Entropy: 0.310063.\n",
      "Iteration 19739: Policy loss: 0.194199. Value loss: 0.046851. Entropy: 0.308531.\n",
      "Iteration 19740: Policy loss: 0.200997. Value loss: 0.034803. Entropy: 0.308376.\n",
      "Training network. lr: 0.000099. clip: 0.039488\n",
      "Iteration 19741: Policy loss: 0.039371. Value loss: 0.117528. Entropy: 0.312232.\n",
      "Iteration 19742: Policy loss: 0.036502. Value loss: 0.056784. Entropy: 0.312280.\n",
      "Iteration 19743: Policy loss: 0.037537. Value loss: 0.035884. Entropy: 0.311779.\n",
      "episode: 6860   score: 650.0  epsilon: 1.0    steps: 264  evaluation reward: 422.25\n",
      "episode: 6861   score: 635.0  epsilon: 1.0    steps: 392  evaluation reward: 422.5\n",
      "Training network. lr: 0.000099. clip: 0.039488\n",
      "Iteration 19744: Policy loss: -0.036638. Value loss: 0.083018. Entropy: 0.303468.\n",
      "Iteration 19745: Policy loss: -0.034978. Value loss: 0.035864. Entropy: 0.303869.\n",
      "Iteration 19746: Policy loss: -0.036597. Value loss: 0.026692. Entropy: 0.303971.\n",
      "episode: 6862   score: 720.0  epsilon: 1.0    steps: 592  evaluation reward: 426.55\n",
      "Training network. lr: 0.000099. clip: 0.039488\n",
      "Iteration 19747: Policy loss: 0.211786. Value loss: 0.135721. Entropy: 0.313003.\n",
      "Iteration 19748: Policy loss: 0.207523. Value loss: 0.066100. Entropy: 0.312012.\n",
      "Iteration 19749: Policy loss: 0.194324. Value loss: 0.046603. Entropy: 0.311506.\n",
      "episode: 6863   score: 335.0  epsilon: 1.0    steps: 80  evaluation reward: 426.75\n",
      "episode: 6864   score: 270.0  epsilon: 1.0    steps: 216  evaluation reward: 425.0\n",
      "Training network. lr: 0.000099. clip: 0.039488\n",
      "Iteration 19750: Policy loss: -0.107755. Value loss: 0.077789. Entropy: 0.307508.\n",
      "Iteration 19751: Policy loss: -0.112224. Value loss: 0.043070. Entropy: 0.306862.\n",
      "Iteration 19752: Policy loss: -0.111871. Value loss: 0.034613. Entropy: 0.307694.\n",
      "episode: 6865   score: 420.0  epsilon: 1.0    steps: 32  evaluation reward: 425.55\n",
      "Training network. lr: 0.000098. clip: 0.039331\n",
      "Iteration 19753: Policy loss: 0.168365. Value loss: 0.116906. Entropy: 0.309561.\n",
      "Iteration 19754: Policy loss: 0.163782. Value loss: 0.045316. Entropy: 0.308245.\n",
      "Iteration 19755: Policy loss: 0.162200. Value loss: 0.032938. Entropy: 0.308985.\n",
      "Training network. lr: 0.000098. clip: 0.039331\n",
      "Iteration 19756: Policy loss: -0.084819. Value loss: 0.092079. Entropy: 0.311096.\n",
      "Iteration 19757: Policy loss: -0.092154. Value loss: 0.037765. Entropy: 0.312212.\n",
      "Iteration 19758: Policy loss: -0.092414. Value loss: 0.029875. Entropy: 0.312325.\n",
      "episode: 6866   score: 290.0  epsilon: 1.0    steps: 64  evaluation reward: 421.9\n",
      "Training network. lr: 0.000098. clip: 0.039331\n",
      "Iteration 19759: Policy loss: 0.296029. Value loss: 0.148721. Entropy: 0.312188.\n",
      "Iteration 19760: Policy loss: 0.304020. Value loss: 0.057377. Entropy: 0.309988.\n",
      "Iteration 19761: Policy loss: 0.284111. Value loss: 0.038917. Entropy: 0.311048.\n",
      "episode: 6867   score: 460.0  epsilon: 1.0    steps: 312  evaluation reward: 419.55\n",
      "Training network. lr: 0.000098. clip: 0.039331\n",
      "Iteration 19762: Policy loss: 0.004162. Value loss: 0.066012. Entropy: 0.303651.\n",
      "Iteration 19763: Policy loss: -0.000157. Value loss: 0.033782. Entropy: 0.304452.\n",
      "Iteration 19764: Policy loss: -0.002431. Value loss: 0.027609. Entropy: 0.304266.\n",
      "episode: 6868   score: 350.0  epsilon: 1.0    steps: 536  evaluation reward: 418.7\n",
      "episode: 6869   score: 380.0  epsilon: 1.0    steps: 960  evaluation reward: 420.7\n",
      "Training network. lr: 0.000098. clip: 0.039331\n",
      "Iteration 19765: Policy loss: 0.047608. Value loss: 0.184819. Entropy: 0.305729.\n",
      "Iteration 19766: Policy loss: 0.037697. Value loss: 0.084672. Entropy: 0.305772.\n",
      "Iteration 19767: Policy loss: 0.028628. Value loss: 0.056535. Entropy: 0.305558.\n",
      "episode: 6870   score: 210.0  epsilon: 1.0    steps: 88  evaluation reward: 417.65\n",
      "Training network. lr: 0.000098. clip: 0.039331\n",
      "Iteration 19768: Policy loss: 0.064813. Value loss: 0.099326. Entropy: 0.304478.\n",
      "Iteration 19769: Policy loss: 0.069600. Value loss: 0.050457. Entropy: 0.305238.\n",
      "Iteration 19770: Policy loss: 0.057136. Value loss: 0.040621. Entropy: 0.304727.\n",
      "episode: 6871   score: 335.0  epsilon: 1.0    steps: 728  evaluation reward: 418.4\n",
      "Training network. lr: 0.000098. clip: 0.039331\n",
      "Iteration 19771: Policy loss: 0.078771. Value loss: 0.073651. Entropy: 0.310820.\n",
      "Iteration 19772: Policy loss: 0.078152. Value loss: 0.041553. Entropy: 0.311129.\n",
      "Iteration 19773: Policy loss: 0.072521. Value loss: 0.033352. Entropy: 0.310970.\n",
      "Training network. lr: 0.000098. clip: 0.039331\n",
      "Iteration 19774: Policy loss: -0.022479. Value loss: 0.067054. Entropy: 0.314255.\n",
      "Iteration 19775: Policy loss: -0.025492. Value loss: 0.031246. Entropy: 0.315044.\n",
      "Iteration 19776: Policy loss: -0.029166. Value loss: 0.021887. Entropy: 0.314079.\n",
      "episode: 6872   score: 390.0  epsilon: 1.0    steps: 496  evaluation reward: 418.3\n",
      "episode: 6873   score: 555.0  epsilon: 1.0    steps: 752  evaluation reward: 417.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000098. clip: 0.039331\n",
      "Iteration 19777: Policy loss: 0.088300. Value loss: 0.268030. Entropy: 0.305361.\n",
      "Iteration 19778: Policy loss: 0.090195. Value loss: 0.152531. Entropy: 0.303943.\n",
      "Iteration 19779: Policy loss: 0.090451. Value loss: 0.111491. Entropy: 0.304836.\n",
      "Training network. lr: 0.000098. clip: 0.039331\n",
      "Iteration 19780: Policy loss: -0.253155. Value loss: 0.305716. Entropy: 0.309202.\n",
      "Iteration 19781: Policy loss: -0.250770. Value loss: 0.113559. Entropy: 0.308811.\n",
      "Iteration 19782: Policy loss: -0.267260. Value loss: 0.055761. Entropy: 0.309909.\n",
      "episode: 6874   score: 335.0  epsilon: 1.0    steps: 616  evaluation reward: 413.7\n",
      "Training network. lr: 0.000098. clip: 0.039331\n",
      "Iteration 19783: Policy loss: -0.078275. Value loss: 0.072441. Entropy: 0.312702.\n",
      "Iteration 19784: Policy loss: -0.084369. Value loss: 0.035078. Entropy: 0.311859.\n",
      "Iteration 19785: Policy loss: -0.084871. Value loss: 0.025764. Entropy: 0.311776.\n",
      "episode: 6875   score: 515.0  epsilon: 1.0    steps: 560  evaluation reward: 415.25\n",
      "Training network. lr: 0.000098. clip: 0.039331\n",
      "Iteration 19786: Policy loss: 0.207085. Value loss: 0.146495. Entropy: 0.307914.\n",
      "Iteration 19787: Policy loss: 0.202475. Value loss: 0.048460. Entropy: 0.307339.\n",
      "Iteration 19788: Policy loss: 0.188057. Value loss: 0.037166. Entropy: 0.307206.\n",
      "episode: 6876   score: 645.0  epsilon: 1.0    steps: 672  evaluation reward: 419.85\n",
      "episode: 6877   score: 400.0  epsilon: 1.0    steps: 696  evaluation reward: 419.0\n",
      "Training network. lr: 0.000098. clip: 0.039331\n",
      "Iteration 19789: Policy loss: 0.033466. Value loss: 0.102616. Entropy: 0.305739.\n",
      "Iteration 19790: Policy loss: 0.025014. Value loss: 0.037758. Entropy: 0.306851.\n",
      "Iteration 19791: Policy loss: 0.022141. Value loss: 0.027435. Entropy: 0.307206.\n",
      "episode: 6878   score: 305.0  epsilon: 1.0    steps: 488  evaluation reward: 416.1\n",
      "Training network. lr: 0.000098. clip: 0.039331\n",
      "Iteration 19792: Policy loss: 0.176038. Value loss: 0.138372. Entropy: 0.306677.\n",
      "Iteration 19793: Policy loss: 0.173449. Value loss: 0.051251. Entropy: 0.306612.\n",
      "Iteration 19794: Policy loss: 0.180302. Value loss: 0.039688. Entropy: 0.306751.\n",
      "Training network. lr: 0.000098. clip: 0.039331\n",
      "Iteration 19795: Policy loss: 0.163666. Value loss: 0.156430. Entropy: 0.312329.\n",
      "Iteration 19796: Policy loss: 0.152587. Value loss: 0.066548. Entropy: 0.311242.\n",
      "Iteration 19797: Policy loss: 0.153765. Value loss: 0.051566. Entropy: 0.311773.\n",
      "episode: 6879   score: 725.0  epsilon: 1.0    steps: 808  evaluation reward: 412.3\n",
      "Training network. lr: 0.000098. clip: 0.039331\n",
      "Iteration 19798: Policy loss: -0.021757. Value loss: 0.103262. Entropy: 0.312670.\n",
      "Iteration 19799: Policy loss: -0.021998. Value loss: 0.048743. Entropy: 0.311865.\n",
      "Iteration 19800: Policy loss: -0.023033. Value loss: 0.038580. Entropy: 0.312276.\n",
      "Training network. lr: 0.000098. clip: 0.039184\n",
      "Iteration 19801: Policy loss: 0.209952. Value loss: 0.074400. Entropy: 0.305401.\n",
      "Iteration 19802: Policy loss: 0.209882. Value loss: 0.029173. Entropy: 0.305608.\n",
      "Iteration 19803: Policy loss: 0.204697. Value loss: 0.022025. Entropy: 0.305493.\n",
      "Training network. lr: 0.000098. clip: 0.039184\n",
      "Iteration 19804: Policy loss: -0.066707. Value loss: 0.061460. Entropy: 0.311887.\n",
      "Iteration 19805: Policy loss: -0.067878. Value loss: 0.022940. Entropy: 0.312046.\n",
      "Iteration 19806: Policy loss: -0.067046. Value loss: 0.016891. Entropy: 0.312172.\n",
      "episode: 6880   score: 295.0  epsilon: 1.0    steps: 432  evaluation reward: 408.55\n",
      "episode: 6881   score: 425.0  epsilon: 1.0    steps: 792  evaluation reward: 407.15\n",
      "episode: 6882   score: 420.0  epsilon: 1.0    steps: 888  evaluation reward: 408.5\n",
      "Training network. lr: 0.000098. clip: 0.039184\n",
      "Iteration 19807: Policy loss: 0.196242. Value loss: 0.102849. Entropy: 0.303068.\n",
      "Iteration 19808: Policy loss: 0.186224. Value loss: 0.044214. Entropy: 0.302866.\n",
      "Iteration 19809: Policy loss: 0.185190. Value loss: 0.030587. Entropy: 0.302657.\n",
      "episode: 6883   score: 500.0  epsilon: 1.0    steps: 488  evaluation reward: 410.1\n",
      "episode: 6884   score: 495.0  epsilon: 1.0    steps: 776  evaluation reward: 410.55\n",
      "episode: 6885   score: 365.0  epsilon: 1.0    steps: 864  evaluation reward: 410.8\n",
      "Training network. lr: 0.000098. clip: 0.039184\n",
      "Iteration 19810: Policy loss: -0.060247. Value loss: 0.114768. Entropy: 0.302168.\n",
      "Iteration 19811: Policy loss: -0.070086. Value loss: 0.059348. Entropy: 0.303933.\n",
      "Iteration 19812: Policy loss: -0.065964. Value loss: 0.044374. Entropy: 0.303559.\n",
      "Training network. lr: 0.000098. clip: 0.039184\n",
      "Iteration 19813: Policy loss: 0.240766. Value loss: 0.112250. Entropy: 0.307405.\n",
      "Iteration 19814: Policy loss: 0.227492. Value loss: 0.056690. Entropy: 0.307065.\n",
      "Iteration 19815: Policy loss: 0.225135. Value loss: 0.038826. Entropy: 0.306388.\n",
      "episode: 6886   score: 210.0  epsilon: 1.0    steps: 200  evaluation reward: 409.25\n",
      "episode: 6887   score: 355.0  epsilon: 1.0    steps: 384  evaluation reward: 408.8\n",
      "Training network. lr: 0.000098. clip: 0.039184\n",
      "Iteration 19816: Policy loss: 0.041298. Value loss: 0.080645. Entropy: 0.302739.\n",
      "Iteration 19817: Policy loss: 0.046853. Value loss: 0.042758. Entropy: 0.303461.\n",
      "Iteration 19818: Policy loss: 0.042258. Value loss: 0.035286. Entropy: 0.303885.\n",
      "Training network. lr: 0.000098. clip: 0.039184\n",
      "Iteration 19819: Policy loss: -0.193613. Value loss: 0.300214. Entropy: 0.311725.\n",
      "Iteration 19820: Policy loss: -0.196314. Value loss: 0.147994. Entropy: 0.311695.\n",
      "Iteration 19821: Policy loss: -0.210430. Value loss: 0.078429. Entropy: 0.309874.\n",
      "Training network. lr: 0.000098. clip: 0.039184\n",
      "Iteration 19822: Policy loss: 0.083415. Value loss: 0.145659. Entropy: 0.317438.\n",
      "Iteration 19823: Policy loss: 0.067962. Value loss: 0.055647. Entropy: 0.317771.\n",
      "Iteration 19824: Policy loss: 0.073987. Value loss: 0.042474. Entropy: 0.316559.\n",
      "Training network. lr: 0.000098. clip: 0.039184\n",
      "Iteration 19825: Policy loss: 0.023157. Value loss: 0.208783. Entropy: 0.311637.\n",
      "Iteration 19826: Policy loss: 0.018867. Value loss: 0.073381. Entropy: 0.312035.\n",
      "Iteration 19827: Policy loss: 0.023507. Value loss: 0.046680. Entropy: 0.312140.\n",
      "episode: 6888   score: 355.0  epsilon: 1.0    steps: 344  evaluation reward: 409.2\n",
      "Training network. lr: 0.000098. clip: 0.039184\n",
      "Iteration 19828: Policy loss: 0.078554. Value loss: 0.103551. Entropy: 0.308250.\n",
      "Iteration 19829: Policy loss: 0.068076. Value loss: 0.051485. Entropy: 0.307655.\n",
      "Iteration 19830: Policy loss: 0.073433. Value loss: 0.038745. Entropy: 0.307609.\n",
      "episode: 6889   score: 350.0  epsilon: 1.0    steps: 152  evaluation reward: 407.75\n",
      "episode: 6890   score: 300.0  epsilon: 1.0    steps: 288  evaluation reward: 405.3\n",
      "episode: 6891   score: 470.0  epsilon: 1.0    steps: 912  evaluation reward: 406.8\n",
      "Training network. lr: 0.000098. clip: 0.039184\n",
      "Iteration 19831: Policy loss: 0.026750. Value loss: 0.140029. Entropy: 0.302086.\n",
      "Iteration 19832: Policy loss: 0.029563. Value loss: 0.065444. Entropy: 0.301466.\n",
      "Iteration 19833: Policy loss: 0.029516. Value loss: 0.045166. Entropy: 0.301233.\n",
      "episode: 6892   score: 235.0  epsilon: 1.0    steps: 240  evaluation reward: 405.95\n",
      "episode: 6893   score: 420.0  epsilon: 1.0    steps: 704  evaluation reward: 407.0\n",
      "Training network. lr: 0.000098. clip: 0.039184\n",
      "Iteration 19834: Policy loss: 0.252421. Value loss: 0.086522. Entropy: 0.309289.\n",
      "Iteration 19835: Policy loss: 0.247264. Value loss: 0.041874. Entropy: 0.309064.\n",
      "Iteration 19836: Policy loss: 0.247227. Value loss: 0.033144. Entropy: 0.310091.\n",
      "episode: 6894   score: 345.0  epsilon: 1.0    steps: 568  evaluation reward: 407.15\n",
      "Training network. lr: 0.000098. clip: 0.039184\n",
      "Iteration 19837: Policy loss: 0.167801. Value loss: 0.078888. Entropy: 0.309462.\n",
      "Iteration 19838: Policy loss: 0.171278. Value loss: 0.038679. Entropy: 0.309786.\n",
      "Iteration 19839: Policy loss: 0.172341. Value loss: 0.027043. Entropy: 0.309241.\n",
      "episode: 6895   score: 570.0  epsilon: 1.0    steps: 936  evaluation reward: 405.95\n",
      "Training network. lr: 0.000098. clip: 0.039184\n",
      "Iteration 19840: Policy loss: -0.028440. Value loss: 0.100046. Entropy: 0.310929.\n",
      "Iteration 19841: Policy loss: -0.025410. Value loss: 0.037442. Entropy: 0.310615.\n",
      "Iteration 19842: Policy loss: -0.028087. Value loss: 0.028870. Entropy: 0.310142.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6896   score: 210.0  epsilon: 1.0    steps: 552  evaluation reward: 402.3\n",
      "Training network. lr: 0.000098. clip: 0.039184\n",
      "Iteration 19843: Policy loss: 0.063888. Value loss: 0.101569. Entropy: 0.309932.\n",
      "Iteration 19844: Policy loss: 0.058005. Value loss: 0.055743. Entropy: 0.309503.\n",
      "Iteration 19845: Policy loss: 0.058830. Value loss: 0.042242. Entropy: 0.309391.\n",
      "Training network. lr: 0.000098. clip: 0.039184\n",
      "Iteration 19846: Policy loss: 0.079135. Value loss: 0.102763. Entropy: 0.314326.\n",
      "Iteration 19847: Policy loss: 0.067735. Value loss: 0.047811. Entropy: 0.313862.\n",
      "Iteration 19848: Policy loss: 0.061522. Value loss: 0.034028. Entropy: 0.314360.\n",
      "episode: 6897   score: 180.0  epsilon: 1.0    steps: 80  evaluation reward: 397.35\n",
      "episode: 6898   score: 210.0  epsilon: 1.0    steps: 984  evaluation reward: 394.5\n",
      "Training network. lr: 0.000098. clip: 0.039184\n",
      "Iteration 19849: Policy loss: -0.044111. Value loss: 0.103766. Entropy: 0.304957.\n",
      "Iteration 19850: Policy loss: -0.049724. Value loss: 0.041520. Entropy: 0.304369.\n",
      "Iteration 19851: Policy loss: -0.051346. Value loss: 0.029440. Entropy: 0.304220.\n",
      "episode: 6899   score: 330.0  epsilon: 1.0    steps: 392  evaluation reward: 394.5\n",
      "Training network. lr: 0.000098. clip: 0.039027\n",
      "Iteration 19852: Policy loss: 0.385374. Value loss: 0.134648. Entropy: 0.307325.\n",
      "Iteration 19853: Policy loss: 0.375582. Value loss: 0.058061. Entropy: 0.305833.\n",
      "Iteration 19854: Policy loss: 0.380985. Value loss: 0.035723. Entropy: 0.305169.\n",
      "Training network. lr: 0.000098. clip: 0.039027\n",
      "Iteration 19855: Policy loss: 0.208508. Value loss: 0.095419. Entropy: 0.314887.\n",
      "Iteration 19856: Policy loss: 0.207352. Value loss: 0.043588. Entropy: 0.314777.\n",
      "Iteration 19857: Policy loss: 0.210474. Value loss: 0.031280. Entropy: 0.313310.\n",
      "episode: 6900   score: 450.0  epsilon: 1.0    steps: 168  evaluation reward: 396.0\n",
      "Training network. lr: 0.000098. clip: 0.039027\n",
      "Iteration 19858: Policy loss: -0.064159. Value loss: 0.120280. Entropy: 0.305767.\n",
      "Iteration 19859: Policy loss: -0.070256. Value loss: 0.054963. Entropy: 0.306033.\n",
      "Iteration 19860: Policy loss: -0.067337. Value loss: 0.036346. Entropy: 0.306311.\n",
      "now time :  2019-09-06 10:45:00.718769\n",
      "episode: 6901   score: 195.0  epsilon: 1.0    steps: 96  evaluation reward: 396.6\n",
      "episode: 6902   score: 105.0  epsilon: 1.0    steps: 240  evaluation reward: 392.9\n",
      "episode: 6903   score: 80.0  epsilon: 1.0    steps: 416  evaluation reward: 387.15\n",
      "Training network. lr: 0.000098. clip: 0.039027\n",
      "Iteration 19861: Policy loss: -0.074236. Value loss: 0.162499. Entropy: 0.306519.\n",
      "Iteration 19862: Policy loss: -0.069895. Value loss: 0.059503. Entropy: 0.306629.\n",
      "Iteration 19863: Policy loss: -0.074650. Value loss: 0.041042. Entropy: 0.307750.\n",
      "episode: 6904   score: 120.0  epsilon: 1.0    steps: 40  evaluation reward: 384.45\n",
      "episode: 6905   score: 450.0  epsilon: 1.0    steps: 824  evaluation reward: 386.6\n",
      "Training network. lr: 0.000098. clip: 0.039027\n",
      "Iteration 19864: Policy loss: -0.134520. Value loss: 0.073641. Entropy: 0.310772.\n",
      "Iteration 19865: Policy loss: -0.136376. Value loss: 0.025428. Entropy: 0.310775.\n",
      "Iteration 19866: Policy loss: -0.140609. Value loss: 0.018201. Entropy: 0.312075.\n",
      "Training network. lr: 0.000098. clip: 0.039027\n",
      "Iteration 19867: Policy loss: -0.103476. Value loss: 0.099769. Entropy: 0.310243.\n",
      "Iteration 19868: Policy loss: -0.101627. Value loss: 0.038268. Entropy: 0.310308.\n",
      "Iteration 19869: Policy loss: -0.105939. Value loss: 0.026580. Entropy: 0.309781.\n",
      "Training network. lr: 0.000098. clip: 0.039027\n",
      "Iteration 19870: Policy loss: 0.006094. Value loss: 0.060850. Entropy: 0.314076.\n",
      "Iteration 19871: Policy loss: 0.003291. Value loss: 0.026855. Entropy: 0.313497.\n",
      "Iteration 19872: Policy loss: 0.004687. Value loss: 0.020032. Entropy: 0.314257.\n",
      "episode: 6906   score: 450.0  epsilon: 1.0    steps: 424  evaluation reward: 386.55\n",
      "Training network. lr: 0.000098. clip: 0.039027\n",
      "Iteration 19873: Policy loss: 0.181984. Value loss: 0.082472. Entropy: 0.308943.\n",
      "Iteration 19874: Policy loss: 0.182835. Value loss: 0.037182. Entropy: 0.308967.\n",
      "Iteration 19875: Policy loss: 0.177550. Value loss: 0.027932. Entropy: 0.308628.\n",
      "episode: 6907   score: 670.0  epsilon: 1.0    steps: 448  evaluation reward: 389.35\n",
      "Training network. lr: 0.000098. clip: 0.039027\n",
      "Iteration 19876: Policy loss: 0.083166. Value loss: 0.108296. Entropy: 0.309616.\n",
      "Iteration 19877: Policy loss: 0.086439. Value loss: 0.049685. Entropy: 0.309848.\n",
      "Iteration 19878: Policy loss: 0.083024. Value loss: 0.035592. Entropy: 0.309637.\n",
      "Training network. lr: 0.000098. clip: 0.039027\n",
      "Iteration 19879: Policy loss: -0.354419. Value loss: 0.350400. Entropy: 0.315822.\n",
      "Iteration 19880: Policy loss: -0.386803. Value loss: 0.233492. Entropy: 0.316377.\n",
      "Iteration 19881: Policy loss: -0.381113. Value loss: 0.165152. Entropy: 0.315877.\n",
      "episode: 6908   score: 330.0  epsilon: 1.0    steps: 184  evaluation reward: 389.4\n",
      "Training network. lr: 0.000098. clip: 0.039027\n",
      "Iteration 19882: Policy loss: -0.048132. Value loss: 0.375732. Entropy: 0.306022.\n",
      "Iteration 19883: Policy loss: -0.055105. Value loss: 0.222894. Entropy: 0.304565.\n",
      "Iteration 19884: Policy loss: -0.052832. Value loss: 0.152285. Entropy: 0.305594.\n",
      "episode: 6909   score: 575.0  epsilon: 1.0    steps: 184  evaluation reward: 391.05\n",
      "episode: 6910   score: 560.0  epsilon: 1.0    steps: 928  evaluation reward: 394.25\n",
      "Training network. lr: 0.000098. clip: 0.039027\n",
      "Iteration 19885: Policy loss: 0.095250. Value loss: 0.075368. Entropy: 0.308176.\n",
      "Iteration 19886: Policy loss: 0.098736. Value loss: 0.026464. Entropy: 0.307641.\n",
      "Iteration 19887: Policy loss: 0.097826. Value loss: 0.020302. Entropy: 0.307719.\n",
      "episode: 6911   score: 240.0  epsilon: 1.0    steps: 576  evaluation reward: 395.15\n",
      "episode: 6912   score: 210.0  epsilon: 1.0    steps: 792  evaluation reward: 393.65\n",
      "episode: 6913   score: 590.0  epsilon: 1.0    steps: 992  evaluation reward: 398.85\n",
      "Training network. lr: 0.000098. clip: 0.039027\n",
      "Iteration 19888: Policy loss: 0.006021. Value loss: 0.147633. Entropy: 0.307537.\n",
      "Iteration 19889: Policy loss: 0.007354. Value loss: 0.060959. Entropy: 0.307272.\n",
      "Iteration 19890: Policy loss: 0.001656. Value loss: 0.040294. Entropy: 0.306502.\n",
      "episode: 6914   score: 565.0  epsilon: 1.0    steps: 704  evaluation reward: 398.0\n",
      "Training network. lr: 0.000098. clip: 0.039027\n",
      "Iteration 19891: Policy loss: 0.319258. Value loss: 0.112824. Entropy: 0.303990.\n",
      "Iteration 19892: Policy loss: 0.318398. Value loss: 0.043133. Entropy: 0.305007.\n",
      "Iteration 19893: Policy loss: 0.312876. Value loss: 0.031835. Entropy: 0.304663.\n",
      "Training network. lr: 0.000098. clip: 0.039027\n",
      "Iteration 19894: Policy loss: -0.084652. Value loss: 0.080762. Entropy: 0.315092.\n",
      "Iteration 19895: Policy loss: -0.087402. Value loss: 0.040445. Entropy: 0.315414.\n",
      "Iteration 19896: Policy loss: -0.091689. Value loss: 0.026550. Entropy: 0.315530.\n",
      "Training network. lr: 0.000098. clip: 0.039027\n",
      "Iteration 19897: Policy loss: -0.454645. Value loss: 0.327688. Entropy: 0.308722.\n",
      "Iteration 19898: Policy loss: -0.474039. Value loss: 0.193126. Entropy: 0.309375.\n",
      "Iteration 19899: Policy loss: -0.449773. Value loss: 0.118530. Entropy: 0.308683.\n",
      "episode: 6915   score: 335.0  epsilon: 1.0    steps: 768  evaluation reward: 398.2\n",
      "Training network. lr: 0.000098. clip: 0.039027\n",
      "Iteration 19900: Policy loss: 0.114973. Value loss: 0.140136. Entropy: 0.304486.\n",
      "Iteration 19901: Policy loss: 0.110117. Value loss: 0.065471. Entropy: 0.304673.\n",
      "Iteration 19902: Policy loss: 0.107197. Value loss: 0.045227. Entropy: 0.304063.\n",
      "Training network. lr: 0.000097. clip: 0.038870\n",
      "Iteration 19903: Policy loss: 0.195163. Value loss: 0.137669. Entropy: 0.308505.\n",
      "Iteration 19904: Policy loss: 0.195427. Value loss: 0.044366. Entropy: 0.309096.\n",
      "Iteration 19905: Policy loss: 0.187301. Value loss: 0.027350. Entropy: 0.308498.\n",
      "episode: 6916   score: 330.0  epsilon: 1.0    steps: 64  evaluation reward: 398.35\n",
      "episode: 6917   score: 210.0  epsilon: 1.0    steps: 224  evaluation reward: 390.45\n",
      "episode: 6918   score: 215.0  epsilon: 1.0    steps: 320  evaluation reward: 388.75\n",
      "Training network. lr: 0.000097. clip: 0.038870\n",
      "Iteration 19906: Policy loss: -0.105861. Value loss: 0.094164. Entropy: 0.300986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19907: Policy loss: -0.099960. Value loss: 0.037371. Entropy: 0.300201.\n",
      "Iteration 19908: Policy loss: -0.102855. Value loss: 0.031030. Entropy: 0.299283.\n",
      "Training network. lr: 0.000097. clip: 0.038870\n",
      "Iteration 19909: Policy loss: -0.211924. Value loss: 0.327893. Entropy: 0.315959.\n",
      "Iteration 19910: Policy loss: -0.197925. Value loss: 0.113384. Entropy: 0.316108.\n",
      "Iteration 19911: Policy loss: -0.210124. Value loss: 0.057639. Entropy: 0.315555.\n",
      "Training network. lr: 0.000097. clip: 0.038870\n",
      "Iteration 19912: Policy loss: -0.113319. Value loss: 0.359648. Entropy: 0.311285.\n",
      "Iteration 19913: Policy loss: -0.116749. Value loss: 0.216372. Entropy: 0.310971.\n",
      "Iteration 19914: Policy loss: -0.117656. Value loss: 0.167340. Entropy: 0.311130.\n",
      "episode: 6919   score: 540.0  epsilon: 1.0    steps: 664  evaluation reward: 391.05\n",
      "Training network. lr: 0.000097. clip: 0.038870\n",
      "Iteration 19915: Policy loss: -0.512927. Value loss: 0.567108. Entropy: 0.304191.\n",
      "Iteration 19916: Policy loss: -0.503747. Value loss: 0.266521. Entropy: 0.303854.\n",
      "Iteration 19917: Policy loss: -0.526985. Value loss: 0.151018. Entropy: 0.304404.\n",
      "episode: 6920   score: 530.0  epsilon: 1.0    steps: 48  evaluation reward: 392.85\n",
      "Training network. lr: 0.000097. clip: 0.038870\n",
      "Iteration 19918: Policy loss: -0.045205. Value loss: 0.116818. Entropy: 0.309929.\n",
      "Iteration 19919: Policy loss: -0.048971. Value loss: 0.049124. Entropy: 0.310747.\n",
      "Iteration 19920: Policy loss: -0.045181. Value loss: 0.032035. Entropy: 0.310630.\n",
      "episode: 6921   score: 470.0  epsilon: 1.0    steps: 576  evaluation reward: 393.35\n",
      "Training network. lr: 0.000097. clip: 0.038870\n",
      "Iteration 19921: Policy loss: -0.235220. Value loss: 0.307911. Entropy: 0.308606.\n",
      "Iteration 19922: Policy loss: -0.236672. Value loss: 0.136006. Entropy: 0.309586.\n",
      "Iteration 19923: Policy loss: -0.236434. Value loss: 0.080742. Entropy: 0.308676.\n",
      "Training network. lr: 0.000097. clip: 0.038870\n",
      "Iteration 19924: Policy loss: 0.043368. Value loss: 0.192935. Entropy: 0.314095.\n",
      "Iteration 19925: Policy loss: 0.031938. Value loss: 0.075978. Entropy: 0.312655.\n",
      "Iteration 19926: Policy loss: 0.043184. Value loss: 0.052787. Entropy: 0.313131.\n",
      "episode: 6922   score: 940.0  epsilon: 1.0    steps: 248  evaluation reward: 396.55\n",
      "Training network. lr: 0.000097. clip: 0.038870\n",
      "Iteration 19927: Policy loss: -0.265448. Value loss: 0.269080. Entropy: 0.311524.\n",
      "Iteration 19928: Policy loss: -0.269157. Value loss: 0.102194. Entropy: 0.311655.\n",
      "Iteration 19929: Policy loss: -0.263376. Value loss: 0.063973. Entropy: 0.311537.\n",
      "episode: 6923   score: 180.0  epsilon: 1.0    steps: 168  evaluation reward: 396.25\n",
      "episode: 6924   score: 490.0  epsilon: 1.0    steps: 632  evaluation reward: 397.2\n",
      "Training network. lr: 0.000097. clip: 0.038870\n",
      "Iteration 19930: Policy loss: 0.385216. Value loss: 0.143425. Entropy: 0.311584.\n",
      "Iteration 19931: Policy loss: 0.381754. Value loss: 0.041433. Entropy: 0.309601.\n",
      "Iteration 19932: Policy loss: 0.388766. Value loss: 0.028047. Entropy: 0.309673.\n",
      "Training network. lr: 0.000097. clip: 0.038870\n",
      "Iteration 19933: Policy loss: 0.000002. Value loss: 0.118026. Entropy: 0.316500.\n",
      "Iteration 19934: Policy loss: -0.003902. Value loss: 0.051489. Entropy: 0.316339.\n",
      "Iteration 19935: Policy loss: -0.013764. Value loss: 0.032578. Entropy: 0.316585.\n",
      "episode: 6925   score: 730.0  epsilon: 1.0    steps: 480  evaluation reward: 402.2\n",
      "Training network. lr: 0.000097. clip: 0.038870\n",
      "Iteration 19936: Policy loss: 0.130569. Value loss: 0.376657. Entropy: 0.305168.\n",
      "Iteration 19937: Policy loss: 0.110874. Value loss: 0.204137. Entropy: 0.304973.\n",
      "Iteration 19938: Policy loss: 0.112096. Value loss: 0.148317. Entropy: 0.304558.\n",
      "episode: 6926   score: 545.0  epsilon: 1.0    steps: 56  evaluation reward: 404.75\n",
      "episode: 6927   score: 725.0  epsilon: 1.0    steps: 128  evaluation reward: 408.0\n",
      "episode: 6928   score: 215.0  epsilon: 1.0    steps: 504  evaluation reward: 407.25\n",
      "Training network. lr: 0.000097. clip: 0.038870\n",
      "Iteration 19939: Policy loss: -0.087181. Value loss: 0.227196. Entropy: 0.306411.\n",
      "Iteration 19940: Policy loss: -0.095859. Value loss: 0.164949. Entropy: 0.306091.\n",
      "Iteration 19941: Policy loss: -0.095056. Value loss: 0.139153. Entropy: 0.305830.\n",
      "Training network. lr: 0.000097. clip: 0.038870\n",
      "Iteration 19942: Policy loss: 0.291271. Value loss: 0.140125. Entropy: 0.314166.\n",
      "Iteration 19943: Policy loss: 0.284385. Value loss: 0.062136. Entropy: 0.314227.\n",
      "Iteration 19944: Policy loss: 0.282839. Value loss: 0.044780. Entropy: 0.313555.\n",
      "episode: 6929   score: 185.0  epsilon: 1.0    steps: 736  evaluation reward: 401.9\n",
      "Training network. lr: 0.000097. clip: 0.038870\n",
      "Iteration 19945: Policy loss: 0.208375. Value loss: 0.301019. Entropy: 0.306511.\n",
      "Iteration 19946: Policy loss: 0.199888. Value loss: 0.136539. Entropy: 0.306693.\n",
      "Iteration 19947: Policy loss: 0.187291. Value loss: 0.078166. Entropy: 0.306332.\n",
      "episode: 6930   score: 495.0  epsilon: 1.0    steps: 928  evaluation reward: 403.6\n",
      "Training network. lr: 0.000097. clip: 0.038870\n",
      "Iteration 19948: Policy loss: 0.216686. Value loss: 0.177447. Entropy: 0.308261.\n",
      "Iteration 19949: Policy loss: 0.201012. Value loss: 0.066494. Entropy: 0.307824.\n",
      "Iteration 19950: Policy loss: 0.204134. Value loss: 0.054219. Entropy: 0.307878.\n",
      "episode: 6931   score: 155.0  epsilon: 1.0    steps: 480  evaluation reward: 401.25\n",
      "Training network. lr: 0.000097. clip: 0.038723\n",
      "Iteration 19951: Policy loss: -0.200241. Value loss: 0.292762. Entropy: 0.312363.\n",
      "Iteration 19952: Policy loss: -0.200780. Value loss: 0.149245. Entropy: 0.311610.\n",
      "Iteration 19953: Policy loss: -0.209877. Value loss: 0.097112. Entropy: 0.312146.\n",
      "Training network. lr: 0.000097. clip: 0.038723\n",
      "Iteration 19954: Policy loss: -0.022320. Value loss: 0.110323. Entropy: 0.309856.\n",
      "Iteration 19955: Policy loss: -0.024118. Value loss: 0.051870. Entropy: 0.309751.\n",
      "Iteration 19956: Policy loss: -0.036262. Value loss: 0.037899. Entropy: 0.309814.\n",
      "episode: 6932   score: 350.0  epsilon: 1.0    steps: 440  evaluation reward: 400.15\n",
      "episode: 6933   score: 535.0  epsilon: 1.0    steps: 752  evaluation reward: 400.15\n",
      "episode: 6934   score: 445.0  epsilon: 1.0    steps: 776  evaluation reward: 399.25\n",
      "episode: 6935   score: 315.0  epsilon: 1.0    steps: 872  evaluation reward: 398.95\n",
      "Training network. lr: 0.000097. clip: 0.038723\n",
      "Iteration 19957: Policy loss: 0.054813. Value loss: 0.167151. Entropy: 0.304176.\n",
      "Iteration 19958: Policy loss: 0.054998. Value loss: 0.073500. Entropy: 0.305632.\n",
      "Iteration 19959: Policy loss: 0.051992. Value loss: 0.043866. Entropy: 0.303442.\n",
      "episode: 6936   score: 820.0  epsilon: 1.0    steps: 848  evaluation reward: 402.9\n",
      "Training network. lr: 0.000097. clip: 0.038723\n",
      "Iteration 19960: Policy loss: -0.063451. Value loss: 0.436869. Entropy: 0.313528.\n",
      "Iteration 19961: Policy loss: -0.065894. Value loss: 0.157413. Entropy: 0.312249.\n",
      "Iteration 19962: Policy loss: -0.079084. Value loss: 0.079798. Entropy: 0.312780.\n",
      "Training network. lr: 0.000097. clip: 0.038723\n",
      "Iteration 19963: Policy loss: 0.113162. Value loss: 0.139164. Entropy: 0.312788.\n",
      "Iteration 19964: Policy loss: 0.103373. Value loss: 0.075149. Entropy: 0.312032.\n",
      "Iteration 19965: Policy loss: 0.105951. Value loss: 0.053919. Entropy: 0.312083.\n",
      "Training network. lr: 0.000097. clip: 0.038723\n",
      "Iteration 19966: Policy loss: -0.011954. Value loss: 0.220279. Entropy: 0.310285.\n",
      "Iteration 19967: Policy loss: -0.016523. Value loss: 0.090999. Entropy: 0.310127.\n",
      "Iteration 19968: Policy loss: -0.022336. Value loss: 0.060018. Entropy: 0.309445.\n",
      "episode: 6937   score: 140.0  epsilon: 1.0    steps: 464  evaluation reward: 400.95\n",
      "Training network. lr: 0.000097. clip: 0.038723\n",
      "Iteration 19969: Policy loss: 0.448660. Value loss: 0.195987. Entropy: 0.306536.\n",
      "Iteration 19970: Policy loss: 0.453282. Value loss: 0.066871. Entropy: 0.306575.\n",
      "Iteration 19971: Policy loss: 0.455650. Value loss: 0.045788. Entropy: 0.306052.\n",
      "episode: 6938   score: 425.0  epsilon: 1.0    steps: 8  evaluation reward: 404.9\n",
      "episode: 6939   score: 545.0  epsilon: 1.0    steps: 56  evaluation reward: 407.4\n",
      "Training network. lr: 0.000097. clip: 0.038723\n",
      "Iteration 19972: Policy loss: 0.314232. Value loss: 0.154123. Entropy: 0.303574.\n",
      "Iteration 19973: Policy loss: 0.312859. Value loss: 0.061748. Entropy: 0.303896.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19974: Policy loss: 0.307555. Value loss: 0.048872. Entropy: 0.302359.\n",
      "episode: 6940   score: 450.0  epsilon: 1.0    steps: 408  evaluation reward: 404.3\n",
      "Training network. lr: 0.000097. clip: 0.038723\n",
      "Iteration 19975: Policy loss: 0.204309. Value loss: 0.212356. Entropy: 0.309302.\n",
      "Iteration 19976: Policy loss: 0.190712. Value loss: 0.092134. Entropy: 0.308700.\n",
      "Iteration 19977: Policy loss: 0.196297. Value loss: 0.064283. Entropy: 0.309334.\n",
      "episode: 6941   score: 335.0  epsilon: 1.0    steps: 904  evaluation reward: 405.55\n",
      "Training network. lr: 0.000097. clip: 0.038723\n",
      "Iteration 19978: Policy loss: -0.298047. Value loss: 0.271012. Entropy: 0.307137.\n",
      "Iteration 19979: Policy loss: -0.299856. Value loss: 0.115330. Entropy: 0.307263.\n",
      "Iteration 19980: Policy loss: -0.317068. Value loss: 0.074110. Entropy: 0.308329.\n",
      "Training network. lr: 0.000097. clip: 0.038723\n",
      "Iteration 19981: Policy loss: 0.028774. Value loss: 0.118103. Entropy: 0.308476.\n",
      "Iteration 19982: Policy loss: 0.030075. Value loss: 0.051853. Entropy: 0.309636.\n",
      "Iteration 19983: Policy loss: 0.023043. Value loss: 0.038665. Entropy: 0.308350.\n",
      "episode: 6942   score: 390.0  epsilon: 1.0    steps: 584  evaluation reward: 401.7\n",
      "Training network. lr: 0.000097. clip: 0.038723\n",
      "Iteration 19984: Policy loss: 0.532205. Value loss: 0.186438. Entropy: 0.309387.\n",
      "Iteration 19985: Policy loss: 0.539395. Value loss: 0.055271. Entropy: 0.308129.\n",
      "Iteration 19986: Policy loss: 0.530349. Value loss: 0.039275. Entropy: 0.309447.\n",
      "episode: 6943   score: 210.0  epsilon: 1.0    steps: 328  evaluation reward: 400.55\n",
      "episode: 6944   score: 240.0  epsilon: 1.0    steps: 664  evaluation reward: 400.55\n",
      "episode: 6945   score: 420.0  epsilon: 1.0    steps: 936  evaluation reward: 402.3\n",
      "Training network. lr: 0.000097. clip: 0.038723\n",
      "Iteration 19987: Policy loss: 0.081710. Value loss: 0.239748. Entropy: 0.304690.\n",
      "Iteration 19988: Policy loss: 0.076875. Value loss: 0.078930. Entropy: 0.304980.\n",
      "Iteration 19989: Policy loss: 0.069399. Value loss: 0.062102. Entropy: 0.305352.\n",
      "episode: 6946   score: 390.0  epsilon: 1.0    steps: 432  evaluation reward: 404.1\n",
      "episode: 6947   score: 635.0  epsilon: 1.0    steps: 1008  evaluation reward: 404.8\n",
      "Training network. lr: 0.000097. clip: 0.038723\n",
      "Iteration 19990: Policy loss: 0.147961. Value loss: 0.108438. Entropy: 0.307379.\n",
      "Iteration 19991: Policy loss: 0.138625. Value loss: 0.037303. Entropy: 0.306643.\n",
      "Iteration 19992: Policy loss: 0.139846. Value loss: 0.028401. Entropy: 0.306107.\n",
      "episode: 6948   score: 210.0  epsilon: 1.0    steps: 704  evaluation reward: 402.7\n",
      "Training network. lr: 0.000097. clip: 0.038723\n",
      "Iteration 19993: Policy loss: -0.048540. Value loss: 0.069211. Entropy: 0.307954.\n",
      "Iteration 19994: Policy loss: -0.048677. Value loss: 0.032982. Entropy: 0.307786.\n",
      "Iteration 19995: Policy loss: -0.055149. Value loss: 0.026009. Entropy: 0.307672.\n",
      "Training network. lr: 0.000097. clip: 0.038723\n",
      "Iteration 19996: Policy loss: -0.314033. Value loss: 0.230507. Entropy: 0.313245.\n",
      "Iteration 19997: Policy loss: -0.325737. Value loss: 0.122482. Entropy: 0.313048.\n",
      "Iteration 19998: Policy loss: -0.325920. Value loss: 0.085187. Entropy: 0.312435.\n",
      "episode: 6949   score: 220.0  epsilon: 1.0    steps: 104  evaluation reward: 400.5\n",
      "Training network. lr: 0.000097. clip: 0.038723\n",
      "Iteration 19999: Policy loss: -0.643974. Value loss: 0.342612. Entropy: 0.308069.\n",
      "Iteration 20000: Policy loss: -0.658055. Value loss: 0.130423. Entropy: 0.308567.\n",
      "Iteration 20001: Policy loss: -0.670398. Value loss: 0.078288. Entropy: 0.308934.\n",
      "Training network. lr: 0.000096. clip: 0.038566\n",
      "Iteration 20002: Policy loss: -0.149344. Value loss: 0.141076. Entropy: 0.314444.\n",
      "Iteration 20003: Policy loss: -0.145959. Value loss: 0.051042. Entropy: 0.315021.\n",
      "Iteration 20004: Policy loss: -0.148816. Value loss: 0.036600. Entropy: 0.315579.\n",
      "episode: 6950   score: 330.0  epsilon: 1.0    steps: 904  evaluation reward: 398.65\n",
      "Training network. lr: 0.000096. clip: 0.038566\n",
      "Iteration 20005: Policy loss: 0.165696. Value loss: 0.291491. Entropy: 0.309640.\n",
      "Iteration 20006: Policy loss: 0.161082. Value loss: 0.111508. Entropy: 0.309022.\n",
      "Iteration 20007: Policy loss: 0.164504. Value loss: 0.071994. Entropy: 0.309773.\n",
      "now time :  2019-09-06 10:53:58.813978\n",
      "episode: 6951   score: 545.0  epsilon: 1.0    steps: 400  evaluation reward: 399.9\n",
      "Training network. lr: 0.000096. clip: 0.038566\n",
      "Iteration 20008: Policy loss: 0.268995. Value loss: 0.286968. Entropy: 0.312115.\n",
      "Iteration 20009: Policy loss: 0.272753. Value loss: 0.103716. Entropy: 0.312286.\n",
      "Iteration 20010: Policy loss: 0.274088. Value loss: 0.065684. Entropy: 0.310729.\n",
      "episode: 6952   score: 330.0  epsilon: 1.0    steps: 552  evaluation reward: 400.35\n",
      "Training network. lr: 0.000096. clip: 0.038566\n",
      "Iteration 20011: Policy loss: 0.318233. Value loss: 0.129997. Entropy: 0.309258.\n",
      "Iteration 20012: Policy loss: 0.308043. Value loss: 0.061833. Entropy: 0.309316.\n",
      "Iteration 20013: Policy loss: 0.307565. Value loss: 0.046555. Entropy: 0.308170.\n",
      "episode: 6953   score: 695.0  epsilon: 1.0    steps: 16  evaluation reward: 405.2\n",
      "episode: 6954   score: 335.0  epsilon: 1.0    steps: 904  evaluation reward: 404.05\n",
      "Training network. lr: 0.000096. clip: 0.038566\n",
      "Iteration 20014: Policy loss: 0.336833. Value loss: 0.199282. Entropy: 0.306338.\n",
      "Iteration 20015: Policy loss: 0.340482. Value loss: 0.067720. Entropy: 0.304379.\n",
      "Iteration 20016: Policy loss: 0.319310. Value loss: 0.050070. Entropy: 0.305046.\n",
      "Training network. lr: 0.000096. clip: 0.038566\n",
      "Iteration 20017: Policy loss: 0.357812. Value loss: 0.171320. Entropy: 0.313175.\n",
      "Iteration 20018: Policy loss: 0.357756. Value loss: 0.064166. Entropy: 0.311992.\n",
      "Iteration 20019: Policy loss: 0.351435. Value loss: 0.042755. Entropy: 0.312266.\n",
      "episode: 6955   score: 240.0  epsilon: 1.0    steps: 184  evaluation reward: 400.0\n",
      "Training network. lr: 0.000096. clip: 0.038566\n",
      "Iteration 20020: Policy loss: 0.227447. Value loss: 0.144991. Entropy: 0.306498.\n",
      "Iteration 20021: Policy loss: 0.206387. Value loss: 0.056086. Entropy: 0.306430.\n",
      "Iteration 20022: Policy loss: 0.210924. Value loss: 0.036705. Entropy: 0.305799.\n",
      "episode: 6956   score: 210.0  epsilon: 1.0    steps: 768  evaluation reward: 397.65\n",
      "episode: 6957   score: 620.0  epsilon: 1.0    steps: 960  evaluation reward: 399.55\n",
      "Training network. lr: 0.000096. clip: 0.038566\n",
      "Iteration 20023: Policy loss: -0.348747. Value loss: 0.302495. Entropy: 0.310174.\n",
      "Iteration 20024: Policy loss: -0.354679. Value loss: 0.153443. Entropy: 0.309687.\n",
      "Iteration 20025: Policy loss: -0.368157. Value loss: 0.101449. Entropy: 0.309641.\n",
      "episode: 6958   score: 740.0  epsilon: 1.0    steps: 136  evaluation reward: 402.6\n",
      "episode: 6959   score: 135.0  epsilon: 1.0    steps: 704  evaluation reward: 401.0\n",
      "Training network. lr: 0.000096. clip: 0.038566\n",
      "Iteration 20026: Policy loss: 0.264141. Value loss: 0.151478. Entropy: 0.303082.\n",
      "Iteration 20027: Policy loss: 0.278849. Value loss: 0.071444. Entropy: 0.303959.\n",
      "Iteration 20028: Policy loss: 0.265878. Value loss: 0.047260. Entropy: 0.304261.\n",
      "episode: 6960   score: 320.0  epsilon: 1.0    steps: 680  evaluation reward: 397.7\n",
      "episode: 6961   score: 210.0  epsilon: 1.0    steps: 864  evaluation reward: 393.45\n",
      "Training network. lr: 0.000096. clip: 0.038566\n",
      "Iteration 20029: Policy loss: -0.080681. Value loss: 0.129703. Entropy: 0.311504.\n",
      "Iteration 20030: Policy loss: -0.083262. Value loss: 0.071487. Entropy: 0.312641.\n",
      "Iteration 20031: Policy loss: -0.091626. Value loss: 0.051302. Entropy: 0.311489.\n",
      "episode: 6962   score: 345.0  epsilon: 1.0    steps: 976  evaluation reward: 389.7\n",
      "Training network. lr: 0.000096. clip: 0.038566\n",
      "Iteration 20032: Policy loss: 0.124176. Value loss: 0.097159. Entropy: 0.306408.\n",
      "Iteration 20033: Policy loss: 0.118412. Value loss: 0.049674. Entropy: 0.306727.\n",
      "Iteration 20034: Policy loss: 0.113805. Value loss: 0.040677. Entropy: 0.306417.\n",
      "Training network. lr: 0.000096. clip: 0.038566\n",
      "Iteration 20035: Policy loss: -0.106172. Value loss: 0.321620. Entropy: 0.309460.\n",
      "Iteration 20036: Policy loss: -0.101491. Value loss: 0.218681. Entropy: 0.309209.\n",
      "Iteration 20037: Policy loss: -0.110227. Value loss: 0.164853. Entropy: 0.308253.\n",
      "Training network. lr: 0.000096. clip: 0.038566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20038: Policy loss: -0.194235. Value loss: 0.223619. Entropy: 0.311539.\n",
      "Iteration 20039: Policy loss: -0.212414. Value loss: 0.092097. Entropy: 0.311728.\n",
      "Iteration 20040: Policy loss: -0.210176. Value loss: 0.049625. Entropy: 0.310967.\n",
      "episode: 6963   score: 185.0  epsilon: 1.0    steps: 688  evaluation reward: 388.2\n",
      "Training network. lr: 0.000096. clip: 0.038566\n",
      "Iteration 20041: Policy loss: 0.048187. Value loss: 0.245429. Entropy: 0.309139.\n",
      "Iteration 20042: Policy loss: 0.062050. Value loss: 0.100801. Entropy: 0.307108.\n",
      "Iteration 20043: Policy loss: 0.046867. Value loss: 0.061280. Entropy: 0.310081.\n",
      "episode: 6964   score: 210.0  epsilon: 1.0    steps: 432  evaluation reward: 387.6\n",
      "episode: 6965   score: 535.0  epsilon: 1.0    steps: 552  evaluation reward: 388.75\n",
      "episode: 6966   score: 285.0  epsilon: 1.0    steps: 888  evaluation reward: 388.7\n",
      "Training network. lr: 0.000096. clip: 0.038566\n",
      "Iteration 20044: Policy loss: 0.162756. Value loss: 0.171010. Entropy: 0.306106.\n",
      "Iteration 20045: Policy loss: 0.163079. Value loss: 0.078783. Entropy: 0.305869.\n",
      "Iteration 20046: Policy loss: 0.160742. Value loss: 0.055047. Entropy: 0.305227.\n",
      "Training network. lr: 0.000096. clip: 0.038566\n",
      "Iteration 20047: Policy loss: 0.135389. Value loss: 0.166634. Entropy: 0.313041.\n",
      "Iteration 20048: Policy loss: 0.118713. Value loss: 0.080787. Entropy: 0.313831.\n",
      "Iteration 20049: Policy loss: 0.122755. Value loss: 0.054727. Entropy: 0.312359.\n",
      "Training network. lr: 0.000096. clip: 0.038566\n",
      "Iteration 20050: Policy loss: 0.336809. Value loss: 0.143822. Entropy: 0.312564.\n",
      "Iteration 20051: Policy loss: 0.328990. Value loss: 0.069734. Entropy: 0.311904.\n",
      "Iteration 20052: Policy loss: 0.323864. Value loss: 0.050311. Entropy: 0.311407.\n",
      "episode: 6967   score: 495.0  epsilon: 1.0    steps: 112  evaluation reward: 389.05\n",
      "episode: 6968   score: 165.0  epsilon: 1.0    steps: 816  evaluation reward: 387.2\n",
      "Training network. lr: 0.000096. clip: 0.038409\n",
      "Iteration 20053: Policy loss: 0.184982. Value loss: 0.124958. Entropy: 0.308026.\n",
      "Iteration 20054: Policy loss: 0.173297. Value loss: 0.057631. Entropy: 0.307665.\n",
      "Iteration 20055: Policy loss: 0.183808. Value loss: 0.038341. Entropy: 0.307981.\n",
      "episode: 6969   score: 260.0  epsilon: 1.0    steps: 256  evaluation reward: 386.0\n",
      "Training network. lr: 0.000096. clip: 0.038409\n",
      "Iteration 20056: Policy loss: -0.022923. Value loss: 0.069780. Entropy: 0.308177.\n",
      "Iteration 20057: Policy loss: -0.020963. Value loss: 0.029502. Entropy: 0.308062.\n",
      "Iteration 20058: Policy loss: -0.020475. Value loss: 0.021741. Entropy: 0.307619.\n",
      "episode: 6970   score: 590.0  epsilon: 1.0    steps: 536  evaluation reward: 389.8\n",
      "episode: 6971   score: 425.0  epsilon: 1.0    steps: 936  evaluation reward: 390.7\n",
      "Training network. lr: 0.000096. clip: 0.038409\n",
      "Iteration 20059: Policy loss: 0.070175. Value loss: 0.134581. Entropy: 0.308962.\n",
      "Iteration 20060: Policy loss: 0.065846. Value loss: 0.052334. Entropy: 0.309839.\n",
      "Iteration 20061: Policy loss: 0.060890. Value loss: 0.033323. Entropy: 0.309801.\n",
      "episode: 6972   score: 240.0  epsilon: 1.0    steps: 432  evaluation reward: 389.2\n",
      "Training network. lr: 0.000096. clip: 0.038409\n",
      "Iteration 20062: Policy loss: 0.055754. Value loss: 0.243054. Entropy: 0.307774.\n",
      "Iteration 20063: Policy loss: 0.056129. Value loss: 0.093626. Entropy: 0.306223.\n",
      "Iteration 20064: Policy loss: 0.039798. Value loss: 0.061065. Entropy: 0.306917.\n",
      "Training network. lr: 0.000096. clip: 0.038409\n",
      "Iteration 20065: Policy loss: -0.109001. Value loss: 0.279905. Entropy: 0.312466.\n",
      "Iteration 20066: Policy loss: -0.123852. Value loss: 0.176902. Entropy: 0.313602.\n",
      "Iteration 20067: Policy loss: -0.127724. Value loss: 0.135625. Entropy: 0.312844.\n",
      "episode: 6973   score: 315.0  epsilon: 1.0    steps: 648  evaluation reward: 386.8\n",
      "Training network. lr: 0.000096. clip: 0.038409\n",
      "Iteration 20068: Policy loss: 0.271457. Value loss: 0.177573. Entropy: 0.308331.\n",
      "Iteration 20069: Policy loss: 0.271460. Value loss: 0.081768. Entropy: 0.306475.\n",
      "Iteration 20070: Policy loss: 0.264459. Value loss: 0.051517. Entropy: 0.307796.\n",
      "Training network. lr: 0.000096. clip: 0.038409\n",
      "Iteration 20071: Policy loss: -0.214147. Value loss: 0.205077. Entropy: 0.311740.\n",
      "Iteration 20072: Policy loss: -0.207795. Value loss: 0.105776. Entropy: 0.310566.\n",
      "Iteration 20073: Policy loss: -0.192017. Value loss: 0.054774. Entropy: 0.310738.\n",
      "episode: 6974   score: 560.0  epsilon: 1.0    steps: 608  evaluation reward: 389.05\n",
      "Training network. lr: 0.000096. clip: 0.038409\n",
      "Iteration 20074: Policy loss: 0.187990. Value loss: 0.080586. Entropy: 0.306025.\n",
      "Iteration 20075: Policy loss: 0.185385. Value loss: 0.032561. Entropy: 0.307256.\n",
      "Iteration 20076: Policy loss: 0.188123. Value loss: 0.024283. Entropy: 0.306636.\n",
      "episode: 6975   score: 335.0  epsilon: 1.0    steps: 224  evaluation reward: 387.25\n",
      "episode: 6976   score: 280.0  epsilon: 1.0    steps: 512  evaluation reward: 383.6\n",
      "Training network. lr: 0.000096. clip: 0.038409\n",
      "Iteration 20077: Policy loss: -0.589742. Value loss: 0.432246. Entropy: 0.304275.\n",
      "Iteration 20078: Policy loss: -0.596520. Value loss: 0.219307. Entropy: 0.303460.\n",
      "Iteration 20079: Policy loss: -0.596901. Value loss: 0.147407. Entropy: 0.304903.\n",
      "episode: 6977   score: 290.0  epsilon: 1.0    steps: 792  evaluation reward: 382.5\n",
      "Training network. lr: 0.000096. clip: 0.038409\n",
      "Iteration 20080: Policy loss: 0.098669. Value loss: 0.104773. Entropy: 0.310178.\n",
      "Iteration 20081: Policy loss: 0.104744. Value loss: 0.039508. Entropy: 0.310528.\n",
      "Iteration 20082: Policy loss: 0.098245. Value loss: 0.025614. Entropy: 0.310057.\n",
      "episode: 6978   score: 620.0  epsilon: 1.0    steps: 40  evaluation reward: 385.65\n",
      "episode: 6979   score: 475.0  epsilon: 1.0    steps: 264  evaluation reward: 383.15\n",
      "Training network. lr: 0.000096. clip: 0.038409\n",
      "Iteration 20083: Policy loss: 0.147225. Value loss: 0.119508. Entropy: 0.305148.\n",
      "Iteration 20084: Policy loss: 0.143358. Value loss: 0.040349. Entropy: 0.303534.\n",
      "Iteration 20085: Policy loss: 0.145205. Value loss: 0.026602. Entropy: 0.303383.\n",
      "episode: 6980   score: 415.0  epsilon: 1.0    steps: 144  evaluation reward: 384.35\n",
      "Training network. lr: 0.000096. clip: 0.038409\n",
      "Iteration 20086: Policy loss: 0.402675. Value loss: 0.135414. Entropy: 0.310414.\n",
      "Iteration 20087: Policy loss: 0.405002. Value loss: 0.051624. Entropy: 0.310794.\n",
      "Iteration 20088: Policy loss: 0.392298. Value loss: 0.034439. Entropy: 0.308994.\n",
      "Training network. lr: 0.000096. clip: 0.038409\n",
      "Iteration 20089: Policy loss: 0.236086. Value loss: 0.139696. Entropy: 0.311524.\n",
      "Iteration 20090: Policy loss: 0.233984. Value loss: 0.079140. Entropy: 0.311335.\n",
      "Iteration 20091: Policy loss: 0.240519. Value loss: 0.053380. Entropy: 0.311795.\n",
      "Training network. lr: 0.000096. clip: 0.038409\n",
      "Iteration 20092: Policy loss: -0.006276. Value loss: 0.100843. Entropy: 0.309773.\n",
      "Iteration 20093: Policy loss: -0.000243. Value loss: 0.049012. Entropy: 0.310017.\n",
      "Iteration 20094: Policy loss: -0.003659. Value loss: 0.033127. Entropy: 0.309334.\n",
      "episode: 6981   score: 210.0  epsilon: 1.0    steps: 336  evaluation reward: 382.2\n",
      "Training network. lr: 0.000096. clip: 0.038409\n",
      "Iteration 20095: Policy loss: -0.239377. Value loss: 0.233183. Entropy: 0.308787.\n",
      "Iteration 20096: Policy loss: -0.246822. Value loss: 0.171686. Entropy: 0.308879.\n",
      "Iteration 20097: Policy loss: -0.210464. Value loss: 0.125630. Entropy: 0.308513.\n",
      "episode: 6982   score: 235.0  epsilon: 1.0    steps: 264  evaluation reward: 380.35\n",
      "episode: 6983   score: 560.0  epsilon: 1.0    steps: 704  evaluation reward: 380.95\n",
      "Training network. lr: 0.000096. clip: 0.038409\n",
      "Iteration 20098: Policy loss: -0.454841. Value loss: 0.233329. Entropy: 0.312044.\n",
      "Iteration 20099: Policy loss: -0.471303. Value loss: 0.088997. Entropy: 0.312822.\n",
      "Iteration 20100: Policy loss: -0.479339. Value loss: 0.064143. Entropy: 0.312546.\n",
      "episode: 6984   score: 820.0  epsilon: 1.0    steps: 720  evaluation reward: 384.2\n",
      "Training network. lr: 0.000096. clip: 0.038262\n",
      "Iteration 20101: Policy loss: 0.237318. Value loss: 0.147486. Entropy: 0.312197.\n",
      "Iteration 20102: Policy loss: 0.233625. Value loss: 0.056121. Entropy: 0.311800.\n",
      "Iteration 20103: Policy loss: 0.229454. Value loss: 0.038290. Entropy: 0.311943.\n",
      "episode: 6985   score: 285.0  epsilon: 1.0    steps: 448  evaluation reward: 383.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6986   score: 285.0  epsilon: 1.0    steps: 544  evaluation reward: 384.15\n",
      "Training network. lr: 0.000096. clip: 0.038262\n",
      "Iteration 20104: Policy loss: 0.181820. Value loss: 0.092898. Entropy: 0.309542.\n",
      "Iteration 20105: Policy loss: 0.174899. Value loss: 0.042415. Entropy: 0.308069.\n",
      "Iteration 20106: Policy loss: 0.177370. Value loss: 0.029708. Entropy: 0.307576.\n",
      "episode: 6987   score: 540.0  epsilon: 1.0    steps: 160  evaluation reward: 386.0\n",
      "episode: 6988   score: 360.0  epsilon: 1.0    steps: 760  evaluation reward: 386.05\n",
      "Training network. lr: 0.000096. clip: 0.038262\n",
      "Iteration 20107: Policy loss: -0.096231. Value loss: 0.367682. Entropy: 0.310103.\n",
      "Iteration 20108: Policy loss: -0.094034. Value loss: 0.124949. Entropy: 0.309170.\n",
      "Iteration 20109: Policy loss: -0.102735. Value loss: 0.063751. Entropy: 0.309549.\n",
      "Training network. lr: 0.000096. clip: 0.038262\n",
      "Iteration 20110: Policy loss: 0.447980. Value loss: 0.144124. Entropy: 0.311961.\n",
      "Iteration 20111: Policy loss: 0.445888. Value loss: 0.065393. Entropy: 0.311460.\n",
      "Iteration 20112: Policy loss: 0.442935. Value loss: 0.047746. Entropy: 0.310724.\n",
      "Training network. lr: 0.000096. clip: 0.038262\n",
      "Iteration 20113: Policy loss: -0.102058. Value loss: 0.127090. Entropy: 0.308731.\n",
      "Iteration 20114: Policy loss: -0.109831. Value loss: 0.038435. Entropy: 0.307936.\n",
      "Iteration 20115: Policy loss: -0.109666. Value loss: 0.029652. Entropy: 0.308901.\n",
      "Training network. lr: 0.000096. clip: 0.038262\n",
      "Iteration 20116: Policy loss: -0.133791. Value loss: 0.206167. Entropy: 0.310584.\n",
      "Iteration 20117: Policy loss: -0.136329. Value loss: 0.090460. Entropy: 0.310944.\n",
      "Iteration 20118: Policy loss: -0.135758. Value loss: 0.049970. Entropy: 0.311027.\n",
      "episode: 6989   score: 250.0  epsilon: 1.0    steps: 152  evaluation reward: 385.05\n",
      "Training network. lr: 0.000096. clip: 0.038262\n",
      "Iteration 20119: Policy loss: -0.052075. Value loss: 0.266194. Entropy: 0.311422.\n",
      "Iteration 20120: Policy loss: -0.056169. Value loss: 0.125020. Entropy: 0.311584.\n",
      "Iteration 20121: Policy loss: -0.067654. Value loss: 0.069149. Entropy: 0.312243.\n",
      "episode: 6990   score: 355.0  epsilon: 1.0    steps: 128  evaluation reward: 385.6\n",
      "episode: 6991   score: 475.0  epsilon: 1.0    steps: 152  evaluation reward: 385.65\n",
      "Training network. lr: 0.000096. clip: 0.038262\n",
      "Iteration 20122: Policy loss: -0.225461. Value loss: 0.240292. Entropy: 0.307370.\n",
      "Iteration 20123: Policy loss: -0.233712. Value loss: 0.145982. Entropy: 0.305317.\n",
      "Iteration 20124: Policy loss: -0.234231. Value loss: 0.112768. Entropy: 0.306159.\n",
      "episode: 6992   score: 260.0  epsilon: 1.0    steps: 568  evaluation reward: 385.9\n",
      "Training network. lr: 0.000096. clip: 0.038262\n",
      "Iteration 20125: Policy loss: 0.100708. Value loss: 0.132747. Entropy: 0.311072.\n",
      "Iteration 20126: Policy loss: 0.095227. Value loss: 0.062243. Entropy: 0.311217.\n",
      "Iteration 20127: Policy loss: 0.088049. Value loss: 0.045761. Entropy: 0.310789.\n",
      "episode: 6993   score: 260.0  epsilon: 1.0    steps: 232  evaluation reward: 384.3\n",
      "episode: 6994   score: 545.0  epsilon: 1.0    steps: 488  evaluation reward: 386.3\n",
      "Training network. lr: 0.000096. clip: 0.038262\n",
      "Iteration 20128: Policy loss: 0.063009. Value loss: 0.081702. Entropy: 0.307283.\n",
      "Iteration 20129: Policy loss: 0.069099. Value loss: 0.043604. Entropy: 0.308556.\n",
      "Iteration 20130: Policy loss: 0.053186. Value loss: 0.033048. Entropy: 0.308072.\n",
      "Training network. lr: 0.000096. clip: 0.038262\n",
      "Iteration 20131: Policy loss: 0.267960. Value loss: 0.136248. Entropy: 0.315870.\n",
      "Iteration 20132: Policy loss: 0.262944. Value loss: 0.036836. Entropy: 0.315028.\n",
      "Iteration 20133: Policy loss: 0.257326. Value loss: 0.026471. Entropy: 0.314308.\n",
      "Training network. lr: 0.000096. clip: 0.038262\n",
      "Iteration 20134: Policy loss: 0.133723. Value loss: 0.189608. Entropy: 0.306022.\n",
      "Iteration 20135: Policy loss: 0.134405. Value loss: 0.060127. Entropy: 0.305533.\n",
      "Iteration 20136: Policy loss: 0.129521. Value loss: 0.040356. Entropy: 0.304523.\n",
      "Training network. lr: 0.000096. clip: 0.038262\n",
      "Iteration 20137: Policy loss: 0.306165. Value loss: 0.226581. Entropy: 0.312991.\n",
      "Iteration 20138: Policy loss: 0.290811. Value loss: 0.108002. Entropy: 0.311408.\n",
      "Iteration 20139: Policy loss: 0.294823. Value loss: 0.059499. Entropy: 0.311312.\n",
      "episode: 6995   score: 155.0  epsilon: 1.0    steps: 432  evaluation reward: 382.15\n",
      "episode: 6996   score: 260.0  epsilon: 1.0    steps: 640  evaluation reward: 382.65\n",
      "Training network. lr: 0.000096. clip: 0.038262\n",
      "Iteration 20140: Policy loss: 0.114372. Value loss: 0.184849. Entropy: 0.305368.\n",
      "Iteration 20141: Policy loss: 0.109690. Value loss: 0.081213. Entropy: 0.305613.\n",
      "Iteration 20142: Policy loss: 0.110812. Value loss: 0.058379. Entropy: 0.304087.\n",
      "episode: 6997   score: 590.0  epsilon: 1.0    steps: 256  evaluation reward: 386.75\n",
      "episode: 6998   score: 315.0  epsilon: 1.0    steps: 864  evaluation reward: 387.8\n",
      "Training network. lr: 0.000096. clip: 0.038262\n",
      "Iteration 20143: Policy loss: -0.300715. Value loss: 0.184880. Entropy: 0.303525.\n",
      "Iteration 20144: Policy loss: -0.301503. Value loss: 0.074112. Entropy: 0.303289.\n",
      "Iteration 20145: Policy loss: -0.309815. Value loss: 0.055606. Entropy: 0.303197.\n",
      "episode: 6999   score: 495.0  epsilon: 1.0    steps: 216  evaluation reward: 389.45\n",
      "episode: 7000   score: 780.0  epsilon: 1.0    steps: 888  evaluation reward: 392.75\n",
      "Training network. lr: 0.000096. clip: 0.038262\n",
      "Iteration 20146: Policy loss: 0.094140. Value loss: 0.153942. Entropy: 0.304542.\n",
      "Iteration 20147: Policy loss: 0.089987. Value loss: 0.074469. Entropy: 0.303332.\n",
      "Iteration 20148: Policy loss: 0.080629. Value loss: 0.057979. Entropy: 0.304386.\n",
      "now time :  2019-09-06 11:02:34.813321\n",
      "episode: 7001   score: 530.0  epsilon: 1.0    steps: 320  evaluation reward: 396.1\n",
      "Training network. lr: 0.000096. clip: 0.038262\n",
      "Iteration 20149: Policy loss: 0.207922. Value loss: 0.272325. Entropy: 0.310934.\n",
      "Iteration 20150: Policy loss: 0.201150. Value loss: 0.110391. Entropy: 0.310332.\n",
      "Iteration 20151: Policy loss: 0.206772. Value loss: 0.084076. Entropy: 0.310873.\n",
      "Training network. lr: 0.000095. clip: 0.038105\n",
      "Iteration 20152: Policy loss: 0.016942. Value loss: 0.273955. Entropy: 0.315998.\n",
      "Iteration 20153: Policy loss: 0.023938. Value loss: 0.127255. Entropy: 0.315670.\n",
      "Iteration 20154: Policy loss: 0.020640. Value loss: 0.079896. Entropy: 0.315560.\n",
      "Training network. lr: 0.000095. clip: 0.038105\n",
      "Iteration 20155: Policy loss: -0.095852. Value loss: 0.300434. Entropy: 0.313929.\n",
      "Iteration 20156: Policy loss: -0.109231. Value loss: 0.171426. Entropy: 0.313728.\n",
      "Iteration 20157: Policy loss: -0.110403. Value loss: 0.121196. Entropy: 0.313272.\n",
      "episode: 7002   score: 465.0  epsilon: 1.0    steps: 608  evaluation reward: 399.7\n",
      "Training network. lr: 0.000095. clip: 0.038105\n",
      "Iteration 20158: Policy loss: 0.074926. Value loss: 0.158672. Entropy: 0.305426.\n",
      "Iteration 20159: Policy loss: 0.070361. Value loss: 0.067669. Entropy: 0.304406.\n",
      "Iteration 20160: Policy loss: 0.064170. Value loss: 0.041397. Entropy: 0.304290.\n",
      "episode: 7003   score: 350.0  epsilon: 1.0    steps: 792  evaluation reward: 402.4\n",
      "episode: 7004   score: 435.0  epsilon: 1.0    steps: 856  evaluation reward: 405.55\n",
      "Training network. lr: 0.000095. clip: 0.038105\n",
      "Iteration 20161: Policy loss: 0.362187. Value loss: 0.127787. Entropy: 0.305411.\n",
      "Iteration 20162: Policy loss: 0.360545. Value loss: 0.051702. Entropy: 0.305307.\n",
      "Iteration 20163: Policy loss: 0.361122. Value loss: 0.038803. Entropy: 0.304587.\n",
      "Training network. lr: 0.000095. clip: 0.038105\n",
      "Iteration 20164: Policy loss: -0.108484. Value loss: 0.122673. Entropy: 0.314580.\n",
      "Iteration 20165: Policy loss: -0.110702. Value loss: 0.059300. Entropy: 0.314607.\n",
      "Iteration 20166: Policy loss: -0.111347. Value loss: 0.040795. Entropy: 0.314893.\n",
      "Training network. lr: 0.000095. clip: 0.038105\n",
      "Iteration 20167: Policy loss: 0.088863. Value loss: 0.104899. Entropy: 0.311541.\n",
      "Iteration 20168: Policy loss: 0.092492. Value loss: 0.052686. Entropy: 0.311958.\n",
      "Iteration 20169: Policy loss: 0.090111. Value loss: 0.032738. Entropy: 0.312118.\n",
      "episode: 7005   score: 490.0  epsilon: 1.0    steps: 328  evaluation reward: 405.95\n",
      "Training network. lr: 0.000095. clip: 0.038105\n",
      "Iteration 20170: Policy loss: 0.275329. Value loss: 0.190846. Entropy: 0.311612.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20171: Policy loss: 0.259510. Value loss: 0.067839. Entropy: 0.309358.\n",
      "Iteration 20172: Policy loss: 0.257566. Value loss: 0.046424. Entropy: 0.308554.\n",
      "Training network. lr: 0.000095. clip: 0.038105\n",
      "Iteration 20173: Policy loss: 0.338953. Value loss: 0.073569. Entropy: 0.307948.\n",
      "Iteration 20174: Policy loss: 0.340225. Value loss: 0.028592. Entropy: 0.307364.\n",
      "Iteration 20175: Policy loss: 0.330006. Value loss: 0.020028. Entropy: 0.307956.\n",
      "episode: 7006   score: 475.0  epsilon: 1.0    steps: 704  evaluation reward: 406.2\n",
      "Training network. lr: 0.000095. clip: 0.038105\n",
      "Iteration 20176: Policy loss: -0.038350. Value loss: 0.135016. Entropy: 0.309459.\n",
      "Iteration 20177: Policy loss: -0.044620. Value loss: 0.055368. Entropy: 0.309134.\n",
      "Iteration 20178: Policy loss: -0.038861. Value loss: 0.038143. Entropy: 0.309645.\n",
      "episode: 7007   score: 670.0  epsilon: 1.0    steps: 344  evaluation reward: 406.2\n",
      "episode: 7008   score: 435.0  epsilon: 1.0    steps: 624  evaluation reward: 407.25\n",
      "Training network. lr: 0.000095. clip: 0.038105\n",
      "Iteration 20179: Policy loss: 0.088930. Value loss: 0.105373. Entropy: 0.308168.\n",
      "Iteration 20180: Policy loss: 0.092423. Value loss: 0.038715. Entropy: 0.307801.\n",
      "Iteration 20181: Policy loss: 0.085204. Value loss: 0.028548. Entropy: 0.307961.\n",
      "episode: 7009   score: 345.0  epsilon: 1.0    steps: 952  evaluation reward: 404.95\n",
      "Training network. lr: 0.000095. clip: 0.038105\n",
      "Iteration 20182: Policy loss: -0.190738. Value loss: 0.124005. Entropy: 0.311901.\n",
      "Iteration 20183: Policy loss: -0.206615. Value loss: 0.054424. Entropy: 0.311178.\n",
      "Iteration 20184: Policy loss: -0.207118. Value loss: 0.039411. Entropy: 0.311615.\n",
      "episode: 7010   score: 565.0  epsilon: 1.0    steps: 168  evaluation reward: 405.0\n",
      "Training network. lr: 0.000095. clip: 0.038105\n",
      "Iteration 20185: Policy loss: 0.093944. Value loss: 0.071762. Entropy: 0.308677.\n",
      "Iteration 20186: Policy loss: 0.077378. Value loss: 0.026565. Entropy: 0.310525.\n",
      "Iteration 20187: Policy loss: 0.080742. Value loss: 0.020251. Entropy: 0.309532.\n",
      "Training network. lr: 0.000095. clip: 0.038105\n",
      "Iteration 20188: Policy loss: 0.241218. Value loss: 0.117750. Entropy: 0.317060.\n",
      "Iteration 20189: Policy loss: 0.246119. Value loss: 0.046826. Entropy: 0.316747.\n",
      "Iteration 20190: Policy loss: 0.240211. Value loss: 0.030922. Entropy: 0.316512.\n",
      "Training network. lr: 0.000095. clip: 0.038105\n",
      "Iteration 20191: Policy loss: 0.219759. Value loss: 0.173294. Entropy: 0.310158.\n",
      "Iteration 20192: Policy loss: 0.215419. Value loss: 0.076025. Entropy: 0.309255.\n",
      "Iteration 20193: Policy loss: 0.215207. Value loss: 0.043561. Entropy: 0.309651.\n",
      "episode: 7011   score: 420.0  epsilon: 1.0    steps: 64  evaluation reward: 406.8\n",
      "episode: 7012   score: 275.0  epsilon: 1.0    steps: 520  evaluation reward: 407.45\n",
      "episode: 7013   score: 470.0  epsilon: 1.0    steps: 544  evaluation reward: 406.25\n",
      "Training network. lr: 0.000095. clip: 0.038105\n",
      "Iteration 20194: Policy loss: 0.213824. Value loss: 0.081046. Entropy: 0.294991.\n",
      "Iteration 20195: Policy loss: 0.216449. Value loss: 0.040350. Entropy: 0.296916.\n",
      "Iteration 20196: Policy loss: 0.210854. Value loss: 0.028677. Entropy: 0.295998.\n",
      "episode: 7014   score: 215.0  epsilon: 1.0    steps: 1000  evaluation reward: 402.75\n",
      "Training network. lr: 0.000095. clip: 0.038105\n",
      "Iteration 20197: Policy loss: -0.091007. Value loss: 0.096208. Entropy: 0.312002.\n",
      "Iteration 20198: Policy loss: -0.098079. Value loss: 0.050795. Entropy: 0.311317.\n",
      "Iteration 20199: Policy loss: -0.096224. Value loss: 0.039598. Entropy: 0.310981.\n",
      "episode: 7015   score: 285.0  epsilon: 1.0    steps: 464  evaluation reward: 402.25\n",
      "Training network. lr: 0.000095. clip: 0.038105\n",
      "Iteration 20200: Policy loss: 0.141491. Value loss: 0.111461. Entropy: 0.306270.\n",
      "Iteration 20201: Policy loss: 0.138315. Value loss: 0.046251. Entropy: 0.306139.\n",
      "Iteration 20202: Policy loss: 0.140777. Value loss: 0.031352. Entropy: 0.305172.\n",
      "Training network. lr: 0.000095. clip: 0.037949\n",
      "Iteration 20203: Policy loss: 0.188450. Value loss: 0.103721. Entropy: 0.311131.\n",
      "Iteration 20204: Policy loss: 0.181830. Value loss: 0.047080. Entropy: 0.311278.\n",
      "Iteration 20205: Policy loss: 0.183828. Value loss: 0.030545. Entropy: 0.310805.\n",
      "episode: 7016   score: 290.0  epsilon: 1.0    steps: 64  evaluation reward: 401.85\n",
      "episode: 7017   score: 230.0  epsilon: 1.0    steps: 464  evaluation reward: 402.05\n",
      "Training network. lr: 0.000095. clip: 0.037949\n",
      "Iteration 20206: Policy loss: 0.025805. Value loss: 0.124535. Entropy: 0.299864.\n",
      "Iteration 20207: Policy loss: 0.023157. Value loss: 0.068017. Entropy: 0.301220.\n",
      "Iteration 20208: Policy loss: 0.024926. Value loss: 0.054138. Entropy: 0.300729.\n",
      "episode: 7018   score: 185.0  epsilon: 1.0    steps: 176  evaluation reward: 401.75\n",
      "Training network. lr: 0.000095. clip: 0.037949\n",
      "Iteration 20209: Policy loss: 0.053956. Value loss: 0.130870. Entropy: 0.307002.\n",
      "Iteration 20210: Policy loss: 0.043495. Value loss: 0.064550. Entropy: 0.307745.\n",
      "Iteration 20211: Policy loss: 0.047834. Value loss: 0.041173. Entropy: 0.307565.\n",
      "Training network. lr: 0.000095. clip: 0.037949\n",
      "Iteration 20212: Policy loss: -0.069610. Value loss: 0.133034. Entropy: 0.311079.\n",
      "Iteration 20213: Policy loss: -0.071305. Value loss: 0.045793. Entropy: 0.311473.\n",
      "Iteration 20214: Policy loss: -0.075192. Value loss: 0.035235. Entropy: 0.311725.\n",
      "episode: 7019   score: 370.0  epsilon: 1.0    steps: 576  evaluation reward: 400.05\n",
      "Training network. lr: 0.000095. clip: 0.037949\n",
      "Iteration 20215: Policy loss: -0.063531. Value loss: 0.142771. Entropy: 0.310349.\n",
      "Iteration 20216: Policy loss: -0.061182. Value loss: 0.051430. Entropy: 0.310532.\n",
      "Iteration 20217: Policy loss: -0.072317. Value loss: 0.033437. Entropy: 0.311564.\n",
      "Training network. lr: 0.000095. clip: 0.037949\n",
      "Iteration 20218: Policy loss: 0.123041. Value loss: 0.081031. Entropy: 0.310759.\n",
      "Iteration 20219: Policy loss: 0.116346. Value loss: 0.034814. Entropy: 0.308821.\n",
      "Iteration 20220: Policy loss: 0.112477. Value loss: 0.023168. Entropy: 0.309644.\n",
      "episode: 7020   score: 300.0  epsilon: 1.0    steps: 376  evaluation reward: 397.75\n",
      "episode: 7021   score: 495.0  epsilon: 1.0    steps: 424  evaluation reward: 398.0\n",
      "Training network. lr: 0.000095. clip: 0.037949\n",
      "Iteration 20221: Policy loss: 0.123147. Value loss: 0.088888. Entropy: 0.300011.\n",
      "Iteration 20222: Policy loss: 0.111943. Value loss: 0.041671. Entropy: 0.300712.\n",
      "Iteration 20223: Policy loss: 0.113205. Value loss: 0.033460. Entropy: 0.302366.\n",
      "episode: 7022   score: 390.0  epsilon: 1.0    steps: 544  evaluation reward: 392.5\n",
      "Training network. lr: 0.000095. clip: 0.037949\n",
      "Iteration 20224: Policy loss: 0.069838. Value loss: 0.126286. Entropy: 0.306124.\n",
      "Iteration 20225: Policy loss: 0.081118. Value loss: 0.050189. Entropy: 0.307946.\n",
      "Iteration 20226: Policy loss: 0.075704. Value loss: 0.035555. Entropy: 0.307231.\n",
      "episode: 7023   score: 255.0  epsilon: 1.0    steps: 440  evaluation reward: 393.25\n",
      "episode: 7024   score: 155.0  epsilon: 1.0    steps: 640  evaluation reward: 389.9\n",
      "Training network. lr: 0.000095. clip: 0.037949\n",
      "Iteration 20227: Policy loss: 0.080367. Value loss: 0.095299. Entropy: 0.304110.\n",
      "Iteration 20228: Policy loss: 0.081760. Value loss: 0.041633. Entropy: 0.303788.\n",
      "Iteration 20229: Policy loss: 0.085925. Value loss: 0.027006. Entropy: 0.303681.\n",
      "episode: 7025   score: 530.0  epsilon: 1.0    steps: 568  evaluation reward: 387.9\n",
      "episode: 7026   score: 490.0  epsilon: 1.0    steps: 624  evaluation reward: 387.35\n",
      "Training network. lr: 0.000095. clip: 0.037949\n",
      "Iteration 20230: Policy loss: -0.043368. Value loss: 0.085861. Entropy: 0.309641.\n",
      "Iteration 20231: Policy loss: -0.050674. Value loss: 0.039545. Entropy: 0.309297.\n",
      "Iteration 20232: Policy loss: -0.047073. Value loss: 0.032313. Entropy: 0.308627.\n",
      "Training network. lr: 0.000095. clip: 0.037949\n",
      "Iteration 20233: Policy loss: -0.074071. Value loss: 0.066282. Entropy: 0.312997.\n",
      "Iteration 20234: Policy loss: -0.075071. Value loss: 0.029508. Entropy: 0.312892.\n",
      "Iteration 20235: Policy loss: -0.071275. Value loss: 0.020656. Entropy: 0.312596.\n",
      "Training network. lr: 0.000095. clip: 0.037949\n",
      "Iteration 20236: Policy loss: -0.349998. Value loss: 0.412343. Entropy: 0.311213.\n",
      "Iteration 20237: Policy loss: -0.336218. Value loss: 0.272819. Entropy: 0.310786.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20238: Policy loss: -0.368692. Value loss: 0.241428. Entropy: 0.311361.\n",
      "episode: 7027   score: 415.0  epsilon: 1.0    steps: 400  evaluation reward: 384.25\n",
      "Training network. lr: 0.000095. clip: 0.037949\n",
      "Iteration 20239: Policy loss: -0.066109. Value loss: 0.059691. Entropy: 0.303147.\n",
      "Iteration 20240: Policy loss: -0.066766. Value loss: 0.029154. Entropy: 0.304852.\n",
      "Iteration 20241: Policy loss: -0.072044. Value loss: 0.021302. Entropy: 0.305858.\n",
      "episode: 7028   score: 400.0  epsilon: 1.0    steps: 968  evaluation reward: 386.1\n",
      "Training network. lr: 0.000095. clip: 0.037949\n",
      "Iteration 20242: Policy loss: 0.013608. Value loss: 0.109246. Entropy: 0.307970.\n",
      "Iteration 20243: Policy loss: 0.013276. Value loss: 0.045955. Entropy: 0.307518.\n",
      "Iteration 20244: Policy loss: 0.007955. Value loss: 0.033181. Entropy: 0.308672.\n",
      "Training network. lr: 0.000095. clip: 0.037949\n",
      "Iteration 20245: Policy loss: -0.152481. Value loss: 0.379364. Entropy: 0.313348.\n",
      "Iteration 20246: Policy loss: -0.167884. Value loss: 0.276068. Entropy: 0.313235.\n",
      "Iteration 20247: Policy loss: -0.162049. Value loss: 0.212341. Entropy: 0.312702.\n",
      "episode: 7029   score: 320.0  epsilon: 1.0    steps: 584  evaluation reward: 387.45\n",
      "Training network. lr: 0.000095. clip: 0.037949\n",
      "Iteration 20248: Policy loss: 0.114948. Value loss: 0.135870. Entropy: 0.311615.\n",
      "Iteration 20249: Policy loss: 0.111500. Value loss: 0.058277. Entropy: 0.310738.\n",
      "Iteration 20250: Policy loss: 0.113615. Value loss: 0.043297. Entropy: 0.310663.\n",
      "episode: 7030   score: 390.0  epsilon: 1.0    steps: 80  evaluation reward: 386.4\n",
      "episode: 7031   score: 395.0  epsilon: 1.0    steps: 272  evaluation reward: 388.8\n",
      "episode: 7032   score: 315.0  epsilon: 1.0    steps: 408  evaluation reward: 388.45\n",
      "Training network. lr: 0.000095. clip: 0.037801\n",
      "Iteration 20251: Policy loss: 0.072131. Value loss: 0.057423. Entropy: 0.302465.\n",
      "Iteration 20252: Policy loss: 0.067080. Value loss: 0.027673. Entropy: 0.302708.\n",
      "Iteration 20253: Policy loss: 0.065991. Value loss: 0.020277. Entropy: 0.302621.\n",
      "Training network. lr: 0.000095. clip: 0.037801\n",
      "Iteration 20254: Policy loss: -0.110931. Value loss: 0.108832. Entropy: 0.315955.\n",
      "Iteration 20255: Policy loss: -0.117374. Value loss: 0.055292. Entropy: 0.316394.\n",
      "Iteration 20256: Policy loss: -0.117422. Value loss: 0.042565. Entropy: 0.316091.\n",
      "episode: 7033   score: 420.0  epsilon: 1.0    steps: 608  evaluation reward: 387.3\n",
      "Training network. lr: 0.000095. clip: 0.037801\n",
      "Iteration 20257: Policy loss: 0.276665. Value loss: 0.107056. Entropy: 0.312311.\n",
      "Iteration 20258: Policy loss: 0.277481. Value loss: 0.046875. Entropy: 0.310689.\n",
      "Iteration 20259: Policy loss: 0.263116. Value loss: 0.031444. Entropy: 0.312200.\n",
      "Training network. lr: 0.000095. clip: 0.037801\n",
      "Iteration 20260: Policy loss: 0.091893. Value loss: 0.082040. Entropy: 0.313367.\n",
      "Iteration 20261: Policy loss: 0.094103. Value loss: 0.031148. Entropy: 0.311300.\n",
      "Iteration 20262: Policy loss: 0.086816. Value loss: 0.020800. Entropy: 0.311858.\n",
      "episode: 7034   score: 260.0  epsilon: 1.0    steps: 72  evaluation reward: 385.45\n",
      "episode: 7035   score: 620.0  epsilon: 1.0    steps: 472  evaluation reward: 388.5\n",
      "Training network. lr: 0.000095. clip: 0.037801\n",
      "Iteration 20263: Policy loss: -0.083373. Value loss: 0.095139. Entropy: 0.307446.\n",
      "Iteration 20264: Policy loss: -0.085321. Value loss: 0.038152. Entropy: 0.305644.\n",
      "Iteration 20265: Policy loss: -0.081653. Value loss: 0.024726. Entropy: 0.306065.\n",
      "episode: 7036   score: 270.0  epsilon: 1.0    steps: 920  evaluation reward: 383.0\n",
      "Training network. lr: 0.000095. clip: 0.037801\n",
      "Iteration 20266: Policy loss: -0.140314. Value loss: 0.402879. Entropy: 0.310437.\n",
      "Iteration 20267: Policy loss: -0.140763. Value loss: 0.250415. Entropy: 0.310674.\n",
      "Iteration 20268: Policy loss: -0.160762. Value loss: 0.205251. Entropy: 0.311819.\n",
      "Training network. lr: 0.000095. clip: 0.037801\n",
      "Iteration 20269: Policy loss: -0.062108. Value loss: 0.081533. Entropy: 0.308592.\n",
      "Iteration 20270: Policy loss: -0.063594. Value loss: 0.039418. Entropy: 0.308322.\n",
      "Iteration 20271: Policy loss: -0.069130. Value loss: 0.032008. Entropy: 0.308720.\n",
      "episode: 7037   score: 315.0  epsilon: 1.0    steps: 144  evaluation reward: 384.75\n",
      "Training network. lr: 0.000095. clip: 0.037801\n",
      "Iteration 20272: Policy loss: -0.458997. Value loss: 0.271713. Entropy: 0.309957.\n",
      "Iteration 20273: Policy loss: -0.465536. Value loss: 0.160207. Entropy: 0.309053.\n",
      "Iteration 20274: Policy loss: -0.450406. Value loss: 0.112635. Entropy: 0.310274.\n",
      "episode: 7038   score: 375.0  epsilon: 1.0    steps: 800  evaluation reward: 384.25\n",
      "episode: 7039   score: 315.0  epsilon: 1.0    steps: 912  evaluation reward: 381.95\n",
      "Training network. lr: 0.000095. clip: 0.037801\n",
      "Iteration 20275: Policy loss: -0.191587. Value loss: 0.310177. Entropy: 0.309305.\n",
      "Iteration 20276: Policy loss: -0.189983. Value loss: 0.207724. Entropy: 0.306570.\n",
      "Iteration 20277: Policy loss: -0.180156. Value loss: 0.145250. Entropy: 0.306127.\n",
      "Training network. lr: 0.000095. clip: 0.037801\n",
      "Iteration 20278: Policy loss: 0.189143. Value loss: 0.117840. Entropy: 0.311448.\n",
      "Iteration 20279: Policy loss: 0.179125. Value loss: 0.043410. Entropy: 0.310400.\n",
      "Iteration 20280: Policy loss: 0.180914. Value loss: 0.030507. Entropy: 0.309832.\n",
      "episode: 7040   score: 660.0  epsilon: 1.0    steps: 560  evaluation reward: 384.05\n",
      "Training network. lr: 0.000095. clip: 0.037801\n",
      "Iteration 20281: Policy loss: 0.046366. Value loss: 0.110871. Entropy: 0.305897.\n",
      "Iteration 20282: Policy loss: 0.043771. Value loss: 0.044754. Entropy: 0.304856.\n",
      "Iteration 20283: Policy loss: 0.044314. Value loss: 0.030985. Entropy: 0.305494.\n",
      "episode: 7041   score: 445.0  epsilon: 1.0    steps: 576  evaluation reward: 385.15\n",
      "Training network. lr: 0.000095. clip: 0.037801\n",
      "Iteration 20284: Policy loss: -0.334466. Value loss: 0.318688. Entropy: 0.304301.\n",
      "Iteration 20285: Policy loss: -0.341367. Value loss: 0.180895. Entropy: 0.303123.\n",
      "Iteration 20286: Policy loss: -0.363900. Value loss: 0.125532. Entropy: 0.304299.\n",
      "episode: 7042   score: 560.0  epsilon: 1.0    steps: 392  evaluation reward: 386.85\n",
      "Training network. lr: 0.000095. clip: 0.037801\n",
      "Iteration 20287: Policy loss: 0.071896. Value loss: 0.114079. Entropy: 0.310859.\n",
      "Iteration 20288: Policy loss: 0.079202. Value loss: 0.047741. Entropy: 0.309981.\n",
      "Iteration 20289: Policy loss: 0.072364. Value loss: 0.035681. Entropy: 0.310119.\n",
      "episode: 7043   score: 620.0  epsilon: 1.0    steps: 576  evaluation reward: 390.95\n",
      "episode: 7044   score: 405.0  epsilon: 1.0    steps: 920  evaluation reward: 392.6\n",
      "Training network. lr: 0.000095. clip: 0.037801\n",
      "Iteration 20290: Policy loss: 0.350878. Value loss: 0.209694. Entropy: 0.306138.\n",
      "Iteration 20291: Policy loss: 0.339702. Value loss: 0.079694. Entropy: 0.306205.\n",
      "Iteration 20292: Policy loss: 0.340517. Value loss: 0.058343. Entropy: 0.304953.\n",
      "episode: 7045   score: 575.0  epsilon: 1.0    steps: 200  evaluation reward: 394.15\n",
      "Training network. lr: 0.000095. clip: 0.037801\n",
      "Iteration 20293: Policy loss: 0.028776. Value loss: 0.139229. Entropy: 0.306493.\n",
      "Iteration 20294: Policy loss: 0.026631. Value loss: 0.070182. Entropy: 0.306015.\n",
      "Iteration 20295: Policy loss: 0.019742. Value loss: 0.048474. Entropy: 0.306302.\n",
      "episode: 7046   score: 315.0  epsilon: 1.0    steps: 736  evaluation reward: 393.4\n",
      "Training network. lr: 0.000095. clip: 0.037801\n",
      "Iteration 20296: Policy loss: 0.406581. Value loss: 0.140404. Entropy: 0.312959.\n",
      "Iteration 20297: Policy loss: 0.411702. Value loss: 0.055454. Entropy: 0.311633.\n",
      "Iteration 20298: Policy loss: 0.396504. Value loss: 0.039657. Entropy: 0.312253.\n",
      "episode: 7047   score: 250.0  epsilon: 1.0    steps: 560  evaluation reward: 389.55\n",
      "Training network. lr: 0.000095. clip: 0.037801\n",
      "Iteration 20299: Policy loss: -0.064286. Value loss: 0.337245. Entropy: 0.309776.\n",
      "Iteration 20300: Policy loss: -0.068403. Value loss: 0.165382. Entropy: 0.309753.\n",
      "Iteration 20301: Policy loss: -0.065322. Value loss: 0.117349. Entropy: 0.310837.\n",
      "episode: 7048   score: 140.0  epsilon: 1.0    steps: 808  evaluation reward: 388.85\n",
      "episode: 7049   score: 210.0  epsilon: 1.0    steps: 968  evaluation reward: 388.75\n",
      "Training network. lr: 0.000094. clip: 0.037645\n",
      "Iteration 20302: Policy loss: 0.135420. Value loss: 0.103519. Entropy: 0.310085.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20303: Policy loss: 0.118877. Value loss: 0.046587. Entropy: 0.309423.\n",
      "Iteration 20304: Policy loss: 0.124024. Value loss: 0.035642. Entropy: 0.309939.\n",
      "episode: 7050   score: 285.0  epsilon: 1.0    steps: 136  evaluation reward: 388.3\n",
      "Training network. lr: 0.000094. clip: 0.037645\n",
      "Iteration 20305: Policy loss: -0.102200. Value loss: 0.108564. Entropy: 0.308955.\n",
      "Iteration 20306: Policy loss: -0.106664. Value loss: 0.051144. Entropy: 0.308140.\n",
      "Iteration 20307: Policy loss: -0.104637. Value loss: 0.036653. Entropy: 0.309550.\n",
      "Training network. lr: 0.000094. clip: 0.037645\n",
      "Iteration 20308: Policy loss: -0.451419. Value loss: 0.341824. Entropy: 0.312411.\n",
      "Iteration 20309: Policy loss: -0.453716. Value loss: 0.174496. Entropy: 0.312929.\n",
      "Iteration 20310: Policy loss: -0.447279. Value loss: 0.117736. Entropy: 0.312489.\n",
      "now time :  2019-09-06 11:12:27.753938\n",
      "episode: 7051   score: 365.0  epsilon: 1.0    steps: 648  evaluation reward: 386.5\n",
      "Training network. lr: 0.000094. clip: 0.037645\n",
      "Iteration 20311: Policy loss: 0.043312. Value loss: 0.118454. Entropy: 0.306964.\n",
      "Iteration 20312: Policy loss: 0.040246. Value loss: 0.047677. Entropy: 0.306456.\n",
      "Iteration 20313: Policy loss: 0.034267. Value loss: 0.033924. Entropy: 0.305316.\n",
      "episode: 7052   score: 625.0  epsilon: 1.0    steps: 512  evaluation reward: 389.45\n",
      "episode: 7053   score: 505.0  epsilon: 1.0    steps: 600  evaluation reward: 387.55\n",
      "Training network. lr: 0.000094. clip: 0.037645\n",
      "Iteration 20314: Policy loss: -0.326383. Value loss: 0.378269. Entropy: 0.302335.\n",
      "Iteration 20315: Policy loss: -0.341477. Value loss: 0.219522. Entropy: 0.299287.\n",
      "Iteration 20316: Policy loss: -0.334503. Value loss: 0.149824. Entropy: 0.299431.\n",
      "Training network. lr: 0.000094. clip: 0.037645\n",
      "Iteration 20317: Policy loss: -0.318109. Value loss: 0.385709. Entropy: 0.312564.\n",
      "Iteration 20318: Policy loss: -0.334209. Value loss: 0.141811. Entropy: 0.312812.\n",
      "Iteration 20319: Policy loss: -0.347974. Value loss: 0.098125. Entropy: 0.312871.\n",
      "episode: 7054   score: 240.0  epsilon: 1.0    steps: 560  evaluation reward: 386.6\n",
      "episode: 7055   score: 680.0  epsilon: 1.0    steps: 648  evaluation reward: 391.0\n",
      "Training network. lr: 0.000094. clip: 0.037645\n",
      "Iteration 20320: Policy loss: -0.328388. Value loss: 0.337617. Entropy: 0.304652.\n",
      "Iteration 20321: Policy loss: -0.327308. Value loss: 0.161300. Entropy: 0.304968.\n",
      "Iteration 20322: Policy loss: -0.316259. Value loss: 0.091557. Entropy: 0.304512.\n",
      "Training network. lr: 0.000094. clip: 0.037645\n",
      "Iteration 20323: Policy loss: -0.034214. Value loss: 0.163577. Entropy: 0.310869.\n",
      "Iteration 20324: Policy loss: -0.040767. Value loss: 0.067642. Entropy: 0.310980.\n",
      "Iteration 20325: Policy loss: -0.039692. Value loss: 0.047302. Entropy: 0.310706.\n",
      "episode: 7056   score: 515.0  epsilon: 1.0    steps: 24  evaluation reward: 394.05\n",
      "episode: 7057   score: 330.0  epsilon: 1.0    steps: 256  evaluation reward: 391.15\n",
      "Training network. lr: 0.000094. clip: 0.037645\n",
      "Iteration 20326: Policy loss: 0.327029. Value loss: 0.190314. Entropy: 0.306548.\n",
      "Iteration 20327: Policy loss: 0.328631. Value loss: 0.066967. Entropy: 0.305794.\n",
      "Iteration 20328: Policy loss: 0.321386. Value loss: 0.044355. Entropy: 0.306028.\n",
      "episode: 7058   score: 600.0  epsilon: 1.0    steps: 752  evaluation reward: 389.75\n",
      "Training network. lr: 0.000094. clip: 0.037645\n",
      "Iteration 20329: Policy loss: -0.050531. Value loss: 0.142439. Entropy: 0.307406.\n",
      "Iteration 20330: Policy loss: -0.049734. Value loss: 0.078751. Entropy: 0.306764.\n",
      "Iteration 20331: Policy loss: -0.056887. Value loss: 0.063027. Entropy: 0.307432.\n",
      "Training network. lr: 0.000094. clip: 0.037645\n",
      "Iteration 20332: Policy loss: -0.319022. Value loss: 0.287922. Entropy: 0.308366.\n",
      "Iteration 20333: Policy loss: -0.327133. Value loss: 0.099072. Entropy: 0.307291.\n",
      "Iteration 20334: Policy loss: -0.339298. Value loss: 0.061840. Entropy: 0.309087.\n",
      "Training network. lr: 0.000094. clip: 0.037645\n",
      "Iteration 20335: Policy loss: -0.006615. Value loss: 0.182948. Entropy: 0.305366.\n",
      "Iteration 20336: Policy loss: -0.013580. Value loss: 0.071954. Entropy: 0.304578.\n",
      "Iteration 20337: Policy loss: -0.009876. Value loss: 0.049652. Entropy: 0.304788.\n",
      "episode: 7059   score: 440.0  epsilon: 1.0    steps: 608  evaluation reward: 392.8\n",
      "Training network. lr: 0.000094. clip: 0.037645\n",
      "Iteration 20338: Policy loss: 0.337316. Value loss: 0.239450. Entropy: 0.297542.\n",
      "Iteration 20339: Policy loss: 0.332862. Value loss: 0.086128. Entropy: 0.298147.\n",
      "Iteration 20340: Policy loss: 0.323618. Value loss: 0.051956. Entropy: 0.297959.\n",
      "episode: 7060   score: 300.0  epsilon: 1.0    steps: 320  evaluation reward: 392.6\n",
      "episode: 7061   score: 395.0  epsilon: 1.0    steps: 952  evaluation reward: 394.45\n",
      "Training network. lr: 0.000094. clip: 0.037645\n",
      "Iteration 20341: Policy loss: -0.064102. Value loss: 0.104135. Entropy: 0.301159.\n",
      "Iteration 20342: Policy loss: -0.065536. Value loss: 0.058096. Entropy: 0.300300.\n",
      "Iteration 20343: Policy loss: -0.066404. Value loss: 0.044531. Entropy: 0.300930.\n",
      "Training network. lr: 0.000094. clip: 0.037645\n",
      "Iteration 20344: Policy loss: -0.156922. Value loss: 0.154396. Entropy: 0.299721.\n",
      "Iteration 20345: Policy loss: -0.160468. Value loss: 0.070351. Entropy: 0.301902.\n",
      "Iteration 20346: Policy loss: -0.159657. Value loss: 0.049643. Entropy: 0.298309.\n",
      "episode: 7062   score: 370.0  epsilon: 1.0    steps: 224  evaluation reward: 394.7\n",
      "Training network. lr: 0.000094. clip: 0.037645\n",
      "Iteration 20347: Policy loss: -0.190476. Value loss: 0.160414. Entropy: 0.298964.\n",
      "Iteration 20348: Policy loss: -0.184322. Value loss: 0.068724. Entropy: 0.298155.\n",
      "Iteration 20349: Policy loss: -0.188868. Value loss: 0.044024. Entropy: 0.298128.\n",
      "episode: 7063   score: 690.0  epsilon: 1.0    steps: 104  evaluation reward: 399.75\n",
      "Training network. lr: 0.000094. clip: 0.037645\n",
      "Iteration 20350: Policy loss: 0.072047. Value loss: 0.318843. Entropy: 0.303912.\n",
      "Iteration 20351: Policy loss: 0.065663. Value loss: 0.195623. Entropy: 0.304369.\n",
      "Iteration 20352: Policy loss: 0.076262. Value loss: 0.125412. Entropy: 0.304777.\n",
      "episode: 7064   score: 470.0  epsilon: 1.0    steps: 360  evaluation reward: 402.35\n",
      "episode: 7065   score: 450.0  epsilon: 1.0    steps: 584  evaluation reward: 401.5\n",
      "Training network. lr: 0.000094. clip: 0.037488\n",
      "Iteration 20353: Policy loss: 0.128762. Value loss: 0.156778. Entropy: 0.298691.\n",
      "Iteration 20354: Policy loss: 0.120264. Value loss: 0.070442. Entropy: 0.298802.\n",
      "Iteration 20355: Policy loss: 0.123259. Value loss: 0.051272. Entropy: 0.297626.\n",
      "Training network. lr: 0.000094. clip: 0.037488\n",
      "Iteration 20356: Policy loss: 0.052207. Value loss: 0.174661. Entropy: 0.306347.\n",
      "Iteration 20357: Policy loss: 0.043327. Value loss: 0.071876. Entropy: 0.306068.\n",
      "Iteration 20358: Policy loss: 0.038424. Value loss: 0.045495. Entropy: 0.305325.\n",
      "Training network. lr: 0.000094. clip: 0.037488\n",
      "Iteration 20359: Policy loss: 0.079919. Value loss: 0.178658. Entropy: 0.306196.\n",
      "Iteration 20360: Policy loss: 0.077556. Value loss: 0.088879. Entropy: 0.305825.\n",
      "Iteration 20361: Policy loss: 0.080449. Value loss: 0.063165. Entropy: 0.305559.\n",
      "Training network. lr: 0.000094. clip: 0.037488\n",
      "Iteration 20362: Policy loss: 0.180822. Value loss: 0.179090. Entropy: 0.305121.\n",
      "Iteration 20363: Policy loss: 0.170348. Value loss: 0.084596. Entropy: 0.304133.\n",
      "Iteration 20364: Policy loss: 0.173626. Value loss: 0.052521. Entropy: 0.302982.\n",
      "episode: 7066   score: 800.0  epsilon: 1.0    steps: 456  evaluation reward: 406.65\n",
      "episode: 7067   score: 385.0  epsilon: 1.0    steps: 736  evaluation reward: 405.55\n",
      "Training network. lr: 0.000094. clip: 0.037488\n",
      "Iteration 20365: Policy loss: 0.245268. Value loss: 0.158176. Entropy: 0.300307.\n",
      "Iteration 20366: Policy loss: 0.243154. Value loss: 0.061285. Entropy: 0.298305.\n",
      "Iteration 20367: Policy loss: 0.243165. Value loss: 0.042807. Entropy: 0.298094.\n",
      "episode: 7068   score: 315.0  epsilon: 1.0    steps: 448  evaluation reward: 407.05\n",
      "episode: 7069   score: 590.0  epsilon: 1.0    steps: 840  evaluation reward: 410.35\n",
      "Training network. lr: 0.000094. clip: 0.037488\n",
      "Iteration 20368: Policy loss: -0.420029. Value loss: 0.467228. Entropy: 0.298584.\n",
      "Iteration 20369: Policy loss: -0.416096. Value loss: 0.204389. Entropy: 0.298986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20370: Policy loss: -0.430704. Value loss: 0.124533. Entropy: 0.298313.\n",
      "Training network. lr: 0.000094. clip: 0.037488\n",
      "Iteration 20371: Policy loss: -0.174034. Value loss: 0.133225. Entropy: 0.309704.\n",
      "Iteration 20372: Policy loss: -0.176155. Value loss: 0.062793. Entropy: 0.310299.\n",
      "Iteration 20373: Policy loss: -0.176857. Value loss: 0.046958. Entropy: 0.309689.\n",
      "Training network. lr: 0.000094. clip: 0.037488\n",
      "Iteration 20374: Policy loss: 0.084760. Value loss: 0.164787. Entropy: 0.304522.\n",
      "Iteration 20375: Policy loss: 0.081884. Value loss: 0.085316. Entropy: 0.303045.\n",
      "Iteration 20376: Policy loss: 0.076283. Value loss: 0.065678. Entropy: 0.303016.\n",
      "episode: 7070   score: 590.0  epsilon: 1.0    steps: 536  evaluation reward: 410.35\n",
      "episode: 7071   score: 520.0  epsilon: 1.0    steps: 712  evaluation reward: 411.3\n",
      "episode: 7072   score: 800.0  epsilon: 1.0    steps: 928  evaluation reward: 416.9\n",
      "Training network. lr: 0.000094. clip: 0.037488\n",
      "Iteration 20377: Policy loss: 0.117771. Value loss: 0.251469. Entropy: 0.294122.\n",
      "Iteration 20378: Policy loss: 0.114150. Value loss: 0.079615. Entropy: 0.295945.\n",
      "Iteration 20379: Policy loss: 0.100119. Value loss: 0.050276. Entropy: 0.296358.\n",
      "Training network. lr: 0.000094. clip: 0.037488\n",
      "Iteration 20380: Policy loss: 0.059549. Value loss: 0.136735. Entropy: 0.306360.\n",
      "Iteration 20381: Policy loss: 0.057800. Value loss: 0.065792. Entropy: 0.305369.\n",
      "Iteration 20382: Policy loss: 0.050056. Value loss: 0.047183. Entropy: 0.307143.\n",
      "episode: 7073   score: 695.0  epsilon: 1.0    steps: 32  evaluation reward: 420.7\n",
      "episode: 7074   score: 210.0  epsilon: 1.0    steps: 296  evaluation reward: 417.2\n",
      "Training network. lr: 0.000094. clip: 0.037488\n",
      "Iteration 20383: Policy loss: 0.089309. Value loss: 0.145893. Entropy: 0.296116.\n",
      "Iteration 20384: Policy loss: 0.088278. Value loss: 0.088073. Entropy: 0.297220.\n",
      "Iteration 20385: Policy loss: 0.080559. Value loss: 0.066623. Entropy: 0.296831.\n",
      "Training network. lr: 0.000094. clip: 0.037488\n",
      "Iteration 20386: Policy loss: -0.507965. Value loss: 0.375038. Entropy: 0.311693.\n",
      "Iteration 20387: Policy loss: -0.508117. Value loss: 0.172560. Entropy: 0.312217.\n",
      "Iteration 20388: Policy loss: -0.522657. Value loss: 0.121990. Entropy: 0.313039.\n",
      "episode: 7075   score: 285.0  epsilon: 1.0    steps: 16  evaluation reward: 416.7\n",
      "episode: 7076   score: 315.0  epsilon: 1.0    steps: 984  evaluation reward: 417.05\n",
      "Training network. lr: 0.000094. clip: 0.037488\n",
      "Iteration 20389: Policy loss: -0.114145. Value loss: 0.097436. Entropy: 0.292823.\n",
      "Iteration 20390: Policy loss: -0.115337. Value loss: 0.046192. Entropy: 0.291750.\n",
      "Iteration 20391: Policy loss: -0.114069. Value loss: 0.033071. Entropy: 0.292389.\n",
      "Training network. lr: 0.000094. clip: 0.037488\n",
      "Iteration 20392: Policy loss: -0.271887. Value loss: 0.273713. Entropy: 0.294424.\n",
      "Iteration 20393: Policy loss: -0.291986. Value loss: 0.107841. Entropy: 0.294730.\n",
      "Iteration 20394: Policy loss: -0.300667. Value loss: 0.071697. Entropy: 0.293514.\n",
      "Training network. lr: 0.000094. clip: 0.037488\n",
      "Iteration 20395: Policy loss: 0.428276. Value loss: 0.242372. Entropy: 0.299198.\n",
      "Iteration 20396: Policy loss: 0.428370. Value loss: 0.080015. Entropy: 0.299251.\n",
      "Iteration 20397: Policy loss: 0.424412. Value loss: 0.050829. Entropy: 0.297990.\n",
      "episode: 7077   score: 535.0  epsilon: 1.0    steps: 784  evaluation reward: 419.5\n",
      "Training network. lr: 0.000094. clip: 0.037488\n",
      "Iteration 20398: Policy loss: -0.184347. Value loss: 0.178015. Entropy: 0.296120.\n",
      "Iteration 20399: Policy loss: -0.173894. Value loss: 0.064793. Entropy: 0.295518.\n",
      "Iteration 20400: Policy loss: -0.195661. Value loss: 0.044550. Entropy: 0.295563.\n",
      "episode: 7078   score: 435.0  epsilon: 1.0    steps: 600  evaluation reward: 417.65\n",
      "episode: 7079   score: 530.0  epsilon: 1.0    steps: 792  evaluation reward: 418.2\n",
      "Training network. lr: 0.000093. clip: 0.037340\n",
      "Iteration 20401: Policy loss: 0.115620. Value loss: 0.279761. Entropy: 0.296316.\n",
      "Iteration 20402: Policy loss: 0.116105. Value loss: 0.092703. Entropy: 0.294731.\n",
      "Iteration 20403: Policy loss: 0.130730. Value loss: 0.046270. Entropy: 0.294965.\n",
      "episode: 7080   score: 465.0  epsilon: 1.0    steps: 936  evaluation reward: 418.7\n",
      "Training network. lr: 0.000093. clip: 0.037340\n",
      "Iteration 20404: Policy loss: 0.069420. Value loss: 0.197651. Entropy: 0.307963.\n",
      "Iteration 20405: Policy loss: 0.062453. Value loss: 0.102092. Entropy: 0.306804.\n",
      "Iteration 20406: Policy loss: 0.073518. Value loss: 0.070992. Entropy: 0.307464.\n",
      "episode: 7081   score: 565.0  epsilon: 1.0    steps: 920  evaluation reward: 422.25\n",
      "episode: 7082   score: 320.0  epsilon: 1.0    steps: 952  evaluation reward: 423.1\n",
      "Training network. lr: 0.000093. clip: 0.037340\n",
      "Iteration 20407: Policy loss: -0.005692. Value loss: 0.139213. Entropy: 0.292223.\n",
      "Iteration 20408: Policy loss: -0.018404. Value loss: 0.081302. Entropy: 0.292904.\n",
      "Iteration 20409: Policy loss: -0.016748. Value loss: 0.060229. Entropy: 0.293597.\n",
      "episode: 7083   score: 630.0  epsilon: 1.0    steps: 1008  evaluation reward: 423.8\n",
      "Training network. lr: 0.000093. clip: 0.037340\n",
      "Iteration 20410: Policy loss: 0.648956. Value loss: 0.294758. Entropy: 0.310668.\n",
      "Iteration 20411: Policy loss: 0.631251. Value loss: 0.110525. Entropy: 0.310830.\n",
      "Iteration 20412: Policy loss: 0.624940. Value loss: 0.065593. Entropy: 0.310362.\n",
      "Training network. lr: 0.000093. clip: 0.037340\n",
      "Iteration 20413: Policy loss: 0.083816. Value loss: 0.099087. Entropy: 0.299462.\n",
      "Iteration 20414: Policy loss: 0.077650. Value loss: 0.042603. Entropy: 0.300342.\n",
      "Iteration 20415: Policy loss: 0.081752. Value loss: 0.030929. Entropy: 0.300617.\n",
      "Training network. lr: 0.000093. clip: 0.037340\n",
      "Iteration 20416: Policy loss: 0.212000. Value loss: 0.130347. Entropy: 0.305406.\n",
      "Iteration 20417: Policy loss: 0.206333. Value loss: 0.043717. Entropy: 0.306803.\n",
      "Iteration 20418: Policy loss: 0.204051. Value loss: 0.030219. Entropy: 0.306132.\n",
      "Training network. lr: 0.000093. clip: 0.037340\n",
      "Iteration 20419: Policy loss: 0.277875. Value loss: 0.167736. Entropy: 0.308918.\n",
      "Iteration 20420: Policy loss: 0.277099. Value loss: 0.048071. Entropy: 0.307867.\n",
      "Iteration 20421: Policy loss: 0.282984. Value loss: 0.032723. Entropy: 0.307181.\n",
      "Training network. lr: 0.000093. clip: 0.037340\n",
      "Iteration 20422: Policy loss: 0.088571. Value loss: 0.395407. Entropy: 0.311092.\n",
      "Iteration 20423: Policy loss: 0.065204. Value loss: 0.228612. Entropy: 0.309358.\n",
      "Iteration 20424: Policy loss: 0.039515. Value loss: 0.186544. Entropy: 0.308424.\n",
      "episode: 7084   score: 665.0  epsilon: 1.0    steps: 584  evaluation reward: 422.25\n",
      "episode: 7085   score: 695.0  epsilon: 1.0    steps: 816  evaluation reward: 426.35\n",
      "Training network. lr: 0.000093. clip: 0.037340\n",
      "Iteration 20425: Policy loss: 0.220387. Value loss: 0.171850. Entropy: 0.301473.\n",
      "Iteration 20426: Policy loss: 0.221073. Value loss: 0.089409. Entropy: 0.301186.\n",
      "Iteration 20427: Policy loss: 0.213565. Value loss: 0.065544. Entropy: 0.299474.\n",
      "episode: 7086   score: 360.0  epsilon: 1.0    steps: 112  evaluation reward: 427.1\n",
      "episode: 7087   score: 210.0  epsilon: 1.0    steps: 304  evaluation reward: 423.8\n",
      "Training network. lr: 0.000093. clip: 0.037340\n",
      "Iteration 20428: Policy loss: 0.124061. Value loss: 0.103450. Entropy: 0.300226.\n",
      "Iteration 20429: Policy loss: 0.129280. Value loss: 0.057066. Entropy: 0.300329.\n",
      "Iteration 20430: Policy loss: 0.124645. Value loss: 0.047301. Entropy: 0.298978.\n",
      "episode: 7088   score: 400.0  epsilon: 1.0    steps: 88  evaluation reward: 424.2\n",
      "episode: 7089   score: 370.0  epsilon: 1.0    steps: 736  evaluation reward: 425.4\n",
      "Training network. lr: 0.000093. clip: 0.037340\n",
      "Iteration 20431: Policy loss: -0.120393. Value loss: 0.112786. Entropy: 0.308321.\n",
      "Iteration 20432: Policy loss: -0.127199. Value loss: 0.054188. Entropy: 0.307563.\n",
      "Iteration 20433: Policy loss: -0.124301. Value loss: 0.039929. Entropy: 0.308049.\n",
      "episode: 7090   score: 260.0  epsilon: 1.0    steps: 672  evaluation reward: 424.45\n",
      "episode: 7091   score: 340.0  epsilon: 1.0    steps: 896  evaluation reward: 423.1\n",
      "Training network. lr: 0.000093. clip: 0.037340\n",
      "Iteration 20434: Policy loss: 0.277669. Value loss: 0.124778. Entropy: 0.304408.\n",
      "Iteration 20435: Policy loss: 0.279991. Value loss: 0.059729. Entropy: 0.304182.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20436: Policy loss: 0.270699. Value loss: 0.046820. Entropy: 0.304118.\n",
      "Training network. lr: 0.000093. clip: 0.037340\n",
      "Iteration 20437: Policy loss: -0.403275. Value loss: 0.400211. Entropy: 0.302465.\n",
      "Iteration 20438: Policy loss: -0.405230. Value loss: 0.200785. Entropy: 0.303319.\n",
      "Iteration 20439: Policy loss: -0.403289. Value loss: 0.128062. Entropy: 0.303167.\n",
      "Training network. lr: 0.000093. clip: 0.037340\n",
      "Iteration 20440: Policy loss: 0.068743. Value loss: 0.121101. Entropy: 0.304359.\n",
      "Iteration 20441: Policy loss: 0.068482. Value loss: 0.057078. Entropy: 0.303012.\n",
      "Iteration 20442: Policy loss: 0.066639. Value loss: 0.039524. Entropy: 0.304057.\n",
      "Training network. lr: 0.000093. clip: 0.037340\n",
      "Iteration 20443: Policy loss: 0.118854. Value loss: 0.176935. Entropy: 0.308745.\n",
      "Iteration 20444: Policy loss: 0.122760. Value loss: 0.074774. Entropy: 0.308987.\n",
      "Iteration 20445: Policy loss: 0.112311. Value loss: 0.051794. Entropy: 0.308052.\n",
      "Training network. lr: 0.000093. clip: 0.037340\n",
      "Iteration 20446: Policy loss: -0.018019. Value loss: 0.216623. Entropy: 0.302544.\n",
      "Iteration 20447: Policy loss: -0.024439. Value loss: 0.108401. Entropy: 0.304243.\n",
      "Iteration 20448: Policy loss: -0.022938. Value loss: 0.074159. Entropy: 0.302713.\n",
      "episode: 7092   score: 620.0  epsilon: 1.0    steps: 832  evaluation reward: 426.7\n",
      "episode: 7093   score: 185.0  epsilon: 1.0    steps: 832  evaluation reward: 425.95\n",
      "Training network. lr: 0.000093. clip: 0.037340\n",
      "Iteration 20449: Policy loss: 0.366803. Value loss: 0.209427. Entropy: 0.301366.\n",
      "Iteration 20450: Policy loss: 0.362928. Value loss: 0.091555. Entropy: 0.301289.\n",
      "Iteration 20451: Policy loss: 0.356008. Value loss: 0.059543. Entropy: 0.299952.\n",
      "episode: 7094   score: 360.0  epsilon: 1.0    steps: 272  evaluation reward: 424.1\n",
      "episode: 7095   score: 390.0  epsilon: 1.0    steps: 288  evaluation reward: 426.45\n",
      "episode: 7096   score: 290.0  epsilon: 1.0    steps: 512  evaluation reward: 426.75\n",
      "Training network. lr: 0.000093. clip: 0.037184\n",
      "Iteration 20452: Policy loss: 0.084126. Value loss: 0.101082. Entropy: 0.296797.\n",
      "Iteration 20453: Policy loss: 0.077551. Value loss: 0.046470. Entropy: 0.293028.\n",
      "Iteration 20454: Policy loss: 0.076722. Value loss: 0.036007. Entropy: 0.294106.\n",
      "Training network. lr: 0.000093. clip: 0.037184\n",
      "Iteration 20455: Policy loss: -0.023543. Value loss: 0.114445. Entropy: 0.303787.\n",
      "Iteration 20456: Policy loss: -0.030191. Value loss: 0.058795. Entropy: 0.304667.\n",
      "Iteration 20457: Policy loss: -0.031213. Value loss: 0.044825. Entropy: 0.303832.\n",
      "episode: 7097   score: 390.0  epsilon: 1.0    steps: 888  evaluation reward: 424.75\n",
      "Training network. lr: 0.000093. clip: 0.037184\n",
      "Iteration 20458: Policy loss: -0.153502. Value loss: 0.362176. Entropy: 0.298685.\n",
      "Iteration 20459: Policy loss: -0.151412. Value loss: 0.241929. Entropy: 0.296409.\n",
      "Iteration 20460: Policy loss: -0.159004. Value loss: 0.183254. Entropy: 0.297538.\n",
      "episode: 7098   score: 815.0  epsilon: 1.0    steps: 488  evaluation reward: 429.75\n",
      "episode: 7099   score: 585.0  epsilon: 1.0    steps: 720  evaluation reward: 430.65\n",
      "Training network. lr: 0.000093. clip: 0.037184\n",
      "Iteration 20461: Policy loss: 0.122488. Value loss: 0.097878. Entropy: 0.292090.\n",
      "Iteration 20462: Policy loss: 0.121627. Value loss: 0.039475. Entropy: 0.287352.\n",
      "Iteration 20463: Policy loss: 0.115195. Value loss: 0.028901. Entropy: 0.288493.\n",
      "Training network. lr: 0.000093. clip: 0.037184\n",
      "Iteration 20464: Policy loss: 0.056575. Value loss: 0.102797. Entropy: 0.310936.\n",
      "Iteration 20465: Policy loss: 0.051073. Value loss: 0.029965. Entropy: 0.311638.\n",
      "Iteration 20466: Policy loss: 0.045079. Value loss: 0.020133. Entropy: 0.311608.\n",
      "Training network. lr: 0.000093. clip: 0.037184\n",
      "Iteration 20467: Policy loss: -0.153142. Value loss: 0.334069. Entropy: 0.309250.\n",
      "Iteration 20468: Policy loss: -0.157568. Value loss: 0.225731. Entropy: 0.308964.\n",
      "Iteration 20469: Policy loss: -0.164812. Value loss: 0.183560. Entropy: 0.308108.\n",
      "episode: 7100   score: 190.0  epsilon: 1.0    steps: 600  evaluation reward: 424.75\n",
      "Training network. lr: 0.000093. clip: 0.037184\n",
      "Iteration 20470: Policy loss: -0.545443. Value loss: 0.476116. Entropy: 0.298380.\n",
      "Iteration 20471: Policy loss: -0.534997. Value loss: 0.212203. Entropy: 0.296873.\n",
      "Iteration 20472: Policy loss: -0.552647. Value loss: 0.140379. Entropy: 0.297424.\n",
      "now time :  2019-09-06 11:22:19.055178\n",
      "episode: 7101   score: 250.0  epsilon: 1.0    steps: 72  evaluation reward: 421.95\n",
      "Training network. lr: 0.000093. clip: 0.037184\n",
      "Iteration 20473: Policy loss: 0.170387. Value loss: 0.113818. Entropy: 0.304404.\n",
      "Iteration 20474: Policy loss: 0.170885. Value loss: 0.046601. Entropy: 0.302744.\n",
      "Iteration 20475: Policy loss: 0.158187. Value loss: 0.028629. Entropy: 0.302296.\n",
      "episode: 7102   score: 470.0  epsilon: 1.0    steps: 496  evaluation reward: 422.0\n",
      "Training network. lr: 0.000093. clip: 0.037184\n",
      "Iteration 20476: Policy loss: -0.099195. Value loss: 0.104157. Entropy: 0.294645.\n",
      "Iteration 20477: Policy loss: -0.095807. Value loss: 0.036977. Entropy: 0.292697.\n",
      "Iteration 20478: Policy loss: -0.103187. Value loss: 0.026478. Entropy: 0.292955.\n",
      "episode: 7103   score: 380.0  epsilon: 1.0    steps: 384  evaluation reward: 422.3\n",
      "Training network. lr: 0.000093. clip: 0.037184\n",
      "Iteration 20479: Policy loss: 0.067508. Value loss: 0.181616. Entropy: 0.302200.\n",
      "Iteration 20480: Policy loss: 0.046232. Value loss: 0.051669. Entropy: 0.303812.\n",
      "Iteration 20481: Policy loss: 0.052904. Value loss: 0.034562. Entropy: 0.303230.\n",
      "episode: 7104   score: 420.0  epsilon: 1.0    steps: 1008  evaluation reward: 422.15\n",
      "Training network. lr: 0.000093. clip: 0.037184\n",
      "Iteration 20482: Policy loss: 0.308650. Value loss: 0.230064. Entropy: 0.303589.\n",
      "Iteration 20483: Policy loss: 0.298799. Value loss: 0.082065. Entropy: 0.302109.\n",
      "Iteration 20484: Policy loss: 0.292124. Value loss: 0.051548. Entropy: 0.302247.\n",
      "episode: 7105   score: 485.0  epsilon: 1.0    steps: 8  evaluation reward: 422.1\n",
      "Training network. lr: 0.000093. clip: 0.037184\n",
      "Iteration 20485: Policy loss: 0.109241. Value loss: 0.094522. Entropy: 0.296141.\n",
      "Iteration 20486: Policy loss: 0.112824. Value loss: 0.044850. Entropy: 0.296701.\n",
      "Iteration 20487: Policy loss: 0.108336. Value loss: 0.031048. Entropy: 0.297883.\n",
      "Training network. lr: 0.000093. clip: 0.037184\n",
      "Iteration 20488: Policy loss: 0.030485. Value loss: 0.101630. Entropy: 0.303211.\n",
      "Iteration 20489: Policy loss: 0.028394. Value loss: 0.048308. Entropy: 0.301538.\n",
      "Iteration 20490: Policy loss: 0.018549. Value loss: 0.035299. Entropy: 0.302523.\n",
      "episode: 7106   score: 555.0  epsilon: 1.0    steps: 1024  evaluation reward: 422.9\n",
      "Training network. lr: 0.000093. clip: 0.037184\n",
      "Iteration 20491: Policy loss: 0.037326. Value loss: 0.195270. Entropy: 0.306544.\n",
      "Iteration 20492: Policy loss: 0.014870. Value loss: 0.065860. Entropy: 0.307009.\n",
      "Iteration 20493: Policy loss: 0.029065. Value loss: 0.037691. Entropy: 0.306629.\n",
      "Training network. lr: 0.000093. clip: 0.037184\n",
      "Iteration 20494: Policy loss: 0.159582. Value loss: 0.154341. Entropy: 0.299296.\n",
      "Iteration 20495: Policy loss: 0.152203. Value loss: 0.061858. Entropy: 0.300989.\n",
      "Iteration 20496: Policy loss: 0.146222. Value loss: 0.042022. Entropy: 0.301144.\n",
      "Training network. lr: 0.000093. clip: 0.037184\n",
      "Iteration 20497: Policy loss: -0.066238. Value loss: 0.118077. Entropy: 0.304744.\n",
      "Iteration 20498: Policy loss: -0.068658. Value loss: 0.046019. Entropy: 0.305536.\n",
      "Iteration 20499: Policy loss: -0.073945. Value loss: 0.031111. Entropy: 0.305086.\n",
      "Training network. lr: 0.000093. clip: 0.037184\n",
      "Iteration 20500: Policy loss: -0.246211. Value loss: 0.102568. Entropy: 0.305977.\n",
      "Iteration 20501: Policy loss: -0.244130. Value loss: 0.048401. Entropy: 0.305250.\n",
      "Iteration 20502: Policy loss: -0.237307. Value loss: 0.032402. Entropy: 0.305403.\n",
      "episode: 7107   score: 760.0  epsilon: 1.0    steps: 816  evaluation reward: 423.8\n",
      "episode: 7108   score: 520.0  epsilon: 1.0    steps: 1024  evaluation reward: 424.65\n",
      "Training network. lr: 0.000093. clip: 0.037027\n",
      "Iteration 20503: Policy loss: 0.121931. Value loss: 0.407171. Entropy: 0.308397.\n",
      "Iteration 20504: Policy loss: 0.110016. Value loss: 0.244727. Entropy: 0.306846.\n",
      "Iteration 20505: Policy loss: 0.104008. Value loss: 0.177386. Entropy: 0.306337.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7109   score: 435.0  epsilon: 1.0    steps: 632  evaluation reward: 425.55\n",
      "episode: 7110   score: 500.0  epsilon: 1.0    steps: 872  evaluation reward: 424.9\n",
      "episode: 7111   score: 390.0  epsilon: 1.0    steps: 880  evaluation reward: 424.6\n",
      "Training network. lr: 0.000093. clip: 0.037027\n",
      "Iteration 20506: Policy loss: -0.031915. Value loss: 0.108419. Entropy: 0.284876.\n",
      "Iteration 20507: Policy loss: -0.039020. Value loss: 0.058129. Entropy: 0.285303.\n",
      "Iteration 20508: Policy loss: -0.039789. Value loss: 0.044790. Entropy: 0.283434.\n",
      "Training network. lr: 0.000093. clip: 0.037027\n",
      "Iteration 20509: Policy loss: -0.064276. Value loss: 0.107360. Entropy: 0.311987.\n",
      "Iteration 20510: Policy loss: -0.066686. Value loss: 0.042971. Entropy: 0.312539.\n",
      "Iteration 20511: Policy loss: -0.069780. Value loss: 0.027934. Entropy: 0.310694.\n",
      "episode: 7112   score: 500.0  epsilon: 1.0    steps: 320  evaluation reward: 426.85\n",
      "Training network. lr: 0.000093. clip: 0.037027\n",
      "Iteration 20512: Policy loss: 0.124137. Value loss: 0.127445. Entropy: 0.304344.\n",
      "Iteration 20513: Policy loss: 0.123083. Value loss: 0.067514. Entropy: 0.302805.\n",
      "Iteration 20514: Policy loss: 0.118187. Value loss: 0.051291. Entropy: 0.302687.\n",
      "episode: 7113   score: 470.0  epsilon: 1.0    steps: 568  evaluation reward: 426.85\n",
      "episode: 7114   score: 470.0  epsilon: 1.0    steps: 984  evaluation reward: 429.4\n",
      "Training network. lr: 0.000093. clip: 0.037027\n",
      "Iteration 20515: Policy loss: 0.641323. Value loss: 0.223559. Entropy: 0.303793.\n",
      "Iteration 20516: Policy loss: 0.627308. Value loss: 0.068276. Entropy: 0.302407.\n",
      "Iteration 20517: Policy loss: 0.632007. Value loss: 0.044594. Entropy: 0.302016.\n",
      "Training network. lr: 0.000093. clip: 0.037027\n",
      "Iteration 20518: Policy loss: 0.198759. Value loss: 0.102135. Entropy: 0.306747.\n",
      "Iteration 20519: Policy loss: 0.199078. Value loss: 0.055630. Entropy: 0.307401.\n",
      "Iteration 20520: Policy loss: 0.195272. Value loss: 0.043197. Entropy: 0.306147.\n",
      "Training network. lr: 0.000093. clip: 0.037027\n",
      "Iteration 20521: Policy loss: 0.253788. Value loss: 0.168873. Entropy: 0.309729.\n",
      "Iteration 20522: Policy loss: 0.246166. Value loss: 0.063100. Entropy: 0.309237.\n",
      "Iteration 20523: Policy loss: 0.242492. Value loss: 0.039960. Entropy: 0.309290.\n",
      "episode: 7115   score: 295.0  epsilon: 1.0    steps: 760  evaluation reward: 429.5\n",
      "Training network. lr: 0.000093. clip: 0.037027\n",
      "Iteration 20524: Policy loss: 0.081055. Value loss: 0.125366. Entropy: 0.302480.\n",
      "Iteration 20525: Policy loss: 0.076173. Value loss: 0.053863. Entropy: 0.302668.\n",
      "Iteration 20526: Policy loss: 0.072972. Value loss: 0.039139. Entropy: 0.301339.\n",
      "episode: 7116   score: 290.0  epsilon: 1.0    steps: 552  evaluation reward: 429.5\n",
      "Training network. lr: 0.000093. clip: 0.037027\n",
      "Iteration 20527: Policy loss: 0.042137. Value loss: 0.110058. Entropy: 0.303985.\n",
      "Iteration 20528: Policy loss: 0.035183. Value loss: 0.036138. Entropy: 0.304045.\n",
      "Iteration 20529: Policy loss: 0.029642. Value loss: 0.027077. Entropy: 0.305319.\n",
      "episode: 7117   score: 260.0  epsilon: 1.0    steps: 48  evaluation reward: 429.8\n",
      "Training network. lr: 0.000093. clip: 0.037027\n",
      "Iteration 20530: Policy loss: 0.024358. Value loss: 0.081157. Entropy: 0.306387.\n",
      "Iteration 20531: Policy loss: 0.014227. Value loss: 0.033475. Entropy: 0.306686.\n",
      "Iteration 20532: Policy loss: 0.012289. Value loss: 0.024245. Entropy: 0.304827.\n",
      "episode: 7118   score: 365.0  epsilon: 1.0    steps: 624  evaluation reward: 431.6\n",
      "Training network. lr: 0.000093. clip: 0.037027\n",
      "Iteration 20533: Policy loss: -0.052773. Value loss: 0.112412. Entropy: 0.300883.\n",
      "Iteration 20534: Policy loss: -0.056805. Value loss: 0.046345. Entropy: 0.301535.\n",
      "Iteration 20535: Policy loss: -0.057804. Value loss: 0.032542. Entropy: 0.300210.\n",
      "Training network. lr: 0.000093. clip: 0.037027\n",
      "Iteration 20536: Policy loss: -0.246455. Value loss: 0.285099. Entropy: 0.305773.\n",
      "Iteration 20537: Policy loss: -0.248554. Value loss: 0.113003. Entropy: 0.307268.\n",
      "Iteration 20538: Policy loss: -0.250638. Value loss: 0.048385. Entropy: 0.305698.\n",
      "episode: 7119   score: 395.0  epsilon: 1.0    steps: 536  evaluation reward: 431.85\n",
      "episode: 7120   score: 495.0  epsilon: 1.0    steps: 896  evaluation reward: 433.8\n",
      "Training network. lr: 0.000093. clip: 0.037027\n",
      "Iteration 20539: Policy loss: -0.031005. Value loss: 0.123841. Entropy: 0.298756.\n",
      "Iteration 20540: Policy loss: -0.045095. Value loss: 0.050862. Entropy: 0.299445.\n",
      "Iteration 20541: Policy loss: -0.037914. Value loss: 0.035068. Entropy: 0.300312.\n",
      "episode: 7121   score: 525.0  epsilon: 1.0    steps: 160  evaluation reward: 434.1\n",
      "Training network. lr: 0.000093. clip: 0.037027\n",
      "Iteration 20542: Policy loss: 0.066628. Value loss: 0.157326. Entropy: 0.302752.\n",
      "Iteration 20543: Policy loss: 0.068937. Value loss: 0.050514. Entropy: 0.300993.\n",
      "Iteration 20544: Policy loss: 0.068223. Value loss: 0.034771. Entropy: 0.302768.\n",
      "episode: 7122   score: 400.0  epsilon: 1.0    steps: 480  evaluation reward: 434.2\n",
      "Training network. lr: 0.000093. clip: 0.037027\n",
      "Iteration 20545: Policy loss: 0.077208. Value loss: 0.141091. Entropy: 0.304467.\n",
      "Iteration 20546: Policy loss: 0.071888. Value loss: 0.064920. Entropy: 0.304660.\n",
      "Iteration 20547: Policy loss: 0.068527. Value loss: 0.041095. Entropy: 0.305149.\n",
      "episode: 7123   score: 505.0  epsilon: 1.0    steps: 304  evaluation reward: 436.7\n",
      "Training network. lr: 0.000093. clip: 0.037027\n",
      "Iteration 20548: Policy loss: 0.183817. Value loss: 0.102065. Entropy: 0.309668.\n",
      "Iteration 20549: Policy loss: 0.177781. Value loss: 0.041117. Entropy: 0.308811.\n",
      "Iteration 20550: Policy loss: 0.179752. Value loss: 0.034763. Entropy: 0.308938.\n",
      "Training network. lr: 0.000092. clip: 0.036880\n",
      "Iteration 20551: Policy loss: 0.023958. Value loss: 0.068595. Entropy: 0.312851.\n",
      "Iteration 20552: Policy loss: 0.025061. Value loss: 0.028975. Entropy: 0.312641.\n",
      "Iteration 20553: Policy loss: 0.025172. Value loss: 0.022097. Entropy: 0.312404.\n",
      "episode: 7124   score: 335.0  epsilon: 1.0    steps: 728  evaluation reward: 438.5\n",
      "Training network. lr: 0.000092. clip: 0.036880\n",
      "Iteration 20554: Policy loss: 0.196022. Value loss: 0.106808. Entropy: 0.307057.\n",
      "Iteration 20555: Policy loss: 0.195769. Value loss: 0.046863. Entropy: 0.306484.\n",
      "Iteration 20556: Policy loss: 0.187386. Value loss: 0.039463. Entropy: 0.306365.\n",
      "episode: 7125   score: 240.0  epsilon: 1.0    steps: 488  evaluation reward: 435.6\n",
      "Training network. lr: 0.000092. clip: 0.036880\n",
      "Iteration 20557: Policy loss: -0.038377. Value loss: 0.081851. Entropy: 0.297887.\n",
      "Iteration 20558: Policy loss: -0.044520. Value loss: 0.029721. Entropy: 0.294894.\n",
      "Iteration 20559: Policy loss: -0.045642. Value loss: 0.022276. Entropy: 0.296008.\n",
      "Training network. lr: 0.000092. clip: 0.036880\n",
      "Iteration 20560: Policy loss: 0.039295. Value loss: 0.124505. Entropy: 0.317209.\n",
      "Iteration 20561: Policy loss: 0.040608. Value loss: 0.055736. Entropy: 0.316486.\n",
      "Iteration 20562: Policy loss: 0.034998. Value loss: 0.038606. Entropy: 0.316268.\n",
      "episode: 7126   score: 295.0  epsilon: 1.0    steps: 440  evaluation reward: 433.65\n",
      "episode: 7127   score: 495.0  epsilon: 1.0    steps: 832  evaluation reward: 434.45\n",
      "Training network. lr: 0.000092. clip: 0.036880\n",
      "Iteration 20563: Policy loss: 0.087013. Value loss: 0.095622. Entropy: 0.301932.\n",
      "Iteration 20564: Policy loss: 0.085547. Value loss: 0.041901. Entropy: 0.299984.\n",
      "Iteration 20565: Policy loss: 0.081198. Value loss: 0.029040. Entropy: 0.299991.\n",
      "Training network. lr: 0.000092. clip: 0.036880\n",
      "Iteration 20566: Policy loss: 0.083015. Value loss: 0.131896. Entropy: 0.303484.\n",
      "Iteration 20567: Policy loss: 0.073987. Value loss: 0.056940. Entropy: 0.304448.\n",
      "Iteration 20568: Policy loss: 0.074744. Value loss: 0.036411. Entropy: 0.304259.\n",
      "episode: 7128   score: 420.0  epsilon: 1.0    steps: 24  evaluation reward: 434.65\n",
      "Training network. lr: 0.000092. clip: 0.036880\n",
      "Iteration 20569: Policy loss: 0.098086. Value loss: 0.093200. Entropy: 0.304768.\n",
      "Iteration 20570: Policy loss: 0.089043. Value loss: 0.031092. Entropy: 0.303694.\n",
      "Iteration 20571: Policy loss: 0.089543. Value loss: 0.023509. Entropy: 0.305281.\n",
      "episode: 7129   score: 420.0  epsilon: 1.0    steps: 744  evaluation reward: 435.65\n",
      "Training network. lr: 0.000092. clip: 0.036880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20572: Policy loss: 0.147771. Value loss: 0.120342. Entropy: 0.301584.\n",
      "Iteration 20573: Policy loss: 0.151026. Value loss: 0.043727. Entropy: 0.300784.\n",
      "Iteration 20574: Policy loss: 0.137263. Value loss: 0.030900. Entropy: 0.301122.\n",
      "episode: 7130   score: 120.0  epsilon: 1.0    steps: 760  evaluation reward: 432.95\n",
      "Training network. lr: 0.000092. clip: 0.036880\n",
      "Iteration 20575: Policy loss: -0.111838. Value loss: 0.377716. Entropy: 0.306511.\n",
      "Iteration 20576: Policy loss: -0.122438. Value loss: 0.232583. Entropy: 0.303249.\n",
      "Iteration 20577: Policy loss: -0.133312. Value loss: 0.182737. Entropy: 0.302538.\n",
      "episode: 7131   score: 345.0  epsilon: 1.0    steps: 224  evaluation reward: 432.45\n",
      "Training network. lr: 0.000092. clip: 0.036880\n",
      "Iteration 20578: Policy loss: -0.004705. Value loss: 0.130478. Entropy: 0.303050.\n",
      "Iteration 20579: Policy loss: -0.013672. Value loss: 0.063903. Entropy: 0.302442.\n",
      "Iteration 20580: Policy loss: -0.007586. Value loss: 0.048999. Entropy: 0.302809.\n",
      "episode: 7132   score: 630.0  epsilon: 1.0    steps: 96  evaluation reward: 435.6\n",
      "episode: 7133   score: 525.0  epsilon: 1.0    steps: 584  evaluation reward: 436.65\n",
      "Training network. lr: 0.000092. clip: 0.036880\n",
      "Iteration 20581: Policy loss: -0.634203. Value loss: 0.410942. Entropy: 0.302980.\n",
      "Iteration 20582: Policy loss: -0.634323. Value loss: 0.267693. Entropy: 0.301834.\n",
      "Iteration 20583: Policy loss: -0.636582. Value loss: 0.209193. Entropy: 0.300695.\n",
      "Training network. lr: 0.000092. clip: 0.036880\n",
      "Iteration 20584: Policy loss: -0.282982. Value loss: 0.321509. Entropy: 0.313685.\n",
      "Iteration 20585: Policy loss: -0.302883. Value loss: 0.232771. Entropy: 0.313354.\n",
      "Iteration 20586: Policy loss: -0.304756. Value loss: 0.175094. Entropy: 0.313096.\n",
      "Training network. lr: 0.000092. clip: 0.036880\n",
      "Iteration 20587: Policy loss: -0.209770. Value loss: 0.195948. Entropy: 0.296110.\n",
      "Iteration 20588: Policy loss: -0.217983. Value loss: 0.078818. Entropy: 0.299319.\n",
      "Iteration 20589: Policy loss: -0.222330. Value loss: 0.044573. Entropy: 0.300538.\n",
      "episode: 7134   score: 525.0  epsilon: 1.0    steps: 64  evaluation reward: 439.3\n",
      "Training network. lr: 0.000092. clip: 0.036880\n",
      "Iteration 20590: Policy loss: -0.176009. Value loss: 0.238216. Entropy: 0.294573.\n",
      "Iteration 20591: Policy loss: -0.204539. Value loss: 0.140184. Entropy: 0.292724.\n",
      "Iteration 20592: Policy loss: -0.204746. Value loss: 0.093764. Entropy: 0.293194.\n",
      "Training network. lr: 0.000092. clip: 0.036880\n",
      "Iteration 20593: Policy loss: -0.059242. Value loss: 0.139156. Entropy: 0.306502.\n",
      "Iteration 20594: Policy loss: -0.071321. Value loss: 0.050018. Entropy: 0.305763.\n",
      "Iteration 20595: Policy loss: -0.081605. Value loss: 0.037305. Entropy: 0.305819.\n",
      "episode: 7135   score: 620.0  epsilon: 1.0    steps: 56  evaluation reward: 439.3\n",
      "episode: 7136   score: 470.0  epsilon: 1.0    steps: 480  evaluation reward: 441.3\n",
      "episode: 7137   score: 555.0  epsilon: 1.0    steps: 768  evaluation reward: 443.7\n",
      "Training network. lr: 0.000092. clip: 0.036880\n",
      "Iteration 20596: Policy loss: -0.182529. Value loss: 0.344955. Entropy: 0.281506.\n",
      "Iteration 20597: Policy loss: -0.189507. Value loss: 0.156756. Entropy: 0.281256.\n",
      "Iteration 20598: Policy loss: -0.192911. Value loss: 0.110335. Entropy: 0.281158.\n",
      "episode: 7138   score: 370.0  epsilon: 1.0    steps: 608  evaluation reward: 443.65\n",
      "Training network. lr: 0.000092. clip: 0.036880\n",
      "Iteration 20599: Policy loss: 0.227509. Value loss: 0.162814. Entropy: 0.303705.\n",
      "Iteration 20600: Policy loss: 0.219698. Value loss: 0.069493. Entropy: 0.301656.\n",
      "Iteration 20601: Policy loss: 0.224172. Value loss: 0.048188. Entropy: 0.302422.\n",
      "episode: 7139   score: 525.0  epsilon: 1.0    steps: 240  evaluation reward: 445.75\n",
      "episode: 7140   score: 530.0  epsilon: 1.0    steps: 448  evaluation reward: 444.45\n",
      "Training network. lr: 0.000092. clip: 0.036723\n",
      "Iteration 20602: Policy loss: 0.290846. Value loss: 0.158559. Entropy: 0.294267.\n",
      "Iteration 20603: Policy loss: 0.285771. Value loss: 0.086859. Entropy: 0.294932.\n",
      "Iteration 20604: Policy loss: 0.285668. Value loss: 0.062158. Entropy: 0.295876.\n",
      "episode: 7141   score: 415.0  epsilon: 1.0    steps: 544  evaluation reward: 444.15\n",
      "Training network. lr: 0.000092. clip: 0.036723\n",
      "Iteration 20605: Policy loss: 0.298762. Value loss: 0.135327. Entropy: 0.306678.\n",
      "Iteration 20606: Policy loss: 0.307386. Value loss: 0.056065. Entropy: 0.305901.\n",
      "Iteration 20607: Policy loss: 0.288925. Value loss: 0.042649. Entropy: 0.305308.\n",
      "episode: 7142   score: 150.0  epsilon: 1.0    steps: 784  evaluation reward: 440.05\n",
      "episode: 7143   score: 345.0  epsilon: 1.0    steps: 864  evaluation reward: 437.3\n",
      "Training network. lr: 0.000092. clip: 0.036723\n",
      "Iteration 20608: Policy loss: 0.005830. Value loss: 0.131419. Entropy: 0.301256.\n",
      "Iteration 20609: Policy loss: 0.006332. Value loss: 0.064185. Entropy: 0.301752.\n",
      "Iteration 20610: Policy loss: -0.001479. Value loss: 0.045409. Entropy: 0.300836.\n",
      "Training network. lr: 0.000092. clip: 0.036723\n",
      "Iteration 20611: Policy loss: -0.546760. Value loss: 0.394924. Entropy: 0.304304.\n",
      "Iteration 20612: Policy loss: -0.535803. Value loss: 0.132951. Entropy: 0.305801.\n",
      "Iteration 20613: Policy loss: -0.566113. Value loss: 0.083499. Entropy: 0.306995.\n",
      "episode: 7144   score: 215.0  epsilon: 1.0    steps: 552  evaluation reward: 435.4\n",
      "episode: 7145   score: 510.0  epsilon: 1.0    steps: 976  evaluation reward: 434.75\n",
      "Training network. lr: 0.000092. clip: 0.036723\n",
      "Iteration 20614: Policy loss: 0.390828. Value loss: 0.268234. Entropy: 0.303982.\n",
      "Iteration 20615: Policy loss: 0.382390. Value loss: 0.094234. Entropy: 0.302221.\n",
      "Iteration 20616: Policy loss: 0.369882. Value loss: 0.069802. Entropy: 0.303714.\n",
      "Training network. lr: 0.000092. clip: 0.036723\n",
      "Iteration 20617: Policy loss: -0.280866. Value loss: 0.269426. Entropy: 0.307177.\n",
      "Iteration 20618: Policy loss: -0.296780. Value loss: 0.155442. Entropy: 0.307422.\n",
      "Iteration 20619: Policy loss: -0.293658. Value loss: 0.094626. Entropy: 0.305573.\n",
      "Training network. lr: 0.000092. clip: 0.036723\n",
      "Iteration 20620: Policy loss: -0.000524. Value loss: 0.173104. Entropy: 0.315515.\n",
      "Iteration 20621: Policy loss: -0.011870. Value loss: 0.081079. Entropy: 0.314549.\n",
      "Iteration 20622: Policy loss: -0.013825. Value loss: 0.052944. Entropy: 0.314730.\n",
      "episode: 7146   score: 365.0  epsilon: 1.0    steps: 664  evaluation reward: 435.25\n",
      "episode: 7147   score: 560.0  epsilon: 1.0    steps: 952  evaluation reward: 438.35\n",
      "Training network. lr: 0.000092. clip: 0.036723\n",
      "Iteration 20623: Policy loss: 0.320552. Value loss: 0.166922. Entropy: 0.300568.\n",
      "Iteration 20624: Policy loss: 0.326298. Value loss: 0.058224. Entropy: 0.301830.\n",
      "Iteration 20625: Policy loss: 0.310688. Value loss: 0.035691. Entropy: 0.300855.\n",
      "episode: 7148   score: 560.0  epsilon: 1.0    steps: 304  evaluation reward: 442.55\n",
      "Training network. lr: 0.000092. clip: 0.036723\n",
      "Iteration 20626: Policy loss: 0.006092. Value loss: 0.189631. Entropy: 0.301489.\n",
      "Iteration 20627: Policy loss: -0.002705. Value loss: 0.073087. Entropy: 0.302757.\n",
      "Iteration 20628: Policy loss: -0.002898. Value loss: 0.051864. Entropy: 0.302149.\n",
      "episode: 7149   score: 435.0  epsilon: 1.0    steps: 48  evaluation reward: 444.8\n",
      "Training network. lr: 0.000092. clip: 0.036723\n",
      "Iteration 20629: Policy loss: -0.255556. Value loss: 0.360951. Entropy: 0.308661.\n",
      "Iteration 20630: Policy loss: -0.256246. Value loss: 0.157093. Entropy: 0.307916.\n",
      "Iteration 20631: Policy loss: -0.252665. Value loss: 0.094851. Entropy: 0.308534.\n",
      "Training network. lr: 0.000092. clip: 0.036723\n",
      "Iteration 20632: Policy loss: 0.125958. Value loss: 0.181123. Entropy: 0.302289.\n",
      "Iteration 20633: Policy loss: 0.129192. Value loss: 0.081957. Entropy: 0.303348.\n",
      "Iteration 20634: Policy loss: 0.121723. Value loss: 0.062552. Entropy: 0.303234.\n",
      "episode: 7150   score: 390.0  epsilon: 1.0    steps: 264  evaluation reward: 445.85\n",
      "Training network. lr: 0.000092. clip: 0.036723\n",
      "Iteration 20635: Policy loss: 0.007345. Value loss: 0.125146. Entropy: 0.296984.\n",
      "Iteration 20636: Policy loss: -0.007460. Value loss: 0.045568. Entropy: 0.294477.\n",
      "Iteration 20637: Policy loss: -0.001043. Value loss: 0.030934. Entropy: 0.296318.\n",
      "now time :  2019-09-06 11:32:21.809456\n",
      "episode: 7151   score: 565.0  epsilon: 1.0    steps: 168  evaluation reward: 447.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000092. clip: 0.036723\n",
      "Iteration 20638: Policy loss: 0.119927. Value loss: 0.162075. Entropy: 0.306804.\n",
      "Iteration 20639: Policy loss: 0.123503. Value loss: 0.081479. Entropy: 0.304634.\n",
      "Iteration 20640: Policy loss: 0.116734. Value loss: 0.060159. Entropy: 0.304460.\n",
      "Training network. lr: 0.000092. clip: 0.036723\n",
      "Iteration 20641: Policy loss: 0.460063. Value loss: 0.221492. Entropy: 0.310864.\n",
      "Iteration 20642: Policy loss: 0.438280. Value loss: 0.075351. Entropy: 0.310887.\n",
      "Iteration 20643: Policy loss: 0.444570. Value loss: 0.051961. Entropy: 0.308888.\n",
      "episode: 7152   score: 300.0  epsilon: 1.0    steps: 376  evaluation reward: 444.6\n",
      "episode: 7153   score: 760.0  epsilon: 1.0    steps: 576  evaluation reward: 447.15\n",
      "Training network. lr: 0.000092. clip: 0.036723\n",
      "Iteration 20644: Policy loss: 0.269905. Value loss: 0.194396. Entropy: 0.289603.\n",
      "Iteration 20645: Policy loss: 0.262511. Value loss: 0.073210. Entropy: 0.288059.\n",
      "Iteration 20646: Policy loss: 0.260849. Value loss: 0.047039. Entropy: 0.287417.\n",
      "episode: 7154   score: 270.0  epsilon: 1.0    steps: 176  evaluation reward: 447.45\n",
      "Training network. lr: 0.000092. clip: 0.036723\n",
      "Iteration 20647: Policy loss: 0.182133. Value loss: 0.118374. Entropy: 0.304922.\n",
      "Iteration 20648: Policy loss: 0.179022. Value loss: 0.058580. Entropy: 0.304439.\n",
      "Iteration 20649: Policy loss: 0.174808. Value loss: 0.047470. Entropy: 0.303673.\n",
      "episode: 7155   score: 440.0  epsilon: 1.0    steps: 592  evaluation reward: 445.05\n",
      "Training network. lr: 0.000092. clip: 0.036723\n",
      "Iteration 20650: Policy loss: -0.227855. Value loss: 0.145789. Entropy: 0.298716.\n",
      "Iteration 20651: Policy loss: -0.224818. Value loss: 0.064126. Entropy: 0.298332.\n",
      "Iteration 20652: Policy loss: -0.226050. Value loss: 0.045681. Entropy: 0.299086.\n",
      "Training network. lr: 0.000091. clip: 0.036566\n",
      "Iteration 20653: Policy loss: 0.381093. Value loss: 0.236068. Entropy: 0.306247.\n",
      "Iteration 20654: Policy loss: 0.383246. Value loss: 0.071707. Entropy: 0.305577.\n",
      "Iteration 20655: Policy loss: 0.387945. Value loss: 0.046837. Entropy: 0.304767.\n",
      "episode: 7156   score: 390.0  epsilon: 1.0    steps: 624  evaluation reward: 443.8\n",
      "Training network. lr: 0.000091. clip: 0.036566\n",
      "Iteration 20656: Policy loss: -0.499437. Value loss: 0.347748. Entropy: 0.299366.\n",
      "Iteration 20657: Policy loss: -0.503544. Value loss: 0.196865. Entropy: 0.300157.\n",
      "Iteration 20658: Policy loss: -0.517280. Value loss: 0.132873. Entropy: 0.300473.\n",
      "episode: 7157   score: 365.0  epsilon: 1.0    steps: 88  evaluation reward: 444.15\n",
      "Training network. lr: 0.000091. clip: 0.036566\n",
      "Iteration 20659: Policy loss: -0.184219. Value loss: 0.246959. Entropy: 0.300471.\n",
      "Iteration 20660: Policy loss: -0.194938. Value loss: 0.080901. Entropy: 0.299796.\n",
      "Iteration 20661: Policy loss: -0.200009. Value loss: 0.065087. Entropy: 0.300219.\n",
      "episode: 7158   score: 865.0  epsilon: 1.0    steps: 608  evaluation reward: 446.8\n",
      "Training network. lr: 0.000091. clip: 0.036566\n",
      "Iteration 20662: Policy loss: 0.133999. Value loss: 0.215901. Entropy: 0.296653.\n",
      "Iteration 20663: Policy loss: 0.128473. Value loss: 0.098197. Entropy: 0.295351.\n",
      "Iteration 20664: Policy loss: 0.125902. Value loss: 0.069313. Entropy: 0.295635.\n",
      "episode: 7159   score: 350.0  epsilon: 1.0    steps: 616  evaluation reward: 445.9\n",
      "Training network. lr: 0.000091. clip: 0.036566\n",
      "Iteration 20665: Policy loss: 0.131905. Value loss: 0.324587. Entropy: 0.297565.\n",
      "Iteration 20666: Policy loss: 0.132873. Value loss: 0.123059. Entropy: 0.297383.\n",
      "Iteration 20667: Policy loss: 0.119144. Value loss: 0.076867. Entropy: 0.296164.\n",
      "episode: 7160   score: 410.0  epsilon: 1.0    steps: 720  evaluation reward: 447.0\n",
      "Training network. lr: 0.000091. clip: 0.036566\n",
      "Iteration 20668: Policy loss: 0.135885. Value loss: 0.106095. Entropy: 0.298828.\n",
      "Iteration 20669: Policy loss: 0.133277. Value loss: 0.044516. Entropy: 0.299617.\n",
      "Iteration 20670: Policy loss: 0.134546. Value loss: 0.030204. Entropy: 0.299228.\n",
      "episode: 7161   score: 925.0  epsilon: 1.0    steps: 968  evaluation reward: 452.3\n",
      "Training network. lr: 0.000091. clip: 0.036566\n",
      "Iteration 20671: Policy loss: -0.204926. Value loss: 0.257318. Entropy: 0.303629.\n",
      "Iteration 20672: Policy loss: -0.205641. Value loss: 0.122144. Entropy: 0.304313.\n",
      "Iteration 20673: Policy loss: -0.221835. Value loss: 0.079539. Entropy: 0.304563.\n",
      "episode: 7162   score: 210.0  epsilon: 1.0    steps: 168  evaluation reward: 450.7\n",
      "episode: 7163   score: 640.0  epsilon: 1.0    steps: 640  evaluation reward: 450.2\n",
      "Training network. lr: 0.000091. clip: 0.036566\n",
      "Iteration 20674: Policy loss: 0.334156. Value loss: 0.201269. Entropy: 0.272780.\n",
      "Iteration 20675: Policy loss: 0.326982. Value loss: 0.081015. Entropy: 0.274550.\n",
      "Iteration 20676: Policy loss: 0.329275. Value loss: 0.052218. Entropy: 0.277040.\n",
      "episode: 7164   score: 345.0  epsilon: 1.0    steps: 776  evaluation reward: 448.95\n",
      "Training network. lr: 0.000091. clip: 0.036566\n",
      "Iteration 20677: Policy loss: 0.252259. Value loss: 0.212962. Entropy: 0.302875.\n",
      "Iteration 20678: Policy loss: 0.245968. Value loss: 0.073764. Entropy: 0.302241.\n",
      "Iteration 20679: Policy loss: 0.245040. Value loss: 0.055428. Entropy: 0.302395.\n",
      "Training network. lr: 0.000091. clip: 0.036566\n",
      "Iteration 20680: Policy loss: -0.174441. Value loss: 0.156285. Entropy: 0.306160.\n",
      "Iteration 20681: Policy loss: -0.188726. Value loss: 0.077925. Entropy: 0.305771.\n",
      "Iteration 20682: Policy loss: -0.181615. Value loss: 0.056348. Entropy: 0.304497.\n",
      "episode: 7165   score: 620.0  epsilon: 1.0    steps: 728  evaluation reward: 450.65\n",
      "Training network. lr: 0.000091. clip: 0.036566\n",
      "Iteration 20683: Policy loss: -0.050586. Value loss: 0.366491. Entropy: 0.300099.\n",
      "Iteration 20684: Policy loss: -0.067022. Value loss: 0.208673. Entropy: 0.299470.\n",
      "Iteration 20685: Policy loss: -0.068502. Value loss: 0.134344. Entropy: 0.299102.\n",
      "episode: 7166   score: 640.0  epsilon: 1.0    steps: 920  evaluation reward: 449.05\n",
      "Training network. lr: 0.000091. clip: 0.036566\n",
      "Iteration 20686: Policy loss: 0.597132. Value loss: 0.360082. Entropy: 0.309037.\n",
      "Iteration 20687: Policy loss: 0.586640. Value loss: 0.115747. Entropy: 0.307490.\n",
      "Iteration 20688: Policy loss: 0.583776. Value loss: 0.065454. Entropy: 0.307487.\n",
      "Training network. lr: 0.000091. clip: 0.036566\n",
      "Iteration 20689: Policy loss: 0.146502. Value loss: 0.182397. Entropy: 0.296326.\n",
      "Iteration 20690: Policy loss: 0.155696. Value loss: 0.083000. Entropy: 0.296443.\n",
      "Iteration 20691: Policy loss: 0.154492. Value loss: 0.064291. Entropy: 0.295151.\n",
      "Training network. lr: 0.000091. clip: 0.036566\n",
      "Iteration 20692: Policy loss: 0.018618. Value loss: 0.110572. Entropy: 0.308590.\n",
      "Iteration 20693: Policy loss: 0.018860. Value loss: 0.046841. Entropy: 0.308599.\n",
      "Iteration 20694: Policy loss: 0.015699. Value loss: 0.034606. Entropy: 0.308033.\n",
      "episode: 7167   score: 290.0  epsilon: 1.0    steps: 424  evaluation reward: 448.1\n",
      "episode: 7168   score: 195.0  epsilon: 1.0    steps: 456  evaluation reward: 446.9\n",
      "Training network. lr: 0.000091. clip: 0.036566\n",
      "Iteration 20695: Policy loss: 0.328167. Value loss: 0.200388. Entropy: 0.288740.\n",
      "Iteration 20696: Policy loss: 0.322722. Value loss: 0.075756. Entropy: 0.287289.\n",
      "Iteration 20697: Policy loss: 0.325766. Value loss: 0.048392. Entropy: 0.286845.\n",
      "Training network. lr: 0.000091. clip: 0.036566\n",
      "Iteration 20698: Policy loss: -0.063645. Value loss: 0.181292. Entropy: 0.319494.\n",
      "Iteration 20699: Policy loss: -0.068843. Value loss: 0.075408. Entropy: 0.318572.\n",
      "Iteration 20700: Policy loss: -0.069564. Value loss: 0.061093. Entropy: 0.319233.\n",
      "episode: 7169   score: 480.0  epsilon: 1.0    steps: 608  evaluation reward: 445.8\n",
      "episode: 7170   score: 640.0  epsilon: 1.0    steps: 720  evaluation reward: 446.3\n",
      "Training network. lr: 0.000091. clip: 0.036419\n",
      "Iteration 20701: Policy loss: 0.233231. Value loss: 0.163778. Entropy: 0.285802.\n",
      "Iteration 20702: Policy loss: 0.230082. Value loss: 0.084786. Entropy: 0.282191.\n",
      "Iteration 20703: Policy loss: 0.231169. Value loss: 0.059017. Entropy: 0.281930.\n",
      "Training network. lr: 0.000091. clip: 0.036419\n",
      "Iteration 20704: Policy loss: 0.424363. Value loss: 0.282328. Entropy: 0.313234.\n",
      "Iteration 20705: Policy loss: 0.419987. Value loss: 0.091115. Entropy: 0.313133.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20706: Policy loss: 0.415077. Value loss: 0.059249. Entropy: 0.311908.\n",
      "episode: 7171   score: 385.0  epsilon: 1.0    steps: 480  evaluation reward: 444.95\n",
      "episode: 7172   score: 705.0  epsilon: 1.0    steps: 920  evaluation reward: 444.0\n",
      "episode: 7173   score: 775.0  epsilon: 1.0    steps: 1008  evaluation reward: 444.8\n",
      "Training network. lr: 0.000091. clip: 0.036419\n",
      "Iteration 20707: Policy loss: 0.304352. Value loss: 0.246083. Entropy: 0.291269.\n",
      "Iteration 20708: Policy loss: 0.292818. Value loss: 0.094704. Entropy: 0.288947.\n",
      "Iteration 20709: Policy loss: 0.295617. Value loss: 0.065373. Entropy: 0.289343.\n",
      "Training network. lr: 0.000091. clip: 0.036419\n",
      "Iteration 20710: Policy loss: 0.253533. Value loss: 0.142501. Entropy: 0.299433.\n",
      "Iteration 20711: Policy loss: 0.243353. Value loss: 0.045673. Entropy: 0.298348.\n",
      "Iteration 20712: Policy loss: 0.247099. Value loss: 0.031018. Entropy: 0.298449.\n",
      "Training network. lr: 0.000091. clip: 0.036419\n",
      "Iteration 20713: Policy loss: 0.419945. Value loss: 0.254259. Entropy: 0.307946.\n",
      "Iteration 20714: Policy loss: 0.415980. Value loss: 0.102720. Entropy: 0.307924.\n",
      "Iteration 20715: Policy loss: 0.405160. Value loss: 0.068762. Entropy: 0.307036.\n",
      "Training network. lr: 0.000091. clip: 0.036419\n",
      "Iteration 20716: Policy loss: 0.181957. Value loss: 0.122030. Entropy: 0.314176.\n",
      "Iteration 20717: Policy loss: 0.181984. Value loss: 0.057747. Entropy: 0.314122.\n",
      "Iteration 20718: Policy loss: 0.176685. Value loss: 0.048130. Entropy: 0.312927.\n",
      "episode: 7174   score: 595.0  epsilon: 1.0    steps: 872  evaluation reward: 448.65\n",
      "Training network. lr: 0.000091. clip: 0.036419\n",
      "Iteration 20719: Policy loss: -0.098036. Value loss: 0.233635. Entropy: 0.307832.\n",
      "Iteration 20720: Policy loss: -0.084675. Value loss: 0.099750. Entropy: 0.308514.\n",
      "Iteration 20721: Policy loss: -0.091867. Value loss: 0.041496. Entropy: 0.307390.\n",
      "episode: 7175   score: 320.0  epsilon: 1.0    steps: 664  evaluation reward: 449.0\n",
      "Training network. lr: 0.000091. clip: 0.036419\n",
      "Iteration 20722: Policy loss: 0.283740. Value loss: 0.169004. Entropy: 0.293851.\n",
      "Iteration 20723: Policy loss: 0.299166. Value loss: 0.057653. Entropy: 0.291626.\n",
      "Iteration 20724: Policy loss: 0.280038. Value loss: 0.033593. Entropy: 0.291727.\n",
      "episode: 7176   score: 260.0  epsilon: 1.0    steps: 96  evaluation reward: 448.45\n",
      "episode: 7177   score: 420.0  epsilon: 1.0    steps: 760  evaluation reward: 447.3\n",
      "Training network. lr: 0.000091. clip: 0.036419\n",
      "Iteration 20725: Policy loss: 0.095576. Value loss: 0.108678. Entropy: 0.290247.\n",
      "Iteration 20726: Policy loss: 0.093125. Value loss: 0.046453. Entropy: 0.291003.\n",
      "Iteration 20727: Policy loss: 0.084097. Value loss: 0.032432. Entropy: 0.290437.\n",
      "Training network. lr: 0.000091. clip: 0.036419\n",
      "Iteration 20728: Policy loss: -0.501177. Value loss: 0.441699. Entropy: 0.303853.\n",
      "Iteration 20729: Policy loss: -0.514085. Value loss: 0.301223. Entropy: 0.304361.\n",
      "Iteration 20730: Policy loss: -0.540370. Value loss: 0.231409. Entropy: 0.303832.\n",
      "episode: 7178   score: 260.0  epsilon: 1.0    steps: 432  evaluation reward: 445.55\n",
      "episode: 7179   score: 260.0  epsilon: 1.0    steps: 672  evaluation reward: 442.85\n",
      "episode: 7180   score: 720.0  epsilon: 1.0    steps: 784  evaluation reward: 445.4\n",
      "Training network. lr: 0.000091. clip: 0.036419\n",
      "Iteration 20731: Policy loss: 0.196980. Value loss: 0.087689. Entropy: 0.284738.\n",
      "Iteration 20732: Policy loss: 0.190949. Value loss: 0.044526. Entropy: 0.284676.\n",
      "Iteration 20733: Policy loss: 0.191683. Value loss: 0.034143. Entropy: 0.286382.\n",
      "Training network. lr: 0.000091. clip: 0.036419\n",
      "Iteration 20734: Policy loss: -0.064254. Value loss: 0.228034. Entropy: 0.301537.\n",
      "Iteration 20735: Policy loss: -0.069871. Value loss: 0.145205. Entropy: 0.302240.\n",
      "Iteration 20736: Policy loss: -0.076087. Value loss: 0.094045. Entropy: 0.302628.\n",
      "Training network. lr: 0.000091. clip: 0.036419\n",
      "Iteration 20737: Policy loss: 0.274960. Value loss: 0.146659. Entropy: 0.296169.\n",
      "Iteration 20738: Policy loss: 0.255841. Value loss: 0.051251. Entropy: 0.296426.\n",
      "Iteration 20739: Policy loss: 0.271948. Value loss: 0.040707. Entropy: 0.296220.\n",
      "Training network. lr: 0.000091. clip: 0.036419\n",
      "Iteration 20740: Policy loss: -0.076606. Value loss: 0.208765. Entropy: 0.305633.\n",
      "Iteration 20741: Policy loss: -0.077197. Value loss: 0.064967. Entropy: 0.307768.\n",
      "Iteration 20742: Policy loss: -0.087875. Value loss: 0.037055. Entropy: 0.308082.\n",
      "Training network. lr: 0.000091. clip: 0.036419\n",
      "Iteration 20743: Policy loss: 0.455528. Value loss: 0.249429. Entropy: 0.313707.\n",
      "Iteration 20744: Policy loss: 0.468360. Value loss: 0.087616. Entropy: 0.313032.\n",
      "Iteration 20745: Policy loss: 0.453381. Value loss: 0.070875. Entropy: 0.312860.\n",
      "episode: 7181   score: 670.0  epsilon: 1.0    steps: 408  evaluation reward: 446.45\n",
      "episode: 7182   score: 550.0  epsilon: 1.0    steps: 544  evaluation reward: 448.75\n",
      "episode: 7183   score: 465.0  epsilon: 1.0    steps: 952  evaluation reward: 447.1\n",
      "Training network. lr: 0.000091. clip: 0.036419\n",
      "Iteration 20746: Policy loss: -0.397180. Value loss: 0.521680. Entropy: 0.282711.\n",
      "Iteration 20747: Policy loss: -0.401949. Value loss: 0.234900. Entropy: 0.283369.\n",
      "Iteration 20748: Policy loss: -0.400296. Value loss: 0.123560. Entropy: 0.282532.\n",
      "episode: 7184   score: 770.0  epsilon: 1.0    steps: 752  evaluation reward: 448.15\n",
      "Training network. lr: 0.000091. clip: 0.036419\n",
      "Iteration 20749: Policy loss: -0.015206. Value loss: 0.083758. Entropy: 0.298879.\n",
      "Iteration 20750: Policy loss: -0.019233. Value loss: 0.038852. Entropy: 0.300087.\n",
      "Iteration 20751: Policy loss: -0.023229. Value loss: 0.024996. Entropy: 0.299554.\n",
      "Training network. lr: 0.000091. clip: 0.036262\n",
      "Iteration 20752: Policy loss: 0.462662. Value loss: 0.275045. Entropy: 0.300917.\n",
      "Iteration 20753: Policy loss: 0.462278. Value loss: 0.092179. Entropy: 0.298460.\n",
      "Iteration 20754: Policy loss: 0.452619. Value loss: 0.050382. Entropy: 0.297448.\n",
      "episode: 7185   score: 665.0  epsilon: 1.0    steps: 488  evaluation reward: 447.85\n",
      "episode: 7186   score: 290.0  epsilon: 1.0    steps: 592  evaluation reward: 447.15\n",
      "Training network. lr: 0.000091. clip: 0.036262\n",
      "Iteration 20755: Policy loss: -0.285874. Value loss: 0.170191. Entropy: 0.287017.\n",
      "Iteration 20756: Policy loss: -0.283886. Value loss: 0.067489. Entropy: 0.286441.\n",
      "Iteration 20757: Policy loss: -0.300852. Value loss: 0.039323. Entropy: 0.286064.\n",
      "episode: 7187   score: 535.0  epsilon: 1.0    steps: 432  evaluation reward: 450.4\n",
      "Training network. lr: 0.000091. clip: 0.036262\n",
      "Iteration 20758: Policy loss: 0.484302. Value loss: 0.259507. Entropy: 0.299048.\n",
      "Iteration 20759: Policy loss: 0.471084. Value loss: 0.070146. Entropy: 0.298445.\n",
      "Iteration 20760: Policy loss: 0.453311. Value loss: 0.044959. Entropy: 0.299419.\n",
      "Training network. lr: 0.000091. clip: 0.036262\n",
      "Iteration 20761: Policy loss: 0.138108. Value loss: 0.147289. Entropy: 0.302990.\n",
      "Iteration 20762: Policy loss: 0.123914. Value loss: 0.061838. Entropy: 0.301645.\n",
      "Iteration 20763: Policy loss: 0.121729. Value loss: 0.039387. Entropy: 0.301926.\n",
      "Training network. lr: 0.000091. clip: 0.036262\n",
      "Iteration 20764: Policy loss: 0.357762. Value loss: 0.287323. Entropy: 0.302099.\n",
      "Iteration 20765: Policy loss: 0.364722. Value loss: 0.099349. Entropy: 0.301609.\n",
      "Iteration 20766: Policy loss: 0.353991. Value loss: 0.066517. Entropy: 0.300609.\n",
      "episode: 7188   score: 700.0  epsilon: 1.0    steps: 288  evaluation reward: 453.4\n",
      "Training network. lr: 0.000091. clip: 0.036262\n",
      "Iteration 20767: Policy loss: 0.255107. Value loss: 0.082533. Entropy: 0.292487.\n",
      "Iteration 20768: Policy loss: 0.252557. Value loss: 0.037773. Entropy: 0.294185.\n",
      "Iteration 20769: Policy loss: 0.248300. Value loss: 0.028189. Entropy: 0.293242.\n",
      "episode: 7189   score: 390.0  epsilon: 1.0    steps: 632  evaluation reward: 453.6\n",
      "Training network. lr: 0.000091. clip: 0.036262\n",
      "Iteration 20770: Policy loss: 0.179526. Value loss: 0.234458. Entropy: 0.299344.\n",
      "Iteration 20771: Policy loss: 0.166487. Value loss: 0.090758. Entropy: 0.299462.\n",
      "Iteration 20772: Policy loss: 0.173073. Value loss: 0.058866. Entropy: 0.298626.\n",
      "Training network. lr: 0.000091. clip: 0.036262\n",
      "Iteration 20773: Policy loss: 0.080167. Value loss: 0.095535. Entropy: 0.305234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20774: Policy loss: 0.079228. Value loss: 0.030316. Entropy: 0.305921.\n",
      "Iteration 20775: Policy loss: 0.077682. Value loss: 0.019878. Entropy: 0.307159.\n",
      "episode: 7190   score: 345.0  epsilon: 1.0    steps: 112  evaluation reward: 454.45\n",
      "episode: 7191   score: 620.0  epsilon: 1.0    steps: 656  evaluation reward: 457.25\n",
      "Training network. lr: 0.000091. clip: 0.036262\n",
      "Iteration 20776: Policy loss: 0.048410. Value loss: 0.105586. Entropy: 0.279664.\n",
      "Iteration 20777: Policy loss: 0.045144. Value loss: 0.051775. Entropy: 0.280795.\n",
      "Iteration 20778: Policy loss: 0.039347. Value loss: 0.037946. Entropy: 0.281000.\n",
      "episode: 7192   score: 315.0  epsilon: 1.0    steps: 360  evaluation reward: 454.2\n",
      "episode: 7193   score: 240.0  epsilon: 1.0    steps: 424  evaluation reward: 454.75\n",
      "episode: 7194   score: 695.0  epsilon: 1.0    steps: 1000  evaluation reward: 458.1\n",
      "Training network. lr: 0.000091. clip: 0.036262\n",
      "Iteration 20779: Policy loss: 0.111858. Value loss: 0.136022. Entropy: 0.290806.\n",
      "Iteration 20780: Policy loss: 0.095956. Value loss: 0.051274. Entropy: 0.290861.\n",
      "Iteration 20781: Policy loss: 0.106207. Value loss: 0.038959. Entropy: 0.290389.\n",
      "Training network. lr: 0.000091. clip: 0.036262\n",
      "Iteration 20782: Policy loss: 0.153550. Value loss: 0.065189. Entropy: 0.300285.\n",
      "Iteration 20783: Policy loss: 0.144859. Value loss: 0.029615. Entropy: 0.298643.\n",
      "Iteration 20784: Policy loss: 0.143940. Value loss: 0.023349. Entropy: 0.297991.\n",
      "Training network. lr: 0.000091. clip: 0.036262\n",
      "Iteration 20785: Policy loss: -0.186078. Value loss: 0.152484. Entropy: 0.300263.\n",
      "Iteration 20786: Policy loss: -0.181792. Value loss: 0.053259. Entropy: 0.300448.\n",
      "Iteration 20787: Policy loss: -0.184238. Value loss: 0.035943. Entropy: 0.301337.\n",
      "Training network. lr: 0.000091. clip: 0.036262\n",
      "Iteration 20788: Policy loss: 0.197913. Value loss: 0.195781. Entropy: 0.310036.\n",
      "Iteration 20789: Policy loss: 0.198424. Value loss: 0.079013. Entropy: 0.309584.\n",
      "Iteration 20790: Policy loss: 0.195441. Value loss: 0.051319. Entropy: 0.310141.\n",
      "episode: 7195   score: 280.0  epsilon: 1.0    steps: 72  evaluation reward: 457.0\n",
      "episode: 7196   score: 695.0  epsilon: 1.0    steps: 408  evaluation reward: 461.05\n",
      "Training network. lr: 0.000091. clip: 0.036262\n",
      "Iteration 20791: Policy loss: 0.000896. Value loss: 0.179963. Entropy: 0.289448.\n",
      "Iteration 20792: Policy loss: 0.001284. Value loss: 0.073901. Entropy: 0.286844.\n",
      "Iteration 20793: Policy loss: -0.002581. Value loss: 0.051656. Entropy: 0.286349.\n",
      "Training network. lr: 0.000091. clip: 0.036262\n",
      "Iteration 20794: Policy loss: -0.138859. Value loss: 0.144358. Entropy: 0.311880.\n",
      "Iteration 20795: Policy loss: -0.140059. Value loss: 0.063976. Entropy: 0.312287.\n",
      "Iteration 20796: Policy loss: -0.148115. Value loss: 0.043504. Entropy: 0.311237.\n",
      "episode: 7197   score: 210.0  epsilon: 1.0    steps: 976  evaluation reward: 459.25\n",
      "Training network. lr: 0.000091. clip: 0.036262\n",
      "Iteration 20797: Policy loss: -0.017925. Value loss: 0.169509. Entropy: 0.297548.\n",
      "Iteration 20798: Policy loss: -0.028648. Value loss: 0.052492. Entropy: 0.298239.\n",
      "Iteration 20799: Policy loss: -0.035241. Value loss: 0.034864. Entropy: 0.299593.\n",
      "episode: 7198   score: 580.0  epsilon: 1.0    steps: 256  evaluation reward: 456.9\n",
      "Training network. lr: 0.000091. clip: 0.036262\n",
      "Iteration 20800: Policy loss: 0.246175. Value loss: 0.076267. Entropy: 0.294690.\n",
      "Iteration 20801: Policy loss: 0.240359. Value loss: 0.031986. Entropy: 0.293571.\n",
      "Iteration 20802: Policy loss: 0.236891. Value loss: 0.020742. Entropy: 0.294608.\n",
      "episode: 7199   score: 335.0  epsilon: 1.0    steps: 280  evaluation reward: 454.4\n",
      "Training network. lr: 0.000090. clip: 0.036105\n",
      "Iteration 20803: Policy loss: -0.212213. Value loss: 0.149973. Entropy: 0.287619.\n",
      "Iteration 20804: Policy loss: -0.237989. Value loss: 0.106698. Entropy: 0.287881.\n",
      "Iteration 20805: Policy loss: -0.235148. Value loss: 0.057222. Entropy: 0.286339.\n",
      "Training network. lr: 0.000090. clip: 0.036105\n",
      "Iteration 20806: Policy loss: -0.264427. Value loss: 0.393398. Entropy: 0.303947.\n",
      "Iteration 20807: Policy loss: -0.288438. Value loss: 0.223342. Entropy: 0.305200.\n",
      "Iteration 20808: Policy loss: -0.269894. Value loss: 0.151318. Entropy: 0.305729.\n",
      "episode: 7200   score: 525.0  epsilon: 1.0    steps: 48  evaluation reward: 457.75\n",
      "Training network. lr: 0.000090. clip: 0.036105\n",
      "Iteration 20809: Policy loss: 0.035883. Value loss: 0.423608. Entropy: 0.300761.\n",
      "Iteration 20810: Policy loss: 0.039836. Value loss: 0.296297. Entropy: 0.299696.\n",
      "Iteration 20811: Policy loss: 0.030808. Value loss: 0.251842. Entropy: 0.298290.\n",
      "now time :  2019-09-06 11:42:57.760189\n",
      "episode: 7201   score: 515.0  epsilon: 1.0    steps: 352  evaluation reward: 460.4\n",
      "episode: 7202   score: 755.0  epsilon: 1.0    steps: 440  evaluation reward: 463.25\n",
      "Training network. lr: 0.000090. clip: 0.036105\n",
      "Iteration 20812: Policy loss: 0.080858. Value loss: 0.077718. Entropy: 0.285080.\n",
      "Iteration 20813: Policy loss: 0.078279. Value loss: 0.036791. Entropy: 0.284254.\n",
      "Iteration 20814: Policy loss: 0.076403. Value loss: 0.026805. Entropy: 0.284720.\n",
      "episode: 7203   score: 600.0  epsilon: 1.0    steps: 616  evaluation reward: 465.45\n",
      "Training network. lr: 0.000090. clip: 0.036105\n",
      "Iteration 20815: Policy loss: 0.147987. Value loss: 0.156274. Entropy: 0.302412.\n",
      "Iteration 20816: Policy loss: 0.150670. Value loss: 0.063439. Entropy: 0.300721.\n",
      "Iteration 20817: Policy loss: 0.139525. Value loss: 0.035725. Entropy: 0.299812.\n",
      "Training network. lr: 0.000090. clip: 0.036105\n",
      "Iteration 20818: Policy loss: -0.247046. Value loss: 0.175578. Entropy: 0.298046.\n",
      "Iteration 20819: Policy loss: -0.255443. Value loss: 0.103779. Entropy: 0.296859.\n",
      "Iteration 20820: Policy loss: -0.256326. Value loss: 0.075105. Entropy: 0.298794.\n",
      "Training network. lr: 0.000090. clip: 0.036105\n",
      "Iteration 20821: Policy loss: -0.243500. Value loss: 0.148542. Entropy: 0.303060.\n",
      "Iteration 20822: Policy loss: -0.249558. Value loss: 0.052261. Entropy: 0.305268.\n",
      "Iteration 20823: Policy loss: -0.254468. Value loss: 0.038843. Entropy: 0.304521.\n",
      "episode: 7204   score: 695.0  epsilon: 1.0    steps: 424  evaluation reward: 468.2\n",
      "episode: 7205   score: 560.0  epsilon: 1.0    steps: 896  evaluation reward: 468.95\n",
      "Training network. lr: 0.000090. clip: 0.036105\n",
      "Iteration 20824: Policy loss: 0.334431. Value loss: 0.218927. Entropy: 0.291586.\n",
      "Iteration 20825: Policy loss: 0.331441. Value loss: 0.073289. Entropy: 0.291340.\n",
      "Iteration 20826: Policy loss: 0.322110. Value loss: 0.044177. Entropy: 0.291747.\n",
      "episode: 7206   score: 675.0  epsilon: 1.0    steps: 912  evaluation reward: 470.15\n",
      "Training network. lr: 0.000090. clip: 0.036105\n",
      "Iteration 20827: Policy loss: -0.240438. Value loss: 0.330493. Entropy: 0.299682.\n",
      "Iteration 20828: Policy loss: -0.257111. Value loss: 0.190251. Entropy: 0.300379.\n",
      "Iteration 20829: Policy loss: -0.251825. Value loss: 0.129679. Entropy: 0.299669.\n",
      "episode: 7207   score: 590.0  epsilon: 1.0    steps: 432  evaluation reward: 468.45\n",
      "episode: 7208   score: 710.0  epsilon: 1.0    steps: 504  evaluation reward: 470.35\n",
      "Training network. lr: 0.000090. clip: 0.036105\n",
      "Iteration 20830: Policy loss: 0.256194. Value loss: 0.128766. Entropy: 0.267515.\n",
      "Iteration 20831: Policy loss: 0.258617. Value loss: 0.059940. Entropy: 0.271072.\n",
      "Iteration 20832: Policy loss: 0.247392. Value loss: 0.043903. Entropy: 0.269585.\n",
      "episode: 7209   score: 345.0  epsilon: 1.0    steps: 744  evaluation reward: 469.45\n",
      "Training network. lr: 0.000090. clip: 0.036105\n",
      "Iteration 20833: Policy loss: -0.003529. Value loss: 0.145094. Entropy: 0.292609.\n",
      "Iteration 20834: Policy loss: -0.006997. Value loss: 0.068238. Entropy: 0.293539.\n",
      "Iteration 20835: Policy loss: -0.011665. Value loss: 0.043024. Entropy: 0.292742.\n",
      "episode: 7210   score: 320.0  epsilon: 1.0    steps: 816  evaluation reward: 467.65\n",
      "Training network. lr: 0.000090. clip: 0.036105\n",
      "Iteration 20836: Policy loss: 0.135001. Value loss: 0.080657. Entropy: 0.291796.\n",
      "Iteration 20837: Policy loss: 0.138340. Value loss: 0.038962. Entropy: 0.290322.\n",
      "Iteration 20838: Policy loss: 0.130995. Value loss: 0.031030. Entropy: 0.290330.\n",
      "Training network. lr: 0.000090. clip: 0.036105\n",
      "Iteration 20839: Policy loss: -0.262549. Value loss: 0.337489. Entropy: 0.301268.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20840: Policy loss: -0.283714. Value loss: 0.181634. Entropy: 0.301911.\n",
      "Iteration 20841: Policy loss: -0.284962. Value loss: 0.127050. Entropy: 0.302302.\n",
      "Training network. lr: 0.000090. clip: 0.036105\n",
      "Iteration 20842: Policy loss: 0.086907. Value loss: 0.137245. Entropy: 0.301442.\n",
      "Iteration 20843: Policy loss: 0.089432. Value loss: 0.060693. Entropy: 0.301518.\n",
      "Iteration 20844: Policy loss: 0.091821. Value loss: 0.039914. Entropy: 0.301829.\n",
      "Training network. lr: 0.000090. clip: 0.036105\n",
      "Iteration 20845: Policy loss: -0.308974. Value loss: 0.366373. Entropy: 0.306984.\n",
      "Iteration 20846: Policy loss: -0.306549. Value loss: 0.174226. Entropy: 0.307440.\n",
      "Iteration 20847: Policy loss: -0.304615. Value loss: 0.115449. Entropy: 0.309583.\n",
      "Training network. lr: 0.000090. clip: 0.036105\n",
      "Iteration 20848: Policy loss: 0.162651. Value loss: 0.233931. Entropy: 0.308955.\n",
      "Iteration 20849: Policy loss: 0.164812. Value loss: 0.087158. Entropy: 0.309363.\n",
      "Iteration 20850: Policy loss: 0.164849. Value loss: 0.063554. Entropy: 0.309804.\n",
      "episode: 7211   score: 470.0  epsilon: 1.0    steps: 640  evaluation reward: 468.45\n",
      "Training network. lr: 0.000090. clip: 0.035958\n",
      "Iteration 20851: Policy loss: 0.188395. Value loss: 0.137775. Entropy: 0.301691.\n",
      "Iteration 20852: Policy loss: 0.179813. Value loss: 0.066565. Entropy: 0.303383.\n",
      "Iteration 20853: Policy loss: 0.185070. Value loss: 0.045516. Entropy: 0.301316.\n",
      "episode: 7212   score: 910.0  epsilon: 1.0    steps: 312  evaluation reward: 472.55\n",
      "Training network. lr: 0.000090. clip: 0.035958\n",
      "Iteration 20854: Policy loss: -0.136744. Value loss: 0.069645. Entropy: 0.298433.\n",
      "Iteration 20855: Policy loss: -0.142640. Value loss: 0.038674. Entropy: 0.299818.\n",
      "Iteration 20856: Policy loss: -0.149671. Value loss: 0.028289. Entropy: 0.299718.\n",
      "episode: 7213   score: 425.0  epsilon: 1.0    steps: 344  evaluation reward: 472.1\n",
      "episode: 7214   score: 360.0  epsilon: 1.0    steps: 736  evaluation reward: 471.0\n",
      "episode: 7215   score: 620.0  epsilon: 1.0    steps: 920  evaluation reward: 474.25\n",
      "Training network. lr: 0.000090. clip: 0.035958\n",
      "Iteration 20857: Policy loss: -0.068927. Value loss: 0.360777. Entropy: 0.281720.\n",
      "Iteration 20858: Policy loss: -0.072603. Value loss: 0.186226. Entropy: 0.281330.\n",
      "Iteration 20859: Policy loss: -0.102714. Value loss: 0.098609. Entropy: 0.282221.\n",
      "Training network. lr: 0.000090. clip: 0.035958\n",
      "Iteration 20860: Policy loss: -0.079369. Value loss: 0.139863. Entropy: 0.302419.\n",
      "Iteration 20861: Policy loss: -0.086280. Value loss: 0.059537. Entropy: 0.300859.\n",
      "Iteration 20862: Policy loss: -0.093889. Value loss: 0.041389. Entropy: 0.299796.\n",
      "episode: 7216   score: 870.0  epsilon: 1.0    steps: 376  evaluation reward: 480.05\n",
      "episode: 7217   score: 730.0  epsilon: 1.0    steps: 640  evaluation reward: 484.75\n",
      "episode: 7218   score: 705.0  epsilon: 1.0    steps: 704  evaluation reward: 488.15\n",
      "Training network. lr: 0.000090. clip: 0.035958\n",
      "Iteration 20863: Policy loss: -0.082770. Value loss: 0.128180. Entropy: 0.259201.\n",
      "Iteration 20864: Policy loss: -0.091696. Value loss: 0.062632. Entropy: 0.260579.\n",
      "Iteration 20865: Policy loss: -0.089799. Value loss: 0.053013. Entropy: 0.259426.\n",
      "Training network. lr: 0.000090. clip: 0.035958\n",
      "Iteration 20866: Policy loss: 0.093893. Value loss: 0.355744. Entropy: 0.302910.\n",
      "Iteration 20867: Policy loss: 0.081087. Value loss: 0.185173. Entropy: 0.302437.\n",
      "Iteration 20868: Policy loss: 0.076691. Value loss: 0.103283. Entropy: 0.302245.\n",
      "Training network. lr: 0.000090. clip: 0.035958\n",
      "Iteration 20869: Policy loss: 0.033098. Value loss: 0.097925. Entropy: 0.298499.\n",
      "Iteration 20870: Policy loss: 0.029460. Value loss: 0.041672. Entropy: 0.296832.\n",
      "Iteration 20871: Policy loss: 0.029141. Value loss: 0.032219. Entropy: 0.296323.\n",
      "Training network. lr: 0.000090. clip: 0.035958\n",
      "Iteration 20872: Policy loss: 0.398310. Value loss: 0.265138. Entropy: 0.301762.\n",
      "Iteration 20873: Policy loss: 0.379042. Value loss: 0.085979. Entropy: 0.303104.\n",
      "Iteration 20874: Policy loss: 0.392895. Value loss: 0.051616. Entropy: 0.300731.\n",
      "episode: 7219   score: 595.0  epsilon: 1.0    steps: 672  evaluation reward: 490.15\n",
      "Training network. lr: 0.000090. clip: 0.035958\n",
      "Iteration 20875: Policy loss: 0.269937. Value loss: 0.131371. Entropy: 0.291635.\n",
      "Iteration 20876: Policy loss: 0.265709. Value loss: 0.050639. Entropy: 0.292969.\n",
      "Iteration 20877: Policy loss: 0.261628. Value loss: 0.039582. Entropy: 0.292891.\n",
      "Training network. lr: 0.000090. clip: 0.035958\n",
      "Iteration 20878: Policy loss: 0.311013. Value loss: 0.146097. Entropy: 0.305787.\n",
      "Iteration 20879: Policy loss: 0.315599. Value loss: 0.063630. Entropy: 0.306233.\n",
      "Iteration 20880: Policy loss: 0.305085. Value loss: 0.042991. Entropy: 0.305273.\n",
      "episode: 7220   score: 405.0  epsilon: 1.0    steps: 688  evaluation reward: 489.25\n",
      "episode: 7221   score: 595.0  epsilon: 1.0    steps: 1016  evaluation reward: 489.95\n",
      "Training network. lr: 0.000090. clip: 0.035958\n",
      "Iteration 20881: Policy loss: 0.197352. Value loss: 0.164795. Entropy: 0.306609.\n",
      "Iteration 20882: Policy loss: 0.189853. Value loss: 0.067393. Entropy: 0.306732.\n",
      "Iteration 20883: Policy loss: 0.182840. Value loss: 0.040098. Entropy: 0.306755.\n",
      "episode: 7222   score: 415.0  epsilon: 1.0    steps: 40  evaluation reward: 490.1\n",
      "Training network. lr: 0.000090. clip: 0.035958\n",
      "Iteration 20884: Policy loss: -0.096934. Value loss: 0.103202. Entropy: 0.293200.\n",
      "Iteration 20885: Policy loss: -0.106215. Value loss: 0.050016. Entropy: 0.293231.\n",
      "Iteration 20886: Policy loss: -0.107931. Value loss: 0.038816. Entropy: 0.291835.\n",
      "episode: 7223   score: 360.0  epsilon: 1.0    steps: 40  evaluation reward: 488.65\n",
      "episode: 7224   score: 420.0  epsilon: 1.0    steps: 168  evaluation reward: 489.5\n",
      "episode: 7225   score: 270.0  epsilon: 1.0    steps: 344  evaluation reward: 489.8\n",
      "episode: 7226   score: 745.0  epsilon: 1.0    steps: 464  evaluation reward: 494.3\n",
      "Training network. lr: 0.000090. clip: 0.035958\n",
      "Iteration 20887: Policy loss: -0.063818. Value loss: 0.058488. Entropy: 0.252609.\n",
      "Iteration 20888: Policy loss: -0.062705. Value loss: 0.041619. Entropy: 0.247525.\n",
      "Iteration 20889: Policy loss: -0.066300. Value loss: 0.036565. Entropy: 0.247585.\n",
      "Training network. lr: 0.000090. clip: 0.035958\n",
      "Iteration 20890: Policy loss: 0.017350. Value loss: 0.084294. Entropy: 0.308718.\n",
      "Iteration 20891: Policy loss: 0.018145. Value loss: 0.044553. Entropy: 0.308798.\n",
      "Iteration 20892: Policy loss: 0.012914. Value loss: 0.035664. Entropy: 0.308454.\n",
      "Training network. lr: 0.000090. clip: 0.035958\n",
      "Iteration 20893: Policy loss: 0.359411. Value loss: 0.146074. Entropy: 0.302916.\n",
      "Iteration 20894: Policy loss: 0.352192. Value loss: 0.050151. Entropy: 0.301764.\n",
      "Iteration 20895: Policy loss: 0.331998. Value loss: 0.033165. Entropy: 0.301936.\n",
      "Training network. lr: 0.000090. clip: 0.035958\n",
      "Iteration 20896: Policy loss: -0.051214. Value loss: 0.340319. Entropy: 0.303748.\n",
      "Iteration 20897: Policy loss: -0.036758. Value loss: 0.215299. Entropy: 0.304021.\n",
      "Iteration 20898: Policy loss: -0.063635. Value loss: 0.186050. Entropy: 0.304150.\n",
      "episode: 7227   score: 400.0  epsilon: 1.0    steps: 448  evaluation reward: 493.35\n",
      "Training network. lr: 0.000090. clip: 0.035958\n",
      "Iteration 20899: Policy loss: 0.182256. Value loss: 0.096487. Entropy: 0.291536.\n",
      "Iteration 20900: Policy loss: 0.185486. Value loss: 0.040810. Entropy: 0.289112.\n",
      "Iteration 20901: Policy loss: 0.191052. Value loss: 0.022372. Entropy: 0.290685.\n",
      "Training network. lr: 0.000090. clip: 0.035801\n",
      "Iteration 20902: Policy loss: -0.619238. Value loss: 0.365578. Entropy: 0.304631.\n",
      "Iteration 20903: Policy loss: -0.620248. Value loss: 0.209666. Entropy: 0.305363.\n",
      "Iteration 20904: Policy loss: -0.624273. Value loss: 0.148051. Entropy: 0.305141.\n",
      "episode: 7228   score: 285.0  epsilon: 1.0    steps: 696  evaluation reward: 492.0\n",
      "Training network. lr: 0.000090. clip: 0.035801\n",
      "Iteration 20905: Policy loss: -0.167033. Value loss: 0.150768. Entropy: 0.296890.\n",
      "Iteration 20906: Policy loss: -0.168819. Value loss: 0.071492. Entropy: 0.296780.\n",
      "Iteration 20907: Policy loss: -0.165690. Value loss: 0.050633. Entropy: 0.297761.\n",
      "episode: 7229   score: 370.0  epsilon: 1.0    steps: 264  evaluation reward: 491.5\n",
      "Training network. lr: 0.000090. clip: 0.035801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20908: Policy loss: -0.031506. Value loss: 0.123444. Entropy: 0.295913.\n",
      "Iteration 20909: Policy loss: -0.036726. Value loss: 0.057807. Entropy: 0.295601.\n",
      "Iteration 20910: Policy loss: -0.040737. Value loss: 0.041876. Entropy: 0.295207.\n",
      "episode: 7230   score: 850.0  epsilon: 1.0    steps: 8  evaluation reward: 498.8\n",
      "episode: 7231   score: 410.0  epsilon: 1.0    steps: 736  evaluation reward: 499.45\n",
      "Training network. lr: 0.000090. clip: 0.035801\n",
      "Iteration 20911: Policy loss: 0.125936. Value loss: 0.083311. Entropy: 0.286720.\n",
      "Iteration 20912: Policy loss: 0.121149. Value loss: 0.037018. Entropy: 0.284145.\n",
      "Iteration 20913: Policy loss: 0.118173. Value loss: 0.026856. Entropy: 0.284702.\n",
      "Training network. lr: 0.000090. clip: 0.035801\n",
      "Iteration 20914: Policy loss: -0.169917. Value loss: 0.324397. Entropy: 0.305902.\n",
      "Iteration 20915: Policy loss: -0.171237. Value loss: 0.204847. Entropy: 0.304410.\n",
      "Iteration 20916: Policy loss: -0.158298. Value loss: 0.138615. Entropy: 0.306050.\n",
      "episode: 7232   score: 740.0  epsilon: 1.0    steps: 192  evaluation reward: 500.55\n",
      "episode: 7233   score: 675.0  epsilon: 1.0    steps: 456  evaluation reward: 502.05\n",
      "episode: 7234   score: 480.0  epsilon: 1.0    steps: 688  evaluation reward: 501.6\n",
      "Training network. lr: 0.000090. clip: 0.035801\n",
      "Iteration 20917: Policy loss: 0.178402. Value loss: 0.118058. Entropy: 0.268946.\n",
      "Iteration 20918: Policy loss: 0.176328. Value loss: 0.057649. Entropy: 0.268035.\n",
      "Iteration 20919: Policy loss: 0.170101. Value loss: 0.043173. Entropy: 0.268015.\n",
      "Training network. lr: 0.000090. clip: 0.035801\n",
      "Iteration 20920: Policy loss: -0.084205. Value loss: 0.268402. Entropy: 0.305235.\n",
      "Iteration 20921: Policy loss: -0.101003. Value loss: 0.206500. Entropy: 0.306754.\n",
      "Iteration 20922: Policy loss: -0.108625. Value loss: 0.138446. Entropy: 0.306955.\n",
      "Training network. lr: 0.000090. clip: 0.035801\n",
      "Iteration 20923: Policy loss: -0.123450. Value loss: 0.102852. Entropy: 0.296901.\n",
      "Iteration 20924: Policy loss: -0.121891. Value loss: 0.040492. Entropy: 0.296680.\n",
      "Iteration 20925: Policy loss: -0.131125. Value loss: 0.029220. Entropy: 0.296645.\n",
      "episode: 7235   score: 370.0  epsilon: 1.0    steps: 568  evaluation reward: 499.1\n",
      "Training network. lr: 0.000090. clip: 0.035801\n",
      "Iteration 20926: Policy loss: 0.196645. Value loss: 0.136911. Entropy: 0.295412.\n",
      "Iteration 20927: Policy loss: 0.204640. Value loss: 0.043220. Entropy: 0.295287.\n",
      "Iteration 20928: Policy loss: 0.202667. Value loss: 0.027900. Entropy: 0.295437.\n",
      "Training network. lr: 0.000090. clip: 0.035801\n",
      "Iteration 20929: Policy loss: -0.175437. Value loss: 0.390511. Entropy: 0.303396.\n",
      "Iteration 20930: Policy loss: -0.178296. Value loss: 0.175309. Entropy: 0.304386.\n",
      "Iteration 20931: Policy loss: -0.183056. Value loss: 0.108497. Entropy: 0.303242.\n",
      "episode: 7236   score: 330.0  epsilon: 1.0    steps: 616  evaluation reward: 497.7\n",
      "episode: 7237   score: 550.0  epsilon: 1.0    steps: 736  evaluation reward: 497.65\n",
      "Training network. lr: 0.000090. clip: 0.035801\n",
      "Iteration 20932: Policy loss: 0.252271. Value loss: 0.103561. Entropy: 0.271280.\n",
      "Iteration 20933: Policy loss: 0.254937. Value loss: 0.042828. Entropy: 0.271598.\n",
      "Iteration 20934: Policy loss: 0.253060. Value loss: 0.028879. Entropy: 0.272334.\n",
      "episode: 7238   score: 595.0  epsilon: 1.0    steps: 40  evaluation reward: 499.9\n",
      "Training network. lr: 0.000090. clip: 0.035801\n",
      "Iteration 20935: Policy loss: -0.112132. Value loss: 0.148876. Entropy: 0.301667.\n",
      "Iteration 20936: Policy loss: -0.120590. Value loss: 0.050969. Entropy: 0.302194.\n",
      "Iteration 20937: Policy loss: -0.124655. Value loss: 0.030997. Entropy: 0.302484.\n",
      "Training network. lr: 0.000090. clip: 0.035801\n",
      "Iteration 20938: Policy loss: -0.027717. Value loss: 0.416528. Entropy: 0.297704.\n",
      "Iteration 20939: Policy loss: -0.068933. Value loss: 0.213451. Entropy: 0.296335.\n",
      "Iteration 20940: Policy loss: -0.067633. Value loss: 0.132912. Entropy: 0.298508.\n",
      "Training network. lr: 0.000090. clip: 0.035801\n",
      "Iteration 20941: Policy loss: 0.245126. Value loss: 0.132450. Entropy: 0.300059.\n",
      "Iteration 20942: Policy loss: 0.241027. Value loss: 0.061557. Entropy: 0.301736.\n",
      "Iteration 20943: Policy loss: 0.234717. Value loss: 0.045349. Entropy: 0.301582.\n",
      "Training network. lr: 0.000090. clip: 0.035801\n",
      "Iteration 20944: Policy loss: -0.031053. Value loss: 0.085502. Entropy: 0.304890.\n",
      "Iteration 20945: Policy loss: -0.036146. Value loss: 0.038118. Entropy: 0.300455.\n",
      "Iteration 20946: Policy loss: -0.038790. Value loss: 0.025394. Entropy: 0.300699.\n",
      "episode: 7239   score: 810.0  epsilon: 1.0    steps: 584  evaluation reward: 502.75\n",
      "episode: 7240   score: 560.0  epsilon: 1.0    steps: 824  evaluation reward: 503.05\n",
      "Training network. lr: 0.000090. clip: 0.035801\n",
      "Iteration 20947: Policy loss: -0.089288. Value loss: 0.519875. Entropy: 0.297819.\n",
      "Iteration 20948: Policy loss: -0.086395. Value loss: 0.169539. Entropy: 0.298530.\n",
      "Iteration 20949: Policy loss: -0.089301. Value loss: 0.093588. Entropy: 0.297870.\n",
      "episode: 7241   score: 450.0  epsilon: 1.0    steps: 536  evaluation reward: 503.4\n",
      "Training network. lr: 0.000090. clip: 0.035801\n",
      "Iteration 20950: Policy loss: 0.297594. Value loss: 0.343799. Entropy: 0.300991.\n",
      "Iteration 20951: Policy loss: 0.271704. Value loss: 0.192957. Entropy: 0.301341.\n",
      "Iteration 20952: Policy loss: 0.294619. Value loss: 0.115029. Entropy: 0.300813.\n",
      "Training network. lr: 0.000089. clip: 0.035645\n",
      "Iteration 20953: Policy loss: 0.196538. Value loss: 0.086633. Entropy: 0.306338.\n",
      "Iteration 20954: Policy loss: 0.199407. Value loss: 0.045461. Entropy: 0.306640.\n",
      "Iteration 20955: Policy loss: 0.194182. Value loss: 0.035014. Entropy: 0.307424.\n",
      "episode: 7242   score: 485.0  epsilon: 1.0    steps: 344  evaluation reward: 506.75\n",
      "Training network. lr: 0.000089. clip: 0.035645\n",
      "Iteration 20956: Policy loss: -0.439578. Value loss: 0.250665. Entropy: 0.294669.\n",
      "Iteration 20957: Policy loss: -0.456891. Value loss: 0.093524. Entropy: 0.296550.\n",
      "Iteration 20958: Policy loss: -0.460409. Value loss: 0.056255. Entropy: 0.295227.\n",
      "episode: 7243   score: 565.0  epsilon: 1.0    steps: 440  evaluation reward: 508.95\n",
      "episode: 7244   score: 620.0  epsilon: 1.0    steps: 744  evaluation reward: 513.0\n",
      "Training network. lr: 0.000089. clip: 0.035645\n",
      "Iteration 20959: Policy loss: 0.053366. Value loss: 0.221463. Entropy: 0.284802.\n",
      "Iteration 20960: Policy loss: 0.057061. Value loss: 0.096896. Entropy: 0.284675.\n",
      "Iteration 20961: Policy loss: 0.063326. Value loss: 0.058005. Entropy: 0.282852.\n",
      "episode: 7245   score: 775.0  epsilon: 1.0    steps: 416  evaluation reward: 515.65\n",
      "Training network. lr: 0.000089. clip: 0.035645\n",
      "Iteration 20962: Policy loss: 0.198264. Value loss: 0.173280. Entropy: 0.286064.\n",
      "Iteration 20963: Policy loss: 0.184949. Value loss: 0.058504. Entropy: 0.286548.\n",
      "Iteration 20964: Policy loss: 0.186480. Value loss: 0.039131. Entropy: 0.287801.\n",
      "episode: 7246   score: 550.0  epsilon: 1.0    steps: 608  evaluation reward: 517.5\n",
      "Training network. lr: 0.000089. clip: 0.035645\n",
      "Iteration 20965: Policy loss: -0.018601. Value loss: 0.147154. Entropy: 0.294013.\n",
      "Iteration 20966: Policy loss: -0.013854. Value loss: 0.067274. Entropy: 0.293609.\n",
      "Iteration 20967: Policy loss: -0.016710. Value loss: 0.046845. Entropy: 0.293464.\n",
      "Training network. lr: 0.000089. clip: 0.035645\n",
      "Iteration 20968: Policy loss: -0.011347. Value loss: 0.181259. Entropy: 0.313380.\n",
      "Iteration 20969: Policy loss: -0.020351. Value loss: 0.053828. Entropy: 0.313986.\n",
      "Iteration 20970: Policy loss: -0.034204. Value loss: 0.036121. Entropy: 0.313042.\n",
      "episode: 7247   score: 335.0  epsilon: 1.0    steps: 952  evaluation reward: 515.25\n",
      "Training network. lr: 0.000089. clip: 0.035645\n",
      "Iteration 20971: Policy loss: -0.055202. Value loss: 0.323535. Entropy: 0.298288.\n",
      "Iteration 20972: Policy loss: -0.084656. Value loss: 0.181821. Entropy: 0.299113.\n",
      "Iteration 20973: Policy loss: -0.097677. Value loss: 0.130758. Entropy: 0.298131.\n",
      "Training network. lr: 0.000089. clip: 0.035645\n",
      "Iteration 20974: Policy loss: -0.016209. Value loss: 0.075734. Entropy: 0.300660.\n",
      "Iteration 20975: Policy loss: -0.025012. Value loss: 0.028956. Entropy: 0.300862.\n",
      "Iteration 20976: Policy loss: -0.021729. Value loss: 0.020875. Entropy: 0.301553.\n",
      "episode: 7248   score: 620.0  epsilon: 1.0    steps: 960  evaluation reward: 515.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000089. clip: 0.035645\n",
      "Iteration 20977: Policy loss: 0.237380. Value loss: 0.219370. Entropy: 0.302447.\n",
      "Iteration 20978: Policy loss: 0.235221. Value loss: 0.068062. Entropy: 0.304024.\n",
      "Iteration 20979: Policy loss: 0.229897. Value loss: 0.043427. Entropy: 0.302988.\n",
      "episode: 7249   score: 330.0  epsilon: 1.0    steps: 624  evaluation reward: 514.8\n",
      "Training network. lr: 0.000089. clip: 0.035645\n",
      "Iteration 20980: Policy loss: 0.014222. Value loss: 0.262939. Entropy: 0.294589.\n",
      "Iteration 20981: Policy loss: 0.018918. Value loss: 0.099297. Entropy: 0.295650.\n",
      "Iteration 20982: Policy loss: -0.022420. Value loss: 0.050466. Entropy: 0.296638.\n",
      "Training network. lr: 0.000089. clip: 0.035645\n",
      "Iteration 20983: Policy loss: -0.071145. Value loss: 0.149020. Entropy: 0.305152.\n",
      "Iteration 20984: Policy loss: -0.072696. Value loss: 0.056152. Entropy: 0.304773.\n",
      "Iteration 20985: Policy loss: -0.078461. Value loss: 0.041555. Entropy: 0.303900.\n",
      "episode: 7250   score: 530.0  epsilon: 1.0    steps: 600  evaluation reward: 516.2\n",
      "now time :  2019-09-06 11:53:35.059715\n",
      "episode: 7251   score: 450.0  epsilon: 1.0    steps: 760  evaluation reward: 515.05\n",
      "Training network. lr: 0.000089. clip: 0.035645\n",
      "Iteration 20986: Policy loss: 0.118508. Value loss: 0.149571. Entropy: 0.286685.\n",
      "Iteration 20987: Policy loss: 0.104114. Value loss: 0.067279. Entropy: 0.287497.\n",
      "Iteration 20988: Policy loss: 0.116246. Value loss: 0.051133. Entropy: 0.286771.\n",
      "Training network. lr: 0.000089. clip: 0.035645\n",
      "Iteration 20989: Policy loss: 0.062324. Value loss: 0.140189. Entropy: 0.307972.\n",
      "Iteration 20990: Policy loss: 0.056041. Value loss: 0.059632. Entropy: 0.309042.\n",
      "Iteration 20991: Policy loss: 0.057899. Value loss: 0.038929. Entropy: 0.307579.\n",
      "episode: 7252   score: 390.0  epsilon: 1.0    steps: 16  evaluation reward: 515.95\n",
      "episode: 7253   score: 670.0  epsilon: 1.0    steps: 688  evaluation reward: 515.05\n",
      "Training network. lr: 0.000089. clip: 0.035645\n",
      "Iteration 20992: Policy loss: 0.174306. Value loss: 0.140636. Entropy: 0.293549.\n",
      "Iteration 20993: Policy loss: 0.172536. Value loss: 0.059736. Entropy: 0.293053.\n",
      "Iteration 20994: Policy loss: 0.180208. Value loss: 0.037874. Entropy: 0.294025.\n",
      "episode: 7254   score: 985.0  epsilon: 1.0    steps: 248  evaluation reward: 522.2\n",
      "episode: 7255   score: 215.0  epsilon: 1.0    steps: 320  evaluation reward: 519.95\n",
      "Training network. lr: 0.000089. clip: 0.035645\n",
      "Iteration 20995: Policy loss: -0.240129. Value loss: 0.208355. Entropy: 0.291053.\n",
      "Iteration 20996: Policy loss: -0.242017. Value loss: 0.075085. Entropy: 0.292086.\n",
      "Iteration 20997: Policy loss: -0.243529. Value loss: 0.044442. Entropy: 0.293661.\n",
      "Training network. lr: 0.000089. clip: 0.035645\n",
      "Iteration 20998: Policy loss: 0.081789. Value loss: 0.180573. Entropy: 0.300948.\n",
      "Iteration 20999: Policy loss: 0.068684. Value loss: 0.071241. Entropy: 0.299917.\n",
      "Iteration 21000: Policy loss: 0.071472. Value loss: 0.050148. Entropy: 0.300631.\n",
      "episode: 7256   score: 335.0  epsilon: 1.0    steps: 280  evaluation reward: 519.4\n",
      "Training network. lr: 0.000089. clip: 0.035497\n",
      "Iteration 21001: Policy loss: 0.262034. Value loss: 0.196296. Entropy: 0.295426.\n",
      "Iteration 21002: Policy loss: 0.256866. Value loss: 0.060660. Entropy: 0.293968.\n",
      "Iteration 21003: Policy loss: 0.261748. Value loss: 0.039040. Entropy: 0.293104.\n",
      "Training network. lr: 0.000089. clip: 0.035497\n",
      "Iteration 21004: Policy loss: 0.083359. Value loss: 0.085946. Entropy: 0.298704.\n",
      "Iteration 21005: Policy loss: 0.087048. Value loss: 0.043151. Entropy: 0.299301.\n",
      "Iteration 21006: Policy loss: 0.073599. Value loss: 0.028926. Entropy: 0.297765.\n",
      "Training network. lr: 0.000089. clip: 0.035497\n",
      "Iteration 21007: Policy loss: -0.273369. Value loss: 0.255505. Entropy: 0.311885.\n",
      "Iteration 21008: Policy loss: -0.282578. Value loss: 0.138713. Entropy: 0.311339.\n",
      "Iteration 21009: Policy loss: -0.273067. Value loss: 0.087975. Entropy: 0.311656.\n",
      "episode: 7257   score: 330.0  epsilon: 1.0    steps: 424  evaluation reward: 519.05\n",
      "Training network. lr: 0.000089. clip: 0.035497\n",
      "Iteration 21010: Policy loss: 0.122906. Value loss: 0.116558. Entropy: 0.301893.\n",
      "Iteration 21011: Policy loss: 0.125143. Value loss: 0.057021. Entropy: 0.300833.\n",
      "Iteration 21012: Policy loss: 0.117909. Value loss: 0.038105. Entropy: 0.301272.\n",
      "episode: 7258   score: 410.0  epsilon: 1.0    steps: 168  evaluation reward: 514.5\n",
      "Training network. lr: 0.000089. clip: 0.035497\n",
      "Iteration 21013: Policy loss: -0.162496. Value loss: 0.352769. Entropy: 0.304144.\n",
      "Iteration 21014: Policy loss: -0.175937. Value loss: 0.197564. Entropy: 0.304441.\n",
      "Iteration 21015: Policy loss: -0.174479. Value loss: 0.130458. Entropy: 0.304367.\n",
      "episode: 7259   score: 370.0  epsilon: 1.0    steps: 264  evaluation reward: 514.7\n",
      "episode: 7260   score: 270.0  epsilon: 1.0    steps: 752  evaluation reward: 513.3\n",
      "Training network. lr: 0.000089. clip: 0.035497\n",
      "Iteration 21016: Policy loss: -0.056441. Value loss: 0.164689. Entropy: 0.289314.\n",
      "Iteration 21017: Policy loss: -0.061767. Value loss: 0.061904. Entropy: 0.288800.\n",
      "Iteration 21018: Policy loss: -0.069518. Value loss: 0.043948. Entropy: 0.287171.\n",
      "Training network. lr: 0.000089. clip: 0.035497\n",
      "Iteration 21019: Policy loss: -0.208825. Value loss: 0.388270. Entropy: 0.309717.\n",
      "Iteration 21020: Policy loss: -0.215504. Value loss: 0.178548. Entropy: 0.308490.\n",
      "Iteration 21021: Policy loss: -0.231645. Value loss: 0.136726. Entropy: 0.308356.\n",
      "episode: 7261   score: 895.0  epsilon: 1.0    steps: 888  evaluation reward: 513.0\n",
      "Training network. lr: 0.000089. clip: 0.035497\n",
      "Iteration 21022: Policy loss: 0.189215. Value loss: 0.168057. Entropy: 0.294716.\n",
      "Iteration 21023: Policy loss: 0.186136. Value loss: 0.058666. Entropy: 0.293869.\n",
      "Iteration 21024: Policy loss: 0.174059. Value loss: 0.039428. Entropy: 0.294388.\n",
      "episode: 7262   score: 450.0  epsilon: 1.0    steps: 376  evaluation reward: 515.4\n",
      "Training network. lr: 0.000089. clip: 0.035497\n",
      "Iteration 21025: Policy loss: -0.006284. Value loss: 0.140736. Entropy: 0.290282.\n",
      "Iteration 21026: Policy loss: -0.016427. Value loss: 0.074667. Entropy: 0.290511.\n",
      "Iteration 21027: Policy loss: -0.010499. Value loss: 0.055165. Entropy: 0.290316.\n",
      "episode: 7263   score: 795.0  epsilon: 1.0    steps: 112  evaluation reward: 516.95\n",
      "Training network. lr: 0.000089. clip: 0.035497\n",
      "Iteration 21028: Policy loss: 0.154208. Value loss: 0.161771. Entropy: 0.301623.\n",
      "Iteration 21029: Policy loss: 0.159859. Value loss: 0.055991. Entropy: 0.303186.\n",
      "Iteration 21030: Policy loss: 0.157762. Value loss: 0.033291. Entropy: 0.300703.\n",
      "episode: 7264   score: 605.0  epsilon: 1.0    steps: 192  evaluation reward: 519.55\n",
      "Training network. lr: 0.000089. clip: 0.035497\n",
      "Iteration 21031: Policy loss: 0.171360. Value loss: 0.142954. Entropy: 0.293351.\n",
      "Iteration 21032: Policy loss: 0.169147. Value loss: 0.059327. Entropy: 0.293672.\n",
      "Iteration 21033: Policy loss: 0.163420. Value loss: 0.040756. Entropy: 0.294213.\n",
      "episode: 7265   score: 385.0  epsilon: 1.0    steps: 816  evaluation reward: 517.2\n",
      "Training network. lr: 0.000089. clip: 0.035497\n",
      "Iteration 21034: Policy loss: -0.017216. Value loss: 0.344078. Entropy: 0.303229.\n",
      "Iteration 21035: Policy loss: -0.031727. Value loss: 0.229606. Entropy: 0.304350.\n",
      "Iteration 21036: Policy loss: -0.034511. Value loss: 0.166317. Entropy: 0.303149.\n",
      "episode: 7266   score: 345.0  epsilon: 1.0    steps: 496  evaluation reward: 514.25\n",
      "Training network. lr: 0.000089. clip: 0.035497\n",
      "Iteration 21037: Policy loss: -0.004692. Value loss: 0.259795. Entropy: 0.292536.\n",
      "Iteration 21038: Policy loss: 0.000121. Value loss: 0.111494. Entropy: 0.291343.\n",
      "Iteration 21039: Policy loss: -0.030017. Value loss: 0.075088. Entropy: 0.290841.\n",
      "Training network. lr: 0.000089. clip: 0.035497\n",
      "Iteration 21040: Policy loss: 0.128328. Value loss: 0.178300. Entropy: 0.309024.\n",
      "Iteration 21041: Policy loss: 0.130267. Value loss: 0.079648. Entropy: 0.310128.\n",
      "Iteration 21042: Policy loss: 0.121038. Value loss: 0.054958. Entropy: 0.310401.\n",
      "episode: 7267   score: 305.0  epsilon: 1.0    steps: 296  evaluation reward: 514.4\n",
      "Training network. lr: 0.000089. clip: 0.035497\n",
      "Iteration 21043: Policy loss: 0.029175. Value loss: 0.103575. Entropy: 0.295026.\n",
      "Iteration 21044: Policy loss: 0.025262. Value loss: 0.057732. Entropy: 0.295383.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21045: Policy loss: 0.030537. Value loss: 0.042639. Entropy: 0.294061.\n",
      "Training network. lr: 0.000089. clip: 0.035497\n",
      "Iteration 21046: Policy loss: 0.186272. Value loss: 0.105470. Entropy: 0.312320.\n",
      "Iteration 21047: Policy loss: 0.182829. Value loss: 0.041799. Entropy: 0.309711.\n",
      "Iteration 21048: Policy loss: 0.179933. Value loss: 0.030764. Entropy: 0.310950.\n",
      "episode: 7268   score: 345.0  epsilon: 1.0    steps: 176  evaluation reward: 515.9\n",
      "episode: 7269   score: 1000.0  epsilon: 1.0    steps: 792  evaluation reward: 521.1\n",
      "Training network. lr: 0.000089. clip: 0.035497\n",
      "Iteration 21049: Policy loss: 0.053880. Value loss: 0.087516. Entropy: 0.301138.\n",
      "Iteration 21050: Policy loss: 0.059106. Value loss: 0.038657. Entropy: 0.300476.\n",
      "Iteration 21051: Policy loss: 0.052452. Value loss: 0.025470. Entropy: 0.301341.\n",
      "Training network. lr: 0.000088. clip: 0.035341\n",
      "Iteration 21052: Policy loss: -0.264555. Value loss: 0.316373. Entropy: 0.299083.\n",
      "Iteration 21053: Policy loss: -0.268201. Value loss: 0.226253. Entropy: 0.302974.\n",
      "Iteration 21054: Policy loss: -0.289324. Value loss: 0.199904. Entropy: 0.300334.\n",
      "Training network. lr: 0.000088. clip: 0.035341\n",
      "Iteration 21055: Policy loss: 0.061681. Value loss: 0.192224. Entropy: 0.306289.\n",
      "Iteration 21056: Policy loss: 0.060456. Value loss: 0.087758. Entropy: 0.305113.\n",
      "Iteration 21057: Policy loss: 0.051112. Value loss: 0.059848. Entropy: 0.305519.\n",
      "episode: 7270   score: 345.0  epsilon: 1.0    steps: 240  evaluation reward: 518.15\n",
      "episode: 7271   score: 670.0  epsilon: 1.0    steps: 408  evaluation reward: 521.0\n",
      "episode: 7272   score: 570.0  epsilon: 1.0    steps: 1008  evaluation reward: 519.65\n",
      "Training network. lr: 0.000088. clip: 0.035341\n",
      "Iteration 21058: Policy loss: 0.010400. Value loss: 0.087608. Entropy: 0.294479.\n",
      "Iteration 21059: Policy loss: 0.008631. Value loss: 0.044781. Entropy: 0.296180.\n",
      "Iteration 21060: Policy loss: 0.002789. Value loss: 0.036692. Entropy: 0.295594.\n",
      "episode: 7273   score: 290.0  epsilon: 1.0    steps: 64  evaluation reward: 514.8\n",
      "episode: 7274   score: 470.0  epsilon: 1.0    steps: 824  evaluation reward: 513.55\n",
      "Training network. lr: 0.000088. clip: 0.035341\n",
      "Iteration 21061: Policy loss: -0.016178. Value loss: 0.082140. Entropy: 0.299021.\n",
      "Iteration 21062: Policy loss: -0.018030. Value loss: 0.038978. Entropy: 0.299272.\n",
      "Iteration 21063: Policy loss: -0.016479. Value loss: 0.025878. Entropy: 0.299704.\n",
      "episode: 7275   score: 330.0  epsilon: 1.0    steps: 408  evaluation reward: 513.65\n",
      "Training network. lr: 0.000088. clip: 0.035341\n",
      "Iteration 21064: Policy loss: 0.211378. Value loss: 0.291717. Entropy: 0.305399.\n",
      "Iteration 21065: Policy loss: 0.211349. Value loss: 0.205228. Entropy: 0.305460.\n",
      "Iteration 21066: Policy loss: 0.211806. Value loss: 0.167586. Entropy: 0.306014.\n",
      "Training network. lr: 0.000088. clip: 0.035341\n",
      "Iteration 21067: Policy loss: -0.112882. Value loss: 0.092631. Entropy: 0.308793.\n",
      "Iteration 21068: Policy loss: -0.113831. Value loss: 0.040420. Entropy: 0.307029.\n",
      "Iteration 21069: Policy loss: -0.115269. Value loss: 0.031448. Entropy: 0.307712.\n",
      "episode: 7276   score: 440.0  epsilon: 1.0    steps: 880  evaluation reward: 515.45\n",
      "episode: 7277   score: 285.0  epsilon: 1.0    steps: 928  evaluation reward: 514.1\n",
      "Training network. lr: 0.000088. clip: 0.035341\n",
      "Iteration 21070: Policy loss: 0.060531. Value loss: 0.039877. Entropy: 0.294643.\n",
      "Iteration 21071: Policy loss: 0.054779. Value loss: 0.020513. Entropy: 0.293186.\n",
      "Iteration 21072: Policy loss: 0.054250. Value loss: 0.015873. Entropy: 0.293118.\n",
      "Training network. lr: 0.000088. clip: 0.035341\n",
      "Iteration 21073: Policy loss: -0.078217. Value loss: 0.118470. Entropy: 0.300481.\n",
      "Iteration 21074: Policy loss: -0.076711. Value loss: 0.052020. Entropy: 0.301493.\n",
      "Iteration 21075: Policy loss: -0.080923. Value loss: 0.035328. Entropy: 0.299683.\n",
      "episode: 7278   score: 410.0  epsilon: 1.0    steps: 256  evaluation reward: 515.6\n",
      "Training network. lr: 0.000088. clip: 0.035341\n",
      "Iteration 21076: Policy loss: -0.283099. Value loss: 0.283654. Entropy: 0.300612.\n",
      "Iteration 21077: Policy loss: -0.294395. Value loss: 0.129130. Entropy: 0.299388.\n",
      "Iteration 21078: Policy loss: -0.292800. Value loss: 0.057011. Entropy: 0.300797.\n",
      "episode: 7279   score: 290.0  epsilon: 1.0    steps: 136  evaluation reward: 515.9\n",
      "episode: 7280   score: 540.0  epsilon: 1.0    steps: 752  evaluation reward: 514.1\n",
      "Training network. lr: 0.000088. clip: 0.035341\n",
      "Iteration 21079: Policy loss: -0.178765. Value loss: 0.135867. Entropy: 0.283681.\n",
      "Iteration 21080: Policy loss: -0.185890. Value loss: 0.054402. Entropy: 0.285230.\n",
      "Iteration 21081: Policy loss: -0.187615. Value loss: 0.037197. Entropy: 0.285093.\n",
      "Training network. lr: 0.000088. clip: 0.035341\n",
      "Iteration 21082: Policy loss: -0.001552. Value loss: 0.115307. Entropy: 0.306193.\n",
      "Iteration 21083: Policy loss: -0.004676. Value loss: 0.042780. Entropy: 0.306087.\n",
      "Iteration 21084: Policy loss: -0.008350. Value loss: 0.033926. Entropy: 0.306229.\n",
      "episode: 7281   score: 380.0  epsilon: 1.0    steps: 464  evaluation reward: 511.2\n",
      "Training network. lr: 0.000088. clip: 0.035341\n",
      "Iteration 21085: Policy loss: 0.209723. Value loss: 0.142970. Entropy: 0.297930.\n",
      "Iteration 21086: Policy loss: 0.205957. Value loss: 0.044264. Entropy: 0.296300.\n",
      "Iteration 21087: Policy loss: 0.196352. Value loss: 0.030421. Entropy: 0.296465.\n",
      "episode: 7282   score: 375.0  epsilon: 1.0    steps: 832  evaluation reward: 509.45\n",
      "Training network. lr: 0.000088. clip: 0.035341\n",
      "Iteration 21088: Policy loss: -0.320703. Value loss: 0.258245. Entropy: 0.298983.\n",
      "Iteration 21089: Policy loss: -0.325362. Value loss: 0.152284. Entropy: 0.299465.\n",
      "Iteration 21090: Policy loss: -0.296852. Value loss: 0.067044. Entropy: 0.297492.\n",
      "episode: 7283   score: 345.0  epsilon: 1.0    steps: 1008  evaluation reward: 508.25\n",
      "Training network. lr: 0.000088. clip: 0.035341\n",
      "Iteration 21091: Policy loss: -0.242533. Value loss: 0.222505. Entropy: 0.304565.\n",
      "Iteration 21092: Policy loss: -0.258771. Value loss: 0.101864. Entropy: 0.304827.\n",
      "Iteration 21093: Policy loss: -0.257000. Value loss: 0.064288. Entropy: 0.304066.\n",
      "episode: 7284   score: 475.0  epsilon: 1.0    steps: 480  evaluation reward: 505.3\n",
      "Training network. lr: 0.000088. clip: 0.035341\n",
      "Iteration 21094: Policy loss: -0.016320. Value loss: 0.077232. Entropy: 0.284551.\n",
      "Iteration 21095: Policy loss: -0.014798. Value loss: 0.033159. Entropy: 0.283646.\n",
      "Iteration 21096: Policy loss: -0.012479. Value loss: 0.023335. Entropy: 0.282586.\n",
      "Training network. lr: 0.000088. clip: 0.035341\n",
      "Iteration 21097: Policy loss: -0.296250. Value loss: 0.312950. Entropy: 0.308103.\n",
      "Iteration 21098: Policy loss: -0.299164. Value loss: 0.131068. Entropy: 0.308221.\n",
      "Iteration 21099: Policy loss: -0.303001. Value loss: 0.053934. Entropy: 0.309237.\n",
      "Training network. lr: 0.000088. clip: 0.035341\n",
      "Iteration 21100: Policy loss: -0.004935. Value loss: 0.208119. Entropy: 0.298462.\n",
      "Iteration 21101: Policy loss: -0.023834. Value loss: 0.085373. Entropy: 0.299011.\n",
      "Iteration 21102: Policy loss: -0.021248. Value loss: 0.057453. Entropy: 0.299329.\n",
      "episode: 7285   score: 660.0  epsilon: 1.0    steps: 208  evaluation reward: 505.25\n",
      "episode: 7286   score: 470.0  epsilon: 1.0    steps: 576  evaluation reward: 507.05\n",
      "Training network. lr: 0.000088. clip: 0.035184\n",
      "Iteration 21103: Policy loss: -0.161599. Value loss: 0.223404. Entropy: 0.281923.\n",
      "Iteration 21104: Policy loss: -0.176932. Value loss: 0.083344. Entropy: 0.282133.\n",
      "Iteration 21105: Policy loss: -0.173296. Value loss: 0.057853. Entropy: 0.282742.\n",
      "episode: 7287   score: 620.0  epsilon: 1.0    steps: 56  evaluation reward: 507.9\n",
      "Training network. lr: 0.000088. clip: 0.035184\n",
      "Iteration 21106: Policy loss: 0.004469. Value loss: 0.109647. Entropy: 0.294905.\n",
      "Iteration 21107: Policy loss: -0.004885. Value loss: 0.053907. Entropy: 0.295393.\n",
      "Iteration 21108: Policy loss: -0.002405. Value loss: 0.037306. Entropy: 0.294768.\n",
      "episode: 7288   score: 405.0  epsilon: 1.0    steps: 432  evaluation reward: 504.95\n",
      "Training network. lr: 0.000088. clip: 0.035184\n",
      "Iteration 21109: Policy loss: 0.048986. Value loss: 0.196458. Entropy: 0.291097.\n",
      "Iteration 21110: Policy loss: 0.037481. Value loss: 0.076835. Entropy: 0.290998.\n",
      "Iteration 21111: Policy loss: 0.043896. Value loss: 0.052206. Entropy: 0.290748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000088. clip: 0.035184\n",
      "Iteration 21112: Policy loss: -0.109084. Value loss: 0.119788. Entropy: 0.312068.\n",
      "Iteration 21113: Policy loss: -0.121756. Value loss: 0.072520. Entropy: 0.312411.\n",
      "Iteration 21114: Policy loss: -0.128650. Value loss: 0.055786. Entropy: 0.312024.\n",
      "Training network. lr: 0.000088. clip: 0.035184\n",
      "Iteration 21115: Policy loss: 0.134583. Value loss: 0.195383. Entropy: 0.303009.\n",
      "Iteration 21116: Policy loss: 0.131238. Value loss: 0.062273. Entropy: 0.302146.\n",
      "Iteration 21117: Policy loss: 0.125293. Value loss: 0.042746. Entropy: 0.302685.\n",
      "episode: 7289   score: 605.0  epsilon: 1.0    steps: 168  evaluation reward: 507.1\n",
      "episode: 7290   score: 580.0  epsilon: 1.0    steps: 256  evaluation reward: 509.45\n",
      "Training network. lr: 0.000088. clip: 0.035184\n",
      "Iteration 21118: Policy loss: 0.229796. Value loss: 0.132829. Entropy: 0.287899.\n",
      "Iteration 21119: Policy loss: 0.227960. Value loss: 0.044612. Entropy: 0.286372.\n",
      "Iteration 21120: Policy loss: 0.222032. Value loss: 0.030553. Entropy: 0.286899.\n",
      "Training network. lr: 0.000088. clip: 0.035184\n",
      "Iteration 21121: Policy loss: 0.132819. Value loss: 0.175995. Entropy: 0.314337.\n",
      "Iteration 21122: Policy loss: 0.126458. Value loss: 0.076024. Entropy: 0.313683.\n",
      "Iteration 21123: Policy loss: 0.122708. Value loss: 0.049113. Entropy: 0.313662.\n",
      "episode: 7291   score: 760.0  epsilon: 1.0    steps: 544  evaluation reward: 510.85\n",
      "episode: 7292   score: 640.0  epsilon: 1.0    steps: 952  evaluation reward: 514.1\n",
      "Training network. lr: 0.000088. clip: 0.035184\n",
      "Iteration 21124: Policy loss: -0.120498. Value loss: 0.131391. Entropy: 0.300883.\n",
      "Iteration 21125: Policy loss: -0.120929. Value loss: 0.058294. Entropy: 0.301176.\n",
      "Iteration 21126: Policy loss: -0.128813. Value loss: 0.045133. Entropy: 0.301858.\n",
      "Training network. lr: 0.000088. clip: 0.035184\n",
      "Iteration 21127: Policy loss: -0.032050. Value loss: 0.294787. Entropy: 0.309233.\n",
      "Iteration 21128: Policy loss: -0.049178. Value loss: 0.184280. Entropy: 0.308692.\n",
      "Iteration 21129: Policy loss: -0.048074. Value loss: 0.097779. Entropy: 0.307831.\n",
      "episode: 7293   score: 525.0  epsilon: 1.0    steps: 792  evaluation reward: 516.95\n",
      "Training network. lr: 0.000088. clip: 0.035184\n",
      "Iteration 21130: Policy loss: -0.025966. Value loss: 0.117614. Entropy: 0.292254.\n",
      "Iteration 21131: Policy loss: -0.032224. Value loss: 0.066504. Entropy: 0.294711.\n",
      "Iteration 21132: Policy loss: -0.027058. Value loss: 0.050781. Entropy: 0.293740.\n",
      "episode: 7294   score: 365.0  epsilon: 1.0    steps: 248  evaluation reward: 513.65\n",
      "Training network. lr: 0.000088. clip: 0.035184\n",
      "Iteration 21133: Policy loss: -0.033490. Value loss: 0.111300. Entropy: 0.296494.\n",
      "Iteration 21134: Policy loss: -0.034971. Value loss: 0.049456. Entropy: 0.296803.\n",
      "Iteration 21135: Policy loss: -0.036648. Value loss: 0.038707. Entropy: 0.296669.\n",
      "episode: 7295   score: 670.0  epsilon: 1.0    steps: 560  evaluation reward: 517.55\n",
      "Training network. lr: 0.000088. clip: 0.035184\n",
      "Iteration 21136: Policy loss: -0.306070. Value loss: 0.463967. Entropy: 0.297842.\n",
      "Iteration 21137: Policy loss: -0.327747. Value loss: 0.282574. Entropy: 0.296729.\n",
      "Iteration 21138: Policy loss: -0.336784. Value loss: 0.202712. Entropy: 0.296959.\n",
      "episode: 7296   score: 545.0  epsilon: 1.0    steps: 568  evaluation reward: 516.05\n",
      "Training network. lr: 0.000088. clip: 0.035184\n",
      "Iteration 21139: Policy loss: -0.023769. Value loss: 0.224464. Entropy: 0.289827.\n",
      "Iteration 21140: Policy loss: -0.051107. Value loss: 0.076326. Entropy: 0.288637.\n",
      "Iteration 21141: Policy loss: -0.044894. Value loss: 0.040909. Entropy: 0.290274.\n",
      "episode: 7297   score: 390.0  epsilon: 1.0    steps: 40  evaluation reward: 517.85\n",
      "episode: 7298   score: 610.0  epsilon: 1.0    steps: 432  evaluation reward: 518.15\n",
      "Training network. lr: 0.000088. clip: 0.035184\n",
      "Iteration 21142: Policy loss: -0.103818. Value loss: 0.108110. Entropy: 0.284123.\n",
      "Iteration 21143: Policy loss: -0.103581. Value loss: 0.048351. Entropy: 0.284839.\n",
      "Iteration 21144: Policy loss: -0.106990. Value loss: 0.038830. Entropy: 0.285334.\n",
      "episode: 7299   score: 545.0  epsilon: 1.0    steps: 472  evaluation reward: 520.25\n",
      "Training network. lr: 0.000088. clip: 0.035184\n",
      "Iteration 21145: Policy loss: 0.570920. Value loss: 0.284885. Entropy: 0.296935.\n",
      "Iteration 21146: Policy loss: 0.567073. Value loss: 0.118781. Entropy: 0.297003.\n",
      "Iteration 21147: Policy loss: 0.564000. Value loss: 0.076535. Entropy: 0.296893.\n",
      "Training network. lr: 0.000088. clip: 0.035184\n",
      "Iteration 21148: Policy loss: -0.122485. Value loss: 0.151631. Entropy: 0.305261.\n",
      "Iteration 21149: Policy loss: -0.126220. Value loss: 0.075768. Entropy: 0.306032.\n",
      "Iteration 21150: Policy loss: -0.122432. Value loss: 0.056122. Entropy: 0.306797.\n",
      "episode: 7300   score: 260.0  epsilon: 1.0    steps: 976  evaluation reward: 517.6\n",
      "Training network. lr: 0.000088. clip: 0.035036\n",
      "Iteration 21151: Policy loss: -0.135044. Value loss: 0.278387. Entropy: 0.309669.\n",
      "Iteration 21152: Policy loss: -0.142916. Value loss: 0.119483. Entropy: 0.310290.\n",
      "Iteration 21153: Policy loss: -0.140295. Value loss: 0.087471. Entropy: 0.310226.\n",
      "Training network. lr: 0.000088. clip: 0.035036\n",
      "Iteration 21154: Policy loss: -0.263914. Value loss: 0.241158. Entropy: 0.297492.\n",
      "Iteration 21155: Policy loss: -0.270978. Value loss: 0.085744. Entropy: 0.298722.\n",
      "Iteration 21156: Policy loss: -0.266868. Value loss: 0.052780. Entropy: 0.297259.\n",
      "now time :  2019-09-06 12:03:58.825156\n",
      "episode: 7301   score: 405.0  epsilon: 1.0    steps: 216  evaluation reward: 516.5\n",
      "Training network. lr: 0.000088. clip: 0.035036\n",
      "Iteration 21157: Policy loss: 0.306187. Value loss: 0.204999. Entropy: 0.297847.\n",
      "Iteration 21158: Policy loss: 0.308483. Value loss: 0.083758. Entropy: 0.296530.\n",
      "Iteration 21159: Policy loss: 0.296337. Value loss: 0.053752. Entropy: 0.296161.\n",
      "episode: 7302   score: 350.0  epsilon: 1.0    steps: 528  evaluation reward: 512.45\n",
      "episode: 7303   score: 760.0  epsilon: 1.0    steps: 584  evaluation reward: 514.05\n",
      "Training network. lr: 0.000088. clip: 0.035036\n",
      "Iteration 21160: Policy loss: 0.179150. Value loss: 0.157609. Entropy: 0.281616.\n",
      "Iteration 21161: Policy loss: 0.172137. Value loss: 0.062107. Entropy: 0.280302.\n",
      "Iteration 21162: Policy loss: 0.161751. Value loss: 0.043595. Entropy: 0.281243.\n",
      "episode: 7304   score: 295.0  epsilon: 1.0    steps: 592  evaluation reward: 510.05\n",
      "episode: 7305   score: 590.0  epsilon: 1.0    steps: 904  evaluation reward: 510.35\n",
      "Training network. lr: 0.000088. clip: 0.035036\n",
      "Iteration 21163: Policy loss: 0.251884. Value loss: 0.148615. Entropy: 0.291363.\n",
      "Iteration 21164: Policy loss: 0.252212. Value loss: 0.064259. Entropy: 0.292518.\n",
      "Iteration 21165: Policy loss: 0.250486. Value loss: 0.045794. Entropy: 0.291832.\n",
      "episode: 7306   score: 595.0  epsilon: 1.0    steps: 832  evaluation reward: 509.55\n",
      "Training network. lr: 0.000088. clip: 0.035036\n",
      "Iteration 21166: Policy loss: 0.350541. Value loss: 0.296560. Entropy: 0.294104.\n",
      "Iteration 21167: Policy loss: 0.321811. Value loss: 0.072889. Entropy: 0.293444.\n",
      "Iteration 21168: Policy loss: 0.320372. Value loss: 0.049190. Entropy: 0.293158.\n",
      "episode: 7307   score: 425.0  epsilon: 1.0    steps: 384  evaluation reward: 507.9\n",
      "Training network. lr: 0.000088. clip: 0.035036\n",
      "Iteration 21169: Policy loss: 0.085792. Value loss: 0.111358. Entropy: 0.287897.\n",
      "Iteration 21170: Policy loss: 0.085002. Value loss: 0.057736. Entropy: 0.286725.\n",
      "Iteration 21171: Policy loss: 0.079785. Value loss: 0.044541. Entropy: 0.286310.\n",
      "Training network. lr: 0.000088. clip: 0.035036\n",
      "Iteration 21172: Policy loss: 0.260246. Value loss: 0.266038. Entropy: 0.310794.\n",
      "Iteration 21173: Policy loss: 0.254959. Value loss: 0.083902. Entropy: 0.308837.\n",
      "Iteration 21174: Policy loss: 0.246644. Value loss: 0.046546. Entropy: 0.309342.\n",
      "Training network. lr: 0.000088. clip: 0.035036\n",
      "Iteration 21175: Policy loss: -0.078231. Value loss: 0.306575. Entropy: 0.311609.\n",
      "Iteration 21176: Policy loss: -0.083375. Value loss: 0.123406. Entropy: 0.310554.\n",
      "Iteration 21177: Policy loss: -0.089423. Value loss: 0.066122. Entropy: 0.311738.\n",
      "Training network. lr: 0.000088. clip: 0.035036\n",
      "Iteration 21178: Policy loss: 0.130758. Value loss: 0.150523. Entropy: 0.313319.\n",
      "Iteration 21179: Policy loss: 0.123112. Value loss: 0.073850. Entropy: 0.313318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21180: Policy loss: 0.114073. Value loss: 0.051064. Entropy: 0.312254.\n",
      "episode: 7308   score: 350.0  epsilon: 1.0    steps: 472  evaluation reward: 504.3\n",
      "episode: 7309   score: 320.0  epsilon: 1.0    steps: 824  evaluation reward: 504.05\n",
      "Training network. lr: 0.000088. clip: 0.035036\n",
      "Iteration 21181: Policy loss: 0.186868. Value loss: 0.120358. Entropy: 0.286683.\n",
      "Iteration 21182: Policy loss: 0.189553. Value loss: 0.041884. Entropy: 0.287280.\n",
      "Iteration 21183: Policy loss: 0.182471. Value loss: 0.025163. Entropy: 0.286348.\n",
      "Training network. lr: 0.000088. clip: 0.035036\n",
      "Iteration 21184: Policy loss: -0.548666. Value loss: 0.330166. Entropy: 0.311566.\n",
      "Iteration 21185: Policy loss: -0.551332. Value loss: 0.166788. Entropy: 0.311682.\n",
      "Iteration 21186: Policy loss: -0.558388. Value loss: 0.094888. Entropy: 0.312086.\n",
      "episode: 7310   score: 450.0  epsilon: 1.0    steps: 240  evaluation reward: 505.35\n",
      "episode: 7311   score: 395.0  epsilon: 1.0    steps: 960  evaluation reward: 504.6\n",
      "Training network. lr: 0.000088. clip: 0.035036\n",
      "Iteration 21187: Policy loss: 0.144328. Value loss: 0.124394. Entropy: 0.290624.\n",
      "Iteration 21188: Policy loss: 0.143725. Value loss: 0.051300. Entropy: 0.291237.\n",
      "Iteration 21189: Policy loss: 0.137302. Value loss: 0.035527. Entropy: 0.292037.\n",
      "episode: 7312   score: 595.0  epsilon: 1.0    steps: 648  evaluation reward: 501.45\n",
      "episode: 7313   score: 565.0  epsilon: 1.0    steps: 872  evaluation reward: 502.85\n",
      "Training network. lr: 0.000088. clip: 0.035036\n",
      "Iteration 21190: Policy loss: 0.308783. Value loss: 0.145765. Entropy: 0.288565.\n",
      "Iteration 21191: Policy loss: 0.306666. Value loss: 0.057691. Entropy: 0.288654.\n",
      "Iteration 21192: Policy loss: 0.299263. Value loss: 0.040891. Entropy: 0.290505.\n",
      "Training network. lr: 0.000088. clip: 0.035036\n",
      "Iteration 21193: Policy loss: 0.106740. Value loss: 0.098945. Entropy: 0.310234.\n",
      "Iteration 21194: Policy loss: 0.106118. Value loss: 0.047462. Entropy: 0.309736.\n",
      "Iteration 21195: Policy loss: 0.108668. Value loss: 0.032111. Entropy: 0.309989.\n",
      "episode: 7314   score: 555.0  epsilon: 1.0    steps: 760  evaluation reward: 504.8\n",
      "Training network. lr: 0.000088. clip: 0.035036\n",
      "Iteration 21196: Policy loss: 0.017517. Value loss: 0.129468. Entropy: 0.292654.\n",
      "Iteration 21197: Policy loss: 0.017342. Value loss: 0.071718. Entropy: 0.292236.\n",
      "Iteration 21198: Policy loss: 0.016197. Value loss: 0.052955. Entropy: 0.291682.\n",
      "episode: 7315   score: 570.0  epsilon: 1.0    steps: 448  evaluation reward: 504.3\n",
      "Training network. lr: 0.000088. clip: 0.035036\n",
      "Iteration 21199: Policy loss: 0.243216. Value loss: 0.162695. Entropy: 0.304017.\n",
      "Iteration 21200: Policy loss: 0.243602. Value loss: 0.070073. Entropy: 0.302094.\n",
      "Iteration 21201: Policy loss: 0.251295. Value loss: 0.045499. Entropy: 0.302774.\n",
      "Training network. lr: 0.000087. clip: 0.034880\n",
      "Iteration 21202: Policy loss: 0.006779. Value loss: 0.086958. Entropy: 0.307552.\n",
      "Iteration 21203: Policy loss: 0.008677. Value loss: 0.039909. Entropy: 0.308038.\n",
      "Iteration 21204: Policy loss: 0.011068. Value loss: 0.029042. Entropy: 0.308161.\n",
      "episode: 7316   score: 355.0  epsilon: 1.0    steps: 160  evaluation reward: 499.15\n",
      "episode: 7317   score: 395.0  epsilon: 1.0    steps: 200  evaluation reward: 495.8\n",
      "Training network. lr: 0.000087. clip: 0.034880\n",
      "Iteration 21205: Policy loss: 0.093999. Value loss: 0.094731. Entropy: 0.284725.\n",
      "Iteration 21206: Policy loss: 0.093689. Value loss: 0.047486. Entropy: 0.284339.\n",
      "Iteration 21207: Policy loss: 0.092010. Value loss: 0.033212. Entropy: 0.283688.\n",
      "Training network. lr: 0.000087. clip: 0.034880\n",
      "Iteration 21208: Policy loss: -0.036473. Value loss: 0.062787. Entropy: 0.307184.\n",
      "Iteration 21209: Policy loss: -0.037702. Value loss: 0.027315. Entropy: 0.306904.\n",
      "Iteration 21210: Policy loss: -0.042593. Value loss: 0.020512. Entropy: 0.306512.\n",
      "episode: 7318   score: 375.0  epsilon: 1.0    steps: 480  evaluation reward: 492.5\n",
      "Training network. lr: 0.000087. clip: 0.034880\n",
      "Iteration 21211: Policy loss: -0.106146. Value loss: 0.244941. Entropy: 0.295289.\n",
      "Iteration 21212: Policy loss: -0.103558. Value loss: 0.096830. Entropy: 0.293711.\n",
      "Iteration 21213: Policy loss: -0.111171. Value loss: 0.064369. Entropy: 0.292349.\n",
      "Training network. lr: 0.000087. clip: 0.034880\n",
      "Iteration 21214: Policy loss: 0.055750. Value loss: 0.102740. Entropy: 0.320845.\n",
      "Iteration 21215: Policy loss: 0.050866. Value loss: 0.050816. Entropy: 0.320518.\n",
      "Iteration 21216: Policy loss: 0.054941. Value loss: 0.036159. Entropy: 0.320409.\n",
      "episode: 7319   score: 420.0  epsilon: 1.0    steps: 976  evaluation reward: 490.75\n",
      "Training network. lr: 0.000087. clip: 0.034880\n",
      "Iteration 21217: Policy loss: 0.097783. Value loss: 0.087421. Entropy: 0.308283.\n",
      "Iteration 21218: Policy loss: 0.097752. Value loss: 0.044506. Entropy: 0.307870.\n",
      "Iteration 21219: Policy loss: 0.097730. Value loss: 0.031650. Entropy: 0.308226.\n",
      "episode: 7320   score: 465.0  epsilon: 1.0    steps: 824  evaluation reward: 491.35\n",
      "Training network. lr: 0.000087. clip: 0.034880\n",
      "Iteration 21220: Policy loss: -0.112428. Value loss: 0.096826. Entropy: 0.297433.\n",
      "Iteration 21221: Policy loss: -0.116105. Value loss: 0.043261. Entropy: 0.298974.\n",
      "Iteration 21222: Policy loss: -0.118787. Value loss: 0.032907. Entropy: 0.299572.\n",
      "Training network. lr: 0.000087. clip: 0.034880\n",
      "Iteration 21223: Policy loss: 0.051463. Value loss: 0.128644. Entropy: 0.311106.\n",
      "Iteration 21224: Policy loss: 0.047532. Value loss: 0.046271. Entropy: 0.311011.\n",
      "Iteration 21225: Policy loss: 0.044056. Value loss: 0.030249. Entropy: 0.311286.\n",
      "episode: 7321   score: 740.0  epsilon: 1.0    steps: 144  evaluation reward: 492.8\n",
      "episode: 7322   score: 460.0  epsilon: 1.0    steps: 720  evaluation reward: 493.25\n",
      "episode: 7323   score: 390.0  epsilon: 1.0    steps: 760  evaluation reward: 493.55\n",
      "Training network. lr: 0.000087. clip: 0.034880\n",
      "Iteration 21226: Policy loss: 0.084883. Value loss: 0.066941. Entropy: 0.271725.\n",
      "Iteration 21227: Policy loss: 0.082986. Value loss: 0.037251. Entropy: 0.274005.\n",
      "Iteration 21228: Policy loss: 0.078519. Value loss: 0.032004. Entropy: 0.271872.\n",
      "episode: 7324   score: 405.0  epsilon: 1.0    steps: 56  evaluation reward: 493.4\n",
      "Training network. lr: 0.000087. clip: 0.034880\n",
      "Iteration 21229: Policy loss: -0.090274. Value loss: 0.265233. Entropy: 0.301534.\n",
      "Iteration 21230: Policy loss: -0.116087. Value loss: 0.212579. Entropy: 0.300903.\n",
      "Iteration 21231: Policy loss: -0.104792. Value loss: 0.177012. Entropy: 0.301107.\n",
      "episode: 7325   score: 680.0  epsilon: 1.0    steps: 408  evaluation reward: 497.5\n",
      "Training network. lr: 0.000087. clip: 0.034880\n",
      "Iteration 21232: Policy loss: 0.260169. Value loss: 0.160076. Entropy: 0.286736.\n",
      "Iteration 21233: Policy loss: 0.254625. Value loss: 0.068547. Entropy: 0.286719.\n",
      "Iteration 21234: Policy loss: 0.251296. Value loss: 0.046181. Entropy: 0.286630.\n",
      "episode: 7326   score: 545.0  epsilon: 1.0    steps: 288  evaluation reward: 495.5\n",
      "Training network. lr: 0.000087. clip: 0.034880\n",
      "Iteration 21235: Policy loss: 0.098693. Value loss: 0.065274. Entropy: 0.295728.\n",
      "Iteration 21236: Policy loss: 0.094192. Value loss: 0.037399. Entropy: 0.296059.\n",
      "Iteration 21237: Policy loss: 0.092634. Value loss: 0.030154. Entropy: 0.295211.\n",
      "episode: 7327   score: 335.0  epsilon: 1.0    steps: 1000  evaluation reward: 494.85\n",
      "Training network. lr: 0.000087. clip: 0.034880\n",
      "Iteration 21238: Policy loss: -0.037151. Value loss: 0.308809. Entropy: 0.308682.\n",
      "Iteration 21239: Policy loss: -0.078195. Value loss: 0.225251. Entropy: 0.307269.\n",
      "Iteration 21240: Policy loss: -0.073309. Value loss: 0.201738. Entropy: 0.308151.\n",
      "Training network. lr: 0.000087. clip: 0.034880\n",
      "Iteration 21241: Policy loss: -0.007999. Value loss: 0.113853. Entropy: 0.295432.\n",
      "Iteration 21242: Policy loss: -0.008108. Value loss: 0.049131. Entropy: 0.295435.\n",
      "Iteration 21243: Policy loss: -0.013235. Value loss: 0.033809. Entropy: 0.294963.\n",
      "episode: 7328   score: 245.0  epsilon: 1.0    steps: 440  evaluation reward: 494.45\n",
      "episode: 7329   score: 420.0  epsilon: 1.0    steps: 888  evaluation reward: 494.95\n",
      "Training network. lr: 0.000087. clip: 0.034880\n",
      "Iteration 21244: Policy loss: 0.260348. Value loss: 0.120239. Entropy: 0.292353.\n",
      "Iteration 21245: Policy loss: 0.253508. Value loss: 0.048892. Entropy: 0.290489.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21246: Policy loss: 0.257961. Value loss: 0.033949. Entropy: 0.292006.\n",
      "episode: 7330   score: 290.0  epsilon: 1.0    steps: 408  evaluation reward: 489.35\n",
      "Training network. lr: 0.000087. clip: 0.034880\n",
      "Iteration 21247: Policy loss: 0.142816. Value loss: 0.088578. Entropy: 0.286909.\n",
      "Iteration 21248: Policy loss: 0.141144. Value loss: 0.042949. Entropy: 0.286066.\n",
      "Iteration 21249: Policy loss: 0.141362. Value loss: 0.032773. Entropy: 0.286774.\n",
      "episode: 7331   score: 535.0  epsilon: 1.0    steps: 280  evaluation reward: 490.6\n",
      "episode: 7332   score: 260.0  epsilon: 1.0    steps: 1008  evaluation reward: 485.8\n",
      "Training network. lr: 0.000087. clip: 0.034880\n",
      "Iteration 21250: Policy loss: 0.010064. Value loss: 0.065829. Entropy: 0.293754.\n",
      "Iteration 21251: Policy loss: 0.004412. Value loss: 0.038662. Entropy: 0.294241.\n",
      "Iteration 21252: Policy loss: 0.005592. Value loss: 0.030896. Entropy: 0.294581.\n",
      "Training network. lr: 0.000087. clip: 0.034723\n",
      "Iteration 21253: Policy loss: -0.030556. Value loss: 0.037333. Entropy: 0.304513.\n",
      "Iteration 21254: Policy loss: -0.030050. Value loss: 0.023281. Entropy: 0.302807.\n",
      "Iteration 21255: Policy loss: -0.036015. Value loss: 0.019004. Entropy: 0.303224.\n",
      "Training network. lr: 0.000087. clip: 0.034723\n",
      "Iteration 21256: Policy loss: 0.035150. Value loss: 0.077385. Entropy: 0.305958.\n",
      "Iteration 21257: Policy loss: 0.030431. Value loss: 0.024740. Entropy: 0.305407.\n",
      "Iteration 21258: Policy loss: 0.029592. Value loss: 0.017666. Entropy: 0.306338.\n",
      "episode: 7333   score: 450.0  epsilon: 1.0    steps: 576  evaluation reward: 483.55\n",
      "episode: 7334   score: 315.0  epsilon: 1.0    steps: 888  evaluation reward: 481.9\n",
      "Training network. lr: 0.000087. clip: 0.034723\n",
      "Iteration 21259: Policy loss: -0.093211. Value loss: 0.343339. Entropy: 0.284000.\n",
      "Iteration 21260: Policy loss: -0.098552. Value loss: 0.195711. Entropy: 0.285041.\n",
      "Iteration 21261: Policy loss: -0.109374. Value loss: 0.147890. Entropy: 0.284522.\n",
      "Training network. lr: 0.000087. clip: 0.034723\n",
      "Iteration 21262: Policy loss: 0.015142. Value loss: 0.096852. Entropy: 0.305778.\n",
      "Iteration 21263: Policy loss: 0.009981. Value loss: 0.039673. Entropy: 0.306071.\n",
      "Iteration 21264: Policy loss: 0.014329. Value loss: 0.026344. Entropy: 0.307418.\n",
      "Training network. lr: 0.000087. clip: 0.034723\n",
      "Iteration 21265: Policy loss: -0.042886. Value loss: 0.115925. Entropy: 0.308349.\n",
      "Iteration 21266: Policy loss: -0.039162. Value loss: 0.050791. Entropy: 0.309141.\n",
      "Iteration 21267: Policy loss: -0.042392. Value loss: 0.036203. Entropy: 0.308874.\n",
      "episode: 7335   score: 360.0  epsilon: 1.0    steps: 800  evaluation reward: 481.8\n",
      "Training network. lr: 0.000087. clip: 0.034723\n",
      "Iteration 21268: Policy loss: 0.256968. Value loss: 0.191983. Entropy: 0.305449.\n",
      "Iteration 21269: Policy loss: 0.243256. Value loss: 0.065779. Entropy: 0.304725.\n",
      "Iteration 21270: Policy loss: 0.244501. Value loss: 0.037654. Entropy: 0.304631.\n",
      "episode: 7336   score: 450.0  epsilon: 1.0    steps: 48  evaluation reward: 483.0\n",
      "episode: 7337   score: 390.0  epsilon: 1.0    steps: 176  evaluation reward: 481.4\n",
      "Training network. lr: 0.000087. clip: 0.034723\n",
      "Iteration 21271: Policy loss: -0.047883. Value loss: 0.201965. Entropy: 0.279061.\n",
      "Iteration 21272: Policy loss: -0.054785. Value loss: 0.140270. Entropy: 0.279147.\n",
      "Iteration 21273: Policy loss: -0.056277. Value loss: 0.122917. Entropy: 0.278465.\n",
      "episode: 7338   score: 580.0  epsilon: 1.0    steps: 568  evaluation reward: 481.25\n",
      "Training network. lr: 0.000087. clip: 0.034723\n",
      "Iteration 21274: Policy loss: 0.159181. Value loss: 0.086243. Entropy: 0.291789.\n",
      "Iteration 21275: Policy loss: 0.161485. Value loss: 0.036665. Entropy: 0.292893.\n",
      "Iteration 21276: Policy loss: 0.151621. Value loss: 0.027794. Entropy: 0.293481.\n",
      "Training network. lr: 0.000087. clip: 0.034723\n",
      "Iteration 21277: Policy loss: 0.030149. Value loss: 0.071413. Entropy: 0.306152.\n",
      "Iteration 21278: Policy loss: 0.032666. Value loss: 0.025432. Entropy: 0.305048.\n",
      "Iteration 21279: Policy loss: 0.026519. Value loss: 0.018062. Entropy: 0.305680.\n",
      "episode: 7339   score: 500.0  epsilon: 1.0    steps: 808  evaluation reward: 478.15\n",
      "Training network. lr: 0.000087. clip: 0.034723\n",
      "Iteration 21280: Policy loss: -0.451246. Value loss: 0.609783. Entropy: 0.300577.\n",
      "Iteration 21281: Policy loss: -0.474365. Value loss: 0.405508. Entropy: 0.298233.\n",
      "Iteration 21282: Policy loss: -0.483751. Value loss: 0.200405. Entropy: 0.299101.\n",
      "episode: 7340   score: 440.0  epsilon: 1.0    steps: 88  evaluation reward: 476.95\n",
      "episode: 7341   score: 730.0  epsilon: 1.0    steps: 744  evaluation reward: 479.75\n",
      "episode: 7342   score: 415.0  epsilon: 1.0    steps: 1008  evaluation reward: 479.05\n",
      "Training network. lr: 0.000087. clip: 0.034723\n",
      "Iteration 21283: Policy loss: -0.430890. Value loss: 0.280259. Entropy: 0.283980.\n",
      "Iteration 21284: Policy loss: -0.426331. Value loss: 0.112532. Entropy: 0.285516.\n",
      "Iteration 21285: Policy loss: -0.445453. Value loss: 0.064286. Entropy: 0.286036.\n",
      "Training network. lr: 0.000087. clip: 0.034723\n",
      "Iteration 21286: Policy loss: 0.038695. Value loss: 0.103105. Entropy: 0.304197.\n",
      "Iteration 21287: Policy loss: 0.035278. Value loss: 0.048584. Entropy: 0.305490.\n",
      "Iteration 21288: Policy loss: 0.027798. Value loss: 0.035685. Entropy: 0.305122.\n",
      "Training network. lr: 0.000087. clip: 0.034723\n",
      "Iteration 21289: Policy loss: 0.374465. Value loss: 0.284626. Entropy: 0.299074.\n",
      "Iteration 21290: Policy loss: 0.349678. Value loss: 0.090676. Entropy: 0.299706.\n",
      "Iteration 21291: Policy loss: 0.365381. Value loss: 0.049267. Entropy: 0.299838.\n",
      "episode: 7343   score: 545.0  epsilon: 1.0    steps: 584  evaluation reward: 478.85\n",
      "Training network. lr: 0.000087. clip: 0.034723\n",
      "Iteration 21292: Policy loss: -0.185809. Value loss: 0.441159. Entropy: 0.299984.\n",
      "Iteration 21293: Policy loss: -0.198171. Value loss: 0.148015. Entropy: 0.299158.\n",
      "Iteration 21294: Policy loss: -0.213406. Value loss: 0.086392. Entropy: 0.298659.\n",
      "episode: 7344   score: 580.0  epsilon: 1.0    steps: 80  evaluation reward: 478.45\n",
      "episode: 7345   score: 670.0  epsilon: 1.0    steps: 376  evaluation reward: 477.4\n",
      "Training network. lr: 0.000087. clip: 0.034723\n",
      "Iteration 21295: Policy loss: 0.290085. Value loss: 0.171321. Entropy: 0.291068.\n",
      "Iteration 21296: Policy loss: 0.282719. Value loss: 0.056614. Entropy: 0.291024.\n",
      "Iteration 21297: Policy loss: 0.275985. Value loss: 0.038237. Entropy: 0.289301.\n",
      "Training network. lr: 0.000087. clip: 0.034723\n",
      "Iteration 21298: Policy loss: -0.018740. Value loss: 0.072462. Entropy: 0.308101.\n",
      "Iteration 21299: Policy loss: -0.018817. Value loss: 0.042560. Entropy: 0.308255.\n",
      "Iteration 21300: Policy loss: -0.020950. Value loss: 0.036829. Entropy: 0.307731.\n",
      "Training network. lr: 0.000086. clip: 0.034576\n",
      "Iteration 21301: Policy loss: -0.124752. Value loss: 0.321500. Entropy: 0.310151.\n",
      "Iteration 21302: Policy loss: -0.134996. Value loss: 0.216036. Entropy: 0.309820.\n",
      "Iteration 21303: Policy loss: -0.136385. Value loss: 0.169205. Entropy: 0.310921.\n",
      "episode: 7346   score: 405.0  epsilon: 1.0    steps: 688  evaluation reward: 475.95\n",
      "Training network. lr: 0.000086. clip: 0.034576\n",
      "Iteration 21304: Policy loss: -0.145136. Value loss: 0.553809. Entropy: 0.296660.\n",
      "Iteration 21305: Policy loss: -0.134561. Value loss: 0.268041. Entropy: 0.297923.\n",
      "Iteration 21306: Policy loss: -0.127745. Value loss: 0.201381. Entropy: 0.296775.\n",
      "episode: 7347   score: 370.0  epsilon: 1.0    steps: 824  evaluation reward: 476.3\n",
      "Training network. lr: 0.000086. clip: 0.034576\n",
      "Iteration 21307: Policy loss: 0.180042. Value loss: 0.097120. Entropy: 0.302782.\n",
      "Iteration 21308: Policy loss: 0.178336. Value loss: 0.054650. Entropy: 0.303371.\n",
      "Iteration 21309: Policy loss: 0.177647. Value loss: 0.043546. Entropy: 0.301547.\n",
      "Training network. lr: 0.000086. clip: 0.034576\n",
      "Iteration 21310: Policy loss: 0.215809. Value loss: 0.134435. Entropy: 0.307583.\n",
      "Iteration 21311: Policy loss: 0.210872. Value loss: 0.066416. Entropy: 0.306946.\n",
      "Iteration 21312: Policy loss: 0.210322. Value loss: 0.047615. Entropy: 0.306978.\n",
      "Training network. lr: 0.000086. clip: 0.034576\n",
      "Iteration 21313: Policy loss: -0.662661. Value loss: 0.504170. Entropy: 0.313031.\n",
      "Iteration 21314: Policy loss: -0.717042. Value loss: 0.293804. Entropy: 0.313900.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21315: Policy loss: -0.699032. Value loss: 0.181099. Entropy: 0.313344.\n",
      "episode: 7348   score: 670.0  epsilon: 1.0    steps: 112  evaluation reward: 476.8\n",
      "episode: 7349   score: 1200.0  epsilon: 1.0    steps: 320  evaluation reward: 485.5\n",
      "episode: 7350   score: 390.0  epsilon: 1.0    steps: 728  evaluation reward: 484.1\n",
      "now time :  2019-09-06 12:13:41.529406\n",
      "episode: 7351   score: 650.0  epsilon: 1.0    steps: 1016  evaluation reward: 486.1\n",
      "Training network. lr: 0.000086. clip: 0.034576\n",
      "Iteration 21316: Policy loss: 0.169411. Value loss: 0.124701. Entropy: 0.279749.\n",
      "Iteration 21317: Policy loss: 0.159000. Value loss: 0.069542. Entropy: 0.278492.\n",
      "Iteration 21318: Policy loss: 0.161020. Value loss: 0.047356. Entropy: 0.278096.\n",
      "episode: 7352   score: 605.0  epsilon: 1.0    steps: 200  evaluation reward: 488.25\n",
      "episode: 7353   score: 750.0  epsilon: 1.0    steps: 264  evaluation reward: 489.05\n",
      "Training network. lr: 0.000086. clip: 0.034576\n",
      "Iteration 21319: Policy loss: 0.008681. Value loss: 0.178149. Entropy: 0.268319.\n",
      "Iteration 21320: Policy loss: 0.015529. Value loss: 0.093532. Entropy: 0.268509.\n",
      "Iteration 21321: Policy loss: 0.002182. Value loss: 0.061183. Entropy: 0.269281.\n",
      "Training network. lr: 0.000086. clip: 0.034576\n",
      "Iteration 21322: Policy loss: 0.163255. Value loss: 0.144605. Entropy: 0.311197.\n",
      "Iteration 21323: Policy loss: 0.165856. Value loss: 0.081559. Entropy: 0.309664.\n",
      "Iteration 21324: Policy loss: 0.160663. Value loss: 0.065376. Entropy: 0.311564.\n",
      "Training network. lr: 0.000086. clip: 0.034576\n",
      "Iteration 21325: Policy loss: -0.165864. Value loss: 0.255212. Entropy: 0.307812.\n",
      "Iteration 21326: Policy loss: -0.175699. Value loss: 0.099947. Entropy: 0.308503.\n",
      "Iteration 21327: Policy loss: -0.183844. Value loss: 0.059961. Entropy: 0.308411.\n",
      "episode: 7354   score: 575.0  epsilon: 1.0    steps: 144  evaluation reward: 484.95\n",
      "Training network. lr: 0.000086. clip: 0.034576\n",
      "Iteration 21328: Policy loss: 0.082348. Value loss: 0.270514. Entropy: 0.294709.\n",
      "Iteration 21329: Policy loss: 0.080863. Value loss: 0.111797. Entropy: 0.294643.\n",
      "Iteration 21330: Policy loss: 0.088434. Value loss: 0.076937. Entropy: 0.294396.\n",
      "episode: 7355   score: 520.0  epsilon: 1.0    steps: 24  evaluation reward: 488.0\n",
      "Training network. lr: 0.000086. clip: 0.034576\n",
      "Iteration 21331: Policy loss: -0.262881. Value loss: 0.176855. Entropy: 0.284745.\n",
      "Iteration 21332: Policy loss: -0.266910. Value loss: 0.133366. Entropy: 0.284984.\n",
      "Iteration 21333: Policy loss: -0.268401. Value loss: 0.114739. Entropy: 0.283647.\n",
      "Training network. lr: 0.000086. clip: 0.034576\n",
      "Iteration 21334: Policy loss: 0.265611. Value loss: 0.177397. Entropy: 0.308179.\n",
      "Iteration 21335: Policy loss: 0.268758. Value loss: 0.106882. Entropy: 0.307595.\n",
      "Iteration 21336: Policy loss: 0.282772. Value loss: 0.070701. Entropy: 0.307739.\n",
      "episode: 7356   score: 530.0  epsilon: 1.0    steps: 488  evaluation reward: 489.95\n",
      "Training network. lr: 0.000086. clip: 0.034576\n",
      "Iteration 21337: Policy loss: -0.065113. Value loss: 0.119560. Entropy: 0.296435.\n",
      "Iteration 21338: Policy loss: -0.069090. Value loss: 0.063824. Entropy: 0.297237.\n",
      "Iteration 21339: Policy loss: -0.066627. Value loss: 0.047148. Entropy: 0.298145.\n",
      "episode: 7357   score: 320.0  epsilon: 1.0    steps: 320  evaluation reward: 489.85\n",
      "episode: 7358   score: 635.0  epsilon: 1.0    steps: 656  evaluation reward: 492.1\n",
      "episode: 7359   score: 395.0  epsilon: 1.0    steps: 688  evaluation reward: 492.35\n",
      "Training network. lr: 0.000086. clip: 0.034576\n",
      "Iteration 21340: Policy loss: -0.030584. Value loss: 0.068279. Entropy: 0.275614.\n",
      "Iteration 21341: Policy loss: -0.034554. Value loss: 0.033652. Entropy: 0.275746.\n",
      "Iteration 21342: Policy loss: -0.031447. Value loss: 0.027036. Entropy: 0.274828.\n",
      "episode: 7360   score: 590.0  epsilon: 1.0    steps: 216  evaluation reward: 495.55\n",
      "Training network. lr: 0.000086. clip: 0.034576\n",
      "Iteration 21343: Policy loss: 0.095905. Value loss: 0.108416. Entropy: 0.291268.\n",
      "Iteration 21344: Policy loss: 0.090996. Value loss: 0.049375. Entropy: 0.291201.\n",
      "Iteration 21345: Policy loss: 0.096449. Value loss: 0.036188. Entropy: 0.290381.\n",
      "Training network. lr: 0.000086. clip: 0.034576\n",
      "Iteration 21346: Policy loss: 0.112766. Value loss: 0.440199. Entropy: 0.309041.\n",
      "Iteration 21347: Policy loss: 0.084084. Value loss: 0.158240. Entropy: 0.306025.\n",
      "Iteration 21348: Policy loss: 0.108906. Value loss: 0.084009. Entropy: 0.306855.\n",
      "episode: 7361   score: 350.0  epsilon: 1.0    steps: 368  evaluation reward: 490.1\n",
      "Training network. lr: 0.000086. clip: 0.034576\n",
      "Iteration 21349: Policy loss: 0.119726. Value loss: 0.199446. Entropy: 0.295804.\n",
      "Iteration 21350: Policy loss: 0.118313. Value loss: 0.061423. Entropy: 0.295387.\n",
      "Iteration 21351: Policy loss: 0.115065. Value loss: 0.038863. Entropy: 0.294727.\n",
      "episode: 7362   score: 665.0  epsilon: 1.0    steps: 584  evaluation reward: 492.25\n",
      "Training network. lr: 0.000086. clip: 0.034419\n",
      "Iteration 21352: Policy loss: 0.753462. Value loss: 0.342192. Entropy: 0.291159.\n",
      "Iteration 21353: Policy loss: 0.741966. Value loss: 0.092252. Entropy: 0.288733.\n",
      "Iteration 21354: Policy loss: 0.733670. Value loss: 0.055501. Entropy: 0.290088.\n",
      "Training network. lr: 0.000086. clip: 0.034419\n",
      "Iteration 21355: Policy loss: 0.219819. Value loss: 0.112199. Entropy: 0.312450.\n",
      "Iteration 21356: Policy loss: 0.216385. Value loss: 0.050140. Entropy: 0.312164.\n",
      "Iteration 21357: Policy loss: 0.221027. Value loss: 0.037452. Entropy: 0.312090.\n",
      "episode: 7363   score: 290.0  epsilon: 1.0    steps: 752  evaluation reward: 487.2\n",
      "Training network. lr: 0.000086. clip: 0.034419\n",
      "Iteration 21358: Policy loss: -0.003929. Value loss: 0.334035. Entropy: 0.303012.\n",
      "Iteration 21359: Policy loss: 0.002857. Value loss: 0.165488. Entropy: 0.302131.\n",
      "Iteration 21360: Policy loss: 0.000583. Value loss: 0.102217. Entropy: 0.302468.\n",
      "episode: 7364   score: 210.0  epsilon: 1.0    steps: 512  evaluation reward: 483.25\n",
      "Training network. lr: 0.000086. clip: 0.034419\n",
      "Iteration 21361: Policy loss: -0.038279. Value loss: 0.149324. Entropy: 0.295776.\n",
      "Iteration 21362: Policy loss: -0.036504. Value loss: 0.073621. Entropy: 0.294214.\n",
      "Iteration 21363: Policy loss: -0.029210. Value loss: 0.041326. Entropy: 0.295012.\n",
      "episode: 7365   score: 910.0  epsilon: 1.0    steps: 88  evaluation reward: 488.5\n",
      "Training network. lr: 0.000086. clip: 0.034419\n",
      "Iteration 21364: Policy loss: 0.153431. Value loss: 0.198697. Entropy: 0.298778.\n",
      "Iteration 21365: Policy loss: 0.156891. Value loss: 0.052834. Entropy: 0.296936.\n",
      "Iteration 21366: Policy loss: 0.140763. Value loss: 0.035432. Entropy: 0.299000.\n",
      "Training network. lr: 0.000086. clip: 0.034419\n",
      "Iteration 21367: Policy loss: -0.420270. Value loss: 0.495619. Entropy: 0.309280.\n",
      "Iteration 21368: Policy loss: -0.436081. Value loss: 0.219118. Entropy: 0.309192.\n",
      "Iteration 21369: Policy loss: -0.437995. Value loss: 0.151768. Entropy: 0.310176.\n",
      "Training network. lr: 0.000086. clip: 0.034419\n",
      "Iteration 21370: Policy loss: 0.025849. Value loss: 0.414478. Entropy: 0.311634.\n",
      "Iteration 21371: Policy loss: 0.026253. Value loss: 0.136524. Entropy: 0.311873.\n",
      "Iteration 21372: Policy loss: 0.009051. Value loss: 0.088922. Entropy: 0.310943.\n",
      "episode: 7366   score: 725.0  epsilon: 1.0    steps: 144  evaluation reward: 492.3\n",
      "Training network. lr: 0.000086. clip: 0.034419\n",
      "Iteration 21373: Policy loss: 0.183430. Value loss: 0.099635. Entropy: 0.300283.\n",
      "Iteration 21374: Policy loss: 0.184685. Value loss: 0.041697. Entropy: 0.299823.\n",
      "Iteration 21375: Policy loss: 0.190424. Value loss: 0.025940. Entropy: 0.301132.\n",
      "episode: 7367   score: 370.0  epsilon: 1.0    steps: 440  evaluation reward: 492.95\n",
      "Training network. lr: 0.000086. clip: 0.034419\n",
      "Iteration 21376: Policy loss: 0.104223. Value loss: 0.127089. Entropy: 0.296567.\n",
      "Iteration 21377: Policy loss: 0.098283. Value loss: 0.059001. Entropy: 0.297332.\n",
      "Iteration 21378: Policy loss: 0.098171. Value loss: 0.042822. Entropy: 0.296147.\n",
      "episode: 7368   score: 650.0  epsilon: 1.0    steps: 64  evaluation reward: 496.0\n",
      "Training network. lr: 0.000086. clip: 0.034419\n",
      "Iteration 21379: Policy loss: -0.026129. Value loss: 0.171717. Entropy: 0.297880.\n",
      "Iteration 21380: Policy loss: -0.029132. Value loss: 0.068633. Entropy: 0.296953.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21381: Policy loss: -0.039286. Value loss: 0.044380. Entropy: 0.296407.\n",
      "episode: 7369   score: 595.0  epsilon: 1.0    steps: 544  evaluation reward: 491.95\n",
      "episode: 7370   score: 545.0  epsilon: 1.0    steps: 792  evaluation reward: 493.95\n",
      "episode: 7371   score: 485.0  epsilon: 1.0    steps: 808  evaluation reward: 492.1\n",
      "Training network. lr: 0.000086. clip: 0.034419\n",
      "Iteration 21382: Policy loss: 0.290732. Value loss: 0.201298. Entropy: 0.281008.\n",
      "Iteration 21383: Policy loss: 0.286918. Value loss: 0.065544. Entropy: 0.279478.\n",
      "Iteration 21384: Policy loss: 0.279547. Value loss: 0.052298. Entropy: 0.280052.\n",
      "episode: 7372   score: 695.0  epsilon: 1.0    steps: 96  evaluation reward: 493.35\n",
      "episode: 7373   score: 345.0  epsilon: 1.0    steps: 352  evaluation reward: 493.9\n",
      "Training network. lr: 0.000086. clip: 0.034419\n",
      "Iteration 21385: Policy loss: -0.032910. Value loss: 0.176306. Entropy: 0.276202.\n",
      "Iteration 21386: Policy loss: -0.029981. Value loss: 0.078870. Entropy: 0.275970.\n",
      "Iteration 21387: Policy loss: -0.037487. Value loss: 0.055125. Entropy: 0.275845.\n",
      "Training network. lr: 0.000086. clip: 0.034419\n",
      "Iteration 21388: Policy loss: 0.145113. Value loss: 0.127556. Entropy: 0.307331.\n",
      "Iteration 21389: Policy loss: 0.133451. Value loss: 0.045610. Entropy: 0.307133.\n",
      "Iteration 21390: Policy loss: 0.131762. Value loss: 0.033376. Entropy: 0.306250.\n",
      "Training network. lr: 0.000086. clip: 0.034419\n",
      "Iteration 21391: Policy loss: 0.001775. Value loss: 0.292818. Entropy: 0.310175.\n",
      "Iteration 21392: Policy loss: -0.003160. Value loss: 0.140804. Entropy: 0.308743.\n",
      "Iteration 21393: Policy loss: -0.001033. Value loss: 0.085630. Entropy: 0.309852.\n",
      "Training network. lr: 0.000086. clip: 0.034419\n",
      "Iteration 21394: Policy loss: 0.643619. Value loss: 0.252506. Entropy: 0.307133.\n",
      "Iteration 21395: Policy loss: 0.635201. Value loss: 0.079780. Entropy: 0.305819.\n",
      "Iteration 21396: Policy loss: 0.623265. Value loss: 0.053571. Entropy: 0.305834.\n",
      "episode: 7374   score: 370.0  epsilon: 1.0    steps: 424  evaluation reward: 492.9\n",
      "Training network. lr: 0.000086. clip: 0.034419\n",
      "Iteration 21397: Policy loss: 0.186995. Value loss: 0.106638. Entropy: 0.295889.\n",
      "Iteration 21398: Policy loss: 0.191468. Value loss: 0.059609. Entropy: 0.296732.\n",
      "Iteration 21399: Policy loss: 0.181561. Value loss: 0.048865. Entropy: 0.296112.\n",
      "Training network. lr: 0.000086. clip: 0.034419\n",
      "Iteration 21400: Policy loss: -0.170692. Value loss: 0.117020. Entropy: 0.308416.\n",
      "Iteration 21401: Policy loss: -0.169245. Value loss: 0.065920. Entropy: 0.307840.\n",
      "Iteration 21402: Policy loss: -0.166881. Value loss: 0.047824. Entropy: 0.308848.\n",
      "Training network. lr: 0.000086. clip: 0.034262\n",
      "Iteration 21403: Policy loss: 0.068483. Value loss: 0.138329. Entropy: 0.310710.\n",
      "Iteration 21404: Policy loss: 0.056113. Value loss: 0.062462. Entropy: 0.309793.\n",
      "Iteration 21405: Policy loss: 0.059138. Value loss: 0.040368. Entropy: 0.309186.\n",
      "episode: 7375   score: 350.0  epsilon: 1.0    steps: 360  evaluation reward: 493.1\n",
      "Training network. lr: 0.000086. clip: 0.034262\n",
      "Iteration 21406: Policy loss: 0.098153. Value loss: 0.100142. Entropy: 0.298063.\n",
      "Iteration 21407: Policy loss: 0.098685. Value loss: 0.036544. Entropy: 0.298385.\n",
      "Iteration 21408: Policy loss: 0.091378. Value loss: 0.026056. Entropy: 0.298670.\n",
      "episode: 7376   score: 405.0  epsilon: 1.0    steps: 448  evaluation reward: 492.75\n",
      "episode: 7377   score: 435.0  epsilon: 1.0    steps: 776  evaluation reward: 494.25\n",
      "Training network. lr: 0.000086. clip: 0.034262\n",
      "Iteration 21409: Policy loss: -0.320654. Value loss: 0.367684. Entropy: 0.283528.\n",
      "Iteration 21410: Policy loss: -0.354461. Value loss: 0.221841. Entropy: 0.284491.\n",
      "Iteration 21411: Policy loss: -0.357294. Value loss: 0.154702. Entropy: 0.284485.\n",
      "episode: 7378   score: 640.0  epsilon: 1.0    steps: 208  evaluation reward: 496.55\n",
      "Training network. lr: 0.000086. clip: 0.034262\n",
      "Iteration 21412: Policy loss: -0.358838. Value loss: 0.272860. Entropy: 0.291721.\n",
      "Iteration 21413: Policy loss: -0.357874. Value loss: 0.157356. Entropy: 0.290568.\n",
      "Iteration 21414: Policy loss: -0.370259. Value loss: 0.105951. Entropy: 0.292284.\n",
      "Training network. lr: 0.000086. clip: 0.034262\n",
      "Iteration 21415: Policy loss: 0.309088. Value loss: 0.193139. Entropy: 0.310261.\n",
      "Iteration 21416: Policy loss: 0.320455. Value loss: 0.087528. Entropy: 0.309913.\n",
      "Iteration 21417: Policy loss: 0.306236. Value loss: 0.054614. Entropy: 0.309535.\n",
      "episode: 7379   score: 470.0  epsilon: 1.0    steps: 224  evaluation reward: 498.35\n",
      "episode: 7380   score: 785.0  epsilon: 1.0    steps: 264  evaluation reward: 500.8\n",
      "Training network. lr: 0.000086. clip: 0.034262\n",
      "Iteration 21418: Policy loss: 0.153538. Value loss: 0.192106. Entropy: 0.280462.\n",
      "Iteration 21419: Policy loss: 0.165460. Value loss: 0.087132. Entropy: 0.282251.\n",
      "Iteration 21420: Policy loss: 0.154603. Value loss: 0.054146. Entropy: 0.282821.\n",
      "Training network. lr: 0.000086. clip: 0.034262\n",
      "Iteration 21421: Policy loss: -0.045322. Value loss: 0.233695. Entropy: 0.307813.\n",
      "Iteration 21422: Policy loss: -0.034734. Value loss: 0.089703. Entropy: 0.307945.\n",
      "Iteration 21423: Policy loss: -0.052947. Value loss: 0.041981. Entropy: 0.308029.\n",
      "episode: 7381   score: 770.0  epsilon: 1.0    steps: 624  evaluation reward: 504.7\n",
      "Training network. lr: 0.000086. clip: 0.034262\n",
      "Iteration 21424: Policy loss: -0.261717. Value loss: 0.313321. Entropy: 0.291943.\n",
      "Iteration 21425: Policy loss: -0.287030. Value loss: 0.204991. Entropy: 0.292984.\n",
      "Iteration 21426: Policy loss: -0.284528. Value loss: 0.155829. Entropy: 0.292674.\n",
      "Training network. lr: 0.000086. clip: 0.034262\n",
      "Iteration 21427: Policy loss: 0.123158. Value loss: 0.198584. Entropy: 0.307303.\n",
      "Iteration 21428: Policy loss: 0.123543. Value loss: 0.064994. Entropy: 0.306572.\n",
      "Iteration 21429: Policy loss: 0.115385. Value loss: 0.038899. Entropy: 0.308089.\n",
      "episode: 7382   score: 670.0  epsilon: 1.0    steps: 576  evaluation reward: 507.65\n",
      "Training network. lr: 0.000086. clip: 0.034262\n",
      "Iteration 21430: Policy loss: -0.004335. Value loss: 0.198557. Entropy: 0.299690.\n",
      "Iteration 21431: Policy loss: -0.012500. Value loss: 0.079836. Entropy: 0.301437.\n",
      "Iteration 21432: Policy loss: -0.011471. Value loss: 0.058485. Entropy: 0.299476.\n",
      "episode: 7383   score: 580.0  epsilon: 1.0    steps: 784  evaluation reward: 510.0\n",
      "Training network. lr: 0.000086. clip: 0.034262\n",
      "Iteration 21433: Policy loss: 0.147401. Value loss: 0.094704. Entropy: 0.304797.\n",
      "Iteration 21434: Policy loss: 0.140776. Value loss: 0.044034. Entropy: 0.304034.\n",
      "Iteration 21435: Policy loss: 0.134143. Value loss: 0.032927. Entropy: 0.302776.\n",
      "Training network. lr: 0.000086. clip: 0.034262\n",
      "Iteration 21436: Policy loss: 0.103302. Value loss: 0.062537. Entropy: 0.308524.\n",
      "Iteration 21437: Policy loss: 0.100919. Value loss: 0.028381. Entropy: 0.308047.\n",
      "Iteration 21438: Policy loss: 0.100675. Value loss: 0.019795. Entropy: 0.307849.\n",
      "episode: 7384   score: 785.0  epsilon: 1.0    steps: 832  evaluation reward: 513.1\n",
      "Training network. lr: 0.000086. clip: 0.034262\n",
      "Iteration 21439: Policy loss: -0.136178. Value loss: 0.203520. Entropy: 0.305233.\n",
      "Iteration 21440: Policy loss: -0.130514. Value loss: 0.073216. Entropy: 0.305544.\n",
      "Iteration 21441: Policy loss: -0.136154. Value loss: 0.048889. Entropy: 0.305301.\n",
      "episode: 7385   score: 450.0  epsilon: 1.0    steps: 312  evaluation reward: 511.0\n",
      "episode: 7386   score: 530.0  epsilon: 1.0    steps: 352  evaluation reward: 511.6\n",
      "episode: 7387   score: 475.0  epsilon: 1.0    steps: 360  evaluation reward: 510.15\n",
      "episode: 7388   score: 295.0  epsilon: 1.0    steps: 416  evaluation reward: 509.05\n",
      "Training network. lr: 0.000086. clip: 0.034262\n",
      "Iteration 21442: Policy loss: -0.096919. Value loss: 0.277163. Entropy: 0.255950.\n",
      "Iteration 21443: Policy loss: -0.094856. Value loss: 0.167958. Entropy: 0.254147.\n",
      "Iteration 21444: Policy loss: -0.086837. Value loss: 0.120502. Entropy: 0.253110.\n",
      "Training network. lr: 0.000086. clip: 0.034262\n",
      "Iteration 21445: Policy loss: 0.071195. Value loss: 0.097335. Entropy: 0.307381.\n",
      "Iteration 21446: Policy loss: 0.062280. Value loss: 0.044037. Entropy: 0.307166.\n",
      "Iteration 21447: Policy loss: 0.057803. Value loss: 0.034863. Entropy: 0.307906.\n",
      "episode: 7389   score: 550.0  epsilon: 1.0    steps: 448  evaluation reward: 508.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000086. clip: 0.034262\n",
      "Iteration 21448: Policy loss: 0.135193. Value loss: 0.149941. Entropy: 0.290755.\n",
      "Iteration 21449: Policy loss: 0.139241. Value loss: 0.069731. Entropy: 0.290713.\n",
      "Iteration 21450: Policy loss: 0.132942. Value loss: 0.049753. Entropy: 0.290606.\n",
      "episode: 7390   score: 220.0  epsilon: 1.0    steps: 592  evaluation reward: 504.9\n",
      "episode: 7391   score: 580.0  epsilon: 1.0    steps: 992  evaluation reward: 503.1\n",
      "Training network. lr: 0.000085. clip: 0.034115\n",
      "Iteration 21451: Policy loss: 0.243366. Value loss: 0.253280. Entropy: 0.287485.\n",
      "Iteration 21452: Policy loss: 0.232031. Value loss: 0.117064. Entropy: 0.287294.\n",
      "Iteration 21453: Policy loss: 0.224716. Value loss: 0.074830. Entropy: 0.288174.\n",
      "Training network. lr: 0.000085. clip: 0.034115\n",
      "Iteration 21454: Policy loss: -0.059419. Value loss: 0.143175. Entropy: 0.286633.\n",
      "Iteration 21455: Policy loss: -0.072656. Value loss: 0.061790. Entropy: 0.287850.\n",
      "Iteration 21456: Policy loss: -0.071137. Value loss: 0.046808. Entropy: 0.287552.\n",
      "Training network. lr: 0.000085. clip: 0.034115\n",
      "Iteration 21457: Policy loss: 0.270063. Value loss: 0.091253. Entropy: 0.312399.\n",
      "Iteration 21458: Policy loss: 0.259743. Value loss: 0.042034. Entropy: 0.313702.\n",
      "Iteration 21459: Policy loss: 0.260225. Value loss: 0.033935. Entropy: 0.311982.\n",
      "episode: 7392   score: 345.0  epsilon: 1.0    steps: 912  evaluation reward: 500.15\n",
      "Training network. lr: 0.000085. clip: 0.034115\n",
      "Iteration 21460: Policy loss: 0.105361. Value loss: 0.084816. Entropy: 0.304706.\n",
      "Iteration 21461: Policy loss: 0.099539. Value loss: 0.051199. Entropy: 0.305439.\n",
      "Iteration 21462: Policy loss: 0.097965. Value loss: 0.040333. Entropy: 0.305037.\n",
      "Training network. lr: 0.000085. clip: 0.034115\n",
      "Iteration 21463: Policy loss: 0.008487. Value loss: 0.140870. Entropy: 0.303271.\n",
      "Iteration 21464: Policy loss: 0.007846. Value loss: 0.055699. Entropy: 0.302733.\n",
      "Iteration 21465: Policy loss: 0.005303. Value loss: 0.036014. Entropy: 0.303835.\n",
      "Training network. lr: 0.000085. clip: 0.034115\n",
      "Iteration 21466: Policy loss: -0.124666. Value loss: 0.073983. Entropy: 0.306434.\n",
      "Iteration 21467: Policy loss: -0.122233. Value loss: 0.033582. Entropy: 0.308185.\n",
      "Iteration 21468: Policy loss: -0.128601. Value loss: 0.021615. Entropy: 0.307602.\n",
      "episode: 7393   score: 370.0  epsilon: 1.0    steps: 40  evaluation reward: 498.6\n",
      "episode: 7394   score: 345.0  epsilon: 1.0    steps: 568  evaluation reward: 498.4\n",
      "Training network. lr: 0.000085. clip: 0.034115\n",
      "Iteration 21469: Policy loss: -0.446453. Value loss: 0.342721. Entropy: 0.288513.\n",
      "Iteration 21470: Policy loss: -0.449286. Value loss: 0.166531. Entropy: 0.288722.\n",
      "Iteration 21471: Policy loss: -0.448899. Value loss: 0.113280. Entropy: 0.288165.\n",
      "Training network. lr: 0.000085. clip: 0.034115\n",
      "Iteration 21472: Policy loss: -0.104725. Value loss: 0.121986. Entropy: 0.313341.\n",
      "Iteration 21473: Policy loss: -0.108031. Value loss: 0.049665. Entropy: 0.313588.\n",
      "Iteration 21474: Policy loss: -0.110460. Value loss: 0.035297. Entropy: 0.314453.\n",
      "episode: 7395   score: 470.0  epsilon: 1.0    steps: 768  evaluation reward: 496.4\n",
      "episode: 7396   score: 675.0  epsilon: 1.0    steps: 904  evaluation reward: 497.7\n",
      "episode: 7397   score: 555.0  epsilon: 1.0    steps: 920  evaluation reward: 499.35\n",
      "episode: 7398   score: 770.0  epsilon: 1.0    steps: 960  evaluation reward: 500.95\n",
      "Training network. lr: 0.000085. clip: 0.034115\n",
      "Iteration 21475: Policy loss: -0.098838. Value loss: 0.288050. Entropy: 0.298427.\n",
      "Iteration 21476: Policy loss: -0.102178. Value loss: 0.117420. Entropy: 0.299304.\n",
      "Iteration 21477: Policy loss: -0.121219. Value loss: 0.085978. Entropy: 0.297992.\n",
      "Training network. lr: 0.000085. clip: 0.034115\n",
      "Iteration 21478: Policy loss: -0.501243. Value loss: 0.298869. Entropy: 0.291909.\n",
      "Iteration 21479: Policy loss: -0.491307. Value loss: 0.142009. Entropy: 0.288982.\n",
      "Iteration 21480: Policy loss: -0.475963. Value loss: 0.072422. Entropy: 0.289267.\n",
      "episode: 7399   score: 360.0  epsilon: 1.0    steps: 832  evaluation reward: 499.1\n",
      "Training network. lr: 0.000085. clip: 0.034115\n",
      "Iteration 21481: Policy loss: 0.124230. Value loss: 0.203056. Entropy: 0.299519.\n",
      "Iteration 21482: Policy loss: 0.121519. Value loss: 0.075620. Entropy: 0.299517.\n",
      "Iteration 21483: Policy loss: 0.114982. Value loss: 0.048192. Entropy: 0.297334.\n",
      "Training network. lr: 0.000085. clip: 0.034115\n",
      "Iteration 21484: Policy loss: 0.293667. Value loss: 0.228468. Entropy: 0.300663.\n",
      "Iteration 21485: Policy loss: 0.273074. Value loss: 0.066146. Entropy: 0.298853.\n",
      "Iteration 21486: Policy loss: 0.275384. Value loss: 0.041301. Entropy: 0.298505.\n",
      "episode: 7400   score: 525.0  epsilon: 1.0    steps: 848  evaluation reward: 501.75\n",
      "Training network. lr: 0.000085. clip: 0.034115\n",
      "Iteration 21487: Policy loss: -0.028153. Value loss: 0.231367. Entropy: 0.304223.\n",
      "Iteration 21488: Policy loss: -0.026043. Value loss: 0.081695. Entropy: 0.304814.\n",
      "Iteration 21489: Policy loss: -0.037201. Value loss: 0.039162. Entropy: 0.303993.\n",
      "Training network. lr: 0.000085. clip: 0.034115\n",
      "Iteration 21490: Policy loss: -0.096848. Value loss: 0.170335. Entropy: 0.294975.\n",
      "Iteration 21491: Policy loss: -0.111444. Value loss: 0.089411. Entropy: 0.294594.\n",
      "Iteration 21492: Policy loss: -0.118955. Value loss: 0.063943. Entropy: 0.294829.\n",
      "now time :  2019-09-06 12:24:30.457914\n",
      "episode: 7401   score: 600.0  epsilon: 1.0    steps: 976  evaluation reward: 503.7\n",
      "Training network. lr: 0.000085. clip: 0.034115\n",
      "Iteration 21493: Policy loss: 0.901725. Value loss: 0.383437. Entropy: 0.310015.\n",
      "Iteration 21494: Policy loss: 0.903005. Value loss: 0.122063. Entropy: 0.310390.\n",
      "Iteration 21495: Policy loss: 0.873896. Value loss: 0.068659. Entropy: 0.310105.\n",
      "Training network. lr: 0.000085. clip: 0.034115\n",
      "Iteration 21496: Policy loss: -0.084222. Value loss: 0.154650. Entropy: 0.297333.\n",
      "Iteration 21497: Policy loss: -0.084505. Value loss: 0.054773. Entropy: 0.296518.\n",
      "Iteration 21498: Policy loss: -0.092495. Value loss: 0.039992. Entropy: 0.296847.\n",
      "episode: 7402   score: 395.0  epsilon: 1.0    steps: 544  evaluation reward: 504.15\n",
      "episode: 7403   score: 395.0  epsilon: 1.0    steps: 936  evaluation reward: 500.5\n",
      "episode: 7404   score: 390.0  epsilon: 1.0    steps: 976  evaluation reward: 501.45\n",
      "Training network. lr: 0.000085. clip: 0.034115\n",
      "Iteration 21499: Policy loss: 0.379145. Value loss: 0.196466. Entropy: 0.296081.\n",
      "Iteration 21500: Policy loss: 0.380137. Value loss: 0.092716. Entropy: 0.295538.\n",
      "Iteration 21501: Policy loss: 0.377682. Value loss: 0.064598. Entropy: 0.294974.\n",
      "episode: 7405   score: 155.0  epsilon: 1.0    steps: 80  evaluation reward: 497.1\n",
      "Training network. lr: 0.000085. clip: 0.033958\n",
      "Iteration 21502: Policy loss: -0.056458. Value loss: 0.110564. Entropy: 0.282608.\n",
      "Iteration 21503: Policy loss: -0.062357. Value loss: 0.064582. Entropy: 0.281189.\n",
      "Iteration 21504: Policy loss: -0.056689. Value loss: 0.049070. Entropy: 0.281480.\n",
      "Training network. lr: 0.000085. clip: 0.033958\n",
      "Iteration 21505: Policy loss: 0.394774. Value loss: 0.179885. Entropy: 0.309340.\n",
      "Iteration 21506: Policy loss: 0.400021. Value loss: 0.062826. Entropy: 0.309062.\n",
      "Iteration 21507: Policy loss: 0.401461. Value loss: 0.044136. Entropy: 0.307634.\n",
      "Training network. lr: 0.000085. clip: 0.033958\n",
      "Iteration 21508: Policy loss: 0.023845. Value loss: 0.101096. Entropy: 0.312383.\n",
      "Iteration 21509: Policy loss: 0.021982. Value loss: 0.039745. Entropy: 0.313102.\n",
      "Iteration 21510: Policy loss: 0.017410. Value loss: 0.027225. Entropy: 0.312866.\n",
      "episode: 7406   score: 475.0  epsilon: 1.0    steps: 520  evaluation reward: 495.9\n",
      "episode: 7407   score: 760.0  epsilon: 1.0    steps: 920  evaluation reward: 499.25\n",
      "Training network. lr: 0.000085. clip: 0.033958\n",
      "Iteration 21511: Policy loss: 0.281360. Value loss: 0.157164. Entropy: 0.291787.\n",
      "Iteration 21512: Policy loss: 0.288667. Value loss: 0.055839. Entropy: 0.291134.\n",
      "Iteration 21513: Policy loss: 0.277185. Value loss: 0.040928. Entropy: 0.291252.\n",
      "Training network. lr: 0.000085. clip: 0.033958\n",
      "Iteration 21514: Policy loss: 0.027705. Value loss: 0.128589. Entropy: 0.300619.\n",
      "Iteration 21515: Policy loss: 0.022197. Value loss: 0.070866. Entropy: 0.299544.\n",
      "Iteration 21516: Policy loss: 0.021287. Value loss: 0.048544. Entropy: 0.299678.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7408   score: 425.0  epsilon: 1.0    steps: 168  evaluation reward: 500.0\n",
      "Training network. lr: 0.000085. clip: 0.033958\n",
      "Iteration 21517: Policy loss: 0.010078. Value loss: 0.097267. Entropy: 0.304921.\n",
      "Iteration 21518: Policy loss: 0.009936. Value loss: 0.037862. Entropy: 0.304198.\n",
      "Iteration 21519: Policy loss: 0.005803. Value loss: 0.028630. Entropy: 0.303631.\n",
      "Training network. lr: 0.000085. clip: 0.033958\n",
      "Iteration 21520: Policy loss: 0.124930. Value loss: 0.166508. Entropy: 0.304696.\n",
      "Iteration 21521: Policy loss: 0.127222. Value loss: 0.071764. Entropy: 0.305524.\n",
      "Iteration 21522: Policy loss: 0.120294. Value loss: 0.053877. Entropy: 0.305054.\n",
      "episode: 7409   score: 355.0  epsilon: 1.0    steps: 48  evaluation reward: 500.35\n",
      "episode: 7410   score: 445.0  epsilon: 1.0    steps: 352  evaluation reward: 500.3\n",
      "Training network. lr: 0.000085. clip: 0.033958\n",
      "Iteration 21523: Policy loss: 0.153752. Value loss: 0.074034. Entropy: 0.281070.\n",
      "Iteration 21524: Policy loss: 0.150128. Value loss: 0.025964. Entropy: 0.281050.\n",
      "Iteration 21525: Policy loss: 0.143356. Value loss: 0.018384. Entropy: 0.281253.\n",
      "Training network. lr: 0.000085. clip: 0.033958\n",
      "Iteration 21526: Policy loss: -0.011579. Value loss: 0.080914. Entropy: 0.308531.\n",
      "Iteration 21527: Policy loss: -0.005571. Value loss: 0.045062. Entropy: 0.309896.\n",
      "Iteration 21528: Policy loss: -0.012958. Value loss: 0.032898. Entropy: 0.309121.\n",
      "episode: 7411   score: 500.0  epsilon: 1.0    steps: 592  evaluation reward: 501.35\n",
      "Training network. lr: 0.000085. clip: 0.033958\n",
      "Iteration 21529: Policy loss: -0.270468. Value loss: 0.313269. Entropy: 0.299786.\n",
      "Iteration 21530: Policy loss: -0.279995. Value loss: 0.199036. Entropy: 0.299715.\n",
      "Iteration 21531: Policy loss: -0.269203. Value loss: 0.142327. Entropy: 0.299472.\n",
      "Training network. lr: 0.000085. clip: 0.033958\n",
      "Iteration 21532: Policy loss: 0.414944. Value loss: 0.216925. Entropy: 0.308191.\n",
      "Iteration 21533: Policy loss: 0.411466. Value loss: 0.086667. Entropy: 0.308361.\n",
      "Iteration 21534: Policy loss: 0.407667. Value loss: 0.058050. Entropy: 0.308232.\n",
      "episode: 7412   score: 560.0  epsilon: 1.0    steps: 544  evaluation reward: 501.0\n",
      "episode: 7413   score: 345.0  epsilon: 1.0    steps: 704  evaluation reward: 498.8\n",
      "episode: 7414   score: 315.0  epsilon: 1.0    steps: 976  evaluation reward: 496.4\n",
      "episode: 7415   score: 770.0  epsilon: 1.0    steps: 984  evaluation reward: 498.4\n",
      "Training network. lr: 0.000085. clip: 0.033958\n",
      "Iteration 21535: Policy loss: 0.117287. Value loss: 0.112248. Entropy: 0.281765.\n",
      "Iteration 21536: Policy loss: 0.116773. Value loss: 0.061586. Entropy: 0.280420.\n",
      "Iteration 21537: Policy loss: 0.113195. Value loss: 0.050217. Entropy: 0.281820.\n",
      "Training network. lr: 0.000085. clip: 0.033958\n",
      "Iteration 21538: Policy loss: 0.197730. Value loss: 0.057552. Entropy: 0.297959.\n",
      "Iteration 21539: Policy loss: 0.202668. Value loss: 0.029183. Entropy: 0.296301.\n",
      "Iteration 21540: Policy loss: 0.201456. Value loss: 0.025539. Entropy: 0.295717.\n",
      "episode: 7416   score: 340.0  epsilon: 1.0    steps: 968  evaluation reward: 498.25\n",
      "Training network. lr: 0.000085. clip: 0.033958\n",
      "Iteration 21541: Policy loss: -0.215389. Value loss: 0.096003. Entropy: 0.306233.\n",
      "Iteration 21542: Policy loss: -0.218212. Value loss: 0.044854. Entropy: 0.307136.\n",
      "Iteration 21543: Policy loss: -0.218012. Value loss: 0.036084. Entropy: 0.306136.\n",
      "Training network. lr: 0.000085. clip: 0.033958\n",
      "Iteration 21544: Policy loss: 0.241388. Value loss: 0.124309. Entropy: 0.300980.\n",
      "Iteration 21545: Policy loss: 0.243575. Value loss: 0.042332. Entropy: 0.300497.\n",
      "Iteration 21546: Policy loss: 0.243536. Value loss: 0.025930. Entropy: 0.300254.\n",
      "episode: 7417   score: 510.0  epsilon: 1.0    steps: 104  evaluation reward: 499.4\n",
      "episode: 7418   score: 405.0  epsilon: 1.0    steps: 520  evaluation reward: 499.7\n",
      "Training network. lr: 0.000085. clip: 0.033958\n",
      "Iteration 21547: Policy loss: -0.163103. Value loss: 0.298626. Entropy: 0.284477.\n",
      "Iteration 21548: Policy loss: -0.180669. Value loss: 0.219793. Entropy: 0.284176.\n",
      "Iteration 21549: Policy loss: -0.165504. Value loss: 0.144925. Entropy: 0.285881.\n",
      "episode: 7419   score: 330.0  epsilon: 1.0    steps: 984  evaluation reward: 498.8\n",
      "Training network. lr: 0.000085. clip: 0.033958\n",
      "Iteration 21550: Policy loss: -0.002983. Value loss: 0.077182. Entropy: 0.299965.\n",
      "Iteration 21551: Policy loss: -0.003893. Value loss: 0.043591. Entropy: 0.299507.\n",
      "Iteration 21552: Policy loss: -0.003877. Value loss: 0.035616. Entropy: 0.299861.\n",
      "Training network. lr: 0.000085. clip: 0.033801\n",
      "Iteration 21553: Policy loss: 0.216381. Value loss: 0.131466. Entropy: 0.308011.\n",
      "Iteration 21554: Policy loss: 0.219032. Value loss: 0.040549. Entropy: 0.306342.\n",
      "Iteration 21555: Policy loss: 0.215033. Value loss: 0.029782. Entropy: 0.307479.\n",
      "episode: 7420   score: 330.0  epsilon: 1.0    steps: 368  evaluation reward: 497.45\n",
      "episode: 7421   score: 345.0  epsilon: 1.0    steps: 960  evaluation reward: 493.5\n",
      "Training network. lr: 0.000085. clip: 0.033801\n",
      "Iteration 21556: Policy loss: 0.065469. Value loss: 0.125821. Entropy: 0.296602.\n",
      "Iteration 21557: Policy loss: 0.066999. Value loss: 0.055349. Entropy: 0.297312.\n",
      "Iteration 21558: Policy loss: 0.061526. Value loss: 0.036312. Entropy: 0.296363.\n",
      "Training network. lr: 0.000085. clip: 0.033801\n",
      "Iteration 21559: Policy loss: -0.242949. Value loss: 0.306758. Entropy: 0.302462.\n",
      "Iteration 21560: Policy loss: -0.251673. Value loss: 0.224426. Entropy: 0.302814.\n",
      "Iteration 21561: Policy loss: -0.257008. Value loss: 0.179657. Entropy: 0.301991.\n",
      "episode: 7422   score: 630.0  epsilon: 1.0    steps: 80  evaluation reward: 495.2\n",
      "Training network. lr: 0.000085. clip: 0.033801\n",
      "Iteration 21562: Policy loss: -0.351345. Value loss: 0.266286. Entropy: 0.295885.\n",
      "Iteration 21563: Policy loss: -0.373544. Value loss: 0.123019. Entropy: 0.295424.\n",
      "Iteration 21564: Policy loss: -0.387045. Value loss: 0.086084. Entropy: 0.295640.\n",
      "Training network. lr: 0.000085. clip: 0.033801\n",
      "Iteration 21565: Policy loss: -0.254605. Value loss: 0.365609. Entropy: 0.307770.\n",
      "Iteration 21566: Policy loss: -0.281820. Value loss: 0.131691. Entropy: 0.307307.\n",
      "Iteration 21567: Policy loss: -0.286528. Value loss: 0.046468. Entropy: 0.306377.\n",
      "episode: 7423   score: 285.0  epsilon: 1.0    steps: 712  evaluation reward: 494.15\n",
      "Training network. lr: 0.000085. clip: 0.033801\n",
      "Iteration 21568: Policy loss: 0.389042. Value loss: 0.192426. Entropy: 0.300830.\n",
      "Iteration 21569: Policy loss: 0.374015. Value loss: 0.062543. Entropy: 0.300123.\n",
      "Iteration 21570: Policy loss: 0.376455. Value loss: 0.038690. Entropy: 0.299734.\n",
      "Training network. lr: 0.000085. clip: 0.033801\n",
      "Iteration 21571: Policy loss: -0.365185. Value loss: 0.363010. Entropy: 0.306595.\n",
      "Iteration 21572: Policy loss: -0.378489. Value loss: 0.251938. Entropy: 0.307099.\n",
      "Iteration 21573: Policy loss: -0.377342. Value loss: 0.207763. Entropy: 0.306460.\n",
      "episode: 7424   score: 450.0  epsilon: 1.0    steps: 816  evaluation reward: 494.6\n",
      "Training network. lr: 0.000085. clip: 0.033801\n",
      "Iteration 21574: Policy loss: 0.145440. Value loss: 0.114449. Entropy: 0.305690.\n",
      "Iteration 21575: Policy loss: 0.141772. Value loss: 0.062749. Entropy: 0.306043.\n",
      "Iteration 21576: Policy loss: 0.149551. Value loss: 0.046965. Entropy: 0.305717.\n",
      "episode: 7425   score: 595.0  epsilon: 1.0    steps: 176  evaluation reward: 493.75\n",
      "Training network. lr: 0.000085. clip: 0.033801\n",
      "Iteration 21577: Policy loss: 0.018195. Value loss: 0.115427. Entropy: 0.295990.\n",
      "Iteration 21578: Policy loss: 0.021524. Value loss: 0.057838. Entropy: 0.296158.\n",
      "Iteration 21579: Policy loss: 0.014448. Value loss: 0.037747. Entropy: 0.296960.\n",
      "episode: 7426   score: 550.0  epsilon: 1.0    steps: 152  evaluation reward: 493.8\n",
      "episode: 7427   score: 415.0  epsilon: 1.0    steps: 496  evaluation reward: 494.6\n",
      "episode: 7428   score: 530.0  epsilon: 1.0    steps: 824  evaluation reward: 497.45\n",
      "Training network. lr: 0.000085. clip: 0.033801\n",
      "Iteration 21580: Policy loss: -0.013230. Value loss: 0.123512. Entropy: 0.275560.\n",
      "Iteration 21581: Policy loss: -0.011367. Value loss: 0.058262. Entropy: 0.274200.\n",
      "Iteration 21582: Policy loss: -0.024421. Value loss: 0.040910. Entropy: 0.273499.\n",
      "Training network. lr: 0.000085. clip: 0.033801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21583: Policy loss: 0.145985. Value loss: 0.115357. Entropy: 0.300125.\n",
      "Iteration 21584: Policy loss: 0.139494. Value loss: 0.051107. Entropy: 0.301756.\n",
      "Iteration 21585: Policy loss: 0.136611. Value loss: 0.035508. Entropy: 0.300546.\n",
      "episode: 7429   score: 985.0  epsilon: 1.0    steps: 192  evaluation reward: 503.1\n",
      "episode: 7430   score: 670.0  epsilon: 1.0    steps: 432  evaluation reward: 506.9\n",
      "Training network. lr: 0.000085. clip: 0.033801\n",
      "Iteration 21586: Policy loss: -0.189468. Value loss: 0.375829. Entropy: 0.284485.\n",
      "Iteration 21587: Policy loss: -0.208477. Value loss: 0.228196. Entropy: 0.283886.\n",
      "Iteration 21588: Policy loss: -0.220033. Value loss: 0.175778. Entropy: 0.283169.\n",
      "Training network. lr: 0.000085. clip: 0.033801\n",
      "Iteration 21589: Policy loss: 0.074810. Value loss: 0.119932. Entropy: 0.305719.\n",
      "Iteration 21590: Policy loss: 0.066897. Value loss: 0.046963. Entropy: 0.305485.\n",
      "Iteration 21591: Policy loss: 0.072961. Value loss: 0.032517. Entropy: 0.305808.\n",
      "Training network. lr: 0.000085. clip: 0.033801\n",
      "Iteration 21592: Policy loss: -0.096655. Value loss: 0.338905. Entropy: 0.306940.\n",
      "Iteration 21593: Policy loss: -0.123526. Value loss: 0.267318. Entropy: 0.307025.\n",
      "Iteration 21594: Policy loss: -0.122115. Value loss: 0.230124. Entropy: 0.306887.\n",
      "Training network. lr: 0.000085. clip: 0.033801\n",
      "Iteration 21595: Policy loss: 0.077398. Value loss: 0.290800. Entropy: 0.296058.\n",
      "Iteration 21596: Policy loss: 0.046966. Value loss: 0.210129. Entropy: 0.295933.\n",
      "Iteration 21597: Policy loss: 0.060979. Value loss: 0.164397. Entropy: 0.297254.\n",
      "episode: 7431   score: 330.0  epsilon: 1.0    steps: 616  evaluation reward: 504.85\n",
      "Training network. lr: 0.000085. clip: 0.033801\n",
      "Iteration 21598: Policy loss: -0.169201. Value loss: 0.386119. Entropy: 0.293562.\n",
      "Iteration 21599: Policy loss: -0.186628. Value loss: 0.249733. Entropy: 0.295674.\n",
      "Iteration 21600: Policy loss: -0.187452. Value loss: 0.191790. Entropy: 0.294160.\n",
      "episode: 7432   score: 390.0  epsilon: 1.0    steps: 920  evaluation reward: 506.15\n",
      "Training network. lr: 0.000084. clip: 0.033654\n",
      "Iteration 21601: Policy loss: 0.220062. Value loss: 0.127629. Entropy: 0.306056.\n",
      "Iteration 21602: Policy loss: 0.213841. Value loss: 0.063410. Entropy: 0.306676.\n",
      "Iteration 21603: Policy loss: 0.213746. Value loss: 0.046125. Entropy: 0.305589.\n",
      "episode: 7433   score: 695.0  epsilon: 1.0    steps: 400  evaluation reward: 508.6\n",
      "episode: 7434   score: 410.0  epsilon: 1.0    steps: 440  evaluation reward: 509.55\n",
      "episode: 7435   score: 640.0  epsilon: 1.0    steps: 808  evaluation reward: 512.35\n",
      "episode: 7436   score: 480.0  epsilon: 1.0    steps: 952  evaluation reward: 512.65\n",
      "Training network. lr: 0.000084. clip: 0.033654\n",
      "Iteration 21604: Policy loss: 0.406464. Value loss: 0.201657. Entropy: 0.269838.\n",
      "Iteration 21605: Policy loss: 0.397719. Value loss: 0.088282. Entropy: 0.270198.\n",
      "Iteration 21606: Policy loss: 0.394814. Value loss: 0.065530. Entropy: 0.269821.\n",
      "Training network. lr: 0.000084. clip: 0.033654\n",
      "Iteration 21607: Policy loss: 0.122808. Value loss: 0.156144. Entropy: 0.299978.\n",
      "Iteration 21608: Policy loss: 0.116465. Value loss: 0.072192. Entropy: 0.300114.\n",
      "Iteration 21609: Policy loss: 0.108780. Value loss: 0.053358. Entropy: 0.300572.\n",
      "episode: 7437   score: 385.0  epsilon: 1.0    steps: 248  evaluation reward: 512.6\n",
      "Training network. lr: 0.000084. clip: 0.033654\n",
      "Iteration 21610: Policy loss: -0.011218. Value loss: 0.108789. Entropy: 0.299389.\n",
      "Iteration 21611: Policy loss: -0.010198. Value loss: 0.055909. Entropy: 0.297277.\n",
      "Iteration 21612: Policy loss: -0.016204. Value loss: 0.040878. Entropy: 0.298447.\n",
      "Training network. lr: 0.000084. clip: 0.033654\n",
      "Iteration 21613: Policy loss: -0.448628. Value loss: 0.616312. Entropy: 0.307689.\n",
      "Iteration 21614: Policy loss: -0.451016. Value loss: 0.407958. Entropy: 0.305605.\n",
      "Iteration 21615: Policy loss: -0.464829. Value loss: 0.316544. Entropy: 0.306946.\n",
      "episode: 7438   score: 210.0  epsilon: 1.0    steps: 864  evaluation reward: 508.9\n",
      "Training network. lr: 0.000084. clip: 0.033654\n",
      "Iteration 21616: Policy loss: -0.106138. Value loss: 0.099258. Entropy: 0.301284.\n",
      "Iteration 21617: Policy loss: -0.106302. Value loss: 0.056560. Entropy: 0.302073.\n",
      "Iteration 21618: Policy loss: -0.111876. Value loss: 0.044434. Entropy: 0.300988.\n",
      "episode: 7439   score: 915.0  epsilon: 1.0    steps: 32  evaluation reward: 513.05\n",
      "Training network. lr: 0.000084. clip: 0.033654\n",
      "Iteration 21619: Policy loss: 0.221229. Value loss: 0.155546. Entropy: 0.285393.\n",
      "Iteration 21620: Policy loss: 0.221205. Value loss: 0.067154. Entropy: 0.285230.\n",
      "Iteration 21621: Policy loss: 0.215679. Value loss: 0.045374. Entropy: 0.283283.\n",
      "episode: 7440   score: 350.0  epsilon: 1.0    steps: 656  evaluation reward: 512.15\n",
      "episode: 7441   score: 385.0  epsilon: 1.0    steps: 992  evaluation reward: 508.7\n",
      "Training network. lr: 0.000084. clip: 0.033654\n",
      "Iteration 21622: Policy loss: 0.233964. Value loss: 0.223105. Entropy: 0.287271.\n",
      "Iteration 21623: Policy loss: 0.225785. Value loss: 0.083704. Entropy: 0.287130.\n",
      "Iteration 21624: Policy loss: 0.225269. Value loss: 0.050585. Entropy: 0.285689.\n",
      "Training network. lr: 0.000084. clip: 0.033654\n",
      "Iteration 21625: Policy loss: -0.013776. Value loss: 0.101009. Entropy: 0.301863.\n",
      "Iteration 21626: Policy loss: -0.014637. Value loss: 0.039352. Entropy: 0.301636.\n",
      "Iteration 21627: Policy loss: -0.020215. Value loss: 0.027472. Entropy: 0.300808.\n",
      "episode: 7442   score: 425.0  epsilon: 1.0    steps: 336  evaluation reward: 508.8\n",
      "episode: 7443   score: 155.0  epsilon: 1.0    steps: 544  evaluation reward: 504.9\n",
      "episode: 7444   score: 650.0  epsilon: 1.0    steps: 608  evaluation reward: 505.6\n",
      "Training network. lr: 0.000084. clip: 0.033654\n",
      "Iteration 21628: Policy loss: -0.038356. Value loss: 0.113277. Entropy: 0.272521.\n",
      "Iteration 21629: Policy loss: -0.043755. Value loss: 0.048008. Entropy: 0.271948.\n",
      "Iteration 21630: Policy loss: -0.048289. Value loss: 0.034220. Entropy: 0.273073.\n",
      "Training network. lr: 0.000084. clip: 0.033654\n",
      "Iteration 21631: Policy loss: -0.321287. Value loss: 0.286323. Entropy: 0.303683.\n",
      "Iteration 21632: Policy loss: -0.330546. Value loss: 0.111825. Entropy: 0.303745.\n",
      "Iteration 21633: Policy loss: -0.327435. Value loss: 0.065457. Entropy: 0.303753.\n",
      "Training network. lr: 0.000084. clip: 0.033654\n",
      "Iteration 21634: Policy loss: -0.381709. Value loss: 0.266393. Entropy: 0.313760.\n",
      "Iteration 21635: Policy loss: -0.393738. Value loss: 0.134821. Entropy: 0.313927.\n",
      "Iteration 21636: Policy loss: -0.393921. Value loss: 0.074399. Entropy: 0.314037.\n",
      "Training network. lr: 0.000084. clip: 0.033654\n",
      "Iteration 21637: Policy loss: 0.388520. Value loss: 0.220575. Entropy: 0.306907.\n",
      "Iteration 21638: Policy loss: 0.378038. Value loss: 0.076253. Entropy: 0.304867.\n",
      "Iteration 21639: Policy loss: 0.368587. Value loss: 0.051924. Entropy: 0.304933.\n",
      "episode: 7445   score: 895.0  epsilon: 1.0    steps: 16  evaluation reward: 507.85\n",
      "Training network. lr: 0.000084. clip: 0.033654\n",
      "Iteration 21640: Policy loss: 0.107375. Value loss: 0.175146. Entropy: 0.297751.\n",
      "Iteration 21641: Policy loss: 0.100238. Value loss: 0.070699. Entropy: 0.298050.\n",
      "Iteration 21642: Policy loss: 0.091853. Value loss: 0.043894. Entropy: 0.298628.\n",
      "episode: 7446   score: 345.0  epsilon: 1.0    steps: 640  evaluation reward: 507.25\n",
      "episode: 7447   score: 690.0  epsilon: 1.0    steps: 952  evaluation reward: 510.45\n",
      "Training network. lr: 0.000084. clip: 0.033654\n",
      "Iteration 21643: Policy loss: 0.124586. Value loss: 0.122485. Entropy: 0.299562.\n",
      "Iteration 21644: Policy loss: 0.116124. Value loss: 0.055638. Entropy: 0.299523.\n",
      "Iteration 21645: Policy loss: 0.114615. Value loss: 0.037962. Entropy: 0.298719.\n",
      "Training network. lr: 0.000084. clip: 0.033654\n",
      "Iteration 21646: Policy loss: -0.102846. Value loss: 0.285952. Entropy: 0.295185.\n",
      "Iteration 21647: Policy loss: -0.104746. Value loss: 0.113190. Entropy: 0.295228.\n",
      "Iteration 21648: Policy loss: -0.099721. Value loss: 0.073682. Entropy: 0.296553.\n",
      "episode: 7448   score: 300.0  epsilon: 1.0    steps: 488  evaluation reward: 506.75\n",
      "Training network. lr: 0.000084. clip: 0.033654\n",
      "Iteration 21649: Policy loss: 0.299269. Value loss: 0.144962. Entropy: 0.307352.\n",
      "Iteration 21650: Policy loss: 0.286416. Value loss: 0.049119. Entropy: 0.307664.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21651: Policy loss: 0.285915. Value loss: 0.034711. Entropy: 0.306000.\n",
      "episode: 7449   score: 685.0  epsilon: 1.0    steps: 352  evaluation reward: 501.6\n",
      "episode: 7450   score: 410.0  epsilon: 1.0    steps: 576  evaluation reward: 501.8\n",
      "Training network. lr: 0.000084. clip: 0.033497\n",
      "Iteration 21652: Policy loss: 0.066833. Value loss: 0.051762. Entropy: 0.290195.\n",
      "Iteration 21653: Policy loss: 0.067673. Value loss: 0.030031. Entropy: 0.289928.\n",
      "Iteration 21654: Policy loss: 0.065934. Value loss: 0.024990. Entropy: 0.289588.\n",
      "Training network. lr: 0.000084. clip: 0.033497\n",
      "Iteration 21655: Policy loss: -0.268488. Value loss: 0.268866. Entropy: 0.307734.\n",
      "Iteration 21656: Policy loss: -0.272231. Value loss: 0.172833. Entropy: 0.308102.\n",
      "Iteration 21657: Policy loss: -0.278530. Value loss: 0.139914. Entropy: 0.308456.\n",
      "now time :  2019-09-06 12:34:33.097809\n",
      "episode: 7451   score: 475.0  epsilon: 1.0    steps: 816  evaluation reward: 500.05\n",
      "Training network. lr: 0.000084. clip: 0.033497\n",
      "Iteration 21658: Policy loss: 0.340404. Value loss: 0.131679. Entropy: 0.297381.\n",
      "Iteration 21659: Policy loss: 0.336034. Value loss: 0.044976. Entropy: 0.297043.\n",
      "Iteration 21660: Policy loss: 0.332183. Value loss: 0.031974. Entropy: 0.297583.\n",
      "episode: 7452   score: 530.0  epsilon: 1.0    steps: 928  evaluation reward: 499.3\n",
      "Training network. lr: 0.000084. clip: 0.033497\n",
      "Iteration 21661: Policy loss: 0.152833. Value loss: 0.149046. Entropy: 0.300600.\n",
      "Iteration 21662: Policy loss: 0.152280. Value loss: 0.060146. Entropy: 0.301584.\n",
      "Iteration 21663: Policy loss: 0.161621. Value loss: 0.038787. Entropy: 0.300308.\n",
      "episode: 7453   score: 440.0  epsilon: 1.0    steps: 112  evaluation reward: 496.2\n",
      "episode: 7454   score: 300.0  epsilon: 1.0    steps: 752  evaluation reward: 493.45\n",
      "episode: 7455   score: 530.0  epsilon: 1.0    steps: 976  evaluation reward: 493.55\n",
      "Training network. lr: 0.000084. clip: 0.033497\n",
      "Iteration 21664: Policy loss: -0.089812. Value loss: 0.286216. Entropy: 0.270156.\n",
      "Iteration 21665: Policy loss: -0.043271. Value loss: 0.172177. Entropy: 0.270998.\n",
      "Iteration 21666: Policy loss: -0.092939. Value loss: 0.124587. Entropy: 0.272103.\n",
      "Training network. lr: 0.000084. clip: 0.033497\n",
      "Iteration 21667: Policy loss: -0.167514. Value loss: 0.116997. Entropy: 0.303210.\n",
      "Iteration 21668: Policy loss: -0.173623. Value loss: 0.063626. Entropy: 0.303573.\n",
      "Iteration 21669: Policy loss: -0.170794. Value loss: 0.046352. Entropy: 0.303440.\n",
      "Training network. lr: 0.000084. clip: 0.033497\n",
      "Iteration 21670: Policy loss: 0.382842. Value loss: 0.272188. Entropy: 0.309329.\n",
      "Iteration 21671: Policy loss: 0.377654. Value loss: 0.117206. Entropy: 0.307068.\n",
      "Iteration 21672: Policy loss: 0.360920. Value loss: 0.066243. Entropy: 0.307773.\n",
      "episode: 7456   score: 535.0  epsilon: 1.0    steps: 832  evaluation reward: 493.6\n",
      "Training network. lr: 0.000084. clip: 0.033497\n",
      "Iteration 21673: Policy loss: 0.217700. Value loss: 0.105686. Entropy: 0.301581.\n",
      "Iteration 21674: Policy loss: 0.204712. Value loss: 0.046946. Entropy: 0.301996.\n",
      "Iteration 21675: Policy loss: 0.206125. Value loss: 0.033738. Entropy: 0.302291.\n",
      "Training network. lr: 0.000084. clip: 0.033497\n",
      "Iteration 21676: Policy loss: 0.235296. Value loss: 0.078249. Entropy: 0.311046.\n",
      "Iteration 21677: Policy loss: 0.231636. Value loss: 0.020957. Entropy: 0.310582.\n",
      "Iteration 21678: Policy loss: 0.226172. Value loss: 0.015668. Entropy: 0.310924.\n",
      "episode: 7457   score: 345.0  epsilon: 1.0    steps: 904  evaluation reward: 493.85\n",
      "Training network. lr: 0.000084. clip: 0.033497\n",
      "Iteration 21679: Policy loss: -0.392598. Value loss: 0.337594. Entropy: 0.305096.\n",
      "Iteration 21680: Policy loss: -0.387152. Value loss: 0.219943. Entropy: 0.304537.\n",
      "Iteration 21681: Policy loss: -0.403731. Value loss: 0.180892. Entropy: 0.305564.\n",
      "Training network. lr: 0.000084. clip: 0.033497\n",
      "Iteration 21682: Policy loss: 0.119431. Value loss: 0.187363. Entropy: 0.305861.\n",
      "Iteration 21683: Policy loss: 0.114590. Value loss: 0.078993. Entropy: 0.304747.\n",
      "Iteration 21684: Policy loss: 0.111850. Value loss: 0.054686. Entropy: 0.304009.\n",
      "episode: 7458   score: 525.0  epsilon: 1.0    steps: 568  evaluation reward: 492.75\n",
      "episode: 7459   score: 385.0  epsilon: 1.0    steps: 744  evaluation reward: 492.65\n",
      "Training network. lr: 0.000084. clip: 0.033497\n",
      "Iteration 21685: Policy loss: -0.009293. Value loss: 0.169872. Entropy: 0.290651.\n",
      "Iteration 21686: Policy loss: -0.018848. Value loss: 0.080076. Entropy: 0.291539.\n",
      "Iteration 21687: Policy loss: -0.019254. Value loss: 0.059165. Entropy: 0.291768.\n",
      "episode: 7460   score: 320.0  epsilon: 1.0    steps: 120  evaluation reward: 489.95\n",
      "episode: 7461   score: 620.0  epsilon: 1.0    steps: 136  evaluation reward: 492.65\n",
      "Training network. lr: 0.000084. clip: 0.033497\n",
      "Iteration 21688: Policy loss: 0.066028. Value loss: 0.074329. Entropy: 0.286285.\n",
      "Iteration 21689: Policy loss: 0.062356. Value loss: 0.041927. Entropy: 0.284719.\n",
      "Iteration 21690: Policy loss: 0.061232. Value loss: 0.033078. Entropy: 0.285078.\n",
      "Training network. lr: 0.000084. clip: 0.033497\n",
      "Iteration 21691: Policy loss: -0.006035. Value loss: 0.075568. Entropy: 0.307247.\n",
      "Iteration 21692: Policy loss: -0.007938. Value loss: 0.036307. Entropy: 0.306069.\n",
      "Iteration 21693: Policy loss: -0.013265. Value loss: 0.025565. Entropy: 0.308310.\n",
      "episode: 7462   score: 755.0  epsilon: 1.0    steps: 480  evaluation reward: 493.55\n",
      "Training network. lr: 0.000084. clip: 0.033497\n",
      "Iteration 21694: Policy loss: 0.019735. Value loss: 0.069028. Entropy: 0.294317.\n",
      "Iteration 21695: Policy loss: 0.021335. Value loss: 0.033298. Entropy: 0.295444.\n",
      "Iteration 21696: Policy loss: 0.012312. Value loss: 0.023249. Entropy: 0.293862.\n",
      "episode: 7463   score: 440.0  epsilon: 1.0    steps: 1024  evaluation reward: 495.05\n",
      "Training network. lr: 0.000084. clip: 0.033497\n",
      "Iteration 21697: Policy loss: -0.138386. Value loss: 0.339485. Entropy: 0.307817.\n",
      "Iteration 21698: Policy loss: -0.154097. Value loss: 0.112915. Entropy: 0.308244.\n",
      "Iteration 21699: Policy loss: -0.167330. Value loss: 0.069195. Entropy: 0.307326.\n",
      "episode: 7464   score: 270.0  epsilon: 1.0    steps: 464  evaluation reward: 495.65\n",
      "Training network. lr: 0.000084. clip: 0.033497\n",
      "Iteration 21700: Policy loss: -0.252171. Value loss: 0.137552. Entropy: 0.276277.\n",
      "Iteration 21701: Policy loss: -0.265878. Value loss: 0.056047. Entropy: 0.277436.\n",
      "Iteration 21702: Policy loss: -0.259858. Value loss: 0.042457. Entropy: 0.277423.\n",
      "Training network. lr: 0.000083. clip: 0.033341\n",
      "Iteration 21703: Policy loss: 0.132414. Value loss: 0.160086. Entropy: 0.299460.\n",
      "Iteration 21704: Policy loss: 0.133974. Value loss: 0.062280. Entropy: 0.299078.\n",
      "Iteration 21705: Policy loss: 0.128865. Value loss: 0.040487. Entropy: 0.298524.\n",
      "Training network. lr: 0.000083. clip: 0.033341\n",
      "Iteration 21706: Policy loss: -0.110966. Value loss: 0.116331. Entropy: 0.308260.\n",
      "Iteration 21707: Policy loss: -0.119279. Value loss: 0.043994. Entropy: 0.308699.\n",
      "Iteration 21708: Policy loss: -0.120801. Value loss: 0.032597. Entropy: 0.308114.\n",
      "episode: 7465   score: 345.0  epsilon: 1.0    steps: 176  evaluation reward: 490.0\n",
      "episode: 7466   score: 485.0  epsilon: 1.0    steps: 240  evaluation reward: 487.6\n",
      "episode: 7467   score: 765.0  epsilon: 1.0    steps: 472  evaluation reward: 491.55\n",
      "Training network. lr: 0.000083. clip: 0.033341\n",
      "Iteration 21709: Policy loss: 0.384611. Value loss: 0.087174. Entropy: 0.276210.\n",
      "Iteration 21710: Policy loss: 0.386184. Value loss: 0.031310. Entropy: 0.277773.\n",
      "Iteration 21711: Policy loss: 0.380059. Value loss: 0.023978. Entropy: 0.276376.\n",
      "Training network. lr: 0.000083. clip: 0.033341\n",
      "Iteration 21712: Policy loss: 0.015382. Value loss: 0.075096. Entropy: 0.300756.\n",
      "Iteration 21713: Policy loss: 0.013142. Value loss: 0.052276. Entropy: 0.302130.\n",
      "Iteration 21714: Policy loss: 0.009329. Value loss: 0.039254. Entropy: 0.302339.\n",
      "Training network. lr: 0.000083. clip: 0.033341\n",
      "Iteration 21715: Policy loss: -0.216673. Value loss: 0.206573. Entropy: 0.293472.\n",
      "Iteration 21716: Policy loss: -0.205086. Value loss: 0.069720. Entropy: 0.291419.\n",
      "Iteration 21717: Policy loss: -0.233002. Value loss: 0.038226. Entropy: 0.291921.\n",
      "episode: 7468   score: 855.0  epsilon: 1.0    steps: 992  evaluation reward: 493.6\n",
      "Training network. lr: 0.000083. clip: 0.033341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21718: Policy loss: -0.132319. Value loss: 0.195609. Entropy: 0.316237.\n",
      "Iteration 21719: Policy loss: -0.133989. Value loss: 0.079747. Entropy: 0.314768.\n",
      "Iteration 21720: Policy loss: -0.140491. Value loss: 0.054971. Entropy: 0.314892.\n",
      "episode: 7469   score: 345.0  epsilon: 1.0    steps: 264  evaluation reward: 491.1\n",
      "Training network. lr: 0.000083. clip: 0.033341\n",
      "Iteration 21721: Policy loss: -0.047256. Value loss: 0.084479. Entropy: 0.294761.\n",
      "Iteration 21722: Policy loss: -0.051432. Value loss: 0.032897. Entropy: 0.296175.\n",
      "Iteration 21723: Policy loss: -0.054894. Value loss: 0.022002. Entropy: 0.294635.\n",
      "Training network. lr: 0.000083. clip: 0.033341\n",
      "Iteration 21724: Policy loss: 0.855454. Value loss: 0.243649. Entropy: 0.306563.\n",
      "Iteration 21725: Policy loss: 0.852798. Value loss: 0.068948. Entropy: 0.306759.\n",
      "Iteration 21726: Policy loss: 0.837635. Value loss: 0.040150. Entropy: 0.307379.\n",
      "episode: 7470   score: 755.0  epsilon: 1.0    steps: 592  evaluation reward: 493.2\n",
      "Training network. lr: 0.000083. clip: 0.033341\n",
      "Iteration 21727: Policy loss: -0.087276. Value loss: 0.099958. Entropy: 0.291078.\n",
      "Iteration 21728: Policy loss: -0.089662. Value loss: 0.050206. Entropy: 0.290175.\n",
      "Iteration 21729: Policy loss: -0.096170. Value loss: 0.036317. Entropy: 0.290459.\n",
      "episode: 7471   score: 345.0  epsilon: 1.0    steps: 528  evaluation reward: 491.8\n",
      "episode: 7472   score: 345.0  epsilon: 1.0    steps: 928  evaluation reward: 488.3\n",
      "episode: 7473   score: 270.0  epsilon: 1.0    steps: 968  evaluation reward: 487.55\n",
      "Training network. lr: 0.000083. clip: 0.033341\n",
      "Iteration 21730: Policy loss: -0.001378. Value loss: 0.056810. Entropy: 0.292769.\n",
      "Iteration 21731: Policy loss: -0.002035. Value loss: 0.031414. Entropy: 0.293308.\n",
      "Iteration 21732: Policy loss: -0.001586. Value loss: 0.024293. Entropy: 0.293646.\n",
      "Training network. lr: 0.000083. clip: 0.033341\n",
      "Iteration 21733: Policy loss: -0.590512. Value loss: 0.544892. Entropy: 0.293623.\n",
      "Iteration 21734: Policy loss: -0.596027. Value loss: 0.343711. Entropy: 0.291756.\n",
      "Iteration 21735: Policy loss: -0.597681. Value loss: 0.263787. Entropy: 0.293887.\n",
      "episode: 7474   score: 670.0  epsilon: 1.0    steps: 120  evaluation reward: 490.55\n",
      "Training network. lr: 0.000083. clip: 0.033341\n",
      "Iteration 21736: Policy loss: -0.030795. Value loss: 0.066399. Entropy: 0.288314.\n",
      "Iteration 21737: Policy loss: -0.030811. Value loss: 0.043480. Entropy: 0.287954.\n",
      "Iteration 21738: Policy loss: -0.031133. Value loss: 0.036296. Entropy: 0.288825.\n",
      "episode: 7475   score: 575.0  epsilon: 1.0    steps: 496  evaluation reward: 492.8\n",
      "Training network. lr: 0.000083. clip: 0.033341\n",
      "Iteration 21739: Policy loss: 0.052641. Value loss: 0.121499. Entropy: 0.299385.\n",
      "Iteration 21740: Policy loss: 0.050327. Value loss: 0.044299. Entropy: 0.299630.\n",
      "Iteration 21741: Policy loss: 0.038516. Value loss: 0.034785. Entropy: 0.299319.\n",
      "Training network. lr: 0.000083. clip: 0.033341\n",
      "Iteration 21742: Policy loss: -0.182535. Value loss: 0.439650. Entropy: 0.308054.\n",
      "Iteration 21743: Policy loss: -0.191471. Value loss: 0.256681. Entropy: 0.309020.\n",
      "Iteration 21744: Policy loss: -0.192781. Value loss: 0.198166. Entropy: 0.307988.\n",
      "Training network. lr: 0.000083. clip: 0.033341\n",
      "Iteration 21745: Policy loss: 0.104827. Value loss: 0.077952. Entropy: 0.308592.\n",
      "Iteration 21746: Policy loss: 0.098549. Value loss: 0.034690. Entropy: 0.309128.\n",
      "Iteration 21747: Policy loss: 0.095692. Value loss: 0.025828. Entropy: 0.309016.\n",
      "Training network. lr: 0.000083. clip: 0.033341\n",
      "Iteration 21748: Policy loss: -0.224332. Value loss: 0.362884. Entropy: 0.315356.\n",
      "Iteration 21749: Policy loss: -0.231148. Value loss: 0.200946. Entropy: 0.315121.\n",
      "Iteration 21750: Policy loss: -0.235774. Value loss: 0.158250. Entropy: 0.314883.\n",
      "Training network. lr: 0.000083. clip: 0.033193\n",
      "Iteration 21751: Policy loss: 0.071530. Value loss: 0.257102. Entropy: 0.306188.\n",
      "Iteration 21752: Policy loss: 0.070207. Value loss: 0.172280. Entropy: 0.304938.\n",
      "Iteration 21753: Policy loss: 0.057594. Value loss: 0.134909. Entropy: 0.304656.\n",
      "episode: 7476   score: 530.0  epsilon: 1.0    steps: 24  evaluation reward: 494.05\n",
      "episode: 7477   score: 470.0  epsilon: 1.0    steps: 264  evaluation reward: 494.4\n",
      "episode: 7478   score: 630.0  epsilon: 1.0    steps: 808  evaluation reward: 494.3\n",
      "Training network. lr: 0.000083. clip: 0.033193\n",
      "Iteration 21754: Policy loss: 0.168121. Value loss: 0.060954. Entropy: 0.281067.\n",
      "Iteration 21755: Policy loss: 0.162369. Value loss: 0.029504. Entropy: 0.282675.\n",
      "Iteration 21756: Policy loss: 0.164261. Value loss: 0.021663. Entropy: 0.282520.\n",
      "Training network. lr: 0.000083. clip: 0.033193\n",
      "Iteration 21757: Policy loss: 0.057450. Value loss: 0.192824. Entropy: 0.302807.\n",
      "Iteration 21758: Policy loss: 0.057953. Value loss: 0.087772. Entropy: 0.302048.\n",
      "Iteration 21759: Policy loss: 0.047767. Value loss: 0.049839. Entropy: 0.303018.\n",
      "episode: 7479   score: 310.0  epsilon: 1.0    steps: 32  evaluation reward: 492.7\n",
      "episode: 7480   score: 555.0  epsilon: 1.0    steps: 568  evaluation reward: 490.4\n",
      "Training network. lr: 0.000083. clip: 0.033193\n",
      "Iteration 21760: Policy loss: -0.353516. Value loss: 0.289987. Entropy: 0.287677.\n",
      "Iteration 21761: Policy loss: -0.358658. Value loss: 0.123510. Entropy: 0.288559.\n",
      "Iteration 21762: Policy loss: -0.368364. Value loss: 0.064766. Entropy: 0.288410.\n",
      "episode: 7481   score: 650.0  epsilon: 1.0    steps: 568  evaluation reward: 489.2\n",
      "Training network. lr: 0.000083. clip: 0.033193\n",
      "Iteration 21763: Policy loss: 0.109761. Value loss: 0.074980. Entropy: 0.297070.\n",
      "Iteration 21764: Policy loss: 0.111950. Value loss: 0.036886. Entropy: 0.295937.\n",
      "Iteration 21765: Policy loss: 0.111343. Value loss: 0.024640. Entropy: 0.295614.\n",
      "Training network. lr: 0.000083. clip: 0.033193\n",
      "Iteration 21766: Policy loss: 0.008711. Value loss: 0.269577. Entropy: 0.303760.\n",
      "Iteration 21767: Policy loss: 0.007692. Value loss: 0.105052. Entropy: 0.303907.\n",
      "Iteration 21768: Policy loss: -0.004257. Value loss: 0.066932. Entropy: 0.303295.\n",
      "Training network. lr: 0.000083. clip: 0.033193\n",
      "Iteration 21769: Policy loss: 0.192275. Value loss: 0.180747. Entropy: 0.302902.\n",
      "Iteration 21770: Policy loss: 0.184242. Value loss: 0.065097. Entropy: 0.303975.\n",
      "Iteration 21771: Policy loss: 0.185541. Value loss: 0.042997. Entropy: 0.302484.\n",
      "episode: 7482   score: 555.0  epsilon: 1.0    steps: 56  evaluation reward: 488.05\n",
      "Training network. lr: 0.000083. clip: 0.033193\n",
      "Iteration 21772: Policy loss: 0.264534. Value loss: 0.103969. Entropy: 0.293659.\n",
      "Iteration 21773: Policy loss: 0.258170. Value loss: 0.048564. Entropy: 0.292882.\n",
      "Iteration 21774: Policy loss: 0.255445. Value loss: 0.035126. Entropy: 0.294260.\n",
      "Training network. lr: 0.000083. clip: 0.033193\n",
      "Iteration 21775: Policy loss: 0.036488. Value loss: 0.187009. Entropy: 0.301280.\n",
      "Iteration 21776: Policy loss: 0.034249. Value loss: 0.070684. Entropy: 0.302127.\n",
      "Iteration 21777: Policy loss: 0.031022. Value loss: 0.046516. Entropy: 0.302236.\n",
      "episode: 7483   score: 495.0  epsilon: 1.0    steps: 72  evaluation reward: 487.2\n",
      "episode: 7484   score: 395.0  epsilon: 1.0    steps: 392  evaluation reward: 483.3\n",
      "Training network. lr: 0.000083. clip: 0.033193\n",
      "Iteration 21778: Policy loss: -0.216600. Value loss: 0.342842. Entropy: 0.288802.\n",
      "Iteration 21779: Policy loss: -0.227599. Value loss: 0.192549. Entropy: 0.291931.\n",
      "Iteration 21780: Policy loss: -0.185472. Value loss: 0.099136. Entropy: 0.290664.\n",
      "episode: 7485   score: 1155.0  epsilon: 1.0    steps: 240  evaluation reward: 490.35\n",
      "episode: 7486   score: 330.0  epsilon: 1.0    steps: 512  evaluation reward: 488.35\n",
      "Training network. lr: 0.000083. clip: 0.033193\n",
      "Iteration 21781: Policy loss: 0.028493. Value loss: 0.119213. Entropy: 0.292056.\n",
      "Iteration 21782: Policy loss: 0.020267. Value loss: 0.039489. Entropy: 0.290596.\n",
      "Iteration 21783: Policy loss: 0.021011. Value loss: 0.025545. Entropy: 0.291647.\n",
      "episode: 7487   score: 365.0  epsilon: 1.0    steps: 840  evaluation reward: 487.25\n",
      "episode: 7488   score: 530.0  epsilon: 1.0    steps: 984  evaluation reward: 489.6\n",
      "episode: 7489   score: 485.0  epsilon: 1.0    steps: 984  evaluation reward: 488.95\n",
      "Training network. lr: 0.000083. clip: 0.033193\n",
      "Iteration 21784: Policy loss: 0.075163. Value loss: 0.125448. Entropy: 0.294425.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21785: Policy loss: 0.069466. Value loss: 0.057167. Entropy: 0.295042.\n",
      "Iteration 21786: Policy loss: 0.072887. Value loss: 0.036725. Entropy: 0.293999.\n",
      "Training network. lr: 0.000083. clip: 0.033193\n",
      "Iteration 21787: Policy loss: -0.157091. Value loss: 0.290118. Entropy: 0.290118.\n",
      "Iteration 21788: Policy loss: -0.156280. Value loss: 0.147581. Entropy: 0.289903.\n",
      "Iteration 21789: Policy loss: -0.161962. Value loss: 0.085225. Entropy: 0.288681.\n",
      "Training network. lr: 0.000083. clip: 0.033193\n",
      "Iteration 21790: Policy loss: -0.259782. Value loss: 0.194593. Entropy: 0.302833.\n",
      "Iteration 21791: Policy loss: -0.239142. Value loss: 0.082799. Entropy: 0.303106.\n",
      "Iteration 21792: Policy loss: -0.262815. Value loss: 0.059257. Entropy: 0.303354.\n",
      "Training network. lr: 0.000083. clip: 0.033193\n",
      "Iteration 21793: Policy loss: -0.182233. Value loss: 0.151592. Entropy: 0.308917.\n",
      "Iteration 21794: Policy loss: -0.185447. Value loss: 0.054322. Entropy: 0.309478.\n",
      "Iteration 21795: Policy loss: -0.188787. Value loss: 0.033973. Entropy: 0.309393.\n",
      "Training network. lr: 0.000083. clip: 0.033193\n",
      "Iteration 21796: Policy loss: 0.156781. Value loss: 0.217800. Entropy: 0.309673.\n",
      "Iteration 21797: Policy loss: 0.167251. Value loss: 0.122486. Entropy: 0.309920.\n",
      "Iteration 21798: Policy loss: 0.158612. Value loss: 0.079258. Entropy: 0.309769.\n",
      "Training network. lr: 0.000083. clip: 0.033193\n",
      "Iteration 21799: Policy loss: -0.400936. Value loss: 0.212038. Entropy: 0.300017.\n",
      "Iteration 21800: Policy loss: -0.398242. Value loss: 0.104178. Entropy: 0.299744.\n",
      "Iteration 21801: Policy loss: -0.403243. Value loss: 0.064512. Entropy: 0.299420.\n",
      "episode: 7490   score: 405.0  epsilon: 1.0    steps: 408  evaluation reward: 490.8\n",
      "episode: 7491   score: 500.0  epsilon: 1.0    steps: 736  evaluation reward: 490.0\n",
      "Training network. lr: 0.000083. clip: 0.033037\n",
      "Iteration 21802: Policy loss: 0.356430. Value loss: 0.202255. Entropy: 0.296708.\n",
      "Iteration 21803: Policy loss: 0.350192. Value loss: 0.075619. Entropy: 0.297384.\n",
      "Iteration 21804: Policy loss: 0.363437. Value loss: 0.047208. Entropy: 0.297074.\n",
      "episode: 7492   score: 480.0  epsilon: 1.0    steps: 312  evaluation reward: 491.35\n",
      "episode: 7493   score: 360.0  epsilon: 1.0    steps: 904  evaluation reward: 491.25\n",
      "Training network. lr: 0.000083. clip: 0.033037\n",
      "Iteration 21805: Policy loss: -0.059495. Value loss: 0.115927. Entropy: 0.299260.\n",
      "Iteration 21806: Policy loss: -0.063008. Value loss: 0.051598. Entropy: 0.299729.\n",
      "Iteration 21807: Policy loss: -0.073680. Value loss: 0.036297. Entropy: 0.299915.\n",
      "episode: 7494   score: 345.0  epsilon: 1.0    steps: 96  evaluation reward: 491.25\n",
      "Training network. lr: 0.000083. clip: 0.033037\n",
      "Iteration 21808: Policy loss: 0.283320. Value loss: 0.127526. Entropy: 0.302899.\n",
      "Iteration 21809: Policy loss: 0.277372. Value loss: 0.042289. Entropy: 0.302002.\n",
      "Iteration 21810: Policy loss: 0.274894. Value loss: 0.028135. Entropy: 0.301246.\n",
      "episode: 7495   score: 730.0  epsilon: 1.0    steps: 688  evaluation reward: 493.85\n",
      "Training network. lr: 0.000083. clip: 0.033037\n",
      "Iteration 21811: Policy loss: 0.158107. Value loss: 0.106931. Entropy: 0.301447.\n",
      "Iteration 21812: Policy loss: 0.158201. Value loss: 0.048312. Entropy: 0.301874.\n",
      "Iteration 21813: Policy loss: 0.155230. Value loss: 0.035422. Entropy: 0.301845.\n",
      "episode: 7496   score: 650.0  epsilon: 1.0    steps: 152  evaluation reward: 493.6\n",
      "Training network. lr: 0.000083. clip: 0.033037\n",
      "Iteration 21814: Policy loss: 0.050448. Value loss: 0.087060. Entropy: 0.304157.\n",
      "Iteration 21815: Policy loss: 0.047420. Value loss: 0.036259. Entropy: 0.304116.\n",
      "Iteration 21816: Policy loss: 0.041641. Value loss: 0.024637. Entropy: 0.304117.\n",
      "Training network. lr: 0.000083. clip: 0.033037\n",
      "Iteration 21817: Policy loss: 0.063688. Value loss: 0.107390. Entropy: 0.297974.\n",
      "Iteration 21818: Policy loss: 0.060341. Value loss: 0.041949. Entropy: 0.297412.\n",
      "Iteration 21819: Policy loss: 0.062880. Value loss: 0.028703. Entropy: 0.296934.\n",
      "episode: 7497   score: 525.0  epsilon: 1.0    steps: 720  evaluation reward: 493.3\n",
      "Training network. lr: 0.000083. clip: 0.033037\n",
      "Iteration 21820: Policy loss: -0.371947. Value loss: 0.225300. Entropy: 0.305855.\n",
      "Iteration 21821: Policy loss: -0.362848. Value loss: 0.074389. Entropy: 0.305241.\n",
      "Iteration 21822: Policy loss: -0.366401. Value loss: 0.043810. Entropy: 0.303146.\n",
      "episode: 7498   score: 375.0  epsilon: 1.0    steps: 728  evaluation reward: 489.35\n",
      "Training network. lr: 0.000083. clip: 0.033037\n",
      "Iteration 21823: Policy loss: -0.073631. Value loss: 0.314201. Entropy: 0.296603.\n",
      "Iteration 21824: Policy loss: -0.076626. Value loss: 0.212188. Entropy: 0.296277.\n",
      "Iteration 21825: Policy loss: -0.068912. Value loss: 0.132913. Entropy: 0.295184.\n",
      "Training network. lr: 0.000083. clip: 0.033037\n",
      "Iteration 21826: Policy loss: -0.113262. Value loss: 0.097952. Entropy: 0.304816.\n",
      "Iteration 21827: Policy loss: -0.108885. Value loss: 0.035010. Entropy: 0.304287.\n",
      "Iteration 21828: Policy loss: -0.112689. Value loss: 0.024870. Entropy: 0.304580.\n",
      "episode: 7499   score: 485.0  epsilon: 1.0    steps: 720  evaluation reward: 490.6\n",
      "Training network. lr: 0.000083. clip: 0.033037\n",
      "Iteration 21829: Policy loss: 0.115208. Value loss: 0.081079. Entropy: 0.304774.\n",
      "Iteration 21830: Policy loss: 0.104583. Value loss: 0.044635. Entropy: 0.305553.\n",
      "Iteration 21831: Policy loss: 0.108180. Value loss: 0.032441. Entropy: 0.305340.\n",
      "episode: 7500   score: 440.0  epsilon: 1.0    steps: 200  evaluation reward: 489.75\n",
      "now time :  2019-09-06 12:45:08.470478\n",
      "episode: 7501   score: 420.0  epsilon: 1.0    steps: 472  evaluation reward: 487.95\n",
      "Training network. lr: 0.000083. clip: 0.033037\n",
      "Iteration 21832: Policy loss: -0.109538. Value loss: 0.143483. Entropy: 0.298633.\n",
      "Iteration 21833: Policy loss: -0.110975. Value loss: 0.056077. Entropy: 0.297826.\n",
      "Iteration 21834: Policy loss: -0.113102. Value loss: 0.042100. Entropy: 0.297453.\n",
      "Training network. lr: 0.000083. clip: 0.033037\n",
      "Iteration 21835: Policy loss: -0.107979. Value loss: 0.146201. Entropy: 0.301627.\n",
      "Iteration 21836: Policy loss: -0.108866. Value loss: 0.077638. Entropy: 0.300582.\n",
      "Iteration 21837: Policy loss: -0.115214. Value loss: 0.055929. Entropy: 0.301987.\n",
      "episode: 7502   score: 610.0  epsilon: 1.0    steps: 200  evaluation reward: 490.1\n",
      "Training network. lr: 0.000083. clip: 0.033037\n",
      "Iteration 21838: Policy loss: -0.440390. Value loss: 0.300337. Entropy: 0.294572.\n",
      "Iteration 21839: Policy loss: -0.439120. Value loss: 0.120543. Entropy: 0.293899.\n",
      "Iteration 21840: Policy loss: -0.450362. Value loss: 0.054650. Entropy: 0.294450.\n",
      "episode: 7503   score: 525.0  epsilon: 1.0    steps: 776  evaluation reward: 491.4\n",
      "Training network. lr: 0.000083. clip: 0.033037\n",
      "Iteration 21841: Policy loss: 0.000942. Value loss: 0.283116. Entropy: 0.302865.\n",
      "Iteration 21842: Policy loss: 0.001184. Value loss: 0.142304. Entropy: 0.302859.\n",
      "Iteration 21843: Policy loss: -0.001132. Value loss: 0.113282. Entropy: 0.302432.\n",
      "episode: 7504   score: 605.0  epsilon: 1.0    steps: 448  evaluation reward: 493.55\n",
      "Training network. lr: 0.000083. clip: 0.033037\n",
      "Iteration 21844: Policy loss: 0.204175. Value loss: 0.093159. Entropy: 0.291432.\n",
      "Iteration 21845: Policy loss: 0.193704. Value loss: 0.042351. Entropy: 0.292663.\n",
      "Iteration 21846: Policy loss: 0.189006. Value loss: 0.032962. Entropy: 0.292432.\n",
      "Training network. lr: 0.000083. clip: 0.033037\n",
      "Iteration 21847: Policy loss: 0.055269. Value loss: 0.122301. Entropy: 0.303228.\n",
      "Iteration 21848: Policy loss: 0.046316. Value loss: 0.060101. Entropy: 0.301703.\n",
      "Iteration 21849: Policy loss: 0.042775. Value loss: 0.049233. Entropy: 0.302692.\n",
      "Training network. lr: 0.000083. clip: 0.033037\n",
      "Iteration 21850: Policy loss: 0.006064. Value loss: 0.207028. Entropy: 0.311552.\n",
      "Iteration 21851: Policy loss: -0.019102. Value loss: 0.073464. Entropy: 0.311188.\n",
      "Iteration 21852: Policy loss: -0.019374. Value loss: 0.049252. Entropy: 0.311402.\n",
      "Training network. lr: 0.000082. clip: 0.032880\n",
      "Iteration 21853: Policy loss: 0.810710. Value loss: 0.377019. Entropy: 0.308337.\n",
      "Iteration 21854: Policy loss: 0.785230. Value loss: 0.120016. Entropy: 0.307320.\n",
      "Iteration 21855: Policy loss: 0.793567. Value loss: 0.058117. Entropy: 0.307353.\n",
      "episode: 7505   score: 285.0  epsilon: 1.0    steps: 1000  evaluation reward: 494.85\n",
      "Training network. lr: 0.000082. clip: 0.032880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21856: Policy loss: 0.088594. Value loss: 0.176507. Entropy: 0.312744.\n",
      "Iteration 21857: Policy loss: 0.088783. Value loss: 0.084290. Entropy: 0.312461.\n",
      "Iteration 21858: Policy loss: 0.083117. Value loss: 0.058071. Entropy: 0.311714.\n",
      "episode: 7506   score: 755.0  epsilon: 1.0    steps: 296  evaluation reward: 497.65\n",
      "Training network. lr: 0.000082. clip: 0.032880\n",
      "Iteration 21859: Policy loss: 0.174418. Value loss: 0.335853. Entropy: 0.306502.\n",
      "Iteration 21860: Policy loss: 0.169637. Value loss: 0.124823. Entropy: 0.306691.\n",
      "Iteration 21861: Policy loss: 0.164170. Value loss: 0.056948. Entropy: 0.307774.\n",
      "episode: 7507   score: 820.0  epsilon: 1.0    steps: 264  evaluation reward: 498.25\n",
      "episode: 7508   score: 390.0  epsilon: 1.0    steps: 728  evaluation reward: 497.9\n",
      "episode: 7509   score: 585.0  epsilon: 1.0    steps: 848  evaluation reward: 500.2\n",
      "Training network. lr: 0.000082. clip: 0.032880\n",
      "Iteration 21862: Policy loss: 0.025961. Value loss: 0.160177. Entropy: 0.297209.\n",
      "Iteration 21863: Policy loss: 0.020801. Value loss: 0.068338. Entropy: 0.297074.\n",
      "Iteration 21864: Policy loss: 0.020666. Value loss: 0.053972. Entropy: 0.296272.\n",
      "episode: 7510   score: 875.0  epsilon: 1.0    steps: 720  evaluation reward: 504.5\n",
      "Training network. lr: 0.000082. clip: 0.032880\n",
      "Iteration 21865: Policy loss: -0.036534. Value loss: 0.102222. Entropy: 0.299263.\n",
      "Iteration 21866: Policy loss: -0.040979. Value loss: 0.055135. Entropy: 0.300173.\n",
      "Iteration 21867: Policy loss: -0.044878. Value loss: 0.046498. Entropy: 0.299821.\n",
      "episode: 7511   score: 400.0  epsilon: 1.0    steps: 200  evaluation reward: 503.5\n",
      "Training network. lr: 0.000082. clip: 0.032880\n",
      "Iteration 21868: Policy loss: 0.186163. Value loss: 0.141743. Entropy: 0.303647.\n",
      "Iteration 21869: Policy loss: 0.183117. Value loss: 0.045735. Entropy: 0.303874.\n",
      "Iteration 21870: Policy loss: 0.175701. Value loss: 0.032575. Entropy: 0.302589.\n",
      "Training network. lr: 0.000082. clip: 0.032880\n",
      "Iteration 21871: Policy loss: 0.133838. Value loss: 0.083578. Entropy: 0.303189.\n",
      "Iteration 21872: Policy loss: 0.133219. Value loss: 0.035847. Entropy: 0.304053.\n",
      "Iteration 21873: Policy loss: 0.135806. Value loss: 0.025755. Entropy: 0.304641.\n",
      "Training network. lr: 0.000082. clip: 0.032880\n",
      "Iteration 21874: Policy loss: -0.014159. Value loss: 0.094703. Entropy: 0.316603.\n",
      "Iteration 21875: Policy loss: -0.020535. Value loss: 0.037202. Entropy: 0.316900.\n",
      "Iteration 21876: Policy loss: -0.018609. Value loss: 0.025612. Entropy: 0.316183.\n",
      "episode: 7512   score: 465.0  epsilon: 1.0    steps: 48  evaluation reward: 502.55\n",
      "Training network. lr: 0.000082. clip: 0.032880\n",
      "Iteration 21877: Policy loss: -0.086135. Value loss: 0.249466. Entropy: 0.309777.\n",
      "Iteration 21878: Policy loss: -0.082333. Value loss: 0.110690. Entropy: 0.309024.\n",
      "Iteration 21879: Policy loss: -0.098164. Value loss: 0.070228. Entropy: 0.308832.\n",
      "Training network. lr: 0.000082. clip: 0.032880\n",
      "Iteration 21880: Policy loss: -0.048357. Value loss: 0.109726. Entropy: 0.306221.\n",
      "Iteration 21881: Policy loss: -0.050336. Value loss: 0.047510. Entropy: 0.305513.\n",
      "Iteration 21882: Policy loss: -0.047825. Value loss: 0.032951. Entropy: 0.305681.\n",
      "episode: 7513   score: 380.0  epsilon: 1.0    steps: 80  evaluation reward: 502.9\n",
      "episode: 7514   score: 360.0  epsilon: 1.0    steps: 656  evaluation reward: 503.35\n",
      "Training network. lr: 0.000082. clip: 0.032880\n",
      "Iteration 21883: Policy loss: 0.078006. Value loss: 0.126673. Entropy: 0.300387.\n",
      "Iteration 21884: Policy loss: 0.073009. Value loss: 0.060390. Entropy: 0.298922.\n",
      "Iteration 21885: Policy loss: 0.067946. Value loss: 0.043989. Entropy: 0.299567.\n",
      "episode: 7515   score: 440.0  epsilon: 1.0    steps: 520  evaluation reward: 500.05\n",
      "episode: 7516   score: 430.0  epsilon: 1.0    steps: 688  evaluation reward: 500.95\n",
      "episode: 7517   score: 425.0  epsilon: 1.0    steps: 936  evaluation reward: 500.1\n",
      "Training network. lr: 0.000082. clip: 0.032880\n",
      "Iteration 21886: Policy loss: -0.145298. Value loss: 0.135325. Entropy: 0.289176.\n",
      "Iteration 21887: Policy loss: -0.161455. Value loss: 0.099537. Entropy: 0.290019.\n",
      "Iteration 21888: Policy loss: -0.160648. Value loss: 0.072083. Entropy: 0.289985.\n",
      "Training network. lr: 0.000082. clip: 0.032880\n",
      "Iteration 21889: Policy loss: -0.364168. Value loss: 0.276417. Entropy: 0.300465.\n",
      "Iteration 21890: Policy loss: -0.363637. Value loss: 0.113607. Entropy: 0.301362.\n",
      "Iteration 21891: Policy loss: -0.370704. Value loss: 0.064381. Entropy: 0.300365.\n",
      "episode: 7518   score: 640.0  epsilon: 1.0    steps: 568  evaluation reward: 502.45\n",
      "Training network. lr: 0.000082. clip: 0.032880\n",
      "Iteration 21892: Policy loss: -0.199588. Value loss: 0.210836. Entropy: 0.299134.\n",
      "Iteration 21893: Policy loss: -0.223311. Value loss: 0.081774. Entropy: 0.297723.\n",
      "Iteration 21894: Policy loss: -0.212240. Value loss: 0.058038. Entropy: 0.298814.\n",
      "Training network. lr: 0.000082. clip: 0.032880\n",
      "Iteration 21895: Policy loss: -0.635499. Value loss: 0.314807. Entropy: 0.303965.\n",
      "Iteration 21896: Policy loss: -0.648334. Value loss: 0.104381. Entropy: 0.304041.\n",
      "Iteration 21897: Policy loss: -0.647449. Value loss: 0.076455. Entropy: 0.303987.\n",
      "Training network. lr: 0.000082. clip: 0.032880\n",
      "Iteration 21898: Policy loss: 0.773781. Value loss: 0.266349. Entropy: 0.303644.\n",
      "Iteration 21899: Policy loss: 0.761362. Value loss: 0.084748. Entropy: 0.303816.\n",
      "Iteration 21900: Policy loss: 0.764257. Value loss: 0.057874. Entropy: 0.303740.\n",
      "Training network. lr: 0.000082. clip: 0.032732\n",
      "Iteration 21901: Policy loss: -0.095672. Value loss: 0.168912. Entropy: 0.309576.\n",
      "Iteration 21902: Policy loss: -0.096125. Value loss: 0.078940. Entropy: 0.308111.\n",
      "Iteration 21903: Policy loss: -0.094044. Value loss: 0.058494. Entropy: 0.308226.\n",
      "episode: 7519   score: 580.0  epsilon: 1.0    steps: 872  evaluation reward: 504.95\n",
      "Training network. lr: 0.000082. clip: 0.032732\n",
      "Iteration 21904: Policy loss: 0.188251. Value loss: 0.341685. Entropy: 0.304331.\n",
      "Iteration 21905: Policy loss: 0.184637. Value loss: 0.133034. Entropy: 0.303588.\n",
      "Iteration 21906: Policy loss: 0.168966. Value loss: 0.094446. Entropy: 0.304793.\n",
      "episode: 7520   score: 995.0  epsilon: 1.0    steps: 88  evaluation reward: 511.6\n",
      "Training network. lr: 0.000082. clip: 0.032732\n",
      "Iteration 21907: Policy loss: 0.303711. Value loss: 0.247681. Entropy: 0.309376.\n",
      "Iteration 21908: Policy loss: 0.290947. Value loss: 0.071670. Entropy: 0.308693.\n",
      "Iteration 21909: Policy loss: 0.300771. Value loss: 0.039612. Entropy: 0.308890.\n",
      "episode: 7521   score: 425.0  epsilon: 1.0    steps: 880  evaluation reward: 512.4\n",
      "Training network. lr: 0.000082. clip: 0.032732\n",
      "Iteration 21910: Policy loss: -0.052643. Value loss: 0.138195. Entropy: 0.298884.\n",
      "Iteration 21911: Policy loss: -0.058884. Value loss: 0.063685. Entropy: 0.297558.\n",
      "Iteration 21912: Policy loss: -0.042816. Value loss: 0.040562. Entropy: 0.298282.\n",
      "episode: 7522   score: 715.0  epsilon: 1.0    steps: 344  evaluation reward: 513.25\n",
      "Training network. lr: 0.000082. clip: 0.032732\n",
      "Iteration 21913: Policy loss: 0.100588. Value loss: 0.073306. Entropy: 0.306279.\n",
      "Iteration 21914: Policy loss: 0.100646. Value loss: 0.023857. Entropy: 0.305094.\n",
      "Iteration 21915: Policy loss: 0.096675. Value loss: 0.017313. Entropy: 0.303019.\n",
      "Training network. lr: 0.000082. clip: 0.032732\n",
      "Iteration 21916: Policy loss: 0.468433. Value loss: 0.277848. Entropy: 0.308904.\n",
      "Iteration 21917: Policy loss: 0.464722. Value loss: 0.099859. Entropy: 0.307791.\n",
      "Iteration 21918: Policy loss: 0.463934. Value loss: 0.074251. Entropy: 0.307412.\n",
      "episode: 7523   score: 830.0  epsilon: 1.0    steps: 168  evaluation reward: 518.7\n",
      "Training network. lr: 0.000082. clip: 0.032732\n",
      "Iteration 21919: Policy loss: 0.300641. Value loss: 0.142778. Entropy: 0.300828.\n",
      "Iteration 21920: Policy loss: 0.289841. Value loss: 0.063082. Entropy: 0.298646.\n",
      "Iteration 21921: Policy loss: 0.293383. Value loss: 0.041266. Entropy: 0.298719.\n",
      "episode: 7524   score: 495.0  epsilon: 1.0    steps: 184  evaluation reward: 519.15\n",
      "episode: 7525   score: 675.0  epsilon: 1.0    steps: 192  evaluation reward: 519.95\n",
      "Training network. lr: 0.000082. clip: 0.032732\n",
      "Iteration 21922: Policy loss: -0.160955. Value loss: 0.373309. Entropy: 0.288807.\n",
      "Iteration 21923: Policy loss: -0.162196. Value loss: 0.208496. Entropy: 0.288743.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21924: Policy loss: -0.165999. Value loss: 0.130046. Entropy: 0.290384.\n",
      "Training network. lr: 0.000082. clip: 0.032732\n",
      "Iteration 21925: Policy loss: 0.122892. Value loss: 0.107591. Entropy: 0.300658.\n",
      "Iteration 21926: Policy loss: 0.118144. Value loss: 0.046003. Entropy: 0.300401.\n",
      "Iteration 21927: Policy loss: 0.115054. Value loss: 0.029500. Entropy: 0.299266.\n",
      "episode: 7526   score: 525.0  epsilon: 1.0    steps: 168  evaluation reward: 519.7\n",
      "episode: 7527   score: 330.0  epsilon: 1.0    steps: 184  evaluation reward: 518.85\n",
      "episode: 7528   score: 470.0  epsilon: 1.0    steps: 912  evaluation reward: 518.25\n",
      "Training network. lr: 0.000082. clip: 0.032732\n",
      "Iteration 21928: Policy loss: 0.036645. Value loss: 0.127131. Entropy: 0.290729.\n",
      "Iteration 21929: Policy loss: 0.035697. Value loss: 0.057006. Entropy: 0.290739.\n",
      "Iteration 21930: Policy loss: 0.033226. Value loss: 0.042281. Entropy: 0.289898.\n",
      "episode: 7529   score: 335.0  epsilon: 1.0    steps: 888  evaluation reward: 511.75\n",
      "Training network. lr: 0.000082. clip: 0.032732\n",
      "Iteration 21931: Policy loss: -0.226236. Value loss: 0.273941. Entropy: 0.293836.\n",
      "Iteration 21932: Policy loss: -0.232928. Value loss: 0.192253. Entropy: 0.292852.\n",
      "Iteration 21933: Policy loss: -0.220502. Value loss: 0.138723. Entropy: 0.293294.\n",
      "episode: 7530   score: 570.0  epsilon: 1.0    steps: 1024  evaluation reward: 510.75\n",
      "Training network. lr: 0.000082. clip: 0.032732\n",
      "Iteration 21934: Policy loss: 0.080271. Value loss: 0.117799. Entropy: 0.302499.\n",
      "Iteration 21935: Policy loss: 0.079016. Value loss: 0.050534. Entropy: 0.302966.\n",
      "Iteration 21936: Policy loss: 0.082210. Value loss: 0.033500. Entropy: 0.302757.\n",
      "Training network. lr: 0.000082. clip: 0.032732\n",
      "Iteration 21937: Policy loss: -0.101065. Value loss: 0.341185. Entropy: 0.299843.\n",
      "Iteration 21938: Policy loss: -0.109156. Value loss: 0.228913. Entropy: 0.300542.\n",
      "Iteration 21939: Policy loss: -0.118721. Value loss: 0.170532. Entropy: 0.300661.\n",
      "Training network. lr: 0.000082. clip: 0.032732\n",
      "Iteration 21940: Policy loss: -0.040581. Value loss: 0.129820. Entropy: 0.298334.\n",
      "Iteration 21941: Policy loss: -0.042913. Value loss: 0.052621. Entropy: 0.299794.\n",
      "Iteration 21942: Policy loss: -0.042939. Value loss: 0.034291. Entropy: 0.300128.\n",
      "episode: 7531   score: 620.0  epsilon: 1.0    steps: 232  evaluation reward: 513.65\n",
      "Training network. lr: 0.000082. clip: 0.032732\n",
      "Iteration 21943: Policy loss: 0.349470. Value loss: 0.146482. Entropy: 0.302736.\n",
      "Iteration 21944: Policy loss: 0.346429. Value loss: 0.048726. Entropy: 0.301149.\n",
      "Iteration 21945: Policy loss: 0.336727. Value loss: 0.028154. Entropy: 0.301371.\n",
      "Training network. lr: 0.000082. clip: 0.032732\n",
      "Iteration 21946: Policy loss: 0.129506. Value loss: 0.104976. Entropy: 0.299152.\n",
      "Iteration 21947: Policy loss: 0.127405. Value loss: 0.047716. Entropy: 0.300210.\n",
      "Iteration 21948: Policy loss: 0.122346. Value loss: 0.032462. Entropy: 0.298395.\n",
      "episode: 7532   score: 330.0  epsilon: 1.0    steps: 72  evaluation reward: 513.05\n",
      "Training network. lr: 0.000082. clip: 0.032732\n",
      "Iteration 21949: Policy loss: -0.209826. Value loss: 0.119795. Entropy: 0.301447.\n",
      "Iteration 21950: Policy loss: -0.207960. Value loss: 0.051814. Entropy: 0.301336.\n",
      "Iteration 21951: Policy loss: -0.210808. Value loss: 0.034204. Entropy: 0.300932.\n",
      "episode: 7533   score: 640.0  epsilon: 1.0    steps: 280  evaluation reward: 512.5\n",
      "episode: 7534   score: 420.0  epsilon: 1.0    steps: 912  evaluation reward: 512.6\n",
      "Training network. lr: 0.000081. clip: 0.032576\n",
      "Iteration 21952: Policy loss: 0.133328. Value loss: 0.100294. Entropy: 0.289395.\n",
      "Iteration 21953: Policy loss: 0.121321. Value loss: 0.044151. Entropy: 0.289037.\n",
      "Iteration 21954: Policy loss: 0.121547. Value loss: 0.035193. Entropy: 0.288442.\n",
      "Training network. lr: 0.000081. clip: 0.032576\n",
      "Iteration 21955: Policy loss: 0.162601. Value loss: 0.100285. Entropy: 0.306869.\n",
      "Iteration 21956: Policy loss: 0.160483. Value loss: 0.047823. Entropy: 0.308633.\n",
      "Iteration 21957: Policy loss: 0.156788. Value loss: 0.031798. Entropy: 0.307457.\n",
      "Training network. lr: 0.000081. clip: 0.032576\n",
      "Iteration 21958: Policy loss: 0.151022. Value loss: 0.029802. Entropy: 0.309232.\n",
      "Iteration 21959: Policy loss: 0.145558. Value loss: 0.014618. Entropy: 0.309346.\n",
      "Iteration 21960: Policy loss: 0.150171. Value loss: 0.011077. Entropy: 0.309227.\n",
      "episode: 7535   score: 740.0  epsilon: 1.0    steps: 720  evaluation reward: 513.6\n",
      "episode: 7536   score: 555.0  epsilon: 1.0    steps: 1000  evaluation reward: 514.35\n",
      "Training network. lr: 0.000081. clip: 0.032576\n",
      "Iteration 21961: Policy loss: -0.107312. Value loss: 0.236309. Entropy: 0.305495.\n",
      "Iteration 21962: Policy loss: -0.111896. Value loss: 0.159258. Entropy: 0.305759.\n",
      "Iteration 21963: Policy loss: -0.108762. Value loss: 0.091408. Entropy: 0.304979.\n",
      "Training network. lr: 0.000081. clip: 0.032576\n",
      "Iteration 21964: Policy loss: -0.136207. Value loss: 0.214732. Entropy: 0.302823.\n",
      "Iteration 21965: Policy loss: -0.135992. Value loss: 0.105624. Entropy: 0.303297.\n",
      "Iteration 21966: Policy loss: -0.141153. Value loss: 0.062485. Entropy: 0.303260.\n",
      "episode: 7537   score: 420.0  epsilon: 1.0    steps: 872  evaluation reward: 514.7\n",
      "episode: 7538   score: 495.0  epsilon: 1.0    steps: 896  evaluation reward: 517.55\n",
      "Training network. lr: 0.000081. clip: 0.032576\n",
      "Iteration 21967: Policy loss: 0.016641. Value loss: 0.138423. Entropy: 0.298757.\n",
      "Iteration 21968: Policy loss: 0.010844. Value loss: 0.080123. Entropy: 0.297089.\n",
      "Iteration 21969: Policy loss: 0.012390. Value loss: 0.063004. Entropy: 0.297057.\n",
      "episode: 7539   score: 400.0  epsilon: 1.0    steps: 488  evaluation reward: 512.4\n",
      "Training network. lr: 0.000081. clip: 0.032576\n",
      "Iteration 21970: Policy loss: -0.232033. Value loss: 0.210606. Entropy: 0.291047.\n",
      "Iteration 21971: Policy loss: -0.252836. Value loss: 0.073662. Entropy: 0.290467.\n",
      "Iteration 21972: Policy loss: -0.254498. Value loss: 0.036213. Entropy: 0.290288.\n",
      "Training network. lr: 0.000081. clip: 0.032576\n",
      "Iteration 21973: Policy loss: 0.015863. Value loss: 0.132282. Entropy: 0.303213.\n",
      "Iteration 21974: Policy loss: 0.006628. Value loss: 0.048844. Entropy: 0.303552.\n",
      "Iteration 21975: Policy loss: 0.008575. Value loss: 0.030269. Entropy: 0.304739.\n",
      "Training network. lr: 0.000081. clip: 0.032576\n",
      "Iteration 21976: Policy loss: 0.047687. Value loss: 0.079335. Entropy: 0.309585.\n",
      "Iteration 21977: Policy loss: 0.047857. Value loss: 0.032793. Entropy: 0.309565.\n",
      "Iteration 21978: Policy loss: 0.045016. Value loss: 0.024399. Entropy: 0.309297.\n",
      "episode: 7540   score: 725.0  epsilon: 1.0    steps: 8  evaluation reward: 516.15\n",
      "episode: 7541   score: 640.0  epsilon: 1.0    steps: 200  evaluation reward: 518.7\n",
      "Training network. lr: 0.000081. clip: 0.032576\n",
      "Iteration 21979: Policy loss: 0.528121. Value loss: 0.365770. Entropy: 0.289934.\n",
      "Iteration 21980: Policy loss: 0.567130. Value loss: 0.069945. Entropy: 0.288802.\n",
      "Iteration 21981: Policy loss: 0.534145. Value loss: 0.041511. Entropy: 0.287942.\n",
      "Training network. lr: 0.000081. clip: 0.032576\n",
      "Iteration 21982: Policy loss: -0.044694. Value loss: 0.094149. Entropy: 0.298460.\n",
      "Iteration 21983: Policy loss: -0.053817. Value loss: 0.040816. Entropy: 0.297599.\n",
      "Iteration 21984: Policy loss: -0.051973. Value loss: 0.030953. Entropy: 0.298736.\n",
      "episode: 7542   score: 820.0  epsilon: 1.0    steps: 616  evaluation reward: 522.65\n",
      "episode: 7543   score: 435.0  epsilon: 1.0    steps: 944  evaluation reward: 525.45\n",
      "Training network. lr: 0.000081. clip: 0.032576\n",
      "Iteration 21985: Policy loss: 0.124704. Value loss: 0.164466. Entropy: 0.294629.\n",
      "Iteration 21986: Policy loss: 0.136906. Value loss: 0.095038. Entropy: 0.293635.\n",
      "Iteration 21987: Policy loss: 0.119874. Value loss: 0.088338. Entropy: 0.293902.\n",
      "Training network. lr: 0.000081. clip: 0.032576\n",
      "Iteration 21988: Policy loss: 0.556960. Value loss: 0.209099. Entropy: 0.300960.\n",
      "Iteration 21989: Policy loss: 0.552659. Value loss: 0.059698. Entropy: 0.299095.\n",
      "Iteration 21990: Policy loss: 0.553718. Value loss: 0.041196. Entropy: 0.300540.\n",
      "episode: 7544   score: 625.0  epsilon: 1.0    steps: 656  evaluation reward: 525.2\n",
      "Training network. lr: 0.000081. clip: 0.032576\n",
      "Iteration 21991: Policy loss: -0.005532. Value loss: 0.107820. Entropy: 0.287557.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21992: Policy loss: 0.002064. Value loss: 0.048381. Entropy: 0.286894.\n",
      "Iteration 21993: Policy loss: -0.010420. Value loss: 0.037268. Entropy: 0.285026.\n",
      "episode: 7545   score: 590.0  epsilon: 1.0    steps: 96  evaluation reward: 522.15\n",
      "Training network. lr: 0.000081. clip: 0.032576\n",
      "Iteration 21994: Policy loss: -0.483542. Value loss: 0.244811. Entropy: 0.297065.\n",
      "Iteration 21995: Policy loss: -0.490404. Value loss: 0.111685. Entropy: 0.299283.\n",
      "Iteration 21996: Policy loss: -0.492357. Value loss: 0.068613. Entropy: 0.298102.\n",
      "Training network. lr: 0.000081. clip: 0.032576\n",
      "Iteration 21997: Policy loss: 0.660747. Value loss: 0.246670. Entropy: 0.305728.\n",
      "Iteration 21998: Policy loss: 0.672051. Value loss: 0.065187. Entropy: 0.306661.\n",
      "Iteration 21999: Policy loss: 0.654435. Value loss: 0.040080. Entropy: 0.305622.\n",
      "Training network. lr: 0.000081. clip: 0.032576\n",
      "Iteration 22000: Policy loss: 0.082799. Value loss: 0.111819. Entropy: 0.315453.\n",
      "Iteration 22001: Policy loss: 0.075495. Value loss: 0.051983. Entropy: 0.315061.\n",
      "Iteration 22002: Policy loss: 0.071721. Value loss: 0.038632. Entropy: 0.315105.\n",
      "episode: 7546   score: 555.0  epsilon: 1.0    steps: 296  evaluation reward: 524.25\n",
      "Training network. lr: 0.000081. clip: 0.032419\n",
      "Iteration 22003: Policy loss: 0.211246. Value loss: 0.099777. Entropy: 0.297197.\n",
      "Iteration 22004: Policy loss: 0.210432. Value loss: 0.035073. Entropy: 0.295251.\n",
      "Iteration 22005: Policy loss: 0.201651. Value loss: 0.026182. Entropy: 0.295709.\n",
      "Training network. lr: 0.000081. clip: 0.032419\n",
      "Iteration 22006: Policy loss: -0.217103. Value loss: 0.398925. Entropy: 0.306645.\n",
      "Iteration 22007: Policy loss: -0.210368. Value loss: 0.192085. Entropy: 0.306013.\n",
      "Iteration 22008: Policy loss: -0.235319. Value loss: 0.136853. Entropy: 0.307263.\n",
      "episode: 7547   score: 365.0  epsilon: 1.0    steps: 248  evaluation reward: 521.0\n",
      "episode: 7548   score: 740.0  epsilon: 1.0    steps: 896  evaluation reward: 525.4\n",
      "Training network. lr: 0.000081. clip: 0.032419\n",
      "Iteration 22009: Policy loss: -0.053118. Value loss: 0.241663. Entropy: 0.293333.\n",
      "Iteration 22010: Policy loss: -0.053058. Value loss: 0.117294. Entropy: 0.291600.\n",
      "Iteration 22011: Policy loss: -0.066131. Value loss: 0.084078. Entropy: 0.291212.\n",
      "episode: 7549   score: 240.0  epsilon: 1.0    steps: 344  evaluation reward: 520.95\n",
      "Training network. lr: 0.000081. clip: 0.032419\n",
      "Iteration 22012: Policy loss: 0.039015. Value loss: 0.297838. Entropy: 0.288200.\n",
      "Iteration 22013: Policy loss: 0.053015. Value loss: 0.158026. Entropy: 0.287269.\n",
      "Iteration 22014: Policy loss: 0.049563. Value loss: 0.097329. Entropy: 0.286620.\n",
      "episode: 7550   score: 425.0  epsilon: 1.0    steps: 880  evaluation reward: 521.1\n",
      "now time :  2019-09-06 12:56:17.853598\n",
      "episode: 7551   score: 775.0  epsilon: 1.0    steps: 1000  evaluation reward: 524.1\n",
      "Training network. lr: 0.000081. clip: 0.032419\n",
      "Iteration 22015: Policy loss: 0.209390. Value loss: 0.171341. Entropy: 0.298707.\n",
      "Iteration 22016: Policy loss: 0.204531. Value loss: 0.073267. Entropy: 0.296879.\n",
      "Iteration 22017: Policy loss: 0.196668. Value loss: 0.053615. Entropy: 0.296817.\n",
      "Training network. lr: 0.000081. clip: 0.032419\n",
      "Iteration 22018: Policy loss: -0.208729. Value loss: 0.315277. Entropy: 0.288306.\n",
      "Iteration 22019: Policy loss: -0.202136. Value loss: 0.129009. Entropy: 0.287559.\n",
      "Iteration 22020: Policy loss: -0.214905. Value loss: 0.086293. Entropy: 0.288622.\n",
      "episode: 7552   score: 605.0  epsilon: 1.0    steps: 40  evaluation reward: 524.85\n",
      "episode: 7553   score: 495.0  epsilon: 1.0    steps: 760  evaluation reward: 525.4\n",
      "Training network. lr: 0.000081. clip: 0.032419\n",
      "Iteration 22021: Policy loss: 0.023025. Value loss: 0.109370. Entropy: 0.289877.\n",
      "Iteration 22022: Policy loss: 0.019535. Value loss: 0.056168. Entropy: 0.290357.\n",
      "Iteration 22023: Policy loss: 0.020914. Value loss: 0.041955. Entropy: 0.290282.\n",
      "episode: 7554   score: 550.0  epsilon: 1.0    steps: 816  evaluation reward: 527.9\n",
      "Training network. lr: 0.000081. clip: 0.032419\n",
      "Iteration 22024: Policy loss: 0.202849. Value loss: 0.124073. Entropy: 0.292091.\n",
      "Iteration 22025: Policy loss: 0.201199. Value loss: 0.051808. Entropy: 0.291341.\n",
      "Iteration 22026: Policy loss: 0.203043. Value loss: 0.038379. Entropy: 0.291554.\n",
      "Training network. lr: 0.000081. clip: 0.032419\n",
      "Iteration 22027: Policy loss: 0.019665. Value loss: 0.264830. Entropy: 0.305577.\n",
      "Iteration 22028: Policy loss: 0.029752. Value loss: 0.079953. Entropy: 0.304222.\n",
      "Iteration 22029: Policy loss: 0.004781. Value loss: 0.040309. Entropy: 0.306195.\n",
      "Training network. lr: 0.000081. clip: 0.032419\n",
      "Iteration 22030: Policy loss: -0.137198. Value loss: 0.250567. Entropy: 0.309016.\n",
      "Iteration 22031: Policy loss: -0.137407. Value loss: 0.081692. Entropy: 0.308164.\n",
      "Iteration 22032: Policy loss: -0.139670. Value loss: 0.056884. Entropy: 0.308448.\n",
      "episode: 7555   score: 210.0  epsilon: 1.0    steps: 208  evaluation reward: 524.7\n",
      "episode: 7556   score: 605.0  epsilon: 1.0    steps: 512  evaluation reward: 525.4\n",
      "Training network. lr: 0.000081. clip: 0.032419\n",
      "Iteration 22033: Policy loss: 0.243881. Value loss: 0.172718. Entropy: 0.285703.\n",
      "Iteration 22034: Policy loss: 0.238750. Value loss: 0.092391. Entropy: 0.285334.\n",
      "Iteration 22035: Policy loss: 0.229835. Value loss: 0.066102. Entropy: 0.286910.\n",
      "episode: 7557   score: 535.0  epsilon: 1.0    steps: 304  evaluation reward: 527.3\n",
      "Training network. lr: 0.000081. clip: 0.032419\n",
      "Iteration 22036: Policy loss: -0.005213. Value loss: 0.254371. Entropy: 0.285957.\n",
      "Iteration 22037: Policy loss: -0.022004. Value loss: 0.128350. Entropy: 0.284348.\n",
      "Iteration 22038: Policy loss: -0.021425. Value loss: 0.095238. Entropy: 0.284569.\n",
      "episode: 7558   score: 370.0  epsilon: 1.0    steps: 424  evaluation reward: 525.75\n",
      "Training network. lr: 0.000081. clip: 0.032419\n",
      "Iteration 22039: Policy loss: 0.025604. Value loss: 0.088044. Entropy: 0.289912.\n",
      "Iteration 22040: Policy loss: 0.022483. Value loss: 0.040889. Entropy: 0.290976.\n",
      "Iteration 22041: Policy loss: 0.016274. Value loss: 0.030872. Entropy: 0.290274.\n",
      "episode: 7559   score: 590.0  epsilon: 1.0    steps: 24  evaluation reward: 527.8\n",
      "episode: 7560   score: 485.0  epsilon: 1.0    steps: 680  evaluation reward: 529.45\n",
      "episode: 7561   score: 345.0  epsilon: 1.0    steps: 904  evaluation reward: 526.7\n",
      "Training network. lr: 0.000081. clip: 0.032419\n",
      "Iteration 22042: Policy loss: 0.071704. Value loss: 0.182405. Entropy: 0.282870.\n",
      "Iteration 22043: Policy loss: 0.072610. Value loss: 0.096273. Entropy: 0.283791.\n",
      "Iteration 22044: Policy loss: 0.058673. Value loss: 0.076500. Entropy: 0.283586.\n",
      "Training network. lr: 0.000081. clip: 0.032419\n",
      "Iteration 22045: Policy loss: -0.247972. Value loss: 0.126735. Entropy: 0.301396.\n",
      "Iteration 22046: Policy loss: -0.246232. Value loss: 0.046108. Entropy: 0.301436.\n",
      "Iteration 22047: Policy loss: -0.252332. Value loss: 0.031430. Entropy: 0.299843.\n",
      "Training network. lr: 0.000081. clip: 0.032419\n",
      "Iteration 22048: Policy loss: -0.077342. Value loss: 0.311821. Entropy: 0.300714.\n",
      "Iteration 22049: Policy loss: -0.067429. Value loss: 0.137058. Entropy: 0.299277.\n",
      "Iteration 22050: Policy loss: -0.076033. Value loss: 0.096823. Entropy: 0.300108.\n",
      "Training network. lr: 0.000081. clip: 0.032272\n",
      "Iteration 22051: Policy loss: 0.022276. Value loss: 0.142865. Entropy: 0.302169.\n",
      "Iteration 22052: Policy loss: 0.012165. Value loss: 0.066713. Entropy: 0.302617.\n",
      "Iteration 22053: Policy loss: 0.015470. Value loss: 0.041906. Entropy: 0.303807.\n",
      "Training network. lr: 0.000081. clip: 0.032272\n",
      "Iteration 22054: Policy loss: -0.065996. Value loss: 0.615820. Entropy: 0.303194.\n",
      "Iteration 22055: Policy loss: -0.079310. Value loss: 0.413536. Entropy: 0.303194.\n",
      "Iteration 22056: Policy loss: -0.060075. Value loss: 0.317686. Entropy: 0.302766.\n",
      "episode: 7562   score: 445.0  epsilon: 1.0    steps: 720  evaluation reward: 523.6\n",
      "Training network. lr: 0.000081. clip: 0.032272\n",
      "Iteration 22057: Policy loss: 0.118756. Value loss: 0.130247. Entropy: 0.295525.\n",
      "Iteration 22058: Policy loss: 0.116242. Value loss: 0.059811. Entropy: 0.295732.\n",
      "Iteration 22059: Policy loss: 0.117918. Value loss: 0.041622. Entropy: 0.295383.\n",
      "episode: 7563   score: 605.0  epsilon: 1.0    steps: 160  evaluation reward: 525.25\n",
      "episode: 7564   score: 900.0  epsilon: 1.0    steps: 296  evaluation reward: 531.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000081. clip: 0.032272\n",
      "Iteration 22060: Policy loss: -0.086945. Value loss: 0.115924. Entropy: 0.291431.\n",
      "Iteration 22061: Policy loss: -0.080778. Value loss: 0.057611. Entropy: 0.289371.\n",
      "Iteration 22062: Policy loss: -0.087568. Value loss: 0.042141. Entropy: 0.288550.\n",
      "Training network. lr: 0.000081. clip: 0.032272\n",
      "Iteration 22063: Policy loss: -0.003481. Value loss: 0.191923. Entropy: 0.300529.\n",
      "Iteration 22064: Policy loss: -0.017358. Value loss: 0.080988. Entropy: 0.298929.\n",
      "Iteration 22065: Policy loss: -0.015315. Value loss: 0.056644. Entropy: 0.297531.\n",
      "episode: 7565   score: 420.0  epsilon: 1.0    steps: 728  evaluation reward: 532.3\n",
      "episode: 7566   score: 730.0  epsilon: 1.0    steps: 784  evaluation reward: 534.75\n",
      "Training network. lr: 0.000081. clip: 0.032272\n",
      "Iteration 22066: Policy loss: 0.254074. Value loss: 0.180597. Entropy: 0.291831.\n",
      "Iteration 22067: Policy loss: 0.238102. Value loss: 0.094500. Entropy: 0.291976.\n",
      "Iteration 22068: Policy loss: 0.236842. Value loss: 0.068377. Entropy: 0.290561.\n",
      "episode: 7567   score: 590.0  epsilon: 1.0    steps: 168  evaluation reward: 533.0\n",
      "Training network. lr: 0.000081. clip: 0.032272\n",
      "Iteration 22069: Policy loss: 0.243757. Value loss: 0.154181. Entropy: 0.295783.\n",
      "Iteration 22070: Policy loss: 0.246888. Value loss: 0.085060. Entropy: 0.294892.\n",
      "Iteration 22071: Policy loss: 0.225905. Value loss: 0.064422. Entropy: 0.295426.\n",
      "episode: 7568   score: 620.0  epsilon: 1.0    steps: 424  evaluation reward: 530.65\n",
      "Training network. lr: 0.000081. clip: 0.032272\n",
      "Iteration 22072: Policy loss: -0.020148. Value loss: 0.132373. Entropy: 0.284689.\n",
      "Iteration 22073: Policy loss: -0.026735. Value loss: 0.077293. Entropy: 0.285478.\n",
      "Iteration 22074: Policy loss: -0.027050. Value loss: 0.055626. Entropy: 0.285794.\n",
      "Training network. lr: 0.000081. clip: 0.032272\n",
      "Iteration 22075: Policy loss: -0.302595. Value loss: 0.307303. Entropy: 0.309160.\n",
      "Iteration 22076: Policy loss: -0.302870. Value loss: 0.155743. Entropy: 0.310288.\n",
      "Iteration 22077: Policy loss: -0.293908. Value loss: 0.099279. Entropy: 0.308931.\n",
      "Training network. lr: 0.000081. clip: 0.032272\n",
      "Iteration 22078: Policy loss: 0.662495. Value loss: 0.185491. Entropy: 0.304291.\n",
      "Iteration 22079: Policy loss: 0.652033. Value loss: 0.062729. Entropy: 0.304123.\n",
      "Iteration 22080: Policy loss: 0.646764. Value loss: 0.039532. Entropy: 0.304320.\n",
      "Training network. lr: 0.000081. clip: 0.032272\n",
      "Iteration 22081: Policy loss: -0.175452. Value loss: 0.256394. Entropy: 0.299917.\n",
      "Iteration 22082: Policy loss: -0.182852. Value loss: 0.164768. Entropy: 0.300013.\n",
      "Iteration 22083: Policy loss: -0.179421. Value loss: 0.120171. Entropy: 0.299858.\n",
      "episode: 7569   score: 425.0  epsilon: 1.0    steps: 424  evaluation reward: 531.45\n",
      "Training network. lr: 0.000081. clip: 0.032272\n",
      "Iteration 22084: Policy loss: 0.169238. Value loss: 0.118049. Entropy: 0.289862.\n",
      "Iteration 22085: Policy loss: 0.169656. Value loss: 0.072418. Entropy: 0.290296.\n",
      "Iteration 22086: Policy loss: 0.166806. Value loss: 0.060704. Entropy: 0.291124.\n",
      "episode: 7570   score: 240.0  epsilon: 1.0    steps: 520  evaluation reward: 526.3\n",
      "episode: 7571   score: 515.0  epsilon: 1.0    steps: 984  evaluation reward: 528.0\n",
      "Training network. lr: 0.000081. clip: 0.032272\n",
      "Iteration 22087: Policy loss: 0.136910. Value loss: 0.157787. Entropy: 0.297814.\n",
      "Iteration 22088: Policy loss: 0.136985. Value loss: 0.078013. Entropy: 0.299833.\n",
      "Iteration 22089: Policy loss: 0.134596. Value loss: 0.055143. Entropy: 0.299455.\n",
      "episode: 7572   score: 420.0  epsilon: 1.0    steps: 96  evaluation reward: 528.75\n",
      "Training network. lr: 0.000081. clip: 0.032272\n",
      "Iteration 22090: Policy loss: -0.011167. Value loss: 0.110829. Entropy: 0.287173.\n",
      "Iteration 22091: Policy loss: -0.007059. Value loss: 0.042989. Entropy: 0.288098.\n",
      "Iteration 22092: Policy loss: -0.013249. Value loss: 0.029404. Entropy: 0.287809.\n",
      "episode: 7573   score: 660.0  epsilon: 1.0    steps: 192  evaluation reward: 532.65\n",
      "episode: 7574   score: 390.0  epsilon: 1.0    steps: 720  evaluation reward: 529.85\n",
      "Training network. lr: 0.000081. clip: 0.032272\n",
      "Iteration 22093: Policy loss: -0.000908. Value loss: 0.124775. Entropy: 0.279271.\n",
      "Iteration 22094: Policy loss: -0.013509. Value loss: 0.076603. Entropy: 0.278302.\n",
      "Iteration 22095: Policy loss: -0.011733. Value loss: 0.054281. Entropy: 0.279548.\n",
      "episode: 7575   score: 755.0  epsilon: 1.0    steps: 224  evaluation reward: 531.65\n",
      "Training network. lr: 0.000081. clip: 0.032272\n",
      "Iteration 22096: Policy loss: -0.098514. Value loss: 0.171638. Entropy: 0.286401.\n",
      "Iteration 22097: Policy loss: -0.099585. Value loss: 0.072057. Entropy: 0.287031.\n",
      "Iteration 22098: Policy loss: -0.114556. Value loss: 0.052978. Entropy: 0.287256.\n",
      "Training network. lr: 0.000081. clip: 0.032272\n",
      "Iteration 22099: Policy loss: 0.179229. Value loss: 0.125441. Entropy: 0.298945.\n",
      "Iteration 22100: Policy loss: 0.176893. Value loss: 0.053747. Entropy: 0.297807.\n",
      "Iteration 22101: Policy loss: 0.172581. Value loss: 0.036679. Entropy: 0.299774.\n",
      "Training network. lr: 0.000080. clip: 0.032115\n",
      "Iteration 22102: Policy loss: -0.617606. Value loss: 0.531348. Entropy: 0.295989.\n",
      "Iteration 22103: Policy loss: -0.612954. Value loss: 0.336837. Entropy: 0.295768.\n",
      "Iteration 22104: Policy loss: -0.630946. Value loss: 0.256717. Entropy: 0.296199.\n",
      "episode: 7576   score: 795.0  epsilon: 1.0    steps: 208  evaluation reward: 534.3\n",
      "Training network. lr: 0.000080. clip: 0.032115\n",
      "Iteration 22105: Policy loss: -0.335776. Value loss: 0.267385. Entropy: 0.293571.\n",
      "Iteration 22106: Policy loss: -0.336665. Value loss: 0.138406. Entropy: 0.292893.\n",
      "Iteration 22107: Policy loss: -0.341791. Value loss: 0.078088. Entropy: 0.293079.\n",
      "episode: 7577   score: 315.0  epsilon: 1.0    steps: 312  evaluation reward: 532.75\n",
      "Training network. lr: 0.000080. clip: 0.032115\n",
      "Iteration 22108: Policy loss: 0.038217. Value loss: 0.109052. Entropy: 0.290526.\n",
      "Iteration 22109: Policy loss: 0.044937. Value loss: 0.049061. Entropy: 0.289589.\n",
      "Iteration 22110: Policy loss: 0.034811. Value loss: 0.034154. Entropy: 0.288925.\n",
      "Training network. lr: 0.000080. clip: 0.032115\n",
      "Iteration 22111: Policy loss: -0.024764. Value loss: 0.199655. Entropy: 0.301359.\n",
      "Iteration 22112: Policy loss: -0.038476. Value loss: 0.104809. Entropy: 0.301410.\n",
      "Iteration 22113: Policy loss: -0.033171. Value loss: 0.072283. Entropy: 0.300949.\n",
      "episode: 7578   score: 315.0  epsilon: 1.0    steps: 896  evaluation reward: 529.6\n",
      "Training network. lr: 0.000080. clip: 0.032115\n",
      "Iteration 22114: Policy loss: 0.226300. Value loss: 0.205301. Entropy: 0.305363.\n",
      "Iteration 22115: Policy loss: 0.232930. Value loss: 0.065278. Entropy: 0.306511.\n",
      "Iteration 22116: Policy loss: 0.225684. Value loss: 0.047298. Entropy: 0.305223.\n",
      "episode: 7579   score: 470.0  epsilon: 1.0    steps: 304  evaluation reward: 531.2\n",
      "episode: 7580   score: 670.0  epsilon: 1.0    steps: 496  evaluation reward: 532.35\n",
      "Training network. lr: 0.000080. clip: 0.032115\n",
      "Iteration 22117: Policy loss: -0.191909. Value loss: 0.135770. Entropy: 0.277378.\n",
      "Iteration 22118: Policy loss: -0.196273. Value loss: 0.049764. Entropy: 0.276999.\n",
      "Iteration 22119: Policy loss: -0.209327. Value loss: 0.038346. Entropy: 0.276289.\n",
      "episode: 7581   score: 945.0  epsilon: 1.0    steps: 952  evaluation reward: 535.3\n",
      "episode: 7582   score: 695.0  epsilon: 1.0    steps: 968  evaluation reward: 536.7\n",
      "Training network. lr: 0.000080. clip: 0.032115\n",
      "Iteration 22120: Policy loss: -0.137997. Value loss: 0.297088. Entropy: 0.301568.\n",
      "Iteration 22121: Policy loss: -0.139463. Value loss: 0.205288. Entropy: 0.301952.\n",
      "Iteration 22122: Policy loss: -0.161755. Value loss: 0.188406. Entropy: 0.302664.\n",
      "episode: 7583   score: 725.0  epsilon: 1.0    steps: 376  evaluation reward: 539.0\n",
      "Training network. lr: 0.000080. clip: 0.032115\n",
      "Iteration 22123: Policy loss: -0.068098. Value loss: 0.117475. Entropy: 0.275512.\n",
      "Iteration 22124: Policy loss: -0.076424. Value loss: 0.076793. Entropy: 0.276814.\n",
      "Iteration 22125: Policy loss: -0.077013. Value loss: 0.064372. Entropy: 0.276846.\n",
      "Training network. lr: 0.000080. clip: 0.032115\n",
      "Iteration 22126: Policy loss: 0.348580. Value loss: 0.298652. Entropy: 0.308187.\n",
      "Iteration 22127: Policy loss: 0.336404. Value loss: 0.106118. Entropy: 0.307382.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22128: Policy loss: 0.343928. Value loss: 0.056424. Entropy: 0.307287.\n",
      "episode: 7584   score: 395.0  epsilon: 1.0    steps: 368  evaluation reward: 539.0\n",
      "Training network. lr: 0.000080. clip: 0.032115\n",
      "Iteration 22129: Policy loss: 0.120970. Value loss: 0.114859. Entropy: 0.282875.\n",
      "Iteration 22130: Policy loss: 0.116071. Value loss: 0.047521. Entropy: 0.284765.\n",
      "Iteration 22131: Policy loss: 0.117405. Value loss: 0.036166. Entropy: 0.284111.\n",
      "Training network. lr: 0.000080. clip: 0.032115\n",
      "Iteration 22132: Policy loss: 0.098119. Value loss: 0.080476. Entropy: 0.308333.\n",
      "Iteration 22133: Policy loss: 0.091473. Value loss: 0.040617. Entropy: 0.308861.\n",
      "Iteration 22134: Policy loss: 0.092692. Value loss: 0.030955. Entropy: 0.308169.\n",
      "Training network. lr: 0.000080. clip: 0.032115\n",
      "Iteration 22135: Policy loss: -0.060115. Value loss: 0.110754. Entropy: 0.306931.\n",
      "Iteration 22136: Policy loss: -0.065720. Value loss: 0.053830. Entropy: 0.306150.\n",
      "Iteration 22137: Policy loss: -0.066319. Value loss: 0.035687. Entropy: 0.306983.\n",
      "episode: 7585   score: 330.0  epsilon: 1.0    steps: 408  evaluation reward: 530.75\n",
      "Training network. lr: 0.000080. clip: 0.032115\n",
      "Iteration 22138: Policy loss: -0.082372. Value loss: 0.388300. Entropy: 0.296037.\n",
      "Iteration 22139: Policy loss: -0.071270. Value loss: 0.179112. Entropy: 0.294196.\n",
      "Iteration 22140: Policy loss: -0.098669. Value loss: 0.091118. Entropy: 0.293610.\n",
      "episode: 7586   score: 305.0  epsilon: 1.0    steps: 808  evaluation reward: 530.5\n",
      "Training network. lr: 0.000080. clip: 0.032115\n",
      "Iteration 22141: Policy loss: 0.224230. Value loss: 0.143851. Entropy: 0.301100.\n",
      "Iteration 22142: Policy loss: 0.215196. Value loss: 0.070142. Entropy: 0.301344.\n",
      "Iteration 22143: Policy loss: 0.215405. Value loss: 0.049441. Entropy: 0.301124.\n",
      "episode: 7587   score: 425.0  epsilon: 1.0    steps: 952  evaluation reward: 531.1\n",
      "Training network. lr: 0.000080. clip: 0.032115\n",
      "Iteration 22144: Policy loss: 0.161440. Value loss: 0.118446. Entropy: 0.303703.\n",
      "Iteration 22145: Policy loss: 0.158579. Value loss: 0.045691. Entropy: 0.303224.\n",
      "Iteration 22146: Policy loss: 0.157553. Value loss: 0.031974. Entropy: 0.303038.\n",
      "episode: 7588   score: 570.0  epsilon: 1.0    steps: 216  evaluation reward: 531.5\n",
      "Training network. lr: 0.000080. clip: 0.032115\n",
      "Iteration 22147: Policy loss: 0.044482. Value loss: 0.129257. Entropy: 0.292975.\n",
      "Iteration 22148: Policy loss: 0.039273. Value loss: 0.052675. Entropy: 0.291851.\n",
      "Iteration 22149: Policy loss: 0.031632. Value loss: 0.036573. Entropy: 0.293307.\n",
      "episode: 7589   score: 755.0  epsilon: 1.0    steps: 768  evaluation reward: 534.2\n",
      "Training network. lr: 0.000080. clip: 0.032115\n",
      "Iteration 22150: Policy loss: -0.472560. Value loss: 0.614939. Entropy: 0.295644.\n",
      "Iteration 22151: Policy loss: -0.463043. Value loss: 0.314967. Entropy: 0.295134.\n",
      "Iteration 22152: Policy loss: -0.475683. Value loss: 0.246915. Entropy: 0.294732.\n",
      "episode: 7590   score: 655.0  epsilon: 1.0    steps: 352  evaluation reward: 536.7\n",
      "Training network. lr: 0.000080. clip: 0.031958\n",
      "Iteration 22153: Policy loss: 0.187506. Value loss: 0.215280. Entropy: 0.300422.\n",
      "Iteration 22154: Policy loss: 0.185830. Value loss: 0.066403. Entropy: 0.300277.\n",
      "Iteration 22155: Policy loss: 0.179631. Value loss: 0.049518. Entropy: 0.299485.\n",
      "Training network. lr: 0.000080. clip: 0.031958\n",
      "Iteration 22156: Policy loss: -0.144311. Value loss: 0.264492. Entropy: 0.296709.\n",
      "Iteration 22157: Policy loss: -0.155460. Value loss: 0.105189. Entropy: 0.296418.\n",
      "Iteration 22158: Policy loss: -0.156164. Value loss: 0.071364. Entropy: 0.296413.\n",
      "episode: 7591   score: 775.0  epsilon: 1.0    steps: 176  evaluation reward: 539.45\n",
      "Training network. lr: 0.000080. clip: 0.031958\n",
      "Iteration 22159: Policy loss: 0.160172. Value loss: 0.160416. Entropy: 0.294611.\n",
      "Iteration 22160: Policy loss: 0.151116. Value loss: 0.080159. Entropy: 0.293876.\n",
      "Iteration 22161: Policy loss: 0.143616. Value loss: 0.051800. Entropy: 0.294253.\n",
      "Training network. lr: 0.000080. clip: 0.031958\n",
      "Iteration 22162: Policy loss: 0.267192. Value loss: 0.284015. Entropy: 0.305184.\n",
      "Iteration 22163: Policy loss: 0.268574. Value loss: 0.106529. Entropy: 0.304355.\n",
      "Iteration 22164: Policy loss: 0.262576. Value loss: 0.070975. Entropy: 0.304751.\n",
      "episode: 7592   score: 945.0  epsilon: 1.0    steps: 264  evaluation reward: 544.1\n",
      "episode: 7593   score: 390.0  epsilon: 1.0    steps: 824  evaluation reward: 544.4\n",
      "Training network. lr: 0.000080. clip: 0.031958\n",
      "Iteration 22165: Policy loss: 0.452380. Value loss: 0.115100. Entropy: 0.283369.\n",
      "Iteration 22166: Policy loss: 0.446657. Value loss: 0.041971. Entropy: 0.281355.\n",
      "Iteration 22167: Policy loss: 0.446772. Value loss: 0.033495. Entropy: 0.282039.\n",
      "episode: 7594   score: 310.0  epsilon: 1.0    steps: 432  evaluation reward: 544.05\n",
      "Training network. lr: 0.000080. clip: 0.031958\n",
      "Iteration 22168: Policy loss: 0.100663. Value loss: 0.085453. Entropy: 0.297305.\n",
      "Iteration 22169: Policy loss: 0.102752. Value loss: 0.044133. Entropy: 0.297848.\n",
      "Iteration 22170: Policy loss: 0.096170. Value loss: 0.037543. Entropy: 0.298938.\n",
      "Training network. lr: 0.000080. clip: 0.031958\n",
      "Iteration 22171: Policy loss: 0.314425. Value loss: 0.099517. Entropy: 0.307018.\n",
      "Iteration 22172: Policy loss: 0.309529. Value loss: 0.042723. Entropy: 0.306110.\n",
      "Iteration 22173: Policy loss: 0.310445. Value loss: 0.032339. Entropy: 0.306528.\n",
      "episode: 7595   score: 360.0  epsilon: 1.0    steps: 24  evaluation reward: 540.35\n",
      "episode: 7596   score: 395.0  epsilon: 1.0    steps: 856  evaluation reward: 537.8\n",
      "Training network. lr: 0.000080. clip: 0.031958\n",
      "Iteration 22174: Policy loss: 0.335146. Value loss: 0.119282. Entropy: 0.291738.\n",
      "Iteration 22175: Policy loss: 0.338405. Value loss: 0.051625. Entropy: 0.292491.\n",
      "Iteration 22176: Policy loss: 0.336125. Value loss: 0.035495. Entropy: 0.293035.\n",
      "Training network. lr: 0.000080. clip: 0.031958\n",
      "Iteration 22177: Policy loss: 0.052089. Value loss: 0.166389. Entropy: 0.311376.\n",
      "Iteration 22178: Policy loss: 0.054248. Value loss: 0.079906. Entropy: 0.310349.\n",
      "Iteration 22179: Policy loss: 0.055396. Value loss: 0.052517. Entropy: 0.311046.\n",
      "episode: 7597   score: 695.0  epsilon: 1.0    steps: 112  evaluation reward: 539.5\n",
      "episode: 7598   score: 350.0  epsilon: 1.0    steps: 864  evaluation reward: 539.25\n",
      "Training network. lr: 0.000080. clip: 0.031958\n",
      "Iteration 22180: Policy loss: 0.111604. Value loss: 0.118328. Entropy: 0.284146.\n",
      "Iteration 22181: Policy loss: 0.107157. Value loss: 0.057324. Entropy: 0.284209.\n",
      "Iteration 22182: Policy loss: 0.103723. Value loss: 0.041027. Entropy: 0.284575.\n",
      "episode: 7599   score: 415.0  epsilon: 1.0    steps: 952  evaluation reward: 538.55\n",
      "Training network. lr: 0.000080. clip: 0.031958\n",
      "Iteration 22183: Policy loss: -0.151616. Value loss: 0.090522. Entropy: 0.302795.\n",
      "Iteration 22184: Policy loss: -0.156740. Value loss: 0.035038. Entropy: 0.301843.\n",
      "Iteration 22185: Policy loss: -0.154571. Value loss: 0.025423. Entropy: 0.302824.\n",
      "episode: 7600   score: 285.0  epsilon: 1.0    steps: 360  evaluation reward: 537.0\n",
      "Training network. lr: 0.000080. clip: 0.031958\n",
      "Iteration 22186: Policy loss: -0.075147. Value loss: 0.162745. Entropy: 0.292705.\n",
      "Iteration 22187: Policy loss: -0.089520. Value loss: 0.057073. Entropy: 0.294308.\n",
      "Iteration 22188: Policy loss: -0.088231. Value loss: 0.036282. Entropy: 0.292163.\n",
      "now time :  2019-09-06 13:07:02.759183\n",
      "episode: 7601   score: 310.0  epsilon: 1.0    steps: 120  evaluation reward: 535.9\n",
      "episode: 7602   score: 395.0  epsilon: 1.0    steps: 480  evaluation reward: 533.75\n",
      "Training network. lr: 0.000080. clip: 0.031958\n",
      "Iteration 22189: Policy loss: -0.107109. Value loss: 0.097414. Entropy: 0.290257.\n",
      "Iteration 22190: Policy loss: -0.107641. Value loss: 0.053401. Entropy: 0.290388.\n",
      "Iteration 22191: Policy loss: -0.111491. Value loss: 0.040759. Entropy: 0.290927.\n",
      "Training network. lr: 0.000080. clip: 0.031958\n",
      "Iteration 22192: Policy loss: 0.381180. Value loss: 0.223145. Entropy: 0.301840.\n",
      "Iteration 22193: Policy loss: 0.403750. Value loss: 0.052172. Entropy: 0.301688.\n",
      "Iteration 22194: Policy loss: 0.396067. Value loss: 0.033570. Entropy: 0.300354.\n",
      "Training network. lr: 0.000080. clip: 0.031958\n",
      "Iteration 22195: Policy loss: -0.073934. Value loss: 0.133971. Entropy: 0.298948.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22196: Policy loss: -0.073798. Value loss: 0.054692. Entropy: 0.298252.\n",
      "Iteration 22197: Policy loss: -0.075381. Value loss: 0.037472. Entropy: 0.299681.\n",
      "Training network. lr: 0.000080. clip: 0.031958\n",
      "Iteration 22198: Policy loss: 0.090128. Value loss: 0.201850. Entropy: 0.311164.\n",
      "Iteration 22199: Policy loss: 0.095594. Value loss: 0.084630. Entropy: 0.311263.\n",
      "Iteration 22200: Policy loss: 0.094653. Value loss: 0.068946. Entropy: 0.310372.\n",
      "Training network. lr: 0.000080. clip: 0.031811\n",
      "Iteration 22201: Policy loss: 0.039884. Value loss: 0.201006. Entropy: 0.304897.\n",
      "Iteration 22202: Policy loss: 0.042256. Value loss: 0.111139. Entropy: 0.304530.\n",
      "Iteration 22203: Policy loss: 0.035508. Value loss: 0.077180. Entropy: 0.303224.\n",
      "episode: 7603   score: 470.0  epsilon: 1.0    steps: 288  evaluation reward: 533.2\n",
      "episode: 7604   score: 425.0  epsilon: 1.0    steps: 640  evaluation reward: 531.4\n",
      "Training network. lr: 0.000080. clip: 0.031811\n",
      "Iteration 22204: Policy loss: 0.198656. Value loss: 0.165620. Entropy: 0.283149.\n",
      "Iteration 22205: Policy loss: 0.199275. Value loss: 0.086676. Entropy: 0.282669.\n",
      "Iteration 22206: Policy loss: 0.191957. Value loss: 0.060304. Entropy: 0.282382.\n",
      "episode: 7605   score: 700.0  epsilon: 1.0    steps: 200  evaluation reward: 535.55\n",
      "Training network. lr: 0.000080. clip: 0.031811\n",
      "Iteration 22207: Policy loss: -0.100007. Value loss: 0.179747. Entropy: 0.294780.\n",
      "Iteration 22208: Policy loss: -0.093167. Value loss: 0.067328. Entropy: 0.294833.\n",
      "Iteration 22209: Policy loss: -0.103102. Value loss: 0.053016. Entropy: 0.294108.\n",
      "episode: 7606   score: 510.0  epsilon: 1.0    steps: 400  evaluation reward: 533.1\n",
      "episode: 7607   score: 315.0  epsilon: 1.0    steps: 616  evaluation reward: 528.05\n",
      "episode: 7608   score: 400.0  epsilon: 1.0    steps: 696  evaluation reward: 528.15\n",
      "Training network. lr: 0.000080. clip: 0.031811\n",
      "Iteration 22210: Policy loss: 0.101568. Value loss: 0.129399. Entropy: 0.268016.\n",
      "Iteration 22211: Policy loss: 0.098381. Value loss: 0.066998. Entropy: 0.267627.\n",
      "Iteration 22212: Policy loss: 0.093950. Value loss: 0.049965. Entropy: 0.268798.\n",
      "episode: 7609   score: 500.0  epsilon: 1.0    steps: 896  evaluation reward: 527.3\n",
      "episode: 7610   score: 345.0  epsilon: 1.0    steps: 944  evaluation reward: 522.0\n",
      "Training network. lr: 0.000080. clip: 0.031811\n",
      "Iteration 22213: Policy loss: 0.024884. Value loss: 0.106819. Entropy: 0.306096.\n",
      "Iteration 22214: Policy loss: 0.017191. Value loss: 0.052428. Entropy: 0.305505.\n",
      "Iteration 22215: Policy loss: 0.019035. Value loss: 0.043515. Entropy: 0.306076.\n",
      "Training network. lr: 0.000080. clip: 0.031811\n",
      "Iteration 22216: Policy loss: 0.219118. Value loss: 0.068146. Entropy: 0.281653.\n",
      "Iteration 22217: Policy loss: 0.217352. Value loss: 0.031902. Entropy: 0.280039.\n",
      "Iteration 22218: Policy loss: 0.209629. Value loss: 0.022024. Entropy: 0.281164.\n",
      "Training network. lr: 0.000080. clip: 0.031811\n",
      "Iteration 22219: Policy loss: 0.286019. Value loss: 0.176009. Entropy: 0.307395.\n",
      "Iteration 22220: Policy loss: 0.285774. Value loss: 0.059600. Entropy: 0.307619.\n",
      "Iteration 22221: Policy loss: 0.275717. Value loss: 0.034799. Entropy: 0.308407.\n",
      "episode: 7611   score: 215.0  epsilon: 1.0    steps: 936  evaluation reward: 520.15\n",
      "Training network. lr: 0.000080. clip: 0.031811\n",
      "Iteration 22222: Policy loss: -0.112662. Value loss: 0.276793. Entropy: 0.303835.\n",
      "Iteration 22223: Policy loss: -0.092652. Value loss: 0.162215. Entropy: 0.304529.\n",
      "Iteration 22224: Policy loss: -0.119138. Value loss: 0.125659. Entropy: 0.303762.\n",
      "episode: 7612   score: 240.0  epsilon: 1.0    steps: 856  evaluation reward: 517.9\n",
      "episode: 7613   score: 360.0  epsilon: 1.0    steps: 968  evaluation reward: 517.7\n",
      "Training network. lr: 0.000080. clip: 0.031811\n",
      "Iteration 22225: Policy loss: 0.304796. Value loss: 0.192869. Entropy: 0.289312.\n",
      "Iteration 22226: Policy loss: 0.294492. Value loss: 0.081160. Entropy: 0.287492.\n",
      "Iteration 22227: Policy loss: 0.299003. Value loss: 0.047680. Entropy: 0.289262.\n",
      "Training network. lr: 0.000080. clip: 0.031811\n",
      "Iteration 22228: Policy loss: -0.024534. Value loss: 0.146643. Entropy: 0.297502.\n",
      "Iteration 22229: Policy loss: -0.025663. Value loss: 0.053624. Entropy: 0.299464.\n",
      "Iteration 22230: Policy loss: -0.026868. Value loss: 0.036358. Entropy: 0.298268.\n",
      "episode: 7614   score: 210.0  epsilon: 1.0    steps: 96  evaluation reward: 516.2\n",
      "Training network. lr: 0.000080. clip: 0.031811\n",
      "Iteration 22231: Policy loss: -0.040781. Value loss: 0.080888. Entropy: 0.295504.\n",
      "Iteration 22232: Policy loss: -0.044731. Value loss: 0.040837. Entropy: 0.296215.\n",
      "Iteration 22233: Policy loss: -0.050158. Value loss: 0.033214. Entropy: 0.295614.\n",
      "episode: 7615   score: 380.0  epsilon: 1.0    steps: 560  evaluation reward: 515.6\n",
      "episode: 7616   score: 645.0  epsilon: 1.0    steps: 752  evaluation reward: 517.75\n",
      "Training network. lr: 0.000080. clip: 0.031811\n",
      "Iteration 22234: Policy loss: -0.256938. Value loss: 0.250099. Entropy: 0.283451.\n",
      "Iteration 22235: Policy loss: -0.260172. Value loss: 0.146945. Entropy: 0.281669.\n",
      "Iteration 22236: Policy loss: -0.243208. Value loss: 0.098832. Entropy: 0.282741.\n",
      "Training network. lr: 0.000080. clip: 0.031811\n",
      "Iteration 22237: Policy loss: 0.209699. Value loss: 0.125393. Entropy: 0.316693.\n",
      "Iteration 22238: Policy loss: 0.207997. Value loss: 0.059519. Entropy: 0.315895.\n",
      "Iteration 22239: Policy loss: 0.210464. Value loss: 0.036360. Entropy: 0.315031.\n",
      "Training network. lr: 0.000080. clip: 0.031811\n",
      "Iteration 22240: Policy loss: -0.008031. Value loss: 0.118288. Entropy: 0.304159.\n",
      "Iteration 22241: Policy loss: -0.009777. Value loss: 0.055191. Entropy: 0.304752.\n",
      "Iteration 22242: Policy loss: -0.019589. Value loss: 0.037271. Entropy: 0.305846.\n",
      "Training network. lr: 0.000080. clip: 0.031811\n",
      "Iteration 22243: Policy loss: -0.487578. Value loss: 0.412838. Entropy: 0.308847.\n",
      "Iteration 22244: Policy loss: -0.456546. Value loss: 0.185821. Entropy: 0.307934.\n",
      "Iteration 22245: Policy loss: -0.497170. Value loss: 0.101756. Entropy: 0.308247.\n",
      "episode: 7617   score: 315.0  epsilon: 1.0    steps: 392  evaluation reward: 516.65\n",
      "Training network. lr: 0.000080. clip: 0.031811\n",
      "Iteration 22246: Policy loss: -0.004266. Value loss: 0.081581. Entropy: 0.297542.\n",
      "Iteration 22247: Policy loss: -0.017954. Value loss: 0.047480. Entropy: 0.298162.\n",
      "Iteration 22248: Policy loss: -0.009475. Value loss: 0.035218. Entropy: 0.297793.\n",
      "episode: 7618   score: 650.0  epsilon: 1.0    steps: 48  evaluation reward: 516.75\n",
      "episode: 7619   score: 670.0  epsilon: 1.0    steps: 488  evaluation reward: 517.65\n",
      "Training network. lr: 0.000080. clip: 0.031811\n",
      "Iteration 22249: Policy loss: -0.227482. Value loss: 0.110089. Entropy: 0.294478.\n",
      "Iteration 22250: Policy loss: -0.227902. Value loss: 0.051875. Entropy: 0.296092.\n",
      "Iteration 22251: Policy loss: -0.228555. Value loss: 0.036663. Entropy: 0.294887.\n",
      "episode: 7620   score: 755.0  epsilon: 1.0    steps: 136  evaluation reward: 515.25\n",
      "Training network. lr: 0.000079. clip: 0.031654\n",
      "Iteration 22252: Policy loss: 0.171993. Value loss: 0.099937. Entropy: 0.296143.\n",
      "Iteration 22253: Policy loss: 0.174161. Value loss: 0.037097. Entropy: 0.297094.\n",
      "Iteration 22254: Policy loss: 0.167532. Value loss: 0.026068. Entropy: 0.296256.\n",
      "episode: 7621   score: 315.0  epsilon: 1.0    steps: 216  evaluation reward: 514.15\n",
      "Training network. lr: 0.000079. clip: 0.031654\n",
      "Iteration 22255: Policy loss: -0.126323. Value loss: 0.188351. Entropy: 0.298624.\n",
      "Iteration 22256: Policy loss: -0.157174. Value loss: 0.109534. Entropy: 0.297459.\n",
      "Iteration 22257: Policy loss: -0.147849. Value loss: 0.069965. Entropy: 0.298798.\n",
      "episode: 7622   score: 395.0  epsilon: 1.0    steps: 816  evaluation reward: 510.95\n",
      "episode: 7623   score: 470.0  epsilon: 1.0    steps: 912  evaluation reward: 507.35\n",
      "Training network. lr: 0.000079. clip: 0.031654\n",
      "Iteration 22258: Policy loss: 0.255205. Value loss: 0.167294. Entropy: 0.294296.\n",
      "Iteration 22259: Policy loss: 0.253484. Value loss: 0.061455. Entropy: 0.293839.\n",
      "Iteration 22260: Policy loss: 0.256110. Value loss: 0.039297. Entropy: 0.293932.\n",
      "episode: 7624   score: 345.0  epsilon: 1.0    steps: 208  evaluation reward: 505.85\n",
      "Training network. lr: 0.000079. clip: 0.031654\n",
      "Iteration 22261: Policy loss: -0.275212. Value loss: 0.251050. Entropy: 0.296173.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22262: Policy loss: -0.285083. Value loss: 0.185865. Entropy: 0.294000.\n",
      "Iteration 22263: Policy loss: -0.283664. Value loss: 0.159291. Entropy: 0.295757.\n",
      "Training network. lr: 0.000079. clip: 0.031654\n",
      "Iteration 22264: Policy loss: 0.107994. Value loss: 0.138625. Entropy: 0.299679.\n",
      "Iteration 22265: Policy loss: 0.110035. Value loss: 0.072218. Entropy: 0.299518.\n",
      "Iteration 22266: Policy loss: 0.100987. Value loss: 0.048625. Entropy: 0.298567.\n",
      "Training network. lr: 0.000079. clip: 0.031654\n",
      "Iteration 22267: Policy loss: 0.033049. Value loss: 0.084462. Entropy: 0.310041.\n",
      "Iteration 22268: Policy loss: 0.026639. Value loss: 0.036771. Entropy: 0.308373.\n",
      "Iteration 22269: Policy loss: 0.035601. Value loss: 0.026966. Entropy: 0.308483.\n",
      "episode: 7625   score: 385.0  epsilon: 1.0    steps: 936  evaluation reward: 502.95\n",
      "Training network. lr: 0.000079. clip: 0.031654\n",
      "Iteration 22270: Policy loss: 0.048207. Value loss: 0.099392. Entropy: 0.308247.\n",
      "Iteration 22271: Policy loss: 0.049247. Value loss: 0.046722. Entropy: 0.308625.\n",
      "Iteration 22272: Policy loss: 0.038329. Value loss: 0.033002. Entropy: 0.308517.\n",
      "Training network. lr: 0.000079. clip: 0.031654\n",
      "Iteration 22273: Policy loss: -0.421560. Value loss: 0.296455. Entropy: 0.301919.\n",
      "Iteration 22274: Policy loss: -0.410842. Value loss: 0.154596. Entropy: 0.300294.\n",
      "Iteration 22275: Policy loss: -0.405324. Value loss: 0.070255. Entropy: 0.299883.\n",
      "Training network. lr: 0.000079. clip: 0.031654\n",
      "Iteration 22276: Policy loss: -0.079055. Value loss: 0.278242. Entropy: 0.304676.\n",
      "Iteration 22277: Policy loss: -0.089123. Value loss: 0.160105. Entropy: 0.302561.\n",
      "Iteration 22278: Policy loss: -0.103383. Value loss: 0.137759. Entropy: 0.303921.\n",
      "episode: 7626   score: 520.0  epsilon: 1.0    steps: 744  evaluation reward: 502.9\n",
      "episode: 7627   score: 440.0  epsilon: 1.0    steps: 872  evaluation reward: 504.0\n",
      "Training network. lr: 0.000079. clip: 0.031654\n",
      "Iteration 22279: Policy loss: 0.127714. Value loss: 0.208908. Entropy: 0.298215.\n",
      "Iteration 22280: Policy loss: 0.118325. Value loss: 0.071811. Entropy: 0.296804.\n",
      "Iteration 22281: Policy loss: 0.111368. Value loss: 0.048970. Entropy: 0.295945.\n",
      "episode: 7628   score: 895.0  epsilon: 1.0    steps: 8  evaluation reward: 508.25\n",
      "episode: 7629   score: 300.0  epsilon: 1.0    steps: 288  evaluation reward: 507.9\n",
      "Training network. lr: 0.000079. clip: 0.031654\n",
      "Iteration 22282: Policy loss: 0.155308. Value loss: 0.117634. Entropy: 0.280542.\n",
      "Iteration 22283: Policy loss: 0.147704. Value loss: 0.049084. Entropy: 0.279848.\n",
      "Iteration 22284: Policy loss: 0.148288. Value loss: 0.036979. Entropy: 0.279079.\n",
      "Training network. lr: 0.000079. clip: 0.031654\n",
      "Iteration 22285: Policy loss: 0.273441. Value loss: 0.163513. Entropy: 0.307837.\n",
      "Iteration 22286: Policy loss: 0.270064. Value loss: 0.072884. Entropy: 0.307699.\n",
      "Iteration 22287: Policy loss: 0.265309. Value loss: 0.053709. Entropy: 0.309101.\n",
      "episode: 7630   score: 475.0  epsilon: 1.0    steps: 400  evaluation reward: 506.95\n",
      "Training network. lr: 0.000079. clip: 0.031654\n",
      "Iteration 22288: Policy loss: 0.142418. Value loss: 0.164937. Entropy: 0.295427.\n",
      "Iteration 22289: Policy loss: 0.121842. Value loss: 0.057237. Entropy: 0.294509.\n",
      "Iteration 22290: Policy loss: 0.135320. Value loss: 0.040402. Entropy: 0.294154.\n",
      "episode: 7631   score: 460.0  epsilon: 1.0    steps: 904  evaluation reward: 505.35\n",
      "Training network. lr: 0.000079. clip: 0.031654\n",
      "Iteration 22291: Policy loss: -0.087505. Value loss: 0.104029. Entropy: 0.307811.\n",
      "Iteration 22292: Policy loss: -0.091723. Value loss: 0.045566. Entropy: 0.308284.\n",
      "Iteration 22293: Policy loss: -0.095605. Value loss: 0.033792. Entropy: 0.308583.\n",
      "episode: 7632   score: 955.0  epsilon: 1.0    steps: 312  evaluation reward: 511.6\n",
      "episode: 7633   score: 400.0  epsilon: 1.0    steps: 848  evaluation reward: 509.2\n",
      "Training network. lr: 0.000079. clip: 0.031654\n",
      "Iteration 22294: Policy loss: 0.086030. Value loss: 0.105879. Entropy: 0.288504.\n",
      "Iteration 22295: Policy loss: 0.076572. Value loss: 0.040935. Entropy: 0.289320.\n",
      "Iteration 22296: Policy loss: 0.082150. Value loss: 0.033108. Entropy: 0.288569.\n",
      "Training network. lr: 0.000079. clip: 0.031654\n",
      "Iteration 22297: Policy loss: -0.029120. Value loss: 0.253299. Entropy: 0.303084.\n",
      "Iteration 22298: Policy loss: -0.031035. Value loss: 0.080376. Entropy: 0.303086.\n",
      "Iteration 22299: Policy loss: -0.034852. Value loss: 0.056767. Entropy: 0.302584.\n",
      "episode: 7634   score: 345.0  epsilon: 1.0    steps: 792  evaluation reward: 508.45\n",
      "Training network. lr: 0.000079. clip: 0.031654\n",
      "Iteration 22300: Policy loss: 0.026379. Value loss: 0.118879. Entropy: 0.293125.\n",
      "Iteration 22301: Policy loss: 0.022701. Value loss: 0.061478. Entropy: 0.292415.\n",
      "Iteration 22302: Policy loss: 0.028575. Value loss: 0.042662. Entropy: 0.291872.\n",
      "episode: 7635   score: 380.0  epsilon: 1.0    steps: 552  evaluation reward: 504.85\n",
      "episode: 7636   score: 335.0  epsilon: 1.0    steps: 704  evaluation reward: 502.65\n",
      "Training network. lr: 0.000079. clip: 0.031497\n",
      "Iteration 22303: Policy loss: -0.040640. Value loss: 0.350963. Entropy: 0.292062.\n",
      "Iteration 22304: Policy loss: -0.025222. Value loss: 0.167243. Entropy: 0.295498.\n",
      "Iteration 22305: Policy loss: -0.050616. Value loss: 0.112272. Entropy: 0.292880.\n",
      "Training network. lr: 0.000079. clip: 0.031497\n",
      "Iteration 22306: Policy loss: 0.008484. Value loss: 0.114765. Entropy: 0.311509.\n",
      "Iteration 22307: Policy loss: 0.002567. Value loss: 0.057645. Entropy: 0.312496.\n",
      "Iteration 22308: Policy loss: -0.000464. Value loss: 0.041743. Entropy: 0.311920.\n",
      "Training network. lr: 0.000079. clip: 0.031497\n",
      "Iteration 22309: Policy loss: 0.096540. Value loss: 0.142283. Entropy: 0.302003.\n",
      "Iteration 22310: Policy loss: 0.083107. Value loss: 0.073639. Entropy: 0.302088.\n",
      "Iteration 22311: Policy loss: 0.088165. Value loss: 0.055013. Entropy: 0.301034.\n",
      "episode: 7637   score: 395.0  epsilon: 1.0    steps: 240  evaluation reward: 502.4\n",
      "Training network. lr: 0.000079. clip: 0.031497\n",
      "Iteration 22312: Policy loss: 0.064828. Value loss: 0.160321. Entropy: 0.295133.\n",
      "Iteration 22313: Policy loss: 0.079419. Value loss: 0.062931. Entropy: 0.296364.\n",
      "Iteration 22314: Policy loss: 0.067903. Value loss: 0.045722. Entropy: 0.296028.\n",
      "Training network. lr: 0.000079. clip: 0.031497\n",
      "Iteration 22315: Policy loss: 0.036343. Value loss: 0.235640. Entropy: 0.306751.\n",
      "Iteration 22316: Policy loss: 0.023097. Value loss: 0.120488. Entropy: 0.307842.\n",
      "Iteration 22317: Policy loss: 0.021242. Value loss: 0.084708. Entropy: 0.307573.\n",
      "Training network. lr: 0.000079. clip: 0.031497\n",
      "Iteration 22318: Policy loss: 0.008048. Value loss: 0.126889. Entropy: 0.311101.\n",
      "Iteration 22319: Policy loss: 0.007866. Value loss: 0.054709. Entropy: 0.310183.\n",
      "Iteration 22320: Policy loss: 0.011372. Value loss: 0.034157. Entropy: 0.310847.\n",
      "episode: 7638   score: 315.0  epsilon: 1.0    steps: 768  evaluation reward: 500.6\n",
      "episode: 7639   score: 525.0  epsilon: 1.0    steps: 864  evaluation reward: 501.85\n",
      "Training network. lr: 0.000079. clip: 0.031497\n",
      "Iteration 22321: Policy loss: 0.149174. Value loss: 0.174501. Entropy: 0.293594.\n",
      "Iteration 22322: Policy loss: 0.157975. Value loss: 0.072615. Entropy: 0.293671.\n",
      "Iteration 22323: Policy loss: 0.145327. Value loss: 0.049671. Entropy: 0.293799.\n",
      "episode: 7640   score: 335.0  epsilon: 1.0    steps: 752  evaluation reward: 497.95\n",
      "Training network. lr: 0.000079. clip: 0.031497\n",
      "Iteration 22324: Policy loss: -0.386496. Value loss: 0.386615. Entropy: 0.297448.\n",
      "Iteration 22325: Policy loss: -0.421909. Value loss: 0.283011. Entropy: 0.297997.\n",
      "Iteration 22326: Policy loss: -0.393950. Value loss: 0.217616. Entropy: 0.298070.\n",
      "episode: 7641   score: 670.0  epsilon: 1.0    steps: 120  evaluation reward: 498.25\n",
      "episode: 7642   score: 350.0  epsilon: 1.0    steps: 192  evaluation reward: 493.55\n",
      "Training network. lr: 0.000079. clip: 0.031497\n",
      "Iteration 22327: Policy loss: 0.031016. Value loss: 0.088348. Entropy: 0.289767.\n",
      "Iteration 22328: Policy loss: 0.025311. Value loss: 0.035989. Entropy: 0.289097.\n",
      "Iteration 22329: Policy loss: 0.023020. Value loss: 0.027153. Entropy: 0.287600.\n",
      "episode: 7643   score: 1060.0  epsilon: 1.0    steps: 472  evaluation reward: 499.8\n",
      "Training network. lr: 0.000079. clip: 0.031497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22330: Policy loss: -0.104370. Value loss: 0.130549. Entropy: 0.297067.\n",
      "Iteration 22331: Policy loss: -0.110784. Value loss: 0.049342. Entropy: 0.296466.\n",
      "Iteration 22332: Policy loss: -0.117409. Value loss: 0.033732. Entropy: 0.297003.\n",
      "Training network. lr: 0.000079. clip: 0.031497\n",
      "Iteration 22333: Policy loss: -0.181539. Value loss: 0.316406. Entropy: 0.314602.\n",
      "Iteration 22334: Policy loss: -0.180343. Value loss: 0.170365. Entropy: 0.313804.\n",
      "Iteration 22335: Policy loss: -0.176515. Value loss: 0.064782. Entropy: 0.313012.\n",
      "episode: 7644   score: 485.0  epsilon: 1.0    steps: 384  evaluation reward: 498.4\n",
      "Training network. lr: 0.000079. clip: 0.031497\n",
      "Iteration 22336: Policy loss: -0.002084. Value loss: 0.336619. Entropy: 0.295829.\n",
      "Iteration 22337: Policy loss: 0.007708. Value loss: 0.168209. Entropy: 0.295847.\n",
      "Iteration 22338: Policy loss: -0.011265. Value loss: 0.116301. Entropy: 0.295482.\n",
      "Training network. lr: 0.000079. clip: 0.031497\n",
      "Iteration 22339: Policy loss: 0.034087. Value loss: 0.162099. Entropy: 0.311494.\n",
      "Iteration 22340: Policy loss: 0.034591. Value loss: 0.062716. Entropy: 0.310096.\n",
      "Iteration 22341: Policy loss: 0.029936. Value loss: 0.040862. Entropy: 0.311122.\n",
      "Training network. lr: 0.000079. clip: 0.031497\n",
      "Iteration 22342: Policy loss: 0.085992. Value loss: 0.170050. Entropy: 0.292403.\n",
      "Iteration 22343: Policy loss: 0.079190. Value loss: 0.086338. Entropy: 0.290537.\n",
      "Iteration 22344: Policy loss: 0.078581. Value loss: 0.060888. Entropy: 0.291887.\n",
      "Training network. lr: 0.000079. clip: 0.031497\n",
      "Iteration 22345: Policy loss: -0.144610. Value loss: 0.285783. Entropy: 0.310847.\n",
      "Iteration 22346: Policy loss: -0.141655. Value loss: 0.111479. Entropy: 0.311458.\n",
      "Iteration 22347: Policy loss: -0.154352. Value loss: 0.071487. Entropy: 0.310927.\n",
      "episode: 7645   score: 345.0  epsilon: 1.0    steps: 208  evaluation reward: 495.95\n",
      "Training network. lr: 0.000079. clip: 0.031497\n",
      "Iteration 22348: Policy loss: -0.003505. Value loss: 0.078396. Entropy: 0.302096.\n",
      "Iteration 22349: Policy loss: -0.003606. Value loss: 0.050524. Entropy: 0.302942.\n",
      "Iteration 22350: Policy loss: -0.001952. Value loss: 0.039256. Entropy: 0.302439.\n",
      "Training network. lr: 0.000078. clip: 0.031350\n",
      "Iteration 22351: Policy loss: -0.118663. Value loss: 0.317440. Entropy: 0.311118.\n",
      "Iteration 22352: Policy loss: -0.116529. Value loss: 0.123394. Entropy: 0.311244.\n",
      "Iteration 22353: Policy loss: -0.139639. Value loss: 0.074401. Entropy: 0.309811.\n",
      "episode: 7646   score: 475.0  epsilon: 1.0    steps: 944  evaluation reward: 495.15\n",
      "Training network. lr: 0.000078. clip: 0.031350\n",
      "Iteration 22354: Policy loss: 0.014013. Value loss: 0.210649. Entropy: 0.310903.\n",
      "Iteration 22355: Policy loss: 0.015029. Value loss: 0.090274. Entropy: 0.311567.\n",
      "Iteration 22356: Policy loss: 0.020144. Value loss: 0.057582. Entropy: 0.311393.\n",
      "episode: 7647   score: 1010.0  epsilon: 1.0    steps: 40  evaluation reward: 501.6\n",
      "episode: 7648   score: 555.0  epsilon: 1.0    steps: 872  evaluation reward: 499.75\n",
      "Training network. lr: 0.000078. clip: 0.031350\n",
      "Iteration 22357: Policy loss: 0.117108. Value loss: 0.218138. Entropy: 0.289649.\n",
      "Iteration 22358: Policy loss: 0.083183. Value loss: 0.178708. Entropy: 0.290116.\n",
      "Iteration 22359: Policy loss: 0.071515. Value loss: 0.163467. Entropy: 0.289493.\n",
      "Training network. lr: 0.000078. clip: 0.031350\n",
      "Iteration 22360: Policy loss: -0.128141. Value loss: 0.109937. Entropy: 0.301316.\n",
      "Iteration 22361: Policy loss: -0.128429. Value loss: 0.057871. Entropy: 0.301472.\n",
      "Iteration 22362: Policy loss: -0.137068. Value loss: 0.042209. Entropy: 0.301270.\n",
      "episode: 7649   score: 725.0  epsilon: 1.0    steps: 152  evaluation reward: 504.6\n",
      "episode: 7650   score: 500.0  epsilon: 1.0    steps: 544  evaluation reward: 505.35\n",
      "now time :  2019-09-06 13:17:51.217237\n",
      "episode: 7651   score: 800.0  epsilon: 1.0    steps: 904  evaluation reward: 505.6\n",
      "Training network. lr: 0.000078. clip: 0.031350\n",
      "Iteration 22363: Policy loss: -0.045175. Value loss: 0.277073. Entropy: 0.282603.\n",
      "Iteration 22364: Policy loss: -0.034735. Value loss: 0.194769. Entropy: 0.282272.\n",
      "Iteration 22365: Policy loss: -0.056518. Value loss: 0.173797. Entropy: 0.281144.\n",
      "Training network. lr: 0.000078. clip: 0.031350\n",
      "Iteration 22366: Policy loss: -0.067257. Value loss: 0.082110. Entropy: 0.308996.\n",
      "Iteration 22367: Policy loss: -0.080503. Value loss: 0.038488. Entropy: 0.308817.\n",
      "Iteration 22368: Policy loss: -0.078679. Value loss: 0.028897. Entropy: 0.309344.\n",
      "Training network. lr: 0.000078. clip: 0.031350\n",
      "Iteration 22369: Policy loss: 0.072202. Value loss: 0.090197. Entropy: 0.304879.\n",
      "Iteration 22370: Policy loss: 0.071808. Value loss: 0.049208. Entropy: 0.304901.\n",
      "Iteration 22371: Policy loss: 0.068887. Value loss: 0.038197. Entropy: 0.306435.\n",
      "episode: 7652   score: 800.0  epsilon: 1.0    steps: 544  evaluation reward: 507.55\n",
      "Training network. lr: 0.000078. clip: 0.031350\n",
      "Iteration 22372: Policy loss: 0.137164. Value loss: 0.084365. Entropy: 0.299466.\n",
      "Iteration 22373: Policy loss: 0.131778. Value loss: 0.034443. Entropy: 0.299712.\n",
      "Iteration 22374: Policy loss: 0.134012. Value loss: 0.024175. Entropy: 0.299054.\n",
      "Training network. lr: 0.000078. clip: 0.031350\n",
      "Iteration 22375: Policy loss: 0.174530. Value loss: 0.126573. Entropy: 0.309237.\n",
      "Iteration 22376: Policy loss: 0.159837. Value loss: 0.053674. Entropy: 0.309050.\n",
      "Iteration 22377: Policy loss: 0.171192. Value loss: 0.040712. Entropy: 0.308796.\n",
      "episode: 7653   score: 280.0  epsilon: 1.0    steps: 464  evaluation reward: 505.4\n",
      "episode: 7654   score: 420.0  epsilon: 1.0    steps: 784  evaluation reward: 504.1\n",
      "Training network. lr: 0.000078. clip: 0.031350\n",
      "Iteration 22378: Policy loss: 0.136667. Value loss: 0.131220. Entropy: 0.292931.\n",
      "Iteration 22379: Policy loss: 0.135801. Value loss: 0.056587. Entropy: 0.293041.\n",
      "Iteration 22380: Policy loss: 0.133599. Value loss: 0.036337. Entropy: 0.293507.\n",
      "episode: 7655   score: 740.0  epsilon: 1.0    steps: 456  evaluation reward: 509.4\n",
      "Training network. lr: 0.000078. clip: 0.031350\n",
      "Iteration 22381: Policy loss: 0.130508. Value loss: 0.134348. Entropy: 0.305040.\n",
      "Iteration 22382: Policy loss: 0.121331. Value loss: 0.063553. Entropy: 0.304340.\n",
      "Iteration 22383: Policy loss: 0.122620. Value loss: 0.049397. Entropy: 0.305222.\n",
      "episode: 7656   score: 365.0  epsilon: 1.0    steps: 960  evaluation reward: 507.0\n",
      "Training network. lr: 0.000078. clip: 0.031350\n",
      "Iteration 22384: Policy loss: 0.130031. Value loss: 0.164982. Entropy: 0.303525.\n",
      "Iteration 22385: Policy loss: 0.122315. Value loss: 0.071766. Entropy: 0.302642.\n",
      "Iteration 22386: Policy loss: 0.125908. Value loss: 0.045229. Entropy: 0.302827.\n",
      "episode: 7657   score: 395.0  epsilon: 1.0    steps: 728  evaluation reward: 505.6\n",
      "Training network. lr: 0.000078. clip: 0.031350\n",
      "Iteration 22387: Policy loss: -0.165352. Value loss: 0.302818. Entropy: 0.294396.\n",
      "Iteration 22388: Policy loss: -0.169543. Value loss: 0.193085. Entropy: 0.295948.\n",
      "Iteration 22389: Policy loss: -0.160081. Value loss: 0.146696. Entropy: 0.293421.\n",
      "episode: 7658   score: 470.0  epsilon: 1.0    steps: 8  evaluation reward: 506.6\n",
      "Training network. lr: 0.000078. clip: 0.031350\n",
      "Iteration 22390: Policy loss: 0.345818. Value loss: 0.178397. Entropy: 0.296460.\n",
      "Iteration 22391: Policy loss: 0.340457. Value loss: 0.084546. Entropy: 0.295943.\n",
      "Iteration 22392: Policy loss: 0.339570. Value loss: 0.060342. Entropy: 0.295948.\n",
      "episode: 7659   score: 290.0  epsilon: 1.0    steps: 224  evaluation reward: 503.6\n",
      "Training network. lr: 0.000078. clip: 0.031350\n",
      "Iteration 22393: Policy loss: 0.344085. Value loss: 0.136533. Entropy: 0.295716.\n",
      "Iteration 22394: Policy loss: 0.349026. Value loss: 0.046940. Entropy: 0.293810.\n",
      "Iteration 22395: Policy loss: 0.349734. Value loss: 0.033966. Entropy: 0.294710.\n",
      "episode: 7660   score: 555.0  epsilon: 1.0    steps: 184  evaluation reward: 504.3\n",
      "episode: 7661   score: 220.0  epsilon: 1.0    steps: 640  evaluation reward: 503.05\n",
      "Training network. lr: 0.000078. clip: 0.031350\n",
      "Iteration 22396: Policy loss: 0.263231. Value loss: 0.185177. Entropy: 0.285980.\n",
      "Iteration 22397: Policy loss: 0.253233. Value loss: 0.079102. Entropy: 0.283156.\n",
      "Iteration 22398: Policy loss: 0.255754. Value loss: 0.050996. Entropy: 0.282595.\n",
      "Training network. lr: 0.000078. clip: 0.031350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22399: Policy loss: 0.123215. Value loss: 0.068096. Entropy: 0.315035.\n",
      "Iteration 22400: Policy loss: 0.119096. Value loss: 0.039402. Entropy: 0.315262.\n",
      "Iteration 22401: Policy loss: 0.118052. Value loss: 0.026869. Entropy: 0.314516.\n",
      "Training network. lr: 0.000078. clip: 0.031193\n",
      "Iteration 22402: Policy loss: 0.035153. Value loss: 0.060428. Entropy: 0.311156.\n",
      "Iteration 22403: Policy loss: 0.031293. Value loss: 0.029572. Entropy: 0.311925.\n",
      "Iteration 22404: Policy loss: 0.032765. Value loss: 0.021480. Entropy: 0.310705.\n",
      "episode: 7662   score: 265.0  epsilon: 1.0    steps: 768  evaluation reward: 501.25\n",
      "Training network. lr: 0.000078. clip: 0.031193\n",
      "Iteration 22405: Policy loss: -0.396152. Value loss: 0.368434. Entropy: 0.303018.\n",
      "Iteration 22406: Policy loss: -0.403164. Value loss: 0.234306. Entropy: 0.303735.\n",
      "Iteration 22407: Policy loss: -0.395975. Value loss: 0.180662. Entropy: 0.303864.\n",
      "episode: 7663   score: 620.0  epsilon: 1.0    steps: 416  evaluation reward: 501.4\n",
      "episode: 7664   score: 290.0  epsilon: 1.0    steps: 504  evaluation reward: 495.3\n",
      "Training network. lr: 0.000078. clip: 0.031193\n",
      "Iteration 22408: Policy loss: 0.130881. Value loss: 0.078104. Entropy: 0.278788.\n",
      "Iteration 22409: Policy loss: 0.130680. Value loss: 0.032737. Entropy: 0.276393.\n",
      "Iteration 22410: Policy loss: 0.130155. Value loss: 0.023468. Entropy: 0.276624.\n",
      "Training network. lr: 0.000078. clip: 0.031193\n",
      "Iteration 22411: Policy loss: 0.083609. Value loss: 0.103801. Entropy: 0.312399.\n",
      "Iteration 22412: Policy loss: 0.077063. Value loss: 0.049751. Entropy: 0.312114.\n",
      "Iteration 22413: Policy loss: 0.083138. Value loss: 0.036014. Entropy: 0.313522.\n",
      "episode: 7665   score: 530.0  epsilon: 1.0    steps: 656  evaluation reward: 496.4\n",
      "episode: 7666   score: 340.0  epsilon: 1.0    steps: 760  evaluation reward: 492.5\n",
      "Training network. lr: 0.000078. clip: 0.031193\n",
      "Iteration 22414: Policy loss: -0.070332. Value loss: 0.140261. Entropy: 0.284753.\n",
      "Iteration 22415: Policy loss: -0.074308. Value loss: 0.083165. Entropy: 0.283802.\n",
      "Iteration 22416: Policy loss: -0.090226. Value loss: 0.066108. Entropy: 0.284028.\n",
      "episode: 7667   score: 625.0  epsilon: 1.0    steps: 728  evaluation reward: 492.85\n",
      "Training network. lr: 0.000078. clip: 0.031193\n",
      "Iteration 22417: Policy loss: 0.149471. Value loss: 0.257552. Entropy: 0.299932.\n",
      "Iteration 22418: Policy loss: 0.142062. Value loss: 0.110216. Entropy: 0.299697.\n",
      "Iteration 22419: Policy loss: 0.129815. Value loss: 0.066867. Entropy: 0.299545.\n",
      "episode: 7668   score: 345.0  epsilon: 1.0    steps: 600  evaluation reward: 490.1\n",
      "episode: 7669   score: 670.0  epsilon: 1.0    steps: 968  evaluation reward: 492.55\n",
      "Training network. lr: 0.000078. clip: 0.031193\n",
      "Iteration 22420: Policy loss: 0.531449. Value loss: 0.181460. Entropy: 0.293280.\n",
      "Iteration 22421: Policy loss: 0.531781. Value loss: 0.073139. Entropy: 0.292693.\n",
      "Iteration 22422: Policy loss: 0.516823. Value loss: 0.054139. Entropy: 0.293955.\n",
      "episode: 7670   score: 215.0  epsilon: 1.0    steps: 616  evaluation reward: 492.3\n",
      "Training network. lr: 0.000078. clip: 0.031193\n",
      "Iteration 22423: Policy loss: 0.121799. Value loss: 0.136431. Entropy: 0.289553.\n",
      "Iteration 22424: Policy loss: 0.111277. Value loss: 0.053379. Entropy: 0.290530.\n",
      "Iteration 22425: Policy loss: 0.118469. Value loss: 0.044277. Entropy: 0.290900.\n",
      "Training network. lr: 0.000078. clip: 0.031193\n",
      "Iteration 22426: Policy loss: -0.448089. Value loss: 0.367437. Entropy: 0.308532.\n",
      "Iteration 22427: Policy loss: -0.445705. Value loss: 0.221991. Entropy: 0.309966.\n",
      "Iteration 22428: Policy loss: -0.445168. Value loss: 0.177299. Entropy: 0.308731.\n",
      "episode: 7671   score: 290.0  epsilon: 1.0    steps: 16  evaluation reward: 490.05\n",
      "episode: 7672   score: 570.0  epsilon: 1.0    steps: 928  evaluation reward: 491.55\n",
      "Training network. lr: 0.000078. clip: 0.031193\n",
      "Iteration 22429: Policy loss: -0.188768. Value loss: 0.345271. Entropy: 0.295160.\n",
      "Iteration 22430: Policy loss: -0.185662. Value loss: 0.251039. Entropy: 0.292703.\n",
      "Iteration 22431: Policy loss: -0.196065. Value loss: 0.128288. Entropy: 0.294210.\n",
      "Training network. lr: 0.000078. clip: 0.031193\n",
      "Iteration 22432: Policy loss: 0.052719. Value loss: 0.124374. Entropy: 0.300937.\n",
      "Iteration 22433: Policy loss: 0.049142. Value loss: 0.045542. Entropy: 0.301342.\n",
      "Iteration 22434: Policy loss: 0.046011. Value loss: 0.033136. Entropy: 0.299600.\n",
      "Training network. lr: 0.000078. clip: 0.031193\n",
      "Iteration 22435: Policy loss: -0.155300. Value loss: 0.144608. Entropy: 0.308393.\n",
      "Iteration 22436: Policy loss: -0.166227. Value loss: 0.065297. Entropy: 0.307642.\n",
      "Iteration 22437: Policy loss: -0.163583. Value loss: 0.042241. Entropy: 0.308126.\n",
      "episode: 7673   score: 405.0  epsilon: 1.0    steps: 400  evaluation reward: 489.0\n",
      "episode: 7674   score: 385.0  epsilon: 1.0    steps: 824  evaluation reward: 488.95\n",
      "episode: 7675   score: 330.0  epsilon: 1.0    steps: 904  evaluation reward: 484.7\n",
      "Training network. lr: 0.000078. clip: 0.031193\n",
      "Iteration 22438: Policy loss: 0.241404. Value loss: 0.097135. Entropy: 0.280903.\n",
      "Iteration 22439: Policy loss: 0.244968. Value loss: 0.047971. Entropy: 0.280623.\n",
      "Iteration 22440: Policy loss: 0.249555. Value loss: 0.037857. Entropy: 0.281296.\n",
      "episode: 7676   score: 215.0  epsilon: 1.0    steps: 528  evaluation reward: 478.9\n",
      "Training network. lr: 0.000078. clip: 0.031193\n",
      "Iteration 22441: Policy loss: -0.015134. Value loss: 0.104824. Entropy: 0.289209.\n",
      "Iteration 22442: Policy loss: -0.017864. Value loss: 0.050657. Entropy: 0.289286.\n",
      "Iteration 22443: Policy loss: -0.015850. Value loss: 0.034404. Entropy: 0.287689.\n",
      "episode: 7677   score: 595.0  epsilon: 1.0    steps: 512  evaluation reward: 481.7\n",
      "Training network. lr: 0.000078. clip: 0.031193\n",
      "Iteration 22444: Policy loss: 0.043578. Value loss: 0.089072. Entropy: 0.291442.\n",
      "Iteration 22445: Policy loss: 0.040196. Value loss: 0.045501. Entropy: 0.290939.\n",
      "Iteration 22446: Policy loss: 0.035062. Value loss: 0.035694. Entropy: 0.290680.\n",
      "Training network. lr: 0.000078. clip: 0.031193\n",
      "Iteration 22447: Policy loss: -0.231184. Value loss: 0.200678. Entropy: 0.307532.\n",
      "Iteration 22448: Policy loss: -0.237329. Value loss: 0.054857. Entropy: 0.309787.\n",
      "Iteration 22449: Policy loss: -0.253881. Value loss: 0.033701. Entropy: 0.308558.\n",
      "episode: 7678   score: 365.0  epsilon: 1.0    steps: 848  evaluation reward: 482.2\n",
      "episode: 7679   score: 375.0  epsilon: 1.0    steps: 880  evaluation reward: 481.25\n",
      "Training network. lr: 0.000078. clip: 0.031193\n",
      "Iteration 22450: Policy loss: -0.077570. Value loss: 0.236863. Entropy: 0.297670.\n",
      "Iteration 22451: Policy loss: -0.095758. Value loss: 0.088411. Entropy: 0.297323.\n",
      "Iteration 22452: Policy loss: -0.093902. Value loss: 0.054225. Entropy: 0.298331.\n",
      "Training network. lr: 0.000078. clip: 0.031037\n",
      "Iteration 22453: Policy loss: -0.357121. Value loss: 0.416674. Entropy: 0.303573.\n",
      "Iteration 22454: Policy loss: -0.356013. Value loss: 0.138207. Entropy: 0.303780.\n",
      "Iteration 22455: Policy loss: -0.353578. Value loss: 0.072979. Entropy: 0.303674.\n",
      "Training network. lr: 0.000078. clip: 0.031037\n",
      "Iteration 22456: Policy loss: 0.065779. Value loss: 0.128698. Entropy: 0.299496.\n",
      "Iteration 22457: Policy loss: 0.063369. Value loss: 0.070832. Entropy: 0.299656.\n",
      "Iteration 22458: Policy loss: 0.066013. Value loss: 0.051307. Entropy: 0.298575.\n",
      "episode: 7680   score: 775.0  epsilon: 1.0    steps: 384  evaluation reward: 482.3\n",
      "Training network. lr: 0.000078. clip: 0.031037\n",
      "Iteration 22459: Policy loss: 0.020191. Value loss: 0.086639. Entropy: 0.297252.\n",
      "Iteration 22460: Policy loss: 0.016142. Value loss: 0.047425. Entropy: 0.296923.\n",
      "Iteration 22461: Policy loss: 0.017163. Value loss: 0.035392. Entropy: 0.296620.\n",
      "Training network. lr: 0.000078. clip: 0.031037\n",
      "Iteration 22462: Policy loss: 0.056075. Value loss: 0.129884. Entropy: 0.312951.\n",
      "Iteration 22463: Policy loss: 0.047057. Value loss: 0.067271. Entropy: 0.312126.\n",
      "Iteration 22464: Policy loss: 0.042330. Value loss: 0.056046. Entropy: 0.312153.\n",
      "episode: 7681   score: 270.0  epsilon: 1.0    steps: 216  evaluation reward: 475.55\n",
      "episode: 7682   score: 540.0  epsilon: 1.0    steps: 232  evaluation reward: 474.0\n",
      "Training network. lr: 0.000078. clip: 0.031037\n",
      "Iteration 22465: Policy loss: 0.157706. Value loss: 0.103445. Entropy: 0.290209.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22466: Policy loss: 0.155391. Value loss: 0.053643. Entropy: 0.292960.\n",
      "Iteration 22467: Policy loss: 0.150427. Value loss: 0.040266. Entropy: 0.290949.\n",
      "episode: 7683   score: 595.0  epsilon: 1.0    steps: 336  evaluation reward: 472.7\n",
      "episode: 7684   score: 420.0  epsilon: 1.0    steps: 880  evaluation reward: 472.95\n",
      "Training network. lr: 0.000078. clip: 0.031037\n",
      "Iteration 22468: Policy loss: -0.135768. Value loss: 0.273305. Entropy: 0.290382.\n",
      "Iteration 22469: Policy loss: -0.130168. Value loss: 0.099671. Entropy: 0.290364.\n",
      "Iteration 22470: Policy loss: -0.149736. Value loss: 0.070893. Entropy: 0.289465.\n",
      "Training network. lr: 0.000078. clip: 0.031037\n",
      "Iteration 22471: Policy loss: -0.046754. Value loss: 0.209088. Entropy: 0.304267.\n",
      "Iteration 22472: Policy loss: -0.050781. Value loss: 0.072356. Entropy: 0.305354.\n",
      "Iteration 22473: Policy loss: -0.060518. Value loss: 0.047311. Entropy: 0.304634.\n",
      "episode: 7685   score: 930.0  epsilon: 1.0    steps: 464  evaluation reward: 478.95\n",
      "episode: 7686   score: 385.0  epsilon: 1.0    steps: 624  evaluation reward: 479.75\n",
      "Training network. lr: 0.000078. clip: 0.031037\n",
      "Iteration 22474: Policy loss: 0.079416. Value loss: 0.076704. Entropy: 0.277131.\n",
      "Iteration 22475: Policy loss: 0.071733. Value loss: 0.031490. Entropy: 0.276944.\n",
      "Iteration 22476: Policy loss: 0.073536. Value loss: 0.027778. Entropy: 0.277307.\n",
      "Training network. lr: 0.000078. clip: 0.031037\n",
      "Iteration 22477: Policy loss: 0.255886. Value loss: 0.160259. Entropy: 0.313029.\n",
      "Iteration 22478: Policy loss: 0.264325. Value loss: 0.053353. Entropy: 0.312710.\n",
      "Iteration 22479: Policy loss: 0.263178. Value loss: 0.033102. Entropy: 0.312344.\n",
      "episode: 7687   score: 590.0  epsilon: 1.0    steps: 696  evaluation reward: 481.4\n",
      "episode: 7688   score: 365.0  epsilon: 1.0    steps: 1024  evaluation reward: 479.35\n",
      "Training network. lr: 0.000078. clip: 0.031037\n",
      "Iteration 22480: Policy loss: -0.496523. Value loss: 0.625271. Entropy: 0.295026.\n",
      "Iteration 22481: Policy loss: -0.483359. Value loss: 0.341609. Entropy: 0.295985.\n",
      "Iteration 22482: Policy loss: -0.486131. Value loss: 0.208983. Entropy: 0.297035.\n",
      "Training network. lr: 0.000078. clip: 0.031037\n",
      "Iteration 22483: Policy loss: -0.247417. Value loss: 0.178336. Entropy: 0.299167.\n",
      "Iteration 22484: Policy loss: -0.234598. Value loss: 0.061449. Entropy: 0.298261.\n",
      "Iteration 22485: Policy loss: -0.241729. Value loss: 0.041302. Entropy: 0.299073.\n",
      "Training network. lr: 0.000078. clip: 0.031037\n",
      "Iteration 22486: Policy loss: 0.233729. Value loss: 0.093710. Entropy: 0.306384.\n",
      "Iteration 22487: Policy loss: 0.236427. Value loss: 0.050230. Entropy: 0.306870.\n",
      "Iteration 22488: Policy loss: 0.232589. Value loss: 0.036028. Entropy: 0.307279.\n",
      "episode: 7689   score: 560.0  epsilon: 1.0    steps: 408  evaluation reward: 477.4\n",
      "Training network. lr: 0.000078. clip: 0.031037\n",
      "Iteration 22489: Policy loss: -0.151271. Value loss: 0.269462. Entropy: 0.295705.\n",
      "Iteration 22490: Policy loss: -0.162955. Value loss: 0.094491. Entropy: 0.294848.\n",
      "Iteration 22491: Policy loss: -0.155025. Value loss: 0.057828. Entropy: 0.294743.\n",
      "Training network. lr: 0.000078. clip: 0.031037\n",
      "Iteration 22492: Policy loss: 0.092830. Value loss: 0.152442. Entropy: 0.312717.\n",
      "Iteration 22493: Policy loss: 0.090594. Value loss: 0.089242. Entropy: 0.313018.\n",
      "Iteration 22494: Policy loss: 0.079437. Value loss: 0.071038. Entropy: 0.312131.\n",
      "episode: 7690   score: 285.0  epsilon: 1.0    steps: 384  evaluation reward: 473.7\n",
      "Training network. lr: 0.000078. clip: 0.031037\n",
      "Iteration 22495: Policy loss: 0.671091. Value loss: 0.271295. Entropy: 0.290206.\n",
      "Iteration 22496: Policy loss: 0.637909. Value loss: 0.074817. Entropy: 0.289251.\n",
      "Iteration 22497: Policy loss: 0.643395. Value loss: 0.046878. Entropy: 0.289441.\n",
      "episode: 7691   score: 635.0  epsilon: 1.0    steps: 136  evaluation reward: 472.3\n",
      "episode: 7692   score: 545.0  epsilon: 1.0    steps: 504  evaluation reward: 468.3\n",
      "Training network. lr: 0.000078. clip: 0.031037\n",
      "Iteration 22498: Policy loss: 0.031874. Value loss: 0.199682. Entropy: 0.283681.\n",
      "Iteration 22499: Policy loss: 0.027708. Value loss: 0.090340. Entropy: 0.283714.\n",
      "Iteration 22500: Policy loss: 0.030356. Value loss: 0.065473. Entropy: 0.282916.\n",
      "episode: 7693   score: 360.0  epsilon: 1.0    steps: 88  evaluation reward: 468.0\n",
      "Training network. lr: 0.000077. clip: 0.030889\n",
      "Iteration 22501: Policy loss: 0.335179. Value loss: 0.338495. Entropy: 0.295375.\n",
      "Iteration 22502: Policy loss: 0.321941. Value loss: 0.145396. Entropy: 0.294036.\n",
      "Iteration 22503: Policy loss: 0.314789. Value loss: 0.081229. Entropy: 0.293918.\n",
      "episode: 7694   score: 520.0  epsilon: 1.0    steps: 568  evaluation reward: 470.1\n",
      "Training network. lr: 0.000077. clip: 0.030889\n",
      "Iteration 22504: Policy loss: 0.107409. Value loss: 0.172562. Entropy: 0.285651.\n",
      "Iteration 22505: Policy loss: 0.098966. Value loss: 0.097774. Entropy: 0.285320.\n",
      "Iteration 22506: Policy loss: 0.105784. Value loss: 0.082067. Entropy: 0.286304.\n",
      "episode: 7695   score: 645.0  epsilon: 1.0    steps: 344  evaluation reward: 472.95\n",
      "episode: 7696   score: 310.0  epsilon: 1.0    steps: 392  evaluation reward: 472.1\n",
      "episode: 7697   score: 410.0  epsilon: 1.0    steps: 808  evaluation reward: 469.25\n",
      "Training network. lr: 0.000077. clip: 0.030889\n",
      "Iteration 22507: Policy loss: -0.072348. Value loss: 0.197453. Entropy: 0.273309.\n",
      "Iteration 22508: Policy loss: -0.079955. Value loss: 0.096868. Entropy: 0.272922.\n",
      "Iteration 22509: Policy loss: -0.069633. Value loss: 0.071689. Entropy: 0.272944.\n",
      "Training network. lr: 0.000077. clip: 0.030889\n",
      "Iteration 22510: Policy loss: -0.371053. Value loss: 0.247975. Entropy: 0.303523.\n",
      "Iteration 22511: Policy loss: -0.371026. Value loss: 0.119399. Entropy: 0.304916.\n",
      "Iteration 22512: Policy loss: -0.376252. Value loss: 0.085048. Entropy: 0.305210.\n",
      "Training network. lr: 0.000077. clip: 0.030889\n",
      "Iteration 22513: Policy loss: 0.157444. Value loss: 0.337636. Entropy: 0.304182.\n",
      "Iteration 22514: Policy loss: 0.152128. Value loss: 0.104688. Entropy: 0.303490.\n",
      "Iteration 22515: Policy loss: 0.141183. Value loss: 0.080176. Entropy: 0.303002.\n",
      "Training network. lr: 0.000077. clip: 0.030889\n",
      "Iteration 22516: Policy loss: 0.214210. Value loss: 0.511665. Entropy: 0.310567.\n",
      "Iteration 22517: Policy loss: 0.192652. Value loss: 0.192922. Entropy: 0.309526.\n",
      "Iteration 22518: Policy loss: 0.221549. Value loss: 0.129793. Entropy: 0.309702.\n",
      "Training network. lr: 0.000077. clip: 0.030889\n",
      "Iteration 22519: Policy loss: 0.284786. Value loss: 0.185661. Entropy: 0.308235.\n",
      "Iteration 22520: Policy loss: 0.273244. Value loss: 0.086145. Entropy: 0.306812.\n",
      "Iteration 22521: Policy loss: 0.279588. Value loss: 0.059801. Entropy: 0.306450.\n",
      "Training network. lr: 0.000077. clip: 0.030889\n",
      "Iteration 22522: Policy loss: 0.106835. Value loss: 0.107955. Entropy: 0.308750.\n",
      "Iteration 22523: Policy loss: 0.113217. Value loss: 0.044794. Entropy: 0.309120.\n",
      "Iteration 22524: Policy loss: 0.102238. Value loss: 0.035450. Entropy: 0.308915.\n",
      "episode: 7698   score: 460.0  epsilon: 1.0    steps: 864  evaluation reward: 470.35\n",
      "Training network. lr: 0.000077. clip: 0.030889\n",
      "Iteration 22525: Policy loss: -0.028733. Value loss: 0.242225. Entropy: 0.303795.\n",
      "Iteration 22526: Policy loss: -0.025740. Value loss: 0.116711. Entropy: 0.304145.\n",
      "Iteration 22527: Policy loss: -0.031893. Value loss: 0.086949. Entropy: 0.303906.\n",
      "episode: 7699   score: 470.0  epsilon: 1.0    steps: 400  evaluation reward: 470.9\n",
      "episode: 7700   score: 420.0  epsilon: 1.0    steps: 920  evaluation reward: 472.25\n",
      "Training network. lr: 0.000077. clip: 0.030889\n",
      "Iteration 22528: Policy loss: 0.392723. Value loss: 0.368072. Entropy: 0.283451.\n",
      "Iteration 22529: Policy loss: 0.371276. Value loss: 0.169639. Entropy: 0.283242.\n",
      "Iteration 22530: Policy loss: 0.376461. Value loss: 0.106483. Entropy: 0.282965.\n",
      "now time :  2019-09-06 13:28:14.481531\n",
      "episode: 7701   score: 345.0  epsilon: 1.0    steps: 80  evaluation reward: 472.6\n",
      "episode: 7702   score: 695.0  epsilon: 1.0    steps: 120  evaluation reward: 475.6\n",
      "Training network. lr: 0.000077. clip: 0.030889\n",
      "Iteration 22531: Policy loss: 0.285091. Value loss: 0.103119. Entropy: 0.276357.\n",
      "Iteration 22532: Policy loss: 0.288188. Value loss: 0.062168. Entropy: 0.277251.\n",
      "Iteration 22533: Policy loss: 0.278786. Value loss: 0.050753. Entropy: 0.277624.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000077. clip: 0.030889\n",
      "Iteration 22534: Policy loss: 0.239463. Value loss: 0.116008. Entropy: 0.306398.\n",
      "Iteration 22535: Policy loss: 0.234212. Value loss: 0.056058. Entropy: 0.306004.\n",
      "Iteration 22536: Policy loss: 0.233496. Value loss: 0.040803. Entropy: 0.305598.\n",
      "episode: 7703   score: 450.0  epsilon: 1.0    steps: 656  evaluation reward: 475.4\n",
      "Training network. lr: 0.000077. clip: 0.030889\n",
      "Iteration 22537: Policy loss: 0.429819. Value loss: 0.174958. Entropy: 0.298722.\n",
      "Iteration 22538: Policy loss: 0.437832. Value loss: 0.080669. Entropy: 0.298145.\n",
      "Iteration 22539: Policy loss: 0.417410. Value loss: 0.058986. Entropy: 0.297986.\n",
      "episode: 7704   score: 495.0  epsilon: 1.0    steps: 768  evaluation reward: 476.1\n",
      "Training network. lr: 0.000077. clip: 0.030889\n",
      "Iteration 22540: Policy loss: 0.084177. Value loss: 0.143619. Entropy: 0.303138.\n",
      "Iteration 22541: Policy loss: 0.092655. Value loss: 0.078896. Entropy: 0.301861.\n",
      "Iteration 22542: Policy loss: 0.085041. Value loss: 0.055953. Entropy: 0.301769.\n",
      "episode: 7705   score: 210.0  epsilon: 1.0    steps: 208  evaluation reward: 471.2\n",
      "episode: 7706   score: 745.0  epsilon: 1.0    steps: 240  evaluation reward: 473.55\n",
      "Training network. lr: 0.000077. clip: 0.030889\n",
      "Iteration 22543: Policy loss: -0.077670. Value loss: 0.084546. Entropy: 0.276284.\n",
      "Iteration 22544: Policy loss: -0.068882. Value loss: 0.045973. Entropy: 0.276722.\n",
      "Iteration 22545: Policy loss: -0.076062. Value loss: 0.036961. Entropy: 0.276613.\n",
      "Training network. lr: 0.000077. clip: 0.030889\n",
      "Iteration 22546: Policy loss: 0.025179. Value loss: 0.136275. Entropy: 0.297660.\n",
      "Iteration 22547: Policy loss: 0.014937. Value loss: 0.057437. Entropy: 0.297303.\n",
      "Iteration 22548: Policy loss: 0.021372. Value loss: 0.037403. Entropy: 0.297478.\n",
      "episode: 7707   score: 315.0  epsilon: 1.0    steps: 768  evaluation reward: 473.55\n",
      "episode: 7708   score: 365.0  epsilon: 1.0    steps: 944  evaluation reward: 473.2\n",
      "Training network. lr: 0.000077. clip: 0.030889\n",
      "Iteration 22549: Policy loss: 0.234514. Value loss: 0.128557. Entropy: 0.294156.\n",
      "Iteration 22550: Policy loss: 0.226099. Value loss: 0.060533. Entropy: 0.294424.\n",
      "Iteration 22551: Policy loss: 0.220537. Value loss: 0.040442. Entropy: 0.293267.\n",
      "episode: 7709   score: 335.0  epsilon: 1.0    steps: 744  evaluation reward: 471.55\n",
      "Training network. lr: 0.000077. clip: 0.030733\n",
      "Iteration 22552: Policy loss: -0.550804. Value loss: 0.385757. Entropy: 0.291524.\n",
      "Iteration 22553: Policy loss: -0.561474. Value loss: 0.159915. Entropy: 0.290610.\n",
      "Iteration 22554: Policy loss: -0.554036. Value loss: 0.093420. Entropy: 0.291339.\n",
      "Training network. lr: 0.000077. clip: 0.030733\n",
      "Iteration 22555: Policy loss: -0.070353. Value loss: 0.133706. Entropy: 0.304224.\n",
      "Iteration 22556: Policy loss: -0.071676. Value loss: 0.055053. Entropy: 0.304454.\n",
      "Iteration 22557: Policy loss: -0.071658. Value loss: 0.034607. Entropy: 0.304661.\n",
      "Training network. lr: 0.000077. clip: 0.030733\n",
      "Iteration 22558: Policy loss: 0.080210. Value loss: 0.167818. Entropy: 0.314616.\n",
      "Iteration 22559: Policy loss: 0.078422. Value loss: 0.081597. Entropy: 0.314265.\n",
      "Iteration 22560: Policy loss: 0.071199. Value loss: 0.054336. Entropy: 0.314440.\n",
      "episode: 7710   score: 265.0  epsilon: 1.0    steps: 296  evaluation reward: 470.75\n",
      "Training network. lr: 0.000077. clip: 0.030733\n",
      "Iteration 22561: Policy loss: -0.058683. Value loss: 0.103152. Entropy: 0.296141.\n",
      "Iteration 22562: Policy loss: -0.066132. Value loss: 0.044069. Entropy: 0.296260.\n",
      "Iteration 22563: Policy loss: -0.062843. Value loss: 0.033243. Entropy: 0.297076.\n",
      "episode: 7711   score: 485.0  epsilon: 1.0    steps: 344  evaluation reward: 473.45\n",
      "episode: 7712   score: 670.0  epsilon: 1.0    steps: 352  evaluation reward: 477.75\n",
      "Training network. lr: 0.000077. clip: 0.030733\n",
      "Iteration 22564: Policy loss: 0.045752. Value loss: 0.107107. Entropy: 0.276892.\n",
      "Iteration 22565: Policy loss: 0.039278. Value loss: 0.035604. Entropy: 0.278207.\n",
      "Iteration 22566: Policy loss: 0.039517. Value loss: 0.024107. Entropy: 0.277288.\n",
      "Training network. lr: 0.000077. clip: 0.030733\n",
      "Iteration 22567: Policy loss: 0.250864. Value loss: 0.149321. Entropy: 0.308594.\n",
      "Iteration 22568: Policy loss: 0.249623. Value loss: 0.061102. Entropy: 0.307697.\n",
      "Iteration 22569: Policy loss: 0.249790. Value loss: 0.041973. Entropy: 0.307994.\n",
      "episode: 7713   score: 345.0  epsilon: 1.0    steps: 800  evaluation reward: 477.6\n",
      "Training network. lr: 0.000077. clip: 0.030733\n",
      "Iteration 22570: Policy loss: -0.087552. Value loss: 0.290414. Entropy: 0.301239.\n",
      "Iteration 22571: Policy loss: -0.099420. Value loss: 0.127960. Entropy: 0.301979.\n",
      "Iteration 22572: Policy loss: -0.099452. Value loss: 0.072398. Entropy: 0.302102.\n",
      "episode: 7714   score: 335.0  epsilon: 1.0    steps: 176  evaluation reward: 478.85\n",
      "Training network. lr: 0.000077. clip: 0.030733\n",
      "Iteration 22573: Policy loss: -0.038137. Value loss: 0.081237. Entropy: 0.296762.\n",
      "Iteration 22574: Policy loss: -0.042522. Value loss: 0.044728. Entropy: 0.296373.\n",
      "Iteration 22575: Policy loss: -0.044952. Value loss: 0.034005. Entropy: 0.297336.\n",
      "episode: 7715   score: 345.0  epsilon: 1.0    steps: 176  evaluation reward: 478.5\n",
      "Training network. lr: 0.000077. clip: 0.030733\n",
      "Iteration 22576: Policy loss: -0.133358. Value loss: 0.105885. Entropy: 0.293790.\n",
      "Iteration 22577: Policy loss: -0.131831. Value loss: 0.052121. Entropy: 0.293035.\n",
      "Iteration 22578: Policy loss: -0.136441. Value loss: 0.037017. Entropy: 0.293295.\n",
      "Training network. lr: 0.000077. clip: 0.030733\n",
      "Iteration 22579: Policy loss: 0.186604. Value loss: 0.128814. Entropy: 0.311538.\n",
      "Iteration 22580: Policy loss: 0.188236. Value loss: 0.061024. Entropy: 0.311647.\n",
      "Iteration 22581: Policy loss: 0.181624. Value loss: 0.049750. Entropy: 0.310988.\n",
      "Training network. lr: 0.000077. clip: 0.030733\n",
      "Iteration 22582: Policy loss: 0.286653. Value loss: 0.189042. Entropy: 0.301861.\n",
      "Iteration 22583: Policy loss: 0.298176. Value loss: 0.047357. Entropy: 0.302902.\n",
      "Iteration 22584: Policy loss: 0.292968. Value loss: 0.027364. Entropy: 0.302386.\n",
      "episode: 7716   score: 635.0  epsilon: 1.0    steps: 48  evaluation reward: 478.4\n",
      "episode: 7717   score: 490.0  epsilon: 1.0    steps: 360  evaluation reward: 480.15\n",
      "Training network. lr: 0.000077. clip: 0.030733\n",
      "Iteration 22585: Policy loss: -0.179103. Value loss: 0.282002. Entropy: 0.282549.\n",
      "Iteration 22586: Policy loss: -0.202966. Value loss: 0.221081. Entropy: 0.282114.\n",
      "Iteration 22587: Policy loss: -0.208645. Value loss: 0.176211. Entropy: 0.282713.\n",
      "episode: 7718   score: 510.0  epsilon: 1.0    steps: 712  evaluation reward: 478.75\n",
      "Training network. lr: 0.000077. clip: 0.030733\n",
      "Iteration 22588: Policy loss: 0.121589. Value loss: 0.134332. Entropy: 0.296083.\n",
      "Iteration 22589: Policy loss: 0.132348. Value loss: 0.053563. Entropy: 0.296289.\n",
      "Iteration 22590: Policy loss: 0.127231. Value loss: 0.032480. Entropy: 0.297131.\n",
      "Training network. lr: 0.000077. clip: 0.030733\n",
      "Iteration 22591: Policy loss: -0.171906. Value loss: 0.225365. Entropy: 0.305097.\n",
      "Iteration 22592: Policy loss: -0.179176. Value loss: 0.075759. Entropy: 0.304421.\n",
      "Iteration 22593: Policy loss: -0.184458. Value loss: 0.057743. Entropy: 0.303933.\n",
      "episode: 7719   score: 315.0  epsilon: 1.0    steps: 176  evaluation reward: 475.2\n",
      "Training network. lr: 0.000077. clip: 0.030733\n",
      "Iteration 22594: Policy loss: -0.027506. Value loss: 0.191356. Entropy: 0.296037.\n",
      "Iteration 22595: Policy loss: -0.033860. Value loss: 0.062210. Entropy: 0.294795.\n",
      "Iteration 22596: Policy loss: -0.042389. Value loss: 0.041874. Entropy: 0.294325.\n",
      "episode: 7720   score: 975.0  epsilon: 1.0    steps: 520  evaluation reward: 477.4\n",
      "episode: 7721   score: 470.0  epsilon: 1.0    steps: 632  evaluation reward: 478.95\n",
      "Training network. lr: 0.000077. clip: 0.030733\n",
      "Iteration 22597: Policy loss: 0.253026. Value loss: 0.121633. Entropy: 0.276634.\n",
      "Iteration 22598: Policy loss: 0.257299. Value loss: 0.049626. Entropy: 0.275618.\n",
      "Iteration 22599: Policy loss: 0.254374. Value loss: 0.037264. Entropy: 0.275562.\n",
      "Training network. lr: 0.000077. clip: 0.030733\n",
      "Iteration 22600: Policy loss: 0.127321. Value loss: 0.252624. Entropy: 0.307917.\n",
      "Iteration 22601: Policy loss: 0.132882. Value loss: 0.075832. Entropy: 0.308837.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22602: Policy loss: 0.143711. Value loss: 0.039770. Entropy: 0.308504.\n",
      "Training network. lr: 0.000076. clip: 0.030576\n",
      "Iteration 22603: Policy loss: 0.208703. Value loss: 0.169661. Entropy: 0.306465.\n",
      "Iteration 22604: Policy loss: 0.205095. Value loss: 0.073171. Entropy: 0.305851.\n",
      "Iteration 22605: Policy loss: 0.213865. Value loss: 0.045100. Entropy: 0.305650.\n",
      "episode: 7722   score: 185.0  epsilon: 1.0    steps: 64  evaluation reward: 476.85\n",
      "Training network. lr: 0.000076. clip: 0.030576\n",
      "Iteration 22606: Policy loss: -0.065908. Value loss: 0.270470. Entropy: 0.298646.\n",
      "Iteration 22607: Policy loss: -0.068302. Value loss: 0.186406. Entropy: 0.298772.\n",
      "Iteration 22608: Policy loss: -0.089378. Value loss: 0.161461. Entropy: 0.297888.\n",
      "episode: 7723   score: 435.0  epsilon: 1.0    steps: 360  evaluation reward: 476.5\n",
      "episode: 7724   score: 560.0  epsilon: 1.0    steps: 1008  evaluation reward: 478.65\n",
      "Training network. lr: 0.000076. clip: 0.030576\n",
      "Iteration 22609: Policy loss: 0.097067. Value loss: 0.179777. Entropy: 0.291023.\n",
      "Iteration 22610: Policy loss: 0.106332. Value loss: 0.082605. Entropy: 0.290031.\n",
      "Iteration 22611: Policy loss: 0.100604. Value loss: 0.060731. Entropy: 0.289759.\n",
      "episode: 7725   score: 630.0  epsilon: 1.0    steps: 736  evaluation reward: 481.1\n",
      "Training network. lr: 0.000076. clip: 0.030576\n",
      "Iteration 22612: Policy loss: -0.233139. Value loss: 0.121333. Entropy: 0.284823.\n",
      "Iteration 22613: Policy loss: -0.242296. Value loss: 0.070409. Entropy: 0.286136.\n",
      "Iteration 22614: Policy loss: -0.245434. Value loss: 0.055668. Entropy: 0.285654.\n",
      "episode: 7726   score: 630.0  epsilon: 1.0    steps: 824  evaluation reward: 482.2\n",
      "Training network. lr: 0.000076. clip: 0.030576\n",
      "Iteration 22615: Policy loss: -0.019541. Value loss: 0.343307. Entropy: 0.292475.\n",
      "Iteration 22616: Policy loss: -0.026966. Value loss: 0.182773. Entropy: 0.294869.\n",
      "Iteration 22617: Policy loss: -0.031377. Value loss: 0.123541. Entropy: 0.293788.\n",
      "Training network. lr: 0.000076. clip: 0.030576\n",
      "Iteration 22618: Policy loss: 0.107253. Value loss: 0.089039. Entropy: 0.306397.\n",
      "Iteration 22619: Policy loss: 0.110662. Value loss: 0.033814. Entropy: 0.306518.\n",
      "Iteration 22620: Policy loss: 0.109568. Value loss: 0.023515. Entropy: 0.307217.\n",
      "episode: 7727   score: 345.0  epsilon: 1.0    steps: 464  evaluation reward: 481.25\n",
      "Training network. lr: 0.000076. clip: 0.030576\n",
      "Iteration 22621: Policy loss: 0.251641. Value loss: 0.174567. Entropy: 0.295452.\n",
      "Iteration 22622: Policy loss: 0.241334. Value loss: 0.061750. Entropy: 0.294575.\n",
      "Iteration 22623: Policy loss: 0.238171. Value loss: 0.040204. Entropy: 0.294867.\n",
      "episode: 7728   score: 420.0  epsilon: 1.0    steps: 928  evaluation reward: 476.5\n",
      "Training network. lr: 0.000076. clip: 0.030576\n",
      "Iteration 22624: Policy loss: -0.031348. Value loss: 0.307551. Entropy: 0.307223.\n",
      "Iteration 22625: Policy loss: -0.042677. Value loss: 0.184301. Entropy: 0.307033.\n",
      "Iteration 22626: Policy loss: -0.054072. Value loss: 0.127487. Entropy: 0.307989.\n",
      "episode: 7729   score: 565.0  epsilon: 1.0    steps: 152  evaluation reward: 479.15\n",
      "episode: 7730   score: 580.0  epsilon: 1.0    steps: 368  evaluation reward: 480.2\n",
      "Training network. lr: 0.000076. clip: 0.030576\n",
      "Iteration 22627: Policy loss: 0.123662. Value loss: 0.104065. Entropy: 0.268138.\n",
      "Iteration 22628: Policy loss: 0.123164. Value loss: 0.055724. Entropy: 0.269719.\n",
      "Iteration 22629: Policy loss: 0.118250. Value loss: 0.035979. Entropy: 0.269522.\n",
      "episode: 7731   score: 270.0  epsilon: 1.0    steps: 824  evaluation reward: 478.3\n",
      "Training network. lr: 0.000076. clip: 0.030576\n",
      "Iteration 22630: Policy loss: -0.314193. Value loss: 0.214690. Entropy: 0.300867.\n",
      "Iteration 22631: Policy loss: -0.301916. Value loss: 0.068051. Entropy: 0.301171.\n",
      "Iteration 22632: Policy loss: -0.338109. Value loss: 0.039630. Entropy: 0.299555.\n",
      "episode: 7732   score: 380.0  epsilon: 1.0    steps: 864  evaluation reward: 472.55\n",
      "Training network. lr: 0.000076. clip: 0.030576\n",
      "Iteration 22633: Policy loss: -0.029136. Value loss: 0.169197. Entropy: 0.293875.\n",
      "Iteration 22634: Policy loss: -0.035302. Value loss: 0.062813. Entropy: 0.293670.\n",
      "Iteration 22635: Policy loss: -0.044438. Value loss: 0.035250. Entropy: 0.294149.\n",
      "Training network. lr: 0.000076. clip: 0.030576\n",
      "Iteration 22636: Policy loss: 0.188228. Value loss: 0.353870. Entropy: 0.307623.\n",
      "Iteration 22637: Policy loss: 0.166988. Value loss: 0.121186. Entropy: 0.308183.\n",
      "Iteration 22638: Policy loss: 0.165565. Value loss: 0.084407. Entropy: 0.307238.\n",
      "episode: 7733   score: 365.0  epsilon: 1.0    steps: 40  evaluation reward: 472.2\n",
      "Training network. lr: 0.000076. clip: 0.030576\n",
      "Iteration 22639: Policy loss: 0.042727. Value loss: 0.085651. Entropy: 0.290668.\n",
      "Iteration 22640: Policy loss: 0.041248. Value loss: 0.037518. Entropy: 0.291112.\n",
      "Iteration 22641: Policy loss: 0.037991. Value loss: 0.024282. Entropy: 0.291606.\n",
      "Training network. lr: 0.000076. clip: 0.030576\n",
      "Iteration 22642: Policy loss: 0.042541. Value loss: 0.113448. Entropy: 0.310652.\n",
      "Iteration 22643: Policy loss: 0.037557. Value loss: 0.059094. Entropy: 0.308016.\n",
      "Iteration 22644: Policy loss: 0.039578. Value loss: 0.040827. Entropy: 0.308697.\n",
      "episode: 7734   score: 595.0  epsilon: 1.0    steps: 272  evaluation reward: 474.7\n",
      "episode: 7735   score: 470.0  epsilon: 1.0    steps: 296  evaluation reward: 475.6\n",
      "Training network. lr: 0.000076. clip: 0.030576\n",
      "Iteration 22645: Policy loss: -0.367295. Value loss: 0.339131. Entropy: 0.284318.\n",
      "Iteration 22646: Policy loss: -0.378924. Value loss: 0.121721. Entropy: 0.286054.\n",
      "Iteration 22647: Policy loss: -0.382762. Value loss: 0.071033. Entropy: 0.284848.\n",
      "episode: 7736   score: 370.0  epsilon: 1.0    steps: 984  evaluation reward: 475.95\n",
      "Training network. lr: 0.000076. clip: 0.030576\n",
      "Iteration 22648: Policy loss: 0.100189. Value loss: 0.120707. Entropy: 0.306589.\n",
      "Iteration 22649: Policy loss: 0.094370. Value loss: 0.060737. Entropy: 0.306492.\n",
      "Iteration 22650: Policy loss: 0.101561. Value loss: 0.050049. Entropy: 0.305968.\n",
      "Training network. lr: 0.000076. clip: 0.030428\n",
      "Iteration 22651: Policy loss: 0.045278. Value loss: 0.051650. Entropy: 0.303263.\n",
      "Iteration 22652: Policy loss: 0.041304. Value loss: 0.025516. Entropy: 0.302593.\n",
      "Iteration 22653: Policy loss: 0.041813. Value loss: 0.017222. Entropy: 0.301783.\n",
      "episode: 7737   score: 625.0  epsilon: 1.0    steps: 712  evaluation reward: 478.25\n",
      "Training network. lr: 0.000076. clip: 0.030428\n",
      "Iteration 22654: Policy loss: -0.087377. Value loss: 0.375335. Entropy: 0.299602.\n",
      "Iteration 22655: Policy loss: -0.094842. Value loss: 0.150468. Entropy: 0.300804.\n",
      "Iteration 22656: Policy loss: -0.079938. Value loss: 0.093087. Entropy: 0.299446.\n",
      "episode: 7738   score: 340.0  epsilon: 1.0    steps: 656  evaluation reward: 478.5\n",
      "Training network. lr: 0.000076. clip: 0.030428\n",
      "Iteration 22657: Policy loss: 0.247273. Value loss: 0.227630. Entropy: 0.290663.\n",
      "Iteration 22658: Policy loss: 0.247909. Value loss: 0.100956. Entropy: 0.291354.\n",
      "Iteration 22659: Policy loss: 0.221328. Value loss: 0.062321. Entropy: 0.290596.\n",
      "episode: 7739   score: 695.0  epsilon: 1.0    steps: 856  evaluation reward: 480.2\n",
      "Training network. lr: 0.000076. clip: 0.030428\n",
      "Iteration 22660: Policy loss: 0.259414. Value loss: 0.103320. Entropy: 0.301001.\n",
      "Iteration 22661: Policy loss: 0.258250. Value loss: 0.028630. Entropy: 0.300792.\n",
      "Iteration 22662: Policy loss: 0.253967. Value loss: 0.020850. Entropy: 0.300809.\n",
      "episode: 7740   score: 390.0  epsilon: 1.0    steps: 320  evaluation reward: 480.75\n",
      "Training network. lr: 0.000076. clip: 0.030428\n",
      "Iteration 22663: Policy loss: 0.118109. Value loss: 0.104604. Entropy: 0.284465.\n",
      "Iteration 22664: Policy loss: 0.118955. Value loss: 0.058570. Entropy: 0.285434.\n",
      "Iteration 22665: Policy loss: 0.115297. Value loss: 0.044796. Entropy: 0.286156.\n",
      "episode: 7741   score: 320.0  epsilon: 1.0    steps: 384  evaluation reward: 477.25\n",
      "episode: 7742   score: 605.0  epsilon: 1.0    steps: 840  evaluation reward: 479.8\n",
      "Training network. lr: 0.000076. clip: 0.030428\n",
      "Iteration 22666: Policy loss: -0.222611. Value loss: 0.338062. Entropy: 0.288417.\n",
      "Iteration 22667: Policy loss: -0.230726. Value loss: 0.115028. Entropy: 0.289091.\n",
      "Iteration 22668: Policy loss: -0.254027. Value loss: 0.093334. Entropy: 0.290111.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7743   score: 560.0  epsilon: 1.0    steps: 872  evaluation reward: 474.8\n",
      "Training network. lr: 0.000076. clip: 0.030428\n",
      "Iteration 22669: Policy loss: -0.015279. Value loss: 0.167451. Entropy: 0.299707.\n",
      "Iteration 22670: Policy loss: -0.017090. Value loss: 0.051529. Entropy: 0.299021.\n",
      "Iteration 22671: Policy loss: -0.015595. Value loss: 0.032572. Entropy: 0.298515.\n",
      "Training network. lr: 0.000076. clip: 0.030428\n",
      "Iteration 22672: Policy loss: -0.070822. Value loss: 0.221725. Entropy: 0.304174.\n",
      "Iteration 22673: Policy loss: -0.065830. Value loss: 0.089177. Entropy: 0.304027.\n",
      "Iteration 22674: Policy loss: -0.072377. Value loss: 0.068390. Entropy: 0.303858.\n",
      "Training network. lr: 0.000076. clip: 0.030428\n",
      "Iteration 22675: Policy loss: -0.742796. Value loss: 0.531033. Entropy: 0.306377.\n",
      "Iteration 22676: Policy loss: -0.741571. Value loss: 0.272618. Entropy: 0.306561.\n",
      "Iteration 22677: Policy loss: -0.754975. Value loss: 0.182070. Entropy: 0.307021.\n",
      "episode: 7744   score: 335.0  epsilon: 1.0    steps: 416  evaluation reward: 473.3\n",
      "episode: 7745   score: 690.0  epsilon: 1.0    steps: 712  evaluation reward: 476.75\n",
      "Training network. lr: 0.000076. clip: 0.030428\n",
      "Iteration 22678: Policy loss: 0.193221. Value loss: 0.203155. Entropy: 0.283918.\n",
      "Iteration 22679: Policy loss: 0.195010. Value loss: 0.063978. Entropy: 0.283507.\n",
      "Iteration 22680: Policy loss: 0.197139. Value loss: 0.045746. Entropy: 0.283423.\n",
      "episode: 7746   score: 365.0  epsilon: 1.0    steps: 96  evaluation reward: 475.65\n",
      "Training network. lr: 0.000076. clip: 0.030428\n",
      "Iteration 22681: Policy loss: 0.276423. Value loss: 0.155801. Entropy: 0.290793.\n",
      "Iteration 22682: Policy loss: 0.281599. Value loss: 0.086829. Entropy: 0.290069.\n",
      "Iteration 22683: Policy loss: 0.275363. Value loss: 0.065342. Entropy: 0.290128.\n",
      "Training network. lr: 0.000076. clip: 0.030428\n",
      "Iteration 22684: Policy loss: -0.056069. Value loss: 0.317294. Entropy: 0.303817.\n",
      "Iteration 22685: Policy loss: -0.057494. Value loss: 0.129647. Entropy: 0.303753.\n",
      "Iteration 22686: Policy loss: -0.050255. Value loss: 0.091152. Entropy: 0.303262.\n",
      "episode: 7747   score: 595.0  epsilon: 1.0    steps: 1016  evaluation reward: 471.5\n",
      "Training network. lr: 0.000076. clip: 0.030428\n",
      "Iteration 22687: Policy loss: -0.042775. Value loss: 0.119259. Entropy: 0.302505.\n",
      "Iteration 22688: Policy loss: -0.039106. Value loss: 0.047567. Entropy: 0.302077.\n",
      "Iteration 22689: Policy loss: -0.038348. Value loss: 0.029157. Entropy: 0.302832.\n",
      "Training network. lr: 0.000076. clip: 0.030428\n",
      "Iteration 22690: Policy loss: 0.330773. Value loss: 0.281090. Entropy: 0.300236.\n",
      "Iteration 22691: Policy loss: 0.328006. Value loss: 0.083798. Entropy: 0.301222.\n",
      "Iteration 22692: Policy loss: 0.321310. Value loss: 0.050097. Entropy: 0.300962.\n",
      "episode: 7748   score: 765.0  epsilon: 1.0    steps: 96  evaluation reward: 473.6\n",
      "episode: 7749   score: 355.0  epsilon: 1.0    steps: 320  evaluation reward: 469.9\n",
      "Training network. lr: 0.000076. clip: 0.030428\n",
      "Iteration 22693: Policy loss: -0.323811. Value loss: 0.218907. Entropy: 0.277788.\n",
      "Iteration 22694: Policy loss: -0.321236. Value loss: 0.090633. Entropy: 0.280909.\n",
      "Iteration 22695: Policy loss: -0.317605. Value loss: 0.052305. Entropy: 0.279540.\n",
      "episode: 7750   score: 590.0  epsilon: 1.0    steps: 816  evaluation reward: 470.8\n",
      "Training network. lr: 0.000076. clip: 0.030428\n",
      "Iteration 22696: Policy loss: 0.189108. Value loss: 0.104635. Entropy: 0.298035.\n",
      "Iteration 22697: Policy loss: 0.180639. Value loss: 0.055098. Entropy: 0.299208.\n",
      "Iteration 22698: Policy loss: 0.180055. Value loss: 0.040077. Entropy: 0.297634.\n",
      "now time :  2019-09-06 13:38:42.178820\n",
      "episode: 7751   score: 280.0  epsilon: 1.0    steps: 808  evaluation reward: 465.6\n",
      "Training network. lr: 0.000076. clip: 0.030428\n",
      "Iteration 22699: Policy loss: 0.162103. Value loss: 0.245181. Entropy: 0.290467.\n",
      "Iteration 22700: Policy loss: 0.139715. Value loss: 0.068774. Entropy: 0.289856.\n",
      "Iteration 22701: Policy loss: 0.153276. Value loss: 0.041413. Entropy: 0.289843.\n",
      "episode: 7752   score: 1005.0  epsilon: 1.0    steps: 608  evaluation reward: 467.65\n",
      "episode: 7753   score: 490.0  epsilon: 1.0    steps: 776  evaluation reward: 469.75\n",
      "Training network. lr: 0.000076. clip: 0.030272\n",
      "Iteration 22702: Policy loss: 0.545831. Value loss: 0.229661. Entropy: 0.283795.\n",
      "Iteration 22703: Policy loss: 0.546188. Value loss: 0.071859. Entropy: 0.284244.\n",
      "Iteration 22704: Policy loss: 0.527053. Value loss: 0.039981. Entropy: 0.282418.\n",
      "Training network. lr: 0.000076. clip: 0.030272\n",
      "Iteration 22705: Policy loss: 0.128337. Value loss: 0.099080. Entropy: 0.306393.\n",
      "Iteration 22706: Policy loss: 0.129070. Value loss: 0.038247. Entropy: 0.305030.\n",
      "Iteration 22707: Policy loss: 0.125317. Value loss: 0.026576. Entropy: 0.305373.\n",
      "Training network. lr: 0.000076. clip: 0.030272\n",
      "Iteration 22708: Policy loss: 0.011922. Value loss: 0.226751. Entropy: 0.304023.\n",
      "Iteration 22709: Policy loss: -0.007139. Value loss: 0.066236. Entropy: 0.304175.\n",
      "Iteration 22710: Policy loss: -0.014491. Value loss: 0.040102. Entropy: 0.304402.\n",
      "episode: 7754   score: 445.0  epsilon: 1.0    steps: 296  evaluation reward: 470.0\n",
      "Training network. lr: 0.000076. clip: 0.030272\n",
      "Iteration 22711: Policy loss: -0.157673. Value loss: 0.207243. Entropy: 0.299452.\n",
      "Iteration 22712: Policy loss: -0.154155. Value loss: 0.080631. Entropy: 0.299243.\n",
      "Iteration 22713: Policy loss: -0.161926. Value loss: 0.055204. Entropy: 0.299883.\n",
      "episode: 7755   score: 390.0  epsilon: 1.0    steps: 488  evaluation reward: 466.5\n",
      "Training network. lr: 0.000076. clip: 0.030272\n",
      "Iteration 22714: Policy loss: 0.169001. Value loss: 0.117832. Entropy: 0.296060.\n",
      "Iteration 22715: Policy loss: 0.159824. Value loss: 0.030376. Entropy: 0.294254.\n",
      "Iteration 22716: Policy loss: 0.160882. Value loss: 0.020887. Entropy: 0.295153.\n",
      "Training network. lr: 0.000076. clip: 0.030272\n",
      "Iteration 22717: Policy loss: -0.042787. Value loss: 0.112234. Entropy: 0.307332.\n",
      "Iteration 22718: Policy loss: -0.041496. Value loss: 0.044208. Entropy: 0.308427.\n",
      "Iteration 22719: Policy loss: -0.056039. Value loss: 0.032912. Entropy: 0.307077.\n",
      "episode: 7756   score: 405.0  epsilon: 1.0    steps: 496  evaluation reward: 466.9\n",
      "Training network. lr: 0.000076. clip: 0.030272\n",
      "Iteration 22720: Policy loss: 0.000748. Value loss: 0.315193. Entropy: 0.295861.\n",
      "Iteration 22721: Policy loss: 0.000027. Value loss: 0.135627. Entropy: 0.296310.\n",
      "Iteration 22722: Policy loss: 0.002016. Value loss: 0.059000. Entropy: 0.295331.\n",
      "episode: 7757   score: 385.0  epsilon: 1.0    steps: 104  evaluation reward: 466.8\n",
      "episode: 7758   score: 335.0  epsilon: 1.0    steps: 568  evaluation reward: 465.45\n",
      "Training network. lr: 0.000076. clip: 0.030272\n",
      "Iteration 22723: Policy loss: 0.135791. Value loss: 0.097352. Entropy: 0.279426.\n",
      "Iteration 22724: Policy loss: 0.133902. Value loss: 0.051497. Entropy: 0.279998.\n",
      "Iteration 22725: Policy loss: 0.132860. Value loss: 0.037709. Entropy: 0.280190.\n",
      "episode: 7759   score: 310.0  epsilon: 1.0    steps: 200  evaluation reward: 465.65\n",
      "episode: 7760   score: 435.0  epsilon: 1.0    steps: 568  evaluation reward: 464.45\n",
      "Training network. lr: 0.000076. clip: 0.030272\n",
      "Iteration 22726: Policy loss: 0.018264. Value loss: 0.077243. Entropy: 0.274103.\n",
      "Iteration 22727: Policy loss: 0.019841. Value loss: 0.045956. Entropy: 0.274379.\n",
      "Iteration 22728: Policy loss: 0.013467. Value loss: 0.039654. Entropy: 0.273370.\n",
      "Training network. lr: 0.000076. clip: 0.030272\n",
      "Iteration 22729: Policy loss: 0.120153. Value loss: 0.124009. Entropy: 0.304790.\n",
      "Iteration 22730: Policy loss: 0.118611. Value loss: 0.059958. Entropy: 0.304557.\n",
      "Iteration 22731: Policy loss: 0.118268. Value loss: 0.041995. Entropy: 0.305640.\n",
      "Training network. lr: 0.000076. clip: 0.030272\n",
      "Iteration 22732: Policy loss: 0.269592. Value loss: 0.172216. Entropy: 0.312583.\n",
      "Iteration 22733: Policy loss: 0.270675. Value loss: 0.051606. Entropy: 0.312390.\n",
      "Iteration 22734: Policy loss: 0.268503. Value loss: 0.037277. Entropy: 0.311855.\n",
      "episode: 7761   score: 775.0  epsilon: 1.0    steps: 232  evaluation reward: 470.0\n",
      "episode: 7762   score: 565.0  epsilon: 1.0    steps: 464  evaluation reward: 473.0\n",
      "Training network. lr: 0.000076. clip: 0.030272\n",
      "Iteration 22735: Policy loss: -0.273675. Value loss: 0.257131. Entropy: 0.281045.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22736: Policy loss: -0.274778. Value loss: 0.146103. Entropy: 0.282118.\n",
      "Iteration 22737: Policy loss: -0.272945. Value loss: 0.078506. Entropy: 0.280706.\n",
      "Training network. lr: 0.000076. clip: 0.030272\n",
      "Iteration 22738: Policy loss: 0.200230. Value loss: 0.118639. Entropy: 0.307982.\n",
      "Iteration 22739: Policy loss: 0.196631. Value loss: 0.044503. Entropy: 0.305373.\n",
      "Iteration 22740: Policy loss: 0.186230. Value loss: 0.033451. Entropy: 0.305510.\n",
      "Training network. lr: 0.000076. clip: 0.030272\n",
      "Iteration 22741: Policy loss: 0.172804. Value loss: 0.142185. Entropy: 0.303478.\n",
      "Iteration 22742: Policy loss: 0.164350. Value loss: 0.047363. Entropy: 0.302317.\n",
      "Iteration 22743: Policy loss: 0.155015. Value loss: 0.032252. Entropy: 0.301600.\n",
      "episode: 7763   score: 395.0  epsilon: 1.0    steps: 656  evaluation reward: 470.75\n",
      "Training network. lr: 0.000076. clip: 0.030272\n",
      "Iteration 22744: Policy loss: -0.008453. Value loss: 0.238316. Entropy: 0.299732.\n",
      "Iteration 22745: Policy loss: -0.031502. Value loss: 0.170922. Entropy: 0.298941.\n",
      "Iteration 22746: Policy loss: -0.035855. Value loss: 0.107243. Entropy: 0.300073.\n",
      "episode: 7764   score: 510.0  epsilon: 1.0    steps: 744  evaluation reward: 472.95\n",
      "episode: 7765   score: 350.0  epsilon: 1.0    steps: 912  evaluation reward: 471.15\n",
      "Training network. lr: 0.000076. clip: 0.030272\n",
      "Iteration 22747: Policy loss: 0.013076. Value loss: 0.144789. Entropy: 0.293909.\n",
      "Iteration 22748: Policy loss: 0.010292. Value loss: 0.058130. Entropy: 0.293785.\n",
      "Iteration 22749: Policy loss: 0.003625. Value loss: 0.042978. Entropy: 0.293856.\n",
      "episode: 7766   score: 510.0  epsilon: 1.0    steps: 880  evaluation reward: 472.85\n",
      "Training network. lr: 0.000076. clip: 0.030272\n",
      "Iteration 22750: Policy loss: 0.101124. Value loss: 0.123481. Entropy: 0.293762.\n",
      "Iteration 22751: Policy loss: 0.085224. Value loss: 0.059782. Entropy: 0.293058.\n",
      "Iteration 22752: Policy loss: 0.086929. Value loss: 0.044592. Entropy: 0.293144.\n",
      "Training network. lr: 0.000075. clip: 0.030115\n",
      "Iteration 22753: Policy loss: -0.110196. Value loss: 0.112345. Entropy: 0.299395.\n",
      "Iteration 22754: Policy loss: -0.111689. Value loss: 0.046897. Entropy: 0.299230.\n",
      "Iteration 22755: Policy loss: -0.114563. Value loss: 0.033966. Entropy: 0.299259.\n",
      "episode: 7767   score: 630.0  epsilon: 1.0    steps: 16  evaluation reward: 472.9\n",
      "episode: 7768   score: 790.0  epsilon: 1.0    steps: 712  evaluation reward: 477.35\n",
      "episode: 7769   score: 365.0  epsilon: 1.0    steps: 864  evaluation reward: 474.3\n",
      "Training network. lr: 0.000075. clip: 0.030115\n",
      "Iteration 22756: Policy loss: -0.263248. Value loss: 0.186508. Entropy: 0.279750.\n",
      "Iteration 22757: Policy loss: -0.283434. Value loss: 0.106151. Entropy: 0.279648.\n",
      "Iteration 22758: Policy loss: -0.287755. Value loss: 0.071060. Entropy: 0.280000.\n",
      "episode: 7770   score: 390.0  epsilon: 1.0    steps: 320  evaluation reward: 476.05\n",
      "Training network. lr: 0.000075. clip: 0.030115\n",
      "Iteration 22759: Policy loss: -0.290491. Value loss: 0.141105. Entropy: 0.290528.\n",
      "Iteration 22760: Policy loss: -0.296800. Value loss: 0.063702. Entropy: 0.290040.\n",
      "Iteration 22761: Policy loss: -0.306056. Value loss: 0.043389. Entropy: 0.290110.\n",
      "Training network. lr: 0.000075. clip: 0.030115\n",
      "Iteration 22762: Policy loss: -0.010784. Value loss: 0.241730. Entropy: 0.304400.\n",
      "Iteration 22763: Policy loss: -0.009582. Value loss: 0.109879. Entropy: 0.302343.\n",
      "Iteration 22764: Policy loss: -0.021448. Value loss: 0.071590. Entropy: 0.303293.\n",
      "Training network. lr: 0.000075. clip: 0.030115\n",
      "Iteration 22765: Policy loss: 0.006602. Value loss: 0.152178. Entropy: 0.304021.\n",
      "Iteration 22766: Policy loss: -0.009749. Value loss: 0.049565. Entropy: 0.305033.\n",
      "Iteration 22767: Policy loss: -0.008756. Value loss: 0.034484. Entropy: 0.305480.\n",
      "Training network. lr: 0.000075. clip: 0.030115\n",
      "Iteration 22768: Policy loss: 0.378922. Value loss: 0.421350. Entropy: 0.310553.\n",
      "Iteration 22769: Policy loss: 0.365106. Value loss: 0.209868. Entropy: 0.309896.\n",
      "Iteration 22770: Policy loss: 0.342247. Value loss: 0.126259. Entropy: 0.309843.\n",
      "Training network. lr: 0.000075. clip: 0.030115\n",
      "Iteration 22771: Policy loss: 0.255668. Value loss: 0.154506. Entropy: 0.309407.\n",
      "Iteration 22772: Policy loss: 0.249965. Value loss: 0.051956. Entropy: 0.308209.\n",
      "Iteration 22773: Policy loss: 0.249813. Value loss: 0.032247. Entropy: 0.309051.\n",
      "episode: 7771   score: 600.0  epsilon: 1.0    steps: 696  evaluation reward: 479.15\n",
      "Training network. lr: 0.000075. clip: 0.030115\n",
      "Iteration 22774: Policy loss: 0.002916. Value loss: 0.215142. Entropy: 0.288727.\n",
      "Iteration 22775: Policy loss: -0.005054. Value loss: 0.079752. Entropy: 0.287992.\n",
      "Iteration 22776: Policy loss: -0.008991. Value loss: 0.055476. Entropy: 0.290510.\n",
      "episode: 7772   score: 390.0  epsilon: 1.0    steps: 120  evaluation reward: 477.35\n",
      "episode: 7773   score: 450.0  epsilon: 1.0    steps: 624  evaluation reward: 477.8\n",
      "episode: 7774   score: 585.0  epsilon: 1.0    steps: 912  evaluation reward: 479.8\n",
      "Training network. lr: 0.000075. clip: 0.030115\n",
      "Iteration 22777: Policy loss: 0.056916. Value loss: 0.146278. Entropy: 0.279909.\n",
      "Iteration 22778: Policy loss: 0.048085. Value loss: 0.068713. Entropy: 0.278168.\n",
      "Iteration 22779: Policy loss: 0.051642. Value loss: 0.046829. Entropy: 0.278440.\n",
      "episode: 7775   score: 605.0  epsilon: 1.0    steps: 944  evaluation reward: 482.55\n",
      "Training network. lr: 0.000075. clip: 0.030115\n",
      "Iteration 22780: Policy loss: 0.004997. Value loss: 0.061028. Entropy: 0.298725.\n",
      "Iteration 22781: Policy loss: 0.002836. Value loss: 0.032556. Entropy: 0.298167.\n",
      "Iteration 22782: Policy loss: -0.003208. Value loss: 0.025741. Entropy: 0.298858.\n",
      "Training network. lr: 0.000075. clip: 0.030115\n",
      "Iteration 22783: Policy loss: -0.072743. Value loss: 0.355777. Entropy: 0.294972.\n",
      "Iteration 22784: Policy loss: -0.081541. Value loss: 0.226833. Entropy: 0.292422.\n",
      "Iteration 22785: Policy loss: -0.092895. Value loss: 0.168340. Entropy: 0.293028.\n",
      "Training network. lr: 0.000075. clip: 0.030115\n",
      "Iteration 22786: Policy loss: -0.261859. Value loss: 0.374764. Entropy: 0.305968.\n",
      "Iteration 22787: Policy loss: -0.282730. Value loss: 0.156705. Entropy: 0.305745.\n",
      "Iteration 22788: Policy loss: -0.278175. Value loss: 0.090834. Entropy: 0.305752.\n",
      "episode: 7776   score: 455.0  epsilon: 1.0    steps: 248  evaluation reward: 484.95\n",
      "Training network. lr: 0.000075. clip: 0.030115\n",
      "Iteration 22789: Policy loss: 0.095554. Value loss: 0.111355. Entropy: 0.298435.\n",
      "Iteration 22790: Policy loss: 0.091648. Value loss: 0.052172. Entropy: 0.299068.\n",
      "Iteration 22791: Policy loss: 0.090908. Value loss: 0.041216. Entropy: 0.298256.\n",
      "Training network. lr: 0.000075. clip: 0.030115\n",
      "Iteration 22792: Policy loss: -0.006391. Value loss: 0.170038. Entropy: 0.303458.\n",
      "Iteration 22793: Policy loss: -0.005731. Value loss: 0.087490. Entropy: 0.303053.\n",
      "Iteration 22794: Policy loss: -0.010005. Value loss: 0.063982. Entropy: 0.302060.\n",
      "Training network. lr: 0.000075. clip: 0.030115\n",
      "Iteration 22795: Policy loss: -0.388861. Value loss: 0.397962. Entropy: 0.305715.\n",
      "Iteration 22796: Policy loss: -0.407601. Value loss: 0.233222. Entropy: 0.306509.\n",
      "Iteration 22797: Policy loss: -0.412390. Value loss: 0.168559. Entropy: 0.307074.\n",
      "episode: 7777   score: 940.0  epsilon: 1.0    steps: 704  evaluation reward: 488.4\n",
      "episode: 7778   score: 805.0  epsilon: 1.0    steps: 1016  evaluation reward: 492.8\n",
      "Training network. lr: 0.000075. clip: 0.030115\n",
      "Iteration 22798: Policy loss: 0.220525. Value loss: 0.206253. Entropy: 0.294859.\n",
      "Iteration 22799: Policy loss: 0.220870. Value loss: 0.085905. Entropy: 0.295149.\n",
      "Iteration 22800: Policy loss: 0.206504. Value loss: 0.064710. Entropy: 0.294459.\n",
      "episode: 7779   score: 475.0  epsilon: 1.0    steps: 240  evaluation reward: 493.8\n",
      "Training network. lr: 0.000075. clip: 0.029968\n",
      "Iteration 22801: Policy loss: -0.205414. Value loss: 0.216096. Entropy: 0.283175.\n",
      "Iteration 22802: Policy loss: -0.219668. Value loss: 0.117532. Entropy: 0.281841.\n",
      "Iteration 22803: Policy loss: -0.225296. Value loss: 0.085914. Entropy: 0.281848.\n",
      "Training network. lr: 0.000075. clip: 0.029968\n",
      "Iteration 22804: Policy loss: 0.353633. Value loss: 0.190210. Entropy: 0.308266.\n",
      "Iteration 22805: Policy loss: 0.355179. Value loss: 0.101770. Entropy: 0.308203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22806: Policy loss: 0.354055. Value loss: 0.070472. Entropy: 0.308377.\n",
      "episode: 7780   score: 570.0  epsilon: 1.0    steps: 496  evaluation reward: 491.75\n",
      "Training network. lr: 0.000075. clip: 0.029968\n",
      "Iteration 22807: Policy loss: 0.055858. Value loss: 0.262876. Entropy: 0.291669.\n",
      "Iteration 22808: Policy loss: 0.059353. Value loss: 0.118567. Entropy: 0.291329.\n",
      "Iteration 22809: Policy loss: 0.049906. Value loss: 0.072062. Entropy: 0.290787.\n",
      "episode: 7781   score: 150.0  epsilon: 1.0    steps: 704  evaluation reward: 490.55\n",
      "Training network. lr: 0.000075. clip: 0.029968\n",
      "Iteration 22810: Policy loss: 0.218836. Value loss: 0.174711. Entropy: 0.294385.\n",
      "Iteration 22811: Policy loss: 0.215007. Value loss: 0.079858. Entropy: 0.294248.\n",
      "Iteration 22812: Policy loss: 0.210963. Value loss: 0.052327. Entropy: 0.293644.\n",
      "episode: 7782   score: 910.0  epsilon: 1.0    steps: 248  evaluation reward: 494.25\n",
      "episode: 7783   score: 420.0  epsilon: 1.0    steps: 272  evaluation reward: 492.5\n",
      "episode: 7784   score: 660.0  epsilon: 1.0    steps: 840  evaluation reward: 494.9\n",
      "episode: 7785   score: 800.0  epsilon: 1.0    steps: 880  evaluation reward: 493.6\n",
      "Training network. lr: 0.000075. clip: 0.029968\n",
      "Iteration 22813: Policy loss: -0.176795. Value loss: 0.322668. Entropy: 0.268285.\n",
      "Iteration 22814: Policy loss: -0.179511. Value loss: 0.223709. Entropy: 0.270272.\n",
      "Iteration 22815: Policy loss: -0.197583. Value loss: 0.187834. Entropy: 0.268368.\n",
      "Training network. lr: 0.000075. clip: 0.029968\n",
      "Iteration 22816: Policy loss: -0.134700. Value loss: 0.175331. Entropy: 0.301331.\n",
      "Iteration 22817: Policy loss: -0.146484. Value loss: 0.075472. Entropy: 0.301269.\n",
      "Iteration 22818: Policy loss: -0.147813. Value loss: 0.048597. Entropy: 0.302291.\n",
      "Training network. lr: 0.000075. clip: 0.029968\n",
      "Iteration 22819: Policy loss: -0.156458. Value loss: 0.305104. Entropy: 0.305229.\n",
      "Iteration 22820: Policy loss: -0.147068. Value loss: 0.151334. Entropy: 0.306905.\n",
      "Iteration 22821: Policy loss: -0.158599. Value loss: 0.099530. Entropy: 0.307593.\n",
      "episode: 7786   score: 485.0  epsilon: 1.0    steps: 64  evaluation reward: 494.6\n",
      "Training network. lr: 0.000075. clip: 0.029968\n",
      "Iteration 22822: Policy loss: 0.084291. Value loss: 0.354538. Entropy: 0.290575.\n",
      "Iteration 22823: Policy loss: 0.096784. Value loss: 0.180540. Entropy: 0.289731.\n",
      "Iteration 22824: Policy loss: 0.078144. Value loss: 0.128392. Entropy: 0.290048.\n",
      "Training network. lr: 0.000075. clip: 0.029968\n",
      "Iteration 22825: Policy loss: 0.199300. Value loss: 0.173504. Entropy: 0.307047.\n",
      "Iteration 22826: Policy loss: 0.196069. Value loss: 0.081962. Entropy: 0.306609.\n",
      "Iteration 22827: Policy loss: 0.192406. Value loss: 0.065029. Entropy: 0.304961.\n",
      "Training network. lr: 0.000075. clip: 0.029968\n",
      "Iteration 22828: Policy loss: -0.146984. Value loss: 0.276087. Entropy: 0.310665.\n",
      "Iteration 22829: Policy loss: -0.159182. Value loss: 0.199989. Entropy: 0.311370.\n",
      "Iteration 22830: Policy loss: -0.152840. Value loss: 0.164257. Entropy: 0.310803.\n",
      "episode: 7787   score: 635.0  epsilon: 1.0    steps: 488  evaluation reward: 495.05\n",
      "episode: 7788   score: 330.0  epsilon: 1.0    steps: 976  evaluation reward: 494.7\n",
      "Training network. lr: 0.000075. clip: 0.029968\n",
      "Iteration 22831: Policy loss: 0.178880. Value loss: 0.362763. Entropy: 0.296018.\n",
      "Iteration 22832: Policy loss: 0.187457. Value loss: 0.111490. Entropy: 0.294984.\n",
      "Iteration 22833: Policy loss: 0.166501. Value loss: 0.075256. Entropy: 0.294910.\n",
      "episode: 7789   score: 515.0  epsilon: 1.0    steps: 704  evaluation reward: 494.25\n",
      "episode: 7790   score: 380.0  epsilon: 1.0    steps: 936  evaluation reward: 495.2\n",
      "Training network. lr: 0.000075. clip: 0.029968\n",
      "Iteration 22834: Policy loss: 0.347676. Value loss: 0.148601. Entropy: 0.282557.\n",
      "Iteration 22835: Policy loss: 0.341984. Value loss: 0.055739. Entropy: 0.281989.\n",
      "Iteration 22836: Policy loss: 0.337165. Value loss: 0.038084. Entropy: 0.282133.\n",
      "Training network. lr: 0.000075. clip: 0.029968\n",
      "Iteration 22837: Policy loss: 0.004767. Value loss: 0.099722. Entropy: 0.300965.\n",
      "Iteration 22838: Policy loss: -0.001655. Value loss: 0.050829. Entropy: 0.300041.\n",
      "Iteration 22839: Policy loss: -0.003973. Value loss: 0.035435. Entropy: 0.301552.\n",
      "episode: 7791   score: 800.0  epsilon: 1.0    steps: 664  evaluation reward: 496.85\n",
      "Training network. lr: 0.000075. clip: 0.029968\n",
      "Iteration 22840: Policy loss: -0.123895. Value loss: 0.241969. Entropy: 0.288006.\n",
      "Iteration 22841: Policy loss: -0.135638. Value loss: 0.146733. Entropy: 0.288667.\n",
      "Iteration 22842: Policy loss: -0.124909. Value loss: 0.096829. Entropy: 0.288025.\n",
      "Training network. lr: 0.000075. clip: 0.029968\n",
      "Iteration 22843: Policy loss: -0.022889. Value loss: 0.172408. Entropy: 0.311575.\n",
      "Iteration 22844: Policy loss: -0.027277. Value loss: 0.070429. Entropy: 0.311652.\n",
      "Iteration 22845: Policy loss: -0.035857. Value loss: 0.040273. Entropy: 0.312231.\n",
      "episode: 7792   score: 610.0  epsilon: 1.0    steps: 96  evaluation reward: 497.5\n",
      "Training network. lr: 0.000075. clip: 0.029968\n",
      "Iteration 22846: Policy loss: -0.098120. Value loss: 0.303674. Entropy: 0.291087.\n",
      "Iteration 22847: Policy loss: -0.101300. Value loss: 0.152189. Entropy: 0.291230.\n",
      "Iteration 22848: Policy loss: -0.097763. Value loss: 0.101936. Entropy: 0.291115.\n",
      "Training network. lr: 0.000075. clip: 0.029968\n",
      "Iteration 22849: Policy loss: -0.001168. Value loss: 0.304071. Entropy: 0.303212.\n",
      "Iteration 22850: Policy loss: 0.007591. Value loss: 0.181507. Entropy: 0.301474.\n",
      "Iteration 22851: Policy loss: -0.002086. Value loss: 0.146500. Entropy: 0.302428.\n",
      "episode: 7793   score: 725.0  epsilon: 1.0    steps: 608  evaluation reward: 501.15\n",
      "episode: 7794   score: 440.0  epsilon: 1.0    steps: 960  evaluation reward: 500.35\n",
      "Training network. lr: 0.000075. clip: 0.029811\n",
      "Iteration 22852: Policy loss: -0.044138. Value loss: 0.163621. Entropy: 0.292894.\n",
      "Iteration 22853: Policy loss: -0.045880. Value loss: 0.063630. Entropy: 0.293632.\n",
      "Iteration 22854: Policy loss: -0.055903. Value loss: 0.045134. Entropy: 0.294073.\n",
      "episode: 7795   score: 545.0  epsilon: 1.0    steps: 832  evaluation reward: 499.35\n",
      "Training network. lr: 0.000075. clip: 0.029811\n",
      "Iteration 22855: Policy loss: 0.258515. Value loss: 0.154926. Entropy: 0.288809.\n",
      "Iteration 22856: Policy loss: 0.244943. Value loss: 0.064133. Entropy: 0.290313.\n",
      "Iteration 22857: Policy loss: 0.253543. Value loss: 0.042558. Entropy: 0.289352.\n",
      "episode: 7796   score: 615.0  epsilon: 1.0    steps: 896  evaluation reward: 502.4\n",
      "Training network. lr: 0.000075. clip: 0.029811\n",
      "Iteration 22858: Policy loss: -0.831639. Value loss: 0.605341. Entropy: 0.294569.\n",
      "Iteration 22859: Policy loss: -0.870017. Value loss: 0.354354. Entropy: 0.295419.\n",
      "Iteration 22860: Policy loss: -0.882257. Value loss: 0.244463. Entropy: 0.295122.\n",
      "Training network. lr: 0.000075. clip: 0.029811\n",
      "Iteration 22861: Policy loss: 0.234701. Value loss: 0.192823. Entropy: 0.305483.\n",
      "Iteration 22862: Policy loss: 0.231357. Value loss: 0.071901. Entropy: 0.303666.\n",
      "Iteration 22863: Policy loss: 0.215016. Value loss: 0.051395. Entropy: 0.303971.\n",
      "Training network. lr: 0.000075. clip: 0.029811\n",
      "Iteration 22864: Policy loss: 0.347061. Value loss: 0.237731. Entropy: 0.304891.\n",
      "Iteration 22865: Policy loss: 0.334068. Value loss: 0.094317. Entropy: 0.304598.\n",
      "Iteration 22866: Policy loss: 0.331691. Value loss: 0.063830. Entropy: 0.305349.\n",
      "episode: 7797   score: 975.0  epsilon: 1.0    steps: 336  evaluation reward: 508.05\n",
      "episode: 7798   score: 345.0  epsilon: 1.0    steps: 952  evaluation reward: 506.9\n",
      "Training network. lr: 0.000075. clip: 0.029811\n",
      "Iteration 22867: Policy loss: -0.036032. Value loss: 0.340659. Entropy: 0.293817.\n",
      "Iteration 22868: Policy loss: -0.024517. Value loss: 0.188117. Entropy: 0.293073.\n",
      "Iteration 22869: Policy loss: -0.032889. Value loss: 0.160571. Entropy: 0.292943.\n",
      "episode: 7799   score: 890.0  epsilon: 1.0    steps: 88  evaluation reward: 511.1\n",
      "episode: 7800   score: 525.0  epsilon: 1.0    steps: 400  evaluation reward: 512.15\n",
      "Training network. lr: 0.000075. clip: 0.029811\n",
      "Iteration 22870: Policy loss: -0.019044. Value loss: 0.309846. Entropy: 0.270808.\n",
      "Iteration 22871: Policy loss: -0.024705. Value loss: 0.236276. Entropy: 0.272773.\n",
      "Iteration 22872: Policy loss: -0.002782. Value loss: 0.196574. Entropy: 0.272235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now time :  2019-09-06 13:49:29.228734\n",
      "episode: 7801   score: 225.0  epsilon: 1.0    steps: 768  evaluation reward: 510.95\n",
      "episode: 7802   score: 530.0  epsilon: 1.0    steps: 832  evaluation reward: 509.3\n",
      "Training network. lr: 0.000075. clip: 0.029811\n",
      "Iteration 22873: Policy loss: 0.305242. Value loss: 0.157610. Entropy: 0.293221.\n",
      "Iteration 22874: Policy loss: 0.299539. Value loss: 0.074100. Entropy: 0.293112.\n",
      "Iteration 22875: Policy loss: 0.308211. Value loss: 0.054562. Entropy: 0.291987.\n",
      "episode: 7803   score: 210.0  epsilon: 1.0    steps: 648  evaluation reward: 506.9\n",
      "Training network. lr: 0.000075. clip: 0.029811\n",
      "Iteration 22876: Policy loss: 0.013163. Value loss: 0.273743. Entropy: 0.288004.\n",
      "Iteration 22877: Policy loss: 0.007829. Value loss: 0.135571. Entropy: 0.288527.\n",
      "Iteration 22878: Policy loss: -0.003242. Value loss: 0.098679. Entropy: 0.287020.\n",
      "Training network. lr: 0.000075. clip: 0.029811\n",
      "Iteration 22879: Policy loss: 0.061742. Value loss: 0.086399. Entropy: 0.308235.\n",
      "Iteration 22880: Policy loss: 0.058111. Value loss: 0.035470. Entropy: 0.307549.\n",
      "Iteration 22881: Policy loss: 0.053664. Value loss: 0.024636. Entropy: 0.308105.\n",
      "Training network. lr: 0.000075. clip: 0.029811\n",
      "Iteration 22882: Policy loss: 0.003293. Value loss: 0.130711. Entropy: 0.310373.\n",
      "Iteration 22883: Policy loss: -0.000355. Value loss: 0.049773. Entropy: 0.310170.\n",
      "Iteration 22884: Policy loss: -0.014392. Value loss: 0.030344. Entropy: 0.311383.\n",
      "Training network. lr: 0.000075. clip: 0.029811\n",
      "Iteration 22885: Policy loss: 0.361675. Value loss: 0.337448. Entropy: 0.313622.\n",
      "Iteration 22886: Policy loss: 0.362891. Value loss: 0.075415. Entropy: 0.312494.\n",
      "Iteration 22887: Policy loss: 0.344255. Value loss: 0.038647. Entropy: 0.312613.\n",
      "episode: 7804   score: 610.0  epsilon: 1.0    steps: 912  evaluation reward: 508.05\n",
      "episode: 7805   score: 290.0  epsilon: 1.0    steps: 944  evaluation reward: 508.85\n",
      "Training network. lr: 0.000075. clip: 0.029811\n",
      "Iteration 22888: Policy loss: 0.042554. Value loss: 0.209678. Entropy: 0.301378.\n",
      "Iteration 22889: Policy loss: 0.051386. Value loss: 0.080732. Entropy: 0.300167.\n",
      "Iteration 22890: Policy loss: 0.038387. Value loss: 0.056720. Entropy: 0.301857.\n",
      "episode: 7806   score: 315.0  epsilon: 1.0    steps: 136  evaluation reward: 504.55\n",
      "Training network. lr: 0.000075. clip: 0.029811\n",
      "Iteration 22891: Policy loss: 0.057863. Value loss: 0.100391. Entropy: 0.284021.\n",
      "Iteration 22892: Policy loss: 0.056431. Value loss: 0.049127. Entropy: 0.284064.\n",
      "Iteration 22893: Policy loss: 0.052636. Value loss: 0.036013. Entropy: 0.283685.\n",
      "episode: 7807   score: 315.0  epsilon: 1.0    steps: 768  evaluation reward: 504.55\n",
      "Training network. lr: 0.000075. clip: 0.029811\n",
      "Iteration 22894: Policy loss: 0.079929. Value loss: 0.129110. Entropy: 0.294644.\n",
      "Iteration 22895: Policy loss: 0.074093. Value loss: 0.072776. Entropy: 0.294191.\n",
      "Iteration 22896: Policy loss: 0.070947. Value loss: 0.044264. Entropy: 0.295412.\n",
      "episode: 7808   score: 395.0  epsilon: 1.0    steps: 864  evaluation reward: 504.85\n",
      "Training network. lr: 0.000075. clip: 0.029811\n",
      "Iteration 22897: Policy loss: -0.120542. Value loss: 0.315457. Entropy: 0.305006.\n",
      "Iteration 22898: Policy loss: -0.114721. Value loss: 0.105229. Entropy: 0.305231.\n",
      "Iteration 22899: Policy loss: -0.107742. Value loss: 0.063757. Entropy: 0.304882.\n",
      "episode: 7809   score: 390.0  epsilon: 1.0    steps: 632  evaluation reward: 505.4\n",
      "Training network. lr: 0.000075. clip: 0.029811\n",
      "Iteration 22900: Policy loss: 0.268690. Value loss: 0.118539. Entropy: 0.297552.\n",
      "Iteration 22901: Policy loss: 0.262024. Value loss: 0.062628. Entropy: 0.297479.\n",
      "Iteration 22902: Policy loss: 0.261807. Value loss: 0.047835. Entropy: 0.295928.\n",
      "Training network. lr: 0.000074. clip: 0.029654\n",
      "Iteration 22903: Policy loss: 0.132947. Value loss: 0.113208. Entropy: 0.304764.\n",
      "Iteration 22904: Policy loss: 0.133210. Value loss: 0.053699. Entropy: 0.303733.\n",
      "Iteration 22905: Policy loss: 0.126854. Value loss: 0.034997. Entropy: 0.304116.\n",
      "episode: 7810   score: 800.0  epsilon: 1.0    steps: 448  evaluation reward: 510.75\n",
      "Training network. lr: 0.000074. clip: 0.029654\n",
      "Iteration 22906: Policy loss: -0.363028. Value loss: 0.355871. Entropy: 0.288807.\n",
      "Iteration 22907: Policy loss: -0.380077. Value loss: 0.233991. Entropy: 0.289959.\n",
      "Iteration 22908: Policy loss: -0.397605. Value loss: 0.171107. Entropy: 0.288732.\n",
      "Training network. lr: 0.000074. clip: 0.029654\n",
      "Iteration 22909: Policy loss: 0.095602. Value loss: 0.145184. Entropy: 0.318214.\n",
      "Iteration 22910: Policy loss: 0.090014. Value loss: 0.058044. Entropy: 0.317389.\n",
      "Iteration 22911: Policy loss: 0.078239. Value loss: 0.036477. Entropy: 0.317200.\n",
      "episode: 7811   score: 800.0  epsilon: 1.0    steps: 152  evaluation reward: 513.9\n",
      "episode: 7812   score: 215.0  epsilon: 1.0    steps: 240  evaluation reward: 509.35\n",
      "episode: 7813   score: 565.0  epsilon: 1.0    steps: 512  evaluation reward: 511.55\n",
      "Training network. lr: 0.000074. clip: 0.029654\n",
      "Iteration 22912: Policy loss: 0.243448. Value loss: 0.208633. Entropy: 0.272672.\n",
      "Iteration 22913: Policy loss: 0.232218. Value loss: 0.093764. Entropy: 0.271079.\n",
      "Iteration 22914: Policy loss: 0.227434. Value loss: 0.062818. Entropy: 0.271886.\n",
      "Training network. lr: 0.000074. clip: 0.029654\n",
      "Iteration 22915: Policy loss: -0.078756. Value loss: 0.179188. Entropy: 0.309224.\n",
      "Iteration 22916: Policy loss: -0.089227. Value loss: 0.075223. Entropy: 0.309124.\n",
      "Iteration 22917: Policy loss: -0.086858. Value loss: 0.049043. Entropy: 0.309132.\n",
      "Training network. lr: 0.000074. clip: 0.029654\n",
      "Iteration 22918: Policy loss: -0.120747. Value loss: 0.189876. Entropy: 0.309190.\n",
      "Iteration 22919: Policy loss: -0.116436. Value loss: 0.065642. Entropy: 0.308652.\n",
      "Iteration 22920: Policy loss: -0.130566. Value loss: 0.044911. Entropy: 0.309529.\n",
      "episode: 7814   score: 345.0  epsilon: 1.0    steps: 272  evaluation reward: 511.65\n",
      "episode: 7815   score: 695.0  epsilon: 1.0    steps: 672  evaluation reward: 515.15\n",
      "Training network. lr: 0.000074. clip: 0.029654\n",
      "Iteration 22921: Policy loss: -0.121667. Value loss: 0.459186. Entropy: 0.281286.\n",
      "Iteration 22922: Policy loss: -0.099770. Value loss: 0.222339. Entropy: 0.281615.\n",
      "Iteration 22923: Policy loss: -0.125166. Value loss: 0.184105. Entropy: 0.280677.\n",
      "episode: 7816   score: 380.0  epsilon: 1.0    steps: 616  evaluation reward: 512.6\n",
      "Training network. lr: 0.000074. clip: 0.029654\n",
      "Iteration 22924: Policy loss: -0.020525. Value loss: 0.123706. Entropy: 0.294816.\n",
      "Iteration 22925: Policy loss: -0.020326. Value loss: 0.044689. Entropy: 0.292963.\n",
      "Iteration 22926: Policy loss: -0.022267. Value loss: 0.031082. Entropy: 0.293949.\n",
      "Training network. lr: 0.000074. clip: 0.029654\n",
      "Iteration 22927: Policy loss: -0.354646. Value loss: 0.324146. Entropy: 0.307631.\n",
      "Iteration 22928: Policy loss: -0.350129. Value loss: 0.116526. Entropy: 0.307359.\n",
      "Iteration 22929: Policy loss: -0.351739. Value loss: 0.068677. Entropy: 0.307096.\n",
      "Training network. lr: 0.000074. clip: 0.029654\n",
      "Iteration 22930: Policy loss: 0.070111. Value loss: 0.535922. Entropy: 0.308229.\n",
      "Iteration 22931: Policy loss: 0.049219. Value loss: 0.246989. Entropy: 0.309437.\n",
      "Iteration 22932: Policy loss: 0.050866. Value loss: 0.153062. Entropy: 0.306968.\n",
      "episode: 7817   score: 515.0  epsilon: 1.0    steps: 8  evaluation reward: 512.85\n",
      "episode: 7818   score: 650.0  epsilon: 1.0    steps: 552  evaluation reward: 514.25\n",
      "Training network. lr: 0.000074. clip: 0.029654\n",
      "Iteration 22933: Policy loss: 0.425534. Value loss: 0.190311. Entropy: 0.285538.\n",
      "Iteration 22934: Policy loss: 0.417717. Value loss: 0.077813. Entropy: 0.285848.\n",
      "Iteration 22935: Policy loss: 0.409735. Value loss: 0.054650. Entropy: 0.285544.\n",
      "Training network. lr: 0.000074. clip: 0.029654\n",
      "Iteration 22936: Policy loss: -0.018436. Value loss: 0.073158. Entropy: 0.309172.\n",
      "Iteration 22937: Policy loss: -0.015465. Value loss: 0.032429. Entropy: 0.309243.\n",
      "Iteration 22938: Policy loss: -0.016611. Value loss: 0.023648. Entropy: 0.310180.\n",
      "Training network. lr: 0.000074. clip: 0.029654\n",
      "Iteration 22939: Policy loss: -0.030351. Value loss: 0.137681. Entropy: 0.313730.\n",
      "Iteration 22940: Policy loss: -0.018653. Value loss: 0.068119. Entropy: 0.315526.\n",
      "Iteration 22941: Policy loss: -0.029838. Value loss: 0.047539. Entropy: 0.314621.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7819   score: 435.0  epsilon: 1.0    steps: 232  evaluation reward: 515.45\n",
      "episode: 7820   score: 835.0  epsilon: 1.0    steps: 752  evaluation reward: 514.05\n",
      "episode: 7821   score: 285.0  epsilon: 1.0    steps: 808  evaluation reward: 512.2\n",
      "Training network. lr: 0.000074. clip: 0.029654\n",
      "Iteration 22942: Policy loss: -0.211149. Value loss: 0.531657. Entropy: 0.277918.\n",
      "Iteration 22943: Policy loss: -0.217743. Value loss: 0.391250. Entropy: 0.278960.\n",
      "Iteration 22944: Policy loss: -0.233529. Value loss: 0.350746. Entropy: 0.278497.\n",
      "episode: 7822   score: 475.0  epsilon: 1.0    steps: 192  evaluation reward: 515.1\n",
      "Training network. lr: 0.000074. clip: 0.029654\n",
      "Iteration 22945: Policy loss: 0.014326. Value loss: 0.058199. Entropy: 0.293591.\n",
      "Iteration 22946: Policy loss: 0.011831. Value loss: 0.033789. Entropy: 0.292970.\n",
      "Iteration 22947: Policy loss: 0.010501. Value loss: 0.026670. Entropy: 0.293031.\n",
      "Training network. lr: 0.000074. clip: 0.029654\n",
      "Iteration 22948: Policy loss: 0.026817. Value loss: 0.169092. Entropy: 0.307334.\n",
      "Iteration 22949: Policy loss: 0.021094. Value loss: 0.074350. Entropy: 0.306776.\n",
      "Iteration 22950: Policy loss: 0.014272. Value loss: 0.050676. Entropy: 0.307043.\n",
      "Training network. lr: 0.000074. clip: 0.029507\n",
      "Iteration 22951: Policy loss: -0.393763. Value loss: 0.502740. Entropy: 0.307624.\n",
      "Iteration 22952: Policy loss: -0.389426. Value loss: 0.332768. Entropy: 0.307330.\n",
      "Iteration 22953: Policy loss: -0.413514. Value loss: 0.276880. Entropy: 0.306698.\n",
      "episode: 7823   score: 755.0  epsilon: 1.0    steps: 8  evaluation reward: 518.3\n",
      "episode: 7824   score: 535.0  epsilon: 1.0    steps: 600  evaluation reward: 518.05\n",
      "episode: 7825   score: 315.0  epsilon: 1.0    steps: 624  evaluation reward: 514.9\n",
      "Training network. lr: 0.000074. clip: 0.029507\n",
      "Iteration 22954: Policy loss: 0.177514. Value loss: 0.150363. Entropy: 0.268014.\n",
      "Iteration 22955: Policy loss: 0.178834. Value loss: 0.083619. Entropy: 0.265438.\n",
      "Iteration 22956: Policy loss: 0.178129. Value loss: 0.066559. Entropy: 0.266088.\n",
      "Training network. lr: 0.000074. clip: 0.029507\n",
      "Iteration 22957: Policy loss: 0.092324. Value loss: 0.122073. Entropy: 0.312108.\n",
      "Iteration 22958: Policy loss: 0.091380. Value loss: 0.072483. Entropy: 0.312363.\n",
      "Iteration 22959: Policy loss: 0.087393. Value loss: 0.052411. Entropy: 0.313219.\n",
      "Training network. lr: 0.000074. clip: 0.029507\n",
      "Iteration 22960: Policy loss: 0.074538. Value loss: 0.212042. Entropy: 0.307949.\n",
      "Iteration 22961: Policy loss: 0.068560. Value loss: 0.089094. Entropy: 0.307572.\n",
      "Iteration 22962: Policy loss: 0.064369. Value loss: 0.056901. Entropy: 0.306362.\n",
      "episode: 7826   score: 530.0  epsilon: 1.0    steps: 424  evaluation reward: 513.9\n",
      "episode: 7827   score: 315.0  epsilon: 1.0    steps: 480  evaluation reward: 513.6\n",
      "episode: 7828   score: 285.0  epsilon: 1.0    steps: 848  evaluation reward: 512.25\n",
      "Training network. lr: 0.000074. clip: 0.029507\n",
      "Iteration 22963: Policy loss: 0.336672. Value loss: 0.144053. Entropy: 0.276656.\n",
      "Iteration 22964: Policy loss: 0.332596. Value loss: 0.065215. Entropy: 0.275332.\n",
      "Iteration 22965: Policy loss: 0.331785. Value loss: 0.046298. Entropy: 0.275526.\n",
      "Training network. lr: 0.000074. clip: 0.029507\n",
      "Iteration 22966: Policy loss: 0.066985. Value loss: 0.091873. Entropy: 0.307110.\n",
      "Iteration 22967: Policy loss: 0.069351. Value loss: 0.043104. Entropy: 0.307343.\n",
      "Iteration 22968: Policy loss: 0.063422. Value loss: 0.035988. Entropy: 0.307461.\n",
      "Training network. lr: 0.000074. clip: 0.029507\n",
      "Iteration 22969: Policy loss: -0.031034. Value loss: 0.129013. Entropy: 0.303357.\n",
      "Iteration 22970: Policy loss: -0.036399. Value loss: 0.066856. Entropy: 0.302456.\n",
      "Iteration 22971: Policy loss: -0.041263. Value loss: 0.046109. Entropy: 0.303501.\n",
      "Training network. lr: 0.000074. clip: 0.029507\n",
      "Iteration 22972: Policy loss: -0.208606. Value loss: 0.370057. Entropy: 0.309616.\n",
      "Iteration 22973: Policy loss: -0.197881. Value loss: 0.199849. Entropy: 0.309793.\n",
      "Iteration 22974: Policy loss: -0.223384. Value loss: 0.136855. Entropy: 0.310237.\n",
      "episode: 7829   score: 285.0  epsilon: 1.0    steps: 80  evaluation reward: 509.45\n",
      "episode: 7830   score: 830.0  epsilon: 1.0    steps: 656  evaluation reward: 511.95\n",
      "episode: 7831   score: 345.0  epsilon: 1.0    steps: 968  evaluation reward: 512.7\n",
      "Training network. lr: 0.000074. clip: 0.029507\n",
      "Iteration 22975: Policy loss: 0.192246. Value loss: 0.111520. Entropy: 0.283540.\n",
      "Iteration 22976: Policy loss: 0.192619. Value loss: 0.060469. Entropy: 0.282614.\n",
      "Iteration 22977: Policy loss: 0.186897. Value loss: 0.048151. Entropy: 0.282109.\n",
      "episode: 7832   score: 440.0  epsilon: 1.0    steps: 56  evaluation reward: 513.3\n",
      "Training network. lr: 0.000074. clip: 0.029507\n",
      "Iteration 22978: Policy loss: 0.127312. Value loss: 0.126979. Entropy: 0.286360.\n",
      "Iteration 22979: Policy loss: 0.126857. Value loss: 0.058482. Entropy: 0.285839.\n",
      "Iteration 22980: Policy loss: 0.122231. Value loss: 0.038282. Entropy: 0.285896.\n",
      "Training network. lr: 0.000074. clip: 0.029507\n",
      "Iteration 22981: Policy loss: 0.334684. Value loss: 0.250093. Entropy: 0.309891.\n",
      "Iteration 22982: Policy loss: 0.325713. Value loss: 0.108064. Entropy: 0.309551.\n",
      "Iteration 22983: Policy loss: 0.316731. Value loss: 0.071826. Entropy: 0.308926.\n",
      "episode: 7833   score: 550.0  epsilon: 1.0    steps: 288  evaluation reward: 515.15\n",
      "episode: 7834   score: 325.0  epsilon: 1.0    steps: 712  evaluation reward: 512.45\n",
      "episode: 7835   score: 290.0  epsilon: 1.0    steps: 744  evaluation reward: 510.65\n",
      "Training network. lr: 0.000074. clip: 0.029507\n",
      "Iteration 22984: Policy loss: 0.211094. Value loss: 0.120556. Entropy: 0.267627.\n",
      "Iteration 22985: Policy loss: 0.213261. Value loss: 0.060752. Entropy: 0.266838.\n",
      "Iteration 22986: Policy loss: 0.209557. Value loss: 0.049161. Entropy: 0.267284.\n",
      "episode: 7836   score: 565.0  epsilon: 1.0    steps: 960  evaluation reward: 512.6\n",
      "Training network. lr: 0.000074. clip: 0.029507\n",
      "Iteration 22987: Policy loss: -0.078203. Value loss: 0.102747. Entropy: 0.310137.\n",
      "Iteration 22988: Policy loss: -0.084679. Value loss: 0.048039. Entropy: 0.309807.\n",
      "Iteration 22989: Policy loss: -0.077501. Value loss: 0.036093. Entropy: 0.310104.\n",
      "Training network. lr: 0.000074. clip: 0.029507\n",
      "Iteration 22990: Policy loss: 0.075434. Value loss: 0.095103. Entropy: 0.299815.\n",
      "Iteration 22991: Policy loss: 0.069410. Value loss: 0.058041. Entropy: 0.300792.\n",
      "Iteration 22992: Policy loss: 0.076418. Value loss: 0.045696. Entropy: 0.300692.\n",
      "episode: 7837   score: 235.0  epsilon: 1.0    steps: 200  evaluation reward: 508.7\n",
      "episode: 7838   score: 315.0  epsilon: 1.0    steps: 544  evaluation reward: 508.45\n",
      "episode: 7839   score: 260.0  epsilon: 1.0    steps: 784  evaluation reward: 504.1\n",
      "Training network. lr: 0.000074. clip: 0.029507\n",
      "Iteration 22993: Policy loss: 0.143501. Value loss: 0.075423. Entropy: 0.278317.\n",
      "Iteration 22994: Policy loss: 0.143287. Value loss: 0.034122. Entropy: 0.276832.\n",
      "Iteration 22995: Policy loss: 0.135411. Value loss: 0.028377. Entropy: 0.275691.\n",
      "Training network. lr: 0.000074. clip: 0.029507\n",
      "Iteration 22996: Policy loss: 0.113630. Value loss: 0.074132. Entropy: 0.311006.\n",
      "Iteration 22997: Policy loss: 0.116614. Value loss: 0.028525. Entropy: 0.310557.\n",
      "Iteration 22998: Policy loss: 0.117928. Value loss: 0.020782. Entropy: 0.310758.\n",
      "episode: 7840   score: 290.0  epsilon: 1.0    steps: 1024  evaluation reward: 503.1\n",
      "Training network. lr: 0.000074. clip: 0.029507\n",
      "Iteration 22999: Policy loss: -0.138967. Value loss: 0.344369. Entropy: 0.306744.\n",
      "Iteration 23000: Policy loss: -0.130435. Value loss: 0.248035. Entropy: 0.304875.\n",
      "Iteration 23001: Policy loss: -0.131501. Value loss: 0.202656. Entropy: 0.305381.\n",
      "Training network. lr: 0.000073. clip: 0.029350\n",
      "Iteration 23002: Policy loss: 0.251774. Value loss: 0.097282. Entropy: 0.296241.\n",
      "Iteration 23003: Policy loss: 0.249725. Value loss: 0.047843. Entropy: 0.296240.\n",
      "Iteration 23004: Policy loss: 0.247316. Value loss: 0.037717. Entropy: 0.297015.\n",
      "episode: 7841   score: 285.0  epsilon: 1.0    steps: 120  evaluation reward: 502.75\n",
      "Training network. lr: 0.000073. clip: 0.029350\n",
      "Iteration 23005: Policy loss: 0.126513. Value loss: 0.107946. Entropy: 0.296503.\n",
      "Iteration 23006: Policy loss: 0.121775. Value loss: 0.049777. Entropy: 0.295836.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23007: Policy loss: 0.117993. Value loss: 0.034147. Entropy: 0.295875.\n",
      "episode: 7842   score: 245.0  epsilon: 1.0    steps: 184  evaluation reward: 499.15\n",
      "episode: 7843   score: 210.0  epsilon: 1.0    steps: 264  evaluation reward: 495.65\n",
      "Training network. lr: 0.000073. clip: 0.029350\n",
      "Iteration 23008: Policy loss: -0.053287. Value loss: 0.068433. Entropy: 0.282683.\n",
      "Iteration 23009: Policy loss: -0.054550. Value loss: 0.032807. Entropy: 0.282144.\n",
      "Iteration 23010: Policy loss: -0.056121. Value loss: 0.026282. Entropy: 0.281550.\n",
      "episode: 7844   score: 565.0  epsilon: 1.0    steps: 96  evaluation reward: 497.95\n",
      "episode: 7845   score: 485.0  epsilon: 1.0    steps: 976  evaluation reward: 495.9\n",
      "Training network. lr: 0.000073. clip: 0.029350\n",
      "Iteration 23011: Policy loss: -0.489300. Value loss: 0.360835. Entropy: 0.297325.\n",
      "Iteration 23012: Policy loss: -0.501536. Value loss: 0.231025. Entropy: 0.296614.\n",
      "Iteration 23013: Policy loss: -0.528467. Value loss: 0.190558. Entropy: 0.296274.\n",
      "episode: 7846   score: 365.0  epsilon: 1.0    steps: 736  evaluation reward: 495.9\n",
      "Training network. lr: 0.000073. clip: 0.029350\n",
      "Iteration 23014: Policy loss: 0.226096. Value loss: 0.122109. Entropy: 0.291262.\n",
      "Iteration 23015: Policy loss: 0.231026. Value loss: 0.050808. Entropy: 0.290022.\n",
      "Iteration 23016: Policy loss: 0.222908. Value loss: 0.039322. Entropy: 0.289597.\n",
      "episode: 7847   score: 395.0  epsilon: 1.0    steps: 376  evaluation reward: 493.9\n",
      "Training network. lr: 0.000073. clip: 0.029350\n",
      "Iteration 23017: Policy loss: -0.206298. Value loss: 0.273968. Entropy: 0.300356.\n",
      "Iteration 23018: Policy loss: -0.218410. Value loss: 0.198974. Entropy: 0.300822.\n",
      "Iteration 23019: Policy loss: -0.225294. Value loss: 0.145386. Entropy: 0.299713.\n",
      "Training network. lr: 0.000073. clip: 0.029350\n",
      "Iteration 23020: Policy loss: -0.126749. Value loss: 0.068641. Entropy: 0.311741.\n",
      "Iteration 23021: Policy loss: -0.125260. Value loss: 0.029793. Entropy: 0.312100.\n",
      "Iteration 23022: Policy loss: -0.124549. Value loss: 0.021361. Entropy: 0.311430.\n",
      "episode: 7848   score: 260.0  epsilon: 1.0    steps: 992  evaluation reward: 488.85\n",
      "Training network. lr: 0.000073. clip: 0.029350\n",
      "Iteration 23023: Policy loss: 0.239755. Value loss: 0.198326. Entropy: 0.310142.\n",
      "Iteration 23024: Policy loss: 0.231020. Value loss: 0.069469. Entropy: 0.308656.\n",
      "Iteration 23025: Policy loss: 0.228962. Value loss: 0.041817. Entropy: 0.309141.\n",
      "episode: 7849   score: 300.0  epsilon: 1.0    steps: 8  evaluation reward: 488.3\n",
      "episode: 7850   score: 365.0  epsilon: 1.0    steps: 48  evaluation reward: 486.05\n",
      "Training network. lr: 0.000073. clip: 0.029350\n",
      "Iteration 23026: Policy loss: 0.141111. Value loss: 0.100137. Entropy: 0.276901.\n",
      "Iteration 23027: Policy loss: 0.130245. Value loss: 0.045434. Entropy: 0.275133.\n",
      "Iteration 23028: Policy loss: 0.134273. Value loss: 0.036695. Entropy: 0.275059.\n",
      "now time :  2019-09-06 13:59:09.843015\n",
      "episode: 7851   score: 245.0  epsilon: 1.0    steps: 584  evaluation reward: 485.7\n",
      "episode: 7852   score: 560.0  epsilon: 1.0    steps: 640  evaluation reward: 481.25\n",
      "Training network. lr: 0.000073. clip: 0.029350\n",
      "Iteration 23029: Policy loss: -0.002618. Value loss: 0.106834. Entropy: 0.282032.\n",
      "Iteration 23030: Policy loss: -0.002535. Value loss: 0.050423. Entropy: 0.281391.\n",
      "Iteration 23031: Policy loss: -0.003430. Value loss: 0.033596. Entropy: 0.281914.\n",
      "Training network. lr: 0.000073. clip: 0.029350\n",
      "Iteration 23032: Policy loss: -0.421772. Value loss: 0.349600. Entropy: 0.309386.\n",
      "Iteration 23033: Policy loss: -0.411694. Value loss: 0.179527. Entropy: 0.309347.\n",
      "Iteration 23034: Policy loss: -0.436468. Value loss: 0.138055. Entropy: 0.308875.\n",
      "episode: 7853   score: 330.0  epsilon: 1.0    steps: 992  evaluation reward: 479.65\n",
      "Training network. lr: 0.000073. clip: 0.029350\n",
      "Iteration 23035: Policy loss: -0.172905. Value loss: 0.192284. Entropy: 0.306381.\n",
      "Iteration 23036: Policy loss: -0.179800. Value loss: 0.089557. Entropy: 0.305078.\n",
      "Iteration 23037: Policy loss: -0.174624. Value loss: 0.056297. Entropy: 0.306369.\n",
      "episode: 7854   score: 380.0  epsilon: 1.0    steps: 456  evaluation reward: 479.0\n",
      "Training network. lr: 0.000073. clip: 0.029350\n",
      "Iteration 23038: Policy loss: 0.037139. Value loss: 0.076986. Entropy: 0.287430.\n",
      "Iteration 23039: Policy loss: 0.033187. Value loss: 0.036467. Entropy: 0.288918.\n",
      "Iteration 23040: Policy loss: 0.036564. Value loss: 0.026578. Entropy: 0.287410.\n",
      "Training network. lr: 0.000073. clip: 0.029350\n",
      "Iteration 23041: Policy loss: 0.259429. Value loss: 0.180759. Entropy: 0.309609.\n",
      "Iteration 23042: Policy loss: 0.254086. Value loss: 0.065341. Entropy: 0.309699.\n",
      "Iteration 23043: Policy loss: 0.241045. Value loss: 0.044110. Entropy: 0.310079.\n",
      "Training network. lr: 0.000073. clip: 0.029350\n",
      "Iteration 23044: Policy loss: 0.044390. Value loss: 0.133528. Entropy: 0.309370.\n",
      "Iteration 23045: Policy loss: 0.043575. Value loss: 0.070268. Entropy: 0.309275.\n",
      "Iteration 23046: Policy loss: 0.037602. Value loss: 0.050239. Entropy: 0.308970.\n",
      "Training network. lr: 0.000073. clip: 0.029350\n",
      "Iteration 23047: Policy loss: 0.069928. Value loss: 0.112535. Entropy: 0.318315.\n",
      "Iteration 23048: Policy loss: 0.063190. Value loss: 0.052397. Entropy: 0.318395.\n",
      "Iteration 23049: Policy loss: 0.062919. Value loss: 0.040280. Entropy: 0.318737.\n",
      "episode: 7855   score: 345.0  epsilon: 1.0    steps: 736  evaluation reward: 478.55\n",
      "episode: 7856   score: 570.0  epsilon: 1.0    steps: 752  evaluation reward: 480.2\n",
      "Training network. lr: 0.000073. clip: 0.029350\n",
      "Iteration 23050: Policy loss: 0.079840. Value loss: 0.051803. Entropy: 0.289187.\n",
      "Iteration 23051: Policy loss: 0.079473. Value loss: 0.027931. Entropy: 0.289133.\n",
      "Iteration 23052: Policy loss: 0.076598. Value loss: 0.020742. Entropy: 0.288410.\n",
      "Training network. lr: 0.000073. clip: 0.029193\n",
      "Iteration 23053: Policy loss: 0.033655. Value loss: 0.093538. Entropy: 0.307302.\n",
      "Iteration 23054: Policy loss: 0.027674. Value loss: 0.039268. Entropy: 0.307988.\n",
      "Iteration 23055: Policy loss: 0.023103. Value loss: 0.026766. Entropy: 0.307460.\n",
      "episode: 7857   score: 635.0  epsilon: 1.0    steps: 128  evaluation reward: 482.7\n",
      "episode: 7858   score: 590.0  epsilon: 1.0    steps: 264  evaluation reward: 485.25\n",
      "episode: 7859   score: 480.0  epsilon: 1.0    steps: 296  evaluation reward: 486.95\n",
      "Training network. lr: 0.000073. clip: 0.029193\n",
      "Iteration 23056: Policy loss: 0.171531. Value loss: 0.063851. Entropy: 0.273086.\n",
      "Iteration 23057: Policy loss: 0.166709. Value loss: 0.028789. Entropy: 0.273000.\n",
      "Iteration 23058: Policy loss: 0.160239. Value loss: 0.024118. Entropy: 0.275273.\n",
      "episode: 7860   score: 335.0  epsilon: 1.0    steps: 264  evaluation reward: 485.95\n",
      "episode: 7861   score: 280.0  epsilon: 1.0    steps: 408  evaluation reward: 481.0\n",
      "episode: 7862   score: 360.0  epsilon: 1.0    steps: 592  evaluation reward: 478.95\n",
      "Training network. lr: 0.000073. clip: 0.029193\n",
      "Iteration 23059: Policy loss: -0.096206. Value loss: 0.106458. Entropy: 0.266681.\n",
      "Iteration 23060: Policy loss: -0.096177. Value loss: 0.042688. Entropy: 0.267673.\n",
      "Iteration 23061: Policy loss: -0.096890. Value loss: 0.029495. Entropy: 0.268177.\n",
      "Training network. lr: 0.000073. clip: 0.029193\n",
      "Iteration 23062: Policy loss: -0.089391. Value loss: 0.065292. Entropy: 0.307543.\n",
      "Iteration 23063: Policy loss: -0.096502. Value loss: 0.031258. Entropy: 0.308748.\n",
      "Iteration 23064: Policy loss: -0.093477. Value loss: 0.023019. Entropy: 0.308130.\n",
      "Training network. lr: 0.000073. clip: 0.029193\n",
      "Iteration 23065: Policy loss: -0.005860. Value loss: 0.391561. Entropy: 0.304915.\n",
      "Iteration 23066: Policy loss: -0.003998. Value loss: 0.288773. Entropy: 0.304800.\n",
      "Iteration 23067: Policy loss: -0.018172. Value loss: 0.253022. Entropy: 0.304029.\n",
      "Training network. lr: 0.000073. clip: 0.029193\n",
      "Iteration 23068: Policy loss: 0.226356. Value loss: 0.204935. Entropy: 0.305284.\n",
      "Iteration 23069: Policy loss: 0.217087. Value loss: 0.095630. Entropy: 0.304633.\n",
      "Iteration 23070: Policy loss: 0.205794. Value loss: 0.069837. Entropy: 0.305332.\n",
      "Training network. lr: 0.000073. clip: 0.029193\n",
      "Iteration 23071: Policy loss: -0.175706. Value loss: 0.065547. Entropy: 0.309399.\n",
      "Iteration 23072: Policy loss: -0.172751. Value loss: 0.033309. Entropy: 0.309545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23073: Policy loss: -0.169770. Value loss: 0.024968. Entropy: 0.308698.\n",
      "episode: 7863   score: 310.0  epsilon: 1.0    steps: 360  evaluation reward: 478.1\n",
      "Training network. lr: 0.000073. clip: 0.029193\n",
      "Iteration 23074: Policy loss: -0.167007. Value loss: 0.134378. Entropy: 0.301601.\n",
      "Iteration 23075: Policy loss: -0.170916. Value loss: 0.062589. Entropy: 0.301593.\n",
      "Iteration 23076: Policy loss: -0.176190. Value loss: 0.044832. Entropy: 0.300693.\n",
      "episode: 7864   score: 305.0  epsilon: 1.0    steps: 112  evaluation reward: 476.05\n",
      "episode: 7865   score: 335.0  epsilon: 1.0    steps: 464  evaluation reward: 475.9\n",
      "Training network. lr: 0.000073. clip: 0.029193\n",
      "Iteration 23077: Policy loss: -0.069744. Value loss: 0.084969. Entropy: 0.284926.\n",
      "Iteration 23078: Policy loss: -0.064538. Value loss: 0.045707. Entropy: 0.284010.\n",
      "Iteration 23079: Policy loss: -0.071205. Value loss: 0.036320. Entropy: 0.284790.\n",
      "episode: 7866   score: 400.0  epsilon: 1.0    steps: 312  evaluation reward: 474.8\n",
      "episode: 7867   score: 400.0  epsilon: 1.0    steps: 424  evaluation reward: 472.5\n",
      "episode: 7868   score: 365.0  epsilon: 1.0    steps: 672  evaluation reward: 468.25\n",
      "Training network. lr: 0.000073. clip: 0.029193\n",
      "Iteration 23080: Policy loss: -0.059567. Value loss: 0.092267. Entropy: 0.268293.\n",
      "Iteration 23081: Policy loss: -0.059792. Value loss: 0.053960. Entropy: 0.268500.\n",
      "Iteration 23082: Policy loss: -0.056996. Value loss: 0.042604. Entropy: 0.267495.\n",
      "episode: 7869   score: 665.0  epsilon: 1.0    steps: 976  evaluation reward: 471.25\n",
      "Training network. lr: 0.000073. clip: 0.029193\n",
      "Iteration 23083: Policy loss: 0.172399. Value loss: 0.180522. Entropy: 0.306310.\n",
      "Iteration 23084: Policy loss: 0.168689. Value loss: 0.107444. Entropy: 0.305119.\n",
      "Iteration 23085: Policy loss: 0.175706. Value loss: 0.085086. Entropy: 0.304726.\n",
      "Training network. lr: 0.000073. clip: 0.029193\n",
      "Iteration 23086: Policy loss: -0.045606. Value loss: 0.109569. Entropy: 0.296377.\n",
      "Iteration 23087: Policy loss: -0.040612. Value loss: 0.055512. Entropy: 0.296556.\n",
      "Iteration 23088: Policy loss: -0.047792. Value loss: 0.041731. Entropy: 0.297041.\n",
      "Training network. lr: 0.000073. clip: 0.029193\n",
      "Iteration 23089: Policy loss: 0.075156. Value loss: 0.117980. Entropy: 0.309908.\n",
      "Iteration 23090: Policy loss: 0.068150. Value loss: 0.055308. Entropy: 0.309771.\n",
      "Iteration 23091: Policy loss: 0.065889. Value loss: 0.041037. Entropy: 0.308699.\n",
      "Training network. lr: 0.000073. clip: 0.029193\n",
      "Iteration 23092: Policy loss: 0.026170. Value loss: 0.076056. Entropy: 0.313713.\n",
      "Iteration 23093: Policy loss: 0.027611. Value loss: 0.034537. Entropy: 0.312901.\n",
      "Iteration 23094: Policy loss: 0.022718. Value loss: 0.026870. Entropy: 0.313115.\n",
      "episode: 7870   score: 785.0  epsilon: 1.0    steps: 48  evaluation reward: 475.2\n",
      "episode: 7871   score: 330.0  epsilon: 1.0    steps: 384  evaluation reward: 472.5\n",
      "episode: 7872   score: 260.0  epsilon: 1.0    steps: 680  evaluation reward: 471.2\n",
      "Training network. lr: 0.000073. clip: 0.029193\n",
      "Iteration 23095: Policy loss: -0.235651. Value loss: 0.341328. Entropy: 0.271358.\n",
      "Iteration 23096: Policy loss: -0.253614. Value loss: 0.220678. Entropy: 0.269715.\n",
      "Iteration 23097: Policy loss: -0.217205. Value loss: 0.152672. Entropy: 0.270331.\n",
      "Training network. lr: 0.000073. clip: 0.029193\n",
      "Iteration 23098: Policy loss: 0.010135. Value loss: 0.110129. Entropy: 0.309771.\n",
      "Iteration 23099: Policy loss: 0.007514. Value loss: 0.053357. Entropy: 0.309232.\n",
      "Iteration 23100: Policy loss: 0.006958. Value loss: 0.037436. Entropy: 0.308371.\n",
      "episode: 7873   score: 515.0  epsilon: 1.0    steps: 160  evaluation reward: 471.85\n",
      "episode: 7874   score: 340.0  epsilon: 1.0    steps: 288  evaluation reward: 469.4\n",
      "Training network. lr: 0.000073. clip: 0.029046\n",
      "Iteration 23101: Policy loss: 0.041986. Value loss: 0.110020. Entropy: 0.285223.\n",
      "Iteration 23102: Policy loss: 0.040614. Value loss: 0.046731. Entropy: 0.286079.\n",
      "Iteration 23103: Policy loss: 0.042608. Value loss: 0.032773. Entropy: 0.285899.\n",
      "episode: 7875   score: 365.0  epsilon: 1.0    steps: 208  evaluation reward: 467.0\n",
      "Training network. lr: 0.000073. clip: 0.029046\n",
      "Iteration 23104: Policy loss: 0.066766. Value loss: 0.127646. Entropy: 0.292517.\n",
      "Iteration 23105: Policy loss: 0.064444. Value loss: 0.056346. Entropy: 0.292474.\n",
      "Iteration 23106: Policy loss: 0.059630. Value loss: 0.041071. Entropy: 0.292814.\n",
      "episode: 7876   score: 390.0  epsilon: 1.0    steps: 248  evaluation reward: 466.35\n",
      "Training network. lr: 0.000073. clip: 0.029046\n",
      "Iteration 23107: Policy loss: 0.079250. Value loss: 0.041734. Entropy: 0.295763.\n",
      "Iteration 23108: Policy loss: 0.079312. Value loss: 0.025981. Entropy: 0.295697.\n",
      "Iteration 23109: Policy loss: 0.079160. Value loss: 0.022826. Entropy: 0.294549.\n",
      "Training network. lr: 0.000073. clip: 0.029046\n",
      "Iteration 23110: Policy loss: 0.303753. Value loss: 0.138434. Entropy: 0.310924.\n",
      "Iteration 23111: Policy loss: 0.303445. Value loss: 0.051931. Entropy: 0.309262.\n",
      "Iteration 23112: Policy loss: 0.293245. Value loss: 0.037908. Entropy: 0.309592.\n",
      "episode: 7877   score: 150.0  epsilon: 1.0    steps: 568  evaluation reward: 458.45\n",
      "Training network. lr: 0.000073. clip: 0.029046\n",
      "Iteration 23113: Policy loss: 0.255078. Value loss: 0.103706. Entropy: 0.295250.\n",
      "Iteration 23114: Policy loss: 0.249729. Value loss: 0.046501. Entropy: 0.293995.\n",
      "Iteration 23115: Policy loss: 0.249901. Value loss: 0.036337. Entropy: 0.293809.\n",
      "episode: 7878   score: 570.0  epsilon: 1.0    steps: 600  evaluation reward: 456.1\n",
      "episode: 7879   score: 270.0  epsilon: 1.0    steps: 976  evaluation reward: 454.05\n",
      "Training network. lr: 0.000073. clip: 0.029046\n",
      "Iteration 23116: Policy loss: -0.106914. Value loss: 0.151724. Entropy: 0.296895.\n",
      "Iteration 23117: Policy loss: -0.108610. Value loss: 0.073964. Entropy: 0.295983.\n",
      "Iteration 23118: Policy loss: -0.110451. Value loss: 0.051355. Entropy: 0.296082.\n",
      "episode: 7880   score: 315.0  epsilon: 1.0    steps: 96  evaluation reward: 451.5\n",
      "episode: 7881   score: 345.0  epsilon: 1.0    steps: 256  evaluation reward: 453.45\n",
      "Training network. lr: 0.000073. clip: 0.029046\n",
      "Iteration 23119: Policy loss: -0.059606. Value loss: 0.073264. Entropy: 0.269418.\n",
      "Iteration 23120: Policy loss: -0.064673. Value loss: 0.034695. Entropy: 0.268671.\n",
      "Iteration 23121: Policy loss: -0.062433. Value loss: 0.029485. Entropy: 0.267713.\n",
      "episode: 7882   score: 285.0  epsilon: 1.0    steps: 792  evaluation reward: 447.2\n",
      "Training network. lr: 0.000073. clip: 0.029046\n",
      "Iteration 23122: Policy loss: -0.123663. Value loss: 0.084580. Entropy: 0.302070.\n",
      "Iteration 23123: Policy loss: -0.124842. Value loss: 0.042251. Entropy: 0.301986.\n",
      "Iteration 23124: Policy loss: -0.133641. Value loss: 0.028936. Entropy: 0.302071.\n",
      "episode: 7883   score: 405.0  epsilon: 1.0    steps: 752  evaluation reward: 447.05\n",
      "episode: 7884   score: 295.0  epsilon: 1.0    steps: 872  evaluation reward: 443.4\n",
      "Training network. lr: 0.000073. clip: 0.029046\n",
      "Iteration 23125: Policy loss: 0.228771. Value loss: 0.067929. Entropy: 0.289465.\n",
      "Iteration 23126: Policy loss: 0.230928. Value loss: 0.048005. Entropy: 0.290878.\n",
      "Iteration 23127: Policy loss: 0.225102. Value loss: 0.041489. Entropy: 0.290982.\n",
      "Training network. lr: 0.000073. clip: 0.029046\n",
      "Iteration 23128: Policy loss: -0.159251. Value loss: 0.053130. Entropy: 0.302396.\n",
      "Iteration 23129: Policy loss: -0.158095. Value loss: 0.027917. Entropy: 0.303937.\n",
      "Iteration 23130: Policy loss: -0.164403. Value loss: 0.020351. Entropy: 0.303017.\n",
      "Training network. lr: 0.000073. clip: 0.029046\n",
      "Iteration 23131: Policy loss: -0.052725. Value loss: 0.059628. Entropy: 0.310363.\n",
      "Iteration 23132: Policy loss: -0.056287. Value loss: 0.022614. Entropy: 0.310123.\n",
      "Iteration 23133: Policy loss: -0.054512. Value loss: 0.016884. Entropy: 0.310141.\n",
      "Training network. lr: 0.000073. clip: 0.029046\n",
      "Iteration 23134: Policy loss: -0.286457. Value loss: 0.263243. Entropy: 0.312087.\n",
      "Iteration 23135: Policy loss: -0.296006. Value loss: 0.116833. Entropy: 0.313501.\n",
      "Iteration 23136: Policy loss: -0.285226. Value loss: 0.075429. Entropy: 0.313900.\n",
      "episode: 7885   score: 375.0  epsilon: 1.0    steps: 408  evaluation reward: 439.15\n",
      "episode: 7886   score: 285.0  epsilon: 1.0    steps: 976  evaluation reward: 437.15\n",
      "Training network. lr: 0.000073. clip: 0.029046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23137: Policy loss: -0.091047. Value loss: 0.261860. Entropy: 0.292193.\n",
      "Iteration 23138: Policy loss: -0.116503. Value loss: 0.094201. Entropy: 0.293053.\n",
      "Iteration 23139: Policy loss: -0.127305. Value loss: 0.062029. Entropy: 0.292818.\n",
      "Training network. lr: 0.000073. clip: 0.029046\n",
      "Iteration 23140: Policy loss: -0.343614. Value loss: 0.228831. Entropy: 0.296412.\n",
      "Iteration 23141: Policy loss: -0.336993. Value loss: 0.082813. Entropy: 0.296889.\n",
      "Iteration 23142: Policy loss: -0.339456. Value loss: 0.050411. Entropy: 0.297511.\n",
      "episode: 7887   score: 390.0  epsilon: 1.0    steps: 552  evaluation reward: 434.7\n",
      "Training network. lr: 0.000073. clip: 0.029046\n",
      "Iteration 23143: Policy loss: -0.165162. Value loss: 0.073333. Entropy: 0.296644.\n",
      "Iteration 23144: Policy loss: -0.164064. Value loss: 0.035830. Entropy: 0.294524.\n",
      "Iteration 23145: Policy loss: -0.168462. Value loss: 0.025901. Entropy: 0.294540.\n",
      "episode: 7888   score: 515.0  epsilon: 1.0    steps: 336  evaluation reward: 436.55\n",
      "episode: 7889   score: 535.0  epsilon: 1.0    steps: 936  evaluation reward: 436.75\n",
      "Training network. lr: 0.000073. clip: 0.029046\n",
      "Iteration 23146: Policy loss: 0.052738. Value loss: 0.276879. Entropy: 0.292682.\n",
      "Iteration 23147: Policy loss: 0.044629. Value loss: 0.124562. Entropy: 0.292519.\n",
      "Iteration 23148: Policy loss: 0.044432. Value loss: 0.076096. Entropy: 0.292519.\n",
      "Training network. lr: 0.000073. clip: 0.029046\n",
      "Iteration 23149: Policy loss: 0.025907. Value loss: 0.124606. Entropy: 0.303588.\n",
      "Iteration 23150: Policy loss: 0.028085. Value loss: 0.068585. Entropy: 0.304330.\n",
      "Iteration 23151: Policy loss: 0.027500. Value loss: 0.048012. Entropy: 0.304290.\n",
      "episode: 7890   score: 495.0  epsilon: 1.0    steps: 88  evaluation reward: 437.9\n",
      "episode: 7891   score: 515.0  epsilon: 1.0    steps: 144  evaluation reward: 435.05\n",
      "episode: 7892   score: 725.0  epsilon: 1.0    steps: 512  evaluation reward: 436.2\n",
      "Training network. lr: 0.000072. clip: 0.028889\n",
      "Iteration 23152: Policy loss: -0.261031. Value loss: 0.156931. Entropy: 0.264143.\n",
      "Iteration 23153: Policy loss: -0.272629. Value loss: 0.078434. Entropy: 0.265220.\n",
      "Iteration 23154: Policy loss: -0.273964. Value loss: 0.045713. Entropy: 0.264303.\n",
      "Training network. lr: 0.000072. clip: 0.028889\n",
      "Iteration 23155: Policy loss: 0.263090. Value loss: 0.218082. Entropy: 0.303559.\n",
      "Iteration 23156: Policy loss: 0.239989. Value loss: 0.084687. Entropy: 0.303379.\n",
      "Iteration 23157: Policy loss: 0.254166. Value loss: 0.059650. Entropy: 0.303668.\n",
      "Training network. lr: 0.000072. clip: 0.028889\n",
      "Iteration 23158: Policy loss: 0.034634. Value loss: 0.064127. Entropy: 0.311084.\n",
      "Iteration 23159: Policy loss: 0.031077. Value loss: 0.033730. Entropy: 0.310999.\n",
      "Iteration 23160: Policy loss: 0.034220. Value loss: 0.026931. Entropy: 0.310610.\n",
      "Training network. lr: 0.000072. clip: 0.028889\n",
      "Iteration 23161: Policy loss: 0.058163. Value loss: 0.183486. Entropy: 0.309760.\n",
      "Iteration 23162: Policy loss: 0.059821. Value loss: 0.060420. Entropy: 0.309619.\n",
      "Iteration 23163: Policy loss: 0.052741. Value loss: 0.037326. Entropy: 0.309073.\n",
      "Training network. lr: 0.000072. clip: 0.028889\n",
      "Iteration 23164: Policy loss: 0.005252. Value loss: 0.122349. Entropy: 0.308570.\n",
      "Iteration 23165: Policy loss: 0.001927. Value loss: 0.055261. Entropy: 0.307872.\n",
      "Iteration 23166: Policy loss: 0.002149. Value loss: 0.042097. Entropy: 0.308179.\n",
      "episode: 7893   score: 425.0  epsilon: 1.0    steps: 560  evaluation reward: 433.2\n",
      "episode: 7894   score: 495.0  epsilon: 1.0    steps: 768  evaluation reward: 433.75\n",
      "Training network. lr: 0.000072. clip: 0.028889\n",
      "Iteration 23167: Policy loss: -0.015948. Value loss: 0.129657. Entropy: 0.281227.\n",
      "Iteration 23168: Policy loss: -0.020733. Value loss: 0.066573. Entropy: 0.279111.\n",
      "Iteration 23169: Policy loss: -0.019271. Value loss: 0.042585. Entropy: 0.281077.\n",
      "episode: 7895   score: 420.0  epsilon: 1.0    steps: 96  evaluation reward: 432.5\n",
      "Training network. lr: 0.000072. clip: 0.028889\n",
      "Iteration 23170: Policy loss: -0.104482. Value loss: 0.135799. Entropy: 0.287040.\n",
      "Iteration 23171: Policy loss: -0.112073. Value loss: 0.075069. Entropy: 0.287281.\n",
      "Iteration 23172: Policy loss: -0.108231. Value loss: 0.059928. Entropy: 0.287584.\n",
      "episode: 7896   score: 420.0  epsilon: 1.0    steps: 872  evaluation reward: 430.55\n",
      "episode: 7897   score: 575.0  epsilon: 1.0    steps: 992  evaluation reward: 426.55\n",
      "Training network. lr: 0.000072. clip: 0.028889\n",
      "Iteration 23173: Policy loss: 0.204063. Value loss: 0.183049. Entropy: 0.297994.\n",
      "Iteration 23174: Policy loss: 0.199375. Value loss: 0.090317. Entropy: 0.296120.\n",
      "Iteration 23175: Policy loss: 0.194333. Value loss: 0.063365. Entropy: 0.295317.\n",
      "episode: 7898   score: 385.0  epsilon: 1.0    steps: 376  evaluation reward: 426.95\n",
      "Training network. lr: 0.000072. clip: 0.028889\n",
      "Iteration 23176: Policy loss: -0.203791. Value loss: 0.312462. Entropy: 0.275769.\n",
      "Iteration 23177: Policy loss: -0.206857. Value loss: 0.126063. Entropy: 0.275699.\n",
      "Iteration 23178: Policy loss: -0.227161. Value loss: 0.075521. Entropy: 0.277494.\n",
      "episode: 7899   score: 500.0  epsilon: 1.0    steps: 432  evaluation reward: 423.05\n",
      "episode: 7900   score: 755.0  epsilon: 1.0    steps: 648  evaluation reward: 425.35\n",
      "Training network. lr: 0.000072. clip: 0.028889\n",
      "Iteration 23179: Policy loss: 0.001883. Value loss: 0.065605. Entropy: 0.279520.\n",
      "Iteration 23180: Policy loss: 0.009193. Value loss: 0.036425. Entropy: 0.277473.\n",
      "Iteration 23181: Policy loss: 0.002026. Value loss: 0.028462. Entropy: 0.278575.\n",
      "Training network. lr: 0.000072. clip: 0.028889\n",
      "Iteration 23182: Policy loss: 0.057208. Value loss: 0.051632. Entropy: 0.309818.\n",
      "Iteration 23183: Policy loss: 0.057938. Value loss: 0.026409. Entropy: 0.310052.\n",
      "Iteration 23184: Policy loss: 0.052894. Value loss: 0.020470. Entropy: 0.310034.\n",
      "Training network. lr: 0.000072. clip: 0.028889\n",
      "Iteration 23185: Policy loss: 0.380679. Value loss: 0.207794. Entropy: 0.308542.\n",
      "Iteration 23186: Policy loss: 0.378411. Value loss: 0.071827. Entropy: 0.307863.\n",
      "Iteration 23187: Policy loss: 0.367063. Value loss: 0.058586. Entropy: 0.307908.\n",
      "Training network. lr: 0.000072. clip: 0.028889\n",
      "Iteration 23188: Policy loss: 0.048105. Value loss: 0.074674. Entropy: 0.312218.\n",
      "Iteration 23189: Policy loss: 0.043382. Value loss: 0.037927. Entropy: 0.312689.\n",
      "Iteration 23190: Policy loss: 0.037579. Value loss: 0.030064. Entropy: 0.312047.\n",
      "now time :  2019-09-06 14:09:13.468694\n",
      "episode: 7901   score: 395.0  epsilon: 1.0    steps: 952  evaluation reward: 427.05\n",
      "episode: 7902   score: 390.0  epsilon: 1.0    steps: 992  evaluation reward: 425.65\n",
      "Training network. lr: 0.000072. clip: 0.028889\n",
      "Iteration 23191: Policy loss: 0.164904. Value loss: 0.068645. Entropy: 0.311374.\n",
      "Iteration 23192: Policy loss: 0.166766. Value loss: 0.037844. Entropy: 0.311218.\n",
      "Iteration 23193: Policy loss: 0.165053. Value loss: 0.031599. Entropy: 0.310908.\n",
      "Training network. lr: 0.000072. clip: 0.028889\n",
      "Iteration 23194: Policy loss: -0.102468. Value loss: 0.221451. Entropy: 0.290853.\n",
      "Iteration 23195: Policy loss: -0.109686. Value loss: 0.096213. Entropy: 0.290133.\n",
      "Iteration 23196: Policy loss: -0.123192. Value loss: 0.066493. Entropy: 0.289757.\n",
      "episode: 7903   score: 640.0  epsilon: 1.0    steps: 752  evaluation reward: 429.95\n",
      "episode: 7904   score: 430.0  epsilon: 1.0    steps: 944  evaluation reward: 428.15\n",
      "episode: 7905   score: 380.0  epsilon: 1.0    steps: 960  evaluation reward: 429.05\n",
      "Training network. lr: 0.000072. clip: 0.028889\n",
      "Iteration 23197: Policy loss: 0.070046. Value loss: 0.139793. Entropy: 0.297086.\n",
      "Iteration 23198: Policy loss: 0.067254. Value loss: 0.067510. Entropy: 0.296237.\n",
      "Iteration 23199: Policy loss: 0.056298. Value loss: 0.051723. Entropy: 0.295677.\n",
      "episode: 7906   score: 340.0  epsilon: 1.0    steps: 504  evaluation reward: 429.3\n",
      "Training network. lr: 0.000072. clip: 0.028889\n",
      "Iteration 23200: Policy loss: 0.052185. Value loss: 0.072877. Entropy: 0.273020.\n",
      "Iteration 23201: Policy loss: 0.047304. Value loss: 0.042228. Entropy: 0.272888.\n",
      "Iteration 23202: Policy loss: 0.049188. Value loss: 0.030695. Entropy: 0.272291.\n",
      "episode: 7907   score: 405.0  epsilon: 1.0    steps: 480  evaluation reward: 430.2\n",
      "Training network. lr: 0.000072. clip: 0.028733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23203: Policy loss: 0.088893. Value loss: 0.090262. Entropy: 0.293616.\n",
      "Iteration 23204: Policy loss: 0.094024. Value loss: 0.033906. Entropy: 0.293548.\n",
      "Iteration 23205: Policy loss: 0.086227. Value loss: 0.024682. Entropy: 0.294107.\n",
      "episode: 7908   score: 420.0  epsilon: 1.0    steps: 456  evaluation reward: 430.45\n",
      "Training network. lr: 0.000072. clip: 0.028733\n",
      "Iteration 23206: Policy loss: 0.189849. Value loss: 0.113602. Entropy: 0.287490.\n",
      "Iteration 23207: Policy loss: 0.189615. Value loss: 0.051981. Entropy: 0.287331.\n",
      "Iteration 23208: Policy loss: 0.175865. Value loss: 0.031825. Entropy: 0.287741.\n",
      "Training network. lr: 0.000072. clip: 0.028733\n",
      "Iteration 23209: Policy loss: -0.146974. Value loss: 0.319974. Entropy: 0.306221.\n",
      "Iteration 23210: Policy loss: -0.173846. Value loss: 0.236009. Entropy: 0.305737.\n",
      "Iteration 23211: Policy loss: -0.155757. Value loss: 0.184846. Entropy: 0.306937.\n",
      "Training network. lr: 0.000072. clip: 0.028733\n",
      "Iteration 23212: Policy loss: 0.036219. Value loss: 0.069825. Entropy: 0.309695.\n",
      "Iteration 23213: Policy loss: 0.039409. Value loss: 0.029076. Entropy: 0.308943.\n",
      "Iteration 23214: Policy loss: 0.035025. Value loss: 0.019873. Entropy: 0.309316.\n",
      "episode: 7909   score: 440.0  epsilon: 1.0    steps: 920  evaluation reward: 430.95\n",
      "Training network. lr: 0.000072. clip: 0.028733\n",
      "Iteration 23215: Policy loss: 0.045401. Value loss: 0.134542. Entropy: 0.305342.\n",
      "Iteration 23216: Policy loss: 0.046862. Value loss: 0.075208. Entropy: 0.304808.\n",
      "Iteration 23217: Policy loss: 0.043798. Value loss: 0.059203. Entropy: 0.305578.\n",
      "episode: 7910   score: 430.0  epsilon: 1.0    steps: 416  evaluation reward: 427.25\n",
      "Training network. lr: 0.000072. clip: 0.028733\n",
      "Iteration 23218: Policy loss: 0.238035. Value loss: 0.126435. Entropy: 0.283757.\n",
      "Iteration 23219: Policy loss: 0.236167. Value loss: 0.051398. Entropy: 0.282939.\n",
      "Iteration 23220: Policy loss: 0.240845. Value loss: 0.035627. Entropy: 0.283468.\n",
      "episode: 7911   score: 350.0  epsilon: 1.0    steps: 272  evaluation reward: 422.75\n",
      "episode: 7912   score: 315.0  epsilon: 1.0    steps: 328  evaluation reward: 423.75\n",
      "Training network. lr: 0.000072. clip: 0.028733\n",
      "Iteration 23221: Policy loss: -0.030938. Value loss: 0.098389. Entropy: 0.279986.\n",
      "Iteration 23222: Policy loss: -0.039755. Value loss: 0.050793. Entropy: 0.278623.\n",
      "Iteration 23223: Policy loss: -0.030201. Value loss: 0.037567. Entropy: 0.278912.\n",
      "episode: 7913   score: 575.0  epsilon: 1.0    steps: 64  evaluation reward: 423.85\n",
      "Training network. lr: 0.000072. clip: 0.028733\n",
      "Iteration 23224: Policy loss: -0.167054. Value loss: 0.101909. Entropy: 0.289849.\n",
      "Iteration 23225: Policy loss: -0.162542. Value loss: 0.055709. Entropy: 0.290367.\n",
      "Iteration 23226: Policy loss: -0.159613. Value loss: 0.041792. Entropy: 0.290521.\n",
      "Training network. lr: 0.000072. clip: 0.028733\n",
      "Iteration 23227: Policy loss: -0.365941. Value loss: 0.279398. Entropy: 0.306296.\n",
      "Iteration 23228: Policy loss: -0.357031. Value loss: 0.169666. Entropy: 0.304858.\n",
      "Iteration 23229: Policy loss: -0.351840. Value loss: 0.117980. Entropy: 0.305588.\n",
      "episode: 7914   score: 500.0  epsilon: 1.0    steps: 808  evaluation reward: 425.4\n",
      "Training network. lr: 0.000072. clip: 0.028733\n",
      "Iteration 23230: Policy loss: 0.162559. Value loss: 0.049806. Entropy: 0.301260.\n",
      "Iteration 23231: Policy loss: 0.156186. Value loss: 0.024026. Entropy: 0.301235.\n",
      "Iteration 23232: Policy loss: 0.156930. Value loss: 0.019272. Entropy: 0.301016.\n",
      "Training network. lr: 0.000072. clip: 0.028733\n",
      "Iteration 23233: Policy loss: -0.398850. Value loss: 0.391205. Entropy: 0.303209.\n",
      "Iteration 23234: Policy loss: -0.394136. Value loss: 0.253879. Entropy: 0.303568.\n",
      "Iteration 23235: Policy loss: -0.408931. Value loss: 0.198960. Entropy: 0.302080.\n",
      "episode: 7915   score: 315.0  epsilon: 1.0    steps: 248  evaluation reward: 421.6\n",
      "episode: 7916   score: 550.0  epsilon: 1.0    steps: 272  evaluation reward: 423.3\n",
      "Training network. lr: 0.000072. clip: 0.028733\n",
      "Iteration 23236: Policy loss: -0.103480. Value loss: 0.123323. Entropy: 0.276128.\n",
      "Iteration 23237: Policy loss: -0.109606. Value loss: 0.058676. Entropy: 0.274839.\n",
      "Iteration 23238: Policy loss: -0.117494. Value loss: 0.043974. Entropy: 0.276201.\n",
      "episode: 7917   score: 285.0  epsilon: 1.0    steps: 296  evaluation reward: 421.0\n",
      "episode: 7918   score: 650.0  epsilon: 1.0    steps: 776  evaluation reward: 421.0\n",
      "Training network. lr: 0.000072. clip: 0.028733\n",
      "Iteration 23239: Policy loss: 0.360090. Value loss: 0.140511. Entropy: 0.277949.\n",
      "Iteration 23240: Policy loss: 0.365536. Value loss: 0.052126. Entropy: 0.277150.\n",
      "Iteration 23241: Policy loss: 0.350311. Value loss: 0.036969. Entropy: 0.277419.\n",
      "Training network. lr: 0.000072. clip: 0.028733\n",
      "Iteration 23242: Policy loss: 0.373460. Value loss: 0.109383. Entropy: 0.306216.\n",
      "Iteration 23243: Policy loss: 0.364889. Value loss: 0.047459. Entropy: 0.306023.\n",
      "Iteration 23244: Policy loss: 0.367607. Value loss: 0.031785. Entropy: 0.305952.\n",
      "episode: 7919   score: 395.0  epsilon: 1.0    steps: 816  evaluation reward: 420.6\n",
      "episode: 7920   score: 380.0  epsilon: 1.0    steps: 960  evaluation reward: 416.05\n",
      "Training network. lr: 0.000072. clip: 0.028733\n",
      "Iteration 23245: Policy loss: -0.262740. Value loss: 0.324729. Entropy: 0.297193.\n",
      "Iteration 23246: Policy loss: -0.268937. Value loss: 0.184832. Entropy: 0.298163.\n",
      "Iteration 23247: Policy loss: -0.265654. Value loss: 0.142886. Entropy: 0.297102.\n",
      "Training network. lr: 0.000072. clip: 0.028733\n",
      "Iteration 23248: Policy loss: -0.050578. Value loss: 0.075828. Entropy: 0.294402.\n",
      "Iteration 23249: Policy loss: -0.048608. Value loss: 0.033299. Entropy: 0.293788.\n",
      "Iteration 23250: Policy loss: -0.046514. Value loss: 0.025681. Entropy: 0.295192.\n",
      "Training network. lr: 0.000071. clip: 0.028585\n",
      "Iteration 23251: Policy loss: -0.153096. Value loss: 0.199454. Entropy: 0.308269.\n",
      "Iteration 23252: Policy loss: -0.147472. Value loss: 0.091891. Entropy: 0.307487.\n",
      "Iteration 23253: Policy loss: -0.148251. Value loss: 0.067466. Entropy: 0.307734.\n",
      "episode: 7921   score: 635.0  epsilon: 1.0    steps: 392  evaluation reward: 419.55\n",
      "Training network. lr: 0.000071. clip: 0.028585\n",
      "Iteration 23254: Policy loss: -0.062189. Value loss: 0.152888. Entropy: 0.282642.\n",
      "Iteration 23255: Policy loss: -0.060787. Value loss: 0.070713. Entropy: 0.282959.\n",
      "Iteration 23256: Policy loss: -0.077057. Value loss: 0.052524. Entropy: 0.282622.\n",
      "Training network. lr: 0.000071. clip: 0.028585\n",
      "Iteration 23257: Policy loss: 0.222379. Value loss: 0.170786. Entropy: 0.304198.\n",
      "Iteration 23258: Policy loss: 0.234019. Value loss: 0.051309. Entropy: 0.304378.\n",
      "Iteration 23259: Policy loss: 0.233447. Value loss: 0.028729. Entropy: 0.304274.\n",
      "episode: 7922   score: 615.0  epsilon: 1.0    steps: 376  evaluation reward: 420.95\n",
      "episode: 7923   score: 210.0  epsilon: 1.0    steps: 936  evaluation reward: 415.5\n",
      "Training network. lr: 0.000071. clip: 0.028585\n",
      "Iteration 23260: Policy loss: -0.227422. Value loss: 0.332274. Entropy: 0.296729.\n",
      "Iteration 23261: Policy loss: -0.237169. Value loss: 0.151935. Entropy: 0.295400.\n",
      "Iteration 23262: Policy loss: -0.214626. Value loss: 0.055743. Entropy: 0.296186.\n",
      "episode: 7924   score: 335.0  epsilon: 1.0    steps: 192  evaluation reward: 413.5\n",
      "Training network. lr: 0.000071. clip: 0.028585\n",
      "Iteration 23263: Policy loss: 0.126733. Value loss: 0.098958. Entropy: 0.281654.\n",
      "Iteration 23264: Policy loss: 0.124913. Value loss: 0.043442. Entropy: 0.283251.\n",
      "Iteration 23265: Policy loss: 0.123249. Value loss: 0.035211. Entropy: 0.281568.\n",
      "episode: 7925   score: 330.0  epsilon: 1.0    steps: 752  evaluation reward: 413.65\n",
      "Training network. lr: 0.000071. clip: 0.028585\n",
      "Iteration 23266: Policy loss: -0.067044. Value loss: 0.157745. Entropy: 0.292511.\n",
      "Iteration 23267: Policy loss: -0.065081. Value loss: 0.065168. Entropy: 0.291900.\n",
      "Iteration 23268: Policy loss: -0.067206. Value loss: 0.041293. Entropy: 0.291134.\n",
      "episode: 7926   score: 525.0  epsilon: 1.0    steps: 464  evaluation reward: 413.6\n",
      "episode: 7927   score: 575.0  epsilon: 1.0    steps: 504  evaluation reward: 416.2\n",
      "Training network. lr: 0.000071. clip: 0.028585\n",
      "Iteration 23269: Policy loss: 0.047553. Value loss: 0.069569. Entropy: 0.279692.\n",
      "Iteration 23270: Policy loss: 0.042869. Value loss: 0.042066. Entropy: 0.278604.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23271: Policy loss: 0.039355. Value loss: 0.032247. Entropy: 0.279228.\n",
      "Training network. lr: 0.000071. clip: 0.028585\n",
      "Iteration 23272: Policy loss: 0.137370. Value loss: 0.191978. Entropy: 0.308166.\n",
      "Iteration 23273: Policy loss: 0.130677. Value loss: 0.068547. Entropy: 0.306573.\n",
      "Iteration 23274: Policy loss: 0.141486. Value loss: 0.042997. Entropy: 0.306621.\n",
      "Training network. lr: 0.000071. clip: 0.028585\n",
      "Iteration 23275: Policy loss: -0.138830. Value loss: 0.111545. Entropy: 0.311662.\n",
      "Iteration 23276: Policy loss: -0.140227. Value loss: 0.052141. Entropy: 0.311263.\n",
      "Iteration 23277: Policy loss: -0.141828. Value loss: 0.040750. Entropy: 0.311626.\n",
      "Training network. lr: 0.000071. clip: 0.028585\n",
      "Iteration 23278: Policy loss: -0.091135. Value loss: 0.167325. Entropy: 0.311072.\n",
      "Iteration 23279: Policy loss: -0.097177. Value loss: 0.084433. Entropy: 0.311264.\n",
      "Iteration 23280: Policy loss: -0.095823. Value loss: 0.064145. Entropy: 0.311185.\n",
      "episode: 7928   score: 400.0  epsilon: 1.0    steps: 736  evaluation reward: 417.35\n",
      "episode: 7929   score: 1120.0  epsilon: 1.0    steps: 960  evaluation reward: 425.7\n",
      "Training network. lr: 0.000071. clip: 0.028585\n",
      "Iteration 23281: Policy loss: 0.102413. Value loss: 0.272123. Entropy: 0.291572.\n",
      "Iteration 23282: Policy loss: 0.124760. Value loss: 0.124866. Entropy: 0.292579.\n",
      "Iteration 23283: Policy loss: 0.096107. Value loss: 0.090081. Entropy: 0.290561.\n",
      "Training network. lr: 0.000071. clip: 0.028585\n",
      "Iteration 23284: Policy loss: -0.133667. Value loss: 0.109085. Entropy: 0.299580.\n",
      "Iteration 23285: Policy loss: -0.134485. Value loss: 0.062281. Entropy: 0.298727.\n",
      "Iteration 23286: Policy loss: -0.139106. Value loss: 0.046900. Entropy: 0.299714.\n",
      "episode: 7930   score: 535.0  epsilon: 1.0    steps: 24  evaluation reward: 422.75\n",
      "Training network. lr: 0.000071. clip: 0.028585\n",
      "Iteration 23287: Policy loss: 0.328791. Value loss: 0.157364. Entropy: 0.294526.\n",
      "Iteration 23288: Policy loss: 0.320008. Value loss: 0.096867. Entropy: 0.293098.\n",
      "Iteration 23289: Policy loss: 0.326786. Value loss: 0.074170. Entropy: 0.293888.\n",
      "episode: 7931   score: 385.0  epsilon: 1.0    steps: 704  evaluation reward: 423.15\n",
      "Training network. lr: 0.000071. clip: 0.028585\n",
      "Iteration 23290: Policy loss: 0.275018. Value loss: 0.144613. Entropy: 0.292942.\n",
      "Iteration 23291: Policy loss: 0.276635. Value loss: 0.070435. Entropy: 0.292662.\n",
      "Iteration 23292: Policy loss: 0.270489. Value loss: 0.051696. Entropy: 0.292109.\n",
      "episode: 7932   score: 400.0  epsilon: 1.0    steps: 48  evaluation reward: 422.75\n",
      "episode: 7933   score: 650.0  epsilon: 1.0    steps: 520  evaluation reward: 423.75\n",
      "episode: 7934   score: 395.0  epsilon: 1.0    steps: 584  evaluation reward: 424.45\n",
      "episode: 7935   score: 575.0  epsilon: 1.0    steps: 696  evaluation reward: 427.3\n",
      "Training network. lr: 0.000071. clip: 0.028585\n",
      "Iteration 23293: Policy loss: 0.053647. Value loss: 0.047315. Entropy: 0.255401.\n",
      "Iteration 23294: Policy loss: 0.051139. Value loss: 0.029433. Entropy: 0.255090.\n",
      "Iteration 23295: Policy loss: 0.049055. Value loss: 0.024927. Entropy: 0.255452.\n",
      "Training network. lr: 0.000071. clip: 0.028585\n",
      "Iteration 23296: Policy loss: -0.311860. Value loss: 0.155688. Entropy: 0.307497.\n",
      "Iteration 23297: Policy loss: -0.312744. Value loss: 0.077990. Entropy: 0.308534.\n",
      "Iteration 23298: Policy loss: -0.314719. Value loss: 0.045209. Entropy: 0.308013.\n",
      "Training network. lr: 0.000071. clip: 0.028585\n",
      "Iteration 23299: Policy loss: -0.211533. Value loss: 0.314588. Entropy: 0.309642.\n",
      "Iteration 23300: Policy loss: -0.226116. Value loss: 0.110597. Entropy: 0.309341.\n",
      "Iteration 23301: Policy loss: -0.210312. Value loss: 0.076788. Entropy: 0.309481.\n",
      "episode: 7936   score: 360.0  epsilon: 1.0    steps: 928  evaluation reward: 425.25\n",
      "Training network. lr: 0.000071. clip: 0.028429\n",
      "Iteration 23302: Policy loss: 0.205398. Value loss: 0.184238. Entropy: 0.301024.\n",
      "Iteration 23303: Policy loss: 0.210175. Value loss: 0.079567. Entropy: 0.301313.\n",
      "Iteration 23304: Policy loss: 0.206039. Value loss: 0.053820. Entropy: 0.300702.\n",
      "episode: 7937   score: 365.0  epsilon: 1.0    steps: 624  evaluation reward: 426.55\n",
      "Training network. lr: 0.000071. clip: 0.028429\n",
      "Iteration 23305: Policy loss: 0.143048. Value loss: 0.148020. Entropy: 0.289981.\n",
      "Iteration 23306: Policy loss: 0.140363. Value loss: 0.044549. Entropy: 0.288021.\n",
      "Iteration 23307: Policy loss: 0.133289. Value loss: 0.030295. Entropy: 0.287803.\n",
      "episode: 7938   score: 565.0  epsilon: 1.0    steps: 760  evaluation reward: 429.05\n",
      "Training network. lr: 0.000071. clip: 0.028429\n",
      "Iteration 23308: Policy loss: 0.132428. Value loss: 0.118469. Entropy: 0.299974.\n",
      "Iteration 23309: Policy loss: 0.134653. Value loss: 0.052117. Entropy: 0.298590.\n",
      "Iteration 23310: Policy loss: 0.128292. Value loss: 0.043117. Entropy: 0.299128.\n",
      "Training network. lr: 0.000071. clip: 0.028429\n",
      "Iteration 23311: Policy loss: 0.046296. Value loss: 0.173011. Entropy: 0.310045.\n",
      "Iteration 23312: Policy loss: 0.049025. Value loss: 0.080007. Entropy: 0.309584.\n",
      "Iteration 23313: Policy loss: 0.046731. Value loss: 0.053752. Entropy: 0.310247.\n",
      "episode: 7939   score: 330.0  epsilon: 1.0    steps: 424  evaluation reward: 429.75\n",
      "Training network. lr: 0.000071. clip: 0.028429\n",
      "Iteration 23314: Policy loss: 0.190594. Value loss: 0.139578. Entropy: 0.297711.\n",
      "Iteration 23315: Policy loss: 0.191420. Value loss: 0.071131. Entropy: 0.296998.\n",
      "Iteration 23316: Policy loss: 0.183839. Value loss: 0.047140. Entropy: 0.298030.\n",
      "episode: 7940   score: 410.0  epsilon: 1.0    steps: 640  evaluation reward: 430.95\n",
      "Training network. lr: 0.000071. clip: 0.028429\n",
      "Iteration 23317: Policy loss: -0.166950. Value loss: 0.298386. Entropy: 0.296797.\n",
      "Iteration 23318: Policy loss: -0.159604. Value loss: 0.220687. Entropy: 0.296938.\n",
      "Iteration 23319: Policy loss: -0.167870. Value loss: 0.203102. Entropy: 0.297067.\n",
      "episode: 7941   score: 410.0  epsilon: 1.0    steps: 696  evaluation reward: 432.2\n",
      "Training network. lr: 0.000071. clip: 0.028429\n",
      "Iteration 23320: Policy loss: 0.249816. Value loss: 0.110099. Entropy: 0.294826.\n",
      "Iteration 23321: Policy loss: 0.243716. Value loss: 0.044830. Entropy: 0.293431.\n",
      "Iteration 23322: Policy loss: 0.238081. Value loss: 0.029289. Entropy: 0.294251.\n",
      "Training network. lr: 0.000071. clip: 0.028429\n",
      "Iteration 23323: Policy loss: 0.330407. Value loss: 0.161128. Entropy: 0.311385.\n",
      "Iteration 23324: Policy loss: 0.331322. Value loss: 0.051597. Entropy: 0.310600.\n",
      "Iteration 23325: Policy loss: 0.329103. Value loss: 0.038979. Entropy: 0.311122.\n",
      "episode: 7942   score: 415.0  epsilon: 1.0    steps: 96  evaluation reward: 433.9\n",
      "episode: 7943   score: 285.0  epsilon: 1.0    steps: 480  evaluation reward: 434.65\n",
      "episode: 7944   score: 235.0  epsilon: 1.0    steps: 592  evaluation reward: 431.35\n",
      "episode: 7945   score: 485.0  epsilon: 1.0    steps: 912  evaluation reward: 431.35\n",
      "episode: 7946   score: 750.0  epsilon: 1.0    steps: 912  evaluation reward: 435.2\n",
      "Training network. lr: 0.000071. clip: 0.028429\n",
      "Iteration 23326: Policy loss: 0.050210. Value loss: 0.117525. Entropy: 0.260672.\n",
      "Iteration 23327: Policy loss: 0.039276. Value loss: 0.062894. Entropy: 0.259674.\n",
      "Iteration 23328: Policy loss: 0.048997. Value loss: 0.051092. Entropy: 0.261247.\n",
      "Training network. lr: 0.000071. clip: 0.028429\n",
      "Iteration 23329: Policy loss: -0.041107. Value loss: 0.091894. Entropy: 0.291413.\n",
      "Iteration 23330: Policy loss: -0.041866. Value loss: 0.045139. Entropy: 0.290483.\n",
      "Iteration 23331: Policy loss: -0.041318. Value loss: 0.039059. Entropy: 0.291563.\n",
      "Training network. lr: 0.000071. clip: 0.028429\n",
      "Iteration 23332: Policy loss: -0.201014. Value loss: 0.304500. Entropy: 0.308853.\n",
      "Iteration 23333: Policy loss: -0.204886. Value loss: 0.227147. Entropy: 0.308800.\n",
      "Iteration 23334: Policy loss: -0.209556. Value loss: 0.185673. Entropy: 0.308354.\n",
      "episode: 7947   score: 210.0  epsilon: 1.0    steps: 184  evaluation reward: 433.35\n",
      "Training network. lr: 0.000071. clip: 0.028429\n",
      "Iteration 23335: Policy loss: -0.030406. Value loss: 0.095315. Entropy: 0.299054.\n",
      "Iteration 23336: Policy loss: -0.032233. Value loss: 0.049457. Entropy: 0.298614.\n",
      "Iteration 23337: Policy loss: -0.027416. Value loss: 0.039652. Entropy: 0.299429.\n",
      "episode: 7948   score: 420.0  epsilon: 1.0    steps: 408  evaluation reward: 434.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000071. clip: 0.028429\n",
      "Iteration 23338: Policy loss: 0.009231. Value loss: 0.275491. Entropy: 0.292225.\n",
      "Iteration 23339: Policy loss: -0.008692. Value loss: 0.254967. Entropy: 0.292215.\n",
      "Iteration 23340: Policy loss: -0.013980. Value loss: 0.243490. Entropy: 0.291013.\n",
      "Training network. lr: 0.000071. clip: 0.028429\n",
      "Iteration 23341: Policy loss: -0.106026. Value loss: 0.105918. Entropy: 0.308052.\n",
      "Iteration 23342: Policy loss: -0.104485. Value loss: 0.062995. Entropy: 0.307707.\n",
      "Iteration 23343: Policy loss: -0.113127. Value loss: 0.044891. Entropy: 0.307589.\n",
      "Training network. lr: 0.000071. clip: 0.028429\n",
      "Iteration 23344: Policy loss: 0.199950. Value loss: 0.172582. Entropy: 0.304775.\n",
      "Iteration 23345: Policy loss: 0.194223. Value loss: 0.080948. Entropy: 0.303038.\n",
      "Iteration 23346: Policy loss: 0.194898. Value loss: 0.058896. Entropy: 0.303080.\n",
      "episode: 7949   score: 340.0  epsilon: 1.0    steps: 64  evaluation reward: 435.35\n",
      "episode: 7950   score: 320.0  epsilon: 1.0    steps: 976  evaluation reward: 434.9\n",
      "Training network. lr: 0.000071. clip: 0.028429\n",
      "Iteration 23347: Policy loss: -0.219234. Value loss: 0.357522. Entropy: 0.292410.\n",
      "Iteration 23348: Policy loss: -0.209688. Value loss: 0.245819. Entropy: 0.290100.\n",
      "Iteration 23349: Policy loss: -0.230870. Value loss: 0.202177. Entropy: 0.291495.\n",
      "now time :  2019-09-06 14:19:04.163514\n",
      "episode: 7951   score: 415.0  epsilon: 1.0    steps: 640  evaluation reward: 436.6\n",
      "Training network. lr: 0.000071. clip: 0.028429\n",
      "Iteration 23350: Policy loss: -0.603510. Value loss: 0.317373. Entropy: 0.284814.\n",
      "Iteration 23351: Policy loss: -0.628481. Value loss: 0.245389. Entropy: 0.284832.\n",
      "Iteration 23352: Policy loss: -0.639868. Value loss: 0.190919. Entropy: 0.285839.\n",
      "episode: 7952   score: 645.0  epsilon: 1.0    steps: 928  evaluation reward: 437.45\n",
      "Training network. lr: 0.000071. clip: 0.028272\n",
      "Iteration 23353: Policy loss: -0.048831. Value loss: 0.114174. Entropy: 0.303074.\n",
      "Iteration 23354: Policy loss: -0.046253. Value loss: 0.063452. Entropy: 0.302980.\n",
      "Iteration 23355: Policy loss: -0.058242. Value loss: 0.047459. Entropy: 0.302375.\n",
      "episode: 7953   score: 910.0  epsilon: 1.0    steps: 280  evaluation reward: 443.25\n",
      "Training network. lr: 0.000071. clip: 0.028272\n",
      "Iteration 23356: Policy loss: 0.114059. Value loss: 0.193613. Entropy: 0.285505.\n",
      "Iteration 23357: Policy loss: 0.113796. Value loss: 0.062271. Entropy: 0.285493.\n",
      "Iteration 23358: Policy loss: 0.122927. Value loss: 0.033439. Entropy: 0.286109.\n",
      "episode: 7954   score: 380.0  epsilon: 1.0    steps: 384  evaluation reward: 443.25\n",
      "Training network. lr: 0.000071. clip: 0.028272\n",
      "Iteration 23359: Policy loss: -0.258707. Value loss: 0.325797. Entropy: 0.299309.\n",
      "Iteration 23360: Policy loss: -0.254988. Value loss: 0.186563. Entropy: 0.298878.\n",
      "Iteration 23361: Policy loss: -0.264474. Value loss: 0.106116. Entropy: 0.299181.\n",
      "episode: 7955   score: 695.0  epsilon: 1.0    steps: 256  evaluation reward: 446.75\n",
      "episode: 7956   score: 670.0  epsilon: 1.0    steps: 416  evaluation reward: 447.75\n",
      "Training network. lr: 0.000071. clip: 0.028272\n",
      "Iteration 23362: Policy loss: -0.043530. Value loss: 0.148800. Entropy: 0.280416.\n",
      "Iteration 23363: Policy loss: -0.046516. Value loss: 0.070410. Entropy: 0.279838.\n",
      "Iteration 23364: Policy loss: -0.041584. Value loss: 0.042895. Entropy: 0.279783.\n",
      "Training network. lr: 0.000071. clip: 0.028272\n",
      "Iteration 23365: Policy loss: 0.215249. Value loss: 0.158771. Entropy: 0.304355.\n",
      "Iteration 23366: Policy loss: 0.216237. Value loss: 0.057658. Entropy: 0.304607.\n",
      "Iteration 23367: Policy loss: 0.212556. Value loss: 0.041185. Entropy: 0.304454.\n",
      "Training network. lr: 0.000071. clip: 0.028272\n",
      "Iteration 23368: Policy loss: 0.026022. Value loss: 0.097916. Entropy: 0.305371.\n",
      "Iteration 23369: Policy loss: 0.028713. Value loss: 0.055442. Entropy: 0.304753.\n",
      "Iteration 23370: Policy loss: 0.021651. Value loss: 0.042669. Entropy: 0.305569.\n",
      "Training network. lr: 0.000071. clip: 0.028272\n",
      "Iteration 23371: Policy loss: -0.014761. Value loss: 0.357178. Entropy: 0.315408.\n",
      "Iteration 23372: Policy loss: -0.052501. Value loss: 0.312999. Entropy: 0.314295.\n",
      "Iteration 23373: Policy loss: -0.048521. Value loss: 0.293073. Entropy: 0.314758.\n",
      "episode: 7957   score: 360.0  epsilon: 1.0    steps: 272  evaluation reward: 445.0\n",
      "episode: 7958   score: 445.0  epsilon: 1.0    steps: 936  evaluation reward: 443.55\n",
      "Training network. lr: 0.000071. clip: 0.028272\n",
      "Iteration 23374: Policy loss: 0.118372. Value loss: 0.098004. Entropy: 0.289655.\n",
      "Iteration 23375: Policy loss: 0.114948. Value loss: 0.055589. Entropy: 0.289327.\n",
      "Iteration 23376: Policy loss: 0.114874. Value loss: 0.042145. Entropy: 0.291095.\n",
      "episode: 7959   score: 290.0  epsilon: 1.0    steps: 864  evaluation reward: 441.65\n",
      "Training network. lr: 0.000071. clip: 0.028272\n",
      "Iteration 23377: Policy loss: 0.127081. Value loss: 0.107763. Entropy: 0.297636.\n",
      "Iteration 23378: Policy loss: 0.121802. Value loss: 0.051894. Entropy: 0.296743.\n",
      "Iteration 23379: Policy loss: 0.127543. Value loss: 0.035539. Entropy: 0.296546.\n",
      "episode: 7960   score: 415.0  epsilon: 1.0    steps: 576  evaluation reward: 442.45\n",
      "episode: 7961   score: 585.0  epsilon: 1.0    steps: 640  evaluation reward: 445.5\n",
      "Training network. lr: 0.000071. clip: 0.028272\n",
      "Iteration 23380: Policy loss: 0.166204. Value loss: 0.106219. Entropy: 0.277339.\n",
      "Iteration 23381: Policy loss: 0.167883. Value loss: 0.054511. Entropy: 0.275407.\n",
      "Iteration 23382: Policy loss: 0.161228. Value loss: 0.036793. Entropy: 0.277289.\n",
      "Training network. lr: 0.000071. clip: 0.028272\n",
      "Iteration 23383: Policy loss: 0.118536. Value loss: 0.117933. Entropy: 0.314486.\n",
      "Iteration 23384: Policy loss: 0.114102. Value loss: 0.043273. Entropy: 0.313433.\n",
      "Iteration 23385: Policy loss: 0.120215. Value loss: 0.028845. Entropy: 0.313750.\n",
      "Training network. lr: 0.000071. clip: 0.028272\n",
      "Iteration 23386: Policy loss: 0.124646. Value loss: 0.096005. Entropy: 0.311775.\n",
      "Iteration 23387: Policy loss: 0.119830. Value loss: 0.050673. Entropy: 0.310843.\n",
      "Iteration 23388: Policy loss: 0.117454. Value loss: 0.035256. Entropy: 0.310741.\n",
      "episode: 7962   score: 420.0  epsilon: 1.0    steps: 56  evaluation reward: 446.1\n",
      "Training network. lr: 0.000071. clip: 0.028272\n",
      "Iteration 23389: Policy loss: -0.393335. Value loss: 0.644388. Entropy: 0.289971.\n",
      "Iteration 23390: Policy loss: -0.440360. Value loss: 0.494317. Entropy: 0.288374.\n",
      "Iteration 23391: Policy loss: -0.469141. Value loss: 0.434347. Entropy: 0.288165.\n",
      "episode: 7963   score: 420.0  epsilon: 1.0    steps: 472  evaluation reward: 447.2\n",
      "Training network. lr: 0.000071. clip: 0.028272\n",
      "Iteration 23392: Policy loss: -0.054064. Value loss: 0.126792. Entropy: 0.292042.\n",
      "Iteration 23393: Policy loss: -0.049373. Value loss: 0.062280. Entropy: 0.291825.\n",
      "Iteration 23394: Policy loss: -0.051360. Value loss: 0.040350. Entropy: 0.291311.\n",
      "episode: 7964   score: 700.0  epsilon: 1.0    steps: 688  evaluation reward: 451.15\n",
      "Training network. lr: 0.000071. clip: 0.028272\n",
      "Iteration 23395: Policy loss: -0.155204. Value loss: 0.366628. Entropy: 0.287179.\n",
      "Iteration 23396: Policy loss: -0.153997. Value loss: 0.233816. Entropy: 0.283605.\n",
      "Iteration 23397: Policy loss: -0.155347. Value loss: 0.189789. Entropy: 0.283712.\n",
      "episode: 7965   score: 555.0  epsilon: 1.0    steps: 752  evaluation reward: 453.35\n",
      "Training network. lr: 0.000071. clip: 0.028272\n",
      "Iteration 23398: Policy loss: 0.186613. Value loss: 0.121899. Entropy: 0.284878.\n",
      "Iteration 23399: Policy loss: 0.178549. Value loss: 0.048885. Entropy: 0.284907.\n",
      "Iteration 23400: Policy loss: 0.180310. Value loss: 0.036857. Entropy: 0.285340.\n",
      "episode: 7966   score: 495.0  epsilon: 1.0    steps: 296  evaluation reward: 454.3\n",
      "Training network. lr: 0.000070. clip: 0.028124\n",
      "Iteration 23401: Policy loss: -0.289041. Value loss: 0.215607. Entropy: 0.295219.\n",
      "Iteration 23402: Policy loss: -0.306828. Value loss: 0.156180. Entropy: 0.295120.\n",
      "Iteration 23403: Policy loss: -0.310801. Value loss: 0.126842. Entropy: 0.295361.\n",
      "episode: 7967   score: 435.0  epsilon: 1.0    steps: 800  evaluation reward: 454.65\n",
      "Training network. lr: 0.000070. clip: 0.028124\n",
      "Iteration 23404: Policy loss: 0.221870. Value loss: 0.208433. Entropy: 0.299427.\n",
      "Iteration 23405: Policy loss: 0.208274. Value loss: 0.096924. Entropy: 0.298742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23406: Policy loss: 0.213893. Value loss: 0.058949. Entropy: 0.298246.\n",
      "Training network. lr: 0.000070. clip: 0.028124\n",
      "Iteration 23407: Policy loss: 0.238223. Value loss: 0.135617. Entropy: 0.307958.\n",
      "Iteration 23408: Policy loss: 0.240513. Value loss: 0.056705. Entropy: 0.306876.\n",
      "Iteration 23409: Policy loss: 0.231511. Value loss: 0.039717. Entropy: 0.306737.\n",
      "episode: 7968   score: 630.0  epsilon: 1.0    steps: 504  evaluation reward: 457.3\n",
      "Training network. lr: 0.000070. clip: 0.028124\n",
      "Iteration 23410: Policy loss: -0.344593. Value loss: 0.344050. Entropy: 0.297919.\n",
      "Iteration 23411: Policy loss: -0.379908. Value loss: 0.250578. Entropy: 0.297257.\n",
      "Iteration 23412: Policy loss: -0.380524. Value loss: 0.196173. Entropy: 0.297588.\n",
      "episode: 7969   score: 480.0  epsilon: 1.0    steps: 256  evaluation reward: 455.45\n",
      "episode: 7970   score: 610.0  epsilon: 1.0    steps: 696  evaluation reward: 453.7\n",
      "Training network. lr: 0.000070. clip: 0.028124\n",
      "Iteration 23413: Policy loss: 0.035900. Value loss: 0.102501. Entropy: 0.280815.\n",
      "Iteration 23414: Policy loss: 0.037779. Value loss: 0.050189. Entropy: 0.281228.\n",
      "Iteration 23415: Policy loss: 0.031001. Value loss: 0.039986. Entropy: 0.280725.\n",
      "Training network. lr: 0.000070. clip: 0.028124\n",
      "Iteration 23416: Policy loss: 0.243072. Value loss: 0.166894. Entropy: 0.312294.\n",
      "Iteration 23417: Policy loss: 0.241976. Value loss: 0.080079. Entropy: 0.312520.\n",
      "Iteration 23418: Policy loss: 0.246120. Value loss: 0.060250. Entropy: 0.311646.\n",
      "episode: 7971   score: 385.0  epsilon: 1.0    steps: 760  evaluation reward: 454.25\n",
      "Training network. lr: 0.000070. clip: 0.028124\n",
      "Iteration 23419: Policy loss: -0.246687. Value loss: 0.342726. Entropy: 0.298573.\n",
      "Iteration 23420: Policy loss: -0.273649. Value loss: 0.206262. Entropy: 0.297661.\n",
      "Iteration 23421: Policy loss: -0.282506. Value loss: 0.141339. Entropy: 0.299251.\n",
      "episode: 7972   score: 390.0  epsilon: 1.0    steps: 8  evaluation reward: 455.55\n",
      "Training network. lr: 0.000070. clip: 0.028124\n",
      "Iteration 23422: Policy loss: 0.236847. Value loss: 0.145387. Entropy: 0.290106.\n",
      "Iteration 23423: Policy loss: 0.224404. Value loss: 0.059385. Entropy: 0.289548.\n",
      "Iteration 23424: Policy loss: 0.229148. Value loss: 0.041079. Entropy: 0.290467.\n",
      "episode: 7973   score: 870.0  epsilon: 1.0    steps: 696  evaluation reward: 459.1\n",
      "Training network. lr: 0.000070. clip: 0.028124\n",
      "Iteration 23425: Policy loss: 0.073441. Value loss: 0.090046. Entropy: 0.299002.\n",
      "Iteration 23426: Policy loss: 0.074037. Value loss: 0.051126. Entropy: 0.299822.\n",
      "Iteration 23427: Policy loss: 0.075167. Value loss: 0.037775. Entropy: 0.299668.\n",
      "Training network. lr: 0.000070. clip: 0.028124\n",
      "Iteration 23428: Policy loss: 0.046807. Value loss: 0.078355. Entropy: 0.310264.\n",
      "Iteration 23429: Policy loss: 0.035147. Value loss: 0.036141. Entropy: 0.309654.\n",
      "Iteration 23430: Policy loss: 0.036276. Value loss: 0.029163. Entropy: 0.308550.\n",
      "episode: 7974   score: 465.0  epsilon: 1.0    steps: 168  evaluation reward: 460.35\n",
      "episode: 7975   score: 290.0  epsilon: 1.0    steps: 304  evaluation reward: 459.6\n",
      "Training network. lr: 0.000070. clip: 0.028124\n",
      "Iteration 23431: Policy loss: 0.050856. Value loss: 0.127178. Entropy: 0.283957.\n",
      "Iteration 23432: Policy loss: 0.045299. Value loss: 0.054108. Entropy: 0.283029.\n",
      "Iteration 23433: Policy loss: 0.041893. Value loss: 0.041610. Entropy: 0.283329.\n",
      "episode: 7976   score: 730.0  epsilon: 1.0    steps: 672  evaluation reward: 463.0\n",
      "Training network. lr: 0.000070. clip: 0.028124\n",
      "Iteration 23434: Policy loss: -0.072867. Value loss: 0.132250. Entropy: 0.291569.\n",
      "Iteration 23435: Policy loss: -0.064269. Value loss: 0.070326. Entropy: 0.291926.\n",
      "Iteration 23436: Policy loss: -0.073087. Value loss: 0.052889. Entropy: 0.292567.\n",
      "episode: 7977   score: 480.0  epsilon: 1.0    steps: 184  evaluation reward: 466.3\n",
      "episode: 7978   score: 460.0  epsilon: 1.0    steps: 960  evaluation reward: 465.2\n",
      "Training network. lr: 0.000070. clip: 0.028124\n",
      "Iteration 23437: Policy loss: 0.088904. Value loss: 0.090910. Entropy: 0.291935.\n",
      "Iteration 23438: Policy loss: 0.092405. Value loss: 0.050385. Entropy: 0.292350.\n",
      "Iteration 23439: Policy loss: 0.088893. Value loss: 0.043973. Entropy: 0.293134.\n",
      "episode: 7979   score: 345.0  epsilon: 1.0    steps: 832  evaluation reward: 465.95\n",
      "Training network. lr: 0.000070. clip: 0.028124\n",
      "Iteration 23440: Policy loss: 0.218468. Value loss: 0.106576. Entropy: 0.294951.\n",
      "Iteration 23441: Policy loss: 0.210177. Value loss: 0.038583. Entropy: 0.293598.\n",
      "Iteration 23442: Policy loss: 0.206602. Value loss: 0.030728. Entropy: 0.294736.\n",
      "Training network. lr: 0.000070. clip: 0.028124\n",
      "Iteration 23443: Policy loss: -0.241885. Value loss: 0.366893. Entropy: 0.300090.\n",
      "Iteration 23444: Policy loss: -0.245854. Value loss: 0.270080. Entropy: 0.299636.\n",
      "Iteration 23445: Policy loss: -0.216516. Value loss: 0.194490. Entropy: 0.299981.\n",
      "Training network. lr: 0.000070. clip: 0.028124\n",
      "Iteration 23446: Policy loss: 0.403845. Value loss: 0.132666. Entropy: 0.312595.\n",
      "Iteration 23447: Policy loss: 0.404930. Value loss: 0.060143. Entropy: 0.311573.\n",
      "Iteration 23448: Policy loss: 0.405179. Value loss: 0.043896. Entropy: 0.311857.\n",
      "Training network. lr: 0.000070. clip: 0.028124\n",
      "Iteration 23449: Policy loss: 0.083923. Value loss: 0.081200. Entropy: 0.306807.\n",
      "Iteration 23450: Policy loss: 0.090241. Value loss: 0.043550. Entropy: 0.305577.\n",
      "Iteration 23451: Policy loss: 0.084513. Value loss: 0.031824. Entropy: 0.306127.\n",
      "episode: 7980   score: 345.0  epsilon: 1.0    steps: 472  evaluation reward: 466.25\n",
      "Training network. lr: 0.000070. clip: 0.027968\n",
      "Iteration 23452: Policy loss: 0.135472. Value loss: 0.103971. Entropy: 0.296800.\n",
      "Iteration 23453: Policy loss: 0.138130. Value loss: 0.042787. Entropy: 0.296910.\n",
      "Iteration 23454: Policy loss: 0.127604. Value loss: 0.027585. Entropy: 0.297199.\n",
      "episode: 7981   score: 395.0  epsilon: 1.0    steps: 264  evaluation reward: 466.75\n",
      "episode: 7982   score: 530.0  epsilon: 1.0    steps: 760  evaluation reward: 469.2\n",
      "Training network. lr: 0.000070. clip: 0.027968\n",
      "Iteration 23455: Policy loss: -0.153167. Value loss: 0.363081. Entropy: 0.283910.\n",
      "Iteration 23456: Policy loss: -0.156424. Value loss: 0.242937. Entropy: 0.283297.\n",
      "Iteration 23457: Policy loss: -0.161629. Value loss: 0.197991. Entropy: 0.281927.\n",
      "Training network. lr: 0.000070. clip: 0.027968\n",
      "Iteration 23458: Policy loss: 0.023086. Value loss: 0.132900. Entropy: 0.308096.\n",
      "Iteration 23459: Policy loss: 0.021029. Value loss: 0.062383. Entropy: 0.308349.\n",
      "Iteration 23460: Policy loss: 0.012487. Value loss: 0.039815. Entropy: 0.308410.\n",
      "episode: 7983   score: 375.0  epsilon: 1.0    steps: 408  evaluation reward: 468.9\n",
      "episode: 7984   score: 345.0  epsilon: 1.0    steps: 760  evaluation reward: 469.4\n",
      "Training network. lr: 0.000070. clip: 0.027968\n",
      "Iteration 23461: Policy loss: 0.148039. Value loss: 0.220880. Entropy: 0.276698.\n",
      "Iteration 23462: Policy loss: 0.132515. Value loss: 0.089132. Entropy: 0.276849.\n",
      "Iteration 23463: Policy loss: 0.134940. Value loss: 0.061441. Entropy: 0.276506.\n",
      "Training network. lr: 0.000070. clip: 0.027968\n",
      "Iteration 23464: Policy loss: -0.063426. Value loss: 0.114815. Entropy: 0.305779.\n",
      "Iteration 23465: Policy loss: -0.064839. Value loss: 0.055466. Entropy: 0.305011.\n",
      "Iteration 23466: Policy loss: -0.073346. Value loss: 0.039384. Entropy: 0.306849.\n",
      "episode: 7985   score: 695.0  epsilon: 1.0    steps: 344  evaluation reward: 472.6\n",
      "episode: 7986   score: 610.0  epsilon: 1.0    steps: 904  evaluation reward: 475.85\n",
      "Training network. lr: 0.000070. clip: 0.027968\n",
      "Iteration 23467: Policy loss: 0.072216. Value loss: 0.129551. Entropy: 0.290436.\n",
      "Iteration 23468: Policy loss: 0.069665. Value loss: 0.068286. Entropy: 0.289413.\n",
      "Iteration 23469: Policy loss: 0.065882. Value loss: 0.052995. Entropy: 0.290031.\n",
      "Training network. lr: 0.000070. clip: 0.027968\n",
      "Iteration 23470: Policy loss: -0.364390. Value loss: 0.273292. Entropy: 0.301208.\n",
      "Iteration 23471: Policy loss: -0.383433. Value loss: 0.180904. Entropy: 0.301721.\n",
      "Iteration 23472: Policy loss: -0.385179. Value loss: 0.088142. Entropy: 0.301830.\n",
      "Training network. lr: 0.000070. clip: 0.027968\n",
      "Iteration 23473: Policy loss: -0.126623. Value loss: 0.350942. Entropy: 0.307518.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23474: Policy loss: -0.131655. Value loss: 0.212009. Entropy: 0.307723.\n",
      "Iteration 23475: Policy loss: -0.139122. Value loss: 0.158638. Entropy: 0.307962.\n",
      "episode: 7987   score: 315.0  epsilon: 1.0    steps: 256  evaluation reward: 475.1\n",
      "episode: 7988   score: 800.0  epsilon: 1.0    steps: 392  evaluation reward: 477.95\n",
      "Training network. lr: 0.000070. clip: 0.027968\n",
      "Iteration 23476: Policy loss: -0.458143. Value loss: 0.547460. Entropy: 0.280922.\n",
      "Iteration 23477: Policy loss: -0.469707. Value loss: 0.285743. Entropy: 0.281741.\n",
      "Iteration 23478: Policy loss: -0.467643. Value loss: 0.158620. Entropy: 0.282432.\n",
      "episode: 7989   score: 590.0  epsilon: 1.0    steps: 736  evaluation reward: 478.5\n",
      "Training network. lr: 0.000070. clip: 0.027968\n",
      "Iteration 23479: Policy loss: 0.245812. Value loss: 0.117430. Entropy: 0.298815.\n",
      "Iteration 23480: Policy loss: 0.246248. Value loss: 0.061347. Entropy: 0.298988.\n",
      "Iteration 23481: Policy loss: 0.248889. Value loss: 0.047474. Entropy: 0.298010.\n",
      "Training network. lr: 0.000070. clip: 0.027968\n",
      "Iteration 23482: Policy loss: -0.041089. Value loss: 0.156504. Entropy: 0.306924.\n",
      "Iteration 23483: Policy loss: -0.036443. Value loss: 0.091929. Entropy: 0.308275.\n",
      "Iteration 23484: Policy loss: -0.039317. Value loss: 0.069182. Entropy: 0.307519.\n",
      "episode: 7990   score: 580.0  epsilon: 1.0    steps: 128  evaluation reward: 479.35\n",
      "episode: 7991   score: 390.0  epsilon: 1.0    steps: 136  evaluation reward: 478.1\n",
      "Training network. lr: 0.000070. clip: 0.027968\n",
      "Iteration 23485: Policy loss: -0.061526. Value loss: 0.161601. Entropy: 0.276105.\n",
      "Iteration 23486: Policy loss: -0.055915. Value loss: 0.052924. Entropy: 0.274117.\n",
      "Iteration 23487: Policy loss: -0.057939. Value loss: 0.041564. Entropy: 0.274476.\n",
      "episode: 7992   score: 870.0  epsilon: 1.0    steps: 272  evaluation reward: 479.55\n",
      "Training network. lr: 0.000070. clip: 0.027968\n",
      "Iteration 23488: Policy loss: -0.178181. Value loss: 0.242399. Entropy: 0.294164.\n",
      "Iteration 23489: Policy loss: -0.188925. Value loss: 0.136470. Entropy: 0.293747.\n",
      "Iteration 23490: Policy loss: -0.194654. Value loss: 0.084930. Entropy: 0.293555.\n",
      "episode: 7993   score: 560.0  epsilon: 1.0    steps: 488  evaluation reward: 480.9\n",
      "Training network. lr: 0.000070. clip: 0.027968\n",
      "Iteration 23491: Policy loss: -0.058257. Value loss: 0.078269. Entropy: 0.295334.\n",
      "Iteration 23492: Policy loss: -0.056154. Value loss: 0.037330. Entropy: 0.295196.\n",
      "Iteration 23493: Policy loss: -0.056984. Value loss: 0.029410. Entropy: 0.294987.\n",
      "episode: 7994   score: 315.0  epsilon: 1.0    steps: 896  evaluation reward: 479.1\n",
      "Training network. lr: 0.000070. clip: 0.027968\n",
      "Iteration 23494: Policy loss: 0.252535. Value loss: 0.351056. Entropy: 0.304106.\n",
      "Iteration 23495: Policy loss: 0.244918. Value loss: 0.144322. Entropy: 0.304378.\n",
      "Iteration 23496: Policy loss: 0.244749. Value loss: 0.087658. Entropy: 0.302952.\n",
      "episode: 7995   score: 370.0  epsilon: 1.0    steps: 936  evaluation reward: 478.6\n",
      "Training network. lr: 0.000070. clip: 0.027968\n",
      "Iteration 23497: Policy loss: 0.378627. Value loss: 0.174486. Entropy: 0.303050.\n",
      "Iteration 23498: Policy loss: 0.379470. Value loss: 0.070967. Entropy: 0.302793.\n",
      "Iteration 23499: Policy loss: 0.369521. Value loss: 0.048102. Entropy: 0.303351.\n",
      "episode: 7996   score: 515.0  epsilon: 1.0    steps: 768  evaluation reward: 479.55\n",
      "Training network. lr: 0.000070. clip: 0.027968\n",
      "Iteration 23500: Policy loss: 0.239683. Value loss: 0.219975. Entropy: 0.292550.\n",
      "Iteration 23501: Policy loss: 0.240124. Value loss: 0.087946. Entropy: 0.293290.\n",
      "Iteration 23502: Policy loss: 0.233506. Value loss: 0.059863. Entropy: 0.292916.\n",
      "Training network. lr: 0.000070. clip: 0.027811\n",
      "Iteration 23503: Policy loss: 0.250063. Value loss: 0.108403. Entropy: 0.312191.\n",
      "Iteration 23504: Policy loss: 0.244548. Value loss: 0.048396. Entropy: 0.311095.\n",
      "Iteration 23505: Policy loss: 0.246049. Value loss: 0.034061. Entropy: 0.311229.\n",
      "Training network. lr: 0.000070. clip: 0.027811\n",
      "Iteration 23506: Policy loss: 0.064906. Value loss: 0.136953. Entropy: 0.311410.\n",
      "Iteration 23507: Policy loss: 0.063920. Value loss: 0.071605. Entropy: 0.311041.\n",
      "Iteration 23508: Policy loss: 0.060472. Value loss: 0.044621. Entropy: 0.310622.\n",
      "episode: 7997   score: 330.0  epsilon: 1.0    steps: 120  evaluation reward: 477.1\n",
      "episode: 7998   score: 620.0  epsilon: 1.0    steps: 176  evaluation reward: 479.45\n",
      "Training network. lr: 0.000070. clip: 0.027811\n",
      "Iteration 23509: Policy loss: 0.364668. Value loss: 0.233502. Entropy: 0.279734.\n",
      "Iteration 23510: Policy loss: 0.352472. Value loss: 0.073930. Entropy: 0.278738.\n",
      "Iteration 23511: Policy loss: 0.334415. Value loss: 0.040916. Entropy: 0.277587.\n",
      "Training network. lr: 0.000070. clip: 0.027811\n",
      "Iteration 23512: Policy loss: 0.019025. Value loss: 0.144701. Entropy: 0.309833.\n",
      "Iteration 23513: Policy loss: 0.014650. Value loss: 0.063675. Entropy: 0.309785.\n",
      "Iteration 23514: Policy loss: 0.003252. Value loss: 0.045693. Entropy: 0.309216.\n",
      "episode: 7999   score: 450.0  epsilon: 1.0    steps: 720  evaluation reward: 478.95\n",
      "episode: 8000   score: 725.0  epsilon: 1.0    steps: 848  evaluation reward: 478.65\n",
      "Training network. lr: 0.000070. clip: 0.027811\n",
      "Iteration 23515: Policy loss: 0.066541. Value loss: 0.142234. Entropy: 0.283463.\n",
      "Iteration 23516: Policy loss: 0.067625. Value loss: 0.061949. Entropy: 0.285174.\n",
      "Iteration 23517: Policy loss: 0.062982. Value loss: 0.048080. Entropy: 0.284420.\n",
      "Training network. lr: 0.000070. clip: 0.027811\n",
      "Iteration 23518: Policy loss: -0.073813. Value loss: 0.321934. Entropy: 0.303928.\n",
      "Iteration 23519: Policy loss: -0.093123. Value loss: 0.170766. Entropy: 0.303676.\n",
      "Iteration 23520: Policy loss: -0.106025. Value loss: 0.114111. Entropy: 0.303728.\n",
      "now time :  2019-09-06 14:29:41.222499\n",
      "episode: 8001   score: 380.0  epsilon: 1.0    steps: 136  evaluation reward: 478.5\n",
      "episode: 8002   score: 330.0  epsilon: 1.0    steps: 792  evaluation reward: 477.9\n",
      "Training network. lr: 0.000070. clip: 0.027811\n",
      "Iteration 23521: Policy loss: 0.316503. Value loss: 0.141181. Entropy: 0.285548.\n",
      "Iteration 23522: Policy loss: 0.320814. Value loss: 0.100658. Entropy: 0.285648.\n",
      "Iteration 23523: Policy loss: 0.319459. Value loss: 0.083031. Entropy: 0.285623.\n",
      "Training network. lr: 0.000070. clip: 0.027811\n",
      "Iteration 23524: Policy loss: 0.199694. Value loss: 0.131938. Entropy: 0.309828.\n",
      "Iteration 23525: Policy loss: 0.195122. Value loss: 0.060954. Entropy: 0.309479.\n",
      "Iteration 23526: Policy loss: 0.192223. Value loss: 0.047886. Entropy: 0.308944.\n",
      "episode: 8003   score: 615.0  epsilon: 1.0    steps: 488  evaluation reward: 477.65\n",
      "Training network. lr: 0.000070. clip: 0.027811\n",
      "Iteration 23527: Policy loss: -0.101509. Value loss: 0.121140. Entropy: 0.297296.\n",
      "Iteration 23528: Policy loss: -0.094503. Value loss: 0.062228. Entropy: 0.296626.\n",
      "Iteration 23529: Policy loss: -0.103415. Value loss: 0.047431. Entropy: 0.295712.\n",
      "episode: 8004   score: 480.0  epsilon: 1.0    steps: 56  evaluation reward: 478.15\n",
      "episode: 8005   score: 525.0  epsilon: 1.0    steps: 488  evaluation reward: 479.6\n",
      "episode: 8006   score: 425.0  epsilon: 1.0    steps: 920  evaluation reward: 480.45\n",
      "Training network. lr: 0.000070. clip: 0.027811\n",
      "Iteration 23530: Policy loss: 0.068181. Value loss: 0.306949. Entropy: 0.270527.\n",
      "Iteration 23531: Policy loss: 0.067977. Value loss: 0.109258. Entropy: 0.269672.\n",
      "Iteration 23532: Policy loss: 0.065577. Value loss: 0.068435. Entropy: 0.270148.\n",
      "Training network. lr: 0.000070. clip: 0.027811\n",
      "Iteration 23533: Policy loss: -0.005118. Value loss: 0.147180. Entropy: 0.297499.\n",
      "Iteration 23534: Policy loss: -0.015875. Value loss: 0.083162. Entropy: 0.298025.\n",
      "Iteration 23535: Policy loss: -0.015151. Value loss: 0.060587. Entropy: 0.296664.\n",
      "episode: 8007   score: 345.0  epsilon: 1.0    steps: 88  evaluation reward: 479.85\n",
      "episode: 8008   score: 325.0  epsilon: 1.0    steps: 648  evaluation reward: 478.9\n",
      "Training network. lr: 0.000070. clip: 0.027811\n",
      "Iteration 23536: Policy loss: 0.128522. Value loss: 0.103461. Entropy: 0.276495.\n",
      "Iteration 23537: Policy loss: 0.124696. Value loss: 0.053233. Entropy: 0.277062.\n",
      "Iteration 23538: Policy loss: 0.125492. Value loss: 0.041477. Entropy: 0.276728.\n",
      "Training network. lr: 0.000070. clip: 0.027811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23539: Policy loss: -0.168349. Value loss: 0.166368. Entropy: 0.309825.\n",
      "Iteration 23540: Policy loss: -0.164324. Value loss: 0.066124. Entropy: 0.309149.\n",
      "Iteration 23541: Policy loss: -0.174824. Value loss: 0.045470. Entropy: 0.309730.\n",
      "Training network. lr: 0.000070. clip: 0.027811\n",
      "Iteration 23542: Policy loss: 0.115452. Value loss: 0.091419. Entropy: 0.309744.\n",
      "Iteration 23543: Policy loss: 0.111914. Value loss: 0.052622. Entropy: 0.309820.\n",
      "Iteration 23544: Policy loss: 0.112730. Value loss: 0.041876. Entropy: 0.309991.\n",
      "episode: 8009   score: 575.0  epsilon: 1.0    steps: 160  evaluation reward: 480.25\n",
      "Training network. lr: 0.000070. clip: 0.027811\n",
      "Iteration 23545: Policy loss: 0.238350. Value loss: 0.138801. Entropy: 0.297237.\n",
      "Iteration 23546: Policy loss: 0.239158. Value loss: 0.051399. Entropy: 0.298272.\n",
      "Iteration 23547: Policy loss: 0.235514. Value loss: 0.035668. Entropy: 0.297977.\n",
      "Training network. lr: 0.000070. clip: 0.027811\n",
      "Iteration 23548: Policy loss: -0.047945. Value loss: 0.092508. Entropy: 0.311878.\n",
      "Iteration 23549: Policy loss: -0.056864. Value loss: 0.037705. Entropy: 0.311416.\n",
      "Iteration 23550: Policy loss: -0.057735. Value loss: 0.029583. Entropy: 0.311691.\n",
      "episode: 8010   score: 335.0  epsilon: 1.0    steps: 480  evaluation reward: 479.3\n",
      "episode: 8011   score: 285.0  epsilon: 1.0    steps: 848  evaluation reward: 478.65\n",
      "episode: 8012   score: 395.0  epsilon: 1.0    steps: 952  evaluation reward: 479.45\n",
      "Training network. lr: 0.000069. clip: 0.027664\n",
      "Iteration 23551: Policy loss: 0.105631. Value loss: 0.099213. Entropy: 0.286973.\n",
      "Iteration 23552: Policy loss: 0.102172. Value loss: 0.057605. Entropy: 0.286528.\n",
      "Iteration 23553: Policy loss: 0.099128. Value loss: 0.046352. Entropy: 0.286502.\n",
      "episode: 8013   score: 565.0  epsilon: 1.0    steps: 736  evaluation reward: 479.35\n",
      "Training network. lr: 0.000069. clip: 0.027664\n",
      "Iteration 23554: Policy loss: -0.110827. Value loss: 0.124186. Entropy: 0.278976.\n",
      "Iteration 23555: Policy loss: -0.118336. Value loss: 0.055886. Entropy: 0.280599.\n",
      "Iteration 23556: Policy loss: -0.123611. Value loss: 0.040530. Entropy: 0.279435.\n",
      "Training network. lr: 0.000069. clip: 0.027664\n",
      "Iteration 23557: Policy loss: 0.057003. Value loss: 0.145753. Entropy: 0.294003.\n",
      "Iteration 23558: Policy loss: 0.054471. Value loss: 0.068635. Entropy: 0.296776.\n",
      "Iteration 23559: Policy loss: 0.047274. Value loss: 0.052412. Entropy: 0.295790.\n",
      "Training network. lr: 0.000069. clip: 0.027664\n",
      "Iteration 23560: Policy loss: 0.290394. Value loss: 0.111173. Entropy: 0.314653.\n",
      "Iteration 23561: Policy loss: 0.289920. Value loss: 0.041114. Entropy: 0.314215.\n",
      "Iteration 23562: Policy loss: 0.291447. Value loss: 0.028904. Entropy: 0.313163.\n",
      "Training network. lr: 0.000069. clip: 0.027664\n",
      "Iteration 23563: Policy loss: 0.097267. Value loss: 0.087948. Entropy: 0.307685.\n",
      "Iteration 23564: Policy loss: 0.091446. Value loss: 0.034977. Entropy: 0.307825.\n",
      "Iteration 23565: Policy loss: 0.088432. Value loss: 0.025682. Entropy: 0.308077.\n",
      "Training network. lr: 0.000069. clip: 0.027664\n",
      "Iteration 23566: Policy loss: -0.437229. Value loss: 0.386331. Entropy: 0.309170.\n",
      "Iteration 23567: Policy loss: -0.423899. Value loss: 0.248291. Entropy: 0.309552.\n",
      "Iteration 23568: Policy loss: -0.441447. Value loss: 0.205781. Entropy: 0.308917.\n",
      "episode: 8014   score: 365.0  epsilon: 1.0    steps: 224  evaluation reward: 478.0\n",
      "episode: 8015   score: 540.0  epsilon: 1.0    steps: 552  evaluation reward: 480.25\n",
      "episode: 8016   score: 510.0  epsilon: 1.0    steps: 648  evaluation reward: 479.85\n",
      "Training network. lr: 0.000069. clip: 0.027664\n",
      "Iteration 23569: Policy loss: 0.122595. Value loss: 0.144205. Entropy: 0.270047.\n",
      "Iteration 23570: Policy loss: 0.115383. Value loss: 0.071860. Entropy: 0.268817.\n",
      "Iteration 23571: Policy loss: 0.116486. Value loss: 0.054213. Entropy: 0.269409.\n",
      "episode: 8017   score: 635.0  epsilon: 1.0    steps: 8  evaluation reward: 483.35\n",
      "Training network. lr: 0.000069. clip: 0.027664\n",
      "Iteration 23572: Policy loss: 0.004793. Value loss: 0.107576. Entropy: 0.292696.\n",
      "Iteration 23573: Policy loss: 0.006120. Value loss: 0.063214. Entropy: 0.293593.\n",
      "Iteration 23574: Policy loss: 0.000421. Value loss: 0.047045. Entropy: 0.292698.\n",
      "Training network. lr: 0.000069. clip: 0.027664\n",
      "Iteration 23575: Policy loss: 0.029477. Value loss: 0.085439. Entropy: 0.314865.\n",
      "Iteration 23576: Policy loss: 0.030654. Value loss: 0.053795. Entropy: 0.315235.\n",
      "Iteration 23577: Policy loss: 0.028027. Value loss: 0.040787. Entropy: 0.314739.\n",
      "episode: 8018   score: 605.0  epsilon: 1.0    steps: 8  evaluation reward: 482.9\n",
      "Training network. lr: 0.000069. clip: 0.027664\n",
      "Iteration 23578: Policy loss: -0.422714. Value loss: 0.531130. Entropy: 0.299445.\n",
      "Iteration 23579: Policy loss: -0.390206. Value loss: 0.290799. Entropy: 0.298380.\n",
      "Iteration 23580: Policy loss: -0.419619. Value loss: 0.192038. Entropy: 0.298434.\n",
      "Training network. lr: 0.000069. clip: 0.027664\n",
      "Iteration 23581: Policy loss: -0.316327. Value loss: 0.299123. Entropy: 0.308463.\n",
      "Iteration 23582: Policy loss: -0.317337. Value loss: 0.151204. Entropy: 0.307492.\n",
      "Iteration 23583: Policy loss: -0.314343. Value loss: 0.096719. Entropy: 0.308033.\n",
      "episode: 8019   score: 405.0  epsilon: 1.0    steps: 384  evaluation reward: 483.0\n",
      "Training network. lr: 0.000069. clip: 0.027664\n",
      "Iteration 23584: Policy loss: 0.141332. Value loss: 0.246787. Entropy: 0.290072.\n",
      "Iteration 23585: Policy loss: 0.149224. Value loss: 0.118668. Entropy: 0.290870.\n",
      "Iteration 23586: Policy loss: 0.142336. Value loss: 0.077391. Entropy: 0.290760.\n",
      "Training network. lr: 0.000069. clip: 0.027664\n",
      "Iteration 23587: Policy loss: 0.413758. Value loss: 0.242848. Entropy: 0.300756.\n",
      "Iteration 23588: Policy loss: 0.402661. Value loss: 0.119204. Entropy: 0.299946.\n",
      "Iteration 23589: Policy loss: 0.397166. Value loss: 0.075068. Entropy: 0.301430.\n",
      "episode: 8020   score: 465.0  epsilon: 1.0    steps: 88  evaluation reward: 483.85\n",
      "episode: 8021   score: 335.0  epsilon: 1.0    steps: 672  evaluation reward: 480.85\n",
      "episode: 8022   score: 390.0  epsilon: 1.0    steps: 904  evaluation reward: 478.6\n",
      "episode: 8023   score: 795.0  epsilon: 1.0    steps: 968  evaluation reward: 484.45\n",
      "Training network. lr: 0.000069. clip: 0.027664\n",
      "Iteration 23590: Policy loss: 0.036950. Value loss: 0.084188. Entropy: 0.280436.\n",
      "Iteration 23591: Policy loss: 0.037585. Value loss: 0.049560. Entropy: 0.281143.\n",
      "Iteration 23592: Policy loss: 0.032887. Value loss: 0.042066. Entropy: 0.280874.\n",
      "Training network. lr: 0.000069. clip: 0.027664\n",
      "Iteration 23593: Policy loss: 0.079177. Value loss: 0.114162. Entropy: 0.291116.\n",
      "Iteration 23594: Policy loss: 0.076124. Value loss: 0.037663. Entropy: 0.290748.\n",
      "Iteration 23595: Policy loss: 0.078279. Value loss: 0.027625. Entropy: 0.291060.\n",
      "episode: 8024   score: 590.0  epsilon: 1.0    steps: 8  evaluation reward: 487.0\n",
      "episode: 8025   score: 360.0  epsilon: 1.0    steps: 432  evaluation reward: 487.3\n",
      "Training network. lr: 0.000069. clip: 0.027664\n",
      "Iteration 23596: Policy loss: 0.013490. Value loss: 0.078981. Entropy: 0.284248.\n",
      "Iteration 23597: Policy loss: 0.014339. Value loss: 0.041692. Entropy: 0.283702.\n",
      "Iteration 23598: Policy loss: 0.011577. Value loss: 0.031555. Entropy: 0.283306.\n",
      "episode: 8026   score: 620.0  epsilon: 1.0    steps: 736  evaluation reward: 488.25\n",
      "Training network. lr: 0.000069. clip: 0.027664\n",
      "Iteration 23599: Policy loss: 0.142416. Value loss: 0.129890. Entropy: 0.281849.\n",
      "Iteration 23600: Policy loss: 0.136667. Value loss: 0.059175. Entropy: 0.282186.\n",
      "Iteration 23601: Policy loss: 0.135477. Value loss: 0.041918. Entropy: 0.281877.\n",
      "Training network. lr: 0.000069. clip: 0.027507\n",
      "Iteration 23602: Policy loss: 0.089867. Value loss: 0.154969. Entropy: 0.309389.\n",
      "Iteration 23603: Policy loss: 0.082272. Value loss: 0.070035. Entropy: 0.309864.\n",
      "Iteration 23604: Policy loss: 0.071791. Value loss: 0.048919. Entropy: 0.308918.\n",
      "Training network. lr: 0.000069. clip: 0.027507\n",
      "Iteration 23605: Policy loss: 0.023328. Value loss: 0.134993. Entropy: 0.313149.\n",
      "Iteration 23606: Policy loss: 0.024756. Value loss: 0.064054. Entropy: 0.313398.\n",
      "Iteration 23607: Policy loss: 0.028048. Value loss: 0.047168. Entropy: 0.312855.\n",
      "Training network. lr: 0.000069. clip: 0.027507\n",
      "Iteration 23608: Policy loss: 0.107054. Value loss: 0.158461. Entropy: 0.301526.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23609: Policy loss: 0.110255. Value loss: 0.074357. Entropy: 0.300694.\n",
      "Iteration 23610: Policy loss: 0.102507. Value loss: 0.053558. Entropy: 0.299421.\n",
      "episode: 8027   score: 425.0  epsilon: 1.0    steps: 632  evaluation reward: 486.75\n",
      "Training network. lr: 0.000069. clip: 0.027507\n",
      "Iteration 23611: Policy loss: 0.074994. Value loss: 0.122660. Entropy: 0.292435.\n",
      "Iteration 23612: Policy loss: 0.070727. Value loss: 0.054175. Entropy: 0.292286.\n",
      "Iteration 23613: Policy loss: 0.075310. Value loss: 0.039712. Entropy: 0.291777.\n",
      "episode: 8028   score: 390.0  epsilon: 1.0    steps: 344  evaluation reward: 486.65\n",
      "episode: 8029   score: 405.0  epsilon: 1.0    steps: 432  evaluation reward: 479.5\n",
      "Training network. lr: 0.000069. clip: 0.027507\n",
      "Iteration 23614: Policy loss: 0.013614. Value loss: 0.087671. Entropy: 0.279373.\n",
      "Iteration 23615: Policy loss: 0.007767. Value loss: 0.038459. Entropy: 0.279582.\n",
      "Iteration 23616: Policy loss: 0.012843. Value loss: 0.028351. Entropy: 0.279711.\n",
      "episode: 8030   score: 500.0  epsilon: 1.0    steps: 408  evaluation reward: 479.15\n",
      "Training network. lr: 0.000069. clip: 0.027507\n",
      "Iteration 23617: Policy loss: 0.000684. Value loss: 0.116855. Entropy: 0.288807.\n",
      "Iteration 23618: Policy loss: -0.003741. Value loss: 0.060095. Entropy: 0.288466.\n",
      "Iteration 23619: Policy loss: -0.005665. Value loss: 0.041900. Entropy: 0.288312.\n",
      "episode: 8031   score: 305.0  epsilon: 1.0    steps: 72  evaluation reward: 478.35\n",
      "Training network. lr: 0.000069. clip: 0.027507\n",
      "Iteration 23620: Policy loss: -0.068422. Value loss: 0.110436. Entropy: 0.291133.\n",
      "Iteration 23621: Policy loss: -0.072701. Value loss: 0.049675. Entropy: 0.292513.\n",
      "Iteration 23622: Policy loss: -0.066702. Value loss: 0.031049. Entropy: 0.292340.\n",
      "episode: 8032   score: 520.0  epsilon: 1.0    steps: 952  evaluation reward: 479.55\n",
      "Training network. lr: 0.000069. clip: 0.027507\n",
      "Iteration 23623: Policy loss: 0.255473. Value loss: 0.132074. Entropy: 0.303564.\n",
      "Iteration 23624: Policy loss: 0.247830. Value loss: 0.059844. Entropy: 0.304056.\n",
      "Iteration 23625: Policy loss: 0.250761. Value loss: 0.038444. Entropy: 0.304447.\n",
      "episode: 8033   score: 395.0  epsilon: 1.0    steps: 352  evaluation reward: 477.0\n",
      "Training network. lr: 0.000069. clip: 0.027507\n",
      "Iteration 23626: Policy loss: -0.148171. Value loss: 0.100812. Entropy: 0.289227.\n",
      "Iteration 23627: Policy loss: -0.154060. Value loss: 0.050353. Entropy: 0.292275.\n",
      "Iteration 23628: Policy loss: -0.148612. Value loss: 0.040741. Entropy: 0.290806.\n",
      "Training network. lr: 0.000069. clip: 0.027507\n",
      "Iteration 23629: Policy loss: -0.115135. Value loss: 0.100084. Entropy: 0.305576.\n",
      "Iteration 23630: Policy loss: -0.119052. Value loss: 0.050321. Entropy: 0.305557.\n",
      "Iteration 23631: Policy loss: -0.121636. Value loss: 0.035933. Entropy: 0.305487.\n",
      "episode: 8034   score: 565.0  epsilon: 1.0    steps: 424  evaluation reward: 478.7\n",
      "episode: 8035   score: 330.0  epsilon: 1.0    steps: 576  evaluation reward: 476.25\n",
      "episode: 8036   score: 385.0  epsilon: 1.0    steps: 872  evaluation reward: 476.5\n",
      "Training network. lr: 0.000069. clip: 0.027507\n",
      "Iteration 23632: Policy loss: -0.183533. Value loss: 0.308685. Entropy: 0.277070.\n",
      "Iteration 23633: Policy loss: -0.178558. Value loss: 0.202063. Entropy: 0.278234.\n",
      "Iteration 23634: Policy loss: -0.192668. Value loss: 0.154870. Entropy: 0.278415.\n",
      "episode: 8037   score: 350.0  epsilon: 1.0    steps: 920  evaluation reward: 476.35\n",
      "Training network. lr: 0.000069. clip: 0.027507\n",
      "Iteration 23635: Policy loss: 0.142824. Value loss: 0.089131. Entropy: 0.299601.\n",
      "Iteration 23636: Policy loss: 0.137598. Value loss: 0.035058. Entropy: 0.298709.\n",
      "Iteration 23637: Policy loss: 0.132239. Value loss: 0.026626. Entropy: 0.299198.\n",
      "Training network. lr: 0.000069. clip: 0.027507\n",
      "Iteration 23638: Policy loss: 0.123012. Value loss: 0.133486. Entropy: 0.301683.\n",
      "Iteration 23639: Policy loss: 0.129294. Value loss: 0.069454. Entropy: 0.301578.\n",
      "Iteration 23640: Policy loss: 0.125368. Value loss: 0.054170. Entropy: 0.302123.\n",
      "episode: 8038   score: 405.0  epsilon: 1.0    steps: 232  evaluation reward: 474.75\n",
      "episode: 8039   score: 560.0  epsilon: 1.0    steps: 560  evaluation reward: 477.05\n",
      "Training network. lr: 0.000069. clip: 0.027507\n",
      "Iteration 23641: Policy loss: 0.170216. Value loss: 0.082734. Entropy: 0.283613.\n",
      "Iteration 23642: Policy loss: 0.166011. Value loss: 0.041276. Entropy: 0.282927.\n",
      "Iteration 23643: Policy loss: 0.163104. Value loss: 0.032141. Entropy: 0.283380.\n",
      "episode: 8040   score: 320.0  epsilon: 1.0    steps: 920  evaluation reward: 476.15\n",
      "Training network. lr: 0.000069. clip: 0.027507\n",
      "Iteration 23644: Policy loss: -0.339602. Value loss: 0.248029. Entropy: 0.298454.\n",
      "Iteration 23645: Policy loss: -0.333119. Value loss: 0.138569. Entropy: 0.298388.\n",
      "Iteration 23646: Policy loss: -0.348584. Value loss: 0.084619. Entropy: 0.299323.\n",
      "episode: 8041   score: 425.0  epsilon: 1.0    steps: 888  evaluation reward: 476.3\n",
      "Training network. lr: 0.000069. clip: 0.027507\n",
      "Iteration 23647: Policy loss: -0.042894. Value loss: 0.126497. Entropy: 0.300300.\n",
      "Iteration 23648: Policy loss: -0.043006. Value loss: 0.060266. Entropy: 0.300196.\n",
      "Iteration 23649: Policy loss: -0.051630. Value loss: 0.044297. Entropy: 0.300206.\n",
      "Training network. lr: 0.000069. clip: 0.027507\n",
      "Iteration 23650: Policy loss: -0.099997. Value loss: 0.080966. Entropy: 0.304172.\n",
      "Iteration 23651: Policy loss: -0.098507. Value loss: 0.038336. Entropy: 0.303635.\n",
      "Iteration 23652: Policy loss: -0.101247. Value loss: 0.030880. Entropy: 0.304440.\n",
      "episode: 8042   score: 340.0  epsilon: 1.0    steps: 600  evaluation reward: 475.55\n",
      "Training network. lr: 0.000068. clip: 0.027350\n",
      "Iteration 23653: Policy loss: 0.013228. Value loss: 0.105805. Entropy: 0.294898.\n",
      "Iteration 23654: Policy loss: 0.009604. Value loss: 0.055005. Entropy: 0.294037.\n",
      "Iteration 23655: Policy loss: 0.006433. Value loss: 0.042411. Entropy: 0.293578.\n",
      "episode: 8043   score: 300.0  epsilon: 1.0    steps: 32  evaluation reward: 475.7\n",
      "episode: 8044   score: 470.0  epsilon: 1.0    steps: 728  evaluation reward: 478.05\n",
      "Training network. lr: 0.000068. clip: 0.027350\n",
      "Iteration 23656: Policy loss: 0.024690. Value loss: 0.093530. Entropy: 0.277574.\n",
      "Iteration 23657: Policy loss: 0.019668. Value loss: 0.036893. Entropy: 0.277475.\n",
      "Iteration 23658: Policy loss: 0.023611. Value loss: 0.025837. Entropy: 0.277901.\n",
      "Training network. lr: 0.000068. clip: 0.027350\n",
      "Iteration 23659: Policy loss: -0.026269. Value loss: 0.095364. Entropy: 0.306202.\n",
      "Iteration 23660: Policy loss: -0.029270. Value loss: 0.045877. Entropy: 0.305849.\n",
      "Iteration 23661: Policy loss: -0.024584. Value loss: 0.031932. Entropy: 0.305328.\n",
      "Training network. lr: 0.000068. clip: 0.027350\n",
      "Iteration 23662: Policy loss: 0.134867. Value loss: 0.081838. Entropy: 0.310182.\n",
      "Iteration 23663: Policy loss: 0.127040. Value loss: 0.035018. Entropy: 0.309972.\n",
      "Iteration 23664: Policy loss: 0.131615. Value loss: 0.026736. Entropy: 0.309870.\n",
      "Training network. lr: 0.000068. clip: 0.027350\n",
      "Iteration 23665: Policy loss: 0.094606. Value loss: 0.090561. Entropy: 0.311408.\n",
      "Iteration 23666: Policy loss: 0.096571. Value loss: 0.032723. Entropy: 0.311453.\n",
      "Iteration 23667: Policy loss: 0.093459. Value loss: 0.020744. Entropy: 0.311190.\n",
      "episode: 8045   score: 330.0  epsilon: 1.0    steps: 672  evaluation reward: 476.5\n",
      "Training network. lr: 0.000068. clip: 0.027350\n",
      "Iteration 23668: Policy loss: -0.197355. Value loss: 0.259969. Entropy: 0.294581.\n",
      "Iteration 23669: Policy loss: -0.186009. Value loss: 0.113531. Entropy: 0.294058.\n",
      "Iteration 23670: Policy loss: -0.192463. Value loss: 0.066371. Entropy: 0.293809.\n",
      "episode: 8046   score: 680.0  epsilon: 1.0    steps: 168  evaluation reward: 475.8\n",
      "episode: 8047   score: 620.0  epsilon: 1.0    steps: 352  evaluation reward: 479.9\n",
      "Training network. lr: 0.000068. clip: 0.027350\n",
      "Iteration 23671: Policy loss: 0.201817. Value loss: 0.214823. Entropy: 0.273984.\n",
      "Iteration 23672: Policy loss: 0.198494. Value loss: 0.089845. Entropy: 0.272904.\n",
      "Iteration 23673: Policy loss: 0.190571. Value loss: 0.056570. Entropy: 0.273416.\n",
      "episode: 8048   score: 595.0  epsilon: 1.0    steps: 312  evaluation reward: 481.65\n",
      "Training network. lr: 0.000068. clip: 0.027350\n",
      "Iteration 23674: Policy loss: 0.155303. Value loss: 0.084489. Entropy: 0.293017.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23675: Policy loss: 0.149706. Value loss: 0.049620. Entropy: 0.291444.\n",
      "Iteration 23676: Policy loss: 0.151000. Value loss: 0.037773. Entropy: 0.291258.\n",
      "episode: 8049   score: 385.0  epsilon: 1.0    steps: 184  evaluation reward: 482.1\n",
      "episode: 8050   score: 495.0  epsilon: 1.0    steps: 856  evaluation reward: 483.85\n",
      "Training network. lr: 0.000068. clip: 0.027350\n",
      "Iteration 23677: Policy loss: 0.063349. Value loss: 0.132609. Entropy: 0.282402.\n",
      "Iteration 23678: Policy loss: 0.049067. Value loss: 0.067608. Entropy: 0.283799.\n",
      "Iteration 23679: Policy loss: 0.053524. Value loss: 0.047574. Entropy: 0.284018.\n",
      "now time :  2019-09-06 14:39:36.004065\n",
      "episode: 8051   score: 405.0  epsilon: 1.0    steps: 792  evaluation reward: 483.75\n",
      "Training network. lr: 0.000068. clip: 0.027350\n",
      "Iteration 23680: Policy loss: -0.289526. Value loss: 0.230517. Entropy: 0.298020.\n",
      "Iteration 23681: Policy loss: -0.297869. Value loss: 0.094629. Entropy: 0.298530.\n",
      "Iteration 23682: Policy loss: -0.302085. Value loss: 0.037078. Entropy: 0.299233.\n",
      "Training network. lr: 0.000068. clip: 0.027350\n",
      "Iteration 23683: Policy loss: -0.265148. Value loss: 0.214173. Entropy: 0.299947.\n",
      "Iteration 23684: Policy loss: -0.276822. Value loss: 0.077547. Entropy: 0.300109.\n",
      "Iteration 23685: Policy loss: -0.279889. Value loss: 0.044453. Entropy: 0.300441.\n",
      "Training network. lr: 0.000068. clip: 0.027350\n",
      "Iteration 23686: Policy loss: 0.103388. Value loss: 0.205715. Entropy: 0.307243.\n",
      "Iteration 23687: Policy loss: 0.084503. Value loss: 0.062348. Entropy: 0.306192.\n",
      "Iteration 23688: Policy loss: 0.096503. Value loss: 0.043298. Entropy: 0.306414.\n",
      "episode: 8052   score: 730.0  epsilon: 1.0    steps: 496  evaluation reward: 484.6\n",
      "Training network. lr: 0.000068. clip: 0.027350\n",
      "Iteration 23689: Policy loss: -0.336762. Value loss: 0.332274. Entropy: 0.288624.\n",
      "Iteration 23690: Policy loss: -0.359647. Value loss: 0.208099. Entropy: 0.289367.\n",
      "Iteration 23691: Policy loss: -0.356506. Value loss: 0.151966. Entropy: 0.290121.\n",
      "episode: 8053   score: 320.0  epsilon: 1.0    steps: 112  evaluation reward: 478.7\n",
      "episode: 8054   score: 545.0  epsilon: 1.0    steps: 112  evaluation reward: 480.35\n",
      "Training network. lr: 0.000068. clip: 0.027350\n",
      "Iteration 23692: Policy loss: 0.022400. Value loss: 0.144563. Entropy: 0.279273.\n",
      "Iteration 23693: Policy loss: 0.032575. Value loss: 0.058669. Entropy: 0.280068.\n",
      "Iteration 23694: Policy loss: 0.020426. Value loss: 0.045393. Entropy: 0.280523.\n",
      "Training network. lr: 0.000068. clip: 0.027350\n",
      "Iteration 23695: Policy loss: 0.263896. Value loss: 0.176218. Entropy: 0.306655.\n",
      "Iteration 23696: Policy loss: 0.254262. Value loss: 0.072798. Entropy: 0.305416.\n",
      "Iteration 23697: Policy loss: 0.256269. Value loss: 0.052646. Entropy: 0.304997.\n",
      "episode: 8055   score: 335.0  epsilon: 1.0    steps: 376  evaluation reward: 476.75\n",
      "Training network. lr: 0.000068. clip: 0.027350\n",
      "Iteration 23698: Policy loss: -0.103987. Value loss: 0.066480. Entropy: 0.297008.\n",
      "Iteration 23699: Policy loss: -0.111671. Value loss: 0.032589. Entropy: 0.296631.\n",
      "Iteration 23700: Policy loss: -0.107973. Value loss: 0.024692. Entropy: 0.296936.\n",
      "episode: 8056   score: 385.0  epsilon: 1.0    steps: 456  evaluation reward: 473.9\n",
      "Training network. lr: 0.000068. clip: 0.027203\n",
      "Iteration 23701: Policy loss: -0.249793. Value loss: 0.228674. Entropy: 0.290129.\n",
      "Iteration 23702: Policy loss: -0.261181. Value loss: 0.105572. Entropy: 0.291037.\n",
      "Iteration 23703: Policy loss: -0.269674. Value loss: 0.073868. Entropy: 0.290056.\n",
      "episode: 8057   score: 975.0  epsilon: 1.0    steps: 944  evaluation reward: 480.05\n",
      "Training network. lr: 0.000068. clip: 0.027203\n",
      "Iteration 23704: Policy loss: -0.177816. Value loss: 0.631587. Entropy: 0.288967.\n",
      "Iteration 23705: Policy loss: -0.168041. Value loss: 0.307143. Entropy: 0.286820.\n",
      "Iteration 23706: Policy loss: -0.197047. Value loss: 0.197337. Entropy: 0.287022.\n",
      "episode: 8058   score: 725.0  epsilon: 1.0    steps: 744  evaluation reward: 482.85\n",
      "Training network. lr: 0.000068. clip: 0.027203\n",
      "Iteration 23707: Policy loss: 0.137572. Value loss: 0.428320. Entropy: 0.290118.\n",
      "Iteration 23708: Policy loss: 0.116829. Value loss: 0.273238. Entropy: 0.289502.\n",
      "Iteration 23709: Policy loss: 0.107446. Value loss: 0.207119. Entropy: 0.288415.\n",
      "Training network. lr: 0.000068. clip: 0.027203\n",
      "Iteration 23710: Policy loss: 0.281575. Value loss: 0.200204. Entropy: 0.308324.\n",
      "Iteration 23711: Policy loss: 0.281207. Value loss: 0.110046. Entropy: 0.307221.\n",
      "Iteration 23712: Policy loss: 0.277925. Value loss: 0.081725. Entropy: 0.307483.\n",
      "episode: 8059   score: 370.0  epsilon: 1.0    steps: 760  evaluation reward: 483.65\n",
      "episode: 8060   score: 555.0  epsilon: 1.0    steps: 952  evaluation reward: 485.05\n",
      "episode: 8061   score: 375.0  epsilon: 1.0    steps: 992  evaluation reward: 482.95\n",
      "Training network. lr: 0.000068. clip: 0.027203\n",
      "Iteration 23713: Policy loss: 0.284041. Value loss: 0.175346. Entropy: 0.292800.\n",
      "Iteration 23714: Policy loss: 0.290391. Value loss: 0.100925. Entropy: 0.292345.\n",
      "Iteration 23715: Policy loss: 0.272674. Value loss: 0.075898. Entropy: 0.292392.\n",
      "episode: 8062   score: 455.0  epsilon: 1.0    steps: 32  evaluation reward: 483.3\n",
      "Training network. lr: 0.000068. clip: 0.027203\n",
      "Iteration 23716: Policy loss: -0.159618. Value loss: 0.068224. Entropy: 0.278576.\n",
      "Iteration 23717: Policy loss: -0.161684. Value loss: 0.029393. Entropy: 0.279737.\n",
      "Iteration 23718: Policy loss: -0.159974. Value loss: 0.026633. Entropy: 0.278932.\n",
      "Training network. lr: 0.000068. clip: 0.027203\n",
      "Iteration 23719: Policy loss: 0.242524. Value loss: 0.123269. Entropy: 0.298688.\n",
      "Iteration 23720: Policy loss: 0.233925. Value loss: 0.063442. Entropy: 0.297945.\n",
      "Iteration 23721: Policy loss: 0.235600. Value loss: 0.047100. Entropy: 0.298347.\n",
      "episode: 8063   score: 425.0  epsilon: 1.0    steps: 992  evaluation reward: 483.35\n",
      "Training network. lr: 0.000068. clip: 0.027203\n",
      "Iteration 23722: Policy loss: -0.029562. Value loss: 0.120224. Entropy: 0.310398.\n",
      "Iteration 23723: Policy loss: -0.028032. Value loss: 0.062748. Entropy: 0.310076.\n",
      "Iteration 23724: Policy loss: -0.029590. Value loss: 0.046079. Entropy: 0.309760.\n",
      "Training network. lr: 0.000068. clip: 0.027203\n",
      "Iteration 23725: Policy loss: -0.073341. Value loss: 0.344764. Entropy: 0.300086.\n",
      "Iteration 23726: Policy loss: -0.082195. Value loss: 0.254803. Entropy: 0.299793.\n",
      "Iteration 23727: Policy loss: -0.080511. Value loss: 0.229762. Entropy: 0.300387.\n",
      "episode: 8064   score: 180.0  epsilon: 1.0    steps: 288  evaluation reward: 478.15\n",
      "Training network. lr: 0.000068. clip: 0.027203\n",
      "Iteration 23728: Policy loss: -0.195912. Value loss: 0.143312. Entropy: 0.290814.\n",
      "Iteration 23729: Policy loss: -0.191448. Value loss: 0.069374. Entropy: 0.290588.\n",
      "Iteration 23730: Policy loss: -0.203688. Value loss: 0.049712. Entropy: 0.291312.\n",
      "episode: 8065   score: 240.0  epsilon: 1.0    steps: 248  evaluation reward: 475.0\n",
      "Training network. lr: 0.000068. clip: 0.027203\n",
      "Iteration 23731: Policy loss: 0.146623. Value loss: 0.118200. Entropy: 0.286116.\n",
      "Iteration 23732: Policy loss: 0.144525. Value loss: 0.053686. Entropy: 0.288159.\n",
      "Iteration 23733: Policy loss: 0.145884. Value loss: 0.034373. Entropy: 0.285077.\n",
      "Training network. lr: 0.000068. clip: 0.027203\n",
      "Iteration 23734: Policy loss: -0.042736. Value loss: 0.171725. Entropy: 0.309836.\n",
      "Iteration 23735: Policy loss: -0.041346. Value loss: 0.103008. Entropy: 0.310135.\n",
      "Iteration 23736: Policy loss: -0.037037. Value loss: 0.083850. Entropy: 0.309986.\n",
      "episode: 8066   score: 395.0  epsilon: 1.0    steps: 224  evaluation reward: 474.0\n",
      "episode: 8067   score: 370.0  epsilon: 1.0    steps: 440  evaluation reward: 473.35\n",
      "episode: 8068   score: 450.0  epsilon: 1.0    steps: 616  evaluation reward: 471.55\n",
      "episode: 8069   score: 655.0  epsilon: 1.0    steps: 632  evaluation reward: 473.3\n",
      "Training network. lr: 0.000068. clip: 0.027203\n",
      "Iteration 23737: Policy loss: 0.196358. Value loss: 0.131249. Entropy: 0.250173.\n",
      "Iteration 23738: Policy loss: 0.190243. Value loss: 0.050428. Entropy: 0.248137.\n",
      "Iteration 23739: Policy loss: 0.183682. Value loss: 0.037259. Entropy: 0.249544.\n",
      "episode: 8070   score: 740.0  epsilon: 1.0    steps: 208  evaluation reward: 474.6\n",
      "Training network. lr: 0.000068. clip: 0.027203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23740: Policy loss: 0.277479. Value loss: 0.124526. Entropy: 0.281733.\n",
      "Iteration 23741: Policy loss: 0.272823. Value loss: 0.052686. Entropy: 0.282491.\n",
      "Iteration 23742: Policy loss: 0.276341. Value loss: 0.036820. Entropy: 0.282767.\n",
      "Training network. lr: 0.000068. clip: 0.027203\n",
      "Iteration 23743: Policy loss: 0.217061. Value loss: 0.140274. Entropy: 0.307491.\n",
      "Iteration 23744: Policy loss: 0.210282. Value loss: 0.083862. Entropy: 0.305718.\n",
      "Iteration 23745: Policy loss: 0.215145. Value loss: 0.064700. Entropy: 0.306086.\n",
      "episode: 8071   score: 550.0  epsilon: 1.0    steps: 960  evaluation reward: 476.25\n",
      "Training network. lr: 0.000068. clip: 0.027203\n",
      "Iteration 23746: Policy loss: 0.124585. Value loss: 0.153357. Entropy: 0.302796.\n",
      "Iteration 23747: Policy loss: 0.118803. Value loss: 0.072207. Entropy: 0.302302.\n",
      "Iteration 23748: Policy loss: 0.111182. Value loss: 0.051751. Entropy: 0.303435.\n",
      "Training network. lr: 0.000068. clip: 0.027203\n",
      "Iteration 23749: Policy loss: -0.022909. Value loss: 0.072798. Entropy: 0.304835.\n",
      "Iteration 23750: Policy loss: -0.026706. Value loss: 0.039808. Entropy: 0.304136.\n",
      "Iteration 23751: Policy loss: -0.026296. Value loss: 0.029527. Entropy: 0.304069.\n",
      "Training network. lr: 0.000068. clip: 0.027046\n",
      "Iteration 23752: Policy loss: -0.277720. Value loss: 0.222188. Entropy: 0.300761.\n",
      "Iteration 23753: Policy loss: -0.283732. Value loss: 0.103772. Entropy: 0.300450.\n",
      "Iteration 23754: Policy loss: -0.289188. Value loss: 0.066330. Entropy: 0.300113.\n",
      "episode: 8072   score: 315.0  epsilon: 1.0    steps: 704  evaluation reward: 475.5\n",
      "Training network. lr: 0.000068. clip: 0.027046\n",
      "Iteration 23755: Policy loss: 0.229903. Value loss: 0.163058. Entropy: 0.295640.\n",
      "Iteration 23756: Policy loss: 0.230932. Value loss: 0.068717. Entropy: 0.294229.\n",
      "Iteration 23757: Policy loss: 0.230369. Value loss: 0.045081. Entropy: 0.293842.\n",
      "episode: 8073   score: 330.0  epsilon: 1.0    steps: 776  evaluation reward: 470.1\n",
      "Training network. lr: 0.000068. clip: 0.027046\n",
      "Iteration 23758: Policy loss: -0.001574. Value loss: 0.150752. Entropy: 0.285120.\n",
      "Iteration 23759: Policy loss: 0.007021. Value loss: 0.066381. Entropy: 0.285096.\n",
      "Iteration 23760: Policy loss: 0.000782. Value loss: 0.046874. Entropy: 0.284617.\n",
      "episode: 8074   score: 495.0  epsilon: 1.0    steps: 192  evaluation reward: 470.4\n",
      "episode: 8075   score: 260.0  epsilon: 1.0    steps: 280  evaluation reward: 470.1\n",
      "Training network. lr: 0.000068. clip: 0.027046\n",
      "Iteration 23761: Policy loss: 0.258838. Value loss: 0.132063. Entropy: 0.266007.\n",
      "Iteration 23762: Policy loss: 0.261833. Value loss: 0.055557. Entropy: 0.268161.\n",
      "Iteration 23763: Policy loss: 0.253445. Value loss: 0.037259. Entropy: 0.267712.\n",
      "episode: 8076   score: 680.0  epsilon: 1.0    steps: 232  evaluation reward: 469.6\n",
      "Training network. lr: 0.000068. clip: 0.027046\n",
      "Iteration 23764: Policy loss: 0.285303. Value loss: 0.124279. Entropy: 0.286034.\n",
      "Iteration 23765: Policy loss: 0.277527. Value loss: 0.053102. Entropy: 0.286817.\n",
      "Iteration 23766: Policy loss: 0.277589. Value loss: 0.038856. Entropy: 0.287499.\n",
      "Training network. lr: 0.000068. clip: 0.027046\n",
      "Iteration 23767: Policy loss: 0.037597. Value loss: 0.152720. Entropy: 0.308696.\n",
      "Iteration 23768: Policy loss: 0.041834. Value loss: 0.056328. Entropy: 0.309205.\n",
      "Iteration 23769: Policy loss: 0.029817. Value loss: 0.038312. Entropy: 0.310768.\n",
      "Training network. lr: 0.000068. clip: 0.027046\n",
      "Iteration 23770: Policy loss: -0.276102. Value loss: 0.118558. Entropy: 0.308215.\n",
      "Iteration 23771: Policy loss: -0.273426. Value loss: 0.057505. Entropy: 0.308794.\n",
      "Iteration 23772: Policy loss: -0.280828. Value loss: 0.042881. Entropy: 0.308398.\n",
      "episode: 8077   score: 510.0  epsilon: 1.0    steps: 392  evaluation reward: 469.9\n",
      "Training network. lr: 0.000068. clip: 0.027046\n",
      "Iteration 23773: Policy loss: -0.186151. Value loss: 0.261744. Entropy: 0.301910.\n",
      "Iteration 23774: Policy loss: -0.193565. Value loss: 0.159619. Entropy: 0.300056.\n",
      "Iteration 23775: Policy loss: -0.174449. Value loss: 0.113014. Entropy: 0.300755.\n",
      "episode: 8078   score: 605.0  epsilon: 1.0    steps: 176  evaluation reward: 471.35\n",
      "Training network. lr: 0.000068. clip: 0.027046\n",
      "Iteration 23776: Policy loss: 0.155012. Value loss: 0.157433. Entropy: 0.289614.\n",
      "Iteration 23777: Policy loss: 0.151186. Value loss: 0.074881. Entropy: 0.289733.\n",
      "Iteration 23778: Policy loss: 0.148337. Value loss: 0.045704. Entropy: 0.289499.\n",
      "episode: 8079   score: 500.0  epsilon: 1.0    steps: 296  evaluation reward: 472.9\n",
      "episode: 8080   score: 495.0  epsilon: 1.0    steps: 584  evaluation reward: 474.4\n",
      "episode: 8081   score: 535.0  epsilon: 1.0    steps: 1024  evaluation reward: 475.8\n",
      "Training network. lr: 0.000068. clip: 0.027046\n",
      "Iteration 23779: Policy loss: 0.130096. Value loss: 0.125565. Entropy: 0.281649.\n",
      "Iteration 23780: Policy loss: 0.120291. Value loss: 0.054051. Entropy: 0.280654.\n",
      "Iteration 23781: Policy loss: 0.126371. Value loss: 0.039976. Entropy: 0.281538.\n",
      "episode: 8082   score: 360.0  epsilon: 1.0    steps: 496  evaluation reward: 474.1\n",
      "Training network. lr: 0.000068. clip: 0.027046\n",
      "Iteration 23782: Policy loss: 0.245156. Value loss: 0.068988. Entropy: 0.274682.\n",
      "Iteration 23783: Policy loss: 0.248409. Value loss: 0.021294. Entropy: 0.274188.\n",
      "Iteration 23784: Policy loss: 0.235488. Value loss: 0.017352. Entropy: 0.274368.\n",
      "Training network. lr: 0.000068. clip: 0.027046\n",
      "Iteration 23785: Policy loss: 0.010984. Value loss: 0.099469. Entropy: 0.306293.\n",
      "Iteration 23786: Policy loss: 0.006591. Value loss: 0.048502. Entropy: 0.306250.\n",
      "Iteration 23787: Policy loss: 0.006739. Value loss: 0.035304. Entropy: 0.307363.\n",
      "episode: 8083   score: 365.0  epsilon: 1.0    steps: 616  evaluation reward: 474.0\n",
      "Training network. lr: 0.000068. clip: 0.027046\n",
      "Iteration 23788: Policy loss: -0.501959. Value loss: 0.507997. Entropy: 0.291073.\n",
      "Iteration 23789: Policy loss: -0.518278. Value loss: 0.290154. Entropy: 0.290400.\n",
      "Iteration 23790: Policy loss: -0.510101. Value loss: 0.226044. Entropy: 0.291157.\n",
      "Training network. lr: 0.000068. clip: 0.027046\n",
      "Iteration 23791: Policy loss: 0.090224. Value loss: 0.127477. Entropy: 0.307582.\n",
      "Iteration 23792: Policy loss: 0.090624. Value loss: 0.065275. Entropy: 0.308715.\n",
      "Iteration 23793: Policy loss: 0.094392. Value loss: 0.051862. Entropy: 0.307898.\n",
      "episode: 8084   score: 765.0  epsilon: 1.0    steps: 368  evaluation reward: 478.2\n",
      "Training network. lr: 0.000068. clip: 0.027046\n",
      "Iteration 23794: Policy loss: -0.063832. Value loss: 0.141554. Entropy: 0.292109.\n",
      "Iteration 23795: Policy loss: -0.063707. Value loss: 0.060668. Entropy: 0.290941.\n",
      "Iteration 23796: Policy loss: -0.072111. Value loss: 0.042795. Entropy: 0.291184.\n",
      "episode: 8085   score: 480.0  epsilon: 1.0    steps: 288  evaluation reward: 476.05\n",
      "episode: 8086   score: 360.0  epsilon: 1.0    steps: 960  evaluation reward: 473.55\n",
      "Training network. lr: 0.000068. clip: 0.027046\n",
      "Iteration 23797: Policy loss: 0.052651. Value loss: 0.190382. Entropy: 0.289425.\n",
      "Iteration 23798: Policy loss: 0.040318. Value loss: 0.080288. Entropy: 0.288839.\n",
      "Iteration 23799: Policy loss: 0.037495. Value loss: 0.060736. Entropy: 0.288441.\n",
      "episode: 8087   score: 620.0  epsilon: 1.0    steps: 296  evaluation reward: 476.6\n",
      "Training network. lr: 0.000068. clip: 0.027046\n",
      "Iteration 23800: Policy loss: -0.069587. Value loss: 0.199100. Entropy: 0.285460.\n",
      "Iteration 23801: Policy loss: -0.073596. Value loss: 0.129502. Entropy: 0.286551.\n",
      "Iteration 23802: Policy loss: -0.069104. Value loss: 0.110725. Entropy: 0.285798.\n",
      "Training network. lr: 0.000067. clip: 0.026889\n",
      "Iteration 23803: Policy loss: -0.245513. Value loss: 0.400242. Entropy: 0.303661.\n",
      "Iteration 23804: Policy loss: -0.223350. Value loss: 0.240756. Entropy: 0.304196.\n",
      "Iteration 23805: Policy loss: -0.213137. Value loss: 0.121533. Entropy: 0.302723.\n",
      "episode: 8088   score: 435.0  epsilon: 1.0    steps: 112  evaluation reward: 472.95\n",
      "episode: 8089   score: 150.0  epsilon: 1.0    steps: 192  evaluation reward: 468.55\n",
      "Training network. lr: 0.000067. clip: 0.026889\n",
      "Iteration 23806: Policy loss: -0.008697. Value loss: 0.079342. Entropy: 0.281484.\n",
      "Iteration 23807: Policy loss: -0.015517. Value loss: 0.044266. Entropy: 0.282223.\n",
      "Iteration 23808: Policy loss: -0.014260. Value loss: 0.036191. Entropy: 0.281969.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000067. clip: 0.026889\n",
      "Iteration 23809: Policy loss: 0.253132. Value loss: 0.090613. Entropy: 0.300665.\n",
      "Iteration 23810: Policy loss: 0.255122. Value loss: 0.043799. Entropy: 0.300751.\n",
      "Iteration 23811: Policy loss: 0.257405. Value loss: 0.032133. Entropy: 0.301808.\n",
      "Training network. lr: 0.000067. clip: 0.026889\n",
      "Iteration 23812: Policy loss: -0.034851. Value loss: 0.117068. Entropy: 0.306741.\n",
      "Iteration 23813: Policy loss: -0.040758. Value loss: 0.059514. Entropy: 0.306702.\n",
      "Iteration 23814: Policy loss: -0.043984. Value loss: 0.042051. Entropy: 0.307309.\n",
      "episode: 8090   score: 495.0  epsilon: 1.0    steps: 144  evaluation reward: 467.7\n",
      "Training network. lr: 0.000067. clip: 0.026889\n",
      "Iteration 23815: Policy loss: 0.226951. Value loss: 0.082954. Entropy: 0.297284.\n",
      "Iteration 23816: Policy loss: 0.226946. Value loss: 0.040168. Entropy: 0.298758.\n",
      "Iteration 23817: Policy loss: 0.222767. Value loss: 0.032862. Entropy: 0.298416.\n",
      "episode: 8091   score: 600.0  epsilon: 1.0    steps: 120  evaluation reward: 469.8\n",
      "Training network. lr: 0.000067. clip: 0.026889\n",
      "Iteration 23818: Policy loss: -0.347579. Value loss: 0.440589. Entropy: 0.292769.\n",
      "Iteration 23819: Policy loss: -0.335763. Value loss: 0.167853. Entropy: 0.293047.\n",
      "Iteration 23820: Policy loss: -0.331601. Value loss: 0.115254. Entropy: 0.293280.\n",
      "episode: 8092   score: 370.0  epsilon: 1.0    steps: 288  evaluation reward: 464.8\n",
      "episode: 8093   score: 900.0  epsilon: 1.0    steps: 688  evaluation reward: 468.2\n",
      "Training network. lr: 0.000067. clip: 0.026889\n",
      "Iteration 23821: Policy loss: 0.067799. Value loss: 0.164141. Entropy: 0.289411.\n",
      "Iteration 23822: Policy loss: 0.050564. Value loss: 0.077463. Entropy: 0.289606.\n",
      "Iteration 23823: Policy loss: 0.046574. Value loss: 0.058887. Entropy: 0.289324.\n",
      "Training network. lr: 0.000067. clip: 0.026889\n",
      "Iteration 23824: Policy loss: 0.206278. Value loss: 0.212430. Entropy: 0.303255.\n",
      "Iteration 23825: Policy loss: 0.200058. Value loss: 0.107151. Entropy: 0.303243.\n",
      "Iteration 23826: Policy loss: 0.195337. Value loss: 0.074154. Entropy: 0.302701.\n",
      "episode: 8094   score: 570.0  epsilon: 1.0    steps: 96  evaluation reward: 470.75\n",
      "episode: 8095   score: 335.0  epsilon: 1.0    steps: 136  evaluation reward: 470.4\n",
      "episode: 8096   score: 210.0  epsilon: 1.0    steps: 920  evaluation reward: 467.35\n",
      "episode: 8097   score: 700.0  epsilon: 1.0    steps: 944  evaluation reward: 471.05\n",
      "Training network. lr: 0.000067. clip: 0.026889\n",
      "Iteration 23827: Policy loss: 0.041590. Value loss: 0.189135. Entropy: 0.284670.\n",
      "Iteration 23828: Policy loss: 0.024800. Value loss: 0.081392. Entropy: 0.284486.\n",
      "Iteration 23829: Policy loss: 0.021784. Value loss: 0.058234. Entropy: 0.285836.\n",
      "Training network. lr: 0.000067. clip: 0.026889\n",
      "Iteration 23830: Policy loss: -0.053031. Value loss: 0.138809. Entropy: 0.295924.\n",
      "Iteration 23831: Policy loss: -0.043708. Value loss: 0.054537. Entropy: 0.296650.\n",
      "Iteration 23832: Policy loss: -0.049234. Value loss: 0.039091. Entropy: 0.295243.\n",
      "episode: 8098   score: 740.0  epsilon: 1.0    steps: 480  evaluation reward: 472.25\n",
      "Training network. lr: 0.000067. clip: 0.026889\n",
      "Iteration 23833: Policy loss: 0.055293. Value loss: 0.073963. Entropy: 0.293424.\n",
      "Iteration 23834: Policy loss: 0.052487. Value loss: 0.045294. Entropy: 0.293322.\n",
      "Iteration 23835: Policy loss: 0.054189. Value loss: 0.033641. Entropy: 0.292934.\n",
      "Training network. lr: 0.000067. clip: 0.026889\n",
      "Iteration 23836: Policy loss: 0.472329. Value loss: 0.173673. Entropy: 0.306077.\n",
      "Iteration 23837: Policy loss: 0.468405. Value loss: 0.053089. Entropy: 0.305645.\n",
      "Iteration 23838: Policy loss: 0.469413. Value loss: 0.033326. Entropy: 0.305985.\n",
      "episode: 8099   score: 335.0  epsilon: 1.0    steps: 88  evaluation reward: 471.1\n",
      "Training network. lr: 0.000067. clip: 0.026889\n",
      "Iteration 23839: Policy loss: 0.069317. Value loss: 0.119632. Entropy: 0.288523.\n",
      "Iteration 23840: Policy loss: 0.070191. Value loss: 0.080515. Entropy: 0.288134.\n",
      "Iteration 23841: Policy loss: 0.062673. Value loss: 0.063879. Entropy: 0.286974.\n",
      "Training network. lr: 0.000067. clip: 0.026889\n",
      "Iteration 23842: Policy loss: 0.046468. Value loss: 0.181343. Entropy: 0.310981.\n",
      "Iteration 23843: Policy loss: 0.050062. Value loss: 0.085754. Entropy: 0.310922.\n",
      "Iteration 23844: Policy loss: 0.038101. Value loss: 0.058121. Entropy: 0.311253.\n",
      "episode: 8100   score: 405.0  epsilon: 1.0    steps: 128  evaluation reward: 467.9\n",
      "Training network. lr: 0.000067. clip: 0.026889\n",
      "Iteration 23845: Policy loss: -0.119147. Value loss: 0.164613. Entropy: 0.285512.\n",
      "Iteration 23846: Policy loss: -0.114436. Value loss: 0.084782. Entropy: 0.287171.\n",
      "Iteration 23847: Policy loss: -0.121507. Value loss: 0.060066. Entropy: 0.286959.\n",
      "Training network. lr: 0.000067. clip: 0.026889\n",
      "Iteration 23848: Policy loss: -0.639996. Value loss: 0.511295. Entropy: 0.308025.\n",
      "Iteration 23849: Policy loss: -0.634372. Value loss: 0.274411. Entropy: 0.305522.\n",
      "Iteration 23850: Policy loss: -0.637386. Value loss: 0.184250. Entropy: 0.306546.\n",
      "now time :  2019-09-06 14:50:14.326443\n",
      "episode: 8101   score: 665.0  epsilon: 1.0    steps: 984  evaluation reward: 470.75\n",
      "Training network. lr: 0.000067. clip: 0.026742\n",
      "Iteration 23851: Policy loss: -0.120440. Value loss: 0.117708. Entropy: 0.313375.\n",
      "Iteration 23852: Policy loss: -0.123116. Value loss: 0.062918. Entropy: 0.312605.\n",
      "Iteration 23853: Policy loss: -0.124498. Value loss: 0.047043. Entropy: 0.311991.\n",
      "episode: 8102   score: 315.0  epsilon: 1.0    steps: 176  evaluation reward: 470.6\n",
      "Training network. lr: 0.000067. clip: 0.026742\n",
      "Iteration 23854: Policy loss: -0.015388. Value loss: 0.280536. Entropy: 0.288639.\n",
      "Iteration 23855: Policy loss: -0.026133. Value loss: 0.100916. Entropy: 0.288794.\n",
      "Iteration 23856: Policy loss: -0.030117. Value loss: 0.068870. Entropy: 0.289114.\n",
      "episode: 8103   score: 395.0  epsilon: 1.0    steps: 400  evaluation reward: 468.4\n",
      "episode: 8104   score: 420.0  epsilon: 1.0    steps: 528  evaluation reward: 467.8\n",
      "episode: 8105   score: 770.0  epsilon: 1.0    steps: 672  evaluation reward: 470.25\n",
      "Training network. lr: 0.000067. clip: 0.026742\n",
      "Iteration 23857: Policy loss: 0.168185. Value loss: 0.140408. Entropy: 0.281786.\n",
      "Iteration 23858: Policy loss: 0.156749. Value loss: 0.058050. Entropy: 0.281865.\n",
      "Iteration 23859: Policy loss: 0.157348. Value loss: 0.043535. Entropy: 0.282211.\n",
      "episode: 8106   score: 340.0  epsilon: 1.0    steps: 96  evaluation reward: 469.4\n",
      "Training network. lr: 0.000067. clip: 0.026742\n",
      "Iteration 23860: Policy loss: -0.127292. Value loss: 0.070802. Entropy: 0.295828.\n",
      "Iteration 23861: Policy loss: -0.133786. Value loss: 0.041650. Entropy: 0.296187.\n",
      "Iteration 23862: Policy loss: -0.129344. Value loss: 0.034095. Entropy: 0.295821.\n",
      "Training network. lr: 0.000067. clip: 0.026742\n",
      "Iteration 23863: Policy loss: -0.144812. Value loss: 0.192156. Entropy: 0.305453.\n",
      "Iteration 23864: Policy loss: -0.140836. Value loss: 0.054056. Entropy: 0.304714.\n",
      "Iteration 23865: Policy loss: -0.143594. Value loss: 0.034488. Entropy: 0.303892.\n",
      "Training network. lr: 0.000067. clip: 0.026742\n",
      "Iteration 23866: Policy loss: 0.043355. Value loss: 0.151748. Entropy: 0.298288.\n",
      "Iteration 23867: Policy loss: 0.041001. Value loss: 0.051350. Entropy: 0.298033.\n",
      "Iteration 23868: Policy loss: 0.038568. Value loss: 0.036158. Entropy: 0.298414.\n",
      "episode: 8107   score: 540.0  epsilon: 1.0    steps: 928  evaluation reward: 471.35\n",
      "Training network. lr: 0.000067. clip: 0.026742\n",
      "Iteration 23869: Policy loss: 0.066613. Value loss: 0.277756. Entropy: 0.312788.\n",
      "Iteration 23870: Policy loss: 0.070978. Value loss: 0.138936. Entropy: 0.313226.\n",
      "Iteration 23871: Policy loss: 0.058881. Value loss: 0.093348. Entropy: 0.313139.\n",
      "Training network. lr: 0.000067. clip: 0.026742\n",
      "Iteration 23872: Policy loss: -0.015010. Value loss: 0.160652. Entropy: 0.302046.\n",
      "Iteration 23873: Policy loss: -0.019772. Value loss: 0.064107. Entropy: 0.301570.\n",
      "Iteration 23874: Policy loss: -0.022374. Value loss: 0.037295. Entropy: 0.300858.\n",
      "episode: 8108   score: 535.0  epsilon: 1.0    steps: 160  evaluation reward: 473.45\n",
      "Training network. lr: 0.000067. clip: 0.026742\n",
      "Iteration 23875: Policy loss: -0.149776. Value loss: 0.192073. Entropy: 0.296241.\n",
      "Iteration 23876: Policy loss: -0.146016. Value loss: 0.101742. Entropy: 0.294915.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23877: Policy loss: -0.156429. Value loss: 0.069770. Entropy: 0.293801.\n",
      "episode: 8109   score: 725.0  epsilon: 1.0    steps: 640  evaluation reward: 474.95\n",
      "episode: 8110   score: 420.0  epsilon: 1.0    steps: 936  evaluation reward: 475.8\n",
      "Training network. lr: 0.000067. clip: 0.026742\n",
      "Iteration 23878: Policy loss: 0.055875. Value loss: 0.239313. Entropy: 0.280897.\n",
      "Iteration 23879: Policy loss: 0.050914. Value loss: 0.101135. Entropy: 0.278908.\n",
      "Iteration 23880: Policy loss: 0.052390. Value loss: 0.067378. Entropy: 0.279252.\n",
      "episode: 8111   score: 415.0  epsilon: 1.0    steps: 880  evaluation reward: 477.1\n",
      "Training network. lr: 0.000067. clip: 0.026742\n",
      "Iteration 23881: Policy loss: 0.221729. Value loss: 0.086509. Entropy: 0.298914.\n",
      "Iteration 23882: Policy loss: 0.218525. Value loss: 0.037843. Entropy: 0.300490.\n",
      "Iteration 23883: Policy loss: 0.218246. Value loss: 0.029119. Entropy: 0.298736.\n",
      "Training network. lr: 0.000067. clip: 0.026742\n",
      "Iteration 23884: Policy loss: -0.305399. Value loss: 0.272430. Entropy: 0.312648.\n",
      "Iteration 23885: Policy loss: -0.289730. Value loss: 0.116030. Entropy: 0.312393.\n",
      "Iteration 23886: Policy loss: -0.325912. Value loss: 0.062916. Entropy: 0.312597.\n",
      "Training network. lr: 0.000067. clip: 0.026742\n",
      "Iteration 23887: Policy loss: -0.083846. Value loss: 0.134331. Entropy: 0.309492.\n",
      "Iteration 23888: Policy loss: -0.078198. Value loss: 0.047392. Entropy: 0.309506.\n",
      "Iteration 23889: Policy loss: -0.073594. Value loss: 0.033318. Entropy: 0.309157.\n",
      "Training network. lr: 0.000067. clip: 0.026742\n",
      "Iteration 23890: Policy loss: 0.034233. Value loss: 0.102985. Entropy: 0.313152.\n",
      "Iteration 23891: Policy loss: 0.031074. Value loss: 0.042966. Entropy: 0.313697.\n",
      "Iteration 23892: Policy loss: 0.030702. Value loss: 0.028912. Entropy: 0.313425.\n",
      "episode: 8112   score: 580.0  epsilon: 1.0    steps: 336  evaluation reward: 478.95\n",
      "episode: 8113   score: 620.0  epsilon: 1.0    steps: 496  evaluation reward: 479.5\n",
      "episode: 8114   score: 620.0  epsilon: 1.0    steps: 640  evaluation reward: 482.05\n",
      "Training network. lr: 0.000067. clip: 0.026742\n",
      "Iteration 23893: Policy loss: 0.086406. Value loss: 0.124761. Entropy: 0.274367.\n",
      "Iteration 23894: Policy loss: 0.095006. Value loss: 0.057985. Entropy: 0.273453.\n",
      "Iteration 23895: Policy loss: 0.091355. Value loss: 0.037792. Entropy: 0.273045.\n",
      "Training network. lr: 0.000067. clip: 0.026742\n",
      "Iteration 23896: Policy loss: -0.001858. Value loss: 0.125205. Entropy: 0.302463.\n",
      "Iteration 23897: Policy loss: -0.000838. Value loss: 0.061100. Entropy: 0.303552.\n",
      "Iteration 23898: Policy loss: -0.007498. Value loss: 0.041730. Entropy: 0.302972.\n",
      "Training network. lr: 0.000067. clip: 0.026742\n",
      "Iteration 23899: Policy loss: -0.043433. Value loss: 0.071738. Entropy: 0.313359.\n",
      "Iteration 23900: Policy loss: -0.039599. Value loss: 0.045752. Entropy: 0.313453.\n",
      "Iteration 23901: Policy loss: -0.047566. Value loss: 0.032226. Entropy: 0.313554.\n",
      "episode: 8115   score: 975.0  epsilon: 1.0    steps: 64  evaluation reward: 486.4\n",
      "Training network. lr: 0.000066. clip: 0.026585\n",
      "Iteration 23902: Policy loss: 0.189255. Value loss: 0.144080. Entropy: 0.303586.\n",
      "Iteration 23903: Policy loss: 0.186035. Value loss: 0.069285. Entropy: 0.303139.\n",
      "Iteration 23904: Policy loss: 0.189066. Value loss: 0.045605. Entropy: 0.304033.\n",
      "Training network. lr: 0.000066. clip: 0.026585\n",
      "Iteration 23905: Policy loss: 0.175022. Value loss: 0.118112. Entropy: 0.297222.\n",
      "Iteration 23906: Policy loss: 0.162249. Value loss: 0.046793. Entropy: 0.296668.\n",
      "Iteration 23907: Policy loss: 0.165874. Value loss: 0.032726. Entropy: 0.297220.\n",
      "episode: 8116   score: 470.0  epsilon: 1.0    steps: 984  evaluation reward: 486.0\n",
      "Training network. lr: 0.000066. clip: 0.026585\n",
      "Iteration 23908: Policy loss: 0.149013. Value loss: 0.280486. Entropy: 0.311294.\n",
      "Iteration 23909: Policy loss: 0.146936. Value loss: 0.107482. Entropy: 0.311624.\n",
      "Iteration 23910: Policy loss: 0.133896. Value loss: 0.045525. Entropy: 0.311264.\n",
      "Training network. lr: 0.000066. clip: 0.026585\n",
      "Iteration 23911: Policy loss: -0.305157. Value loss: 0.284982. Entropy: 0.294215.\n",
      "Iteration 23912: Policy loss: -0.299048. Value loss: 0.141979. Entropy: 0.294314.\n",
      "Iteration 23913: Policy loss: -0.311048. Value loss: 0.069490. Entropy: 0.293731.\n",
      "episode: 8117   score: 640.0  epsilon: 1.0    steps: 256  evaluation reward: 486.05\n",
      "episode: 8118   score: 755.0  epsilon: 1.0    steps: 264  evaluation reward: 487.55\n",
      "episode: 8119   score: 295.0  epsilon: 1.0    steps: 360  evaluation reward: 486.45\n",
      "episode: 8120   score: 365.0  epsilon: 1.0    steps: 544  evaluation reward: 485.45\n",
      "Training network. lr: 0.000066. clip: 0.026585\n",
      "Iteration 23914: Policy loss: -0.118115. Value loss: 0.085996. Entropy: 0.255640.\n",
      "Iteration 23915: Policy loss: -0.117917. Value loss: 0.048003. Entropy: 0.254931.\n",
      "Iteration 23916: Policy loss: -0.114666. Value loss: 0.038437. Entropy: 0.255815.\n",
      "episode: 8121   score: 425.0  epsilon: 1.0    steps: 224  evaluation reward: 486.35\n",
      "Training network. lr: 0.000066. clip: 0.026585\n",
      "Iteration 23917: Policy loss: -0.174515. Value loss: 0.135518. Entropy: 0.290358.\n",
      "Iteration 23918: Policy loss: -0.177396. Value loss: 0.068548. Entropy: 0.290407.\n",
      "Iteration 23919: Policy loss: -0.186440. Value loss: 0.049280. Entropy: 0.290836.\n",
      "Training network. lr: 0.000066. clip: 0.026585\n",
      "Iteration 23920: Policy loss: -0.148825. Value loss: 0.201868. Entropy: 0.312789.\n",
      "Iteration 23921: Policy loss: -0.155909. Value loss: 0.097045. Entropy: 0.312164.\n",
      "Iteration 23922: Policy loss: -0.145183. Value loss: 0.064217. Entropy: 0.311858.\n",
      "episode: 8122   score: 345.0  epsilon: 1.0    steps: 128  evaluation reward: 485.9\n",
      "Training network. lr: 0.000066. clip: 0.026585\n",
      "Iteration 23923: Policy loss: 0.468376. Value loss: 0.197512. Entropy: 0.300083.\n",
      "Iteration 23924: Policy loss: 0.451056. Value loss: 0.081392. Entropy: 0.299335.\n",
      "Iteration 23925: Policy loss: 0.454385. Value loss: 0.064631. Entropy: 0.300289.\n",
      "episode: 8123   score: 645.0  epsilon: 1.0    steps: 528  evaluation reward: 484.4\n",
      "episode: 8124   score: 180.0  epsilon: 1.0    steps: 824  evaluation reward: 480.3\n",
      "Training network. lr: 0.000066. clip: 0.026585\n",
      "Iteration 23926: Policy loss: 0.059809. Value loss: 0.086549. Entropy: 0.283526.\n",
      "Iteration 23927: Policy loss: 0.058624. Value loss: 0.046253. Entropy: 0.282318.\n",
      "Iteration 23928: Policy loss: 0.060459. Value loss: 0.035072. Entropy: 0.283784.\n",
      "Training network. lr: 0.000066. clip: 0.026585\n",
      "Iteration 23929: Policy loss: -0.194096. Value loss: 0.177350. Entropy: 0.301127.\n",
      "Iteration 23930: Policy loss: -0.196418. Value loss: 0.080610. Entropy: 0.302436.\n",
      "Iteration 23931: Policy loss: -0.203857. Value loss: 0.055102. Entropy: 0.301105.\n",
      "episode: 8125   score: 315.0  epsilon: 1.0    steps: 568  evaluation reward: 479.85\n",
      "Training network. lr: 0.000066. clip: 0.026585\n",
      "Iteration 23932: Policy loss: -0.337544. Value loss: 0.251146. Entropy: 0.295793.\n",
      "Iteration 23933: Policy loss: -0.337669. Value loss: 0.125824. Entropy: 0.296518.\n",
      "Iteration 23934: Policy loss: -0.334538. Value loss: 0.065683. Entropy: 0.296650.\n",
      "episode: 8126   score: 405.0  epsilon: 1.0    steps: 928  evaluation reward: 477.7\n",
      "Training network. lr: 0.000066. clip: 0.026585\n",
      "Iteration 23935: Policy loss: 0.155251. Value loss: 0.143897. Entropy: 0.305241.\n",
      "Iteration 23936: Policy loss: 0.144912. Value loss: 0.047814. Entropy: 0.303948.\n",
      "Iteration 23937: Policy loss: 0.150600. Value loss: 0.032162. Entropy: 0.303750.\n",
      "Training network. lr: 0.000066. clip: 0.026585\n",
      "Iteration 23938: Policy loss: 0.279796. Value loss: 0.160314. Entropy: 0.305884.\n",
      "Iteration 23939: Policy loss: 0.271591. Value loss: 0.050789. Entropy: 0.305546.\n",
      "Iteration 23940: Policy loss: 0.265725. Value loss: 0.032113. Entropy: 0.305439.\n",
      "episode: 8127   score: 695.0  epsilon: 1.0    steps: 1016  evaluation reward: 480.4\n",
      "Training network. lr: 0.000066. clip: 0.026585\n",
      "Iteration 23941: Policy loss: -0.169811. Value loss: 0.319215. Entropy: 0.309876.\n",
      "Iteration 23942: Policy loss: -0.158095. Value loss: 0.117240. Entropy: 0.310111.\n",
      "Iteration 23943: Policy loss: -0.190155. Value loss: 0.070926. Entropy: 0.309578.\n",
      "episode: 8128   score: 185.0  epsilon: 1.0    steps: 752  evaluation reward: 478.35\n",
      "episode: 8129   score: 625.0  epsilon: 1.0    steps: 960  evaluation reward: 480.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000066. clip: 0.026585\n",
      "Iteration 23944: Policy loss: 0.311367. Value loss: 0.213871. Entropy: 0.287294.\n",
      "Iteration 23945: Policy loss: 0.301036. Value loss: 0.085116. Entropy: 0.287864.\n",
      "Iteration 23946: Policy loss: 0.289094. Value loss: 0.067489. Entropy: 0.286909.\n",
      "episode: 8130   score: 495.0  epsilon: 1.0    steps: 440  evaluation reward: 480.5\n",
      "episode: 8131   score: 540.0  epsilon: 1.0    steps: 584  evaluation reward: 482.85\n",
      "Training network. lr: 0.000066. clip: 0.026585\n",
      "Iteration 23947: Policy loss: 0.007381. Value loss: 0.131185. Entropy: 0.274226.\n",
      "Iteration 23948: Policy loss: -0.001548. Value loss: 0.049901. Entropy: 0.274143.\n",
      "Iteration 23949: Policy loss: 0.011725. Value loss: 0.036129. Entropy: 0.274266.\n",
      "episode: 8132   score: 450.0  epsilon: 1.0    steps: 728  evaluation reward: 482.15\n",
      "Training network. lr: 0.000066. clip: 0.026585\n",
      "Iteration 23950: Policy loss: 0.036825. Value loss: 0.125897. Entropy: 0.297405.\n",
      "Iteration 23951: Policy loss: 0.037545. Value loss: 0.070195. Entropy: 0.295284.\n",
      "Iteration 23952: Policy loss: 0.034521. Value loss: 0.056019. Entropy: 0.296727.\n",
      "episode: 8133   score: 555.0  epsilon: 1.0    steps: 816  evaluation reward: 483.75\n",
      "Training network. lr: 0.000066. clip: 0.026429\n",
      "Iteration 23953: Policy loss: -0.092834. Value loss: 0.124570. Entropy: 0.301861.\n",
      "Iteration 23954: Policy loss: -0.091073. Value loss: 0.050379. Entropy: 0.301342.\n",
      "Iteration 23955: Policy loss: -0.098836. Value loss: 0.038555. Entropy: 0.301656.\n",
      "Training network. lr: 0.000066. clip: 0.026429\n",
      "Iteration 23956: Policy loss: 0.183837. Value loss: 0.086237. Entropy: 0.307286.\n",
      "Iteration 23957: Policy loss: 0.187716. Value loss: 0.038141. Entropy: 0.305979.\n",
      "Iteration 23958: Policy loss: 0.182856. Value loss: 0.023812. Entropy: 0.306474.\n",
      "Training network. lr: 0.000066. clip: 0.026429\n",
      "Iteration 23959: Policy loss: -0.792395. Value loss: 0.359035. Entropy: 0.311969.\n",
      "Iteration 23960: Policy loss: -0.808853. Value loss: 0.181683. Entropy: 0.313044.\n",
      "Iteration 23961: Policy loss: -0.805932. Value loss: 0.093376. Entropy: 0.311956.\n",
      "Training network. lr: 0.000066. clip: 0.026429\n",
      "Iteration 23962: Policy loss: 0.126589. Value loss: 0.256509. Entropy: 0.305523.\n",
      "Iteration 23963: Policy loss: 0.134075. Value loss: 0.071286. Entropy: 0.304414.\n",
      "Iteration 23964: Policy loss: 0.128439. Value loss: 0.047392. Entropy: 0.304577.\n",
      "episode: 8134   score: 425.0  epsilon: 1.0    steps: 864  evaluation reward: 482.35\n",
      "Training network. lr: 0.000066. clip: 0.026429\n",
      "Iteration 23965: Policy loss: 0.086173. Value loss: 0.151067. Entropy: 0.303671.\n",
      "Iteration 23966: Policy loss: 0.088799. Value loss: 0.078771. Entropy: 0.303515.\n",
      "Iteration 23967: Policy loss: 0.085299. Value loss: 0.047240. Entropy: 0.303412.\n",
      "Training network. lr: 0.000066. clip: 0.026429\n",
      "Iteration 23968: Policy loss: 0.007453. Value loss: 0.134771. Entropy: 0.307817.\n",
      "Iteration 23969: Policy loss: -0.001767. Value loss: 0.067781. Entropy: 0.307318.\n",
      "Iteration 23970: Policy loss: -0.003190. Value loss: 0.048068. Entropy: 0.307105.\n",
      "episode: 8135   score: 605.0  epsilon: 1.0    steps: 272  evaluation reward: 485.1\n",
      "episode: 8136   score: 640.0  epsilon: 1.0    steps: 288  evaluation reward: 487.65\n",
      "episode: 8137   score: 395.0  epsilon: 1.0    steps: 360  evaluation reward: 488.1\n",
      "Training network. lr: 0.000066. clip: 0.026429\n",
      "Iteration 23971: Policy loss: -0.155030. Value loss: 0.306331. Entropy: 0.260826.\n",
      "Iteration 23972: Policy loss: -0.177424. Value loss: 0.166399. Entropy: 0.261690.\n",
      "Iteration 23973: Policy loss: -0.191466. Value loss: 0.102544. Entropy: 0.260756.\n",
      "Training network. lr: 0.000066. clip: 0.026429\n",
      "Iteration 23974: Policy loss: 0.176285. Value loss: 0.104812. Entropy: 0.309271.\n",
      "Iteration 23975: Policy loss: 0.180851. Value loss: 0.047382. Entropy: 0.308755.\n",
      "Iteration 23976: Policy loss: 0.174631. Value loss: 0.031937. Entropy: 0.308785.\n",
      "Training network. lr: 0.000066. clip: 0.026429\n",
      "Iteration 23977: Policy loss: -0.200667. Value loss: 0.123788. Entropy: 0.309513.\n",
      "Iteration 23978: Policy loss: -0.200779. Value loss: 0.062056. Entropy: 0.309951.\n",
      "Iteration 23979: Policy loss: -0.206652. Value loss: 0.046401. Entropy: 0.309136.\n",
      "episode: 8138   score: 510.0  epsilon: 1.0    steps: 80  evaluation reward: 489.15\n",
      "episode: 8139   score: 965.0  epsilon: 1.0    steps: 384  evaluation reward: 493.2\n",
      "Training network. lr: 0.000066. clip: 0.026429\n",
      "Iteration 23980: Policy loss: 0.025954. Value loss: 0.231091. Entropy: 0.272415.\n",
      "Iteration 23981: Policy loss: 0.019649. Value loss: 0.086935. Entropy: 0.274162.\n",
      "Iteration 23982: Policy loss: 0.010279. Value loss: 0.055280. Entropy: 0.271665.\n",
      "episode: 8140   score: 695.0  epsilon: 1.0    steps: 504  evaluation reward: 496.95\n",
      "Training network. lr: 0.000066. clip: 0.026429\n",
      "Iteration 23983: Policy loss: 0.623583. Value loss: 0.289754. Entropy: 0.297550.\n",
      "Iteration 23984: Policy loss: 0.579170. Value loss: 0.081264. Entropy: 0.298187.\n",
      "Iteration 23985: Policy loss: 0.605304. Value loss: 0.051657. Entropy: 0.296192.\n",
      "episode: 8141   score: 325.0  epsilon: 1.0    steps: 664  evaluation reward: 495.95\n",
      "Training network. lr: 0.000066. clip: 0.026429\n",
      "Iteration 23986: Policy loss: 0.174058. Value loss: 0.140891. Entropy: 0.294248.\n",
      "Iteration 23987: Policy loss: 0.173024. Value loss: 0.063862. Entropy: 0.293401.\n",
      "Iteration 23988: Policy loss: 0.170829. Value loss: 0.049353. Entropy: 0.294257.\n",
      "episode: 8142   score: 705.0  epsilon: 1.0    steps: 720  evaluation reward: 499.6\n",
      "episode: 8143   score: 260.0  epsilon: 1.0    steps: 904  evaluation reward: 499.2\n",
      "Training network. lr: 0.000066. clip: 0.026429\n",
      "Iteration 23989: Policy loss: 0.377015. Value loss: 0.153248. Entropy: 0.294526.\n",
      "Iteration 23990: Policy loss: 0.372728. Value loss: 0.082678. Entropy: 0.293763.\n",
      "Iteration 23991: Policy loss: 0.372526. Value loss: 0.062688. Entropy: 0.294670.\n",
      "episode: 8144   score: 350.0  epsilon: 1.0    steps: 536  evaluation reward: 498.0\n",
      "Training network. lr: 0.000066. clip: 0.026429\n",
      "Iteration 23992: Policy loss: 0.037727. Value loss: 0.114660. Entropy: 0.294156.\n",
      "Iteration 23993: Policy loss: 0.040169. Value loss: 0.070279. Entropy: 0.293574.\n",
      "Iteration 23994: Policy loss: 0.033607. Value loss: 0.050591. Entropy: 0.293989.\n",
      "episode: 8145   score: 570.0  epsilon: 1.0    steps: 96  evaluation reward: 500.4\n",
      "episode: 8146   score: 180.0  epsilon: 1.0    steps: 160  evaluation reward: 495.4\n",
      "Training network. lr: 0.000066. clip: 0.026429\n",
      "Iteration 23995: Policy loss: -0.215025. Value loss: 0.130857. Entropy: 0.280314.\n",
      "Iteration 23996: Policy loss: -0.222505. Value loss: 0.058999. Entropy: 0.281357.\n",
      "Iteration 23997: Policy loss: -0.222920. Value loss: 0.046712. Entropy: 0.282186.\n",
      "Training network. lr: 0.000066. clip: 0.026429\n",
      "Iteration 23998: Policy loss: -0.186100. Value loss: 0.377072. Entropy: 0.299724.\n",
      "Iteration 23999: Policy loss: -0.180213. Value loss: 0.201836. Entropy: 0.299267.\n",
      "Iteration 24000: Policy loss: -0.201775. Value loss: 0.156946. Entropy: 0.298801.\n",
      "episode: 8147   score: 365.0  epsilon: 1.0    steps: 360  evaluation reward: 492.85\n",
      "Training network. lr: 0.000066. clip: 0.026281\n",
      "Iteration 24001: Policy loss: -0.084374. Value loss: 0.072899. Entropy: 0.289942.\n",
      "Iteration 24002: Policy loss: -0.077461. Value loss: 0.036888. Entropy: 0.288526.\n",
      "Iteration 24003: Policy loss: -0.082630. Value loss: 0.029088. Entropy: 0.289665.\n",
      "Training network. lr: 0.000066. clip: 0.026281\n",
      "Iteration 24004: Policy loss: 0.316604. Value loss: 0.346340. Entropy: 0.310033.\n",
      "Iteration 24005: Policy loss: 0.316096. Value loss: 0.250657. Entropy: 0.310012.\n",
      "Iteration 24006: Policy loss: 0.307157. Value loss: 0.194662. Entropy: 0.310219.\n",
      "Training network. lr: 0.000066. clip: 0.026281\n",
      "Iteration 24007: Policy loss: 0.096517. Value loss: 0.119587. Entropy: 0.306631.\n",
      "Iteration 24008: Policy loss: 0.094593. Value loss: 0.064067. Entropy: 0.305972.\n",
      "Iteration 24009: Policy loss: 0.098272. Value loss: 0.046549. Entropy: 0.305018.\n",
      "episode: 8148   score: 325.0  epsilon: 1.0    steps: 992  evaluation reward: 490.15\n",
      "Training network. lr: 0.000066. clip: 0.026281\n",
      "Iteration 24010: Policy loss: -0.081881. Value loss: 0.390123. Entropy: 0.306295.\n",
      "Iteration 24011: Policy loss: -0.090392. Value loss: 0.230726. Entropy: 0.307406.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24012: Policy loss: -0.088506. Value loss: 0.162557. Entropy: 0.306543.\n",
      "episode: 8149   score: 620.0  epsilon: 1.0    steps: 360  evaluation reward: 492.5\n",
      "episode: 8150   score: 670.0  epsilon: 1.0    steps: 400  evaluation reward: 494.25\n",
      "Training network. lr: 0.000066. clip: 0.026281\n",
      "Iteration 24013: Policy loss: 0.237382. Value loss: 0.095917. Entropy: 0.269268.\n",
      "Iteration 24014: Policy loss: 0.241140. Value loss: 0.047891. Entropy: 0.269171.\n",
      "Iteration 24015: Policy loss: 0.242660. Value loss: 0.038421. Entropy: 0.269422.\n",
      "now time :  2019-09-06 15:00:28.051386\n",
      "episode: 8151   score: 350.0  epsilon: 1.0    steps: 208  evaluation reward: 493.7\n",
      "episode: 8152   score: 575.0  epsilon: 1.0    steps: 888  evaluation reward: 492.15\n",
      "Training network. lr: 0.000066. clip: 0.026281\n",
      "Iteration 24016: Policy loss: -0.050947. Value loss: 0.075410. Entropy: 0.291281.\n",
      "Iteration 24017: Policy loss: -0.060181. Value loss: 0.039890. Entropy: 0.290407.\n",
      "Iteration 24018: Policy loss: -0.057887. Value loss: 0.031263. Entropy: 0.291061.\n",
      "Training network. lr: 0.000066. clip: 0.026281\n",
      "Iteration 24019: Policy loss: -0.225061. Value loss: 0.305040. Entropy: 0.300030.\n",
      "Iteration 24020: Policy loss: -0.223901. Value loss: 0.188029. Entropy: 0.299366.\n",
      "Iteration 24021: Policy loss: -0.188464. Value loss: 0.097518. Entropy: 0.299896.\n",
      "episode: 8153   score: 500.0  epsilon: 1.0    steps: 504  evaluation reward: 493.95\n",
      "Training network. lr: 0.000066. clip: 0.026281\n",
      "Iteration 24022: Policy loss: 0.254968. Value loss: 0.170634. Entropy: 0.301266.\n",
      "Iteration 24023: Policy loss: 0.244711. Value loss: 0.065058. Entropy: 0.298422.\n",
      "Iteration 24024: Policy loss: 0.245759. Value loss: 0.048067. Entropy: 0.300034.\n",
      "Training network. lr: 0.000066. clip: 0.026281\n",
      "Iteration 24025: Policy loss: 0.081091. Value loss: 0.104807. Entropy: 0.312132.\n",
      "Iteration 24026: Policy loss: 0.084324. Value loss: 0.038790. Entropy: 0.311987.\n",
      "Iteration 24027: Policy loss: 0.077265. Value loss: 0.027981. Entropy: 0.312559.\n",
      "Training network. lr: 0.000066. clip: 0.026281\n",
      "Iteration 24028: Policy loss: -0.017402. Value loss: 0.127789. Entropy: 0.311589.\n",
      "Iteration 24029: Policy loss: -0.023887. Value loss: 0.058536. Entropy: 0.308711.\n",
      "Iteration 24030: Policy loss: -0.021549. Value loss: 0.043320. Entropy: 0.308886.\n",
      "Training network. lr: 0.000066. clip: 0.026281\n",
      "Iteration 24031: Policy loss: -0.170565. Value loss: 0.347469. Entropy: 0.308763.\n",
      "Iteration 24032: Policy loss: -0.178623. Value loss: 0.189819. Entropy: 0.308864.\n",
      "Iteration 24033: Policy loss: -0.191029. Value loss: 0.118871. Entropy: 0.307851.\n",
      "episode: 8154   score: 465.0  epsilon: 1.0    steps: 488  evaluation reward: 493.15\n",
      "episode: 8155   score: 330.0  epsilon: 1.0    steps: 720  evaluation reward: 493.1\n",
      "episode: 8156   score: 575.0  epsilon: 1.0    steps: 856  evaluation reward: 495.0\n",
      "Training network. lr: 0.000066. clip: 0.026281\n",
      "Iteration 24034: Policy loss: 0.239296. Value loss: 0.159334. Entropy: 0.283940.\n",
      "Iteration 24035: Policy loss: 0.240573. Value loss: 0.079011. Entropy: 0.282747.\n",
      "Iteration 24036: Policy loss: 0.223174. Value loss: 0.053480. Entropy: 0.283462.\n",
      "Training network. lr: 0.000066. clip: 0.026281\n",
      "Iteration 24037: Policy loss: -0.047567. Value loss: 0.121499. Entropy: 0.299808.\n",
      "Iteration 24038: Policy loss: -0.049880. Value loss: 0.066423. Entropy: 0.299451.\n",
      "Iteration 24039: Policy loss: -0.049299. Value loss: 0.047033. Entropy: 0.299503.\n",
      "episode: 8157   score: 480.0  epsilon: 1.0    steps: 840  evaluation reward: 490.05\n",
      "Training network. lr: 0.000066. clip: 0.026281\n",
      "Iteration 24040: Policy loss: 0.021067. Value loss: 0.284208. Entropy: 0.306888.\n",
      "Iteration 24041: Policy loss: 0.019353. Value loss: 0.170218. Entropy: 0.304770.\n",
      "Iteration 24042: Policy loss: 0.019707. Value loss: 0.099264. Entropy: 0.306011.\n",
      "Training network. lr: 0.000066. clip: 0.026281\n",
      "Iteration 24043: Policy loss: 0.072246. Value loss: 0.134638. Entropy: 0.307123.\n",
      "Iteration 24044: Policy loss: 0.063047. Value loss: 0.063557. Entropy: 0.307560.\n",
      "Iteration 24045: Policy loss: 0.059180. Value loss: 0.045971. Entropy: 0.307817.\n",
      "episode: 8158   score: 525.0  epsilon: 1.0    steps: 520  evaluation reward: 488.05\n",
      "episode: 8159   score: 935.0  epsilon: 1.0    steps: 528  evaluation reward: 493.7\n",
      "Training network. lr: 0.000066. clip: 0.026281\n",
      "Iteration 24046: Policy loss: 0.182526. Value loss: 0.106815. Entropy: 0.283583.\n",
      "Iteration 24047: Policy loss: 0.181901. Value loss: 0.052460. Entropy: 0.282089.\n",
      "Iteration 24048: Policy loss: 0.168691. Value loss: 0.041267. Entropy: 0.281755.\n",
      "episode: 8160   score: 595.0  epsilon: 1.0    steps: 1000  evaluation reward: 494.1\n",
      "Training network. lr: 0.000066. clip: 0.026281\n",
      "Iteration 24049: Policy loss: 0.239276. Value loss: 0.149593. Entropy: 0.307293.\n",
      "Iteration 24050: Policy loss: 0.238376. Value loss: 0.076259. Entropy: 0.306256.\n",
      "Iteration 24051: Policy loss: 0.235196. Value loss: 0.047973. Entropy: 0.307349.\n",
      "Training network. lr: 0.000065. clip: 0.026125\n",
      "Iteration 24052: Policy loss: -0.338503. Value loss: 0.471233. Entropy: 0.301135.\n",
      "Iteration 24053: Policy loss: -0.331809. Value loss: 0.288736. Entropy: 0.299678.\n",
      "Iteration 24054: Policy loss: -0.341479. Value loss: 0.213166. Entropy: 0.299428.\n",
      "episode: 8161   score: 485.0  epsilon: 1.0    steps: 304  evaluation reward: 495.2\n",
      "episode: 8162   score: 330.0  epsilon: 1.0    steps: 432  evaluation reward: 493.95\n",
      "episode: 8163   score: 260.0  epsilon: 1.0    steps: 448  evaluation reward: 492.3\n",
      "Training network. lr: 0.000065. clip: 0.026125\n",
      "Iteration 24055: Policy loss: -0.340934. Value loss: 0.313689. Entropy: 0.268660.\n",
      "Iteration 24056: Policy loss: -0.352052. Value loss: 0.158375. Entropy: 0.271432.\n",
      "Iteration 24057: Policy loss: -0.358664. Value loss: 0.066113. Entropy: 0.271012.\n",
      "episode: 8164   score: 730.0  epsilon: 1.0    steps: 704  evaluation reward: 497.8\n",
      "Training network. lr: 0.000065. clip: 0.026125\n",
      "Iteration 24058: Policy loss: -0.268429. Value loss: 0.170647. Entropy: 0.300393.\n",
      "Iteration 24059: Policy loss: -0.268897. Value loss: 0.075923. Entropy: 0.298871.\n",
      "Iteration 24060: Policy loss: -0.274129. Value loss: 0.052444. Entropy: 0.298567.\n",
      "Training network. lr: 0.000065. clip: 0.026125\n",
      "Iteration 24061: Policy loss: 0.369426. Value loss: 0.167492. Entropy: 0.304660.\n",
      "Iteration 24062: Policy loss: 0.367358. Value loss: 0.077591. Entropy: 0.304401.\n",
      "Iteration 24063: Policy loss: 0.362030. Value loss: 0.056092. Entropy: 0.303915.\n",
      "episode: 8165   score: 225.0  epsilon: 1.0    steps: 352  evaluation reward: 497.65\n",
      "Training network. lr: 0.000065. clip: 0.026125\n",
      "Iteration 24064: Policy loss: -0.028703. Value loss: 0.393612. Entropy: 0.300652.\n",
      "Iteration 24065: Policy loss: -0.008778. Value loss: 0.255224. Entropy: 0.299679.\n",
      "Iteration 24066: Policy loss: -0.021980. Value loss: 0.209376. Entropy: 0.301002.\n",
      "episode: 8166   score: 420.0  epsilon: 1.0    steps: 352  evaluation reward: 497.9\n",
      "Training network. lr: 0.000065. clip: 0.026125\n",
      "Iteration 24067: Policy loss: -0.035710. Value loss: 0.094018. Entropy: 0.292657.\n",
      "Iteration 24068: Policy loss: -0.031110. Value loss: 0.052356. Entropy: 0.294346.\n",
      "Iteration 24069: Policy loss: -0.035800. Value loss: 0.040657. Entropy: 0.292780.\n",
      "episode: 8167   score: 285.0  epsilon: 1.0    steps: 560  evaluation reward: 497.05\n",
      "Training network. lr: 0.000065. clip: 0.026125\n",
      "Iteration 24070: Policy loss: -0.299376. Value loss: 0.347384. Entropy: 0.293834.\n",
      "Iteration 24071: Policy loss: -0.319324. Value loss: 0.178522. Entropy: 0.292804.\n",
      "Iteration 24072: Policy loss: -0.324012. Value loss: 0.110709. Entropy: 0.291495.\n",
      "Training network. lr: 0.000065. clip: 0.026125\n",
      "Iteration 24073: Policy loss: 0.098774. Value loss: 0.197076. Entropy: 0.308626.\n",
      "Iteration 24074: Policy loss: 0.096479. Value loss: 0.088266. Entropy: 0.309407.\n",
      "Iteration 24075: Policy loss: 0.090343. Value loss: 0.061710. Entropy: 0.307929.\n",
      "episode: 8168   score: 940.0  epsilon: 1.0    steps: 952  evaluation reward: 501.95\n",
      "Training network. lr: 0.000065. clip: 0.026125\n",
      "Iteration 24076: Policy loss: -0.108212. Value loss: 0.362536. Entropy: 0.308794.\n",
      "Iteration 24077: Policy loss: -0.117125. Value loss: 0.241411. Entropy: 0.307318.\n",
      "Iteration 24078: Policy loss: -0.083737. Value loss: 0.149155. Entropy: 0.306980.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8169   score: 445.0  epsilon: 1.0    steps: 136  evaluation reward: 499.85\n",
      "episode: 8170   score: 520.0  epsilon: 1.0    steps: 832  evaluation reward: 497.65\n",
      "Training network. lr: 0.000065. clip: 0.026125\n",
      "Iteration 24079: Policy loss: -0.062788. Value loss: 0.143775. Entropy: 0.281454.\n",
      "Iteration 24080: Policy loss: -0.061671. Value loss: 0.061914. Entropy: 0.282830.\n",
      "Iteration 24081: Policy loss: -0.064479. Value loss: 0.040158. Entropy: 0.281695.\n",
      "Training network. lr: 0.000065. clip: 0.026125\n",
      "Iteration 24082: Policy loss: -0.003981. Value loss: 0.141493. Entropy: 0.311492.\n",
      "Iteration 24083: Policy loss: -0.003267. Value loss: 0.078520. Entropy: 0.311050.\n",
      "Iteration 24084: Policy loss: -0.007262. Value loss: 0.048195. Entropy: 0.310696.\n",
      "episode: 8171   score: 420.0  epsilon: 1.0    steps: 736  evaluation reward: 496.35\n",
      "episode: 8172   score: 775.0  epsilon: 1.0    steps: 872  evaluation reward: 500.95\n",
      "Training network. lr: 0.000065. clip: 0.026125\n",
      "Iteration 24085: Policy loss: -0.093407. Value loss: 0.205101. Entropy: 0.293018.\n",
      "Iteration 24086: Policy loss: -0.098507. Value loss: 0.099306. Entropy: 0.292825.\n",
      "Iteration 24087: Policy loss: -0.093325. Value loss: 0.065101. Entropy: 0.293591.\n",
      "episode: 8173   score: 335.0  epsilon: 1.0    steps: 304  evaluation reward: 501.0\n",
      "Training network. lr: 0.000065. clip: 0.026125\n",
      "Iteration 24088: Policy loss: -0.001066. Value loss: 0.075981. Entropy: 0.288378.\n",
      "Iteration 24089: Policy loss: -0.000272. Value loss: 0.031275. Entropy: 0.289309.\n",
      "Iteration 24090: Policy loss: -0.002108. Value loss: 0.025410. Entropy: 0.288199.\n",
      "Training network. lr: 0.000065. clip: 0.026125\n",
      "Iteration 24091: Policy loss: 0.323739. Value loss: 0.176372. Entropy: 0.300934.\n",
      "Iteration 24092: Policy loss: 0.327683. Value loss: 0.065868. Entropy: 0.299480.\n",
      "Iteration 24093: Policy loss: 0.323378. Value loss: 0.042535. Entropy: 0.300901.\n",
      "Training network. lr: 0.000065. clip: 0.026125\n",
      "Iteration 24094: Policy loss: -0.131365. Value loss: 0.183591. Entropy: 0.311320.\n",
      "Iteration 24095: Policy loss: -0.150684. Value loss: 0.069238. Entropy: 0.310841.\n",
      "Iteration 24096: Policy loss: -0.136090. Value loss: 0.041989. Entropy: 0.311021.\n",
      "Training network. lr: 0.000065. clip: 0.026125\n",
      "Iteration 24097: Policy loss: -0.033426. Value loss: 0.208339. Entropy: 0.307580.\n",
      "Iteration 24098: Policy loss: -0.016820. Value loss: 0.088868. Entropy: 0.307455.\n",
      "Iteration 24099: Policy loss: -0.023488. Value loss: 0.067988. Entropy: 0.307723.\n",
      "Training network. lr: 0.000065. clip: 0.026125\n",
      "Iteration 24100: Policy loss: 0.286416. Value loss: 0.196102. Entropy: 0.307619.\n",
      "Iteration 24101: Policy loss: 0.281379. Value loss: 0.089324. Entropy: 0.306267.\n",
      "Iteration 24102: Policy loss: 0.284256. Value loss: 0.054806. Entropy: 0.306296.\n",
      "episode: 8174   score: 635.0  epsilon: 1.0    steps: 176  evaluation reward: 502.4\n",
      "episode: 8175   score: 415.0  epsilon: 1.0    steps: 792  evaluation reward: 503.95\n",
      "Training network. lr: 0.000065. clip: 0.025968\n",
      "Iteration 24103: Policy loss: 0.159374. Value loss: 0.169291. Entropy: 0.286906.\n",
      "Iteration 24104: Policy loss: 0.155650. Value loss: 0.086132. Entropy: 0.287122.\n",
      "Iteration 24105: Policy loss: 0.156455. Value loss: 0.061672. Entropy: 0.286614.\n",
      "Training network. lr: 0.000065. clip: 0.025968\n",
      "Iteration 24106: Policy loss: 0.192368. Value loss: 0.148988. Entropy: 0.308183.\n",
      "Iteration 24107: Policy loss: 0.188828. Value loss: 0.077154. Entropy: 0.308833.\n",
      "Iteration 24108: Policy loss: 0.178801. Value loss: 0.058294. Entropy: 0.307455.\n",
      "episode: 8176   score: 845.0  epsilon: 1.0    steps: 488  evaluation reward: 505.6\n",
      "episode: 8177   score: 620.0  epsilon: 1.0    steps: 728  evaluation reward: 506.7\n",
      "episode: 8178   score: 615.0  epsilon: 1.0    steps: 744  evaluation reward: 506.8\n",
      "Training network. lr: 0.000065. clip: 0.025968\n",
      "Iteration 24109: Policy loss: 0.200523. Value loss: 0.115906. Entropy: 0.282296.\n",
      "Iteration 24110: Policy loss: 0.197579. Value loss: 0.065804. Entropy: 0.282949.\n",
      "Iteration 24111: Policy loss: 0.205798. Value loss: 0.047235. Entropy: 0.282021.\n",
      "episode: 8179   score: 765.0  epsilon: 1.0    steps: 224  evaluation reward: 509.45\n",
      "Training network. lr: 0.000065. clip: 0.025968\n",
      "Iteration 24112: Policy loss: -0.033852. Value loss: 0.061688. Entropy: 0.296933.\n",
      "Iteration 24113: Policy loss: -0.035695. Value loss: 0.030630. Entropy: 0.295526.\n",
      "Iteration 24114: Policy loss: -0.039300. Value loss: 0.024487. Entropy: 0.296390.\n",
      "Training network. lr: 0.000065. clip: 0.025968\n",
      "Iteration 24115: Policy loss: 0.478593. Value loss: 0.230496. Entropy: 0.306025.\n",
      "Iteration 24116: Policy loss: 0.481595. Value loss: 0.088768. Entropy: 0.306178.\n",
      "Iteration 24117: Policy loss: 0.477436. Value loss: 0.066783. Entropy: 0.306274.\n",
      "Training network. lr: 0.000065. clip: 0.025968\n",
      "Iteration 24118: Policy loss: 0.076238. Value loss: 0.456893. Entropy: 0.306965.\n",
      "Iteration 24119: Policy loss: 0.059931. Value loss: 0.348055. Entropy: 0.306833.\n",
      "Iteration 24120: Policy loss: 0.039707. Value loss: 0.282482. Entropy: 0.306600.\n",
      "episode: 8180   score: 405.0  epsilon: 1.0    steps: 104  evaluation reward: 508.55\n",
      "episode: 8181   score: 465.0  epsilon: 1.0    steps: 400  evaluation reward: 507.85\n",
      "Training network. lr: 0.000065. clip: 0.025968\n",
      "Iteration 24121: Policy loss: -0.100350. Value loss: 0.125040. Entropy: 0.284278.\n",
      "Iteration 24122: Policy loss: -0.098235. Value loss: 0.068365. Entropy: 0.283863.\n",
      "Iteration 24123: Policy loss: -0.096289. Value loss: 0.046959. Entropy: 0.283653.\n",
      "Training network. lr: 0.000065. clip: 0.025968\n",
      "Iteration 24124: Policy loss: -0.682462. Value loss: 0.491984. Entropy: 0.307375.\n",
      "Iteration 24125: Policy loss: -0.673691. Value loss: 0.238865. Entropy: 0.305753.\n",
      "Iteration 24126: Policy loss: -0.693097. Value loss: 0.122539. Entropy: 0.307467.\n",
      "episode: 8182   score: 410.0  epsilon: 1.0    steps: 448  evaluation reward: 508.35\n",
      "Training network. lr: 0.000065. clip: 0.025968\n",
      "Iteration 24127: Policy loss: 0.215525. Value loss: 0.122511. Entropy: 0.297825.\n",
      "Iteration 24128: Policy loss: 0.213933. Value loss: 0.062460. Entropy: 0.296585.\n",
      "Iteration 24129: Policy loss: 0.219031. Value loss: 0.048052. Entropy: 0.297238.\n",
      "episode: 8183   score: 330.0  epsilon: 1.0    steps: 600  evaluation reward: 508.0\n",
      "episode: 8184   score: 535.0  epsilon: 1.0    steps: 760  evaluation reward: 505.7\n",
      "episode: 8185   score: 290.0  epsilon: 1.0    steps: 800  evaluation reward: 503.8\n",
      "Training network. lr: 0.000065. clip: 0.025968\n",
      "Iteration 24130: Policy loss: 0.303722. Value loss: 0.272772. Entropy: 0.281286.\n",
      "Iteration 24131: Policy loss: 0.317941. Value loss: 0.109528. Entropy: 0.280835.\n",
      "Iteration 24132: Policy loss: 0.292776. Value loss: 0.073228. Entropy: 0.280590.\n",
      "episode: 8186   score: 565.0  epsilon: 1.0    steps: 216  evaluation reward: 505.85\n",
      "episode: 8187   score: 620.0  epsilon: 1.0    steps: 968  evaluation reward: 505.85\n",
      "Training network. lr: 0.000065. clip: 0.025968\n",
      "Iteration 24133: Policy loss: 0.204509. Value loss: 0.134145. Entropy: 0.291048.\n",
      "Iteration 24134: Policy loss: 0.203073. Value loss: 0.064762. Entropy: 0.291682.\n",
      "Iteration 24135: Policy loss: 0.196548. Value loss: 0.046850. Entropy: 0.290194.\n",
      "Training network. lr: 0.000065. clip: 0.025968\n",
      "Iteration 24136: Policy loss: 0.080083. Value loss: 0.128134. Entropy: 0.299843.\n",
      "Iteration 24137: Policy loss: 0.082949. Value loss: 0.074286. Entropy: 0.299943.\n",
      "Iteration 24138: Policy loss: 0.080953. Value loss: 0.061291. Entropy: 0.300124.\n",
      "Training network. lr: 0.000065. clip: 0.025968\n",
      "Iteration 24139: Policy loss: -0.083967. Value loss: 0.121238. Entropy: 0.306427.\n",
      "Iteration 24140: Policy loss: -0.084251. Value loss: 0.053970. Entropy: 0.305479.\n",
      "Iteration 24141: Policy loss: -0.087485. Value loss: 0.040813. Entropy: 0.306328.\n",
      "episode: 8188   score: 335.0  epsilon: 1.0    steps: 152  evaluation reward: 504.85\n",
      "episode: 8189   score: 285.0  epsilon: 1.0    steps: 520  evaluation reward: 506.2\n",
      "Training network. lr: 0.000065. clip: 0.025968\n",
      "Iteration 24142: Policy loss: 0.256189. Value loss: 0.141345. Entropy: 0.286748.\n",
      "Iteration 24143: Policy loss: 0.261548. Value loss: 0.045966. Entropy: 0.285249.\n",
      "Iteration 24144: Policy loss: 0.244508. Value loss: 0.030516. Entropy: 0.285516.\n",
      "Training network. lr: 0.000065. clip: 0.025968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24145: Policy loss: -0.009392. Value loss: 0.100938. Entropy: 0.306183.\n",
      "Iteration 24146: Policy loss: -0.011758. Value loss: 0.036473. Entropy: 0.305576.\n",
      "Iteration 24147: Policy loss: -0.010866. Value loss: 0.025687. Entropy: 0.306567.\n",
      "Training network. lr: 0.000065. clip: 0.025968\n",
      "Iteration 24148: Policy loss: 0.037180. Value loss: 0.228977. Entropy: 0.308004.\n",
      "Iteration 24149: Policy loss: 0.027417. Value loss: 0.094739. Entropy: 0.306724.\n",
      "Iteration 24150: Policy loss: 0.026070. Value loss: 0.075546. Entropy: 0.306173.\n",
      "episode: 8190   score: 640.0  epsilon: 1.0    steps: 400  evaluation reward: 507.65\n",
      "episode: 8191   score: 315.0  epsilon: 1.0    steps: 488  evaluation reward: 504.8\n",
      "Training network. lr: 0.000065. clip: 0.025820\n",
      "Iteration 24151: Policy loss: 0.041552. Value loss: 0.130620. Entropy: 0.285773.\n",
      "Iteration 24152: Policy loss: 0.039407. Value loss: 0.066788. Entropy: 0.284991.\n",
      "Iteration 24153: Policy loss: 0.035917. Value loss: 0.049012. Entropy: 0.286143.\n",
      "episode: 8192   score: 390.0  epsilon: 1.0    steps: 464  evaluation reward: 505.0\n",
      "episode: 8193   score: 485.0  epsilon: 1.0    steps: 1024  evaluation reward: 500.85\n",
      "Training network. lr: 0.000065. clip: 0.025820\n",
      "Iteration 24154: Policy loss: -0.065825. Value loss: 0.186545. Entropy: 0.297782.\n",
      "Iteration 24155: Policy loss: -0.065433. Value loss: 0.134180. Entropy: 0.297047.\n",
      "Iteration 24156: Policy loss: -0.065807. Value loss: 0.096675. Entropy: 0.298017.\n",
      "Training network. lr: 0.000065. clip: 0.025820\n",
      "Iteration 24157: Policy loss: -0.099540. Value loss: 0.073163. Entropy: 0.299732.\n",
      "Iteration 24158: Policy loss: -0.101955. Value loss: 0.037835. Entropy: 0.300341.\n",
      "Iteration 24159: Policy loss: -0.102519. Value loss: 0.028007. Entropy: 0.298878.\n",
      "Training network. lr: 0.000065. clip: 0.025820\n",
      "Iteration 24160: Policy loss: -0.172091. Value loss: 0.205732. Entropy: 0.302685.\n",
      "Iteration 24161: Policy loss: -0.169499. Value loss: 0.058619. Entropy: 0.303195.\n",
      "Iteration 24162: Policy loss: -0.183179. Value loss: 0.039556. Entropy: 0.302949.\n",
      "Training network. lr: 0.000065. clip: 0.025820\n",
      "Iteration 24163: Policy loss: 0.381418. Value loss: 0.359591. Entropy: 0.308899.\n",
      "Iteration 24164: Policy loss: 0.370446. Value loss: 0.171752. Entropy: 0.308510.\n",
      "Iteration 24165: Policy loss: 0.360655. Value loss: 0.134146. Entropy: 0.308162.\n",
      "episode: 8194   score: 570.0  epsilon: 1.0    steps: 16  evaluation reward: 500.85\n",
      "episode: 8195   score: 715.0  epsilon: 1.0    steps: 128  evaluation reward: 504.65\n",
      "episode: 8196   score: 545.0  epsilon: 1.0    steps: 992  evaluation reward: 508.0\n",
      "Training network. lr: 0.000065. clip: 0.025820\n",
      "Iteration 24166: Policy loss: -0.047620. Value loss: 0.256381. Entropy: 0.289420.\n",
      "Iteration 24167: Policy loss: -0.050928. Value loss: 0.155199. Entropy: 0.288832.\n",
      "Iteration 24168: Policy loss: -0.052511. Value loss: 0.090074. Entropy: 0.288901.\n",
      "episode: 8197   score: 215.0  epsilon: 1.0    steps: 168  evaluation reward: 503.15\n",
      "Training network. lr: 0.000065. clip: 0.025820\n",
      "Iteration 24169: Policy loss: -0.100068. Value loss: 0.114589. Entropy: 0.290393.\n",
      "Iteration 24170: Policy loss: -0.095011. Value loss: 0.053762. Entropy: 0.291673.\n",
      "Iteration 24171: Policy loss: -0.099089. Value loss: 0.040367. Entropy: 0.291529.\n",
      "Training network. lr: 0.000065. clip: 0.025820\n",
      "Iteration 24172: Policy loss: -0.153313. Value loss: 0.171838. Entropy: 0.304968.\n",
      "Iteration 24173: Policy loss: -0.151572. Value loss: 0.090423. Entropy: 0.304309.\n",
      "Iteration 24174: Policy loss: -0.166539. Value loss: 0.071033. Entropy: 0.304696.\n",
      "episode: 8198   score: 560.0  epsilon: 1.0    steps: 136  evaluation reward: 501.35\n",
      "episode: 8199   score: 695.0  epsilon: 1.0    steps: 840  evaluation reward: 504.95\n",
      "Training network. lr: 0.000065. clip: 0.025820\n",
      "Iteration 24175: Policy loss: 0.140209. Value loss: 0.484188. Entropy: 0.292245.\n",
      "Iteration 24176: Policy loss: 0.119116. Value loss: 0.244328. Entropy: 0.294821.\n",
      "Iteration 24177: Policy loss: 0.122791. Value loss: 0.183819. Entropy: 0.293531.\n",
      "Training network. lr: 0.000065. clip: 0.025820\n",
      "Iteration 24178: Policy loss: 0.005532. Value loss: 0.191094. Entropy: 0.311857.\n",
      "Iteration 24179: Policy loss: 0.006394. Value loss: 0.078239. Entropy: 0.310872.\n",
      "Iteration 24180: Policy loss: 0.010814. Value loss: 0.047823. Entropy: 0.311353.\n",
      "Training network. lr: 0.000065. clip: 0.025820\n",
      "Iteration 24181: Policy loss: -0.128293. Value loss: 0.104959. Entropy: 0.311660.\n",
      "Iteration 24182: Policy loss: -0.136529. Value loss: 0.054018. Entropy: 0.310903.\n",
      "Iteration 24183: Policy loss: -0.138398. Value loss: 0.038433. Entropy: 0.310863.\n",
      "episode: 8200   score: 420.0  epsilon: 1.0    steps: 880  evaluation reward: 505.1\n",
      "Training network. lr: 0.000065. clip: 0.025820\n",
      "Iteration 24184: Policy loss: 0.340759. Value loss: 0.249260. Entropy: 0.297823.\n",
      "Iteration 24185: Policy loss: 0.336706. Value loss: 0.105209. Entropy: 0.297313.\n",
      "Iteration 24186: Policy loss: 0.329464. Value loss: 0.065256. Entropy: 0.297191.\n",
      "now time :  2019-09-06 15:11:06.097836\n",
      "episode: 8201   score: 470.0  epsilon: 1.0    steps: 864  evaluation reward: 503.15\n",
      "Training network. lr: 0.000065. clip: 0.025820\n",
      "Iteration 24187: Policy loss: 0.106416. Value loss: 0.232368. Entropy: 0.293195.\n",
      "Iteration 24188: Policy loss: 0.101399. Value loss: 0.085795. Entropy: 0.294660.\n",
      "Iteration 24189: Policy loss: 0.097517. Value loss: 0.058744. Entropy: 0.292309.\n",
      "episode: 8202   score: 600.0  epsilon: 1.0    steps: 848  evaluation reward: 506.0\n",
      "Training network. lr: 0.000065. clip: 0.025820\n",
      "Iteration 24190: Policy loss: 0.064242. Value loss: 0.105421. Entropy: 0.301091.\n",
      "Iteration 24191: Policy loss: 0.066905. Value loss: 0.045670. Entropy: 0.301622.\n",
      "Iteration 24192: Policy loss: 0.054836. Value loss: 0.031886. Entropy: 0.301068.\n",
      "Training network. lr: 0.000065. clip: 0.025820\n",
      "Iteration 24193: Policy loss: 0.062527. Value loss: 0.095702. Entropy: 0.307414.\n",
      "Iteration 24194: Policy loss: 0.061274. Value loss: 0.051659. Entropy: 0.305438.\n",
      "Iteration 24195: Policy loss: 0.055011. Value loss: 0.036400. Entropy: 0.306461.\n",
      "Training network. lr: 0.000065. clip: 0.025820\n",
      "Iteration 24196: Policy loss: -0.061727. Value loss: 0.422384. Entropy: 0.306735.\n",
      "Iteration 24197: Policy loss: -0.051393. Value loss: 0.170313. Entropy: 0.306873.\n",
      "Iteration 24198: Policy loss: -0.064597. Value loss: 0.094111. Entropy: 0.306805.\n",
      "episode: 8203   score: 360.0  epsilon: 1.0    steps: 128  evaluation reward: 505.65\n",
      "Training network. lr: 0.000065. clip: 0.025820\n",
      "Iteration 24199: Policy loss: -0.350334. Value loss: 0.225704. Entropy: 0.299850.\n",
      "Iteration 24200: Policy loss: -0.367534. Value loss: 0.077665. Entropy: 0.298597.\n",
      "Iteration 24201: Policy loss: -0.366945. Value loss: 0.057830. Entropy: 0.298212.\n",
      "episode: 8204   score: 900.0  epsilon: 1.0    steps: 416  evaluation reward: 510.45\n",
      "Training network. lr: 0.000064. clip: 0.025664\n",
      "Iteration 24202: Policy loss: 0.308986. Value loss: 0.196060. Entropy: 0.291729.\n",
      "Iteration 24203: Policy loss: 0.303012. Value loss: 0.072945. Entropy: 0.291056.\n",
      "Iteration 24204: Policy loss: 0.300872. Value loss: 0.043146. Entropy: 0.290600.\n",
      "Training network. lr: 0.000064. clip: 0.025664\n",
      "Iteration 24205: Policy loss: -0.065247. Value loss: 0.156177. Entropy: 0.310998.\n",
      "Iteration 24206: Policy loss: -0.071181. Value loss: 0.056884. Entropy: 0.310051.\n",
      "Iteration 24207: Policy loss: -0.067681. Value loss: 0.040799. Entropy: 0.311449.\n",
      "episode: 8205   score: 755.0  epsilon: 1.0    steps: 120  evaluation reward: 510.3\n",
      "episode: 8206   score: 470.0  epsilon: 1.0    steps: 416  evaluation reward: 511.6\n",
      "episode: 8207   score: 600.0  epsilon: 1.0    steps: 488  evaluation reward: 512.2\n",
      "Training network. lr: 0.000064. clip: 0.025664\n",
      "Iteration 24208: Policy loss: 0.110809. Value loss: 0.141714. Entropy: 0.274422.\n",
      "Iteration 24209: Policy loss: 0.108210. Value loss: 0.079961. Entropy: 0.273533.\n",
      "Iteration 24210: Policy loss: 0.108030. Value loss: 0.056031. Entropy: 0.273999.\n",
      "episode: 8208   score: 395.0  epsilon: 1.0    steps: 536  evaluation reward: 510.8\n",
      "Training network. lr: 0.000064. clip: 0.025664\n",
      "Iteration 24211: Policy loss: -0.235156. Value loss: 0.216306. Entropy: 0.295065.\n",
      "Iteration 24212: Policy loss: -0.223967. Value loss: 0.092100. Entropy: 0.294505.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24213: Policy loss: -0.247719. Value loss: 0.045618. Entropy: 0.294308.\n",
      "episode: 8209   score: 210.0  epsilon: 1.0    steps: 200  evaluation reward: 505.65\n",
      "episode: 8210   score: 430.0  epsilon: 1.0    steps: 896  evaluation reward: 505.75\n",
      "Training network. lr: 0.000064. clip: 0.025664\n",
      "Iteration 24214: Policy loss: -0.221518. Value loss: 0.074599. Entropy: 0.286899.\n",
      "Iteration 24215: Policy loss: -0.231684. Value loss: 0.038563. Entropy: 0.288355.\n",
      "Iteration 24216: Policy loss: -0.233558. Value loss: 0.030276. Entropy: 0.288031.\n",
      "Training network. lr: 0.000064. clip: 0.025664\n",
      "Iteration 24217: Policy loss: -0.242535. Value loss: 0.547059. Entropy: 0.306943.\n",
      "Iteration 24218: Policy loss: -0.270838. Value loss: 0.245465. Entropy: 0.306440.\n",
      "Iteration 24219: Policy loss: -0.282456. Value loss: 0.119870. Entropy: 0.305052.\n",
      "episode: 8211   score: 625.0  epsilon: 1.0    steps: 744  evaluation reward: 507.85\n",
      "Training network. lr: 0.000064. clip: 0.025664\n",
      "Iteration 24220: Policy loss: -0.042156. Value loss: 0.106010. Entropy: 0.298500.\n",
      "Iteration 24221: Policy loss: -0.056083. Value loss: 0.057691. Entropy: 0.298572.\n",
      "Iteration 24222: Policy loss: -0.054206. Value loss: 0.045948. Entropy: 0.299713.\n",
      "Training network. lr: 0.000064. clip: 0.025664\n",
      "Iteration 24223: Policy loss: -0.089146. Value loss: 0.328602. Entropy: 0.312121.\n",
      "Iteration 24224: Policy loss: -0.097296. Value loss: 0.230195. Entropy: 0.311847.\n",
      "Iteration 24225: Policy loss: -0.097014. Value loss: 0.168615. Entropy: 0.311491.\n",
      "Training network. lr: 0.000064. clip: 0.025664\n",
      "Iteration 24226: Policy loss: -0.208282. Value loss: 0.142830. Entropy: 0.305508.\n",
      "Iteration 24227: Policy loss: -0.208516. Value loss: 0.068101. Entropy: 0.305136.\n",
      "Iteration 24228: Policy loss: -0.210257. Value loss: 0.053423. Entropy: 0.304775.\n",
      "episode: 8212   score: 620.0  epsilon: 1.0    steps: 928  evaluation reward: 508.25\n",
      "Training network. lr: 0.000064. clip: 0.025664\n",
      "Iteration 24229: Policy loss: 0.105319. Value loss: 0.202106. Entropy: 0.295532.\n",
      "Iteration 24230: Policy loss: 0.109905. Value loss: 0.070585. Entropy: 0.294439.\n",
      "Iteration 24231: Policy loss: 0.103175. Value loss: 0.044346. Entropy: 0.294422.\n",
      "episode: 8213   score: 620.0  epsilon: 1.0    steps: 344  evaluation reward: 508.25\n",
      "Training network. lr: 0.000064. clip: 0.025664\n",
      "Iteration 24232: Policy loss: 0.119372. Value loss: 0.118981. Entropy: 0.289078.\n",
      "Iteration 24233: Policy loss: 0.116523. Value loss: 0.042570. Entropy: 0.288707.\n",
      "Iteration 24234: Policy loss: 0.110799. Value loss: 0.031636. Entropy: 0.288767.\n",
      "episode: 8214   score: 420.0  epsilon: 1.0    steps: 232  evaluation reward: 506.25\n",
      "Training network. lr: 0.000064. clip: 0.025664\n",
      "Iteration 24235: Policy loss: 0.022788. Value loss: 0.145198. Entropy: 0.295840.\n",
      "Iteration 24236: Policy loss: 0.021860. Value loss: 0.070789. Entropy: 0.294913.\n",
      "Iteration 24237: Policy loss: 0.023337. Value loss: 0.050163. Entropy: 0.296248.\n",
      "episode: 8215   score: 365.0  epsilon: 1.0    steps: 272  evaluation reward: 500.15\n",
      "Training network. lr: 0.000064. clip: 0.025664\n",
      "Iteration 24238: Policy loss: 0.216828. Value loss: 0.130985. Entropy: 0.296315.\n",
      "Iteration 24239: Policy loss: 0.217535. Value loss: 0.059761. Entropy: 0.296074.\n",
      "Iteration 24240: Policy loss: 0.210342. Value loss: 0.042029. Entropy: 0.296603.\n",
      "episode: 8216   score: 870.0  epsilon: 1.0    steps: 264  evaluation reward: 504.15\n",
      "episode: 8217   score: 295.0  epsilon: 1.0    steps: 360  evaluation reward: 500.7\n",
      "Training network. lr: 0.000064. clip: 0.025664\n",
      "Iteration 24241: Policy loss: 0.222682. Value loss: 0.134397. Entropy: 0.279960.\n",
      "Iteration 24242: Policy loss: 0.213191. Value loss: 0.047042. Entropy: 0.281309.\n",
      "Iteration 24243: Policy loss: 0.222051. Value loss: 0.035979. Entropy: 0.279447.\n",
      "Training network. lr: 0.000064. clip: 0.025664\n",
      "Iteration 24244: Policy loss: -0.357676. Value loss: 0.386786. Entropy: 0.303484.\n",
      "Iteration 24245: Policy loss: -0.367135. Value loss: 0.178818. Entropy: 0.305510.\n",
      "Iteration 24246: Policy loss: -0.348058. Value loss: 0.093537. Entropy: 0.305494.\n",
      "Training network. lr: 0.000064. clip: 0.025664\n",
      "Iteration 24247: Policy loss: -0.642127. Value loss: 0.379118. Entropy: 0.309732.\n",
      "Iteration 24248: Policy loss: -0.657908. Value loss: 0.257783. Entropy: 0.310562.\n",
      "Iteration 24249: Policy loss: -0.634890. Value loss: 0.191918. Entropy: 0.310057.\n",
      "episode: 8218   score: 555.0  epsilon: 1.0    steps: 240  evaluation reward: 498.7\n",
      "Training network. lr: 0.000064. clip: 0.025664\n",
      "Iteration 24250: Policy loss: 0.253431. Value loss: 0.179110. Entropy: 0.295870.\n",
      "Iteration 24251: Policy loss: 0.253911. Value loss: 0.077767. Entropy: 0.294924.\n",
      "Iteration 24252: Policy loss: 0.240474. Value loss: 0.052173. Entropy: 0.294880.\n",
      "episode: 8219   score: 580.0  epsilon: 1.0    steps: 944  evaluation reward: 501.55\n",
      "Training network. lr: 0.000064. clip: 0.025507\n",
      "Iteration 24253: Policy loss: 0.179412. Value loss: 0.119101. Entropy: 0.306666.\n",
      "Iteration 24254: Policy loss: 0.173493. Value loss: 0.051158. Entropy: 0.305941.\n",
      "Iteration 24255: Policy loss: 0.173389. Value loss: 0.038922. Entropy: 0.305368.\n",
      "episode: 8220   score: 985.0  epsilon: 1.0    steps: 280  evaluation reward: 507.75\n",
      "Training network. lr: 0.000064. clip: 0.025507\n",
      "Iteration 24256: Policy loss: 0.203782. Value loss: 0.131974. Entropy: 0.290580.\n",
      "Iteration 24257: Policy loss: 0.194768. Value loss: 0.067908. Entropy: 0.290662.\n",
      "Iteration 24258: Policy loss: 0.197194. Value loss: 0.050699. Entropy: 0.289574.\n",
      "episode: 8221   score: 610.0  epsilon: 1.0    steps: 336  evaluation reward: 509.6\n",
      "Training network. lr: 0.000064. clip: 0.025507\n",
      "Iteration 24259: Policy loss: 0.077825. Value loss: 0.127027. Entropy: 0.295209.\n",
      "Iteration 24260: Policy loss: 0.066103. Value loss: 0.064813. Entropy: 0.294154.\n",
      "Iteration 24261: Policy loss: 0.075057. Value loss: 0.044190. Entropy: 0.294783.\n",
      "episode: 8222   score: 580.0  epsilon: 1.0    steps: 64  evaluation reward: 511.95\n",
      "episode: 8223   score: 525.0  epsilon: 1.0    steps: 328  evaluation reward: 510.75\n",
      "Training network. lr: 0.000064. clip: 0.025507\n",
      "Iteration 24262: Policy loss: -0.118861. Value loss: 0.138703. Entropy: 0.290048.\n",
      "Iteration 24263: Policy loss: -0.124786. Value loss: 0.054669. Entropy: 0.289160.\n",
      "Iteration 24264: Policy loss: -0.124197. Value loss: 0.038691. Entropy: 0.289294.\n",
      "episode: 8224   score: 700.0  epsilon: 1.0    steps: 712  evaluation reward: 515.95\n",
      "Training network. lr: 0.000064. clip: 0.025507\n",
      "Iteration 24265: Policy loss: -0.096923. Value loss: 0.173004. Entropy: 0.293058.\n",
      "Iteration 24266: Policy loss: -0.111147. Value loss: 0.083780. Entropy: 0.291790.\n",
      "Iteration 24267: Policy loss: -0.098719. Value loss: 0.063837. Entropy: 0.292966.\n",
      "Training network. lr: 0.000064. clip: 0.025507\n",
      "Iteration 24268: Policy loss: -0.065014. Value loss: 0.131638. Entropy: 0.311002.\n",
      "Iteration 24269: Policy loss: -0.066666. Value loss: 0.058569. Entropy: 0.310284.\n",
      "Iteration 24270: Policy loss: -0.073305. Value loss: 0.037843. Entropy: 0.310108.\n",
      "Training network. lr: 0.000064. clip: 0.025507\n",
      "Iteration 24271: Policy loss: -0.097376. Value loss: 0.187972. Entropy: 0.305770.\n",
      "Iteration 24272: Policy loss: -0.105322. Value loss: 0.072844. Entropy: 0.305720.\n",
      "Iteration 24273: Policy loss: -0.118695. Value loss: 0.057035. Entropy: 0.306677.\n",
      "episode: 8225   score: 480.0  epsilon: 1.0    steps: 80  evaluation reward: 517.6\n",
      "Training network. lr: 0.000064. clip: 0.025507\n",
      "Iteration 24274: Policy loss: -0.588418. Value loss: 0.508568. Entropy: 0.298458.\n",
      "Iteration 24275: Policy loss: -0.595611. Value loss: 0.228913. Entropy: 0.297828.\n",
      "Iteration 24276: Policy loss: -0.619113. Value loss: 0.146032. Entropy: 0.298547.\n",
      "episode: 8226   score: 330.0  epsilon: 1.0    steps: 56  evaluation reward: 516.85\n",
      "episode: 8227   score: 670.0  epsilon: 1.0    steps: 328  evaluation reward: 516.6\n",
      "Training network. lr: 0.000064. clip: 0.025507\n",
      "Iteration 24277: Policy loss: 0.413667. Value loss: 0.309724. Entropy: 0.278822.\n",
      "Iteration 24278: Policy loss: 0.421542. Value loss: 0.128962. Entropy: 0.278013.\n",
      "Iteration 24279: Policy loss: 0.421473. Value loss: 0.080488. Entropy: 0.280426.\n",
      "episode: 8228   score: 440.0  epsilon: 1.0    steps: 328  evaluation reward: 519.15\n",
      "Training network. lr: 0.000064. clip: 0.025507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24280: Policy loss: -0.020323. Value loss: 0.159462. Entropy: 0.297807.\n",
      "Iteration 24281: Policy loss: -0.012968. Value loss: 0.063811. Entropy: 0.297190.\n",
      "Iteration 24282: Policy loss: -0.020144. Value loss: 0.043409. Entropy: 0.296868.\n",
      "episode: 8229   score: 440.0  epsilon: 1.0    steps: 472  evaluation reward: 517.3\n",
      "Training network. lr: 0.000064. clip: 0.025507\n",
      "Iteration 24283: Policy loss: 0.138452. Value loss: 0.140815. Entropy: 0.297825.\n",
      "Iteration 24284: Policy loss: 0.142677. Value loss: 0.068288. Entropy: 0.296735.\n",
      "Iteration 24285: Policy loss: 0.145489. Value loss: 0.046432. Entropy: 0.296569.\n",
      "episode: 8230   score: 725.0  epsilon: 1.0    steps: 760  evaluation reward: 519.6\n",
      "Training network. lr: 0.000064. clip: 0.025507\n",
      "Iteration 24286: Policy loss: 0.429371. Value loss: 0.187955. Entropy: 0.303745.\n",
      "Iteration 24287: Policy loss: 0.419106. Value loss: 0.050425. Entropy: 0.302069.\n",
      "Iteration 24288: Policy loss: 0.423597. Value loss: 0.034413. Entropy: 0.302686.\n",
      "episode: 8231   score: 360.0  epsilon: 1.0    steps: 384  evaluation reward: 517.8\n",
      "Training network. lr: 0.000064. clip: 0.025507\n",
      "Iteration 24289: Policy loss: 0.196742. Value loss: 0.155007. Entropy: 0.295419.\n",
      "Iteration 24290: Policy loss: 0.190809. Value loss: 0.052908. Entropy: 0.295398.\n",
      "Iteration 24291: Policy loss: 0.194056. Value loss: 0.036109. Entropy: 0.295626.\n",
      "Training network. lr: 0.000064. clip: 0.025507\n",
      "Iteration 24292: Policy loss: -0.116324. Value loss: 0.187371. Entropy: 0.304552.\n",
      "Iteration 24293: Policy loss: -0.117017. Value loss: 0.093191. Entropy: 0.305114.\n",
      "Iteration 24294: Policy loss: -0.116085. Value loss: 0.063660. Entropy: 0.304044.\n",
      "episode: 8232   score: 695.0  epsilon: 1.0    steps: 480  evaluation reward: 520.25\n",
      "Training network. lr: 0.000064. clip: 0.025507\n",
      "Iteration 24295: Policy loss: 0.284547. Value loss: 0.143861. Entropy: 0.298236.\n",
      "Iteration 24296: Policy loss: 0.277172. Value loss: 0.048410. Entropy: 0.296942.\n",
      "Iteration 24297: Policy loss: 0.282853. Value loss: 0.035297. Entropy: 0.297877.\n",
      "episode: 8233   score: 390.0  epsilon: 1.0    steps: 72  evaluation reward: 518.6\n",
      "Training network. lr: 0.000064. clip: 0.025507\n",
      "Iteration 24298: Policy loss: -0.154164. Value loss: 0.259335. Entropy: 0.299896.\n",
      "Iteration 24299: Policy loss: -0.141626. Value loss: 0.169297. Entropy: 0.300636.\n",
      "Iteration 24300: Policy loss: -0.153174. Value loss: 0.142300. Entropy: 0.299830.\n",
      "episode: 8234   score: 395.0  epsilon: 1.0    steps: 56  evaluation reward: 518.3\n",
      "episode: 8235   score: 465.0  epsilon: 1.0    steps: 760  evaluation reward: 516.9\n",
      "Training network. lr: 0.000063. clip: 0.025360\n",
      "Iteration 24301: Policy loss: -0.240284. Value loss: 0.113543. Entropy: 0.286765.\n",
      "Iteration 24302: Policy loss: -0.250217. Value loss: 0.049280. Entropy: 0.286723.\n",
      "Iteration 24303: Policy loss: -0.243638. Value loss: 0.034804. Entropy: 0.287019.\n",
      "episode: 8236   score: 470.0  epsilon: 1.0    steps: 664  evaluation reward: 515.2\n",
      "Training network. lr: 0.000063. clip: 0.025360\n",
      "Iteration 24304: Policy loss: 0.297099. Value loss: 0.179600. Entropy: 0.298638.\n",
      "Iteration 24305: Policy loss: 0.300564. Value loss: 0.056419. Entropy: 0.298153.\n",
      "Iteration 24306: Policy loss: 0.287949. Value loss: 0.035446. Entropy: 0.297279.\n",
      "episode: 8237   score: 605.0  epsilon: 1.0    steps: 536  evaluation reward: 517.3\n",
      "Training network. lr: 0.000063. clip: 0.025360\n",
      "Iteration 24307: Policy loss: 0.184912. Value loss: 0.150823. Entropy: 0.289662.\n",
      "Iteration 24308: Policy loss: 0.186506. Value loss: 0.067107. Entropy: 0.291141.\n",
      "Iteration 24309: Policy loss: 0.174473. Value loss: 0.047956. Entropy: 0.291130.\n",
      "episode: 8238   score: 335.0  epsilon: 1.0    steps: 264  evaluation reward: 515.55\n",
      "Training network. lr: 0.000063. clip: 0.025360\n",
      "Iteration 24310: Policy loss: 0.288805. Value loss: 0.138444. Entropy: 0.277724.\n",
      "Iteration 24311: Policy loss: 0.288391. Value loss: 0.059499. Entropy: 0.277016.\n",
      "Iteration 24312: Policy loss: 0.284469. Value loss: 0.042584. Entropy: 0.277521.\n",
      "Training network. lr: 0.000063. clip: 0.025360\n",
      "Iteration 24313: Policy loss: -0.016211. Value loss: 0.228855. Entropy: 0.309471.\n",
      "Iteration 24314: Policy loss: -0.022779. Value loss: 0.091626. Entropy: 0.308466.\n",
      "Iteration 24315: Policy loss: -0.013297. Value loss: 0.064576. Entropy: 0.308889.\n",
      "Training network. lr: 0.000063. clip: 0.025360\n",
      "Iteration 24316: Policy loss: 0.037360. Value loss: 0.138141. Entropy: 0.311066.\n",
      "Iteration 24317: Policy loss: 0.032781. Value loss: 0.083092. Entropy: 0.310694.\n",
      "Iteration 24318: Policy loss: 0.031006. Value loss: 0.058403. Entropy: 0.311324.\n",
      "episode: 8239   score: 345.0  epsilon: 1.0    steps: 184  evaluation reward: 509.35\n",
      "episode: 8240   score: 500.0  epsilon: 1.0    steps: 672  evaluation reward: 507.4\n",
      "Training network. lr: 0.000063. clip: 0.025360\n",
      "Iteration 24319: Policy loss: -0.004991. Value loss: 0.178691. Entropy: 0.292734.\n",
      "Iteration 24320: Policy loss: -0.012671. Value loss: 0.093055. Entropy: 0.291950.\n",
      "Iteration 24321: Policy loss: -0.009890. Value loss: 0.062061. Entropy: 0.292000.\n",
      "episode: 8241   score: 415.0  epsilon: 1.0    steps: 936  evaluation reward: 508.3\n",
      "Training network. lr: 0.000063. clip: 0.025360\n",
      "Iteration 24322: Policy loss: 0.169067. Value loss: 0.098186. Entropy: 0.308912.\n",
      "Iteration 24323: Policy loss: 0.167585. Value loss: 0.050997. Entropy: 0.309469.\n",
      "Iteration 24324: Policy loss: 0.166204. Value loss: 0.034255. Entropy: 0.309070.\n",
      "episode: 8242   score: 215.0  epsilon: 1.0    steps: 408  evaluation reward: 503.4\n",
      "Training network. lr: 0.000063. clip: 0.025360\n",
      "Iteration 24325: Policy loss: 0.076670. Value loss: 0.096441. Entropy: 0.291200.\n",
      "Iteration 24326: Policy loss: 0.070222. Value loss: 0.050525. Entropy: 0.290850.\n",
      "Iteration 24327: Policy loss: 0.081339. Value loss: 0.036379. Entropy: 0.290888.\n",
      "episode: 8243   score: 640.0  epsilon: 1.0    steps: 672  evaluation reward: 507.2\n",
      "episode: 8244   score: 360.0  epsilon: 1.0    steps: 864  evaluation reward: 507.3\n",
      "Training network. lr: 0.000063. clip: 0.025360\n",
      "Iteration 24328: Policy loss: -0.007541. Value loss: 0.188577. Entropy: 0.287422.\n",
      "Iteration 24329: Policy loss: -0.016161. Value loss: 0.104926. Entropy: 0.286992.\n",
      "Iteration 24330: Policy loss: -0.015440. Value loss: 0.076577. Entropy: 0.286367.\n",
      "Training network. lr: 0.000063. clip: 0.025360\n",
      "Iteration 24331: Policy loss: 0.157580. Value loss: 0.120830. Entropy: 0.305592.\n",
      "Iteration 24332: Policy loss: 0.152483. Value loss: 0.052019. Entropy: 0.305718.\n",
      "Iteration 24333: Policy loss: 0.154349. Value loss: 0.037901. Entropy: 0.304782.\n",
      "episode: 8245   score: 670.0  epsilon: 1.0    steps: 472  evaluation reward: 508.3\n",
      "episode: 8246   score: 450.0  epsilon: 1.0    steps: 648  evaluation reward: 511.0\n",
      "Training network. lr: 0.000063. clip: 0.025360\n",
      "Iteration 24334: Policy loss: -0.257810. Value loss: 0.301822. Entropy: 0.287230.\n",
      "Iteration 24335: Policy loss: -0.266076. Value loss: 0.209513. Entropy: 0.284549.\n",
      "Iteration 24336: Policy loss: -0.257628. Value loss: 0.152185. Entropy: 0.284979.\n",
      "Training network. lr: 0.000063. clip: 0.025360\n",
      "Iteration 24337: Policy loss: 0.250571. Value loss: 0.180212. Entropy: 0.306033.\n",
      "Iteration 24338: Policy loss: 0.246268. Value loss: 0.075189. Entropy: 0.305834.\n",
      "Iteration 24339: Policy loss: 0.230913. Value loss: 0.046822. Entropy: 0.306163.\n",
      "episode: 8247   score: 320.0  epsilon: 1.0    steps: 144  evaluation reward: 510.55\n",
      "episode: 8248   score: 210.0  epsilon: 1.0    steps: 552  evaluation reward: 509.4\n",
      "Training network. lr: 0.000063. clip: 0.025360\n",
      "Iteration 24340: Policy loss: -0.034841. Value loss: 0.306033. Entropy: 0.282306.\n",
      "Iteration 24341: Policy loss: -0.061572. Value loss: 0.213117. Entropy: 0.279993.\n",
      "Iteration 24342: Policy loss: -0.061637. Value loss: 0.160614. Entropy: 0.281000.\n",
      "Training network. lr: 0.000063. clip: 0.025360\n",
      "Iteration 24343: Policy loss: 0.170214. Value loss: 0.137910. Entropy: 0.311511.\n",
      "Iteration 24344: Policy loss: 0.172539. Value loss: 0.065503. Entropy: 0.311554.\n",
      "Iteration 24345: Policy loss: 0.167062. Value loss: 0.045228. Entropy: 0.311876.\n",
      "Training network. lr: 0.000063. clip: 0.025360\n",
      "Iteration 24346: Policy loss: 0.023968. Value loss: 0.101989. Entropy: 0.308189.\n",
      "Iteration 24347: Policy loss: 0.016681. Value loss: 0.052922. Entropy: 0.308496.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24348: Policy loss: 0.021055. Value loss: 0.036871. Entropy: 0.308211.\n",
      "episode: 8249   score: 640.0  epsilon: 1.0    steps: 512  evaluation reward: 509.6\n",
      "Training network. lr: 0.000063. clip: 0.025360\n",
      "Iteration 24349: Policy loss: -0.120154. Value loss: 0.297896. Entropy: 0.295800.\n",
      "Iteration 24350: Policy loss: -0.131526. Value loss: 0.225232. Entropy: 0.295728.\n",
      "Iteration 24351: Policy loss: -0.122774. Value loss: 0.196740. Entropy: 0.294416.\n",
      "episode: 8250   score: 420.0  epsilon: 1.0    steps: 184  evaluation reward: 507.1\n",
      "now time :  2019-09-06 15:21:22.512782\n",
      "episode: 8251   score: 440.0  epsilon: 1.0    steps: 760  evaluation reward: 508.0\n",
      "episode: 8252   score: 420.0  epsilon: 1.0    steps: 888  evaluation reward: 506.45\n",
      "Training network. lr: 0.000063. clip: 0.025203\n",
      "Iteration 24352: Policy loss: 0.153101. Value loss: 0.114723. Entropy: 0.286973.\n",
      "Iteration 24353: Policy loss: 0.148463. Value loss: 0.055324. Entropy: 0.286498.\n",
      "Iteration 24354: Policy loss: 0.143427. Value loss: 0.043006. Entropy: 0.285263.\n",
      "Training network. lr: 0.000063. clip: 0.025203\n",
      "Iteration 24355: Policy loss: -0.349780. Value loss: 0.256787. Entropy: 0.305615.\n",
      "Iteration 24356: Policy loss: -0.364731. Value loss: 0.148600. Entropy: 0.305104.\n",
      "Iteration 24357: Policy loss: -0.361110. Value loss: 0.105541. Entropy: 0.304844.\n",
      "episode: 8253   score: 485.0  epsilon: 1.0    steps: 608  evaluation reward: 506.3\n",
      "Training network. lr: 0.000063. clip: 0.025203\n",
      "Iteration 24358: Policy loss: 0.124185. Value loss: 0.194283. Entropy: 0.296359.\n",
      "Iteration 24359: Policy loss: 0.116751. Value loss: 0.063079. Entropy: 0.295418.\n",
      "Iteration 24360: Policy loss: 0.109869. Value loss: 0.040788. Entropy: 0.295175.\n",
      "Training network. lr: 0.000063. clip: 0.025203\n",
      "Iteration 24361: Policy loss: -0.290430. Value loss: 0.316152. Entropy: 0.304101.\n",
      "Iteration 24362: Policy loss: -0.299637. Value loss: 0.158918. Entropy: 0.303388.\n",
      "Iteration 24363: Policy loss: -0.304771. Value loss: 0.091840. Entropy: 0.303582.\n",
      "episode: 8254   score: 725.0  epsilon: 1.0    steps: 976  evaluation reward: 508.9\n",
      "Training network. lr: 0.000063. clip: 0.025203\n",
      "Iteration 24364: Policy loss: -0.073796. Value loss: 0.262353. Entropy: 0.293763.\n",
      "Iteration 24365: Policy loss: -0.078230. Value loss: 0.082069. Entropy: 0.292365.\n",
      "Iteration 24366: Policy loss: -0.082085. Value loss: 0.052779. Entropy: 0.291919.\n",
      "Training network. lr: 0.000063. clip: 0.025203\n",
      "Iteration 24367: Policy loss: 0.091368. Value loss: 0.335487. Entropy: 0.292102.\n",
      "Iteration 24368: Policy loss: 0.072676. Value loss: 0.123426. Entropy: 0.291571.\n",
      "Iteration 24369: Policy loss: 0.075086. Value loss: 0.075483. Entropy: 0.289985.\n",
      "episode: 8255   score: 325.0  epsilon: 1.0    steps: 616  evaluation reward: 508.85\n",
      "Training network. lr: 0.000063. clip: 0.025203\n",
      "Iteration 24370: Policy loss: 0.201971. Value loss: 0.121887. Entropy: 0.294090.\n",
      "Iteration 24371: Policy loss: 0.195626. Value loss: 0.073080. Entropy: 0.293889.\n",
      "Iteration 24372: Policy loss: 0.185566. Value loss: 0.054229. Entropy: 0.295190.\n",
      "episode: 8256   score: 670.0  epsilon: 1.0    steps: 840  evaluation reward: 509.8\n",
      "episode: 8257   score: 695.0  epsilon: 1.0    steps: 952  evaluation reward: 511.95\n",
      "Training network. lr: 0.000063. clip: 0.025203\n",
      "Iteration 24373: Policy loss: 0.352311. Value loss: 0.205130. Entropy: 0.302965.\n",
      "Iteration 24374: Policy loss: 0.356090. Value loss: 0.080398. Entropy: 0.301935.\n",
      "Iteration 24375: Policy loss: 0.355994. Value loss: 0.049514. Entropy: 0.303722.\n",
      "episode: 8258   score: 540.0  epsilon: 1.0    steps: 160  evaluation reward: 512.1\n",
      "episode: 8259   score: 545.0  epsilon: 1.0    steps: 272  evaluation reward: 508.2\n",
      "Training network. lr: 0.000063. clip: 0.025203\n",
      "Iteration 24376: Policy loss: -0.043714. Value loss: 0.057076. Entropy: 0.267928.\n",
      "Iteration 24377: Policy loss: -0.045544. Value loss: 0.032248. Entropy: 0.267896.\n",
      "Iteration 24378: Policy loss: -0.044868. Value loss: 0.024834. Entropy: 0.268285.\n",
      "Training network. lr: 0.000063. clip: 0.025203\n",
      "Iteration 24379: Policy loss: 0.165745. Value loss: 0.193741. Entropy: 0.308884.\n",
      "Iteration 24380: Policy loss: 0.164330. Value loss: 0.104721. Entropy: 0.307761.\n",
      "Iteration 24381: Policy loss: 0.156713. Value loss: 0.073333. Entropy: 0.308462.\n",
      "Training network. lr: 0.000063. clip: 0.025203\n",
      "Iteration 24382: Policy loss: 0.124857. Value loss: 0.153789. Entropy: 0.306593.\n",
      "Iteration 24383: Policy loss: 0.119447. Value loss: 0.072374. Entropy: 0.307626.\n",
      "Iteration 24384: Policy loss: 0.109893. Value loss: 0.053047. Entropy: 0.305952.\n",
      "Training network. lr: 0.000063. clip: 0.025203\n",
      "Iteration 24385: Policy loss: 0.297912. Value loss: 0.161033. Entropy: 0.310136.\n",
      "Iteration 24386: Policy loss: 0.288348. Value loss: 0.053309. Entropy: 0.309646.\n",
      "Iteration 24387: Policy loss: 0.289098. Value loss: 0.037483. Entropy: 0.311081.\n",
      "episode: 8260   score: 495.0  epsilon: 1.0    steps: 144  evaluation reward: 507.2\n",
      "episode: 8261   score: 240.0  epsilon: 1.0    steps: 584  evaluation reward: 504.75\n",
      "Training network. lr: 0.000063. clip: 0.025203\n",
      "Iteration 24388: Policy loss: -0.151130. Value loss: 0.101009. Entropy: 0.279564.\n",
      "Iteration 24389: Policy loss: -0.152833. Value loss: 0.040100. Entropy: 0.279177.\n",
      "Iteration 24390: Policy loss: -0.154832. Value loss: 0.028753. Entropy: 0.281105.\n",
      "Training network. lr: 0.000063. clip: 0.025203\n",
      "Iteration 24391: Policy loss: -0.225127. Value loss: 0.237044. Entropy: 0.307683.\n",
      "Iteration 24392: Policy loss: -0.231557. Value loss: 0.085653. Entropy: 0.308146.\n",
      "Iteration 24393: Policy loss: -0.238233. Value loss: 0.056983. Entropy: 0.308001.\n",
      "episode: 8262   score: 505.0  epsilon: 1.0    steps: 392  evaluation reward: 506.5\n",
      "episode: 8263   score: 295.0  epsilon: 1.0    steps: 800  evaluation reward: 506.85\n",
      "episode: 8264   score: 385.0  epsilon: 1.0    steps: 864  evaluation reward: 503.4\n",
      "Training network. lr: 0.000063. clip: 0.025203\n",
      "Iteration 24394: Policy loss: -0.039736. Value loss: 0.161287. Entropy: 0.285290.\n",
      "Iteration 24395: Policy loss: -0.039807. Value loss: 0.093620. Entropy: 0.286036.\n",
      "Iteration 24396: Policy loss: -0.035062. Value loss: 0.070036. Entropy: 0.286132.\n",
      "episode: 8265   score: 435.0  epsilon: 1.0    steps: 984  evaluation reward: 505.5\n",
      "Training network. lr: 0.000063. clip: 0.025203\n",
      "Iteration 24397: Policy loss: 0.090444. Value loss: 0.080792. Entropy: 0.300022.\n",
      "Iteration 24398: Policy loss: 0.089584. Value loss: 0.038687. Entropy: 0.300210.\n",
      "Iteration 24399: Policy loss: 0.085014. Value loss: 0.029680. Entropy: 0.301084.\n",
      "episode: 8266   score: 420.0  epsilon: 1.0    steps: 496  evaluation reward: 505.5\n",
      "Training network. lr: 0.000063. clip: 0.025203\n",
      "Iteration 24400: Policy loss: 0.001288. Value loss: 0.071209. Entropy: 0.285399.\n",
      "Iteration 24401: Policy loss: 0.004788. Value loss: 0.035821. Entropy: 0.284977.\n",
      "Iteration 24402: Policy loss: -0.001438. Value loss: 0.027593. Entropy: 0.284926.\n",
      "Training network. lr: 0.000063. clip: 0.025046\n",
      "Iteration 24403: Policy loss: -0.101295. Value loss: 0.311499. Entropy: 0.311453.\n",
      "Iteration 24404: Policy loss: -0.098087. Value loss: 0.205823. Entropy: 0.312101.\n",
      "Iteration 24405: Policy loss: -0.110040. Value loss: 0.178650. Entropy: 0.311416.\n",
      "Training network. lr: 0.000063. clip: 0.025046\n",
      "Iteration 24406: Policy loss: -0.011246. Value loss: 0.108033. Entropy: 0.304187.\n",
      "Iteration 24407: Policy loss: -0.011018. Value loss: 0.053335. Entropy: 0.304700.\n",
      "Iteration 24408: Policy loss: -0.007054. Value loss: 0.035733. Entropy: 0.304555.\n",
      "Training network. lr: 0.000063. clip: 0.025046\n",
      "Iteration 24409: Policy loss: 0.375600. Value loss: 0.104366. Entropy: 0.305272.\n",
      "Iteration 24410: Policy loss: 0.371308. Value loss: 0.030057. Entropy: 0.306033.\n",
      "Iteration 24411: Policy loss: 0.373220. Value loss: 0.020690. Entropy: 0.305084.\n",
      "episode: 8267   score: 335.0  epsilon: 1.0    steps: 360  evaluation reward: 506.0\n",
      "episode: 8268   score: 560.0  epsilon: 1.0    steps: 400  evaluation reward: 502.2\n",
      "episode: 8269   score: 245.0  epsilon: 1.0    steps: 952  evaluation reward: 500.2\n",
      "Training network. lr: 0.000063. clip: 0.025046\n",
      "Iteration 24412: Policy loss: 0.096426. Value loss: 0.100596. Entropy: 0.278946.\n",
      "Iteration 24413: Policy loss: 0.092187. Value loss: 0.049640. Entropy: 0.279720.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24414: Policy loss: 0.090017. Value loss: 0.035248. Entropy: 0.279645.\n",
      "Training network. lr: 0.000063. clip: 0.025046\n",
      "Iteration 24415: Policy loss: 0.014776. Value loss: 0.128832. Entropy: 0.308530.\n",
      "Iteration 24416: Policy loss: 0.010595. Value loss: 0.057898. Entropy: 0.308714.\n",
      "Iteration 24417: Policy loss: 0.000551. Value loss: 0.040271. Entropy: 0.308631.\n",
      "episode: 8270   score: 390.0  epsilon: 1.0    steps: 1008  evaluation reward: 498.9\n",
      "Training network. lr: 0.000063. clip: 0.025046\n",
      "Iteration 24418: Policy loss: -0.090048. Value loss: 0.119803. Entropy: 0.311917.\n",
      "Iteration 24419: Policy loss: -0.093670. Value loss: 0.065531. Entropy: 0.311346.\n",
      "Iteration 24420: Policy loss: -0.096585. Value loss: 0.050746. Entropy: 0.311243.\n",
      "episode: 8271   score: 275.0  epsilon: 1.0    steps: 160  evaluation reward: 497.45\n",
      "Training network. lr: 0.000063. clip: 0.025046\n",
      "Iteration 24421: Policy loss: 0.146955. Value loss: 0.045665. Entropy: 0.283619.\n",
      "Iteration 24422: Policy loss: 0.143858. Value loss: 0.020803. Entropy: 0.282220.\n",
      "Iteration 24423: Policy loss: 0.145620. Value loss: 0.015443. Entropy: 0.282300.\n",
      "episode: 8272   score: 430.0  epsilon: 1.0    steps: 288  evaluation reward: 494.0\n",
      "Training network. lr: 0.000063. clip: 0.025046\n",
      "Iteration 24424: Policy loss: -0.355936. Value loss: 0.294886. Entropy: 0.292445.\n",
      "Iteration 24425: Policy loss: -0.356562. Value loss: 0.203578. Entropy: 0.291126.\n",
      "Iteration 24426: Policy loss: -0.372993. Value loss: 0.171885. Entropy: 0.291790.\n",
      "episode: 8273   score: 820.0  epsilon: 1.0    steps: 144  evaluation reward: 498.85\n",
      "Training network. lr: 0.000063. clip: 0.025046\n",
      "Iteration 24427: Policy loss: -0.490248. Value loss: 0.333561. Entropy: 0.295648.\n",
      "Iteration 24428: Policy loss: -0.503245. Value loss: 0.222449. Entropy: 0.295713.\n",
      "Iteration 24429: Policy loss: -0.471981. Value loss: 0.155898. Entropy: 0.295638.\n",
      "Training network. lr: 0.000063. clip: 0.025046\n",
      "Iteration 24430: Policy loss: 0.106393. Value loss: 0.167948. Entropy: 0.311013.\n",
      "Iteration 24431: Policy loss: 0.104282. Value loss: 0.077510. Entropy: 0.310716.\n",
      "Iteration 24432: Policy loss: 0.101888. Value loss: 0.055209. Entropy: 0.310097.\n",
      "episode: 8274   score: 320.0  epsilon: 1.0    steps: 64  evaluation reward: 495.7\n",
      "Training network. lr: 0.000063. clip: 0.025046\n",
      "Iteration 24433: Policy loss: 0.043881. Value loss: 0.102443. Entropy: 0.299198.\n",
      "Iteration 24434: Policy loss: 0.048105. Value loss: 0.036347. Entropy: 0.298844.\n",
      "Iteration 24435: Policy loss: 0.050394. Value loss: 0.026692. Entropy: 0.298909.\n",
      "Training network. lr: 0.000063. clip: 0.025046\n",
      "Iteration 24436: Policy loss: 0.035076. Value loss: 0.073056. Entropy: 0.310376.\n",
      "Iteration 24437: Policy loss: 0.032294. Value loss: 0.031286. Entropy: 0.310132.\n",
      "Iteration 24438: Policy loss: 0.031809. Value loss: 0.024855. Entropy: 0.309827.\n",
      "episode: 8275   score: 455.0  epsilon: 1.0    steps: 152  evaluation reward: 496.1\n",
      "Training network. lr: 0.000063. clip: 0.025046\n",
      "Iteration 24439: Policy loss: -0.115280. Value loss: 0.090591. Entropy: 0.287729.\n",
      "Iteration 24440: Policy loss: -0.116213. Value loss: 0.038259. Entropy: 0.286751.\n",
      "Iteration 24441: Policy loss: -0.116698. Value loss: 0.029402. Entropy: 0.287245.\n",
      "Training network. lr: 0.000063. clip: 0.025046\n",
      "Iteration 24442: Policy loss: 0.394803. Value loss: 0.146473. Entropy: 0.311281.\n",
      "Iteration 24443: Policy loss: 0.389482. Value loss: 0.055593. Entropy: 0.312183.\n",
      "Iteration 24444: Policy loss: 0.393220. Value loss: 0.034572. Entropy: 0.312082.\n",
      "episode: 8276   score: 360.0  epsilon: 1.0    steps: 512  evaluation reward: 491.25\n",
      "Training network. lr: 0.000063. clip: 0.025046\n",
      "Iteration 24445: Policy loss: -0.118162. Value loss: 0.137331. Entropy: 0.297312.\n",
      "Iteration 24446: Policy loss: -0.118309. Value loss: 0.069343. Entropy: 0.296362.\n",
      "Iteration 24447: Policy loss: -0.128300. Value loss: 0.049576. Entropy: 0.297841.\n",
      "Training network. lr: 0.000063. clip: 0.025046\n",
      "Iteration 24448: Policy loss: -0.581533. Value loss: 0.379078. Entropy: 0.306535.\n",
      "Iteration 24449: Policy loss: -0.579536. Value loss: 0.279365. Entropy: 0.306565.\n",
      "Iteration 24450: Policy loss: -0.590145. Value loss: 0.253150. Entropy: 0.305777.\n",
      "Training network. lr: 0.000062. clip: 0.024899\n",
      "Iteration 24451: Policy loss: -0.395052. Value loss: 0.423031. Entropy: 0.315996.\n",
      "Iteration 24452: Policy loss: -0.403185. Value loss: 0.266995. Entropy: 0.317037.\n",
      "Iteration 24453: Policy loss: -0.375525. Value loss: 0.173246. Entropy: 0.315569.\n",
      "episode: 8277   score: 600.0  epsilon: 1.0    steps: 288  evaluation reward: 491.05\n",
      "episode: 8278   score: 700.0  epsilon: 1.0    steps: 680  evaluation reward: 491.9\n",
      "episode: 8279   score: 575.0  epsilon: 1.0    steps: 824  evaluation reward: 490.0\n",
      "Training network. lr: 0.000062. clip: 0.024899\n",
      "Iteration 24454: Policy loss: 0.408095. Value loss: 0.246458. Entropy: 0.275143.\n",
      "Iteration 24455: Policy loss: 0.410315. Value loss: 0.093631. Entropy: 0.273947.\n",
      "Iteration 24456: Policy loss: 0.391318. Value loss: 0.054264. Entropy: 0.275861.\n",
      "Training network. lr: 0.000062. clip: 0.024899\n",
      "Iteration 24457: Policy loss: -0.033787. Value loss: 0.126475. Entropy: 0.305091.\n",
      "Iteration 24458: Policy loss: -0.044143. Value loss: 0.074576. Entropy: 0.303742.\n",
      "Iteration 24459: Policy loss: -0.043905. Value loss: 0.055793. Entropy: 0.303327.\n",
      "episode: 8280   score: 525.0  epsilon: 1.0    steps: 144  evaluation reward: 491.2\n",
      "episode: 8281   score: 395.0  epsilon: 1.0    steps: 976  evaluation reward: 490.5\n",
      "Training network. lr: 0.000062. clip: 0.024899\n",
      "Iteration 24460: Policy loss: 0.004908. Value loss: 0.115924. Entropy: 0.289653.\n",
      "Iteration 24461: Policy loss: -0.002461. Value loss: 0.058162. Entropy: 0.289181.\n",
      "Iteration 24462: Policy loss: -0.001166. Value loss: 0.046012. Entropy: 0.289080.\n",
      "episode: 8282   score: 610.0  epsilon: 1.0    steps: 192  evaluation reward: 492.5\n",
      "episode: 8283   score: 995.0  epsilon: 1.0    steps: 424  evaluation reward: 499.15\n",
      "Training network. lr: 0.000062. clip: 0.024899\n",
      "Iteration 24463: Policy loss: -0.021647. Value loss: 0.071559. Entropy: 0.268654.\n",
      "Iteration 24464: Policy loss: -0.018840. Value loss: 0.037001. Entropy: 0.269094.\n",
      "Iteration 24465: Policy loss: -0.016898. Value loss: 0.030776. Entropy: 0.267384.\n",
      "Training network. lr: 0.000062. clip: 0.024899\n",
      "Iteration 24466: Policy loss: -0.204933. Value loss: 0.291900. Entropy: 0.305990.\n",
      "Iteration 24467: Policy loss: -0.216547. Value loss: 0.207133. Entropy: 0.306082.\n",
      "Iteration 24468: Policy loss: -0.213221. Value loss: 0.112412. Entropy: 0.306568.\n",
      "episode: 8284   score: 450.0  epsilon: 1.0    steps: 320  evaluation reward: 498.3\n",
      "Training network. lr: 0.000062. clip: 0.024899\n",
      "Iteration 24469: Policy loss: -0.223491. Value loss: 0.273046. Entropy: 0.297305.\n",
      "Iteration 24470: Policy loss: -0.224532. Value loss: 0.197560. Entropy: 0.295451.\n",
      "Iteration 24471: Policy loss: -0.227966. Value loss: 0.158295. Entropy: 0.296242.\n",
      "Training network. lr: 0.000062. clip: 0.024899\n",
      "Iteration 24472: Policy loss: 0.499739. Value loss: 0.268234. Entropy: 0.308296.\n",
      "Iteration 24473: Policy loss: 0.499259. Value loss: 0.056913. Entropy: 0.309361.\n",
      "Iteration 24474: Policy loss: 0.498650. Value loss: 0.035076. Entropy: 0.306992.\n",
      "Training network. lr: 0.000062. clip: 0.024899\n",
      "Iteration 24475: Policy loss: -0.096496. Value loss: 0.124931. Entropy: 0.303300.\n",
      "Iteration 24476: Policy loss: -0.105844. Value loss: 0.066353. Entropy: 0.302900.\n",
      "Iteration 24477: Policy loss: -0.101532. Value loss: 0.049317. Entropy: 0.302666.\n",
      "episode: 8285   score: 380.0  epsilon: 1.0    steps: 320  evaluation reward: 499.2\n",
      "Training network. lr: 0.000062. clip: 0.024899\n",
      "Iteration 24478: Policy loss: 0.141359. Value loss: 0.180606. Entropy: 0.303496.\n",
      "Iteration 24479: Policy loss: 0.140900. Value loss: 0.080875. Entropy: 0.302837.\n",
      "Iteration 24480: Policy loss: 0.138009. Value loss: 0.051142. Entropy: 0.302395.\n",
      "Training network. lr: 0.000062. clip: 0.024899\n",
      "Iteration 24481: Policy loss: 0.319125. Value loss: 0.108230. Entropy: 0.301363.\n",
      "Iteration 24482: Policy loss: 0.312400. Value loss: 0.054129. Entropy: 0.301424.\n",
      "Iteration 24483: Policy loss: 0.309757. Value loss: 0.040778. Entropy: 0.301566.\n",
      "episode: 8286   score: 370.0  epsilon: 1.0    steps: 264  evaluation reward: 497.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8287   score: 580.0  epsilon: 1.0    steps: 376  evaluation reward: 496.85\n",
      "episode: 8288   score: 335.0  epsilon: 1.0    steps: 672  evaluation reward: 496.85\n",
      "episode: 8289   score: 400.0  epsilon: 1.0    steps: 896  evaluation reward: 498.0\n",
      "Training network. lr: 0.000062. clip: 0.024899\n",
      "Iteration 24484: Policy loss: -0.347422. Value loss: 0.351498. Entropy: 0.270661.\n",
      "Iteration 24485: Policy loss: -0.339318. Value loss: 0.154337. Entropy: 0.270828.\n",
      "Iteration 24486: Policy loss: -0.339086. Value loss: 0.112843. Entropy: 0.271011.\n",
      "episode: 8290   score: 720.0  epsilon: 1.0    steps: 936  evaluation reward: 498.8\n",
      "Training network. lr: 0.000062. clip: 0.024899\n",
      "Iteration 24487: Policy loss: 0.186204. Value loss: 0.154071. Entropy: 0.304478.\n",
      "Iteration 24488: Policy loss: 0.190796. Value loss: 0.072432. Entropy: 0.303174.\n",
      "Iteration 24489: Policy loss: 0.185023. Value loss: 0.048480. Entropy: 0.302861.\n",
      "Training network. lr: 0.000062. clip: 0.024899\n",
      "Iteration 24490: Policy loss: 0.030384. Value loss: 0.073822. Entropy: 0.296697.\n",
      "Iteration 24491: Policy loss: 0.029398. Value loss: 0.042578. Entropy: 0.297848.\n",
      "Iteration 24492: Policy loss: 0.030913. Value loss: 0.033741. Entropy: 0.297196.\n",
      "episode: 8291   score: 315.0  epsilon: 1.0    steps: 728  evaluation reward: 498.8\n",
      "Training network. lr: 0.000062. clip: 0.024899\n",
      "Iteration 24493: Policy loss: -0.101556. Value loss: 0.367827. Entropy: 0.299093.\n",
      "Iteration 24494: Policy loss: -0.098692. Value loss: 0.295966. Entropy: 0.298322.\n",
      "Iteration 24495: Policy loss: -0.102325. Value loss: 0.257425. Entropy: 0.300110.\n",
      "episode: 8292   score: 1015.0  epsilon: 1.0    steps: 504  evaluation reward: 505.05\n",
      "Training network. lr: 0.000062. clip: 0.024899\n",
      "Iteration 24496: Policy loss: -0.284412. Value loss: 0.333815. Entropy: 0.292742.\n",
      "Iteration 24497: Policy loss: -0.295318. Value loss: 0.243580. Entropy: 0.293404.\n",
      "Iteration 24498: Policy loss: -0.288462. Value loss: 0.172870. Entropy: 0.294221.\n",
      "episode: 8293   score: 385.0  epsilon: 1.0    steps: 984  evaluation reward: 504.05\n",
      "Training network. lr: 0.000062. clip: 0.024899\n",
      "Iteration 24499: Policy loss: 0.176189. Value loss: 0.148834. Entropy: 0.303825.\n",
      "Iteration 24500: Policy loss: 0.183333. Value loss: 0.078734. Entropy: 0.303361.\n",
      "Iteration 24501: Policy loss: 0.169691. Value loss: 0.058894. Entropy: 0.303384.\n",
      "Training network. lr: 0.000062. clip: 0.024742\n",
      "Iteration 24502: Policy loss: -0.328976. Value loss: 0.217169. Entropy: 0.299258.\n",
      "Iteration 24503: Policy loss: -0.333070. Value loss: 0.114928. Entropy: 0.299245.\n",
      "Iteration 24504: Policy loss: -0.341239. Value loss: 0.064586. Entropy: 0.299478.\n",
      "Training network. lr: 0.000062. clip: 0.024742\n",
      "Iteration 24505: Policy loss: 0.094695. Value loss: 0.161982. Entropy: 0.307080.\n",
      "Iteration 24506: Policy loss: 0.103204. Value loss: 0.072129. Entropy: 0.306443.\n",
      "Iteration 24507: Policy loss: 0.097392. Value loss: 0.050004. Entropy: 0.306039.\n",
      "episode: 8294   score: 350.0  epsilon: 1.0    steps: 24  evaluation reward: 501.85\n",
      "episode: 8295   score: 580.0  epsilon: 1.0    steps: 480  evaluation reward: 500.5\n",
      "Training network. lr: 0.000062. clip: 0.024742\n",
      "Iteration 24508: Policy loss: -0.207700. Value loss: 0.114699. Entropy: 0.278916.\n",
      "Iteration 24509: Policy loss: -0.207154. Value loss: 0.040375. Entropy: 0.277426.\n",
      "Iteration 24510: Policy loss: -0.216685. Value loss: 0.028464. Entropy: 0.278379.\n",
      "episode: 8296   score: 395.0  epsilon: 1.0    steps: 864  evaluation reward: 499.0\n",
      "Training network. lr: 0.000062. clip: 0.024742\n",
      "Iteration 24511: Policy loss: 0.187761. Value loss: 0.164342. Entropy: 0.298809.\n",
      "Iteration 24512: Policy loss: 0.186303. Value loss: 0.075416. Entropy: 0.298610.\n",
      "Iteration 24513: Policy loss: 0.173474. Value loss: 0.048772. Entropy: 0.298559.\n",
      "episode: 8297   score: 570.0  epsilon: 1.0    steps: 792  evaluation reward: 502.55\n",
      "Training network. lr: 0.000062. clip: 0.024742\n",
      "Iteration 24514: Policy loss: 0.178082. Value loss: 0.109516. Entropy: 0.296470.\n",
      "Iteration 24515: Policy loss: 0.175616. Value loss: 0.054263. Entropy: 0.295227.\n",
      "Iteration 24516: Policy loss: 0.174217. Value loss: 0.036897. Entropy: 0.296709.\n",
      "episode: 8298   score: 435.0  epsilon: 1.0    steps: 64  evaluation reward: 501.3\n",
      "Training network. lr: 0.000062. clip: 0.024742\n",
      "Iteration 24517: Policy loss: -0.506999. Value loss: 0.397376. Entropy: 0.296227.\n",
      "Iteration 24518: Policy loss: -0.500783. Value loss: 0.255934. Entropy: 0.296559.\n",
      "Iteration 24519: Policy loss: -0.497110. Value loss: 0.170447. Entropy: 0.294584.\n",
      "episode: 8299   score: 340.0  epsilon: 1.0    steps: 304  evaluation reward: 497.75\n",
      "episode: 8300   score: 745.0  epsilon: 1.0    steps: 608  evaluation reward: 501.0\n",
      "Training network. lr: 0.000062. clip: 0.024742\n",
      "Iteration 24520: Policy loss: 0.063132. Value loss: 0.057422. Entropy: 0.273989.\n",
      "Iteration 24521: Policy loss: 0.061618. Value loss: 0.033341. Entropy: 0.275248.\n",
      "Iteration 24522: Policy loss: 0.063327. Value loss: 0.025887. Entropy: 0.274045.\n",
      "now time :  2019-09-06 15:32:06.817323\n",
      "episode: 8301   score: 575.0  epsilon: 1.0    steps: 440  evaluation reward: 502.05\n",
      "Training network. lr: 0.000062. clip: 0.024742\n",
      "Iteration 24523: Policy loss: 0.040974. Value loss: 0.303564. Entropy: 0.297154.\n",
      "Iteration 24524: Policy loss: 0.044042. Value loss: 0.140820. Entropy: 0.298169.\n",
      "Iteration 24525: Policy loss: 0.028575. Value loss: 0.102557. Entropy: 0.297039.\n",
      "Training network. lr: 0.000062. clip: 0.024742\n",
      "Iteration 24526: Policy loss: -0.053534. Value loss: 0.127306. Entropy: 0.306337.\n",
      "Iteration 24527: Policy loss: -0.053275. Value loss: 0.050949. Entropy: 0.306274.\n",
      "Iteration 24528: Policy loss: -0.057852. Value loss: 0.039211. Entropy: 0.305769.\n",
      "Training network. lr: 0.000062. clip: 0.024742\n",
      "Iteration 24529: Policy loss: 0.453465. Value loss: 0.294506. Entropy: 0.308246.\n",
      "Iteration 24530: Policy loss: 0.446490. Value loss: 0.095564. Entropy: 0.306839.\n",
      "Iteration 24531: Policy loss: 0.451033. Value loss: 0.063702. Entropy: 0.307291.\n",
      "episode: 8302   score: 595.0  epsilon: 1.0    steps: 1024  evaluation reward: 502.0\n",
      "Training network. lr: 0.000062. clip: 0.024742\n",
      "Iteration 24532: Policy loss: 0.057341. Value loss: 0.192849. Entropy: 0.305372.\n",
      "Iteration 24533: Policy loss: 0.050231. Value loss: 0.115241. Entropy: 0.305536.\n",
      "Iteration 24534: Policy loss: 0.045600. Value loss: 0.080998. Entropy: 0.304843.\n",
      "Training network. lr: 0.000062. clip: 0.024742\n",
      "Iteration 24535: Policy loss: 0.161928. Value loss: 0.072236. Entropy: 0.295328.\n",
      "Iteration 24536: Policy loss: 0.157373. Value loss: 0.035822. Entropy: 0.295843.\n",
      "Iteration 24537: Policy loss: 0.151000. Value loss: 0.024948. Entropy: 0.295410.\n",
      "episode: 8303   score: 345.0  epsilon: 1.0    steps: 336  evaluation reward: 501.85\n",
      "episode: 8304   score: 255.0  epsilon: 1.0    steps: 664  evaluation reward: 495.4\n",
      "Training network. lr: 0.000062. clip: 0.024742\n",
      "Iteration 24538: Policy loss: -0.044141. Value loss: 0.160194. Entropy: 0.278313.\n",
      "Iteration 24539: Policy loss: -0.043339. Value loss: 0.082282. Entropy: 0.278896.\n",
      "Iteration 24540: Policy loss: -0.043323. Value loss: 0.057996. Entropy: 0.278145.\n",
      "episode: 8305   score: 400.0  epsilon: 1.0    steps: 888  evaluation reward: 491.85\n",
      "Training network. lr: 0.000062. clip: 0.024742\n",
      "Iteration 24541: Policy loss: -0.116725. Value loss: 0.341859. Entropy: 0.293060.\n",
      "Iteration 24542: Policy loss: -0.145579. Value loss: 0.167205. Entropy: 0.292029.\n",
      "Iteration 24543: Policy loss: -0.134802. Value loss: 0.114349. Entropy: 0.292893.\n",
      "Training network. lr: 0.000062. clip: 0.024742\n",
      "Iteration 24544: Policy loss: 0.220734. Value loss: 0.156802. Entropy: 0.296379.\n",
      "Iteration 24545: Policy loss: 0.215165. Value loss: 0.066477. Entropy: 0.296811.\n",
      "Iteration 24546: Policy loss: 0.215067. Value loss: 0.047061. Entropy: 0.296402.\n",
      "episode: 8306   score: 985.0  epsilon: 1.0    steps: 432  evaluation reward: 497.0\n",
      "Training network. lr: 0.000062. clip: 0.024742\n",
      "Iteration 24547: Policy loss: 0.037663. Value loss: 0.082343. Entropy: 0.284819.\n",
      "Iteration 24548: Policy loss: 0.036775. Value loss: 0.042652. Entropy: 0.284751.\n",
      "Iteration 24549: Policy loss: 0.034335. Value loss: 0.032186. Entropy: 0.285953.\n",
      "episode: 8307   score: 725.0  epsilon: 1.0    steps: 768  evaluation reward: 498.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network. lr: 0.000062. clip: 0.024742\n",
      "Iteration 24550: Policy loss: 0.209590. Value loss: 0.424789. Entropy: 0.297083.\n",
      "Iteration 24551: Policy loss: 0.214206. Value loss: 0.202050. Entropy: 0.297104.\n",
      "Iteration 24552: Policy loss: 0.211050. Value loss: 0.098946. Entropy: 0.296472.\n",
      "Training network. lr: 0.000061. clip: 0.024585\n",
      "Iteration 24553: Policy loss: 0.080842. Value loss: 0.148883. Entropy: 0.303897.\n",
      "Iteration 24554: Policy loss: 0.079248. Value loss: 0.078777. Entropy: 0.303485.\n",
      "Iteration 24555: Policy loss: 0.075857. Value loss: 0.057344. Entropy: 0.304401.\n",
      "episode: 8308   score: 530.0  epsilon: 1.0    steps: 536  evaluation reward: 499.6\n",
      "episode: 8309   score: 270.0  epsilon: 1.0    steps: 632  evaluation reward: 500.2\n",
      "Training network. lr: 0.000061. clip: 0.024585\n",
      "Iteration 24556: Policy loss: 0.080488. Value loss: 0.148854. Entropy: 0.273447.\n",
      "Iteration 24557: Policy loss: 0.086128. Value loss: 0.069177. Entropy: 0.273901.\n",
      "Iteration 24558: Policy loss: 0.092705. Value loss: 0.049951. Entropy: 0.273072.\n",
      "episode: 8310   score: 955.0  epsilon: 1.0    steps: 328  evaluation reward: 505.45\n",
      "episode: 8311   score: 285.0  epsilon: 1.0    steps: 496  evaluation reward: 502.05\n",
      "Training network. lr: 0.000061. clip: 0.024585\n",
      "Iteration 24559: Policy loss: 0.230317. Value loss: 0.207994. Entropy: 0.276308.\n",
      "Iteration 24560: Policy loss: 0.227367. Value loss: 0.092822. Entropy: 0.276066.\n",
      "Iteration 24561: Policy loss: 0.227159. Value loss: 0.061014. Entropy: 0.274935.\n",
      "episode: 8312   score: 360.0  epsilon: 1.0    steps: 160  evaluation reward: 499.45\n",
      "Training network. lr: 0.000061. clip: 0.024585\n",
      "Iteration 24562: Policy loss: 0.203301. Value loss: 0.128780. Entropy: 0.287724.\n",
      "Iteration 24563: Policy loss: 0.203394. Value loss: 0.075481. Entropy: 0.286999.\n",
      "Iteration 24564: Policy loss: 0.206560. Value loss: 0.059870. Entropy: 0.287514.\n",
      "Training network. lr: 0.000061. clip: 0.024585\n",
      "Iteration 24565: Policy loss: 0.127911. Value loss: 0.116924. Entropy: 0.295352.\n",
      "Iteration 24566: Policy loss: 0.125388. Value loss: 0.054256. Entropy: 0.294387.\n",
      "Iteration 24567: Policy loss: 0.126625. Value loss: 0.037097. Entropy: 0.295287.\n",
      "Training network. lr: 0.000061. clip: 0.024585\n",
      "Iteration 24568: Policy loss: 0.161890. Value loss: 0.106167. Entropy: 0.304226.\n",
      "Iteration 24569: Policy loss: 0.158144. Value loss: 0.049989. Entropy: 0.304324.\n",
      "Iteration 24570: Policy loss: 0.158450. Value loss: 0.034466. Entropy: 0.305036.\n",
      "episode: 8313   score: 290.0  epsilon: 1.0    steps: 720  evaluation reward: 496.15\n",
      "episode: 8314   score: 445.0  epsilon: 1.0    steps: 840  evaluation reward: 496.4\n",
      "Training network. lr: 0.000061. clip: 0.024585\n",
      "Iteration 24571: Policy loss: -0.224878. Value loss: 0.322116. Entropy: 0.293631.\n",
      "Iteration 24572: Policy loss: -0.233985. Value loss: 0.219636. Entropy: 0.292089.\n",
      "Iteration 24573: Policy loss: -0.235834. Value loss: 0.158926. Entropy: 0.292444.\n",
      "episode: 8315   score: 185.0  epsilon: 1.0    steps: 424  evaluation reward: 494.6\n",
      "Training network. lr: 0.000061. clip: 0.024585\n",
      "Iteration 24574: Policy loss: 0.046446. Value loss: 0.126474. Entropy: 0.289893.\n",
      "Iteration 24575: Policy loss: 0.044754. Value loss: 0.067543. Entropy: 0.290736.\n",
      "Iteration 24576: Policy loss: 0.043462. Value loss: 0.052314. Entropy: 0.291167.\n",
      "episode: 8316   score: 525.0  epsilon: 1.0    steps: 408  evaluation reward: 491.15\n",
      "episode: 8317   score: 515.0  epsilon: 1.0    steps: 616  evaluation reward: 493.35\n",
      "Training network. lr: 0.000061. clip: 0.024585\n",
      "Iteration 24577: Policy loss: -0.050746. Value loss: 0.097170. Entropy: 0.279786.\n",
      "Iteration 24578: Policy loss: -0.046790. Value loss: 0.062404. Entropy: 0.280074.\n",
      "Iteration 24579: Policy loss: -0.052769. Value loss: 0.051900. Entropy: 0.281059.\n",
      "Training network. lr: 0.000061. clip: 0.024585\n",
      "Iteration 24580: Policy loss: 0.092284. Value loss: 0.125998. Entropy: 0.295625.\n",
      "Iteration 24581: Policy loss: 0.086357. Value loss: 0.064646. Entropy: 0.294971.\n",
      "Iteration 24582: Policy loss: 0.091721. Value loss: 0.050873. Entropy: 0.294405.\n",
      "Training network. lr: 0.000061. clip: 0.024585\n",
      "Iteration 24583: Policy loss: -0.199505. Value loss: 0.330447. Entropy: 0.314117.\n",
      "Iteration 24584: Policy loss: -0.191789. Value loss: 0.245080. Entropy: 0.316133.\n",
      "Iteration 24585: Policy loss: -0.203348. Value loss: 0.189220. Entropy: 0.314493.\n",
      "episode: 8318   score: 440.0  epsilon: 1.0    steps: 616  evaluation reward: 492.2\n",
      "Training network. lr: 0.000061. clip: 0.024585\n",
      "Iteration 24586: Policy loss: -0.118784. Value loss: 0.264384. Entropy: 0.287973.\n",
      "Iteration 24587: Policy loss: -0.120242. Value loss: 0.157946. Entropy: 0.285921.\n",
      "Iteration 24588: Policy loss: -0.118069. Value loss: 0.115425. Entropy: 0.287041.\n",
      "Training network. lr: 0.000061. clip: 0.024585\n",
      "Iteration 24589: Policy loss: -0.046509. Value loss: 0.249580. Entropy: 0.309858.\n",
      "Iteration 24590: Policy loss: -0.047268. Value loss: 0.093203. Entropy: 0.309770.\n",
      "Iteration 24591: Policy loss: -0.052185. Value loss: 0.055544. Entropy: 0.309964.\n",
      "episode: 8319   score: 525.0  epsilon: 1.0    steps: 304  evaluation reward: 491.65\n",
      "episode: 8320   score: 440.0  epsilon: 1.0    steps: 432  evaluation reward: 486.2\n",
      "Training network. lr: 0.000061. clip: 0.024585\n",
      "Iteration 24592: Policy loss: -0.223807. Value loss: 0.202966. Entropy: 0.289592.\n",
      "Iteration 24593: Policy loss: -0.207971. Value loss: 0.114551. Entropy: 0.290316.\n",
      "Iteration 24594: Policy loss: -0.225169. Value loss: 0.078883. Entropy: 0.290944.\n",
      "episode: 8321   score: 430.0  epsilon: 1.0    steps: 616  evaluation reward: 484.4\n",
      "episode: 8322   score: 315.0  epsilon: 1.0    steps: 864  evaluation reward: 481.75\n",
      "Training network. lr: 0.000061. clip: 0.024585\n",
      "Iteration 24595: Policy loss: 0.030892. Value loss: 0.095475. Entropy: 0.289496.\n",
      "Iteration 24596: Policy loss: 0.028368. Value loss: 0.045187. Entropy: 0.289785.\n",
      "Iteration 24597: Policy loss: 0.029333. Value loss: 0.036373. Entropy: 0.289874.\n",
      "Training network. lr: 0.000061. clip: 0.024585\n",
      "Iteration 24598: Policy loss: 0.028204. Value loss: 0.106141. Entropy: 0.308407.\n",
      "Iteration 24599: Policy loss: 0.019037. Value loss: 0.045862. Entropy: 0.307226.\n",
      "Iteration 24600: Policy loss: 0.023869. Value loss: 0.036278. Entropy: 0.307665.\n",
      "episode: 8323   score: 575.0  epsilon: 1.0    steps: 736  evaluation reward: 482.25\n",
      "Training network. lr: 0.000061. clip: 0.024438\n",
      "Iteration 24601: Policy loss: 0.025778. Value loss: 0.207159. Entropy: 0.298489.\n",
      "Iteration 24602: Policy loss: 0.009458. Value loss: 0.085563. Entropy: 0.297080.\n",
      "Iteration 24603: Policy loss: 0.008841. Value loss: 0.053085. Entropy: 0.297048.\n",
      "Training network. lr: 0.000061. clip: 0.024438\n",
      "Iteration 24604: Policy loss: -0.258150. Value loss: 0.281118. Entropy: 0.304868.\n",
      "Iteration 24605: Policy loss: -0.254300. Value loss: 0.178129. Entropy: 0.305947.\n",
      "Iteration 24606: Policy loss: -0.256415. Value loss: 0.130684. Entropy: 0.305479.\n",
      "episode: 8324   score: 755.0  epsilon: 1.0    steps: 232  evaluation reward: 482.8\n",
      "episode: 8325   score: 305.0  epsilon: 1.0    steps: 440  evaluation reward: 481.05\n",
      "Training network. lr: 0.000061. clip: 0.024438\n",
      "Iteration 24607: Policy loss: -0.192799. Value loss: 0.227663. Entropy: 0.296595.\n",
      "Iteration 24608: Policy loss: -0.206741. Value loss: 0.104147. Entropy: 0.298731.\n",
      "Iteration 24609: Policy loss: -0.209998. Value loss: 0.076249. Entropy: 0.297342.\n",
      "episode: 8326   score: 645.0  epsilon: 1.0    steps: 304  evaluation reward: 484.2\n",
      "Training network. lr: 0.000061. clip: 0.024438\n",
      "Iteration 24610: Policy loss: 0.067454. Value loss: 0.099421. Entropy: 0.290441.\n",
      "Iteration 24611: Policy loss: 0.065685. Value loss: 0.056138. Entropy: 0.291030.\n",
      "Iteration 24612: Policy loss: 0.066108. Value loss: 0.042984. Entropy: 0.290908.\n",
      "episode: 8327   score: 345.0  epsilon: 1.0    steps: 424  evaluation reward: 480.95\n",
      "Training network. lr: 0.000061. clip: 0.024438\n",
      "Iteration 24613: Policy loss: -0.120710. Value loss: 0.090438. Entropy: 0.296882.\n",
      "Iteration 24614: Policy loss: -0.125073. Value loss: 0.051885. Entropy: 0.297712.\n",
      "Iteration 24615: Policy loss: -0.130468. Value loss: 0.040098. Entropy: 0.297397.\n",
      "Training network. lr: 0.000061. clip: 0.024438\n",
      "Iteration 24616: Policy loss: -0.181566. Value loss: 0.334973. Entropy: 0.306802.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24617: Policy loss: -0.191766. Value loss: 0.140903. Entropy: 0.306027.\n",
      "Iteration 24618: Policy loss: -0.202964. Value loss: 0.085269. Entropy: 0.305784.\n",
      "episode: 8328   score: 565.0  epsilon: 1.0    steps: 104  evaluation reward: 482.2\n",
      "episode: 8329   score: 595.0  epsilon: 1.0    steps: 872  evaluation reward: 483.75\n",
      "Training network. lr: 0.000061. clip: 0.024438\n",
      "Iteration 24619: Policy loss: 0.141199. Value loss: 0.152142. Entropy: 0.300291.\n",
      "Iteration 24620: Policy loss: 0.146375. Value loss: 0.053792. Entropy: 0.299839.\n",
      "Iteration 24621: Policy loss: 0.141768. Value loss: 0.035099. Entropy: 0.299343.\n",
      "Training network. lr: 0.000061. clip: 0.024438\n",
      "Iteration 24622: Policy loss: 0.001728. Value loss: 0.090001. Entropy: 0.309952.\n",
      "Iteration 24623: Policy loss: 0.006513. Value loss: 0.048013. Entropy: 0.309868.\n",
      "Iteration 24624: Policy loss: 0.007602. Value loss: 0.036674. Entropy: 0.309985.\n",
      "episode: 8330   score: 650.0  epsilon: 1.0    steps: 712  evaluation reward: 483.0\n",
      "episode: 8331   score: 645.0  epsilon: 1.0    steps: 928  evaluation reward: 485.85\n",
      "Training network. lr: 0.000061. clip: 0.024438\n",
      "Iteration 24625: Policy loss: -0.065637. Value loss: 0.263689. Entropy: 0.294229.\n",
      "Iteration 24626: Policy loss: -0.069508. Value loss: 0.107928. Entropy: 0.293604.\n",
      "Iteration 24627: Policy loss: -0.078655. Value loss: 0.082609. Entropy: 0.295660.\n",
      "episode: 8332   score: 560.0  epsilon: 1.0    steps: 496  evaluation reward: 484.5\n",
      "Training network. lr: 0.000061. clip: 0.024438\n",
      "Iteration 24628: Policy loss: 0.020534. Value loss: 0.073686. Entropy: 0.296506.\n",
      "Iteration 24629: Policy loss: 0.025927. Value loss: 0.043335. Entropy: 0.296867.\n",
      "Iteration 24630: Policy loss: 0.023345. Value loss: 0.031793. Entropy: 0.295620.\n",
      "Training network. lr: 0.000061. clip: 0.024438\n",
      "Iteration 24631: Policy loss: -0.025097. Value loss: 0.080425. Entropy: 0.305581.\n",
      "Iteration 24632: Policy loss: -0.026545. Value loss: 0.043380. Entropy: 0.304694.\n",
      "Iteration 24633: Policy loss: -0.021954. Value loss: 0.028614. Entropy: 0.305350.\n",
      "episode: 8333   score: 360.0  epsilon: 1.0    steps: 400  evaluation reward: 484.2\n",
      "Training network. lr: 0.000061. clip: 0.024438\n",
      "Iteration 24634: Policy loss: -0.142723. Value loss: 0.373049. Entropy: 0.296829.\n",
      "Iteration 24635: Policy loss: -0.163580. Value loss: 0.115099. Entropy: 0.293724.\n",
      "Iteration 24636: Policy loss: -0.153978. Value loss: 0.074445. Entropy: 0.293739.\n",
      "episode: 8334   score: 380.0  epsilon: 1.0    steps: 688  evaluation reward: 484.05\n",
      "episode: 8335   score: 420.0  epsilon: 1.0    steps: 992  evaluation reward: 483.6\n",
      "Training network. lr: 0.000061. clip: 0.024438\n",
      "Iteration 24637: Policy loss: 0.332092. Value loss: 0.556314. Entropy: 0.304505.\n",
      "Iteration 24638: Policy loss: 0.342272. Value loss: 0.291100. Entropy: 0.304563.\n",
      "Iteration 24639: Policy loss: 0.340920. Value loss: 0.210978. Entropy: 0.303713.\n",
      "Training network. lr: 0.000061. clip: 0.024438\n",
      "Iteration 24640: Policy loss: 0.075743. Value loss: 0.153370. Entropy: 0.302762.\n",
      "Iteration 24641: Policy loss: 0.074338. Value loss: 0.078903. Entropy: 0.303542.\n",
      "Iteration 24642: Policy loss: 0.081179. Value loss: 0.058761. Entropy: 0.303740.\n",
      "episode: 8336   score: 570.0  epsilon: 1.0    steps: 960  evaluation reward: 484.6\n",
      "Training network. lr: 0.000061. clip: 0.024438\n",
      "Iteration 24643: Policy loss: 0.078974. Value loss: 0.108343. Entropy: 0.300680.\n",
      "Iteration 24644: Policy loss: 0.081705. Value loss: 0.069401. Entropy: 0.299578.\n",
      "Iteration 24645: Policy loss: 0.080838. Value loss: 0.055730. Entropy: 0.300055.\n",
      "Training network. lr: 0.000061. clip: 0.024438\n",
      "Iteration 24646: Policy loss: -0.059997. Value loss: 0.104353. Entropy: 0.306966.\n",
      "Iteration 24647: Policy loss: -0.063779. Value loss: 0.048825. Entropy: 0.307924.\n",
      "Iteration 24648: Policy loss: -0.062569. Value loss: 0.037996. Entropy: 0.307282.\n",
      "episode: 8337   score: 260.0  epsilon: 1.0    steps: 248  evaluation reward: 481.15\n",
      "episode: 8338   score: 395.0  epsilon: 1.0    steps: 416  evaluation reward: 481.75\n",
      "Training network. lr: 0.000061. clip: 0.024438\n",
      "Iteration 24649: Policy loss: -0.510149. Value loss: 0.496169. Entropy: 0.287412.\n",
      "Iteration 24650: Policy loss: -0.526884. Value loss: 0.267057. Entropy: 0.289288.\n",
      "Iteration 24651: Policy loss: -0.535661. Value loss: 0.194654. Entropy: 0.289585.\n",
      "episode: 8339   score: 790.0  epsilon: 1.0    steps: 312  evaluation reward: 486.2\n",
      "Training network. lr: 0.000061. clip: 0.024281\n",
      "Iteration 24652: Policy loss: -0.298701. Value loss: 0.149006. Entropy: 0.299450.\n",
      "Iteration 24653: Policy loss: -0.294545. Value loss: 0.081137. Entropy: 0.300316.\n",
      "Iteration 24654: Policy loss: -0.296849. Value loss: 0.058624. Entropy: 0.299678.\n",
      "Training network. lr: 0.000061. clip: 0.024281\n",
      "Iteration 24655: Policy loss: 0.567437. Value loss: 0.340106. Entropy: 0.311641.\n",
      "Iteration 24656: Policy loss: 0.532745. Value loss: 0.122717. Entropy: 0.309963.\n",
      "Iteration 24657: Policy loss: 0.543723. Value loss: 0.083290. Entropy: 0.309546.\n",
      "episode: 8340   score: 255.0  epsilon: 1.0    steps: 376  evaluation reward: 483.75\n",
      "episode: 8341   score: 400.0  epsilon: 1.0    steps: 680  evaluation reward: 483.6\n",
      "Training network. lr: 0.000061. clip: 0.024281\n",
      "Iteration 24658: Policy loss: 0.235648. Value loss: 0.155093. Entropy: 0.296758.\n",
      "Iteration 24659: Policy loss: 0.229510. Value loss: 0.075706. Entropy: 0.294551.\n",
      "Iteration 24660: Policy loss: 0.237346. Value loss: 0.054531. Entropy: 0.296729.\n",
      "episode: 8342   score: 680.0  epsilon: 1.0    steps: 248  evaluation reward: 488.25\n",
      "Training network. lr: 0.000061. clip: 0.024281\n",
      "Iteration 24661: Policy loss: 0.088673. Value loss: 0.157359. Entropy: 0.309994.\n",
      "Iteration 24662: Policy loss: 0.082196. Value loss: 0.077091. Entropy: 0.309263.\n",
      "Iteration 24663: Policy loss: 0.091073. Value loss: 0.052571. Entropy: 0.309130.\n",
      "Training network. lr: 0.000061. clip: 0.024281\n",
      "Iteration 24664: Policy loss: -0.037107. Value loss: 0.152513. Entropy: 0.304752.\n",
      "Iteration 24665: Policy loss: -0.036274. Value loss: 0.062972. Entropy: 0.304026.\n",
      "Iteration 24666: Policy loss: -0.042139. Value loss: 0.044653. Entropy: 0.303261.\n",
      "episode: 8343   score: 740.0  epsilon: 1.0    steps: 920  evaluation reward: 489.25\n",
      "Training network. lr: 0.000061. clip: 0.024281\n",
      "Iteration 24667: Policy loss: 0.162062. Value loss: 0.147027. Entropy: 0.297487.\n",
      "Iteration 24668: Policy loss: 0.166563. Value loss: 0.083094. Entropy: 0.297183.\n",
      "Iteration 24669: Policy loss: 0.166639. Value loss: 0.058065. Entropy: 0.297730.\n",
      "episode: 8344   score: 395.0  epsilon: 1.0    steps: 872  evaluation reward: 489.6\n",
      "Training network. lr: 0.000061. clip: 0.024281\n",
      "Iteration 24670: Policy loss: -0.173180. Value loss: 0.239081. Entropy: 0.308943.\n",
      "Iteration 24671: Policy loss: -0.179779. Value loss: 0.133862. Entropy: 0.308916.\n",
      "Iteration 24672: Policy loss: -0.176724. Value loss: 0.078425. Entropy: 0.308732.\n",
      "episode: 8345   score: 335.0  epsilon: 1.0    steps: 664  evaluation reward: 486.25\n",
      "Training network. lr: 0.000061. clip: 0.024281\n",
      "Iteration 24673: Policy loss: 0.107594. Value loss: 0.131728. Entropy: 0.306056.\n",
      "Iteration 24674: Policy loss: 0.104859. Value loss: 0.055407. Entropy: 0.305774.\n",
      "Iteration 24675: Policy loss: 0.109600. Value loss: 0.039340. Entropy: 0.305002.\n",
      "Training network. lr: 0.000061. clip: 0.024281\n",
      "Iteration 24676: Policy loss: 0.090617. Value loss: 0.148545. Entropy: 0.299513.\n",
      "Iteration 24677: Policy loss: 0.089685. Value loss: 0.071507. Entropy: 0.299289.\n",
      "Iteration 24678: Policy loss: 0.079210. Value loss: 0.046913. Entropy: 0.298605.\n",
      "episode: 8346   score: 360.0  epsilon: 1.0    steps: 656  evaluation reward: 485.35\n",
      "Training network. lr: 0.000061. clip: 0.024281\n",
      "Iteration 24679: Policy loss: -0.313181. Value loss: 0.306709. Entropy: 0.292960.\n",
      "Iteration 24680: Policy loss: -0.326865. Value loss: 0.206123. Entropy: 0.294437.\n",
      "Iteration 24681: Policy loss: -0.321848. Value loss: 0.148074. Entropy: 0.292198.\n",
      "episode: 8347   score: 575.0  epsilon: 1.0    steps: 664  evaluation reward: 487.9\n",
      "episode: 8348   score: 270.0  epsilon: 1.0    steps: 896  evaluation reward: 488.5\n",
      "Training network. lr: 0.000061. clip: 0.024281\n",
      "Iteration 24682: Policy loss: 0.124271. Value loss: 0.185524. Entropy: 0.293454.\n",
      "Iteration 24683: Policy loss: 0.115977. Value loss: 0.077243. Entropy: 0.292511.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24684: Policy loss: 0.116256. Value loss: 0.053527. Entropy: 0.293941.\n",
      "episode: 8349   score: 545.0  epsilon: 1.0    steps: 208  evaluation reward: 487.55\n",
      "Training network. lr: 0.000061. clip: 0.024281\n",
      "Iteration 24685: Policy loss: -0.166642. Value loss: 0.281432. Entropy: 0.295353.\n",
      "Iteration 24686: Policy loss: -0.157152. Value loss: 0.122279. Entropy: 0.295162.\n",
      "Iteration 24687: Policy loss: -0.168361. Value loss: 0.079756. Entropy: 0.296232.\n",
      "Training network. lr: 0.000061. clip: 0.024281\n",
      "Iteration 24688: Policy loss: -0.001016. Value loss: 0.138601. Entropy: 0.303282.\n",
      "Iteration 24689: Policy loss: 0.008094. Value loss: 0.066677. Entropy: 0.304028.\n",
      "Iteration 24690: Policy loss: 0.008127. Value loss: 0.044577. Entropy: 0.302488.\n",
      "Training network. lr: 0.000061. clip: 0.024281\n",
      "Iteration 24691: Policy loss: 0.007825. Value loss: 0.281477. Entropy: 0.305760.\n",
      "Iteration 24692: Policy loss: -0.004129. Value loss: 0.125106. Entropy: 0.305885.\n",
      "Iteration 24693: Policy loss: 0.002370. Value loss: 0.087017. Entropy: 0.306581.\n",
      "episode: 8350   score: 760.0  epsilon: 1.0    steps: 448  evaluation reward: 490.95\n",
      "Training network. lr: 0.000061. clip: 0.024281\n",
      "Iteration 24694: Policy loss: 0.341136. Value loss: 0.200872. Entropy: 0.301998.\n",
      "Iteration 24695: Policy loss: 0.333933. Value loss: 0.063237. Entropy: 0.300656.\n",
      "Iteration 24696: Policy loss: 0.339024. Value loss: 0.036410. Entropy: 0.301235.\n",
      "now time :  2019-09-06 15:43:09.296125\n",
      "episode: 8351   score: 615.0  epsilon: 1.0    steps: 280  evaluation reward: 492.7\n",
      "episode: 8352   score: 640.0  epsilon: 1.0    steps: 672  evaluation reward: 494.9\n",
      "Training network. lr: 0.000061. clip: 0.024281\n",
      "Iteration 24697: Policy loss: -0.149701. Value loss: 0.196235. Entropy: 0.296817.\n",
      "Iteration 24698: Policy loss: -0.154313. Value loss: 0.097693. Entropy: 0.296356.\n",
      "Iteration 24699: Policy loss: -0.143039. Value loss: 0.069567. Entropy: 0.296778.\n",
      "episode: 8353   score: 540.0  epsilon: 1.0    steps: 944  evaluation reward: 495.45\n",
      "Training network. lr: 0.000061. clip: 0.024281\n",
      "Iteration 24700: Policy loss: 0.323112. Value loss: 0.164611. Entropy: 0.299908.\n",
      "Iteration 24701: Policy loss: 0.320987. Value loss: 0.085394. Entropy: 0.301287.\n",
      "Iteration 24702: Policy loss: 0.316636. Value loss: 0.056807. Entropy: 0.301136.\n",
      "episode: 8354   score: 695.0  epsilon: 1.0    steps: 160  evaluation reward: 495.15\n",
      "Training network. lr: 0.000060. clip: 0.024125\n",
      "Iteration 24703: Policy loss: 0.123737. Value loss: 0.167953. Entropy: 0.297201.\n",
      "Iteration 24704: Policy loss: 0.114554. Value loss: 0.078471. Entropy: 0.294899.\n",
      "Iteration 24705: Policy loss: 0.120846. Value loss: 0.058882. Entropy: 0.296300.\n",
      "episode: 8355   score: 435.0  epsilon: 1.0    steps: 656  evaluation reward: 496.25\n",
      "episode: 8356   score: 260.0  epsilon: 1.0    steps: 736  evaluation reward: 492.15\n",
      "Training network. lr: 0.000060. clip: 0.024125\n",
      "Iteration 24706: Policy loss: 0.215935. Value loss: 0.162053. Entropy: 0.285878.\n",
      "Iteration 24707: Policy loss: 0.209285. Value loss: 0.078248. Entropy: 0.284850.\n",
      "Iteration 24708: Policy loss: 0.207428. Value loss: 0.061739. Entropy: 0.284218.\n",
      "episode: 8357   score: 445.0  epsilon: 1.0    steps: 216  evaluation reward: 489.65\n",
      "Training network. lr: 0.000060. clip: 0.024125\n",
      "Iteration 24709: Policy loss: 0.069602. Value loss: 0.122560. Entropy: 0.295731.\n",
      "Iteration 24710: Policy loss: 0.070852. Value loss: 0.054643. Entropy: 0.294453.\n",
      "Iteration 24711: Policy loss: 0.071165. Value loss: 0.040143. Entropy: 0.295326.\n",
      "Training network. lr: 0.000060. clip: 0.024125\n",
      "Iteration 24712: Policy loss: -0.389439. Value loss: 0.380398. Entropy: 0.303060.\n",
      "Iteration 24713: Policy loss: -0.402850. Value loss: 0.236892. Entropy: 0.303092.\n",
      "Iteration 24714: Policy loss: -0.406326. Value loss: 0.174105. Entropy: 0.302746.\n",
      "Training network. lr: 0.000060. clip: 0.024125\n",
      "Iteration 24715: Policy loss: -0.424113. Value loss: 0.678549. Entropy: 0.300592.\n",
      "Iteration 24716: Policy loss: -0.409747. Value loss: 0.458547. Entropy: 0.299158.\n",
      "Iteration 24717: Policy loss: -0.438587. Value loss: 0.360004. Entropy: 0.300538.\n",
      "episode: 8358   score: 485.0  epsilon: 1.0    steps: 56  evaluation reward: 489.1\n",
      "episode: 8359   score: 395.0  epsilon: 1.0    steps: 232  evaluation reward: 487.6\n",
      "Training network. lr: 0.000060. clip: 0.024125\n",
      "Iteration 24718: Policy loss: 0.255453. Value loss: 0.113739. Entropy: 0.297334.\n",
      "Iteration 24719: Policy loss: 0.249587. Value loss: 0.058201. Entropy: 0.296832.\n",
      "Iteration 24720: Policy loss: 0.246338. Value loss: 0.043977. Entropy: 0.297333.\n",
      "episode: 8360   score: 515.0  epsilon: 1.0    steps: 528  evaluation reward: 487.8\n",
      "Training network. lr: 0.000060. clip: 0.024125\n",
      "Iteration 24721: Policy loss: 0.012200. Value loss: 0.087040. Entropy: 0.287816.\n",
      "Iteration 24722: Policy loss: 0.008998. Value loss: 0.048804. Entropy: 0.289475.\n",
      "Iteration 24723: Policy loss: 0.007010. Value loss: 0.037797. Entropy: 0.289097.\n",
      "Training network. lr: 0.000060. clip: 0.024125\n",
      "Iteration 24724: Policy loss: 0.189794. Value loss: 0.127248. Entropy: 0.302480.\n",
      "Iteration 24725: Policy loss: 0.186248. Value loss: 0.057143. Entropy: 0.303745.\n",
      "Iteration 24726: Policy loss: 0.183948. Value loss: 0.042527. Entropy: 0.301906.\n",
      "episode: 8361   score: 620.0  epsilon: 1.0    steps: 40  evaluation reward: 491.6\n",
      "episode: 8362   score: 395.0  epsilon: 1.0    steps: 56  evaluation reward: 490.5\n",
      "episode: 8363   score: 295.0  epsilon: 1.0    steps: 280  evaluation reward: 490.5\n",
      "Training network. lr: 0.000060. clip: 0.024125\n",
      "Iteration 24727: Policy loss: 0.056840. Value loss: 0.096563. Entropy: 0.287542.\n",
      "Iteration 24728: Policy loss: 0.057007. Value loss: 0.054428. Entropy: 0.288722.\n",
      "Iteration 24729: Policy loss: 0.051857. Value loss: 0.044471. Entropy: 0.288997.\n",
      "episode: 8364   score: 370.0  epsilon: 1.0    steps: 264  evaluation reward: 490.35\n",
      "Training network. lr: 0.000060. clip: 0.024125\n",
      "Iteration 24730: Policy loss: 0.141739. Value loss: 0.128819. Entropy: 0.287158.\n",
      "Iteration 24731: Policy loss: 0.137489. Value loss: 0.064722. Entropy: 0.288205.\n",
      "Iteration 24732: Policy loss: 0.135938. Value loss: 0.049771. Entropy: 0.286433.\n",
      "episode: 8365   score: 180.0  epsilon: 1.0    steps: 888  evaluation reward: 487.8\n",
      "Training network. lr: 0.000060. clip: 0.024125\n",
      "Iteration 24733: Policy loss: 0.171733. Value loss: 0.075290. Entropy: 0.285207.\n",
      "Iteration 24734: Policy loss: 0.169522. Value loss: 0.033745. Entropy: 0.284826.\n",
      "Iteration 24735: Policy loss: 0.171901. Value loss: 0.025149. Entropy: 0.284918.\n",
      "Training network. lr: 0.000060. clip: 0.024125\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(action_size, mode='PPO_LSTM')\n",
    "torch.save(agent.policy_net.state_dict(), \"./save_model/spaceinvaders_ppo_best\")\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n",
    "reset_max = 10\n",
    "\n",
    "\n",
    "### Loop through all environments and run PPO on them\n",
    "\n",
    "#env_names = ['Breakout-v0', 'Phoenix-v0', 'Asteroids-v0', 'SpaceInvaders-v0', 'MsPacman-v0', 'Asterix-v0', 'Atlantis-v0', 'Alien-v0', 'Amidar-v0', 'Assault-v0', 'BankHeist-v0']\n",
    "env_names = ['SpaceInvaders-v4']\n",
    "for a in range(len(env_names)):\n",
    "    name = env_names[a]\n",
    "    print(\"\\n\\n\\n ------- STARTING TRAINING FOR %s ------- \\n\\n\\n\" % (name))\n",
    "    \n",
    "    envs = []\n",
    "    for i in range(num_envs):\n",
    "        envs.append(GameEnv(name))\n",
    "        envs[i].reset_memory(agent.init_hidden())\n",
    "    #env.render()\n",
    "    \n",
    "\n",
    "    number_lives = envs[0].life\n",
    "    state_size = envs[0].observation_space.shape\n",
    "    if (name == 'SpaceInvaders-v0' or name == 'Breakout-v0'):\n",
    "        action_size = 4\n",
    "    else:\n",
    "        action_size = envs[0].action_space.n\n",
    "    rewards, episodes = [], []\n",
    "\n",
    "    vis_env_idx = 0\n",
    "    vis_env = envs[vis_env_idx]\n",
    "    e = 0\n",
    "    frame = 0\n",
    "    max_eval = -np.inf\n",
    "    reset_count = 0\n",
    "\n",
    "\n",
    "    agent = Agent(action_size, mode='PPO_LSTM')\n",
    "    torch.save(agent.policy_net.state_dict(), \"./save_model/\" + name + \"_best\")\n",
    "    evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "    frame = 0\n",
    "    memory_size = 0\n",
    "    reset_max = 10\n",
    "    \n",
    "    print(\"Determing min/max rewards of environment\")\n",
    "    [low, high] = score_range = get_score_range(name)\n",
    "    print(\"Min: %d. Max: %d.\" % (low, high))\n",
    "\n",
    "    while (frame < 10000000):\n",
    "        step = 0\n",
    "        assert(num_envs * env_mem_size == train_frame)\n",
    "        frame_next_vals = []\n",
    "        \n",
    "        for j in range(env_mem_size):\n",
    "            \n",
    "            curr_states = np.stack([envs[i].history[[HISTORY_SIZE-1],:,:] for i in range(num_envs)])\n",
    "            hiddens = torch.cat([envs[i].memory for i in range(num_envs)])\n",
    "            next_states = []\n",
    "            step += num_envs\n",
    "            frame += num_envs\n",
    "            actions, values, hiddens = agent.get_action(np.float32(curr_states) / 255., hiddens)\n",
    "            hiddens = hiddens.detach()\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                next_states.append(next_state)\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                env = envs[i]\n",
    "                \"\"\"\n",
    "                next_state, env.reward, env.done, env.info = env.step(actions[i])\n",
    "                if (i == vis_env_idx):\n",
    "                    vis_env._env.render()\n",
    "                \"\"\"\n",
    "                \n",
    "                frame_next_state = get_frame(next_states[i])\n",
    "                env.history[HISTORY_SIZE,:,:] = frame_next_state\n",
    "                env.memory = hiddens[[i]]\n",
    "                terminal_state = check_live(env.life, env.info['ale.lives'])\n",
    "                env.life = env.info['ale.lives']\n",
    "                r = (env.reward / high) * 20.0 #np.log(max(env.reward+1, 1))#((env.reward - low) / (high - low)) * 30\n",
    "                agent.memory.push(i, [deepcopy(curr_states[i]), hiddens[i].detach().cpu().data.numpy()], actions[i], r, terminal_state, values[i], 0, 0)\n",
    "                \n",
    "                if (j == env_mem_size-1):\n",
    "                    #net_in = np.stack([envs[k].history[1:,:,:] for k in range(num_envs)])\n",
    "                    net_in = np.stack([envs[k].history[[-1],:,:] for k in range(num_envs)])\n",
    "                    _, frame_next_vals, _ = agent.get_action(np.float32(net_in) / 255., hiddens)\n",
    "                \n",
    "                env.score += env.reward\n",
    "                env.history[:HISTORY_SIZE, :, :] = env.history[1:,:,:]\n",
    "        \n",
    "                if (env.done):\n",
    "                    if (e % 50 == 0):\n",
    "                        print('now time : ', datetime.now())\n",
    "                        rewards.append(np.mean(evaluation_reward))\n",
    "                        episodes.append(e)\n",
    "                        pylab.plot(episodes, rewards, 'b')\n",
    "                        pylab.savefig(\"./save_graph/\" + name + \"_ppo.png\")\n",
    "                        torch.save(agent.policy_net, \"./save_model/\" + name + \"_ppo\")\n",
    "\n",
    "                        if np.mean(evaluation_reward) > max_eval:\n",
    "                            torch.save(agent.policy_net.state_dict(), \"./save_model/\"  + name + \"_ppo_best\")\n",
    "                            max_eval = float(np.mean(evaluation_reward))\n",
    "                            reset_count = 0\n",
    "                        elif e > 5000:\n",
    "                            reset_count += 1\n",
    "                            \"\"\"\n",
    "                            if (reset_count == reset_max):\n",
    "                                print(\"Training went nowhere, starting again at best model\")\n",
    "                                agent.policy_net.load_state_dict(torch.load(\"./save_model/spaceinvaders_ppo_best\"))\n",
    "                                agent.update_target_net()\n",
    "                                reset_count = 0\n",
    "                            \"\"\"\n",
    "                    e += 1\n",
    "                    evaluation_reward.append(env.score)\n",
    "                    print(\"episode:\", e, \"  score:\", env.score,  \" epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                      \" evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "                    env.done = False\n",
    "                    env.score = 0\n",
    "                    env.history = np.zeros([HISTORY_SIZE+1,84,84], dtype=np.uint8)\n",
    "                    env.state = env.reset()\n",
    "                    env.life = number_lives\n",
    "                    get_init_state(env.history, env.state)\n",
    "                    env.reset_memory(agent.init_hidden())\n",
    "            \n",
    "        agent.train_policy_net(frame, frame_next_vals)\n",
    "        agent.update_target_net()\n",
    "    print(\"FINISHED TRAINING FOR %s\" % (name))\n",
    "    pylab.figure()\n",
    "    \n",
    "    for i in range(len(envs)):\n",
    "        envs[i]._env.close()\n",
    "    del envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
