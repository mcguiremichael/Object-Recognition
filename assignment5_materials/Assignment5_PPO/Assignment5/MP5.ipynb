{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvadersDeterministic-v4')\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lifes(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 6\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/home/michael/Documents/CS498DL/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x[:,:self.action_size] - torch.max(x[:,:self.action_size],0)[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 6.0   memory length: 519   epsilon: 1.0    steps: 519     evaluation reward: 6.0\n",
      "episode: 1   score: 6.0   memory length: 1050   epsilon: 1.0    steps: 531     evaluation reward: 6.0\n",
      "episode: 2   score: 6.0   memory length: 1725   epsilon: 1.0    steps: 675     evaluation reward: 6.0\n",
      "episode: 3   score: 9.0   memory length: 2407   epsilon: 1.0    steps: 682     evaluation reward: 6.75\n",
      "episode: 4   score: 9.0   memory length: 2954   epsilon: 1.0    steps: 547     evaluation reward: 7.2\n",
      "episode: 5   score: 7.0   memory length: 3839   epsilon: 1.0    steps: 885     evaluation reward: 7.166666666666667\n",
      "episode: 6   score: 18.0   memory length: 4857   epsilon: 1.0    steps: 1018     evaluation reward: 8.714285714285714\n",
      "episode: 7   score: 6.0   memory length: 5494   epsilon: 1.0    steps: 637     evaluation reward: 8.375\n",
      "episode: 8   score: 13.0   memory length: 6325   epsilon: 1.0    steps: 831     evaluation reward: 8.88888888888889\n",
      "episode: 9   score: 5.0   memory length: 6731   epsilon: 1.0    steps: 406     evaluation reward: 8.5\n",
      "episode: 10   score: 13.0   memory length: 7807   epsilon: 1.0    steps: 1076     evaluation reward: 8.909090909090908\n",
      "episode: 11   score: 6.0   memory length: 8297   epsilon: 1.0    steps: 490     evaluation reward: 8.666666666666666\n",
      "episode: 12   score: 15.0   memory length: 9216   epsilon: 1.0    steps: 919     evaluation reward: 9.153846153846153\n",
      "Training network\n",
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/Documents/CS498DL/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:167: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pol_loss += pol_avg.detach().cpu()[0]\n",
      "/home/michael/Documents/CS498DL/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:168: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  vf_loss += value_loss.detach().cpu()[0]\n",
      "/home/michael/Documents/CS498DL/Object-Recognition/assignment5_materials/Assignment5_PPO/Assignment5/agent.py:169: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  ent_total += ent.detach().cpu()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy loss: -0.004894. Value loss: 0.181557. Entropy: 1.781206.\n",
      "Iteration 2\n",
      "Policy loss: -0.012812. Value loss: 0.137354. Entropy: 1.778384.\n",
      "Iteration 3\n",
      "Policy loss: -0.020791. Value loss: 0.104406. Entropy: 1.776633.\n",
      "Iteration 4\n",
      "Policy loss: -0.025393. Value loss: 0.081929. Entropy: 1.774028.\n",
      "Iteration 5\n",
      "Policy loss: -0.026135. Value loss: 0.064560. Entropy: 1.774901.\n",
      "episode: 13   score: 14.0   memory length: 10826   epsilon: 1.0    steps: 1610     evaluation reward: 9.5\n",
      "episode: 14   score: 6.0   memory length: 11573   epsilon: 1.0    steps: 747     evaluation reward: 9.266666666666667\n",
      "episode: 15   score: 10.0   memory length: 12198   epsilon: 1.0    steps: 625     evaluation reward: 9.3125\n",
      "episode: 16   score: 13.0   memory length: 13031   epsilon: 1.0    steps: 833     evaluation reward: 9.529411764705882\n",
      "episode: 17   score: 9.0   memory length: 13709   epsilon: 1.0    steps: 678     evaluation reward: 9.5\n",
      "episode: 18   score: 7.0   memory length: 14101   epsilon: 1.0    steps: 392     evaluation reward: 9.368421052631579\n",
      "episode: 19   score: 5.0   memory length: 14589   epsilon: 1.0    steps: 488     evaluation reward: 9.15\n",
      "episode: 20   score: 4.0   memory length: 15118   epsilon: 1.0    steps: 529     evaluation reward: 8.904761904761905\n",
      "episode: 21   score: 12.0   memory length: 15757   epsilon: 1.0    steps: 639     evaluation reward: 9.045454545454545\n",
      "episode: 22   score: 4.0   memory length: 16371   epsilon: 1.0    steps: 614     evaluation reward: 8.826086956521738\n",
      "episode: 23   score: 18.0   memory length: 17348   epsilon: 1.0    steps: 977     evaluation reward: 9.208333333333334\n",
      "episode: 24   score: 7.0   memory length: 17802   epsilon: 1.0    steps: 454     evaluation reward: 9.12\n",
      "episode: 25   score: 11.0   memory length: 18689   epsilon: 1.0    steps: 887     evaluation reward: 9.192307692307692\n",
      "episode: 26   score: 8.0   memory length: 19086   epsilon: 1.0    steps: 397     evaluation reward: 9.148148148148149\n",
      "episode: 27   score: 5.0   memory length: 19601   epsilon: 1.0    steps: 515     evaluation reward: 9.0\n",
      "episode: 28   score: 7.0   memory length: 20321   epsilon: 1.0    steps: 720     evaluation reward: 8.931034482758621\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.001241. Value loss: 0.124176. Entropy: 1.765842.\n",
      "Iteration 2\n",
      "Policy loss: -0.008486. Value loss: 0.089800. Entropy: 1.760360.\n",
      "Iteration 3\n",
      "Policy loss: -0.012129. Value loss: 0.072721. Entropy: 1.758571.\n",
      "Iteration 4\n",
      "Policy loss: -0.015355. Value loss: 0.066411. Entropy: 1.757098.\n",
      "Iteration 5\n",
      "Policy loss: -0.019162. Value loss: 0.056814. Entropy: 1.757629.\n",
      "episode: 29   score: 12.0   memory length: 21077   epsilon: 1.0    steps: 756     evaluation reward: 9.033333333333333\n",
      "episode: 30   score: 8.0   memory length: 21570   epsilon: 1.0    steps: 493     evaluation reward: 9.0\n",
      "episode: 31   score: 10.0   memory length: 22338   epsilon: 1.0    steps: 768     evaluation reward: 9.03125\n",
      "episode: 32   score: 10.0   memory length: 23125   epsilon: 1.0    steps: 787     evaluation reward: 9.06060606060606\n",
      "episode: 33   score: 2.0   memory length: 23565   epsilon: 1.0    steps: 440     evaluation reward: 8.852941176470589\n",
      "episode: 34   score: 7.0   memory length: 24118   epsilon: 1.0    steps: 553     evaluation reward: 8.8\n",
      "episode: 35   score: 16.0   memory length: 24999   epsilon: 1.0    steps: 881     evaluation reward: 9.0\n",
      "episode: 36   score: 5.0   memory length: 25474   epsilon: 1.0    steps: 475     evaluation reward: 8.891891891891891\n",
      "episode: 37   score: 19.0   memory length: 26566   epsilon: 1.0    steps: 1092     evaluation reward: 9.157894736842104\n",
      "episode: 38   score: 8.0   memory length: 27221   epsilon: 1.0    steps: 655     evaluation reward: 9.128205128205128\n",
      "episode: 39   score: 5.0   memory length: 27730   epsilon: 1.0    steps: 509     evaluation reward: 9.025\n",
      "episode: 40   score: 6.0   memory length: 28243   epsilon: 1.0    steps: 513     evaluation reward: 8.951219512195122\n",
      "episode: 41   score: 10.0   memory length: 28902   epsilon: 1.0    steps: 659     evaluation reward: 8.976190476190476\n",
      "episode: 42   score: 13.0   memory length: 30403   epsilon: 1.0    steps: 1501     evaluation reward: 9.069767441860465\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.000343. Value loss: 0.107526. Entropy: 1.742377.\n",
      "Iteration 2\n",
      "Policy loss: -0.009126. Value loss: 0.077622. Entropy: 1.733754.\n",
      "Iteration 3\n",
      "Policy loss: -0.012987. Value loss: 0.066609. Entropy: 1.732637.\n",
      "Iteration 4\n",
      "Policy loss: -0.017879. Value loss: 0.057582. Entropy: 1.729777.\n",
      "Iteration 5\n",
      "Policy loss: -0.021816. Value loss: 0.052945. Entropy: 1.727460.\n",
      "episode: 43   score: 9.0   memory length: 30961   epsilon: 1.0    steps: 558     evaluation reward: 9.068181818181818\n",
      "episode: 44   score: 5.0   memory length: 31359   epsilon: 1.0    steps: 398     evaluation reward: 8.977777777777778\n",
      "episode: 45   score: 4.0   memory length: 31972   epsilon: 1.0    steps: 613     evaluation reward: 8.869565217391305\n",
      "episode: 46   score: 9.0   memory length: 32563   epsilon: 1.0    steps: 591     evaluation reward: 8.872340425531915\n",
      "episode: 47   score: 12.0   memory length: 33358   epsilon: 1.0    steps: 795     evaluation reward: 8.9375\n",
      "episode: 48   score: 11.0   memory length: 34155   epsilon: 1.0    steps: 797     evaluation reward: 8.979591836734693\n",
      "episode: 49   score: 7.0   memory length: 34668   epsilon: 1.0    steps: 513     evaluation reward: 8.94\n",
      "episode: 50   score: 9.0   memory length: 35184   epsilon: 1.0    steps: 516     evaluation reward: 8.941176470588236\n",
      "episode: 51   score: 4.0   memory length: 35687   epsilon: 1.0    steps: 503     evaluation reward: 8.846153846153847\n",
      "episode: 52   score: 8.0   memory length: 36132   epsilon: 1.0    steps: 445     evaluation reward: 8.830188679245284\n",
      "episode: 53   score: 5.0   memory length: 36579   epsilon: 1.0    steps: 447     evaluation reward: 8.75925925925926\n",
      "episode: 54   score: 14.0   memory length: 37267   epsilon: 1.0    steps: 688     evaluation reward: 8.854545454545455\n",
      "episode: 55   score: 6.0   memory length: 37848   epsilon: 1.0    steps: 581     evaluation reward: 8.803571428571429\n",
      "episode: 56   score: 6.0   memory length: 38442   epsilon: 1.0    steps: 594     evaluation reward: 8.75438596491228\n",
      "episode: 57   score: 9.0   memory length: 38890   epsilon: 1.0    steps: 448     evaluation reward: 8.758620689655173\n",
      "episode: 58   score: 6.0   memory length: 39478   epsilon: 1.0    steps: 588     evaluation reward: 8.711864406779661\n",
      "episode: 59   score: 10.0   memory length: 40443   epsilon: 1.0    steps: 965     evaluation reward: 8.733333333333333\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.004023. Value loss: 0.073918. Entropy: 1.697163.\n",
      "Iteration 2\n",
      "Policy loss: -0.012090. Value loss: 0.061843. Entropy: 1.689904.\n",
      "Iteration 3\n",
      "Policy loss: -0.019326. Value loss: 0.055802. Entropy: 1.687642.\n",
      "Iteration 4\n",
      "Policy loss: -0.022105. Value loss: 0.051982. Entropy: 1.685575.\n",
      "Iteration 5\n",
      "Policy loss: -0.027094. Value loss: 0.048577. Entropy: 1.681132.\n",
      "episode: 60   score: 7.0   memory length: 41109   epsilon: 1.0    steps: 666     evaluation reward: 8.704918032786885\n",
      "episode: 61   score: 12.0   memory length: 41937   epsilon: 1.0    steps: 828     evaluation reward: 8.758064516129032\n",
      "episode: 62   score: 5.0   memory length: 42412   epsilon: 1.0    steps: 475     evaluation reward: 8.698412698412698\n",
      "episode: 63   score: 11.0   memory length: 43099   epsilon: 1.0    steps: 687     evaluation reward: 8.734375\n",
      "episode: 64   score: 12.0   memory length: 43908   epsilon: 1.0    steps: 809     evaluation reward: 8.784615384615385\n",
      "episode: 65   score: 12.0   memory length: 44753   epsilon: 1.0    steps: 845     evaluation reward: 8.833333333333334\n",
      "episode: 66   score: 13.0   memory length: 46002   epsilon: 1.0    steps: 1249     evaluation reward: 8.895522388059701\n",
      "episode: 67   score: 6.0   memory length: 46391   epsilon: 1.0    steps: 389     evaluation reward: 8.852941176470589\n",
      "episode: 68   score: 12.0   memory length: 47218   epsilon: 1.0    steps: 827     evaluation reward: 8.898550724637682\n",
      "episode: 69   score: 8.0   memory length: 47779   epsilon: 1.0    steps: 561     evaluation reward: 8.885714285714286\n",
      "episode: 70   score: 8.0   memory length: 48265   epsilon: 1.0    steps: 486     evaluation reward: 8.873239436619718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 71   score: 11.0   memory length: 49070   epsilon: 1.0    steps: 805     evaluation reward: 8.902777777777779\n",
      "episode: 72   score: 7.0   memory length: 49593   epsilon: 1.0    steps: 523     evaluation reward: 8.876712328767123\n",
      "episode: 73   score: 6.0   memory length: 49987   epsilon: 1.0    steps: 394     evaluation reward: 8.837837837837839\n",
      "now time :  2018-12-17 23:52:01.998568\n",
      "episode: 74   score: 12.0   memory length: 50640   epsilon: 1.0    steps: 653     evaluation reward: 8.88\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.006815. Value loss: 0.068802. Entropy: 1.635168.\n",
      "Iteration 2\n",
      "Policy loss: -0.014771. Value loss: 0.058445. Entropy: 1.629181.\n",
      "Iteration 3\n",
      "Policy loss: -0.021800. Value loss: 0.053694. Entropy: 1.622944.\n",
      "Iteration 4\n",
      "Policy loss: -0.026540. Value loss: 0.049355. Entropy: 1.622539.\n",
      "Iteration 5\n",
      "Policy loss: -0.031320. Value loss: 0.047998. Entropy: 1.619112.\n",
      "episode: 75   score: 8.0   memory length: 51373   epsilon: 1.0    steps: 733     evaluation reward: 8.868421052631579\n",
      "episode: 76   score: 8.0   memory length: 52043   epsilon: 1.0    steps: 670     evaluation reward: 8.857142857142858\n",
      "episode: 77   score: 6.0   memory length: 52438   epsilon: 1.0    steps: 395     evaluation reward: 8.820512820512821\n",
      "episode: 78   score: 15.0   memory length: 53374   epsilon: 1.0    steps: 936     evaluation reward: 8.89873417721519\n",
      "episode: 79   score: 5.0   memory length: 53767   epsilon: 1.0    steps: 393     evaluation reward: 8.85\n",
      "episode: 80   score: 12.0   memory length: 54540   epsilon: 1.0    steps: 773     evaluation reward: 8.88888888888889\n",
      "episode: 81   score: 9.0   memory length: 55159   epsilon: 1.0    steps: 619     evaluation reward: 8.890243902439025\n",
      "episode: 82   score: 15.0   memory length: 56156   epsilon: 1.0    steps: 997     evaluation reward: 8.963855421686747\n",
      "episode: 83   score: 13.0   memory length: 56810   epsilon: 1.0    steps: 654     evaluation reward: 9.011904761904763\n",
      "episode: 84   score: 16.0   memory length: 57729   epsilon: 1.0    steps: 919     evaluation reward: 9.094117647058823\n",
      "episode: 85   score: 6.0   memory length: 58370   epsilon: 1.0    steps: 641     evaluation reward: 9.05813953488372\n",
      "episode: 86   score: 5.0   memory length: 58876   epsilon: 1.0    steps: 506     evaluation reward: 9.011494252873563\n",
      "episode: 87   score: 12.0   memory length: 59742   epsilon: 1.0    steps: 866     evaluation reward: 9.045454545454545\n",
      "episode: 88   score: 10.0   memory length: 60445   epsilon: 1.0    steps: 703     evaluation reward: 9.0561797752809\n",
      "episode: 89   score: 11.0   memory length: 61183   epsilon: 1.0    steps: 738     evaluation reward: 9.077777777777778\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.009445. Value loss: 0.071537. Entropy: 1.572789.\n",
      "Iteration 2\n",
      "Policy loss: -0.019487. Value loss: 0.059613. Entropy: 1.559913.\n",
      "Iteration 3\n",
      "Policy loss: -0.026856. Value loss: 0.054993. Entropy: 1.556407.\n",
      "Iteration 4\n",
      "Policy loss: -0.032559. Value loss: 0.051252. Entropy: 1.555202.\n",
      "Iteration 5\n",
      "Policy loss: -0.035463. Value loss: 0.048822. Entropy: 1.551562.\n",
      "episode: 90   score: 8.0   memory length: 61780   epsilon: 1.0    steps: 597     evaluation reward: 9.065934065934066\n",
      "episode: 91   score: 12.0   memory length: 62565   epsilon: 1.0    steps: 785     evaluation reward: 9.097826086956522\n",
      "episode: 92   score: 12.0   memory length: 63364   epsilon: 1.0    steps: 799     evaluation reward: 9.129032258064516\n",
      "episode: 93   score: 8.0   memory length: 63761   epsilon: 1.0    steps: 397     evaluation reward: 9.117021276595745\n",
      "episode: 94   score: 12.0   memory length: 64696   epsilon: 1.0    steps: 935     evaluation reward: 9.147368421052631\n",
      "episode: 95   score: 8.0   memory length: 65409   epsilon: 1.0    steps: 713     evaluation reward: 9.135416666666666\n",
      "episode: 96   score: 8.0   memory length: 66166   epsilon: 1.0    steps: 757     evaluation reward: 9.123711340206185\n",
      "episode: 97   score: 9.0   memory length: 66814   epsilon: 1.0    steps: 648     evaluation reward: 9.122448979591837\n",
      "episode: 98   score: 18.0   memory length: 67948   epsilon: 1.0    steps: 1134     evaluation reward: 9.212121212121213\n",
      "episode: 99   score: 4.0   memory length: 68569   epsilon: 1.0    steps: 621     evaluation reward: 9.16\n",
      "episode: 100   score: 20.0   memory length: 69752   epsilon: 1.0    steps: 1183     evaluation reward: 9.3\n",
      "episode: 101   score: 8.0   memory length: 70395   epsilon: 1.0    steps: 643     evaluation reward: 9.32\n",
      "episode: 102   score: 17.0   memory length: 71338   epsilon: 1.0    steps: 943     evaluation reward: 9.43\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.007878. Value loss: 0.069083. Entropy: 1.486472.\n",
      "Iteration 2\n",
      "Policy loss: -0.020521. Value loss: 0.059037. Entropy: 1.473537.\n",
      "Iteration 3\n",
      "Policy loss: -0.028497. Value loss: 0.054030. Entropy: 1.466247.\n",
      "Iteration 4\n",
      "Policy loss: -0.032173. Value loss: 0.051401. Entropy: 1.460339.\n",
      "Iteration 5\n",
      "Policy loss: -0.038849. Value loss: 0.048614. Entropy: 1.458400.\n",
      "episode: 103   score: 4.0   memory length: 71745   epsilon: 1.0    steps: 407     evaluation reward: 9.38\n",
      "episode: 104   score: 13.0   memory length: 72571   epsilon: 1.0    steps: 826     evaluation reward: 9.42\n",
      "episode: 105   score: 13.0   memory length: 73421   epsilon: 1.0    steps: 850     evaluation reward: 9.48\n",
      "episode: 106   score: 12.0   memory length: 74214   epsilon: 1.0    steps: 793     evaluation reward: 9.42\n",
      "episode: 107   score: 4.0   memory length: 74579   epsilon: 1.0    steps: 365     evaluation reward: 9.4\n",
      "episode: 108   score: 13.0   memory length: 75543   epsilon: 1.0    steps: 964     evaluation reward: 9.4\n",
      "episode: 109   score: 10.0   memory length: 76167   epsilon: 1.0    steps: 624     evaluation reward: 9.45\n",
      "episode: 110   score: 10.0   memory length: 76694   epsilon: 1.0    steps: 527     evaluation reward: 9.42\n",
      "episode: 111   score: 21.0   memory length: 77852   epsilon: 1.0    steps: 1158     evaluation reward: 9.57\n",
      "episode: 112   score: 19.0   memory length: 78846   epsilon: 1.0    steps: 994     evaluation reward: 9.61\n",
      "episode: 113   score: 7.0   memory length: 79362   epsilon: 1.0    steps: 516     evaluation reward: 9.54\n",
      "episode: 114   score: 7.0   memory length: 79877   epsilon: 1.0    steps: 515     evaluation reward: 9.55\n",
      "episode: 115   score: 7.0   memory length: 80372   epsilon: 1.0    steps: 495     evaluation reward: 9.52\n",
      "episode: 116   score: 8.0   memory length: 80898   epsilon: 1.0    steps: 526     evaluation reward: 9.47\n",
      "episode: 117   score: 23.0   memory length: 81917   epsilon: 1.0    steps: 1019     evaluation reward: 9.61\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.009582. Value loss: 0.070145. Entropy: 1.379184.\n",
      "Iteration 2\n",
      "Policy loss: -0.025027. Value loss: 0.058597. Entropy: 1.369073.\n",
      "Iteration 3\n",
      "Policy loss: -0.033118. Value loss: 0.054308. Entropy: 1.357251.\n",
      "Iteration 4\n",
      "Policy loss: -0.038136. Value loss: 0.050128. Entropy: 1.349288.\n",
      "Iteration 5\n",
      "Policy loss: -0.045727. Value loss: 0.048351. Entropy: 1.345329.\n",
      "episode: 118   score: 4.0   memory length: 82286   epsilon: 1.0    steps: 369     evaluation reward: 9.58\n",
      "episode: 119   score: 8.0   memory length: 82923   epsilon: 1.0    steps: 637     evaluation reward: 9.61\n",
      "episode: 120   score: 17.0   memory length: 83986   epsilon: 1.0    steps: 1063     evaluation reward: 9.74\n",
      "episode: 121   score: 8.0   memory length: 84593   epsilon: 1.0    steps: 607     evaluation reward: 9.7\n",
      "episode: 122   score: 6.0   memory length: 85088   epsilon: 1.0    steps: 495     evaluation reward: 9.72\n",
      "episode: 123   score: 8.0   memory length: 85884   epsilon: 1.0    steps: 796     evaluation reward: 9.62\n",
      "episode: 124   score: 4.0   memory length: 86443   epsilon: 1.0    steps: 559     evaluation reward: 9.59\n",
      "episode: 125   score: 10.0   memory length: 87227   epsilon: 1.0    steps: 784     evaluation reward: 9.58\n",
      "episode: 126   score: 14.0   memory length: 88053   epsilon: 1.0    steps: 826     evaluation reward: 9.64\n",
      "episode: 127   score: 8.0   memory length: 88438   epsilon: 1.0    steps: 385     evaluation reward: 9.67\n",
      "episode: 128   score: 7.0   memory length: 89145   epsilon: 1.0    steps: 707     evaluation reward: 9.67\n",
      "episode: 129   score: 4.0   memory length: 89670   epsilon: 1.0    steps: 525     evaluation reward: 9.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 130   score: 6.0   memory length: 90184   epsilon: 1.0    steps: 514     evaluation reward: 9.57\n",
      "episode: 131   score: 15.0   memory length: 90986   epsilon: 1.0    steps: 802     evaluation reward: 9.62\n",
      "episode: 132   score: 7.0   memory length: 91722   epsilon: 1.0    steps: 736     evaluation reward: 9.59\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.006453. Value loss: 0.059832. Entropy: 1.265498.\n",
      "Iteration 2\n",
      "Policy loss: -0.023314. Value loss: 0.053430. Entropy: 1.253711.\n",
      "Iteration 3\n",
      "Policy loss: -0.032596. Value loss: 0.050855. Entropy: 1.243698.\n",
      "Iteration 4\n",
      "Policy loss: -0.040911. Value loss: 0.047739. Entropy: 1.237917.\n",
      "Iteration 5\n",
      "Policy loss: -0.043652. Value loss: 0.045978. Entropy: 1.234080.\n",
      "episode: 133   score: 13.0   memory length: 92746   epsilon: 1.0    steps: 1024     evaluation reward: 9.7\n",
      "episode: 134   score: 7.0   memory length: 93135   epsilon: 1.0    steps: 389     evaluation reward: 9.7\n",
      "episode: 135   score: 7.0   memory length: 93810   epsilon: 1.0    steps: 675     evaluation reward: 9.61\n",
      "episode: 136   score: 4.0   memory length: 94287   epsilon: 1.0    steps: 477     evaluation reward: 9.6\n",
      "episode: 137   score: 10.0   memory length: 95000   epsilon: 1.0    steps: 713     evaluation reward: 9.51\n",
      "episode: 138   score: 7.0   memory length: 95688   epsilon: 1.0    steps: 688     evaluation reward: 9.5\n",
      "episode: 139   score: 8.0   memory length: 96089   epsilon: 1.0    steps: 401     evaluation reward: 9.53\n",
      "episode: 140   score: 8.0   memory length: 96768   epsilon: 1.0    steps: 679     evaluation reward: 9.55\n",
      "episode: 141   score: 4.0   memory length: 97308   epsilon: 1.0    steps: 540     evaluation reward: 9.49\n",
      "episode: 142   score: 19.0   memory length: 98181   epsilon: 1.0    steps: 873     evaluation reward: 9.55\n",
      "episode: 143   score: 17.0   memory length: 99326   epsilon: 1.0    steps: 1145     evaluation reward: 9.63\n",
      "now time :  2018-12-18 00:03:10.574584\n",
      "episode: 144   score: 5.0   memory length: 100001   epsilon: 1.0    steps: 675     evaluation reward: 9.63\n",
      "episode: 145   score: 8.0   memory length: 100605   epsilon: 1.0    steps: 604     evaluation reward: 9.67\n",
      "episode: 146   score: 14.0   memory length: 101549   epsilon: 1.0    steps: 944     evaluation reward: 9.72\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: -0.000784. Value loss: 0.057432. Entropy: 1.144615.\n",
      "Iteration 2\n",
      "Policy loss: -0.019191. Value loss: 0.050823. Entropy: 1.125611.\n",
      "Iteration 3\n",
      "Policy loss: -0.029949. Value loss: 0.048219. Entropy: 1.124268.\n",
      "Iteration 4\n",
      "Policy loss: -0.035658. Value loss: 0.045872. Entropy: 1.113890.\n",
      "Iteration 5\n",
      "Policy loss: -0.040533. Value loss: 0.044928. Entropy: 1.103999.\n",
      "episode: 147   score: 12.0   memory length: 102520   epsilon: 1.0    steps: 971     evaluation reward: 9.72\n",
      "episode: 148   score: 8.0   memory length: 103118   epsilon: 1.0    steps: 598     evaluation reward: 9.69\n",
      "episode: 149   score: 6.0   memory length: 103642   epsilon: 1.0    steps: 524     evaluation reward: 9.68\n",
      "episode: 150   score: 12.0   memory length: 104450   epsilon: 1.0    steps: 808     evaluation reward: 9.71\n",
      "episode: 151   score: 11.0   memory length: 105217   epsilon: 1.0    steps: 767     evaluation reward: 9.78\n",
      "episode: 152   score: 6.0   memory length: 105745   epsilon: 1.0    steps: 528     evaluation reward: 9.76\n",
      "episode: 153   score: 3.0   memory length: 106350   epsilon: 1.0    steps: 605     evaluation reward: 9.74\n",
      "episode: 154   score: 6.0   memory length: 106840   epsilon: 1.0    steps: 490     evaluation reward: 9.66\n",
      "episode: 155   score: 5.0   memory length: 107455   epsilon: 1.0    steps: 615     evaluation reward: 9.65\n",
      "episode: 156   score: 15.0   memory length: 108536   epsilon: 1.0    steps: 1081     evaluation reward: 9.74\n",
      "episode: 157   score: 6.0   memory length: 109086   epsilon: 1.0    steps: 550     evaluation reward: 9.71\n",
      "episode: 158   score: 15.0   memory length: 110042   epsilon: 1.0    steps: 956     evaluation reward: 9.8\n",
      "episode: 159   score: 16.0   memory length: 111049   epsilon: 1.0    steps: 1007     evaluation reward: 9.86\n",
      "episode: 160   score: 11.0   memory length: 111840   epsilon: 1.0    steps: 791     evaluation reward: 9.9\n",
      "episode: 161   score: 6.0   memory length: 112316   epsilon: 1.0    steps: 476     evaluation reward: 9.84\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.006569. Value loss: 0.056604. Entropy: 1.010893.\n",
      "Iteration 2\n",
      "Policy loss: -0.011309. Value loss: 0.050931. Entropy: 0.992518.\n",
      "Iteration 3\n",
      "Policy loss: -0.022338. Value loss: 0.048522. Entropy: 0.989795.\n",
      "Iteration 4\n",
      "Policy loss: -0.030858. Value loss: 0.047134. Entropy: 0.985091.\n",
      "Iteration 5\n",
      "Policy loss: -0.034937. Value loss: 0.045668. Entropy: 0.977632.\n",
      "episode: 162   score: 13.0   memory length: 112951   epsilon: 1.0    steps: 635     evaluation reward: 9.92\n",
      "episode: 163   score: 8.0   memory length: 113560   epsilon: 1.0    steps: 609     evaluation reward: 9.89\n",
      "episode: 164   score: 7.0   memory length: 114353   epsilon: 1.0    steps: 793     evaluation reward: 9.84\n",
      "episode: 165   score: 7.0   memory length: 114848   epsilon: 1.0    steps: 495     evaluation reward: 9.79\n",
      "episode: 166   score: 16.0   memory length: 115673   epsilon: 1.0    steps: 825     evaluation reward: 9.82\n",
      "episode: 167   score: 12.0   memory length: 116496   epsilon: 1.0    steps: 823     evaluation reward: 9.88\n",
      "episode: 168   score: 10.0   memory length: 117304   epsilon: 1.0    steps: 808     evaluation reward: 9.86\n",
      "episode: 169   score: 8.0   memory length: 118054   epsilon: 1.0    steps: 750     evaluation reward: 9.86\n",
      "episode: 170   score: 11.0   memory length: 118737   epsilon: 1.0    steps: 683     evaluation reward: 9.89\n",
      "episode: 171   score: 9.0   memory length: 119502   epsilon: 1.0    steps: 765     evaluation reward: 9.87\n",
      "episode: 172   score: 6.0   memory length: 119902   epsilon: 1.0    steps: 400     evaluation reward: 9.86\n",
      "episode: 173   score: 6.0   memory length: 120568   epsilon: 1.0    steps: 666     evaluation reward: 9.86\n",
      "episode: 174   score: 12.0   memory length: 121391   epsilon: 1.0    steps: 823     evaluation reward: 9.86\n",
      "episode: 175   score: 7.0   memory length: 122056   epsilon: 1.0    steps: 665     evaluation reward: 9.85\n",
      "episode: 176   score: 8.0   memory length: 122435   epsilon: 1.0    steps: 379     evaluation reward: 9.85\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.024593. Value loss: 0.055834. Entropy: 0.876178.\n",
      "Iteration 2\n",
      "Policy loss: -0.001168. Value loss: 0.050459. Entropy: 0.867825.\n",
      "Iteration 3\n",
      "Policy loss: -0.002779. Value loss: 0.049398. Entropy: 0.860581.\n",
      "Iteration 4\n",
      "Policy loss: -0.016858. Value loss: 0.047481. Entropy: 0.853694.\n",
      "Iteration 5\n",
      "Policy loss: -0.023708. Value loss: 0.046825. Entropy: 0.850332.\n",
      "episode: 177   score: 12.0   memory length: 123274   epsilon: 1.0    steps: 839     evaluation reward: 9.91\n",
      "episode: 178   score: 8.0   memory length: 123781   epsilon: 1.0    steps: 507     evaluation reward: 9.84\n",
      "episode: 179   score: 12.0   memory length: 124528   epsilon: 1.0    steps: 747     evaluation reward: 9.91\n",
      "episode: 180   score: 5.0   memory length: 125127   epsilon: 1.0    steps: 599     evaluation reward: 9.84\n",
      "episode: 181   score: 7.0   memory length: 125656   epsilon: 1.0    steps: 529     evaluation reward: 9.82\n",
      "episode: 182   score: 7.0   memory length: 126295   epsilon: 1.0    steps: 639     evaluation reward: 9.74\n",
      "episode: 183   score: 9.0   memory length: 126901   epsilon: 1.0    steps: 606     evaluation reward: 9.7\n",
      "episode: 184   score: 16.0   memory length: 127635   epsilon: 1.0    steps: 734     evaluation reward: 9.7\n",
      "episode: 185   score: 5.0   memory length: 128021   epsilon: 1.0    steps: 386     evaluation reward: 9.69\n",
      "episode: 186   score: 13.0   memory length: 128968   epsilon: 1.0    steps: 947     evaluation reward: 9.77\n",
      "episode: 187   score: 9.0   memory length: 129603   epsilon: 1.0    steps: 635     evaluation reward: 9.74\n",
      "episode: 188   score: 6.0   memory length: 130146   epsilon: 1.0    steps: 543     evaluation reward: 9.7\n",
      "episode: 189   score: 21.0   memory length: 131247   epsilon: 1.0    steps: 1101     evaluation reward: 9.8\n",
      "episode: 190   score: 10.0   memory length: 132026   epsilon: 1.0    steps: 779     evaluation reward: 9.82\n",
      "episode: 191   score: 18.0   memory length: 133103   epsilon: 1.0    steps: 1077     evaluation reward: 9.88\n",
      "Training network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Policy loss: 0.077270. Value loss: 0.061027. Entropy: 0.731165.\n",
      "Iteration 2\n",
      "Policy loss: 0.039028. Value loss: 0.055285. Entropy: 0.736255.\n",
      "Iteration 3\n",
      "Policy loss: 0.020652. Value loss: 0.054018. Entropy: 0.731674.\n",
      "Iteration 4\n",
      "Policy loss: 0.014550. Value loss: 0.052156. Entropy: 0.728283.\n",
      "Iteration 5\n",
      "Policy loss: 0.018015. Value loss: 0.051309. Entropy: 0.716410.\n",
      "episode: 192   score: 7.0   memory length: 133510   epsilon: 1.0    steps: 407     evaluation reward: 9.83\n",
      "episode: 193   score: 17.0   memory length: 134883   epsilon: 1.0    steps: 1373     evaluation reward: 9.92\n",
      "episode: 194   score: 9.0   memory length: 135519   epsilon: 1.0    steps: 636     evaluation reward: 9.89\n",
      "episode: 195   score: 8.0   memory length: 136316   epsilon: 1.0    steps: 797     evaluation reward: 9.89\n",
      "episode: 196   score: 9.0   memory length: 136929   epsilon: 1.0    steps: 613     evaluation reward: 9.9\n",
      "episode: 197   score: 9.0   memory length: 137556   epsilon: 1.0    steps: 627     evaluation reward: 9.9\n",
      "episode: 198   score: 5.0   memory length: 138044   epsilon: 1.0    steps: 488     evaluation reward: 9.77\n",
      "episode: 199   score: 28.0   memory length: 139344   epsilon: 1.0    steps: 1300     evaluation reward: 10.01\n",
      "episode: 200   score: 10.0   memory length: 140017   epsilon: 1.0    steps: 673     evaluation reward: 9.91\n",
      "episode: 201   score: 12.0   memory length: 140811   epsilon: 1.0    steps: 794     evaluation reward: 9.95\n",
      "episode: 202   score: 9.0   memory length: 141326   epsilon: 1.0    steps: 515     evaluation reward: 9.87\n",
      "episode: 203   score: 7.0   memory length: 142129   epsilon: 1.0    steps: 803     evaluation reward: 9.9\n",
      "episode: 204   score: 11.0   memory length: 142767   epsilon: 1.0    steps: 638     evaluation reward: 9.88\n",
      "episode: 205   score: 9.0   memory length: 143288   epsilon: 1.0    steps: 521     evaluation reward: 9.84\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.157606. Value loss: 0.066778. Entropy: 0.615090.\n",
      "Iteration 2\n",
      "Policy loss: 0.075384. Value loss: 0.060942. Entropy: 0.614429.\n",
      "Iteration 3\n",
      "Policy loss: 0.100422. Value loss: 0.059230. Entropy: 0.605037.\n",
      "Iteration 4\n",
      "Policy loss: 0.068540. Value loss: 0.057745. Entropy: 0.608131.\n",
      "Iteration 5\n",
      "Policy loss: 0.088042. Value loss: 0.058501. Entropy: 0.595581.\n",
      "episode: 206   score: 10.0   memory length: 143975   epsilon: 1.0    steps: 687     evaluation reward: 9.82\n",
      "episode: 207   score: 7.0   memory length: 144740   epsilon: 1.0    steps: 765     evaluation reward: 9.85\n",
      "episode: 208   score: 10.0   memory length: 145538   epsilon: 1.0    steps: 798     evaluation reward: 9.82\n",
      "episode: 209   score: 9.0   memory length: 146190   epsilon: 1.0    steps: 652     evaluation reward: 9.81\n",
      "episode: 210   score: 15.0   memory length: 147423   epsilon: 1.0    steps: 1233     evaluation reward: 9.86\n",
      "episode: 211   score: 7.0   memory length: 148078   epsilon: 1.0    steps: 655     evaluation reward: 9.72\n",
      "episode: 212   score: 9.0   memory length: 148759   epsilon: 1.0    steps: 681     evaluation reward: 9.62\n",
      "episode: 213   score: 2.0   memory length: 149322   epsilon: 1.0    steps: 563     evaluation reward: 9.57\n",
      "now time :  2018-12-18 00:20:31.794496\n",
      "episode: 214   score: 11.0   memory length: 150106   epsilon: 1.0    steps: 784     evaluation reward: 9.61\n",
      "episode: 215   score: 12.0   memory length: 150937   epsilon: 1.0    steps: 831     evaluation reward: 9.66\n",
      "episode: 216   score: 6.0   memory length: 151505   epsilon: 1.0    steps: 568     evaluation reward: 9.64\n",
      "episode: 217   score: 7.0   memory length: 152038   epsilon: 1.0    steps: 533     evaluation reward: 9.48\n",
      "episode: 218   score: 7.0   memory length: 152494   epsilon: 1.0    steps: 456     evaluation reward: 9.51\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.316148. Value loss: 0.071724. Entropy: 0.498587.\n",
      "Iteration 2\n",
      "Policy loss: 0.196243. Value loss: 0.067911. Entropy: 0.489787.\n",
      "Iteration 3\n",
      "Policy loss: 0.190773. Value loss: 0.067889. Entropy: 0.480512.\n",
      "Iteration 4\n",
      "Policy loss: 0.220533. Value loss: 0.068407. Entropy: 0.462764.\n",
      "Iteration 5\n",
      "Policy loss: 0.268969. Value loss: 0.069113. Entropy: 0.455240.\n",
      "episode: 219   score: 20.0   memory length: 153663   epsilon: 1.0    steps: 1169     evaluation reward: 9.63\n",
      "episode: 220   score: 9.0   memory length: 154325   epsilon: 1.0    steps: 662     evaluation reward: 9.55\n",
      "episode: 221   score: 11.0   memory length: 154830   epsilon: 1.0    steps: 505     evaluation reward: 9.58\n",
      "episode: 222   score: 16.0   memory length: 155920   epsilon: 1.0    steps: 1090     evaluation reward: 9.68\n",
      "episode: 223   score: 10.0   memory length: 156512   epsilon: 1.0    steps: 592     evaluation reward: 9.7\n",
      "episode: 224   score: 4.0   memory length: 157010   epsilon: 1.0    steps: 498     evaluation reward: 9.7\n",
      "episode: 225   score: 7.0   memory length: 157465   epsilon: 1.0    steps: 455     evaluation reward: 9.67\n",
      "episode: 226   score: 5.0   memory length: 157846   epsilon: 1.0    steps: 381     evaluation reward: 9.58\n",
      "episode: 227   score: 3.0   memory length: 158236   epsilon: 1.0    steps: 390     evaluation reward: 9.53\n",
      "episode: 228   score: 5.0   memory length: 158899   epsilon: 1.0    steps: 663     evaluation reward: 9.51\n",
      "episode: 229   score: 3.0   memory length: 159391   epsilon: 1.0    steps: 492     evaluation reward: 9.5\n",
      "episode: 230   score: 14.0   memory length: 160125   epsilon: 1.0    steps: 734     evaluation reward: 9.58\n",
      "episode: 231   score: 7.0   memory length: 160523   epsilon: 1.0    steps: 398     evaluation reward: 9.5\n",
      "episode: 232   score: 14.0   memory length: 161383   epsilon: 1.0    steps: 860     evaluation reward: 9.57\n",
      "episode: 233   score: 4.0   memory length: 161902   epsilon: 1.0    steps: 519     evaluation reward: 9.48\n",
      "episode: 234   score: 12.0   memory length: 162705   epsilon: 1.0    steps: 803     evaluation reward: 9.53\n",
      "episode: 235   score: 7.0   memory length: 163220   epsilon: 1.0    steps: 515     evaluation reward: 9.53\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 0.522124. Value loss: 0.085605. Entropy: 0.380075.\n",
      "Iteration 2\n",
      "Policy loss: 0.956614. Value loss: 0.084168. Entropy: 0.364391.\n",
      "Iteration 3\n",
      "Policy loss: 0.493246. Value loss: 0.083740. Entropy: 0.352124.\n",
      "Iteration 4\n",
      "Policy loss: 1.155662. Value loss: 0.085021. Entropy: 0.336767.\n",
      "Iteration 5\n",
      "Policy loss: 9.267099. Value loss: 0.086729. Entropy: 0.329903.\n",
      "episode: 236   score: 10.0   memory length: 164041   epsilon: 1.0    steps: 821     evaluation reward: 9.59\n",
      "episode: 237   score: 4.0   memory length: 164544   epsilon: 1.0    steps: 503     evaluation reward: 9.53\n",
      "episode: 238   score: 12.0   memory length: 165351   epsilon: 1.0    steps: 807     evaluation reward: 9.58\n",
      "episode: 239   score: 19.0   memory length: 166470   epsilon: 1.0    steps: 1119     evaluation reward: 9.69\n",
      "episode: 240   score: 6.0   memory length: 166865   epsilon: 1.0    steps: 395     evaluation reward: 9.67\n",
      "episode: 241   score: 7.0   memory length: 167423   epsilon: 1.0    steps: 558     evaluation reward: 9.7\n",
      "episode: 242   score: 12.0   memory length: 168097   epsilon: 1.0    steps: 674     evaluation reward: 9.63\n",
      "episode: 243   score: 21.0   memory length: 169296   epsilon: 1.0    steps: 1199     evaluation reward: 9.67\n",
      "episode: 244   score: 15.0   memory length: 170677   epsilon: 1.0    steps: 1381     evaluation reward: 9.77\n",
      "episode: 245   score: 14.0   memory length: 171526   epsilon: 1.0    steps: 849     evaluation reward: 9.83\n",
      "episode: 246   score: 7.0   memory length: 172039   epsilon: 1.0    steps: 513     evaluation reward: 9.76\n",
      "episode: 247   score: 10.0   memory length: 172694   epsilon: 1.0    steps: 655     evaluation reward: 9.74\n",
      "episode: 248   score: 6.0   memory length: 173235   epsilon: 1.0    steps: 541     evaluation reward: 9.72\n",
      "episode: 249   score: 6.0   memory length: 173952   epsilon: 1.0    steps: 717     evaluation reward: 9.72\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 3.713853. Value loss: 0.104245. Entropy: 0.274849.\n",
      "Iteration 2\n",
      "Policy loss: 6.322644. Value loss: 0.102904. Entropy: 0.256687.\n",
      "Iteration 3\n",
      "Policy loss: 8.396249. Value loss: 0.100566. Entropy: nan.\n",
      "Iteration 4\n",
      "Policy loss: 11.329246. Value loss: 0.103412. Entropy: 0.241905.\n",
      "Iteration 5\n",
      "Policy loss: 5.956896. Value loss: 0.102575. Entropy: 0.234991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 250   score: 7.0   memory length: 174675   epsilon: 1.0    steps: 723     evaluation reward: 9.67\n",
      "episode: 251   score: 12.0   memory length: 175408   epsilon: 1.0    steps: 733     evaluation reward: 9.68\n",
      "episode: 252   score: 12.0   memory length: 176246   epsilon: 1.0    steps: 838     evaluation reward: 9.74\n",
      "episode: 253   score: 4.0   memory length: 176960   epsilon: 1.0    steps: 714     evaluation reward: 9.75\n",
      "episode: 254   score: 5.0   memory length: 177419   epsilon: 1.0    steps: 459     evaluation reward: 9.74\n",
      "episode: 255   score: 9.0   memory length: 178199   epsilon: 1.0    steps: 780     evaluation reward: 9.78\n",
      "episode: 256   score: 9.0   memory length: 178858   epsilon: 1.0    steps: 659     evaluation reward: 9.72\n",
      "episode: 257   score: 6.0   memory length: 179653   epsilon: 1.0    steps: 795     evaluation reward: 9.72\n",
      "episode: 258   score: 9.0   memory length: 180320   epsilon: 1.0    steps: 667     evaluation reward: 9.66\n",
      "episode: 259   score: 7.0   memory length: 180953   epsilon: 1.0    steps: 633     evaluation reward: 9.57\n",
      "episode: 260   score: 14.0   memory length: 181967   epsilon: 1.0    steps: 1014     evaluation reward: 9.6\n",
      "episode: 261   score: 8.0   memory length: 182693   epsilon: 1.0    steps: 726     evaluation reward: 9.62\n",
      "episode: 262   score: 10.0   memory length: 183458   epsilon: 1.0    steps: 765     evaluation reward: 9.59\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: 41.391483. Value loss: 0.117727. Entropy: nan.\n",
      "Iteration 2\n",
      "Policy loss: 614.071350. Value loss: 0.116178. Entropy: nan.\n",
      "Iteration 3\n",
      "Policy loss: 110.282845. Value loss: 0.116752. Entropy: nan.\n",
      "Iteration 4\n",
      "Policy loss: 174.227631. Value loss: 0.121030. Entropy: nan.\n",
      "Iteration 5\n",
      "Policy loss: 1886.021362. Value loss: 0.126241. Entropy: nan.\n",
      "episode: 263   score: 14.0   memory length: 184851   epsilon: 1.0    steps: 1393     evaluation reward: 9.65\n",
      "episode: 264   score: 10.0   memory length: 185628   epsilon: 1.0    steps: 777     evaluation reward: 9.68\n",
      "episode: 265   score: 7.0   memory length: 186241   epsilon: 1.0    steps: 613     evaluation reward: 9.68\n",
      "episode: 266   score: 15.0   memory length: 187138   epsilon: 1.0    steps: 897     evaluation reward: 9.67\n",
      "episode: 267   score: 6.0   memory length: 187764   epsilon: 1.0    steps: 626     evaluation reward: 9.61\n",
      "episode: 268   score: 13.0   memory length: 188583   epsilon: 1.0    steps: 819     evaluation reward: 9.64\n",
      "episode: 269   score: 9.0   memory length: 189391   epsilon: 1.0    steps: 808     evaluation reward: 9.65\n",
      "episode: 270   score: 4.0   memory length: 189909   epsilon: 1.0    steps: 518     evaluation reward: 9.58\n",
      "episode: 271   score: 11.0   memory length: 190650   epsilon: 1.0    steps: 741     evaluation reward: 9.6\n",
      "episode: 272   score: 4.0   memory length: 191193   epsilon: 1.0    steps: 543     evaluation reward: 9.58\n",
      "episode: 273   score: 14.0   memory length: 192052   epsilon: 1.0    steps: 859     evaluation reward: 9.66\n",
      "episode: 274   score: 14.0   memory length: 192869   epsilon: 1.0    steps: 817     evaluation reward: 9.68\n",
      "episode: 275   score: 9.0   memory length: 193562   epsilon: 1.0    steps: 693     evaluation reward: 9.7\n",
      "episode: 276   score: 7.0   memory length: 194175   epsilon: 1.0    steps: 613     evaluation reward: 9.69\n",
      "Training network\n",
      "Iteration 1\n",
      "Policy loss: inf. Value loss: 0.173820. Entropy: nan.\n",
      "Iteration 2\n",
      "Policy loss: inf. Value loss: 0.221577. Entropy: nan.\n",
      "Iteration 3\n",
      "Policy loss: inf. Value loss: 0.235638. Entropy: nan.\n",
      "Iteration 4\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        if render_breakout:\n",
    "            env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        action, value = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        r = np.clip(reward, -1, 1)\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state, value, 0, 0)\n",
    "        # Start training after random sample generation\n",
    "        if(frame % train_frame == 0):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network\n",
    "            agent.update_target_net()\n",
    "        score += r\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > 40 and len(evaluation_reward) > 5:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn\")\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
